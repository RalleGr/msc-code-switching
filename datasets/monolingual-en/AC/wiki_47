<doc id="28765" url="https://en.wikipedia.org/wiki?curid=28765" title="Lawrence Alma-Tadema">
Lawrence Alma-Tadema

Sir Lawrence Alma-Tadema, (; born Lourens Alma Tadema ; 8 January 1836 – 25 June 1912) was a Dutch painter of special British denizenship. Born in Dronryp, the Netherlands, and trained at the Royal Academy of Antwerp, Belgium, he settled in England in 1870 and spent the rest of his life there. A classical-subject painter, he became famous for his depictions of the luxury and decadence of the Roman Empire, with languorous figures set in fabulous marbled interiors or against a backdrop of dazzling blue Mediterranean Sea and sky. Alma-Tadema was considered one of the most popular Victorian painters. Though admired during his lifetime for his draftsmanship and depictions of Classical antiquity, his work fell into disrepute after his death, and only since the 1960s has it been re-evaluated for its importance within nineteenth-century British art. 

Lourens Alma Tadema was born on 8 January 1836 in the village of Dronryp in the province of Friesland in the north of the Netherlands. The surname "Tadema" is an old Frisian patronymic, meaning 'son of Tade', while the names "Lourens" and "Alma" came from his godfather. He was the sixth child of Pieter Jiltes Tadema (1797–1840), the village notary, and the third child of Hinke Dirks Brouwer (–1863). His father had three sons from a previous marriage. His parents' first child died young, and the second was Atje (–1876), Lourens' sister, for whom he had great affection.

The Tadema family moved in 1838 to the nearby city of Leeuwarden, where Pieter's position as a notary would be more lucrative. His father died when Lourens was four, leaving his mother with five children: Lourens, his sister, and three boys from his father's first marriage. His mother had artistic leanings, and decided that drawing lessons should be incorporated into the children's education. He received his first art training with a local drawing master hired to teach his older half-brothers.

It was intended that the boy would become a lawyer; but in 1851 at the age of fifteen he suffered a physical and mental breakdown. Diagnosed as consumptive and given only a short time to live, he was allowed to spend his remaining days at his leisure, drawing and painting. Left to his own devices he regained his health and decided to pursue a career as an artist. 

In 1852 he entered the Royal Academy of Antwerp in Belgium where he studied early Dutch and Flemish art, under Gustaf Wappers. During Alma-Tadema's four years as a registered student at the Academy, he won several respectable awards.

Before leaving school, towards the end of 1855, he became assistant to the painter and professor Louis (Lodewijk) Jan de Taeye, whose courses in history and historical costume he had greatly enjoyed at the Academy. Although de Taeye was not an outstanding painter, Alma-Tadema respected him and became his studio assistant, working with him for three years. De Taeye introduced him to books that influenced his desire to portray Merovingian subjects early in his career. He was encouraged to depict historical accuracy in his paintings, a trait for which the artist became known.

Alma-Tadema left Taeye's studio in November 1858 returning to Leeuwarden before settling in Antwerp, where he began working with the painter Baron Jan August Hendrik Leys, whose studio was one of the most highly regarded in Belgium. Under his guidance Alma-Tadema painted his first major work: "The Education of the children of Clovis" (1861). This painting created a sensation among critics and artists when it was exhibited that year at the Artistic Congress in Antwerp. It is said to have laid the foundation of his fame and reputation. Alma-Tadema related that although Leys thought the completed painting better than he had expected, he was critical of the treatment of marble, which he compared to cheese.

Alma-Tadema took this criticism very seriously, and it led him to improve his technique and to become the world's foremost painter of marble and variegated granite. Despite any reproaches from his master, "The Education of the Children of Clovis" was honorably received by critics and artists alike and was eventually purchased and subsequently given to King Leopold of Belgium.

In 1860 he befriended the Anglo-Dutch Dommersen family of artists in Utrecht In 1862 he made pencil drawings of Mrs. Cornelia Dommershuizen and one of her sons Thomas Hendrik, whose brothers were the painters Pieter Cornelis Dommersen and Cornelis Christiaan Dommersen.

Merovingian themes were the painter's favourite subject up to the mid-1860s. It is perhaps in this series that we find the artist moved by the deepest feeling and the strongest spirit of romance. However Merovingian subjects did not have a wide international appeal, so he switched to themes of life in ancient Egypt that were more popular. On these scenes of Frankish and Egyptian life Alma-Tadema spent great energy and much research. In 1862 Alma-Tadema left Leys's studio and started his own career, establishing himself as a significant classical-subject European artist.

1863 was to alter the course of Alma-Tadema's personal and professional life: on 3 January his invalid mother died, and on 24 September he was married, in Antwerp City Hall, to Marie-Pauline Gressin-Dumoulin de Boisgirard, the daughter of Eugène Gressin-Dumoulin, a French journalist living near Brussels. Nothing is known of their meeting and little of Pauline herself, as Alma-Tadema never spoke about her after her death in 1869. Her image appears in a number of oils, though he painted her portrait only three times, the most notable appearing in "My studio" (1867). The couple had three children. Their eldest and only son lived only a few months dying of smallpox. Their two daughters, Laurence (1864–1940) and Anna (1867–1943), both had artistic leanings: the former in literature, the latter in art. Neither would marry. 

Alma-Tadema and his wife spent their honeymoon in Florence, Rome, Naples and Pompeii. This, his first visit to Italy, developed his interest in depicting the life of ancient Greece and Rome, especially the latter since he found new inspiration in the ruins of Pompeii, which fascinated him and would inspire much of his work in the coming decades.

During the summer of 1864, Tadema met Ernest Gambart, the most influential print publisher and art dealer of the period. Gambart was highly impressed with the work of Tadema, who was then painting "Egyptian chess players" (1865). The dealer, recognising at once the unusual gifts of the young painter, gave him an order for twenty-four pictures and arranged for three of Tadema's paintings to be shown in London. In 1865, Tadema relocated to Brussels where he was named a knight of the Order of Leopold.

On 28 May 1869, after years of ill health, Pauline died at Schaerbeek, in Belgium, at the age of thirty-two, of smallpox. Her death left Tadema disconsolate and depressed. He ceased painting for nearly four months. His sister Artje, who lived with the family, helped with the two daughters then aged five and two. Artje took over the role of housekeeper and remained with the family until 1873 when she married.

During the summer Tadema himself began to suffer from a medical problem which doctors in Brussels were frustratingly unable to diagnose. Gambart eventually advised him to go to England for another medical opinion. Soon after his arrival in London in December 1869, Alma-Tadema was invited to the home of the painter Ford Madox Brown. There he met Laura Theresa Epps, who was seventeen years old, and fell in love with her at first sight.

The outbreak of the Franco-Prussian War in July 1870 compelled Alma-Tadema to leave the continent and move to London. His infatuation with Laura Epps played a great part in his relocation to England and Gambart felt that the move would be advantageous to the artist's career. In stating his reasons for the move, Tadema simply said "I lost my first wife, a French lady with whom I married in 1863, in 1869. Having always had a great predilection for London, the only place where, up till then my work had met with buyers, I decided to leave the continent and go to settle in England, where I have found a true home."

With his small daughters and sister Atje, Alma-Tadema arrived in London at the beginning of September 1870. The painter wasted no time in contacting Laura, and it was arranged that he would give her painting lessons. During one of these, he proposed marriage. As he was then thirty-four and Laura was now only eighteen, her father was initially opposed to the idea. Dr Epps finally agreed on the condition that they should wait until they knew each other better. They married in July 1871. Laura, under her married name, also won a high reputation as an artist, and appears in numerous of Alma-Tadema's canvases after their marriage ("The Women of Amphissa" (1887) being a notable example). This second marriage was enduring and happy, though childless, and Laura became stepmother to Anna and Laurence. Anna became a painter and Laurence became a novelist.

He would initially adopt the name "Laurence Alma Tadema" instead of "Lourens Alma Tadema" and later adopt the more English "Lawrence" for his forename, and incorporate "Alma" into his surname so that he appeared at the beginning of exhibition catalogues, under "A" rather than under "T". He did not actually hyphenate his last name, but it was done by others and this has since become the convention.

After his arrival in England, where he was to spend the rest of his life, Alma-Tadema's career was one of continued success. He became one of the most famous and highly paid artists of his time, acknowledged and rewarded. By 1871 he had met and befriended most of the major Pre-Raphaelite painters and it was in part due to their influence that the artist brightened his palette, varied his hues, and lightened his brushwork.

In 1872 Alma-Tadema organised his paintings into an identification system by including an opus number under his signature and assigning his earlier pictures numbers as well. "Portrait of my sister, Artje", painted in 1851, is numbered opus I, while two months before his death he completed "Preparations in the Coliseum", opus CCCCVIII. Such a system would make it difficult for fakes to be passed off as originals.

In 1873 Queen Victoria in Council by letters patent made Alma-Tadema and his wife what are now the last British Denizens (the legal process has theoretically not yet been abolished in the United Kingdom), with some limited special rights otherwise only accorded to and enjoyed by British subjects (what would now be called British citizens). The previous year he and his wife made a journey on the Continent that lasted five and a half months and took them through Brussels, Germany, and Italy. In Italy they were able to take in the ancient ruins again; this time he purchased several photographs, mostly of the ruins, which began his immense collection of folios with archival material sufficient for the documentation used in the completion of future paintings. In January 1876, he rented a studio in Rome. The family returned to London in April, visiting the Parisian Salon on their way back. In London he regularly met with fellow-artist Emil Fuchs.

Among the most important of his pictures during this period was "An Audience at Agrippa's" (1876). When an admirer of the painting offered to pay a substantial sum for a painting with a similar theme, Alma-Tadema simply turned the emperor around to show him leaving in "After the Audience".

On 19 June 1879, Alma-Tadema was made a full Academician, his most personally important award. Three years later a major retrospective of his entire oeuvre was organised at the Grosvenor Gallery in London, including 185 of his pictures.

In 1883 he returned to Rome and, most notably, Pompeii, where further excavations had taken place since his last visit. He spent a significant amount of time studying the site, going there daily. These excursions gave him an ample source of subject matter as he began to further his knowledge of daily Roman life. At times, however, he integrated so many objects into his paintings that some said they resembled museum catalogues.

One of his most famous paintings is "The Roses of Heliogabalus" (1888) – based on an episode from the life of the debauched Roman Emperor Elagabalus (Heliogabalus), the painting depicts the Emperor suffocating his guests at an orgy under a cascade of rose petals. The blossoms depicted were sent weekly to the artist's London studio from the Riviera for four months during the winter of 1887–1888.

Among Alma-Tadema's works of this period are: "An Earthly Paradise" (1891), "Unconscious Rivals" (1893) "Spring" (1894), "The Coliseum" (1896) and "The Baths of Caracalla" (1899). Although Alma-Tadema's fame rests on his paintings set in Antiquity, he also painted portraits, landscapes and watercolours, and made some etchings himself (although many more were made of his paintings by others).

For all the quiet charm and erudition of his paintings, Alma-Tadema himself preserved a youthful sense of mischief. He was childlike in his practical jokes and in his sudden bursts of bad temper, which could as suddenly subside into an engaging smile.

In his personal life, Alma-Tadema was an extrovert and had a remarkably warm personality. He had most of the characteristics of a child, coupled with the admirable traits of a consummate professional. A perfectionist, he remained in all respects a diligent, if somewhat obsessive and pedantic worker. He was an excellent businessman, and one of the wealthiest artists of the nineteenth century. Alma-Tadema was as firm in money matters as he was with the quality of his work.

As a man, Lawrence Alma-Tadema was a robust, fun loving and rather portly gentleman. There was not a hint of the delicate artist about him; he was a cheerful lover of wine, women, and parties.

Alma-Tadema's output decreased with time, partly on account of health, but also because of his obsession with decorating his new home, to which he moved in 1883. Nevertheless, he continued to exhibit throughout the 1880s and into the next decade, receiving a plentiful amount of accolades along the way, including the medal of Honour at the Paris Exposition Universelle of 1889, election to an honorary member of the Oxford University Dramatic Society in 1890, the Great Gold Medal at the International Exposition in Brussels of 1897. In 1899 he was Knighted in England, only the eighth artist from the Continent to receive the honour. Not only did he assist with the organisation of the British section at the 1900 Exposition Universelle in Paris, he also exhibited two works that earned him the Grand Prix Diploma. He also assisted with the St. Louis World's Fair of 1904 where he was well represented and received.

During this time, Alma-Tadema was very active with theatre design and production, designing many costumes. He also spread his artistic boundaries and began to design furniture, often modelled after Pompeian or Egyptian motifs, illustrations, textiles, and frame making. His diverse interests highlight his talents. Each of these exploits were used in his paintings, as he often incorporated some of his designed furniture into the composition, and must have used many of his own designs for the clothing of his female subjects. Through his last period of creativity Alma-Tadema continued to produce paintings, which repeat the successful formula of women in marble terraces overlooking the sea such as in "Silver Favourites" (1903). Between 1906 and his death six years later, Alma-Tadema painted less but still produced ambitious paintings like "The Finding of Moses" (1904).

On 15 August 1909 Alma-Tadema's wife, Laura, died at the age of fifty-seven. The grief-stricken widower outlived his second wife by less than three years. His last major composition was "Preparation in the Coliseum" (1912). In the summer of 1912, Alma-Tadema was accompanied by his daughter Anna to Kaiserhof Spa, Wiesbaden, Germany where he was to undergo treatment for ulceration of the stomach. He died there on 28 June 1912 at the age of seventy-six. He was buried in a crypt in St Paul's Cathedral in London.

Alma-Tadema's works are remarkable for the way in which flowers, textures and hard reflecting substances, like metals, pottery, and especially marble, are painted – indeed, his realistic depiction of marble led him to be called the 'marbellous painter'. His work shows much of the fine execution and brilliant colour of the old Dutch masters. By the human interest with which he imbues all his scenes from ancient life he brings them within the scope of modern feeling, and charms us with gentle sentiment and playfulness.

From early in his career, Alma-Tadema was particularly concerned with architectural accuracy, often including objects that he would see at museums – such as the British Museum in London – in his works. He also read many books and took many images from them. He amassed an enormous number of photographs from ancient sites in Italy, which he used for the most precise accuracy in the details of his compositions.

Alma-Tadema was a perfectionist. He worked assiduously to make the most of his paintings, often repeatedly reworking parts of paintings before he found them satisfactory to his own high standards. One humorous story relates that one of his paintings was rejected and instead of keeping it, he gave the canvas to a maid who used it as her table cover. He was sensitive to every detail and architectural line of his paintings, as well as the settings he was depicting. For many of the objects in his paintings, he would depict what was in front of him, using fresh flowers imported from across the continent and even from Africa, rushing to finish the paintings before the flowers died. It was this commitment to veracity that earned him recognition but also caused many of his adversaries to take up arms against his almost encyclopaedic works.

Alma-Tadema's work has been linked with that of European Symbolist painters. As an artist of international reputation, he can be cited as an influence on European figures such as Gustav Klimt and Fernand Khnopff. Both painters incorporate classical motifs into their works and use Alma-Tadema's unconventional compositional devices such as abrupt cut-off at the edge of the canvas. They, like Alma-Tadema, also employ coded imagery to convey meaning to their paintings.

Alma-Tadema was considered one of the most popular Victorian painters. He was among the most financially successful painters of the Victorian era, though never matching Edwin Henry Landseer. For over sixty years he gave his audience exactly what they wanted: distinctive, elaborate paintings of beautiful people in classical settings. His incredibly detailed reconstructions of ancient Rome, with languid men and women posed against white marble in dazzling sunlight provided his audience with a glimpse of a world of the kind they might one day construct for themselves at least in attitude if not in detail. As with other painters, the reproduction rights for prints were often worth more than the canvas, and a painting with its rights still attached may have been sold to Gambart for £10,000 in 1874; without rights it was sold again in 1903, when Alma-Tadema's prices were actually higher, for £2,625. Typical prices were between £2,000 and £3,000 in the 1880s, but at least three works sold for between £5,250 and £6,060 in the 1900s. Prices held well until the general collapse of Victorian prices in the early 1920s, when they fell to the hundreds, where they remained until the 1960s; by 1969 £4,600 had been reached again (the huge effect of inflation must of course be remembered for all these figures).

The last years of Alma-Tadema's life saw the rise of Post-Impressionism, Fauvism, Cubism and Futurism, of which he heartily disapproved. As his pupil John Collier wrote, 'it is impossible to reconcile the art of Alma-Tadema with that of Matisse, Gauguin and Picasso.'

His artistic legacy almost vanished. As attitudes of the public in general and the artists in particular became more sceptical of the possibilities of human achievement, his paintings were increasingly denounced. He was declared "the worst painter of the 19th century" by John Ruskin, and one critic even remarked that his paintings were "about worthy enough to adorn bourbon boxes". After this brief period of being actively derided, he was consigned to relative obscurity for many years. Only since the 1960s has Alma-Tadema's work been re-evaluated for its importance within the nineteenth century, and more specifically, within the evolution of English art.

He is now regarded as one of the principal classical-subject painters of the nineteenth century whose works demonstrate the care and exactitude of an era mesmerised by trying to visualise the past, some of which was being recovered through archaeological research.

Alma-Tadema's meticulous archaeological research, including research into Roman architecture (which was so thorough that every building featured in his canvases could have been built using Roman tools and methods) led to his paintings being used as source material by Hollywood directors in their vision of the ancient world for films such as D. W. Griffith's "Intolerance" (1916), "Ben Hur" (1926)," Cleopatra" (1934), and most notably of all, Cecil B. DeMille's epic remake of "The Ten Commandments" (1956). Indeed, Jesse Lasky Jr., the co-writer on "The Ten Commandments", described how the director would customarily spread out prints of Alma-Tadema paintings to indicate to his set designers the look he wanted to achieve. The designers of the Oscar-winning Roman epic "Gladiator" used the paintings of Alma-Tadema as a central source of inspiration. Alma-Tadema's paintings were also the inspiration for the design of the interior of Cair Paravel castle in the 2005 film "".

In 1962, New York art dealer Robert Isaacson mounted the first show of Alma-Tadema's work in fifty years; by the late 1960s, the revival of interest in Victorian painting gained impetus, and a number of well-attended exhibitions were held. Allen Funt, the creator and host of the American version of the television show "Candid Camera", was a collector of Alma-Tadema paintings at a time when the artist's reputation in the 20th century was at its nadir; in a relatively few years he bought 35 works, about ten percent of Alma-Tadema's output. After Funt was robbed by his accountant (who subsequently committed suicide), he was forced to sell his collection at Sotheby's in London in November 1973. From this sale, the interest in Alma-Tadema was re-awakened.

In 1960, the Newman Gallery firstly tried to sell, then give away (without success) one of his most celebrated works, "The Finding of Moses" (1904). The initial purchaser had paid £5,250 for it on its completion, and subsequent sales were for £861 in 1935, £265 in 1942, and it was "bought in" at £252 in 1960 (having failed to meet its reserve), but when the same picture was auctioned at Christies in New York in May 1995, it sold for £1.75 million. On 4 November 2010 it was sold for $35,922,500 to an undisclosed bidder at Sotheby's New York, a new record both for an Alma-Tadema work and for a Victorian painting. On 5 May 2011 his "The Meeting of Antony and Cleopatra: 41 BC" was sold at the same auction house for $29.2 million.

Alma-Tadema's "The Tepidarium" (1881) is included in the 2006 book "1001 Paintings You Must See Before You Die". Julian Treuherz, Keeper of Art Galleries at National Museums Liverpool, describes it as an "exquisitely painted picture..." which "carries a strong erotic charge, rare for a Victorian painting of the nude".

A blue plaque unveiled in 1975 commemorates Alma-Tadema at 44 Grove End Road, St John's Wood, his home from 1886 until his death in 1912.



</doc>
<doc id="28766" url="https://en.wikipedia.org/wiki?curid=28766" title="Surrealism">
Surrealism

Surrealism was a cultural movement which developed in Europe in the aftermath of World War I and was largely influenced by Dada. The movement is best known for its visual artworks and writings and the juxtaposition of uncommon imagery. Artists painted unnerving, illogical scenes, sometimes with photographic precision, creating strange creatures from everyday objects, and developing painting techniques that allowed the unconscious to express itself. Its aim was, according to leader André Breton, to "resolve the previously contradictory conditions of dream and reality into an absolute reality, a super-reality", or "surreality."

Works of surrealism feature the element of surprise, unexpected juxtapositions and "non sequitur"; however, many surrealist artists and writers regard their work as an expression of the philosophical movement first and foremost, with the works themselves being an artifact. Leader Breton was explicit in his assertion that Surrealism was, above all, a revolutionary movement. At the time, the movement was associated with political causes such as communism and anarchism.

The term "Surrealism" is said to have been coined by Guillaume Apollinaire as early as 1917. However, the Surrealist movement was not officially established until October 15, 1924, when the French poet and critic André Breton published the Surrealist Manifesto in Paris. The most important center of the movement was Paris, France. From the 1920s onward, the movement spread around the globe, impacting the visual arts, literature, film, and music of many countries and languages, as well as political thought and practice, philosophy, and social theory.

The word 'surrealism' was first coined in March 1917 by Guillaume Apollinaire. He wrote in a letter to Paul Dermée: "All things considered, I think in fact it is better to adopt surrealism than supernaturalism, which I first used" ["Tout bien examiné, je crois en effet qu'il vaut mieux adopter surréalisme que surnaturalisme que j'avais d'abord employé"].

Apollinaire used the term in his program notes for Sergei Diaghilev's Ballets Russes, "Parade", which premiered 18 May 1917. "Parade" had a one-act scenario by Jean Cocteau and was performed with music by Erik Satie. Cocteau described the ballet as "realistic". Apollinaire went further, describing "Parade" as "surrealistic":

This new alliance—I say new, because until now scenery and costumes were linked only by factitious bonds—has given rise, in "Parade", to a kind of surrealism, which I consider to be the point of departure for a whole series of manifestations of the New Spirit that is making itself felt today and that will certainly appeal to our best minds. We may expect it to bring about profound changes in our arts and manners through universal joyfulness, for it is only natural, after all, that they keep pace with scientific and industrial progress. (Apollinaire, 1917)

The term was taken up again by Apollinaire, both as subtitle and in the preface to his play "Les Mamelles de Tirésias: Drame surréaliste", which was written in 1903 and first performed in 1917.

World War I scattered the writers and artists who had been based in Paris, and in the interim many became involved with Dada, believing that excessive rational thought and bourgeois values had brought the conflict of the war upon the world. The Dadaists protested with anti-art gatherings, performances, writings and art works. After the war, when they returned to Paris, the Dada activities continued.

During the war, André Breton, who had trained in medicine and psychiatry, served in a neurological hospital where he used Sigmund Freud's psychoanalytic methods with soldiers suffering from shell-shock. Meeting the young writer Jacques Vaché, Breton felt that Vaché was the spiritual son of writer and pataphysics founder Alfred Jarry. He admired the young writer's anti-social attitude and disdain for established artistic tradition. Later Breton wrote, "In literature, I was successively taken with Rimbaud, with Jarry, with Apollinaire, with Nouveau, with Lautréamont, but it is Jacques Vaché to whom I owe the most."

Back in Paris, Breton joined in Dada activities and started the literary journal "Littérature" along with Louis Aragon and Philippe Soupault. They began experimenting with automatic writing—spontaneously writing without censoring their thoughts—and published the writings, as well as accounts of dreams, in the magazine. Breton and Soupault continued writing evolving their techniques of automatism and published "The Magnetic Fields" (1920).

By October 1924 two rival Surrealist groups had formed to publish a Surrealist Manifesto. Each claimed to be successors of a revolution launched by Appolinaire. One group, led by Yvan Goll consisted of Pierre Albert-Birot, Paul Dermée, Céline Arnauld, Francis Picabia, Tristan Tzara, Giuseppe Ungaretti, Pierre Reverdy, Marcel Arland, Joseph Delteil, Jean Painlevé and Robert Delaunay, among others The group led by André Breton claimed that automatism was a better tactic for societal change than those of Dada, as led by Tzara, who was now among their rivals. Breton's group grew to include writers and artists from various media such as Paul Éluard, Benjamin Péret, René Crevel, Robert Desnos, Jacques Baron, Max Morise, Pierre Naville, Roger Vitrac, Gala Éluard, Max Ernst, Salvador Dalí, Luis Buñuel, Man Ray, Hans Arp, Georges Malkine, Michel Leiris, Georges Limbour, Antonin Artaud, Raymond Queneau, André Masson, Joan Miró, Marcel Duchamp, Jacques Prévert, and Yves Tanguy.

As they developed their philosophy, they believed that Surrealism would advocate the idea that ordinary and depictive expressions are vital and important, but that the sense of their arrangement must be open to the full range of imagination according to the Hegelian Dialectic. They also looked to the Marxist dialectic and the work of such theorists as Walter Benjamin and Herbert Marcuse.

Freud's work with free association, dream analysis, and the unconscious was of utmost importance to the Surrealists in developing methods to liberate imagination. They embraced idiosyncrasy, while rejecting the idea of an underlying madness. As Dalí later proclaimed, "There is only one difference between a madman and me. I am not mad."

Beside the use of dream analysis, they emphasized that "one could combine inside the same frame, elements not normally found together to produce illogical and startling effects." Breton included the idea of the startling juxtapositions in his 1924 manifesto, taking it in turn from a 1918 essay by poet Pierre Reverdy, which said: "a juxtaposition of two more or less distant realities. The more the relationship between the two juxtaposed realities is distant and true, the stronger the image will be−the greater its emotional power and poetic reality."

The group aimed to revolutionize human experience, in its personal, cultural, social, and political aspects. They wanted to free people from false rationality, and restrictive customs and structures. Breton proclaimed that the true aim of Surrealism was "long live the social revolution, and it alone!" To this goal, at various times Surrealists aligned with communism and anarchism.

In 1924 two Surrealist factions declared their philosophy in two separate Surrealist Manifestos. That same year the Bureau of Surrealist Research was established, and began publishing the journal "La Révolution surréaliste".

Leading up to 1924, two rival surrealist groups had formed. Each group claimed to be successors of a revolution launched by Apollinaire. One group, led by Yvan Goll, consisted of Pierre Albert-Birot, Paul Dermée, Céline Arnauld, Francis Picabia, Tristan Tzara, Giuseppe Ungaretti, Pierre Reverdy, Marcel Arland, Joseph Delteil, Jean Painlevé and Robert Delaunay, among others.

The other group, led by Breton, included Aragon, Desnos, Éluard, Baron, Crevel, Malkine, Jacques-André Boiffard and Jean Carrive, among others.

Yvan Goll published the "Manifeste du surréalisme", 1 October 1924, in his first and only issue of "Surréalisme" two weeks prior to the release of Breton's "Manifeste du surréalisme", published by Éditions du Sagittaire, 15 October 1924.

Goll and Breton clashed openly, at one point literally fighting, at the Comédie des Champs-Élysées, over the rights to the term Surrealism. In the end, Breton won the battle through tactical and numerical superiority. Though the quarrel over the anteriority of Surrealism concluded with the victory of Breton, the history of surrealism from that moment would remain marked by fractures, resignations, and resounding excommunications, with each surrealist having their own view of the issue and goals, and accepting more or less the definitions laid out by André Breton.

Breton's 1924 "Surrealist Manifesto" defines the purposes of Surrealism. He included citations of the influences on Surrealism, examples of Surrealist works, and discussion of Surrealist automatism. He provided the following definitions:

The movement in the mid-1920s was characterized by meetings in cafes where the Surrealists played collaborative drawing games, discussed the theories of Surrealism, and developed a variety of techniques such as automatic drawing. Breton initially doubted that visual arts could even be useful in the Surrealist movement since they appeared to be less malleable and open to chance and automatism. This caution was overcome by the discovery of such techniques as frottage and decalcomania.

Soon more visual artists became involved, including Giorgio de Chirico, Max Ernst, Joan Miró, Francis Picabia, Yves Tanguy, Salvador Dalí, Luis Buñuel, Alberto Giacometti, Valentine Hugo, Méret Oppenheim, Toyen, Kansuke Yamamoto and later after the second war: Enrico Donati. Though Breton admired Pablo Picasso and Marcel Duchamp and courted them to join the movement, they remained peripheral. More writers also joined, including former Dadaist Tristan Tzara, René Char, and Georges Sadoul.
In 1925 an autonomous Surrealist group formed in Brussels. The group included the musician, poet, and artist E. L. T. Mesens, painter and writer René Magritte, Paul Nougé, Marcel Lecomte, and André Souris. In 1927 they were joined by the writer Louis Scutenaire. They corresponded regularly with the Paris group, and in 1927 both Goemans and Magritte moved to Paris and frequented Breton's circle. The artists, with their roots in Dada and Cubism, the abstraction of Wassily Kandinsky, Expressionism, and Post-Impressionism, also reached to older "bloodlines" or proto-surrealists such as Hieronymus Bosch, and the so-called primitive and naive arts.

André Masson's automatic drawings of 1923 are often used as the point of the acceptance of visual arts and the break from Dada, since they reflect the influence of the idea of the unconscious mind. Another example is Giacometti's 1925 "Torso", which marked his movement to simplified forms and inspiration from preclassical sculpture.

However, a striking example of the line used to divide Dada and Surrealism among art experts is the pairing of 1925's "Little Machine Constructed by Minimax Dadamax in Person (Von minimax dadamax selbst konstruiertes maschinchen)" with "The Kiss (Le Baiser)" from 1927 by Max Ernst. The first is generally held to have a distance, and erotic subtext, whereas the second presents an erotic act openly and directly. In the second the influence of Miró and the drawing style of Picasso is visible with the use of fluid curving and intersecting lines and colour, whereas the first takes a directness that would later be influential in movements such as Pop art.

Giorgio de Chirico, and his previous development of metaphysical art, was one of the important joining figures between the philosophical and visual aspects of Surrealism. Between 1911 and 1917, he adopted an unornamented depictional style whose surface would be adopted by others later. "The Red Tower (La tour rouge)" from 1913 shows the stark colour contrasts and illustrative style later adopted by Surrealist painters. His 1914 "The Nostalgia of the Poet (La Nostalgie du poète)" has the figure turned away from the viewer, and the juxtaposition of a bust with glasses and a fish as a relief defies conventional explanation. He was also a writer whose novel "Hebdomeros" presents a series of dreamscapes with an unusual use of punctuation, syntax, and grammar designed to create an atmosphere and frame its images. His images, including set designs for the Ballets Russes, would create a decorative form of Surrealism, and he would be an influence on the two artists who would be even more closely associated with Surrealism in the public mind: Dalí and Magritte. He would, however, leave the Surrealist group in 1928.

In 1924, Miró and Masson applied Surrealism to painting. The first Surrealist exhibition, "La Peinture Surrealiste", was held at Galerie Pierre in Paris in 1925. It displayed works by Masson, Man Ray, Paul Klee, Miró, and others. The show confirmed that Surrealism had a component in the visual arts (though it had been initially debated whether this was possible), and techniques from Dada, such as photomontage, were used. The following year, on March 26, 1926 Galerie Surréaliste opened with an exhibition by Man Ray. Breton published "Surrealism and Painting" in 1928 which summarized the movement to that point, though he continued to update the work until the 1960s.

The first Surrealist work, according to leader Brêton, was Les Chants de Maldoror; and the first work written and published by his group of "Surréalistes" was "Les Champs Magnétiques" (May–June 1919). "Littérature" contained automatist works and accounts of dreams. The magazine and the portfolio both showed their disdain for literal meanings given to objects and focused rather on the undertones, the poetic undercurrents present. Not only did they give emphasis to the poetic undercurrents, but also to the connotations and the overtones which "exist in ambiguous relationships to the visual images"

Because Surrealist writers seldom, if ever, appear to organize their thoughts and the images they present, some people find much of their work difficult to parse. This notion however is a superficial comprehension, prompted no doubt by Breton's initial emphasis on automatic writing as the main route toward a higher reality. But—as in Breton's case—much of what is presented as purely automatic is actually edited and very "thought out". Breton himself later admitted that automatic writing's centrality had been overstated, and other elements were introduced, especially as the growing involvement of visual artists in the movement forced the issue, since automatic painting required a rather more strenuous set of approaches. Thus such elements as collage were introduced, arising partly from an ideal of startling juxtapositions as revealed in Pierre Reverdy's poetry. And—as in Magritte's case (where there is no obvious recourse to either automatic techniques or collage)—the very notion of convulsive joining became a tool for revelation in and of itself. Surrealism was meant to be always in flux—to be more modern than modern—and so it was natural there should be a rapid shuffling of the philosophy as new challenges arose.

Surrealists revived interest in Isidore Ducasse, known by his pseudonym Comte de Lautréamont, and for the line "beautiful as the chance meeting on a dissecting table of a sewing machine and an umbrella", and Arthur Rimbaud, two late 19th-century writers believed to be the precursors of Surrealism.

Examples of Surrealist literature are Artaud's "Le Pèse-Nerfs" (1926), Aragon's "Irene's Cunt" (1927), Péret's "Death to the Pigs" (1929), Crevel's "Mr. Knife Miss Fork" (1931), Sadegh Hedayat's "the Blind Owl" (1937), and Breton's "Sur la route de San Romano" (1948).

"La Révolution surréaliste" continued publication into 1929 with most pages densely packed with columns of text, but also included reproductions of art, among them works by de Chirico, Ernst, Masson, and Man Ray. Other works included books, poems, pamphlets, automatic texts and theoretical tracts.

Early films by Surrealists include:

The word "surrealist" was first used by Apollinaire to describe his 1917 play "Les Mamelles de Tirésias" ("The Breasts of Tiresias"), which was later adapted into an opera by Francis Poulenc.

Antonin Artaud, an early Surrealist, rejected the majority of Western theatre as a perversion of its original intent, which he felt should be a mystical, metaphysical experience. He thought that rational discourse comprised "falsehood and illusion". Theorising a new theatrical form that would be immediate and direct, that would link the unconscious minds of performers and spectators in a sort of ritual event, Artaud created the Theatre of Cruelty, in which emotions, feelings, and the metaphysical were expressed not through language but physically, creating a mythological, archetypal, allegorical vision, closely related to the world of dreams.

The other major theatre practitioner to have experimented with surrealism in the theatre is the Spanish playwright and director Federico García Lorca, particularly in his plays "The Public" (1930), "When Five Years Pass" (1931), and "Play Without a Title" (1935). Other surrealist plays include Aragon's "Backs to the Wall" (1925) and Roger Vitrac's "The Mysteries of Love" (1927) and "Victor, or The Children Take Over" (1928). Gertrude Stein's opera "Doctor Faustus Lights the Lights" (1938) has also been described as "American Surrealism", though it is also related to a theatrical form of cubism.

In the 1920s several composers were influenced by Surrealism, or by individuals in the Surrealist movement. Among them were Bohuslav Martinů, André Souris, Erik Satie, and Edgard Varèse, who stated that his work "Arcana" was drawn from a dream sequence. Souris in particular was associated with the movement: he had a long relationship with Magritte, and worked on Paul Nougé's publication "Adieu Marie".

Germaine Tailleferre of the French group Les Six wrote several works which could be considered to be inspired by Surrealism, including the 1948 ballet "Paris-Magie" (scenario by Lise Deharme), the operas "La Petite Sirène" (book by Philippe Soupault) and "Le Maître" (book by Eugène Ionesco). Tailleferre also wrote popular songs to texts by Claude Marci, the wife of Henri Jeanson, whose portrait had been painted by Magritte in the 1930s.

Even though Breton by 1946 responded rather negatively to the subject of music with his essay "Silence is Golden", later Surrealists, such as Paul Garon, have been interested in—and found parallels to—Surrealism in the improvisation of jazz and the blues. Jazz and blues musicians have occasionally reciprocated this interest. For example, the 1976 World Surrealist Exhibition included performances by David "Honeyboy" Edwards.

Surrealism as a political force developed unevenly around the world: in some places more emphasis was on artistic practices, in other places on political practices, and in other places still, Surrealist praxis looked to supersede both the arts and politics. During the 1930s, the Surrealist idea spread from Europe to North America, South America (founding of the "Mandrágora" group in Chile in 1938), Central America, the Caribbean, and throughout Asia, as both an artistic idea and as an ideology of political change.

Politically, Surrealism was Trotskyist, communist, or anarchist. The split from Dada has been characterised as a split between anarchists and communists, with the Surrealists as communist. Breton and his comrades supported Leon Trotsky and his International Left Opposition for a while, though there was an openness to anarchism that manifested more fully after World War II. Some Surrealists, such as Benjamin Péret, Mary Low, and Juan Breá, aligned with forms of left communism. Others fought for complete liberty from political ideologies, like Wolfgang Paalen, who, after Trotsky's assassination in Mexico, prepared a schism between art and politics through his counter-surrealist art-magazine DYN and so prepared the ground for the abstract expressionists. Dalí supported capitalism and the fascist dictatorship of Francisco Franco but cannot be said to represent a trend in Surrealism in this respect; in fact he was considered, by Breton and his associates, to have betrayed and left Surrealism. Benjamin Péret, Mary Low and Juan Breá joined the POUM during the Spanish Civil War.

Breton's followers, along with the Communist Party, were working for the "liberation of man". However, Breton's group refused to prioritize the proletarian struggle over radical creation such that their struggles with the Party made the late 1920s a turbulent time for both. Many individuals closely associated with Breton, notably Aragon, left his group to work more closely with the Communists.

Surrealists have often sought to link their efforts with political ideals and activities. In the "Declaration of January 27, 1925", for example, members of the Paris-based Bureau of Surrealist Research (including Breton, Aragon and Artaud, as well as some two dozen others) declared their affinity for revolutionary politics. While this was initially a somewhat vague formulation, by the 1930s many Surrealists had strongly identified themselves with communism. The foremost document of this tendency within Surrealism is the "Manifesto for a Free Revolutionary Art", published under the names of Breton and Diego Rivera, but actually co-authored by Breton and Leon Trotsky.

However, in 1933 the Surrealists’ assertion that a 'proletarian literature' within a capitalist society was impossible led to their break with the Association des Ecrivains et Artistes Révolutionnaires, and the expulsion of Breton, Éluard and Crevel from the Communist Party.

In 1925, the Paris Surrealist group and the extreme left of the French Communist Party came together to support Abd-el-Krim, leader of the Rif uprising against French colonialism in Morocco. In an open letter to writer and French ambassador to Japan, Paul Claudel, the Paris group announced:

The anticolonial revolutionary and proletarian politics of "Murderous Humanitarianism" (1932) which was drafted mainly by Crevel, signed by Breton, Éluard, Péret, Tanguy, and the Martiniquan Surrealists Pierre Yoyotte and J.M. Monnerot perhaps makes it the original document of what is later called 'black Surrealism', although it is the contact between Aimé Césaire and Breton in the 1940s in Martinique that really lead to the communication of what is known as 'black Surrealism'.

Anticolonial revolutionary writers in the Négritude movement of Martinique, a French colony at the time, took up Surrealism as a revolutionary method - a critique of European culture and a radical subjective. This linked with other Surrealists and was very important for the subsequent development of Surrealism as a revolutionary praxis. The journal "Tropiques", featuring the work of Césaire along with Suzanne Césaire, René Ménil, Lucie Thésée, Aristide Maugée and others, was first published in 1941.

In 1938 André Breton traveled with his wife, the painter Jacqueline Lamba, to Mexico to meet Trotsky (staying as the guest of Diego Rivera's former wife Guadalupe Marin), and there he met Frida Kahlo and saw her paintings for the first time. Breton declared Kahlo to be an "innate" Surrealist painter.

In 1929 the satellite group associated with the journal "Le Grand Jeu", including Roger Gilbert-Lecomte, Maurice Henry and the Czech painter Josef Sima, was ostracized. Also in February, Breton asked Surrealists to assess their "degree of moral competence", and theoretical refinements included in the second "manifeste du surréalisme" excluded anyone reluctant to commit to collective action, a list which included Leiris, Limbour, Morise, Baron, Queneau, Prévert, Desnos, Masson and Boiffard. Excluded members launched a counterattack, sharply criticizing Breton in the pamphlet "Un Cadavre", which featured a picture of Breton wearing a crown of thorns. The pamphlet drew upon an earlier act of subversion by likening Breton to Anatole France, whose unquestioned value Breton had challenged in 1924.n

The disunion of 1929-30 and the effects of "Un Cadavre" had very little negative impact upon Surrealism as Breton saw it, since core figures such as Aragon, Crevel, Dalí and Buñuel remained true the idea of group action, at least for the time being. The success (or the controversy) of Dalí and Buñuel's film L'Age d'Or in December 1930 had a regenerative effect, drawing a number of new recruits, and encouraging countless new artistic works the following year and throughout the 1930s.

Disgruntled surrealists moved to the periodical "Documents", edited by Georges Bataille, whose anti-idealist materialism formed a hybrid Surrealism intending to expose the base instincts of humans. To the dismay of many, "Documents" fizzled out in 1931, just as Surrealism seemed to be gathering more steam.

There were a number of reconciliations after this period of disunion, such as between Breton and Bataille, while Aragon left the group after committing himself to the French Communist Party in 1932. More members were ousted over the years for a variety of infractions, both political and personal, while others left in pursuit of their own style.

By the end of World War II the surrealist group led by André Breton decided to explicitly embrace anarchism. In 1952 Breton wrote "It was in the black mirror of anarchism that surrealism first recognised itself." "Breton was consistent in his support for the francophone Anarchist Federation and he continued to offer his solidarity after the Platformists supporting Fontenis transformed the FA into the Fédération Communiste Libertaire. He was one of the few intellectuals who continued to offer his support to the FCL during the Algerian war when the FCL suffered severe repression and was forced underground. He sheltered Fontenis whilst he was in hiding. He refused to take sides on the splits in the French anarchist movement and both he and Peret expressed solidarity as well with the new Fédération anarchiste set up by the synthesist anarchists and worked in the Antifascist Committees of the 60s alongside the FA."

Throughout the 1930s, Surrealism continued to become more visible to the public at large. A Surrealist group developed in London and, according to Breton, their 1936 London International Surrealist Exhibition was a high-water mark of the period and became the model for international exhibitions. Another English Surrealist group developed in Birmingham, meanwhile, and was distinguished by its opposition to the London surrealists and preferences for surrealism's French heartland. The two groups would reconcile later in the decade.

Dalí and Magritte created the most widely recognized images of the movement. Dalí joined the group in 1929, and participated in the rapid establishment of the visual style between 1930 and 1935.

Surrealism as a visual movement had found a method: to expose psychological truth; stripping ordinary objects of their normal significance, to create a compelling image that was beyond ordinary formal organization, in order to evoke empathy from the viewer.

1931 was a year when several Surrealist painters produced works which marked turning points in their stylistic evolution: Magritte's "Voice of Space (La Voix des airs)" is an example of this process, where three large spheres representing bells hang above a landscape. Another Surrealist landscape from this same year is Yves Tanguy's "", with its molten forms and liquid shapes. Liquid shapes became the trademark of Dalí, particularly in his "The Persistence of Memory", which features the image of watches that sag as if they were melting.

The characteristics of this style—a combination of the depictive, the abstract, and the psychological—came to stand for the alienation which many people felt in the modern period, combined with the sense of reaching more deeply into the psyche, to be "made whole with one's individuality".

Between 1930 and 1933, the Surrealist Group in Paris issued the periodical "Le Surréalisme au service de la révolution" as the successor of "La Révolution surréaliste".

From 1936 through 1938 Wolfgang Paalen, Gordon Onslow Ford, and Roberto Matta joined the group. Paalen contributed Fumage and Onslow Ford Coulage as new pictorial automatic techniques.

Long after personal, political and professional tensions fragmented the Surrealist group, Magritte and Dalí continued to define a visual program in the arts. This program reached beyond painting, to encompass photography as well, as can be seen from a Man Ray self-portrait, whose use of assemblage influenced Robert Rauschenberg's collage boxes.

During the 1930s Peggy Guggenheim, an important American art collector, married Max Ernst and began promoting work by other Surrealists such as Yves Tanguy and the British artist John Tunnard.

Major exhibitions in the 1930s

World War II created havoc not only for the general population of Europe but especially for the European artists and writers that opposed Fascism and Nazism. Many important artists fled to North America and relative safety in the United States. The art community in New York City in particular was already grappling with Surrealist ideas and several artists like Arshile Gorky, Jackson Pollock, and Robert Motherwell converged closely with the surrealist artists themselves, albeit with some suspicion and reservations. Ideas concerning the unconscious and dream imagery were quickly embraced. By the Second World War, the taste of the American avant-garde in New York City swung decisively towards Abstract Expressionism with the support of key taste makers, including Peggy Guggenheim, Leo Steinberg and Clement Greenberg. However, it should not be easily forgotten that Abstract Expressionism itself grew directly out of the meeting of American (particularly New York) artists with European Surrealists self-exiled during World War II. In particular, Gorky and Paalen influenced the development of this American art form, which, as Surrealism did, celebrated the instantaneous human act as the well-spring of creativity. The early work of many Abstract Expressionists reveals a tight bond between the more superficial aspects of both movements, and the emergence (at a later date) of aspects of Dadaistic humor in such artists as Rauschenberg sheds an even starker light upon the connection. Up until the emergence of Pop Art, Surrealism can be seen to have been the single most important influence on the sudden growth in American arts, and even in Pop, some of the humor manifested in Surrealism can be found, often turned to a cultural criticism.

The Second World War overshadowed, for a time, almost all intellectual and artistic production. In 1939 Wolfgang Paalen was the first to leave Paris for the New World as exile. After a long trip through the forests of British Columbia, he settled in Mexico and founded his influential art-magazine Dyn. In 1940 Yves Tanguy married American Surrealist painter Kay Sage. In 1941, Breton went to the United States, where he co-founded the short-lived magazine "VVV" with Max Ernst, Marcel Duchamp, and the American artist David Hare. However, it was the American poet, Charles Henri Ford, and his magazine "View" which offered Breton a channel for promoting Surrealism in the United States. The "View" special issue on Duchamp was crucial for the public understanding of Surrealism in America. It stressed his connections to Surrealist methods, offered interpretations of his work by Breton, as well as Breton's view that Duchamp represented the bridge between early modern movements, such as Futurism and Cubism, to Surrealism. Wolfgang Paalen left the group in 1942 due to political/philosophical differences with Breton.

Though the war proved disruptive for Surrealism, the works continued. Many Surrealist artists continued to explore their vocabularies, including Magritte. Many members of the Surrealist movement continued to correspond and meet. While Dalí may have been excommunicated by Breton, he neither abandoned his themes from the 1930s, including references to the "persistence of time" in a later painting, nor did he become a depictive pompier. His classic period did not represent so sharp a break with the past as some descriptions of his work might portray, and some, such as André Thirion, argued that there were works of his after this period that continued to have some relevance for the movement.

During the 1940s Surrealism's influence was also felt in England, America and the Netherlands where Gertrude Pape and her husband Theo van Baaren helped to popularize it in their publication The Clean Handkerchief. Mark Rothko took an interest in biomorphic figures, and in England Henry Moore, Lucian Freud, Francis Bacon and Paul Nash used or experimented with Surrealist techniques. However, Conroy Maddox, one of the first British Surrealists whose work in this genre dated from 1935, remained within the movement, and organized an exhibition of current Surrealist work in 1978 in response to an earlier show which infuriated him because it did not properly represent Surrealism. Maddox's exhibition, titled "Surrealism Unlimited", was held in Paris and attracted international attention. He held his last one-man show in 2002, and died three years later.
Magritte's work became more realistic in its depiction of actual objects, while maintaining the element of juxtaposition, such as in 1951's "Personal Values (Les Valeurs Personnelles)" and 1954's "Empire of Light (L’Empire des lumières)". Magritte continued to produce works which have entered artistic vocabulary, such as "Castle in the Pyrenees (Le Château des Pyrénées)", which refers back to "Voix" from 1931, in its suspension over a landscape.

Other figures from the Surrealist movement were expelled. Several of these artists, like Roberto Matta (by his own description) "remained close to Surrealism".

After the crushing of the Hungarian Revolution of 1956, Endre Rozsda returned to Paris to continue creating his own word that had been transcended the surrealism. The preface to his first exhibition in the Furstenberg Gallery (1957) was written by Breton yet.

Many new artists explicitly took up the Surrealist banner. Dorothea Tanning and Louise Bourgeois continued to work, for example, with Tanning's "Rainy Day Canape" from 1970. Duchamp continued to produce sculpture in secret including an installation with the realistic depiction of a woman viewable only through a peephole.

Breton continued to write and espouse the importance of liberating the human mind, as with the publication "The Tower of Light" in 1952. Breton's return to France after the War, began a new phase of Surrealist activity in Paris, and his critiques of rationalism and dualism found a new audience. Breton insisted that Surrealism was an ongoing revolt against the reduction of humanity to market relationships, religious gestures and misery and to espouse the importance of liberating the human mind.

Major exhibitions of the 1940s, '50s and '60s

In the 1960s, the artists and writers associated with the Situationist International were closely associated with Surrealism. While Guy Debord was critical of and distanced himself from Surrealism, others, such as Asger Jorn, were explicitly using Surrealist techniques and methods. The events of May 1968 in France included a number of Surrealist ideas, and among the slogans the students spray-painted on the walls of the Sorbonne were familiar Surrealist ones. Joan Miró would commemorate this in a painting titled "May 1968." There were also groups who associated with both currents and were more attached to Surrealism, such as the Revolutionary Surrealist Group.

During the 1980s, behind the Iron Curtain, Surrealism again entered into politics with an underground artistic opposition movement known as the Orange Alternative. The Orange Alternative was created in 1981 by Waldemar Fydrych (alias 'Major'), a graduate of history and art history at the University of Wrocław. They used Surrealist symbolism and terminology in their large scale happenings organized in the major Polish cities during the Jaruzelski regime, and painted Surrealist graffiti on spots covering up anti-regime slogans. Major himself was the author of a "Manifesto of Socialist Surrealism". In this manifesto, he stated that the socialist (communist) system had become so Surrealistic that it could be seen as an expression of art itself.

Surrealistic art also remains popular with museum patrons. The Guggenheim Museum in New York City held an exhibit, "Two Private Eyes", in 1999, and in 2001 Tate Modern held an exhibition of Surrealist art that attracted over 170,000 visitors. In 2002 the Met in New York City held a show, "Desire Unbound", and the Centre Georges Pompidou in Paris a show called "La Révolution surréaliste".

Surrealists groups and literary publications have continued to be active up to the present day, with groups such as the Chicago Surrealist Group, the Leeds Surrealist Group, and the Surrealist Group of Stockholm. Jan Švankmajer of the Czech-Slovak Surrealists continues to make films and experiment with objects.

While Surrealism is typically associated with the arts, it has impacted many other fields. In this sense, Surrealism does not specifically refer only to self-identified "Surrealists", or those sanctioned by Breton, rather, it refers to a range of creative acts of revolt and efforts to liberate imagination. In addition to Surrealist theory being grounded in the ideas of Hegel, Marx and Freud, to its advocates its inherent dynamic is dialectical thought.

Surrealists have also drawn on sources as seemingly diverse as Clark Ashton Smith, Montague Summers, Horace Walpole, Fantômas, The Residents, Bugs Bunny, comic strips, the obscure poet Samuel Greenberg and the hobo writer and humourist T-Bone Slim. One might say that Surrealist strands may be found in movements such as Free Jazz (Don Cherry, Sun Ra, Cecil Taylor etc.) and even in the daily lives of people in confrontation with limiting social conditions. Thought of as the effort of humanity to liberate imagination as an act of insurrection against society, Surrealism finds precedents in the alchemists, possibly Dante, Hieronymus Bosch, Marquis de Sade, Charles Fourier, Comte de Lautreamont and Arthur Rimbaud.

Surrealists believe that non-Western cultures also provide a continued source of inspiration for Surrealist activity because some may induce a better balance between instrumental reason and imagination in flight than Western culture. Surrealism has had an identifiable impact on radical and revolutionary politics, both directly — as in some Surrealists joining or allying themselves with radical political groups, movements and parties — and indirectly — through the way in which Surrealists emphasize the intimate link between freeing imagination and the mind, and liberation from repressive and archaic social structures. This was especially visible in the New Left of the 1960s and 1970s and the French revolt of May 1968, whose slogan "All power to the imagination" quoted by The Situationists and Enragés from the originally Marxist “"Rêvé"-lutionary“ theory and praxis of Breton's French Surrealist group.

Many significant literary movements in the later half of the 20th century were directly or indirectly influenced by Surrealism. This period is known as the Postmodern era; though there's no widely agreed upon central definition of Postmodernism, many themes and techniques commonly identified as Postmodern are nearly identical to Surrealism.

Many writers from and associated with the Beat Generation were influenced greatly by Surrealists. Philip Lamantia and Ted Joans are often categorized as both Beat and Surrealist writers. Many other Beat writers show significant evidence of Surrealist influence. A few examples include Bob Kaufman, Gregory Corso, Allen Ginsberg, and Lawrence Ferlinghetti. Artaud in particular was very influential to many of the Beats, but especially Ginsberg and Carl Solomon. Ginsberg cites Artaud's "Van Gogh -- The Man Suicided by Society" as a direct influence on "Howl", along with Apollinaire's "Zone", García Lorca's "Ode to Walt Whitman", and Schwitters' "Priimiititiii". The structure of Breton's "Free Union" had a significant influence on Ginsberg's "Kaddish". In Paris, Ginsberg and Corso met their heroes Tristan Tzara, Marcel Duchamp, Man Ray, and Benjamin Péret, and to show their admiration Ginsberg kissed Duchamp's feet and Corso cut off Duchamp's tie.

William S. Burroughs, a core member of the Beat Generation and a postmodern novelist, developed the cut-up technique with former surrealist Brion Gysin—in which chance is used to dictate the composition of a text from words cut out of other sources—referring to it as the "Surrealist Lark" and recognizing its debt to the techniques of Tristan Tzara.

Postmodern novelist Thomas Pynchon, who was also influenced by Beat fiction, experimented since the 1960s with the surrealist idea of startling juxtapositions; commenting on the "necessity of managing this procedure with some degree of care and skill", he added that "any old combination of details will not do. Spike Jones Jr., whose father's orchestral recordings had a deep and indelible effect on me as a child, said once in an interview, 'One of the things that people don't realize about Dad's kind of music is, when you replace a C-sharp with a gunshot, it has to be a C-sharp gunshot or it sounds awful.'"

Many other postmodern fiction writers have been directly influenced by Surrealism. Paul Auster, for example, has translated Surrealist poetry and said the Surrealists were "a real discovery" for him. Salman Rushdie, when called a Magical Realist, said he saw his work instead "allied to surrealism". David Lynch regarded as a surrealist filmmaker being quoted, "David Lynch has once again risen to the spotlight as a champion of surrealism," in regard to his show Twin Peaks. For the work of other postmodernists, such as Donald Barthelme and Robert Coover, a broad comparison to Surrealism is common.

Magic realism, a popular technique among novelists of the latter half of the 20th century especially among Latin American writers, has some obvious similarities to Surrealism with its juxtaposition of the normal and the dream-like, as in the work of Gabriel García Márquez. Carlos Fuentes was inspired by the revolutionary voice in Surrealist poetry and points to inspiration Breton and Artaud found in Fuentes' homeland, Mexico. Though Surrealism was a direct influence on Magic Realism in its early stages, many Magic Realist writers and critics, such as Amaryll Chanady and S. P. Ganguly, while acknowledging the similarities, cite the many differences obscured by the direct comparison of Magic Realism and Surrealism such as an interest in psychology and the artefacts of European culture they claim is not present in Magic Realism. A prominent example of a Magic Realist writer who points to Surrealism as an early influence is Alejo Carpentier who also later criticized Surrealism's delineation between real and unreal as not representing the true South American experience.

Surrealist individuals and groups have carried on with Surrealism after the death of André Breton in 1966. The original Paris Surrealist Group was disbanded by member Jean Schuster in 1969, but another Parisian surrealist group was later formed. The current Surrealist Group of Paris has recently published the first issue of their new journal, "Alcheringa". The Group of Czech-Slovak Surrealists never disbanded, and continue to publish their journal "Analogon", which now spans 80 volumes.

Surrealist theatre and Artaud's "Theatre of Cruelty" were inspirational to many within the group of playwrights that the critic Martin Esslin called the "Theatre of the Absurd" (in his 1963 book of the same name). Though not an organized movement, Esslin grouped these playwrights together based on some similarities of theme and technique; Esslin argues that these similarities may be traced to an influence from the Surrealists. Eugène Ionesco in particular was fond of Surrealism, claiming at one point that Breton was one of the most important thinkers in history. Samuel Beckett was also fond of Surrealists, even translating much of the poetry into English. Other notable playwrights whom Esslin groups under the term, for example Arthur Adamov and Fernando Arrabal, were at some point members of the Surrealist group.

Alice Farley is an American-born artist who became active during the 1970s in San Francisco after training in dance at the California Institute of the Arts. Farley uses vivid and elaborate costuming that she describes as "the vehicles of transformation capable of making a character's thoughts visible". Often collaborating with musicians such as Henry Threadgill, Farley explores the role of improvisation in dance, bringing in an automatic aspect to the productions. Farley has performed in a number of surrealist collaborations including the World Surrealist Exhibition in Chicago in 1976.

Various much older artists are sometimes claimed as precursors of Surrealism. Foremost among these are Hieronymus Bosch, and Giuseppe Arcimboldo, who Dalí called the "father of Surrealism." Apart from their followers, other artists who may be mentioned in this context include Joos de Momper, for some anthropomorphic landscapes. Many critics feel these works belong to fantastic art rather than having a significant connection with Surrealism.


André Breton

Other sources






</doc>
<doc id="28767" url="https://en.wikipedia.org/wiki?curid=28767" title="Statics">
Statics

Statics is the branch of mechanics that is concerned with the analysis of loads (force and torque, or "moment") acting on physical systems that do not experience an acceleration ("a"=0), but rather, are in static equilibrium with their environment. The application of Newton's second law to a system gives:

Where bold font indicates a vector that has magnitude and direction. formula_2 is the total of the forces acting on the system, formula_3 is the mass of the system and formula_4 is the acceleration of the system. The summation of forces will give the direction and the magnitude of the acceleration and will be inversely proportional to the mass. The assumption of static equilibrium of formula_4 = 0 leads to:

The summation of forces, one of which might be unknown, allows that unknown to be found. So when in static equilibrium, the acceleration of the system is zero and the system is either at rest, or its center of mass moves at constant velocity. Likewise the application of the assumption of zero acceleration to the summation of moments acting on the system leads to:

Here, formula_8 is the summation of all moments acting on the system, formula_9 is the moment of inertia of the mass and formula_10 = 0 the angular acceleration of the system, which when assumed to be zero leads to:

The summation of moments, one of which might be unknown, allows that unknown to be found.
These two equations together, can be applied to solve for as many as two loads (forces and moments) acting on the system.

From Newton's first law, this implies that the net force and net torque on every part of the system is zero. The net forces equaling zero is known as the "first condition for equilibrium," and the net torque equaling zero is known as the "second condition for equilibrium." See statically indeterminate.

Archimedes (c. 287–c. 212 BC) did pioneering work in statics.
Later developments in the field of statics are found in works of Thebit.

A scalar is a quantity which only has a magnitude, such as mass or temperature. A vector has a magnitude and a direction. There are several notations to identify a vector, including:

Vectors are added using the parallelogram law or the triangle law. Vectors contain components in orthogonal bases. Unit vectors i, j, and k are, by convention, along the x, y, and z axes, respectively.

Force is the action of one body on another. A force is either a push or a pull, and it tends to move a body in the direction of its action. The action of a force is characterized by its magnitude, by the direction of its action, and by its point of application. Thus, force is a vector quantity, because its effect depends on the direction as well as on the magnitude of the action.

Forces are classified as either contact or body forces. A contact force is produced by direct physical contact; an example is the force exerted on a body by a supporting surface. A body force is generated by virtue of the position of a body within a force field such as a gravitational, electric, or magnetic field and is independent of contact with any other body. An example of a body force is the weight of a body in the Earth's gravitational field.

In addition to the tendency to move a body in the direction of its application, a force can also tend to rotate a body about an axis. The axis may be any line which neither intersects nor is parallel to the line of action of the force. This rotational tendency is known as the "moment" (M) of the force. Moment is also referred to as "torque".

The magnitude of the moment of a force at a point "O", is equal to the perpendicular distance from "O" to the line of action of "F", multiplied by the magnitude of the force: , where

The direction of the moment is given by the right hand rule, where counter clockwise (CCW) is out of the page, and clockwise (CW) is into the page. The moment direction may be accounted for by using a stated sign convention, such as a plus sign (+) for counterclockwise moments and a minus sign (−) for clockwise moments, or vice versa. Moments can be added together as vectors.

In vector format, the moment can be defined as the cross product between the radius vector, r (the vector from point O to the line of action), and the force vector, F:

Varignon's theorem states that the moment of a force about any point is equal to the sum of the moments of the components of the force about the same point.

The static equilibrium of a particle is an important concept in statics. A particle is in equilibrium only if the resultant of all forces acting on the particle is equal to zero. In a rectangular coordinate system the equilibrium equations can be represented by three scalar equations, where the sums of forces in all three directions are equal to zero. An engineering application of this concept is determining the tensions of up to three cables under load, for example the forces exerted on each cable of a hoist lifting an object or of guy wires restraining a hot air balloon to the ground.

In classical mechanics, moment of inertia, also called mass moment, rotational inertia, polar moment of inertia of mass, or the angular mass, (SI units kg·m²) is a measure of an object's resistance to changes to its rotation. It is the inertia of a rotating body with respect to its rotation. The moment of inertia plays much the same role in rotational dynamics as mass does in linear dynamics, describing the relationship between angular momentum and angular velocity, torque and angular acceleration, and several other quantities. The symbols I and J are usually used to refer to the moment of inertia or polar moment of inertia.

While a simple scalar treatment of the moment of inertia suffices for many situations, a more advanced tensor treatment allows the analysis of such complicated systems as spinning tops and gyroscopic motion.

The concept was introduced by Leonhard Euler in his 1765 book "Theoria motus corporum solidorum seu rigidorum"; he discussed the moment of inertia and many related concepts, such as the principal axis of inertia.

Statics is used in the analysis of structures, for instance in architectural and structural engineering. Strength of materials is a related field of mechanics that relies heavily on the application of static equilibrium. A key concept is the center of gravity of a body at rest: it represents an imaginary point at which all the mass of a body resides. The position of the point relative to the foundations on which a body lies determines its stability in response to external forces. If the center of gravity exists outside the foundations, then the body is unstable because there is a torque acting: any small disturbance will cause the body to fall or topple. If the center of gravity exists within the foundations, the body is stable since no net torque acts on the body. If the center of gravity coincides with the foundations, then the body is said to be metastable.

Hydrostatics, also known as fluid statics, is the study of fluids at rest (i.e. in static equilibrium). The characteristic of any fluid at rest is that the force exerted on any particle of the fluid is the same at all points at the same depth (or altitude) within the fluid. If the net force is greater than zero the fluid will move in the direction of the resulting force. This concept was first formulated in a slightly extended form by French mathematician and philosopher Blaise Pascal in 1647 and became known as Pascal's Law. It has many important applications in hydraulics. Archimedes, Abū Rayhān al-Bīrūnī, Al-Khazini and Galileo Galilei were also major figures in the development of hydrostatics.





</doc>
<doc id="28768" url="https://en.wikipedia.org/wiki?curid=28768" title="Southern Cross (disambiguation)">
Southern Cross (disambiguation)

The Southern Cross or Crux is a star group visible mainly in the Southern Hemisphere. It has been known by this English term since the late 18th century.

Southern Cross may also refer to:













</doc>
<doc id="28769" url="https://en.wikipedia.org/wiki?curid=28769" title="Maritime transport">
Maritime transport

Maritime transport (or ocean transport) and fluvial transport, or more generally waterborne transport, is the transport of people (passengers) or goods (cargo) via waterways. Freight transport by sea has been widely used throughout recorded history. The advent of aviation has diminished the importance of sea travel for passengers, though it is still popular for short trips and pleasure cruises. Transport by water is cheaper than transport by air, despite fluctuating exchange rates and a fee placed on top of freighting charges for carrier companies known as the currency adjustment factor (CAF).

Maritime transport can be realized over any distance by boat, ship, sailboat or barge, over oceans and lakes, through canals or along rivers. Shipping may be for commerce, recreation, or for military purposes. While extensive inland shipping is less critical today, the major waterways of the world including many canals are still very important and are integral parts of worldwide economies. Virtually any material can be moved by water; however, water transport becomes impractical when material delivery is time-critical such as various types of perishable produce. Still, water transport is highly cost effective with regular schedulable cargoes, such as trans-oceanic shipping of consumer products – and especially for heavy loads or bulk cargos, such as coal, coke, ores, or grains. Arguably, the industrial revolution took place best where cheap water transport by canal, navigations, or shipping by all types of watercraft on natural waterways supported cost effective bulk transport.

Containerization revolutionized maritime transport starting in the 1970s. "General cargo" includes goods packaged in boxes, cases, pallets, and barrels. When a cargo is carried in more than one mode, it is intermodal or co-modal.

"Merchant shipping:" A nation's shipping fleet (merchant navy, merchant marine, merchant fleet) consists of the ships operated by civilian crews to transport passengers or cargo from one place to another. Merchant shipping also includes water transport over the river and canal systems connecting inland destinations, large and small. For example, during the early modern era, cities in the Hanseatic League began taming Northern Europe's rivers and harbors. And, for instance, the Saint Lawrence Seaway connects the port cities on the Great Lakes in Canada and the United States with the Atlantic Ocean shipping routes; while the various Illinois Canals connect the Great Lakes and Canada with New Orleans. Ores, Coal, and grains can travel along the rivers of the American midwest to Pittsburgh, or Birmingham. Professional mariners are merchant seaman, merchant sailor, and merchant mariner, or simply seaman, sailor, or mariners. The terms "seaman" or "sailor" may refer to a member of a country's navy.

According to the 2005 CIA World Factbook, the total number of merchant ships of at least 1,000 gross register tons in the world was 30,936. In 2010, it was 38,988, an increase of 26%. , a quarter of all merchant mariners were born in the Philippines. Statistics for individual countries are available at the list of merchant navy capacity by country.

A ship may also be categorized as to how it is operated.

Ships and other watercraft are used for maritime transport. Types can be distinguished by propulsion, size or cargo type. Recreational or educational craft still use wind power, while some smaller craft use internal combustion engines to drive one or more propellers, or in the case of jet boats, an inboard water jet. In shallow draft areas, such as the Everglades, some craft, such as the hovercraft, are propelled by large pusher-prop fans.

Most modern merchant ships can be placed in one of a few categories, such as:
A cargo ship sailing from a European port to a US one will typically take 10–12 days depending on water currents and other factors. In order to make container ship transport more economical in the face of declining demand for intercontinental shipping, ship operators sometimes reduce cruising speed, thereby increasing transit time, to reduce fuel consumption, a strategy referred to as "slow steaming".

A ship's complement can be divided into four categories: 

 Officer positions in the deck department include but not limited to: Master and his Chief, Second, and Third officers. The official classifications for unlicensed members of the deck department are Able Seaman and Ordinary Seaman.

A common deck crew for a ship includes:

A deck cadet is a person who is carrying out mandatory sea time to achieve their officer of the watch certificate. Their time on board is spent learning the operations and tasks of everyday life on a merchant vessel.

A ship's engine department consists of the members of a ship's crew that operate and maintain the propulsion and other systems on board the vessel. Engine staff also deal with the "Hotel" facilities on board, notably the sewage, lighting, air conditioning and water systems. They deal with bulk fuel transfers, and require training in firefighting and first aid, as well as in dealing with the ship's boats and other nautical tasks- especially with cargo loading/discharging gear and safety systems, though the specific cargo discharge function remains the responsibility of deck officers and deck workers. On LPG and LNG tankers however, a cargo engineer works with the deck department during cargo operations, as well as being a watchkeeping engineer.

A common engine crew for a ship includes:

Many American ships also carry a motorman. Other possible positions include machinist, electrician, refrigeration engineer, and tankerman. Engine cadets are engineer trainees who are completing sea time necessary before they can obtain a watchkeeping license.

A typical Steward's department for a cargo ship would be composed of a Chief Steward, a Chief Cook, and a Steward's Assistant. All three positions are typically filled by unlicensed personnel. The chief steward directs, instructs, and assigns personnel performing such functions as preparing and serving meals; cleaning and maintaining officers' quarters and steward department areas; and receiving, issuing, and inventorying stores. On large passenger vessels, the Catering Department is headed by the Chief Purser and managed by Assistant Pursers. Although they enjoy the benefits of having officer rank, they generally progress through the ranks to become pursers. Under the Pursers are the department heads – such as chief cook, head waiter, head barman etc. They are responsible for the administration of their own areas.

The chief steward also plans menus; compiles supply, overtime, and cost control records. They may requisition or purchase stores and equipment. They may bake bread, rolls, cakes, pies, and pastries. A chief steward's duties may overlap with those of the Steward's Assistant, the Chief Cook, and other Steward's Department crewmembers.

In the United States Merchant Marine, a chief steward must have a Merchant Mariner's Document issued by the United States Coast Guard. Because of international law, conventions, and agreements, all chief cooks who sail internationally are similarly documented by their respective countries.

Staff officer positions on a ship, including Junior Assistant Purser, Senior Assistant Purser, Purser, Chief Purser, Medical Doctor, Professional Nurse, Marine Physician Assistant, and Hospital Corpsman, are considered administrative positions and are therefore regulated by Certificates of Registry issued by the United States Coast Guard. Pilots are also merchant marine officers and are licensed by the Coast Guard. Formerly, there was also a radio department, headed by a chief radio officer and supported by a number of radio officers. Since the introduction of GMDSS (Satellite communications) and the subsequent exemptions from carrying radio officers if the vessel is so equipped, this department has fallen away, although many ships do still carry specialist radio officers, particularly passenger vessels. Many radio officers became 'electro-technical officers', and transferred into the engine department.

Mariners spend much of their life beyond the reach of land. They sometimes face dangerous conditions at sea or on Lakes – the fishing port of Gloucester, Massachusetts has a seaside memorial listing over 10,000 fishermen that lost their lives to the sea, and the Great Lakes have seen over 10,000 lost vessels since the 1800s, yet men and women still go to sea. For some, the attraction is a life unencumbered with the restraints of life ashore. Seagoing adventure and a chance to see the world also appeal to many seafarers. Whatever the calling, those who live and work at sea invariably confront social isolation.

Findings by the Seafarer's International Research Center indicate a leading cause of mariners leaving the industry is "almost invariably because they want to be with their families." U.S. merchant ships typically do not allow family members to accompany seafarers on voyages. Industry experts increasingly recognize isolation, stress, and fatigue as occupational hazards. Advocacy groups such as International Labour Organization, a United Nations agency, and the Nautical Institute are seeking improved international standards for mariners. Satellite phones have improved communication and efficiency aboard sea-faring ships. This technology has contributed to crew welfare, although both equipment and fees are expensive.

Ocean voyages are steeped in routine. Maritime tradition dictates that each day be divided into six four-hour periods. Three groups of watch keepers from the engine and deck departments work four hours on then have eight hours off watch keeping. However, there are many overtime jobs to be done daily. This cycle repeats endlessly, 24 hours a day while the ship is at sea. Members of the steward department typically are day workers who put in at least eight-hour shifts. Operations at sea, including repairs, safeguarding against piracy, securing cargo, underway replenishment, and other duties provide opportunities for overtime work. Service aboard ships typically extends for months at a time, followed by protracted shore leave. However, some seamen secure jobs on ships they like and stay aboard for years.

The quick turnaround of many modern ships, spending only a few hours in port, limits a seafarer's free-time ashore. Moreover, some foreign seamen entering U.S. ports from a watch list of 25 countries face restrictions on shore leave due to maritime security concerns. However, shore leave restrictions while in U.S. ports impact American seamen as well. For example, the International Organization of Masters, Mates & Pilots notes a trend of U.S. shipping terminal operators restricting seamen from traveling from the ship to the terminal gate. Furthermore, in cases where transit is allowed, special "security fees" are at times assessed.

Such restrictions on shore leave, coupled with reduced time in port, translate into longer periods at sea. Mariners report that extended periods at sea living and working with shipmates, who for the most part are strangers, takes getting used to. At the same time, there is an opportunity to meet people from other ethnic and cultural backgrounds. Recreational opportunities have improved aboard some U.S. ships, which may feature gyms and day rooms for watching movies, swapping sea stories, and other activities. And in some cases, especially tankers, it is possible for a mariner to be accompanied by members of his family. However, a mariner’s off-duty time is largely a solitary affair, pursuing hobbies, reading, writing letters, and sleeping.

On modern ocean-going vessels, typically registered with a flag of convenience, life has changed immensely in the last 20 years. Most large vessels include a gym and often a swimming pool for use by the crew. Since the "Exxon Valdez" incident, the focus of leisure time activity has shifted from having officer and crew bars, to simply having lounge-style areas where officers or crew can sit to watch movies. With many companies now providing TVs and DVD players in cabins, and enforcing strict smoking policies, it is not surprising that the bar is now a much quieter place on most ships. In some instances games consoles are provided for the officers and crew. The officers enjoy a much higher standard of living on board ocean-going vessels.

Crews are generally poorly paid, poorly qualified and have to complete contracts of approximately 9 months before returning home on leave. They often come from countries where the average industrial wage is still very low, such as the Philippines or India. Officers however, come from all over the world and it is not uncommon to mix the nationality of the officers on board ships. Officers are often the recipients of university degrees and have completed vast amounts of training in order to reach their rank. Officers benefit e.g. by having larger, more comfortable cabins and table service for their meals.

Contracts average at the 4 month mark for officers, with generous leave. Most ocean-going vessels now operate an unmanned engine room system allowing engineers to work days only. The engine room is computer controlled by night, although the duty engineer will make inspections during unmanned operation. Engineers work in a hot, humid, noisy atmosphere. Communication in the engine room is therefore by hand signals and lip-reading, and good teamwork often stands in place of any communication at all.

The environmental impact of shipping includes greenhouse gas emissions, acoustic, and oil pollution. The International Maritime Organization (IMO) estimates that Carbon dioxide emissions from shipping were equal to 2.2% of the global human-made emissions in 2012 and expects them to rise 50 to 250 percent by 2050 if no action is taken.

For a port to efficiently send and receive cargo, it requires infrastructure: docks, bollards, pilings, cranes, bulk cargo handling equipment, and so on – equipment and organization supporting the role of the facilities. From pier to pier these may differ, one dock handling intermodal transport needs (container-ships linked to rail by cranes); another bulk handling capabilities (such as conveyors, elevators, tanks, pumps) for loading and unloading bulk cargoes like grain, coal, or fuels. Others may be outfitted as passenger terminals or for mixed mode operations.

Generally, Harbors, seaports and marinas all host watercraft, and consist of components such as piers, wharfs, docks and roadsteads.
<br>






</doc>
<doc id="28771" url="https://en.wikipedia.org/wiki?curid=28771" title="St. John's, Antigua and Barbuda">
St. John's, Antigua and Barbuda

St. John's is the capital and largest city of Antigua and Barbuda, part of the West Indies in the Caribbean Sea. With a population of 22,219, St. John's is the commercial centre of the nation and the chief port of the island of Antigua.

The settlement of St. John's has been the administrative centre of Antigua and Barbuda since the islands were first colonised in 1632, and it became the seat of government when the nation achieved independence in 1981.

St. John's is one of the most developed and cosmopolitan municipalities in the Lesser Antilles. The city is famous for its shopping malls as well as boutiques throughout the city, selling designer jewellery and haute-couture clothing.

St. John's attracts tourists from the resorts on the island and from the cruise ships which dock in its harbour at Heritage Quay and Redcliffe Quay several times a week.

The investment banking industry has a strong presence in the city. Major world financial institutions have offices in St. John's.

There is a market on the southwestern edge of the city where fresh produce, meats, and fresh fish are sold daily.

The Antigua Rum Distillery is located at the Citadel and is the only rum distillery on the island. Annual production yields more than 180,000 gallons bottled.

The majority of the population of St. John's reflects that of the rest of Antigua: people of African and mixed European-African ancestry, with a European minority, including British and Portuguese. There is a population of Levantine Christian Arabs.
The Eastern Caribbean Civil Aviation Authority has its headquarters on Factory Road in St. John's.

St John's is twinned with Waltham Forest borough in London, England.

There are several museums, including the Museum of Antigua and Barbuda and the Museum of Marine Art, a small facility containing fossilised bedrock, volcanic stones, petrified wood, a collection of more than 10,000 shells, and artefacts from English shipwrecks.
Just east of St. John's is the Sir Vivian Richards Stadium, a multi-use stadium in North Sound, that was created mostly for cricket matches, and has hosted the matches during the 2007 Cricket World Cup. The Antigua Recreation Ground, Antigua and Barbuda's national stadium, is located in St. John's.

Nearby villages and settlements include St. Johnston.

The city's skyline is dominated by the white baroque towers of St. John's Cathedral.

The Botanical Garden is near the intersection of Factory Road and Independence Avenue. This small park's shaded benches and gazebo provide a quiet refuge from the bustle of activity of St. John's.
Sandy Island is a Lighthouse located on a small island about 5 km off the coast leading the way to St. John's harbour.

Fort James stands at the entrance to St. John's harbour. Other nearby forts include Fort George, Fort Charles, Fort Shirley, Fort Berkeley and Fort Barrington.

Government House is the Governor's residence, originally a 19th-century parsonage building. It is included on the World Monuments Fund's 2018 list of monuments at risk, following exposure to severe weather events.

St. John's is served by the V. C. Bird International Airport.

St. John's is home to two medical schools – the American University of Antigua and University of Health Sciences Antigua. Secondary schools include Christ the King High School, Princess Margaret School and the Antigua Girls High School. Private grade schools include St. John's Lutheran School of the WELS.



</doc>
<doc id="28779" url="https://en.wikipedia.org/wiki?curid=28779" title="Sigtuna Municipality">
Sigtuna Municipality

Sigtuna Municipality ("Sigtuna kommun") is a municipality in Stockholm County in east central Sweden. Its seat is located in the town of Märsta, approximately north of the Swedish capital, Stockholm.

The municipality is a part of Metropolitan Stockholm.

The municipality consists of several former local government units and was formed in 1971. It got its name from the small, but very old, "City of Sigtuna", but the seat was placed in the larger modern town of Märsta.

The three towns of the municipality are Märsta (pop. 23,000), Sigtuna (pop. 8,000) and Rosersberg (pop. 1,400), of which Märsta is the municipal seat and Sigtuna with its old and important history is a popular tourist destination.

On 31 December 2017 the number of people with a (persons born outside of Sweden or with two parents born outside of Sweden) was 20 291, or 43.04% of the population (47 146 on 31 December 2017). On 31 December 2002 the number of residents with a foreign background was (per the same definition) 9 426, or 26.35% of the population (35 771 on 31 December 2002). On 31 December 2017 there were 47 146 residents in Sigtuna, of which 15 268 people (32.38%) were born in a country other than Sweden. Divided by country in the table below - the Nordic countries as well as the 12 most common countries of birth outside of Sweden for Swedish residents have been included, with other countries of birth bundled together by continent by Statistics Sweden.

In the municipality lies the largest workplace in Sweden, the Arlanda Airport, with 13,000 employees in 200 companies. As a result, Siguna is travelled through by 18,300,000 visitors yearly, and has the fourth most hotel stays, following to the commercial and regional centres Stockholm, Gothenburg and Malmö.

Swedavia, the Swedish airport management company, has its head office on the airport property. Scandinavian Airlines previously had its head office on the airport property.

The municipality is twinned with:



</doc>
<doc id="28782" url="https://en.wikipedia.org/wiki?curid=28782" title="Self-similarity">
Self-similarity

In mathematics, a self-similar object is exactly or approximately similar to a part of itself (i.e., the whole has the same shape as one or more of the parts). Many objects in the real world, such as coastlines, are statistically self-similar: parts of them show the same statistical properties at many scales. Self-similarity is a typical property of fractals. Scale invariance is an exact form of self-similarity where at any magnification there is a smaller piece of the object that is similar to the whole. For instance, a side of the Koch snowflake is both symmetrical and scale-invariant; it can be continually magnified 3x without changing shape. The non-trivial similarity evident in fractals is distinguished by their fine structure, or detail on arbitrarily small scales. As a counterexample, whereas any portion of a straight line may resemble the whole, further detail is not revealed.

A time developing phenomenon is said to exhibit self-similarity if the numerical value of certain observable quantity 
formula_1 measured at different times are different but the corresponding dimensionless quantity at given value of formula_2 remain invariant. It happens if the quantity formula_1 exhibits dynamic scaling. The idea is just an extension of the idea of similarity of two triangles. Note that two triangles are similar if the numerical values of their sides are different however the corresponding dimensionless quantities, such as their angles, coincide. 

Peitgen "et al." explain the concept as such: 

Since mathematically, a fractal may show self-similarity under indefinite magnification, it is impossible to recreate this physically. Peitgen "et al." suggest studying self-similarity using approximations:

In mathematics, self-affinity is a feature of a fractal whose pieces are scaled by different amounts in the x- and y-directions. This means that to appreciate the self similarity of these fractal objects, they have to be rescaled using an anisotropic affine transformation.

A compact topological space "X" is self-similar if there exists a finite set "S" indexing a set of non-surjective homeomorphisms formula_4 for which

If formula_6, we call "X" self-similar if it is the only non-empty subset of "Y" such that the equation above holds for formula_7. We call

a "self-similar structure". The homeomorphisms may be iterated, resulting in an iterated function system. The composition of functions creates the algebraic structure of a monoid. When the set "S" has only two elements, the monoid is known as the dyadic monoid. The dyadic monoid can be visualized as an infinite binary tree; more generally, if the set "S" has "p" elements, then the monoid may be represented as a p-adic tree.

The automorphisms of the dyadic monoid is the modular group; the automorphisms can be pictured as hyperbolic rotations of the binary tree.

A more general notion than self-similarity is Self-affinity.

The Mandelbrot set is also self-similar around Misiurewicz points.

Self-similarity has important consequences for the design of computer networks, as typical network traffic has self-similar properties. For example, in teletraffic engineering, packet switched data traffic patterns seem to be statistically self-similar. This property means that simple models using a Poisson distribution are inaccurate, and networks designed without taking self-similarity into account are likely to function in unexpected ways.

Similarly, stock market movements are described as displaying self-affinity, i.e. they appear self-similar when transformed via an appropriate affine transformation for the level of detail being shown. Andrew Lo describes stock market log return self-similarity in econometrics.

Finite subdivision rules are a powerful technique for building self-similar sets, including the Cantor set and the Sierpinski triangle.

The Viable System Model of Stafford Beer is an organizational model with an affine self-similar hierarchy, where a given viable system is one element of the System One of a viable system one recursive level higher up, and for whom the elements of its System One are viable systems one recursive level lower down.

Self-similarity can be found in nature, as well. To the right is a mathematically generated, perfectly self-similar image of a fern, which bears a marked resemblance to natural ferns. Other plants, such as Romanesco broccoli, exhibit strong self-similarity.





</doc>
<doc id="28785" url="https://en.wikipedia.org/wiki?curid=28785" title="Small beer">
Small beer

Small beer (also known as small ale or table beer) is a lager or ale that contains a lower amount of alcohol by volume than most others, usually between 0.5% to 2.8%. Sometimes unfiltered and porridge-like, it was a favoured drink in Medieval Europe and colonial North America compared with more expensive beer containing higher alcohol. Small beer was also produced in households for consumption by children and by servants.

Before the 19th century, potable water had the potential to cause sickness because of poorer sanitation. Practical experience demonstrated that fermented beverages were less likely to bring on human illness. At mealtimes in the Middle Ages, all drank small beer, regardless of age, particularly while eating a meal at the table. Table beer was around this time typically less than 1% ABV.

It was common for workers such as sailors who engaged in laborious tasks to drink more than ten imperial pints (5.7 litres) of small beer a day to quench their thirst. Small beer was also consumed for its nutrition content. It might contain traces of wheat or bread suspended within it. In his "A Plan for the Conduct of Female Education, in Boarding Schools" published 1797, writer Erasmus Darwin agreed that "For the drink of the more robust children water is preferable, and for the weaker ones, small beer ...". Larger educational establishments like Eton, Winchester, and Oxford University even ran their own breweries. 

In 17th century England, it was an excise class which was determined by its wholesale price. Between the years 1782 and 1802, table beer was said to define that which cost between six and eleven shillings per barrel and the tax on this class was around three shillings. Cheaper beer was considered "small beer" while the more expensive brands were classed as strong (big) beer. The differences between small beer and table beer were removed in 1802 because there was much fraudulent mixing of the types. Small beer was socially acceptable in 18th-century England because of its lower alcohol content, allowing people to drink several glasses without becoming drunk. William Hogarth's portrait "Beer Street" (1751) shows a group of happy workers going about their business after drinking table beer. It became increasingly popular during the 19th century, displacing malt liquor as the drink of choice for families and servants.

Small beer and small ale can also refer to beers made from the second runnings from the stronger beer (e.g., scotch ale). Such beers can be as strong as a mild ale, but it depends on the strength of the original mash. This was an economic measure in household brewing in England until the 18th century, and still produced by some homebrewers.

In Belgium, small or table beer is known as "tafelbier" and their many varieties are still brewed. Breweries that perpetuated in this type included De Es of Schalkhoven and Gigi of Gérouville in Luxembourg. In the US, a Vienna lager was a popular table beer before prohibition. Small beers are also produced in Germany and Switzerland albeit using local brewing methods. In Finland, new alternatives continue to be sought for the beverage.

In Sweden beer with an alcohol content of 2.25 per cent by volume, or less, sold as "lättöl" ("light beer"), is legally classified as a soft drink ("lättdryck"), exempt from alcohol tax and age restrictions, made by virtually all breweries, sold in all grocery stores and commonly served even in company lunch canteens.

One beverage exists in the United Kingdom which meets the criteria of "small beer" by combining traditional Czech Saaz hop with British barley to produce a drink with ABV 2.1%.

Metaphorically, "small beer" means a trifle, or a thing of little importance.

In The Forsyte Saga by John Galsworthy the character Old Jolyon Forsyte describes his ancestors as small bere which meant not as successful or important as the current generation.




</doc>
<doc id="28787" url="https://en.wikipedia.org/wiki?curid=28787" title="Sambia Peninsula">
Sambia Peninsula

Sambia (, "Sambiysky poluostrov", literally the Sambiysky Peninsula;) or Samland (, "Zemlandsky poluostrov", literally the Zemlandsky Peninsula) or Kaliningrad Peninsula (official name, , "Kaliningradsky poluostrov") is a peninsula in the Kaliningrad Oblast of Russia, on the southeastern shore of the Baltic Sea. The peninsula is bounded by the Curonian Lagoon (to the north-east), the Vistula Lagoon (on the southwest), the Pregel River (on the south), and the Deyma River (on the east). As Samland is surrounded on all sides by water, it is technically an island. Prior to 1945 it formed an important part of East Prussia. 

Sambia is named after the Sambians, an extinct tribe of Old Prussians. "Samland" is the name for peninsula in the Germanic languages. Polish and Latin speakers call the area "Sambia", while the Lithuanian name is "Semba".

Sambia was originally sparsely populated by the Sambians. The German Teutonic Knights conquered the region during the 13th century and the Bishopric of Samland became, along with Bishopric of Pomesania, Bishopric of Ermland, and Bishopric of Culm, one of the four dioceses of Prussia in 1243. Settlers from the Holy Roman Empire began colonizing the region, and the Sambian Prussians gradually became assimilated. The peninsula was the last area in which the Old Prussian language was spoken before becoming extinct at the beginning of the 18th century.

The peninsula became part of the Duchy of Prussia when Albert of Brandenburg-Ansbach, the 37th Grand Master, was forced to secularize the Monastic State of the Teutonic Knights in 1525 and became the vasal duchy of the Kingdom of Poland. The Margraviate of Brandenburg inherited the duchy in 1618. 

Because the Duchy of Prussia failed to fulfill its feudal obligations as a vassal of Poland during the Polish–Swedish wars, George William's rule in Prussia was suspended in 1635 and he was replaced by the Polish king by a viceroy, Jerzy Ossoliński. However, under the Treaty of Sztumska Wieś the Duchy (and so the Sambia peninsula) was given back to Georg William. In 1701 Hohenzollern ruler proclaimed the Kingdom of Prussia and Sambia became part of the Province of East Prussia in 1773. (The Kingdom of Prussia completed the unification of Germany by setting up the German Empire in 1871.) After World War I Sambia formed part of the East Prussian province of Weimar Germany.

In 1945 after World War II, the Soviet Union annexed northern East Prussia, including Sambia (German: Samland), while southern East Prussia was given to Poland. Sambia became part of the Soviet Kaliningrad Oblast, named after the nearby city of Kaliningrad (historically ), and the new authorities expelled its German inhabitants.

The Soviet Union gradually repopulated the Kaliningrad Oblast, including Sambia, with Russians and Belarusians. Until the dissolution of the Soviet Union in 1991, much of the district was a closed military area.

While today the Kursenieki, also known as Kuršininkai are a nearly extinct Baltic ethnic group living along the Curonian Spit, in 1649 Kuršininkai settlement spanned from Memel (Klaipėda) to Danzig (Gdańsk), including the coastline of the Sambian Peninsula. The Kuršininkai were eventually assimilated by the Germans, except along the Curonian Spit where some still live. The Kuršininkai were considered Latvians until after World War I when Latvia gained independence from the Russian Empire, a consideration based on linguistic arguments. This was the rationale for Latvian claims over the Curonian Spit, Memel, and other territories of East Prussia which would be later dropped.

Baedeker describes Samland as "a fertile and wooded district, with several lakes, lying to the north of Königsberg" (since 1946 Kaliningrad). The highest point, 360 feet, is found twelve miles north of Pereslavskoe ("Drugehnen") at the ski resort then called the Galtgarben. There also used to be a Samland railway station. the Pereslavskoe railway station serves the "Blue Arrow" railway line from Kaliningrad to Svetlogorsk.

Sambia includes two famous seaside resorts, Zelenogradsk (former German name: Cranz) and Svetlogorsk (former German name: Rauschen).

Amber has been found in the area for over two thousand years, especially on the coast near Kaliningrad. History and legends tell of the ancient trade routes known as the Amber Road leading from the Old Prussian settlements of Kaup (in Sambia) and Truso (near Elbląg - German: Elbing, near the mouth of the Vistula) southwards to the Black and Adriatic seas. In Imperial Germany, the right to collect amber was restricted to the Hohenzollern dynasty, and visitors to Samland's beaches were forbidden to pick up any fragments they found. Beginning in the 19th century, amber was mined on an industrial scale by the Germans before 1945 and by the Soviets / Russians thereafter at Yantarny (former German name: Palmnicken).



</doc>
<doc id="28791" url="https://en.wikipedia.org/wiki?curid=28791" title="Sovereignty">
Sovereignty

Sovereignty is the full right and power of a governing body over itself, without any interference from outside sources or bodies. In political theory, sovereignty is a substantive term designating supreme legitimate authority over some polity. In international law, sovereignty is the exercise of power by a state. "De jure" sovereignty refers to the legal right to do so; "de facto" sovereignty refers to the factual ability to do so. This can become an issue of special concern upon the failure of the usual expectation that "de jure" and "de facto" sovereignty exist at the place and time of concern, and reside within the same organization.

The term arises from the unattested Vulgar Latin's "*superanus", (itself derived form of Latin "super" - "over") meaning "chief", "ruler". Its spelling, which varied from the word's first appearance in English in the fourteenth century, was influenced by the English reign.

The concepts of sovereignty have been discussed throughout history, and are still actively debated. Its definition, concept, and application has changed throughout, especially during the Age of Enlightenment. The current notion of state sovereignty contains four aspects consisting of territory, population, authority and recognition. According to Stephen D. Krasner, the term could also be understood in four different ways: 

Often, these four aspects all appear together, but this is not necessarily the case – they are not affected by one another, and there are historical examples of states that were non-sovereign in one aspect while at the same time being sovereign in another of these aspects. According to Immanuel Wallerstein, another fundamental feature of sovereignty is that it is a claim that must be recognised by others if it is to have any meaning: 

The Roman jurist Ulpian observed that:

Ulpian was expressing the idea that the Emperor exercised a rather absolute form of sovereignty, that originated in the people, although he did not use the term expressly.

Ulpian's statements were known in medieval Europe, but sovereignty was an important concept in medieval times. Medieval monarchs were "not" sovereign, at least not strongly so, because they were constrained by, and shared power with, their feudal aristocracy. Furthermore, both were strongly constrained by custom.

Sovereignty existed during the Medieval period as the "de jure" rights of nobility and royalty, and in the "de facto" capability of individuals to make their own choices in life.

Around 1380–1400, the issue of feminine sovereignty was addressed in Geoffrey Chaucer's Middle English collection of "Canterbury Tales", specifically in "The Wife of Bath's Tale."

A later English Arthurian romance, "The Wedding of Sir Gawain and Dame Ragnell" (c. 1450), uses many of the same elements of the Wife of Bath's tale, yet changes the setting to the court of King Arthur and the Knights of the Round Table. The story revolves around the knight Sir Gawain granting to Dame Ragnell, his new bride, what is purported to be wanted most by women: sovereignty.

Sovereignty reemerged as a concept in the late 16th century, a time when civil wars had created a craving for stronger central authority, when monarchs had begun to gather power onto their own hands at the expense of the nobility, and the modern nation state was emerging. Jean Bodin, partly in reaction to the chaos of the French wars of religion, presented theories of sovereignty calling for strong central authority in the form of absolute monarchy. In his 1576 treatise "Les Six Livres de la République" ("Six Books of the Republic") Bodin argued that it is inherent in the nature of the state that sovereignty must be:


Bodin rejected the notion of transference of sovereignty from people to the ruler (also known as "the sovereign"); natural law and divine law confer upon the sovereign the right to rule. And the sovereign is not above divine law or natural law. He is above ("ie." not bound by) only positive law, that is, laws made by humans. He emphasized that a sovereign is bound to observe certain basic rules derived from the divine law, the law of nature or reason, and the law that is common to all nations (jus gentium), as well as the fundamental laws of the state that determine who is the sovereign, who succeeds to sovereignty, and what limits the sovereign power. Thus, Bodin’s sovereign was restricted by the constitutional law of the state and by the higher law that was considered as binding upon every human being. The fact that the sovereign must obey divine and natural law imposes ethical constraints on him. Bodin also held that the "lois royales", the fundamental laws of the French monarchy which regulated matters such as succession, are natural laws and are binding on the French sovereign.

Despite his commitment to absolutism, Bodin held some moderate opinions on how government should in practice be carried out. He held that although the sovereign is not obliged to, it is advisable for him, as a practical expedient, to convene a senate from whom he can obtain advice, to delegate some power to magistrates for the practical administration of the law, and to use the Estates as a means of communicating with the people. Bodin believed that “the most divine, most excellent, and the state form most proper to royalty is governed partly aristocratically and partly democratically”.

With his doctrine that sovereignty is conferred by divine law, Bodin predefined the scope of the divine right of kings.
During the Age of Enlightenment, the idea of sovereignty gained both legal and moral force as the main Western description of the meaning and power of a State. In particular, the "Social contract" as a mechanism for establishing sovereignty was suggested and, by 1800, widely accepted, especially in the new United States and France, though also in Great Britain to a lesser extent.

Thomas Hobbes, in "Leviathan" (1651) put forward a conception of sovereignty similar to Bodin's, which had just achieved legal status in the "Peace of Westphalia", but for different reasons. He created the first modern version of the social contract (or contractarian) theory, arguing that to overcome the "nasty, brutish and short" quality of life without the cooperation of other human beings, people must join in a "commonwealth" and submit to a "Soveraigne Power" that is able to compel them to act in the common good. This expediency argument attracted many of the early proponents of sovereignty. Hobbes strengthened the definition of sovereignty beyond either Westphalian or Bodin's, by saying that it must be:


Hobbes' hypothesis—that the ruler's sovereignty is contracted to him by the people in return for his maintaining their physical safety—led him to conclude that if and when the ruler fails, the people recover their ability to protect themselves by forming a new contract.

Hobbes's theories decisively shape the concept of sovereignty through the medium of social contract theories. Jean-Jacques Rousseau's (1712–1778) definition of popular sovereignty (with early antecedents in Francisco Suárez's theory of the origin of power), provides that the people are the legitimate sovereign. Rousseau considered sovereignty to be inalienable; he condemned the distinction between the origin and the exercise of sovereignty, a distinction upon which constitutional monarchy or representative democracy is founded. John Locke, and Montesquieu are also key figures in the unfolding of the concept of sovereignty; their views differ with Rousseau and with Hobbes on this issue of alienability.

The second book of Jean-Jacques Rousseau's "Du Contrat Social, ou Principes du droit politique" (1762) deals with sovereignty and its rights. Sovereignty, or the general will, is inalienable, for the will cannot be transmitted; it is indivisible, since it is essentially general; it is infallible and always right, determined and limited in its power by the common interest; it acts through laws. Law is the decision of the general will in regard to some object of common interest, but though the general will is always right and desires only good, its judgment is not always enlightened, and consequently does not always see wherein the common good lies; hence the necessity of the legislator. But the legislator has, of himself, no authority; he is only a guide who drafts and proposes laws, but the people alone (that is, the sovereign or general will) has authority to make and impose them.

Rousseau, in the Social Contract"
argued, "the growth of the State giving the trustees of public authority more and means to abuse their power, the more the Government has to have force to contain the people, the more force the Sovereign should have in turn in order to contain the Government," with the understanding that the Sovereign is "a collective being of wonder" (Book II, Chapter I) resulting from "the general will" of the people, and that "what any man, whoever he may be, orders on his own, is not a law" (Book II, Chapter VI) – and furthermore predicated on the assumption that the people have an unbiased means by which to ascertain the general will. Thus the legal maxim, "there is no law without a sovereign."

An important factor of sovereignty is its degree of absoluteness. A sovereign power has absolute sovereignty when it is not restricted by a constitution, by the laws of its predecessors, or by custom, and no areas of law or policy are reserved as being outside its control. International law; policies and actions of neighboring states; cooperation and respect of the populace; means of enforcement; and resources to enact policy are factors that might limit sovereignty. For example, parents are not guaranteed the right to decide some matters in the upbringing of their children independent of societal regulation, and municipalities do not have unlimited jurisdiction in local matters, thus neither parents nor municipalities have absolute sovereignty. Theorists have diverged over the desirability of increased absoluteness.

A key element of sovereignty in a legalistic sense is that of exclusivity of jurisdiction. Specifically, the degree to which decisions made by a sovereign entity might be contradicted by another authority. Along these lines, the German sociologist Max Weber proposed that sovereignty is a community's monopoly on the legitimate use of force; and thus any group claiming the same right must either be brought under the yoke of the sovereign, proven illegitimate, or otherwise contested and defeated for sovereignty to be genuine. International law, competing branches of government, and authorities reserved for subordinate entities (such as federated states or republics) represent legal infringements on exclusivity. Social institutions such as religious bodies, corporations, and competing political parties might represent "de facto" infringements on exclusivity.

"De jure", or legal, sovereignty concerns the expressed and institutionally recognised right to exercise control over a territory. "De facto", or actual, sovereignty is concerned with whether control in fact exists. Cooperation and respect of the populace; control of resources in, or moved into, an area; means of enforcement and security; and ability to carry out various functions of state all represent measures of "de facto" sovereignty. When control is practiced predominantly by military or police force it is considered "coercive sovereignty".

State sovereignty is sometimes viewed synonymously with independence, however, sovereignty can be transferred as a legal right whereas independence cannot. A state can achieve "de facto" independence long after acquiring sovereignty, such as in the case of Cambodia, Laos and Vietnam. Additionally, independence can also be suspended when an entire region becomes subject to an occupation such as when Iraq had been overrun by the forces to take part in the Iraq War of 2003, Iraq had not been annexed by any country, so its sovereignty during this period was not contested by any state including those present on the territory. Alternatively, independence can be lost completely when sovereignty itself becomes the subject of dispute. The pre-World War II administrations of Latvia, Lithuania and Estonia maintained an exile existence (and considerable international recognition) whilst their territories were annexed by the Soviet Union and governed locally by their pro-Soviet functionaries. When in 1991 Latvia, Lithuania and Estonia re-enacted independence, it was done so on the basis of continuity directly from the pre-Soviet republics. Another complicated sovereignty scenario can arise when regime itself is the subject of dispute. In the case of Poland, the People's Republic of Poland which governed Poland from 1945 to 1989 is now seen to have been an illegal entity by the modern Polish administration. The post-1989 Polish state claims direct continuity from the Second Polish Republic which ended in 1939. For other reasons however, Poland maintains its communist-era outline as opposed to its pre-World War II shape which included areas now in Belarus, Czech Republic, Lithuania, Slovakia and Ukraine but did not include some of its western regions that were then in Germany.

At the opposite end of the scale, there is no dispute regarding the self-governance of certain self-proclaimed states such as the Republic of Kosovo or Somaliland (see List of states with limited recognition, but most of them are puppet states) since their governments neither answer to a bigger state, nor is their governance subjected to supervision. The sovereignty (i.e. legal right to govern) however, is disputed in all three cases as the first entity is claimed by Serbia and the second by Somalia.

Internal sovereignty is the relationship between a sovereign power and the political community. A central concern is legitimacy: by what right does a government exercise authority? Claims of legitimacy might refer to the divine right of kings, or to a social contract (i.e. popular sovereignty). Max Weber offered a first categorization of political authority and legitimacy with the categories of traditional, charismatic and legal-rational.

With Sovereignty meaning holding supreme, independent authority over a region or state, Internal Sovereignty refers to the internal affairs of the state and the location of supreme power within it. A state that has internal sovereignty is one with a government that has been elected by the people and has the popular legitimacy. Internal sovereignty examines the internal affairs of a state and how it operates. It is important to have strong internal sovereignty in relation to keeping order and peace. When you have weak internal sovereignty, organisations such as rebel groups will undermine the authority and disrupt the peace. The presence of a strong authority allows you to keep agreement and enforce sanctions for the violation of laws. The ability for leadership to prevent these violations is a key variable in determining internal sovereignty. The lack of internal sovereignty can cause war in one of two ways: first, undermining the value of agreement by allowing costly violations; and second, requiring such large subsidies for implementation that they render war cheaper than peace. Leadership needs to be able to promise members, especially those like armies, police forces, or paramilitaries will abide by agreements. The presence of strong internal sovereignty allows a state to deter opposition groups in exchange for bargaining. It has been said that a more decentralized authority would be more efficient in keeping peace because the deal must please not only the leadership but also the opposition group. While the operations and affairs within a state are relative to the level of sovereignty within that state, there is still an argument over who should hold the authority in a sovereign state.

This argument between who should hold the authority within a sovereign state is called the traditional doctrine of public sovereignty. This discussion is between an internal sovereign or an authority of public sovereignty. An internal sovereign is a political body that possesses ultimate, final and independent authority; one whose decisions are binding upon all citizens, groups and institutions in society. Early thinkers believe sovereignty should be vested in the hands of a single person, a monarch. They believed the overriding merit of vesting sovereignty in a single individual was that sovereignty would therefore be indivisible; it would be expressed in a single voice that could claim final authority. An example of an internal sovereign or monarch is Louis XIV of France during the seventeenth century; Louis XIV claimed that he was the state. Jean-Jacques Rousseau rejected monarchical rule in favor of the other type of authority within a sovereign state, public sovereignty. Public Sovereignty is the belief that ultimate authority is vested in the people themselves, expressed in the idea of the general will. This means that the power is elected and supported by its members, the authority has a central goal of the good of the people in mind. The idea of public sovereignty has often been the basis for modern democratic theory.

Within the modern governmental system, internal sovereignty is usually found in states that have public sovereignty and rarely found within a state controlled by an internal sovereign. A form of government that is a little different from both is the UK parliament system. John Austin argued that sovereignty in the UK was vested neither in the Crown nor in the people but in the "Queen-in-Parliament". This is the origin of the doctrine of parliamentary sovereignty and is usually seen as the fundamental principle of the British constitution. With these principles of parliamentary sovereignty majority control can gain access to unlimited constitutional authority, creating what has been called "elective dictatorship" or "modern autocracy". Public sovereignty in modern governments is a lot more common with examples like the USA, Canada, Australia and India where government is divided into different levels.

External sovereignty concerns the relationship between a sovereign power and other states. For example, the United Kingdom uses the following criterion when deciding under what conditions other states recognise a political entity as having sovereignty over some territory;
External sovereignty is connected with questions of international law – such as: when, if ever, is intervention by one country into another's territory permissible?

Following the Thirty Years' War, a European religious conflict that embroiled much of the continent, the Peace of Westphalia in 1648 established the notion of territorial sovereignty as a norm of noninterference in the affairs of other states, so-called Westphalian sovereignty, even though the actual treaty itself reaffirmed the multiple levels of sovereignty of the Holy Roman Empire. This resulted as a natural extension of the older principle of "cuius regio, eius religio" (Whose realm, his religion), leaving the Roman Catholic Church with little ability to interfere with the internal affairs of many European states. It is a myth, however, that the Treaties of Westphalia created a new European order of equal sovereign states.

In international law, sovereignty means that a government possesses full control over affairs within a territorial or geographical area or limit. Determining whether a specific entity is sovereign is not an exact science, but often a matter of diplomatic dispute. There is usually an expectation that both "de jure" and "de facto" sovereignty rest in the same organisation at the place and time of concern. Foreign governments use varied criteria and political considerations when deciding whether or not to recognise the sovereignty of a state over a territory. Membership in the United Nations requires that "[t]he admission of any such state to membership in the United Nations will be effected by a decision of the General Assembly upon the recommendation of the Security Council."

Sovereignty may be recognized even when the sovereign body possesses no territory or its territory is under partial or total occupation by another power. The Holy See was in this position between the annexation in 1870 of the Papal States by Italy and the signing of the Lateran Treaties in 1929, a 59-year period during which it was recognised as sovereign by many (mostly Roman Catholic) states despite possessing no territory – a situation resolved when the Lateran Treaties granted the Holy See sovereignty over the Vatican City. Another case, "sui generis", is the Sovereign Military Order of Malta, the third sovereign entity inside Italian territory (after San Marino and the Vatican City State) and the second inside the Italian capital (since in 1869 the Palazzo di Malta and the Villa Malta receive extraterritorial rights, in this way becoming the only "sovereign" territorial possessions of the modern Order), which is the last existing heir to one of several once militarily significant, crusader states of sovereign military orders. In 1607 its Grand masters were also made Reichsfürst (princes of the Holy Roman Empire) by the Holy Roman Emperor, granting them seats in the Reichstag, at the time the closest permanent equivalent to a UN-type general assembly; confirmed 1620). These sovereign rights were never deposed, only the territories were lost. 100 modern states still maintain full diplomatic relations with the order (now "de facto" "the most prestigious service club"), and the UN awarded it observer status.

The governments-in-exile of many European states (for instance, Norway, Netherlands or Czechoslovakia) during the Second World War were regarded as sovereign despite their territories being under foreign occupation; their governance resumed as soon as the occupation had ended. The government of Kuwait was in a similar situation "vis-à-vis" the Iraqi occupation of its country during 1990–1991. The government of Republic of China was recognized as sovereign over China from 1911 to 1971 despite that its mainland China territory became occupied by Communist Chinese forces since 1949. In 1971 it lost UN recognition to Chinese Communist-led People's Republic of China and its sovereign and political status as a state became disputed and it lost its ability to use "China" as its name and therefore became commonly known as Taiwan.

The International Committee of the Red Cross is commonly mistaken to be sovereign. It has been granted various degrees of special privileges and legal immunities in many countries, including Belgium, France, Switzerland and soon in Ireland. Similarly for Australia, Russia, South Korea, South Africa and the US. that in cases like Switzerland are considerable, The Committee is a private organisation governed by Swiss law.

Just as the office of head of state can be vested jointly in several persons within a state, the sovereign jurisdiction over a single political territory can be shared jointly by two or more consenting powers, notably in the form of a condominium.

Likewise the member states of international organizations may voluntarily bind themselves by treaty to a supranational organization, such as a continental union. In the case of the European Union members states this is called "pooled sovereignty".

Another example of shared and pooled sovereignty is the Acts of Union 1707 which created the unitary state now known as the United Kingdom. It was a full economic union, meaning the Scottish and English systems of currency, taxation and laws regulating trade were aligned. Nonetheless, Scotland and England never fully surrendered or pooled all of their governance sovereignty; they retained many of their previous national institutional features and characteristics, particularly relating to their legal, religious and educational systems. In 2012, the Scottish Government, created in 1998 through devolution in the United Kingdom, negotiated terms with the Government of the United Kingdom for the 2014 Scottish independence referendum which resulted in the people of Scotland deciding to continue the pooling of its sovereignty with the rest of the United Kingdom.

A community of people who claim the right of self-determination based on a common ethnicity, history and culture might seek to establish sovereignty over a region, thus creating a nation-state. Such nations are sometimes recognised as autonomous areas rather than as fully sovereign, independent states.

In a federal system of government, "sovereignty" also refers to powers which a constituent state or republic possesses independently of the national government. In a confederation, constituent entities retain the right to withdraw from the national body and the union is often more temporary than a federation.

Different interpretations of state sovereignty in the United States of America, as it related to the expansion of slavery and fugitive slave laws, led to the outbreak of the American Civil War. Depending on the particular issue, sometimes both northern and southern states justified their political positions by appealing to state sovereignty. Fearing that slavery would be threatened by results of the 1860 presidential election, eleven slave states declared their independence from the federal Union and formed a new confederation. The United States government rejected the secessions as rebellion, declaring that secession from the Union by an individual state was unconstitutional, as the states were part of an indissolvable federation. 

A number of modes for acquisition of sovereignty are presently or have historically been recognized in international law as lawful methods by which a state may acquire sovereignty over external territory. The classification of these modes originally derived from Roman property law and from the 15th and 16th century with the development of international law. The modes are:

There exist vastly differing views on the moral basis of sovereignty. A fundamental polarity is between theories that assert that sovereignty is vested directly in the sovereigns by divine or natural right and theories that assert it originates from the people. In the latter case there is a further division into those that assert that the people transfer their sovereignty to the sovereign (Hobbes), and those that assert that the people retain their sovereignty (Rousseau).

During the brief period of absolute monarchies in Europe, the divine right of kings was an important competing justification for the exercise of sovereignty. The Mandate of Heaven had some similar implications in China.

A republic is a form of government in which the people, or some significant portion of them, retain sovereignty over the government and where offices of state are not granted through heritage. A common modern definition of a republic is a government having a head of state who is not a monarch.

Democracy is based on the concept of "popular sovereignty". In a direct democracy the public plays an active role in shaping and deciding policy. Representative democracy permits a transfer of the exercise of sovereignty from the people to a legislative body or an executive (or to some combination of legislature, executive and Judiciary). Many representative democracies provide limited direct democracy through referendum, initiative, and recall.

Parliamentary sovereignty refers to a representative democracy where the parliament is ultimately sovereign and not the executive power nor the judiciary.


According to Matteo Laruffa "sovereignty resides in every public action and policy as the exercise of executive powers by institutions open to the participation of citizens to the decision-making processes"

Another topic is whether the law is held to be sovereign, that is, whether it is above political or other interference. Sovereign law constitutes a true state of law, meaning the letter of the law (if constitutionally correct) is applicable and enforceable, even when against the political will of the nation, as long as not formally changed following the constitutional procedure. Strictly speaking, any deviation from this principle constitutes a revolution or a coup d'état, regardless of the intentions.




</doc>
<doc id="28798" url="https://en.wikipedia.org/wiki?curid=28798" title="SNAFU">
SNAFU

SNAFU is an acronym that is widely used to stand for the sarcastic expression Situation Normal: All Fucked Up. It is a well-known example of military acronym slang; however the original military acronym stood for "Status Nominal: All Fucked Up". It is sometimes bowdlerized to "all fouled up" or similar. It means that the situation is bad, but that this is a normal state of affairs. The acronym is believed to have originated in the United States Marine Corps during World War II.

In modern usage, "SNAFU" is sometimes used as an interjection. "SNAFU" also sometimes refers to a bad situation, mistake, or cause of trouble. It is more commonly used in modern vernacular to describe running into an error or problem that is large and unexpected. For example, in 2005, "The New York Times" published an article titled "Hospital Staff Cutback Blamed for Test Result Snafu".

"SNAFU" was first recorded in "American Notes and Queries" in their September 1941 issue. "Time" magazine used the term in their June 16, 1942, issue: "Last week U.S. citizens knew that gasoline rationing and rubber requisitioning were snafu." Most reference works, including the "Random House Unabridged Dictionary", supply an origin date of 1940–1944, generally attributing it to the U.S. Army. Rick Atkinson ascribes the origin of "SNAFU", "FUBAR", and a bevy of other terms to cynical GIs ridiculing the Army's penchant for acronyms.

The attribution of "SNAFU" to the American military is not universally accepted: it has also been attributed to the British, although the "Oxford English Dictionary" gives its origin and first recorded use as U.S. military slang.

In a wider study of military slang, Elkin noted in 1946 that there "are a few acceptable substitutes such as 'screw up' or 'mess up,' but these do not have the emphasis value of the obscene equivalent." He considered the expression to be "a caricature of Army direction. The soldier resignedly accepts his own less responsible position and expresses his cynicism at the inefficiency of Army authority." He also noted that "the expression […] is coming into general civilian use."

SUSFU is an acronym for Situation unchanged: still fucked up", but can also be bowdlerized—just like "SNAFU"—to Situation unchanged: still fouled up" or similar. It is used in a military context, was first recorded in the "American Notes and Queries" in their September 1941 issue.




</doc>
<doc id="28800" url="https://en.wikipedia.org/wiki?curid=28800" title="Democratic Left Alliance">
Democratic Left Alliance

Democratic Left Alliance (, SLD) is a social-democratic political party in Poland. It was formed in 1991 as an electoral alliance of centre-left parties, and became a single party on 15 April 1999. The party is a member of the Party of European Socialists and Progressive Alliance.

The coalition can be classified as left-wing. However, during the 1990s, it managed to attract voters from the pro-market and even right-wing camp. The main support for SLD came from middle-rank state sector employees, retired people, former communist Polish United Workers Party (PZPR) and All-Poland Alliance of Trade Unions (OPZZ) members and those who were unlikely to be frequent church-goers. The core of the coalition (Social Democracy of the Republic of Poland) rejected concepts such as lustration and de-communization, supported a parliamentarian regime with only the role of an arbiter for the president and criticized the right-wing camp for the introduction of religious education into school. The ex-communists criticized the economic reforms, pointing to the high social costs, without negating the reforms per se.

SdRP, SDU and some other socialist and social-democratic parties had formed the original Democratic Left Alliance as a centre-left coalition just prior to the nation's first free elections in 1991. In 1999 the coalition became a party but lost some members.

At the time, the coalition's membership drew mostly from the former PZPR. An alliance between the SLD and the Polish People's Party (PSL) ruled Poland in the years 1993–1997. However, the coalition lost power to the right-wing Solidarity Electoral Action in the 1997 election as the right-wing opposition was united this time and because of the decline of support for SLD's coalition partner PSL, though the SLD itself actually gained votes.

SLD formed a coalition with Labour Union before the 2001 Polish election and won it overwhelmingly at last by capturing about 5.3 million votes, 42% of the whole and won 200 of 460 seats in the Sejm and 75 of 100 in the Senate. After the elections, the coalition was joined by the Polish People's Party (PSL) in forming a government and Leszek Miller became the Prime Minister. In March 2003, the PSL left the coalition.

By 2004 the support for SLD in the polls had dropped from about 30% to just below 10%, and several high-ranking party members had been accused of taking part in high-profile political scandals by the mainstream press (most notably the Rywin affair: Rywin-gate).

On 6 March 2004, Leszek Miller resigned as party leader and was replaced by Krzysztof Janik. On March 26 the Sejm speaker Marek Borowski, together with other high-ranking SLD officials, announced the creation of a new centre-left party, the Social Democratic Party of Poland. On the next day, Leszek Miller announced he would step down as Prime Minister on 2 May 2004, the day after Poland joined the European Union. Miller proceeded to do so.

In the 2004 European Parliament election, it only received 9% of the votes, giving it 5 of 54 seats reserved for Poland in the European Parliament, as part of the Party of European Socialists.

Wojciech Olejniczak, the former Minister of Agriculture and Rural Development, was elected the president of SLD on 29 May 2004, succeeded Józef Oleksy, who resigned from the post of Polish Prime Minister due to false accusations of links to the KGB.

The 2004 European elections foreshadowed the SLD's huge defeat in the 2005 parliamentary election, in which it won only 11.3% of the vote. This gave the party 55 seats, barely a quarter of what it had had prior to the election. It also lost all of its Senators. In late 2006 a centre-left political alliance called Left and Democrats was created, comprising SLD and smaller centre-left parties, the Labour Union, the Social Democratic Party of Poland, and the liberal Democratic Party – demokraci.pl. The coalition won a disappointing 13% in the 2007 parliamentary election and was dissolved soon after in April 2008. On 31 May 2008, Olejniczak was replaced by Grzegorz Napieralski as an SLD leader.

In the 2009 European election the Democratic Left Alliance-Labor Union joint ticket received 12% of the vote and 7 MEPs were elected as part of the newly retitled Socialists & Democrats group.

In the 2011 parliamentary election, SLD received 8.24% of the vote which gave it 27 seats in the Sejm. After the elections, one of the party members, Sławomir Kopyciński, decided to leave SLD and join Palikot's Movement. On December 10, 2011, Leszek Miller was chosen to return as the party leader.

In the 2014 European elections on 25 May 2014, the SLD received 9.4% of the national vote and returned 4 MEPs.

In July 2015 the SLD formed the United Left electoral alliance along with Your Movement (TR), Labour United (UP) and The Greens (PZ) and minor parties to contest the upcoming election.

In the 2015 parliamentary election held on 25 October 2015, the United Left list received 7.6% of the vote, below the 8% threshold (electoral alliances must win at least 8% of the vote, as opposed to the 5% for individual parties), leaving the SLD without parliamentary representation for the first time. Indeed, for the first time since the end of Communism, no centre-left parties won any seats in this election.

In 2017, the party withdrew from the Socialist International, while maintaining ties with the Progressive Alliance.

For the 2019 parliamentary election SLD formed an alliance with Razem and Wiosna, known as The Left. In the 2019 parliamentary election, the alliance won 12.6% of the vote and 49 seats in the Sejm, with the SLD winning 24. Later, it was announced that the Democratic Left Alliance would form with the Spring new political party called the New Left. The creation was delayed due to the COVID-19 pandemic.

The SLD is usually seen as the face of the standard Polish left, having achieved notable electoral success during the 90s and benefitting from a strongly organized network of local offices, which span 320 of Poland's 380 administrative counties. For this reason, it was often viewed as the go-to party for left-leaning Poles for the majority of Poland's modern history. The party's monopoly on mainstream left-wing economic ideas in Poland however ended, after the right-wing PiS party adopted many economically interventionist positions, which led a considerable portion of economically left-wing Poles to vote for PiS instead.

Besides self-described left-wingers, the party enjoys the support of many members of the country's police and military, but its largest voting bloc resides among former PZPR members, government officials and civil servants during the PPR period, which are seen as the party's core supporters. The loyal support of this voting bloc enabled the SLD to remain the largest party of the Polish left, even throughout the scandals that rocked the party in the early 2000s.

However, this electoral bloc was seen as unreliable by political observers, as despite the fact that it originally constituted a huge voting bloc, that segment of the population would inevitably shrink as its members steadily age. Following the passage of a "degradation law" by the ruling right-wing PiS party, which cut pensions and disability benefits to thousands of former bureaucrats, however, the party has undergone a revival, as more and more people's primary income came to be threatened by the new government policy. This led many of those affected to support the SLD, thus enlarging and mobilizing the formerly shrinking voting bloc.

The SLD nonetheless made a significant effort to broaden its political appeal by joining forces with two smaller left-wing parties in 2019, creating The Left political alliance, which poses itself as a 'modern' take on leftism.




</doc>
<doc id="28801" url="https://en.wikipedia.org/wiki?curid=28801" title="SLD">
SLD

SLD may refer to:






</doc>
<doc id="28803" url="https://en.wikipedia.org/wiki?curid=28803" title="Segmentation fault">
Segmentation fault

In computing, a segmentation fault (often shortened to segfault) or access violation is a fault, or failure condition, raised by hardware with memory protection, notifying an operating system (OS) the software has attempted to access a restricted area of memory (a memory access violation). On standard x86 computers, this is a form of general protection fault. The OS kernel will, in response, usually perform some corrective action, generally passing the fault on to the offending process by sending the process a signal. Processes can in some cases install a custom signal handler, allowing them to recover on their own, but otherwise the OS default signal handler is used, generally causing abnormal termination of the process (a program crash), and sometimes a core dump.

Segmentation faults are a common class of error in programs written in languages like C that provide low-level memory access. They arise primarily due to errors in use of pointers for virtual memory addressing, particularly illegal access. Another type of memory access error is a bus error, which also has various causes, but is today much rarer; these occur primarily due to incorrect "physical" memory addressing, or due to misaligned memory access – these are memory references that the hardware "cannot" address, rather than references that a process is not "allowed" to address.

Many programming languages may employ mechanisms designed to avoid segmentation faults and improve memory safety. For example, the Rust programming language employs an 'Ownership' based model to ensure memory safety. Other languages, such as Lisp and Java, employ garbage collection, which avoids certain classes of memory errors that could lead to segmentation faults.

A segmentation fault occurs when a program attempts to access a memory location that it is not allowed to access, or attempts to access a memory location in a way that is not allowed (for example, attempting to write to a read-only location, or to overwrite part of the operating system).

The term "segmentation" has various uses in computing; in the context of "segmentation fault", a term used since the 1950s, it refers to the address space of a "program." With memory protection, only the program's own address space is readable, and of this, only the stack and the read/write portion of the data segment of a program are writable, while read-only data and the code segment are not writable. Thus attempting to read outside of the program's address space, or writing to a read-only segment of the address space, results in a segmentation fault, hence the name.

On systems using hardware memory segmentation to provide virtual memory, a segmentation fault occurs when the hardware detects an attempt to refer to a non-existent segment, or to refer to a location outside the bounds of a segment, or to refer to a location in a fashion not allowed by the permissions granted for that segment. On systems using only paging, an invalid page fault generally leads to a segmentation fault, and segmentation faults and page faults are both faults raised by the virtual memory management system. Segmentation faults can also occur independently of page faults: illegal access to a valid page is a segmentation fault, but not an invalid page fault, and segmentation faults can occur in the middle of a page (hence no page fault), for example in a buffer overflow that stays within a page but illegally overwrites memory.

At the hardware level, the fault is initially raised by the memory management unit (MMU) on illegal access (if the referenced memory exists), as part of its memory protection feature, or an invalid page fault (if the referenced memory does not exist). If the problem is not an invalid logical address but instead an invalid physical address, a bus error is raised instead, though these are not always distinguished.

At the operating system level, this fault is caught and a signal is passed on to the offending process, activating the process's handler for that signal. Different operating systems have different signal names to indicate that a segmentation fault has occurred. On Unix-like operating systems, a signal called SIGSEGV (abbreviated from "segmentation violation") is sent to the offending process. On Microsoft Windows, the offending process receives a STATUS_ACCESS_VIOLATION exception.

The conditions under which segmentation violations occur and how they manifest themselves are specific to hardware and the operating system: different hardware raises different faults for given conditions, and different operating systems convert these to different signals that are passed on to processes. The proximate cause is a memory access violation, while the underlying cause is generally a software bug of some sort. Determining the root cause – debugging the bug – can be simple in some cases, where the program will consistently cause a segmentation fault (e.g., dereferencing a null pointer), while in other cases the bug can be difficult to reproduce and depend on memory allocation on each run (e.g., dereferencing a dangling pointer).

The following are some typical causes of a segmentation fault:
These in turn are often caused by programming errors that result in invalid memory access:

In C code, segmentation faults most often occur because of errors in pointer use, particularly in C dynamic memory allocation. Dereferencing a null pointer will always result in a segmentation fault, but wild pointers and dangling pointers point to memory that may or may not exist, and may or may not be readable or writable, and thus can result in transient bugs. For example:

char *p1 = NULL; // Null pointer
char *p2; // Wild pointer: not initialized at all.
char *p3 = malloc(10 * sizeof(char)); // Initialized pointer to allocated memory
free(p3); // p3 is now a dangling pointer, as memory has been freed

Now, dereferencing any of these variables could cause a segmentation fault: dereferencing the null pointer generally will cause a segfault, while reading from the wild pointer may instead result in random data but no segfault, and reading from the dangling pointer may result in valid data for a while, and then random data as it is overwritten.

The default action for a segmentation fault or bus error is abnormal termination of the process that triggered it. A core file may be generated to aid debugging, and other platform-dependent actions may also be performed. For example, Linux systems using the grsecurity patch may log SIGSEGV signals in order to monitor for possible intrusion attempts using buffer overflows.

On some systems, like Linux and Windows, it is possible for the program itself to handle a segmentation fault.. Depending on the architecture and operating system, the running program can not only handle the event but may extract some information about its state like getting a stack trace, processor register values, the line of the source code when it was triggered, memory address that was invalidly accessed and whether the action was a read or a write.

Although a segmentation fault generally means that the program has a bug that needs fixing, it is also possible to intentionally cause such failure for the purposes of testing, debugging and also to emulate platforms were direct access to memory is needed. On the latter case, the system must be able to allow the program to run even after the fault occurs. In this case, when the system allows, it is possible to handle the event and increment the processor program counter to "jump" over the failing instruction to continue the execution.

Writing to read-only memory raises a segmentation fault. At the level of code errors, this occurs when the program writes to part of its own code segment or the read-only portion of the data segment, as these are loaded by the OS into read-only memory.

Here is an example of ANSI C code that will generally cause a segmentation fault on platforms with memory protection. It attempts to modify a string literal, which is undefined behavior according to the ANSI C standard. Most compilers will not catch this at compile time, and instead compile this to executable code that will crash:
int main(void)

When the program containing this code is compiled, the string "hello world" is placed in the rodata section of the program executable file: the read-only section of the data segment. When loaded, the operating system places it with other strings and constant data in a read-only segment of memory. When executed, a variable, "s", is set to point to the string's location, and an attempt is made to write an "H" character through the variable into the memory, causing a segmentation fault. Compiling such a program with a compiler that does not check for the assignment of read-only locations at compile time, and running it on a Unix-like operating system produces the following runtime error:
$ gcc segfault.c -g -o segfault
$ ./segfault
Segmentation fault
Backtrace of the core file from GDB:
Program received signal SIGSEGV, Segmentation fault.
0x1c0005c2 in main () at segfault.c:6
6 *s = 'H';
This code can be corrected by using an array instead of a character pointer, as this allocates memory on stack and initializes it to the value of the string literal:

char s[] = "hello world";
s[0] = 'H'; // equivalently, *s = 'H';

Even though string literals should not be modified (this has undefined behavior in the C standard), in C they are of codice_1 type, so there is no implicit conversion in the original code (which points a codice_2 at that array), while in C++ they are of codice_3 type, and thus there is an implicit conversion, so compilers will generally catch this particular error.

In C and C-like languages, null pointers are used to mean "pointer to no object" and as an error indicator, and dereferencing a null pointer (a read or write through a null pointer) is a very common program error. The C standard does not say that the null pointer is the same as the pointer to memory address 0, though that may be the case in practice. Most operating systems map the null pointer's address such that accessing it causes a segmentation fault. This behavior is not guaranteed by the C standard. Dereferencing a null pointer is undefined behavior in C, and a conforming implementation is allowed to assume that any pointer that is dereferenced is not null.
int *ptr = NULL;
printf("%d", *ptr);
This sample code creates a null pointer, and then tries to access its value (read the value). Doing so causes a segmentation fault at runtime on many operating systems.

Dereferencing a null pointer and then assigning to it (writing a value to a non-existent target) also usually causes a segmentation fault:

int *ptr = NULL;
The following code includes a null pointer dereference, but when compiled will often not result in a segmentation fault, as the value is unused and thus the dereference will often be optimized away by dead code elimination:

int *ptr = NULL;

Another example is recursion without a base case:
int main(void)

which causes the stack to overflow which results in a segmentation fault. Infinite recursion may not necessarily result in a stack overflow depending on the language, optimizations performed by the compiler and the exact structure of a code. In this case, the behavior of unreachable code (the return statement) is undefined, so the compiler can eliminate it and use a tail call optimization that might result in no stack usage. Other optimizations could include translating the recursion into iteration, which given the structure of the example function would result in the program running forever, while probably not overflowing its stack.




</doc>
<doc id="28804" url="https://en.wikipedia.org/wiki?curid=28804" title="Signal separation">
Signal separation

Source separation, blind signal separation (BSS) or blind source separation, is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. It is most commonly applied in digital signal processing and involves the analysis of mixtures of signals; the objective is to recover the original component signals from a mixture signal. The classical example of a source separation problem is the cocktail party problem, where a number of people are talking simultaneously in a room (for example, at a cocktail party), and a listener is trying to follow one of the discussions. The human brain can handle this sort of auditory source separation problem, but it is a difficult problem in digital signal processing.

This problem is in general highly underdetermined, but useful solutions can be derived under a surprising variety of conditions. Much of the early literature in this field focuses on the separation of temporal signals such as audio. However, blind signal separation is now routinely performed on multidimensional data, such as images and tensors, which may involve no time dimension whatsoever.

Several approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent component analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal. The field of computational auditory scene analysis attempts to achieve auditory source separation using an approach that is based on human hearing.

The human brain must also solve this problem in real time. In human perception this ability is commonly referred to as auditory scene analysis or the cocktail party effect.

At a cocktail party, there is a group of people talking at the same time. You have multiple microphones picking up mixed signals, but you want to isolate the speech of a single person. BSS can be used to separate the individual sources by using mixed signals. In the presence of noise, dedicated optimization criteria need to be used

Figure 2 shows the basic concept of BSS. The individual source signals are shown as well as the mixed signals which are received signals. BSS is used to separate the mixed signals with only knowing mixed signals and nothing about original signal or how they were mixed. The separated signals are only approximations of the source signals. The separated images, were separated using Python and the Shogun toolbox using Joint Approximation Diagonalization of Eigen-matrices (JADE) algorithm which is based on independent component analysis, ICA. This toolbox method can be used with multi-dimensions but for an easy visual aspect images(2-D) were used.

One of the practical applications being researched in this area is medical imaging of the brain with magnetoencephalography (MEG). This kind of imaging involves careful measurements of magnetic fields outside the head which yield an accurate 3D-picture of the interior of the head. However, external sources of electromagnetic fields, such as a wristwatch on the subject's arm, will significantly degrade the accuracy of the measurement. Applying source separation techniques on the measured signals can help remove undesired artifacts from the signal.

In electroencephalogram (EEG) and magnetoencephalography (MEG), the interference from muscle activity masks the desired signal from brain activity. BSS, however, can be used to separate the two so an accurate representation of brain activity may be achieved.

Another application is the separation of musical signals. For a stereo mix of relatively simple signals it is now possible to make a fairly accurate separation, although some artifacts remain.

Other applications:

The set of individual source signals, formula_1, is 'mixed' using a matrix, formula_2, to produce a set of 'mixed' signals, formula_3, as follows. Usually, formula_4 is equal to formula_5. If formula_6, then the system of equations is overdetermined and thus can be unmixed using a conventional linear method. If formula_7, the system is underdetermined and a non-linear method must be employed to recover the unmixed signals. The signals themselves can be multidimensional.

formula_8

The above equation is effectively 'inverted' as follows. Blind source separation separates the set of mixed signals, formula_9, through the determination of an 'unmixing' matrix, formula_10, to 'recover' an approximation of the original signals, formula_11.

formula_12

Since the chief difficulty of the problem is its underdetermination, methods for blind source separation generally seek to narrow the set of possible solutions in a way that is unlikely to exclude the desired solution. In one approach, exemplified by principal and independent component analysis, one seeks source signals that are minimally correlated or maximally independent in a probabilistic or information-theoretic sense. A second approach, exemplified by nonnegative matrix factorization, is to impose structural constraints on the source signals. These structural constraints may be derived from a generative model of the signal, but are more commonly heuristics justified by good empirical performance. A common theme in the second approach is to impose some kind of low-complexity constraint on the signal, such as sparsity in some basis for the signal space. This approach can be particularly effective if one requires not the whole signal, but merely its most salient features.

There are different methods of blind signal separation:



S. An, Y. Hua, J. Manton and Z. Fang, "Group decorrelation enhanced subspace method for identifying FIR MIMO channels driven by unknown uncorrelated colored sources," IEEE Transactions on Signal Processing, Vol. 53, No. 12, pp. 4429-4441, Dec 2005.

Y. Hua, S. An and Y. Xiang, "Blind identification of FIR MIMO channels by decorrelating subchannels," IEEE Transactions on Signal Processing, pp. 1143-1155, No. 5, Vol. 51, May 2003.

K. Abed-Meraim, Y. Xiang, J. Manton and Y. Hua, "Blind source separation using second-order cyclostationary statistics," IEEE Trans on Signal Processing, pp. 694-701, No.3, Vol. 49, April 2001.



</doc>
<doc id="28805" url="https://en.wikipedia.org/wiki?curid=28805" title="Stephen Cole Kleene">
Stephen Cole Kleene

Stephen Cole Kleene ( ; January 5, 1909 – January 25, 1994) was an American mathematician. One of the students of Alonzo Church, Kleene, along with Rózsa Péter, Alan Turing, Emil Post, and others, is best known as a founder of the branch of mathematical logic known as recursion theory, which subsequently helped to provide the foundations of theoretical computer science. Kleene's work grounds the study of computable functions. A number of mathematical concepts are named after him: Kleene hierarchy, Kleene algebra, the Kleene star (Kleene closure), Kleene's recursion theorem and the Kleene fixed-point theorem. He also invented regular expressions in 1951 to describe McCulloch-Pitts neural networks, and made significant contributions to the foundations of mathematical intuitionism.

Kleene was awarded the BA degree from Amherst College in 1930. He was awarded the Ph.D. in mathematics from Princeton University in 1934. His thesis, entitled "A Theory of Positive Integers in Formal Logic", was supervised by Alonzo Church. In the 1930s, he did important work on Church's lambda calculus. In 1935, he joined the mathematics department at the University of Wisconsin–Madison, where he spent nearly all of his career. After two years as an instructor, he was appointed assistant professor in 1937.

While a visiting scholar at the Institute for Advanced Study in Princeton, 1939–1940, he laid the foundation for recursion theory, an area that would be his lifelong research interest. In 1941, he returned to Amherst College, where he spent one year as an associate professor of mathematics.

During World War II, Kleene was a lieutenant commander in the United States Navy. He was an instructor of navigation at the U.S. Naval Reserve's Midshipmen's School in New York, and then a project director at the Naval Research Laboratory in Washington, D.C.

In 1946, Kleene returned to Wisconsin, becoming a full professor in 1948 and the Cyrus C. MacDuffee professor of mathematics in 1964. He was chair of the Department of Mathematics and Computer Science, 1962–1963, and Dean of the College of Letters and Science from 1969 to 1974. The latter appointment he took on despite the considerable student unrest of the day, stemming from the Vietnam War. He retired from the University of Wisconsin in 1979. In 1999 the mathematics library at the University of Wisconsin was renamed in his honor.

Kleene's teaching at Wisconsin resulted in three texts in mathematical logic, Kleene (1952, 1967) and Kleene and Vesley (1965). The first two are often cited and still in print. Kleene (1952) wrote alternative proofs to the Gödel's incompleteness theorems that enhanced their canonical status and made them easier to teach and understand. Kleene and Vesley (1965) is the classic American introduction to intuitionist logic and mathematics.

Kleene served as president of the Association for Symbolic Logic, 1956–1958, and of the International Union of History and Philosophy of Science, 1961. The importance of Kleene's work led to Daniel Clement Dennett coining the saying, published in 1978, that "Kleeneness is next to Gödelness." In 1990, he was awarded the National Medal of Science.

Kleene and his wife Nancy Elliott had four children. He had a lifelong devotion to the family farm in Maine. An avid mountain climber, he had a strong interest in nature and the environment, and was active in many conservation causes.

At each conference of the Symposium on Logic in Computer Science the Kleene award, in honour of Stephen Cole Kleene, is given for the best student paper.





</doc>
<doc id="28809" url="https://en.wikipedia.org/wiki?curid=28809" title="Shabbat">
Shabbat

Shabbat ( or ; , "rest" or "cessation"), Shabbos (, Ashkenazi Hebrew and ), or the Sabbath, is Judaism's day of rest and seventh day of the week. On this day, religious Jews, Samaritans and certain Christians (such as Seventh-day Adventists, the Church of God (Seventh-Day) and Seventh Day Baptists) remember the biblical story describing the creation of the heavens and the earth in six days and look forward to a future Messianic Age.

Shabbat observance entails refraining from work activities, often with great rigor, and engaging in restful activities to honour the day. Judaism's traditional position is that unbroken seventh-day Shabbat originated among the Jewish people, as their first and most sacred institution, though some suggest other origins. Variations upon Shabbat are widespread in Judaism and, with adaptations, throughout the Abrahamic and many other religions.

According to "halakha" (Jewish religious law), Shabbat is observed from a few minutes before sunset on Friday evening until the appearance of three stars in the sky on Saturday night. Shabbat is ushered in by lighting candles and reciting a blessing. Traditionally, three festive meals are eaten: The first one is held on Friday evening, the second is traditionally a lunch meal on Saturday and the third being held later in the afternoon. The evening meal and the early afternoon meal typically begin with a blessing called "kiddush" and another blessing recited over two loaves of challah. The third meal does not have the Kiddush recited but all have the two loaves. Shabbat is closed Saturday evening with a "havdalah" blessing. 

Shabbat is a festive day when Jews exercise their freedom from the regular labours of everyday life. It offers an opportunity to contemplate the spiritual aspects of life and to spend time with family.

The word "Shabbat" derives from the Hebrew verb "shavat" (). Although frequently translated as "rest" (noun or verb), another accurate translation of these words is "ceasing [from work]", as resting is not necessarily denoted. The related modern Hebrew word "shevita" (labour strike), has the same implication of active rather than passive abstinence from work. The notion of active cessation from labour is also regarded as more consistent with an omnipotent God's activity on the seventh day of Creation according to Genesis. Other significant connotations are to "shevet" (שֶּׁבֶת) which means sitting or staying, and to "sheva" (שֶׁבַע) meaning seven, as Shabbat is the seventh day of the week; the other days of the week do not have names but called by their ordinals.

Sabbath is given special status as a holy day at the very beginning of the Torah in . It is first commanded after the Exodus from Egypt, in (relating to the cessation of manna) and in (relating to the distance one may travel by foot on the Sabbath), as also in (as the fourth of the Ten Commandments). Sabbath is commanded and commended many more times in the Torah and Tanakh; double the normal number of animal sacrifices are to be offered on the day. Sabbath is also described by the prophets Isaiah, Jeremiah, Ezekiel, Hosea, Amos, and Nehemiah.

The longstanding traditional Jewish position is that unbroken seventh-day Shabbat originated among the Jewish people, as their first and most sacred institution. The origins of Shabbat and a seven-day week are not clear to scholars; the Mosaic tradition claims an origin from the Biblical creation.

Seventh-day Shabbat did not originate with the Egyptians, to whom it was unknown; and other origin theories based on the day of Saturn, or on the planets generally, have also been abandoned.

The first non-Biblical reference to Sabbath is in an ostracon found in excavations at Mesad Hashavyahu, which is dated 630 BCE.

For the Babylonian concept of "sapattu" or "sabattu", see here.

Connection to Sabbath observance has been suggested in the designation of the seventh, fourteenth, nineteenth, twenty-first and twenty-eight days of a lunar month in an Assyrian religious calendar as a 'holy day', also called ‘evil days’ (meaning "unsuitable" for prohibited activities). The prohibitions on these days, spaced seven days apart, include abstaining from chariot riding, and the avoidance of eating meat by the King. On these days officials were prohibited from various activities and common men were forbidden to "make a wish", and at least the 28th was known as a "rest-day". The "Universal Jewish Encyclopedia" advanced a theory of Assyriologists like Friedrich Delitzsch (and of Marcello Craveri) that Shabbat originally arose from the lunar cycle in the Babylonian calendar containing four weeks ending in Sabbath, plus one or two additional unreckoned days per month. The difficulties of this theory include reconciling the differences between an unbroken week and a lunar week, and explaining the absence of texts naming the lunar week as Sabbath in any language.

The Tanakh and siddur describe Shabbat as having three purposes:

Judaism accords Shabbat the status of a joyous holy day. In many ways, Jewish law gives Shabbat the status of being the most important holy day in the Jewish calendar:

Honoring Shabbat ("kavod Shabbat") on Preparation Day (Friday) includes bathing, having a haircut and cleaning and beautifying the home (with flowers, for example).
According to Jewish law, Shabbat starts a few minutes before sunset. Candles are lit at this time. It is customary in many communities to light the candles 18 minutes before sundown ("tosefet Shabbat", though sometimes 36 minutes), and most printed Jewish calendars adhere to this custom. The Kabbalat Shabbat service is a prayer service welcoming the arrival of Shabbat. Before Friday night dinner, it is customary to sing two songs, one "greeting" two Shabbat angels into the house (Shalom Aleichem -Peace Be Upon You) and the other praising the woman of the house for all the work she has done over the past week (Aishes Chayil - Women Of Valour). After blessings over the wine and challah, a festive meal is served. Singing is traditional at Sabbath meals. In modern times, many composers have written sacred music for use during the Kabbalat Shabbat observance, including Robert Strassburg and Samuel Adler.

According to rabbinic literature, God via the Torah commands Jews to "observe" (refrain from forbidden activity) and "remember" (with words, thoughts, and actions) Shabbat, and these two actions are symbolized by the customary two Shabbat candles. Candles are lit usually by the woman of the house (or else by a man who lives alone). Some families light more candles, sometimes in accordance with the number of children.

Shabbat is a day of celebration as well as prayer. It is customary to eat three festive meals: Dinner on Shabbat eve (Friday night), lunch on Shabbat day (Saturday), and a third meal (a "Seudah Shlishit"/"Shalosh Seudot") in the late afternoon (Saturday). It is also customary to wear nice clothing (different from during the week) on Shabbat to honor the day.

On June 13, 2014, Am Yisrael Foundation’s White City Shabbat organization set the Guinness World Record for the world's largest Shabbat dinner. Held at Hangar 11 at Tel Aviv Port, the event was attended by 2,226 people, including Alan Dershowitz, Tel Aviv mayor Ron Huldai, Israeli basketball star Tal Brody and former US Ambassador Michael Oren. The event took almost a year of preparation and involved “60 days of crowd-sourced fundraising, 800 bottles of Israeli wine, 80 bottles of vodka, 50 bottles of whiskey, 2,000 challah rolls, 80 long tables, 1,800 pieces of chicken, 1,000 portions of beef and 250 vegetarian meals.” A total of 2,300 diners signed up for the dinner and another 3,000 were placed on the waiting list.

Many Jews attend synagogue services on Shabbat even if they do not do so during the week. Services are held on Shabbat eve (Friday night), Shabbat morning (Saturday morning), and late Shabbat afternoon (Saturday afternoon).

With the exception of Yom Kippur, which is referred to in the Torah (Lev 23:32) as "Shabbat of Shabbatoth", days of public fasting are postponed or advanced if they coincide with Shabbat. Mourners sitting "shivah" (week of mourning subsequent to the death of a spouse or first-degree relative) outwardly conduct themselves normally for the duration of the day and are forbidden to display public signs of mourning.

Although most Shabbat laws are restrictive, the fourth of the Ten Commandments in Exodus is taken by the Talmud and Maimonides to allude to the "positive" commandments of Shabbat. These include:

"Havdalah" (Hebrew: הַבְדָּלָה, "separation") is a Jewish religious ceremony that marks the symbolic end of Shabbat, and ushers in the new week. At the conclusion of Shabbat at nightfall, after the appearance of three stars in the sky, the "havdalah" blessings are recited over a cup of wine, and with the use of fragrant spices and a candle, usually braided. Some communities delay "havdalah" later into the night in order to prolong Shabbat. There are different customs regarding how much time one should wait after the stars have surfaced until the sabbath technically ends. Some people hold by 72 minutes later and other hold longer and shorter than that.

Jewish law (halakha) prohibits doing any form of "melakhah" (מְלָאכָה, plural "melakhoth") on Shabbat, unless an urgent human or medical need is life-threatening. Though "melakhah" is commonly translated as "work" in English, a better definition is "deliberate activity" or "skill and craftmanship". There are 39 categories of "melakhah":

The 39 "melakhoth" are not so much activities as "categories of activity". For example, while "winnowing" usually refers exclusively to the separation of chaff from grain, and "selecting" refers exclusively to the separation of debris from grain, they refer in the Talmudic sense to any separation of intermixed materials which renders edible that which was inedible. Thus, filtering undrinkable water to make it drinkable falls under this category, as does picking small bones from fish (gefilte fish is one solution to this problem).

The categories of labors prohibited on Shabbat are exegetically derived – on account of Biblical passages juxtaposing Shabbat observance () to making the Tabernacle () – that they are the kinds of work that were necessary for the construction of the Tabernacle. They are not explicitly listed in the Torah; the Mishnah observes that "the laws of Shabbat ... are like mountains hanging by a hair, for they are little Scripture but many laws". Many rabbinic scholars have pointed out that these labors have in common activity that is "creative", or that exercises control or dominion over one's environment.

In addition to the 39 "melakhot", additional activities were prohibited by the rabbis for various reasons.

The term "shomer Shabbat" is used for a person (or organization) who adheres to Shabbat laws consistently. The (strict) observance of the Sabbath is often seen as a benchmark for orthodoxy and indeed has legal bearing on the way a Jew is seen by an orthodox religious court regarding their affiliation to Judaism.

Orthodox and some Conservative authorities rule that turning electric devices on or off is prohibited as a "melakhah"; however, authorities are not in agreement about exactly which one(s). One view is that tiny sparks are created in a switch when the circuit is closed, and this would constitute lighting a fire (category 37). If the appliance is purposed for light or heat (such as an incandescent bulb or electric oven), then the lighting or heating elements may be considered as a type of fire that falls under both lighting a fire (category 37) and cooking (i.e., baking, category 11). Turning lights off would be extinguishing a fire (category 36). Another view is that completing an electrical circuit constitutes building (category 35) and turning it the circuit would be demolishing (category 34). Some schools of thought consider the use of electricity to be forbidden only by rabbinic injunction, rather than a "melakhah".

A common solution to the problem of electricity involves preset timers (Shabbat clocks) for electric appliances, to turn them on and off automatically, with no human intervention on Shabbat itself. Some Conservative authorities reject altogether the arguments for prohibiting the use of electricity. Some Orthodox also hire a "Shabbos goy", a Gentile to perform prohibited tasks (like operating light switches) on Shabbat.

Orthodox and many Conservative authorities completely prohibit the use of automobiles on Shabbat as a violation of multiple categories, including lighting a fire, extinguishing a fire, and transferring between domains (category 39). However, the Conservative movement's Committee on Jewish Law and Standards permits driving to a synagogue on Shabbat, as an emergency measure, on the grounds that if Jews lost contact with synagogue life they would become lost to the Jewish people.

A halakhically authorized Shabbat mode added to a power-operated mobility scooter may be used on the observance of Shabbat for those with walking limitations, often referred to as a Shabbat scooter. It is intended only for individuals whose limited mobility is dependent on a scooter or automobile consistently throughout the week.

Seemingly "forbidden" acts may be performed by modifying technology such that no law is actually violated. In Sabbath mode, a "Sabbath elevator" will stop automatically at every floor, allowing people to step on and off without anyone having to press any buttons, which would normally be needed to work. (Dynamic braking is also disabled if it is normally used, i.e., shunting energy collected from downward travel, and thus the gravitational potential energy of passengers, into a resistor network.) However, many rabbinical authorities consider the use of such elevators by those who are otherwise capable as a violation of Shabbat, with such workarounds being for the benefit of the frail and handicapped and not being in the spirit of the day.

Many observant Jews avoid the prohibition of carrying by use of an eruv. Others make their keys into a tie bar, part of a belt buckle, or a brooch, because a legitimate article of clothing or jewelry may be worn rather than carried. An elastic band with clips on both ends, and with keys placed between them as integral links, may be considered a belt.

Shabbat lamps have been developed to allow a light in a room to be turned on or off at will while the electricity remains on. A special mechanism blocks out the light when the off position is desired without violating Shabbat.

The Shabbos App is a proposed Android app claimed by its creators to enable Orthodox Jews, and all Jewish Sabbath-observers, to use a smartphone to text on the Jewish Sabbath. It has met with resistance from some authorities.

In the event that a human life is in danger (pikuach nefesh), a Jew is not only allowed, but required, to violate any halakhic law that stands in the way of saving that person (excluding murder, idolatry, and forbidden sexual acts). The concept of life being in danger is interpreted broadly: for example, it is mandated that one violate Shabbat to bring a woman in active labor to a hospital. Lesser rabbinic restrictions are often violated under much less urgent circumstances (a patient who is ill but not critically so).

Various other legal principles closely delineate which activities constitute desecration of Shabbat. Examples of these include the principle of "shinui" ("change" or "deviation"): A violation is not regarded as severe if the prohibited act was performed in a way that would be considered abnormal on a weekday. Examples include writing with one's nondominant hand, according to many rabbinic authorities. This legal principle operates "bedi'avad" ("ex post facto") and does not cause a forbidden activity to be permitted barring extenuating circumstances.

Generally, adherents of Reform and Reconstructionist Judaism believe that the individual Jew determines whether to follow Shabbat prohibitions or not. For example, some Jews might find activities, such as writing or cooking for leisure, to be enjoyable enhancements to Shabbat and its holiness, and therefore may encourage such practices. Many Reform Jews believe that what constitutes "work" is different for each person, and that only what the person considers "work" is forbidden. The radical Reform rabbi Samuel Holdheim advocated moving Sabbath to Sunday for many no longer observed it, a step taken by dozens of congregations in the United States in late 19th century.

More rabbinically traditional Reform and Reconstructionist Jews believe that these "halakhoth" in general may be valid, but that it is up to each individual to decide how and when to apply them. A small fraction of Jews in the Progressive Jewish community accept these laws much the same way as Orthodox Jews.

The Talmud, especially in tractate Shabbat, defines rituals and activities to both "remember" and "keep" the Sabbath and to sanctify it at home and in the synagogue. In addition to refraining from creative work, the sanctification of the day through blessings over wine, the preparation of special Sabbath meals, and engaging in prayer and Torah study were required as an active part of Shabbat observance to promote intellectual activity and spiritual regeneration on the day of rest from physical creation. The Talmud states that the best food should be prepared for the Sabbath, for "one who delights in the Sabbath is granted their heart's desires" (BT, Shabbat 118a-b).

All Jewish denominations encourage the following activities on Shabbat:

Special Shabbatot are the Shabbatot that precede important Jewish holidays: e.g., "Shabbat HaGadol" (Shabbat preceding Pesach), "Shabbat Zachor" (Shabbat preceding Purim), and "Shabbat Shuvah" (Shabbat between Rosh Hashanah and Yom Kippur).

Most Christians do not observe Saturday Sabbath, but instead observe a weekly day of worship on Sunday, which is often called the "Lord's Day". Several Christian denominations, such as the Seventh-day Adventist Church, the Church of God (7th Day), the Seventh Day Baptists, and many others, observe seventh-day Sabbath. This observance is celebrated from Friday sunset to Saturday sunset.

The principle of weekly Sabbath also exists in other beliefs. Examples include the Babylonian calendar, the Buddhist "uposatha", and the Unification Church's Ahn Shi Il.



</doc>
<doc id="28810" url="https://en.wikipedia.org/wiki?curid=28810" title="Saki">
Saki

Hector Hugh Munro (18 December 1870 – 14 November 1916), better known by the pen name Saki and also frequently as H. H. Munro, was a British writer whose witty, mischievous and sometimes macabre stories satirize Edwardian society and culture. He is considered by English teachers and scholars as a master of the short story, and often compared to O. Henry and Dorothy Parker. Influenced by Oscar Wilde, Lewis Carroll and Rudyard Kipling, he himself influenced A. A. Milne, Noël Coward and P. G. Wodehouse.

Besides his short stories (which were first published in newspapers, as was customary at the time, and then collected into several volumes), he wrote a full-length play, "The Watched Pot", in collaboration with Charles Maude; two one-act plays; a historical study, "The Rise of the Russian Empire" (the only book published under his own name); a short novel, "The Unbearable Bassington"; the episodic "The Westminster Alice" (a parliamentary parody of "Alice in Wonderland"); and "When William Came", subtitled "A Story of London Under the Hohenzollerns", a fantasy about a future German invasion and occupation of Britain.

Hector Hugh Munro was born in Akyab (now Sittwe), British Burma, which was then part of British India. Saki was the son of Charles Augustus Munro, an Inspector General for the Indian Imperial Police, and his wife, Mary Frances Mercer (1843–1872), the daughter of Rear Admiral Samuel Mercer. Her nephew Cecil William Mercer became a novelist under the name Dornford Yates.

In 1872, on a home visit to England, Mary Munro was charged by a cow, and the shock caused her to miscarry. She never recovered and soon died.

After his wife's death Charles Munro sent his children, including two-year-old Hector, home to England. The children were sent to Broadgate Villa, in Pilton near Barnstaple, North Devon, to be raised by their grandmother and paternal maiden aunts, Charlotte and Augusta, in a strict and puritanical household. It is said that his aunts were most likely models for some of his characters, notably the aunt in "The Lumber Room" and the guardian in "Sredni Vashtar": Munro's sister Ethel said that the aunt in "The Lumber Room" was an almost perfect portrait of Aunt Augusta. Munro and his siblings led slightly insular lives during their early years and were educated by governesses. At the age of 12 the young Hector Munro was educated at Pencarwick School in Exmouth and then as a boarder at Bedford School.

In 1887, after his retirement, his father returned from Burma and embarked upon a series of European travels with Hector and his siblings.

Hector followed his father in 1893 into the Indian Imperial Police and was posted to Burma, but successive bouts of fever caused his return home after only fifteen months.

In 1896, he decided to move to London to make a living as a writer.

Munro started his writing career as a journalist for newspapers such as "The Westminster Gazette", the "Daily Express", "The Morning Post", and magazines such as the "Bystander" and "Outlook". His first book "The Rise of the Russian Empire", a historical study modelled upon Edward Gibbon's "The Decline and Fall of the Roman Empire", appeared in 1900, under his real name, but proved to be something of a false start.

While writing "The Rise of the Russian Empire", he made his first foray into short story writing and published a piece called 'Dogged' in "St Paul's" in February 1899. He then moved into the world of political satire in 1900 with a collaboration with Francis Carruthers Gould entitled "Alice in Westminster". Gould produced the sketches, and Munro wrote the text accompanying them, using the pen-name "Saki" for the first time. The series lampooned political figures of the day ('Alice in Downing Street' begins with the memorable line, '"Have you ever seen an Ineptitude?"' - referring to a zoomorphised Arthur Balfour), and was published in the Liberal "Westminster Gazette".

In 1902 he moved to "The Morning Post", described as one of the 'organs of intransigence' by Stephen Koss, to work as a foreign correspondent, first in the Balkans, and then in Russia, where he was witness to the 1905 revolution in St Petersburg. He then went on to Paris, before returning to London in 1908, where 'the agreeable life of a man of letters with a brilliant reputation awaited him.' In the intervening period "Reginald" had been published in 1904, the stories having first appeared in "The Westminster Gazette", and all this time he was writing sketches for "The Morning Post", the "Bystander," and "The Westminster Gazette". He kept a place in Mortimer Street, wrote, played bridge at the Cocoa Tree Club, and lived simply. "Reginald in Russia" appeared in 1910, and "The Chronicles of Clovis" was published in 1911, and "Beasts and Super-Beasts" in 1914, along with many other short stories that appeared in newspapers not published in collections in his lifetime.

He also produced two novels, "The Unbearable Bassington" (1912) and "When William Came" (1913).

At the start of the First World War Munro was 43 and officially over-age to enlist, but he refused a commission and joined the 2nd King Edward's Horse as an ordinary trooper. He later transferred to the 22nd Battalion of the Royal Fusiliers, in which he rose to the rank of lance sergeant. More than once he returned to the battlefield when officially too sick or injured. In November 1916 he was sheltering in a shell crater near Beaumont-Hamel, France, during the Battle of the Ancre, when he was killed by a German sniper. According to several sources, his last words were "Put that bloody cigarette out!"

Munro has no known grave. He is commemorated on Pier and Face 8C 9A and 16A of the Thiepval Memorial.

In 2003 English Heritage marked Munro's flat at 97 Mortimer Street, in Fitzrovia with a blue plaque.

After his death his sister Ethel destroyed most of his papers and wrote her own account of their childhood, which appeared at the beginning of "The Square Egg and Other Sketches" (1924). Rothay Reynolds, a close friend, wrote a relatively lengthy memoir in "The Toys of Peace" (1919), but aside from this, the only other biographies of Munro are "Saki: A Life of Hector Hugh Munro" (1982) by A. J. Langguth, and "The Unbearable Saki" (2007) by Sandie Byrne. All later biographies have had to draw heavily upon Ethel's account of her brother's life.

Munro was homosexual at a time when in Britain sexual activity between men was a crime. The Cleveland Street scandal (1889), followed by the downfall of Oscar Wilde (1895), meant "that side of [Munro's] life had to be secret".

The pen name "Saki" is most commonly assumed to be a reference to the cupbearer in the "Rubáiyát of Omar Khayyam." Both Rothay Reynolds and Ethel Munro confirm this. This reference is stated as fact by Emlyn Williams in his introduction to a Saki anthology published in 1978. 

"Saki" may also be a reference to the South American Saki monkey, which at least two commentators, Tom Sharpe and Will Self, have connected to the "small, long-tailed monkey from the Western Hemisphere" that is a central character in .

Much of Saki's work contrasts the conventions and hypocrisies of Edwardian England with the ruthless but straightforward life-and-death struggles of nature. Writing in "The Guardian" to mark the centenary of Saki's death, Stephen Moss noted, "In many of his stories, stuffy authority figures are set against forces of nature – polecats, hyenas, tigers. Even if they are not eaten, the humans rarely have the best of it".

"The Interlopers" is a story about two men, Georg Znaeym and Ulrich von Gradwitz, whose families have fought over a forest in the eastern Carpathian Mountains for generations. Ulrich's family legally owns the land, and so considers Georg an interloper when he hunts in the forest. But Georg, believing that the forest rightfully belongs to his family, hunts there often and believes that Ulrich is the real interloper for trying to stop him. One winter night, Ulrich catches Georg hunting in the forest. Neither man can shoot the other without warning, as they would soil their family's honour, so they hesitate to acknowledge one another. In an "act of God", a tree branch suddenly falls on each of them, trapping them both under a log. Gradually they realize the futility of their quarrel, become friends and end the feud. They then call out for their men's assistance and, after a brief period, Ulrich makes out nine or ten figures approaching over a hill. The story ends with Ulrich's realization that the approaching figures on the hill are actually hungry wolves. The wolves, it seems, are the true owners of the forest, while both humans are interlopers.

"Gabriel-Ernest" starts with a warning: "There is a wild beast in your woods …" Gabriel, a naked boy sunbathing by the river, is "adopted" by well-meaning Townspeople. Lovely and charming, but also rather vague and distant, he seems bemused by his "benefactors." Asked how he managed by himself in the woods, he replies that he hunts "on four legs," which they take to mean that he has a dog. The climax comes when a small child disappears while walking home from Sunday school. A pursuit ensues, but Gabriel and the child disappear near a river. The only items found are Gabriel's clothes, and the two are never seen again. The story includes many of the author's favourite themes: good intentions gone awry, the banality of polite society, the attraction of the sinister, and the allure of the wild and the forbidden. There is also a recognition of basic decency, upheld when the story's protagonist 'flatly refuses' to subscribe to a Gabriel-Ernst memorial, for his supposedly gallant attempt to save a drowning child, and drowning himself, as well. He realises that Gabriel-Ernst was actually a werewolf, who had eaten the child, then run off.

At a railway station an arrogant and overbearing woman, Mrs Quabarl, mistakes the mischievous Lady Carlotta, who has been inadvertently left behind by a train, for the governess, Miss Hope, whom she has been expecting, Miss Hope having erred about the date of her arrival. Lady Carlotta decides not to correct the mistake, acknowledges herself as Miss Hope, a proponent of "the Schartz-Metterklume method" of making children understand history by acting it out themselves, and chooses the Rape of the Sabine Women (exemplified by a washerwoman's two girls) as the first lesson. After creating chaos for two days, she departs, explaining that her delayed luggage will include a leopard cub.

Preferring not to give her young sons toy soldiers or guns, and having taken away their toy depicting the Siege of Adrianople, Eleanor instructs her brother Harvey to give them innovative "peace toys" as an Easter present. When the packages are opened young Bertie shouts "It's a fort!" and is disappointed when his uncle replies "It's a municipal dustbin." The boys are initially baffled as to how to obtain any enjoyment from models of a school of art and a public library, or from little figures of John Stuart Mill, Felicia Hemans and Sir John Herschel. Youthful inventiveness finds a way, however, as the boys combine their history lessons on Louis XIV with a lurid and violent play-story about the invasion of Britain and the storming of the Young Women's Christian Association. The end of the story has Harvey reporting failure to Eleanor, explaining "We have begun too late.", not realising he was doomed to failure whenever he had begun.

An aunt is travelling by train with her two nieces and a nephew. The children are inquisitive and mischievous. A bachelor is also travelling in the same compartment. The aunt starts telling a moralistic story, but is unable to satisfy the children's curiosity. The bachelor butts in and tells a story in which a "good" person ends up being devoured by a wolf, to the children's delight. The bachelor is amused by the thought that in the future the children will embarrass their guardian by begging to be told "an improper story."

Framton Nuttel, a nervous man, has come to stay in the country for his health. His sister, who thinks he should socialise while he is there, has given him letters of introduction to families in the neighbourhood whom she got to know when she was staying there a few years previously. Framton goes to visit Mrs Sappleton and, while he is waiting for her to come down, is entertained by her fifteen-year-old, witty niece. The niece tells him that the French window is kept open, even though it is October, because Mrs Sappleton believes that her husband and her brothers, who were drowned in a bog three years before, will come back one day. When Mrs Sappleton comes down she talks about her husband and her brothers, and how they are going to come back from shooting soon, and Framton, believing that she is deranged, tries to distract her by talking about his health. Then, to his horror, Mrs Sappleton points out that her husband and her brothers are coming, and he sees them walking towards the window with their dog. He thinks he is seeing ghosts and runs away. Mrs Sappleton cannot understand why he has run away and, when her husband and her brothers come in, she tells them about the odd man who has just left. The niece explains that Framton Nuttel ran away because of the spaniel: he is afraid of dogs since he was hunted by a pack of stray dogs in India and had to spend a night in the newly dug grave with creatures grinning and foaming just above him. The last line summarizes the story, saying of the niece, "Romance at short notice was her speciality."

Saki's recurring hero Clovis Sangrail, a clever, mischievous young man, overhears the complacent middle-aged Huddle complaining of his own addiction to routine and aversion to change. Huddle's friend makes the wry suggestion that he needs an "unrest-cure" (the opposite of a rest cure), to be performed, if possible, in the home. Clovis takes it upon himself to "help" the man and his sister by involving them in an invented outrage that will be a "blot on the twentieth century".
A baroness tells Clovis a story about a hyena that she and her friend Constance encountered while out fox hunting. Later, the hyena follows them, stopping briefly to eat a gypsy child. Shortly after this, the hyena is killed by a motorcar. The baroness immediately claims the corpse as her beloved dog Esmé, and the guilty owner of the car gets his chauffeur to bury the animal and later sends her a diamond brooch to make up for her loss.

A sickly child named Conradin is raised by his aunt and guardian, Mrs De Ropp, who "would never... have confessed to herself that she disliked Conradin, though she might have been dimly aware that thwarting him 'for his good' was a duty which she did not find particularly irksome". Conradin rebels against his aunt and her choking authority. He invents a religion in which his polecat ferret is imagined as a vengeful deity, and Conradin prays that "Sredni Vashtar" will deliver retribution upon De Ropp. When De Ropp attempts to dispose of the animal, it attacks and kills her. The entire household is shocked and alarmed; Conradin calmly butters another piece of toast.

At a country-house party, one guest, Cornelius Appin, announces to the others that he has perfected a procedure for teaching animals human speech. He demonstrates this on his host's cat, Tobermory. Soon it is clear that animals are permitted to view and listen to many private things on the assumption that they will remain silent, such as the host Sir Wilfred's commentary on one guest's intelligence and the hope that she will buy his car, or the implied sexual activities of some of the other guests. The guests are angered, especially when Tobermory runs away to pursue a rival cat, but plans to poison him fail when Tobermory is instead killed by the rival cat. "An archangel ecstatically proclaiming the Millennium, and then finding that it clashed unpardonably with Henley and would have to be indefinitely postponed, could hardly have felt more crestfallen than Cornelius Appin at the reception of his wonderful achievement." Appin is killed shortly afterwards when attempting to teach an elephant in a zoo in Dresden to speak German.
His fellow house party guest, Clovis Sangrail (Saki's recurring hero), remarks callously that if he was teaching "the poor beast" irregular German verbs, he deserved no pity.

Tom Yorkfield, a farmer, receives a visit from his half-brother Laurence. Tom has no great liking for Laurence or respect for his profession as a painter of animals. Tom shows Laurence his prize bull and expects him to be impressed, but Laurence nonchalantly tells Tom that he has sold a painting of a different bull, which Tom has seen and does not like, for three hundred pounds. Tom is angry that a mere picture of a bull should be worth more than his real bull. This and Laurence's condescending attitude give him the urge to strike him. Laurence, running away across the field, is attacked by the bull, but is saved by Tom from serious injury. Tom, looking after Laurence as he recovers, feels no more rancour because he knows that, however valuable Laurence's painting might be, only a real bull like his can attack someone.

This is a "rediscovered" short story that was previously cited as a play. A house party is beset by a fire in the middle of the night in the east wing of the house. Begged by their hostess to save "my poor darling Eva – Eva of the golden hair," Lucien demurs, on the grounds that he has never even met her. It is only on discovering that Eva is not a flesh-and-blood daughter but Mrs Gramplain's painting of the daughter she wished that she had had, and which she has faithfully updated with the passing years, that Lucien declares a willingness to forfeit his life to rescue her, since "death in this case is more beautiful," a sentiment endorsed by the Major. As the two men disappear into the blaze, Mrs Gramplain recollects that she "sent Eva to Exeter to be cleaned". The two men have lost their lives for nothing.


Posthumous publications:

The 5th broadcast of Orson Welles' series for CBS Radio, "The Mercury Theatre on the Air", from 8 August 1938, dramatizes three short stories rather than one long story. The second of the three stories is "The Open Window."

"The Open Window" is also adapted (by John Allen) in the 1962 Golden Records release "Alfred Hitchcock Presents: Ghost Stories for Young People", a record album of six ghost stories for children.

A dramatisation of "The Schartz-Metterklume Method" was an episode in the series "Alfred Hitchcock Presents" in 1960.

"Saki: The Improper Stories of H. H. Munro" (a reference to the ending of "The Story Teller") was an eight-part series produced by Philip Mackie for Granada Television in 1962. Actors involved included Mark Burns as Clovis, Fenella Fielding as Mary Drakmanton, Heather Chasen as Agnes Huddle, Richard Vernon as the Major, Rosamund Greenwood as Veronique and Martita Hunt as Lady Bastable.

A dramatisation of "The Open Window" was an episode in the series "Tales of the Unexpected" in 1984.

"Who Killed Mrs De Ropp?", a BBC TV production in 2007, starring Ben Daniels and Gemma Jones, showcased three of Saki's short stories, "The Storyteller", "The Lumber Room" and "Sredni Vashtar".





</doc>
<doc id="28811" url="https://en.wikipedia.org/wiki?curid=28811" title="Static program analysis">
Static program analysis

Static program analysis is the analysis of computer software that is performed without actually executing programs, in contrast with dynamic analysis, which is analysis performed on programs while they are executing. In most cases the analysis is performed on some version of the source code, and in the other cases, some form of the object code.

The term is usually applied to the analysis performed by an automated tool, with human analysis being called program understanding, program comprehension, or code review. Software inspections and software walkthroughs are also used in the latter case.

The sophistication of the analysis performed by tools varies from those that only consider the behaviour of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behaviour matches that of its specification).

Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called "software quality objectives".

A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example, the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:


A study in 2012 by VDC Research reports that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.

In the application security industry the name "Static Application Security Testing" (SAST) is also used. SAST is an important part of Security Development Lifecycles (SDLs) such as the SDL defined by Microsoft and a common practice in software companies.

The OMG (Object Management Group) published a study regarding the types of software analysis required for software quality measurement and assessment. This document on "How to Deliver Resilient, Secure, Efficient, and Easily Changed IT Systems in Line with CISQ Recommendations" describes three levels of software analysis.

A further level of software analysis can be defined.


Formal methods is the term applied to the analysis of software (and computer hardware) whose results are obtained purely through the use of rigorous mathematical methods. The mathematical techniques used include denotational semantics, axiomatic semantics, operational semantics, and abstract interpretation.

By a straightforward reduction to the halting problem, it is possible to prove that (for any Turing complete language), finding all possible run-time errors in an arbitrary program (or more generally any kind of violation of a specification on the final result of a program) is undecidable: there is no mechanical method that can always answer truthfully whether an arbitrary program may or may not exhibit runtime errors. This result dates from the works of Church, Gödel and Turing in the 1930s (see: Halting problem and Rice's theorem). As with many undecidable questions, one can still attempt to give useful approximate solutions.

Some of the implementation techniques of formal static analysis include:

Data-driven static analysis uses large amounts of code to infer coding rules. For instance, one can use all Java open-source packages on GitHub to learn a good analysis strategy. The rule inference can use machine learning techniques. For instance, it has been shown that when one deviates too much in the way one uses an object-oriented API, it is likely to be a bug. It is also possible to learn from a large amount of past fixes and warnings.





</doc>
<doc id="28812" url="https://en.wikipedia.org/wiki?curid=28812" title="Samuel Mudd">
Samuel Mudd

Samuel Alexander Mudd Sr. (December 20, 1833 – January 10, 1883) was an American physician who was imprisoned for conspiring with John Wilkes Booth in the assassination of President Abraham Lincoln.

Mudd worked as a doctor and tobacco farmer in Southern Maryland. The Civil War seriously damaged his business, especially when Maryland abolished slavery in 1864. That year, he first met Booth, who was planning to kidnap Lincoln, and Mudd was seen in company with three of the conspirators. However, his part in the plot, if any, remains unclear.

After mortally wounding Lincoln on April 14, 1865, Booth rode with conspirator David Herold to Mudd's home in the early hours of April 15 for surgery on his fractured leg before he crossed into Virginia. Sometime that day, Mudd must have learned of the assassination but did not report Booth's visit to the authorities for another 24 hours. That appeared to link him to the crime, as did his various changes of story under interrogation. A military commission found him guilty of aiding and conspiring in a murder, and he was sentenced to life imprisonment, escaping the death penalty by a single vote.

Mudd was pardoned by President Andrew Johnson and released from prison in 1869. Despite repeated attempts by family members and others to have it expunged, his conviction has not been overturned.

Born in Charles County, Maryland, Mudd was the fourth of 10 children of Henry Lowe and Sarah Ann Reeves Mudd. He grew up on Oak Hill, his father's tobacco plantation of several hundred acres, southeast of Washington, DC, and worked by 89 slaves.

At 15, after several years of home tutoring, Mudd went off to boarding school at St. John's Literary Institute, now known as Saint John's Catholic Prep School in Frederick, Maryland. Two years later, he enrolled at Georgetown College in Washington, DC. He then studied medicine at the University of Maryland, Baltimore, writing his thesis on dysentery.

Upon graduation in 1856, Mudd returned to Charles County to practice medicine, marrying his childhood sweetheart, Sarah Frances (Frankie) Dyer Mudd one year later.

As a wedding present, Mudd's father gave the couple of his best farmland and a new house named St. Catherine. While the house was under construction, the young Mudds lived with Frankie's bachelor brother, Jeremiah Dyer, finally moving into their new home in 1859. They had nine children in all: four before Mudd's arrest and five after his release from prison. To supplement his income from his medical practice, Mudd became a small scale tobacco grower, using five slaves according to the 1860 census. Mudd believed that slavery was divinely ordained and wrote a letter to the theologian Orestes Brownson to that effect.

With the advent of the American Civil War in 1861, the Southern Maryland slave system and the economy that it supported rapidly began to collapse. In 1863, the Union Army established Camp Stanton, just from the Mudd farm to enlist black freedmen and runaway slaves. Six regiments totaling over 8,700 black soldiers, many from Southern Maryland, were trained there. In 1864, Maryland, which was exempt from Lincoln's 1863 Emancipation Proclamation, abolished slavery, making it difficult for growers like Mudd to operate their plantations. As a result, Mudd considered selling his farm and depending on his medical practice. As Mudd pondered his alternatives, he was introduced to someone who said he might be interested in buying his property, a 26-year-old actor, John Wilkes Booth.

Many historians agree that President Abraham Lincoln's future assassin, John Wilkes Booth, visited Bryantown, Maryland, in November and December 1864, claiming to look for real estate investments. Bryantown is about from Washington, DC, and about from Mudd's farm. The real estate story was merely a cover; Booth's true purpose was to plan an escape route as part of a plan to kidnap Lincoln. Booth believed the federal government would ransom Lincoln by releasing a large number of Confederate prisoners of war.
Historians agree that Booth met Mudd at St. Mary's Catholic Church in Bryantown during one of those visits, probably in November. Booth visited Mudd at his farm the next day, and stayed there overnight. The following day, Booth purchased a horse from Mudd's neighbor and returned to Washington. Some historians believe that Booth used his visit to Bryantown to recruit Mudd to his kidnapping plot, but others believe that Mudd would have had no interest in such a scheme.

A short time later, on December 23, 1864, Mudd went to Washington where he met Booth again. Some historians believe the meeting had been arranged, but others disagree. The two men, as well as John Surratt, Jr., and Louis J. Weichmann, had a conversation and drinks together, first at Booth's hotel and later at Mudd's.

According to a statement made by associated conspirator George Atzerodt, found long after his death and taken down while he was in federal custody on May 1, 1865, Mudd knew in advance about Booth's plans; Atzerodt was sure the doctor knew, he said, because Booth had "sent (as he told me) liquors & provisions... about two weeks before the murder to Dr. Mudd's."

Although that is true, some historians believe that there may be other reasons behind Mudd's relationship to Booth. The trial brought forth many theories of Mudd's involvement in the assassination of Lincoln. One theory posits that Mudd was involved in a completely different conspiracy to gain an upper hand for the southern states. Prior to the assassination of Lincoln, Booth originally intended to kidnap the president and hold him and other political affiliates of the Union for a large sum of money. The plan was in effect until the night of the assassination, when Booth met up with Atzerodt, David Herold and Lewis Powell (who gave his name as Lewis Payne when he was arrested at Mary Surratt's house days after the murder) and disclosed the plot to assassinate the president. Following the assassination, Powell came forth by stating that Booth had not told him until the meeting and that the other men did not know about the plot until the night of the assassination. That supports the theory that Mudd may have been an accomplice to the plot to kidnap the president but not a conspirator to the assassination.

After Booth shot Lincoln on April 14, 1865, he broke his left fibula while fleeing Ford's Theater. Booth met up with Herold and both made for Virginia, via southern Maryland. They stopped at Mudd's house around 4 a.m. on April 15. Mudd splinted Booth's leg, and gave him a shoe to wear. He also arranged for a carpenter, John Best, to make a pair of crutches for Booth. Booth paid Mudd $25 in greenbacks for his services. He and Herold spent between twelve and fifteen hours at Mudd's house. They slept in the front bedroom on the second floor. It is unclear whether Mudd had yet been informed that Booth had killed Lincoln.

Mudd went to Bryantown during the day on April 15 to run errands; if he did not already know the news of the assassination from Booth, he certainly learned of it on the trip. He returned home that evening, and accounts differ as to whether Booth and Herold had already left, whether Mudd met them as they were leaving, and whether they left at Mudd's urging and with his assistance.

It is certain that Mudd did not immediately contact the authorities. When questioned, he stated that he had not wanted to leave his family alone in the house in case the assassins returned and found him absent and his family unprotected. He waited until Mass the following day, Easter Sunday, when he asked his second cousin, Dr. George Mudd, a resident of Bryantown, to notify the 13th New York Cavalry in Bryantown, under the command of Lieutenant David Dana. The delay in contacting the authorities drew suspicion and was a significant factor in tying Mudd to the conspiracy.

During his initial investigative interview on April 18, Mudd stated that he had never seen either of the parties before. In his sworn statement of April 22, he told about Booth's visit to Bryantown in November 1864
but then said, "I have never seen Booth since that time to my knowledge until last Saturday morning." He hid his meeting with Booth in Washington in December 1864. In prison, Mudd admitted the Washington meeting and said he ran into Booth by chance during a Christmas shopping trip. Mudd's failure to mention the meeting in his interview with detectives was a big mistake. When Weichmann later told the authorities of the meeting, they realized that Mudd had misled them and immediately began to treat him as a suspect, rather than a witness.

During the conspiracy trial, Lieutenant Alexander Lovett testified, "On Friday, the 21st of April, I went to Mudd's again, for the purpose of arresting him. When he found we were going to search the house, he said something to his wife, and she went upstairs and brought down a boot. Mudd said he had cut it off the man's leg. I turned down the top of the boot, and saw the name 'J. Wilkes' written in it."

After Booth's death on April 26, 1865, Mudd was arrested and charged with conspiracy to murder Lincoln. Representative Frederick Stone was the senior defense counsel for Mudd.

On May 1, 1865, President Johnson ordered the formation of a nine-man military commission to try the conspirators. Mudd was represented by General Thomas Ewing, Jr.. The trial began on May 10, 1865. Mary Surratt, Lewis Powell, George Atzerodt, David Herold, Samuel Mudd, Michael O'Laughlen, Edmund Spangler and Samuel Arnold were all charged with conspiring to murder Lincoln. The prosecution called 366 witnesses.
The defense sought to prove that Mudd was a loyal citizen, citing his self-description as a "Union man" and asserting that he was "a deeply religious man, devoted to family, and a kind master to his slaves." The prosecution presented witnesses who testified that he had shot one of his slaves in the leg and threatened to send others to Richmond, Virginia, to assist in the construction of Confederate defenses. The prosecution also contended that he had been a member of a Confederate communications distribution agency and had sheltered Confederate soldiers on his plantation.

On June 29, 1865, Mudd was found guilty with the others. The testimony of Louis J. Weichmann was crucial in obtaining the convictions. According to historian Edward Steers, the testimony presented by former slaves was also crucial, but it faded from public memory. Mudd escaped the death penalty by one vote and was sentenced to life imprisonment. Four of the defendants (Surratt, Powell, Atzerodt and Herold) were hanged at the Old Penitentiary at the Washington Arsenal on July 7, 1865.

Mudd, O'Laughlen, Arnold, and Spangler were imprisoned at Fort Jefferson, in the Dry Tortugas, about west of Key West, Florida. The fort housed Union Army deserters and held about 600 prisoners when Mudd and the others arrived. Prisoners lived on the second tier of the fort, in unfinished, open-air gun rooms called casemates. Mudd and his three companions lived in the casemate directly above the fort's main entrance, called the sally port.

In September 1865, two months after Mudd arrived, the control of Fort Jefferson was transferred from the 161st New York Volunteer Infantry Regiment to the 82nd US Colored Troops. On September 25, 1865, he attempted to escape from Fort Jefferson by stowing away on the transport "Thomas A. Scott".

He was quickly discovered and placed, along with Arnold, O'Laughlen, Spangler, and George St. Leger Grenfell, in a large empty ground-level gunroom that soldiers referred to as "the dungeon." The men were let out of the dungeon every working day for 12 hours and were required to wear leg irons. However, following a December 22 letter from his wife to President Johnson, the War Department ordered the discontinuance of the shackles and the move to better quarters, which was accomplished by January.

After three months in the dungeon, Mudd and the others were returned to the general prison population. However, because of his attempted escape, Mudd lost his privilege of working in the prison hospital and was assigned to work in the prison carpentry shop with Spangler.
There was an outbreak of yellow fever in the fall of 1867 at the fort. O'Laughlen eventually died of it on September 23. The prison doctor died, and Mudd agreed to take over the position. He was able to help stem the spread of the disease. The soldiers in the fort wrote a petition to Johnson in October 1867 stating the degree of Mudd's assistance: "He inspired the hopeless with courage and by his constant presence in the midst of danger and infection... [Many] doubtless owe their lives to the care and treatment they received at his hands." Probably as a reward for his work in the yellow fever epidemic, Mudd was reassigned from the carpentry shop to a clerical job in the Provost Marshal's office, where he remained until his pardon.

The influence of his defense attorney, Thomas Ewing Jr., who was also influential in the President's administration, was one reason why Mudd was pardoned by Johnson on February 8, 1869. He was released from prison on March 8, 1869, and returned to his home in Maryland on March 20, 1869. On March 2, 1869, three weeks after he pardoned Mudd, Johnson also pardoned Spangler and Arnold.
When Mudd returned home, well-wishing friends and strangers, as well as inquiring newspaper reporters, besieged him. Mudd was very reluctant to talk to the press because he felt it had misquoted him in the past. He gave one interview to the "New York Herald" after his release but immediately regretted it and complained that the article had several factual errors and misrepresented his work during the yellow fever epidemic. On the whole, though, Mudd continued to enjoy the support of his friends and neighbors. He resumed his medical practice and slowly brought the family farm back to productivity.

In 1873, Spangler traveled to the Mudd farm, where Mudd and his wife welcomed him. Spangler lived with the Mudd family for about 18 months, earning his keep by doing carpentry, gardening, and other farm chores, until his death on February 7, 1875.

Mudd always had an interest in politics. In prison, he learned about political happenings by reading the newspapers that were sent to him. After his release, he again became active in community affairs. In 1874, he was elected chief officer of the local farmers association, the Bryantown Grange. In 1876, he was elected Vice President of the local Democratic Tilden-Hendricks presidential election committee. Tilden lost that year to Republican Rutherford B. Hayes in a hotly-disputed election. The next year, Mudd ran as a Democratic candidate for the Maryland House of Delegates but was defeated by the popular Republican William Mitchell.

Mudd's ninth child, Mary Eleanor "Nettie" Mudd, was born in 1878.

In 1880, the Port Tobacco Times reported that Mudd's barn which contained almost 8000 lb. of tobacco, two horses, a wagon, and farm implements was destroyed by fire.

Mudd was just 49 years old when he died of pneumonia, on January 10, 1883, and was buried in the cemetery at St. Mary's Catholic Church in Bryantown, the same church in which he once met Booth.

The degree of Samuel Mudd's culpability has remained controversial ever since. Some, including Mudd's grandson Richard Mudd, claimed that Mudd was innocent of any wrongdoing and that he had merely been imprisoned for treating a man who came to his house late at night with a fractured leg. Over a century after the assassination, Presidents Jimmy Carter and Ronald Reagan both wrote letters to Richard Mudd in which they agreed that his grandfather had committed no crime. However, others, including authors Edward Steers, Jr. and James Swanson, assert evidence that Samuel Mudd visited Booth three times in the months before the failed kidnapping attempt. The first time was November 1864 when Booth, who was looking for help in his kidnapping plot, was directed to Mudd by agents of the Confederate Secret Service. In December, Booth again met with Mudd and spent the night at his farm. Later that December, Mudd went to Washington and introduced Booth to a Confederate agent whom he knew: John Surratt. Additionally, George Atzerodt testified that Booth sent supplies to Mudd's house in preparation for the kidnapping plan. Mudd lied to the authorities who came to his house after the assassination, claiming that he did not recognize the man who showed up on his doorstep in need of treatment and giving them false information about where Booth and Herold went. He also hid the monogrammed boot that he had cut off Booth's injured leg behind a panel in his attic, but the thorough search of Mudd's house soon revealed this further piece of evidence which was later used against him. One hypothesis is that Dr. Mudd was originally complicit in the kidnapping plot, likely as the person who the conspirators would have turned to for medical treatment in case Lincoln was injured, and that Booth thus remembered the doctor and went to his house to get help in the early hours of April 15.
Mudd's grandson, Richard Mudd, unsuccessfully tried to clear his grandfather's name from the stigma of aiding Booth. In 1951, he published "The Mudd Family of the United States", an encyclopedic two-volume history of the Mudd family in America, beginning with Thomas Mudd, who arrived from England in 1665. A second edition was published in 1969. Following his death in 2002, his papers, which detailed his attempts to clear his grandfather's name, were donated to Georgetown University's Lauinger Library. They are available to the public in the Special Collections Department.

Richard Mudd petitioned several successive presidents, receiving replies from Presidents Jimmy Carter and Ronald Reagan. Carter, while sympathetic, responded by stating that he had no authority under law to set aside the conviction; Reagan responded by stating that he had come to believe that Samuel Mudd was innocent of any wrongdoing. In 1992, Representatives Steny Hoyer and Thomas W. Ewing introduced House Bill 1885 to overturn the conviction, but it failed in committee. Mudd then turned to the Army Board for Correction of Military Records, which recommended that the conviction be overturned on the basis that Mudd should have been tried by a civilian court. The recommendation was rejected by Acting Army Assistant Secretary William D. Clark.

Several other legal venues were attempted, ending in 2003 when the US Supreme Court refused to hear the case because the deadline for filing it had been missed.

St. Catharine, also known as the Dr. Samuel A. Mudd House, was listed on the National Register of Historic Places in 1974.

Mudd's life was the subject of a 1936 John Ford-directed film "The Prisoner of Shark Island", based on a script by Nunnally Johnson. A radio adaptation of "The Prisoner of Shark Island" aired, as an episode of the radio series Lux Radio Theater, with Gary Cooper as Dr. Mudd, on May 2, 1938, in which significant dramatic license was used by introducing fictional characters and altering several of the known facts of the case for melodramatic expediency. For example, Fort Jefferson was never called "Shark Island."

Another production, with the same title, aired on the radio series Encore Theatre in 1946. Another film, "The Ordeal of Dr. Mudd", was made in 1980. It starred Dennis Weaver as Mudd. At the end, a written message appears incorrectly stating that President Carter gave Mudd a posthumous pardon. All of these productions espoused the point of view that Mudd was essentially innocent of any conspiracy.

Roger Mudd, an Emmy Award-winning journalist, television host and former CBS, NBC, and PBS news anchor is related to Samuel Mudd, but he is not a descendant, as has mistakenly been reported.

Samuel Mudd's life was also the subject of an episode of the TV western "Laramie", "Time of the Traitor" which aired in 1962.

On the episode "Swiss Diplomacy" on "The West Wing", the First Lady and cardiac surgeon, Dr. Abby Bartlet commented on the duty of a physician to treat an injured patient despite potential legal repercussions. She responded to Mudd's conviction: "So that's the way it goes. You set the leg."

Samuel Mudd's name is sometimes given as the origin of the phrase "your name is mud," as in, for example, the 2007 feature film "". However, according to an online etymology dictionary, the phrase has its earliest known recorded instance in 1823, ten years before Mudd's birth, and it is based on an obsolete sense of the word "mud" meaning "a stupid twaddling fellow." 




</doc>
<doc id="28814" url="https://en.wikipedia.org/wiki?curid=28814" title="Secure Shell">
Secure Shell

Secure Shell (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network. Typical applications include remote command-line, login, and remote command execution, but any network service can be secured with SSH.

SSH provides a secure channel over an unsecured network by using a client–server architecture, connecting an SSH client application with an SSH server. The protocol specification distinguishes between two major versions, referred to as SSH-1 and SSH-2. The standard TCP port for SSH is 22. SSH is generally used to access Unix-like operating systems, but it can also be used on Microsoft Windows. Windows 10 uses OpenSSH as its default SSH client and SSH server.

Despite popular misconception, SSH is not an implementation of Telnet with cryptography provided by the Secure Sockets Layer (SSL).

SSH was designed as a replacement for Telnet and for unsecured remote shell protocols such as the Berkeley rsh and the related rlogin and rexec protocols. Those protocols send information, notably passwords, in plaintext, rendering them susceptible to interception and disclosure using packet analysis. The encryption used by SSH is intended to provide confidentiality and integrity of data over an unsecured network, such as the Internet, although files leaked by Edward Snowden indicate that the National Security Agency can sometimes decrypt SSH, allowing them to read, modify and selectively suppress the contents of SSH sessions.

SSH can also be run using SCTP rather than TCP as the connection oriented transport layer protocol.

The IANA has assigned TCP port 22, UDP port 22 and SCTP port 22 for this protocol.

SSH uses public-key cryptography to authenticate the remote computer and allow it to authenticate the user, if necessary. There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on.

Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker's public key without validation will authorize an unauthorized attacker as a valid user.

On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys. This file is respected by SSH only if it is not writable by anything apart from the owner and root. When the public key is present on the remote end and the matching private key is present on the local end, typing in the password is no longer required. However, for additional security the private key itself can be locked with a passphrase.

The private key can also be looked for in standard places, and its full path can be specified as a command line setting (the option "-i" for ssh). The ssh-keygen utility produces the public and private keys, always in pairs.

SSH also supports password-based authentication that is encrypted by automatically generated keys. In this case, the attacker could imitate the legitimate server side, ask for the password, and obtain it (man-in-the-middle attack). However, this is possible only if the two sides have never authenticated before, as SSH remembers the key that the server side previously used. The SSH client raises a warning before accepting the key of a new, previously unknown server. Password authentication can be disabled.

SSH is typically used to log into a remote machine and execute commands, but it also supports tunneling, forwarding TCP ports and X11 connections; it can transfer files using the associated SSH file transfer (SFTP) or secure copy (SCP) protocols. SSH uses the client-server model.

The standard TCP port 22 has been assigned for contacting SSH servers.

An SSH client program is typically used for establishing connections to an SSH daemon accepting remote connections. Both are commonly present on most modern operating systems, including macOS, most distributions of Linux, OpenBSD, FreeBSD, NetBSD, Solaris and OpenVMS. Notably, versions of Windows prior to Windows 10 version 1709 do not include SSH by default. Proprietary, freeware and open source (e.g. PuTTY, and the version of OpenSSH which is part of Cygwin) versions of various levels of complexity and completeness exist. File managers for UNIX-like systems (e.g. Konqueror) can use the FISH protocol to provide a split-pane GUI with drag-and-drop. The open source Windows program WinSCP provides similar file management (synchronization, copy, remote delete) capability using PuTTY as a back-end. Both WinSCP and PuTTY are available packaged to run directly off a USB drive, without requiring installation on the client machine. Setting up an SSH server in Windows typically involves enabling a feature in Settings app. In Windows 10 version 1709, an official Win32 port of OpenSSH is available.

SSH is important in cloud computing to solve connectivity problems, avoiding the security issues of exposing a cloud-based virtual machine directly on the Internet. An SSH tunnel can provide a secure path over the Internet, through a firewall to a virtual machine.

In 1995, Tatu Ylönen, a researcher at Helsinki University of Technology, Finland, designed the first version of the protocol (now called SSH-1) prompted by a password-sniffing attack at his university network. The goal of SSH was to replace the earlier rlogin, TELNET, FTP and rsh protocols, which did not provide strong authentication nor guarantee confidentiality. Ylönen released his implementation as freeware in July 1995, and the tool quickly gained in popularity. Towards the end of 1995, the SSH user base had grown to 20,000 users in fifty countries.

In December 1995, Ylönen founded SSH Communications Security to market and develop SSH. The original version of the SSH software used various pieces of free software, such as GNU libgmp, but later versions released by SSH Communications Security evolved into increasingly proprietary software.

It was estimated that by the year 2000 the number of users had grown to 2 million.

"Secsh" was the official Internet Engineering Task Force's (IETF) name for the IETF working group responsible for version 2 of the SSH protocol. In 2006, a revised version of the protocol, SSH-2, was adopted as a standard. This version is incompatible with SSH-1. SSH-2 features both security and feature improvements over SSH-1. Better security, for example, comes through Diffie–Hellman key exchange and strong integrity checking via message authentication codes. New features of SSH-2 include the ability to run any number of shell sessions over a single SSH connection. Due to SSH-2's superiority and popularity over SSH-1, some implementations such as libssh (v0.8.0+), Lsh and Dropbear support only the SSH-2 protocol.

In January 2006, well after version 2.1 was established, RFC 4253 specified that an SSH server which supports both 2.0 and prior versions of SSH should identify its protoversion as 1.99. This is not an actual version but a method to identify backward compatibility.

In 1999, developers, wanting a free software version to be available, went back to the older 1.2.12 release of the original SSH program, which was the last released under an open source license. Björn Grönvall's OSSH was subsequently developed from this codebase. Shortly thereafter, OpenBSD developers forked Grönvall's code and did extensive work on it, creating OpenSSH, which shipped with the 2.6 release of OpenBSD. From this version, a "portability" branch was formed to port OpenSSH to other operating systems.

, OpenSSH was the single most popular SSH implementation, coming by default in a large number of operating systems. OSSH meanwhile has become obsolete. OpenSSH continues to be maintained and supports the SSH-2 protocol, having expunged SSH-1 support from the codebase with the OpenSSH 7.6 release.

SSH is a protocol that can be used for many applications across many platforms including most Unix variants (Linux, the BSDs including Apple's macOS, and Solaris), as well as Microsoft Windows. Some of the applications below may require features that are only available or compatible with specific SSH clients or servers. For example, using the SSH protocol to implement a VPN is possible, but presently only with the OpenSSH server and client implementation.

The Secure Shell protocols are used in several file transfer mechanisms.

The SSH-2 protocol has an internal architecture (defined in RFC 4251) with well-separated layers, namely:


This open architecture provides considerable flexibility, allowing the use of SSH for a variety of purposes beyond a secure shell. The functionality of the transport layer alone is comparable to Transport Layer Security (TLS); the user-authentication layer is highly extensible with custom authentication methods; and the connection layer provides the ability to multiplex many secondary sessions into a single SSH connection, a feature comparable to BEEP and not available in TLS.


In 1998, a vulnerability was described in SSH 1.5 which allowed the unauthorized insertion of content into an encrypted SSH stream due to insufficient data integrity protection from CRC-32 used in this version of the protocol. A fix known as SSH Compensation Attack Detector was introduced into most implementations. Many of these updated implementations contained a new integer overflow vulnerability that allowed attackers to execute arbitrary code with the privileges of the SSH daemon, typically root.

In January 2001 a vulnerability was discovered that allows attackers to modify the last block of an IDEA-encrypted session. The same month, another vulnerability was discovered that allowed a malicious server to forward a client authentication to another server.

Since SSH-1 has inherent design flaws which make it vulnerable, it is now generally considered obsolete and should be avoided by explicitly disabling fallback to SSH-1. Most modern servers and clients support SSH-2.

In November 2008, a theoretical vulnerability was discovered for all versions of SSH which allowed recovery of up to 32 bits of plaintext from a block of ciphertext that was encrypted using what was then the standard default encryption mode, CBC. The most straightforward solution is to use CTR, counter mode, instead of CBC mode, since this renders SSH resistant to the attack.

On December 28, 2014 Der Spiegel published classified information leaked by whistleblower Edward Snowden which suggests that the National Security Agency may be able to decrypt some SSH traffic. The technical details associated with such a process were not disclosed.

An analysis in 2017 of the hacking tools BothanSpy & Gyrfalcon suggested that the SSH protocol itself was not compromised.

The following RFC publications by the IETF "secsh" working group document SSH-2 as a proposed Internet standard.

It was later modified and expanded by the following publications.

In addition, the OpenSSH project includes several vendor protocol specifications/extensions:





</doc>
<doc id="28819" url="https://en.wikipedia.org/wiki?curid=28819" title="Generalissimo Francisco Franco is still dead">
Generalissimo Francisco Franco is still dead

"Generalissimo Francisco Franco is still dead" is a catchphrase that originated in 1975 during the first season of "NBC's Saturday Night" (now called "Saturday Night Live", or "SNL") and which mocked the weeks-long media reports of the impending death of Francisco Franco. It was one of the first catchphrases from the series to enter the general lexicon.

The death of Francisco Franco, Spanish Caudillo, during the first season of "NBC's Saturday Night" served as the source of the phrase. Franco lingered near death for weeks before dying. On slow news days, United States network television newscasters sometimes noted that Franco was still alive, or not yet dead. The imminent death of Franco was a headline story on NBC News for a number of weeks prior to his death on November 20, 1975.

After Franco's death, Chevy Chase, reader of the news on "NBC's Saturday Night"'s comedic news segment "Weekend Update", announced Franco's death and read a quotation from Richard Nixon: "General Franco was a loyal friend and ally of the United States. He earned worldwide respect for Spain through firmness and fairness." As an ironic counterpoint to this, a picture was displayed behind Chase, showing Franco giving the Roman salute alongside Adolf Hitler.

In subsequent weeks Chase developed the joke into a parody of the earlier news coverage of Franco's illness, treating his death as the top story. "This breaking news just in", Chase would announce – "Generalissimo Francisco Franco is "still" dead!" Occasionally, Chase would change the wording slightly in order to keep the joke fresh, e.g. "Generalissimo Francisco Franco is still valiantly holding on in his fight to remain dead." The joke was sometimes combined with another running gag in which, rather than having a sign language interpreter visually presenting the news to aid the deaf, the show would provide assistance from Garrett Morris, "head of the New York School for the Hard of Hearing", whose "aid" involved cupping his hands around his mouth and shouting the news as Chase read it. The gag ran until early 1977, with occasional callbacks in later seasons.

The phrase has remained in use since Franco's death. James Taranto's "Best of the Web Today" column at OpinionJournal.com uses the phrase as a tag for newspaper headlines that indicate something is still happening when it should be obvious. On February 8, 2007, during Jack Cafferty's segment on CNN's "The Situation Room" on the day of the death of Anna Nicole Smith, he asked of CNN correspondent Wolf Blitzer "Is Anna Nicole Smith still dead, Wolf?" It was also used now and then on "NBC News Overnight" in the early 1980s, and Keith Olbermann occasionally used it on "Countdown". In 2013, it experienced a brief resurgence in a different context, when it began appearing on social media a few days after the death of Spanish filmmaker Jesús Franco.

"The Wall Street Journal" used the headline "Generalísimo Francisco Franco Is Still Dead – And His Statues Are Next" on its front page March 2, 2009. The newspaper used it once again on its front page in the headline "Generalísimo Francisco Franco Is Still Dead – But for some not dead enough" on August 21, 2015 when it reported about critics calling to enforce a 2007 anti-Franco law in Madrid and to rename streets and plazas, after the last election had ended the 24-year reign of conservatives in the city council.

Although "SNL"s use is perhaps the most widely known, it is predated by the "'John Garfield Still Dead' syndrome," which originated as a result of extensive coverage in the wake of the actor John Garfield's death and funeral in 1952.

After a brief "in memoriam" during "SNL"s 40th Anniversary Special on February 15, 2015, Bill Murray ended the segment with the famous phrase which "just came in from Spain."

The phrase is listed in The Oxford Dictionary of Catchphrases.

Notes
Citations


</doc>
<doc id="28820" url="https://en.wikipedia.org/wiki?curid=28820" title="Son House">
Son House

Eddie James "Son" House, Jr. (March 21, 1902 – October 19, 1988) was an American delta blues singer and guitarist, noted for his highly emotional style of singing and slide guitar playing.

After years of hostility to secular music, as a preacher and for a few years also working as a church pastor, he turned to blues performance at the age of 25. He quickly developed a unique style by applying the rhythmic drive, vocal power and emotional intensity of his preaching to the newly learned idiom. In a short career interrupted by a spell in Parchman Farm penitentiary, he developed to the point that Charley Patton, the foremost blues artist of the Mississippi Delta region, invited him to share engagements and to accompany him to a 1930 recording session for Paramount Records.

Issued at the start of the Great Depression, the records did not sell and did not lead to national recognition. Locally, House remained popular, and in the 1930s, together with Patton's associate Willie Brown, he was the leading musician of Coahoma County. There he was a formative influence on Robert Johnson and Muddy Waters. In 1941 and 1942, House and the members of his band were recorded by Alan Lomax and John W. Work for the Library of Congress and Fisk University. The following year, he left the Delta for Rochester, New York, and gave up music.

In 1964, a group of young record collectors discovered House, whom they knew of from his records issued by Paramount and by the Library of Congress. With their encouragement, he relearned his repertoire and established a career as an entertainer, performing for young, mostly white audiences in coffeehouses, at folk festivals and on concert tours during the American folk music revival, billed as a "folk blues" singer. He recorded several albums, and some informally taped concerts have also been issued as albums. House died in 1988. In 2017, his single, "Preachin' the Blues" was inducted in to the Blues Hall of Fame.

House was born in the hamlet of Lyon, north of Clarksdale, Mississippi, the second of three brothers, and lived in the rural Mississippi Delta until his parents separated, when he was about seven or eight years old. His father, Eddie House, Sr., was a musician, playing the tuba in a band with his brothers and sometimes playing the guitar. He was a church member but also a drinker; he left the church for a time, on account of his drinking, but then gave up alcohol and became a Baptist deacon. Young Eddie House adopted the family commitment to religion and churchgoing. He also absorbed the family love of music but confined himself to singing, showing no interest in the family instrumental band, and hostile to the blues on religious grounds.

When House's parents separated, his mother took him to Tallulah, Louisiana, across the Mississippi River from Vicksburg, Mississippi. When he was in his early teens, they moved to Algiers, New Orleans. Recalling these years, he would later speak of his hatred of blues and his passion for churchgoing (he described himself as "churchy" and "churchified"). At fifteen, probably while living in Algiers, he began preaching sermons.

At the age of nineteen, while living in the Delta, he married Carrie Martin, an older woman from New Orleans. This was a significant step for House; he married in church and against family opposition. The couple moved to her hometown of Centerville, Louisiana, to help run her father's farm. After a couple of years, feeling used and disillusioned, House recalled, "I left her hanging on the gatepost, with her father tellin' me to come back so we could plow some more." Around the same time, probably 1922, House's mother died. In later years, he was still angry about his marriage and said of Carrie, "She wasn't nothin' but one of them New Orleans whores".

House's resentment of farming extended to the many menial jobs he took as a young adult. He moved frequently, on one occasion taking off to East Saint Louis to work in a steel plant. The one job he enjoyed was on a Louisiana horse ranch, which later he celebrated by wearing a cowboy hat in his performances. He found an escape from manual labor when, following a conversion experience ("getting religion") in his early twenties, he was accepted as a paid pastor, first in the Baptist Church and then in the Colored Methodist Episcopal Church. However, he fell into habits which conflicted with his calling—drinking like his father and probably also womanizing. This led him after several years of conflict to leave the church, ceasing his full-time commitment, although he continued to preach sermons from time to time.

In 1927, at the age of 25, House underwent a change of musical perspective as rapid and dramatic as a religious conversion. In a hamlet south of Clarksdale, he heard one of his drinking companions, either James McCoy or Willie Wilson (his recollections differed), playing bottleneck guitar, a style he had never heard before. He immediately changed his attitude about the blues, bought a guitar from a musician called Frank Hoskins, and within weeks was playing with Hoskins, McCoy and Wilson. Two songs he learned from McCoy would later be among his best known: "My Black Mama" and "Preachin' the Blues". Another source of inspiration was Rube Lacey, a much better known performer who had recorded for Columbia Records in 1927 (no titles were released) and for Paramount Records in 1928 (two titles were released). In an astonishingly short time, with only these four musicians as models, House developed to a professional standard a blues style based on his religious singing and simple bottleneck guitar style.

Around 1927 or 1928, he had been playing in a juke joint when a man went on a shooting spree, wounding House in the leg, and he allegedly shot the man dead. House received a 15-year sentence at the Mississippi State Penitentiary (Parchman Farm), of which he served two years between 1928 and 1929. He credited his re-examination and release to an appeal by his family, but also spoke of the intervention by the influential white planter for whom they worked. The date of the killing and the duration of his sentence are unclear; House gave different accounts to different interviewers, and searches by his biographer Daniel Beaumont found no details in the court records of Coahoma County or in the archive of the Mississippi Department of Corrections.

Upon his release in 1929 or early 1930, House was strongly advised to leave Clarksdale and stay away. He walked to Jonestown and caught a train to the small town of Lula, Mississippi, sixteen miles north of Clarksdale and eight miles from the blues hub of Helena, Arkansas. Coincidentally, the great star of Delta blues, Charley Patton, was also in virtual exile in Lula, having been expelled from his base on the Dockery Plantation. With his partner Willie Brown, Patton dominated the local market for professional blues performance. Patton watched House busking when he arrived penniless at Lula station, but did not approach him. He observed House's showmanship attracting a crowd to the café and bootleg whiskey business of a woman called Sara Knight. Patton invited House to be a regular musical partner with him and Brown. House formed a liaison with Knight, and both musicians profited from association with her bootlegging activities. The musical partnership is disputed by Patton's biographers Stephen Calt and Gayle Dean Wardlow. They consider that House's musicianship was too limited to play with Patton and Brown, who were also rumoured to be estranged at the time. They also cite one statement by House that he did not play for dances in Lula. Beaumont concluded that House became a friend of Patton's, traveling with him to gigs but playing separately.

In 1930, Art Laibly of Paramount Records traveled to Lula to persuade Patton to record several more sides in Grafton, Wisconsin. Along with Patton came House, Brown, and the pianist Louise Johnson, all of whom recorded sides for the label. House recorded nine songs during that session, eight of which were released, but they were commercial failures. He did not record again commercially for 35 years, but he continued to play with Patton and Brown, and with Brown after Patton's death in 1934. During this time, House worked as a tractor driver for various plantations in the Lake Cormorant area.

Alan Lomax recorded House for the Library of Congress in 1941. Willie Brown, the mandolin player Fiddlin' Joe Martin, and the harmonica player Leroy Williams played with House on these recordings. Lomax returned to the area in 1942, where he recorded House once more.

House then faded from the public view, moving to Rochester, New York, in 1943, and working as a railroad porter for the New York Central Railroad and as a chef.

In 1964, after a long search of the Mississippi Delta region by Nick Perls, Dick Waterman and Phil Spiro, House was "rediscovered" in Rochester, New York working at a train station. He had been retired from the music business for many years and was unaware of the 1960s folk blues revival and international enthusiasm for his early recordings.

He subsequently toured extensively in the United States and Europe and recorded for CBS Records. Like Mississippi John Hurt, he was welcomed into the music scene of the 1960s and played at the Newport Folk Festival in 1964, the New York Folk Festival in July 1965, and the October 1967 European tour of the American Folk Festival, along with Skip James and Bukka White.

The young guitarist Alan Wilson (later of Canned Heat) was a fan of House's. The producer John Hammond asked Wilson, who was just 22 years old, to teach "Son House how to play like Son House," because Wilson had such a good knowledge of blues styles. House subsequently recorded the album "Father of Folk Blues", later reissued as a 2-CD set "Father of Delta Blues: The Complete 1965 Sessions". House performed with Wilson live, as can be heard on "Levee Camp Moan" on the album "John the Revelator: The 1970 London Sessions".

House appeared in Seattle on Mar 19, 1968, arranged by the Seattle Folklore Society. The concert was recorded by Bob West and issued on Acola Records as a CD in 2006. The Arcola CD also included an interview of House recorded on November 15, 1969 in Seattle.

In the summer of 1970, House toured Europe once again, including an appearance at the Montreux Jazz Festival; a recording of his London concerts was released by Liberty Records. He also played at the two Days of Blues Festival in Toronto in 1974. On an appearance on the TV arts show "Camera Three", he was accompanied by the blues guitarist Buddy Guy.

Ill health plagued House in his later years, and in 1974 he retired once again. He later moved to Detroit, Michigan, where he remained until his death from cancer of the larynx. He had been married five times. He was buried at the Mt. Hazel Cemetery. Members of the Detroit Blues Society raised money through benefit concerts to put a monument on his grave.

In addition to his early influence on Robert Johnson and Muddy Waters, he was an inspiration to John Hammond, Alan Wilson (of Canned Heat), Bonnie Raitt, Jack White of the White Stripes, Dallas Green and John Mooney.

In 2007, House was honored with a marker on the Mississippi Blues Trail in Tunica, Mississippi.

In 2017, his single "Preachin' the Blues" was inducted in to the Blues Hall of Fame.

78-RPM recordings

Recorded May 28, 1930, in Grafton, Wisconsin, for Paramount Records


Recordings for Library of Congress and Fisk University

Recorded August 1941, at Klack's Store, Lake Cormorant, Mississippi.
There are some railway noises in the background on some titles, as the store (which had electricity necessary for the recording) was close to a branch line between Lake Cormorant and Robinsonville.

Recorded July 17, 1942, Robinsonville, Mississippi

The music from both sessions and most of the recorded interviews have been reissued on LP and CD.

Singles

Other albums

This list is incomplete. For a complete list, see external links.



</doc>
<doc id="28822" url="https://en.wikipedia.org/wiki?curid=28822" title="Sex worker">
Sex worker

A sex worker is a person who is employed in the sex industry. The term is used in reference to all those in all areas of the sex industry, including those who provide direct sexual services as well as the staff and management of such industries. Some sex workers are paid to engage in sex acts or sexually explicit behavior which involves varying degrees of physical contact with clients (prostitutes and some but not all professional dominants); pornographic models and actors engage in sexually explicit behavior which is filmed or photographed. Phone sex operators have sexually-oriented conversations with clients, and may do verbal sexual roleplay.

Other sex workers are paid to engage in live sexual performance, such as webcam sex and performers in live sex shows. Some sex workers perform erotic dances and other acts for an audience. These include: striptease, go-go dancing, lap dancing, neo-burlesque, and peep shows. Sexual surrogates work with psychoanalysts to engage in sexual activity as part of therapy with their clients. Thus, although the term "sex worker" is sometimes viewed as a synonym or euphemism for "prostitute", it is more general. "Sex worker" can refer to individuals who do not directly engage in sexual activity such as pole dancers, sex toy testers, and strip club managers. Another example of sex workers that would not fall under the term "prostitute" would be an adult talent manager, who negotiates and secures pornographic roles for clients. There are also erotic photographers who shoot and edit for adult media and porn reviewers who watch and rate adult films.

Some people use the term "sex worker" to avoid invoking the stigma associated with the word "prostitute". Using the term "sex worker" rather than "prostitute" also allows more members of the sex industry to be represented and helps ensure that individuals who are actually prostitutes are not singled out and associated with the negative connotations of "prostitute." In addition, choosing to use the term "sex worker" rather than "prostitute" shows ownership over the individuals' career choice. Some argue that those who prefer the term "sex worker" wish to separate their occupation from their person. Describing someone as a sex worker recognizes that the individual may have many different facets, and are not necessarily defined by their job.

According to one view, sex work is different from sexual exploitation, or the forcing of a person to commit sexual acts, in that sex work is voluntary "and is seen as the commercial exchange of sex for money or goods". In an attempt to further clarify the broad term that "sex work" is, John E. Exner, an American psychologist, worked with his colleagues to create five distinct classes for categorizing sex workers. One scholarly article details the classes as follows: "specifically, the authors articulated Class I, or the upper class of the profession, consisting of call girls; Class II was referred to as the middle class, consisting of 'in-house girls' who typically work in an establishment on a commission basis; Class III, the lower middle class, were 'streetwalkers' whose fees and place of work fluctuate considerably; Class IV sex workers have been known as 'commuter housewives', and they are typically involved in sex work to supplement family income; and Class V consists of 'streetwalker addicts', or 'drugs-for-sex streetwalkers' who are considered the lower class of the profession."

The term "sex worker" was coined in 1978 by sex worker activist Carol Leigh. Its use became popularized after publication of the anthology, "Sex Work: Writings By Women In The Sex Industry" in 1987, edited by Frédérique Delacoste and Priscilla Alexander.</ref> The term "sex worker" has since spread into much wider use, including in academic publications, by NGOs and labor unions, and by governmental and intergovernmental agencies, such as the World Health Organization. The term is listed in the Oxford English Dictionary and Merriam-Webster's Dictionary.

The term is strongly opposed, however, by many who are morally opposed to the sex industry, such as social conservatives, anti-prostitution feminists, and other prohibitionists. Such groups view prostitution variously as a crime or as victimization, and see the term "sex work" as legitimizing criminal activity or exploitation as a type of labor.

Sex workers may be any gender and exchange sexual services or favors for money or other gifts. The motives of sex workers vary widely and can include debt, coercion, survival, or simply as a way to earn a living. Sexual empowerment is another possible reasons why people engage in sex work. One Canadian study found that a quarter of the sex workers interviewed started sex work because they found it "appealing". The flexibility to choose hours of work and ability to select their own client base may also contribute the appeal of sex work when compared to other service industry jobs. Sex work may also be a way to fund addiction. This line of work can be fueled by an individual's addiction to illegal substances before entering the industry or being introduced to these substances after entering the industry. These motives also align with varying climates surrounding sex work in different communities and cultures. In some cases, sex work is linked to tourism. Sex work can take the form of prostitution, stripping or lap dancing, performance in pornography, phone or internet sex, or any other exchange of sexual services for financial or material gain. The variety in the tasks encompassed by sex work lead to a large range in both severity and nature of risks that sex workers face in their occupations. Sex workers can act independently as individuals, work for a company or corporation, or work as part of a brothel. All of the above can be undertaken either by free choice or by coercion, or, as some argue, along a continuum between conflict and agency. Sex workers may also be hired to be companions on a trip or to perform sexual services within the context of a trip; either of these can be voluntary or forced labor. Transgender people are more likely than the general population to do sex work, particularly trans women and trans people of color. In a study of female Indian sex workers, illiteracy and lower social status were more prevalent than among the general female population.

Many studies struggle to gain demographic information about the prevalence of sex work, as many countries or cities have laws prohibiting prostitution or other sex work. In addition, sex trafficking, or forced sex work, is also difficult to quantify due to its underground and covert nature. In addition, finding a representative sample of sex workers in a given city can be nearly impossible because the size of the population itself is unknown. Maintaining privacy and confidentiality in research is also difficult because many sex workers may face prosecution and other consequences if their identities are revealed.

While demographic characteristics of sex workers vary by region and are hard to measure, some studies have attempted to estimate the composition of the sex work communities in various places. For example, one study of sex work in Tijuana, Mexico found that the majority of sex workers there are young, female and heterosexual. Many of these studies attempt to use smaller samples of sex workers and pimps in order to extrapolate about larger populations of sex workers. One report on the underground sex trade in the United States used known data on the illegal drug and weapon trades and interviews with sex workers and pimps in order to draw conclusions about the number of sex workers in eight American cities. However, studies like this one can come under scrutiny for a perceived emphasis on the activities and perspectives of pimps and other sex work managers rather than those of sex work providers themselves. Another criticism is that sex trafficking may not be adequately assessed in its relation to sex work in these studies.

Sex workers may be stereotyped as deviant, hypersexual, sexually risky, and substance abusive. Sex workers cope with this stigmatization, or othering, in ways such as hiding their occupation from non-sex workers, social withdrawal, and creating a false self to perform at work. Sex-work-related stigma perpetuates rape culture and leads to slut-shaming.

Sex work is also often conflated with sex trafficking, despite the fact that sex workers choose to consensually engage in the sex trade. For example, the Fight Online Sex Trafficking Act in the United States was passed to ostensibly protect victims of sex trafficking but included language making it illegal to advertise consensual sex online. Such laws have a significantly negative impact on sex workers. 

Globally, sex workers encounter barriers in accessing health care, legislation, legal resources, and labor rights. In a study of U.S sex workers, 43% of interview participants reported exposure to intimate-partner violence, physical violence, armed physical violence, and sexual violence in the forms of sexual coercion and rape. In this same study, a sex worker reported, "in this lifestyle nothing’s safe". Sex workers experience police abuse as well. Police use their authority to intimidate sex workers. Police officers have been reported to exploit street-based sex workers’ fear of incarceration to force them to have sex with the police without payment, sometimes still arresting them after the coerced sex. Police also compromise sex workers safety, often holding sex workers responsible for crimes acted against them because of the stigma attached to their occupation, also known as victim-blaming. The effects of whorephobia impacts sex workers’ agency, safety, and mental health. There is growth in advocacy organizations to reduce and erase prejudice and stigma against sex work, and to provide more support and resources for sex workers.

Depending on local law, sex workers' activities may be regulated, controlled, tolerated, or prohibited. In most countries, even those where sex work is legal, sex workers may be stigmatized and marginalized, which may prevent them from seeking legal redress for discrimination (e.g., racial discrimination by a strip club owner), non-payment by a client, assault or rape. Sex worker advocates have identified this as whorephobia.

The legality of different types of sex work varies within and between regions of the world. For example, while pornography is legal in the United States, prostitution is illegal in most parts of the US. However, in other regions of the world, both pornography and prostitution are illegal; in others, both are legal. One example of a country in which pornography, prostitution, and all professions encompassed under the umbrella of sex work are all legal is New Zealand. Under the Prostitution Reform Act of New Zealand, laws and regulations have been put into place in order to ensure the safety and protection of its sex workers. For example, since the implementation of the Prostitution Reform Act, "any person seeking to open a larger brothel, where more than four sex workers will be working requires a Brothel Operators Certificate, which certifies them as a suitable person to exercise control over sex workers in the workplace. [In addition,] sex workers operating in managed premises have access to labour rights and human rights protection and can pursue claims before the courts, like any other worker or employee." In regions where sex work is illegal, advocates for sex workers' rights argue that the covert nature of illegal prostitution is a barrier to access to legal resources. However, some who oppose the legalization of prostitution argue that sex work is inherently exploitative and can never be legalized or practiced in a way that respects the rights of those who perform it.

There are many arguments against legalizing prostitution/sex work. In one study, women involved in sex work were interviewed and asked if they thought it should be made legal. They answered that they thought it should not, as it would put women at higher risk from violent customers if it were considered legitimate work, and they would not want their friends or family entering the sex industry to earn money. Another argument is that legalizing sex work would increase the demand for it, and women should not be treated as sexual merchandise. A study showed that in countries that have legalized prostitution, there was an increase in child prostitution. An argument against legalizing sex work is to keep children from being involved in this industry. The studies also showed that legalizing sex work lead to an increase in sex trafficking, which is another reason people give for making sex work illegal.

There are also arguments for legalizing prostitution/sex work. One major argument for legalizing prostitution is that women should have a right to do what they want with their own bodies. The government should not have a say in what they do for work, and if they want to sell their bodies it is their own decision. Another common argument for legalizing prostitution is that enforcing prostitution laws is a waste of money. This is because prostitution has always, and will continue to persist despite whatever laws and regulations are implemented against it. In arguing for the decriminalization of sex work, the Minister of Justice of the Netherlands expanded upon this argument in court when stating that, "prostitution has existed for a long time and will continue to do so…Prohibition is not the way to proceed…One should allow for voluntary prostitution. The authorities can then regulate prostitution, [and] it can become healthy, safe, transparent, and cleansed from criminal side-effects." People who wish to legalize prostitution do not see enforcing laws against sex work as effective and think the money is better spent elsewhere. Many people also argue that legalization of prostitution will lead to less harm for the sex workers. They argue that the decriminalization of sex work will decrease the exploitation of sex workers by third parties such as pimps and managers. A final argument for the legalization of sex work is that prostitution laws are unconstitutional. Some argue that these laws go against people's rights to free speech, privacy, etc.

Risk reduction in sex work is a highly debated topic. "Abolitionism" and "nonabolitionism" or "empowerment" are regarded as opposing ways in which risk reduction is approached. While abolitionism would call for an end to all sex work, empowerment would encourage the formation of networks among sex workers and enable them to prevent STIs and other health risks by communicating with each other. Both approaches aim to reduce rates of disease and other negative effects of sex work.

In addition, sex workers themselves have disputed the dichotomous nature of abolitionism and nonabolitionism, advocating instead a focus on sex workers' rights. In 1999, the Network of Sex Worker Projects claimed that "Historically, anti-trafficking measures have been more concerned with protecting 'innocent' women from becoming prostitutes than with ensuring the human rights of those in the sex industry. Penelope Saunders, a sex workers' rights advocate, claims that the sex workers' rights approach considers more of the historical context of sex work than either abolitionism or empowerment. In addition, Jo Doezema has written that the dichotomy of the voluntary and forced approaches to sex work has served to deny sex workers agency.

Sex workers are unlikely to disclose their work to healthcare providers. This can be due to embarrassment, fear of disapproval, or a disbelief that sex work can have effects on their health. The criminalization of sex work in many places can also lead to a reluctance to disclose for fear of being turned in for illegal activities. There are very few legal protections for sex workers due to criminalization; thus, in many cases, a sex worker reporting violence to a healthcare provider may not be able to take legal action against their aggressor.

Health risks of sex work relate primarily to sexually transmitted infections and to drug use. In one study, nearly 40% of sex workers who visited a health center reported illegal drug use. In general, transgender women sex workers have a higher risk of contracting HIV than male and female sex workers and transgender women who are not sex workers.

The reason transgender women are at higher risk for developing HIV is their combination of risk factors. They face biological, personal, relational, and structural risks that all increase their chances of getting HIV. Biological factors include incorrect condom usage because of erectile dysfunction from hormones taken to become more feminine and receptive anal intercourse without a condom which is a high risk for developing HIV. Personal factors include mental health issues that lead to increased sexual risk, such as anxiety, depression, and substance abuse provoked through lack of support, violence, etc. Structural risks include involvement in sex work being linked to poverty, substance abuse, and other factors that are more prevalent in transgender women based on their tendency to be socially marginalized and not accepted for challenging gender norms. The largest risk for HIV is unprotected sex with male partners, and studies have been emerging that show men who have sex with transgender women are more likely to use drugs than men that do not.

Condom use is one way to mitigate the risk of contracting an STI. However, negotiating condom use with one's clients and partners is often an obstacle to practicing safer sex. While there is not much data on rates of violence against sex workers, many sex workers do not use condoms due to the fear of resistance and violence from clients. Some countries also have laws prohibiting condom possession; this reduces the likelihood that sex workers will use condoms. Increased organization and networking among sex workers has been shown to increase condom use by increasing access to and education about STI prevention. Brothels with strong workplace health practices, including the availability of condoms, have also increased condom use among their workers.

Health Concerns of Exotic Dancers<br>"Mental Health and Stigma"<br>In order to protect themselves from the stigma of sex work, many dancers resort to othering themselves. Othering involves constructing oneself as superior to one's peers, and the dancer persona provides an internal boundary that separates the "authentic" from the stripper self. This practice creates a lot of stress for the dancers, in turn leading many to resort to using drugs and alcohol to cope. Since it is so widespread, the use of drugs has become normalized in the exotic dance scene.<br>Despite this normalization, passing as nonusers, or covering as users of less maligned drugs, is necessary. This is because strippers concurrently attribute a strong moral constitution to those that resist the drug atmosphere; it is a testament to personal strength and will power. It is also an occasion for dancers to "other" fellow strippers. Valorizing resistance to the drug space discursively positions "good" strippers against such a drug locale and indicates why dancers are motivated to closet hard drug use.<br>Stigma causes strippers to hide their lifestyles from friends and family alienating themselves from a support system. Further, the stress of trying to hide their lifestyles from others due to fear of scrutiny affects the mental health of dancers. Stigma is a difficult area to address because it is more abstract, but it would be helpful to work toward normalizing sex work as a valid way of making a living. This normalization of sex work would relieve the stress many dancers experience increasing the likelihood that they will be open about their work. Being open will allow them access to a viable support system and reduce the othering and drug use so rampant in the sex industry.

Forced sex work is when an individual enters into any sex trade due to coercion rather than by choice. Forced sex work increases the likelihood that a sex worker will contract HIV/AIDS or another sexually transmitted infection, particularly when an individual enters sex work before the age of 18. In addition, even when sex workers do consent to certain sex acts, they are often forced or coerced into others (often anal intercourse) by clients. Sex workers may also experience strong resistance to condom use by their clients, which may extend into a lack of consent by the worker to any sexual act performed in the encounter; this risk is magnified when sex workers are trafficked or forced into sex work.

Forced sex work often involves deception - workers are told that they can make a living and are then not allowed to leave. This deception can cause ill effects on the mental health of many sex workers. In addition, an assessment of studies estimates that between 40% and 70% of sex workers face violence within a year. Currently, there is little support for migrant workers in many countries, including those who have been trafficked to a location for sex.

Sex worker's rights advocates argue that sex workers should have the same basic human and labor rights as other working people. For example, the Canadian Guild for Erotic Labour calls for the legalization of sex work, the elimination of state regulations that are more repressive than those imposed on other workers and businesses, the right to recognition and protection under labour and employment laws, the right to form and join professional associations or unions, and the right to legally cross borders to work. Advocates also want to see changes in legal practices involving sex work, the Red Umbrella Project has pushed for the decriminalization of condoms and changes to New York's sex workers diversion program. Advocacy for the interests of sex workers can come from a variety of sources, including non-governmental organizations, labor rights organizations, governments, or sex workers themselves. Each year in London The Sexual Freedom Awards is held to honor the most notable advocates and pioneers of sexual freedom and sex workers' rights in the UK, where sex work is essentially legal.

The unionization of sex workers is a recent development. The first organization within the contemporary sex workers' rights movement was Call Off Your Old Tired Ethics (COYOTE), founded in 1973 in San Francisco, California. Many organizations in Western countries were established in the decade after the founding of COYOTE. Currently, a small number of sex worker unions exist worldwide. One of the largest is the International Union of Sex Workers, headquartered in the United Kingdom. The IUSW advocates for the rights of all sex workers, whether they chose freely or were coerced to enter the trade, and promotes policies that benefit the interests of sex workers both in the UK and abroad. Many regions are home to sex worker unions, including Latin America, Brazil, Canada, Europe, and Africa.

In unionizing, many sex workers face issues relating to communication and to the legality of sex work. Because sex work is illegal in many places where they wish to organize, it is difficult to communicate with other sex workers in order to organize. There is also concern with the legitimacy of sex work as a career and an activity that merits formal organizing, largely because of the sexism often present in sex work and the devaluation of sex work as not comparable to other paid labor and employment.

A factor affecting the unionization of sex work is that many sex workers belong to populations that historically have not had a strong representation in labor unions. While this unionization can be viewed as a way of empowering sex workers and granting them agency within their profession, it is also criticized as implicitly lending its approval to sexism and power imbalances already present in sex work. Unionization also implies a submission to or operation within the systems of capitalism, which is of concern to some feminists.

"Independent contractor vs Employee"<br>Performers in general are problematic to categorize because they often exercise a high level of control over their work product, one characteristic of an independent contractor. Additionally, their work can be artistic in nature and often done on a freelance basis. Often, the work of performers does not possess the obvious attributes of employees such as regular working hours, places or duties. Consequently, employers misclassify them because they are unsure of their workers' status, or they purposely misclassify them to take advantage of independent contractors' low costs. Exotic dance clubs are one such employer that purposely misclassify their performers as independent contractors.

There are additional hurdles in terms of self-esteem and commitment to unionize. On the most basic level, dancers themselves must have the desire to unionize for collective action. For those who wish not to conform to group activity or want to remain independent, a union may seem as controlling as club management since joining a union would obligate them to pay dues and abide by decisions made through majority vote, with or without their personal approval.

In the "Lusty Lady" case study, this strip club was the first all-woman-managed club to successfully unionize in 1996. Some of the working conditions they were able to address included "protest[ing] racist hiring practices, customers being allowed to videotape dancers without their consent via one-way mirrors, inconsistent disciplinary policies, lack of health benefits, and an overall dearth of job security". Unionizing exotic dancers can certainly bring better work conditions and fair pay, but it is difficult to do at times because of their dubious employee categorization. Also, as is the case with many other unions, dancers are often reluctant to join them. This reluctance can be due to many factors, ranging from the cost of joining a union to the dancers believing they do not need union support because they will not be exotic dancers for a long enough period of time to justify joining a union.

NGOs often play a large role in outreach to sex workers, particularly in HIV and STI prevention efforts. However, NGO outreach to sex workers for HIV prevention is sometimes less coordinated and organized than similar HIV prevention programs targeted at different groups (such as men who have sex with men). This lack of organization may be due to the legal status of prostitution and other sex work in the country in question; in China, many sex work and drug abuse NGOs do not formally register with the government and thus run many of their programs on a small scale and discreetly.

While some NGOs have increased their programming to improve conditions within the context of sex work, these programs are criticized at times due to their failure to dismantle the oppressive structures of prostitution, particularly forced trafficking. Some scholars believe that advocating for rights within the institution of prostitution is not enough; rather, programs that seek to empower sex workers must empower them to leave sex work as well as improve their rights within the context of sex work.











</doc>
<doc id="28824" url="https://en.wikipedia.org/wiki?curid=28824" title="Stéphane Mallarmé">
Stéphane Mallarmé

Stéphane Mallarmé ( , ; 18 March 1842 – 9 September 1898), pen name of Étienne Mallarmé, was a French poet and critic. He was a major French symbolist poet, and his work anticipated and inspired several revolutionary artistic schools of the early 20th century, such as Cubism, Futurism, Dadaism, and Surrealism.

Stéphane Mallarmé was born in Paris. He was a boarder at the "Pensionnat des Frères des écoles chrétiennes à Passy" between 6 or 9 October 1852 and March 1855. He worked as an English teacher and spent much of his life in relative poverty but was famed for his "salons", occasional gatherings of intellectuals at his house on the rue de Rome for discussions of poetry, art and philosophy. The group became known as "les Mardistes," because they met on Tuesdays (in French, "mardi"), and through it Mallarmé exerted considerable influence on the work of a generation of writers. For many years, those sessions, where Mallarmé held court as judge, jester, and king, were considered the heart of Paris intellectual life. Regular visitors included W.B. Yeats, Rainer Maria Rilke, Paul Valéry, Stefan George, Paul Verlaine, and many others.

Along with other members of "La Revue Blanche" such as Jules Renard, Julien Benda and Ioannis Psycharis, Mallarmé was a Dreyfusard.

On 10 August 1863, he married Maria Christina Gerhard. Their daughter, (Stéphanie Françoise) Geneviève Mallarmé, was born on 19 November 1864. Mallarmé died in Valvins (present-day Vulaines-sur-Seine) September 9, 1898.

Mallarmé's earlier work owes a great deal to the style of Charles Baudelaire who was recognised as the forerunner of literary Symbolism. Mallarmé's later "fin de siècle" style, on the other hand, anticipates many of the fusions between poetry and the other arts that were to blossom in the next century. Most of this later work explored the relationship between content and form, between the text and the arrangement of words and spaces on the page. This is particularly evident in his last major poem, "Un coup de dés jamais n'abolira le hasard" ('A roll of the dice will never abolish chance') of 1897.

Some consider Mallarmé one of the French poets most difficult to translate into English. The difficulty is due in part to the complex, multilayered nature of much of his work, but also to the important role that the sound of the words, rather than their meaning, plays in his poetry. When recited in French, his poems allow alternative meanings which are not evident on reading the work on the page. For example, Mallarmé's "Sonnet en '-yx"' opens with the phrase "ses purs ongles" ('her pure nails'), whose first syllables when spoken aloud sound very similar to the words "c'est pur son" ('it's pure sound'). Indeed, the 'pure sound' aspect of his poetry has been the subject of musical analysis and has inspired musical compositions. These phonetic ambiguities are very difficult to reproduce in a translation which must be faithful to the meaning of the words.

Mallarmé's poetry has been the inspiration for several musical pieces, notably Claude Debussy's "Prélude à l'après-midi d'un faune" (1894), a free interpretation of Mallarmé's poem "L'après-midi d'un faune" (1876), which creates powerful impressions by the use of striking but isolated phrases. Maurice Ravel set Mallarmé's poetry to music in "Trois poèmes de Mallarmé" (1913). Other composers to use his poetry in song include Darius Milhaud ("Chansons bas de Stéphane Mallarmé", 1917) and Pierre Boulez ("Pli selon pli", 1957–62).

Man Ray's last film, entitled "Les Mystères du Château de Dé (The Mystery of the Chateau of Dice)" (1929), was greatly influenced by Mallarmé's work, prominently featuring the line "A roll of the dice will never abolish chance".

Mallarmé is referred to extensively in the latter section of Joris-Karl Huysmans' "À rebours", where Des Esseintes describes his fervour-infused enthusiasm for the poet: "These were Mallarmé's masterpieces and also ranked among the masterpieces of prose poetry, for they combined a style so magnificently that in itself it was as soothing as a melancholy incantation, an intoxicating melody, with irresistibly suggestive thoughts, the soul-throbs of a sensitive artist whose quivering nerves vibrate with an intensity that fills you with a painful ecstasy." [p. 198, Robert Baldick translation]

The critic and translator Barbara Johnson has emphasized Mallarmé's influence on twentieth-century French criticism and theory: "It was largely by learning the lesson of Mallarmé that critics like Roland Barthes came to speak of 'the death of the author' in the making of literature. Rather than seeing the text as the emanation of an individual author's intentions, structuralists and deconstructors followed the paths and patterns of the linguistic signifier, paying new attention to syntax, spacing, intertextuality, sound, semantics, etymology, and even individual letters. The theoretical styles of Jacques Derrida, Julia Kristeva, Maurice Blanchot, and especially Jacques Lacan also owe a great deal to Mallarmé's 'critical poem.'"

It has been suggested that "much of Mallarmé's work influenced the conception of hypertext, with his purposeful use of blank space and careful placement of words on the page, allowing multiple non-linear readings of the text. This becomes very apparent in his work "Un coup de dés"."

On the publishing of "Un Coup de Dés" and its mishaps after the death of Mallarmé, consult the notes and commentary of for his edition of the complete works of Mallarmé, Volume 1, Bibliothèque de la Pléiade, Gallimard 1998. To delve more deeply, consult "Igitur, Divagations, Un Coup de Dés," edited by Bertrand Marchal with a preface by Yves Bonnefoy, nfr Poésie/Gallimard.

In 1990, Greenhouse Review Press published D. J. Waldie's American translation of "Un Coup de Dés" in a letterpress edition of 60 copies, its typography and format based on examination of the final (or near final) corrected proofs of the poem in the collection of Harvard's Houghton Library.

Prior to 2004, "Un Coup de Dés" was never published in the typography and format conceived by Mallarmé. In 2004, 90 copies on vellum of a new edition were published by Michel Pierson et Ptyx. This edition reconstructs the typography originally designed by Mallarmé for the projected Vollard edition in 1897 and which was abandoned after the sudden death of the author in 1898. All the pages are printed in the format (38 cm by 28 cm) and in the typography chosen by the author. The reconstruction has been made from the proofs which are kept in the Bibliothèque Nationale de France, taking into account the written corrections and wishes of Mallarmé and correcting certain errors on the part of the printers Firmin-Didot.

A copy of this new edition can be consulted in the Bibliothèque François-Mitterrand. Copies have been acquired by the Bibliothèque littéraire Jacques-Doucet and University of California - Irvine, as well as by private collectors. A copy has been placed in the Museum Stéphane Mallarmé at Vulaines-sur-Seine, Valvins, where Mallarmé lived and died and where, according to Paul Valéry, he made his final corrections on the proofs prior to the projected printing of the poem.

The poet and visual artist Marcel Broodthaers created a purely graphical version of "Un coup de Dés", using Mallarmé's typographical layout but with the words replaced by black bars.

In 2012, the French philosopher Quentin Meillassoux published "The Number and the Siren", a rigorous attempt at 'deciphering' the poem on the basis of a unique interpretation of the phrase 'the unique Number, which cannot be another.'

In 2015, Wave Books published "A Roll of the Dice Will Never Abolish Chance", a definitive dual-language edition of the poem, translated by Robert Bononno and Jeff Clark (designer). Readers may also consider Henry Weinfield's translation (in dual-language edition) to merit consideration as "definitive"—or, indeed, each generation will find its own definitive translation.

In 2018, Apple Pie Editions published "un coup de des jamais n'abolira le hasard: translations" by Eric Zboya, an English edition that transforms the poem not only through erasure, but through graphic imaging software.







</doc>
<doc id="28825" url="https://en.wikipedia.org/wiki?curid=28825" title="Submarine">
Submarine

A submarine (or sub) is a watercraft capable of independent operation underwater. It differs from a submersible, which has more limited underwater capability. It is also sometimes used historically or colloquially to refer to remotely operated vehicles and robots, as well as medium-sized or smaller vessels, such as the midget submarine and the wet sub. Submarines are referred to as "boats" rather than "ships" irrespective of their size.

Although experimental submarines had been built before, submarine design took off during the 19th century, and they were adopted by several navies. Submarines were first widely used during World War I (1914–1918), and are now used in many navies large and small. Military uses include attacking enemy surface ships (merchant and military), or other submarines, aircraft carrier protection, blockade running, ballistic missile submarines as part of a nuclear strike force, reconnaissance, conventional land attack (for example using a cruise missile), and covert insertion of special forces. Civilian uses for submarines include marine science, salvage, exploration and facility inspection and maintenance. Submarines can also be modified to perform more specialized functions such as search-and-rescue missions or undersea cable repair. Submarines are also used in tourism and undersea archaeology.

Most large submarines consist of a cylindrical body with hemispherical (or conical) ends and a vertical structure, usually located amidships, which houses communications and sensing devices as well as periscopes. In modern submarines, this structure is the "sail" in American usage and "fin" in European usage. A "conning tower" was a feature of earlier designs: a separate pressure hull above the main body of the boat that allowed the use of shorter periscopes. There is a propeller (or pump jet) at the rear, and various hydrodynamic control fins. Smaller, deep-diving and specialty submarines may deviate significantly from this traditional layout. Submarines use diving planes and also change the amount of water and air in ballast tanks to change buoyancy for submerging and surfacing.

Submarines have one of the widest ranges of types and capabilities of any vessel. They range from small autonomous examples and one- or two-person subs that operate for a few hours to vessels that can remain submerged for six months—such as the Russian , the biggest submarines ever built. Submarines can work at greater depths than are survivable or practical for human divers. Modern deep-diving submarines derive from the bathyscaphe, which in turn evolved from the diving bell.
Whereas the principal meaning of "submarine" is an armed, submersible warship, the more general meaning is for any type of submersible craft. The definition as of 1899 was for any type of "submarine boat". By naval tradition, submarines are still usually referred to as "boats" rather than as "ships", regardless of their size. In other navies with a history of large submarine fleets they are also "boats"; in German it is an or (under-sea boat) and in Russian it is a (underwater boat). Although referred to informally as "boats", U.S. submarines employ the designation USS (United States Ship) at the beginning of their names, such as . In the Royal Navy, submarines continue to be referred to officially as "boats", despite their "Her Majesty's Ship" designations.

According to a report in "Opusculum Taisnieri" published in 1562:
In 1578, the English mathematician William Bourne recorded in his book "Inventions or Devises" one of the first plans for an underwater navigation vehicle. A few years later the Scottish mathematician and theologian John Napier wrote in his "Secret Inventions" (1596) that "These inventions besides devises of sayling under water with divers, other devises and strategems for harming of the enemyes by the Grace of God and worke of expert Craftsmen I hope to perform." It's unclear whether he ever carried out his idea.

The first submersible of whose construction there exists reliable information was designed and built in 1620 by Cornelis Drebbel, a Dutchman in the service of James I of England. It was propelled by means of oars.

By the mid-18th century, over a dozen patents for submarines/submersible boats had been granted in England. In 1747, Nathaniel Symons patented and built the first known working example of the use of a ballast tank for submersion. His design used leather bags that could fill with water to submerge the craft. A mechanism was used to twist the water out of the bags and cause the boat to resurface. In 1749, the Gentlemen's Magazine reported that a similar design had initially been proposed by Giovanni Borelli in 1680. Further design improvement stagnated for over a century, until application of new technologies for propulsion and stability.

The first military submersible was (1775), a hand-powered acorn-shaped device designed by the American David Bushnell to accommodate a single person. It was the first verified submarine capable of independent underwater operation and movement, and the first to use screws for propulsion.

In 1800, France built a human-powered submarine designed by American Robert Fulton, . The French eventually gave up on the experiment in 1804, as did the British when they later considered Fulton's submarine design.

In 1864, late in the American Civil War, the Confederate navy's became the first military submarine to sink an enemy vessel, the Union sloop-of-war . In the aftermath of its successful attack against the ship, "H. L. Hunley" also sank, possibly because it was too close to its own exploding torpedo.

In 1866, was the first submarine to successfully dive, cruise underwater, and resurface under the control of the crew. The design by German American Julius H. Kroehl (in German, "Kröhl") incorporated elements that are still used in modern submarines.

In 1866, was built at the request of the Chilean government, by Karl Flach, a German engineer and immigrant. It was the fifth submarine built in the world and, along with a second submarine, was intended to defend the port of Valparaiso against attack by the Spanish Navy during the Chincha Islands War.

The first submarine not relying on human power for propulsion was the French ("Diver"), launched in 1863, which used compressed air at . Narcís Monturiol designed the first air-independent and combustion-powered submarine, , which was launched in Barcelona, Spain in 1864.

The submarine became a potentially viable weapon with the development of the Whitehead torpedo, designed in 1866 by British engineer Robert Whitehead, the first practical self-propelled or 'locomotive' torpedo. The spar torpedo that had been developed earlier by the Confederate States Navy was considered to be impracticable, as it was believed to have sunk both its intended target, and probably "H. L. Hunley", the submarine that deployed it. In 1878, John Philip Holland demonstrated the Holland I prototype.

Discussions between the English clergyman and inventor George Garrett and the Swedish industrialist Thorsten Nordenfelt led to the first practical steam-powered submarines, armed with torpedoes and ready for military use. The first was "Nordenfelt I", a 56-tonne, vessel similar to Garrett's ill-fated (1879), with a range of , armed with a single torpedo, in 1885.

A reliable means of propulsion for the submerged vessel was only made possible in the 1880s with the advent of the necessary electric battery technology. The first electrically powered boats were built by Isaac Peral y Caballero in Spain (who built ), Dupuy de Lôme (who built ) and Gustave Zédé (who built "Sirène") in France, and James Franklin Waddington (who built "Porpoise") in England. Peral's design featured torpedoes and other systems that later became standard in submarines.

Submarines were not put into service for any widespread or routine use by navies until the early 1900s. This era marked a pivotal time in submarine development, and several important technologies appeared. A number of nations built and used submarines. Diesel electric propulsion became the dominant power system and equipment such as the periscope became standardized. Countries conducted many experiments on effective tactics and weapons for submarines, which led to their large impact in World War I.

The Irish inventor John Philip Holland built a model submarine in 1876 and a full-scale version in 1878, which were followed by a number of unsuccessful ones. In 1896 he designed the Holland Type VI submarine, which used internal combustion engine power on the surface and electric battery power underwater. Launched on 17 May 1897 at Navy Lt. Lewis Nixon's Crescent Shipyard in Elizabeth, New Jersey, "Holland VI" was purchased by the United States Navy on 11 April 1900, becoming the Navy's first commissioned submarine, christened .

Commissioned in June 1900, the French steam and electric employed the now typical double-hull design, with a pressure hull inside the outer shell. These 200-ton ships had a range of over underwater. The French submarine "Aigrette" in 1904 further improved the concept by using a diesel rather than a gasoline engine for surface power. Large numbers of these submarines were built, with seventy-six completed before 1914.

The Royal Navy commissioned five s from Vickers, Barrow-in-Furness, under licence from the Holland Torpedo Boat Company from 1901 to 1903. Construction of the boats took longer than anticipated, with the first only ready for a diving trial at sea on 6 April 1902. Although the design had been purchased entirely from the US company, the actual design used was an untested improvement to the original Holland design using a new petrol engine.

These types of submarines were first used during the Russo-Japanese War of 1904–05. Due to the blockade at Port Arthur, the Russians sent their submarines to Vladivostok, where by 1 January 1905 there were seven boats, enough to create the world's first "operational submarine fleet". The new submarine fleet began patrols on 14 February, usually lasting for about 24 hours each. The first confrontation with Japanese warships occurred on 29 April 1905 when the Russian submarine "Som" was fired upon by Japanese torpedo boats, but then withdrew.

Military submarines first made a significant impact in World War I. Forces such as the U-boats of Germany saw action in the First Battle of the Atlantic, and were responsible for sinking , which was sunk as a result of unrestricted submarine warfare and is often cited among the reasons for the entry of the United States into the war.

At the outbreak of the war, Germany had only twenty submarines immediately available for combat, although these included vessels of the diesel-engined "U-19" class, which had a sufficient range of and speed of to allow them to operate effectively around the entire British coast., By contrast, the Royal Navy had a total of 74 submarines, though of mixed effectiveness. In August 1914, a flotilla of ten U-boats sailed from their base in Heligoland to attack Royal Navy warships in the North Sea in the first submarine war patrol in history.

The U-boats' ability to function as practical war machines relied on new tactics, their numbers, and submarine technologies such as combination diesel-electric power system developed in the preceding years. More submersibles than true submarines, U-boats operated primarily on the surface using regular engines, submerging occasionally to attack under battery power. They were roughly triangular in cross-section, with a distinct keel to control rolling while surfaced, and a distinct bow. During World War I more than 5,000 Allied ships were sunk by U-boats.

The British tried to catch up to the Germans in terms of submarine technology with the creation of the K-class submarines. However, these were extremely large and often collided with each other forcing the British to scrap the K-class design shortly after the war.

During World War II, Germany used submarines to devastating effect in the Battle of the Atlantic, where it attempted to cut Britain's supply routes by sinking more merchant ships than Britain could replace. (Shipping was vital to supply Britain's population with food, industry with raw material, and armed forces with fuel and armaments.) While U-boats destroyed a significant number of ships, the strategy ultimately failed. Although the U-boats had been updated in the interwar years, the major innovation was improved communications, encrypted using the famous Enigma cipher machine. This allowed for mass-attack naval tactics ("Rudeltaktik", commonly known as "wolfpack"), but was also ultimately the U-boats' downfall. By the end of the war, almost 3,000 Allied ships (175 warships, 2,825 merchantmen) had been sunk by U-boats. Although successful early in the war, ultimately Germany's U-boat fleet suffered heavy casualties, losing 793 U-boats and about 28,000 submariners out of 41,000, a casualty rate of about 70%.

The Imperial Japanese Navy operated the most varied fleet of submarines of any navy, including "Kaiten" crewed torpedoes, midget submarines ( and es), medium-range submarines, purpose-built supply submarines and long-range fleet submarines. They also had submarines with the highest submerged speeds during World War II (s) and submarines that could carry multiple aircraft (s). They were also equipped with one of the most advanced torpedoes of the conflict, the oxygen-propelled Type 95. Nevertheless, despite their technical prowess, Japan chose to use its submarines for fleet warfare, and consequently were relatively unsuccessful, as warships were fast, maneuverable and well-defended compared to merchant ships.

The submarine force was the most effective anti-ship weapon in the American arsenal. Submarines, though only about 2 percent of the U.S. Navy, destroyed over 30 percent of the Japanese Navy, including 8 aircraft carriers, 1 battleship and 11 cruisers. US submarines also destroyed over 60 percent of the Japanese merchant fleet, crippling Japan's ability to supply its military forces and industrial war effort. Allied submarines in the Pacific War destroyed more Japanese shipping than all other weapons combined. This feat was considerably aided by the Imperial Japanese Navy's failure to provide adequate escort forces for the nation's merchant fleet.

During World War II, 314 submarines served in the US Navy, of which nearly 260 were deployed to the Pacific. When the Japanese attacked Hawaii in December 1941, 111 boats were in commission; 203 submarines from the , , and es were commissioned during the war. During the war, 52 US submarines were lost to all causes, with 48 directly due to hostilities. US submarines sank 1,560 enemy vessels, a total tonnage of 5.3 million tons (55% of the total sunk).

The Royal Navy Submarine Service was used primarily in the classic Axis blockade. Its major operating areas were around Norway, in the Mediterranean (against the Axis supply routes to North Africa), and in the Far East. In that war, British submarines sank 2  million tons of enemy shipping and 57 major warships, the latter including 35 submarines. Among these is the only documented instance of a submarine sinking another submarine while both were submerged. This occurred when engaged ; the "Venturer" crew manually computed a successful firing solution against a three-dimensionally maneuvering target using techniques which became the basis of modern torpedo computer targeting systems. Seventy-four British submarines were lost, the majority, forty-two, in the Mediterranean.

The first launch of a cruise missile (SSM-N-8 Regulus) from a submarine occurred in July 1953, from the deck of , a World War II fleet boat modified to carry the missile with a nuclear warhead. "Tunny" and its sister boat, , were the United States' first nuclear deterrent patrol submarines. In the 1950s, nuclear power partially replaced diesel-electric propulsion. Equipment was also developed to extract oxygen from sea water. These two innovations gave submarines the ability to remain submerged for weeks or months. Most of the naval submarines built since that time in the US, the Soviet Union/Russian Federation, Britain, and France have been powered by nuclear reactors.

In 1959–1960, the first ballistic missile submarines were put into service by both the United States () and the Soviet Union () as part of the Cold War nuclear deterrent strategy.

During the Cold War, the US and the Soviet Union maintained large submarine fleets that engaged in cat-and-mouse games. The Soviet Union lost at least four submarines during this period: was lost in 1968 (a part of which the CIA retrieved from the ocean floor with the Howard Hughes-designed ship "Glomar Explorer"), in 1970, in 1986, and in 1989 (which held a depth record among military submarines—). Many other Soviet subs, such as (the first Soviet nuclear submarine, and the first Soviet sub to reach the North Pole) were badly damaged by fire or radiation leaks. The US lost two nuclear submarines during this time: due to equipment failure during a test dive while at its operational limit, and due to unknown causes.

During India's intervention in the Bangladesh Liberation War, the Pakistan Navy's sank the Indian frigate . This was the first sinking by a submarine since World War II. During the same war, , a "Tench"-class submarine on loan to Pakistan from the US, was sunk by the Indian Navy. It was the first submarine combat loss since World War II. In 1982 during the Falklands War, the Argentine cruiser was sunk by the British submarine , the first sinking by a nuclear-powered submarine in war.

Before and during World War II, the primary role of the submarine was anti-surface ship warfare. Submarines would attack either on the surface using deck guns, or submerged using torpedoes. They were particularly effective in sinking Allied transatlantic shipping in both World Wars, and in disrupting Japanese supply routes and naval operations in the Pacific in World War II.

Mine-laying submarines were developed in the early part of the 20th century. The facility was used in both World Wars. Submarines were also used for inserting and removing covert agents and military forces in special operations, for intelligence gathering, and to rescue aircrew during air attacks on islands, where the airmen would be told of safe places to crash-land so the submarines could rescue them. Submarines could carry cargo through hostile waters or act as supply vessels for other submarines.

Submarines could usually locate and attack other submarines only on the surface, although managed to sink with a four torpedo spread while both were submerged. The British developed a specialized anti-submarine submarine in WWI, the R class. After WWII, with the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to hunt each other effectively.

The development of submarine-launched ballistic missile and submarine-launched cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster bombs to nuclear weapons.

The primary defense of a submarine lies in its ability to remain concealed in the depths of the ocean. Early submarines could be detected by the sound they made. Water is an excellent conductor of sound (much better than air), and submarines can detect and track comparatively noisy surface ships from long distances. Modern submarines are built with an emphasis on stealth. Advanced propeller designs, extensive sound-reducing insulation, and special machinery help a submarine remain as quiet as ambient ocean noise, making them difficult to detect. It takes specialized technology to find and attack modern submarines.

Active sonar uses the reflection of sound emitted from the search equipment to detect submarines. It has been used since WWII by surface ships, submarines and aircraft (via dropped buoys and helicopter "dipping" arrays), but it reveals the emitter's position, and is susceptible to counter-measures.

A concealed military submarine is a real threat, and because of its stealth, can force an enemy navy to waste resources searching large areas of ocean and protecting ships against attack. This advantage was vividly demonstrated in the 1982 Falklands War when the British nuclear-powered submarine sank the Argentine cruiser . After the sinking the Argentine Navy recognized that they had no effective defense against submarine attack, and the Argentine surface fleet withdrew to port for the remainder of the war, though an Argentine submarine remained at sea.

Although the majority of the world's submarines are military, there are some civilian submarines, which are used for tourism, exploration, oil and gas platform inspections, and pipeline surveys. Some are also used in illegal activities.

The Submarine Voyage ride opened at Disneyland in 1959, but although it ran under water it was not a true submarine, as it ran on tracks and was open to the atmosphere. The first tourist submarine was , which went into service in 1964 at Expo64. By 1997 there were 45 tourist submarines operating around the world. Submarines with a crush depth in the range of are operated in several areas worldwide, typically with bottom depths around , with a carrying capacity of 50 to 100 passengers.

In a typical operation a surface vessel carries passengers to an offshore operating area and loads them into the submarine. The submarine then visits underwater points of interest such as natural or artificial reef structures. To surface safely without danger of collision the location of the submarine is marked with an air release and movement to the surface is coordinated by an observer in a support craft.

A recent development is the deployment of so-called narco submarines by South American drug smugglers to evade law enforcement detection. Although they occasionally deploy true submarines, most are self-propelled semi-submersibles, where a portion of the craft remains above water at all times. In September 2011, Colombian authorities seized a 16-meter-long submersible that could hold a crew of 5, costing about $2 million. The vessel belonged to FARC rebels and had the capacity to carry at least 7 tonnes of drugs.


All surface ships, as well as surfaced submarines, are in a positively buoyant condition, weighing less than the volume of water they would displace if fully submerged. To submerge hydrostatically, a ship must have negative buoyancy, either by increasing its own weight or decreasing its displacement of water. To control their displacement, submarines have ballast tanks, which can hold varying amounts of water and air.

For general submersion or surfacing, submarines use the forward and aft tanks, called Main Ballast Tanks (MBT), which are filled with water to submerge or with air to surface. Submerged, MBTs generally remain flooded, which simplifies their design, and on many submarines these tanks are a section of interhull space. For more precise and quick control of depth, submarines use smaller Depth Control Tanks (DCT)—also called hard tanks (due to their ability to withstand higher pressure), or trim tanks. The amount of water in depth control tanks can be controlled to change depth or to maintain a constant depth as outside conditions (chiefly water density) change. Depth control tanks may be located either near the submarine's center of gravity, or separated along the submarine body to prevent affecting trim.

When submerged, the water pressure on a submarine's hull can reach for steel submarines and up to for titanium submarines like , while interior pressure remains relatively unchanged. This difference results in hull compression, which decreases displacement. Water density also marginally increases with depth, as the salinity and pressure are higher. This change in density incompletely compensates for hull compression, so buoyancy decreases as depth increases. A submerged submarine is in an unstable equilibrium, having a tendency to either sink or float to the surface. Keeping a constant depth requires continual operation of either the depth control tanks or control surfaces.

Submarines in a neutral buoyancy condition are not intrinsically trim-stable. To maintain desired trim, submarines use forward and aft trim tanks. Pumps can move water between the tanks, changing weight distribution and pointing the sub up or down. A similar system is sometimes used to maintain stability.
The hydrostatic effect of variable ballast tanks is not the only way to control the submarine underwater. Hydrodynamic maneuvering is done by several control surfaces, collectively known as diving planes or hydroplanes, which can be moved to create hydrodynamic forces when a submarine moves at sufficient speed. In the classic cruciform stern configuration, the horizontal stern planes serve the same purpose as the trim tanks, controlling the trim. Most submarines additionally have forward horizontal planes, normally placed on the bow until the 1960s but often on the sail on later designs. These are closer to the center of gravity and are used to control depth with less effect on the trim.

When a submarine performs an emergency surfacing, all depth and trim methods are used simultaneously, together with propelling the boat upwards. Such surfacing is very quick, so the sub may even partially jump out of the water, potentially damaging submarine systems.

Intuitively, the best way to configure the control surfaces at the stern of a submarine would seem to be to give them the shape of a cross when seen from the rear end of the vessel. In this configuration, which remained for long the dominant one, the horizontal planes are used to control the trim and depth and the vertical planes to control sideways maneuvers, just like the rudder of a surface ship. 

Alternatively, however, the rear control surfaces can be combined into what has become known as an x-stern or an x-rudder. Although less intuitive, such a configuration has turned out to have several advantages over the traditional cruciform arrangement. First, it improves maneuvrability, horisontally as well as vertically. Second, the control surfaces are less likely to get damaged when landing on, or departing from, the seabed as well as when mooring and unmooring. Finally, it is safer in that one of the two diagonal lines can counteract the other with respect to vertical as well as horizontal motion if one of them would accidentally get stuck.

The x-stern was first tried in practice in the early 1960s on the USS "Albacore", an experimental submarine of the US Navy. While the arrangement was found to be advantageous, it was nevertheless not used on the US production submarines that followed due to the fact that it requires the use of a computer to manipulate the control surfaces to the desired effect. Instead, the first to use an x-stern operatively was the Swedish Navy with its "Sjöormen" class, the lead submarine of which was launched already in 1967, before the "Albacore" had even finished her test runs. Since it turned out to work very well in practice, all subsequent classes of Swedish submarines ("Näcken", "Västergötland", "Gotland", and "Blekinge" class) have or will come with an x-rudder.

The Kockums shipyard responsible for the design of the x-stern on Swedish submarines eventually exported it to Australia with the "Collins" class as well as to Japan with the "Sōryū" class. With the introduction of the type 212, the German and Italian Navies came to feature it as well. The US Navy with its "Columbia" class, the British Navy with its "Dreadnought" class, and the French Navy with its "Barracuda" class are all about to join the x-stern family. Hence, as judged by the situation in the early 2020s, the x-stern is about to become the dominant technology.

Modern submarines are cigar-shaped. This design, visible in early submarines, is sometimes called a "teardrop hull". It reduces the hydrodynamic drag when submerged, but decreases the sea-keeping capabilities and increases drag while surfaced. Since the limitations of the propulsion systems of early submarines forced them to operate surfaced most of the time, their hull designs were a compromise. Because of the slow submerged speeds of those subs, usually, well below 10 kt (18 km/h), the increased drag for underwater travel was acceptable. Late in World War II, when technology allowed faster and longer submerged operation and increased aircraft surveillance forced submarines to stay submerged, hull designs became teardrop shaped again to reduce drag and noise. was a unique research submarine that pioneered the American version of the teardrop hull form (sometimes referred to as an "Albacore hull") of modern submarines. On modern military submarines, the outer hull is covered with a layer of sound-absorbing rubber, or anechoic plating, to reduce detection.

The occupied pressure hulls of deep-diving submarines such as are spherical instead of cylindrical. This allows a more even distribution of stress at the great depth. A titanium frame is usually affixed to the pressure hull, providing attachment for ballast and trim systems, scientific instrumentation, battery packs, syntactic flotation foam, and lighting.

A raised tower on top of a submarine accommodates the periscope and electronics masts, which can include radio, radar, electronic warfare, and other systems including the snorkel mast. In many early classes of submarines (see history), the control room, or "conn", was located inside this tower, which was known as the "conning tower". Since then, the conn has been located within the hull of the submarine, and the tower is now called the "sail". The conn is distinct from the "bridge", a small open platform in the top of the sail, used for observation during surface operation.

"Bathtubs" are related to conning towers but are used on smaller submarines. The bathtub is a metal cylinder surrounding the hatch that prevents waves from breaking directly into the cabin. It is needed because surfaced submarines have limited freeboard, that is, they lie low in the water. Bathtubs help prevent swamping the vessel.

Modern submarines and submersibles, as well as the oldest ones, usually have a single hull. Large submarines generally have an additional hull or hull sections outside. This external hull, which actually forms the shape of submarine, is called the outer hull ("casing" in the Royal Navy) or light hull, as it does not have to withstand a pressure difference. Inside the outer hull there is a strong hull, or pressure hull, which withstands sea pressure and has normal atmospheric pressure inside.

As early as World War I, it was realized that the optimal shape for withstanding pressure conflicted with the optimal shape for seakeeping and minimal drag, and construction difficulties further complicated the problem. This was solved either by a compromise shape, or by using two hulls: internal for holding pressure, and external for optimal shape. Until the end of World War II, most submarines had an additional partial cover on the top, bow and stern, built of thinner metal, which was flooded when submerged. Germany went further with the Type XXI, a general predecessor of modern submarines, in which the pressure hull was fully enclosed inside the light hull, but optimized for submerged navigation, unlike earlier designs that were optimized for surface operation.
After World War II, approaches split. The Soviet Union changed its designs, basing them on German developments. All post-World War II heavy Soviet and Russian submarines are built with a double hull structure. American and most other Western submarines switched to a primarily single-hull approach. They still have light hull sections in the bow and stern, which house main ballast tanks and provide a hydrodynamically optimized shape, but the main cylindrical hull section has only a single plating layer. Double hulls are being considered for future submarines in the United States to improve payload capacity, stealth and range.

The pressure hull is generally constructed of thick high-strength steel with a complex structure and high strength reserve, and is separated with watertight bulkheads into several compartments. There are also examples of more than two hulls in a submarine, like the , which has two main pressure hulls and three smaller ones for control room, torpedoes and steering gear, with the missile launch system between the main hulls.

The dive depth cannot be increased easily. Simply making the hull thicker increases the weight and requires reduction of onboard equipment weight, ultimately resulting in a "bathyscaphe". This is acceptable for civilian research submersibles, but not military submarines.

WWI submarines had hulls of carbon steel, with a maximum depth. During WWII, high-strength alloyed steel was introduced, allowing depths. High-strength alloy steel remains the primary material for submarines today, with depths, which cannot be exceeded on a military submarine without design compromises. To exceed that limit, a few submarines were built with titanium hulls. Titanium can be stronger than steel, lighter, and is not ferromagnetic, important for stealth. Titanium submarines were built by the Soviet Union, which developed specialized high-strength alloys. It has produced several types of titanium submarines. Titanium alloys allow a major increase in depth, but other systems must be redesigned to cope, so test depth was limited to for the , the deepest-diving combat submarine. An may have successfully operated at , though continuous operation at such depths would produce excessive stress on many submarine systems. Titanium does not flex as readily as steel, and may become brittle after many dive cycles. Despite its benefits, the high cost of titanium construction led to the abandonment of titanium submarine construction as the Cold War ended. Deep-diving civilian submarines have used thick acrylic pressure hulls.

The deepest deep-submergence vehicle (DSV) to date is "Trieste". On 5 October 1959, "Trieste" departed San Diego for Guam aboard the freighter "Santa Maria" to participate in "Project Nekton", a series of very deep dives in the Mariana Trench. On 23 January 1960, "Trieste" reached the ocean floor in the Challenger Deep (the deepest southern part of the Mariana Trench), carrying Jacques Piccard (son of Auguste) and Lieutenant Don Walsh, USN. This was the first time a vessel, manned or unmanned, had reached the deepest point in the Earth's oceans. The onboard systems indicated a depth of , although this was later revised to and more accurate measurements made in 1995 have found the Challenger Deep slightly shallower, at .

Building a pressure hull is difficult, as it must withstand pressures at its required diving depth. When the hull is perfectly round in cross-section, the pressure is evenly distributed, and causes only hull compression. If the shape is not perfect, the hull is bent, with several points heavily strained. Inevitable minor deviations are resisted by stiffener rings, but even a one-inch (25 mm) deviation from roundness results in over 30 percent decrease of maximal hydrostatic load and consequently dive depth. The hull must therefore be constructed with high precision. All hull parts must be welded without defects, and all joints are checked multiple times with different methods, contributing to the high cost of modern submarines. (For example, each attack submarine costs US$2.6 billion, over US$200,000 per ton of displacement.)

The first submarines were propelled by humans. The first mechanically driven submarine was the 1863 French , which used compressed air for propulsion. Anaerobic propulsion was first employed by the Spanish "Ictineo II" in 1864, which used a solution of zinc, manganese dioxide, and potassium chlorate to generate sufficient heat to power a steam engine, while also providing oxygen for the crew. A similar system was not employed again until 1940 when the German Navy tested a hydrogen peroxide-based system, the Walter turbine, on the experimental V-80 submarine and later on the naval and type XVII submarines; the system was further developed for the British , completed in 1958.

Until the advent of nuclear marine propulsion, most 20th-century submarines used electric motors and batteries for running underwater and combustion engines on the surface, and for battery recharging. Early submarines used gasoline (petrol) engines but this quickly gave way to kerosene (paraffin) and then diesel engines because of reduced flammability and, with diesel, improved fuel-efficiency and thus also greater range. A combination of diesel and electric propulsion became the norm. 

Initially, the combustion engine and the electric motor were in most cases connected to the same shaft so that both could directly drive the propeller. The combustion engine was placed at the front end of the stern section with the electric motor behind it followed by the propeller shaft. The engine was connected to the motor by a clutch and the motor in turn connected to the propeller shaft by another clutch. 

With only the rear clutch engaged, the electric motor could drive the propeller, as required for fully submerged operation. With both clutches engaged, the combustion engine could drive the propeller, as was possible when operating on the surface or, at a later stage, when snorkeling. The electric motor would in this case serve as a generator to charge the batteries or, if no charging was needed, be allowed to rotate freely. With only the front clutch engaged, the combustion engine could drive the electric motor as a generator for charging the batteries without simultaneously forcing the propeller to move.

The motor could have multiple armatures on the shaft, which could be electrically coupled in series for slow speed and in parallel for high speed (these connections were called "group down" and "group up", respectively).

While most early submarines used a direct mechanical connection between the combustion engine and the propeller, an alternative solution was considered as well as implemented at a very early stage. That solution consists in first converting the work of the combustion engine into electric energy via a dedicated generator. This energy is then used to drive the propeller via the electric motor and, to the extent required, for charging the batteries. In this configuration, the electric motor is thus responsible for driving the propeller at all times, regardless of whether air is available so that the combustion engine can also be used or not.

Among the pioneers of this alternative solution was the very first submarine of the Swedish Navy, (later renamed "Ub no 1"), launched in 1904. While its design was generally inspired by the first submarine commissioned by the US Navy, USS "Holland", it deviated from the latter in at least three significant ways: by adding a periscope, by replacing the gasoline engine by a semidiesel engine (a hot-bulb engine primarily meant to be fueled by kerosene, later replaced by a true diesel engine) and by severing the mechanical link between the combustion engine and the propeller by instead letting the former drive a dedicated generator. By so doing, it took three significant steps toward what was eventually to become the dominant technology for conventional (i.e., non-nuclear) submarines.
In the following years, the Swedish Navy added another seven submarines in three different classes (, , and ) using the same propulsion technology but fitted with true diesel engines rather than semidiesels from the outset. Since by that time, the technology was usually based on the diesel engine rather than some other type of combustion engine, it eventually came to be known as diesel-electric transmission.

Like many other early submarines, those initially designed in Sweden were quite small (less than 200 tonnes) and thus confined to littoral operation. When the Swedish Navy wanted to add larger vessels, capable of operating further from the shore, their designs were purchased from companies abroad that already had the required experience: first Italian (Fiat-Laurenti) and later German (A.G. Weser and IvS). As a side-effect, the diesel-electric transmission was temporarily abandoned. 

However, diesel-electric transmission was immediately reintroduced when Sweden began designing its own submarines again in the mid 1930s. From that point onwards, it has been consistently used for all new classes of Swedish submarines, albeit supplemented by air-independent propulsion (AIP) as provided by Stirling engines beginning with HMS "Näcken" in 1988.
Another early adopter of diesel-electric transmission was the US Navy, whose Bureau of Engineering proposed its use in 1928. It was subsequently tried in the S-class submarines , , and before being put into production with the "Porpoise" class of the 1930s. From that point onwards, it continued to be used on most US conventional submarines. 

Apart from the British U-class and some submarines of the Imperial Japanese Navy that used separate diesel generators for low speed running, few navies other than those of Sweden and the US made much use of diesel-electric transmission before 1945. After World War II, by contrast, it gradually became the dominant mode of propulsion for conventional submarines. However, its adoption was not always swift. Notably, the Soviet Navy did not introduce diesel-electric transmission on its conventional submarines until 1980 with its "Paltus" class.

If diesel-electric transmission had only brought advantages and no disadvantages in comparison with a system that mechanically connects the diesel engine to the propeller, it would undoubtedly have become dominant much earlier. The disadvantages include the following:




The reason why diesel-electric transmission has become the dominant alternative in spite of these disadvantages is of course that it also comes with many advantages and that, on balance, these have eventually been found to be more important. The advantages include the following:







During World War II the Germans experimented with the idea of the "schnorchel" (snorkel) from captured Dutch submarines but did not see the need for them until rather late in the war. The "schnorchel" is a retractable pipe that supplies air to the diesel engines while submerged at periscope depth, allowing the boat to cruise and recharge its batteries while maintaining a degree of stealth. 

Especially as first implemented however, it turned out to be far from a perfect solution. There were problems with the device's valve sticking shut or closing as it dunked in rough weather. Since the system used the entire pressure hull as a buffer, the diesels would instantaneously suck huge volumes of air from the boat's compartments, and the crew often suffered painful ear injuries. Speed was limited to , lest the device snap from stress. The "schnorchel" also created noise that made the boat easier to detect with sonar, yet more difficult for the on-board sonar to detect signals from other vessels. Finally, allied radar eventually became sufficiently advanced that the "schnorchel" mast could be detected beyond visual range.

While the snorkel renders a submarine far less detectable, it is thus not perfect. In clear weather, diesel exhausts can be seen on the surface to a distance of about three miles, while "periscope feather" (the wave created by the snorkel or periscope moving through the water) is visible from far off in calm sea conditions. Modern radar is also capable of detecting a snorkel in calm sea conditions.

The problem of the diesels causing a vacuum in the submarine when the head valve is submerged still exists in later model diesel submarines but is mitigated by high-vacuum cut-off sensors that shut down the engines when the vacuum in the ship reaches a pre-set point. Modern snorkel induction masts have a fail-safe design using compressed air, controlled by a simple electrical circuit, to hold the "head valve" open against the pull of a powerful spring. Seawater washing over the mast shorts out exposed electrodes on top, breaking the control, and shutting the "head valve" while it is submerged. US submarines did not adopt the use of snorkels until after WWII.

During World War II, German Type XXI submarines (also known as ""Elektroboote"") were the first submarines designed to operate submerged for extended periods. Initially they were to carry hydrogen peroxide for long-term, fast air-independent propulsion, but were ultimately built with very large batteries instead. At the end of the War, the British and Soviets experimented with hydrogen peroxide/kerosene (paraffin) engines that could run surfaced and submerged. The results were not encouraging. Though the Soviet Union deployed a class of submarines with this engine type (codenamed by NATO), they were considered unsuccessful.

The United States also used hydrogen peroxide in an experimental midget submarine, X-1. It was originally powered by a hydrogen peroxide/diesel engine and battery system until an explosion of her hydrogen peroxide supply on 20 May 1957. X-1 was later converted to use diesel-electric drive.

Today several navies use air-independent propulsion. Notably Sweden uses Stirling technology on the and s. The Stirling engine is heated by burning diesel fuel with liquid oxygen from cryogenic tanks. A newer development in air-independent propulsion is hydrogen fuel cells, first used on the German Type 212 submarine, with nine 34 kW or two 120 kW cells. Fuel cells are also used in the new Spanish s although with the fuel stored as ethanol and then converted into hydrogen before use.

One new technology that is being introduced starting with the Japanese Navy's eleventh "Sōryū"-class submarine (JS "Ōryū") is a more modern battery, the lithium-ion battery. These batteries have about double the electric storage of traditional batteries, and by changing out the lead-acid batteries in their normal storage areas plus filling up the large hull space normally devoted to AIP engine and fuel tanks with many tons of lithium-ion batteries, modern submarines can actually return to a "pure" diesel-electric configuration yet have the added underwater range and power normally associated with AIP equipped submarines.

Steam power was resurrected in the 1950s with a nuclear-powered steam turbine driving a generator. By eliminating the need for atmospheric oxygen, the time that a submarine could remain submerged was limited only by its food stores, as breathing air was recycled and fresh water distilled from seawater. More importantly, a nuclear submarine has unlimited range at top speed. This allows it to travel from its operating base to the combat zone in a much shorter time and makes it a far more difficult target for most anti-submarine weapons. Nuclear-powered submarines have a relatively small battery and diesel engine/generator powerplant for emergency use if the reactors must be shut down.

Nuclear power is now used in all large submarines, but due to the high cost and large size of nuclear reactors, smaller submarines still use diesel-electric propulsion. The ratio of larger to smaller submarines depends on strategic needs. The US Navy, French Navy, and the British Royal Navy operate only nuclear submarines, which is explained by the need for distant operations. Other major operators rely on a mix of nuclear submarines for strategic purposes and diesel-electric submarines for defense. Most fleets have no nuclear submarines, due to the limited availability of nuclear power and submarine technology.

Diesel-electric submarines have a stealth advantage over their nuclear counterparts. Nuclear submarines generate noise from coolant pumps and turbo-machinery needed to operate the reactor, even at low power levels. Some nuclear submarines such as the American can operate with their reactor coolant pumps secured, making them quieter than electric subs. A conventional submarine operating on batteries is almost completely silent, the only noise coming from the shaft bearings, propeller, and flow noise around the hull, all of which stops when the sub hovers in mid-water to listen, leaving only the noise from crew activity. Commercial submarines usually rely only on batteries, since they operate in conjunction with a mother ship.

Several serious nuclear and radiation accidents have involved nuclear submarine mishaps. The reactor accident in 1961 resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. The accident in 1985 resulted in 10 fatalities and 49 other radiation injuries.

Oil-fired steam turbines powered the British K-class submarines, built during World War I and later, to give them the surface speed to keep up with the battle fleet. The K-class subs were not very successful, however.

Toward the end of the 20th century, some submarines—such as the British "Vanguard" class—began to be fitted with pump-jet propulsors instead of propellers. Though these are heavier, more expensive, and less efficient than a propeller, they are significantly quieter, providing an important tactical advantage.

The success of the submarine is inextricably linked to the development of the torpedo, invented by Robert Whitehead in 1866. His invention is essentially the same now as it was 140 years ago. Only with self-propelled torpedoes could the submarine make the leap from novelty to a weapon of war. Until the perfection of the guided torpedo, multiple "straight-running" torpedoes were required to attack a target. With at most 20 to 25 torpedoes stored on board, the number of attacks was limited. To increase combat endurance most World War I submarines functioned as submersible gunboats, using their deck guns against unarmed targets, and diving to escape and engage enemy warships. The importance of guns encouraged the development of the unsuccessful Submarine Cruiser such as the French and the Royal Navy's and M-class submarines. With the arrival of Anti-submarine warfare (ASW) aircraft, guns became more for defense than attack. A more practical method of increasing combat endurance was the external torpedo tube, loaded only in port.

The ability of submarines to approach enemy harbours covertly led to their use as minelayers. Minelaying submarines of World War I and World War II were specially built for that purpose. Modern submarine-laid mines, such as the British Mark 5 Stonefish and Mark 6 Sea Urchin, can be deployed from a submarine's torpedo tubes.

After World War II, both the US and the USSR experimented with submarine-launched cruise missiles such as the SSM-N-8 Regulus and P-5 Pyatyorka. Such missiles required the submarine to surface to fire its missiles. They were the forerunners of modern submarine-launched cruise missiles, which can be fired from the torpedo tubes of submerged submarines, for example the US BGM-109 Tomahawk and Russian RPK-2 Viyuga and versions of surface-to-surface anti-ship missiles such as the Exocet and Harpoon, encapsulated for submarine launch. Ballistic missiles can also be fired from a submarine's torpedo tubes, for example missiles such as the anti-submarine SUBROC. With internal volume as limited as ever and the desire to carry heavier warloads, the idea of the external launch tube was revived, usually for encapsulated missiles, with such tubes being placed between the internal pressure and outer streamlined hulls.

The strategic mission of the SSM-N-8 and the P-5 was taken up by submarine-launched ballistic missile beginning with the US Navy's Polaris missile, and subsequently the Poseidon and Trident missiles.

Germany is working on the torpedo tube-launched short-range IDAS missile, which can be used against ASW helicopters, as well as surface ships and coastal targets.

A submarine can have a variety of sensors, depending on its missions. Modern military submarines rely almost entirely on a suite of passive and active sonars to locate targets. Active sonar relies on an audible "ping" to generate echoes to reveal objects around the submarine. Active systems are rarely used, as doing so reveals the sub's presence. Passive sonar is a set of sensitive hydrophones set into the hull or trailed in a towed array, normally trailing several hundred feet behind the sub. The towed array is the mainstay of NATO submarine detection systems, as it reduces the flow noise heard by operators. Hull mounted sonar is employed in addition to the towed array, as the towed array can't work in shallow depth and during maneuvering. In addition, sonar has a blind spot "through" the submarine, so a system on both the front and back works to eliminate that problem. As the towed array trails behind and below the submarine, it also allows the submarine to have a system both above and below the thermocline at the proper depth; sound passing through the thermocline is distorted resulting in a lower detection range.

Submarines also carry radar equipment to detect surface ships and aircraft. Submarine captains are more likely to use radar detection gear than active radar to detect targets, as radar can be detected far beyond its own return range, revealing the submarine. Periscopes are rarely used, except for position fixes and to verify a contact's identity.

Civilian submarines, such as the or the Russian "Mir" submersibles, rely on small active sonar sets and viewing ports to navigate. The human eye cannot detect sunlight below about underwater, so high intensity lights are used to illuminate the viewing area.

Early submarines had few navigation aids, but modern subs have a variety of navigation systems. Modern military submarines use an inertial guidance system for navigation while submerged, but drift error unavoidably builds over time. To counter this, the crew occasionally uses the Global Positioning System to obtain an accurate position. The periscope—a retractable tube with a prism system that provides a view of the surface—is only used occasionally in modern submarines, since the visibility range is short. The and s use photonics masts rather than hull-penetrating optical periscopes. These masts must still be deployed above the surface, and use electronic sensors for visible light, infrared, laser range-finding, and electromagnetic surveillance. One benefit to hoisting the mast above the surface is that while the mast is above the water the entire sub is still below the water and is much harder to detect visually or by radar.

Military submarines use several systems to communicate with distant command centers or other ships. One is VLF (very low frequency) radio, which can reach a submarine either on the surface or submerged to a fairly shallow depth, usually less than . ELF (extremely low frequency) can reach a submarine at greater depths, but has a very low bandwidth and is generally used to call a submerged sub to a shallower depth where VLF signals can reach. A submarine also has the option of floating a long, buoyant wire antenna to a shallower depth, allowing VLF transmissions by a deeply submerged boat.

By extending a radio mast, a submarine can also use a "burst transmission" technique. A burst transmission takes only a fraction of a second, minimizing a submarine's risk of detection.

To communicate with other submarines, a system known as Gertrude is used. Gertrude is basically a sonar telephone. Voice communication from one submarine is transmitted by low power speakers into the water, where it is detected by passive sonars on the receiving submarine. The range of this system is probably very short, and using it radiates sound into the water, which can be heard by the enemy.

Civilian submarines can use similar, albeit less powerful systems to communicate with support ships or other submersibles in the area.

With nuclear power or air-independent propulsion, submarines can remain submerged for months at a time. Conventional diesel submarines must periodically resurface or run on snorkel to recharge their batteries. Most modern military submarines generate breathing oxygen by electrolysis of water (using a device called an "Electrolytic Oxygen Generator"). Atmosphere control equipment includes a CO scrubber, which uses an amine absorbent to remove the gas from air and diffuse it into waste pumped overboard. A machine that uses a catalyst to convert carbon monoxide into carbon dioxide (removed by the CO scrubber) and bonds hydrogen produced from the ship's storage battery with oxygen in the atmosphere to produce water, is also used. An atmosphere monitoring system samples the air from different areas of the ship for nitrogen, oxygen, hydrogen, R-12 and R-114 refrigerants, carbon dioxide, carbon monoxide, and other gases. Poisonous gases are removed, and oxygen is replenished by use of an oxygen bank located in a main ballast tank. Some heavier submarines have two oxygen bleed stations (forward and aft). The oxygen in the air is sometimes kept a few percent less than atmospheric concentration to reduce fire risk.

Fresh water is produced by either an evaporator or a reverse osmosis unit. The primary use for fresh water is to provide feedwater for the reactor and steam propulsion plants. It is also available for showers, sinks, cooking and cleaning once propulsion plant needs have been met. Seawater is used to flush toilets, and the resulting "black water" is stored in a sanitary tank until it is blown overboard using pressurized air or pumped overboard by using a special sanitary pump. The blackwater-discharge system is difficult to operate, and the German Type VIIC boat was lost with casualties because of human error while using this system. Water from showers and sinks is stored separately in "grey water" tanks and discharged overboard using drain pumps.

Trash on modern large submarines is usually disposed of using a tube called a Trash Disposal Unit (TDU), where it is compacted into a galvanized steel can. At the bottom of the TDU is a large ball valve. An ice plug is set on top of the ball valve to protect it, the cans atop the ice plug. The top breech door is shut, and the TDU is flooded and equalized with sea pressure, the ball valve is opened and the cans fall out assisted by scrap iron weights in the cans. The TDU is also flushed with seawater to ensure it is completely empty and the ball valve is clear before closing the valve.

A typical nuclear submarine has a crew of over 80; conventional boats typically have fewer than 40. The conditions on a submarine can be difficult because crew members must work in isolation for long periods of time, without family contact. Submarines normally maintain radio silence to avoid detection. Operating a submarine is dangerous, even in peacetime, and many submarines have been lost in accidents.

Most navies prohibited women from serving on submarines, even after they had been permitted to serve on surface warships. The Royal Norwegian Navy became the first navy to allow women on its submarine crews in 1985. The Royal Danish Navy allowed female submariners in 1988. Others followed suit including the Swedish Navy (1989), the Royal Australian Navy (1998), the Spanish Navy (1999), the German Navy (2001) and the Canadian Navy (2002). In 1995, Solveig Krey of the Royal Norwegian Navy became the first female officer to assume command on a military submarine, HNoMS "Kobben".

On 8 December 2011, British Defence Secretary Philip Hammond announced that the UK's ban on women in submarines was to be lifted from 2013. Previously there were fears that women were more at risk from a build-up of carbon dioxide in the submarine. But a study showed no medical reason to exclude women, though pregnant women would still be excluded. Similar dangers to the pregnant woman and her fetus barred women from submarine service in Sweden in 1983, when all other positions were made available for them in the Swedish Navy. Today, pregnant women are still not allowed to serve on submarines in Sweden. However, the policymakers thought that it was discriminatory with a general ban and demanded that women should be tried on their individual merits and have their suitability evaluated and compared to other candidates. Further, they noted that a woman complying with such high demands is unlikely to become pregnant. In May 2014, three women became the RN's first female submariners.

Women have served on US Navy surface ships since 1993, and , began serving on submarines for the first time. Until presently, the Navy allowed only three exceptions to women being on board military submarines: female civilian technicians for a few days at most, women midshipmen on an overnight during summer training for Navy ROTC and Naval Academy, and family members for one-day dependent cruises. In 2009, senior officials, including then-Secretary of the Navy Ray Mabus, Joint Chief of Staff Admiral Michael Mullen, and Chief of Naval Operations Admiral Gary Roughead, began the process of finding a way to implement women on submarines. The US Navy rescinded its "no women on subs" policy in 2010.

Both the US and British navies operate nuclear-powered submarines that deploy for periods of six months or longer. Other navies that permit women to serve on submarines operate conventionally powered submarines, which deploy for much shorter periods—usually only for a few months. Prior to the change by the US, no nation using nuclear submarines permitted women to serve on board.

In 2011, the first class of female submarine officers graduated from Naval Submarine School's Submarine Officer Basic Course (SOBC) at the Naval Submarine Base New London. Additionally, more senior ranking and experienced female supply officers from the surface warfare specialty attended SOBC as well, proceeding to fleet Ballistic Missile (SSBN) and Guided Missile (SSGN) submarines along with the new female submarine line officers beginning in late 2011. By late 2011, several women were assigned to the "Ohio"-class ballistic missile submarine . On 15 October 2013, the US Navy announced that two of the smaller "Virginia"-class attack submarines, and , would have female crew-members by January 2015.
In 2020, Japan's national naval submarine academy accepted its first female candidate.

In an emergency, submarines can transmit a signal to other ships. The crew can use Submarine Escape Immersion Equipment to abandon the submarine. The crew can avoid lung injury from over-expansion of air in the lungs due to the pressure change known as pulmonary barotrauma by exhaling during the ascent. Following escape from a pressurized submarine, the crew is at risk of developing decompression sickness. An alternative escape means is via a deep-submergence rescue vehicle that can dock onto the disabled submarine.


General history
Culture
Submarines before 1914
1900/Russo-Japanese War 1904–1905
World War II

Cold War



</doc>
<doc id="28827" url="https://en.wikipedia.org/wiki?curid=28827" title="Second Epistle to the Thessalonians">
Second Epistle to the Thessalonians

The Second Epistle to the Thessalonians, commonly referred to as Second Thessalonians or 2 Thessalonians is a book from the New Testament of the Christian Bible. It is traditionally attributed to Paul the Apostle, with Timothy as a co-author. Modern biblical scholarship is divided on whether the epistle was written by Paul; many scholars reject its authenticity based on what they see as differences in style and theology between this and the First Epistle to the Thessalonians.

Scholars who support its authenticity view it as having been written around 51–52 AD, shortly after the First Epistle. Those who see it as a later composition assign a date of around 80–115 AD.

The authenticity of this epistle is still in widespread dispute. As Professor Ernest Best, New Testament scholar, explains the problem;

The structures of the two letters (to which Best refers) include opening greetings ("1 Thess." 1:1, "2 Thess." 1:1–2) and closing benedictions ("1 Thess." 5:28, "2 Thess." 3:16d–18) which frame two, balancing, sections (AA'). In "2 Thessalonians" these begin with similar successions of nine Greek words, at 1:3 and 2:13. The opening letter section (1:3–2:12) itself comprises two halves, 1:3–12 (where the introductory piece, A, is 1:3–5; the first development, B, is 1:6–10; and the paralleling and concluding development, B', is 1:11–12) and 2:1–12 (with pieces: A 2:1–4, B 2:5–7, B' 2:8–12).

The second, balancing, letter section (2:13–3:16c) also comprises two halves: 2:13–3:5 (with pieces: A 2:13–14, B 2:15–17, B' 3:1–5) and 3:6–16c (with pieces: A 3:6–9, B 3:10–12, B' 3:13-16c). Of the twelve pieces in "2 Thessalonians" seven begin with 'brother' introductions. Of the eighteen pieces in "1 Thessalonians" fourteen begin with 'brother' introductions. In both letters, the sections balance in size and focus, and in many details. In "2 Thessalonians", in 2:5 and 3:10, for example, there is a structural balance of the use of 'when I was with you...' and 'when we were with you...'.

One piece of evidence for the authenticity of the epistle is that it was included in Marcion's canon and the Muratorian fragment. It was also mentioned by name by Irenaeus, and quoted by Ignatius, Justin, and Polycarp.

G. Milligan argued that a church which possessed an authentic letter of Paul would be unlikely to accept a fake addressed to them. So also Colin Nicholl who has put forward a substantial argument for the authenticity of Second Thessalonians. He points out that 'the pseudonymous view is ... more vulnerable than most of its advocates conceded. ... The lack of consensus regarding a date and destination ... reflects a dilemma for this position: on the one hand, the date needs to be early enough for the letter to be have been accepted as Pauline ... [on] the other hand, the date and destination need to be such that the author could be confident that no contemporary of 1 Thessalonians ... could have exposed 2 Thessalonians as a ... forgery.'. pp. 5–6

Another scholar who argues for the authenticity of this letter is Jerome Murphy-O'Connor. Admitting that there are stylistic problems between "Second Thessalonians" and "First Thessalonians", he argues that part of the problem is due to the composite nature of "First Thessalonians" (Murphy-O'Connor is only one of many scholars who argue that the current text of "Second Thessalonians" is the product of merging two or more authentic letters of Paul). Once the text of this interpolated letter is removed and the two letters compared, Murphy-O'Connor asserts that this objection is "drastically weakened", and concludes, "The arguments against the authenticity of 2 Thessalonians are so weak that it is preferable to accept the traditional ascription of the letter to Paul."

Those who believe Paul was the author of "Second Thessalonians" also note how Paul drew attention to the authenticity of the letter by signing it himself: "I, Paul, write this greeting with my own hand, which is how I write in every letter.". Bruce Metzger writes, "Paul calls attention to his signature, which was added by his own hand as a token of genuineness to every letter of his (3:17)."

Other scholars who hold to authenticity include Beale, Green, Jones, Morris, Witherington, and Kretzmann. According to Leon Moris in 1986, the majority of current scholars at that time still held to Paul's authorship of 2 Thessalonians.

At least as early as 1798, when J.E.C. Schmidt published his opinion, Paul's authorship of this epistle was questioned. More recent challenges to this traditional belief came from scholars such as William Wrede in 1903 and Alfred Loisy in 1933, who challenged the traditional view of the authorship. 

Regarding Nicholl's argument for authenticity, on the one hand, it is worth noting that at least some forged Pauline letters were written well after a date modern scholars might deem early enough for the letter to be considered Pauline, such as the Third Epistle to the Corinthians, estimated to have been written around 160-170 CE; forgers were not forced to write close in time to the writers they imitated. On the other hand, it is not clear that a forger would need to ensure his writing was not contemporaneous with 1 Thessalonians if he was not actually writing the letter to Thessalonica; furthermore, if Nicholls is correct in believing 2 Thessalonians to be authentic, then Paul in 2 Thessalonians 2:2 provides evidence that forgeries in his name already existed in his own lifetime, discrediting his argument that forgers would take care to write far enough apart in time to ensure contemporaries could not denounce the forgery.

In his book "Forged", New Testament scholar Bart D. Ehrman puts forward some of the most common arguments against the authenticity of 2 Thessalonians. For example, he argues that the views concerning the Second Coming of Christ expressed in 2 Thessalonians differ so strikingly from those found in 1 Thessalonians that they cannot be written by the same author:

Ehrman also argues that the self-referencing signature at the end of 2 Thessalonians was likely used by the forger of the epistle to authenticate what he had written. If Paul had actually written the letter, Ehrman reasons, he would not have needed to include such an autograph:

Many modern scholars agree with Ehrman that 2 Thessalonians was not written by Paul but by an associate or disciple after his death. See, for example, Beverly Roberts Gaventa, Vincent Smiles, Udo Schnelle, Eugene Boring, and Joseph Kelly. Norman Perrin observes, "The best understanding of 2 Thessalonians… is to see it as a deliberate imitation of 1 Thessalonians, updating the apostle's thought." Perrin bases this claim on his hypothesis that prayer at the time usually treated God the Father as ultimate judge, rather than Jesus.

Thessalonica was the second city in Europe where Paul helped to create an organized Christian community. At some point after the first letter was sent, probably soon, some of the Thessalonicans grew concerned over whether those who had died would share in the parousia. This letter was written in response to this concern. The problem then arises, as Raymond Brown points out, whether this letter is an authentic writing of Paul written by one of his followers in his name.

If this letter is authentic, then it might have been written soon after Paul's first letter to this community—or possibly years later. Brown notes that Paul "most likely visited Thessalonica several times in his journeys to Macedonia". However, if the letter is not authentic, Brown notes that "in some ways interpretation becomes more complex." Brown believes that the majority of scholars who advocate pseudonymity would place it towards the end of the first century, the same time that Revelation was written. These scholars emphasize the appearance of "man of sin" in the second chapter of this letter, whether this personage is identified with the Antichrist of 1 John and Revelation, or with a historical person like Caligula.

The traditional view is that the second epistle to the Thessalonians was probably written from Corinth not many months after the first.

Biblical commentator and pastor John Macarthur writes, "The emphasis is on how to maintain a church with an effective testimony in proper response to sound eschatology and obedience to the truth."

Paul opens the letter praising this church for their faithfulness and perseverance in the face of persecution:

"We ought always to give thanks to God for you, brethren, as is only fitting, because your faith is greatly enlarged, and the love of each one of you toward one another grows ever greater; therefore, we ourselves speak proudly of you among the churches of God for your perseverance and faith in the midst of all your persecutions and afflictions which you endure" (2 Thess 1:3–5 [NASB]).

The letter contains a whole chapter regarding the second advent of Christ, among other themes and instructions.

From the inference of 2:1–2, the Thessalonians were faced with a false teaching, saying that Christ had already returned. This error is corrected in chapter 2 (2:1–12), where Paul tells the Thessalonians that a great tribulation must occur before Christ's return. Seeing as how this series of events has not yet happened, his argument reads, Christ cannot have returned yet. He then expresses thanks that his readers were the elect of God, chosen for salvation and saved by His grace through faith, and thus not susceptible to the deception of the "Great Apostasy," (2 Thess 2:13–14) first mentioned here as is the "Katechon" (2 Thess 2:6–7).

In 2 Thess 2:15, Paul instructs his readers to "[h]old fast to the traditions (, ) which you were taught, whether by word of mouth or by our letter." Quoting this verse, in his "On the Holy Spirit", Basil the Great writes, "These [traditions] have been passed on by word of mouth from Paul or from the other apostles, without necessarily being written down," and mentions the Trinitarian confession of faith as an example of "unwritten tradition". Cyril of Jerusalem shares a similar view in his "Catechetical Lectures", argues that the traditions stated by Paul should be preserved and memorized, at a minimum in the form of the Creed. In his homily on this verse, John Chrysostom differentiates oral tradition from written tradition. At that time, the oral tradition has been defined as the "tradition" and the written tradition as "Scripture", united together in "the authenticity of their apostolic origin". Everett Ferguson says Paul's reference to tradition implicates that "what was delivered was from the Lord", and John Stott calls the tradition (, "paradosis") "apostolic 'tradition.

The letter continues by encouraging the Thessalonian church to stand firm in their faith, and to "keep away from every brother who leads an unruly life and not according to the tradition which you received from us... do not associate with him, so that he will be put to shame. Yet do not regard him as an enemy, but admonish him as a brother" (2 Thess 3:6–7, 14–15).

Paul ends this letter by saying, "I, Paul, write this greeting with my own hand, and this is a distinguishing mark in every letter; this is the way I write. The grace of our Lord Jesus Christ be with you all" (2 Thess 3:17–18). Macarthur writes, "Paul added an identifying signature (cf. 1 Cor. 16:21; Col. 4:18) so his readers could be sure he was truly the author."

A passage from this book reading "For even when we were with you, this we commanded you, that if any would not work, neither should he eat", (2 Thess. 3:10), was later adapted by Vladimir Lenin as an adage of the Soviet Union, He who does not work, neither shall he eat.



Online translations of the Second Epistle to the Thessalonians:

Exegetical Papers on Second Thessalonians:


</doc>
<doc id="28828" url="https://en.wikipedia.org/wiki?curid=28828" title="Poetry slam">
Poetry slam

A poetry slam is a competition arts event, in which poets perform spoken word poetry before a live audience and a panel of judges. Culturally, poetry slams are a break with the past image of poetry as an elitist or rigid art form. While formats can vary, slams are often loud and lively, with audience participation, cheering and dramatic delivery. Hip-hop music and urban culture are strong influences, and backgrounds of participants tend to be diverse.

Poetry slams began in Chicago in 1984, with the first slam competition designed to move poetry recitals from academia to a popular audience. American poet Marc Smith, believing the poetry scene at the time was "too structured and stuffy", began experimenting by attending open microphone poetry readings, and then turning them into slams by introducing the element of competition.

The performances at a poetry slam are judged as much on enthusiasm and style as content, and poets may compete as individuals or in teams. The judging is often handled by a panel of judges, typically five, who are usually selected from the audience. Sometimes the poets are judged by audience response.

American poet Marc Smith is credited with starting the poetry slam at the Get Me High Lounge in Chicago in November 1984. In July 1986, the original slam moved to its permanent home, the Green Mill Jazz Club. In 1987 the Ann Arbor Poetry Slam was founded by Vince Keuter and eventually made its home at the Heidelberg (moving later 2010, 2013, and 2015 to its new home at Espresso Royale). In August 1988, the first poetry slam held in New York City was hosted by Bob Holman at the Nuyorican Poet's Cafe. In 1990, the first National Poetry Slam took place at Fort Mason, San Francisco. This slam included teams from Chicago and San Francisco, and an individual poet from New York. Soon afterward, poetry slam increased popularity allowed some poets to make full-time careers in performance and competition, touring the United States and eventually the world.

In 1999, National Poetry Slam, held in major cities each year, was in Chicago. The event was covered nationally by The New York Times and 60 Minutes (CBS). 60 Minutes taped a 20 segment on Slam Poetry with live poetry scenes at Chopin Theatre. 

In 2001, the grounding of aircraft following the September 11 attacks left a number of performers stranded in cities they had been performing in. After the attacks, a new wave of poetry slam started within New York City with a community focus on poets coming together to speak about the terrorist attacks.

, the National Poetry Slam featured 72 certified teams, culminating in five days of competition.

Today, there are poetry slam competitions in a number of countries around the globe.

Poetry Slam Inc. sanctions three major annual poetry competitions (for poets 18+) on a national and international scale: the National Poetry Slam (NPS), the individual World Poetry Slam (iWPS), and the Women of the World Poetry Slam (WoWPS)

In a poetry slam, members of the audience are chosen by an emcee or host to act as judges for the event. In the national slam, there are five judges, but smaller slams generally have three. After each poet performs, each judge awards a score to that poem. Scores generally range between zero and ten. The highest and lowest score are dropped, giving each performance a rating between zero and thirty points.

Before the competition begins, the host will often bring up a "sacrificial" poet, whom the judges will score in order to calibrate their judging.

A single round at a standard slam consists of performances by all eligible poets. Most slams last multiple rounds, and many involve the elimination of lower-scoring poets in successive rounds. An elimination rubric might run 8-4-2; eight poets in the first round, four in the second, and two in the last. Some slams do not eliminate poets at all. The Green Mill usually runs its slams with 6 poets in the first round. At the end of the slam, the poet with the highest number of points earned is the winner.

The Boston Poetry Slam takes a different approach; it uses the 8-4-2 three-round rubric, but the poets go head-to-head in separate bouts within the round.

Props, costumes, and music are forbidden in slams, which differs greatly from its immediate predecessor, performance poetry. Hedwig Gorski, the founder of performance poetry as a distinct genre, saw props, costumes, and music as essential for a complete theatrical experience while also following theorist Jerzy Grotowski's Poor Theater by blurring lines between the real person, actor, and speakers in scripted literary art. Other rules for slams enforce a time limit of three minutes (and a grace period of ten seconds), after which a poet's score may be docked according to how long the poem exceeded the limit. Many youth slams, however, allow the poets up to three and a half minutes on stage.

In an "Open Slam", the most common slam type, competition is open to all who wish to compete, given the number of slots available. In an "Invitational Slam", only those invited to do so may compete.

Poetry Slam, Inc. holds several national and international competitions, including the Individual World Poetry Slam, the National Poetry Slam and The Women of the World Poetry Slam. The current (2013) IWPS champion is Ed Mabrey. Ed Mabrey is the only three-time IWPS champion in the history of the event. The current (2013) National Poetry Slam Team champions are Slam New Orleans (SNO), who have won the competition for the second year in a row. The current (2014) Women of the World Poetry Slam Champion is Dominique Christina.

From 10–11 December 2016 Salzburg, Austria held a world-record poetry slam competition (28 hours of classic slam poetry) and broke the so-far-record of Nuremberg, Germany (25 hours) by Michl Jakob. The winner of the competition (Friedrich Herrmann) scored one point better in the finals than the second ranked (Darryl Kiermeier). The event was organized by Lukas Wagner (Slamlabor) and took place in the SN-Saal of the Salzburger Nachrichten.

A "Theme Slam" is one in which all performances must conform to a specified theme, genre, or formal constraint. Themes may include Nerd, Erotica, Queer, Improv, or other conceptual limitations. In theme slams, poets can sometimes be allowed to break "traditional" slam rules. For instance, they sometimes allow performance of work by another poet (e.g. the "Dead Poet Slam", in which all work must be by a deceased poet). They can also allow changes on the restrictions on costumes or props (e.g. the Swedish "Triathlon" slams that allow for a poet, musician, and dancer to all take the stage at the same time), changing the judging structure (e.g. having a specific guest judge), or changing the time limits (e.g. a "1-2-3" slam with three rounds of one minute, two minutes, and three minutes, respectively).

Although theme slams may seem restricting in nature, slam venues frequently use them to advocate participation by particular and perhaps underrepresented demographics (which vary from slam to slam), like younger poets and women.

Poetry slams can feature a broad range of voices, styles, cultural traditions, and approaches to writing and performance. The originator of performance poetry, Hedwig Gorski, credits slam poetry for carrying on the poetics of ancient oral poetry designed to grab attention in barrooms and public squares.
Some poets are closely associated with the vocal delivery style found in hip-hop music and draw heavily on the tradition of dub poetry, a rhythmic and politicized genre belonging to black and particularly West Indian culture. Others employ an unrhyming narrative formula. Some use traditional theatrical devices including shifting voices and tones, while others may recite an entire poem in ironic monotone. Some poets use nothing but their words to deliver a poem, while others stretch the boundaries of the format, tap-dancing or beatboxing or using highly choreographed movements.

What is a dominant / successful style one year may not be passed to the next. Cristin O'Keefe Aptowicz, slam poet and author of "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam", was quoted in an interview on the Best American Poetry blog as saying:

One of the more interesting end products (to me, at least) of this constant shifting is that poets in the slam always worry that something—a style, a project, a poet—will become so dominant that it will kill the scene, but it never does. Ranting hipsters, freestyle rappers, bohemian drifters, proto-comedians, mystical shamans and gothy punks have all had their time at the top of the slam food chain, but in the end, something different always comes along and challenges the poets to try something new.
One of the goals of a poetry slam is to challenge the authority of anyone who claims absolute authority over literary value. No poet is beyond critique, as everyone is dependent upon the goodwill of the audience. Since only the poets with the best cumulative scores advance to the final round of the night, the structure assures that the audience gets to choose from whom they will hear more poetry. Audience members furthermore become part of each poem's presence, thus breaking down the barriers between poet/performer, critic, and audience.

Bob Holman, a poetry activist and former slammaster of the Nuyorican Poets Cafe, once called the movement "the democratization of verse". In 2005, Holman was also quoted as saying: "The spoken word revolution is led a lot by women and by poets of color. It gives a depth to the nation's dialogue that you don't hear on the floor of Congress. I want a floor of Congress to look more like a National Poetry Slam. That would make me happy."

At the 1993 National Poetry Slam in San Francisco, a participating team from Canada (Kedrick James, Alex Ferguson and John Sobol) wrote, printed and circulated an instant broadside titled "Like Lambs to the Slammer", that criticized what they perceived as the complacency, conformity, and calculated tear-jerking endemic to the poetry slam scene.

In an interview in the "Paris Review," literary critic Harold Bloom said about slamming:
I can't bear these accounts I read in the "Times" and elsewhere of these poetry slams, in which various young men and women in various late-spots are declaiming rant and nonsense at each other. The whole thing is judged by an applause meter which is actually not there, but might as well be. This isn't even silly; it is the death of art.

Poet and lead singer of King Missile, John S. Hall has also long been a vocal opponent, taking issue with such factors as its inherently competitive nature and what he considers its lack of stylistic diversity. In his 2005 interview in "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam," he recalls seeing his first slam, at the Nuyorican Poets Café: "...I hated it. And it made me really uncomfortable and ... it was very much like a sport, and I was interested in poetry in large part because it was like the antithesis of sports. ... [I]t seemed to me like a very macho, masculine form of poetry and not at all what I was interested in."

The poet Tim Clare offers a "for and against" account of the phenomenon in "Slam: A Poetic Dialogue".

Ironically, slam poetry movement founder Marc Smith has been critical of the commercially successful Def Poetry television and Broadway live stage shows produced by Russell Simmons, decrying it as "an exploitive entertainment [program that] diminished the value and aesthetic of performance poetry".

As of 2011, four poets who have competed at National Poetry Slam have won National Endowment of the Arts (NEA) Fellowships for Literature:

As of 2017, one poet who has competed at National Poetry Slam has won the Pulitzer Prize for Poetry:

A number of poets belong to both academia and slam: 

Some renowned poets have competed in slams, with less successful results. Henry Taylor, winner of the 1985 Pulitzer Prize for Poetry, competed in the 1997 National Poetry Slam as an individual and placed 75th out of 150.

While slam poetry has often been ignored in traditional higher learning institutions, it slowly is finding its way into courses and programs of study. For example, at Berklee College of Music, in Boston, slam poetry is now available as a Minor course of study.

Slam poetry has found popularity as a form of self-expression among many teenagers. Young Chicago Authors (YCA) provides workshops, mentoring, and competition opportunities to youth in the Chicago area. Every year YCA presents Louder Than a Bomb, the world's largest team-based youth slam and subject of a documentary by the same name. The youth poetry slam movement was the focus of a documentary film series produced by HBO and released in 2009. It featured poets from Youth Speaks, Urban Word, Louder than a Bomb and other related youth poetry slam organizations.

In a 2005 interview, one of slam's best known poets Saul Williams praised the youth poetry slam movement, explaining:
In 2012, more than 12,000 young people took part in an England-wide youth slam "Shake the Dust", organised by Apples and Snakes as part of the London 2012 Festival.
An Open Letter to Honey Singh, a rap video featuring Rene Sharanya Verma performing at Delhi Poetry Slam, went viral on YouTube receiving over 1.5 million hits.

Slam Poetry has been in Egypt since the twentieth century and was introduced by Hussain Shafiq al-Misry; who was the editor of a sarcastic magazine. According to al-Misry, having different jobs gave him the experience to understand the struggles of Egyptian people in different classes of life. He had good knowledge of Arabic literature, grammar and some commonly used foreign words as well as slang; which he used to form Halamantishi poetry. Muhammad Ragab Bayyoumi in 1986 wrote an article entitled Hussein Shafiq al-Misry: Ustaz la Tilmeeth lah" (Hussein Shafiq al-Misry: A Teacher with No Student of His) in which he introduced al-Misry's poems and explained al-Misry's literary poetry techniques. In Egypt Performance Poetry is new in popularity, the term "Ash-Shi'r al-Mu'adda" was recently introduced as the term for performance poetry. Poets such as  Bayram At-Tunisi, Ahmad Rami, and Kamel Ash-Shennawy paved the way after al-Misry with lyrical slam poems that use a melodic rhythm to attract the audience.

In Japan, Professor Katsunori Kusunoki, a professor of communications at Toyo University found a way to incorporate slam poetry into his students lives; allowing them to showcase their competitiveness and love of poetry by putting together “poetry boxing” matches. Professor Kusunoki created annual “poetry boxing” tournaments in order to provide an medium for expression and social interaction . The rules are “16 boxers face off in pairs in competitions of stand-up verse that last for three minutes. Winners compete in series of challenges such as timed presentation and a round of improvised jousting.” A MC adds to the event by providing nicknames for the competitors. Professor Kusunoki's goal was to try to get his students to open up by breaking language barriers and allowing them to express themselves through Slam Poetry.



</doc>
<doc id="28829" url="https://en.wikipedia.org/wiki?curid=28829" title="Sestina">
Sestina

A sestina (Italian: "sestina", from "sesto", sixth; Old Occitan: "cledisat" ; also known as "sestine", "sextine", "sextain") is a fixed verse form consisting of six stanzas of six lines each, normally followed by a three-line envoi. The words that end each line of the first stanza are used as line endings in each of the following stanzas, rotated in a set pattern.

The invention of the form is usually attributed to Arnaut Daniel, a troubadour of 12th-century Provence, and the first sestinas were written in the Occitan language of that region. The form was cultivated by his fellow troubadours, then by other poets across Continental Europe in the subsequent centuries; they contributed to what would become the "standard form" of the sestina. The earliest example of the form in English appeared in 1579, though they were rarely written in Britain until the end of the 19th century. The sestina remains a popular poetic form, and many sestinas continue to be written by contemporary poets.

The oldest-known sestina is "Lo ferm voler qu'el cor m'intra", written around 1200 by Arnaut Daniel, a troubadour of Aquitanian origin; he refers to it as "cledisat", meaning, more or less, "interlock". Hence, Daniel is generally considered the form's inventor, though it has been suggested that he may only have innovated an already existing form. Nevertheless, two other original troubadouric sestinas are known, the best known being "Eras, pus vey mon benastruc" by Guilhem Peire Cazals de Caortz; there are also two contrafacta built on the same end-words, the best known being "Ben gran avoleza intra" by Bertran de Born. These early sestinas were written in Old Occitan; the form started spilling into Italian with Dante in the 13th century; by the 15th, it was used in Portuguese by Luís de Camões.

The involvement of Dante and Petrarch in establishing the sestina form, together with the contributions of others in the country, account for its classification as an Italian verse form—despite not originating there. The result was that the sestina was re-imported into France from Italy in the 16th century. Pontus de Tyard was the first poet to attempt the form in French, and the only one to do so prior to the 19th century; he introduced a partial rhyme scheme in his sestina.

The first appearance of the sestina in English print is "Ye wastefull woodes", comprising lines 151–89 of the August Æglogue in Edmund Spenser's "Shepherd's Calendar", published in 1579. It is in unrhymed iambic pentameter, but the order of end-words in each stanza is non-standard – ending 123456, 612345, etc. – each stanza promoting the previous final end-word to the first line, but otherwise leaving the order intact; the envoi order is (1) 2 / (3) 4 / (5) 6. This scheme was set by the Spaniard Gutierre de Cetina.

Although they appeared in print later, Philip Sidney's three sestinas may have been written earlier, and are often credited as the first in English. The first published (toward the end of Book I of "The Countess of Pembroke's Arcadia", 1590) is the double sestina "Ye Goatherd Gods". In this variant the standard end-word pattern is repeated for twelve stanzas, ending with a three-line envoi, resulting in a poem of 75 lines. Two others were published in subsequent editions of the "Arcadia". The second, "Since wailing is a bud of causeful sorrow", is in the "standard" form. Like "Ye Goatherd Gods" it is written in unrhymed iambic pentameter and uses exclusively feminine endings, reflecting the Italian "endecasillabo". The third, "Farewell, O sun, Arcadia's clearest light", is the first rhyming sestina in English: it is in iambic pentameters and follows the standard end-word scheme, but rhymes ABABCC in the first stanza (the rhyme scheme necessarily changes in each subsequent stanza, a consequence of which is that the 6th stanza is in rhyming couplets). Sidney uses the same envoi structure as Spenser. William Drummond of Hawthornden published two sestinas (which he called "sextains") in 1616, which copy the form of Sidney's rhyming sestina. After this, there is an absence of notable sestinas for over 250 years, with John Frederick Nims noting that, "... there is not a single sestina in the three volumes of the Oxford anthologies that cover the seventeenth, eighteenth and nineteenth centuries."

In the 1870s, there was a revival of interest in French forms, led by Andrew Lang, Austin Dobson, Edmund Gosse, W. E. Henley, John Payne, and others. The earliest sestina of this period is Algernon Charles Swinburne's "Sestina". It is in iambic pentameter rhyming ABABAB in the first stanza; each stanza begins by repeating the previous end-words 6 then 1, but the following 4 lines repeat the remaining end-words "ad lib"; the envoi is (1) 4 / (2) 3 / (5) 6. In the same volume ("Poems and Ballads, Second Series", 1878) Swinburne introduces a "double sestina" ("The Complaint of Lisa") that is unlike Sidney's: it comprises 12 stanzas of 12 iambic pentameter lines each, the first stanza rhyming ABCABDCEFEDF. Similar to his "Sestina", each stanza first repeats end-words 12 then 1 of the previous stanza; the rest are "ad lib". The envoi is (12) 10 / (8) 9 / (7) 4 / (3) 6 / (2) 1 / (11) 5.

From the 1930s, a revival of the form took place across the English-speaking world, led by poets such as W. H. Auden, and the 1950s were described as the "age of the sestina" by James E. B. Breslin. "Sestina: Altaforte" by Ezra Pound and "Paysage moralisé" by W. H. Auden are distinguished modern examples of the sestina. The sestina remains a popular closed verse form, and many sestinas continue to be written by contemporary poets; notable examples include "The Guest Ellen at the Supper for Street People" by David Ferry and "IVF" by Kona Macphee.

Although the sestina has been subject to many revisions throughout its development, there remain several features that define the form. The sestina is composed of six stanzas of six lines (sixains), followed by a stanza of three lines (a tercet). There is no rhyme within the stanzas; instead the sestina is structured through a recurrent pattern of the words that end each line, a technique known as "lexical repetition".

In the original form composed by Daniel, each line is of ten syllables, except the first of each stanza which are of seven. The established form, as developed by Petrarch and Dante, was in hendecasyllables. Since then, changes to the line length have been a relatively common variant, such that Stephanie Burt has written: "sestinas, as the form exists today, [do not] require expertise with inherited meter ...".

The pattern that the line-ending words follow is often explained if the numbers 1 to 6 are allowed to stand for the end-words of the first stanza. Each successive stanza takes its pattern based upon a bottom-up pairing of the lines of the preceding stanza (i.e., last and first, then second-from-last and second, then third-from-last and third). Given that the pattern for the first stanza is 123456, this produces 615243 in the second stanza.

Another way of visualising the pattern of line-ending words for each stanza is by the procedure known as "retrogradatio cruciata", which may be rendered as "backward crossing". The second stanza can be seen to have been formed from three sets of pairs (6–1, 5–2, 4–3), or two triads (1–2–3, 4–5–6). The 1–2–3 triad appears in its original order, but the 4–5–6 triad is reversed and superimposed upon it.

The pattern of the line-ending words in a sestina is represented both numerically and alphabetically in the following table:

The sixth stanza is followed by a tercet that is known variably by the French term envoi, the Occitan term tornada, or, with reference to its size in relation to the preceding stanzas, a "half-stanza". It consists of three lines that include all six of the line-endings words of the preceding stanzas. This should take the pattern of 2–5, 4–3, 6–1 (numbers relative to the first stanza); the first end-word of each pair can occur anywhere in the line, while the second must end the line. However, the end-word order of the envoi is no longer strictly enforced.

The sestina has been subject to some variations, with changes being made to both the size and number of stanzas, and also to individual line length. A "double sestina" is the name given to either: two sets of six six-line stanzas, with a three-line envoy (for a total of 75 lines), or twelve twelve-line stanzas, with a six-line envoy (for a total of 150 lines). Examples of either variation are rare; "Ye Goatherd Gods" by Philip Sidney is a notable example of the former variation, while "The Complaint of Lisa" by Algernon Charles Swinburne is a notable example of the latter variation. In the former variation, the original pattern of line-ending words, i.e. that of the first stanza, recurs in the seventh stanza, and thus the entire change of pattern occurs twice throughout. In the second variation, the pattern of line-ending words returns to the starting sequence in the eleventh stanza; thus it does not, unlike the "single" sestina, allow for every end-word to occupy each of the stanza ends; end-words 5 and 10 fail to couple between stanzas.

The structure of the sestina, which demands adherence to a strict and arbitrary order, produces several effects within a poem. Stephanie Burt notes that, "The sestina has served, historically, as a complaint", its harsh demands acting as "signs for deprivation or duress". The structure can enhance the subject matter that it orders; in reference to Elizabeth Bishop's "A Miracle for Breakfast", David Caplan suggests that the form's "harshly arbitrary demands echo its subject's". Nevertheless, the form's structure has been criticised; Paul Fussell considers the sestina to be of "dubious structural expressiveness" when composed in English and, irrespective of how it is used, "would seem to be [a form] that gives more structural pleasure to the contriver than to the apprehender."

Margaret Spanos highlights "a number of corresponding levels of tension and resolution" resulting from the structural form, including: structural, semantic and aesthetic tensions. She believes that the aesthetic tension, which results from the ""conception" of its mathematical completeness and perfection", set against the ""experiences" of its labyrinthine complexities" can be resolved in the apprehension of the "harmony of the whole."

The strength of the sestina, according to Stephen Fry, is the "repetition and recycling of elusive patterns that cannot be quite held in the mind all at once". For Shanna Compton, these patterns are easily discernible by newcomers to the form; she says that: "Even someone unfamiliar with the form's rules can tell by the end of the second stanza ... what's going on ...".

The 1972 television play "Between Time and Timbuktu", based on the writings of Kurt Vonnegut, was about a poet-astronaut who wanted to compose a sestina in outer space. Vonnegut wrote a sestina for the production.




</doc>
<doc id="28830" url="https://en.wikipedia.org/wiki?curid=28830" title="Song">
Song

A song is a musical composition intended to be performed by the human voice. This is often done at distinct and fixed pitches (melodies) using patterns of sound and silence. Songs contain various forms, such as those including the repetition and variation of sections.

Written words created specifically for music or for which music is specifically created, are called lyrics. If a pre-existing poem is set to composed music in classical music it is an art song. Songs that are sung on repeated pitches without distinct contours and patterns that rise and fall are called chants. Songs composed in a simple style that are learned informally "by ear" are often referred to as folk songs. Songs that are composed for professional singers who sell their recordings or live shows to the mass market are called popular songs. These songs, which have broad appeal, are often composed by professional songwriters, composers, and lyricists. Art songs are composed by trained classical composers for concert or recital performances. Songs are performed live and recorded on audio or video (or, in some cases, a song may be performed live and simultaneously recorded). Songs may also appear in plays, musical theatre, stage shows of any form, and within operas, films, and TV shows.

A song may be for a solo singer, a lead singer supported by background singers, a duet, trio, or larger ensemble involving more voices singing in harmony, although the term is generally not used for large classical music vocal forms including opera and oratorio, which use terms such as aria and recitative instead. A song can be sung without accompaniment by instrumentalists (a cappella) or accompanied by instruments. In popular music, a singer may perform with an acoustic guitarist, pianist, organist, accordionist, or a backing band. In jazz, a singer may perform with a single pianist, a small combo (such as a trio or quartet), or with a big band. A Classical singer may perform with a single pianist, a small ensemble, or an orchestra. In jazz and blues, singers often learn songs "by ear" and they may improvise some melody lines. In Classical music, melodies are written by composers in sheet music format, so singers learn to read music.

Songs with more than one voice to a part singing in polyphony or harmony are considered choral works. Songs can be broadly divided into many different forms and types, depending on the criteria used. Through semantic widening, a broader sense of the word "song" may refer to instrumentals, such as Mendelssohn's 19th century "Songs Without Words" pieces for solo piano.

Art songs are songs created for performance by classical artists, often with piano or other instrumental accompaniment, although they can be sung solo. Art songs require strong vocal technique, understanding of language, diction, and poetry for interpretation. Though such singers may also perform popular or folk songs on their programs, these characteristics and the use of poetry are what distinguish art songs from popular songs. Art songs are a tradition from most European countries, and now other countries with classical music traditions. German-speaking communities use the term art song ("Kunstlied") to distinguish so-called "serious" compositions from folk song ("Volkslied"). The lyrics are often written by a poet or lyricist and the music separately by a composer. Art songs may be more formally complicated than popular or folk songs, though many early Lieder by the likes of Franz Schubert are in simple strophic form. The accompaniment of European art songs is considered as an important part of the composition. Some art songs are so revered that they take on characteristics of national identification.

Art songs emerge from the tradition of singing romantic love songs, often to an ideal or imaginary person and from religious songs. The troubadours and bards of Europe began the documented tradition of romantic songs, continued by the Elizabethan lutenists. Some of the earliest art songs are found in the music of Henry Purcell. The tradition of the romance, a love song with a flowing accompaniment, often in triple meter, entered opera in the 19th century and spread from there throughout Europe. It spread into popular music and became one of the underpinnings of popular songs. While a romance generally has a simple accompaniment, art songs tend to have complicated, sophisticated accompaniments that underpin, embellish, illustrate or provide contrast to the voice. Sometimes the accompaniment performer has the melody, while the voice sings a more dramatic part.

Folk songs are songs of often anonymous origin (or are public domain) that are transmitted orally. They are frequently a major aspect of national or cultural identity. Art songs often approach the status of folk songs when people forget who the author was. Folk songs are also frequently transmitted non-orally (that is, as sheet music), especially in the modern era. Folk songs exist in almost every culture. Popular songs may eventually become folk songs by the same process of detachment from its source. Folk songs are more-or-less in the public domain by definition, though there are many folk song entertainers who publish and record copyrighted original material. This tradition led also to the singer-songwriter style of performing, where an artist has written confessional poetry or personal statements and sings them set to music, most often with guitar accompaniment.

There are many genres of popular songs, including torch songs, ballads, novelty songs, anthems, rock, blues and soul songs as well as indie music. Other commercial genres include rapping. Folk songs include ballads, lullabies, love songs, mourning songs, dance songs, work songs, ritual songs and many more.

A sporting song is a folk song which celebrates fox hunting, horse racing, gambling and other recreations.

Although songs about boxers and successful racehorses were common in the nineteenth century, few are performed by current singers. In particular fox-hunting is considered politically incorrect. The most famous song about a foxhunter, "D'ye ken John Peel" was included in "The National Song Book" in 1906 and is now often heard as a marching tune. A. L. Lloyd recorded two EPs of sporting ballads; "Bold Sportsmen All" (1958) and "Gamblers and Sporting Blades (Songs of the Ring and the Racecourse)" (1962). The High Level Ranters and Martin Wyndham-Read recorded an album called "English Sporting Ballads" in 1977. "The Prospect Before Us" (1976) by The Albion Dance Band contains two rarely heard hunting songs.

The term lute song is given to a music style from the late 16th century to early 17th century, late Renaissance to early Baroque, that was predominantly in England and France. Lute songs were generally in strophic form or verse repeating with a homophonic texture. The composition was written for a solo voice with an accompaniment, usually the lute. It was not uncommon for other forms of accompaniments such as bass viol or other string instruments, and could also be written for more voices. The composition could be performed either solo or with a small group of instruments. 

A part song, part-song or partsong is a form of choral music that consists of a secular (vs. ecclesiastical) song written or arranged for several vocal parts. Part songs are commonly sung by an SATB choir, but sometimes for an all-male or all-female ensemble.

The patter song is characterised by a moderately fast to very fast tempo with a rapid succession of rhythmic patterns in which each syllable of text corresponds to one note. It is a staple of comic opera, especially Gilbert and Sullivan, but it has also been used in musicals and elsewhere.




</doc>
<doc id="28833" url="https://en.wikipedia.org/wiki?curid=28833" title="Sir Gawain and the Green Knight">
Sir Gawain and the Green Knight

Sir Gawain and the Green Knight (Middle English: "Sir Gawayn and þe Grene Knyȝt") is a late 14th-century Middle English chivalric romance. It is one of the best known Arthurian stories, with its plot combining two types of folk motifs, the beheading game and the exchange of winnings. Written in stanzas of alliterative verse, each of which ends in a rhyming bob and wheel, it draws on Welsh, Irish and English stories, as well as the French chivalric tradition. It is an important example of a chivalric romance, which typically involves a hero who goes on a quest which tests his prowess. It remains popular in modern English renderings from J. R. R. Tolkien, Simon Armitage and others, as well as through film and stage adaptations.

It describes how Sir Gawain, a knight of King Arthur's Round Table, accepts a challenge from a mysterious "Green Knight" who dares any knight to strike him with his axe if he will take a return blow in a year and a day. Gawain accepts and beheads him with his blow, at which the Green Knight stands up, picks up his head and reminds Gawain of the appointed time. In his struggles to keep his bargain, Gawain demonstrates chivalry and loyalty until his honour is called into question by a test involving Lady Bertilak, the lady of the Green Knight's castle.

The poem survives in one manuscript, "Cotton Nero A.x.", which also includes three religious narrative poems: "Pearl", "Purity" and "Patience". All are thought to have been written by the same author, dubbed the "Pearl Poet" or "Gawain Poet", since all four are written in a North West Midland dialect of Middle English.

In Camelot on New Year's Day, King Arthur's court is exchanging gifts and waiting for the feasting to start when the king asks to see or hear of an exciting adventure. A gigantic figure, entirely green in appearance and riding a green horse, rides unexpectedly into the hall. He wears no armour but bears an axe in one hand and a holly bough in the other. Refusing to fight anyone there on the grounds that they are all too weak to take him on, he insists he has come for a friendly christmas game: someone is to strike him once with his axe on the condition that the Green Knight may return the blow in a year and a day. The splendid axe will belong to whoever accepts this deal. Arthur himself is prepared to accept the challenge when it appears no other knight will dare, but Sir Gawain, youngest of Arthur's knights and his nephew, asks for the honour instead. The giant bends and bares his neck before him and Gawain neatly beheads him in one stroke. However, the Green Knight neither falls nor falters, but instead reaches out, picks up his severed head and remounts, holding up his bleeding head to Queen Guinevere while its writhing lips remind Gawain that the two must meet again at the Green Chapel. He then rides away. Gawain and Arthur admire the axe, hang it up as a trophy and encourage Guinevere to treat the whole matter lightly.

As the date approaches, Sir Gawain sets off to find the Green Chapel and keep his side of the bargain. Many adventures and battles are alluded to (but not described) until Gawain comes across a splendid castle where he meets Bertilak de Hautdesert, the lord of the castle, and his beautiful wife, who are pleased to have such a renowned guest. Also present is an old and ugly lady, unnamed but treated with great honour by all. Gawain tells them of his New Year's appointment at the Green Chapel and that he only has a few days remaining. Bertilak laughs, explaining that there is a path that will take him to there less than two miles away, and proposes that Gawain rest at the castle until then. Relieved and grateful, Gawain agrees.

Before going hunting the next day, Bertilak proposes a bargain: he will give Gawain whatever he catches on the condition that Gawain give him whatever he might gain during the day. Gawain accepts. After Bertilak leaves, Lady Bertilak visits Gawain's bedroom and behaves seductively, but despite her best efforts he yields nothing but a single kiss in his unwillingness to offend her. When Bertilak returns and gives Gawain the deer he has killed, his guest gives a kiss to Bertilak without divulging its source. The next day the lady comes again, Gawain again courteously foils her advances, and later that day there is a similar exchange of a hunted boar for two kisses. She comes once more on the third morning, but once her advances are denied, she offers Gawain a gold ring as a keepsake. He gently but steadfastly refuses but she pleads that he at least take her belt, a girdle of green and gold silk. The belt, the lady assures him, is charmed and will keep him from all physical harm. Tempted, as he may otherwise die the next day, Gawain accepts it, and they exchange three kisses. The lady has Gawain swear that he will keep the gift secret from Bertilak. That evening, Bertilak returns with a fox, which he exchanges with Gawain for the three kisses – but Gawain says nothing of the girdle.

The next day, Gawain binds the belt twice around his waist. He finds the Green Knight sharpening an axe and, as promised, Gawain bends his bared neck to receive his blow. At the first swing, Gawain flinches slightly and the Green Knight belittles him for it. Ashamed of himself, Gawain doesn't flinch with the second swing; but again the Green Knight withholds the full force of his blow. The knight explains he was testing Gawain's nerve. Angrily Gawain tells him to deliver his blow and so the knight does, causing only a slight wound on Gawain's neck. The game is over. Gawain seizes his sword, helmet and shield, but the Green Knight, laughing, reveals himself to be the lord of the castle, Bertilak de Hautdesert, transformed by magic. He explains that the entire adventure was a trick of the "elderly lady" Gawain saw at the castle, who is actually the sorceress Morgan le Fay, Arthur's sister, who intended to test Arthur's knights and frighten Guinevere to death. Gawain is ashamed to have behaved deceitfully but the Green Knight laughs and professes him the most blameless knight in all the land. The two part on cordial terms. Gawain returns to Camelot wearing the girdle as a token of his failure to keep his promise. The Knights of the Round Table absolve him of blame and decide that henceforth each will wear a green sash in recognition of Gawain's adventure and as a reminder to be always honest.

Though the real name of "The "Gawain" Poet" (or poets) is unknown, some inferences about him can be drawn from an informed reading of his works. The manuscript of "Gawain" is known in academic circles as "Cotton Nero A.x.", following a naming system used by one of its owners, the sixteenth century Robert Bruce Cotton, a collector of Medieval English texts. Before the "Gawain" manuscript came into Cotton's possession, it was in the library of Henry Savile in Yorkshire. Little is known about its previous ownership, and until 1824, when the manuscript was introduced to the academic community in a second edition of Thomas Warton's "History" edited by Richard Price, it was almost entirely unknown. Even then, the "Gawain" poem was not published in its entirety until 1839. Now held in the British Library, it has been dated to the late 14th century, meaning the poet was a contemporary of Geoffrey Chaucer, author of "The Canterbury Tales", though it is unlikely that they ever met, and the Gawain Poet's English is considerably different from Chaucer's. The three other works found in the same manuscript as "Gawain" (commonly known as "Pearl", "Patience", and "Purity" or "Cleanliness") are often considered to be written by the same author. However, the manuscript containing these poems was transcribed by a copyist and not by the original poet. Although nothing explicitly suggests that all four poems are by the same poet, comparative analysis of dialect, verse form, and diction have pointed towards single authorship.

What is known today about the poet is largely general. As J. R. R. Tolkien and E. V. Gordon, after reviewing the text's allusions, style, and themes, concluded in 1925:
The most commonly suggested candidate for authorship is John Massey of Cotton, Cheshire. He is known to have lived in the dialect region of the Gawain Poet and is thought to have written the poem "St. Erkenwald", which some scholars argue bears stylistic similarities to "Gawain". "St. Erkenwald", however, has been dated by some scholars to a time outside the Gawain Poet's era. Thus, ascribing authorship to John Massey is still controversial and most critics consider the Gawain Poet an unknown.

The 2,530 lines and 101 stanzas that make up "Sir Gawain and the Green Knight" are written in what linguists call the "Alliterative Revival" style typical of the 14th century. Instead of focusing on a metrical syllabic count and rhyme, the alliterative form of this period usually relied on the agreement of a pair of stressed syllables at the beginning of the line and another pair at the end. Each line always includes a pause, called a "caesura", at some point after the first two stresses, dividing it into two half-lines. Although he largely follows the form of his day, the Gawain poet was somewhat freer with convention than his or her predecessors. The poet broke the alliterative lines into variable-length groups and ended these nominal stanzas with a rhyming section of five lines known as the "bob and wheel", in which the "bob" is a very short line, sometimes of only two syllables, followed by the "wheel," longer lines with internal rhyme.

The earliest known story to feature a beheading game is the 8th-century Middle Irish tale "Bricriu's Feast". This story parallels "Gawain" in that, like the Green Knight, Cú Chulainn's antagonist feints three blows with the axe before letting his target depart without injury. A beheading exchange also appears in the late 12th-century "Life of Caradoc", a Middle French narrative embedded in the anonymous First Continuation of Chrétien de Troyes' "Perceval, the Story of the Grail". A notable difference in this story is that Caradoc's challenger is his father in disguise, come to test his honour. Lancelot is given a beheading challenge in the early 13th-century "Perlesvaus", in which a knight begs him to chop off his head or else put his own in jeopardy. Lancelot reluctantly cuts it off, agreeing to come to the same place in a year to put his head in the same danger. When Lancelot arrives, the people of the town celebrate and announce that they have finally found a true knight, because many others had failed this test of chivalry.

The stories "The Girl with the Mule" (alternately titled "The Mule Without a Bridle") and "Hunbaut" feature Gawain in beheading game situations. In "Hunbaut," Gawain cuts off a man's head and, before he can replace it, removes the magic cloak keeping the man alive, thus killing him. Several stories tell of knights who struggle to stave off the advances of women sent by their lords as a test; these stories include "Yder", the Lancelot-Grail, "Hunbaut", and "The Knight of the Sword". The last two involve Gawain specifically. Usually the temptress is the daughter or wife of a lord to whom the knight owes respect, and the knight is tested to see whether or not he will remain chaste in trying circumstances.

In the first branch of the medieval Welsh collection of tales known as The Four Branches of the Mabinogi, Pwyll exchanges places for a year with Arawn, the lord of Annwn (the Otherworld). Despite having his appearance changed to resemble Arawn exactly, Pwyll does not have sexual relations with Arawn's wife during this time, thus establishing a lasting friendship between the two men. This story may, then, provide a background to Gawain's attempts to resist the wife of the Green Knight; thus, the story of Sir Gawain and the Green Knight may be seen as a tale which combines elements of the Celtic beheading game and seduction test stories. Additionally, in both stories a year passes before the completion of the conclusion of the challenge or exchange. Some scholars disagree with this interpretation, however, as Arawn seems to have accepted the notion that Pwyll may reciprocate with his wife, making it less of a "seduction test" per se, as seduction tests typically involve a Lord and Lady conspiring to seduce a knight, seemingly "against" the wishes of the Lord.

After the writing of "Sir Gawain and the Green Knight", several similar stories followed. "The Greene Knight" (15th–17th century) is a rhymed retelling of nearly the same tale. In it, the plot is simplified, motives are more fully explained, and some names are changed. Another story, "The Turke and Gowin" (15th century), begins with a Turk entering Arthur's court and asking, "Is there any will, as a brother, To give a buffett and take another?" At the end of this poem the Turk, rather than buffeting Gawain back, asks the knight to cut off his head, which Gawain does. The Turk then praises Gawain and showers him with gifts. "The Carle of Carlisle" (17th century) also resembles "Gawain" in a scene in which the Carle (Churl), a lord, takes Sir Gawain to a chamber where two swords are hanging and orders Gawain to cut off his head or suffer his own to be cut off. Gawain obliges and strikes, but the Carle rises, laughing and unharmed. Unlike the "Gawain" poem, no return blow is demanded or given.

At the heart of "Sir Gawain and the Green Knight" is the test of Gawain's adherence to the code of chivalry. The typical temptation fable of medieval literature presents a series of tribulations assembled as tests or "proofs" of moral virtue. The stories often describe several individuals' failures after which the main character is tested. Success in the proofs will often bring immunity or good fortune. Gawain's ability to pass the tests of his host are of utmost importance to his survival, though he does not know it. It is only by fortuity or "instinctive-courtesy" that Sir Gawain is able to pass his test. Gawain does not realize, however, that these tests are all orchestrated by Sir Bertilak.
In addition to the laws of chivalry, Gawain must respect another set of laws concerning courtly love. The knight's code of honour requires him to do whatever a damsel asks. Gawain must accept the girdle from the Lady, but he must also keep the promise he has made to his host that he will give whatever he gains that day. Gawain chooses to keep the girdle out of fear of death, thus breaking his promise to the host but honouring the lady. Upon learning that the Green Knight is actually his host (Bertilak), he realises that although he has completed his quest, he has failed to be virtuous. This test demonstrates the conflict between honour and knightly duties. In breaking his promise, Gawain believes he has lost his honour and failed in his duties.

Scholars have frequently noted the parallels between the three hunting scenes and the three seduction scenes in "Gawain". They are generally agreed that the fox chase has significant parallels to the third seduction scene, in which Gawain accepts the girdle from Bertilak's wife. Gawain, like the fox, fears for his life and is looking for a way to avoid death from the Green Knight's axe. Like his counterpart, he resorts to trickery in order to save his skin. The fox uses tactics so unlike the first two animals, and so unexpectedly, that Bertilak has the hardest time hunting it. Similarly, Gawain finds the Lady's advances in the third seduction scene more unpredictable and challenging to resist than her previous attempts. She changes her evasive language, typical of courtly love relationships, to a more assertive style. Her dress, relatively modest in earlier scenes, is suddenly voluptuous and revealing.

The deer- and boar-hunting scenes are less clearly connected, although scholars have attempted to link each animal to Gawain's reactions in the parallel seduction scene. Attempts to connect the deer hunt with the first seduction scene have unearthed a few parallels. Deer hunts of the time, like courtship, had to be done according to established rules. Women often favoured suitors who hunted well and skinned their animals, sometimes even watching while a deer was cleaned. The sequence describing the deer hunt is relatively unspecific and nonviolent, with an air of relaxation and exhilaration. The first seduction scene follows in a similar vein, with no overt physical advances and no apparent danger; the entire exchange is humorously portrayed.

The boar-hunting scene is, in contrast, laden with detail. Boars were (and are) much more difficult to hunt than deer; approaching one with only a sword was akin to challenging a knight to single combat. In the hunting sequence, the boar flees but is cornered before a ravine. He turns to face Bertilak with his back to the ravine, prepared to fight. Bertilak dismounts and in the ensuing fight kills the boar. He removes its head and displays it on a pike. In the seduction scene, Bertilak's wife, like the boar, is more forward, insisting that Gawain has a romantic reputation and that he must not disappoint her. Gawain, however, is successful in parrying her attacks, saying that surely she knows more than he about love. Both the boar hunt and the seduction scene can be seen as depictions of a moral victory: both Gawain and Bertilak face struggles alone and emerge triumphant.
Masculinity has also been associated with hunting. The theme of masculinity is present throughout. In an article by Vern L. Bullough, "Being a Male in the Middle Ages," he discusses Sir Gawain and how normally, masculinity is often viewed in terms of being sexually active. He notes that Sir Gawain is not part of this normalcy.

Some argue that nature represents a chaotic, lawless order which is in direct confrontation with the civilisation of Camelot throughout "Sir Gawain and the Green Knight". The green horse and rider that first invade Arthur's peaceful halls are iconic representations of nature's disturbance. Nature is presented throughout the poem as rough and indifferent, constantly threatening the order of men and courtly life. Nature invades and disrupts order in the major events of the narrative, both symbolically and through the inner nature of humanity. This element appears first with the disruption caused by the Green Knight, later when Gawain must fight off his natural lust for Bertilak's wife, and again when Gawain breaks his vow to Bertilak by choosing to keep the green girdle, valuing survival over virtue. Represented by the sin-stained girdle, nature is an underlying force, forever within man and keeping him imperfect (in a chivalric sense). In this view, Gawain is part of a wider conflict between nature and chivalry, an examination of the ability of man's order to overcome the chaos of nature.

Several critics have made exactly the opposite interpretation, reading the poem as a comic critique of the Christianity of the time, particularly as embodied in the Christian chivalry of Arthur's court. In its zeal to extirpate all traces of paganism, Christianity had cut itself off from the sources of life in nature and the female. The green girdle represents all the pentangle lacks. The Arthurian enterprise is doomed unless it can acknowledge the unattainability of the ideals of the Round Table, and, for the sake of realism and wholeness, recognize and incorporate the pagan values represented by the Green Knight.

The chivalry that is represented within 'Gawain' is one which was constructed by court nobility. The violence that is part of this chivalry is steeply contrasted by the fact that King Arthur's court is Christian and the initial beheading event takes place while celebrating Christmas. The violence of an act of beheading seems to be counterintuitive to chivalric and Christian ideals, and yet it is seen as part of knighthood.

The question of politeness and chivalry is a main theme during Gawain's interactions with Bertilak's wife. He cannot accept her advances or else lose his honour, and yet he cannot utterly refuse her advances or else risk upsetting his hostess. Gawain plays a very fine line and the only part where he appears to fail is when he conceals the green girdle from Bertilak.

The word "gomen" (game) is found 18 times in "Gawain". Its similarity to the word "gome" (man), which appears 21 times, has led some scholars to see men and games as centrally linked. Games at this time were seen as tests of worthiness, as when the Green Knight challenges the court's right to its good name in a "Christmas game". The "game" of exchanging gifts was common in Germanic cultures. If a man received a gift, he was obliged to provide the giver with a better gift or risk losing his honour, almost like an exchange of blows in a fight (or in a "beheading game"). The poem revolves around two games: an exchange of beheading and an exchange of winnings. These appear at first to be unconnected. However, a victory in the first game will lead to a victory in the second. Elements of both games appear in other stories; however, the linkage of outcomes is unique to "Gawain".

Times, dates, seasons, and cycles within "Gawain" are often noted by scholars because of their symbolic nature. The story starts on New Year's Eve with a beheading and culminates on the next New Year's Day. Gawain leaves Camelot on All Saints Day and arrives at Bertilak's castle on Christmas Eve. Furthermore, the Green Knight tells Gawain to meet him at the Green Chapel in "a year and a day"—a period of time seen often in medieval literature. Some scholars interpret the yearly cycles, each beginning and ending in winter, as the poet's attempt to convey the inevitable fall of all things good and noble in the world. Such a theme is strengthened by the image of Troy, a powerful nation once thought to be invincible which, according to the "Aeneid", fell to the Greeks due to pride and ignorance. The Trojan connection shows itself in the presence of two virtually identical descriptions of Troy's destruction. The poem's first line reads: "Since the siege and the assault were ceased at Troy" and the final stanzaic line (before the bob and wheel) is "After the siege and the assault were ceased at Troy".

The entire 'Gawain' poem follows one individual experiencing highly emotional situations. He participates in the beheading contest, watches as a man he has beheaded walks away unscathed, prepares for a journey where he will then also receive a blow that will behead him, is tempted by the sexual advances of Sir Bertilak's wife, decides what to do with the moral conundrum that is the girdle, suffering humiliation, and returning to court to retell his entire adventure. These events invite the reader to empathize with Gawain, the flawed hero, and understand that he is also human.

Humans experience an emotional contagion, which was defined by psychologists Elaine Hatfield, John Cacioppo, and Richard Rapson as 'the tendency to automatically mimic and synchronize expressions, vocalizations, postures, and movements with those of another person, and, consequently, to converge emotionally.' Amy Coplan explains that emotional contagion is something that happens so quickly and automatically that we are typically unaware of it happening. The poet capitalized on this emotional reaction and thus has the reader empathizing with Sir Gawain almost without realizing it.

Given the varied and even contradictory interpretations of the colour green, its precise meaning in the poem remains ambiguous. In English folklore and literature, green was traditionally used to symbolise nature and its associated attributes: fertility and rebirth. Stories of the medieval period also used it to allude to love and the base desires of man. Because of its connection with faeries and spirits in early English folklore, green also signified witchcraft, devilry and evil. It can also represent decay and toxicity. When combined with gold, as with the Green Knight and the girdle, green was often seen as representing youth's passing. In Celtic mythology, green was associated with misfortune and death, and therefore avoided in clothing. The green girdle, originally worn for protection, became a symbol of shame and cowardice; it is finally adopted as a symbol of honour by the knights of Camelot, signifying a transformation from good to evil and back again; this displays both the spoiling and regenerative connotations of the colour green. Ovid's Metamorphoses interprets Gawain poet's association of green with envy. Morgan envies Queen Guinevere's good fortune at Arthur's court and, furious about her own expulsion from the court engineered by the queen, transforms Bertilak into her instrument of jealousy, the Green Knight. The Lady's green girdle is also a device used to test Gawain's own envy, tempting him to sin.

Scholars have puzzled over the Green Knight's symbolism since the discovery of the poem. British medievalist C. S. Lewis said the character was "as vivid and concrete as any image in literature" and J. R. R. Tolkien said he was the "most difficult character" to interpret in "Sir Gawain". His major role in Arthurian literature is that of a judge and tester of knights, thus he is at once terrifying, friendly, and mysterious. He appears in only two other poems: "The Greene Knight" and "King Arthur and King Cornwall". Scholars have attempted to connect him to other mythical characters, such as Jack in the green of English tradition and to Al-Khidr, but no definitive connection has yet been established.

However, there is a possibility, as Alice Buchanan has argued, that the colour green is erroneously attributed to the Green Knight due to the poet's mistranslation or misunderstanding of the Irish word "glas", which could either mean grey or green. In the Death of Curoi (one of the Irish stories from Bricriu's Feast), Curoi stands in for Bertilak, and is often called "the man of the grey mantle". Though the words usually used for grey in the Death of Curoi are "lachtna" or "odar", roughly meaning milk-coloured and shadowy respectively, in later works featuring a green knight, the word "glas" is used and may have been the basis of misunderstanding.

The girdle's symbolic meaning, in "Sir Gawain and the Green Knight", has been construed in a variety of ways. Interpretations range from sexual to spiritual. Those who argue for the sexual inference view the girdle as a "trophy". It is not entirely clear if the "winner" is Sir Gawain or Lady Bertilak. The girdle is given to Gawain by Lady Bertilak to keep him safe when he confronts the Green Knight. When Lord Bertilak comes home from his hunting trip, Gawain does not reveal the girdle to his host, instead he hides it. This introduces the spiritual interpretation, that Gawain's acceptance of the girdle is a sign of his faltering faith in God, at least in the face of death. To some, the Green Knight is Christ, who overcomes death, while Gawain is the Every Christian, who in his struggles to follow Christ faithfully, chooses the easier path. In "Sir Gawain", the easier choice is the girdle, which promises what Gawain most desires. Faith in God, alternatively, requires one's acceptance that what one most desires does not always coincide with what God has planned. It is arguably best to view the girdle not as an either–or situation, but as a complex, multi-faceted symbol that acts to test Gawain in more ways than one. While Gawain is able to resist Bertilak's wife's sexual advances, he is unable to resist the powers of the girdle. Gawain is operating under the laws of chivalry which, evidently, have rules that can contradict each other. In the story of "Sir Gawain", Gawain finds himself torn between doing what a damsel asks (accepting the girdle) and keeping his promise (returning anything given to him while his host is away). 

The poem contains the first recorded use of the word "pentangle" in English. It contains the only representation of such a symbol on Gawain's shield in the Gawain literature. What is more, the poet uses a total of 46 lines in order to describe the meaning of the pentangle; no other symbol in the poem receives as much attention or is described in such detail. The poem describes the pentangle as a symbol of faithfulness and an "endless knot". From lines 640 to 654, the five points of the pentangle relate directly to Gawain in five ways: five senses, his five fingers, his faith found in the five wounds of Christ, the five joys of Mary (whose face was on the inside of the shield) and finally friendship, fraternity, purity, politeness and pity (traits that Gawain possessed around others). In line 625, it is described as "a sign by Solomon". Solomon, the third king of Israel, in the 10th century BC, was said to have the mark of the pentagram on his ring, which he received from the archangel Michael. The pentagram seal on this ring was said to give Solomon power over demons.

Along these lines, some academics link the Gawain pentangle to magical traditions. In Germany, the symbol was called a "Drudenfuß" and was placed on household objects to keep out evil. The symbol was also associated with magical charms that, if recited or written on a weapon, would call forth magical forces. However, concrete evidence tying the magical pentagram to Gawain's pentangle is scarce.

Gawain's pentangle also symbolises the "phenomenon of physically endless objects signifying a temporally endless quality." Many poets use the symbol of the circle to show infinity or endlessness, but Gawain's poet insisted on using something more complex. In medieval number theory, the number five is considered a "circular number", since it "reproduces itself in its last digit when raised to its powers". Furthermore, it replicates itself geometrically; that is, every pentangle has a smaller pentagon that allows a pentangle to be embedded in it and this "process may be repeated forever with decreasing pentangles". Thus, by reproducing the number five, which in medieval number symbolism signified incorruptibility, Gawain's pentangle represents his eternal incorruptibility.

Gawain's refusal of the Lady Bertilak's ring has major implications for the remainder of the story. While the modern student may tend to pay more attention to the girdle as the eminent object offered by the lady, readers in the time of Gawain would have noticed the significance of the offer of the ring as they believed that rings, and especially the embedded gems, had talismanic properties similarly done by the Gawain-poet in "Pearl". This is especially true of the lady's ring as scholars believe it to be a ruby or carbuncle, indicated when the Gawain-Poet describes it as a "brygt sunne" (line 1819), a "fiery sun." This red colour can be seen as symbolizing royalty, divinity, and the Passion of the Christ, something that Gawain as a knight of the Round Table would strive for, but this colour could also represent the negative qualities of temptation and covetousness. Given the importance of magic rings in Arthurian romance, this remarkable ring would also have been believed to protect the wearer from harm just as Lady Bertilak claims the girdle will.

The poet highlights number symbolism to add symmetry and meaning to the poem. For example, three kisses are exchanged between Gawain and Bertilak's wife; Gawain is tempted by her on three separate days; Bertilak goes hunting three times, and the Green Knight swings at Gawain three times with his axe. The number two also appears repeatedly, as in the two beheading scenes, two confession scenes, and two castles. The five points of the pentangle, the poet adds, represent Gawain's virtues, for he is "faithful five ways and five times each". The poet goes on to list the ways in which Gawain is virtuous: all five of his senses are without fault; his five fingers never fail him, and he always remembers the five wounds of Christ, as well as the five joys of the Virgin Mary. The fifth five is Gawain himself, who embodies the five moral virtues of the code of chivalry: "friendship, generosity, chastity, courtesy, and piety". All of these virtues reside, as the poet says, in the "Endless Knot" of the pentangle, which forever interlinks and is never broken. This intimate relationship between symbol and faith allows for rigorous allegorical interpretation, especially in the physical role that the shield plays in Gawain's quest. Thus, the poet makes Gawain the epitome of perfection in knighthood through number symbolism.
The number five is also found in the structure of the poem itself. "Sir Gawain" is 101 stanzas long, traditionally organised into four 'Fits' of 21, 24, 34, and 22 stanzas. These divisions, however, have since been disputed; scholars have begun to believe that they are the work of the copyist and not of the poet. The surviving manuscript features a series of capital letters added after the fact by another scribe, and some scholars argue that these additions were an attempt to restore the original divisions. These letters divide the manuscript into nine parts. The first and last parts are 22 stanzas long. The second and second-to-last parts are only one stanza long, and the middle five parts are eleven stanzas long. The number eleven is associated with transgression in other medieval literature (being one more than ten, a number associated with the Ten Commandments). Thus, this set of five elevens (55 stanzas) creates the perfect mix of transgression and incorruption, suggesting that Gawain is faultless in his faults.

At the story's climax, Gawain is wounded superficially in the neck by the Green Knight's axe. During the medieval period, the body and the soul were believed to be so intimately connected that wounds were considered an outward sign of inward sin. The neck, specifically, was believed to correlate with the part of the soul related to will, connecting the reasoning part (the head) and the courageous part (the heart). Gawain's sin resulted from using his will to separate reasoning from courage. By accepting the girdle from the lady, he employs reason to do something less than courageous—evade death in a dishonest way. Gawain's wound is thus an outward sign of an internal wound. The Green Knight's series of tests shows Gawain the weakness that has been in him all along: the desire to use his will pridefully for personal gain, rather than submitting his will in humility to God. The Green Knight, by engaging with the greatest knight of Camelot, also reveals the moral weakness of pride in all of Camelot, and therefore all of humanity. However, the wounds of Christ, believed to offer healing to wounded souls and bodies, are mentioned throughout the poem in the hope that this sin of prideful "stiffneckedness" will be healed among fallen mortals.

Many critics argue that "Sir Gawain and the Green Knight" should be viewed, above all, as a romance. Medieval romances typically recount the marvellous adventures of a chivalrous, heroic knight, often of super-human ability, who abides by chivalry's strict codes of honour and demeanour, embarks upon a quest and defeats monsters, thereby winning the favour of a lady. Thus, medieval romances focus not on love and sentiment (as the term "romance" implies today), but on adventure.

Gawain's function, as medieval scholar Alan Markman says, "is the function of the romance hero … to stand as the champion of the human race, and by submitting to strange and severe tests, to demonstrate human capabilities for good or bad action." Through Gawain's adventure, it becomes clear that he is merely human. The reader becomes attached to this human view in the midst of the poem's romanticism, relating to Gawain's humanity while respecting his knightly qualities. Gawain "shows us what moral conduct is. We shall probably not equal his behaviour, but we admire him for pointing out the way."

In viewing the poem as a medieval romance, many scholars see it as intertwining chivalric and courtly love laws under the English Order of the Garter. The group's motto, 'honi soit qui mal y pense', or "Shamed be he who finds evil here," is written at the end of the poem. Some critics describe Gawain's peers wearing girdles of their own as evidence of the origin of the Order of the Garter. However, in the parallel poem "The Greene Knight", the lace is white, not green, and is considered the origin of the collar worn by the knights of the Bath, not the Order of the Garter. The motto on the poem was probably written by a copyist and not by the original author. Still, the connection made by the copyist to the Order is not extraordinary.

The poem is in many ways deeply Christian, with frequent references to the fall of Adam and Eve and to Jesus Christ. Scholars have debated the depth of the Christian elements within the poem by looking at it in the context of the age in which it was written, coming up with varying views as to what represents a Christian element of the poem and what does not. For example, some critics compare "Sir Gawain" to the other three poems of the "Gawain" manuscript. Each has a heavily Christian theme, causing scholars to interpret "Gawain" similarly. Comparing it to the poem "Cleanness" (also known as "Purity"), for example, they see it as a story of the apocalyptic fall of a civilisation, in "Gawain's" case, Camelot. In this interpretation, Sir Gawain is like Noah, separated from his society and warned by the Green Knight (who is seen as God's representative) of the coming doom of Camelot. Gawain, judged worthy through his test, is spared the doom of the rest of Camelot. King Arthur and his knights, however, misunderstand Gawain's experience and wear garters themselves. In "Cleanness" the men who are saved are similarly helpless in warning their society of impending destruction.

One of the key points stressed in this interpretation is that salvation is an individual experience difficult to communicate to outsiders. In his depiction of Camelot, the poet reveals a concern for his society, whose inevitable fall will bring about the ultimate destruction intended by God. "Gawain" was written around the time of the Black Death and Peasants' Revolt, events which convinced many people that their world was coming to an apocalyptic end and this belief was reflected in literature and culture. However, other critics see weaknesses in this view, since the Green Knight is ultimately under the control of Morgan le Fay, often viewed as a figure of evil in Camelot tales. This makes the knight's presence as a representative of God problematic.

While the character of the Green Knight is usually not viewed as a representation of Christ in "Sir Gawain and the Green Knight", critics do acknowledge a parallel. Lawrence Besserman, a specialist in medieval literature, explains that "the Green Knight is not a figurative representative of Christ. But the idea of Christ's divine/human nature provides a medieval conceptual framework that supports the poet's serious/comic account of the Green Knight's supernatural/human qualities and actions." This duality exemplifies the influence and importance of Christian teachings and views of Christ in the era of the Gawain Poet.

Furthermore, critics note the Christian reference to Christ's crown of thorns at the conclusion of "Sir Gawain and the Green Knight". After Gawain returns to Camelot and tells his story regarding the newly acquired green sash, the poem concludes with a brief prayer and a reference to "the thorn-crowned God". Besserman theorises that "with these final words the poet redirects our attention from the circular girdle-turned-sash (a double image of Gawain's "yntrawpe/renoun") to the circular Crown of Thorns (a double image of Christ's humiliation turned triumph)."

Throughout the poem, Gawain encounters numerous trials testing his devotion and faith in Christianity. When Gawain sets out on his journey to find the Green Chapel, he finds himself lost, and only after praying to the Virgin Mary does he find his way. As he continues his journey, Gawain once again faces anguish regarding his inevitable encounter with the Green Knight. Instead of praying to Mary, as before, Gawain places his faith in the girdle given to him by Bertilak's wife. From the Christian perspective, this leads to disastrous and embarrassing consequences for Gawain as he is forced to reevaluate his faith when the Green Knight points out his betrayal. Another interpretation sees the work in terms of the perfection of virtue, with the pentangle representing the moral perfection of the connected virtues, the Green Knight as Christ exhibiting perfect fortitude, and Gawain as slightly imperfect in fortitude by virtue of flinching when under the threat of death.

An analogy is also made between Gawain's trial and the Biblical test that Adam encounters in the Garden of Eden. Adam succumbs to Eve just as Gawain surrenders to Bertilak's wife by accepting the girdle. Although Gawain sins by putting his faith in the girdle and not confessing when he is caught, the Green Knight pardons him, thereby allowing him to become a better Christian by learning from his mistakes. Through the various games played and hardships endured, Gawain finds his place within the Christian world.

Feminist literary critics see the poem as portraying women's ultimate power over men. Morgan le Fay and Bertilak's wife, for example, are the most powerful characters in the poem—Morgan especially, as she begins the game by enchanting the Green Knight. The girdle and Gawain's scar can be seen as symbols of feminine power, each of them diminishing Gawain's masculinity. Gawain's misogynist passage, in which he blames all of his troubles on women and lists the many men who have fallen prey to women's wiles, further supports the feminist view of ultimate female power in the poem.

In contrast, others argue that the poem focuses mostly on the opinions, actions, and abilities of men. For example, on the surface, it appears that Bertilak's wife is a strong leading character. By adopting the masculine role, she appears to be an empowered individual, particularly in the bedroom scene. This is not entirely the case, however. While the Lady is being forward and outgoing, Gawain's feelings and emotions are the focus of the story, and Gawain stands to gain or lose the most. The Lady "makes the first move", so to speak, but Gawain ultimately decides what is to become of those actions. He, therefore, is in charge of the situation and even the relationship.

In the bedroom scene, both the negative and positive actions of the Lady are motivated by her desire. Her feelings cause her to step out of the typical female role and into that of the male, thus becoming more empowered. At the same time, those same actions make the Lady appear adulterous; some scholars compare her with Eve in the Bible. By forcing Gawain to take her girdle, i.e. the apple, the pact made with Bertilak—and therefore the Green Knight—is broken. In this sense, it is clear that at the hands of the Lady, Gawain is a "good man seduced".

From 1350 to 1400—the period in which the poem is thought to have been written—Wales experienced several raids at the hands of the English, who were attempting to colonise the area. The Gawain poet uses a North West Midlands dialect common on the Welsh–English border, potentially placing him in the midst of this conflict. Patricia Clare Ingham is credited with first viewing the poem through the lens of postcolonialism, and since then a great deal of dispute has emerged over the extent to which colonial differences play a role in the poem. Most critics agree that gender plays a role, but differ about whether gender supports the colonial ideals or replaces them as English and Welsh cultures interact in the poem.

A large amount of critical debate also surrounds the poem as it relates to the bi-cultural political landscape of the time. Some argue that Bertilak is an example of the hybrid Anglo-Welsh culture found on the Welsh–English border. They therefore view the poem as a reflection of a hybrid culture that plays strong cultures off one another to create a new set of cultural rules and traditions. Other scholars, however, argue that historically much Welsh blood was shed well into the 14th century, creating a situation far removed from the more friendly hybridisation suggested by Ingham. To support this argument further, it is suggested that the poem creates an "us versus them" scenario contrasting the knowledgeable civilised English with the uncivilised borderlands that are home to Bertilak and the other monsters that Gawain encounters.

In contrast to this perception of the colonial lands, others argue that the land of Hautdesert, Bertilak's territory, has been misrepresented or ignored in modern criticism. They suggest that it is a land with its own moral agency, one that plays a central role in the story. Bonnie Lander, for example, argues that the denizens of Hautdesert are "intelligently immoral", choosing to follow certain codes and rejecting others, a position which creates a "distinction … of moral insight versus moral faith". Lander thinks that the border dwellers are more sophisticated because they do not unthinkingly embrace the chivalric codes but challenge them in a philosophical, and—in the case of Bertilak's appearance at Arthur's court—literal sense. Lander's argument about the superiority of the denizens of Hautdesert hinges on the lack of self-awareness present in Camelot, which leads to an unthinking populace that frowns on individualism. In this view, it is not Bertilak and his people, but Arthur and his court, who are the monsters.

Several scholars have attempted to find a real-world correspondence for Gawain's journey to the Green Chapel. The Anglesey islands, for example, are mentioned in the poem. They exist today as a single island off the coast of Wales. In line 700, Gawain is said to pass the "Holy Head", believed by many scholars to be either Holywell or the Cistercian abbey of Poulton in Pulford. Holywell is associated with the beheading of Saint Winifred. As the story goes, Winifred was a virgin who was beheaded by a local leader after she refused his sexual advances. Her uncle, another saint, put her head back in place and healed the wound, leaving only a white scar. The parallels between this story and Gawain's make this area a likely candidate for the journey.

Gawain's trek leads him directly into the centre of the Pearl Poet's dialect region, where the candidates for the locations of the Castle at Hautdesert and the Green Chapel stand. Hautdesert is thought to be in the area of Swythamley in northwest Midland, as it lies in the writer's dialect area and matches the topographical features described in the poem. The area is also known to have housed all of the animals hunted by Bertilak (deer, boar, fox) in the 14th century. The Green Chapel is thought to be in either Lud's Church or Wetton Mill, as these areas closely match the descriptions given by the author. Ralph Elliott located the chapel ("two myle henne" v1078) from the old manor house at Swythamley Park at the bottom of a valley ("bothm of the brem valay" v2145) on a hillside ("loke a littel on the launde, on thi lyfte honde" v2147) in an enormous fissure ("an olde caue,/or a creuisse of an olde cragge" v2182-83). Several have tried to replicate this expedition and others such as Michael Twomey have created a virtual tour of Gawain's journey entitled 'Travels with Sir Gawain' that include photographs of landscapes mentioned and particular views mentioned in the text.

According to Queer scholar Richard Zeikowitz, the Green Knight represents a threat to homosocial friendship in his medieval world. Zeikowitz argues that the narrator of the poem seems entranced by the Knight's beauty, homoeroticising him in poetic form. The Green Knight's attractiveness challenges the homosocial rules of King Arthur's court and poses a threat to their way of life. Zeikowitz also states that Gawain seems to find Bertilak as attractive as the narrator finds the Green Knight. Bertilak, however, follows the homosocial code and develops a friendship with Gawain. Gawain's embracing and kissing Bertilak in several scenes thus represents not a homosexual but a homosocial expression. Men of the time often embraced and kissed and this was acceptable under the chivalric code. Nonetheless, Zeikowitz claims the Green Knight blurs the lines between homosociality and homosexuality, representing the difficulty medieval writers sometimes had in separating the two.

Queer scholar Carolyn Dinshaw argues that the poem may have been a response to accusations that Richard II had a male lover—an attempt to reestablish the idea that heterosexuality was the Christian norm. Around the time the poem was written, the Catholic Church was beginning to express concerns about kissing between males. Many religious figures were trying to make the distinction between strong trust and friendship between males and homosexuality. She asserts that the Pearl Poet seems to have been simultaneously entranced and repulsed by homosexual desire. According to Dinshaw, in his other poem "Cleanness", he points out several grievous sins, but spends lengthy passages describing them in minute detail, and she sees this alleged' obsession' as carryong over to "Gawain" in his descriptions of the Green Knight.

Beyond this, Dinshaw proposes that Gawain can be read as a woman-like figure. In her view, he is the passive one in the advances of Lady Bertilak, as well as in his encounters with Lord Bertilak, where he acts the part of a woman in kissing the man. However, while the poem does have homosexual elements, these elements are brought up by the poet in order to establish heterosexuality as the normal lifestyle of Gawain's world. The poem does this by making the kisses between Lady Bertilak and Gawain sexual in nature, but rendering the kisses between Gawain and Lord Bertilak "unintelligible" to the medieval reader. In other words, the poet portrays kisses between a man and a woman as having the possibility of leading to sex, while in a heterosexual world kisses between a man and a man are portrayed as having no such possibility.

Though the surviving manuscript dates from the fourteenth century, the first published version of the poem did not appear until as late as 1839, when Sir Frederic Madden of the British Museum recognized the poem as worth reading. Madden's scholarly, Middle English edition of the poem was followed in 1898 by the first Modern English translation – a prose version by literary scholar Jessie L. Weston. In 1925, J. R. R. Tolkien and E. V. Gordon published a scholarly edition of the Middle English text of "Sir Gawain and the Green Knight"; a revised edition of this text was prepared by Norman Davis and published in 1967. The book, featuring a text in Middle English with extensive scholarly notes, is frequently confused with the translation into Modern English that Tolkien prepared, along with translations of "Pearl" and "Sir Orfeo", late in his life. Many editions of the latter work, first published in 1975, shortly after his death, list Tolkien on the cover as author rather than translator.

For students, especially undergraduate students, the text is usually given in translation. Notable translators include Jessie Weston, whose 1898 prose translation and 1907 poetic translation took many liberties with the original; Theodore Banks, whose 1929 translation was praised for its adaptation of the language to modern usage; and Marie Borroff, whose imitative translation was first published in 1967 and "entered the academic canon" in 1968, in the second edition of the "Norton Anthology of English Literature". In 2010, her (slightly revised) translation was published as a Norton Critical Edition, with a foreword by Laura Howes. In 2007, Simon Armitage, who grew up near the Gawain poet's purported residence, published a translation which attracted attention in the US and the United Kingdom, and was published in the United States by Norton, which replaced Borroff's translation with Armitage's for the ninth edition of the "Norton Anthology of English Literature". Other modern translations include those by Larry Benson, Brian Stone, James Winny, Helen Cooper, W. S. Merwin, Jacob Rosenberg, William Vantuono, Joseph Glaser, Bernard O'Donoghue, John Gardner, and Francis Ingledew. In 2015, Zach Weinersmith published "Augie and the Green Knight", a children's adaptation in which the protagonist is a young girl. In 2017, the graphic novel adaptation "Gawain and the Green Knight" was self-published by Emily Cheeseman. An illustrated contextual translation of the work by historian and printmaker Michael Smith was published in July 2018 by Unbound. The young adult novel "The Squire, His Knight, and His Lady" by Gerald Morris includes a faithful adaptation of the story.

The poem has been adapted to film three times, twice by writer-director Stephen Weeks: first as "Gawain and the Green Knight" in 1973 and again in 1984 as "", featuring Miles O'Keeffe as Gawain and Sean Connery as the Green Knight. Both films have been criticized for deviating from the poem's plot. Also, Bertilak and the Green Knight are never connected. French/Australian director Martin Beilby directed a short (30') film adaptation in 2014. There have been at least two television adaptations, "Gawain and the Green Knight" in 1991 and the animated "Sir Gawain and the Green Knight" in 2002. The BBC broadcast a documentary presented by Simon Armitage in which the journey depicted in the poem is traced, using what are believed to be the actual locations. On 5 November 2018, it was announced that a new film adaptation titled "The Green Knight" is in the works, to be directed by American filmmaker David Lowery for A24.

The poem was also adapted for the "Adventure Time" episode "Seventeen" where the Green Knight arrives on Finn's 17th birthday, issuing him the challenge, as well as a series of follow-up games.

The Tyneside Theatre company presented a stage version of "Sir Gawain and the Green Knight" at the University Theatre, Newcastle at Christmas 1971. It was directed by Michael Bogdanov and adapted for the stage from the translation by Brian Stone. The music and lyrics were composed by Iwan Williams using medieval carols, such as the "Boar's Head Carol", as inspiration and folk instruments such as the Northumbrian pipes, whistles and bhodran to create a "rough" feel. Stone had referred Bogdanov to "Cuchulain and the Beheading Game", a sequence which is contained in The Grenoside Sword dance. Bogdanov found the pentangle theme to be contained in most sword dances, and so incorporated a long sword dance while Gawain lay tossing uneasily before getting up to go to the Green Chapel. The dancers made the knot of the pentangle around his drowsing head with their swords. The interlacing of the hunting and wooing scenes was achieved by frequent cutting of the action from hunt to bed-chamber and back again, while the locale of both remained on-stage.

In 1992 Simon Corble created an adaptation with medieval songs and music for The Midsommer Actors' Company. performed as walkabout productions in the summer 1992 at Thurstaston Common and Beeston Castle and in August 1995 at Brimham Rocks, North Yorkshire. Corble later wrote a substantially revised version which was produced indoors at the O'Reilly Theatre, Oxford in February 2014.

"Sir Gawain and the Green Knight" was first adapted as an opera in 1978 by the composer Richard Blackford on commission from the village of Blewbury, Oxfordshire. The libretto was written for the adaptation by the children's novelist John Emlyn Edwards. The "Opera in Six Scenes" was subsequently recorded by Decca between March and June 1979 and released on the Argo label in November 1979.

"Sir Gawain and the Green Knight" was adapted into an opera called "Gawain" by Harrison Birtwistle, first performed in 1991. Birtwistle's opera was praised for maintaining the complexity of the poem while translating it into lyric, musical form. Another operatic adaptation is Lynne Plowman's "Gwyneth and the Green Knight", first performed in 2002. This opera uses "Sir Gawain" as the backdrop but refocuses the story on Gawain's female squire, Gwyneth, who is trying to become a knight. Plowman's version was praised for its approachability, as its target is the family audience and young children, but criticised for its use of modern language and occasional preachy nature.



</doc>
<doc id="28834" url="https://en.wikipedia.org/wiki?curid=28834" title="Sultan Bashiruddin Mahmood">
Sultan Bashiruddin Mahmood

Sultan Bashiruddin Mahmood (; b. 1940;, ) is a Pakistani nuclear engineer and a scholar of Islamic studies. He was the subject of a criminal investigation launched by the Federal Investigation Agency (FIA) over unauthorized travel in Afghanistan prior to the September 11 attacks in 2001.

Having spent a distinguished career in the Pakistan Atomic Energy Commission (PAEC), he founded the Ummah Tameer-e-Nau (UTN) in 1999– a right-wing organization that was banned and sanctioned by the United States in 2001. Mahmood was among those who were listed and sanctioned by the Al-Qaida Sanctions Committee in December 2001. Having been cleared by the FIA, he has been living in anonymity in Islamabad, authoring books on the relationship between Islam and science.

Mahmood was born in Amritsar, Punjab, British India to a Punjabi family. There are conflicting reports concerning his date of birth; his personal admission noted the birth year as 1940, while the UN reports estimated as 1938. His father, Chaudhry Muhammad Sharif, was a local "Zamindar" (lit. feudal lord). His family emigrated from India to Pakistan following religious violence in India in 1947; the family settled in Lahore, Punjab.

After graduating with distinctions from a local high school standing at top of his class, Mahmood was awarded a scholarship and enrolled at the Government College University to study electrical engineering. After spending a semester, he transferred to the University of Engineering and Technology, Lahore, and graduated with a Bachelor of Science with honors in 1960. His credentials led him to join the Pakistan Atomic Energy Commission (PAEC) where he gained another scholarship to study in the United Kingdom.

In 1962, Mahmood went to attend the University of Manchester where he studied for a double master's degree. First completing a masters' program in control systems in 1965, he then received another master's degree in nuclear engineering in 1969 from the University of Manchester. While in Manchester, Mahmood was an expert on the Manhattan Project and was reportedly in contact with South African scientists in discussing the jet-nozzle method for uranium enrichment. However, it remains unclear how much interaction was taken place during that time.

Mahmood joined the Pakistan Atomic Energy Commission (PAEC) in 1968, joining the Nuclear Physics Division at the Pakistan Institute of Nuclear Science and Technology (PINSTECH) working under Dr. Naeem Ahmad Khan. His collaboration took place with Samar Mubarakmand, Hafeez Qureshi, and he was a vital member of the group before it was discontinued in 1970. Mahmood was one of the foremost experts on civilian reactor technology and was a senior engineer at the Karachi Nuclear Power Plant (KANUPP I)— the first commercial nuclear power plant in Pakistan. He gained notability and publicity in the Pakistan Physics Society for inventing a scientific instrument, the 'SBM probe', to detect leaks in steam pipes, a problem that was affecting nuclear plants all over the world and is still used worldwide.

After witnessing the Indo-Pakistani War of 1971, which saw the unconditional surrender of Pakistan in 1971, Mahmood attended the winter seminar at Multan and delivered a speech on atomic science. On 20 January 1972, the President of Pakistan, Zulfikar Ali Bhutto, approved a crash atomic weapon program, under Munir Ahmad Khan, for the sake of "national survivor." Though, he continued his work at the KANUPP I engineering division.

In the aftermath of 'Smiling Buddha', a surprise nuclear test conducted by India in May 1974, Munir Ahmad appointed Mahmood as the director of the enrichment division at PAEC, where the majority of calculations were conducted by Dr. Khalil Qureshi– a physical chemist. Mahmood analyzed the gaseous diffusion, gas centrifuge, jet-nozzle and molecular laser isotope separation method for uranium-enrichment; recommending the gas centrifuge method as economical. After submitting the report, Mahmood was asked to depart to the Netherlands to interview Dr. Abdul Qadeer Khan on behalf of President Bhutto in 1974. In 1975, his proposal was approved and the work on uranium enrichment started with Mahmood as its director, a move that irked the more qualified but more difficult to manage Dr. Abdul Qadeer Khan, who had coveted the job for himself. His relations with Dr. Khan remains extremely tense and the pairs disagreed with each other and developed differences at great height. In private meetings with Munir Ahmad, Mahmood often complained and pictured him as "egomaniac". In 1976, Mahmood was removed from the enrichment division, Project-706, by Abdul Qadeer Khan, and Khan moved the enrichment division at the Engineering Research Laboratories (ERL) under military control.

Eventually, Munir Ahmad removed Mahmood from other classified works and posted him back to the Karachi Nuclear Power Plant (KANUPP-I) with no reason given as a principle engineer. In the 1980s, Munir Ahmad secured Mahmood a job as project manager for the construction of the Khushab Reactor (Khushab-I) where he served as chief engineer and aided with designing the coolant systems. In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.

After the reactor went critical in April 1998, Mahmood said in an interview: ""This reactor (can produce enough plutonium for two to three nuclear weapons per year) Pakistan had "acquired the capability to produce... boosted thermonuclear weapons and hydrogen bombs"." In 1998, Mahmood was honored with the Sitara-e-Imtiaz award in a ceremony by Prime Minister Nawaz Sharif.

In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.

Though publicly endorsing the 1988 decision to carry out the Chagai-I nuclear tests by Prime Minister Sharif, Mahmood began appearing on news channels as an outspoken opponent of Sharif, as Mahmood vehemently opposed Pakistan becoming a signatory state of the Nuclear Nonproliferation Treaty (NPT) and Comprehensive Nuclear-Test-Ban Treaty (CTBT). In Pakistan's popular news channels and newspapers, Mahmood gave numerous interviews, wrote articles, and lobbied against Sharif when learning that the Prime Minister had been willing to sign anti-nuclear weapon treaties, prompting the Pakistan Government to forcefully transfer Mahmood to a non-technical position at PAEC.

Seeking premature retirement from PAEC in 1999, Mahmood moved towards publishing books and articles involving the relationship between Islam and science. Mahmood founded the Ummah Tameer-e-Nau (UTN)– a right-wing organization– with his close associates. In 2000, he began attending lectures and religious sessions with Dr. Israr Ahmed who would later influence his political views and philosophy. Through UTN, Mahmood stepped into more radical politics, and began visiting Afghanistan where he wanted to be focused on rebuilding educational institutions, hospitals, and relief work.

In August 2001, Mahmood and his colleague Chaudhry Abdul Majeed at the UTN met with Osama bin Laden and Ayman al-Zawahiri in Kandahar, Afghanistan. Describing the meeting, the "New York Times" editorial quoted:""There is little doubt that Mahmood talked to the two al-Qaeda leaders about nuclear weapons, or that Al Qaeda desperately wanted the bomb"".

Since 1999 and 2000 onwards, Pakistan's intelligence community had been tracking and monitoring Mahmood whose bushy beard advertised his deep attachment to the Afghan Taliban. After the September 11 attacks in the United States, the Federal Investigation Agency (FIA) launched a criminal investigation against him, leveling charges of unauthorized travel to Afghanistan. Director of the Central Intelligence Agency, George Tenet, later described intelligence reports of his meeting with Al Qaeda as ""frustratingly vague"." When asked by Pakistani and American investigators about the nature of Ummah Tameer-e-Nau's (UTN) work and discussions, Mahmood said that he had nothing to do with the al-Qaeda and was only working on humanitarian issues like food, health and education. Investigators from Inter-Services Intelligence (ISI) and the Central Intelligence Agency (CIA) were astonished and surprised when finding out that Mahmood knew nothing of nuclear weapons as 

During his debriefing, his son Dr. Asim Mahmood, a family medicine doctor, told ISI officials that: "My father [Mahmood] did meet with Osama bin Laden and Osama Bin Laden seemed interested in that matter but my father showed no interest in the matter as he met him for food, water and healthcare matters on which his charity was working".

The FIA criminal probe continued for four months and yielded no concrete results. Pressure from Pakistani society and court inquiries against the FIA's criminal probe led to Mahmood's release in 2001. His family did confirm his release but had been constantly under surveillance by the FIA; his name was placed on the "Exit Control Lists" so he is not allowed to travel out of Pakistan. Since his release, Mahmood has been out of the public eye and lives a quiet life in Islamabad, devoting most of his time to writing books and doing research work on Islam and science.

Dr. Bashir Syed, former president of the Association of Pakistani Scientists and Engineers of North America (APSENA), said: "I know both of these persons and can tell you there is not an iota of truth that both these respected scientists and friends will do anything to harm the interest of their own country."

Mahmood has written over fifteen books, the most well-known being ""The Mechanics of Doomsday and Life After Death"", which is an analysis of the events leading to doomsday in light of scientific theories and Quranic knowledge. However, his scientific arguments and theories have been challenged by some prominent scientists in Pakistan. His religiosity and eccentricity began troubling the Pakistan Physics Society; his peers often quoted him as "a rather strange man".

In 1988, Mahmood was invited to the University of Islamabad to deliver a lecture on science. During his lecture at the university's 'Physics Hall' he and several other academcians debated his book. While debating, a well known Pakistani nuclear physicist, Dr. Pervez Hoodbhoy, and Mahmood, had an acrimonious public debate. Hoodbhoy had severely criticised Mahmood's theories and the notion of Islamic science in general, calling it "ludicrous science." Mahmood protested that Dr. Hoodbhoy misrepresented his views, quoting: "This is crossing all limits of decency," he wrote. "But should one expect any honesty or decency from anti-Islamic sources?"

In his writings and speeches, Mahmood has advocated for nuclear sharing with other Islamic nations which he believed would give rise to Muslim dominance in the world. He has also written a tafseer of the Quran in English.

Mahmood is reported to be fascinated "with the role sunspots played in triggering the French and Russian Revolutions, World War II and assorted anti-colonial uprisings." According to his book ""Cosmology and Human Destiny"", Mahmood argued that sunspots have influenced major human events, including the French Revolution, the Russian Revolution, and World War II. He concluded that governments across the world ""are already being subjected to great emotional aggression under the catalytic effect of the abnormally high sunspot activity under which they are most likely to adapt aggression as the natural solution for their problems"". In this book, first published in 1998, he predicted that the period from 2007 to 2014 would be of great turmoil and destruction in the world. Other books written by him include a biography of the Islamic prophet Muhammad titled ""First and the Last"", while his other books are focused more on the relation between Islam and science like "Miraculous Quran", "Life After Death and Doomsday", and "Kitab-e-Zindagi" (in Urdu).

One passage of the book reportedly states: ""At the international level, terrorism will rule; and in this scenario use of mass destruction weapons cannot be ruled out. Millions, by 2020, may die through mass destruction weapons, hunger, disease, street violence, terrorist attacks, and suicide.""

Mahmood's lifelong friend, Member of Parliament Farhatullah Babar, who is currently serving as a spokesperson for the President of Pakistan, while talking to media, said: "Mahmood predicted in "Cosmology and Human Destiny" that "the year 2002 was likely to be a year of maximum sunspot activity. It means upheaval, particularly on the South Asia, with the possibility of nuclear exchanges"."

Mahmood has published papers concerning djinni, which are described in the Quran as beings made of fire. He has proposed that djinni could be tapped to solve the energy crisis. "I think that if we develop our souls, we can develop communication with them," Mahmood said about djinni in "The Wall Street Journal" in an interview in 1988: "Every new idea has its opponents," he added. "But there is no reason for this controversy over Islam and science because there is no conflict between Islam and science."

The New York Times has described Mahmood as "an autodidact intellectual with grand aspirations," and noted that "his fellow scientists at PAEC began to wonder if Mahmood was mentally sound." Mahmood made it clear that he believed Pakistan's bomb was "the property of the whole Ummah," referring to the worldwide Muslim community. "This guy was our ultimate nightmare," an American intelligence official told the Times in late 2001. He has been awarded a Gold Medal by the Pakistan Academy of Sciences.






</doc>
<doc id="28837" url="https://en.wikipedia.org/wiki?curid=28837" title="Siege tower">
Siege tower

A siege tower or breaching tower (or in the Middle Ages, a belfry) is a specialized siege engine, constructed to protect assailants and ladders while approaching the defensive walls of a fortification. The tower was often rectangular with four wheels with its height roughly equal to that of the wall or sometimes higher to allow archers to stand on top of the tower and shoot arrows into the fortification. Because the towers were wooden and thus flammable, they had to have some non-flammable covering of iron or fresh animal skins.

Used since the 11th century BC by the Babylonians and Assyrians in the ancient Near East, the 4th century BC in Europe and also in antiquity in the Far East, siege towers were of unwieldy dimensions and, like trebuchets, were therefore mostly constructed on site of the siege. Taking considerable time to construct, siege towers were mainly built if the defense of the opposing fortification could not be overcome by ladder assault ("escalade"), by mining or by breaking walls or gates.

The siege tower sometimes housed spearmen, pikemen, swordsmen, archers or crossbowmen who shot arrows and quarrels at the defenders. Because of the size of the tower it would often be the first target of large stone catapults but it had its own projectiles with which to retaliate.

Siege towers were used to get troops over an enemy curtain wall. When a siege tower was near a wall, it would drop a gangplank between it and the wall. Troops could then rush onto the walls and into the castle or city.

The oldest known siege towers were used by the armies of the Neo-Assyrian Empire in the 9th century BC, under Ashurnasirpal II (r. 884 BC – 859 BC). Reliefs from his reign, and subsequent reigns, depict siege towers in use with a number of other siege works, including ramps and battering rams.

Centuries after they were employed in Assyria, the use of the siege tower spread throughout the Mediterranean. The biggest siege towers of antiquity, such as the "Helepolis" (meaning ""The Taker of Cities"") of the siege of Rhodes in 305 BC, could be as high as 40 m and as wide as 20 m. Such large engines would require a rack and pinion to be moved effectively. It was manned by 200 soldiers and was divided into nine stories; the different levels housed various types of catapults and ballistae. Subsequent siege towers down through the centuries often had similar engines.

But this huge tower was defeated by the defenders by flooding the ground in front of the wall, creating a moat that caused the tower to get bogged in the mud. The siege of Rhodes illustrates the important point that the larger siege towers needed level ground. Many castles and hill-top towns and forts were virtually invulnerable to siege tower attack simply due to topography. Smaller siege towers might be used on top of siege-mounds, made of earth, rubble and timber mounds in order to overtop a defensive wall. The remains of such a siege-ramp at Masada, for example, has survived almost 2,000 years and can still be seen today.

On the other hand, almost all the largest cities were on large rivers, or the coast, and so did have part of their circuit wall vulnerable to these towers. Furthermore, the tower for such a target might be prefabricated elsewhere and brought dismantled to the target city by water. In some rare circumstances, such towers were mounted on ships to assault the coastal wall of a city: at the siege of Cyzicus during the Third Mithridatic War, for example, towers were used in conjunction with more conventional siege weapons.

One of the oldest references to the mobile siege tower in Ancient China was a written dialogue primarily discussing naval warfare. In the Chinese "Yuejueshu" (Lost Records of the State of Yue) written by the later Han Dynasty author Yuan Kang in the year 52 AD, Wu Zixu (526 BC – 484 BC) purportedly discussed different ship types with King Helü of Wu (r. 514 BC – 496 BC) while explaining military preparedness. Before labeling the types of warships used, Zixu said:

With the collapse of the Roman Empire in the West into independent states, and the Eastern Roman Empire on the defensive, the use of siege towers reached its height during the medieval period. Siege towers were used when the Avars laid siege unsuccessfully to Constantinople in 626, as the "Chronicon Paschale" recounts:

At this siege the attackers also made use of "sows" – mobile armoured shelters which were used throughout the medieval period, and allowed workers to fill in moats with protection from the defenders (thus levelling the ground for the siege towers to be moved to the walls). However, the construction of a sloping talus at the base of a castle wall (as was common in Crusader fortification) could have reduced the effectiveness of this tactic to an extent.

Siege towers also became more elaborate during the medieval period; at the Siege of Kenilworth Castle in 1266, for example, 200 archers and 11 catapults operated from a single tower. Even then, the siege lasted almost a year, making it the longest siege in all of English history. They were not invulnerable either, as during the Fall of Constantinople in 1453, Ottoman siege towers were sprayed by the defenders with Greek fire.

Siege towers became vulnerable and obsolete with the development of large cannon. They had only ever existed to get assaulting troops over high walls and towers and large cannon also made high walls obsolete as fortification took a new direction. However, later constructions known as battery-towers took on a similar role in the gunpowder age; like siege-towers, these were built out of wood on-site for mounting siege artillery. One of these was built by the Russian military engineer Ivan Vyrodkov during the siege of Kazan in 1552 (as part of the Russo-Kazan Wars), and could hold ten large-calibre cannon and 50 lighter cannons. Likely, it was a development of the gulyay-gorod (that is a mobile fortification assembled on wagons or sleds from prefabricated wall-sized shields with holes for cannons). Later battery towers were often used by the Ukrainian Cossacks.

On 1 March 2007, police officers entered Ungdomshuset in Copenhagen, Denmark by being lifted to the upper levels of an illegally occupied structure using small boom cranes for a purpose similar to that for which siege towers were constructed. The officers were placed in containers that crane operators raised and placed against the structure's windows, enabling the officers to gain access to the structure. Tactical vehicles such as the Lenco BearCat can be fitted with structures to enable access of elevated structures in a comparable fashion.


</doc>
<doc id="28840" url="https://en.wikipedia.org/wiki?curid=28840" title="Sharia">
Sharia

Sharia (, ), Islamic law, or redundantly Sharia law, is a religious law forming part of the Islamic tradition. It is derived from the religious precepts of Islam, particularly the Quran and the hadith. In Arabic, the term "sharīʿah" refers to God's immutable divine law and is contrasted with "fiqh", which refers to its human scholarly interpretations. The manner of its application in modern times has been a subject of dispute between Muslim fundamentalists and modernists.

Traditional theory of Islamic jurisprudence recognizes four sources of Sharia: the Quran, "sunnah" (authentic hadith), "qiyas" (analogical reasoning), and "ijma" (juridical consensus). Different legal schools—of which the most prominent are Hanafi, Maliki, Shafi'i, Hanbali and Jafari—developed methodologies for deriving Sharia rulings from scriptural sources using a process known as "ijtihad". Traditional jurisprudence ("fiqh") distinguishes two principal branches of law, "ʿibādāt" (rituals) and "muʿāmalāt" (social relations), which together comprise a wide range of topics. Its rulings are concerned with ethical standards as much as with legal norms, assigning actions to one of five categories: mandatory, recommended, neutral, abhorred, and prohibited. Thus, some areas of Sharia overlap with the Western notion of law while others correspond more broadly to living life in accordance with God's will.

Classical jurisprudence was elaborated by private religious scholars, largely through legal opinions (fatwas) issued by qualified jurists (muftis). It was historically applied in Sharia courts by ruler-appointed judges, who dealt mainly with civil disputes and community affairs. Sultanic courts, the police and market inspectors administered criminal justice, which was influenced by Sharia but not bound by its rules. Non-Muslim (dhimmi) communities had legal autonomy to adjudicate their internal affairs. Over the centuries, Sunni muftis were gradually incorporated into state bureaucracies, and fiqh was complemented by various economic, criminal and administrative laws issued by Muslim rulers. The Ottoman civil code of 1869–1876 was the first partial attempt to codify Sharia.

In the modern era, traditional laws in the Muslim world have been widely replaced by statutes inspired by European models. Judicial procedures and legal education were likewise brought in line with European practice. While the constitutions of most Muslim-majority states contain references to Sharia, its classical rules were largely retained only in personal status (family) laws. Legislators who codified these laws sought to modernize them without abandoning their foundations in traditional jurisprudence. The Islamic revival of the late 20th century brought along calls by Islamist movements for full implementation of Sharia, including "hudud" corporal punishments, such as stoning. In some cases, this resulted in traditionalist legal reform, while other countries witnessed juridical reinterpretation of Sharia advocated by progressive reformers. Some Muslim-minority countries recognize the use of Sharia-based family laws for their Muslim populations. Sharia also continues to influence other aspects of private and public life.

The role of Sharia has become a contested topic around the world. Introduction of Sharia-based laws sparked intercommunal violence in Nigeria and may have contributed to the breakup of Sudan. Some jurisdictions in North America have passed bans on use of Sharia, framed as restrictions on religious or foreign laws. There are ongoing debates as to whether Sharia is compatible with democracy, human rights, freedom of thought, women's rights, LGBT rights, and banking.

The word "sharīʿah" is used by Arabic-speaking peoples of the Middle East to designate a prophetic religion in its totality. For example, "sharīʿat Mūsā" means law or religion of Moses and "sharīʿatu-nā" can mean "our religion" in reference to any monotheistic faith. Within Islamic discourse, "šarīʿah" refers to religious regulations governing the lives of Muslims. For many Muslims, the word means simply "justice," and they will consider any law that promotes justice and social welfare to conform to Sharia.

Jan Michiel Otto distinguishes four senses conveyed by the term "sharia" in religious, legal and political discourse:

A related term "" (, Islamic law), which was borrowed from European usage in the late 19th century, is used in the Muslim world to refer to a legal system in the context of a modern state.

The primary range of meanings of the Arabic word "šarīʿah", derived from the root "š-r-ʕ", is related to religion and religious law. The lexicographical tradition records two major areas of use where the word "šarīʿah" can appear without religious connotation. In texts evoking a pastoral or nomadic environment, the word, and its derivatives refer to watering animals at a permanent water-hole or to the seashore, with special reference to animals who come there. Another area of use relates to notions of stretched or lengthy. This range of meanings is cognate with the Hebrew "saraʿ" and is likely to be the origin of the meaning "way" or "path". Both these areas have been claimed to have given rise to aspects of the religious meaning.

Some scholars describe the word "šarīʿah" as an archaic Arabic word denoting "pathway to be followed" (analogous to the Hebrew term Halakhah ["The Way to Go"]), or "path to the water hole" and argue that its adoption as a metaphor for a divinely ordained way of life arises from the importance of water in an arid desert environment.

In the Quran, "šarīʿah" and its cognate "širʿah" occur once each, with the meaning "way" or "path". The word "šarīʿah" was widely used by Arabic-speaking Jews during the Middle Ages, being the most common translation for the word "torah" in the 10th-century Arabic translation of the Torah by Saʿadya Gaon. A similar use of the term can be found in Christian writers. The Arabic expression "Sharīʿat Allāh" (شريعة الله "God’s Law") is a common translation for תורת אלוהים (‘God’s Law’ in Hebrew) and νόμος τοῦ θεοῦ (‘God’s Law’ in Greek in the New Testament [Rom. 7: 22]). In Muslim literature, "šarīʿah" designates the laws or message of a prophet or God, in contrast to "fiqh", which refers to a scholar's interpretation thereof.

In older English-language law-related works in the late 19th/early 20th centuries, the word used for Sharia was sheri. It, along with the French variant "chéri", was used during the time of the Ottoman Empire, and is from the Turkish "şer’"("i").

According to the traditional Muslim view, the major precepts of Sharia were passed down directly from the Islamic prophet Muhammad without "historical development," and the emergence of Islamic jurisprudence ("fiqh") also goes back to the lifetime of Muhammad. In this view, his companions and followers took what he did and approved of as a model (sunnah) and transmitted this information to the succeeding generations in the form of hadith. These reports led first to informal discussion and then systematic legal thought, articulated with greatest success in the eighth and ninth centuries by the master jurists Abu Hanifah, Malik ibn Anas, Al-Shafi‘i, and Ahmad ibn Hanbal, who are viewed as the founders of the Hanafi, Maliki, Shafiʿi, and Hanbali legal schools ("madhhabs") of Sunni jurisprudence.

Modern historians have presented alternative theories of the formation of fiqh. At first Western scholars accepted the general outlines of the traditional account. In the late 19th century, an influential revisionist hypothesis was advanced by Ignac Goldziher and elaborated by Joseph Schacht in the mid-20th century. Schacht and other scholars argued that having conquered much more populous agricultural and urban societies with already existing laws and legal needs unknown to the desert-dwelling conquerors, 
the initial Muslim efforts to formulate legal norms
regarded the Quran and Muhammad's hadiths as just one sources of law, with jurist personal opinions, the legal practice of conquered peoples, and the decrees and decisions of the caliphs also being valid sources. At least one source (historian Tom Holland) has argued that the strong scholarly tradition of Mobad among the conquered Zoroastrians of Persia and rabbis among the conquered Jews influenced the law of their largely illiterate warrior conquerors; and that this can explain such issues as why the Quran mentions only three prayers (24:58) while Muslims pray five times a day (Zoroastrians prayed five times a day) and why the Quran commands adulterers be lashed, while Sharia calls for their execution by stoning (Deuteronomy 22:21 of the Jewish Torah calls for stoning to death of women who have been found to have had sex before marriage).

According to this theory, most canonical hadiths did not originate with Muhammad but were actually created at a later date, despite the efforts of hadith scholars to weed out fabrications.
After it became accepted that legal norms must be formally grounded in scriptural sources, proponents of rules of jurisprudence supported by the hadith would extend the chains of transmission of the hadith back to Muhammad's companions. In his view, the real architect of Islamic jurisprudence was Al-Shafi‘i (d. 820 CE/204 AH), who formulated this idea (that legal norms must be formally grounded in scriptural sources) and other elements of classical legal theory in his work "al-risala", but who was preceded by a body of Islamic law not based on primacy of Muhammad's hadiths.

While the origin of hadith remains a subject of scholarly controversy, this theory (of Goldziher and Schacht) has given rise to objections, and modern historians generally adopt more cautious, intermediate positions, 
and it is generally accepted that early Islamic jurisprudence developed out of a combination of administrative and popular practices shaped by the religious and ethical precepts of Islam. It continued some aspects of pre-Islamic laws and customs of the lands that fell under Muslim rule in the aftermath of the early conquests and modified other aspects, aiming to meet the practical need of establishing Islamic norms of behavior and adjudicating disputes arising in the early Muslim communities. Juristic thought gradually developed in study circles, where independent scholars met to learn from a local master and discuss religious topics. At first, these circles were fluid in their membership, but with time distinct regional legal schools crystallized around shared sets of methodological principles. As the boundaries of the schools became clearly delineated, the authority of their doctrinal tenets came to be vested in a master jurist from earlier times, who was henceforth identified as the school's founder. In the course of the first three centuries of Islam, all legal schools came to accept the broad outlines of classical legal theory, according to which Islamic law had to be firmly rooted in the Quran and hadith.

Fiqh is traditionally divided into the fields of "uṣūl al-fiqh" (lit. the roots of fiqh), which studies the theoretical principles of jurisprudence, and "furūʿ al-fiqh" (lit. the branches of fiqh), which is devoted to elaboration of rulings on the basis of these principles.

Classical jurists held that human reason is a gift from God which should be exercised to its fullest capacity. However, they believed that use of reason alone is insufficient to distinguish right from wrong, and that rational argumentation must draw its content from the body of transcendental knowledge revealed in the Quran and through the sunnah of Muhammad.

Traditional theory of Islamic jurisprudence elaborates how scriptures should be interpreted from the standpoint of linguistics and rhetoric. It also comprises methods for establishing authenticity of hadith and for determining when the legal force of a scriptural passage is abrogated by a passage revealed at a later date. In addition to the Quran and sunnah, the classical theory of Sunni fiqh recognizes two other sources of law: juristic consensus ("ijmaʿ") and analogical reasoning ("qiyas"). It therefore studies the application and limits of analogy, as well as the value and limits of consensus, along with other methodological principles, some of which are accepted by only certain legal schools. This interpretive apparatus is brought together under the rubric of ijtihad, which refers to a jurist's exertion in an attempt to arrive at a ruling on a particular question. The theory of Twelver Shia jurisprudence parallels that of Sunni schools with some differences, such as recognition of reason ("ʿaql") as a source of law in place of "qiyas" and extension of the notion of sunnah to include traditions of the imams.


The classical process of ijtihad combined these generally recognized principles with other methods, which were not adopted by all legal schools, such as "istihsan" (juristic preference), "istislah" (consideration of public interest) and "istishab" (presumption of continuity). A jurist who is qualified to practice ijtihad is known as a "mujtahid". The use of independent reasoning to arrive at a ruling is contrasted with "taqlid" (imitation), which refers to following the rulings of a mujtahid. By the beginning of the 10th century, development of Sunni jurisprudence prompted leading jurists to state that the main legal questions had been addressed and the scope of ijtihad was gradually restricted. From the 18th century on, leading Muslim reformers began calling for abandonment of taqlid and renewed emphasis on ijtihad, which they saw as a return to the vitality of early Islamic jurisprudence.

Fiqh is concerned with ethical standards as much as with legal norms, seeking to establish not only what is and is not legal, but also what is morally right and wrong. Sharia rulings fall into one of five categories known as “the five decisions” ("al-aḥkām al-khamsa"): mandatory ("farḍ" or "wājib"), recommended ("mandūb" or "mustaḥabb"), neutral ("mubāḥ"), reprehensible ("makrūh"), and forbidden ("ḥarām"). It is a sin or a crime to perform a forbidden action or not to perform a mandatory action. Reprehensible acts should be avoided, but they are not considered to be sinful or punishable in court. Avoiding reprehensible acts and performing recommended acts is held to be subject of reward in the afterlife, while neutral actions entail no judgement from God. Jurists disagree on whether the term "ḥalāl" covers the first three or the first four categories. The legal and moral verdict depends on whether the action is committed out of necessity ("ḍarūra") and on the underlying intention ("niyya"), as expressed in the legal maxim "acts are [evaluated according] to intention."

"Maqāṣid" (aims or purposes) of Sharia and "maṣlaḥa" (welfare or public interest) are two related classical doctrines which have come to play an increasingly prominent role in modern times. They were first clearly articulated by al-Ghazali (d. 1111), who argued that "maslaha" was God's general purpose in revealing the divine law, and that its specific aim was preservation of five essentials of human well-being: religion, life, intellect, offspring, and property. Although most classical-era jurists recognized "maslaha" and "maqasid" as important legal principles, they held different views regarding the role they should play in Islamic law. Some jurists viewed them as auxiliary rationales constrained by scriptural sources and analogical reasoning. Others regarded them as an independent source of law, whose general principles could override specific inferences based on the letter of scripture. While the latter view was held by a minority of classical jurists, in modern times it came to be championed in different forms by prominent scholars who sought to adapt Islamic law to changing social conditions by drawing on the intellectual heritage of traditional jurisprudence. These scholars expanded the inventory of "maqasid" to include such aims of Sharia as reform and women's rights (Rashid Rida); justice and freedom (Mohammed al-Ghazali); and human dignity and rights (Yusuf al-Qaradawi).

The domain of "furūʿ al-fiqh" (lit. branches of fiqh) is traditionally divided into "ʿibādāt" (rituals or acts of worship) and "muʿāmalāt" (social relations). Many jurists further divided the body of substantive jurisprudence into "the four quarters", called rituals, sales, marriage and injuries. Each of these terms figuratively stood for a variety of subjects. For example, the quarter of sales would encompass partnerships, guaranty, gifts, and bequests, among other topics. Juristic works were arranged as a sequence of such smaller topics, each called a "book" ("kitab"). The special significance of ritual was marked by always placing its discussion at the start of the work.

Some historians distinguish a field of Islamic criminal law, which combines several traditional categories. Several crimes with scripturally prescribed punishments are known as "hudud". Jurists developed various restrictions which in many cases made them virtually impossible to apply. Other crimes involving intentional bodily harm are judged according to a version of "lex talionis" that prescribes a punishment analogous to the crime ("qisas"), but the victims or their heirs may accept a monetary compensation ("diya") or pardon the perpetrator instead; only "diya" is imposed for non-intentional harm. Other criminal cases belong to the category of "taʿzīr", where the goal of punishment is correction or rehabilitation of the culprit and its form is largely left to the judge's discretion. In practice, since early on in Islamic history, criminal cases were usually handled by ruler-administered courts or local police using procedures which were only loosely related to Sharia.

The two major genres of "furūʿ" literature are the "mukhtasar" (concise summary of law) and the "mabsut" (extensive commentary). "Mukhtasars" were short specialized treatises or general overviews that could be used in a classroom or consulted by judges. A "mabsut", which usually provided a commentary on a "mukhtasar" and could stretch to dozens of large volumes, recorded alternative rulings with their justifications, often accompanied by a proliferation of cases and conceptual distinctions. The terminology of juristic literature was conservative and tended to preserve notions which had lost their practical relevance. At the same time, the cycle of abridgement and commentary allowed jurists of each generation to articulate a modified body of law to meet changing social conditions. Other juristic genres include the "qawāʿid" (succinct formulas meant to aid the student remember general principles) and collections of fatwas by a particular scholar.

Classical jurisprudence has been described as "one of the major intellectual achievements of Islam" and its importance in Islam has been compared to that of theology in Christianity.

The main Sunni schools of law ("madhhabs") are the Hanafi, Maliki, Shafi'i and Hanbali madhhabs. They emerged in the ninth and tenth centuries and by the twelfth century almost all jurists aligned themselves with a particular madhhab. These four schools recognize each other's validity and they have interacted in legal debate over the centuries. Rulings of these schools are followed across the Muslim world without exclusive regional restrictions, but they each came to dominate in different parts of the world. For example, the Maliki school is predominant in North and West Africa; the Hanafi school in South and Central Asia; the Shafi'i school in Lower Egypt, East Africa, and Southeast Asia; and the Hanbali school in North and Central Arabia. The first centuries of Islam also witnessed a number of short-lived Sunni madhhabs. The Zahiri school, which is commonly identified as extinct, continues to exert influence over legal thought. The development of Shia legal schools occurred along the lines of theological differences and resulted in formation of the Twelver, Zaidi and Ismaili madhhabs, whose differences from Sunni legal schools are roughly of the same order as the differences among Sunni schools. The Ibadi legal school, distinct from Sunni and Shia madhhabs, is predominant in Oman.

The transformations of Islamic legal institutions in the modern era have had profound implications for the madhhab system. Legal practice in most of the Muslim world has come to be controlled by government policy and state law, so that the influence of the madhhabs beyond personal ritual practice depends on the status accorded to them within the national legal system. State law codification commonly utilized the methods of "takhayyur" (selection of rulings without restriction to a particular madhhab) and "talfiq" (combining parts of different rulings on the same question). Legal professionals trained in modern law schools have largely replaced traditional ulema as interpreters of the resulting laws. Global Islamic movements have at times drawn on different madhhabs and at other times placed greater focus on the scriptural sources rather than classical jurisprudence. The Hanbali school, with its particularly strict adherence to the Quran and hadith, has inspired conservative currents of direct scriptural interpretation by the Salafi and Wahhabi movements. Other currents, such as networks of Indonesian ulema and Islamic scholars residing in Muslim-minority countries, have advanced liberal interpretations of Islamic law without focusing on traditions of a particular madhhab.

Sharia was traditionally interpreted by muftis. During the first few centuries of Islam, muftis were private legal specialists who normally also held other jobs. They issued fatwas (legal opinions), generally free of charge, in response to questions from laypersons or requests for consultation coming from judges, which would be stated in general terms. Fatwas were regularly upheld in courts, and when they were not, it was usually because the fatwa was contradicted by a more authoritative legal opinion. The stature of jurists was determined by their scholarly reputation. The majority of classical legal works, written by author-jurists, were based in large part on fatwas of distinguished muftis. These fatwas functioned as a form of legal precedent, unlike court verdicts, which were valid only for the given case. Although independent muftis never disappeared, from the 12th century onward Muslim rulers began to appoint salaried muftis to answer questions from the public. Over the centuries, Sunni muftis were gradually incorporated into state bureaucracies, while Shia jurists in Iran progressively asserted an autonomous authority starting from the early modern era.
Islamic law was initially taught in study circles that gathered in mosques and private homes. The teacher, assisted by advanced students, provided commentary on concise treatises of law and examined the students' understanding of the text. This tradition continued to be practiced in "madrasas", which spread during the 10th and 11th centuries. Madrasas were institutions of higher learning devoted principally to study of law, but also offering other subjects such as theology, medicine, and mathematics. The madrasa complex usually consisted of a mosque, boarding house, and a library. It was maintained by a "waqf" (charitable endowment), which paid salaries of professors, stipends of students, and defrayed the costs of construction and maintenance. At the end of a course, the professor granted a license ("ijaza") certifying a student's competence in its subject matter. Students specializing in law would complete a curriculum consisting of preparatory studies, the doctrines of a particular madhhab, and training in legal disputation, and finally write a dissertation, which earned them a license to teach and issue fatwas.

A judge (qadi) was in charge of the qadi's court ("mahkama"), also called the Sharia court. Qadis were trained in Islamic law, though not necessarily to a level required to issue fatwas. Court personnel also included a number of assistants performing various roles. Judges were theoretically independent in their decisions, though they were appointed by the ruler and often experienced pressure from members of the ruling elite where their interests were at play. The role of qadis was to evaluate the evidence, establish the facts of the case, and issue a verdict based on the applicable rulings of Islamic jurisprudence. The qadi was supposed to solicit a fatwa from a mufti if it was unclear how the law should be applied to the case. Since Islamic legal theory does not recognize the distinction between private and public law, court procedures were identical for civil and criminal cases, and required a private plaintiff to produce evidence against the defendant. The main type of evidence was oral witness testimony. The standards of evidence for criminal cases were so strict that a conviction was often difficult to obtain even for apparently clear-cut cases. Most historians believe that because of these stringent procedural norms, qadi's courts at an early date lost their jurisdiction over criminal cases, which were instead handled in other types of courts.

If an accusation did not result in a verdict in a qadi's court, the plaintiff could often pursue it in another type of court called the "mazalim" court, administered by the ruler's council. The rationale for "mazalim" (lit. wrongs, grievances) courts was to address the wrongs that Sharia courts were unable to address, including complaints against government officials. Islamic jurists were commonly in attendance and a judge often presided over the court as a deputy of the ruler. "Mazalim" verdicts were supposed to conform to the spirit of Sharia, but they were not bound by the letter of the law or the procedural restrictions of qadi's courts.

The police ("shurta"), which took initiative in preventing and investigating crime, operated its own courts. Like the mazalim courts, police courts were not bound by the rules of Sharia and had the powers to inflict discretionary punishments. Another office for maintaining public order was the "muhtasib" (market inspector), who was charged with preventing fraud in economic transactions and infractions against public morality. The "muhtasib" took an active role in pursuing these types of offenses and meted out punishments based on local custom.

The social fabric of pre-modern Islamic societies was largely defined by close-knit communities organized around kinship groups and local neighborhoods. Conflicts between individuals had the potential to escalate into a conflict between their supporting groups and disrupt the life of the entire community. Court litigation was seen as a last resort for cases where informal mediation had failed. This attitude was reflected in the legal maxim "amicable settlement is the best verdict" ("al-sulh sayyid al-ahkam"). In court disputes, qadis were generally less concerned with legal theory than with achieving an outcome that enabled the disputants to resume their previous social relationships. This could be accomplished by avoiding a total loss for the losing side or simply giving them a chance to articulate their position in public and obtain a measure of psychological vindication. Islamic law required judges to be familiar with local customs, and they exercised a number of other public functions in the community, including mediation and arbitration, supervision of public works, auditing waqf finances, and looking after the interests of orphans.

Unlike pre-modern cultures where the ruling dynasty promulgated the law, Islamic law was formulated by religious scholars without involvement of the rulers. The law derived its authority not from political control, but rather from the collective doctrinal positions of the legal schools (madhhabs) in their capacity as interpreters of the scriptures. The ulema (religious scholars) were involved in management of communal affairs and acted as representatives of the Muslim population vis-à-vis the ruling dynasties, who before the modern era had limited capacity for direct governance. Military elites relied on the ulema for religious legitimation, with financial support for religious institutions being one of the principal means through which these elites established their legitimacy. In turn, the ulema depended on the support of the ruling elites for the continuing operation of religious institutions. Although the relationship between secular rulers and religious scholars underwent a number of shifts and transformations in different times and places, this mutual dependence characterized Islamic history until the start of the modern era. Additionally, since Sharia contained few provisions in several areas of public law, Muslim rulers were able to legislate various collections of economic, criminal and administrative laws outside the jurisdiction of Islamic jurists, the most famous of which is the "qanun" promulgated by Ottoman sultans beginning from the 15th century. The Mughal emperor Aurangzeb (r. 1658-1707) issued a hybrid body of law known as Fatawa-e-Alamgiri, based on Hanafi fatwas as well as decisions of Islamic courts, and made it applicable to all religious communities on the Indian subcontinent. This early attempt to turn Islamic law into semi-codified state legislation sparked rebellions against Mughal rule.

In both the rules of civil disputes and application of penal law, classical Sharia distinguishes between men and women, between Muslims and non-Muslims, and between free persons and slaves.
Traditional Islamic law assumes a patriarchal society with a man at the head of the household. Different legal schools formulated a variety of legal norms which could be manipulated to the advantage of men or women, but women were generally at a disadvantage with respect to the rules of inheritance, blood money ("diya"), and witness testimony, where a woman's value is effectively treated as half of that of a man. In economic terms women enjoyed greater advantages under Islamic law than under other Mediterranean and Middle Eastern legal systems, including the right to own personal property and dispose of it freely, which women in the West did not possess until "quite recently". Various financial obligations imposed on the husband acted as a deterrent against unilateral divorce and commonly gave the wife financial leverage in divorce proceedings. Women were active in Sharia courts as both plaintiffs and defendants in a wide variety of cases, though some opted to be represented by a male relative.

Sharia was intended to regulate affairs of the Muslim community. Non-Muslims residing under Islamic rule had the legal status of dhimmi, which entailed a number of protections, restrictions, freedoms and legal inequalities, including payment of the jizya tax. Dhimmi communities had legal autonomy to adjudicate their internal affairs. Cases involving litigants from two different religious groups fell under jurisdiction of Sharia courts, where (unlike in secular courts) testimony of non-Muslim witnesses against a Muslim was inadmissible in criminal cases or at all. This legal framework was implemented with varying degree of rigor. In some periods or towns, all inhabitants apparently used the same court without regard for their religious affiliation. The Mughal emperor Aurangzeb imposed Islamic law on all his subjects, including provisions traditionally applicable only to Muslims, while some of his predecessors and successors are said to have abolished jizya. According to Ottoman records, non-Muslim women took their cases to a Sharia court when they expected a more favorable outcome on marital, divorce and property questions than in Christian and Jewish courts.

Classical fiqh acknowledges and regulates slavery as a legitimate institution. It granted slaves certain rights and protections, improving their status relative to Greek and Roman law, and restricted the scenarios under which people could be enslaved. However, slaves could not inherit or enter into a contract, and were subject to their master's will in a number of ways. The labor and property of slaves were owned by the master, who was also entitled to sexual submission of his unmarried slaves.

Formal legal disabilities for some groups coexisted with a legal culture that viewed Sharia as a reflection of universal principles of justice, which involved protection of the weak against injustices committed by the strong. This conception was reinforced by the historical practice of Sharia courts, where peasants "almost always" won cases against oppressive landowners, and non-Muslims often prevailed in disputes against Muslims, including such powerful figures as the governor of their province. In family matters the Sharia court was seen as a place where the rights of women could be asserted against their husband's transgressions.

Starting from the 17th century, European powers began to extend political influence over lands ruled by Muslim dynasties, and by the end of the 19th century, much of the Muslim world came under colonial domination. The first areas of Islamic law to be impacted were usually commercial and criminal laws, which impeded colonial administration and were soon replaced by European regulations. Islamic commercial laws were also replaced by European (mostly French) laws in Muslim states which retained formal independence, because these states increasingly came to rely on Western capital and could not afford to lose the business of foreign merchants who refused to submit to Islamic regulations.
The first significant changes to the legal system of British India were initiated in the late 18th century by the governor of Bengal Warren Hastings. Hastings' plan of legal reform envisioned a multi-tiered court system for the Muslim population, with a middle tier of British judges advised by local Islamic jurists, and a lower tier of courts operated by qadis. Hastings also commissioned a translation of the classic manual of Hanafi fiqh, "Al-Hidayah", from Arabic into Persian and then English, later complemented by other texts. These translations enabled British judges to pass verdicts in the name of Islamic law based on a combination of Sharia rules and common law doctrines, and eliminated the need to rely on consultation by local ulema, whom they mistrusted. In the traditional Islamic context, a concise text like "Al-Hidayah" would be used as a basis for classroom commentary by a professor, and the doctrines thus learned would be mediated in court by judicial discretion, consideration of local customs and availability of different legal opinions that could fit the facts of the case. The British use of "Al-Hidayah", which amounted to an inadvertent codification of Sharia, and its interpretation by judges trained in Western legal traditions anticipated later legal reforms in the Muslim world.

British administrators felt that Sharia rules too often allowed criminals to escape punishment, as exemplified by Hastings' complaint that Islamic law was "founded on the most lenient principles and on an abhorrence of bloodshed". In the course of the 19th century, criminal laws and other aspects of the Islamic legal system in India were supplanted by British law, with the exception of Sharia rules retained in family laws and some property transactions. Among other changes, these reforms brought about abolition of slavery, prohibition of child marriage, and a much more frequent use of capital punishment. The resulting legal system, known as "Anglo-Muhammadan law", was treated by the British as a model for legal reforms in their other colonies. Like the British in India, colonial administrations typically sought to obtain precise and authoritative information about indigenous laws, which prompted them to prefer classical Islamic legal texts over local judicial practice. This, together with their conception of Islamic law as a collection of inflexible rules, led to an emphasis on traditionalist forms of Sharia that were not rigorously applied in the pre-colonial period and served as a formative influence on the modern identity politics of the Muslim world.

During the colonial era, Muslim rulers concluded that they could not resist European pressure unless they modernized their armies and built centrally administered states along the lines of Western models. In the Ottoman empire, the first such changes in the legal sphere involved placing the formerly independent waqfs under state control. This reform, passed in 1826, enriched the public treasury at the expense of the waqfs, thereby depleting the financial support for traditional Islamic legal education. Over the second half of the 19th century, a new hierarchical system of secular courts was established to supplement and eventually replace most religious courts. Students hoping to pursue legal careers in the new court system increasingly preferred attending secular schools over the traditional path of legal education with its dimming financial prospects. The Tanzimat reforms of the 19th century saw reorganization of both Islamic civil law and sultanic criminal law after the model of the Napoleonic Code. In the 1870s, a codification of civil law and procedure (excepting marriage and divorce), called the "Mecelle", was produced for use in both Sharia and secular courts. It adopted the Turkish language for the benefit of the new legal class who no longer possessed competence in the Arabic idiom of traditional jurisprudence. The code was based on Hanafi law, and its authors selected minority opinions over authoritative ones when they were felt to better "suit the present conditions". The Mecelle was promulgated as a "qanun" (sultanic code), which represented an unprecedented assertion of the state's authority over Islamic civil law, traditionally the preserve of the ulema. The 1917 Ottoman Law of Family Rights adopted an innovative approach of drawing rules from minority and majority opinions of all Sunni madhhabs with a modernizing intent. The Republic of Turkey, which emerged after the dissolution of the Ottoman Empire, abolished its Sharia courts and replaced Ottoman civil laws with the Swiss Civil Code, but Ottoman civil laws remained in force for several decades in Jordan, Lebanon, Palestine, Syria, and Iraq.

Westernization of legal institutions and expansion of state control in all areas of law, which began during the colonial era, continued in nation-states of the Muslim world. Sharia courts at first continued to exist alongside state courts as in earlier times, but the doctrine that sultanic courts should implement the ideals of Sharia was gradually replaced by legal norms imported from Europe. Court procedures were also brought in line with European practice. Though the Islamic terms "qadi" and "mahkama" (qadi's/Sharia court) were preserved, they generally came to mean judge and court in the Western sense. While in the traditional Sharia court all parties represented themselves, in modern courts they are represented by professional lawyers educated in Western-style law schools, and the verdicts are subject to review in an appeals court. In the 20th century, most countries abolished a parallel system of Sharia courts and brought all cases under a national civil court system.

In most Muslim-majority countries, traditional rules of classical fiqh have been largely preserved only in family law. In some countries religious minorities such as Christians or Shia Muslims have been subject to separate systems of family laws. Many Muslims today believe that contemporary Sharia-based laws are an authentic representation of the pre-modern legal tradition. In reality, they generally represent the result of extensive legal reforms made in the modern era. As traditional Islamic jurists lost their role as authoritative interpreters of the laws applied in courts, these laws were codified by legislators and administered by state systems which employed a number of devices to effect changes, including:

The most powerful influence on liberal reformist thought came from the work of the Egyptian Islamic scholar Muhammad ʿAbduh (1849–1905). Abduh viewed only Sharia rules pertaining to religious rituals as inflexible, and argued that the other Islamic laws should be adapted based on changing circumstances in consideration of social well-being. Following precedents of earlier Islamic thinkers, he advocated restoring Islam to its original purity by returning to the Quran and the sunna instead of following the medieval schools of jurisprudence. He championed a creative approach to ijtihad that involved direct interpretation of scriptures as well as the methods of "takhayyur" and "talfiq".

One of the most influential figures in modern legal reforms was the Egyptian legal scholar Abd El-Razzak El-Sanhuri (1895–1971), who possessed expertise in both Islamic and Western law. Sanhuri argued that reviving Islamic legal heritage in a way that served the needs of contemporary society required its analysis in light of the modern science of comparative law. He drafted the civil codes of Egypt (1949) and Iraq (1951) based on a variety of sources, including classical fiqh, European laws, existing Arab and Turkish codes, and the history of local court decisions. Sanhuri's Egyptian code incorporated few classical Sharia rules, but he drew on traditional jurisprudence more frequently for the Iraqi code. Sanhuri's codes were subsequently adopted in some form by most Arab countries.

Aside from the radical reforms of Islamic family law carried out in Tunisia (1956) and Iran (1967), governments often preferred to make changes that made a clear break from traditional Sharia rules by imposing administrative hurdles rather than changing the rules themselves, in order to minimize objections from religious conservatives. Various procedural changes have been made in a number of countries to restrict polygamy, give women greater rights in divorce, and eliminate child marriage. Inheritance has been the legal domain least susceptible to reform, as legislators have been generally reluctant to tamper with the highly technical system of Quranic shares. Some reforms have faced strong conservative opposition. For example, the 1979 reform of Egyptian family law, promulgated by Anwar Sadat through presidential decree, provoked an outcry and was annulled in 1985 by the supreme court on procedural grounds, to be later replaced by a compromise version. The 2003 reform of Moroccan family law, which sought to reconcile universal human rights norms and the country's Islamic heritage, was drafted by a commission that included parliamentarians, religious scholars and feminist activists, and the result has been praised by international rights groups as an example of progressive legislation achieved within an Islamic framework.

The Islamic revival of the late 20th century brought the topic of Sharia to international attention in the form of numerous political campaigns in the Muslim world calling for full implementation of Sharia. A number of factors have contributed to the rise of these movements, classified under the rubric of Islamism or political Islam, including the failure of authoritarian secular regimes to meet the expectations of their citizens, and a desire of Muslim populations to return to more culturally authentic forms of socio-political organization in the face of a perceived cultural invasion from the West. Islamist leaders such as Ayatollah Khomeini drew on leftist anticolonialist rhetoric by framing their call for Sharia as a resistance struggle. They accused secular leaders of corruption and predatory behavior, and claimed that a return to Sharia would replace despotic rulers with pious leaders striving for social and economic justice. In the Arab world these positions are often encapsulated in the slogan "Islam is the solution" ("al-Islam huwa al-hall").

Full implementation of Sharia theoretically refers to expanding its scope to all fields of law and all areas of public life. In practice, Islamization campaigns have focused on a few highly visible issues associated with the conservative Muslim identity, particularly women's hijab and the "hudud" criminal punishments (whipping, stoning and amputation) prescribed for certain crimes. For many Islamists, "hudud" punishments are at the core of the divine Sharia because they are specified by the letter of scripture rather than by human interpreters. Modern Islamists have often rejected, at least in theory, the stringent procedural constraints developed by classical jurists to restrict their application. To the broader Muslim public, the calls for Sharia often represent, even more than any specific demands, a vague vision of their current economic and political situation being replaced by a "just utopia".

A number of legal reforms have been made under the influence of these movements, starting from the 1970s when Egypt and Syria amended their constitutions to specify Sharia as the basis of legislation. The Iranian Revolution of 1979 represented a watershed for Islamization advocates, demonstrating that it was possible to replace a secular regime with a theocracy. Several countries, including Iran, Pakistan, Sudan, and some Nigerian states have incorporated hudud rules into their criminal justice systems, which, however, retained fundamental influences of earlier Westernizing reforms. In practice, these changes were largely symbolic, and aside from some cases brought to trial to demonstrate that the new rules were being enforced, hudud punishments tended to fall into disuse, sometimes to be revived depending on the local political climate. The supreme courts of Sudan and Iran have rarely approved verdicts of stoning or amputation, and the supreme courts of Pakistan and Nigeria have never done so. Nonetheless, Islamization campaigns have also had repercussions in several other areas of law, leading to curtailment of rights of women and religious minorities, and in the case of Sudan contributing to the breakout of a civil war.

Advocates of Islamization have often been more concerned with ideology than traditional jurisprudence and there is no agreement among them as to what form a modern Sharia-based "Islamic state" should take. This is particularly the case for the theorists of Islamic economics and Islamic finance, who have advocated both free-market and socialist economic models. The notion of "Sharia-compliant" finance has become an active area of doctrinal innovation and its development has had a major impact on business operations around the world.

The legal systems of most Muslim-majority countries can be classified as either secular or mixed. Sharia plays no role in secular legal systems. In mixed legal systems, Sharia rules are allowed to influence some national laws, which are codified and may be based on European or Indian models, and the central legislative role is played by politicians and modern jurists rather than the ulema (traditional Islamic scholars). Saudi Arabia and some other Gulf states possess what may be called classical Sharia systems, where national law is largely uncodified and formally equated with Sharia, with ulema playing a decisive role in its interpretation. Iran has adopted some features of classical Sharia systems, while also maintaining characteristics of mixed systems, like codified laws and a parliament.

Constitutions of many Muslim-majority countries refer to Sharia as a source or the main source of law, though these references are not in themselves indicative of how much the legal system is influenced by Sharia, and whether the influence has a traditionalist or modernist character. The same constitutions usually also refer to universal principles such as democracy and human rights, leaving it up to legislators and the judiciary to work out how these norms are to be reconciled in practice. Conversely, some countries (e.g., Algeria), whose constitution does not mention Sharia, possess Sharia-based family laws. Nisrine Abiad identifies Bahrain, Iran, Pakistan, and Saudi Arabia as states with "strong constitutional consequences" of Sharia "on the organization and functioning of power".

Except for secular systems, Muslim-majority countries possess Sharia-based laws dealing with family matters (marriage, inheritance, etc.). These laws generally reflect influence of various modern-era reforms and tend to be characterized by ambiguity, with traditional and modernist interpretations often manifesting themselves in the same country, both in legislation and court decisions. In some countries (e.g., parts of Nigeria and Greece), people can choose whether to pursue a case in a Sharia or secular court.

Countries in the Muslim world generally have criminal codes influenced by French law or common law, and in some cases a combination of Western legal traditions. Saudi Arabia has never adopted a criminal code and Saudi judges still follow traditional Hanbali jurisprudence. In the course of Islamization campaigns, several countries (Libya, Pakistan, Iran, Sudan, Mauritania, and Yemen) inserted Islamic criminal laws into their penal codes, which were otherwise based on Western models. In some countries only "hudud" penalties were added, while others also enacted provisions for "qisas" (law of retaliation) and "diya" (monetary compensation). Iran subsequently issued a new "Islamic Penal Code". The criminal codes of Afghanistan and United Arab Emirates contain a general provision that certain crimes are to be punished according to Islamic law, without specifying the penalties. Some Nigerian states have also enacted Islamic criminal laws. Laws in the Indonesian province of Aceh provide for application of discretionary ("ta'zir") punishments for violation of Islamic norms, but explicitly exclude "hudud" and "qisas". Brunei has been implementing a "Sharia Penal Code", which includes provisions for stoning and amputation, in stages since 2014. The countries where "hudud" penalties are legal do not use stoning and amputation routinely, and generally apply other punishments instead.

Sharia also plays a role beyond religious rituals and personal ethics in some countries with Muslim minorities. For example, in Israel Sharia-based family laws are administered for the Muslim population by the Ministry of Justice through the Sharia Courts. In India, the Muslim Personal Law (Shariat) Application Act provides for the use of Islamic law for Muslims in several areas, mainly related to family law. In England, the Muslim Arbitration Tribunal makes use of Sharia family law to settle disputes, though this limited adoption of Sharia is controversial.

Sharia courts traditionally do not rely on lawyers; plaintiffs and defendants represent themselves. In Saudi Arabia and Qatar, which have preserved traditional procedure in Sharia courts, trials are conducted solely by the judge, and there is no jury system. There is no pre-trial discovery process, and no cross-examination of witnesses. Unlike common law, judges' verdicts do not set binding precedents under the principle of "stare decisis", and unlike civil law, Sharia is left to the interpretation in each case and has no formally codified universal statutes.

The rules of evidence in Sharia courts traditionally prioritize oral testimony, and witnesses must be Muslim. Male Muslim witnesses are deemed more reliable than female Muslim witnesses, and non-Muslim witnesses considered unreliable and receive no priority in a Sharia court. In civil cases in some countries, a Muslim woman witness is considered half the worth and reliability than a Muslim man witness. In criminal cases, women witnesses are unacceptable in stricter, traditional interpretations of Sharia, such as those found in Hanbali jurisprudence, which forms the basis of law in Saudi Arabia.

A confession, an oath, or the oral testimony of Muslim witnesses are the main evidence admissible in traditional sharia courts for hudud crimes, i.e., the religious crimes of adultery, fornication, rape, accusing someone of illicit sex but failing to prove it, apostasy, drinking intoxicants and theft. According to classical jurisprudence, testimony must be from at least two free Muslim male witnesses, or one Muslim male and two Muslim females, who are not related parties and who are of sound mind and reliable character. Testimony to establish the crime of adultery, fornication or rape must be from four Muslim male witnesses, with some fiqhs allowing substitution of up to three male with six female witnesses; however, at least one must be a Muslim male. Forensic evidence ("i.e.", fingerprints, ballistics, blood samples, DNA etc.) and other circumstantial evidence may likewise rejected in hudud cases in favor of eyewitnesses in some modern interpretations. In the case of regulations that were part of local Malaysian legislation that did not go into effect, this could cause severe difficulties for women plaintiffs in rape cases. In Pakistan, DNA evidence is rejected in paternity cases on the basis of legislation that favors the presumption of children's legitimacy, while in sexual assault cases DNA evidence is regarded as equivalent to expert opinion and evaluated on a case-by-case basis.

 recommends written financial contracts with reliable witnesses, although there is dispute about equality of female testimony.

Marriage is solemnized as a written financial contract, in the presence of two Muslim male witnesses, and it includes a brideprice (Mahr) payable from a Muslim man to a Muslim woman. The brideprice is considered by a Sharia court as a form of debt. Written contracts were traditionally considered paramount in Sharia courts in the matters of dispute that are debt-related, which includes marriage contracts. Written contracts in debt-related cases, when notarized by a judge, is deemed more reliable.

In commercial and civil contracts, such as those relating to exchange of merchandise, agreement to supply or purchase goods or property, and others, oral contracts and the testimony of Muslim witnesses historically triumphed over written contracts. Islamic jurists traditionally held that written commercial contracts may be forged. Timur Kuran states that the treatment of written evidence in religious courts in Islamic regions created an incentive for opaque transactions, and the avoidance of written contracts in economic relations. This led to a continuation of a "largely oral contracting culture" in Muslim-majority nations and communities.

In lieu of written evidence, oaths are traditionally accorded much greater weight; rather than being used simply to guarantee the truth of ensuing testimony, they are themselves used as evidence. Plaintiffs lacking other evidence to support their claims may demand that defendants take an oath swearing their innocence, refusal thereof can result in a verdict for the plaintiff. Taking an oath for Muslims can be a grave act; one study of courts in Morocco found that lying litigants would often "maintain their testimony right up to the moment of oath-taking and then to stop, refuse the oath, and surrender the case." Accordingly, defendants are not routinely required to swear before testifying, which would risk casually profaning the Quran should the defendant commit perjury; instead oaths are a solemn procedure performed as a final part of the evidence process.

In classical jurisprudence monetary compensation for bodily harm ("diya" or blood money) is assessed differently for different classes of victims. For example, for Muslim women the amount was half that assessed for a Muslim man. "Diya" for the death of a free Muslim man is twice as high as for Jewish and Christian victims according to the Maliki and Hanbali madhhabs and three times as high according to Shafi'i rules. Several legal schools assessed "diya" for Magians ("majus") at one-fifteenth the value of a free Muslim male.

Modern countries which incorporate classical "diya" rules into their legal system treat them in different ways. The Pakistan Penal Code modernized the Hanafi doctrine by eliminating distinctions between Muslims and non-Muslims. In Iran, "diya" for non-Muslim victims professing one of the faiths protected under the constitution (Jews, Christians, and Zoroastrians) was made equal to "diya" for Muslims in 2004, though according to a 2006 US State Department report, the penal code still discriminates against other religious minorities and women. According to Human Rights Watch and the US State Department, in Saudi Arabia Jewish or Christian male plaintiffs are entitled to half the amount a Muslim male would receive, while for all other non-Muslim males the proportion is one-sixteenth.

The spread of codified state laws and Western-style legal education in the modern Muslim world has displaced traditional muftis from their historical role of clarifying and elaborating the laws applied in courts. Instead, fatwas have increasingly served to advise the general public on other aspects of Sharia, particularly questions regarding religious rituals and everyday life. Modern fatwas deal with topics as diverse as insurance, sex-change operations, moon exploration and beer drinking. Most Muslim-majority states have established national organizations devoted to issuing fatwas, and these organizations to a considerable extent replaced independent muftis as religious guides for the general population. State-employed muftis generally promote a vision of Islam that is compatible with state law of their country.

Modern public and political fatwas have addressed and sometimes sparked controversies in the Muslim world and beyond. Ayatollah Khomeini's proclamation condemning Salman Rushdie to death for his novel "The Satanic Verses" is credited with bringing the notion of fatwa to world's attention, although some scholars have argued that it did not qualify as one. Together with later militant fatwas, it has contributed to the popular misconception of the fatwa as a religious death warrant.

Modern fatwas have been marked by an increased reliance on the process of "ijtihad", i.e. deriving legal rulings based on an independent analysis rather than conformity with the opinions of earlier legal authorities ("taqlid"), and some of them are issued by individuals who do not possess the qualifications traditionally required of a mufti. The most notorious examples are the fatwas of militant extremists. When Osama Bin Laden and his associates issued a fatwa in 1998 proclaiming "jihad against Jews and Crusaders", many Islamic jurists, in addition to denouncing its content, stressed that bin Laden was not qualified to either issue a fatwa or proclaim a jihad. New forms of ijtihad have also given rise to fatwas that support such notions as gender equality and banking interest, which are at variance with classical jurisprudence.

In the internet age, a large number of websites provide fatwas in response to queries from around the world, in addition to radio shows and satellite television programs offering call-in fatwas. Erroneous and sometimes bizarre fatwas issued by unqualified or eccentric individuals in recent times have sometimes given rise to complaints about a "chaos" in the modern practice of issuing fatwas. There exists no international Islamic authority to settle differences in interpretation of Islamic law. An International Islamic Fiqh Academy was created by the Organisation of Islamic Cooperation, but its legal opinions are not binding. The vast amount of fatwas produced in the modern world attests to the importance of Islamic authenticity to many Muslims. However, there is little research available to indicate to what extent Muslims acknowledge the authority of different muftis or heed their rulings in real life.

The classical doctrine of "hisba", associated with the Quranic injunction of "enjoining good and forbidding wrong", refers to the duty of Muslims to promote moral rectitude and intervene when another Muslim is acting wrongly. Historically, its legal implementation was entrusted to a public official called "muhtasib" (market inspector), who was charged with preventing fraud, disturbance of public order and infractions against public morality. This office disappeared in the modern era everywhere in the Muslim world, but it was revived in Arabia by the first Saudi state, and later instituted as a government committee responsible for supervising markets and public order. It has been aided by volunteers enforcing attendance of daily prayers, gender segregation in public places, and a conservative notion of hijab. Committee officers were authorized to detain violators before a 2016 reform. With the rising international influence of Wahhabism, the conception of "hisba" as an individual obligation to police religious observance has become more widespread, which led to the appearance of activists around the world who urge fellow Muslims to observe Islamic rituals, dress code, and other aspects of Sharia.

In Iran, "hisba" was enshrined in the constitution after the 1979 Revolution as a "universal and reciprocal duty", incumbent upon both the government and the people. Its implementation has been carried out by official committees as well as volunteer forces ("basij"). Elsewhere, policing of various interpretations of Sharia-based public morality has been carried out by the Kano State Hisbah Corps in the Nigerian state of Kano, by "Polisi Perda Syariah Islam" in the Aceh province of Indonesia, by the Committee for the Propagation of Virtue and the Prevention of Vice in the Gaza Strip, and by the Taleban during their 1996-2001 rule of Afghanistan. Religious police organizations tend to have support from conservative currents of public opinion, but their activities are often disliked by other segments of the population, especially liberals, urban women, and younger people.

In Egypt, a law based on the doctrine of hisba had for a time allowed a Muslim to sue another Muslim over beliefs that may harm society, though because of abuses it has been amended so that only the state prosecutor may bring suit based on private requests. Before the amendment was passed, a hisba suit brought by a group of Islamists against the liberal theologian Nasr Abu Zayd on charges of apostasy led to annulment of his marriage. The law was also invoked in an unsuccessful blasphemy suit against the feminist author Nawal El Saadawi. Hisba has also been invoked in several Muslim-majority countries as rationale for blocking pornographic content on the internet and for other forms of faith-based censorship.

A 2013 survey based on interviews of 38,000 Muslims, randomly selected from urban and rural parts in 39 countries using area probability designs, by the Pew Forum on Religion and Public Life found that a majority—in some cases "overwhelming" majority—of Muslims in a number of countries support making "Sharia" or "Islamic law" the law of the land, including Afghanistan (99%), Iraq (91%), Niger (86%), Malaysia (86%), Pakistan (84%), Morocco (83%), Bangladesh (82%), Egypt (74%), Indonesia (72%), Jordan (71%), Uganda (66%), Ethiopia (65%), Mali (63%), Ghana (58%), and Tunisia (56%). In Muslim regions of Southern-Eastern Europe and Central Asia, the support is less than 50%: Russia (42%), Kyrgyzstan (35%), Tajikistan (27%), Kosovo (20%), Albania (12%), Turkey (12%), Kazakhstan (10%), Azerbaijan (8%). Regional averages of support were 84% in South Asia, 77% in Southeast Asia, 74% in the Middle-East/North Africa, 64%, in Sub-Saharan Africa, 18% in Southern-Eastern Europe, and 12% in Central Asia .

However, while most of those who support implementation of Sharia favor using it in family and property disputes, fewer supported application of severe punishments such as whippings and cutting off hands, and interpretations of some aspects differed widely. According to the Pew poll, among Muslims who support making Sharia the law of the land, most do not believe that it should be applied to non-Muslims. In the Muslim-majority countries surveyed this proportion varied between 74% (of 74% in Egypt) and 19% (of 10% in Kazakhstan), as percentage of those who favored making Sharia the law of the land.

In all of the countries surveyed, respondents were more likely to define Sharia as "the revealed word of God" rather than as "a body of law developed by men based on the word of God". In analyzing the poll, Amaney Jamal has argued that there is no single, shared understanding of the notions "Sharia" and "Islamic law" among the respondents. In particular, in countries where Muslim citizens have little experience with rigid application of Sharia-based state laws, these notions tend to be more associated with Islamic ideals like equality and social justice than with prohibitions. Other polls have indicated that for Egyptians, the word "Sharia" is associated with notions of political, social and gender justice.

In 2008, Rowan Williams, the Archbishop of Canterbury, has suggested that Islamic and Orthodox Jewish courts should be integrated into the British legal system alongside ecclesiastical courts to handle marriage and divorce, subject to agreement of all parties and strict requirements for protection of equal rights for women. His reference to the sharia sparked a controversy. Later that year, Nicholas Phillips, then Lord Chief Justice of England and Wales, stated that there was "no reason why sharia principles [...] should not be the basis for mediation or other forms of alternative dispute resolution." A 2008 YouGov poll in the United Kingdom found 40% of Muslim students interviewed supported the introduction of sharia into British law for Muslims. Michael Broyde, professor of law at Emory University specializing in alternative dispute resolution and Jewish law, has argued that sharia courts can be integrated into the American religious arbitration system, provided that they adopt appropriate institutional requirements as American rabbinical courts have done.

In the Western world, Sharia has been called a source of "hysteria", "more controversial than ever", the one aspect of Islam that inspires "particular dread". On the Internet, "dozens of self-styled counter-jihadis" emerged to campaign against Sharia law, describing it in strict interpretations resembling those of Salafi Muslims. Also, fear of Sharia law and of the ideology of extremism among Muslims as well as certain congregations donating money to terrorist organizations within the Muslim community reportedly spread to mainstream conservative Republicans in the United States. Former House Speaker Newt Gingrich won ovations calling for a federal ban on Sharia law.
The issue of "liberty versus Sharia" was called a "momentous civilisational debate" by right-wing pundit Diana West.
In 2008 in Britain, the future Prime Minister (David Cameron) declared his opposition to "any expansion of Sharia law in the UK." In Germany, in 2014, the Interior Minister (Thomas de Maizière) told a newspaper ("Bild"), "Sharia law is not tolerated on German soil."

Some countries and jurisdictions have explicit bans on sharia law. In Canada, for example, sharia law has been explicitly banned in Quebec by a 2005 unanimous vote of the National Assembly, while the province of Ontario allows family law disputes to be arbitrated only under Ontario law. In the U.S., opponents of Sharia have sought to ban it from being considered in courts, where it has been routinely used alongside traditional Jewish and Catholic laws to decide legal, business, and family disputes subject to contracts drafted with reference to such laws, as long as they do not violate secular law or the U.S. constitution. After failing to gather support for a federal law making observing Sharia a felony punishable by up to 20 years in prison, anti-Sharia activists have focused on state legislatures. By 2014, bills aimed against use of Sharia have been introduced in 34 states and passed in 11. These bills have generally referred to banning foreign or religious law in order to thwart legal challenges.

According to Jan Michiel Otto, Professor of Law and Governance in Developing Countries at Leiden University, "[a]nthropological research shows that people in local communities often do not distinguish clearly whether and to what extent their norms and practices are based on local tradition, tribal custom, or religion. Those who adhere to a confrontational view of Sharia tend to ascribe many undesirable practices to Sharia and religion overlooking custom and culture, even if high-ranking religious authorities have stated the opposite."

It has been argued that the extent to which Sharia is compatible with democracy depends on how it is culturally interpreted, with a cultural position that Sharia represents the human attempt to interpret God’s message associated with a greater preference for democracy than an islamist interpretation that Sharia law is the literal word of God .

Esposito and DeLong-Bas distinguish four attitudes toward Sharia and democracy prominent among Muslims today:


Polls conducted by Gallup and PEW in Muslim-majority countries indicate that most Muslims see no contradiction between democratic values and religious principles, desiring neither a theocracy, nor a secular democracy, but rather a political model where democratic institutions and values can coexist with the values and principles of Sharia.

Muslih and Browers identify three major perspectives on democracy among prominent Muslims thinkers who have sought to develop modern, distinctly Islamic theories of socio-political organization conforming to Islamic values and law:


In 1998 the Constitutional Court of Turkey banned and dissolved Turkey's Refah Party over its announced intention to introduce Sharia-based laws, ruling that it would change Turkey's secular order and undermine democracy. On appeal by Refah the European Court of Human Rights determined that "sharia is incompatible with the fundamental principles of democracy". Refah's Sharia-based notion of a "plurality of legal systems, grounded on religion" was ruled to contravene the European Convention for the Protection of Human Rights and Fundamental Freedoms. It was determined that it would "do away with the State's role as the guarantor of individual rights and freedoms" and "infringe the principle of non-discrimination between individuals as regards their enjoyment of public freedoms, which is one of the fundamental principles of democracy". In an analysis, Maurits S. Berger found the ruling to be "nebulous" and surprising from a legal point of view, since the Court neglected to define what it meant by "Sharia" and would not, for example, be expected to regard Sharia rules for Islamic rituals as contravening European human rights values. Kevin Boyle also criticized the decision for not distinguishing between extremist and mainstream interpretations of Islam and implying that peaceful advocacy of Islamic doctrines ("an attitude which fails to respect [the principle of secularism]") is not protected by the European Convention provisions for freedom of religion.
Governments of several predominantly Muslim countries have criticized the Universal Declaration of Human Rights (UDHR) for its perceived failure to take into account the cultural and religious context of non-Western countries. Iran declared in the UN assembly that UDHR was "a secular understanding of the Judeo-Christian tradition", which could not be implemented by Muslims without trespassing the Islamic law. Islamic scholars and Islamist political parties consider 'universal human rights' arguments as imposition of a non-Muslim culture on Muslim people, a disrespect of customary cultural practices and of Islam. In 1990, the Organisation of Islamic Cooperation, a group representing all Muslim-majority nations, met in Cairo to respond to the UDHR, then adopted the Cairo Declaration on Human Rights in Islam.

Ann Elizabeth Mayer points to notable absences from the Cairo Declaration: provisions for democratic principles, protection for religious freedom, freedom of association and freedom of the press, as well as equality in rights and equal protection under the law. Article 24 of the Cairo declaration states that "all the rights and freedoms stipulated in this Declaration are subject to the Islamic "shari'a"".

In 2009, the journal "Free Inquiry" summarized the criticism of the Cairo Declaration in an editorial: "We are deeply concerned with the changes to the Universal Declaration of Human Rights by a coalition of Islamic states within the United Nations that wishes to prohibit any criticism of religion and would thus protect Islam's limited view of human rights. In view of the conditions inside the Islamic Republic of Iran, Egypt, Pakistan, Saudi Arabia, the Sudan, Syria, Bangladesh, Iraq, and Afghanistan, we should expect that at the top of their human rights agenda would be to rectify the legal inequality of women, the suppression of political dissent, the curtailment of free expression, the persecution of ethnic minorities and religious dissenters – in short, protecting their citizens from egregious human rights violations. Instead, they are worrying about protecting Islam."

H. Patrick Glenn states that Sharia is structured around the concept of mutual obligations of a collective, and it considers individual human rights as potentially disruptive and unnecessary to its revealed code of mutual obligations. In giving priority to this religious collective rather than individual liberty, the Islamic law justifies the formal inequality of individuals (women, non-Islamic people). Bassam Tibi states that Sharia framework and human rights are incompatible. Abdel al-Hakeem Carney, in contrast, states that Sharia is misunderstood from a failure to distinguish "Sharia" from "siyasah" (politics).

In classical fiqh, blasphemy refers to any form of cursing, questioning or annoying God, Muhammad or anything considered sacred in Islam, including denying one of the Islamic prophets or scriptures, insulting an angel or refusing to accept a religious commandment. Jurists of different schools prescribed different punishment for blasphemy against Islam, by Muslims and non-Muslims, ranging from imprisonment or fines to the death penalty. In some cases, sharia allows non-Muslims to escape death by converting and becoming a devout follower of Islam. In the modern Muslim world, the laws pertaining to blasphemy vary by country, and some countries prescribe punishments consisting of fines, imprisonment, flogging, hanging, or beheading.

Blasphemy laws were rarely enforced in pre-modern Islamic societies, but in the modern era some states and radical groups have used charges of blasphemy in an effort to burnish their religious credentials and gain popular support at the expense of liberal Muslim intellectuals and religious minorities.

Blasphemy, as interpreted under Sharia, is controversial. Representatives of the Organisation of Islamic Cooperation have petitioned the United Nations to condemn "defamation of religions" because "Unrestricted and disrespectful freedom of opinion creates hatred and is contrary to the spirit of peaceful dialogue". The Cairo Declaration on Human Rights in Islam subjects free speech to unspecified Sharia restrictions: Article 22(a) of the Declaration states that "Everyone shall have the right to express his opinion freely in such manner as would not be contrary to the principles of the Shariah." Others, in contrast, consider blasphemy laws to violate freedom of speech, stating that freedom of expression is essential to empowering both Muslims and non-Muslims, and point to the abuse of blasphemy laws in prosecuting members of religious minorities, political opponents, and settling personal scores. In Pakistan, blasphemy laws have been used to convict more than a thousand people, about half of them Ahmadis and Christians. While none have been legally executed, two Pakistani politicians, Shahbaz Bhatti and Salmaan Taseer, have been assassinated over their criticism of the blasphemy laws. Although the laws were inherited from British colonial legislation and then expanded and "Islamized" in the 1980s, many Pakistanis believe that they are taken directly from the Quran.

According to the classical doctrine, apostasy from Islam is a crime as well as a sin, punishable with the death penalty, typically after a waiting period to allow the apostate time to repent and to return to Islam. Wael Hallaq writes that "[in] a culture whose lynchpin is religion, religious principles and religious morality, apostasy is in some way equivalent to high treason in the modern nation-state". Early Islamic jurists set the standard for apostasy from Islam so high that practically no apostasy verdict could be passed before the 11th century, but later jurists lowered the bar for applying the death penalty, allowing judges to interpret the apostasy law in different ways, which they did sometimes leniently and sometimes strictly. In the late 19th century, the use of criminal penalties for apostasy fell into disuse, although civil penalties were still applied.

According to Abdul Rashied Omar, the majority of modern Islamic jurists continue to regard apostasy as a crime deserving the death penalty. This view is dominant in conservative societies like Saudi Arabia and Pakistan. A number of liberal and progressive Islamic scholars have argued that apostasy should not be viewed as a crime.

Twenty-three Muslim-majority countries, , penalized apostasy from Islam through their criminal laws. 
, apostasy from Islam was a capital offense in Afghanistan, Brunei, Mauritania, Qatar, Saudi Arabia, Sudan, the United Arab Emirates, and Yemen. In other countries, Sharia courts could use family laws to void the Muslim apostate's marriage and to deny child-custody rights as well as inheritance rights. In the years 1985–2006, four individuals were legally executed for apostasy from Islam: "one in Sudan in 1985; two in Iran, in 1989 and 1998; and one in Saudi Arabia in 1992." While modern states have rarely prosecuted apostasy, the issue has a "deep cultural resonance" in some Muslim societies and Islamists have tended to exploit it for political gain. In a 2008-2012 Pew Research Center poll, public support for capital punishment for apostasy among Muslims ranged from 78% in Afghanistan to less than 1% in Kazakhstan, reaching over 50% in 6 of the 20 countries surveyed.

Homosexual intercourse is illegal in classical Sharia, with different penalties, including capital punishment, stipulated depending of the situation and legal school. In pre-modern Islam, the penalties prescribed for homosexual acts were "to a large extent theoretical", owing in part to stringent procedural requirements for their harsher ("hudud") forms and in part to prevailing social tolerance toward same-sex relationships. Historical instances of prosecution for homosexual acts are rare, and those which followed Sharia rules are even rarer. Public attitudes toward homosexuality in the Muslim world turned more negative starting from the 19th century under the influence of sexual notions prevalent in Europe at that time. A number of Muslim-majority countries have retained criminal penalties for homosexual acts enacted under colonial rule. In recent decades, prejudice against LGBT individuals in the Muslim world has been exacerbated by increasingly conservative attitudes and the rise of Islamist movements, resulting in Sharia-based penalties enacted in several countries. The death penalty for homosexual acts is currently a legal punishment in Brunei, Iran, Mauritania, some northern states in Nigeria, Pakistan, Qatar, Saudi Arabia, parts of Somalia, Sudan, and Yemen, all of which have Sharia-based criminal laws. It is unclear whether the laws of Afghanistan and United Arab Emirates provide for the death penalty for gay sex, as they have never been carried out. Criminalization of consensual homosexual acts and especially making them liable to capital punishment has been condemned by international rights groups. According to polls, the level of social acceptance for homosexuality ranges from 52% among Muslims in the U.S. to less than 10% in a number of Muslim-majority nations.

Some extremists have used their interpretation of Islamic scriptures and Sharia, in particular the doctrine of jihad, to justify acts of war and terror against Muslim as well as non-Muslim individuals and governments. The expert on terrorism Rachel Ehrenfeld wrote that the "Sharia's finance (Islamic banking) is a new weapon in the arsenal of what might be termed fifth-generation warfare (5GW)". 

In classical fiqh, the term "jihad" refers to armed struggle against unbelievers. Classical jurists developed an elaborate set of rules pertaining to jihad, including prohibitions on harming those who are not engaged in combat. According to Bernard Lewis, "[a]t no time did the classical jurists offer any approval or legitimacy to what we nowadays call terrorism" and the terrorist practice of suicide bombing "has no justification in terms of Islamic theology, law or tradition". In the modern era the notion of jihad has lost its jurisprudential relevance and instead gave rise to an ideological and political discourse. While modernist Islamic scholars have emphasized defensive and non-military aspects of jihad, some radical Islamists have advanced aggressive interpretations that go beyond the classical theory. For al-Qaeda ideologues, in jihad all means are legitimate, including targeting Muslim non-combatants and the mass killing of non-Muslim civilians. According to these interpretations, Islam does not discriminate between military and civilian targets, but rather between Muslims and nonbelievers, whose blood can be legitimately spilled.

Some modern ulema, such as Yusuf al-Qaradawi and Sulaiman Al-Alwan, have supported suicide attacks against Israeli civilians, arguing that they are army reservists and hence should be considered as soldiers, while Hamid bin Abdallah al-Ali declared that suicide attacks in Chechnya were justified as a "sacrifice". Many prominent Islamic scholars, including al-Qaradawi himself, have issued condemnations of terrorism in general terms. For example, Abdul-Aziz ibn Abdullah Al ash-Sheikh, the Grand Mufti of Saudi Arabia has stated that "terrorizing innocent people [...] constitute[s] a form of injustice that cannot be tolerated by Islam", while Muhammad Sayyid Tantawy, Grand Imam of al-Azhar and former Grand Mufti of Egypt has stated that "attacking innocent people is not courageous; it is stupid and will be punished on the Day of Judgment".

According to some interpretations, Sharia condones certain forms of domestic violence against women, when a husband suspects "nushuz" (disobedience, disloyalty, rebellion, ill conduct) in his wife. Others believe that wife beating is not consistent with modern perspectives of the Quran.

One of the verses of the Quran relating to permissibility of domestic violence is Surah 4:34, which has been subject to varied interpretations. Traditional interpretations of Sharia have been criticized as inconsistent with women's rights in domestic abuse cases. Musawah, CEDAW, KAFA and other organizations have proposed ways to modify Sharia-inspired laws to improve women's rights in Muslim-majority nations, including women's rights in domestic abuse cases.

Shari'a is the basis for personal status laws in most Islamic-majority nations. These personal status laws determine rights of women in matters of marriage, divorce and child custody. A 2011 UNICEF report concludes that Sharia law provisions are discriminatory against women from a human rights perspective. In many countries, in legal proceedings relating to Sharia-based personal status law, a woman's testimony is worth half of a man's before a court.

The 1917 codification of Islamic family law in the Ottoman empire distinguished between the age of competence for marriage, which was set at 18 for boys and 17 for girls, and the minimum age for marriage, which followed the traditional Hanafi limits of 12 for boys and 9 for girls. Marriage below the age of competence was permissible only if proof of sexual maturity was accepted in court, while marriage under the minimum age was forbidden. During the 20th century, most countries in the Middle East followed the Ottoman precedent in defining the age of competence, while raising the minimum age to 15 or 16 for boys and 13-16 for girls. Marriage below the age of competence is subject to approval by a judge and the legal guardian of the adolescent. Egypt diverged from this pattern by setting the age limits of 18 for boys and 16 for girls, without a distinction between competence for marriage and minimum age. Many senior clerics in Saudi Arabia have opposed setting a minimum age for marriage, arguing that a woman reaches adulthood at puberty.

Rape is considered a crime in all countries of the North Africa and Middle East region, but as of 2011, Sharia-based or secular laws in some countries, including Bahrain, Iraq, Jordan, Libya, Morocco, Syria and Tunisia, allowed a rapist to escape punishment by marrying his victim, while in other countries, including Libya, Oman, Saudi Arabia and United Arab Emirates, rape victims who press charges risk being prosecuted for extramarital sex ("zina").

Islamic law granted Muslim women certain legal rights, such as property rights which women in the West did not possess until "comparatively recent times". Starting with the 20th century, Western legal systems evolved to expand women's rights, but women's rights in the Muslim world have to varying degree remained tied to the Quran, hadiths and their traditional interpretations by Islamic jurists. Sharia grants women the right to inherit property from other family members, and these rights are detailed in the Quran. A woman's inheritance is unequal and less than a man's, and dependent on many factors. For instance, a daughter's inheritance is usually half that of her brother's.

Sharia recognizes the basic inequality between master and women slave, between free women and slave women, between Believers and non-Believers, as well as their unequal rights. Sharia authorized the institution of slavery, using the words "abd" (slave) and the phrase "ma malakat aymanukum" ("that which your right hand owns") to refer to women slaves, seized as captives of war. Under Islamic law, Muslim men could have sexual relations with female captives and slaves. Slave women under sharia did not have a right to own property or to move freely. Sharia, in Islam's history, provided a religious foundation for enslaving non-Muslim women (and men), but allowed for the manumission of slaves. However, manumission required that the non-Muslim slave first convert to Islam. A slave woman who bore a child to her Muslim master ("umm al-walad") could not be sold, becoming legally free upon her master's death, and the child was considered free and a legitimate heir of the father.

Islamic legal tradition has a number of parallels with Judaism. In both religions, revealed law holds a central place, in contrast to Christianity which does not possess a body of revealed law, and where theology rather than law is considered to be the principal field of religious study. Both Islamic and Jewish law ("Halakha") are derived from formal textual revelations (Quran and Pentateuch) as well as less formal, orally transmitted prophetic traditions (hadith and "mishna"). According to some scholars, the words "sharia" and "halakha" both mean literally "the path to follow". The "fiqh" literature parallels rabbinical law developed in the Talmud, with fatwas being analogous to rabbinic "responsa". However, the emphasis on "qiyas" in classical Sunni legal theory is both more explicitly permissive than Talmudic law with respect to authorizing individual reason as a source of law, and more implicitly restrictive, in excluding other, unauthorized forms of reasoning.

Early Islamic law developed a number of legal concepts that anticipated similar such concepts that later appeared in English common law. Similarities exist between the royal English contract protected by the action of debt and the Islamic "Aqd", between the English assize of novel disseisin and the Islamic "Istihqaq", and between the English jury and the Islamic "Lafif" in classical Maliki jurisprudence. The law schools known as Inns of Court also parallel Madrasahs. The methodology of legal precedent and reasoning by analogy ("Qiyas") are also similar in both the Islamic and common law systems, as are the English trust and agency institutions to the Islamic "Waqf" and "Hawala" institutions, respectively.

Elements of Islamic law also have other parallels in Western legal systems. For example, the influence of Islam on the development of an international law of the sea can be discerned alongside that of the Roman influence.

George Makdisi has argued that the madrasa system of attestation paralleled the legal scholastic system in the West, which gave rise to the modern university system. The triple status of "faqih" ("master of law"), "mufti" ("professor of legal opinions") and "mudarris" ("teacher"), conferred by the classical Islamic legal degree, had its equivalents in the medieval Latin terms "magister", "professor" and "doctor", respectively, although they all came to be used synonymously in both East and West. Makdisi suggested that the medieval European doctorate, "licentia docendi" was modeled on the Islamic degree "ijazat al-tadris wa-l-ifta’", of which it is a word-for-word translation, with the term "ifta’" (issuing of fatwas) omitted. He also argued that these systems shared fundamental freedoms: the freedom of a professor to profess his personal opinion and the freedom of a student to pass judgement on what he is learning.

There are differences between Islamic and Western legal systems. For example, Sharia classically recognizes only natural persons, and never developed the concept of a legal person, or corporation, i.e., a legal entity that limits the liabilities of its managers, shareholders, and employees; exists beyond the lifetimes of its founders; and that can own assets, sign contracts, and appear in court through representatives. Interest prohibitions imposed secondary costs by discouraging record keeping and delaying the introduction of modern accounting. Such factors, according to Timur Kuran, have played a significant role in retarding economic development in the Middle East.






</doc>
<doc id="28841" url="https://en.wikipedia.org/wiki?curid=28841" title="Sunnah">
Sunnah

Sunnah (, , plural ), also sunna or sunnat, is the Arabic word for traditional customs and practices; in the Islamic community it refers to the traditions and practices of the Prophet Muhammad, that constitute a model for Muslims to follow. Sunnah is separate from Hadith (the verbally transmitted record of the teachings, deeds and sayings of the Prophet by some of his assumed companions) and Sunnah is what all the Muslims of Prophet Muhammad's time, evidently saw and followed and passed on to the next generations, without any doubt. (For example, the method of offering 5 daily prayers). However, According to classical Islamic theories of Sunnah they are documented by "hadith" (the verbally transmitted record of the teachings, deeds and sayings, silent permissions or disapprovals of Muhammad), and along with the Quran (the holy book of Islam), are the divine revelation ("Wahy") delivered through the Prophet that make up the primary sources of Islamic law and belief/theology. Differing from Sunni classical Islamic theories are those of Shia Muslims, who hold that the Twelve Imams interpret the sunnah, and Sufi who hold that Muhammad transmitted the values of Sunnah "through a series of Sufi teachers."

According to Muslim belief, Muhammad was the best exemplar for Muslims, and several verses in the Quran declare his conduct exemplary, and enjoin his followers to obey him. Sunnah provides a basis not only for major laws and rituals in Islam like how to pray salat, but for "even the most mundane activities", such as the order in which to cut fingernails or the proper length of a beard.

In the pre-Islamic period, "sunnah" was used to mean "manner of acting", whether good or bad. During the early Islamic period, the term referred to any good precedent set by people of the past, including both Muhammad, and his companions. In addition, the Sunnah of the Prophet was not necessarily associated with hadith. 

The classical meaning that now prevails was introduced later in the late second century of Islam, when under the influence of the scholar Al-Shafi‘i, Muhammad's example as recorded in hadith was given priority of over all other precedents set by other authorities. The term "al-sunnah" then eventually came to be viewed as synonymous with the "sunnah" of Muhammad, based on hadith reports. Recording the "sunnah" was also an Arabian tradition and once they converted to Islam, Arabians brought this custom to their religion.

The "sunnah" of Muhammad as based on hadith includes his specific words ("Sunnah Qawliyyah"), habits, practices ("Sunnah Fiiliyyah"), and silent approvals ("Sunnah Taqririyyah").
In Islam, the word ""sunnah"" is also used to refer to religious duties that are optional, such as "Sunnah salat".

' ( , plural ' ) is an Arabic word that means 

Its religious definition can be:
Islam Web gives two slightly different definitions:
It was first used with the meaning of "law" in the Syro-Roman law book before it became widely used in Islamic jurisprudence.

Sunnah and hadith (the words, actions or approval that are narrated about Muhammad and which are believed to document Sunnah) are sometimes used synonymously, but not always. 

In addition to being "the way" of Islam or the traditional social and legal custom and practice of the Islamic community, sunnah is often used as a synonym for "mustahabb" (encouraged) rather than "wajib"/"fard" (obligatory), regarding some commendable action (usually the saying of a prayer).

Sunni Muslims are also referred to as "Ahl as-Sunnah wa'l-Jamā'ah" ("people of the tradition and the community (of Muhammad)") or "Ahl as-Sunnah" for short. Some early Sunnî Muslim scholars (such as Abu Hanifa, al-Humaydî, Ibn Abî `Âsim, Abû Dâwûd, and Abû Nasr al-Marwazî) reportedly used the term "the sunnah" narrowly to refer to Sunni Doctrine as opposed to the creeds of Shia and other non-Sunni Islamic sects. Sunnah literally means face, nature, lifestyle, etc. In the time of prophet Muhammad's companion, newly converted muslims accepted and rejected some set of creed by using reason. So many early muslim scholars started writing books on creed entitled as 'sunnah'.

The word “Sunna” appears several times in the Qur’an, but there is no specific mention of sunnah of the messenger or prophet ("sunnat al-rasool", "sunnat al-nabi" or "sunna al-nabawiyyah"), i.e. the way/practice of Prophet Muhammad. (There "are" several verses calling on Muslims to obey Muhammad—see below.) Four verses (8.38, 15.13, 18.55) use the expression “"sunnat al-awwalin"”, which is thought to mean “the way or practice of the ancients.” It is described as something "that has passed away" or prevented unbelievers from accepting God. “"Sunnat Allah"” (the “way of God”) appears eight times in five verses. In addition, verse 17.77 talks of both the way of other, earlier Muslim messengers (Ibrahim, Musa, etc.), and of "our way", i.e. God's way.
[This is] the way ("sunna") of those whom we sent [as messengers] before you, and you will not find any change in Our way ("sunnatuna").

This indicates to some scholars (such as Javed Ahmad Ghamidi) that sunnah predates both the Quran and Muhammad, and is actually the tradition of the prophets of God, specifically the tradition of Abraham. Christians, Jews and the Arab descendants of Ishmael, the Arabized Arabs or Ishmaelites, when Muhammad reinstituted this practice as an integral part of Islam.

According to historians (particularly Daniel W. Brown), the classical Islamic definition of Sunnah as the customs and practices of Muhammad (only) was not the original one.

Prior to the "golden age of classical Islamic jurisprudence", the "ancient schools" of law prevailed. 
The golden age, starting with the creation of the Hanafi, Maliki, Shafi'i, Hanbali, etc. schools of fiqh in the second century of Islam, limited sunnah to “traditions traced back to the Prophet Muhammad himself” ("sunna al-nabawiyyah"). But the ancient "regional" schools of law, located in several major cities of the new Arab empire of Islam -- Mecca, Kufa, Basra, Syria, etc., -- had a more flexible definition of sunnah than is now commonly used. This being the "acceptable norms" or "custom", which included examples of the Prophet's Companions, the rulings of the Caliphs, and practices that “had gained general acceptance among the jurists of that school”. 

Examples of the use of non-Muhammadan sunnahs at this time is found in a (non-Muhammadan) tradition/hadith comment 

In al-Ṭabarī's history of early Islam, the term "Sunnah of the Prophet" is not only used "surprisingly infrequently", but used to refer to "political oaths or slogans used by rebels", or "a general standard of justice and right conduct", and not "to specific precedents set by Muhammad", let alone hadith. An early theological writing by Hasan al-Basri ("Risala fi'l Qadar") also is "empty of references to specific cases" when mentioning Sunnah of the Prophet.
Daniel Brown states that the first extant writings of Islamic legal reasoning were "virtually hadith-free" and argues that other examples of a lack of connection between sunnah and hadith" can be found in:

According to one source (Ahmad Kazemi Moussavi and Karim Douglas Crow), early Sunni scholars often considered "sunnah" equivalent to the biography of Muhammed ("sira"). As the "hadith" came to be better documented and the scholars who validated them gained prestige, the "sunnah" came often to be known mostly through the "hadith", especially as variant or fictional biographies of Muhammad spread.

Abū ʿAbdullāh Muhammad ibn Idrīs al-Shāfiʿī (150-204 AH), known as al-Shafi'i, argued against flexible sunnah and the use of precedents from multiple sources, emphasizing the final authority of a hadith of Muhammad, so that even the Qur'an was "to be interpreted in the light of traditions (i.e. hadith), and not vice versa." While the sunnah has often been called "second to the Quran", hadith has also been said to "rule over and interpret the Quran". 
Al-Shafiʿi "forcefully argued" that the sunnah stands "on equal footing with the Quran", (according to scholar Daniel Brown) both being divine revelation. As Al-Shafi'i put it, “the command of the Prophet is the command of God.” (Notwithstanding the triumph of this theory, in practice the schools of fiqh resisted the thorough application of hadith and fiqh was little changed from the days before Al-Shafi'i.)

Sunnah of Muhammad outranked all other, and "broad agreement" developed that "hadith must be the basis for authentication of any Sunnah," (according to M.O. Farooq). Al-Shafiʿi's success was such that later writers “hardly ever thought of sunnah as comprising anything but that of the Prophet”, and sunnah was often considered synonymous with hadith.

While the earliest Muslim lawyers "felt no obligation" to provide documentation of hadith when arguing their case, and the Sunnah was not recorded and written during the Prophet's lifetime, (according to scholar Khaled Abou El Fadl), all this changed with the triumph of Al-Shafi‘i and a "broad agreement" that Hadith should be used to authenticate Sunnah, (according to M.O. Farooq), over the course of the second century, when legal works began incorporated Prophetic hadith. 

Hadith was now systematically collected and documented, but several generations having passed since the time of its occurrence meant that "many of the reports attributed to the Prophet are apocryphal or at least are of dubious historical authenticity," (according to Abou El Fadl). "In fact, one of the most complex disciplines in Islamic jurisprudence is one which attempts to differentiate between authentic and inauthentic traditions."
Islam jurists divide sunnah into that which has no legal consequences --"al-sunna al-ʿādīyah" -- (the "personal habits and preferences" of Muhammad); and that which is binding on Muslims -- "al-sunna al-hudā". The literalist Zāhirī school disagrees holding that there was no sunnah whose fulfillment is not rewarded or neglect punished, while classical Islam holds that following non-binding "al-sunna al-ʿādīyah" is meritorious but not obligatory. 

Sufis see the "division between binding and non-binding" sunnah as "meaningless". Muhammad is "al-insān al-kāmil", the perfect man, "labib-Allah" beloved of God, an intercessor, a "channel of divine light". Imitating his every action is "the ultimate expression" of piety. or in the words of Al-Ghazālī: 
Know that the key to joy is following the sunnah and imitating the Prophet in all his comings and goings, words and deeds, extending to his manner of eating, rising, sleeping and speaking. I do not say this only in relation to requirements of religion ["ʿibādāt"], for there is no escaping these; rather, this includes every area of behavior ["ʿādāt"]. 
In the 19th century, "social and political turmoil" starting with the decline of the Moghal empire, caused some Muslims to seek a more humanized figure of Muhammad. The miracle-performing "larger than life" prophetic figure was de-emphasized in favor of "a practical model for restoration of the Muslim community," a virtuous, progressive social reformer. Nasserist Egypt, for example, celebrated the "imam of socialism" rather than the cosmic "perfect man". One who argued against the idea of sunnah as divine revelation, and for the idea that Muhammad's mission was simply to transmit the Quran was Ghulam Ahmed Perwez (1903–1985). He quoted the Quranic verse "The messenger has no duty except to proclaim [the message]," (Q.5:99) and pointed out several other verses where God corrects something Muhammad has done or said (8:67),(9:43), (66:1), thus demonstrating Muhammad's lack of supernatural knowledge.

This era of rapid social and technological change, decline of Muslim power, and replacement of classical madhhab by Western-inspired legal codes in Muslim lands, also suggested a turn away from the "detailed precedents in civil and political affairs," called for by traditional Hadith, "for if worldly matters require detailed prophetic guidance, then every age will require a new prophet to accommodate changing circumstances".

With de-colonialization in the late 20th century, a new Islamic revival emerged. Activists rather than theorists, they sought "to restore Islam to ascendency", and in particular to restore Sharia to the law of the lands of Islam it had been before being replaced by "secular, Western inspired law codes" of colonialism and modernity. Like modernists, revivalists "vehemently rejected" "taqlid" and were not particularly interested in the classical schools of law ("madhhab"). But revivalists like Abul A'la Maududi and Mustafa al-Siba'i support for "the authority of Sunnah and the authenticity of Hadith in general" was "unwavering", as was their opposition to "Hadith denialism". At the same time they agreed that restoring "relevant" Sharia required "some reformulation" of the law, which would require a return to sources, which required agreement on how the sources were be to be "interpreted and understand" and reassessment of hadith. This involved examining hadith content ("matn") for its spirit and relevance "within the context of the Sharia as a whole" according to the method of scholars of Islamic law ("fuqaha") and weeding out corrupted hadith inconsistent with "reason, with human nature, and with historical conditions". Shibli Nomani, Abul A'la Maududi, Rashid Rida, and Mohammed al-Ghazali being proponents of this effort.

Although "most writers agree", including skeptics, that "sunnah and hadith must stand or fall together", some (Fazlur Rahman Malik, Javed Ahmad Ghamidi) have attempted to "establish a basis for sunnah independent of hadith", working around problem of hadith authenticity raised by modernist and Western critics, while reaching back to pre-al-Shafiʿi meaning of Sunnah.

In the 1960s, Fazlur Rahman Malik, an Islamic modernist and former head of Pakistan's Central Institute for Islamic Research, advanced another idea for how the (Prophetic) sunnah -- the normative example of the Prophet -- should be understood: as "a general umbrella concept" but not one "filled with absolutely specific content", or that was static over the centuries. He argued that Muhammad had come as a "moral reformer" and not a "pan-legit", and that the specifics of the sunnah would be agreed upon community of his followers, evolving with changing times as a "living and on-going process". He accepted the criticism of Western and Muslim scholars that the content of many hadith and isnad (chain of transmitters) had been tampered with by Muslims trying to prove the Muhammad had made a specific statement -- but this did not make them fraudulent or forgeries, because if "Hadith verbally speaking does not go back to the Prophet, its spirit certainly does". Instead these collections of ahadith of al-Bukhari and al-Muslim's were "ijma" (consensus or agreement of the Muslim scholars -- which is another classical source of Islamic law). Doing so they follow the spirit of the Prophet's mission, and "resurrect" the legal methodology of the pre-Shafi'i "Ancient schools". But just as second and third century Muslims could re-formulate hadith and law around a prophetic spirit, so can modern Muslims -- redefining "riba" and replacing medieval laws against bank interest with measures that help the poor without harming economic productivity.

Some of the most basic and important features of the sunnah — worship rituals like "salat" (ritual prayer), "zakat" (ritual tithing), "hajj" (pilgrimage to Mecca), "sawm" (dawn to dusk fasting during Ramadan) — are known to Muslim from being passed down `from the many to the many` (according to scholars of fiqh such as Al-Shafi'i), bypassing books of hadith, (which were more often consulted for answers to details not agreed upon or not frequently practiced) and issues of authenticity.

Modernist Rashid Rida thought this "the only source of sunnah that is beyond dispute". S.M. Yusuf argued "practice is best transmitted through practice", and a more reliable way to establish Sunnah than hadith. He also believed that the passing down of practice from generation to generation independent of hadith explained why early schools of law did not differentiate between sunnah of the caliphate and sunnah of the prophet.
According to Javed Ahmad Ghamidi, another Modernist, this passing down by continuous practice of the Muslim community (which also indicates consensus, "ijma") was similar to how the Qur’ān has been "received by the "ummah"" (Muslim community) through the consensus of the Prophet's Companions and through their perpetual recitation. Consequently, Ghamidi sees this more limited Sunnah of continuous practice as the true sunnah — equally authentic to the Quran, but shedding orthodox sunnah and avoiding problematic basis of the hadith.

Sufi thinkers "emphasized personal spirituality and piety rather than the details of fiqh".
According to the view of some Sufi Muslims who incorporate both the outer and inner reality of Muhammad, the deeper and true sunnah are the noble characteristics and inner state of Muhammad -- "Khuluqin Azim" or 'Exalted Character'. To them Muhammad's attitude, his piety, the quality of his character constitute the truer and deeper aspect of what it means by sunnah in Islam, rather than the external aspects alone. They argue that the external customs of Muhammad loses its meaning without the inner attitude and also many hadiths are simply custom of the Arabs, not something that is unique to Muhammad.

The Qur'an contains numerous commands to follow the Prophet. Among the Quranic verses quoted as demonstrating the importance of hadith/sunnah to Muslims are 
Say: Obey Allah and obey the Messenger,

Which appears in several verses: , , , 

Your companion [Muhammad] has not strayed, nor has he erred, Nor does he speak from [his own] inclination or desire.

"A similar (favour have ye already received) in that We have sent among you a Messenger of your own, rehearsing to you Our Signs, and sanctifying you, and instructing you in Scripture and Wisdom, and in new knowledge.

"Ye have indeed in the Messenger of Allah a beautiful pattern (of conduct) for any one whose hope is in Allah and the Final Day, and who engages much in the Praise of Allah."

The teachings of "wisdom" ("hikma") have been declared to be a function of Muhammad along with the teachings of the scripture. Several Quranic verses mention "wisdom" ("hikmah") coupled with "scripture" or "the book" (i.e. the Quran) -- "al-kitāb wa al-ḥikma". Mainstream scholars starting with al-Shafi'i believe "hikma" refers to the Sunnah, and this connection between Sunnah and the Quran is evidence of the Sunnah's divinity and authority.

Therefore, along with the Quran, the "sunnah" was revealed. Modern Sunni scholars have examined both the "sira" and the "hadith" in order to justify modifications to jurisprudence ("fiqh"). 
Hense, the imitation of Muhammad helps Muslims to know and be loved by God.

Another piece of evidence for the divinity of the Sunnah -- according to its supporters -- are verses in the Quran that refer to revelations not found "in" the Quran. For example, there is no verse mentioning the original direction of prayer (the "qibla") in the Quran, but God in the Quran does say He appointed the original qibla (). Other events mentioned in the Quran that already happened without Quranic command or description include a dream in which Muhammad would enter Mecca (); Muhammad's marriage to Zayd's ex-wife (); and the dispute over the division of spoils after the Battle of Badr (); all "definitive proof that besides the Quran other commands came to the Prophet by the agency of waḥy," according to revivalist Abul A'la Maududi.

The minority argument against the Sunnah of the prophet being divine revelation ("waḥy") goes back to the "ahl al-Kalam" who al-Shāfiʿī argued against in the second century of Islam. Their modern "Quranists", the modern successors of the "ahl al-Kalam", argue that the sunnah falls short of the standard of the Quran in divinity. Specifically because

According to John Burton, paraphrasing Al-Shafi'i, "it must be remembered that the Quran text are couched in very general terms which it is the function of the sunnah to expand and elucidate, to make God's meaning absolutely clear."
There are a number of verses in the Quran where "to understand the context, as well as the meaning", Muslims need to refer to the record of the life and example of the Prophet.

It is thought that verses 16:44 and 64 indicate that Muhammed's mission "is not merely that of a deliveryman who simply delivers the revelation from Allah to us, rather, he has been entrusted with the most important task of explaining and illustrating" the Quran. 
And We have also sent down unto you (O Muhammad) the reminder and the advice (the Quran), that you may explain clearly to men what is sent down to them, and that they may give thought.

And We have not sent down the Book (the Quran) to you (O Muhammad), except that you may explain clearly unto them those things in which they differ, and (as) a guidance and a mercy for a folk who believe. [Quran 16:64]

For example, while the Quran presents the general principles of praying, fasting, paying zakat, or making pilgrimage, they are presented "without the illustration found in Hadith, for these acts of worship remain as abstract imperatives in the Qur’an".

Sunnah upon which "fiqh" is based may be divided into:

It may be also divided into sunnah that is binding for Muslims and that which is not. Ibn Qutaybah (213-276 AH) distinguished between:
In the terminology of "fiqh" (Islamic jurisprudence), sunnah denotes whatever though not obligatory, is "firmly established ("thabata") as called for ("matlub")" in Islam "on the basis of a legal proof ("dalîl shar`î").

According to scholar Gibril Fouad Haddad, the "sciences of the Sunnah" ("`ulûm as-Sunna") refer to:
the biography of the Prophet ("as-sîra"), the chronicle of his battles ("al-maghâzî"), his everyday sayings and acts or "ways" ("sunan"), his personal and moral qualities ("ash-shamâ'il"), and the host of the ancillary hadîth sciences such as the circumstances of occurrence ("asbâb al-wurûd"), knowledge of the abrogating and abrogated hadîth, difficult words ("gharîb al-hadîth"), narrator criticism ("al-jarh wat-ta`dîl"), narrator biographies ("al-rijâl"), etc., as discussed in great detail in the authoritative books of al-Khatîb al-Baghdâdî.

Shia Islam does not use the "Kutub al-Sittah" (six major "hadith" collections) followed by Sunni Islam, therefore the Sunnah of Shia Islam and the Sunnah of Sunni Islam refer to different collections of religious canonical literature.

The primary collections of Sunnah of Shia Islam were written by three authors known as the 'Three Muhammads', and they are:


Unlike Akhbari Twelver Shiites, Usuli Twelver Shiite scholars do not believe that everything in the four major books of the Sunnah of Shia Islam is authentic.

In Shia "hadees" one often finds sermons attributed to Ali in The Four Books or in the Nahj al-Balagha.





</doc>
<doc id="28845" url="https://en.wikipedia.org/wiki?curid=28845" title="Safe sex">
Safe sex

Safe sex is sexual activity using methods or devices (such as condoms) to reduce the risk of transmitting or acquiring sexually transmitted infections (STIs), especially HIV. "Safe sex" is also sometimes referred to as safer sex or protected sex to indicate that some safe sex practices do not completely eliminate STI risks. It is also sometimes used colloquially to describe methods aimed at preventing pregnancy that may or may not also lower STI risks.

The concept of "safe sex" emerged in the 1980s as a response to the global AIDS epidemic, and possibly more specifically to the AIDS crisis in the US. Promoting safe sex is now one of the main aims of sex education and STI prevention, especially reducing new HIV infections. Safe sex is regarded as a harm reduction strategy aimed at reducing the risk of STI transmission.

Although some safe sex practices (like condoms) can also be used as birth control ("contraception"), most forms of contraception do not protect against STIs. Likewise, some safe sex practices, such as partner selection and low-risk sex behavior, might not be effective forms of contraception.

Although strategies for avoiding STIs like syphilis and gonorrhea have existed for centuries and the term "safe sex" existed in English as early as the 1930s, the use of the term to refer to STI-risk reduction dates to the mid-1980s in the United States. It emerged in response to the HIV/AIDS crisis.

A year before the HIV virus was isolated and named, the San Francisco chapter of the Sisters of Perpetual Indulgence published a small pamphlet titled "Play Fair!" out of concern over widespread STIs among the city's gay male population. It specifically named illnesses (Kaposi's sarcoma and pneumocystis pneumonia) that would later be understood as symptoms of advanced HIV disease (or AIDS). The pamphlet advocated a range of safe-sex practices, including abstinence, condoms, personal hygiene, use of personal lubricants, and STI testing/treatment. It took a casual, sex-positive approach while also emphasizing personal and social responsibility. In May 1983—the same month HIV was isolated and named in France—the New York City-based HIV/AIDS activists Richard Berkowitz and Michael Callen published similar advice in their booklet, "". Neither publication used the term "safe sex" but both included recommendations that are now standard advice for reducing STI (including HIV) risks.

Safe sex as a form of STI risk reduction appeared in journalism as early as 1984, in the British publication 'The Intelligencer': ""The goal is to reach about 50 million people with messages about safe sex and AIDS education."

Although "safe sex" is used by individuals to refer to protection against both pregnancy and HIV/AIDS or other STI transmissions, the term was born in response to the HIV/AIDS epidemic. It is believed that the term "safe sex" was used in the professional literature in 1984, in the content of a paper on the psychological effect that HIV/AIDS may have on gay and bisexual men.
A year later, the same term appeared in an article in "The New York Times." This article emphasized that most specialists advised their AIDS patients to practice safe sex. The concept included limiting the number of sexual partners, using prophylactics, avoiding bodily fluid exchange, and resisting the use of drugs that reduced inhibitions for high-risk sexual behavior. Moreover, in 1985, the first safe sex guidelines were established by the 'Coalition for Sexual Responsibilities'. According to these guidelines, safe sex was practiced by using condoms also when engaging in anal or oral sex.

Although the term "safe sex" was primarily used in reference to sexual activity between men, in 1986 the concept was spread to the general population. Various programs were developed with the aim of promoting safe sex practices among college students. These programs were focused on promoting the use of the condom, a better knowledge about the partner's sexual history and limiting the number of sexual partners. The first book on this subject appeared in the same year. The book was entitled "Safe Sex in the Age of AIDS", it had 88 pages and it described both positive and negative approaches to sexual life. Sexual behavior could be either safe (kissing, hugging, massage, body-to-body rubbing, mutual masturbation, exhibitionism, phone sex, and use of separate sex toys); possibly safe (use of condoms); and unsafe.

In 1997, specialists in this matter promoted the use of condoms as the most accessible safe sex method (besides abstinence) and they called for TV commercials featuring condoms. During the same year, the Catholic Church in the United States issued their own "safer sex" guidelines on which condoms were listed, though two years later the Vatican urged chastity and heterosexual marriage, attacking the American Catholic bishops' guidelines.

A study carried out in 2006 by Californian specialists showed that the most common definitions of safe sex are condom use (68% of the interviewed subjects), abstinence (31.1% of the interviewed subjects), monogamy (28.4% of the interviewed subjects) and safe partner (18.7% of the interviewed subjects).

The term "safer sex" in Canada and the United States has gained greater use by health workers, reflecting that risk of transmission of sexually transmitted infections in various sexual activities is a continuum. The term "safe sex" is still in common use in the United Kingdom, Australia and New Zealand.

"Safer sex" is thought to be a more aggressive term which may make it more obvious to individuals that any type of sexual activity carries a certain degree of risk.

The term "safe love" has also been used, notably by the French Sidaction in the promotion of men's underpants incorporating a condom pocket and including the red ribbon symbol in the design, which were sold to support the charity.

A range of safe-sex practices are commonly recommended by sexual health educators and public health agencies. Many of these practices can reduce (but not completely eliminate) risk of transmitting or acquiring STIs.

Sexual activities, such as phone sex, cybersex, and sexting, that do not include direct contact with the skin or bodily fluids of sexual partners, carry no STI risks and, thus, are forms of safe sex.

A range of sex acts called "non-penetrative sex" or "outercourse" can significantly reduce STI risks. Non-penetrative sex includes practices such as kissing, mutual masturbation, rubbing or stroking. According to the Health Department of Western Australia, this sexual practice may prevent pregnancy and most STIs. However, non-penetrative sex may not protect against infections that can be transmitted via skin-to-skin contact, such as herpes and human papilloma virus. Mutual or partnered masturbation carries some STI risk, especially if there is skin contact or shared bodily fluids with sexual partners, although the risks are significantly lower than many other sexual activities.

Barriers, such as condoms, dental dams, and medical gloves can prevent contact with body fluids (such as blood, vaginal fluid, semen, rectal mucus), and other means of transmitting STIs (like skin, hair and shared objects) during sexual activity.

Oil-based lubrication can break down the structure of latex condoms, dental dams or gloves, reducing their effectiveness for STI protection.

While use of external condoms can reduce STI risks during sexual activity, they are not 100% effective. One study has suggested condoms might reduce HIV transmission by 85% to 95%; effectiveness beyond 95% was deemed unlikely because of slippage, breakage, and incorrect use. It also said, "In practice, inconsistent use may reduce the overall effectiveness of condoms to as low as 60–70%".

Pre-exposure prophylaxis (often abbreviated as "PrEP") is the use of prescription drugs by those who do not have HIV to prevent HIV infection. PrEP drugs are taken "prior" to HIV exposure to prevent the transmission of the virus, usually between sexual partners. PrEP drugs do not prevent other STI infections or pregnancy.

As of 2018, the most-widely approved form of "PrEP" combines two drugs (tenofovir and emtricitabine) in one pill. That drug combination is sold under the brand name Truvada by Gilead Sciences. It is also sold in generic formulations worldwide. Other drugs are also being studied for use as PrEP.

Different countries have approved different protocols for using the tenofovir/emtricitabine-combination drug as "PrEP". That two-drug combination has been shown to prevent HIV infection in different populations when taken daily, intermittently, and on demand. Numerous studies have found the tenofovir/emtricitabine combination to be over 90% effective at preventing HIV transmission between sexual partners.

Treatment as Prevention (often abbreviated as "TasP") is the practice of testing for and treating HIV infection as a way to prevent further spread of the virus. Those having knowledge of their HIV-positive status can use safe-sex practices to protect themselves and their partners (such as using condoms, sero-sorting partners, or choosing less-risky sexual activities). And, because HIV-positive people with durably suppressed or undetectable amounts of HIV in their blood "cannot transmit HIV to sexual partners", sexual activity with HIV-positive partners on effective treatment is a form of safe sex (to prevent HIV infection). This fact has given rise to the concept of "U=U" ("Undetectable = Untransmittable").

Other methods proven effective at reducing STI risks during sexual activity are:

Most methods of contraception are not effective at preventing the spread of STIs. This includes birth control pills, vasectomy, tubal ligation, periodic abstinence, IUDs and many non-barrier methods of pregnancy prevention. However, condoms are highly effective for birth control and STI prevention.

The spermicide Nonoxynol-9 has been claimed to reduce the likelihood of STI transmission. However, a technical report by the World Health Organization has shown that Nonoxynol-9 is an irritant and can produce tiny tears in mucous membranes, which may increase the risk of transmission by offering pathogens more easy points of entry into the system. They reported that Nonoxynol-9 lubricant do not have enough spermicide to increase contraceptive effectiveness cautioned they should not be promoted. There is no evidence that spermicidal condoms are better at preventing STD transmission compared to condoms that do not have spermicide. If used properly, spermicidal condoms can prevent pregnancy, but there is still an increased risk that nonoxynyl-9 can irritate the skin, making it more susceptible for infections.

The use of a diaphragm or contraceptive sponge provides some women with better protection against certain sexually transmitted diseases, but they are not effective for all STIs.

Hormonal methods of preventing pregnancy (such as oral contraceptives [i.e. 'The pill'], depoprogesterone, hormonal IUDs, the vaginal ring, and the patch) offer no protection against STIs. The copper intrauterine device and the hormonal intrauterine device provide an up to 99% protection against pregnancies but no protection against STIs. Women with copper intrauterine device "may" be subject to greater risk of infection from bacterial infectious such as gonorrhea or chlamydia, although this is debated.

Coitus interruptus (or "pulling out"), in which the penis is removed from the vagina, anus, or mouth before ejaculation, may reduce transmission of STIs but still carries significant risk. This is because pre-ejaculate, a fluid that oozes from the penile urethra before ejaculation, may contain STI pathogens. Additionally, the microbes responsible for some diseases, including genital warts and syphilis, can be transmitted through skin-to-skin or mucous membrane contact.

Unprotected anal penetration is considered a high-risk sexual activity because the thin tissues of the anus and rectum can be easily damaged. Slight injuries can allow the passage of bacteria and viruses, including HIV. This includes penetration of the anus by fingers, hands, or sex toys such as dildos. Also, condoms may be more likely to break during anal sex than during vaginal sex, increasing the risk of STI transmission.

The main risk which individuals are exposed to when performing anal sex is the transmission of HIV. Other possible infections include Hepatitis A, B and C; intestinal parasite infections like "Giardia"; and bacterial infections such as "Escherichia coli."

Anal sex should be avoided by couples in which one of the partners has been diagnosed with an STI until the treatment has proven to be effective.

In order to make anal sex safer, the couple must ensure that the anal area is clean and the bowel empty and the partner on whom anal penetration occurs should be able to relax. Regardless of whether anal penetration occurs by using a finger or the penis, the condom is the best barrier method to prevent transmission of STI. Enemas should not be used as they can increase the risk of HIV infection and lymphogranuloma venereum proctitis.

Since the rectum can be easily damaged, the use of lubricants is highly recommended even when penetration occurs by using the finger. Especially for beginners, using a condom on the finger is both a protection measure against STI and a lubricant source. Most condoms are lubricated and they allow less painful and easier penetration. Oil-based lubricants damage latex and should not be used with condoms; water-based and silicone-based lubricants are available instead. Non-latex condoms are available for people who are allergic to latex made out of polyurethane or polyisoprene. Polyurethane condoms can safely be used with oil-based lubricant. The "female condom" may also be used effectively by the anal receiving partner.

Anal stimulation with a sex toy requires similar safety measures to anal penetration with a penis, in this case using a condom on the sex toy in a similar way.

It is important that the man washes and cleans his penis after anal intercourse if he intends to penetrate the vagina. Bacteria from the rectum are easily transferred to the vagina, which may cause vaginal and urinary tract infections.

When anal-oral contact occurs, protection is required since this is a risky sexual behavior in which illnesses as Hepatitis A or STIs can be easily transmitted, as well as enteric infections. The dental dam or the plastic wrap are effective protection means whenever anilingus is performed.

Putting a condom on a sex toy provides better sexual hygiene and can help to prevent transmission of infections if the sex toy is shared, provided the condom is replaced when used by a different partner. Some sex toys are made of porous materials, and pores retain viruses and bacteria, which makes it necessary to clean sex toys thoroughly, preferably with use of cleaners specifically for sex toys. Glass is non-porous and medical grade glass sex toys more easily sterilized between uses.

In cases in which one of the partners is treated for an STI, it is recommended that the couple not use sex toys until the treatment has proved to be effective.

All sex toys have to be properly cleaned after use. The way in which a sex toy is cleaned varies on the type of material it is made of. Some sex toys can be boiled or cleaned in a dishwasher. Most of the sex toys come with advice on the best way to clean and store them and these instructions should be carefully followed. A sex toy should be cleaned not only when it is shared with other individuals but also when it is used on different parts of the body (such as mouth, vagina or anus).

A sex toy should regularly be checked for scratches or breaks that can be breeding ground for bacteria. It is best if the damaged sex toy is replaced by a new undamaged one. Even more hygiene protection should be considered by pregnant women when using sex toys. Sharing any type of sex toy that may draw blood, like whips or needles, is not recommended, and is not safe.

When using sex toys in the anus, sex toys "...can easily get lost" as "rectal muscles contract and can suck an object up and up, potentially obstructing the colon"; to prevent this serious problem, sex toy users are advised to use sex "...toys with a flared base or a string".

Sexual abstinence reduces STIs and pregnancy risks associated with sexual contact, but STIs may also be transmitted through non-sexual means, or by rape. HIV may be transmitted through contaminated needles used in tattooing, body piercing, or injections. Medical or dental procedures using contaminated instruments can also spread HIV, while some health-care workers have acquired HIV through occupational exposure to accidental injuries with needles. Evidence does not support the use of abstinence-only sex education. Abstinence-only sex education programs have been found to be ineffective in decreasing rates of HIV infection in the developed world and unplanned pregnancy. Abstinence-only sex education primarily relies on the consequences of character and morality while health care professionals are concerned about matters regarding health outcomes and behaviors. Though abstinence is the best course to prevent pregnancy and STIs, in reality, it is unrealistic so intentions to abstain from sexual activity are often unsuccessful. It leaves young people without the information and skills they need to avoid unwanted pregnancies and STIs.




</doc>
<doc id="28846" url="https://en.wikipedia.org/wiki?curid=28846" title="STD">
STD

STD may refer to:





</doc>
<doc id="28848" url="https://en.wikipedia.org/wiki?curid=28848" title="Scabies">
Scabies

Scabies (also known as the seven-year itch) is a contagious skin infestation by the mite "Sarcoptes scabiei". The most common symptoms are severe itchiness and a pimple-like rash. Occasionally, tiny burrows may appear on the skin. In a first-ever infection, infected person will usually develop symptoms within two to six weeks. During a second infection, symptoms may begin within 24 hours. These symptoms can be present across most of the body or just certain areas such as the wrists, between fingers, or along the waistline. The head may be affected, but this is typically only in young children. The itch is often worse at night. Scratching may cause skin breakdown and an additional bacterial infection in the skin.
Scabies is caused by infection with the female mite "Sarcoptes scabiei "var." hominis", an ectoparasite. The mites burrow into the skin to live and deposit eggs. The symptoms of scabies are due to an allergic reaction to the mites. Often, only between 10 and 15 mites are involved in an infection. Scabies is most often spread during a relatively long period of direct skin contact with an infected person (at least 10 minutes) such as that which may occur during sex or living together. Spread of disease may occur even if the person has not developed symptoms yet. Crowded living conditions, such as those found in child-care facilities, group homes, and prisons, increase the risk of spread. Areas with a lack of access to water also have higher rates of disease. Crusted scabies is a more severe form of the disease. It typically only occurs in those with a poor immune system and people may have millions of mites, making them much more contagious. In these cases, spread of infection may occur during brief contact or by contaminated objects. The mite is very small and usually not directly visible. Diagnosis is based on the signs and symptoms.
A number of medications are available to treat those infected, including permethrin, crotamiton, and lindane creams and ivermectin pills. Sexual contacts within the last month and people who live in the same house should also be treated at the same time. Bedding and clothing used in the last three days should be washed in hot water and dried in a hot dryer. As the mite does not live for more than three days away from human skin, more washing is not needed. Symptoms may continue for two to four weeks following treatment. If after this time symptoms continue, retreatment may be needed.
Scabies is one of the three most common skin disorders in children, along with ringworm and bacterial skin infections. As of 2015, it affects about 204 million people (2.8% of the world population). It is equally common in both sexes. The young and the old are more commonly affected. It also occurs more commonly in the developing world and tropical climates. The word scabies is from "", "to scratch". Other animals do not spread human scabies. Infection in other animals is typically caused by slightly different but related mites and is known as sarcoptic mange.

The characteristic symptoms of a scabies infection include intense itching and superficial burrows. Because the host develops the symptoms as a reaction to the mites' presence over time, typically a delay of four to six weeks occurs between the onset of infestation and the onset of itching. Similarly, symptoms often persist for one to several weeks after successful eradication of the mites. As noted, those re-exposed to scabies after successful treatment may exhibit symptoms of the new infestation in a much shorter period—as little as one to four days.

In the classic scenario, the itch is made worse by warmth, and is usually experienced as being worse at night, possibly because distractions are fewer. As a symptom, it is less common in the elderly.

The superficial burrows of scabies usually occur in the area of the finger webs, feet, ventral wrists, elbows, back, buttocks, and external genitals. Except in infants and the immunosuppressed, infection generally does not occur in the skin of the face or scalp. The burrows are created by excavation of the adult mite in the epidermis. Acropustulosis, or blisters and pustules on the palms and soles of the feet, are characteristic symptoms of scabies in infants.
In most people, the trails of the burrowing mites are linear or S-shaped tracks in the skin often accompanied by rows of small, pimple-like mosquito or insect bites. These signs are often found in crevices of the body, such as on the webs of fingers and toes, around the genital area, in stomach folds of the skin, and under the breasts of women.

Symptoms typically appear two to six weeks after infestation for individuals never before exposed to scabies. For those having been previously exposed, the symptoms can appear within several days after infestation. However, symptoms may appear after several months or years.

The elderly, disabled, and people with an impaired immune system, such as those with HIV, cancer, or those on immunosuppressive medications, are susceptible to crusted scabies (also called Norwegian scabies). On those with weaker immune systems, the host becomes a more fertile breeding ground for the mites, which spread over the host's body, except the face. The mites in crusted scabies are not more virulent than in noncrusted scabies; however, they are much more numerous (up to two million). People with crusted scabies exhibit scaly rashes, slight itching, and thick crusts of skin that contain large numbers of scabies mites. For this reason, persons with crusted scabies are more contagious to others than those with typical scabies. Such areas make eradication of mites particularly difficult, as the crusts protect the mites from topical miticides/scabicides, necessitating prolonged treatment of these areas.

In the 18th century, Italian biologists Giovanni Cosimo Bonomo and Diacinto Cestoni (1637–1718) described the mite now called "Sarcoptes scabiei", variety "hominis", as the cause of scabies. "Sarcoptes" is a genus of skin parasites and part of the larger family of mites collectively known as scab mites. These organisms have eight legs as adults, and are placed in the same phylogenetic class (Arachnida) as spiders and ticks.

"S. scabiei" mites are under 0.5 mm in size, but are sometimes visible as pinpoints of white. Gravid females tunnel into the dead, outermost layer (stratum corneum) of a host's skin and deposit eggs in the shallow burrows. The eggs hatch into larvae in three to ten days. These young mites move about on the skin and molt into a "nymphal" stage, before maturing as adults, which live three to four weeks in the host's skin. Males roam on top of the skin, occasionally burrowing into the skin. In general, the total number of adult mites infesting a healthy hygienic person with noncrusted scabies is small, about 11 females in burrows, on average.

The movement of mites within and on the skin produces an intense itch, which has the characteristics of a delayed cell-mediated inflammatory response to allergens. IgE antibodies are present in the serum and the site of infection, which react to multiple protein allergens in the body of the mite. Some of these cross-react to allergens from house dust mites. Immediate antibody-mediated allergic reactions (wheals) have been elicited in infected persons, but not in healthy persons; immediate hypersensitivity of this type is thought to explain the observed far more rapid allergic skin response to reinfection seen in persons having been previously infected (especially having been infected within the previous year or two).

Scabies is contagious and can be contracted through prolonged physical contact with an infested person. This includes sexual intercourse, although a majority of cases are acquired through other forms of skin-to-skin contact. Less commonly, scabies infestation can happen through the sharing of clothes, towels, and bedding, but this is not a major mode of transmission; individual mites can survive for only two to three days, at most, away from human skin at room temperature. As with lice, a latex condom is ineffective against scabies transmission during intercourse, because mites typically migrate from one individual to the next at sites other than the sex organs.

Healthcare workers are at risk of contracting scabies from patients, because they may be in extended contact with them.

The symptoms are caused by an allergic reaction of the host's body to mite proteins, though exactly which proteins remains a topic of study. The mite proteins are also present from the gut, in mite feces, which are deposited under the skin. The allergic reaction is both of the delayed (cell-mediated) and immediate (antibody-mediated) type, and involves IgE (antibodies are presumed to mediate the very rapid symptoms on reinfection). The allergy-type symptoms (itching) continue for some days, and even several weeks, after all mites are killed. New lesions may appear for a few days after mites are eradicated. Nodular lesions from scabies may continue to be symptomatic for weeks after the mites have been killed.

Rates of scabies are negatively related to temperature and positively related to humidity.

Scabies may be diagnosed clinically in geographical areas where it is common when diffuse itching presents along with either lesions in two typical spots or itchiness is present in another household member. The classical sign of scabies is the burrow made by a mite within the skin. To detect the burrow, the suspected area is rubbed with ink from a fountain pen or a topical tetracycline solution, which glows under a special light. The skin is then wiped with an alcohol pad. If the person is infected with scabies, the characteristic zigzag or S pattern of the burrow will appear across the skin; however, interpreting this test may be difficult, as the burrows are scarce and may be obscured by scratch marks. A definitive diagnosis is made by finding either the scabies mites or their eggs and fecal pellets. Searches for these signs involve either scraping a suspected area, mounting the sample in potassium hydroxide and examining it under a microscope, or using dermoscopy to examine the skin directly.

Symptoms of early scabies infestation mirror other skin diseases, including dermatitis, syphilis, erythema multiforme, various urticaria-related syndromes, allergic reactions, ringworm-related diseases, and other ectoparasites such as lice and fleas.

Mass-treatment programs that use topical permethrin or oral ivermectin have been effective in reducing the prevalence of scabies in a number of populations. No vaccine is available for scabies. The simultaneous treatment of all close contacts is recommended, even if they show no symptoms of infection (asymptomatic), to reduce rates of recurrence. Since mites can survive for only two to three days without a host, other objects in the environment pose little risk of transmission except in the case of crusted scabies. Therefore cleaning is of little importance. Rooms used by those with crusted scabies require thorough cleaning.It is also recommended that the hosts should bathe everyday,though it makes the itch worse it brings great results when following the doctor`s prescription.

A number of medications are effective in treating scabies. Treatment should involve the entire household, and any others who have had recent, prolonged contact with the infested individual. Options to control itchiness include antihistamines and prescription anti-inflammatory agents. Bedding, clothing and towels used during the previous three days should be washed in hot water and dried in a hot dryer.

Permethrin, a pyrethroid insecticide, is the most effective treatment for scabies, and remains the treatment of choice. It is applied from the neck down, usually before sleep, and left on for about eight to 14 hours, then washed off in the morning. Care should be taken to coat the entire skin surface, not just symptomatic areas; any patch of skin left untreated can provide a "safe haven" for one or more mites to survive. One application is normally sufficient, as permethrin kills eggs and hatchlings, as well as adult mites, though many physicians recommend a second application three to seven days later as a precaution. Crusted scabies may require multiple applications, or supplemental treatment with oral ivermectin (below). Permethrin may cause slight irritation of the skin that is usually tolerable.

Oral ivermectin is effective in eradicating scabies, often in a single dose. It is the treatment of choice for crusted scabies, and is sometimes prescribed in combination with a topical agent. It has not been tested on infants, and is not recommended for children under six years of age.

Topical ivermectin preparations have been shown to be effective for scabies in adults, though only one such formulation is available in the United States at present, and it is not FDA-approved as a scabies treatment. It has also been useful for sarcoptic mange (the veterinary analog of human scabies). conditions.

Other treatments include lindane, benzyl benzoate, crotamiton, malathion, and sulfur preparations. Lindane is effective, but concerns over potential neurotoxicity have limited its availability in many countries. It is banned in California, but may be used in other states as a second-line treatment. Sulfur ointments or benzyl benzoate are often used in the developing world due to their low cost; Some 10% sulfur solutions have been shown to be effective, and sulfur ointments are typically used for at least a week, though many people find the odor of sulfur products unpleasant. Crotamiton has been found to be less effective than permethrin in limited studies. Crotamiton or sulfur preparations are sometimes recommended instead of permethrin for children, due to concerns over dermal absorption of permethrin.

Scabies is endemic in many developing countries, where it tends to be particularly problematic in rural and remote areas. In such settings, community-wide control strategies are required to reduce the rate of disease, as treatment of only individuals is ineffective due to the high rate of reinfection. Large-scale mass drug administration strategies may be required where coordinated interventions aim to treat whole communities in one concerted effort. Although such strategies have shown to be able to reduce the burden of scabies in these kinds of communities, debate remains about the best strategy to adopt, including the choice of drug.

The resources required to implement such large-scale interventions in a cost-effective and sustainable way are significant. Furthermore, since endemic scabies is largely restricted to poor and remote areas, it is a public health issue that has not attracted much attention from policy makers and international donors.

Scabies is one of the three most common skin disorders in children, along with tinea and pyoderma. As of 2010, it affects about 100 million people (1.5% of the population) and its frequency is not related to gender. The mites are distributed around the world and equally infect all ages, races, and socioeconomic classes in different climates. Scabies is more often seen in crowded areas with unhygienic living conditions. Globally as of 2009, an estimated 300 million cases of scabies occur each year, although various parties claim the figure is either over- or underestimated. About 1–10% of the global population is estimated to be infected with scabies, but in certain populations, the infection rate may be as high as 50–80%.

Scabies has been observed in humans since ancient times. Archeological evidence from Egypt and the Middle East suggests scabies was present as early as 494 BC. In the fourth century BC, Aristotle reported on "lice" that "escape from little pimples if they are pricked" – a description consistent with scabies. Arab physician, Ibn Zuhr is believed to have been the first to discover the scabies mites.

The Roman encyclopedist and medical writer Aulus Cornelius Celsus (c. 25 BC – 50 AD) is credited with naming the disease "scabies" and describing its characteristic features. The parasitic etiology of scabies was documented by the Italian physician Giovanni Cosimo Bonomo (1663–1696) in his 1687 letter, "Observations concerning the fleshworms of the human body". Bonomo's description established scabies as one of the first human diseases with a well-understood cause.

In Europe in the late 19th through mid-20th centuries, a sulfur-bearing ointment called by the medical eponym of Wilkinson's ointment was widely used for topical treatment of scabies. The contents and origins of several versions of the ointment were detailed in correspondence published in the "British Medical Journal" in 1945.

The International Alliance for the Control of Scabies was started in 2012, and brings together over 150 researchers, clinicians, and public-health experts from more than 15 different countries. It has managed to bring the global health implications of scabies to the attention of the World Health Organization. Consequently, the WHO has included scabies on its official list of neglected tropical diseases and other neglected conditions.

Scabies may occur in a number of domestic and wild animals; the mites that cause these infestations are of different subspecies from the one typically causing the human form. These subspecies can infest animals that are not their usual hosts, but such infections do not last long. Scabies-infected animals suffer severe itching and secondary skin infections. They often lose weight and become frail.

The most frequently diagnosed form of scabies in domestic animals is sarcoptic mange, caused by the subspecies "Sarcoptes scabiei canis", most commonly in dogs and cats. Sarcoptic mange is transmissible to humans who come into prolonged contact with infested animals, and is distinguished from human scabies by its distribution on skin surfaces covered by clothing. Scabies-infected domestic fowl suffer what is known as "scaly leg". Domestic animals that have gone feral and have no veterinary care are frequently afflicted with scabies and a host of other ailments. Nondomestic animals have also been observed to suffer from scabies. Gorillas, for instance, are known to be susceptible to infection by contact with items used by humans.

Moxidectin is being evaluated as a treatment for scabies. It is established in veterinary medicine to treat a range of parasites, including sarcoptic mange. Its advantage over ivermectin is its longer half life in humans and, thus, potential duration of action. Tea tree oil appears to be effective in the laboratory setting.



</doc>
<doc id="28849" url="https://en.wikipedia.org/wiki?curid=28849" title="Shiva">
Shiva

Shiva (; , , ISO: , ), also known as Mahadeva (), is one of the principal deities of Hinduism. He is the supreme being within Shaivism, one of the major traditions within contemporary Hinduism.

Shiva is known as "The Destroyer" within the Trimurti, the Hindu trinity that includes Brahma and Vishnu. In Shaivism tradition, Shiva is one of the supreme beings who creates, protects and transforms the universe. In the Shaktism tradition, the Goddess, or Devi, is described as one of the supreme, yet Shiva is revered along with Vishnu and Brahma. A goddess is stated to be the energy and creative power (Shakti) of each, with Parvati (Sati) the equal complementary partner of Shiva. He is one of the five equivalent deities in Panchayatana puja of the Smarta tradition of Hinduism.

According to the Shaivism sect, the highest form of Ishvar is formless, limitless, transcendent and unchanging absolute Brahman, and the primal Atman (soul, self) of the universe. There are many both benevolent and fearsome depictions of Shiva. In benevolent aspects, he is depicted as an omniscient Yogi who lives an ascetic life on Mount Kailash as well as a householder with wife Parvati and his two children, Ganesha and Kartikeya. In his fierce aspects, he is often depicted slaying demons. Shiva is also known as Adiyogi Shiva, regarded as the patron god of yoga, meditation and arts.

The iconographical attributes of Shiva are the serpent around his neck, the adorning crescent moon, the holy river Ganga flowing from his matted hair, the third eye on his forehead, the trishula or trident, as his weapon, and the damaru drum. He is usually worshipped in the aniconic form of lingam. Shiva is a pan-Hindu deity, revered widely by Hindus, in India, Nepal and Sri Lanka.

In the earliest of the vedic texts, the word "Shiva" means Auspicious, Sacred. It does not relate to any deity, but refers to the quality of being sacred and auspicious.

In later vedic texts, Shiva becomes a deity. Shiva is also called "Brahman", the supreme universal consciousness. The word "shivo'ham" translates as "I am Shiva", conveying that one's consciousness is where the lord resides, uniting it with the supreme "transcendence". In Tamil, he was called by different names other than Sivan. Nataraja (Dancing form of Shiva), Rudra (Enraged form of Shiva), and Dhakshinamoorthy (Yoga form of Shiva). Nataraja is the only form of Shiva worshipped in a human figure format. Elsewhere he is worshipped in Lingam figure. Pancha Bootha temples are located in south India. Pancha Bhoota Stalam refers to five temples dedicated to Shiva. Tamil literature is enriched by Shiva devotees called 63 Nayanmars (Nayanars).
The Sanskrit word "" (, also transliterated as "shiva") means, states Monier Monier-Williams, "auspicious, propitious, gracious, benign, kind, benevolent, friendly". The roots of in folk etymology are "śī" which means "in whom all things lie, pervasiveness" and "va" which means "embodiment of grace".

The word Shiva is used as an adjective in the Rig Veda (approximately 1700–1100 BC), as an epithet for several Rigvedic deities, including Rudra. The term Shiva also connotes "liberation, final emancipation" and "the auspicious one", this adjective sense of usage is addressed to many deities in Vedic layers of literature. The term evolved from the Vedic "Rudra-Shiva" to the noun "Shiva" in the Epics and the Puranas, as an auspicious deity who is the "creator, reproducer and dissolver".

Sharva, sharabha presents another etymology with the Sanskrit root "-", which means "to injure" or "to kill", interprets the name to connote "one who can kill the forces of darkness".

The Sanskrit word ' means "relating to the god Shiva", and this term is the Sanskrit name both for one of the principal sects of Hinduism and for a member of that sect. It is used as an adjective to characterize certain beliefs and practices, such as Shaivism.

The "Vishnu sahasranama" interprets "Shiva" to have multiple meanings: "The Pure One", and "the One who is not affected by three Guṇas of Prakṛti (Sattva, Rajas, and Tamas)".

Shiva is known by many names such as Viswanatha (lord of the universe), Mahadeva, Mahandeo, Mahasu, Mahesha, Maheshvara, Shankara, Shambhu, Rudra, Hara, Trilochana, Devendra (chief of the gods), Neelakanta, Subhankara, Trilokinatha (lord of the three realms), and Ghrneshwar (lord of compassion). The highest reverence for Shiva in Shaivism is reflected in his epithets ' ("Great god"; ' "Great" and "deva" "god"), ' ("Great Lord"; ' "great" and "" "lord"), and "" ("Supreme Lord").

Sahasranama are medieval Indian texts that list a thousand names derived from aspects and epithets of a deity. There are at least eight different versions of the "Shiva Sahasranama", devotional hymns ("stotras") listing many names of Shiva. The version appearing in Book 13 () of the "Mahabharata" provides one such list. Shiva also has "Dasha-Sahasranamas" (10,000 names) that are found in the "Mahanyasa". The "Shri Rudram Chamakam", also known as the "Śatarudriya", is a devotional hymn to Shiva hailing him by many names.

The Shiva-related tradition is a major part of Hinduism, found all over the Indian subcontinent, such as India, Nepal, Sri Lanka, and Southeast Asia, such as Bali, Indonesia. Scholars have interpreted early prehistoric paintings at the Bhimbetka rock shelters, carbon dated to be from pre-10,000 BCE period, as Shiva dancing, Shiva's trident, and his mount Nandi. Rock paintings from Bhimbetka, depicting a figure with a trishul, have been described as Nataraja by Erwin Neumayer, who dates them to the mesolithic.

Of several Indus valley seals that show animals, one seal that has attracted attention shows a large central figure, either horned or wearing a horned headdress and possibly ithyphallic, seated in a posture reminiscent of the Lotus position, surrounded by animals. This figure was named by early excavators of Mohenjo-daro as "Pashupati" (Lord of Animals, Sanskrit '), an epithet of the later Hindu deities Shiva and Rudra.

Sir John Marshall and others suggested that this figure is a prototype of Shiva, with three faces, seated in a "yoga posture" with the knees out and feet joined. Semi-circular shapes on the head were interpreted as two horns. Scholars such as Gavin Flood, John Keay and Doris Meth Srinivasan have expressed doubts about this suggestion.

Gavin Flood states that it is not clear from the seal that the figure has three faces, is seated in a yoga posture, or even that the shape is intended to represent a human figure. He characterizes these views as "speculative", but adds that it is nevertheless possible that there are echoes of Shaiva iconographic themes, such as half-moon shapes resembling the horns of a bull. John Keay writes that "he may indeed be an early manifestation of Lord Shiva as Pashu-pati", but a couple of his specialties of this figure does not match with Rudra. Writing in 1997, Srinivasan interprets what John Marshall interpreted as facial as not human but more bovine, possibly a divine buffalo-man.

The interpretation of the seal continues to be disputed. McEvilley, for example, states that it is not possible to "account for this posture outside the yogic account". Asko Parpola states that other archaeological finds such as the early Elamite seals dated to 3000-2750 BCE show similar figures and these have been interpreted as "seated bull" and not a yogi, and the bovine interpretation is likely more accurate. Gregory L. Possehl in 2002, associated it with the water buffalo, and concluded that while it would be appropriate to recognize the figure as a deity, and its posture as one of ritual discipline, regarding it as a proto-Shiva would "go too far".

The Vedic literature refers to a minor atmospheric deity, with fearsome powers called Rudra. The Rigveda, for example, has 3 out of 1,028 hymns dedicated to Rudra, and he finds occasional mention in other hymns of the same text. The term Shiva also appears in the Rigveda, but simply as an epithet, that means "kind, auspicious", one of the adjectives used to describe many different Vedic deities. While fierce ruthless natural phenomenon and storm-related Rudra is feared in the hymns of the Rigveda, the beneficial rains he brings are welcomed as Shiva aspect of him. This healing, nurturing, life-enabling aspect emerges in the Vedas as Rudra-Shiva, and in post-Vedic literature ultimately as Shiva who combines the destructive and constructive powers, the terrific and the gentle, as the ultimate recycler and rejuvenator of all existence.

The similarities between the iconography and theologies of Shiva with Greek and European deities have led to proposals for an Indo-European link for Shiva, or lateral exchanges with ancient central Asian cultures. His contrasting aspects such as being terrifying or blissful depending on the situation, are similar to those of the Greek god Dionysus, as are their iconic associations with bull, snakes, anger, bravery, dancing and carefree life. The ancient Greek texts of the time of Alexander the Great call Shiva as "Indian Dionysus", or alternatively call Dionysus as ""god of the Orient"". Similarly, the use of phallic symbol as an icon for Shiva is also found for Irish, Nordic, Greek (Dionysus) and Roman deities, as was the idea of this aniconic column linking heaven and earth among early Indo-Aryans, states Roger Woodward. Others contest such proposals, and suggest Shiva to have emerged from indigenous pre-Aryan tribal origins.

Shiva as we know him today shares many features with the Vedic god Rudra, and both Shiva and Rudra are viewed as the same personality in Hindu scriptures. The two names are used synonymously. Rudra, the god of the roaring storm, is usually portrayed in accordance with the element he represents as a fierce, destructive deity.

The oldest surviving text of Hinduism is the Rig Veda, which is dated to between 1700 and 1100 BC based on linguistic and philological evidence. A god named Rudra is mentioned in the Rig Veda. The name Rudra is still used as a name for Shiva. In RV 2.33, he is described as the "Father of the Rudras", a group of storm gods.

The hymn 10.92 of the Rigveda states that deity Rudra has two natures, one wild and cruel (Rudra), another that is kind and tranquil (Shiva). The Vedic texts do not mention bull or any animal as the transport vehicle ("vahana") of Rudra or other deities. However, post-Vedic texts such as the Mahabharata and the Puranas state the Nandi bull, the Indian zebu, in particular, as the vehicle of Rudra and of Shiva, thereby unmistakably linking them as same.

Rudra and Agni have a close relationship. The identification between Agni and Rudra in the Vedic literature was an important factor in the process of Rudra's gradual development into the later character as Rudra-Shiva. The identification of Agni with Rudra is explicitly noted in the "Nirukta", an important early text on etymology, which says, "Agni is also called Rudra." The interconnections between the two deities are complex, and according to Stella Kramrisch:
In the "Śatarudrīya", some epithets of Rudra, such as ("Of golden red hue as of flame") and ("Flaming bright"), suggest a fusing of the two deities. Agni is said to be a bull, and Lord Shiva possesses a bull as his vehicle, Nandi. The horns of Agni, who is sometimes characterized as a bull, are mentioned. In medieval sculpture, both Agni and the form of Shiva known as Bhairava have flaming hair as a special feature.

According to Wendy Doniger, the Puranic Shiva is a continuation of the Vedic Indra. Doniger gives several reasons for her hypothesis. Both are associated with mountains, rivers, male fertility, fierceness, fearlessness, warfare, the transgression of established mores, the Aum sound, the Supreme Self. In the Rig Veda the term "" is used to refer to Indra. (2.20.3, 6.45.17, and 8.93.3.) Indra, like Shiva, is likened to a bull. In the Rig Veda, Rudra is the father of the Maruts, but he is never associated with their warlike exploits as is Indra.

The Vedic beliefs and practices of the pre-classical era were closely related to the hypothesised Proto-Indo-European religion, and the pre-Islamic Indo-Iranian religion. The earliest iconic artworks of Shiva may be from Gandhara and northwest parts of ancient India. There is some uncertainty as the artwork that has survived is damaged and they show some overlap with meditative Buddha-related artwork, but the presence of Shiva's trident and phallic symbolism in this art suggests it was likely Shiva. Numismatics research suggests that numerous coins of the ancient Kushan Empire that have survived, were images of a god who is probably Shiva. The Shiva in Kushan coins is referred to as Oesho of unclear etymology and origins, but the simultaneous presence of Indra and Shiva in the Kushan era artwork suggest that they were revered deities by the start of the Kushan Empire.

The texts and artwork of Jainism show Indra as a dancer, although not identical generally resembling the dancing Shiva artwork found in Hinduism, particularly in their respective mudras. For example, in the Jain caves at Ellora, extensive carvings show dancing Indra next to the images of Tirthankaras in a manner similar to Shiva Nataraja. The similarities in the dance iconography suggests that there may be a link between ancient Indra and Shiva.

Rudra's evolution from a minor Vedic deity to a supreme being is first evidenced in the "Shvetashvatara Upanishad" (400–200 BC), according to Gavin Flood. Prior to it, the Upanishadic literature is monistic, and the "Shvetashvatara" text presents the earliest seeds of theistic devotion to Rudra-Shiva. Here Rudra-Shiva is identified as the creator of the cosmos and liberator of souls from the birth-rebirth cycle. The period of 200 BC to 100 AD also marks the beginning of the Shaiva tradition focused on the worship of Shiva as evidenced in other literature of this period. Shaiva devotees and ascetics are mentioned in Patanjali's "Mahābhāṣya" (2nd-century BC) and in the "Mahabharata". Other scholars such as Robert Hume and Doris Srinivasan state that the "Shvetashvatara Upanishad" presents pluralism, pantheism, or henotheism, rather than being a text just on Shiva theism.
The Shaiva Upanishads are a group of 14 minor Upanishads of Hinduism variously dated from the last centuries of the 1st millennium BCE through the 17th century. These extol Shiva as the metaphysical unchanging reality Brahman and the Atman (soul, self), and include sections about rites and symbolisms related to Shiva.

A few texts such as "Atharvashiras Upanishad" mention Rudra, and assert all gods are Rudra, everyone and everything is Rudra, and Rudra is the principle found in all things, their highest goal, the innermost essence of all reality that is visible or invisible. The "Kaivalya Upanishad" similarly, states Paul Deussen – a German Indologist and professor of Philosophy, describes the self-realized man as who "feels himself only as the one divine essence that lives in all", who feels identity of his and everyone's consciousness with Shiva (highest Atman), who has found this highest Atman within, in the depths of his heart.

The Shaiva Puranas, particularly the Shiva Purana and the Linga Purana, present the various aspects of Shiva, mythologies, cosmology and pilgrimage ("Tirtha") associated with him. The Shiva-related Tantra literature, composed between the 8th and 11th centuries, are regarded in devotional dualistic Shaivism as Sruti. Dualistic Shaiva Agamas which consider soul within each living being and Shiva as two separate realities (dualism, "dvaita"), are the foundational texts for Shaiva Siddhanta. Other Shaiva Agamas teach that these are one reality (monism, "advaita"), and that Shiva is the soul, the perfection and truth within each living being. In Shiva related sub-traditions, there are ten dualistic Agama texts, eighteen qualified monism-cum-dualism Agama texts and sixty-four monism Agama texts.

Shiva-related literature developed extensively across India in the 1st millennium CE and through the 13th century, particularly in Kashmir and Tamil Shaiva traditions. The monist Shiva literature posit absolute oneness, that is Shiva is within every man and woman, Shiva is within every living being, Shiva is present everywhere in the world including all non-living being, and there is no spiritual difference between life, matter, man and Shiva. The various dualistic and monist Shiva-related ideas were welcomed in medieval southeast Asia, inspiring numerous Shiva-related temples, artwork and texts in Indonesia, Myanmar, Cambodia, Laos, Vietnam, Thailand and Malaysia, with syncretic integration of local pre-existing theologies.

The figure of Shiva as we know him today may be an amalgamation of various older deities into a single figure. How the persona of Shiva converged as a composite deity is not understood, a challenge to trace and has attracted much speculation. According to Vijay Nath, for example: 
An example of assimilation took place in Maharashtra, where a regional deity named Khandoba is a patron deity of farming and herding castes. The foremost center of worship of Khandoba in Maharashtra is in Jejuri. Khandoba has been assimilated as a form of Shiva himself, in which case he is worshipped in the form of a lingam. Khandoba's varied associations also include an identification with Surya and Karttikeya.

Shaivism is one of the four major sects of Hinduism, the others being Vaishnavism, Shaktism and the Smarta Tradition. Followers of Shaivism, called "Shaivas", revere Shiva as the Supreme Being. Shaivas believe that Shiva is All and in all, the creator, preserver, destroyer, revealer and concealer of all that is. He is not only the creator in Shaivism, but he is also the creation that results from him, he is everything and everywhere. Shiva is the primal soul, the pure consciousness and Absolute Reality in the Shaiva traditions.

The Shaivism theology is broadly grouped into two: the popular theology influenced by Shiva-Rudra in the Vedas, Epics and the Puranas; and the esoteric theology influenced by the Shiva and Shakti-related Tantra texts. The Vedic-Brahmanic Shiva theology includes both monist ("Advaita") and devotional traditions ("Dvaita") such as Tamil Shaiva Siddhanta and Lingayatism with temples featuring items such as linga, Shiva-Parvati iconography, bull Nandi within the premises, relief artwork showing mythologies and aspects of Shiva.

The Tantric Shiva tradition ignored the mythologies and Puranas related to Shiva, and depending on the sub-school developed a variety of practices. For example, historical records suggest the tantric Kapalikas (literally, the 'skull-men') co-existed with and shared many Vajrayana Buddhist rituals, engaged in esoteric practices that revered Shiva and Shakti wearing skulls, begged with empty skulls, used meat, alcohol, and sexuality as a part of ritual. In contrast, the esoteric tradition within Kashmir Shaivism has featured the "Krama" and "Trika" sub-traditions. The Krama sub-tradition focussed on esoteric rituals around Shiva-Kali pair. The Trika sub-tradition developed a theology of triads involving Shiva, combined it with an ascetic lifestyle focusing on personal Shiva in the pursuit of monistic self liberation.

The Vaishnava (Vishnu-oriented) literature acknowledges and discusses Shiva. Like Shaiva literature that presents Shiva as supreme, the Vaishnava literature presents Vishnu as supreme. However, both traditions are pluralistic and revere both Shiva and Vishnu (along with Devi), their texts do not show exclusivism, and Vaishnava texts such as the "Bhagavata Purana" while praising Krishna as the Ultimate Reality, also present Shiva and Shakti as a personalized form an equivalent to the same Ultimate Reality. The texts of Shaivism tradition similarly praise Vishnu. The Skanda Purana, for example, states:

Mythologies of both traditions include legends about who is superior, about Shiva paying homage to Vishnu, and Vishnu paying homage to Shiva. However, in texts and artwork of either tradition, the mutual salutes are symbolism for complementarity. The Mahabharata declares the unchanging Ultimate Reality (Brahman) to be identical to Shiva and to Vishnu, that Vishnu is the highest manifestation of Shiva, and Shiva is the highest manifestation of Vishnu.

The goddess-oriented Shakti tradition of Hinduism is based on the premise that the Supreme Principle and the Ultimate Reality called Brahman is female (Devi), but it treats the male as her equal and complementary partner. This partner is Shiva.

The earliest evidence of the tradition of reverence for the feminine with Rudra-Shiva context, is found in the Hindu scripture "Rigveda", in a hymn called the Devi Sukta:

The "Devi Upanishad" in its explanation of the theology of Shaktism, mentions and praises Shiva such as in its verse 19. Shiva, along with Vishnu, is a revered god in the "Devi Mahatmya", a text of Shaktism considered by the tradition to be as important as the "Bhagavad Gita". The Ardhanarisvara concept co-mingles god Shiva and goddess Shakti by presenting an icon that is half-man and half woman, a representation and theme of union found in many Hindu texts and temples.

In the Smarta tradition of Hinduism, Shiva is a part of its Panchayatana puja. This practice consists of the use of icons or anicons of five deities considered equivalent, set in a quincunx pattern. Shiva is one of the five deities, others being Vishnu, Devi (such as Parvati), Surya and Ganesha or Skanda or any personal god of devotee's preference (Ishta Devata).

Philosophically, the Smarta tradition emphasizes that all idols (murti) are icons to help focus on and visualize aspects of Brahman, rather than distinct beings. The ultimate goal in this practice is to transition past the use of icons, recognize the Absolute symbolized by the icons, on the path to realizing the nondual identity of one's Atman (soul, self) and the Brahman. Popularized by Adi Shankara, many Panchayatana mandalas and temples have been uncovered that are from the Gupta Empire period, and one Panchayatana set from the village of Nand (about 24 kilometers from Ajmer) has been dated to belong to the Kushan Empire era (pre-300 CE). The Kushan period set includes Shiva, Vishnu, Surya, Brahma and one deity whose identity is unclear.

Shiva is considered the Great Yogi who is totally absorbed in himself – the transcendental reality. He is the Lord of Yogis, and the teacher of Yoga to sages. As Shiva Dakshinamurthi, states Stella Kramrisch, he is the supreme guru who "teaches in silence the oneness of one's innermost self ("atman") with the ultimate reality ("brahman")."

The theory and practice of Yoga, in different styles, has been a part of all major traditions of Hinduism, and Shiva has been the patron or spokesperson in numerous Hindu Yoga texts. These contain the philosophy and techniques for Yoga. These ideas are estimated to be from or after the late centuries of the 1st millennium CE, and have survived as Yoga texts such as the "Isvara Gita" (literally, 'Shiva's song'), which Andrew Nicholson – a professor of Hinduism and Indian Intellectual History – states have had "a profound and lasting influence on the development of Hinduism".

Other famed Shiva-related texts influenced Hatha Yoga, integrated monistic ("Advaita Vedanta") ideas with Yoga philosophy and inspired the theoretical development of Indian classical dance. These include the "Shiva Sutras", the "Shiva Samhita", and those by the scholars of Kashmir Shaivism such as the 10th-century scholar Abhinavagupta. Abhinavagupta writes in his notes on the relevance of ideas related to Shiva and Yoga, by stating that "people, occupied as they are with their own affairs, normally do nothing for others", and Shiva and Yoga spirituality helps one look beyond, understand interconnectedness, and thus benefit both the individual and the world towards a more blissful state of existence.

The Trimurti is a concept in Hinduism in which the cosmic functions of creation, maintenance, and destruction are personified by the forms of Brahma the creator, Vishnu the maintainer or preserver and Shiva the destroyer or transformer. These three deities have been called "the Hindu triad" or the "Great Trinity". However, the ancient and medieval texts of Hinduism feature many triads of gods and goddesses, some of which do not include Shiva.


According to Gavin Flood, "Shiva is a god of ambiguity and paradox," whose attributes include opposing themes. The ambivalent nature of this deity is apparent in some of his names and the stories told about him.

In Yajurveda, two contrary sets of attributes for both malignant or terrifying (Sanskrit: ') and benign or auspicious (Sanskrit: ') forms can be found, leading Chakravarti to conclude that "all the basic elements which created the complex Rudra-Śiva sect of later ages are to be found here". In the Mahabharata, Shiva is depicted as "the standard of invincibility, might, and terror", as well as a figure of honor, delight, and brilliance.

The duality of Shiva's fearful and auspicious attributes appears in contrasted names. The name Rudra reflects Shiva's fearsome aspects. According to traditional etymologies, the Sanskrit name "Rudra" is derived from the root "rud-", which means "to cry, howl". Stella Kramrisch notes a different etymology connected with the adjectival form "raudra", which means "wild, of "rudra" nature", and translates the name "Rudra" as "the wild one" or "the fierce god". R. K. Sharma follows this alternate etymology and translates the name as "terrible". Hara is an important name that occurs three times in the Anushasanaparvan version of the "Shiva sahasranama", where it is translated in different ways each time it occurs, following a commentorial tradition of not repeating an interpretation. Sharma translates the three as "one who captivates", "one who consolidates", and "one who destroys". Kramrisch translates it as "the ravisher". Another of Shiva's fearsome forms is as "time" and "great time", which ultimately destroys all things. The name appears in the "Shiva Sahasranama", where it is translated by Ram Karan Sharma as "(the Supreme Lord of) Time". Bhairava "terrible" or "frightful" is a fierce form associated with annihilation. In contrast, the name , "beneficent" or "conferring happiness" reflects his benign form. This name was adopted by the great Vedanta philosopher Adi Shankara (c. 788–820), who is also known as Shankaracharya. The name (Sanskrit: swam-on its own; bhu-burn/shine) "self-shining/ shining on its own", also reflects this benign aspect.

Shiva is depicted as both an ascetic yogi and as a householder (grihasta), roles which have been traditionally mutually exclusive in Hindu society. When depicted as a yogi, he may be shown sitting and meditating. His epithet Mahāyogi ("the great Yogi: "" = "great", "Yogi" = "one who practices Yoga") refers to his association with yoga. While Vedic religion was conceived mainly in terms of sacrifice, it was during the Epic period that the concepts of tapas, yoga, and asceticism became more important, and the depiction of Shiva as an ascetic sitting in philosophical isolation reflects these later concepts.

As a family man and householder, he has a wife, Parvati and two sons, Ganesha and Kartikeya. His epithet ("The husband of ") refers to this idea, and Sharma notes that two other variants of this name that mean the same thing, and , also appear in the "sahasranama". in epic literature is known by many names, including the benign . She is identified with Devi, the Divine Mother; Shakti (divine energy) as well as goddesses like Tripura Sundari, Durga, Kali, Kamakshi and Minakshi. The consorts of Shiva are the source of his creative energy. They represent the dynamic extension of Shiva onto this universe. His son Ganesha is worshipped throughout India and Nepal as the Remover of Obstacles, Lord of Beginnings and Lord of Obstacles. Kartikeya is worshipped in South India (especially in Tamil Nadu, Kerala and Karnataka) by the names Subrahmanya, Subrahmanyan, Shanmughan, Swaminathan and Murugan, and in Northern India by the names Skanda, Kumara, or Karttikeya.

Some regional deities are also identified as Shiva's children. As one story goes, Shiva is enticed by the beauty and charm of Mohini, Vishnu's female avatar, and procreates with her. As a result of this union, Shasta – identified with regional deities Ayyappan and Aiyanar – is born. In outskirts of Ernakulam in Kerala, a deity named Vishnumaya is stated to be offspring of Shiva and invoked in local exorcism rites, but this deity is not traceable in Hindu pantheon and is possibly a local tradition with "vaguely Chinese" style rituals, states Saletore. In some traditions, Shiva has daughters like the serpent-goddess Manasa and Ashokasundari. According to Doniger, two regional stories depict demons Andhaka and Jalandhara as the children of Shiva who war with him, and are later destroyed by Shiva.

The depiction of Shiva as Nataraja (Sanskrit: ', "Lord of Dance") is popular. The names Nartaka ("dancer") and Nityanarta ("eternal dancer") appear in the Shiva Sahasranama. His association with dance and also with music is prominent in the Puranic period. In addition to the specific iconographic form known as Nataraja, various other types of dancing forms (Sanskrit: ') are found in all parts of India, with many well-defined varieties in Tamil Nadu in particular. The two most common forms of the dance are the Tandava, which later came to denote the powerful and masculine dance as Kala-Mahakala associated with the destruction of the world. When it requires the world or universe to be destroyed, Shiva does it by the Tandava, and Lasya, which is graceful and delicate and expresses emotions on a gentle level and is considered the feminine dance attributed to the goddess Parvati. "Lasya" is regarded as the female counterpart of "Tandava". The "Tandava"-"Lasya" dances are associated with the destruction-creation of the world.

Dakshinamurthy "()" literally describes a form (') of Shiva facing south ('). This form represents Shiva in his aspect as a teacher of yoga, music, and wisdom and giving exposition on the "shastras". This iconographic form for depicting Shiva in Indian art is mostly from Tamil Nadu. Elements of this motif can include Shiva seated upon a deer-throne and surrounded by sages who are receiving his instruction.

An iconographic representation of Shiva called Ardhanarishvara ("") shows him with one half of the body as male and the other half as female. According to Ellen Goldberg, the traditional Sanskrit name for this form is best translated as "the lord who is half woman", not as "half-man, half-woman".

Shiva is often depicted as an archer in the act of destroying the triple fortresses, "Tripura", of the Asuras. Shiva's name Tripurantaka (""), "ender of Tripura", refers to this important story.

Apart from anthropomorphic images of Shiva, he is also represented in aniconic form of a lingam. These are depicted in various designs. One common form is the shape of a vertical rounded column in the centre of a lipped, disk-shaped object, the "yoni", symbolism for the goddess Shakti. In Shiva temples, the "linga" is typically present in its sanctum sanctorum and is the focus of votary offerings such as milk, water, flower petals, fruit, fresh leaves, and rice. According to Monier Williams and Yudit Greenberg, "linga" literally means 'mark, sign or emblem', and also refers to a "mark or sign from which the existence of something else can be reliably inferred". It implies the regenerative divine energy innate in nature, symbolized by Shiva. Some scholars, such as Wendy Doniger, view "linga" merely as an erotic phallic symbol, although this interpretation is criticized by others, including Swami Vivekananda, Sivananda Saraswati, and S. N. Balagangadhara. According to Moriz Winternitz, the "linga" in the Shiva tradition is "only a symbol of the productive and creative principle of nature as embodied in Shiva", and it has no historical trace in any obscene phallic cult.

The worship of the lingam originated from the famous hymn in the "Atharva-Veda Samhitâ" sung in praise of the "Yupa-Stambha", the sacrificial post. In that hymn, a description is found of the beginningless and endless "Stambha" or "Skambha", and it is shown that the said "Skambha" is put in place of the eternal Brahman. Just as the Yajna (sacrificial) fire, its smoke, ashes, and flames, the "Soma" plant, and the ox that used to carry on its back the wood for the Vedic sacrifice gave place to the conceptions of the brightness of Shiva's body, his tawny matted hair, his blue throat, and the riding on the bull of the Shiva, the "Yupa-Skambha" gave place in time to the "Shiva-Linga". In the text "Linga Purana", the same hymn is expanded in the shape of stories, meant to establish the glory of the great Stambha and the superiority of Shiva as Mahadeva.

The oldest known archaeological "linga" as an icon of Shiva is the Gudimallam lingam from 3rd-century BCE. In Shaivism pilgrimage tradition, twelve major temples of Shiva are called Jyotirlinga, which means "linga of light", and these are located across India.

Five is a sacred number for Shiva. One of his most important mantras has five syllables ().

Shiva's body is said to consist of five mantras, called the . As forms of God, each of these have their own names and distinct iconography:

These are represented as the five faces of Shiva and are associated in various texts with the five elements, the five senses, the five organs of perception, and the five organs of action. Doctrinal differences and, possibly, errors in transmission, have resulted in some differences between texts in details of how these five forms are linked with various attributes. The overall meaning of these associations is summarized by Stella Kramrisch:
According to the "Pañcabrahma Upanishad":
Puranic scriptures contain occasional references to "ansh" – literally 'portion, or avatars of Shiva', but the idea of Shiva avatars is not universally accepted in Saivism. The Linga Purana mentions twenty-eight forms of Shiva which are sometimes seen as avatars, however such mention is unusual and the avatars of Shiva is relatively rare in Shaivism compared to the well emphasized concept of Vishnu avatars in Vaishnavism.
Some Vaishnava literature reverentially link Shiva to characters in its mythologies. For example, in the "Hanuman Chalisa", Hanuman is identified as the eleventh avatar of Shiva. The "Bhagavata Purana" and the "Vishnu Purana" claim sage Durvasa to be a portion of Shiva. Some medieval era writers have called the Advaita Vedanta philosopher Adi Shankara an incarnation of Shiva.

There is a "Shivaratri" in every lunar month on its 13th night/14th day, but once a year in late winter (February/March) and before the arrival of spring, marks "Maha Shivaratri" which means "the Great Night of Shiva".

Maha Shivaratri is a major Hindu festival, but one that is solemn and theologically marks a remembrance of "overcoming darkness and ignorance" in life and the world, and meditation about the polarities of existence, of Shiva and a devotion to humankind. It is observed by reciting Shiva-related poems, chanting prayers, remembering Shiva, fasting, doing Yoga and meditating on ethics and virtues such as self-restraint, honesty, noninjury to others, forgiveness, introspection, self-repentance and the discovery of Shiva. The ardent devotees keep awake all night. Others visit one of the Shiva temples or go on pilgrimage to Jyotirlingam shrines. Those who visit temples, offer milk, fruits, flowers, fresh leaves and sweets to the lingam. Some communities organize special dance events, to mark Shiva as the lord of dance, with individual and group performances. According to Jones and Ryan, Maha Sivaratri is an ancient Hindu festival which probably originated around the 5th-century.

Another major festival involving Shiva worship is Kartik Purnima, commemorating Shiva's victory on the demons Tripurasura. Across India, various Shiva temples are illuminated throughout the night. Shiva icons are carried in procession in some places.

Thiruvathira is a festival observed in Kerala dedicated to Shiva. It is believed that on this day, Parvathi met Lord Shiva after her long penance and Lord Shiva took
her as his wife. On this day Hindu women performs the Thiruvathirakali accompanied by Thiruvathira paattu (folk songs about Parvati and her longing and penance for Lord Shiva's affection).

Regional festivals dedicated to Shiva include the Chittirai festival in Madurai around April/May, one of the largest festivals in South India, celebrating the wedding of Minakshi (Parvati) and Shiva. The festival is one where both the Vaishnava and Shaiva communities join the celebrations, because Vishnu gives away his sister Minakshi in marriage to Shiva.

Some Shaktism-related festivals revere Shiva along with the goddess considered primary and Supreme. These include festivals dedicated to Annapurna such as "Annakuta" and those related to Durga. In Himalayan regions such as Nepal, as well as in northern, central and western India, the festival of Teej is celebrated by girls and women in the monsoon season, in honor of goddess Parvati, with group singing, dancing and by offering prayers in Parvati-Shiva temples.

The ascetic, Vedic and Tantric sub-traditions related to Shiva, such as those that became ascetic warriors during the Islamic rule period of India, celebrate the Kumbha Mela festival. This festival cycles every 12 years, in four pilgrimage sites within India, with the event moving to the next site after a gap of three years. The biggest is in Prayaga (renamed Allahabad during the Mughal rule era), where millions of Hindus of different traditions gather at the confluence of rivers Ganges and Yamuna. In the Hindu tradition, the Shiva-linked ascetic warriors ("Nagas") get the honor of starting the event by entering the "Sangam" first for bathing and prayers.

In Shaivism of Indonesia, the popular name for Shiva has been "Batara Guru", which is derived from Sanskrit "Bhattaraka" which means "noble lord". He is conceptualized as a kind spiritual teacher, the first of all Gurus in Indonesian Hindu texts, mirroring the Dakshinamurti aspect of Shiva in the Indian subcontinent. However, the Batara Guru has more aspects than the Indian Shiva, as the Indonesian Hindus blended their spirits and heroes with him. Batara Guru's wife in southeast Asia is the same Hindu deity Durga, who has been popular since ancient times, and she too has a complex character with benevolent and fierce manifestations, each visualized with different names such as Uma, Sri, Kali and others. Shiva has been called Sadasiva, Paramasiva, Mahadeva in benevolent forms, and Kala, Bhairava, Mahakala in his fierce forms. The Indonesian Hindu texts present the same philosophical diversity of Shaivism traditions found on the subcontinent. However, among the texts that have survived into the contemporary era, the more common are of those of Shaiva Siddhanta (locally also called Siwa Siddhanta, Sridanta).

In the pre-Islamic period on the island of Java, Shaivism and Buddhism were considered very close and allied religions, though not identical religions. The medieval era Indonesian literature equates Buddha with Siwa (Shiva) and Janardana (Vishnu). This tradition continues in predominantly Hindu Bali Indonesia in the modern era, where Buddha is considered the younger brother of Shiva.

The worship of Shiva became popular in Central Asia through the Hephthalite Empire, and Kushan Empire. Shaivism was also popular in Sogdia and the Kingdom of Yutian as found from the wall painting from Penjikent on the river Zervashan. In this depiction, Shiva is portrayed with a sacred halo and a sacred thread ("Yajnopavita"). He is clad in tiger skin while his attendants are wearing Sogdian dress. A panel from Dandan Oilik shows Shiva in His Trimurti form with Shakti kneeling on her right thigh. Another site in the Taklamakan Desert depicts him with four legs, seated cross-legged on a cushioned seat supported by two bulls. It is also noted that Zoroastrian wind god Vayu-Vata took on the iconographic appearance of Shiva.

Daikokuten, one of the Seven Lucky Gods in Japan, is considered to be evolved from Shiva. The god enjoys an exalted position as a household deity in Japan and is worshipped as the god of wealth and fortune. The name is the Japanese equivalent of Mahākāla, the Buddhist name for Shiva. Shiva is also mentioned in Buddhist Tantra. Shiva as "Upaya" and Shakti as "Prajna". In cosmologies of Buddhist tantra, Shiva is depicted as passive, with Shakti being his active counterpart. In Mahayana Buddhist cosmology, Shiva resides in Akaniṣṭha, highest of Śuddhāvāsa (Pure Abodes) where Anāgāmi ("Non-returners") who are already on the path to Arhat-hood and who will attain enlightenment are born in.

The Japuji Sahib of the Guru Granth Sahib says, "The Guru is Shiva, the Guru is Vishnu and Brahma; the Guru is Paarvati and Lakhshmi." In the same chapter, it also says, "Shiva speaks, and the Siddhas listen." In Dasam Granth, Guru Gobind Singh has mentioned two avtars of Rudra: Dattatreya Avtar and Parasnath Avtar.

In contemporary culture, Shiva is depicted in films, books, tattoos and art. He has been referred to as "the god of cool things" and a "bonafide rock hero".

Popular films include the Gujarati language movie "Har Har Mahadev", the Kannada movie "Gange Gowri" and well-known books include Amish Tripathi's "Shiva Trilogy", which has sold over a million copies. On television, "Devon Ke Dev...Mahadev", a television serial about Lord Shiva on the Life OK channel was among the most watched shows at its peak popularity. A 90's television series of DD National titled Om Namah Shivay was also based on legends of Shiva.

In the "Final Fantasy" videogame series, Shiva is often depicted as a benevolent ancient being of Ice Element who frequently aids the heroes against mighty foes (via summoning). Shiva is also a character in the video game "Dark Souls", with the name Shiva of the East.

 


</doc>
<doc id="28851" url="https://en.wikipedia.org/wiki?curid=28851" title="Sap beetle">
Sap beetle

The sap beetles, also known as Nitidulidae, are a family of beetles.

They are small (2–6 mm) ovoid, usually dull-coloured beetles, with knobbed antennae. Some have red or yellow spots or bands. They feed mainly on decaying vegetable matter, over-ripe fruit, and sap. There are a few pest species. An example of a pest species is the strawberry sap beetle that infest crops in Brazil between the months of August and February. 

The family includes these genera:



</doc>
<doc id="28852" url="https://en.wikipedia.org/wiki?curid=28852" title="Syphilis">
Syphilis

Syphilis is a sexually transmitted infection caused by the bacterium "Treponema pallidum" subspecies "pallidum". The signs and symptoms of syphilis vary depending in which of the four stages it presents (primary, secondary, latent, and tertiary). The primary stage classically presents with a single chancre (a firm, painless, non-itchy skin ulceration usually between 1 cm and 2 cm in diameter) though there may be multiple sores. In secondary syphilis, a diffuse rash occurs, which frequently involves the palms of the hands and soles of the feet. There may also be sores in the mouth or vagina. In latent syphilis, which can last for years, there are few or no symptoms. In tertiary syphilis, there are gummas (soft, non-cancerous growths), neurological problems, or heart symptoms. Syphilis has been known as "the great imitator" as it may cause symptoms similar to many other diseases.
Syphilis is most commonly spread through sexual activity. It may also be transmitted from mother to baby during pregnancy or at birth, resulting in congenital syphilis. Other diseases caused by the "Treponema" bacteria include yaws (subspecies "pertenue"), pinta (subspecies "carateum"), and nonvenereal endemic syphilis (subspecies "endemicum"). These three diseases are not typically sexually transmitted. Diagnosis is usually made by using blood tests; the bacteria can also be detected using dark field microscopy. The Centers for Disease Control and Prevention (U.S.) recommend all pregnant women be tested.
The risk of sexual transmission of syphilis can be reduced by using a latex or polyurethane condom. Syphilis can be effectively treated with antibiotics. The preferred antibiotic for most cases is benzathine benzylpenicillin injected into a muscle. In those who have a severe penicillin allergy, doxycycline or tetracycline may be used. In those with neurosyphilis, intravenous benzylpenicillin or ceftriaxone is recommended. During treatment people may develop fever, headache, and muscle pains, a reaction known as Jarisch–Herxheimer.
In 2015, about 45.4 million people were infected with syphilis, with six million new cases. During 2015, it caused about 107,000 deaths, down from 202,000 in 1990. After decreasing dramatically with the availability of penicillin in the 1940s, rates of infection have increased since the turn of the millennium in many countries, often in combination with human immunodeficiency virus (HIV). This is believed to be partly due to increased promiscuity, prostitution, decreasing use of condoms, and unsafe sexual practices among men who have sex with men.
Syphilis can present in one of four different stages: primary, secondary, latent, and tertiary, and may also occur congenitally. It was referred to as "the great imitator" by Sir William Osler due to its varied presentations.

Primary syphilis is typically acquired by direct sexual contact with the infectious lesions of another person. Approximately 2–6 weeks after contact (with a range of 10–90 days) a skin lesion, called a chancre, appears at the site and this contains infectious spirochetes. This is classically (40% of the time) a single, firm, painless, non-itchy skin ulceration with a clean base and sharp borders approximately 0.3–3.0 cm in size. The lesion may take on almost any form. In the classic form, it evolves from a macule to a papule and finally to an erosion or ulcer. Occasionally, multiple lesions may be present (~40%), with multiple lesions being more common when coinfected with HIV. Lesions may be painful or tender (30%), and they may occur in places other than the genitals (2–7%). The most common location in women is the cervix (44%), the penis in heterosexual men (99%), and anally and rectally in men who have sex with men (34%). Lymph node enlargement frequently (80%) occurs around the area of infection, occurring seven to 10 days after chancre formation. The lesion may persist for three to six weeks if left untreated.

Secondary syphilis occurs approximately four to ten weeks after the primary infection. While secondary disease is known for the many different ways it can manifest, symptoms most commonly involve the skin, mucous membranes, and lymph nodes. There may be a symmetrical, reddish-pink, non-itchy rash on the trunk and extremities, including the palms and soles. The rash may become maculopapular or pustular. It may form flat, broad, whitish, wart-like lesions on mucous membranes, known as condyloma latum. All of these lesions harbor bacteria and are infectious. Other symptoms may include fever, sore throat, malaise, weight loss, hair loss, and headache. Rare manifestations include liver inflammation, kidney disease, joint inflammation, periostitis, inflammation of the optic nerve, uveitis, and interstitial keratitis. The acute symptoms usually resolve after three to six weeks; about 25% of people may present with a recurrence of secondary symptoms. Many people who present with secondary syphilis (40–85% of women, 20–65% of men) do not report previously having had the classical chancre of primary syphilis.

Latent syphilis is defined as having serologic proof of infection without symptoms of disease. It develops after secondary syphilis and is divided into early latent and late latent stages. Early latent syphilis is defined by the World Health Organization as less than 2 years after original infection. Early latent syphilis is infectious as up to 25% of people can develop a recurrent secondary infection (during which spirochetes are actively replicating and are infectious). Two years after the original infection the person will enter late latent syphilis and is not as infectious as the early phase. The latent phase of syphilis can last many years after which, without treatment, approximately 15-40% of people can develop tertiary syphilis.

Tertiary syphilis may occur approximately 3 to 15 years after the initial infection, and may be divided into three different forms: gummatous syphilis (15%), late neurosyphilis (6.5%), and cardiovascular syphilis (10%). Without treatment, a third of infected people develop tertiary disease. People with tertiary syphilis are not infectious.

Gummatous syphilis or late benign syphilis usually occurs 1 to 46 years after the initial infection, with an average of 15 years. This stage is characterized by the formation of chronic gummas, which are soft, tumor-like balls of inflammation which may vary considerably in size. They typically affect the skin, bone, and liver, but can occur anywhere.

Cardiovascular syphilis usually occurs 10–30 years after the initial infection. The most common complication is syphilitic aortitis, which may result in aortic aneurysm formation.

Neurosyphilis refers to an infection involving the central nervous system. Involvement of the central nervous system in syphilis (either asymptomatic or symptomatic) can occur at any stage of the infection. It may occur early, being either asymptomatic or in the form of syphilitic meningitis, or late as meningovascular syphilis, general paresis, or tabes dorsalis.

Meningovascular syphilis involves inflammation of the small and medium arteries of the central nervous system. It can present between 1–10 years after the initial infection. Meningovascular syphilis is characterized by stroke, cranial nerve palsis and spinal cord inflammation. Late symptomatic neurosyphilis can develop decades after the original infection and includes 2 types; general paresis and tabes dorsalis. General paresis presents with dementia, personality changes, delusions, seizures, psychosis and depression. Tabes dorsalis is characterized by gait instability, sharp pains in the trunk and limbs, impaired positional sensation of the limbs as well as having a positive Romberg's sign. Both tabes dorsalis and general paresis may present with Argyll Robertson pupil which are pupils that constrict when the person focuses on near objects (accommodation reflex) but do not constrict when exposed to bright light (pupillary reflex).

Congenital syphilis is that which is transmitted during pregnancy or during birth. Two-thirds of syphilitic infants are born without symptoms. Common symptoms that develop over the first couple of years of life include enlargement of the liver and spleen (70%), rash (70%), fever (40%), neurosyphilis (20%), and lung inflammation (20%). If untreated, late congenital syphilis may occur in 40%, including saddle nose deformation, Higouménakis' sign, saber shin, or Clutton's joints among others. Infection during pregnancy is also associated with miscarriage. The three main dental defects in congenital syphilis are Hutchinson's incisors (screwdriver shaped incisors), Moon's molars or bud molars, and Fournier's molars or mulberry molars (molars with abnormal occlusal anatomy resembling a mulberry).

"Treponema pallidum" subspecies" pallidum" is a spiral-shaped, Gram-negative, highly mobile bacterium. Three other human diseases are caused by related "Treponema pallidum" subspecies, including yaws (subspecies "pertenue"), pinta (subspecies "carateum") and bejel (subspecies "endemicum"). Unlike subspecies "pallidum", they do not cause neurological disease. Humans are the only known natural reservoir for subspecies "pallidum". It is unable to survive more than a few days without a host. This is due to its small genome (1.14Mbp) failing to encode the metabolic pathways necessary to make most of its macronutrients. It has a slow doubling time of greater than 30 hours. The bacterium is known for its ability to evade the immune system and its invasiveness.

Syphilis is transmitted primarily by sexual contact or during pregnancy from a mother to her baby; the spirochete is able to pass through intact mucous membranes or compromised skin. It is thus transmissible by kissing near a lesion, as well as oral, vaginal, and anal sex. Approximately 30% to 60% of those exposed to primary or secondary syphilis will get the disease. Its infectivity is exemplified by the fact that an individual inoculated with only 57 organisms has a 50% chance of being infected. Most new cases in the United States (60%) occur in men who have sex with men; and in this population 20% of syphilis cases were due to oral sex alone. Syphilis can be transmitted by blood products, but the risk is low due to screening of donated blood in many countries. The risk of transmission from sharing needles appears to be limited.

It is not generally possible to contract syphilis through toilet seats, daily activities, hot tubs, or sharing eating utensils or clothing. This is mainly because the bacteria die very quickly outside of the body, making transmission by objects extremely difficult.

Syphilis is difficult to diagnose clinically during early infection. Confirmation is either via blood tests or direct visual inspection using dark field microscopy. Blood tests are more commonly used, as they are easier to perform. Diagnostic tests are unable to distinguish between the stages of the disease.

Blood tests are divided into nontreponemal and treponemal tests.

Nontreponemal tests are used initially, and include venereal disease research laboratory (VDRL) and rapid plasma reagin (RPR) tests. False positives on the nontreponemal tests can occur with some viral infections, such as varicella (chickenpox) and measles. False positives can also occur with lymphoma, tuberculosis, malaria, endocarditis, connective tissue disease, and pregnancy.

Because of the possibility of false positives with nontreponemal tests, confirmation is required with a treponemal test, such as treponemal pallidum particle agglutination (TPHA) or fluorescent treponemal antibody absorption test (FTA-Abs). Treponemal antibody tests usually become positive two to five weeks after the initial infection. Neurosyphilis is diagnosed by finding high numbers of leukocytes (predominately lymphocytes) and high protein levels in the cerebrospinal fluid in the setting of a known syphilis infection.

Dark field microscopy of serous fluid from a chancre may be used to make an immediate diagnosis. Hospitals do not always have equipment or experienced staff members, and testing must be done within 10 minutes of acquiring the sample. Two other tests can be carried out on a sample from the chancre: direct fluorescent antibody (DFA) and polymerase chain reaction (PCR) tests. DFA uses antibodies tagged with fluorescein, which attach to specific syphilis proteins, while PCR uses techniques to detect the presence of specific syphilis genes. These tests are not as time-sensitive, as they do not require living bacteria to make the diagnosis.

, there is no vaccine effective for prevention. Several vaccines based on treponemal proteins reduce lesion development in an animal model but research continues.

Condom use reduces the likelihood of transmission during sex, but does not completely eliminate the risk. The Centers for Disease Control and Prevention (CDC) states, "Correct and consistent use of latex condoms can reduce the risk of syphilis only when the infected area or site of potential exposure is protected. However, a syphilis sore outside of the area covered by a latex condom can still allow transmission, so caution should be exercised even when using a condom."

Abstinence from intimate physical contact with an infected person is effective at reducing the transmission of syphilis. The CDC states, "The surest way to avoid transmission of sexually transmitted diseases, including syphilis, is to abstain from sexual contact or to be in a long-term mutually monogamous relationship with a partner who has been tested and is known to be uninfected."

Congenital syphilis in the newborn can be prevented by screening mothers during early pregnancy and treating those who are infected. The United States Preventive Services Task Force (USPSTF) strongly recommends universal screening of all pregnant women, while the World Health Organization (WHO) recommends all women be tested at their first antenatal visit and again in the third trimester. If they are positive, it is recommended their partners also be treated. Congenital syphilis is still common in the developing world, as many women do not receive antenatal care at all, and the antenatal care others receive does not include screening. It still occasionally occurs in the developed world, as those most likely to acquire syphilis are least likely to receive care during pregnancy. Several measures to increase access to testing appear effective at reducing rates of congenital syphilis in low- to middle-income countries. Point-of-care testing to detect syphilis appeared to be reliable although more research is needed to assess its effectiveness and into improving outcomes in mothers and babies.

The CDC recommends that sexually active men who have sex with men be tested at least yearly. The USPSTF also recommends screening among those at high risk.

Syphilis is a notifiable disease in many countries, including Canada the European Union, and the United States. This means health care providers are required to notify public health authorities, which will then ideally provide partner notification to the person's partners. Physicians may also encourage patients to send their partners to seek care. Several strategies have been found to improve follow-up for STI testing, including email and text messaging of reminders for appointments.

The first-line treatment for uncomplicated syphilis (primary or secondary stages) remains a single dose of intramuscular benzathine benzylpenicillin. Doxycycline and tetracycline are alternative choices for those allergic to penicillin; due to the risk of birth defects, these are not recommended for pregnant women. Resistance to macrolides, rifampicin, and clindamycin is often present. Ceftriaxone, a third-generation cephalosporin antibiotic, may be as effective as penicillin-based treatment. It is recommended that a treated person avoid sex until the sores are healed.

For neurosyphilis, due to the poor penetration of benzathine penicillin into the central nervous system, those affected are given large doses of intravenous penicillin G for a minimum of 10 days. If a person is allergic to penicillin, ceftriaxone may be used or penicillin desensitization attempted. Other late presentations may be treated with once-weekly intramuscular benzathine penicillin for three weeks. Treatment at this stage solely limits further progression of the disease and has a limited effect on damage which has already occurred. Serologic cure can be measured when the non-treponemal titers decline by a factor of 4 or more in 6–12 months in early syphilis or 12–24 months in late syphilis.

One of the potential side effects of treatment is the Jarisch–Herxheimer reaction. It frequently starts within one hour and lasts for 24 hours, with symptoms of fever, muscle pains, headache, and a fast heart rate. It is caused by cytokines released by the immune system in response to lipoproteins released from rupturing syphilis bacteria.

Penicillin is an effective treatment for syphilis in pregnancy but there is no agreement on which dose or route of delivery is most effective.

In 2012, about 0.5% of adults were infected with syphilis, with 6 million new cases. In 1999, it is believed to have infected 12 million additional people, with greater than 90% of cases in the developing world. It affects between 700,000 and 1.6 million pregnancies a year, resulting in spontaneous abortions, stillbirths, and congenital syphilis. During 2015, it caused about 107,000 deaths, down from 202,000 in 1990. In sub-Saharan Africa, syphilis contributes to approximately 20% of perinatal deaths. Rates are proportionally higher among intravenous drug users, those who are infected with HIV, and men who have sex with men. In the United States about 55,400 people are newly infected each year. In the United States as of 2020, rates of syphilis have increased by more than threefold; in 2018 approximately 86% of all cases of syphilis in the United States were in men. African Americans accounted for almost half of all cases in 2010. As of 2014, syphilis infections continue to increase in the United States.

Syphilis was very common in Europe during the 18th and 19th centuries. Flaubert found it universal among nineteenth-century Egyptian prostitutes. In the developed world during the early 20th century, infections declined rapidly with the widespread use of antibiotics, until the 1980s and 1990s. Since 2000, rates of syphilis have been increasing in the US, Canada, the UK, Australia and Europe, primarily among men who have sex with men. Rates of syphilis among US women have remained stable during this time, while rates among UK women have increased, but at a rate less than that of men. Increased rates among heterosexuals have occurred in China and Russia since the 1990s. This has been attributed to unsafe sexual practices, such as sexual promiscuity, prostitution, and decreasing use of barrier protection.

Left untreated, it has a mortality rate of 8% to 58%, with a greater death rate among males. The symptoms of syphilis have become less severe over the 19th and 20th centuries, in part due to widespread availability of effective treatment, and partly due to virulence of the bacteria. With early treatment, few complications result. Syphilis increases the risk of HIV transmission by two to five times, and coinfection is common (30–60% in some urban centers). In 2015, Cuba became the first country to eliminate mother-to-child transmission of syphilis.

The origin of syphilis is disputed. Syphilis was present in the Americas before European contact, and it may have been carried from the Americas to Europe by the returning crewmen from Christopher Columbus's voyage to the Americas, or it may have existed in Europe previously but gone unrecognized until shortly after Columbus's return. These are the "Columbian" and "pre-Columbian" hypotheses, respectively, with the "Columbian" hypothesis better supported by the evidence.

The first written records of an outbreak of syphilis in Europe occurred in 1494 or 1495 in Naples, Italy, during a French invasion (Italian War of 1494–98). Since it was claimed to have been spread by French troops, it was initially called the "French disease" by the people of Naples. In 1530, the pastoral name "syphilis" (the name of a character) was first used by the Italian physician and poet Girolamo Fracastoro as the title of his Latin poem in dactylic hexameter describing the ravages of the disease in Italy. It was also called the "Great Pox".

In the 16th through 19th centuries, syphilis was one of the largest public health burdens in prevalence, symptoms, and disability, although records of its true prevalence were generally not kept because of the fearsome and sordid status of sexually transmitted diseases in those centuries. According to a 2020 study, more than 20% of individual in the age range 15–34 years in late 18th century London were treated for syphilis. At the time the causative agent was unknown but it was well known that it was spread sexually and also often from mother to child. Its association with sex, especially sexual promiscuity and prostitution, made it an object of fear and revulsion and a taboo. The magnitude of its morbidity and mortality in those centuries reflected that, unlike today, there was no adequate understanding of its pathogenesis and no truly effective treatments. Its damage was caused not so much by great sickness or death early in the course of the disease but rather by its gruesome effects decades after infection as it progressed to neurosyphilis with tabes dorsalis. Mercury compounds and isolation were commonly used, with treatments often worse than the disease.

The causative organism, "Treponema pallidum", was first identified by Fritz Schaudinn and Erich Hoffmann, in 1905. The first effective treatment for syphilis was Arsphenamine, discovered by Sahachiro Hata in 1909, during a survey of hundreds of newly synthesized organic arsenical compounds led by Paul Ehrlich. It was manufactured and marketed from 1910 under the trade name Salvarsan by Hoechst AG. This organoarsenic compound was the first modern chemotherapeutic agent.

During the 20th century, as both microbiology and pharmacology advanced greatly, syphilis, like many other infectious diseases, became more of a manageable burden than a scary and disfiguring mystery, at least in developed countries among those people who could afford to pay for timely diagnosis and treatment. Penicillin was discovered in 1928, and effectiveness of treatment with penicillin was confirmed in trials in 1943, at which time it became the main treatment.

Many famous historical figures, including Franz Schubert, Arthur Schopenhauer, Édouard Manet, Charles Baudelaire, and Guy de Maupassant are believed to have had the disease. Friedrich Nietzsche was long believed to have gone mad as a result of tertiary syphilis, but that diagnosis has recently come into question.
The earliest known depiction of an individual with syphilis is Albrecht Dürer's "Syphilitic Man", a woodcut believed to represent a Landsknecht, a Northern European mercenary. The myth of the "femme fatale" or "poison women" of the 19th century is believed to be partly derived from the devastation of syphilis, with classic examples in literature including John Keats' "La Belle Dame sans Merci".

The Flemish artist Stradanus designed a print called "Preparation and Use of Guayaco for Treating Syphilis", a scene of a wealthy man receiving treatment for syphilis with the tropical wood guaiacum sometime around 1590.

The "Tuskegee Study of Untreated Syphilis in the Negro Male" was an infamous, unethical, and racist clinical study conducted between 1932 and 1972 by the U.S. Public Health Service. Whereas the purpose of this study was to observe the natural history of untreated syphilis; the African-American men in the study were told they were receiving free treatment for "bad blood" from the United States government.

The Public Health Service started working on this study in 1932 in collaboration with Tuskegee University, a historically black college in Alabama. Researchers enrolled 600 poor, African-American sharecroppers from Macon County, Alabama in the study. Of these men, 399 had contracted syphilis before the study began, and 201 did not have the disease. Medical care, hot meals, and free burial insurance were given to those who participated. The men were told that the study would last six months, but in the end it continued for 40 years. After funding for treatment was lost, the study was continued without informing the men that they were only being studied and would not be treated. Facing insufficient participation, the Macon County Health Department nevertheless wrote to subjects to offer them a "last chance" to get a special "treatment", which was not a treatment at all, but a spinal tap administered exclusively for diagnostic purposes. None of the men infected were ever told that they had the disease, and none were treated with penicillin even after the antibiotic had been proven to successfully treat syphilis. According to the Centers for Disease Control, the men were told they were being treated for "bad blood"—a colloquialism describing various conditions such as fatigue, anemia, and syphilis—which was a leading cause of death among southern African-American men.

The 40-year study became a textbook example of poor medical ethics because researchers had knowingly withheld treatment with penicillin and because the subjects had been misled concerning the purposes of the study. The revelation in 1972 of these study failures by a whistleblower, Peter Buxtun, led to major changes in U.S. law and regulation on the protection of participants in clinical studies. Now studies require informed consent, communication of diagnosis, and accurate reporting of test results.
Similar experiments were carried out in Guatemala from 1946 to 1948. It was done during the administration of American President Harry S. Truman and Guatemalan President Juan José Arévalo with the cooperation of some Guatemalan health ministries and officials. Doctors infected soldiers, prostitutes, prisoners and mental patients with syphilis and other sexually transmitted diseases, without the informed consent of the subjects, and treated most subjects with antibiotics. The experiment resulted in at least 83 deaths. In October 2010, the U.S. formally apologized to Guatemala for the ethical violations that took place. Secretary of State Hillary Clinton and Health and Human Services Secretary Kathleen Sebelius stated "Although these events occurred more than 64 years ago, we are outraged that such reprehensible research could have occurred under the guise of public health. We deeply regret that it happened, and we apologize to all the individuals who were affected by such abhorrent research practices." The experiments were led by physician John Charles Cutler who also participated in the late stages of the Tuskegee syphilis experiment.

First called "grande verole" or the "great pox" by the French. Other historical names have included "button scurvy", sibbens, frenga and dichuchwa, among others.



</doc>
<doc id="28853" url="https://en.wikipedia.org/wiki?curid=28853" title="Smiling Buddha">
Smiling Buddha

Smiling Buddha (MEA designation: Pokhran-I) was the assigned code name of India's first successful nuclear bomb test on 18 May 1974. The bomb was detonated on the Pokhran Test Range (PTR), in Rajasthan, by the Indian Army under the supervision of several key .

"Pokhran-I" was also the first confirmed nuclear weapons test by a nation outside the five permanent members of the United Nations Security Council. Officially, the Indian Ministry of External Affairs (MEA) characterised this test as a "peaceful nuclear explosion". A series of nuclear tests was carried out in 1998 under the name Pokhran-II.

India started its own nuclear programme in 1944 when Homi Jehangir Bhabha founded the Tata Institute of Fundamental Research. Physicist Raja Ramanna played an essential role in nuclear weapons technology research; he expanded and supervised scientific research on nuclear weapons and was the first directing officer of the small team of scientists that supervised and carried out the test.<ref name = "http://nuclearweaponarchive.org"></ref>

After Indian independence from the British Empire, Indian Prime Minister Jawaharlal Nehru authorised the development of a nuclear programme headed by Homi Bhabha. The "Atomic Energy Act" of 1948 focused on peaceful development. India was heavily involved in the development of the Nuclear Non-Proliferation Treaty, but ultimately opted not to sign it.
In 1954, Homi Jehangir Bhabha steered the nuclear programme in the direction of weapons design and production. Two important infrastructure projects were commissioned. The first established Trombay Atomic Energy Establishment at Mumbai (Bombay). The other created a governmental secretariat, Department of Atomic Energy (DAE), of which Bhabha was the first secretary. From 1954 to 1959, the nuclear programme grew swiftly. By 1958, the DAE had one-third of the defence budget for research purposes. In 1954, India reached a verbal understanding with Canada and the United States under the Atoms for Peace programme; Canada and the United States ultimately agreed to provide and establish the CIRUS research reactor also at Trombay. The acquisition of CIRUS was a watershed event in nuclear proliferation with the understanding between India and the United States that the reactor would be used for peaceful purposes only. CIRUS was an ideal facility to develop a plutonium device, and therefore Nehru refused to accept nuclear fuel from Canada and started the programme to develop an indigenous nuclear fuel cycle.

In July 1958, Nehru authorised "Project Phoenix" to build a reprocessing plant with a capacity of 20 tons of fuel a year – a size to match the production capacity of CIRUS. The plant used the PUREX process and was designed by the Vitro Corporation of America. Construction of the plutonium plant began at Trombay on 27 March 1961, and it was commissioned in mid-1964.

The nuclear programme continued to mature, and by 1960, Nehru made the critical decision to move the programme into production. At about the same time, Nehru held discussions with the American firm Westinghouse Electric to construct India's first nuclear power plant in Tarapur, Maharashtra. Kenneth Nichols, a US Army engineer, recalls from a meeting with Nehru, "it was that time when Nehru turned to Bhabha and asked Bhabha for the timeline of the development of a nuclear weapon". Bhabha estimated he would need about a year to accomplish the task.

By 1962, the nuclear programme was still developing, but at a slow rate. Nehru was distracted by the Sino-Indian War, during which India lost territory to China. Nehru turned to the Soviet Union for help, but the Soviet Union was preoccupied with the Cuban Missile Crisis. The Soviet Politburo turned down Nehru's request for arms and continued backing the Chinese. India concluded that the Soviet Union was an unreliable ally, and this conclusion strengthened India's determination to create a nuclear deterrent. Design work began in 1965 under Bhabha and proceeded under Raja Ramanna who took over the programme after the former's death.

Bhabha was now aggressively lobbying for nuclear weapons and made several speeches on Indian radio. In 1964, Bhabha told the Indian public via radio that "such nuclear weapons are remarkably cheap" and supported his arguments by referring to the economic cost of the American nuclear testing programme "Project Plowshare". Bhabha stated to the politicians that a 10 kt device would cost around $350,000, and $600,000 for a 2 mt. From this, he estimated that "a stockpile" of around 50 atomic bombs would cost under $21 million and a stockpile of 50 two-megaton hydrogen bombs would cost around $31.5 million." Bhabha did not realise, however, that the U.S. "Plowshare" cost-figures were produced by a vast industrial complex costing tens of billions of dollars, which had already manufactured nuclear weapons numbering in the tens of thousands. The delivery systems for nuclear weapons typically cost several times as much as the weapons themselves.

The nuclear programme was partially slowed when Lal Bahadur Shastri became the prime minister. Shastri faced the Indo-Pakistani War of 1965. He appointed physicist Vikram Sarabhai as the head of the nuclear programme but, because of his non-violent Gandhian beliefs, Sarabhai directed it toward peaceful purposes rather than military development.

In 1967, Indira Gandhi became the prime minister and work on the nuclear programme resumed with renewed vigour. Homi Sethna, a chemical engineer, played a significant role in the development of weapon-grade plutonium, while Ramanna designed and manufactured the whole nuclear device. The first nuclear bomb project did not employ more than 75 scientists because of its sensitivity. The weapons programme was now directed towards the production of plutonium rather than uranium.

In 1968–69, P. K. Iyengar visited the Soviet Union with three colleagues and toured the nuclear research facilities at Dubna, Russia. During his visit, Iyengar was impressed by the plutonium-fueled pulsed fast reactor. Upon his return to India, Iyengar set about developing plutonium reactors approved by the Indian political leadership in January 1969. The secret plutonium plant was known as "Purnima", and construction began in March 1969. The plant's leadership included Iyengar, Ramanna, Homi Sethna, and Sarabhai. Sarabhai's presence indicates that, with or without formal approval, the work on nuclear weapons at Trombay had been commenced.

India continued to harbour ambivalent feelings about nuclear weapons, and accorded low priority to their production until the Indo-Pakistani War of 1971. In December 1971, Richard Nixon sent a carrier battle group led by the into the Bay of Bengal in an attempt to intimidate India. The Soviet Union responded by sending a submarine armed with nuclear missiles from Vladivostok to trail the US task force. The Soviet response demonstrated the deterrent value and significance of nuclear weapons and ballistic missile submarines to Indira Gandhi. India gained the military and political initiative over Pakistan after acceding to the treaty that divided Pakistan into two different political entities.

On 7 September 1972, near the peak of her post-war popularity, Indira Gandhi authorised the Bhabha Atomic Research Centre (BARC) to manufacture a nuclear device and prepare it for a test. Although the Indian Army was not fully involved in the nuclear testing, the army's highest command was kept fully informed of the test preparations. The preparations were carried out under the watchful eyes of the Indian political leadership, with civilian scientists assisting the Indian Army.

The device was formally called the "Peaceful Nuclear Explosive", but it was usually referred to as the "Smiling Buddha". The device was detonated on 18 May 1974, Buddha Jayanti (a festival day in India marking the birth of Gautama Buddha). Indira Gandhi maintained tight control of all aspects of the preparations of the "Smiling Buddha" test, which was conducted in extreme secrecy; besides Gandhi, only advisers Parmeshwar Haksar and Durga Dhar were kept informed. Scholar Raj Chengappa asserts the Indian Defence Minister Jagjivan Ram was not provided with any knowledge of this test and came to learn of it only after it was conducted. Swaran Singh, the Minister of External Affairs, was given 48 hours advance notice. The Indira Gandhi administration employed no more than 75 civilian scientists while General G. G. Bewoor, Indian army chief, and the commander of Indian Western Command were the only military commanders kept informed.

The head of this entire nuclear bomb project was the director of the BARC, Raja Ramanna. In later years, his role in the nuclear programme would be more deeply integrated as he remained head of the nuclear programme most of his life. The designer and creator of the bomb was P. K. Iyengar, who was the second in command of this project. Iyengar's work was further assisted by the chief metallurgist, R. Chidambaram, and by Nagapattinam Sambasiva Venkatesan of the Terminal Ballistics Research Laboratory, who developed and manufactured the high explosive implosion system. The explosive materials and the detonation system were developed by Waman Dattatreya Patwardhan of the High Energy Materials Research Laboratory.

The overall project was supervised by chemical engineer Homi Sethna, Chairman of the Atomic Energy Commission of India. Chidambaram, who would later coordinate work on the Pokhran-II tests, began work on the equation of state of plutonium in late 1967 or early 1968. To preserve secrecy, the project employed no more than 75 scientists and engineers from 1967–74. Abdul Kalam also arrived at the test site as the representative of the DRDO.

A. K. Ganguly, of the BARC, was the "Test" Project chief of health and safety, as well as, chief of post "Test" scientific investigations programme. As early as 1956, Ganguly was selected by Homi J. Bhabha, from his academic perch, at the University of Notre Dame, USA, where he had originated the Ganguly-Magee theory in Radiation Chemistry. During Ganguly’s career, in the BARC, Vikram A. Sarabhai selected him to lead the process of formation of the Ministry of Environment.

The device was of the implosion-type design and had a close resemblance to the American nuclear bomb called the "Fat Man". The implosion system was assembled at the Terminal Ballistics Research Laboratory (TBRL) of the DRDO in Chandigarh. The detonation system was developed at the High Energy Materials Research Laboratory (HEMRL) of the DRDO in Pune, Maharashtra State. The 6 kg of plutonium came from the CIRUS reactor at BARC. The neutron initiator was of the polonium–beryllium type and code-named "Flower". The complete nuclear bomb was engineered and finally assembled by Indian engineers at Trombay before transportation to the test site.

The fully assembled device had a hexagonal cross section, 1.25 metres in diameter, and weighed 1400 kg. The device was mounted on a hexagonal metal tripod, and was transported to the shaft on rails which the army kept covered with sand. The device was detonated when Dr Pranab R. Dastidar pushed the firing button at 8.05 a.m.; it was in a shaft 107 m under the army Pokhran test range in the Thar Desert (or Great Indian Desert), Rajasthan.

The nuclear yield of this test still remains controversial, with unclear data provided by Indian sources, although Indian politicians have given the country's press a range from 2 kt to 20 kt. The official yield was initially set at 12 kt; post-Operation Shakti claims have raised it to 13 kt. Independent seismic data from outside and analysis of the crater features indicate a lower figure. Analysts usually estimate the yield at 4 to 6 kt, using conventional seismic magnitude-to-yield conversion formulas. In recent years, both Homi Sethna and P. K. Iyengar have conceded the official yield to be an exaggeration.

Iyengar has variously stated that the yield was 8–10 kt, that the device was designed to yield 10 kt, and that the yield was 8 kt "exactly as predicted". Although seismic scaling laws lead to an estimated yield range between 3.2 kt and 21 kt, an analysis of hard rock cratering effects suggests a narrow range of around 8 kt for the yield, which is within the uncertainties of the seismic yield estimate.

Indian Prime Minister Indira Gandhi had already gained much popularity and publicity after her successful military campaign against Pakistan in the 1971 war. The test caused an immediate revival of Indira Gandhi's popularity, which had flagged considerably from its height after the 1971 war. The overall popularity and image of the Congress Party was enhanced and the Congress Party was well received in the Indian Parliament. In 1975, Homi Sethna, a chemical engineer and the chairman of the Indian Atomic Energy Commission (AECI), Raja Ramanna of BARC, and Basanti Nagchaudhuri of DRDO, all were honoured with the "Padma Vibhushan", India's second highest civilian award. Five other project members received the "Padma Shri", India's fourth highest civilian award. India consistently maintained that this was a peaceful nuclear bomb test and that it had no intentions of militarising its nuclear programme. However, according to independent monitors, this test was part of an accelerated Indian nuclear programme.
In 1997 Raja Ramanna, speaking to the "Press Trust of India", maintained:

While India continued to state that the test was for peaceful purposes, it encountered opposition from many quarters. The Nuclear Suppliers Group (NSG) was formed in reaction to the Indian tests to check international nuclear proliferation. The NSG decided in 1992 to require full-scope IAEA safeguards for any new nuclear export deals, which effectively ruled out nuclear exports to India, but in 2008 it waived this restriction on nuclear trade with India as part of the Indo-US civilian nuclear agreement.

Pakistan did not view the test as a "peaceful nuclear explosion", and cancelled talks scheduled for 10 June on normalisation of relations. Pakistan's Prime Minister Zulfikar Ali Bhutto vowed in June 1974 that he would never succumb to "nuclear blackmail" or accept "Indian hegemony or domination over the subcontinent". The chairman of the Pakistan Atomic Energy Commission, Munir Ahmed Khan, said that the test would force Pakistan to test its own nuclear bomb. Pakistan's leading nuclear physicist, Pervez Hoodbhoy, stated in 2011 that he believed the test "pushed [Pakistan] further into the nuclear arena".

The plutonium used in the test was created in the CIRUS reactor supplied by Canada and using heavy water supplied by the United States. Both countries reacted negatively, especially in light of then ongoing negotiations on the Nuclear Non-Proliferation Treaty and the economic aid both countries had provided to India. Canada concluded that the test violated a 1971 understanding between the two states, and froze nuclear energy assistance for the two heavy water reactors then under construction. The United States concluded that the test did not violate any agreement and proceeded with a June 1974 shipment of enriched uranium for the Tarapur reactor.

France sent a congratulatory telegram to India but later withdrew it.

Despite many proposals, India did not carry out further nuclear tests until 1998. After the 1998 general elections, Operation Shakti (also known as Pokhran-II) was carried out at the Pokhran test site, using devices designed and built over the preceding two decades.



</doc>
<doc id="28855" url="https://en.wikipedia.org/wiki?curid=28855" title="Shea Stadium">
Shea Stadium

Shea Stadium (; formally known as William A. Shea Municipal Stadium) was a stadium in Flushing Meadows–Corona Park, Queens, New York City. Built as a multi-purpose stadium, it was the home park of Major League Baseball's New York Mets for 45 seasons as well as the New York Jets football team from 1964 to 1983.

The venue was named in honor of William A. Shea, the man who was most responsible for bringing National League baseball back to New York after the Dodgers and Giants left for California in 1957. It was demolished in 2009 to create additional parking for the adjacent Citi Field, Shea's replacement and the current home of the Mets.

The origins of Shea Stadium go back to the Brooklyn Dodgers' and the New York Giants' relocations to the U.S. west coast in 1958, which left New York without a National League baseball team for the next four years.

Prior to the Dodgers' departure, New York City official Robert Moses tried to interest owner Walter O'Malley in the site as the location for a new stadium, but O'Malley refused, unable to agree on location, ownership, and lease terms. O'Malley preferred to pay construction costs himself so he could own the stadium outright. He wanted total control over revenue from parking, concessions, and other events.

New York City, in contrast, wanted to build the stadium, rent it, and retain the ancillary revenue rights to pay off its construction bonds. Additionally, O'Malley wanted to build his new stadium in Brooklyn, while Moses insisted on Flushing Meadows. When Los Angeles offered O'Malley what the City of New York wouldn't—complete ownership of the facility—he left for southern California in a preemptive bid to install the Dodgers there before a new or existing major league franchise could beat him to it. At the same time, Horace Stoneham moved his New York Giants to San Francisco (although he originally considered moving them to Minneapolis), ensuring that there would be two National League teams in California, and preserving the longstanding rivalry with the Dodgers that continues to this day.

In , the National League agreed to grant an expansion franchise to the owners of the New York franchise in the abortive Continental League, provided that a new stadium be built. Mayor Robert Wagner, Jr. had to personally wire all National League owners and assure them that the city would build a stadium.

On October 6, 1961, the Mets signed a 30-year stadium lease, with an option for a 10-year renewal. Rent for what was originally budgeted as a $9 million facility was set at $450,000 annually, with a reduction of $20,000 each year until it reached $300,000 annually.

In their inaugural season in 1962, the expansion Mets played in the Polo Grounds, with original plans to move to a new stadium in 1963. In October 1962, Mets official Tom Meany said, "Only a series of blizzards or some other unforeseen trouble might hamper construction." That unforeseen trouble surfaced in a number of ways: the severe winter of 1962–1963, along with the bankruptcies of two subcontractors and labor issues. The end result was that both the Mets and Jets played at the Polo Grounds for one more year.
It was originally to be called "Flushing Meadow Park Municipal Stadium" – the name of the public park within which it was built – but an ultimately successful movement was launched to name it in honor of William Shea, the New York attorney who brought National League baseball back to New York.

After 29 months of construction and $28.5 million spent, Shea Stadium opened on April 17, 1964, with the Pittsburgh Pirates beating the Mets before a crowd of There were no prior exhibition games or events, and the stadium was barely finished in time for the home opener. Because of a jurisdictional dispute between Local 3 of the International Brotherhood of Electrical Workers and Local 1106 of the Communications Workers of America, the telephone and telegraph wiring was not finished in time for opening day. The stadium opened five days before the 1964-65 New York World's Fair, across Roosevelt Avenue. Although not officially part of the fair grounds, the stadium sported steel panels on its exterior in the blue-and-orange colors of the Fair, the same team colors of the Mets. The panels were removed in 1980.

In accordance with New York City law, in 2009 Shea Stadium was dismantled, rather than imploded. The company with the rights to sell memorabilia was given two weeks after the final game to remove seats, signage and other potentially saleable and collectable items before demolition was to begin. The seats were the first ($869 per pair plus tax, a combination of '86 and '69, the team's two World Series championship years), followed by other memorabilia such as the foul poles, dugouts, stadium signage, and the giant letters that spelled out "SHEA" at the front of the building.

After salvaging operations concluded, demolition of the ballpark began on October 14, 2008. On October 18, the scoreboard in right field was demolished, with the bleachers, batter's eye and bullpens shortly thereafter.

By November 10, the field, dugouts and the rest of the field level seats had been demolished.

On January 31, Mets fans all over New York came to Shea Stadium for one final farewell. Fans took a tour of the site, told stories, and sang songs. The last remaining section of seats was demolished on February 18. Fans stood in awe as the remaining structure of Shea Stadium (one section of ramps) was torn down at 11:22 am.

The locations of Shea's home plate, pitcher's mound, and bases are marked in Citi Field's parking lot. The plaques feature engravings of the neon baseball players that graced the exterior of the stadium from 1988 onward.

On October 9, 2013, the New York City Council approved a plan to build a mall and entertainment center called Willets West in the Citi Field parking lot where Shea Stadium stood, as part of an effort by the city to redevelop the nearby neighborhood of Willets Point. However, in 2015, the Appellate Division of the New York State Supreme Court ruled that the site, considered parkland, could not be used for commercial development without permission from the New York state government.

Shea Stadium was the home of the New York Mets starting in 1964, and it hosted what would be its only All-Star Game that first year, with Johnny Callison of the Philadelphia Phillies hitting a home run in the ninth inning to win the only Mid-Summer Classic held in the Queens ballpark. A month earlier, on Father's Day, Callison's teammate, future Hall of Fame member and U.S. Senator Jim Bunning, pitched a perfect game against the Mets.

The stadium was often criticized by baseball purists for many reasons, even though it was retrofitted to be a baseball-only stadium after the Jets left. The upper deck was one of the highest in the majors. The lower boxes were farther from the field than similar seats in other parks because they were still on the rails that had swiveled them into position for football. Outfield seating was sparse, in part because the stadium was designed to be fully enclosed.

At one time, Shea's foul territory was one of the most expansive in the majors. This was very common for ballparks built during the 1960s, in part due to the need to accommodate the larger football field. This was also because the stadium was designed to be fully enclosed. However, seats added over the years in the lower level greatly reduced the size of foul territory by the dawn of the 21st century. On the plus side, Shea always used a natural grass surface, in contrast to other multi-purpose stadiums such as Three Rivers Stadium, Veterans Stadium, and Riverfront Stadium, which were built in the same era and style and had artificial turf.

Shea Stadium hosted postseason baseball in 1969, 1973, 1986, 1988, 1999, 2000, and 2006; it hosted the World Series in , , , and . It had the distinction of being the home of the 1969 "Miracle Mets"— led by former Brooklyn Dodger Gil Hodges that defied 100–1 odds and won the World Series, after seven straight seasons in last or next-to-last place. Shea became famous for the bedlam that took place after the Mets won the decisive Game 5 of the World Series, as fans stormed the field in celebration. Similar scenes took place a few weeks earlier after the Mets clinched the National League East title, and then defeated the Atlanta Braves in the first National League Championship Series to win the pennant.

Tommie Agee, Lenny Dykstra, Todd Pratt, Robin Ventura, and Benny Agbayani hit post-season, walk-off home runs at Shea (although, while the ball hit by Ventura over the fence may have been the most famous of the postseason walk off hits, it was famously called "the grand slam single", because when he hit the game winning ball over the fence, he was mobbed by his teammates before he could reach second base, and never wound up touching second base, third base and home plate. It was not ruled a home run as he never circled the bases completely. It probably made Ventura, known for his penchant for hitting Grand slam home runs, even more famous, and the hit itself more famous, because of the very fact that he never circled the bases fully, technically not making it a homer).

Agee was the only player in the history of the ballpark to hit a fair ball into the upper deck in left field. The spot was marked with a sign featuring Agee's number 20 and the date, which was April 10, 1969. Teammate Cleon Jones said the ball was still rising when it hit the seats, so it very likely could have been the longest home run hit at Shea. It came in the second inning, and Agee hit another in the seventh over the center field wall; both solo shots were off of Montreal Expos starter Larry Jaster, and the Mets 

In 1971, Dave Kingman – then with the San Francisco Giants and later to play for the Mets on two occasions – hit a home run that smashed off the windshield of the Giants' team bus, parked behind the left field bullpen.

For many years, the Mets' theme song, "Meet the Mets", was played at Shea before every home game. Jane Jarvis, a local jazz artist, played the popular songs on the Thomas organ at Mets games for many years at the stadium.

On October 3, 2004, it was the venue for the last game in the history of the Montreal Expos, and the Mets won Montreal's major league story ended where it had started 35 years earlier: at Shea Stadium. The following year, the Expos relocated to Washington, D.C. and became the Nationals.

The last game played at Shea Stadium was a loss to the Florida Marlins on September 28, 2008. However, the Mets were in the thick of the playoff chase until the last day. A win would have meant another game for Shea as the Mets were scheduled to play the Milwaukee Brewers in a one-game playoff for the National League Wild Card berth. Following the game, there was a "Shea Goodbye" tribute in which many players from the Mets' glory years entered the stadium and touched home plate one final time so that fans could pay their last respects to the players and the stadium the Mets called home for 45 years. The ceremony ended with Tom Seaver throwing a final pitch to Mike Piazza, then, as the Beatles' "In My Life" played on the stadium speakers the two former Met stars walked out of the centerfield gate and closed it behind them, followed by a display of blue and orange fireworks.

Three National League Division Series were played at Shea Stadium. The Mets won all three, and never lost a Division Series game at Shea.

Seven National League Championship Series were played at Shea Stadium.

The decisive seventh game of this series was played at Shea Stadium, marking the only time that the Mets lost the deciding game of a National League Championship Series at Shea.

Four World Series were played in Shea Stadium.

The Yankees' World Series win in 2000 was the only time that a visiting team won a World Series at Shea Stadium. The Mets won both their World Series titles at Shea Stadium (in Game 5 in 1969, and Game 7 in 1986).
The New York Yankees played their home games in Shea Stadium during the 1974 and 1975 seasons while Yankee Stadium was being renovated. The move to Shea had been proposed earlier in the decade, but the Mets, as Shea's primary tenants, refused to sign off on the deal. However, when the city stepped in to pay for renovating Yankee Stadium, the Mets had little choice but to agree to share Shea with the Yankees.

On the afternoon of April 15, 1998, the Yankees also played one home game at Shea, against the Anaheim Angels after a beam collapsed at Yankee Stadium two days before, destroying several rows of seats. With the Mets playing a game at Shea that evening against the Chicago Cubs, the Yankees used the visitor's locker room and dugout and the Angels used the home dugout and old locker room of the New York Jets. Former Mets star Darryl Strawberry, then playing for the Yankees, hit a home run during the game. Stadium operators partially raised the Mets' home run apple signal before lowering it back down, to the delight of the crowd.

Shea Stadium also hosted the first extra-inning regular season baseball opener played in New York, on March 31, 1998, when the Mets opened their season against their rival Philadelphia Phillies, playing the longest scoreless opening day game in the National League and the longest one in Major League Baseball since . The Mets won the game 1–0 in the bottom of the 14th inning.

During the 1977 New York City blackout the stadium was plunged into darkness at approximately 9:30 p.m. during a game between the Mets and the Chicago Cubs. It occurred during the bottom of the sixth inning, with the Mets losing 2–1 and Lenny Randle at bat. Jane Jarvis, Shea's organist (affectionately known as Shea's "Queen of Melody") played "Jingle Bells" and "White Christmas". The game was eventually completed on September 16, with the Cubs winning 5–2.

Shea Stadium held boxing matches in the mid-1960s.

The New York Jets of the American Football League and later, the National Football League played at Shea for 20 seasons, from 1964 through 1983 (excluding their first home game in 1977, played at Giants Stadium). The stadium hosted three Jets playoff games: the American Football League Championship in 1968 (defeating the Oakland Raiders, 27–23), an AFL Divisional Playoff in 1969 (a 13–6 loss to the Kansas City Chiefs) and the 1981 AFC Wild Card Playoff game (lost 31–27 to the Buffalo Bills).

For most of the Jets' tenure at Shea, they were burdened by onerous lease terms imposed at the insistence of the Mets. Until 1978, the Jets could not play their first home game until the Mets' season was finished. For instance, in 1969, the defending Super Bowl champion Jets didn't play a home game until October 20 due to the Mets advancing to (and winning) the World Series. As a result, the 1969 Jets opened with five consecutive road games, and then played all seven home games in consecutive weeks before closing with two road games. Even after 1978, the Mets' status as Shea's primary tenants would require the Jets to go on long road trips (switching Shea from baseball to football configuration was a complex process involving electrical, plumbing, field, and other similar work). The stadium was also not well maintained in the 1970s. The Jets moved to Giants Stadium for the 1984 season, enticed by the more than 15,000 additional seats there. Fans ripped apart Shea after the last game of the 1983 season, which also was the last game for Hall of Fame quarterback Terry Bradshaw, who threw two touchdown passes to lead the Pittsburgh Steelers to a 34–7 victory. Even the scoreboard operator had a field day, displaying the home team as the "N.J. Jets".
It was at Shea Stadium on December 16, 1973 that O.J. Simpson became the first running back to gain 2,000 yards in a single season (and, to date, the only player to do it in 14 games or fewer). In the 1983 season, a Jets game against the Los Angeles Rams featured an 85-yard touchdown run by rookie Eric Dickerson, as well as a brawl between Rams offensive tackle Jackie Slater and Jets defensive end Mark Gastineau when Slater blindsided Gastineau after the Jet performed his infamous "Sack Dance" over fallen Rams quarterback Vince Ferragamo.

The NFL's New York Giants played their 1975 season at Shea while Giants Stadium was being built. The Giants were that year (2–5 at Shea). Their coach was Bill Arnsparger and their quarterback was Craig Morton. The Giants played their final five home games of 1973 and all seven in 1974 at the Yale Bowl in New Haven, Connecticut; Yankee Stadium was closed in October 1973 for a massive renovation, which was completed in time for the 1976 baseball season.

On the night of October 9, 1965, Shea Stadium hosted the football rivalry between Army and Notre Dame for the first and only time. The Fighting Irish blanked the Cadets, 17–0, beginning a 15-game winning streak for Notre Dame in the storied series.

In 1966, the Brooklyn Dodgers of the minor Continental Football League unsuccessfully sued the Jets in an attempt to use the stadium; the team wound up playing on Randall's Island and soon folded. In 1974, the New York Stars of the nascent World Football League also made inquiries to play at Shea, whose schedule was already overcrowded by the Mets, Jets and Yankees (and the following year, the Giants; see below). The Stars also moved out to Randall's Island, playing only a handful of games before shifting to Charlotte.

The football field at Shea extended from around home plate to centerfield, with the baseline seating rotating out to fill left and right fields.

The first soccer game at Shea Stadium occurred during International Soccer League tournament play on June 17, 1965.

The original New York Cosmos beat the Washington Diplomats, 2–0, in an NASL playoff game at Shea on August 17, 1976.

New York United of the American Soccer League called Shea home in 1980.

On Sunday, August 15, 1965, The Beatles opened their 1965 North American tour there to a record audience of 55,600. "Beatlemania" was at one of its peaks at their Shea concert. Film footage shows many teenagers and women crying, screaming, and even fainting. The crowd noise was such that security guards can be seen covering their ears as the Beatles entered the field. The sound of the crowd was so deafening that none of the Beatles (or anyone else) could hear what they were playing. Nevertheless, it was the first concert to be held at a major stadium and set records for attendance and revenue generation, demonstrating that outdoor concerts on a large scale could be successful and profitable, and led the Beatles to return to Shea for a successful encore on August 23, 1966. The attendance record stood until 1973 when it was broken by Led Zeppelin with 56,800 fans at Tampa Stadium.

The next major music event to play Shea Stadium after the Beatles successful appearances was the Summer Festival for Peace on August 6, 1970. It was a day-long fundraiser, which featured many of the era's biggest selling and seminal rock, folk, blues and jazz performers including: Janis Joplin, Paul Simon, Creedence Clearwater Revival, Steppenwolf, The James Gang, Miles Davis, Tom Paxton, John Sebastian, and others.

The next music show at Shea Stadium was the historic concert by Grand Funk Railroad in 1971, which broke the Beatles' then-record for fastest ticket sales. Humble Pie was the opening band. The same filmmakers for the documentary of the Rolling Stones concert at Altamont were commissioned to film it, but to date, a final film has not been released.

The stadium subsequently hosted numerous concerts, including Jethro Tull with opening act Robin Trower in July 1976 (billed as Tull v. Boeing because of the proximity to LaGuardia Airport), The Who with opening act The Clash in October 1982, and Simon & Garfunkel in August 1983. On August 18, 1983, The Police played in front of 70,000 fans at Shea, a concert that the band's singer and bassist Sting described as "like playing the top of Everest", and announced near the end of the concert: "We'd like to thank the Beatles for lending us their stadium." The Rolling Stones performed at Shea for a six-night run in October 1989, and Elton John & Eric Clapton played a concert in August 1992. Bruce Springsteen and the E Street Band ended The Rising Tour with three concerts at Shea in early October 2003, with Bob Dylan making a special guest appearance at the final show to perform "Forever Young" with Springsteen.

The last concert event was a two-night engagement by Billy Joel on July 16 and July 18, 2008. The concert was dubbed "The Last Play at Shea", and featured many special guest appearances, including former Beatle Paul McCartney who closed the second show with an emotional rendition of the Beatles classic "Let It Be". Other artists that joined Joel on stage for the shows were former Shea performer Roger Daltrey of The Who, Tony Bennett, Don Henley, John Mayer, John Mellencamp, Garth Brooks, and Steven Tyler of Aerosmith. The concert was the subject of a documentary film of the same name, which is used along with Shea's history to tell the story of changes in American suburban life.

The 1978 International Convention of Jehovah's Witnesses was held at Shea Stadium from July 12 to July 16, 1978.
Grand Funk Railroad played a sold out show at Shea in 1971.
During his tour of America in October 1979, Pope John Paul II was also among those hosted by Shea Stadium. On the morning of the Pontiff's visit, Shea Stadium was awash in torrential rain, causing ankle-deep mud puddles, and threatened to ruin the event. But as the Popemobile entered the stadium, the rain stopped although the deep mud remained.

On December 9, 1979, as part of the halftime show of a National Football League game between the New York Jets and New England Patriots, a model airplane group put on a remote control airplane display. The grand finale was a remote control airplane, weighing 40 lbs, made to look like a red flying lawnmower. The pilot lost control of the airplane, and it crashed into the stands, hitting Kevin Rourke, of Lynn, Massachusetts and John Bowen of Nashua, New Hampshire. Both suffered serious head injuries; Rourke survived but Bowen died four days later.

Between 1972 and 1980, Shea also hosted a Showdown at Shea event three separate times, by the then World Wrestling Federation. In 1980, it hosted a simulcast of the first fight between Roberto Duran and Sugar Ray Leonard, won by Duran.

From 1970 to 1987, the Cape Cod Baseball League (CCBL) played its annual all-star game at various major league stadiums. The games were interleague contests between the CCBL and the Atlantic Collegiate Baseball League (ACBL). The 1982 and 1986 games were played at Shea. The 1986 contest starred game MVP and future Cincinnati Reds all-star pitcher Jack Armstrong.

In the aftermath of the September 11 attacks, the stadium became a staging area for rescuers, its parking lots filled with food, water, medical supplies, even makeshift shelters where relief workers could sleep. Ten days later Shea reopened for the first post-attack sporting event in New York where the Mets beat the Braves, behind a dramatic home run by Mets catcher Mike Piazza.

In the television serial drama "Mad Men", the main character, Don Draper, has his secretary buy a pair of tickets for the Beatles' concert at Shea Stadium in 1965.

The ballpark was parodied as being "named after the Cuban guerilla leader Che Stadium" in The Rutles film "All You Need is Cash".

Shea Stadium was parodied as Spray Stadium in an episode of Batman 66.

In 1987, Marvel Comics rented Shea Stadium to re-enact the wedding of Spider-Man/Peter Parker and Mary Jane Watson.

Recently on VH1's documentary series "7 Ages of Rock", Shea Stadium was named the most hallowed venue in all of rock music.

In "", the stadium was destroyed in a fight between Godzilla and Crackler.

Shea Stadium was used in the 1970s for filming the 1973 movie "Bang The Drum Slowly" starring Robert De Niro and Michael Moriarty and the 1978 film "The Wiz". In the latter film, the exterior pedestrian ramps were used for a motorcycle chase scene with Michael Jackson & Diana Ross.

A scene in the 2002 movie "Two Weeks Notice" takes place at Shea.

In "Men in Black", a Mets game at Shea was featured in the film, with outfielder Bernard Gilkey dropping a fly ball after being distracted by an alien spacecraft in the sky. Shea was also featured in "Men in Black 3" which is where K and J intercept Griffin and the ArcNet in 1969 before Boris the Animal can capture it.

Shea Stadium was also the setting for two episodes of "The King of Queens": "Doug Out" (1999) and "Catching Hell" (2005).

The exterior part of the Stadium is featured in the 2006 videogame .

The Mets, Yankees, Jets and Giants all called Shea home in 1975, the only time in professional sports history that two baseball teams and two football teams shared the same facility in the same year.

As Yankee Stadium was being renovated and Giants Stadium was nearing completion, there were scheduling clashes between the four teams once the calendar turned to September. Neither the Jets nor the Giants could play "home" games at Shea Stadium until the baseball season ended for the Mets and Yankees. The matter was simplified when neither baseball team qualified for the postseason; still, there was a one-week overlap as the NFL season started on Sunday, September 21 while the MLB campaign ended on Sunday, September 28. This meant the Jets opened at home on Sunday, October 5, the third week of the season, and the Giants on Sunday, October 12, the season's fourth week. It also meant that the Giants and Jets had to play a combined 14 home games in the final 12 weeks of the 14-week NFL season. To do so, the Giants played two Saturday afternoon home games, neither of which were televised, and both of which were played the day before a Jets' Sunday home game. New York football fans thus enjoyed either the Jets or the Giants hosting a Sunday home game every weekend from October 5 through December 21. Shea wound up hosting all four teams on consecutive Sundays: Mets (September 21), Yankees (September 28), Jets (October 5) and Giants (October 12).

In total, the "Big Four" drew 3,738,546 customers to Shea: 1,730,566 by the Mets (76 home dates); 1,288,048 by the Yankees (71 home dates); 361,102 by the Jets (seven home games) and 358,830 by the Giants (also seven). Having both the Giants and Jets share Shea Stadium for one season foreshadowed what was to come in the future with the Meadowlands (a.k.a. Giants Stadium), after the Jets left Flushing Meadows for New Jersey following the 1983 NFL season. 

Shea was a circular stadium, with the grandstand forming about two-thirds of a circle around the field and ending a short distance beyond the foul lines. The remainder of the perimeter was mostly empty space beyond the outfield fences. This space was occupied by the bullpens, scoreboards, and a section of bleachers beyond the left field fence. The stadium boasted 54 restrooms, 21 escalators, seats for 57,343 fans (although as seating configuration changed constantly over the life of the stadium, that number varied often, dropping to 55,601 by the 1986 World Series, and then increased again over following years to between approximately 56,000 and 57,000, until its closing), and a massive 86' x 175' scoreboard. Also, rather than the standard light towers, Shea featured lamps along its upper reaches. Some deemed Shea a showplace, praised for its convenience, even its "elegance". The stadium's scoreboard in right field, one of the largest in MLB when it opened, weighed over 60 tons. One of its distinctive features was a giant rearview slide projector screen on the top center of the scoreboard; it was intended to display a picture of the current player at bat (a groundbreaking innovation at the time); however, due to lighting issues (it only worked at night when the light was really low; during day games, the picture would not show up at all), it was not used very often and was eventually covered with a giant Mets logo (or a Jets logo when they played).

The stadium was located close to LaGuardia Airport. For many years, interruptions for planes flying overhead were common at Shea; the noise was so loud that radio and television broadcasts could not be heard. Players would usually ask for time during noisy flight approaches and takeoffs.
Shea was originally designed with two motor-operated stands that allow the field level seats to rotate on underground tracks, allowing the stadium to be converted between a baseball and an American football/soccer configuration. In 1982, a new Mitsubishi DiamondVision screen was installed in left field. After the New York Jets football team moved to Giants Stadium in East Rutherford, New Jersey in 1984, the Mets took over operation of the stadium and retrofitted it for exclusive baseball use. As part of the refitting, Shea Stadium's exterior was painted blue and neon signs of baseball player silhouettes were added to the windscreens prior to the 1988 season. Around the same time, the original scoreboard was removed, and a new one installed in its place (fitting into the shell left behind by the old one) allowing for a much greater space for information and entertainment after the original message board was covered up by the Budweiser advertisement. Also, after years of injuries to players crashing into the wooden outfield wall, most notably to 1973 star player Rusty Staub, where one injury caused a dislocated shoulder and forced him to miss or play severely injured during that Championship Season, the original wall finally had padding added to it, as most in baseball already did, greatly reducing injuries to outfielders.
Banks of ramps that provided access from the ground to the upper levels were built around the outside circumference of the stadium. The ramps were not walled in and were visible from outside the stadium. The ramps were originally partly covered with many rectangular panels in blue and orange, the Mets' colors. These panels can be seen in the 1970s movie "The Wiz", which used the exterior pedestrian ramps for a motorcycle chase scene with Michael Jackson and Diana Ross. The 1960s-style decorations were removed in 1980. The banks of ramps resulted in the outer wall of the stadium jutting out where the banks existed.

The design also allowed for Shea Stadium to be expandable to 90,000 seats, simply by completely enclosing the grandstand. It was also designed to be later enclosed by a dome if warranted. In March 1965, a plan was formally announced to add a glass dome and add 15,000 seats. The Mets strongly objected to the proposal. The idea was later dropped after engineering studies concluded that the stadium's foundation would be unable to support the weight of the dome.

The distances to the right and left field foul poles were initially both . There was a horizontal orange line that determined where a batted ball was a home run or still in play. In 1978, Manager Joe Torre suggested moving in the fences to in the corners with a wall in front of the original brick wall, to decrease the number of disputed calls.

Originally, all of the seats were wooden, with each level having a different color. The field boxes were yellow, the loge level seats were brown, the mezzanine seats were blue, and the upper deck seats were green. Each level above the field level was divided into box seats below the entrance/exit portals and reserved seats above the portals. The box seats were a darker shade than the reserved seats. The game ticket was the same color as the seat that it represented, and the signs in the lobby for that section were the same color as the seat and the ticket. Before the 1980 baseball season, they were replaced with red (upper deck), green (mezzanine), blue (loge), and orange (field level) plastic seats.
Unlike Yankee Stadium, Shea was built on an open field, so there was no need to have it conform to the surrounding streets.

Before Shea Stadium closed in 2008, it was the only stadium in the major leagues with orange foul poles. This tradition is carried on at Citi Field as the foul poles there are the same color.

After the Jets left Shea, the exterior of the stadium was painted blue and white, two of the Mets' team colors.

In 2003, large murals celebrating the Mets' two world championships were added, covering the two ends of the grandstand. The 1986 mural was removed after the 2006 season because of deterioration (the wall was re-painted solid blue, and a window was opened on the mezzanine level where fans could view the progress of Citi Field), but the 1969 mural survived until the final game at the end of .

With its refurbishment in 1988, the scoreboard was topped by a representation of the New York Skyline, a prominent part of the team logo. After the September 11 terrorist attacks, the Twin Towers of the World Trade Center were kept unlit, with a red-white-and-blue ribbon placed over them. The scoreboard was demolished in October 2008, but the skyline was preserved and is now located on the Shake Shack in Citi Field's "Taste Of The City" food court behind the giant scoreboard in center field.

During the 2007 and 2008 seasons, the construction of Citi Field was visible beyond the left and center field walls of Shea.

From 1973 to 1979, fans could estimate the distance of home run balls, since there were several signs beyond the outfield wall giving the distance in feet from home plate, in addition to the nine markers within the field.

The Home Run Apple came out of a magic hat after every Mets home run at Shea Stadium. It was first installed in May 1980 as a symbol of the Mets' advertising slogan "The Magic Is Back!" (the hat originally said "Mets Magic" in script but was changed in the mid-1980s to a simple "Home Run" in block capital letters). A bigger apple was placed in center field at Citi Field. The original apple was installed inside Citi Field's bullpen gate and was visible from outside, on 126th Street. In 2010, the original Shea apple was relocated outside the Citi Field, in front of the Jackie Robinson Rotunda.

Four players in the National League named their children after Shea Stadium.

Actor Kevin James, a devoted Mets fan, named his youngest daughter Shea Joelle.


 

 


</doc>
<doc id="28857" url="https://en.wikipedia.org/wiki?curid=28857" title="Signal transduction">
Signal transduction

Signal transduction is the process by which a chemical or physical signal is transmitted through a cell as a series of molecular events, most commonly protein phosphorylation catalyzed by protein kinases, which ultimately results in a cellular response. Proteins responsible for detecting stimuli are generally termed receptors, although in some cases the term sensor is used. The changes elicited by ligand binding (or signal sensing) in a receptor give rise to a biochemical cascade, which is a chain of biochemical events known as a signaling pathway.

When signaling pathways interact with one another they form networks, which allow cellular responses to be coordinated, often by combinatorial signaling events. At the molecular level, such responses include changes in the transcription or translation of genes, and post-translational and conformational changes in proteins, as well as changes in their location. These molecular events are the basic mechanisms controlling cell growth, proliferation, metabolism and many other processes. In multicellular organisms, signal transduction pathways regulate cell communication in a wide variety of ways.

Each component (or node) of a signaling pathway is classified according to the role it plays with respect to the initial stimulus. Ligands are termed "first messengers", while receptors are the "signal transducers", which then activate "primary effectors". Such effectors are often linked to second messengers, which can activate "secondary effectors", and so on. Depending on the efficiency of the nodes, a signal can be amplified (a concept known as signal gain), so that one signaling molecule can generate a response involving hundreds to millions of molecules. As with other signals, the transduction of biological signals is characterised by delay, noise, signal feedback and feedforward and interference, which can range from negligible to pathological. With the advent of computational biology, the analysis of signaling pathways and networks has become an essential tool to understand cellular functions and disease, including signaling rewiring mechanisms underlying responses to acquired drug resistance.

The basis for signal transduction is the transformation of a certain stimulus into a biochemical signal. The nature of such stimuli can vary widely, ranging from extracellular cues, such as the presence of EGF, to intracellular events, such as the DNA damage resulting from replicative telomere attrition. Traditionally, signals that reach the central nervous system are classified as senses. These are transmitted from neuron to neuron in a process called synaptic transmission. Many other intercellular signal relay mechanisms exist in multicellular organisms, such as those that govern embryonic development.

The majority of signal transduction pathways involve the binding of signaling molecules, known as ligands, to receptors that trigger events inside the cell. The binding of a signaling molecule with a receptor causes a change in the conformation of the receptor, known as "receptor activation". Most ligands are soluble molecules from the extracellular medium which bind to cell surface receptors. These include growth factors, cytokines and neurotransmitters. Components of the extracellular matrix such as fibronectin and hyaluronan can also bind to such receptors (integrins and CD44, respectively). In addition, some molecules such as steroid hormones are lipid-soluble and thus cross the plasma membrane to reach nuclear receptors. In the case of steroid hormone receptors, their stimulation leads to binding to the promoter region of steroid-responsive genes.

Not all classifications of signaling molecules take into account the molecular nature of each class member. For example, odorants belong to a wide range of molecular classes, as do neurotransmitters, which range in size from small molecules such as dopamine to neuropeptides such as endorphins. Moreover, some molecules may fit into more than one class, e.g. epinephrine is a neurotransmitter when secreted by the central nervous system and a hormone when secreted by the adrenal medulla.

Some receptors such as HER2 are capable of ligand-independent activation when overexpressed or mutated. This leads to constituitive activation of the pathway, which may or may not be overturned by compensation mechanisms. In the case of HER2, which acts as a dimerization partner of other EGFRs, constituitive activation leads to hyperproliferation and cancer.

The prevalence of basement membranes in the tissues of Eumetazoans means that most cell types require attachment to survive. This requirement has led to the development of complex mechanotransduction pathways, allowing cells to sense the stiffness of the substratum. Such signaling is mainly orchestrated in focal adhesions, regions where the integrin-bound actin cytoskeleton detects changes and transmits them downstream through YAP1. Calcium-dependent cell adhesion molecules such as cadherins and selectins can also mediate mechanotransduction. Specialised forms of mechanotransduction within the nervous system are responsible for mechanosensation: hearing, touch, proprioception and balance.

Cellular and systemic control of osmotic pressure (the difference in osmolarity between the cytosol and the extracellular medium) is critical for homeostasis. There are three ways in which cells can detect osmotic stimuli: as changes in macromolecular crowding, ionic strength, and changes in the properties of the plasma membrane or cytoskeleton (the latter being a form of mechanotransduction). These changes are detected by proteins known as osmosensors or osmoreceptors. In humans, the best characterised osmosensors are transient receptor potential channels present in the primary cilium of human cells. In yeast, the HOG pathway has been extensively characterised.

The sensing of temperature in cells is known as thermoception and is primarily mediated by transient receptor potential channels. Additionally, animal cells contain a conserved mechanism to prevent high temperatures from causing cellular damage, the heat-shock response. Such response is triggered when high temperatures cause the dissociation of inactive HSF1 from complexes with heat shock proteins Hsp40/Hsp70 and Hsp90. With help from the ncRNA "hsr1", HSF1 then trimerizes, becoming active and upregulating the expression of its target genes. Many other thermosensory mechanisms exist in both prokaryotes and eukaryotes.

In mammals, light controls the sense of sight and the circadian clock by activating light-sensitive proteins in photoreceptor cells in the eye's retina. In the case of vision, light is detected by rhodopsin in rod and cone cells. In the case of the circadian clock, a different photopigment, melanopsin, is responsible for detecting light in intrinsically photosensitive retinal ganglion cells.

Receptors can be roughly divided into two major classes: intracellular and extracellular receptors.

Extracellular receptors are integral transmembrane proteins and make up most receptors. They span the plasma membrane of the cell, with one part of the receptor on the outside of the cell and the other on the inside. Signal transduction occurs as a result of a ligand binding to the outside region of the receptor (the ligand does not pass through the membrane). Ligand-receptor binding induces a change in the conformation of the inside part of the receptor, a process sometimes called "receptor activation". This results in either the activation of an enzyme domain of the receptor or the exposure of a binding site for other intracellular signaling proteins within the cell, eventually propagating the signal through the cytoplasm.

In eukaryotic cells, most intracellular proteins activated by a ligand/receptor interaction possess an enzymatic activity; examples include tyrosine kinase and phosphatases. Often such enzymes are covalently linked to the receptor. Some of them create second messengers such as cyclic AMP and IP, the latter controlling the release of intracellular calcium stores into the cytoplasm. Other activated proteins interact with adaptor proteins that facilitate signaling protein interactions and coordination of signaling complexes necessary to respond to a particular stimulus. Enzymes and adaptor proteins are both responsive to various second messenger molecules.

Many adaptor proteins and enzymes activated as part of signal transduction possess specialized protein domains that bind to specific secondary messenger molecules. For example, calcium ions bind to the EF hand domains of calmodulin, allowing it to bind and activate calmodulin-dependent kinase. PIP and other phosphoinositides do the same thing to the Pleckstrin homology domains of proteins such as the kinase protein AKT.

G protein–coupled receptors (GPCRs) are a family of integral transmembrane proteins that possess seven transmembrane domains and are linked to a heterotrimeric G protein. With nearly 800 members, this is the largest family of membrane proteins and receptors in mammals. Counting all animal species, they add up to over 5000. Mammalian GPCRs are classified into 5 major families: rhodopsin-like, secretin-like, metabotropic glutamate, adhesion and frizzled/smoothened, with a few GPCR groups being difficult to classify due to low sequence similarity, e.g. vomeronasal receptors. Other classes exist in eukaryotes, such as the "Dictyostelium" cyclic AMP receptors and fungal mating pheromone receptors.

Signal transduction by a GPCR begins with an inactive G protein coupled to the receptor; the G protein exists as a heterotrimer consisting of Gα, Gβ, and Gγ subunits. Once the GPCR recognizes a ligand, the conformation of the receptor changes to activate the G protein, causing Gα to bind a molecule of GTP and dissociate from the other two G-protein subunits. The dissociation exposes sites on the subunits that can interact with other molecules. The activated G protein subunits detach from the receptor and initiate signaling from many downstream effector proteins such as phospholipases and ion channels, the latter permitting the release of second messenger molecules. The total strength of signal amplification by a GPCR is determined by the lifetimes of the ligand-receptor complex and receptor-effector protein complex and the deactivation time of the activated receptor and effectors through intrinsic enzymatic activity; e.g. via protein kinase phosphorylation or b-arrestin-dependent internalization.

A study was conducted where a point mutation was inserted into the gene encoding the chemokine receptor CXCR2; mutated cells underwent a malignant transformation due to the expression of CXCR2 in an active conformation despite the absence of chemokine-binding. This meant that chemokine receptors can contribute to cancer development.

Receptor tyrosine kinases (RTKs) are transmembrane proteins with an intracellular kinase domain and an extracellular domain that binds ligands; examples include growth factor receptors such as the insulin receptor. To perform signal transduction, RTKs need to form dimers in the plasma membrane; the dimer is stabilized by ligands binding to the receptor. The interaction between the cytoplasmic domains stimulates the autophosphorylation of tyrosine residues within the intracellular kinase domains of the RTKs, causing conformational changes. Subsequent to this, the receptors' kinase domains are activated, initiating phosphorylation signaling cascades of downstream cytoplasmic molecules that facilitate various cellular processes such as cell differentiation and metabolism. Many Ser/Thr and dual-specificity protein kinases are important for signal transduction, either acting downstream of [receptor tyrosine kinases], or as membrane-embedded or cell-soluble versions in their own right. The process of signal transduction involves around 560 known protein kinases and pseudokinases, encoded by the human kinome 

As is the case with GPCRs, proteins that bind GTP play a major role in signal transduction from the activated RTK into the cell. In this case, the G proteins are members of the Ras, Rho, and Raf families, referred to collectively as small G proteins. They act as molecular switches usually tethered to membranes by isoprenyl groups linked to their carboxyl ends. Upon activation, they assign proteins to specific membrane subdomains where they participate in signaling. Activated RTKs in turn activate small G proteins that activate guanine nucleotide exchange factors such as SOS1. Once activated, these exchange factors can activate more small G proteins, thus amplifying the receptor's initial signal. The mutation of certain RTK genes, as with that of GPCRs, can result in the expression of receptors that exist in a constitutively activated state; such mutated genes may act as oncogenes.

Histidine-specific protein kinases are structurally distinct from other protein kinases and are found in prokaryotes, fungi, and plants as part of a two-component signal transduction mechanism: a phosphate group from ATP is first added to a histidine residue within the kinase, then transferred to an aspartate residue on a receiver domain on a different protein or the kinase itself, thus activating the aspartate residue.

Integrins are produced by a wide variety of cells; they play a role in cell attachment to other cells and the extracellular matrix and in the transduction of signals from extracellular matrix components such as fibronectin and collagen. Ligand binding to the extracellular domain of integrins changes the protein's conformation, clustering it at the cell membrane to initiate signal transduction. Integrins lack kinase activity; hence, integrin-mediated signal transduction is achieved through a variety of intracellular protein kinases and adaptor molecules, the main coordinator being integrin-linked kinase. As shown in the adjacent picture, cooperative integrin-RTK signaling determines the timing of cellular survival, apoptosis, proliferation, and differentiation.

Important differences exist between integrin-signaling in circulating blood cells and non-circulating cells such as epithelial cells; integrins of circulating cells are normally inactive. For example, cell membrane integrins on circulating leukocytes are maintained in an inactive state to avoid epithelial cell attachment; they are activated only in response to stimuli such as those received at the site of an inflammatory response. In a similar manner, integrins at the cell membrane of circulating platelets are normally kept inactive to avoid thrombosis. Epithelial cells (which are non-circulating) normally have active integrins at their cell membrane, helping maintain their stable adhesion to underlying stromal cells that provide signals to maintain normal functioning.

In plants, there are no bona fide integrin receptors identified to date; nevertheless, several integrin-like proteins were proposed based on structural homology with the metazoan receptors. Plants contain integrin-linked kinases that are very similar in their primary structure with the animal ILKs. In the experimental model plant "Arabidopsis thaliana", one of the integrin-linked kinase genes, "ILK1", has been shown to be a critical element in the plant immune response to signal molecules from bacterial pathogens and plant sensitivity to salt and osmotic stress. ILK1 protein interacts with the high-affinity potassium transporter HAK5 and with the calcium sensor CML9.

When activated, toll-like receptors (TLRs) take adapter molecules within the cytoplasm of cells in order to propagate a signal. Four adaptor molecules are known to be involved in signaling, which are Myd88, TIRAP, TRIF, and TRAM. These adapters activate other intracellular molecules such as IRAK1, IRAK4, TBK1, and IKKi that amplify the signal, eventually leading to the induction or suppression of genes that cause certain responses. Thousands of genes are activated by TLR signaling, implying that this method constitutes an important gateway for gene modulation.

A ligand-gated ion channel, upon binding with a ligand, changes conformation to open a channel in the cell membrane through which ions relaying signals can pass. An example of this mechanism is found in the receiving cell of a neural synapse. The influx of ions that occurs in response to the opening of these channels induces action potentials, such as those that travel along nerves, by depolarizing the membrane of post-synaptic cells, resulting in the opening of voltage-gated ion channels.

An example of an ion allowed into the cell during a ligand-gated ion channel opening is Ca; it acts as a second messenger initiating signal transduction cascades and altering the physiology of the responding cell. This results in amplification of the synapse response between synaptic cells by remodelling the dendritic spines involved in the synapse.

Intracellular receptors, such as nuclear receptors and cytoplasmic receptors, are soluble proteins localized within their respective areas. The typical ligands for nuclear receptors are non-polar hormones like the steroid hormones testosterone and progesterone and derivatives of vitamins A and D. To initiate signal transduction, the ligand must pass through the plasma membrane by passive diffusion. On binding with the receptor, the ligands pass through the nuclear membrane into the nucleus, altering gene expression.

Activated nuclear receptors attach to the DNA at receptor-specific hormone-responsive element (HRE) sequences, located in the promoter region of the genes activated by the hormone-receptor complex. Due to their enabling gene transcription, they are alternatively called inductors of gene expression. All hormones that act by regulation of gene expression have two consequences in their mechanism of action; their effects are produced after a characteristically long period of time and their effects persist for another long period of time, even after their concentration has been reduced to zero, due to a relatively slow turnover of most enzymes and proteins that would either deactivate or terminate ligand binding onto the receptor.

Nucleic receptors have DNA-binding domains containing zinc fingers and a ligand-binding domain; the zinc fingers stabilize DNA binding by holding its phosphate backbone. DNA sequences that match the receptor are usually hexameric repeats of any kind; the sequences are similar but their orientation and distance differentiate them. The ligand-binding domain is additionally responsible for dimerization of nucleic receptors prior to binding and providing structures for transactivation used for communication with the translational apparatus.

Steroid receptors are a subclass of nuclear receptors located primarily within the cytosol. In the absence of steroids, they associate in an aporeceptor complex containing chaperone or heatshock proteins (HSPs). The HSPs are necessary to activate the receptor by assisting the protein to fold in a way such that the signal sequence enabling its passage into the nucleus is accessible. Steroid receptors, on the other hand, may be repressive on gene expression when their transactivation domain is hidden. Receptor activity can be enhanced by phosphorylation of serine residues at their N-terminal as a result of another signal transduction pathway, a process called crosstalk.

Retinoic acid receptors are another subset of nuclear receptors. They can be activated by an endocrine-synthesized ligand that entered the cell by diffusion, a ligand synthesised from a precursor like retinol brought to the cell through the bloodstream or a completely intracellularly synthesised ligand like prostaglandin. These receptors are located in the nucleus and are not accompanied by HSPs. They repress their gene by binding to their specific DNA sequence when no ligand binds to them, and vice versa.

Certain intracellular receptors of the immune system are cytoplasmic receptors; recently identified NOD-like receptors (NLRs) reside in the cytoplasm of some eukaryotic cells and interact with ligands using a leucine-rich repeat (LRR) motif similar to TLRs. Some of these molecules like NOD2 interact with RIP2 kinase that activates NF-κB signaling, whereas others like NALP3 interact with inflammatory caspases and initiate processing of particular cytokines like interleukin-1β.

First messengers are the signaling molecules (hormones, neurotransmitters, and paracrine/autocrine agents) that reach the cell from the extracellular fluid and bind to their specific receptors. Second messengers are the substances that enter the cytoplasm and act within the cell to trigger a response. In essence, second messengers serve as chemical relays from the plasma membrane to the cytoplasm, thus carrying out intracellular signal transduction.

The release of calcium ions from the endoplasmic reticulum into the cytosol results in its binding to signaling proteins that are then activated; it is then sequestered in the smooth endoplasmic reticulum and the mitochondria. Two combined receptor/ion channel proteins control the transport of calcium: the InsP-receptor that transports calcium upon interaction with inositol triphosphate on its cytosolic side; and the ryanodine receptor named after the alkaloid ryanodine, similar to the InsP receptor but having a feedback mechanism that releases more calcium upon binding with it. The nature of calcium in the cytosol means that it is active for only a very short time, meaning its free state concentration is very low and is mostly bound to organelle molecules like calreticulin when inactive.

Calcium is used in many processes including muscle contraction, neurotransmitter release from nerve endings, and cell migration. The three main pathways that lead to its activation are GPCR pathways, RTK pathways, and gated ion channels; it regulates proteins either directly or by binding to an enzyme.

Lipophilic second messenger molecules are derived from lipids residing in cellular membranes; enzymes stimulated by activated receptors activate the lipids by modifying them. Examples include diacylglycerol and ceramide, the former required for the activation of protein kinase C.

Nitric oxide (NO) acts as a second messenger because it is a free radical that can diffuse through the plasma membrane and affect nearby cells. It is synthesised from arginine and oxygen by the NO synthase and works through activation of soluble guanylyl cyclase, which when activated produces another second messenger, cGMP. NO can also act through covalent modification of proteins or their metal co-factors; some have a redox mechanism and are reversible. It is toxic in high concentrations and causes damage during stroke, but is the cause of many other functions like relaxation of blood vessels, apoptosis, and penile erections.

In addition to nitric oxide, other electronically activated species are also signal-transducing agents in a process called redox signaling. Examples include superoxide, hydrogen peroxide, carbon monoxide, and hydrogen sulfide. Redox signaling also includes active modulation of electronic flows in semiconductive biological macromolecules.

Gene activations and metabolism alterations are examples of cellular responses to extracellular stimulation that require signal transduction. Gene activation leads to further cellular effects, since the products of responding genes include instigators of activation; transcription factors produced as a result of a signal transduction cascade can activate even more genes. Hence, an initial stimulus can trigger the expression of a large number of genes, leading to physiological events like the increased uptake of glucose from the blood stream and the migration of neutrophils to sites of infection. The set of genes and their activation order to certain stimuli is referred to as a genetic program.

Mammalian cells require stimulation for cell division and survival; in the absence of growth factor, apoptosis ensues. Such requirements for extracellular stimulation are necessary for controlling cell behavior in unicellular and multicellular organisms; signal transduction pathways are perceived to be so central to biological processes that a large number of diseases are attributed to their disregulation.
Three basic signals determine cellular growth:
The combination of these signals are integrated in an altered cytoplasmic machinery which leads to altered cell behaviour.

Following are some major signaling pathways, demonstrating how ligands binding to their receptors can affect second messengers and eventually result in altered cellular responses.


The earliest notion of signal transduction can be traced back to 1855, when Claude Bernard proposed that ductless glands such as the spleen, the thyroid and adrenal glands, were responsible for the release of "internal secretions" with physiological effects. Bernard's "secretions" were later named "hormones" by Ernest Starling in 1905. Together with William Bayliss, Starling had discovered secretin in 1902. Although many other hormones, most notably insulin, were discovered in the following years, the mechanisms remained largely unknown.

The discovery of nerve growth factor by Rita Levi-Montalcini in 1954, and epidermal growth factor by Stanley Cohen in 1962, led to more detailed insights into the molecular basis of cell signaling, in particular growth factors. Their work, together with Earl Wilbur Sutherland's discovery of cyclic AMP in 1956, prompted the redefinition of endocrine signaling to include only signaling from glands, while the terms autocrine and paracrine began to be used. Sutherland was awarded the 1971 Nobel Prize in Physiology or Medicine, while Levi-Montalcini and Cohen shared it in 1986.

In 1970, Martin Rodbell examined the effects of glucagon on a rat's liver cell membrane receptor. He noted that guanosine triphosphate disassociated glucagon from this receptor and stimulated the G-protein, which strongly influenced the cell's metabolism. Thus, he deduced that the G-protein is a transducer that accepts glucagon molecules and affects the cell. For this, he shared the 1994 Nobel Prize in Physiology or Medicine with Alfred G. Gilman. Thus, the characterization of RTKs and GPCRs led to the formulation of the concept of "signal transduction", a word first used in 1972. Some early articles used the terms "signal transmission" and "sensory transduction". In 2007, a total of 48,377 scientific papers—including 11,211 review papers—were published on the subject. The term first appeared in a paper's title in 1979. Widespread use of the term has been traced to a 1980 review article by Rodbell: Research papers focusing on signal transduction first appeared in large numbers in the late 1980s and early 1990s.
The purpose of this section is to briefly describe some developments in immunology in the 1960s and 1970s, relevant to the initial stages of transmembrane signal transduction, and how they impacted our understanding of immunology, and ultimately of other areas of cell biology. 

The relevant events begin with the sequencing of myeloma protein light chains, which are found in abundance in the urine of individuals with multiple myeloma. Biochemical experiments revealed that these so-called Bence Jones proteins consisted of 2 discrete domains –one that varied from one molecule to the next (the V domain) and one that did not (the Fc domain or the Fragment crystallizable region) . An analysis of multiple V region sequences by Wu and Kabat identified locations within the V region that were hypervariable and which, they hypothesized, combined in the folded protein to form the antigen recognition site. Thus, within a relatively short time a plausible model was developed for the molecular basis of immunological specificity, and for mediation of biological function through the Fc domain. Crystallization of an IgG molecule soon followed ) confirming the inferences based on sequencing, and providing an understanding of immunological specificity at the highest level of resolution. 

The biological significance of these developments was encapsulated in the theory of clonal selection which holds that a B cell has on its surface immunoglobulin receptors whose antigen binding site is identical to that of antibodies that are secreted by the cell when it encounters antigen, and more specifically a particular B cell clone secretes antibodies with identical sequences. The final piece of the story, the Fluid mosaic model of the plasma membrane provided all the ingredients for a new model for the initiation of signal transduction; viz, receptor dimerization.

The first hints of this were obtained by Becker et al who demonstrated that the extent to which human basophils -- for which bivalent Immunoglobulin E (IgE) functions as a surface receptor – degranulate, depends on the concentration of anti IgE antibodies to which they are exposed, and results in a redistribution of surface molecules, which is absent when monovalent ligand is used. The latter observation was consistent with earlier findings by Fanger et al . These observations tied a biological response to events and structural details of molecules on the cell surface. A preponderance of evidence soon developed that receptor dimerization initiates responses (reviewed in ) in a variety of cell types, including B cells. 

Such observations led to a number of theoretical (mathematical) developments. The first of these was a simple model proposed by Bell which resolved an apparent paradox: clustering forms stable networks; i.e. binding is essentially irreversible, whereas the affinities of antibodies secreted by B cells increases as the immune response progresses. A theory of the dynamics of cell surface clustering on lymphocyte membranes was developed by DeLisi and Perelson who found the size distribution of clusters as a function of time, and its dependence on the affinity and valence of the ligand. Subsequent theories for basophils and mast cells were developed by Goldstein and Sobotka and their collaborators , all aimed at analysis of dose response patterns of immune cells and their biological correlates . For a recent review of clustering in immunological systems see.

Ligand binding to cell surface receptors is also critical to motility, a phenomenon that is best understood in single-celled organisms. An example is the detection and response to concentration gradients by bacteria -–the classic mathematical theory appearing in . A recent account can be found in 




</doc>
<doc id="28858" url="https://en.wikipedia.org/wiki?curid=28858" title="Stone–Weierstrass theorem">
Stone–Weierstrass theorem

In mathematical analysis, the Weierstrass approximation theorem states that every continuous function defined on a closed interval can be uniformly approximated as closely as desired by a polynomial function. Because polynomials are among the simplest functions, and because computers can directly evaluate polynomials, this theorem has both practical and theoretical relevance, especially in polynomial interpolation. The original version of this result was established by Karl Weierstrass in 1885 using the Weierstrass transform.

Marshall H. Stone considerably generalized the theorem and simplified the proof . His result is known as the Stone–Weierstrass theorem. The Stone–Weierstrass theorem generalizes the Weierstrass approximation theorem in two directions: instead of the real interval , an arbitrary compact Hausdorff space is considered, and instead of the algebra of polynomial functions, approximation with elements from more general subalgebras of is investigated. The Stone–Weierstrass theorem is a vital result in the study of the algebra of continuous functions on a compact Hausdorff space.

Further, there is a generalization of the Stone–Weierstrass theorem to noncompact Tychonoff spaces, namely, any continuous function on a Tychonoff space is approximated uniformly on compact sets by algebras of the type appearing in the Stone–Weierstrass theorem and described below.

A different generalization of Weierstrass' original theorem is Mergelyan's theorem, which generalizes it to functions defined on certain subsets of the complex plane.

The statement of the approximation theorem as originally discovered by Weierstrass is as follows:

A constructive proof of this theorem using Bernstein polynomials is outlined on that page.

As a consequence of the Weierstrass approximation theorem, one can show that the space is separable: the polynomial functions are dense, and each polynomial function can be uniformly approximated by one with rational coefficients; there are only countably many polynomials with rational coefficients. Since is metrizable and separable it follows that has cardinality at most . (Remark: This cardinality result also follows from the fact that a continuous function on the reals is uniquely determined by its restriction to the rationals.)

The set of continuous real-valued functions on , together with the supremum norm , is a Banach algebra, (that is, an associative algebra and a Banach space such that for all ). The set of all polynomial functions forms a subalgebra of (that is, a vector subspace of that is closed under multiplication of functions), and the content of the Weierstrass approximation theorem is that this subalgebra is dense in .

Stone starts with an arbitrary compact Hausdorff space and considers the algebra of real-valued continuous functions on , with the topology of uniform convergence. He wants to find subalgebras of which are dense. It turns out that the crucial property that a subalgebra must satisfy is that it "separates points": a set of functions defined on is said to separate points if, for every two different points and in there exists a function in with . Now we may state:

This implies Weierstrass’ original statement since the polynomials on form a subalgebra of which contains the constants and separates points.

A version of the Stone–Weierstrass theorem is also true when is only locally compact. Let be the space of real-valued continuous functions on which vanish at infinity; that is, a continuous function is in if, for every , there exists a compact set such that on . Again, is a Banach algebra with the supremum norm. A subalgebra of is said to vanish nowhere if not all of the elements of simultaneously vanish at a point; that is, for every in , there is some in such that . The theorem generalizes as follows:

This version clearly implies the previous version in the case when is compact, since in that case . There are also more general versions of the Stone–Weierstrass that weaken the assumption of local compactness.

The Stone–Weierstrass theorem can be used to prove the following two statements which go beyond Weierstrass's result.


The theorem has many other applications to analysis, including:


Slightly more general is the following theorem, where we consider the algebra formula_1 of complex-valued continuous functions on the compact space formula_2, again with the topology of uniform convergence. This is a C*-algebra with the *-operation given by pointwise complex conjugation.

The complex unital *-algebra generated by formula_4 consists of all those functions that can be obtained from the elements of formula_4 by throwing in the constant function and adding them, multiplying them, conjugating them, or multiplying them with complex scalars, and repeating finitely many times.

This theorem implies the real version, because if a sequence of complex-valued functions uniformly approximate a given function formula_10, then the real parts of those functions uniformly approximate the real part of formula_10. As in the real case, an analog of this theorem is true for locally compact Hausdorff spaces.

Following : consider the algebra of quaternion-valued continuous functions on the compact space , again with the topology of uniform convergence. If a quaternion "q" is written in the form "q" = "a" + "ib";+ "jc" + "kd" then the scalar part a is the real number ("q" − "iqi" − "jqj" − "kqk")/4. Likewise being the scalar part of −"qi", −"qj" and −"qk" : b,c and d are respectively the real numbers (−"qi" − "iq" + "jqk" − "kqj")/4,
(−"qj" − "iqk" − "jq" + "kqi")/4 and (−"qk" + "iqj" − "jqk" − "kq")/4. Then we may state :

The space of complex-valued continuous functions on a compact Hausdorff space formula_2 i.e. formula_1 is the canonical example of a unital commutative C*-algebra formula_14. The space "X" may be viewed as the space of pure states on formula_14, with the weak-* topology. Following the above cue, a non-commutative extension of the Stone–Weierstrass theorem, which has remain unsolved, is as follows:

In 1960, Jim Glimm proved a weaker version of the above conjecture.

Let be a compact Hausdorff space. Stone's original proof of the theorem used the idea of lattices in . A subset of is called a lattice if for any two elements , the functions also belong to . The lattice version of the Stone–Weierstrass theorem states:

The above versions of Stone–Weierstrass can be proven from this version once one realizes that the lattice property can also be formulated using the absolute value which in turn can be approximated by polynomials in . A variant of the theorem applies to linear subspaces of closed under max :

More precise information is available:

Another generalization of the Stone–Weierstrass theorem is due to Errett Bishop. Bishop's theorem is as follows :

Nachbin's theorem gives an analog for Stone–Weierstrass theorem for algebras of complex valued smooth functions on a smooth manifold . Nachbin's theorem is as follows :



The historical publication of Weierstrass (in German language) is freely available from the digital online archive of the "Berlin Brandenburgische Akademie der Wissenschaften":


Important historical works of Stone include:


</doc>
<doc id="28860" url="https://en.wikipedia.org/wiki?curid=28860" title="Simile">
Simile

A simile () is a figure of speech that directly "compares" two things. Similes differ from metaphors by highlighting the similarities between two things using words such as "like", "as", or "than", while metaphors create an implicit comparison (i.e. saying something ""is"" something else). This distinction is evident in the etymology of the words: simile derives from the Latin word "simile" ("similar, like"), while metaphor derives from the Greek word "metaphor in" ("to transfer"). While similes are mainly used in forms of poetry that compare the inanimate and the living, there are also terms in which similes are used for humorous purposes and comparison


Similes are used extensively in British comedy, notably in the slapstick era of the 1960s and 1970s. In comedy, the simile is often used in negative style: "he was as daft as a brush." They are also used in comedic context where a sensitive subject is broached, and the comedian will test the audience with response to subtle implicit simile before going deeper. The sitcom "Blackadder" featured the use of extended similes, normally said by the title character. For example:

Given that similes emphasize affinities between different objects, they occur in many cultures and languages.

Sayf al-Din al-Amidi discussed Arabic similes in 1805: "On Substantiation Through Transitive Relations".

Thuy Nga Nguyen and Ghil'ad Zuckermann (2012) classify Vietnamese similes into two types: Meaning Similes and Rhyming Similes.

The following is an example:
<br>
<br>Nghèo như con mèo
<br>/ŋɛu ɲɯ kɔn mɛu/
<br>"Poor as a cat"
Whereas the above Vietnamese example is of a rhyming simile, the English simile "(as) poor as a church mouse" is only a semantic simile.



</doc>
<doc id="28861" url="https://en.wikipedia.org/wiki?curid=28861" title="Serengeti">
Serengeti

The Serengeti ( ) ecosystem is a geographical region in Africa, spanning northern Tanzania. The protected area within the region includes approximately of land, including the Serengeti National Park and several game reserves. The Serengeti hosts the second largest terrestrial mammal migration in the world, which helps secure it as one of the Seven Natural Wonders of Africa, and as one of the ten natural travel wonders of the world. 

The Serengeti is also renowned for its large lion population and is one of the best places to observe prides in their natural environment. Approximately 70 large mammal and 500 bird species are found there. This high diversity is a function of diverse habitats, including riverine forests, swamps, kopjes, grasslands, and woodlands. Blue wildebeest, gazelles, zebras, and buffalos are some of the commonly found large mammals in the region. 

The Serengeti also contains the Serengeti District of Tanzania. There has been controversy about a proposal to build a road through the Serengeti.

The name "Serengeti" is often said to be derived from the word from "seringit" in the Maasai language, Maa, meaning "endless plains". However, this etymology does not appear in Maa dictionaries.

Much of the Serengeti was known to outsiders as Maasailand. The Maasai are known as fierce warriors and live alongside most wild animals with an aversion to eating game and birds, subsisting exclusively on their cattle. Historically, their strength and reputation kept the newly arrived Europeans from exploiting the animals and resources of most of their land. A rinderpest epidemic and drought during the 1890s greatly reduced the numbers of both Maasai and animal populations. The Tanzanian government later in the 20th century re-settled the Maasai around the Ngorongoro Crater. Poaching and the absence of fires, which had been the result of human activity, set the stage for the development of dense woodlands and thickets over the next 30–50 years. Tsetse fly populations now prevented any significant human settlement in the area.

By the mid-1970s, wildebeest and the Cape buffalo populations had recovered and were increasingly cropping the grass, reducing the amount of fuel available for fires. The reduced intensity of fires has allowed acacia to once again become established.

In the 21st century, mass rabies vaccination programmes for domestic dogs in the Serengeti have not only indirectly prevented hundreds of human deaths, but also protected wildlife species such as the endangered African wild dog.

Each year around the same time, the circular great wildebeest migration begins in the Ngorongoro Conservation Area of the southern Serengeti in Tanzania and loops in a clockwise direction through the Serengeti National Park and north towards the Masai Mara reserve in Kenya. This migration is a natural phenomenon determined by the availability of grazing. The initial phase lasts from approximately January to March, when the calving season begins – a time when there is plenty of rain-ripened grass available for the 260,000 zebra that precede 1.7 million and the following hundreds of thousands of other plains game, including around 470,000 gazelles.

During February, the wildebeest spend their time on the short grass plains of the southeastern part of the ecosystem, grazing and giving birth to approximately 500,000 calves within a 2 to 3-week period. Few calves are born ahead of time and of these, hardly any survive. The main reason is that very young calves are more noticeable to predators when mixed with older calves from the previous year. As the rains end in May, the animals start moving northwest into the areas around the Grumeti River, where they typically remain until late June. The crossings of the Grumeti and Mara rivers beginning in July are a popular safari attraction because crocodiles are lying in wait. The herds arrive in Kenya in late July / August, where they stay for the remainder of the dry season, except that the Thomson's and Grant's gazelles move only east/west. In early November, with the start of the short rains the migration starts moving south again, to the short grass plains of the southeast, usually arriving in December in plenty of time for calving in February.

About 250,000 wildebeest die during the journey from Tanzania to the Maasai Mara National Reserve in southwestern Kenya, a total of . Death is usually from thirst, hunger, exhaustion, or predation.

The Serengeti has some of East Africa's finest game areas. Besides being known for the great migration, the Serengeti is also famous for its abundant large predators. The ecosystem is home to over 3,000 Lions, 1,000 African leopards, and 7,700 to 8,700 spotted hyenas ("Crocuta crocuta).".The East African cheetah are also present in Serengeti.

Wild dogs are relatively scarce in much of the Serengeti. This is particularly true in places such as Serengeti National Park (where they became extinct in 1992), in which lions and spotted hyenas, predators that steal wild dog kills and are a direct cause of wild dog mortality, are abundant.

The Serengeti is also home to a diversity of grazers, including Cape buffalo, African Elephant, Warthog, Grant's Gazelle, Eland, Waterbuck, and Topi. The Serengeti can support this remarkable variety of grazers only because each species, even those that are closely related, has a different diet. For example, wildebeests prefer to consume shorter grasses, while Plains Zebras prefer taller ones. Similarly, Dik-Diks eat the lowest leaves of a tree, impalas eat the leaves that are higher up, and Giraffes eat leaves that are even higher.The governments of Tanzania and Kenya maintain a number of protected areas, including national parks, conservation areas, and game reserves, that give legal protection to over 80 percent of the Serengeti.

The southeastern area lies in the rain shadow of the Ngorongoro Conservation Area's highlands and is composed of shortgrass treeless plains with abundant small dicots. Soils are high in nutrients, overlying a shallow calcareous hardpan due to natrocarbonatite eruptions from Ol Doinyo Lengai. A gradient of soil depth northwestward across the plains results in changes in the herbaceous community and taller grass. About west, acacia woodlands appear suddenly and stretch west to Lake Victoria and north to the Loita Plains, north of the Maasai Mara National Reserve. The sixteen acacia species vary over this range, their distribution determined by edaphic conditions. Near Lake Victoria, floodplains have developed from ancient lakebeds.

In the far northwest, acacia woodlands are replaced by broadleaved Terminalia-Combretum woodlands, caused by a change in geology. This area has the highest rainfall in the system and forms a refuge for the migrating ungulates at the end of the dry season.Altitudes in the Serengeti range from with mean temperatures varying from . Although the climate is usually warm and dry, rainfall occurs in two rainy seasons: March to May, and a shorter season in October and November. Rainfall amounts vary from a low of in the lee of the Ngorongoro highlands to a high of on the shores of Lake Victoria. The highlands, which are considerably cooler than the plains and are covered by montane forest, mark the eastern border of the basin in which the Serengeti lies.

The Serengeti plain is punctuated by granite and gneiss outcroppings known as kopjes. These outcroppings are the result of volcanic activity. Kopjes provide a microhabitat for non-plains wildlife. One kopje likely to be seen by visitors to the Serengeti is the Simba Kopje (Lion Kopje).

The area is also home to the Ngorongoro Conservation Area, which contains Ngorongoro Crater and the Olduvai Gorge, where some of the oldest hominin fossils have been found.





</doc>
<doc id="28863" url="https://en.wikipedia.org/wiki?curid=28863" title="Sea of Marmara">
Sea of Marmara

The Sea of Marmara (; ; ), also known as the Sea of Marmora or the Marmara Sea, and in the context of classical antiquity as the Propontis, is the inland sea, entirely within the borders of Turkey, that connects the Black Sea to the Aegean Sea, thus separating Turkey's Asian and European parts. The Bosphorus strait connects it to the Black Sea and the Dardanelles strait to the Aegean Sea. The former also separates Istanbul into its Asian and European sides. The Sea of Marmara is a small sea with an area of , and dimensions . Its greatest depth is .

The sea takes its name from Marmara Island, which is rich in sources of marble, from the Greek ("mármaron"), "marble".

The sea's ancient Greek name "Propontis" derives from "pro-" (before) and "pontos" (sea), deriving from the fact that the Greeks sailed through it to reach the Black Sea, "Pontos". In Greek mythology, a storm on Propontis brought the Argonauts back to an island they had left, precipitating a battle where either Jason or Heracles killed King Cyzicus, who mistook them for his Pelasgian enemies.

The surface salinity of the sea averages about 22 parts per thousand, which is slightly greater than that of the Black Sea, but only about two-thirds that of most oceans. The water is much more saline at the sea bottom, averaging salinities of around 38 parts per thousand, similar to that of the Mediterranean Sea. This high-density saline water, like that of the Black Sea, does not migrate to the surface. Water from the Susurluk, Biga (Granicus) and Gonen Rivers also reduces the salinity of the sea, though with less influence than on the Black Sea. With little land in Thrace draining southward, almost all of these rivers flow from Anatolia.

The sea contains the archipelago of the Prince Islands and Marmara Island, Avşa and Paşalimanı.

The south coast of the sea is heavily indented, and includes the Gulf of İzmit (), the Gulf of Gemlik (), Gulf of Bandırma () and the Gulf of Erdek (). During a storm on December 29, 1999, the Russian oil tanker "Volgoneft" broke in two in the Sea of Marmara, and more than 1,500 tonnes of oil were spilled into the water.

The North Anatolian Fault, which has triggered many major earthquakes in recent years, such as the August and November 1999 earthquakes in Izmit and Düzce, respectively, runs under the sea.

The International Hydrographic Organization defines the limits of the Sea of Marmara as follows:

Towns and cities on the Marmara Sea coast include:



</doc>
<doc id="28866" url="https://en.wikipedia.org/wiki?curid=28866" title="Saint John, New Brunswick">
Saint John, New Brunswick

Saint John is a seaport city of the Atlantic Ocean located on the Bay of Fundy in the province of New Brunswick, Canada. Saint John is the oldest incorporated city in Canada, established by royal charter on May 18, 1785, during the reign of King George III. The port is Canada's third largest port by tonnage with a cargo base that includes dry and liquid bulk, break bulk, containers, and cruise. Historically New Brunswick's largest city, in 2016 the city fell to second place, with a population of 67,575 over an area of . Greater Saint John covers a land area of across the Caledonia Highlands, with a growing population of 126,202 (as of 2016).

French colonist Samuel de Champlain landed at Saint John Harbour on June 24, 1604 (the feast of St. John the Baptist) and is where the Saint John River gets its name although Mi'kmaq and Wolastoqiyik peoples lived in the region for thousands of years prior calling the river Wolastoq. The Saint John area was an important area for trade and defence for Acadia during the French colonial era and Fort La Tour, in the city's harbour, was a pivotal battleground during the Acadian Civil War.

After over a century of ownership disputes over the land surrounding Saint John between the French and English, the English deported the French colonists in 1755 and constructed Fort Howe above the harbour in 1779. In 1785, the City of Saint John was established by uniting the two towns of Parrtown and Carleton on each side of the harbour after the arrival of thousands of refugees from the American Revolution who wished to remain British and were forced to leave their U.S. homes. Over the next century, waves of immigration via Partridge Island, especially during the Great Famine, would fundamentally change the city's demographics and culture.

The area has been the home of peoples of the Wabanaki Confederacy for thousands of years. The northwestern coastal region of the Bay of Fundy inhabited by the Passamaquoddy Nation, while the Saint John River valley north of the bay became the domain of the Wolastoqiyik Nation. The Mi'kmaq also ventured into the Saint John area regularly as the harbour and coast was an important hunting ground for seals. The area around the harbour, where the city is, has been traditionally called Menahkwesk by the Wolastoqiyik people, who still live in and around the city today. In pre-colonial times the Wolastoqiyik lived in mostly self-sustaining villages living largely off bass, sturgeon, salmon, corn, wild roots and berries.

Samuel de Champlain landed at Saint John Harbour in 1604, though he did not settle the area. Saint John was a key area for trade and defence for Acadia during the French colonial era. Moreover, Fort La Tour in the city's harbour, was a pivotal battleground during the Acadian Civil War. The region was conquered by the British after a century of English and French warfare by the end of the Seven Years' War. After being incorporated as a city in 1785 with an influx of Black and White British Loyalists from the northern of the former Thirteen Colonies and also immigrants from Ireland and Italy, the city grew as a global hub for shipping and shipbuilding. After the partitioning of the colony of Nova Scotia in 1784, the new colony of New Brunswick was thought to be named 'New Ireland' with the capital to be in Saint John before being vetoed by George III. In 1851 the city cemented itself as a global shipbuilding hub when the , built from a Saint John yard, became the fastest in the world.

However, as the city grew in strategic importance to English power and capital, unrest grew among many of its working class. Black Saint Johners were forbidden from trade, fishing and voting, thus the majority of the city's Black community settled in Portland (the city's north end), which later became amalgamated with Saint John. From 1840 to 1860 sectarian violence was rampant in Saint John, as tensions grew in reaction to poor living conditions of poor Irish Catholics resulting in some of the worst urban riots in Canadian history. The city experienced a cholera outbreak in 1854 with the death of over 1,500 people, as well as a great fire in 1877 that destroyed 40% of the city and left 20,000 people homeless.


Situated in the south-central portion of the province, along the north shore of the Bay of Fundy at the mouth of the Saint John River, the city is split by the south-flowing river and the east side is bordered on the north by the Kennebecasis River where it meets the Saint John River at Grand Bay. Saint John Harbour, where the two rivers meet the Bay of Fundy, is a deep water port and ice-free all year long. Partridge Island is in the harbour.

Stonehammer UNESCO Geopark, the first Geopark in North America, is centred around Saint John. The Geopark has been recognized by UNESCO as having exceptional geological significance. The park contains rock formations that date back to the Precambrian era and some of the rocks may be a billion years old.

The Saint John River itself flows into the Bay of Fundy through a narrow gorge several hundred metres wide at the centre of the city. It contains a unique phenomenon called the Reversing Falls where the diurnal tides of the bay reverse the water flow of the river for several kilometres. A series of underwater ledges at the narrowest point of this gorge also create a series of rapids.

The topography surrounding Saint John is hilly; a result of the influence of two coastal mountain ranges which run along the Bay of Fundy – the "St. Croix Highlands" and the "Caledonia Highlands". The soil throughout the region is extremely rocky with frequent granite outcrops. The coastal plain hosts numerous freshwater lakes in the eastern, western and northern parts of the city.

In Saint John the height difference from low to high tide is approximately 8 metres (28 ft) due to the funnelling effect of the Bay of Fundy as it narrows. The Reversing Falls in Saint John, actually an area of strong rapids, provides one example of the power of these tides; at every high tide, ocean water is pushed through a narrow gorge in the middle of the city and forces the Saint John River to reverse its flow for several hours.


The climate of Saint John is humid continental (Köppen climate classification ""). The Bay of Fundy never fully freezes, thus moderating the winter temperatures compared with inland locations. Even so, with the prevailing wind blowing from the west (from land to sea), the average January temperature is about . Summers are usually warm to hot, and daytime temperatures often exceed . The highest temperature recorded in a given year is usually . The confluence of cold Bay of Fundy air and inland warmer temperatures often creates onshore winds that bring periods of fog and cooler temperatures during the summer months.

Precipitation in Saint John totals about annually and is well distributed throughout the year, although the late autumn and early winter are typically the wettest time of year. Snowfalls can often be heavy, but rain is as common as snow in winter, and it is not unusual for the ground to be snow-free even in mid-winter.

The highest temperature ever recorded in Saint John was on June 22, 1941, August 15, 1944, and August 22, 1976. The coldest temperature ever recorded was on February 11, 1948.


The population of the city declined from the 1970s to the early 21st century. This trend reversed itself and the city's population increased in the 2011 census, but then declined again by 2016. Saint John was New Brunswick's largest city until 2016.

In 2011, the population of the Greater Saint John area was 127,761, of whom 49% were male and 51% female. Children under fifteen accounted for approximately 16% of the population. People 65 and over accounted for approximately 15% of the population. When the census was taken in May 2011 the population of the Saint John metropolitan area was 127,761 compared with 122,389 in 2006.

Historically, as one of Canada's main ports, Saint John has been a centre for immigration from all over the world. The city was incorporated in the late 1700s after more than 3,300 Black Loyalist refugees came to Saint John along with more than 10,000 white refugees after the American Revolution. In the years between 1815 and 1867, when immigration of that era passed its peak, more than 150,000 immigrants from Ireland came to Saint John dramatically changing the city.

Those who came in the earlier period were largely tradesmen, and many stayed in Saint John, becoming the backbone of its builders. But when the Great Irish Potato Famine raged between 1845 and 1852, huge waves of famine refugees flooded the city's shores. It is estimated that between 1845 and 1847, some 30,000 arrived, more people than were living in the city at the time. In 1847, dubbed "Black 47", one of the worst years of the famine, some 16,000 immigrants, most of them from Ireland, arrived at Partridge Island, the immigration and quarantine station at the mouth of Saint John Harbour.

As of the 2016 census, approximately 87.7% of the residents were white, while 7% were visible minorities and 5.3% were aboriginal. The largest visible minority groups were Black (2.1%), Chinese (1.4%), Arab (0.9%), and South Asian (0.7%). 5% of Saint Johners are francophone.

With regard to religion, 89.2% identify as Christian (47.6% Protestant, 40.3% Roman Catholic, and 1.3% other Christian, mostly Orthodox and independent churches). 10.1% state no religious affiliation, and other religions including Islam, Judaism, Buddhism, and Hinduism together comprise less than 1%.

Saint John is one of five chartered cities in Canada, giving it unique legislative powers.

Saint John is governed by a body of elected officials, referred to as "Common Council", whose responsibilities include:

The Common Council consists of:
One is elected by the council to serve as Deputy Mayor.

As of 2017, the council's members are:

In the October 9, 2007 Plebiscite, it was decided that as of the May 2008 quadrennial municipal elections, the city will be divided into four wards of approximately equal population, with two councillors to be elected by the voters in that ward, and two councillors to be elected at large.

Saint John derived its economy from maritime industries such as shipping, fishing and shipbuilding. Saint John has a long history of shipbuilding at the city's dry dock, which is one of the largest in the world. Since 2003 shipbuilding has ended on the scale it once was, forcing the city to adopt a new economic strategy. The University of New Brunswick, the New Brunswick Museum and the New Brunswick Community College are important institutions, and along with Radian6, Horizon Health Network and many others, they are a part of Saint John's fast-growing research and information-technology sectors. As the city moves away from its industrial past it now begins to capitalize on the growing sector of tourism, hosting over 1.5 million visitors a year and 200,000 cruise ship visitors a year, creating a renaissance in the city's historic downtown (locally known as uptown). Many small businesses have moved into Uptown and large scale waterfront developments are underway, such as the Fundy Quay (condo, hotel and office space), Saint John Law Courts, and the Three Sisters Harbourfront condos.

The arts and culture sector plays a large role in Saint John's economy. The Imperial Theatre is home to the highly acclaimed Saint John Theatre Company, and the Symphony New Brunswick and hosts a large collection of plays, concerts and other stage productions year-round. Harbour Station entertainment complex is home to the Saint John Sea Dogs of the QMJHL and the Saint John Riptide of the NBL.

Art galleries in Saint John cover the uptown, more than any other Atlantic Canadian city. Artists like Miller Brittain and Fred Ross have made Uptown Saint John their home, and now the torch has been passed to artists like Gerard Collins, Cliff Turner and Peter Salmon and their respective galleries. Uptown art galleries also include the Paris Crew, Trinity Galleries, Citadel Gallery, Handworks Gallery and the Saint John Arts Centre (SJAC). The SJAC in the Carnegie Building hosts art exhibits, workshops, local songwriters' circles and other shows too small to be featured at the grand Imperial Theatre.

Saint John maintains industrial infrastructure in the city's East side such as Canada's largest oil refinery as well as the country's largest dry dock. Capitalist K.C. Irving and his family built his unfettered industrial conglomerate in the city by buying up mills, shipyards, media outlets, and other industrial infrastructure during the 20th century, and still continue to this day. Today Irving dominates the city and province with stakes in oil, forestry, shipbuilding, media and transportation. Irving companies remain dominant employers in the region with North America's first deepwater oil terminal, a pulp mill, a paper mill and a tissue paper plant.

Other important economic activity in the city is generated by the Port of Saint John.

Saint John has a long history of brewers, such as Simeon Jones, The Olands, and James Ready. The city is now home to Moosehead Breweries, James Ready Brewing Co., Big Tide Brewing Co., Picaroon's and other craft brewers. The Moosehead Brewery (established in 1867, is Canada's only nationally distributed independent brewery [M. Nicholson]), James Ready Brewing Co., the New Brunswick Power Corporation which operates three electrical generating stations in the region including the Point Lepreau Nuclear Generating Station, Bell Aliant which operates out of the former New Brunswick Telephone headquarters, the Horizon Health Network, which operates 5 hospitals in the Saint John area, and numerous information technology companies. There are also a number of call centres which were established in the 1990s under provincial government incentives.

Saint John is a major Canadian port, and the only city on the Bay of Fundy. Until the first decade of the 21st century, Canada's largest shipyard (Irving Shipbuilding) had been an important employer in the city. During the 1980s-early 1990s the shipyard was responsible for building 9 of the 12 multi-purpose patrol frigates for the Canadian Navy. However, the Irving family closed the shipyard in 2003 and centralized in Halifax leaving the Saint John dry dock sitting idle.

Ecological research on surrounding marine life of the Bay of Fundy and the Saint John and Kennebecasis Rivers is centred in the city. The University of New Brunswick's Marine Biology department in Saint John as well as local NGO's and the federal Department of Fisheries and Oceans heads the majority of research and monitoring work on marine life and environments.

Traditional fisheries (lobster, scallops etc.) still make up the livelihood for many Saint Johners today. Aquaculture, primarily Atlantic Salmon farming, has grown to be a major employer in the region as the decline of other traditional wild fisheries has unfolded in recent decades. Cooke Aquaculture, one of the largest companies in the industry is headquartered in Saint John.

Prior to the opening of the St. Lawrence Seaway in 1959, the Port of Saint John functioned as the winter port for Montreal, Quebec when shipping was unable to traverse the sea ice in the Gulf of St. Lawrence and St. Lawrence River. The Canadian Pacific Railway opened a line to Saint John from Montreal in 1889 across the state of Maine and transferred the majority of its trans-Atlantic passenger and cargo shipping to the port during the winter months. The port fell into decline following the seaway opening and the start of year-round icebreaker services in the 1960s. In 1994 CPR left Saint John when it sold the line to shortline operator New Brunswick Southern Railway. The Canadian National Railway still services Saint John with a secondary mainline from Moncton. Despite these setbacks, Port Saint John is the largest port by volume in Eastern Canada, at about 28 million metric tonnes of cargo per year, including containers and bulk cargo.

Besides being the location of several historical forts, such as Fort Howe, Fort Dufferin, Fort Latour, and the Carleton Martello Tower, Saint John is the location of a number of reserve units of the Canadian Forces.


Saint John is often described as the birthplace of unionism in Canada and is one of the few pre-capitalist colonial settlements in North America. The city has a history of labour achievements and sparked the Canadian labour movement with Canada's first trade union, the Labourers' Benevolent Association (now International Longshoremen's Association Local 273). In 1849 the union was formed when Saint John's longshoremen banded together to lobby for regular pay and a shorter workday. One of their first resolutions was to apply to the city council for permission to erect the bell, which would announce the beginning and end of the labourers' 10-hour workday. As the bell shears were hardly finished when capitalists and merchants in the city objected to the bell and successfully lobbied city hall to keep the bell from being put up. But then, citizens and longshoremen defied the order and erected a larger bell and merchants withdrew their opposition to the "Labourers' Bell". ILA Local 273 remain one of the city's strongest trade unions to this day.

The 1914 Saint John street railway strike (sometimes called the "Saint John street railwaymen's strike") was a strike by workers on the street railway system in the city which lasted from July 22 to 24, 1914, with rioting by Saint John inhabitants occurring on July 23 and 24. The strike was important for shattering the image of Saint John as a conservative town dominated primarily by ethnic and religious (rather than class) divisions, and highlighting tensions between railway industrialists and the local working population.

The Saint John General Strike of 1976 was a result of the Bill C-73 passed by Prime Minister of Canada, Pierre Elliott Trudeau, and the House of Commons in Ottawa on October 14, 1975. This bill limited wage increases to 8% the first year, 6% the second year, and 4% the third year after its enactment. Most provinces of Canada accepted the bill by spring of 1976, but within eighteen months they began to withdraw from the program. After its introduction in 1975, it was not until 1976 that the Anti-Inflation Board (AIB) began to roll back workers' wages. The employees of Irving Pulp and Paper, members of the Canadian Paper Workers Union, were among the first to experience the roll backs implemented by the AIB. The paper workers were required to give back to the employer 9.8% of their previous wage increase the first year, and 11% the second year. The Atlantic Sugar Refinery workers of the Bakery and Confectionary Workers International Union of America soon felt the burden as well. The majority of workers within Saint John were influenced by the AIB by January 1976. On February 5, 1976, the Saint John District and the Labour Council held a conference to plan an organized opposition to the AIB. Fifty-two people came to the meeting as representatives of twenty-six unions in Saint John. The council was led by the Labour Council president, George Vair. They began by educating those present on wage control legislation, but swiftly transitioned into rallying and demonstrating in opposition throughout the city. Five thousand marched from numerous ends of the town to King Square. All major industries in Saint John were shut down.

On May 12, 1994, at 4:30 pm, members of Local 691 of the Communications, Energy and Paperworkers (CEP) union at the Irving Oil Ltd. Refinery went on strike. At this time the refinery's management took over its operations. Irving had argued the refinery might have to shut down and had to bring in a bevy of rollbacks to the workers’ pay and benefits and other changes to the collective agreement. Local 691 argued Irving simply wished to lengthen the work week without paying workers overtime rates. The strike lasted 27 months and was based on Irving's demands for flexibility of the workers to ensure the refinery was competitive. The strike is seen as symbolic of a rollback of labour and democratic collective bargaining rights that have been in decline across North America.

Air service into Saint John is provided by the Saint John Airport/Aéroport de Saint-Jean, near Loch Lomond east northeast of the central business district or approximately by road northeast of the city centre. Flights are offered by Sunwing Airlines (seasonal) and Air Canada (Air Canada Express and Air Canada Rouge). In 2011, WestJet decided to withdraw from the Saint John Airport. Quebec-based Pascan Aviation announced its expansion into Saint John in late 2012, with direct flights from Saint John to Quebec City, Newfoundland, and other destinations beginning in September 2012. Porter Airlines flies once daily from Saint John, to Ottawa and Toronto Island Airport.

The main highway in the city is the Saint John Throughway (Route 1). Route 1 extends west to the United States border, and northeast towards both Prince Edward Island and Nova Scotia. A second major highway, Route 7, connects Saint John with Fredericton. There are two main road crossings over the Saint John River: the Harbour Bridge and the Reversing Falls Bridge, approximately upstream.

The Reversing Falls Railway Bridge carries rail traffic for the New Brunswick Southern Railway on the route from Saint John to Maine. Saint John was serviced by the "Atlantic" Line of Via Rail passenger service. Passenger rail service in Saint John was discontinued in December 1994, although the Canadian National Railway and New Brunswick Southern Railway continue to provide freight service.

Port Saint John is located where the Saint John River meets the Atlantic Ocean. Thus both the ocean and the river system is navigable from Saint John docks.Bay Ferries operates a ferry service, , across the Bay of Fundy to Digby, Nova Scotia. The Summerville to Millidgeville Ferry, a free propeller (as opposed to cable) ferry service operated by the New Brunswick Department of Transportation, connects the Millidgeville neighbourhood with Summerville, New Brunswick, across the Kennebecasis River on the Kingston Peninsula.

Saint John Transit is the largest transit system in New Brunswick in both area coverage and ridership. Bus service is provided by Saint John Transit (Greater Saint John Area) and Maritime Bus (Inter-city). Acadian Lines used to operate regular inter-city bus services between New Brunswick, Prince Edward Island, Nova Scotia, Bangor, as well as Rivière-du-Loup, Quebec (connecting with Orléans Express). Maritime Bus has since replaced Acadian Lines as the regional bus service.

The city has always been a traditional hub for the arts on the east coast, boasting many notable artists, actors and musicians, including Walter Pidgeon, Donald Sutherland, Louis B. Mayer, and Miller Brittain.
What is considered the golden age of the Saint John arts community was during the post-war era from 1940 to 1970 when the city produced renowned artists and writers such as poet Kay Smith, painters Jack Humphrey, Miller Brittain, Bruno Bobak, Fred Ross, and sculptor John Hooper and folk-singer Tom Connors. Poet Bliss Carman once wrote about Saint John, "All the beauty and mystery Of life were there, adventure bold, Youth, and the glamour of the sea, And all its sorrows old." 




Early settlers influenced music in Saint John from the time the area had been a series of forts for the English and French colonists. Working class fishers, labourers and shipbuilders carried Maritime traditions and folk songs with kitchen parties and outdoor gatherings. But musical high culture was captured by the wealthy. New Brunswick's solicitor-general 1784–1808, Ward Chipman Sr was known to have fancy soirées at his home with all the latest songs from London. A notable Loyalist musician, Stephen Humbert, moved in 1783 from New Jersey to Saint John and opened a Sacred Vocal Music School. In 1801 Humbert published Union Harmony, the first Canadian music book in English. The Mechanics' Institute, built in 1840, was the first large-scale platform for comic opera and concerts. In 1950 The Saint John Symphony was founded by Kelsey Jones; by 1983 the organization became Symphony New Brunswick. Some musicians from Saint John include Berkley Chadwick, Stompin' Tom Connors, Ken Tobias, Blank Banshee, Stevedore Steve, Jane Coop, Bruce Holder, Frances James, the songwriter Michael F. Kelly, Ned Landry, the composer and teacher Edward Betts Manning, Paul Murray, Catherine McKinnon, Patricia Rideout, Philip Thomson, and the tenor and choir conductor Gordon Wry.

Music festivals have long been a part of the city's cultural scene. New Brunswick's Music Festival was held in Saint John every Spring in the early- to mid-20th century. As the city's music changed with the times, so did its festivals. Other popular festivals include the now defunct Festival By The Sea and Salty Jam catering to various genres of pop music.

The Area 506 music festival is held every New Brunswick Day long-weekend at Long Wharf on Saint John Harbour. The festival is set up with shipping containers from the port with vendors from New Brunswick companies to promote local business. A main stage area is also set up for night time shows with local acts as well as major groups. Major bands to have played Area 506 include Tegan and Sara, Stars, Bahamas, Interpol, and Arkells. Each year the festival also includes a bevy of bands coming out of the Saint John music scene. Quality Block Party music festival hosts independent New Brunswick musicians in smaller venues throughout uptown Saint John. The festival gets its name from the old quality block on Germain Street.

The following teams are based in Saint John:

The following sporting events have been held here:

In 1964, the University of New Brunswick created UNB Saint John in buildings throughout the uptown CBD. In 1968, UNBSJ opened a new campus in the city's Tucker Park neighbourhood. This campus has undergone expansion over the years and is the fastest-growing component of the UNB system, with many new buildings constructed from the 1970s to the first decade of the 21st century. A trend in recent years has been a growth in the number of international students. The city also hosts a New Brunswick Community College campus in the East End of the city. There has also been a satellite campus of Dalhousie Medical School added within the UNBSJ campus in 2010, instructing 30 medical students each year.

In the fall of 2007, a report commissioned by the provincial government recommended UNBSJ and the NBCC be reformed and consolidated into a new polytechnic post-secondary institute. The proposal immediately came under heavy criticism and led to the organizing of several protests in the uptown area, citing the diminishment of UNB as a nationally accredited university, the reduction in accessibility to receive degrees – and these are only a couple of the reasons why the community was enraged by the recommendation. Support for keeping UNBSJ as it was, and expanding the university under its current structure, fell slightly below 90%. Seeing too much political capital would be lost, and several Saint John MPs were likely not to support the initiative if the policies recommended by the report were legislated, the government abandoned the commission's report and created an intra-provincial post-secondary commission.

Saint John is served by two school boards: Anglophone South School District schools and Francophone Sud School District (based out of Dieppe, New Brunswick) for the city's only Francophone school, Centre-Scolaire-Communautaire Samuel-de-Champlain. Saint John is also home to Canada's oldest publicly funded school, Saint John High School. The other high schools in the city are Harbour View High School, St. Malachy's High School, and Simonds High School.





</doc>
<doc id="28867" url="https://en.wikipedia.org/wiki?curid=28867" title="Sigyn">
Sigyn

In Norse mythology, Sigyn (Old Norse "victorious girl-friend") is a goddess and is the wife of Loki. Sigyn is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In the "Poetic Edda", little information is provided about Sigyn other than her role in assisting Loki during his captivity. In the "Prose Edda", her role in helping her husband through his time spent in bondage is stated again, she appears in various kennings, and her status as a goddess is mentioned twice. Sigyn may appear on the Gosforth Cross and has been the subject of an amount of theory and cultural references.

Sigyn is attested in the following works:

In stanza 35 of the "Poetic Edda" poem "Völuspá", a völva tells Odin that, amongst many other things, she sees Sigyn sitting very unhappily with her bound husband, Loki, under a "grove of hot springs". Sigyn is mentioned a second (and final) time in the ending prose section of the poem "Lokasenna". In the prose, Loki has been bound by the gods with the guts of his son Nari, his son Váli is described as having been turned into a wolf, and the goddess Skaði fastens a venomous snake over Loki's face, from which venom drips. Sigyn, again described as Loki's wife, holds a basin under the dripping venom. The basin grows full, and she pulls it away, during which time venom drops on Loki, causing him to writhe so violently that earthquakes occur that shake the entire earth.

Sigyn appears in the books "Gylfaginning" and "Skáldskaparmál" in the "Prose Edda". In "Gylfaginning", Sigyn is introduced in chapter 31. There, she is introduced as the wife of Loki, and that they have a son by the name of "Nari or Narfi". Sigyn is mentioned again in "Gylfaginning" in chapter 50, where events are described differently than in "Lokasenna". Here, the gods have captured Loki and his two sons, who are stated as Váli, described as a son of Loki, and "Nari or Narfi", the latter earlier described as also a son of Sigyn. Váli is changed into a wolf by the gods, and rips apart his brother "Nari or Narfi". The guts of "Nari or Narfi" are then used to tie Loki to three stones, after which the guts turn to iron, and Skaði places a snake above Loki. Sigyn places herself beside him, where she holds out a bowl to catch the dripping venom. However, when the bowl becomes full she leaves to pour out the venom. As a result, Loki is again described as shaking so violently that the planet shakes, and this process repeats until he breaks free, setting Ragnarök into motion.

Sigyn is introduced as a goddess, an ásynja, in the "Prose Edda" book "Skáldskaparmál", where the gods are holding a grand feast for the visiting Ægir, and in kennings for Loki: "husband of Sigyn", "cargo [Loki] of incantation-fetter's [Sigyn's] arms", and in a passage quoted from the 9th-century "Haustlöng", "the burden of Sigyn's arms". The final mention of Sigyn in "Skáldskaparmál" is in the list of ásynjur in the appended Nafnaþulur section, chapter 75.

The mid-11th century Gosforth Cross located in Cumbria, England, has been interpreted as featuring various figures from Norse mythology. The bottom portion of the west side of the cross features a depiction of a long-haired female, kneeling figure holding an object above another prostrate, bound figure. Above and to their left is a knotted serpent. This has been interpreted as Sigyn soothing the bound Loki.

While the name "Sigyn" is found as a female personal name in Old Norse sources (Old Norse "sigr" meaning 'victory' and "vina" meaning 'girl-friend'), and though in surviving sources she is largely restricted to a single role, she appears in the 9th century skaldic poem "Haustlöng" from pagan times, written by the skald Þjóðólfr of Hvinir. Due to this early connection with Loki, Sigyn has been theorized as being a goddess dating back to an older form of Germanic paganism.

The scene of Sigyn Loki has been depicted on a number of paintings, including "Loke och Sigyn" (1850) by Nils Blommér, "Loke och Sigyn" (1863) by Mårten Eskil Winge, "Loki och Sigyn (1879) by Oscar Wergeland, and the illustration "Loki und Sigyn; Hel mit dem Hunde Garm" (1883) by K. Ehrenberg. Various objects and places have been named after Sigyn in modern times, including the Norwegian stiff-straw winter wheat varieties "Sigyn I" and "Sigyn II", a Marvel Comics character (1978) of the same name, the Swedish vessel MS Sigyn, which transports spent nuclear fuel in an allusion to Sigyn holding a bowl beneath the venom to spare Loki, and the antarctic Sigyn Glacier.



</doc>
<doc id="28868" url="https://en.wikipedia.org/wiki?curid=28868" title="Saudi Arabian–Iraqi neutral zone">
Saudi Arabian–Iraqi neutral zone

The Saudi–Iraqi neutral zone was an area of on the border between Saudi Arabia and the Iraqi Republic within which the border between the two countries had not been settled. The neutral zone came into existence following the Uqair Protocol of 1922 which defined the border between Iraq and the Sultanate of Nejd (Saudi Arabia's predecessor state). The neutral zone ended on 26 December 1981, when Iraq and Saudi Arabia agreed on the partition of the zone, but this wasn't filed with the United Nations until June 1991.

The Treaty of Muhammarah (Khorramshahr), 5 May 1922, forestalled the imminent conflict between the United Kingdom, which held the mandate for Iraq, and the Kingdom of Nejd, which later became Saudi Arabia (when combined with the Kingdom of Hejaz). The treaty specifically avoided defining boundaries. Following further negotiations, the Protocol of Uqair (Uqayr), 2 December 1922, defined most of the borders between them and created the neutral zone.

No military or permanent buildings were to be built in or near the neutral zone and the nomads of both countries were to have unimpeded access to its pastures and wells.

Administrative division of the zone was achieved in 1975, and a border treaty concluded in 1981. For unknown reasons, the treaty was not filed with the United Nations and nobody outside Iraq and Saudi Arabia was notified of the change or shown maps with details of the new boundary. As the Persian Gulf War approached in early 1991, Iraq cancelled all international agreements with Saudi Arabia since 1968. Saudi Arabia responded by registering all previous boundary agreements negotiated with Iraq at the United Nations in June 1991. That ended the legal existence of the Saudi Arabian–Iraqi neutral zone. 
Most official maps no longer show the diamond-shaped neutral zone but rather draw the border line approximately through the centre of the territory. For example, the United States Office of the Geographer regarded the area as having only an approximate boundary rather than a precise one.

The Saudi Arabian–Iraqi neutral zone formerly had the ISO 3166-1 codes NT and NTZ. The codes were discontinued in 1993. The FIPS 10-4 code for the Saudi Arabian–Iraqi neutral zone was IY; that code was deleted in 1992.




</doc>
<doc id="28869" url="https://en.wikipedia.org/wiki?curid=28869" title="Solidarity (Polish trade union)">
Solidarity (Polish trade union)

Solidarity (, ; full name: Independent Self-governing Trade Union "Solidarity"—"Niezależny Samorządny Związek Zawodowy "Solidarność"" ) is a trade union founded in August–September 1980 at the Lenin Shipyard in Gdańsk, Poland. Subsequently, it was the first independent union in a Warsaw Pact country to be recognised by the state. The union's membership peaked at 10 million in September 1981, representing one-third of the country's working-age population. Solidarity's leader, Lech Wałęsa was awarded the Nobel Peace Prize in 1983 and the union is widely recognised as having played a central role in the end of communist rule in Poland.

In the 1980s, Solidarity was a broad anti-bureaucratic social movement, using methods of civil resistance to advance the causes of workers' rights and social change. Government attempts in the early 1980s to destroy the union through the imposition of martial law in Poland and the use of political repression failed. Operating underground, with significant financial support from the Vatican and the United States, estimated to be as much as US$50 million, the union survived and by the latter 1980s had entered into negotiations with the government.

The 1989 round table talks between the government and the Solidarity-led opposition produced agreement for the 1989 legislative elections, the country's first pluralistic election since 1947. By the end of August, a Solidarity-led coalition government was formed and in December 1990, Wałęsa was elected President of Poland.

Following Poland's transition to liberal capitalism in the 1990s and the extensive privatization of state assets, Solidarity's membership and influence declined significantly; by 2010, 30 years after being founded, the union had lost more than 90% of its original membership.

In the 1970s Poland's government raised food prices while wages were stagnant. This and other stresses led to protests in 1976 and a subsequent government crackdown on dissent. The KOR, the ROPCIO and other groups began to form underground networks to monitor and oppose the government's behavior. Labour unions formed an important part of this network. In 1979, the Polish economy shrank for the first time since World War II, by 2 percent. Foreign debt reached around $18 billion by 1980.

Anna Walentynowicz was fired from the Gdańsk Shipyard on 7 August 1980, five months before she was due to retire, for participation in the illegal trade union. This management decision enraged the workers of the shipyard, who staged a strike action on 14 August defending Anna Walentynowicz and demanding her return. She and Alina Pienkowska transformed a strike over bread and butter issues into a solidarity strike in sympathy with strikes on other establishments.

Solidarity emerged on 31 August 1980 at the Gdańsk Shipyard when the communist government of Poland signed the agreement allowing for its existence. On 17 September 1980, over twenty Inter-factory Founding Committees of free trade unions merged at the congress into one national organization NSZZ Solidarity. It officially registered on 10 November 1980.

Lech Wałęsa and others formed a broad anti-Soviet social movement ranging from people associated with the Catholic Church to members of the anti-Soviet left. Polish nationalism, together with pro-American liberalism, played an important part in the development of Solidarity in the 1980s. Solidarity advocated non-violence in its members' activities. In September 1981, Solidarity's first national congress elected Wałęsa as a president and adopted a republican program, the "Self-governing Republic". The government attempted to destroy the union with the martial law of 1981 and several years of repression, but in the end it had to start negotiating with the union.

Roundtable Talks between the government and Solidarity-led opposition led to semi-free elections in 1989. By the end of August a Solidarity-led coalition government was formed, and in December Tadeusz Mazowiecki was elected Prime Minister. Since 1989, Solidarity has become a more traditional trade union, and had relatively little impact on the political scene of Poland in the early 1990s. A political arm founded in 1996 as Solidarity Electoral Action (AWS) won the parliamentary election in 1997, but lost the following 2001 election. Currently, "Solidarity" has little influence on modern Polish politics.

In the year leading up to martial law, Reagan Administration policies supported the Solidarity movement, waging a public relations campaign to deter what the Carter administration had seen as "an imminent move by large Soviet military forces into Poland." Michael Reisman from Yale Law School named operations in Poland as one of the covert regime change actions of the CIA during the Cold War. Colonel Ryszard Kukliński, a senior officer on the Polish General Staff, was secretly sending reports to CIA officer David Forden. The Central Intelligence Agency (CIA) transferred around $2 million yearly in cash to Solidarity, for a total of $10 million over five years. There were no direct links between the CIA and Solidarnosc, and all money was channeled through third parties. CIA officers were barred from meeting Solidarity leaders, and the CIA's contacts with Solidarnosc activists were weaker than those of the AFL-CIO, which raised $300,000 from its members, which were used to provide material and cash directly to Solidarity, with no control of Solidarity's use of it. The U.S. Congress authorized the National Endowment for Democracy to promote democracy, and the NED allocated $10 million to Solidarity.

The Polish government enacted martial law in December 1981, however, Solidarity was not alerted. Potential explanations for this vary; some believe that the CIA was caught off guard, while others suggest that American policy-makers viewed an internal crackdown as preferable to an "inevitable Soviet intervention." CIA support for Solidarity included money, equipment and training, which was coordinated by Special Operations. Henry Hyde, U.S. House intelligence committee member, stated that the USA provided "supplies and technical assistance in terms of clandestine newspapers, broadcasting, propaganda, money, organizational help and advice". Initial funds for covert actions by CIA were $2 million, but soon after authorization were increased and by 1985 CIA successfully infiltrated Poland.

In 2017, Solidarity backed a proposal to implement blue laws to prohibit Sunday shopping, a move supported by Polish bishops. A 2018 new Polish law banning almost all trade on Sundays has taken effect, with large supermarkets and most other retailers closed for the first time since liberal shopping laws were introduced in the 1990s. The Law and Justice party passed the legislation with the support of Prime Minister Mateusz Morawiecki.

Although Leszek Kołakowski's works were officially banned in Poland, and he lived outside the country from the late 1960s, the philosopher's ideas nonetheless exerted an influence on the Solidarity movement. Underground copies of his books and essays shaped the opinions of the Polish intellectual opposition. His 1971 essay "Theses on Hope and Hopelessness", which suggested that self-organized social groups could gradually expand the spheres of civil society in a totalitarian state, helped inspire the dissident movements of the 1970s that led to the creation of Solidarity and provided a philosophical underpinning for the movement.

Kołakowski later described Solidarity as "perhaps [the] closest to the working class revolution" that Karl Marx had predicted in the mid-1800s. Ironically, however, Solidarity featured many elements contrary to socialism as conceived by Marx: "[workers organized] against the exploiters, that is to say, the state. And this solitary example of a working class revolution (if even this may be counted) was directed against a socialist state, and carried out under the sign of the cross, with the blessing of the Pope."

The survival of Solidarity was an unprecedented event not only in Poland, a satellite state of the USSR ruled (in practice) by a one-party Communist regime, but the whole of the Eastern bloc. It meant a break in the hard-line stance of the communist Polish United Workers' Party, which had bloodily ended a 1970 protest with machine-gun fire (killing over thirty and injuring over 1,000), and the broader Soviet communist government in the Eastern Bloc, which had quelled both the 1956 Hungarian Uprising and the 1968 Prague Spring with Soviet-led invasions.

Solidarity's influence led to the intensification and spread of anti-communist ideals and movements throughout the countries of the Eastern Bloc, weakening their communist governments. As a result of the Round Table Agreement between the Polish government and the Solidarity-led opposition, elections were held in Poland on 4 June 1989, in which the opposition was allowed to field candidates against the Communist Party—the first free elections in any Soviet bloc country. A new upper chamber (the Senate) was created in the Polish parliament and all of its 100 seats were contestable in the election, as well as one-third of the seats in the more important lower chamber (the Sejm). Solidarity won 99 of the 100 Senate seats and all 161 contestable seats in the Sejm—a victory that also triggered a chain reaction across the Soviet Union's satellite states, leading to almost entirely peaceful anti-communist revolutions in Central and Eastern Europe known as the Revolutions of 1989 ("Jesień Ludów" or "Wiosna Obywatelów"), which ended in the overthrow of each Moscow-imposed regime, and ultimately to the dissolution of the Soviet Union in the early 1990s.

Given the union's support from many western governments, relations with trade unions in capitalist countries could be complicated. For example, during the UK miners' strike of 1984–85, Wałęsa said that "The miners should fight, but with common sense—not with destruction" and said of Margaret Thatcher "With such a wise and brave woman, Britain will find a solution to the strike." However, David Jastrzębski, the president of Upper Silesia Solidarity, voiced his support of the striking miners: "Neither the British government's mounted police charges nor its truncheon blows, any more than the Polish junta's tanks or rifle fire, can break our common will to struggle for a better future for the working class." This was despite the fact that Arthur Scargill, president of the British National Union of Mineworkers had been highly critical of Solidarity, condemning it as an "anti-socialist organization which desires the overthrow of a socialist state".

In 2005, the trade union Solidarity – The Union for British Workers was created in honour of the original Polish union by the far-right British National Party.

In late 2008, several democratic opposition groups in the Russian Federation formed a Solidarity movement.

In the United States, the American Solidarity Party (formerly the Christian Democratic Party USA), a Christian democratic political party, attributes its namesake to Solidarity.

In a 2011 essay "The Jacobin Spirit" in the American magazine "Jacobin", philosopher Slavoj Zizek called Solidarnosc' one of the "free spaces at a distance from state power" that used "defensive violence" to protect itself from state control. The notion of "defensive violence" runs in the vein of ideas postulated by Alain Badiou.

The union was officially founded on 17 September 1980, the union's supreme powers were vested in a legislative body, the "Convention of Delegates" ("Zjazd Delegatów"). The executive branch was the National Coordinating Commission ("Krajowa Komisja Porozumiewawcza"), later renamed the National Commission ("Komisja Krajowa"). The Union had a regional structure, comprising 38 regions ("region") and two districts ("okręg"). At its highest, the Union had over 10 million members, which became the largest union membership in the world. During the communist era the 38 regional delegates were arrested and jailed when martial law came into effect on 13 December 1981 under General Wojciech Jaruzelski. After a one-year prison term the high-ranking members of the union were offered one way trips to any country accepting them (including Canada, the United States, and nations in the Middle East).

Solidarity was organized as an industrial union, or more specifically according to the One Big Union principle, along the lines of the Industrial Workers of the World and the Spanish Confederación Nacional del Trabajo (workers in every trade were organized by region, rather than by craft).

In 2010, Solidarity had more than 400,000 members. National Commission of Independent Self-Governing Trade Union is located in Gdańsk and is composed of Delegates from Regional General Congresses.

Solidarity is divided into 37 regions, and the territorial structure to a large degree reflects the shape of Polish voivodeships, established in 1975 and annulled in 1998 (see: Administrative division of People's Republic of Poland). The regions are:

The network of Solidarity branches of the key factories of Poland was created on 14 April 1981 in Gdańsk. It was made of representatives of seventeen factories; each stood for the most important factory of every voivodeship of the pre-1975 Poland (see: Administrative division of People's Republic of Poland). However, there were two exceptions. There was no representative of the Koszalin Voivodeship, and the Katowice Voivodeship was represented by two factories:





</doc>
<doc id="28876" url="https://en.wikipedia.org/wiki?curid=28876" title="Surtsey">
Surtsey

Surtsey ("Surtr's island" in Icelandic, ) is a volcanic island located in the Vestmannaeyjar archipelago off the southern coast of Iceland. At , Surtsey is the southernmost point of Iceland. It was formed in a volcanic eruption which began below sea level, and reached the surface on 14 November 1963. The eruption lasted until 5 June 1967, when the island reached its maximum size of . Since then, wave erosion has caused the island to steadily diminish in size: , its surface area was . The most recent survey (2007) shows the island's maximum elevation at above sea level.

The new island was named after "Surtr", a fire "jötunn" or giant from Norse mythology. It was intensively studied by volcanologists during its eruption, and afterwards by botanists and other biologists as life forms gradually colonised the originally barren island. The undersea vents that produced Surtsey are part of the "Vestmannaeyjar" submarine volcanic system, part of the fissure of the sea floor called the Mid-Atlantic Ridge. Vestmannaeyjar also produced the famous eruption of "Eldfell" on the island of Heimaey in 1973. The eruption that created Surtsey also created a few other small islands along this volcanic chain, such as "Jólnir" and other unnamed peaks. Most of these eroded away fairly quickly. It is estimated that Surtsey will remain above sea level for another 100 years.

The eruption was unexpected, and almost certainly began some days before it became apparent at the surface. The sea floor at the eruption site is below sea level, and at this depth volcanic emissions and explosions would be suppressed, quenched and dissipated by the water pressure and density. Gradually, as repeated flows built up a mound of material that approached sea level, the explosions could no longer be contained, and activity broke the surface.

The first noticeable indications of volcanic activity were recorded at the seismic station in Kirkjubæjarklaustur, Iceland from 6 to 8 November, which detected weak tremors emanating from an epicentre approximately west-south-west at a distance of , the location of Surtsey. Another station in Reykjavík recorded even weaker tremors for ten hours on 12 November at an undetermined location, when seismic activity ceased until 21 November. That same day, people in the coastal town of Vík away noticed a smell of hydrogen sulphide. On 13 November, a fishing vessel in search of herring, equipped with sensitive thermometers, noted sea temperatures SW of the eruption center were higher than surrounding waters.

At 07:15 UTC on 14 November 1963, the cook of "Ísleifur II", a trawler sailing these same waters, spotted a rising column of dark smoke southwest of the boat. The captain thought it might have been a boat on fire, and ordered his crew to investigate. Instead, they encountered explosive eruptions giving off black columns of ash, indicating that a volcanic eruption had begun to penetrate the surface of the sea. By 11:00 the same day, the eruption column had reached several kilometres in height. At first the eruptions took place at three separate vents along a northeast by southwest trending fissure, but by the afternoon the separate eruption columns had merged into one along the erupting fissure. Over the next week, explosions were continuous, and after just a few days the new island, formed mainly of scoria, measured over in length and had reached a height of .

As the eruptions continued, they became concentrated at one vent along the fissure and began to build the island into a more circular shape. By 24 November, the island measured about . The violent explosions caused by the meeting of lava and sea water meant that the island consisted of a loose pile of volcanic rock (scoria), which was eroded rapidly by North Atlantic storms during the winter. However, eruptions more than kept pace with wave erosion, and by February 1964, the island had a maximum diameter of over .

The explosive phreatomagmatic eruptions caused by the easy access of water to the erupting vents threw rocks up to a kilometre (0.6 mi) away from the island, and sent ash clouds as high as up into the atmosphere. The loose pile of unconsolidated tephra would quickly have been washed away had the supply of fresh magma dwindled, and large clouds of dust were often seen blowing away from the island during this stage of the eruption.

The new island was named after the fire jötunn Surtur from Norse mythology ("Surts" is the genitive case of "Surtur", plus -ey, "island"). Three French journalists representing the magazine "Paris Match" notably landed there on 6 December 1963, staying for about 15 minutes before violent explosions encouraged them to leave. The journalists jokingly claimed French sovereignty over the island, but Iceland quickly asserted that the new island belonged to it.

By early 1964, though, the continuing eruptions had built the island to such a size that sea water could no longer easily reach the vents, and the volcanic activity became much less explosive. Instead, lava fountains and flows became the main form of activity. These resulted in a hard cap of extremely erosion-resistant rock being laid down on top of much of the loose volcanic pile, which prevented the island from being washed away rapidly. Effusive eruptions continued until 1965, by which time the island had a surface area of .

On 28 December 1963, submarine activity to the northeast of Surtsey caused the formation of a ridge high on the sea floor. This seamount was named Surtla, but never reached sea level. Eruptions at Surtla ended on 6 January 1964, and it has since been eroded from its minimum depth of below sea level.

In 1965, the activity on the main island diminished, but at the end of May that year an eruption began at a vent off the northern shore. By 28 May, an island had appeared, and was named Syrtlingur (Little Surtsey). The new island was washed away during early June, but reappeared on 14 June. Eruptions at Syrtlingur were much smaller in scale than those that had built Surtsey, with the average rate of emission of volcanic materials being about a tenth of the rate at the main vent. Activity was short-lived, continuing until the beginning of October 1965, by which time the islet had an area of . Once the eruptions had ceased, wave erosion rapidly wore the island away, and it disappeared beneath the waves on 24 October.

During December 1965, more submarine activity occurred southwest of Surtsey, and another island was formed. It was named Jólnir, and over the following eight months it appeared and disappeared several times, as wave erosion and volcanic activity alternated in dominance. Activity at Jólnir was much weaker than the activity at the main vent, and even weaker than that seen at Syrtlingur, but the island eventually grew to a maximum size of in height, covering an area of , during July and early August 1966. Like Syrtlingur, though, after activity ceased on 8 August 1966, it was rapidly eroded, and dropped below sea level during October 1966.

Effusive eruptions on the main island returned on 19 August 1966, with fresh lava flows giving it further resistance to erosion. The eruption rate diminished steadily, though, and on 5 June 1967, the eruption ended. The volcano has been dormant ever since. The total volume of lava emitted during the three-and-a-half-year eruption was about one cubic kilometre (0.24 cu mi), and the island's highest point was above sea level at that time.

Since the end of the eruption, erosion has seen the island diminish in size. A large area on the southeast side has been eroded away completely, while a sand spit called "Norðurtangi" (north point) has grown on the north side of the island. It is estimated that about of material has been lost due to erosion—this represents about a quarter of the original above-sea-level volume of the island. Its maximum elevation has diminished to .

Following the end of the eruption, scientists established a grid of benchmarks against which they measured the change in the shape of the island. In the 20 years following the end of the eruption, measurements revealed that the island was steadily subsiding and had lost about one metre in height. The rate of subsidence was initially about per year but slowed to a year by the 1990s. It had several causes: settling of the loose tephra forming the bulk of the volcano, compaction of sea floor sediments underlying the island, and downward warping of the lithosphere due to the weight of the volcano.

Volcanoes in the Vestmannaeyjar archipelago are typically monogenetic, and so the island is unlikely to be enlarged in the future by further eruptions. The heavy seas around the island have been eroding it ever since the island appeared, and since the end of the eruption almost half of its original area has been lost. The island currently loses about of its surface area each year.

As a suspected part of the Iceland plume, this island is unlikely to disappear entirely in the near future. The eroded area consisted mostly of loose tephra, easily washed away. Most of the remaining area is capped by hard lava flows, which are much more resistant to erosion. In addition, complex chemical reactions within the loose tephra within the island have gradually formed highly erosion resistant tuff material, in a process known as palagonitization. On Surtsey this process has happened quite rapidly, due to high temperatures not far below the surface.

Estimates of how long Surtsey will survive are based on the rate of erosion seen up to the present day. Assuming that the current rate does not change, the island will be mostly at or below sea level by 2100. However, the rate of erosion is likely to slow as the tougher core of the island is exposed: an assessment assuming that the rate of erosion will slow exponentially suggests that the island will survive for many centuries. An idea of what it will look like in the future is given by the other small islands in the Vestmannaeyjar archipelago, which formed in the same way as Surtsey several thousand years ago, and have eroded away substantially since they were formed.

A classic site for the study of biocolonisation from founder populations that arrive from outside ("allochthonous"), Surtsey was declared a nature reserve in 1965, while the eruption was still in active progress. Today only a few scientists are permitted to land on Surtsey; the only way anyone else can see it closely is from a small plane. This allows the natural ecological succession for the island to proceed without outside interference. In 2008, UNESCO declared the island a World Heritage Site, in recognition of its great scientific value.

In the spring of 1965, the first vascular plant was found growing on the northern shore of Surtsey, mosses became visible in 1967, and lichens were first found on the Surtsey lava in 1970. Plant colonisation on Surtsey has been closely studied, the vascular plants in particular as they have been of far greater significance than mosses, lichens and fungi in the development of vegetation.

Mosses and lichens now cover much of the island. During the island's first 20 years, 20 species of plants were observed at one time or another, but only 10 became established in the nutrient-poor sandy soil. As birds began nesting on the island, soil conditions improved, and more vascular plant species were able to survive. In 1998, the first bush was found on the island—a tea-leaved willow ("Salix phylicifolia"), which can grow to heights of up to . By 2008, 69 species of plant had been found on Surtsey, of which about 30 had become established. This compares to the approximately 490 species found on mainland Iceland. More species continue to arrive, at a typical rate of roughly 2–5 new species per year.

The expansion of bird life on the island has both relied on and helped to advance the spread of plant life. Birds use the plants for nesting material, but also continue to assist in the spreading of seeds, and fertilize the soil with their guano. Birds first began nesting on Surtsey three years after the eruptions ended, with fulmar and guillemot the first species to set up home. Twelve species are now regularly found on the island.

A gull colony has been present since 1984, although gulls were seen briefly on the shores of the new island only weeks after it first appeared. The gull colony has been particularly important in developing the plant life on Surtsey, and the gulls have had much more of an impact on plant colonisation than other breeding species due to their abundance. An expedition in 2004 found the first evidence of nesting Atlantic puffins, which are extremely common in the rest of the archipelago.

As well as providing a home for some species of birds, Surtsey has also been used as a stopping-off point for migrating birds, particularly those en route between Europe and Iceland. Species that have been seen briefly on the island include whooper swans, various species of geese, and common ravens. Although Surtsey lies to the west of the main migration routes to Iceland, it has become a more common stopping point as its vegetation has improved. In 2008, the 14th bird species was detected with the discovery of a common raven's nest.

According to a 30 May 2009 report, a golden plover was nesting on the island with four eggs.

Soon after the island's formation, seals were seen around the island. They soon began basking there, particularly on the northern spit, which grew as the waves eroded the island. Seals were found to be breeding on the island in 1983, and a group of up to 70 made the island their breeding spot. Grey seals are more common on the island than harbour seals, but both are now well established. The presence of seals attracts orcas, which are frequently seen in the waters around the Vestmannaeyjar archipelago and now frequent the waters around Surtsey.

On the submarine portion of the island, many marine species are found. Starfish are abundant, as are sea urchins and limpets. The rocks are covered in algae, and seaweed covers much of the submarine slopes of the volcano, with its densest cover between below sea level.

Insects arrived on Surtsey soon after its formation, and were first detected in 1964. The original arrivals were flying insects, carried to the island by winds and their own power. Some were believed to have been blown across from as far away as mainland Europe. Later insect life arrived on floating driftwood, and both live animals and carcasses washed up on the island. When a large, grass-covered tussock was washed ashore in 1974, scientists took half of it for analysis and discovered 663 land invertebrates, mostly mites and springtails, the great majority of which had survived the crossing.

The establishment of insect life provided some food for birds, and birds in turn helped many species to become established on the island. The bodies of dead birds provide sustenance for carnivorous insects, while the fertilisation of the soil and resulting promotion of plant life provides a viable habitat for herbivorous insects.

Some higher forms of land life are now colonising the soil of Surtsey. The first earthworm was found in a soil sample in 1993, probably carried over from Heimaey by a bird. However, the next year earthworms were not found. Slugs were found in 1998, and appeared to be similar to varieties found in the southern Icelandic mainland. Spiders and beetles have also become established.

The only significant human impact is a small prefabricated hut which is used by researchers while staying on the island. The hut includes a few bunk beds and a solar power source to drive an emergency radio and other key electronics. There is also an abandoned lighthouse foundation. All visitors check themselves and belongings to ensure no seeds are accidentally introduced by humans to this ecosystem. It is believed that some young boys tried to introduce potatoes, which were promptly dug up once discovered. An improperly handled human defecation resulted in a tomato plant taking root which was also destroyed.
In 2009 a weather station for weather observations and a webcam were installed on Surtsey.




</doc>
<doc id="28878" url="https://en.wikipedia.org/wiki?curid=28878" title="Software Engineering Body of Knowledge">
Software Engineering Body of Knowledge

The Software Engineering Body of Knowledge (SWEBOK) is an international standard ISO/IEC TR 19759:2005 specifying a guide to the generally accepted software engineering body of knowledge.

The Guide to the Software Engineering Body of Knowledge (SWEBOK Guide) has been created through cooperation among several professional bodies and members of industry and is published by the IEEE Computer Society (IEEE). The standard can be accessed freely from the IEEE Computer Society. In late 2013, SWEBOK V3 was approved for publication and released. In 2016, the IEEE Computer Society kicked off the SWEBoK Evolution effort to develop future iterations of the body of knowledge.

The published version of SWEBOK V3 has the following 15 knowledge areas (KAs) within the field of software engineering:


It also recognized, but did not define, these related disciplines:


The 2004 edition of the SWEBOK guide defined ten knowledge areas (KAs) within the field of software engineering:

The SWEBOK also defines disciplines related to software engineering:

A similar effort to define a body of knowledge for software engineering is the "Computing Curriculum Software Engineering (CCSE)," officially named Software Engineering 2004 (SE2004). The curriculum largely overlaps with the 2004 SWEBOK V2 because the SWEBOK has been used as one of its sources; however, it is more directed towards academia. Whereas the SWEBOK Guide defines the software engineering knowledge that practitioners should have after four years of practice, SE2004 defines the knowledge that an undergraduate software engineering student should possess upon graduation (including knowledge of mathematics, general engineering principles, and other related areas). SWEBOK V3 aims to address these intersections.



</doc>
<doc id="28884" url="https://en.wikipedia.org/wiki?curid=28884" title="SOE">
SOE

SOE may refer to:






</doc>
<doc id="28887" url="https://en.wikipedia.org/wiki?curid=28887" title="Scottish National Party">
Scottish National Party

The Scottish National Party (SNP; , ) is a Scottish nationalist and social-democratic political party in Scotland. The SNP supports and campaigns for Scottish independence within the European Union, with a platform based on civic nationalism. The SNP is the third-largest political party by membership in the United Kingdom, behind the Labour Party and the Conservative Party and it is the largest political party in Scotland, where it has the most seats in the Scottish Parliament and 48 out of the 59 Scottish seats in the House of Commons at Westminster. The current Scottish National Party leader, Nicola Sturgeon, has served as First Minister of Scotland since November 2014.

Founded in 1934 with the amalgamation of the National Party of Scotland and the Scottish Party, the party has had continuous parliamentary representation in Westminster since Winnie Ewing won the 1967 Hamilton by-election. With the establishment of the devolved Scottish Parliament in 1999, the SNP became the second-largest party, serving two terms as the opposition. The SNP gained power at the 2007 Scottish Parliament election, forming a minority government, before going on to win the 2011 Parliament election, after which it formed Holyrood's first majority government. It was reduced back to being a minority government at the 2016 election.

The SNP is the largest political party in Scotland in terms of both seats in the Westminster and Holyrood parliaments, and membership, reaching 125,482 members as of December 2019, 48 MPs, 61 MSPs and over 400 local councillors. The SNP is a member of the European Free Alliance (EFA). The party does not have any members of the House of Lords, as it has always maintained a position of objecting to an unelected upper house.

The SNP was formed in 1934 through the merger of the National Party of Scotland and the Scottish Party, with The Duke of Montrose and Robert Bontine Cunninghame Graham as its first, joint, president. Sir Alexander MacEwen was its first chairman. Professor Douglas Young, who was the leader of the Scottish National Party from 1942 to 1945 campaigned for the Scottish people to refuse conscription and his activities were popularly vilified as undermining the British war effort against the Axis powers. Young was imprisoned for refusing to be conscripted.

The party suffered its first split during this period with John MacCormick leaving the party in 1942, owing to his failure to change the party's policy from supporting all-out independence to Home Rule at that year's conference in Glasgow. McCormick went on to form the Scottish Covenant Association, a non-partisan political organisation campaigning for the establishment of a devolved Scottish Assembly.

However, wartime conditions also enabled the SNP's first parliamentary success at the Motherwell by-election in 1945, but Robert McIntyre MP lost the seat at the general election three months later. The 1950s were characterised by similarly low levels of support, and this made it difficult for the party to advance. Indeed, in most general elections they were unable to put up more than a handful of candidates.

The 1960s, however, offered more electoral successes, with candidates polling credibly at Glasgow Bridgeton in 1961, West Lothian in 1962 and Glasgow Pollok in 1967. Indeed, this foreshadowed Winnie Ewing's surprise victory in a by-election at the previously safe Labour seat of Hamilton. This brought the SNP to national prominence, leading to the establishment of the Kilbrandon Commission.

Despite this breakthrough, the 1970 general election was to prove a disappointment for the party as, despite an increase in vote share, Ewing failed to retain her seat in Hamilton. The party did receive some consolation with the capture of the Western Isles, making Donald Stewart the party's only MP. This was to be the case until the 1973 by-election at Glasgow Govan where a hitherto safe Labour seat was claimed by Margo MacDonald.

1974 was to prove something of an "annus mirabilis" for the party as it deployed its highly effective "It's Scotland's oil" campaign. The SNP gained 6 seats at the February general election before hitting a high point in the October re-run, polling almost a third of all votes in Scotland and returning 11 MPs to Westminster. Furthermore, during that year's local elections the party claimed overall control of Cumbernauld and Kilsyth.

This success was to continue for much of the decade, and at the 1977 district elections the SNP saw victories at councils including East Kilbride and Falkirk and held the balance of power in Glasgow. However, this level of support was not to last and by 1978 Labour revival was evident at three by-elections (Glasgow Garscadden, Hamilton and Berwick and East Lothian) as well as the regional elections.

This was to culminate when the party experienced a large drop in its support at the 1979 general election, precipitated by the party bringing down the incumbent Labour minority government following the controversial failure of that year's devolution referendum. Reduced to just 2 MPs, the successes of October 1974 were not to be surpassed until the 2015 general election.

Following this defeat, a period of internal strife occurred within the party, culminating with the formation of two internal groups: the proto-fascist Siol nan Gaidheal and left-wing 79 Group. Traditionalists within the party, centred around Winnie Ewing, by this time an MEP, responded by establishing the Campaign for Nationalism in Scotland which sought to ensure that the primary objective of the SNP was campaigning for independence outwith a traditional left-right orientation, even though this would have undone the work of figures such as William Wolfe, who developed a clearly social democratic policy platform throughout the 1970s.

These events ensured the success of a leadership motion at the party's annual conference of 1982, in Ayr, despite the 79 Group being bolstered by the merger of Jim Sillars' Scottish Labour Party (SLP) although this influx of ex-SLP members further shifted the characteristics of the party leftwards. Despite this, traditionalist figure Gordon Wilson remained party leader through the electoral disappointments of 1983 and 1987, where he lost his own Dundee East seat won 13 years prior.

Through this period, Sillars grew influence in the party, developing a clear socio-economic platform including "Independence in Europe," reversing the SNP's previous opposition to membership of the then-EEC which had been unsuccessful in a 1975 referendum. This position was enhanced further by Sillars reclaiming Glasgow Govan in a by-election in 1988.

Despite this moderation, the party did not join Labour, the Liberal Democrats and the Greens as well as civil society in the Scottish Constitutional Convention which developed a blueprint for a devolved Scottish Parliament due to the unwillingness of the convention to discuss independence as a constitutional option.

Alex Salmond had been elected MP for Banff and Buchan in 1987, after the re-admittance of 79 Group members, and was able to seize the party leadership after Wilson's resignation in 1990 after a contest with Margaret Ewing. This was a surprise victory as Ewing had the backing of much of the party establishment, including Sillars and then-Party Secretary John Swinney. The defection of Labour MP Dick Douglas further evidenced the party's clear left-wing positioning, particularly regarding opposition to the poll tax. Despite this, Salmond's leadership was unable to avert a fourth successive general election disappointment in 1992 with the party reduced back from 5 to 3 MPs.

The mid-90s offered some successes for the party, with North East Scotland being gained at the 1994 European elections and the party securing a by-election at Perth and Kinross in 1995 after a near-miss at Monklands East the previous year.

1997 offered the party's most successful general election for 23 years, although in the face of the Labour landslide the party was unable to match either 1974 election. That September, the party joined with the other members of the Scottish Constitutional Convention in the successful Yes-Yes campaign in the devolution referendum which lead to the establishment of a Scottish Parliament with tax-varying powers.

By 1999, the first elections to the parliament were being held, although the party suffered a disappointing result, gaining just 35 MSPs in the face of Salmond's unpopular 'Kosovo Broadcast' which opposed NATO intervention in the country.

This meant that the party began as the official opposition in the parliament to a Labour-Liberal Democrat coalition government. Salmond found the move to a more consensual politics difficult and sought a return to Westminster, resigning the leadership in 2000 with John Swinney, like Salmond a gradualist, victorious in the ensuring leadership election. Swinney's leadership proved ineffectual, with a loss of one MP in 2001 and a further reduction to 27 MSPs in 2003 despite the Officegate scandal unseating previous First Minister Henry McLeish. However, the only parties to gain seats in that election where the Scottish Greens and the Scottish Socialist Party (SSP) which like the SNP support independence.

After an unsuccessful coup attempt in 2003, Swinney stepped down following disappointing results in the European elections of 2004 with Salmond victorious in the subsequent leadership contest despite initially refusing to be candidate. Nicola Sturgeon was elected Depute Leader and became the party's leader in the Scottish Parliament until Salmond was able to return at the next parliamentary election.

In 2007, the SNP emerged as the largest party in the Scottish Parliament with 47 of 129 seats, narrowly ousting the Scottish Labour Party with 46 seats and Alex Salmond becoming First Minister after ousting the Liberal Democrats in Gordon. The Scottish Green Party supported Salmond's election as First Minister, and his subsequent appointments of ministers, in return for early tabling of the climate change bill and the SNP nominating a Green MSP to chair a parliamentary committee. Despite this, Salmond's minority government tended to strike budget deals with the Conservatives to stay in office.

In May 2011, the SNP won an overall majority in the Scottish Parliament with 69 seats. This was a significant feat as the additional member system used for Scottish Parliament elections was specifically designed to prevent one party from winning an outright majority. This was followed by a reverse in the party's previous opposition to NATO membership at the party's annual conference in 2012 despite Salmond's refusal to apologise for the Kosovo broadcast on the occasion of the Kosovo Declaration of Independence.

This majority enabled the SNP government to hold a referendum on Scottish independence in 2014. The "No" vote prevailed in a close-fought campaign, prompting the resignation of First Minister Alex Salmond. Forty-five percent of Scottish voters cast their ballots for independence, with the "Yes" side receiving less support than late polling predicted. This was suggested as due to Salmond's unpopularity among women and Nicola Sturgeon won that year's leadership election unopposed.

The SNP rebounded from the loss in the independence referendum at the 2015 UK general election, led by Nicola Sturgeon. The party went from holding six seats in the House of Commons to 56, mostly at the expense of the Labour Party. All but three of the fifty-nine constituencies in the country elected an SNP candidate in the party's most comprehensive electoral victory at any level.

At the 2016 Scottish election, the SNP lost a net total of 6 seats, losing its overall majority in the Scottish Parliament, but returning for a third consecutive term as a minority government despite gaining an additional 1.1% of the constituency vote, for the party's best-ever result, from the 2011 election however 2.3% of the regional list vote. On the constituency vote, the SNP gained 11 seats from Labour, but lost the Edinburgh Southern constituency to the party. The Conservatives and Liberal Democrats each gained two constituency seats from the SNP on 2011 (Aberdeenshire West and Edinburgh Central for the Conservatives and Edinburgh Western and North East Fife for the Liberal Democrats).

This election was followed by the 2016 European Union referendum after which the SNP joined with the Liberal Democrats and Greens to call for continued membership of the EU. Despite a consequential increase in the Conservative vote at the 2017 local elections the SNP for the first time became the largest party in each of Scotland's four city councils: Aberdeen, Dundee, Edinburgh and Glasgow, where a Labour administration was ousted after 37 years.

At the 2017 UK general election the SNP underperformed compared to polling expectations, losing 21 seats to bring their number of Westminster MPs down to 35 – however this was still the party's second best result ever at the time. This was largely attributed by many, including former Deputy First Minister John Swinney, to their stance on holding a second Scottish independence referendum and saw a swing to the Unionist parties, with seats being picked up by the Conservatives, Labour and the Liberal Democrats and a reduction in their majorities in the other seats. Stephen Gethins, MP for North East Fife, came out of this election with a majority of just 2 to the Liberal Democrat candidate. High-profile losses included SNP Commons leader Angus Robertson in Moray and former party leader and First Minister Alex Salmond in Gordon.

The SNP went on to achieve its best-ever European Parliament result in the final election before Brexit, the party taking its MEP total to 3 or half of Scottish seats and achieving a record vote share for the party. This was also the best performance of any party in the era of proportional elections to the European Parliament in Scotland. This was suggested as being due to the party's europhile sentiment during what amounted to a single-issue election, with parties that lacked a clear message performing poorly, such as Labour finishing in 5th place and losing all of their Scottish MEPs for the first time.

Later that year the SNP experienced a surge in the 2019 general election, winning 45.0% of the vote and 48 seats, its second-best result ever. Although the party suffered a loss to the Liberal Democrats, it gained the seat of its then UK leader Jo Swinson, along with 7 from the Conservatives and 6 from Labour. This victory was generally attributed to Sturgeon's cautious approach regarding holding a second independence referendum and a strong emphasis on EU membership during the election. Despite this, the UK-wide Conservative majority ensured that the UK left the EU the following January.

The local Branches are the primary level of organisation in the SNP. All of the Branches within each Scottish Parliament constituency form a Constituency Association, which coordinates the work of the Branches within the constituency, coordinates the activities of the party in the constituency and acts as a point of liaison between an MSP or MP and the party. Constituency Associations are composed of delegates from all of the Branches within the constituency.

The annual National Conference is the supreme governing body of the SNP and is responsible for determining party policy and electing the National Executive Committee. The National Conference is composed of:

The National Council serves as the SNP's governing body between National Conferences, and its decisions are binding unless rescinded or modified by the National Conference. There are also regular meetings of the National Assembly, which provides a forum for detailed discussions of party policy by party members.

The party has an active youth wing, the Young Scots for Independence, as well as a student wing, the Federation of Student Nationalists. There is also an SNP Trade Union Group. There is an independently owned monthly newspaper, "The Scots Independent", which is highly supportive of the party.

The SNP's leadership is vested in its National Executive Committee (NEC), which is made up of the party's elected office bearers and six elected members (voted for at conference). The SNP parliamentarians (Holyrood and Westminster) and councillors have representation on the NEC, as do the Trade Union Group, the youth wing and the student wing.

The 42 members of the current National Executive Committtee:

Since 18 September 2014 (the day of the Scottish independence referendum), party membership has more than quadrupled (from 25,642), surpassing the Liberal Democrats and, briefly, Conservatives to become the second-largest political party in the United Kingdom in terms of membership. As of August 2018, the Party had 125,482 members.

The SNP retains close links with Plaid Cymru, its counterpart in Wales. MPs from both parties co-operate closely with each other and work as a single parliamentary group within the House of Commons. Both the SNP and Plaid Cymru are members of the European Free Alliance (EFA), a European political party comprising regionalist political parties. The EFA co-operates with the larger European Green Party to form The Greens–European Free Alliance (Greens/EFA) group in the European Parliament.

Before its affiliation with The Greens–European Free Alliance, the SNP had previously been allied with the European Progressive Democrats (1979–1984), Rainbow Group (1989–1994) and European Radical Alliance (1994–1999).

As the UK is no longer a member of the EU, the SNP has no MEPs.

The Scottish National Party did not have a clear ideological position until the 1970s, when it sought to explicitly present itself as a social democratic party in terms of party policy and publicity. During the period from its foundation until the 1960s, the SNP was essentially a moderate centrist party. Debate within the party focused more on the SNP being distinct as an all-Scotland national movement, with it being neither of the left nor the right, but constituting a new politics that sought to put Scotland first.

The SNP was formed through the merger of the centre-left National Party of Scotland (NPS) and the centre-right Scottish Party. The SNP's founders were united over self-determination in principle, though not its exact nature, or the best strategic means to achieve self-government. From the mid-1940s onwards, SNP policy was radical and redistributionist concerning land and in favour of 'the diffusion of economic power', including the decentralisation of industries such as coal to include the involvement of local authorities and regional planning bodies to control industrial structure and development. Party policies supported the economic and social policy status quo of the post-war welfare state.

By the 1960s, the SNP was starting to become defined ideologically, with a social democratic tradition emerging as the party grew in urban, industrial Scotland, and its membership experienced an influx of social democrats from the Labour Party, the trade unions and the Campaign for Nuclear Disarmament. The emergence of Billy Wolfe as a leading figure in the SNP also contributed to the leftwards shift. By this period, the Labour Party were also the dominant party in Scotland, in terms of electoral support and representation. Targeting Labour through emphasising left-of-centre policies and values was therefore electorally logical for the SNP, as well as tying in with the ideological preferences of many new party members. In 1961, the SNP conference expressed the party's opposition to the siting of the US Polaris submarine base at the Holy Loch. This policy was followed in 1963 by a motion opposed to nuclear weapons: a policy that has remained in place ever since. The 1964 policy document, "SNP & You", contained a clear centre-left policy platform, including commitments to full employment, government intervention in fuel, power and transport, a state bank to guide economic development, encouragement of cooperatives and credit unions, extensive building of council houses (social housing) by central and local government, pensions adjusted to cost of living, a minimum wage and an improved national health service.

The 1960s also saw the beginnings of the SNP's efforts to establish an industrial organisation and mobilise amongst trade unionists in Scotland, with the establishment of the SNP Trade Union Group, and identifying the SNP with industrial campaigns, such as the Upper-Clyde Shipbuilders Work-in and the attempt of the workers at the Scottish "Daily Express" to run as a co-operative. For the party manifestos for the two 1974 general elections, the SNP finally self-identified as a social democratic party, and proposed a range of social democratic policies. There was also an unsuccessful proposal at the 1975 party conference to rename the party as the "Scottish National Party (Social Democrats)". In the UK wide referendum on Britain's membership of the European Economic Community (EEC) in the same year as the aforementioned attempted name change, the SNP campaigned for Britain to leave the EEC.

There were further ideological and internal struggles after 1979, with the 79 Group attempting to move the SNP further to the left, away from being what could be described a "social-democratic" party, to an expressly "socialist" party. Members of the 79 Group – including future party leader and First Minister Alex Salmond – were expelled from the party. This produced a response in the shape of the Campaign for Nationalism in Scotland from those who wanted the SNP to remain a "broad church", apart from arguments of left vs. right. The 1980s saw the SNP further define itself as a party of the political left, such as campaigning against the introduction of the poll tax in Scotland in 1989; one year before the tax was imposed on the rest of the UK.

Ideological tensions inside the SNP are further complicated by arguments between the so-called SNP gradualists and SNP fundamentalists. In essence, gradualists seek to advance Scotland to independence through further devolution, in a "step-by-step" strategy. They tend to be in the moderate left grouping, though much of the 79 Group was gradualist in approach. However, this 79 Group gradualism was as much a reaction against the fundamentalists of the day, many of whom believed the SNP should not take a clear left or right position.

With the SNP's policy base being mostly in the mainstream Western European social democratic tradition, the party is committed to policies such as progressive personal taxation. This vision was achieved by the Sturgeon Government in 2017, reducing income tax rates for a slight majority of the population by shifting the tax burden to the wealthier. Previously the party had replaced the flat rate Stamp Duty with the LBTT, which is founded on progressive principles. Whilst in government, the party was also responsible for the establishment of Revenue Scotland to administer devolved taxation.

Having previously defined itself in opposition to the poll tax the SNP has also championed progressive taxation at a local level. Despite pledging to introduce a local income tax the Salmond Government found itself unable to replace the council tax and the party has, particularly since the ending of the council tax freeze under Nicola Sturgeon's leadership, committing to increasing the graduated nature of the tax. Conversely, the party has also supported capping and reducing Business Rates in an attempt to support small businesses.

It has been noted that the party contains a broader spectrum of opinion regarding economic issues than most political parties in the UK due to its status as "the only viable vehicle for Scottish independence", with the party's parliamentary group at Westminster including socialists such as Tommy Sheppard and Mhairi Black as well as supporters of tax cuts like Stewart Hosie and, previously, former Conservative Tasmina Ahmed-Sheikh.

The SNP can be seen to be at the heart of the secularisation and liberalisation of Scotland with the party achieving the legalisation of same sex marriage in 2014, indeed under Sturgeon's leadership Scotland was twice in succession named the best country in Europe for LGBTI legal equality. Party policy aims to introduce gender self-identification to allow an easier process of gender recognition for transgender community. This is in stark contrast to Scotland's recent history as a deeply socially conservative country although this transformation can be seen to have taken place in the country's other main political parties largely simultaneously.

Particularly since Nicola Sturgeon's elevation to First Minister the party has highlighted its commitments to gender equality – with her first act being to appoint a gender balanced cabinet. The SNP have also taken steps to implement all-women shortlists whilst Stugeon has introduced a mentoring scheme to encourage women's political engagement.

Stressing that their brand of nationalism is civic nationalism, the SNP are keen to show their support for multiculturalism with Scotland receiving thousands of refugees from the Syrian Civil War. To this end it has been claimed that refugees in Scotland are better supported than those in England. More generally, the SNP take a liberal stance on immigration, seeking to increase numbers to combat a declining population and calling for a separate Scottish visa even within the UK.

Despite traditionally supporting military neutrality the SNP's policy has in recent years moved to support both the Atlanticist and Europeanist traditions. This is particularly evident in the conclusion of the NATO debate within the party in favour of those who support membership of the military alliance. This is despite the party's continuing opposition to Scotland hosting nuclear weapons and then-leader Salmond's criticism of both the Kosovo intervention and the Iraq War. The party has placed an emphasis on developing positive relations with the United States in recent years despite a lukewarm reaction to the election of Scottish American Donald Trump as President due to long running legal disputes.

Having opposed membership in the 1975 referendum, the party has supported membership of the European Union since the adoption of the Independence in Europe policy during the 1980s. Consequentially, the SNP supported remaining within the EU during the 2016 referendum where every Scottish council area backed this position. Consequently, the party opposed Brexit and sought a People's Vote on the withdrawal agreement, ultimately unsuccessfully. Indeed, the SNP would like to see an independent Scotland as a member of the European Union and NATO and has left open the prospect of an independent Scotland joining the Euro. 

The SNP has also taken a stance against Russian interference abroad – the party supporting the enlargement of the EU and NATO to areas such as the Western Balkans and Ukraine to counter this influence. Indeed, the party has called for repercussions regarding the poisoning of Sergei and Yulia Skripal and has criticised former leader Alex Salmond for broadcasting a chat show on Kremlin-backed network RT. Consequently, party representatives have expressed support for movements such as Euromaidan that support the independence of countries across Eastern Europe.

The party have supported measures including foreign aid which seek to facilitate international development through various charitable organisations. In recognition of Scotland's historic links to the country, these programmes are mostly focused in Malawi in common with previous Scottish governments. With local authorities across the country, including Glasgow City Council being involved in this partnership since before the SNP took office in 2007.

The SNP have pledged to uphold the public service nature of NHS Scotland and are consequently opposed to any attempts at privatisation of the health service, including any inclusion in a post-Brexit trade deal with the United States. The party has been fond of increasing provision under the NHS with the introduction of universal baby boxes based on the Finnish scheme. This supported child development alongside other commitments including the expansion of free childcare for children younger than school age and the introduction of universal free school meals in the first three years of school.

Previously, SNP governments have abolished hospital parking charges as well as prescription charges in efforts to promote enhanced public health outcomes by increasing access to care and treatment. Furthermore, during Sturgeon's premiership, Scotland became the first country in the world to introduce alcohol minimum unit pricing to counter alcohol problems. Recently, the party has also committed to providing universal access to sanitary products and the liberalisation of drugs policy through devolution, in an effort to increase access to treatment and improve public health outcomes.

The party also promotes universal access to education, with one of the first acts of the Salmond government being to abolish tuition fees. More cently, the party has turned its attention to widening access to higher education with Nicola Sturgeon stating that education is her number one priority. At school level, the Curriculum for Excellence is currently undergoing a review.

The foundations of the SNP are a belief that Scotland would be more prosperous by being governed independently from the United Kingdom, although the party was defeated in the 2014 referendum on this issue. The party has since sought to hold a second referendum at some point in the future, perhaps related to the outcome of Brexit, as the party sees a referendum as the only route to independence. The party is constitutionalist and as such rejects holding such a referendum unilaterally or any course of actions that could lead to comparisons with cases such as Catalonia with the party seeing independence as a process that should be undertaken through a consensual process alongside the UK Government. As part of this process towards independence, the party supports increased devolution to the Scottish Parliament and the Scottish Government, particularly in areas such as welfare and immigration.

Official SNP policy is supportive of the monarchy however members are divided on the issue. The party does propose reducing the funds spent on the royal family. Separately, the SNP has always opposed the UK's unelected upper house and would like to see both it and the House of Commons elected by a form of proportional representation. The party also supports the introduction of a written constitution, either for an independent Scotland or the UK as a whole, going as far as producing a proposed interim constitution for Scotland during the independence referendum campaign.

With how to achieve independence, the party was traditionally split between fundamentalists and gradualists.

The SNP leadership generally subscribe to the gradualist viewpoint, that being the idea that Scottish independence can be won by the accumulation by the Scottish Parliament of powers that the UK Parliament currently has over time. It is also a philosophy that emphasises the election of an SNP government should bring about trust in the Scottish people in the ability to govern themselves, thus bringing increased support for independence.

Fundamentalism stands in opposition to the so-called gradualist point of view, which believes that the SNP should emphasise independence more widely to achieve it. The argument goes that if the SNP is unprepared to argue for its central policy then it is unlikely ever to persuade the public of its worthiness.







, the Cabinet of the Scottish Government is as follows:
As of January 2020, the Shadow Cabinet of the SNP in Westminster is as follows.

The SNP had 431 councillors in Local Government elected from the 2017 Scottish local elections.





</doc>
<doc id="28889" url="https://en.wikipedia.org/wiki?curid=28889" title="Scotch-Irish">
Scotch-Irish

Scotch-Irish or Scots-Irish may refer to:



</doc>
<doc id="28891" url="https://en.wikipedia.org/wiki?curid=28891" title="Snowy Mountains">
Snowy Mountains

The Snowy Mountains, known informally as "The Snowies", is an IBRA subregion and the highest mountain range on the continent of mainland Australia. It contains the Australian mainland's highest mountain, Mount Kosciuszko, which reaches to a height of above sea level. The range also contains the five highest peaks on the Australian mainland (including Mount Kosciuszko), all of which are above . They are located in southern New South Wales and are part of the larger Australian Alps and Great Dividing Range, experiencing large natural snowfalls every winter. Snow normally falls during June, July, August and early September, with the snow cover melting by late spring. The Tasmanian highlands makes up the other (major) alpine region present in Australia.

The range is host to the mountain plum-pine, a low-lying type of conifer. It is considered to be one of the centres of the Australian ski industry during the winter months, with all four snow resorts in New South Wales being located in the region.

The Alpine Way and Snowy Mountains Highway are the major roads through the Snowy Mountains region.

The mountain range is thought to have had Aboriginal occupation for 20,000 years. Large scale intertribal gatherings were held in the High Country during summer for collective feasting on the Bogong moth. This practice continued until around 1865.

The area was first explored by Europeans in 1835, and in 1840, Edmund Strzelecki ascended Mount Kosciuszko and named it after the Polish patriot. High country stockmen followed who used the Snowy Mountains for grazing during the summer months. Banjo Paterson's famous poem The Man From Snowy River recalls this era. The cattle graziers have left a legacy of mountain huts scattered across the area. Today these huts are maintained by the National Parks and Wildlife Service or volunteer organisations like the Kosciuszko Huts Association.

In the 19th century gold was mined on the high plains near Kiandra. At its height this community had a population of about 4,000 people, and ran 14 hotels. Since the last resident left in 1974, Kiandra has become a ghost town of ruins and abandoned diggings.

The Kosciuszko National Park came into existence as the National Chase Snowy Mountains on 5 December 1906. In 1944 this became the Kosciuszko State Park, and then the Kosciuszko National Park in 1967.

Recreational skiing began at Kiandra in the 1860s and experienced a boom in the 20th century following the commencement of the construction of the Snowy Mountains Hydro-Electric Scheme between 1949 and 1976 which brought many European workers to the district and opened up access to the ranges.

The discovery of gold at Kiandra (elevation ), in 1859, briefly enticed a population of thousands above the snowline and saw the introduction of recreational skiing to the Snowy Mountains around 1861. The Kiandra Goldrush was short-lived, but the township remained a service centre for recreational and survival skiing for over a century. Australia's first T-Bar was installed at Kiandra in 1957, but the ski facilities were finally shifted up the hill to Selwyn Snowfields in 1978. Steeper slopes and more reliable snows lie further to the south and in the 20th Century, the focus of recreational skiing in New South Wales shifted southward, to the Mount Kosciuszko region.

The Kosciuszko Chalet was built at Charlotte Pass in 1930, giving relatively comfortable access to Australia's highest terrain. In 1964, Australia briefly boasted the "World's Longest Chairlift", designed to carry skiers from the Thredbo Valley to Charlotte Pass, but technical difficulties soon closed the facility. At , Charlotte Pass has the highest village base elevation of any Australia ski resort and can only be accessed via over-snow transport in winter. The growing number of ski enthusiasts heading to Charlotte Pass led to the establishment of a cafe at Smiggin Holes around 1939, where horse-drawn sleighs would deliver skiers to begin the arduous ski to the Kosciusko Chalet. It was the construction of the vast Snowy Mountains Hydro-Electric Scheme from 1949 that really opened up the Snowy Mountains for large scale development of a ski industry and led to the establishment of Thredbo and Perisher as leading Australian resorts. The Construction of Guthega Dam brought skiers to the isolated Guthega district and a rope tow was installed there in 1957.

Skifields up by Kosciusko's side were also established during this period, though their existence is now little realised. The Australian Alpine Club was founded in 1950 by Charles Anton. Huts were constructed in the "Backcountry" close to Mount Kosciusko, including Kunama Hut, which opened for the 1953 season. A rope tow was installed on Mount Northcote at the site and opened in 1954. The site proved excellent for speed skiing, but the hut was destroyed in an avalanche, which also killed one person, in 1956.

Anton also recognised the potential of the Thredbo Valley for construction of a major resort and village, with good vertical terrain. Construction began in 1957. Today, Thredbo has 14 ski-lifts and possesses Australia's longest ski resort run, the 5.9 km from Karel's T-Bar to Friday Flat; Australia's greatest vertical drop of 672 m; and the highest lifted point in Australia at 2037 m.

The last establishment of a major skifield in NSW came with the development of Mount Blue Cow in the 1980s. In 1987 the Swiss designed Skitube Alpine Railway opened to deliver skiers from Bullocks Flat, on the Alpine Way, to Perisher Valley and to Blue Cow, which also opened in 1987. The operators of Blue Cow purchased Guthega in 1991, and the new combined resort later merged with Perisher-Smiggins to become the largest ski resort in the Southern Hemisphere. In 2009 Perisher had 48 lifts covering 1,245 hectares and four village base areas: Perisher Valley, Blue Cow, Smiggin Holes and Guthega.

The Snowy Mountains also feed the Murrumbidgee and Murray rivers from the Tooma River, Whites River and Yarrangobilly River. The range is perhaps best known for the Snowy Mountains Scheme, a project to dam the Snowy River, providing both water for irrigation and hydroelectricity.

The project began in 1949 employing 100,000 men, two-thirds of whom came from thirty other countries during the post-World War II years. Socially this project symbolises a period during which Australia became an ethnic "melting pot" of the twentieth century but which also changed Australia's character and increased its appreciation for a wide range of cultural diversity.

The Scheme built several temporary towns for its construction workers, several of which have become permanent: Cabramurra (the highest town in Australia); and Khancoban. Additionally, the economy of Cooma has been sustained by the Scheme. Townships at Adaminaby, Jindabyne and Talbingo were inundated by the construction of Lakes Eucumbene, Jindabyne and Talbingo. Improved vehicular access to the High Country enabled ski-resort villages to be constructed at Thredbo and Guthega in the 1950s by ex-Snowy Scheme workers who realised the potential for expansion of the Australian ski industry.

By 1974, of tunnels and of aqueducts connected the sixteen dams, seven power stations (two underground), and one pumping station. The American Society of Civil Engineers has rated the Snowy Scheme as "a world-class civil engineering project".

The principal lakes created by the scheme include: Lake Eucumbene, Blowering Dam, Talbingo Dam, Lake Jindabyne and Tantangara Dam.

The higher regions of the park experience an alpine climate which is unusual on mainland Australia. However, only the peaks of the main range are subject to consistent heavy winter snow. The climate station at Charlotte Pass recorded Australia's lowest temperature of -23.0 °C on 28 June 1994.

Part of the mountains known as Main Range contains mainland Australia's five glacial lakes. The largest of these lakes is Blue Lake, one of the headwaters of the Snowy River. The other four glacial lakes are Lake Albina, Lake Cootapatamba, Club Lake and Headley Tarn.

During the last ice age, which peaked about 20,000 years ago in the Pleistocene epoch, the highest peaks of the main range near Mount Kosciuszko experienced a climate which favoured the formation of glaciers, evidence of which can still be seen today. Cirques moraines, tarn lakes, roche moutonnées and other glacial features can all be seen in the area. Lake Cootapatamba, which was formed by an ice spilling from Mount Kosciuszko's southern flank, is the highest lake on the Australian mainland. Lake Albina, Club Lake, Blue Lake, and Hedley Tarn also have glacial origins.

There is some disagreement as to exactly how widespread Pleistocene glaciation was on the main range, and little or no evidence from earlier glacial periods exists. The 'David Moraine', a one kilometre long ridge running across Spencers Creek valley seems to indicate a larger glacier existed in this area at some time, however the glacial origin of this feature is disputed.

There is evidence of periglacial activity in the area. Solifluction appears to have created terraces on the north west flank of Mount Northcote. Frost heave is also a significant agent of soil erosion in the Kosciuszko Area.

The Snowy Mountains cover a variety of climatic regions which support several distinct ecosystems. The alpine area above the tree line is one of the most fragile and covers the smallest area. This area is a patchwork of alpine heaths, herbfields, feldmarks, bogs and fens. The windswept feldmark ecotope is endemic to the alpine region, and covers a mere . It is most vulnerable to the wandering footsteps of unmindful tourists.

Many rare or threatened plant and animal species occur within the Snowy Mountains. The Kosciuszko National Park is home to one of Australia's most threatened species (the corroboree frog). The endangered mountain pygmy possum and the more common dusky antechinus are located in the high country of the park.

By 2008, wild horse numbers in the National Park had reached 1,700 with that figure growing by 300 each year, resulting in park authorities coordinating their culling and relocation.

The high country is dominated by alpine woodlands, characterised by the snow gum. Montane and wet sclerophyll forests also occur across the ranges, supporting large stands of alpine ash and mountain gum. In the southern Byadbo wilderness area, dry sclerophyll and wattle forests predominate. Amongst the many different native trees in the park, the large Chinese elm has become naturalised.

In summer 2003, the Australian Alps experienced their largest bushfires for over 60 years with an estimated 1.73 million hectares burning. The bushfires burnt across Victoria, New South Wales (NSW) and the Australian Capital Territory (ACT) during a drought that ranks as one of the worst in 103 years of official Australian weather records. Fires are a natural feature of the park's ecosystem, but it will take some time for the region to return to its pre-2003 condition.
In November 2004, a committee "The Snowy Mountains Bushfire Recovery Taskforce" was set up by the NSW State Premier's Department to help residents in the region recover from the fires. The Taskforce commissioned Louise Darmody from Sound Memories to produce a documentary involving 26 people from the Snowy Mountains to talk about their experiences. The interviewees included farmers, school children, volunteers and employees from the NSW Rural Fire Service and National Park Snowy Hydro.

Again in 2020, there was a big bushfire in the Snowy Mountains. 




</doc>
<doc id="28892" url="https://en.wikipedia.org/wiki?curid=28892" title="Skara Brae">
Skara Brae

Skara Brae is a stone-built Neolithic settlement, located on the Bay of Skaill on the west coast of Mainland, the largest island in the Orkney archipelago of Scotland. Consisting of eight clustered houses, it was occupied from roughly 3180 BC to about 2500 BC and is Europe's most complete Neolithic village. Skara Brae gained UNESCO World Heritage Site status as one of four sites making up "The Heart of Neolithic Orkney". Older than Stonehenge and the Great Pyramids, it has been called the "Scottish Pompeii" because of its excellent preservation.

In the winter of 1850 a severe storm hit Scotland causing widespread damage and over 200 deaths. In the Bay of Skaill the storm stripped the earth from a large irregular knoll known as "Skerrabra". When the storm cleared, local villagers found the outline of a village consisting of a number of small houses without roofs. William Watt of Skaill, the local laird, began an amateur excavation of the site, but after four houses were uncovered the work was abandoned in 1868. The site remained undisturbed until 1913, when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artefacts. In 1924 another storm swept away part of one of the houses, and it was determined the site should be secured and properly investigated. The job was given to the University of Edinburgh’s Professor V. Gordon Childe, who travelled to Skara Brae for the first time in mid-1927.

The inhabitants of Skara Brae were makers and users of grooved ware, a distinctive style of pottery that had recently appeared in northern Scotland. The houses used earth sheltering, being sunk into the ground. They were sunk into mounds of pre-existing prehistoric domestic waste known as middens. This provided the houses with a stability and also acted as insulation against Orkney's harsh winter climate. On average, each house measures with a large square room containing a stone hearth used for heating and cooking. Given the number of homes, it seems likely that no more than fifty people lived in Skara Brae at any given time.

It is not clear what material the inhabitants burned in their hearths. Childe was sure that the fuel was peat, but a detailed analysis of vegetation patterns and trends suggests that climatic conditions conducive to the development of thick beds of peat did not develop in this part of Orkney until after Skara Brae was abandoned. Other possible fuels include driftwood and animal dung. There is evidence that dried seaweed may have been used significantly. At some sites in Orkney, investigators have found a glassy, slag-like material called "kelp" or "cramp" that may be residual burnt seaweed.

The dwellings contain a number of stone-built pieces of furniture, including cupboards, dressers, seats, and storage boxes. Each dwelling was entered through a low doorway that had a stone slab door that could be closed "by a bar that slid in bar-holes cut in the stone door jambs". A number of dwellings offered a small connected antechamber, offering access to a partially covered stone drain leading away from the village. It is suggested that these chambers served as indoor privies.

Seven of the houses have similar furniture, with the beds and dresser in the same places in each house. The dresser stands against the wall opposite the door, and was the first thing seen by anyone entering the dwelling. Each of these houses had the larger bed on the right side of the doorway and the smaller on the left. Lloyd Laing noted that this pattern accorded with Hebrides custom up to the early 20th century suggesting that the husband's bed was the larger and the wife's was the smaller. The discovery of beads and paint-pots in some of the smaller beds may support this interpretation. Additional support may come from the recognition that stone boxes lie to the left of most doorways, forcing the person entering the house to turn to the right-hand, "male", side of the dwelling. At the front of each bed lie the stumps of stone pillars that may have supported a canopy of fur; another link with recent Hebridean style.
House 8 has no storage boxes or dresser and has been divided into something resembling small cubicles. Fragments of stone, bone and antler were excavated suggesting the house may have been used to make tools such as bone needles or flint axes. The presence of heat-damaged volcanic rocks and what appears to be a flue, support this interpretation. House 8 is distinctive in other ways as well: it is a stand-alone structure not surrounded by midden; instead it is above ground with walls over thick and has a "porch" protecting the entrance.

The site provided the earliest known record of the human flea ("Pulex irritans") in Europe.

The Grooved Ware People who built Skara Brae were primarily pastoralists who raised cattle and sheep. Childe originally believed that the inhabitants did not practice agriculture, but excavations in 1972 unearthed seed grains from a midden suggesting that barley was cultivated. Fish bones and shells are common in the middens indicating that dwellers ate seafood. Limpet shells are common and may have been fish-bait that was kept in stone boxes in the homes. The boxes were formed from thin slabs with joints carefully sealed with clay to render them waterproof.

This pastoral lifestyle is in sharp contrast to some of the more exotic interpretations of the culture of the Skara Brae people. Euan MacKie suggested that Skara Brae might be the home of a privileged theocratic class of wise men who engaged in astronomical and magical ceremonies at nearby Ring of Brodgar and the Standing Stones of Stenness. Graham and Anna Ritchie cast doubt on this interpretation noting that there is no archaeological evidence for this claim, although a Neolithic "low road" that goes from Skara Brae passes near both these sites and ends at the chambered tomb of Maeshowe. Low roads connect Neolithic ceremonial sites throughout Britain.
Originally, Childe believed that the settlement dated from around 500 BC. This interpretation was coming under increasing challenge by the time new excavations in 1972–73 settled the question. Radiocarbon results obtained from samples collected during these excavations indicate that occupation of Skara Brae began about 3180 BC with occupation continuing for about six hundred years. Around 2500 BC, after the climate changed, becoming much colder and wetter, the settlement may have been abandoned by its inhabitants. There are many theories as to why the people of Skara Brae left; particularly popular interpretations involve a major storm. Evan Hadingham combined evidence from found objects with the storm scenario to imagine a dramatic end to the settlement:
Anna Ritchie strongly disagrees with catastrophic interpretations of the village's abandonment:
The site was farther from the sea than it is today, and it is possible that Skara Brae was built adjacent to a fresh water lagoon protected by dunes. Although the visible buildings give an impression of an organic whole, it is certain that an unknown quantity of additional structures had already been lost to sea erosion before the site's rediscovery and subsequent protection by a seawall. Uncovered remains are known to exist immediately adjacent to the ancient monument in areas presently covered by fields, and others, of uncertain date, can be seen eroding out of the cliff edge a little to the south of the enclosed area.

A number of enigmatic carved stone balls have been found at the site and some are on display in the museum. Similar objects have been found throughout northern Scotland. The spiral ornamentation on some of these "balls" has been stylistically linked to objects found in the Boyne Valley in Ireland. Similar symbols have been found carved into stone lintels and bed posts. These symbols, sometimes referred to as "runic writings", have been subjected to controversial translations. For example, Castleden suggested that "colons" found punctuating vertical and diagonal symbols may represent separations between words.

Lumps of red ochre found here and at other Neolithic sites have been interpreted as evidence that body painting may have been practised. Nodules of haematite with highly polished surfaces have been found as well; the shiny surfaces suggest that the nodules were used to finish leather.

Other artefacts excavated on site made of animal, fish, bird, and whalebone, whale and walrus ivory, and killer whale teeth included awls, needles, knives, beads, adzes, shovels, small bowls and, most remarkably, ivory pins up to long. These pins are very similar to examples found in passage graves in the Boyne Valley, another piece of evidence suggesting a linkage between the two cultures. So-called Skaill knives were commonly used tools in Skara Brae; these consist of large flakes knocked off sandstone cobbles. Skaill knives have been found throughout Orkney and Shetland.

The 1972 excavations reached layers that had remained waterlogged and had preserved items that otherwise would have been destroyed. These include a twisted skein of Heather, one of a very few known examples of Neolithic rope, and a wooden handle.

A comparable, though smaller, site exists at Rinyo on Rousay. Unusually, no Maeshowe-type tombs have been found on Rousay and although there are a large number of Orkney–Cromarty chambered cairns, these were built by Unstan ware people.

Knap of Howar, on the Orkney island of Papa Westray, is a well-preserved Neolithic farmstead. Dating from 3500 BC to 3100 BC, it is similar in design to Skara Brae, but from an earlier period, and it is thought to be the oldest preserved standing building in northern Europe.

There is also a site currently under excavation at Links of Noltland on Westray that appears to have similarities to Skara Brae.

"The Heart of Neolithic Orkney" was inscribed as a World Heritage site in December 1999. In addition to Skara Brae the site includes Maeshowe, the Ring of Brodgar, the Standing Stones of Stenness and other nearby sites. It is managed by Historic Environment Scotland, whose "Statement of Significance" for the site begins:

In 2019, a risk assessment was performed to assess the site's vulnerability to climate change. The report by Historic Environment Scotland, the Orkney Islands Council and others concludes that the entire Heart of Neolithic Orkney World Heritage Site, and in particular Skara Brae, is "extremely vulnerable" to climate change due to rising sea levels, increased rainfall and other factors; it also highlights the risk that Skara Brae could be partially destroyed by one unusually severe storm.




</doc>
<doc id="28893" url="https://en.wikipedia.org/wiki?curid=28893" title="Sinners in the Hands of an Angry God">
Sinners in the Hands of an Angry God

"Sinners in the Hands of an Angry God" is a sermon written by British Colonial Christian theologian Jonathan Edwards, preached to his own congregation in Northampton, Massachusetts, to profound effect, and again on July 8, 1741 in Enfield, Connecticut. The preaching of this sermon was the catalyst for the First Great Awakening. Like Edwards' other works, it combines vivid imagery of Hell with observations of the world and citations of the scripture. It is Edwards' most famous written work, is a fitting representation of his preaching style, and is widely studied by Christians and historians, providing a glimpse into the theology of the First Great Awakening of c. 1730–1755.

This is a typical sermon of the Great Awakening, emphasizing the teaching that Hell is real—a place that actually exists. Edwards hoped that the imagery and language of his sermon would awaken audiences to the horrific reality of hell that awaits them should they continue living without calling on Christ to be saved. The underlying point is that God has given humans a chance to confess their sins. Edwards said that it is the mere will of God that keeps wicked men from the depths of Hell. This act of restraint has given humans a chance to believe and trust in Christ.

""There is nothing that keeps wicked men at any one moment out of hell, but the mere pleasure of God.""

Most of the sermon's text consists of ten "considerations":

One church in Enfield, Connecticut, had been largely unaffected during the First Great Awakening of New England. Edwards was invited by the pastor of the church to preach to them. Edwards's aim was to teach his listeners about the horrors of hell, the dangers of sin, and the terrors of being lost. Edwards described the position of those who do not follow Christ's urgent call to receive forgiveness. Edwards scholar John E. Smith notes that despite the apparent pessimism of the notion of an angry God, that pessimism is "overcome by the comforting hope of salvation through a triumphant, loving savior. Whenever Edwards preached terror, it was part of a larger campaign to turn sinners from their disastrous path and to the rightful object of their affections, Jesus Christ."

In the final section of "Sinners in the Hands of an Angry God," Edwards shows that his theological argument holds throughout scripture and biblical history. He invokes stories and examples throughout the whole Bible. Edwards ends the sermon with one final appeal: "Therefore let everyone that is out of Christ, now awake and fly from the wrath to come." According to Edwards, only by returning to Christ can one escape the stark fate he outlines.

Edwards was interrupted many times during the sermon by people moaning and crying out, "What shall I do to be saved?". Although the sermon has received criticism, Edwards' words have endured and are still read to this day. Edwards' sermon continues to be the leading example of a First Great Awakening sermon and is still used in religious and academic studies.

Since the 1950s, a number of critical perspectives were used to analyse the sermon. The first comprehensive academic analysis of "Sinners in the Hands of an Angry God" was published by Edwin Cady in 1949, who comments on the imagery of the sermon and distinguishes between the "cliché" and "fresh" figurative images, stressing how the former related to the colonial life. Lee Stuart questions that the message of the sermon was solely negative and attributes its success to the final passages in which the sinners are actually "comforted". Rosemary Hearn argues that it is the logical structure of the sermon that constitutes its most important persuasive element. Lemay looks into the changes in the syntactic categories, like grammatical tenses, in the text of the sermon. Lukasik stresses how in the sermon Edwards appropriates Newtonian physics, especially the image of the gravitational pull that would relentlessly bring the sinners down. Gallagher focuses on the "beat" of the sermon, and on how the consecutive structural elements of the sermon serve different persuasive aims. Choiński suggests that the rhetorical success of the sermon consists in the use of the "deictic shift" that transported the hearers mentally into the figurative images of hell.

Ironically, Jonathan Edwards wrote and spoke a great deal on heaven and angels writes John Gerstner in "Jonathan Edwards on Heaven and Hell,1998"and those theme are less remembered, namely "Heaven is a World of Love".





</doc>
<doc id="28894" url="https://en.wikipedia.org/wiki?curid=28894" title="Scottish Highlands">
Scottish Highlands

The Highlands (; , 'the place of the Gaels') is a historic region of Scotland. Culturally, the Highlands and the Lowlands diverged from the later Middle Ages into the modern period, when Lowland Scots replaced Scottish Gaelic throughout most of the Lowlands. The term is also used for the area north and west of the Highland Boundary Fault, although the exact boundaries are not clearly defined, particularly to the east. The Great Glen divides the Grampian Mountains to the southeast from the Northwest Highlands. The Scottish Gaelic name of "" literally means "the place of the Gaels" and traditionally, from a Gaelic-speaking point of view, includes both the Western Isles and the Highlands.

The area is very sparsely populated, with many mountain ranges dominating the region, and includes the highest mountain in the British Isles, Ben Nevis. During the 18th and early 19th centuries the population of the Highlands rose to around 300,000, but from c. 1841 and for the next 160 years, the natural increase in population was exceeded by emigration (mostly to Canada, the United States, Australia and New Zealand, and migration to the industrial cities of Scotland and England.) The area is now one of the most sparsely populated in Europe. At in 2012, the population density in the Highlands and Islands is less than one seventh of Scotland's as a whole, comparable with that of Bolivia, Chad and Russia.

The Highland Council is the administrative body for much of the Highlands, with its administrative centre at Inverness. However, the Highlands also includes parts of the council areas of Aberdeenshire, Angus, Argyll and Bute, Moray, North Ayrshire, Perth and Kinross, Stirling and West Dunbartonshire.

The Scottish highlands is the only area in the British Isles to have the taiga biome as it features concentrated populations of Scots pine forest: see Caledonian Forest.

Between the 15th century and the mid-20th century, the area differed from most of the Lowlands in terms of language. In Scottish Gaelic, the region is known as the "", because it was traditionally the Gaelic-speaking part of Scotland, although the language is now largely confined to The Hebrides. The terms are sometimes used interchangeably but have different meanings in their respective languages. Scottish English (in its Highland form) is the predominant language of the area today, though Highland English has been influenced by Gaelic speech to a significant extent. Historically, the "Highland line" distinguished the two Scottish cultures. While the Highland line broadly followed the geography of the Grampians in the south, it continued in the north, cutting off the north-eastern areas, that is Eastern Caithness, Orkney and Shetland, from the more Gaelic Highlands and Hebrides.

Historically, the major social unit of the Highlands was the clan. Scottish kings, particularly James VI, saw clans as a challenge to their authority; the Highlands was seen by many as a lawless region. Following the Union of the Crowns, James VI had the military strength to back up any attempts to impose some control. The result was, in 1609, the Statutes of Iona which started the process of integrating clan leaders into Scottish society. The gradual changes continued into the 19th century, as clan chiefs thought of themselves less as patriarchal leaders of their people and more as commercial landlords. The first effect on the clansmen who were their tenants was the change to rents being payable in money rather than in kind. Later, rents were increased as Highland landowners sought to increase their income. This was followed, mostly in the period 1760–1850, by agricultural improvement that often (particularly in the Western Highlands) involved clearance of the population to make way for large scale sheep farms. Displaced tenants were set up in crofting communities in the process. The crofts were intended not to provide all the needs of their occupiers; they were expected to work in other industries such as kelping and fishing. Crofters came to rely substantially on seasonal migrant work, particularly in the Lowlands. This gave impetus to the learning of English, which was seen by many rural Gaelic speakers to be the essential "language of work".

Older historiography attributes the collapse of the clan system to the aftermath of the Jacobite risings. This is now thought less influential by historians. Following the Jacobite rising of 1745 the British government enacted a series of laws to try to suppress the clan system, including bans on the bearing of arms and the wearing of tartan, and limitations on the activities of the Scottish Episcopal Church. Most of this legislation was repealed by the end of the 18th century as the Jacobite threat subsided. There was soon a rehabilitation of Highland culture. Tartan was adopted for Highland regiments in the British Army, which poor Highlanders joined in large numbers in the era of the Revolutionary and Napoleonic Wars (1790–1815). Tartan had largely been abandoned by the ordinary people of the region, but in the 1820s, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe. The international craze for tartan, and for idealising a romanticised Highlands, was set off by the Ossian cycle, and further popularised by the works of Walter Scott. His "staging" of the visit of King George IV to Scotland in 1822 and the king's wearing of tartan resulted in a massive upsurge in demand for kilts and tartans that could not be met by the Scottish woollen industry. Individual clan tartans were largely designated in this period and they became a major symbol of Scottish identity. This "Highlandism", by which all of Scotland was identified with the culture of the Highlands, was cemented by Queen Victoria's interest in the country, her adoption of Balmoral as a major royal retreat, and her interest in "tartenry".

Recurrent famine affected the Highlands for much of its history, with significant instances as late as 1817 in the Eastern Highlands and the early 1850s in the West. Over the 18th century, the region had developed a trade of black cattle into Lowland markets, and this was balanced by imports of meal into the area. There was a critical reliance on this trade to provide sufficient food, and it is seen as an essential prerequisite for the population growth that started in the 18th century. Most of the Highlands, particularly in the North and West was short of the arable land that was essential for the mixed, run rig based, communal farming that existed before agricultural improvement was introduced into the region. Between the 1760s and the 1830s there was a substantial trade in unlicensed whisky that had been distilled in the Highlands. Lowland distillers (who were not able to avoid the heavy taxation of this product) complained that Highland whisky made up more than half the market. The development of the cattle trade is taken as evidence that the pre-improvement Highlands was not an immutable system, but did exploit the economic opportunities that came its way. The illicit whisky trade demonstrates the entrepreneurial ability of the peasant classes.

Agricultural improvement reached the Highlands mostly over the period 1760 to 1850. Agricultural advisors, factors, land surveyors and others educated in the thinking of Adam Smith were keen to put into practice the new ideas taught in Scottish universities. Highland landowners, many of whom were burdened with chronic debts, were generally receptive to the advice they offered and keen to increase the income from their land. In the East and South the resulting change was similar to that in the Lowlands, with the creation of larger farms with single tenants, enclosure of the old run rig fields, introduction of new crops (such as turnips), land drainage and, as a consequence of all this, eviction, as part of the Highland clearances, of many tenants and cottars. Some of those cleared found employment on the new, larger farms, others moved to the accessible towns of the Lowlands.

In the West and North, evicted tenants were usually given tenancies in newly created crofting communities, whilst their former holdings were converted into large sheep farms. Sheep farmers could pay substantially higher rents than the run rig farmers and were much less prone to falling into arrears. Each croft was limited in size so that the tenants would have to find work elsewhere. The major alternatives were fishing and the kelp industry. Landlords took control of the kelp shores, deducting the wages earned by their tenants from the rent due and retaining the large profits that could be earned at the high prices paid for the processed product during the Napoleonic wars.

When the Napoleonic wars finished in 1815, the Highland industries were affected by the return to a peacetime economy. The price of black cattle fell, nearly halving between 1810 and the 1830s. Kelp prices had peaked in 1810, but reduced from £9 a ton in 1823 to £3 13s 4d a ton in 1828. Wool prices were also badly affected. This worsened the financial problems of debt-encumbered landlords. Then, in 1846, potato blight arrived in the Highlands, wiping out the essential subsistence crop for the overcrowded crofting communities. As the famine struck, the government made clear to landlords that it was their responsibility to provide famine relief for their tenants. The result of the economic downturn had been that a large proportion of Highland estates were sold in the first half of the 19th century. T M Devine points out that in the region most affected by the potato famine, by 1846, 70 per cent of the landowners were new purchasers who had not owned Highland property before 1800. More landlords were obliged to sell due to the cost of famine relief. Those who were protected from the worst of the crisis were those with extensive rental income from sheep farms. Government loans were made available for drainage works, road building and other improvements and many crofters became temporary migrants – taking work in the Lowlands. When the potato famine ceased in 1856, this established a pattern of more extensive working away from the Highlands.

The unequal concentration of land ownership remained an emotional and controversial subject, of enormous importance to the Highland economy, and eventually became a cornerstone of liberal radicalism. The poor crofters were politically powerless, and many of them turned to religion. They embraced the popularly oriented, fervently evangelical Presbyterian revival after 1800. Most joined the breakaway "Free Church" after 1843. This evangelical movement was led by lay preachers who themselves came from the lower strata, and whose preaching was implicitly critical of the established order. The religious change energised the crofters and separated them from the landlords; it helped prepare them for their successful and violent challenge to the landlords in the 1880s through the Highland Land League.
Violence erupted, starting on the Isle of Skye, when Highland landlords cleared their lands for sheep and deer parks. It was quietened when the government stepped in, passing the Crofters' Holdings (Scotland) Act, 1886 to reduce rents, guarantee fixity of tenure, and break up large estates to provide crofts for the homeless. This contrasted with the Irish Land War under way at the same time, where the Irish were intensely politicised through roots in Irish nationalism, while political dimensions were limited. In 1885 three Independent Crofter candidates were elected to Parliament, which listened to their pleas. The results included explicit security for the Scottish smallholders in the "crofting counties"; the legal right to bequeath tenancies to descendants; and the creation of a Crofting Commission. The Crofters as a political movement faded away by 1892, and the Liberal Party gained their votes.

Today, the Highlands are the largest of Scotland's whisky producing regions; the relevant area runs from Orkney to the Isle of Arran in the south and includes the northern isles and much of Inner and Outer Hebrides, Argyll, Stirlingshire, Arran, as well as sections of Perthshire and Aberdeenshire. (Other sources treat The Islands, except Islay, as a separate whisky producing region.) This massive area has over 30 distilleries, or 47 when the Islands sub-region is included in the count. According to one source, the top five are The Macallan, Glenfiddich, Aberlour, Glenfarclas and Balvenie. While Speyside is geographically within the Highlands, that region is specified as distinct in terms of whisky productions. Speyside single malt whiskies are produced by about 50 distilleries.

According to "Visit Scotland", Highlands whisky is "fruity, sweet, spicy, malty". Another review states that Northern Highlands single malt is "sweet and full-bodied", the Eastern Highlands and Southern Highlands whiskies tend to be "lighter in texture" while the distilleries in the Western Highlands produce single malts with a "much peatier influence".

The Scottish Reformation achieved partial success in the Highlands. Roman Catholicism remained strong in some areas, owing to remote locations and the efforts of Franciscan missionaries from Ireland, who regularly came to celebrate Mass. There remain significant Catholic strongholds within the Highlands and Islands such as Moidart and Morar on the mainland and South Uist and Barra in the southern Outer Hebrides.
The remoteness of the region and the lack of a Gaelic-speaking clergy undermined the missionary efforts of the established church. The later 18th century saw somewhat greater success, owing to the efforts of the SSPCK missionaries and to the disruption of traditional society after the Battle of Culloden in 1746. In the 19th century, the evangelical Free Churches, which were more accepting of Gaelic language and culture, grew rapidly, appealing much more strongly than did the established church.

For the most part, however, the Highlands are considered predominantly Protestant, loyal to the Church of Scotland. In contrast to the Catholic southern islands, the northern Outer Hebrides islands (Lewis, Harris and North Uist) have an exceptionally high proportion of their population belonging to the Protestant Free Church of Scotland or the Free Presbyterian Church of Scotland. The Outer Hebrides have been described as the last bastion of Calvinism in Britain and the Sabbath remains widely observed. Inverness and the surrounding area has a majority Protestant population, with most locals belonging to either The Kirk or the Free Church of Scotland. The church maintains a noticeable presence within the area, with church attendance notably higher than in other parts of Scotland. Religion continues to play an important role in Highland culture, with Sabbath observance still widely practised, particularly in the Hebrides.

In traditional Scottish geography, the Highlands refers to that part of Scotland north-west of the Highland Boundary Fault, which crosses mainland Scotland in a near-straight line from Helensburgh to Stonehaven. However the flat coastal lands that occupy parts of the counties of Nairnshire, Morayshire, Banffshire and Aberdeenshire are often excluded as they do not share the distinctive geographical and cultural features of the rest of the Highlands. The north-east of Caithness, as well as Orkney and Shetland, are also often excluded from the Highlands, although the Hebrides are usually included. The Highland area, as so defined, differed from the Lowlands in language and tradition, having preserved Gaelic speech and customs centuries after the anglicisation of the latter; this led to a growing perception of a divide, with the cultural distinction between Highlander and Lowlander first noted towards the end of the 14th century. In Aberdeenshire, the boundary between the Highlands and the Lowlands is not well defined. There is a stone beside the A93 road near the village of Dinnet on Royal Deeside which states 'You are now in the Highlands', although there are areas of Highland character to the east of this point.

A much wider definition of the Highlands is that used by the Scotch Whisky industry. Highland Single Malts are produced at distilleries north of an imaginary line between Dundee and Greenock, thus including all of Aberdeenshire and Angus.

Inverness is regarded as the Capital of the Highlands, although less so in the Highland parts of Aberdeenshire, Angus, Perthshire and Stirlingshire which look more to Aberdeen, Dundee, Perth, and Stirling as their commercial centres. 

The Highland Council area, created as one of the local government regions of Scotland, has been a unitary council area since 1996. The council area excludes a large area of the southern and eastern Highlands, and the Western Isles, but includes Caithness. "Highlands" is sometimes used, however, as a name for the council area, as in the former "Highlands and Islands Fire and Rescue Service". "Northern" is also used to refer to the area, as in the former "Northern Constabulary". These former bodies both covered the Highland council area and the island council areas of Orkney, Shetland and the Western Isles.

Highland Council signs in the Pass of Drumochter, between Glen Garry and Dalwhinnie, say "Welcome to the Highlands".

Much of the Highlands area overlaps the Highlands and Islands area. An electoral region called "Highlands and Islands" is used in elections to the Scottish Parliament: this area includes Orkney and Shetland, as well as the Highland Council local government area, the Western Isles and most of the Argyll and Bute and Moray local government areas. "Highlands and Islands" has, however, different meanings in different contexts. It means Highland (the local government area), Orkney, Shetland, and the Western Isles in "Highlands and Islands Fire and Rescue Service". "Northern", as in "Northern Constabulary", refers to the same area as that covered by the fire and rescue service.

There have been trackways from the Lowlands to the Highlands since prehistoric times. Many traverse the Mounth, a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven. The most well-known and historically important trackways are the Causey Mounth, Elsick Mounth, Cryne Corse Mounth and Cairnamounth.

Although most of the Highlands is geographically on the British mainland, it is somewhat less accessible than the rest of Britain; thus most UK couriers categorise it separately, alongside Northern Ireland, the Isle of Man, and other offshore islands. They thus charge additional fees for delivery to the Highlands, or exclude the area entirely. Whilst the physical remoteness from the largest population centres inevitably leads to higher transit cost, there is confusion and consternation over the scale of the fees charged and the effectiveness of their communication, and the use of the word Mainland in their justification. Since the charges are often based on postcode areas, many far less remote areas, including some which are traditionally considered part of the lowlands, are also subject to these charges. Royal Mail is the only delivery network bound by a Universal Service Obligation to charge a uniform tariff across the UK. This, however, applies only to mail items and not larger packages which are dealt with by its Parcelforce division.

The Highlands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland is largely composed of ancient rocks from the Cambrian and Precambrian periods which were uplifted during the later Caledonian Orogeny. Smaller formations of Lewisian gneiss in the northwest are up to 3 billion years old. The overlying rocks of the Torridon Sandstone form mountains in the Torridon Hills such as Liathach and Beinn Eighe in Wester Ross.

These foundations are interspersed with many igneous intrusions of a more recent age, the remnants of which have formed mountain massifs such as the Cairngorms and the Cuillin of Skye. A significant exception to the above are the fossil-bearing beds of Old Red Sandstone found principally along the Moray Firth coast and partially down the Highland Boundary Fault. The Jurassic beds found in isolated locations on Skye and Applecross reflect the complex underlying geology. They are the original source of much North Sea oil. The Great Glen is formed along a transform fault which divides the Grampian Mountains to the southeast from the Northwest Highlands.

The entire region was covered by ice sheets during the Pleistocene ice ages, save perhaps for a few nunataks. The complex geomorphology includes incised valleys and lochs carved by the action of mountain streams and ice, and a topography of irregularly distributed mountains whose summits have similar heights above sea-level, but whose bases depend upon the amount of denudation to which the plateau has been subjected in various places.





</doc>
<doc id="28896" url="https://en.wikipedia.org/wiki?curid=28896" title="Scotch whisky">
Scotch whisky

Scotch whisky (; often simply called whisky or Scotch) is malt whisky or grain whisky (or a blend of the two), made in Scotland. Scotch whisky must be made in a manner specified by law. , there were 133 Scotch whisky distilleries operating in Scotland.

All Scotch whisky was originally made from malted barley. Commercial distilleries began introducing whisky made from wheat and rye in the late 18th century. Scotch whisky is divided into five distinct categories: single malt Scotch whisky, single grain Scotch whisky, blended malt Scotch whisky (formerly called "vatted malt" or "pure malt"), blended grain Scotch whisky, and blended Scotch whisky.

All Scotch whisky must be aged in oak barrels for at least three years. Any age statement on a bottle of Scotch whisky, expressed in numerical form, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed-age whisky. A whisky without an age statement is known as a no age statement (NAS) whisky, the only guarantee being that all whisky contained in that bottle is at least three years old. The minimum bottling strength according to the regulation is 40% alcohol by volume.

The first known written mention of Scotch whisky is in the Exchequer Rolls of Scotland, 1494. Scotch whisky was known as "aqua vitae", Latin for "water of life".

Many Scotch whisky drinkers refer to a unit for drinking as a dram.

, the Scotch Whisky Regulations 2009 (SWR) define and regulate the production, labelling, packaging as well as the advertising of Scotch whisky in the United Kingdom. They replace previous regulations that focussed solely on production, including the Scotch Whisky Act 1988.

Since the previous act focussed primarily on production standards, it was repealed and superseded by the 2009 Regulations. The SWR includes broader definitions and requirements for the crafting, bottling, labelling, branding, and selling of "Scotch Whisky". International trade agreements have the effect of making some provisions of the SWR apply in various other countries as well as in the UK. The SWR define "Scotch whisky" as whisky that:

A Scotch whisky label comprises several elements that indicate aspects of production, age, bottling, and ownership. Some of these elements are regulated by the SWR, and some reflect tradition and marketing. The spelling of the term "whisky" is often debated by journalists and consumers. Scottish, English, Welsh, Australian and Canadian whiskies use "whisky", Irish whiskies use "whiskey", while American and other styles vary in their spelling of the term.

The label always features a declaration of the malt or grain whiskies used. A single malt Scotch whisky is one that is entirely produced from malt in one distillery. One may also encounter the term "single cask", signifying the bottling comes entirely from one cask. The term "blended malt" signifies that single malt whisky from different distilleries are blended in the bottle. The Cardhu distillery also began using the term "pure malt" for the same purpose, causing a controversy in the process over clarity in labelling—the Glenfiddich distillery was using the term to describe some single malt bottlings. As a result, the Scotch Whisky Association declared that a mixture of single malt whiskies must be labelled a "blended malt". The use of the former terms "vatted malt" and "pure malt" is prohibited. The term "blended malt" is still debated, as some bottlers maintain that consumers confuse the term with "blended Scotch whisky", which contains some proportion of grain whisky.

The brand name featured on the label is usually the same as the distillery name (for example, the Talisker distillery labels its whiskies with the Talisker name). Indeed, the SWR prohibit bottlers from using a distillery name when the whisky was not made there. A bottler name may also be listed, sometimes independent of the distillery. In addition to requiring that Scotch whisky be distilled in Scotland, the SWR require that it also be bottled and labelled in Scotland. Labels may also indicate the region of the distillery (for example, Islay or Speyside).

Alcoholic strength is expressed on the label with "Alcohol By Volume" ("ABV") or sometimes simply "Vol". Typically, bottled whisky is between 40% and 46% ABV. Whisky is considerably stronger when first emerging from the cask—normally 60–63% ABV. Water is then added to create the desired bottling strength. If the whisky is not diluted before bottling, it can be labelled as cask strength.

A whisky's age may be listed on the bottle providing a guarantee of the youngest whisky used. An age statement on the bottle, in the form of a number, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed age whisky. Scotch whisky without an age statement may, by law, be as young as three years old. In the early 21st century, such "No age statement" whiskies have become more common, as distilleries respond to the depletion of aged stocks caused by improved sales. A label may carry a distillation date or a bottling date. Whisky does not mature once bottled, so if no age statement is provided, one may calculate the age of the whisky if both the distillation date and bottling date are given.

Labels may also carry various declarations of filtration techniques or final maturation processes. A Scotch whisky labelled as "natural" or "non-chill-filtered" has not been through a filtration process during bottling that removes compounds that some consumers see as desirable. Whisky is aged in various types of casks—and often in used sherry or port casks—during distinct portions of the maturation process, and will take on characteristics, flavour and aromas from such casks. Special casks are sometimes used at the end of the maturation process, and such whiskies may be labelled as "wood finished", "sherry/port finished", and so on.

According to the Scotch Whisky Association, the word whisky comes from the Gaelic "uisge beatha" or "usquebaugh", which means "water of life". The earliest record of distillation in Scotland occurred as long ago as 1494 at Lindores Abbey a monastery in Fife, as documented in the "Exchequer Rolls", which were records of royal income and expenditure. The quote above records eight bolls of malt given to Friar John Cor wherewith to make aqua vitae (Latin for "water of life," = "uisge beatha") over the previous year. This would be enough for 1,500 bottles, which suggests that distillation was well-established by the late 15th century.

Aqua vitae (in the form of wine or spirits) was used when making gunpowder to moisten the slurry of saltpetre, charcoal and sulphur. As a drink, Scotch whisky was a favourite of King James IV of Scotland.

Whisky production was first taxed in 1644, causing a rise in illicit whisky distilling in the country. Between the 1760s and the 1830s a substantial unlicensed trade originated from the Highlands, forming a significant part of the region's export economy. In 1782, more than 1,000 illegal stills were seized in the Highlands: these can only have been a fraction of those in operation. The Lowland distillers, who had no opportunity to avoid taxation, complained that un-taxed Highland whisky made up more than half the market. The heavy taxation during the Napoleonic Wars gave the illicit trade a big advantage, but their product was also considered better quality, commanding a higher price in the Lowlands. This was due to the method of taxation: malt was subject to tax (at a rate that climbed substantially between the 1790s and 1822). The licensed distillers therefore used more raw grain in an effort to reduce their tax bill.

The Highland magistrates, themselves members of the landowning classes, had a lenient attitude to unlicensed distillers—all of whom would be tenants in the local area. They understood that the trade supported the rents paid. Imprisoned tenants would not be able to pay any rent.

In 1823, Parliament eased restrictions on licensed distilleries with the "Excise Act", while at the same time making it harder for the illegal stills to operate. Magistrates found counsel for the Crown appearing in their courts, so forcing the maximum penalties to be applied, with some cases removed to the Court of Exchequer in Edinburgh for tougher sentences. Highland landowners were now happy to remove tenants who were distillers in clearances on their estates. These changes ushered in the modern era of Scotch production: in 1823 2,232,000 gallons of whisky had duty paid on it; in 1824 this increased to 4,350,000 gallons.

A farmer, George Smith, working under landlord the Duke of Gordon, was the first person in Scotland to take out a licence for a distillery under the new Act, founding the Glenlivet Distillery in 1824, to make single malt Scotch. Some of the distilleries which started legal operations in the next few years included Bowmore, Strathisla, Balblair, and Glenmorangie; all remain in business today.

Two events helped to increase whisky's popularity: first, the introduction in 1831 of the column still. Aeneas Coffey patented a refined version of a design originally created by Robert Stein, based on early innovations by Sir Anthony Perrier, for the new type of still which produced whisky much more efficiently than the traditional pot stills. The column still allowed for continuous distillation, without the need for cleaning after each batch was made. This process made manufacturing more affordable by performing the equivalent of multiple distillation steps. The new still dramatically increased production; the whisky was less intense and smoother making it more popular.

Second, there was a shortage of wine, brandy and cognac in France, significant by 1880, due to the "phylloxera" bug, a parasitic insect, destroying many of the wine vines; that shortage increased the demand for whisky. By the 1890s, almost forty new distilleries had opened in Scotland. The boom years continued until the industry was significantly affected by World War I and later, by the Great Depression; many of the companies closed and never re-opened.

The Scotch Whisky Association estimated that Scotland's whisky industry supported 40,000 jobs and accounted for £4.37 billion in exports in 2017. Of that total, single malt Scotch accounted for £1.17 billion in exports, a 14% increase over 2016.

The industry's contribution to the economy of the UK was estimated as £5.5 billion in 2018; the industry provided £3.8 billion in direct GVA (gross value added) to Scotland. Whisky tourism has also become significant and accounts for £68.3 million per year. One factor negatively affected sales, an extra 3.9% duty on spirits imposed by the UK in 2017. (The effect of the 25% increase in tariffs imposed by the U.S. in October 2019 would not be apparent until 2020.) Nonetheless, by year-end 2017, exports had reached a record-breaking amount.

In November 2019, the Association announced that the government of the UK had agreed to consider revising the alcohol taxation system, hopefully producing a new plan that was simplified and "fairer".

Exports in 2018 again increased 7.8% by value, and 3.6% in number of bottles, in spite of the duty imposed in 2017; exports grew to a record level, £4.7 billion. The US imported Scotch whisky with a value of just over £1 billion while the European Union was the second largest importer, taking 30% of global value. This was a boom year with a record high in exports, but the Scotch Whisky Association expressed concern for the future, particularly "the challenges posed by Brexit and by tensions in the global trading system".

Scotch whisky tourism has developed around the industry, with distilleries being the third most visited attractions in Scotland; roughly 2 million visits were recorded in 2018. Some 68 distilleries operate visitors' centres in Scotland and another eight accept visits by appointment. Hotels, restaurants and other facilities are also impacted by the tourism phenomenon. The tourism has had an especially visible impact on the economy in some remote rural areas, according to Fiona Hyslop MSP, Cabinet Secretary for Culture, Tourism and External Affairs. "The Scottish Government is committed to working with partners like the Scotch Whisky Association to increase our tourism offer and encourage more people to visit our distilleries," the Secretary said.

A 2016 report stated that only 20% of the whisky was made by companies owned in Scotland. Distilleries owned by Diageo, a London-based company, produce 40% of all Scotch whisky, with over 24 brands, such as Johnnie Walker, Oban, and Talisker. Another 20% of the product is made by distillers owned by Pernod Ricard of France, including brands such as Glenlivet, Chivas Regal, and Ballantine's. There are also smaller distillers that are owned by foreign companies, such as BenRiach whose parent is the Brown–Forman Corporation based in Kentucky. Nonetheless, Scotch whisky is produced according to the current regulations, as to ageing, production and so on, ensuring that it remains Scottish.

Independents owned by Scots companies make a substantial amount of Scotch whisky, with the largest, William Grant & Sons, producing 8%, or about 7.6 million cases per year. Its brands include Glenfiddich and Balvenie. Glenfiddich is the best-selling single malt Scotch in the world. Roughly 14 million bottles of Glenfiddich are sold annually.

Most malt distilleries sell a significant amount of whisky by the cask for blending, and sometimes to private buyers as well. Whisky from such casks is sometimes bottled as a single malt by "independent bottling" firms such as Duncan Taylor, Master of Malt, Gordon & MacPhail, Cadenhead's, The Scotch Malt Whisky Society, Murray McDavid, Berry Bros. & Rudd, Douglas Laing, and others. These are usually labelled with the distillery's name, but not using the distillery's trademarked logos or typefaces. An "official bottling" (or "proprietary bottling"), by comparison, is from the distillery (or its owner). Many independent bottlings are from single casks, and they may sometimes be very different from an official bottling.

For a variety of reasons, some independent brands do not identify which facility distilled the whisky in the bottle. They may instead identify only the general geographical area of the source, or they simply market the product using their own brand name without identifying their source. This may, in some cases, be simply to give the independent bottling company the flexibility to purchase from multiple distillers without changing their labels.

There are two basic types of Scotch whisky, from which all blends are made:

Excluded from the definition of "single grain Scotch whisky" is any spirit that qualifies as a single malt Scotch whisky or as a blended Scotch whisky. The latter exclusion is to ensure that a blended Scotch whisky produced from single malt(s) and single grain(s) distilled at the same distillery does not also qualify as single grain Scotch whisky.

Nearly 90% of the bottles of Scotch sold per year are blended whiskies. Three types of blends are defined for Scotch whisky:

The five Scotch whisky definitions are structured in such a way that the categories are mutually exclusive. The 2009 regulations changed the formal definition of blended Scotch whisky to achieve this result, but in a way that reflected traditional and current practice: before the 2009 SWR, any combination of Scotch whiskies qualified as a blended Scotch whisky, including for example a blend of single malt Scotch whiskies.

As was the case under the Scotch Whisky Act 1988, regulation 5 of the SWR 2009 stipulates that the only whisky that may be manufactured in Scotland is Scotch whisky. The definition of "manufacture" is "keeping for the purpose of maturation; and keeping, or using, for the purpose of blending, except for domestic blending for domestic consumption". This provision prevents the existence of two "grades" of whisky originating from Scotland, one "Scotch whisky" and the other, a "whisky – product of Scotland" that complies with the generic EU standard for whisky. According to the Scotch Whisky Association, allowing non-Scotch whisky production in Scotland would make it difficult to protect Scotch whisky as a distinctive product.

The SWR regulation also states that no additives may be used except for plain (E150A) caramel colouring.

To qualify for this category the Scotch whisky must be made in one distillery, in a pot still by batch distillation, using only water and malted barley. As with any other Scotch whisky, the Scotch Whisky Regulations of 2009 also require single malt Scotch to be made completely and bottled in Scotland and aged for at least three years. Most are aged longer.

Another term is sometimes seen, called "double wood" or "triple wood", sometimes incorrectly referred to as "double malt" or "triple malt". These indicate that the whisky was aged in two or three types of casks. Hence, if the whisky otherwise meets the criteria of a single malt, it still falls into the single malt category even if more than one type of cask was used for ageing. Examples include The Balvenie 12 Years Old DoubleWood and Laphroaig Triple Wood.

Another nuance is that Lowland Scotch malts used a triple distillation just like the Irish do, breaking away from the general rule that all Scotch is double distilled.

Single grain whisky is made with water and a malted barley but the distillery then adds other grains or cereals, wheat, corn or rye, for example. From that moment on, it can no longer be called single malt. This type of product must be from a single distillery and is often used in making blended Scotch. Single grain whiskies are usually not distilled in pot stills but with column stills.

Blended malt whisky—formerly called "vatted malt" or "pure malt" (terms that are now prohibited in the SWR 2009)—is one of the least common types of Scotch: a blend of single malts from more than one distillery (possibly with differing ages).

Blended malts contain only single malt whiskies from two or more distilleries. This type must contain no grain whiskies and is distinguished by the absence of the word "single" on the bottle. The age of the vat is that of the youngest of the original ingredients. For example, a blended malt marked "8 years old" may include older whiskies, with the youngest constituent being eight years old. Johnnie Walker Green Label and Monkey Shoulder are examples of blended malt whisky. Starting from November 2011, no Scotch whisky could be labelled as a vatted malt or pure malt, the SWR requiring them to be labelled blended malt instead.

Blended Scotch whisky constitutes about 90% of the whisky produced in Scotland. Blended Scotch whiskies contain both malt whisky and grain whisky. Producers combine the various malts and grain whiskies to produce a consistent brand style. Notable blended Scotch whisky brands include Ballantine's, Bell's, Chivas Regal, Cutty Sark, Dewar's, J&B, Johnnie Walker, Teacher's Highland Cream, The Famous Grouse, and Whyte and Mackay.

The term blended grain Scotch refers to whisky that contains at least two single grain Scotch whiskies from at least two distilleries, combined to create one batch of the product.

Dozens of compounds contribute to Scotch whisky flavour and aroma characteristics, including volatile alcohol congeners (also called "higher oils") formed during fermentation, such as acetaldehyde, methanol, ethyl acetate, n-propanol, and isobutanol. Other flavour and aroma compounds include vanillic acid, syringic acid, vanillin, syringaldehyde, furfural, phenyl ethanol, and acetic acid. One analysis established 13 distinct flavour characteristics dependent on individual compounds, including sour, sweet, grainy, and floral as major flavour perceptions.

Some distilleries use a peat fire to dry the barley for some of their products before grinding it and making the mash. Peat smoke contributes phenolic compounds, such as guaiacol, that give aromas similar to smoke. The Maillard browning process of the residual sugars in the mashing process, particularly through formation of 2-furanmethanol and pyrazines imparting nutty or cereal characteristics, contributes to the baked bread notes in the flavour and aroma profile. Maturation during multi-year casking from reusing sherry oak barrels originally used for bourbon whiskey production contributes a vanilla aroma to some premium Scotch whiskies.

Refilling and fabrication or tampering of branded Scotch whiskies are types of Scotch whisky adulteration that diminishes brand integrity, consumer confidence, and profitability in the Scotch industry. Deviation from normal concentrations of major constituents, such as alcohol congeners, provides a precise, quantitative method for determining authenticity of Scotch whiskies. Over 100 compounds can be detected during counterfeit analysis, including phenolics and terpenes which may vary in concentration by different geographic origins, the barley used in the fermentation mash, or the oak cask used during ageing. Typical high-throughput instruments used in counterfeit detection are liquid chromatography and mass spectrometry.

Scotland was traditionally divided into four regions: The Highlands, The Lowlands, The Isle of Islay, and Campbeltown. Due to the large number of distilleries found there, the Speyside area became the fifth, recognised by the Scotch Whisky Association (SWA) as a distinct region in 2014. The whisky-producing islands other than Islay are not recognised as a distinct region by the SWA, which groups them into the Highlands region.



Although only five regions are specified, any Scottish locale may be used to describe a whisky if it is distilled entirely within that place; for example a single malt whisky distilled on Orkney could be described as "Orkney Single Malt Scotch Whisky" instead of as an Island whisky.






</doc>
<doc id="28897" url="https://en.wikipedia.org/wiki?curid=28897" title="Special drawing rights">
Special drawing rights

Special drawing rights (SDRs) are supplementary foreign exchange reserve assets defined and maintained by the International Monetary Fund (IMF). SDRs are units of account for the IMF, and not a currency "per se". They represent a claim to currency held by IMF member countries for which they may be exchanged. SDRs were created in 1969 to supplement a shortfall of preferred foreign exchange reserve assets, namely gold and U.S. dollars. The ISO 4217 currency code for special drawing rights is XDR and the numeric code is "960".

SDRs are allocated by the IMF to countries, and cannot be held or used by private parties. The number of SDRs in existence was around XDR 21.4 billion in August 2009. During the global financial crisis of 2009, an additional XDR 182.6 billion was allocated to "provide liquidity to the global economic system and supplement member countries’ official reserves". By October 2014, the number of SDRs in existence was XDR 204 billion.

The value of a SDR is based on a basket of key international currencies reviewed by IMF every five years. The weights assigned to each currency in the XDR basket are adjusted to take into account their current prominence in terms of international trade and national foreign exchange reserves. In the review conducted in November 2015, the IMF decided to add the Renminbi (Chinese yuan) to the basket, effective 1 October 2016. Since that date, the XDR basket has consisted of the following five currencies: U.S. dollar 41.73%, euro 30.93%, renminbi (Chinese yuan) 10.92%, Japanese yen 8.33%, British pound 8.09%.

While the ISO 4217 currency code for special drawing rights is XDR, they are often referred to by their acronym SDR. Both refer to the name "special drawing rights".

Intentionally innocuous and free of connotations because of disagreements over the nature of this new reserve asset during its creation, the name derives from a debate about its primary function—money or credit. While the name would offend neither side, it can be argued that prior to 1981 the XDR was a debt security and so a form of credit. Member countries receiving XDR allocations were required by the reconstitution provision of the XDR articles to hold a prescribed number of XDRs. If a state used any of its allotment, it was expected to rebuild its XDR holdings. As the reconstitution provisions were abrogated in 1981, the XDR now functions less like credit than previously. Countries are still expected to maintain their XDR holdings at a certain level, but penalties for holding fewer than the allocated amount are now less onerous.

The name may actually derive from an early proposal for IMF "reserve drawing rights". The word "reserve" was later replaced with "special" because the idea that the IMF was creating a foreign exchange reserve asset was contentious.

Special drawing rights were created by the IMF in 1969 and were intended to be an asset held in foreign exchange reserves under the Bretton Woods system of fixed exchange rates. 1 XDR was initially defined as US$1, equal to 0.888671 g of gold. After the collapse of that system in the early 1970s the SDR has taken on a less important role. Acting as the unit of account for the IMF has been its primary purpose since 1972.

The IMF itself calls the current role of the XDR "insignificant". Developed countries, who hold the greatest number of XDRs, are unlikely to use them for any purpose. The only actual users of XDRs may be those developing countries that see them as "a rather cheap line of credit".

One reason XDRs may not see much use as foreign exchange reserve assets is that they must be exchanged into a currency before use. This is due in part to the fact private parties do not hold XDRs: they are only used and held by IMF member countries, the IMF itself, and a select few organizations licensed to do so by the IMF. Basic functions of foreign exchange reserves, such as market intervention and liquidity provision, as well as some less prosaic ones, such as maintaining export competitiveness via favorable exchange rates, cannot be accomplished directly using XDRs. This fact has led the IMF to label the XDR as an "imperfect reserve asset".

Another reason they may see little use is that the number of XDRs in existence is relatively few. As of January 2011, XDRs represented less than 4% of global foreign exchange reserve assets. To function well a foreign exchange reserve asset must have sufficient liquidity, but XDRs, because of their small number, may be perceived to be an illiquid asset. The IMF says, "expanding the volume of official XDRs is a prerequisite for them to play a more meaningful role as a substitute reserve asset."

The XDR comes to prominence when the U.S. dollar is weak or otherwise unsuitable to be a foreign exchange reserve asset. This usually manifests itself as an allocation of XDRs to IMF member countries. Distrust of the U.S. dollar is not the only stated reason allocations have been made, however.
One of its first roles was to alleviate an expected shortfall of U.S. dollars c. 1970. At this time, the United States had a conservative monetary policy and did not want to increase the total amount of U.S. dollars in existence. If the United States had continued down this path, the dollar would have become a less attractive foreign exchange reserve asset: it would not have had the necessary liquidity to serve this function. Soon after XDR allocations began, the United States reversed its former policy and provided sufficient liquidity. In the process a potential role for the XDR was removed. During this first round of allocations, 9.3 billion XDRs were distributed to IMF member countries.

The XDR resurfaced in 1978 when many countries were wary of taking on more foreign exchange reserve assets denominated in U.S. dollars. This suspicion of the dollar precipitated an allocation of 12 billion XDRs over a period of four years.

Concomitant with the financial crisis of 2007–08, the third round of XDR allocations occurred in the years 2009 and 2011. The IMF recognized the financial crisis as the cause for distributing the large majority of these third-round allotments, but some allocations were couched as distributing XDRs to countries that had never received any and others as a re-balancing of IMF quotas, which determine how many XDRs a country is allotted, to better represent the economic strength of emerging markets.

During this time China, a country with large holdings of U.S. dollar foreign exchange reserves, voiced its displeasure at the current international monetary system, and promoted measures that would allow the XDR to "fully satisfy the member countries' demand for a reserve currency." These comments, made by a chairman of the People's Bank of China, Zhou Xiaochuan, drew media attention, and the IMF showed some support for China's stance. It produced a paper exploring ways the substance and function of the XDR could be increased. China has also suggested the creation of a substitution account to allow exchange of U.S. dollars into XDRs. When substitution was proposed before, in 1978, the United States appeared reluctant to allow such a mechanism to become operational.

In 2001, the UN suggested allocating XDRs to developing countries for use by them as cost-free alternatives to building foreign exchange reserves through borrowing or running current account surpluses. In 2009, an XDR allocation was made to countries that had joined the IMF after the 1979–1981 round of allocations was complete (and so had never been allocated any). First proposed in 1997, many of the beneficiaries of this 2009 allocation were developing countries.

The IMF takes into account the value of several currencies important to the world’s trading and financial systems. Firstly, it is widely used in international transactions, including export quotas in the IMF members and the number of official reserve assets which were in their own currencies. Secondly, it is widely traded on the main foreign exchange market, including foreign exchange trading volume, whether there are forward exchange markets and so on. Also it requires no less than 70% of the votes among the IMF members. Initially its value was fixed at 1 XDR = 1 U.S. dollar (as equivalent to 0.888671 grams of fine gold), but this was abandoned in favor of a currency basket after the 1973 collapse of the Bretton Woods system of fixed exchange rates.

From July 1974 to December 1980, the XDR basket consisted of 16 currencies. From January 1981 until the birth of the euro, the basket consisted of only five currencies: the U.S. dollar, the Deutsche mark, the French franc, the British pound, and the Japanese yen. When the euro was introduced in January 1999, it replaced the German mark and French franc; the basket consisted of the U.S. dollar, the euro, the British pound and the Japanese yen. Since 1 October 2016, the XDR basket has included the Chinese renminbi.

This basket is re-evaluated every five years, and the currencies included as well as the weights given to them can then change. A currency's importance is currently measured by the degree to which it is used as a foreign exchange reserve asset and the amount of exports sold in that currency. 

Because of fluctuating exchange rates, the relative value of each currency varies continuously, as does the value of the XDR. The IMF sets the value of the XDR in terms of U.S. dollars every day. The latest U.S. dollar valuation of the XDR is published on the IMF website.

SDRs are allocated to member countries by the IMF. A country's IMF quota, the maximum amount of financial resources that it is obligated to contribute to the fund, determines its allotment of XDRs. Any new allocations must be voted on in the XDR Department of the IMF and pass with an 85% majority. All IMF member countries are represented in the XDR Department, but this is not a one country, one vote system; voting power is determined by a member country's IMF quota. For example, the United States has 16.7% of the vote as of March 2, 2011.

Allocations are not made on a regular basis and have only occurred on rare occasions. The first round took place because of a situation that was soon reversed, the possibility of an insufficient amount of U.S. dollars because of U.S. reluctance to run the deficit necessary to supply future demand. Extraordinary circumstances have, likewise, led to the other XDR allocation events. For example, during the global financial crisis of 2009, XDR 182.6 billion was allocated to "provide liquidity to the global economic system and supplement member countries’ official reserves". The 2011 allocations were to low-income member countries.

An IMF member country that requires actual foreign currency may sell its SDRs to another member country in exchange for the currency. To sell a part or all its SDRs, the country must find a willing party to buy them. The IMF acts as an intermediary in this voluntary exchange. 

The IMF also has the authority under the designation mechanism to ask member countries with strong foreign exchange reserves to purchase XDRs from those with weak reserves. The maximum obligation any country has under this mechanism is currently equal to twice the amount of its SDR allocation. As of 2015, XDRs may only be exchanged for euros, Japanese yen, UK pounds, or US dollars. The IMF says exchanging XDRs can take "several days."

It is not, however, the IMF that pays out foreign currency in exchange for XDRs: the claim to currency that XDRs represent is not a claim on the IMF.

The IMF calculates a weekly interest rate, which is based on "a weighted average of representative interest rates on short-term debt in the money markets of the XDR basket currencies". No interest is payable on the SDRs allocated to a country by the IMF. However, interest is payable by an IMF member country that has exchanged (sold) some or all of the SDRs it was allocated, and interest is paid to a member country that holds more SDRs than it was allocated (ie., the country that bought SDRs from another member).

Some international organizations use the XDR as a unit of account. The IMF says using the XDR in this way "help[s] cope with exchange rate volatility." As of 2001, organizations that use the XDR as a unit of account, besides the IMF itself, include: Universal Postal Union, African Development Bank, Arab Monetary Fund, Asian Development Bank, Bank for International Settlements, Common Fund for Commodities, East African Development Bank, Economic Community of West African States, International Center for Settlement of Investment Disputes, International Fund for Agricultural Development, and Islamic Development Bank. It is not only international organizations that use the XDR in this way. JETRO uses XDRs to price foreign aid. In addition, charges, liabilities, and fees prescribed by some international treaties are denominated in XDRs. In 2003, the Bank for International Settlements ceased to use the gold franc as their currency, in favour of XDR.

Some bonds are also denominated in SDR, like the IBRD 2016 SDR denominated bonds.

In some international treaties and agreements, XDRs are used to value penalties, charges or prices. For example, the Convention on Limitation of Liability for Maritime Claims caps personal liability for damages to ships at XDR 330,000. The Montreal Convention and other treaties also use XDRs in this way.

According to the IMF, "the SDR may not be any country’s optimal basket", but a few countries do peg their currencies to the XDR. One possible benefit to nations with XDR pegs is that they may be perceived to be more transparent. As of 2000, the number of countries that did so was four. This is a substantial decrease from 1983, when 14 countries had XDR pegs. As of 2010, Syria pegs its pound to the XDR.





</doc>
