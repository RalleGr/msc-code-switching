<doc id="27956" url="https://en.wikipedia.org/wiki?curid=27956" title="South Carolina">
South Carolina

South Carolina () is a state in the Southeastern United States and the easternmost of the Deep South. It is bordered to the north by North Carolina, to the southeast by the Atlantic Ocean, and to the southwest by Georgia across the Savannah River. 

South Carolina became the eighth state to ratify the U.S. Constitution on May 23, 1788. It also became the first state to vote in favor of secession from the Union on December 20, 1860. After the American Civil War, it was readmitted into the United States on June 25, 1868.

South Carolina is the 40th most extensive and 23rd most populous U.S. state. In 2019 its GDP was $249.9 billion. South Carolina is composed of 46 counties. The capital is Columbia with a population of 133,451 in 2018; while its largest city is Charleston with a 2018 population of 136,208. The Greenville–Anderson–Mauldin metropolitan area is the largest in the state, with a 2018 population estimate of 906,626.

South Carolina is named in honor of King Charles I of England, who first formed the English colony, with "Carolus" being Latin for "Charles".

The state can be divided into three natural geographic areas which then can then be subdivided into five distinct cultural regions. The natural environment is divided from east to west by the Atlantic coastal plain, the Piedmont, and the Blue Ridge Mountains. Culturally, the coastal plain is split into the Lowcountry and the Pee Dee region. While, the upper Piedmont region is referred to as the Piedmont and the lower Piedmont region is referred to as the Midlands. The area surrounding the Blue Ridge Mountains is known as the Upstate. The Atlantic Coastal Plain makes up two-thirds of the state. Its eastern border is the Sea Islands, a chain of tidal and barrier islands. The border between the lowcountry and the upcountry is defined by the Atlantic Seaboard fall line, which marks the limit of navigable rivers.

The Atlantic Coastal Plain consists of sediments and sedimentary rocks that range in age from Cretaceous to Present. The terrain is relatively flat and the soil is composed predominantly of sand, silt, and clay. Areas with better drainage make excellent farmland, though some land is swampy. An unusual feature of the coastal plain is a large number of low-relief topographic depressions named Carolina bays. The bays tend to be oval, lining up in a northwest to southeast orientation. The eastern portion of the coastal plain contains many salt marshes and estuaries, as well as natural ports such as Georgetown and Charleston. The natural areas of the coastal plain are part of the Middle Atlantic coastal forests ecoregion.

The Sandhills or Carolina Sandhills is a wide region within the Atlantic Coastal Plain province, along the inland margin of this province. The Carolina Sandhills are interpreted as eolian (wind-blown) sand sheets and dunes that were mobilized episodically from approximately 75,000 to 6,000 years ago. Most of the published luminescence ages from the sand are coincident with the last glaciation, a time when the southeastern United States was characterized by colder air temperatures and stronger winds.

Much of Piedmont consists of Paleozoic metamorphic and igneous rocks, and the landscape has relatively low relief. Due to the changing economics of farming, much of the land is now reforested in loblolly pine for the lumber industry. These forests are part of the Southeastern mixed forests ecoregion. At the southeastern edge of Piedmont is the fall line, where rivers drop to the coastal plain. The fall line was an important early source of water power. Mills built to harness this resource encouraged the growth of several cities, including the capital, Columbia. The larger rivers are navigable up to the fall line, providing a trade route for mill towns.

The northwestern part of Piedmont is also known as the Foothills. The Cherokee Parkway is a scenic driving route through this area. This is where Table Rock State Park is located.

The Blue Ridge consists primarily of Precambrian metamorphic rocks, and the landscape has relatively high relief. The Blue Ridge Region contains an escarpment of the Blue Ridge Mountains that continues into North Carolina and Georgia as part of the southern Appalachian Mountains. Sassafras Mountain, South Carolina's highest point at , is in this area. Also in this area is Caesars Head State Park. The environment here is that of the Appalachian-Blue Ridge forests ecoregion. The Chattooga River, on the border between South Carolina and Georgia, is a favorite whitewater rafting destination.

South Carolina has several major lakes covering over . All major lakes in South Carolina are man-made. The following are the lakes listed by size.

The Charleston area, along the central coastline of the state, demonstrates the greatest frequency of earthquakes in South Carolina. South Carolina averages 10–15 earthquakes a year below magnitude3 (FEMA). The Charleston earthquake of 1886 was the largest quake ever to hit the eastern United States. The 7.0–7.3 magnitude earthquake killed 60 people and destroyed much of the city. Faults in this region are difficult to study at the surface due to thick sedimentation on top of them. Many of the ancient faults are within plates rather than along plate boundaries.

South Carolina has a humid subtropical climate (Köppen climate classification "Cfa"), although high-elevation areas in the Upstate area have fewer subtropical characteristics than areas on the Atlantic coastline. In the summer, South Carolina is hot and humid, with daytime temperatures averaging between in most of the state and overnight lows averaging on the coast and from inland. Winter temperatures are much less uniform in South Carolina.Coastal areas of the state have very mild winters, with high temperatures approaching an average of and overnight lows around 40 °F (5–8 °C). Inland, the average January overnight low is around in Columbia and temperatures well below freezing in the Upstate. While precipitation is abundant the entire year in almost the entire state, the coast tends to have a slightly wetter summer, while inland, the spring and autumn transitions tend to be the wettest periods and winter the driest season, with November being the driest month. The highest recorded temperature is in Johnston and Columbia on June 29, 2012, and the lowest recorded temperature is at Caesars Head on January 21, 1985.

Snowfall in South Carolina is somewhat uncommon in most of the state, while coastal areas receive less than an inch (2.5 cm) annually on average. It is not uncommon for the state (especially the southern coast) to receive no recordable snowfall in a given year. The interior receives a little more snow, although nowhere in the state averages more than of snow annually. The mountains of extreme northwestern South Carolina tend to have the most substantial snow accumulation. Freezing rain and ice tend to be more common than snow in many areas of the state.

South Carolina is also prone to tropical cyclones and tornadoes. Two of the strongest hurricanes to strike South Carolina in recent history were Hurricane Hazel (1954) and Hurricane Hugo (1989).

The state is occasionally affected by tropical cyclones. This is an annual concern during hurricane season, which lasts from June1 to November 30. The peak time of vulnerability for the southeast Atlantic coast is from early August to early October, during the Cape Verde hurricane season. Memorable hurricanes to hit South Carolina include Hazel (1954), Florence (2018), and Hugo (1989), all Category 4 hurricanes.

South Carolina averages around 50 days of thunderstorm activity a year. This is less than some of the states further south, and it is slightly less vulnerable to tornadoes than the states which border on the Gulf of Mexico. Some notable tornadoes have struck South Carolina, and the state averages around 14 tornadoes annually. Hail is common with many of the thunderstorms in the state, as there is often a marked contrast in temperature of warmer ground conditions compared to the cold air aloft.


The United States Census Bureau estimates the population of South Carolina was 5,148,714 on July 1, 2019, an 11.31 percentage increase since the 2010 census.

As of the 2017 census estimate, the racial make up of the state is 68.5% White (63.8% non-Hispanic white), 27.3% Black or African American, 0.5% American Indian and Alaska Native, 1.7% Asian, 0.1% Native Hawaiian and other Pacific Islander, 1.9% from two or more races. 5.7% of the total population was of Hispanic or Latino origin of any race.

According to the United States Census Bureau, as of 2019, South Carolina had an estimated population of 5,148,714, which is an increase of 64,587 from the prior year and an increase of 523,350, or 11.31%, since the year 2010. Immigration from outside the United States resulted in a net increase of 36,401 people, and migration within the country produced a net increase of 115,084 people. According to the University of South Carolina's Arnold School of Public Health, Consortium for Latino Immigration Studies, South Carolina's foreign-born population grew faster than any other state between 2000 and 2005. South Carolina has banned sanctuary cities.

Historical South Carolina Racial Breakdown of Population

Some Primary Statistical Areas of South Carolina overlap with neighbouring states of North Carolina and Georgia
The following table shows the major metropolitan areas of South Carolina.
In 2019, the U.S. Census Bureau released 2018 population estimates for South Carolina's most populous cities.

South Carolina's state government consists of the Executive, Legislative, and Judicial branches. Also relevant are the state constitution, law enforcement agencies, federal representation, state finances, and state taxes.

South Carolina has historically had a weak executive branch and a strong legislature. Before 1865, governors in South Carolina were appointed by the General Assembly, and held the title "President of State". The 1865 Constitution changed this process, requiring a popular election. Local governments were also weak. But, the 1867 Constitution, passed during the Reconstruction era, extended democratization by establishing home rule for counties, which were established from the formerly designated districts of the state.

The 1895 state constitution overturned this, reducing the role of counties and strengthening the relative role of the state legislature; essentially the counties were agents of the state and ruled by the General Assembly through the legislative delegation for each county. They are geographically comprehensive; all areas of the state are included in counties. As each county had one state senator, that position was particularly powerful. This status continued until 1973, when the state constitution was amended to provide for home rule for the counties. During this time the state had changed, with increasing urbanization, but rural counties retained proportionally more power as the legislature was based in representatives elected from counties rather than population districts.

The federal court case, "Reynolds v. Sims" (1964), "established the one-man, one-vote
concept for electoral representation at the state level. Legislators were now supposed to represent
more or less equal numbers of people." Residents of urban areas had been found to be markedly underrepresented in the legislature under the county-based system. Reapportionment made obvious the need for other changes to county structure, leading to the legislature passing the constitutional amendment. The Home Rule Act of 1975 implemented the amendment giving more power to the counties. With urbanization, their governments have become increasingly important in the state.

Several changes to the state constitution have affected the office of the governor and the cabinet. In 1926 the governor's term was extended from two to four years; in 1982 the governor was allowed to run for a second succeeding term. In 1993, the state passed an amendment requiring a limited cabinet (all of whom must be popularly elected).

As of January 2, 2016, there were 2,948,772 registered voters.

There is evidence of human activity in the area about 40,000 years ago. At the time Europeans arrived, marking the end of the Pre-Columbian era around 1600, there were many separate Native American tribes, the largest being the Cherokee, and the Catawba, and the total population being up to 20,000.

Up the rivers of the eastern coastal plain lived about a dozen tribes of Siouan background. Along the Savannah River were the Apalachee, Yuchi, and the Yamasee. Further west were the Cherokee, and along the Catawba River, the Catawba. These tribes were village-dwellers, relying on agriculture as their primary food source. The Cherokee lived in wattle and daub houses made with wood and clay, roofed with wood or thatched grass.
About a dozen separate small tribes summered on the coast harvesting oysters and fish, and cultivating corn, peas and beans. Travelling inland as much as mostly by canoe, they wintered on the coastal plain, hunting deer and gathering nuts and fruit. The names of these tribes survive in place names like Edisto Island, Kiawah Island, and the Ashepoo River.

The Spanish were the first Europeans in the area. From June 24 to July 14, 1521, they explored the land around Winyah Bay. On October 8, 1526, they founded San Miguel de Gualdape, near present-day Georgetown, South Carolina. It was the first European settlement in what is now mainland USA. Established with five hundred settlers, it was abandoned eight months later by one hundred and fifty survivors. In 1540, Hernando de Soto explored the region and the main town of Cofitachequi, where he captured the queen of the Maskoki (Muscogee) and the Chelaque (Cherokee) who had welcomed him.
In 1562 French Huguenots established a settlement at what is now the Charlesfort-Santa Elena archaeological site on Parris Island. Many of these settlers preferred a natural life far from civilization and the atrocities of the Wars of Religion. The garrison lacked supplies, however, and the soldiers (as in the France Antarctique) soon ran away. The French returned two years later but settled in present-day Florida rather than South Carolina.

Sixty years later, in 1629, King of England Charles I established the Province of Carolina, an area covering what is now South and North Carolina, Georgia and Tennessee. In 1663, Charles II granted the land to eight Lords Proprietors in return for their financial and political assistance in restoring him to the throne in 1660. Anthony Ashley Cooper, one of the Lord Proprietors, planned the Grand Model for the Province of Carolina and wrote the Fundamental Constitutions of Carolina, which laid the basis for the future colony. His utopia was inspired by John Locke, an English philosopher and physician, widely regarded as one of the most influential of Enlightenment thinkers and commonly known as the "Father of Liberalism".

In the 1670s, English planters from Barbados established themselves near what is now Charleston. Settlers from all over Europe built rice plantations in the South Carolina Lowcountry, east of the Atlantic Seaboard fall line. Plantation labor was done by African slaves who formed the majority of the population by 1720. Another cash crop was the indigo plant, a plant source of blue dye, developed by Eliza Lucas.

Meanwhile, Upstate South Carolina, west of the Fall Line, was settled by small farmers and traders, who displaced Native American tribes westward. Colonists overthrew the proprietors' rule, seeking more direct representation. In 1719, the colony was officially made a crown colony. In 1729, North Carolina was split off into a separate colony.

South Carolina prospered from the fertility of the lowcountry and the harbors, such as at Charleston. It allowed religious toleration, encouraging Settlements spread, and trade in deerskin, lumber, and beef thrived. Rice cultivation was developed on a large scale.

By the second half of the 1700s, South Carolina was one of the richest of the Thirteen Colonies.

On March 26, 1776, the colony adopted the Constitution of South Carolina, electing John Rutledge as the state's first president. In February, 1778, South Carolina became the first state to ratify the Articles of Confederation, the initial governing document of the United States, and in May 1788, South Carolina ratified the United States Constitution, becoming the eighth state to enter the union.

During the American Revolutionary War (1775–1783), about a third of combat action took place in South Carolina, more than in any other state. Inhabitants of the state endured being invaded by British forces and an ongoing civil war between loyalists and partisans that devastated the backcountry. It is estimated 25,000 slaves (30% of those in South Carolina) fled, migrated or died during the war.

America's first census in 1790 put the state's population at nearly 250,000. By the 1800 census the population had increased 38 per cent to nearly 340,000 of which 146,000 were slaves. At that time South Carolina had the largest population of Jews in the 16 United States, mostly based in Savannah and Charleston, the latter being the country's fifth largest city.

In the Antebellum period (before the Civil War) the state's economy and population grew. Cotton became an important crop after the invention of the cotton gin. While nominally democratic, from 1790 until 1865, wealthy landowners were in control of South Carolina. For example, a man was not eligible to sit in the State House of Representatives unless he possessed an estate of 500 acres of land and 10 Negroes, or at least 150 pounds sterling, diminishing the electorate. Further, the state maintained indirect election of electors by the state legislature until 1868, the last state to do so. Voters thus did not participate in presidential elections, other than through state-wide elections.

Columbia, the new state capital was founded in the center of the state, and the State Legislature first met there in 1790. The town grew after it was connected to Charleston by the Santee Canal in 1800, one of the first canals in the United States.

As dissatisfaction with the federal government grew, in the 1820s John C. Calhoun became a leading proponent of states' rights, limited government, nullification of the U.S. Constitution, and free trade. In 1832, the Ordinance of Nullification declared federal tariff laws unconstitutional and not to be enforced in the state, leading to the Nullification Crisis. The federal Force Bill was enacted to use whatever military force necessary to enforce federal law in the state, bringing South Carolina back into line.

In the United States presidential election of 1860 voting was sharply divided, with the south voting for the Southern Democrats and the north for Abraham Lincoln's Republican Party. Lincoln was anti-slavery, did not acknowledge the right to secession, and would not yield federal property in Southern states. Southern secessionists believed Lincoln's election meant long-term doom for their slavery-based agrarian economy and social system.

Lincoln was elected president on November 6, 1860. The state House of Representatives three days later passed the "Resolution to Call the Election of Abraham Lincoln as U.S. President a Hostile Act", and within weeks South Carolina became the first state to secede.

On April 12, 1861, Confederate (southern) batteries began shelling the Union (northern) Fort Sumter in Charleston Harbor, and the American Civil War began. In November of that year the Union attacked Port Royal Sound and soon occupied Beaufort County and the neighboring Sea Islands. For the rest of the war this area served as a Union base and staging point for other operations. Whites abandoned their plantations, leaving behind about ten thousand slaves. Several Northern charities partnered with the federal government to help these people run the cotton farms themselves under the Port Royal Experiment. Workers were paid by the pound harvested and thus became the first former slaves freed by the Union forces to earn wages.

Although the state was not a major battleground, the war ruined the economy. Under conscription, all men aged 18–35 (later 45) were drafted for Confederate service. More than 60,000 served, and the state lost nearly one-third of the white male population of fighting age.

At the end of the war in early 1865, the troops of General William Tecumseh Sherman marched across the state devastating plantations and most of Columbia.

After the war, South Carolina was restored to the United States during Reconstruction. Under presidential Reconstruction (1865–66), freedmen (former slaves) were given limited rights. Under Radical reconstruction (1867–1877), a Republican coalition of freedmen, carpetbaggers and scalawags was in control, supported by Union Army forces. They established public education, welfare institutions, and home rule for counties, expanding democracy.

In "Texas vs. White" (1869), the Supreme Court ruled the ordinances of secession (including that of South Carolina) were invalid, and thus those states had never left the Union. However, South Carolina did not regain representation in Congress until that date.

Until the 1868 presidential election, South Carolina's legislature, not the voters, chose the state's electors for the presidential election. South Carolina was the last state to choose its electors in this manner. On October 19, 1871 President Ulysses S. Grant suspended habeas corpus in nine South Carolina counties under the authority of the Ku Klux Klan Act. Led by Grant's Attorney General Amos T. Akerman, hundreds of Klansmen were arrested while 2000 Klansmen fled the state. This was done to suppress Klan violence against African-American and white voters in the South. In the mid to late 1870s, white Democrats used paramilitary groups such as the Red Shirts to intimidate and terrorize black voters. They regained political control of the state under conservative white "Redeemers" and pro-business Bourbon Democrats. In 1877, the federal government withdrew its troops as part of the Compromise of 1877 that ended Reconstruction.

The state became a hotbed of racial and economic tensions during the Populist and Agrarian movements of the 1890s. A Republican-Populist biracial coalition took power away from White Democrats temporarily. To prevent that from happening again, Democrats gained passage of a new constitution in 1895 which effectively disenfranchised almost all blacks and many poor whites by new requirements for poll taxes, residency, and literacy tests that dramatically reduced the voter rolls. By 1896, only 5,500 black voters remained on the voter registration rolls, although they constituted a majority of the state's population. The 1900 census demonstrated the extent of disenfranchisement: the 782,509 African American citizens comprised more than 58% of the state's population, but they were essentially without any political representation in the Jim Crow society.

The 1895 constitution overturned local representative government, reducing the role of the counties to agents of state government, effectively ruled by the General Assembly, through the legislative delegations for each county. As each county had one state senator, that person had considerable power. The counties lacked representative government until home rule was passed in 1975.

Governor "Pitchfork Ben Tillman", a Populist, led the effort to disenfranchise the blacks and poor whites, although he controlled Democratic state politics from the 1890s to 1910 with a base among poor white farmers. During the constitutional convention in 1895, he supported another man's proposal that the state adopt a one-drop rule, as well as prohibit marriage between whites and anyone with any known African ancestry.

Some members of the convention realized prominent white families with some African ancestry could be affected by such legislation. In terms similar to a debate in Virginia in 1853 on a similar proposal (which was dropped), George Dionysius Tillman said in opposition:
If the law is made as it now stands respectable families in Aiken, Barnwell, Colleton, and Orangeburg will be denied the right to intermarry among people with whom they are now associated and identified. At least one hundred families would be affected to my knowledge. They have sent good soldiers to the Confederate Army, and are now landowners and taxpayers. Those men served creditably, and it would be unjust and disgraceful to embarrass them in this way. It is a scientific fact that there is not one full-blooded Caucasian on the floor of this convention. Every member has in him a certain mixture of... colored blood. The pure-blooded white has needed and received a certain infusion of darker blood to give him readiness and purpose. It would be a cruel injustice and the source of endless litigation, of scandal, horror, feud, and bloodshed to undertake to annul or forbid marriage for a remote, perhaps obsolete trace of Negro blood. The doors would be open to scandal, malice and greed; to statements on the witness stand that the father or grandfather or grandmother had said that A or B had Negro blood in their veins. Any man who is half a man would be ready to blow up half the world with dynamite to prevent or avenge attacks upon the honor of his mother in the legitimacy or purity of the blood of his father.

The state postponed such a one-drop law for years. Virginian legislators adopted a one-drop law in 1924, forgetting that their state had many people of mixed ancestry among those who identified as white.

Early in the 20th century, South Carolina developed a thriving textile industry. The state also converted its agricultural base from cotton to more profitable crops; attracted large military bases through its powerful Democratic congressional delegation, part of the one-party South following disfranchisement of blacks at the turn of the century; and created tourism industries. During the early part of the 20th century, millions of African Americans left South Carolina and other southern states for jobs, opportunities and relative freedom in U.S. cities outside the former Confederate states. In total from 1910 to 1970, 6.5 million blacks left the South in the Great Migration. By 1930 South Carolina had a white majority for the first time since 1708.

South Carolina was one of several states that initially rejected the Nineteenth Amendment (1920) giving women the right to vote. The South Carolina legislature later ratified the amendment on July 1, 1969.

The struggle of the civil rights movement took place in South Carolina as they did in other Southern states. However, South Carolina experienced a much less violent movement than other Southern states. This tranquil transition from a Jim Crow society occurred because the state's white and black leaders were willing to accept slow change rather than being utterly unwilling to accept change at all. Other South Carolinians, like Sen. Strom Thurmond, on the other hand, were among the nation's most radical and effective opponents of social equality and integration.

As of 2015, South Carolina had one of the lowest percentages among all states of women in state legislature, at 13.5% (only Louisiana, Oklahoma, and Wyoming had a lower percentage; the national average is 24.3%; with the highest percentage being in Colorado at 41%). In 2011, South Carolina ranked first in the country in the rate of women killed by men.

As the 21st century progresses, South Carolina attracts new business by having a 5% corporate income tax rate, no state property tax, no local income tax, no inventory tax, no sales tax on manufacturing equipment, industrial power or materials for finished products; no wholesale tax, no unitary tax on worldwide profits.

South Carolina was one of the first states to stop paying for 'early elective' deliveries of babies, under either Medicaid and private insurance. The term early elective is defined as a labor induction or Cesarean section between 37–39 weeks that is not medically based. This change is intended to result in healthier babies and fewer unnecessary costs for South Carolina.

On November 20, 2014, South Carolina became the 35th state to legalize same-sex marriages, when a federal court ordered the change.

South Carolina has many venues for visual and performing arts. The Gibbes Museum of Art in Charleston, the Greenville County Museum of Art, the Columbia Museum of Art, Spartanburg Art Museum, and the South Carolina State Museum in Columbia among others provide access to visual arts to the state. There are also numerous historic sites and museums scattered throughout the state paying homage to many events and periods in the state's history from Native American inhabitation to the present day.

South Carolina also has performing art venues including the Peace Center in Greenville, the Koger Center for the Arts in Columbia, and the Newberry Opera House, among others to bring local, national, and international talent to the stages of South Carolina. Several large venues can house major events, including Colonial Life Arena in Columbia, Bon Secours Wellness Arena in Greenville, and North Charleston Coliseum.

One of the nation's major performing arts festivals, Spoleto Festival USA, is held annually in Charleston. There are also countless local festivals throughout the state highlighting many cultural traditions, historical events, and folklore.

According to the South Carolina Arts Commission, creative industries generate $9.2 billion annually and support over 78,000 jobs in the state. A 2009 statewide poll by the University of South Carolina Institute for Public Service and Policy Research found that 67% of residents had participated in the arts in some form during the past year and on average citizens had participated in the arts 14 times in the previous year.

According to the Association of Religion Data Archives (ARDA), in 2010 the largest denominations were the Southern Baptist Convention with 913,763 adherents, the United Methodist Church with 274,111 adherents, and the Roman Catholic Church with 181,743 adherents. Fourth largest is the African Methodist Episcopal Church with 564 congregations and 121,000 members and fifth largest is the Presbyterian Church (USA) with 320 congregations and almost 100,000 members.

As of 2010, South Carolina is the American state with the highest per capita proportion of Baha'is with 17,559 adherents, making the Baha'i Faith the second largest religion in the state.

Although no major league professional sports teams are based in South Carolina, the Carolina Panthers do have training facilities in the state and played their inaugural season's home games at Clemson's Memorial Stadium, however, they currently play in North Carolina. The Panthers consider themselves "The Carolinas' Team" and refrained from naming themselves after Charlotte or either of the Carolinas. The state is also home to numerous minor league professional teams. College teams represent their particular South Carolina institutions, and are the primary options for football, basketball and baseball attendance in the state. South Carolina is also a top destination for golf and water sports.

South Carolina is also home to one of NASCAR's first tracks and its first paved speedway, Darlington Raceway northwest of Florence.


In 2019, South Carolina's GDP was $249.9 billion, making the state the 26th largest by GDP in the United States. According to the U.S. Bureau of Economic Analysis, South Carolina's gross state product (GSP) in was $97 billion in 1997 and $153 billion in 2007. Its per-capita real gross domestic product (GDP) in chained 2000 dollars was $26,772 in 1997 and $28,894 in 2007; which represented 85% of the $31,619 per-capita real GDP for the United States overall in 1997, and 76% of the $38,020 for the U.S. in 2007. The state debt in 2012 was calculated by one source to be $22.9bn, or $7,800 per taxpayer.

Industrial outputs include textile goods, chemical products, paper products, machinery, automobiles, automotive products and tourism. Major agricultural outputs of the state are tobacco, poultry, cotton, cattle, dairy products, soybeans, hay, rice, and swine. According to the Bureau of Labor Statistics, as of March 2012, South Carolina had 1,852,700 nonfarm jobs of which 12% are in manufacturing, 11.5% are in leisure and hospitality, 19% are in trade, transportation, and utilities, and 11.8% are in education and health services. The service sector accounts for 83.7% of the South Carolina economy.

Many large corporations have moved their locations to South Carolina. Boeing opened an aircraft manufacturing facility in Charleston in 2011, which serves as one of two final assembly sites for the 787 Dreamliner. South Carolina is a right-to-work state and many businesses utilize staffing agencies to temporarily fill positions. Domtar, in Rock Hill, is the only Fortune 500 company headquartered in South Carolina. The Fortune 1000 list includes SCANA, Sonoco Products and ScanSource.

South Carolina also benefits from foreign investment. There are 1,950 foreign-owned firms operating in South Carolina employing almost 135,000 people. Foreign Direct Investment (FDI) brought 1.06 billion dollars to the state economy in 2010. Since 1994, BMW has had a production facility in Spartanburg County near Greer and since 1996 the Zapp Group operates in Summerville near Charleston.

There are 36 TV stations (including PBS affiliates) serving South Carolina with terrestrial, and some online streaming access. Markets in which the stations are located include Columbia, Florence, Allendale, Myrtle Beach, Greenville, Charleston, Conway, Beaufort, Hardeeville, Spartanburg, Greenwood, Anderson and Sumter.

The state has the fourth largest state-maintained system in the country, consisting of 11 Interstates, numbered highways, state highways, and secondary roads, totalling approximately 41,500 miles.

On secondary roads, South Carolina uses a numbering system to keep track of all non-interstate and primary highways that the South Carolina Department of Transportation maintains. Secondary roads are numbered by the number of the county followed by a unique number for the particular road.

Passenger rail
Amtrak operates four passenger routes in South Carolina: the "Crescent", the "Palmetto", the "Silver Meteor", and the "Silver Star". The "Crescent" route serves the Upstate cities, the "Silver Star" serves the Midlands cities, and the "Palmetto" and "Silver Meteor" routes serve the lowcountry cities.

Station stops
Freight
CSX Transportation and Norfolk Southern are the only Class I railroad companies in South Carolina, as other freight companies in the state are short lines.

There are seven significant airports in South Carolina, all of which act as regional airport hubs. The busiest by passenger volume is Charleston International Airport. Just across the border in North Carolina is Charlotte/Douglas International Airport, the 30th busiest airport in the world, in terms of passengers.

As of 2010, South Carolina is one of three states that have not agreed to use competitive international math and language standards.

In 2014, the South Carolina Supreme Court ruled the state had failed to provide a "minimally adequate" education to children in all parts of the state as required by the state's constitution.

South Carolina has 1,144 K–12 schools in 85 school districts with an enrollment of 712,244 as of fall 2009. As of the 2008–2009 school year, South Carolina spent $9,450 per student which places it 31st in the country for per student spending.

In 2015, the national average SAT score was 1490 and the South Carolina average was 1442, 48 points lower than the national average.

South Carolina is the only state which owns and operates a statewide school bus system. As of December 2016, the state maintains a 5,582-bus fleet with the average vehicle in service being fifteen years old (the national average is six) having logged 236,000 miles. Half of the state's school buses are more than 15 years old and some are reportedly up to 30 years old. In 2017 in the budget proposal, Superintendent of Education Molly Spearman requested the state lease to purchase 1,000 buses to replace the most decrepit vehicles. An additional 175 buses could be purchased immediately through the State Treasurer's master lease program. On January 5, 2017, the U.S. Environmental Protection Agency awarded South Carolina more than $1.1 million to replace 57 school buses with new cleaner models through its Diesel Emissions Reduction Act program.

South Carolina has diverse institutions from large state-funded research universities to small colleges that cultivate a liberal arts, religious or military tradition.


For overall health care, South Carolina is ranked 33rd out of the 50 states, according to the Commonwealth Fund, a private health foundation working to improve the health care system. The state's teen birth rate was 53 births per 1,000 teens, compared to the national average of 41.9 births, according to the Kaiser Family Foundation. The state's infant mortality rate was 9.4 deaths per 1,000 births compared to the national average of 6.9 deaths.

There were 2.6 physicians per 1,000 people compared to the national average of 3.2 physicians. There was $5,114 spent on health expenses per capita in the state, compared to the national average of $5,283. There were 26 percent of children and 13 percent of elderly living in poverty in the state, compared to 23 percent and 13 percent, respectively, doing so in the U.S. And, 34 percent of children were overweight or obese, compared to the national average of 32 percent.



List of TV stations in South Carolina: Television Stations – Station Index


</doc>
<doc id="27960" url="https://en.wikipedia.org/wiki?curid=27960" title="Sessions">
Sessions

Sessions may refer to:





</doc>
<doc id="27962" url="https://en.wikipedia.org/wiki?curid=27962" title="Session">
Session

Session may refer to:









</doc>
<doc id="27964" url="https://en.wikipedia.org/wiki?curid=27964" title="Sikhism">
Sikhism

Sikhi (', or Sikhism (); , from ), is a monotheistic religion that originated in the Punjab region of the Indian subcontinent around the end of the 15th century. 
It is one of the youngest of the major religions and world's fifth largest organized religion, with about 25 million Sikhs as of the early 21st century.

Sikhism is based on the spiritual teachings of Guru Nanak, the first Guru (1469–1539), and the nine Sikh Gurus that succeeded him. The tenth Guru, Guru Gobind Singh, named the Sikh scripture "Guru Granth Sahib" as his successor, terminating the line of human Gurus and establishing the scripture as the eternal, religious spiritual guide for Sikhs. Guru Nanak taught that living an "active, creative, and practical life" of "truthfulness, fidelity, self-control and purity" is above the metaphysical truth, and that the ideal man is one who "establishes union with God, knows His Will, and carries out that Will." Guru Hargobind, the sixth Sikh Guru, established the "miri" (political/temporal) and "piri" (spiritual) realms to be mutually coexistent.

The Sikh scripture opens with the "Mul Mantar" (), fundamental prayer about "ik onkar" (). The core beliefs of Sikhism, articulated in the "Guru Granth Sahib", include faith and meditation on the name of the one creator; divine unity and equality of all humankind; engaging in "seva" ('selfless service'); striving for justice for the benefit and prosperity of all; and honest conduct and livelihood while living a householder's life. Following this standard, Sikhism rejects claims that any particular religious tradition has a monopoly on Absolute Truth.

Sikhism emphasizes "simran" (, meditation and remembrance of the words of God), which can be expressed musically through "kirtan", or internally through "naam japna" ('meditation on His name') as a means to feel God's presence. It teaches followers to transform the "Five Thieves" (i.e. lust, rage, greed, attachment, and ego). 

The religion developed and evolved in times of religious persecution, gaining converts from both Hinduism and Islam. Two of the Sikh gurus—Guru Arjan (1563–1605) and Guru Tegh Bahadur (1621–1675)—were tortured and executed by the Mughal rulers after they refused to convert to Islam. The persecution of Sikhs triggered the founding of the "Khalsa", by Guru Gobind Singh, as an order to protect the freedom of conscience and religion, with qualities of a "Sant-Sipāhī"—a 'saint-soldier'.

The majority of Sikh scriptures were originally written in the alphabet of "Gurmukhī", a script standardised by Guru Angad out of Laṇḍā scripts used in North India. Adherents of Sikhism are known as "Sikhs", meaning 'students' or 'disciples' of the Guru. The anglicised word "Sikhism" derives from the Punjabi verb "Sikhi", which connotes the "temporal path of learning" and is rooted in the word "sikhana" ('to learn').

Sikhism is classified as an Indian religion along with Buddhism, Hinduism and Jainism.

The basis of Sikhism lies in the teachings of Guru Nanak and his successors. Many sources call Sikhism a monotheistic religion, while others call it a monistic and panentheistic religion. According to Nesbitt (2005), English renderings of Sikhism as a monotheistic religion "tend misleadingly to reinforce a Semitic understanding of monotheism, rather than Guru Nanak's mystical awareness of the one that is expressed through the many. However, what is not in doubt is the emphasis on 'one'."

In Sikhism, the concept of "God" is "Waheguru" ('wondrous Lord') considered to be "nirankar" ('shapeless'), "akal" ('timeless'), "karta purakh" ('The Creator'), and "agam agochar" ('incomprehensible and invisible'). The Sikh scripture begins with "ik onkar" (ੴ), which refers to the "formless one", understood in the Sikh tradition as monotheistic unity of God. Sikh ethics emphasize the congruence between spiritual development and everyday moral conduct. Its founder Guru Nanak summarized this perspective:Truth is the highest virtue, but higher still is truthful living.

Sikhism lays emphasis on "Ėk nūr te sab jag upjiā", 'From the one light, the entire universe welled up.'

God in Sikhism is known as "ik onkar" (), the One Supreme Reality, the One Creator or the all-pervading spirit (i.e. God). This spirit has no gender in Sikhism, though translations may present it as masculine. It is also "akaal purkh" ('beyond time and space') and "nirankar" ('without form'). In addition, Nanak wrote that there are many worlds on which it has created life.

The traditional "Mul Mantar" goes from "ik onkar" until "Nanak hosee bhee sach". The opening line of the Guru Granth Sahib and each subsequent "raga", mentions "ik onkar":

"Māyā", defined as a temporary illusion or "unreality", is one of the core deviations from the pursuit of God and salvation: where worldly attractions which give only illusory temporary satisfaction and pain which distract the process of the devotion of God. However, Nanak emphasised māyā as not a reference to the unreality of the world, but of its values. In Sikhism, the influences of ego, anger, greed, attachment, and lust, known as the "pānj chor" ('five thieves'), are believed to be particularly distracting and hurtful. Sikhs believe the world is currently in a state of "kali yuga" ('age of darkness') because the world is led astray by the love of and attachment to maya. The fate of people vulnerable to the five thieves, is separation from God, and the situation may be remedied only after intensive and relentless devotion.

According to Guru Nanak the supreme purpose of human life is to reconnect with "Akal" ('The Timeless One;), however, egotism is the biggest barrier in doing this. Using the Guru's teaching remembrance of "nām" (the divine Name of the Lord) leads to the end of egotism. Guru Nanak designated the word "Guru" ('teacher') to mean the voice of "the spirit": the source of knowledge and the guide to salvation. As "ik onkar" is universally immanent, "Guru" is indistinguishable from "Akal" and are one and the same. One connects with "Guru" only with accumulation of selfless search of truth. Ultimately the seeker realises that it is the consciousness within the body which is seeker/follower of the Word that is the true "Guru". The human body is just a means to achieve the reunion with Truth. Once truth starts to shine in a person's heart, the essence of current and past holy books of all religions is understood by the person.

Guru Nanak's teachings are founded not on a final destination of heaven or hell but on a spiritual union with the "Akal" which results in salvation or "jivanmukti" ('enlightenment/liberation within one's lifetime'), a concept also found in Hinduism. Guru Gobind Singh makes it clear that human birth is obtained with great fortune, therefore one needs to be able to make the most of this life.

Sikhs believe in reincarnation and karma concepts found in Buddhism, Hinduism, and Jainism. However, in Sikhism, both karma and liberation "is modified by the concept of God's grace" ("nadar, mehar, kirpa, karam", etc.). Guru Nanak states that "the body takes birth because of karma, but salvation is attained through grace." To get closer to God, Sikhs: avoid the evils of "maya"; keep the everlasting truth in mind; practice "shabad kirtan" (musical recitation of hymns); meditate on "naam"; and serve humanity. Sikhs believe that being in the company of the "satsang" (association with "sat", 'true', people) or "sadh sangat" is one of the key ways to achieve liberation from the cycles of reincarnation.

Sikhism was influenced by the Bhakti movement, but it was not simply an extension of Bhakti.

Guru Nanak, the first Sikh Guru and the founder of Sikhism, was a Bhakti saint. He taught that the most important form of worship is "Bhakti" (devotion to Bhagvan). Guru Arjan, in the "Sukhmani Sahib", recommended the true religion is one of loving devotion to God. The "Guru Granth Sahib" includes suggestions on how a Sikh should perform constant Bhakti. Some scholars call Sikhism a Bhakti sect of Indian traditions, adding that it emphasises ""nirguni Bhakti"," i.e. loving devotion to a divine without qualities or physical form. However, Sikhism also accepts the concept of "saguni", i.e. a divine with qualities and form. While Western scholarship generally places Sikhism as arising primarily within a Hindu Bhakti movement milieu while recognizing some Sufi Islamic influences, Indian Sikh scholars disagree and state that Sikhism transcended the environment it emerged from.

Some Sikh sects outside the Punjab-region of India, such as those found in Maharashtra and Bihar, practice Aarti with lamps during bhakti in a Sikh Gurdwara. But, most Sikh Gurdwaras forbid "aarti" (the ceremonial use of lamps) during their bhakti practices.

While emphasizing Bhakti, the Sikh Gurus also taught that the spiritual life and secular householder life are intertwined. In Sikh worldview, the everyday world is part of the Infinite Reality, increased spiritual awareness leads to increased and vibrant participation in the everyday world. Guru Nanak described living an "active, creative, and practical life" of "truthfulness, fidelity, self-control and purity" as being higher than the metaphysical truth.

The 6th Sikh Guru, Guru Hargobind, after Guru Arjan martyrdom and faced with oppression by the Islamic Mughal Empire, affirmed the philosophy that the political/temporal ("Miri") and spiritual ("Piri") realms are mutually coexistent. According to the 9th Sikh Guru, Tegh Bahadur, the ideal Sikh should have both "Shakti" (power that resides in the temporal), and "Bhakti" (spiritual meditative qualities). This was developed into the concept of the Saint Soldier by the 10th Sikh Guru, Gobind Singh.

The concept of man as elaborated by Guru Nanak refines and negates the "monotheistic concept of self/God," and "monotheism becomes almost redundant in the movement and crossings of love." The goal of man, taught the Sikh Gurus, is to end all dualities of "self and other, I and not-I," attain the "attendant balance of separation-fusion, self-other, action-inaction, attachment-detachment, in the course of daily life."

Sikhs refer to the hymns of the Gurus as "Gurbani" ('The Guru's word'). Shabad Kirtan is the singing of Gurbani. The entire verses of Guru Granth Sahib are written in a form of poetry and rhyme to be recited in thirty one Ragas of the Classical Indian Music as specified. However, the exponents of these are rarely to be found amongst the Sikhs who are conversant with all the Ragas in the Guru Granth Sahib. Guru Nanak started the Shabad Kirtan tradition and taught that listening to kirtan is a powerful way to achieve tranquility while meditating; Singing of the glories of the Supreme Timeless One (God) with devotion is the most effective way to come in communion with the Supreme Timeless One. The three morning prayers for Sikhs consist of Japji Sahib, Jaap Sahib and Tav-Prasad Savaiye. Baptised Sikhs – Amritdharis, rise early and meditate and then recite all the Five Banis of Nitnem before breakfast.

A key practice by Sikhs is remembrance of the Divine Name WaheGuru (Naam – the Name of the Lord). This contemplation is done through "Nām Japna" (repetition of the divine name) or "Naam Simran" (remembrance of the divine Name through recitation). The verbal repetition of the name of God or a sacred syllable has been an ancient established practice in religious traditions in India, however, Sikhism developed "Naam-simran" as an important Bhakti practice. Guru Nanak's ideal is the total exposure of one's being to the divine Name and a total conforming to Dharma or the "Divine Order". Nanak described the result of the disciplined application of "nām simraṇ" as a "growing towards and into God" through a gradual process of five stages. The last of these is "sach khaṇḍ" ("The Realm of Truth")the final union of the spirit with God.

The Sikh Gurus taught that by constantly remembering the divine name ("naam simran") and through selfless service, or "sēvā", the devotee overcomes egotism ("Haumai"). This, it states, is the primary root of five evil impulses and the cycle of rebirth.

Service in Sikhism takes three forms: "Tan" – physical service; "Man" – mental service (such as studying to help others); and "Dhan" – material service. Sikhism stresses "kirat karō": that is "honest work". Sikh teachings also stress the concept of sharing, or "vaṇḍ chakkō", giving to the needy for the benefit of the community.

Sikhism regards God as the true king, the king of all kings, the one who dispenses justice through the law of "karma", a retributive model and divine grace.

The term for justice in the Sikh tradition is "Niau". It is related to the term "dharam" which in Sikhism connotes 'moral order' and righteousness. According to the Tenth Sikh Guru Guru Gobind Singh, states Pashaura Singh – a professor of Sikh Studies, "one must first try all the peaceful means of negotiation in the pursuit of justice" and if these fail then it is legitimate to "draw the sword in defense of righteousness". Sikhism considers "an attack on dharam is an attack on justice, on righteousness, and on the moral order generally" and the dharam "must be defended at all costs". The divine name is its antidote for all pain and vices. Forgiveness is taught as a virtue in Sikhism, yet it also teaches its faithful to shun those with evil intentions and to pick up the sword to fight injustice and religious persecution.

Sikhism does not differentiate religious obligations by gender. God in Sikhism has no gender, and the Sikh scripture does not discriminate against women, nor bar them from any roles. Women in Sikhism have led battles and issued hukamnamas.

The term Guru comes from the Sanskrit "gurū", meaning teacher, guide, or mentor. The traditions and philosophy of Sikhism were established by ten Gurus from 1469 to 1708. Each Guru added to and reinforced the message taught by the previous, resulting in the creation of the Sikh religion. Guru Nanak was the first Guru and appointed a disciple as successor. Guru Gobind Singh was the final Guru in human form. Before his death, Guru Gobind Singh decreed in 1708, that the Gurū Granth Sāhib would be the final and perpetual Guru of the Sikhs.

Guru Nanak stated that his Guru is God who is the same from the beginning of time to the end of time. Nanak claimed to be God's mouthpiece, God's slave and servant, but maintained that he was only a guide and teacher. Nanak stated that the human Guru is mortal, who is to be respected and loved but not worshipped. When Guru, or SatGuru (The true Guru) is used in "Gurbani" it is often referring to the highest expression of truthfulness – God.

Guru Angad succeeded Guru Nanak. Later, an important phase in the development of Sikhism came with the third successor, Guru Amar Das. Guru Nanak's teachings emphasised the pursuit of salvation; Guru Amar Das began building a cohesive community of followers with initiatives such as sanctioning distinctive ceremonies for birth, marriage, and death. Amar Das also established the "manji" (comparable to a diocese) system of clerical supervision.

Guru Amar Das's successor and son-in-law Guru Ram Das founded the city of Amritsar, which is home of the Harimandir Sahib and regarded widely as the holiest city for all Sikhs. Guru Arjan was arrested by Mughal authorities who were suspicious and hostile to the religious order he was developing. His persecution and death inspired his successors to promote a military and political organization of Sikh communities to defend themselves against the attacks of Mughal forces.

The Sikh Gurus established a mechanism which allowed the Sikh religion to react as a community to changing circumstances. The sixth Guru, Guru Hargobind, was responsible for the creation of the concept of Akal Takht ("throne of the timeless one"), which serves as the supreme decision-making centre of Sikhism and sits opposite the Harmandir Sahib. The Akal Takht is located in the city of Amritsar. The leader is appointed by the Shiromani Gurdwara Pabandhak Committee (SPGC). The "Sarbat Ḵẖālsā" (a representative portion of the Khalsa Panth) historically gathers at the Akal Takht on special festivals such as Vaisakhi or Hola Mohalla and when there is a need to discuss matters that affect the entire Sikh nation. A "gurmatā" (literally, "Guru's intention") is an order passed by the Sarbat Ḵẖālsā in the presence of the Gurū Granth Sāhib. A gurmatā may only be passed on a subject that affects the fundamental principles of Sikh religion; it is binding upon all Sikhs. The term "hukamnāmā" (literally, "edict" or "royal order") is often used interchangeably with the term gurmatā. However, a hukamnāmā formally refers to a hymn from the Gurū Granth Sāhib which is a given order to Sikhs.
The word Guru in Sikhism also refers to "Akal Purkh" (God), and God and Guru are often synonymous in "Gurbani" (Sikh writings).

There is one primary scripture for the Sikhs: the Gurū Granth Sāhib. It is sometimes synonymously referred to as the Ādi Granth. Chronologically, however, the Ādi Granth – literally, "The First Volume", refers to the version of the scripture created by Guru Arjan in 1604. The Gurū Granth Sāhib is the final expanded version of the scripture compiled by Guru Gobind Singh. While the Guru Granth Sahib is an unquestioned scripture in Sikhism, another important religious text, the Dasam Granth, does not enjoy universal consensus, and is considered a secondary scripture by many Sikhs.

The Ādi Granth was compiled primarily by Bhai Gurdas under the supervision of Guru Arjan between the years 1603 and 1604. It is written in the Gurmukhī script, which is a descendant of the Laṇḍā script used in the Punjab at that time. The Gurmukhī script was standardised by Guru Angad, the second Guru of the Sikhs, for use in the Sikh scriptures and is thought to have been influenced by the Śāradā and Devanāgarī scripts. An authoritative scripture was created to protect the integrity of hymns and teachings of the Sikh Gurus, and thirteen Hindu and two Muslim bhagats of the Bhakti movement sant tradition in medieval India. The thirteen Hindu bhagats whose teachings were entered into the text included Ramananda, Namdev, Pipa, Ravidas, Beni, Bhikhan, Dhanna, Jaidev, Parmanand, Sadhana, Sain, Sur, Trilochan, while the two Muslim bhagats were Kabir and Sufi saint Farid. However the bhagats in context often spoke of transcending their religious labels, Kabir often attributed to being a Muslim states in the Adi Granth "I am not Hindu nor Muslim. The Gurus following on this message taught that different methods of devotion are for the same infinite God.

The Guru Granth Sahib is the holy scripture of the Sikhs, and regarded as the living Guru.

The Guru Granth started as a volume of Guru Nanak's poetic compositions. Prior to his death, he passed on his volume to Guru Angad (Guru 1539–1551). The final version of the Gurū Granth Sāhib was compiled by Guru Gobind Singh in 1678. It consists of the original Ādi Granth with the addition of Guru Tegh Bahadur's hymns. The predominant bulk of Guru Granth Sahib is compositions by seven Sikh Gurus – Guru Nanak, Guru Angad, Guru Amar Das, Guru Ram Das, Guru Arjan, Guru Teg Bahadur and Guru Gobind Singh. It also contains the traditions and teachings of thirteen Hindu Bhakti movement "sants" (saints) such as Ramananda, Namdev among others, and two Muslim saints namely Kabir and the Sufi Sheikh Farid.

The text comprises 6,000 "śabads" (line compositions), which are poetically rendered and set to rhythmic ancient north Indian classical music. The bulk of the scripture is classified into thirty one "rāgas", with each Granth rāga subdivided according to length and author. The hymns in the scripture are arranged primarily by the "rāgas" in which they are read.

The main language used in the scripture is known as "Sant Bhāṣā", a language related to both Punjabi and Hindi and used extensively across medieval northern India by proponents of popular devotional religion (bhakti). The text is printed in Gurumukhi script, believed to have been developed by Guru Angad, but it shares the Indo-European roots found in numerous regional languages of India.

The vision in the Guru Granth Sahib, states Torkel Brekke, is a society based on divine justice without oppression of any kind.

The Granth begins with the "Mūl Mantra", an iconic verse which received Guru Nanak directly from Akal Purakh (God).
The traditional Mul Mantar goes from Ik Oankar until Nanak Hosee Bhee Sach.

The Tenth Guru, Guru Gobind Singh ji, named the Sikh scripture Guru Granth Sahib as his successor, terminating the line of human Gurus and making the scripture the literal embodiment of the eternal, impersonal Guru, where Gods/Gurus word serves as the spiritual guide for Sikhs.

The Guru Granth Sahib is installed in Sikh "Gurdwara" (temple); many Sikhs bow or prostrate before it on entering the temple. The Guru Granth Sahib is installed every morning and put to bed at night in many "Gurdwaras". The Granth is revered as eternal "gurbānī" and the spiritual authority.

The copies of the Guru Granth Sahib are not regarded as material objects, but as living subjects which are alive. According to Myrvold, the Sikh scripture is treated with respect like a living person, in a manner similar to the Gospel in early Christian worship. Old copies of the Sikh scripture are not thrown away, rather funerary services are performed.

In India the Guru Granth Sahib is even officially recognised by the Supreme Court of India as a judicial person which can receive donations and own land. Yet, some Sikhs also warn that, without true comprehension of the text, veneration for the text can lead to bibliolatry, with the concrete form of the teachings becoming the object of worship instead of the teachings themselves.

The Sikh scriptures use Hindu terminology, with references to the Vedas, and the names of gods and goddesses in Hindu bhakti movement traditions, such as Vishnu, Shiva, Brahma, Parvati, Lakshmi, Saraswati, Rama, Krishna, but not to worship. It also refers to the spiritual concepts in Hinduism ("Ishvara, Bhagavan, Brahman") and the concept of God in Islam ("Allah") to assert that these are just "alternate names for the Almighty One".

While the Guru Granth Sahib acknowledges the Vedas, Puranas and Qur'an, it does not imply a syncretic bridge between Hinduism and Islam, but emphasises focusing on nitnem banis like Japu (repeating mantra of the divine Name of God – WaheGuru), instead of Muslim practices such as circumcision or praying on a carpet, or Hindu rituals such as wearing thread or praying in a river.

The Dasam Granth is a scripture of Sikhs which contains texts attributed to the Guru Gobind Singh. The "Dasam Granth" is important to a great number of Sikhs, however it does not have the same authority as the "Guru Granth Sahib". Some compositions of the "Dasam Granth" like Jaap Sahib, (Amrit Savaiye), and Benti Chaupai are part of the daily prayers (Nitnem) for Sikhs. The "Dasam Granth" is largely versions of Hindu mythology from the Puranas, secular stories from a variety of sources called "Charitro Pakhyan" – tales to protect careless men from perils of lust.

Five versions of "Dasam Granth" exist, and the authenticity of the "Dasam Granth" has in modern times become one of the most debated topics within Sikhism. The text played a significant role in Sikh history, but in modern times parts of the text have seen antipathy and discussion among Sikhs.

The Janamsākhīs (literally "birth stories"), are writings which profess to be biographies of Guru Nanak. Although not scripture in the strictest sense, they provide a hagiographic look at Guru Nanak's life and the early start of Sikhism. There are several – often contradictory and sometimes unreliable – Janamsākhīs and they are not held in the same regard as other sources of scriptural knowledge.

Observant Sikhs adhere to long-standing practices and traditions to strengthen and express their faith. The daily recitation of the divine name of God VaheGuru and from memory of specific passages from the Gurū Granth Sāhib, like the "Japu" (or "Japjī", literally "chant") hymns is recommended immediately after rising and bathing. Baptized Sikhs recite the five morning prayers, the evening and night prayer. Family customs include both reading passages from the scripture and attending the gurdwara (also "gurduārā", meaning "the doorway to God"; sometimes transliterated as "Gurudwara"). There are many gurdwaras prominently constructed and maintained across India, as well as in almost every nation where Sikhs reside. Gurdwaras are open to all, regardless of religion, background, caste, or race.

Worship in a gurdwara consists chiefly of singing of passages from the scripture. Sikhs will commonly enter the gurdwara, touch the ground before the holy scripture with their foreheads. The recitation of the eighteenth century "ardās" is also customary for attending Sikhs. The ardās recalls past sufferings and glories of the community, invoking divine grace for all humanity.

The gurdwara is also the location for the historic Sikh practice of "Langar" or the community meal. All gurdwaras are open to anyone of any faith for a free meal, always vegetarian. People eat together, and the kitchen is maintained and serviced by Sikh community volunteers.

Guru Amar Das chose festivals for celebration by Sikhs like Vaisakhi, wherein he asked Sikhs to assemble and share the festivities as a community.

Vaisakhi is one of the most important festivals of Sikhs, while other significant festivals commemorate the birth, lives of the Gurus and Sikh martyrs. Historically, these festivals have been based on the moon calendar Bikrami calendar. In 2003, the SGPC, the Sikh organisation in charge of upkeep of the historical gurdwaras of Punjab, adopted Nanakshahi calendar. The new calendar is highly controversial among Sikhs and is not universally accepted. Sikh festivals include the following:


Khalsa Sikhs have also supported and helped develop major pilgrimage traditions to sacred sites such as Harmandir Sahib, Anandpur Sahib, Fatehgarh Sahib, Patna Sahib, Hazur Nanded Sahib, Hemkund Sahib and others. Sikh pilgrims and Sikhs of other sects customarily consider these as holy and a part of their "Tirath". The Hola Mohalla around the festival of Holi, for example, is a ceremonial and customary gathering every year in Anandpur Sahib attracting over 100,000 Sikhs. Major Sikh temples feature a "sarovar" where some Sikhs take a customary dip. Some take home the sacred water of the tank particularly for sick friends and relatives, believing that the waters of such sacred sites have restorative powers and the ability to purify one's "karma". The various Gurus of Sikhism have had different approach to pilgrimage.

Upon a child's birth, the Guru Granth Sahib is opened at a random point and the child is named using the first letter on the top left hand corner of the left page. All boys are given the last name Singh, and all girls are given the last name Kaur (this was once a title which was conferred on an individual upon joining the Khalsa).

The Sikh marriage ritual includes the "anand kāraj" ceremony. The marriage ceremony is performed in front of the Guru Granth Sahib by a baptized Khalsa, Granthi of the Gurdwara. The tradition of circling the Guru Granth Sahib and Anand Karaj among Khalsa is practised since the fourth Guru, Guru Ram Das. Its official recognition and adoption came in 1909, during the Singh Sabha Movement.

Upon death, the body of a Sikh is usually cremated. If this is not possible, any respectful means of disposing the body may be employed. The "kīrtan sōhilā" and "ardās" prayers are performed during the funeral ceremony (known as "antim sanskār").

Khalsa (meaning "pure and sovereign") is the collective name given by Guru Gobind Singh to those Sikhs who have been fully initiated by taking part in a ceremony called "ammrit sañcār" (nectar ceremony). During this ceremony, sweetened water is stirred with a double-edged sword while liturgical prayers are sung; it is offered to the initiating Sikh, who ritually drinks it. Many Sikhs are not formally and fully initiated, as they do not undergo this ceremony, but do adhere to some components of Sikhism and identify as Sikhs. The initiated Sikh, who is believed to be reborn, is referred to as Amritdhari or Khalsa Sikh, while those who are not initiated or baptised are referred to as Kesdhari or Sahajdhari Sikhs.

The first time that this ceremony took place was on Vaisakhi, which fell on 30 March 1699 at Anandpur Sahib in Punjab. It was on that occasion that Gobind Singh baptised the Pañj Piārē – the five beloved ones, who in turn baptised Guru Gobind Singh himself. To males who initiated, the last name Singh, meaning "lion", was given, while the last name Kaur, meaning "princess", was given to baptised Sikh females.

Baptised Sikhs wear five items, called the Five Ks (in Punjabi known as "pañj kakkē" or "pañj kakār"), at all times. The five items are: "kēs" (uncut hair), "kaṅghā" (small wooden comb), "kaṛā" (circular steel or iron bracelet), "kirpān" (sword/dagger), and "kacchera" (special undergarment). The Five Ks have both practical and symbolic purposes.

Guru Nanak (1469–1539), the founder of Sikhism, was born in the village of "Rāi Bhōi dī Talwandī", now called Nankana Sahib (in present-day Pakistan). His parents were Punjabi Khatri Hindus. According to the hagiography "Puratan Janamsakhi" composed more than two centuries after his death and probably based on oral tradition, Nanak as a boy was fascinated by religion and spiritual matters, spending time with wandering ascetics and holy men. His friend was Mardana, a Muslim. Together they would sing devotional songs all night in front of the public, and bathe in the river in the morning. One day, at the usual bath, Nanak went missing and his family feared he had drowned. Three days later he returned home, and declared: "There is no Hindu, there is no Muslim" (""nā kōi hindū nā kōi musalmān""). Thereafter, Nanak started preaching his ideas that form the tenets of Sikhism. In 1526, Guru Nanak at age 50, started a small commune in Kartarpur and his disciples came to be known as "Sikhs". Although the exact account of his itinerary is disputed, hagiographic accounts state he made five major journeys, spanning thousands of miles, the first tour being east towards Bengal and Assam, the second south towards Andhra and Tamil Nadu, the third north to Kashmir, Ladakh, and Mount Sumeru in Tibet, and the fourth to Baghdad. In his last and final tour, he returned to the banks of the Ravi River to end his days.

There are two competing theories on Guru Nanak's teachings. One, according to Cole and Sambhi, is based on hagiographical Janamsakhis, and states that Nanak's teachings and Sikhism were a revelation from God, and not a social protest movement nor any attempt to reconcile Hinduism and Islam in the 15th century. The other states, Nanak was a Guru. According to Singha, "Sikhism does not subscribe to the theory of incarnation or the concept of prophethood. But it has a pivotal concept of Guru. He is not an incarnation of God, not even a prophet. He is an illumined soul." The hagiographical "Janamsakhis" were not written by Nanak, but by later followers without regard for historical accuracy, and contain numerous legends and myths created to show respect for Nanak. The term revelation, clarify Cole and Sambhi, in Sikhism is not limited to the teachings of Nanak, they include all Sikh Gurus, as well as the words of past, present and future men and women, who possess divine knowledge intuitively through meditation. The Sikh revelations include the words of non-Sikh bhagats, some who lived and died before the birth of Nanak, and whose teachings are part of the Sikh scriptures. The Adi Granth and successive Sikh Gurus repeatedly emphasised, states Mandair, that Sikhism is "not about hearing voices from God, but it is about changing the nature of the human mind, and anyone can achieve direct experience and spiritual perfection at any time".

The roots of the Sikh tradition are, states Louis Fenech, perhaps in the Sant-tradition of India whose ideology grew to become the Bhakti tradition. Furthermore, adds Fenech, "Indic mythology permeates the Sikh sacred canon, the "Guru Granth Sahib" and the secondary canon, the "Dasam Granth" and adds delicate nuance and substance to the sacred symbolic universe of the Sikhs of today and of their past ancestors".

The development of Sikhism was influenced by the Bhakti movement; and Vaishnawa Hinduism. however, Sikhism was not simply an extension of the Bhakti movement. Sikhism developed while the region was being ruled by the Mughal Empire. Two of the Sikh Gurus – Guru Arjan and Guru Tegh Bahadur, after they refused to convert to Islam, were tortured and executed by the Mughal rulers. The Islamic era persecution of Sikhs triggered the founding of the Khalsa, as an order for freedom of conscience and religion. A Sikh is expected to embody the qualities of a "Sant-Sipāhī" a saint-soldier.

In 1539, Guru Nanak chose his disciple "Lahiṇā" as a successor to the Guruship rather than either of his sons. Lahiṇā was named Guru Angad and became the second Guru of the Sikhs. Nanak conferred his choice at the town of Kartarpur on the banks of the river Ravi. Sri Chand, Guru Nanak's son was also a religious man, and continued his own commune of Sikhs. His followers came to be known as the Udasi Sikhs, the first parallel sect of Sikhism that formed in Sikh history. The Udasis believe that the Guruship should have gone to Sri Chand, since he was a man of pious habits in addition to being Nanak's son.

Guru Angad, before joining Guru Nanak's commune, worked as a "pujari" (priest) and religious teacher centered around Hindu goddess Durga. On Nanak's advice, Guru Angad moved from Kartarpur to Khadur, where his wife Khivi and children were living, until he was able to bridge the divide between his followers and the Udasis. Guru Angad continued the work started by Guru Nanak and is widely credited for standardising the Gurmukhī script as used in the sacred scripture of the Sikhs.

Guru Amar Das became the third Sikh Guru in 1552 at the age of 73. He adhered to the Vaishnavism tradition of Hinduism for much of his life, before joining the commune of Guru Angad. Goindval became an important centre for Sikhism during the Guruship of Guru Amar Das. He was a reformer, and discouraged veiling of women's faces (a Muslim custom) as well as sati (a Hindu custom). He encouraged the Kshatriya people to fight in order to protect people and for the sake of justice, stating this is Dharma. Guru Amar Das started the tradition of appointing "manji" (zones of religious administration with an appointed chief called "sangatias"), introduced the "dasvandh" ("the tenth" of income) system of revenue collection in the name of Guru and as pooled community religious resource, and the famed "langar" tradition of Sikhism where anyone, without discrimination of any kind, could get a free meal in a communal seating. The collection of revenue from Sikhs through regional appointees helped Sikhism grow.

Guru Amar Das named his disciple and son-in-law "Jēṭhā" as the next Guru, who came to be known as Guru Ram Das. The new Guru faced hostilities from the sons of Guru Amar Das and therefore shifted his official base to lands identified by Guru Amar Das as Guru-ka-Chak. He moved his commune of Sikhs there and the place then was called Ramdaspur, after him. This city grew and later became Amritsar – the holiest city of Sikhism. Guru Ram Das expanded the "manji" organization for clerical appointments in Sikh temples, and for revenue collections to theologically and economically support the Sikh movement.

In 1581, Guru Arjan – youngest son of Guru Ram Das, became the fifth Guru of the Sikhs. The choice of successor, as throughout most of the history of Sikh Guru successions, led to disputes and internal divisions among the Sikhs. The elder son of Guru Ram Das named Prithi Chand is remembered in the Sikh tradition as vehemently opposing Guru Arjan, creating a faction Sikh community which the Sikhs following Guru Arjan called as "Minas" (literally, "scoundrels").

Guru Arjan is remembered in the Sikh for many things. He built the first Harimandir Sahib (later to become the Golden Temple). He was a poet and created the first edition of Sikh sacred text known as the Ādi Granth (literally "the first book") and included the writings of the first five Gurus and other enlightened 13 Hindu and 2 Muslim Sufi saints. In 1606, he was tortured and killed by the Mughal emperor Jahangir, for refusing to convert to Islam. His martyrdom is considered a watershed event in the history of Sikhism.
After the martyrdom of Guru Arjan, his son Guru Hargobind at age eleven became the sixth Guru of the Sikhs and Sikhism dramatically evolved to become a political movement in addition to being religious. Guru Hargobind carried two swords, calling one spiritual and the other for temporal purpose (known as "mīrī" and "pīrī" in Sikhism). According to the Sikh tradition, Guru Arjan asked his son Hargobind to start a military tradition to protect the Sikh people and always keep himself surrounded by armed Sikhs. The building of an armed Sikh militia began with Guru Hargobind. Guru Hargobind was soon arrested by the Mughals and kept in jail in Gwalior. It is unclear how many years he served in prison, with different texts stating it to be between 2 and 12 years. He married three women, built a fort to defend Ramdaspur and created a formal court called Akal Takht, now the highest Khalsa Sikh religious authority.

In 1644, Guru Hargobind named his grandson Har Rai as the Guru. The Mughal Emperor Shah Jahan attempted political means to undermine the Sikh tradition, by dividing and influencing the succession. The Mughal ruler gave land grants to Dhir Mal, a grandson of Guru Hargobind living in Kartarpur, and attempted to encourage Sikhs to recognise Dhir Mal as the rightful successor to Guru Hargobind. Dhir Mal issued statements in favour of the Mughal state, and critical of his grandfather Guru Arjan. Guru Hargobind rejected Dhir Mal, the later refused to give up the original version of the Adi Granth he had, and the Sikh community was divided.

Guru Har Rai is famed to have met Dara Shikoh during a time Dara Shikoh and his younger brother Aurangzeb were in a bitter succession fight. Aurangzeb summoned Guru Har Rai, who refused to go and sent his elder son Ram Rai instead. The emperor found a verse in the Sikh scripture insulting to Muslims, and Ram Rai agreed it was a mistake then changed it. Ram Rai thus pleased Aurangzeb, but displeased Guru Har Rai who excommunicated his elder son. He nominated his younger son Guru Har Krishan to succeed him in 1661. Aurangzeb responded by granting Ram Rai a jagir (land grant). Ram Rai founded a town there and enjoyed Aurangzeb's patronage, the town came to be known as Dehradun, after "Dehra" referring to Ram Rai's shrine. Sikhs who followed Ram Rai came to be known as Ramraiya Sikhs. Guru Har Krishan became the eighth Guru at the age of five, and died of smallpox before reaching the age of eight. No hymns composed by these three Gurus are included in the Guru Granth Sahib.

Guru Tegh Bahadur, the uncle of Guru Har Krishan, became Guru in 1665. Tegh Bahadur resisted the forced conversions of Kashmiri Pandits and non-Muslims to Islam, and was publicly beheaded in 1675 on the orders of Mughal emperor Aurangzeb in Delhi for refusing to convert to Islam. His beheading traumatized the Sikhs. His body was cremated in Delhi, the head was carried secretively by Sikhs and cremated in Anandpur. He was succeeded by his son, Gobind Rai who militarised his followers by creating the Khalsa in 1699, and baptising the "Pañj Piārē". From then on, he was known as Guru Gobind Singh, and Sikh identity was redefined into a political force resisting religious persecution.

Guru Gobind Singh inaugurated the Khalsa (the collective body of all initiated Sikhs) as the Sikh temporal authority in the year 1699. It created a community that combines its spiritual purpose and goals with political and military duties. Shortly before his death, Guru Gobind Singh proclaimed the Gurū Granth Sāhib (the Sikh Holy Scripture) to be the ultimate spiritual authority for the Sikhs.

The Sikh Khalsa's rise to power began in the 17th century during a time of growing militancy against Mughal rule. The creation of a Sikh Empire began when Guru Gobind Singh sent a Sikh general, Banda Singh Bahadur, to fight the Mughal rulers of India and those who had committed atrocities against Pir Buddhu Shah. Banda Singh advanced his army towards the main Muslim Mughal city of Sirhind and, following the instructions of the Guru, punished all the culprits. Soon after the invasion of Sirhind, while resting in his chamber after the Rehras prayer Guru Gobind Singh was stabbed by a Pathan assassin hired by Mughals. Gobind Singh killed the attacker with his sword. Though a European surgeon stitched the Guru's wound, the wound re-opened as the Guru tugged at a hard strong bow after a few days, causing profuse bleeding that led to Gobind Singh's death.

After the Guru's death, Baba Banda Singh Bahadur became the commander-in-chief of the Khalsa. He organised the civilian rebellion and abolished or halted the Zamindari system in time he was active and gave the farmers proprietorship of their own land. Banda Singh was executed by the emperor Farrukh Siyar after refusing the offer of a pardon if he converted to Islam. The confederacy of Sikh warrior bands known as "misls" emerged, but these fought between themselves. Ranjit Singh achieved a series of military victories and created a Sikh Empire in 1799.

The Sikh empire had its capital in Lahore, spread over almost comprising what is now northwestern Indian subcontinent. The Sikh Empire entered into a treaty with the colonial British powers, with each side recognizing Sutlej River as the line of control and agreeing not to invade the other side. Ranjit Singh's most lasting legacy was the restoration and expansion of the Harmandir Sahib, most revered Gurudwara of the Sikhs, with marble and gold, from which the popular name of the "Golden Temple" is derived. After the death of Ranjit Singh in 1839, the Sikh Empire fell into disorder. Ranjit Singh had failed to establish a lasting structure for Sikh government or stable succession, and the Sikh Empire rapidly declined after his death. Factions divided the Sikhs, and led to Anglo-Sikh wars. The British easily defeated the confused and demoralised Khalsa forces, then disbanded them into destitution. The youngest son of Ranjit Singh named Duleep Singh ultimately succeeded, but he was arrested and exiled after the defeat of Sikh Khalsa.

The Singh Sabha movement, a movement to revitalize Sikhism, also saw the resurgence of the Khalsa after their defeat by the British in the Anglo-Sikh wars, and the subsequent decline and corruption of Sikh institutions during colonial rule, and the proselytization of other faith groups in the Punjab. It was started in the 1870s, and after a period of interfactional rivalry, united under the Tat Khalsa to reinvigorate Sikh practice and institutions.

The last Maharaja of the Sikh Empire Duleep Singh converted to Christianity in 1853, a controversial but influential event in Sikh history. Along with his conversion, and after Sikh Empire had been dissolved and the region made a part of the colonial British Empire, proselytising activities of Christians, Brahmo Samajis, Arya Samaj, Muslim Anjuman-i-Islamia and Ahmadiyah sought to convert the Sikhs in northwestern Indian subcontinent into their respective faiths. These developments launched the Singh Sabha Movement.

The first meeting of the movement was in the Golden Temple, Amritsar in 1873, and it was largely launched by the Sanatan Sikhs, Gianis, priests, and granthis. Shortly thereafter, Nihang Sikhs began influencing the movement, followed by a sustained campaign by the Tat Khalsa, which had quickly gained dominance by the early 1880s. The movement became a struggle between Sanatan Sikhs and Tat Khalsa in defining and interpreting Sikhism.

Sanatan Sikhs led by Khem Singh Bedi – who claimed to be a direct descendant of Guru Nanak, Avtar Singh Vahiria and others supported a more inclusive approach which considered Sikhism as a reformed tradition of Hinduism, while Tat Khalsa campaigned for an exclusive approach to the Sikh identity, disagreeing with Sanatan Sikhs and seeking to modernize Sikhism. The Sikh Sabha movement expanded in north and northwest Indian subcontinent, leading to more than 100 Singh Sabhas. By the early decades of the 20th century, the influence of Tat Khalsa increased in interpreting the nature of Sikhism and their control over the Sikh Gurdwaras. The Tat Khalsa banished Brahmanical practices including the use of the "yagna" fire, replaced by the "Anand Karaj" marriage ceremony in accordance with Sikh scripture, and the idols and the images of Sikh Gurus from the Golden Temple in 1905, traditions which had taken root during the administration of the "mahants" during the 1800s. They undertook a sustained campaign to standardize how Sikh Gurdwaras looked and ran, while looking to Sikh scriptures and the early Sikh tradition to purify the Sikh identity.

The spiritual successors of the Singh Sabha include the Akali movement of the 1920s, as well as the modern-day Shiromani Gurdwara Parbandhak Committee (SGPC), a gurdwara administration body, and the Akali Dal political party.

Sikhs participated and contributed to the decades-long Indian independence movement from the colonial rule in the first half of the 20th century. Ultimately when the British Empire recognized independent India, the land was partitioned into Hindu majority India and Muslim majority Pakistan (East and West) in 1947. This event, states Banga, was a watershed event in Sikh history. The Sikhs had historically lived in northwestern region of Indian subcontinent on both sides of the partition line ("Radcliffe Line"). According to Banga and other scholars, the Sikhs had strongly opposed the Muslim League demands and saw it as "perpetuation of Muslim domination" and anti-Sikh policies in what just a hundred years before was a part of the Sikh Empire. As such, Sikh organizations, including the Chief Khalsa Dewan and Shiromani Akali Dal led by Master Tara Singh, condemned the Lahore Resolution and the movement to create Pakistan, viewing it as inviting possible persecution; the Sikhs largely thus strongly opposed the partition of India. During the discussions with the colonial authorities, Tara Singh emerged as an important leader who campaigned to prevent the partition of colonial India and for the recognition of Sikhs as the third community. In 1940, a few Sikhs such as the victims of Komagata Maru in Canada proposed the idea of Khalistan as a buffer state between Pakistan and India. These leaders, however, were largely ignored. Many other Sikh leaders supported the partition along religious and demographic lines.

When partition was announced, the newly created line divided the Sikh population into two halves. The Sikhs suffered organized violence and riots against them in West Pakistan, and Sikhs moved en masse to the Indian side leaving behind their property and the sacred places of Sikhism. This reprisals on Sikhs were not one sided, because as Sikhs entered the Indian side, the Muslims in East Punjab experienced reprisals and they moved to West Pakistan. Before the partition, Sikhs constituted about 15% of the population in West Punjab that became a part of Pakistan, the majority being Muslims (55%). The Sikhs were the economic elite and wealthiest in West Punjab, with them having the largest representation in West Punjab's aristocracy, nearly 700 Gurdwaras and 400 educational institutions that served the interests of the Sikhs. Prior to the partition, there were a series of disputes between the majority Muslims and minority Sikhs, such as on the matters of jhatka versus halal meat, the disputed ownership of Gurdwara Sahidganj in Lahore which Muslims sought as a mosque and Sikhs as a Gurdwara, and the insistence of the provincial Muslim government in switching from Indian Gurmukhi script to Arabic-Persian Nastaliq script in schools. During and after the Simla Conference in June 1945, headed by Lord Wavell, the Sikh leaders initially expressed their desire to be recognized as the third party, but ultimately relegated their demands and sought a United India where Sikhs, Hindus and Muslims would live together, under a Swiss style constitution. The Muslim League rejected this approach, demanding that entire Punjab should be granted to Pakistan. The Sikh leaders then sought the partition instead, and Congress Working Committee passed a resolution in support of partitioning Punjab and Bengal.
Between March and August 1947, a series of riots, arson, plunder of Sikh property, assassination of Sikh leaders, and killings in Jhelum districts, Rawalpindi, Attock and other places made Tara Singh call the situation in Punjab as "civil war", while Lord Mountbatten stated "civil war preparations were going on". The riots had triggered the early waves of migration in April, with some 20,000 people leaving northwest Punjab and moving to Patiala. In Rawalpindi, 40,000 people became homeless. The Sikh leaders made desperate petitions, but all religious communities were suffering in the political turmoil. Sikhs, states Banga, were "only 4 million out of a total of 28 million in Punjab, and 6 million out of nearly 400 million in India; they did not constitute the majority, not even in a single district".

When the partition line was formally announced in August 1947, the violence was unprecedented, with Sikhs being one of the most affected religious community both in terms of deaths, as well as property loss, injury, trauma and disruption. Sikhs and Muslims were both victims and perpetrators of retaliatory violence against each other. Estimates range between 200,000 and 2 million deaths of Sikhs, Hindus and Muslims. There were numerous rapes of and mass suicides by Sikh women, they being taken captives, their rescues and above all a mass exodus of Sikhs from newly created Pakistan into newly created India. The partition created the "largest foot convoy of refugees recorded in [human] history, stretching over 100 kilometer long", states Banga, with nearly 300,000 people consisting of mostly "distraught, suffering, injured and angry Sikhs". Sikh and Hindu refugees from Pakistan flooded into India, Muslim refugees from India flooded into Pakistan, each into their new homeland.

The early 1980s witnessed some Sikh groups seeking an independent nation named Khalistan carved out from India and Pakistan. The Golden Temple and Akal Takht were occupied by various militant groups in 1982. These included the Dharam Yudh Morcha led by Jarnail Singh Bhindranwale, the Babbar Khalsa, the AISSF and the National Council of Khalistan. Between 1982 and 1983, there were Anandpur Resolution demand-related terrorist attacks against civilians in parts of India. By late 1983, the Bhindranwale led group had begun to build bunkers and observations posts in and around the Golden Temple, with militants involved in weapons training. In June 1984, the then Prime Minister of India Indira Gandhi ordered Indian Army to begin Operation Blue Star against the militants. The fierce engagement took place in the precincts of Darbar Sahib and resulted in many deaths, including Bhindranwale, the destruction of the Sikh Reference Library, which was considered a national treasure that contained over a thousand rare manuscripts, and destroyed Akal Takht. Numerous soldiers, civilians and militants died in the cross fire. Within days of the Operation Bluestar, some 2,000 Sikh soldiers in India mutinied and attempted to reach Amritsar to liberate the Golden Temple. Within six months, on 31 October 1984, Indira Gandhi's Sikh bodyguards assassinated her. The assassination triggered the 1984 anti-Sikh riots. According to Donald Horowitz, while anti-Sikh riots led to much damage and deaths, many serious provocations by militants also failed to trigger ethnic violence in many cases throughout the 1980s. The Sikhs and their neighbors, for most part, ignored attempts to provoke riots and communal strife.

Estimates state that Sikhism has some 25 million followers worldwide. According to Pew Research, a religion demographics and research group in Washington DC, "more than nine-in-ten Sikhs are in India, but there are also sizable Sikh communities in the United States, the United kingdom and Canada." Within India, the Sikh population is found in every state and union territory, but it is predominantly found in the northwestern and northern states. Only in the state of Punjab, Sikhs constitute a majority (58% of the total, per 2011 census). The states and union territories of India where Sikhs constitute more than 1.5% of its population are Punjab, Chandigarh, Haryana, Delhi, Uttarakhand and Jammu & Kashmir. Forming 4.7% of the total population, the western Canadian province of British Columbia is home to over 200,000 Sikhs and is the only province or state in the world outside India with Sikhism as the second most followed religion among the population.

Sikhism was founded in northwestern region of the Indian subcontinent in what is now Pakistan. Some of the Gurus were born near Lahore and in other parts of Pakistan. Prior to 1947, in British India, millions of Sikhs lived in what later became Pakistan. During the partition, Sikhs and Hindus left the newly created Muslim-majority Pakistan and mostly moved to Hindu-majority India - with some moving to Muslim-majority Afghanistan), - while Muslims in India left and moved to Pakistan. According to 2017 news reports, only about 20,000 Sikhs remain in Pakistan and their population is dwindling (0.01% of its estimated 200 million population). The Sikhs in Pakistan, like others in the region, have been "rocked by an Islamist insurgency for more than a decade".

Sikh sects are sub-traditions within Sikhism that believe in an alternate lineage of Gurus, or have a different interpretation of the Sikh scriptures, or believe in following a living Guru, or other concepts that differ from the orthodox Khalsa Sikhs. The major historic sects of Sikhism, states Harjot Oberoi, have included Udasi, Nirmala, Nanakpanthi, Khalsa, Sahajdhari, Namdhari Kuka, Nirankari and Sarvaria.
The early Sikh sects were Udasis and Minas founded by Sri Chand – the elder son of Guru Nanak, and Prithi Chand – the elder son of Guru Ram Das respectively, in parallel to the official succession of the Sikh Gurus. Later on Ramraiya sect grew in Dehradun with the patronage of Aurangzeb. Many splintered Sikh communities formed during the Mughal Empire era. Some of these sects were financially and administratively supported by the Mughal rulers in the hopes of gaining a more favorable and compliant citizenry.

After the collapse of Mughal Empire, and particularly during the rule of Ranjit Singh, Udasi Sikhs protected Sikh shrines, preserved the Sikh scripture and rebuilt those that were desecrated or destroyed during the Muslim–Sikh wars. However, Udasi Sikhs kept idols and images inside these Sikh temples. In the 19th century, Namdharis and Nirankaris sects were formed in Sikhism, seeking to reform and return to what each believed was the pure form of Sikhism.

All these sects differ from Khalsa orthodox Sikhs in their beliefs and practices, such as continuing to solemnize their weddings around fire and being strictly vegetarian. Many accept the concept of living Gurus such as Guru Baba Dyal Singh. The Nirankari sect though unorthodox was influential in shaping the views of Tat Khalsa and the contemporary era Sikh beliefs and practices. Another significant Sikh sect of the 19th century was the Radhasoami movement in Punjab led by Baba Shiv Dyal. Other contemporary era Sikhs sects include the 3HO, formed in 1971, which exists outside India, particularly in North America and Europe.

According to Surinder Jodhka, the state of Punjab with a Sikh majority has the "largest proportion of scheduled caste population in India". Although decried by Sikhism, Sikhs have practiced a caste system. The system, along with untouchability, has been more common in rural parts of Punjab.The landowning dominant Sikh castes, states Jodhka, "have not shed all their prejudices against the lower castes or dalits; while dalits would be allowed entry into the village gurdwaras they would not be permitted to cook or serve langar." The Sikh dalits of Punjab have tried to build their own gurdwara, other local level institutions and sought better material circumstances and dignity. According to Jodhka, due to economic mobility in contemporary Punjab, castes no longer mean an inherited occupation nor are work relations tied to a single location.
In 1953, the government of India acceded to the demands of the Sikh leader, Master Tara Singh, to include Sikh dalit castes in the list of scheduled castes. In the Shiromani Gurdwara Prabandhak Committee, 20 of the 140 seats are reserved for low-caste Sikhs.

Over 60% of Sikhs belong to the Jat caste, which is an agrarian caste. Despite being very small in numbers, the Kshatriya Khatri and Arora castes wield considerable influence within the Sikh community. Other common Sikh castes include Sainis, Ramgarhias (artisans), Ahluwalias (formerly brewers), Kambojs (rural caste), Labanas, Kumhars and the two Dalit castes, known in Sikh terminology as the Mazhabis (the Chuhras) and the Ravidasias (the Chamars).

Sikhism is the fifth-largest amongst the major world religions, and one of the youngest. Worldwide, there are 25.8 million Sikhs, which makes up 0.39% of the world's population. Approximately 75% of Sikhs live in Punjab, where they constitute over 50% of the state's population. Large communities of Sikhs migrate to the neighboring states such as Indian State of Haryana which is home to the second largest Sikh population in India with 1.1 million Sikhs as per 2001 census, and large immigrant communities of Sikhs can be found across India. However, Sikhs only comprise about 2% of the Indian population.

Sikh migration to Canada began in the 19th century and led to the creation of significant Sikh communities, predominantly in South Vancouver, British Columbia, Surrey, British Columbia, and Brampton, Ontario. Today temples, newspapers, radio stations, and markets cater to these large, multi-generational Indo-Canadian groups. Sikh festivals such as Vaisakhi and Bandi Chhor are celebrated in those Canadian cities by the largest groups of followers in the world outside the Punjab.

Sikhs also migrated to East Africa, West Africa, the Middle East, Southeast Asia, the United Kingdom, the United States, and Australia. These communities developed as Sikhs migrated out of Punjab to fill in gaps in imperial labour markets. In the early twentieth century a significant community began to take shape on the west coast of the United States. Smaller populations of Sikhs are found within many countries in Western Europe, Mauritius, Malaysia, Philippines, Fiji, Nepal, China, Pakistan, Afghanistan, Iran, Singapore, United States, and many other countries.

Some major prohibitions include:




</doc>
<doc id="27969" url="https://en.wikipedia.org/wiki?curid=27969" title="Structural isomer">
Structural isomer

In chemistry, a structural isomer (or constitutional isomer in the IUPAC nomenclature) of a compound is another compound whose molecule has the same number of atoms of each element, but with logically distinct bonds between them. The term metamer was formerly used for the same concept.

For example, butanol ––OH, methyl propyl ether ––O–, and diethyl ether (––)O have the same molecular formula but are three distinct structural isomers

The concept applies also to polyatomic ions with the same total charge. A classical example is the cyanate ion O=C=N and the fulminate ion C≡NO. It is also extended to ionic compounds, so that (for example) ammonium cyanate [] [O=C=N] and urea (–)C=O are considered structural isomers, and so are methylammonium formate [–] [] and ammonium acetate [] [–].

Structural isomerism is the most radical type of isomerism. It is opposed to stereoisomerism, in which the atoms and bonding scheme are the same, but only the relative spatial arrangement of the atoms is different. Examples of the latter are the enantiomers, whose molecules are mirror images of each other, and the "cis" and "trans" versions of 2-butene.

Among the structural isomers, one can distinguish several classes including skeletal isomers, positional isomers (or regioisomers), functional isomers, tautomers, and structural topoisomers.

A skeletal isomer of a compound is a structural isomer that differs from it in the atoms and bonds that are considered to comprise the "skeleton" of the molecule. For organic compounds, such as alkanes, that usually means the carbon atoms and the bonds between them.

For example, there are three skeletal isomers of pentane: "n"-pentane (often called simply "pentane"), isopentane (2-methylbutane) and neopentane (dimethylpropane).

If the skeleton is acyclic, as in the above example, one may use the term chain isomerism.

Position isomers (also positional isomers or regioisomers) are structural isomers that can be viewed as differing only on the position of a functional group, substituent, or some other feature on a "parent" structure. 

For example, replacement one of the 12 hydrogen atoms –H by a hydroxyl group –OH on the "n"-pentane parent molecule can give any of three different position isomers:

Functional isomers are structural isomers which have different functional groups, resulting in significantly different chemical and physical properties.

An example is the pair propanal HC–CH–C(=O)-H and acetone HC–C(=O)–CH: the first has a –C(=O)H functional group, which makes it an aldehyde, whereas the second has a C–C(=O)–C group, that makes it a ketone. 

Another example is the pair ethanol HC–CH–OH (an alcohol) and dimethyl ether HC–O–CHH (an ether). In contrast, 1-propanol and 2-propanol are structural isomers, but not functional isomers, since they have the same significant functional group (the hydroxyl –OH) and are both alcohols.

Besides the different chemistry, functional isomers typically have very different infrared spectra. The infrared spectrum is largely determined by the vibration modes of the molecule, and functional groups like hydroxyl and esters have very different vibration modes. Thus 1-propanol and 2-propanol have relatively similar infrared spectra because of the hydroxyl group, which are fairly different from that of methyl ethyl ether.

In chemistry, one usually ignores distinctions between isotopes of the same element. However, in some situations (for instance in Raman, NMR, or microwave spectroscopy) one may treat different isotopes of the same element as different elements. In the second case, two molecules with the same number of atoms of each isotope but distinct bonding schemes are said to be structural isotopomers.

Thus, for example, ethene would have no structural isomers under the first interpretation; but replacing two of the hydrogen atoms (H) by deuterium atoms (H) may yield any of two structural isotopomers (1,1-dideuteroethene and 1,2-dideuteroethene), if both carbon atoms are the same isotope. If, in addition, the two carbons are different isotopes (say, C and C), there would be three distinct structural isotopomers, sice 1-C-1,1-dideuteroethene would be different from 1-C-2,2-dideuteroethene.) And, in both cases, the 1,2-dideutero structural isotopomer would occur as two stereo isotopoomers, "cis" and "trans".

One says that two molecules (including polyatomic ions) A and B have the same structure if each atom of A can be paired with an atom of B of the same element, in a one-to-one way, so that for every bond in A there is a bond in B, of the same type, between corresponding atoms; and vice-versa. This requirement applies also complex bonds that involve three or more atoms, such as the delocalized bonding in the benzene molecule and other aromatic compounds. 

Depending on the context, one may require that each atom be paired with an atom of the same isotope, not just of the same element.

Two molecules then can be said to be structural isomers (or, if isotopes matter, structural isotopomers) if they have the same molecular formula but do not have the same structure.

Structural symmetry of a molecule can be defined mathematically as a permutation of the atoms that exchanges at least two atoms but does not change the molecule's structure. Two atoms then can be said to be structurally equivalent if there is a structural symmetry that takes one to the other. 

Thus, for example, all four hydrogen atoms of methane are structurally equivalent, because any permutation of them will preserve all the bonds of the molecule.

Likewise, all six hydrogens of ethane () are structurally equivalent to each other, as are the two carbons; because any hydrogen can be switched with any other, either by a permutation that swaps just those two atoms, or by a permutation that swaps the two carbons and each hydrogen in one methyl group with a different hydrogen on the other methyl. Either operation preserves the structure of the molecule. That is the case also for the hydrogen atoms cyclopentane, allene, 2-butyne, hexamethylenetetramine, prismane, cubane, dodecahedrane, etc.

On the other hand, the hydrogen atoms of propane are not all structurally equivalent. The six hydrogens attached to the first an third carbons are equivalent, as in ethane, and the two attached to the middle carbon are equivalent to each other; but there is no equivalence between these two equivalence classes.

Structural equivalences between atoms of a parent molecule reduce the number of positional isomers that can be obtained by replacing those atoms for a different element or group. Thus, for example, the structural equivalence between the six hydrogens of ethane means that there is just one structural isomer of ethanol, not 6. The eight hydrogens of propane are partitioned into two structural equivalence classes (the six on the methyl groups, and the two on the central carbon); therefore there are only two positional isomers of propanol (1-propanol and 2-propanol). Likewise there are only two positional isomers of butanol, and three of pentanol or hexanol.

Once a substitution is made on a parent molecule, its structural symmetry is usually reduced, meaning that atoms that were formerly equivalent may no longer be so. Thus substitution of two or more equivalent atoms by the same element may generate more than one positional isomer. 

The classical example is the derivatives of benzene. Its six hydrogens are all structurally equivalent, and so are the six carbons; because the structure is not changed if the atoms are permuted in ways that correspond to flipping the molecule over or rotating it by multiples of 60 degrees. Therefore, replacing any hydrogen by chlorine yields only one chlorobenzene. However, with that replacement, the atom permutations that moved that hydrogen are no longer valid. Only one permutation remains, that corresponds to flipping the molecule over while keeping the chlorine fixed. The five remaining hydrogens then fall into three different equivalence classes: the one opposite to the chlorine is a class by itself (called the "para" position), the two closest to the chlorine form another class ("ortho"), and the remaining two are the third class ("meta"). Thus a second substitution of hydrogen by chlorine can yield three positional isomers: 1,2- or "ortho"-, 1,3- or "meta"-, and 1,4- or "para"-dichlorobenzene. 

For the same reason, there is only one phenol (hydroxybenzene), but three benzenediols; and one toluene (methylbenzene), but three toluols, and three xylenes.

On the other hand, the second replacement (by the same substituent) may preserve or even increase the symmetry of the molecule, and thus may preserve or reduce the number of equivalence classes for the next replacement. Thus, the four remaining hydrogens in "meta"-dichlorobenzene still fall into three classes, while those of "ortho"- fall into two, and those of "para"- are all equivalent again. Still, some of these 3 + 2 + 1 = 6 substitutions end up yielding the same structure, so there are only three structurally distinct trichlorobenzenes: 1,2,3-, 1,2,4-, and 1,3,5-.

If the substituents at each step are different, there will usually be more structural isomers. Xylenol, which is benzene with one hydroxyl substituent and two methyl substituents, has a total of 6 isomers:

Enumerating or counting structural isomers in general is a difficult problem, since one must take into account several bond types (including delocalized ones), cyclic structures, and structures that cannot possibly be realized due to valence or geometric constraints, and non-separable tautomers.

For example, there are nine structural isomers with molecular formula CHO having different bond connectivities. Seven of them are air-stable at room temperature, and these are given in the table below. 

Two structural isomers are the enol tautomers of the carbonyl isomers (propionaldehyde and acetone), but these are not stable.




</doc>
<doc id="27970" url="https://en.wikipedia.org/wiki?curid=27970" title="Stereoisomerism">
Stereoisomerism

In stereochemistry, stereoisomerism, or spatial isomerism, is a form of isomerism in which molecules have the same molecular formula and sequence of bonded atoms (constitution), but differ in the three-dimensional orientations of their atoms in space. This contrasts with structural isomers, which share the same molecular formula, but the bond connections or their order differs. By definition, molecules that are stereoisomers of each other represent the same structural isomer.

Enantiomers, also known as optical isomers, are two stereoisomers that are related to each other by a reflection: they are mirror images of each other that are non-superposable. Human hands are a macroscopic analog of this. Every stereogenic center in one has the opposite configuration in the other. Two compounds that are enantiomers of each other have the same physical properties, except for the direction in which they rotate polarized light and how they interact with different optical isomers of other compounds. As a result, different enantiomers of a compound may have substantially different biological effects. Pure enantiomers also exhibit the phenomenon of optical activity and can be separated only with the use of a chiral agent. In nature, only one enantiomer of most chiral biological compounds, such as amino acids (except glycine, which is achiral), is present. An optically active compound shows two forms: -(+) form and -(−) form.

Diastereomers are stereoisomers not related through a reflection operation. They are not mirror images of each other. These include meso compounds, "cis"–"trans" isomers, E-Z isomers, and non-enantiomeric optical isomers. Diastereomers seldom have the same physical properties. In the example shown below, the meso form of tartaric acid forms a diastereomeric pair with both levo and dextro tartaric acids, which form an enantiomeric pair.

The - and - labeling of the isomers above is not the same as the "d"- and "l"- labeling more commonly seen, explaining why these may appear reversed to those familiar with only the latter naming convention.

Stereoisomerism about double bonds arises because rotation about the double bond is restricted, keeping the substituents fixed relative to each other. If the two substituents on at least one end of a double bond are the same, then there is no stereoisomer and the double bond is not a stereocenter, e.g. propene, CHCH=CH where the two substituents at one end are both H.

Traditionally, double bond stereochemistry was described as either "cis" (Latin, on this side) or "trans" (Latin, across), in reference to the relative position of substituents on either side of a double bond. The simplest examples of "cis"-"trans" isomerism are the 1,2-disubstituted ethenes, like the dichloroethene (CHCl) isomers shown below.

Molecule I is "cis"-1,2-dichloroethene and molecule II is "trans"-1,2-dichloroethene. Due to occasional ambiguity, IUPAC adopted a more rigorous system wherein the substituents at each end of the double bond are assigned priority based on their atomic number. If the high-priority substituents are on the same side of the bond, it is assigned Z (Ger. "zusammen", together). If they are on opposite sides, it is E (Ger. "entgegen", opposite). Since chlorine has a larger atomic number than hydrogen, it is the highest-priority group. Using this notation to name the above pictured molecules, molecule I is (Z)-1,2-dichloroethene and molecule II is (E)-1,2-dichloroethene. It is not the case that Z and "cis" or E and "trans" are always interchangeable. Consider the following fluoromethylpentene:

The proper name for this molecule is either "trans"-2-fluoro-3-methylpent-2-ene because the alkyl groups that form the backbone chain (i.e., methyl and ethyl) reside across the double bond from each other, or (Z)-2-fluoro-3-methylpent-2-ene because the highest-priority groups on each side of the double bond are on the same side of the double bond. Fluoro is the highest-priority group on the left side of the double bond, and ethyl is the highest-priority group on the right side of the molecule.

The terms "cis" and "trans" are also used to describe the relative position of two substituents on a ring; "cis" if on the same side, otherwise "trans".

Conformational isomerism is a form of isomerism that describes the phenomenon of molecules with the same structural formula but with different shapes due to rotations about one or more bonds. Different conformations can have different energies, can usually interconvert, and are very rarely isolatable. For example, cyclohexane can exist in a variety of different conformations including a chair conformation and a boat conformation, but, for cyclohexane itself, these can never be separated. The boat conformation represents the energy maximum on a conformational itinerary between the two equivalent chair forms; however, it does not represent the transition state for this process, because there are lower-energy pathways.

There are some molecules that can be isolated in several conformations, due to the large energy barriers between different conformations. 2,2',6,6'-Tetrasubstituted biphenyls can fit into this latter category.

Anomerism is an identity for single bonded ring structures where "cis" or "E" and "trans" or "Z" (geometric isomerism) needs to name the substitutions on a carbon atom that also displays the identity of chirality; so anomers have carbon atoms that have geometric isomerism and optical isomerism (Enantiomerism) on one or more of the carbons of the ring. Anomers are named "alpha" or "axial" and "beta" or "equatorial" when substituting a cyclic ring structure that has single bonds between the carbon atoms of the ring for example, a hydroxyl group, a methyl hydroxyl group, a methoxy group or another pyranose or furanose group which are typical single bond substitutions but not limited to these. Axial geometric isomerism will be perpendicular (90 degrees) to a reference plane and equatorial will be 120 degrees away from the axial bond or deviate 30 degrees from the reference plane.

Atropisomers are stereoisomers resulting from hindered rotation about single bonds where the steric strain barrier to rotation is high enough to allow for the isolation of the conformers.


Le Bel-van't Hoff rule states that for a structure with "n" asymmetric carbon atoms, there is a maximum of 2 different stereoisomers possible. As an example, -glucose is an aldohexose and has the formula CHO. Four of its six carbon atoms are stereogenic, which means -glucose is one of 2=16 possible stereoisomers.


</doc>
<doc id="27972" url="https://en.wikipedia.org/wiki?curid=27972" title="Sylvia Sayer">
Sylvia Sayer

Sylvia Olive Pleadwell Sayer, Lady Sayer (6 March 1904 – 4 January 2000), was a passionate conservationist and environmental campaigner on behalf of Dartmoor, an area of mostly granite moorland in Devon in the south-west of England. She was chairman of the Dartmoor Preservation Association from 1951 to 1973, and remained deeply involved with the organisation until her death.

Sayer's grandfather was Robert Burnard (1848–1920), who with Sabine Baring-Gould performed the first scientific excavations of ancient monuments on Dartmoor, including Grimspound; and who was one of the founding members in 1883 of the Dartmoor Preservation Association. He leased Huccaby House, on the West Dart River, near Hexworthy, from the Duchy of Cornwall and Sayer used to visit as a child.

Her mother was Olive Louise Munday (born Burnard; 1873–1960), Robert Burnard's eldest daughter. Her father was the Principal Medical Officer at the Naval Hospital School in Greenwich. She attended Princess Helena College in Ealing, and then the Central School of Art in London. In 1925 she married Guy Sayer, who was a midshipman in the Royal Navy, and they spent some time in China. Three years later they bought Old Middle Cator, a dilapidated Dartmoor longhouse about two miles west of the village of Widecombe-in-the-Moor in Dartmoor. They had twin sons, Geoffrey and Oliver, born in 1930, and until World War Two the family travelled widely to meet the needs of Guy's navy career. After VE Day, Guy was posted to the Far East and Sylvia settled at Cator and became interested in local politics, at first as a parish councillor for Widecombe, then as a Rural District Councillor and a member of the Dartmoor Sub-Committee of Devon County Council.

Lady Sayer acquired her title in 1959 when her husband was knighted on his retirement as the vice-admiral commanding the Reserve Fleet. After his retirement he spent much of his time helping his wife with her conservation work. She was chairman of the Dartmoor Preservation Association between 1951 and 1973, and after that, as its patron, she continued to attend virtually every meeting of its executive committee until 1999.

She lived at Cator almost until her death, moving to a nursing home in Chagford two weeks before. On 10 February 2000 a service of celebration for her life was held in the parish church of Widecombe-in-the-Moor. It was attended by over 300 people, including representatives of the Dartmoor National Park Authority, the Association of National Park Authorities, the Council for National Parks, the Campaign to Protect Rural England (CPRE), the Ramblers' Association, and the Duchy of Cornwall.

Sayer was described in "The Times" newspaper in 1971 as "a militant conservationist, who is a full-time thorn in the sides of those authorities and others who want to flood, fence, dig up, knock down and otherwise damage the Dartmoor national park." Crispin Gill wrote about her in his introduction to "Dartmoor – A New Study" published in 1970 as having "roused the conscience of a [vast] number of people" and he described her as an indefatigable worker with an enormous knowledge; he also referred to Henry Slesser's description of her as "the shield of the moor".

She regularly wrote letters to newspapers, both local and national, about matters related to Dartmoor. In her first published letter to "The Times", in 1948, she expressed concerns about local authorities (specifically Devon County Council) seeking to subvert the implementation of Arthur Hobhouse's recommendations for the creation of national parks by demanding that they retain their own planning powers. She noted that local authorities had been unable to control development by Government departments in areas such as Dartmoor, referring to the 32,800 acres held by the Admiralty and War Department and the 3,763 acres that had been taken by the Forestry Commission. She also referred to Dartmoor's uniqueness in that most of it was owned by the Duchy of Cornwall which, as a department of the Crown, could basically do what it liked with its land. She urged that control of the soon-to-be-formed National Parks should be at the highest possible level within the Government so there would be a chance of exercising control over the Duchy and other Government departments.

The National Parks and Access to the Countryside Act 1949 created the National Parks Commission whose first chairman was Sir Patrick Duff. Ten National Parks were created in the 1950s under this Act – Dartmoor National Park was the fourth to be created, in October 1951. It was administered by Dartmoor National Park Authority which was a special committee of Devon County Council and subsidiary to the County Planning Committee which could veto its recommendations. Sayer was a member of the committee from its formation, but she resigned in 1957 in protest at its failure to protect the moor as she would wish.

As chairman of the Dartmoor Preservation Association (DPA), Sayer was heavily involved in all that organisation's fights for what it saw as conservation issues. The first of these was against the proposed installation of a television transmitting mast on North Hessary Tor in the centre of the moor. When the Dartmoor Standing Committee voted in June 1952 to approve the application, Sayer complained that it had relied on the casting vote of the chairman in the absence of three members who would have voted against.

Continued objection from Sayer and the DPA, and the CPRE, led to a public enquiry which took place in September 1953. Sir Patrick Duff, the National Parks Commission chairman, was well briefed by Sayer and at the enquiry his case was mainly based on the damage the mast would do to the scenery of the moor. Although congratulatory letters were passed between all the main objectors after the enquiry, the ministry granted the planning application in January 1954, though with some minor provisos to minimise the impact. Although Duff had failed to stop the installation of the mast, Sayer rewarded him for his efforts with a painting of North Hessary Tor saying it was "almost the last representation of that landscape that can be made while it is still unshadowed and unspoiled".

From 1955 onwards Sayer kept up a correspondence about the military roads that lead across the northern moor from Okehampton Camp.

In 1966 she and her husband deliberately interrupted live-firing exercises on Dartmoor's Royal Marines firing range to inspect and photograph any damage done to a prehistoric stone row. In February 1967 she disrupted a large-scale mock battle at Ringmoor Down that involved low-flying helicopters. She told the press that she did this to exercise her rights and to ensure that no damage was caused to ancient monuments, pointing out that the public could not be excluded from the area involved because it was not a firing area, and that sheep and ponies had been frightened away as the helicopters converged – "it could have been pony trekkers and hikers and might have resulted in a serious accident", she said.
In the late 1960s and early 1970s she was involved, as DPA chairman, with the disputes over the proposed construction of two new reservoirs on Dartmoor. The largest, which was to supply Plymouth, was known as "Swincombe" after the small River Swincombe that flows through Foxtor Mires, the proposed site of the reservoir. The proposal was eventually rejected in December 1970 at the Bill's committee stage, and a reservoir known as Roadford Lake was built west of the moor near the village of Broadwoodwidger instead.

However, the other reservoir at Meldon on the north west edge of the moor was passed, despite claims that the water would be poisoned by arsenic and lead because of the presence of three disused metalliferous mines and their spoil heaps in the area to be flooded. The dam was built in 1972, and in that year Sayer wrote a 62-page booklet entitled "The Meldon Story" that was published by the DPA. After expounding at length all the arguments made against building a dam at Meldon and in favour of an alternative site at Gorhuish, and the responses from the establishment, it ended with this statement:

Following these efforts, she concentrated on the two companies involved in the extraction of china clay in the south west of the moor. They had permission dating from 1951 to expand their pits and tips. Shaugh Moor is an adjacent area that is rich in ancient monuments and it was there that the companies planned to tip the vast quantities of spoil that is generated from clay extraction. At the time the area became known as "Area Y", from an explanatory diagram that Sayer had drawn. The activism culminated in an adjournment debate in the House of Commons in which Janet Fookes, a Plymouth MP, argued against irreparably damaging the ancient landscape. In June 1978, the two companies agreed to share their waste tips, as Sayer had recommended, saving Shaugh Moor.

In the early 1980s there were plans to create a bypass for the A30 road around the town of Okehampton on the northern edge of Dartmoor. Two alternative routes were proposed: a northern one through agricultural land, or a southern one which would encroach on the National Park. After a public enquiry was held in 1980 arguments continued for over five years with Sayer vigorously opposing the route through the moor. The matter was finally settled when the southern route was approved in December 1985 by the House of Lords. After the decision had been made, Sayer wrote a letter to Peter Bottomley, the then Minister of Transport that included the following extract:

She opposed proposals to build a new Dartmoor Prison at Princetown in the centre of the moor in 1959. In the 1960s she complained about off-road car parking, and the poor treatment of Dartmoor ponies by those who only keep them for the subsidies they can obtain.

In 1983 she refused an invitation from the Prince of Wales to attend the launch of the Duchy of Cornwall's management plan for Dartmoor, since it allowed for a continuance of military usage. She was also one of a deputation who met the Prince in 1990 to explain to him why they thought he should not renew the military licences for a further term. However, the licences were renewed that year until 2011.

The DPA set up a Lady Sayer Land Purchase Fund after her retirement as chairman in 1973. It was used in 1984 to purchase 32 acres of land at Sharpitor, near Burrator Reservoir, in celebration of the successful fight against the Swincombe reservoir. As of March 2013 the fund held about £29,400.

On the centenary of her birth in 2004 John Bainbridge, the then chief executive of the DPA, revealed plans to memorialise Sayer by organising annual walks to some part of Dartmoor that she had saved, and also by holding an annual Sylvia Sayer lecture given by a prominent speaker.

Writing in 2009 in "Dartmoor – A Statement of its Time", one of the New Naturalist series of books, Professor Ian Mercer (former Chief Officer of the Dartmoor National Park Authority), said of Sayer:


</doc>
<doc id="27977" url="https://en.wikipedia.org/wiki?curid=27977" title="South Park">
South Park

South Park is an American animated sitcom created by Trey Parker and Matt Stone and developed by Brian Graden for Comedy Central. The series revolves around four boys—Stan Marsh, Kyle Broflovski, Eric Cartman, and Kenny McCormick—and their exploits in and around the titular Colorado town. The show became infamous for its profanity and dark, surreal humor that satirizes a wide range of topics towards a mature audience.

Parker and Stone developed the show from "The Spirit of Christmas", two consecutive animated shorts. The latter became one of the first Internet viral videos, ultimately leading to "South Park"s production. The pilot episode was produced using cutout animation, leading to all subsequent episodes being produced with computer animation that emulated the cutout technique. "South Park" features a very large ensemble cast of recurring characters.

Since its debut on August 13, 1997, episodes of "South Park" have been broadcast. It debuted with great success, consistently earning the highest ratings of any basic cable program. Subsequent ratings have varied but it remains one of Comedy Central's highest-rated shows and is slated to air new episodes through 2022. Since 2000, each episode has typically been written and produced in the week preceding its broadcast, with Parker serving as the primary writer and director. The show's twenty-third season premiered on September 25, 2019.

"South Park" has received numerous accolades, including five Primetime Emmy Awards, a Peabody Award, and numerous inclusions in various publications' lists of greatest television shows. The show's popularity resulted in a feature-length theatrical film, "" which was released in June 1999, less than two years after the show's premiere, and became a commercial and critical success, and garnered a nomination for an Academy Award. In 2013, "TV Guide" ranked "South Park" the tenth Greatest TV Cartoon of All Time.

The show follows the exploits of four boys, Stan Marsh, Kyle Broflovski, Eric Cartman and Kenny McCormick. The boys live in the fictional small town of South Park, located within the real-life South Park basin in the Rocky Mountains of central Colorado. The town is also home to an assortment of frequent characters such as students, families, elementary school staff, and other various residents, who tend to regard South Park as a bland, quiet place to live. Prominent settings on the show include the local elementary school, bus stop, various neighborhoods and the surrounding snowy landscape, actual Colorado landmarks, and the shops and businesses along the town's main street, all of which are based on the appearance of similar locations in Fairplay, Colorado.
Stan is portrayed as the everyman of the group, as the show's website describes him as an "average, American 4th grader". Kyle is the lone Jew among the group, and his portrayal in this role is often dealt with satirically. Stan is modeled after Parker, while Kyle is modeled after Stone. They are best friends, and their friendship, symbolically intended to reflect Parker and Stone's friendship, is a common topic throughout the series. Eric Cartman (usually nicknamed by his surname only) is loud, obnoxious, and amoral, often portrayed as an antagonist. His anti-Semitic attitude has resulted in a progressive rivalry with Kyle, although the deeper reason is the strong clash between Kyle's strong morality and Cartman's complete lack of such. Kenny, who comes from a poor family, wears his parka hood so tightly that it covers most of his face and muffles his speech. During the show's first five seasons, Kenny died in nearly every episode before returning in the next with little-to-no definitive explanation given. He was written out of the show's sixth season in 2002, re-appearing in the season finale. Since then, Kenny's death has been seldom used by the show's creators. During the show's first 58 episodes, the boys were in the third grade. In the season four episode "4th Grade" (2000), they entered the fourth grade, and have remained there ever since.

Plots are often set in motion by events, ranging from the fairly typical to the supernatural and extraordinary, which frequently happen in the town. The boys often act as the voice of reason when these events cause panic or incongruous behavior among the adult populace, who are customarily depicted as irrational, gullible, and prone to vociferation. The boys are also frequently confused by the contradictory and hypocritical behavior of their parents and other adults, and often perceive them as having distorted views on morality and society.

Each episode opens with a tongue-in-cheek all persons fictitious disclaimer: "All characters and events in this show—even those based on real people—are entirely fictional. All celebrity voices are impersonated...poorly. The following program contains coarse language and due to its content it should not be viewed by anyone."

"South Park" was the first weekly program to be rated TV-MA, and is generally intended for adult audiences. The boys and most other child characters use strong profanity, with only the most taboo words being bleeped during a typical broadcast. Parker and Stone perceive this as the manner in which real-life small boys speak when they are alone.

"South Park" commonly makes use of carnivalesque and absurdist techniques, numerous running gags, violence, sexual content, offhand pop-cultural references, and satirical portrayal of celebrities.

Early episodes tended to be shock value-oriented and featured more slapstick-style humor. While social satire had been used on the show occasionally earlier on, it became more prevalent as the series progressed, with the show retaining some of its focus on the boys' fondness of scatological humor in an attempt to remind adult viewers "what it was like to be eight years old." Parker and Stone also began further developing other characters by giving them larger roles in certain storylines, and began writing plots as parables based on religion, politics, and numerous other topics. This provided the opportunity for the show to spoof both extreme sides of contentious issues, while lampooning both liberal and conservative points of view. Parker and Stone describe themselves as "equal opportunity offenders", whose main purpose is to "be funny" and "make people laugh", while stating that no particular topic or group of people be exempt from mockery and satire.

Parker and Stone insist that the show is still more about "kids being kids" and "what it's like to be in [elementary school] in America", stating that the introduction of a more satirical element to the series was the result of the two adding more of a "moral center" to the show so that it would rely less on simply being crude and shocking in an attempt to maintain an audience. While profane, Parker notes that there is still an "underlying sweetness" aspect to the child characters, and "Time" described the boys as "sometimes cruel but with a core of innocence." Usually, the boys or other characters pondered over what transpired during an episode and conveyed the important lesson taken from it with a short monologue. During earlier seasons, this speech commonly began with a variation of the phrase "You know, I've learned something today...".

Parker and Stone met in film class at the University of Colorado in 1992 and discovered a shared love of Monty Python, which they often cite as one of their primary inspirations. They created an animated short entitled "The Spirit of Christmas". The film was created by animating construction paper cutouts with stop motion, and features prototypes of the main characters of "South Park", including a character resembling Cartman but named "Kenny", an unnamed character resembling what is today Kenny, and two near-identical unnamed characters who resemble Stan and Kyle. Fox Broadcasting Company executive and mutual friend Brian Graden commissioned Parker and Stone to create a second short film as a video Christmas card. Created in 1995, the second "The Spirit of Christmas" short resembled the style of the later series more closely. To differentiate between the two homonymous shorts, the first short is often referred to as "Jesus vs. Frosty", and the second short as "Jesus vs. Santa". Graden sent copies of the video to several of his friends, and from there it was copied and distributed, including on the internet, where it became one of the first viral videos.

As "Jesus vs. Santa" became more popular, Parker and Stone began talks of developing the short into a television series about four children residing in the fictional Colorado town of South Park. Fox eagerly agreed to meet with the duo about the show's premise, having pride itself with edgier products such as "Cops", "The Simpsons", and "The X-Files". However, during the meeting at the Fox office in Century City, disagreements between the two creators and the network began to arise, mainly over the latter's refusal to air a show that included a supporting talking stool character named Mr. Hankey. Some executives at 20th Century Fox Television (which was to produce the series) agreed with its then-sister network's stance on Mr. Hankey and repeatedly requested Parker and Stone to remove the character in order for the show to proceed. Refusing to their demands, the duo cut ties with Fox and its sister companies all together and began shipping the series somewhere else. 

The two then entered negotiations with both MTV and Comedy Central. Parker preferred the show be produced by Comedy Central, fearing that MTV would turn it into a kids show. When Comedy Central executive Doug Herzog watched the short, he commissioned for it to be developed into a series. Parker and Stone assembled a small staff and spent three months creating the pilot episode "Cartman Gets an Anal Probe". "South Park" was in danger of being canceled before it even aired when the show fared poorly with test audiences, particularly with women. However, the shorts were still gaining more popularity over the Internet, and Comedy Central ordered a run of six episodes. "South Park" debuted with "Cartman Gets an Anal Probe" on August 13, 1997.

Except for the pilot episode, which was produced using cutout animation, all episodes of "South Park" are created with the use of software, primarily Autodesk Maya. As opposed to the pilot, which took three months to complete, and other animated sitcoms, which are traditionally hand-drawn by companies in South Korea in a process that takes roughly eight to nine months, individual episodes of "South Park" take significantly less time to produce. Using computers as an animation method, the show's production staff were able to generate an episode in about three weeks during the first seasons. Now, with a staff of about 70 people, episodes are typically completed in one week, with some in as little as three to four days. Nearly the entire production of an episode is accomplished within one set of offices, which were originally at a complex in Westwood, Los Angeles, California and are now part of South Park Studios in Culver City, California. Parker and Stone have been the show's executive producers throughout its entire history. Debbie Liebling, who was Senior Vice President of original programming and development for Comedy Central, also served as an executive producer during the show's first five seasons, coordinating the show's production efforts between South Park Studios and Comedy Central's headquarters in New York City. During its early stages, finished episodes of "South Park" were hastily recorded to D-2 to be sent to Comedy Central for airing in just a few days' time.
Scripts are not written before a season begins. Production of an episode begins on a Thursday, with the show's writing consultants brainstorming with Parker and Stone. Former staff writers include Pam Brady, who has since written scripts for the films "Hot Rod", "Hamlet 2" and "" (with Parker and Stone), and Nancy Pimental, who served as co-host of "Win Ben Stein's Money" and wrote the film "The Sweetest Thing" after her tenure with the show during its first three seasons. Television producer and writer Norman Lear, an idol of both Parker and Stone, served as a guest writing consultant for the season seven (2003) episodes "Cancelled" and "I'm a Little Bit Country". During the 12th and 13th seasons, "Saturday Night Live" actor and writer Bill Hader served as a creative consultant and co-producer.

After exchanging ideas, Parker will write a script, and from there the entire team of animators, editors, technicians, and sound engineers will each typically work 100–120 hours in the ensuing week. Since the show's fourth season (2000), Parker has assumed most of the show's directorial duties, while Stone relinquished his share of the directing to focus on handling the coordination and business aspects of the production. On Wednesday, a completed episode is sent to Comedy Central's headquarters via satellite uplink, sometimes just a few hours before its air time of 10 PM Eastern Time.

Parker and Stone state that subjecting themselves to a one-week deadline creates more spontaneity amongst themselves in the creative process, which they feel results in a funnier show. The schedule also allows "South Park" to both stay more topical and respond more quickly to specific current events than other satiric animated shows. One of the earliest examples of this was in the season four (2000) episode "Quintuplets 2000", which references the United States Border Patrol's raid of a house during the Elián González affair, an event which occurred only four days before the episode originally aired. The season nine (2005) episode "Best Friends Forever" references the Terri Schiavo case, and originally aired in the midst of the controversy and less than 12 hours before she died. A scene in the season seven (2003) finale "It's Christmas in Canada" references the discovery of dictator Saddam Hussein in a "spider hole" and his subsequent capture, which happened a mere three days prior to the episode airing. The season 12 (2008) episode "About Last Night..." revolves around Barack Obama's victory in the 2008 presidential election, and aired less than 24 hours after Obama was declared the winner, using segments of dialogue from Obama's real victory speech.

On October 16, 2013, the show failed to meet their production deadline for the first time ever, after a power outage on October 15 at the production studio prevented the episode, season 17's "", from being finished in time. The episode was rescheduled to air a week later on October 23, 2013. In July 2015, "South Park" was renewed through 2019; extending the show through season 23 with 307 episodes overall. On September 12, 2019, the show was renewed for seasons 24 through 26 until 2022.

The show's style of animation is inspired by the paper cut-out cartoons made by Terry Gilliam for "Monty Python's Flying Circus", of which Parker and Stone have been lifelong fans. Construction paper and traditional stop motion cutout animation techniques were used in the original animated shorts and in the pilot episode. Subsequent episodes have been produced by computer animation, providing a similar look to the originals while requiring a fraction of the time to produce. Before computer artists begin animating an episode, a series of animatics drawn in Toon Boom are provided by the show's storyboard artists.

The characters and objects are composed of simple geometrical shapes and primary and secondary colors. Most child characters are the same size and shape, and are distinguished by their clothing, hair and skin colors, and headwear. Characters are mostly presented two-dimensionally and from only one angle. Their movements are animated in an intentionally jerky fashion, as they are purposely not offered the same free range of motion associated with hand-drawn characters. Occasionally, some non-fictional characters are depicted with photographic cutouts of their actual head and face in lieu of a face reminiscent of the show's traditional style. Canadians on the show are often portrayed in an even more minimalist fashion; they have simple beady eyes, and the top halves of their heads simply flap up and down when the characters speak.

When the show began using computers, the cardboard cutouts were scanned and re-drawn with CorelDRAW, then imported into PowerAnimator, which was used with SGI workstations to animate the characters. The workstations were linked to a 54-processor render farm that could render 10 to 15 shots an hour. Beginning with season five, the animators began using Maya instead of PowerAnimator. The studio now runs a 120-processor render farm that can produce 30 or more shots an hour.

PowerAnimator and Maya are high-end programs mainly used for 3D computer graphics, while co-producer and former animation director Eric Stough notes that PowerAnimator was initially chosen because its features helped animators retain the show's "homemade" look. PowerAnimator was also used for making some of the show's visual effects, which are now created using Motion, a newer graphics program created by Apple, Inc. for their Mac OS X operating system. The show's visual quality has improved in recent seasons, though several other techniques are used to intentionally preserve the cheap cutout animation look.

A few episodes feature sections of live-action footage, while others have incorporated other styles of animation. Portions of the season eight (2004) premiere "Good Times with Weapons" are done in anime style, while the season 10 episode "Make Love, Not Warcraft" is done partly in machinima. The season 12 episode "Major Boobage", a homage to the 1981 animated film "Heavy Metal", implements scenes accomplished with rotoscoping.

Parker and Stone voice most of the male "South Park" characters. Mary Kay Bergman voiced the majority of the female characters until her death in November 1999. Mona Marshall and Eliza Schneider succeeded Bergman, with Schneider leaving the show after its seventh season (2003). She was replaced by April Stewart, who, along with Marshall, continues to voice most of the female characters. Bergman was originally listed in the credits under the alias Shannen Cassidy to protect her reputation as the voice of several Disney and other kid-friendly characters. Stewart was originally credited under the name Gracie Lazar, while Schneider was sometimes credited under her rock opera performance pseudonym Blue Girl.

Other voice actors and members of "South Park"<nowiki>'s</nowiki> production staff have voiced minor characters for various episodes, while a few staff members voice recurring characters; supervising producer Jennifer Howell voices student Bebe Stevens, co-producer and storyboard artist Adrien Beard voices Token Black, who was the school's only African-American student until the introduction of Nichole in "Cartman Finds Love", writing consultant Vernon Chatman voices an anthropomorphic towel named Towelie, and production supervisor John Hansen voices Mr. Slave, the former gay lover of Mr. Garrison. Throughout the show's run, the voices for toddler and kindergarten characters have been provided by various small children of the show's production staff.

When voicing child characters, the voice actors speak within their normal vocal range while adding a childlike inflection. The recorded audio is then edited with Pro Tools, and the pitch is altered to make the voice sound more like that of a fourth grader.

Isaac Hayes voiced the character of Chef, an African-American, soul-singing cafeteria worker who was one of the few adults the boys consistently trusted. Hayes agreed to voice the character after being among Parker and Stone's ideal candidates, which also included Lou Rawls and Barry White. Hayes, who lived and hosted a radio show in New York during his tenure with "South Park", recorded his dialogue on a digital audio tape while a director gave directions over the phone, after which the tape would be shipped to the show's production studio in California. After Hayes left the show in early 2006, the character of Chef was killed off in the season 10 (2006) premiere "The Return of Chef".

Celebrities who are depicted on the show are usually impersonated, though some celebrities do their own voices for the show. Celebrities who have voiced themselves include Michael Buffer, Brent Musburger, Jay Leno, Robert Smith, and the bands Radiohead and Korn.
Comedy team Cheech & Chong voiced characters representing their likenesses for the season four (2000) episode "Cherokee Hair Tampons", which was the duo's first collaborative effort in 20 years. Malcolm McDowell appears in live-action sequences as the narrator of the season four episode "Pip".

Jennifer Aniston, Richard Belzer,
Natasha Henstridge, Norman Lear, and Peter Serafinowicz have guest starred as other speaking characters. During "South Park"<nowiki>'s</nowiki> earliest seasons, several high-profile celebrities inquired about guest-starring on the show. As a joke, Parker and Stone responded by offering low-profile, non-speaking roles, most of which were accepted; George Clooney provided the barks for Stan's dog Sparky in the season one (1997) episode "Big Gay Al's Big Gay Boat Ride", Leno provided the meows for Cartman's cat in the season one finale "Cartman's Mom Is a Dirty Slut", and Henry Winkler voiced the various growls and grunts of a kid-eating monster in the season two (1998) episode "City on the Edge of Forever". Jerry Seinfeld offered to lend his voice for the Thanksgiving episode "Starvin' Marvin", but declined to appear when he was only offered a role as "Turkey #2".

Parker says that the varying uses of music is of utmost importance to "South Park". Several characters often play or sing songs in order to change or influence a group's behavior, or to educate, motivate, or indoctrinate others. The show also frequently features scenes in which its characters have disapproving reactions to the performances of certain popular musicians.

Adam Berry, the show's original score composer, used sound synthesis to simulate a small orchestra, and frequently alluded to existing famous pieces of music. Berry also used signature acoustic guitar and mandolin cues as leitmotifs for the show's establishing shots. After Berry left in 2001, Jamie Dunlap and Scott Nickoley of the Los Angeles-based Mad City Production Studios provided the show's original music for the next seven seasons.. Since 2008, Dunlap has been credited as the show's sole score composer. Dunlap's contributions to the show are one of the few that are not achieved at the show's own production offices. Dunlap reads a script, creates a score using digital audio software, and then e-mails the audio file to South Park Studios, where it is edited to fit with the completed episode.

In addition to singing in an effort to explain something to the children, Chef would also sing about things relevant to what had transpired in the plot. These songs were original compositions written by Parker, and they were performed by Hayes in the same sexually suggestive R&B style he had used during his own music career. The band DVDA, which consists of Parker and Stone, along with show staff members Bruce Howell and D.A. Young, performed the music for these compositions and, until the character's death on the show, were listed as "Chef's Band" in the closing credits.

Rick James, Elton John, Meat Loaf, Joe Strummer, Ozzy Osbourne, Primus, Rancid, and Ween all guest starred and briefly performed in the season two (1998) episode "Chef Aid". Korn debuted their single "Falling Away from Me" as guest stars on the season three (1999) episode "Korn's Groovy Pirate Ghost Mystery".

The show's theme song was a musical score performed by the band Primus, with the lyrics alternately sung by the band's lead singer, Les Claypool, and the show's four central characters during the opening title sequence. Kenny's muffled lines are altered after every few seasons. His lines are usually sexually explicit in nature, such as his original lines, "I like girls with big fat titties, I like girls with deep vaginas". 
The original unaired opening composition was originally slower and had a length of 40 seconds. It was deemed too long for the opening sequence. So Parker and Stone sped up it for the show's opening, having the band's lead singer Claypool re-record his vocals. The instrumental version of the original composition, though, is often played during the show's closing credits and is wordless. 

The opening song played in the first four seasons (and the end credits in all seasons) has a folk rock instrumentation with bass guitar, trumpets and rhythmic drums. Its beat is fast in the opening and leisurely in the closing credits. It is in the minor key and it features a tritone or a diminished fifth, creating a melodic dissonance, which captures the show's surrealistic nature. In the latter parts of season 4 and season 5, the opening tune has an electro funk arrangement with pop qualities. Seasons 6-9 have a sprightly bluegrass instrumentation with a usage of banjo and is set in the major key. For the later seasons, the arrangement is electro rock with a breakbeat influence, which feature electric guitars backed up by synthesized, groovy drumbeats.

The opening theme song has been remixed three times during the course of the series, including a remix performed by Paul Robb. In 2006, the theme music was remixed with the song "Whamola" by Colonel Les Claypool's Fearless Flying Frog Brigade, from the album "Purple Onion".

Internationally, "South Park" is broadcast in India, New Zealand, and several countries throughout Europe and Latin America on channels that are subsidiaries of Comedy Central and ViacomCBS Domestic Media Networks, both subsidiaries of ViacomCBS. In distribution deals with Comedy Central, other independent networks also broadcast the series in other international markets. In Australia, the show is broadcast on The Comedy Channel, Comedy Central and free-to-air channel SBS Viceland (before 2009, it was aired on SBS). The series is broadcast uncensored in Canada in English on The Comedy Network and, later, Much. "South Park" also airs in Irish on TG4 in Ireland, STV in Scotland, Comedy Central and MTV in the UK (previously on Channel 4 and Viva, with 5Star recently picking up where Viva left off), B92 in Serbia, and on Game One and NRJ 12 in France.

Broadcast syndication rights to "South Park" were acquired by Debmar-Mercury and Tribune Entertainment in 2003 and 2004 respectively. Episodes further edited for content began running in syndication on September 19, 2005, and are aired in the United States with the TV-14 rating. 20th Television replaced Tribune as co-distributor in early 2008. The series is currently aired in syndication in 90 percent of the television markets across the U.S. and Canada, where it generates an estimated US$25 million a year in advertising revenue. In 2019, CBS Television Distribution (the syndication arm of ViacomCBS, the parent company of Comedy Central), took over the full distribution rights following the acquisition of 21st Century Fox (parent of 20th Television) by The Walt Disney Company (who had employed Debmar-Mercury founder Mort Marcus as the head of their syndication division), distributing the show in syndication and ViacomCBS airings.

Complete seasons of "South Park" have been regularly released on their entirety on DVD since 2002, with season twenty-one being the most recently released. Several other themed DVD compilations have been released by Rhino Entertainment and Comedy Central, while the three-episode "" story arc was reissued straight-to-DVD as a full-length feature in 2008. Blu-ray releases started in 2008 with the release of season twelve. Subsequent seasons have been released in this format alongside the longer-running DVD releases. The first eleven seasons were released on Blu-ray for the first time in December 2017.

In March 2008, Comedy Central made every episode of "South Park" available for free full-length on-demand legal streaming on the official South Park Studios website. From March 2008 until December 2013 new episodes were added to the site the day following their debut, and an uncensored version was posted the following day. The episode stayed up for the remainder of the week, then taken down, and added to the site three weeks later.

Within a week, the site served more than a million streams of full episodes, and the number grew to 55 million by October 2008. Legal issues prevent the U.S. content from being accessible outside the U.S., so local servers have been set up in other countries. In September 2009, a South Park Studios website with streaming episodes was launched in the UK and Ireland. In Canada, episodes were available for streaming from The Comedy Network's website, though due to digital rights restrictions, they are no longer available.

In July 2014 it was announced that Hulu had signed a three-year deal purchasing exclusive online streaming rights to the "South Park" for a reported 80 million dollars. Following the announcement every episode remained available for free on the South Park Studios website, using the Hulu player. As of September 2014, following the premiere of the eighteenth season, only 30 select episodes are featured for free viewing at a time on a rationing basis on the website, with new episodes being available for an entire month starting the day following their original airings. The entire series is available for viewing on Hulu.

In April 2010, the season five episode "Super Best Friends" and the season fourteen episodes "200" and "201" were removed from the site; additionally, these episodes no longer air in reruns and are only available exclusively on DVD. These episodes remain unavailable following the 2014 purchase by Hulu.

As of July 1, 2015, all episodes of "South Park" are available for streaming in Canada on the service CraveTV, which first consisted of seasons 1–18. Subsequent seasons were released the following July.

In early October 2019, industry rumors suggested that the streaming rights for "South Park" were being offered to various services, creating an intense bidding war that was estimated to be as high as . HBO and South Park Digital Studios announced that HBO had secured a multi-year deal for the exclusive streaming rights for "South Park" on their HBO Max service starting June 24, 2020. While the terms of the deal were not disclosed, "Variety" reported the deal feel between and .

From its debut in 1997 to the season twelve finale in 2008 the series had been natively produced in 480i standard definition. In 2009 the series switched to being natively produced in 1080i high definition with the beginning of the thirteenth season. Since this, all seasons originally produced in standard definition with 4:3 aspect ratio have been remastered by South Park Studios, being fully re-rendered in 1080i 16:9 high definition. The re-rendered versions were also released on Blu-ray. Several of the re-rendered episodes from the earlier seasons have their original uncensored audio tracks; they had previously been released in censored form.

The fifth-season episode "Super Best Friends", which was pulled from syndication and online streams following the controversy surrounding episode "201", was not released alongside the rest of the season when it was released in HD on iTunes in 2011. The episode was later re-rendered and made available for the Blu-ray release of the season that was released on December 5, 2017. The episode is presented in its original presentation, without Muhammad's image being obscured as in later episodes of the series.

When "South Park" debuted, it was a huge ratings success for Comedy Central and is seen as being largely responsible for the success of the channel, with Herzog crediting it for putting the network "on the map".

The show's first episode, "Cartman Gets an Anal Probe", earned a Nielsen rating of 1.3 (980,000 viewers), at the time considered high for a cable program. The show instantly generated buzz among television viewers, and mass viewing parties began assembling on college campuses. By the time the eighth episode, "Starvin' Marvin", aired — three months after the show debuted — ratings and viewership had tripled, and "South Park" was already the most successful show in Comedy Central's history. When the tenth episode "Damien" aired the following February, viewership increased another 33 percent. The episode earned a 6.4 rating, which at the time was over 10 times the average rating earned by a cable show aired in prime time. The ratings peaked with the second episode of season two, "Cartman's Mom Is Still a Dirty Slut", which aired on April 22, 1998. The episode earned an 8.2 rating (6.2 million viewers) and, at the time, set a record as the highest-rated non-sports show in basic cable history. During the spring of 1998, eight of the ten highest-rated shows on basic cable were "South Park" episodes.

The success of "South Park" prompted more cable companies to carry Comedy Central and led it to its becoming one of the fastest-growing cable channels. The number of households that had Comedy Central jumped from 9.1 million in 1997 to 50 million in June 1998. When the show debuted, the most Comedy Central had earned for a 30-second commercial was US$7,500. Within a year, advertisers were paying an average of US$40,000 for 30 seconds of advertising time during airings of "South Park" in its second season, while some paid as much as US$80,000.

By the third season (1999), the series' ratings began to decrease. The third-season premiere episode drew 3.4 million viewers, a dramatic drop from the 5.5 million of the previous season's premiere. Stone and Parker attributed this drop in the show's ratings to the media hype that surrounded the show in the previous year, adding that the third season ratings reflected the show's "true" fan base. The show's ratings dropped further in its fourth season (2000), with episodes averaging just above 1.5 million viewers. The ratings eventually increased, and seasons five through nine consistently averaged about 3 million viewers per episode. Though its viewership is lower than it was at the height of its popularity in its earliest seasons, "South Park" remains one of the highest-rated series on Comedy Central. The season 14 (2010) premiere gained 3.7 million viewers, the show's highest-rated season premiere since 1998. In 2016, a "New York Times" study of the 50 TV shows with the most Facebook Likes found that "perhaps unsurprisingly, South Park ... is most popular in Colorado".

In 2004, Channel 4 voted "South Park" the third-greatest cartoon of all time. In 2007, "Time" magazine included the show on its list of the "100 Best TV Shows of All Time", proclaiming it as "America's best source of rapid-fire satire for [the past] decade". The same year, "Rolling Stone" declared it to be the funniest show on television since its debut 10 years prior. In 2008, "South Park" was named the 12th-greatest TV show of the past 25 years by "Entertainment Weekly", while AOL declared it as having the "most astute" characters of any show in history when naming it the 16th-best television comedy series of all time. In 2011, "South Park" was voted number one in the "25 Greatest Animated TV Series" poll by "Entertainment Weekly". The character of Cartman ranked 10th on TV Guide's 2002 list of the "Top 50 Greatest Cartoon Characters", 198th on VH1's "200 Greatest Pop Culture Icons", 19th on Bravo's "100 Greatest TV Characters" television special in 2004, and second on MSNBC's 2005 list of TV's scariest characters behind Mr. Burns from "The Simpsons". In 2006, Comedy Central received a Peabody Award for "South Park"<nowiki>'s</nowiki> "stringent social commentary" and "undeniably fearless lampooning of all that is self-important and hypocritical in American life". In 2013, the Writers Guild of America ranked "South Park" at number 63 among the "101 Best-Written Shows Ever". Also in 2013, TV Guide listed the show at number 10 among the "60 Greatest Cartoons of All Time". In 2019, the series was ranked 42nd on "The Guardian" newspaper's list of the 100 best TV shows of the 21st century.

"South Park" won the CableACE Award for Best Animated Series in 1997, the last year the awards were given out. In 1998, "South Park" was nominated for the Annie Award for Outstanding Achievement in an Animated Primetime or Late Night Television Program. It was also nominated for the 1998 GLAAD Award for Outstanding TV – Individual Episode for "Big Gay Al's Big Gay Boat Ride".

"South Park" has been nominated for the Emmy Award for Outstanding Animated Program sixteen times (1998, 2000, 2002, 2004–2011, and 2013–2017). The show has won the award for Outstanding Animated Program (For Programming Less Than One Hour) four times, for the 2005 episode "Best Friends Forever", the 2006 episode "Make Love, Not Warcraft", the 2009 episode "Margaritaville", and the 2012 episode "Raising the Bar". The "Imaginationland" trilogy of episodes won the Emmy Award for Outstanding Animated Program (For Programming One Hour or More) in 2008.

The show's frequent depiction of taboo subject matter, general toilet humor, accessibility to younger viewers, disregard for conservative sensibilities, negative depiction of liberal causes, and portrayal of religion for comic effect have generated controversy and debate over the course of its run. 

As the series became popular, students in two schools were barred from wearing "South Park"-related T-shirts, and the headmaster of a UK public school asked parents not to let their children watch the programme after eight- and nine-year-old children voted the "South Park" character Cartman as their favorite personality in a 1999 poll. Parker and Stone assert that the show is not meant to be viewed by young children, and the show is certified with TV ratings that indicate its intention for mature audiences.

Parents Television Council founder L. Brent Bozell III and Action for Children's Television founder Peggy Charren have both condemned the show, with the latter claiming it is "dangerous to the democracy". Several other activist groups have protested the show's parodies of Christianity and portrayal of Jesus Christ. Stone has stated that parents who disapprove of "South Park" for its portrayal of how kids behave are upset because they "have an idyllic vision of what kids are like", adding "[kids] don't have any kind of social tact or etiquette, they're just complete little raging bastards".

The show further lampooned the controversy surrounding its use of profanity, as well as the media attention surrounding the network show "Chicago Hope"<nowiki>'s</nowiki> singular use of the word "shit", with the season five premiere "It Hits the Fan", in which the word "shit" is said 162 times without being bleeped for censorship purposes, while also appearing uncensored in written form. In the days following the show's original airing, 5,000 disapproving e-mails were sent to Comedy Central. Despite its 43 uncensored uses of the racial slur "nigger", the season 11 episode "With Apologies to Jesse Jackson" generated relatively little controversy, as most in the black community and the NAACP praised the episode for its context and its comedic way of conveying other races' perceptions of how black people feel when hearing the word.

Specific controversies regarding the show have included an April Fools' Day prank played on its viewers in 1998, its depiction of the Virgin Mary in the season nine (2005) finale "Bloody Mary" that angered several Catholics, its depiction of Steve Irwin with a stingray barb stuck in his chest in the episode "Hell on Earth 2006", which originally aired less than two months after Irwin was killed in the same fashion, and Comedy Central's censorship of the depiction of Muhammad in the season 10 episode "Cartoon Wars Part II" in the wake of the "Jyllands-Posten" Muhammad cartoons controversy.

The season nine (2005) episode "Trapped in the Closet" denounces Scientology as nothing more than "a big fat global scam", while freely divulging church information that Scientology normally only reveals to members who make significant monetary contributions to the church. The episode also ambiguously parodies the rumors involving the sexual orientation of Scientologist Tom Cruise, who allegedly demanded any further reruns of the episode be canceled. Isaac Hayes, a Scientologist, later quit "South Park" because of his objection to the episode.

The season fourteen episodes "200" and "201" were mired in controversy for satirizing issues surrounding the depiction of the Islamic prophet, Muhammad. The website for the organization Revolution Muslim, a New York-based radical Muslim organization, posted an entry that included a warning to creators Parker and Stone that they risk violent retribution for their depictions of Muhammad. It said that they "will probably wind up like Theo van Gogh for airing this show". The posting provided the addresses to Comedy Central in New York and the production company in Los Angeles. The author of the post, Zachary Adam Chesser (who prefers to be called Abu Talhah al-Amrikee), said it was meant to serve as a warning to Parker and Stone, not a threat, and that providing the addresses was meant to give people the opportunity to protest. 

Despite al-Amrikee's claims that the website entry was a warning, several media outlets and observers interpreted it as a threat. Support for the episode has come in the form of Everybody Draw Mohammed Day, a movement started on Facebook that encourages people to draw Muhammad on May 20. The "200" episode, which also depicted the Buddha snorting cocaine, prompted the government of Sri Lanka to ban the series outright.

Due to many taboo topics in China, such as Dalai Lama, Winnie the Pooh, summary execution, cannabis culture, and organ harvesting being involved in the season 23 (2019) episode "Band in China", "South Park" was entirely banned in China after the episode's broadcast. The series' Baidu Baike article, Baidu Tieba forum, Douban page, Zhihu page and Bilibili videos have been deleted or inaccessible to the public, all related keywords and topics have been prohibited from being searched and discussed on China-based search engines and social media sites including Baidu, QQ, Sina Weibo and on WeChat public platforms. Parker and Stone issued a sarcastic apology in response.

Commentary made in episodes has been interpreted as statements Parker and Stone are attempting to make to the viewing public, and these opinions have been subject to much critical analysis in the media and literary world within the framework of popular philosophical, theological, social, and political concepts. Since "South Park" debuted, college students have written term papers and doctoral theses analyzing the show, while Brooklyn College offers a course called ""South Park" and Political Correctness".

Soon after one of Kenny's trademark deaths on the show, other characters would typically shout "Oh my God, they killed Kenny!". The exclamation quickly became a popular catchphrase, while the running gag of Kenny's recurring deaths are one of the more recognized hallmarks among viewers of modern television. Cartman's exclamations of "Respect my authori-tah!" and "Screw you guys ...I'm going home!" became catchphrases as well, and during the show's earlier seasons, were highly popular in the lexicon of viewers. Cartman's eccentric intonation of "Hey!" was included in the 2002 edition of "The Oxford Dictionary of Catchphrases".

In the season two episode "Chef Aid", attorney Johnnie Cochran uses what's called in the show the Chewbacca defense, which is a legal strategy that involves addressing plot holes related to Chewbacca in the film "Return of the Jedi" rather than discussing the trial at hand during a closing argument in a deliberate attempt to confuse jurors into thinking there is reasonable doubt. The term "Chewbacca defense" has been documented as being used by criminologists, forensic scientists, and political commentators in their various discussions of similar methods used in legal cases and public forums.

Another season two episode, "Gnomes", revolves around a group of "underpants gnomes" who, as their name suggests, run a corporation stealing people's underpants. When asked about their business model, various gnomes reply that theirs is a three-step process: Phase 1 is "collect underpants". Phase 3 is "profit". However, the gnomes are unable to explain what is to occur between the first and final steps, and "Phase 2" is accompanied by a large question mark on their corporate flow chart. Using "????" and "PROFIT!" as the last two steps in a process (usually jokingly) has become a widely popular Internet meme because of this. Especially in the context of politics and economics, "underpants gnomes" has been used by some commentators to characterize a conspicuous gap of logic or planning.

When Sophie Rutschmann of the University of Strasbourg discovered a mutated gene that causes an adult fruit fly to die within two days after it is infected with certain bacteria, she named the gene "kep1" in honor of Kenny.

While some conservatives have condemned "South Park" for its vulgarity, a growing population of people who hold center-right political beliefs, including teenagers and young adults, have embraced the show for its tendency to mock liberal viewpoints and lampoon liberal celebrities and icons. Political commentator Andrew Sullivan dubbed the group "South Park" Republicans, or "South Park" conservatives. Sullivan averred that members of the group are "extremely skeptical of political correctness but also are socially liberal on many issues", though he says the phrase applied to them is meant to be more of a casual indication of beliefs than a strong partisan label. Brian C. Anderson describes the group as "generally characterized by holding strong libertarian beliefs and rejecting more conservative social policy", and notes that although the show makes "wicked fun of conservatives", it is "at the forefront of a conservative revolt against liberal media."

Parker and Stone reject the idea that the show has any underlying political position, and deny having a political agenda when creating an episode.
The two claim the show's higher proportion of instances lampooning liberal rather than conservative orthodoxies stems simply from their preference for making fun of liberals. While Stone has been quoted saying, "I hate conservatives, but I really fucking hate liberals", Stone and Parker have explained that their drive to lampoon a given target comes first from the target's insistence on telling other people how to behave. The duo explain that they regard liberals as having both delusions of entitlement to remain free from satire, and a propensity to enforce political correctness while patronizing the citizens of Middle America. Parker and Stone are uncomfortable with the idea of themselves or "South Park" being assigned any kind of partisan classification. Parker said he rejects the ""South Park" Republican" and ""South Park" conservative" labels, feeling that either tag implies that one only adheres to strictly conservative or liberal viewpoints. Canadian columnist Jaime J. Weinman observes that the most die-hard conservatives who identified themselves as ""South Park" Republicans" began turning away from the label when the show ridiculed Republicans in the season nine (2005) episode "Best Friends Forever."

In 1999, less than two years after the series first aired, a was released. The film, a musical comedy, was directed by Parker, who co-wrote the script with Stone and Pam Brady. The film was generally well received by critics, and earned a combined US$83.1 million at the domestic and foreign box office. The film satirizes the controversy surrounding the show itself and gained a spot in the 2001 edition of "Guinness World Records" for "Most Swearing in an Animated Film". The song "Blame Canada" from earned song co-writers Parker and Marc Shaiman an Academy Award nomination for Best Music, Original Song.

As a tribute to the Dead Parrot sketch, a short that features Cartman attempting to return a dead Kenny to a shop run by Kyle aired during a 1999 BBC television special commemorating the 30th anniversary of "Monty Python's Flying Circus". "South Park" parodied Scientology in a short that aired as part of the 2000 MTV Movie Awards. The short was entitled "The Gauntlet" and also poked fun at John Travolta, a Scientologist. The four main characters were featured in the documentary film "The Aristocrats", listening to Cartman tell his version of the film's titular joke. Short clips of Cartman introducing the starting lineup for the University of Colorado football team were featured during ABC's coverage of the 2007 matchup between the University of Colorado and the University of Nebraska. In 2008, Parker, as Cartman, gave answers to a Proust Questionnaire conducted by Julie Rovner of NPR. The Snakes & Arrows Tour for Rush in 2007 used an intro from Cartman, Stan, Kyle, and Kenny preceding "Tom Sawyer". As Parker, Stone and producer Frank Agnone are Los Angeles Kings fans, special "South Park" pre-game videos have been featured at Kings home games at Staples Center, and the club even sent the Stanley Cup to visit South Park Studios after winning the 2012 finals. Parker and Stone have also created Denver Broncos and Denver Nuggets-themed shorts, featuring Cartman, for home games at Pepsi Center.

"", a compilation of original songs from the show, characters performing cover songs, and tracks performed by guest artists was released in 1998, while "Mr. Hankey's Christmas Classics", a compilation of songs performed by the characters in the episode of the same name as well as other Christmas-themed songs was released in 1999, as was the to the feature film. The song "Chocolate Salty Balls" (performed by Hayes as Chef) was released as a single in the UK in 1998 to support the "Chef Aid: The South Park Album" and became a number one hit.

Merchandising related to the show is an industry which generates several million dollars a year. In 1998, the top-selling specialty T-shirt in the United States was based on "South Park", and US$30 million in T-shirt sales was reached during the show's first season.

A "South Park" pinball machine was released in 1999 by Sega Pinball. The companies Fun 4 All, Mezco Toyz, and Mirage have produced various South Park action figures, collectibles, and plush dolls.

Comedy Central entered into an agreement with Frito-Lay to sell 1.5 million bags of Cheesy Poofs, Cartman's favorite snack from the show, at Walmart until the premiere of the second half of the fifteenth season on October 5, 2011.
Further Reading



</doc>
<doc id="27978" url="https://en.wikipedia.org/wiki?curid=27978" title="Skin">
Skin

Skin is the layer of usually soft, flexible outer tissue covering the body of a vertebrate animal, with three main functions: protection, regulation, and sensation.

Other animal coverings, such as the arthropod exoskeleton, have different developmental origin, structure and chemical composition. The adjective cutaneous means "of the skin" (from Latin "cutis", skin). In mammals, the skin is an organ of the integumentary system made up of multiple layers of ectodermal tissue, and guards the underlying muscles, bones, ligaments and internal organs. Skin of a different nature exists in amphibians, reptiles, and birds. All mammals have some hair on their skin, even marine mammals like whales, dolphins, and porpoises which appear to be hairless.
The skin interfaces with the environment and is the first line of defense from external factors. For example, the skin plays a key role in protecting the body against pathogens and excessive water loss. Its other functions are insulation, temperature regulation, sensation, and the production of vitamin D folates. Severely damaged skin may heal by forming scar tissue. This is sometimes discoloured and depigmented. The thickness of skin also varies from location to location on an organism. In humans for example, the skin located under the eyes and around the eyelids is the thinnest skin in the body at 0.5 mm thick, and is one of the first areas to show signs of aging such as "crows feet" and wrinkles. The skin on the palms and the soles of the feet is 4 mm thick and is the thickest skin on the body. The speed and quality of wound healing in skin is promoted by the reception of estrogen.

Fur is dense hair. Primarily, fur augments the insulation the skin provides but can also serve as a secondary sexual characteristic or as camouflage. On some animals, the skin is very hard and thick, and can be processed to create leather. Reptiles and most fish have hard protective scales on their skin for protection, and birds have hard feathers, all made of tough β-keratins. Amphibian skin is not a strong barrier, especially regarding the passage of chemicals via skin and is often subject to osmosis and diffusive forces. For example, a frog sitting in an anesthetic solution would be sedated quickly, as the chemical diffuses through its skin. Amphibian skin plays key roles in everyday survival and their ability to exploit a wide range of habitats and ecological conditions.

Mammalian skin is composed of two primary layers:

The epidermis is composed of the outermost layers of the skin. It forms a protective barrier over the body's surface, responsible for keeping water in the body and preventing pathogens from entering, and is a stratified squamous epithelium, composed of proliferating basal and differentiated suprabasal keratinocytes.

Keratinocytes are the major cells, constituting 95% of the epidermis, while Merkel cells, melanocytes and Langerhans cells are also present. The epidermis can be further subdivided into the following "strata" or layers (beginning with the outermost layer):
Keratinocytes in the stratum basale proliferate through mitosis and the daughter cells move up the strata changing shape and composition as they undergo multiple stages of cell differentiation to eventually become anucleated. During that process, keratinocytes will become highly organized, forming cellular junctions (desmosomes) between each other and secreting keratin proteins and lipids which contribute to the formation of an extracellular matrix and provide mechanical strength to the skin. Keratinocytes from the stratum corneum are eventually shed from the surface (desquamation).

The epidermis contains no blood vessels, and cells in the deepest layers are nourished by diffusion from blood capillaries extending to the upper layers of the dermis.

The epidermis and dermis are separated by a thin sheet of fibers called the basement membrane, which is made through the action of both tissues.
The basement membrane controls the traffic of the cells and molecules between the dermis and epidermis but also serves, through the binding of a variety of cytokines and growth factors, as a reservoir for their controlled release during physiological remodeling or repair processes.

The dermis is the layer of skin beneath the epidermis that consists of connective tissue and cushions the body from stress and strain.
The dermis provides tensile strength and elasticity to the skin through an extracellular matrix composed of collagen fibrils, microfibrils, and elastic fibers, embedded in hyaluronan and proteoglycans. Skin proteoglycans are varied and have very specific locations. For example, hyaluronan, versican and decorin are present throughout the dermis and epidermis extracellular matrix, whereas biglycan and perlecan are only found in the epidermis.

It harbors many mechanoreceptors (nerve endings) that provide the sense of touch and heat through nociceptors and thermoreceptors. It also contains the hair follicles, sweat glands, sebaceous glands, apocrine glands, lymphatic vessels and blood vessels. The blood vessels in the dermis provide nourishment and waste removal from its own cells as well as for the epidermis.

The dermis is tightly connected to the epidermis through a basement membrane and is structurally divided into two areas: a superficial area adjacent to the epidermis, called the "papillary region", and a deep thicker area known as the "reticular region".

The papillary region is composed of loose areolar connective tissue. This is named for its fingerlike projections called "papillae" that extend toward the epidermis. The papillae provide the dermis with a "bumpy" surface that interdigitates with the epidermis, strengthening the connection between the two layers of skin.

The reticular region lies deep in the papillary region and is usually much thicker. It is composed of dense irregular connective tissue and receives its name from the dense concentration of collagenous, elastic, and reticular fibers that weave throughout it. These protein fibers give the dermis its properties of strength, extensibility, and elasticity.
Also located within the reticular region are the roots of the hair, sweat glands, sebaceous glands, receptors, nails, and blood vessels.

The subcutaneous tissue (also hypodermis) is not part of the skin, and lies below the dermis. Its purpose is to attach the skin to underlying bone and muscle as well as supplying it with blood vessels and nerves. It consists of loose connective tissue and elastin. The main cell types are fibroblasts, macrophages and adipocytes (the subcutaneous tissue contains 50% of body fat). Fat serves as padding and insulation for the body.

Microorganisms like "Staphylococcus epidermidis" colonize the skin surface. The density of skin flora depends on region of the skin. The disinfected skin surface gets recolonized from bacteria residing in the deeper areas of the hair follicle, gut and urogenital openings.

The epidermis of fish and of most amphibians consists entirely of live cells, with only minimal quantities of keratin in the cells of the superficial layer. It is generally permeable, and in the case of many amphibians, may actually be a major respiratory organ. The dermis of bony fish typically contains relatively little of the connective tissue found in tetrapods. Instead, in most species, it is largely replaced by solid, protective bony scales. Apart from some particularly large dermal bones that form parts of the skull, these scales are lost in tetrapods, although many reptiles do have scales of a different kind, as do pangolins. Cartilaginous fish have numerous tooth-like denticles embedded in their skin, in place of true scales.

Sweat glands and sebaceous glands are both unique to mammals, but other types of skin gland are found in other vertebrates. Fish typically have a numerous individual mucus-secreting skin cells that aid in insulation and protection, but may also have poison glands, photophores, or cells that produce a more watery, serous fluid. In amphibians, the mucus cells are gathered together to form sac-like glands. Most living amphibians also possess "granular glands" in the skin, that secrete irritating or toxic compounds.

Although melanin is found in the skin of many species, in the reptiles, the amphibians, and fish, the epidermis is often relatively colorless. Instead, the color of the skin is largely due to chromatophores in the dermis, which, in addition to melanin, may contain guanine or carotenoid pigments. Many species, such as chameleons and flounders may be able to change the color of their skin by adjusting the relative size of their chromatophores.

"See also: amphibians"

Amphibians possess two types of glands, mucous and granular (serous). Both of these glands are part of the integument and thus considered cutaneous. Mucous and granular glands are both divided into three different sections which all connect to structure the gland as a whole. The three individual parts of the gland are the duct, the intercalary region, and lastly the alveolar gland (sac). Structurally, the duct is derived via keratinocytes and passes through to the surface of the epidermal or outer skin layer thus allowing external secretions of the body. The gland alveolus is a sac shaped structure that is found on the bottom or base region of the granular gland. The cells in this sac specialize in secretion. Between the alveolar gland and the duct is the intercalary system which can be summed up as a transitional region connecting the duct to the grand alveolar beneath the epidermal skin layer. In general, granular glands are larger in size than the mucous glands, however mucous glands hold a much greater majority in overall number.
Granular glands can be identified as venomous and often differ in the type of toxin as well as the concentrations of secretions across various orders and species within the amphibians. They are located in clusters differing in concentration depending on amphibian taxa. The toxins can be fatal to most vertebrates or have no effect against others. These glands are alveolar meaning they structurally have little sacs in which venom is produced and held before it is secreted upon defensive behaviors.

Structurally, the ducts of the granular gland initially maintain a cylindrical shape. However, when the ducts become mature and full of fluid, the base of the ducts become swollen due to the pressure from the inside. This causes the epidermal layer to form a pit like opening on the surface of the duct in which the inner fluid will be secreted in an upwards fashion.

The intercalary region of granular glands is more developed and mature in comparison with mucous glands. This region resides as a ring of cells surrounding the basal portion of the duct which are argued to have an ectodermal muscular nature due to their influence over the lumen (space inside the tube) of the duct with dilation and constriction functions during secretions. The cells are found radially around the duct and provide a distinct attachment site for muscle fibers around the gland's body.

The gland alveolus is a sac that is divided into three specific regions/layers. The outer layer or tunica fibrosa is composed of densely packed connective-tissue which connects with fibers from the spongy intermediate layer where elastic fibers, as well as nerves, reside. The nerves send signals to the muscles as well as the epithelial layers. Lastly, the epithelium or tunica propria encloses the gland.

Mucous glands are non-venomous and offer a different functionality for amphibians than granular. Mucous glands cover the entire surface area of the amphibian body and specialize in keeping the body lubricated. There are many other functions of the mucous glands such as controlling the pH, thermoregulation, adhesive properties to the environment, anti-predator behaviors (slimy to the grasp), chemical communication, even anti-bacterial/viral properties for protection against pathogens.

The ducts of the mucous gland appear as cylindrical vertical tubes that break through the epidermal layer to the surface of the skin. The cells lining the inside of the ducts are oriented with their longitudinal axis forming 90-degree angles surrounding the duct in a helical fashion.

Intercalary cells react identically to those of granular glands but on a smaller scale. Among the amphibians, there are taxa which contain a modified intercalary region (depending on the function of the glands), yet the majority share the same structure.

The alveolor of mucous glands are much more simple and only consist of an epithelium layer as well as connective tissue which forms a cover over the gland. This gland lacks a tunica propria and appears to have delicate and intricate fibers which pass over the gland's muscle and epithelial layers.

The epidermis of birds and reptiles is closer to that of mammals, with a layer of dead keratin-filled cells at the surface, to help reduce water loss. A similar pattern is also seen in some of the more terrestrial amphibians such as toads. However, in all of these animals there is no clear differentiation of the epidermis into distinct layers, as occurs in humans, with the change in cell type being relatively gradual. The mammalian epidermis always possesses at least a stratum germinativum and stratum corneum, but the other intermediate layers found in humans are not always distinguishable.
Hair is a distinctive feature of mammalian skin, while feathers are (at least among living species) similarly unique to birds.

Birds and reptiles have relatively few skin glands, although there may be a few structures for specific purposes, such as pheromone-secreting cells in some reptiles, or the uropygial gland of most birds.

Skin performs the following functions:


Skin is a soft tissue and exhibits key mechanical behaviors of these tissues. The most pronounced feature is the J-curve stress strain response, in which a region of large strain and minimal stress exists and corresponds to the microstructural straightening and reorientation of collagen fibrils. In some cases the intact skin is prestreched, like wetsuits around the diver's body, and in other cases the intact skin is under compression. Small circular holes punched on the skin may widen or close into ellipses, or shrink and remain circular, depending on preexisting stresses.

Tissue homeostasis generally declines with age, in part because stem/progenitor cells fail to self-renew or differentiate. In the skin of mice, mitochondrial oxidative stress can promote cellular senescence and aging phenotypes. Ordinarily mitochondrial superoxide dismutase (SOD2) protects against oxidative stress. Using a mouse model of genetic SOD2 deficiency, it was shown that failure to express this important antioxidant enzyme in epidermal cells caused cellular senescence, nuclear DNA damage, and irreversible arrest of proliferation of a fraction of keratinocytes.

Skin aging is caused in part by TGF-β, which reduces the subcutaneous fat that gives skin a pleasant appearance and texture. TGF-β does this by blocking the conversion of dermal fibroblasts into fat cells; with fewer fat cells underneath to provide support, the skin becomes saggy and wrinkled. Subcutaneous fat also produces cathelicidin, which is a peptide that fights bacterial infections.

The term "skin" may also refer to the covering of a small animal, such as a sheep, goat (goatskin), pig, snake (snakeskin) etc. or the young of a large animal.

The term hides or rawhide refers to the covering of a large adult animal such as a cow, buffalo, horse etc.

Skins and hides from the different animals are used for clothing, bags and other consumer products, usually in the form of leather, but also as furs.

Skin from sheep, goat and cattle was used to make parchment for manuscripts.

Skin can also be cooked to make pork rind or crackling.



</doc>
<doc id="27979" url="https://en.wikipedia.org/wiki?curid=27979" title="Sunlight">
Sunlight

Sunlight is a portion of the electromagnetic radiation given off by the Sun, in particular infrared, visible, and ultraviolet light. On Earth, sunlight is scattered and filtered through Earth's atmosphere, and is obvious as daylight when the Sun is above the horizon. When direct solar radiation is not blocked by clouds, it is experienced as sunshine, a combination of bright light and radiant heat. When blocked by clouds or reflected off other objects, sunlight is diffused. The World Meteorological Organization uses the term "sunshine duration" to mean the cumulative time during which an area receives direct irradiance from the Sun of at least 120 watts per square meter. Other sources indicate an "Average over the entire earth" of "164 Watts per square meter over a 24 hour day".

The ultraviolet radiation in sunlight has both positive and negative health effects, as it is both a requisite for vitamin D synthesis and a mutagen. 

Sunlight takes about 8.3 minutes to reach Earth from the surface of the Sun. A photon starting at the center of the Sun and changing direction every time it encounters a charged particle would take between 10,000 and 170,000 years to get to the surface.

Sunlight is a key factor in photosynthesis, the process used by plants and other autotrophic organisms to convert light energy, normally from the Sun, into chemical energy that can be used to synthesize carbohydrates and to fuel the organisms' activities.

Researchers can measure the intensity of sunlight using a sunshine recorder, pyranometer, or pyrheliometer. To calculate the amount of sunlight reaching the ground, both the eccentricity of Earth's elliptic orbit and the attenuation by Earth's atmosphere have to be taken into account. The extraterrestrial solar illuminance (), corrected for the elliptic orbit by using the day number of the year (dn), is given to a good approximation by


</doc>
<doc id="27980" url="https://en.wikipedia.org/wiki?curid=27980" title="Stellar evolution">
Stellar evolution

Stellar evolution is the process by which a star changes over the course of time. Depending on the mass of the star, its lifetime can range from a few million years for the most massive to trillions of years for the least massive, which is considerably longer than the age of the universe. The table shows the lifetimes of stars as a function of their masses. All stars are formed from collapsing clouds of gas and dust, often called nebulae or molecular clouds. Over the course of millions of years, these protostars settle down into a state of equilibrium, becoming what is known as a main-sequence star.

Nuclear fusion powers a star for most of its existence. Initially the energy is generated by the fusion of hydrogen atoms at the core of the main-sequence star. Later, as the preponderance of atoms at the core becomes helium, stars like the Sun begin to fuse hydrogen along a spherical shell surrounding the core. This process causes the star to gradually grow in size, passing through the subgiant stage until it reaches the red giant phase. Stars with at least half the mass of the Sun can also begin to generate energy through the fusion of helium at their core, whereas more-massive stars can fuse heavier elements along a series of concentric shells. Once a star like the Sun has exhausted its nuclear fuel, its core collapses into a dense white dwarf and the outer layers are expelled as a planetary nebula. Stars with around ten or more times the mass of the Sun can explode in a supernova as their inert iron cores collapse into an extremely dense neutron star or black hole. Although the universe is not old enough for any of the smallest red dwarfs to have reached the end of their existence, stellar models suggest they will slowly become brighter and hotter before running out of hydrogen fuel and becoming low-mass white dwarfs.

Stellar evolution is not studied by observing the life of a single star, as most stellar changes occur too slowly to be detected, even over many centuries. Instead, astrophysicists come to understand how stars evolve by observing numerous stars at various points in their lifetime, and by simulating stellar structure using computer models.

Stellar evolution starts with the gravitational collapse of a giant molecular cloud. Typical giant molecular clouds are roughly across and contain up to . As it collapses, a giant molecular cloud breaks into smaller and smaller pieces. In each of these fragments, the collapsing gas releases gravitational potential energy as heat. As its temperature and pressure increase, a fragment condenses into a rotating ball of superhot gas known as a protostar.

A protostar continues to grow by accretion of gas and dust from the molecular cloud, becoming a pre-main-sequence star as it reaches its final mass. Further development is determined by its mass. Mass is typically compared to the mass of the Sun: means 1 solar mass.

Protostars are encompassed in dust, and are thus more readily visible at infrared wavelengths.
Observations from the Wide-field Infrared Survey Explorer (WISE) have been especially important for unveiling numerous galactic protostars and their parent star clusters.

Protostars with masses less than roughly never reach temperatures high enough for nuclear fusion of hydrogen to begin. These are known as brown dwarfs. The International Astronomical Union defines brown dwarfs as stars massive enough to fuse deuterium at some point in their lives (13 Jupiter masses (), 2.5 × 10 kg, or ). Objects smaller than are classified as sub-brown dwarfs (but if they orbit around another stellar object they are classified as planets). Both types, deuterium-burning and not, shine dimly and fade away slowly, cooling gradually over hundreds of millions of years.

For a more-massive protostar, the core temperature will eventually reach 10 million kelvin, initiating the proton–proton chain reaction and allowing hydrogen to fuse, first to deuterium and then to helium. In stars of slightly over , the carbon–nitrogen–oxygen fusion reaction (CNO cycle) contributes a large portion of the energy generation. The onset of nuclear fusion leads relatively quickly to a hydrostatic equilibrium in which energy released by the core maintains a high gas pressure, balancing the weight of the star's matter and preventing further gravitational collapse. The star thus evolves rapidly to a stable state, beginning the main-sequence phase of its evolution.

A new star will sit at a specific point on the main sequence of the Hertzsprung–Russell diagram, with the main-sequence spectral type depending upon the mass of the star. Small, relatively cold, low-mass red dwarfs fuse hydrogen slowly and will remain on the main sequence for hundreds of billions of years or longer, whereas massive, hot O-type stars will leave the main sequence after just a few million years. A mid-sized yellow dwarf star, like the Sun, will remain on the main sequence for about 10 billion years. The Sun is thought to be in the middle of its main sequence lifespan.

Eventually the core exhausts its supply of hydrogen and the star begins to evolve off of the main sequence. Without the outward radiation pressure generated by the fusion of hydrogen to counteract the force of gravity the core contracts until either electron degeneracy pressure becomes sufficient to oppose gravity or the core becomes hot enough (around 100 MK) for helium fusion to begin. Which of these happens first depends upon the star's mass.

What happens after a low-mass star ceases to produce energy through fusion has not been directly observed; the universe is around 13.8 billion years old, which is less time (by several orders of magnitude, in some cases) than it takes for fusion to cease in such stars.

Recent astrophysical models suggest that red dwarfs of may stay on the main sequence for some six to twelve trillion years, gradually increasing in both temperature and luminosity, and take several hundred billion years more to collapse, slowly, into a white dwarf. Such stars will not become red giants as the whole star is a convection zone and it will not develop a degenerate helium core with a shell burning hydrogen. Instead, hydrogen fusion will proceed until almost the whole star is helium.

Slightly more massive stars do expand into red giants, but their helium cores are not massive enough to reach the temperatures required for helium fusion so they never reach the tip of the red giant branch. When hydrogen shell burning finishes, these stars move directly off the red giant branch like a post-asymptotic-giant-branch (AGB) star, but at lower luminosity, to become a white dwarf. A star with an initial mass about will be able to reach temperatures high enough to fuse helium, and these "mid-sized" stars go on to further stages of evolution beyond the red giant branch.

Stars of roughly become red giants, which are large non-main-sequence stars of stellar classification K or M. Red giants lie along the right edge of the Hertzsprung–Russell diagram due to their red color and large luminosity. Examples include Aldebaran in the constellation Taurus and Arcturus in the constellation of Boötes.

Mid-sized stars are red giants during two different phases of their post-main-sequence evolution: red-giant-branch stars, with inert cores made of helium and hydrogen-burning shells, and asymptotic-giant-branch stars, with inert cores made of carbon and helium-burning shells inside the hydrogen-burning shells. Between these two phases, stars spend a period on the horizontal branch with a helium-fusing core. Many of these helium-fusing stars cluster towards the cool end of the horizontal branch as K-type giants and are referred to as red clump giants.

When a star exhausts the hydrogen in its core, it leaves the main sequence and begins to fuse hydrogen in a shell outside the core. The core increases in mass as the shell produces more helium. Depending on the mass of the helium core, this continues for several million to one or two billion years, with the star expanding and cooling at a similar or slightly lower luminosity to its main sequence state. Eventually either the core becomes degenerate, in stars around the mass of the sun, or the outer layers cool sufficiently to become opaque, in more massive stars. Either of these changes cause the hydrogen shell to increase in temperature and the luminosity of the star to increase, at which point the star expands onto the red giant branch.

The expanding outer layers of the star are convective, with the material being mixed by turbulence from near the fusing regions up to the surface of the star. For all but the lowest-mass stars, the fused material has remained deep in the stellar interior prior to this point, so the convecting envelope makes fusion products visible at the star's surface for the first time. At this stage of evolution, the results are subtle, with the largest effects, alterations to the isotopes of hydrogen and helium, being unobservable. The effects of the CNO cycle appear at the surface during the first dredge-up, with lower C/C ratios and altered proportions of carbon and nitrogen. These are detectable with spectroscopy and have been measured for many evolved stars.

The helium core continues to grow on the red giant branch. It is no longer in thermal equilibrium, either degenerate or above the Schoenberg-Chandrasekhar limit, so it increases in temperature which causes the rate of fusion in the hydrogen shell to increase. The star increases in luminosity towards the tip of the red-giant branch. Red giant branch stars with a degenerate helium core all reach the tip with very similar core masses and very similar luminosities, although the more massive of the red giants become hot enough to ignite helium fusion before that point.

In the helium cores of stars in the 0.6 to 2.0 solar mass range, which are largely supported by electron degeneracy pressure, helium fusion will ignite on a timescale of days in a helium flash. In the nondegenerate cores of more massive stars, the ignition of helium fusion occurs relatively slowly with no flash. The nuclear power released during the helium flash is very large, on the order of 10 times the luminosity of the Sun for a few days and 10 times the luminosity of the Sun (roughly the luminosity of the Milky Way Galaxy) for a few seconds. However, the energy is consumed by the thermal expansion of the initially degenerate core and thus cannot be seen from outside the star. Due to the expansion of the core, the hydrogen fusion in the overlying layers slows and total energy generation decreases. The star contracts, although not all the way to the main sequence, and it migrates to the horizontal branch on the Hertzsprung–Russell diagram, gradually shrinking in radius and increasing its surface temperature.

Core helium flash stars evolve to the red end of the horizontal branch but do not migrate to higher temperatures before they gain a degenerate carbon-oxygen core and start helium shell burning. These stars are often observed as a red clump of stars in the colour-magnitude diagram of a cluster, hotter and less luminous than the red giants. Higher-mass stars with larger helium cores move along the horizontal branch to higher temperatures, some becoming unstable pulsating stars in the yellow instability strip (RR Lyrae variables), whereas some become even hotter and can form a blue tail or blue hook to the horizontal branch. The morphology of the horizontal branch depends on parameters such as metallicity, age, and helium content, but the exact details are still being modelled.

After a star has consumed the helium at the core, hydrogen and helium fusion continues in shells around a hot core of carbon and oxygen. The star follows the asymptotic giant branch on the Hertzsprung–Russell diagram, paralleling the original red giant evolution, but with even faster energy generation (which lasts for a shorter time). Although helium is being burnt in a shell, the majority of the energy is produced by hydrogen burning in a shell further from the core of the star. Helium from these hydrogen burning shells drops towards the center of the star and periodically the energy output from the helium shell increases dramatically. This is known as a thermal pulse and they occur towards the end of the asymptotic-giant-branch phase, sometimes even into the post-asymptotic-giant-branch phase. Depending on mass and composition, there may be several to hundreds of thermal pulses.

There is a phase on the ascent of the asymptotic-giant-branch where a deep convective zone forms and can bring carbon from the core to the surface. This is known as the second dredge up, and in some stars there may even be a third dredge up. In this way a carbon star is formed, very cool and strongly reddened stars showing strong carbon lines in their spectra. A process known as hot bottom burning may convert carbon into oxygen and nitrogen before it can be dredged to the surface, and the interaction between these processes determines the observed luminosities and spectra of carbon stars in particular clusters.

Another well known class of asymptotic-giant-branch stars is the Mira variables, which pulsate with well-defined periods of tens to hundreds of days and large amplitudes up to about 10 magnitudes (in the visual, total luminosity changes by a much smaller amount). In more-massive stars the stars become more luminous and the pulsation period is longer, leading to enhanced mass loss, and the stars become heavily obscured at visual wavelengths. These stars can be observed as OH/IR stars, pulsating in the infra-red and showing OH maser activity. These stars are clearly oxygen rich, in contrast to the carbon stars, but both must be produced by dredge ups.

These mid-range stars ultimately reach the tip of the asymptotic-giant-branch and run out of fuel for shell burning. They are not sufficiently massive to start full-scale carbon fusion, so they contract again, going through a period of post-asymptotic-giant-branch superwind to produce a planetary nebula with an extremely hot central star. The central star then cools to a white dwarf. The expelled gas is relatively rich in heavy elements created within the star and may be particularly oxygen or carbon enriched, depending on the type of the star. The gas builds up in an expanding shell called a circumstellar envelope and cools as it moves away from the star, allowing dust particles and molecules to form. With the high infrared energy input from the central star, ideal conditions are formed in these circumstellar envelopes for maser excitation.

It is possible for thermal pulses to be produced once post-asymptotic-giant-branch evolution has begun, producing a variety of unusual and poorly understood stars known as born-again asymptotic-giant-branch stars. These may result in extreme horizontal-branch stars (subdwarf B stars), hydrogen deficient post-asymptotic-giant-branch stars, variable planetary nebula central stars, and R Coronae Borealis variables.

In massive stars, the core is already large enough at the onset of the hydrogen burning shell that helium ignition will occur before electron degeneracy pressure has a chance to become prevalent. Thus, when these stars expand and cool, they do not brighten as dramatically as lower-mass stars; however, they were more luminous on the main sequence and they evolve to highly luminous supergiants. Their cores become massive enough that they cannot support themselves by electron degeneracy and will eventually collapse to produce a neutron star or black hole.

Extremely massive stars (more than approximately ), which are very luminous and thus have very rapid stellar winds, lose mass so rapidly due to radiation pressure that they tend to strip off their own envelopes before they can expand to become red supergiants, and thus retain extremely high surface temperatures (and blue-white color) from their main-sequence time onwards. The largest stars of the current generation are about because the outer layers would be expelled by the extreme radiation. Although lower-mass stars normally do not burn off their outer layers so rapidly, they can likewise avoid becoming red giants or red supergiants if they are in binary systems close enough so that the companion star strips off the envelope as it expands, or if they rotate rapidly enough so that convection extends all the way from the core to the surface, resulting in the absence of a separate core and envelope due to thorough mixing.

The core of a massive star, defined as the region depleted of hydrogen, grows hotter and more dense as it accretes material from the fusion of hydrogen outside the core. In sufficiently massive stars, the core reaches temperatures and densities high enough to fuse carbon and heavier elements via the alpha process. At the end of helium fusion, the core of a star consists primarily of carbon and oxygen. In stars heavier than about , the carbon ignites and fuses to form neon, sodium, and magnesium. Stars somewhat less massive may partially ignite carbon, but are unable to fully fuse the carbon before electron degeneracy sets in, and these stars will eventually leave an oxygen-neon-magnesium white dwarf.

The exact mass limit for full carbon burning depends on several factors such as metallicity and the detailed mass lost on the asymptotic giant branch, but is approximately . After carbon burning is complete, the core of these stars reaches about and becomes hot enough for heavier elements to fuse. Before oxygen starts to fuse, neon begins to capture electrons which triggers neon burning. For a range of stars of approximately , this process is unstable and creates runaway fusion resulting in an electron capture supernova.

In more massive stars, the fusion of neon proceeds without a runaway deflagration. This is followed in turn by complete oxygen burning and silicon burning, producing a core consisting largely of iron-peak elements. Surrounding the core are shells of lighter elements still undergoing fusion. The timescale for complete fusion of a carbon core to an iron core is so short, just a few hundred years, that the outer layers of the star are unable to react and the appearance of the star is largely unchanged. The iron core grows until it reaches an "effective Chandrasekhar mass", higher than the formal Chandrasekhar mass due to various corrections for the relativistic effects, entropy, charge, and the surrounding envelope. The effective Chandrasekhar mass for an iron core varies from about in the least massive red supergiants to more than or more in more massive stars. Once this mass is reached, electrons begin to be captured into the iron-peak nuclei and the core becomes unable to support itself. The core collapses and the star is destroyed, either in a supernova or direct collapse to a black hole.

When the core of a massive star collapses, it will form a neutron star, or in the case of cores that exceed the Tolman-Oppenheimer-Volkoff limit, a black hole. Through a process that is not completely understood, some of the gravitational potential energy released by this core collapse is converted into a Type Ib, Type Ic, or Type II supernova. It is known that the core collapse produces a massive surge of neutrinos, as observed with supernova SN 1987A. The extremely energetic neutrinos fragment some nuclei; some of their energy is consumed in releasing nucleons, including neutrons, and some of their energy is transformed into heat and kinetic energy, thus augmenting the shock wave started by rebound of some of the infalling material from the collapse of the core. Electron capture in very dense parts of the infalling matter may produce additional neutrons. Because some of the rebounding matter is bombarded by the neutrons, some of its nuclei capture them, creating a spectrum of heavier-than-iron material including the radioactive elements up to (and likely beyond) uranium. Although non-exploding red giants can produce significant quantities of elements heavier than iron using neutrons released in side reactions of earlier nuclear reactions, the abundance of elements heavier than iron (and in particular, of certain isotopes of elements that have multiple stable or long-lived isotopes) produced in such reactions is quite different from that produced in a supernova. Neither abundance alone matches that found in the Solar System, so both supernovae and ejection of elements from red giants are required to explain the observed abundance of heavy elements and isotopes thereof.

The energy transferred from collapse of the core to rebounding material not only generates heavy elements, but provides for their acceleration well beyond escape velocity, thus causing a Type Ib, Type Ic, or Type II supernova. Current understanding of this energy transfer is still not satisfactory; although current computer models of Type Ib, Type Ic, and Type II supernovae account for part of the energy transfer, they are not able to account for enough energy transfer to produce the observed ejection of material. However, neutrino oscillations may play an important role in the energy transfer problem as they not only affect the energy available in a particular flavour of neutrinos but also through other general-relativistic effects on neutrinos.

Some evidence gained from analysis of the mass and orbital parameters of binary neutron stars (which require two such supernovae) hints that the collapse of an oxygen-neon-magnesium core may produce a supernova that differs observably (in ways other than size) from a supernova produced by the collapse of an iron core.

The most massive stars that exist today may be completely destroyed by a supernova with an energy greatly exceeding its gravitational binding energy. This rare event, caused by pair-instability, leaves behind no black hole remnant. In the past history of the universe, some stars were even larger than the largest that exists today, and they would immediately collapse into a black hole at the end of their lives, due to photodisintegration.
After a star has burned out its fuel supply, its remnants can take one of three forms, depending on the mass during its lifetime.

For a star of , the resulting white dwarf is of about , compressed into approximately the volume of the Earth. White dwarfs are stable because the inward pull of gravity is balanced by the degeneracy pressure of the star's electrons, a consequence of the Pauli exclusion principle. Electron degeneracy pressure provides a rather soft limit against further compression; therefore, for a given chemical composition, white dwarfs of higher mass have a smaller volume. With no fuel left to burn, the star radiates its remaining heat into space for billions of years.

A white dwarf is very hot when it first forms, more than 100,000 K at the surface and even hotter in its interior. It is so hot that a lot of its energy is lost in the form of neutrinos for the first 10 million years of its existence, but will have lost most of its energy after a billion years.

The chemical composition of the white dwarf depends upon its mass. A star of a few solar masses will ignite carbon fusion to form magnesium, neon, and smaller amounts of other elements, resulting in a white dwarf composed chiefly of oxygen, neon, and magnesium, provided that it can lose enough mass to get below the Chandrasekhar limit (see below), and provided that the ignition of carbon is not so violent as to blow the star apart in a supernova. A star of mass on the order of magnitude of the Sun will be unable to ignite carbon fusion, and will produce a white dwarf composed chiefly of carbon and oxygen, and of mass too low to collapse unless matter is added to it later (see below). A star of less than about half the mass of the Sun will be unable to ignite helium fusion (as noted earlier), and will produce a white dwarf composed chiefly of helium.

In the end, all that remains is a cold dark mass sometimes called a black dwarf. However, the universe is not old enough for any black dwarfs to exist yet.

If the white dwarf's mass increases above the Chandrasekhar limit, which is for a white dwarf composed chiefly of carbon, oxygen, neon, and/or magnesium, then electron degeneracy pressure fails due to electron capture and the star collapses. Depending upon the chemical composition and pre-collapse temperature in the center, this will lead either to collapse into a neutron star or runaway ignition of carbon and oxygen. Heavier elements favor continued core collapse, because they require a higher temperature to ignite, because electron capture onto these elements and their fusion products is easier; higher core temperatures favor runaway nuclear reaction, which halts core collapse and leads to a Type Ia supernova. These supernovae may be many times brighter than the Type II supernova marking the death of a massive star, even though the latter has the greater total energy release. This instability to collapse means that no white dwarf more massive than approximately can exist (with a possible minor exception for very rapidly spinning white dwarfs, whose centrifugal force due to rotation partially counteracts the weight of their matter). Mass transfer in a binary system may cause an initially stable white dwarf to surpass the Chandrasekhar limit.

If a white dwarf forms a close binary system with another star, hydrogen from the larger companion may accrete around and onto a white dwarf until it gets hot enough to fuse in a runaway reaction at its surface, although the white dwarf remains below the Chandrasekhar limit. Such an explosion is termed a nova.

Ordinarily, atoms are mostly electron clouds by volume, with very compact nuclei at the center (proportionally, if atoms were the size of a football stadium, their nuclei would be the size of dust mites). When a stellar core collapses, the pressure causes electrons and protons to fuse by electron capture. Without electrons, which keep nuclei apart, the neutrons collapse into a dense ball (in some ways like a giant atomic nucleus), with a thin overlying layer of degenerate matter (chiefly iron unless matter of different composition is added later). The neutrons resist further compression by the Pauli exclusion principle, in a way analogous to electron degeneracy pressure, but stronger.

These stars, known as neutron stars, are extremely small—on the order of radius 10 km, no bigger than the size of a large city—and are phenomenally dense. Their period of rotation shortens dramatically as the stars shrink (due to conservation of angular momentum); observed rotational periods of neutron stars range from about 1.5 milliseconds (over 600 revolutions per second) to several seconds. When these rapidly rotating stars' magnetic poles are aligned with the Earth, we detect a pulse of radiation each revolution. Such neutron stars are called pulsars, and were the first neutron stars to be discovered. Though electromagnetic radiation detected from pulsars is most often in the form of radio waves, pulsars have also been detected at visible, X-ray, and gamma ray wavelengths.

If the mass of the stellar remnant is high enough, the neutron degeneracy pressure will be insufficient to prevent collapse below the Schwarzschild radius. The stellar remnant thus becomes a black hole. The mass at which this occurs is not known with certainty, but is currently estimated at between 2 and .

Black holes are predicted by the theory of general relativity. According to classical general relativity, no matter or information can flow from the interior of a black hole to an outside observer, although quantum effects may allow deviations from this strict rule. The existence of black holes in the universe is well supported, both theoretically and by astronomical observation.

Because the core-collapse mechanism of a supernova is, at present, only partially understood, it is still not known whether it is possible for a star to collapse directly to a black hole without producing a visible supernova, or whether some supernovae initially form unstable neutron stars which then collapse into black holes; the exact relation between the initial mass of the star and the final remnant is also not completely certain. Resolution of these uncertainties requires the analysis of more supernovae and supernova remnants.

A stellar evolutionary model is a mathematical model that can be used to compute the evolutionary phases of a star from its formation until it becomes a remnant. The mass and chemical composition of the star are used as the inputs, and the luminosity and surface temperature are the only constraints. The model formulae are based upon the physical understanding of the star, usually under the assumption of hydrostatic equilibrium. Extensive computer calculations are then run to determine the changing state of the star over time, yielding a table of data that can be used to determine the evolutionary track of the star across the Hertzsprung–Russell diagram, along with other evolving properties. Accurate models can be used to estimate the current age of a star by comparing its physical properties with those of stars along a matching evolutionary track.





</doc>
<doc id="27982" url="https://en.wikipedia.org/wiki?curid=27982" title="Snake River">
Snake River

The Snake River is a major river of the greater Pacific Northwest region in the United States. At long, it is the largest tributary of the Columbia River, in turn the largest North American river that empties into the Pacific Ocean. The Snake River rises in western Wyoming, then flows through the Snake River Plain of southern Idaho, the rugged Hells Canyon on the Oregon–Idaho border and the rolling Palouse Hills of Washington, emptying into the Columbia River at the Tri-Cities, Washington.

The Snake River drainage basin encompasses parts of six U.S. states (Idaho, Washington, Oregon, Utah, Nevada, and Wyoming) and is known for its varied geologic history. The Snake River Plain was created by a volcanic hotspot which now lies underneath the Snake River headwaters in Yellowstone National Park. Gigantic glacial-retreat flooding episodes that occurred during the previous Ice Age carved out canyons, cliffs and waterfalls along the middle and lower Snake River. Two of these catastrophic flooding events, the Missoula Floods and Bonneville Flood, significantly affected the river and its surroundings.

Prehistoric Native Americans lived along the Snake starting more than 11,000 years ago. Salmon from the Pacific Ocean spawned by the millions in the river, and were a vital resource for people living on the Snake downstream of Shoshone Falls. By the time Lewis and Clark explored the area, the Nez Perce and Shoshone were the dominant Native American groups in the region. Later explorers and fur trappers further changed and used the resources of the Snake River basin. At one point, sign language used by the Shoshones representing weaving baskets was misinterpreted to represent a snake, giving the Snake River its name.

By the middle 19th century, the Oregon Trail had become well established, bringing numerous settlers to the Snake River region. Steamboats and railroads moved agricultural products and minerals along the river throughout the 19th and early 20th centuries. Starting in the 1890s, fifteen major dams have been built on the Snake River to generate hydroelectricity, enhance navigation, and provide irrigation water. However, these dams blocked salmon migration above Hells Canyon and have led to water quality and environmental issues in certain parts of the river. The removal of several dams on the lower Snake River has been proposed, in order to restore some of the river's once-tremendous salmon runs.

Formed by the confluence of three tiny streams on the southwest flank of Two Oceans Plateau in Yellowstone National Park, western Wyoming, the Snake starts out flowing west and south into Jackson Lake. Its first run through Jackson Hole, a wide valley between the Teton Range and the Gros Ventre Range. Below the tourist town of Jackson, the river turns west and flows through Snake River Canyon, cutting through the Snake River Range and into eastern Idaho. It receives the Hoback and Greys Rivers before entering Palisades Reservoir, where the Salt River joins at the mouth of Star Valley. Below Palisades Dam, the Snake River flows through the Snake River Plain, a vast arid physiographic province extending through southern Idaho southwest of the Rocky Mountains and underlain by the Snake River Aquifer, one of the most productive aquifers in the United States.

Southwest of Rexburg, Idaho, the Snake is joined from the north by Henrys Fork. The Henrys Fork is sometimes called the North Fork of the Snake River, with the main Snake above their confluence known as the "South Fork". From there it turns south, flowing through downtown Idaho Falls, then past the Fort Hall Indian Reservation and into American Falls Reservoir, where it is joined by the Portneuf River. The Portneuf River Valley is an overflow channel that in the last glacial period carried floodwaters from pluvial Lake Bonneville into the Snake River, significantly altering the landscape of the Snake River Plain through massive erosion. From there the Snake resumes its journey west, entering the Snake River Canyon of Idaho. It is interrupted by several major cataracts, the largest being Shoshone Falls, which historically marked the upriver limit of migrating salmon. A short distance downstream it passes under the Perrine Bridge. Near Twin Falls, the Snake approaches the southernmost point in its entire course, after which it starts to flow west-northwest.
The Snake continues through its canyon, receiving the Malad River from the east near Bliss and then the Bruneau River from the south in C.J. Strike Reservoir. It passes through an agricultural valley about southwest of Boise and flows briefly west into Oregon, before turning north to define the Idaho–Oregon border. Here the Snake River almost doubles in size as it receives several major tributaries – the Owyhee from the southwest, then the Boise and Payette rivers from the east, and further downstream the Malheur River from the west and Weiser River from the east. North of Boise, the Snake enters Hells Canyon, a steep, spectacular, rapid-strewn gorge that cuts through the Salmon River Mountains and Blue Mountains of Idaho and Oregon. Hells Canyon is one of the most rugged and treacherous portions of the course of the Snake River, posing a major obstacle for 19th-century American explorers. Here the Snake is also impounded by Hells Canyon, Oxbow, and Brownlee Dams, which together make up the Hells Canyon Hydroelectric Project.

At the halfway point in Hells Canyon, in one of the most remote and inaccessible sections of its course, the Snake River is joined from the east by its largest tributary, the Salmon River. From there, the Snake begins to form the Washington–Idaho border, receiving the Grande Ronde River from the west before receiving the Clearwater River from the east at Lewiston, which marks the head of navigation on the Snake. The river leaves Hells Canyon and turns west, winding through the Palouse Hills of eastern Washington. The Lower Snake River Project's four dams and navigation locks have transformed this part of the Snake River into a series of reservoirs. The confluence of the Snake and Columbia rivers at Burbank, Washington is part of Lake Wallula, the reservoir of McNary Dam. The Columbia River flows about further west to the Pacific Ocean near Astoria, Oregon.

As recently as 165 million years ago, most of western North America was still part of the Pacific Ocean. The nearly complete subduction of the Farallon Plate underneath the westward-moving North American Plate created the Rocky Mountains, which were pushed up by rising magma trapped between the sinking Farallon plate and the North American plate. As the North American Plate moved westwards over a stationary hotspot beneath the crust, a series of tremendous lava flows and volcanic eruptions carved out the Snake River Plain beginning about 12 million years ago, west of the Continental Divide. Even larger lava flows of Columbia River basalts issued over eastern Washington, forming the Columbia Plateau southeast of the Columbia River and the Palouse Hills in the lower Snake. Separate volcanic activity formed the northwestern portion of the plain, an area far from the path of the hotspot which now lies beneath Yellowstone National Park. At this point, the Snake River watershed was beginning to take shape.
The Snake River Plain and the gap between the Sierra Nevada and Cascade Range together formed a "moisture channel," opening the way for Pacific storms to travel more than inland to the headwaters of the Snake River. When the Teton Range uplifted about 9 million years ago along a detachment fault running north–south through the central Rockies, the river maintained its original course and cut through the southern end of the mountains, forming the Snake River Canyon of Wyoming. About 6 million years ago, the Salmon River Mountains and Blue Mountains at the far end of the plain began to rise; the river cut through these mountains as well, forming Hells Canyon. Lake Idaho, formed during the Miocene, covered a large portion of the Snake River Plain between Twin Falls and Hells Canyon, and its lava dam was finally breached about 2 million years ago.
Lava flowing from Cedar Butte in present southeast Idaho blocked the Snake River at Eagle Rock about 42,000 years ago, near the present-day site of American Falls Dam. A lake, known as American Falls Lake, formed behind the barrier. The lake was stable and survived for nearly 30,000 years. About 14,500 years ago, pluvial Lake Bonneville in the Great Salt Lake area, formed in the last glacial period, spilled catastrophically down the Portneuf River into the Snake in an event known as the Bonneville Flood. This was one of the first in a series of catastrophic flooding events in the Northwest known as the Ice Age Floods.

The deluge caused American Falls Lake to breach its natural lava dam, which was rapidly eroded with only the American Falls left in the end. The flood waters of Lake Bonneville, approximately twenty times the flow of the Columbia River or 5 million ft/s (140,000 m/s), swept down the Snake River and across the entirety of southern Idaho. For miles on either side of the river, flood waters stripped away soils and scoured the underlying basalt bedrock, forming the Snake River Canyon and creating Shoshone Falls, Twin Falls, Crane Falls, Swan Falls and other waterfalls along the Idaho section of the river. The Bonneville flood waters continued through Hells Canyon and eventually reached the Columbia River. The flood widened Hells Canyon but did not deepen it.
As the Bonneville Floods rushed down the Snake River, the Missoula Floods occurred in the same period, but originating farther north. The Missoula Floods, which occurred more than 40 times between 15,000 and 13,000 years ago, were caused by Glacial Lake Missoula on the Clark Fork repeatedly being impounded by ice dams then breaking through, with the lake's water rushing over much of eastern Washington in massive surges far larger than the Lake Bonneville Flood. These floods pooled behind the Cascade Range into enormous lakes and spilled over the northern drainage divide of the Snake River watershed, carving deep canyons through the Palouse Hills including the Palouse River canyon and Palouse Falls. The Lake Bonneville Floods and the Missoula Floods helped widen and deepen the Columbia River Gorge, a giant water gap which allows water from the Columbia and Snake rivers to take a direct route through the Cascade Range to the Pacific.

The massive amounts of sediment deposited by the Lake Bonneville Floods in the Snake River Plain also had a lasting effect on most of the middle Snake River. The high hydraulic conductivity of the mostly-basalt rocks in the plain led to the formation of the Snake River Aquifer, one of the most productive aquifers in North America. Many rivers and streams flowing from the north side of the plain sink into the aquifer instead of flowing into the Snake River, a group of watersheds called the lost streams of Idaho. The aquifer filled to hold nearly of water, underlying about in a plume thick. In places, water exits from rivers at rates of nearly . Much of the water lost by the Snake River as it transects the plain issues back into the river at its western end, by way of many artesian springs.

The Snake River is the thirteenth longest river in the United States. Its watershed is the 10th largest among North American rivers, and covers almost in portions of six U.S. states: Wyoming, Idaho, Nevada, Utah, Oregon, and Washington, with the largest portion in Idaho. Most of the Snake River watershed lies between the Rocky Mountains on the east and the Columbia Plateau on the northwest. The largest tributary of the Columbia River, the Snake River watershed makes up about 41% of the entire Columbia River Basin. Its average discharge at the mouth constitutes 31% of the Columbia's flow at that point. Above the confluence, the Snake is slightly longer than the Columbia— compared to —and its drainage basin is slightly larger—4% bigger than the upstream Columbia River watershed.

The mostly semi-arid, even desert climate of the Snake River watershed on average, receives less than of precipitation per year. However, precipitation in the Snake River watershed varies widely. At Twin Falls, in the center of the Snake River Plain, the climate is nearly desert, with an annual rainfall of just , although the average snowfall is . This desert climate occupies the majority of the basin of the Snake River, so although it is longer than the Columbia River above the Tri-Cities, its discharge is on average significantly less. However, in the high Rockies of Wyoming, in the upper Jackson Hole area, the average precipitation is over , and snowfall averages . Most of the Snake River basin consists of wide, arid plains and rolling hills, bordered by high mountains. In the upper parts of the watershed, however, the river flows through an area with a distinct alpine climate. There are also stretches where the river and its tributaries have incised themselves into tight gorges. The Snake River watershed includes parts of Yellowstone National Park, Grand Teton National Park, Hells Canyon National Recreation Area, and many other national and state parks.
Much of the area along the river, within a few miles of its banks, is irrigated farmland, especially in its middle and lower course. Irrigation dams include American Falls Dam, Minidoka Dam, and C.J. Strike Dam. Aside from water from the river, water is also pulled from the Snake River Aquifer for irrigation. Major cities along the river include Jackson in Wyoming, Twin Falls, Idaho Falls, Boise, and Lewiston in Idaho, and the Tri-Cities in Washington (Kennewick, Pasco and Richland). There are fifteen dams in total along the Snake River, which aside from irrigation, also produce electricity, maintain a navigation channel along part of the river's route, and provide flood control. However, fish passage is limited to the stretch below Hells Canyon.

The Snake River watershed is bounded by several other major North American watersheds, which drain both to the Atlantic or the Pacific, or into endorheic basins. On the southwest side a divide separates the Snake watershed from Oregon's Harney Basin, which is endorheic. On the south, the Snake watershed borders that of the Humboldt River in Nevada, and the watershed of the Great Salt Lake (the Bear, Jordan and Weber rivers) on the south.
The Snake River also shares a boundary with the Green River to the southeast; the Green River drains parts of Wyoming and Utah and is the largest tributary of the Colorado River. On the western extremity for a short stretch the Continental Divide separates the Snake watershed from the Bighorn River, a tributary of the Yellowstone River, which the Snake begins near. On the north the Snake River watershed is bounded by the Red Rock River, a tributary of the Beaverhead River, which flows into the Jefferson River and into the Missouri River, part of the Gulf of Mexico drainage basin.

The rest of the Snake River watershed borders on several other major Columbia River tributaries - mostly the Spokane River to the north, but also Clark Fork in Montana to the northeast and the John Day River to the west. Of these, the Clark Fork (via the Pend Oreille River) and the Spokane join the Columbia above the Snake, while the John Day joins downstream of the Snake, in the Columbia River Gorge. It is of note that the northeastern divide of the Snake River watershed forms the Idaho-Montana boundary, so the Snake River watershed does not extend into Montana.

Mountain ranges in the Snake watershed include the Teton Range, Bitterroot Range, Clearwater Mountains, Seven Devils Mountains, and the extreme northwestern end of the Wind River Range. Grand Teton is the highest point in the Snake River watershed, reaching in elevation. The elevation of the Snake River is when it joins the Columbia River.

Agricultural runoff from farms and ranches in the Snake River Plain and many other areas has severely damaged the ecology of the river throughout the 20th century. After the first irrigation dams on the river began operation in the first decade of the 20th century, much of the arable land in a strip a few miles wide along the Snake River was cultivated or turned to pasture, and agricultural return flows began to pollute the Snake. Runoff from several feedlots was dumped into the river until laws made the practice illegal. Fertilizer, manure and other chemicals and pollutants washed into the river greatly increase the nutrient load, especially of phosphorus, fecal coliforms and nitrogen. During low water, algae blooms occur throughout the calm stretches of the river, depleting its oxygen supply.
Much of the return flows do not issue directly back into the Snake River, but rather feed the Snake River Aquifer underneath the Snake River Plain. Water diverted from the river for irrigation, after absorbing any surface pollutants, re-enters the ground and feeds the aquifer. Although the aquifer has maintained its level, it has become increasingly laced with contaminants. Water in the aquifer eventually travels to the west side of the Snake River Plain and re-enters the river as springs. Throughout much of the Snake River Plain and Hells Canyon, excessive sediment is also a recurring problem. In December 2007, the U.S. Environmental Protection Agency (EPA) issued a permit requiring owners of fish farms along the Snake River to reduce their phosphorus discharge by 40%. Pollutant levels in Hells Canyon upstream of the Salmon River confluence, including that of water temperature, dissolved nutrients, and sediment, are required to meet certain levels.

The Snake River's average flow is . The United States Geological Survey recorded the river's discharge from a period of 1963–2000 at a stream gauge below Ice Harbor Dam. In that period, the largest average annual flow recorded was in 1997, and the lowest was in 1992. The lowest recorded daily mean flow was on February 4, 1979. On August 27, 1965, there was temporarily no flow as a result of testing at Ice Harbor Dam. The highest recorded flow was on June 19, 1974. The highest flow ever recorded on the Snake River was at a different USGS stream gauge near Clarkston, which operated from 1915 to 1972. This gauge recorded a maximum flow of —more than the Columbia's average discharge—on May 29, 1948. An even larger peak discharge, estimated at , occurred during the flood of June 1894.

The river's flow is also measured at several other points in its course. Above Jackson Lake, Wyoming, the discharge is about from a drainage area of . At Minidoka, Idaho, about halfway through the Snake River Plain, the river's discharge rises to . However, at Buhl, Idaho, only about downstream, the river's flow decreases to because of agricultural diversions and seepage. But at the border of Idaho and Oregon, near Weiser at the beginning of Hells Canyon, the Snake's flow rises to after receiving several major tributaries such as the Payette, Owyhee and Malheur. The discharge further increases to at Hells Canyon Dam on the border of Idaho and Oregon. At Anatone, Washington, downstream of the confluence with the Salmon, one of the Snake's largest tributaries, the mean discharge is .

Canadian explorer David Thompson first recorded the Native American name of the Snake River as "Shawpatin" when he arrived at its mouth by boat in 1800. When the Lewis and Clark Expedition crossed westwards into the Snake River watershed in 1805, they first gave it the name "Lewis River", "Lewis Fork" or "Lewis's Fork", as Meriwether Lewis was the first of their group to sight the river.
They also made note of the "Snake Indians" who lived along the river, who were actually the Shoshone tribe, and learned that the Native Americans called the river ' or ' (for an herb that grew prolifically along its banks).
Later American explorers, some of whom were originally part of the Lewis and Clark expedition, journeyed into the Snake River watershed and records show a variety of names have been associated with the river. The explorer Wilson Price Hunt of the Astor Expedition named the river as "Mad River". Others gave the river names including "Shoshone River" (after the tribe) and "Saptin River". Eventually, the name "Snake River" was derived from an S-shaped gesture the Shoshone tribe made with their hands to represent swimming salmon. Explorers misinterpreted it to represent a snake, giving the river its present-day name.

People have been living along the Snake River for at least 11,000 years. Historian Daniel S. Meatte divides the prehistory of the western Snake River Basin into three main phases or "adaptive systems". The first he calls "Broad Spectrum Foraging", dating from 11,500 to 4,200 years before present. During this period people drew upon a wide variety of food resources. The second period, "Semisedentary Foraging", dates from 4,200–250 years before present and is distinctive for an increased reliance upon fish, especially salmon, as well as food preservation and storage. The third phase, from 250 to 100 years before present, he calls "Equestrian Foragers". It is characterized by large horse-mounted tribes that spent long amounts of time away from their local foraging range hunting bison. In the eastern Snake River Plain there is some evidence of Clovis, Folsom, and Plano cultures dating back over 10,000 years ago.

Early fur traders and explorers noted regional trading centers, and archaeological evidence has shown some to be of considerable antiquity. One such trading center in the Weiser area existed as early as 4,500 years ago. The Fremont culture may have contributed to the historic Shoshones, but it is not well understood. Another poorly understood early cultural component is called the Midvale Complex. The introduction of the horse to the Snake River Plain around 1700 helped in establishing the Shoshone and Northern Paiute cultures.

On the Snake River in southeastern Washington there are several ancient sites. One of the oldest and most well-known is called the Marmes Rockshelter, which was used from over 11,000 years ago to relatively recent times. The Marmes Rockshelter was flooded in 1968 by Lake Herbert G. West, the Lower Monumental Dam's reservoir.

Eventually, two large Native American groups controlled most of the Snake River: the Nez Perce, whose territory stretched from the southeastern Columbia Plateau into northern Oregon and western Idaho, and the Shoshone, who occupied the Snake River Plain both above and below Shoshone Falls. Lifestyles along the Snake River varied widely. Below Shoshone Falls, the economy centered on salmon, who often came up the river in enormous numbers. Salmon were the mainstay of the Nez Perce and most of the other tribes below Shoshone Falls. Above the falls, life was significantly different. The Snake River Plain forms one of the only relatively easy paths across the main Rocky Mountains for many hundreds of miles, allowing Native Americans both east and west of the mountains to interact. As a result, the Shoshone centered on a trading economy.

According to legend, the Nez Perce tribe was first founded in the valley of the Clearwater River, one of the Snake River's lowermost major tributaries. At its height, there were at least 27 Nez Perce settlements along the Clearwater River and 11 more on the Snake between the mouth of the Clearwater and Imnaha Rivers. There were also villages on the Salmon River, Grande Ronde River, Tucannon River, and the lower Hells Canyon area. The Snake River's annual salmon run, which was estimated at that time to exceed four million in good years, supported the Nez Perce, who lived in permanent, well-defined villages, unlike the nomadic southeastern tribes along the Snake River. The Nez Perce also were involved in trade with the Flathead tribe to the north and other middle Columbia River tribes. However, they were enemies to the Shoshone and the other upstream Snake River tribes.

The Shoshone or Shoshoni were characterized by nomadic groups that took their culture from the earlier Bitterroot culture and Great Basin tribes that migrated north via the Owyhee River. They were the most powerful tribe in the Rocky Mountains area, and were known to many Great Plains tribes as the "Snakes". In the 18th century, Shoshone territory extended beyond the Snake River Plain, extending over the Continental Divide into the upper Missouri River watershed and even further north into Canada. A smallpox epidemic brought by European explorers and fur trappers was responsible for wiping out much of the Shoshone east of the Rocky Mountains, but the Shoshone continued to occupy the Snake River Plain. Eventually, the Shoshone culture merged with that of the Paiute and Bannock tribes, which came from the Great Basin and the Hells Canyon area, respectively. The Bannock brought with them the skill of buffalo hunting and horses they had acquired from Europeans, changing the Shoshone way of life significantly.

The Lewis and Clark Expedition (1804–06) was the first American group to cross the Rocky Mountains and sail down the Snake and Columbia rivers to the Pacific Ocean. Meriwether Lewis supposedly became the first American to sight the drainage basin of the Snake River after he crossed the mountains a few days ahead of his party on August 12, 1805, and sighted the Salmon River valley (a major Snake tributary) from Lemhi Pass, a few miles from the present-day site of Salmon, Idaho. The party later traveled north, descended the Lemhi River to the Salmon and attempted to descend it to the Snake, but found it impassable because of its violent rapids. The expedition named the Snake River the "Lewis River", "Lewis's River", or "Lewis Fork", in his honor, and they traveled northwards to the Lochsa River, which they traveled via the Clearwater River into the lower Snake, and into the Columbia. They also referred to the Shoshone Indians as the "Snake Indians", which became the present-day name of the river. The name "Lewis Fork", however, did not last.
Later American explorers traveled throughout the Snake River area and up its major tributaries beginning in 1806, just after Lewis and Clark had returned. The first was John Ordway in 1806, who also explored the lower Salmon River. John Colter in 1808 was the first to sight the upper headwaters of the Snake River, including the Jackson Hole area.
In 1810, Andrew Henry, along with a party of fur trappers, discovered the Henrys Fork of the Snake River, which is now named after him. Donald Mackenzie sailed the lower Snake River in 1811, and later explorers included Wilson Price Hunt of the Astor Expedition (who gave the river the name "Mad River"), Ramsay Crooks, Francisco Payelle, John Grey, Thyery Goddin, and many others after the 1830s. Many of these later explorers were original members of the Lewis and Clark Expedition who had returned to map and explore the area in greater detail. Even later, American fur trappers scouted the area for beaver streams, but Canadian trappers from the British Hudson's Bay Company were by now a major competitor.

The Hudson's Bay Company first sent fur trappers into the Snake River watershed in 1819. The party of three traveled into the headwaters of the Owyhee River, a major southern tributary of the Snake, but disappeared. Meanwhile, as American fur trappers kept coming to the region, the Hudson's Bay Company ordered the Canadian trappers to kill as many beavers as they could, eventually nearly eradicating the species from the Snake River watershed, under the "rationale [that] if there are no beavers, there will be no reason for the Yanks ([Americans]) to come." Their goal was to eventually gain rights over the Oregon Territory, a region covering Washington, Oregon, Idaho, and parts of Montana and Wyoming (most of the present-day region called the Pacific Northwest). However, the area was eventually annexed into the United States.

By the middle 19th century, the Oregon Trail had been established, generally following much of the Snake River. One crossing the trail made over the Snake River was near the present-day site of Glenns Ferry. Several years later, a ferry was established at the site, replacing the old system where pioneers had to ford the wide, powerful and deep Snake. Another place where pioneers crossed the Snake was further upstream, at a place called "Three Island Crossing", near the mouth of the Boise River. This area has a group of three islands (hence the name) that splits the Snake into four channels each about wide. Some emigrants chose to ford the Snake and proceed down the west side and recross the river near Fort Boise into Hells Canyon, continue down the drier east side into the gorge, or float the Snake and Columbia to the Willamette River, the destination of the Oregon Trail. The reason for the Three Island Crossing was the better availability of grass and water access. Numerous ferries have provided crossings of the upper Snake from the Brownlee Ferry at the head of Hell's Canyon to Menor's Ferry, which operates today at Moose, Wyoming. Sophistication varied from reed boats pulled by Indians on horse back at Snake Fort, Fort Boise, as described by Narcissa Whitman in 1836 to an electric operated ferry, the Swan Falls Ferry, at Swan Falls Dam of the early 20th century.

One contemporary diarist crossing near Salmon Falls complains of "exorbitant" fees at the crossings that were a "constant drain" on the travelers purse. She writes that this particular route was controlled by Mormons who had "built bridges where they were not needed-most unmercifully fleecing the poor emigrants". The diarist expresses regret at having made the crossing describing the landscape as "desolate country". Another writer similarly notes several days travel through "a desert so desolate and rocky that we almost regretted that we had not continued on the south side of that stream".

Unlike the Columbia River, it was far more difficult for steamboats to navigate on the Snake. The Columbia River drops from source to mouth, while the Snake drops over in elevation over a length more than shorter. Still, from the 1860s to the 1940s, steamboats traveled on the Snake River from its mouth at the Columbia River to near the mouth of the Imnaha River in lower Hells Canyon. However, most of the steamboats only sailed from the river's mouth to Lewiston, located at the confluence of the Snake and Clearwater rivers. This stretch of the river is the easiest to navigate for watercraft since it has the least elevation change, although it still contained over 60 sets of rapids.

Passenger and freight service downstream of Lewiston lasted throughout the late 19th century and persisted until the introduction of railroads in the Palouse Hills grain-growing region and ultimately, the construction of dams on the lower Snake to facilitate barge traffic, which caused the demise of both the steamboats and the railroad. Lewiston, from the confluence of the Snake and Columbia and from the mouth of the Columbia on the Pacific Ocean, became connected with Portland and other Pacific ports via steamboat service from the mouth of the Snake through the Columbia River Gorge. A commonly traveled route was from Wallula, Washington, downstream of the Snake River's mouth, upstream to Lewiston. The Oregon Steam Navigation Company launched the Shoshone at Fort Boise in 1866 which provided passenger and freight service on the upper Snake for the Boise and Owyhee mines.

By the 1870s, the OSN Company, owned by the Northern Pacific Railroad, was operating seven steamboats for transporting wheat and grain from the productive Palouse region along the Snake and Columbia to lower Columbia River ports. These boats were the "Harvest Queen", "John Gates", "Spokane", "Annie Faxon", "Mountain Queen", "R.R. Thompson", and "Wide West", all of which were built on the Columbia River. However, there were more resources along the Snake River than wheat and grain. In the 1890s, a huge copper deposit was discovered at Eureka Bar in Hells Canyon. Several ships were built specifically to transport ore from there to Lewiston: these included "Imnaha", "Mountain Gem", and "Norma". In 1893 the "Annie Faxon" suffered a boiler explosion and sank on the Snake below Lewiston.

A total of fifteen dams have been constructed along the Snake River for a multitude of different purposes, from its headwaters in the Rocky Mountains to its mouth on Lake Wallula, the reservoir formed behind McNary Dam on the Columbia River. Dams on the Snake can be grouped into three major categories. From its headwaters to the beginning of Hells Canyon, many small dams block the Snake to provide irrigation water. Between here and Hells Canyon, the first dam on the Snake, Swan Falls Dam, was built in 1901. In Hells Canyon, a cascade of dams produce hydroelectricity from the river's steep fall over a comparatively short distance. Finally, a third cascade of dams, from Hells Canyon to the mouth, facilitates navigation. Many different government and private agencies have worked to build dams on the Snake River, which now serve an important purpose for people living in the drainage basin and trade of agricultural products to Pacific seaports.

The Minidoka Irrigation Project of the U.S. Bureau of Reclamation, created with the passage of the Reclamation Act of 1902, involved the diversion of Snake River water into the Snake River Plain upstream of Shoshone Falls in order to irrigate approximately in the Snake River Plain and store of water in Snake River reservoirs.
The first studies for irrigation in the Snake River Plain were conducted by the United States Geological Survey in the late 19th century, and the project was authorized on April 23, 1904. The first dam constructed for the project was Minidoka Dam in 1904; its power plant began operating in 1909, producing 7 MW of electricity. This capacity was revised to 20 MW in 1993.

Jackson Lake Dam, far upstream in Wyoming's Grand Teton National Park, was built in 1907 to raise Jackson Lake for providing additional water storage in dry years. American Falls Dam, upstream of Minidoka, was completed in 1927 and replaced in 1978. As the dams were constructed above Shoshone Falls, the historical upriver limit of salmon and also a total barrier to boats and ships, no provisions were made for fish passage or navigation. Several other irrigation dams were also built - including Twin Falls Dam and Palisades Dam.
The Hells Canyon Project was built and maintained by Idaho Power Company starting in the 1940s, and was the second of the three major water projects on the river. The three dams of the project, Brownlee Dam, Oxbow Dam and Hells Canyon Dam, are located in upper Hells Canyon. All three dams are primarily for power generation and flood control, and do not have fish passage or navigation locks.

Brownlee Dam, the most upriver dam, was constructed in 1959, and generates 728 megawatts (MW). Oxbow Dam, the second dam in the project, was built in 1961 and generates 220 MW. The dam was named for a bend in the Snake River, shaped like an oxbow. Hells Canyon Dam was the last and most downriver of the three. It was constructed in 1967 and generates 450 MW.

Downriver of Hells Canyon is the Lower Snake River Project, authorized by the Rivers and Harbors Act of 1945 for the U.S. Army Corps of Engineers to create a navigable channel on the Snake River from its mouth to the beginning of Hells Canyon. These dams are, from upstream to downstream: Lower Granite Lock and Dam, Little Goose Lock and Dam, Lower Monumental Lock and Dam, and Ice Harbor Lock and Dam. Dredging work was also done throughout the length of the navigation channel to facilitate ship passage. These dams form a cascade of reservoirs with no stretches of free-flowing river in between. Immediately below Ice Harbor Dam is Lake Wallula, formed by the construction of the McNary Dam on the Columbia River. (McNary Dam is not part of the Lower Snake River Project.) Above Lower Granite Dam, the river channel from Lewiston to Johnson Bar, just below Hells Canyon, is also maintained for jet-boats as this section is too rugged for ships.

These dams have been proposed for removal, and if they were to be removed, it would be the largest dam removal project ever undertaken in the United States. The removal has been proposed on the grounds that it would restore salmon runs to the lower Snake River and the Clearwater River and other smaller tributaries. Idaho's Snake river once teemed with sockeye salmon. However, there are almost no wild sockeye salmon left in the river due to a number of factors.

There are many reasons why Sockeye Salmon in the Snake River are reduced in number. One reason is that the river runs through three different states, and is over long. Salmon swimming upstream in this river are faced with predators and dams. The Snake River has fifteen dams and is extremely difficult for salmon to access because of hydroelectric dams. Hell's Canyon Dam blocks passage to the entire upper Snake River. The Grand Coulee Dam also blocks spawning grounds to the famous "June Hogs" (legendary Chinook salmon that weighed over ).

Between 1985 and 2007, only an average of 18 sockeye salmon returned to Idaho each year. Serious conservation efforts by wildlife biologists and fish hatcheries have captured the few remaining wild sockeye salmon, collected their sperm and eggs, and in a laboratory, have them spawn. Instead of spawning naturally, these sockeye begin their lives in an incubator in a fishery biologist's laboratory. These baby salmon then are transported by ship, bypassing the dams. (The dams can hurt juvenile baby sockeye salmon with their powerful tides and currents, which suck the baby salmon down.) Another conservation effort that has helped the salmon recover, is the destruction of old, outdated dams, such as the Lewiston Dam on the Clearwater River, a tributary of the Snake. After destroying the dam, salmon populations noticeably recovered.

Another interesting recovery method conservationists and biologists are using is called Fish Transportation. Since many juvenile salmon perish at each dam while swimming out to the ocean, massive ships filter and collect these baby salmon by size and take them out to the ocean for a ride, where they can be guaranteed to make it alive to saltwater. This method raises controversy to the effectiveness and costs, since this method is extremely expensive, almost costing $15 million. Another possible upstream passage solution is the Whooshh Fish Transport System. Engineers at Whooshh Innovations have developed a fish passage system that allows for the safe and timely transportation of fish over barriers through a flexible tube system via volitional entry into the system.

Overall, these combined efforts have had good success. In the summer of 2006, the Snake River reportedly only had 3 sockeye salmon that returned to their spawning grounds. In the summer of 2013, more than 13,000 sockeye salmon returned to the spawning grounds.

It is found that over 60% of fisherman are in favor of dam removal on the Snake River.

In the 1960s and 1970s the U.S. Army Corps of Engineers built four dams and locks on the lower Snake River to facilitate shipping. The lower Columbia River has likewise been dammed for navigation. Thus a deep shipping channel through locks and slackwater reservoirs for heavy barges exists from the Pacific Ocean to Lewiston, Idaho. Most barge traffic originating on the Snake River goes to deep-water ports on the lower Columbia River, such as Portland. Grain, mostly wheat, is the main product shipped from the Snake, and nearly all of it is exported internationally from the lower Columbia River ports.

The shipping channel is authorized to be at least deep and wide. Where river depths were less than , the shipping channel has been dredged in most places. Dredging and redredging work is ongoing and actual depths vary over time. With a channel about deeper than the Mississippi River system, the Columbia and Snake rivers can float barges twice as heavy. Agricultural products from Idaho and eastern Washington are among the main goods transported by barge on the Snake and Columbia rivers. Grain, mainly wheat, accounts for more than 85% of the cargo barged on the lower Snake River. In 1998, over of grain were barged on the Snake. Before the completion of the lower Snake dams, grain from the region was transported by truck or rail to Columbia River ports around the Tri-Cities. Other products barged on the lower Snake River include peas, lentils, forest products, and petroleum.

The World Wide Fund for Nature (WWF) divides the Snake River's watershed into two freshwater ecoregions: the "Columbia Unglaciated" ecoregion and the "Upper Snake" ecoregion. Shoshone Falls marks the boundary between the two. The WWF placed the ecoregion boundary about downriver from Shoshone Falls in order to include the Big Wood River (the main tributary of the Malad River) in the Upper Snake ecoregion, because the Wood River is biologically distinct from the rest of the downriver Snake. Shoshone Falls has presented a total barrier to the upstream movement of fish for 30,000 to 60,000 years. As a result, only 35% of the fish fauna above the falls, and 40% of the Wood River's fish fauna, are shared with the lower Snake River.

The Upper Snake freshwater ecoregion includes most of southeastern Idaho and extends into small portions of Wyoming, Utah, and Nevada, including major freshwater habitats such as Jackson Lake. Compared to the lower Snake River and the rest of the Columbia River's watershed, the Upper Snake ecoregion has a high level of endemism, especially among freshwater molluscs such as snails and clams. There are at least 21 snail and clam species of special concern, including 15 that appear to exist only in single clusters. There are 14 fish species found in the Upper Snake region that do not occur elsewhere in the Columbia's watershed, but which do occur in Bonneville freshwater ecoregion of western Utah, part of the Great Basin and related to the prehistoric Lake Bonneville. The Wood River sculpin ("Cottus leiopomus") is endemic to the Wood River. The Shoshone sculpin ("Cottus greenei") is endemic to the small portion of the Snake River between Shoshone Falls and the Wood River.

The Snake River below Shoshone Falls is home to thirty-five native fish species, of which twelve are also found in the Columbia River and four of which are endemic to the Snake: the relict sand roller ("Percopsis transmontana") of the family Percopsidae, the shorthead sculpin ("Cottus confusus"), the maginated sculpin ("Cottus marginatus"), and the Oregon chub ("Oregonichthys crameri"). The Oregon chub is also found in the Umpqua River and nearby basins. The lower Snake River also supports seven species of Pacific salmon and trout ("Oncorhynchus"). There are also high, often localized levels of mollusc endemism, especially in Hells Canyon and the basins of the Clearwater River, Salmon River, and middle Snake River. The mollusc richness extends into the lower Columbia River and tributaries such as the Deschutes River.

Aside from aquatic species, much of the Snake River watershed supports larger animals including numerous species of mammals, birds, amphibians, and reptiles. Especially in the headwaters and the other mountainous areas strewn throughout the watershed, the gray wolf, grizzly bear, wolverine, mountain lion and Canada lynx are common. It has been determined that there are 97 species of mammals in the upper part of the Snake River, upstream from the Henrys Fork confluence. Pronghorn and bighorn sheep are common in the area drained by the "lost streams of Idaho", several rivers and large creeks that flow south from the Rocky Mountains and disappear into the Snake River Aquifer. About 274 bird species, some endangered or threatened, use the Snake River watershed, including bald eagle, peregrine falcon, whooping crane, greater sage-grouse, and yellow-billed cuckoo. Barrow's goldeneye are a species of bird that occurs commonly along the lower section of the Snake River.
Ten amphibian and twenty species of reptiles inhabit the upper Snake River's wetland and riparian zones. Several species of frogs are common in the "lost streams" basin and the northeasternmost part of the Snake River watershed, including the inland tailed frog, northern leopard frog, western toad, Columbia spotted frog, long-toed salamander, spadefoot toad. However, in the lower and middle portions of the Snake River watershed, several native species have been severely impacted by agriculture practices and the resulting non-native species supported by them. Introduced birds include the gray partridge, ring-necked pheasant, and chukar. Other non-native species include the bullfrog, brown-headed cowbird, and European starling, attracted by the construction of cities and towns.

The Snake River watershed includes a diversity of vegetation zones both past and present. A majority of the watershed was once covered with shrub-steppe grassland, most common in the Snake River Plain and also the Columbia Plateau in southeastern Washington. Riparian zones, wetlands and marshes once occurred along the length of the Snake River and its tributaries. In higher elevations, conifer forests, of which ponderosa pine is most common, dominate the landscape. The basin ranges from semi-desert to alpine climates, providing habitat for hundreds of species of plants. In the lowermost part of the watershed, in southeastern Washington, the Snake River is surrounded by an area called the Columbia Plateau Ecoprovince, which is now mostly occupied by irrigated farms. The rest of the Plateau area is characterized by low hills, dry lakes, and an arid, nearly desert climate.

The headwaters of the Snake River and the high mountains elsewhere in the watershed were historically heavily forested. These include aspen, Douglas fir, and spruce fir, comprising about 20% of the historic watershed. At the base of mountains and in the Lost River basin, sagebrush was and is the predominant vegetation cover. Because of deforestation, up to one quarter of the forests have been taken over by sagebrush, leaving the remaining forests to cover about 15% of the watershed. However, the lodgepole pine has increased in number, taking over historic stands of other conifers. There are also up to 118 species of rare or endemic plants that occur in the Snake River watershed.

The Snake River was once one of the most important rivers for the spawning of anadromous fish—which are hatched in the headwaters of rivers, live in the ocean for most of their lives, and return to the river to spawn—in the United States.
The river supported species including chinook salmon, coho salmon, and sockeye salmon, as well as steelhead, white sturgeon, and Pacific lamprey. It is known that before the construction of dams on the river, there were three major chinook salmon runs in the Snake River; in the spring, summer and fall, totaling about 120,000 fish, and the sockeye salmon run was about 150,000. The historical barrier to fish migration on the Snake River was Shoshone Falls, a waterfall that occurs as the Snake River passes through the Snake River Plain.
Since the early 20th century, when Swan Falls Dam was constructed on the middle Snake River upstream of Hells Canyon, the fifteen dams and reservoirs on the river have posed an increasing problem for migrating salmon. Agricultural lands and their resulting runoff have also had a significant impact on the success rate of migrating fish. Salmon can travel up the Snake River as far as Hells Canyon Dam, using the fish passage facilities of the four lower Snake River dams, leaving the Clearwater, Grande Ronde and Salmon river to sustain spawning salmon. Rising in several forks in the Clearwater Mountains of central Idaho, the Clearwater and Salmon River watersheds are nearly undeveloped with the enormous exception of Dworshak Dam on the North Fork Clearwater River. The watershed of the Grande Ronde in northeastern Oregon is also largely undeveloped. The four reservoirs formed by the lower Snake River dams—Lake Sacagawea, Lake Herbert G. West, Lake Bryan, and Lower Granite Lake—have also formed problems, as the downstream current in the pools is often not enough for the fish to sense, confusing their migration routes.

At the confluence of the Snake and Clearwater Rivers, young salmon that swim down from spawning gravels in the headwaters of the Clearwater River often delay their migrations because of a significant temperature difference. (Prior to the removal of Lewiston Dam on the main Clearwater and Grangeville Dam on the South Fork Clearwater, the Clearwater was completely unusable by migrating salmon.) Agricultural runoff and water held in reservoirs higher upstream on the Snake warm its waters as it flows through the Snake River Plain, so as the Snake meets the Clearwater, its average temperature is much higher. Directly below the confluence, the river flows into Lower Granite Lake, formed by Lower Granite Dam, the uppermost dam of the Lower Snake River Project. Paradoxically, the combination of these factors gives the young salmon further time to grow and to feed in Lower Granite Lake, so when they begin the migration to the Pacific Ocean, they often have a higher chance at survival, compared to those salmon who migrate to the ocean earlier.

A controversy has erupted since the late 20th century over the four lower Snake River dams, with the primary argument being that removing the dams would allow anadromous fish to reach the lower Snake River tributaries—the Clearwater River, the Tucannon River and the Grande Ronde River—and spawn in much higher numbers. However, removal of the dams has been fiercely opposed by some groups in the Pacific Northwest. Because much of the electricity in the Northwest comes from dams, removing the four dams would create a hole in the energy grid that would not be immediately replaceable. Navigation on the lower Snake would also suffer, as submerged riffles, rapids and islands would be exposed by the removal of the dams. Irrigation pumps for fields in southeastern Washington would also have to reach further to access the water of the Snake River. However, aside from restoring salmon runs, dam removal proponents argue that the power is replaceable, that the grain transportation system could be replaced by railroads, and that only one of the four reservoirs supplies irrigation water. Irrigators in the Snake River Plain would likely need to allow less water into the Snake River during low flow in order to create a current in the four lower reservoirs, and recreation and tourism would likely benefit.

The Salmon River is the second largest tributary. Although the Salmon has a larger drainage than the Clearwater, the Salmon drains much drier country and therefore has a smaller discharger than the Clearwater, about annually compared to about annually for the Clearwater River.

The Snake River has over 20 major tributaries, most of which are in the mountainous regions of the basin. The largest by far is the Clearwater River, which drains in north central Idaho. Many of the rivers that flow into the Snake River Plain from the north sink into the Snake River Aquifer, but still contribute their water to the river. Aside from rivers, the Snake is fed by many significant springs, many of which arise from the aquifer on the west side of the plain.



</doc>
<doc id="27983" url="https://en.wikipedia.org/wiki?curid=27983" title="Surd">
Surd




</doc>
<doc id="27984" url="https://en.wikipedia.org/wiki?curid=27984" title="Strong interaction">
Strong interaction

In nuclear physics and particle physics, the strong interaction is the mechanism responsible for the strong nuclear force, and is one of the four known fundamental interactions, with the others being electromagnetism, the weak interaction, and gravitation. At the range of 10 m (1 femtometer), the strong force is approximately 137 times as strong as electromagnetism, a million times as strong as the weak interaction, and 10 times as strong as gravitation. The strong nuclear force holds most ordinary matter together because it confines quarks into hadron particles such as the proton and neutron. In addition, the strong force binds these neutrons and protons to create atomic nuclei. Most of the mass of a common proton or neutron is the result of the strong force field energy; the individual quarks provide only about 1% of the mass of a proton.

The strong interaction is observable at two ranges and mediated by two force carriers. On a larger scale (about 1 to 3 fm), it is the force (carried by mesons) that binds protons and neutrons (nucleons) together to form the nucleus of an atom. On the smaller scale (less than about 0.8 fm, the radius of a nucleon), it is the force (carried by gluons) that holds quarks together to form protons, neutrons, and other hadron particles. In the latter context, it is often known as the color force. The strong force inherently has such a high strength that hadrons bound by the strong force can produce new massive particles. Thus, if hadrons are struck by high-energy particles, they give rise to new hadrons instead of emitting freely moving radiation (gluons). This property of the strong force is called color confinement, and it prevents the free "emission" of the strong force: instead, in practice, jets of massive particles are produced.

In the context of atomic nuclei, the same strong interaction force (that binds quarks within a nucleon) also binds protons and neutrons together to form a nucleus. In this capacity it is called the nuclear force (or "residual strong force"). So the residuum from the strong interaction within protons and neutrons also binds nuclei together. As such, the residual strong interaction obeys a distance-dependent behavior between nucleons that is quite different from that when it is acting to bind quarks within nucleons. Additionally, distinctions exist in the binding energies of the nuclear force of nuclear fusion vs nuclear fission. Nuclear fusion accounts for most energy production in the Sun and other stars. Nuclear fission allows for decay of radioactive elements and isotopes, although it is often mediated by the weak interaction. Artificially, the energy associated with the nuclear force is partially released in nuclear power and nuclear weapons, both in uranium or plutonium-based fission weapons and in fusion weapons like the hydrogen bomb.

The strong interaction is mediated by the exchange of massless particles called gluons that act between quarks, antiquarks, and other gluons. Gluons are thought to interact with quarks and other gluons by way of a type of charge called color charge. Color charge is analogous to electromagnetic charge, but it comes in three types (±red, ±green, ±blue) rather than one, which results in a different type of force, with different rules of behavior. These rules are detailed in the theory of quantum chromodynamics (QCD), which is the theory of quark–gluon interactions.

Before the 1970s, physicists were uncertain as to how the atomic nucleus was bound together. It was known that the nucleus was composed of protons and neutrons and that protons possessed positive electric charge, while neutrons were electrically neutral. By the understanding of physics at that time, positive charges would repel one another and the positively charged protons should cause the nucleus to fly apart. However, this was never observed. New physics was needed to explain this phenomenon.

A stronger attractive force was postulated to explain how the atomic nucleus was bound despite the protons' mutual electromagnetic repulsion. This hypothesized force was called the "strong force", which was believed to be a fundamental force that acted on the protons and neutrons that make up the nucleus.

It was later discovered that protons and neutrons were not fundamental particles, but were made up of constituent particles called quarks. The strong attraction between nucleons was the side-effect of a more fundamental force that bound the quarks together into protons and neutrons. The theory of quantum chromodynamics explains that quarks carry what is called a color charge, although it has no relation to visible color. Quarks with unlike color charge attract one another as a result of the strong interaction, and the particle that mediated this was called the gluon.

The word "strong" is used since the strong interaction is the "strongest" of the four fundamental forces. At a distance of 1 femtometer (1 fm = 10 meters) or less, its strength is around 137 times that of the electromagnetic force, some 10 times as great as that of the weak force, and about 10 times that of gravitation.

The strong force is described by quantum chromodynamics (QCD), a part of the standard model of particle physics. Mathematically, QCD is a non-Abelian gauge theory based on a local (gauge) symmetry group called SU(3).

The force carrier particle of the strong interaction is the gluon, a massless boson. Unlike the photon in electromagnetism, which is neutral, the gluon carries a color charge. Quarks and gluons are the only fundamental particles that carry non-vanishing color charge, and hence they participate in strong interactions only with each other. The strong force is the expression of the gluon interaction with other quark and gluon particles.

All quarks and gluons in QCD interact with each other through the strong force. The strength of interaction is parameterized by the strong coupling constant. This strength is modified by the gauge color charge of the particle, a group theoretical property.

The strong force acts between quarks. Unlike all other forces (electromagnetic, weak, and gravitational), the strong force does not diminish in strength with increasing distance between pairs of quarks. After a limiting distance (about the size of a hadron) has been reached, it remains at a strength of about 10,000 newtons (N), no matter how much farther the distance between the quarks. As the separation between the quarks grows, the energy added to the pair creates new pairs of matching quarks between the original two; hence it is impossible to create separate quarks. The explanation is that the amount of work done against a force of 10,000 newtons is enough to create particle–antiparticle pairs within a very short distance of that interaction. The very energy added to the system required to pull two quarks apart would create a pair of new quarks that will pair up with the original ones. In QCD, this phenomenon is called color confinement; as a result only hadrons, not individual free quarks, can be observed. The failure of all experiments that have searched for free quarks is considered to be evidence of this phenomenon.

The elementary quark and gluon particles involved in a high energy collision are not directly observable. The interaction produces jets of newly created hadrons that are observable. Those hadrons are created, as a manifestation of mass–energy equivalence, when sufficient energy is deposited into a quark–quark bond, as when a quark in one proton is struck by a very fast quark of another impacting proton during a particle accelerator experiment. However, quark–gluon plasmas have been observed.

It is not the case that every quark in the universe attracts every other quark in the above distance independent manner. Color confinement implies that the strong force acts without distance-diminishment only between pairs of quarks, and that in collections of bound quarks (hadrons), the net color-charge of the quarks essentially cancels out, resulting in a limit of the action of the forces. Collections of quarks (hadrons) therefore appear nearly without color-charge, and the strong force is therefore nearly absent between those hadrons. However, the cancellation is not quite perfect, and a residual force (described below) remains. This residual force "does" diminish rapidly with distance, and is thus very short-range (effectively a few femtometers). It manifests as a force between the "colorless" hadrons, and is sometimes known as the strong nuclear force or simply nuclear force.
The nuclear force acts between hadrons, known as mesons and baryons. This "residual strong force", acting indirectly, transmits gluons that form part of the virtual "π" and "ρ" mesons, which, in turn, transmit the force between nucleons that holds the nucleus (beyond protium) together.

The residual strong force is thus a minor residuum of the strong force that binds quarks together into protons and neutrons. This same force is much weaker "between" neutrons and protons, because it is mostly neutralized "within" them, in the same way that electromagnetic forces between neutral atoms (van der Waals forces) are much weaker than the electromagnetic forces that hold electrons in association with the nucleus, forming the atoms.

Unlike the strong force itself, the residual strong force, "does" diminish in strength, and it in fact diminishes rapidly with distance. The decrease is approximately as a negative exponential power of distance, though there is no simple expression known for this; see Yukawa potential. The rapid decrease with distance of the attractive residual force and the less-rapid decrease of the repulsive electromagnetic force acting between protons within a nucleus, causes the instability of larger atomic nuclei, such as all those with atomic numbers larger than 82 (the element lead).

Although the nuclear force is weaker than strong interaction itself, it is still highly energetic: transitions produce gamma rays. The mass of a nucleus is significantly different from the summed masses of the individual nucleons. This mass defect is due to the potential energy associated with the nuclear force. Differences between mass defects power nuclear fusion and nuclear fission.

The so-called Grand Unified Theories (GUT) aim to describe the strong interaction and the electroweak interaction as aspects of a single force, similarly to how the electromagnetic and weak interactions were unified by the Glashow–Weinberg–Salam model into the electroweak interaction. The strong interaction has a property called asymptotic freedom, wherein the strength of the strong force diminishes at higher energies (or temperatures). The theorized energy where its strength becomes equal to the electroweak interaction is the grand unification energy. However, no Grand Unified Theory has yet been successfully formulated to describe this process, and Grand Unification remains an unsolved problem in physics.

If GUT is correct, after the Big Bang and during the electroweak epoch of the universe, the electroweak force separated from the strong force. Accordingly, a grand unification epoch is hypothesized to have existed prior to this.




</doc>
<doc id="27989" url="https://en.wikipedia.org/wiki?curid=27989" title="September 3">
September 3





</doc>
<doc id="27990" url="https://en.wikipedia.org/wiki?curid=27990" title="September 5">
September 5





</doc>
<doc id="27991" url="https://en.wikipedia.org/wiki?curid=27991" title="Stout">
Stout

Stout is a dark, top-fermented beer with a number of variations, including dry stout, oatmeal stout, milk stout, and imperial stout.

The first known use of the word "stout" for beer was in a document dated 1677 found in the Egerton Manuscripts, the sense being that a "stout beer" was a "strong" beer, not a "dark" beer. The name "porter" was first used in 1721 to describe a dark brown beer that had been made with roasted malts. Because of the huge popularity of porters, brewers made them in a variety of strengths. The stronger beers, typically 7% or 8% alcohol by volume (ABV), were called "stout porters", so the history and development of stout and porter are intertwined, and the term stout has become firmly associated with dark beer, rather than just strong beer.

Porter originated in London, England in the early 1720s. The style quickly became popular in the City especially with porters (hence its name): it had a strong flavour, took longer to spoil than other beers, was significantly cheaper than other beers, and was not easily affected by heat. Within a few decades, porter breweries in London had grown "beyond any previously known scale". Large volumes were exported to Ireland and by 1776 it was being brewed by Arthur Guinness at his St. James's Gate Brewery. In the 19th century, the beer gained its customary black colour through the use of black patent malt, and became stronger in flavour.

Originally, the adjective "stout" meant "proud" or "brave", but later, after the 14th century, it took on the connotation of "strong". The first known use of the word "stout" for beer was in a document dated 1677 found in the Egerton Manuscript, the sense being that a stout beer was a strong beer. The expression "stout porter" was applied during the 18th century to strong versions of porter. "Stout" still meant only "strong" and it could be related to any kind of beer, as long as it was strong: in the UK it was possible to find "stout pale ale", for example. Later, "stout" was eventually to be associated only with porter, becoming a synonym of dark beer.

Because of the huge popularity of porters, brewers made them in a variety of strengths. The beers with higher gravities were called "Stout Porters". There is still division and debate on whether stouts should be a separate style from porter. Usually the only deciding factor is strength.

"Nourishing" and sweet "milk" stouts became popular in Great Britain in the years following the First World War, though their popularity declined towards the end of the 20th century, apart from pockets of local interest such as in Glasgow with Sweetheart Stout.

Beer writer Michael Jackson wrote about stouts and porters in the 1970s, but in the mid 1980s a survey by "What’s Brewing" found just 29 brewers in the UK and Channel Islands still making stout, most of them milk stouts. In the 21st century, stout is making a comeback with a new generation of drinkers, thanks to new products from burgeoning craft and regional brewers.

"Milk stout" (also called "sweet stout" or "cream stout") is a stout containing lactose, a sugar derived from milk. Because lactose cannot be fermented by beer yeast, it adds sweetness and body to the finished beer. Milk stout was claimed to be nutritious, and was given to nursing mothers, along with other stouts. Milk stout was also said to be prescribed by doctors to help nursing mothers increase their milk production. The classic surviving example of milk stout is Mackeson's, for which the original brewers advertised that "each pint contains the energising carbohydrates of 10 ounces [280 ml] of pure dairy milk". The style was rare until being revived by a number of craft breweries during the craft beer boom of the twenty-first century.

It is widely reported that, in the period just after the Second World War when rationing was in place, the British government required brewers to remove the word "milk" from labels and adverts, and any imagery associated with milk. However, no specific legislation or orders have been found to support this, though there were some prosecutions in Newcastle upon Tyne in 1944 under the Food and Drugs Act 1938 regarding misleading labelling.

With milk or sweet stout becoming the dominant stout in the UK in the early 20th century, it was mainly in Ireland that the non-sweet or standard stout was being made. As standard stout has a drier taste than the English and American sweet stouts, they came to be called "dry stout" or "Irish stout" to differentiate them from stouts with added lactose or oatmeal. This is the style that represents a typical stout to most people. The best selling stouts worldwide are Irish stouts made by Guinness (currently owned by Diageo) at St. James's Gate Brewery (also known as the Guinness Brewery) in Dublin. Guinness makes a number of different varieties of its Irish stouts. Other examples of Irish dry stout include Murphy's and Beamish. Draught Irish stout is normally served with a nitrogen propellant, (rather than carbon dioxide as most beers use) to create a creamy texture with a long-lasting head. Some canned and bottled stouts include a special device called a "widget" to nitrogenate the beer in the container to replicate the experience of the keg varieties.

While there is a great deal of disagreement in the brewing world on this subject, there are no differences between stout and porter historically, though there has been a tendency for breweries to differentiate the strengths of their dark beers with the words "extra", "double" and "stout". The term "stout" was initially used to indicate a stronger porter than other porters issued by an individual brewery. Though not consistent, this is the usage that was most commonly employed. More recently, "stout" tends to be used to describe dry stouts (containing a small amount of unmalted roast barley), or sweet stouts (such as milk stouts), whilst "porter" describes a beer flavoured with roast malted barley.

"Oatmeal stout" is a stout with a proportion of oats, normally a maximum of 30%, added during the brewing process. Even though a larger proportion of oats in beer can lead to a bitter or astringent taste, during the medieval period in Europe, oats were a common ingredient in ale, and proportions up to 35% were standard. Despite some areas of Europe, such as Norway, still clinging to the use of oats in brewing until the early part of the 20th century, the practice had largely died out by the 16th century, so much so that in 1513 Tudor sailors refused to drink oat beer offered to them because of the bitter flavour.

There was a revival of interest in using oats during the end of the 19th century, when (supposedly) restorative, nourishing and invalid beers, such as the later milk stout, were popular, because of the association of porridge with health. Maclay of Alloa produced an Original Oatmalt Stout in 1895 which used 70% "oatmalt", and a 63/- Oatmeal Stout in 1909, which used 30% "flaked (porridge) oats".

In the 20th century many oatmeal stouts contained only a minimal amount of oats. For example, in 1936 Barclay Perkins Oatmeal Stout used only 0.5% oats. As the oatmeal stout was parti-gyled with their porter and standard stout, these two also contained the same proportion of oats. (Parti-gyle brewing involves blending the worts drawn from multiple mashes or sparges after the boil to produces beers of different gravities.) The name seems to have been a marketing device more than anything else. In the 1920s and 1930s Whitbread's London Stout and Oatmeal Stout were identical, just packaged differently. The amount of oats Whitbread used was minimal, again around 0.5%. With such a small quantity of oats used, it could only have had little impact on the flavour or texture of these beers.

Many breweries were still brewing oatmeal stouts in the 1950s, for example Brickwoods in Portsmouth, Matthew Brown in Blackburn and Ushers in Trowbridge. When Michael Jackson mentioned the defunct Eldrige Pope "Oat Malt Stout" in his 1977 book "The World Guide to Beer", oatmeal stout was no longer being made anywhere, but Charles Finkel, founder of Merchant du Vin, was curious enough to commission Samuel Smith to produce a version. Samuel Smith's Oatmeal Stout then became the template for other breweries' versions.

Oatmeal stouts do not usually taste specifically of oats. The smoothness of oatmeal stouts comes from the high content of proteins, lipids (includes fats and waxes), and gums imparted by the use of oats. The gums increase the viscosity and body adding to the sense of smoothness.

Oysters have had a long association with stout. When stouts were emerging in the 18th century, oysters were a commonplace food often served in public houses and taverns. By the 20th century, oyster beds were in decline, and stout had given way to pale ale. Ernest Barnes came up with the idea of combining oysters with stout using an oyster concentrate made by Thyrodone Development Ltd. in Bluff, New Zealand, where he was factory manager. It was first sold by the Dunedin Brewery Company in New Zealand in 1938, with the Hammerton Brewery in London, UK, beginning production using the same formula the following year. Hammerton Brewery was re-established in 2014 and is once again brewing an oyster stout.

Modern "oyster stouts" may be made with a handful of oysters in the barrel, hence the warning by one establishment, the Porterhouse Brewery in Dublin, that their award-winning Oyster Stout was not suitable for vegetarians. Others, such as Marston's Oyster Stout, use the name with the implication that the beer would be suitable for drinking with oysters.

"Chocolate stout" is a name brewers sometimes give to certain stouts having a noticeable dark chocolate flavour through the use of darker, more aromatic malt; particularly chocolate malt—a malt that has been roasted or kilned until it acquires a chocolate colour. Sometimes, as with Muskoka Brewery's Double Chocolate Cranberry Stout, Young's Double Chocolate Stout, and Rogue Brewery's Chocolate Stout, the beers are also brewed with a small amount of chocolate or chocolate flavouring.

Imperial stout, also known as "Russian Imperial stout", is a strong dark beer in the style that was brewed in the 18th century by Thrale's Anchor Brewery in London for export to the court of Catherine II of Russia. In 1781 the brewery changed hands and the beer became known as "Barclay Perkins Imperial Brown Stout". It was shipped to Russia by Albert von Le Coq who was awarded a Russian royal warrant which entitled him to use the name "Imperial". A recipe from 1856 shows it had an original gravity of 1.107 (almost certainly over 10% abv) and over 10 pounds of hops to the barrel. When Barclay's brewery was taken over by Courage in 1955, the beer was renamed "Courage Imperial Russian Stout" and it was brewed sporadically until 1993.

In Canada, Imperial Stout was produced in Prince Albert first by Fritz Sick, and then by Molson following a 1958 takeover. Denmark's Wiibroe Brewery launched its 8.2 percent Imperial Stout in 1930. The first brewery to brew an Imperial Stout in the United States was Bert Grant's Yakima Brewing.

Imperial stouts have a high alcohol content, usually over 9% abv, and are among the darkest available beer styles. Samuel Smith's brewed a version for export to the United States in the early 1980s, and today imperial stout is among the most popular beer styles with U.S. craft brewers. American interpretations of the style often include ingredients such as vanilla beans, chili powder, maple syrup, coffee, and marshmallows. Many are aged in bourbon barrels to add additional layers of flavour. The word "Imperial" is now commonly added to other beer styles to denote a stronger version, hence Imperial IPAs, Imperial pilsners etc.

Baltic porter is a version of imperial stout which originated in the Baltic region in the 19th century. Imperial stouts imported from Britain were recreated locally using local ingredients and brewing traditions.


</doc>
<doc id="27992" url="https://en.wikipedia.org/wiki?curid=27992" title="Slavery">
Slavery

Slavery and enslavement are the state and condition of being a slave. A person is enslaved when a slaver coerces him or her into working for them and is deprived of the opportunity to leave. In chattel slavery, the enslaved person is legally rendered the personal property (chattel) of the slave owner. In economics, the term "de facto slavery" describes the conditions of unfree labour and forced labour, where people are forced or compelled to work against their will. In the course of human history, slavery was often a feature of civilisation and legal in most societies, but is now outlawed in most countries of the world.

In 2019, approximately 40 million people, of whom 26 percent were children, were enslaved throughout the world. In the modern world, more than 50 percent of enslaved people provide forced labor, usually in the factories and sweatshops of the private sector of a country's economy. In the industrialised countries, human trafficking is the modern variety of slavery; in the unindustrialised countries, enslavement by debt bondage is a common form of enslaving a person, such as captive domestic servants, forced marriage, and child soldiers.

The word "slave" is derived from the ethnonym (ethnic name) Slav. It arrived in English via the Old French "sclave". In Medieval Latin the word was "sclavus" and in Byzantine Greek σκλάβος. Use of the word arose during the Early Medieval Period, when Slavs from Central and Eastern Europe ("Saqaliba") were frequently enslaved by Moors from the Iberian Peninsula and North Africa. An older interpretation connected "slave" to the Greek verb "skyleúo" 'to strip a slain enemy'.

There is a dispute among historians about whether terms such as "unfree labourer" or enslaved person, rather than "slave", should be used when describing the victims of slavery. According to those proposing a change in terminology, "slave" perpetuates the crime of slavery in language; by reducing its victims to a nonhuman noun instead of "carry[ing] them forward as people, not the property that they were". Other historians prefer "slave" because the term is familiar and shorter, or because it accurately reflects the inhumanity of slavery, with "person" implying a degree of autonomy that slavery does not allow.

Indenture, otherwise known as bonded labour or debt bondage, is a form of unfree labour under which a person pledges himself or herself against a loan. The services required to repay the debt, and their duration, may be undefined. Debt bondage can be passed on from generation to generation, with children required to pay off their progenitors' debt. It is the most widespread form of slavery today. Debt bondage is most prevalent in South Asia.

As a social institution, chattel slavery (traditional slavery) denies the human agency of people, by legalistically dehumanising them into "chattels" (personal property) owned by the slaver; therefore slaves give birth to slaves; the children of slaves are born enslaved, by way of the legalistic philosophy of "partus sequitur ventrem" (That which is brought forth follows the belly). They are also bought and sold at will, as a result.Although chattel slavery was the usual form of enslavement in most societies that practiced slavery throughout human history, since the 19th century, this form of slavery was formally abolished.

"Slavery" has also been used to refer to a legal state of dependency to somebody else. For example, in Persia, the situations and lives of such slaves could be better than those of common citizens.

Forced labour, or unfree labour, is sometimes used to describe an individual who is forced to work against their own will, under threat of violence or other punishment, but the generic term unfree labour is also used to describe chattel slavery, as well as any other situation in which a person is obliged to work against their own will, and a person's ability to work productively is under the complete control of another person. This may also include institutions not commonly classified as slavery, such as serfdom, conscription and penal labour. While some unfree labourers, such as serfs, have substantive, "de jure" legal or traditional rights, they also have no ability to terminate the arrangements under which they work and are frequently subject to forms of coercion, violence, and restrictions on their activities and movement outside their place of work.

Human trafficking primarily involves women and children forced into prostitution and is the fastest growing form of forced labour, with Thailand, Cambodia, India, Brazil and Mexico having been identified as leading hotspots of commercial sexual exploitation of children. Examples of sexual slavery, often in military contexts, include detention in "rape camps" or "comfort stations," "comfort women", forced "marriages" to soldiers and other practices involving the treatment of women or men as chattel and, as such, violations of the peremptory norm prohibiting slavery.

In 2007, Human Rights Watch estimated that 200,000 to 300,000 children served as soldiers in current conflicts. More girls under 16 work as domestic workers than any other category of child labor, often sent to cities by parents living in rural poverty such as in restaveks in Haiti.

Forced marriages or early marriages are often considered types of slavery. Forced marriage continues to be practiced in parts of the world including some parts of Asia and Africa and in immigrant communities in the West. Sacred prostitution is where girls and women are pledged to priests or those of higher castes, such as the practice of Devadasi in South Asia or fetish slaves in West Africa. Marriage by abduction occurs in many places in the world today, with a national average of 69% of marriages in Ethiopia being through abduction.

Economists have attempted to model the circumstances under which slavery (and variants such as serfdom) appear and disappear. One observation is that slavery becomes more desirable for landowners where land is abundant but labour is scarce, such that rent is depressed and paid workers can demand high wages. If the opposite holds true, then it becomes more costly for landowners to have guards for the slaves than to employ paid workers who can only demand low wages because of the amount of competition. Thus, first slavery and then serfdom gradually decreased in Europe as the population grew but were reintroduced in the Americas and in Russia as large areas of new land with few people became available.

Slavery is more common when the labor done is relatively simple and thus easy to supervise, such as large-scale growing of a single crop, like sugar and cotton, in which output was based on economies of scale. This enables such systems of labor, such as the gang system in the United States, to become prominent on large plantations where field hands were monitored and worked with factory-like precision. For example, each work gang was based on an internal division of labour that assigned every member of the gang to a precise task and simultaneously made their own performance dependent on the actions of the others. The hoe hands chopped out the weeds that surrounded the cotton plants as well as excessive sprouts. The plow gangs followed behind, stirring the soil near the rows of cotton plants and tossing it back around the plants. Thus, the gang system worked like an assembly line.

Since the 18th century, critics have argued that slavery tends to retard technological advancement because the focus is on increasing the number of slaves doing simple tasks rather than upgrading the efficiency of labour. For example, it is sometime argued that, because of this narrow focus, theoretical knowledge and learning in Greece – and later in Rome – was not applied to ease physical labour or improve manufacturing.
Scottish economist Adam Smith states that free labour was economically better than slave labour, and that it is nearly impossible to end slavery in a free, democratic, or republican form of government since many of its legislators or political figures were slave owners, and would not punish themselves. He further states that slaves would be better able to gain their freedom when there was centralized government, or a central authority like a king or the church. Similar arguments appear later in the works of Auguste Comte, especially when it comes to Smith's belief in the separation of powers, or what Comte called the "separation of the spiritual and the temporal" during the Middle Ages and the end of slavery, and Smith's criticism of masters, past and present. As Smith states in the "Lectures on Jurisprudence", "The great power of the clergy thus concurring with that of the king set the slaves at liberty. But it was absolutely necessary both that the authority of the king and of the clergy should be great. Where ever any one of these was wanting, slavery still continues..."

Worldwide, slavery is a criminal offense, but slave owners can get very high returns for their risk. According to researcher Siddharth Kara, the profits generated worldwide by all forms of slavery in 2007 were $91.2 billion. That is second only to drug trafficking, in terms of global criminal enterprises. Currently, the weighted average global sales price of a slave is calculated to be approximately $340, with a high of $1,895 for the average trafficked sex slave, and a low of $40 to $50 for debt bondage slaves in part of Asia and Africa. The weighted average annual profits generated by a slave in 2007 was $3,175, with a low of an average $950 for bonded labor and $29,210 for a trafficked sex slave. Approximately 40% of slave profits each year are generated by trafficked sex slaves, representing slightly more than 4% of the world's 29 million slaves.

Throughout history, slaves were clothed in a distinctive fashion, particularly with respect to the frequent lack of footwear, as they were rather commonly forced to go barefoot. This was partly because of economic reasons but also served as a distinguishing feature, especially in South Africa and South America. For example, the Cape Town slave code stated that "Slaves must go barefoot and must carry passes." It also puts slaves at a physical disadvantage because of the lack of protection against environmental adversities and also in situations of possible confrontation, thereby making it more difficult to escape or to rebel against their owners.

This was the case in the majority of states that abolished slavery later in history, as most images from the respective historical period suggest that slaves were barefoot.
To quote Brother Riemer (1779): "[the slaves] are, even in their most beautiful suit, obliged to go barefoot. Slaves were forbidden to wear shoes. This was a prime mark of distinction between the free and the bonded and no exceptions were permitted."

According to the Bible, shoes have been considered badges of freedom since antiquity: "But the father said to his servants, Bring forth the best robe, and put [it] on him; and put a ring on his hand, and shoes on [his] feet" (). This aspect can be viewed as an informal law in areas where slavery existed as any person sighted barefoot in public would be conclusively regarded as a slave.

In certain societies this rule is valid to this day. As with the Tuareg, where slavery is still unofficially practiced, their slaves are constantly forced to remain barefoot as a recognition mark. Mainly through their bare feet their societal status and rank opposite their owners is displayed to the public in a plainly visible way.

Another widespread practice was branding the slaves either to generally mark them as property or as punishment usually reserved for fugitives.

Some scholars differentiate between ancient forms of slavery and the large-scale, largely race-based slavery which grew to immense proportions starting in the 14th century. The first type of slavery, sometimes called "just title servitude," was suffered by prisoners of war, debtors, and other vulnerable people. The second, race-based type of slavery was argued even by some contemporary writers to be intrinsically immoral.

Evidence of slavery predates written records and has existed in many cultures. Slavery is rare among hunter-gatherer populations because it requires economic surpluses and a high population density to be viable. Thus, although it has existed among unusually resource-rich hunter gatherers, such as the American Indian peoples of the salmon-rich rivers of the Pacific Northwest Coast, slavery became widespread only with the invention of agriculture during the Neolithic Revolution about 11,000 years ago.

In the earliest known records, slavery is treated as an established institution. The Code of Hammurabi (c. 1760 BC), for example, prescribed death for anyone who helped a slave escape or who sheltered a fugitive. The Bible mentions slavery as an established institution. Slavery was known in almost every ancient civilization and society. Such institutions included debt bondage, punishment for crime, the enslavement of prisoners of war, child abandonment, and the birth of slave children to slaves.

Slavery existed in Pharaonic Egypt, but studying it is complicated by terminology used by the Egyptians to refer to different classes of servitude over the course of history. Interpretation of the textual evidence of classes of slaves in ancient Egypt has been difficult to differentiate by word usage alone. There were three apparent types of enslavement in Ancient Egypt: chattel slavery, bonded labor, and forced labor.

Slavery is known to have existed in ancient China as early as the Shang dynasty. Slavery was largely employed by governments as a means of maintaining a public labor force.

Records of slavery in Ancient Greece date as far back as Mycenaean Greece. It is certain that Classical Athens had the largest slave population, with as many as 80,000 in the 6th and 5th centuries BC. As the Roman Republic expanded outward, entire populations were enslaved, thus creating an ample supply from all over Europe and the Mediterranean. Slaves were used for labour, as well as for amusement (e.g. gladiators and sex slaves). This oppression by an elite minority eventually led to slave revolts (see Roman Servile Wars); the Third Servile War, led by Spartacus, (a Thracian) being the most famous.

By the late Republican era, slavery had become a vital economic pillar in the wealth of Rome, as well as a very significant part of Roman society. It is estimated that 25% or more of the population of Ancient Rome was enslaved, although the actual percentage is debated by scholars and varied from region to region. Slaves represented 15–25% of Italy's population, mostly captives in war, especially from Gaul and Epirus. Estimates of the number of slaves in the Roman Empire suggest that the majority of slaves were scattered throughout the provinces outside of Italy. Generally, slaves in Italy were indigenous Italians, with a minority of foreigners (including both slaves and freedmen) born outside of Italy estimated at 5% of the total in the capital at its peak, where their number was largest. Those from outside of Europe were predominantly of Greek descent, while the Jewish ones never fully assimilated into Roman society, remaining an identifiable minority. These slaves (especially the foreigners) had higher death rates and lower birth rates than natives and were sometimes even subjected to mass expulsions. The average recorded age at death for the slaves of the city of Rome was seventeen and a half years (17.2 for males; 17.9 for females).

Slavery was widespread in Africa, with both internal and external slave trade. In the Senegambia region, between 1300 and 1900, close to one-third of the population was enslaved. In early Islamic states of the western Sahel, including Ghana, Mali, Segou, and Songhai, about a third of the population were enslaved.
The Arab slave trade, across the Sahara desert and across the Indian Ocean, began after Muslim Arab and Swahili traders won control of the Swahili Coast and sea routes during the 9th century (see Sultanate of Zanzibar). These traders captured Bantu peoples (Zanj) from the interior in present-day Kenya, Mozambique and Tanzania and brought them to the coast. There, the slaves gradually assimilated in the rural areas, particularly on the Unguja and Pemba islands.

Slavery in Mexico can be traced back to the Aztecs. Other Amerindians, such as the Inca of the Andes, the Tupinambá of Brazil, the Creek of Georgia, and the Comanche of Texas, also owned slaves.

Many Han Chinese were enslaved in the process of the Mongol invasion of China proper. According to Japanese historians Sugiyama Masaaki (杉山正明) and Funada Yoshiyuki (舩田善之), there were also a certain number of Mongolian slaves owned by Han Chinese during the Yuan dynasty. Moreover, there is no evidence that the Han Chinese, who were at the bottom of Yuan society according to some research, suffered particularly cruel abuse.

Slavery in Korea existed since before the Three Kingdoms of Korea period, approximately 2,000 years ago. Slavery has been described as "very important in medieval Korea, probably more important than in any other East Asian country, but by the 16th century, population growth was making [it] unnecessary". Slavery went into decline around the 10th century but came back in the late Goryeo period when Korea also experienced a number of slave rebellions.

In the Joseon period of Korea, members of the slave class were known as "nobi". The nobi were socially indistinct from freemen (i.e., the middle and common classes) other than the ruling yangban class, and some possessed property rights, legal entities and civil rights. Hence, some scholars argue that it is inappropriate to call them "slaves", while some scholars describe them as serfs. The nobi population could fluctuate up to about one-third of the population, but on average the nobi made up about 10% of the total population. In 1801, the vast majority of government nobi were emancipated, and by 1858 the nobi population stood at about 1.5 percent of the total population of Korea.

Slavery largely disappeared from Western Europe in the Middle Ages but persisted longer in Eastern Europe. Large-scale trading in slaves was mainly confined to the South and East of early medieval Europe: the Byzantine Empire and the Muslim world were the destinations, while pagan Central and Eastern Europe (along with the Caucasus and Tartary) were important sources. Viking, Arab, Greek, and Radhanite Jewish merchants were all involved in the slave trade during the Early Middle Ages. The trade in European slaves reached a peak in the 10th century following the Zanj Rebellion which dampened the use of African slaves in the Arab world.

Slavery in early medieval Europe was so common that the Catholic Church repeatedly prohibited it, or at least the export of Christian slaves to non-Christian lands, as for example at the Council of Koblenz (922), the Council of London (1102) (which aimed mainly at the sale of English slaves to Ireland) and the Council of Armagh (1171). Serfdom, on the contrary, was widely accepted. In 1452, Pope Nicholas V issued the papal bull Dum Diversas, granting the kings of Spain and Portugal the right to reduce any "Saracens (Muslims), pagans and any other unbelievers" to perpetual slavery, legitimizing the slave trade as a result of war. The approval of slavery under these conditions was reaffirmed and extended in his Romanus Pontifex bull of 1455.

In Britain, slavery continued to be practiced following the fall of Rome, and sections of Hywel the Good's laws dealt with slaves in medieval Wales. The trade particularly picked up after the Viking invasions, with major markets at Chester and Bristol supplied by Danish, Mercian, and Welsh raiding of one another's borderlands. At the time of the "Domesday Book", nearly 10% of the English population were slaves. William the Conqueror introduced a law preventing the sale of slaves overseas. According to historian John Gillingham, by 1200 slavery in the British Isles was non-existent.

However, when England began to participate in the slave trade, and cultivated colonies in the Caribbean, African slaves began to make their appearance in Tudor England, and remained a presence until abolition in the 19th century. The slave trade was abolished by the Slave Trade Act 1807, although slavery remained legal in possessions outside Europe until the passage of the Slavery Abolition Act 1833 and the Indian Slavery Act, 1843.

The Byzantine-Ottoman wars and the Ottoman wars in Europe brought large numbers of slaves into the Islamic world. To staff its bureaucracy, the Ottoman Empire established a janissary system which seized hundreds of thousands of Christian boys through the devşirme system. They were well cared for but were legally slaves owned by the government and were not allowed to marry. They were never bought or sold. The empire gave them significant administrative and military roles. The system began about 1365; there were 135,000 janissaries in 1826, when the system ended.

After the Battle of Lepanto, 12,000 Christian galley slaves were recaptured and freed from the Ottoman fleet. Eastern Europe suffered a series of Tatar invasions, the goal of which was to loot and capture slaves into "jasyr". Seventy-five Crimean Tatar raids were recorded into Poland–Lithuania between 1474 and 1569.

Slavery in Poland was forbidden in the 15th century; in Lithuania, slavery was formally abolished in 1588; they were replaced by the second serfdom.

Medieval Spain and Portugal were the scene of almost constant Muslim invasion of the predominantly Christian area. Periodic raiding expeditions were sent from Al-Andalus to ravage the Iberian Christian kingdoms, bringing back booty and slaves. In a raid against Lisbon in 1189, for example, the Almohad caliph Yaqub al-Mansur took 3,000 female and child captives, while his governor of Córdoba, in a subsequent attack upon Silves, Portugal, in 1191, took 3,000 Christian slaves. From the 11th to the 19th century, North African Barbary Pirates engaged in "Razzias", raids on European coastal towns, to capture Christian slaves to sell at slave markets in places such as Algeria and Morocco.
The maritime town of Lagos was the first slave market created in Portugal (one of the earliest colonizers of the Americas) for the sale of imported African slaves – the "Mercado de Escravos", opened in 1444. In 1441, the first slaves were brought to Portugal from northern Mauritania.

By 1552, black African slaves made up 10% of the population of Lisbon. In the second half of the 16th century, the Crown gave up the monopoly on slave trade, and the focus of European trade in African slaves shifted from import to Europe to slave transports directly to tropical colonies in the Americas – especially Brazil. In the 15th century one-third of the slaves were resold to the African market in exchange of gold.

In Kievan Rus and Muscovy, slaves were usually classified as kholops. According to David P. Forsythe, "In 1649 up to three-quarters of Muscovy's peasants, or 13 to 14 million people, were serfs whose material lives were barely distinguishable from slaves. Perhaps another 1.5 million were formally enslaved, with Russian slaves serving Russian masters." Slavery remained a major institution in Russia until 1723, when Peter the Great converted the household slaves into house serfs. Russian agricultural slaves were formally converted into serfs earlier in 1679.

In Scandinavia, thralldom was abolished in the mid-14th century.

During the Second World War Nazi Germany effectively enslaved about 12 million people, both those considered undesirable and citizens of conquered countries, with the avowed intention of treating these "Untermenschen" (sub-humans) as a permanent slave-class of inferior beings who could be worked until they died, and who possessed neither the rights nor the legal status of members of the Aryan race.

The Arab slave trade lasted more than a millennium. As recently as the early 1960s, Saudi Arabia's slave population was estimated at 300,000. Along with Yemen, the Saudis abolished slavery in 1962. Historically, slaves in the Arab World came from many different regions, including Sub-Saharan Africa (mainly "Zanj"), the Caucasus (mainly Circassians), Central Asia (mainly Tartars), and Central and Eastern Europe (mainly "Saqaliba").

Some historians assert that as many as 17 million people were sold into slavery on the coast of the Indian Ocean, the Middle East, and North Africa, and approximately 5 million African slaves were bought by Muslim slave traders and taken from Africa across the Red Sea, Indian Ocean, and Sahara desert between 1500 and 1900. The captives were sold throughout the Middle East. This trade accelerated as superior ships led to more trade and greater demand for labour on plantations in the region. Eventually, tens of thousands of captives were being taken every year. The Indian Ocean slave trade was multi-directional and changed over time. To meet the demand for menial labor, Bantu slaves bought by Arab slave traders from southeastern Africa were sold in cumulatively large numbers over the centuries to customers in Egypt, Arabia, the Persian Gulf, India, European colonies in the Far East, the Indian Ocean islands, Ethiopia and Somalia.

According to the "Encyclopedia of African History", "It is estimated that by the 1890s the largest slave population of the world, about 2 million people, was concentrated in the territories of the Sokoto Caliphate. The use of slave labor was extensive, especially in agriculture." The Anti-Slavery Society estimated there were 2 million slaves in Ethiopia in the early 1930s out of an estimated population of 8 to 16 million.

Slave labor in East Africa was drawn from the "Zanj," Bantu peoples that lived along the East African coast. The Zanj were for centuries shipped as slaves by Arab traders to all the countries bordering the Indian Ocean. The Umayyad and Abbasid caliphs recruited many Zanj slaves as soldiers and, as early as 696, there were slave revolts of the Zanj against their Arab enslavers in Iraq. The Zanj Rebellion, a series of uprisings that took place between 869 and 883 near Basra (also known as Basara), situated in present-day Iraq, is believed to have involved enslaved Zanj that had originally been captured from the African Great Lakes region and areas further south in East Africa. It grew to involve over 500,000 slaves and free men who were imported from across the Muslim empire and claimed over "tens of thousands of lives in lower Iraq".
The Zanj who were taken as slaves to the Middle East were often used in strenuous agricultural work. As the plantation economy boomed and the Arabs became richer, agriculture and other manual labor work was thought to be demeaning. The resulting labor shortage led to an increased slave market.
In Algiers, the capital of Algeria, captured Christians and Europeans were forced into slavery. In about 1650, there were as many as 35,000 Christian slaves in Algiers. By one estimate, raids by Barbary pirates on coastal villages and ships extending from Italy to Iceland, enslaved an estimated 1 to 1.25 million Europeans between the 16th and 19th centuries. However, to this estimate is extrapolated by assuming the number of European, slaves captured by Barbary pirates, was constant for 250 years period:

Davis' numbers have been refuted by other historians, such as David Earle, who cautions that true picture of Europeans slaves is clouded by the fact the corsairs also seized non-Christian whites from eastern Europe. In addition, the number of slaves traded was hyperactive, with exaggerated estimates relying on peak years to calculate averages for entire centuries, or millennia. Hence, there were wide fluctuations year-to-year, particularly in the 18th and 19th centuries, given slave imports, and also given the fact that, prior to the 1840s, there are no consistent records. Middle East expert, John Wright, cautions that modern estimates are based on back-calculations from human observation. Such observations, across the late 16th and early 17th century observers, account for around 35,000 European Christian slaves held throughout this period on the Barbary Coast, across Tripoli, Tunis, but mostly in Algiers. The majority were sailors (particularly those who were English), taken with their ships, but others were fishermen and coastal villagers. However, most of these captives were people from lands close to Africa, particularly Spain and Italy. This eventually led to the bombardment of Algiers by an Anglo-Dutch fleet in 1816.
Under Omani Arabs, Zanzibar became East Africa's main slave port, with as many as 50,000 enslaved Africans passing through every year during the 19th century. Some historians estimate that between 11 and 18 million African slaves crossed the Red Sea, Indian Ocean, and Sahara Desert from 650 to 1900 AD. Eduard Rüppell described the losses of Sudanese slaves being transported on foot to Egypt: "after the Daftardar bey's 1822 campaign in the southern Nuba mountains, nearly 40,000 slaves were captured. However, through bad treatment, disease and desert travel barely 5,000 made it to Egypt.." W.A. Veenhoven wrote: "The German doctor, Gustav Nachtigal, an eye-witness, believed that for every slave who arrived at a market three or four died on the way ... Keltie ("The Partition of Africa", London, 1920) believes that for every slave the Arabs brought to the coast at least six died on the way or during the slavers' raid. Livingstone puts the figure as high as ten to one."

Systems of servitude and slavery were common in parts of Africa, as they were in much of the ancient world. In many African societies where slavery was prevalent, the enslaved people were not treated as chattel slaves and were given certain rights in a system similar to indentured servitude elsewhere in the world. The forms of slavery in Africa were closely related to kinship structures. In many African communities, where land could not be owned, enslavement of individuals was used as a means to increase the influence a person had and expand connections. This made slaves a permanent part of a master's lineage and the children of slaves could become closely connected with the larger family ties. Children of slaves born into families could be integrated into the master's kinship group and rise to prominent positions within society, even to the level of chief in some instances. However, stigma often remained attached and there could be strict separations between slave members of a kinship group and those related to the master. Slavery was practiced in many different forms: debt slavery, enslavement of war captives, military slavery, and criminal slavery were all practiced in various parts of Africa. Slavery for domestic and court purposes was widespread throughout Africa.
When the Atlantic slave trade began, many of the local slave systems began supplying captives for chattel slave markets outside Africa. Although the Atlantic slave trade was not the only slave trade from Africa, it was the largest in volume and intensity. As Elikia M’bokolo wrote in "Le Monde diplomatique":

The trans-Atlantic slave trade peaked in the late 18th century, when the largest number of slaves were captured on raiding expeditions into the interior of West Africa. These expeditions were typically carried out by African kingdoms, such as the Oyo Empire (Yoruba), the Ashanti Empire, the kingdom of Dahomey, and the Aro Confederacy. It is estimated that about 15 percent of slaves died during the voyage, with mortality rates considerably higher in Africa itself in the process of capturing and transporting indigenous peoples to the ships.

Slavery in America remains a contentious issue and played a major role in the history and evolution of some countries, triggering a revolution, a civil war, and numerous rebellions.

In order to establish itself as an American empire, Spain had to fight against the relatively powerful civilizations of the New World. The Spanish conquest of the indigenous peoples in the Americas included using the Natives as forced labour. The Spanish colonies were the first Europeans to use African slaves in the New World on islands such as Cuba and Hispaniola. Bartolomé de las Casas, a 16th-century Dominican friar and Spanish historian, participated in campaigns in Cuba (at Bayamo and Camagüey) and was present at the massacre of Hatuey; his observation of that massacre led him to fight for a social movement away from the use of natives as slaves. Also, the alarming decline in the native population had spurred the first royal laws protecting the native population. The first African slaves arrived in Hispaniola in 1501. England played a prominent role in the Atlantic slave trade. The "slave triangle" was pioneered by Francis Drake and his associates.

Many Africans who arrived in North America during the 17th and 18th centuries came under contract as indentured servants. The transformation from indentured servitude to slavery was a gradual process in Virginia. The earliest legal documentation of such a shift was in 1640 where a negro, John Punch, was sentenced to lifetime slavery, forcing him to serve his master, Hugh Gwyn, for the remainder of his life, for attempting to run away. This case was significant because it established the disparity between his sentence as a black man and that of the two white indentured servants who escaped with him (one described as Dutch and one as a Scotchman). It is the first documented case of a black man sentenced to lifetime servitude and is considered one of the first legal cases to make a racial distinction between black and white indentured servants.

After 1640, planters started to ignore the expiration of indentured contracts and keep their servants as slaves for life. This was demonstrated by the 1655 case "Johnson v. Parker", where the court ruled that a black man, Anthony Johnson of Virginia, was granted ownership of another black man, John Casor, as the result of a civil case. This was the first instance of a judicial determination in the Thirteen Colonies holding that a person who had committed no crime could be held in servitude for life.

In the early 17th century, the majority of the labour in Barbados was provided by European indentured servants, mainly English, Irish and Scottish, with enslaved Africans and enslaved Amerindians providing little of the workforce. The introduction of sugar cane from Dutch Brazil in 1640 completely transformed society and the economy. Barbados eventually had one of the world's largest sugar industries.

As the effects of the new crop increased, so did the shift in the ethnic composition of Barbados and surrounding islands. The workable sugar plantation required a large investment and a great deal of heavy labour. At first, Dutch traders supplied the equipment, financing, and enslaved Africans, in addition to transporting most of the sugar to Europe. In 1644, the population of Barbados was estimated at 30,000, of which about 800 were of African descent, with the remainder mainly of English descent. These English smallholders were eventually bought out, and the island filled up with large sugar plantations worked by enslaved Africans. By 1660, there was near parity with 27,000 blacks and 26,000 whites. By 1666, at least 12,000 white smallholders had been bought out, died, or left the island. Many of the remaining whites were increasingly poor. By 1680, there were 17 slaves for every indentured servant. By 1700, there were 15,000 free whites and 50,000 enslaved Africans.

Because of the increased implementation of slave codes, which created differential treatment between Africans and the white workers and ruling planter class, the island became increasingly unattractive to poor whites. Black or slave codes were implemented in 1661, 1676, 1682, and 1688. In response to these codes, several slave rebellions were attempted or planned during this time, but none succeeded. Nevertheless, poor whites who had or acquired the means to emigrate often did so. Planters expanded their importation of enslaved Africans to cultivate sugar cane.

Slavery in Brazil began long before the first Portuguese settlement was established in 1532, as members of one tribe would enslave captured members of another.

Later, Portuguese colonists were heavily dependent on indigenous labor during the initial phases of settlement to maintain the subsistence economy, and natives were often captured by expeditions called "". The importation of African slaves began midway through the 16th century, but the enslavement of indigenous peoples continued well into the 17th and 18th centuries.

During the Atlantic slave trade era, Brazil imported more African slaves than any other country. Nearly 5 million slaves were brought from Africa to Brazil during the period from 1501 to 1866. Until the early 1850s, most enslaved Africans who arrived on Brazilian shores were forced to embark at West Central African ports, especially in Luanda (present-day Angola). Today, with the exception of Nigeria, the largest population of people of African descent is in Brazil.

Slave labor was the driving force behind the growth of the sugar economy in Brazil, and sugar was the primary export of the colony from 1600 to 1650. Gold and diamond deposits were discovered in Brazil in 1690, which sparked an increase in the importation of African slaves to power this newly profitable market. Transportation systems were developed for the mining infrastructure, and population boomed from immigrants seeking to take part in gold and diamond mining. Demand for African slaves did not wane after the decline of the mining industry in the second half of the 18th century. Cattle ranching and foodstuff production proliferated after the population growth, both of which relied heavily on slave labor. 1.7 million slaves were imported to Brazil from Africa from 1700 to 1800, and the rise of coffee in the 1830s further enticed expansion of the slave trade.

Brazil was the last country in the Western world to abolish slavery. Forty percent of the total number of slaves brought to the Americas were sent to Brazil. For reference, the United States received 10 percent. Despite being abolished, there are still people working in slavery-like conditions in Brazil in the 21st century.

In 1789 the Spanish Crown led an effort to reform slavery, as the demand for slave labor in Cuba was growing. The Crown issued a decree, "Código Negro Español" (Spanish Black Codex), that specified food and clothing provisions, put limits on the number of work hours, limited punishments, required religious instruction, and protected marriages, forbidding the sale of young children away from their mothers. The British made other changes to the institution of slavery in Cuba. But planters often flouted the laws and protested against them, considering them a threat to their authority and an intrusion into their personal lives.

The slaveowners did not protest against all the measures of the codex, many of which they argued were already common practices. They objected to efforts to set limits on their ability to apply physical punishment. For instance, the Black Codex limited whippings to 25 and required the whippings "not to cause serious bruises or bleeding". The slave-owners thought that the slaves would interpret these limits as weaknesses, ultimately leading to resistance. Another contested issue was the work hours that were restricted "from sunrise to sunset"; plantation owners responded by explaining that cutting and processing of cane needed 20-hour days during the harvest season.

Those slaves who worked on sugar plantations and in sugar mills were often subject to the harshest of conditions. The field work was rigorous manual labor which the slaves began at an early age. The work days lasted close to 20 hours during harvest and processing, including cultivating and cutting the crops, hauling wagons, and processing sugarcane with dangerous machinery. The slaves were forced to reside in barracoons, where they were crammed in and locked in by a padlock at night, getting about three to four hours of sleep. The conditions of the barracoons were harsh; they were highly unsanitary and extremely hot. Typically there was no ventilation; the only window was a small barred hole in the wall.
Cuba's slavery system was gendered in a way that some duties were performed only by male slaves, some only by female slaves. Female slaves in Havana from the 16th century onwards performed duties such as operating the town taverns, eating houses, and lodges, as well as being laundresses and domestic laborers and servants. Female slaves also served as the town prostitutes.

Some Cuban women could gain freedom by having children with white men. As in other Latin cultures, there were looser borders with the mulatto or mixed-race population. Sometimes men who took slaves as wives or concubines freed both them and their children. As in New Orleans and Saint-Domingue, mulattos began to be classified as a third group between the European colonists and African slaves. Freedmen, generally of mixed race, came to represent 20% of the total Cuban population and 41% of the non-white Cuban population.

Planters encouraged Afro-Cuban slaves to have children in order to reproduce their work force. The masters wanted to pair strong and large-built black men with healthy black women. They were placed in the barracoons and forced to have sex and create offspring of “breed stock” children, who would sell for around 500 pesos. The planters needed children to be born to replace slaves who died under the harsh regime. Sometimes if the overseers did not like the quality of children, they separate the parents and sent the mother back to working in the fields.

Both women and men were subject to the punishments of violence and humiliating abuse. Slaves who misbehaved or disobeyed their masters were often placed in stocks in the depths of the boiler houses where they were abandoned for days at a time, and oftentimes two to three months. These wooden stocks were made in two types: lying-down or stand-up types. women were punished, even when pregnant. They were subjected to whippings: they had to lie "face down over a scooped-out piece of round [earth] to protect their bellies." Some masters reportedly whipped pregnant women in the belly, often causing miscarriages. The wounds were treated with “compresses of tobacco leaves, urine and salt."

Slavery in Haiti started with the arrival of Christopher Columbus on the island in 1492. The practice was devastating to the native population. Following the indigenous Taíno's near decimation from forced labour, disease and war, the Spanish, under advisement of the Catholic priest Bartolomeu de las Casas, and with the blessing of the Catholic church began engaging in earnest in the kidnapped and forced labour of enslaved Africans. During the French colonial period beginning in 1625, the economy of Haiti (then known as Saint-Domingue) was based on slavery, and the practice there was regarded as the most brutal in the world.

Following the Treaty of Ryswick of 1697, Hispaniola was divided between France and Spain. France received the western third and subsequently named it Saint-Domingue. To develop it into sugarcane plantations, the French imported thousands of slaves from Africa. Sugar was a lucrative commodity crop throughout the 18th century. By 1789, approximately 40,000 white colonists lived in Saint-Domingue. The whites were vastly outnumbered by the tens of thousands of African slaves they had imported to work on their plantations, which were primarily devoted to the production of sugarcane. In the north of the island, slaves were able to retain many ties to African cultures, religion and language; these ties were continually being renewed by newly imported Africans. Blacks outnumbered whites by about ten to one.
The French-enacted "Code Noir" ("Black Code"), prepared by Jean-Baptiste Colbert and ratified by Louis XIV, had established rules on slave treatment and permissible freedoms. Saint-Domingue has been described as one of the most brutally efficient slave colonies; one-third of newly imported Africans died within a few years. Many slaves died from diseases such as smallpox and typhoid fever. They had birth rates around 3 percent, and there is evidence that some women aborted fetuses, or committed infanticide, rather allow their children to live within the bonds of slavery.

As in its Louisiana colony, the French colonial government allowed some rights to free people of color: the mixed-race descendants of white male colonists and black female slaves (and later, mixed-race women). Over time, many were released from slavery. They established a separate social class. White French Creole fathers frequently sent their mixed-race sons to France for their education. Some men of color were admitted into the military. More of the free people of color lived in the south of the island, near Port-au-Prince, and many intermarried within their community. They frequently worked as artisans and tradesmen, and began to own some property. Some became slave holders. The free people of color petitioned the colonial government to expand their rights.

Slaves that made it to Haiti from the trans-Atlantic journey and slaves born in Haiti were first documented in Haiti's archives and transferred to France's Ministry of Defense and the Ministry of Foreign Affairs. , these records are in The National Archives of France. According to the 1788 Census, Haiti's population consisted of nearly 40,000 whites, 30,000 free coloureds and 450,000 slaves.

The Haitian Revolution of 1804, the only successful slave revolt in human history, precipitated the end of slavery in all French colonies.

Jamaica was colonized by the Taino tribes prior to the arrival of Columbus in 1494. The Spanish enslaved many of the Taino; some escaped, but most died from European diseases and overwork. The Spaniards also introduced the first African slaves.

The Spanish colonists did not bring women in the first expeditions and took Taíno women for their common-law wives, resulting in mestizo children. Sexual violence with the Taíno women by the Spanish was also common.

Although the African slave population in the 1670s and 1680s never exceeded 10,000, by 1800 it had increased to over 300,000.

In 1519, Hernán Cortés brought the first modern slave to the area. In the mid-16th century, the second viceroy to Mexico, Luis de Velasco, prohibited slavery of the Aztecs. A labor shortage resulted as the Aztecs were either killed or died from disease. This led to the African slaves being imported, as they were not susceptible to smallpox. In exchange, many Africans were afforded the opportunity to buy their freedom, while eventually others were granted their freedom by their masters.

When Ponce de León and the Spaniards arrived on the island of Borikén (Puerto Rico), they enslaved Taíno tribes on the island, forcing them to work in the gold mines and in the construction of forts. Many Taíno died, particularly from smallpox, of which they had no immunity. Other Taínos committed suicide or left the island after the failed Taíno revolt of 1511. The Spanish colonists, fearing the loss of their labor force, complained the courts that they needed manpower. As an alternative, Las Casas suggested the importation and use of African slaves. In 1517, the Spanish Crown permitted its subjects to import twelve slaves each, thereby beginning the slave trade on the colonies.

African slaves were legally branded with a hot iron on the forehead, prevented their "theft" or lawsuits that challenged their captivity. The colonists continued this branding practice for more than 250 years. They were sent to work in the gold mines, or in the island's ginger and sugar fields. They were allowed to live with their families in a hut on the master's land, and given a patch of land where they could farm, but otherwise were subjected to harsh treatment; including sexual abuse as the majority of colonists had arrived without women; many of them intermarried with the Africans or Taínos. Their mixed-race descendants formed the first generations of the early Puerto Rican population.

The slaves faced heavy discrimination and had no opportunity for advancement, though they were educated by their masters. The Spaniards considered the Africans superior to the Taíno, since the latter were unwilling to assimilate. The slaves, in contrast, had little choice but to adapt. Many converted to Christianity and were given their masters' surnames.

By 1570, the colonists found that the gold mines were depleted, relegating the island to a garrison for passing ships. The cultivation of crops such as tobacco, cotton, cocoa, and ginger became the cornerstone of the economy. With rising demand for sugar on the international market, major planters increased their labor-intensive cultivation and processing of sugar cane. Sugar plantations supplanted mining as Puerto Rico's main industry and kept demand high for African slavery.

After 1784, Spain provided five ways by which slaves could obtain freedom. Five years later, the Spanish Crown issued the "Royal Decree of Graces of 1789", which set new rules related to the slave trade and added restrictions to the granting of freedman status. The decree granted its subjects the right to purchase slaves and to participate in the flourishing slave trade in the Caribbean. Later that year a new slave code, also known as "El Código Negro" (The Black Code), was introduced.

Under "El Código Negro", a slave could buy his freedom, in the event that his master was willing to sell, by paying the price sought in installments. Slaves were allowed to earn money during their spare time by working as shoemakers, cleaning clothes, or selling the produce they grew on their own plots of land. For the freedom of their newborn child, not yet baptized, they paid at half the going price for a baptized child. Many of these freedmen started settlements in the areas which became known as Cangrejos (Santurce), Carolina, Canóvanas, Loíza, and Luquillo. Some became slave owners themselves. Despite these paths to freedom, from 1790 onwards, the number of slaves more than doubled in Puerto Rico as a result of the dramatic expansion of the sugar industry in the island.

On March 22, 1873, slavery was legally abolished in Puerto Rico. However, slaves were not emancipated but rather had to buy their own freedom, at whatever price was set by their last masters. They were also required to work for another three years for their former masters, for other colonists interested in their services, or for the state in order to pay some compensation. Between 1527 and 1873, slaves in Puerto Rico had carried out more than twenty revolts.

The planters of the Dutch colony relied heavily on African slaves to cultivate, harvest and process the commodity crops of coffee, cocoa, sugar cane and cotton plantations along the rivers. Planters' treatment of the slaves was notoriously bad. Historian C. R. Boxer wrote that "man's inhumanity to man just about reached its limits in Surinam."

Many slaves escaped the plantations. With the help of the native South Americans living in the adjoining rain forests, these runaway slaves established a new and unique culture in the interior that was highly successful in its own right. They were known collectively in English as Maroons, in French as "Nèg'Marrons" (literally meaning "brown negroes", that is "pale-skinned negroes"), and in Dutch as "Marrons." The Maroons gradually developed several independent tribes through a process of ethnogenesis, as they were made up of slaves from different African ethnicities. These tribes include the Saramaka, Paramaka, Ndyuka or Aukan, Kwinti, Aluku or Boni, and Matawai.

The Maroons often raided plantations to recruit new members from the slaves and capture women, as well as to acquire weapons, food and supplies. They sometimes killed planters and their families in the raids. The colonists also mounted armed campaigns against the Maroons, who generally escaped through the rain forest, which they knew much better than did the coloniss. To end hostilities, in the 18th century the European colonial authorities signed several peace treaties with different tribes. They granted the Maroons sovereign status and trade rights in their inland territories, giving them autonomy.

In 1861–63, President Abraham Lincoln of the United States and his administration looked abroad for places to relocate freed slaves who wanted to leave the United States. It opened negotiations with the Dutch government regarding African-American emigration to and colonization of the Dutch colony of Suriname in South America. Nothing came of it and after 1864, the proposal was dropped.

The Netherlands abolished slavery in Suriname, in 1863, under a gradual process that required slaves to work on plantations for 10 transition years for minimal pay, which was considered as partial compensation for their masters. After 1873, most freedmen largely abandoned the plantations where they had worked for several generations in favor of the capital city, Paramaribo.

Slavery in the United States was the legal institution of human chattel enslavement, primarily of Africans and African Americans, that existed in the United States of America in the 18th and 19th centuries after it gained independence from the British and before the end of the American Civil War. Slavery had been practiced in British America from early colonial days and was legal in all Thirteen Colonies at the time of the Declaration of Independence in 1776.

By the time of the American Revolution, the status of slave had been institutionalized as a racial caste associated with African ancestry. The United States became polarized over the issue of slavery, represented by the slave and free states divided by the Mason–Dixon line, which separated free Pennsylvania from slave Maryland and Delaware.

Congress, during the Jefferson administration prohibited the importation of slaves, effective 1808, although smuggling (illegal importing) was not unusual. Domestic slave trading, however, continued at a rapid pace, driven by labor demands from the development of cotton plantations in the Deep South. Those states attempted to extend slavery into the new western territories to keep their share of political power in the nation. Such laws proposed to Congress to continue the spread of slavery into newly ratified states include the Kansas-Nebraska Act.

The treatment of slaves in the United States varied widely depending on conditions, times, and places. The power relationships of slavery corrupted many whites who had authority over slaves, with children showing their own cruelty. Masters and overseers resorted to physical punishments to impose their wills. Slaves were punished by whipping, shackling, hanging, beating, burning, mutilation, branding and imprisonment. Punishment was most often meted out in response to disobedience or perceived infractions, but sometimes abuse was carried out to re-assert the dominance of the master or overseer of the slave. Treatment was usually harsher on large plantations, which were often managed by overseers and owned by absentee slaveholders.
William Wells Brown, who escaped to freedom, reported that on one plantation, slave men were required to pick 80 pounds of cotton per day, while women were required to pick 70 pounds per day; if any slave failed in his or her quota, they were subject to whip lashes for each pound they were short. The whipping post stood next to the cotton scales. A New York man who attended a slave auction in the mid-19th century reported that at least three-quarters of the male slaves he saw at sale had scars on their backs from whipping. By contrast, small slave-owning families had closer relationships between the owners and slaves; this sometimes resulted in a more humane environment but was not a given.

More than one million slaves were sold from the Upper South, which had a surplus of labor, and taken to the Deep South in a forced migration, splitting up many families. New communities of African-American culture were developed in the Deep South, and the total slave population in the South eventually reached 4 million before liberation. In the 19th century, proponents of slavery often defended the institution as a "necessary evil". White people of that time feared that emancipation of black slaves would have more harmful social and economic consequences than the continuation of slavery. The French writer and traveler Alexis de Tocqueville, in "Democracy in America" (1835), expressed opposition to slavery while observing its effects on American society. He felt that a multiracial society without slavery was untenable, as he believed that prejudice against black people increased as they were granted more rights. Others, like James Henry Hammond argued that slavery was a "positive good" stating: "Such a class you must have, or you would not have that other class which leads progress, civilization, and refinement."

The Southern state governments wanted to keep a balance between the number of slave and free states to maintain a political balance of power in Congress. The new territories acquired from Britain, France, and Mexico were the subject of major political compromises. By 1850, the newly rich cotton-growing South was threatening to secede from the Union, and tensions continued to rise. Many white Southern Christians, including church ministers, attempted to justify their support for slavery as modified by Christian paternalism. The largest denominations, the Baptist, Methodist, and Presbyterian churches, split over the slavery issue into regional organizations of the North and South.
When Abraham Lincoln won the 1860 election on a platform of halting the expansion of slavery, according to the 1860 U.S. census, roughly 400,000 individuals, representing 8% of all U.S. families, owned nearly 4,000,000 slaves. One-third of Southern families owned slaves. The South was heavily invested in slavery. As such, upon Lincoln's election, seven states broke away to form the Confederate States of America. The first six states to secede held the greatest number of slaves in the South. Shortly after, over the issue of slavery, the United States erupted into an all out Civil War, with slavery legally ceasing as an institution following the war in December 1865.

In 2018, the "Orlando Sentinel" reported some private Christian schools in Florida as teaching students a creationist curriculum which includes assertions such as, “most black and white southerners had long lived together in harmony” and that “power-hungry individuals stirred up the people” leading to the Civil Rights Movement.

Slavery has existed all throughout Asia, and forms of slavery still exist today.

Slavery has taken various forms throughout China's history. It was reportedly abolished as a legally recognized institution, including in a 1909 law fully enacted in 1910, although the practice continued until at least 1949.

The Tang dynasty purchased Western slaves from the Radhanite Jews. Tang Chinese soldiers and pirates enslaved Koreans, Turks, Persians, Indonesians, and people from Inner Mongolia, central Asia, and northern India. The greatest source of slaves came from southern tribes, including Thais and aboriginals from the southern provinces of Fujian, Guangdong, Guangxi, and Guizhou. Malays, Khmers, Indians, and "black skinned" peoples (who were either Austronesian Negritos of Southeast Asia and the Pacific Islands, or Africans, or both) were also purchased as slaves in the Tang dynasty.

In the 17th century Qing Dynasty, there was a hereditarily servile people called "Booi Aha" (Manchu:booi niyalma; Chinese transliteration: 包衣阿哈), which is a Manchu word literally translated as "household person" and sometimes rendered as "nucai." The Manchu was establishing close personal and paternalist relationship between masters and their slaves, as Nurhachi said, "The Master should love the slaves and eat the same food as him". However, booi aha "did not correspond exactly to the Chinese category of "bond-servant slave" (Chinese:奴僕); instead, it was a relationship of personal dependency on a master which in theory guaranteed close personal relationships and equal treatment, even though many western scholars would directly translate "booi" as "bond-servant" (some of the "booi" even had their own servant).

Chinese Muslim (Tungans) Sufis who were charged with practicing xiejiao (heterodox religion), were punished by exile to Xinjiang and being sold as a slave to other Muslims, such as the Sufi begs. Han Chinese who committed crimes such as those dealing with opium became slaves to the begs, this practice was administered by Qing law. Most Chinese in Altishahr were exile slaves to Turkestani Begs. While free Chinese merchants generally did not engage in relationships with East Turkestani women, some of the Chinese slaves belonging to begs, along with Green Standard soldiers, Bannermen, and Manchus, engaged in affairs with the East Turkestani women that were serious in nature.

Slavery in India was widespread by the 6th century BC, and perhaps even as far back as the Vedic period. Slavery intensified during the Muslim domination of northern India after the 11th-century. Slavery existed in Portuguese India after the 16th century. The Dutch, too, largely dealt in Abyssian slaves, known in India as Habshis or Sheedes. Arakan/Bengal, Malabar, and Coromandel remained the largest sources of forced labour until the 1660s.

Between 1626 and 1662, the Dutch exported on an average 150–400 slaves annually from the Arakan-Bengal coast. During the first 30 years of Batavia's existence, Indian and Arakanese slaves provided the main labour force of the Dutch East India Company, Asian headquarters. An increase in Coromandel slaves occurred during a famine following the revolt of the Nayaka Indian rulers of South India (Tanjavur, Senji, and Madurai) against Bijapur overlordship (1645) and the subsequent devastation of the Tanjavur countryside by the Bijapur army. Reportedly, more than 150,000 people were taken by the invading Deccani Muslim armies to Bijapur and Golconda. In 1646, 2,118 slaves were exported to Batavia, the overwhelming majority from southern Coromandel. Some slaves were also acquired further south at Tondi, Adirampatnam, and Kayalpatnam. Another increase in slaving took place between 1659 and 1661 from Tanjavur as a result of a series of successive Bijapuri raids. At Nagapatnam, Pulicat, and elsewhere, the company purchased 8,000–10,000 slaves, the bulk of whom were sent to Ceylon, while a small portion were exported to Batavia and Malacca. Finally, following a long drought in Madurai and southern Coromandel, in 1673, which intensified the prolonged Madurai-Maratha struggle over Tanjavur and punitive fiscal practices, thousands of people from Tanjavur, mostly children, were sold into slavery and exported by Asian traders from Nagapattinam to Aceh, Johor, and other slave markets.

In September 1687, 665 slaves were exported by the English from Fort St. George, Madras. And, in 1694–96, when warfare once more ravaged South India, a total of 3,859 slaves were imported from Coromandel by private individuals into Ceylon.
The volume of the total Dutch Indian Ocean slave trade has been estimated to be about 15–30% of the Atlantic slave trade, slightly smaller than the trans-Saharan slave trade, and one-and-a-half to three times the size of the Swahili and Red Sea coast and the Dutch West India Company slave trades.
According to Sir Henry Bartle Frere (who sat on the Viceroy's Council), there were an estimated 8 or 9 million slaves in India in 1841. About 15% of the population of Malabar were slaves. Slavery was legally abolished in the possessions of the East India Company by the Indian Slavery Act, 1843.

The hill tribe people in Indochina were "hunted incessantly and carried off as slaves by the Siamese (Thai), the Anamites (Vietnamese), and the Cambodians". A Siamese military campaign in Laos in 1876 was described by a British observer as having been "transformed into slave-hunting raids on a large scale". The census, taken in 1879, showed that 6% of the population in the Malay sultanate of Perak were slaves. Enslaved people made up about two-thirds of the population in part of North Borneo in the 1880s.

After the Portuguese first made contact with Japan in 1543, a large scale slave trade developed in which Portuguese purchased Japanese as slaves in Japan and sold them to various locations overseas, including Portugal, throughout the 16th and 17th centuries. Many documents mention the large slave trade along with protests against the enslavement of Japanese. Japanese slaves are believed to be the first of their nation to end up in Europe, and the Portuguese purchased large numbers of Japanese slave girls to bring to Portugal for sexual purposes, as noted by the Church in 1555. Japanese slave women were even sold as concubines to Asian lascar and African crew members, along with their European counterparts serving on Portuguese ships trading in Japan, mentioned by Luis Cerqueira, a Portuguese Jesuit, in a 1598 document. Japanese slaves were brought by the Portuguese to Macau, where they were enslaved to Portuguese or became slaves to other slaves.

Some Korean slaves were bought by the Portuguese and brought back to Portugal from Japan, where they had been among the tens of thousands of Korean prisoners of war transported to Japan during the Japanese invasions of Korea (1592–98). Historians pointed out that at the same time Hideyoshi expressed his indignation and outrage at the Portuguese trade in Japanese slaves, he was engaging in a mass slave trade of Korean prisoners of war in Japan.
Fillippo Sassetti saw some Chinese and Japanese slaves in Lisbon among the large slave community in 1578, although most of the slaves were black.
The Portuguese "highly regarded" Asian slaves from the East much more "than slaves from sub-Saharan Africa". The Portuguese attributed qualities like intelligence and industriousness to Chinese and Japanese slaves.

King Sebastian of Portugal feared rampant slavery was having a negative effect on Catholic proselytization, so he commanded that it be banned in 1571.Hideyoshi was so disgusted that his own Japanese people were being sold "en masse" into slavery on Kyushu, that he wrote a letter to Jesuit Vice-Provincial Gaspar Coelho on July 24, 1587, to demand the Portuguese, Siamese (Thai), and Cambodians stop purchasing and enslaving Japanese and return Japanese slaves who ended up as far as India. Hideyoshi blamed the Portuguese and Jesuits for this slave trade and banned Christian proselytizing as a result. In 1595, a law was passed by Portugal banning the selling and buying of Chinese and Japanese slaves.

During the Joseon period, the nobi population could fluctuate up to about one-third of the population, but on average the nobi made up about 10% of the total population. The nobi system declined beginning in the 18th century. Since the outset of the Joseon dynasty and especially beginning in the 17th century, there was harsh criticism among prominent thinkers in Korea about the nobi system. Even within the Joseon government, there were indications of a shift in attitude toward the nobi. King Yeongjo implemented a policy of gradual emancipation in 1775, and he and his successor King Jeongjo made many proposals and developments that lessened the burden on nobi, which led to the emancipation of the vast majority of government nobi in 1801. In addition, population growth, numerous escaped slaves, growing commercialization of agriculture, and the rise of the independent small farmer class contributed to the decline in the number of nobi to about 1.5% of the total population by 1858. The hereditary nobi system was officially abolished around 1886–87, and the rest of the nobi system was abolished with the Gabo Reform of 1894. However, slavery did not completely disappear in Korea until 1930, during Imperial Japanese rule.

During the Imperial Japanese occupation of Korea around World War II, some Koreans were used in forced labor by the Imperial Japanese, in conditions which have been compared to slavery. These included women forced into sexual slavery by the Imperial Japanese Army before and during World War II, known as "comfort women".

Slaves ("he mōkai") had a recognised social role in traditional Māori society in New Zealand.

Blackbirding occurred in the Pacific, especially in the 19th century.

In Constantinople, about one-fifth of the population consisted of slaves. The city was a major centre of the slave trade in the 15th and later centuries.
Slaves were provided by Tatar raids on Slavic villages but also by conquest and the suppression of rebellions, in the aftermath of which entire populations were sometimes enslaved and sold across the Empire, reducing the risk of future rebellion. The Ottomans also purchased slaves from traders who brought slaves into the Empire from Europe and Africa. It has been estimated that some 200,000 slaves – mainly Circassians – were imported into the Ottoman Empire between 1800 and 1909. As late as 1908, women slaves were still sold in the Ottoman Empire.

Until the late 18th century, the Crimean Khanate (a Muslim Tatar state) maintained a massive slave trade with the Ottoman Empire and the Middle East. The slaves were captured in southern Russia, Poland-Lithuania, Moldavia, Wallachia, and Circassia by Tatar horsemen and sold in the Crimean port of Kaffa. About 2 million mostly Christian slaves were exported over the 16th and 17th centuries until the Crimean Khanate was destroyed by the Russian Empire in 1783.
A slave market for captured Russian and Persian slaves was centred in the Central Asian khanate of Khiva. In the early 1840s, the population of the Uzbek states of Bukhara and Khiva included about 900,000 slaves. Darrel P. Kaiser wrote, "Kazakh-Kirghiz tribesmen kidnapped 1573 settlers from colonies [German settlements in Russia] in 1774 alone and only half were successfully ransomed. The rest were killed or enslaved."

Depending upon the era and the country, slaves sometimes had a limited set of legal rights. For example, in the Province of New York, people who deliberately killed slaves were punishable under a 1686 statute. And, as already mentioned, certain legal rights attached to the nobi in Korea, to enslaved people in various African societies, and to black female slaves in the French colony of Louisiana. Giving slaves legal rights has sometimes been a matter of morality, but also sometimes a matter of self-interest. For example, in ancient Athens, protecting slaves from mistreatment simultaneously protected people who might be mistaken for slaves, and giving slaves limited property rights incentivized slaves to work harder to get more property. In the southern United States prior to the extirpation of slavery in 1865, a proslavery legal treatise reported that slaves accused of crimes typically had a legal right to counsel, freedom from double jeopardy, a right to trial by jury in graver cases, and the right to grand jury indictment, but they lacked many other rights such as white adults’ ability to control their own lives.

Even though slavery is now outlawed in every country, the number of slaves today is estimated as between 12 million and 29.8 million. According to a broad definition of slavery, there were 27 million people in slavery in 1999, spread all over the world. In 2005, the International Labour Organization provided an estimate of 12.3 million forced labourers. Siddharth Kara has also provided an estimate of 28.4 million slaves at the end of 2006 divided into three categories: bonded labour/debt bondage (18.1 million), forced labour (7.6 million), and trafficked slaves (2.7 million). Kara provides a dynamic model to calculate the number of slaves in the world each year, with an estimated 29.2 million at the end of 2009.

According to a 2003 report by Human Rights Watch, an estimated 15 million children in debt bondage in India work in slavery-like conditions to pay off their family's debts.

A report by the Walk Free Foundation in 2013, found India had the highest number of slaves, nearly 14 million, followed by China (2.9 million), Pakistan (2.1 million), Nigeria, Ethiopia, Russia, Thailand, Democratic Republic of Congo, Myanmar and Bangladesh; while the countries with the highest proportions of slaves were Mauritania, Haiti, Pakistan, India and Nepal.

In June 2013, U.S. State Department released a report on slavery. It placed Russia, China, and Uzbekistan in the worst offenders category. Cuba, Iran, North Korea, Sudan, Syria, and Zimbabwe were at the lowest level. The list also included Algeria, Libya, Saudi Arabia and Kuwait among a total of 21 countries.

The Walk Free Foundation reported in 2018 that slavery in wealthy Western societies is much more prevalent than previously known, in particular the United States and Great Britain, which have 403,000 (one in 800) and 136,000 slaves respectively. Andrew Forrest, founder of the organization, said that "The United States is one of the most advanced countries in the world yet has more than 400,000 modern slaves working under forced labor conditions." An estimated 40.3 million are enslaved globally, with North Korea having the most slaves at 2.6 million (one in 10). The foundation defines contemporary slavery as "situations of exploitation that a person cannot refuse or leave because of threats, violence, coercion, abuse of power, or deception."

During the Second Libyan Civil War, Libyans started capturing Sub-Saharan African migrants trying to get to Europe through Libya and selling them on slave markets or holding them hostage for ransom Women are often raped, used as sex slaves, or sold to brothels. Child migrants suffer from abuse and child rape in Libya.

In Mauritania, the last country to abolish slavery (in 1981), it is estimated that 20% of its 3 million population, are enslaved as bonded laborers. Slavery in Mauritania was criminalized in August 2007. However, although slavery, as a practice, was legally banned in 1981, it was not a crime to own a slave until 2007. Although many slaves have escaped or have been freed since 2007, , only one slave owner had been sentenced to serve time in prison.

While American slaves in 1809 were sold for around $40,000 (in inflation adjusted dollars), a slave nowadays can be bought for just $90, making replacement more economical than providing long-term care. Slavery is a multibillion-dollar industry with estimates of up to $35 billion generated annually.

Victims of human trafficking are typically recruited through deceit or trickery (such as a false job offer, false migration offer, or false marriage offer), sale by family members, recruitment by former slaves, or outright abduction. Victims are forced into a "debt slavery" situation by coercion, deception, fraud, intimidation, isolation, threat, physical force, debt bondage or even force-feeding with drugs to control their victims. "Annually, according to U.S. government-sponsored research completed in 2006, approximately 800,000 people are trafficked across national borders, which does not include millions trafficked within their own countries. Approximately 80% of transnational victims are women and girls, and up to 50% are minors, reports the U.S. State Department in a 2008 study.

While the majority of trafficking victims are women who are forced into prostitution (in which case the practice is called sex trafficking), victims also include men, women and children who are forced into manual labour. Because of the illegal nature of human trafficking, its extent is unknown. A U.S. government report, published in 2005, estimates that about 700,000 people worldwide are trafficked across borders each year. This figure does not include those who are trafficked internally. Another research effort revealed that roughly 1.5 million individuals are trafficked either internally or internationally each year, of which about 500,000 are sex trafficking victims.

Slavery has existed, in one form or another, throughout recorded human history – as have, in various periods, movements to free large or distinct groups of slaves.

Ashoka, who ruled the Maurya Empire in the Indian subcontinent from 269–232 BCE, abolished the slave trade but not slavery. The Qin dynasty, which ruled China from 221 to 206 BC, abolished slavery and discouraged serfdom. However, many of its laws were overturned when the dynasty was overthrown. Slavery was again abolished by Wang Mang in China in 17 CE but was reinstituted after his assassination.

The Spanish colonization of the Americas sparked a discussion about the right to enslave Native Americans. A prominent critic of slavery in the Spanish New World colonies was Bartolomé de las Casas, who opposed the enslavement of Native Americans, and as well as Africans in America.

One of the first protests against slavery came from German and Dutch Quakers in Pennsylvania in 1688. In 1777, Vermont, at the time an independent nation, became the first portion of what would become the United States to abolish slavery.

In the United States, all of the northern states had abolished slavery by 1804, with New Jersey being the last to act. Abolitionist pressure produced a series of small steps towards emancipation. After the Act Prohibiting Importation of Slaves went into effect on January 1, 1808, the importation of slaves into the United States was prohibited, but not the internal slave trade, nor involvement in the international slave trade externally. Legal slavery persisted; most of those slaves already in the U.S. were legally emancipated only in 1863. Many American abolitionists took an active role in opposing slavery by supporting the Underground Railroad. Violent clashes between anti-slavery and pro-slavery Americans included Bleeding Kansas, a series of political and armed disputes in 1854–1861 as to whether Kansas would join the United States as a slave or free state. By 1860, the total number of slaves reached almost four million, and the American Civil War, beginning in 1861, led to the end of slavery in the United States. In 1863, Lincoln issued the Emancipation Proclamation, which freed slaves held in the Confederate States; the 13th Amendment to the U. S. Constitution prohibited most forms of slavery throughout the country.

Many of the freed slaves became sharecroppers and indentured servants. In this manner, some became tied to the very parcel of land into which they had been born a slave having little freedom or economic opportunity because of Jim Crow laws which perpetuated discrimination, limited education, promoted persecution without due process and resulted in continued poverty. Fear of reprisals such as unjust incarcerations and lynchings deterred upward mobility further.

France abolished slavery in 1794.

One of the most significant milestones in the campaign to abolish slavery throughout the world occurred in England in 1772, with British Judge Lord Mansfield, whose opinion in Somersett's Case was widely taken to have held that slavery was illegal in England. This judgement also laid down the principle that slavery contracted in other jurisdictions could not be enforced in England.

Sons of Africa was a late 18th-century British group that campaigned to end slavery. Its members were Africans in London, freed slaves who included Ottobah Cugoano, Olaudah Equiano and other leading members of London's black community. It was closely connected to the Society for the Abolition of the Slave Trade, a non-denominational group founded in 1787, whose members included Thomas Clarkson. British Member of Parliament William Wilberforce led the anti-slavery movement in the United Kingdom, although the groundwork was an anti-slavery essay by Clarkson. Wilberforce was urged by his close friend, Prime Minister William Pitt the Younger, to make the issue his own and was also given support by reformed Evangelical John Newton. The Slave Trade Act was passed by the British Parliament on March 25, 1807, making the slave trade illegal throughout the British Empire, Wilberforce also campaigned for abolition of slavery in the British Empire, which he lived to see in the Slavery Abolition Act 1833.

After the 1807 act abolishing the slave trade was passed, these campaigners switched to encouraging other countries to follow suit, notably France and the British colonies. Between 1808 and 1860, the British West Africa Squadron seized approximately 1,600 slave ships and freed 150,000 Africans who were aboard. Action was also taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against "the usurping King of Lagos", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.

In 1839, the world's oldest international human rights organization, Anti-Slavery International, was formed in Britain by Joseph Sturge, which campaigned to outlaw slavery in other countries. There were celebrations in 2007 to commemorate the 200th anniversary of the abolition of the slave trade in the United Kingdom through the work of the British Anti-Slavery Society.

In the 1860s, David Livingstone's reports of atrocities within the Arab slave trade in Africa stirred up the interest of the British public, reviving the flagging abolitionist movement. The Royal Navy throughout the 1870s attempted to suppress "this abominable Eastern trade", at Zanzibar in particular. In 1905, the French abolished indigenous slavery in most of French West Africa.

On December 10, 1948, the United Nations General Assembly adopted the Universal Declaration of Human Rights, which declared freedom from slavery is an internationally recognized human right. Article 4 of the Universal Declaration of Human Rights states:
In 2014, for the first time in history, major leaders of many religions, Buddhist, Anglican, Catholic, Orthodox Christian, Hindu, Jewish, and Muslim met to sign a shared commitment against modern-day slavery; the declaration they signed calls for the elimination of slavery and human trafficking by 2020. The signatories were: Pope Francis, Mātā Amṛtānandamayī, Bhikkhuni Thich Nu Chân Không (representing Zen Master Thích Nhất Hạnh), Datuk K Sri Dhammaratana, Chief High Priest of Malaysia, Rabbi Abraham Skorka, Rabbi David Rosen, Abbas Abdalla Abbas Soliman, Undersecretary of State of Al Azhar Alsharif (representing Mohamed Ahmed El-Tayeb, Grand Imam of Al-Azhar), Grand Ayatollah Mohammad Taqi al-Modarresi, Sheikh Naziyah Razzaq Jaafar, Special advisor of Grand Ayatollah (representing Grand Ayatollah Sheikh Basheer Hussain al Najafi), Sheikh Omar Abboud, Justin Welby, Archbishop of Canterbury, and Metropolitan Emmanuel of France (representing Ecumenical Patriarch Bartholomew.)

Groups such as the American Anti-Slavery Group, Anti-Slavery International, Free the Slaves, the Anti-Slavery Society, and the Norwegian Anti-Slavery Society continue to campaign to eliminate slavery.

On May 21, 2001, the National Assembly of France passed the Taubira law, recognizing slavery as a crime against humanity. Apologies on behalf of African nations, for their role in trading their countrymen into slavery, remain an open issue since slavery was practiced in Africa even before the first Europeans arrived and the Atlantic slave trade was performed with a high degree of involvement of several African societies. The black slave market was supplied by well-established slave trade networks controlled by local African societies and individuals.

There is adequate evidence citing case after case of African control of segments of the trade. Several African nations such as the Calabar and other southern parts of Nigeria had economies depended solely on the trade. African peoples such as the Imbangala of Angola and the Nyamwezi of Tanzania would serve as middlemen or roving bands warring with other African nations to capture Africans for Europeans.

Several historians have made important contributions to the global understanding of the African side of the Atlantic slave trade. By arguing that African merchants determined the assemblage of trade goods accepted in exchange for slaves, many historians argue for African agency and ultimately a shared responsibility for the slave trade.

In 1999, President Mathieu Kerekou of Benin issued a national apology for the central role Africans played in the Atlantic slave trade. Luc Gnacadja, minister of environment and housing for Benin, later said: "The slave trade is a shame, and we do repent for it." Researchers estimate that 3 million slaves were exported out of the Slave Coast bordering the Bight of Benin. President Jerry Rawlings of Ghana also apologized for his country's involvement in the slave trade.

The issue of an apology is linked to reparations for slavery and is still being pursued by entities across the world. For example, the Jamaican Reparations Movement approved its declaration and action plan. In 2007, British Prime Minister Tony Blair made a formal apology for Great Britain's involvement in slavery.

On February 25, 2007, the Commonwealth of Virginia resolved to 'profoundly regret' and apologize for its role in the institution of slavery. Unique and the first of its kind in the U.S., the apology was unanimously passed in both Houses as Virginia approached the 400th anniversary of the founding of Jamestown.

On August 24, 2007, Mayor Ken Livingstone of London apologized publicly for Britain's role in colonial slave trade. "You can look across there to see the institutions that still have the benefit of the wealth they created from slavery," he said, pointing towards the financial district. He claimed that London was still tainted by the horrors of slavery. Specifically, London outfitted, financed, and insured many of the ships, which helped fund the building of London's docks. Officials in Liverpool, which was a large slave trading port, apologized in 1999.

On July 30, 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws. In June 2009, the U.S. Senate passed a resolution apologizing to African-Americans for the "fundamental injustice, cruelty, brutality, and inhumanity of slavery". The news was welcomed by President Barack Obama, the nation's first president of African descent. Some of President Obama's ancestors may have been slave owners.

In 2010, Libyan leader Muammar Gaddafi apologized for Arab involvement in the slave trade, saying: "I regret the behavior of the Arabs… They brought African children to North Africa, they made them slaves, they sold them like animals, and they took them as slaves and traded them in a shameful way."

There have been movements to achieve reparations for those formerly held as slaves or for their descendants. Claims for reparations for being held in slavery are handled as a civil law matter in almost every country. This is often decried as a serious problem, since former slaves' relatives lack of money means they often have limited access to a potentially expensive and futile legal process. Mandatory systems of fines and reparations paid to an as yet undetermined group of claimants from fines, paid by unspecified parties, and collected by authorities have been proposed by advocates to alleviate this "civil court problem." Since in almost all cases there are no living ex-slaves or living ex-slave owners these movements have gained little traction. In nearly all cases the judicial system has ruled that the statute of limitations on these possible claims has long since expired.

The word "slavery" is often used as a pejorative to describe any activity in which one is coerced into performing. Some argue that military drafts and other forms of coerced government labour constitute "state-operated slavery." Some libertarians and anarcho-capitalists view government taxation as a form of slavery.

"Slavery" has been used by some anti-psychiatry proponents to define involuntary psychiatric patients, claiming there are no unbiased physical tests for mental illness and yet the psychiatric patient must follow the orders of the psychiatrist. They assert that instead of chains to control the slave, the psychiatrist uses drugs to control the mind. Drapetomania was a psychiatric diagnosis for a slave who did not want to be a slave.

Some proponents of animal rights have applied the term "slavery" to the condition of some or all human-owned animals, arguing that their status is comparable to that of human slaves.

The labor market, as institutionalized under today's market economic systems, has been criticized by mainstream socialists and by anarcho-syndicalists, who utilise the term wage slavery as a pejorative or dysphemism for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.

Film has been the most influential medium in the presentation of the history of slavery to the general public around the world. The American film industry has had a complex relationship with slavery and until recent decades often avoided the topic. Films such as "Birth of a Nation" (1915) and "Gone with the Wind" (1939) became controversial because they gave a favourable depiction. In 1940 "The Santa Fe Trail" gave a liberal but ambiguous interpretation of John Brown's attacks on slavery. "Song of the South" gave a favorable outlook on slavery in the United States in 1946.

The Civil Rights Movement in the 1950s made defiant slaves into heroes. The question of slavery in American memory necessarily involves its depictions in feature films. 

Most Hollywood films used American settings, although "Spartacus" (1960), dealt with an actual revolt in the Roman Empire known as the Third Servile War. The revolt failed, and all the rebels were executed, but their spirit lived on according to the film. "Spartacus" stays surprisingly close to the historical record.

"The Last Supper" ("La última cena" in Spanish) was a 1976 film directed by Cuban Tomás Gutiérrez Alea about the teaching of Christianity to slaves in Cuba, and emphasizes the role of ritual and revolt. "Burn!" takes place on the imaginary Portuguese island of Queimada (where the locals speak Spanish) and it merges historical events that took place in Brazil, Cuba, Santo Domingo, Jamaica, and elsewhere.

Historians agree that films have largely shaped historical memories, but they debate issues of accuracy, plausibility, moralism, sensationalism, how facts are stretched in search of broader truths, and suitability for the classroom. Berlin argues that critics complain if the treatment emphasizes historical brutality, or if it glosses over the harshness to highlight the emotional impact of slavery.








</doc>
<doc id="27993" url="https://en.wikipedia.org/wiki?curid=27993" title="September 17">
September 17




</doc>
<doc id="27995" url="https://en.wikipedia.org/wiki?curid=27995" title="Supply chain management">
Supply chain management

In commerce, supply chain management (SCM), the management of the flow of goods and services, involves the movement and storage of raw materials, of work-in-process inventory, and of finished goods as well as end to end order fulfillment from point of origin to point of consumption. Interconnected, interrelated or interlinked networks, channels and node businesses combine in the provision of products and services required by end customers in a supply chain. Supply-chain management has been defined as the "design, planning, execution, control, and monitoring of supply-chain activities with the objective of creating net value, building a competitive infrastructure, leveraging worldwide logistics, synchronizing supply with demand and measuring performance globally."
SCM practice draws heavily from the areas of industrial engineering, systems engineering, operations management, logistics, procurement, information technology, and marketing and strives for an integrated approach. Marketing channels play an important role in supply-chain management. Current research in supply-chain management is concerned with topics related to sustainability and risk management, among others. Some suggest that the “people dimension” of SCM, ethical issues, internal integration, transparency/visibility, and human capital/talent management are topics that have, so far, been underrepresented on the research agenda.

Although it has the same goals as supply chain engineering, supply chain management is focused on a more traditional management and business based approach, whereas supply chain engineering is focused on a mathematical model based one.

Supply-chain management, techniques with the aim of coordinating all parts of SC from supplying raw materials to delivering and/or resumption of products, tries to minimize total costs with respect to existing conflicts among the chain partners. An example of these conflicts is the interrelation between the sale department desiring to have higher inventory levels to fulfill demands and the warehouse for which lower inventories are desired to reduce holding costs.

In 1982, Keith Oliver, a consultant at Booz Allen Hamilton introduced the term "supply chain management" to the public domain in an interview for the Financial Times. In 1983  WirtschaftsWoche in Germany published for the first time the results of an implemented and so called "Supply Chain Management project", led by Wolfgang Partsch.

In the mid-1990s, more than a decade later, the term "supply chain management" gained currency when a flurry of articles and books came out on the subject. Supply chains were originally defined as encompassing all activities associated with the flow and transformation of goods from raw materials through to the end user, as well as the associated information flows. Supply-chain management was then further defined as the integration of supply chain activities through improved supply-chain relationships to achieve a competitive advantage.

In the late 1990s, "supply-chain management" (SCM) rose to prominence, and operations managers began to use it in their titles with increasing regularity.

Other commonly accepted definitions of supply-chain management include:

A supply chain, as opposed to supply-chain management, is a set of organizations directly linked by one or more upstream and downstream flows of products, services, finances, or information from a source to a customer. Supply-chain management is the management of such a chain.

Supply-chain-management software includes tools or modules used to execute supply chain transactions, manage supplier relationships, and control associated business processes.

Supply-chain event management (SCEM) considers all possible events and factors that can disrupt a supply chain. With SCEM, possible scenarios can be created and solutions devised.

In some cases, the supply chain includes the collection of goods after consumer use for recycling or the reverse logistics processes for returning faulty or unwanted products backwards to producers early in the value chain.

Supply-chain management is a cross-functional approach that includes managing the movement of raw materials into an organization, certain aspects of the internal processing of materials into finished goods, and the movement of finished goods out of the organization and toward the end consumer. As organizations strive to focus on core competencies and become more flexible, they reduce their ownership of raw materials sources and distribution channels. These functions are increasingly being outsourced to other firms that can perform the activities better or more cost effectively. The effect is to increase the number of organizations involved in satisfying customer demand, while reducing managerial control of daily logistics operations. Less control and more supply-chain partners lead to the creation of the concept of supply-chain management. The purpose of supply-chain management is to improve trust and collaboration among supply-chain partners thus improving inventory visibility and the velocity of inventory movement. in this section we have to communicate with all the vendors, suppliers and after that we have to take some comparisons after that we have to place the order.

Organizations increasingly find that they must rely on effective supply chains, or networks, to compete in the global market and networked economy. In Peter Drucker's (1998) new management paradigms, this concept of business relationships extends beyond traditional enterprise boundaries and seeks to organize entire business processes throughout a value chain of multiple companies.

In recent decades, globalization, outsourcing, and information technology have enabled many organizations, such as Dell and Hewlett Packard, to successfully operate collaborative supply networks in which each specialized business partner focuses on only a few key strategic activities. This inter-organisational supply network can be acknowledged as a new form of organisation. However, with the complicated interactions among the players, the network structure fits neither "market" nor "hierarchy" categories. It is not clear what kind of performance impacts different supply-network structures could have on firms, and little is known about the coordination conditions and trade-offs that may exist among the players. From a systems perspective, a complex network structure can be decomposed into individual component firms. Traditionally, companies in a supply network concentrate on the inputs and outputs of the processes, with little concern for the internal management working of other individual players. Therefore, the choice of an internal management control structure is known to impact local firm performance.

In the 21st century, changes in the business environment have contributed to the development of supply-chain networks. First, as an outcome of globalization and the proliferation of multinational companies, joint ventures, strategic alliances, and business partnerships, significant success factors were identified, complementing the earlier "just-in-time", lean manufacturing, and agile manufacturing practices. Second, technological changes, particularly the dramatic fall in communication costs (a significant component of transaction costs), have led to changes in coordination among the members of the supply chain network.

Many researchers have recognized supply network structures as a new organisational form, using terms such as "Keiretsu", "Extended Enterprise", "Virtual Corporation", "Global Production Network", and "Next Generation Manufacturing System". In general, such a structure can be defined as "a group of semi-independent organisations, each with their capabilities, which collaborate in ever-changing constellations to serve one or more markets in order to achieve some business goal specific to that collaboration".

The importance of supply chain management proved crucial in the 2019-2020 fight against the coronavirus (COVID-19) pandemic that swept across the world.  During that event, governments in countries that had in place an effective domestic supply chain management had enough medical supplies to support their needs and enough to donate their surplus to front-line health workers in other jurisdictions.  Some organizations were able to quickly develop foreign supply chains in order to import much needed medical supplies.

Supply-chain management is also important for organizational learning. Firms with geographically more extensive supply chains connecting diverse trading cliques tend to become more innovative and productive.

The security-management system for supply chains is described in ISO/IEC 28000 and ISO/IEC 28001 and related standards published jointly by the ISO and the IEC. Supply-Chain Management draws heavily from the areas of operations management, logistics, procurement, and information technology, and strives for an integrated approach.

Six major movements can be observed in the evolution of supply-chain management studies: creation, integration, and globalization, specialization phases one and two, and SCM 2.0.

The term "supply chain management" was first coined by Keith Oliver in 1982. However, the concept of a supply chain in management was of great importance long before, in the early 20th century, especially with the creation of the assembly line. The characteristics of this era of supply-chain management include the need for large-scale changes, re-engineering, downsizing driven by cost reduction programs, and widespread attention to Japanese management practices. However, the term became widely adopted after the publication of the seminal book "Introduction to Supply Chain Management" in 1999 by Robert B. Handfield and Ernest L. Nichols, Jr., which published over 25,000 copies and was translated into Japanese, Korean, Chinese, and Russian.

This era of supply-chain-management studies was highlighted with the development of electronic data interchange (EDI) systems in the 1960s, and developed through the 1990s by the introduction of enterprise resource planning (ERP) systems. This era has continued to develop into the 21st century with the expansion of Internet-based collaborative systems. This era of supply-chain evolution is characterized by both increasing value added and reducing costs through integration.

A supply chain can be classified as a stage 1, 2 or 3 network. In a stage 1–type supply chain, systems such as production, storage, distribution, and material control are not linked and are independent of each other. In a stage 2 supply chain, these are integrated under one plan and enterprise resource planning (ERP) is enabled. A stage 3 supply chain is one that achieves vertical integration with upstream suppliers and downstream customers. An example of this kind of supply chain is Tesco.

It is the third movement of supply-chain-management development, the globalization era, can be characterized by the attention given to global systems of supplier relationships and the expansion of supply chains beyond national boundaries and into other continents. Although the use of global sources in organisations' supply chains can be traced back several decades (e.g., in the oil industry), it was not until the late 1980s that a considerable number of organizations started to integrate global sources into their core business. This era is characterized by the globalization of supply-chain management in organizations with the goal of increasing their competitive advantage, adding value, and reducing costs through global sourcing.

In the 1990s, companies began to focus on "core competencies" and specialization. They abandoned vertical integration, sold off non-core operations, and outsourced those functions to other companies. This changed management requirements, as the supply chain extended beyond the company walls and management was distributed across specialized supply-chain partnerships.

This transition also refocused the fundamental perspectives of each organization. Original equipment manufacturers (OEMs) became brand owners that required visibility deep into their supply base. They had to control the entire supply chain from above, instead of from within. Contract manufacturers had to manage bills of material with different part-numbering schemes from multiple OEMs and support customer requests for work-in-process visibility and vendor-managed inventory (VMI).

The specialization model creates manufacturing and distribution networks composed of several individual supply chains specific to producers, suppliers, and customers that work together to design, manufacture, distribute, market, sell, and service a product. This set of partners may change according to a given market, region, or channel, resulting in a proliferation of trading partner environments, each with its own unique characteristics and demands.

Specialization within the supply chain began in the 1980s with the inception of transportation brokerages, warehouse management (storage and inventory), and non-asset-based carriers, and has matured beyond transportation and logistics into aspects of supply planning, collaboration, execution, and performance management.

Market forces sometimes demand rapid changes from suppliers, logistics providers, locations, or customers in their role as components of supply-chain networks. This variability has significant effects on supply-chain infrastructure, from the foundation layers of establishing and managing electronic communication between trading partners, to more complex requirements such as the configuration of processes and work flows that are essential to the management of the network itself.

Supply-chain specialization enables companies to improve their overall competencies in the same way that outsourced manufacturing and distribution has done; it allows them to focus on their core competencies and assemble networks of specific, best-in-class partners to contribute to the overall value chain itself, thereby increasing overall performance and efficiency. The ability to quickly obtain and deploy this domain-specific supply-chain expertise without developing and maintaining an entirely unique and complex competency in house is a leading reason why supply-chain specialization is gaining popularity.

Outsourced technology hosting for supply-chain solutions debuted in the late 1990s and has taken root primarily in transportation and collaboration categories. This has progressed from the application service provider (ASP) model from roughly 1998 through 2003, to the on-demand model from approximately 2003 through 2006, to the software as a service (SaaS) model currently in focus today.

Building on globalization and specialization, the term "SCM 2.0" has been coined to describe both changes within supply chains themselves as well as the evolution of processes, methods, and tools to manage them in this new "era". The growing popularity of collaborative platforms is highlighted by the rise of TradeCard's supply-chain-collaboration platform, which connects multiple buyers and suppliers with financial institutions, enabling them to conduct automated supply-chain finance transactions.

Web 2.0 is a trend in the use of the World Wide Web that is meant to increase creativity, information sharing, and collaboration among users. At its core, the common attribute of Web 2.0 is to help navigate the vast information available on the Web in order to find what is being bought. It is the notion of a usable pathway. SCM 2.0 replicates this notion in supply chain operations. It is the pathway to SCM results, a combination of processes, methodologies, tools, and delivery options to guide companies to their results quickly as the complexity and speed of the supply-chain increase due to global competition; rapid price fluctuations; changing oil prices; short product life cycles; expanded specialization; near-, far-, and off-shoring; and talent scarcity.

Successful SCM requires a change from managing individual functions to integrating activities into key supply-chain processes. In an example scenario, a purchasing department places orders as its requirements become known. The marketing department, responding to customer demand, communicates with several distributors and retailers as it attempts to determine ways to satisfy this demand. Information shared between supply-chain partners can only be fully leveraged through process integration.

Supply-chain business-process integration involves collaborative work between buyers and suppliers, joint product development, common systems, and shared information. According to Lambert and Cooper (2000), operating an integrated supply chain requires a continuous information flow.
However, in many companies, management has concluded that optimizing product flows cannot be accomplished without implementing a process approach. The key supply-chain processes stated by Lambert (2004) are:

Much has been written about demand management. Best-in-class companies have similar characteristics, which include the following:

One could suggest other critical supply business processes that combine these processes stated by Lambert, such as:




Integration of suppliers into the new product development process was shown to have a major impact on product target cost, quality, delivery, and market share. Tapping into suppliers as a source of innovation requires an extensive process characterized by development of technology sharing, but also involves managing intellectual property issues.







There are gaps in the literature on supply-chain management studies at present (2015): there is no theoretical support for explaining the existence or the boundaries of supply-chain management. A few authors, such as Halldorsson et al., Ketchen and Hult (2006), and Lavassani et al. (2009), have tried to provide theoretical foundations for different areas related to supply chain by employing organizational theories, which may include the following:

However, the unit of analysis of most of these theories is not the supply chain but rather another system, such as the firm or the supplier-buyer relationship. Among the few exceptions is the relational view, which outlines a theory for considering dyads and networks of firms as a key unit of analysis for explaining superior individual firm performance (Dyer and Singh, 1998).

The management of supply chains involve a number of specific challenges regarding the organization of relationships among the different partners along the value chain. Formal and informal governance mechanisms are central elements in the management of supply chain. Research in supply chain management has noted the importance of using the appropriate combination of contracts and relational norms to mitigate the risks and prevent conflicts between supply chain partners. In turn, particular combinations of governance mechanisms may impact the relational dynamics within the supply chain.

In the study of supply-chain management, the concept of centroids has become an important economic consideration. A centroid is a location that has a high proportion of a country's population and a high proportion of its manufacturing, generally within . In the US, two major supply chain centroids have been defined, one near Dayton, Ohio, and a second near Riverside, California.

The centroid near Dayton is particularly important because it is closest to the population center of the US and Canada. Dayton is within 500 miles of 60% of the US population and manufacturing capacity, as well as 60% of Canada's population. The region includes the interchange between I-70 and I-75, one of the busiest in the nation, with 154,000 vehicles passing through per day, of which 30–35% are trucks hauling goods. In addition, the I-75 corridor is home to the busiest north-south rail route east of the Mississippi River.

In 2010, Wal-Mart announced a big change in its sourcing strategy. Initially, Wal-Mart relied on intermediaries in the sourcing process. It bought only 20% of its stock directly, but the rest were bought through the intermediaries. Therefore, the company came to realize that the presence of many intermediaries in the product sourcing was actually increasing the costs in the supply chain. To cut these costs, Wal-Mart decided to do away with intermediaries in the supply chain and started direct sourcing of its goods from the suppliers. Eduardo Castro-Wright, the then Vice President of Wal-Mart, set an ambitious goal of buying 80% of all Wal-Mart goods directly from the suppliers. Walmart started purchasing fruits and vegetables on a global scale, where it interacted directly with the suppliers of these goods. The company later engaged the suppliers of other goods, such as cloth and home electronics appliances, directly and eliminated the importing agents. The purchaser, in this case Wal-Mart, can easily direct the suppliers on how to manufacture certain products so that they can be acceptable to the consumers. Thus, Wal-Mart, through direct sourcing, manages to get the exact product quality as it expects, since it engages the suppliers in the producing of these products, hence quality consistency. Using agents in the sourcing process in most cases lead to inconsistency in the quality of the products, since the agent's source the products from different manufacturers that have varying qualities.

Wal-Mart managed to source directly 80% profit its stock; this has greatly eliminated the intermediaries and cut down the costs between 5-15%, as markups that are introduced by these middlemen in the supply chain are cut. This saves approximately $4–15 billion. This strategy of direct sourcing not only helped Wal-Mart in reducing the costs in the supply chain but also helped in the improvement of supply chain activities through boosting efficiency throughout the entire process. In other words, direct sourcing reduced the time that takes the company to source and stocks the products in its stock. The presence of the intermediaries elongated the time in the process of procurement, which sometimes led to delays in the supply of the commodities in the stores, thus, customers finding empty shelves. Wal-Mart adopted this strategy of sourcing through centralizing the entire process of procurement and sourcing by setting up four global merchandising points for general goods and clothing. The company instructed all the suppliers to bring their products to these central points that are located in different markets. The procurement team assesses the quality brought by the suppliers, buys the goods, and distributes them to various regional markets. The procurement and sourcing at centralized places helped the company to consolidate the suppliers.

The company has established four centralized points, including an office in Mexico City and Canada. Just a mere piloting test on combining the purchase of fresh apples across the United States, Mexico, and Canada led to the savings of about 10%. As a result, the company intended to increase centralization of its procurement in North America for all its fresh fruits and vegetables. Thus, centralization of the procurement process to various points where the suppliers would be meeting with the procurement team is the latest strategy which the company is implementing, and signs show that this strategy is going to cut costs and also improve the efficiency of the procumbent process.

Strategic vendor partnerships is another strategy the company is using in the sourcing process. Wal-Mart realized that in order for it to ensure consistency in the quality of the products it offers to the consumers and also maintain a steady supply of goods in its stores at a lower cost, it had to create strategic vendor partnerships with the suppliers. Wal-Mart identified and selected the suppliers who met its demand and at the same time offered it the best prices for the goods. It then made a strategic relationship with these vendors by offering and assuring the long-term and high volume of purchases in exchange for the lowest possible prices. Thus, the company has managed to source its products from same suppliers as bulks, but at lower prices. This enables the company to offer competitive prices for its products in its stores, hence, maintaining a competitive advantage over its competitors whose goods are a more expensive in comparison.

Another sourcing strategy Wal-Mart uses is implementing efficient communication relationships with the vendor networks; this is necessary to improve the material flow. The company has all the contacts with the suppliers whom they communicate regularly and make dates on when the goods would be needed, so that the suppliers get ready to deliver the goods in time. The efficient communication between the company's procurement team and the inventory management team enables the company to source goods and fill its shelves on time, without causing delays and empty shelves. In other words, the company realized that in ensuring a steady flow of the goods into the store, the suppliers have to be informed early enough, so that they can act accordingly to avoid delays in the delivery of goods. Thus, efficient communication is another tool which Wal-Mart is using to make the supply chain be more efficient and to cut costs.

Cross-docking is another strategy that Wal-Mart is using to cut costs in its supply chain. Cross-docking is the process of transferring goods directly from inbound trucks to outbound trucks. When the trucks from the suppliers arrive at the distribution centers, most of the trucks are not offloaded to keep the goods in the distribution centers or warehouses; they are transferred directly to another truck designated to deliver goods to specific retail stores for sale. Cross-docking helps in saving the storage costs. Initially, the company was incurring considerable costs of storing the suppliers from the suppliers in its warehouses and the distributions centers to await the distribution trucks to the retail stores in various regions.

Tax-efficient supply-chain management is a business model that considers the effect of tax in the design and implementation of supply-chain management. As the consequence of globalization, cross-national businesses pay different tax rates in different countries. Due to these differences, they may legally optimize their supply chain and increase profits based on tax efficiency.

Supply-chain sustainability is a business issue affecting an organization's supply chain or logistics network, and is frequently quantified by comparison with SECH ratings, which uses a triple bottom line incorporating economic, social, and environmental aspects. SECH ratings are defined as social, ethical, cultural, and health' footprints. Consumers have become more aware of the environmental impact of their purchases and companies' SECH ratings and, along with non-governmental organizations (NGOs), are setting the agenda for transitions to organically grown foods, anti-sweatshop labor codes, and locally produced goods that support independent and small businesses. Because supply chains may account for over 75% of a company's carbon footprint, many organizations are exploring ways to reduce this and thus improve their SECH rating.

For example, in July 2009, Wal-Mart announced its intentions to create a global sustainability index that would rate products according to the environmental and social impacts of their manufacturing and distribution. The index is intended to create environmental accountability in Wal-Mart's supply chain and to provide motivation and infrastructure for other retail companies to do the same.

It has been reported that companies are increasingly taking environmental performance into account when selecting suppliers. A 2011 survey by the Carbon Trust found that 50% of multinationals expect to select their suppliers based upon carbon performance in the future and 29% of suppliers could lose their places on 'green supply chains' if they do not have adequate performance records on carbon.

The US Dodd–Frank Wall Street Reform and Consumer Protection Act, signed into law by President Obama in July 2010, contained a supply chain sustainability provision in the form of the Conflict Minerals law. This law requires SEC-regulated companies to conduct third party audits of their supply chains in order to determine whether any tin, tantalum, tungsten, or gold (together referred to as "conflict minerals") is mined or sourced from the Democratic Republic of the Congo, and create a report (available to the general public and SEC) detailing the due diligence efforts taken and the results of the audit. The chain of suppliers and vendors to these reporting companies will be expected to provide appropriate supporting information.

Incidents like the 2013 Savar building collapse with more than 1,100 victims have led to widespread discussions about corporate social responsibility across global supply chains. Wieland and Handfield (2013) suggest that companies need to audit products and suppliers and that supplier auditing needs to go beyond direct relationships with first-tier suppliers. They also demonstrate that visibility needs to be improved if supply cannot be directly controlled and that smart and electronic technologies play a key role to improve visibility. Finally, they highlight that collaboration with local partners, across the industry and with universities is crucial to successfully managing social responsibility in supply chains.

Circular Supply-Chain Management (CSCM) is "the configuration and coordination of the organisational functions marketing, sales, R&D, production, logistics, IT, finance, and customer service within and across business units and organizations to close, slow, intensify, narrow, and dematerialise material and energy loops to minimise resource input into and waste and emission leakage out of the system, improve its operative effectiveness and efficiency and generate competitive advantages". By reducing resource input and waste leakage along the supply chain and configure it to enable the recirculation of resources at different stages of the product or service lifecycle, potential economic and environmental benefits can be achieved. These comprise e.g. a decrease in material and waste management cost and reduced emissions and resource consumption.

SCM components are the third element of the four-square circulation framework. The level of integration and management of a business process link is a function of the number and level of components added to the link. Consequently, adding more management components or increasing the level of each component can increase the level of integration of the business process link.

Literature on business process reengineering buyer-supplier relationships, and SCM suggests various possible components that should receive managerial attention when managing supply relationships. Lambert and Cooper (2000) identified the following components:

However, a more careful examination of the existing literature leads to a more comprehensive understanding of what should be the key critical supply chain components, or "branches" of the previously identified supply chain business processes—that is, what kind of relationship the components may have that are related to suppliers and customers. Bowersox and Closs (1996) state that the emphasis on cooperation represents the synergism leading to the highest level of joint achievement. A primary-level channel participant is a business that is willing to participate in responsibility for inventory ownership or assume other financial risks, thus including primary level components. A secondary-level participant (specialized) is a business that participates in channel relationships by performing essential services for primary participants, including secondary level components, which support primary participants. Third-level channel participants and components that support primary-level channel participants and are the fundamental branches of secondary-level components may also be included.

Consequently, Lambert and Cooper's framework of supply chain components does not lead to any conclusion about what are the primary- or secondary-level (specialized) supply chain components —that is, which supply chain components should be viewed as primary or secondary, how these components should be structured in order to achieve a more comprehensive supply chain structure, and how to examine the supply chain as an integrative one.

Reverse logistics is the process of managing the return of goods and may be considered as an aspect of "aftermarket customer services". Any time money is taken from a company's warranty reserve or service logistics budget, one can speak of a reverse logistics operation. Reverse logistics also includes the process of managing the return of goods from store, which the returned goods are sent back to warehouse and after that either warehouse scrap the goods or send them back to supplier for replacement depending on the warranty of the merchandise.

Consultancies and media expect the performance efficacy of digitalisation in supply chains to be very high. Renowned journals such as the Journal of Business Logistics, the Journal of Operations Management and the International Journal of Physical Distribution & Logistics Management have already published several special issues for various aspects of digitalisation in supply chain management. Additive manufacturing and blockchain technology have emerged as the two technologies with some of the highest economic relevance. The potential of additive manufacturing is particularly high in the production of spare parts, since its introduction can reduce warehousing costs of slowly rotating spare parts. Yet, the technology bears the potential to completely disrupt and restructure supply chains and hitherto existing production routes.

In comparison, research on the influence of blockchain technology on the supply chain is still in its early stages. The conceptual literature has argued for a considerably long time that the highest performance efficacy is expected in the technology’s potential for automatic contract creation. However, latest empirical findings contradict this hypothesis. The highest potential is currently expected in the arenas of verified customer reviews and certifications of product quality and standards. Companies like DNVG-L have already developed blockchain products such as MY STORY that can be used to create more secure supply chains. Further empirical results are expected soon.

In addition, the technological features of blockchains support transparency and traceability of information as well as high levels of reliability and immutability of records. Blockchains thus offer opportunities to foster collaboration in the supply chain.

Supply chain systems configure value for those that organize the networks. Value is the additional revenue over and above the costs of building the network. Co-creating value and sharing the benefits appropriately to encourage effective participation is a key challenge for any supply system. Tony Hines defines value as follows: "Ultimately it is the customer who pays the price for service delivered that confirms value and not the producer who simply adds cost until that point".

Global supply chains pose challenges regarding both quantity and value. Supply and value chain trends include:

These trends have many benefits for manufacturers because they make possible larger lot sizes, lower taxes, and better environments (e.g., culture, infrastructure, special tax zones, or sophisticated OEM) for their products. There are many additional challenges when the scope of supply chains is global. This is because with a supply chain of a larger scope, the lead time is much longer, and because there are more issues involved, such as multiple currencies, policies, and laws. The consequent problems include different currencies and valuations in different countries, different tax laws, different trading protocols, vulnerability to natural disasters and cyber threats, and lack of transparency of cost and profit.

Supply-chain consulting is the providing of expert knowledge in order to assess the productivity of a supply-chain and, ideally, to enhance the productivity.

Supply chain Consulting is a service involved in transfer of knowledge on how to exploit existing assets through improved coordination and can hence be a source of competitive advantage; Hereby the role of the consultant is to help management by adding value to the whole process through the various sectors from the ordering of the raw materials to the final product.

On this regard, firms either build internal teams of consultants to tackle the issue or use external ones, (companies choose between these two approaches taking into consideration various factors).

The use of external consultants is a common practice among companies. The whole consulting process generally involves the analysis of the entire supply-chain process, including the countermeasures or correctives to take to achieve a better overall performance.

Supply chain professionals need to have knowledge of managing supply chain functions such as transportation, warehousing, inventory management, and production planning. In the past, supply chain professionals emphasized logistics skills, such as knowledge of shipping routes, familiarity with warehousing equipment and distribution center locations and footprints, and a solid grasp of freight rates and fuel costs. More recently, supply-chain management extends to logistical support across firms and management of global supply chains. Supply chain professionals need to have an understanding of business continuity basics and strategies.

Supply chain professionals play major roles in the design and management of supply chains. In the design of supply chains, they help determine whether a product or service is provided by the firm itself (insourcing) or by another firm elsewhere (outsourcing). In the management of supply chains, supply chain professionals coordinate production among multiple providers, ensuring that production and transport of goods happen with minimal quality control or inventory problems. One goal of a well-designed and maintained supply chain for a product is to successfully build the product at minimal cost. Such a supply chain could be considered a competitive advantage for a firm.

Beyond design and maintenance of a supply chain itself, supply chain professionals participate in aspects of business that have a bearing on supply chains, such as sales forecasting, quality management, strategy development, customer service, and systems analysis. Production of a good may evolve over time, rendering an existing supply chain design obsolete. Supply chain professionals need to be aware of changes in production and business climate that affect supply chains and create alternative supply chains as the need arises.
Individuals working in supply-chain management can attain a professional certification by passing an exam developed by a third party certification organizations. The purpose of certification is to guarantee a certain level of expertise in the field.

The knowledge needed to pass a certification exam may be gained from several sources. Some knowledge may come from college courses, but most of it is acquired from a mix of on-the-job learning experiences, attending industry events, learning best practices with their peers, and reading books and articles in the field. Certification organizations may provide certification workshops tailored to their exams. There are also free websites that provide a significant amount of educational articles, as well as blogs that are internationally recognized which provide good sources of news and updates.

The following North American universities rank high in their master's education in the SCM World University 100 ranking, which was published in 2017 and which is based on the opinions of supply chain managers: Michigan State University, Penn State University, University of Tennessee, Massachusetts Institute of Technology, Arizona State University, University of Texas at Austin and Western Michigan University. In the same ranking, the following European universities rank high: Cranfield School of Management, Vlerick Business School, INSEAD, Cambridge University, Eindhoven University of Technology, London Business School and Copenhagen Business School. In the 2016 Eduniversal Best Masters Ranking Supply Chain and Logistics the following universities rank high: Massachusetts Institute of Technology, KEDGE Business School, Purdue University, Rotterdam School of Management, Pontificia Universidad Catolica del Peru, Universidade Nova de Lisboa, Vienna University of Economics and Business and Copenhagen Business School.

A number of organizations provide certification in supply chain management, such as the Council of Supply Chain Management Professionals (CSCMP), IIPMR (International Institute for Procurement and Market Research), APICS (the Association for Operations Management), ISCEA (International Supply Chain Education Alliance) and IoSCM (Institute of Supply Chain Management). APICS' certification is called "Certified Supply Chain Professional", or CSCP, and ISCEA's certification is called the "Certified Supply Chain Manager" (CSCM), CISCM (Chartered Institute of Supply Chain Management) awards certificate as "Chartered Supply Chain Management Professional" (CSCMP). Another, the Institute for Supply Management, is developing one called the "Certified Professional in Supply Management" (CPSM) focused on the procurement and sourcing areas of supply-chain management. The Supply Chain Management Association (SCMA) is the main certifying body for Canada with the designations having global reciprocity. The designation Supply Chain Management Professional (SCMP) is the title of the supply chain leadership designation.

The following table compares topics addressed by selected professional supply chain certification programmes.




</doc>
<doc id="27998" url="https://en.wikipedia.org/wiki?curid=27998" title="Synchronized swimming">
Synchronized swimming

Synchronised swimming (in American English, synchronized swimming) or artistic swimming is a hybrid form of swimming, dance, and gymnastics, consisting of swimmers performing a synchronised routine (either solo, duet, trio, mixed duet, free team, free combination, and highlight) of elaborate moves in the water, accompanied by music. Artistic swimming is governed internationally by FINA, and has been part of the Summer Olympics programme since 1984.

Synchronised swimming demands advanced water skills, requires great strength, endurance, flexibility, grace, artistry and precise timing, as well as exceptional breath control when upside down underwater. Competitors show off their strength, flexibility, and aerobic endurance required to perform difficult routines. Swimmers perform two routines for judges, one technical and one free, as well as age group routines and figures. Synchronized swimming is both an individual and team sport. Swimmers compete individually during figures, and then as a team during the routine. Figures are made up of a combination of skills and positions that often require control, strength, and flexibility. Swimmers are ranked individually for this part of the competition. The routine involves teamwork and synchronisation. It is choreographed to music and often has a theme.

Since the 20th century, synchronised swimming has predominantly been considered a women's sport, with the Summer Olympics only featuring women's duet and team events. However, international, national and regional competitions may allow men to compete, and FINA introduced a new mixed duet competition at the 2015 World Aquatics Championships. FINA officially renamed the sport from "synchronized swimming" to "artistic swimming" in 2017—a decision that faced mixed reception.

At the turn of the 20th century, synchronised swimming was known as water ballet. The first recorded competition was in 1891 in Berlin, Germany. Many swim clubs were formed around that time, and the sport simultaneously developed in Canada. As well as existing as a sport, it often constituted a popular addition to Music Hall evenings, in the larger variety theatres of London or Glasgow which were equipped with on-stage water tanks for the purpose.

In 1907, Australian Annette Kellerman popularised the sport when she performed in a glass tank as an underwater ballerina (the first water ballet in a glass tank) in the New York Hippodrome. After experimenting with various diving actions and stunts in the water, Katherine Curtis started one of the first water ballet clubs at the University of Chicago, where the team began executing strokes, "tricks," and floating formations. On May 27, 1939, the first U.S. synchronised swimming competition took place at Wright Junior College between Wright and the Chicago Teachers' College.

In 1924, the first competition in North America was in Montreal, with Peg Seller as the first champion.

Other important pioneers for the sport are Beulah Gundling, Käthe Jacobi, Marion Kane Elston, Dawn Bean, Billie MacKellar, Teresa Anderson, Gail Johnson, Gail Emery, Charlotte Davis, Mary Derosier, Norma Olsen and Clark Leach. Charlotte Davis coached Tracie Ruiz and Candy Costie, who won the gold medal in duet synchronised swimming at the 1984 Olympics in Los Angeles.

In 1933 and 1934, Katherine Whitney Curtis organised a show, "The Kay Curtis Modern Mermaids", for the World Exhibition in Chicago. The announcer, Norman Ross, introduced the sport as "synchronised swimming" for the first time. The term eventually became standardised through the AAU, but Curtis still used the term "rhythmic swimming" in her book, "Rhythmic Swimming: A Source Book of Synchronised Swimming and Water Pageantry" (Minneapolis: Burgess Publishing Co., 1936).

Curtis persuaded the AAU to make synchronised swimming an officially recognised sport in December 1941, but she herself transferred overseas in 1943. She served as the Recreation Director of the Red Cross under Generals Patton and Eisenhower, during which time she produced the first international aquacade in Caserta, Italy. She was the Director of Travel in post-war Europe until 1962. In 1959 the Helms Hall of Fame officially recognised Curtis (along with Annette Kellerman) – ascribing to her the primary development of synchronised swimming. In 1979 the International Swimming Hall of Fame inducted Curtis with similar accolades.

The first Official National Team Championships were held in Chicago at Riis Pool on August 11, 1946. The Town Club 'C' team were the first national champions. The team was composed of: Polly Wesner, Nancy Hanna, Doris Dieskow, Marion Mittlacher, Shirley Brown, Audrey Huettenrauch, Phyllis Burrell and Priscilla Hirsch.

Esther Williams, a national AAU champion swimmer, popularized synchronised swimming during WWII and after, through (often elaborately staged) scenes in Hollywood films such as "Bathing Beauty" (1944), "Million Dollar Mermaid" (1952), and "Jupiter's Darling" (1955). In the 1970s and 1980s, Ft. Lauderdale swimming champion Charkie Phillips revived water ballet on television with The Krofftettes in "The Brady Bunch Hour" (1976–1977), NBC's "The Big Show" (1980), and then on screen with Miss Piggy in "The Great Muppet Caper" (1981).

Margaret Swan Forbes published "Coaching Synchronized Swimming Effectively" in 1984; it was the first official teaching manual for synchronized swimming.

In the late 19th century, synchronised swimming was a male-only event. However, in the 20th century it became a women's sport, with men banned from many competitions. In the U.S., men were allowed to participate with women until 1941, when synchronised swimming became part of the Amateur Athletic Union (AAU). The AAU required men and women to compete separately, which resulted in a decline of male participants. In the 1940s and 1950s, Bert Hubbard and Donn Squire were among the top US male competitors.

In 1978, the U.S. changed their rules to allow men to once again compete with women. Rules in other countries varied; in the UK, men were prohibited from competing until 2014, while in France, Benoît Beaufils was allowed to competed at national events in the 1990s. American Bill May was a top competitor in the late-1990s and early-2000s. He medalled in several international events, including the 1998 Goodwill Games. However, male competitors were barred from top competitions, including the World Aquatics Championships and the Olympics. However, at the 2015 World Aquatics Championships, FINA introduced a new mixed duet discipline. Both May and Beaufils returned from decade-long retirements to represent their countries. Among their competitors were Russian Aleksandr Maltsev and Italian Giorgio Minisini, both over 15 years younger than May and Beaufils. Pairs from ten countries competed in the inaugural events. The 2016 European Aquatics Championships was the first time men were allowed to compete at the European Championships. While men are allowed in more events, they were still barred from competing in the 2016 Summer Olympics. FINA did propose adding the mixed duet competition to the 2020 Summer Olympics.
In July 2017, following a request by the IOC, FINA approved changes to its constitution that renamed synchronised swimming to artistic swimming. FINA justified the change by stating that it would help to clarify the nature of the sport (with the new name being similar to artistic gymnastics), and would help "enhance its popularity". The changes received criticism, with some swimmers and coaches arguing that the name "artistic swimming" diminishes the athleticism of the sport, and that rebranding federations and other groups involved in the sport would be costly. Deputy Prime Minister of Russia Vitaly Mutko vowed that the country would still refer to the sport as synchronised swimming, stating that "to keep the name synchronised swimming is our right, and if the Federation itself, the coaches will want it, we will do it". Most national governing bodies have adopted the new name, with the CEO of USA Artistic Swimming stating in 2020 that "19 of the top 25 countries in the world are either partially or fully using the name artistic swimming". Competitions where the new name was first used include the 2019 World Aquatics Championships and the 2018 Asian Games. It will also be used at the 2020 Summer Olympics and the 2020 European Aquatics Championships.

The first Olympic demonstration was at the 1952 Olympic Games, where the Helsinki officials welcomed Kay Curtis and lit a torch in her honor. Curtis died in 1980, but synchronised swimming did not become an official Olympic sport until the 1984 Summer Olympic Games. It was not until 1968 that synchronised swimming became officially recognized by FINA as the fourth water sport next to swimming, platform diving and water polo.

From 1984 through 1992, the Summer Olympic Games featured solo and duet competitions, but they were both dropped in 1996 in favor of team competition. At the 2000 Olympic Games, however, the duet competition was restored and is now featured alongside the team competition. 
Artistic swimming has been part of the World Aquatics Championships since the beginning. From 1973 through 2001, the World Aquatics Championships featured solo, duet and team competitions. In 2003, a free routine combination, comprising elements of solo, duet and team, was added. In 2005, it was renamed free combination. In 2007, solo, duet and team events were split between technical and free routines. Since 2007, seven World championship titles are at stake. In 2015, the mixed duet (technical and free) were added to the competition program.

Sculls (hand movements used to propel the body) are some of the most essential part to synchronised swimming. Commonly used sculls include support scull, stationary scull, propeller scull, alligator scull, torpedo scull, split scull, barrel scull, spinning scull and paddle scull. The support scull is used most often to support the body while a swimmer is performing upside down.

The support scull or "American Scull" was invented by Marion Kane Elston and propelled the sport to new heights. The sport was transformed from water ballet to the athleticism of modern-day synchronized swimming. See the International Swimming Hall of Fame as a reference.

Support scull is performed by holding the upper arms against the sides of the body and the fore arms at 90-degree angles to the body, with hands facing the bottom of the pool. The fore arms are then moved back and forth while maintaining the right angle. The resulting pressure against the hands allows the swimmer to hold their legs above water while upside down.

The "eggbeater kick" is another important skill of synchronised swimming. It is a form of treading water that allows for stability and height above the water while leaving the hands free to perform arm motions. An average eggbeater height is usually around collarbone level. Eggbeater is used in all "arm" sections, a piece of choreography in which the swimmer is upright, often with one or both arms in the air. Another variation is a body boost, which is executed through an eggbeater buildup and a strong whip kick, propelling the swimmer out of the water vertically. A body boost can raise a swimmer out of the water to hip level.

A lift or highlight is when members of the team propel another teammate relatively high out of the water. They are quite common in routines of older age groups and higher skill levels.
There are many variations on lifts and these can include partner lifts, float patterns or other areas of unique, artistic choreography intended to exceptionally impress the judges and audience.

There are three parts to every lift in artistic swimming: The top (or "flyer"), the base, and the pushers. *Sometimes there is no base and the pushers push the flyer directly*


There are hundreds of different regular positions that can be used to create seemingly infinite combinations. These are a few basic and commonly used ones:


Further descriptions of technical positions can be found on the International Olympic Committee website.

Routines are composed of "figures" (leg movements), arm sections and highlights. Swimmers are synchronised both to each other and to the music. During a routine swimmers can never use the bottom of the pool for support, but rather depend on sculling motions with the arms, and eggbeater kick to keep afloat. After the performance, the swimmers are judged and scored on their performance based on execution, artistic impression, and difficulty. Execution of technical skill, difficulty, patterns, choreography, and synchronization are all critical to achieving a high score.

Depending on the competition level, swimmers will perform a "technical" routine with predetermined elements that must be performed in a specific order. The technical routine acts as a replacement for the figure event. In addition to the technical routine, the swimmers will perform a longer "free" routine, which has no requirements and is a chance for the swimmers to get creative and innovative with their choreography.

The type of routine and competition level determines the length of routines. Routines typically last two to four minutes, the shortest being the technical solo, with length added as the number of swimmers is increased (duets, teams, combos and highlight). Age and skill level are other important factors in determining the required routine length.

Routines are scored on a scale of 100, with points for execution, artistic impression, and difficulty.
In group routines a group consists of 8 competitors for World Championships and FINA events, each missing participant brings penalty points to the team. A group can consist of a minimum of 4 competitors and a maximum of 10 (for Free Combination and Highlight). If a swimmer uses the bottom, they will be disqualified.

When performing routines in competition and practice, competitors wear a rubber noseclip to keep water from entering their nose when submerged. Some swimmers wear ear-plugs to keep the water out of their ears. Hair is worn in a bun and flavorless gelatin, Knox, is applied to keep hair in place; a decorative headpiece is bobby-pinned to the bun. Occasionally, swimmers wear custom-made swimming caps in place of their hair in buns.

Competitors wear custom swimsuits, usually elaborately decorated with bright fabric and sequins to reflect the music to which they are swimming. The costume and music are not judged but create and aesthetic appeal to the audience.

Makeup is also worn in this sport, but FINA has required a more natural look. No "theatrical make-up" is allowed, only makeup that provides a natural, clean and healthy glow is acceptable. In Canada, eye makeup must be smaller than a circle made by the swimmers thumb and forefinger, and be used solely for "natural enhancement".

Underwater speakers ensure that swimmers can hear the music and aid their ability to synchronize with each other. Routines are prepared and set to counts in the music, to further ensure synchronization. Coaches use underwater speakers to communicate with the swimmers during practice. Goggles, though worn during practice, are not permitted during routine competition.

A standard meet begins with the swimmers doing "figures", which are progressions between positions performed individually without music. All swimmers must compete wearing the standard black swimsuit and white swimcap, as well as goggles and a noseclip. Figures are performed in front of a panel of 5 judges who score individual swimmers from 1 to 10 (10 being the best). The figure competition prefaces the routine events.

In the United States, competitors are divided into groups by age. The eight age groups are: 12 and under, 13–15, 16–17, 18–19, Junior (elite 15–18), Senior (elite 15+), Collegiate, and Master. In addition to these groups, younger swimmers may be divided by ability into 3 levels: Novice, Intermediate, and Junior Olympic. Certain competitions require the athlete(s) to pass a certain Grade Level. Grades as of now range from Level one to Level five, and will soon go to Level ten. Seasons range in length, and some swimmers participate year-round in competitions. There are many levels of competition, including but not limited to: State, Regional, Zone, Junior Olympic, and US Junior and Senior Opens. Each swimmer may compete in up to four of the following routine events: solo, duet, combo (consisting of four to ten swimmers), and team (consisting of four to eight swimmers). In the 12 & under and 13-15 age groups, figure scores are combined with routines to determine the final rankings. The 16-17 and 18-19 age groups combine the scores of the technical and free routines to determine the final rankings. USA Synchro's annual intercollegiate championships have been dominated by The Ohio State University, Stanford University, Lindenwood University, and The University of the Incarnate Word.

In Canada, synchronized swimming has an age-based Structure system as of 2010 with age groups 10 & under, 12 & under, and 13–15 for the provincial levels. There is also a skill level which is 13–15 and juniors (16–18) known as national stream, as well as competition at the Masters and University levels. 13–15 age group and 16–18 age group are national stream athletes that fall in line with international age groups – 15 and Under and Junior (16–18) and Senior (18+) level athletes.
There are also the Wildrose age group. This is for competitors before they reach 13–15 national stream. Wildrose ranges from Tier 8 and under to 16 and over provincial/wildrose. These are also competitive levels. There are also the recreational levels which are called "stars". Synchro Canada requires that a competitor must pass Star 3 before entering Tier 1. To get into a Tier a swimmer must take a test for that Tier. In these tests, the swimmer must be able to perform the required movements for the level. (Canada no longer uses Tiers as a form of level placement).
The Canadian University Artistic Swimming League (CUASL) is intended for Canadian Swimmers who wish to continue their participation in the sport during their university studies, as well as offering a "Novice" category for those new to the sport. Traditionally, the top teams hail from McGill University, Queens University and the University of Ottawa.

In their 2012 book "Concussions and Our Kids", Dr. Robert Cantu and Mark Hyman quoted Dr. Bill Moreau, the medical director for the U.S. Olympic Committee (USOC), as saying, "These women are superior athletes. They're in the pool eight hours a day. Literally, they're within inches of one another, sculling and paddling. As they go through their various routines, they're literally kicking each other in the head." Dr. Moreau said that during a two-week training session in Colorado Springs, the female athletes suffered a 50% concussion rate. As a result, the USOC began reassessing concussion awareness and prevention for all sports.

Others believe the incidence of concussions among synchronized swimmers is much higher, especially among the sport's elite athletes. "I would say 100 percent of my athletes will get a concussion at some point," said Myriam Glez, chief executive of U.S.A. Synchro, the sport's national organizing body. "It might be minor, might be more serious, but at some point or another, they will get hit."

Synchronised swimmers often suffer from tendon injuries, as the sport tends to cause muscle imbalances. Common joint injuries include the rotator cuff and the knees.




</doc>
<doc id="27999" url="https://en.wikipedia.org/wiki?curid=27999" title="Swimming">
Swimming

Swimming is the self-propulsion of a person through water, usually for recreation, sport, exercise, or survival. Locomotion is achieved through coordinated movement of the limbs, the body, or both. Humans can hold their breath underwater and undertake rudimentary locomotive swimming within weeks of birth, as a survival response.

Swimming is consistently among the top public recreational activities, and in some countries, swimming lessons are a compulsory part of the educational curriculum. As a formalized sport, swimming features in a range of local, national, and international competitions, including every modern Summer Olympics.

Swimming relies on the nearly neutral buoyancy of the human body. On average, the body has a relative density of 0.98 compared to water, which causes the body to float. However, buoyancy varies on the basis of body composition, lung inflation, and the salinity of the water. Higher levels of body fat and saltier water both lower the relative density of the body and increase its buoyancy. See also: "Hydrostatic weighing"

Since the human body is (slightly) less dense than water, water supports the weight of the body during swimming. As a result, swimming is “low-impact” compared to land activities such as running. The density and viscosity of water also create resistance for objects moving through the water. Swimming strokes use this resistance to create propulsion, but this same resistance also generates drag on the body.

Hydrodynamics is important to stroke technique for swimming faster, and swimmers who want to swim faster or exhaust less try to reduce the drag of the body's motion through the water. To be more hydrodynamic, swimmers can either increase the power of their strokes or reduce water resistance, though power must increase by a factor of three to achieve the same effect as reducing resistance. Efficient swimming by reducing water resistance involves a horizontal water position, rolling the body to reduce the breadth of the body in the water, and extending the arms as far as possible to reduce wave resistance.

Just before plunging into the pool, swimmers may perform exercises such as squatting. Squatting helps in enhancing a swimmer's start by warming up the thigh muscles.

Human babies demonstrate an innate swimming or diving reflex from newborn until the age of approximately 6 months. Other mammals also demonstrate this phenomenon (see mammalian diving reflex). The diving response involves apnea, reflex bradycardia, and peripheral vasoconstriction; in other words, babies immersed in water spontaneously hold their breath, slow their heart rate, and reduce blood circulation to the extremities (fingers and toes).
Because infants are innately able to swim, classes for babies of about 6 months old are offered in many locations. This helps build muscle memory and makes strong swimmers from a young age.

Swimming can be undertaken using a wide range of styles, known as 'strokes,' and these strokes are used for different purposes, or to distinguish between classes in competitive swimming. It is not necessary to use a defined stroke for propulsion through the water, and untrained swimmers may use a 'doggy paddle' of arm and leg movements, similar to the way four-legged animals swim.

There are four main strokes used in competition and recreation swimming: the front crawl, also known as freestyle, breaststroke, backstroke and butterfly. Competitive swimming in Europe started around 1800, mostly using the breaststroke. In 1873, John Arthur Trudgen introduced the trudgen to Western swimming competitions. Butterfly was developed in the 1930s, and was considered a variant of the breaststroke until accepted as a separate style in 1953. Butterfly is considered the hardest stroke by many people, but it is the most effective for all-around toning and the building of muscles. It also burns the most calories and can be the second fastest stroke if practiced regularly.

Other strokes exist for specific purposes, such as training or rescue, and it is also possible to adapt strokes to avoid using parts of the body, either to isolate certain body parts, such as swimming with arms only or legs only to train them harder, or for use by amputees or those affected by paralysis.

Swimming has been recorded since prehistoric times, and the earliest records of swimming date back to Stone Age paintings from around 7,000 years ago. Written references date from 2000 BC. Some of the earliest references include the Epic of Gilgamesh, the Iliad, the Odyssey, the Bible (Ezekiel 47:5, Acts 27:42, Isaiah 25:11), Beowulf, and other sagas.

The coastal tribes living in the volatile Low Countries were known as excellent swimmers by the Romans. Men and horses of the Batavi tribe could cross the Rhine without losing formation, according to Tacitus. Dio Cassius describes one surprise tactic employed by Aulus Plautius against the Celts at the Battle of the Medway:

The [British Celts] thought that Romans would not be able to cross it without a bridge, and consequently bivouacked in rather careless fashion on the opposite bank; but he sent across a detachment of [Batavii], who were accustomed to swim easily in full armour across the most turbulent streams. . . . Thence the Britons retired to the river Thames at a point near where it empties into the ocean and at flood-tide forms a lake. This they easily crossed because they knew where the firm ground and the easy passages in this region were to be found, but the Romans in attempting to follow them were not so successful. However, the [Batavii] swam across again and some others got over by a bridge a little way up-stream, after which they assailed the barbarians from several sides at once and cut down many of them."

In 1538, Nikolaus Wynmann, a Swiss–German professor of languages, wrote the earliest known complete book about swimming, "Colymbetes, sive de arte natandi dialogus et festivus et iucundus lectu" ("The Swimmer, or A Dialogue on the Art of Swimming and Joyful and Pleasant to Read").

There are many reasons why people swim, from swimming as a recreational pursuit to swimming as a necessary part of a job or other activity. Swimming may also be used to rehabilitate injuries, especially various cardiovascular and muscle injuries. People may also pursue swimming as a career or field of interest. Some may be gifted and choose to compete professionally and go onto claim fame.

Many swimmers swim for recreation, with swimming consistently ranking as one of the physical activities people are most likely to take part in. Recreational swimming can also be used for exercise, relaxation or rehabilitation. The support of the water, and the reduction in impact, make swimming accessible for people who are unable to undertake activities such as running. Swimming is one of the most relaxing activities, water is known to calm us and can help reduce stress.

Swimming is primarily a cardiovascular/aerobic exercise due to the long exercise time, requiring a constant oxygen supply to the muscles, except for short sprints where the muscles work anaerobically. Furthermore, swimming can help tone and strengthen muscles. Swimming allows sufferers of arthritis to exercise affected joints without worsening their symptoms. However, swimmers with arthritis may wish to avoid swimming breaststroke, as improper technique can exacerbate arthritic knee pain. As with most aerobic exercise, swimming reduces the harmful effects of stress. Swimming is also effective in improving health for people with cardiovascular problems and chronic illnesses. It is proven to positively impact the mental health of pregnant women and mothers. Swimming can even improve mood.

As of 2020, the Americans with Disabilities Act requires that swimming pools in the United States be accessible to disabled swimmers.

"Water-based exercise can benefit older adults by improving quality of life and decreasing disability. It also improves or maintains the bone health of post-menopausal women."
Swimming is an ideal workout for the elderly, mainly because it presents little risk of injury and is also low impact. Exercise in the water works out all muscle groups, helping with conditions such as muscular dystrophy which is common in seniors.

Swimming as a sport predominantly involves participants competing to be the fastest over a given distance in a certain period of time. Competitors swim different distances in different levels of competition. For example, swimming has been an Olympic sport since 1896, and the current program includes events from 50 m to 1500 m in length, across all four main strokes and medley. During the season competitive swimmers typically train several times a week, this is in order to preserve fitness as well as promoting overload in training. Furthermore when the cycle of work is completed swimmers go through a stage called taper where intensity is reduce in preparation for racing, during taper power and feel in the water are concentrated.

The sport is governed internationally by the Fédération Internationale de Natation (FINA), and competition pools for FINA events are 25 or 50 meters in length. In the United States, a pool 25 yards in length is commonly used for competition.

Other swimming and water-related sporting disciplines include open water swimming, diving, synchronized swimming, water polo, triathlon, and the modern pentathlon.

As a popular leisure activity done all over the world, one of the primary risks of swimming is drowning. Drowning may occur from a variety of factors, from swimming fatigue to simply inexperience in the water. From 2005 to 2014, an average of 3,536 fatal unintentional drownings occurred in the United States, approximating 10 deaths a day.

To minimize the risk and prevent potential drownings from occurring, lifeguards are often employed to supervise swimming locations such as pools and beaches. Different lifeguards receive different training depending on the sites that they are employed at; i.e. a waterfront lifeguard receives more rigorous training than a poolside lifeguard. Well-known aquatic training services include the National Lifesaving Society and the Canadian Red Cross, which specialize in training lifeguards in North America.

Some occupations require workers to swim, such as abalone and pearl diving, and spearfishing.

Swimming is used to rescue people in the water who are in distress, including exhausted swimmers, non-swimmers who have accidentally entered the water, and others who have come to harm on the water. Lifeguards or volunteer lifesavers are deployed at many pools and beaches worldwide to fulfill this purpose, and they, as well as rescue swimmers, may use specific swimming styles for rescue purposes.

Swimming is also used in marine biology to observe plants and animals in their natural habitat. Other sciences use swimming; for example, Konrad Lorenz swam with geese as part of his studies of animal behavior.

Swimming also has military purposes. Military swimming is usually done by special operation forces, such as Navy SEALs and US Army Special Forces. Swimming is used to approach a location, gather intelligence, engage in sabotage or combat, and subsequently depart. This may also include airborne insertion into water or exiting a submarine while it is submerged. Due to regular exposure to large bodies of water, all recruits in the United States Navy, Marine Corps, and Coast Guard are required to complete basic swimming or water survival training.

Swimming is also a professional sport. Companies sponsor swimmers who have the skills to compete at the international level. Many swimmers compete competitively to represent their home countries in the Olympics. Professional swimmers may also earn a living as entertainers, performing in water ballets.

Locomotion by swimming over brief distances is frequent when alternatives are precluded. There have been cases of political refugees swimming in the Baltic Sea and of people jumping in the water and swimming ashore from vessels not intended to reach land where they planned to go.

There are many risks associated with voluntary or involuntary human presence in water, which may result in death directly or through drowning asphyxiation. Swimming is both the goal of much voluntary presence and the prime means of regaining land in accidental situations.

Most recorded water deaths fall into these categories:

Adverse effects of swimming can include:

Around any pool area, safety equipment is often important, and is a zoning requirement for most residential pools in the United States. Supervision by personnel trained in rescue techniques is required at most competitive swimming meets and public pools.

Traditionally, children were considered not able to swim independently until 4 years of age,
although now infant swimming lessons are recommended to prevent drowning.

In Sweden, Denmark, Norway, Estonia and Finland, the curriculum for the fifth grade (fourth grade in Estonia) states that all children should learn how to swim as well as how to handle emergencies near water. Most commonly, children are expected to be able to swim —of which at least on their back – after first falling into deep water and getting their head under water. Even though about 95 percent of Swedish school children know how to swim, drowning remains the third most common cause of death among children.

In both the Netherlands and Belgium swimming lessons under school time ("schoolzwemmen", school swimming) are supported by the government. Most schools provide swimming lessons. There is a long tradition of swimming lessons in the Netherlands and Belgium, the Dutch translation for the breaststroke swimming style is even "schoolslag" (schoolstroke). In France, swimming is a compulsory part of the curriculum for primary schools. Children usually spend one semester per year learning swimming during CP/CE1/CE2/CM1 (1st, 2nd, 3rd and 4th grade).

In many places, swimming lessons are provided by local swimming pools, both those run by the local authority and by private leisure companies. Many schools also include swimming lessons into their Physical Education curricula, provided either in the schools' own pool or in the nearest public pool.

In the UK, the "Top-ups scheme" calls for school children who cannot swim by the age of 11 to receive intensive daily lessons. Children who have not reached Great Britain's National Curriculum standard of swimming 25 meters by the time they leave primary school receive a half-hour lesson every day for two weeks during term-time.

In Canada and Mexico there has been a call to include swimming in public school curriculum.

In the United States there is the Infant Swimming Resource (ISR) initiative that provides lessons for infant children, to cope with an emergency where they have fallen into the water. They are taught how to roll-back-to-float (hold their breath underwater, to roll onto their back, to float unassisted, rest and breathe until help arrives).

In Switzerland, swimming lessons for babies are popular, to help them getting used to be in another element. At the competition level, unlike in other countries - such as the Commonwealth countries, swimming teams are not related to educational institutions (high-schools and universities), but rather to cities or regions.

Standard everyday clothing is usually impractical for swimming and is unsafe under some circumstances. Most cultures today expect swimmers to wear swimsuits.

Men's swimsuits commonly resemble shorts, or briefs. Casual men's swimsuits (for example, boardshorts) are rarely skintight, unlike competitive swimwear, like jammers or diveskins. In most cases, boys and men swim with their upper body exposed, except in countries where custom or law prohibits it in a public setting, or for practical reasons such as sun protection.

Modern women's swimsuits are generally skintight, covering the pubic region and the breasts (See bikini). Women's swimwear may also cover the midriff as well. Women's swimwear is often a fashion statement, and whether it is modest or not is a subject of debate by many groups, religious and secular.

Competitive swimwear is built so that the wearer can swim faster and more efficiently. Modern competitive swimwear is skintight and lightweight. There are many kinds of competitive swimwear for each gender. It is used in aquatic competitions, such as water polo, swim racing, diving, and rowing.

Wetsuits provide both thermal insulation and flotation. Many swimmers lack buoyancy in the leg. The wetsuit reduces density and therefore improves buoyancy while swimming. It provides insulation by absorbing some of the surrounding water, which then heats up when in direct contact with skin. The wetsuit is the usual choice for those who swim in cold water for long periods of time, as it reduces susceptibility to hypothermia.

Some people also choose to wear no clothing while swimming; this is known as skinny dipping. In some European countries public pools have naturist sessions to allow clothes-free swimming and many countries have naturist beaches where one can swim naked. It is legal to swim naked in the sea at all UK beaches. It was common for males to swim naked in a public setting up to the early 20th century. Today, skinny dipping can be a rebellious activity or merely a casual one.





</doc>
<doc id="28002" url="https://en.wikipedia.org/wiki?curid=28002" title="Simple machine">
Simple machine

A simple machine is a mechanical device that changes the direction or magnitude of a force. In general, they can be defined as the simplest mechanisms that use mechanical advantage (also called leverage) to multiply force. Usually the term refers to the six classical simple machines that were defined by Renaissance scientists:

A simple machine uses a single applied force to do work against a single load force. Ignoring friction losses, the work done on the load is equal to the work done by the applied force. The machine can increase the amount of the output force, at the cost of a proportional decrease in the distance moved by the load. The ratio of the output to the applied force is called the "mechanical advantage".

Simple machines can be regarded as the elementary "building blocks" of which all more complicated machines (sometimes called "compound machines") are composed. For example, wheels, levers, and pulleys are all used in the mechanism of a bicycle. The mechanical advantage of a compound machine is just the product of the mechanical advantages of the simple machines of which it is composed.

Although they continue to be of great importance in mechanics and applied science, modern mechanics has moved beyond the view of the simple machines as the ultimate building blocks of which all machines are composed, which arose in the Renaissance as a neoclassical amplification of ancient Greek texts. The great variety and sophistication of modern machine linkages, which arose during the Industrial Revolution, is inadequately described by these six simple categories. Various post-Renaissance authors have compiled expanded lists of "simple machines", often using terms like "basic machines", "compound machines", or "machine elements" to distinguish them from the classical simple machines above. By the late 1800s, Franz Reuleaux had identified hundreds of machine elements, calling them "simple machines". Modern machine theory analyzes machines as kinematic chains composed of elementary linkages called kinematic pairs.

The idea of a simple machine originated with the Greek philosopher Archimedes around the 3rd century BC, who studied the Archimedean simple machines: lever, pulley, and screw. He discovered the principle of mechanical advantage in the lever. Archimedes' famous remark with regard to the lever: "Give me a place to stand on, and I will move the Earth," () expresses his realization that there was no limit to the amount of force amplification that could be achieved by using mechanical advantage. Later Greek philosophers defined the classic five simple machines (excluding the inclined plane) and were able to calculate their (ideal) mechanical advantage. For example, Heron of Alexandria (c. 10–75 AD) in his work "Mechanics" lists five mechanisms that can "set a load in motion"; lever, windlass, pulley, wedge, and screw, and describes their fabrication and uses. However the Greeks' understanding was limited to the statics of simple machines (the balance of forces), and did not include dynamics, the tradeoff between force and distance, or the concept of work.

During the Renaissance the dynamics of the "Mechanical Powers", as the simple machines were called, began to be studied from the standpoint of how far they could lift a load, in addition to the force they could apply, leading eventually to the new concept of mechanical work. In 1586 Flemish engineer Simon Stevin derived the mechanical advantage of the inclined plane, and it was included with the other simple machines. The complete dynamic theory of simple machines was worked out by Italian scientist Galileo Galilei in 1600 in "Le Meccaniche" ("On Mechanics"), in which he showed the underlying mathematical similarity of the machines as force amplifiers. He was the first to explain that simple machines do not create energy, only transform it.

The classic rules of sliding friction in machines were discovered by Leonardo da Vinci (1452–1519), but were unpublished and merely documented in his notebooks, and were based on pre-Newtonian science such as believing friction was an ethereal fluid. They were rediscovered by Guillaume Amontons (1699) and were further developed by Charles-Augustin de Coulomb (1785).
If a simple machine does not dissipate energy through friction, wear or deformation, then energy is conserved and it is called an ideal simple machine. In this case, the power into the machine equals the power out, and the mechanical advantage can be calculated from its geometric dimensions.

Although each machine works differently mechanically, the way they function is similar mathematically. In each machine, a force formula_1 is applied to the device at one point, and it does work moving a load, formula_2 at another point. Although some machines only change the direction of the force, such as a stationary pulley, most machines multiply the magnitude of the force by a factor, the mechanical advantage
that can be calculated from the machine's geometry and friction.

Simple machines do not contain a source of energy, so they cannot do more work than they receive from the input force. A simple machine with no friction or elasticity is called an "ideal machine". Due to conservation of energy, in an ideal simple machine, the power output (rate of energy output) at any time formula_4 is equal to the power input formula_5
The power output equals the velocity of the load formula_7 multiplied by the load force formula_8. Similarly the power input from the applied force is equal to the velocity of the input point formula_9 multiplied by the applied force formula_10.
Therefore,
So the mechanical advantage of an ideal machine formula_12 is equal to the "velocity ratio", the ratio of input velocity to output velocity

The "velocity ratio" is also equal to the ratio of the distances covered in any given period of time
Therefore the mechanical advantage of an ideal machine is also equal to the "distance ratio", the ratio of input distance moved to output distance moved
This can be calculated from the geometry of the machine. For example, the mechanical advantage and distance ratio of the lever is equal to the ratio of its lever arms.

The mechanical advantage can be greater or less than one:

In the screw, which uses rotational motion, the input force should be replaced by the torque, and the velocity by the angular velocity the shaft is turned.

All real machines have friction, which causes some of the input power to be dissipated as heat. If formula_19 is the power lost to friction, from conservation of energy
The mechanical efficiency formula_21 of a machine (where formula_22) is defined as the ratio of power out to the power in, and is a measure of the frictional energy losses
As above, the power is equal to the product of force and velocity, so
Therefore,
So in non-ideal machines, the mechanical advantage is always less than the velocity ratio by the product with the efficiency "η". So a machine that includes friction will not be able to move as large a load as a corresponding ideal machine using the same input force.

A "compound machine" is a machine formed from a set of simple machines connected in series with the output force of one providing the input force to the next. For example, a bench vise consists of a lever (the vise's handle) in series with a screw, and a simple gear train consists of a number of gears (wheels and axles) connected in series.

The mechanical advantage of a compound machine is the ratio of the output force exerted by the last machine in the series divided by the input force applied to the first machine, that is

Because the output force of each machine is the input of the next, formula_27, this mechanical advantage is also given by
Thus, the mechanical advantage of the compound machine is equal to the product of the mechanical advantages of the series of simple machines that form it

Similarly, the efficiency of a compound machine is also the product of the efficiencies of the series of simple machines that form it

In many simple machines, if the load force "F" on the machine is high enough in relation to the input force "F", the machine will move backwards, with the load force doing work on the input force. So these machines can be used in either direction, with the driving force applied to either input point. For example, if the load force on a lever is high enough, the lever will move backwards, moving the input arm backwards against the input force. These are called ""reversible"", ""non-locking"" or ""overhauling"" machines, and the backward motion is called ""overhauling"". 

However, in some machines, if the frictional forces are high enough, no amount of load force can move it backwards, even if the input force is zero. This is called a ""self-locking"", ""nonreversible"", or ""non-overhauling"" machine. These machines can only be set in motion by a force at the input, and when the input force is removed will remain motionless, "locked" by friction at whatever position they were left.

Self-locking occurs mainly in those machines with large areas of sliding contact between moving parts: the screw, inclined plane, and wedge:

A machine will be self-locking if and only if its efficiency "η" is below 50%:

Whether a machine is self-locking depends on both the friction forces (coefficient of static friction) between its parts, and the distance ratio "d/d" (ideal mechanical advantage). If both the friction and ideal mechanical advantage are high enough, it will self-lock.

When a machine moves in the forward direction from point 1 to point 2, with the input force doing work on a load force, from conservation of energy the input work formula_32 is equal to the sum of the work done on the load force formula_33 and the work lost to friction formula_34

If the efficiency is below 50%
formula_35
From 

When the machine moves backward from point 2 to point 1 with the load force doing work on the input force, the work lost to friction formula_34 is the same
So the output work is
Thus the machine self-locks, because the work dissipated in friction is greater than the work done by the load force moving it backwards even with no input force

Machines are studied as mechanical systems consisting of actuators and mechanisms that transmit forces and movement, monitored by sensors and controllers. The components of actuators and mechanisms consist of links and joints that form kinematic chains.

Simple machines are elementary examples of kinematic chains that are used to model mechanical systems ranging from the steam engine to robot manipulators. The bearings that form the fulcrum of a lever and that allow the wheel and axle and pulleys to rotate are examples of a kinematic pair called a hinged joint. Similarly, the flat surface of an inclined plane and wedge are examples of the kinematic pair called a sliding joint. The screw is usually identified as its own kinematic pair called a helical joint.

Two levers, or cranks, are combined into a planar four-bar linkage by attaching a link that connects the output of one crank to the input of another. Additional links can be attached to form a six-bar linkage or in series to form a robot.

The identification of simple machines arises from a desire for a systematic method to invent new machines. Therefore, an important concern is how simple machines are combined to make more complex machines. One approach is to attach simple machines in series to obtain compound machines.

However, a more successful strategy was identified by Franz Reuleaux, who collected and studied over 800 elementary machines. He realized that a lever, pulley, and wheel and axle are in essence the same device: a body rotating about a hinge. Similarly, an inclined plane, wedge, and screw are a block sliding on a flat surface.

This realization shows that it is the joints, or the connections that provide movement, that are the primary elements of a machine. Starting with four types of joints, the revolute joint, sliding joint, cam joint and gear joint, and related connections such as cables and belts, it is possible to understand a machine as an assembly of solid parts that connect these joints.

The design of mechanisms to perform required movement and force transmission is known as kinematic synthesis. This is a collection of geometric techniques for the mechanical design of linkages, cam and follower mechanisms and gears and gear trains.



</doc>
<doc id="28005" url="https://en.wikipedia.org/wiki?curid=28005" title="Semi-Automatic Ground Environment">
Semi-Automatic Ground Environment

The Semi-Automatic Ground Environment (SAGE) was a system of large computers and associated networking equipment that coordinated data from many radar sites and processed it to produce a single unified image of the airspace over a wide area. SAGE directed and controlled the NORAD response to a Soviet air attack, operating in this role from the late 1950s into the 1980s. Its enormous computers and huge displays remain a part of cold war lore, and a common prop in movies such as "Dr. Strangelove" and .

The processing power behind SAGE was supplied by the largest discrete component-based computer ever built, the IBM-manufactured AN/FSQ-7. Each SAGE Direction Center (DC) housed an FSQ-7 which occupied an entire floor, approximately not including supporting equipment. Information was fed to the DCs from a network of radar stations as well as readiness information from various defence sites. The computers, based on the raw radar data, developed "tracks" for the reported targets, and automatically calculated which defences were within range. Operators used light guns to select targets on-screen for further information, select one of the available defences, and issue commands to attack. These commands would then be automatically sent to the defence site via teleprinter.

Connecting the various sites was an enormous network of telephones, modems and teleprinters. Later additions to the system allowed SAGE's tracking data to be sent directly to CIM-10 Bomarc missiles and some of the US Air Force's interceptor aircraft in-flight, directly updating their autopilots to maintain an intercept course without operator intervention. Each DC also forwarded data to a Combat Center (CC) for "supervision of the several sectors within the division" ("each combat center [had] the capability to coordinate defense for the whole nation").

SAGE became operational in the late 1950s and early 1960s at a combined cost of billions of dollars. It was noted that the deployment cost more than the Manhattan Project—which it was, in a way, defending against. Throughout its development, there were continual concerns about its real ability to deal with large attacks, and the Operation Skyshield tests showed that only about one-fourth of enemy bombers would have been intercepted. Nevertheless, SAGE was the backbone of NORAD's air defense system into the 1980s, by which time the tube-based FSQ-7's were increasingly costly to maintain and completely outdated. Today the same command and control task is carried out by microcomputers, based on the same basic underlying data.

Just prior to World War II, Royal Air Force tests with the new Chain Home (CH) radars had demonstrated that relaying information to the fighter aircraft directly from the radar sites was not feasible. The radars determined the map coordinates of the enemy, but could generally not see the fighters at the same time. This meant the fighters had to be able to determine where to fly to perform an interception but were often unaware of their own exact location and unable to calculate an interception while also flying their aircraft.

The solution was to send all of the radar information to a central control station where operators collated the reports into single "tracks", and then reported these tracks out to the airbases, or "sectors". The sectors used additional systems to track their own aircraft, plotting both on a single large map. Operators viewing the map could then easily see what direction their fighters would have to fly to approach their targets and relay that simply by telling them to fly along a certain heading or "vector". This Dowding system was the first ground-controlled interception (GCI) system of large scale, covering the entirety of the UK. It proved enormously successful during the Battle of Britain, and is credited as being a key part of the RAF's success.

The system was slow, often providing information that was up to five minutes out of date. Against propeller driven bombers flying at perhaps this was not a serious concern, but it was clear the system would be of little use against jet-powered bombers flying at perhaps . The system was extremely expensive in manpower terms, requiring hundreds of telephone operators, plotters and trackers in addition to the radar operators. This was a serious drain on manpower reserves, making it difficult to expand the network.

The idea of using a computer to handle the task of taking reports and developing tracks had been explored beginning late in the war. By 1944, analog computers had been installed at the CH stations to automatically convert radar readings into map locations, eliminating two people. Meanwhile, the Royal Navy began experimenting with the Comprehensive Display System (CDS), another analog computer that took X and Y locations from a map and automatically generated tracks from repeated inputs. Similar systems began development with the Royal Canadian Navy, DATAR, and the US Navy, the Naval Tactical Data System. A similar system was also specified for the Nike SAM project, specifically referring to a US version of CDS, coordinating the defense over a battle area so that multiple batteries did not fire on a single target. All of these systems were relatively small in geographic scale, generally tracking within a city-sized area.

When the Soviet Union tested its first atomic bomb in August 1949, the topic of air defense of the US became important for the first time. A study group, the "Air Defense Systems Engineering Committee" was set up under the direction of Dr. George Valley to consider the problem, and is known to history as the Valley Committee.

Their December report noted a key problem in air defense using ground-based radars. A bomber approaching a radar station would detect the signals from the radar long before the reflection off the bomber was strong enough to be detected by the station. The committee suggested that when this occurred, the bomber would descend to low altitude, thereby greatly limiting the radar horizon, allowing the bomber to fly past the station undetected. Although flying at low altitude greatly increased fuel consumption, the team calculated that the bomber would only need to do this for about 10% of its flight, making the fuel penalty acceptable.

The only solution to this problem was to build a huge number of stations with overlapping coverage. At that point the problem became one of managing the information. Manual plotting was immediately ruled out as too slow, and a computerized solution was the only possibility. To handle this task, the computer would need to be fed information directly, eliminating any manual translation by phone operators, and it would have to be able to analyze that information and automatically develop tracks. A system tasked with defending cities against the predicted future Soviet bomber fleet would have to be dramatically more powerful that the models used in the NTDS or DATAR.

The Committee then had to consider whether or not such a computer was possible. Valley was introduced to Jerome Wiesner, associate director of the Research Laboratory of Electronics at MIT. Wiesner noted that the Servomechanisms Laboratory had already begun development of a machine that might be fast enough. This was the Whirlwind I, originally developed for the Office of Naval Research as a general purpose flight simulator that could simulate any current or future aircraft simply by changing its software.

Wiesner introduced Valley to Whirlwind's project lead, Jay Forrester, who convinced him that Whirlwind was sufficiently capable. In September 1950, an early microwave early-warning radar system at Hanscom Field was connected to Whirlwind using a custom interface developed by Forrester's team. An aircraft was flown past the site, and the system digitized the radar information and successfully sent it to Whirlwind. With this demonstration, the technical concept was proven. Forrester was invited to join the committee.

With this successful demonstration, Louis Ridenour, chief scientist of the Air Force, wrote a memo stating "It is now apparent that the experimental work necessary to develop, test, and evaluate the systems proposals made by ADSEC will require a substantial amount of laboratory and field effort." Ridenour approached MIT President James Killian with the aim of beginning a development lab similar to the war-era Radiation Laboratory that made enormous progress in radar technology. Killian was initially uninterested, desiring to return the school to its peacetime civilian charter. Ridenour eventually convinced Killian the idea was sound by describing the way the lab would lead to the development of a local electronics industry based on the needs of the lab and the students who would leave the lab to start their own companies. Killian agreed to at least consider the issue, and began Project Charles to consider the size and scope of such a lab.

Project Charles was placed under the direction of Francis Wheeler Loomis and included 28 scientists, about half of whom were already associated with MIT. Their study ran from February to August 1951, and in their final report they stated that "We endorse the concept of a centralized system as proposed by the Air Defense Systems Engineering Committee, and we agree that the central coordinating apparatus of this system should be a high-speed electronic digital computer." The report went on to describe a new lab that would be used for generic technology development for the Air Force, Army and Navy, and would be known as Project Lincoln.

Loomis took over direction of Project Lincoln and began planning by following the lead of the earlier RadLab. By September 1951, only months after the Charles report, Project Lincoln had more than 300 employees. By the end of the summer of 1952 this had risen to 1300, and after another year, 1800. The only building suitable for classified work at that point was Building 22, suitable for a few hundred people at most, although some relief was found by moving the non-classified portions of the project, administration and similar, to Building 20. But this was clearly not enough space, and after considering a variety of suitable locations, a site at Laurence G. Hanscom Field was selected, with the official groundbreaking taking place in 1951.

The terms of the National Security Act were formulated during 1947, leading to the creation of the US Air Force out of the former US Army Air Force. During April of the same year, US Air Force staff were identifying specifically the requirement for the creation of automatic equipment for radar-detection which would relay information to an air defence control system, a system which would function without the inclusion of persons for its operation. The December 1949 "Air Defense Systems Engineering Committee" led by Dr. George Valley had recommended computerized networking for "radar stations guarding the northern air approaches to the United States" (e.g., in Canada). After a January 1950 meeting, Valley and Jay Forrester proposed using the Whirlwind I (completed 1951) for air defense. On August 18, 1950, when the "1954 Interceptor" requirements were issued, the USAF "noted that manual techniques of aircraft warning and control would impose "intolerable" delays" (Air Material Command (AMC) published "Electronic Air Defense Environment for 1954" in December .) During February–August 1951 at the new Lincoln Laboratory, the USAF conducted Project Claude which concluded an improved air defense system was needed.

In a test for the US military at Bedford on 20 April 1951, data produced by a radar was transmitted through telephone lines to a computer for the first time, showing the detection of a mock enemy aircraft. This first test was directed by C. Robert Wieser.

The "Summer Study Group" of scientists in 1952 recommended "computerized air direction centers…to be ready by 1954."

IBM's "Project High" assisted under their October 1952 Whirlwind subcontract with Lincoln Laboratory, and a 1952 USAF Project Lincoln "fullscale study" of "a large scale integrated ground control system" resulted in the SAGE approval "first on a trial basis in 1953". The USAF had decided by April 10, 1953, to cancel the competing ADIS (based on CDS), and the University of Michigan's Aeronautical Research Center withdrew in the spring. Air Research and Development Command (ARDC) planned to "finalize a production contract for the Lincoln Transition System". Similarly, the July 22, 1953, report by the Bull Committee (NSC 159) identified completing the Mid-Canada Line radars as the top priority and "on a second-priority-basis: the Lincoln automated system" (the decision to control Bomarc with the automated system was also in 1953.)

The Priority Permanent System with the initial (priority) radar stations was completed in 1952 as a "manual air defense system" (e.g., NORAD/ADC used a "Plexiglas plotting board" at the Ent command center.) The Permanent System radar stations included 3 subsequent phases of deployments and by June 30, 1957, had 119 "Fixed CONUS" radars, 29 "Gap-filler low altitude" radars, and 23 control centers". At "the end of 1957, ADC operated 182 radar stations [and] 17 control centers … 32 [stations] had been added during the last half of the year as low-altitude, unmanned gap-filler radars. The total consisted of 47 gap-filler stations, 75 Permanent System radars, 39 semimobile radars, 19 Pinetree stations,…1 Lashup -era radar and a single Texas Tower". "On 31 December 1958, USAF ADC had 187 operational land-based radar stations" (74 were "P-sites", 29 "M-sites", 13 "SM-sites", & 68 "ZI Gap Fillers").

Jay Forrester was instrumental in directing the development of the key concept of an interception system during his work at Servomechanisms Laboratory of MIT. The concept of the system, according to the Lincoln Laboratory site was to:

The AN/FSQ-7 was developed by the Lincoln Laboratory's Digital Computer Laboratory and Division 6, working closely with IBM as the manufacturer. Each FSQ-7 actually consisted of two nearly identical computers operating in "duplex" for redundancy. The design used an improved version of the Whirlwind I magnetic core memory and was an extension of the Whirlwind II computer program, renamed AN/FSQ-7 in 1953 to comply with Air Force nomenclature. It has been suggested the FSQ-7 was based on the IBM 701 but, while the 701 was investigated by MIT engineers, its design was ultimately rejected due to high error rates and generally being "inadequate to the task." IBM's contributions were essential to the success of the FSQ-7 but IBM benefited immensely from its association with the SAGE project, most evidently during development of the IBM 704.

On October 28, 1953, the Air Force Council recommended 1955 funding for "ADC to convert to the Lincoln automated system" ("redesignated the SAGE System in 1954"). The ""experimental SAGE subsector, located in Lexington, Mass., was completed in 1955…with a prototype AN/FSQ-7…known as XD-1"" (single computer system in Building F). In 1955, Air Force personnel began IBM training at the Kingston, New York, prototype facility, and the "4620th Air Defense Wing (experimental SAGE) was established at Lincoln Laboratory"

On May 3, 1956, General Partridge presented "CINCNORAD's Operational Concept for Control of Air Defense Weapons" to the Armed Forces Policy Council, and a June 1956 symposium presentation identified advanced programming methods of SAGE code. For SAGE consulting Western Electric and Bell Telephone Laboratories formed the Air Defense Engineering Service (ADES), which was contracted in January 1954. IBM delivered the FSQ-7 computer's prototype in June 1956, and Kingston's XD-2 with dual computers guided a Cape Canaveral BOMARC to a successful aircraft intercept on August 7, 1958. Initially contracted to RCA, the AN/FSQ-7 production units were started by IBM in 1958 (32 DCs were planned for networking NORAD regions.) IBM's production contract developed 56 SAGE computers for $½ billion (~$18 million per computer pair in each FSQ-7)—cf. the $2 billion WWII Manhattan Project.

General Operational Requirements (GOR) 79 and 97 were "the basic USAF documents guiding development and improvement of [the semi-automatic] ground environment. Prior to fielding the AN/FSQ-7 centrals, the USAF initially deployed "pre-SAGE semiautomatic intercept systems" (AN/GPA-37) to Air Defense Direction Centers, ADDCs (e.g., at "NORAD Control Centers"). On April 22, 1958, NORAD approved Nike AADCPs to be collocated with the USAF manual ADDCs at Duncanville Air Force Station TX, Olathe Air Force Station KS, Belleville Air Force Station IL, and Osceola Air Force Station KS.

In 1957, SAGE System groundbreaking at McChord AFB was for DC-12 where the "electronic brain" began arriving in November 1958, and the "first SAGE regional battle post [CC-01] began operating in Syracuse, New York in early 1959". BOMARC "crew training was activated January 1, 1958", and AT&T "hardened many of its switching centers, putting them in deep underground bunkers", The North American Defense Objectives Plan (NADOP 59-63) submitted to Canada in December 1958 scheduled 5 Direction Centers and 1 Combat Center to be complete in Fiscal Year 1959, 12 DCs and 3 CCs complete at the end of FY 60, 19 DC/4 CC FY 61, 25/6 FY 62, and 30/10 FY 63. On June 30 NORAD ordered that "Air Defense Sectors (SAGE) were to be designated as NORAD sectors", (the military reorganization had begun when effective April 1, 1958, CONAD "designated four SAGE sectors – New York, Boston, Syracuse, and Washington – as CONAD Sectors".)
SAGE Geographic Reorganization: The SAGE Geographic Reorganization Plan of July 25, 1958, by NORAD was "to provide a means for the orderly transition and phasing from the manual to the SAGE system." The plan identified deactivation of the Eastern, Central, and Western Region/Defense Forces on July 1, 1960, and "current manual boundaries" were to be moved to the new "eight SAGE divisions" (1 in Canada, "the 35th") as soon as possible. Manual divisions "not to get SAGE computers were to be phased out" along with their Manual Air Defense Control Centers at the headquarters base: "9th [at] Geiger Field… 32d, Syracuse AFS… 35th, Dobbins AFB… 58th, Wright-Patterson AFB… 85th, Andrews AFB". The 26th SAGE Division (New York, Boston, Syracuse & Bangor SAGE sectors)--the 1st of the SAGE divisions—became operational at Hancock Field on 1 January 1959 after the redesignation started for AC&W Squadrons (e.g., the Highlands P-9 unit became the 646th Radar Squadron (SAGE) October 1.) Additional sectors included the Los Angeles Air Defense Sector (SAGE) designated in February 1959. A June 23 JCS memorandum approved the new "March 1959 Reorganization Plan" for HQ NORAD/CONAD/ADC.

Project Wild Goose teams of Air Material Command personnel installed the Ground Air Transmit Receive stations for the SAGE TDDL (in April 1961, Sault Ste Marie was the first operational sector with TDDL.) ... By the middle of 1960, AMC had determined that about 800,000 man-hours (involving 130 changes) would be required to bring the F-106 fleet to the point where it would be a valuable adjunct to the air defense system. Part of the work (Project Broad Jump) was accomplished by Sacramento Air Materiel Area. The remainder (Project Wild Goose) was done at ADC bases by roving AMC field assistance teams supported by ADC maintenance personnel. (cited by Volume I p. 271 & Schaffel p. 325) After a September 1959 experimental ATABE test between an "abbreviated" AN/FSQ-7 staged at Fort Banks and the Lexington XD-1, the 1961 "SAGE/Missile Master test program" conducted large-scale field testing of the ATABE "mathematical model" using radar tracks of actual SAC and ADC aircraft flying mock penetrations into defense sectors. Similarly conducted was the joint SAC-NORAD Sky Shield II exercise followed by Sky Shield III on 2 September 1962 On July 15, 1963, ESD's CMC Management Office assumed "responsibilities in connection with BMEWS, Space Track, SAGE, and BUIC." The Chidlaw Building's computerized NORAD/ADC Combined Operations Center in 1963 became the highest echelon of the SAGE computer network when operations moved from Ent AFB's 1954 manual Command Center to the partially underground "war room". Also in 1963, radar stations were renumbered (e.g., Cambria AFS was redesignated from P-2 to Z-2 on July 31) and the vacuum-tube SAGE System was completed (and obsolete).

On "June 26, 1958,…the New York sector became operational" and on December 1, 1958, the Syracuse sector's DC-03 was operational ("the SAGE system [did not] become operational until January 1959.") Construction of CFB North Bay in Canada was started in 1959 for a bunker ~ underground (operational October 1, 1963), and by 1963 the system had 3 Combat Centers. The 23 SAGE centers included 1 in Canada, and the "SAGE control centers reached their full 22 site deployments in 1961 (out of 46 originally planned)." The completed Minot AFB blockhouse received an AN/FSQ-7, but never received the FSQ-8 (the April 1, 1959, Minot Air Defense Sector consolidated with the Grand Forks ADS on March 1, 1963).

The SAGE system included a direction center (DC) assigned to air defense sectors as they were defined at the time.
<nowiki>*</nowiki>

The environment allowed radar station personnel to monitor the radar data and systems' status (e.g., Arctic Tower radome pressure) and to use the range height equipment to process height requests from Direction Center (DC) personnel. DCs received the Long Range Radar Input from the sector's radar stations, and DC personnel monitored the radar tracks and IFF data provided by the stations, requested height-finder radar data on targets, and monitored the computer's evaluation of which fighter aircraft or Bomarc missile site could reach the threat first. The DC's "NORAD sector commander's operational staff" could designate fighter intercept of a target or, using the Senior Director's keyed console in the Weapons Direction room, launch a Bomarc intercept with automatic Q-7 guidance of the surface-to-air missile to a final homing dive (equipped fighters eventually were automatically guided to intercepts).

The "NORAD sector direction center (NSDC) [also had] air defense artillery director (ADAD) consoles [and an Army] ADA battle staff officer", and the NSDC automatically communicated crosstelling of "SAGE reference track data" to/from adjacent sectors' DCs and to 10 Nike Missile Master AADCPs. Forwardtelling automatically communicated data from multiple DCs to a 3-story Combat Center (CC) usually at one of the sector's DCs (cf. planned Hamilton AFB CC-05 near the Beale AFB DC-18) for coordinating the air battle in the NORAD region (multiple sectors) and which forwarded data to the NORAD Command Center (Ent AFB, 1963 Chidlaw Building, & 1966 Cheyenne Mountain). NORAD's integration of air warning data (at the ADOC) along with space surveillance, intelligence, and other data allowed attack assessment of an Air Defense Emergency for alerting the SAC command centers (465L SACCS nodes at Offutt AFB & The Notch), The Pentagon/Raven Rock NMCC/ANMCC, and the public via CONELRAD radio stations.

The Burroughs 416L SAGE component (ESD Project 416L, Semi Automatic Ground Environment System) was the Cold War network connecting IBM supplied computer system at the various DC and that created the display and control environment for operation of the separate radars and to provide outbound command guidance for ground-controlled interception by air defense aircraft in the "SAGE Defense System" ("Air Defense Weapons System"). Burroughs Corporation was a prime contractor for SAGE network interface equipment which included 134 Burroughs AN/FST-2 Coordinate Data Transmitting Sets (CDTS) at radar stations and other sites, the IBM supplied AN/FSQ-7 at 23 Direction Centers, and the AN/FSQ-8 Combat Control Computers at 8 Combat Centers. The 2 computers of each AN/FSQ-7 together weighing used about ⅓ of the DC's 2nd floor space and at ~$50 per instruction had approximately 125,000 "computer instructions support[ing] actual operational air-defense mission" processing. The AN/FSQ-7 at Luke AFB had additional memory (32K total) and was used as a "computer center for all other" DCs. Project 416L was the USAF predecessor of NORAD, SAC, and other military organizations' "Big L" computer systems (e.g., 438L Air Force Intelligence Data Handling System & 496L Space Detection and Tracking System).

Network communications: The SAGE network of computers connected by a "Digital Radar Relay" (SAGE data system) used AT&T voice lines, microwave towers, switching centers (e.g., SAGE NNX 764 was at Delta, Utah & 759 at Mounds, Oklahoma), etc.; and AT&T's "main underground station" was in Kansas (Fairview) with other bunkers in Connecticut (Cheshire), California (Santa Rosa), Iowa (Boone) and Maryland (Hearthstone Mountain). CDTS modems at automated radar stations transmitted range and azimuth, and the Air Movements Identification Service (AMIS) provided air traffic data to the SAGE System. Radar tracks by telephone calls (e.g., from Manual Control Centers in the Albuquerque, Minot, and Oklahoma City sectors) could be entered via consoles of the 4th floor "Manual Inputs" room adjacent to the "Communication Recording-Monitoring and VHF" room. In 1966, SAGE communications were integrated into the AUTOVON Network.

SAGE Sector Warning Networks (cf. NORAD Division Warning Networks) provided the radar netting communications for each DC and eventually also allowed transfer of command guidance to autopilots of TDDL-equipped interceptors for vectoring to targets via the Ground to Air Data Link Subsystem and the Ground Air Transmit Receive (GATR) network of radio sites for "HF/VHF/UHF voice & TDDL" each generally co-located at a CDTS site. SAGE Direction Centers and Combat Centers were also nodes of NORAD's Alert Network Number 1, and SAC Emergency War Order Traffic included "Positive Control/Noah's Ark instructions" through northern NORAD radio sites to confirm or recall SAC bombers if "SAC decided to launch the alert force before receiving an execution order from the JCS".

A SAGE System ergonomic test at Luke AFB in 1964 ""showed conclusively that the wrong timing of human and technical operations was leading to frequent truncation of the flight path tracking system"" (Harold Sackman). SAGE software development was "grossly underestimated" (60,000 lines in September 1955): "the biggest mistake [of] the SAGE computer program was [underestimating the] jump from the 35,000 [WWI] instructions … to the more than 100,000 instructions on the" AN/FSQ-8. NORAD conducted a "Sage/Missile Master Integration/ECM-ECCM Test" in 1963, and although SAGE used AMIS input of air traffic information, the 1959 plan developed by the July 1958 USAF Air Defense Systems Integration Division for SAGE Air Traffic Integration (SATIN) was cancelled by the DoD.

SAGE radar stations, including 78 DEW Line sites in December 1961, provided radar tracks to DCs and had frequency diversity (FD) radars United States Navy picket ships also provided radar tracks, and seaward radar coverage was provided. By the late 1960s EC-121 Warning Star aircraft based at Otis AFB MA and McClellan AFB CA provided radar tracks via automatic data link to the SAGE System. Civil Aeronautics Administration radars were at some stations (e.g., stations of the Joint Use Site System), and the ARSR-1 Air Route Surveillance Radar rotation rate had to be modified "for SAGE [IFF/SIF] Modes III and IV" ("antenna gear box modification" for compatibility with FSQ-7 & FSG-1 centrals.)

ADC aircraft such as the F-94 Starfire, F-89 Scorpion, F-101B Voodoo, and F-4 Phantom were controlled by SAGE GCI. The F-104 Starfighter was "too small to be equipped with [SAGE] data link equipment" and used voice-commanded GCI, but the F-106 Delta Dart was equipped for the automated data link (ADL). The ADL was designed to allow Interceptors that reached targets to transmit real-time tactical friendly and enemy movements and to determine whether sector defence reinforcement was necessary.

Familiarization flights allowed SAGE weapons directors to fly on two-seat interceptors to observe GCI operations. Surface-to-air missile installations for CIM-10 Bomarc interceptors were displayed on SAGE consoles.

Partially solid-state AN/FST-2B and later AN/FYQ-47 computers replaced the AN/FST-2, and sectors without AN/FSQ-7 centrals requiring a "weapon direction control device" for USAF air defense used the solid-state AN/GSG-5 CCCS instead of the AN/GPA-73 recommended by ADC in June 1958. Back-Up Interceptor Control (BUIC) with CCCS dispersed to radar stations for survivability allowed a diminished but functional SAGE capability. In 1962, Burroughs "won the contract to provide a military version of its D825" modular data processing system for BUIC II. BUIC II was 1st used at North Truro Z-10 in 1966, and the Hamilton AFB BUIC II was installed in the former MCC building when it was converted to a SAGE Combat Center in 1966 (CC-05). On June 3, 1963, the Direction Centers at Marysville CA, Marquette/K I Sawyer AFB (DC-14) MI, Stewart AFB NY (DC-02), and Moses Lake WA (DC-15) were planned for closing and at the end of 1969, only 6 CONUS SAGE DCs remained (DC-03, -04, -10, -12, -20, & -21) all with the vacuum tube AN/FSQ-7 centrals. In 1966, NORAD Combined Operations Center operations at Chidlaw transferred to the Cheyenne Mountain Operations Center (425L System) and in December 1963, the DoD approved solid state replacement of Martin AN/FSG-1 centrals with the AN/GSG-5 and subsequent Hughes AN/TSQ-51. The "416L/M/N Program Office" at Hanscom Field had deployed the BUIC III by 1971 (e.g., to Fallon NAS), and the initial BUIC systems were phased out 1974–5. ADC had been renamed Aerospace Defense Command on January 15, 1968, and its general surveillance radar stations transferred to ADTAC in 1979 when the ADC major command was broken up (space surveillance stations went to SAC and the Aerospace Defense Center was activated as a DRU.)

For airborne command posts, "as early as 1962 the Air Force began exploring possibilities for an Airborne Warning and Control System (AWACS)", and the Strategic Defense Architecture (SDA-2000) planned an integrated air defense and air traffic control network. The USAF declared full operational capability of the first seven Joint Surveillance System ROCCs on December 23, 1980, with Hughes AN/FYQ-93 systems, and many of the SAGE radar stations became Joint Surveillance System (JSS) sites (e.g., San Pedro Hill Z-39 became FAA Ground Equipment Facility J-31.) The North Bay AN/FSQ-7 was dismantled and sent to Boston's Computer Museum. In 1996, AN/FSQ-7 components were moved to Moffett Federal Airfield for storage and later moved to the Computer History Museum in Mountain View, California. The last AN/FSQ-7 centrals were demolished at McChord AFB (August 1983) and Luke AFB (February 1984). Decommissioned AN/FSQ-7 equipment was also used as TV series props (e.g. in The Time Tunnel and Voyage to the Bottom of the Sea).

SAGE histories include a 1983 special issue of the "Annals of the History of Computing", and various personal histories were published, e.g., Valley in 1985 and Jacobs in 1986. In 1998, the SAGE System was identified as 1 of 4 "Monumental Projects", and a SAGE lecture presented the vintage film "In Your Defense" followed by anecdotal information from Les Earnest, Jim Wong, and Paul Edwards. In 2013, a copy of a 1950s cover girl image programmed for SAGE display was identified as the "earliest known figurative computer art". Company histories identifying employees' roles in SAGE include the 1981 "System Builders: The Story of SDC" and the 1998 "Architects of Information Advantage: The MITRE Corporation Since 1958".


</ref>




</doc>
<doc id="28011" url="https://en.wikipedia.org/wiki?curid=28011" title="Subgroup">
Subgroup

In group theory, a branch of mathematics, given a group "G" under a binary operation ∗, a subset "H" of "G" is called a subgroup of "G" if "H" also forms a group under the operation ∗. More precisely, "H" is a subgroup of "G" if the restriction of ∗ to is a group operation on "H". This is usually denoted , read as ""H" is a subgroup of "G"".

The trivial subgroup of any group is the subgroup {"e"} consisting of just the identity element.

A proper subgroup of a group "G" is a subgroup "H" which is a proper subset of "G" (that is, ). This is usually represented notationally by , read as ""H" is a proper subgroup of "G"". Some authors also exclude the trivial group from being proper (that is, }).

If "H" is a subgroup of "G", then "G" is sometimes called an overgroup of "H".

The same definitions apply more generally when "G" is an arbitrary semigroup, but this article will only deal with subgroups of groups. The group "G" is sometimes denoted by the ordered pair , usually to emphasize the operation ∗ when "G" carries multiple algebraic or other structures.


Given a subgroup "H" and some "a" in G, we define the left coset "aH" = {"ah" : "h" in "H"}. Because "a" is invertible, the map φ : "H" → "aH" given by φ("h") = "ah" is a bijection. Furthermore, every element of "G" is contained in precisely one left coset of "H"; the left cosets are the equivalence classes corresponding to the equivalence relation "a" ~ "a" if and only if "a""a" is in "H". The number of left cosets of "H" is called the index of "H" in "G" and is denoted by ["G" : "H"].

Lagrange's theorem states that for a finite group "G" and a subgroup "H", 
where |"G"| and |"H"| denote the orders of "G" and "H", respectively. In particular, the order of every subgroup of "G" (and the order of every element of "G") must be a divisor of |"G"|.

Right cosets are defined analogously: "Ha" = {"ha" : "h" in "H"}. They are also the equivalence classes for a suitable equivalence relation and their number is equal to ["G" : "H"].

If "aH" = "Ha" for every "a" in "G", then "H" is said to be a normal subgroup. Every subgroup of index 2 is normal: the left cosets, and also the right cosets, are simply the subgroup and its complement. More generally, if "p" is the lowest prime dividing the order of a finite group "G," then any subgroup of index "p" (if such exists) is normal.

Let "G" be the cyclic group Z whose elements are
and whose group operation is addition modulo eight. Its Cayley table is
This group has two nontrivial subgroups: "J"={0,4} and "H"={0,2,4,6}, where "J" is also a subgroup of "H". The Cayley table for "H" is the top-left quadrant of the Cayley table for "G". The group "G" is cyclic, and so are its subgroups. In general, subgroups of cyclic groups are also cyclic.

Every group has as many small subgroups as neutral elements on the main diagonal:

The trivial group and two-element groups Z. These small subgroups are not counted in the following list.





</doc>
<doc id="28012" url="https://en.wikipedia.org/wiki?curid=28012" title="Series">
Series

Series may refer to:







</doc>
<doc id="28013" url="https://en.wikipedia.org/wiki?curid=28013" title="Silicon Graphics">
Silicon Graphics

Silicon Graphics, Inc. (stylized as SiliconGraphics before 1999, later rebranded SGI, historically known as Silicon Graphics Computer Systems or SGCS) was an American high-performance computing manufacturer, producing computer hardware and software. Founded in Mountain View, California in November 1981 by Jim Clark, its initial market was 3D graphics computer workstations, but its products, strategies and market positions developed significantly over time.

Early systems were based on the Geometry Engine that Clark and Marc Hannah had developed at Stanford University, and were derived from Clark's broader background in computer graphics. The Geometry Engine was the first very-large-scale integration (VLSI) implementation of a "geometry pipeline", specialized hardware that accelerated the "inner-loop" geometric computations needed to display three-dimensional images. For much of its history, the company focused on 3D imaging and was a major supplier of both hardware and software in this market.

Silicon Graphics reincorporated as a Delaware corporation in January 1990. Through the mid to late-1990s, the rapidly improving performance of commodity Wintel machines began to erode SGI's stronghold in the 3D market. The porting of Maya to other platforms was a major event in this process. SGI made several attempts to address this, including a disastrous move from their existing MIPS platforms to the Intel Itanium, as well as introducing their own Linux-based Intel IA-32 based workstations and servers that failed in the market. In the mid-2000s the company repositioned itself as a supercomputer vendor, a move that also failed.

On April 1, 2009, SGI filed for Chapter 11 bankruptcy protection and announced that it would sell substantially all of its assets to Rackable Systems, a deal finalized on May 11, 2009, with Rackable assuming the name Silicon Graphics International. The remains of Silicon Graphics, Inc. became Graphics Properties Holdings, Inc.

James H. Clark left his position as an electrical engineering associate professor at Stanford University to found SGI in 1982 along with a group of seven graduate students and research staff from Stanford: Kurt Akeley, David J. Brown, Tom Davis, Rocky Rhodes, Marc Hannah, Herb Kuta, and Mark Grossman; along with Abbey Silverstone and a few others.

Ed McCracken was CEO of Silicon Graphics from 1984 to 1997. During those years, SGI grew from annual revenues of $5.4 million to $3.7 billion.

The addition of 3D graphic capabilities to PCs, and the ability of clusters of Linux- and BSD-based PCs to take on many of the tasks of larger SGI servers, ate into SGI's core markets. The porting of Maya to Linux, Mac OS X and Microsoft Windows further eroded the low end of SGI's product line.

In response to challenges faced in the marketplace and a falling share price Ed McCracken was fired and SGI brought in Richard Belluzzo to replace him. Under Belluzzo's leadership a number of initiatives were taken which are considered to have accelerated the corporate decline.

One such initiative was trying to sell workstations running Windows NT called Visual Workstations in addition to workstations running IRIX, the company's version of UNIX. This put the company in even more direct competition with the likes of Dell, making it more difficult to justify a price premium. The product line was unsuccessful and abandoned a few years later.

SGI's premature announcement of its migration from MIPS to Itanium and its abortive ventures into IA-32 architecture systems (the Visual Workstation line, the ex-Intergraph Zx10 range and the SGI 1000-series Linux servers) damaged SGI's credibility in the market.

In 1999, in an attempt to clarify their current market position as more than a graphics company, Silicon Graphics Inc. changed its corporate identity to "SGI", although its legal name was unchanged.

At the same time, SGI announced a new logo consisting of only the letters "sgi" in a proprietary font called "SGI", created by branding and design consulting firm Landor Associates, in collaboration with designer Joe Stitzlein. SGI continued to use the "Silicon Graphics" name for its workstation product line, and later re-adopted the cube logo for some workstation models.

In November 2005, SGI announced that it had been delisted from the New York Stock Exchange because its common stock had fallen below the minimum share price for listing on the exchange. SGI's market capitalization dwindled from a peak of over seven billion dollars in 1995 to just $120 million at the time of delisting. In February 2006, SGI noted that it could run out of cash by the end of the year.

In mid-2005, SGI hired Alix Partners to advise it on returning to profitability and received a new line of credit. SGI announced it was postponing its scheduled annual December stockholders meeting until March 2006. It proposed a reverse stock split to deal with the de-listing from the New York Stock Exchange.

In January 2006, SGI hired Dennis McKenna as its new CEO and chairman of the board of directors. Mr. McKenna succeeded Robert Bishop, who remained vice chairman of the board of directors.

On May 8, 2006, SGI announced that it had filed for Chapter 11 bankruptcy protection for itself and U.S. subsidiaries as part of a plan to reduce debt by $250 million. Two days later, the U.S. Bankruptcy Court approved its first day motions and its use of a $70 million financing facility provided by a group of its bondholders. Foreign subsidiaries were unaffected.

On September 6, 2006, SGI announced the end of development for the MIPS/IRIX line and the IRIX operating system. Production would end on December 29 and the last orders would be fulfilled by March 2007. Support for these products would end after December 2013.

SGI emerged from bankruptcy protection on October 17, 2006. Its stock symbol at that point, "SGID.pk", was canceled, and new stock was issued on the NASDAQ exchange under the symbol "SGIC". This new stock was distributed to the company's creditors, and the SGID common stockholders were left with worthless shares. At the end of that year, the company moved its headquarters from Mountain View to Sunnyvale. Its earlier North Shoreline headquarters is now occupied by the Computer History Museum; the newer Amphitheater Parkway headquarters was sold to Google. Both of these locations were award-winning designs by Studios Architecture.

In April 2008, SGI re-entered the visualization market with the SGI Virtu range of visualization servers and workstations, which were re-badged systems from BOXX Technologies based on Intel Xeon or AMD Opteron processors and Nvidia Quadro graphics chipsets, running Red Hat Enterprise Linux, SUSE Linux Enterprise Server or Windows Compute Cluster Server.

In December 2008, SGI received a delisting notification from NASDAQ, as its market value had been below the minimum $35 million requirement for 10 consecutive trading days, and also did not meet NASDAQ's alternative requirements of a minimum stockholders' equity of $2.5 million or annual net income from continuing operations of $500,000 or more.

On April 1, 2009, SGI filed for Chapter 11 again, and announced that it would sell substantially all of its assets to Rackable Systems for $25 million. The sale, ultimately for $42.5 million, was finalized on May 11, 2009; at the same time, Rackable announced their adoption of "Silicon Graphics International" as their global name and brand. The Bankruptcy Court scheduled continuing proceedings and hearings for June 3 and 24, 2009, and July 22, 2009.

After the Rackable acquisition, "Vizworld" magazine published a series of six articles that chronicle the downfall of SGI.

Hewlett Packard Enterprise acquired SGI in November 2016 which allowed HPE to place the TOP500 NASA Ames Research Center, Pleiades SGI ICE supercomputer in its portfolio.

During the Silicon Graphics Inc.'s second bankruptcy phase, it was renamed to Graphics Properties Holdings, Inc.(GPHI) in June 2009.

In 2010, GPHI announced it had won a significant favorable ruling in its litigation with ATI Technologies and AMD in June 2010, following the patent lawsuit originally filed during the Silicon Graphics, Inc. era. Following the 2008 appeal by ATI over the validity of ('327) and Silicon Graphics Inc's voluntary dismissal of the ('376) patent from the lawsuit, the Federal Circuit upheld the jury verdict on the validity of GPHI's U.S. Patent No. 6,650,327, and furthermore found that AMD had lost its right to challenge patent validity in future proceedings. On January 31, 2011, the District Court entered an order that permits AMD to pursue its invalidity affirmative defense at trial and does not permit SGI to accuse AMD's Radeon R700 series of graphics products of infringement in this case. On April 18, 2011, GPHI and AMD had entered into a confidential Settlement and License Agreement that resolved this litigation matter for an immaterial amount and that provides immunity under all GPHI patents for alleged infringement by AMD products, including components, software and designs. On April 26, 2011, the Court entered an order granting the parties' agreed motion for dismissal and final judgment.

In November 2011, GPHI filed another patent infringement lawsuit against Apple Inc. in Delaware involving more patents than their original patent infringement case against Apple last November, for alleged violation of U.S. patents 6,650,327 ('327), ('145) and ('881).

In 2012, the GPHI filed lawsuit against Apple, Sony, HTC Corp, LG Electronics Inc. and Samsung Electronics Co., Research in Motion Ltd. for allegedly violating patent relating to a computer graphics process that turns text and images into pixels to be displayed on screens. Affected devices include Apple iPhone, HTC EVO4G, LG Thrill, Research in Motion Torch, Samsung Galaxy S and Galaxy S II, and Sony Xperia Play smartphones.


SGI's first generation products, starting with the IRIS (Integrated Raster Imaging System) 1000 series of high-performance graphics terminals, were based on the Motorola 68000 family of microprocessors. The later IRIS 2000 and 3000 models developed into full UNIX workstations.

The first entries in the 1000 series (models 1000 and 1200, introduced in 1984) were graphics terminals, peripherals to be connected to a general-purpose computer such as a Digital Equipment Corporation VAX, to provide graphical raster display abilities. They used 8 MHz Motorola 68000 CPUs with of RAM and had no disk drives. They booted over the network (via an Excelan EXOS/101 Ethernet card) from their controlling computer. They used the "PM1" CPU board, which was a variant of the board that was used in Stanford University's SUN workstation and later in the Sun-1 workstation from Sun Microsystems. The graphics system was composed of the GF1 frame buffer, the UC3 "Update Controller", DC3 "Display Controller", and the BP2 bitplane. The 1000-series machines were designed around the Multibus standard.

Later 1000-series machines, the 1400 and 1500, ran at 10 MHz and had 1.5 MB of RAM. The 1400 had a 73 MB ST-506 disk drive, while the 1500 had a 474 MB SMD-based disk drive with a Xylogics 450 disk controller. They may have used the PM2 CPU and PM2M1 RAM board from the 2000 series. The usual monitor for the 1000 series ran at 30 Hz interlaced. Six beta-test units of the 1400 workstation were produced, and the first production unit (SGI's first commercial computer) was shipped to Carnegie-Mellon University's Electronic Imaging Laboratory in 1984.

SGI rapidly developed its machines into workstations with its second product line — the IRIS 2000 series, first released in August 1985. SGI began using the UNIX System V operating system. There were five models in two product ranges, the 2000/2200/2300/2400/2500 range which used 68010 CPUs (the PM2 CPU module), and the later "Turbo" systems, the 2300T, 2400T and 2500T, which had 68020s (the IP2 CPU module). All used the Excelan EXOS/201 Ethernet card, the same graphics hardware (GF2 Frame Buffer, UC4 Update Controller, DC4 Display Controller, BP3 Bitplane). Their main differences were the CPU, RAM, and Weitek Floating Point Accelerator boards, disk controllers and disk drives (both ST-506 and SMD were available). These could be upgraded, for example from a 2400 to a 2400T. The 2500 and 2500T had a larger chassis, a standard 6' 19" EIA rack with space at the bottom for two SMD disk drives weighing approximately each. The non-Turbo models used the Multibus for the CPU to communicate with the floating point accelerator, while the Turbos added a ribbon cable dedicated for this. 60 Hz monitors were used for the 2000 series.

The height of the machines using Motorola CPUs was reached with the IRIS 3000 series (models 3010/3020/3030 and 3110/3115/3120/3130, the 30s both being full-size rack machines). They used the same graphics subsystem and Ethernet as the 2000s, but could also use up to 12 "geometry engines", the first widespread use of hardware graphics accelerators. The standard monitor was a 19" 60 Hz non-interlaced unit with a tilt/swivel base; 19" 30 Hz interlaced and a 15" 60 Hz non-interlaced (with tilt/swivel base) were also available.

The IRIS 3130 and its smaller siblings were impressive for the time, being complete UNIX workstations. The 3130 was powerful enough to support a complete 3D animation and rendering package without mainframe support. With large capacity hard drives by standards of the day (two 300 MB drives), streaming tape and Ethernet, it could be the centerpiece of an animation operation.

The line was formally discontinued in November 1989, with about 3500 systems shipped of all 2000 and 3000 models combined.

With the introduction of the IRIS 4D series, SGI switched to MIPS microprocessors. These machines were more powerful and came with powerful on-board floating-point capability. As 3D graphics became more popular in television and film during this time, these systems were responsible for establishing much of SGI's reputation.

SGI produced a broad range of MIPS-based workstations and servers during the 1990s, running SGI's version of UNIX System V, now called IRIX. These included the massive Onyx visualization systems, the size of refrigerators and capable of supporting up to 64 processors while managing up to three streams of high resolution, fully realized 3D graphics.

In October 1991, MIPS announced the first commercially available 64-bit microprocessor, the R4000. SGI used the R4000 in its Crimson workstation. IRIX 6.2 was the first fully 64-bit IRIX release, including 64-bit pointers.

To secure the supply of future generations of MIPS microprocessors (the 64-bit R4000), SGI acquired the company in 1992 for $333 million and renamed it as MIPS Technologies Inc., a wholly owned subsidiary of SGI.

In 1993, Silicon Graphics (SGI) signed a deal with Nintendo to develop the Reality Coprocessor (RCP) GPU used in the Nintendo 64 (N64) video game console. The deal was signed in early 1993, and it was later made public in August of that year. The console itself was later released in 1996. The RCP was developed by SGI's Nintendo Operations department, led by engineer Dr. Wei Yen. In 1997, twenty SGI employees, led by Yen, left SGI and founded ArtX (later acquired by ATI Technologies in 2000).

In 1998, SGI relinquished some ownership of MIPS Technologies, Inc in a Re-IPO, and fully divested itself in 2000.

In the late 1990s, when much of the industry expected the Itanium to replace both CISC and RISC architectures in non-embedded computers, SGI announced their intent to phase out MIPS in their systems. Development of new MIPS microprocessors stopped, and the existing R12000 design was extended multiple times until 2003 to provide existing customers more time to migrate to Itanium.

In August 2006, SGI announced the end of production for MIPS/IRIX systems, and by the end of the year MIPS/IRIX products were no longer generally available from SGI.

Until the second generation Onyx Reality Engine machines, SGI offered access to its high performance 3D graphics subsystems through a proprietary API known as "IRIS Graphics Language" (IRIS GL). As more features were added over the years, IRIS GL became harder to maintain and more cumbersome to use. In 1992, SGI decided to clean up and reform IRIS GL and made the bold move of allowing the resulting OpenGL API to be cheaply licensed by SGI's competitors, and set up an industry-wide consortium to maintain the OpenGL standard (the OpenGL Architecture Review Board).

This meant that for the first time, fast, efficient, cross-platform graphics programs could be written. To this day, OpenGL remains the only real-time 3D graphics standard to be portable across a variety of operating systems. OpenGL-ES even runs on many types of cell phones. Its main competitor (Direct3D from Microsoft) runs only on Microsoft Windows-based machines and some consoles.

SGI was part of the Advanced Computing Environment initiative, formed in the early 1990s with 20 other companies, including Compaq, Digital Equipment Corporation, MIPS Computer Systems, Groupe Bull, Siemens, NEC, NeTpower, Microsoft and Santa Cruz Operation. Its intent was to introduce workstations based on the MIPS architecture and able to run Windows NT and SCO UNIX. The group produced the Advanced RISC Computing (ARC) specification, but began to unravel little more than a year after its formation.

For eight consecutive years (1995–2002), all films nominated for an Academy Award for Distinguished Achievement in Visual Effects were created on Silicon Graphics computer systems. the technology was also used in commercials for a host of companies.

An SGI Crimson system with the fsn three-dimensional file system navigator appeared in the 1993 movie "Jurassic Park".

In the movie "Twister", protagonists can be seen using an SGI laptop computer; however, the unit shown was not an actual working computer, but rather a fake laptop shell built around an SGI Corona LCD flat screen display.

The 1995 film "Congo" also features an SGI laptop computer being used by Dr. Ross (Laura Linney) to communicate via satellite to TraviCom HQ.

The purple, lowercased "sgi" logo can be seen at the beginning of the opening credits of the HBO series "Silicon Valley", before being taken down and replaced by the Google logo as the intro graphics progress. Google leased the former SGI buildings in 2003 for their headquarters in Mountain View, CA until they purchased the buildings outright in 2006.

Once inexpensive PCs began to have graphics performance close to the more expensive specialized graphical workstations which were SGI's core business, SGI shifted its focus to high performance servers for digital video and the Web. Many SGI graphics engineers left to work at other computer graphics companies such as ATI and Nvidia, contributing to the PC 3D graphics revolution.

SGI was a promoter of free software, supporting several projects such as Linux and Samba, and opening some of its own previously proprietary code such as the XFS filesystem and the Open64 compiler.

SGI was also important in its contribution to the C++ Standard Template Library (STL) with many useful extensions in the MIT-like licensed SGI STL implementation. The extension keeps being carried by the direct descendant STLport and GNU's libstdc++.

In 1995, SGI purchased Alias Research, Kroyer Films, and Wavefront Technologies in a deal totaling approximately $500 million and merged the companies into Alias|Wavefront. In June 2004 SGI sold the business, later renamed to Alias/Wavefront, to the private equity investment firm Accel-KKR for $57.1 million. In October 2005, Autodesk announced that it signed a definitive agreement to acquire Alias for $182 million in cash.

In February 1996, SGI purchased the well-known supercomputer manufacturer Cray Research for $740 million, and began to use marketing names such as "CrayLink" for (SGI-developed) technology integrated into the SGI server line. Three months later, it sold the Cray Business Systems Division, responsible for the CS6400 SPARC/Solaris server, to Sun Microsystems for an undisclosed amount (acknowledged later by a Sun executive to be "significantly less than $100 million"). Many of the Cray T3E engineers designed and developed the SGI Altix and NUMAlink technology. SGI sold the Cray brand and product lines to Tera Computer Company on March 31, 2000, for $35 million plus one million shares. SGI also distributed its remaining interest in MIPS Technologies through a spin-off effective June 20, 2000.

In September 2000, SGI acquired the Zx10 series of Windows workstations and servers from Intergraph Computer Systems (for a rumored $100 million), and rebadged them as SGI systems. The product line was discontinued in June 2001.

Another attempt by SGI in the late 1990s to introduce its own family of Intel-based workstations running Windows NT or Red Hat Linux (see also SGI Visual Workstation) proved to be a financial disaster, and shook customer confidence in SGI's commitment to its own MIPS-based line.

In 1998, SGI announced that future generations of its machines would be based not on their own MIPS processors, but the upcoming "super-chip" from Intel, code-named "Merced" and later called Itanium. Funding for its own high-end processors was reduced, and it was planned that the R10000 would be the last MIPS mainstream processor. MIPS Technologies would focus entirely on the embedded market, where it was having some success, and SGI would no longer have to fund development of a CPU that, since the failure of ARC, found use only in their own machines. This plan quickly went awry. As early as 1999 it was clear the Itanium was going to be delivered very late and would have nowhere near the performance originally expected. As the production delays increased, MIPS' existing R10000-based machines grew increasingly uncompetitive. Eventually it was forced to introduce faster MIPS processors, the R12000, R14000 and R16000, which were used in a series of models from 1999 through 2006.

SGI's first Itanium-based system was the short-lived SGI 750 workstation, launched in 2001. SGI's MIPS-based systems were not to be superseded until the launch of the Itanium 2-based Altix servers and Prism workstations some time later. Unlike the MIPS systems, which ran IRIX, the Itanium systems used SuSE Linux Enterprise Server with SGI enhancements as their operating system. SGI used Transitive Corporation's QuickTransit software to allow their old MIPS/IRIX applications to run (in emulation) on the new Itanium/Linux platform.

In the server market the Itanium 2-based Altix eventually replaced the MIPS-based Origin product line. In the workstation market, the switch to Itanium was not completed before SGI exited the market.

The Altix was the most powerful computer in the world in 2006, assuming that a "computer" is defined as a collection of hardware running under a single instance of an operating system. The Altix had 512 Itanium processors running under a single instance of Linux. A cluster of 20 machines was then the eighth-fastest supercomputer. All faster supercomputers were clusters, but none have as many FLOPS per machine. However, more recent supercomputers are very large clusters of machines that are individually less capable. SGI acknowledged this and in 2007 moved away from the "massive NUMA" model to clusters.

Although SGI continued to market Itanium-based machines, its more recent machines were based on the Intel Xeon processor. The first Altix XE systems were relatively low-end machines, but by December 2006 the XE systems were more capable than the Itanium machines by some measures (e.g., power consumption in FLOPS/W, density in FLOPS/m, cost/FLOPS). The XE1200 and XE1300 servers used a cluster architecture. This was a departure from the pure NUMA architectures of the earlier Itanium and MIPS servers.

In June 2007, SGI announced the Altix ICE 8200, a blade-based Xeon system with up to 512 Xeon cores per rack. An Altix ICE 8200 installed at New Mexico Computing Applications Center (with 14336 processors) ranked at number 3 on the TOP500 list of November 2007.

Conventional wisdom holds that SGI's core market has traditionally been Hollywood visual effects studios. In fact, SGI's largest revenue has always been generated by government and defense applications, energy, and scientific and technical computing. In one case Silicon Graphics' largest single sale ever was to the United States Postal Service. SGI's servers powered an artificial intelligence program to mechanically read, tag and sort the mail (hand-written and block) at a number of USPS's key mail centers. The rise of cheap yet powerful commodity workstations running Linux, Windows and Mac OS X, and the availability of diverse professional software for them, effectively pushed SGI out of the visual effects industry in all but the most niche markets.

SGI continued to enhance its line of servers (including some supercomputers) based on the SN architecture. SN, for Scalable Node, is a technology developed by SGI in the mid-1990s that uses cache-coherent non-uniform memory access (cc-NUMA). In an SN system, processors, memory, and a bus- and memory-controller are coupled together into an entity called a node, usually on a single circuit board. Nodes are connected by a high-speed interconnect called NUMAlink (originally marketed as CrayLink). There is no internal bus, and instead access between processors, memory, and I/O devices is done through a switched fabric of links and routers.

Thanks to the cache coherence of the distributed shared memory, SN systems scale along several axes at once: as CPU count increases, so does memory capacity, I/O capacity, and system bisection bandwidth. This allows the combined memory of all the nodes to be accessed under a single OS image using standard shared-memory synchronization methods. This makes an SN system far easier to program and able to achieve higher sustained-to-peak performance than non-cache-coherent systems like conventional clusters or massively parallel computers which require applications code to be written (or re-written) to do explicit message-passing communication between their nodes.

The first SN system, known as SN-0, was released in 1996 under the product name Origin 2000. Based on the MIPS R10000 processor, it scaled from 2 to 128 processors and a smaller version, the Origin 200 (SN-00), scaled from 1 to 4. Later enhancements enabled systems of as large as 512 processors.

The second generation system, originally called SN-1 but later SN-MIPS, was released in July 2000, as the Origin 3000. It scaled from 4 to 512 processors, and 1,024-processor configurations were delivered by special order to some customers. A smaller, less scalable implementation followed, called Origin 300.

In November 2002, SGI announced a repackaging of its SN system, under the name Origin 3900. It quadrupled the processor area density of the SN-MIPS system, from 32 up to 128 processors per rack while moving to a "fat tree" interconnect topology.

In January 2003, SGI announced a variant of the SN platform called the Altix 3000 (internally called SN-IA). It used Intel Itanium 2 processors and ran the Linux operating system kernel. At the time it was released, it was the world's most scalable Linux-based computer, supporting up to 64 processors in a single system node. Nodes could be connected using the same NUMAlink technology to form what SGI predictably termed "superclusters".

In February 2004, SGI announced general support for 128 processor nodes to be followed by 256 and 512 processor versions that year.

In April 2004, SGI announced the sale of its Alias software business for approximately $57 million.

In October 2004, SGI built the supercomputer Columbia, which broke the world record for computer speed, for the NASA Ames Research Center. It was a cluster of 20 Altix supercomputers each with 512 Intel Itanium 2 processors running Linux, and achieved sustained speed of 42.7 trillion floating-point operations per second (teraflops), easily topping Japan's famed Earth Simulator's record of 35.86 teraflops. (A week later, IBM's upgraded Blue Gene/L clocked in at 70.7 teraflops.)

In July 2006, SGI announced an SGI Altix 4700 system with 1,024 processors and 4 TB of memory running a single Linux system image.

Some 68k and MIPS-based models were also rebadged by other vendors, including CDC, Tandem Computers, Prime Computer and Siemens-Nixdorf.
SGI Onyx and SGI Indy series systems were used for game development for the Nintendo 64.















Donkey Kong Country developers used Silicon Graphics computers to pre-render graphics in their three games. 



</doc>
<doc id="28016" url="https://en.wikipedia.org/wiki?curid=28016" title="Steiner system">
Steiner system

In combinatorial mathematics, a Steiner system (named after Jakob Steiner) is a type of block design, specifically a with λ = 1 and "t" ≥ 2.

A Steiner system with parameters "t", "k", "n", written S("t","k","n"), is an "n"-element set "S" together with a set of "k"-element subsets of "S" (called blocks) with the property that each "t"-element subset of "S" is contained in exactly one block. In an alternate notation for block designs, an S("t","k","n") would be a "t"-("n","k",1) design.

This definition is relatively new. The classical definition of Steiner systems also required that "k" = "t" + 1. An S(2,3,"n") was (and still is) called a "Steiner triple" (or "triad") "system", while an S(3,4,"n") is called a "Steiner quadruple system", and so on. With the generalization of the definition, this naming system is no longer strictly adhered to.

Long-standing problems in design theory were whether there exist any nontrivial Steiner systems (nontrivial meaning "t" < "k" < "n") with "t" ≥ 6; also whether infinitely many have "t" = 4 or 5. Both existences were proved by Peter Keevash in 2014. His proof is non-constructive and, as of 2019, no actual Steiner systems are known for large values of "t".

A finite projective plane of order , with the lines as blocks, is an , since it has points, each line passes through points, and each pair of distinct points lies on exactly one line.

A finite affine plane of order , with the lines as blocks, is an . An affine plane of order can be obtained from a projective plane of the same order by removing one block and all of the points in that block from the projective plane. Choosing different blocks to remove in this way can lead to non-isomorphic affine planes.

An S(3,4,"n") is called a Steiner quadruple system. A necessary and sufficient condition for the existence of an S(3,4,"n") is that "n" formula_1 2 or 4 (mod 6). The abbreviation SQS("n") is often used for these systems. Up to isomorphism, SQS(8) and SQS(10) are unique, there are 4 SQS(14)s and 1,054,163 SQS(16)s.

An S(4,5,"n") is called a Steiner quintuple system. A necessary condition for the existence of such a system is that "n" formula_1 3 or 5 (mod 6) which comes from considerations that apply to all the classical Steiner systems. An additional necessary condition is that "n" formula_3 4 (mod 5), which comes from the fact that the number of blocks must be an integer. Sufficient conditions are not known. There is a unique Steiner quintuple system of order 11, but none of order 15 or order 17. Systems are known for orders 23, 35, 47, 71, 83, 107, 131, 167 and 243. The smallest order for which the existence is not known (as of 2011) is 21.

An S(2,3,"n") is called a Steiner triple system, and its blocks are called triples. It is common to see the abbreviation STS("n") for a Steiner triple system of order "n". The total number of pairs is "n(n-1)/2", of which three appear in a triple, and so the total number of triples is "n"("n"−1)/6. This shows that "n" must be of the form "6k+1" or "6k + 3" for some "k". The fact that this condition on "n" is sufficient for the existence of an S(2,3,"n") was proved by Raj Chandra Bose and T. Skolem. The projective plane of order 2 (the Fano plane) is an STS(7) and the affine plane of order 3 is an STS(9). Up to isomorphism, the STS(7) and STS(9) are unique, there are two STS(13)s, 80 STS(15)s, and 11,084,874,829 STS(19)s.

Some of the S(2,3,n) systems can have their blocks be partitioned into (n-1)/2 sets of (n/3) triples each. This is called "resolvable" and such systems are called "Kirkman triple systems" after Thomas Kirkman, who studied such resolvable systems before Steiner. Dale Mesner, Earl Kramer, and others investigated collections of Steiner triple systems that are mutually disjoint (i.e., no two Steiner systems in such a collection share a common triplet). It is known (Bays 1917, Kramer & Mesner 1974) that seven different S(2,3,9) systems can be generated to together cover all 84 triplets on a 9-set; it was also known by them that there are 15360 different ways to find such 7-sets of solutions, which reduce to two non-isomorphic solutions under relabeling, with multiplicities 6720 and 8640 respectively. The corresponding question for finding thirteen different disjoint S(2,3,15) systems was asked by James Sylvester in 1860 and answered by RHF Denniston in 1974. There is at least one such 13-set of S(2,3,15) but its isomorphism is not known.

We can define a multiplication on the set "S" using the Steiner triple system by setting "aa" = "a" for all "a" in "S", and "ab" = "c" if {"a","b","c"} is a triple. This makes "S" an idempotent, commutative quasigroup. It has the additional property that "ab" = "c" implies "bc" = "a" and "ca" = "b". Conversely, any (finite) quasigroup with these properties arises from a Steiner triple system. Commutative idempotent quasigroups satisfying this additional property are called "Steiner quasigroups".

It is clear from the definition of that formula_4. (Equalities, while technically possible, lead to trivial systems.)

If exists, then taking all blocks containing a specific element and discarding that element gives a "derived system" . Therefore, the existence of is a necessary condition for the existence of .

The number of -element subsets in is formula_5, while the number of -element subsets in each block is formula_6. Since every -element subset is contained in exactly one block, we have formula_7, or 
where is the number of blocks. Similar reasoning about -element subsets containing a particular element gives us formula_9, or 
where is the number of blocks containing any given element. From these definitions follows the equation formula_12. It is a necessary condition for the existence of that and are integers. As with any block design, Fisher's inequality formula_13 is true in Steiner systems.

Given the parameters of a Steiner system and a subset of size formula_14, contained in at least one block, one can compute the number of blocks intersecting that subset in a fixed number of elements by constructing a Pascal triangle. In particular, the number of blocks intersecting a fixed block in any number of elements is independent of the chosen block.

The number of blocks that contain any "i"-element set of points is:

It can be shown that if there is a Steiner system , where is a prime power greater than 1, then formula_1 1 or . In particular, a Steiner triple system must have . And as we have already mentioned, this is the only restriction on Steiner triple systems, that is, for each natural number , systems and exist.

Steiner triple systems were defined for the first time by Wesley S. B. Woolhouse in 1844 in the Prize question #1733 of Lady's and Gentlemen's Diary. The posed problem was solved by . In 1850 Kirkman posed a variation of the problem known as Kirkman's schoolgirl problem, which asks for triple systems having an additional property (resolvability). Unaware of Kirkman's work, reintroduced triple systems, and as this work was more widely known, the systems were named in his honor.

Several examples of Steiner systems are closely related to group theory. In particular, the finite simple groups called Mathieu groups arise as automorphism groups of Steiner systems:


There is a unique S(5,6,12) Steiner system; its automorphism group is the Mathieu group M, and in that context it is denoted by W.

This construction is due to Carmichael (1937).

Add a new element, call it , to the 11 elements of the finite field (that is, the integers mod 11). This set, , of 12 elements can be formally identified with the points of the projective line over . Call the following specific subset of size 6,
a "block" (it contains together with the 5 nonzero squares in ). From this block, we obtain the other blocks of the (5,6,12) system by repeatedly applying the linear fractional transformations:
where are in and .
With the usual conventions of defining and , these functions map the set onto itself. In geometric language, they are projectivities of the projective line. They form a group under composition which is the projective special linear group (2,11) of order 660. There are exactly five elements of this group that leave the starting block fixed setwise, namely those such that and so that . So there will be 660/5 = 132 images of that block. As a consequence of the multiply transitive property of this group acting on this set, any subset of five elements of will appear in exactly one of these 132 images of size six.

An alternative construction of W is obtained by use of the 'kitten' of R.T. Curtis, which was intended as a "hand calculator" to write down blocks one at a time. The kitten method is based on completing patterns in a 3x3 grid of numbers, which represent an affine geometry on the vector space FxF, an S(2,3,9) system.

The relations between the graph factors of the complete graph K generate an S(5,6,12). A K graph has 6 vertices, 15 edges, 15 perfect matchings, and 6 different 1-factorizations (ways to partition the edges into disjoint perfect matchings). The set of vertices (labeled 123456) and the set of factorizations (labeled "ABCDEF") provide one block each. Every pair of factorizations has exactly one perfect matching in common. Suppose factorizations "A" and "B" have the common matching with edges 12, 34 and 56. Add three new blocks "AB"3456, 12"AB"56, and 1234"AB", replacing each edge in the common matching with the factorization labels in turn. Similarly add three more blocks 12"CDEF", 34"CDEF", and 56"CDEF", replacing the factorization labels by the corresponding edge labels of the common matching. Do this for all 15 pairs of factorizations to add 90 new blocks. Finally, take the full set of formula_19 combinations of 6 objects out of 12, and discard any combination that has 5 or more objects in common with any of the 92 blocks generated so far. Exactly 40 blocks remain, resulting in blocks of the S(5,6,12). This method works because there is an outer automorphism on the symmetric group "S", which maps the vertices to factorizations and the edges to partitions. Permuting the vertices causes the factorizations to permute differently, in accordance with the outer automorphism.

The Steiner system S(5, 8, 24), also known as the Witt design or Witt geometry, was first described by and rediscovered by . This system is connected with many of the sporadic simple groups and with the exceptional 24-dimensional lattice known as the Leech lattice. The automorphism group of S(5, 8, 24) is the Mathieu group M, and in that context the design is denoted W ("W" for "Witt")

All 8-element subsets of a 24-element set are generated in lexicographic order, and any such subset which differs from some subset already found in fewer than four positions is discarded.

The list of octads for the elements 01, 02, 03, ..., 22, 23, 24 is then:

Each single element occurs 253 times somewhere in some octad. Each pair occurs 77 times. Each triple occurs 21 times. Each quadruple (tetrad) occurs 5 times. Each quintuple (pentad) occurs once. Not every hexad, heptad or octad occurs.

The 4096 codewords of the 24-bit binary Golay code are generated, and the 759 codewords with a Hamming weight of 8 correspond to the S(5,8,24) system.

The Golay code can be constructed by many methods, such as generating all 24-bit binary strings in lexicographic order and discarding those that differ from some earlier one in fewer than 8 positions. The result looks like this:

The codewords form a group under the XOR operation.

The Miracle Octad Generator (MOG) is a tool to generate octads, such as those containing specified subsets. It consists of a 4x6 array with certain weights assigned to the rows. In particular, an 8-subset should obey three rules in order to be an octad of S(5,8,24). First, each of the 6 columns should have the same parity, that is, they should all have an odd number of cells or they should all have an even number of cells. Second, the top row should have the same parity as each of the columns. Third, the rows are assigned respectively the weights 0, 1, ω, and ω, where ω is a complex cube root of unity and column sums are calculated for the 8 cells selected over the 6 columns. The resulting column sums should form a valid "hexacodeword" of the form where "a, b, c" are in }. If the column sums' parities don't match the row sum parity, or each other, or if there do not exist "a, b, c" such that the column sums form a valid hexacodeword, then that subset of 8 is not an octad of S(5,8,24).

The MOG is based on creating a bijection (Conwell 1910, "The three-space PG(3,2) and its group") between the 35 ways to partition an 8-set into two different 4-sets, and the 35 lines of the Fano 3-space PG(3,2).

It is also geometrically related (Cullinane, "Symmetry Invariance in a Diamond Ring", Notices of the AMS, pp A193-194, Feb 1979) to the 35 different ways to partition a 4x4 array into 4 different groups of 4 cells each, such that if the 4x4 array represents a four-dimensional finite affine space, then the groups form a set of parallel subspaces.





</doc>
<doc id="28017" url="https://en.wikipedia.org/wiki?curid=28017" title="Sirius">
Sirius

Sirius (, designated α Canis Majoris (Latinized to Alpha Canis Majoris, abbreviated Alpha CMa, α CMa)) is the brightest star in the night sky. Its name is derived from the Greek word "Seirios" "glowing" or "scorching". With a visual apparent magnitude of −1.46, Sirius is almost twice as bright as Canopus, the next brightest star. Sirius is a binary star consisting of a main-sequence star of spectral type A0 or A1, termed Sirius A, and a faint white dwarf companion of spectral type DA2, termed Sirius B. The distance between the two varies between 8.2 and 31.5 astronomical units as they orbit every 50 years.

Sirius appears bright because of its intrinsic luminosity and its proximity to the Solar System. At a distance of , the Sirius system is one of Earth's nearest neighbours. Sirius is gradually moving closer to the Solar System, so it will slightly increase in brightness over the next 60,000 years. After that time, its distance will begin to increase, and it will become fainter, but it will continue to be the brightest star in the Earth's night sky for the next 210,000 years.

Sirius A is about twice as massive as the Sun () and has an absolute visual magnitude of +1.42. It is 25 times more luminous than the Sun but has a significantly lower luminosity than other bright stars such as Canopus or Rigel. The system is between 200 and years old. It was originally composed of two bright bluish stars. The more massive of these, Sirius B, consumed its resources and became a red giant before shedding its outer layers and collapsing into its current state as a white dwarf around years ago.

Sirius is known colloquially as the "Dog Star", reflecting its prominence in its constellation, Canis Major (the Greater Dog). The heliacal rising of Sirius marked the flooding of the Nile in Ancient Egypt and the "dog days" of summer for the ancient Greeks, while to the Polynesians, mostly in the Southern Hemisphere, the star marked winter and was an important reference for their navigation around the Pacific Ocean.

The brightest star in the night sky, Sirius is recorded in some of the earliest astronomical records. Its displacement from the ecliptic causes its heliacal rising to be remarkably regular compared to other stars, with a period of almost exactly 365.25 days holding it constant relative to the solar year. This rising occurs at Cairo on 19July (Julian), placing it just prior to the onset of the annual flooding of the Nile during antiquity. Owing to the flood's own irregularity, the extreme precision of the star's return made it important to the ancient Egyptians, who worshipped it as the goddess Sopdet (, "Triangle"; , "Sō̂this"), guarantor of the fertility of their land. The Egyptian civil calendar was apparently initiated to have its New Year "Mesori" coincide with the appearance of Sirius, although its lack of leap years meant that this congruence only held for four years until its date began to wander backwards through the months. The Egyptians continued to note the times of Sirius's annual return, which may have led them to the discovery of the 1460-year Sothic cycle and influenced the development of the Julian and Alexandrian calendars.

The ancient Greeks observed that the appearance of Sirius heralded the hot and dry summer and feared that it caused plants to wilt, men to weaken, and women to become aroused. Due to its brightness, Sirius would have been seen to twinkle more in the unsettled weather conditions of early summer. To Greek observers, this signified emanations that caused its malignant influence. Anyone suffering its effects was said to be "star-struck" (, "astrobólētos"). It was described as "burning" or "flaming" in literature. The season following the star's reappearance came to be known as the "dog days". The inhabitants of the island of Ceos in the Aegean Sea would offer sacrifices to Sirius and Zeus to bring cooling breezes and would await the reappearance of the star in summer. If it rose clear, it would portend good fortune; if it was misty or faint then it foretold (or emanated) pestilence. Coins retrieved from the island from the 3rd century BC feature dogs or stars with emanating rays, highlighting Sirius's importance. The Romans celebrated the heliacal setting of Sirius around April 25, sacrificing a dog, along with incense, wine, and a sheep, to the goddess Robigo so that the star's emanations would not cause wheat rust on wheat crops that year.

Ptolemy of Alexandria mapped the stars in Books VII and VIII of his "Almagest", in which he used Sirius as the location for the globe's central meridian. He depicted it as one of six red-coloured stars (see the Colour controversy section below). The other five are class M and K stars, such as Arcturus and Betelgeuse.

Bright stars were important to the ancient Polynesians for navigation of the Pacific Ocean. They also served as latitude markers; the declination of Sirius matches the latitude of the archipelago of Fiji at 17°S and thus passes directly over the islands each night. Sirius served as the body of a "Great Bird" constellation called "Manu", with Canopus as the southern wingtip and Procyon the northern wingtip, which divided the Polynesian night sky into two hemispheres. Just as the appearance of Sirius in the morning sky marked summer in Greece, it marked the onset of winter for the Māori, whose name "Takurua" described both the star and the season. Its culmination at the winter solstice was marked by celebration in Hawaii, where it was known as "Ka'ulua", "Queen of Heaven". Many other Polynesian names have been recorded, including "Tau-ua" in the Marquesas Islands, "Rehua" in New Zealand, and "Ta'urua-fau-papa" "Festivity of original high chiefs" and "Ta'urua-e-hiti-i-te-tara-te-feiai" "Festivity who rises with prayers and religious ceremonies" in Tahiti. The Hawaiian people had many names for Sirius, including "Aa" ("glowing"), "Hoku-kauopae", "Kau-ano-meha" (also "Kaulanomeha"), "Standing-alone-and-sacred", "Hiki-kauelia" or "Hiki-kauilia" (the navigational name), "Hiki-kau-lono-meha" ("star of solitary Lono", the astrological name), "Kaulua" (also "Kaulua-ihai-mohai", "flower of the heavens"), "Hiki-kauelia", "Hoku-hoo-kele-waa" ("star which causes the canoe to sail", a marine navigation name), and "Kaulua-lena" ("yellow star"). The people of the Society Islands called Sirius variously "Taurua-fau-papa", "Taurua-nui-te-amo-aha", and "Taurua-e-hiti-i-tara-te-feiai". Other names for Sirius included "Palolo-mua" (Futuna), "Mere" (Mangaia), "Apura" (Manihiki), "Taku-ua" (Marquesas Islands), and "Tokiva" (Pukapuka). In the cosmology of the Tuamotus, Sirius had various names, including "Takurua-te-upuupu", "Te Kaha" ("coconut fibre"), "Te Upuupu", "Taranga", and "Vero-ma-torutoru" ("flaming and diminishing").

The indigenous Boorong people of northwestern Victoria named Sirius as "Warepil".

In 1717, Edmond Halley discovered the proper motion of the hitherto presumed "fixed" stars after comparing contemporary astrometric measurements with those from the second century AD given in Ptolemy's "Almagest". The bright stars Aldebaran, Arcturus and Sirius were noted to have moved significantly; Sirius had progressed about 30 arc minutes (about the diameter of the Moon) to the southwest.

In 1868, Sirius became the first star to have its velocity measured, the beginning of the study of celestial radial velocities. Sir William Huggins examined the spectrum of the star and observed a red shift. He concluded that Sirius was receding from the Solar System at about 40 km/s. Compared to the modern value of −5.5 km/s, this was an overestimate and had the wrong sign; the minus sign (−) means that it is approaching the Sun. It is possible that Huggins did not account for the Earth's orbital velocity, which would cause an error of up to 30 km/s.

In his 1698 book, "Cosmotheoros", Christiaan Huygens estimated the distance to Sirius at 27664 times the distance from the Earth to the Sun (about 0.437 light years, translating to a parallax of roughly 7.5 arcseconds). There were several unsuccessful attempts to measure the parallax of Sirius: by Jacques Cassini (6 seconds); by some astronomers (including Nevil Maskelyne) using Lacaille's observations made at the Cape of Good Hope (4 seconds); by Piazzi (the same amount); using Lacaille's observations made at Paris, more numerous and certain than those made at the Cape (no sensible parallax); by Bessel (no sensible parallax).

Scottish astronomer Thomas Henderson used his observations made in 1832–1833 and South African astronomer Thomas Maclear's observations made in 1836–1837, to determine that the value of the parallax was 0.23 arcseconds, and error of the parallax was estimated not to exceed a quarter of a second, or as Henderson wrote in 1839, "On the whole we may conclude that the parallax of Sirius is not greater than half a second in space; and that it is probably much less." Astronomers adopted a value of 0.25 arcseconds for much of the 19th century. It is now known to have a parallax of arcseconds and therefore a distance of parsecs, showing Henderson's estimate to be accurate.

In 1844, the German astronomer Friedrich Bessel deduced from changes in the proper motion of Sirius that it had an unseen companion. On January 31, 1862, American telescope-maker and astronomer Alvan Graham Clark first observed the faint companion, which is now called Sirius B, or affectionately "the Pup". This happened during testing of an aperture great refractor telescope for Dearborn Observatory, which was one of the largest refracting telescope lenses in existence at the time, and the largest telescope in the United States. Sirius B's sighting was confirmed on March 8 with smaller telescopes.

The visible star is now sometimes known as Sirius A. Since 1894, some apparent orbital irregularities in the Sirius system have been observed, suggesting a third very small companion star, but this has never been confirmed. The best fit to the data indicates a six-year orbit around Sirius A and a mass of . This star would be five to ten magnitudes fainter than the white dwarf Sirius B, which would make it difficult to observe. Observations published in 2008 were unable to detect either a third star or a planet. An apparent "third star" observed in the 1920s is now believed to be a background object.

In 1915, Walter Sydney Adams, using a 60-inch (1.5 m) reflector at Mount Wilson Observatory, observed the spectrum of Sirius B and determined that it was a faint whitish star. This led astronomers to conclude that it was a white dwarf, the second to be discovered. The diameter of Sirius A was first measured by Robert Hanbury Brown and Richard Q. Twiss in 1959 at Jodrell Bank using their stellar intensity interferometer. In 2005, using the Hubble Space Telescope, astronomers determined that Sirius B has nearly the diameter of the Earth, , with a mass 102% of the Sun's.

Around the year 150 CE, the Greek astronomer of the Roman period, Claudius Ptolemy, described Sirius as reddish, along with five other stars, Betelgeuse, Antares, Aldebaran, Arcturus and Pollux, all of which are of orange or red hue. The discrepancy was first noted by amateur astronomer Thomas Barker, squire of Lyndon Hall in Rutland, who prepared a paper and spoke at a meeting of the Royal Society in London in 1760. The existence of other stars changing in brightness gave credibility to the idea that some may change in colour too; Sir John Herschel noted this in 1839, possibly influenced by witnessing Eta Carinae two years earlier. Thomas Jefferson Jackson See resurrected discussion on red Sirius with the publication of several papers in 1892, and a final summary in 1926. He cited not only Ptolemy but also the poet Aratus, the orator Cicero, and general Germanicus as calling the star red, though acknowledging that none of the latter three authors were astronomers, the last two merely translating Aratus's poem "Phaenomena". Seneca had described Sirius as being of a deeper red than Mars. Not all ancient observers saw Sirius as red. The 1st-century poet Marcus Manilius described it as "sea-blue", as did the 4th century Avienus. It was the standard white star in ancient China, and multiple records from the 2nd century BCE up to the 7th century CE all describe Sirius as white.

In 1985, German astronomers Wolfhard Schlosser and Werner Bergmann published an account of an 8th-century Lombardic manuscript, which contains "De cursu stellarum ratio" by St. Gregory of Tours. The Latin text taught readers how to determine the times of nighttime prayers from positions of the stars, and Sirius is described within as "rubeola" – "reddish". The authors proposed this was further evidence Sirius B had been a red giant at the time. Other scholars replied that it was likely St. Gregory had been referring to Arcturus.

The possibility that stellar evolution of either Sirius A or Sirius B could be responsible for this discrepancy has been rejected by astronomers on the grounds that the timescale of thousands of years is much too short and that there is no sign of the nebulosity in the system that would be expected had such a change taken place. An interaction with a third star, to date undiscovered, has also been proposed as a possibility for a red appearance. Alternative explanations are either that the description as red is a poetic metaphor for ill fortune, or that the dramatic scintillations of the star when rising left the viewer with the impression that it was red. To the naked eye, it often appears to be flashing with red, white, and blue hues when near the horizon.

With an apparent magnitude of −1.46, Sirius is the brightest star in the night sky, almost twice as bright as the second-brightest star, Canopus. From Earth, Sirius always appears dimmer than Jupiter and Venus, as well as Mercury and Mars at certain times. Sirius is visible from almost everywhere on Earth, except latitudes north of 73° N, and it does not rise very high when viewed from some northern cities (reaching only 13° above the horizon from Saint Petersburg). Due to its declination of roughly −17°, Sirius is a circumpolar star from latitudes south of 73° S. From the Southern Hemisphere in early July, Sirius can be seen in both the evening where it sets after the Sun and in the morning where it rises before the Sun.

Sirius, along with Procyon and Betelgeuse, forms one of the three vertices of the Winter Triangle to observers in the Northern Hemisphere.

Due to precession (and slight proper motion), Sirius will move further south in the future. Starting around the year 9000, Sirius will no longer be visible from northern and central Europe, and in 14,000 its declination will be −67° and thus it will be circumpolar throughout South Africa and in most parts of Australia.

Sirius can be observed in daylight with the naked eye under the right conditions. Ideally, the sky should be very clear, with the observer at a high altitude, the star passing overhead, and the Sun low on the horizon. These observing conditions are more easily met in the Southern Hemisphere, due to the southerly declination of Sirius.

The orbital motion of the Sirius binary system brings the two stars to a minimum angular separation of 3 arcseconds and a maximum of 11 arcseconds. At the closest approach, it is an observational challenge to distinguish the white dwarf from its more luminous companion, requiring a telescope with at least 300 mm (12 in) aperture and excellent seeing conditions. After a periastron occurred in 1994, the pair moved apart, making them easier to separate with a telescope. Apoastron occurred in 2019, but from the Earth's vantage point, the greatest observational separation will occur in 2023, with an angular separation of 11.333".

At a distance of 2.6 parsecs (8.6 ly), the Sirius system contains two of the eight nearest stars to the Sun, and it is the fifth closest stellar system to the Sun. This proximity is the main reason for its brightness, as with other near stars such as Alpha Centauri and in contrast to distant, highly luminous supergiants such as Canopus, Rigel or Betelgeuse. It is still around 25 times more luminous than the Sun. The closest large neighbouring star to Sirius is Procyon, 1.61 parsecs (5.24 ly) away. The "Voyager 2" spacecraft, launched in 1977 to study the four giant planets in the Solar System, is expected to pass within of Sirius in approximately 296,000 years.

Sirius is a binary star system consisting of two white stars orbiting each other with a separation of about 20 AU (roughly the distance between the Sun and Uranus) and a period of 50.1 years. The brighter component, termed Sirius A, is a main-sequence star of spectral type early A, with an estimated surface temperature of 9,940 K. Its companion, Sirius B, is a star that has already evolved off the main sequence and become a white dwarf. Currently 10,000 times less luminous in the visual spectrum, Sirius B was once the more massive of the two. The age of the system has been estimated at around 230 million years. Early in its life, it is thought to have been two bluish-white stars orbiting each other in an elliptical orbit every 9.1 years. The system emits a higher than expected level of infrared radiation, as measured by IRAS space-based observatory. This might be an indication of dust in the system, which is considered somewhat unusual for a binary star. The Chandra X-ray Observatory image shows Sirius B outshining its partner as an X-ray source.

In 2015, Vigan and colleagues used the VLT Survey Telescope to search for evidence of substellar companions, and were able to rule out the presence of giant planets 11 times more massive than Jupiter at 0.5 AU distance from Sirius A, 6–7
times the mass of Jupiter at 1–2 AU distance, and down to around 4 times the mass of Jupiter at 10 AU distance.

Sirius A has a mass of . The radius of this star has been measured by an astronomical interferometer, giving an estimated angular diameter of 5.936±0.016 mas. The projected rotational velocity is a relatively low 16 km/s, which does not produce any significant flattening of its disk. This is at marked variance with the similar-sized Vega, which rotates at a much faster 274 km/s and bulges prominently around its equator. A weak magnetic field has been detected on the surface of Sirius A.

Stellar models suggest that the star formed during the collapsing of a molecular cloud and that, after years, its internal energy generation was derived entirely from nuclear reactions. The core became convective and used the CNO cycle for energy generation. It is predicted that Sirius A will have completely exhausted the store of hydrogen at its core within a billion (10) years of its formation. At this point, it will pass through a red giant stage, then settle down to become a white dwarf.

Sirius A is classed as an Am star because the spectrum shows deep metallic absorption lines, indicating an enhancement in elements heavier than helium, such as iron. The spectral type has been reported as A0mA1 Va, which indicates that it would be classified as A1 from hydrogen and helium lines, but A0 from the metallic lines that cause it to be grouped with the Am stars. When compared to the Sun, the proportion of iron in the atmosphere of Sirius A relative to hydrogen is given by formula_1, meaning iron is 316% as abundant as in the Sun's atmosphere. The high surface content of metallic elements is unlikely to be true of the entire star; rather the iron-peak and heavy metals are radiatively levitated towards the surface.

Sirius B is one of the most massive white dwarfs known. With a mass of , it is almost double the average. This mass is packed into a volume roughly equal to the Earth's. The current surface temperature is 25,200 K. Because there is no internal heat source, Sirius B will steadily cool as the remaining heat is radiated into space over more than two billion years.

A white dwarf forms after a star has evolved from the main sequence and then passed through a red giant stage. This occurred when Sirius B was less than half its current age, around 120 million years ago. The original star had an estimated and was a B-type star (roughly B4–5) when it was still on the main sequence. While it passed through the red giant stage, Sirius B may have enriched the metallicity of its companion.

This star is primarily composed of a carbon–oxygen mixture that was generated by helium fusion in the progenitor star. This is overlaid by an envelope of lighter elements, with the materials segregated by mass because of the high surface gravity. The outer atmosphere of Sirius B is now almost pure hydrogen—the element with the lowest mass—and no other elements are seen in its spectrum.

Since 1894, irregularities have been observed in the orbits of Sirius A and B with an apparent periodicity of 6–6.4 years. A 1995 study concluded that such a companion likely exists, with a mass of roughly 0.05 solar masses- a small red dwarf or large brown dwarf, with an apparent magnitude of >15, and less than 3 arcseconds from Sirius A.

More recent (and accurate) astrometric observations by the Hubble Space Telescope ruled out the existence of such a Sirius C entirely. The 1995 study predicted an astrometric movement of roughly 90 mas (0.09 arcseconds), but Hubble was unable to detect any location anomaly to an accuracy of 5 mas (0.005 arcsec). This ruled out any objects orbiting Sirius A with more than 0.033 solar masses orbiting in 0.5 years, and 0.014 in 2 years. The study was also able to rule out any companions to Sirius B with more than 0.024 solar masses orbiting in 0.5 years, and 0.0095 orbiting in 1.8 years. Effectively, there are almost certainly no additional bodies in the Sirius system larger than a small brown dwarf or large exoplanet.

In 1909, Ejnar Hertzsprung was the first to suggest that Sirius was a member of the Ursa Major Moving Group, based on his observations of the system's movements across the sky. The Ursa Major Group is a set of 220 stars that share a common motion through space. It was once a member of an open cluster, but has since become gravitationally unbound from the cluster. Analyses in 2003 and 2005 found Sirius's membership in the group to be questionable: the Ursa Major Group has an estimated age of 500 ± 100 million years, whereas Sirius, with metallicity similar to the Sun's, has an age that is only half this, making it too young to belong to the group. Sirius may instead be a member of the proposed Sirius Supercluster, along with other scattered stars such as Beta Aurigae, Alpha Coronae Borealis, Beta Crateris, Beta Eridani and Beta Serpentis. This would be one of three large clusters located within of the Sun. The other two are the Hyades and the Pleiades, and each of these clusters consists of hundreds of stars.

In 2017, a massive star cluster was discovered only 10′ from Sirius. It was discovered during a statistical analysis of Gaia data. The cluster is over a thousand times further away from us than the star system.

The proper name "Sirius" comes from the Latin "Sīrius", from the Ancient Greek "Σείριος" ("Seirios", "glowing" or "scorcher"). The Greek word itself may have been imported from elsewhere before the Archaic period, one authority suggesting a link with the Egyptian god Osiris. The name's earliest recorded use dates from the 7th century BC in Hesiod's poetic work "Works and Days". In 2016, the International Astronomical Union organized a Working Group on Star Names (WGSN) to catalog and standardize proper names for stars. The WGSN's first bulletin of July 2016 included a table of the first two batches of names approved by the WGSN; which included "Sirius" for the star α Canis Majoris A. It is now so entered in the IAU Catalog of Star Names.

Sirius has over 50 other designations and names attached to it. In Geoffrey Chaucer's essay "Treatise on the Astrolabe", it bears the name Alhabor and is depicted by a hound's head. This name is widely used on medieval astrolabes from Western Europe. In Sanskrit it is known as "Mrgavyadha" "deer hunter", or "Lubdhaka" "hunter". As Mrgavyadha, the star represents Rudra (Shiva). The star is referred as "Makarajyoti" in Malayalam and has religious significance to the pilgrim center Sabarimala. In Scandinavia, the star has been known as "Lokabrenna" ("burning done by Loki", or "Loki's torch"). In the astrology of the Middle Ages, Sirius was a Behenian fixed star, associated with beryl and juniper. Its astrological symbol was listed by Heinrich Cornelius Agrippa.

Many cultures have historically attached special significance to Sirius, particularly in relation to dogs. It is often colloquially called the "Dog Star" as the brightest star of Canis Major, the "Great Dog" constellation. Canis Major was classically depicted as Orion's dog. The Ancient Greeks thought that Sirius's emanations could affect dogs adversely, making them behave abnormally during the "dog days", the hottest days of the summer. The Romans knew these days as , and the star Sirius was called Canicula, "little dog". The excessive panting of dogs in hot weather was thought to place them at risk of desiccation and disease. In extreme cases, a foaming dog might have rabies, which could infect and kill humans they had bitten. Homer, in the "Iliad", describes the approach of Achilles toward Troy in these words:
In Iranian mythology, especially in Persian mythology and in Zoroastrianism, the ancient religion of Persia, Sirius appears as "Tishtrya" and is revered as the rain-maker divinity (Tishtar of New Persian poetry). Beside passages in the sacred texts of the Avesta, the Avestan language "Tishtrya" followed by the version "Tir" in Middle and New Persian is also depicted in the Persian epic Shahnameh of Ferdowsi. Due to the concept of the yazatas, powers which are "worthy of worship", Tishtrya is a divinity of rain and fertility and an antagonist of apaosha, the demon of drought. In this struggle, Tishtrya is depicted as a white horse.

In Chinese astronomy Sirius is known as the star of the "celestial wolf" ( Chinese romanization: Tiānláng; Japanese romanization: Tenrō;) in the Mansion of Jǐng (井宿). Many nations among the indigenous peoples of North America also associated Sirius with canines; the Seri and Tohono O'odham of the southwest note the star as a dog that follows mountain sheep, while the Blackfoot called it "Dog-face". The Cherokee paired Sirius with Antares as a dog-star guardian of either end of the "Path of Souls". The Pawnee of Nebraska had several associations; the Wolf (Skidi) tribe knew it as the "Wolf Star", while other branches knew it as the "Coyote Star". Further north, the Alaskan Inuit of the Bering Strait called it "Moon Dog".

Several cultures also associated the star with a bow and arrows. The ancient Chinese visualized a large bow and arrow across the southern sky, formed by the constellations of Puppis and Canis Major. In this, the arrow tip is pointed at the wolf Sirius. A similar association is depicted at the Temple of Hathor in Dendera, where the goddess Satet has drawn her arrow at Hathor (Sirius). Known as "Tir", the star was portrayed as the arrow itself in later Persian culture.

Sirius is mentioned in "Surah", "An-Najm" ("The Star"), of the Qur'an, where it is given the name (transliteration: "aš-ši‘rā" or "ash-shira"; the leader). The verse is: "", "That He is the Lord of Sirius (the Mighty Star)." (An-Najm:49) Ibn Kathir said in his commentary "that it is the bright star, named Mirzam Al-Jawza' (Sirius), which a group of Arabs used to worship". The alternate name "Aschere", used by Johann Bayer, is derived from this.

In theosophy, it is believed the "Seven Stars of the Pleiades" transmit the spiritual energy of the Seven Rays from the "Galactic Logos" to the "Seven Stars of the Great Bear", then to Sirius. From there is it sent via the Sun to the god of Earth (Sanat Kumara), and finally through the seven Masters of the Seven Rays to the human race.

The Dogon people are an ethnic group in Mali, West Africa, reported by some researchers to have traditional astronomical knowledge about Sirius that would normally be considered impossible without the use of telescopes. According to Marcel Griaule, they knew about the fifty-year orbital period of Sirius and its companion prior to western astronomers. In his pseudoarcheology book "The Sirius Mystery", Robert Temple claimed that the Dogon people have a tradition of contact with intelligent extraterrestrial beings from Sirius.

Doubts have been raised about the validity of Griaule and Dieterlein's work. In 1991, anthropologist Walter van Beek concluded about the Dogon, "Though they do speak about "sigu tolo" [which is what Griaule claimed the Dogon called Sirius] they disagree completely with each other as to which star is meant; for some it is an invisible star that should rise to announce the "sigu" [festival], for another it is Venus that, through a different position, appears as "sigu tolo". All agree, however, that they learned about the star from Griaule."

Noah Brosch claims that the cultural transfer of relatively modern astronomical information could have taken place in 1893, when a French expedition arrived in Central West Africa to observe the total eclipse on April 16.

In the religion of the Serer people of Senegal, the Gambia and Mauritania, Sirius is called "Yoonir" from the Serer language (and some of the Cangin language speakers, who are all ethnically Serers). The star Sirius is one of the most important and sacred stars in Serer religious cosmology and symbolism. The Serer high priests and priestesses (Saltigues, the hereditary "rain priests") chart "Yoonir" in order to forecast rainfall and enable Serer farmers to start planting seeds. In Serer religious cosmology, it is the symbol of the universe.

Sirius is a frequent subject of science fiction, and has been the subject of poetry. Dante and John Milton reference the star, and it is the "powerful western fallen star" of Walt Whitman's "Then Lilacs Last in the Dooryard Bloom'd", while Tennyson's poem "The Princess" describes the star's scintillation:
Other modern references:
Vehicles:




</doc>
<doc id="28018" url="https://en.wikipedia.org/wiki?curid=28018" title="Simon Magus">
Simon Magus

Simon Magus (Greek Σίμων ὁ μάγος, Latin: Simon Magus), also known as Simon the Sorcerer or Simon the Magician, was a religious figure whose confrontation with Peter is recorded in Acts . The act of simony, or paying for position and influence in the church, is named after Simon.

According to Acts, Simon was a Samaritan magus or religious figure of the 1st century AD and a convert to Christianity, baptised by Philip the Evangelist. Simon later clashed with Peter. Accounts of Simon by writers of the second century exist, but are not considered verifiable. Surviving traditions about Simon appear in orthodox texts, such as those of Irenaeus, Justin Martyr, Hippolytus, and Epiphanius, where he is often described as the founder of Gnosticism, which has been accepted by some modern scholars, while others reject that he was a Gnostic, just designated as one by the Church Fathers.

Justin, who was himself a 2nd-century native of Samaria, wrote that nearly all the Samaritans in his time were adherents of a certain Simon of Gitta, a village not far from Flavia Neapolis. According to Josephus, Gitta (also spelled Getta) was settled by the tribe of Dan. Irenaeus held him as being the founder of the sect of the Simonians. Hippolytus quotes from a work he attributes to Simon or his followers the Simonians, "Apophasis Megale", or "Great Declaration". According to the early church heresiologists, Simon is also supposed to have written several lost treatises, two of which bear the titles "The Four Quarters of the World" and "The Sermons of the Refuter".

In apocryphal works including the "Acts of Peter", Pseudo-Clementines, and the "Epistle of the Apostles", Simon also appears as a formidable sorcerer with the ability to levitate and fly at will. He is sometimes referred to as "the Bad Samaritan" due to his malevolent character. The "Apostolic Constitutions" also accuses him of "lawlessness" (antinomianism).

The canonical Acts of the Apostles features a short narrative about Simon Magus; this is his only appearance in the New Testament.

Josephus mentions a magician named [Atomus] (Simon in Latin manuscripts) as being involved with the procurator Felix, King Agrippa II and his sister Drusilla, where Felix has Simon convince Drusilla to marry him instead of the man she was engaged to. Some scholars have considered the two to be identical, although this is not generally accepted, as the Simon of Josephus is a Jew rather than a Samaritan.

Justin Martyr (in his "Apologies", and in a lost work against heresies, which Irenaeus used as his main source) and Irenaeus ("Adversus Haereses") record that after being cast out by the Apostles, Simon Magus came to Rome where, having joined to himself a profligate woman of the name of Helen, he gave out that it was he who appeared among the Jews as the Son, in Samaria as the Father and among other nations as the Holy Spirit. He performed such signs by magic acts during the reign of Claudius that he was regarded as a god and honored with a statue on the island in the Tiber which the two bridges cross, with the inscription "Simoni Deo Sancto", "To Simon the Holy God" (). However, in the 16th century, a statue was unearthed on the island in question, inscribed to Semo Sancus, a Sabine deity, leading most scholars to believe that Justin Martyr confused "Semoni Sancus" with Simon.

Justin and Irenaeus are the first to recount the myth of Simon and Helen, which became the center of Simonian doctrine. Epiphanius of Salamis also makes Simon speak in the first person in several places in his "Panarion", and the implication is that he is quoting from a version of it, though perhaps not verbatim.

As described by Epiphanius, in the beginning God had his first thought, his "Ennoia", which was female, and that thought was to create the angels. The First Thought then descended into the lower regions and created the angels. But the angels rebelled against her out of jealousy and created the world as her prison, imprisoning her in a female body. Thereafter, she was reincarnated many times, each time being shamed. Her many reincarnations included Helen of Troy, among others, and she finally was reincarnated as Helen, a slave and prostitute in the Phoenician city of Tyre. God then descended in the form of Simon Magus, to rescue his "Ennoia", and to confer salvation upon men through knowledge of himself.

For as the angels were mismanaging the world, owing to their individual lust for rule, he had come to set things straight, and had descended under a changed form, likening himself to the Principalities and Powers through whom he passed, so that among men he appeared as a man, though he was not a man, and was thought to have suffered in Judaea, though he had not suffered.

But the prophets had delivered their prophecies under the inspiration of the world-creating angels: wherefore those who had their hope in him and in Helen minded them no more, and, as being free, did what they pleased; for men were saved according to his grace, but not according to just works. For works were not just by nature, but only by convention, in accordance with the enactments of the world-creating angels, who by precepts of this kind sought to bring men into slavery. Wherefore he promised that the world should be dissolved, and that those who were his should be freed from the dominion of the world-creators.

In this account of Simon there is a large portion common to almost all forms of Gnostic myths, together with something special to this form. They have in common the place in the work of creation assigned to the female principle, the conception of the Deity; the ignorance of the rulers of this lower world with regard to the Supreme Power; the descent of the female (Sophia) into the lower regions, and her inability to return. Special to the Simonian tale is the identification of Simon himself with the Supreme, and of his consort Helena with the female principle.

In "Philosophumena", Hippolytus retells the narrative on Simon written by Irenaeus (who in his turn based it on the lost "Syntagma" of Justin). Upon the story of "the lost sheep," Hippolytus comments as follows:

Also, Hippolytus demonstrates acquaintance with the folk tradition on Simon which depicts him rather as a magician than Gnostic, and in constant conflict with Peter (also present in the apocrypha and Pseudo-Clementine literature). Reduced to despair by the curse laid upon him by Peter in the Acts, Simon soon abjured the faith and embarked on the career of a sorcerer:

Hippolytus gives a much more doctrinally detailed account of Simonianism, including a system of divine emanations and interpretations of the Old Testament, with extensive quotations from the "Apophasis Megale". Some believe that Hippolytus' account is of a later, more developed form of Simonianism, and that the original doctrines of the group were simpler, close to the account given by Justin Martyr and Irenaeus (this account however is also included in Hippolytus' work).

Hippolytus says the free love doctrine was held by them in its purest form, and speaks in language similar to that of Irenaeus about the variety of magic arts practiced by the Simonians, and also of their having images of Simon and Helen under the forms of Zeus and Athena. But he also adds, "if any one, on seeing the images either of Simon or Helen, shall call them by those names, he is cast out, as showing ignorance of the mysteries."

Epiphanius writes that there were some Simonians still in existence in his day (c. AD 367), but he speaks of them as almost extinct. Gitta, he says, had sunk from a town into a village. Epiphanius further charges Simon with having tried to wrest the words of St. Paul about the armour of God (Ephesians 6:14–16) into agreement with his own identification of the "Ennoia" with Athena. He tells us also that he gave barbaric names to the "principalities and powers," and that he was the beginning of the Gnostics. The Law, according to him, was not of God, but of "the sinister power." The same was the case with the prophets, and it was death to believe in the Old Testament.

Cyril of Jerusalem (346 AD) in the sixth of his Catechetical Lectures prefaces his history of the Manichaeans by a brief account of earlier heresies: Simon Magus, he says, had given out that he was going to be translated to heaven, and was actually careening through the air in a chariot drawn by demons when Peter and Paul knelt down and prayed, and their prayers brought him to earth a mangled corpse.

The apocryphal "Acts of Peter" gives a more elaborate tale of Simon Magus' death. Simon is performing magic in the Forum, and in order to prove himself to be a god, he levitates up into the air above the Forum. The apostle Peter prays to God to stop his flying, and he stops mid-air and falls into a place called "the "Sacra Via"" (meaning "Holy Way" in Latin), breaking his legs "in three parts". The previously non-hostile crowd then stones him. Now gravely injured, he had some people carry him on a bed at night from Rome to Ariccia, and was brought from there to Terracina to a person named Castor, who on accusations of sorcery was banished from Rome. The Acts then continue to say that he died "while being sorely cut by two physicians".

Another apocryphal document, the "Acts of Peter and Paul" gives a slightly different version of the above incident, which was shown in the context of a debate in front of the Emperor Nero. In this version, Paul the Apostle is present along with Peter, Simon levitates from a high wooden tower made upon his request, and dies "divided into four parts" due to the fall. Peter and Paul were then put in prison by Nero while ordering Simon's body be kept carefully for three days (thinking he would rise again).

The Pseudo-Clementine "Recognitions" and "Homilies" give an account of Simon Magus and some of his teachings in regards to the Simonians. They are of uncertain date and authorship, and seem to have been worked over by several hands in the interest of diverse forms of belief.

Simon was a Samaritan, and a native of Gitta. The name of his father was Antonius, that of his mother Rachel. He studied Greek literature in Alexandria, and, having in addition to this great power in magic, became so ambitious that he wished to be considered a highest power, higher even than the God who created the world. And sometimes he "darkly hinted" that he himself was Christ, calling himself the Standing One. Which name he used to indicate that he would stand for ever, and had no cause in him for bodily decay. He did not believe that the God who created the world was the highest, nor that the dead would rise. He denied Jerusalem, and introduced Mount Gerizim in its stead. In place of the Christ of the Christians he proclaimed himself; and the Law he allegorized in accordance with his own preconceptions. He did indeed preach righteousness and judgment to come.

There was one John the Baptist, who was the forerunner of Jesus in accordance with the law of parity; and as Jesus had twelve Apostles, bearing the number of the twelve solar months, so had he thirty leading men, making up the monthly tale of the moon. One of these thirty leading men was a woman called Helen, and the first and most esteemed by John was Simon. But on the death of John he was away in Egypt for the practice of magic, and one Dositheus, by spreading a false report of Simon's death, succeeded in installing himself as head of the sect. Simon on coming back thought it better to dissemble, and, pretending friendship for Dositheus, accepted the second place. Soon, however, he began to hint to the thirty that Dositheus was not as well acquainted as he might be with the doctrines of the school.

The encounter between Dositheus and Simon Magus was the beginnings of the sect of Simonians. The narrative goes on to say that Simon, having fallen in love with Helen, took her about with him, saying that she had come down into the world from the highest heavens, and was his mistress, inasmuch as she was Sophia, the Mother of All. It was for her sake, he said, that the Greeks and Barbarians fought the Trojan War, deluding themselves with an image of truth, for the real being was then present with the First God. By such allegories Simon deceived many, while at the same time he astounded them by his magic. A description is given of how he made a familiar spirit for himself by conjuring the soul out of a boy and keeping his image in his bedroom, and many instances of his feats of magic are given.

The Pseudo-Clementine writings were used in the 4th century by members of the Ebionite sect, one characteristic of which was hostility to Paul, whom they refused to recognize as an apostle. Ferdinand Christian Baur (1792–1860), founder of the Tübingen School, drew attention to the anti-Pauline characteristic in the Pseudo-Clementines, and pointed out that in the disputations between Simon and Peter, some of the claims Simon is represented as making (e.g. that of having seen the Lord, though not in his lifetime, yet subsequently in vision) were really the claims of Paul; and urged that Peter's refutation of Simon was in some places intended as a polemic against Paul. The enmity between Peter and Simon is clearly shown. Simon's magical powers are juxtaposed with Peter's powers in order to express Peter's authority over Simon through the power of prayer, and in the the identification of Paul with Simon Magus is effected. Simon is there made to maintain that he has a better knowledge of the mind of Jesus than the disciples, who had seen and conversed with Jesus in person. His reason for this strange assertion is that visions are superior to waking reality, as divine is superior to human. Peter has much to say in reply to this, but the passage which mainly concerns us is as follows:

The anti-Pauline context of the Pseudo-Clementines is recognised, but the association with Simon Magus is surprising, according to Jozef Verheyden, since they have little in common. However the majority of scholars accept Baur's identification, though others, including Lightfoot, argued extensively that the "Simon Magus" of the Pseudo-Clementines was not meant to stand for Paul. Recently, Berlin pastor Hermann Detering (1995) has made the case that the veiled anti-Pauline stance of the Pseudo-Clementines has historical roots, that the Acts 8 encounter between Simon the magician and Peter is itself based on the conflict between Peter and Paul. Detering's belief has not found general support among scholars, but Robert M. Price argues much the same case in "The Amazing Colossal Apostle:The Search for the Historical Paul" (2012).

There are other features in the portrait which are reminiscent of Marcion. The first thing mentioned in the "Homilies" about Simon's opinions is that he denied that God was just.
By "God" he meant the creator god. But he undertakes to prove from the Jewish scriptures that there is a higher god, who really possesses the perfections which are falsely ascribed to the lower god.
On these grounds Peter complains that, when he was setting out for the gentiles to convert them from their worship of "many gods upon earth", Satan had sent Simon before him to make them believe that there were "many gods in heaven".

In Irish legend Simon Magus came to be associated with Druidism. He is said to have come to the aid of the Druid Mog Ruith. The fierce denunciation of Christianity by Irish Druids appears to have resulted in Simon Magus being associated with Druidism. The word Druid was sometimes translated into Latin as "magus", and Simon Magus was also known in Ireland as "Simon the Druid".
The church of Santa Francesca Romana, Rome, is claimed to have been built on the spot where Simon fell. Within the Church is a dented slab of marble that purports to bear the imprints of the knees of Peter and Paul during their prayer. The fantastic stories of Simon the Sorcerer persisted into the later Middle Ages, becoming a possible inspiration for the "Faustbuch" and Goethe's Faust.

The opening story in Danilo Kiš's 1983 collection "The Encyclopedia of the Dead", "Simon Magus", retells the confrontation between Simon and Peter agreeing with the account in the "Acts of Peter", and provides an additional alternative ending in which Simon asks to be buried alive in order to be resurrected three days later (after which his body is found putrefied).



</doc>
<doc id="28020" url="https://en.wikipedia.org/wiki?curid=28020" title="September 10">
September 10





</doc>
<doc id="28021" url="https://en.wikipedia.org/wiki?curid=28021" title="September 12">
September 12





</doc>
<doc id="28022" url="https://en.wikipedia.org/wiki?curid=28022" title="School">
School

A school is an educational institution designed to provide learning spaces and learning environments for the teaching of students (or "pupils") under the direction of teachers. Most countries have systems of formal education, which is commonly compulsory. In these systems, students progress through a series of schools. The names for these schools vary by country (discussed in the "Regional" section below) but generally include primary school for young children and secondary school for teenagers who have completed primary education. An institution where higher education is taught, is commonly called a university college or university, but these higher education institutions are usually not compulsory.

In addition to these core schools, students in a given country may also attend schools before and after primary (Elementary in the US) and secondary (Middle school in the US) education. Kindergarten or preschool provide some schooling to very young children (typically ages 3–5). University, vocational school, college or seminary may be available after secondary school. A school may be dedicated to one particular field, such as a school of economics or a school of dance. Alternative schools may provide nontraditional curriculum and methods.

Non-government schools, also known as private schools may be required when the government does not supply adequate, or specific educational needs. Other private schools can also be religious, such as Christian schools, Gurukula,Hindu School, madrasa, hawzas (Shi'a schools), yeshivas (Jewish schools), and others; or schools that have a higher standard of education or seek to foster other personal achievements. Schools for adults include institutions of corporate training, military education and training and business schools.

In home schooling and online schools, teaching and learning take place outside a traditional school building. Schools are commonly organized in several different organizational models, including departmental, small learning communities, academies, integrated, and schools-within-a-school.

The word "school" derives from Greek " ("), originally meaning "leisure" and also "that in which leisure is employed", but later "a group to whom lectures were given, school".

The concept of grouping students together in a centralized location for learning has existed since Classical antiquity. Formal schools have existed at least since ancient Greece (see Academy), ancient Rome (see Education in Ancient Rome) ancient India (see Gurukul), and ancient China (see History of education in China). The Byzantine Empire had an established schooling system beginning at the primary level. According to "Traditions and Encounters", the founding of the primary education system began in 425 AD and "... military personnel usually had at least a primary education ...". The sometimes efficient and often large government of the Empire meant that educated citizens were a must. Although Byzantium lost much of the grandeur of Roman culture and extravagance in the process of surviving, the Empire emphasized efficiency in its war manuals. The Byzantine education system continued until the empire's collapse in 1453 AD.

In Western Europe a considerable number of cathedral schools were founded during the Early Middle Ages in order to teach future clergy and administrators, with the oldest still existing, and continuously operated, cathedral schools being The King's School, Canterbury (established 597 CE), King's School, Rochester (established 604 CE), St Peter's School, York (established 627 CE) and Thetford Grammar School (established 631 CE). Beginning in the 5th century CE monastic schools were also established throughout Western Europe, teaching both religious and secular subjects.

Islam was another culture that developed a school system in the modern sense of the word. Emphasis was put on knowledge, which required a systematic way of teaching and spreading knowledge, and purpose-built structures. At first, mosques combined both religious performance and learning activities, but by the 9th century, the madrassa was introduced, a school that was built independently from the mosque, such as al-Qarawiyyin, founded in 859 CE. They were also the first to make the "Madrassa" system a public domain under the control of the Caliph.

Under the Ottomans, the towns of Bursa and Edirne became the main centers of learning. The Ottoman system of Külliye, a building complex containing a mosque, a hospital, madrassa, and public kitchen and dining areas, revolutionized the education system, making learning accessible to a wider public through its free meals, health care and sometimes free accommodation.
In Europe, universities emerged during the 12th century; here, scholasticism was an important tool, and the academicians were called "schoolmen". During the Middle Ages and much of the Early Modern period, the main purpose of schools (as opposed to universities) was to teach the Latin language. This led to the term grammar school, which in the United States informally refers to a primary school, but in the United Kingdom means a school that selects entrants based on ability or aptitude. Following this, the school curriculum has gradually broadened to include literacy in the vernacular language as well as technical, artistic, scientific and practical subjects.
Obligatory school attendance became common in parts of Europe during the 18th century. In Denmark-Norway, this was introduced as early as in 1739–1741, the primary end being to increase the literacy of the "", i.e. the "regular people". Many of the earlier public schools in the United States and elsewhere were one-room schools where a single teacher taught seven grades of boys and girls in the same classroom. Beginning in the 1920s, one-room schools were consolidated into multiple classroom facilities with transportation increasingly provided by kid hacks and school buses.

The use of the term "school" varies by country, as do the names of the various levels of education within the country.

In the United Kingdom, the term "school" refers primarily to pre-university institutions, and these can, for the most part, be divided into pre-schools or nursery schools, primary schools (sometimes further divided into infant school and junior school), and secondary schools. Various types of secondary schools in England and Wales include grammar schools, comprehensives, secondary moderns, and city academies. In Scotland, while they may have different names, there is only one type of secondary school, although they may be funded either by the state or independently funded. School performance in Scotland is monitored by Her Majesty's Inspectorate of Education. Ofsted reports on performance in England and Estyn reports on performance in Wales.

In the United Kingdom, most schools are publicly funded and known as state schools or maintained schools in which tuition is provided for free. There are also private schools or independent schools that charge fees. Some of the most selective and expensive private schools are known as public schools, a usage that can be confusing to speakers of North American English. In North American usage, a public school is one that is publicly funded or run.

In much of the Commonwealth of Nations, including Australia, New Zealand, India, Pakistan, Bangladesh, Sri Lanka, South Africa, Kenya, and Tanzania, the term "school" refers primarily to pre-university institutions.

In ancient India, schools were in the form of Gurukuls. Gurukuls were traditional Hindu residential schools of learning; typically the teacher's house or a monastery. During the Mughal rule, Madrasahs were introduced in India to educate the children of Muslim parents. British records show that indigenous education was widespread in the 18th century, with a school for every temple, mosque or village in most regions of the country. The subjects taught included Reading, Writing, Arithmetic, Theology, Law, Astronomy, Metaphysics, Ethics, Medical Science and Religion.

Under the British rule in India, Christian missionaries from England, USA and other countries established missionary and boarding schools throughout the country. Later as these schools gained in popularity, more were started and some gained prestige. These schools marked the beginning of modern schooling in India and the syllabus and calendar they followed became the benchmark for schools in modern India. Today most of the schools follow the missionary school model in terms of tutoring, subject / syllabus, governance etc.with minor changes. Schools in India range from schools with large campuses with thousands of students and hefty fees to schools where children are taught under a tree with a small / no campus and are totally free of cost. There are various boards of schools in India, namely Central Board for Secondary Education (CBSE), Council for the Indian School Certificate Examinations (CISCE), Madrasa Boards of various states, Matriculation Boards of various states, State Boards of various boards, Anglo Indian Board, and so on. The typical syllabus today includes Language(s), Mathematics, Science – Physics, Chemistry, Biology, Geography, History, General Knowledge, Information Technology / Computer Science etc.. Extra curricular activities include physical education / sports and cultural activities like music, choreography, painting, theater / drama etc.

In much of continental Europe, the term "school" usually applies to primary education, with primary schools that last between four and nine years, depending on the country. It also applies to secondary education, with secondary schools often divided between "Gymnasiums" and vocational schools, which again depending on country and type of school educate students for between three and six years. In Germany students graduating from Grundschule are not allowed to directly progress into a vocational school, but are supposed to proceed to one of Germany's general education schools such as Gesamtschule, Hauptschule, Realschule or Gymnasium. When they leave that school, which usually happens at age 15–19 they are allowed to proceed to a vocational school. The term school is rarely used for tertiary education, except for some "upper" or "high" schools (German: Hochschule), which describe colleges and universities.

In Eastern Europe modern schools (after World War II), of both primary and secondary educations, often are combined, while secondary education might be split into accomplished or not. The schools are classified as middle schools of general education and for the technical purposes include "degrees" of the education they provide out of three available: the first – primary, the second – unaccomplished secondary, and the third – accomplished secondary. Usually the first two degrees of education (eight years) are always included, while the last one (two years) gives option for the students to pursue vocational or specialized educations.

In North America, the term "school" can refer to any educational institution at any level, and covers all of the following: preschool (for toddlers), kindergarten, elementary school, middle school (also called intermediate school or junior high school, depending on specific age groups and geographic region), high school (or in some cases senior high school), college, university, and graduate school.

In the United States, school performance through high school is monitored by each state's department of education. Charter schools are publicly funded elementary or secondary schools that have been freed from some of the rules, regulations, and statutes that apply to other public schools. The terms grammar school and "grade school" are sometimes used to refer to a primary school. In addition, there are tax-funded magnet schools which offer different programs and instruction not available in traditional schools.

In Western Africa, the term school can also refer to "bush" schools, Quranic schools, or apprenticeships. These schools include formal and informal learning.

Bush schools are training camps that pass down cultural skills, traditions, and knowledge to their students. Bush schools are semi similar to traditional western schools because they are separated from the larger community. These schools are located in forests outside of the towns and villages, and the space used is solely for these schools. Once the students have arrived in the forest, they are not allowed to leave until their training is complete. Visitors are absolutely prohibited from these areas.
Instead of being separated by age, Bush schools are separated by gender. Women and girls are not allowed to enter the territory of the boys' bush school and vice versa. Boys receive training in cultural crafts, fighting, hunting, and community laws among other subjects. Girls are trained in their own version of the boys' bush school. They practice domestic affairs such as cooking, childcare, as well as how to be a good wife. Their training is focused on how to be a proper woman by societal standards.

Qur’anic schools are the principle way of teaching the Quran and knowledge of the Islamic faith. These schools also fostered literacy and writing during the time of colonization. Today, the emphasis is on the different levels of reading, memorizing, and reciting the Quran. Attending a Qur’anic school is how children become recognized members of the Islamic faith. Children often attend state schools and a Qur’anic school.

In Mozambique, specifically, there are two kinds of Qur’anic schools. They are the tariqa based and the Wahhabi-based schools. What makes these schools different is who controls them. Tariqa schools are controlled at the local level while the Wahhabi are controlled by the Islamic Council. Within the Qur’anic school system, there are levels of education. They range from a basic level of understanding, called chuo and kioni in local languages, to the most advanced which is called ilimu.

In Nigeria, the term "school" broadly covers daycares, nursery schools, primary schools, secondary schools and tertiary institutions. Primary and secondary schools are either privately funded by religious institutions and corporate organisations, or government-funded. Government-funded schools are commonly referred to as public schools. Students spend 6 years in primary school, 3 years in junior secondary school and 3 years in senior secondary school. The first 9 years of formal schooling is compulsory under the Universal Basic Education Program (UBEC). Tertiary institutions include public and private universities, polytechnics, and colleges of education. Universities can be funded by the federal government, state governments, religious institutions or individuals and organisations.

Many schools are owned or funded by states. Private schools operate independently from the government. Private schools usually rely on fees from families whose children attend the school for funding; however, sometimes such schools also receive government support (for example, through School vouchers). Many private schools are affiliated with a particular religion; these are known as parochial schools.

Schools are organized spaces purposed for teaching and learning. The classrooms, where teachers teach and students learn, are of central importance. Classrooms may be specialized for certain subjects, such as laboratory classrooms for science education and workshops for industrial arts education.

Typical schools have many other rooms and areas, which may include:

In low-income countries, only 32% of primary, 43% of lower secondary and 52% of upper secondary schools have access to electricity. This affects access to the internet, which is just 37% in upper secondary schools in low-income countries, as compared to 59% in those in middle-income countries and 93% in those in high-income countries.

Access to basic water, sanitation and hygiene is also far from universal. Among upper secondary schools, only 53% in low-income countries and 84% in middle-income countries have access to basic drinking water. Access to water and sanitation is universal in high-income countries.

The safety of staff and students is increasingly becoming an issue for school communities, an issue most schools are addressing through improved security. Some have also taken measures such as installing metal detectors or video surveillance. Others have even taken measures such as having the children swipe identification cards as they board the school bus. For some schools, these plans have included the use of door numbering to aid public safety response.

Other security concerns faced by schools include bomb threats, gangs, and vandalism.

School health services are services from medical, teaching and other professionals applied in or out of school to improve the health and well-being of children and in some cases whole families. These services have been developed in different ways around the globe but the fundamentals are constant: the early detection, correction, prevention or amelioration of disease, disability and abuse from which school-aged children can suffer.

Some schools offer remote access to their classes over the Internet. Online schools also can provide support to traditional schools, as in the case of the School Net Namibia.
Some online classes also provide experience in a class, so that when people take them, they have already been introduced to the subject and know what to expect, and even more classes provide High School/College credit allowing people to take the classes at their own pace. Many online classes cost money to take but some are offered free.

Internet-based distance learning programs are offered widely through many universities. Instructors teach through online activities and assignments. Online classes are taught the same as physically being in class with the same curriculum. The instructor offers the syllabus with their fixed requirements like any other class. Students can virtually turn their assignments in to their instructors according to deadlines. This being through via email or in the course webpage. This allowing students to work at their own pace, yet meeting the correct deadline. Students taking an online class have more flexibility in their schedules to take their classes at a time that works best for them. Conflicts with taking an online class may include not being face to face with the instructor when learning or being in an environment with other students. Online classes can also make understanding the content difficult, especially when not able to get in quick contact with the instructor. Online students do have the advantage of using other online sources with assignments or exams for that specific class. Online classes also have the advantage of students not needing to leave their house for a morning class or worrying about their attendance for that class. Students can work at their own pace to learn and achieve within that curriculum.

The convenience of learning at home has been a major attractive point for enrolling online. Students can attend class anywhere a computer can go – at home, a library or while traveling internationally. Online school classes are designed to fit your needs, while allowing you to continue working and tending to your other obligations. Online school education is divided into three subcategories: Online Elementary School, Online Middle School, Online High school.

As a profession, teaching has levels of work-related stress (WRS) that are among the highest of any profession in some countries, such as the United Kingdom and the United States. The degree of this problem is becoming increasingly recognized and support systems are being put into place.

Stress sometimes affects students more severely than teachers, up to the point where the students are prescribed stress medication. This stress is claimed to be related to standardized testing, and the pressure on students to score above average. "See Cram school".
According to a 2008 mental health study by the Associated Press and mtvU, eight in 10 college students said they had sometimes or frequently experienced stress in their daily lives. This was an increase of 20% from a survey five years previously. 34 percent had felt depressed at some point in the past three months, 13 percent had been diagnosed with a mental health condition such as an anxiety disorder or depression, and 9 percent had seriously considered suicide.

Schools and their teachers have always been under pressure – for instance, pressure to cover the curriculum, to perform well in comparison to other schools, and to avoid the stigma of being "soft" or "spoiling" toward students. Forms of discipline, such as control over when students may speak, and normalized behaviour, such as raising a hand to speak, are imposed in the name of greater efficiency. Practitioners of critical pedagogy maintain that such disciplinary measures have no positive effect on student learning. Indeed, some argue that disciplinary practices detract from learning, saying that they undermine students' individual dignity and sense of self-worth – the latter occupying a more primary role in students' hierarchy of needs.




</doc>
<doc id="28024" url="https://en.wikipedia.org/wiki?curid=28024" title="Sontaran">
Sontaran

The Sontarans () ; are a fictional race of extraterrestrial humanoids created by Robert Holmes for the British science fiction television series "Doctor Who", also seen in spin-off series "The Sarah Jane Adventures". They are a warrior race characterised by their ruthlessness and fearlessness of death.

During rehearsals for their first appearance, Kevin Lindsay, who portrayed the original Sontaran, Linx, pronounced the race's name as ""son-TAR-an"." Alan Bromly, the director, tried to correct him by saying it should be pronounced with the stress on the first syllable. Lindsay declared "Well, I think it's ""son-TAR-an"", and since I'm from the place, I should know." His preferred pronunciation was retained.

The Sontarans made their first appearance in 1973 in the serial "The Time Warrior" by Robert Holmes, where Linx is stranded in the Middle Ages. Linx uses a projector to bring back human scientists from the future to fix his spacecraft.

Another Sontaran named Styre appears in "The Sontaran Experiment" (1975), experimenting on captured astronauts on a far future Earth. Their third appearance is in "The Invasion of Time", where they successfully invade Gallifrey, but are driven out again after less than a day. They appeared for the final time in the original series in "The Two Doctors". The Sontarans also appeared in a skit for the BBC children's programme "Jim'll Fix It" titled "A Fix with Sontarans", along with Colin Baker as the Sixth Doctor and Janet Fielding as Tegan Jovanka. References are made in Sontaran episodes to the Rutan Host, an equally militaristic race with whom the Sontarans have been at war for thousands of years though the Rutans were not shown until the 1977 serial "Horror of Fang Rock".

Sporting an updated design, Sontarans returned to the revived series in the Series 4 episodes "The Sontaran Strategem" and "The Poison Sky" (2008). The Sontarans plan to terraform the Earth into a new clone world, but their plans are averted by the Tenth Doctor (David Tennant). It is also revealed that the race was excluded from the Last Great Time War of the revived series' backstory. In "Turn Left" (2008), the same events are depicted in a parallel universe, where Rose Tyler (Billie Piper) describes their plan as foiled by Torchwood (characters from the spin-off show of that name), at the cost of their lives, with Torchwood leader Captain Jack Harkness (John Barrowman) being captured by the Sontarans. In "The Stolen Earth" (2008), UNIT is revealed to have developed a teleportation device based on Sontaran technology. A lone survivor from the events of "The Poison Sky", Commander Kaagh (Anthony O'Donnell), next appears in "The Last Sontaran" (2008), from spin-off series "The Sarah Jane Adventures". Kaagh appears again in "Enemy of the Bane" (2008). In "Doctor Who"s "The End of Time, Part Two" (2010), a Sontaran sniper (Dan Starkey) briefly appears pursuing the Doctor's former companions Mickey Smith (Noel Clarke) and his wife Martha Jones (Freema Agyeman), but is defeated by the Doctor before he can assassinate them. Alongside the Eleventh Doctor (Matt Smith), Sontarans battle fleets are seen in Series 5 finale episode, "The Pandorica Opens" (2010), as part of an alliance of the Doctor's enemies. Series 6 episode "A Good Man Goes to War" (2011) introduces Commander Strax (Starkey), a Sontaran nurse who has been assigned this role as a means of making penance. He fights on the side of the Doctor and his allies, which include the Silurian warrior Madame Vastra (Neve McIntosh) and her wife, Jenny Flint (Catrin Stewart). Strax then appears alongside Vastra and Jenny in "The Snowmen" (2012), "The Crimson Horror", "The Name of the Doctor" (both 2013), and "Deep Breath" (2014). A troop of Sontarans is also shown among Trenzalore's invaders in the 2013 Christmas special "The Time of the Doctor". A Sontaran appears briefly in the 2015 episode "Face the Raven" as a refugee.

The origins of the Sontarans have not been revealed in the television series. The "Doctor Who" role-playing game published by FASA claimed that they were all descended from the genetic stock of General Sontar (or Sontaris), who used newly developed bioengineering techniques to clone millions of duplicates of himself and annihilated the non-clone population. He renamed the race after himself and turned the Sontarans into an expansionist and warlike society set on universal conquest. However, this origin has no basis in anything seen in the television series.

The Sontarans have also appeared as a character in the PC game "Destiny of the Doctors" released on 5 December 1997, by BBC Multimedia. They can be defeated by firing the occupants of an angry beehive at them.

The Sontarans appear in the "" episode, "The Gunpowder Plot".

Big Finish Productions first used the Sontarans for their audio drama "Heroes of Sontar", a 2011 Fifth Doctor (Peter Davison) story, depicting the Doctor and his companions being forced to aid a Sontaran attack squad against a dangerous enemy that has threatened the Sontaran race by compromising their strategic methods. They next featured in The Five Companions and were stuck in an alternative version of the Death Zone with the Fifth Doctor and various companions. In 2012, "The First Sontarans" was released. A Sixth Doctor Lost Story from the mid-1980s, written by Andrew Smith, it features the Sontarans and the Rutans on nineteenth century Earth, tracking down a scientist named Jacob, who escaped through time and space. It is revealed that Jacob is from Sontar, and was responsible for genetically creating the Sontarans as a defence against a Rutan invasion. They were first developed on Sontar's gravity-heavy moon and quickly proved themselves to be at least on par with the unstoppable Rutan horde. However, believing themselves to be superior, the Sontarans turned on their creators to prevent their knowledge of Sontaran weaknesses being discovered and exploited by their enemies, conquering the planet Sontar and changing it to suit their biology. The audio ends with the Doctor and Peri helping Jacob and his wife fake their deaths so that they can go into hiding on a primitive, isolated planet to get away from their need for revenge on the Sontarans. The Sontarans also feature in the Early Adventures audio "The Sontarans", which depicts the First Doctor (William Hartnell), Steven Taylor and Sara Kingdom encountering the Sontarans during an invasion of an asteroid colony at some point before Sara's time, prompting Sara to observe that she is aware of how this confrontation will end from her own histories; this audio serves as the Doctor's chronologically earliest encounter with the Sontarans. The audio "Terror of the Sontarans" saw the Seventh Doctor (Sylvester McCoy) and Mel Bush (Bonnie Langford) meet the Sontarans on an old mining colony that had been used as a Sontaran outpost, until a complex chain of events unintentionally created a new life form that was psychically tortured by the pain of the Sontarans' victims, forcing the Doctor to destroy the colony in the hope that the non-corporeal entity will reform into something more benevolent. They also appear in the 2014 story the sontaran ordeal as part of the classic doctors new monsters range. In this story the eight doctor (Played by Paul Mcgann comes against a group of sontarans led by General stenk (played by Christopher Ryan) try to get in to the greatest of all wars the time war.

Other appearances by the Sontarans include the spin-off videos "Mindgame", "" and "Do You Have A License To Save This Planet?"; three audio plays by BBV: "Silent Warrior", "Old Soldiers" and "Conduct Unbecoming"; the Faction Paradox audio "The Shadow Play"; and a cameo appearance in "Infidel's Comet". "Shakedown" marks the only occasion in which the Sontarans and their Rutan foes appear on screen together, and was adapted into a Virgin New Adventures novel where the Seventh Doctor must prevent the Sontarans gained a clear advantage in the conflict.

They have also appeared in several spin-off novels, including "Lords of the Storm" by David A. McIntee, where the Fifth Doctor and Turlough have to stop a Sontaran scheme to take control of a colony world where Tzun technology has been hidden. In "The Infinity Doctors" by Lance Parkin, an apparently alternate version of the Doctor negotiated a peace between the Sontarans and the Rutan Host when two of them were left trapped in a TARDIS for several hours and got to talking due to their inability to kill each other. General Sontar also made an appearance in that novel. In "The Crystal Bucephalus" by Craig Hinton, the name of their planet was given as Sontara. The Sontarans also briefly appear in "The Eight Doctors", sent to the Eye of Orion by an agent of the Celestial Intervention Agency to kill the Fifth and Eighth Doctors. The novel "Warmonger" sees the Sontarans join an alliance of alien races assembled by the Fifth Doctor to defeat the mercenary army of renegade Time Lord Morbius, although the Sontarans are unaware that they follow the Doctor as he adopts the alias of 'Supreme Coordinator', which is shortened to 'Supremo' by his Ogron bodyguards.

In 1982, Jean Airey's novella "The Doctor and the Enterprise" featured a crossover between the universes of "Doctor Who" and "Star Trek", in which the Fourth Doctor (Tom Baker) finds himself on the USS "Enterprise". The "Enterprise" is attacked by a Sontaran fleet (which is unrecognizable to Captain Kirk and crew), prompting the Doctor to urgently warn the crew to flee the area.

They appear in 2009, in the novella "The Sontaran Games" by Jacqueline Rayner, featuring the Tenth Doctor and appeared in the New Series Adventures (Doctor Who) book "The Taking of Chelsea 426" by David Llewellyn, featuring the Tenth Doctor, fighting both times against the Rutan Host.

In 2008, as part of Character options first series 4 2008 wave of action figures, they released some Sontaran action figures. These include General Staal, Commander Skorr and several Sontaran soldiers.

The Sontarans are mentioned in the audio book Wraith World, when Clyde Langer (Daniel Anthony) remarks he cannot understand why Luke Smith (Tommy Knight) and Rani Chandra (Anjli Mohindra) would want to read about made up adventures, when they have faced Sontarans.

The Sontarans have also appeared several times in the "Doctor Who Magazine" comic strip, both as adversaries of the Doctor and in strips not involving the Doctor. In "The Outsider" (DWM #25-26), by Steve Moore and David Lloyd, a Sontaran named Skrant invaded the world of Brahtilis with the unwitting help of Demimon, a local astrologer. The Fourth Doctor faced the Sontarans in "Dragon's Claw" (DWM #39-#45), by Steve Moore and Dave Gibbons, where a crew of Sontarans menaced China in 1522 AD.

In Steven Moffat's short story "What I Did on My Christmas Holidays by Sally Sparrow" (the basis for the Tenth Doctor episode "Blink"), the Ninth Doctor has a rooftop sword fight with two Sontarans in 21st century Istanbul, defeating them with the help of spy Sally Sparrow, apparently before the events of "Rose" in his personal timeline.

The Sontaran homeworld was destroyed in the future during the events of the Seventh Doctor strip "Pureblood" (DWM #193-196) but the Sontaran race pool survived, allowing for further cloning; the strip introduced the concept of "pureblood" Sontarans not born of cloning. The Sontarans also feature in the Kroton solo strip "Unnatural Born Killers" (DWM #277) and the Tenth Doctor's comic strip debut "The Betrothal of Sontar" (DWM #365-#368), by John Tomlinson and Nick Abadzis, where a Sontaran mining rig on the ice planet Serac comes under attack by a mysterious force.

The Sontarans are a race of humanoids with a stocky build, greenish brown skin, a distinctive dome-shaped head, and they have only three fingers on each hand, though some members of their species do have five fingers. Their musculature is designed for load-bearing rather than leverage, because of the significant amount of gravity on their home planet. Ross Jenkins in "The Sontaran Stratagem" describes a Sontaran as resembling "a talking baked potato". Sontarans come from a large, dense planet named Sontar in the "southern spiral arm of the galaxy" which has a very strong gravitational field, which explains their compact stocky form. They are far stronger than humans and, in the recent series, are shorter than the average human male.

The Sontarans have an extremely militaristic culture which prizes discipline and honour as its highest virtues; every aspect of their draconian society is geared toward warfare, and every experience is viewed in terms of its martial relevance. In "The Sontaran Experiment", the Fourth Doctor comments that "Sontarans never do anything without a military reason." In fact, to die heroically in battle is their ultimate goal. Aside from a ritualistic chant ("Son-tar-ha!") in "The Sontaran Strategem"/"The Poison Sky", they are never seen to engage in any activity that would be considered recreation, though a few offhand comments by Commander Skorr in "The Poison Sky" suggest they do consider hunting a sport. According to their creator Robert Holmes, Sontarans do have a highly developed artistic culture, but have put it on hold for the duration of the war, while the opening chapter of the novelisation of "The Time Warrior", based on Holmes' incomplete draft, refers to Linx listening to the Sontaran anthem while his spaceship is in flight.

The Sontarans depicted in the series have detached, smug personalities, and a highly developed sense of honour; on multiple occasions, the Doctor has used his knowledge of their pride in their species to manipulate them. In "The Sontaran Stratagem", the Doctor nevertheless referred to them as "the finest soldiers in the galaxy".

Although physically formidable, the Sontarans' weak spot is the "probic vent" at the back of their neck, through which they draw nutrition. It is also part of their cloning process. It provides incentive to continue moving forward in battle since retreat would expose this area to their enemies. They have been killed by targeting that location with a knife ("The Invasion of Time"), a screwdriver (""), and an arrow ("The Time Warrior"). Even something as simple as a squash ball aimed at that point ("The Sontaran Stratagem") or contact by the heel of a shoe ("The Last Sontaran") is capable of incapacitating them temporarily. They are also vulnerable to "coronic acid" ("The Two Doctors"). While the Sontarans wear protective helmets in battle, to fight without their helmets, or to be "open-skinned," is an honour for the Sontarans.

In the episode "The Poison Sky", it is revealed that the Sontaran Empire have been at war with the Rutan Host for more than 50,000 years, and which, at a time around 2008, they are losing. The war is still raging at least 20,000 years later, in the serial "The Sontaran Experiment".

Most of the Sontarans depicted in the television series have had monosyllabic names, many beginning with an initial 'st' sound (e.g. Styre ("The Sontaran Experiment"), Stor ("The Invasion of Time"), Stike ("The Two Doctors"), Staal ("The Sontaran Stratagem"), Skorr ("The Sontaran Stratagem"), Stark ("The Pandorica Opens"), and Strax ("A Good Man Goes To War"); exceptions are Linx ("The Time Warrior"), Varl ("The Two Doctors"), Jask ("The End of Time"), and Kaagh ("The Sarah Jane Adventures")). Elements of the Sontaran military structure mentioned in the series include the "Sontaran G3 Military Assessment Survey" and the "Grand Strategic Council", the Ninth Sontaran Battle Group, the "Fifth Army Space Fleet of the Sontaran Army Space Corps", and the "Tenth Sontaran Battle Fleet". Military titles include Commander, Group Marshal, Field Major, and General. Agnomens include "the Undefeated", "the Bloodbringer", "the Avenger", and "the Slayer".

The Sontarans are a monogender-asexual (a "male gender-only" species); they reproduce by means of cloning rather than sexual reproduction, and thus for the most part are extremely similar in appearance. Human characters in both "The Sontaran Experiment" and "The Sontaran Stratagem" comment on how closely individual Sontarans resemble one another; however, their height, skin tone, facial features, vocal timbre and accent, hair, spacing of teeth and even number of fingers have varied from story to story, and sometimes within stories. When Luke Rattigan asks how they can tell each other apart in "The Sontaran Stratagem", General Staal remarks that they say the same of humans.

In "The Time Warrior", Linx states that "at the Sontaran Military Academy we have hatchings of a million cadets at each muster parade." The Doctor also comments in "The Invasion of Time" that Sontarans can mass-clone themselves at rates up to a million embryos every four minutes. Thereafter the clones take just ten minutes to grow to adulthood. When the Sontaran reach adulthood, under the charge of the Sontaran High Command, each warrior is immediately given a rank and dispatched on a battle mission. From day one, the Sontarans are sent to battle. The audio "The King of Sontar" introduces a unique occasion where one Sontaran is the only result of a clone batch, the resulting Sontaran being essentially a million Sontarans in one, making him far taller than the average for his species and lacking a probic vent.

Sontarans reproduce asexually and all the Sontarans depicted in the television series are of one gender; referred to with masculine pronouns, however it is not known if they possess distinctly male physiologies. General Staal comments that "words are the weapons of womenfolk" and that the clone of Martha Jones performed well "for a female" as commentary on the gender inequalities of other species. This typifies a Sontaran trait: interested only in the strongest fighters in any group or race. Despite this, Strax appeared perfectly comfortable with the prospect of wearing dresses in "The Battle of Demon's Run - Two Days Later"; he ultimately dressed in human gentleman's attire, nevertheless. In "The Time Warrior", when Linx examines Sarah Jane, he comments on how the human reproduction system is 'inefficient' and that humans 'should change it'. As multiple genders are foreign to them, Sontarans are known to confuse the human male and female sexes; Strax routinely addresses young women as "Boy" and vice versa, and claims not to have known that River Song was a woman.

In "The Sontaran Stratagem", the Sontarans are seen to create human clones by growing them in tubs of green fluid. "Enemy of the Bane" confirms that Sontarans are cloned in the same way. In a human clone, the umbilical corresponds to the probic vent on the back of a Sontaran's neck, suggesting that the vent is not unlike the human navel, albeit clearly more complex.

The Tenth Sontaran Battle Fleet in the new series consists of a Command Ship and a number of capsules that can be moved into position when Battle Status is enjoined. Sontaran ships are impervious to nuclear missiles. In both the classic and new series, Sontarans are depicted using spherical or semi-spherical single-occupant spacecraft known as capsules. Each capsule is small enough to avoid detection by radar and is piloted by an individual Sontaran. "The Sontaran Stratagem" also saw the introduction of a large mothership from which the small Sontaran capsules could be seen to originate. The Doctor notes that the one ship by itself is enough to completely wipe out Earth.

The Sontarans have a variety of weapons. Their distinctive weapon is a small rod with two handles and a plunger at one end, giving it a syringe style. This is so it can be held and fired using three fingers. This weapon fires a disabling beam that can temporarily render a person useless and emits an energy pulse that can repair systems like the teleport, and has appeared in every Sontaran story except "The Sontaran Experiment". When first used by Commander Linx in "The Time Warrior", it shows the ability to fire a beam which can disarm by knocking the weapon out of the wielder's hand, hypnotise, as well as cutting through wood, disabling limbs and killing.

In "The Sontaran Experiment", Field Major Styre instead used a small red laser pistol which only killed (although it did not kill the Doctor, because of a small metal plate the Doctor had been keeping in his inside pocket). "The Invasion of Time" saw Commander Stor using the small rod again, but also in episode six, a Sontaran trooper uses a short black rifle-like laser to try to burn through a lock on a door inside the TARDIS. "The Two Doctors" introduced a weapon called the Meson Gun (as named in the Jim'll Fix It Sketch, "A Fix with Sontarans"), a large silver rifle with a red fuel tank in the centre which was used by Group Marshal Stike and Varl in the third episode. It seemed to be some kind of flamethrower as it fired a jet of flames very briefly. Group Marshal Stike was also seen carrying a baton.

It would not be until "The Sontaran Stratagem" that General Staal would show that the baton can fire an orange beam that could stun the target. In "The Poison Sky", Commander Skorr and his troops carry large laser rifles into battle. These rifles are the Sontaran gun of the Tenth Sontaran Battle Fleet. Each rifle has a laser beam that kills instantly and is designed for a three-fingered grip. In "The Invasion of Time", their armour is shown to be resistant to Time Lord stasers and K-9's blaster. However, their armour is vulnerable to standard human firearms in "The Poison Sky", but the Sontarans in that episode used a 'cordolane signal' which caused the copper-lined bullets to expand, jamming most firearms instantly. UNIT troops overcame this by switching to steel-lined bullets.

"The Sarah Jane Adventures" story "The Last Sontaran" showed further technological advancements of the modern Sontarans. Commander Kaagh, a surviving pilot from the Tenth Sontaran Battle Fleet, had slightly different armour due to being from the special forces. His suit featured no gloves, so his bare hands were visible, and on his left arm was a control panel for his suit and ship. His helmet could fold up and retract and both his suit and ship featured cloaking devices, turning them both invisible. While the soldiers of the Tenth Fleet were armed with large laser rifles, Kaagh has a smaller laser carbine. Rather than hypnotising humans (as Sarah pointed out they usually do), instead, Kaagh fixed neural control devices to the back of the necks of his human agents. A red light flashes when it is operational, and Kaagh can activate and deactivate them when he wants with his control panel.

A pair of Sontarans that tried to invade Trenzalore in "Time of the Doctor" used a two-man craft with an invisibility field.


</doc>
<doc id="28027" url="https://en.wikipedia.org/wiki?curid=28027" title="Skateboarding">
Skateboarding

Skateboarding is an action sport that involves riding and performing tricks using a skateboard, as well as a recreational activity, an art form, an entertainment industry job, and a method of transportation. Skateboarding has been shaped and influenced by many skateboarders throughout the years. A 2009 report found that the skateboarding market is worth an estimated $4.8 billion in annual revenue, with 11.08 million active skateboarders in the world. In 2016, it was announced that skateboarding will be represented at the 2020 Summer Olympics in Tokyo.

Since the 1970s, skateparks have been constructed specifically for use by skateboarders, freestyle BMXers, aggressive skaters, and very recently, scooters. However, skateboarding has become controversial in areas in which the activity, although illegal, has damaged curbs, stoneworks, steps, benches, plazas and parks.

The first skateboards started with wooden boxes, or boards, with roller skate wheels attached to the bottom. Crate scooters preceded skateboards, having a wooden crate attached to the nose (front of the board), which formed rudimentary handlebars. The boxes turned into planks, similar to the skateboard decks of today. 
Skateboarding, as we know it, was probably born sometime in the late 1940s, or early 1950s, when surfers in California wanted something to do when the waves were flat. This was called "sidewalk surfing" – a new wave of surfing on the sidewalk as the sport of surfing became highly popular. No one knows who made the first board; it seems that several people came up with similar ideas at around the same time. The first manufactured skateboards were ordered by a Los Angeles, California surf shop, meant to be used by surfers in their downtime. The shop owner, Bill Richard, made a deal with the Chicago Roller Skate Company to produce sets of skate wheels, which they attached to square wooden boards. Accordingly, skateboarding was originally denoted "sidewalk surfing" and early skaters emulated surfing style and maneuvers, and performed barefoot.
By the 1960s a small number of surfing manufacturers in Southern California such as Jack's, Kips', Hobie, Bing's and Makaha started building skateboards that resembled small surfboards, and assembled teams to promote their products. One of the earliest Skateboard exhibitions was sponsored by Makaha's founder, Larry Stevenson, in 1963 and held at the Pier Avenue Junior High School in Hermosa Beach, California. Some of these same teams of skateboarders were also featured on a television show called "Surf's Up" in 1964, hosted by Stan Richards, that helped promote skateboarding as something new and fun to do.

As the popularity of skateboarding began expanding, the first skateboarding magazine, "The Quarterly Skateboarder" was published in 1964. John Severson, who published the magazine, wrote in his first editorial:

The magazine only lasted four issues, but resumed publication as "Skateboarder" in 1975. The first broadcast of an actual skateboarding competition was the 1965 National Skateboarding Championships, which were held in Anaheim, California and aired on ABC's "Wide World of Sports". Because skateboarding was a new sport during this time, there were only two original disciplines during competitions: flatland freestyle and slalom downhill racing.

One of the earliest sponsored skateboarders, Patti McGee, was paid by Hobie and Vita Pak to travel around the country to do skateboarding exhibitions and to demonstrate skateboarding safety tips. McGee made the cover of "Life" magazine in 1965 and was featured on several popular television programs—"The Mike Douglas Show", "What's My Line?" and "The Tonight Show Starring Johnny Carson"—which helped make skateboarding even more popular at the time. Some other well known surfer-style skateboarders of the time were Danny Bearer, Torger Johnson, Bruce Logan, Bill and Mark Richards, Woody Woodward, & Jim Fitzpatrick.

The growth of the sport during this period can also be seen in sales figures for Makaha, which quoted $10 million worth of board sales between 1963 and 1965 (Weyland, 2002:28). By 1966 a variety of sources began to claim that skateboarding was dangerous, resulting in shops being reluctant to sell them, and parents being reluctant to buy them. In 1966 sales had dropped significantly (ibid) and Skateboarder Magazine had stopped publication. The popularity of skateboarding dropped and remained low until the early 1970s.

In the early 1970s, Frank Nasworthy started to develop a skateboard wheel made of polyurethane, calling his company Cadillac Wheels. Prior to this new material, skateboards wheels were metal or "clay" wheels. The improvement in traction and performance was so immense that from the wheel's release in 1972 the popularity of skateboarding started to rise rapidly again, causing companies to invest more in product development. Nasworthy commissioned artist Jim Evans to do a series of paintings promoting Cadillac Wheels, they were featured as ads and posters in the resurrected Skateboarder magazine, and proved immensely popular in promoting the new style of skateboarding.

In the early 1970s skateparks hadn't been invented yet, so skateboarders would flock and skateboard in such urban places as The Escondido reservoir in San Diego, California. Skateboarding magazine would publish the location and Skateboarders made up nicknames for each location such as the Tea Bowl, the Fruit Bowl, Bellagio, the Rabbit Hole, Bird Bath, the Egg Bowl, Upland Pool and the Sewer Slide. Some of the development concepts in the terrain of skateparks were actually taken from the Escondido reservoir. Many companies started to manufacture trucks (axles) specially designed for skateboarding, reached in 1976 by Tracker Trucks. As the equipment became more maneuverable, the decks started to get wider, reaching widths of and over, thus giving the skateboarder even more control. A banana board is a skinny, flexible skateboard made of polypropylene with ribs on the underside for structural support. These were very popular during the mid-1970s and were available in a myriad of colors, bright yellow probably being the most memorable, hence the name.

In 1975 skateboarding had risen back in popularity enough to have one of the largest skateboarding competitions since the 1960s, the Del Mar National Championships, which is said to have had up to 500 competitors. The competition lasted two days and was sponsored by Bahne Skateboards & Cadillac Wheels. While the main event was won by freestyle spinning skate legend Russ Howell, a local skate team from Santa Monica, California, the Zephyr team, ushered in a new era of surfer style skateboarding during the competition that would have a lasting impact on skateboarding's history. With a team of 12, including skating legends such as Jay Adams, Tony Alva, Peggy Oki & Stacy Peralta, they brought a new progressive style of skateboarding to the event, based on the style of Hawaiian surfers Larry Bertlemann, Buttons Kaluhiokalani and Mark Liddell. Craig Stecyk, a photo journalist for Skateboarder Magazine, wrote about and photographed the team, along with Glen E. Friedman, and shortly afterwards ran a series on the team called the Dogtown articles, which eventually immortalized the Zephyr skateboard team. The team became known as the Z-Boys and would go on to become one of the most influential teams in skateboarding's history.

Soon, skateboarding contests for cash and prizes, using a professional tier system, began to be held throughout California, such as the California Free Former World Professional Skateboard Championships, which featured Freestyle and Slalom competitions.

A precursor to the extreme sport of street luge, that was sanctioned by the United States Skateboarding Association (USSA), also took place during the 1970s in Signal Hill, California. The competition was called "The Signal Hill Skateboarding Speed Run", with several competitors earning entries into the Guinness Book of World Records, at the time clocking speeds of over 50 mph on a skateboard. Due to technology and safety concerns at the time, when many competitors crashed during their runs, the sport did not gain popularity or support during this time.

In March 1976, Skateboard City skatepark in Port Orange, Florida and Carlsbad Skatepark in San Diego County, California would be the first two skateparks to be opened to the public, just a week apart. They were the first of some 200 skateparks that would be built through 1982. This was due in part to articles that were running in the investment journals at the time, stating that skateparks were a good investment. Notable skateboarders from the 1970s also include Ty Page, Tom Inouye, Laura Thornhill, Ellen O'Neal, Kim Cespedes, Bob Biniak, Jana Payne, Waldo Autry, Robin Logan, Bobby Piercy, Russ Howell, Ellen Berryman, Shogo Kubo, Desiree Von Essen, Henry Hester, Robin Alaway, Paul Hackett, Michelle Matta, Bruce Logan, Steve Cathey, Edie Robertson, Mike Weed, David Hackett, Gregg Ayres, Darren Ho, and Tom Sims.

Manufacturers started to experiment with more exotic composites and metals, like fiberglass and aluminium, but the common skateboards were made of maple plywood. The skateboarders took advantage of the improved handling of their skateboards and started inventing new tricks. Skateboarders, most notably Ty Page, Bruce Logan, Bobby Piercy, Kevin Reed, and the Z-Boys started to skate the vertical walls of swimming pools that were left empty in the 1976 California drought. This started the "vert" trend in skateboarding. With increased control, vert skaters could skate faster and perform more dangerous tricks, such as slash grinds and frontside/backside airs. This caused liability concerns and increased insurance costs to skatepark owners, and the development (first by Norcon, then more successfully by Rector) of improved knee pads that had a hard sliding cap and strong strapping proved to be too-little-too-late. During this era, the "freestyle" movement in skateboarding began to splinter off and develop into a much more specialized discipline, characterized by the development of a wide assortment of flat-ground tricks.

As a result of the "vert" skating movement, skate parks had to contend with high liability costs that led to many park closures. In response, vert skaters started making their own ramps, while freestyle skaters continued to evolve their flatland style. Thus, by the beginning of the 1980s, skateboarding had once again declined in popularity.

This period was fueled by skateboard companies that were run by skateboarders. The focus was initially on vert ramp skateboarding. The invention of the no-hands aerial (later known as the ollie) by Alan Gelfand in Florida in 1976, and the almost parallel development of the grabbed aerial by George Orton and Tony Alva in California, made it possible for skaters to perform airs on vertical ramps. While this wave of skateboarding was sparked by commercialized vert ramp skating, a majority of people who skateboarded during this period didn't ride vert ramps. As most people could not afford to build vert ramps, or did not have access to nearby ramps, street skating increased in popularity.

Freestyle skating remained healthy throughout this period, with pioneers such as Rodney Mullen inventing many of the basic tricks that would become the foundation of modern street skating, such as the "Impossible" and the "kickflip". The influence that freestyle exerted upon street skating became apparent during the mid-1980s; however, street skating was still performed on wide vert boards with short noses, slide rails, and large soft wheels. In response to the tensions created by this confluence of skateboarding "genres", a rapid evolution occurred in the late 1980s to accommodate the street skater. Since few skateparks were available to skaters at this time, street skating pushed skaters to seek out shopping centers and public and private property as their "spot" to skate. (Public opposition, in which businesses, governments, and property owners have banned skateboarding on properties under their jurisdiction or ownership, would progressively intensify over the following decades.) By 1992, only a small fraction of skateboarders continuing to take part in a highly technical version of street skating, combined with the decline of vert skating, produced a sport that lacked the mainstream appeal to attract new skaters.

During this period, numerous skateboarders - as well as companies in the industry - paid tribute to the scenes of Marty McFly skateboarding in the film "Back to the Future" for its influence in this regard. Examples can be seen in promotional material, in interviews in which professional skateboarders cite the film as an initiation into the action sport, and in the public's recognition of the film's influence.

Skateboarding during the 1990s became dominated by street skateboarding. Most boards are about wide and long. The wheels are made of an extremely hard polyurethane, with hardness (durometer) approximately 99A. The wheel sizes are relatively small so that the boards are lighter, and the wheels' inertia is overcome quicker, thus making tricks more manageable. Board styles have changed dramatically since the 1970s but have remained mostly alike since the mid-1990s. The contemporary shape of the skateboard is derived from the freestyle boards of the 1980s with a largely symmetrical shape and relatively narrow width. This form had become standard by the mid '90s.

By 2001 skateboarding had gained so much popularity that more American people under the age of 18 rode skateboards (10.6 million) than played baseball (8.2 million), although traditional organized team sports still dominated youth programs overall. Skateboarding and skateparks began to be viewed and used in a variety of new ways to complement academic lessons in schools, including new non-traditional physical education skateboarding programs, like Skatepass and Skateistan, to encourage youth to have better attendance, self-discipline and confidence. This was also based on the healthy physical opportunities skateboarding was understood to bring participants for muscle & bone strengthening and balance, as well as the positive impacts it can have on youth in teaching them mutual respect, social networking, artistic expression and an appreciation of the environment.

In 2003 Go Skateboarding Day was founded in southern California by the International Association of Skateboard Companies to promote skateboarding throughout the world. It is celebrated annually on June 21 "to define skateboarding as the rebellious, creative celebration of independence it continues to be."
According to market research firm American Sports Data the number of skateboarders worldwide increased by more than 60 percent between 1999 and 2002—from 7.8 million to 12.5 million.

Many cities also began implementing recreation plans and statutes during this time period, as part of their vision for local parks and communities to make public lands more available, in particular, for skateboarding, inviting skateboarders to come in off of the city streets and into organized skateboarding activity areas. By 2006 there were over 2,400 skateparks worldwide and the design of skateparks themselves had made a transition, as skaters turned designers. Many new places to skateboard designed specifically for street skaters, such as the "Safe Spot Skate Spot" program, first initiated by professional skateboarder Rob Dyrdek throughout many cities, allowed for the creation of smaller alternative safe skate plazas to be built at a lower cost. One of the largest locations ever built to skateboard in the world, SMP Skatepark in China, at 12,000 square meters in size, was built complete with a 5,000-seat stadium.

In 2009 Skatelab opened the Skateboarding Hall of Fame & Skateboard Museum. Nominees are chosen by the International Association of Skateboard Companies (IASC).

Efforts have been taken to improve recognition of the cultural heritage as well as the positive effects of encouraging skateboarding within designated spaces. In 2015, the John F. Kennedy Center for the Performing Arts in Washington, D.C., hosted an event at which skateboarders accompanied by music did tricks on a ramp constructed for a festival of American culture. The event was the climax of a ten-day project that transformed a federal institution formerly off-limits to the skateboarding community into a platform for that community to show its relevance through shared cultural action in a cultural common space.

By raising £790,000, the Long Live Southbank initiative managed in 2017 to curb the destruction of a forty years old spot in London due to urban planning, a salvaging operation whose effect extends beyond skateboarding. The presence of a designated skating area within this public space keeps the space under nearly constant watch and drives homeless people away, increasing the feeling of safety in and near the space. The activity attracts artists such as photographers and film makers, as well as a significant number of tourists, which in turn drives economic activity in the neighborhood.

Recently, barefoot skating has been experiencing a revival. Many skaters ride barefoot, particularly in summer and in warmer countries, such as South Africa, Australia, Spain and South America. The plastic penny board is intended to be ridden barefoot, as is the surfboard-inspired hamboard.

In the 2010s, electric skateboards became popular, along with self-balancing unicycles in a board format.
The sport of skateboarding will make its olympic debut at the 2020 Tokyo Olympic Games, with both men's and women's events. Competition will take place in two disciplines: street and park.

With the evolution of skateparks and ramp skating, the skateboard began to change. Early skate tricks had consisted mainly of two-dimensional freestyle manoeuvres like riding on only two wheels ("wheelie" or "manual"), spinning only on the back wheels (a "pivot"), high jumping over a bar and landing on the board again, also known as a "hippie jump", long jumping from one board to another, (often over small barrels or fearless teenagers), or slalom. Another popular trick was the Bertlemann slide, named after Larry Bertelemann's surfing manoeuvres.

In 1976, skateboarding was transformed by the invention of the ollie by Alan "Ollie" Gelfand. It remained largely a unique Florida trick until the summer of 1978, when Gelfand made his first visit to California. Gelfand and his revolutionary maneuvers caught the attention of the West Coast skaters and the media where it began to spread worldwide. The ollie was adapted to flat ground by Rodney Mullen in 1982. Mullen also invented the "Magic Flip," which was later renamed the kickflip, as well as many other tricks including, the 360 Kickflip, which is a 360 pop shove-it and a kickflip in the same motion. The flat ground ollie allowed skateboarders to perform tricks in mid-air without any more equipment than the skateboard itself, it has formed the basis of many street skating tricks. A recent development in the world of trick skating is the 1080, which was first ever landed by Tom Schaar in 2012.

Skateboarding was popularized by the 1986 skateboarding cult classic "Thrashin'". Directed by David Winters and starring Josh Brolin, it features appearances from many famous skaters such as Tony Alva, Tony Hawk, Christian Hosoi and Steve Caballero. "Thrashin'" also had a direct impact on "Lords of Dogtown", as Catherine Hardwicke, who directed "Lords of Dogtown", was hired by Winters to work on "Thrashin"' as a production designer where she met, worked with and befriended many famous skaters including the real Tony Alva, Tony Hawk, Christian Hosoi and Steve Caballero.

These films have helped improve the reputation of skateboarding youth, depicting individuals of this subculture as having a positive outlook on life, prone to poking harmless fun at each other, and engaging in healthy sportsman's competition. According to the film, lack of respect, egotism and hostility towards fellow skateboarders is generally frowned upon, albeit each of the characters (and as such, proxies of the "stereotypical" skateboarder) have a firm disrespect for authority and for rules in general. "Gleaming the Cube", a 1989 movie starring Christian Slater as a skateboarding teen investigating the death of his adopted Vietnamese brother, was somewhat of an iconic landmark to the skateboarding genre of the era. Many well-known skaters had cameos in the film, including Tony Hawk and Rodney Mullen, where Mullen served as Slater's stunt double.

Skateboarding was, at first, tied to the culture of surfing. As skateboarding spread across the United States to places unfamiliar with surfing or surfing culture, it developed an image of its own. For example, the classic film short "Video Days" (1991) portrayed skateboarders as reckless rebels.

California duo Jan and Dean recorded the song "Sidewalk Surfin'" in 1964, which is the Beach Boys song "Catch a Wave" with new lyrics associated with skateboarding.

Certain cities still oppose the building of skate parks in their neighborhoods, for fear of increased crime and drugs in the area. The rift between the old image of skateboarding and a newer one is quite visible: magazines such as "Thrasher" portray skateboarding as dirty, rebellious, and still firmly tied to punk, while other publications, "Transworld Skateboarding" as an example, paint a more diverse and controlled picture of skateboarding. As more professional skaters use hip hop, reggae, or hard rock music accompaniment in their videos, many urban youths, hip-hop fans, reggae fans, and hard rock fans are also drawn to skateboarding, further diluting the sport's punk image.

Group spirit supposedly influences the members of this community. In presentations of this sort, showcasing of criminal tendencies is absent, and no attempt is made to tie extreme sports to any kind of illegal activity. Female based skateboarding groups also exist, such as Brujas which is based in New York City. Many women use their participation in skate crews to perform an alternative form of femininity. These female skate crews offer a safe haven for women and girls in cities, where they can skate and bond without male expectations or competition.

The increasing availability of technology is apparent within the skateboarding community. Many skateboarders record and edit videos of themselves and friends skateboarding. However, part of this culture is to not merely replicate but to innovate; emphasis is placed on finding new places and landing new tricks.

Skateboarding video games have also become very popular in skateboarding culture. Some of the most popular are the "Tony Hawk" series and "Skate series" for various consoles (including hand-held) and personal computer.

Whilst early skateboarders generally rode barefoot, preferring direct foot-to-board contact, and some skaters continue to do so, one of the early leading trends associated with the sub-culture of skateboarding itself, was the sticky-soled slip-on skate shoe, most popularized by Sean Penn's skateboarding character from the film "Fast Times at Ridgemont High". Because early skateboarders were actually surfers trying to emulate the sport of surfing, at the time when skateboards first came out on the market, many skateboarded barefoot. But skaters often lacked traction, which led to foot injuries. This necessitated the need for a shoe that was specifically designed and marketed for skateboarding, such as the Randy "720", manufactured by the Randolph Rubber Company, and Vans sneakers, which eventually became cultural iconic signifiers for skateboarders during the 1970s and '80s as skateboarding became more widespread.

While the skate shoes design afforded better connection and traction with the deck, skaterboarders themselves could often be identified when wearing the shoes, with Tony Hawk once saying, "If you were wearing Vans shoes in 86, you were a skateboarder" Because of its connection with skateboarding, Vans financed the legendary skateboarding documentary "Dogtown and Z-Boys" and was the first sneaker company to endorse a professional skateboarder Stacy Peralta. Vans has a long history of being a major sponsor of many of skateboarding's competitions and events throughout skateboarding's history as well, including the Vans Warped Tour and the Vans Triple Crown Series.

As it eventually became more apparent that skateboarding had a particular identity with a style of shoe, other brands of shoe companies began to specifically design skate shoes for functionality and style to further enhance the experience and culture of skateboarding including such brands as; Converse, Nike, DC Shoes, Globe, Adidas, Zoo York and World Industries. Many professional skateboarders are designed a pro-model skate shoe, with their name on it, once they have received a skateboarding sponsorship after becoming notable skateboarders. Some shoe companies involved with skateboarding, like Sole Technology, an American footwear company that makes the Etnies skate shoe brand, further distinguish themselves in the market by collaborating with local cities to open public Skateparks, such as the etnies skatepark in Lake Forest, California.

Individuality and a self-expressed casual style have always been cultural values for skateboarders, as uniforms and jerseys are not typically worn. This type of personal style for skateboarders is often reflected in the graphical designs illustrated on the bottom of the deck of skateboards, since its initial conception in the mid seventies, when Wes Humpston and Jim Muri first began doing design work for Dogtown Skateboards out of their garage by hand, creating the very first iconic skateboard-deck art with the design of the "Dogtown Cross".

Prior to the mid-seventies many early skateboards were originally based upon the concept of “Sidewalk Surfing” and were tied to the surf culture, skateboards were surfboard like in appearance with little to no graphics located under the bottom of the skateboard-deck. Some of the early manufactured skateboards such as "Roller Derby", the "Duraflex Surfer" and the "Banana board" are characteristic. Some skateboards during that time were manufactured with company logo's or stickers across the top of the deck of the skateboard, as griptape was not initially used for construction. But as skateboarding progressed & evolved, and as artist began to design and add influence to the artwork of skateboards, designs and themes began to change.

There were several artistic skateboarding pioneers that had an influence on the culture of skateboarding during the 1980s, that transformed skateboard-deck art like Jim Phillips, whose edgy comic-book style "Screaming Hand", not only became the main logo for Santa Cruz Skateboards, but eventually transcended into tattoos of the same image for thousands of people and vinyl collectible figurines over the years. Artist Vernon Courtlandt Johnson is said to have used his artwork of skeletons and skulls, for Powell Peralta, during the same time that the music genres of punk rock and new wave music were beginning to mesh with the culture of skateboarding. Some other notable skateboard artists that made contribrutions to the culture of skateboarding also include Andy Jenkins, Todd Bratrud, Neil Blender, Marc McKee, Tod Swank, Mark Gonzales, Lance Mountain, Natas Kaupas and Jim Evans.

Over the years skateboard-deck art has continued to influence and expand the culture of skateboarding, as many people began collecting skateboards based on their artistic value and nostalgia. Productions of limited editions with particular designs and types of collectible prints that can be hung on the wall, have been created by such famous artists as Andy Warhol and Keith Haring. Most professional skateboarders today have their own signature skateboard decks, with their favorite artistic designs printed on them using computer graphics.

In January 2019, Sotheby's in New York auctioned the full set of the 248 skateboard deck designs ever sold by Supreme, collected by Ryan Fuller. The full set sold for $800,000 to 17 year old Carson Guo from Vancouver who plans to exhibit them in a local gallery.

New York based SHUT skateboards had a goldplated skateboard for sale at $15,000 in 2014, then the most expensive skateboard in the world.

In 2019, artist Adrian Wilson created the SUPREME Mundi, a cross between an artist palette and a skateboard as a commentary on the record bids at auction of the Supreme decks and the restored Salvatore Mundi which was sold by a New York art gallery for $20,000

Skateboards, along with other small-wheeled transportation such as in-line skates and scooters, suffer a safety problem: riders may easily be thrown from small cracks and outcroppings in pavement, especially where the cracks run across the direction of travel. Hitting such an irregularity is the major cause of falls and injuries. The risk may be reduced at higher travel speeds.

Severe injuries are relatively rare. Commonly, a skateboarder who falls suffers from scrapes, cuts, bruises, and sprains. Among injuries reported to a hospital, about half involve broken bones, usually the long bones in the leg or arm. One-third of skateboarders with reported injuries are very new to the sport, having started skating within one week of the injury. Although less common, involving 3.5–9 percent of reported injuries, traumatic head injuries and death are possible severe outcomes.

Skating as a form of transportation exposes the skateboarder to the dangers of other traffic. Skateboarders on the street may be hit by other vehicles or may fall into vehicular traffic.

Skateboarders also pose a risk to other pedestrians and traffic. If the skateboarder falls, the skateboard may roll or fly into another person. A skateboarder who collides with a person who is walking or biking may injure or, rarely, kill that person.

Many jurisdictions require skateboarders to wear bicycle helmets to reduce the risk of head injuries and death. Other protective gear, such as wrist guards, also reduce injury. Some medical researchers have proposed restricting skateboarding to designated, specially designed areas, to reduce the number and severity of injuries, and to eliminate injuries caused by motor vehicles or to other pedestrians.

The use, ownership and sale of skateboards were forbidden in Norway from 1978 to 1989 because of the high number of injuries caused by boards. The ban led skateboarders to construct ramps in the forest and other secluded areas to avoid the police. There was, however, one legal skatepark in the country in Frogner Park in Oslo.

The use of skateboards solely as a form of transportation is often associated with the longboard. Depending on local laws, using skateboards as a form of transportation outside residential areas may or may not be legal. Backers cite portability, exercise, and environmental friendliness as some of the benefits of skateboarding as an alternative to automobiles.

The United States Marine Corps tested the usefulness of commercial off-the-shelf skateboards during urban combat military exercises in the late 1990s in a program called Urban Warrior '99. Their special purpose was "for maneuvering inside buildings in order to detect tripwires and sniper fire".

Trampboarding is a variant of skateboarding that uses a board without the trucks and the wheels on a trampoline. Using the bounce of the trampoline gives height to perform tricks, whereas in skateboarding you need to make the height by performing an ollie. Trampboarding is seen on YouTube in numerous videos.

Swing boarding is the activity where a skateboard deck is suspended from a pivot point above the rider which allows the rider to swing about that pivot point. The board swings in an arc which is a similar movement to riding a half pipe. The incorporation of a harness and frame allows the rider to perform turns and spins all while flying through the air.

Skateboarding damages urban terrain features such as curbs, benches, and ledges when skateboarders perform "grinds" and other tricks on these surfaces. Private industry has responded to this problem by using skate deterrent devices, such as the Skatestopper, in efforts to prevent further damage and to reduce skateboarding on these surfaces.

The enactment of ordinances and the posting of signs stating "Skateboarding is not allowed" have also become common methods to discourage skateboarding in public areas in many cities, to protect pedestrians and property. In the area of street skating, tickets and arrest from police for trespassing and vandalism are not uncommon.

Skateboarding has become an important problem in Freedom Plaza, a National Park within the Pennsylvania Avenue National Historic Site in Washington, D.C. The Plaza contains copies of portions of Pierre (Peter) Charles L'Enfant's 1791 plan for the nation's capital city that have been inscribed in the park's raised marble surface.

Freedom Plaza has become a popular location for skateboarding, although the activity is illegal and has resulted in police actions. A 2016 National Park Service management plan for the Historic Site states that skateboarding has damaged stonework, sculptures, walls, benches, steps, and other surfaces in some areas of the Plaza. The management plan further states that skateboarding presents a persistent law enforcement and management challenge, as popular websites advertise the Plaza's attractiveness for the activity. The plan notes that vandals have removed "No Skateboarding" signs and recommends the replacement of those signs.

A professional skateboarder promoted on Facebook the use of governmental sites for the prohibited activity during the 2013 federal government shutdown in the United States.





</doc>
<doc id="28028" url="https://en.wikipedia.org/wiki?curid=28028" title="Speed skating">
Speed skating

Speed skating is a competitive form of ice skating in which the competitors race each other in travelling a certain distance on skates. Types of speed skating are long track speed skating, short track speed skating, and marathon speed skating. In the Olympic Games, long-track speed skating is usually referred to as just "speed skating", while short-track speed skating is known as "short track". The International Skating Union (ISU), the governing body of both ice sports, refers to long track as "speed skating" and short track as "short track skating".

An international federation was founded in 1892, the first for any winter sport. The sport enjoys large popularity in the Netherlands, Norway and South Korea. There are top international rinks in a number of other countries, including Canada, the United States, Germany, Italy, Japan, Russia, Kazakhstan, China, Belarus and Poland. A World Cup circuit is held with events in those countries plus two events in the Thialf ice hall in Heerenveen, Netherlands.

The standard rink for long track is 400 meters long, but tracks of 200, 250 and 333 meters are used occasionally. It is one of two Olympic forms of the sport and the one with the longer history.

ISU rules allow some leeway in the size and radius of curves.

Short track speed skating takes place on a smaller rink, normally the size of an ice hockey rink, on a 111.12 m oval track. Distances are shorter than in long-track racing, with the longest Olympic individual race being 1500 meters (the women's relay is 3000 meters and the men's relay 5000 meters). Event are usually held with a knockout format, with the best two in heats of four or five qualifying for the final race, where medals are awarded. Disqualifications and falls are not uncommon.

There are variations on the mass-start races. In the regulations of roller sports, eight different types of mass starts are described. Among them are elimination races, where one or more competitors are eliminated at fixed points during the course; simple distance races, which may include preliminary races; endurance races with time limits instead of a fixed distance; points races; and individual pursuits.

Races usually have some rules about disqualification if an opponent is unfairly hindered; these rules vary between the disciplines. In long track speed skating, almost any infringement on the pairmate is punished, though skaters are permitted to change from the inner to the outer lane out of the final curve if they are not able to hold the inner curve, as long as they are not interfering with the other skater. In mass-start races, skaters will usually be allowed some physical contact.

Team races are also held; in long track speed skating, the only team race at the highest level of competition is the Team pursuit, though athletics-style relay races are held at children's competitions. Relay races are also held in short track and inline competitions, but here, exchanges may take place at any time during the race, though exchanges may be banned during the last couple of laps.

Most speed skating races are held on an oval course, but there are exceptions. Oval sizes vary; in short track speed skating, the rink must be an oval of 111.12 metres, while long track speed skating uses a similarly standardized 400 m rink. Inline skating rinks are between 125 and 400 metres, though banked tracks can only be 250 metres long. Inline skating can also be held on closed road courses between 400 and 1,000 metres, as well as open-road competitions where starting and finishing lines do not coincide. This is also a feature of outdoor marathons.

In the Netherlands, marathon competitions may be held on natural ice on canals, and bodies of water such as lakes and rivers, but may also be held on artificially frozen 400 m tracks, with skaters circling the track 100 times, for example.

The roots of speed skating date back over a millennium in the North of Europe, especially Scandinavia and the Netherlands, where the natives added bones to their shoes and used them to travel on frozen rivers, canals and lakes. In contrast to what people think, ice skating has always been an activity of joy and sports and not a matter of transport. For example, winters in the Netherlands have never been stable and cold enough to make ice skating a regular way of travelling or a mode of transport.
This has already been described in 1194 by William Fitzstephen, who described a sport in London.

Later, in Norway, King Eystein Magnusson, later King Eystein I of Norway, boasts of his skills racing on ice legs.

However, skating and speed skating was not limited to the Netherlands and Scandinavia; in 1592, a Scotsman designed a skate with an iron blade. It was iron-bladed skates that led to the spread of skating and, in particular, speed skating.
By 1642, the first official skating club, The Skating Club Of Edinburgh, was born, and, in 1763, the world saw its first official speed skating race, at Wisbech on the Fens in England for a prize sum of 70 guineas.
While in the Netherlands, people began touring the waterways connecting the 11 cities of Friesland, a challenge which eventually led to the Elfstedentocht.

The first known official speed skating competition for women was in Heerenveen, the Netherlands from 1 to 2 February 1805. The competition was won by Trijntje Pieters Westra.

By 1851, North Americans had discovered a love of the sport, and the all-steel blade was later developed there. In Norway speed skating also became popular, as there was a huge interest in the 1885 speed skating race at Frognerkilen between Axel Paulsen and Renke van der Zee.
The Netherlands came back to the fore in 1889 with the organization of the first world championships. The ISU (International Skating Union) was also born in the Netherlands in 1892.
By the start of the 20th century, skating and speed skating had come into its own as a major popular sporting activity.

Organized races on ice skates developed in the 19th century. Norwegian clubs hosted competitions from 1863, with races in Christiania drawing five-digit crowds. In 1884, the Norwegian Axel Paulsen was named Amateur Champion Skater of the World after winning competitions in the United States. Five years later, a sports club in Amsterdam held an ice-skating event they called a world championship, with participants from Russia, the United States and the United Kingdom, as well as the host country. The "Internationale Eislauf Vereinigung", now known as the International Skating Union, was founded at a meeting of 15 national representatives in Scheveningen in 1892, the first international winter sports federation. The Nederlandse Schaatsrijderbond was founded in 1882 and organised the world championships of 1890 and 1891. Competitions were held around tracks of varying lengths—the 1885 match between Axel Paulsen and Remke van der Zee was skated on a track of 6/7 mile (1400 metres)—but the 400 metre track was standardised by the ISU in 1892, along with the standard distances for world championships, 500 m, 1500 m, 5000 m and 10,000 m. Skaters started in pairs, each to their own lane, and changed lanes for every lap to ensure that each skater completed the same distance. This is what is now known as long track speed skating. Competitions were exclusively for amateur skaters, which was enforced. Peter Sinnerud was disqualified for professionalism in 1904 and lost his world title.

Long track world records were first registered in 1891 and improved rapidly, Jaap Eden lowering the world 5000-metre record by half a minute during the Hamar European Championships in 1894. The record stood for 17 years, and it took 50 years to lower it by further half a minute.

The Elfstedentocht was organized as a competition in 1909 and has been held at irregular intervals, whenever the ice on the course is deemed good enough. Other outdoor races developed later, with Friesland in the northern Netherlands hosting a race in 1917, but the Dutch natural ice conditions have rarely been conducive to skating. The Elfstedentocht has been held 15 times in the nearly 100 years since 1909, and, before artificial ice was available in 1962, national championships had been held in 25 of the years between 1887, when the first championship was held in Slikkerveer, and 1961. Since artificial ice became common in the Netherlands, Dutch speed skaters have been among the world top in long track ice skating and marathon skating. Another solution to still be able to skate marathons on natural ice became the Alternative Elfstedentocht. The Alternative Elfstedentocht races take part in other countries, such as Austria, Finland or Canada, and all top marathon skaters, as well as thousands of recreative skaters, travel from the Netherlands to the location where the race is held. According to the NRC Handelsblad journalist Jaap Bloembergen, the country "takes a carnival look" during international skating championships.

At the 1914 Olympic Congress, the delegates agreed to include ice speed skating in the 1916 Olympics, after figure skating had featured in the 1908 Olympics. However, World War I put an end to the plans of Olympic competition, and it was not until the winter sports week in Chamonix in 1924—retroactively awarded Olympic status—that ice speed skating reached the Olympic programme. Charles Jewtraw from Lake Placid, New York, won the first Olympic gold medal, though several Norwegians in attendance claimed Oskar Olsen had clocked a better time. Timing issues on the 500 were a problem within the sport until electronic clocks arrived in the 1960s; during the 1936 Olympic 500–metre race, it was suggested that Ivar Ballangrud's 500-metre time was almost a second too good. Finland won the remaining four gold medals at the 1924 Games, with Clas Thunberg winning 1,500 metres, 5,000 metres, and allround. It was the first and only time an allround Olympic gold medal has been awarded in speed skating. Speed Skating is also a sport in today's Olympics.

Norwegian and Finnish skaters won all the gold medals in world championships between the world wars, with Latvians and Austrians visiting the podium in the European Championships. However, North American races were usually conducted pack-style, similar to the marathon races in the Netherlands, but the Olympic races were to be held over the four ISU-approved distances. The ISU approved the suggestion that the speed skating at the 1932 Winter Olympics should be held as pack-style races, and Americans won all four gold medals. Canada won five medals, all silver and bronze, while defending World Champion Clas Thunberg stayed at home, protesting against this form of racing. At the World Championships held immediately after the games, without the American champions, Norwegian racers won all four distances and occupied the three top spots in the allround standings.

Norwegians, Swedes, Finns, and Japanese skating leaders protested to the USOC, condemning the manner of competition and expressing the wish that mass-start races were never to be held again at the Olympics. However, the ISU adopted the short track speed skating branch, with mass-start races on shorter tracks, in 1967, arranged international competitions from 1976, and brought them back to the Olympics in 1992.

Artificial ices entered the long track competitions with the 1960 Winter Olympics, and the competitions in 1956 on Lake Misurina were the last Olympic competitions on natural ice. 1960 also saw the first Winter Olympic competitions for women. Lidia Skoblikova won two gold medals in 1960 and four in 1964.

More aerodynamic skating suits were also developed, with Swiss skater Franz Krienbühl (who finished 8th on the Olympic 10,000 m at the age of 46) at the front of development. After a while, national teams took over development of body suits, which are also used in short track skating, though without headcover attached to the suit—short trackers wear helmets instead, as falls are more common in mass-start races. Suits and indoor skating, as well as the clap skate, has helped to lower long track world records considerably; from 1971 to 2009, the average speed on the men's 1500 metres has been raised from 45 to 52 km/h. Similar speed increases are shown in the other distances.

After the 1972 season, European long track skaters founded a professional league, International Speedskating League, which included Ard Schenk, three-time Olympic gold medallist in 1972, as well as five Norwegians, four other Dutchmen, three Swedes, and a few other skaters. Jonny Nilsson, 1963 world champion and Olympic gold medallist, was the driving force behind the league, which folded in 1974 for economic reasons, and the ISU also excluded tracks hosting professional races from future international championships. The ISU later organised its own World Cup circuit with monetary prizes, and full-time professional teams developed in the Netherlands during the 1990s, which led them to a dominance on the men's side only challenged by Japanese 500 m racers and American inline skaters who changed to long tracks to win Olympic gold.

During the 20th century, roller skating also developed as a competitive sport. Roller-skating races were professional from an early stage. Professional World Championships were arranged in North America between the competitors on that circuit. Later, roller derby leagues appeared, a professional contact sport that originally was a form of racing. FIRS World Championships of inline speed skating go back to the 1980s, but many world champions, such as Derek Parra and Chad Hedrick, have switched to ice in order to win Olympic medals.

Like roller skating, ice speed skating was also professional in North America. Oscar Mathisen, five-time ISU world champion and three-time European champion, renounced his amateur status in 1916 and travelled to America, where he won many races but was beaten by Bobby McLean of Chicago, four-time American champion, in one of the races. Chicago was a centre of ice speed skating in America; the "Chicago Tribune" sponsored a competition called the Silver Skates from 1912 to 2014.

In 1992, short track speed skating was accepted as an Olympic sport. Short track speed skating had little following in the long track speed skating countries of Europe, such as Norway, the Netherlands and the former Soviet Union, with none of these nations having won official medals (though the Netherlands won two gold medals when the sport was a demonstration event in 1988). The Norwegian publication "Sportsboken" spent ten pages detailing the long track speed skating events at the Albertville Games in 1993, but short track was not mentioned by word, though the results pages appeared in that section.

Although this form of speed skating is newer, it is growing faster than long-track speed skating, largely because short track can be done on an ice hockey rink rather than a long-track oval.

Races are run counter-clockwise on a 111-meter track. Short track races are almost always run in a mass start format in which two to six skaters may race at once. Skaters may be disqualified for false starts, impeding, and cutting inside the track. False starts occur when a skater moves before the gun goes off at the start of a race. Skaters are disqualified for impeding when one skater cuts in front of another skater and causes the first skater to stand up to avoid collision or fall. Cutting inside the track occurs when a skater's skates goes inside the blocks which mark the track on the ice. If disqualified the skater will be given last place in their heat or final.

Races are run counter-clockwise on a 400-meter oval. In all individual competition forms, only two skaters are allowed to race at once. Skaters must change lanes every lap. The skater changing from the outside lane to the inside has right-of-way. Skaters may be disqualified for false starts, impeding, and cutting inside the track. If a skater misses their race or falls they have the option to race their distance again. There are no heats or finals in long track, all rankings are by time.

The starting procedure in long-track speed skating consists of three parts. First, the referee tells the athletes to ""Go to the start"". Second, the referee cues the athletes to get "Ready", and waits until the skaters have stopped moving. Finally, the referee waits for a random duration between 1 and 1.5 seconds, and then fires the starting shot. Some argue that this inherent timing variability could disadvantage athletes that start after longer pauses, due to the alerting effect.

In the only non-individual competition form, the team pursuit, two teams of each three to four skaters are allowed to race at once. Both teams remain in the inner lane for the duration of the race; they start on opposite sides of the rink. If four skaters are racing one skater is allowed to drop off and stop racing. The clock stops when the third skater crosses the finish line.

Speed skates Speed skates differ greatly from hockey skates and figure skates. Unlike hockey skates and figure skates, speed skates cut off at the ankle and are built more like a shoe than a boot to allow for more ankle compression. The blades range in length from 30 to 45 cm depending on the age and height of the skater. Short track blades are fixed to the boot in two places once at the heel and the other right behind the ball of the foot. Long track skates, also called clap skates, attach firmly to the boot only at the front. The heel of the boot detaches from the blade on every stroke, through a spring mechanism located at the front connector. Speed skates are manually sharpened using a jig to hold them in place.

Short track
All short track skaters must have speed skates, a spandex skin suit, protective helmet, specific cut proof skating gloves, knee pads and shin pads (in suit), neck guard (bib style) and ankle protection. Protective eyewear is mandatory. Many skaters wear smooth ceramic or carbon fiber tips on the left hand glove to reduce friction when their hand is on the ice at corners. All skaters who race at a national level must wear a cutproof kevlar suit to protect against being cut from another skater's blade.

Long track
For long track skaters the same equipment should be worn as short track racers but with the exception of a helmet, shin pads, knee pads, and neck guard which are not required. Protective eyewear is not mandatory. The suit also does not need to be kevlar. Long track skaters wear a hood that is built into the suit.





</doc>
<doc id="28030" url="https://en.wikipedia.org/wiki?curid=28030" title="September 13">
September 13





</doc>
<doc id="28032" url="https://en.wikipedia.org/wiki?curid=28032" title="Square (disambiguation)">
Square (disambiguation)

A square is a regular quadrilateral with four equal sides and four right angles.

Square may also refer to:









</doc>
<doc id="28034" url="https://en.wikipedia.org/wiki?curid=28034" title="Scanning electron microscope">
Scanning electron microscope

A scanning electron microscope (SEM) is a type of electron microscope that produces images of a sample by scanning the surface with a focused beam of electrons. The electrons interact with atoms in the sample, producing various signals that contain information about the surface topography and composition of the sample. The electron beam is scanned in a raster scan pattern, and the position of the beam is combined with the intensity of the detected signal to produce an image. In the most common SEM mode, secondary electrons emitted by atoms excited by the electron beam are detected using a secondary electron detector (Everhart-Thornley detector). The number of secondary electrons that can be detected, and thus the signal intensity, depends, among other things, on specimen topography. SEM can achieve resolution better than 1 nanometer.

Specimens are observed in high vacuum in conventional SEM, or in low vacuum or wet conditions in variable pressure or environmental SEM, and at a wide range of cryogenic or elevated temperatures with specialized instruments.

An account of the early history of SEM has been presented by McMullan. Although Max Knoll produced a photo with a 50 mm object-field-width showing channeling contrast by the use of an electron beam scanner, it was Manfred von Ardenne who in 1937 invented a microscope with high resolution by scanning a very small raster with a demagnified and finely focused electron beam. Ardenne applied scanning of the electron beam in an attempt to surpass the resolution of the transmission electron microscope (TEM), as well as to mitigate substantial problems with chromatic aberration inherent to real imaging in the TEM. He further discussed the various detection modes, possibilities and theory of SEM, together with the construction of the . Further work was reported by Zworykin's group, followed by the Cambridge groups in the 1950s and early 1960s headed by Charles Oatley, all of which finally led to the marketing of the first commercial instrument by Cambridge Scientific Instrument Company as the "Stereoscan" in 1965, which was delivered to DuPont.

The signals used by a scanning electron microscope to produce an image result from interactions of the electron beam with atoms at various depths within the sample. Various types of signals are produced including secondary electrons (SE), reflected or back-scattered electrons (BSE), characteristic X-rays and light (cathodoluminescence) (CL), absorbed current (specimen current) and transmitted electrons. Secondary electron detectors are standard equipment in all SEMs, but it is rare for a single machine to have detectors for all other possible signals.

Secondary electrons have very low energies on the order of 50 eV, which limits their mean free path in solid matter. Consequently, SEs can only escape from the top few nanometers of the surface of a sample. The signal from secondary electrons tends to be highly localized at the point of impact of the primary electron beam, making it possible to collect images of the sample surface with a resolution of below 1 nm. Back-scattered electrons (BSE) are beam electrons that are reflected from the sample by elastic scattering. They emerge from deeper locations within the specimen and, consequently, the resolution of BSE images is less than SE images. However, BSE are often used in analytical SEM, along with the spectra made from the characteristic X-rays, because the intensity of the BSE signal is strongly related to the atomic number (Z) of the specimen. BSE images can provide information about the distribution, but not the identity, of different elements in the sample. In samples predominantly composed of light elements, such as biological specimens, BSE imaging can image colloidal gold immuno-labels of 5 or 10 nm diameter, which would otherwise be difficult or impossible to detect in secondary electron images. Characteristic X-rays are emitted when the electron beam removes an inner shell electron from the sample, causing a higher-energy electron to fill the shell and release energy. The energy or wavelength of these characteristic X-rays can be measured by Energy-dispersive X-ray spectroscopy or Wavelength-dispersive X-ray spectroscopy and used to identify and measure the abundance of elements in the sample and map their distribution.

Due to the very narrow electron beam, SEM micrographs have a large depth of field yielding a characteristic three-dimensional appearance useful for understanding the surface structure of a sample. This is exemplified by the micrograph of pollen shown above. A wide range of magnifications is possible, from about 10 times (about equivalent to that of a powerful hand-lens) to more than 500,000 times, about 250 times the magnification limit of the best light microscopes.

SEM samples have to be small enough to fit on the specimen stage, and may need special preparation to increase their electrical conductivity and to stabilize them, so that they can withstand the high vacuum conditions and the high energy beam of electrons. Samples are generally mounted rigidly on a specimen holder or stub using a conductive adhesive. SEM is used extensively for defect analysis of semiconductor wafers, and manufacturers make instruments that can examine any part of a 300 mm semiconductor wafer. Many instruments have chambers that can tilt an object of that size to 45° and provide continuous 360° rotation.

Nonconductive specimens collect charge when scanned by the electron beam, and especially in secondary electron imaging mode, this causes scanning faults and other image artifacts. For conventional imaging in the SEM, specimens must be electrically conductive, at least at the surface, and electrically grounded to prevent the accumulation of electrostatic charge. Metal objects require little special preparation for SEM except for cleaning and conductively mounting to a specimen stub. Non-conducting materials are usually coated with an ultrathin coating of electrically conducting material, deposited on the sample either by low-vacuum sputter coating or by high-vacuum evaporation. Conductive materials in current use for specimen coating include gold, gold/palladium alloy, platinum, iridium, tungsten, chromium, osmium, and graphite. Coating with heavy metals may increase signal/noise ratio for samples of low atomic number (Z). The improvement arises because secondary electron emission for high-Z materials is enhanced.

An alternative to coating for some biological samples is to increase the bulk conductivity of the material by impregnation with osmium using variants of the OTO staining method (O-osmium tetroxide, T-thiocarbohydrazide, O-osmium).

Nonconducting specimens may be imaged without coating using an environmental SEM (ESEM) or low-voltage mode of SEM operation. In ESEM instruments the specimen is placed in a relatively high-pressure chamber and the electron optical column is differentially pumped to keep vacuum adequately low at the electron gun. The high-pressure region around the sample in the ESEM neutralizes charge and provides an amplification of the secondary electron signal. Low-voltage SEM is typically conducted in an instrument with a field emission guns (FEG) which is capable of producing high primary electron brightness and small spot size even at low accelerating potentials. To prevent charging of non-conductive specimens, operating conditions must be adjusted such that the incoming beam current is equal to sum of outgoing secondary and backscattered electron currents, a condition that is most often met at accelerating voltages of 0.3–4 kV.

Synthetic replicas can be made to avoid the use of original samples when they are not suitable or available for SEM examination due to methodological obstacles or legal issues. This technique is achieved in two steps: (1) a mold of the original surface is made using a silicone-based dental elastomer, and (2) a replica of the original surface is obtained by pouring a synthetic resin into the mold.

Embedding in a resin with further polishing to a mirror-like finish can be used for both biological and materials specimens when imaging in backscattered electrons or when doing quantitative X-ray microanalysis.

The main preparation techniques are not required in the environmental SEM outlined below, but some biological specimens can benefit from fixation.

For SEM, a specimen is normally required to be completely dry, since the specimen chamber is at high vacuum. Hard, dry materials such as wood, bone, feathers, dried insects, or shells (including egg shells) can be examined with little further treatment, but living cells and tissues and whole, soft-bodied organisms require chemical fixation to preserve and stabilize their structure.

Fixation is usually performed by incubation in a solution of a buffered chemical fixative, such as glutaraldehyde, sometimes in combination with formaldehyde and other fixatives, and optionally followed by postfixation with osmium tetroxide. The fixed tissue is then dehydrated. Because air-drying causes collapse and shrinkage, this is commonly achieved by replacement of water in the cells with organic solvents such as ethanol or acetone, and replacement of these solvents in turn with a transitional fluid such as liquid carbon dioxide by critical point drying. The carbon dioxide is finally removed while in a supercritical state, so that no gas–liquid interface is present within the sample during drying.

The dry specimen is usually mounted on a specimen stub using an adhesive such as epoxy resin or electrically conductive double-sided adhesive tape, and sputter-coated with gold or gold/palladium alloy before examination in the microscope. Samples may be sectioned (with a microtome) if information about the organism's internal ultrastructure is to be exposed for imaging.

If the SEM is equipped with a cold stage for cryo microscopy, cryofixation may be used and low-temperature scanning electron microscopy performed on the cryogenically fixed specimens. Cryo-fixed specimens may be cryo-fractured under vacuum in a special apparatus to reveal internal structure, sputter-coated and transferred onto the SEM cryo-stage while still frozen. Low-temperature scanning electron microscopy (LT-SEM) is also applicable to the imaging of temperature-sensitive materials such as ice and fats.

Freeze-fracturing, freeze-etch or freeze-and-break is a preparation method particularly useful for examining lipid membranes and their incorporated proteins in "face on" view. The preparation method reveals the proteins embedded in the lipid bilayer.

Back-scattered electron imaging, quantitative X-ray analysis, and X-ray mapping of specimens often requires grinding and polishing the surfaces to an ultra-smooth surface. Specimens that undergo WDS or EDS analysis are often carbon-coated. In general, metals are not coated prior to imaging in the SEM because they are conductive and provide their own pathway to ground.

Fractography is the study of fractured surfaces that can be done on a light microscope or, commonly, on an SEM. The fractured surface is cut to a suitable size, cleaned of any organic residues, and mounted on a specimen holder for viewing in the SEM.

Integrated circuits may be cut with a focused ion beam (FIB) or other ion beam milling instrument for viewing in the SEM. The SEM in the first case may be incorporated into the FIB.

Metals, geological specimens, and integrated circuits all may also be chemically polished for viewing in the SEM.

Special high-resolution coating techniques are required for high-magnification imaging of inorganic thin films.

In a typical SEM, an electron beam is thermionically emitted from an electron gun fitted with a tungsten filament cathode. Tungsten is normally used in thermionic electron guns because it has the highest melting point and lowest vapor pressure of all metals, thereby allowing it to be electrically heated for electron emission, and because of its low cost. Other types of electron emitters include lanthanum hexaboride () cathodes, which can be used in a standard tungsten filament SEM if the vacuum system is upgraded or field emission guns (FEG), which may be of the cold-cathode type using tungsten single crystal emitters or the thermally assisted Schottky type, that use emitters of zirconium oxide.

The electron beam, which typically has an energy ranging from 0.2 keV to 40 keV, is focused by one or two condenser lenses to a spot about 0.4 nm to 5 nm in diameter. The beam passes through pairs of scanning coils or pairs of deflector plates in the electron column, typically in the final lens, which deflect the beam in the "x" and "y" axes so that it scans in a raster fashion over a rectangular area of the sample surface.

When the primary electron beam interacts with the sample, the electrons lose energy by repeated random scattering and absorption within a teardrop-shaped volume of the specimen known as the interaction volume, which extends from less than 100 nm to approximately 5 µm into the surface. The size of the interaction volume depends on the electron's landing energy, the atomic number of the specimen and the specimen's density. The energy exchange between the electron beam and the sample results in the reflection of high-energy electrons by elastic scattering, emission of secondary electrons by inelastic scattering and the emission of electromagnetic radiation, each of which can be detected by specialized detectors. The beam current absorbed by the specimen can also be detected and used to create images of the distribution of specimen current. Electronic amplifiers of various types are used to amplify the signals, which are displayed as variations in brightness on a computer monitor (or, for vintage models, on a cathode ray tube). Each pixel of computer video memory is synchronized with the position of the beam on the specimen in the microscope, and the resulting image is, therefore, a distribution map of the intensity of the signal being emitted from the scanned area of the specimen. Older microscopes captured images on film, but most modern instrument collect digital images.

Magnification in an SEM can be controlled over a range of about 6 orders of magnitude from about 10 to 3,000,000 times. Unlike optical and transmission electron microscopes, image magnification in an SEM is not a function of the power of the objective lens. SEMs may have condenser and objective lenses, but their function is to focus the beam to a spot, and not to image the specimen. Provided the electron gun can generate a beam with sufficiently small diameter, a SEM could in principle work entirely without condenser or objective lenses, although it might not be very versatile or achieve very high resolution. In an SEM, as in scanning probe microscopy, magnification results from the ratio of the dimensions of the raster on the specimen and the raster on the display device. Assuming that the display screen has a fixed size, higher magnification results from reducing the size of the raster on the specimen, and vice versa. Magnification is therefore controlled by the current supplied to the x, y scanning coils, or the voltage supplied to the x, y deflector plates, and not by objective lens power.

The most common imaging mode collects low-energy (<50 eV) secondary electrons that are ejected from conduction or valence bands of the specimen atoms by inelastic scattering interactions with beam electrons. Due to their low energy, these electrons originate from within a few nanometers below the sample surface. The electrons are detected by an Everhart-Thornley detector, which is a type of collector-scintillator-photomultiplier system. The secondary electrons are first collected by attracting them towards an electrically biased grid at about +400 V, and then further accelerated towards a phosphor or scintillator positively biased to about +2,000 V. The accelerated secondary electrons are now sufficiently energetic to cause the scintillator to emit flashes of light (cathodoluminescence), which are conducted to a photomultiplier outside the SEM column via a light pipe and a window in the wall of the specimen chamber. The amplified electrical signal output by the photomultiplier is displayed as a two-dimensional intensity distribution that can be viewed and photographed on an analogue video display, or subjected to analog-to-digital conversion and displayed and saved as a digital image. This process relies on a raster-scanned primary beam. The brightness of the signal depends on the number of secondary electrons reaching the detector. If the beam enters the sample perpendicular to the surface, then the activated region is uniform about the axis of the beam and a certain number of electrons "escape" from within the sample. As the angle of incidence increases, the interaction volume increases and the "escape" distance of one side of the beam decreases, resulting in more secondary electrons being emitted from the sample. Thus steep surfaces and edges tend to be brighter than flat surfaces, which results in images with a well-defined, three-dimensional appearance. Using the signal of secondary electrons image resolution less than 0.5 nm is possible.

Backscattered electrons (BSE) consist of high-energy electrons originating in the electron beam, that are reflected or back-scattered out of the specimen interaction volume by elastic scattering interactions with specimen atoms. Since heavy elements (high atomic number) backscatter electrons more strongly than light elements (low atomic number), and thus appear brighter in the image, BSEs are used to detect contrast between areas with different chemical compositions. The Everhart-Thornley detector, which is normally positioned to one side of the specimen, is inefficient for the detection of backscattered electrons because few such electrons are emitted in the solid angle subtended by the detector, and because the positively biased detection grid has little ability to attract the higher energy BSE. Dedicated backscattered electron detectors are positioned above the sample in a "doughnut" type arrangement, concentric with the electron beam, maximizing the solid angle of collection. BSE detectors are usually either of scintillator or of semiconductor types. When all parts of the detector are used to collect electrons symmetrically about the beam, atomic number contrast is produced. However, strong topographic contrast is produced by collecting back-scattered electrons from one side above the specimen using an asymmetrical, directional BSE detector; the resulting contrast appears as illumination of the topography from that side. Semiconductor detectors can be made in radial segments that can be switched in or out to control the type of contrast produced and its directionality.

Backscattered electrons can also be used to form an electron backscatter diffraction (EBSD) image that can be used to determine the crystallographic structure of the specimen.

The nature of the SEM's probe, energetic electrons, makes it uniquely suited to examining the optical and electronic properties of semiconductor materials. The high-energy electrons from the SEM beam will inject charge carriers into the semiconductor. Thus, beam electrons lose energy by promoting electrons from the valence band into the conduction band, leaving behind holes.

In a direct bandgap material, recombination of these electron-hole pairs will result in cathodoluminescence; if the sample contains an internal electric field, such as is present at a p-n junction, the SEM beam injection of carriers will cause electron beam induced current (EBIC) to flow. Cathodoluminescence and EBIC are referred to as "beam-injection" techniques, and are very powerful probes of the optoelectronic behavior of semiconductors, in particular for studying nanoscale features and defects.

Cathodoluminescence, the emission of light when atoms excited by high-energy electrons return to their ground state, is analogous to UV-induced fluorescence, and some materials such as zinc sulfide and some fluorescent dyes, exhibit both phenomena. Over the last decades, cathodoluminescence was most commonly experienced as the light emission from the inner surface of the cathode ray tube in television sets and computer CRT monitors. In the SEM, CL detectors either collect all light emitted by the specimen or can analyse the wavelengths emitted by the specimen and display an emission spectrum or an image of the distribution of cathodoluminescence emitted by the specimen in real color.

Characteristic X-rays that are produced by the interaction of electrons with the sample may also be detected in an SEM equipped for energy-dispersive X-ray spectroscopy or wavelength dispersive X-ray spectroscopy. Analysis of the x-ray signals may be used to map the distribution and estimate the abundance of elements in the sample.

SEM is not a camera and the detector is not continuously image-forming like a CCD array or film. Unlike in an optical system, the resolution is not limited by the diffraction limit, fineness of lenses or mirrors or detector array resolution. The focusing optics can be large and coarse, and the SE detector is fist-sized and simply detects current. Instead, the spatial resolution of the SEM depends on the size of the electron spot, which in turn depends on both the wavelength of the electrons and the electron-optical system that produces the scanning beam. The resolution is also limited by the size of the interaction volume, the volume of specimen material that interacts with the electron beam. The spot size and the interaction volume are both large compared to the distances between atoms, so the resolution of the SEM is not high enough to image individual atoms, as is possible with a transmission electron microscope (TEM). The SEM has compensating advantages, though, including the ability to image a comparatively large area of the specimen; the ability to image bulk materials (not just thin films or foils); and the variety of analytical modes available for measuring the composition and properties of the specimen. Depending on the instrument, the resolution can fall somewhere between less than 1 nm and 20 nm. As of 2009, The world's highest resolution conventional (≤30 kV) SEM can reach a point resolution of 0.4 nm using a secondary electron detector.

Conventional SEM requires samples to be imaged under vacuum, because a gas atmosphere rapidly spreads and attenuates electron beams. As a consequence, samples that produce a significant amount of vapour, e.g. wet biological samples or oil-bearing rock, must be either dried or cryogenically frozen. Processes involving phase transitions, such as the drying of adhesives or melting of alloys, liquid transport, chemical reactions, and solid-air-gas systems, in general cannot be observed with conventional high-vacuum SEM. In environmental SEM (ESEM), the chamber is evacuated of air, but water vapor is retained near its saturation pressure, and the residual pressure remains relatively high. This allows the analysis of samples containing water or other volatile substances. With ESEM, observations of living insects have been possible.

The first commercial development of the ESEM in the late 1980s allowed samples to be observed in low-pressure gaseous environments (e.g. 1–50 Torr or 0.1–6.7 kPa) and high relative humidity (up to 100%). This was made possible by the development of a secondary-electron detector capable of operating in the presence of water vapour and by the use of pressure-limiting apertures with differential pumping in the path of the electron beam to separate the vacuum region (around the gun and lenses) from the sample chamber. The first commercial ESEMs were produced by the ElectroScan Corporation in USA in 1988. ElectroScan was taken over by Philips (who later sold their electron-optics division to FEI Company) in 1996.

ESEM is especially useful for non-metallic and biological materials because coating with carbon or gold is unnecessary. Uncoated plastics and elastomers can be routinely examined, as can uncoated biological samples. This is useful because coating can be difficult to reverse, may conceal small features on the surface of the sample and may reduce the value of the results obtained. X-ray analysis is difficult with a coating of a heavy metal, so carbon coatings are routinely used in conventional SEMs, but ESEM makes it possible to perform X-ray microanalysis on uncoated non-conductive specimens; however some specific for ESEM artifacts are introduced in X-ray analysis. ESEM may be the preferred for electron microscopy of unique samples from criminal or civil actions, where forensic analysis may need to be repeated by several different experts. It is possible to study specimens in liquid with ESEM or with other liquid-phase electron microscopy methods.

The SEM can also be used in transmission mode by simply incorporating an appropriate detector below a thin specimen section. Detectors are available for bright field, dark field, as well as segmented detectors for mid-field to high angle annular dark-field. Despite the difference in instrumentation, this technique is still commonly referred to as scanning transmission electron microscopy (STEM).

Electron microscopes do not naturally produce color images, as an SEM produces a single value per pixel; this value corresponds to the number of electrons received by the detector during a small period of time of the scanning when the beam is targeted to the (x, y) pixel position.

This single number is usually represented, for each pixel, by a grey level, forming a "black-and-white" image. However, several ways have been used to get color electron microscopy images.

The easiest way to get color is to associate to this single number an arbitrary color, using a color look-up table (i.e. each grey level is replaced by a chosen color). This method is known as false color. On a BSE image, false color may be performed to better distinguish the various phases of the sample.

As an alternative to simply replacing each grey level by a color, a sample observed by an oblique beam allows researchers to create an approximative topography image (see further section "Photometric 3D rendering from a single SEM image"). Such topography can then be processed by 3D-rendering algorithms for a more natural rendering of the surface texture
Very often, published SEM images are artificially colored. This may be done for aesthetic effect, to clarify structure or to add a realistic appearance to the sample and generally does not add information about the specimen.

Coloring may be performed manually with photo-editing software, or semi-automatically with dedicated software using feature-detection or object-oriented segmentation.

In some configurations more information is gathered per pixel, often by the use of multiple detectors.

As a common example, secondary electron and backscattered electron detectors are superimposed and a color is assigned to each of the images captured by each detector, with an end result of a combined color image where colors are related to the density of the components. This method is known as density-dependent color SEM (DDC-SEM). Micrographs produced by DDC-SEM retain topographical information, which is better captured by the secondary electrons detector and combine it to the information about density, obtained by the backscattered electron detector.

Measurement of the energy of photons emitted from the specimen is a common method to get analytical capabilities. Examples are the energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors it is common to color code these extra signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal data, which is not modified in any way.

SEMs do not naturally provide 3D images contrary to SPMs. However 3D data can be obtained using an SEM with different methods as follows.


This method typically uses a four-quadrant BSE detector (alternatively for one manufacturer, a 3-segment detector). The microscope produces four images of the same specimen at the same time, so no tilt of the sample is required. The method gives metrological 3D dimensions as far as the slope of the specimen remains reasonable. Most SEM manufacturers now (2018) offer such a built-in or optional four-quadrant BSE detector, together with proprietary software to calculate a 3D image in real time.
Other approaches use more sophisticated (and sometimes GPU-intensive) methods like the optimal estimation algorithm and offer much better results at the cost of high demands on computing power.

In all instances, this approach works by integration of the slope, so vertical slopes and overhangs are ignored; for instance, if an entire sphere lies on a flat, little more than the upper hemisphere is seen emerging above the flat, resulting in wrong altitude of the sphere apex. The prominence of this effect depends on the angle of the BSE detectors with respect to the sample, but these detectors are usually situated around (and close to) the electron beam, so this effect is very common.

This method requires an SEM image obtained in oblique low angle lighting. The grey-level is then interpreted as the slope, and the slope integrated to restore the specimen topography. This method is interesting for visual enhancement and the detection of the shape and position of objects ; however the vertical heights cannot usually be calibrated, contrary to other methods such as photogrammetry.



One possible application is measuring the roughness of ice crystals. This method can combine variable-pressure environmental SEM and the 3D capabilities of the SEM to measure roughness on individual ice crystal facets, convert it into a computer model and run further statistical analysis on the model. Other measurements include fractal dimension, examining fracture surface of metals, characterization of materials, corrosion measurement, and dimensional measurements at the nano scale (step height, volume, angle, flatness, bearing ratio, coplanarity, etc.).

The following are examples of images taken using an SEM.





</doc>
<doc id="28044" url="https://en.wikipedia.org/wiki?curid=28044" title="Timeline of the September 11 attacks">
Timeline of the September 11 attacks

The following timeline is a chronological list of all the major events leading up to, during, and immediately following the terrorist attacks on New York and Washington, DC on September 11, 2001. The timeline starts with the completion of the first World Trade Center tower in 1970 through the first anniversary of the attacks in 2002.



All times are in local time (EDT or UTC − 4).







</doc>
<doc id="28045" url="https://en.wikipedia.org/wiki?curid=28045" title="Hijackers in the September 11 attacks">
Hijackers in the September 11 attacks

The hijackers in the September 11 attacks were 19 men affiliated with al-Qaeda. Fifteen of the 19 were citizens of Saudi Arabia, two were from the United Arab Emirates, one from Lebanon, and one from Egypt. The hijackers were organized into four teams, each led by a pilot-trained hijacker with three or four "muscle hijackers", who were trained to help subdue the pilots, passengers, and crew.

The first hijackers to arrive in the United States were Khalid al-Mihdhar and Nawaf al-Hazmi, who settled in San Diego County, California, in January 2000. They were followed by three hijacker-pilots, Mohamed Atta, Marwan al-Shehhi, and Ziad Jarrah in mid-2000 to undertake flight training in South Florida. The fourth hijacker-pilot, Hani Hanjour, arrived in San Diego in December 2000. The rest of the "muscle hijackers" arrived in early- and mid-2001.

Khalid al-Mihdhar and Nawaf al-Hazmi were both experienced and respected jihadists in the eyes of al-Qaeda leader Osama bin Laden.

As for the pilots who would go on to participate in the attacks, three of them were original members of the Hamburg cell (Mohamed Atta, Marwan al-Shehhi and Ziad Jarrah). Following their training at al-Qaeda training camps in Afghanistan, they were chosen by Bin Laden and al-Qaeda's military wing due to their extensive knowledge of western culture and language skills, increasing the mission's operational security and its chances for success. The fourth intended pilot, Ramzi bin al-Shibh, a member of the Hamburg cell, was also chosen to participate in the attacks yet was unable to obtain a visa for entry into the United States. He was later replaced by Hani Hanjour, a Saudi national.

Mihdhar and Hazmi were also potential pilot hijackers, but did not do well in their initial pilot lessons in San Diego. Both were kept on as "muscle" hijackers, who would help overpower the passengers and crew and allow the pilot hijackers to take control of the flights. In addition to Mihdhar and Hazmi, thirteen other muscle hijackers were selected in late 2000 or early 2001. All were from Saudi Arabia, with the exception of Fayez Banihammad, who was from the United Arab Emirates.

Hijackers: Mohamed Atta (Egyptian), Abdulaziz al-Omari (Saudi Arabian), Wail al-Shehri (Saudi Arabian), Waleed al-Shehri (Saudi Arabian), Satam al-Suqami (Saudi Arabian).

Two flight attendants called the American Airlines reservation desk during the hijacking. Betty Ong reported that "the five hijackers had come from first-class seats: 2A, 2B, 9A, 9C and 9B." Flight attendant Amy Sweeney called a flight services manager at Logan Airport in Boston and described them as Middle Eastern. She gave the staff the seat numbers and they pulled up the ticket and credit card information of the hijackers, identifying Mohamed Atta.

Mohamed Atta's voice was heard over the air traffic control system, broadcasting messages thought to be intended for the passengers.

Hijackers: Marwan al-Shehhi (United Arab Emirates), Fayez Banihammad (United Arab Emirates), Mohand al-Shehri (Saudi Arabian), Hamza al-Ghamdi (Saudi Arabian), Ahmed al-Ghamdi (Saudi Arabian).

A United Airlines mechanic was called by a flight attendant who stated the crew had been murdered and the plane hijacked.

Hijackers: Hani Hanjour (Saudi Arabian), Khalid al-Mihdhar (Saudi Arabian), Majed Moqed (Saudi Arabian), Nawaf al-Hazmi (Saudi Arabian), Salem al-Hazmi (Saudi Arabian).

Two hijackers, Hani Hanjour and Majed Moqed were identified by clerks as having bought single, first-class tickets for Flight 77 from Advance Travel Service in Totowa, New Jersey with $1,842.25 in cash. Renee May, a flight attendant on Flight 77, used a cell phone to call her mother in Las Vegas. She said her flight was being hijacked by six individuals who had moved them to the rear of the plane. Unlike the other flights, there was no report of stabbings or bomb threats. According to the 9/11 Commission Report, it is possible that pilots were not stabbed to death and were sent to the rear of the plane. One of the hijackers, most likely Hanjour, announced on the intercom that the flight had been hijacked.
Passenger Barbara Olson called her husband, Theodore Olson, the Solicitor General of the United States, stating the flight had been hijacked and the hijackers had knives and box cutters. Two of the passengers had been on the FBI's terrorist-alert list: Khalid al-Mihdhar and Nawaf al-Hazmi. Al-Mihdhar and Nawaf al-Hazmi flew to Los Angeles in January 2000 and later took flying lessons in San Diego, during which time they were allegedly assisted by Omar al-Bayoumi and Saudi diplomats Fahad al-Thumairy and Mussaed Ahmed al-Jarrah.

Hijackers: Ziad Jarrah (Lebanese), Ahmed al-Haznawi (Saudi Arabian), Ahmed al-Nami (Saudi Arabian), Saeed al-Ghamdi (Saudi Arabian).

Passenger Jeremy Glick stated that the hijackers were Arabic-looking, wearing red headbands, and carrying knives.

Spoken messages (from Ziad Jarrah) intended for passengers were broadcast over the air traffic control system, presumably by mistake:

Jarrah is also heard on the cockpit voice recorder. In addition, DNA samples submitted by his girlfriend were matched to remains recovered in Shanksville.

Before the attacks, FBI agent Robert Wright, Jr. had written vigorous criticisms of FBI's alleged incompetence in investigating terrorists residing within the United States. Wright was part of the Bureau's Chicago counter-terrorism task force and involved in project Vulgar Betrayal, which was linked to Yasin al-Qadi.

According to James Bamford, the NSA had picked up communications of al-Mihdhar and al-Hazmi back in 1999, but had been hampered by internal bureaucratic conflicts between itself and the CIA and did not do a full analysis of the information it passed on to the agency. For example, it only passed on the first names, Nawaf and Khalid.

Bamford also claims that the CIA's Alec Station (a unit assigned to bin Laden) knew that al-Mihdhar was planning to come to New York as far back as January 2000. Doug Miller, one of three FBI agents working inside the CIA station, tried to send a message (a CIR) to the FBI to alert them about this, so they could put al-Mihdhar on a watch list. His CIA boss, Tom Wilshire, deputy station chief, allegedly denied permission to Miller. Miller asked his associate Mark Rossini for advice; Rossini pressed Wilshire's deputy but was rebuffed also.

Bamford also claims that al-Mihdhar and Hazmi wound up living with Abdussattar Shaikh for a time to save money. Shaikh was, coincidentally, an FBI informant, but since they never acted suspiciously around him, he never reported them. The CIA Bangkok station told Alec Station that Hazmi had gone to Los Angeles. None of this information made it back to the FBI headquarters.

Within minutes of the attacks, the Federal Bureau of Investigation opened the largest FBI investigation in United States history, operation PENTTBOM. The suspects were identified within 72 hours because few made any attempt to disguise their names on flight and credit card records. They were also among the few non-U.S. citizens and nearly the only passengers with Arabic names on their flights, enabling the FBI to identify them using such details as dates of birth, known or possible residences, visa status, and specific identification of the suspected pilots. On September 27, 2001, the FBI released photos of the 19 hijackers, along with information about many of their possible nationalities and aliases. The suspected hijackers were from Saudi Arabia (fifteen hijackers), United Arab Emirates (two hijackers), Lebanon (one hijacker) and Egypt (one hijacker).

The passport of Satam al-Suqami was reportedly recovered "a few blocks from where the World Trade Center's twin towers once stood"; a passerby picked it up and gave it to a NYPD detective shortly before the towers collapsed. The passports of two other hijackers, Ziad Jarrah and Saeed al-Ghamdi, were recovered from the crash site of United Airlines Flight 93 in Pennsylvania, and a fourth passport, that of Abdulaziz al-Omari was recovered from luggage that did not make it onto American Airlines Flight 11.

According to the 9/11 Commission Report, 26 al-Qaeda terrorist conspirators sought to enter the United States to carry out a suicide mission. In the end, the FBI reported that there were 19 hijackers in all: five on three of the flights, and four on the fourth. On September 14, three days after the attacks, the FBI announced the names of 19 persons. After a controversy about an earlier remark, U.S. Homeland Security Secretary Janet Napolitano stated in May 2009 that the 9/11 Commission found that none of the hijackers entered the United States through Canada.

Nawaf al-Hazmi and Hani Hanjour, attended the Dar al-Hijrah Falls Church, Virginia, Islamic Center where the Imam Anwar al-Awlaki preached, in early April 2001. Through interviews with the FBI, it was discovered that Awlaki had previously met Nawaf al-Hazmi several times while the two lived in San Diego. At the time, Hazmi was living with Khalid al-Mihdhar, another 9/11 hijacker. The hijackers of the same plane often had very strong ties as many of them attended school together or lived together prior to the attacks.

Soon after the attacks and before the FBI had released the pictures of all the hijackers, several reports claimed some of the men named as hijackers on 9/11 were alive and had their identities stolen.





</doc>
<doc id="28046" url="https://en.wikipedia.org/wiki?curid=28046" title="Closings and cancellations following the September 11 attacks">
Closings and cancellations following the September 11 attacks

Many closings and cancellations followed the September 11, 2001 attacks, including major landmarks, buildings, restrictions on access to Lower Manhattan, and postponement or cancellation of major sporting and other events. Landmarks were closed primarily because of fears that they may be attacked. At some places, streets leading up to the institutions were also closed. When they reopened, there was heightened security. Many states declared a state of emergency.

Speaking at a press conference at 11:02 a.m. on the morning of the attacks, Mayor Rudy Giuliani told New Yorkers: "If you are south of Canal Street, get out. Walk slowly and carefully. If you can't figure what else to do, just walk north." The neighborhood was covered in dust and debris, and electrical failures caused traffic light outages. Emergency vehicles were given priority to respond to ongoing fires, building collapses, and expected mass casualties. Over a million workers and residents south of Canal Street evacuated, and police stopped pedestrians from entering lower Manhattan. With subways shut down, vehicle traffic restricted, and tunnels closed, they mainly fled on foot, pouring over bridges and ferries to Brooklyn and New Jersey.

On September 12, vehicle traffic was banned south of 14th Street, subway stations south of Canal Street were bypassed, and pedestrians were not permitted below Chambers Street. Vehicle traffic below Canal Street was not allowed until October 13.

The New York Stock Exchange did not open on September 11 even as CNBC showed futures numbers early in the day. As Wall Street was covered in debris from the World Trade Center and suffered infrastructure damage, it remained closed until September 17.

For at least a full day after the attacks, bridges and tunnels to Manhattan were closed to non-emergency traffic in both directions. Among other things, this interrupted scheduled deliveries of food and other perishables, leading to shortages in restaurants. From September 27, 2001, one-occupant cars were banned from crossing into Lower Manhattan from Midtown on weekday mornings in an effort to relieve some of the crush of traffic in the city (the morning rush hour lasts from 5:30 a.m. to noon), caused largely by the increased security measures and closure of major vehicle and transit crossings.

The tracks and stations under the WTC were shut down within minutes of the first plane crash. All remaining New York City Subway service was suspended from 10:20 a.m. to 12:48 p.m. Immediately after the attacks and more so after the collapses of the Twin Towers, many trains running in Lower Manhattan lost power and had to be evacuated through the tunnels. Some trains had power but the signals did not, requiring special operating procedures to ensure safety.

The IRT Broadway–Seventh Avenue Line, which ran below the World Trade Center between Chambers Street and Rector Street, was the most crippled. This section of the tunnel, including Cortlandt Street station (located directly underneath the World Trade Center), was badly damaged, and had to be rebuilt. Service was immediately suspended south of Chambers Street and then cut back to 14th Street. There was also subsequent flooding on the line south of 34th Street–Penn Station. After the flood was cleaned up, express service was able to resume on September 17 with trains running between Van Cortlandt Park–242nd Street and 14th Street, making local stops north of and express stops south of 96th Street, while and trains made all stops in Manhattan (but bypassed all stations between Canal Street and Fulton Street until October 1). 1/9 skip-stop service was suspended.

After a few switching delays at 96th Street, service was changed on September 19. The train resumed local service in Manhattan, but was extended to New Lots Avenue in Brooklyn (switching onto the express tracks at Chambers Street) to replace the 3, which now terminated at 14th Street as an express. The train continued to make local stops in Manhattan and service between Chambers Street and South Ferry as well as skip-stop service remained suspended. Normal service on all four trains was restored September 15, 2002, but Cortlandt Street remained closed until September 8, 2018.

Service on the BMT Broadway Line was also disrupted because the tracks from the Montague Street Tunnel run adjacent to the World Trade Center and there were concerns that train movements could cause unsafe settling of the debris pile. Cortlandt Street station, which sits under Church Street, sustained significant damage in the collapse of the towers. It was closed until September 15, 2002 for removal of debris, structural repairs, and restoration of the track beds, which had suffered flood damage in the aftermath of the collapse. Starting September 17, 2001, and service was suspended and respectively replaced by the (which was extended to Coney Island–Stillwell Avenue via the BMT Montague Street Tunnel, BMT Fourth Avenue Line, and BMT Sea Beach Line) and the (also extended via Fourth Avenue to Bay Ridge–95th Street). In Queens, the replaced the while the replaced the . All service on the BMT Broadway Line ran local north of Canal Street except for the <Q>, which ran normally from 57th Street to Brighton Beach via Broadway and Brighton Express. J/Z skip-stop service was suspended at this time. Normal service on all seven trains resumed on October 28.

The only subway line running between Midtown and Lower Manhattan was the IRT Lexington Avenue Line, which was overcrowded before the attacks and at crush density until the BMT Broadway Line reopened. Wall Street was closed until September 21.

The IND Eighth Avenue Line, which has a stub terminal serving the train under Five World Trade Center, was undamaged, but covered in soot. E trains were extended to Euclid Avenue, Brooklyn, replacing the then suspended train (the and trains replaced it as the local north of 59th Street–Columbus Circle on nights and weekends, respectively. The train, which ran normally from 145th Street or Bedford Park Boulevard to 34th Street–Herald Square via Central Park West Local, also replaced C trains on weekdays). Service was cut back to Canal Street when C service resumed on September 21, but Chambers Street and Broadway–Nassau Street remained closed until October 1. World Trade Center remained closed until January 2002.

There were no reported casualties on the subway or loss of train cars, but an MCI coach bus was destroyed. Another bus was damaged, but repaired and is back in normal service with a special commemoration livery.

PATH started evacuating passengers from its Manhattan trains and tracks within minutes of the first plane crash. The PATH station at World Trade Center was heavily damaged (a train parked in the station was crushed by debris and was removed during the excavation process in January 2002) and all service there was suspended. For several hours, PATH did not run any trains to Manhattan, but was able to restore service on the Uptown Hudson Tubes to 33rd Street by the afternoon. Exchange Place was unusable since the switch configuration at the time required all trains to continue to World Trade Center. As a result, PATH ran a modified service: Hoboken-Journal Square, Hoboken-33rd Street, and Newark-33rd Street. Exchange Place reopened with modifications on June 29, 2003; a temporary station replacing World Trade Center opened on November 23.

Liberty Water Taxi and NY Waterway had a ferry terminal at the World Financial Center. As the area around the terminal was in the restricted zone, NY Waterway suspended service to the terminal with alternate service going to Midtown and Wall Street and Liberty Water Taxi service was suspended. Free ad-hoc ferry service to New Jersey, Brooklyn, and Queens began by evening, with about half a million evacuees transported by Circle Line Tours, NY Waterway, privately owned dining boats, tug boats, and at least one fire boat.

MTA buses were temporarily suspended south of Canal Street, and MTA and NJ Transit buses were re-routed to serve passengers arriving in Brooklyn and New Jersey by walking and taking ferries out of Manhattan.

The Port Authority Bus Terminal was closed until September 13. Amtrak suspended all of its rail service nationwide until 6pm. Greyhound Bus Lines cancelled its bus service in the Northeast, but was running normally by September 13.

The entire airspaces of the United States and Canada were closed ("ground stop") by order of FAA National Operations Manager Ben Sliney (who was working his first day in that position) except for military, police, and medical flights. The unprecedented implementation of Security Control of Air Traffic and Air Navigation Aids (SCATANA) was the first unplanned closure in the U.S.; military exercises known as Operation Skyshield had temporarily closed the airspace in the early 1960s. Domestic planes were diverted to the nearest available airport. All non-military flights needed specific approval from the United States Air Force and the FAA. There were only a few dozen private aircraft which received approvals in that time period. Civil Air Patrol's aerial photography unit was the earliest non-military flight granted approval. United Airlines cancelled all flights worldwide temporarily. Grounded passengers and planes were searched for security threats. Amtrak was closed until 6pm on September 11, but by September 13 it had increased capacity 30% to deal with an influx of stranded plane passengers. President George W. Bush was transported to a secure location via Air Force One.

Many incoming international flights were diverted to Atlantic Canada to avoid proximity to potential targets in the US and large cities in Canada. Some international flights that departed from South America were diverted to Mexico, however its airspace was not shut down. On Thursday night, the New York area airports (JFK, LaGuardia, and Newark) were closed again and reopened the next morning. The only traffic from LaGuardia during the closure was a single C-9C government VIP jet, departing at approximately 5:15 p.m. on the 12th.

Civilian air traffic was allowed to resume on September 13, 2001, with stricter airport security checks, disallowing for example the box cutting knives that were used by the hijackers. (Reinforcement of cockpit doors began in October 2001, and was required for larger airlines by 2003.) First, stranded planes were allowed to fly to their intended destinations, then limited service resumed. The backlog of delayed passengers took several days to clear.

Due to a translation error, controllers believed Korean Air Flight 85 might have been hijacked. Canadian Prime Minister Jean Chrétien and U.S. authorities ordered the United States Air Force to surround the plane and force it to land in Whitehorse, Yukon and to shoot down the plane if the pilots did not cooperate. Alaska Governor Tony Knowles ordered the evacuation of large hotels and government buildings in Anchorage. Also in Alaska at nearby Valdez, the U.S. Coast Guard ordered all tankers filling up with oil to head out to sea. Canadian officials evacuated all schools and large buildings in Whitehorse before the plane landed safely.

Many businesses across the United States closed after the intentional nature of the events became clear, and many national landmarks and financial district skyscrapers were evacuated out of fear of further attacks.



In an atmosphere reminiscent of the assassination of John F. Kennedy in 1963, everyday life around the world came to a standstill in the days after the September 11 attacks. For this reason, as well as for reasons of perceived threat associated with large gatherings, many events were postponed or cancelled. Other events were also cancelled, postponed, or modified:


</doc>
<doc id="28047" url="https://en.wikipedia.org/wiki?curid=28047" title="Memorials and services for the September 11 attacks">
Memorials and services for the September 11 attacks

The first memorials to the victims of the September 11 attacks in 2001 began to take shape online, as hundreds of webmasters posted their own thoughts, links to the Red Cross and other rescue agencies, photos, and eyewitness accounts. Numerous online September 11 memorials began appearing a few hours after the attacks, although many of these memorials were only temporary. Around the world, U.S. embassies and consulates became makeshift memorials as people came out to pay their respects.

The "Tribute in Light" was the first major physical memorial at the World Trade Center site. A permanent memorial and museum, the National September 11 Memorial & Museum at the World Trade Center, were built as part of the design for overall site redevelopment. The Memorial consists of two massive pools set within the original footprints of the Twin Towers with waterfalls cascading down their sides. The names of the victims of the attacks are inscribed around the edges of the waterfalls. Other permanent memorials have been constructed around the world.

One of the places that saw many memorials and candlelight vigils was Pier A in Hoboken, New Jersey. There was also a memorial service on March 11, 2002, at dusk on Pier A when the "Tribute in Light" first turned on, marking the half-year anniversary of the terrorist attack. A permanent September 11 memorial for Hoboken, called Hoboken Island, was chosen in September 2004.

Soon after the attacks, temporary memorials were set up in New York and elsewhere.


In Europe, annually a commemoration of September 11 to never forget, Nissoria was one of the first public places that dedicated a memorial to September 11 in Europe. Nissoria is in a small town located in the Province of Enna in Sicily, Italy. Two family members of this community, Vincenzo DiFazio and Salvatore Lopez, lost their lives on Sept 11 at the World Trade Center. 

The then-mayor Dr. Marco Murgo along with the Chiara family Benito Sr. and son Mario developed the project to dedicate a small plot of land adjacent to a local school and museum that was entitled “Parco 11 Settembre”. The Commanding Officer of the nearby U.S. Naval Air Station Sigonella met with this delegation from Nissoria and embraced this truly heartfelt initiative. 

Ever since its dedication, and thanks to the present-day Mayor of Nissoria Dott. Armando Glorioso and Dott. Alberto Lunetta who continue this important event, a representation of both American and Italian military personnel from the nearby Military base NAS Sigonella come to visit and annually commemorate along with all local Italian Authorities, Dignitaries and citizens who truly will never forget this tragic event. 







The Raoul Wallenberg Award was given to New York City in 2001 "For all of its citizens who searched for the missing, cared for the injured, gave comfort to loved ones of the missing or lost, and provided sustenance and encouragement to those who searched through the rubble at Ground Zero."

On February 3, 2002, during the Halftime Show of Super Bowl XXXVI, rock group U2 performed Where the Streets Have No Name, while the names of the victims were projected onto banners. Bono opened his jacket to reveal a U.S. flag pattern sewn in the inside lining.
At the opening ceremony of the 2002 Winter Olympics in Salt Lake City on February 8, a tattered American flag recovered from the World Trade Center site was carried into the stadium by American athletes, members of the Port Authority police, and members of the New York City police and fire departments.

On February 23, 2003, the 45th Annual Grammy Awards were held at Madison Square Garden and paid tribute to those who died during the 9/11 attacks, to whom the ceremony was dedicated. Ceremony host Bruce Springsteen performed "The Rising" at the Awards.

American country singer Darryl Worley paid tribute to the people with his 2003 single, "Have You Forgotten?" from the album of the same name.

Newark International Airport was renamed "Newark Liberty International Airport".

On September 11, 2002, representatives from over 90 countries came to Battery Park City as New York City Mayor Michael Bloomberg lit an eternal flame to mark the first anniversary of the attacks. Leading the dignitaries were Canadian Prime Minister Jean Chrétien, U.N. Secretary General Kofi Annan, Bloomberg, and Secretary of State Colin Powell. The same day, the Victims of Terrorist Attack on the Pentagon Memorial was dedicated at Arlington National Cemetery near the Pentagon. The memorial is dedicated to the five individuals at the Pentagon whose remains were never found, and the partial remains of another 25 victims are buried beneath the memorial. The names of the 184 victims of the Pentagon attack are inscribed on the memorial's side.

A 9/11 memorial public sculpture by Ingrid Lahti was on display at Bellevue Downtown Park, Bellevue, Washington; it was installed September 11, 2002, and displayed through October.

Many organizations held memorial services and events for the 10th anniversary of the attacks.


The National 9/11 Flag was made from a tattered remains of a American flag found by recovery workers in the early morning of September 12, 2001. It was hanging precariously from some scaffolding at a construction site next to Ground Zero. Because of safety reasons the flag could not be taken down until late October 2001. Charlie Vitchers, a construction superintendent for the Ground Zero cleanup effort, had a crew recover the flag. It was placed in storage for seven years.

The flag has made a number appearances across the country including a Boston Red Sox Game, a New York Giants Home Opener, and the USS "New York" Commissioning Ceremony. It also appeared on the CBS Evening News and on ABC World News Tonight "Persons of the Week."

The flag began a national tour on Flag day, which was on June 14, 2009. It will visit all 50 states where service heroes, veterans, and other honorees will each add stitching and material from other retired American flags in order to restore the original 13 stripes of the flag. The flag will have a permanent home at the National September 11 Memorial and Museum.

The 9-11 Remembrance Flag was created to be a permanent reminder of the thousands of people lost in the September 11 attacks. The purpose of keeping the memories of September 11 alive is not to be forever mourning, but for "learning from the circumstances and making every effort to prevent similar tragedies in our future." The flag is also meant to be a reminder of how the people of this country came together to help each other after the attacks. The red background of the flag represents the blood shed by Americans for their country. The stars represent the lost airplanes and their passengers. The blue rectangles stand for the twin towers and the white pentagon represents the Pentagon building. The blue circle symbolizes the unity of this country after the attacks.

The 9/11 National Remembrance Flag was designed by Stephan and Joanne Galvin soon after September 11, 2001. They wanted to do something to help and were inspired by a neighbor's POW/MIA flag. They wanted to sell the flag so people would remember the September 11 attacks and in order to raise money for relief efforts. The blue represents the colors of the state flags that were involved in the attacks. The black represents sorrow for innocent lives lost. The four stars stand for the four planes that crashed and the lives lost, both in the crash and in the rescue efforts, as well as the survivors. The blue star is a representation of American Airlines Flight 77 and the Pentagon. The two white stars represent American Airlines Flight 11 and United Airlines flight 175, as well as the twin towers. The red star stands for United Flight 93 that crashed in Shanksville, Pennsylvania and all those who sacrifice their lives to protect the innocent. The colors of the stars represent the American flag. The four stars are touching each other and the blue parts of the flag in order to symbolize the unity of the people of the United States.

The National Flag of Honor and the National Flag of Heroes were created by John Michelotti for three main reasons: (1)"To immortalize the individual victims that were killed in the terrorist attacks of September 11, 2001." (2)"To give comfort to the families left behind knowing that their loved one will be forever honored and remembered." (2)"To create an enduring symbol, recognized by the world, of the human sacrifice that occurred on September 11, 2001."

The Flag of Honor and the Flag of Heroes are based on the American flag. They both have the names of all the innocent people who were killed in the September 11 attacks printed on the red and white stripes of the American Flag. Both flags have a white space across the bottom with the name of the flag and a description printed in black. The Flag of Honor reads: "This flag contains the names of those killed in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them" The Flag of Heroes reads: " This flag contains the names of the emergency service personnel who gave their lives to save others in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them."

The Flag of Honor and the Flag of Heroes were featured at the NYC 9/11 Memorial Field 5th Anniversary in Manhattan's Inwood Hill Park September 8–12, 2006. There 3,000 flags which represented those who died in the September 11 attacks. The flags were also featured on the msnbc Today Show and on ABC 13 News, Norfolk, VA.

The Remembrance Flag has a white background with large, black Roman numerals IX/XI in the center and four black stars across the top. The IX/XI are the Roman numerals for 9/11. The four stars represent World Trade Center North, World Trade Center South, the Pentagon, and Shanksville, PA.

The 10th Anniversary September 11 Memorial Flag was designed by Carrot-Top Industries, a privately owned company in Hillsborough, NC. The exclusive 9/11 memorial flag was designed with the two World Trade Towers set inside a pentagon decorated with a ribbon to commemorate all of the Americans that lost their lives on September 11, 2001.

The growing popularity of virtual worlds such as Secondlife has led to the construction of permanent virtual memorials and exhibits. Examples include:

On September 11, 2007, a virtual reality World Trade Center Memorial will be presented to the people of the world. The location is in Second Life, on the island we have named after the original design: Celestial Requiem NYC. We have built this memorial because, to be blunt, the world needed it done years ago, and the two years longer to await the completion of the "Reflected Absence" memorial in New York city (by Michael Arad and Peter Walker) was in our opinion two years too long.






</doc>
<doc id="28051" url="https://en.wikipedia.org/wiki?curid=28051" title="Airport security repercussions due to the September 11 attacks">
Airport security repercussions due to the September 11 attacks

After the September 11 attacks, questions were raised regarding the effectiveness of airport security at the time, as all 19 hijackers involved in 9/11 managed to pass existing checkpoints and board the airplanes without incident. In the months and years following September 11, 2001, security at many airports worldwide was escalated to deter similar terrorist plots.

Prior to September 11, 2001, airport screening was provided in the U.S. by private security companies contracted by the airline or airport. In November 2001, the Transportation Security Administration (TSA) was introduced to take over all of the security functions of the FAA, the airlines, and the airports. Among other changes introduced by TSA, bulletproof and locked cockpit doors became standard on commercial passenger aircraft.

In some countries, for example Sweden, Norway and Finland, there were no or only random security checks for domestic flights in 2001 and before that. On or quickly after September 11, decisions were made to introduce full security checks there. It was immediately implemented where possible, but took one to two years to implement everywhere since terminals were often not prepared with room for it.

Cockpit doors on many aircraft are reinforced and bulletproof to prevent unauthorized access. Passengers are now prohibited from entering the cockpit during flight. Some aircraft are also equipped with CCTV cameras, so the pilots can monitor cabin activity. Pilots are now allowed to carry firearms, but they must be trained and licensed. In the U.S., more air marshals have been placed on flights to improve security.

On September 11, hijackers Khalid al-Mihdhar, Majed Moqed, Nawaf al-Hazmi and Salem al-Hazmi all set off the metal detector. Despite being scanned with a hand-held detector, the hijackers were passed through. Security camera footage later showed some hijackers had what appeared to be box cutters clipped to their back pockets. Box cutters and similar small knives were allowed on board certain aircraft at the time.

Airport checkpoint screening has been significantly tightened since 2001, and security personnel are more thoroughly trained to detect weapons or explosives. In addition to standard metal detectors, many U.S. airports now employ full-body scanning machines, in which passengers are screened with millimeter wave technology to check for potential hidden weapons or explosives on their persons. Initially, early body scanners provoked quite a bit of controversy because the images produced by the machines were deemed graphic and intrusive. Many considered this an invasion of personal privacy, as TSA screeners were essentially shown an image of each passenger's naked body. Newer body scanners have since been introduced which do not produce an image, but rather alert TSA screeners of areas on the body where an unknown item or substance may be hidden. A TSA security screener then inspects the indicated area(s) manually.

On September 11, some hijackers lacked proper identification, yet they were allowed to board due to being on domestic aircraft. After 9/11, all passengers 18 years or older must now have valid, government-issued identification in order to fly. Airports may check the ID of any passenger (and staff member) at any time to ensure the details on the ID match those on the printed boarding pass. Only under exceptional circumstances may an individual fly without a valid ID. If approved for flying without an ID, the individual will be subject to extra screening of their person and their carry-on items. TSA does not have the capability to conduct background checks on passengers at checkpoints. Sensitive areas in airports, including airport ramps and operational spaces, are restricted from the general public. Called a SIDA (Security Identification Display Area) in the U.S., these spaces require special qualifications to enter.

A European Union regulation demanded airlines make sure that the individual boarding the aircraft is the same individual who checked in his or her luggage; this was implemented by verifying an individual's identification both at luggage check-in and when boarding.

Some countries also fingerprint travellers or use retina and iris scanning to help detect potential criminals. 

With regard to the 2015 Germanwings flight 9525 crash incident, a suicide by pilot where the captain was unable to regain access to the flight deck, some have stated that security features added to commercial airliners after 9/11 actually work against the safety of such planes.
In 2003, John Gilmore sued United Airlines, Southwest Airlines, and then-U.S. Attorney General John Ashcroft, arguing that requiring passengers to show identification before boarding domestic flights is tantamount to an internal passport, and is unconstitutional. Gilmore lost the case, known as "Gilmore v. Gonzales", and an appeal to the U.S. Supreme Court was denied.



</doc>
<doc id="28061" url="https://en.wikipedia.org/wiki?curid=28061" title="U.S. government response to the September 11 attacks">
U.S. government response to the September 11 attacks

After the September 11, 2001 attacks, the U.S. government responded with immediate action (including rescue operations at the site of the World Trade Center and grounding civilian aircraft), and long-term action, including investigations, legislative changes, military action and restoration projects. Investigations into the motivations and execution of the attacks led to the declaration of War on Terrorism that lead to ongoing military engagements in Afghanistan and subsequently Iraq. Clean-up and restoration efforts led to the rebuilding of Lower Manhattan, and federal grants supported the development of the National September 11 Memorial & Museum.

Within hours of the attacks in New York, a massive search and rescue (SAR) operation was launched, which included over 350 search and rescue dogs. Initially, only a handful of wounded people were found at the site, and in the weeks that followed it became evident that there weren't any survivors to be found. Only twenty survivors were found alive in the rubble.
Rescue and recovery efforts took months to complete. It took several weeks to put out the fires burning in the rubble of the buildings, with the clean-up not being completed until May 2002. Temporary wooden "viewing platforms" were set up for tourists to view construction crews clearing out the gaping holes where the towers once stood. All of these platforms were closed on May 30, 2002.

Many relief funds were immediately set up to assist victims of the attacks, with the task of providing financial assistance to the survivors and the families of victims. By the deadline for victim's compensation of September 11, 2003, 2,833 applications had been received from the families of those killed.

In the aftermath of the attacks, many U.S. citizens held the view that the attacks had "changed the world forever." The Bush administration announced a war on terrorism, with the goal of bringing Osama bin Laden and al-Qaeda to justice and preventing the emergence of other terrorist networks. These goals would be accomplished by means including economic and military sanctions against states perceived as harboring terrorists and increasing global surveillance and intelligence sharing. Immediately after the September 11 attacks U.S. officials speculated on possible involvement by Saddam Hussein.

Because the attacks on the United States were judged to be within the parameters of its charter, NATO declared that Article 5 of the NATO agreement was satisfied on September 12, 2001, making the US war on terrorism the first time since its inception that NATO would actually participate in a "hot" war.

Following the attacks, 762 suspects were taken into custody in the United States. On December 12, 2001, Fox News reported that some 60 Israelis were among them. Federal investigators were reported to have described them as part of a long-running effort to spy on American government officials. A "handful" of these Israelis were described as active Israeli military or intelligence operatives.

In a letter to the editor, Ira Glaser, former head of the ACLU, claimed that none of those 762 detainees were charged with terrorism. "The Justice Department inspector general's report implies more than the violation of the civil liberties of 762 non-citizens. It also implies a dysfunctional and ineffective approach to protecting the public after Sept. 11, 2001... No one can be made safer by arresting the wrong people".

Immediately after opening the hunt on Osama bin Laden, President Bush also visited the Islamic Center of Washington and asked the public to view Arabs and Muslims living in the United States as American patriots.

Congress passed and President Bush signed the Homeland Security Act of 2002, creating the Department of Homeland Security, representing the largest restructuring of the U.S. government in contemporary history. Congress passed the USA PATRIOT Act, stating that it would help detect and prosecute terrorism and other crimes. Civil liberties groups have criticized the PATRIOT Act, saying that it allows law enforcement to invade the privacy of citizens and eliminates judicial oversight of law-enforcement and domestic intelligence gathering. The Bush Administration also invoked 9/11 as the reason to have the National Security Agency initiate a secret operation, "to eavesdrop on telephone and e-mail communications between the United States and people overseas without a warrant."

On June 6, 2002, Attorney General Ashcroft proposed regulations that would create a special registration program that required males aged 16 to 64 who were citizens of designated foreign nations resident in the U.S. to register with the Immigration and Naturalization Service (INS), have their identity verified, and be interviewed, photographed and fingerprinted. Called the National Security Entry-Exit Registration System (NSEERS), it comprised two programs, the tracking of arrivals and departures on the one hand, and voluntary registrations of those already in the U.S., known as the "call-in" program. The DOJ acted under the authority of the Immigration and Nationality Act of 1952, which had authorized a registration system but was allowed to lapse in the 1980s because of budget concerns. Ashcroft identified those required to register as "individuals of elevated national security concern who stay in the country for more than 30 days."

The processing of arrivals as part of their customs screening began in October 2002. It first focused on arrivals from Iran, Iraq, Libya, Sudan, and Syria. It handled 127,694 people before being phased out as universal screening processes were put in place.

The "call-in" registrations began in December. It initially applied to nationals of five countries, Iran, Iraq, Syria, Libya and Sudan, who were required to register by December 16, 2002. On November 6, the United States Department of Justice (DOJ) set a deadline of January 10 for those from another 13 countries: Afghanistan, Algeria, Bahrain, Eritrea, Lebanon, Morocco, North Korea, Oman, Qatar, Somalia, Tunisia, the United Arab Emirates, and Yemen. On December 16, it set a deadline of February 21 for those from Armenia, Pakistan and Saudi Arabia. It later included those from Egypt, Jordan, Kuwait, Indonesia, and Bangladesh. It eventually included citizens of 23 nations with majority Muslim populations, as well as Eritrea, which has a large Muslim population, and North Korea. Failure to register at an INS office resulted in deportation. Those found in violation of their visa were allowed to post bail while processed for deportation. The program registered 82,880 people, of whom 13,434 were found in violation of their visas. Because nationality and Muslim affiliation are only approximations for one another, the program extended to such non-Muslims as Iranian Jews. The program was phased out beginning in May 2003.

The program received a mixed response. Some government officials pronounced the program a success. They said in the course of the combined programs, registration upon entry and that of residents, they had arrested 11 suspected terrorists, found more than 800 criminal suspects or deportable convicts, and identified more than 9,000 illegal aliens. DOJ general counsel Kris Kobach said: "I regard this as a great success. Sept. 11th awakened the country to the fact that weak immigration enforcement presents a huge vulnerability that terrorists can exploit." DOJ officials said fewer than 5% of those who came in to INS offices to register were detained. James W. Ziglar, former head of INS who left the agency early in 2002, in part because of his differing opinions about the program with Ashcroft, said his objections to it had been proven correct: "The people who could be identified as terrorists weren't going to show up. This project was a huge exercise and caused us to use resources in the field that could have been much better deployed." "As expected, we got nothing out of it." Although Homeland Security officials said that six men allegedly linked to terrorism were arrested as a result of the call-in program, that contention was challenged by the Sept. 11 commission, which found little evidence to support that claim.

A federal technical building and fire safety investigation of the collapses of the Twin Towers was conducted by the United States Department of Commerce's National Institute of Standards and Technology (NIST). The goals of this investigation, completed on April 6, 2005, were to investigate the building construction, the materials used, and the technical conditions that contributed to the outcome of the WTC disaster. The investigation was to serve as the basis for:

The report concludes that the fireproofing on the Twin Towers' steel infrastructures was blown off by the initial impact of the planes and that, if this had not occurred, the towers would likely have remained standing. The fires weakened the trusses supporting the floors, making the floors sag. The sagging floors pulled on the exterior steel columns to the point where exterior columns bowed inward. With the damage to the core columns, the buckling exterior columns could no longer support the buildings, causing them to collapse. In addition, the report asserts that the towers' stairwells were not adequately reinforced to provide emergency escape for people above the impact zones. NIST stated that the final report on the collapse of 7 WTC will appear in a separate report.

The Inspector General of the CIA conducted an internal review of the CIA's performance prior to 9/11, and was harshly critical of senior CIA officials for not doing everything possible to confront terrorism, including failing to stop two of the 9/11 hijackers, Nawaf al-Hazmi and Khalid al-Mihdhar, as they entered the United States and failing to share information on the two men with the FBI.

The "National Commission on Terrorist Attacks Upon the United States" (9/11 Commission), chaired by former New Jersey Governor Thomas Kean, was formed in late 2002 to prepare a full and complete account of the circumstances surrounding the attacks, including preparedness for, and the immediate response to, the attacks. On July 22, 2004, the "9/11 Commission Report" was released. The commission has been subject to criticism.

For the first time in history, all nonemergency civilian aircraft in the United States and several other countries including Canada were immediately grounded, stranding tens of thousands of passengers across the world. The order was given at 9:42 by Federal Aviation Administration Command Center national operations manager Ben Sliney. According to the 9/11 Commission Report, "This was an unprecedented order. The air traffic control system handled it with great skill, as about 4,500 commercial and general aviation aircraft soon landed without incident.

Contingency plans for the continuity of government and the evacuation of leaders were implemented almost immediately after the attacks. Congress, however, was not told that the US was under a continuity of government status until February 2002.




</doc>
<doc id="28064" url="https://en.wikipedia.org/wiki?curid=28064" title="Financial assistance following the September 11 attacks">
Financial assistance following the September 11 attacks

Charities and relief agencies raised over $657 million in the three weeks following the September 11, 2001 attacks, the vast bulk going to immediate survivors and victims' families.

On September 21, 2001, the Congress approved a bill to aid the airline industry and establish a federal fund for victims. The cost of the mostly open-ended fund reached $7 billion. Victims of earlier terrorist attacks, including those linked to al-Qaida, were not included in the fund—nor were those who would not surrender the right to hold the airlines legally responsible.

From the donations to the Emergency Relief Fund, as of 19 November 2001, the American Red Cross granted 3,165 checks to 2,776 families totaling $54.3 million.

172,612 cases were referred to mental health contacts. The 866-GET INFO number received 29,820 calls. As of 3:10 p.m. November 20, 2001, there had been 1,592,295 blood donations since September 11.

"Fire Donations" took charitable contributions on behalf of firefighters, EMS, and rescue workers.


On Thursday and Friday, September 14–15 September 2001, various relief supplies for the World Trade Center relief effort were collected from the New York City area, and dropped off at the Javits Convention Center or at a staging area at Union Square. By Saturday morning, enough supplies (and volunteers) were collected.

Many families and friends of victims have set up memorial funds and projects to give back to their communities and change the world in honor of their loved ones' lives. Examples include:



</doc>
<doc id="28066" url="https://en.wikipedia.org/wiki?curid=28066" title="Rescue and recovery effort after the September 11 attacks on the World Trade Center">
Rescue and recovery effort after the September 11 attacks on the World Trade Center

The September 11 attacks on the World Trade Center elicited a large response of local emergency and rescue personnel to assist in the evacuation of the two towers, resulting in a large loss of the same personnel when the towers collapsed. After the attacks, the media termed the World Trade Center site "Ground Zero", while rescue personnel referred to it as "the Pile".

In the ensuing recovery and cleanup efforts, personnel related to the metalwork and construction professions would descend on the site to offer their services and remained until the site was cleared in May 2002. In the years since, investigations and studies have examined effects upon those who participated, noting a variety of afflictions attributed to the debris and stress.

After American Airlines Flight 11 crashed into the North Tower (1 WTC) of the World Trade Center, a standard announcement was given to tenants in the South Tower (2 WTC) to stay put and that the building was secure. However, many defied those instructions and proceeded to evacuate the South Tower (most notably, Rick Rescorla, Morgan Stanley Security Director, evacuated 2,687 of the 2,700 Morgan Stanley employees in the building). People evacuating from WTC 2 were ordered up from the lobby level to a door on the mezzanine level that led to a covered footbridge over West Street to a building complex then called the World Financial Center. People evacuating from WTC 1 were directed from the lobby level through the WTC shopping mall beneath the outdoor plaza. The firefighters directing evacuees did not want anyone going through the front doors due to falling debris and falling people.

Standard evacuation procedures for fires in the World Trade Center called for evacuating only the floors immediately above and below the fire, as simultaneous evacuation of up to 50,000 workers would be too chaotic.

Firefighters from the New York City Fire Department rushed to the World Trade Center minutes after the first plane struck the north tower. Chief Joseph Pfeifer and his crew with Battalion 1 were among the first on the scene. At 8:50 a.m., an Incident Command Post was established in the lobby of the North Tower. By 9:00 a.m., shortly before United Airlines Flight 175 hit the South Tower, the FDNY chief had arrived and took over command of the response operations. Due to falling debris and safety concerns, he moved the incident command center to a spot located across West Street, but numerous fire chiefs remained in the lobby which continued to serve as an operations post where alarms, elevators, communications systems, and other equipment were operated. The initial response by the FDNY was on rescue and evacuation of building occupants, which involved sending firefighters up to assist people that were trapped in elevators and elsewhere. Firefighters were also required to ensure all floors were completely evacuated.

Numerous staging areas were set up near the World Trade Center, where responding fire units could report and get deployment instructions. However, many firefighters arrived at the World Trade Center without stopping at the staging areas. As a result, many chiefs could not keep track of the whereabouts of their units. Numerous firefighters reported directly to the building lobbies, and were ordered by those commanding the operating post to proceed into the building.

Problems with radio communication caused commanders to lose contact with many of the firefighters who went into the buildings. The repeater system in the World Trade Center, which was required for portable radio signals to transmit reliably, was malfunctioning after the impact of the planes. As a result, firefighters were unable to report to commanders on their progress, and were unable to hear evacuation orders. Also, many off-duty firefighters arrived to help, without their radios. FDNY commanders lacked communication with the NYPD, who had helicopters at the scene, or with Emergency Medical Services (EMS) dispatchers. The firefighters on the scene also did not have access to television reports or other outside information, which could help in assessing the situation. When the South Tower collapsed at 9:59 a.m., firefighters in the North Tower were not aware of exactly what had happened. The battalion chief in the North Tower lobby immediately issued an order over the radio for firefighters in the tower to evacuate, but many did not hear the order, due to the faulty radios. Because of this, 343 firefighters died in the collapse of the towers.

The command post located across West Street was taken out when the South Tower collapsed, making command and control even more difficult and disorganized. When the North Tower collapsed, falling debris killed Peter Ganci, the FDNY chief. Following the collapse of the World Trade Center, a command post was set up at a firehouse in Greenwich Village.

The FDNY deployed 200 units (half of all units) to the site, with more than 400 firefighters on the scene when the buildings collapsed. This included a total of 121 engine companies, 62 ladder companies, and other special units. The FDNY also received assistance from fire departments in Nassau, Suffolk, Westchester County, and other neighboring jurisdictions, but with limited ability to manage and coordinate efforts.
Besides assisting with recovery operations at Ground Zero, volunteer firefighters from Long Island and Westchester manned numerous firehouses throughout the city to assist with other fire and emergency calls.

FDNY Emergency medical technicians (EMT's) and Paramedics, along with 9-1-1 system ambulances operated by voluntary hospitals and volunteer ambulance corps, began arriving at 8:53 a.m., and quickly set up a staging area outside the North Tower, at West Street, which was quickly moved over to the corner of Vesey and West Streets. As more providers responded to the scene, five triage areas were set up around the World Trade Center site. EMS chiefs experienced difficulties communicating via their radios, due to the overwhelming volume of radio traffic. At 9:45, an additional dispatch channel was set aside for use by chiefs and supervisors only, but many did not know about this and continued to operate on the other channel. The communication difficulties meant that commanders lacked good situational awareness.

Dispatchers at the 9-1-1 call center, who coordinate EMS response and assign units, were overwhelmed with incoming calls, as well as communications over the radio system. Dispatchers were unable to process and make sense of all the incoming information, including information from people trapped in the towers, about conditions on the upper floors. Overwhelmed dispatchers were unable to effectively give instructions and manage the situation.

EMS personnel were in disarray after the collapse of the South Tower at 9:59 a.m. Following the collapse of the North Tower at 10:28 a.m., EMS commanders regrouped on the North End of Battery Park City, at the Embassy Suites Hotel. Around 11:00 a.m., EMS triage centers were relocated and consolidated at the Chelsea Piers and the Staten Island Ferry Terminal. Throughout the early afternoon, the soundstages at the pier were separated into two areas, one for the more seriously injured and one for the walking wounded. On the acute side, multiple makeshift tables, each with a physician, nurse, and other health care workers, and non-emergency service volunteers, were set up for the arrival of mass casualties.

Supplies, including equipment for airway and vascular control, were obtained from neighboring hospitals. Throughout the afternoon, local merchants arrived to donate food. Despite this, few patients arrived for treatment, the earliest at about 5 p.m., and were not seriously injured, being limited to smoke inhalation. An announcement was made around 6–7 p.m. that a second shift of providers would cover the evening shift, and that an area was being set up for the day personnel to sleep. Soon after, when it was realized that few would have survived the collapse and be brought to the piers, many decided to leave and the area was closed down.

The New York City Police Department quickly responded with the Emergency Service Units (ESU) and other responders after the crash of American Airlines Flight 11 into the North Tower. The NYPD set up its incident command center at Church Street and Vesey Street, on the opposite side of the World Trade Center from where the FDNY was commanding its operations. NYPD helicopters were soon at the scene, reporting on the status of the burning buildings. When the buildings collapsed, 23 NYPD officers were killed, along with 37 Port Authority Police Department officers. The NYPD helped facilitate the evacuation of civilians out of Lower Manhattan, including approximately 5,000 civilians evacuated by the Harbor Unit to Staten Island and to New Jersey. In ensuing days, the police department worked alternating 12-hour shifts to help in the rescue and recovery efforts.

Immediately after the first attack, the captains and crews of a large number of local boats steamed into the attack zone to assist in evacuation. These ships had responded by a request from the U.S. Coast Guard to help evacuate those stranded on Manhattan Island. Others, such as the "John J. Harvey", provided supplies and water, which became urgently needed after the Towers' collapse severed downtown water mains. The Coast Guard Auxiliary helped lead a massive maritime evacuation with estimates of the number of people evacuated by water from Lower Manhattan that day in the eight-hour period following the attacks ranging from 500,000 to 1,000,000. Norman Mineta, Secretary of Transportation during the attacks, called the efforts "the largest maritime evacuation conducted in the United States". The evacuation was the largest maritime evacuation or "boatlift" in history by most estimates, passing the nine-day evacuation of Dunkirk during World War II. As many as 2,000 people injured in the attacks were evacuated by this means.

Amateur radio played a role in the rescue and clean-up efforts. Amateur radio operators established communications, maintained emergency networks, and formed bucket brigades with hundreds of other volunteer personnel. Approximately 500 amateur radio operators volunteered their services during the disaster and recovery.

The New Jersey Legislature honored the role of Amateur Radio operators in a proclamation on December 12, 2002.

Note: "Government exhibits are from the trial of Zacarias Moussaoui."

On the day following the attacks, 11 people were rescued from the rubble, including six firefighters and three police officers. One woman was rescued from the rubble, near where a West Side Highway pedestrian bridge had been. Two PAPD officers, John McLoughlin and Will Jimeno, were also rescued. Discovered by former U.S. Marines Jason Thomas and Dave Karnes, McLoughlin and Jimeno were pulled out alive after spending nearly 24 hours beneath 30 feet of rubble. Their rescue was later portrayed in the Oliver Stone film, "World Trade Center". In total, only twenty survivors were pulled out of the rubble. The final survivor, Port Authority secretary Genelle Guzman-McMillan, was rescued 27 hours after the collapse of the North Tower.

Some firefighters and civilians who survived made cell phone calls from voids beneath the rubble, though the amount of debris made it difficult for rescue workers to get to them.

By Wednesday night, 82 deaths had been confirmed by officials in New York City.

Rescue efforts were paused numerous times in the days after the attack, due to concerns that nearby buildings, including One Liberty Plaza, were in danger of collapsing.

The search and rescue effort in the immediate aftermath at the World Trade Center site involved ironworkers, structural engineers, heavy machinery operators, asbestos workers, boilermakers, carpenters, cement masons, construction managers, electricians, insulators, machinists, plumbers and pipefitters, riggers, sheet metal workers, steelworkers, truckers and teamsters, American Red Cross volunteers, and many others.
Lower Manhattan, south of 14th Street, was off-limits, except for rescue and recovery workers. There were also about 400 working dogs, the largest deployment of dogs in the nation's history.

New York City Office of Emergency Management was the agency responsible for coordination of the City's response to the attacks. Headed by then-Director Richard Sheirer, the agency was forced to vacate its headquarters, located in 7 World Trade Center, within hours of the attack. The building later collapsed. OEM reestablished operations temporarily at the police academy, where Mayor Giuliani gave many press conferences throughout the afternoon and evening of September 11. By Friday, rescue and reliefs were organized and administered from Pier 92 on the Hudson River.

Volunteers quickly descended on Ground Zero to help in the rescue and recovery efforts. At Jacob Javits Convention Center, thousands showed up to offer help, where they registered with authorities. Construction projects around the city came to a halt, as workers walked off the jobs to help at Ground Zero. Ironworkers, welders, steel burners, and others with such skills were in high demand. By the end of the first week, over one thousand ironworkers from across North America had arrived to help, along with countless others.

The New York City Department of Design & Construction oversaw the recovery efforts. Beginning on September 12, the Structural Engineers Association of New York (SEAoNY) became involved in the recovery efforts, bringing in experts to review the stability of the rubble, evaluate safety of hundreds of buildings near the site, and designing support for the cranes brought in to clear the debris. The City of New York hired the engineering firm, LZA-Thornton Tomasetti, to oversee the structural engineering operations at the site.

To make the effort more manageable, the World Trade Center site was divided into four quadrants or zones. Each zone was assigned a lead contractor, and a team of three structural engineers, subcontractors, and rescue workers.


The Federal Emergency Management Agency (FEMA), the United States Army Corps of Engineers, the Occupational Safety and Health Administration (OSHA), and the New York City Office of Emergency Management (OEM) provided support. Forestry incident management teams (IMTs) also provided support beginning in the days after the attacks to help manage operations.

A nearby Burger King restaurant was used as a center for police operations. Given that workers worked at the site, or "The Pile", for shifts as long as twelve hours, a specific culture developed at the site, leading to workers developing their own argot.

"The Pile" was the term coined by the rescue workers to describe the 1.8 million tons of wreckage left from the collapse of the World Trade Center. They avoided the use of "ground zero", which describes the epicenter of a bomb explosion.

Numerous volunteers organized to form "bucket brigades", which passed 5-gallon buckets full of debris down a line to investigators, who sifted through the debris in search of evidence and human remains. Ironworkers helped cut up steel beams into more manageable sizes for removal. Much of the debris was hauled off to the Fresh Kills Landfill on Staten Island where it was further searched and sorted.

According to the New York Times, by September 24, 2001 more than 100,000 tons of debris had been removed from the site. Some structural engineers have criticized the decision to recycle the steel from the buildings before it could be analyzed as part of the post-collapse investigation. 

Some of the steel was reused for memorials. New York City firefighters donated a cross made of steel from the World Trade Center to the Shanksville Volunteer Fire Company in Shanksville, Pennsylvania. The beam, mounted atop a platform shaped like the Pentagon, was erected outside the Shanksville's firehouse near the crash site of United Airlines Flight 93.

Twenty-four tons of the steel used in construction of USS "New York" (LPD-21) came from the small amount of rubble from the World Trade Center preserved for posterity.
In the days following the destruction of the towers, rescuers found scorch marks, likely made by a cutting torch on a basement doorway underneath 4 WTC; this was thought to be the result of looters. Further exploration of the building's basement revealed that the vault contained large amounts of gold and silver in the form of coins, as well as gold and silver bars. Over the subsequent months, much of the bullion was recovered. Approximately 560,000 dollars worth of the coins, having been stored in the vault by the Bank of Nova Scotia prior to September 11, 2001, were purchased by Lee S. Minshull of Palos Verdes, California, who then sent them to PCGS for grading in 2002. These coins were then sold to collectors. Coins salvaged from 4 WTC's vault included American Silver Eagles, Canadian Gold Maple Leafs, South African Krugerrands and British Gold Britannias.

Hazards at the World Trade Center site included a diesel fuel tank buried seven stories below. Approximately 2,000 automobiles that had been in the parking garage also presented a risk, with each containing, on average, at least five gallons of gasoline. Once recovery workers reached down to the parking garage level, they found some cars that had exploded and burned. The United States Customs Service, which was housed in 6 World Trade Center, had 1.2 million rounds of ammunition and weapons in storage in a third-floor vault, to support their firing range.

In the hours immediately after the attacks on the World Trade Center, three firefighters raised an American flag over the rubble. The flag was taken from a yacht, and the moment, which was captured on a well-known photograph, evoked comparisons to the iconic Iwo Jima photograph. Morale of rescue workers was boosted on September 14, 2001 when President George W. Bush paid a visit to Ground Zero. Standing with retired firefighter Bob Beckwith, Bush addressed the firefighters and rescue workers with a bullhorn and thanked them. Bush later remarked, "I'm shocked at the size of the devastation, It's hard to describe what it's like to see the gnarled steel and broken glass and twisted buildings silhouetted against the smoke. I said that this was the first act of war on America in the 21st century, and I was right, particularly having seen the scene." After some workers shouted that they could not hear the President, Bush famously responded by saying "I can hear you! The rest of the world hears you. And the people who knocked these buildings down will hear all of us soon!"

At some point, rescue workers realized that they were not going to find any more survivors. After a couple of weeks, the conditions at Ground Zero remained harsh, with lingering odors of decaying human remains and smoke. Morale among workers was boosted by letters they received from children around the United States and the world, as well as support from thousands of neighbors in TriBeCa and other Lower Manhattan neighborhoods.

This support continued to spread and eventually led to the founding of over 250 non-profit organizations of which raised almost $700 million within their first two years of operation. One of the nonprofits included One Day's Pay, later changed to MyGoodDeed, which championed the effort to designate September 11 as an official National Day of Service (9/11 Day).

By 2012, many of the 250 plus organizations had disbanded due to lack of funding as the years progressed. Of the ones that remain, a handful remained functioning for those who remain in need. One of these organizations, Tuesday's Children, was founded the day after September 11 in hopes of supporting the children immediately affected by the attacks. The founder of this non-profit, David Weild IV, now calls them one of the "last men standing" in that they are now one of the few remaining organizations who "provide direct services for what social-service groups and survivors of the attacks call the '9-11 Community.'"

Other notable non-profits who are "still standing" include:

Immediately following the attacks, members of the Civil Air Patrol (CAP) were called up to help respond. Northeast Region placed their region personnel and assets on alert mere moments after they learned of the attack. With the exception of CAP, civilian flights were grounded by the Federal Aviation Administration. CAP flew aerial reconnaissance missions over Ground Zero, to provide detailed analysis of the wreckage and to aid in recovery efforts, including transportation of blood donations.

Elements of the New York Army National Guard's 1-101st Cavalry (Staten Island), 258th Field Artillery, 442nd Military Police Company, and 69th Infantry Regiment based in Manhattan were the first military force to secure Ground Zero on September 11. The 69th Infantry's armory on Lexington Avenue became the Family Information Center to assist persons in locating missing family members.

The National Guard supplemented the NYPD and FDNY, with 2,250 guard members on the scene by the next morning. Eventually thousands of New York Army and Air National Guardsmen participated in the rescue/recovery efforts. They conducted site security at the WTC, and at other locations. They provided the NYPD with support for traffic control, and they participated directly in recovery operations providing manpower in the form of "bucket brigades" sorting through the debris by hand.

Additionally service members provided security at a variety of location throughout the city and New York State to deter further attacks and reassure the public.

Members of the Air National Guard's 109th Airlift Wing out of Scotia, and Syracuse's 174th Fighter Wing immediately responded to New York City, setting up camp at places such as Fort Hamilton. Mostly civil engineers, firefighters and military police, they greatly aided in the clean-up effort. F-16s from the 174th Fighter Wing also ramped up their flying sorties and patrolled the skies.

The New Jersey National Guard assisted the New York National Guard's efforts following the attacks.

U.S. Marines were also present to assist in the rescue efforts. No official numbers of men who helped out was released but there was evidence that they were there.
Films such as 2006 docudrama "World Trade Center" talked of two Marines who rescued two trapped police officers in the rubble. U.S. Marines were headquartered at 340 Westside Hwy Bloomberg News Building. The commanding officer was Navy Commander Hardy, and executive officer was Maj. Priester. These two oversaw 110 military personnel of various branches, various police departments and EMTs.

The U.S. Navy deployed a hospital ship USNS "Comfort" (T-AH-20) to Pier 92 in Manhattan. Crew members provided food and shelter for more than 10,000 relief workers. Comfort's 24-hour galley also provided an impressive 30,000 meals. Its medical resources were also used to provide first-aid and sick call services to nearly 600 people. The ship's psychological response team also saw more than 500 patients.

A May 14, 2007, "New York Times" article, "Ground Zero Illness Clouding Giuliani's Legacy", gave the interpretation that thousands of workers at Ground Zero have become sick and that "regard Mr. Giuliani's triumph of leadership as having come with a human cost". The article reported that the mayor seized control of the cleanup of Ground Zero, taking control away from established federal agencies, such as the Federal Emergency Management Agency, the U.S. Army Corps of Engineers and the Occupational Safety and Health Administration. He instead handed over responsibility to the "largely unknown" city Department of Design and Construction. Documents indicate that the Giuliani administration never enforced federal requirements requiring the wearing of respirators. Concurrently, the administration threatened companies with dismissal if cleanup work slowed.

Workers at the Ground Zero pit worked without proper respirators. They wore painters' masks or no facial covering. Specialists claim that the only effective protection against toxins, such as airborne asbestos, is a special respirator. New York Committee for Occupational Safety and Health industrial hygienist David Newman said, "I was down there watching people working without respirators." He continued, "Others took off their respirators to eat. It was a surreal, ridiculous, unacceptable situation."

The local EPA office sidelined the regional EPA office. Dr. Cate Jenkins, a whistle-blower EPA scientist, said that on September 12, 2001, a regional EPA office offered to dispatch 30 to 40 electron microscopes to the WTC pit to test bulk dust samples for the presence of asbestos fibers. Instead, the local office chose the less effective polarized light microscopy testing method. Dr. Jenkins alleged that the local office refused, and said, "We don't want you fucking cowboys here. The best thing they could do is reassign you to Alaska."

There were many health problems caused by the toxins. 99% of exposed firefighters reported at least one new respiratory problem while working at the World Trade Center site that they had not experienced before. Chronic airway disease is the main lung injury among firefighters who were exposed to toxins during 9/11. Six years after the attacks, among those who never smoked, approximately 13% of firefighters and 22% of EMS had lungs that did not function as well as others around the same age. Steep declines in pulmonary lung function has been a problem since first detected among firefighters and EMS within a year of 9/11 have persisted. 
Increasing numbers of Ground Zero workers are getting illnesses, such as cancer. Between September 11, 2001, through 2008, there were 263 new cases of cancer found in 8,927 male firefighters who responded to 9/11 attacks. This number is 25 more than what is expected from men from a similar age group and race. There is a 19% increase in cancer overall, between firefighters who responded to the attacks and those who were not exposed to toxins from responding to the attacks on September 11.

On January 30, 2007, Ground Zero workers and groups such as Sierra Club and Unsung Heroes Helping Heroes met at the Ground Zero site and urged President George Bush to spend more money on aid for sick Ground Zero workers. They said that the $25 million that Bush promised for the ill workers was inadequate. A Long Island iron-worker, John Sferazo, at the protest rally said, "Why has it taken you 5½ years to meet with us, Mr. President?"

Firefighters, police and their unions, have criticized Mayor Rudy Giuliani over the issue of protective equipment and illnesses after the attacks. An October study by the National Institute of Environmental Safety and Health said that cleanup workers lacked adequate protective gear. The Executive Director of the National Fraternal Order of Police reportedly said of Giuliani: "Everybody likes a Churchillian kind of leader who jumps up when the ashes are still falling and takes over. But two or three good days don't expunge an eight-year record." Sally Regenhard, said, "There's a large and growing number of both FDNY families, FDNY members, former and current, and civilian families who want to expose the true failures of the Giuliani administration when it comes to 9/11." She told the "New York Daily News" that she intends to "Swift Boat" Giuliani.

Various health programs arose after the attacks to provide treatment for 9/11-related illnesses among responders, recovery workers, and other survivors. When the James Zadroga 9/11 Health and Compensation Act became federal law in January 2011, these programs were replaced by the World Trade Center Health Program.

Soon after the attacks, New York City commissioned McKinsey & Company to investigate the response of both the New York City Fire Department and New York City Police Department and make recommendations on how to respond more effectively to such large-scale emergencies in the future.

Officials with the International Association of Fire Fighters have also criticized Rudy Giuliani for failing to support modernized radios that might have spared the lives of more firefighters. Some firefighters never heard the evacuation orders and died in the collapse of the towers.

Estimated total costs, as of October 3, 2001

Plans for the World Trade Center rebuilding started in July 2002 which was headed by the Lower Manhattan Development Corporation. There were many proposals on how to build the World Trade Center back however many lacked creativity. Several architects were chosen throughout this construction process, but all of them ran into many problems with the design. The financial crisis in 2008 also forced the construction process over to the Port Authority; however, the Port Authority construction is not going as smoothly as planned. City officials are looking for better ways to lower the problems and delays. The World Trade Center completion of construction was scheduled for 2016. , four of seven planned buildings were completed, as were the transportation hub, 9/11 Memorial, and Liberty Park.


Notes
Bibliography

New York Times:

Other:


</doc>
<doc id="28070" url="https://en.wikipedia.org/wiki?curid=28070" title="Communication during the September 11 attacks">
Communication during the September 11 attacks

Communication problems and successes played an important role in the September 11, 2001, attacks and their aftermath. Systems were variously destroyed or overwhelmed by loads greater than they were designed to carry, or failed to operate as intended or desired.
The organizers of the September 11, 2001, attacks apparently planned and coordinated their mission in face to face meetings and used little or no electronic communication. This "radio silence" made their plan more difficult to detect.

According to 9/11 Commission staff statement No. 17 there were several communications failures at the federal government level during and after the 9/11 attacks. Perhaps the most serious occurred in an "Air Threat Conference Call" initiated by the National Military Command Center (NMCC) after two planes had crashed into the World Trade Center, but shortly before The Pentagon was hit. The participants were unable to include the Federal Aviation Administration (FAA) air traffic control command center, which had the most information about the hijackings, in the call.

According to the staff report:
Operators worked feverishly to include the FAA in this teleconference, but they had equipment problems and difficulty finding secure phone numbers. NORAD asked three times before 10:03 to confirm the presence of FAA on the conference, to provide an update on hijackings. The FAA did not join the call until 10:17. The FAA representative who joined the call had no familiarity with or responsibility for a hijack situation, had no access to decision makers, and had none of the information available to senior FAA officials by that time.

We found no evidence that, at this critical time, during the morning of September 11, NORAD’s top commanders, in Florida or Cheyenne Mountain Complex, ever coordinated with their counterparts at FAA headquarters to improve situational awareness and organize a common response. Lower-level officials improvised—the FAA’s Boston Center bypassing the chain of command to contact NEADS. But the highest level Defense Department officials relied on the NMCC’s Air Threat Conference, in which FAA did not meaningfully participate.

After the 1993 World Trade Center bombing, radio repeaters for New York City Fire Department communication were installed in the tower complex. Because they were unaware that several controls needed to be operated to fully activate the repeater system, fire chiefs at their command post in the lobby of the North Tower thought the repeater was not functioning and did not use it, though it did work and was used by some firefighters. When police officials concluded the twin towers were in danger of collapsing and ordered police to leave the complex, fire officials were not notified. Fire officials on the scene were not monitoring broadcast news reports and did not immediately understand what had happened when the first (South) tower did collapse. 

There was little communication between New York City Police Department and fire department commands even though an Office of Emergency Management (OEM) had been created in 1996 in part to provide such coordination. A primary reason for OEM's inability to coordinate communications and information-sharing in the early hours of the WTC response was the loss of its emergency operations center, located on the twenty third floor of 7 World Trade Center which had been evacuated after debris from tower's collapse struck the building, igniting several fires.

Emergency relief efforts in both Lower Manhattan and at the Pentagon were augmented by volunteer amateur radio operators in the weeks after the attacks.

Cell phones and in-plane credit card phones played a major role during and after the attack, starting with hijacked passengers who called family or notified the authorities about what was happening. Passengers and crew who made calls include: Sandra Bradshaw, Todd Beamer, Tom Burnett, Mark Bingham, Peter Hanson, Jeremy Glick, Barbara K. Olson, Renee May, Madeline Amy Sweeney, Betty Ong, Robert Fangman, Brian David Sweeney, and Ed Felt. Innocent occupants aboard United Airlines Flight 93 were able to assess their situation based on these conversations and plan a revolt that resulted in the aircraft crashing. According to the commission staff: "Their actions saved the lives of countless others, and may have saved either the U.S. Capitol or the White House from destruction."

According to the 9/11 Commission Report, 13 passengers from Flight 93 made a total of over 30 calls to both family and emergency personnel (twenty-two confirmed air phone calls, two confirmed cell phone and eight not specified in the report). Brenda Raney, Verizon Wireless spokesperson, said that Flight 93 was supported by several cell sites. There were reportedly three phone calls from Flight 11, five from Flight 175, and three calls from Flight 77. Two calls from these flights were recorded, placed by flight attendants: Betty Ong on Flight 11 and CeeCee Lyles on Flight 93.

Alexa Graf, an AT&T spokesperson, said it was almost a fluke
that the calls reached their destinations. Marvin Sirbu, professor of Engineering and Public Policy at Carnegie Mellon University said on September 14, 2001, that "The fact of the matter is that cell phones can work in almost all phases of a commercial flight." Other industry experts said that it is possible to use cell phones with varying degrees of success during the ascent and descent of commercial airline flights.

After each of the hijacked aircraft struck the World Trade Center, people inside the towers made calls to family and loved ones; for the victims, this was their last communication. Other callers directed their pleas for help to 9-1-1. Over nine hours of the 9-1-1 calls were eventually released after petitioning by "The New York Times" and families of the WTC victims. In 2001, U.S. cell phones did not yet have the photography capabilities that became widespread by the mid-2000s.

After the attack, the cell phone network of New York City was rapidly overloaded (a mass call event) as traffic doubled over normal levels. Cell phone traffic also overloaded across the East Coast, leading to crashes of the cell phone network. Verizon's downtown wire phone service was interrupted for days and weeks because of cut subscriber cables, and to the 140 West Street exchange being shut for days. Capacity between Brooklyn and Manhattan was also diminished by cut trunk cables.

Following the attacks, the issues with the cell network weren't resolved until 36 cellular COWs (cell towers on wheels) were deployed by September 14, 2001, in Lower Manhattan to support the U.S. Federal Emergency Management Agency (FEMA) and provide critical phone service to rescue and recovery workers.

Since three of the major television broadcast network owned-and-operated stations had their transmission towers atop the North Tower (One World Trade Center), coverage was limited after the collapse of the tower. The FM transmitter of National Public Radio station WNYC was also destroyed in the collapse of the North Tower and its offices evacuated. For an interim period, it continued broadcasting on its AM frequency and used NPR's New York offices to produce its programming.
The satellite feed of one television station, WPIX, froze on the last image received from the WTC mast; the image (a remote-camera shot of the burning towers), viewable across North America (as WPIX is available on cable TV in many areas), remained on the screen for much of the day until WPIX was able to set up alternate transmission facilities. It shows the WTC at the moment power cut off to the WPIX transmitter, prior to the towers' collapse.

During the September 11 attacks, WCBS-TV channel 2 and WXTV-TV channel 41 stayed on the air. Unlike most other major New York television stations, WCBS-TV maintained a full-powered backup transmitter at the Empire State Building after moving its main transmitter to the North Tower of the World Trade Center. The station was also simulcasted nationally on Viacom (which at the time owned CBS) cable network VH1 that day. In the immediate aftermath of the attacks, the station lent transmission time to the other stations who had lost their transmitters, until they found suitable backup equipment and locations.

The Emergency Alert System was never activated in the terrorist attacks, as the extensive media coverage made it unnecessary.

AT&T eliminated any costs for domestic calls originating from the New York City area (phones using area codes 212, 718, 917, 646, and 347) in the days following 9/11.

Radio communications during the September 11 attacks served a vital role in coordinating rescue efforts by New York Police Department, New York Fire Department, Port Authority Police Department, and emergency medical services.

While radio communications were modified to address problems discovered after the 1993 World Trade Center bombing, investigations into the radio communications during the September 11th attacks discovered that communication systems and protocols that distinguished each department was hampered by the lack of interoperability, damaged or failed network infrastructure during the attack, and overwhelmed by simultaneous communication between superiors and subordinates.

A rough time line of the incident could include:

The scale of the incident was described in the National Commission report on the attacks as "unprecedented". In roughly seventeen minutes from 8:46 to 9:03 am, over a thousand police, fire, and emergency medical services (EMS) staff arrived at the scene. At some point during a large incident, any agency will reach a point where they find their resources overrun by needs. For example, the Port Authority Police could not schedule staff as if a September 11 attack would occur every shift. There is always a balance struck between readiness and costs. There is conflicting data but some sources suggest there may have been 2,000 to 3,000 workers involved in the rescue operation. It would be rare for most agencies to see an event where there were that many people to be rescued.

There is some level of confusion present in any large incident. The National Institute for Standards and Technology (NIST) asserts commanders did not have adequate information and interagency information sharing was inadequate. For example, on September 11, persons in the New York City Police Department (NYPD) 9-1-1 center told callers from the World Trade Center to remain in place and wait for instruction from firefighters and police officers. This was the plan for managing a fire incident in the building and the 9-1-1 center staff were following the plan. This was partly countered by public safety workers going floor-by-floor and telling people to evacuate. The Commission report suggests people in the NYPD 9-1-1 center and New York City Fire Department (FDNY) dispatch would benefit from better situation awareness. The Commission described the call centers as not "fully integrated" with line personnel at the WTC. The report suggests the NYPD 9-1-1 center and FDNY dispatch were overrun by call volumes that had never been seen before. Adding to the confusion, radio coverage problems, radio traffic blocking, and building system problems occurred inside the burning towers. The facts show that much of the equipment worked as designed and users made the best of what was available to them.

Typical of any large fire, many 9-1-1 calls with conflicting information were received beginning at 8:46 am. In addition to reports that a plane had hit the World Trade Center, the EMS computer-aided dispatch (CAD) log shows reports of a helicopter crash, explosions, and a building fire. Throughout the incident, people at different locations had very different views of the situation. After the collapse of the first tower, many firefighters in the remaining tower had no idea the first tower had fallen.

A factor in radio communications problems included the fact that off-duty personnel self-dispatched to the incident scene. Some off-duty staff went into the towers without radios. According to the Commission report and news coverage, this was true of NYPD, Port Authority Police Department (PAPD), and FDNY personnel. Regardless of any radio coverage problems, these persons could not be commanded or informed by radio. In any incident of this scale, self-dispatched staff without radios would likely be a problem. Even if a cache of radios were brought to the scene to hand out, the scale of this incident would be likely to overrun the number of radios in the cache.

NIST concluded, at the beginning of the incident, there was an approximate factor of five (peak) increase in radio communications traffic over a normal level. After the initial peak, radio traffic through the incident followed an approximate factor of three steady increase. FDNY recordings suggest the dispatch personnel were overloaded: both fire and EMS dispatch were often delayed in responding to radio calls. Many 9-1-1 telephone calls to dispatch were disconnected or routed to "all circuits are busy now", intercept recordings.

NIST calculated that about one third of radio messages transmitted during the surge of communications were incomplete or unintelligible. Documentary footage suggests the tactical channels were also overloaded; some footage captured audio of two or three conversations occurring simultaneously on a particular channel.

In this study of WTC incident communications, radio systems used at the site had problems but were generally effective in that users were able to communicate with one another. A 2002 video documentary "9/11" by Gedeon and Jules Naudet, (referred to as "the documentary") was reviewed. It captured audio from hand-held radios in use at the incident and showed users communicating over radios from the lobby command post in the North Tower. 26 Red Book audio CDs of New York City Fire Department radio transmissions, covering the incident's initial dispatch and the tower failures, were reviewed. These CDs were digitized versions of audio from the Fire Department's logging recorders. In addition, text on an oral history CD with transcripts of fire personnel debriefed on the incident were reviewed.

In 2001, the NYPD used Ultra High Frequency (UHF) radios and divided the city into 35 radio zones. Most hand-held radios had at least 20 channels: while not all officers had all channels, all officers had the ability to communicate citywide. As a characteristic of physics, UHF signals penetrate buildings better than lower Very High Frequency (VHF) frequencies used by the FDNY fire units but generally cover shorter distances over open terrain. The Commission report did not cite any technical flaws with the NYPD radio system.

PAPD has systems described as "low-power UHF". The Commission report says the systems were specific to a single site with the exception of one channel which was Port-Authority-wide. It's unclear whether the PAPD systems were interstitial and limited to 2 watts output, used normal local-control channels but were limited in power output by the frequency coordinator, or used leaky cable systems which were solely intended to work inside the Port Authority buildings. The report says there were 7, site-specific Port Authority Police channels. In 2001, officers at one site could not, (in all cases), carry their radios to another site and use them. Not all radios had all channels.

Recordings of Citywide, Brooklyn, and Manhattan channels for Fire and Citywide, Brooklyn, and Manhattan channels for Emergency medical services were reviewed. Systems generally performed well. The audio coupling point for the logging recorder on Manhattan Fire made the dispatcher's voice difficult to hear. An anonymous fire dispatcher who identifies as "Dispatcher 416" is noteworthy.

The Commission report says that, in 2001, FDNY used a system with 5 repeater channels: one for each of the boroughs of Manhattan, Brooklyn, Queens, with the Bronx and Staten Island sharing a single frequency using different Private Line (PL) tones, and a citywide channel. There were also five simplex channels in FDNY radios.

Observation shows, back in 2001, that the citywide EMS channel was voting more frequently than normal, signals were noisy, interfering signals were present, and that some receiver sites had equalization differences. Some transmissions had choppy audio possibly representative of interference from FSK paging or intermittent microwave radio paths to one or more receiver sites. For example, if a microwave radio path fails for half-second intervals, the voting comparator may vote out that receiver site until silence is detected. This can cause dropped syllables in the voted audio. Some transmissions were noisy, although transactions show the dispatcher was understanding radio traffic in spite of audio drop-outs in almost every case.

The Port Authority repeater, intended to allow communications inside the towers, did not appear to work as intended on September 11. The system, also called "Port Authority Channel 30", was installed after the 1993 World Trade Center attack. News accounts said the system had been turned off for unspecified technical reasons. The Commission report said it was customary to turn the system off because it somehow caused interference to radios in use at fire operations in other parts of the city. The documentary film gives different information, with a Fire Department member from Engine 7/Ladder 1 claiming that the aircraft's impact caused the system to fail. Evidence suggests the remote control console in the lobby command was not working but the repeater was. The radio repeater was located in 5 World Trade Center. A remote control console was connected to the repeater allowing staff at the North Tower lobby command post to communicate without using a hand-held radio.

In a review of the logging recorder track of the Port Authority repeater, someone arrived early during the incident and began to establish a command post. From the command post in the lobby of the North Tower (1 World Trade Center), the user can be heard trying to transmit using a remote control unit. After several failed attempts to communicate with a user on the channel, the user steps through every channel selection on the remote, trying each one. The recording contains the tone remote control console stepping through all of its eight function tones. Someone says, "... the wireline isn't working", over the Port Authority channel. Something that looks like a Motorola T-1380-series remote is shown in the documentary. The fact that users pressing buttons on the remote control can clearly be heard on the logging recorder shows the transmit audio path was working. The recording does not reveal whether or not the console function tones were keying the transmitter.

Some users in the North Tower lobby interpreted the remote control unit not working as a failure of the entire channel. Other fire units, not knowing the channel had failed, arrived and began using it successfully. The recordings show at least some units were successfully using the repeater to communicate inside the South Tower until the moment it collapsed. The Commission report says the North Tower lobby command may not have worked because of a technical problem, the volume control turned all the way down, or because a button that must be pressed to enable it had not been pushed.

On the audio track, an outside agency, possibly in New Jersey and using a repeater, comes through the receive audio on the Port Authority Repeater 7 system. An ambulance being dispatched by the outside (non-FDNY) agency is heard. This may be what the FDNY had described as interference caused when the repeater was left enabled at all times. The distant user appears to be repeated through the system, (possibly on the same CTCSS tone as was configured in Repeater 7). This appears to be a distant co-channel user on the same input frequency as Repeater 7. It's possible that by the random button pressing, a user sent a function tone that temporarily put the base station in "monitor" and that's what caused the outside agency's traffic to be heard. This is unlikely because subsequent transmit function tones should have toggled the receiver from monitor back to CTCSS-enabled.

An oral history interview revealed the Port Authority UHF radios were normally used at incidents inside the World Trade Center. The interviewee said in normal, day-to-day calls, the WTC staff handed Port Authority UHF radios to firefighters on their arrival and that these radios, "worked all over." This implies, but does not prove, that it was common knowledge among department members that FDNY radios had coverage problems inside the buildings. The 9-11 Commission uses the phrase, "performed poorly" to describe FDNY radios during the incident.

Oral history files show that at least four channels were employed at WTC:

One officer said a channel named "Command 3" was used for the North Tower. To those unfamiliar with the details of the FDNY system, it is unclear whether the interviewee meant Tactical 3 or a fifth channel.

FDNY personnel are seen using radios during the documentary footage of the WTC lobby area. Analysis of these scenes showed the radios all appeared to be receiving properly. Oral history files confirm radio communications were at least partly functional.

A problem that shows up in these types of incidents is that receivers in hand-held radios are subjected to signal levels that are likely to overload the receiver. Several radios may be transmitting within feet of one another on different channels. If overloading occurs, only very strong signals can be received while weaker signals disappear and are not received. The hand held radio receivers shown in the documentary appeared to work properly even though several other hand-held radios were transmitting only feet away. This is a hostile environment and suggests the hand-held equipment used by FDNY had good quality receivers, though in this case, the suggestion is incorrect. Second-hand observation is hardly the proper way to 'test' radio receivers or to distinguish 'good quality' from 'bad' and this is likely a source of continued misunderstanding; particularly when these same radios were operating at higher floors, in closer proximity to, and in direct line-of-sight of digital cellular repeaters. Those repeaters were likely operating at unlicensed power levels, which was a common practice of cellular providers at the time, and continues to this day. Footage reveals intelligible recovered audio coming out of the radios and shows radio users communicating with others. This may not have been true of the entire WTC complex but was true of radio users in the crowded lobby.

Analysis of the 26 FDNY audio CDs showed the radios seemed to transmit into the radio systems okay. Radios calling dispatch got through. Calling units were intelligible. Users spoke with dispatchers. Dispatchers answered in ways that suggest they understood what was said. There were no noisy or truncated transmissions heard on any channel, (the equivalent of a dropped cellular call). This suggests the Fire Department's radio backbone is soundly designed and working properly. It is possible that system coverage problems are present; problems that could have been mitigated had the Command Post radio (with greater transmit power) been used. It is also likely that some transmissions did not reach any of the receivers in the system and therefore would not be a detectable problem when listening to the recordings. At the same time those recordings were made, the cellular system was operating at or near full-capacity, meaning every cellular repeater was transmitting. The dense RF interference environment created in NYC that day was essentially a 'perfect storm'; one in which a radio designed 25 years prior could not possibly contend with.

In some scenes, captured documentary audio showed the channels were busy. In some cases, two or more conversations were taking place over a single radio channel at the same time. Users on Tactical 1 may have been close enough to one another to communicate because signals in proximity to each other would overpower weaker signals. At any incident of this size, there is likely to be some overlapping radio traffic. In the same way that large incidents exhaust all the firefighting vehicles and staff, the radio channel resources may become taxed to their limits. NIST says about one third of the fire department radio transmissions were not complete or not understandable.

Some radio users had selected the wrong channels. For example, on the Repeater 7 channel, a unit was heard to call "Manhattan" dispatch and "Citywide". Although the circumstances that lead to the user selecting the wrong channel are not known, this can occur when the user is trapped in darkness or smoke and cannot see the radio. Users will typically try to count steps in a rotary switch channel selector starting from one end of the switch's travel.

A communications van operated by FDNY responded to the incident. Its radio identifier was, "Field Comm." A backup van was in use on the day of the incident because the primary van was out-of-service. The backup van was destroyed and audio recordings of tactical channels used at the incident site were lost.

One annoyance with the fire systems was the presence of "unit ID" data bursts. These constant squawks, heard at the end of transmissions, are decoded at dispatch to identify the calling radio. The annoyance of the data bursts is a trade-off that could help find a firefighter who has been injured or needs help. It also automatically displays the unit ID at the dispatch console. In most systems, it also saves dispatch personnel from typing the unit ID. They press one key and the calling unit's ID is inserted into the current CAD screen or command line.

Recordings show radios were programmed to send unit ID on tactical channels. Radios accept unit ID on a per-channel basis. When mobile or hand-held radios are programmed, the unit ID encoders should be disabled on all channels where the feature is not used. This saves air time for about two to three syllables of speech per push-to-talk press. For example, unless the communications van or chief's vehicles had push-to-talk unit ID decoders, or the channels were recorded for later analysis where unit IDs were decoded from the recordings, the encoders should be turned off for tactical channels to reduce air time used.

It also sounded like some vehicle radios may have had "status buttons" using the data bursts. If true, the operator presses a button on the vehicle radio which sends a short data burst to dispatch. Dispatch gets the unit identity and the new status from a data decoder. These can cause interruptions in voice traffic but cut down on total air time required to conduct business because they occupy the channel for less time than it takes to say, "Engine fifty on scene."

This channel was the primary method of communication in the North tower. It was a simplex channel. Users complained it would only reach from the lobby to floors in the thirties. Tactical 1 was a default channel for use at some fire scenes. Some users who realized Repeater 7 was functional switched to that channel and were afforded better coverage than simplex users on Tactical 1. Audio recordings on the documentary film and NIST analysis show Tactical 1 was overloaded with heavy radio traffic. In contrast, the audio CD of Repeater 7 shows the channel was mostly idle.

The 9-11 commission report said a new portable repeater system had been developed to address shortcomings of Tactical 1 at a large incident. The system, called, "the post", is carried to an area near the incident and set up for the duration to augment weak signals.

The command channel used by officers at the incident was either called "Channel 5" or "Command 5" in documentation. Documents suggest the channel had a repeater but it was not clear if the repeater was citywide, installed in the Field Comm van, or housed in a battalion chief's vehicle. Recordings of this channel were lost when the Field Comm van was destroyed. The documentary film and oral history records show the channel being used effectively.

The federal 9/11 Commission Report included recommendations on communications systems used by police, fire, and emergency medical services (EMS) at the WTC incident. In the report and in appearances on television news programs, commissioners said the capabilities of communications systems lacked the ability to communicate across department lines. That is to say, police units could not communicate with fire units directly by radio. Ambulances could not talk with police units directly by radio. Commission member Lee Hamilton, in several television appearances related to a 2006 book on the topic of the WTC incident, reiterated this factually correct view.

An example that was cited by Hamilton: during the incident the Police Department helicopter was unable to communicate with Fire Department units in order to warn them of the towers' imminent collapse. The NIST document suggests the helicopter may have been able to offer several minutes warning. "Several minutes" may have been enough to get some people from the lower floors outside. This warning of imminent collapse went out over at least one police radio channel but there is nothing showing it was relayed to other people or channels. FDNY operates at least two communications vans: one of which was brought to the scene at the WTC incident. The Commission report reveals the primary FDNY van was equipped to talk to NYPD helicopters but the backup van (which had no NYPD helicopter capability) was in use on September 11, 2001.

In practice, many US helicopters used in emergency services are equipped with radios that allow communications on nearly any conventional two-way radio system, so long as the aircrew know the frequency and associated signaling tones. The radios usually have presets, like a car's broadcast radio, that allow some channels to be configured ahead of need. There was no information in the Commission report suggesting NYPD helicopters had such a capability.

While it is technically possible to implement communications across departments, doing so introduces a host of new training and incident command problems. These are problems that would need to be managed in addition to the existing set of issues present at any large incident. The ability to maintain command, and monitor the safety of, groups working at an incident is diminished if a group of firefighters cannot be reached because they've switched over to the EMS channel. This could cause people to be sent to rescue them when there was no need. Similarly, if the Manhattan EMS dispatcher can't reach an ambulance because they are on one of the fire channels, patient care is affected. New York City Police Commissioner Raymond Kelly, appearing on the "Charlie Rose" show, expressed his view that the existing radio systems performed satisfactorily during the WTC incident. In his view, the interoperability desired by the 9-11 Commission was not needed.

These problems are not new to the World Trade Center incident; cross-department and cross-discipline communication has been a hotly contested and long-identified issue. For example, at the Oklahoma City federal building bombing incident, the inability to communicate among departments was also cited as a problem. Firefighters heard an evacuation order on their radio channel because of the reported presence of a second bomb. Police and EMS workers reportedly did not know of the order.

In Hurricane Katrina's wake, a sergeant in the Louisiana Department of Wildlife and Fisheries appeared on national television to describe not being able to reach persons from other agencies who were assisting with the recovery. She described seeing the people in a nearby boat but not being able to communicate with them.

Even if the technical problems are solved, the issue is more complicated than just adding radio channels or talk groups. It is also a cultural problem. In one local incident, a large number of officers from three police agencies were fielded to search for a violent criminal who had evaded officers from one of the agencies. The officers did not coordinate by switching to a shared radio channel. After the incident, one participant said the users thought their radios were incompatible and did not understand how the shared channel worked. This possibly reflects a training problem or a technology literacy problem. The problem seems to have been remedied since then.

In another instance, a fire agency had thoroughly trained for interoperability scenarios. During an incident where two agencies with different radio channels responded, a decision-maker said personnel from his agency would stay on their own channel. Decision-makers may occasionally act in unpredictable ways, even if technology literate and well-trained. It is not solely a technical problem, but an operational problem as well. Changes to ICS command structure, or operational changes in how the command post for an incident is set up, may produce better results than buying equipment or adding channels. Sometimes there are interoperability problems even where a structure for interoperability exists.

One view of the Incident Command System is that units across department lines would communicate with their own representative at the command post or division level. That representative would relay any needs to another department. For example, a fire unit requesting five paramedic ambulances would identify the magnitude of a medical problem to their fire officer at the command post. This request would add to their commander's operational picture of the division or incident command as she called EMS to request the ambulances. Situation awareness is an important part of effective command and is easy to lose at a large incident. Bypassing incident commanders can contribute to a decomposing of command.

One approach to cross-department netting is the capability of some modern trunked systems to provide a function called "dynamic regrouping"; a feature that Motorola doesn't support in simplex (e.g. 'fireground') operations. It is therefore necessary for a disaster to be near enough the infrastructure to allow for repeater access/operation. Many agencies with Motorola trunked systems already have this capability but it's hardly ever used; even in a crisis. The difficulty of operating such a system is often too great for poorly educated dispatchers who often have no college – much less any particular training in computers or communications systems – other than the 'cursory' training they receive in a 3 or 5 day class the vendors offer. The feature allows the dispatch center personnel to send units from different agencies who are responding to the same incident to a common talk group or virtual channel. This assumes the agencies all share a capability to operate over the same trunked radio system, which is rare. In an informal survey of three agencies with trunked systems that included this feature, users at two sites reported they did not think their system included the feature. A representative from a third site said he "...thought they had the feature but never used it." Of the three agencies with the feature, no one knew how to use it. This would suggest, (in at least the three agencies contacted,) that "dynamic regrouping" was not valuable. Like other disaster readiness processes, users would have to practice using the feature in order for it to be useful during an incident.

Some agencies use commercial two-way radio as an adjunct to their own communications networks. One professional engineering evaluation of public safety radio systems explains that commercial systems such as Nextel's are not built to the same standards of coverage and non-blocking as public safety trunked systems. Like toy walkie talkies marketed to children, they are usable and helpful for non-urgent communications but should not be considered reliable enough for life safety uses. It is also true that most trunked radio system users are likely to hear busy signals, (error tones showing no channels are available,) for the first time during a large disaster. All systems have a finite capacity.

"We don't want or need trunking" is what Chief Charles Dowd (NYPD) was heard to say at an APCO convention in Orlando (2006). NYPD operates a large, conventional repeater network with many legacy channels in the UHF band; and a technology developed "so a large number of users can share a small number of channels" (e.g. trunking) is clearly unnecessary and a frivolous waste of money.

With sufficient channels, there is no need for trunking. There are no 'busy' tones in a conventional repeater system. In the event an individual needs to chime in, he simply waits his turn – just as he would do in a trunked system.

All 911 ambulances and other FDNY vehicles have data terminals, sometimes referred to by staff in recordings and transcripts as MDTs. These terminals are connected to the computer-aided dispatch (CAD) back end or server. They can display text, page through screens describing jobs, and display lists of units assigned to a job.

A thorough analysis of data communications is not possible. What recordings show is that data terminals in at least some field units did not work properly during at least a portion of the incident. At 09:11:14, "Division 3" told Manhattan Fire dispatch, referring to the "summary" screen, "Summary is only giving me a few units. You're going to have to give it to me over the radio. I'm ready to write." This means the terminal was not displaying the entire list of units assigned to Division 3, as it would under normal conditions. The work-around: the Chief had to hand-write the list of units responding. In this one instance, the dispatcher reading the list of about 29 units tied up the Manhattan Fire channel for 53 seconds. During the reading of the list of units responding, one can hear several FDNY units try to interrupt the dispatcher. Their radio traffic was delayed until the entire list was read. This need to read lists of units because of slow or inoperable terminals occurred in at least three or four cases.

It's unclear what caused data delays and incomplete screens on the mobile data terminals. Evidenced by the dispatcher reading the list of units assigned to Division 3, the CAD system was working properly at dispatch positions. At least some field units experienced problems. Possible causes of problems with data terminals in vehicles may have included:

Data terminals are partly purchased and installed to reduce load on dispatch staff and to reduce traffic on voice channels. When they work properly, they have a significant operational benefit. A data outage during an occurrence of high call traffic can quickly overrun dispatch and voice channel capacity in cases where a routine level of calls for service requires both data terminals and voice channels.

New York City Council member Eric Gioia introduced a measure to have the Council investigate the issue of FDNY radio problems.





</doc>
<doc id="28082" url="https://en.wikipedia.org/wiki?curid=28082" title="Timeline for October following the September 11 attacks">
Timeline for October following the September 11 attacks

This article summarizes the events in October 2001 that were related to the September 11 attacks. All times, except where otherwise noted, are in Eastern Daylight Time (EDT), or .
















</doc>
<doc id="28113" url="https://en.wikipedia.org/wiki?curid=28113" title="Timeline for September following the September 11 attacks">
Timeline for September following the September 11 attacks

This article summarizes the events in the remaining days of September 2001 following the September 11 attacks which relate to the attacks. All times, except where otherwise noted, are in Eastern Daylight Time (EDT), or .



The National Day of Prayer and Remembrance





















</doc>
<doc id="28117" url="https://en.wikipedia.org/wiki?curid=28117" title="SAC">
SAC

SAC or Sac may refer to:















</doc>
<doc id="28118" url="https://en.wikipedia.org/wiki?curid=28118" title="Strategic Air Command">
Strategic Air Command

Strategic Air Command (SAC) was both a United States Department of Defense (DoD) Specified Command and a United States Air Force (USAF) Major Command (MAJCOM), responsible for Cold War command and control of two of the three components of the U.S. military's strategic nuclear strike forces, the so-called "nuclear triad", with SAC having control of land-based strategic bomber aircraft and intercontinental ballistic missiles or ICBMs (the third leg of the triad being submarine-launched ballistic missiles (SLBM) of the U.S. Navy).

SAC also operated all strategic reconnaissance aircraft, all strategic airborne command post aircraft, and all USAF aerial refueling aircraft, to include those in the Air Force Reserve (AFRES) and Air National Guard (ANG).

However, SAC did not operate the KB-50, WB-50 and WB-47 weather reconnaissance aircraft operated through the mid and late 1960s by the Air Weather Service, nor did SAC operate the HC-130 or MC-130 operations aircraft capable of aerial refueling helicopters that were assigned to Tactical Air Command (TAC), then Military Airlift Command (MAC), and from 1990 onward, those MC-130 aircraft operated by the Air Force Special Operations Command (AFSOC), or any AFRES (now Air Force Reserve Command (AFRC)) or ANG tactical aerial refueling aircraft (e.g., HC-130, MC-130) operationally gained by TAC, MAC or AFSOC.

SAC primarily consisted of the Second Air Force (2AF), Eighth Air Force (8AF) and the Fifteenth Air Force (15AF), while SAC headquarters (HQ SAC) included Directorates for Operations & Plans, Intelligence, Command & Control, Maintenance, Training, Communications, and Personnel. At a lower echelon, SAC headquarters divisions included Aircraft Engineering, Missile Concept, and Strategic Communications.

In 1992, as part of an overall post-Cold War reorganization of the U.S. Air Force, SAC was disestablished as both a Specified Command and as a MAJCOM, and its and equipment redistributed among the Air Combat Command (ACC), Air Mobility Command (AMC), Pacific Air Forces (PACAF), United States Air Forces in Europe (USAFE), and Air Education and Training Command (AETC), while SAC's central headquarters complex at Offutt AFB, Nebraska was concurrently transferred to the newly created United States Strategic Command (USSTRATCOM), which was established as a joint Unified Combatant Command to replace SAC's Specified Command role.

In 2009, SAC's previous USAF MAJCOM role was reactivated and redesignated as the Air Force Global Strike Command (AFGSC), with AFGSC eventually acquiring claimancy and control of all USAF bomber aircraft and the USAF strategic ICBM force.

The Strategic Air Forces of the United States during World War II included General Carl Spaatz's European command, United States Strategic Air Forces in Europe (USSTAF), consisting of the 8AF and 15AF, and the United States Strategic Air Forces in the Pacific (USASTAF) and its Twentieth Air Force (20AF).

The Operation Overlord air plan for the strategic bombing of both Germany and German military forces in continental Europe prior to the 1944 invasion of France used several Air Forces, primarily those of the USAAF and those of the Royal Air Force (RAF), with command of air operations transferring to the Supreme Commander of the Allied Expeditionary Force on 14 April 1944.

Planning to reorganize for a separate and independent postwar U.S. Air Force had begun by the fall of 1945, with the Simpson Board tasked to plan, "...the reorganization of the Army and the Air Force...". In January 1946, Generals Eisenhower and Spaatz agreed on an Air Force organization composed of the Strategic Air Command, the Air Defense Command, the Tactical Air Command, the Air Transport Command and the supporting Air Technical Service Command, Air Training Command, the Air University, and the Air Force Center.

Strategic Air Command was originally established in the U.S. Army Air Forces on 21 March 1946, acquiring part of the personnel and facilities of the Continental Air Forces (CAF), the World War II command tasked with the air defense of the continental United States (CONUS). At the time, CAF headquarters was located at Bolling Field (later Bolling AFB) in the District of Columbia and SAC assumed occupancy of its headquarters facilities until relocating SAC headquarters (HQ SAC) to nearby Andrews Field (later Andrews AFB), Maryland as a tenant activity until assuming control of Andrews Field in October 1946.

SAC initially totaled 37,000 USAAF personnel. In addition to Bolling Field and, seven months later, Andrews Field, SAC also assumed responsibility for:

SAC also had seven additional CAF bases transferred on 21 March 1946 which remained in SAC through the 1947 establishment of the U.S. Air Force as an independent service. Those installations included:

On 31 March 1946, the following additional installation was also assigned to SAC:

Under the first SAC Commander in Chief, General George C. Kenney, initial units reporting to the Strategic Air Command headquarters on 21 March 1946 included the Second Air Force, the IX Troop Carrier Command and the 73d Air Division.

Fifteenth Air Force was assigned to SAC on 31 March (15th AF's 263rd Army Air Force Base Unit—with —transferred the same date directly under HQ SAC ), while the IX Troop Carrier Command was inactivated the same date and its assets redistributed within SAC.

With postwar demobilization still underway, eight of the ten assigned bomb groups were inactivated before the Eighth Air Force was assigned to SAC on 7 June 1946

Despite the pressures of demobilization, SAC continued the training and evaluation of bomber crews and units still on active duty in the postwar Army Air Forces. Radar Bomb Scoring became the preferred method of evaluating bomber crews, with the last of 888 simulated bomb runs scored against a bombing site near San Diego, California during 1946, subsequently increasing to 2,449 bomb runs by 1947. In the wake of the successful employment of air-dropped nuclear weapons against Hiroshima and Nagasaki to effectively end World War II, SAC became the focus of the nation's nuclear strike capability, to the extent that Joint Chiefs of Staff (JCS) Publication 1259/27 on 12 December 1946 identified that, "...the 'air atomic' strategic air force should only come under the orders of the JCS."

In addition to the strategic bombing mission, SAC also devoted significant resources to aerial reconnaissance. In 1946, SAC's reconnaissance aircraft inventory consisted of F-2 photo variants of the C-45 Expeditor support aircraft, but by 1947 SAC had acquired an F-9C squadron consisting of twelve photo-reconnaissance variants of the B-17G Flying Fortress. An F-13 squadron, the F-13 later re-designated as the RB-29 Superfortress, was also established. SAC conducted routine aerial reconnaissance missions near the Soviet borders or near the 12-mile international waters limit, although some missions actually penetrated into Soviet airspace. The flight profiles of these missions—above 30,000 feet and in excess of 300 knots—made interception by Soviet air forces difficult until the Soviet's 1948 introduction of the MiG-15 jet fighter. Project Nanook, the Cold War's first Top Secret reconnaissance effort, used the first RB-29 missions for mapping and visual reconnaissance in the Arctic and along the northern Soviet coast. Later missions were Project LEOPARD along the Chukchi Peninsula, followed by Projects RICKRACK, STONEWORK, and COVERALLS.

In 1946, the US possessed only nine atomic bombs and twenty-seven B-29s capable at any one time of delivering them. Furthermore, it was later determined that an attack by the 509th Composite Bomb Group during the 1947 to 1948 time frame would have required at least five to six days just to transfer custody of the bombs from United States Atomic Energy Commission (AEC) sites to SAC and deploy the aircraft and weapons to forward operating bases before launching nuclear strikes.

Unfortunately, postwar budget and personnel cuts had had an insidious effect on SAC as its Deputy Commander, Major General Clements McMullen, implemented mandated force reductions. This continued to wear down SAC as a command and morale plummeted. As a result, by the end of 1947, only two of SAC's eleven groups were combat ready. After the 1948 Bikini Atoll nuclear tests, the "Half Moon" Joint Emergency War Plan developed in May 1948 proposed dropping 50 atomic bombs on twenty Soviet cities, with President Harry S. Truman approving "Half Moon" during the June 1948 Berlin Blockade, (Truman sent B-29s to Europe in July). SAC also ordered special ELINT RB-29s to detect improved Soviet radars and, in cooperation with the 51st Air Force Base Unit, SAC also monitored radioactive fallout from Soviet atomic testing on Novaya Zemlya.

In terms of overall Air Force basing and infrastructure, SAC continued to acquire an ever-increasing share of USAF infrastructure and the USAF associated budget. In 1947, before the USAF was established as an independent service, construction commenced on Limestone AAF, Maine (later renamed Loring AFB), a new SAC installation specifically designed to accommodate the B-36 Peacemaker. Fort Dix AAF, New Jersey (later McGuire AFB); Spokane AAF, Washington (later Fairchild AFB); and Wendover Field, Utah (later Wendover AFB) were also transferred to SAC between 30 April and 1 September 1947. Following establishment of the USAF as a separate service, SAC bases in the United States consisted of:

Those bases subsequently added to SAC in the United States included:

In addition to bases under its operational control, SAC also maintained tenant wings at several bases under the control of other USAF MAJCOMs. These non-SAC bases with SAC tenants included

SAC also often maintained a tenant presence at former SAC bases that the command subsequently transferred and relinquished to other MAJCOMs, to include but not limited to:

SAC transferred to the United States Air Force on 26 September 1947, concurrent with the latter's establishment as a separate military service. Units directly under SAC HQ included the 8AF and 15AF, as well as the 311th Air Division, 4th Fighter Wing, 82nd Fighter Wing, 307th Bomb Wing, and two reconnaissance units, the 311th Reconnaissance Wing and the 46th Reconnaissance Squadron. The 56th Fighter Wing was subsequently assigned to SAC on 1 October 1947.

Following the establishment of the U.S. Air Force, most SAC installations on U.S. territory were renamed as "Air Force Base" during late 1947 and into 1948, while non-U.S. installations were renamed as "Air Base".

In May 1948, in an exercise versus Air Defense Command's "Blue" force, a SAC "Red" strike force simulated attacks on Eastern Seaboard targets as far south as Virginia. After a "scathing" 1948 Lindbergh review of SAC operations in the air and at six SAC bases, General Kenney was removed as Commanding General on 15 October 1948 and replaced on 19 October 1948 by 8AF's commander, Lieutenant General Curtis LeMay. Upon Lemay's assumption of command, SAC had only 60 nuclear-capable aircraft, none of which possessed a realistic long range capability against the Soviet Union.

The B-29D, which had become the B-50 in December 1945, was first delivered to SAC in June 1948. This was followed by SAC's first Convair B-36 Peacemaker bomber arriving at Kirtland AFB, New Mexico in September 1948.

In November 1948, LeMay had SAC's headquarters and its command post moved from Andrews AFB, Maryland to Offutt AFB, Nebraska. At Offutt, the command moved into the "A Building", a three-story facility that had previously been used by the Glenn L. Martin Company during World War II. Concurrent with the establishment of this new headquarters facility, Lemay also increased SAC Radar Bomb Scoring (RBS) runs the same year to 12,084. SAC also enhanced its organic fighter escort capability by initiating replacement of its World War II vintage piston-engine F-51D Mustang and F-82E Twin Mustang fighter aircraft with F-84G Thunderjets.

In January 1949, SAC conducted simulated raids on Wright-Patterson AFB, Ohio. Assessments of these simulated raids by "...LeMay's entire command...were appalling", despite the SAC deputy commander, Major General McMullen, having instructed all bomber units to improve their effectiveness. To motivate crews and improve operational effectiveness command-wide, SAC established a competition, the first so-called "Bomb Comp" in 1948. Winners of this inaugural event were the 43rd Bombardment Group (unit) and, for aircrew award, a B-29 team from the 509th Bombardment Group.

Given its global operating environment, SAC also opened its own survival school at Camp Carson, Colorado in 1949, later moving this school to Stead AFB, Nevada in 1952 before transferring the school to the Air Training Command in 1954.

SAC also created Emergency War Plan 1–49 (EWP 1–49), which outlined the means for delivering 133 atomic bombs, "...the entire stockpile...in a single massive attack..." on 70 Soviet cities over a 30-day period.

The first Soviet atomic bomb test occurred on 29 August 1949 and the Joint Chiefs of Staff (JCS) subsequently identified SAC's primary objective was to damage or destroy the Soviet Union's ability to deliver nuclear weapons. The JCS further defined SAC's secondary objective was to stop any Soviet advances into Western Europe, and its tertiary objective was the previous EWP 1–49 industrial mission.

In July 1950, in response to combat operations on the Korean peninsula, SAC dispatched ten nuclear-capable bombers to Guam and deployed four B-29 bomber wings in Korea for tactical operations, although this action caused SAC commander Lemay to comment "...too many splinters were being whittled off the [deterrence] stick".

Initial SAC B-29 successes against North Korea in the summer of 1950 were countered by subsequent Soviet MiG-15 fighter-interceptors, and SAC's 27th Fighter Escort Wing began escorting the bombers with F–84 Thunderjets. Ground-directed bombing (GDB) was subsequently used for close air support (CAS) missions after three SAC radar bomb scoring (RBS) squadron detachments (Dets C, K, & N) arrived at Pusan in September 1950. In 1951, SAC "began to eliminate its combat groups", transferring medium bombardment groups "to Far East Air Forces (FEAF) Bomber Command for combat." In 1951, LeMay convinced the Air Staff to allow SAC to approve nuclear targets, and he continued refusing to submit war plans for JCS review, which the JCS eventually came to accept (of 20,000 candidates in 1960, SAC designated 3,560 as bombing targets—mostly Soviet air defense: airfields and suspected missile sites.)

Although experimented with prior to World War II, SAC refined aerial refueling to a fine art. SAC's in-flight refueling mission began in July 1952 when its 31st Fighter-Escort Wing refueled sixty F-84G Thunderjets from Turner AFB, Georgia to Travis AFB, California non-stop with fuel from twenty-four KB-29P Superfortresses modified into aerial tankers. Exercise FOX PETER ONE followed with 31st FEW fighters being refueled Hickam AFB en route to Hawaii.

On 15 March 1953, a 38th Strategic Reconnaissance Squadron RB-50 returned fire on a Soviet MiG-15, while a 343d Strategic Reconnaissance Squadron RB-50 was shot down over the Sea of Japan 2 days after the Korean Armistice, while on 7 November 1954, an RB-29 was shot down near Hokkaido Island in northern Japan. By the time of 27 July 1953 Korean War cease-fire, SAC B-29s had flown over 21,000 sorties and dropped nearly 167,000 tons of bombs, with thirty-four B-29s lost in combat and forty-eight B-29s were lost to damage or crashes.

SAC's first jet strategic bomber was the swept-wing B-47 medium bomber, which first entered service in 1951 and became operational within SAC in 1953. The B-47 was a component of the October 1953 "New Look" strategy, which articulated, in part, that: ""...to minimize the threat...the major purpose of air defense was not to shoot down enemy bombers—it was to allow SAC...to get into the air"[--and]" not be destroyed on the ground"[--to allow]" massive retaliation".".

Concern of a bomber gap grew after the 1955 Soviet Aviation Day and the Soviets rejected the "Open Skies" Treaty proposed at the Geneva Summit on 21 July 1955. US bomber strength peaked with "over 2,500 bombers" after production "of over 2,000 B-47s and almost 750 B-52s" (circa 1956, 50% of SAC aircraft & 80% of SAC bombers were B-47s).

In an effort to concurrently enhance it reconnaissance capabilities, SAC also received several RB-57D Canberra aircraft in April 1956, with the aircraft initially based at Turner AFB, Georgia. In 1957, these aircraft were forward deployed to Rhein-Main Air Base, West Germany, in order to conduct reconnaissance missions along the borders of the Soviet Union and other Warsaw Pact nations. However, an unintended consequence of this deployment was that Hawker Hunter fighters of the Royal Air Force stationed in the United Kingdom and in continental Europe often intercepted these classified RB-57 missions as they returned to Rhein-Main AB from over the Baltic.

Since it was designed as a medium bomber, SAC's B-47 Stratojet traded speed for range. Because of this shorter range, and in order to better enable the B-47 fleet to reach its target sets in the Soviet Union, SAC routinely deployed its US-based B-47 wings to overseas forward operating bases in North Africa, Spain and Turkey. This program, in effect from 1957 to 1966, was known as "Reflex" with Sixteenth Air Force (16AF), a SAC numbered air force permanently stationed in Europe, having tactical and administrative control of the forward-deployed aircraft and units.

Beginning in 1955, SAC also moved a portion of its bomber and aerial refueling aircraft to 24-hour alert status, either on the ground or airborne. By 1960, fully one third of SAC's bombers and aerial refueling aircraft were on 24-hour alert, with those crews and aircraft not already airborne ready to take off from designated alert sites at their respective bases within fifteen minutes. Bomber aircraft on ground alert were armed with nuclear weapons while aerial tanker aircraft were sufficiently fueled to provide maximum combat fuel offload to the bombers.

Concurrent with this increased alert posture and in order to better hone strategic bombing skillsets, the 1955 SAC Bombing and Navigation Competition was characterized by radar bomb scoring (RBS) runs on Amarillo, Denver, Salt Lake City, Kansas City, San Antonio and Phoenix; and the 1957 competition (nicknamed "Operation Longshot") had three targets: Atlanta, Kansas City, and St. Louis. This use of RBS with simulated target areas utilizing mobile and fixed bomb scoring sites adjacent to major cities, industrial areas, military installations and dedicated bombing ranges throughout the United States. This format would continue through successive SAC Bombing and Navigation Competitions through the remainder of the 1950s, 1960s, 1970s and 1980s. Commencing in the late 1950s, in addition to representation from every SAC wing with a bombing and/or air refueling mission, later SAC competitions would also include participating bomber and aerial refueling units from the Royal Air Force's Bomber Command and (after 30 April 1968) its successor, RAF Strike Command.

It was described as the "Western Pentagon," specifically a, "...four-story, reinforced concrete and masonry office building..." above ground and a "...segregated, adjacent three-story below ground command post." This was the description of what would become Building 500 at Offutt AFB and the new headquarters complex built expressly for SAC, with construction commencing in 1955. SAC headquarters moved from the A Building at Offutt AFB to Building 500 in 1957. The underground nuclear bunker had 24-inch thick walls and base floor, 10-inch thick intermediate floors, and 24-to-42-inch thick roof. It also contained a war room with six 16-foot data display screens and the capacity to sustain up to 800 people underground for two weeks. The below ground bunker portion of the headquarters complex also contained an IBM 704 computer, which was used to develop monthly weather forecasts at targets, as well as for computing fuel consumption and fallout cloud patterns for planning strike routes and egress routes (e.g., determining the timing as to which targets to bomb first).

In 1957, SAC also constructed The Notch, a facility alternatively known as the 8th Air Force Combat Operations Center (COC) and the Westover Communications Annex, since it was a sub-post of nearby Westover AFB. A 3-story nuclear bunker located on Bare Mountain, Massachusetts, The Notch was built with three-foot thick walls, 1.5 foot thick steel blast doors, and 20 feet underground to protect 350 people for 35 days. The Notch was shut down as a SAC facility in 1970 when 8th Air Force was relocated to Barksdale AFB, Louisiana.

Despite this investment in "hardened" headquarters and command and control facilities, the 1957 Gaither Commission identified, "...little likelihood of SAC's bombers surviving [a Soviet first strike] since there was no way to detect an incoming attack until the first [Soviet nuclear weapon] warhead landed." As a result, SAC's bombers and tankers began sitting armed ground alert at their respective bases on 1 Oct 57.

In another organizational change during this time period, SAC's fighter escort wings were transferred to Tactical Air Command (TAC) during 1957 and 1958. Finally, during January 1958's Exercise Fir Fly, SAC "faker" aircraft (twelve B-47s) simulated bombing strikes against metropolitan areas and military installations in the United States defended by Air Defense Command's 28th Air Division.

After SAC's 1st Missile Division was activated on 18 March 1957, SAC HQ established the Office of Assistant CINCSAC (SAC MIKE) at the Air Force Ballistic Missile Division in California on 1 January 1958. SAC MIKE was responsible for missile development liaison, the intermediate range Jupiter and Thor missiles having been transferred to SAC for alert in 1958.

Beginning on 1 February 1958, a SAC Liaison Team was also located at the NORAD Command Post at Ent AFB, Colorado, and the two commands agreed that direct land line communications should connect SAC bases with NORAD's Air Defense Direction Centers. Also in the late 1950s, SAC continued to enhance its intelligence collection activities and develop innovative means of improving the survivability of its forces to surprise attack. From 1958–, a SAC Detachment (TUSLOG Det 50) operated at Incirlik AB, Turkey, monitoring Soviet missile telemetry from the Kapustin Yar and Tyuratam launch complexes, while in 1959, SAC's Operation Big Star studied, prototyped and evaluated the potential of deploying of Minuteman I ICBMs on civilian railroad tracks via USAF-operated locomotives and trains.

President Eisenhower approved the first Atlas ICBM launch by a SAC crew for 9 September 1959 at Vandenberg AFB.

While missile operations continued to ramp up, robust training for flight crews to ensure survivability for strike missions also continued. In some instances SAC bombers would oppose ADC fighter-interceptors simulating Soviet interceptors. Conversely, SAC assisted ADC readiness by simulating Soviet bomber threats to the continental United States that ADC fighters would respond to. However, following a mid-air collision between an ADC F-102 and a SAC B-47 during a 17 December 1959 Quick Kick exercise, simulated NORAD fighter attacks were prohibited against SAC bombers.

On 18 March 1960, SAC intercontinental missiles began alert at Maine's Snark Missile Launch Complex adjacent to Presque Isle AFB. The following month, on 22 April 1960, SAC turned over the last British-based PGM-17 Thor IRBM to the Royal Air Force. This was soon followed by SAC's first Titan I ICBMs at Lowry AFB's Titan I Missile Complex 1A in Colorado being placed on alert that June.

Beginning in November 1959, in order to counter Soviet surface-to-air missile threats, SAC began adding low-altitude bombing training for its manned bomber force as an adjunct to its legacy high-altitude training. Use of low level flight route corridors known as "Oil Burner" routes (later renamed "Olive Branch" routes in the 1970s), and the first of three SAC RBS trains were utilized starting in 1960. On 30 June 1960, SAC had 696 aircraft on alert in the Zone of Interior, also known as the ZI (referred to today as the Continental United States, or CONUS) and at overseas bases. These 696 aircraft were 113 B-52s, 346 B-47s, 85 KC-135s, and 152 KC-97s. SAC's Emergency War Order (EWO) required the first aircraft to be airborne within 8 minutes and all aircraft to be airborne within 15 minutes after notification.

During the mid-1950s, having recalled numerous World War II USAAF and Korean War USAF combat veteran pilots, navigators, bombardiers and aircrewmen from inactive reserve status back to various lengths of active duty, SAC took the lead in integrating the Air Force's reserve components into the overall SAC structure. By the beginning of the 1960s, SAC had also engineered the assignment of KC-97 Stratotanker aerial refueling aircraft to Air National Guard groups and wings and having them fall under SAC's operational claimancy.

On 11 August 1960, President Eisenhower approved the creation of the Joint Strategic Target Planning Staff (JSTPS), co-located at SAC headquarters at Offutt AFB.) JSTPS also included non-SAC agencies tasked with preparing the Single Integrated Operation Plan, or SIOP, and the National Strategic Target List for nuclear war.

On 1 July 1960, a SAC RB-47 with a six-man crew was shot down in international airspace over the Barents Sea by a Soviet MiG-19. Four of the crewmen were killed and two surviving crewmen were captured and held in Lubyanka Prison in Moscow for seven months.

On 3 February 1961, SAC's Boeing EC-135 Looking Glass, began operations as the Airborne Command Post for the Nuclear Triad and the Post-Attack Command and Control System. From this date and for the next 29 1/2 years, until 24 July 1990, SAC would maintain at least one Looking Glass aircraft continuously aloft 24 hours a day, 365 days a year, with an embarked SAC general officer and battle staff, ready to assume command of all strategic nuclear strike forces in the event that SAC headquarters was destroyed in a Soviet first strike. 

SAC's airborne alerts during this period also included Operation Chrome Dome for the bomber and tanker force. Although ostensibly a peacetime mission, Chrome Dome placed heavy demands on flight crews and five B-52 aircraft were lost to airborne mishaps during the operation's eight-year period.

On 11 May 1961, SAC took delivery of its first B-58 Hustler supersonic medium bomber, assigning it to the 305th Bombardment Wing at Bunker Hill AFB. Optimized for high-altitude, high-speed penetration into Soviet territory prior to Soviet advancements in high-altitude surface-to-air missiles, the B-58 was expensive to operate and inefficient at lower altitudes. Its service in SAC would be comparatively short, eventually being replaced by the FB-111 by 1970.

After an early 1961 development by SAC of a Radar Bomb Scoring (RBS) field kit for use in the U.S. Army's Nike surface-to-air missile systems, SAC aircraft flew several mock penetrations into Air Defense Command sectors in the 1961 SAGE/Missile Master test program, as well as the joint SAC-NORAD Sky Shield II exercise followed by Sky Shield III on 2 September 1962.

In 1961, following the Berlin Crisis, President John F. Kennedy increased the number of SAC aircraft on alert to 50 percent and during periods of increased tensions SAC kept some B-52 airborne in the event of a surprise attack.

In 1962, SAC gained full control of the various "Q Areas" developed by Sandia Laboratories for nuclear weapon storage adjacent to Loring AFB (Site E (Maine)/Caribou AFS), Ellsworth AFB (Site F (South Dakota)/Rushmore AFS), Fairchild AFB (Site G (Washington)/Deep Creek AFS), Travis AFB (Site H (California)/Fairfield AFS), and Westover AFB (Site I (Massachusetts)/Stony Brook AFS). These adjunct sites were subsequently converted to USAF-operated and maintained weapon storage areas (WSAs) in the same manner as WSAs on other SAC bases.

The solid fuel LGM-30A Minuteman I was first deployed in 1962 and the LGM-25C Titan II reached operational service in 1963. Project Added Effort phased out all first-generation ICBMs beginning on 1 May 1964 when Atlas-D were taken off alert at Vandenberg AFB's 576th SMS (LGM-30F Minuteman II replaced Minuteman I in 1965).

In October 1962, an SAC BRASS KNOB mission U-2 piloted by Major Richard S. Heyser detected Soviet intermediate range ballistic missiles in Cuba. BRASS KNOB operations involving multiple U-2 aircraft were subsequently commenced at a forward operating location at McCoy AFB, Florida the same month. On the morning of 27 October, a SAC RB-47H of the 55th Strategic Reconnaissance Wing, forward deployed to Kindley AFB, Bermuda crashed on takeoff, killing all four crewmembers, while later that afternoon, a 4028th Strategic Reconnaissance Squadron U-2 forward deployed to McCoy AFB for BRASS KNOB operations was shot down over Cuba by an SA-2 Guideline missile, killing the pilot, Major Rudolf Anderson.

Throughout the early 1960s, the Kennedy Administration, under the aegis of Secretary of Defense McNamara, cancelled numerous SAC modernization programs. This included the Mach 3 North American B-70 Valkyrie in 1961, the GAM-87 Skybolt missile in 1962, and the Rocky Mountain Deep Underground Support Center in 1963. The B-70's demise came due to its design as a high-altitude bomber with very limited low-altitude performance, making it vulnerable to rapid advances in Soviet high altitude surface-to-air missile defense systems. The following year, Skybolt, an air-launched ballistic missile, was cancelled following numerous test failures and the perceived greater reliability of land-based and submarine-based ballistic missile systems. Although initially entering service in 1957, SAC's 2nd-generation aerial refueling aircraft, the KC-135 Stratotanker, had reached sufficient inventory numbers to allow SAC to begin divestiture of its KC-97 Stratofreighter tankers, transferring them to SAC-gained Air Force Reserve and Air National Guard units. As the KC-135 became the primary aerial tanker in active service, SAC employed the aircraft for several non-stop B-52 and KC-135 flights around the world, demonstrating that SAC no longer needed to depend on Reflex stations at air bases in Spain and Britain.)

After the Secretary of Defense rejected LeMay's November 1964 proposal for a "...strategic air campaign against 94 targets in North Vietnam...", thirty SAC B-52Fs were deployed to Andersen AFB, Guam on 17 February 1965, representing the first increment of SAC aircraft forward deployed for the Vietnam War. The following month, in March 1965, the Strategic Air Command Advanced Echelon (SACADVON) was established as a "...liaison unit for CINCSAC [was] located at MACV Headquarters to assist with the B-52 effort."

On 23 May 1965, SAC B-52Fs began unarmed missions for radar mapping "...and later to test bombing with the assistance of ground homing beacons..." SAC began saturation bombing on 18 June 1965 (8000 tons per month in 1966) and conducted Operation Arc Light missions from 1965 until the end of hostilities involving U.S. forces in 1973.

All B-52F missions in 1965 were against targets in South Vietnam (RVN) except for the December "...Duck Flight mission [that] hit a suspected VC supply storage area [for which] part of the target box was in Laos." In April 1966, Vietnam operations began with the B-52D model, a 1956 model designed to use the AGM-28 Hound Dog cruise missile and the ADM-20 Quail aerial decoys for low altitude operations and modified in late 1965 by Project Big Belly to increase conventional bomb capacity.

SAC's RBS Squadrons were discontinued when most detachment personnel transferred to Vietnam from 1966 to 1973 for Combat Skyspot ground-directed bombing operations. The first "Quick Reaction" bombing was the "Pink Lady" mission on 6 July 1966 using SAC B-52D/Fs to support the U.S. Army's 1st Air Cavalry Division. The 1972 Operation Linebacker II also used Skyspot for Hanoi/Haiphong bombings in North Vietnam which resulted in the loss of 25 SAC aircrew members.

By May 1967, SACADVON had moved to Seventh Air Force headquarters at Tan Son Nhut Air Base, South Vietnam to schedule and coordinate "...strikes for the 7th AF and MACV." From a level of 161,921 military and 20,215 civilian assigned to SAC in June 1968, SAC lost 13,698 first term airmen from November 1968 to May 1969 in a three phase drawdown known as Project 693 to comply with Public Law 90-364.

While conventional bombing, air refueling and strategic air reconnaissance operations in Southeast Asia increasingly occupied SAC's operational commitments, SAC's primary mission of nuclear deterrence continued to remain its primary focus. In 1969, "...SAC's B-52s and B-58s could carry B28, B41, B43, B53, and BA53 nuclear weapons" (SAC had 311 nuclear AGM-28 Hound Dog missiles at the end of the year.) This also coincided with the B-58 Hustler's in-progress retirement from SAC's active inventory and its replacement with the FB-111.

On 18 March 1969, along the South Vietnamese border, SAC first bombed Cambodia (Operation Menu through 26 May 1970 was controlled by Skyspot). On 17 February 1970, SAC conducted the first "GOOD LOOK" bombing of Laos at the Plaine des Jarres after B-52 photorecon missions ("GOOD LOOK ALPHA" in August 1969 and "GOOD LOOK BRAVO" ) and the observations of a Skyspot installation in Thailand. SAC transferred "...HQ 8th AF...to Andersen AFB, Guam on 1 April 1970 to oversee B-52D/G operations and to complement SACADVON". 8th AF took over from Third Air Division the generation of "frag" orders based on daily strike requests and amendments from COMUSMACV.

In 1970, SAC deployed the LGM-30G Minuteman III ICBM with multiple independently targetable reentry vehicle or MIRVs, for striking 3 targets, while concurrently retiring the B-58 Hustler supersonic bomber.

1972 saw the commencement of Operation Linebacker II, a combined Seventh Air Force and U.S. Navy Task Force 77 aerial bombing campaign, conducted against targets in North Vietnam during the final period of US involvement in the Vietnam War. Linebacker II was conducted from 18 December to 29 December 1972, leading to several informal names such as "The December Raids" and "The Christmas Bombings". Unlike the previous Operation Rolling Thunder and Operation Linebacker interdiction operations, Linebacker II would be a "maximum effort" bombing campaign to destroy major target complexes in the Hanoi and Haiphong areas which could only be accomplished by SAC B-52D/Gs. It saw the largest heavy bomber strikes launched by the U.S. Air Force since the end of World War II. Linebacker II was a modified extension of the Operation Linebacker bombings conducted from May to October 1972, with the emphasis of the new campaign shifted to attacks by B-52 Stratofortress heavy bombers rather than smaller tactical fighter aircraft. During Linebacker II, a total of 741 B-52D/G sorties were dispatched from bases in Thailand and Guam to bomb North Vietnam and 729 actually completed their missions. Overall SAC losses during Linebacker II numbered fifteen B-52s. The U.S. government claimed that the operation had succeeded in forcing North Vietnam's Politburo to return to the negotiating table, with the Paris Peace Accords signed shortly after the operation.

By early 1973, offensive SAC air operations in Southeast Asia ceased and numerous SAC aircrewmen who had been shot down and captured as prisoners of war by North Vietnam were repatriated to the United States.

SAC aircraft used during the Vietnam War included B-52D, B-52F, B-52G, KC-135A, KC-135Q, various versions of the RC-135, SR-71, U-2, and EC-135.

During the Vietnam War, due to the escalating costs of combat operations in Southeast Asia, SAC was required to close several SAC bases, consolidate other bases, or transfer several bases to other MAJCOMs, other services, or the Air Reserve Component in order to remain within budgetary constraints. This included:

With the Vietnam War draw-down following the Paris Peace Treaty in 1973, reduced defense budgets forced SAC to inactivate several more wings, close still more bases in CONUS and Puerto Rico, transfer still additional bases to other MAJCOMS or the Air Reserve Component, and retire older B-52B, B-52C, B-52E and B-52F aircraft:

In 1973, the National Emergency Airborne Command Post, or NEACP, aircraft entered SAC's inventory. Consisting of four Boeing E-4 aircraft, these highly modified Boeing 747 airframes were assigned to the 55th Strategic Reconnaissance Wing at Offutt AFB and were forward deployed as necessary to support the National Command Authority.

By 1975, SAC's manned bomber strength included several hundred B-52D, B-52G, B-52H and FB-111A aircraft, and "...SAC's first major exercise in 23 years" was Exercise Global Shield 79. As for the ICBM force, SAC reached a peak strength of 1000 Minuteman II and III and 54 Titan II ICBMs on active status before seeing reductions and retirements through a combination of obsolescing systems and various arms reduction treaties with the Soviet Union.

By 1977, SAC had been pinning its hopes for a new manned strategic bomber in the form of the Rockwell B-1A Lancer. However, on 30 June 1977, President Jimmy Carter Carter announced that the B-1A would be canceled in favor of ICBMs, submarine-launched ballistic missiles (SLBMs), and a fleet of modernized B-52s armed with air-launched cruise missiles (ALCMs).

On 1 December 1979, SAC assumed control of the ballistic missile warning system (BMEWS) and all Space Surveillance Network facilities from the inactivating Aerospace Defense Command (ADC). These activities would later be (transferred to Air Force Space Command (AFSPC) when the latter was established in 1982. SAC also continued to operate the Air Force's entire KC-135 aerial refueling fleet, its EC-135 LOOKING GLASS and E-4 NEACAP command post aircraft, as well the entire strategic reconnaissance aircraft fleet consisting of the U-2, SR-71, RC-135, and WC-135.

In 1981, SAC received a new air refueling tanker aircraft to supplement the aging KC-135 Stratotanker force. Based on the McDonnell Douglas DC-10 commercial airliner, the KC-10A Extender was deployed equipped with improved military avionics, aerial refueling, and satellite communications equipment. That same year, President Ronald Reagan reversed the 1977 Carter administration decision regarding the B-1, directing that 100 examples of a refined version of the aircraft, now designated the B-1B Lancer, be procured as a long-range combat aircraft for SAC.

The LGM-118A Peacekeeper ICBM reached SAC in 1986, and the 114 Peacekeepers had a total warhead yield of about 342 megatons. This also served to offset the retirement of the obsolescent and maintenance-intensive LGM-25C Titan II ICBM, the last example of which was deactivated in May 1987. An additional underground "16,000 square-foot, two-story reinforced concrete" command post for HQ SAC was also constructed at Offutt AFB from 1986 to 1989 from a design by Leo A. Daly, who had designed the adjoining 1957 bunker. The first Rockwell B-1B Lancer was also delivered to SAC in 1987. 

On 22 November 1988, the Northrop Grumman B-2 Spirit, under development as the Advanced Technology Bomber (ATB), a so-called "black program" since 1979, was officially acknowledged and rolled out for the first time for public display. The first "stealth bomber" designed for SAC, the aircraft made its first flight in May 1989.

SAC reorganization at the end of the Cold War began as early as 1988 when the Carlucci Commission planned the closure of:

The closures were the beginning of a post-Cold War process that would later become known as Base Realignment and Closure or BRAC. Although Mather AFB's navigator training mission would relocate to Randolph AFB, Texas, the Mather bomber/tanker wing would inactivate and the AFRES tanker group would relocate to nearby McClellan AFB, relocating again four years later to Beale AFB when another BRAC process would close McClellan AFB.

Concurrently, the Pease AFB bomber/tanker wing would lose its FB-111 aircraft and transfer to Whiteman AFB, Missouri in preparation for transition to the B-2 Spirit while a portion of Pease would be transferred to the New Hampshire Air National Guard for its ANG air refueling wing and be renamed Pease Air National Guard Base.

Additional closures and divestments of SAC bases would continue throughout the late 1980s and early 1990s, accelerating even more so as a result the START I Treaty's mandated elimination of both the entire B-52G fleet and the inactivation of all Minuteman II and Peacekeeper ICBMs, as well as the 1992 reorganization of the Air Force that disestablished SAC and dispersed its assets to other new or existing MAJCOMs, primarily ACC and AMC. In addition to closures of Mather AFB and Pease AFB, this would eventually include the following subsequent closure and realignment actions, primarily due to BRAC:

On 1 July 1989, the 1st Combat Evaluation Group reporting directly to SAC headquarters was split with most HQ 1CEVG organizations transferring to SAC HQ (e.g., the Command Instrument Flight Division) and RBS personnel, equipment, and becoming the 1st Electronic Combat Range Group. Airborne NEACP alerts ended in 1990 and during 1991's Operation Desert Storm to liberate Kuwait from Iraqi invasion and occupation, SAC bomber, tanker and reconnaissance aircraft flew operations (e.g., B-52s with conventional bombs and conventional warhead AGM-86 ALCMs) near Iraq from bases in Great Britain, Turkey, Cyprus, Diego Garcia, Saudi Arabia, and the United Arab Emirates.

Following Operation Desert Storm, the dissolution of the Soviet Union and the "de facto" end of the Cold War, President George H. W. Bush and Secretary of Defense Dick Cheney directed SAC to take all bomber and refueling aircraft and Minuteman II ICBMs off of continuous nuclear alert on 27 September 1991 and placing said aircraft on quick reaction ground alert.

The 31 May 1992 major reorganization of the USAF organizational structure subsequently disestablished SAC, moving its bomber, reconnaissance and aerial command post aircraft and all SAC ICBMs, along with all Tactical Air Command aircraft, to the newly established Air Combat Command (ACC). The newly established Air Mobility Command (AMC) inherited most of SAC's KC-135 Stratotanker aircraft and the entire KC-10 Extender aerial refueling tanker force, while some KC-135s were reassigned directly to USAFE and PACAF, with one additional air refueling wing assigned to the Air Education and Training Command (AETC) as the KC-135 formal training unit.

Land-based ICBMs were later transferred from ACC to Air Force Space Command (AFSPC), while manned bombers remained in ACC. USAF nuclear forces in ACC and AFSPC were then combined with the United States Navy's Fleet Ballistic Missile submarine forces to form the United States Strategic Command (USSTRATCOM), which took over the SAC Headquarters complex at Offutt AFB.

In 2009, the entire land-based USAF ICBM force and that portion of the USAF manned bomber force that was still nuclear-capable, e.g., the B-2 Spirit and B-52 Stratofortress, was transferred to the newly established Air Force Global Strike Command (AFGSC), while the B-1 Lancer conventional bomber force remained in ACC. In 2015, these B-1 units were also transferred to Air Force Global Strike Command, which assumed responsibility for all current and future USAF bomber forces.

The SAC Museum located adjacent to Offutt AFB was moved in 1998 to a site near Ashland, Nebraska and renamed as the Strategic Air and Space Museum in 2001.

Organizations commemorating SAC include the Strategic Air Command Veterans Association, the SAC Society, the B-47 Stratojet Association, the B-52 Stratofortress Association, the FB-111 Association, the SAC Airborne Command Control Association, the Association of Air Force Missileers, the SAC Elite Guard Association and the Strategic Air Command Memorial Amateur Radio Club. After the Cold War, SAC histories included a 1996 almanac and a 2006 organizational history.

In 2009, the Air Force Global Strike Command (AFGSC) was activated with the lineage of Strategic Air Command. AFGSC, headquartered at Barksdale AFB, Louisiana, is one of two USAF component commands assigned to United States Strategic Command (USSTRATCOM). AFGSC currently consists of Eighth Air Force (8AF), responsible for the nuclear-capable manned heavy bomber force, and Twentieth Air Force (20AF), responsible for the ICBM force.



Strategic Air Command in the United Kingdom was among the command's largest overseas concentrations of forces, with additional forces under SAC's 16th Air Force at air bases in North Africa, Spain and Turkey during the 1950s and 1960s.

SAC "Provisional" wings were also located in Kadena AB, Okinawa and U-Tapao Royal Thai Navy Airfield / U-Tapao AB, Thailand during the Vietnam War

SAC also maintained bomber, tanker, and/or reconnaissance aircraft assets at the former Ramey AFB, Puerto Rico in the 1950s, 1960s and 1970s, and at Andersen AFB, Guam; RAF Mildenhall, RAF Fairford and RAF Alconbury in the United Kingdom; Moron AB, Spain; Lajes Field, Azores (Portugal); Diego Garcia, BIOT; and the former NAS Keflavik, Iceland through the 1990s.

SAC also conducted operations from RAF Fairford, RAF Alconbury and RAF Mildenhall in the United Kingdom, Moron AB in Spain, Lajes Field in the Azores (Portugal), RAF Akrotiri in Cyprus, Incirlik AB in Turkey, Diego Garcia in the British Indian Ocean Territory, and from multiple air bases in Egypt, Saudi Arabia, Oman, and the United Arab Emirates during the first Gulf War (Operations Desert Shield and Desert Storm) from 1990 to 1991.




</doc>
<doc id="28119" url="https://en.wikipedia.org/wiki?curid=28119" title="Scheme (programming language)">
Scheme (programming language)

Scheme is a minimalist dialect of the Lisp family of programming languages. Scheme consists of a small standard core with powerful tools for language extension.

Scheme was created during the 1970s at the MIT AI Lab and released by its developers, Guy L. Steele and Gerald Jay Sussman, via a series of memos now known as the Lambda Papers. It was the first dialect of Lisp to choose lexical scope and the first to require implementations to perform tail-call optimization, giving stronger support for functional programming and associated techniques such as recursive algorithms. It was also one of the first programming languages to support first-class continuations. It had a significant influence on the effort that led to the development of Common Lisp.

The Scheme language is standardized in the official IEEE standard and a "de facto" standard called the "Revised Report on the Algorithmic Language Scheme" (R"n"RS). The most widely implemented standard is R5RS (1998); a new standard, R6RS, was ratified in 2007. Scheme has a diverse user base due to its compactness and elegance, but its minimalist philosophy has also caused wide divergence between practical implementations, so much that the Scheme Steering Committee calls it "the world's most unportable programming language" and "a "family" of dialects" rather than a single language.

Scheme started in the 1970s as an attempt to understand Carl Hewitt's Actor model, for which purpose Steele and Sussman wrote a "tiny Lisp interpreter" using Maclisp and then "added mechanisms for creating actors and sending messages". Scheme was originally called "Schemer", in the tradition of other Lisp-derived languages such as Planner or "Conniver". The current name resulted from the authors' use of the ITS operating system, which limited filenames to two components of at most six characters each. Currently, "Schemer" is commonly used to refer to a Scheme programmer.

A new language standardization process began at the 2003 Scheme workshop, with the goal of producing an R6RS standard in 2006. This process broke with the earlier R"n"RS approach of unanimity.

R6RS features a standard module system, allowing a split between the core language and libraries. A number of drafts of the R6RS specification were released, the final version being R5.97RS. A successful vote resulted in the ratification of the new standard, announced on August 28, 2007.

Currently the newest releases of various Scheme implementations support the R6RS standard. There is a portable reference implementation of the proposed implicitly phased libraries for R6RS, called psyntax, which loads and bootstraps itself properly on various older Scheme implementations.

A feature of R6RS is the record-type descriptor (RTD). When an RTD is created and used, the record type representation can show the memory layout. It also calculated object field bit mask and mutable Scheme object field bit masks, and helped the garbage collector know what to do with the fields without traversing the whole fields list that are saved in the RTD. RTD allows users to expand the basic RTD to create a new record system.

R6RS introduces numerous significant changes to the language. The source code is now specified in Unicode, and a large subset of Unicode characters may now appear in Scheme symbols and identifiers, and there are other minor changes to the lexical rules. Character data is also now specified in Unicode. Many standard procedures have been moved to the new standard libraries, which themselves form a large expansion of the standard, containing procedures and syntactic forms that were formerly not part of the standard. A new module system has been introduced, and systems for exception handling are now standardized. Syntax-rules has been replaced with a more expressive syntactic abstraction facility (syntax-case) which allows the use of all of Scheme at macro expansion time. Compliant implementations are now "required" to support Scheme's full numeric tower, and the semantics of numbers have been expanded, mainly in the direction of support for the IEEE 754 standard for floating point numerical representation.

The R6RS standard has caused controversy because it is seen to have departed from the minimalist philosophy. In August 2009, the Scheme Steering Committee, which oversees the standardization process, announced its intention to recommend splitting Scheme into two languages: a large modern programming language for programmers; and a small version, a subset of the large version retaining the minimalism praised by educators and casual implementors. Two working groups were created to work on these two new versions of Scheme. The Scheme Reports Process site has links to the working groups' charters, public discussions and issue tracking system.

The ninth draft of R7RS (small language) was made available on April 15, 2013. A vote ratifying this draft closed on May 20, 2013, and the final report has been available since August 6, 2013, describing "the 'small' language of that effort: therefore it cannot be considered in isolation as the successor to R6RS".

Scheme is primarily a functional programming language. It shares many characteristics with other members of the Lisp programming language family. Scheme's very simple syntax is based on s-expressions, parenthesized lists in which a prefix operator is followed by its arguments. Scheme programs thus consist of sequences of nested lists. Lists are also the main data structure in Scheme, leading to a close equivalence between source code and data formats (homoiconicity). Scheme programs can easily create and evaluate pieces of Scheme code dynamically.

The reliance on lists as data structures is shared by all Lisp dialects. Scheme inherits a rich set of list-processing primitives such as codice_1, codice_2 and codice_3 from its Lisp progenitors. Scheme uses strictly but dynamically typed variables and supports first class procedures. Thus, procedures can be assigned as values to variables or passed as arguments to procedures.

This section concentrates mainly on innovative features of the language, including those features that distinguish Scheme from other Lisps. Unless stated otherwise, descriptions of features relate to the R5RS standard.

"In examples provided in this section, the notation "===> result" is used to indicate the result of evaluating the expression on the immediately preceding line. This is the same convention used in R5RS."

This subsection describes those features of Scheme that have distinguished it from other programming languages from its earliest days. These are the aspects of Scheme that most strongly influence any product of the Scheme language, and they are the aspects that all versions of the Scheme programming language, from 1973 onward, share.

Scheme is a very simple language, much easier to implement than many other languages of comparable expressive power. This ease is attributable to the use of lambda calculus to derive much of the syntax of the language from more primitive forms. For instance of the 23 s-expression-based syntactic constructs defined in the R5RS Scheme standard, 14 are classed as derived or library forms, which can be written as macros involving more fundamental forms, principally lambda. As R5RS says (R5RS sec. 3.1): "The most fundamental of the variable binding constructs is the lambda expression, because all other variable binding constructs can be explained in terms of lambda expressions."

Example: a macro to implement codice_4 as an expression using codice_5 to perform the variable bindings.
(define-syntax let
Thus using codice_4 as defined above a Scheme implementation would rewrite "codice_7" as "codice_8", which reduces implementation's task to that of coding procedure instantiations.

In 1998, Sussman and Steele remarked that the minimalism of Scheme was not a conscious design goal, but rather the unintended outcome of the design process. "We were actually trying to build something complicated and discovered, serendipitously, that we had accidentally designed something that met all our goals but was much simpler than we had intended...we realized that the lambda calculus—a small, simple formalism—could serve as the core of a powerful and expressive programming language."

Like most modern programming languages and unlike earlier Lisps such as Maclisp, Scheme is lexically scoped: all possible variable bindings in a program unit can be analyzed by reading the text of the program unit without consideration of the contexts in which it may be called. This contrasts with dynamic scoping which was characteristic of early Lisp dialects, because of the processing costs associated with the primitive textual substitution methods used to implement lexical scoping algorithms in compilers and interpreters of the day. In those Lisps, it was perfectly possible for a reference to a free variable inside a procedure to refer to quite distinct bindings external to the procedure, depending on the context of the call.

The impetus to incorporate lexical scoping, which was an unusual scoping model in the early 1970s, into their new version of Lisp, came from Sussman's studies of ALGOL. He suggested that ALGOL-like lexical scoping mechanisms would help to realize their initial goal of implementing Hewitt's Actor model in Lisp.

The key insights on how to introduce lexical scoping into a Lisp dialect were popularized in Sussman and Steele's 1975 Lambda Paper, "Scheme: An Interpreter for Extended Lambda Calculus", where they adopted the concept of the lexical closure (on page 21), which had been described in an AI Memo in 1970 by Joel Moses, who attributed the idea to Peter J. Landin.

Alonzo Church's mathematical notation, the lambda calculus, has inspired Lisp's use of "lambda" as a keyword for introducing a procedure, as well as influencing the development of functional programming techniques involving the use of higher-order functions in Lisp. But early Lisps were not suitable expressions of the lambda calculus because of their treatment of free variables.

A formal lambda system has axioms and a complete calculation rule. It is helpful for the analysis using mathematical logic and tools. In this system, calculation can be seen as a directional deduction. The syntax of lambda calculus follows the recursive expressions from x, y, z, ...,parentheses, spaces, the period and the symbol λ. The function of lambda calculation includes: First, serve as a starting point of powerful mathematical logic. Second, it can reduce the requirement of programmers to consider the implementation details, because it can be used to imitate machine evaluation. Finally, the lambda calculation created a substantial meta-theory.

The introduction of lexical scope resolved the problem by making an equivalence between some forms of lambda notation and their practical expression in a working programming language. Sussman and Steele showed that the new language could be used to elegantly derive all the imperative and declarative semantics of other programming languages including ALGOL and Fortran, and the dynamic scope of other Lisps, by using lambda expressions not as simple procedure instantiations but as "control structures and environment modifiers". They introduced continuation-passing style along with their first description of Scheme in the first of the Lambda Papers, and in subsequent papers, they proceeded to demonstrate the raw power of this practical use of lambda calculus.

Scheme inherits its block structure from earlier block structured languages, particularly ALGOL. In Scheme, blocks are implemented by three "binding constructs": codice_4, codice_10 and codice_11. For instance, the following construct creates a block in which a symbol called codice_12 is bound to the number 10:
(let ((var 10))
Blocks can be nested to create arbitrarily complex block structures according to the need of the programmer. The use of block structuring to create local bindings alleviates the risk of namespace collision that can otherwise occur.

One variant of codice_4, codice_10, permits bindings to refer to variables defined earlier in the same construct, thus:

The other variant, codice_11, is designed to enable mutually recursive procedures to be bound to one another.

===> ((1 . 0) (1 . 0) (2 . 1) (2 . 2) (3 . 2) (3 . 3) (4 . 4) (5 . 4) (5 . 5))

All procedures bound in a single codice_11 may refer to one another by name, as well as to values of variables defined earlier in the same codice_11, but they may not refer to "values" defined later in the same codice_11.

A variant of codice_4, the "named let" form, has an identifier after the codice_4 keyword. This binds the let variables to the argument of a procedure whose name is the given identifier and whose body is the body of the let form. The body may be repeated as desired by calling the procedure. The named let is widely used to implement iteration.

Example: a simple counter

===> (1 2 3 4 5 6 7 8 9 10)
Like any procedure in Scheme, the procedure created in the named let is a first class object.

Scheme has an iteration construct, codice_21, but it is more idiomatic in Scheme to use tail recursion to express iteration. Standard-conforming Scheme implementations are required to optimize tail calls so as to support an unbounded number of active tail calls (R5RS sec. 3.5)—a property the Scheme report describes as "proper tail recursion"—making it safe for Scheme programmers to write iterative algorithms using recursive structures, which are sometimes more intuitive. Tail recursive procedures and the "named codice_4" form provide support for iteration using tail recursion.

===> (0 1 4 9 16 25 36 49 64 81)

Continuations in Scheme are first-class objects. Scheme provides the procedure codice_23 (also known as codice_24) to capture the current continuation by packing it up as an escape procedure bound to a formal argument in a procedure provided by the programmer. (R5RS sec. 6.4) First-class continuations enable the programmer to create non-local control constructs such as iterators, coroutines, and backtracking.

Continuations can be used to emulate the behavior of return statements in imperative programming languages. The following function codice_25, given function codice_26 and list codice_27, returns the first element codice_28 in codice_27 such that codice_30 returns true.

===> 7
===> #f
The following example, a traditional programmer's puzzle, shows that Scheme can handle continuations as first-class objects, binding them to variables and passing them as arguments to procedures.
(let* ((yin

When executed this code displays a counting sequence: codice_31
In contrast to Common Lisp, all data and procedures in Scheme share a common namespace, whereas in Common Lisp functions and data have separate namespaces making it possible for a function and a variable to have the same name, and requiring special notation for referring to a function as a value. This is sometimes known as the "Lisp-1 vs. Lisp-2" distinction, referring to the unified namespace of Scheme and the separate namespaces of Common Lisp.

In Scheme, the same primitives that are used to manipulate and bind data can be used to bind procedures. There is no equivalent of Common Lisp's codice_32 and codice_33 primitives.
(define f 10)
f
===> 10
(set! f (+ f f 6))
f
===> 26
(set! f (lambda (n) (+ n 12)))
===> 18
(set! f (f 1))
f
===> 13
(apply + '(1 2 3 4 5 6))
===> 21
===> (101 102 103)
This subsection documents design decisions that have been taken over the years which have given Scheme a particular character, but are not the direct outcomes of the original design.

Scheme specifies a comparatively full set of numerical datatypes including complex and rational types, which is known in Scheme as the numerical tower (R5RS sec. 6.2). The standard treats these as abstractions, and does not commit the implementor to any particular internal representations.

Numbers may have the quality of exactness. An exact number can only be produced by a sequence of exact operations involving other exact numbers—inexactness is thus contagious. The standard specifies that any two implementations must produce equivalent results for all operations resulting in exact numbers.

The R5RS standard specifies procedures codice_34 and codice_35 which can be used to change the exactness of a number. codice_35 produces "the exact number that is numerically closest to the argument". codice_34 produces "the inexact number that is numerically closest to the argument". The R6RS standard omits these procedures from the main report, but specifies them as R5RS compatibility procedures in the standard library (rnrs r5rs (6)).

In the R5RS standard, Scheme implementations are not required to implement the whole numerical tower, but they must implement "a coherent subset consistent with both the purposes of the implementation and the spirit of the Scheme language" (R5RS sec. 6.2.3). The new R6RS standard does require implementation of the whole tower, and "exact integer objects and exact rational number objects of practically unlimited size and precision, and to implement certain procedures...so they always return exact results when given exact arguments" (R6RS sec. 3.4, sec. 11.7.1).

Example 1: exact arithmetic in an implementation that supports exact 
rational complex numbers.

(define x (+ 1/3 1/4 -1/5 -1/3i 405/50+2/3i))
x
===> 509/60+1/3i
(exact? x)
===> #t
Example 2: Same arithmetic in an implementation that supports neither exact 
rational numbers nor complex numbers but does accept real numbers in rational notation.

(define xr (+ 1/3 1/4 -1/5 405/50))
(define xi (+ -1/3 2/3))
xr
===> 8.48333333333333
xi
===> 0.333333333333333
(exact? xr)
===> #f
===> #f
Both implementations conform to the R5RS standard but the second does not conform to R6RS because it does not implement the full numerical tower.

Scheme supports delayed evaluation through the codice_38 form and the procedure codice_39.

===> 22
===> 70
===> 22

The lexical context of the original definition of the promise is preserved, and its value is also preserved after the first use of codice_39. The promise is only ever evaluated once.

These primitives, which produce or handle values known as promises, can be used to implement advanced lazy evaluation constructs such as streams.

In the R6RS standard, these are no longer primitives, but instead, are provided as part of the R5RS compatibility library (rnrs r5rs (6)).

In R5RS, a suggested implementation of codice_38 and codice_39 is given, implementing the promise as a procedure with no arguments (a thunk) and using memoization to ensure that it is only ever evaluated once, irrespective of the number of times codice_39 is called (R5RS sec. 6.4).

SRFI 41 enables the expression of both finite and infinite sequences with extraordinary economy. For example, this is a definition of the fibonacci sequence using the functions defined in SRFI 41:
(define fibs
(stream-ref fibs 99)
===> 218922995834555169026
Most Lisps specify an order of evaluation for procedure arguments. Scheme does not. Order of evaluation—including the order in which the expression in the operator position is evaluated—may be chosen by an implementation on a call-by-call basis, and the only constraint is that "the effect of any concurrent evaluation of the operator and operand expressions is constrained to be consistent with some sequential order of evaluation." (R5RS sec. 4.1.3)

===> 3

ev is a procedure that describes the argument passed to it, then returns the value of the argument. In contrast with other Lisps, the appearance of an expression in the operator position (the first item) of a Scheme expression is quite legal, as long as the result of the expression in the operator position is a procedure.

In calling the procedure "+" to add 1 and 2, the expressions (ev +), (ev 1) and (ev 2) may be evaluated in any order, as long as the effect is not as if they were evaluated in parallel. Thus the following three lines may be displayed in any order by standard Scheme when the above example code is executed, although the text of one line may not be interleaved with another because that would violate the sequential evaluation constraint.

In the R5RS standard and also in later reports, the syntax of Scheme can easily be extended via the macro system. The R5RS standard introduced a powerful hygienic macro system that allows the programmer to add new syntactic constructs to the language using a simple pattern matching sublanguage (R5RS sec 4.3). Prior to this, the hygienic macro system had been relegated to an appendix of the R4RS standard, as a "high level" system alongside a "low level" macro system, both of which were treated as extensions to Scheme rather than an essential part of the language.

Implementations of the hygienic macro system, also called codice_44, are required to respect the lexical scoping of the rest of the language. This is assured by special naming and scoping rules for macro expansion and avoids common programming errors that can occur in the macro systems of other programming languages. R6RS specifies a more sophisticated transformation system, codice_45, which has been available as a language extension to R5RS Scheme for some time.
(define-syntax when
Invocations of macros and procedures bear a close resemblance—both are s-expressions—but they are treated differently. When the compiler encounters an s-expression in the program, it first checks to see if the symbol is defined as a syntactic keyword within the current lexical scope. If so, it then attempts to expand the macro, treating the items in the tail of the s-expression as arguments without compiling code to evaluate them, and this process is repeated recursively until no macro invocations remain. If it is not a syntactic keyword, the compiler compiles code to evaluate the arguments in the tail of the s-expression and then to evaluate the variable represented by the symbol at the head of the s-expression and call it as a procedure with the evaluated tail expressions passed as actual arguments to it.

Most Scheme implementations also provide additional macro systems. Among popular ones are syntactic closures, explicit renaming macros and codice_46, a non-hygienic macro system similar to codice_47 system provided in Common Lisp.

The inability to specify whether or not a macro is hygienic is one of the shortcomings of the macro system. Alternative models for expansion such as scope sets provide a potential solution.

Prior to R5RS, Scheme had no standard equivalent of the codice_48 procedure which is ubiquitous in other Lisps, although the first Lambda Paper had described codice_49 as "similar to the LISP function EVAL" and the first Revised Report in 1978 replaced this with codice_50, which took two arguments. The second, third and fourth revised reports omitted any equivalent of codice_48.

The reason for this confusion is that in Scheme with its lexical scoping the result of evaluating an expression depends on where it is evaluated. For instance, it is not clear whether the result of evaluating the following expression should be 5 or 6:

If it is evaluated in the outer environment, where codice_52 is defined, the result is the sum of the operands. If it is evaluated in the inner environment, where the symbol "+" has been bound to the value of the procedure "*", the result is the product of the two operands.

R5RS resolves this confusion by specifying three procedures that return environments and providing a procedure codice_48 that takes an s-expression and an environment and evaluates the expression in the environment provided. (R5RS sec. 6.5) R6RS extends this by providing a procedure called codice_54 by which the programmer can specify exactly which objects to import into the evaluation environment.

With modern scheme (usually compatible with R5RS) to evaluate this expression, you need to define function codice_49 which can look like this:
codice_56 is global environment from your interpreter. That's why codice_57 still point to plus operation.

In most dialects of Lisp including Common Lisp, by convention the value codice_58 evaluates to the value false in a boolean expression. In Scheme, since the IEEE standard in 1991, all values except #f, including codice_58's equivalent in Scheme which is written as '(), evaluate to the value true in a boolean expression. (R5RS sec. 6.3.1)

Where the constant representing the boolean value of true is codice_60 in most Lisps, in Scheme it is codice_61.

In Scheme the primitive datatypes are disjoint. Only one of the following predicates can be true of any Scheme object: codice_62, codice_63, codice_64, codice_65, codice_66, codice_67, codice_68, codice_69, codice_70. (R5RS sec 3.2)

Within the numerical datatype, by contrast, the numerical values overlap. For example, an integer value satisfies all of the codice_71, codice_72, codice_73, codice_74 and codice_65 predicates at the same time. (R5RS sec 6.2)

Scheme has three different types of equivalence between arbitrary objects denoted by three different "equivalence predicates", relational operators for testing equality, codice_76, codice_77 and codice_78:

Type dependent equivalence operations also exist in Scheme: codice_86 and codice_87 compare two strings (the latter performs a case-independent comparison); codice_88 and codice_89 compare characters; codice_90 compares numbers.

Up to the R5RS standard, the standard comment in Scheme was a semicolon, which makes the rest of the line invisible to Scheme. Numerous implementations have supported alternative conventions permitting comments to extend for more than a single line, and the R6RS standard permits two of them: an entire s-expression may be turned into a comment (or "commented out") by preceding it with codice_91 (introduced in SRFI 62) and a multiline comment or "block comment" may be produced by surrounding text with codice_92 and codice_93.

Scheme's input and output is based on the "port" datatype. (R5RS sec 6.6) R5RS defines two default ports, accessible with the procedures codice_94 and codice_95, which correspond to the Unix notions of standard input and standard output. Most implementations also provide codice_96. Redirection of input and standard output is supported in the standard, by standard procedures such as codice_97 and codice_98. Most implementations provide string ports with similar redirection capabilities, enabling many normal input-output operations to be performed on string buffers instead of files, using procedures described in SRFI 6. The R6RS standard specifies much more sophisticated and capable port procedures and many new types of port.

The following examples are written in strict R5RS Scheme.

Example 1: With output defaulting to (current-output-port):

Example 2: As 1, but using optional port argument to output procedures

Example 3: As 1, but output is redirected to a newly created file

(let ((hello0 (lambda () (display "Hello world") (newline))))
Example 4: As 2, but with explicit file open and port close to send output to file

Example 5: As 2, but with using call-with-output-file to send output to a file.

Similar procedures are provided for input. R5RS Scheme provides the predicates codice_99 and codice_100. For character input and output, codice_101, codice_102, codice_103 and codice_104 are provided. For writing and reading Scheme expressions, Scheme provides codice_105 and codice_106. On a read operation, the result returned is the end-of-file object if the input port has reached the end of the file, and this can be tested using the predicate codice_107.

In addition to the standard, SRFI 28 defines a basic formatting procedure resembling Common Lisp's codice_108 function, after which it is named.

In Scheme, procedures are bound to variables. At R5RS the language standard formally mandated that programs may change the variable bindings of built-in procedures, effectively redefining them. (R5RS "Language changes") For example, one may extend codice_57 to accept strings as well as numbers by redefining it:
(set! +
===> 6
===> "123"
In R6RS every binding, including the standard ones, belongs to some library, and all exported bindings are immutable. (R6RS sec 7.1) Because of this, redefinition of standard procedures by mutation is forbidden. Instead, it is possible to import a different procedure under the name of a standard one, which in effect is similar to redefinition.

In Standard Scheme, procedures that convert from one datatype to another contain the character string "->" in their name, predicates end with a "?", and procedures that change the value of already-allocated data end with a "!". These conventions are often followed by Scheme programmers.

In formal contexts such as Scheme standards, the word "procedure" is used in preference to "function" to refer to a lambda expression or primitive procedure. In normal usage, the words "procedure" and "function" are used interchangeably. Procedure application is sometimes referred to formally as "combination".

As in other Lisps, the term "thunk" is used in Scheme to refer to a procedure with no arguments. The term "proper tail recursion" refers to the property of all Scheme implementations, that they perform tail-call optimization so as to support an indefinite number of active tail calls.

The form of the titles of the standards documents since R3RS, "Revised Report on the Algorithmic Language Scheme", is a reference to the title of the ALGOL 60 standard document, "Revised Report on the Algorithmic Language Algol 60," The Summary page of R3RS is closely modeled on the Summary page of the ALGOL 60 Report.

The language is formally defined in the standards R5RS (1998) and R6RS (2007). They describe standard "forms": keywords and accompanying syntax, which provide the control structure of the language, and standard procedures which perform common tasks.

This table describes the standard forms in Scheme. Some forms appear in more than one row because they cannot easily be classified into a single function in the language.

Forms marked "L" in this table are classed as derived "library" forms in the standard and are often implemented as macros using more fundamental forms in practice, making the task of implementation much easier than in other languages.

Note that codice_110 is defined as a library syntax in R5RS, but the expander needs to know about it to achieve the splicing functionality. In R6RS it is no longer a library syntax.

The following two tables describe the standard procedures in R5RS Scheme. R6RS is far more extensive and a summary of this type would not be practical.

Some procedures appear in more than one row because they cannot easily be classified into a single function in the language.
String and character procedures that contain "-ci" in their names perform case-independent comparisons between their arguments: upper case and lower case versions of the same character are taken to be equal.

Implementations of - and / that take more than two arguments are defined but left optional at R5RS.

Because of Scheme's minimalism, many common procedures and syntactic forms are not defined by the standard. In order to keep the core language small but facilitate standardization of extensions, the Scheme community has a "Scheme Request for Implementation" (SRFI) process by which extension libraries are defined through careful discussion of extension proposals. This promotes code portability. Many of the SRFIs are supported by all or most Scheme implementations.

SRFIs with fairly wide support in different implementations include:


The elegant, minimalist design has made Scheme a popular target for language designers, hobbyists, and educators, and because of its small size, that of a typical interpreter, it is also a popular choice for embedded systems and scripting. This has resulted in scores of implementations, most of which differ from each other so much that porting programs from one implementation to another is quite difficult, and the small size of the standard language means that writing a useful program of any great complexity in standard, portable Scheme is almost impossible. The R6RS standard specifies a much broader language, in an attempt to broaden its appeal to programmers.

Almost all implementations provide a traditional Lisp-style read–eval–print loop for development and debugging. Many also compile Scheme programs to executable binary. Support for embedding Scheme code in programs written in other languages is also common, as the relative simplicity of Scheme implementations makes it a popular choice for adding scripting capabilities to larger systems developed in languages such as C. The Gambit, Chicken, and Bigloo Scheme interpreters compile Scheme to C, which makes embedding particularly easy. In addition, Bigloo's compiler can be configured to generate JVM bytecode, and it also features an experimental bytecode generator for .NET.

Some implementations support additional features. For example, Kawa and JScheme provide integration with Java classes, and the Scheme to C compilers often make it easy to use external libraries written in C, up to allowing the embedding of actual C code in the Scheme source. Another example is Pvts, which offers a set of visual tools for supporting the learning of Scheme.

Scheme is widely used by a number of schools; in particular, a number of introductory Computer Science courses use Scheme in conjunction with the textbook "Structure and Interpretation of Computer Programs" (SICP). For the past 12 years, PLT has run the ProgramByDesign (formerly TeachScheme!) project, which has exposed close to 600 high school teachers and thousands of high school students to rudimentary Scheme programming. MIT's old introductory programming class 6.001 was taught in Scheme, Although 6.001 has been replaced by more modern courses, SICP continues to be taught at MIT. Likewise, the introductory class at UC Berkeley, CS 61A, was until 2011 taught entirely in Scheme, save minor diversions into Logo to demonstrate dynamic scope. Today, like MIT, Berkeley has replaced the syllabus with a more modern version that is primarily taught in Python 3, but the current syllabus is still based on the old curriculum, and parts of the class are still taught in Scheme.

The textbook "How to Design Programs" by Matthias Felleisen, currently at Northeastern University, is used by some institutes of higher education for their introductory computer science courses. Both Northeastern University and Worcester Polytechnic Institute use Scheme exclusively for their introductory courses Fundamentals of Computer Science (CS2500) and Introduction to Program Design (CS1101), respectively. Rose-Hulman uses Scheme in its more advanced Programming Language Concepts course. Indiana University's introductory class, C211, is taught entirely in Scheme. A self-paced version of the course, CS 61AS, continues to use Scheme. The introductory computer science courses at Yale and Grinnell College are also taught in Scheme. Programming Design Paradigms, a mandatory course for the Computer science Graduate Students at Northeastern University, also extensively uses Scheme.
The former introductory Computer Science course at the University of Minnesota - Twin Cities, CSCI 1901, also used Scheme as its primary language, followed by a course that introduced students to the Java programming language; however, following the example of MIT, the department replaced 1901 with the Python-based CSCI 1133, while functional programming is covered in detail in the third-semester course CSCI 2041. In the software industry, Tata Consultancy Services, Asia's largest software consultancy firm, uses Scheme in their month-long training program for fresh college graduates.

Scheme is/was also used for the following:






</doc>
<doc id="28122" url="https://en.wikipedia.org/wiki?curid=28122" title="Society for Psychical Research">
Society for Psychical Research

The Society for Psychical Research (SPR) is a nonprofit organisation in the United Kingdom. Its stated purpose is to understand events and abilities commonly described as psychic or paranormal. It describes itself as the "first society to conduct organised scholarly research into human experiences that challenge contemporary scientific models." It does not, however, since its inception in 1882, hold any corporate opinions: SPR members assert a variety of beliefs with regard to the nature of the phenomena studied.

The Society for Psychical Research (SPR) originated from a discussion between journalist Edmund Rogers and the physicist William F. Barrett in autumn 1881. This led to a conference on 5 and 6 January 1882 at the headquarters of the British National Association of Spiritualists which the foundation of the Society was proposed. The committee included Barrett, Rogers, Stainton Moses, Charles Massey, Edmund Gurney, Hensleigh Wedgwood and Frederic W. H. Myers. The SPR was formally constituted on 20 February 1882 with philosopher Henry Sidgwick as its first president.

The SPR was the first organisation of its kind in the world, its stated purpose being "to approach these varied problems without prejudice or prepossession of any kind, and in the same spirit of exact and unimpassioned enquiry which has enabled science to solve so many problems, once not less obscure nor less hotly debated."

Other early members included the author Jane Barlow, the renowned chemist Sir William Crookes, physicist Sir Oliver Lodge, Nobel laureate Charles Richet and psychologist William James.

Members of the SPR initiated and organised the International Congresses of Physiological/Experimental psychology.

Areas of study included hypnotism, dissociation, thought-transference, mediumship, Reichenbach phenomena, apparitions and haunted houses and the physical phenomena associated with séances. The SPR were to introduce a number of neologisms which have entered the English language, such as 'telepathy', which was coined by Frederic Myers.

The Society is run by a President and a Council of twenty members, and is open to interested members of the public to join. The organisation is based at 1 Vernon Mews, London, with a library and office open to members, and with large book and archival holdings in Cambridge University Library, Cambridgeshire, England. It publishes the peer reviewed quarterly "Journal of the Society for Psychical Research" ("JSPR"), the irregular "Proceedings" and the magazine "Paranormal Review". It holds an annual conference, regular lectures and two study days per year and supports the "LEXSCIEN" on-line library project.

Among the first important works was the two-volume publication in 1886, "Phantasms of the Living", concerning telepathy and apparitions, co-authored by Gurney, Myers and Frank Podmore. This text, and subsequent research in this area, was received negatively by the scientific mainstream, though Gurney and Podmore provided a defense of the society's early work in this area in mainstream publications.

The SPR "devised methodological innovations such as randomized study designs" and conducted "the first experiments investigating the psychology of eyewitness testimony (Hodgson and Davey, 1887), [and] empirical and conceptual studies illuminating mechanisms of dissociation and hypnotism"

In 1894, the "Census of Hallucinations" was published which sampled 17,000 people. Out of these, 1, 684 persons reported having experienced a hallucination of an apparition. Such efforts were claimed to have undermined "the notion of dissociation and hallucinations as intrinsically pathological phenomena"

The SPR investigated many spiritualist mediums such as Eva Carrière and Eusapia Palladino.

During the early twentieth century, the SPR studied a series of automatic scripts and trance utterances from a group of automatic writers, known as the cross-correspondences.

Famous cases investigated by the Society include Borley Rectory and the Enfield Poltergeist.

In 1912 the Society extended a request for a contribution to a special medical edition of its Proceedings to Sigmund Freud. Though according to Ronald W. Clark (1980) "Freud surmised, no doubt correctly, that the existence of any link between the founding fathers of psychoanalysis and investigation of the paranormal would hamper acceptance of psychoanalysis" as would any perceived involvement with the occult. Nonetheless, Freud did respond, contributing an essay titled "A Note on the Unconscious in Psycho-Analysis" to the Medical Supplement to the Proceedings of the Society for Psychical Research.

Much of the early work involved investigating, exposing and in some cases duplicating fake phenomena. In the late 19th century, SPR investigations into séance phenomena led to the exposure of many fraudulent mediums.

Richard Hodgson distinguished himself in that area. In 1884, Hodgson was sent by the SPR to India to investigate Helena Blavatsky and concluded that her claims of psychic power were fraudulent. However these findings were much later (April 1986) reviewed and retracted by the SPR.

In 1886 and 1887 a series of publications by S. J. Davey, Hodgson and Sidgwick in the SPR journal exposed the slate writing tricks of the medium William Eglinton. Hodgson with his friend, S. J. Davey, had staged fake séances for educating the public (including SPR members). Davey gave sittings under an assumed name, duplicating the phenomena produced by Eglinton, and then proceeded to point out to the sitters the manner in which they had been deceived. Because of this, some spiritualist members such as Stainton Moses resigned from the SPR.

In 1891, Alfred Russel Wallace requested for the Society to properly investigate spirit photography. Eleanor Sidgwick responded with a critical paper in the SPR which cast doubt on the subject and discussed the fraudulent methods that spirit photographers such as Édouard Isidore Buguet, Frederic Hudson and William H. Mumler had utilised.

Due to the exposure of William Hope and other fraudulent mediums, Arthur Conan Doyle led a mass resignation of eighty-four members of the Society for Psychical Research, as they believed the Society was opposed to spiritualism. Science historian William Hodson Brock has noted that "By the 1900s most avowed spiritualists had left the SPR and gone back to the BNAS (the London Spiritualist Alliance since 1884), having become upset by the sceptical tone of most of the SPR's investigations."

The Society has been criticised by both spiritualists and sceptics.

Prominent spiritualists at first welcomed the SPR and cooperated fully. But relations soured when spiritualists discovered that the SPR would not accept outside testimony as proof, and the society accused some prominent mediums of fraud. Spiritualist Arthur Conan Doyle resigned from the SPR in 1930, to protest what he regarded as the SPR's overly restrictive standards of proof. Psychic investigator and believer in spiritualism Nandor Fodor criticised the SPR for its "strong bias" against physical manifestations of spiritualism.

Sceptics have criticised members of the SPR for having motives liable to impair scientific objectivity. According to SPR critics John Grant and Eric Dingwall (a member of the SPR), early SPR members such as Henry Sidgwick, Frederic W. H. Myers, and William Barrett hoped to cling to something spiritual through psychical research. Myers stated that "[T]he Society for Psychical Research was founded, with the establishment of thought-transference—already rising within measurable distance of proof—as its primary aim." Defenders of the SPR have stated in reply that "a 'will to believe' in post-mortem survival, telepathy and other scientifically unpopular notions, does not necessarily exclude a "will to know" and thus the capacity for thorough self-criticism, methodological rigour and relentless suspicion of errors."

The sceptic and physicist Victor J. Stenger has written:

Ivor Lloyd Tuckett an author of an early sceptical work on psychical research wrote that although the SPR have collected some valuable work, most of its active members have "no training in psychology fitting them for their task, and have been the victims of pronounced bias, as sometimes they themselves have admitted." Trevor H. Hall, an ex-member of the Society for Psychical Research, criticised SPR members as "credulous and obsessive wish... to believe." Hall also claimed SPR members "lack knowledge of deceptive methods."

Writer Edward Clodd asserted that the SPR members William F. Barrett and Oliver Lodge had insufficient competence for the detection of fraud and suggested that their spiritualist beliefs were based on magical thinking and primitive superstition. Clodd described the SPR as offering "barbaric spiritual philosophy", and characterised the language of SPR members as using such terms as "subliminal consciousness" and "telepathic energy," as a disguise for "bastard supernaturalism."

A 2004 psychological study involving 174 members of the Society for Psychical Research completed a delusional ideation questionnaire and a deductive reasoning task. As predicted, the study showed that "individuals who reported a strong belief in the paranormal made more errors and displayed more delusional ideation than sceptical individuals". There was also a reasoning bias which was limited to people who reported a belief in, rather than experience of, paranormal phenomena. The results suggested that reasoning abnormalities may have a causal role in the formation of paranormal belief.

Some sceptical members have resigned from the SPR. Eric Dingwall resigned and wrote " After sixty years' experience and personal acquaintance with most of the leading parapsychologists of that period I do not think I could name half a dozen whom I could call objective students who honestly wished to discover the truth. The great majority wanted to prove something or other: They wanted the phenomena into which they were inquiring to serve some purpose in supporting preconceived theories of their own."

The following is a list of presidents:

The Society publishes "Proceedings of the Society for Psychical Research", the "Journal of the Society for Psychical Research", and the "Paranormal Review", as well as the online "Psi Encyclopedia".

First published in 1882 as a public record of the activities of the SPR, the "Proceedings" are now reserved for longer pieces of work, such as Presidential Addresses, and are only occasionally published. The current editor is Dr David Vernon.

The "Journal of the Society for Psychical Research" has been published quarterly since 1884. It was introduced as a private, members-only periodical to supplement the "Proceedings". It now focuses on current laboratory and field research, but also includes theoretical, methodological and historical papers on parapsychology. It also publishes book reviews and correspondence. The current editor is Dr David Vernon.

The "Paranormal Review" is the magazine of the Society for Psychical Research. Formerly known as the "Psi Researcher", it has been published since 1996. Previous editors have included Dr Nicola J. Holt. The current editor is Dr Leo Ruickbie.

The "Psi Encyclopedia" is a collection of articles and case studies about psi research, involving the scientific investigation of psychic phenomena. A bequest of Nigel Buckmaster enabled the foundation of the encyclopedia.

A number of other psychical research organisations use the term 'Society for Psychical Research' in their name.



SPR histories


Scholarly studies


Criticism




</doc>
<doc id="28123" url="https://en.wikipedia.org/wiki?curid=28123" title="Sniper">
Sniper

A sniper is a military/paramilitary marksman who engages targets from positions of concealment or at distances exceeding the target's detection capabilities. Snipers generally have specialized training and are equipped with high-precision rifles and high-magnification optics, and often also serve as scouts/observers feeding tactical information back to their units or command headquarters.

In addition to long-range and high-grade marksmanship, military snipers are trained in a variety of special operation techniques: detection, stalking, target range estimation methods, camouflage, field craft, infiltration, special reconnaissance and observation, surveillance and target acquisition.

The verb "to snipe" originated in the 1770s among soldiers in British India in reference to shooting snipes, which was considered an extremely challenging game bird for hunters due to its alertness, camouflaging color and erratic flight behavior. Snipe hunters therefore needed to be stealthy in addition to being good trackers and marksmen. The agent noun "sniper" appears by the 1820s.

The term "sniper" was first attested militarily in 1824 in the sense of the somewhat older term "sharpshooter", an 18th-century calque of German "Scharfschütze", in use in British newspapers as early as 1801.

Different countries use different military doctrines regarding snipers in military units, settings, and tactics.

Generally, a sniper's primary function in modern warfare is to provide detailed surveillance from a concealed position and, if necessary, to reduce the enemy's combat ability by neutralizing high-value targets (especially officers and other key personnel) and in the process pinning down and demoralizing the enemy. Typical sniper missions include managing intelligence information they gather during reconnaissance, target acquisition and impact feedback for air strikes and artillery, assisting employed combat force with accurate fire support and counter-sniper tactics, killing enemy commanders, selecting targets of opportunity, and even destruction of military equipment, which tend to require use of anti-materiel rifles in the larger calibers such as the .50 BMG, like the Barrett M82, McMillan Tac-50, and Denel NTW-20.

Soviet- and Russian-derived military doctrines include squad-level snipers. Snipers have increasingly been demonstrated as useful by US and UK forces in the recent Iraq campaign in a fire support role to cover the movement of infantry, especially in urban areas.

Military snipers from the US, UK and other countries that adopt their military doctrine are typically deployed in two-man sniper teams consisting of a shooter and a spotter. A common practice is for a shooter and a spotter to take turns in order to avoid eye fatigue. In most recent combat operations occurring in large densely populated towns, such as Fallujah, Iraq, two teams would be deployed together to increase their security and effectiveness in an urban environment. A sniper team would be armed with a long-range weapon and a rapid-firing shorter-ranged weapon in case of close quarter combat.

The German doctrine of largely independent snipers and emphasis on concealment, developed during the Second World War, has been most influential on modern sniper tactics, and is currently used throughout Western militaries (examples are specialized camouflage clothing, concealment in terrain and emphasis on coup d'œil).

Sniper rifles are classified as crew-served, as the term is used in the United States military. A sniper team (or sniper cell) consists of a combination of one or more "shooters" with force protection elements and support personnel: such as a "spotter" or a "flanker". Within the Table of Organization and Equipment for both the United States Army and the U.S. Marine Corps, the operator of the weapon has an assistant trained to fulfill multiple roles, in addition to being sniper-qualified in the operation of the weapon.

The shooter fires the shot while the spotter assists in observation of targets, atmospheric conditions and handles ancillary tasks as immediate security of their location, communication with other parties; including directing artillery fire and close air support. A flanker's task is to observe areas not immediately visible to the sniper or spotter and assist with the team's perimeter and rear security, therefore flankers are usually armed with an assault rifle or battle rifle. Both spotter and flanker carry additional ammunition and associated equipment.

The spotter detects, observes, and assigns targets and watches for the results of the shot. Using a spotting scope or a rangefinder, the spotter will also read the wind by using physical indicators and the mirage caused by the heat on the ground. Also, in conjunction with the shooter, the spotter will make calculations for distance, angle shooting (slant range), mil dot related calculations, correction for atmospheric conditions and leads for moving targets. It is not unusual for the spotter to be equipped with a notepad and a laptop computer specifically for performing these calculations.

Law enforcement snipers, commonly called police snipers, and military snipers differ in many ways, including their areas of operation and tactics. A police sharpshooter is part of a police operation and usually takes part in relatively short missions. Police forces typically deploy such sharpshooters in hostage scenarios. This differs from a military sniper, who operates as part of a larger army, engaged in warfare. Sometimes as part of a SWAT team, police snipers are deployed alongside negotiators and an assault team trained for close quarters combat. As policemen, they are trained to shoot only as a last resort, when there is a direct threat to life; the police sharpshooter has a well-known rule: "Be prepared to take a life to save a life." Police snipers typically operate at much shorter ranges than military snipers, generally under and sometimes even less than . Both types of snipers do make difficult shots under pressure, and often perform one-shot kills.
Police units that are unequipped for tactical operations may rely on a specialized SWAT team, which may have a dedicated sniper. Some police sniper operations begin with military assistance. Police snipers placed in vantage points, such as high buildings, can provide security for events. In one high-profile incident commonly referred to as "The Shot Seen Around the World" due to going viral online, Mike Plumb, a SWAT sniper in Columbus, Ohio, prevented a suicide by shooting a revolver out of the individual's hand, leaving him unharmed.
The need for specialized training for police sharpshooters was made apparent in 1972 during the Munich massacre when the German police could not deploy specialized personnel or equipment during the standoff at the airport in the closing phase of the crisis, and consequently all of the Israeli hostages were killed. While the German army did have snipers in 1972, the use of army snipers in the scenario was impossible due to the German constitution's explicit prohibition of the use of the military in domestic matters. This lack of trained snipers who could be used in civilian roles was later addressed with the founding of the specialized police counter-terrorist unit GSG 9.

The longest confirmed sniper kill in combat was achieved by an undisclosed member of the Canadian JTF2 special forces in June 2017 at a distance of .

The previous record holder was Craig Harrison, a Corporal of Horse (CoH) in the Blues and Royals RHG/D of the British Army. In November 2009, Harrison struck two Taliban machine gunners consecutively south of Musa Qala in Helmand Province in Afghanistan at a range of or 1.54 miles using a L115A3 Long Range Rifle.
The QTU Lapua external ballistics software, using continuous doppler drag coefficient (C) data provided by Lapua, predicts that such shots traveling would likely have struck their targets after nearly 6.0 seconds of flight time, having lost 93% of their kinetic energy, retaining of their original velocity, and having dropped or 2.8° from the original bore line. Due to the extreme distances and travel time involved, even a light cross-breeze of would have diverted such shots off target, which would have required compensation.

The calculation assumes a "flat-fire scenario" (a situation where the shooting and target positions are at equal elevation), utilizing British military custom high-pressure .338 Lapua Magnum cartridges, loaded with 16.2 g (250 gr) Lapua LockBase B408 bullets, fired at 936 m/s (3,071 ft/s) muzzle velocity under the following on-site (average) atmospheric conditions: barometric pressure: at sea-level equivalent or on-site, humidity: 25.9%, and temperature: in the region for November 2009, resulting in an air density ρ = 1.0854 kg/m at the elevation of Musa Qala. Harrison mentions in reports that the environmental conditions were perfect for long range shooting, "... no wind, mild weather, clear visibility." In a BBC interview, Harrison reported it took about nine shots for him and his spotter to initially range the target successfully.

Before the development of rifling, firearms were smoothbore and inaccurate over long distance. Barrel rifling was invented at the end of the fifteenth century, but was only employed in large cannons. Over time, rifling, along with other gunnery advances, has increased the performance of modern firearms.

Early forms of sniping or marksmanship were used during the American Revolutionary War. For instance, in 1777 at the battle of Saratoga the Colonists hid in the trees and used early model rifles to shoot British officers. Most notably, Timothy Murphy shot and killed General Simon Fraser of Balnain on 7 October 1777 at a distance of about 400 yards. During the Battle of Brandywine, Capt. Patrick Ferguson had a tall, distinguished American officer in his rifle's iron sights. Ferguson did not take the shot, as the officer had his back to Ferguson; only later did Ferguson learn that George Washington had been on the battlefield that day.

A special unit of marksmen was established during the Napoleonic Wars in the British Army. While most troops at that time used inaccurate smoothbore muskets, the British "Green Jackets" (named for their distinctive green uniforms) used the famous Baker rifle. Through the combination of a leather wad and tight grooves on the inside of the barrel (rifling), this weapon was far more accurate, though slower to load. These Riflemen were the elite of the British Army, and served at the forefront of any engagement, most often in skirmish formation, scouting out and delaying the enemy. Another term, "sharp shooter" was in use in British newspapers as early as 1801. In the "Edinburgh Advertiser", 23 June 1801, can be found the following quote in a piece about the North British Militia; "This Regiment has several Field Pieces, and two companies of Sharp Shooters, which are very necessary in the modern Stile of War". The term appears even earlier, around 1781, in Continental Europe, translated from the German Scharfschütze.

The Whitworth rifle was arguably the first long-range sniper rifle in the world. A muzzleloader designed by Sir Joseph Whitworth, a prominent British engineer, it used polygonal rifling instead, which meant that the projectile did not have to bite into grooves as was done with conventional rifling. The Whitworth rifle was far more accurate than the Pattern 1853 Enfield, which had shown some weaknesses during the recent Crimean War. At trials in 1857 which tested the accuracy and range of both weapons, Whitworth's design outperformed the Enfield at a rate of about three to one. The Whitworth rifle was capable of hitting the target at a range of 2,000 yards, whereas the Enfield could only manage it at 1,400 yards.

During the Crimean War, the first optical sights were designed to fit onto rifles. Much of this pioneering work was the brainchild of Colonel D. Davidson, using optical sights produced by Chance Brothers of Birmingham. This allowed a marksman to observe and target objects more accurately at a greater distance than ever before. The telescopic sight, or scope, was originally fixed and could not be adjusted, which therefore limited its range.

Despite its success at the trials, the rifle was not adopted by the British Army. However, the Whitworth Rifle Company was able to sell the weapon to the French army, and also to the Confederacy during the American Civil War, where both the Union and Confederate armies employed sharpshooters. The most notable incident was during the Battle of Spotsylvania Court House, where on 9 May 1864, Union General John Sedgwick was killed by a Confederate Whitworth sharpshooter at a range of about after saying the enemy "couldn't hit an elephant at this distance".

During the Boer War the latest breech-loading rifled guns with magazines and smokeless powder were used by both sides. The British were equipped with the Lee–Metford rifle, while the Boers had received the latest Mauser rifles from Germany. In the open terrain of South Africa the marksmen were a crucial component to the outcome of the battle.

The first British sniper unit began life as the Lovat Scouts, a Scottish Highland regiment formed in 1899, that earned high praise during the Second Boer War (1899–1902). The unit was formed by Lord Lovat and reported to an American, Major Frederick Russell Burnham, the British Army Chief of Scouts under Lord Roberts. Burnham fittingly described these scouts as "half wolf and half jackrabbit.". Just like their Boer scout opponents, these scouts were well practised in the arts of marksmanship, field craft, map reading, observation, and military tactics. They were skilled woodsmen and practitioners of discretion: "He who shoots and runs away, lives to shoot another day." They were also the first known military unit to wear a ghillie suit. 
Hesketh Hesketh-Prichard said of them that "keener men never lived", and that "Burnham was the greatest scout of our time." Burnham distinguished himself in wars in South Africa, Rhodesia, and in Arizona fighting the Apaches, and his definitive work, "Scouting on Two Continents," provides a dramatic and enlightening picture of what a sniper was at the time and how he operated.

After the war, this regiment went on to formally become the first official sniper unit, then better known as "sharpshooters".

During World War I, snipers appeared as deadly sharpshooters in the trenches. At the start of the war, only Imperial Germany had troops that were issued scoped sniper rifles. Although sharpshooters existed on all sides, the Germans specially equipped some of their soldiers with scoped rifles that could pick off enemy soldiers showing their heads out of their trench. At first the French and British believed such hits to be coincidental hits, until the German scoped rifles were discovered. During World War I, the German army received a reputation for the deadliness and efficiency of its snipers, partly because of the high-quality lenses that German industry could manufacture.

During the First World War, the static movement of trench warfare and a need for protection from snipers created a requirement for loopholes both for discharging firearms and for observation. Often a steel plate was used with a "key hole", which had a rotating piece to cover the loophole when not in use.

Soon the British army began to train their own snipers in specialized sniper schools. Major Hesketh Hesketh-Prichard was given formal permission to begin sniper training in 1915, and founded the First Army School of Sniping, Observation, and Scouting at Linghem in France in 1916. Starting with a first class of only six, in time he was able to lecture to large numbers of soldiers from different Allied nations, proudly proclaiming in a letter that his school was turning out snipers at three times the rate of any such other school in the world.

He also devised a metal-armoured double loophole that would protect the sniper observer from enemy fire. The front loophole was fixed, but the rear was housed in a metal shutter sliding in grooves. Only when the two loopholes were lined up—a one-to-twenty chance—could an enemy shoot between them. Another innovation was the use of a dummy head to find the location of an enemy sniper. The papier-mâché figures were painted to resemble soldiers to draw sniper fire. Some were equipped with rubber surgical tubing so the dummy could "smoke" a cigarette and thus appear realistic. Holes punched in the dummy by enemy sniper bullets then could be used for triangulation purposes to determine the position of the enemy sniper, who could then be attacked with artillery fire. He developed many of the modern techniques in sniping, including the use of spotting scopes and working in pairs, and using Kim's Game to train observational skills.

In 1920, he wrote his account of his war time activities in his book "", to which reference is still made by modern authors regarding the subject.

The main sniper rifles used during the First World War were the German Mauser Gewehr 98; the British Pattern 1914 Enfield and Lee–Enfield SMLE Mk III, the Canadian Ross rifle, the American M1903 Springfield, the Italian M1891 Carcano, and the Russian M1891 Mosin–Nagant

During the interbellum, most nations dropped their specialized sniper units, notably the Germans. Effectiveness and dangers of snipers once again came to the fore during the Spanish Civil War. The only nation that had specially trained sniper units during the 1930s was the Soviet Union. Soviet snipers were trained in their skills as marksmen, in using the terrain to hide themselves from the enemy and the ability to work alongside regular forces. This made the Soviet sniper training focus more on "normal" combat situations than those of other nations.

Snipers reappeared as important factors on the battlefield from the first campaign of World War II. During Germany's 1940 campaigns, lone, well-hidden French and British snipers were able to halt the German advance for a considerable amount of time. For example, during the pursuit to Dunkirk, British snipers were able to significantly delay the German infantry's advance. This prompted the British once again to increase training of specialized sniper units. Apart from marksmanship, British snipers were trained to blend in with the environment, often by using special camouflage clothing for concealment. However, because the British Army offered sniper training exclusively to officers and non-commissioned officers, the resulting small number of trained snipers in combat units considerably reduced their overall effectiveness.

During the Winter War, Finnish snipers took a heavy toll of the invading Soviet army. Simo Häyhä is credited with 505 confirmed kills, most with the Finnish version of the iron-sighted bolt-action Mosin–Nagant.

One of the best known battles involving snipers, and the battle that made the Germans reinstate their specialized sniper training, was the Battle of Stalingrad. Their defensive position inside a city filled with rubble meant that Soviet snipers were able to inflict significant casualties on the Wehrmacht troops. Because of the nature of fighting in city rubble, snipers were very hard to spot and seriously dented the morale of the German attackers. The best known of these snipers was probably Vasily Zaytsev, featured in the novel "War of the Rats" and the subsequent film "Enemy At The Gates".

German "Scharfschützen" were prepared before the war, equipped with Karabiner 98 and later Gewehr 43 rifles, but there were often not enough of these weapons available, and as such some were armed with captured scoped Mosin–Nagant 1891/30, SVT or Czech Mauser rifles. The Wehrmacht re-established its sniper training in 1942, drastically increasing the number of snipers per unit with the creation of an additional 31 sniper training companies by 1944. German snipers were at the time the only snipers in the world issued with purpose-manufactured sniping ammunition, known as the 'effect-firing' sS round. The 'effect-firing' sS round featured an extra carefully measured propellant charge and seated a heavy 12.8 gram (198 gr) full-metal-jacketed boat-tail projectile of match-grade build quality, lacking usual features such as a seating ring to improve the already high ballistic coefficient of .584 (G1) further. For aiming optics German snipers used the Zeiss Zielvier 4x (ZF39) telescopic sight which had bullet drop compensation in 50 m increments for ranges from 100 m up to 800 m or in some variations from 100 m up to 1000 m or 1200 m. There were ZF42, Zielfernrohr 43 (ZF 4), Zeiss Zielsechs 6x, Zeiss Zielacht 8x and other telescopic sights by various manufacturers like the Ajack 4x, Hensoldt Dialytan 4x and Kahles Heliavier 4x with similar features employed on German sniper rifles. Several different mountings produced by various manufacturers were used for mounting aiming optics to the rifles. In February 1945 the Zielgerät 1229 active infrared aiming device was issued for night sniping with the StG 44 assault rifle.

A total of 428,335 individuals received Red Army sniper training, including Soviet and non-Soviet partisans, with 9,534 receiving the sniping 'higher qualification'. During World War ІІ, two six-month training courses for women alone trained nearly 55,000 snipers, of which more than two thousand later served in the army. On average there was at least one sniper in an infantry platoon and one in every reconnaissance platoon, including in tank and even artillery units. Some used the PTRD anti-tank rifle with an adapted scope as an early example of an anti-materiel rifle.

In the United States Armed Forces, sniper training was only very elementary and was mainly concerned with being able to hit targets over long distances. Snipers were required to be able to hit a body over 400 meters away, and a head over 200 meters away. There was almost no instruction in blending into the environment. Sniper training varied from place to place, resulting in wide variation in the qualities of snipers. The main reason the US did not extend sniper training beyond long-range shooting was the limited deployment of US soldiers until the Normandy Invasion. During the campaigns in North Africa and Italy, most fighting occurred in arid and mountainous regions where the potential for concealment was limited, in contrast to Western and Central Europe.

The U.S. Army's lack of familiarity with sniping tactics proved disastrous in Normandy and the campaign in Western Europe where they encountered well trained German snipers. In Normandy, German snipers remained hidden in the dense vegetation and were able to encircle American units, firing at them from all sides. The American and British forces were surprised by how near the German snipers could approach in safety and attack them, as well as by their ability to hit targets at up to 1,000m. A notable mistake made by inexperienced American soldiers was to lie down and wait when targeted by German snipers, allowing the snipers to pick them off one after another. German snipers often infiltrated Allied lines and sometimes when the front-lines moved, they continued to fight from their sniping positions, refusing to surrender until their rations and munitions were exhausted.

Those tactics were also a consequence of changes in German enlistment. After several years of war and heavy losses on the Eastern Front, the German army was forced to rely more heavily on enlisting teenage soldiers. Due to lack of training in more complex group tactics, and thanks to rifle training provided by the Hitlerjugend, those soldiers were often used as autonomous left-behind snipers. While an experienced sniper would take a few lethal shots and retreat to a safer position, those young boys, due both to a disregard for their own safety and to lack of tactical experience would frequently remain in a concealed position and fight until they ran out of ammunition or were killed or wounded. While this tactic generally ended in the demise of the sniper, giving rise to the nickname "Suicide Boys" that was given to those soldiers, this irrational behavior proved quite disruptive to the Allied forces' progress. After World War II, many elements of German sniper training and doctrine were copied by other countries.

In the Pacific War, the Empire of Japan trained snipers. In the jungles of Asia and the Pacific Islands, snipers posed a serious threat to U.S., British, and Commonwealth troops. Japanese snipers were specially trained to use the environment to conceal themselves. Japanese snipers used foliage on their uniforms and dug well-concealed hide-outs that were often connected with small trenches. There was no need for long range accuracy because most combat in the jungle took place within a few hundred meters. Japanese snipers were known for their patience and ability to remain hidden for long periods. They almost never left their carefully camouflaged hiding spots. This meant that whenever a sniper was in the area, the location of the sniper could be determined after the sniper had fired a few shots. The Allies used their own snipers in the Pacific, notably the U.S. Marines, who used M1903 Springfield rifles.

Common sniper rifles used during the Second World War include: the Soviet M1891/30 Mosin–Nagant and, to a lesser extent, the SVT-40; the German Mauser Karabiner 98k and Gewehr 43; the British Lee–Enfield No. 4 and Pattern 1914 Enfield; the Japanese Arisaka 97; the American M1903A4 Springfield and M1C Garand. The Italians trained few snipers and supplied them with a scoped Carcano Model 1891.

Military sniper training aims to teach a high degree of proficiency in camouflage and concealment, stalking, observation and map reading as well as precision marksmanship under various operational conditions. Trainees typically shoot thousands of rounds over a number of weeks, while learning these core skills.

Snipers are trained to squeeze the trigger straight back with the ball of their finger, to avoid jerking the gun sideways. The most accurate position is prone, with a sandbag supporting the stock, and the stock's cheek-piece against the cheek. In the field, a bipod can be used instead. Sometimes a sling is wrapped around the weak arm (or both) to reduce stock movement. Some doctrines train a sniper to breathe deeply before shooting, then hold their lungs empty while they line up and take their shot. Some go further, teaching their snipers to shoot between heartbeats to minimize barrel motion.

The key to sniping is accuracy, which applies to both the weapon and the shooter. The weapon should be able to consistently place shots within tight tolerances. The sniper in turn must utilize the weapon to accurately place shots under varying conditions.

A sniper must have the ability to accurately estimate the various factors that influence a bullet's trajectory and point of impact such as: range to the target, wind direction, wind velocity, altitude and elevation of the sniper and the target and ambient temperature. Mistakes in estimation compound over distance and can decrease lethality or cause a shot to miss completely.

Snipers zero their weapons at a target range or in the field. This is the process of adjusting the scope so that the bullets' points-of-impact is at the point-of-aim (centre of scope or scope's cross-hairs) for a specific distance. A rifle and scope should retain its zero as long as possible under all conditions to reduce the need to re-zero during missions.

A sandbag can serve as a useful platform for shooting a sniper rifle, although any soft surface such as a rucksack will steady a rifle and contribute to consistency. In particular, bipods help when firing from a prone position, and enable the firing position to be sustained for an extended period of time. Many police and military sniper rifles come equipped with an adjustable bipod. Makeshift bipods known as shooting sticks can be constructed from items such as tree branches or ski poles. Some military snipers use three-legged shooting sticks.

Range and accuracy vary depending on the cartridge and specific ammunition types that are used. Typical ranges for common battle field cartridges are as follows:

Servicemen volunteer for the rigorous sniper training and are accepted on the basis of their aptitude, physical ability, marksmanship, patience and mental stability. Military snipers may be further trained as forward air controllers (FACs) to direct air strikes or forward observers (FOs) to direct artillery or mortar fire.

From 2011, the Russian armed forces has run newly developed sniper courses in military district training centres. In place of the Soviet practice of mainly squad sharpshooters, which were often designated during initial training (and of whom only few become snipers "per se"), "new" Army snipers are to be trained intensively for 3 months (for conscripts) or longer (for contract soldiers). The training program includes theory and practice of countersniper engagements, artillery spotting and coordination of air support. The first instructors are the graduates of the Solnechnogorsk sniper training centre.

The method of sniper deployment, according to the Ministry of Defence, is likely to be one three-platoon company at the brigade level, with one of the platoons acting independently and the other two supporting the battalions as needed.

The range to the target is measured or estimated as precisely as conditions permit and correct range estimation becomes absolutely critical at long ranges, because a bullet travels with a curved trajectory and the sniper must compensate for this by aiming higher at longer distances. If the exact distance is not known the sniper may compensate incorrectly and the bullet path may be too high or low. As an example, for a typical military sniping cartridge such as 7.62×51mm NATO (.308 Winchester) M118 Special Ball round this difference (or “drop”) from is . This means that if the sniper incorrectly estimated the distance as 700 meters when the target was in fact 800 meters away, the bullet will be 200 millimeters lower than expected by the time it reaches the target.

Laser rangefinders may be used, and range estimation is often the job of both parties in a team. One useful method of range finding without a laser rangefinder is comparing the height of the target (or nearby objects) to their size on the mil dot scope, or taking a known distance and using some sort of measure (utility poles, fence posts) to determine the additional distance. The average human head is in width, average human shoulders are apart and the average distance from a person's pelvis to the top of their head is .

To determine the range to a target without a laser rangefinder, the sniper may use the mil dot reticle on a scope to accurately find the range. Mil dots are used like a slide rule to measure the height of a target, and if the height is known, the range can be as well. The height of the target (in yards) ×1000, divided by the height of the target (in mils), gives the range in yards. This is only in general, however, as both scope magnification (7×, 40×) and mil dot spacing change. The USMC standard is that 1 mil (that is, 1 milliradian) equals 3.438 MOA (minute of arc, or, equivalently, minute of angle), while the US Army standard is 3.6 MOA, chosen so as to give a diameter of 1 yard at a distance of 1,000 yards (or equivalently, a diameter of 1 meter at a range of 1 kilometer.) Many commercial manufacturers use 3.5, splitting the difference, since it is easier to work with.

It is important to note that angular mil ("mil") is only an approximation of a milliradian and different organizations use different approximations.

At longer ranges, bullet drop plays a significant role in targeting. The effect can be estimated from a chart, which may be memorized or taped to the rifle, although some scopes come with Bullet Drop Compensator (BDC) systems that only require the range be dialed in. These are tuned to both a specific class of rifle and specific ammunition. Every bullet type and load will have different ballistics. .308 Federal 175 grain (11.3 g) BTHP match shoots at . Zeroed at , a 16.2 MOA adjustment would have to be made to hit a target at . If the same bullet was shot with 168 grain (10.9 g), a 17.1 MOA adjustment would be necessary.

Shooting uphill or downhill is confusing for many because gravity does not act perpendicular to the direction the bullet is traveling. Thus, gravity must be divided into its component vectors. Only the fraction of gravity equal to the cosine of the angle of fire with respect to the horizon affects the rate of fall of the bullet, with the remainder adding or subtracting negligible velocity to the bullet along its trajectory. To find the correct zero, the sniper multiplies the actual distance to the range by this fraction and aims as if the target were that distance away. For example, a sniper who observes a target 500 meters away at a 45-degree angle downhill would multiply the range by the cosine of 45 degrees, which is 0.707. The resulting distance will be 353 meters. This number is equal to the horizontal distance to the target. All other values, such as windage, time-to-target, impact velocity, and energy will be calculated based on the actual range of 500 meters. Recently, a small device known as a cosine indicator has been developed. This device is clamped to the tubular body of the telescopic sight, and gives an indicative readout in numerical form as the rifle is aimed up or down at the target. This is translated into a figure used to compute the horizontal range to the target.

Windage plays a significant role, with the effect increasing with wind speed or the distance of the shot. The slant of visible convections near the ground can be used to estimate crosswinds, and correct the point of aim. All adjustments for range, wind, and elevation can be performed by aiming off the target, called "holding over" or Kentucky windage. Alternatively, the scope can be adjusted so that the point of aim is changed to compensate for these factors, sometimes referred to as "dialing in". The shooter must remember to return the scope to zeroed position. Adjusting the scope allows for more accurate shots, because the cross-hairs can be aligned with the target more accurately, but the sniper must know exactly what differences the changes will have on the point-of-impact at each target range.

For moving targets, the point-of-aim is ahead of the target in the direction of movement. Known as "leading" the target, the amount of "lead" depends on the speed and angle of the target's movement as well as the distance to the target. For this technique, holding over is the preferred method. Anticipating the behavior of the target is necessary to accurately place the shot.

The term "hide site" refers to a covered and concealed position from which a sniper and his team can conduct surveillance or fire at targets. A good hide conceals and camouflages the sniper effectively, provides cover from enemy fire and allows a wide view of the surrounding area.

The main purpose of ghillie suits and hide sites is to break up the outline of a person with a rifle.

Many snipers use ghillie suits to hide and stay hidden. Ghillie suits vary according to the terrain into which the sniper wishes to blend. For example, in dry grassland the sniper will typically wear a ghillie suit covered in dead grass.

Shot placement, which is where on the body the sniper is aiming, varies with the type of sniper. Military snipers, who generally do not shoot at targets at less than , usually attempt body shots, aiming at the chest. These shots depend on tissue damage, organ trauma, and blood loss to kill the target. Body shots are used because the chest is a larger target.

Police snipers, who generally shoot at much shorter distances, may attempt a more precise shot at particular parts of body or particular devices: in one incident in 2007 in Marseille, a GIPN sniper took a shot from at the pistol of a police officer threatening to commit suicide, destroying the weapon and preventing the police officer from killing himself.

In a high-risk or hostage-taking situation where a suspect is imminently threatening to kill a hostage, police snipers may take head shots to ensure an instant kill. The snipers aim for the medulla oblongata to sever the spine from the brain. While this is believed to prevent the target from reflexively firing their weapon, there is evidence that any brain-hit is sufficient.

Snipers are trained for the detection, identification, and location of a targeted soldier in sufficient detail to permit the effective employment of lethal and non-lethal means. Since most kills in modern warfare are by crew-served weapons, reconnaissance is one of the most effective uses of snipers. They use their aerobic conditioning, infiltration skills and excellent long-distance observation equipment (optical scopes) and tactics to approach and observe the enemy. In this role, their rules of engagement typically let them shoot at high-value targets of opportunity, such as enemy officers.

The targets may be personnel or high-value materiel (military equipment and weapons) but most often they target the most important enemy personnel such as officers or specialists (e.g. communications operators) so as to cause maximum disruption to enemy operations. Other personnel they might target include those who pose an immediate threat to the sniper, like dog handlers, who are often employed in a search for snipers. A sniper identifies officers by their appearance and behavior such as symbols of rank, talking to radio operators, sitting as a passenger in a car, sitting in a car with a large radio antenna, having military servants, binoculars/map cases or talking and moving position more frequently. If possible, snipers shoot in descending order by rank, or if rank is unavailable, they shoot to disrupt communications.

Some rifles, such as the Denel NTW-20 and Vidhwansak, are designed for a purely anti-materiel (AM) role, e.g. shooting turbine disks of parked aircraft, missile guidance packages, expensive optics, and the bearings, tubes or wave guides of radar sets. A sniper equipped with the correct rifle can target radar dishes, water containers, the engines of vehicles, and any number of other targets. Other rifles, such as the .50 caliber rifles produced by Barrett and McMillan, are not designed exclusively as AM rifles, but are often employed in such a way, providing the range and power needed for AM applications in a lightweight package compared to most traditional AM rifles. Other calibers, such as the .408 Cheyenne Tactical and the .338 Lapua Magnum, are designed to be capable of limited AM application, but are ideally suited as long range anti-personnel rounds.

Often in situations with multiple targets, snipers use relocation. After firing a few shots from a certain position, snipers move unseen to another location before the enemy can determine where they are and mount a counter-attack. Snipers will frequently use this tactic to their advantage, creating an atmosphere of chaos and confusion. In other, rarer situations, relocation is used to eliminate the factor of wind.

As sniper rifles are often extremely powerful and consequently loud, it is common for snipers to use a technique known as sound masking. When employed by a highly skilled marksman, this tactic can be used as a substitute for a noise suppressor. Very loud sounds in the environment, such as artillery shells air bursting or claps of thunder, can often mask the sound of the shot. This technique is frequently used in clandestine operations, infiltration tactics, and guerrilla warfare.

Due to the surprise nature of sniper fire, high lethality of aimed shots and frustration at the inability to locate and counterattack snipers, sniper tactics have a significant negative effect on morale. Extensive use of sniper tactics can be used to induce constant stress and fear in opposing forces, making them afraid to move about or leave cover. In many ways, the psychological impact imposed by snipers is quite similar to those of landmines, booby-traps, and IEDs (constant threat, high "per event" lethality, inability to strike back).

Historically, captured snipers are often summarily executed. This happened during World War I and World War II; for example the second Biscari Massacre when 36 suspected snipers were lined up and shot on 14 July 1943.

As a result, if a sniper is in imminent danger of capture, he may discard any items (sniper rifle, laser rangefinder, etc.) which might indicate his status as a sniper. The risk of captured snipers being summarily executed is explicitly referred to in Chapter 6 of US Army doctrine document FM 3-060.11 entitled "SNIPER AND COUNTERSNIPER TACTICS, TECHNIQUES, AND PROCEDURES":

The negative reputation and perception of snipers can be traced back to the American Revolution, when American "Marksmen" intentionally targeted British officers, an act considered uncivilized by the British Army at the time (this reputation was cemented during the Battle of Saratoga, when Benedict Arnold allegedly ordered his marksmen to target British General Simon Fraser, an act that won the battle and French support). The British side used specially selected sharpshooters as well, often German mercenaries.

To demoralize enemy troops, snipers can follow predictable patterns. During the 26th of July Movement in the Cuban Revolution, the revolutionaries led by Fidel Castro always killed the foremost man in a group of President Batista's soldiers. Realizing this, none of Batista's men would walk first, as it was suicidal. This effectively decreased the army's willingness to search for rebel bases in the mountains. An alternative approach to this psychological process is to kill the second man in the row, leading to the psychological effect of nobody wanting to follow the "leader".

The occurrence of sniper warfare has led to the evolution of many counter-sniper tactics in modern military strategies. These aim to reduce the damage caused by a sniper to an army, which can often be harmful to both combat capabilities and morale.

The risk of damage to a chain of command can be reduced by removing or concealing features that would otherwise indicate an officer's rank. Modern armies tend to avoid saluting officers in the field, and eliminate rank insignia on battle dress uniforms (BDU). Officers can seek maximum cover before revealing themselves as good candidates for elimination through actions such as reading maps or using radios.

Friendly snipers can be used to hunt the enemy sniper. Besides direct observation, defending forces can use other techniques. These include calculating the trajectory of a bullet by triangulation. Traditionally, triangulation of a sniper's position was done manually, though radar-based technology has recently become available. Once located, the defenders can attempt to approach the sniper from cover and overwhelm them. The United States military is funding a project known as RedOwl (Robot Enhanced Detection Outpost With Lasers), which uses laser and acoustic sensors to determine the exact direction from which a sniper round has been fired.

The more rounds fired by a sniper, the greater the chance the target has of locating him. Thus, attempts to draw fire are often made, sometimes by offering a helmet slightly out of concealment, a tactic successfully employed in the Winter War by the Finns known as "Kylmä-Kalle" (Cold Charlie). They used a shop mannequin or other doll dressed as a tempting target, such as an officer. The doll was then presented as if it were a real man sloppily covering himself. Usually, Soviet snipers were unable to resist the temptation of an apparently easy kill. Once the angle where the bullet came from was determined, a large caliber gun, such as a Lahti L-39 "Norsupyssy" ("Elephant rifle") anti-tank rifle was fired at the sniper to kill him.

Other tactics include directing artillery or mortar fire onto suspected sniper positions, the use of smoke screens, placing tripwire-operated munitions, mines, or other booby-traps near suspected sniper positions. Even dummy trip-wires can be placed to hamper sniper movement. If anti-personnel mines are unavailable, it is possible to improvise booby-traps by connecting trip-wires to hand grenades, smoke grenades or flares. Though these may not kill a sniper, they will reveal their location. Booby-trap devices can be placed near likely sniper hides, or along the probable routes to and from positions. Knowledge of sniper field-craft will assist in this task.

The use of canine units had been very successful, especially during the Vietnam War.

The use of sniping (in the sense of shooting at relatively long range from a concealed position) to murder came to public attention in a number of sensational U.S. criminal cases, including the Austin sniper incident of 1966 (Charles Whitman), the John F. Kennedy assassination (Lee Harvey Oswald), and the Beltway sniper attacks of late 2002 (Lee Boyd Malvo). However, these incidents usually do not involve the range or skill of military snipers; in all three cases the perpetrators had U.S. military training, but in other specialties. News reports will often (inaccurately) use the term sniper to describe anyone shooting with a rifle at another person.

Sniping has been used in asymmetric warfare situations, for example in the Northern Ireland Troubles, where in 1972, the bloodiest year of the conflict, the majority of the soldiers killed were shot by concealed IRA riflemen. There were some instances in the early 1990s of British soldiers and RUC personnel being shot with .50 caliber Barrett rifles by sniper teams collectively known as the South Armagh sniper.

The sniper is particularly suited to combat environments where one side is at a disadvantage. A careful sniping strategy can use a few individuals and resources to thwart the movement or other progress of a much better equipped or larger force. Sniping enables a few persons to instil terror in a much larger regular force — regardless of the size of the force the snipers are attached to. It is widely accepted that sniping, while effective in specific instances, is much more effective as a broadly deployed psychological attack or as a force-multiplier.

Snipers are less likely to be treated mercifully than non-snipers if captured by the enemy. The rationale for this is that ordinary soldiers shoot at each other at 'equal opportunity' whilst snipers take their time in tracking and killing individual targets in a methodical fashion with a relatively low risk of retaliation.

In 2003, the U.S.-led multinational coalition composed of primarily U.S. and UK troops occupied Iraq and attempted to establish a new government in the country. However, shortly after the initial invasion, violence against coalition forces and among various sectarian groups led to asymmetric warfare with the Iraqi insurgency and civil war between many Sunni and Shia Iraqis.

Through to November 2005 the Army had attributed 28 of 2,100 U.S. deaths to enemy snipers. In 2006, it was claimed that one insurgent sniper, "Juba", had shot up to 37 American soldiers.

Training materials obtained by U.S. intelligence had among its tips for shooting U.S. troops, "Killing doctors and chaplains is suggested as a means of psychological warfare.", suggesting that those casualties would demoralize entire units.

Sniper activity was reported during the Arab Spring civil unrest in Libya in 2011, both from anti-governmental and pro-governmental supporters, and in Syria at least from pro-government forces.

Even before firearms were available, soldiers such as archers were specially trained as elite marksmen.







</small>


</doc>
<doc id="28130" url="https://en.wikipedia.org/wiki?curid=28130" title="Sign">
Sign

A sign is an object, quality, event, or entity whose presence or occurrence indicates the probable presence or occurrence of something else. A natural sign bears a causal relation to its object—for instance, thunder is a sign of storm, or medical symptoms a sign of disease. A conventional sign signifies by agreement, as a full stop signifies the end of a sentence; similarly the words and expressions of a language, as well as bodily gestures, can be regarded as signs, expressing particular meanings. The physical objects most commonly referred to as signs (notices, road signs, etc., collectively known as signage) generally inform or instruct using written text, symbols, pictures or a combination of these.
The philosophical study of signs and symbols is called semiotics; this includes the study of semiosis, which is the way in which signs (in the semiotic sense) operate.

Semiotics, epistemology, logic, and philosophy of language are concerned about the nature of signs, what they are and how they signify. The nature of signs and symbols and significations, their definition, elements, and types, is mainly established by Aristotle, Augustine, and Aquinas. According to these classic sources, significance is a relationship between two sorts of things: signs and the kinds of things they signify (intend, express or mean), where one term necessarily causes something else to come to the mind. Distinguishing natural signs and conventional signs, the traditional theory of signs (Augustine) sets the following threefold partition of things:
all sorts of indications, evidences, symptoms, and physical signals, there are signs which are "always" signs (the entities of the mind as ideas and images, thoughts and feelings, constructs and intentions); and there are signs that "have" to get their signification (as linguistic entities and cultural symbols). So, while natural signs serve as the source of signification, the human mind is the agency through which signs signify naturally occurring things, such as objects, states, qualities, quantities, events, processes, or relationships. Human language and discourse, communication, philosophy, science, logic, mathematics, poetry, theology, and religion are only some of fields of human study and activity where grasping the nature of signs and symbols and patterns of signification may have a decisive value. Communication takes place without words but via the mind as a result of signs and symbols; They communicate/pass across/ messages to the human mind through their pictorial representation.

The word "sign" has a variety of meanings in English, including:

St. Augustine was the first man who synthesized the classical and Hellenistic theories of signs. For him a sign is a thing which is used to signify other things and to make them come to mind ("De Doctrina Christiana" (hereafter DDC) 1.2.2; 2.1.1). The most common signs are spoken and written words (DDC 1.2.2; 2.3.4-2.4.5). Although God cannot be fully expressible, Augustine gave emphasis to the possibility of God’s communication with humans by signs in Scripture (DDC 1.6.6). Augustine endorsed and developed the classical and Hellenistic theories of signs. Among the mainstream in the theories of signs, i.e., that of Aristotle and that of Stoics, the former theory filtered into the works of Cicero (106-43 BC, "De inventione rhetorica" 1.30.47-48) and Quintilian (circa 35-100, "Institutio Oratoria" 5.9.9-10), which regarded the sign as an instrument of inference. In his commentary on Aristotle’s "De Interpretatione", Ammonius said, "according to the division of the philosopher Theophrastus, the relation of speech is twofold, first in regard to the audience, to which speech signifies something, and secondly in regard to the things about which the speaker intends to persuade the audience." If we match DDC with this division, the first part belongs to DDC Book IV and the second part to DDC Books I-III. Augustine, although influenced by these theories, advanced his own theological theory of signs, with whose help one can infer the mind of God from the events and words of Scripture.

Books II and III of DDC enumerate all kinds of signs and explain how to interpret them. Signs are divided into natural ("naturalia") and conventional ("data"); the latter is divided into animal ("bestiae") and human ("homines"); the latter is divided into non-words ("cetera") and words ("verba"); the latter is divided into spoken words ("voces") and written words ("litterae"); the latter is divided into unknown signs ("signa ignota") and ambiguous signs ("signa ambigua"); both the former and the latter are divided respectively into particular signs ("signa propria") and figurative signs ("signa translata"), among which the unknown figurative signs belong to the pagans.
In addition to exegetical knowledge (Quintilian, "Institutio Oratoria" 1.4.1-3 and 1.8.1-21) which follows the order of reading ("lectio"), textual criticism ("emendatio"), explanation ("enarratio"), and judgment ("iudicium"), one needs to know the original language (Hebrew and Greek) and broad background information on Scripture (DDC 2.9.14-2.40.60).

Augustine’s understanding of signs includes several hermeneutical presuppositions as important factors. First, the interpreter should proceed with humility, because only a humble person can grasp the truth of Scripture (DDC 2.41.62). Second, the interpreter must have a spirit of active inquiry and should not hesitate to learn and use pagan education for the purpose of leading to Christian learning, because all truth is God’s truth (DDC 2.40.60-2.42.63). Third, the heart of interpreter should be founded, rooted, and built up in love which is the final goal of the entire Scriptures (DDC 2.42.63).

The sign does not function as its own goal, but its purpose lies in its role as a signification ("res significans", DDC 3.9.13). God gave signs as a means to reveal himself; Christians need to exercise hermeneutical principles in order to understand that divine revelation. Even if the Scriptural text is obscure, it has meaningful benefits. For the obscure text prevents us from falling into pride, triggers our intelligence (DDC 2.6.7), tempers our faith in the history of revelation (DDC 3.8.12), and refines our mind to be suitable to the holy mysteries (DDC 4.8.22). When interpreting signs, the literal meaning should first be sought, and then the figurative meaning (DDC 3.10.14-3.23.33). Augustine suggests the hermeneutical principle that the obscure Scriptural verse is interpreted with the help of plain and simple verses, which formed the doctrine of "scriptura scripturae interpres" (Scripture is the Interpreter of Scripture) in the Reformation Era. Moreover, he introduces the seven rules of Tyconius the Donatist to interpret the obscure meaning of the Bible, which demonstrates his understanding that all truth belongs to God (DDC 3.3.42-3.37.56). In order to apply Augustine's hermeneutics of the sign appropriately in modern times, every division of theology must be involved and interdisciplinary approaches must be taken.


</doc>
<doc id="28131" url="https://en.wikipedia.org/wiki?curid=28131" title="Standard Alphabet by Lepsius">
Standard Alphabet by Lepsius

The Standard Alphabet is a Latin-script alphabet developed by Karl Richard Lepsius. Lepsius initially used it to transcribe Egyptian hieroglyphs and extended it to write African languages, published in 1854 and 1855, and in a revised edition in 1863. The alphabet was comprehensive but was not used much as it contained a lot of diacritic marks and was difficult to read and typeset at that time. It was, however, influential in later projects such as Ellis's Paleotype, and diacritics such as the acute accent for palatalization, under-dot for retroflex, underline for Arabic emphatics, and the click letters continue in modern use.

Vowel length is indicated by a macron ("ā") or a breve ("ă") for long and short vowels, respectively. Open vowels are marked by a line under the letter ("e̱"), while a dot below the letter makes it a close vowel ("ẹ"). Rounded front vowels are written with an umlaut ("ö" and "ü" ), either on top or below, when the space above the letter is needed for vowel length marks (thus "ṳ̄" or "ṳ̆"). Unrounded back vowels are indicated by a corner (˻) below "e" or "i". (Central vowels may be written as one of these series, or as reduced vowels.) 

As in the , nasal vowels get a tilde ("ã"). 

A small circle below a letter is used to mark both the schwa ("e̥", also "ḁ" etc. for other reduced vowels) and syllabic consonants ("r̥" or "l̥", for instance). 

Diphthongs do not receive any special marking, they are simply juxtaposed ("ai" ). A short sign can be used to distinguish which element of the diphthong is the on- or off-glide ("uĭ, ŭi") Vowels in hiatus can be indicated with a diaeresis when necessary ("aï" ).

Other vowels are "a" with a subscript "e" for ; "a" with a subscript "o" for , and "o̩" for or maybe . The English syllabic is "ṙ̥".

Word stress is marked with an acute accent on a long vowel ("á") and with a grave accent on a short vowel ("à").

The Lepsius letters without predictable diacritics are as follows:

Other consonant sounds may be derived from these. For example, palatal and palatalized consonants are marked with an acute accent: "ḱ" , "ǵ" , "ń" , "χ́" , "š́" , "γ́" , "ž́" , "ĺ" , "‘ĺ" , "ı́" , "ṕ" , etc. These can also be written "ky, py" etc.

Labialized velars are written with an over-dot: "ġ" , "n̈" , etc. (A dot on a non-velar letter, as in "ṅ" and "ṙ" in the table above, indicates a guttural articulation.)

Retroflex consonants are marked with an under-dot: "ṭ" , "ḍ" , "ṇ" , "ṣ̌" , "ẓ̌" , "ṛ" , "ḷ" , and "ı̣" .

The Semitic "emphatic" consonants are marked with an underline: "ṯ" , "ḏ" , "s̱" , "ẕ" , "δ̱" , "ḻ" .

Aspiration is typically marked by "h": "kh" , but a turned apostrophe (Greek "spiritus asper") is also used: "k̒" , "ģ" . Either convention may be used for voiceless sonorants: "m̒" , "‘l" .

Affricates are generally written as sequences, e.g. "tš" for . But the single letters "č" , "ǰ" , "c̀" , "j̀" , "ț" , and "d̦" are also used.

Implosives are written with a macron: "b̄" , "d̄" , "j̄" , "ḡ" . As with vowels, long (geminate) consonants may also be written with a macron, so this transcription can be ambiguous. 

Lepsius typically characterized ejective consonants as tenuis, as they are completely unaspirated, and wrote them with the Greek "spiritus lenis" ("p’", "t’", etc.), which may be the source of the modern convention for ejectives in the IPA. However, when his sources made it clear that there was some activity in the throat, he transcribed them as emphatics. 

When transcribing consonant letters which are pronounced the same but are etymologically distinct, as in Armenian, diacritics from the original alphabet or roman transliteration may be carried over. Similarly, unique sounds such as Czech "ř" may be carried over into Lepsius transcription. Lepsius used a diacritic "r" under "t᷊" and "d᷊" for some poorly described sounds in Dravidian languages. 

Standard capitalization is used. For example, when written in all caps, "γ" becomes "Γ" (as in "AFΓAN" "Afghan").

Tones are marked with an acute and grave accents (backticks) to the right and near the top or the bottom of the corresponding vowel. The diacritic may be underlined for a lower pitch, distinguishing in all eight possible tones.

Tone is not written directly, but rather needs to be established separately for each language. For example, the acute accent may indicate a high tone, a rising tone, or, in the case of Chinese, any tone called "rising" (上) for historical reasons. 

Low rising and falling tones can be distinguished from high rising and falling tones by underlining the accent mark: . The underline also transcribes the Chinese "yin" tones, under the mistaken impression that these tones are actually lower. Two additional tone marks, without any defined phonetic value, are used for Chinese: "level" maˏ (平) and checked maˎ (入); these may also be underlined.




</doc>
<doc id="28132" url="https://en.wikipedia.org/wiki?curid=28132" title="Sidehill gouger">
Sidehill gouger

In American folklore, a Sidehill gouger is a fearsome critter adapted to living on hillsides by having legs on one side of their body shorter than the legs on the opposite side. This peculiarity allows them to walk on steep hillsides, although only in one direction; when lured or chased into the plain, they are trapped in an endless circular path. The creature is variously known as the Sidehill Dodger, Sidehill Hoofer, Sidehill Ousel, Sidehill Loper, Gyascutus, Sidewinder, Wampus, Gudaphro, Hunkus, Rickaboo Racker, Prock, Gwinter, or Cutter Cuss.

Sidehill gougers are mammals who dwell in hillside burrows, and are occasionally depicted as laying eggs There are usually 6 to 8 pups to a litter. Since the gouger is footed for hillsides, it cannot stand up on level ground. If by accident a gouger falls from a hill, it can easily be captured or starve to death. When a clockwise gouger meets a counter-clockwise gouger, they have to fight to the death since they can only go in one direction. The formation of terracettes has been attributed to gouger activity.

Gougers are said to have migrated to the west from New England, a feat accomplished by a pair of gougers who clung to each other in a fashion comparable to "a pair of drunks going home from town with their longer legs on the outer sides. 
A Vermont variation is known as the Wampahoofus. It was reported that farmers crossbreed them with their cows so they could graze easily on mountain sides.

Frank C. Whitmore and Nicholas Hotton, in their joint tongue-in-cheek response to an article in "Smithsonian Magazine", expounded the taxonomy of sidehill gougers ("Membriinequales declivitous"), noting in particular "the sidehill dodger, which inhabits the Driftless Area of Wisconsin; the dextrosinistral limb ratio approaches unity although the metapodials on the downhill side are noticeably stouter." A special award, the Order of the Sidehill Gouger, is awarded to worthy members for hard and long standing volunteer efforts by the Alberta Group of the Royal Canadian Air Force Association.





</doc>
<doc id="28133" url="https://en.wikipedia.org/wiki?curid=28133" title="Strike">
Strike

Strike may refer to:














</doc>
