<doc id="25292" url="https://en.wikipedia.org/wiki?curid=25292" title="Quest for Glory">
Quest for Glory

Quest for Glory is a series of hybrid adventure/role-playing video games, which were designed by Corey and Lori Ann Cole. The series was created in the Sierra Creative Interpreter, a toolset developed at Sierra specifically to assist with adventure game development. The series combines humor, puzzle elements, themes and characters borrowed from various legends, puns, and memorable characters, creating a 5-part series in the Sierra stable.

The series was originally titled "Hero's Quest". However, Sierra failed to trademark the name. The Milton Bradley Company successfully trademarked an electronic version of their unrelated joint Games Workshop board game, "HeroQuest", which forced Sierra to change the series' title to "Quest for Glory". This decision meant that all future games in the series (as well as newer releases of "Hero's Quest I") used the new name.

Lori Cole pitched Quest for Glory to Sierra as a: "rich, narrative-driven, role-playing experience".

The series consisted of five games, each of which followed directly upon the events of the last. New games frequently referred to previous entries in the series, often in the form of cameos by recurring characters. The objective of the series is to transform the player character from an average adventurer to a hero by completing non-linear quests.

The game also was revolutionary in its character import system. This allowed players to import their individual character, including the skills and wealth s/he had acquired, from one game to the next.

Hybrids by their gameplay and themes, the games feature serious stories leavened with humor throughout. There are real dangers to face, and true heroic feats to perform, but silly details and overtones creep in (when the drama of adventuring does not force them out). Cheap word play is particularly frequent, to the point that the second game's ending refers to itself as the hero's "latest set of adventures and miserable puns."

The games have recurring story elements. For example, each installment in the series requires the player to create a dispel potion.

The games include a number of easter eggs, including a number of allusions to other Sierra games. For example, if a player types "pick nose" in the first game, (or clicks the lockpick icon on the player in the new version), if their lock-picking skill is high enough, the game responds: "Success! You now have an open nose". If the skill is too low, the player could insert the lock pick too far, killing himself. Another example is Dr. Cranium, an allusion to "The Castle of Dr. Brain", in the fourth game.

Each game draws its inspiration from a different culture and mythology: (in order, Germanic/fairy tale; Middle Eastern/Arabian Nights; Egyptian/African; Slavic folklore; and finally Greco-Mediterranean) with the hero facing increasingly powerful opponents with help from characters who become more familiar from game to game.

Each game varies somewhat from the tradition it is derived from; for example, Baba Yaga, a character borrowed from Slavic folklore, appears in the first game which is based on German mythology. The second game, which uses Middle Eastern folklore, introduces several Arab and African-themed characters who reappear in the third game based on Egyptian mythology. Characters from every game and genre in the series reappear in the fourth and fifth games. In addition to deviating from the player's expectations of the culture represented in each game, the series also includes a number of intentional anachronisms, such as the pizza-loving, mad scientists in the later games.

Many CRPG enthusiasts consider the "Quest for Glory" series to be among the best in the genre, and the series is lauded for its non-linearity. The games are notable for blending the mechanics of adventure video games and roleplaying video games, their unique tone which combines pathos and humour, and the game systems which were ahead of their time, such as day-night cycles, non-playable characters which adhered to their own schedules within the games, and character improvement through both skill practice and point investiture. The website Polygon and the Kotaku blog have characterised the game as a precursor to modern day RPGs. Fraser Brown of the Destructoid blog considers the games: "one of the greatest adventure series of all time".

Rowan Kaizer of the blog Engadget credits the games' hybrid adventure and roleplaying systems for the series' success. "The binary succeed/fail form of adventure game puzzles tended to either make those games too easy or too hard," he wrote, "But most puzzles in "Quest For Glory" involved some kind of skill check for your hero. This meant that you could succeed at most challenges by practicing or exploring, instead of getting stuck on bizarre item-combination puzzles".

The first four games are hybrid Adventure/Role playing video games with real-time combat, while the fifth game switches to the Action/RPG genre.

The gameplay standards established in earlier Sierra adventure games are enhanced by the player's ability to choose his character's career path from among the three traditional role-playing game backgrounds: fighter, magic-user/wizard and thief. Further variation is added by the ability to customize the Hero's abilities, including the option of selecting skills normally reserved for another character class, leading to unique combinations often referred to as "hybrid characters". During the second or third games, a character can be initiated as a Paladin by performing honorable actions, changing his class and abilities, and receiving a unique sword. This applies when the character is exported into later games. Any character that finishes any game in the series (except "Dragon Fire", the last in the series) can be exported to a more recent game ("Shadows of Darkness" has a glitch which allows one to import characters from the same game), keeping the character's statistics and parts of its inventory. If the character received the paladin sword, he would keep the magic sword (Soulforge or Piotyr's sword) and special paladin magic abilities. A character imported into a later game in the series from any other game can be assigned any character class, including Paladin.

Each career path has its own strengths and weaknesses, and scenarios unique to the class because of the skills associated with it. Each class also has its own distinct way to solve various in-game puzzles, which encourage replay: some puzzles have up to four different solutions. For instance, if a door is closed, instead of lockpicking or casting an open spell, the fighter can simply knock down the door. The magic user and the thief are both non-confrontational characters, as they lack the close range ability of the fighter, but are better able to attack from a distance, using daggers or spells. An example of these separate paths can be seen early in the first game. A gold ring belonging to the healer rests in a nest on top of a tree; fighters might make it fall by hurling rocks, thieves may want to climb the tree, while a magic user can simply cast the fetch spell to retrieve the nest, and then, while the fighter and magic user return the ring for a reward, the thief can choose between returning or selling the same ring in the thieves' guild (which is not available for those not possessing the "thieving" skills). It is also possible to build, over the course of several games, a character that has points in every skill in the game, and can therefore perform nearly every task.

Each character class features special abilities unique to that class, as well as a shared set of attributes which can be developed by performing tasks and completing quests. In general, for a particular game the maximum value which can be reached for an ability is 100*[the number of that game]. "Quest for Glory V" allows stat bonuses which can push an attribute over the maximum and lets certain classes raise certain attributes beyond the normal limits. "Quest for Glory V" also features special kinds of equipment which lower some stats while raising others. At the beginning of each game, the player may assign points to certain attributes, and certain classes only have specific attributes enabled, although skills can be added for an extra cost.

General attributes influence all characters' classes and how they interact with objects and other people in the game; high values in strength allows movement of heavier objects and communication helps with bargaining goods with sellers. These attributes are changed by performing actions related to the skill; climbing a tree eventually increases the skill value in climb, running increases vitality, and so on. There are also complementing skills which are only of associated with some classes; parry (the ability to block a blow with the sword), for instance, is mainly used by fighters and paladins, lock picking and sneaking thief's hobby, and the ability to cast magic spells is usually associated with magic user.

Vital statistics are depleted by performing some actions. Health, (determined by strength and vitality), determines the hit points of the character, which decreases when the player is attacked or harms himself. Stamina, (based on agility and vitality), limits the number of actions (exercise, fighting, running, etc.) the character is able to perform before needing rest or risking injury. Mana is only required by characters with skill in magic, and is calculated according to the character's intelligence and magic attributes.

Puzzle and Experience points only show the development of the player and his progress in the game, though in the first game also affects the kind of random encounters a player faces, as some monsters only appear after a certain level of experience is reached.

In the valley barony of Spielburg, the evil ogress Baba Yaga has cursed the land and the baron who tried to drive her off. His children have disappeared, while the land is ravaged by monsters and brigands. The Valley of Spielburg is in need of a Hero able to solve these problems.

The original game was released in 1989 while a VGA remake was released in 1992.

"Quest for Glory II: Trial by Fire" takes place in the land of Shapeir, in the world of Gloriana. Directly following from the events of the first game, the newly proclaimed Hero of Spielburg travels by flying carpet with his friends Abdulla Doo, Shameen and Shema to the desert city of Shapeir. The city is threatened by magical elementals, while the Emir Arus al-Din of Shapeir's sister city Raseir is missing and his city fallen under tyranny.

"Quest for Glory II" is the only game in the series not to have originated or have been remade beyond the EGA graphics engine by Sierra, but AGD Interactive released a VGA fan remake of the game using the Adventure Game Studio engine on August 24, 2008.

Rakeesh the Paladin brings the Hero (and Prince of Shapeir) along with Uhura and her son Simba to his homeland, the town of Tarna in a jungle and savannah country called Fricana that resembles central African ecosystems.

Tarna is on the brink of war; the Simbani, the tribe of Uhura, are ready to do battle with the Leopardmen. Each tribe has stolen a sacred relic from the other, and both refuse to return it until the other side does. The Hero must prevent the war then thwart a demon who may be loosed upon the world.

Drawn without warning from his victory in Fricana, the Hero arrives without equipment or explanation in the middle of the hazardous Dark One Caves in the distant land of Mordavia. While struggling to survive in this land plagued with undead, the Hero must prevent a dark power from summoning eternal darkness into the world.

Erasmus introduces the player character, the Hero, to the Greece-like kingdom of Silmaria, whose king was recently assassinated. Thus, the traditional Rites of Rulership are due to commence, and the victor will be crowned king. The Hero enters the contest with the assistance of Erasmus, Rakeesh, and many old friends from previous entries in the series. The Hero competes against competitors, including the Silmarian guard Kokeeno Pookameeso, the warlord Magnum Opus, the hulking Gort, and the warrior Elsa Von Spielburg.


Originally, the series was to be a tetralogy, consisting of 4 games, with the following themes and cycles: the 4 cardinal directions, the 4 classical elements, the 4 seasons and 4 different mythologies.

This is what the creators originally had in mind:
However, when "" was designed, it was thought that it would be too difficult for the hero to go straight from Shapeir to Mordavia and defeat the Dark One. To solve the problem, a new game, "", was inserted into the canon, and resulting in a renumbering of the series. Evidence for this can be found in the end of "": the player is told that the next game will be "" and a fanged vampiric moon is shown, to hint at the next game's theme.

The developers discussed this in the Fall 1992 issue of Sierra's "InterAction" magazine, and an online chat room:
Somewhere between finishing "Trial by Fire" and cranking up the design process for "Shadows of Darkness", the husband-and-wife team realized a fifth chapter would have to be added to bridge the games. That chapter became "Wages of War".

The concept of seasons in the games represents the maturation of the Hero as he moves from story to story. It's a critical component in a series that – from the very beginning – was designed to be a defined quartet of stories, representing an overall saga with a distinct beginning, middle, and end.

In the first episode, the player is a new graduate of the Famous Adventurer's Correspondence School, ready to venture out into the springtime of his career and build a rep. It's a light-hearted, exhilarating journey into the unknown that can be replayed three times with three distinct outlooks at puzzle-solving.

In the second chapter – "Trial by Fire" – the Hero enters the summer of his experience, facing more difficult challenges with more highly developed skills. While the episode is more serious and dangerous than its predecessor, it retains the enchanting mixture of fantasy, challenge, and humor that made the first game a hit with so many fans.

Of all the reasons Lori and Corey found for creating a bridge between "Trial by Fire" and "Shadows of Darkenss", the most compelling was the feeling that the Hero character simply hadn't matured enough to face the very grim challenges awaiting him in Transylvania.

Along with the Hero, several recurring characters appear and re-appear throughout the series including: Rakeesh Sah Tarna, Baba Yaga, Abdullah Doo, Elsa von Spielburg, the evil Ad Avis, and others.

The fictional world in which the Quest for Glory series takes place includes the town of Spielburg (based on German folklore), the desert city of Shapeir (based on the Arabia of "One Thousand and One Nights"), the jungle city of Tarna (based on African mythology, especially Egypt), the hamlet of Mordavia (based on Slavic mythology) and Silmaria (based on Greek mythology). Adventures, monsters and story of the games are usually drawn from legends of the respective mythology on which a title is based, although there are several cross-over exceptions, like the Eastern European Baba Yaga also appearing in the first game, which is distinctly German.


</doc>
<doc id="25293" url="https://en.wikipedia.org/wiki?curid=25293" title="Quango">
Quango

A quango or QUANGO (less often QuANGO or QANGO) is a quasi non-governmental organisation. It is typically an organisation to which a government has devolved power, but which is still partly controlled and/or financed by government bodies. As its name suggests, a quango is a hybrid form of organization, with elements of both non-government organizations (NGOs) and public sector bodies. The concept is most often applied in the United Kingdom and, to a lesser degree, Australia, Canada, Ireland, New Zealand, the United States, and other English-speaking countries. 

In the UK, the term quango covers different "arm's-length" government bodies, including "non-departmental public bodies" (NDPBs), non-ministerial government departments, and executive agencies. One UK example is the Forestry Commission, which is a non-ministerial government department responsible for forestry in England. The term has spawned the derivative quangocrat; the Taxpayers' Alliance faulted a majority of them for not making declarations of political activity.
The acronym has been extended to cover government agencies of all kinds, often being spelt out as quasi-autonomous national government organization and sometimes modified to qango.

In 2006, there were 832 quangos in Ireland - 482 at national and 350 at local level - with a total of 5,784 individual appointees and a combined annual budget of €13 billion.

The Irish majority party, Fine Gael, had promised to eliminate 145 quangos should they be the governing party in the 2016 election. Since coming to power they have reduced the overall number of quangos by 17. This reduction also included agencies which the former government had already planned to remove.

Despite a 'commitment' from the 1979 Conservative party to curb the growth of unelected bodies, their numbers grew rapidly through their time in power throughout the 80s. 

The Cabinet Office 2009 report on non-departmental public bodies found that there are 766 NDPBs sponsored by the UK government.
The number has been falling: There was 827 in 2007 and 790 in 2008. The number of NDPBs has fallen by over 10% since 1997. Staffing and expenditure of NDPBs have increased. They employed 111,000 people in 2009 and spent £46.5 billion, of which £38.4 billion was directly funded by the Government.

Since the coalition government of Conservatives and Liberal Democrats was formed in May 2010, numerous NDPBs have been abolished under Conservative plans to reduce the overall budget deficit by reducing the size of the public sector. As of the end of July 2010, the government had abolished at least 80 NDPBs and warned many others that they faced mergers or deep cuts. In September 2010, "The Telegraph" published a leaked Cabinet Office list suggesting that a further 94 could be abolished, while four would be privatised and 129 merged. In August 2012, Cabinet Office minister Francis Maude said the government was on course to abolish 204 public bodies by 2015, and said this would create a net saving of at least £2.6 billion.

Use of the term quango is less common and therefore more controversial in the United States due to their commitment to limited government and electoral accountability. However, Paul Krugman has stated that the US Federal Reserve is, effectively, "what the British call a quango... Its complex structure divides power between the federal government and the private banks that are its members, and in effect gives substantial autonomy to a governing board of long-term appointees."

Two other U.S.-based organizations that might be described as quangos are the Internet Corporation for Assigned Names and Numbers (ICANN) and the National Center for Missing and Exploited Children (NCMEC).

The term "quasi non-governmental organisation" was created in 1967 by Alan Pifer of the US-based Carnegie Foundation, in an essay on the independence and accountability of public-funded bodies that are incorporated in the private sector. This essay got the attention of David Howell, a Conservative M.P. in Britain, who then organized an Anglo-American project with Pifer, to examine the pros and cons of such enterprises. The lengthy term was shortened to the acronym (later lowercased quango) by a British participant to the joint project, Anthony Barker, during one of the conferences on the subject.

It describes an ostensibly non-governmental organisation performing governmental functions, often in receipt of funding or other support from government, while mainstream NGOs mostly get their donations or funds from the public and other organisations that support their cause. Numerous quangos were created from the 1980s onwards. Examples in the United Kingdom include those engaged in the regulation of various commercial and service sectors, such as the Water Services Regulation Authority.

An essential feature of a quango in the original definition was that it should not be a formal part of the state structure. The term was then extended to apply to a range of organisations, such as executive agencies providing (from 1988) health, education and other services. Particularly in the UK, this occurred in a polemical atmosphere in which it was alleged that proliferation of such bodies was undesirable and should be reversed. In this context, the original acronym was often replaced by a backronym spelt out as "quasi-autonomous national government organisation, and often rendered as 'qango' This spawned the related acronym "qualgo", a 'quasi-autonomous "local" government organisation'. "London Waste Regulation Authority, the first 'qualgo' formed after abolition of the Greater London Council...The new body is a joint board of councilors from London boroughs. 'Qualgo' stands for 'quasi-autonomous local government organization', the municipal equivalent of a quango, in which members are appointed by other councilors". 

The less contentious term non-departmental public body (NDPB) is often employed to identify numerous organisations with devolved governmental responsibilities. The UK government's definition in 1997 of a non-departmental public body or quango was:
"The Times" has accused quangos of bureaucratic waste and excess. In 2005, Dan Lewis, author of "The Essential Guide to Quangos", claimed that the UK had 529 quangos, many of which were useless and duplicated the work of others.

Quangos are filled with appointed members. This means, unlike governmental bodies, members of quangos do not need to seek re-election. This is seen as a major criticism in liberal democracy as members of quangos have not been legitimised by the electorate, but have governmental power and influence. They also do not have the same level of accountability as elected officials, worsened by the lack of media coverage of their work.




</doc>
<doc id="25295" url="https://en.wikipedia.org/wiki?curid=25295" title="Quiver">
Quiver

A quiver is a container for holding arrows, bolts, darts, or javelins. It can be carried on an archer's body, the bow, or the ground, depending on the type of shooting and the archer's personal preference. Quivers were traditionally made of leather, wood, furs, and other natural materials, but are now often made of metal or plastic.

The English word quiver has its origins in Old French, written as quivre, cuevre or coivre . 

The most common style of quiver is a flat or cylindrical container suspended from the belt. They are found across many cultures from North America to China. Many variations of this type exist, such as being canted forwards or backwards, and being carried on the dominant hand side, off-hand side, or the small of the back. Some variants enclose almost the entire arrow, while minimalist "pocket quivers" consist of little more than a small stiff pouch that only covers the first few inches. The Bayeux Tapestry shows that most bowmen in medieval Europe used belt quivers.

Back quivers are secured to the archer's back by leather straps, with the nock ends protruding above the dominant hand's shoulder. Arrows can be drawn over the shoulder rapidly by the nock. This style of quiver was used by native peoples of North America and Africa, and was also commonly depicted in bas-reliefs from ancient Assyria. While popular in cinema and 20th century art for depictions of medieval European characters (such as Robin Hood), this style of quiver was rarely used in medieval Europe.

A ground quiver is used for both target shooting or warfare when the archer is shooting from a fixed location. They can be simply stakes in the ground with a ring at the top to hold the arrows, or more elaborate designs that hold the arrows within reach without the archer having to lean down to draw.

A modern invention, the bow quiver attaches directly to the bow's limbs and holds the arrows steady with a clip of some kind. They are popular with compound bow hunters as it allows one piece of equipment to be carried in the field without encumbering the hunter's body.

A style used by medieval English Longbowmen and several other cultures, an arrow bag is a simple drawstring cloth sack with a leather spacer at the top to keep the arrows divided. When not in use, the drawstring could be closed, completely covering the arrows so as to protect them from rain and dirt. Some had straps or rope sewn to them for carrying, but many either were tucked into the belt or set on the ground before battle to allow easier access.

Yebira refers to a variety of quiver designs. The Yazutsu is a different type, used in Kyudo. Arrows are removed from it before shooting, and held in the hand, so it is mainly used to transport and protect arrows.



Dr. Brian Marin, author of Ancient Warfare| Concordia Press| page 137


</doc>
<doc id="25296" url="https://en.wikipedia.org/wiki?curid=25296" title="Quid">
Quid

Quid may refer to:




</doc>
<doc id="25297" url="https://en.wikipedia.org/wiki?curid=25297" title="Quinine">
Quinine

Quinine is a medication used to treat malaria and babesiosis. This includes the treatment of malaria due to "Plasmodium falciparum" that is resistant to chloroquine when artesunate is not available. While sometimes used for restless legs syndrome, quinine is not recommended for this purpose due to the risk of serious side effects. It can be taken by mouth or intravenously. Malaria resistance to quinine occurs in certain areas of the world. Quinine is also the ingredient in tonic water that gives it its bitter taste.
Common side effects include headache, ringing in the ears, trouble seeing, and sweating. More severe side effects include deafness, low blood platelets, and an irregular heartbeat. Use can make one more prone to sunburn. While it is unclear if use during pregnancy causes harm to the baby, treating malaria during pregnancy with quinine when appropriate is still recommended. Quinine is an alkaloid, a naturally occurring chemical compound. How it works as a medicine is not entirely clear.
Quinine was first isolated in 1820 from the bark of a cinchona tree, which is native to Peru. Bark extracts had been used to treat malaria since at least 1632 and it was introduced to Spain as early as 1636 by Jesuit missionaries from the New World. It is on the World Health Organization's List of Essential Medicines.

As of 2006, quinine is no longer recommended by the World Health Organization (WHO) as a first-line treatment for malaria, because there are other substances that are equally effective with fewer side effects. They recommend that it be used only when artemisinins are not available. Quinine is also used to treat lupus and arthritis.

Quinine was frequently prescribed as an off-label treatment for leg cramps at night, but this has become less common due to a warning from the US Food and Drug Administration (FDA) that such practice is associated with life-threatening side effects.

Quinine is a basic amine and is usually provided as a salt. Various existing preparations include the hydrochloride, dihydrochloride, sulfate, bisulfate and gluconate. In the United States, quinine sulfate is commercially available in 324-mg tablets under the brand name Qualaquin.

All quinine salts may be given orally or intravenously (IV); quinine gluconate may also be given intramuscularly (IM) or rectally (PR). The main problem with the rectal route is that the dose can be expelled before it is completely absorbed; in practice, this is corrected by giving a further half dose. No injectable preparation of quinine is licensed in the US; quinidine is used instead.

Quinine is a flavor component of tonic water and bitter lemon drink mixers. On the soda gun behind many bars, tonic water is designated by the letter "Q" representing quinine.

According to tradition, because of the bitter taste of anti-malarial quinine tonic, British colonials in India mixed it with gin to make it more palatable, thus creating the gin and tonic cocktail, which is still popular today.

In France, quinine is an ingredient of an known as , or ""Cap Corse,"" and the wine-based Dubonnet. In Spain, quinine (also known as "Peruvian bark" for its origin from the native cinchona tree) is sometimes blended into sweet Malaga wine, which is then called ""Malaga Quina"". In Italy, the traditional flavoured wine Barolo Chinato is infused with quinine and local herbs, and is served as a . In Canada and Italy, quinine is an ingredient in the carbonated chinotto beverages Brio and San Pellegrino. In Scotland, the company A.G. Barr uses quinine as an ingredient in the carbonated and caffeinated beverage Irn-Bru. In Uruguay and Argentina, quinine is an ingredient of a PepsiCo tonic water named Paso de los Toros. In Denmark, it is used as an ingredient in the carbonated sports drink Faxe Kondi made by Royal Unibrew.

As a flavouring agent in drinks, quinine is limited to less than 83 parts per million in the United States, and in the European Union.

Quinine (and quinidine) are used as the chiral moiety for the ligands used in Sharpless asymmetric dihydroxylation as well as for numerous other chiral catalyst backbones. Because of its relatively constant and well-known fluorescence quantum yield, quinine is used in photochemistry as a common fluorescence standard.

Because of the narrow difference between its therapeutic and toxic effects, quinine is a common cause of drug-induced disorders, including thrombocytopenia and thrombotic microangiopathy. Even from minor levels occurring in common beverages, quinine can have severe adverse effects involving multiple organ systems, among which are immune system effects and fever, hypotension, hemolytic anemia, acute kidney injury, liver toxicity, and blindness. In people with atrial fibrillation, conduction defects, or heart block, quinine can cause heart arrhythmias, and should be avoided.

Quinine can cause hemolysis in G6PD deficiency (an inherited deficiency), but this risk is small and the physician should not hesitate to use quinine in people with G6PD deficiency when there is no alternative.

Quinine can cause unpredictable serious and life-threatening blood and cardiovascular reactions including low platelet count and hemolytic-uremic syndrome/thrombotic thrombocytopenic purpura (HUS/TTP), long QT syndrome and other serious cardiac arrhythmias including torsades de pointes, blackwater fever, disseminated intravascular coagulation, leukopenia, and neutropenia. Some people who have developed TTP due to quinine have gone on to develop kidney failure. It can also cause serious hypersensitivity reactions include anaphylactic shock, urticaria, serious skin rashes, including Stevens–Johnson syndrome and toxic epidermal necrolysis, angioedema, facial edema, bronchospasm, granulomatous hepatitis, and itchiness.

The most common adverse effects involve a group of symptoms called cinchonism, which can include headache, vasodilation and sweating, nausea, tinnitus, hearing impairment, vertigo or dizziness, blurred vision, and disturbance in color perception. More severe cinchonism includes vomiting, diarrhea, abdominal pain, deafness, blindness, and disturbances in heart rhythms. Cinchonism is much less common when quinine is given by mouth, but oral quinine is not well tolerated (quinine is exceedingly bitter and many people will vomit after ingesting quinine tablets). Other drugs, such as Fansidar (sulfadoxine with pyrimethamine) or Malarone (proguanil with atovaquone), are often used when oral therapy is required. Quinine ethyl carbonate is tasteless and odourless, but is available commercially only in Japan. Blood glucose, electrolyte and cardiac monitoring are not necessary when quinine is given by mouth.

Quinine has diverse unwanted interactions with numerous prescription drugs, such as potentiating the anticoagulant effects of warfarin.

Quinine is used for its toxicity to the malarial pathogen, "Plasmodium falciparum", by interfering with the parasite's ability to dissolve and metabolize hemoglobin. As with other quinoline antimalarial drugs, the precise mechanism of action of quinine has not been fully resolved, although in vitro studies indicate it inhibits nucleic acid and protein synthesis, and inhibits glycolysis in "P. falciparum". The most widely accepted hypothesis of its action is based on the well-studied and closely related quinoline drug, chloroquine. This model involves the inhibition of hemozoin biocrystallization in the heme detoxification pathway, which facilitates the aggregation of cytotoxic heme. Free cytotoxic heme accumulates in the parasites, causing their deaths. Quinine may target the malaria purine nucleoside phosphorylase enzyme.

The UV absorption of quinine peaks around 350 nm (in UVA). Fluorescent emission peaks at around 460 nm (bright blue/cyan hue). Quinine is highly fluorescent (quantum yield ~0.58) in 0.1 M sulfuric acid solution.

Cinchona trees remain the only economically practical source of quinine. However, under wartime pressure during World War II, research towards its synthetic production was undertaken. A formal chemical synthesis was accomplished in 1944 by American chemists R.B. Woodward and W.E. Doering. Since then, several more efficient quinine total syntheses have been achieved, but none of them can compete in economic terms with isolation of the alkaloid from natural sources. The first synthetic organic dye, mauveine, was discovered by William Henry Perkin in 1856 while he was attempting to synthesize quinine.

In the first step of quinine biosynthesis, the enzyme strictosidine synthase catalyzes a stereoselective Pictet–Spengler reaction between tryptamine and secologanin to yield strictosidine. Suitable modification of strictosidine leads to an aldehyde. Hydrolysis and decarboxylation would initially remove one carbon from the iridoid portion and produce corynantheal. Then the tryptamine side-chain were cleaved adjacent to the nitrogen, and this nitrogen was then bonded to the acetaldehyde function to yield cinchonaminal. Ring opening in the indole heterocyclic ring could generate new amine and keto functions. The new quinoline heterocycle would then be formed by combining this amine with the aldehyde produced in the tryptamine side-chain cleavage, giving cinchonidinone. For the last step, hydroxylation and methylation gives quinine.

Quinine was used as a muscle relaxant by the Quechua people, who are indigenous to Peru, Bolivia and Ecuador, to halt shivering due to low temperatures. The Quechua would mix the ground bark of cinchona trees with sweetened water to offset the bark's bitter taste, thus producing something similar to tonic water.

Spanish Jesuit missionaries were the first to bring cinchona to Europe. The Spanish had observed the Quechua's use of cinchona and were aware of the medicinal properties of cinchona bark by the 1570s or earlier: Nicolás Monardes (1571) and Juan Fragoso (1572) both described a tree, which was subsequently identified as the cinchona tree, whose bark was used to produce a drink to treat diarrhea. Quinine has been used in unextracted form by Europeans since at least the early 17th century.

It was first used to treat malaria in Rome in 1631. A popular story of how it was brought to Europe by the Countess of Chinchon was debunked by medical historian Alec Haggis around 1941. During the 17th century, malaria was endemic to the swamps and marshes surrounding the city of Rome. It had caused the deaths of several popes, many cardinals and countless common Roman citizens. Most of the Catholic priests trained in Rome had seen malaria victims and were familiar with the shivering brought on by the febrile phase of the disease.

The Jesuit brother Agostino Salumbrino (1564–1642), an apothecary by training who lived in Lima (now in present-day Peru), observed the Quechua using the bark of the cinchona tree to treat such shivering. While its effect in treating malaria (and malaria-induced shivering) was unrelated to its effect in controlling shivering from rigors, it was a successful medicine against malaria. At the first opportunity, Salumbrino sent a small quantity to Rome for testing as a malaria treatment. In the years that followed, cinchona bark, known as Jesuit's bark or Peruvian bark, became one of the most valuable commodities shipped from Peru to Europe. When King Charles II was cured of malaria at the end of the 17th Century with quinine, it became popular in London. It remained the antimalarial drug of choice until the 1940s, when other drugs took over.

The form of quinine most effective in treating malaria was found by Charles Marie de La Condamine in 1737. In 1820, French researchers Pierre Joseph Pelletier and Joseph Bienaimé Caventou first isolated quinine from the bark of a tree in the genus "Cinchona" – probably "Cinchona officinalis" – and subsequently named the substance. The name was derived from the original Quechua (Inca) word for the cinchona tree bark, "quina" or "quina-quina", which means "bark of bark" or "holy bark". Prior to 1820, the bark was dried, ground to a fine powder, and mixed into a liquid (commonly wine) in order to be drunk. Large-scale use of quinine as a malaria prophylaxis started around 1850. In 1853 Paul Briquet published a brief history and discussion of the literature on "quinquina".

Quinine played a significant role in the colonization of Africa by Europeans. The availability of quinine for treatment had been said to be the prime reason Africa ceased to be known as the "white man's grave". A historian said, "it was quinine's efficacy that gave colonists fresh opportunities to swarm into the Gold Coast, Nigeria and other parts of west Africa".

To maintain their monopoly on cinchona bark, Peru and surrounding countries began outlawing the export of cinchona seeds and saplings in the early 19th century. The Dutch government persisted in its attempts to smuggle the seeds, and by the late 19th century the Dutch grew the plants in Indonesian plantations. Soon they became the main suppliers of the tree. In 1913 they set up the Kina Bureau, a cartel of cinchona producers charged with controlling price and production. By the 1930s Dutch plantations in Java were producing 22 million pounds of cinchona bark, or 97% of the world's quinine production. U.S. attempts to prosecute the Kina Bureau proved unsuccessful.

During World War II, Allied powers were cut off from their supply of quinine when Germany conquered the Netherlands, and Japan controlled the Philippines and Indonesia. The US had obtained four million cinchona seeds from the Philippines and began operating cinchona plantations in Costa Rica. Additionally, they began harvesting wild cinchona bark during the Cinchona Missions. Such supplies came too late. Tens of thousands of US troops in Africa and the South Pacific died of malaria due to the lack of quinine. Despite controlling the supply, the Japanese did not make effective use of quinine, and thousands of Japanese troops in the southwest Pacific died as a result.

Quinine remained the antimalarial drug of choice until after World War II. Since then, other drugs that have fewer side effects, such as chloroquine, have largely replaced it.

"Bromo Quinine" were brand name cold tablets containing quinine, manufactured by Grove Laboratories. They were first marketed in 1889 and available until at least the 1960s.

Conducting research in central Missouri, Dr. John S. Sappington independently developed an anti-malaria pill from quinine. Sappington began importing cinchona bark from Peru in 1820. In 1832, using quinine derived from the cinchona bark, Sappington developed a pill to treat a variety of fevers, such as scarlet fever, yellow fever, and influenza in addition to malaria. These illnesses were widespread in the Missouri and Mississippi valleys. He manufactured and sold "Dr. Sappington's Anti-Fever Pills" across Missouri. Demand became so great that within three years, Dr. Sappington founded a company known as Sappington and Sons to sell his pills nationwide.

The bark of "Remijia" contains 0.5–2% of quinine. The bark is cheaper than bark of "Cinchona". As it has an intense taste, it is used for making tonic water.

From 1969, to 1992, the US Food and Drug Administration (FDA) received 157 reports of health problems related to quinine use, including 23 which had resulted in death. In 1994, the FDA banned the marketing of over-the-counter quinine as a treatment for nocturnal leg cramps. Pfizer Pharmaceuticals had been selling the brand name Legatrin for this purpose. Also sold as a Softgel (by SmithKlineBeecham) as Q-vel. Doctors may still prescribe quinine, but the FDA has ordered firms to stop marketing unapproved drug products containing quinine. The FDA is also cautioning consumers about off-label use of quinine to treat leg cramps. Quinine is approved for treatment of malaria, but was also commonly prescribed to treat leg cramps and similar conditions. Because malaria is life-threatening, the risks associated with quinine use are considered acceptable when used to treat that affliction.

Though Legatrin was banned by the FDA for the treatment of leg cramps, the drug manufacturer URL Mutual has branded a quinine-containing drug named Qualaquin. It is marketed as a treatment for malaria and is sold in the United States only by prescription. In 2004, the CDC reported only 1,347 confirmed cases of malaria in the United States.

Quinine is sometimes detected as a cutting agent in street drugs such as cocaine and heroin.

Quinine is used as a treatment for "Cryptocaryon irritans" (commonly referred to as white spot, crypto or marine ich) infection of marine aquarium fish.




</doc>
<doc id="25298" url="https://en.wikipedia.org/wiki?curid=25298" title="Quincy">
Quincy

Quincy may refer to:











</doc>
<doc id="25299" url="https://en.wikipedia.org/wiki?curid=25299" title="Quintuplet (disambiguation)">
Quintuplet (disambiguation)

A quintuplet is a set of five similar items. It may refer to:



</doc>
<doc id="25301" url="https://en.wikipedia.org/wiki?curid=25301" title="Quimby">
Quimby

Quimby may refer to:




</doc>
<doc id="25302" url="https://en.wikipedia.org/wiki?curid=25302" title="Quail">
Quail

Quail is a collective name for several genera of mid-sized birds generally placed in the order Galliformes. 
Old World quail are placed in the family Phasianidae, and New World quail are placed in the family Odontophoridae. The species of buttonquail are named for their superficial resemblance to quail, and form the family Turnicidae in the order Charadriiformes. The king quail, an Old World quail, often is sold in the pet trade, and within this trade is commonly, though mistakenly, referred to as a "button quail". Many of the common larger species are farm-raised for table food or egg consumption, and are hunted on game farms or in the wild, where they may be released to supplement the wild population, or extend into areas outside their natural range. In 2007, 40 million quail were produced in the U.S. 

The collective noun for a group of quail is a flock, covey, or bevy.


Quail that have fed on hemlock (e.g., during migration) may induce acute kidney injury due to accumulation of toxic substances from the hemlock in the meat; this problem is referred to as "coturnism".




</doc>
<doc id="25303" url="https://en.wikipedia.org/wiki?curid=25303" title="Quagmire (disambiguation)">
Quagmire (disambiguation)

A quagmire is a wetland type, dominated by living, peat-forming plants.

Quagmire may also refer to:






</doc>
<doc id="25305" url="https://en.wikipedia.org/wiki?curid=25305" title="Crossbow bolt">
Crossbow bolt

A quarrel or bolt is the ammunition used in a crossbow. The name "quarrel" is derived from the French word "carré", meaning square, referring to their typically square heads. Although their lengths vary, bolts are typically shorter than traditional arrows.

A bolt consists of four main parts;


There is not any hard and fast rule of bolt sizing. Generally, the bolts are of 15 to 22 inches long but the standard length is 20 inches. Experts recommend longer bolts but they have certain disadvantages as well. 

The weight of bolt can have a serious effect on the range of the bolt. The bolt's total weight includes the bolt's weight, nock, insert, vanes, and broadhead or field point. Almost all bolt manufacturers will list how many grains each shaft weighs or how many grains are in each inch of the shaft. A more massive bolt, e.g. at least 400 grains, will have better downrange energy and offer better penetration, but will travel more slowly and thus drop more due to gravity during its flight. A lighter bolt will fly quicker and give the shooter a longer range, but might not have the desired penetration.


</doc>
<doc id="25308" url="https://en.wikipedia.org/wiki?curid=25308" title="Quasispecies model">
Quasispecies model

The quasispecies model is a description of the process of the Darwinian evolution of certain self-replicating entities within the framework of physical chemistry. A quasispecies is a large group or "cloud" of related genotypes that exist in an environment of high mutation rate (at stationary state), where a large fraction of offspring are expected to contain one or more mutations relative to the parent. This is in contrast to a species, which from an evolutionary perspective is a more-or-less stable single genotype, most of the offspring of which will be genetically accurate copies.

It is useful mainly in providing a qualitative understanding of the evolutionary processes of self-replicating macromolecules such as RNA or DNA or simple asexual organisms such as bacteria or viruses (see also viral quasispecies), and is helpful in explaining something of the early stages of the origin of life. Quantitative predictions based on this model are difficult because the parameters that serve as its input are impossible to obtain from actual biological systems. The quasispecies model was put forward by Manfred Eigen and Peter Schuster based on initial work done by Eigen.

When evolutionary biologists describe competition between species, they generally assume that each species is a single genotype whose descendants are mostly accurate copies. (Such genotypes are said to have a high reproductive "fidelity".) In evolutionary terms, we are interested in the behavior and fitness of that one species or genotype over time.

Some organisms or genotypes, however, may exist in circumstances of low fidelity, where most descendants contain one or more mutations. A group of such genotypes is constantly changing, so discussions of which single genotype is the most fit become meaningless. Importantly, if many closely related genotypes are only one mutation away from each other, then genotypes in the group can mutate back and forth into each other. For example, with one mutation per generation, a child of the sequence AGGT could be AGTT, and a grandchild could be AGGT again. Thus we can envision a "cloud" of related genotypes that is rapidly mutating, with sequences going back and forth among different points in the cloud. Though the proper definition is mathematical, that cloud, roughly speaking, is a quasispecies.

Quasispecies behavior exists for large numbers of individuals existing at a certain (high) range of mutation rates.

In a species, though reproduction may be mostly accurate, periodic mutations will give rise to one or more competing genotypes. If a mutation results in greater replication and survival, the mutant genotype may out-compete the parent genotype and come to dominate the species. Thus, the individual genotypes (or species) may be seen as the units on which selection acts and biologists will often speak of a single genotype's fitness.

In a quasispecies, however, mutations are ubiquitous and so the fitness of an individual genotype becomes meaningless: if one particular mutation generates a boost in reproductive success, it can't amount to much because that genotype's offspring are unlikely to be accurate copies with the same properties. Instead, what matters is the "connectedness" of the cloud. For example, the sequence AGGT has 12 (3+3+3+3) possible single point mutants AGGA, AGGG, and so on. If 10 of those mutants are viable genotypes that may reproduce (and some of whose offspring or grandchildren may mutate back into AGGT again), we would consider that sequence a well-connected node in the cloud. If instead only two of those mutants are viable, the rest being lethal mutations, then that sequence is poorly connected and most of its descendants will not reproduce. The analog of fitness for a quasispecies is the tendency of nearby relatives within the cloud to be well-connected, meaning that more of the mutant descendants will be viable and give rise to further descendants within the cloud.

When the fitness of a single genotype becomes meaningless because of the high rate of mutations, the cloud as a whole or quasispecies becomes the natural unit of selection.

Quasispecies represents the evolution of high-mutation-rate viruses such as HIV and sometimes single genes or molecules within the genomes of other organisms. Quasispecies models have also been proposed by Jose Fontanari and Emmanuel David Tannenbaum to model the evolution of sexual reproduction. Quasispecies was also shown in compositional replicators (based on the Gard model for abiogenesis) and was also suggested to be applicable to describe cell's replication, which amongst other things requires the maintenance and evolution of the internal composition of the parent and bud.

The model rests on four assumptions:

In the quasispecies model, mutations occur through errors made in the process of copying already existing sequences. Further, selection arises because different types of sequences tend to replicate at different rates, which leads to the suppression of sequences that replicate more slowly in favor of sequences that replicate faster. However, the quasispecies model does not predict the ultimate extinction of all but the fastest replicating sequence. Although the sequences that replicate more slowly cannot sustain their abundance level by themselves, they are constantly replenished as sequences that replicate faster mutate into them. At equilibrium, removal of slowly replicating sequences due to decay or outflow is balanced by replenishing, so that even relatively slowly replicating sequences can remain present in finite abundance.

Due to the ongoing production of mutant sequences, selection does not act on single sequences, but on mutational "clouds" of closely related sequences, referred to as "quasispecies". In other words, the evolutionary success of a particular sequence depends not only on its own replication rate, but also on the replication rates of the mutant sequences it produces, and on the replication rates of the sequences of which it is a mutant. As a consequence, the sequence that replicates fastest may even disappear completely in selection-mutation equilibrium, in favor of more slowly replicating sequences that are part of a quasispecies with a higher average growth rate. Mutational clouds as predicted by the quasispecies model have been observed in RNA viruses and in "in vitro" RNA replication.

The mutation rate and the general fitness of the molecular sequences and their neighbors is crucial to the formation of a quasispecies. If the mutation rate is zero, there is no exchange by mutation, and each sequence is its own species. If the mutation rate is too high, exceeding what is known as the error threshold, the quasispecies will break down and be dispersed over the entire range of available sequences.

A simple mathematical model for a quasispecies is as follows: let there be formula_1 possible sequences and let there be formula_2 organisms with sequence "i". Let's say that each of these organisms asexually gives rise to formula_3 offspring. Some are duplicates of their parent, having sequence "i", but some are mutant and have some other sequence. Let the mutation rate formula_4 correspond to the probability that a "j" type parent will produce an "i" type organism. Then the expected fraction of offspring generated by "j" type organisms that would be "i" type organisms is formula_5,

where formula_6.

Then the total number of "i"-type organisms after the first round of reproduction, given as formula_7, is

Sometimes a death rate term formula_9 is included so that:

where formula_11 is equal to 1 when i=j and is zero otherwise. Note that the "n-th" generation can be found by just taking the "n-th" power of W substituting it in place of W in the above formula.

This is just a system of linear equations. The usual way to solve such a system is to first diagonalize the W matrix. Its diagonal entries will be eigenvalues corresponding to certain linear combinations of certain subsets of sequences which will be eigenvectors of the W matrix. These subsets of sequences are the quasispecies. Assuming that the matrix W is a primitive matrix (irreducible and aperiodic), then after very many generations only the eigenvector with the largest eigenvalue will prevail, and it is this quasispecies that will eventually dominate. The components of this eigenvector give the relative abundance of each sequence at equilibrium.

W being primitive means that for some integer formula_12, that the formula_13 power of W is > 0, i.e. all the entries are positive. If W is primitive then each type can, through a sequence of mutations (i.e. powers of W) mutate into all the other types after some number of generations. W is not primitive if it is periodic, where the population can perpetually cycle through different disjoint sets of compositions, or if it is reducible, where the dominant species (or quasispecies) that develops can depend on the initial population, as is the case in the simple example given below.

The quasispecies formulae may be expressed as a set of linear differential equations. If we consider the difference between the new state formula_7 and the old state formula_2 to be the state change over one moment of time, then we can state that the time derivative of formula_2 is given by this difference, formula_17 we can write:

The quasispecies equations are usually expressed in terms of concentrations formula_19 where

The above equations for the quasispecies then become for the discrete version:

or, for the continuum version:

The quasispecies concept can be illustrated by a simple system consisting of 4 sequences. Sequences [0,0], [0,1], [1,0], and [1,1] are numbered 1, 2, 3, and 4, respectively. Let's say the [0,0] sequence never mutates and always produces a single offspring. Let's say the other 3 sequences all produce, on average, formula_24 replicas of themselves, and formula_25 of each of the other two types, where formula_26. The W matrix is then:

The diagonalized matrix is:

And the eigenvectors corresponding to these eigenvalues are:

Only the eigenvalue formula_29 is more than unity. For the n-th generation, the corresponding eigenvalue will be formula_30 and so will increase without bound as time goes by. This eigenvalue corresponds to the eigenvector [0,1,1,1], which represents the quasispecies consisting of sequences 2, 3, and 4, which will be present in equal numbers after a very long time. Since all population numbers must be positive, the first two quasispecies are not legitimate. The third quasispecies consists of only the non-mutating sequence 1. It's seen that even though sequence 1 is the most fit in the sense that it reproduces more of itself than any other sequence, the quasispecies consisting of the other three sequences will eventually dominate (assuming that the initial population was not homogeneous of the sequence 1 type).



</doc>
<doc id="25310" url="https://en.wikipedia.org/wiki?curid=25310" title="Qing dynasty">
Qing dynasty

The Qing dynasty, officially the Great Qing () or commonly known as China, was the last imperial dynasty of China. It was established in 1636, and ruled China proper from 1644 to 1912. It was preceded by the Ming dynasty and succeeded by the Republic of China. The multiethnic Qing empire lasted for almost three centuries and formed the territorial base for modern China. It was the fourth largest empire in world history in terms of territorial size.

The dynasty was founded by the Manchu Aisin Gioro clan in Manchuria. In the late sixteenth century, Nurhaci, originally a Ming vassal, began organizing "Banners" which were military-social units that included Manchu, Han, and Mongol elements. Nurhaci united Manchu clans and officially proclaimed the Later Jin dynasty in 1616. His son Hong Taiji began driving Ming forces out of the Liaodong Peninsula and declared a new dynasty, the Qing, in 1636. As Ming control disintegrated, peasant rebels led by Li Zicheng conquered the capital Beijing in 1644. Ming general Wu Sangui refused to serve them, but opened the Shanhai Pass to the Banner Armies led by the regent Prince Dorgon, who defeated the rebels and seized the capital. Dorgon served as Prince Regent under the Shunzhi Emperor. Resistance from the Ming loyalists in the south and the Revolt of the Three Feudatories led by Wu Sangui delayed the complete conquest until 1683 under the Kangxi Emperor (1661–1722). The Ten Great Campaigns of the Qianlong Emperor from the 1750s to the 1790s extended Qing control into Inner Asia. During the peak of the Qing dynasty, the empire ruled over the entirety of today's Mainland China, Hainan, Taiwan, Mongolia, Outer Manchuria and Outer Northwest China. The early Qing rulers maintained their Manchu customs, they were patrons of Tibetan Buddhism, and while their title was Emperor, used "Bogd khaan" when dealing with the Mongols. They governed using a Confucian style and bureaucratic institutions, retaining the imperial examinations to recruit Han Chinese to work under or in parallel with the Manchus. They also adapted the ideals of the Chinese tributary system in asserting superiority over peripheral countries such as Korea and Vietnam, while annexing neighboring territories such as Tibet and Mongolia.

The dynasty reached its high point in the late 18th century, then gradually declined in the face of challenges from abroad, internal revolts, population growth, disruption of the economy, corruption, and the reluctance of ruling elites to change their mindsets. The population rose to some 400 million, but taxes and government revenues were fixed at a low rate, leading to fiscal crisis. Following the Opium Wars, European powers led by Great Britain imposed "unequal treaties", free trade, extraterritoriality and treaty ports under foreign control. The Taiping Rebellion (1850–1864) and the Dungan Revolt (1862–1877) in Central Asia led to the death of some 20 million people, due to famine, disease, and war. In spite of these disasters, in the Tongzhi Restoration of the 1860s, Han Chinese elites rallied to the defense of the Confucian order and the Manchu rulers. The initial gains in the Self-Strengthening Movement were lost in the First Sino-Japanese War of 1895, in which the Qing lost its influence over Korea and the possession of Taiwan. New Armies were organized, but the ambitious Hundred Days' Reform of 1898 was turned back in a coup by the conservative Empress Dowager Cixi (1835–1908), who was the dominant voice in the national government (with one interruption) after 1861. When the Scramble for Concessions by foreign powers triggered the violently anti-foreign "Boxers" in 1900, with many foreigners and Christians killed, the foreign powers invaded China. Cixi sided with the Boxers and was decisively defeated by the eight invading powers, leading to the flight of the Imperial Court to Xi'an.

After agreeing to sign the Boxer Protocol, the government initiated unprecedented fiscal and administrative reforms, including elections, a new legal code, and abolition of the examination system. Sun Yat-sen and other revolutionaries competed with constitutional monarchists such as Kang Youwei and Liang Qichao to transform the Qing Empire into a modern nation. After the deaths of the Guangxu Emperor and Cixi in 1908, the hardline Manchu court alienated reformers and local elites alike by obstructing social reform. The Wuchang Uprising on 11 October 1911 led to the Xinhai Revolution. General Yuan Shikai negotiated the abdication of Puyi, the last emperor, on 12 February 1912, bringing the dynasty to an end.

Nurhaci declared himself the "Bright Khan" of the "Jin" (lit. "gold"; known in Chinese historiography as the "Later Jin") state in honor both of the 12th–13th century Jurchen-led Jin dynasty and of his Aisin Gioro clan ("Aisin" being Manchu for the Chinese ("jīn", "gold")). His son Hong Taiji renamed the dynasty "Great Qing" in 1636. There are competing explanations on the meaning of "Qīng" (lit. "clear" or "pure"). The name may have been selected in reaction to the name of the Ming dynasty (), which consists of the Chinese characters for "sun" () and "moon" (), both associated with the fire element of the Chinese zodiacal system. The character "Qīng" () is composed of "water" () and "azure" (), both associated with the water element. This association would justify the Qing conquest as defeat of fire by water. The water imagery of the new name may also have had Buddhist overtones of perspicacity and enlightenment and connections with the Bodhisattva Manjusri. The Manchu name "daicing", which sounds like a phonetic rendering of "Dà Qīng" or "Dai Ching", may in fact have been derived from a Mongolian word ", дайчин" that means "warrior". "Daicing gurun" may therefore have meant "warrior state", a pun that was only intelligible to Manchu and Mongol people. In the later part of the dynasty, however, even the Manchus themselves had forgotten this possible meaning.

After conquering "China proper", the Manchus identified their state as "China" (中國, "Zhōngguó"; "Middle Kingdom"), and referred to it as "Dulimbai Gurun" in Manchu ("Dulimbai" means "central" or "middle," "gurun" means "nation" or "state"). The emperors equated the lands of the Qing state (including present-day Northeast China, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, and rejecting the idea that "China" only meant Han areas. The Qing emperors proclaimed that both Han and non-Han peoples were part of "China". They used both "China" and "Qing" to refer to their state in official documents, international treaties (as the Qing was known internationally as "China" or the "Chinese Empire") and foreign affairs, and "Chinese language" (Manchu: "Dulimbai gurun i bithe") included Chinese, Manchu, and Mongol languages, and "Chinese people" (中國之人 "Zhōngguó zhī rén"; Manchu: "Dulimbai gurun i niyalma") referred to all subjects of the empire. In the Chinese-language versions of its treaties and its maps of the world, the Qing government used "Qing" and "China" interchangeably.

The Qing dynasty was founded not by Han Chinese, who constitute the majority of the Chinese population, but by the Manchu, descendants of a sedentary farming people known as the Jurchen, a Tungusic people who lived around the region now comprising the Chinese provinces of Jilin and Heilongjiang. The Manchus are sometimes mistaken for a nomadic people, which they were not.

What was to become the Manchu state was founded by Nurhaci, the chieftain of a minor Jurchen tribethe Aisin Gioroin Jianzhou in the early 17th century. Nurhaci may have spent time in a Chinese household in his youth, and became fluent in Chinese as well as Mongol, and read the Chinese novels Romance of the Three Kingdoms and Water Margin. Originally a vassal of the Ming emperors, Nurhaci embarked on an intertribal feud in 1582 that escalated into a campaign to unify the nearby tribes. By 1616, he had sufficiently consolidated Jianzhou so as to be able to proclaim himself Khan of the Great Jin in reference to the previous Jurchen dynasty.

Two years later, Nurhaci announced the "Seven Grievances" and openly renounced the sovereignty of Ming overlordship in order to complete the unification of those Jurchen tribes still allied with the Ming emperor. After a series of successful battles, he relocated his capital from Hetu Ala to successively bigger captured Ming cities in Liaodong: first Liaoyang in 1621, then Shenyang (Manchu: Mukden) in 1625.

When the Jurchens were reorganized by Nurhaci into the Eight Banners, many Manchu clans were artificially created as a group of unrelated people founded a new Manchu clan (Manchu: "mukūn") using a geographic origin name such as a toponym for their "hala" (clan name). The irregularities over Jurchen and Manchu clan origin led the Qing to document and systematize the creation of histories for Manchu clans, including manufacturing an entire legend around the origin of the Aisin Gioro clan by taking mythology from the northeast.

Relocating his court from Jianzhou to Liaodong provided Nurhaci access to more resources; it also brought him in close contact with the Khorchin Mongol domains on the plains of Mongolia. Although by this time the once-united Mongol nation had long since fragmented into individual and hostile tribes, these tribes still presented a serious security threat to the Ming borders. Nurhaci's policy towards the Khorchins was to seek their friendship and cooperation against the Ming, securing his western border from a powerful potential enemy.

Furthermore, the Khorchin proved a useful ally in the war, lending the Jurchens their expertise as cavalry archers. To guarantee this new alliance, Nurhaci initiated a policy of inter-marriages between the Jurchen and Khorchin nobilities, while those who resisted were met with military action. This is a typical example of Nurhaci's initiatives that eventually became official Qing government policy. During most of the Qing period, the Mongols gave military assistance to the Manchus.

Some other important contributions by Nurhaci include ordering the creation of a written Manchu script, based on Mongolian script, after the earlier Jurchen script was forgotten (it had been derived from Khitan and Chinese). Nurhaci also created the civil and military administrative system that eventually evolved into the Eight Banners, the defining element of Manchu identity and the foundation for transforming the loosely-knitted Jurchen tribes into a single nation.

There were too few ethnic Manchus to conquer China proper, so they gained strength by defeating and absorbing Mongols. More importantly, they added Han Chinese to the Eight Banners. The Manchus had to create an entire "Jiu Han jun" (Old Han Army) due to the massive number of Han Chinese soldiers who were absorbed into the Eight Banners by both capture and defection. Ming artillery was responsible for many victories against the Manchus, so the Manchus established an artillery corps made out of Han Chinese soldiers in 1641, and the swelling of Han Chinese numbers in the Eight Banners led in 1642 to all Eight Han Banners being created. Armies of defected Ming Han Chinese conquered southern China for the Qing.

Han Chinese played a massive role in the Qing conquest of China. Han Chinese Generals who defected to the Manchu were often given women from the Imperial Aisin Gioro family in marriage while the ordinary soldiers who surrendered were often given non-royal Manchu women as wives. Jurchen (Manchu) women married Han Chinese in Liaodong. Manchu Aisin Gioro princesses were also given in marriage to Han Chinese officials' sons.

The unbroken series of Nurhaci's military successes ended in January 1626 when he was defeated by Yuan Chonghuan while laying siege to Ningyuan. He died a few months later and was succeeded by his eighth son, Hong Taiji, who emerged as the new Khan after a short political struggle amongst other contenders . Although Hong Taiji was an experienced leader and the commander of two Banners at the time of his succession, his reign did not start well on the military front. The Jurchens suffered yet another defeat in 1627 at the hands of Yuan Chonghuan. This defeat was also in part due to the Ming's newly acquired Portuguese cannons.

To redress the technological and numerical disparity, Hong Taiji created his own artillery corps in 1634, the "ujen cooha" (Chinese: ) from his existing Han troops who cast their own cannons in the European design with the help of defector Chinese metallurgists. One of the defining events of Hong Taiji's reign was the official adoption of the name "Manchu" for the united Jurchen people in November 1635. In 1635, the Manchus' Mongol allies were fully incorporated into a separate Banner hierarchy under direct Manchu command. Hong Taiji conquered the territory north of Shanhai Pass by Ming dynasty and Ligdan Khan in Inner Mongolia. In April 1636, Mongol nobility of Inner Mongolia, Manchu nobility and the Han mandarin held the Kurultai in Shenyang, and recommended the khan of Later Jin to be the emperor of the Great Qing empire. One of the Yuan Dynasty's jade seal was also dedicated to the emperor (Bogd Setsen Khan) by the nobility. When he was presented with the imperial seal of the Yuan dynasty after the defeat of the last Khagan of the Mongols, Hong Taiji renamed his state from "Great Jin" to "Great Qing" and elevated his position from Khan to Emperor, suggesting imperial ambitions beyond unifying the Manchu territories. Hong Taiji then proceeded to invade Korea again in 1636.

The change of name from Jurchen to Manchu was made to hide the fact that the ancestors of the Manchus, the Jianzhou Jurchens, were ruled by the Chinese. The Qing dynasty carefully hid the original editions of the books of ""Qing Taizu Wu Huangdi Shilu"" and the ""Manzhou Shilu Tu"" (Taizu Shilu Tu) in the Qing palace, forbidden from public view because they showed that the Manchu Aisin Gioro family had been ruled by the Ming dynasty and followed many Manchu customs that seemed "uncivilized" to later observers. The Qing also deliberately excluded references and information that showed the Jurchens (Manchus) as subservient to the Ming dynasty, from the History of Ming to hide their former subservient relationship to the Ming. The Veritable Records of Ming were not used to source content on Jurchens during Ming rule in the History of Ming because of this.

In the Ming period, the Koreans of Joseon referred to the Jurchen-inhabited lands north of the Korean peninsula, above the rivers Yalu and Tumen to be part of Ming China, as the "superior country" (sangguk) which they called Ming China. After the Second Manchu invasion of Korea, Joseon Korea was forced to give several of their royal princesses as concubines to the Qing Manchu regent Prince Dorgon. In 1650, Dorgon married the Korean Princess Uisun.

This was followed by the creation of the first two Han Banners in 1637 (increasing to eight in 1642). Together these military reforms enabled Hong Taiji to resoundingly defeat Ming forces in a series of battles from 1640 to 1642 for the territories of Songshan and Jinzhou. This final victory resulted in the surrender of many of the Ming dynasty's most battle-hardened troops, the death of Yuan Chonghuan at the hands of the Chongzhen Emperor (who thought Yuan had betrayed him), and the complete and permanent withdrawal of the remaining Ming forces north of the Great Wall.
Meanwhile, Hong Taiji set up a rudimentary bureaucratic system based on the Ming model. He established six boards or executive level ministries in 1631 to oversee finance, personnel, rites, military, punishments, and public works. However, these administrative organs had very little role initially, and it was not until the eve of completing the conquest ten years later that they fulfilled their government roles.

Hong Taiji's bureaucracy was staffed with many Han Chinese, including many newly surrendered Ming officials. The Manchus' continued dominance was ensured by an ethnic quota for top bureaucratic appointments. Hong Taiji's reign also saw a fundamental change of policy towards his Han Chinese subjects. Nurhaci had treated Han in Liaodong differently according to how much grain they had: those with less than 5 to 7 sin were treated badly, while those with more than that amount were rewarded with property. Due to a revolt by Han in Liaodong in 1623, Nurhaci, who previously gave concessions to conquered Han subjects in Liaodong, turned against them and ordered that they no longer be trusted. He enacted discriminatory policies and killings against them, while ordering that Han who assimilated to the Jurchen (in Jilin) before 1619 be treated equally, as Jurchens were, and not like the conquered Han in Liaodong. Hong Taiji recognized that the Manchus needed to attract Han Chinese, explaining to reluctant Manchus why he needed to treat the Ming defector General Hong Chengchou leniently. Hong Taiji instead incorporated them into the Jurchen "nation" as full (if not first-class) citizens, obligated to provide military service. By 1648, less than one-sixth of the bannermen were of Manchu ancestry. This change of policy not only increased Hong Taiji's manpower and reduced his military dependence on banners not under his personal control, it also greatly encouraged other Han Chinese subjects of the Ming dynasty to surrender and accept Jurchen rule when they were defeated militarily. Through these and other measures Hong Taiji was able to centralize power unto the office of the Khan, which in the long run prevented the Jurchen federation from fragmenting after his death.

Hong Taiji died suddenly in September 1643. As the Jurchens had traditionally "elected" their leader through a council of nobles, the Qing state did not have a clear succession system. The leading contenders for power were Hong Taiji's oldest son Hooge and Hong Taiji's half brother Dorgon. A compromise installed Hong Taiji's five-year-old son, Fulin, as the Shunzhi Emperor, with Dorgon as regent and de facto leader of the Manchu nation.

Meanwhile, Ming government officials fought against each other, against fiscal collapse, and against a series of peasant rebellions. They were unable to capitalise on the Manchu succession dispute and the presence of a minor as emperor. In April 1644, the capital, Beijing, was sacked by a coalition of rebel forces led by Li Zicheng, a former minor Ming official, who established a short-lived Shun dynasty. The last Ming ruler, the Chongzhen Emperor, committed suicide when the city fell to the rebels, marking the official end of the dynasty.

Li Zicheng then led a collection of rebel forces numbering some 200,000 to confront Wu Sangui, the general commanding the Ming garrison at Shanhai Pass, a key pass of the Great Wall, located fifty miles northeast of Beijing, which defended the capital. Wu Sangui, caught between a rebel army twice his size and an enemy he had fought for years, cast his lot with the foreign but familiar Manchus. Wu Sangui may have been influenced by Li Zicheng's mistreatment of wealthy and cultured officials, including Li's own family; it was said that Li took Wu's concubine Chen Yuanyuan for himself. Wu and Dorgon allied in the name of avenging the death of the Chongzhen Emperor. Together, the two former enemies met and defeated Li Zicheng's rebel forces in battle on May 27, 1644.

The newly allied armies captured Beijing on 6 June. The Shunzhi Emperor was invested as the "Son of Heaven" on 30 October. The Manchus, who had positioned themselves as political heirs to the Ming emperor by defeating Li Zicheng, completed the symbolic transition by holding a formal funeral for the Chongzhen Emperor. However, conquering the rest of China Proper took another seventeen years of battling Ming loyalists, pretenders and rebels. The last Ming pretender, Prince Gui, sought refuge with the King of Burma, Pindale Min, but was turned over to a Qing expeditionary army commanded by Wu Sangui, who had him brought back to Yunnan province and executed in early 1662.

The Qing had taken shrewd advantage of Ming civilian government discrimination against the military and encouraged the Ming military to defect by spreading the message that the Manchus valued their skills. Banners made up of Han Chinese who defected before 1644 were classed among the Eight Banners, giving them social and legal privileges in addition to being acculturated to Manchu traditions. Han defectors swelled the ranks of the Eight Banners so greatly that ethnic Manchus became a minority—only 16% in 1648, with Han Bannermen dominating at 75% and Mongol Bannermen making up the rest. Gunpowder weapons like muskets and artillery were wielded by the Chinese Banners. Normally, Han Chinese defector troops were deployed as the vanguard, while Manchu Bannermen acted as reserve forces or in the rear and were used predominantly for quick strikes with maximum impact, so as to minimize ethnic Manchu losses.

This multi-ethnic force conquered China for the Qing, The three Liaodong Han Bannermen officers who played key roles in the conquest of southern China were Shang Kexi, Geng Zhongming, and Kong Youde, who governed southern China autonomously as viceroys for the Qing after the conquest. Han Chinese Bannermen made up the majority of governors in the early Qing, and they governed and administered China after the conquest, stabilizing Qing rule. Han Bannermen dominated the post of governor-general in the time of the Shunzhi and Kangxi Emperors, and also the post of governor, largely excluding ordinary Han civilians from these posts.

To promote ethnic harmony, a 1648 decree allowed Han Chinese civilian men to marry Manchu women from the Banners with the permission of the Board of Revenue if they were registered daughters of officials or commoners, or with the permission of their banner company captain if they were unregistered commoners. Later in the dynasty the policies allowing intermarriage were done away with.

The southern cadet branch of Confucius' descendants who held the title "Wujing boshi" (Doctor of the Five Classics) and 65th generation descendant in the northern branch who held the title Duke Yansheng both had their titles confirmed by the Shunzhi Emperor upon the Qing entry into Beijing on 31 October. The Kong's title of Duke was maintained in later reigns.

The first seven years of the Shunzhi Emperor's reign were dominated by the regent prince Dorgon. Because of his own political insecurity, Dorgon followed Hong Taiji's example by ruling in the name of the emperor at the expense of rival Manchu princes, many of whom he demoted or imprisoned under one pretext or another. Although the period of his regency was relatively short, Dorgon's precedents and example cast a long shadow over the dynasty.

First, the Manchus had entered "South of the Wall" because Dorgon responded decisively to Wu Sangui's appeal. Then, after capturing Beijing, instead of sacking the city as the rebels had done, Dorgon insisted, over the protests of other Manchu princes, on making it the dynastic capital and reappointing most Ming officials. Choosing Beijing as the capital had not been a straightforward decision, since no major Chinese dynasty had directly taken over its immediate predecessor's capital. Keeping the Ming capital and bureaucracy intact helped quickly stabilize the regime and sped up the conquest of the rest of the country. Dorgon then drastically reduced the influence of the eunuchs, a major force in the Ming bureaucracy, and directed Manchu women not to bind their feet in the Chinese style.

However, not all of Dorgon's policies were equally popular or as easy to implement. The controversial July 1645 edict (the "haircutting order") forced adult Han Chinese men to shave the front of their heads and comb the remaining hair into the queue hairstyle which was worn by Manchu men, on pain of death. The popular description of the order was: "To keep the hair, you lose the head; To keep your head, you cut the hair." To the Manchus, this policy was a test of loyalty and an aid in distinguishing friend from foe. For the Han Chinese, however, it was a humiliating reminder of Qing authority that challenged traditional Confucian values. The "Classic of Filial Piety" ("Xiaojing") held that "a person's body and hair, being gifts from one's parents, are not to be damaged". Under the Ming dynasty, adult men did not cut their hair but instead wore it in the form of a top-knot. The order triggered strong resistance to Qing rule in Jiangnan and massive killing of Han Chinese. It was Han Chinese defectors who carried out massacres against people refusing to wear the queue. Li Chengdong, a Han Chinese general who had served the Ming but surrendered to the Qing, ordered his Han troops to carry out three separate massacres in the city of Jiading within a month, resulting in tens of thousands of deaths. At the end of the third massacre, there was hardly a living person left in this city. Jiangyin also held out against about 10,000 Han Chinese Qing troops for 83 days. When the city wall was finally breached on 9 October 1645, the Han Chinese Qing army led by the Han Chinese Ming defector Liu Liangzuo (劉良佐), who had been ordered to "fill the city with corpses before you sheathe your swords", massacred the entire population, killing between 74,000 and 100,000 people. 

Han Chinese did not object to wearing the queue braid on the back of the head, as they traditionally wore all their hair long, but fiercely objected to shaving the forehead, which the Qing government focused on. Han rebels in the first half of the Qing wore the braid but defied orders to shave the front of the head. One person was executed for refusing to shave the front but he had willingly braided the back of his hair. Later westernized revolutionaries, influenced by western hairstyle began to view the braid as backward and advocated adopting short haired western hairstyles. Han rebels, such as the Taiping, even retained their queue braids but grew hair on the front of the head. The Qing government accordingly viewed shaving the front of the head as the primary sign of loyalty, rather than the braid on the back, which traditional Han did not object to. Koxinga insulted and criticized the Qing hairstyle by referring to the shaven pate as looking like a fly. Koxinga and his men objected when the Qing demanded they shave in exchange for recognizing Koxinga as a feudatory. The Qing demanded that Zheng Jing and his men on Taiwan shave in order to receive recognition as a fiefdom. His men and Ming prince Zhu Shugui fiercely objected to shaving.

On 31 December 1650, Dorgon suddenly died during a hunting expedition, marking the official start of the Shunzhi Emperor's personal rule. Because the emperor was only 12 years old at that time, most decisions were made on his behalf by his mother, Empress Dowager Xiaozhuang, who turned out to be a skilled political operator.

Although his support had been essential to Shunzhi's ascent, Dorgon had centralised so much power in his hands as to become a direct threat to the throne. So much so that upon his death he was bestowed the extraordinary posthumous title of Emperor Yi (), the only instance in Qing history in which a Manchu "prince of the blood" () was so honored. Two months into Shunzhi's personal rule, however, Dorgon was not only stripped of his titles, but his corpse was disinterred and mutilated. to atone for multiple "crimes", one of which was persecuting to death Shunzhi's agnate eldest brother, Hooge. More importantly, Dorgon's symbolic fall from grace also led to the purge of his family and associates at court, thus reverting power back to the person of the emperor. After a promising start, Shunzhi's reign was cut short by his early death in 1661 at the age of 24 from smallpox. He was succeeded by his third son Xuanye, who reigned as the Kangxi Emperor.

The Manchus sent Han Bannermen to fight against Koxinga's Ming loyalists in Fujian. They removed the population from coastal areas in order to deprive Koxinga's Ming loyalists of resources. This led to a misunderstanding that Manchus were "afraid of water". Han Bannermen carried out the fighting and killing, casting doubt on the claim that fear of the water led to the coastal evacuation and ban on maritime activities. Even though a poem refers to the soldiers carrying out massacres in Fujian as "barbarians", both Han Green Standard Army and Han Bannermen were involved and carried out the worst slaughter. 400,000 Green Standard Army soldiers were used against the Three Feudatories in addition to the 200,000 Bannermen.

The sixty-one year reign of the Kangxi Emperor was the longest of any Chinese emperor. Kangxi's reign is also celebrated as the beginning of an era known as the "High Qing", during which the dynasty reached the zenith of its social, economic and military power. Kangxi's long reign started when he was eight years old upon the untimely demise of his father. To prevent a repeat of Dorgon's dictatorial monopolizing of power during the regency, the Shunzhi Emperor, on his deathbed, hastily appointed four senior cabinet ministers to govern on behalf of his young son. The four ministers – Sonin, Ebilun, Suksaha, and Oboi – were chosen for their long service, but also to counteract each other's influences. Most important, the four were not closely related to the imperial family and laid no claim to the throne. However, as time passed, through chance and machination, Oboi, the most junior of the four, achieved such political dominance as to be a potential threat. Even though Oboi's loyalty was never an issue, his personal arrogance and political conservatism led him into an escalating conflict with the young emperor. In 1669 Kangxi, through trickery, disarmed and imprisoned Oboi – a significant victory for a fifteen-year-old emperor over a wily politician and experienced commander.

The early Manchu rulers established two foundations of legitimacy that help to explain the stability of their dynasty. The first was the bureaucratic institutions and the neo-Confucian culture that they adopted from earlier dynasties. Manchu rulers and Han Chinese scholar-official elites gradually came to terms with each other. The examination system offered a path for ethnic Han to become officials. Imperial patronage of Kangxi Dictionary demonstrated respect for Confucian learning, while the Sacred Edict of 1670 effectively extolled Confucian family values. His attempts to discourage Chinese women from foot binding, however, were unsuccessful.

The second major source of stability was the Central Asian aspect of their Manchu identity, which allowed them to appeal to Mongol, Tibetan and Uighur constituents. The ways of the Qing legitimization were different for the Chinese, Mongolian and Tibetan peoples. This contradicted traditional Chinese worldview requiring acculturation of "barbarians". Qing emperors, on the contrary, sought to prevent this in regard to Mongols and Tibetans. The Qing used the title of Emperor (Huangdi) in Chinese, while among Mongols the Qing monarch was referred to as Bogda khan (wise Khan), and referred to as Gong Ma in Tibet. The Qianlong Emperor propagated the image of himself as a Buddhist sage ruler, a patron of Tibetan Buddhism. In the Manchu language, the Qing monarch was alternately referred to as either Huwangdi (Emperor) or Khan with no special distinction between the two usages. The Kangxi Emperor also welcomed to his court Jesuit missionaries, who had first come to China under the Ming. Missionaries including Tomás Pereira, Martino Martini, Johann Adam Schall von Bell, Ferdinand Verbiest and Antoine Thomas held significant positions as military weapons experts, mathematicians, cartographers, astronomers and advisers to the emperor. The relationship of trust was however lost in the later Chinese Rites controversy.

Yet controlling the "Mandate of Heaven" was a daunting task. The vastness of China's territory meant that there were only enough banner troops to garrison key cities forming the backbone of a defense network that relied heavily on surrendered Ming soldiers. In addition, three surrendered Ming generals were singled out for their contributions to the establishment of the Qing dynasty, ennobled as feudal princes (藩王), and given governorships over vast territories in Southern China. The chief of these was Wu Sangui, who was given the provinces of Yunnan and Guizhou, while generals Shang Kexi and Geng Jingzhong were given Guangdong and Fujian provinces respectively.

As the years went by, the three feudal lords and their extensive territories became increasingly autonomous. Finally, in 1673, Shang Kexi petitioned Kangxi for permission to retire to his hometown in Liaodong province and nominated his son as his successor. The young emperor granted his retirement, but denied the heredity of his fief. In reaction, the two other generals decided to petition for their own retirements to test Kangxi's resolve, thinking that he would not risk offending them. The move backfired as the young emperor called their bluff by accepting their requests and ordering that all three fiefdoms to be reverted to the crown.

Faced with the stripping of their powers, Wu Sangui, later joined by Geng Zhongming and by Shang Kexi's son Shang Zhixin, felt they had no choice but to revolt. The ensuing Revolt of the Three Feudatories lasted for eight years. Wu attempted, ultimately in vain, to fire the embers of south China Ming loyalty by restoring Ming customs but then declared himself emperor of a new dynasty instead of restoring the Ming. At the peak of the rebels' fortunes, they extended their control as far north as the Yangtze River, nearly establishing a divided China. Wu hesitated to go further north, not being able to coordinate strategy with his allies, and Kangxi was able to unify his forces for a counterattack led by a new generation of Manchu generals. By 1681, the Qing government had established control over a ravaged southern China which took several decades to recover.

Manchu Generals and Bannermen were initially put to shame by the better performance of the Han Chinese Green Standard Army. Kangxi accordingly assigned generals Sun Sike, Wang Jinbao, and Zhao Liangdong to crush the rebels, since he thought that Han Chinese were superior to Bannermen at battling other Han people. Similarly, in north-western China against Wang Fuchen, the Qing used Han Chinese Green Standard Army soldiers and Han Chinese generals as the primary military forces. This choice was due to the rocky terrain, which favoured infantry troops over cavalry, to the desire to keep Bannermen in reserve, and, again, to the belief that Han troops were better at fighting other Han people. These Han generals achieved victory over the rebels. Also due to the mountainous terrain, Sichuan and southern Shaanxi were retaken by the Green Standard Army in 1680, with Manchus participating only in logistics and provisions. 400,000 Green Standard Army soldiers and 150,000 Bannermen served on the Qing side during the war. 213 Han Chinese Banner companies, and 527 companies of Mongol and Manchu Banners were mobilized by the Qing during the revolt. 400,000 Green Standard Army soldiers were used against the Three Feudatories besides 200,000 Bannermen.

The Qing forces were crushed by Wu from 1673–1674. The Qing had the support of the majority of Han Chinese soldiers and Han elite against the Three Feudatories, since they refused to join Wu Sangui in the revolt, while the Eight Banners and Manchu officers fared poorly against Wu Sangui, so the Qing responded with using a massive army of more than 900,000 Han Chinese (non-Banner) instead of the Eight Banners, to fight and crush the Three Feudatories. Wu Sangui's forces were crushed by the Green Standard Army, made out of defected Ming soldiers.

To extend and consolidate the dynasty's control in Central Asia, the Kangxi Emperor personally led a series of military campaigns against the Dzungars in Outer Mongolia. The Kangxi Emperor was able to successfully expel Galdan's invading forces from these regions, which were then incorporated into the empire. Galdan was eventually killed in the Dzungar–Qing War. In 1683, Qing forces received the surrender of Formosa (Taiwan) from Zheng Keshuang, grandson of Koxinga, who had conquered Taiwan from the Dutch colonists as a base against the Qing. Zheng Keshuang was awarded the title "Duke Haicheng" (海澄公) and was inducted into the Han Chinese Plain Red Banner of the Eight Banners when he moved to Beijing. Several Ming princes had accompanied Koxinga to Taiwan in 1661–1662, including the Prince of Ningjing Zhu Shugui and Prince Zhu Honghuan (朱弘桓), son of Zhu Yihai, where they lived in the Kingdom of Tungning. The Qing sent the 17 Ming princes still living on Taiwan in 1683 back to mainland China where they spent the rest of their lives in exile since their lives were spared from execution. Winning Taiwan freed Kangxi's forces for series of battles over Albazin, the far eastern outpost of the Tsardom of Russia. Zheng's former soldiers on Taiwan like the rattan shield troops were also inducted into the Eight Banners and used by the Qing against Russian Cossacks at Albazin. The 1689 Treaty of Nerchinsk was China's first formal treaty with a European power and kept the border peaceful for the better part of two centuries. After Galdan's death, his followers, as adherents to Tibetan Buddhism, attempted to control the choice of the next Dalai Lama. Kangxi dispatched two armies to Lhasa, the capital of Tibet, and installed a Dalai Lama sympathetic to the Qing.

By the end of the 17th century, China was at its greatest height of confidence and political control since the Ming dynasty.

The reigns of the Yongzheng Emperor (r. 1723–1735) and his son, the Qianlong Emperor (r. 1735–1796), marked the height of Qing power. During this period, the Qing Empire ruled over 13 million square kilometers of territory. Yet, as the historian Jonathan Spence puts it, the empire by the end of the Qianlong reign was "like the sun at midday". In the midst of "many glories", he writes, "signs of decay and even collapse were becoming apparent".

After the death of the Kangxi Emperor in the winter of 1722, his fourth son, Prince Yong (雍親王), became the Yongzheng Emperor. In the later years of Kangxi's reign, Yongzheng and his brothers had fought, and there were rumours that he had usurped the throne – most of the rumours held that Yongzheng's brother Yingzhen (Kangxi's 14th son) was the real successor of the Kangxi Emperor, and that Yongzheng and his confidant Keduo Long had tampered with the Kangxi's testament on the night when Kangxi died, though there was little evidence for these charges. In fact, his father had trusted him with delicate political issues and discussed state policy with him. When Yongzheng came to power at the age of 45, he felt a sense of urgency about the problems that had accumulated in his father's later years, and he did not need instruction on how to exercise power. In the words of one recent historian, he was "severe, suspicious, and jealous, but extremely capable and resourceful", and in the words of another, he turned out to be an "early modern state-maker of the first order".

Yongzheng moved rapidly. First, he promoted Confucian orthodoxy and reversed what he saw as his father's laxness by cracking down on unorthodox sects and by decapitating an anti-Manchu writer his father had pardoned. In 1723 he outlawed Christianity and expelled Christian missionaries, though some were allowed to remain in the capital. Next, he moved to control the government. He expanded his father's system of Palace Memorials, which brought frank and detailed reports on local conditions directly to the throne without being intercepted by the bureaucracy, and he created a small Grand Council of personal advisors, which eventually grew into the emperor's "de facto" cabinet for the rest of the dynasty. He shrewdly filled key positions with Manchu and Han Chinese officials who depended on his patronage. When he began to realize that the financial crisis was even greater than he had thought, Yongzheng rejected his father's lenient approach to local landowning elites and mounted a campaign to enforce collection of the land tax. The increased revenues were to be used for "money to nourish honesty" among local officials and for local irrigation, schools, roads, and charity. Although these reforms were effective in the north, in the south and lower Yangzi valley, where Kangxi had wooed the elites, there were long established networks of officials and landowners. Yongzheng dispatched experienced Manchu commissioners to penetrate the thickets of falsified land registers and coded account books, but they were met with tricks, passivity, and even violence. The fiscal crisis persisted.

Yongzheng also inherited diplomatic and strategic problems. A team made up entirely of Manchus drew up the Treaty of Kyakhta (1727) to solidify the diplomatic understanding with Russia. In exchange for territory and trading rights, the Qing would have a free hand dealing with the situation in Mongolia. Yongzheng then turned to that situation, where the Zunghars threatened to re-emerge, and to the southwest, where local Miao chieftains resisted Qing expansion. These campaigns drained the treasury but established the emperor's control of the military and military finance.

The Yongzheng Emperor died in 1735. His 24-year-old son, Prince Bao (寶親王), then became the Qianlong Emperor. Qianlong personally led military campaigns near Xinjiang and Mongolia, putting down revolts and uprisings in Sichuan and parts of southern China while expanding control over Tibet.

The Qianlong Emperor launched several ambitious cultural projects, including the compilation of the "Siku Quanshu", or "Complete Repository of the Four Branches of Literature". With a total of over 3,400 books, 79,000 chapters, and 36,304 volumes, the "Siku Quanshu" is the largest collection of books in Chinese history. Nevertheless, Qianlong used Literary Inquisition to silence opposition. The accusation of individuals began with the emperor's own interpretation of the true meaning of the corresponding words. If the emperor decided these were derogatory or cynical towards the dynasty, persecution would begin. Literary inquisition began with isolated cases at the time of Shunzhi and Kangxi, but became a pattern under Qianlong's rule, during which there were 53 cases of literary persecution.

Beneath outward prosperity and imperial confidence, the later years of Qianlong's reign were marked by rampant corruption and neglect. Heshen, the emperor's handsome young favorite, took advantage of the emperor's indulgence to become one of the most corrupt officials in the history of the dynasty. Qianlong's son, the Jiaqing Emperor (r. 1796–1820), eventually forced Heshen to commit suicide.

China also began suffering from mounting overpopulation during this period. Population growth was stagnant for the first half of the 17th century due to civil wars and epidemics, but prosperity and internal stability gradually reversed this trend. The introduction of new crops from the Americas such as the potato and peanut allowed an improved food supply as well, so that the total population of China during the 18th century ballooned from 100 million to 300 million people. Soon all available farmland was used up, forcing peasants to work ever-smaller and more intensely worked plots. The Qianlong Emperor once bemoaned the country's situation by remarking, "The population continues to grow, but the land does not." The only remaining part of the empire that had arable farmland was Manchuria, where the provinces of Jilin and Heilongjiang had been walled off as a Manchu homeland. The emperor decreed for the first time that Han Chinese civilians were forbidden to settle. Mongols were forbidden by the Qing from crossing the borders of their banners, even into other Mongol Banners, and from crossing into neidi (the Han Chinese 18 provinces) and were given serious punishments if they did in order to keep the Mongols divided against each other to benefit the Qing. Mongol pilgrims wanting to leave their banner's borders for religious reasons such as pilgrimage had to apply for passports to give them permission.

Select groups of Han Chinese bannermen were mass transferred into Manchu Banners by the Qing, changing their ethnicity from Han Chinese to Manchu. Han Chinese bannermen of Tai Nikan 台尼堪 (watchpost Chinese) and Fusi Nikan 抚顺尼堪 (Fushun Chinese) backgrounds into the Manchu banners in 1740 by order of the Qing Qianlong emperor. It was between 1618–1629 when the Han Chinese from Liaodong who later became the Fushun Nikan and Tai Nikan defected to the Jurchens (Manchus). These Han Chinese origin Manchu clans continue to use their original Han surnames and are marked as of Han origin on Qing lists of Manchu clans.

Despite officially prohibiting Han Chinese settlement on the Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia. Han Chinese then streamed into Manchuria, both illegally and legally, over the Great Wall and Willow Palisade. As Manchu landlords desired Han Chinese to rent their land and grow grain, most Han Chinese migrants were not evicted. During the eighteenth century Han Chinese farmed 500,000 hectares of privately owned land in Manchuria and 203,583 hectares of lands that were part of courrier stations, noble estates, and Banner lands. In garrisons and towns in Manchuria Han Chinese made up 80% of the population.

In 1796, open rebellion broke out by the White Lotus Society against the Qing government. The White Lotus Rebellion continued for eight years, until 1804, and marked a turning point in the history of the Qing dynasty.

At the start of the dynasty, the Chinese empire continued to be the hegemonic power in East Asia. Although there was no formal ministry of foreign relations, the Lifan Yuan was responsible for relations with the Mongol and Tibetans in Central Asia, while the tributary system, a loose set of institutions and customs taken over from the Ming, in theory governed relations with East and Southeast Asian countries. The Treaty of Nerchinsk (1689) stabilized relations with Czarist Russia.

In the Jahriyya revolt sectarian violence between two suborders of the Naqshbandi Sufis, the Jahriyya Sufi Muslims and their rivals, the Khafiyya Sufi Muslims, led to a Jahriyya Sufi Muslim rebellion which the Qing dynasty in China crushed with the help of the Khafiyya Sufi Muslims. The Eight Trigrams uprising of 1813 broke out in 1813.

However, during the 18th century European empires gradually expanded across the world, as European states developed economies built on maritime trade. The dynasty was confronted with newly developing concepts of the international system and state to state relations. European trading posts expanded into territorial control in nearby India and on the islands that are now Indonesia. The Qing response, successful for a time, was to establish the Canton System in 1756, which restricted maritime trade to that city (modern-day Guangzhou) and gave monopoly trading rights to private Chinese merchants. The British East India Company and the Dutch East India Company had long before been granted similar monopoly rights by their governments.

In 1793, the British East India Company, with the support of the British government, sent a delegation to China under Lord George Macartney in order to open free trade and put relations on a basis of equality. The imperial court viewed trade as of secondary interest, whereas the British saw maritime trade as the key to their economy. The Qianlong Emperor told Macartney "the kings of the myriad nations come by land and sea with all sorts of precious things", and "consequently there is nothing we lack ..."

Demand in Europe for Chinese goods such as silk, tea, and ceramics could only be met if European companies funneled their limited supplies of silver into China. In the late 1700s, the governments of Britain and France were deeply concerned about the imbalance of trade and the drain of silver. To meet the growing Chinese demand for opium, the British East India Company greatly expanded its production in Bengal. Since China's economy was essentially self-sufficient, the country had little need to import goods or raw materials from the Europeans, so the usual way of payment was through silver. The Daoguang Emperor, concerned both over the outflow of silver and the damage that opium smoking was causing to his subjects, ordered Lin Zexu to end the opium trade. Lin confiscated the stocks of opium without compensation in 1839, leading Britain to send a military expedition the following year.

The First Opium War revealed the outdated state of the Chinese military. The Qing navy, composed entirely of wooden sailing junks, was severely outclassed by the modern tactics and firepower of the British Royal Navy. British soldiers, using advanced muskets and artillery, easily outmanoeuvred and outgunned Qing forces in ground battles. The Qing surrender in 1842 marked a decisive, humiliating blow to China. The Treaty of Nanjing, the first of the "unequal treaties", demanded war reparations, forced China to open up the Treaty Ports of Canton, Amoy, Fuchow, Ningpo and Shanghai to Western trade and missionaries, and to cede Hong Kong Island to Britain. It revealed weaknesses in the Qing government and provoked rebellions against the regime. In 1842, the Qing dynasty fought a war with the Sikh Empire (the last independent kingdom of India), resulting in a negotiated peace and a return to the "status quo ante bellum".

The Taiping Rebellion in the mid-19th century was the first major instance of anti-Manchu sentiment. Amid widespread social unrest and worsening famine, the rebellion not only posed the most serious threat towards Qing rulers, it has also been called the "bloodiest civil war of all time"; during its fourteen-year course from 1850 to 1864 between 20 and 30 million people died. Hong Xiuquan, a failed civil service candidate, in 1851 launched an uprising in Guizhou province, and established the Taiping Heavenly Kingdom with Hong himself as king. Hong announced that he had visions of God and that he was the brother of Jesus Christ. Slavery, concubinage, arranged marriage, opium smoking, footbinding, judicial torture, and the worship of idols were all banned. However, success led to internal feuds, defections and corruption. In addition, British and French troops, equipped with modern weapons, had come to the assistance of the Qing imperial army. It was not until 1864 that Qing armies under Zeng Guofan succeeded in crushing the revolt. After the outbreak of this rebellion, there were also revolts by the Muslims and Miao people of China against the Qing dynasty, most notably in the Miao Rebellion (1854–73) in Guizhou, the Panthay Rebellion (1856–1873) in Yunnan and the Dungan Revolt (1862–77) in the northwest.

The Western powers, largely unsatisfied with the Treaty of Nanjing, gave grudging support to the Qing government during the Taiping and Nian Rebellions. China's income fell sharply during the wars as vast areas of farmland were destroyed, millions of lives were lost, and countless armies were raised and equipped to fight the rebels. In 1854, Britain tried to re-negotiate the Treaty of Nanjing, inserting clauses allowing British commercial access to Chinese rivers and the creation of a permanent British embassy at Beijing.

In 1856, Qing authorities, in searching for a pirate, boarded a ship, the "Arrow", which the British claimed had been flying the British flag, an incident which led to the Second Opium War. In 1858, facing no other options, the Xianfeng Emperor agreed to the Treaty of Tientsin, which contained clauses deeply insulting to the Chinese, such as a demand that all official Chinese documents be written in English and a proviso granting British warships unlimited access to all navigable Chinese rivers.

Ratification of the treaty in the following year led to a resumption of hostilities. In 1860, with Anglo-French forces marching on Beijing, the emperor and his court fled the capital for the imperial hunting lodge at Rehe. Once in Beijing, the Anglo-French forces looted the Old Summer Palace and, in an act of revenge for the arrest of several Englishmen, burnt it to the ground. Prince Gong, a younger half-brother of the emperor, who had been left as his brother's proxy in the capital, was forced to sign the Convention of Beijing. The humiliated emperor died the following year at Rehe.

Yet the dynasty rallied. Chinese generals and officials such as Zuo Zongtang led the suppression of rebellions and stood behind the Manchus. When the Tongzhi Emperor came to the throne at the age of five in 1861, these officials rallied around him in what was called the Tongzhi Restoration. Their aim was to adopt Western military technology in order to preserve Confucian values. Zeng Guofan, in alliance with Prince Gong, sponsored the rise of younger officials such as Li Hongzhang, who put the dynasty back on its feet financially and instituted the Self-Strengthening Movement. The reformers then proceeded with institutional reforms, including China's first unified ministry of foreign affairs, the Zongli Yamen; allowing foreign diplomats to reside in the capital; establishment of the Imperial Maritime Customs Service; the formation of modernized armies, such as the Beiyang Army, as well as a navy; and the purchase from Europeans of armament factories.

The dynasty lost control of peripheral territories bit by bit. In return for promises of support against the British and the French, the Russian Empire took large chunks of territory in the Northeast in 1860. The period of cooperation between the reformers and the European powers ended with the Tientsin Massacre of 1870, which was incited by the murder of French nuns set off by the belligerence of local French diplomats. Starting with the Cochinchina Campaign in 1858, France expanded control of Indochina. By 1883, France was in full control of the region and had reached the Chinese border. The Sino-French War began with a surprise attack by the French on the Chinese southern fleet at Fuzhou. After that the Chinese declared war on the French. A French invasion of Taiwan was halted and the French were defeated on land in Tonkin at the Battle of Bang Bo. However Japan threatened to enter the war against China due to the Gapsin Coup and China chose to end the war with negotiations. The war ended in 1885 with the Treaty of Tientsin (1885) and the Chinese recognition of the French protectorate in Vietnam.

In 1884, pro-Japanese Koreans in Seoul led the Gapsin Coup. Tensions between China and Japan rose after China intervened to suppress the uprising. Japanese Prime Minister Itō Hirobumi and Li Hongzhang signed the Convention of Tientsin, an agreement to withdraw troops simultaneously, but the First Sino-Japanese War of 1895 was a military humiliation. The Treaty of Shimonoseki recognized Korean independence and ceded Taiwan and the Pescadores to Japan. The terms might have been harsher, but when a Japanese citizen attacked and wounded Li Hongzhang, an international outcry shamed the Japanese into revising them. The original agreement stipulated the cession of Liaodong Peninsula to Japan, but Russia, with its own designs on the territory, along with Germany and France, in the Triple Intervention, successfully put pressure on the Japanese to abandon the peninsula.

These years saw an evolution in the participation of Empress Dowager Cixi (Wade–Giles: Tz'u-Hsi) in state affairs. She entered the imperial palace in the 1850s as a concubine to the Xianfeng Emperor (r. 1850–1861) and came to power in 1861 after her five-year-old son, the Tongzhi Emperor ascended the throne. She, the Empress Dowager Ci'an (who had been Xianfeng's empress), and Prince Gong (a son of the Daoguang Emperor), staged a coup that ousted several regents for the boy emperor. Between 1861 and 1873, she and Ci'an served as regents, choosing the reign title "Tongzhi" (ruling together). Following the emperor's death in 1875, Cixi's nephew, the Guangxu Emperor, took the throne, in violation of the dynastic custom that the new emperor be of the next generation, and another regency began. In the spring of 1881, Ci'an suddenly died, aged only forty-three, leaving Cixi as sole regent.

From 1889, when Guangxu began to rule in his own right, to 1898, the Empress Dowager lived in semi-retirement, spending the majority of the year at the Summer Palace. On 1 November 1897, two German Roman Catholic missionaries were murdered in the southern part of Shandong province (the Juye Incident). Germany used the murders as a pretext for a naval occupation of Jiaozhou Bay. The occupation prompted a "scramble for concessions" in 1898, which included the German lease of Jiazhou Bay, the Russian acquisition of Liaodong, and the British lease of the New Territories of Hong Kong.

In the wake of these external defeats, the Guangxu Emperor initiated the Hundred Days' Reform of 1898. Newer, more radical advisers such as Kang Youwei were given positions of influence. The emperor issued a series of edicts and plans were made to reorganize the bureaucracy, restructure the school system, and appoint new officials. Opposition from the bureaucracy was immediate and intense. Although she had been involved in the initial reforms, the Empress Dowager stepped in to call them off, arrested and executed several reformers, and took over day-to-day control of policy. Yet many of the plans stayed in place, and the goals of reform were implanted.

Widespread drought in North China, combined with the imperialist designs of European powers and the instability of the Qing government, created conditions that led to the emergence of the Righteous and Harmonious Fists, or "Boxers." In 1900, local groups of Boxers proclaiming support for the Qing dynasty murdered foreign missionaries and large numbers of Chinese Christians, then converged on Beijing to besiege the Foreign Legation Quarter. A coalition of European, Japanese, and Russian armies (the Eight-Nation Alliance) then entered China without diplomatic notice, much less permission. Cixi declared war on all of these nations, only to lose control of Beijing after a short, but hard-fought campaign. She fled to Xi'an. The victorious allies drew up scores of demands on the Qing government, including compensation for their expenses in invading China and execution of complicit officials.

By the early 20th century, mass civil disorder had begun in China, and it was growing continuously. To overcome such problems, Empress Dowager Cixi issued an imperial edict in 1901 calling for reform proposals from the governors-general and governors and initiated the era of the dynasty's "New Policies", also known as the "Late Qing Reform". The edict paved the way for the most far-reaching reforms in terms of their social consequences, including the creation of a national education system and the abolition of the imperial examinations in 1905.

The Guangxu Emperor died on 14 November 1908, and on 15 November 1908, Cixi also died. Rumors held that she or Yuan Shikai ordered trusted eunuchs to poison the Guangxu Emperor, and an autopsy conducted nearly a century later confirmed lethal levels of arsenic in his corpse. Puyi, the oldest son of Zaifeng, Prince Chun, and nephew to the childless Guangxu Emperor, was appointed successor at the age of two, leaving Zaifeng with the regency. This was followed by the dismissal of General Yuan Shikai from his former positions of power. In April 1911 Zaifeng created a cabinet in which there were two vice-premiers. Nonetheless, this cabinet was also known by contemporaries as "The Royal Cabinet" because among the thirteen cabinet members, five were members of the imperial family or Aisin Gioro relatives. This brought a wide range of negative opinions from senior officials like Zhang Zhidong.
The Wuchang Uprising of 10 October 1911 was a success; by 14 November of the 15 provinces had rejected Qing rule. This led to the creation of a new central government, the Republic of China, in Nanjing with Sun Yat-sen as its provisional head. Many provinces soon began "separating" from Qing control. Seeing a desperate situation unfold, the Qing government brought Yuan Shikai back to military power. He took control of his Beiyang Army to crush the revolution in Wuhan at the Battle of Yangxia. After taking the position of Prime Minister and creating his own cabinet, Yuan Shikai went as far as to ask for the removal of Zaifeng from the regency. This removal later proceeded with directions from Empress Dowager Longyu. Yuan Shikai was now a dictator—the ruler of China and the Manchu dynasty had lost all power; it formally abdicated in early 1912.

Premier Yuan Shikai and his Beiyang commanders decided that going to war would be unreasonable and costly. Similarly, Sun Yat-sen wanted a republican constitutional reform, for the benefit of China's economy and populace. With permission from Empress Dowager Longyu, Yuan Shikai began negotiating with Sun Yat-sen, who decided that his goal had been achieved in forming a republic, and that therefore he could allow Yuan to step into the position of President of the Republic of China.

On 12 February 1912, after rounds of negotiations, Longyu issued an imperial edict bringing about the abdication of the child emperor Puyi. This brought an end to over 2,000 years of Imperial China and began an extended period of instability of warlord factionalism. The unorganized political and economic systems combined with a widespread criticism of Chinese culture led to questioning and doubt about the future. Some Qing loyalists organized themselves as "Royalist Party", and tried to use militant activism and open rebellions to restore the monarchy, but to no avail. In July 1917, there was an abortive attempt to restore the Qing dynasty led by Zhang Xun, which was quickly reversed by republican troops. In the 1930s, the Empire of Japan invaded Northeast China and founded Manchukuo in 1932, with Puyi as its emperor. After the invasion by the Soviet Union, Manchukuo fell in 1945.

The early Qing emperors adopted the bureaucratic structures and institutions from the preceding Ming dynasty but split rule between Han Chinese and Manchus, with some positions also given to Mongols. Like previous dynasties, the Qing recruited officials via the imperial examination system, until the system was abolished in 1905. The Qing divided the positions into civil and military positions, each having nine grades or ranks, each subdivided into a and b categories. Civil appointments ranged from an attendant to the emperor or a Grand Secretary in the Forbidden City (highest) to being a prefectural tax collector, deputy jail warden, deputy police commissioner, or tax examiner. Military appointments ranged from being a field marshal or chamberlain of the imperial bodyguard to a third class sergeant, corporal or a first or second class private.

The formal structure of the Qing government centered on the Emperor as the absolute ruler, who presided over six Boards (Ministries), each headed by two presidents and assisted by four vice presidents. In contrast to the Ming system, however, Qing ethnic policy dictated that appointments were split between Manchu noblemen and Han officials who had passed the highest levels of the state examinations. The Grand Secretariat, which had been an important policy-making body under the Ming, lost its importance during the Qing and evolved into an imperial chancery. The institutions which had been inherited from the Ming formed the core of the Qing "Outer Court", which handled routine matters and was located in the southern part of the Forbidden City.

In order not to let the routine administration take over the running of the empire, the Qing emperors made sure that all important matters were decided in the "Inner Court", which was dominated by the imperial family and Manchu nobility and which was located in the northern part of the Forbidden City. The core institution of the inner court was the Grand Council. It emerged in the 1720s under the reign of the Yongzheng Emperor as a body charged with handling Qing military campaigns against the Mongols, but it soon took over other military and administrative duties and served to centralize authority under the crown. The Grand Councillors served as a sort of privy council to the emperor.

The Six Ministries and their respective areas of responsibilities were as follows:

Board of Civil Appointments

Board of Revenue

Board of Rites

Board of War

Board of Punishments

Board of Works

From the early Qing, the central government was characterized by a system of dual appointments by which each position in the central government had a Manchu and a Han Chinese assigned to it. The Han Chinese appointee was required to do the substantive work and the Manchu to ensure Han loyalty to Qing rule.

In addition to the six boards, there was a Lifan Yuan unique to the Qing government. This institution was established to supervise the administration of Tibet and the Mongol lands. As the empire expanded, it took over administrative responsibility of all minority ethnic groups living in and around the empire, including early contacts with Russia – then seen as a tribute nation. The office had the status of a full ministry and was headed by officials of equal rank. However, appointees were at first restricted only to candidates of Manchu and Mongol ethnicity, until later open to Han Chinese as well.

Even though the Board of Rites and Lifan Yuan performed some duties of a foreign office, they fell short of developing into a professional foreign service. It was not until 1861 – a year after losing the Second Opium War to the Anglo-French coalition – that the Qing government bowed to foreign pressure and created a proper foreign affairs office known as the Zongli Yamen. The office was originally intended to be temporary and was staffed by officials seconded from the Grand Council. However, as dealings with foreigners became increasingly complicated and frequent, the office grew in size and importance, aided by revenue from customs duties which came under its direct jurisdiction.

There was also another government institution called Imperial Household Department which was unique to the Qing dynasty. It was established before the fall of the Ming, but it became mature only after 1661, following the death of the Shunzhi Emperor and the accession of his son, the Kangxi Emperor. The department's original purpose was to manage the internal affairs of the imperial family and the activities of the inner palace (in which tasks it largely replaced eunuchs), but it also played an important role in Qing relations with Tibet and Mongolia, engaged in trading activities (jade, ginseng, salt, furs, etc.), managed textile factories in the Jiangnan region, and even published books. Relations with the Salt Superintendents and salt merchants, such as those at Yangzhou, were particularly lucrative, especially since they were direct, and did not go through absorptive layers of bureaucracy. The department was manned by "booi", or "bondservants," from the Upper Three Banners. By the 19th century, it managed the activities of at least 56 subagencies.

Qing China reached its largest extent during the 18th century, when it ruled China proper (eighteen provinces) as well as the areas of present-day Northeast China, Inner Mongolia, Outer Mongolia, Xinjiang and Tibet, at approximately 13 million km in size. There were originally 18 provinces, all of which in China proper, but later this number was increased to 22, with Manchuria and Xinjiang being divided or turned into provinces. Taiwan, originally part of Fujian province, became a province of its own in the 19th century, but was ceded to the Empire of Japan following the First Sino-Japanese War by the end of the century. In addition, many surrounding countries, such as Korea (Joseon dynasty), Vietnam frequently paid tribute to China during much of this period. The Katoor dynasty of Afghanistan also paid tribute to the Qing dynasty of China until the mid-19th century. During the Qing dynasty the Chinese claimed suzerainty over the Taghdumbash Pamir in the south-west of Taxkorgan Tajik Autonomous County but permitted the Mir of Hunza to administer the region in return for a tribute. Until 1937 the inhabitants paid tribute to the Mir of Hunza, who exercised control over the pastures. Khanate of Kokand were forced to submit as protectorate and pay tribute to the Qing dynasty in China between 1774 and 1798.




The Qing organization of provinces was based on the fifteen administrative units set up by the Ming dynasty, later made into eighteen provinces by splitting for example, Huguang into Hubei and Hunan provinces. The provincial bureaucracy continued the Yuan and Ming practice of three parallel lines, civil, military, and censorate, or surveillance. Each province was administered by a governor (, "xunfu") and a provincial military commander (, "tidu"). Below the province were prefectures (, "fu") operating under a prefect (, "zhīfǔ"), followed by subprefectures under a subprefect. The lowest unit was the county, overseen by a county magistrate. The eighteen provinces are also known as "China proper". The position of viceroy or governor-general (, "zongdu") was the highest rank in the provincial administration. There were eight regional viceroys in China proper, each usually took charge of two or three provinces. The Viceroy of Zhili, who was responsible for the area surrounding the capital Beijing, is usually considered as the most honorable and powerful viceroy among the eight.


By the mid-18th century, the Qing had successfully put outer regions such as Inner and Outer Mongolia, Tibet and Xinjiang under its control. Imperial commissioners and garrisons were sent to Mongolia and Tibet to oversee their affairs. These territories were also under supervision of a central government institution called Lifan Yuan. Qinghai was also put under direct control of the Qing court. Xinjiang, also known as Chinese Turkestan, was subdivided into the regions north and south of the Tian Shan mountains, also known today as Dzungaria and Tarim Basin respectively, but the post of Ili General was established in 1762 to exercise unified military and administrative jurisdiction over both regions. Dzungaria was fully opened to Han migration by the Qianlong Emperor from the beginning. Han migrants were at first forbidden from permanently settling in the Tarim Basin but were the ban was lifted after the invasion by Jahangir Khoja in the 1820s. Likewise, Manchuria was also governed by military generals until its division into provinces, though some areas of Xinjiang and Northeast China were lost to the Russian Empire in the mid-19th century. Manchuria was originally separated from China proper by the Inner Willow Palisade, a ditch and embankment planted with willows intended to restrict the movement of the Han Chinese, as the area was off-limits to civilian Han Chinese until the government started colonizing the area, especially since the 1860s.

With respect to these outer regions, the Qing maintained imperial control, with the emperor acting as Mongol khan, patron of Tibetan Buddhism and protector of Muslims. However, Qing policy changed with the establishment of Xinjiang province in 1884. During The Great Game era, taking advantage of the Dungan revolt in northwest China, Yaqub Beg invaded Xinjiang from Central Asia with support from the British Empire, and made himself the ruler of the kingdom of Kashgaria. The Qing court sent forces to defeat Yaqub Beg and Xinjiang was reconquered, and then the political system of China proper was formally applied onto Xinjiang. The Kumul Khanate, which was incorporated into the Qing empire as a vassal after helping Qing defeat the Zunghars in 1757, maintained its status after Xinjiang turned into a province through the end of the dynasty in the Xinhai Revolution up until 1930. In the early 20th century, Britain sent an expedition force to Tibet and forced Tibetans to sign a treaty. The Qing court responded by asserting Chinese sovereignty over Tibet, resulting in the 1906 Anglo-Chinese Convention signed between Britain and China. The British agreed not to annex Tibetan territory or to interfere in the administration of Tibet, while China engaged not to permit any other foreign state to interfere with the territory or internal administration of Tibet. Furthermore, similar to Xinjiang which was converted into a province earlier, the Qing government also turned Manchuria into three provinces in the early 20th century, officially known as the "Three Northeast Provinces", and established the post of Viceroy of the Three Northeast Provinces to oversee these provinces, making the total number of regional viceroys to nine.

The early Qing military was rooted in the Eight Banners first developed by Nurhaci to organize Jurchen society beyond petty clan affiliations. There were eight banners in all, differentiated by color. The yellow, bordered yellow, and white banners were known as the "Upper Three Banners" and were under the direct command of the emperor. Only Manchus belonging to the Upper Three Banners, and selected Han Chinese who had passed the highest level of martial exams could serve as the emperor's personal bodyguards. The remaining Banners were known as the "Lower Five Banners". They were commanded by hereditary Manchu princes descended from Nurhachi's immediate family, known informally as the "Iron cap princes". Together they formed the ruling council of the Manchu nation as well as high command of the army. Nurhachi's son Hong Taiji expanded the system to include mirrored Mongol and Han Banners. After capturing Beijing in 1644, the relatively small Banner armies were further augmented by the Green Standard Army, made up of those Ming troops who had surrendered to the Qing, which eventually outnumbered Banner troops three to one. They maintained their Ming era organization and were led by a mix of Banner and Green Standard officers.

Banner Armies were organized along ethnic lines, namely Manchu and Mongol, but included non-Manchu bondservants registered under the household of their Manchu masters. The years leading up to the conquest increased the number of Han Chinese under Manchu rule, leading Hong Taiji to create the , and around the time of the Qing takeover of Beijing, their numbers rapidly swelled. Han Bannermen held high status and power, especially immediately after the conquest during Shunzhi and Kangxi's reign where they dominated Governor-Generalships and Governorships at the expense of both Manchu Bannermen and Han civilians. Han also numerically dominated the Banners up until the mid 18th century. European visitors in Beijing called them "Tartarized Chinese" or "Tartarified Chinese".

The Qianlong Emperor, concerned about maintaining Manchu identity, re-emphasized Manchu ethnicity, ancestry, language, and culture in the Eight Banners and started a mass discharge of Han Bannermen, either asking them to voluntarily resign from the Banner rolls or striking their names off. This led to a change from Han majority to a Manchu majority within the Banner system, and previous Han Bannermen garrisons in southern China such as at Fuzhou, Zhenjiang, Guangzhou, were replaced by Manchu Bannermen in the purge, which started in 1754. The turnover impacted garrisons in the provinces, leaving a larger proportion of remaining Han Bannermen in Beijing than the provinces. Han Bannermen status decreased, Manchu Banners gained higher status. Han Bannermen numbered 75% in 1648 Shunzhi's reign, 72% in 1723 Yongzheng's reign, but decreased to 43% in 1796 during the first year of Jiaqing's reign, after Qianlong's purge. The mass discharge was known as the . Qianlong directed most of his ire at Han Bannermen descended from defectors who joined the Qing after 1644, since traitors to the Ming would be untrustworthy, while retaining Han Bannermen who were descended from defectors who joined the Qing before 1644 and marched through Shanhai pass, known as those who "followed the Dragon through the pass" ().

The Manchu Banner troops eventually lost their fighting edge. Before the conquest, the Manchu banner had been a "citizen" army whose members were farmers and herders obligated to provide military service in times of war. Turning the banner troops into a professional force whose every need was met by the state brought wealth, corruption, and decline as a fighting force. The Green Standard Army declined in a similar way.

Facing Europeans with newly applied technologies in the Opium Wars (1839-1860) led to substantial reforms in organization, finance, and armament. Early during the Taiping Rebellion, Qing forces suffered a series of disastrous defeats culminating in the loss of the regional capital city of Nanjing in 1853. Shortly thereafter, a Taiping expeditionary force penetrated as far north as the suburbs of Tianjin, the imperial heartlands. In desperation the Qing court ordered a Chinese official, Zeng Guofan, to organize regional and village militias into an emergency army called tuanlian. Zeng Guofan's strategy was to rely on local gentry to raise a new type of military organization from those provinces that the Taiping rebels directly threatened. This new force became known as the Xiang Army, named after the Hunan region where it was raised. The Xiang Army was a hybrid of local militia and a standing army. It was given professional training, but was paid for out of regional coffers and funds its commanders – mostly members of the Chinese gentry – could muster. The Xiang Army and its successor, the Huai Army, created by Zeng Guofan's colleague and protégée Li Hongzhang, were collectively called the "Yong Ying" (Brave Camp).

Zeng Guofan had no prior military experience. Being a classically educated official, he took his blueprint for the Xiang Army from the Ming general Qi Jiguang, who, because of the weakness of regular Ming troops, had decided to form his own "private" army to repel raiding Japanese pirates in the mid-16th century. Qi Jiguang's doctrine was based on Neo-Confucian ideas of binding troops' loyalty to their immediate superiors and also to the regions in which they were raised. Zeng Guofan's original intention for the Xiang Army was simply to eradicate the Taiping rebels. However, the success of the Yongying system led to its becoming a permanent regional force within the Qing military, which in the long run created problems for the beleaguered central government.

First, the Yongying system signaled the end of Manchu dominance in Qing military establishment. Although the Banners and Green Standard armies lingered on as a drain on resources, henceforth the Yongying corps became the Qing government's de facto first-line troops. Second, the Yongying corps were financed through provincial coffers and were led by regional commanders, weakening central government's grip on the whole country. Finally, the nature of Yongying command structure fostered nepotism and cronyism amongst its commanders, who laid the seeds of regional warlordism in the first half of the 20th century.

By the late 19th century, the most conservative elements within the Qing court could no longer ignore China's military weakness. In 1860, during the Second Opium War, the capital Beijing was captured and the Summer Palace sacked by a relatively small Anglo-French coalition force numbering 25,000. The advent of modern weaponry resulting from the European Industrial Revolution had rendered China's traditionally trained and equipped army and navy obsolete. The government attempts to modernize during the Self-Strengthening Movement were initially successful, but yielded few lasting results because of the central government's lack of funds, lack of political will, and unwillingness to depart from tradition.

Losing the First Sino-Japanese War of 1894–1895 was a watershed. Japan, a country long regarded by the Chinese as little more than an upstart nation of pirates, annihilated the Qing government's modernized Beiyang Fleet, then deemed to be the strongest naval force in Asia. The Japanese victory occurred a mere three decades after the Meiji Restoration set a feudal Japan on course to emulate the Western nations in their economic and technological achievements. Finally, in December 1894, the Qing government took concrete steps to reform military institutions and to re-train selected units in Westernized drills, tactics and weaponry. These units were collectively called the New Army. The most successful of these was the Beiyang Army under the overall supervision and control of a former Huai Army commander, General Yuan Shikai, who used his position to build networks of loyal officers and eventually become President of the Republic of China.

The most significant facts of early and mid-Qing social history was growth in population, population density, and mobility. The population in 1700, according to widely accepted estimates, was roughly 150 million, about what it had been under the late Ming a century before, then doubled over the next century, and reached a height of 450 million on the eve of the Taiping Rebellion in 1850.

One reason for this growth was the spread of New World crops like peanuts, sweet potatoes, and potatoes, which helped to sustain the people during shortages of harvest for crops such as rice or wheat.  These crops could be grown under harsher conditions, and thus were cheaper as well, which led to them becoming staples for poorer farmers, decreasing the number of deaths from malnutrition. Diseases such as smallpox, widespread in the seventeenth century, were brought under control by an increase in inoculations. In addition, infant deaths were also greatly decreased due to improvements in birthing techniques and childcare performed by doctors and midwives and through an increase in medical books available to the public. Government campaigns decreased the incidence of infanticide. Unlike Europe, where population growth in this period was greatest in the cities, in China the growth in cities and the lower Yangzi was low. The greatest growth was in the borderlands and the highlands, where farmers could clear large tracts of marshlands and forests.

The population was also remarkably mobile, perhaps more so than at any time in Chinese history. Indeed, the Qing government did far more to encourage mobility than to discourage it. Millions of Han Chinese migrated to Yunnan and Guizhou in the 18th century, and also to Taiwan. After the conquests of the 1750s and 1760s, the court organized agricultural colonies in Xinjiang. Migration might be permanent, for resettlement, or the migrants (in theory at least) might regard the move as a temporary sojourn. The latter included an increasingly large and mobile workforce. Local-origin-based merchant groups also moved freely. This mobility also included the organized movement of Qing subjects overseas, largely to Southeastern Asia, in search of trade and other economic opportunities.

According to statute, Qing society was divided into relatively closed estates, of which in most general terms there were five. Apart from the estates of the officials, the comparatively minuscule aristocracy, and the degree-holding literati, there also existed a major division among ordinary Chinese between commoners and people with inferior status. They were divided into two categories: one of them, the good "commoner" people, the other "mean" people who were seen as debased and servile. The majority of the population belonged to the first category and were described as "liangmin", a legal term meaning good people, as opposed to "jianmin" meaning the mean (or ignoble) people. Qing law explicitly stated that the traditional four occupational groups of scholars, farmers, artisans and merchants were "good", or having a status of commoners. On the other hand, slaves or bondservants, entertainers (including prostitutes and actors), tattooed criminals, and those low-level employees of government officials were the "mean people". Mean people were considered legally inferior to commoners and suffered unequal treatments, forbidden to take the imperial examination. Furthermore, such people were usually not allowed to marry with free commoners and were even often required to acknowledge their abasement in society through actions such as bowing. However, throughout the Qing dynasty, the emperor and his court, as well as the bureaucracy, worked towards reducing the distinctions between the debased and free but did not completely succeed even at the end of its era in merging the two classifications together.

Although there had been no powerful hereditary aristocracy since the Song dynasty, the gentry ("shenshi"), like their British counterparts, enjoyed imperial privileges and managed local affairs. The status of this scholar-official was defined by passing at least the first level of civil service examinations and holding a degree, which qualified him to hold imperial office, although he might not actually do so. The gentry member could legally wear gentry robes and could talk to other officials as equals. Officials who had served for one or two terms could then retire to enjoy the glory of their status. Informally, the gentry then presided over local society and could use their connections to influence the magistrate, acquire land, and maintain large households. The gentry thus included not only the males holding degrees but also their wives, descendants, some of their relatives.

The Qing gentry were defined as much by their refined lifestyle as by their legal status. They lived more refined and comfortable lives than the commoners and used sedan-chairs to travel any significant distance. They were usually highly literate and often showed off their learning. They commonly collected objects such as scholars' stones,
porcelain or pieces of art for their beauty, which set them off from less cultivated commoners.

In Qing society, women did not enjoy the same rights as men. The Confucian moral system, which was built by and thus favored men, restrained their rights, and they were often seen as a type of "merchandise" that could be traded away by their family. Once a woman married, she essentially became the property of her husband's family, and could not divorce her husband except under very specific circumstances, such as severe physical harm or an attempt to sell her into prostitution. Men, on the other hand, could divorce their wives for trivial matters such as excessive talkativeness. Furthermore, women were extremely restricted in owning property and inheritance and were essentially confined to their homes and stripped of social interaction and mobility. Mothers often bound their young daughters' feet, a practice that was seen as a standard of feminine beauty and a necessity to be marriageable, but was also a way to restrict a woman's physical movement in society.

By early Qing, the romanticized courtesan culture, which had been much more popular in the late-Ming with men who had sought a model of a refinement and literacy that was missing from their marriage partners, had mostly disappeared. Such a decline was the result of the Qing's reinforced defense of fundamental Confucian family values as well as an attempt to put a stop to the cultural revolution that was happening at the time. The court thus began to rain down heavily on such practices as prostitution, pornography, rape, and homosexuality. However, by the time of the Qianlong emperor, red-light districts had once again become capitals of tasteful and trending courtesanship. In economically diverse port cities such as Tianjin, Chongqing, and Hankou, the sex trade became a large business, which helped supply a fine hierarchy of prostitutes to all classes of men. Shanghai, which had been rapidly growing in the late nineteenth century, became a city where prostitutes of different ranks whom male patrons fawned over and gossiped about, as some became recognized as national entities of femininity.

Another rising phenomenon, especially during the eighteenth century, was the cult of widow chastity. The fact that many young women were betrothed during early adolescence coupled with the high rate of early mortality resulted in a significant number of young widows. This resulted in a problem, as most women had already moved into their husband's household and upon her husband's death essentially became a burden who could never fulfill her original duty of producing a male heir. Widow chastity began to be seen as a form of devout filiality for other relationships including loyalty to the emperor, which resulted in the Qing court's attempt to reward those families who resisted selling off their unneeded daughters-in-law in order to underline such women's virtue. However, this system began to decline when families who attempted to "abuse" the system appeared for social competition and authorities speculated that some families coerced their young widows to commit suicide at the time of their husband's death to obtain more honors. Such corruption showed a lack of respect for human life, and was thus greatly disapproved of by the officials who then chose to reward the families more sparingly.

One of the main reasons for a shift in gender roles was the unprecedentedly high incidence of men leaving their homes to travel, which in turn gave women more freedom for action. Wives of such men often became the ones to run the household, especially in financial matters. Elite women also began to pursue different fashionable activities, such as writing poetry, and a new frenzy of female sociability appeared. Women started to leave their households to attend local opera performances and temple festivals and some even began to form little societies to visit famous sacred sites with other restless women, which helped to shape a new view of the conventional societal norms on how women should behave.

Patrilineal kinship had compelling power socially and culturally; local lineages became the building blocks of society. A person's success or failure depended, people believed, on guidance from a father, from which the family's success and prosperity also grew. The patrilineage kinship structure, that is, descent through the male line, was often translated as "clan" in earlier scholarship. By the Qing, the patrilineage had become the primary organizational device in society. This change began during the Song dynasty when the civil service examination became a means for gaining status versus nobility and inheritance of status. Elite families began to shift their marital practices, identity and loyalty. Instead of intermarrying within aristocratic elites of the same social status, they tended to form marital alliances with nearby families of the same or higher wealth, and established the local people's interests as first and foremost which helped to form intermarried townships. The Neo-Confucian ideology particular Cheng-Zhu thinking adopted by the Qing placed emphasis on patrilineal families and genealogy in society. The emperors exhorted families to compile genealogies in order to strengthen local society.

Inner Mongols and Khalkha Mongols in the Qing rarely knew their ancestors beyond four generations and Mongol tribal society was not organized among patrilineal clans, contrary to what was commonly thought, but included unrelated people at the base unit of organization. The Qing tried but failed to promote the Chinese Neo-Confucian ideology of organizing society along patrimonial clans among the Mongols.

Qing lineages claimed to be based on biological descent but they were often purposefully crafted. When a member of a lineage gained office or became wealthy, he might look back to identify a "founding ancestor", sometimes using considerable creativity in selecting a prestigious local figure. Once such a person had been chosen, a Chinese character was assigned to be used in the given name of each male in each succeeding generation. A written genealogy was compiled to record the lineage's history, biographies of respected ancestors, a chart of all the family members of each generation, rules for the members to follow, and often copies of title contracts for collective property as well. Lastly, an ancestral hall was built to serve as the lineage's headquarters and a place for annual ancestral sacrifice. Such worship was intended to ensure that the ancestors remain content and benevolent spirits ("shen") who would keep watch over and protect the family. Later observers felt that the ancestral cult focused on the family and lineage, rather than on more public matters such as community and nation.

Catholic missionaries—mostly Jesuits—had arrived during the Ming dynasty. By 1701, there were 117 Catholic missionaries, and at most 300,000 converts in a population of hundreds of millions. There were many persecutions and reverses in the 18th century and by 1800 there was little help from the main supporters in France, Spain and Portugal. The impact on Chinese society was difficult to see, apart from some contributions to mathematics, astronomy and the calendar. By the 1840s, China was again becoming a major destination for Protestant and Catholic missionaries from Europe and the United States. They encountered significant opposition from local elites, who were committed to Confucianism. These elites resented Western ethical systems, which were seen as a threat to their power, and often viewed missionaries as a tool of Western imperialism. The mandarins claim to power lay in the knowledge of the Chinese classics—all government officials had to pass extremely difficult tests on Confucianism. The elite feared this might be replaced by the Bible, scientific training and Western education. In the early 20th century, the examination system was abolished by reformers who admired Western models of modernization. According to Paul Cohen, from 1860 to 1900:
Catholic missionaries of the 19th century arrived primarily from France. While they arrived somewhat later than the Protestants, their congregations grew at a faster rate. By 1900, there were about 1,400 Catholic priests and nuns in China serving nearly 1 million Catholics. Over 3,000 Protestant missionaries were active among the 250,000 Protestant Christians in China. Missionaries, like all foreigners, enjoyed extraterritorial legal rights. The main goal was conversions, but they made relatively few. They were much more successful in setting up schools, hospitals and dispensaries. They usually avoided Chinese politics, but were opponents of foot-binding and opium. Western governments could protect them in the treaty ports, but outside those limited areas they were at the mercy of local government officials and threats were common. Chinese elites often associated missionary activity with the imperialistic exploitation of China, and with promoting "new technology and ideas that threatened their positions". Historian John K. Fairbank wrote, "To most Chinese, Christian missionaries seem to be the ideological arm of foreign aggression... To the scholar-gentry, missionaries were foreign subversives, whose immoral conduct and teachings were backed by gunboats. Conservative patriots hated and feared these alien, intruders." The missionaries and their converts were a prime target of attack and murder by Boxers in 1900.

Medical missions in China by the late 19th century laid the foundations for modern medicine in China. Western medical missionaries established the first modern clinics and hospitals, and led medical training in China. By 1901, China was the most popular destination for medical missionaries. The 150 foreign physicians operated 128 hospitals and 245 dispensaries, treating 1.7 million patients. In 1894, male medical missionaries comprised 14% of all missionaries; female doctors were 4%. Modern medical education in China started in the early 20th century at hospitals run by international missionaries. They began establishing nurse training schools in China in the late 1880s, but nursing of sick men by women was rejected by local traditions, so the number of Chinese students was small until the practice became accepted in the 1930s. There was also a level of distrust on the part of traditional evangelical missionaries who thought hospitals were diverting resources away from the primary goal of conversions.

Appointed by the London Missionary Society (LMS), Robert Morrison (1782–1834) is the pioneering Protestant missionary to China. Before his departure on January 31, 1807, he received missionary training from David Bogue (1750–1825) at the Gosport Academy. Bogue's missionary strategy comprised three steps: mastering the native language after arriving at the mission locale, prioritizing the translation and publishing of the Bible above all, and establishing a local seminary to prepare the native Christians. Upon his arrival at Canton on September 6, 1807, Morrison followed Bogue's instruction, learned the language, and proceeded with translation and publication work on the Bible. Morrison, assisted by William Milne (1785–1822) who was sent by the LMS, finished the translation of the entire Bible in 1819. Meanwhile, they founded the first Asian Protestant seminary (the Anglo-Chinese College) in Malacca in 1818, which adopted the Gosport curriculum. Afterward, Liang Afa (1789–1855), the Morrison-trained Chinese convert, succeeded and branched out the evangelization mission in inner China. In retrospect, Bogue's three-part strategy has been implemented through Morrison and Milne's mission to China.

The two Opium Wars (1839–1860) marked the watershed of the Protestant Christian mission in China. From 1724 to 1858, it was the period of proscription.<ref name="Reilly 2004 https://archive.org/details/taipingheavenlyk00reil/page/n55 43"></ref> In 1724, the Yongzheng emperor (1678–1735) announced that Christianity was a "heterodox teaching" and hence proscribed. In 1811, Christian religious activities were further criminalized by the Jiaqing Emperor (1760–1820). It was in such a background that Morrison arrived at Canton in China, experienced not only the difficulty in proceeding the missionary work but also the high living cost. Meanwhile, for sustaining his living and securing his legal residence in Canton, Morrison got approval from the LMS and, thus, accepted the employment of the East India Company and worked as a translator since 1809. However, his decision was challenged. In 1823, a newly arrived missionary refused to comply with Morrison's practice of accepting salary from a company which profited from the opium trade, and denounced that the opium trade contradicted the morality of Christianity. According to Platt's studies on the existing records, aside from this exceptional case, neither Morrison nor foreigners who benefited from selling opium mentioned anything but financial terms.

After the Opium Wars, a new world order arose between Qing China and the Western states.<ref name="Reilly 2004 https://archive.org/details/taipingheavenlyk00reil/page/n61 49"></ref> As Codified in the 1842 Treaty of Nanjing, the American treaty and the French treaty signed in 1844, and the 1858 Treaty of Tianjin, Christianity was distinguished from the local religions and protected. Subsequently, the Chinese popular cults, such as the White Lotus and the Eight Trigram, attached themselves to Christianity to share this protection. Meanwhile, the lifting of the proscription made room for the emergence of the Christian-inspired Taiping Movement in the Yangtze River Delta. According to Reilly, the Chinese Bible translated by Morrison, as well as Liang Afa's evangelistic pamphlet, significantly impacted the formation of the Taiping movement and its religious thoughts.

At the outset of the twentieth century, along with the Western states' attempt to justify their military invasions and plunders, the missionary publications served as a medium to shape the prevailing narrative of the Boxer Uprising that "continue to circulate into the present". The Boxer Uprising occurred in 1900, in which the Chinese people in northern China stormed certain areas that they were barred from entering, such as the missionary stations and the legation areas in Beijing. In 1901, shortly after the suppression of the uprising, a series of Protestant missionary accounts were published, pioneered by Arthur Smith (1845–1932). The missionary discourse reiterates the "Chinese antiforeignism" underpinned by the Qing government, on the one hand; on the other hand, it highlights the missionaries' sacrifices for the preservation of Christian religion in facing "pagan barbarism". According to Hevia, despite the conflicting and inconsistent accounts given by the witnesses, these works help to make the Western military retaliation in responding to the "Chinese brutality" to be reasonable. The ongoing creation and circulation of such narratives and memory, therefore, solidified images of "Chinese savagery" and the victimized and heroized Western states.

By the end of the 17th century, the Chinese economy had recovered from the devastation caused by the wars in which the Ming dynasty were overthrown, and the resulting breakdown of order. In the following century, markets continued to expand as in the late Ming period, but with more trade between regions, a greater dependence on overseas markets and a greatly increased population. By the end of the 18th century the population had risen to 300 million from approximately 150 million during the late Ming dynasty. The dramatic rise in population was due to several reasons, including the long period of peace and stability in the 18th century and the import of new crops China received from the Americas, including peanuts, sweet potatoes and maize. New species of rice from Southeast Asia led to a huge increase in production. Merchant guilds proliferated in all of the growing Chinese cities and often acquired great social and even political influence. Rich merchants with official connections built up huge fortunes and patronized literature, theater and the arts. Textile and handicraft production boomed.

The government broadened land ownership by returning land that had been sold to large landowners in the late Ming period by families unable to pay the land tax. To give people more incentives to participate in the market, they reduced the tax burden in comparison with the late Ming, and replaced the corvée system with a head tax used to hire laborers. The administration of the Grand Canal was made more efficient, and transport opened to private merchants. A system of monitoring grain prices eliminated severe shortages, and enabled the price of rice to rise slowly and smoothly through the 18th century. Wary of the power of wealthy merchants, Qing rulers limited their trading licenses and usually refused them permission to open new mines, except in poor areas. These restrictions on domestic resource exploration, as well as on foreign trade, are held by some scholars as a cause of the Great Divergence, by which the Western world overtook China economically.

During the Ming–Qing period (1368–1911) the biggest development in the Chinese economy was its transition from a command to a market economy, the latter becoming increasingly more pervasive throughout the Qing's rule. From roughly 1550 to 1800 China proper experienced a second commercial revolution, developing naturally from the first commercial revolution of the Song period which saw the emergence of long-distance inter-regional trade of luxury goods. During the second commercial revolution, for the first time, a large percentage of farming households began producing crops for sale in the local and national markets rather than for their own consumption or barter in the traditional economy. Surplus crops were placed onto the national market for sale, integrating farmers into the commercial economy from the ground up. This naturally led to regions specializing in certain cash-crops for export as China's economy became increasingly reliant on inter-regional trade of bulk staple goods such as cotton, grain, beans, vegetable oils, forest products, animal products, and fertilizer.

Perhaps the most important factor in the development of the second commercial revolution was the mass influx of silver that entered into the country from foreign trade. After the Spanish conquered the Philippines in the 1570s they mined for silver around the New World, greatly expanding the circulating supply of silver. Foreign trade stimulated the ubiquity of the silver standard, after the re-opening of the southeast coast, which had been closed in the late 17th century, foreign trade was quickly re-established, and was expanding at 4% per annum throughout the latter part of the 18th century. China continued to export tea, silk and manufactures, creating a large, favorable trade balance with the West. The resulting inflow of silver expanded the money supply, facilitating the growth of competitive and stable markets. During the mid-Ming China had gradually shifted to silver as the standard currency for large scale transactions and by the late Kangxi reign the assessment and collection of the land tax was done in silver. By standardizing the collection of the land tax in silver, landlords followed suit and began only accepting rent payments in silver rather than in crops themselves, which in turn incentivized farmers to produce crops for sale in local and national markets rather than for their own personal consumption or barter. Unlike the copper coins, "qian" or cash, used mainly for smaller peasant transactions, silver was not properly minted into a coin but rather was traded in designated units of weight: the "liang" or "tael", which equaled roughly 1.3 ounces of silver. Since it was never properly minted, a third-party had to be brought in to assess the weight and purity of the silver, resulting in an extra "meltage fee" added on to the price of transaction. Furthermore, since the "meltage fee" was unregulated until the reign of the Yongzheng emperor it was the source of much corruption at each level of the bureaucracy. The Yongzheng emperor cracked down on the corrupt "meltage fees," legalizing and regulating them so that they could be collected as a tax, "returning meltage fees to the public coffer." From this newly increased public coffer, the Yongzheng emperor increased the salaries of the officials who collected them, further legitimizing silver as the standard currency of the Qing economy.

The second commercial revolution also had a profound effect on the dispersion of the Qing populace. Up until the late Ming there existed a stark contrast between the rural countryside and city metropoles and very few mid-sized cities existed. This was due to the fact that extraction of surplus crops from the countryside was traditionally done by the state and not commercial organizations. However, as commercialization expanded exponentially in the late-Ming and early-Qing, mid-sized cities began popping up to direct the flow of domestic, commercial trade. Some towns of this nature had such a large volume of trade and merchants flowing through them that they developed into full-fledged market-towns. Some of these more active market-towns even developed into small-cities and became home to the new rising merchant-class. The proliferation of these mid-sized cities was only made possible by advancements in long-distance transportation and methods of communication. As more and more Chinese-citizens were travelling the country conducting trade they increasingly found themselves in a far-away place needing a place to stay, in response the market saw the expansion of guild halls to house these merchants.

A key distinguishing feature of the Qing economy was the emergence of guild halls around the nation. As inter-regional trade and travel became ever more common during the Qing, guild halls dedicated to facilitating commerce, "huiguan", gained prominence around the urban landscape. The location where two merchants would meet to exchange commodities was usually mediated by a third-party broker who served a variety of roles for the market and local citizenry including bringing together buyers and sellers, guaranteeing the good faith of both parties, standardizing the weights, measurements, and procedures of the two parties, collecting tax for the government, and operating inns and warehouses. It was these broker's and their places of commerce that were expanded during the Qing into full-fledged trade guilds, which, among other things, issued regulatory codes and price schedules, and provided a place for travelling merchants to stay and conduct their business. The first recorded trade guild set up to facilitate inter-regional commerce was in Hankou in 1656. Along with the "huiguan" trade guilds, guild halls dedicated to more specific professions, "gongsuo", began to appear and to control commercial craft or artisanal industries such as carpentry, weaving, banking, and medicine. By the nineteenth century guild halls had much more impact on the local communities than simply facilitating trade, they transformed urban areas into cosmopolitan, multi-cultural hubs, staged theatre performances open to general public, developed real estate by pooling funds together in the style of a trust, and some even facilitated the development of social services such as maintaining streets, water supply, and sewage facilities.

In 1685 the Kangxi emperor legalized private maritime trade along the coast, establishing a series of customs stations in major port cities. The customs station at Canton became by far the most active in foreign trade and by the late Kangxi reign more than forty mercantile houses specializing in trade with the West had appeared. The Yongzheng emperor made a parent corporation comprising those forty individual houses in 1725 known as the Cohong system. Firmly established by 1757, the Canton Cohong was an association of thirteen business firms that had been awarded exclusive rights to conduct trade with Western merchants in Canton. Until its abolition after the Opium War in 1842, the Canton Cohong system was the only permitted avenue of Western trade into China, and thus became a booming hub of international trade by the early eighteenth century. By the eighteenth century the most significant export China had was tea. British demand for tea increased exponentially up until they figured out how to grow it for themselves in the hills of northern India in the 1880s. By the end of the eighteenth century tea exports going through the Canton Cohong system amounted to one-tenth of the revenue from taxes collected from the British and nearly the entire revenue of the British East India Company and until the early nineteenth century tea comprised ninety percent of exports leaving Canton.

Chinese scholars, court academies, and local officials carried on late Ming dynasty strengths in astronomy, mathematics, and geography, as well as technologies in ceramics, metallurgy, water transport, printing. Contrary to stereotypes in some Western writing, 16th and 17th century Qing dynasty officials and literati eagerly explored the technology and science introduced by Jesuit missionaries. Manchu leaders employed Jesuits to use cannon and gunpowder to great effect in the conquest of China, and the court sponsored their research in astronomy. The aim of these efforts, however, was to reform and improve inherited science and technology, not to replace it.

Scientific knowledge advanced during the Qing, but there was not a change in the way this knowledge was organized or the way scientific evidence was defined or its truth tested. The powerful official Ruan Yuan at the end of the eighteenth and early nineteenth centuries, for instance, supported a community of scientists and compiled the "Chouren zhuan" (畴人传; Biographies of mathematical scientists), a collection of biographies that eventually included nearly 700 Chinese and over 200 Western scientists. His attempt to reconcile Chinese and the Western science introduced by the Jesuits by arguing that both had originated in ancient China did not succeed, but he did show that science could be conceived and practiced separately from humanistic scholarship. Those who studied the physical universe shared their findings with each other and identified themselves as men of science, but they did not have a separate and independent professional role with its own training and advancement. They were still literati.

The Opium Wars, however, demonstrated the power of steam engine and military technology that had only recently been put into practice in the West. During the Self-Strengthening Movement of the 1860s and 1870s Confucian officials in several coastal provinces established an industrial base in military technology. The introduction of railroads into China raised questions that were more political than technological. A British company built the twelve-mile Shanghai—Woosung line in 1876, obtaining the land under false pretenses, and it was soon torn up. Court officials feared local public opinion and that railways would help invaders, harm farmlands, and obstruct feng shui. To keep development in Chinese hands, the Qing government borrowed 34 billion taels of silver from foreign lenders for railway construction between 1894 and 1911. As late as 1900, only 292 miles were in operation, with 4000 more miles in the planning stage. Finally, 5,200 miles of railway were completed. The British and French After 1905 were finally able to open lines to Burma and Vietnam.

Protestant missionaries by the 1830s translated and printed Western science and medical textbooks. The textbooks found homes in the rapidly enlarging network of missionary schools, and universities. The textbooks opened learning open possibilities for the small number of Chinese students interested in science, and a very small number interested in technology. After 1900, Japan had a greater role in bringing modern science and technology to Chinese audiences but even then they reached chiefly the children of the rich landowning gentry, who seldom engaged in industrial careers.

Under the Qing, inherited forms of art flourished and innovations occurred at many levels and in many types. High levels of literacy, a successful publishing industry, prosperous cities, and the Confucian emphasis on cultivation all fed a lively and creative set of cultural fields.

By the end of the nineteenth century, national artistic and cultural worlds had begun to come to terms with the cosmopolitan culture of the West and Japan. The decision to stay within old forms or welcome Western models was now a conscious choice rather than an unchallenged acceptance of tradition. Classically trained Confucian scholars such as Liang Qichao and Wang Guowei read widely and broke aesthetic and critical ground later cultivated in the New Culture Movement.

The Qing emperors were generally adept at poetry and often skilled in painting, and offered their patronage to Confucian culture. The Kangxi and Qianlong Emperors, for instance, embraced Chinese traditions both to control them and to proclaim their own legitimacy. The Kangxi Emperor sponsored the "Peiwen Yunfu", a rhyme dictionary published in 1711, and the "Kangxi Dictionary" published in 1716, which remains to this day an authoritative reference. The Qianlong Emperor sponsored the largest collection of writings in Chinese history, the "Siku Quanshu," completed in 1782. Court painters made new versions of the Song masterpiece, Zhang Zeduan's "Along the River During the Qingming Festival" whose depiction of a prosperous and happy realm demonstrated the beneficence of the emperor. The emperors undertook tours of the south and commissioned monumental scrolls to depict the grandeur of the occasion. Imperial patronage also encouraged the industrial production of ceramics and Chinese export porcelain. Peking glassware became popular after European glass making processes were introduced by Jesuits to Beijing.

Yet the most impressive aesthetic works were done among the scholars and urban elite. Calligraphy and painting remained a central interest to both court painters and scholar-gentry who considered the Four Arts part of their cultural identity and social standing. The painting of the early years of the dynasty included such painters as the orthodox Four Wangs and the individualists Bada Shanren (1626–1705) and Shitao (1641–1707). The nineteenth century saw such innovations as the Shanghai School and the Lingnan School which used the technical skills of tradition to set the stage for modern painting.

Traditional learning flourished, especially among Ming loyalists such as Dai Zhen and Gu Yanwu, but scholars in the school of evidential learning made innovations in skeptical textual scholarship. Scholar-bureaucrats, including Lin Zexu and Wei Yuan, developed a school of practical statecraft which rooted bureaucratic reform and restructuring in classical philosophy.

Philosophy and literature grew to new heights in the Qing period. Poetry continued as a mark of the cultivated gentleman, but women wrote in larger and larger numbers and came from all walks of life. The poetry of the Qing dynasty is a lively field of research, being studied (along with the poetry of the Ming dynasty) for its association with Chinese opera, developmental trends of Classical Chinese poetry, the transition to a greater role for vernacular language, and for poetry by women. The Qing dynasty was a period of literary editing and criticism, and many of the modern popular versions of Classical Chinese poems were transmitted through Qing dynasty anthologies, such as the Quan Tangshi and the "Three Hundred Tang Poems". Although fiction did not have the prestige of poetry, novels flourished. Pu Songling brought the short story to a new level in his "Strange Stories from a Chinese Studio", published in the mid-18th century, and Shen Fu demonstrated the charm of the informal memoir in "Six Chapters of a Floating Life", written in the early 19th century but published only in 1877. The art of the novel reached a pinnacle in Cao Xueqin's "Dream of the Red Chamber", but its combination of social commentary and psychological insight were echoed in highly skilled novels such as Wu Jingzi's "Rulin waishi" (1750) and Li Ruzhen's "Flowers in the Mirror" (1827).

In drama, Kong Shangren's Kunqu opera "The Peach Blossom Fan", completed in 1699, portrayed the tragic downfall of the Ming dynasty in romantic terms. The most prestigious form became the so-called Peking opera, though local and folk opera were also widely popular.

Cuisine aroused a cultural pride in the richness of a long and varied past. The gentleman gourmet, such as Yuan Mei, applied aesthetic standards to the art of cooking, eating, and appreciation of tea at a time when New World crops and products entered everyday life. Yuan's "Suiyuan Shidan" expounded culinary aesthetics and theory, along with a range of recipes. The Manchu Han Imperial Feast originated at the court. Although this banquet was probably never common, it reflected an appreciation of Manchu culinary customs. Nevertheless, culinary traditionalists such as Yuan Mei lambasted the opulence of the Manchu Han Feast. Yuan wrote that the feast was caused in part by the "vulgar habits of bad chefs" and that "displays this trite are useful only for welcoming new relations through one's gates or when the boss comes to visit". (皆惡廚陋習。只可用之於新親上門，上司入境)

After 1912, writers, historians and scholars in China and abroad generally deprecated the failures of the late imperial system. However, in the 21st century, a favorable view has emerged in popular culture. Building pride in Chinese history, nationalists have portrayed Imperial China as benevolent, strong and more advanced than the West. They blame ugly wars and diplomatic controversies on imperialist exploitation by Western nations and Japan. Although officially still communist and Maoist, in practice China's rulers have used this grassroots settlement to proclaim that their current policies are restoring China's historical glory. Chinese Communist Party General Secretary Xi Jinping has sought parity between Beijing and Washington and promised to restore China to its historical glory.

The New Qing History is a revisionist historiographical trend starting in the mid-1990s emphasizing the Manchu nature of the dynasty. Earlier historians had emphasized the power of Han Chinese to "sinicize" their conquerors, that is, to assimilate and make them Chinese in their thought and institutions. In the 1980s and early 1990s, American scholars began to learn Manchu and took advantage of newly opened Chinese- and Manchu-language documents in the archives. This research found that the Manchu rulers manipulated their subjects and from the 1630s through at least the 18th century, emperors developed a sense of Manchu identity and used Central Asian models of rule as much as they did Confucian ones. According to the new school the Manchu ruling class regarded "China" as only a part, although a very important part, of a much wider empire that extended into the Inner Asian territories of Mongolia, Tibet, the Manchuria and Xinjiang.

Ping-ti Ho criticized the new approach for exaggerating the Manchu character of the dynasty and argued for the sinification of its rule. Some scholars in China accused the American group of imposing American concerns with race and identity or even of imperialist misunderstanding to weaken China. Still others in China agree that this scholarship has opened new vistas for the study of Qing history.

The "New Qing History" is not related to the "New Qing History", a multi-volume history of the Qing dynasty that was authorized by the Chinese State Council in 2003.





</doc>
<doc id="25312" url="https://en.wikipedia.org/wiki?curid=25312" title="Quantum gravity">
Quantum gravity

Quantum gravity (QG) is a field of theoretical physics that seeks to describe gravity according to the principles of quantum mechanics, and where quantum effects cannot be ignored, such as in the vicinity of black holes or similar compact astrophysical objects where the effects of gravity are strong.

Three of the four fundamental forces of physics are described within the framework of quantum mechanics and quantum field theory. The current understanding of the fourth force, gravity, is based on Albert Einstein's general theory of relativity, which is formulated within the entirely different framework of classical physics. However, that description is incomplete: describing the gravitational field of a black hole in the general theory of relativity, physical quantities such as the spacetime curvature diverge at the center of the black hole.

This signals the breakdown of the general theory of relativity and the need for a theory that goes beyond general relativity into the quantum. At distances very close to the center of the black hole (closer than the Planck length), quantum fluctuations of spacetime are expected to play an important role. To describe these quantum effects a theory of quantum gravity is needed. Such a theory should allow the description to be extended closer to the center and might even allow an understanding of physics at the center of a black hole. On more formal grounds one can argue that a classical system cannot consistently be coupled to a quantum one.

The field of quantum gravity is actively developing and theorists are exploring a variety of approaches to the problem of quantum gravity, the most popular approaches being string theory and loop quantum gravity. 
All these approaches aim to describe the quantum behavior of the gravitational field. This does not necessarily include unifying all fundamental interactions into a single mathematical framework. However, many approaches to quantum gravity, such as string theory, try to develop a framework that describes all fundamental forces. Such theories are often referred to as a theory of everything. Others, such as loop quantum gravity, make no such attempt; instead, they make an effort to quantize the gravitational field while it is kept separate from the other forces.

One of the difficulties of formulating a quantum gravity theory is that quantum gravitational effects only appear at length scales near the Planck scale, around 10 meters, a scale far smaller, and hence only accessible with far higher energies, than those currently available in high energy particle accelerators. Therefore, physicists lack experimental data which could distinguish between the competing theories which have been proposed and thus thought experiment approaches are suggested as a testing tool for these theories.

Much of the difficulty in meshing these theories at all energy scales comes from the different assumptions that these theories make on how the universe works. General relativity models gravity as curvature of spacetime: in the slogan of John Archibald Wheeler, "Spacetime tells matter how to move; matter tells spacetime how to curve." On the other hand, quantum field theory is typically formulated in the "flat" spacetime used in special relativity. No theory has yet proven successful in describing the general situation where the dynamics of matter, modeled with quantum mechanics, affect the curvature of spacetime. If one attempts to treat gravity as simply another quantum field, the resulting theory is not renormalizable. Even in the simpler case where the curvature of spacetime is fixed "a priori," developing quantum field theory becomes more mathematically challenging, and many ideas physicists use in quantum field theory on flat spacetime are no longer applicable.

It is widely hoped that a theory of quantum gravity would allow us to understand problems of very high energy and very small dimensions of space, such as the behavior of black holes, and the origin of the universe.

The observation that all fundamental forces except gravity have one or more known messenger particles leads researchers to believe that at least one must exist for gravity. This hypothetical particle is known as the "graviton". These particles act as a force particle similar to the photon of the electromagnetic interaction. Under mild assumptions the structure of general relativity requires them to follow the quantum mechanical description of interacting theoretical spin-2 massless particles.
Many of the accepted notions of a unified theory of physics since the 1970s assume, and to some degree depend upon, the existence of the graviton. 
The Weinberg–Witten theorem places some constraints on theories in which the graviton is a composite particle.
While gravitons are an important theoretical step in a quantum mechanical description of gravity, they are generally believed to be indetectable because they interact too weakly.

General relativity, like electromagnetism, is a classical field theory. One might expect that, as with electromagnetism, the gravitational force should also have a corresponding quantum field theory.

However, gravity is perturbatively nonrenormalizable. For a quantum field theory to be well defined according to this understanding of the subject, it must be asymptotically free or asymptotically safe. The theory must be characterized by a choice of "finitely many" parameters, which could, in principle, be set by experiment. For example, in quantum electrodynamics these parameters are the charge and mass of the electron, as measured at a particular energy scale.

On the other hand, in quantizing gravity there are, in perturbation theory, "infinitely many independent parameters" (counterterm coefficients) needed to define the theory. For a given choice of those parameters, one could make sense of the theory, but since it is impossible to conduct infinite experiments to fix the values of every parameter, it has been argued that one does not, in perturbation theory, have a meaningful physical theory. At low energies, the logic of the renormalization group tells us that, despite the unknown choices of these infinitely many parameters, quantum gravity will reduce to the usual Einstein theory of general relativity. On the other hand, if we could probe very high energies where quantum effects take over, then "every one" of the infinitely many unknown parameters would begin to matter, and we could make no predictions at all.

It is conceivable that, in the correct theory of quantum gravity, the infinitely many unknown parameters will reduce to a finite number that can then be measured. One possibility is that normal perturbation theory is not a reliable guide to the renormalizability of the theory, and that there really "is" a UV fixed point for gravity. Since this is a question of non-perturbative quantum field theory, finding a reliable answer is difficult, pursued in the asymptotic safety program. Another possibility is that there are new, undiscovered symmetry principles that constrain the parameters and reduce them to a finite set. This is the route taken by string theory, where all of the excitations of the string essentially manifest themselves as new symmetries.

In an effective field theory, all but the first few of the infinite set of parameters in a nonrenormalizable theory are suppressed by huge energy scales and hence can be neglected when computing low-energy effects. Thus, at least in the low-energy regime, the model is a predictive quantum field theory. Furthermore, many theorists argue that the Standard Model should be regarded as an effective field theory itself, with "nonrenormalizable" interactions suppressed by large energy scales and whose effects have consequently not been observed experimentally.

By treating general relativity as an effective field theory, one can actually make legitimate predictions for quantum gravity, at least for low-energy phenomena. An example is the well-known calculation of the tiny first-order quantum-mechanical correction to the classical Newtonian gravitational potential between two masses.

A fundamental lesson of general relativity is that there is no fixed spacetime background, as found in Newtonian mechanics and special relativity; the spacetime geometry is dynamic. While easy to grasp in principle, this is the hardest idea to understand about general relativity, and its consequences are profound and not fully explored, even at the classical level. To a certain extent, general relativity can be seen to be a relational theory, in which the only physically relevant information is the relationship between different events in space-time.

On the other hand, quantum mechanics has depended since its inception on a fixed background (non-dynamic) structure. In the case of quantum mechanics, it is time that is given and not dynamic, just as in Newtonian classical mechanics. In relativistic quantum field theory, just as in classical field theory, Minkowski spacetime is the fixed background of the theory.

String theory can be seen as a generalization of quantum field theory where instead of point particles, string-like objects propagate in a fixed spacetime background, although the interactions among closed strings give rise to space-time in a dynamical way.
Although string theory had its origins in the study of quark confinement and not of quantum gravity, it was soon discovered that the string spectrum contains the graviton, and that "condensation" of certain vibration modes of strings is equivalent to a modification of the original background. In this sense, string perturbation theory exhibits exactly the features one would expect of a perturbation theory that may exhibit a strong dependence on asymptotics (as seen, for example, in the AdS/CFT correspondence) which is a weak form of background dependence.

Loop quantum gravity is the fruit of an effort to formulate a background-independent quantum theory.

Topological quantum field theory provided an example of background-independent quantum theory, but with no local degrees of freedom, and only finitely many degrees of freedom globally. This is inadequate to describe gravity in 3+1 dimensions, which has local degrees of freedom according to general relativity. In 2+1 dimensions, however, gravity is a topological field theory, and it has been successfully quantized in several different ways, including spin networks.

Quantum field theory on curved (non-Minkowskian) backgrounds, while not a full quantum theory of gravity, has shown many promising early results. In an analogous way to the development of quantum electrodynamics in the early part of the 20th century (when physicists considered quantum mechanics in classical electromagnetic fields), the consideration of quantum field theory on a curved background has led to predictions such as black hole radiation.

Phenomena such as the Unruh effect, in which particles exist in certain accelerating frames but not in stationary ones, do not pose any difficulty when considered on a curved background (the Unruh effect occurs even in flat Minkowskian backgrounds). The vacuum state is the state with the least energy (and may or may not contain particles).
See Quantum field theory in curved spacetime for a more complete discussion.

A conceptual difficulty in combining quantum mechanics with general relativity arises from the contrasting role of time within these two frameworks. In quantum theories time acts as an independent background through which states evolve, with the Hamiltonian operator acting as the generator of infinitesimal translations of quantum states through time. In contrast, general relativity treats time as a dynamical variable which interacts directly with matter and moreover requires the Hamiltonian constraint to vanish, removing any possibility of employing a notion of time similar to that in quantum theory.

There are a number of proposed quantum gravity theories. Currently, there is still no complete and consistent quantum theory of gravity, and the candidate models still need to overcome major formal and conceptual problems. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests, although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.

The central idea of string theory is to replace the classical concept of a point particle in quantum field theory, with a quantum theory of one-dimensional extended objects: string theory. At the energies reached in current experiments, these strings are indistinguishable from point-like particles, but, crucially, different modes of oscillation of one and the same type of fundamental string appear as particles with different (electric and other) charges. In this way, string theory promises to be a unified description of all particles and interactions. The theory is successful in that one mode will always correspond to a graviton, the messenger particle of gravity; however, the price of this success are unusual features such as six extra dimensions of space in addition to the usual three for space and one for time.

In what is called the , it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity. As presently understood, however, string theory admits a very large number (10 by some estimates) of consistent vacua, comprising the so-called "string landscape". Sorting through this large family of solutions remains a major challenge.

Loop quantum gravity seriously considers general relativity's insight that spacetime is a dynamical field and is therefore a quantum object. Its second idea is that the quantum discreteness that determines the particle-like behavior of other field theories (for instance, the photons of the electromagnetic field) also affects the structure of space.

The main result of loop quantum gravity is the derivation of a granular structure of space at the Planck length. This is derived from following considerations: In the case of electromagnetism, the quantum operator representing the energy of each frequency of the field has a discrete spectrum. Thus the energy of each frequency is quantized, and the quanta are the photons. In the case of gravity, the operators representing the area and the volume of each surface or space region likewise have discrete spectrum. Thus area and volume of any portion of space are also quantized, where the quanta are elementary quanta of space. It follows, then, that spacetime has an elementary quantum granular structure at the Planck scale, which cuts off the ultraviolet infinities of quantum field theory.

The quantum state of spacetime is described in the theory by means of a mathematical structure called spin networks. Spin networks were initially introduced by Roger Penrose in abstract form, and later shown by Carlo Rovelli and Lee Smolin to derive naturally from a non-perturbative quantization of general relativity. Spin networks do not represent quantum states of a field in spacetime: they represent directly quantum states of spacetime.

The theory is based on the reformulation of general relativity known as Ashtekar variables, which represent geometric gravity using mathematical analogues of electric and magnetic fields.
In the quantum theory, space is represented by a network structure called a spin network, evolving over time in discrete steps.

The dynamics of the theory is today constructed in several versions. One version starts with the canonical quantization of general relativity. The analogue of the Schrödinger equation is a Wheeler–DeWitt equation, which can be defined within the theory.
In the covariant, or spinfoam formulation of the theory, the quantum dynamics is obtained via a sum over discrete versions of spacetime, called spinfoams. These represent histories of spin networks.

There are a number of other approaches to quantum gravity. The approaches differ depending on which features of general relativity and quantum theory are accepted unchanged, and which features are modified. Examples include:

As was emphasized above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, the possibility of experimentally testing quantum gravity had not received much attention prior to the late 1990s. However, in the past decade, physicists have realized that evidence for quantum gravitational effects can guide the development of the theory. Since theoretical development has been slow, the field of phenomenological quantum gravity, which studies the possibility of experimental tests, has obtained increased attention.

The most widely pursued possibilities for quantum gravity phenomenology include violations of Lorentz invariance, imprints of quantum gravitational effects in the cosmic microwave background (in particular its polarization), and decoherence induced by fluctuations in the space-time foam.

ESA's INTEGRAL satellite measured polarization of photons of different wavelengths and was able to place a limit in the granularity of space that is less than 10⁻⁴⁸m or 13 orders of magnitude below the Planck scale .

The BICEP2 experiment detected what was initially thought to be primordial B-mode polarization caused by gravitational waves in the early universe. Had the signal in fact been primordial in origin, it could have been an indication of quantum gravitational effects, but it soon transpired that the polarization was due to interstellar dust interference.

As explained above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, thought experiments are becoming an important theoretical tool.
An important aspect of quantum gravity relates to the question of the coupling of spin and spacetime.
While spin and spacetime are expected to be coupled, the precise nature of this coupling is currently unknown. In particular and most importantly, it is not known how quantum spin sources gravity and what is the correct characterization of the spacetime of a single spin-half particle.
To analyze this question, thought experiments in the context of quantum information, have been suggested.
This work shows that, in order to avoid violation of relativistic causality, the measurable spacetime around a spin-half particle's (rest frame) must be spherically symmetric - i.e., either spacetime is spherically symmetric, or somehow measurements of the spacetime (e.g., time-dilation measurements) should create some sort of back action that affects and changes the quantum spin.




</doc>
<doc id="25315" url="https://en.wikipedia.org/wiki?curid=25315" title="Quality of service">
Quality of service

Quality of service (QoS) is the description or measurement of the overall performance of a service, such as a telephony or computer network or a cloud computing service, particularly the performance seen by the users of the network. To quantitatively measure quality of service, several related aspects of the network service are often considered, such as packet loss, bit rate, throughput, transmission delay, availability, jitter, etc.

In the field of computer networking and other packet-switched telecommunication networks, quality of service refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priorities to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.

Quality of service is particularly important for the transport of traffic with special requirements. In particular, developers have introduced Voice over IP technology to allow computer networks to become as useful as telephone networks for audio conversations, as well as supporting new applications with even stricter network performance requirements.

In the field of telephony, quality of service was defined by the ITU in 1994. Quality of service comprises requirements on all the aspects of a connection, such as service response time, loss, signal-to-noise ratio, crosstalk, echo, interrupts, frequency response, loudness levels, and so on. A subset of telephony QoS is grade of service (GoS) requirements, which comprises aspects of a connection relating to capacity and coverage of a network, for example guaranteed maximum blocking probability and outage probability.

In the field of computer networking and other packet-switched telecommunication networks, teletraffic engineering refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priorities to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow. For example, a required bit rate, delay, delay variation, packet loss or bit error rates may be guaranteed. Quality of service is important for real-time streaming multimedia applications such as voice over IP, multiplayer online games and IPTV, since these often require fixed bit rate and are delay sensitive. Quality of service is especially important in networks where the capacity is a limited resource, for example in cellular data communication.

A network or protocol that supports QoS may agree on a traffic contract with the application software and reserve capacity in the network nodes, for example during a session establishment phase. During the session it may monitor the achieved level of performance, for example the data rate and delay, and dynamically control scheduling priorities in the network nodes. It may release the reserved capacity during a tear down phase.

A best-effort network or service does not support quality of service. An alternative to complex QoS control mechanisms is to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load. The resulting absence of network congestion reduces or eliminates the need for QoS mechanisms.

QoS is sometimes used as a quality measure, with many alternative definitions, rather than referring to the ability to reserve resources. Quality of service sometimes refers to the level of quality of service, i.e. the guaranteed service quality. High QoS is often confused with a high level of performance, for example high bit rate, low latency and low bit error rate.

QoS is sometimes used in application layer services such as telephony and streaming video to describe a metric that reflects or predicts the subjectively experienced quality. In this context, QoS is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service. Other terms with similar meaning are the quality of experience (QoE), mean opinion score (MOS), perceptual speech quality measure (PSQM) and perceptual evaluation of video quality (PEVQ). See also Subjective video quality.

A number of attempts for layer 2 technologies that add QoS tags to the data have gained popularity in the past. Examples are frame relay, asynchronous transfer mode (ATM) and multiprotocol label switching (MPLS) (a technique between layer 2 and 3). Despite these network technologies remaining in use today, this kind of network lost attention after the advent of Ethernet networks. Today Ethernet is, by far, the most popular layer 2 technology. Conventional Internet routers and LAN switches operate on a best effort basis. This equipment is less expensive, less complex and faster and thus more popular than earlier more complex technologies that provide QoS mechanisms.

Ethernet optionally uses 802.1p to signal the priority of a frame.

There were four "type of service" bits and three "precedence" bits originally provided in each IP packet header, but they were not generally respected. These bits were later re-defined as Differentiated services code points (DSCP).

With the advent of IPTV and IP telephony, QoS mechanisms are increasingly available to the end user.

In packet-switched networks, quality of service is affected by various factors, which can be divided into human and technical factors. Human factors include: stability of service quality, availability of service, waiting times and user information. Technical factors include: reliability, scalability, effectiveness, maintainability and network congestion.

Many things can happen to packets as they travel from origin to destination, resulting in the following problems as seen from the point of view of the sender and receiver:


A defined quality of service may be desired or required for certain types of network traffic, for example:

These types of service are called "inelastic", meaning that they require a certain minimum bit rate and a certain maximum latency to function. By contrast, "elastic" applications can take advantage of however much or little bandwidth is available. Bulk file transfer applications that rely on TCP are generally elastic.

Circuit switched networks, especially those intended for voice transmission, such as Asynchronous Transfer Mode (ATM) or GSM, have QoS in the core protocol, resources are reserved at each step on the network for the call as it is set up, there is no need for additional procedures to achieve required performance. Shorter data units and built-in QoS were some of the unique selling points of ATM for applications such as video on demand.

When the expense of mechanisms to provide QoS is justified, network customers and providers can enter into a contractual agreement termed a service-level agreement (SLA) which specifies guarantees for the ability of a connection to give guaranteed performance in terms of throughput or latency based on mutually agreed measures.

An alternative to complex QoS control mechanisms is to provide high quality communication by generously over-provisioning a network so that capacity is based on peak traffic load estimates. This approach is simple for networks with predictable peak loads. This calculation may need to appreciate demanding applications that can compensate for variations in bandwidth and delay with large receive buffers, which is often possible for example in video streaming.

Over-provisioning can be of limited use in the face of transport protocols (such as TCP) that over time increase the amount of data placed on the network until all available bandwidth is consumed and packets are dropped. Such greedy protocols tend to increase latency and packet loss for all users.

The amount of over-provisioning in interior links required to replace QoS depends on the number of users and their traffic demands. This limits usability of over-provisioning. Newer more bandwidth intensive applications and the addition of more users results in the loss of over-provisioned networks. This then requires a physical update of the relevant network links which is an expensive process. Thus over-provisioning cannot be blindly assumed on the Internet.

Commercial VoIP services are often competitive with traditional telephone service in terms of call quality even without QoS mechanisms in use on the user's connection to their ISP and the VoIP provider's connection to a different ISP. Under high load conditions, however, VoIP may degrade to cell-phone quality or worse. The mathematics of packet traffic indicate that network requires just 60% more raw capacity under conservative assumptions.

Unlike single-owner networks, the Internet is a series of exchange points interconnecting private networks. Hence the Internet's core is owned and managed by a number of different network service providers, not a single entity. Its behavior is much more unpredictable.

There are two principal approaches to QoS in modern packet-switched IP networks, a parameterized system based on an exchange of application requirements with the network, and a prioritized system where each packet identifies a desired service level to the network.

Early work used the integrated services (IntServ) philosophy of reserving network resources. In this model, applications used RSVP to request and reserve resources through a network. While IntServ mechanisms do work, it was realized that in a broadband network typical of a larger service provider, Core routers would be required to accept, maintain, and tear down thousands or possibly tens of thousands of reservations. It was believed that this approach would not scale with the growth of the Internet, and in any event was antithetical to the end-to-end principle, the notion of designing networks so that core routers do little more than simply switch packets at the highest possible rates.

Under DiffServ, packets are marked either by the traffic sources themselves or by the edge devices where the traffic enters the network. In response to these markings, routers and switches use various queuing strategies to tailor performance to requirements. At the IP layer, DSCP markings use the 6 bit DS field in the IP packet header. At the MAC layer, VLAN IEEE 802.1Q can be used to carry 3 bit of essentially the same information. Routers and switches supporting DiffServ configure their network scheduler to use multiple queues for packets awaiting transmission from bandwidth constrained (e.g., wide area) interfaces. Router vendors provide different capabilities for configuring this behavior, to include the number of queues supported, the relative priorities of queues, and bandwidth reserved for each queue.

In practice, when a packet must be forwarded from an interface with queuing, packets requiring low jitter (e.g., VoIP or videoconferencing) are given priority over packets in other queues. Typically, some bandwidth is allocated by default to network control packets (such as Internet Control Message Protocol and routing protocols), while best-effort traffic might simply be given whatever bandwidth is left over.

At the Media Access Control (MAC) layer, VLAN IEEE 802.1Q and IEEE 802.1p can be used to distinguish between Ethernet frames and classify them. Queueing theory models have been developed on performance analysis and QoS for MAC layer protocols.

Cisco IOS NetFlow and the Cisco Class Based QoS (CBQoS) Management Information Base (MIB) are marketed by Cisco Systems.
One compelling example of the need for QoS on the Internet relates to congestive collapse. The Internet relies on congestion avoidance protocols, primarily as built into Transmission Control Protocol (TCP), to reduce traffic under conditions that would otherwise lead to congestive collapse. QoS applications, such as VoIP and IPTV, require largely constant bitrates and low latency, therefore they cannot use TCP and cannot otherwise reduce their traffic rate to help prevent congestion. Service-level agreements limit traffic that can be offered to the Internet and thereby enforce traffic shaping that can prevent it from becoming overloaded, and are hence an indispensable part of the Internet's ability to handle a mix of real-time and non-real-time traffic without collapse.

Several QoS mechanisms and schemes exist for IP networking.

QoS capabilities are available in the following network technologies.

End-to-end quality of service can require a method of coordinating resource allocation between one autonomous system and another. The Internet Engineering Task Force (IETF) defined the Resource Reservation Protocol (RSVP) for bandwidth reservation as a proposed standard in 1997. RSVP is an end-to-end bandwidth reservation and admission control protocol. RSVP was not widely adopted due to scalability limitations. The more scalable traffic engineering version, RSVP-TE, is used in many networks to establish traffic-engineered Multiprotocol Label Switching (MPLS) label-switched paths. The IETF also defined Next Steps in Signaling (NSIS) with QoS signalling as a target. NSIS is a development and simplification of RSVP.

Research consortia such as "end-to-end quality of service support over heterogeneous networks" (EuQoS, from 2004 through 2007) and fora such as the IPsphere Forum developed more mechanisms for handshaking QoS invocation from one domain to the next. IPsphere defined the Service Structuring Stratum (SSS) signaling bus in order to establish, invoke and (attempt to) assure network services. EuQoS conducted experiments to integrate Session Initiation Protocol, Next Steps in Signaling and IPsphere's SSS with an estimated cost of about 15.6 million Euro and published a book.

A research project Multi Service Access Everywhere (MUSE) defined another QoS concept in a first phase from January 2004 through February 2006, and a second phase from January 2006 through 2007. Another research project named PlaNetS was proposed for European funding circa 2005.
A broader European project called "Architecture and design for the future Internet" known as 4WARD had a budget estimated at 23.4 million Euro and was funded from January 2008 through June 2010.
It included a "Quality of Service Theme" and published a book. Another European project, called WIDENS (Wireless Deployable Network System), proposed a bandwidth reservation approach for mobile wireless multirate adhoc networks.

Strong cryptography network protocols such as Secure Sockets Layer, I2P, and virtual private networks obscure the data transferred using them. As all electronic commerce on the Internet requires the use of such strong cryptography protocols, unilaterally downgrading the performance of encrypted traffic creates an unacceptable hazard for customers. Yet, encrypted traffic is otherwise unable to undergo deep packet inspection for QoS.

Protocols like ICA and RDP may encapsulate other traffic (e.g. printing, video streaming) with varying requirements that can make optimization difficult.

The Internet2 project found, in 2001, that the QoS protocols were probably not deployable inside its Abilene Network with equipment available at that time. The group predicted that “logistical, financial, and organizational barriers will block the way toward any bandwidth guarantees” by protocol modifications aimed at QoS.
They believed that the economics would encourage network providers to deliberately erode the quality of best effort traffic as a way to push customers to higher priced QoS services. Instead they proposed over-provisioning of capacity as more cost-effective at the time.

The Abilene network study was the basis for the testimony of Gary Bachula to the US Senate Commerce Committee's hearing on Network Neutrality in early 2006. He expressed the opinion that adding more bandwidth was more effective than any of the various schemes for accomplishing QoS they examined. Bachula's testimony has been cited by proponents of a law banning quality of service as proof that no legitimate purpose is served by such an offering. This argument is dependent on the assumption that over-provisioning isn't a form of QoS and that it is always possible. Cost and other factors affect the ability of carriers to build and maintain permanently over-provisioned networks.

Mobile cellular service providers may offer mobile QoS to customers just as the wired public switched telephone network services providers and Internet service providers may offer QoS. QoS mechanisms are always provided for circuit switched services, and are essential for inelastic services, for example streaming multimedia.

Mobility adds complication to the QoS mechanisms, for several reasons:

Quality of service in the field of telephony was first defined in 1994 in the ITU-T Recommendation E.800. This definition is very broad, listing 6 primary components: Support, Operability, Accessibility, Retainability, Integrity and Security.
A 1995 recommendation X.902 included a definition is the OSI reference model.
In 1998 the ITU published a document discussing QoS in the field of data networking. X.641 offers a means of developing or enhancing standards related to QoS and provide concepts and terminology that will assist in maintaining the consistency of related standards.

Some QoS-related IETF Request For Comments (RFC)s are , and ; both these are discussed above. The IETF has also published two RFCs giving background on QoS: , and .

The IETF has also published as an informative or "best practices" document about the practical aspects of designing a QoS solution for a DiffServ network. They try to identify which types of applications are commonly run over an IP network to group them into traffic classes, study what treatment do each of these classes need from the network, and suggest which of the QoS mechanisms commonly available in routers can be used to implement those treatments.





</doc>
<doc id="25316" url="https://en.wikipedia.org/wiki?curid=25316" title="Quadrature amplitude modulation">
Quadrature amplitude modulation

Quadrature amplitude modulation (QAM) is the name of a family of digital modulation methods and a related family of analog modulation methods widely used in modern telecommunications to transmit information. It conveys two analog message signals, or two digital bit streams, by changing ("modulating") the amplitudes of two carrier waves, using the amplitude-shift keying (ASK) digital modulation scheme or amplitude modulation (AM) analog modulation scheme. The two carrier waves of the same frequency are out of phase with each other by 90°, a condition known as orthogonality or quadrature. The transmitted signal is created by adding the two carrier waves together. At the receiver, the two waves can be coherently separated (demodulated) because of their orthogonality property. Another key property is that the modulations are low-frequency/low-bandwidth waveforms compared to the carrier frequency, which is known as the narrowband assumption.

Phase modulation (analog PM) and phase-shift keying (digital PSK) can be regarded as a special case of QAM, where the amplitude of the transmitted signal is a constant, but its phase varies. This can also be extended to frequency modulation (FM) and frequency-shift keying (FSK), for these can be regarded as a special case of phase modulation.

QAM is used extensively as a modulation scheme for digital telecommunication systems, such as in 802.11 Wi-Fi standards. Arbitrarily high spectral efficiencies can be achieved with QAM by setting a suitable constellation size, limited only by the noise level and linearity of the communications channel.  QAM is being used in optical fiber systems as bit rates increase; QAM16 and QAM64 can be optically emulated with a 3-path interferometer.

In a QAM signal, one carrier lags the other by 90°, and its amplitude modulation is customarily referred to as the in-phase component, denoted by The other modulating function is the quadrature component, So the composite waveform is mathematically modeled as:

where is the carrier frequency.  At the receiver, a coherent demodulator multiplies the received signal separately with both a cosine and sine signal to produce the received estimates of and . For example:

Using standard trigonometric identities, we can write this as:

Low-pass filtering removes the high frequency terms (containing ), leaving only the term. This filtered signal is unaffected by showing that the in-phase component can be received independently of the quadrature component.  Similarly, we can multiply by a sine wave and then low-pass filter to extract 

The addition of two sinusoids is a linear operation that creates no new frequency components. So the bandwidth of the composite signal is comparable to the bandwidth of the DSB (Double-Sideband) components. Effectively, the spectral redundancy of DSB enables a doubling of the information capacity using this technique. This comes at the expense of demodulation complexity. In particular, a DSB signal has zero-crossings at a regular frequency, which makes it easy to recover the phase of the carrier sinusoid. It is said to be self-clocking. But the sender and receiver of a quadrature-modulated signal must share a clock or otherwise send a clock signal. If the clock phases drift apart, the demodulated "I" and "Q" signals bleed into each other, yielding crosstalk. In this context, the clock signal is called a "phase reference". Clock synchronization is typically achieved by transmitting a burst subcarrier or a pilot signal. The phase reference for NTSC, for example, is included within its colorburst signal.

Analog QAM is used in:

In the frequency domain, QAM has a similar spectral pattern to DSB-SC modulation. Applying Euler's formula to the sinusoids in , the positive-frequency portion of (or analytic representation) is:

where formula_5 denotes the Fourier transform, and and are the transforms of and This result represents the sum of two DSB-SC signals with the same center frequency. The factor of represents the 90° phase shift that enables their individual demodulations.

As in many digital modulation schemes, the constellation diagram is useful for QAM. In QAM, the constellation points are usually arranged in a square grid with equal vertical and horizontal spacing, although other configurations are possible (e.g. Cross-QAM). Since in digital telecommunications the data is usually binary, the number of points in the grid is usually a power of 2 (2, 4, 8, …). Since QAM is usually square, some of these are rare—the most common forms are 16-QAM, 64-QAM and 256-QAM. By moving to a higher-order constellation, it is possible to transmit more bits per symbol. However, if the mean energy of the constellation is to remain the same (by way of making a fair comparison), the points must be closer together and are thus more susceptible to noise and other corruption; this results in a higher bit error rate and so higher-order QAM can deliver more data less reliably than lower-order QAM, for constant mean constellation energy. Using higher-order QAM without increasing the bit error rate requires a higher signal-to-noise ratio (SNR) by increasing signal energy, reducing noise, or both.

If data-rates beyond those offered by 8-PSK are required, it is more usual to move to QAM since it achieves a greater distance between adjacent points in the I-Q plane by distributing the points more evenly. The complicating factor is that the points are no longer all the same amplitude and so the demodulator must now correctly detect both phase and amplitude, rather than just phase.

64-QAM and 256-QAM are often used in digital cable television and cable modem applications. In the United States, 64-QAM and 256-QAM are the mandated modulation schemes for digital cable (see QAM tuner) as standardised by the SCTE in the standard ANSI/SCTE 07 2013. Note that many marketing people will refer to these as QAM-64 and QAM-256. In the UK, 64-QAM is used for digital terrestrial television (Freeview) whilst 256-QAM is used for Freeview-HD.

Communication systems designed to achieve very high levels of spectral efficiency usually employ very dense QAM constellations. For example, current Homeplug AV2 500-Mbit/s powerline Ethernet devices use 1024-QAM and 4096-QAM, as well as future devices using ITU-T G.hn standard for networking over existing home wiring (coaxial cable, phone lines and power lines); 4096-QAM provides 12 bits/symbol. Another example is ADSL technology for copper twisted pairs, whose constellation size goes up to 32768-QAM (in ADSL terminology this is referred to as bit-loading, or bit per tone, 32768-QAM being equivalent to 15 bits per tone).

Ultra-high capacity Microwave Backhaul Systems also use 1024-QAM. With 1024-QAM, adaptive coding and modulation (ACM) and XPIC, vendors can obtain gigabit capacity in a single 56 MHz channel.

In moving to a higher order QAM constellation (higher data rate and mode) in hostile RF/microwave QAM application environments, such as in broadcasting or telecommunications, multipath interference typically increases. There is a spreading of the spots in the constellation, decreasing the separation between adjacent states, making it difficult for the receiver to decode the signal appropriately. In other words, there is reduced noise immunity. There are several test parameter measurements which help determine an optimal QAM mode for a specific operating environment. The following three are most significant:





</doc>
<doc id="25317" url="https://en.wikipedia.org/wiki?curid=25317" title="QAM (disambiguation)">
QAM (disambiguation)

QAM stands for Quadrature amplitude modulation

QAM may also refer to:



</doc>
<doc id="25319" url="https://en.wikipedia.org/wiki?curid=25319" title="Quetzalcoatlus">
Quetzalcoatlus

Quetzalcoatlus is a pterosaur known from the Late Cretaceous of North America (Maastrichtian stage) and one of the biggest known flying animals of all time. It is a member of the family Azhdarchidae, a family of advanced toothless pterosaurs with unusually long, stiffened necks. Its name comes from the Aztec feathered serpent god, Quetzalcoatl. The type and only species is Q. northropi.

The first "Quetzalcoatlus" fossils were discovered in Texas, United States, from the Maastrichtian Javelina Formation at Big Bend National Park (dated to around 68 million years ago) in 1971 by Douglas A. Lawson, a geology graduate student from the Jackson School of Geosciences at the University of Texas at Austin. The specimen consisted of a partial wing (in pterosaurs composed of the forearms and elongated fourth finger), from an individual later estimated at over in wingspan.

Lawson discovered a second site of the same age, about from the first, where between 1972 and 1974 he and Professor Wann Langston Jr. of the Texas Memorial Museum unearthed three fragmentary skeletons of much smaller individuals. Lawson in 1975 announced the find in an article in "Science". That same year, in a subsequent letter to the same journal, he made the original large specimen, TMM 41450-3, the holotype of a new genus and species, Quetzalcoatlus northropi. The genus name refers to the Aztec feathered serpent god, Quetzalcoatl. The specific name honors John Knudsen Northrop, the founder of Northrop, who drove the development of large tailless flying wing aircraft designs resembling "Quetzalcoatlus".
At first it was assumed that the smaller specimens were juvenile or subadult forms of the larger type. Later, when more remains were found, it was realized they could have been a separate species. This possible second species from Texas was provisionally referred to as a "Quetzalcoatlus" sp. by Alexander Kellner and Langston in 1996, indicating that its status was too uncertain to give it a full new species name. The smaller specimens are more complete than the "Q. northropi" holotype, and include four partial skulls, though they are much less massive, with an estimated wingspan of .

The holotype specimen of "Q. northropi" has yet to be properly described and diagnosed, and the current status of the genus "Quetzalcoatlus" has been identified as problematic. Mark Witton and colleagues (2010) noted that the type species of the genus—the fragmentary wing bones comprising "Q. northropi"—represent elements which are typically considered undiagnostic to generic or specific level, and that this complicates interpretations of azhdarchid taxonomy. For instance, Witton "et al." (2010) suggested that the "Q. northropi" type material is of generalized enough morphology to be near identical to that of other giant azhdarchids, such as the overlapping elements of the contemporary Romanian giant azhdarchid" Hatzegopteryx". This being the case, and assuming "Q. northropi" can be distinguished from other pterosaurs (i.e., if it is not a "nomen dubium"), perhaps "Hatzegopteryx" should be regarded as a European occurrence of "Quetzalcoatlus". However, Witton "et al." also noted that the skull material of "Hatzegopteryx" and "Q." sp. differ enough that they cannot be regarded as the same animal, but that the significance of this cannot be ascertained given uncertainty over the relationships of "Quetzalcoatlus" specimens. These issues can only be resolved by "Q. northropi" being demonstrated as a valid taxon and its relationships with "Q". sp. being investigated. An additional complication to these discussions are the likelihood that huge pterosaurs such as "Q. northropi" could have made long, transcontinental flights, suggesting that locations as disparate as North America and Europe could have shared giant azhdarchid species.

An azhdarchid neck vertebra, discovered in 2002 from the Maastrichtian age Hell Creek Formation, may also belong to "Quetzalcoatlus". The specimen (BMR P2002.2) was recovered accidentally when it was included in a field jacket prepared to transport part of a "Tyrannosaurus" specimen. Despite this association with the remains of a large carnivorous dinosaur, the vertebra shows no evidence that it was chewed on by the dinosaur. The bone came from an individual azhdarchid pterosaur estimated to have had a wingspan of .

When it was first named as a new species in 1975, scientists estimated that the largest "Quetzalcoatlus" fossils came from an individual with a wingspan as large as . Choosing the middle of three extrapolations from the proportions of other pterosaurs gave an estimate of 11 m, 15.5 m, and 21 m, respectively (36 ft, 50.85 ft, 68.9 ft). In 1981, further advanced studies lowered these estimates to .

More recent estimates based on greater knowledge of azhdarchid proportions place its wingspan at . Remains found in Texas in 1971 indicate that this reptile had a minimum wingspan of about . Generalized height in a bipedal stance, based on its wingspan, would have been at least high at the shoulder.

Weight estimates for giant azhdarchids are extremely problematic because no existing species share a similar size or body plan, and in consequence, published results vary widely. Generalized weight, based on some studies that have historically found extremely low weight estimates for "Quetzalcoatlus", was as low as for a individual. A majority of estimates published since the 2000s have been substantially higher, around .
Skull material (from smaller specimens, possibly a related species) shows that "Quetzalcoatlus" had a very sharp and pointed beak. That is contrary to some earlier reconstructions that showed a blunter snout, based on the inadvertent inclusion of jaw material from another pterosaur species, possibly a tapejarid or a form related to "Tupuxuara". A skull crest was also present but its exact form and size are still unknown.

Below is a cladogram showing the phylogenetic placement of "Quetzalcoatlus" within Neoazhdarchia from Andres and Myers (2013).

"Quetzalcoatlus" was abundant in Texas during the Lancian in a fauna dominated by "Alamosaurus". The "Alamosaurus"-"Quetzalcoatlus" association probably represents semi-arid inland plains. "Quetzalcoatlus" had precursors in North America and its apparent rise to widespreadness may represent the expansion of its preferred habitat rather than an immigration event, as some experts have suggested.

There have been a number of different ideas proposed about the lifestyle of "Quetzalcoatlus". Because the area of the fossil site was removed from the coastline and there were no indications of large rivers or deep lakes nearby at the end of the Cretaceous, Lawson in 1975 rejected a fish-eating lifestyle, instead suggesting that "Quetzalcoatlus" scavenged like the marabou stork (which will scavenge, but is more of a terrestrial predator of small animals), but then on the carcasses of titanosaur sauropods such as "Alamosaurus". Lawson had found the remains of the giant pterosaur while searching for the bones of this dinosaur, which formed an important part of its ecosystem.

In 1996, Lehman and Langston rejected the scavenging hypothesis, pointing out that the lower jaw bent so strongly downwards that even when it closed completely a gap of over remained between it and the upper jaw, very different from the hooked beaks of specialized scavenging birds. They suggested that with its long neck vertebrae and long toothless jaws "Quetzalcoatlus" fed like modern-day skimmers, catching fish during flight while cleaving the waves with its beak. While this skim-feeding view became widely accepted, it was not subjected to scientific research until 2007 when a study showed that for such large pterosaurs it was not a viable method because the energy costs would be too high due to excessive drag. In 2008 pterosaur workers Mark Witton and Darren Naish published an examination of possible feeding habits and ecology of azhdarchids. Witton and Naish noted that most azhdarchid remains are found in inland deposits far from seas or other large bodies of water required for skimming. Additionally, the beak, jaw, and neck anatomy are unlike those of any known skimming animal. Rather, they concluded that azhdarchids were more likely terrestrial stalkers, similar to modern storks, and probably hunted small vertebrates on land or in small streams. Though "Quetzalcoatlus", like other pterosaurs, was a quadruped when on the ground, "Quetzalcoatlus" and other azhdarchids have fore and hind limb proportions more similar to modern running ungulate mammals than to their smaller cousins, implying that they were uniquely suited to a terrestrial lifestyle.

The nature of flight in "Quetzalcoatlus" and other giant azhdarchids was poorly understood until serious biomechanical studies were conducted in the 21st century. One early (1984) experiment by Paul MacCready used practical aerodynamics to test the flight of "Quetzalcoatlus". MacCready constructed a model flying machine or ornithopter with a simple computer functioning as an autopilot. The model successfully flew with a combination of soaring and wing flapping; the model was based on a then-current weight estimate of around , far lower than more modern estimates of over . The method of flight in these pterosaurs depends largely on weight, which has been controversial, and widely differing masses have been favored by different scientists. Some researchers have suggested that these animals employed slow, soaring flight, while others have concluded that their flight was fast and dynamic. In 2010, Donald Henderson argued that the mass of "Q. northropi" had been underestimated, even the highest estimates, and that it was too massive to have achieved powered flight. He estimated it in his 2010 paper as . Henderson argued that it may have been flightless.

Other flight capability estimates have disagreed with Henderson's research, suggesting instead an animal superbly adapted to long-range, extended flight. In 2010, Mike Habib, a professor of biomechanics at Chatham University, and Mark Witton, a British paleontologist, undertook further investigation into the claims of flightlessness in large pterosaurs. After factoring wingspan, body weight, and aerodynamics, computer modeling led the two researchers to conclude that "Q. northropi" was capable of flight up to for 7 to 10 days at altitudes of . Habib further suggested a maximum flight range of for "Q. northropi". Henderson's work was also further criticized by Witton and Habib in another study, which pointed out that although Henderson used excellent mass estimations, they were based on outdated pterosaur models, which caused Henderson's mass estimations to be more than double what Habib used in his estimations, and that anatomical study of "Q. northropi" and other big pterosaur forelimbs showed a higher degree of robustness than would be expected if they were purely quadrupedal. This study proposed that large pterosaurs most likely utilized a short burst of powered flight to then transition to thermal soaring.

In 1975, artist Giovanni Casselli depicted "Quetzalcoatlus" as a small-headed scavenger with an extremely long neck in the book "The evolution and ecology of the Dinosaurs" by British paleontologist Beverly Halstead. Over the next twenty-five years prior to future discoveries, it would launch similar depictions colloquially known as a Paleomeme in various books as noted by Darren Naish.

In 1985, the US Defense Advanced Research Projects Agency (DARPA) and AeroVironment used "Quetzalcoatlus northropi" as the basis for an experimental ornithopter unmanned aerial vehicle (UAV). They produced a half-scale model weighing , with a wingspan of . Coincidentally, Douglas A. Lawson, who discovered "Q. northropi" in Texas in 1971, named it after John "Jack" Northrop, a developer of tailless flying wing aircraft in the 1940s. The replica of "Q. northropi" incorporates a "flight control system/autopilot which processes pilot commands and sensor inputs, implements several feedback loops, and delivers command signals to its various servo-actuators". It is on exhibit at the National Air and Space Museum.

In 2010, several life-sized models of "Q. northropi" were put on display on London's South Bank as the centerpiece exhibit for the Royal Society's 350th-anniversary exhibition. The models, which included both flying and standing individuals with wingspans of over , were intended to help build public interest in science. The models were created by scientists from the University of Portsmouth.




</doc>
<doc id="25320" url="https://en.wikipedia.org/wiki?curid=25320" title="Quedlinburg">
Quedlinburg

Quedlinburg () is a town situated just north of the Harz mountains, in the district of Harz in the west of Saxony-Anhalt, Germany. In 1994, the castle, church and old town were added to the UNESCO World Heritage List.

Quedlinburg has a population of more than 24,000. The town was the capital of the district of Quedlinburg until 2007, when the district was dissolved. Several locations in the town are designated stops along a scenic holiday route, the Romanesque Road.

The town of Quedlinburg is known to have existed since at least the early 9th century, when there was a settlement known as "Gross Orden" on the eastern bank of the River Bode. It was first mentioned as a town in 922 as part of a donation by King Henry the Fowler ("Heinrich der Vögler"). The records of this donation were held by the abbey of Corvey.

According to legend, Henry had been offered the German crown at Quedlinburg in 919 by Franconian nobles, giving rise to the town being called the "cradle of the German Reich".

After Henry's death in 936, his widow Saint Matilda founded a religious community for women ("Frauenstift") on the castle hill, where daughters of the higher nobility were educated. The main task of this collegiate foundation, Quedlinburg Abbey, was to pray for the memory of King Henry and the rulers who came after him. The "Annals of Quedlinburg" were also compiled there. The first abbess was Matilda, a granddaughter of King Henry and St. Matilda.

The Quedlinburg castle complex, founded by King Henry I and built up by Emperor Otto I in 936, was an imperial "Pfalz" of the Saxon emperors. The "Pfalz", including the male convent, was in the valley, where today the Roman Catholic Church of "St. Wiperti" is situated, while the women's convent was located on the castle hill.

In 973, shortly before the death of Emperor Otto I, a "Reichstag" (Imperial Convention) was held at the imperial court in which Mieszko, duke of Polans, and Boleslav, duke of Bohemia, as well as numerous other nobles from as far away as Byzantium and Bulgaria, gathered to pay homage to the emperor. On the occasion, Otto the Great introduced his new daughter-in-law Theophanu, a Byzantine princess whose marriage to Otto II brought hope for recognition and continued peace between the rulers of the Eastern and Western empires.

In 994, Otto III granted the right of market, tax, and coining, and established the first market place to the north of the castle hill.

The town became a member of the Hanseatic League in 1426. Quedlinburg Abbey frequently disputed the independence of the town, which sought the aid of the Bishopric of Halberstadt. In 1477, Abbess Hedwig, aided by her brothers Ernest and Albert, broke the resistance of the town and expelled the bishop's forces. Quedlinburg was forced to leave the Hanseatic League and was subsequently protected by the Electorate of Saxony. Both town and abbey converted to Lutheranism in 1539 during the Protestant Reformation.

In 1697, Elector Frederick Augustus I of Saxony sold his rights to Quedlinburg to Elector Frederick III of Brandenburg for 240,000 thalers. Quedlinburg Abbey contested Brandenburg-Prussia's claims throughout the 18th century, however. The abbey was secularized in 1802 during the German Mediatisation, and Quedlinburg passed to the Kingdom of Prussia as part of the Principality of Quedlinburg. Part of the Napoleonic Kingdom of Westphalia from 1807–13, it was included within the new Prussian Province of Saxony in 1815. In all this time, ladies ruled Quedlinburg as abbesses without "taking the veil"; they were free to marry. The last of these ladies was a Swedish princess, an early fighter for women's rights, Sofia Albertina.

During the Nazi regime, the memory of Henry I became a sort of cult, as Heinrich Himmler saw himself as the reincarnation of the "most German of all German" rulers. The collegiate church and castle were to be turned into a shrine for Nazi Germany. The Nazi Party tried to create a new religion. The cathedral was closed from 1938 and during the war. The local crematory was kept busy burning the victims of the Langenstein-Zwieberge concentration camp. Georg Ay was local party chief from 1931 until the end of the war. Liberation in 1945 brought back the Protestant bishop and the church bells, and the Nazi-style eagle was taken down from the tower.

During the last months of World War II, the United States military had occupied Quedlinburg. In the 1980s, upon the death of one of the US military men, the theft of medieval art from Quedlinburg came to light.

Quedlinburg was administered within Bezirk Halle while part of the Communist East Germany from 1949 to 1990. It became part of the state of Saxony-Anhalt upon German reunification in 1990.

During Quedlinburg's Communist era, restoration specialists from Poland were called in during the 1980s to carry out repairs on the old architecture. Today, Quedlinburg is a center of restoration of "Fachwerk" houses.
Quedlinburg is the setting for the acclaimed 2016 Frantz (film), serving as a quintessential small German town in the wake of WWI, home to the family who is reeling from the death of a son in the war.

The town is located north of the Harz mountains, about 123 m above NHN. The nearest mountains reach 181 m above NHN. The largest part of the town is located in the western part of the Bode river valley. This river comes from the Harz mountains and flows into the river Saale, a tributary of the river Elbe. The municipal area of Quedlinburg is . Before the incorporation of the two (previously independent) municipalities of Gernrode and Bad Suderode in January 2014 it was only .

Quedlinburg has an oceanic climate (Cfb) resulting from prevailing westerlies, blowing from the high-pressure area in the central Atlantic towards Scandinavia. Snowfall occurs almost every winter. January and February are the coldest months of the year, with an average temperature of 0.5 °C and 1.5 °C. July and August are the hottest months, with an average temperature of 17 °C (63 °F) and 18 °C (64 °F). The average annual precipitation is close to 438 mm with rain occurring usually from May to September. This precipitation is one of the lowest in Germany, which has an annual average close to 700 mm. In August 2010, Quedlinburg was the driest place in Germany, with only 72,4 l/m.

The mayor is Frank Ruch (CDU).

Quedlinburg is twinned with:

In the centre of the town are a wide selection of half-timbered buildings from at least five different centuries (including a 14th-century structure, one of Germany's oldest), while around the outer fringes of the old town are examples of "Jugendstil" buildings, dating from the late 19th and early 20th centuries.

The old town of Quedlinburg belongs to the largest in Germany with a size of around 90 hectares. 2000 half-timbered houses can be found here. The oldest, the “Ständerbau”, dates back from 1347.

Another famous building is called “Klopstockhaus”, the birthplace of poet Friedrich Gottlieb Klopstock.

Since December 1994, the old town of Quedlinburg and the castle mount with the "Stiftskirche" (collegiate church) are listed as one of UNESCO's World Heritage Sites. Quedlinburg is one of the best-preserved medieval and Renaissance towns in Europe, having escaped major damage in World War II.

In 2006, the Selke valley branch of the Harz Narrow Gauge Railways was extended to Quedlinburg from Gernrode, giving access to the historic steam narrow gauge railway, Alexisbad and the high Harz plateau.

The castle and "Stiftskirche St. Servatius" still dominate the town like in the early Middle Ages. The church is a prime example of German Romanesque style. The treasure of the church, containing ancient Christian religious artifacts and books, was stolen by an American soldier but brought back to Quedlinburg in 1993 and is again on display here.

The former "Stiftskirche St. Wiperti" was established in 936 when the "Kanonikerstift St. Wigpertus" (of male canons) was moved from the castle hill to make way for what became Quedlinburg Abbey. The church was built at the location of the first Ottonian Royal palace at Quedlinburg. Around 1020, a three-aisled crypt was added to the basilica. The crypt, which survived all later alterations to the church, is also a designated stop on the Romanesque Road today.

The nearest airports to Quedlinburg are Hannover, northwest, and Leipzig/Halle Airport, southeast. Much closer, but only served by a few airlines, is Magdeburg-Cochstedt. An airfield is located at Ballenstedt-Assmussstedt for general aviation.

Regional trains operated by Deutsche Bahn and the private Transdev company run on the standard-gauge Magdeburg–Thale line connecting Quedlinburg station with Magdeburg, Thale, and Halberstadt.

In 2006, the Selke Valley branch of the Harz Narrow Gauge Railways was extended into Quedlinburg from Gernrode, giving access via the historic steam-operated narrow-gauge railway to Alexisbad and the High Harz plateau.

Quedlinburg is connected by regional buses to the surrounding villages and small towns. Additionally, there are long-distance buses to Berlin.







</doc>
<doc id="25321" url="https://en.wikipedia.org/wiki?curid=25321" title="Quantization">
Quantization

Quantization, in general, is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers).

The terms quantization and "discretization" are often synonymous in denotation but not always in connotation.

Specifically, quantization may refer to:





</doc>
<doc id="25322" url="https://en.wikipedia.org/wiki?curid=25322" title="Quantum theory">
Quantum theory

Quantum theory may refer to:





</doc>
<doc id="25323" url="https://en.wikipedia.org/wiki?curid=25323" title="QRP operation">
QRP operation

In amateur radio, QRP operation refers to transmitting at reduced power while attempting to maximize one's effective range. QRP operation is a specialized pursuit within the hobby that was first popularized in the early 1920s. QRP operators generally limit their transmitted RF output power to 5 watts or less for CW operation and 10 watts or less for SSB operation. Reliable two-way communication at such low power levels can be challenging due to changing radio propagation and the difficulty of receiving the relatively weak transmitted signals. QRP enthusiasts may employ optimized antenna systems, enhanced operating skills, and a variety of special modes, in order to maximize their ability to make and maintain radio contact. Since the late 1960s, commercial transceivers specially designed for QRP operation have evolved from vacuum tube to solid state technology. A number of organizations dedicated to QRP operation exist, and aficionados participate in various contests designed to test their skill in making long-distance contacts at low power levels.

The term QRP derives from the standard Q code used in radio communications, where "QRP" and "QRP?" are used to request "Reduce power" and ask "Should I reduce power?" respectively. The opposite of QRP is QRO, or increased power operation.

Most amateur transceivers are capable of transmitting approximately 100 watts, but in some parts of the world, such as the U.S., amateurs can transmit up to 1,500 watts. QRP enthusiasts contend that this is not always necessary, and doing so wastes power, increases the likelihood of causing interference to nearby televisions, radios, and telephones and, for United States' amateurs, is incompatible with FCC Part 97 rule, which states that one must use "the minimum power necessary to carry out the desired communications". QRP can also be used for emergency communications during disaster recovery.

The practice of operating with low power was popularized as early as 1924, with a variety of reports, editorials and articles published in U.S. amateur radio magazines and journals that encouraged amateurs to lower power output, both for purposes of experimentation, and for improving operating conditions by reducing interference.

There is not complete agreement on what constitutes QRP power. Most amateur organizations agree that for CW, AM, FM, and data modes, the transmitter output power should be 5 watts (or less). The maximum output power for SSB (single sideband) is not always agreed upon. Some believe that the power should be no more than 10 watts peak envelope power (PEP), while others strongly hold that the power limit should be 5 watts. QRPers are known to use even less than five watts, sometimes operating with as little as 100 milliwatts or even less. Extremely low power—1 watt and below—is often referred to by hobbyists as QRPp.

Communicating using QRP can be difficult since the QRPer must face the same challenges of radio propagation faced by amateurs using higher power levels, but with the inherent disadvantages associated with having a weaker signal on the receiving end, all other things being equal. QRP aficionados try to make up for this through more efficient antenna systems and enhanced operating skills.

QRP enthusiasts may use special modes that employ technology and software designed to enhance reception of the relatively weak transmitted signals resulting from low power levels.

Many of the larger, more powerful commercial transceivers permit the operator to lower their output level to QRP levels. Commercial transceivers specially designed to operate at or near QRP power levels have been commercially available since the late 1960s. In 1969 the American manufacturer Ten-Tec produced the Powermite-1, one of Ten-Tec's first assembled transceivers, and featured modular construction. All stages of the transceiver were on individual circuit boards: the transmitter was capable of about one or two watts of RF, and the receiver was a direct-conversion unit, similar to that found in the Heathkit HW-7 and HW-8 lines, which introduced many amateurs to QRP'ing and led to the popularity of the mode. Enthusiasts operate QRP radios on the HF bands in portable modes, usually carrying the radios in backpacks, with whip antennas. Some QRPers prefer to construct their equipment from kits, published plans, or homebrew it from scratch. Many popular designs are based on the NE612 mixer IC, i.e. the K1, K2, ATS series and the Softrock SDR.

Amateur radio organizations dedicated to QRP include QRP Amateur Radio Club International (QRPARCI), American QRP Club, G-QRP Club based in the United Kingdom, and The Adventure Radio Society emphasizing portable QRP operation. Major QRP gatherings are held yearly at hamfests such as Dayton Hamvention, Pacificon, and Frederichshafen.

There are specific operating awards, contests, clubs, and conventions devoted to QRP enthusiasts. In the United States, the November Sweepstakes, June and September VHF QSO Parties, January VHF Sweepstakes, and the ARRL International DX Contest, as well as many major international contests have designated special QRP categories. For example, during the annual ARRL's Field Day contest, making a QSO (ham-to-ham contact) using "QRP battery power" is worth five times as many points as a contact made by conventional means. The QRP ARCI club sponsors 12 contests during the year specifically for QRP operators.

Typical awards include the QRP ARCI club's "thousand-miles-per-watt" award, available to anyone presenting evidence of a qualifying contact. QRP ARCI also offers special awards for achieving the ARRL's Worked All States, Worked All Continents, and DX Century Club awards under QRP conditions. Other QRP clubs also offer similar versions of these awards, as well as general QRP operating achievement awards.


</doc>
<doc id="25327" url="https://en.wikipedia.org/wiki?curid=25327" title="QCD (disambiguation)">
QCD (disambiguation)

QCD, or Quantum chromodynamics, is the theory of the strong interaction between quarks and gluons.

QCD may also refer to:


</doc>
<doc id="25328" url="https://en.wikipedia.org/wiki?curid=25328" title="Quicksilver">
Quicksilver

Quicksilver may refer to:













</doc>
<doc id="25330" url="https://en.wikipedia.org/wiki?curid=25330" title="Quartet">
Quartet

In music, a quartet or quartette (, , , , ) is an ensemble of four singers or instrumental performers; or a musical composition for four voices or instruments.

In Classical music, one of the most common combinations of four instruments in chamber music is the string quartet. String quartets most often consist of two violins, a viola, and a cello. The particular choice and number of instruments derives from the registers of the human voice: soprano, alto, tenor and bass. In the string quartet, two violins play the soprano and alto vocal registers, the viola plays the tenor register and the cello plays the bass register.

Composers of notable string quartets include Joseph Haydn (68 compositions), Wolfgang Amadeus Mozart (23), Ludwig van Beethoven (16), Franz Schubert (15), Felix Mendelssohn (6), Johannes Brahms (3), Antonín Dvořák (14), Alexander Borodin (2), Béla Bartók (6), Elizabeth Maconchy (13), Darius Milhaud (18), Heitor Villa-Lobos (17), and Dmitri Shostakovich (15). The Italian composer Luigi Boccherini (1743–1805), wrote 91 string quartets.

Less often, string quartets are written for other combinations of the standard string ensemble. These include quartets for one violin, two violas, and one cello, notably by Carl Stamitz (6 compositions) and others; and for one violin, one viola, and two cellos, by Johann Georg Albrechtsberger and others.

Another common standard classical quartet is the piano quartet, consisting of violin, viola, cello, and piano. Romantic composers Beethoven, Brahms, and Mendelssohn each wrote three important compositions in this form, and Mozart, Dvořák, and Gabriel Fauré each wrote two.

Wind quartets are scored either the same as a string quartet with the wind instrument replacing the first violin (i.e. scored for wind, violin, viola and cello) or are groups of four wind instruments. Among the latter, the SATB format woodwind quartet of flute, oboe, clarinet, and bassoon is relatively common.

An example of a wind quartet featuring four of the same types of wind instruments is the saxophone quartet, consisting of soprano saxophone, alto saxophone, tenor saxophone and baritone saxophone or (SATB). Often a second alto may be substituted for the soprano part (AATB) or a bass saxophone may be substituted for the baritone.

Compositions for four singers have been written for quartets a cappella; accompanied by instruments, such as a piano; and accompanied by larger vocal forces, such as a choir. Brahms and Schubert wrote numerous pieces for four voices that were once popular in private salons, although they are seldom performed today. Vocal quartets also feature within larger classical compositions, such as opera, choral works, and symphonic compositions. The final movement of Beethoven's Ninth Symphony and the Verdi Requiem are two examples of renowned concert works that include vocal quartets.

Typically, a vocal quartet is composed of:

The baroque quartet is a form of music composition similar to the trio sonata, but with four music parts performed by three solo melodic instruments and basso continuo. The solo instruments could be strings or wind instruments.

Examples of baroque quartets are Telemann's Paris quartets.

Quartets are popular in jazz and jazz fusion music. Jazz quartet ensembles are often composed of a horn, classically clarinet (or saxophone, trumpet, etc.), a chordal instrument (e.g., electric guitar, piano, Hammond organ, etc.), a bass instrument (e.g., double bass, tuba or bass guitar) and a drum kit. This configuration is sometimes modified by using a second horn replacing the chordal instrument, such as a trumpet and saxophone with string bass and drum kit, or by using two chordal instruments (e.g., piano and electric guitar).

In 20th century Western popular music, the term "vocal quartet" usually refers to ensembles of four singers of the same gender. This is particularly common for barbershop quartets and Gospel quartets.

Some well-known female US vocal quartets include The Carter Sisters; The Forester Sisters; The Chiffons; The Chordettes; The Lennon Sisters; and En Vogue. Some well-known male US vocal quartets include The Oak Ridge Boys; The Statler Brothers; The Ames Brothers; The Chi-Lites; Crosby Stills Nash & Young; The Dixie Hummingbirds; The Four Aces; Four Freshmen; The Four Seasons; The Four Tops; The Cathedral Quartet; Ernie Haase and Signature Sound; The Golden Gate Quartet; The Hilltoppers; The Jordanaires; and Mills Brothers. The only known U.S. drag quartet is The Kinsey Sicks. Some mixed-gender vocal quartets include The Pied Pipers; The Mamas & the Papas; The Merry Macs; and The Weavers.

The quartet lineup also is very common in pop and rock music. A standard quartet formation in pop and rock music is an ensemble consisting of two electric guitars, a bass guitar, and a drum kit. This configuration is sometimes modified by using a keyboard instrument (e.g., organ, piano, synthesizer) or a soloing instrument (e.g., saxophone) in place of the second electric guitar.

A Russian folk-instrument quartet commonly consists of a bayan, a prima balalaika, a prima or alto domra, and a contrabass balalaika (e.g., Quartet Moskovskaya Balalaika). Configurations without a bayan include a prima domra, a prima balalaika, an alto domra, and a bass balalaika (Quartet Skaz); or two prima domras, a prima balalaika, and a bass balalaika. 



</doc>
<doc id="25336" url="https://en.wikipedia.org/wiki?curid=25336" title="Quantum entanglement">
Quantum entanglement

Quantum entanglement is a physical phenomenon that occurs when a pair or group of particles is generated, interact, or share spatial proximity in a way such that the quantum state of each particle of the pair or group cannot be described independently of the state of the others, including when the particles are separated by a large distance. The topic of quantum entanglement is at the heart of the disparity between classical and quantum physics: entanglement is a primary feature of quantum mechanics lacking in classical mechanics.

Measurements of physical properties such as position, momentum, spin, and polarization performed on entangled particles can, in some cases, be found to be perfectly correlated. For example, if a pair of entangled particles is generated such that their total spin is known to be zero, and one particle is found to have clockwise spin on a first axis, then the spin of the other particle, measured on the same axis, will be found to be counterclockwise. However, this behavior gives rise to seemingly paradoxical effects: any measurement of a property of a particle results in an irreversible wave function collapse of that particle and will change the original quantum state. In the case of entangled particles, such a measurement will affect the entangled system as a whole.

Such phenomena were the subject of a 1935 paper by Albert Einstein, Boris Podolsky, and Nathan Rosen, and several papers by Erwin Schrödinger shortly thereafter, describing what came to be known as the EPR paradox. Einstein and others considered such behavior to be impossible, as it violated the local realism view of causality (Einstein referring to it as "spooky action at a distance") and argued that the accepted formulation of quantum mechanics must therefore be incomplete.

Later, however, the counterintuitive predictions of quantum mechanics were verified experimentally in tests in which polarization or spin of entangled particles were measured at separate locations, statistically violating Bell's inequality. In earlier tests, it couldn't be absolutely ruled out that the test result at one point could have been subtly transmitted to the remote point, affecting the outcome at the second location. However, so-called "loophole-free" Bell tests have been performed in which the locations were separated such that communications at the speed of light would have taken longer--in one case 10,000 times longer—than the interval between the measurements.

According to "some" interpretations of quantum mechanics, the effect of one measurement occurs instantly. Other interpretations which don't recognize wavefunction collapse dispute that there is any "effect" at all. However, all interpretations agree that entanglement produces "correlation" between the measurements and that the mutual information between the entangled particles can be exploited, but that any "transmission" of information at faster-than-light speeds is impossible.

Quantum entanglement has been demonstrated experimentally with photons, neutrinos, electrons, molecules as large as buckyballs, and even small diamonds. The utilization of entanglement in communication, computation and quantum radar is a very active area of research and development.

The counterintuitive predictions of quantum mechanics about strongly correlated systems were first discussed by Albert Einstein in 1935, in a joint paper with Boris Podolsky and Nathan Rosen.
In this study, the three formulated the Einstein–Podolsky–Rosen paradox (EPR paradox), a thought experiment that attempted to show that quantum mechanical theory was incomplete. They wrote: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete."

However, the three scientists did not coin the word "entanglement", nor did they generalize the special properties of the state they considered. Following the EPR paper, Erwin Schrödinger wrote a letter to Einstein in German in which he used the word "Verschränkung" (translated by himself as "entanglement") "to describe the correlations between two particles that interact and then separate, as in the EPR experiment."

Schrödinger shortly thereafter published a seminal paper defining and discussing the notion of "entanglement." In the paper, he recognized the importance of the concept, and stated: "I would not call [entanglement] "one" but rather "the" characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought."

Like Einstein, Schrödinger was dissatisfied with the concept of entanglement, because it seemed to violate the speed limit on the transmission of information implicit in the theory of relativity. Einstein later famously derided entanglement as ""spukhafte Fernwirkung"" or "spooky action at a distance."

The EPR paper generated significant interest among physicists, which inspired much discussion about the foundations of quantum mechanics (perhaps most famously Bohm's interpretation of quantum mechanics), but produced relatively little other published work. Despite the interest, the weak point in EPR's argument was not discovered until 1964, when John Stewart Bell proved that one of their key assumptions, the principle of locality, as applied to the kind of hidden variables interpretation hoped for by EPR, was mathematically inconsistent with the predictions of quantum theory.

Specifically, Bell demonstrated an upper limit, seen in Bell's inequality, regarding the strength of correlations that can be produced in any theory obeying local realism, and showed that quantum theory predicts violations of this limit for certain entangled systems. His inequality is experimentally testable, and there have been numerous relevant experiments, starting with the pioneering work of Stuart Freedman and John Clauser in 1972 and Alain Aspect's experiments in 1982. An early experimental breakthrough was due to Carl Kocher, who already in 1967 presented an apparatus in which two photons successively emitted from a calcium atom were shown to be entangled – the first case of entangled visible light. The two photons passed diametrically positioned parallel polarizers with higher probability than classically predicted but with correlations in quantitative agreement with quantum mechanical calculations. He also showed that the correlation varied only upon (as cosine square of) the angle between the polarizer settings and decreased exponentially with time lag between emitted photons. Kocher’s apparatus, equipped with better polarizers, was used by Freedman and Clauser who could confirm the cosine square dependence and use it to demonstrate a violation of Bell’s inequality for a set of fixed angles. All these experiments have shown agreement with quantum mechanics rather than the principle of local realism.

For decades, each had left open at least one loophole by which it was possible to question the validity of the results. However, in 2015 an experiment was performed that simultaneously closed both the detection and locality loopholes, and was heralded as "loophole-free"; this experiment ruled out a large class of local realism theories with certainty. Alain Aspect notes that the "setting-independence loophole" – which he refers to as "far-fetched", yet, a "residual loophole" that "cannot be ignored" – has yet to be closed, and the free-will / "superdeterminism" loophole is unclosable; saying "no experiment, as ideal as it is, can be said to be totally loophole-free."

A minority opinion holds that although quantum mechanics is correct, there is no superluminal instantaneous action-at-a-distance between entangled particles once the particles are separated.

Bell's work raised the possibility of using these super-strong correlations as a resource for communication. It led to the 1984 discovery of quantum key distribution protocols, most famously BB84 by Charles H. Bennett and Gilles Brassard and E91 by Artur Ekert. Although BB84 does not use entanglement, Ekert's protocol uses the violation of a Bell's inequality as a proof of security.

An entangled system is defined to be one whose quantum state cannot be factored as a product of states of its local constituents; that is to say, they are not individual particles but are an inseparable whole. In entanglement, one constituent cannot be fully described without considering the other(s). The state of a composite system is always expressible as a sum, or superposition, of products of states of local constituents; it is entangled if this sum necessarily has more than one term.

Quantum systems can become entangled through various types of interactions. For some ways in which entanglement may be achieved for experimental purposes, see the section below on methods. Entanglement is broken when the entangled particles decohere through interaction with the environment; for example, when a measurement is made.

As an example of entanglement: a subatomic particle decays into an entangled pair of other particles. The decay events obey the various conservation laws, and as a result, the measurement outcomes of one daughter particle must be highly correlated with the measurement outcomes of the other daughter particle (so that the total momenta, angular momenta, energy, and so forth remains roughly the same before and after this process). For instance, a spin-zero particle could decay into a pair of spin-½ particles. Since the total spin before and after this decay must be zero (conservation of angular momentum), whenever the first particle is measured to be spin up on some axis, the other, when measured on the same axis, is always found to be spin down. (This is called the spin anti-correlated case; and if the prior probabilities for measuring each spin are equal, the pair is said to be in the singlet state.)

The special property of entanglement can be better observed if we separate the said two particles. Let's put one of them in the White House in Washington and the other in Buckingham Palace (think about this as a thought experiment, not an actual one). Now, if we measure a particular characteristic of one of these particles (say, for example, spin), get a result, and then measure the other particle using the same criterion (spin along the same axis), we find that the result of the measurement of the second particle will match (in a complementary sense) the result of the measurement of the first particle, in that they will be opposite in their values.

The above result may or may not be perceived as surprising. A classical system would display the same property, and a hidden variable theory (see below) would certainly be required to do so, based on conservation of angular momentum in classical and quantum mechanics alike. The difference is that a classical system has definite values for all the observables all along, while the quantum system does not. In a sense to be discussed below, the quantum system considered here seems to acquire a probability distribution for the outcome of a measurement of the spin along any axis of the other particle upon measurement of the first particle. This probability distribution is in general different from what it would be without measurement of the first particle. This may certainly be perceived as surprising in the case of spatially separated entangled particles.

The paradox is that a measurement made on either of the particles apparently collapses the state of the entire entangled system—and does so instantaneously, before any information about the measurement result could have been communicated to the other particle (assuming that information cannot travel faster than light) and hence assured the "proper" outcome of the measurement of the other part of the entangled pair. In the Copenhagen interpretation, the result of a spin measurement on one of the particles is a collapse into a state in which each particle has a definite spin (either up or down) along the axis of measurement. The outcome is taken to be random, with each possibility having a probability of 50%. However, if both spins are measured along the same axis, they are found to be anti-correlated. This means that the random outcome of the measurement made on one particle seems to have been transmitted to the other, so that it can make the "right choice" when it too is measured.

The distance and timing of the measurements can be chosen so as to make the interval between the two measurements spacelike, hence, any causal effect connecting the events would have to travel faster than light. According to the principles of special relativity, it is not possible for any information to travel between two such measuring events. It is not even possible to say which of the measurements came first. For two spacelike separated events and there are inertial frames in which is first and others in which is first. Therefore, the correlation between the two measurements cannot be explained as one measurement determining the other: different observers would disagree about the role of cause and effect.

A possible resolution to the paradox is to assume that quantum theory is incomplete, and the result of measurements depends on predetermined "hidden variables". The state of the particles being measured contains some hidden variables, whose values effectively determine, right from the moment of separation, what the outcomes of the spin measurements are going to be. This would mean that each particle carries all the required information with it, and nothing needs to be transmitted from one particle to the other at the time of measurement. Einstein and others (see the previous section) originally believed this was the only way out of the paradox, and the accepted quantum mechanical description (with a random measurement outcome) must be incomplete.

Local hidden variable theories fail, however, when measurements of the spin of entangled particles along different axes are considered. If a large number of pairs of such measurements are made (on a large number of pairs of entangled particles), then statistically, if the local realist or hidden variables view were correct, the results would always satisfy Bell's inequality. A number of experiments have shown in practice that Bell's inequality is not satisfied. However, prior to 2015, all of these had loophole problems that were considered the most important by the community of physicists. When measurements of the entangled particles are made in moving relativistic reference frames, in which each measurement (in its own relativistic time frame) occurs before the other, the measurement results remain correlated.

The fundamental issue about measuring spin along different axes is that these measurements cannot have definite values at the same time―they are incompatible in the sense that these measurements' maximum simultaneous precision is constrained by the uncertainty principle. This is contrary to what is found in classical physics, where any number of properties can be measured simultaneously with arbitrary accuracy. It has been proven mathematically that compatible measurements cannot show Bell-inequality-violating correlations, and thus entanglement is a fundamentally non-classical phenomenon.
In experiments in 2012 and 2013, polarization correlation was created between photons that never coexisted in time. The authors claimed that this result was achieved by entanglement swapping between two pairs of entangled photons after measuring the polarization of one photon of the early pair, and that it proves that quantum non-locality applies not only to space but also to time.

In three independent experiments in 2013 it was shown that classically communicated separable quantum states can be used to carry entangled states. The first loophole-free Bell test was held in TU Delft in 2015 confirming the violation of Bell inequality.

In August 2014, Brazilian researcher Gabriela Barreto Lemos and team were able to "take pictures" of objects using photons that had not interacted with the subjects, but were entangled with photons that did interact with such objects. Lemos, from the University of Vienna, is confident that this new quantum imaging technique could find application where low light imaging is imperative, in fields like biological or medical imaging.

In 2015, Markus Greiner's group at Harvard performed a direct measurement of Renyi entanglement in a system of ultracold bosonic atoms.

From 2016 various companies like IBM, Microsoft etc. have successfully created quantum computers and allowed developers and tech enthusiasts to openly experiment with concepts of quantum mechanics including quantum entanglement.

There have been suggestions to look at the concept of time as an emergent phenomenon that is a side effect of quantum entanglement.
In other words, time is an entanglement phenomenon, which places all equal clock readings (of correctly prepared clocks, or of any objects usable as clocks) into the same history. This was first fully theorized by Don Page and William Wootters in 1983.
The Wheeler–DeWitt equation that combines general relativity and quantum mechanics – by leaving out time altogether – was introduced in the 1960s and it was taken up again in 1983, when Page and Wootters made a solution based on quantum entanglement. Page and Wootters argued that entanglement can be used to measure time.

In 2013, at the Istituto Nazionale di Ricerca Metrologica (INRIM) in Turin, Italy, researchers performed the first experimental test of Page and Wootters' ideas. Their result has been interpreted to confirm that time is an emergent phenomenon for internal observers but absent for external observers of the universe just as the Wheeler-DeWitt equation predicts.

Physicist Seth Lloyd says that quantum uncertainty gives rise to entanglement, the putative source of the arrow of time. According to Lloyd; "The arrow of time is an arrow of increasing correlations." The approach to entanglement would be from the perspective of the causal arrow of time, with the assumption that the cause of the measurement of one particle determines the effect of the result of the other particle's measurement.

Based on AdS/CFT correspondence, Mark Van Raamsdonk suggested that spacetime arises as an emergent phenomenon of the quantum degrees of freedom that are entangled and live in the boundary of the space-time. Induced gravity can emerge from the entanglement first law.

In the media and popular science, quantum non-locality is often portrayed as being equivalent to entanglement. While this is true for pure bipartite quantum states, in general entanglement is only necessary for non-local correlations, but there exist mixed entangled states that do not produce such correlations. A well-known example is the Werner states that are entangled for certain values of formula_1, but can always be described using local hidden variables. Moreover, it was shown that, for arbitrary numbers of parties, there exist states that are genuinely entangled but admit a local model.
The mentioned proofs about the existence of local models assume that there is only one copy of the quantum state available at a time. If the parties are allowed to perform local measurements on many copies of such states, then many apparently local states (e.g., the qubit Werner states) can no longer be described by a local model. This is, in particular, true for all distillable states. However, it remains an open question whether all entangled states become non-local given sufficiently many copies.

In short, entanglement of a state shared by two parties is necessary but not sufficient for that state to be non-local. It is important to recognize that entanglement is more commonly viewed as an algebraic concept, noted for being a prerequisite to non-locality as well as to quantum teleportation and to superdense coding, whereas non-locality is defined according to experimental statistics and is much more involved with the foundations and interpretations of quantum mechanics.

The following subsections are for those with a good working knowledge of the formal, mathematical description of quantum mechanics, including familiarity with the formalism and theoretical framework developed in the articles: bra–ket notation and mathematical formulation of quantum mechanics.

Consider two noninteracting systems and , with respective Hilbert spaces and . The Hilbert space of the composite system is the tensor product

If the first system is in state formula_3 and the second in state formula_4, the state of the composite system is

States of the composite system that can be represented in this form are called separable states, or product states.

Not all states are separable states (and thus product states). Fix a basis formula_6 for and a basis formula_7 for . The most general state in is of the form

This state is separable if there exist vectors formula_9 so that formula_10 yielding formula_11 and formula_12 It is inseparable if for any vectors formula_13 at least for one pair of coordinates formula_14 we have formula_15 If a state is inseparable, it is called an 'entangled state'.

For example, given two basis vectors formula_16 of and two basis vectors formula_17 of , the following is an entangled state:

If the composite system is in this state, it is impossible to attribute to either system or system a definite pure state. Another way to say this is that while the von Neumann entropy of the whole state is zero (as it is for any pure state), the entropy of the subsystems is greater than zero. In this sense, the systems are "entangled". This has specific empirical ramifications for interferometry. The above example is one of four Bell states, which are (maximally) entangled pure states (pure states of the space, but which cannot be separated into pure states of each and ).

Now suppose Alice is an observer for system , and Bob is an observer for system . If in the entangled state given above Alice makes a measurement in the formula_19 eigenbasis of , there are two possible outcomes, occurring with equal probability:


If the former occurs, then any subsequent measurement performed by Bob, in the same basis, will always return 1. If the latter occurs, (Alice measures 1) then Bob's measurement will return 0 with certainty. Thus, system has been altered by Alice performing a local measurement on system . This remains true even if the systems and are spatially separated. This is the foundation of the EPR paradox.

The outcome of Alice's measurement is random. Alice cannot decide which state to collapse the composite system into, and therefore cannot transmit information to Bob by acting on her system. Causality is thus preserved, in this particular scheme. For the general argument, see no-communication theorem.

As mentioned above, a state of a quantum system is given by a unit vector in a Hilbert space. More generally, if one has less information about the system, then one calls it an 'ensemble' and describes it by a density matrix, which is a positive-semidefinite matrix, or a trace class when the state space is infinite-dimensional, and has trace 1. Again, by the spectral theorem, such a matrix takes the general form:

where the "w" are positive-valued probabilities (they sum up to 1), the vectors are unit vectors, and in the infinite-dimensional case, we would take the closure of such states in the trace norm. We can interpret as representing an ensemble where is the proportion of the ensemble whose states are formula_23. When a mixed state has rank 1, it therefore describes a 'pure ensemble'. When there is less than total information about the state of a quantum system we need density matrices to represent the state.

Experimentally, a mixed ensemble might be realized as follows. Consider a "black box" apparatus that spits electrons towards an observer. The electrons' Hilbert spaces are identical. The apparatus might produce electrons that are all in the same state; in this case, the electrons received by the observer are then a pure ensemble. However, the apparatus could produce electrons in different states. For example, it could produce two populations of electrons: one with state formula_24 with spins aligned in the positive direction, and the other with state formula_25 with spins aligned in the negative direction. Generally, this is a mixed ensemble, as there can be any number of populations, each corresponding to a different state.

Following the definition above, for a bipartite composite system, mixed states are just density matrices on . That is, it has the general form

where the "w" are positively valued probabilities, formula_27, and the vectors are unit vectors. This is self-adjoint and positive and has trace 1.

Extending the definition of separability from the pure case, we say that a mixed state is separable if it can be written as

where the are positively valued probabilities and the formula_29's and formula_30's are themselves mixed states (density operators) on the subsystems and respectively. In other words, a state is separable if it is a probability distribution over uncorrelated states, or product states. By writing the density matrices as sums of pure ensembles and expanding, we may assume without loss of generality that formula_29 and formula_30 are themselves pure ensembles. A state is then said to be entangled if it is not separable.

In general, finding out whether or not a mixed state is entangled is considered difficult. The general bipartite case has been shown to be NP-hard. For the and cases, a necessary and sufficient criterion for separability is given by the famous Positive Partial Transpose (PPT) condition.

The idea of a reduced density matrix was introduced by Paul Dirac in 1930. Consider as above systems and each with a Hilbert space . Let the state of the composite system be

As indicated above, in general there is no way to associate a pure state to the component system . However, it still is possible to associate a density matrix. Let

which is the projection operator onto this state. The state of is the partial trace of over the basis of system :

For example, the reduced density matrix of for the entangled state

discussed above is

This demonstrates that, as expected, the reduced density matrix for an entangled pure ensemble is a mixed ensemble. Also not surprisingly, the density matrix of for the pure product state formula_38 discussed above is

In general, a bipartite pure state ρ is entangled if and only if its reduced states are mixed rather than pure.

Reduced density matrices were explicitly calculated in different spin chains with unique ground state. An example is the one-dimensional AKLT spin chain: the ground state can be divided into a block and an environment. The reduced density matrix of the block is proportional to a projector to a degenerate ground state of another Hamiltonian.

The reduced density matrix also was evaluated for XY spin chains, where it has full rank. It was proved that in the thermodynamic limit, the spectrum of the reduced density matrix of a large block of spins is an exact geometric sequence in this case.

In quantum information theory, entangled states are considered a 'resource', i.e., something costly to produce and that allows to implement valuable transformations. The setting in which this perspective is most evident is that of "distant labs", i.e., two quantum systems labeled "A" and "B" on each of which arbitrary quantum operations can be performed, but which do not interact with each other quantum mechanically. The only interaction allowed is the exchange of classical information, which combined with the most general local quantum operations gives rise to the class of operations called LOCC (local operations and classical communication). These operations do not allow the production of entangled states between the systems A and B. But if A and B are provided with a supply of entangled states, then these, together with LOCC operations can enable a larger class of transformations. For example, an interaction between a qubit of A and a qubit of B can be realized by first teleporting A's qubit to B, then letting it interact with B's qubit (which is now a LOCC operation, since both qubits are in B's lab) and then teleporting the qubit back to A. Two maximally entangled states of two qubits are used up in this process. Thus entangled states are a resource that enables the realization of quantum interactions (or of quantum channels) in a setting where only LOCC are available, but they are consumed in the process. There are other applications where entanglement can be seen as a resource, e.g., private communication or distinguishing quantum states.

Not all quantum states are equally valuable as a resource. To quantify this value, different entanglement measures (see below) can be used, that assign a numerical value to each quantum state. However, it is often interesting to settle for a coarser way to compare quantum states. This gives rise to different classification schemes. Most entanglement classes are defined based on whether states can be converted to other states using LOCC or a subclass of these operations. The smaller the set of allowed operations, the finer the classification. Important examples are:

A different entanglement classification is based on what the quantum correlations present in a state allow A and B to do: one distinguishes three subsets of entangled states: (1) the "non-local states", which produce correlations that cannot be explained by a local hidden variable model and thus violate a Bell inequality, (2) the "steerable states" that contain sufficient correlations for A to modify ("steer") by local measurements the conditional reduced state of B in such a way, that A can prove to B that the state they possess is indeed entangled, and finally (3) those entangled states that are neither non-local nor steerable. All three sets are non-empty.

In this section, the entropy of a mixed state is discussed as well as how it can be viewed as a measure of quantum entanglement.

In classical information theory , the Shannon entropy, is associated to a probability distribution,formula_50, in the following way:

Since a mixed state is a probability distribution over an ensemble, this leads naturally to the definition of the von Neumann entropy:

In general, one uses the Borel functional calculus to calculate a non-polynomial function such as . If the nonnegative operator acts on a finite-dimensional Hilbert space and has eigenvalues formula_53, turns out to be nothing more than the operator with the same eigenvectors, but the eigenvalues formula_54. The Shannon entropy is then:

Since an event of probability 0 should not contribute to the entropy, and given that

the convention is adopted. This extends to the infinite-dimensional case as well: if has spectral resolution

assume the same convention when calculating

As in statistical mechanics, the more uncertainty (number of microstates) the system should possess, the larger the entropy. For example, the entropy of any pure state is zero, which is unsurprising since there is no uncertainty about a system in a pure state. The entropy of any of the two subsystems of the entangled state discussed above is (which can be shown to be the maximum entropy for mixed states).

Entropy provides one tool that can be used to quantify entanglement, although other entanglement measures exist. If the overall system is pure, the entropy of one subsystem can be used to measure its degree of entanglement with the other subsystems.

For bipartite pure states, the von Neumann entropy of reduced states is the unique measure of entanglement in the sense that it is the only function on the family of states that satisfies certain axioms required of an entanglement measure.

It is a classical result that the Shannon entropy achieves its maximum at, and only at, the uniform probability distribution {1/"n"...,1/"n"}. Therefore, a bipartite pure state is said to be a maximally entangled state if the reduced state of is the diagonal matrix

For mixed states, the reduced von Neumann entropy is not the only reasonable entanglement measure.

As an aside, the information-theoretic definition is closely related to entropy in the sense of statistical mechanics (comparing the two definitions in the present context, it is customary to set the Boltzmann constant ). For example, by properties of the Borel functional calculus, we see that for any unitary operator ,

Indeed, without this property, the von Neumann entropy would not be well-defined.

In particular, could be the time evolution operator of the system, i.e.,

where is the Hamiltonian of the system. Here the entropy is unchanged.

The reversibility of a process is associated with the resulting entropy change, i.e., a process is reversible if, and only if, it leaves the entropy of the system invariant. Therefore, the march of the arrow of time towards thermodynamic equilibrium is simply the growing spread of quantum entanglement.
This provides a connection between quantum information theory and thermodynamics.

Rényi entropy also can be used as a measure of entanglement.

Entanglement measures quantify the amount of entanglement in a (often viewed as a bipartite) quantum state. As aforementioned, entanglement entropy is the standard measure of entanglement for pure states (but no longer a measure of entanglement for mixed states). For mixed states, there are some entanglement measures in the literature and no single one is standard.
Most (but not all) of these entanglement measures reduce for pure states to entanglement entropy, and are difficult (NP-hard) to compute.

The Reeh-Schlieder theorem of quantum field theory is sometimes seen as an analogue of quantum entanglement.

Entanglement has many applications in quantum information theory. With the aid of entanglement, otherwise impossible tasks may be achieved.

Among the best-known applications of entanglement are superdense coding and quantum teleportation.

Most researchers believe that entanglement is necessary to realize quantum computing (although this is disputed by some).

Entanglement is used in some protocols of quantum cryptography. This is because the "shared noise" of entanglement makes for an excellent one-time pad. Moreover, since measurement of either member of an entangled pair destroys the entanglement they share, entanglement-based quantum cryptography allows the sender and receiver to more easily detect the presence of an interceptor.

In interferometry, entanglement is necessary for surpassing the standard quantum limit and achieving the Heisenberg limit.

There are several canonical entangled states that appear often in theory and experiments.

For two qubits, the Bell states are

These four pure states are all maximally entangled (according to the entropy of entanglement) and form an orthonormal basis (linear algebra) of the Hilbert space of the two qubits. They play a fundamental role in Bell's theorem.

For M>2 qubits, the GHZ state is

which reduces to the Bell state formula_65 for formula_66. The traditional GHZ state was defined for formula_67. GHZ states are occasionally extended to qudits, i.e., systems of "d" rather than 2 dimensions.

Also for M>2 qubits, there are spin squeezed states. Spin squeezed states are a class of squeezed coherent states satisfying certain restrictions on the uncertainty of spin measurements, and are necessarily entangled. Spin squeezed states are good candidates for enhancing precision measurements using quantum entanglement.

For two bosonic modes, a NOON state is

This is like the Bell state formula_69 except the basis kets 0 and 1 have been replaced with "the "N" photons are in one mode" and "the "N" photons are in the other mode".

Finally, there also exist twin Fock states for bosonic modes, which can be created by feeding a Fock state into two arms leading to a beam splitter. They are the sum of multiple of NOON states, and can used to achieve the Heisenberg limit.

For the appropriately chosen measure of entanglement, Bell, GHZ, and NOON states are maximally entangled while spin squeezed and twin Fock states are only partially entangled. The partially entangled states are generally easier to prepare experimentally.

Entanglement is usually created by direct interactions between subatomic particles. These interactions can take numerous forms. One of the most commonly used methods is spontaneous parametric down-conversion to generate a pair of photons entangled in polarisation. Other methods include the use of a fiber coupler to confine and mix photons, photons emitted from decay cascade of the bi-exciton in a quantum dot, the use of the Hong–Ou–Mandel effect, etc., In the earliest tests of Bell's theorem, the entangled particles were generated using atomic cascades.

It is also possible to create entanglement between quantum systems that never directly interacted, through the use of entanglement swapping. Two independently prepared, identical particles may also be entangled if their wave functions merely spatially overlap, at least partially.

A density matrix ρ is called separable if it can be written as a convex sum of product states, namely

formula_70

with formula_71 probabilities. By definition, a state is entangled if it is not separable.

For 2-Qubit and Qubit-Qutrit systems (2 × 2 and 2 × 3 respectively) the simple Peres–Horodecki criterion provides both a necessary and a sufficient criterion for separability, and thus--inadvertently--for detecting entanglement. However, for the general case, the criterion is merely a necessary one for separability, as the problem becomes NP-hard when generalized. Other separability criteria include (but not limited to) the range criterion, reduction criterion, and those based on uncertainty relations. See Ref. for a review of separability criteria in discrete variable systems.

A numerical approach to the problem is suggested by Jon Magne Leinaas, Jan Myrheim and Eirik Ovrum in their paper "Geometrical aspects of entanglement". Leinaas et al. offer a numerical approach, iteratively refining an estimated separable state towards the target state to be tested, and checking if the target state can indeed be reached. An implementation of the algorithm (including a built-in Peres-Horodecki criterion testing) is "StateSeparator" web-app.

In continuous variable systems, the Peres-Horodecki criterion also applies. Specifically, Simon formulated a particular version of the Peres-Horodecki criterion in terms of the second-order moments of canonical operators and showed that it is necessary and sufficient for formula_72-mode Gaussian states (see Ref. for a seemingly different but essentially equivalent approach). It was later found that Simon's condition is also necessary and sufficient for formula_73-mode Gaussian states, but no longer sufficient for formula_74-mode Gaussian states. Simon's condition can be generalized by taking into account the higher order moments of canonical operators or by using entropic measures.

In 2016 China launched the world’s first quantum communications satellite. The $100m Quantum Experiments at Space Scale (QUESS) mission was launched on Aug 16, 2016, from the Jiuquan Satellite Launch Center in northern China at 01:40 local time.

For the next two years, the craft – nicknamed "Micius" after the ancient Chinese philosopher – will demonstrate the feasibility of quantum
communication between Earth and space, and test quantum entanglement over unprecedented distances.

In the June 16, 2017, issue of "Science", Yin et al. report setting a new quantum entanglement distance record of 1,203 km, demonstrating the survival of a two-photon pair and a violation of a Bell inequality, reaching a CHSH valuation of 2.37 ± 0.09, under strict Einstein locality conditions, from the Micius satellite to bases in Lijian, Yunnan and Delingha, Quinhai, increasing the efficiency of transmission over prior fiberoptic experiments by an order of magnitude.

The electron shells of multi-electron atoms always consist of entangled electrons. The correct ionization energy can be calculated only by consideration of electron entanglement.

It has been suggested that in the process of photosynthesis, entanglement is involved in the transfer of energy between light-harvesting complexes and photosynthetic reaction centers where light (energy) is harvested in the form of chemical energy. Without such a process, the efficient conversion of light into chemical energy cannot be explained. Using femtosecond spectroscopy, the coherence of entanglement in the Fenna-Matthews-Olson complex was measured over hundreds of femtoseconds (a relatively long time in this regard) providing support to this theory.
However, critical follow-up studies question the interpretation of these results and assign the reported signatures of electronic quantum coherence to nuclear dynamics in the chromophores.

In October 2018, physicists reported producing quantum entanglement using living organisms, particularly between living bacteria and quantized light.

Living organisms (green sulphur bacteria) have been studied as mediators to create quantum entanglement between otherwise non-interacting light modes, showing high entanglement between light and bacterial modes, and to some extent, even entanglement within the bacteria.




</doc>
<doc id="25343" url="https://en.wikipedia.org/wiki?curid=25343" title="Quasi-War">
Quasi-War

The Quasi-War () was an undeclared naval war fought from 1798 to 1800 between the United States and France. Most of the fighting took place in the Caribbean and off the Atlantic coast.

The war originated in disputes over the application of the 1778 treaties of Alliance and Commerce between the two countries. Then engaged in the 1792 to 1797 War of the First Coalition which included Great Britain, France viewed the 1794 Jay Treaty between the US and Britain as incompatible with those treaties, and retaliated by seizing American ships trading with Britain.

The US responded by suspending repayment of French loans from the American Revolutionary War; when diplomatic negotiations failed to resolve the issue, French privateers began attacking merchant ships in American waters. On July 7, 1798, Congress authorized the use of military force against French warships, and re-established the United States Navy, or USN.

The USN informally co-operated with the Royal Navy, chiefly in allowing merchant ships to join each other's convoys. As the British had four to five times the number of ships available, they focused on escort duties, enabling the US to concentrate on attacking French warships. Many of these battles involved famous naval officers such as Stephen Decatur, Silas Talbot and William Bainbridge; the combination allowed the US to quickly regain control of its home waters.

However, President John Adams continued diplomatic efforts to resolve underlying issues; this coincided with Napoleon taking power in France, who for various reasons was also keen to agree terms. This led to the Convention of 1800, which ended the war.

Under the Treaty of Alliance (1778), the United States agreed to protect French colonies in the Caribbean in return for their support in the American Revolutionary War. As the treaty had no termination date, this obligation technically included defending them from British and Dutch attacks during the 1792 to 1797 War of the First Coalition. Despite popular enthusiasm for the French Revolution, there was little support for this in Congress; neutrality allowed Northern shipowners to earn huge profits evading the British blockade, and Southern plantation-owners feared the example set by France's abolition of slavery in 1794.

Arguing the 1793 execution of Louis XVI voided existing agreements, the Neutrality Act of 1794 unilaterally cancelled the military obligations of the 1778 treaty. France accepted on the basis of 'benevolent neutrality', which meant allowing French privateers access to US ports, and the right to sell captured British ships in American prize courts, but not vice versa. It was soon apparent the US interpreted 'neutrality' as the right to trade with and provide the same privileges to both.

This conflict became apparent when the US agreed the 1794 Jay Treaty with Britain, which contradicted the 1778 Commercial Treaty with France. It resolved outstanding issues from the American Revolution, and expanded trade between the two countries; between 1794 and 1801, American exports nearly tripled in value, from US$33 million to $94 million. The treaty was opposed by the Jeffersonian Democratic-Republicans, who were generally pro-French and anti-British.

When France retaliated by seizing American ships trading with the British, an effective response was hampered by the almost complete lack of a United States Navy. Driven by Jeffersonian opposition to Federal institutions, its last warship had been sold in 1785, leaving only a small flotilla belonging to the United States Revenue Cutter Service and a few neglected coastal forts. This allowed French privateers to roam virtually unchecked; from October 1796 to June 1797, they captured 316 ships, 6% of the entire American merchant fleet, causing losses of $12 to $15 million.

Congress responded by suspending repayment of French loans made during the Revolutionary War; efforts to resolve this through diplomacy ended in the 1797 dispute known as the XYZ Affair. However, it created support for establishing a limited naval force, and on June 18, President John Adams appointed Benjamin Stoddert the first Secretary of the Navy. On July 7, 1798, Congress approved the use of force against French warships in American waters.

Since ships of the line were extremely expensive and required highly specialised construction facilities, Congress compromised by ordering six large frigates in 1794. Three were nearly complete by 1798, and on July 16, 1798, they approved funding for the , , and , plus the frigates "USS General Greene" and "USS Adams". The provision of naval stores and equipment by the British allowed these to be built relatively quickly, and all saw action during the war.

The USN was further reinforced by so-called 'subscription ships', privately funded vessels provided by individual cities. These included five frigates, among them the "USS Philadelphia", commanded by Stephen Decatur, and four sloops, which were converted merchantmen. These privately funded vessels were noted for their speed, and very successful; the "USS Boston" captured over 80 enemy vessels, including the French corvette Berceau.

With most of the French fleet confined to home ports by the Royal Navy, Secretary Stoddert was able to concentrate his forces against the limited number of frigates and smaller vessels that evaded the blockade and reached the Caribbean. The other need was for convoy protection, and while there was no formal agreement with the British, there was considerable co-operation at a local level. The two navies shared a signal system, and allowed their merchantmen to join each other's convoys, mostly provided by the British, since they had four to five times more escorts available.

However, the biggest threat came from small privateers, carrying between one and twenty guns and of very shallow draft. Operating from French and Spanish bases in the Caribbean, particularly Guadeloupe, they made opportunistic attacks on passing ships, before escaping back into port. To combat this, the US used similar sized vessels from the United States Revenue Cutter Service, as well as commissioning their own privateers. The first American ship to see action was the "USS Ganges", a converted East Indiaman with 26 guns; most were far smaller.

From the perspective of the USN, the Quasi-War consisted of a series of ship to ship actions in US coastal waters and the Caribbean; one of the first was the Capture of La Croyable on 7 July 1798, by outside Egg Harbor, New Jersey. On 20 November, a pair of French frigates, "Insurgente" and "Volontaire", captured the schooner , commanded by Lieutenant William Bainbridge; "Retaliation" would be recaptured on 28 June 1799.

On 9 February 1799, the frigate captured the French Navy's frigate "L'Insurgente" and severely damaged the frigate "La Vengeance", largely due to . By 1 July, under the command of Stephen Decatur, had been refitted and repaired and embarked on its mission to patrol the South Atlantic coast and West Indies in search of French ships which were preying on American merchant vessels.

On 1 January 1800, a convoy of American merchant ships and their escort, United States naval schooner , engaged a squadron of armed barges manned by French-allied Haitians known as picaroons off the coast of present-day Haiti. On 1 February, the American frigate unsuccessfully tried to capture the French frigate "La Vengeance" off the coast of Saint Kitts. In early May, Captain Silas Talbot organized a naval expedition to Puerto Plata on the island of Hispaniola in order to harass French shipping, capturing the Spanish coastal fort at Puerto Plata and a French corvette. Following the French invasion of Curaçao in July, the American sloops and began a blockade of the island in September that led to a French withdrawal. On 12 October, the frigate captured the corvette . On 25 October, the defeated the French brig "Flambeau" near the island of Dominica in the Caribbean Sea. "Enterprise" also captured eight privateers and freed eleven U.S. merchant ships from captivity, while captured the French privateers "Deux Amis" and "Diane" and liberated numerous American merchant ships.

American naval losses may have been light, but the French had successfully seized many American merchant ships by the war's end in 1800 – more than 2,000, according to one source.

Revenue cutters in the service of the American Revenue-Marine also took part in the conflict. The cutter USRC "Pickering", commanded by Edward Preble, made two cruises to the West Indies and captured ten prizes. Preble turned command of "Pickering" over to Benjamin Hillar, who captured the much larger and more heavily armed French privateer "lEgypte Conquise" after a nine-hour battle. In September 1800, Hillar, "Pickering", and her entire crew were lost at sea in a storm. Preble next commanded the frigate , which he sailed around Cape Horn into the Pacific to protect U.S. merchantmen in the East Indies. He recaptured several U.S. ships that had been seized by French privateers.

For various reasons, the support provided by the Royal Navy was minimised at the time, and since; its most significant contribution was convoying merchant shipping, freeing the USN to attack French privateers. As a result, the first significant study of the war by US naval historian Gardner W Allen in 1909 focused exclusively on ship to ship actions, and this is how the war is remembered.

In his work "Stoddert’s War: Naval Operations During the Quasi-War with France, 1798-1801", historian Michael Palmer writes;

By late 1800, the United States Navy and the Royal Navy, combined with a more conciliatory diplomatic stance by the government of First Consul Napoleon Bonaparte, had reduced the activity of the French privateers and warships. The Convention of 1800, signed on 30 September, ended the Quasi-War. It affirmed the rights of Americans as neutrals upon the sea and abrogated the alliance with France of 1778. However, it failed to provide compensation for the $20 million "French Spoliation Claims" of the United States. The agreement between the two nations implicitly ensured that the United States would remain neutral toward France in the wars of Napoleon and ended the "entangling" French alliance. This alliance had been viable only between 1778 and 1783.






</doc>
<doc id="25345" url="https://en.wikipedia.org/wiki?curid=25345" title="Quality management system">
Quality management system

A quality management system (QMS) is a collection of business processes focused on consistently meeting customer requirements and enhancing their satisfaction. It is aligned with an organization's purpose and strategic direction (ISO9001:2015). It is expressed as the organizational goals and aspirations, policies, processes, documented information and resources needed to implement and maintain it. Early quality management systems emphasized predictable outcomes of an industrial product production line, using simple statistics and random sampling. By the 20th century, labor inputs were typically the most costly inputs in most industrialized societies, so focus shifted to team cooperation and dynamics, especially the early signaling of problems via a continual improvement cycle. In the 21st century, QMS has tended to converge with sustainability and transparency initiatives, as both investor and customer satisfaction and perceived quality is increasingly tied to these factors. Of QMS regimes, the ISO 9000 family of standards is probably the most widely implemented worldwide – the ISO 19011 audit regime applies to both, and deals with quality and sustainability and their integration.

Other QMS, e.g. Natural Step, focus on sustainability issues and assume that other quality problems will be reduced as result of the systematic thinking, transparency, documentation and diagnostic discipline.

The term "Quality Management System" and the initialism "QMS" were invented in 1991 by Ken Croucher, a British management consultant working on designing and implementing a generic model of a QMS within the IT industry.


The concept of a quality as we think of it now first emerged from the Industrial Revolution. Previously goods had been made from start to finish by the same person or team of people, with handcrafting and tweaking the product to meet 'quality criteria'. Mass production brought huge teams of people together to work on specific stages of production where one person would not necessarily complete a product from start to finish. In the late 19th century pioneers such as Frederick Winslow Taylor and Henry Ford recognized the limitations of the methods being used in mass production at the time and the subsequent varying quality of output. Birland established Quality Departments to oversee the quality of production and rectifying of errors, and Ford emphasized standardization of design and component standards to ensure a standard product was produced. Management of quality was the responsibility of the Quality department and was implemented by Inspection of product output to 'catch' defects.

Application of statistical control came later as a result of World War production methods, which were advanced by the work done of W. Edwards Deming, a statistician, after whom the Deming Prize for quality is named. Joseph M. Juran focused more on managing for quality. The first edition of Juran's Quality Control Handbook was published in 1951. He also developed the "Juran's trilogy", an approach to cross-functional management that is composed of three managerial processes: quality planning, quality control, and quality improvement. These functions all play a vital role when evaluating quality.

Quality, as a profession and the managerial process associated with the quality function, was introduced during the second half of the 20th century and has evolved since then. Over this period, few other disciplines have seen as many changes as the quality profession.

The quality profession grew from simple control to engineering, to systems engineering. Quality control activities were predominant in the 1940s, 1950s, and 1960s. The 1970s were an era of quality engineering and the 1990s saw quality systems as an emerging field. Like medicine, accounting, and engineering, quality has achieved status as a recognized profession

As Lee and Dale (1998) state, there are many organizations that are striving to assess the methods and ways in which their overall productivity, the quality of their products and services and the required operations to achieve them are done.

The two primary, state of the art, guidelines for medical device manufacturer QMS and related services today are the ISO 13485 standards and the US FDA 21 CFR 820 regulations. The two have a great deal of similarity, and many manufacturers adopt QMS that is compliant with both guidelines.

ISO 13485 are harmonized with the European Union medical devices directive (93/42/EEC) as well as the IVD and AIMD directives. The ISO standard is also incorporated in regulations for other jurisdictions such as Japan (JPAL) and Canada (CMDCAS).

Quality System requirements for medical devices have been internationally recognized as a way to assure product safety and efficacy and customer satisfaction since at least 1983 and were instituted as requirements in a final rule published on October 7, 1996. The U.S. Food and Drug Administration (FDA) had documented design defects in medical devices that contributed to recalls from 1983 to 1989 that would have been prevented if Quality Systems had been in place. The rule is promulgated at 21 CFR 820.

According to current Good Manufacturing Practice (GMP), medical device manufacturers have the responsibility to use good judgment when developing their quality system and apply those sections of the FDA Quality System (QS) Regulation that are applicable to their specific products and operations, in Part 820 of the QS regulation. As with GMP, operating within this flexibility, it is the responsibility of each manufacturer to establish requirements for each type or family of devices that will result in devices that are safe and effective, and to establish methods and procedures to design, produce, and distribute devices that meet the quality system requirements.

The FDA has identified in the QS regulation the 7 essential subsystems of a quality system. These subsystems include:

all overseen by management and quality audits.

Because the QS regulation covers a broad spectrum of devices and production processes, it allows some leeway in the details of quality system elements. It is left to manufacturers to determine the necessity for, or extent of, some quality elements and to develop and implement procedures tailored to their particular processes and devices. For example, if it is impossible to mix up labels at a manufacturer because there is only one label to each product, then there is no necessity for the manufacturer to comply with all of the GMP requirements under device labeling.

Drug manufactures are regulated under a different section of the Code of Federal Regulations:

The International Organization for Standardization's ISO 9001:2015 series describes standards for a QMS addressing the principles and processes surrounding the design, development, and delivery of a general product or service. Organizations can participate in a continuing certification process to ISO 9001:2008 to demonstrate their compliance with the standard, which includes a requirement for continual (i.e. planned) improvement of the QMS, as well as more foundational QMS components such as failure mode and effects analysis (FMEA).

ISO 9000:2005 provides information on the fundamentals and vocabulary used in quality management systems. ISO 9004:2009 provides guidance on quality management approach for the sustained success of an organization. Neither of these standards can be used for certification purposes as they provide guidance, not requirements.

The Baldrige Performance Excellence Program educates organizations in improving their performance and administers the Malcolm Baldrige National Quality Award. The Baldrige Award recognizes U.S. organizations for performance excellence based on the Baldrige Criteria for Performance Excellence. The Criteria address critical aspects of management that contribute to performance excellence: leadership; strategy; customers; measurement, analysis, and knowledge management; workforce; operations; and results.

The European Foundation for Quality Management's EFQM Excellence Model supports an award scheme similar to the Baldrige Award for European companies.

In Canada, the National Quality Institute presents the 'Canada Awards for Excellence' on an annual basis to organizations that have displayed outstanding performance in the areas of Quality and Workplace Wellness, and have met the institute's criteria with documented overall achievements and results.

The European Quality in Social Service (EQUASS) is a sector-specific quality system designed for the social services sector and addresses quality principles that are specific to service delivery to vulnerable groups, such as empowerment, rights, and person-centredness.

The Alliance for Performance Excellence is a network of state and local organizations that use the Baldrige Criteria for Performance Excellence at the grassroots level to improve the performance of local organizations and economies. browsers can find Alliance members in their state and get the latest news and events from the Baldrige community.

A QMS process is an element of an organizational QMS. The ISO 9001:2000 standard requires organizations seeking compliance or certification to define the processes which form the QMS and the sequence and interaction of these processes. Butterworth-Heinemann and other publishers have offered several books which provide step-by-step guides to those seeking the quality certifications of their products 

Examples of such processes include:


ISO9001 requires that the performance of these processes be measured, analyzed and continually improved, and the results of this form an input into the management review process.




</doc>
<doc id="25346" url="https://en.wikipedia.org/wiki?curid=25346" title="Québécois (word)">
Québécois (word)

Québécois (pronounced ); feminine: Québécoise (pronounced ), ' (fem.: '), or (fem.: ) is a word used primarily to refer to a native or inhabitant of the Canadian province of Quebec that speaks French as a mother tongue; sometimes, it is used more generally to refer to any native or inhabitant of Quebec. It can refer to French spoken in Quebec. It may also be used, with an upper- or lower-case initial, as an adjective relating to Quebec, or to the French culture of Quebec. A resident or native of Quebec is often referred to in English as a Quebecer or Quebecker. In French, Québécois or Québécoise usually refers to any native or resident of Quebec. Its use became more prominent in the 1960s as French Canadians from Quebec increasingly self-identified as Québécois.

The name "Quebec" comes from a Mi'kmaq word "k'webeq" meaning "where the waters get narrow" and originally referred to the area around Quebec City, where the Saint Lawrence River narrows to a cliff-lined gap. French explorer Samuel de Champlain chose this name in 1608 for the colonial outpost he would use as the administrative seat for the French colony of Canada and New France. The Province of Quebec was first founded as a British colony in the Royal Proclamation of 1763 after the Treaty of Paris formally transferred the French colony of New France to Britain after the Seven Years' War. Quebec City remained the capital. In 1774, Guy Carleton obtained from the British Government the Quebec Act, which gave Canadiens most of the territory they held before 1763; the right of religion; and their right of language and culture. The British Government did this to in order to keep their loyalty, in the face of a growing menace of independence from the 13 original British colonies.

The term became more common in English as "Québécois" largely replacing "French Canadian" as an expression of cultural and national identity among French Canadians living in Quebec during the Quiet Revolution of the 1960s. The predominant French Canadian nationalism and identity of previous generations was based on the protection of the French language, the Roman Catholic Church, and Church-run institutions across Canada and in parts of the United States. In contrast, the modern Québécois identity is secular and based on a social democratic ideal of an active Quebec government promoting the French language and French-speaking culture in the arts, education, and business within the Province of Quebec. Politically, this resulted in a push towards more autonomy for Quebec and an internal debate on Quebec independence and identity that continues to this day. The emphasis on the French language and Quebec autonomy means that French-speakers across Canada now self-identify more specifically with provincial or regional identity-tags, such as "acadienne", or "franco-canadienne", "franco-manitobaine", "franco-ontarienne" or "fransaskoise". Terms such as Franco-Ontarian, acadian and Franco-Manitoban are still predominant. Francophones and anglophones use many terms when discussing issues of francophone linguistic and cultural identity in English.

The political shift towards a new Quebec nationalism in the 1960s led to Québécois increasingly referring to provincial institutions as being national. This was reflected in the change of the provincial "Legislative Assembly" to "National Assembly" in 1968. Nationalism reached an apex the 1970s and 1990s, with contentious constitutional debates resulting in close to half of all of French-speaking Québécois seeking recognition of nation status through tight referendums on Quebec sovereignty in 1980 and 1995. Having lost both referendums, the sovereigntist Parti Québécois government renewed the push for recognition as a nation through symbolic motions that gained the support of all parties in the National Assembly. They affirmed the right to determine the independent status of Quebec. They also renamed the area around Quebec City the "Capitale-Nationale" (national capital) region and renamed provincial parks "Parcs Nationaux" (national parks). In opposition in October 2003, the Parti Québécois tabled a motion that was unanimously adopted in the National Assembly affirming that the Quebec people formed a nation. Bloc Québécois leader Gilles Duceppe scheduled a similar motion in the House of Commons for November 23, 2006, that would have recognized "Quebecers as a nation". Conservative Prime Minister Stephen Harper tabled the "Québécois nation motion" the day before the Bloc Québécois resolution came to a vote. The English version changed the word "Quebecer" to "Québécois" and added "within a united Canada" at the end of the Bloc motion.

The "Québécois nation" was recognized by the House of Commons of Canada on November 27, 2006. The Prime Minister specified that the motion used the ""cultural"" and ""sociological"" as opposed to the ""legal"" sense of the word ""nation"". According to Harper, the motion was of a symbolic political nature, representing no constitutional change, no recognition of Quebec sovereignty, and no legal change in its political relations within the federation. The Prime Minister has further elaborated, stating that the motion's definition of Québécois relies on personal decisions to self-identify as Québécois, and therefore is a personal choice.

Despite near-universal support in the House of Commons, several important dissenters criticized the motion. Intergovernmental Affairs minister Michael Chong resigned from his position and abstained from voting, arguing that this motion was too ambiguous and had the potential of recognizing a destructive ethnic nationalism in Canada. Liberals were the most divided on the issue and represented 15 of the 16 votes against the motion. Liberal MP Ken Dryden summarized the view of many of these dissenters, maintaining that it was a game of semantics that cheapened issues of national identity. A survey by Leger Marketing in November 2006 showed that Canadians were deeply divided on this issue. When asked if Québécois are a nation, only 53 per cent of Canadians agreed, 47 per cent disagreed, with 33 per cent strongly disagreeing; 78 per cent of French-speaking Canadians agreed that Québécois are a nation, compared with 38 per cent of English-speaking Canadians. As well, 78 per cent of 1,000 Québécois polled thought that Québécois should be recognized as a nation.

The Québécois self-identify as an ethnic group in both the English and French versions of the Canadian census and in demographic studies of ethnicity in Canada.

In the 2016 census, 74,575 chose Québécois as one of multiple responses with 119,985 choosing it as a single response (194,555 as a combined response).

In the 2001 Census of Canada, 98,670 Canadians, or just over 1% of the population of Quebec identified "Québécois" as their ethnicity, ranking "Québécois" as the 37th most common response. These results were based on a question on residents in each household in Canada: ""To which ethnic or cultural group(s) did this person's ancestors belong?"", along with a list of sample choices ("Québécois" did not appear among the various sample choices). The ethnicity""Canadien"" or Canadian, did appear as an example on the questionnaire, and was selected by 4.9 million people or 68.2% of the Quebec population.

In the more detailed "Ethnic Diversity Survey",
Québécois was the most common ethnic identity in Quebec, reported by 37% of
Quebec's population aged 15 years and older, either as their only identity or alongside
other identities. The survey, based on interviews, asked the following questions: ""1) I would now like to ask you about your ethnic ancestry, heritage or background. What were the ethnic or cultural origins of your ancestors? 2) In addition to "Canadian", what were the other ethnic or cultural origins of your ancestors on first coming to North America?"" This survey did not list possible choices of ancestry and permitted multiple answers.
In census ethnic surveys, French-speaking Canadians identify their ethnicity most often as French, "Canadien", "Québécois", or French Canadian, with the latter three referred to by Jantzen (2005) as "French New World" ancestries because they originate in Canada. Jantzen (2005) distinguishes the English "Canadian", meaning "someone whose family has been in Canada for multiple generations", and the French "Canadien", used to refer to descendants of the original settlers of New France in the 17th and 18th centuries.

Those reporting "French New World" ancestries overwhelmingly had ancestors that went back at least 4 generations in Canada: specifically, 90% of "Québécois" traced their ancestry back this far. Fourth generation Canadiens and Québécois showed considerable attachment to their ethno-cultural group, with 70% and 61% respectively reporting a strong sense of belonging.

The generational profile and strength of identity of French New World ancestries contrast with those of British or Canadian ancestries, which represent the largest ethnic identities in Canada. Although deeply rooted Canadians express a deep attachment to their ethnic identity, most English-speaking Canadians of British ancestry generally cannot trace their ancestry as far back in Canada as French-speakers. As a result, their identification with their ethnicity is weaker tending to have a more broad based cultural identification: for example, only 50% of third generation "Canadians" strongly identify as such, bringing down the overall average. The survey report notes that 80% of Canadians whose families had been in Canada for three or more generations reported "Canadian and provincial or regional ethnic identities". These identities include "Québécois" (37% of Quebec population), "Acadian" (6% of Atlantic provinces) and "Newfoundlander" (38% of Newfoundland and Labrador).

English expressions employing the term may imply specific reference to francophones; such as "Québécois music", "a Québécois rocker" or "Québécois literature".

The dictionary "Le Petit Robert", published in France, states that the adjective "québécois", in addition to its territorial meaning, may refer specifically to francophone or French Canadian culture in Quebec. The dictionary gives as examples "cinéma québécois" and "littérature québécoise".

However, an ethnic or linguistic sense is absent from "Le Petit Larousse, also published in France, as well as from French dictionaries published in Canada such as "Le Dictionnaire québécois d'aujourd'hui" and "Le Dictionnaire du français Plus", which indicate instead "Québécois francophone" "francophone Quebecer" in the linguistic sense.

The online dictionary "Grand dictionnaire terminologique" of the Office québécois de la langue française mentions only a territorial meaning for "Québécois".

Newspaper editor Lysiane Gagnon has referred to an ethnic sense of the word "Québécois" in both English and French.

French expressions employing "Québécois" often appear in both French and English.





</doc>
<doc id="25348" url="https://en.wikipedia.org/wiki?curid=25348" title="Quantico, Virginia">
Quantico, Virginia

Quantico (; formerly Potomac) is a town in Prince William County, Virginia, United States. The population was 480 at the 2010 census. Quantico is bordered by the Potomac River to the east and the Quantico Creek to the north. The word Quantico is a derivation of the name of a Doeg village recorded by English colonists as "Pamacocack".

Quantico is surrounded on its remaining two sides by one of the largest U.S. Marine Corps bases, Marine Corps Base Quantico. The base is the site of the Marine Corps Combat Development Command and HMX-1 (the presidential helicopter squadron), Officer Candidates School, and The Basic School. The United States Drug Enforcement Administration's training academy, the FBI Academy, the FBI Laboratory, the Naval Criminal Investigative Service, the United States Army Criminal Investigation Command, and the Air Force Office of Special Investigations headquarters are on the base. A replica of the United States Marine Corps War Memorial stands at one of the entrances to the base. 

According to the United States Census Bureau, the town has a total area of , of which, of it is land and none of the area is covered with water.

Quantico has a humid subtropical climate (Köppen climate classification "Cfa").

As of the census of 2000, there were 561 people, 295 households, and 107 families living in the town. The population density was . The racial makeup was 61.32% White, 20.32% African American, 10.16% Asian, 0.36% Native American, 2.32% from other races, and 5.53% from two or more races. Hispanic or Latino of any race were 5.53% of the population. The median income for a household in the town was $26,250. About 22.4% of families and 21.4% of the population were below the poverty line, including 39.4% of those under the age of 18.

There are no significant highways passing through Quantico. All road vehicles must pass through Marine Corps Base Quantico in order to reach the town. Therefore, all vehicle drivers must present a valid driver’s license to the military security officer stationed at the gate, and may be required to state their destination and reason for visiting. More thorough searches and checks may also be undertaken, according to the discretion and authority of base security.

Amtrak and Virginia Railway Express trains stop at the Quantico station.


The Headquarters of the FBI Academy in Quantico are featured in "The Silence of the Lambs" (1991) and the Headquarters of the FBI Behavioral Analysis Unit are frequently featured in the crime drama series Criminal Minds.




</doc>
<doc id="25349" url="https://en.wikipedia.org/wiki?curid=25349" title="QSIG">
QSIG

QSIG is an ISDN based signaling protocol for signaling between private branch exchanges (PBXs) in a private integrated services network (PISN). It makes use of the connection-level Q.931 protocol and the application-level ROSE protocol. ISDN "proper" functions as the physical link layer.

QSIG was originally developed by Ecma International, adopted by ETSI and is defined by a set of ISO standard documents, so is not owned by any company. This allows interoperability between communications platforms provided by disparate vendors. 

QSIG has two layers, called BC (basic call) and GF (generic function). QSIG BC describes how to set up calls between PBXs. QSIG GF provides supplementary services for large-scale corporate, educational, and government networks, such as line identification, call intrusion and call forwarding. Thus for a large or very distributed company that requires multiple PBXs, users can receive the same services across the network and be unaware of the switch that their telephone is connected to. This greatly eases the problems of management of large networks.

QSIG will likely never rival each vendor's private network protocols, but it does provide an option for a higher level of integration than that of the traditional choices.

Note: This list is not complete. See the "source" after the list for more information.

Source : ECMA - list of standards (search the list for PISN to find all QSIG related standards at ECMA)

QSIG basically uses ROSE to invoke specific supplementary service at the remote PINX. These ROSE operations are coded in a Q.931 FACILITY info element. Here a list of QSIG opcodes:

Source : European Telecommunications Standards Institute (ETSI)

Source : International Telecommunications Union (ITU)



</doc>
<doc id="25350" url="https://en.wikipedia.org/wiki?curid=25350" title="Quasicrystal">
Quasicrystal

A quasiperiodic crystal, or quasicrystal, is a structure that is ordered but not periodic. A quasicrystalline pattern can continuously fill all available space, but it lacks translational symmetry. While crystals, according to the classical crystallographic restriction theorem, can possess only two-, three-, four-, and six-fold rotational symmetries, the Bragg diffraction pattern of quasicrystals shows sharp peaks with other symmetry orders—for instance, five-fold.

Aperiodic tilings were discovered by mathematicians in the early 1960s, and, some twenty years later, they were found to apply to the study of natural quasicrystals. The discovery of these aperiodic forms in nature has produced a paradigm shift in the fields of crystallography. In crystallography the quasicrystals were predicted in 1981 by a five-fold symmetry study of Alan Lindsay Mackay, — that also brought in 1982, with the crystallographic Fourier transform of a Penrose tiling, the possibility of identifying quasiperiodic order in a material through diffraction.

Quasicrystals had been investigated and observed earlier, but, until the 1980s, they were disregarded in favor of the prevailing views about the atomic structure of matter. In 2009, after a dedicated search, a mineralogical finding, icosahedrite, offered evidence for the existence of natural quasicrystals.

Roughly, an ordering is non-periodic if it lacks translational symmetry, which means that a shifted copy will never match exactly with its original. The more precise mathematical definition is that there is never translational symmetry in more than "n" – 1 linearly independent directions, where "n" is the dimension of the space filled, e.g., the three-dimensional tiling displayed in a quasicrystal may have translational symmetry in two directions. Symmetrical diffraction patterns result from the existence of an indefinitely large number of elements with a regular spacing, a property loosely described as long-range order. Experimentally, the aperiodicity is revealed in the unusual symmetry of the diffraction pattern, that is, symmetry of orders other than two, three, four, or six. 
In 1982 materials scientist Dan Shechtman observed that certain aluminium-manganese alloys produced the unusual diffractograms which today are seen as revelatory of quasicrystal structures. Due to fear of the scientific community's reaction, it took him two years to publish the results for which he was awarded the Nobel Prize in Chemistry in 2011.
On 25 October 2018, Luca Bindi and Paul Steinhardt were awarded the Aspen Institute 2018 Prize for collaboration and scientific research between Italy and the United States.

In 1961, Hao Wang asked whether determining if a set of tiles admits a tiling of the plane is an algorithmically unsolvable problem or not. He conjectured that it is solvable, relying on the hypothesis that every set of tiles that can tile the plane can do it "periodically" (hence, it would suffice to try to tile bigger and bigger patterns until obtaining one that tiles periodically). Nevertheless, two years later, his student Robert Berger constructed a set of some 20,000 square tiles (now called "Wang tiles") that can tile the plane but not in a periodic fashion. As further aperiodic sets of tiles were discovered, sets with fewer and fewer shapes were found. In 1976 Roger Penrose discovered a set of just two tiles, now referred to as Penrose tiles, that produced only non-periodic tilings of the plane. These tilings displayed instances of fivefold symmetry. One year later Alan Mackay showed experimentally that the diffraction pattern from the Penrose tiling had a two-dimensional Fourier transform consisting of sharp 'delta' peaks arranged in a fivefold symmetric pattern. Around the same time, Robert Ammann created a set of aperiodic tiles that produced eightfold symmetry.

Mathematically, quasicrystals have been shown to be derivable from a general method that treats them as projections of a higher-dimensional lattice. Just as circles, ellipses, and hyperbolic curves in the plane can be obtained as sections from a three-dimensional double cone, so too various (aperiodic or periodic) arrangements in two and three dimensions can be obtained from postulated hyperlattices with four or more dimensions. Icosahedral quasicrystals in three dimensions were projected from a six-dimensional hypercubic lattice by Peter Kramer and Roberto Neri in 1984. The tiling is formed by two tiles with rhombohedral shape.

Shechtman first observed ten-fold electron diffraction patterns in 1982, as described in his notebook. The observation was made during a routine investigation, by electron microscopy, of a rapidly cooled alloy of aluminium and manganese prepared at the US National Bureau of Standards (later NIST).

In the summer of the same year Shechtman visited Ilan Blech and related his observation to him. Blech responded that such diffractions had been seen before. Around that time, Shechtman also related his finding to John Cahn of NIST who did not offer any explanation and challenged him to solve the observation. Shechtman quoted Cahn as saying: "Danny, this material is telling us something and I challenge you to find out what it is".

The observation of the ten-fold diffraction pattern lay unexplained for two years until the spring of 1984, when Blech asked Shechtman to show him his results again. A quick study of Shechtman's results showed that the common explanation for a ten-fold symmetrical diffraction pattern, the existence of twins, was ruled out by his experiments. Since periodicity and twins were ruled out, Blech, unaware of the two-dimensional tiling work, was looking for another possibility: a completely new structure containing cells connected to each other by defined angles and distances but without translational periodicity. Blech decided to use a computer simulation to calculate the diffraction intensity from a cluster of such a material without long-range translational order but still not random. He termed this new structure multiple polyhedral.

The idea of a new structure was the necessary paradigm shift to break the impasse. The "Eureka moment" came when the computer simulation showed sharp ten-fold diffraction patterns, similar to the observed ones, emanating from the three-dimensional structure devoid of periodicity. The multiple polyhedral structure was termed later by many researchers as icosahedral glass but in effect it embraces "any arrangement of polyhedra connected with definite angles and distances" (this general definition includes tiling, for example).

Shechtman accepted Blech's discovery of a new type of material and it gave him the courage to publish his experimental observation. Shechtman and Blech jointly wrote a paper entitled "The Microstructure of Rapidly Solidified AlMn" and sent it for publication around June 1984 to the "Journal of Applied Physics" (JAP). The JAP editor promptly rejected the paper as being better fit for a metallurgical readership. As a result, the same paper was re-submitted for publication to the "Metallurgical Transactions A", where it was accepted. Although not noted in the body of the published text, the published paper was slightly revised prior to publication.

Meanwhile, on seeing the draft of the Shechtman–Blech paper in the summer of 1984, John Cahn suggested that Shechtman's experimental results merit a fast publication in a more appropriate scientific journal. Shechtman agreed and, in hindsight, called this fast publication "a winning move”. This paper, published in the "Physical Review Letters" (PRL), repeated Shechtman's observation and used the same illustrations as the original Shechtman–Blech paper in the "Metallurgical Transactions A". The PRL paper, the first to appear in print, caused considerable excitement in the scientific community.

Next year Ishimasa "et al." reported twelvefold symmetry in Ni-Cr particles. Soon, eightfold diffraction patterns were recorded in V-Ni-Si and Cr-Ni-Si alloys. Over the years, hundreds of quasicrystals with various compositions and different symmetries have been discovered. The first quasicrystalline materials were thermodynamically unstable—when heated, they formed regular crystals. However, in 1987, the first of many stable quasicrystals were discovered, making it possible to produce large samples for study and opening the door to potential applications. 
Almost concurrently Paul Steinhardt (Princeton University) hypothesized the possibility to find a quasicrystal in nature, developing a method of recognition, published on Physical Review Letters in 2001, inviting all the mineralogical collections of the world to identify any badly cataloged crystals. In 2007 Steinhardt received a reply by Luca Bindi (University of Florence) that stated to have found an almost perfect match crystal in Florence Mineralogical Collection with quasicrystal characteristics, originally coming from Khatyrka. So in 2008 the crystal samples were sent to Princeton University for other tests and on 2009 New Year's Eve Steinhardt obtained the smoking gun, the final evidence, communicating the great discovery to Luca Bindi. After other studies was stated that the found quasicrystal was extraterrestrial and 4,57 mld old. In 2011 Bindi, Steinhardt and a team of specialists did an expedition in the desolate lands around the Kathyrka river, in Chukotka Autonomous Okrug managing to find other natural quasicrystal samples. This natural quasicrystal exhibits high crystalline quality, equalling the best artificial examples. The natural quasicrystal phase, with a composition of AlCuFe, was named icosahedrite and it was approved by the International Mineralogical Association in 2010. Furthermore, analysis indicates it may be meteoritic in origin, possibly delivered from a carbonaceous chondrite asteroid.
A further study of Khatyrka meteorites revealed micron-sized grains of another natural quasicrystal, which has a ten-fold symmetry and a chemical formula of AlNiFe. This quasicrystal is stable in a narrow temperature range, from 1120 to 1200 K at ambient pressure, which suggests that natural quasicrystals are formed by rapid quenching of a meteorite heated during an impact-induced shock.
In 1972 de Wolf and van Aalst reported that the diffraction pattern produced by a crystal of sodium carbonate cannot be labeled with three indices but needed one more, which implied that the underlying structure had four dimensions in reciprocal space. Other puzzling cases have been reported, but until the concept of quasicrystal came to be established, they were explained away or denied. However, at the end of the 1980s the idea became acceptable, and in 1992 the International Union of Crystallography altered its definition of a crystal, broadening it as a result of Shechtman's findings, reducing it to the ability to produce a clear-cut diffraction pattern and acknowledging the possibility of the ordering to be either periodic or aperiodic. Now, the symmetries compatible with translations are defined as "crystallographic", leaving room for other "non-crystallographic" symmetries. Therefore, aperiodic or quasiperiodic structures can be divided into two main classes: those with crystallographic point-group symmetry, to which the incommensurately modulated structures and composite structures belong, and those with non-crystallographic point-group symmetry, to which quasicrystal structures belong.

Originally, the new form of matter was dubbed "Shechtmanite". The term "quasicrystal" was first used in print by Steinhardt and Levine shortly after Shechtman's paper was published.
The adjective "quasicrystalline" had already been in use, but now it came to be applied to any pattern with unusual symmetry. 'Quasiperiodical' structures were claimed to be observed in some decorative tilings devised by medieval Islamic architects. For example, Girih tiles in a medieval Islamic mosque in Isfahan, Iran, are arranged in a two-dimensional quasicrystalline pattern. These claims have, however, been under some debate.

Shechtman was awarded the Nobel Prize in Chemistry in 2011 for his work on quasicrystals. "His discovery of quasicrystals revealed a new principle for packing of atoms and molecules," stated the Nobel Committee and pointed that "this led to a paradigm shift within chemistry." In 2014, Post of Israel issued a stamp dedicated to quasicrystals and the 2011 Nobel Prize.

Earlier in 2009, it was found that thin-film quasicrystals can be formed by self-assembly of uniformly shaped, nano-sized molecular units at an air-liquid interface. It was later demonstrated that those units can be not only inorganic, but also organic.

In 2018, chemists from Brown University announced the successful creation of a self-constructing lattice structure based on a strangely shaped quantum dot. While single-component quasicrystal lattices have been previously predicted mathematically and in computer simulations, they had not been demonstrated prior to this.

There are several ways to mathematically define quasicrystalline patterns. One definition, the "cut and project" construction, is based on the work of Harald Bohr (mathematician brother of Niels Bohr). The concept of an almost periodic function (also called a quasiperiodic function) was studied by Bohr, including work of Bohl and Escanglon.
He introduced the notion of a superspace. Bohr showed that quasiperiodic functions arise as restrictions of high-dimensional periodic functions to an irrational slice (an intersection with one or more hyperplanes), and discussed their Fourier point spectrum. These functions are not exactly periodic, but they are arbitrarily close in some sense, as well as being a projection of an exactly periodic function.

In order that the quasicrystal itself be aperiodic, this slice must avoid any lattice plane of the higher-dimensional lattice. De Bruijn showed that Penrose tilings can be viewed as two-dimensional slices of five-dimensional hypercubic structures. Equivalently, the Fourier transform of such a quasicrystal is nonzero only at a dense set of points spanned by integer multiples of a finite set of basis vectors (the projections of the primitive reciprocal lattice vectors of the higher-dimensional lattice).
The intuitive considerations obtained from simple model aperiodic tilings are formally expressed in the concepts of Meyer and Delone sets. The mathematical counterpart of physical diffraction is the Fourier transform and the qualitative description of a diffraction picture as 'clear cut' or 'sharp' means that singularities are present in the Fourier spectrum. There are different methods to construct model quasicrystals. These are the same methods that produce aperiodic tilings with the additional constraint for the diffractive property. Thus, for a substitution tiling the eigenvalues of the substitution matrix should be Pisot numbers. The aperiodic structures obtained by the cut-and-project method are made diffractive by choosing a suitable orientation for the construction; this is a geometric approach that has also a great appeal for physicists.

Classical theory of crystals reduces crystals to point lattices where each point is the center of mass of one of the identical units of the crystal. The structure of crystals can be analyzed by defining an associated group. Quasicrystals, on the other hand, are composed of more than one type of unit, so, instead of lattices, quasilattices must be used. Instead of groups, groupoids, the mathematical generalization of groups in category theory, is the appropriate tool for studying quasicrystals.

Using mathematics for construction and analysis of quasicrystal structures is a difficult task for most experimentalists. Computer modeling, based on the existing theories of quasicrystals, however, greatly facilitated this task. Advanced programs have been developed allowing one to construct, visualize and analyze quasicrystal structures and their diffraction patterns. The aperiodic nature of quasicrystals can also make theoretical studies of physical properties, such as electronic structure, difficult due to the inapplicability of Bloch's theorem. However, spectra of quasicrystals can still be computed with error control.

Study of quasicrystals may shed light on the most basic notions related to the quantum critical point observed in heavy fermion metals. Experimental measurements on the gold-aluminium-ytterbium quasicrystal have revealed a quantum critical point defining the divergence of the magnetic susceptibility as temperature tends to zero. It is suggested that the electronic system of some quasicrystals is located at a quantum critical point without tuning, while quasicrystals exhibit the typical scaling behaviour of their thermodynamic properties and belong to the well-known family of heavy fermion metals.

Since the original discovery by Dan Shechtman, hundreds of quasicrystals have been reported and confirmed. Undoubtedly, the quasicrystals are no longer a unique form of solid; they exist
universally in many metallic alloys and some polymers. Quasicrystals are found most often in aluminium alloys (Al-Li-Cu, Al-Mn-Si, Al-Ni-Co, Al-Pd-Mn, Al-Cu-Fe, Al-Cu-V, etc.), but numerous other compositions are also known (Cd-Yb, Ti-Zr-Ni, Zn-Mg-Ho, Zn-Mg-Sc, In-Ag-Yb, Pd-U-Si, etc.).

Two types of quasicrystals are known. The first type, polygonal (dihedral) quasicrystals, have an axis of 8, 10, or 12-fold local symmetry (octagonal, decagonal, or dodecagonal quasicrystals, respectively). They are periodic along this axis and quasiperiodic in planes normal to it. The second type, icosahedral quasicrystals, are aperiodic in all directions.

Quasicrystals fall into three groups of different thermal stability:

Except for the Al–Li–Cu system, all the stable quasicrystals are almost free of defects and disorder, as evidenced by X-ray and electron diffraction revealing peak widths as sharp as those of perfect crystals such as Si. Diffraction patterns exhibit fivefold, threefold, and twofold symmetries, and reflections are arranged quasiperiodically in three dimensions.

The origin of the stabilization mechanism is different for the stable and metastable quasicrystals. Nevertheless, there is a common feature observed in most quasicrystal-forming liquid alloys or their undercooled liquids: a local icosahedral order. The icosahedral order is in equilibrium in the "liquid state" for the stable quasicrystals, whereas the icosahedral order prevails in the "undercooled liquid state" for the metastable quasicrystals.

A nanoscale icosahedral phase was formed in Zr-, Cu- and Hf-based bulk metallic glasses alloyed with noble metals.

Most quasicrystals have ceramic-like properties including high thermal and electrical resistance, hardness and brittleness, resistance to corrosion, and non-stick
properties. Many metallic quasicrystalline substances are impractical for most applications due to their thermal instability; the Al-Cu-Fe ternary system and the Al-Cu-Fe-Cr and Al-Co-Fe-Cr quaternary systems, thermally stable up to 700 °C, are notable exceptions.

Quasicrystalline substances have potential applications in several forms.

Metallic quasicrystalline coatings can be applied by plasma-coating or magnetron sputtering. A problem that must be resolved is the tendency for cracking due to the materials' extreme brittleness. The cracking could be suppressed by reducing sample dimensions or coating thickness. Recent studies show typically brittle quasicrystals can exhibit remarkable ductility of over 50% strains at room temperature and sub-micrometer scales (<500 nm).

An application was the use of low-friction Al-Cu-Fe-Cr quasicrystals as a coating for frying pans. Food did not stick to it as much as to stainless steel making the pan moderately non-stick and easy to clean; heat transfer and durability were better than PTFE non-stick cookware and the pan was free from perfluorooctanoic acid (PFOA); the surface was very hard, claimed to be ten times harder than stainless steel, and not harmed by metal utensils or cleaning in a dishwasher; and the pan could withstand temperatures of without harm. However, cooking with a lot of salt would etch the quasicrystalline coating used, and the pans were eventually withdrawn from production. Shechtman had one of these pans.

The Nobel citation said that quasicrystals, while brittle, could reinforce steel "like armor". When Shechtman was asked about potential applications of quasicrystals he said that a precipitation-hardened stainless steel is produced that is strengthened by small quasicrystalline particles. It does not corrode and is extremely strong, suitable for razor blades and surgery instruments. The small quasicrystalline particles impede the motion of dislocation in the material.

Quasicrystals were also being used to develop heat insulation, LEDs, diesel engines, and new materials that convert heat to electricity. Shechtman suggested new applications taking advantage of the low coefficient of friction and the hardness of some quasicrystalline materials, for example embedding particles in plastic to make strong, hard-wearing, low-friction plastic gears. The low heat conductivity of some quasicrystals makes them good for heat insulating coatings.

Other potential applications include selective solar absorbers for power conversion, broad-wavelength reflectors, and bone repair and prostheses applications where biocompatibility, low friction and corrosion resistance are required. Magnetron sputtering can be readily applied to other stable quasicrystalline alloys such as Al-Pd-Mn.

While saying that the discovery of icosahedrite, the first quasicrystal found in nature, was important, Shechtman saw no practical applications.
[i] Bourdillon, A.J., (2013) Micron, 51, 21-25. https://doi.org/10.1016/j.micron.2013.06.004
[ii] Bourdillon, A.J. (2020) Complete soliution for quasicrystals https://www.youtube.com/watch?v=OFcSDKCecDA
[iii] Bourdillon, A.J., (2011) Logarithmically Periodic Solids. Nova Science, New York.




</doc>
<doc id="25381" url="https://en.wikipedia.org/wiki?curid=25381" title="Recreation">
Recreation

Recreation is an activity of leisure, leisure being discretionary time. The "need to do something for recreation" is an essential element of human biology and psychology. Recreational activities are often done for enjoyment, amusement, or pleasure and are considered to be "fun".

The term "recreation" appears to have been used in English first in the late 14th century, first in the sense of "refreshment or curing of a sick person", and derived turn from Latin ("re": "again", "creare": "to create, bring forth, beget").

Humans spend their time in activities of daily living, work, sleep, social duties, and leisure, the latter time being free from prior commitments to physiologic or social needs, a prerequisite of recreation. Leisure has increased with increased longevity and, for many, with decreased hours spent for physical and economic survival, yet others argue that time pressure has increased for modern people, as they are committed to too many tasks. Other factors that account for an increased role of recreation are affluence, population trends, and increased commercialization of recreational offerings. While one perception is that leisure is just "spare time", time not consumed by the necessities of living, another holds that leisure is a force that allows individuals to consider and reflect on the values and realities that are missed in the activities of daily life, thus being an essential element of personal development and civilization. This direction of thought has even been extended to the view that leisure is the purpose of work, and a reward in itself, and "leisure life" reflects the values and character of a nation. Leisure is considered a human right under the Universal Declaration of Human Rights.

Recreation is difficult to separate from the general concept of play, which is usually the term for children's recreational activity. Children may playfully imitate activities that reflect the realities of adult life. It has been proposed that play or recreational activities are outlets of or expression of excess energy, channeling it into socially acceptable activities that fulfill individual as well as societal needs, without need for compulsion, and providing satisfaction and pleasure for the participant. A traditional view holds that work is supported by recreation, recreation being useful to "recharge the battery" so that work performance is improved. 

Work, an activity generally performed out of economic necessity and useful for society and organized within the economic framework, however can also be pleasurable and may be self-imposed thus blurring the distinction to recreation. Many activities in entertainment are work for one person and recreation for another. Over time, a recreational activity may become work, and vice versa. Thus, for a musician, playing an instrument may be at one time a profession, and at another a recreation. 

Similarly, it may be difficult to separate education from recreation as in the case of recreational mathematics.

Recreation has many health benefits, and, accordingly, Therapeutic Recreation has been developed to take advantage of this effect. The National Council for Therapeutic Recreation Certification (NCTRC) is the nationally recognized credentialing organization for the profession of Therapeutic Recreation. Professionals in the field of Therapeutic Recreation who are certified by the NCTRC are called "Certified Therapeutic Recreation Specialists". The job title "Recreation Therapist" is identified in the U.S. Dept of Labor's Occupation Outlook. Such therapy is applied in rehabilitation, psychiatric facilities for youth and adults, and in the care of the elderly, the disabled, or people with chronic diseases. Recreational physical activity is important to reduce obesity, and the risk of osteoporosis and of cancer, most significantly in men that of colon and prostate, and in women that of the breast; however, not all malignancies are reduced as outdoor recreation has been linked to a higher risk of melanoma. Extreme adventure recreation naturally carries its own hazards.

Recreation is an essential part of human life and finds many different forms which are shaped naturally by individual interests but also by the surrounding social construction. Recreational activities can be communal or solitary, active or passive, outdoors or indoors, healthy or harmful, and useful for society or detrimental. A significant section of recreational activities are designated as hobbies which are activities done for pleasure on a regular basis. Some recreational activities – such as gambling, recreational drug use, or delinquent activities – may violate societal norms and laws. A list of typical activities could be almost endless

Bricolage and DIY are some of the terms describing the building, modifying, or repairing things without the direct aid of experts or professionals. Academic research has described DIY as behaviors where "individuals engage raw and semi-raw materials and parts to produce, transform, or reconstruct material possessions, including those drawn from the natural environment (e.g., landscaping)". DIY behavior can be triggered by various motivations previously categorized as marketplace motivations (economic benefits, lack of product availability, lack of product quality, need for customization), and identity enhancement (craftsmanship, empowerment, community seeking, uniqueness). They could involve crafts that requires particular skills and knowledge of skilled work. Typical interests enjoyed by the maker culture include engineering-oriented pursuits such as home improvement, electronics, robotics, 3-D printing, and the use of Computer Numeric Control tools, as well as more traditional activities such as metalworking, woodworking, and, mainly, its predecessor, traditional arts and crafts. The subculture stresses a cut-and-paste approach to standardized hobbyist technologies, and encourages cookbook re-use of designs published on websites and maker-oriented publications. There is a strong focus on using and learning practical skills and applying them to reference designs. There is also growing work on equity and the maker culture.

Any structured form of play could become a game. Games are played sometimes purely for recreation, sometimes for achievement or monetary rewards as well. They are played for recreation alone, in teams, or online; by amateurs. Professionals can play as part of their work for entertainment of the audience. The games could be Board games, Puzzles, computer or Video games.

Recreation engaged in out of doors, most commonly in natural settings. The activities themselves — such as fishing, hunting, backpacking, and horseback riding — characteristically dependent on the environment practiced in. While many of these activities can be classified as sports, they do not all demand that a participant be an athlete. Competition generally is less stressed than in individual or team sports organized into opposing squads in pursuit of a trophy or championship. When the activity involves exceptional excitement, physical challenge, or risk, it is sometimes referred to as "adventure recreation" or "adventure training", rather than an extreme sport.

Other traditional examples of outdoor recreational activities include hiking, camping, mountaineering, cycling, canoeing, caving, kayaking, rafting, rock climbing, running, sailing, skiing, sky diving and surfing. As new pursuits, often hybrids of prior ones, emerge, they gain their own identities, such as coasteering, canyoning, fastpacking, and plogging.

Participatory dance whether it be a folk dance, a social dance, a group dance such as a line, circle, chain or square dance, or a partner dance such as is common in western Western ballroom dancing, is undertaken primarily for a common purpose, such as entertainment, social interaction or exercise, of participants rather than onlookers. The many forms of dance provide recreation for all age groups and cultures.

Music is composed and performed for many purposes, ranging from recreation, religious or ceremonial purposes, or for entertainment. When music was only available through sheet music scores, such as during the Classical and Romantic eras in Europe, music lovers would buy the sheet music of their favourite pieces and songs so that they could perform them at home on their instruments.

Drawing goes back at least 16,000 years to Paleolithic cave representations of animals such as those at Lascaux in France and Altamira in Spain. In ancient Egypt, ink drawings on papyrus, often depicting people, were used as models for painting or sculpture. Drawings on Greek vases, initially geometric, later developed to the human form with black-figure pottery during the 7th century BC.

With paper becoming common in Europe by the 15th century, drawing was adopted by masters such as Sandro Botticelli, Raphael, Michelangelo, and Leonardo da Vinci who sometimes treated drawing as an art in its own right rather than a preparatory stage for painting or sculpture.

Writing may involve letters, journals and weblogs.
In the US, about half of all adults read one or more books for pleasure each year. About 5% read more than 50 books per year.Elocution is another way to use literature for recreation.

Like drawing, painting has its documented origins in caves and on rock faces. The finest examples, believed by some to be 32,000 years old, are in the Chauvet and Lascaux caves in southern France. In shades of red, brown, yellow and black, the paintings on the walls and ceilings are of bison, cattle, horses and deer.

Paintings of human figures can be found in the tombs of ancient Egypt. In the great temple of Ramses II, Nefertari, his queen, is depicted being led by Isis. The Greeks contributed to painting but much of their work has been lost. One of the best remaining representations are the Hellenistic Fayum mummy portraits. Another example is mosaic of the Battle of Issus at Pompeii, which was probably based on a Greek painting. Greek and Roman art contributed to Byzantine art in the 4th century BC, which initiated a tradition in icon painting.

An amateur photographer practices photography as a hobby/passion and not for monetary profit. The quality of some amateur work may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the Hand-held camera. Nowadays it has spread widely through social media and is carried out throughout different platforms and equipment, including the use of cell phone. Clear pictures can now be taken with a cell phone which is a key tool for making photography more accessible to everyone.

Many recreational activities are organized, typically by public institutions, voluntary group-work agencies, private groups supported by membership fees, and commercial enterprises. Examples of each of these are the National Park Service, the YMCA, the Kiwanis, and Walt Disney World. Public space such as parks and beaches are essential venues for many recreational activities and Tourism has recognized that many visitors are specifically attracted by recreational offerings. In particular, beach and waterfront promenades such as the beach area of Venice Beach in California, the Promenade de la Croisette in Cannes, the Promenade des Anglais in Nice or the lungomare of Barcola with Miramare Castle in Trieste are important recreational areas for the city population on the one hand and on the other also important tourist destinations with all advantages and disadvantages for the locals. 

In support of recreational activities government has taken an important role in their creation, maintenance, and organization, and whole industries have developed merchandise or services. Recreation-related business is an important factor in the economy; it has been estimated that the outdoor recreation sector alone contributes $730 billion annually to the U.S. economy and generates 6.5 million jobs. Research has shown that practicing creative leisure activities is interrelated with the emotional creativity.

A recreation center is a place for recreational activities usually administered by a municipal government agency. Swimming, basketball, weightlifting, volleyball and kids' play areas are very common.

A recreation specialist would be expected to meet the recreational needs of a community or assigned interest group. Educational institutions offer courses that lead to a degree as a Bachelor of Arts in recreation management. People with such degrees often work in parks and recreation centers in towns, on community projects and activities. Networking with instructors, budgeting, and evaluation of continuing programs are common job duties.

In the United States, most states have a professional organization for continuing education and certification in recreation management. The National Recreation and Park Association administers a certification program called the CPRP (Certified Park and Recreation Professional) that is considered a national standard for professional recreation specialist practices.

Since the beginning of the 2000s, there are more and more online booking / ticketing platforms for recreational activities that emerged. Many of them leveraged the ever-growing prevalence of internet, mobile devices and e-payments to build comprehensive online booking solutions. The first successful batch includes tourist recreation activities platform like TripAdvisor that went public. More examples of recreational activities booking platform includes Klook and KKDay that came to the market after 2010s. For recreational activities within the home city of people, there are bigger breakthrough in China like DianPing, Reubird and FunNow. The emergence of these platforms infers the rising needs for recreation and entertainment from the growing urban citizens worldwide.


</doc>
<doc id="25382" url="https://en.wikipedia.org/wiki?curid=25382" title="Recession">
Recession

In economics, a recession is a business cycle contraction when there is a general decline in economic activity. Recessions generally occur when there is a widespread drop in spending (an adverse demand shock). This may be triggered by various events, such as a financial crisis, an external trade shock, an adverse supply shock, the bursting of an economic bubble, or a large-scale natural or anthropogenic disaster (e.g. a pandemic). In the United States, it is defined as "a significant decline in economic activity spread across the market, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales". In the United Kingdom, it is defined as a negative economic growth for two consecutive quarters.

Governments usually respond to recessions by adopting expansionary macroeconomic policies, such as increasing money supply or increasing government spending and decreasing taxation.

Put simply, a recession is the decline of economic activity, which means that the public have stopped buying products for a while which can cause the downfall of GDP after a period of economic expansion (a time where products become popular and the income profit of a business becomes large). This causes inflation (the rise of product prices). In a recession, the rate of inflation slows down, stops or decreases.

In a 1974 "The New York Times" article, Commissioner of the Bureau of Labor Statistics Julius Shiskin suggested several rules of thumb for defining a recession, one of which was two consecutive quarters of negative GDP growth. In time, the other rules of thumb were forgotten. Some economists prefer a definition of a 1.5-2 percentage points rise in unemployment within 12 months.

In the United States, the Business Cycle Dating Committee of the National Bureau of Economic Research (NBER) is generally seen as the authority for dating US recessions. The NBER, a private economic research organization, defines an economic recession as: "a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales". Almost universally, academics, economists, policy makers, and businesses refer to the determination by the NBER for the precise dating of a recession's onset and end.

In the United Kingdom, recessions are generally defined as two consecutive quarters of negative economic growth, as measured by the seasonal adjusted quarter-on-quarter figures for real GDP. The same definition is used by member states of the European Union.

A recession has many attributes that can occur simultaneously and includes declines in component measures of economic activity (GDP) such as consumption, investment, government spending, and net export activity. These summary measures reflect underlying drivers such as employment levels and skills, household savings rates, corporate investment decisions, interest rates, demographics, and government policies.

Economist Richard C. Koo wrote that under ideal conditions, a country's economy should have the household sector as net savers and the corporate sector as net borrowers, with the government budget nearly balanced and net exports near zero. When these relationships become imbalanced, recession can develop within the country or create pressure for recession in another country. Policy responses are often designed to drive the economy back towards this ideal state of balance.

A severe (GDP down by 10%) or prolonged (three or four years) recession is referred to as an economic depression, although some argue that their causes and cures can be different. As an informal shorthand, economists sometimes refer to different recession shapes, such as V-shaped, U-shaped, L-shaped and W-shaped recessions.

The type and shape of recessions are distinctive. In the US, v-shaped, or short-and-sharp contractions followed by rapid and sustained recovery, occurred in 1954 and 1990–91; U-shaped (prolonged slump) in 1974–75, and W-shaped, or double-dip recessions in 1949 and 1980–82. Japan's 1993–94 recession was U-shaped and its 8-out-of-9 quarters of contraction in 1997–99 can be described as L-shaped. Korea, Hong Kong and South-east Asia experienced U-shaped recessions in 1997–98, although Thailand’s eight consecutive quarters of decline should be termed L-shaped.

Recessions have psychological and confidence aspects. For example, if companies expect economic activity to slow, they may reduce employment levels and save money rather than invest. Such expectations can create a self-reinforcing downward cycle, bringing about or worsening a recession. Consumer confidence is one measure used to evaluate economic sentiment. The term animal spirits has been used to describe the psychological factors underlying economic activity. Economist Robert J. Shiller wrote that the term "...refers also to the sense of trust we have in each other, our sense of fairness in economic dealings, and our sense of the extent of corruption and bad faith. When animal spirits are on ebb, consumers do not want to spend and businesses do not want to make capital expenditures or hire people."
Behavioral economics, has also explained some psychological biases that may trigger a recession including availability heuristic, money illusion, and non-regressive prediction. 

High levels of indebtedness or the bursting of a real estate or financial asset price bubble can cause what is called a "balance sheet recession". This is when large numbers of consumers or corporations pay down debt (i.e., save) rather than spend or invest, which slows the economy. The term balance sheet derives from an accounting identity that holds that assets must always equal the sum of liabilities plus equity. If asset prices fall below the value of the debt incurred to purchase them, then the equity must be negative, meaning the consumer or corporation is insolvent. Economist Paul Krugman wrote in 2014 that "the best working hypothesis seems to be that the financial crisis was only one manifestation of a broader problem of excessive debt—that it was a so-called "balance sheet recession". In Krugman's view, such crises require debt reduction strategies combined with higher government spending to offset declines from the private sector as it pays down its debt.

For example, economist Richard Koo wrote that Japan's "Great Recession" that began in 1990 was a "balance sheet recession". It was triggered by a collapse in land and stock prices, which caused Japanese firms to have negative equity, meaning their assets were worth less than their liabilities. Despite zero interest rates and expansion of the money supply to encourage borrowing, Japanese corporations in aggregate opted to pay down their debts from their own business earnings rather than borrow to invest as firms typically do. Corporate investment, a key demand component of GDP, fell enormously (22% of GDP) between 1990 and its peak decline in 2003. Japanese firms overall became net savers after 1998, as opposed to borrowers. Koo argues that it was massive fiscal stimulus (borrowing and spending by the government) that offset this decline and enabled Japan to maintain its level of GDP. In his view, this avoided a U.S. type Great Depression, in which U.S. GDP fell by 46%. He argued that monetary policy was ineffective because there was limited demand for funds while firms paid down their liabilities. In a balance sheet recession, GDP declines by the amount of debt repayment and un-borrowed individual savings, leaving government stimulus spending as the primary remedy.

Krugman discussed the balance sheet recession concept during 2010, agreeing with Koo's situation assessment and view that sustained deficit spending when faced with a balance sheet recession would be appropriate. However, Krugman argued that monetary policy could also affect savings behavior, as inflation or credible promises of future inflation (generating negative real interest rates) would encourage less savings. In other words, people would tend to spend more rather than save if they believe inflation is on the horizon. In more technical terms, Krugman argues that the private sector savings curve is elastic even during a balance sheet recession (responsive to changes in real interest rates) disagreeing with Koo's view that it is inelastic (non-responsive to changes in real interest rates).

A July 2012 survey of balance sheet recession research reported that consumer demand and employment are affected by household leverage levels. Both durable and non-durable goods consumption declined as households moved from low to high leverage with the decline in property values experienced during the subprime mortgage crisis. Further, reduced consumption due to higher household leverage can account for a significant decline in employment levels. Policies that help reduce mortgage debt or household leverage could therefore have stimulative effects.

A liquidity trap is a Keynesian theory that a situation can develop in which interest rates reach near zero (zero interest-rate policy) yet do not effectively stimulate the economy. In theory, near-zero interest rates should encourage firms and consumers to borrow and spend. However, if too many individuals or corporations focus on saving or paying down debt rather than spending, lower interest rates have less effect on investment and consumption behavior; the lower interest rates are like "pushing on a string". Economist Paul Krugman described the U.S. 2009 recession and Japan's lost decade as liquidity traps. One remedy to a liquidity trap is expanding the money supply via quantitative easing or other techniques in which money is effectively printed to purchase assets, thereby creating inflationary expectations that cause savers to begin spending again. Government stimulus spending and mercantilist policies to stimulate exports and reduce imports are other techniques to stimulate demand. He estimated in March 2010 that developed countries representing 70% of the world's GDP were caught in a liquidity trap.

Behavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save (or pay down debt) simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a "paradox of deleveraging" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.

During April 2009, U.S. Federal Reserve Vice Chair Janet Yellen discussed these paradoxes: "Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole."

The U.S. Conference Board's Present Situation Index year-over-year change turns negative by more than 15 points before a recession.

The U.S. Conference Board Leading Economic Indicator year-over-year change turns negative before a recession.

When the CFNAI Diffusion Index drops below the value of -0.35, then there is an increased probability of the beginning a recession. Usually the signal happens in the three months of the recession. The CFNAI Diffusion Index signal tends to happen about one month before a related signal by the CFNAI-MA3 (3-month moving average) drops below the -0.7 level. The CFNAI-MA3 correctly identified the 7 recessions between March 1967–August 2019, while triggering only 2 false alarms.

Except for the above, there are no known completely reliable predictors, but the following are considered possible predictors.



Analysis by Prakash Loungani of the International Monetary Fund found that only two of the sixty recessions around the world during the 1990s had been predicted by a consensus of economists one year earlier, while there were zero consensus predictions one year earlier for the 49 recessions during 2009.

Most mainstream economists believe that recessions are caused by inadequate aggregate demand in the economy, and favor the use of expansionary macroeconomic policy during recessions. Strategies favored for moving an economy out of a recession vary depending on which economic school the policymakers follow. Monetarists would favor the use of expansionary monetary policy, while Keynesian economists may advocate increased government spending to spark economic growth. Supply-side economists may suggest tax cuts to promote business capital investment. When interest rates reach the boundary of an interest rate of zero percent (zero interest-rate policy) conventional monetary policy can no longer be used and government must use other measures to stimulate recovery. Keynesians argue that fiscal policy—tax cuts or increased government spending—works when monetary policy fails. Spending is more effective because of its larger multiplier but tax cuts take effect faster.

For example, Paul Krugman wrote in December 2010 that significant, sustained government spending was necessary because indebted households were paying down debts and unable to carry the U.S. economy as they had previously: "The root of our current troubles lies in the debt American families ran up during the Bush-era housing bubble...highly indebted Americans not only can’t spend the way they used to, they’re having to pay down the debts they ran up in the bubble years. This would be fine if someone else were taking up the slack. But what’s actually happening is that some people are spending much less while nobody is spending more — and this translates into a depressed economy and high unemployment. What the government should be doing in this situation is spending more while the private sector is spending less, supporting employment while those debts are paid down. And this government spending needs to be sustained..."

John Maynard Keynes believed that government institutions could stimulate aggregate demand in a crisis. 
“Keynes showed that if somehow the level of aggregate demand could be triggered, possibly by the government printing currency notes to employ people to dig holes and fill them up, the wages that would be paid out would resuscitate the economy by generating successive rounds of demand through the multiplier process”

Some recessions have been anticipated by the stock market declines. In "Stocks for the Long Run", Siegel mentions that since 1948, ten recessions were preceded by a stock market decline, by a lead time of 0 to 13 months (average 5.7 months), while ten stock market declines of greater than 10% in the Dow Jones Industrial Average were not followed by a recession.

The real-estate market also usually weakens before a recession. However real-estate declines can last much longer than recessions.

Since the business cycle is very hard to predict, Siegel argues that it is not possible to take advantage of economic cycles for timing investments. Even the National Bureau of Economic Research (NBER) takes a few months to determine if a peak or trough has occurred in the US.

During an economic decline, high-yield stocks such as fast-moving consumer goods, pharmaceuticals, and tobacco tend to hold up better. However, when the economy starts to recover and the bottom of the market has passed, growth stocks tend to recover faster. There is significant disagreement about how health care and utilities tend to recover. Diversifying one's portfolio into international stocks may provide some safety; however, economies that are closely correlated with that of the U.S. may also be affected by a recession in the U.S.

There is a view termed the "halfway rule" according to which investors start discounting an economic recovery about halfway through a recession. In the 16 U.S. recessions since 1919, the average length has been 13 months, although the recent recessions have been shorter. Thus, if the 2008 recession had followed the average, the downturn in the stock market would have bottomed around November 2008. The actual US stock market bottom of the 2008 recession was in March 2009.

Generally an administration gets credit or blame for the state of economy during its time. This has caused disagreements about on how it actually started. In an economic cycle, a downturn can be considered a consequence of an expansion reaching an unsustainable state, and is corrected by a brief decline. Thus it is not easy to isolate the causes of specific phases of the cycle.

The 1981 recession is thought to have been caused by the tight-money policy adopted by Paul Volcker, chairman of the Federal Reserve Board, before Ronald Reagan took office. Reagan supported that policy. Economist Walter Heller, chairman of the Council of Economic Advisers in the 1960s, said that "I call it a Reagan-Volcker-Carter recession." The resulting taming of inflation did, however, set the stage for a robust growth period during Reagan's presidency. .

Economists usually teach that to some degree recession is unavoidable, and its causes are not well understood.

Unemployment is particularly high during a recession. Many economists working within the neoclassical paradigm argue that there is a natural rate of unemployment which, when subtracted from the actual rate of unemployment, can be used to calculate the negative GDP gap during a recession. In other words, unemployment never reaches 0 percent, and thus is not a negative indicator of the health of an economy unless above the "natural rate," in which case it corresponds directly to a loss in the gross domestic product, or GDP.

The full impact of a recession on employment may not be felt for several quarters. Research in Britain shows that low-skilled, low-educated workers and the young are most vulnerable to unemployment in a downturn. After recessions in Britain in the 1980s and 1990s, it took five years for unemployment to fall back to its original levels. Many companies often expect employment discrimination claims to rise during a recession.

Productivity tends to fall in the early stages of a recession, then rises again as weaker firms close. The variation in profitability between firms rises sharply. The fall in productivity could also be attributed to several macro-economic factors, such as the loss in productivity observed across UK due to Brexit, which may create a mini-recession in the region. Global epidemics, such as COVID-19, could be another example, since they disrupt the global supply chain or prevent movement of goods, services and people.

Recessions have also provided opportunities for anti-competitive mergers, with a negative impact on the wider economy: the suspension of competition policy in the United States in the 1930s may have extended the Great Depression.

The living standards of people dependent on wages and salaries are not more affected by recessions than those who rely on fixed incomes or welfare benefits. The loss of a job is known to have a negative impact on the stability of families, and individuals' health and well-being. Fixed income benefits receive small cuts which make it tougher to survive.

According to the International Monetary Fund (IMF), "Global recessions seem to occur over a cycle lasting between eight and 10 years." The IMF takes many factors into account when defining a global recession. Until April 2009, IMF several times communicated to the press, that a global annual real GDP growth of 3.0 percent or less in their view was "...equivalent to a global recession".
By this measure, six periods since 1970 qualify: 1974–1975, 1980–1983, 1990–1993, 1998, 2001–2002, and 2008–2009. During what IMF in April 2002 termed the past three global recessions of the last three decades, global per capita output growth was zero or negative, and IMF argued—at that time—that because of the opposite being found for 2001, the economic state in this year by itself did not qualify as a "global recession".

In April 2009, IMF had changed their Global recession definition to:
By this new definition, a total of four global recessions took place since World War II: 1975, 1982, 1991 and 2009. All of them only lasted one year, although the third would have lasted three years (1991–93) if IMF as criteria had used the normal exchange rate weighted percapita real World GDP rather than the purchase power parity weighted percapita real World GDP.

The worst recession Australia has ever suffered happened in the beginning of the 1930s. As a result of late 1920s profit issues in agriculture and cutbacks, 1931-1932 saw Australia's biggest recession in its entire history. It fared better than other nations, that underwent depressions, but their poor economic states influenced Australia's as well, that depended on them for export, as well as foreign investments. The nation also benefited from bigger productivity in manufacturing, facilitated by trade protection, which also helped with feeling the effects less.

Due to a credit squeeze, the economy had gone into a brief recession in 1961
Australia was facing a rising level of inflation in 1973, caused partially by the oil crisis happening in that same year, which brought inflation at a 13% increase. Economic recession hit by the middle of the year 1974, with no change in policy enacted by the government as a measure to counter the economic situation of the country. Consequently, the unemployment level rose and the trade deficit increased significantly.

Another recession – the most recent one to date – came in the 1990s, at the beginning of the decade. It was the result of a major stock collapse in 1987, in October, referred to now as Black Monday. Although the collapse was larger than the one in 1929, the global economy recovered quickly, but North America still suffered a decline in lumbering savings and loans, which led to a crisis. The recession wasn't limited to only America, but it also affected partnering nations, such as Australia. The unemployment level increased to 10.8%, employment declined by 3.4% and the GDP also decreased as much as 1.7%. Inflation, however, was successfully reduced. Australia is facing recession in 2020 due to the impact of the bush fires and Covid-19 impacting tourism and other important aspects of the economy. 

The most recent recession to affect the United Kingdom was the 2020 recession attributed to the COVID‑19 global pandemic, the first recession since the late-2000s recession.

According to economists, since 1854, the U.S. has encountered 32 cycles of expansions and contractions, with an average of 17 months of contraction and 38 months of expansion. However, since 1980 there have been only eight periods of negative economic growth over one fiscal quarter or more, and four periods considered recessions:

For the past three recessions, the NBER decision has approximately conformed with the definition involving two consecutive quarters of decline. While the 2001 recession did not involve two consecutive quarters of decline, it was preceded by two quarters of alternating decline and weak growth.

Official economic data shows that a substantial number of nations were in recession as of early 2009. The US entered a recession at the end of 2007, and 2008 saw many other nations follow suit. The US recession of 2007 ended in June 2009 as the nation entered the current economic recovery. The timeline of the Great Recession details the many elements of this period.

The United States housing market correction (a consequence of the United States housing bubble) and subprime mortgage crisis significantly contributed to a recession.

The 2007–2009 recession saw private consumption fall for the first time in nearly 20 years. This indicated the depth and severity of the recession. With consumer confidence so low, economic recovery took a long time. Consumers in the U.S. were hit hard by the Great Recession, with the value of their houses dropping and their pension savings decimated on the stock market.

U.S. employers shed 63,000 jobs in February 2008, the most in five years. Former Federal Reserve chairman Alan Greenspan said on 6 April 2008 that "There is more than a 50 percent chance the United States could go into recession." On 1 October, the Bureau of Economic Analysis reported that an additional 156,000 jobs had been lost in September. On 29 April 2008, Moody's declared that nine US states were in a recession. In November 2008, employers eliminated 533,000 jobs, the largest single-month loss in 34 years. In 2008, an estimated 2.6 million U.S. jobs were eliminated.

The unemployment rate in the U.S. grew to 8.5 percent in March 2009, and there were 5.1 million job losses by March 2009 since the recession began in December 2007. That was about five million more people unemployed compared to just a year prior, which was the largest annual jump in the number of unemployed persons since the 1940s.

Although the US Economy grew in the first quarter by 1%, by June 2008 some analysts stated that due to a protracted credit crisis and "...rampant inflation in commodities such as oil, food, and steel," the country was nonetheless in a recession. The third quarter of 2008 brought on a GDP retraction of 0.5% the biggest decline since 2001. The 6.4% decline in spending during Q3 on non-durable goods, like clothing and food, was the largest since 1950.

A 17 November 2008 report from the Federal Reserve Bank of Philadelphia based on the survey of 51 forecasters, suggested that the recession started in April 2008 and would last 14 months. They project real GDP declining at an annual rate of 2.9% in the fourth quarter and 1.1% in the first quarter of 2009. These forecasts represent significant downward revisions from the forecasts of three months ago.

A 1 December 2008 report from the National Bureau of Economic Research stated that the U.S. had been in a recession since December 2007 (when economic activity peaked), based on a number of measures including job losses, declines in personal income, and declines in real GDP. By July 2009 a growing number of economists believed that the recession may have ended. The National Bureau of Economic Research announced on 20 September 2010 that the 2008/2009 recession ended in June 2009, making it the longest recession since World War II. Prior to the start of the recession, it appears that no known formal theoretical or empirical model was able to accurately predict the advance of this recession, except for minor signals in the sudden rise of forecasted probabilities, which were still well under 50%.



</doc>
<doc id="25385" url="https://en.wikipedia.org/wiki?curid=25385" title="RSA (cryptosystem)">
RSA (cryptosystem)

RSA (Rivest–Shamir–Adleman) is one of the first public-key cryptosystems and is widely used for secure data transmission. The acronym RSA is the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who publicly described the algorithm in 1977. In such a cryptosystem, the encryption key is public and distinct from the decryption key which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of factoring the product of two large prime numbers, the "factoring problem". Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), had developed an equivalent system in 1973, which was not declassified until 1997.

A user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but only someone with knowledge of the prime numbers can decode the message.
Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem is an open question. There are no published methods to defeat the system if a large enough key is used.

RSA is a relatively slow algorithm, and because of this, it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.

The idea of an asymmetric public-private key cryptosystem is attributed to Whitfield Diffie and Martin Hellman, who published this concept in 1976. They also introduced digital signatures and attempted to apply number theory. Their formulation used a shared-secret-key created from exponentiation of some number, modulo a prime number. However, they left open the problem of realizing a one-way function, possibly because the difficulty of factoring was not well-studied at the time.

Ron Rivest, Adi Shamir, and Leonard Adleman at the Massachusetts Institute of Technology, made several attempts over the course of a year to create a one-way function that was hard to invert. Rivest and Shamir, as computer scientists, proposed many potential functions, while Adleman, as a mathematician, was responsible for finding their weaknesses. They tried many approaches including "knapsack-based" and "permutation polynomials". For a time, they thought what they wanted to achieve was impossible due to contradictory requirements. In April 1977, they spent Passover at the house of a student and drank a good deal of Manischewitz wine before returning to their homes at around midnight. Rivest, unable to sleep, lay on the couch with a math textbook and started thinking about their one-way function. He spent the rest of the night formalizing his idea, and he had much of the paper ready by daybreak. The algorithm is now known as RSA – the initials of their surnames in same order as their paper.

Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), described an equivalent system in an internal document in 1973. However, given the relatively expensive computers needed to implement it at the time, it was considered to be mostly a curiosity and, as far as is publicly known, was never deployed. His discovery, however, was not revealed until 1997 due to its top-secret classification.

Kid-RSA (KRSA) is a simplified public-key cipher published in 1997, designed for educational purposes. Some people feel that learning Kid-RSA gives insight into RSA and other public-key ciphers, analogous to simplified DES.

MIT was granted for a "Cryptographic communications system and method" that used the algorithm, on September 20, 1983. Though the patent was going to expire on September 21, 2000 (the term of patent was 17 years at the time), the algorithm was released to the public domain by RSA Security on September 6, 2000, two weeks earlier. Since a detailed description of the algorithm had been published in the Mathematical Games column in the August 1977 issue of Scientific American, prior to the December 1977 filing date of the patent application, regulations in much of the rest of the world precluded patents elsewhere and only the US patent was granted. Had Cocks's work been publicly known, a patent in the United States would not have been legal either.

From the DWPI's abstract of the patent,
The RSA algorithm involves four steps: key generation, key distribution, encryption and decryption.

A basic principle behind RSA is the observation that it is practical to find three very large positive integers , and such that with modular exponentiation for all integers (with ):

and that knowing and , or even , it can be extremely difficult to find . The triple bar (≡) here denotes modular congruence.

In addition, for some operations it is convenient that the order of the two exponentiations can be changed and that this relation also implies:

RSA involves a "public key" and a "private key." The public key can be known by everyone, and it is used for encrypting messages. The intention is that messages encrypted with the public key can only be decrypted in a reasonable amount of time by using the private key. The public key is represented by the integers and ; and, the private key, by the integer (although is also used during the decryption process. Thus, it might be considered to be a part of the private key, too). represents the message (previously prepared with a certain technique explained below).

The keys for the RSA algorithm are generated in the following way:
The "public key" consists of the modulus "n" and the public (or encryption) exponent "e". The "private key" consists of the private (or decryption) exponent "d", which must be kept secret. "p", "q", and "λ"("n") must also be kept secret because they can be used to calculate "d". In fact, they can all be discarded after "d" has been computed.

In the original RSA paper, the Euler totient function is used instead of "λ"("n") for calculating the private exponent "d". Since "φ"("n") is always divisible by "λ"("n") the algorithm works as well. That the Euler totient function can be used can also be seen as a consequence of the Lagrange's theorem applied to the multiplicative group of integers modulo pq. Thus any "d" satisfying also satisfies . However, computing "d" modulo "φ"("n") will sometimes yield a result that is larger than necessary (i.e. ). Most of the implementations of RSA will accept exponents generated using either method (if they use the private exponent "d" at all, rather than using the optimized decryption method based on the Chinese remainder theorem described below), but some standards like FIPS 186-4 may require that . Any "oversized" private exponents not meeting that criterion may always be reduced modulo "λ"("n") to obtain a smaller equivalent exponent.

Since any common factors of and are present in the factorisation of = = , it is recommended that and have only very small common factors, if any besides the necessary 2.

Note: The authors of the original RSA paper carry out the key generation by choosing "d" and then computing "e" as the modular multiplicative inverse of "d" modulo "φ"("n"), whereas most current implementations of RSA, such as those following PKCS#1, do the reverse (choose "e" and compute "d"). Since the chosen key can be small whereas the computed key normally is not, the RSA paper's algorithm optimizes decryption compared to encryption, while the modern algorithm optimizes encryption instead.

Suppose that Bob wants to send information to Alice. If they decide to use RSA, Bob must know Alice's public key to encrypt the message and Alice must use her private key to decrypt the message.
To enable Bob to send his encrypted messages, Alice transmits her public key to Bob via a reliable, but not necessarily secret, route. Alice's private key is never distributed.

After Bob obtains Alice's public key, he can send a message to Alice.

To do it, he first turns (strictly speaking, the un-padded plaintext) into an integer (strictly speaking, the padded plaintext), such that by using an agreed-upon reversible protocol known as a padding scheme. He then computes the ciphertext , using Alice's public key , corresponding to

This can be done reasonably quickly, even for very large numbers, using modular exponentiation. Bob then transmits to Alice.
Alice can recover from by using her private key exponent by computing

Given , she can recover the original message by reversing the padding scheme.

Here is an example of RSA encryption and decryption. The parameters used here are artificially small, but one can also .


The public key is (, ). For a padded plaintext message "m", the encryption function is

The private key is (, ). For an encrypted ciphertext "c", the decryption function is

For instance, in order to encrypt , we calculate

To decrypt , we calculate

Both of these calculations can be computed efficiently using the square-and-multiply algorithm for modular exponentiation. In real-life situations the primes selected would be much larger; in our example it would be trivial to factor "n", 3233 (obtained from the freely available public key) back to the primes "p" and "q". "e", also from the public key, is then inverted to get "d", thus acquiring the private key.

Practical implementations use the Chinese remainder theorem to speed up the calculation using modulus of factors (mod "pq" using mod "p" and mod "q").

The values "d", "d" and "q", which are part of the private key are computed as follows:

Here is how "d", "d" and "q" are used for efficient decryption. (Encryption is efficient by choice of a suitable "d" and "e" pair)

Suppose Alice uses Bob's public key to send him an encrypted message. In the message, she can claim to be Alice but Bob has no way of verifying that the message was actually from Alice since anyone can use Bob's public key to send him encrypted messages. In order to verify the origin of a message, RSA can also be used to sign a message.

Suppose Alice wishes to send a signed message to Bob. She can use her own private key to do so. She produces a hash value of the message, raises it to the power of "d" (modulo "n") (as she does when decrypting a message), and attaches it as a "signature" to the message. When Bob receives the signed message, he uses the same hash algorithm in conjunction with Alice's public key. He raises the signature to the power of "e" (modulo "n") (as he does when encrypting a message), and compares the resulting hash value with the message's actual hash value. If the two agree, he knows that the author of the message was in possession of Alice's private key, and that the message has not been tampered with since.

This works because of exponentiation rules:

Thus, the keys may be swapped without loss of generality, that is a private key of a key pair may be used either to:

The proof of the correctness of RSA is based on Fermat's little theorem, stating that for any integer "a" and prime "p", not dividing "a".

We want to show that
for every integer "m" when "p" and "q" are distinct prime numbers and "e" and "d" are positive integers satisfying .

Since is, by construction, divisible by both and , we can write
for some nonnegative integers "h" and "k".

To check whether two numbers, like "m" and "m", are congruent mod "pq", it suffices (and in fact is equivalent) to check that they are congruent mod "p" and mod "q" separately. 

To show , we consider two cases:

The verification that proceeds in a completely analogous way:

This completes the proof that, for any integer "m", and integers "e", "d" such that ,

"Notes:"
Although the original paper of Rivest, Shamir, and Adleman used Fermat's little theorem to explain why RSA works, it is common to find proofs that rely instead on Euler's theorem.

We want to show that , where is a product of two different prime numbers and "e" and "d" are positive integers satisfying . Since "e" and "d" are positive, we can write for some non-negative integer "h". "Assuming" that "m" is relatively prime to "n", we have

where the second-last congruence follows from Euler's theorem.

More generally, for any "e" and "d" satisfying , the same conclusion follows from Carmichael's generalization of Euler's theorem, which states that for all "m" relatively prime to "n".

When "m" is not relatively prime to "n", the argument just given is invalid. This is highly improbable (only a proportion of numbers have this property), but even in this case the desired congruence is still true. Either or , and these cases can be treated using the previous proof.

There are a number of attacks against plain RSA as described below.


To avoid these problems, practical RSA implementations typically embed some form of structured, randomized padding into the value "m" before encrypting it. This padding ensures that "m" does not fall into the range of insecure plaintexts, and that a given message, once padded, will encrypt to one of a large number of different possible ciphertexts.

Standards such as PKCS#1 have been carefully designed to securely pad messages prior to RSA encryption. Because these schemes pad the plaintext "m" with some number of additional bits, the size of the un-padded message "M" must be somewhat smaller. RSA padding schemes must be carefully designed so as to prevent sophisticated attacks which may be facilitated by a predictable message structure. Early versions of the PKCS#1 standard (up to version 1.5) used a construction that appears to make RSA semantically secure. However, at Crypto 1998, Bleichenbacher showed that this version is vulnerable to a practical adaptive chosen ciphertext attack. Furthermore, at Eurocrypt 2000, Coron et al. showed that for some types of messages, this padding does not provide a high enough level of security. Later versions of the standard include Optimal Asymmetric Encryption Padding (OAEP), which prevents these attacks. As such, OAEP should be used in any new application, and PKCS#1 v1.5 padding should be replaced wherever possible. The PKCS#1 standard also incorporates processing schemes designed to provide additional security for RSA signatures, e.g. the Probabilistic Signature Scheme for RSA (RSA-PSS).

Secure padding schemes such as RSA-PSS are as essential for the security of message signing as they are for message encryption. Two US patents on PSS were granted (USPTO 6266771 and USPTO 70360140); however, these patents expired on 24 July 2009 and 25 April 2010, respectively. Use of PSS no longer seems to be encumbered by patents. Note that using different RSA key-pairs for encryption and signing is potentially more secure.

For efficiency many popular crypto libraries (like OpenSSL, Java and .NET) use the following optimization for decryption and signing based on the Chinese remainder theorem. The following values are precomputed and stored as part of the private key:

These values allow the recipient to compute the exponentiation more efficiently as follows:

This is more efficient than computing exponentiation by squaring even though two modular exponentiations have to be computed. The reason is that these two modular exponentiations both use a smaller exponent and a smaller modulus.

The security of the RSA cryptosystem is based on two mathematical problems: the problem of factoring large numbers and the RSA problem. Full decryption of an RSA ciphertext is thought to be infeasible on the assumption that both of these problems are hard, i.e., no efficient algorithm exists for solving them. Providing security against "partial" decryption may require the addition of a secure padding scheme.

The RSA problem is defined as the task of taking "e"th roots modulo a composite "n": recovering a value "m" such that , where is an RSA public key and "c" is an RSA ciphertext. Currently the most promising approach to solving the RSA problem is to factor the modulus "n". With the ability to recover prime factors, an attacker can compute the secret exponent "d" from a public key , then decrypt "c" using the standard procedure. To accomplish this, an attacker factors "n" into "p" and "q", and computes which allows the determination of "d" from "e". No polynomial-time method for factoring large integers on a classical computer has yet been found, but it has not been proven that none exists. "See integer factorization for a discussion of this problem".

Multiple polynomial quadratic sieve (MPQS) can be used to factor the public modulus "n".

The first RSA-512 factorization in 1999 used hundreds of computers and required the equivalent of 8,400 MIPS years, over an elapsed time of about seven months. By 2009, Benjamin Moody could factor an RSA-512 bit key in 73 days using only public software (GGNFS) and his desktop computer (a dual-core Athlon64 with a 1,900 MHz cpu). Just less than five gigabytes of disk storage was required and about 2.5 gigabytes of RAM for the sieving process.

Rivest, Shamir, and Adleman noted that Miller has shown that – assuming the truth of the Extended Riemann Hypothesis – finding "d" from "n" and "e" is as hard as factoring "n" into "p" and "q" (up to a polynomial time difference). However, Rivest, Shamir, and Adleman noted, in section IX/D of their paper, that they had not found a proof that inverting RSA is equally as hard as factoring.

, the largest publicly known factored RSA number was 829 bits (250 decimal digits, RSA-250). Its factorization, by a state-of-the-art distributed implementation, took approximately 2700 CPU years. In practice, RSA keys are typically 1024 to 4096 bits long. RSA Security thought that 1024-bit keys were likely to become crackable by 2010,; as of 2020 it's not known that it has been, but minimum recommendations have moved to at least 2048 bits. It is generally presumed that RSA is secure if "n" is sufficiently large, outside of quantum computing.

If "n" is 300 bits or shorter, it can be factored in a few hours in a personal computer, using software already freely available. Keys of 512 bits have been shown to be practically breakable in 1999 when RSA-155 was factored by using several hundred computers, and these are now factored in a few weeks using common hardware. Exploits using 512-bit code-signing certificates that may have been factored were reported in 2011. A theoretical hardware device named TWIRL, described by Shamir and Tromer in 2003, called into question the security of 1024 bit keys.
In 1994, Peter Shor showed that a quantum computer – if one could ever be practically created for the purpose – would be able to factor in polynomial time, breaking RSA; see Shor's algorithm.

Finding the large primes "p" and "q" is usually done by testing random numbers of the right size with probabilistic primality tests that quickly eliminate virtually all of the nonprimes.

The numbers "p" and "q" should not be "too close", lest the Fermat factorization for "n" be successful. If "p" − "q" is less than 2"n" ("n" = "p" * "q", which for even small 1024-bit values of "n" is ) solving for "p" and "q" is trivial. Furthermore, if either "p" − 1 or "q" − 1 has only small prime factors, "n" can be factored quickly by Pollard's p − 1 algorithm, and such values of "p" or "q" should hence be discarded.

It is important that the private exponent "d" be large enough. Michael J. Wiener showed that if "p" is between "q" and 2"q" (which is quite typical) and , then "d" can be computed efficiently from "n" and "e".

There is no known attack against small public exponents such as , provided that the proper padding is used. Coppersmith's Attack has many applications in attacking RSA specifically if the public exponent "e" is small and if the encrypted message is short and not padded. 65537 is a commonly used value for "e"; this value can be regarded as a compromise between avoiding potential small exponent attacks and still allowing efficient encryptions (or signature verification). The NIST Special Publication on Computer Security (SP 800-78 Rev 1 of August 2007) does not allow public exponents "e" smaller than 65537, but does not state a reason for this restriction.

In October 2017, a team of researchers from Masaryk University announced the ROCA vulnerability, which affects RSA keys generated by an algorithm embodied in a library from Infineon known as RSALib. Large number of smart cards and trusted platform modules (TPMs) were shown to be affected. Vulnerable RSA keys are easily identified using a test program the team released.

A cryptographically strong random number generator, which has been properly seeded with adequate entropy, must be used to generate the primes "p" and "q". An analysis comparing millions of public keys gathered from the Internet was carried out in early 2012 by Arjen K. Lenstra, James P. Hughes, Maxime Augier, Joppe W. Bos, Thorsten Kleinjung and Christophe Wachter. They were able to factor 0.2% of the keys using only Euclid's algorithm.

They exploited a weakness unique to cryptosystems based on integer factorization. If is one public key and is another, then if by chance (but "q" is not equal to "q"′), then a simple computation of factors both "n" and "n"′, totally compromising both keys. Lenstra et al. note that this problem can be minimized by using a strong random seed of bit-length twice the intended security level, or by employing a deterministic function to choose "q" given "p", instead of choosing "p" and "q" independently.

Nadia Heninger was part of a group that did a similar experiment. They used an idea of Daniel J. Bernstein to compute the GCD of each RSA key "n" against the product of all the other keys "n"′ they had found (a 729 million digit number), instead of computing each gcd("n","n"′) separately, thereby achieving a very significant speedup since after one large division the GCD problem is of normal size.

Heninger says in her blog that the bad keys occurred almost entirely in embedded applications, including "firewalls, routers, VPN devices, remote server administration devices, printers, projectors, and VOIP phones" from over 30 manufacturers. Heninger explains that the one-shared-prime problem uncovered by the two groups results from situations where the pseudorandom number generator is poorly seeded initially and then reseeded between the generation of the first and second primes. Using seeds of sufficiently high entropy obtained from key stroke timings or electronic diode noise or atmospheric noise from a radio receiver tuned between stations should solve the problem.

Strong random number generation is important throughout every phase of public key cryptography. For instance, if a weak generator is used for the symmetric keys that are being distributed by RSA, then an eavesdropper could bypass RSA and guess the symmetric keys directly.

Kocher described a new attack on RSA in 1995: if the attacker Eve knows Alice's hardware in sufficient detail and is able to measure the decryption times for several known ciphertexts, Eve can deduce the decryption key "d" quickly. This attack can also be applied against the RSA signature scheme. In 2003, Boneh and Brumley demonstrated a more practical attack capable of recovering RSA factorizations over a network connection (e.g., from a Secure Sockets Layer (SSL)-enabled webserver) This attack takes advantage of information leaked by the Chinese remainder theorem optimization used by many RSA implementations.

One way to thwart these attacks is to ensure that the decryption operation takes a constant amount of time for every ciphertext. However, this approach can significantly reduce performance. Instead, most RSA implementations use an alternate technique known as cryptographic blinding. RSA blinding makes use of the multiplicative property of RSA. Instead of computing , Alice first chooses a secret random value "r" and computes . The result of this computation after applying Euler's Theorem is and so the effect of "r" can be removed by multiplying by its inverse. A new value of "r" is chosen for each ciphertext. With blinding applied, the decryption time is no longer correlated to the value of the input ciphertext and so the timing attack fails.

In 1998, Daniel Bleichenbacher described the first practical adaptive chosen ciphertext attack, against RSA-encrypted messages using the PKCS #1 v1 padding scheme (a padding scheme randomizes and adds structure to an RSA-encrypted message, so it is possible to determine whether a decrypted message is valid). Due to flaws with the PKCS #1 scheme, Bleichenbacher was able to mount a practical attack against RSA implementations of the Secure Socket Layer protocol, and to recover session keys. As a result of this work, cryptographers now recommend the use of provably secure padding schemes such as Optimal Asymmetric Encryption Padding, and RSA Laboratories has released new versions of PKCS #1 that are not vulnerable to these attacks.

A side-channel attack using branch prediction analysis (BPA) has been described. Many processors use a branch predictor to determine whether a conditional branch in the instruction flow of a program is likely to be taken or not. Often these processors also implement simultaneous multithreading (SMT). Branch prediction analysis attacks use a spy process to discover (statistically) the private key when processed with these processors.

Simple Branch Prediction Analysis (SBPA) claims to improve BPA in a non-statistical way. In their paper, "On the Power of Simple Branch Prediction Analysis", the authors of SBPA (Onur Aciicmez and Cetin Kaya Koc) claim to have discovered 508 out of 512 bits of an RSA key in 10 iterations.

A power fault attack on RSA implementations has been described in 2010. The author recovered the key by varying the CPU power voltage outside limits; this caused multiple power faults on the server.

The generated primes can be attacked by rainbow tables because the random numbers are fixed and finite sets.

Some cryptography libraries that provide support for RSA include:






</doc>
<doc id="25389" url="https://en.wikipedia.org/wiki?curid=25389" title="Robert A. Heinlein">
Robert A. Heinlein

Robert Anson Heinlein (; July 7, 1907 – May 8, 1988) was an American science-fiction author, aeronautical engineer, and retired Naval officer. Sometimes called the "dean of science fiction writers", he was among the first to emphasize scientific accuracy in his fiction, and was thus a pioneer of the subgenre of hard science fiction. His published works, both fiction and non-fiction, express admiration for competence and emphasize the value of critical thinking. His work continues to have an influence on the science-fiction genre, and on modern culture more generally.

Heinlein became one of the first American science-fiction writers to break into mainstream magazines such as "The Saturday Evening Post" in the late 1940s. He was one of the best-selling science-fiction novelists for many decades, and he, Isaac Asimov, and Arthur C. Clarke are often considered the "Big Three" of English-language science fiction authors. Notable Heinlein works include "Stranger in a Strange Land", "Starship Troopers" (which helped mold the space marine and mecha archetypes) and "The Moon Is a Harsh Mistress". His work sometimes had controversial aspects, such as plural marriage in "The Moon is a Harsh Mistress", militarism in "Starship Troopers" and technologically competent women characters that were formidable, yet often stereotypically feminine – such as "Friday".

A writer also of numerous science-fiction short stories, Heinlein was one of a group of writers who came to prominence under the editorship (1937–1971) of John W. Campbell at "Astounding Science Fiction" magazine, though Heinlein denied that Campbell influenced his writing to any great degree.

Heinlein used his science fiction as a way to explore provocative social and political ideas, and to speculate how progress in science and engineering might shape the future of politics, race, religion, and sex. Within the framework of his science-fiction stories, Heinlein repeatedly addressed certain social themes: the importance of individual liberty and self-reliance, the nature of sexual relationships, the obligation individuals owe to their societies, the influence of organized religion on culture and government, and the tendency of society to repress nonconformist thought. He also speculated on the influence of space travel on human cultural practices.

Heinlein was named the first Science Fiction Writers Grand Master in 1974. Four of his novels won Hugo Awards. In addition, fifty years after publication, seven of his works were awarded "Retro Hugos"—awards given retrospectively for works that were published before the Hugo Awards came into existence. In his fiction, Heinlein coined terms that have become part of the English language, including grok,
waldo and speculative fiction, as well as popularizing existing terms like "TANSTAAFL", "pay it forward", and "space marine". He also anticipated mechanical computer-aided design with "Drafting Dan" and described a modern version of a waterbed in his novel "Beyond This Horizon". In the first chapter of the novel "Space Cadet" he anticipated the cell phone, 35 years before Motorola invented the technology. Several of Heinlein's works have been adapted for film and television.

Heinlein, born on July 7, 1907, to Rex Ivar Heinlein (an accountant) and Bam Lyle Heinlein, in Butler, Missouri, and was the third of seven children. He was a sixth-generation German-American; a family tradition had it that Heinleins fought in every American war, starting with the War of Independence.

He spent his childhood in Kansas City, Missouri.
The outlook and values of this time and place (in his own words, "The Bible Belt") had a definite influence on his fiction, especially in his later works, as he drew heavily upon his childhood in establishing the setting and cultural atmosphere in works like "Time Enough for Love" and "To Sail Beyond the Sunset". The 1910 appearance of Halley's Comet inspired the young child's life-long interest in astronomy.

When Heinlein graduated from Central High School in Kansas City in 1924, he aspired to a career as an officer in the United States Navy. However, he was initially prevented from attending the United States Naval Academy at Annapolis because his older brother Rex was a student there, and regulations discouraged multiple family-members from attending the Academy simultaneously. He instead matriculated at Kansas City Community College and began vigorously petitioning Missouri Senator James A. Reed for an appointment to the Naval Academy. In part due to the influence of the Pendergast machine, the Naval Academy admitted him in June 1925.

Heinlein's experience in the Navy exerted a strong influence on his character and writing. In 1929, he graduated from the Naval Academy with the equivalent of a Bachelor of Arts degree in Engineering, ranking fifth in his class academically but with a class standing of 20th of 243 due to disciplinary demerits. Shortly after graduation, he was commissioned as an ensign by the U.S. Navy. He advanced to lieutenant, junior grade while serving aboard the new aircraft carrier in 1931. He worked in radio communications with the carrier’s aircraft. Radio communications was then in its earlier phases. The captain of this carrier was Ernest J. King, who served as the Chief of Naval Operations and Commander-in-Chief, U.S. Fleet during World War II. Heinlein was frequently interviewed during his later years by military historians who asked him about Captain King and his service as the commander of the U.S. Navy's first modern aircraft carrier. Heinlein also served as gunnery officer aboard the destroyer in 1933 and 1934, reaching the rank of lieutenant. His brother, Lawrence Heinlein, served in the U.S. Army, the U.S. Air Force, and the Missouri National Guard, reaching the rank of major general in the National Guard.

In 1929, Heinlein married Elinor Curry of Kansas City. However, their marriage only lasted about a year. His second marriage in 1932 to Leslyn MacDonald (1904–1981) lasted for 15 years. MacDonald was, according to the testimony of Heinlein's Navy friend, Rear Admiral Cal Laning, "astonishingly intelligent, widely read, and extremely liberal, though a registered Republican", while Isaac Asimov later recalled that Heinlein was, at the time, "a flaming liberal". "(See section: Politics of Robert Heinlein.)"

At the Philadelphia Naval Shipyard Heinlein met and befriended a chemical engineer named Virginia "Ginny" Gerstenfeld. After the war, her engagement having fallen through, she moved to UCLA for doctoral studies in chemistry and made contact again.

As his second wife's alcoholism gradually spun out of control, Heinlein moved out and the couple filed for divorce. Heinlein's friendship with Virginia turned into a relationship and on October 21, 1948—shortly after the decree nisi came through—they married in the town of Raton, New Mexico, shortly after setting up housekeeping in Colorado. They remained married until Heinlein's death.

As Heinlein's increasing success as a writer resolved their initial financial woes, they had a house custom built with various innovative features, later described in an article in "Popular Mechanics". In 1965, after various chronic health problems of Virginia's were traced back to altitude sickness, they moved to Santa Cruz, California, which is at sea level. They built a new residence in the adjacent village of Bonny Doon, California. Robert and Virginia designed and built their California house in Bonny Doon themselves; the home is in a circular shape.

Ginny undoubtedly served as a model for many of his intelligent, fiercely independent female characters. She was a chemist and rocket test engineer, and held a higher rank in the Navy than Heinlein himself. She was also an accomplished college athlete, earning four letters. In 1953–1954, the Heinleins voyaged around the world (mostly via ocean liners and cargo liners, as Ginny detested flying), which Heinlein described in "Tramp Royale", and which also provided background material for science fiction novels set aboard spaceships on long voyages, such as "Podkayne of Mars", "Friday" and "", the latter initially being set on a cruise much as detailed in "Tramp Royale". Ginny acted as the first reader of his manuscripts. Isaac Asimov believed that Heinlein made a swing to the right politically at the same time he married Ginny.

In 1934, Heinlein was discharged from the Navy due to pulmonary tuberculosis. During a lengthy hospitalization, and inspired by his own experience while bed-ridden, he developed a design for a waterbed.

After his discharge, Heinlein attended a few weeks of graduate classes in mathematics and physics at the University of California at Los Angeles (UCLA), but he soon quit either because of his health or from a desire to enter politics.

Heinlein supported himself at several occupations, including real estate sales and silver mining, but for some years found money in short supply. Heinlein was active in Upton Sinclair's socialist End Poverty in California movement (EPIC) in the early 1930s. He was deputy publisher of the "EPIC News", which Heinlein noted "recalled a mayor, kicked out a district attorney, replaced the governor with one of our choice." When Sinclair gained the Democratic nomination for Governor of California in 1934, Heinlein worked actively in the campaign. Heinlein himself ran for the California State Assembly in 1938, but was unsuccessful. Heinlein was running as a left-wing Democrat in a conservative district, and he never made it past the Democratic primary because of trickery by his Republican opponent .

While not destitute after the campaign—he had a small disability pension from the Navy—Heinlein turned to writing to pay off his mortgage. His first published story, "Life-Line", was printed in the August 1939 issue of "Astounding Science Fiction". Originally written for a contest, he sold it to "Astounding" for significantly more than the contest's first-prize payoff. Another Future History story, "Misfit", followed in November. Some saw Heinlein's talent and stardom from his first story, and he was quickly acknowledged as a leader of the new movement toward "social" science fiction. In California he hosted the Mañana Literary Society, a 1940–41 series of informal gatherings of new authors. He was the guest of honor at Denvention, the 1941 Worldcon, held in Denver. During World War II, Heinlein was employed by the Navy as a civilian aeronautical engineer at the Navy Aircraft Materials Center at the Philadelphia Naval Shipyard in Pennsylvania. Heinlein recruited Isaac Asimov and L. Sprague de Camp to also work there. While at the Philadelphia Naval Shipyards, Asimov, Heinlein, and de Camp brainstormed unconventional approaches to kamikaze attacks, such as using sound to detect approaching planes.

As the war wound down in 1945, Heinlein began to re-evaluate his career. The atomic bombings of Hiroshima and Nagasaki, along with the outbreak of the Cold War, galvanized him to write nonfiction on political topics. In addition, he wanted to break into better-paying markets. He published four influential short stories for "The Saturday Evening Post" magazine, leading off, in February 1947, with "The Green Hills of Earth". That made him the first science fiction writer to break out of the "pulp ghetto". In 1950, the movie "Destination Moon"—the documentary-like film for which he had written the story and scenario, co-written the script, and invented many of the effects—won an Academy Award for special effects. Also, he embarked on a series of juvenile novels for the Charles Scribner's Sons publishing company that went from 1947 through 1959, at the rate of one book each autumn, in time for Christmas presents to teenagers. He also wrote for "Boys' Life" in 1952.

Heinlein had used topical materials throughout his juvenile series beginning in 1947, but in 1958 he interrupted work on "The Heretic" (the working title of "Stranger in a Strange Land") to write and publish a book exploring ideas of civic virtue, initially serialized as "Starship Soldiers". In 1959, his novel (now entitled "Starship Troopers") was considered by the editors and owners of Scribner's to be too controversial for one of its prestige lines, and it was rejected. Heinlein found another publisher (Putnam), feeling himself released from the constraints of writing novels for children. He had told an interviewer that he did not want to do stories that merely added to categories defined by other works. Rather he wanted to do his own work, stating that: "I want to do my own stuff, my own way". He would go on to write a series of challenging books that redrew the boundaries of science fiction, including "Stranger in a Strange Land" (1961) and "The Moon Is a Harsh Mistress" (1966).

Beginning in 1970, Heinlein had a series of health crises, broken by strenuous periods of activity in his hobby of stonemasonry: in a private correspondence, he referred to that as his "usual and favorite occupation between books". The decade began with a life-threatening attack of peritonitis, recovery from which required more than two years, and treatment of which required multiple transfusions of Heinlein's rare blood type, A2 negative. As soon as he was well enough to write again, he began work on "Time Enough for Love" (1973), which introduced many of the themes found in his later fiction.

In the mid-1970s, Heinlein wrote two articles for the "Britannica Compton Yearbook". He and Ginny crisscrossed the country helping to reorganize blood donation in the United States in an effort to assist the system which had saved his life. At science fiction conventions to receive his autograph, fans would be asked to co-sign with Heinlein a beautifully embellished pledge form he supplied stating that the recipient agrees that they will donate blood. He was the guest of honor at the Worldcon in 1976 for the third time at MidAmeriCon in Kansas City, Missouri. At that Worldcon, Heinlein hosted a blood drive and donors' reception to thank all those who had helped save lives.

Beginning in 1977 and including an episode while vacationing in Tahiti in early 1978, he had episodes of reversible neurologic dysfunction due to transient ischemic attacks. Over the next few months, he became more and more exhausted, and his health again began to decline. The problem was determined to be a blocked carotid artery, and he had one of the earliest known carotid bypass operations to correct it. Heinlein and Virginia had been smokers, and smoking appears often in his fiction, as do fictitious strikable self-lighting cigarettes.

In 1980 Robert Heinlein was a member of the Citizens Advisory Council on National Space Policy, chaired by Jerry Pournelle, which met at the home of SF writer Larry Niven to write space policy papers for the incoming Reagan Administration. Members included such aerospace industry leaders as former astronaut Buzz Aldrin, General Daniel O. Graham, aerospace engineer Max Hunter and North American Rockwell VP for Space Shuttle development George Merrick. Policy recommendations from the Council included ballistic missile defense concepts which were later transformed into what was called the Strategic Defense Initiative, or "Star Wars" as derided by Senator Ted Kennedy. Heinlein assisted with Council contribution to the Reagan "Star Wars" speech of Spring 1983.

Asked to appear before a Joint Committee of the United States Congress that year, he testified on his belief that spin-offs from space technology were benefiting the infirm and the elderly. Heinlein's surgical treatment re-energized him, and he wrote five novels from 1980 until he died in his sleep from emphysema and heart failure on May 8, 1988.

At that time, he had been putting together the early notes for another "World as Myth" novel. Several of his other works have been published posthumously. Based on an outline and notes created by Heinlein in 1955, Spider Robinson has written the novel "Variable Star". Heinlein's posthumously published nonfiction includes a selection of correspondence and notes edited into a somewhat autobiographical examination of his career, published in 1989 under the title "Grumbles from the Grave" by his wife, Virginia; his book on practical politics written in 1946 published as "Take Back Your Government"; and a travelogue of their first around-the-world tour in 1954, "Tramp Royale". The novels "Podkayne of Mars" and "Red Planet", which were edited against his wishes in their original release, have been reissued in restored editions. "Stranger In a Strange Land" was originally published in a shorter form, but both the long and short versions are now simultaneously available in print.

Heinlein's archive is housed by the Special Collections department of McHenry Library at the University of California at Santa Cruz. The collection includes manuscript drafts, correspondence, photographs and artifacts. A substantial portion of the archive has been digitized and it is available online through the Robert A. and Virginia Heinlein Archives.

Heinlein published 32 novels, 59 short stories, and 16 collections during his life. Four films, two television series, several episodes of a radio series, and a board game have been derived more or less directly from his work. He wrote a screenplay for one of the films. Heinlein edited an anthology of other writers' SF short stories.

Three nonfiction books and two poems have been published posthumously. "For Us, the Living: A Comedy of Customs" was published posthumously in 2003; "Variable Star", written by Spider Robinson based on an extensive outline by Heinlein, was published in September 2006. Four collections have been published posthumously.

Over the course of his career, Heinlein wrote three somewhat overlapping series:

Heinlein began his career as a writer of stories for "Astounding Science Fiction" magazine, which was edited by John Campbell. The science fiction writer Frederik Pohl has described Heinlein as "that greatest of Campbell-era sf writers". Isaac Asimov said that, from the time of his first story, the science fiction world accepted that Heinlein was the best science fiction writer in existence, adding that he would hold this title through his lifetime.

Alexei and Cory Panshin noted that Heinlein's impact was immediately felt. In 1940, the year after selling 'Life-Line' to Campbell, he wrote three short novels, four novelettes, and seven short stories. They went on to say that "No one ever dominated the science fiction field as Bob did in the first few years of his career." Alexei expresses awe in Heinlein's ability to show readers a world so drastically different from the one we live in now, yet have so many similarities. He says that "We find ourselves not only in a world other than our own, but identifying with a living, breathing individual who is operating within its context, and thinking and acting according to its terms."

The first novel that Heinlein wrote, "" (1939), did not see print during his lifetime, but Robert James tracked down the manuscript and it was published in 2003. Though some regard it as a failure as a novel, considering it little more than a disguised lecture on Heinlein's social theories, some readers took a very different view. In a review of it, John Clute wrote: I'm not about to suggest that if Heinlein had been able to publish [such works] openly in the pages of "Astounding" in 1939, SF would have gotten the future right; I would suggest, however, that if Heinlein, and his colleagues, had been able to publish adult SF in "Astounding" and its fellow journals, then SF might not have done such a grotesquely poor job of prefiguring something of the flavor of actually living here at the onset of 2004.

"For Us, the Living" was intriguing as a window into the development of Heinlein's radical ideas about man as a social animal, including his interest in free love. The root of many themes found in his later stories can be found in this book. It also contained a large amount of material that could be considered background for his other novels. This included a detailed description of the protagonist's treatment to avoid being banned to Coventry (a lawless land in the Heinlein mythos where unrepentant law-breakers are exiled).
It appears that Heinlein at least attempted to live in a manner consistent with these ideals, even in the 1930s, and had an open relationship in his marriage to his second wife, Leslyn. He was also a nudist; nudism and body taboos are frequently discussed in his work. At the height of the Cold War, he built a bomb shelter under his house, like the one featured in "Farnham's Freehold".

After "For Us, the Living", Heinlein began selling (to magazines) first short stories, then novels, set in a Future History, complete with a time line of significant political, cultural, and technological changes. A chart of the future history was published in the May 1941 issue of "Astounding". Over time, Heinlein wrote many novels and short stories that deviated freely from the Future History on some points, while maintaining consistency in some other areas. The Future History was eventually overtaken by actual events. These discrepancies were explained, after a fashion, in his later World as Myth stories.

Heinlein's first novel published as a book, "Rocket Ship Galileo", was initially rejected because going to the moon was considered too far-fetched, but he soon found a publisher, Scribner's, that began publishing a Heinlein juvenile once a year for the Christmas season. Eight of these books were illustrated by Clifford Geary in a distinctive white-on-black scratchboard style. Some representative novels of this type are "Have Space Suit—Will Travel", "Farmer in the Sky", and "Starman Jones". Many of these were first published in serial form under other titles, e.g., "Farmer in the Sky" was published as "Satellite Scout" in the Boy Scout magazine "Boys' Life". There has been speculation that Heinlein's intense obsession with his privacy was due at least in part to the apparent contradiction between his unconventional private life and his career as an author of books for children. However, "For Us, the Living" explicitly discusses the political importance Heinlein attached to privacy as a matter of principle.

The novels that Heinlein wrote for a young audience are commonly called "the Heinlein juveniles", and they feature a mixture of adolescent and adult themes. Many of the issues that he takes on in these books have to do with the kinds of problems that adolescents experience. His protagonists are usually intelligent teenagers who have to make their way in the adult society they see around them. On the surface, they are simple tales of adventure, achievement, and dealing with stupid teachers and jealous peers. Heinlein was a vocal proponent of the notion that juvenile readers were far more sophisticated and able to handle more complex or difficult themes than most people realized. His juvenile stories often had a maturity to them that made them readable for adults. "Red Planet", for example, portrays some subversive themes, including a revolution in which young students are involved; his editor demanded substantial changes in this book's discussion of topics such as the use of weapons by children and the misidentified sex of the Martian character. Heinlein was always aware of the editorial limitations put in place by the editors of his novels and stories, and while he observed those restrictions on the surface, was often successful in introducing ideas not often seen in other authors' juvenile SF.

In 1957, James Blish wrote that one reason for Heinlein's success "has been the high grade of machinery which goes, today as always, into his story-telling. Heinlein seems to have known from the beginning, as if instinctively, technical lessons about fiction which other writers must learn the hard way (or often enough, never learn). He does not always operate the machinery to the best advantage, but he always seems to be aware of it."

Heinlein decisively ended his juvenile novels with "Starship Troopers" (1959), a controversial work and his personal riposte to leftists calling for President Dwight D. Eisenhower to stop nuclear testing in 1958. "The 'Patrick Henry' ad shocked 'em", he wrote many years later. ""Starship Troopers" outraged 'em." "Starship Troopers" is a coming-of-age story about duty, citizenship, and the role of the military in society. The book portrays a society in which suffrage is earned by demonstrated willingness to place society's interests before one's own, at least for a short time and often under onerous circumstances, in government service; in the case of the protagonist, this was military service.

Later, in "Expanded Universe", Heinlein said that it was his intention in the novel that service could include positions outside strictly military functions such as teachers, police officers, and other government positions. This is presented in the novel as an outgrowth of the failure of unearned suffrage government and as a very successful arrangement. In addition, the franchise was only awarded after leaving the assigned service; thus those serving their terms—in the military, or any other service—were excluded from exercising any franchise. Career military were completely disenfranchised until retirement.

The name "Starship Troopers" was licensed for an unrelated, B movie script called "Bug Hunt at Outpost Nine", which was then retitled to benefit from the book's credibility. The resulting film, entitled "Starship Troopers" (1997), which was written by Ed Neumeier and directed by Paul Verhoeven, had little relationship to the book, beyond the inclusion of character names, the depiction of space marines, and the concept of suffrage earned by military service. Fans of Heinlein were critical of the movie, which they considered a betrayal of Heinlein's philosophy, presenting the society in which the story takes place as fascist.

Likewise, the powered armor technology that is not only central to the book, but became a standard subgenre of science fiction thereafter, is completely absent in the movie, where the characters use World War II-technology weapons and wear light combat gear little more advanced than that. In Verhoeven's movie of the same name, there is no battle armor. Verhoeven commented that he had tried to read the book after he had bought the rights to it, in order to add it to his existing movie. However he read only the first two chapters, finding it too boring to continue. He thought it was a bad book and asked Ed Neumeier to tell him the story because he couldn't read it.

From about 1961 ("Stranger in a Strange Land") to 1973 ("Time Enough for Love"), Heinlein explored some of his most important themes, such as individualism, libertarianism, and free expression of physical and emotional love. Three novels from this period, "Stranger in a Strange Land", "The Moon Is a Harsh Mistress", and "Time Enough for Love", won the Libertarian Futurist Society's Prometheus Hall of Fame Award, designed to honor classic libertarian fiction. Jeff Riggenbach described "The Moon Is a Harsh Mistress" as "unquestionably one of the three or four most influential libertarian novels of the last century".

Heinlein did not publish "Stranger in a Strange Land" until some time after it was written, and the themes of free love and radical individualism are prominently featured in his long-unpublished first novel, "For Us, the Living: A Comedy of Customs".

"The Moon Is a Harsh Mistress" tells of a war of independence waged by the Lunar penal colonies, with significant comments from a major character, Professor La Paz, regarding the threat posed by government to individual freedom.

Although Heinlein had previously written a few short stories in the fantasy genre, during this period he wrote his first fantasy novel, "Glory Road". In "Stranger in a Strange Land" and "I Will Fear No Evil", he began to mix hard science with fantasy, mysticism, and satire of organized religion. Critics William H. Patterson, Jr., and Andrew Thornton believe that this is simply an expression of Heinlein's longstanding philosophical opposition to positivism. Heinlein stated that he was influenced by James Branch Cabell in taking this new literary direction. The penultimate novel of this period, "I Will Fear No Evil", is according to critic James Gifford "almost universally regarded as a literary failure" and he attributes its shortcomings to Heinlein's near-death from peritonitis.

After a seven-year hiatus brought on by poor health, Heinlein produced five new novels in the period from 1980 ("The Number of the Beast") to 1987 ("To Sail Beyond the Sunset"). These books have a thread of common characters and time and place. They most explicitly communicated Heinlein's philosophies and beliefs, and many long, didactic passages of dialog and exposition deal with government, sex, and religion. These novels are controversial among his readers and one critic, David Langford, has written about them very negatively. Heinlein's four Hugo awards were all for books written before this period.

Most of the novels from this period are recognized by critics as forming an offshoot from the Future History series, and referred to by the term World as Myth.

The tendency toward authorial self-reference begun in "Stranger in a Strange Land" and "Time Enough for Love" becomes even more evident in novels such as "The Cat Who Walks Through Walls", whose first-person protagonist is a disabled military veteran who becomes a writer, and finds love with a female character.

The 1982 novel "Friday", a more conventional adventure story (borrowing a character and backstory from the earlier short story "Gulf", also containing suggestions of connection to "The Puppet Masters") continued a Heinlein theme of expecting what he saw as the continued disintegration of Earth's society, to the point where the title character is strongly encouraged to seek a new life off-planet. It concludes with a traditional Heinlein note, as in "The Moon Is a Harsh Mistress" or "Time Enough for Love", that freedom is to be found on the frontiers.

The 1984 novel "" is a sharp satire of organized religion. Heinlein himself was agnostic.

Several Heinlein works have been published since his death, including the aforementioned "" as well as 1989's "Grumbles from the Grave", a collection of letters between Heinlein and his editors and agent; 1992's "Tramp Royale", a travelogue of a southern hemisphere tour the Heinleins took in the 1950s; "Take Back Your Government", a how-to book about participatory democracy written in 1946; and a tribute volume called "Requiem: Collected Works and Tributes to the Grand Master", containing some additional short works previously unpublished in book form. "Off the Main Sequence", published in 2005, includes three short stories never before collected in any Heinlein book (Heinlein called them "stinkeroos").

Spider Robinson, a colleague, friend, and admirer of Heinlein, wrote "Variable Star", based on an outline and notes for a juvenile novel that Heinlein prepared in 1955. The novel was published as a collaboration, with Heinlein's name above Robinson's on the cover, in 2006.

A complete collection of Heinlein's published work has been published by the Heinlein Prize Trust as the "Virginia Edition", after his wife. See the Complete Works section of Robert A. Heinlein bibliography for details.

On February 1, 2019, Phoenix Pick announced that through a collaboration with the Heinlein Prize Trust, a reconstruction of the full text of an unpublished Heinlein novel had been produced. It was published in March, 2020. The reconstructed novel, entitled "The Pursuit of the Pankera: A Parallel Novel about Parallel Universes", is an alternative version of "The Number of the Beast", with the first one-third of "The Pursuit of the Pankera" mostly the same as the first one-third of "The Number of the Beast" but the remainder of "The Pursuit of the Pankera" deviating entirely from "The Number of the Beast", with a completely different story-line. The newly reconstructed novel pays homage to Edgar Rice Burroughs and E. E. “Doc” Smith. It was edited by Patrick Lobrutto. Some reviewers describe the newly-reconstructed novel as more in line with the style of a traditional Heinlein novel than was 'The Number of the Beast.' The Pursuit of the Pankera was considered superior to the original version of The Number of the Beast by some reviewers. Both "The Pursuit of the Pankera" and a new edition of "The Number of the Beast" were published in March, 2020. The new edition of the latter shares the subtitle of "The Pursuit of the Pankera", hence entitled "The Number of the Beast: A Parallel Novel about Parallel Universes"

The primary influence on Heinlein's writing style may have been Rudyard Kipling. Kipling is the first known modern example of "indirect exposition", a writing technique for which Heinlein later became famous. In his famous text on "On the Writing of Speculative Fiction", Heinlein quotes Kipling:

"Stranger in a Strange Land" originated as a modernized version of Kipling's "The Jungle Book", his wife suggesting that the child be raised by Martians instead of wolves. Likewise, "Citizen of the Galaxy" can be seen as a reboot of Kipling's novel "Kim".

The "Starship Troopers" idea of needing to serve in the military in order to vote, can be found in Kipling's "The Army of a Dream":

Poul Anderson once said of Kipling's science fiction story "As Easy as A.B.C.", "a wonderful science fiction yarn, showing the same eye for detail that would later distinguish the work of Robert Heinlein".

Heinlein described himself as also being influenced by George Bernard Shaw, having read most of his plays. Shaw is an example of an earlier author who used the competent man, a favorite Heinlein archetype. He denied, though, any direct influence of "Back to Methuselah" on "Methuselah's Children".

Heinlein's books probe a range of ideas about a range of topics such as sex, race, politics, and the military. Many were seen as radical or as ahead of their time in their social criticism. His books have inspired considerable debate about the specifics, and the evolution, of Heinlein's own opinions, and have earned him both lavish praise and a degree of criticism. He has also been accused of contradicting himself on various philosophical questions.

Brian Doherty cites William Patterson, saying that the best way to gain an understanding of Heinlein is as a "full-service iconoclast, the unique individual who decides that things do not have to be, and won't continue, as they are". He says this vision is "at the heart of Heinlein, science fiction, libertarianism, and America. Heinlein imagined how everything about the human world, from our sexual mores to our religion to our automobiles to our government to our plans for cultural survival, might be flawed, even fatally so."

The critic Elizabeth Anne Hull, for her part, has praised Heinlein for his interest in exploring fundamental life questions, especially questions about "political power—our responsibilities to one another" and about "personal freedom, particularly sexual freedom".

Edward R. Murrow hosted a series on CBS Radio called This I Believe, which solicited an Entry from Heinlein that is probably the most enduring and popular of the title: Our Noble, Essential Decency. In it, Heinlein broke with the normal trends, stating that he believed in his neighbors (some of whom he named and described), community, and towns across America that share the same sense of good will and intentions as his own, going on to apply this same philosophy to the US, and humanity in general.

Heinlein's political positions shifted throughout his life. Heinlein's early political leanings were liberal. In 1934, he worked actively for the Democratic campaign of Upton Sinclair for Governor of California. After Sinclair lost, Heinlein became an anti-Communist Democratic activist. He made an unsuccessful bid for a California State Assembly seat in 1938. Heinlein's first novel, "For Us, the Living" (written 1939), consists largely of speeches advocating the Social Credit system, and the early story "Misfit" (1939) deals with an organization—"The Cosmic Construction Corps"—that seems to be Franklin D. Roosevelt's Civilian Conservation Corps translated into outer space.

Of this time in his life, Heinlein later said:

Heinlein's fiction of the 1940s and 1950s, however, began to espouse conservative views. After 1945, he came to believe that a strong world government was the only way to avoid mutual nuclear annihilation. His 1949 novel "Space Cadet" describes a future scenario where a military-controlled global government enforces world peace. Heinlein ceased considering himself a Democrat in 1954.

The Heinleins formed the Patrick Henry League in 1958, and they worked in the 1964 Barry Goldwater Presidential campaign.

That ad was entitled Who Are the Heirs of Patrick Henry?. It started with the famous Henry quotation: "Is life so dear, or peace so sweet, as to be purchased at the price of chains and slavery? Forbid it, Almighty God! I know not what course others may take, but as for me, give me liberty, or give me death!!". It then went on to admit that there was some risk to nuclear testing (albeit less than the "willfully distorted" claims of the test ban advocates), and risk of nuclear war, but that "The alternative is surrender. We accept the risks." Heinlein was among those who in 1968 signed a pro-Vietnam War ad in "Galaxy Science Fiction". In his essay "Starship Stormtroopers", Michael Moorcock posits that Heinlein was a fascist who fetishized violence and militarism.

Heinlein always considered himself a libertarian; in a letter to Judith Merril in 1967 (never sent) he said, "As for libertarian, I've been one all my life, a radical one. You might use the term 'philosophical anarchist' or 'autarchist' about me, but 'libertarian' is easier to define and fits well enough."

"Stranger in a Strange Land" was embraced by the hippie counterculture, and libertarians have found inspiration in "The Moon Is a Harsh Mistress". Both groups found resonance with his themes of personal freedom in both thought and action.

Heinlein grew up in the era of racial segregation in the United States and wrote some of his most influential fiction at the height of the Civil Rights Movement. He explicitly made the case for using his fiction not only to predict the future but also to educate his readers about the value of racial equality and the importance of racial tolerance. His early novels were very much ahead of their time both in their explicit rejection of racism and in their inclusion of protagonists of color. In the context of science fiction before the 1960s, the mere existence of characters of color was a remarkable novelty, with green occurring more often than brown. For example, his 1948 novel "Space Cadet" explicitly uses aliens as a metaphor for minorities. In his novel "The Star Beast", the "de facto" foreign minister of the Terran government is an undersecretary, a Mr. Kiku, who is from Africa. Heinlein explicitly states his skin is "ebony black" and that Kiku is in an arranged marriage that is happy.

In a number of his stories, Heinlein challenges his readers' possible racial preconceptions by introducing a strong, sympathetic character, only to reveal much later that he or she is of African or other ancestry. In several cases, the covers of the books show characters as being light-skinned when the text states or at least implies that they are dark-skinned or of African ancestry. Heinlein repeatedly denounced racism in his nonfiction works, including numerous examples in "Expanded Universe".

Heinlein reveals in "Starship Troopers" that the novel's protagonist and narrator, Johnny Rico, the formerly disaffected scion of a wealthy family, is Filipino, actually named "Juan Rico" and speaks Tagalog in addition to English.

Race was a central theme in some of Heinlein's fiction. The most prominent and controversial example is "Farnham's Freehold", which casts a white family into a future in which white people are the slaves of cannibalistic black rulers. In the 1941 novel "Sixth Column" (also known as "The Day After Tomorrow"), a white resistance movement in the United States defends itself against an invasion by an Asian fascist state (the "Pan-Asians") using a "super-science" technology that allows ray weapons to be tuned to specific races. The book is sprinkled with racist slurs against Asian people, and black and Hispanic people are not mentioned at all. The idea for the story was pushed on Heinlein by editor John W. Campbell, and Heinlein wrote later that he had "had to re-slant it to remove racist aspects of the original story line" and that he did not "consider it to be an artistic success". However, the novel prompted a heated debate in the scientific community regarding the plausibility of developing ethnic bioweapons.

In keeping with his belief in individualism, his work for adults—and sometimes even his work for juveniles—often portrays both the oppressors and the oppressed with considerable ambiguity. Heinlein believed that individualism was incompatible with ignorance. He believed that an appropriate level of adult competence was achieved through a wide-ranging education, whether this occurred in a classroom or not. In his juvenile novels, more than once a character looks with disdain at a student's choice of classwork, saying, "Why didn't you study something useful?" In "Time Enough for Love", Lazarus Long gives a long list of capabilities that anyone should have, concluding, "Specialization is for insects." The ability of the individual to create himself is explored in stories such as "I Will Fear No Evil", "—All You Zombies—", and "By His Bootstraps".

Heinlein claimed to have written "Starship Troopers" in response to "calls for the unilateral ending of nuclear testing by the United States". Heinlein suggests in the book that the Bugs are a good example of Communism being something that humans cannot successfully adhere to, since humans are strongly defined individuals, whereas the Bugs, being a collective, can all contribute to the whole without consideration of individual desire.

For Heinlein, personal liberation included sexual liberation, and free love was a major subject of his writing starting in 1939, with "For Us, the Living". During his early period, Heinlein's writing for younger readers needed to take account of both editorial perceptions of sexuality in his novels, and potential perceptions among the buying public; as critic William H. Patterson has put it, his dilemma was "to sort out what was really objectionable from what was only excessive over-sensitivity to imaginary librarians".

By his middle period, sexual freedom and the elimination of sexual jealousy became a major theme; for instance, in "Stranger in a Strange Land" (1961), the progressively minded but sexually conservative reporter, Ben Caxton, acts as a dramatic foil for the less parochial characters, Jubal Harshaw and Valentine Michael Smith (Mike). Another of the main characters, Jill, is homophobic, and says that "nine times out of ten, if a girl gets raped it's partly her own fault."

According to Gary Westfahl, 

In books written as early as 1956, Heinlein dealt with incest and the sexual nature of children. Many of his books including "Time for the Stars", "Glory Road", "Time Enough for Love", and "The Number of the Beast" dealt explicitly or implicitly with incest, sexual feelings and relations between adults, children, or both. The treatment of these themes include the romantic relationship and eventual marriage, once the girl becomes an adult via time-travel, of a 30-year-old engineer and an 11-year-old girl in "The Door into Summer" or the more overt intra-familial incest in "To Sail Beyond the Sunset" and "Farnham's Freehold". Heinlein often posed situations where the nominal purpose of sexual taboos was irrelevant to a particular situation, due to future advances in technology. For example, in "Time Enough for Love" Heinlein describes a brother and sister (Joe and Llita) who were mirror twins, being complementary diploids with entirely disjoint genomes, and thus not at increased risk for unfavorable gene duplication due to consanguinity. In this instance, Llita and Joe were props used to explore the concept of incest, where the usual objection to incest—heightened risk of genetic defect in their children—was not a consideration. Peers such as L. Sprague de Camp and Damon Knight have commented critically on Heinlein's portrayal of incest and pedophilia in a lighthearted and even approving manner. However, Heinlein's intent seems more to provoke the reader and to question sexual mores than to promote any particular sexual agenda.

In "To Sail Beyond the Sunset", Heinlein has the main character, Maureen, state that the purpose of metaphysics is to ask questions: "Why are we here?" "Where are we going after we die?" (and so on); and that you are not allowed to answer the questions. "Asking" the questions is the point of metaphysics, but "answering" them is not, because once you answer this kind of question, you cross the line into religion. Maureen does not state a reason for this; she simply remarks that such questions are "beautiful" but lack answers. Maureen's son/lover Lazarus Long makes a related remark in "Time Enough for Love". In order for us to answer the "big questions" about the universe, Lazarus states at one point, it would be necessary to stand "outside" the universe.

During the 1930s and 1940s, Heinlein was deeply interested in Alfred Korzybski's general semantics and attended a number of seminars on the subject. His views on epistemology seem to have flowed from that interest, and his fictional characters continue to express Korzybskian views to the very end of his writing career. Many of his stories, such as "Gulf", "If This Goes On—", and "Stranger in a Strange Land", depend strongly on the premise, related to the well-known Sapir–Whorf hypothesis, that by using a correctly designed language, one can change or improve oneself mentally, or even realize untapped potential (as in the case of Joe in "Gulf" – whose last name may be Greene, Gilead or Briggs).

When Ayn Rand's novel "The Fountainhead" was published, Heinlein was very favorably impressed, as quoted in "Grumbles ..." and mentioned John Galt—the hero in Rand's "Atlas Shrugged"—as a heroic archetype in "The Moon Is a Harsh Mistress". He was also strongly affected by the religious philosopher P. D. Ouspensky. Freudianism and psychoanalysis were at the height of their influence during the peak of Heinlein's career, and stories such as "Time for the Stars" indulged in psychological theorizing.

However, he was skeptical about Freudianism, especially after a struggle with an editor who insisted on reading Freudian sexual symbolism into his juvenile novels. Heinlein was fascinated by the social credit movement in the 1930s. This is shown in "Beyond This Horizon" and in his 1938 novel "", which was finally published in 2003, long after his death.

The phrase "pay it forward", though it was already in occasional use as a quotation, was popularized by Robert A. Heinlein in his book "Between Planets", published in 1951:

He referred to this in a number of other stories, although sometimes just saying to pay a debt back by helping others, as in one of his last works, "Job, a Comedy of Justice".

Heinlein was a mentor to Ray Bradbury, giving him help and quite possibly passing on the concept, made famous by the publication of a letter from him to Heinlein thanking him. In Bradbury's novel "Dandelion Wine", published in 1957, when the main character Douglas Spaulding is reflecting on his life being saved by Mr. Jonas, the Junkman:

Bradbury has also advised that writers he has helped thank him by helping other writers.

Heinlein both preached and practiced this philosophy; now the Heinlein Society, a humanitarian organization founded in his name, does so, attributing the philosophy to its various efforts, including Heinlein for Heroes, the Heinlein Society Scholarship Program, and Heinlein Society blood drives. Author Spider Robinson made repeated reference to the doctrine, attributing it to his spiritual mentor Heinlein.

Heinlein is usually identified, along with Isaac Asimov and Arthur C. Clarke, as one of the three masters of science fiction to arise in the so-called Golden Age of science fiction, associated with John W. Campbell and his magazine "Astounding".
In the 1950s he was a leader in bringing science fiction out of the low-paying and less prestigious "pulp ghetto". Most of his works, including short stories, have been continuously in print in many languages since their initial appearance and are still available as new paperbacks decades after his death.

He was at the top of his form during, and himself helped to initiate, the trend toward social science fiction, which went along with a general maturing of the genre away from space opera to a more literary approach touching on such adult issues as politics and human sexuality. In reaction to this trend, hard science fiction began to be distinguished as a separate subgenre, but paradoxically Heinlein is also considered a seminal figure in hard science fiction, due to his extensive knowledge of engineering and the careful scientific research demonstrated in his stories. Heinlein himself stated—with obvious pride—that in the days before pocket calculators, he and his wife Virginia once worked for several days on a mathematical equation describing an Earth-Mars rocket orbit, which was then subsumed in a single sentence of the novel "Space Cadet".

Heinlein is often credited with bringing serious writing techniques to the genre of science fiction.

For example, when writing about fictional worlds, previous authors were often limited by the reader's existing knowledge of a typical "space opera" setting, leading to a relatively low creativity level: The same starships, death rays, and horrifying rubbery aliens becoming ubiquitous. This was necessary unless the author was willing to go into long expositions about the setting of the story, at a time when the word count was at a premium in SF.

But Heinlein utilized a technique called "indirect exposition", perhaps first introduced by Rudyard Kipling in his own science fiction venture, the Aerial Board of Control stories. Kipling had picked this up during his time in India, using it to avoid bogging down his stories set in India with explanations for his English readers. This technique—mentioning details in a way that lets the reader infer more about the universe than is actually spelled out became a trademark rhetorical technique of both Heinlein and generation of writers influenced by him. Heinlein was significantly influenced by Kipling beyond this, for example quoting him in On the Writing of Speculative Fiction.
Likewise, Heinlein's name is often associated with the competent hero, a character archetype who, though he or she may have flaws and limitations, is a strong, accomplished person able to overcome any soluble problem set in their path. They tend to feel confident overall, have a broad life experience and set of skills, and not give up when the going gets tough. This style influenced not only the writing style of a generation of authors, but even their personal character. Harlan Ellison once said, "Very early in life when I read Robert Heinlein I got the thread that runs through his stories—the notion of the competent man ... I've always held that as my ideal. I've tried to be a very competent man."

When fellow writers, or fans, wrote Heinlein asking for writing advice, he famously gave out his own list of rules for becoming a successful writer:

About which he said:
Heinlein later published an entire article, "On the Writing of Speculative Fiction", which included his rules, and from which the above quote is taken. When he says "anything said above them", he refers to his other guidelines. For example, he describes most stories as fitting into one of a handful of basic categories:


In the article, Heinlein proposes that most stories fit into the either the gadget story or the human interest story, which is itself subdivided into the three latter categories. He also credits L. Ron Hubbard as having identified "The Man-Who-Learned-Better".

Heinlein has had a pervasive influence on other science fiction writers. In a 1953 poll of leading science fiction authors, he was cited more frequently as an influence than any other modern writer. Critic James Gifford writes that 

Heinlein gave Larry Niven and Jerry Pournelle extensive advice on a draft manuscript of "The Mote in God's Eye". He contributed a cover blurb "Possibly the finest science fiction novel I have ever read." Writer David Gerrold, responsible for creating the tribbles in "Star Trek", also credited Heinlein as the inspiration for his "Dingilliad" series of novels. Gregory Benford refers to his novel "Jupiter Project" as a Heinlein tribute. Similarly, Charles Stross says his Hugo Award-nominated novel "Saturn's Children" is "a space opera and late-period Robert A. Heinlein tribute", referring to Heinlein's "Friday". The theme and plot of Kameron Hurley's novel, "The Light Brigade" clearly echo those of Heinlein's "Starship Troopers".

Even outside the science fiction community, several words and phrases coined or adopted by Heinlein have passed into common English usage:

In 1962, Oberon Zell-Ravenheart (then still using his birth name, Tim Zell) founded the Church of All Worlds, a Neopagan religious organization modeled in many ways (including its name) after the treatment of religion in the novel "Stranger in a Strange Land". This spiritual path included several ideas from the book, including non-mainstream family structures, social libertarianism, water-sharing rituals, an acceptance of all religious paths by a single tradition, and the use of several terms such as "grok", "Thou art God", and "Never Thirst". Though Heinlein was neither a member nor a promoter of the Church, there was a frequent exchange of correspondence between Zell and Heinlein, and he was a paid subscriber to their magazine, "Green Egg". This Church still exists as a 501(C)(3) religious organization incorporated in California, with membership worldwide, and it remains an active part of the neopagan community today. Zell-Ravenheart's wife, Morning Glory coined the term polyamory in 1990, another movement that includes Heinlein concepts among its roots.

Heinlein was influential in making space exploration seem to the public more like a practical possibility. His stories in publications such as "The Saturday Evening Post" took a matter-of-fact approach to their outer-space setting, rather than the "gee whiz" tone that had previously been common. The documentary-like film "Destination Moon" advocated a Space Race with an unspecified foreign power almost a decade before such an idea became commonplace, and was promoted by an unprecedented publicity campaign in print publications. Many of the astronauts and others working in the U.S. space program grew up on a diet of the Heinlein juveniles, best evidenced by the naming of a crater on Mars after him, and a tribute interspersed by the Apollo 15 astronauts into their radio conversations while on the moon.

Heinlein was also a guest commentator (along with fellow sci-fi author Arthur C. Clarke) for Walter Cronkite's coverage of the Apollo 11 Moon landing. He remarked to Cronkite during the landing that, "This is the greatest event in human history, up to this time. This is—today is New Year's Day of the Year One." Businessman and entrepreneur Elon Musk says that Heinlein's books have helped inspire his career.

The Heinlein Society was founded by Virginia Heinlein on behalf of her husband, to "pay forward" the legacy of the writer to future generations of "Heinlein's Children". The foundation has programs to:

The Heinlein society also established the Robert A. Heinlein Award in 2003 "for outstanding published works in science fiction and technical writings to inspire the human exploration of space".


In his lifetime, Heinlein received four Hugo Awards, for "Double Star", "Starship Troopers", "Stranger in a Strange Land", and "The Moon Is a Harsh Mistress", and was nominated for four Nebula Awards, for "Stranger in a Strange Land", "Friday", "Time Enough for Love", and "Job: A Comedy of Justice". He was also given seven Retro-Hugos: two for best novel: "Beyond This Horizon" and "Farmer in the Sky"; Three for best novella: :"If This Goes On ...", "Waldo", and "The Man Who Sold the Moon"; one for best novelette: "The Roads Must Roll"; and one for best dramatic presentation: "Destination Moon".
Heinlein was also nominated for six Hugo Awards: "Have Space Suit - Will Travel, Glory Road, Time Enough for Love, Friday, Job: A Comedy of Justice, Grumbles from the Grave"; and six Retro Hugo Awards: "Magic, Inc.", "Requiem", "Coventry", "Blowups Happen", "Goldfish Bowl", "The Unpleasant Profession of Jonathan Hoag".

Heinlein was nominated for four Nebula Awards: "The Moon is a Harsh Mistress, Time Enough for Love, Friday, Job: A Comedy of Justice"

The Science Fiction Writers of America named Heinlein its first Grand Master in 1974, presented 1975. Officers and past presidents of the Association select a living writer for lifetime achievement (now annually and including fantasy literature).

Main-belt asteroid 6312 Robheinlein (1990 RH4), discovered on September 14, 1990 by H. E. Holt, at Palomar was named after him.

There is no lunar feature named explicitly for Heinlein, but in 1994 the International Astronomical Union named Heinlein crater on Mars in his honor.

The Science Fiction and Fantasy Hall of Fame inducted Heinlein in 1998, its third class of two deceased and two living writers and editors.

In 2001 the United States Naval Academy created the Robert A. Heinlein Chair In Aerospace Engineering.

In 2016, after an intensive online campaign to win a vote for the opening, Heinlein was inducted into the Hall of Famous Missourians. His bronze bust, created by Kansas City sculptor E. Spencer Schubert, is on permanent display in the Missouri State Capitol in Jefferson City.

The Libertarian Futurist Society has honored five of Heinlein's novels and two short stories with their Hall of Fame award. The first two were given during his lifetime for "The Moon Is a Harsh Mistress" and "Stranger in a Strange Land". Five more were awarded posthumously for "Red Planet", "Methuselah's Children", "Time Enough for Love", and the short stories "Requiem" and "Coventry".







</doc>
<doc id="25391" url="https://en.wikipedia.org/wiki?curid=25391" title="Russia">
Russia

Russia (), or the Russian Federation, is a transcontinental country located in Eastern Europe and Northern Asia. Covering an area of , it is the largest country in the world by area, spanning more than one-eighth of the Earth's inhabited land area, stretching eleven time zones, and bordering 16 sovereign nations. The territory of Russia extends from the Baltic Sea in the west to the Pacific Ocean in the east, and from the Arctic Ocean in the north to the Black Sea and the Caucasus in the south. With 146.7 million inhabitants living in the country's 85 federal subjects, Russia is the most populous nation in Europe and the ninth-most populous nation in the world. Russia's capital and largest city is Moscow; other major urban areas include Saint Petersburg, Novosibirsk, Yekaterinburg, Nizhny Novgorod, Kazan, Chelyabinsk and Samara.

The East Slavs emerged as a recognisable group in Europe between the 3rd and 8th centuries AD. The medieval state of Rus' arose in the 9th century. In 988 it adopted Orthodox Christianity from the Byzantine Empire, beginning the synthesis of Byzantine and Slavic cultures that defined Russian culture for the next millennium. Rus' ultimately disintegrated into a number of smaller states, until it was finally reunified by the Grand Duchy of Moscow in the 15th century. By the 18th century, the nation had greatly expanded through conquest, annexation, and exploration to become the Russian Empire, which became a major European power, and the third-largest empire in history, stretching from Norway on the west to Canada on the east. Following the Russian Revolution, the Russian SFSR became the largest and leading constituent of the Soviet Union, the world's first constitutionally socialist state. The Soviet Union played a decisive role in the Allied victory in World War II, and emerged as a recognised superpower and rival to the United States during the Cold War. The Soviet era saw some of the most significant technological achievements of the 20th century, including the world's first human-made satellite and the launching of the first humans in space. Following the dissolution of the Soviet Union in 1991, the Russian SFSR reconstituted itself as the Russian Federation and is recognised as the continuing legal personality and a successor of the USSR.

Following the constitutional crisis of 1993, a new constitution was adopted and Russia has been governed as a federal semi-presidential republic. Vladimir Putin became acting president on 31 December 1999 after Boris Yeltsin resigned and was elected president in March 2000. Since then, he has dominated Russia's political system as either president or prime minister. His government has been accused by non-governmental organisations of numerous human rights abuses, authoritarianism and corruption. In response, Putin has argued that Western-style liberalism is obsolete in Russia, while maintaining that the country is still a democratic nation.

The Russian economy ranks as the fifth-largest in Europe, the eleventh-largest in the world by nominal GDP and the fifth-largest by PPP. Russia's extensive mineral and energy resources are the largest such reserves in the world, making it one of the leading producers of oil and natural gas globally. The country is one of the five recognised nuclear weapons states and possesses the largest stockpile of nuclear warheads. Russia is a major great power, as well as a regional power, and has been characterised as a potential superpower. The Russian Armed Forces have been ranked as the world's second most powerful, and the most powerful in Europe. Russia hosts the world's ninth-greatest number of UNESCO World Heritage Sites, at 29, and is among the world's most popular tourist destinations. It is a permanent member of the United Nations Security Council and an active global partner of ASEAN, as well as a member of the Shanghai Cooperation Organisation (SCO), the G20, the Council of Europe, the Asia-Pacific Economic Cooperation (APEC), the Organization for Security and Co-operation in Europe (OSCE), the International Investment Bank (IIB) and the World Trade Organization (WTO), as well as being the leading member of the Commonwealth of Independent States (CIS), the Collective Security Treaty Organization (CSTO) and a member of the Eurasian Economic Union (EAEU).

The name "Russia" is derived from Rus', a medieval state populated mostly by the East Slavs. However, this proper name became more prominent in the later history, and the country typically was called by its inhabitants "Русская Земля" ("russkaja zemlja"), which can be translated as "Russian Land" or "Land of Rus'". In order to distinguish this state from other states derived from it, it is denoted as "Kievan Rus'" by modern historiography. The name "Rus" itself comes from the early medieval Rus' people, Swedish merchants and warriors who relocated from across the Baltic Sea and founded a state centered on Novgorod that later became Kievan Rus.

An old Latin version of the name Rus' was Ruthenia, mostly applied to the western and southern regions of Rus' that were adjacent to Catholic Europe. The current name of the country, Россия ("Rossija"), comes from the Byzantine Greek designation of the Rus', Ρωσσία "Rossía"—spelled Ρωσία ("Rosía" ) in Modern Greek.

The standard way to refer to citizens of Russia is "Russians" in English and "rossiyane" () in Russian. There are two Russian words which are commonly translated into English as "Russians". One is "русские" ("russkiye"), which most often means "ethnic Russians". Another is "россияне" ("rossiyane"), which means "citizens of Russia, regardless of ethnicity". Translations into other languages often do not distinguish these two groups.

Nomadic pastoralism developed in the Pontic-Caspian steppe beginning in the Chalcolithic.

In classical antiquity, the Pontic Steppe was known as Scythia. Beginning in the 8th century BC, Ancient Greek traders brought their civilization to the trade emporiums in Tanais and Phanagoria. Ancient Greek explorers, most notably Pytheas, even went as far as modern day Kaliningrad, on the Baltic Sea. Romans settled on the western part of the Caspian Sea, where their empire stretched towards the east. In the 3rd to 4th centuries AD a semi-legendary Gothic kingdom of Oium existed in Southern Russia until it was overrun by Huns. Between the 3rd and 6th centuries AD, the Bosporan Kingdom, a Hellenistic polity which succeeded the Greek colonies, was also overwhelmed by nomadic invasions led by warlike tribes, such as the Huns and Eurasian Avars. A Turkic people, the Khazars, ruled the lower Volga basin steppes between the Caspian and Black Seas until the 10th century.

The ancestors of modern Russians are the Slavic tribes, whose original home is thought by some scholars to have been the wooded areas of the Pinsk Marshes. The East Slavs gradually settled Western Russia in two waves: one moving from Kiev toward present-day Suzdal and Murom and another from Polotsk toward Novgorod and Rostov. From the 7th century onwards, the East Slavs constituted the bulk of the population in Western Russia and assimilated the native Finno-Ugric peoples, including the Merya, the Muromians, and the Meshchera.

The establishment of the first East Slavic states in the 9th century coincided with the arrival of Varangians, the traders, warriors and settlers from the Baltic Sea region. Primarily they were Vikings of Scandinavian origin, who ventured along the waterways extending from the eastern Baltic to the Black and Caspian Seas. According to the "Primary Chronicle", a Varangian from Rus' people, named Rurik, was elected ruler of Novgorod in 862. In 882, his successor Oleg ventured south and conquered Kiev, which had been previously paying tribute to the Khazars. Oleg, Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar khaganate and launched several military expeditions to Byzantium and Persia.

In the 10th to 11th centuries Kievan Rus' became one of the largest and most prosperous states in Europe. The reigns of Vladimir the Great (980–1015) and his son Yaroslav the Wise (1019–1054) constitute the Golden Age of Kiev, which saw the acceptance of Orthodox Christianity from Byzantium and the creation of the first East Slavic written legal code, the "Russkaya Pravda".

In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchaks and the Pechenegs, caused a massive migration of Slavic populations to the safer, heavily forested regions of the north, particularly to the area known as Zalesye.
The age of feudalism and decentralization was marked by constant in-fighting between members of the Rurik Dynasty that ruled Kievan Rus' collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, Novgorod Republic in the north-west and Galicia-Volhynia in the south-west.

Ultimately Kievan Rus' disintegrated, with the final blow being the Mongol invasion of 1237–40 that resulted in the destruction of Kiev and the death of about half the population of Rus'. The invading Mongol elite, together with their conquered Turkic subjects (Cumans, Kipchaks, Bulgars), became known as Tatars, forming the state of the Golden Horde, which pillaged the Russian principalities; the Mongols ruled the Cuman-Kipchak confederation and Volga Bulgaria (modern-day southern and central expanses of Russia) for over two centuries.

Galicia-Volhynia was eventually assimilated by the Kingdom of Poland, while the Mongol-dominated Vladimir-Suzdal and Novgorod Republic, two regions on the periphery of Kiev, established the basis for the modern Russian nation. The Novgorod together with Pskov retained some degree of autonomy during the time of the Mongol yoke and were largely spared the atrocities that affected the rest of the country. Led by Prince Alexander Nevsky, Novgorodians repelled the invading Swedes in the Battle of the Neva in 1240, as well as the Germanic crusaders in the Battle of the Ice in 1242, breaking their attempts to colonise the Northern Rus'.

The most powerful state to eventually arise after the destruction of Kievan Rus' was the Grand Duchy of Moscow ("Muscovy" in the Western chronicles), initially a part of Vladimir-Suzdal. While still under the domain of the Mongol-Tatars and with their connivance, Moscow began to assert its influence in the Central Rus' in the early 14th century, gradually becoming the leading force in the process of the Rus' lands' reunification and expansion of Russia. Moscow's last rival, the Novgorod Republic, prospered as the chief fur trade center and the easternmost port of the Hanseatic League.

Times remained difficult, with frequent Mongol-Tatar raids. Agriculture suffered from the beginning of the Little Ice Age. As in the rest of Europe, plague was a frequent occurrence between 1350 and 1490. However, because of the lower population density and better hygiene—widespread practicing of banya, a wet steam bath—the death rate from plague was not as severe as in Western Europe, and population numbers recovered by 1500.

Led by Prince Dmitry Donskoy of Moscow and helped by the Russian Orthodox Church, the united army of Russian principalities inflicted a milestone defeat on the Mongol-Tatars in the Battle of Kulikovo in 1380. Moscow gradually absorbed the surrounding principalities, including formerly strong rivals such as Tver and Novgorod.

Ivan III ("the Great") finally threw off the control of the Golden Horde and consolidated the whole of Central and Northern Rus' under Moscow's dominion. He was also the first to take the title "Grand Duke of all the Russias". After the fall of Constantinople in 1453, Moscow claimed succession to the legacy of the Eastern Roman Empire. Ivan III married Sophia Palaiologina, the niece of the last Byzantine emperor Constantine XI, and made the Byzantine double-headed eagle his own, and eventually Russia's, coat-of-arms.

In development of the Third Rome ideas, the Grand Duke Ivan IV (the "Terrible") was officially crowned first "Tsar" ("Caesar") of Russia in 1547. The "Tsar" promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (Zemsky Sobor) and introduced local self-management into the rural regions.

During his long reign, Ivan the Terrible nearly doubled the already large Russian territory by annexing the three Tatar khanates (parts of the disintegrated Golden Horde): Kazan and Astrakhan along the Volga River, and the Siberian Khanate in southwestern Siberia. Thus, by the end of the 16th century Russia was transformed into a multiethnic, multidenominational and transcontinental state.

However, the Tsardom was weakened by the long and unsuccessful Livonian War against the coalition of Poland, Lithuania, and Sweden for access to the Baltic coast and sea trade. At the same time, the Tatars of the Crimean Khanate, the only remaining successor to the Golden Horde, continued to raid Southern Russia. In an effort to restore the Volga khanates, Crimeans and their Ottoman allies invaded central Russia and were even able to burn down parts of Moscow in 1571. But in the next year the large invading army was thoroughly defeated by Russians in the Battle of Molodi, forever eliminating the threat of an Ottoman–Crimean expansion into Russia. The slave raids of Crimeans, however, did not cease until the late 17th century though the construction of new fortification lines across Southern Russia, such as the Great Abatis Line, constantly narrowed the area accessible to incursions.

The death of Ivan's sons marked the end of the ancient Rurik Dynasty in 1598, and in combination with the famine of 1601–03 led to civil war, the rule of pretenders, and foreign intervention during the Time of Troubles in the early 17th century. The Polish–Lithuanian Commonwealth occupied parts of Russia, including Moscow. In 1612, the Poles were forced to retreat by the Russian volunteer corps, led by two national heroes, merchant Kuzma Minin and Prince Dmitry Pozharsky. The Romanov Dynasty acceded to the throne in 1613 by the decision of Zemsky Sobor, and the country started its gradual recovery from the crisis.

Russia continued its territorial growth through the 17th century, which was the age of Cossacks. Cossacks were warriors organised into military communities, resembling pirates and pioneers of the New World. In 1648, the peasants of Ukraine joined the Zaporozhian Cossacks in rebellion against Poland-Lithuania during the Khmelnytsky Uprising in reaction to the social and religious oppression they had been suffering under Polish rule. In 1654, the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian Tsar, Aleksey I. Aleksey's acceptance of this offer led to another Russo-Polish War. Finally, Ukraine was split along the Dnieper River, leaving the western part, right-bank Ukraine, under Polish rule and the eastern part (Left-bank Ukraine and Kiev) under Russian rule. Later, in 1670–71, the Don Cossacks led by Stenka Razin initiated a major uprising in the Volga Region, but the Tsar's troops were successful in defeating the rebels.

In the east, the rapid Russian exploration and colonisation of the huge territories of Siberia was led mostly by Cossacks hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian River Routes, and by the mid-17th century there were Russian settlements in Eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the Pacific coast. In 1648, the Bering Strait between Asia and North America was passed for the first time by Fedot Popov and Semyon Dezhnyov.

Under Peter the Great, Russia was proclaimed an Empire in 1721 and became recognised as a world power. Ruling from 1682 to 1725, Peter defeated Sweden in the Great Northern War, forcing it to cede West Karelia and Ingria (two regions lost by Russia in the Time of Troubles), as well as Estland and Livland, securing Russia's access to the sea and sea trade. On the Baltic Sea, Peter founded a new capital called Saint Petersburg, later known as Russia's "window to Europe". Peter the Great's reforms brought considerable Western European cultural influences to Russia.

The reign of Peter I's daughter Elizabeth in 1741–62 saw Russia's participation in the Seven Years' War (1756–63). During this conflict Russia annexed East Prussia for a while and even took Berlin. However, upon Elizabeth's death, all these conquests were returned to the Kingdom of Prussia by pro-Prussian Peter III of Russia.

Catherine II ("the Great"), who ruled in 1762–96, presided over the Age of Russian Enlightenment. She extended Russian political control over the Polish-Lithuanian Commonwealth and incorporated most of its territories into Russia during the Partitions of Poland, pushing the Russian frontier westward into Central Europe. In the south, after successful Russo-Turkish Wars against Ottoman Turkey, Catherine advanced Russia's boundary to the Black Sea, defeating the Crimean Khanate. As a result of victories over Qajar Iran through the Russo-Persian Wars, by the first half of the 19th century Russia also made significant territorial gains in Transcaucasia and the North Caucasus, forcing the former to irrevocably cede what is nowadays Georgia, Dagestan, Azerbaijan and Armenia to Russia. Catherine's successor, her son Paul, was unstable and focused predominantly on domestic issues. Following his short reign, Catherine's strategy was continued with Alexander I's (1801–25) wresting of Finland from the weakened kingdom of Sweden in 1809 and of Bessarabia from the Ottomans in 1812. At the same time, Russians colonised Alaska and even founded settlements in California, such as Fort Ross.
In 1803–1806, the first Russian circumnavigation was made, later followed by other notable Russian sea exploration voyages. In 1820, a Russian expedition discovered the continent of Antarctica.

In alliances with various European countries, Russia fought against Napoleon's France. The French invasion of Russia at the height of Napoleon's power in 1812 reached Moscow, but eventually failed miserably as the obstinate resistance in combination with the bitterly cold Russian winter led to a disastrous defeat of invaders, in which more than 95% of the pan-European Grande Armée perished. Led by Mikhail Kutuzov and Barclay de Tolly, the Russian army ousted Napoleon from the country and drove through Europe in the war of the Sixth Coalition, finally entering Paris. Alexander I headed Russia's delegation at the Congress of Vienna that defined the map of post-Napoleonic Europe.

The officers of the Napoleonic Wars brought ideas of liberalism back to Russia with them and attempted to curtail the tsar's powers during the abortive Decembrist revolt of 1825. At the end of the conservative reign of Nicolas I (1825–55), a zenith period of Russia's power and influence in Europe was disrupted by defeat in the Crimean War. Between 1847 and 1851, about one million people died of Asiatic cholera.

Nicholas's successor Alexander II (1855–81) enacted significant changes in the country, including the emancipation reform of 1861. These "Great Reforms" spurred industrialization and modernised the Russian army, which had successfully liberated Bulgaria from Ottoman rule in the 1877–78 Russo-Turkish War.

The late 19th century saw the rise of various socialist movements in Russia. Alexander II was killed in 1881 by revolutionary terrorists, and the reign of his son
Alexander III (1881–94) was less liberal but more peaceful. The last Russian Emperor, Nicholas II (1894–1917), was unable to prevent the events of the Russian Revolution of 1905, triggered by the unsuccessful Russo-Japanese War and the demonstration incident known as Bloody Sunday. The uprising was put down, but the government was forced to concede major reforms (Russian Constitution of 1906), including granting the freedoms of speech and assembly, the legalization of political parties, and the creation of an elected legislative body, the State Duma of the Russian Empire. The Stolypin agrarian reform led to a massive peasant migration and settlement into Siberia. More than four million settlers arrived in that region between 1906 and 1914.

In 1914, Russia entered World War I in response to Austria-Hungary's declaration of war on Russia's ally Serbia, and fought across multiple fronts while isolated from its Triple Entente allies. In 1916, the Brusilov Offensive of the Russian Army almost completely destroyed the military of Austria-Hungary. However, the already-existing public distrust of the regime was deepened by the rising costs of war, high casualties, and rumors of corruption and treason. All this formed the climate for the Russian Revolution of 1917, carried out in two major acts.

The February Revolution forced Nicholas II to abdicate; he and his family were imprisoned and later executed in Yekaterinburg during the Russian Civil War. The monarchy was replaced by a shaky coalition of political parties that declared itself the Provisional Government. On 1 September (14), 1917, upon a decree of the Provisional Government, the Russian Republic was proclaimed. On 6 January (19), 1918, the Russian Constituent Assembly declared Russia a democratic federal republic (thus ratifying the Provisional Government's decision). The next day the Constituent Assembly was dissolved by the All-Russian Central Executive Committee.
An alternative socialist establishment co-existed, the Petrograd Soviet, wielding power through the democratically elected councils of workers and peasants, called "Soviets". The rule of the new authorities only aggravated the crisis in the country, instead of resolving it. Eventually, the October Revolution, led by Bolshevik leader Vladimir Lenin, overthrew the Provisional Government and gave full governing power to the Soviets, leading to the creation of the world's first socialist state.

Following the October Revolution, a civil war broke out between the anti-Communist White movement and the new Soviet regime with its Red Army. Bolshevist Russia lost its Ukrainian, Polish, Baltic, and Finnish territories by signing the Treaty of Brest-Litovsk that concluded hostilities with the Central Powers of World War I. The Allied powers launched an unsuccessful military intervention in support of anti-Communist forces. In the meantime both the Bolsheviks and White movement carried out campaigns of deportations and executions against each other, known respectively as the Red Terror and White Terror. By the end of the civil war, Russia's economy and infrastructure were heavily damaged. There were an estimated 7–12 million casualties during the war, mostly civilians. Millions became White émigrés, and the Russian famine of 1921–22 claimed up to five million victims.

The Russian Soviet Federative Socialist Republic (called "Russian Socialist Federative Soviet Republic" at the time), together with the Ukrainian, Byelorussian, and Transcaucasian Soviet Socialist Republics, formed the Union of Soviet Socialist Republics (USSR), or Soviet Union, on 30 December 1922. Out of the 15 republics that would make up the USSR, the largest in size and over half of the total USSR population was the Russian SFSR, which came to dominate the union for its entire 69-year history.

Following Lenin's death in 1924, a troika was designated to govern the Soviet Union. However, Joseph Stalin, an elected General Secretary of the Communist Party, managed to suppress all opposition groups within the party and consolidate power in his hands. Leon Trotsky, the main proponent of world revolution, was exiled from the Soviet Union in 1929, and Stalin's idea of Socialism in One Country became the primary line. The continued internal struggle in the Bolshevik party culminated in the Great Purge, a period of mass repressions in 1937–38, during which hundreds of thousands of people were executed, including original party members and military leaders accused of coup d'état plots.

Under Stalin's leadership, the government launched a command economy, industrialization of the largely rural country, and collectivization of its agriculture. During this period of rapid economic and social change, millions of people were sent to penal labor camps, including many political convicts for their opposition to Stalin's rule; millions were deported and exiled to remote areas of the Soviet Union. The transitional disorganisation of the country's agriculture, combined with the harsh state policies and a drought, led to the Soviet famine of 1932–1933, which killed between 2 and 3 million people in the Russian SFSR. The Soviet Union made the costly transformation from a largely agrarian economy to a major industrial powerhouse in a short span of time.

Under the doctrine of state atheism in the Soviet Union, there was a "government-sponsored program of forced conversion to atheism" conducted by Communists. The communist regime targeted religions based on State interests, and while most organised religions were never outlawed, religious property was confiscated, believers were harassed, and religion was ridiculed while atheism was propagated in schools. In 1925 the government founded the League of Militant Atheists to intensify the persecution. Accordingly, although personal expressions of religious faith were not explicitly banned, a strong sense of social stigma was imposed on them by the official structures and mass media and it was generally considered unacceptable for members of certain professions (teachers, state bureaucrats, soldiers) to be openly religious. As for the Russian Orthodox Church, Soviet authorities sought to control it and, in times of national crisis, to exploit it for the regime's own purposes; but their ultimate goal was to eliminate it. During the first five years of Soviet power, the Bolsheviks executed 28 Russian Orthodox bishops and over 1,200 Russian Orthodox priests. Many others were imprisoned or exiled. Believers were harassed and persecuted. Most seminaries were closed, and the publication of most religious material was prohibited. By 1941 only 500 churches remained open out of about 54,000 in existence prior to World War I.

The Appeasement policy of Great Britain and France towards Adolf Hitler's annexation of Austria and Czechoslovakia did not stem an increase in the power of Nazi Germany. Around the same time, the Third Reich allied with the Empire of Japan, a rival of the USSR in the Far East and an open enemy of the USSR in the Soviet–Japanese Border Wars in 1938–39.

In August 1939, the Soviet government decided to improve relations with Germany by concluding the Molotov–Ribbentrop Pact, pledging non-aggression between the two countries and dividing Eastern Europe into their respective spheres of influence. When Germany launched the Invasion of Poland, the Soviets followed weeks later with their own invasion of the country, claiming the eastern half of Poland while avoiding war with the Allied Powers. The Soviet government engaged in significant cooperation with Nazi Germany between 1939 and 1941, through extensive trade agreements which supplied Germany with vital raw materials for her war effort against Britain and France. As the other European powers were busy fighting in World War II, the USSR expanded her own military, and occupied the Hertza region as a result of the Winter War, annexed the Baltic states and annexed Bessarabia and Northern Bukovina from Romania.

On 22 June 1941, Nazi Germany broke their non-aggression treaty with their erstwhile partner and invaded the Soviet Union with the largest and most powerful invasion force in human history, opening the largest theater of World War II. The Nazi Hunger Plan foresaw the "extinction of industry as well as a great part of the population". Nearly 3 million Soviet POWs in German captivity were murdered in just eight months of 1941–42. Although the German army had considerable early success, their attack was halted in the Battle of Moscow. Subsequently, the Germans were dealt major defeats first at the Battle of Stalingrad in the winter of 1942–43, and then in the Battle of Kursk in the summer of 1943. Another German failure was the Siege of Leningrad, in which the city was fully blockaded on land between 1941 and 1944 by German and Finnish forces, and suffered starvation and more than a million deaths, but never surrendered. Under Stalin's administration and the leadership of such commanders as Georgy Zhukov and Konstantin Rokossovsky, Soviet forces took Eastern Europe in 1944–45 and captured Berlin in May 1945. In August 1945 the Soviet Army ousted the Japanese from China's Manchukuo and North Korea, contributing to the allied victory over Japan.

The 1941–45 period of World War II is known in Russia as the "Great Patriotic War". The Soviet Union together with the United States, the United Kingdom and China were considered as the Big Four of Allied powers in World War II and later became the Four Policemen which was the foundation of the United Nations Security Council. During this war, which included many of the most lethal battle operations in human history, Soviet civilian and military death were about 27 million, accounting for about a third of all World War II casualties. The full demographic loss to the Soviet peoples was even greater. The Soviet economy and infrastructure suffered massive devastation which caused the Soviet famine of 1946–47, but the Soviet Union emerged as an acknowledged military superpower on the continent.

The Soviet rear was also badly damaged by the German invasion. Luftwaffe bombed the cities of the Soviet Union from the air. Gorky suffered the most from the bombing. This city was the main industrial center of the USSR and was located near the Moscow Defence Zone. The bombing of the Volga capital destroyed the largest automobile plant GAZ. This plant supplied tanks for the front. Whole residential areas and other large factories of the city were destroyed. From 1941 to 1943, German pilots bombed different areas of the city. This bombardment is comparable to the London Blitz. Some damage remains until this time.

After the war, Eastern and Central Europe including East Germany and part of Austria was occupied by Red Army according to the Potsdam Conference. Dependent socialist governments were installed in the Eastern Bloc satellite states. Becoming the world's second nuclear weapons power, the USSR established the Warsaw Pact alliance and entered into a struggle for global dominance, known as the Cold War, with the United States and NATO. The Soviet Union supported revolutionary movements across the world, including the newly formed People's Republic of China, the Democratic People's Republic of Korea and, later on, the Republic of Cuba. Significant amounts of Soviet resources were allocated in aid to the other socialist states.

After Stalin's death and a short period of collective rule, the new leader Nikita Khrushchev denounced the cult of personality of Stalin and launched the policy of de-Stalinization. The penal labor system was reformed and many prisoners were released and rehabilitated (many of them posthumously). The general easement of repressive policies became known later as the Khrushchev Thaw. At the same time, tensions with the United States heightened when the two rivals clashed over the deployment of the United States Jupiter missiles in Turkey and Soviet missiles in Cuba.

In 1957, the Soviet Union launched the world's first artificial satellite, "Sputnik 1", thus starting the Space Age. Russia's cosmonaut Yuri Gagarin became the first human to orbit the Earth, aboard the "Vostok 1" manned spacecraft on 12 April 1961.

Following the ousting of Khrushchev in 1964, another period of collective rule ensued, until Leonid Brezhnev became the leader. The era of the 1970s and the early 1980s was later designated as the Era of Stagnation, a period when economic growth slowed and social policies became static. The 1965 Kosygin reform aimed for partial decentralization of the Soviet economy and shifted the emphasis from heavy industry and weapons to light industry and consumer goods but was stifled by the conservative Communist leadership.

In 1979, after a Communist-led revolution in Afghanistan, Soviet forces entered that country. The occupation drained economic resources and dragged on without achieving meaningful political results. Ultimately, the Soviet Army was withdrawn from Afghanistan in 1989 due to international opposition, persistent anti-Soviet guerrilla warfare, and a lack of support by Soviet citizens.
From 1985 onwards, the last Soviet leader Mikhail Gorbachev, who sought to enact liberal reforms in the Soviet system, introduced the policies of "glasnost" (openness) and "perestroika" (restructuring) in an attempt to end the period of economic stagnation and to democratise the government. This, however, led to the rise of strong nationalist and separatist movements. Prior to 1991, the Soviet economy was the second largest in the world, but during its last years it was afflicted by shortages of goods in grocery stores, huge budget deficits, and explosive growth in the money supply leading to inflation.

By 1991, economic and political turmoil began to boil over, as the Baltic states chose to secede from the Soviet Union. On 17 March, a referendum was held, in which the vast majority of participating citizens voted in favour of changing the Soviet Union into a renewed federation. In August 1991, a coup d'état attempt by members of Gorbachev's government, directed against Gorbachev and aimed at preserving the Soviet Union, instead led to the end of the Communist Party of the Soviet Union. On 25 December 1991, the USSR was dissolved into 15 post-Soviet states.

In June 1991, Boris Yeltsin became the first directly elected president in Russian history when he was elected President of the Russian Soviet Federative Socialist Republic, which became the independent Russian Federation in December of that year. The economic and political collapse of USSR led to a deep and prolonged depression, characterised by a 50% decline in both GDP and industrial output between 1990 and 1995, although some of the recorded declines may have been a result of an upward bias in Soviet-era economic data. During and after the disintegration of the Soviet Union, wide-ranging reforms including privatization and market and trade liberalization were undertaken, including radical changes along the lines of "shock therapy" as recommended by the United States and the International Monetary Fund.

The privatization largely shifted control of enterprises from state agencies to individuals with inside connections in the government. Many of the newly rich moved billions in cash and assets outside of the country in an enormous capital flight. The depression of the economy led to the collapse of social services; the birth rate plummeted while the death rate skyrocketed. Millions plunged into poverty, from a level of 1.5% in the late Soviet era to 39–49% by mid-1993. The 1990s saw extreme corruption and lawlessness, the rise of criminal gangs and violent crime.

The 1990s were plagued by armed conflicts in the North Caucasus, both local ethnic skirmishes and separatist Islamist insurrections. From the time Chechen separatists declared independence in the early 1990s, an intermittent guerrilla war has been fought between the rebel groups and the Russian military. Terrorist attacks against civilians carried out by separatists, most notably the Moscow theater hostage crisis and Beslan school siege, caused hundreds of deaths and drew worldwide attention.
Russia took up the responsibility for settling the USSR's external debts, even though its population made up just half of the population of the USSR at the time of its dissolution. In 1992, most consumer price controls were eliminated, causing extreme inflation and significantly devaluing the Ruble. With a devalued Ruble, the Russian government struggled to pay back its debts to internal debtors, as well as international institutions like the International Monetary Fund. Despite significant attempts at economic restructuring, Russia's debt outpaced GDP growth. High budget deficits coupled with increasing capital flight and inability to pay back debts caused the 1998 Russian financial crisis and resulted in a further GDP decline.

On 31 December 1999, President Yeltsin unexpectedly resigned, handing the post to the recently appointed Prime Minister, Vladimir Putin, who then won the 2000 presidential election. Putin suppressed the Chechen insurgency although sporadic violence still occurs throughout the Northern Caucasus. High oil prices and the initially weak currency followed by increasing domestic demand, consumption, and investments helped the economy grow at an average of 7% per year from 1998 to 2008, improving the standard of living and increasing Russia's influence on the world stage. Following the world economic crisis of 2008 and a subsequent drop in oil prices, Russia's economy stagnated and poverty again started to rise until 2017 when, after the prolonged recession, Russia's economy began to grow again, supported by stronger global growth, higher oil prices, and solid macro fundamentals. While many reforms made during the Putin presidency have been generally criticised by Western nations as undemocratic, Putin's leadership over the return of order, stability, and progress has won him widespread admiration in Russia.

On 2 March 2008, Dmitry Medvedev was elected President of Russia while Putin became Prime Minister. Putin returned to the presidency following the 2012 presidential elections, and Medvedev was appointed Prime Minister. This quick succession in leadership change was coined "tandemocracy" by outside media. Some critics claimed that the leadership change was superficial, and that Putin remained as the decision making force in the Russian government. Within the context of the ongoing Russia–Ukraine gas dispute in early January 2009, Nikolai Petrov, an analyst with the Carnegie Moscow Center said: "What we see right now is the dominant role of Putin. We see him as a real head of state. ... This is not surprising. We are still living in Putin's Russia." Some Russian political analysts and commentators viewed the political power as truly tandem between Medvedev and Putin. Prior to the 2008 election, political scientists Gleb Pavlovsky and Stanislav Belkovsky discussed the future configuration of power. According to Mr. Pavlovsky, people would be very suited with the option of the union of Putin and Medvedev "similar to the two Consuls of Rome". Belkovsky called Medvedev "President of a dream", referring to the early 1990s when people ostensibly dreamed of the time they "would live without the stranglehold of ubiquitous ideology, and a common person would become the head of the state".

In 2014, after President Viktor Yanukovych of Ukraine fled as a result of a revolution, Putin requested and received authorization from the Russian Parliament to deploy Russian troops to Ukraine, leading to the takeover of Crimea. Following a Crimean referendum in which separation was favored by a large majority of voters, the Russian leadership announced the accession of Crimea into the Russian Federation, though this and the referendum that preceded it were not accepted internationally. On 27 March the United Nations General Assembly voted in favor of a non-binding resolution opposing the Russian annexation of Crimea by a vote of 100 member states in favor, 11 against and 58 abstentions. The annexation of Crimea lead to sanctions by Western countries, in which the Russian government responded with its own against a number of countries.

In September 2015, Russia started military intervention in the Syrian Civil War, consisting of air strikes against militant groups of the Islamic State, al-Nusra Front (al-Qaeda in the Levant), and the Army of Conquest.

According to the Constitution of Russia, the country is an asymmetric federation and semi-presidential republic, wherein the President is the head of state and the Prime Minister is the head of government. The Russian Federation is fundamentally structured as a multi-party representative democracy, with the federal government composed of three branches:

The president is elected by popular vote for a six-year term (eligible for a second term, but not for a third consecutive term). Ministries of the government are composed of the Premier and his deputies, ministers, and selected other individuals; all are appointed by the President on the recommendation of the Prime Minister (whereas the appointment of the latter requires the consent of the State Duma). Leading political parties in Russia include United Russia, the Communist Party, the Liberal Democratic Party, and A Just Russia. In 2019, Russia was ranked as 134th of 167 countries in the Democracy Index, compiled by The Economist Intelligence Unit, while the World Justice Project, , ranked Russia 80th of 99 countries surveyed in terms of rule of law.

The Russian Federation is recognised in international law as a successor state of the former Soviet Union. Russia continues to implement the international commitments of the USSR, and has assumed the USSR's permanent seat in the UN Security Council, membership in other international organisations, the rights and obligations under international treaties, and property and debts. Russia has a multifaceted foreign policy. , it maintains diplomatic relations with 191 countries and has 144 embassies. The foreign policy is determined by the President and implemented by the Ministry of Foreign Affairs of Russia.

Although it is the successor state to a former superpower, Russia is commonly accepted to be a major great power, as well as a regional power. Russia is one of five permanent members of the UN Security Council. The country participates in the Quartet on the Middle East and the Six-party talks with North Korea. Russia is a member of the Council of Europe, OSCE, and APEC. Russia usually takes a leading role in regional organisations such as the CIS, EurAsEC, CSTO, and the SCO. Russia became the 39th member state of the Council of Europe in 1996. In 1998, Russia ratified the European Convention on Human Rights. The legal basis for EU relations with Russia is the Partnership and Cooperation Agreement, which came into force in 1997. The Agreement recalls the parties' shared respect for democracy and human rights, political and economic freedom and commitment to international peace and security. In May 2003, the EU and Russia agreed to reinforce their cooperation on the basis of common values and shared interests. President Vladimir Putin had advocated a strategic partnership with close integration in various dimensions, including establishment of EU-Russia Common Spaces. From the dissolution of the Soviet Union, Russia has initially developed a friendlier relationship with the United States and NATO, however today, the trilateral relationship has significantly deteriorated due to several issues and conflicts between Russia and the Western countries. The NATO-Russia Council was established in 2002 to allow the United States, Russia and the 27 allies in NATO to work together as equal partners to pursue opportunities for joint collaboration.
Russia maintains strong and positive relations with other SCO and BRICS countries. In recent years, the country has significantly strengthened bilateral ties with the People's Republic of China by signing the Treaty of Friendship as well as building the Trans-Siberian oil pipeline and gas pipeline from Siberia to China, and has since formed a special relationship with China. India is the largest customer of Russian military equipment and the two countries share extensive defense and strategic relations.

An important aspect of Russia's relations with the West is the criticism of Russia's political system and human rights management (including LGBT rights, media freedom, and reports about killed journalists) by Western governments, the mass media and the leading democracy and human rights watchdogs. In particular, such organisations as Amnesty International and Human Rights Watch consider Russia to have not enough democratic attributes and to allow few political rights and civil liberties to its citizens. Freedom House, an international organisation funded by the United States, ranks Russia as "not free", citing "carefully engineered elections" and "absence" of debate. Russian authorities dismiss these claims and especially criticise Freedom House. The Russian Ministry of Foreign Affairs has called the 2006 "Freedom in the World" report "prefabricated", stating that the human rights issues have been turned into a political weapon in particular by the United States. The ministry also claims that such organisations as Freedom House and Human Rights Watch use the same scheme of voluntary extrapolation of "isolated facts that of course can be found in any country" into "dominant tendencies".

Russia's power on the international stage depends on its petroleum revenue. If the world completes a transition to renewable energy and international demand for Russian oil, gas and coal resources is dramatically reduced, so will Russia's international power be. Russia is ranked 148 out of 156 countries in the index of Geopolitical Gains and Losses after energy transition (GeGaLo).

The Russian military is divided into the Ground Forces, Navy, and Air Force. There are also three independent arms of service: Strategic Missile Troops, Aerospace Defence Forces, and the Airborne Troops. , the military comprised over one million active duty personnel, the fifth-largest in the world. Additionally, there are over 2.5 million reservists, with the total number of reserve troops possibly being as high as 20 million. It is mandatory for all male citizens aged 18–27 to be drafted for a year of service in Armed Forces.

Russia has the largest stockpile of nuclear weapons in the world, the second-largest fleet of ballistic missile submarines, and the only modern strategic bomber force outside the United States. More than 90% of world's 14,000 nuclear weapons are owned by Russia and the United States. Russia's tank force is the largest in the world, while its surface navy and air force are among the largest.

The country has a large and fully indigenous arms industry, producing most of its own military equipment with only a few types of weapons imported. It has been one of the world's top supplier of arms since 2001, accounting for around 30% of worldwide weapons sales and exporting weapons to about 80 countries. The Stockholm International Peace Research Institute, SIPRI, found that Russia was the second biggest exporter of arms in 2010–14, increasing their exports by 37 per cent from the period 2005–2009. SIPRI estimated in 2020 that Russia is the third biggest exporters of arms, only behind the US and China. In 2010–14, Russia delivered weapons to 56 states and to rebel forces in eastern Ukraine.

The Russian government's official 2014 military budget is about 2.49 trillion rubles (approximately US$69.3 billion), the third-largest in the world behind the United States and the People's Republic of China. The official budget is set to rise to 3.03 trillion rubles (approximately US$83.7 billion) in 2015, and 3.36 trillion rubles (approximately US$93.9 billion) in 2016. However, unofficial estimates put the budget significantly higher, for example the Stockholm International Peace Research Institute (SIPRI) 2013 Military Expenditure Database estimated Russia's military expenditure in 2012 at US$90.749 billion. This estimate is an increase of more than US$18 billion on SIPRI's estimate of the Russian military budget for 2011 (US$71.9 billion). , Russia's military budget is higher than any other European nation.

According to the Constitution, the country comprises eighty-five federal subjects, including the disputed Republic of Crimea and federal city of Sevastopol. In 1993, when the Constitution was adopted, there were eighty-nine federal subjects listed, but later some of them were merged. These subjects have equal representation—two delegates each—in the Federation Council. However, they differ in the degree of autonomy they enjoy.

Federal subjects are grouped into eight federal districts, each administered by an envoy appointed by the President of Russia. Unlike the federal subjects, the federal districts are not a subnational level of government, but are a level of administration of the federal government. Federal districts' envoys serve as liaisons between the federal subjects and the federal government and are primarily responsible for overseeing the compliance of the federal subjects with the federal laws.

Russia is the largest country in the world; its total area is . This makes it larger than the continents of Oceania, Europe and Antarctica. It lies between latitudes 41° and 82° N, and longitudes 19° E and 169° W.

Russia's territorial expansion was achieved largely in the late 16th century under the Cossack Yermak Timofeyevich during the reign of Ivan the Terrible, at a time when competing city-states in the western regions of Russia had banded together to form one country. Yermak mustered an army and pushed eastward where he conquered nearly all the lands once belonging to the Mongols, defeating their ruler, Khan Kuchum.

Russia has a wide natural resource base, including major deposits of timber, petroleum, natural gas, coal, ores and other mineral resources.

The two most widely separated points in Russia are about apart along a geodesic line. These points are: a long Vistula Spit the boundary with Poland separating the Gdańsk Bay from the Vistula Lagoon and the most southeastern point of the Kuril Islands. The points which are farthest separated in longitude are apart along a geodesic line. These points are: in the west, the same spit on the boundary with Poland, and in the east, the Big Diomede Island. The Russian Federation spans 11 time zones.

Most of Russia consists of vast stretches of plains that are predominantly steppe to the south and heavily forested to the north, with tundra along the northern coast. Russia possesses 10% of the world's arable land. Mountain ranges are found along the southern borders, such as the Caucasus (containing Mount Elbrus, which at is the highest point in both Russia and Europe) and the Altai (containing Mount Belukha, which at the is the highest point of Siberia outside of the Russian Far East); and in the eastern parts, such as the Verkhoyansk Range or the volcanoes of Kamchatka Peninsula (containing Klyuchevskaya Sopka, which at the is the highest active volcano in Eurasia as well as the highest point of Asian Russia). The Ural Mountains, rich in mineral resources, form a north–south range that divides Europe and Asia.

Russia has an extensive coastline of over along the Arctic and Pacific Oceans, as well as along the Baltic Sea, Sea of Azov, Black Sea and Caspian Sea. The Barents Sea, White Sea, Kara Sea, Laptev Sea, East Siberian Sea, Chukchi Sea, Bering Sea, Sea of Okhotsk, and the Sea of Japan are linked to Russia via the Arctic and Pacific. Russia's major islands and archipelagos include Novaya Zemlya, the Franz Josef Land, the Severnaya Zemlya, the New Siberian Islands, Wrangel Island, the Kuril Islands, and Sakhalin. The Diomede Islands (one controlled by Russia, the other by the United States) are just apart, and Kunashir Island is about from Hokkaido, Japan.

Russia has thousands of rivers and inland bodies of water, providing it with one of the world's largest surface water resources. Its lakes contain approximately one-quarter of the world's liquid fresh water. The largest and most prominent of Russia's bodies of fresh water is Lake Baikal, the world's deepest, purest, oldest and most capacious fresh water lake. Baikal alone contains over one-fifth of the world's fresh surface water. Other major lakes include Ladoga and Onega, two of the largest lakes in Europe. Russia is second only to Brazil in volume of the total renewable water resources. Of the country's 100,000 rivers, the Volga is the most famous, not only because it is the longest river in Europe, but also because of its major role in Russian history. The Siberian rivers Ob, Yenisey, Lena and Amur are among the longest rivers in the world.

The enormous size of Russia and the remoteness of many areas from the sea result in the dominance of the humid continental climate, which is prevalent in all parts of the country except for the tundra and the extreme southwest. Mountains in the south obstruct the flow of warm air masses from the Indian Ocean, while the plain of the west and north makes the country open to Arctic and Atlantic influences.

Most of Northern European Russia and Siberia has a subarctic climate, with extremely severe winters in the inner regions of Northeast Siberia (mostly the Sakha Republic, where the Northern Pole of Cold is located with the record low temperature of ), and more moderate winters elsewhere. Both the strip of land along the shore of the Arctic Ocean and the Russian Arctic islands have a polar climate.

The coastal part of Krasnodar Krai on the Black Sea, most notably in Sochi, possesses a humid subtropical climate with mild and wet winters. In many regions of East Siberia and the Far East, winter is dry compared to summer; other parts of the country experience more even precipitation across seasons. Winter precipitation in most parts of the country usually falls as snow. The region along the Lower Volga and Caspian Sea coast, as well as some areas of southernmost Siberia, possesses a semi-arid climate.
Throughout much of the territory there are only two distinct seasons—winter and summer—as spring and autumn are usually brief periods of change between extremely low and extremely high temperatures. The coldest month is January (February on the coastline); the warmest is usually July. Great ranges of temperature are typical. In winter, temperatures get colder both from south to north and from west to east. Summers can be quite hot, even in Siberia. The continental interiors are the driest areas.

From north to south the East European Plain, also known as Russian Plain, is clad sequentially in Arctic tundra, coniferous forest (taiga), mixed and broad-leaf forests, grassland (steppe), and semi-desert (fringing the Caspian Sea), as the changes in vegetation reflect the changes in climate. Siberia supports a similar sequence but is largely taiga. Russia has the world's largest forest reserves, known as "the lungs of Europe", second only to the Amazon Rainforest in the amount of carbon dioxide it absorbs.

There are 266 mammal species and 780 bird species in Russia. A total of 415 animal species have been included in the Red Data Book of the Russian Federation as of 1997 and are now protected. There are 28 UNESCO World Heritage Sites in Russia, 40 UNESCO biosphere reserves, 41 national parks and 101 nature reserves. Russia still has many ecosystems which are still untouched by man— mainly in the northern areas taiga and in subarctic tundra of Siberia. Over time Russia has been having improvement and application of environmental legislation, development and implementation of various federal and regional strategies and programmes, and study, inventory and protection of rare and endangered plants, animals, and other organisms, and including them in the Red Data Book of the Russian Federation.

Russia has an upper-middle income<ref name="https://datahelpdesk.worldbank.org">, "World Bank"</ref> mixed economy with enormous natural resources, particularly oil and natural gas. It has the 11th largest economy in the world by nominal GDP and the 6th largest by purchasing power parity (PPP). Since the turn of the 21st century, higher domestic consumption and greater political stability have bolstered economic growth in Russia. The country ended 2008 with its ninth straight year of growth, but growth has slowed with the decline in the price of oil and gas. Real GDP per capita, PPP (current international) was 19,840 in 2010. Growth was primarily driven by non-traded services and goods for the domestic market, as opposed to oil or mineral extraction and exports. The average nominal salary in Russia was $967 per month in early 2013, up from $80 in 2000. In May 2016 the average nominal monthly wages fell below $450 per month, and tax on the income of individuals is payable at the rate of 13% on most incomes. Approximately 19.2 million of Russians lived below the national poverty line in 2016, significantly up from 16.1 million in 2015. Unemployment in Russia was 5.4% in 2014, down from about 12.4% in 1999. Officially, about 20–25% of the Russian population is categorised as middle class; however some economists and sociologists think this figure is inflated and the real fraction is about 7%. After the United States, the European Union and other countries imposed economic sanctions after the annexation of Crimea and a collapse in oil prices, the proportion of middle-class could decrease drastically. The economic development of the country has been uneven geographically with the Moscow region contributing a very large share of the country's GDP.
Oil, natural gas, metals, and timber account for more than 80% of Russian exports abroad. Since 2003, the exports of natural resources started decreasing in economic importance as the internal market strengthened considerably. the oil-and-gas sector accounted for 16% of GDP, 52% of federal budget revenues and over 80% of total exports. Oil export earnings allowed Russia to increase its foreign reserves from $12 billion in 1999 to $597.3 billion on 1 August 2008. , foreign reserves in Russia fell to US$332 Billion. The macroeconomic policy under Finance Minister Alexei Kudrin was prudent and sound, with excess income being stored in the Stabilization Fund of Russia. In 2006, Russia repaid most of its formerly massive debts, leaving it with one of the lowest foreign debts among major economies. The Stabilization Fund helped Russia to come out of the global financial crisis in a much better state than many experts had expected.

A simpler, more streamlined tax code adopted in 2001 reduced the tax burden on people and dramatically increased state revenue. Russia has a flat tax rate of 13%. This ranks it as the country with the second most attractive personal tax system for single managers in the world after the United Arab Emirates. According to Bloomberg, Russia is considered well ahead of most other resource-rich countries in its economic development, with a long tradition of education, science, and industry. The country has a higher proportion of higher education graduates than any other country in Eurasia.

Inequality of household income and wealth has also been noted, with Credit Suisse finding Russian wealth distribution so much more extreme than other countries studied it "deserves to be placed in a separate category."
Another problem is modernisation of infrastructure, ageing and inadequate after years of being neglected in the 1990s; the government has said $1 trillion will be invested in development of infrastructure by 2020. In December 2011, Russia was approved as a member of the World Trade Organisation after 18 years of dialogue, allowing it a greater access to overseas markets. Some analysts estimate that WTO membership could bring the Russian economy a bounce of up to 3% annually. Russia ranks as the second-most corrupt country in Europe (after Ukraine), according to the Corruption Perceptions Index. The Norwegian-Russian Chamber of Commerce also states that "[c]orruption is one of the biggest problems both Russian and international companies have to deal with." Corruption in Russia is perceived as a significant problem impacting all aspects of life, including public administration, law enforcement, healthcare and education. The phenomenon of corruption is strongly established in the historical model of public governance in Russia and attributed to general weakness of rule of law in Russia. According to Transparency International's Corruption Perceptions Index, Russia's public sector ranked 137th (out of 180 countries) with a score of 28 out of 100 in 2019.

The Russian central bank announced plans in 2013 to free float the Russian ruble in 2015. According to a stress test conducted by the central bank Russian financial system would be able to handle a currency decline of 25%–30% without major central bank interference. However, the Russian economy began stagnating in late 2013 and in combination with the War in Donbass is in danger of entering stagflation, slow growth and high inflation. The recent decline in the Russian ruble has increased the costs for Russian companies to make interest payments on debt issued in U.S. dollar or other foreign currencies that have strengthened against the ruble; thus it costs Russian companies more of their ruble-denominated revenue to repay their debt holders in dollars or other foreign currencies. , the ruble was devalued more than 50 percent since July 2014. Moreover, after bringing inflation down to 3.6% in 2012, the lowest rate since gaining independence from the Soviet Union, inflation in Russia jumped to nearly 7.5% in 2014, causing the central bank to increase its lending rate to 8% from 5.5% in 2013. In an October 2014 article in "Bloomberg Business Week", it was reported that Russia had significantly started shifting its economy towards China in response to increasing financial tensions following its annexation of Crimea and subsequent Western economic sanctions.

In recent years, Russia has frequently been described in the media as an energy superpower. The country has the world's largest natural gas reserves, the 8th largest oil reserves, and the second largest coal reserves. Russia is the world's leading natural gas exporter and second largest natural gas producer, while also the largest oil exporter and the largest oil producer.

Russia is the third largest electricity producer in the world and the 5th largest renewable energy producer, the latter because of the well-developed hydroelectricity production in the country. Large cascades of hydropower plants are built in European Russia along big rivers like the Volga. The Asian part of Russia also features a number of major hydropower stations; however, the gigantic hydroelectric potential of Siberia and the Russian Far East largely remains unexploited.

Russia was the first country to develop civilian nuclear power and to construct the world's first nuclear power plant. Currently the country is the 4th largest nuclear energy producer, with all nuclear power in Russia being managed by Rosatom State Corporation. The sector is rapidly developing, with an aim of increasing the total share of nuclear energy from current 16.9% to 23% by 2020. The Russian government plans to allocate 127 billion rubles ($5.42 billion) to a federal program dedicated to the next generation of nuclear energy technology. About 1 trillion rubles ($42.7 billion) is to be allocated from the federal budget to nuclear power and industry development before 2015.

In May 2014 on a two-day trip to Shanghai, President Putin signed a deal on behalf of Gazprom for the Russian energy giant to supply China with 38 billion cubic meters of natural gas per year. Construction of a pipeline to facilitate the deal was agreed whereby Russia would contribute $55bn to the cost, and China $22bn, in what Putin described as "the world's biggest construction project for the next four years." The natural gas would begin to flow sometime between 2018 and 2020 and would continue for 30 years at an ultimate cost to China of $400bn.

Russia recorded a trade surplus of US$130.1 billion in 2017. Russia's Trade Balance recorded a surplus of US$19.7 billion in October 2018, compared with a surplus of US$10.1 billion in October 2017.

The European Union is Russia's largest trading partner and Russia is the EU's fourth largest trading partner. 75% of foreign direct investment (FDI) stocks in Russia come from the EU.

Reuters reported that U.S. companies "generated more than $90 billion in revenue from Russia in 2017." According to the AALEP, "there are almost 3,000 American companies in Russia, and the U.S. is also the leader in terms of foreign companies in Special Economic Zones, with 11 projects."
Russia recorded a trade surplus of US$15.8 billion in 2013. Balance of trade in Russia is reported by the Central Bank of Russia. Historically, from 1997 until 2013, Russia balance of trade averaged US$8338.23 million reaching an all-time high of US$20647 million in December 2011 and a record low of −185 USD million in February 1998. Russia runs regular trade surpluses primarily due to exports of commodities.

In 2015, Russia main exports are oil and natural gas (62.8% of total exports), ores and metals (5.9%), chemical products (5.8%), machinery and transport equipment (5.4%) and food (4.7%). Others include: agricultural raw materials (2.2%) and textiles (0.2%).

Russia imports food, ground transports, pharmaceuticals and textile and footwear. Main trading partners are: China (7% of total exports and 10% of imports), Germany (7% of exports and 8% of imports) and Italy. This page includes a chart with historical data for Russia balance of trade. Exports in Russia decreased to US$39038 million in January 2013 from US$48568 million in December 2012. Exports in Russia is reported by the Central Bank of Russia. Historically, from 1994 until 2013, Russia Exports averaged US$18668.83 million reaching an all-time high of US$51338 million in December 2011 and a record low of US$4087 million in January 1994. Russia is the 16th largest export economy in the world (2016) and is a leading exporter of oil and natural gas. In Russia, services are the biggest sector of the economy and account for 58% of GDP. Within services the most important segments are: wholesale and retail trade, repair of motor vehicles, motorcycles and personal and household goods (17% of total GDP); public administration, health and education (12%); real estate (9%) and transport storage and communications (7%). Industry contributes 40% to total output. Mining (11% of GDP), manufacturing (13%) and construction (4%) are the most important industry segments. Agriculture accounts for the remaining 2%. This page includes a chart with historical data for Russia Exports. Imports in Russia decreased to US$21296 million in January 2013 from US$31436 million in December 2012. Imports in Russia is reported by the Central Bank of Russia. Historically, from 1994 until 2013, Russia imports averaged US$11392.06 million reaching an all-time high of US$31553 million in October 2012 and a record low of US$2691 million in January 1999. Russia main imports are food (13% of total imports) and ground transports (12%). Others include: pharmaceuticals, textile and footwear, plastics and optical instruments. Main import partners are China (10% of total imports) and Germany (8%). Others include: Italy, France, Japan and United States. This page includes a chart with historical data for Russia Imports.

Foreign trade of Russia – Russian export and import

Foreign trade rose 34% to $151.5 billion in the first half of 2005, mainly due to the increase in oil and gas prices which now form 64% of all exports by value. Trade with CIS countries is up 13.2% to $23.3 billion. Trade with the EU forms 52.9%, with the CIS 15.4%, Eurasian Economic Community 7.8% and Asia-Pacific Economic Cooperation 15.9%.

Russia's total area of cultivated land is estimated at , the fourth largest in the world. From 1999 to 2009, Russia's agriculture grew steadily, and the country turned from a grain importer to the third largest grain exporter after the EU and the United States. The production of meat has grown from 6,813,000 tonnes in 1999 to 9,331,000 tonnes in 2008, and continues to grow.

The 2014 devaluation of the rouble and imposition of sanctions spurred domestic production, and in 2016 Russia exceeded Soviet grain production levels, and became the world's largest exporter of wheat.

This restoration of agriculture was supported by a credit policy of the government, helping both individual farmers and large privatised corporate farms that once were Soviet kolkhozes and which still own the significant share of agricultural land. While large farms concentrate mainly on grain production and husbandry products, small private household plots produce most of the country's potatoes, vegetables and fruits.

Since Russia borders three oceans (the Atlantic, Arctic, and Pacific), Russian fishing fleets are a major world fish supplier. Russia captured 3,191,068 tons of fish in 2005. Both exports and imports of fish and sea products grew significantly in recent years, reaching $2,415 and $2,036 million, respectively, in 2008.

Sprawling from the Baltic Sea to the Pacific Ocean, Russia has more than a fifth of the world's forests, which makes it the largest forest country in the world. However, according to a 2012 study by the Food and Agriculture Organization of the United Nations and the Government of the Russian Federation, the considerable potential of Russian forests is underutilised and Russia's share of the global trade in forest products is less than four percent.

Railway transport in Russia is mostly under the control of the state-run Russian Railways monopoly. The company accounts for over 3.6% of Russia's GDP and handles 39% of the total freight traffic (including pipelines) and more than 42% of passenger traffic. The total length of common-used railway tracks exceeds , second only to the United States. Over of tracks are electrified, which is the largest number in the world, and additionally there are more than of industrial non-common carrier lines. Railways in Russia, unlike in the most of the world, use broad gauge of , with the exception of on Sakhalin island using narrow gauge of . The most renowned railway in Russia is Trans-Siberian ("Transsib"), spanning a record seven time zones and serving the longest single continuous services in the world, Moscow-Vladivostok (), Moscow–Pyongyang () and Kiev–Vladivostok ().

Much of Russia's inland waterways, which total, are made up of natural rivers or lakes. In the European part of the country the network of channels connects the basins of major rivers. Russia's capital, Moscow, is sometimes called "the port of the five seas", because of its waterway connections to the Baltic, White, Caspian, Azov and Black Seas.
Major sea ports of Russia include Rostov-on-Don on the Azov Sea, Novorossiysk on the Black Sea, Astrakhan and Makhachkala on the Caspian, Kaliningrad and St Petersburg on the Baltic, Arkhangelsk on the White Sea, Murmansk on the Barents Sea, Petropavlovsk-Kamchatsky and Vladivostok on the Pacific Ocean. In 2008 the country owned 1,448 merchant marine ships. The world's only fleet of nuclear-powered icebreakers advances the economic exploitation of the Arctic continental shelf of Russia and the development of sea trade through the Northern Sea Route between Europe and East Asia.

By total length of pipelines Russia is second only to the United States. Currently many new pipeline projects are being realised, including Nord Stream and South Stream natural gas pipelines to Europe, and the Eastern Siberia – Pacific Ocean oil pipeline (ESPO) to the Russian Far East and China.

Russia has 1,216 airports, the busiest being Sheremetyevo, Domodedovo, and Vnukovo in Moscow, and Pulkovo in St. Petersburg.

Typically, major Russian cities have well-developed systems of public transport, with the most common varieties of exploited vehicles being bus, trolleybus and tram. Seven Russian cities, namely Moscow, Saint Petersburg, Nizhny Novgorod, Novosibirsk, Samara, Yekaterinburg, and Kazan, have underground metros, while Volgograd features a metrotram. The total length of metros in Russia is . Moscow Metro and Saint Petersburg Metro are the oldest in Russia, opened in 1935 and 1955 respectively. These two are among the fastest and busiest metro systems in the world, and some of them are famous for rich decorations and unique designs of their stations, which is a common tradition in Russian metros and railways.

Science and technology in Russia blossomed since the Age of Enlightenment, when Peter the Great founded the Russian Academy of Sciences and Saint Petersburg State University, and polymath Mikhail Lomonosov established the Moscow State University, paving the way for a strong native tradition in learning and innovation. In the 19th and 20th centuries the country produced a large number of notable scientists and inventors.

The Russian physics school began with Lomonosov who proposed the law of conservation of matter preceding the energy conservation law. Russian discoveries and inventions in physics include the electric arc, electrodynamical Lenz's law, space groups of crystals, photoelectric cell, superfluidity, Cherenkov radiation, electron paramagnetic resonance, heterotransistors and 3D holography. Lasers and masers were co-invented by Nikolai Basov and Alexander Prokhorov, while the idea of tokamak for controlled nuclear fusion was introduced by Igor Tamm, Andrei Sakharov and Lev Artsimovich, leading eventually the modern international ITER project, where Russia is a party.

Since the time of Nikolay Lobachevsky (the "Copernicus of Geometry" who pioneered the non-Euclidean geometry) and a prominent tutor Pafnuty Chebyshev, the Russian mathematical school became one of the most influential in the world. Chebyshev's students included Aleksandr Lyapunov, who founded the modern stability theory, and Andrey Markov who invented the Markov chains. In the 20th century Soviet mathematicians, such as Andrey Kolmogorov, Israel Gelfand, and Sergey Sobolev, made major contributions to various areas of mathematics. Nine Soviet/Russian mathematicians were awarded with the Fields Medal, a most prestigious award in mathematics. Recently Grigori Perelman was offered the first ever Clay Millennium Prize Problems Award for his final proof of the Poincaré conjecture in 2002.

Russian chemist Dmitry Mendeleev invented the Periodic table, the main framework of modern chemistry. Aleksandr Butlerov was one of the creators of the theory of chemical structure, playing a central role in organic chemistry. Russian biologists include Dmitry Ivanovsky who discovered viruses, Ivan Pavlov who was the first to experiment with the classical conditioning, and Ilya Mechnikov who was a pioneer researcher of the immune system and probiotics.

Many Russian scientists and inventors were émigrés, like Igor Sikorsky, who built the first airliners and modern-type helicopters; Vladimir Zworykin, often called the father of television; chemist Ilya Prigogine, noted for his work on dissipative structures and complex systems; Nobel Prize-winning economists Simon Kuznets and Wassily Leontief; physicist Georgiy Gamov (an author of the Big Bang theory) and social scientist Pitirim Sorokin. Many foreigners worked in Russia for a long time, like Leonard Euler and Alfred Nobel.

Russian inventions include arc welding by Nikolay Benardos, further developed by Nikolay Slavyanov, Konstantin Khrenov and other Russian engineers. Gleb Kotelnikov invented the knapsack parachute, while Evgeniy Chertovsky introduced the pressure suit. Alexander Lodygin and Pavel Yablochkov were pioneers of electric lighting, and Mikhail Dolivo-Dobrovolsky introduced the first three-phase electric power systems, widely used today. Sergei Lebedev invented the first commercially viable and mass-produced type of synthetic rubber. The first ternary computer, "Setun", was developed by Nikolay Brusentsov.

In the 20th century a number of prominent Soviet aerospace engineers, inspired by the fundamental works of Nikolai Zhukovsky, Sergei Chaplygin and others, designed many hundreds of models of military and civilian aircraft and founded a number of "KBs" ("Construction Bureaus") that now constitute the bulk of Russian United Aircraft Corporation. Famous Russian aircraft include the civilian Tu-series, Su and MiG fighter aircraft, Ka and Mi-series helicopters; many Russian aircraft models are on the list of most produced aircraft in history.

Famous Russian battle tanks include T34, the most heavily produced tank design of World War II, and further tanks of T-series, including the most produced tank in history, T54/55. The AK47 and AK74 by Mikhail Kalashnikov constitute the most widely used type of assault rifle throughout the world—so much so that more AK-type rifles have been manufactured than all other assault rifles combined.

With all these achievements, however, since the late Soviet era Russia was lagging behind the West in a number of technologies, mostly those related to energy conservation and consumer goods production. The crisis of the 1990s led to the drastic reduction of the state support for science and a brain drain migration from Russia.

In the 2000s, on the wave of a new economic boom, the situation in the Russian science and technology has improved, and the government launched a campaign aimed into modernisation and innovation. Russian President Dmitry Medvedev formulated top priorities for the country's technological development:

Currently Russia has completed the GLONASS satellite navigation system. The country is developing its own fifth-generation jet fighter and constructing the first serial mobile nuclear plant in the world.

Russian achievements in the field of space technology and space exploration are traced back to Konstantin Tsiolkovsky, the father of theoretical astronautics. His works had inspired leading Soviet rocket engineers, such as Sergey Korolyov, Valentin Glushko, and many others who contributed to the success of the Soviet space program in the early stages of the Space Race and beyond.

In 1957 the first Earth-orbiting artificial satellite, "Sputnik 1", was launched; in 1961 the first human trip into space was successfully made by Yuri Gagarin. Many other Soviet and Russian space exploration records ensued, including the first spacewalk performed by Alexei Leonov, Luna 9 was the first spacecraft to land on the Moon, Zond 5 brought the first Earthlings (two tortoises and other life forms) to circumnavigate the Moon, Venera 7 was the first to land on another planet (Venus), Mars 3 then the first to land on Mars, the first space exploration rover "Lunokhod 1", and the first space station "Salyut 1" and "Mir".

After the collapse of the Soviet Union, some government-funded space exploration programs, including the Buran space shuttle program, were cancelled or delayed, while participation of the Russian space industry in commercial activities and international cooperation intensified.
Nowadays Russia is the largest satellite launcher. After the United States Space Shuttle program ended in 2011, Soyuz rockets became the only provider of transport for astronauts at the International Space Station.

Luna-Glob is a Russian Moon exploration programme, with first planned mission launch in 2021. Roscosmos is also developing the Orel spacecraft, to replace the aging Soyuz, it could also conduct mission to lunar orbit as early as 2026. In February 2019, it was announced that Russia is intending to conduct its first crewed mission to land on the Moon in 2031.

In Russia, approximately 70 per cent of drinking water comes from surface water and 30 per cent from groundwater. In 2004, water supply systems had a total capacity of 90 million cubic metres a day. The average residential water use was 248 litres per capita per day. One fourth of the world's fresh surface and groundwater is located in Russia. The water utilities sector is one of the largest industries in Russia serving the entire Russian population.

Lake Baikal is famous for its record depth and clear waters. It contains 20% of the world's liquid fresh water. However, as water pollution gets worse, the lake is going to be a swamp instead of a freshwater lake soon.

There are many different estimates of the actual cost of corruption. According to official government statistics from Rosstat, the "shadow economy" occupied only 15% of Russia's GDP in 2011, and this included unreported salaries (to avoid taxes and social payments) and other types of tax evasion. According to Rosstat's estimates, corruption in 2011 amounted to only 3.5 to 7% of GDP. In comparison, some independent experts maintain that corruption consumes as much of 25% of Russia's GDP. A World Bank report puts this figure at 48%. There is also an interesting shift in the main focus of bribery: whereas previously officials took bribes to shut their eyes to legal infractions, they now take them simply to perform their duties. Many experts admit that in recent years corruption in Russia has become a business. In the 1990s, businessmen had to pay different criminal groups to provide a ""krysha"" (literally, a "roof", i.e., protection). Nowadays, this "protective" function is performed by officials. Corrupt hierarchies characterise different sectors of the economy, including education.

In the end, the Russian population pays for this corruption. For example, some experts believe that the rapid increases in tariffs for housing, water, gas and electricity, which significantly outpace the rate of inflation, are a direct result of high volumes of corruption at the highest levels. In the recent years the reaction to corruption has changed: starting from Putin's second term, very few corruption cases have been the subject of outrage. Putin's system is remarkable for its ubiquitous and open merging of the civil service and business, as well as its use of relatives, friends, and acquaintances to benefit from budgetary expenditures and take over state property. Corporate, property, and land raiding is commonplace.

On 26 March 2017, protests against alleged corruption in the federal government took place simultaneously in many cities across the country. They were triggered by the lack of proper response from the Russian authorities to the published investigative film "He Is Not Dimon To You", which has garnered more than 20 million views on YouTube.

In the 2018 results of the Corruption Perceptions Index by Transparency International, Russia ranked 138th out of 180 countries with a score of 28 out of 100, tying with Guinea, Iran, Lebanon, Mexico and Papua New Guinea.

With a population of 142.8 million according to the 2010 census, rising to 146.7 million as of 2020. Russia is the most populous country in Europe, and the ninth-most populous country in the world, its population density stands at 9 inhabitants per square kilometre (23 per square mile). The overall life expectancy in Russia at birth is 72.4 years (66.9 years for males and 77.6 years for females). Since the 1990s, Russia's death rate has exceeded its birth rate. As of 2018, the total fertility rate (TFR) across Russia was estimated to be 1.57 born per woman, one of the lowest fertility rates in the world, below the replacement rate of 2.1, and considerably below the high of 7.44 children born per woman in 1908. Subsequently, the country has one of the oldest population in the world, with an average age of 40.3 years.

Nevertheless, Russia's overall birth rate is higher than that of most European countries (13.3 births per 1000 people in 2014 compared to the European Union average of 10.1 per 1000), though its death rate is also substantially higher (in 2014, Russia's death rate was 13.1 per 1000 people compared to the EU average of 9.7 per 1000). Since 2010, Russia has seen increased population growth due to declining death rates, increased birth rates and increased immigration. In 2009, it recorded annual population growth for the first time in fifteen years, with total growth of 10,500. In 2012, the trend continued, with 1,896,263 births, the highest since 1990, and even exceeding annual births during the period 1967–1969.

Russia is home to approximately 111 million ethnic Russians, and about 20 million ethnic Russians live outside Russia in the former republics of the Soviet Union, mostly in Ukraine and Kazakhstan. The 2010 census recorded 81% of the population as ethnically Russian, and 19% as other ethnicities: 3.7% Tatars; 1.4% Ukrainians; 1.1% Bashkirs; 1% Chuvashes; 11.8% others and unspecified. According to the Census, 84.93% of the Russian population belongs to European ethnic groups (Slavic, Germanic, Finnic, Greek, and others).

The government is implementing a number of programs designed to increase the birth rate and attract more migrants. Monthly government child-assistance payments were doubled to US$55, and a one-time payment of US$9,200 has been offered to women who have a second child since 2007.

The number of Russian emigrants steadily declined from 359,000 in 2000 to 32,000 in 2009. According to the UN, Russia's immigrant population is the third largest in the world, numbering 11.6 million. Ukraine, Uzbekistan, Tajikistan, Azerbaijan, Moldova and Kazakhstan were the leading countries of origin for immigrants to Russia. There are about 3 million Ukrainians living in Russia. In 2016, 196,000 migrants arrived, mostly from the ex-Soviet states.

Since 2006, the Russian government started simplifying immigration laws and launched a state program "for providing assistance to voluntary immigration of ethnic Russians from former Soviet republics". In light of these trends, President Putin declared that Russia's population could reach 146 million by 2025, mainly as a result of immigration.

Ethnic Russians comprise 81% of the country's population. Russia is a multi-national state with over 185 ethnic groups designated as nationalities; the populations of these groups vary enormously, from millions (e.g., Russians and Tatars) to under 10,000 (e.g., Samis and Eskimo).

Russia's 185 ethnic groups speak over 100 languages. According to the 2002 Census, 142.6 million people speak Russian, followed by Tatar with 5.3 million and Ukrainian with 1.8 million speakers. Russian is the only official state language, but the Constitution gives the individual republics the right to establish their own state languages in addition to Russian.
Despite its wide distribution, the Russian language is homogeneous throughout the country. Russian is the most geographically widespread language of Eurasia, as well as the most widely spoken Slavic language. It belongs to the Indo-European language family and is one of the living members of the East Slavic languages, the others being Belarusian and Ukrainian (and possibly Rusyn). Written examples of Old East Slavic ("Old Russian") are attested from the 10th century onwards.

Russian is the second-most used language on the Internet after English, one of two official languages aboard the International Space Station and is one of the six official languages of the UN. 35 languages are officially recognised in Russia in various regions by local governments.

Though a secular state under the constitution, Russia is often said to have Russian Orthodoxy as the "de facto" national religion, despite other minorities: "The Russian Orthodox Church is de facto privileged religion of the state, claiming the right to decide which other religions or denominations are to be granted the right of registration".

Russians have practised Orthodox Christianity since the 10th century. According to the historical traditions of the Orthodox Church, Christianity was first brought to the territory of modern Belarus, Russia and Ukraine by Saint Andrew, the first Apostle of Jesus Christ. Following the "Primary Chronicle", the definitive Christianization of Kievan Rus' dates from the year 988 (the year is disputed), when Vladimir the Great was baptised in Chersonesus and proceeded to baptise his family and people in Kiev. The latter events are traditionally referred to as the "baptism of Rus'" (, ) in Russian and Ukrainian literature. Much of the Russian population, like other Slavic peoples, preserved for centuries a double belief ("dvoeverie") in both indigenous religion and Orthodox Christianity.

At the time of the 1917 Revolution, the Russian Orthodox Church was deeply integrated into the autocratic state, enjoying official status. This was a significant factor that contributed to the Bolshevik attitude to religion and the steps they took to control it. Moreover, the Bolsheviks including many people with non-Russian and/or non-Christian background, such as Vladimir Lenin, Leon Trotsky, Grigory Zinoviev, Lev Kamenev, and Grigori Sokolnikov, who were, at best, indifferent towards Christianity, and at worst hostile to it. The ideas of German philosopher Karl Marx were synthesised with Lenin's own political thought to form the Communist Party.

Thus the USSR became one of the first communist states to proclaim, as an ideological objective, the elimination of religion and its replacement with universal atheism. The communist government ridiculed religions and their believers, and propagated atheism in schools. The confiscation of religious assets was often based on accusations of illegal accumulation of wealth. State atheism in the Soviet Union was known in Russian as "gosateizm", and was based on the ideology of Marxism–Leninism, which consistently advocated has consistently advocated the control, suppression, and elimination of religion. Within about a year of the revolution, the state expropriated all church property, including the churches themselves, and in the period from 1922 to 1926, 28 Russian Orthodox bishops and more than 1,200 priests were killed. Many more were persecuted.

After the collapse of the Soviet Union there has been a renewal of religions in Russia, and among Slavs various movements have emerged besides Christianity, including Rodnovery (Slavic Native Faith), Assianism, and other ethnic Paganisms, Roerichism, Ringing Cedars' Anastasianism, Hinduism, Siberian shamanism or Tengrism, and other religions.

As of a different sociological surveys on religious adherence, from 41% to over 80% of the total population of Russia adhere to the Russian Orthodox Church. In 2012 the research organization Sreda, in cooperation with the 2010 census and the Ministry of Justice, published the Arena Atlas, a detailed enumeration of religious populations and nationalities in Russia, based on a large-sample country-wide survey. The results showed that 46.8% of Russians declared themselves Christians—including 41% Russian Orthodox, 1.5% simply Orthodox or members of non-Russian Orthodox churches, 4.1% unaffiliated Christians, and less than 1% for both Old Believers, Catholics, and Protestants—while 25% were spiritual but not religious, 13% were atheists, 6.5% were Muslims, 1.2% were followers of "traditional religions honoring gods and ancestors" (including Rodnovery, Tengrism and other ethnic religions), and 0.5% were Buddhists, 0.1% were religious Jews and 0.1% were Hindus.

The 2017 Survey "Religious Belief and National Belonging in Central and Eastern Europe" made by the Pew Research Center showed that 73% of Russians declared themselves Christians—including 71% Orthodox, 1% Catholic, and 2% Other Christians, while 15% were unaffiliated, 10% were Muslims, and 1% were from other religions. According to the same study, Christianity experienced significant increase since the fall of the USSR in 1991, and more Russians say they are Christian now (73%) than say they were raised Christian (65%).
According to various reports, the proportion of not religious people in Russia is between 16% and 48% of the population. According to recent studies, the proportion of atheists has significantly decreased over the decades after the dissolution of the Soviet Union.

Orthodox Christianity, Islam, Buddhism, and Paganism (either preserved or revived) are recognised by law as Russia's traditional religions, marking the country's "historical heritage".

An estimated 95% of the registered Orthodox parishes belong to the Russian Orthodox Church while there are a number of smaller Orthodox churches. However, the vast majority of Orthodox believers do not attend church on a regular basis. Easter is the most popular religious holiday in Russia, celebrated by a large segment of the Russian population, including large numbers of those who are non-religious. More than three-quarters of the Russian population celebrate Easter by making traditional Easter cakes, coloured eggs and paskha.

Islam is the second largest religion in Russia after Russian Orthodoxy. It is the traditional or predominant religion amongst some Caucasian ethnicities (notably the Chechens, the Ingush and the Circassians), and amongst some Turkic peoples (notably the Tatars and the Bashkirs). Survey published in 2019 by the Pew Research Center found that 76% of Russians had a favourable view of Muslims.

Buddhism is traditional in three republics of Russia: Buryatia, Tuva, and Kalmykia, the latter being the only region in Europe where Buddhism is the most practiced religion.
In cultural and social affairs, Vladimir Putin has collaborated closely with the Russian Orthodox Church. Patriarch Kirill of Moscow, head of the Church, endorsed his election in 2012. Steven Myers reports, "The church, once heavily repressed, had emerged from the Soviet collapse as one of the most respected institutions... Now Kiril led the faithful directly into an alliance with the state." Baptist minister Mark Woods provides specific examples of how the Church under Patriarch Kirill of Moscow has backed the expansion of Russian power into Crimea and eastern Ukraine.

The Holy Synod of the Russian Orthodox Church, at its session on 15 October 2018, severed ties with the Ecumenical Patriarchate of Constantinople. The decision was taken in response to the move made by the Patriarchate of Constantinople a few days prior that effectively ended the Moscow Patriarchate's jurisdiction over Ukraine and promised autocephaly to Ukraine.

On 26 April 2017, for the first time, the U.S. Commission on International Religious Freedom classified Russia as one of the world's worst violators of religious liberty, recommending in its 2017 annual report that the U.S. government deem Russia a "country of particular concern" under the International Religious Freedom Act and negotiate for religious liberty. The report states, "—it is the sole state to have not only continually intensified its repression of religious freedom since USCIRF commenced monitoring it, but also to have expanded its repressive policies...ranging from administrative harassment to arbitrary imprisonment to extrajudicial killing, are implemented in a fashion that is systematic, ongoing, and egregious." On 4 April 2017 UN Special Rapporteur on Freedom of Opinion and Expression David Kaye, UN Special Rapporteur on Freedoms of Peaceful Assembly and Association Maina Kiai, and UN Special Rapporteur on Freedom of Religion and Belief Ahmed Shaheed condemned Russia's treatment of Jehovah's Witnesses. Many other countries and international organizations have spoken out on Russia's religious abuses.

The Russian Constitution guarantees free, universal health care for all its citizens. In practice, however, free health care is partially restricted because of mandatory registration. While Russia has more physicians, hospitals, and health care workers than almost any other country in the world on a per capita basis, since the dissolution of the Soviet Union the health of the Russian population has declined considerably as a result of social, economic, and lifestyle changes; the trend has been reversed only in the recent years, with average life expectancy having increased 6.8 years for males and 4.2 years for females between 2006 and 2018.

Due to the ongoing Russian financial crisis since 2014, major cuts in health spending have resulted in a decline in the quality of service of the state healthcare system. About 40% of basic medical facilities have fewer staff than they are supposed to have, with others being closed down. Waiting times for treatment have increased, and patients have been forced to pay for more services that were previously free.

, the average life expectancy at birth in Russia is 72.4 years (66.9 years for males and 77.6 years for females). The biggest factor contributing to the relatively low life expectancy for males is a high mortality rate among working-age males. Deaths mostly occur from preventable causes, including alcohol poisoning, smoking, traffic accidents and violent crime. As a result, Russia has one of the world's most female-biased sex ratios, with 0.859 males to every female.

Russia has the most college-level or higher graduates in terms of percentage of population in the world, at 54%. Russia has a free education system, which is guaranteed for all citizens by the Constitution, however entry to subsidised higher education is highly competitive. As a result of great emphasis on science and technology in education, Russian medical, mathematical, scientific, and aerospace research is generally of a high order.

Since 1990, the 11-year school education has been introduced. Education in state-owned secondary schools is free. University level education is free, with exceptions. A substantial share of students is enrolled for full pay (many state institutions started to open commercial positions in the last years).

The oldest and largest Russian universities are Moscow State University and Saint Petersburg State University. In the 2000s, in order to create higher education and research institutions of comparable scale in Russian regions, the government launched a program of establishing "federal universities", mostly by merging existing large regional universities and research institutes and providing them with a special funding. These new institutions include the Southern Federal University, Siberian Federal University, Kazan Volga Federal University, North-Eastern Federal University, and Far Eastern Federal University. According to the 2018 QS World University Rankings, the highest-ranking Russian educational institution is Moscow State University, rated 95th in the world.

There are over 160 different ethnic groups and indigenous peoples in Russia. The country's vast cultural diversity spans ethnic Russians with their Slavic Orthodox traditions, Tatars and Bashkirs with their Turkic Muslim culture, Buddhist nomadic Buryats and Kalmyks, Shamanistic peoples of the Extreme North and Siberia, highlanders of the Northern Caucasus, and Finno-Ugric peoples of the Russian North West and Volga Region.

Handicraft, like Dymkovo toy, khokhloma, gzhel and palekh miniature represent an important aspect of Russian folk culture. Ethnic Russian clothes include kaftan, kosovorotka and ushanka for men, sarafan and kokoshnik for women, with lapti and valenki as common shoes. The clothes of Cossacks from Southern Russia include burka and papaha, which they share with the peoples of the Northern Caucasus.

Russian cuisine widely uses fish, caviar, poultry, mushrooms, berries, and honey. Crops of rye, wheat, barley, and millet provide the ingredients for various breads, pancakes and cereals, as well as for kvass, beer and vodka drinks. Black bread is rather popular in Russia, compared to the rest of the world. Flavourful soups and stews include shchi, borsch, ukha, solyanka and okroshka. Smetana (a heavy sour cream) is often added to soups and salads. Pirozhki, blini and syrniki are native types of pancakes. Chicken Kiev, pelmeni and shashlyk are popular meat dishes, the last two being of Tatar and Caucasus origin respectively. Other meat dishes include stuffed cabbage rolls "(golubtsy)" usually filled with meat. Salads include Olivier salad, vinegret and dressed herring.

Russia's large number of ethnic groups have distinctive traditions regarding folk music. Typical ethnic Russian musical instruments are gusli, balalaika, zhaleika, and garmoshka. Folk music had a significant influence on Russian classical composers, and in modern times it is a source of inspiration for a number of popular folk bands, like Melnitsa. Russian folk songs, as well as patriotic Soviet songs, constitute the bulk of the repertoire of the world-renowned Red Army choir and other popular ensembles.

Russians have many traditions, including the washing in banya, a hot steam bath somewhat similar to sauna. Old Russian folklore takes its roots in the pagan Slavic religion. Many Russian fairy tales and epic bylinas were adapted for animation films, or for feature movies by the prominent directors like Aleksandr Ptushko ("Ilya Muromets", "Sadko") and Aleksandr Rou ("Morozko", "Vasilisa the Beautiful"). Russian poets, including Pyotr Yershov and Leonid Filatov, made a number of well-known poetical interpretations of the classical fairy tales, and in some cases, like that of Alexander Pushkin, also created fully original fairy tale poems of great popularity.

Since the Christianization of Kievan Rus' for several ages Russian architecture was influenced predominantly by the Byzantine architecture. Apart from fortifications (kremlins), the main stone buildings of ancient Rus' were Orthodox churches with their many domes, often gilded or brightly painted.

Aristotle Fioravanti and other Italian architects brought Renaissance trends into Russia since the late 15th century, while the 16th century saw the development of unique tent-like churches culminating in Saint Basil's Cathedral. By that time the onion dome design was also fully developed. In the 17th century, the "fiery style" of ornamentation flourished in Moscow and Yaroslavl, gradually paving the way for the Naryshkin baroque of the 1690s. After the reforms of Peter the Great the change of architectural styles in Russia generally followed that in the Western Europe.

The 18th-century taste for rococo architecture led to the ornate works of Bartolomeo Rastrelli and his followers. The reigns of Catherine the Great and her grandson Alexander I saw the flourishing of Neoclassical architecture, most notably in the capital city of Saint Petersburg. The second half of the 19th century was dominated by the Neo-Byzantine and Russian Revival styles. Prevalent styles of the 20th century were the Art Nouveau, Constructivism, and the Stalin Empire style.

With the change in values imposed by communist ideology, the tradition of preservation was broken. Independent preservation societies, even those that defended only secular landmarks such as Moscow-based OIRU were disbanded by the end of the 1920s. A new anti-religious campaign, launched in 1929, coincided with collectivization of peasants; destruction of churches in the cities peaked around 1932. A number of churches were demolished, including the Cathedral of Christ the Saviour in Moscow. In Moscow alone losses of 1917–2006 are estimated at over 640 notable buildings (including 150 to 200 listed buildings, out of a total inventory of 3,500) – some disappeared completely, others were replaced with concrete replicas.

In 1955, a new Soviet leader, Nikita Khrushchev, condemned the "excesses" of the former academic architecture, and the late Soviet era was dominated by plain functionalism in architecture. This helped somewhat to resolve the housing problem, but created a large quantity of buildings of low architectural quality, much in contrast with the previous bright styles. In 1959 Nikita Khrushchev launched his anti-religious campaign. By 1964 over 10 thousand churches out of 20 thousand were shut down (mostly in rural areas) and many were demolished. Of 58 monasteries and convents operating in 1959, only sixteen remained by 1964; of Moscow's fifty churches operating in 1959, thirty were closed and six demolished.

Early Russian painting is represented in icons and vibrant frescos, the two genres inherited from Byzantium. As Moscow rose to power, Theophanes the Greek, Dionisius and Andrei Rublev became vital names associated with a distinctly Russian art.

The Russian Academy of Arts was created in 1757 and gave Russian artists an international role and status. Ivan Argunov, Dmitry Levitzky, Vladimir Borovikovsky and other 18th-century academicians mostly focused on portrait painting. In the early 19th century, when neoclassicism and romantism flourished, mythological and Biblical themes inspired many prominent paintings, notably by Karl Briullov and Alexander Ivanov.

In the mid-19th century the "Peredvizhniki" ("Wanderers") group of artists broke with the Academy and initiated a school of art liberated from academic restrictions. These were mostly realist painters who captured Russian identity in landscapes of wide rivers, forests, and birch clearings, as well as vigorous genre scenes and robust portraits of their contemporaries. Some artists focused on depicting dramatic moments in Russian history, while others turned to social criticism, showing the conditions of the poor and caricaturing authority; critical realism flourished under the reign of Alexander II. Leading realists include Ivan Shishkin, Arkhip Kuindzhi, Ivan Kramskoi, Vasily Polenov, Isaac Levitan, Vasily Surikov, Viktor Vasnetsov, Ilya Repin, and Boris Kustodiev.

The turn of the 20th century saw the rise of symbolist painting, represented by Mikhail Vrubel, Kuzma Petrov-Vodkin, and Nicholas Roerich.

The Russian avant-garde was a large, influential wave of modernist art that flourished in Russia from approximately 1890 to 1930. The term covers many separate, but inextricably related art movements that occurred at the time, namely neo-primitivism, suprematism, constructivism, rayonism, and Russian Futurism. Notable artists from this era include El Lissitzky, Kazimir Malevich, Wassily Kandinsky, and Marc Chagall. Since the 1930s the revolutionary ideas of the avant-garde clashed with the newly emerged conservative direction of socialist realism.

Soviet art produced works that were furiously patriotic and anti-fascist during and after the Great Patriotic War. Multiple war memorials, marked by a great restrained solemnity, were built throughout the country. Soviet artists often combined innovation with socialist realism, notably the sculptors Vera Mukhina, Yevgeny Vuchetich and Ernst Neizvestny.

Music in 19th-century Russia was defined by the tension between classical composer Mikhail Glinka along with other members of The Mighty Handful, who embraced Russian national identity and added religious and folk elements to their compositions, and the Russian Musical Society led by composers Anton and Nikolay Rubinsteins, which was musically conservative. The later tradition of Pyotr Ilyich Tchaikovsky, one of the greatest composers of the Romantic era, was continued into the 20th century by Sergei Rachmaninoff. World-renowned composers of the 20th century include Alexander Scriabin, Igor Stravinsky, Sergei Prokofiev, Dmitri Shostakovich and Alfred Schnittke.

Russian conservatories have turned out generations of famous soloists. Among the best known are violinists Jascha Heifetz, David Oistrakh, Leonid Kogan, Gidon Kremer, and Maxim Vengerov; cellists Mstislav Rostropovich, Natalia Gutman; pianists Vladimir Horowitz, Sviatoslav Richter, Emil Gilels, Vladimir Sofronitsky and Evgeny Kissin; and vocalists Fyodor Shalyapin, Mark Reizen, Elena Obraztsova, Tamara Sinyavskaya, Nina Dorliak, Galina Vishnevskaya, Anna Netrebko and Dmitry Hvorostovsky.

During the early 20th century, Russian ballet dancers Anna Pavlova and Vaslav Nijinsky rose to fame, and impresario Sergei Diaghilev and his Ballets Russes' travels abroad profoundly influenced the development of dance worldwide. Soviet ballet preserved the perfected 19th-century traditions, and the Soviet Union's choreography schools produced many internationally famous stars, including Galina Ulanova, Maya Plisetskaya, Rudolf Nureyev, and Mikhail Baryshnikov. The Bolshoi Ballet in Moscow and the Mariinsky Ballet in St Petersburg remain famous throughout the world.

Modern Russian rock music takes its roots both in the Western rock and roll and heavy metal, and in traditions of the Russian bards of the Soviet era, such as Vladimir Vysotsky and Bulat Okudzhava. Popular Russian rock groups include Mashina Vremeni, DDT, Aquarium, Alisa, Kino, Kipelov, Nautilus Pompilius, Aria, Grazhdanskaya Oborona, Splean, and Korol i Shut. Russian pop music developed from what was known in the Soviet times as "estrada" into full-fledged industry, with some performers gaining wide international recognition, such as t.A.T.u., Nu Virgos and Vitas.

In the 18th century, during the era of Russian Enlightenment, the development of Russian literature was boosted by the works of Mikhail Lomonosov and Denis Fonvizin. By the early 19th century a modern national tradition had emerged, producing some of the greatest writers in Russian history. This period, known also as the Golden Age of Russian Poetry, began with Alexander Pushkin, who is considered the founder of the modern Russian literary language and often described as the "Russian Shakespeare". It continued with the poetry of Mikhail Lermontov and Nikolay Nekrasov, dramas of Alexander Ostrovsky and Anton Chekhov, and the prose of Nikolai Gogol and Ivan Turgenev. Leo Tolstoy and Fyodor Dostoyevsky have been described by literary critics as the greatest novelists of all time.

By the 1880s, the age of the great novelists was over, and short fiction and poetry became the dominant genres. The next several decades became known as the Silver Age of Russian Poetry, when the previously dominant literary realism was replaced by symbolism. Leading authors of this era include such poets as Valery Bryusov, Vyacheslav Ivanov, Alexander Blok, Nikolay Gumilev and Anna Akhmatova, and novelists Leonid Andreyev, Ivan Bunin, and Maxim Gorky.

Russian philosophy blossomed in the 19th century, when it was defined initially by the opposition of Westernisers, who advocated Western political and economical models, and Slavophiles, who insisted on developing Russia as a unique civilization. The latter group includes Nikolai Danilevsky and Konstantin Leontiev, the founders of eurasianism. In its further development Russian philosophy was always marked by a deep connection to literature and interest in creativity, society, politics and nationalism; Russian cosmism and religious philosophy were other major areas. Notable philosophers of the late 19th and the early 20th centuries include Vladimir Solovyev, Sergei Bulgakov, and Vladimir Vernadsky.

Following the Russian Revolution of 1917 many prominent writers and philosophers left the country, including Bunin, Vladimir Nabokov and Nikolay Berdyayev, while a new generation of talented authors joined together in an effort to create a distinctive working-class culture appropriate for the new Soviet state. In the 1930s censorship over literature was tightened in line with the policy of socialist realism. In the late 1950s restrictions on literature were eased, and by the 1970s and 1980s, writers were increasingly ignoring official guidelines. Leading authors of the Soviet era include novelists Yevgeny Zamyatin (emigrated), Ilf and Petrov, Mikhail Bulgakov (censored) and Mikhail Sholokhov, and poets Vladimir Mayakovsky, Yevgeny Yevtushenko, and Andrey Voznesensky.

The Soviet Union was also a major producer of science fiction, written by authors like Arkady and Boris Strugatsky, Kir Bulychov, Alexander Belayev and Ivan Yefremov. Traditions of Russian science fiction and fantasy are continued today by numerous writers.

Russian and later Soviet cinema was a hotbed of invention in the period immediately following 1917, resulting in world-renowned films such as "The Battleship Potemkin" by Sergei Eisenstein. Eisenstein was a student of filmmaker and theorist Lev Kuleshov, who developed the Soviet montage theory of film editing at the world's first film school, the All-Union Institute of Cinematography. Dziga Vertov, whose "kino-glaz" ("film-eye") theory—that the camera, like the human eye, is best used to explore real life—had a huge impact on the development of documentary film making and cinema realism. The subsequent state policy of socialist realism somewhat limited creativity; however, many Soviet films in this style were artistically successful, including "Chapaev", "The Cranes Are Flying", and "Ballad of a Soldier".

The 1960s and 1970s saw a greater variety of artistic styles in Soviet cinema. Eldar Ryazanov's and Leonid Gaidai's comedies of that time were immensely popular, with many of the catch phrases still in use today. In 1961–68 Sergey Bondarchuk directed an Oscar-winning film adaptation of Leo Tolstoy's epic "War and Peace", which was the most expensive film made in the Soviet Union. In 1969, Vladimir Motyl's "White Sun of the Desert" was released, a very popular film in a genre of ostern; the film is traditionally watched by cosmonauts before any trip into space.

Russian animation dates back to late Russian Empire times. During the Soviet era, Soyuzmultfilm studio was the largest animation producer. Soviet animators developed a great variety of pioneering techniques and aesthetic styles, with prominent directors including Ivan Ivanov-Vano, Fyodor Khitruk and Aleksandr Tatarsky. Many Soviet cartoon heroes such as the Russian-style Winnie-the-Pooh, cute little Cheburashka, Wolf and Hare from "Nu, Pogodi!", are iconic images in Russia and many surrounding countries.

The late 1980s and 1990s were a period of crisis in Russian cinema and animation. Although Russian filmmakers became free to express themselves, state subsidies were drastically reduced, resulting in fewer films produced. The early years of the 21st century have brought increased viewership and subsequent prosperity to the industry on the back of the economic revival. Production levels are already higher than in Britain and Germany. Russia's total box-office revenue in 2007 was $565 million, up 37% from the previous year. In 2002 the "Russian Ark" became the first feature film ever to be shot in a single take. The traditions of Soviet animation were developed recently by such directors as Aleksandr Petrov and studios like Melnitsa Animation.

While there were few stations or channels in the Soviet time, in the past two decades many new state and privately owned radio stations and TV channels have appeared. In 2005 a state-run English language Russia Today TV started broadcasting, and its Arabic version Rusiya Al-Yaum was launched in 2007. Censorship and Media freedom in Russia has always been a main theme of Russian media.

Soviet and later Russian athletes have always been in the top four for the number of gold medals collected at the Summer Olympics. Soviet gymnasts, track-and-field athletes, weightlifters, wrestlers, boxers, fencers, shooters, cross country skiers, biathletes, speed skaters and figure skaters were consistently among the best in the world, along with Soviet basketball, handball, futsal, volleyball and ice hockey players. The 1980 Summer Olympics were held in Moscow while the 2014 Winter Olympics were hosted in Sochi.

Although ice hockey was only introduced during the Soviet era, the Soviet Union national team managed to win gold at almost all the Olympics and World Championships they contested. Russian players Valery Kharlamov, Sergei Makarov, Vyacheslav Fetisov and Vladislav Tretiak hold four of six positions in the IIHF "Team of the Century". Russia has not won the Olympic ice hockey tournament since the Unified Team won gold in 1992. Russia won the 1993, 2008, 2009, 2012 and the 2014 IIHF World Championships.

The Kontinental Hockey League (KHL) was founded in 2008 as a successor to the Russian Superleague. It is ranked the top hockey league in Europe , and the second-best in the world. It is an international professional ice hockey league in Eurasia and consists of 29 teams, of which 21 are based in Russia and 7 more are located in Latvia, Kazakhstan, Belarus, Finland, Slovakia, Croatia and China. KHL is on the 4th place by attendance in Europe.

Bandy, also known as Russian hockey, is another traditionally popular ice sport. The Soviet Union won all the Bandy World Championships for men between 1957–79 and some thereafter too. After the dissolution of the Soviet Union, Russia has continuously been one of the most successful teams, winning many world championships.
Association football is one of the most popular sports in modern Russia. The Soviet national team became the first European Champions by winning Euro 1960. Appearing in four FIFA World Cups from 1958 to 1970, Lev Yashin is regarded as one of the greatest goalkeepers in the history of football, and was chosen on the FIFA World Cup Dream Team. The Soviet national team reached the finals of Euro 1988. In 1956 and 1988, the Soviet Union won gold at the Olympic football tournament. Russian clubs CSKA Moscow and Zenit St Petersburg won the UEFA Cup in 2005 and 2008. The Russian national football team reached the semi-finals of Euro 2008, losing only to the eventual champions Spain. Russia was the host nation for the 2018 FIFA World Cup. The matches were held from 14 June to 15 July 2018 in the stadiums of 11 host cities located in the European part of the country and in the Ural region. This was the first football World Cup ever held in Eastern Europe, and the first held in Europe since 2006. Russia will also host games of Euro 2020.
In 2007, the Russian national basketball team won the European Basketball Championship. The Russian basketball club PBC CSKA Moscow is one of the top teams in Europe, winning the Euroleague in 2006 and 2008.

Larisa Latynina, who currently holds the record for the most gold Olympic medals won by a woman, established the USSR as the dominant force in gymnastics for many years. Today, Russia is the leading nation in rhythmic gymnastics with Yevgeniya Kanayeva. Double 50 m and 100 m freestyle Olympic gold medalist Alexander Popov is widely considered the greatest sprint swimmer in history. Russian synchronised swimming is the best in the world, with almost all gold medals at Olympics and World Championships having been swept by Russians in recent decades. Figure skating is another popular sport in Russia, especially pair skating and ice dancing. With the exception of 2010 and 2018 a Soviet or Russian pair has won gold at every Winter Olympics since 1964.

Since the end of the Soviet era, tennis has grown in popularity and Russia has produced a number of famous players, including Maria Sharapova. In martial arts, Russia produced the sport Sambo and renowned fighters, like Fedor Emelianenko. Chess is a widely popular pastime in Russia; from 1927, Russian grandmasters have held the world chess championship almost continuously.

The 2014 Winter Olympics were held in Sochi in the south of Russia. In 2016 the McLaren Report found evidence of widespread state-sponsored doping and an institutional conspiracy to cover up Russian competitors' positive drug tests. 25 athletes are disqualified, 11 medals are stripped.

Formula One is also becoming increasingly popular in Russia. In 2010 Vitaly Petrov of Vyborg became the first Russian to drive in Formula One, and was soon followed by a second – Daniil Kvyat, from Ufa – in 2014. There had only been two Russian Grands Prix (in 1913 and 1914), but the Russian Grand Prix returned as part of the Formula One season in 2014, as part of a six-year deal.
Russia has the most Olympic medals stripped for doping violations (51), the most of any country, four times the number of the runner-up, and more than a third of the global total, and 129 athletes caught doping at the Olympics, also the most of any country. From 2011 to 2015, more than a thousand Russian competitors in various sports, including summer, winter, and Paralympic sports, benefited from a state-sponsored cover-up, with no indication that the program has ceased since then.

There are seven public holidays in Russia, except those always celebrated on Sunday. Russian New Year traditions resemble those of the Western Christmas, with New Year Trees and gifts, and Ded Moroz (Father Frost) playing the same role as Santa Claus. Orthodox Christmas falls on 7 January, because the Russian Orthodox Church still follows the Julian calendar, and all Orthodox holidays are 13 days after Western ones. Two other major Christian holidays are Easter and Trinity Sunday. Kurban Bayram and Uraza Bayram are celebrated by Russian Muslims.

Further Russian public holidays include Defender of the Fatherland Day (23 February), which honors Russian men, especially those serving in the army; International Women's Day (8 March), which combines the traditions of Mother's Day and Valentine's Day; Spring and Labor Day (1 May); Victory Day (9 May); Russia Day (12 June); and Unity Day (4 November), commemorating the popular uprising which expelled the Polish occupation force from Moscow in 1612.

Victory Day is the second most popular holiday in Russia; it commemorates the victory over Nazi Germany and its allies in the Great Patriotic War. A huge military parade, hosted by the President of Russia, is annually organised in Moscow on Red Square. Similar parades take place in all major Russian cities and cities with the status "Hero city" or "City of Military Glory".

Popular non-public holidays include Old New Year (the New Year according to the Julian Calendar on 14 January), Tatiana Day (students holiday on 25 January), Maslenitsa (a pre-Christian spring holiday a week before the Great Lent), Cosmonautics Day (in tribute to the first human trip into space), Ivan Kupala Day (another pre-Christian holiday on 7 July) and Peter and Fevronia Day (which takes place on 8 July and is the Russian analogue of Valentine's Day, focusing, however, on family love and fidelity).
State symbols of Russia include the Byzantine double-headed eagle, combined with St. George of Moscow in the Russian coat of arms. The Russian flag dates from the late Tsardom of Russia period and has been widely used since the time of the Russian Empire. The Russian anthem shares its music with the Soviet Anthem, though not the lyrics. The imperial motto "God is with us" and the Soviet motto "Proletarians of all countries, unite!" are now obsolete and no new motto has replaced them. The hammer and sickle and the full Soviet coat of arms are still widely seen in Russian cities as a part of old architectural decorations. The Soviet Red Stars are also encountered, often on military equipment and war memorials. The Red Banner continues to be honored, especially the Banner of Victory of 1945.

The Matryoshka doll is a recognizable symbol of Russia, and the towers of Moscow Kremlin and Saint Basil's Cathedral in Moscow are Russia's main architectural icons. Cheburashka is a mascot of the Russian national Olympic team. St. Mary, St. Nicholas, St. Andrew, St. George, St. Alexander Nevsky, St. Sergius of Radonezh and St. Seraphim of Sarov are Russia's patron saints. Chamomile is the national flower, while birch is the national tree. The Russian bear is an animal symbol and Mother Russia a national personification of Russia, though the bear image has a Western origin and Russians themselves have accepted it only fairly recently.

Tourism in Russia has seen rapid growth since the late Soviet period, first domestic tourism and then international tourism, fueled by the rich cultural heritage and great natural variety of the country. Major tourist routes in Russia include a journey around the Golden Ring theme route of ancient cities, cruises on the big rivers like the Volga, and long journeys on the famous Trans-Siberian Railway. In 2013, Russia was visited by 28.4 million tourists; it is the ninth-most visited country in the world and the seventh-most visited in Europe. The number of Western visitors dropped in 2014.
The most visited destinations in Russia are Moscow and Saint Petersburg, the current and former capitals of the country. Recognised as World Cities, they feature such world-renowned museums as the Tretyakov Gallery and the Hermitage, famous theaters like Bolshoi and Mariinsky, ornate churches like Saint Basil's Cathedral, Cathedral of Christ the Saviour, Saint Isaac's Cathedral and Church of the Savior on Blood, impressive fortifications like the Kremlin and Peter and Paul Fortress, beautiful squares and streets like Red Square, Palace Square, Tverskaya Street, Nevsky Prospect, and Arbat Street. Rich palaces and parks are found in the former imperial residences in suburbs of Moscow (Kolomenskoye, Tsaritsyno) and St Petersburg (Peterhof, Strelna, Oranienbaum, Gatchina, Pavlovsk and Tsarskoye Selo). Moscow displays Soviet architecture at its best, along with modern skyscrapers, while St Petersburg, nicknamed "Venice of the North", boasts of its classical architecture, many rivers, canals and bridges.

Kazan, the capital of Tatarstan, shows a mix of Christian Russian and Muslim Tatar cultures. The city has registered a brand "The Third Capital of Russia", though a number of other major cities compete for this status, including Novosibirsk, Yekaterinburg and Nizhny Novgorod.

The warm subtropical Black Sea coast of Russia is the site for a number of popular sea resorts, like Sochi, the follow-up host of the 2014 Winter Olympics. The mountains of the Northern Caucasus contain popular ski resorts such as Dombay. The most famous natural destination in Russia is Lake Baikal, "the Blue Eye of Siberia". This unique lake, the oldest and deepest in the world, has crystal-clear waters and is surrounded by taiga-covered mountains. Other popular natural destinations include Kamchatka with its volcanoes and geysers, Karelia with its lakes and granite rocks, the snowy Altai Mountains, and the wild steppes of Tuva.





</doc>
<doc id="25400" url="https://en.wikipedia.org/wiki?curid=25400" title="Rational choice theory">
Rational choice theory

Rational choice theory, also known as choice theory or rational action theory, is a framework for understanding and often formally modeling social and economic behavior. The basic premise of rational choice theory is that aggregate social behavior results from the behavior of individual actors, each of whom is making their individual decisions. The theory also focuses on the determinants of the individual choices (methodological individualism). Rational choice theory then assumes that an individual has preferences among the available choice alternatives that allow them to state which option they prefer. These preferences are assumed to be complete (the person can always say which of two alternatives they consider preferable or that neither is preferred to the other) and transitive (if option A is preferred over option B and option B is preferred over option C, then A is preferred over C). The rational agent is assumed to take account of available information, probabilities of events, and potential costs and benefits in determining preferences, and to act consistently in choosing the self-determined best choice of action. In simpler terms, this theory dictates that every person, even when carrying out the most mundane of tasks, perform their own personal cost and benefit analysis in order to determine whether the action is worth pursuing for the best possible outcome. And following this, a person will choose the optimum venture in every case. This could culminate in a student deciding on whether to attend a lecture or stay in bed, a shopper deciding to provide their own bag to avoid the five pence charge or even a voter deciding which candidate or party based on who will fulfill their needs the best on issues that have an impact on themselves especially.

Rationality is widely used as an assumption of the behavior of individuals in microeconomic models and analyses and appears in almost all economics textbook treatments of human decision-making. It is also used in political science, sociology, and philosophy. Gary Becker was an early proponent of applying rational actor models more widely. Becker won the 1992 Nobel Memorial Prize in Economic Sciences for his studies of discrimination, crime, and human capital.

A particular version of rationality is instrumental rationality, which involves seeking the most cost-effective means to achieve a specific goal without reflecting on the worthiness of that goal.

Rational choice theorists do not claim that the theory describes the choice "process", but rather that it predicts the outcome and pattern of choices. 
An assumption often added to the rational choice paradigm is that individual preferences are self-interested, in which case the individual can be referred to as a homo economicus. Such an individual acts "as if" balancing costs against benefits to arrive at action that maximizes personal advantage. Proponents of such models, particularly those associated with the Chicago school of economics, do not claim that a model's assumptions are an accurate description of reality, only that they help formulate clear and falsifiable hypotheses. In this view, the only way to judge the success of a hypothesis is empirical tests. To use an example from Milton Friedman, if a theory that says that the behavior of the leaves of a tree is explained by their rationality passes the empirical test, it is seen as successful.

Without specifying the individual's goal or preferences it may not be possible to empirically test, or falsify, the rationality assumption. However, the predictions made by a specific version of the theory are testable. In recent years, the most prevalent version of rational choice theory, expected utility theory, has been challenged by the experimental results of behavioral economics. Economists are learning from other fields, such as psychology, and are enriching their theories of choice in order to get a more accurate view of human decision-making. For example, the behavioral economist and experimental psychologist Daniel Kahneman won the Nobel Memorial Prize in Economic Sciences in 2002 for his work in this field.

Rational choice theory has become increasingly employed in social sciences other than economics, such as sociology, evolutionary theory and political science in recent decades. It has had far-reaching impacts on the study of political science, especially in fields like the study of interest groups, elections, behaviour in legislatures, coalitions, and bureaucracy. In these fields, the use of the rational choice paradigm to explain broad social phenomena is the subject of controversy.

The concept of rationality used in rational choice theory is different from the colloquial and most philosophical use of the word. Colloquially, "rational" behaviour typically means "sensible", "predictable", or "in a thoughtful, clear-headed manner." Rational choice theory uses a narrower definition of rationality. At its most basic level, behavior is rational if it is goal-oriented, reflective (evaluative), and consistent (across time and different choice situations). This contrasts with behavior that is random, impulsive, conditioned, or adopted by (unevaluative) imitation.

Early neoclassical economists writing about rational choice, including William Stanley Jevons, assumed that agents make consumption choices so as to maximize their happiness, or utility. Contemporary theory bases rational choice on a set of choice axioms that need to be satisfied, and typically does not specify where the goal (preferences, desires) comes from. It mandates just a consistent ranking of the alternatives. Individuals choose the best action according to their personal preferences and the constraints facing them. E.g., there is nothing irrational in preferring fish to meat the first time, but there is something irrational in preferring fish to meat in one instant and preferring meat to fish in another, without anything else having changed.

The premise of rational choice theory as a social science methodology is that the aggregate behavior in society reflects the sum of the choices made by individuals. Each individual, in turn, makes their choice based on their own preferences and the constraints (or choice set) they face.

At the individual level, rational choice theory stipulates that the agent chooses the action (or outcome) they most prefer. In the case where actions (or outcomes) can be evaluated in terms of costs and benefits, a rational individual chooses the action (or outcome) that provides the maximum net benefit, i.e., the maximum benefit minus cost.

The theory applies to more general settings than those identified by costs and benefit. In general, rational decision making entails choosing among all available alternatives the alternative that the individual most prefers. The "alternatives" can be a set of actions ("what to do?") or a set of objects ("what to choose/buy"). In the case of actions, what the individual really cares about are the outcomes that results from each possible action. Actions, in this case, are only an instrument for obtaining a particular outcome.

The available alternatives are often expressed as a set of objects, for example a set of "j" exhaustive and exclusive actions:
For example, if a person can choose to vote for either Roger or Sara or to abstain, their set of possible alternatives is:

The theory makes two technical assumptions about individuals' preferences over alternatives:

Together these two assumptions imply that given a set of exhaustive and exclusive actions to choose from, an individual can "rank" the elements of this set in terms of his preferences in an internally consistent way (the ranking constitutes a partial ordering), and the set has at least one maximal element.

The preference between two alternatives can be:

Research that took off in the 1980s sought to develop models which drop these assumptions and argue that such behaviour could still be rational, Anand (1993). This work, often conducted by economic theorists and analytical philosophers, suggests ultimately that the assumptions or axioms above are not completely general and might at best be regarded as approximations.


Alternative theories of human action include such components as Amos Tversky and Daniel Kahneman's prospect theory, which reflects the empirical finding that, contrary to standard preferences assumed under neoclassical economics, individuals attach extra value to items that they already own compared to similar items owned by others. Under standard preferences, the amount that an individual is willing to pay for an item (such as a drinking mug) is assumed to equal the amount he or she is willing to be paid in order to part with it. In experiments, the latter price is sometimes significantly higher than the former (but see Plott and Zeiler 2005, Plott and Zeiler 2007 and Klass and Zeiler, 2013). Tversky and Kahneman do not characterize loss aversion as irrational. Behavioral economics includes a large number of other amendments to its picture of human behavior that go against neoclassical assumptions.

Often preferences are described by their utility function or "payoff function". This is an ordinal number that an individual assigns over the available actions, such as:
The individual's preferences are then expressed as the relation between these ordinal assignments. For example, if an individual prefers the candidate Sara over Roger over abstaining, their preferences would have the relation:
A preference relation that as above satisfies completeness, transitivity, and, in addition, continuity, can be equivalently represented by a utility function.

Both the assumptions and the behavioral predictions of rational choice theory have sparked criticism from various camps. As mentioned above, some economists have developed models of bounded rationality, which hope to be more psychologically plausible without completely abandoning the idea that reason underlies decision-making processes. Other economists have developed more theories of human decision-making that allow for the roles of uncertainty, institutions, and determination of individual tastes by their socioeconomic environment (cf. Fernandez-Huerga, 2008).

Martin Hollis and Edward J. Nell's 1975 book offers both a philosophical critique of neo-classical economics and an innovation in the field of economic methodology. Further they outlined an alternative vision to neo-classicism based on a rationalist theory of knowledge. Within neo-classicism, the authors addressed consumer behaviour (in the form of indifference curves and simple versions of revealed preference theory) and marginalist producer behaviour in both product and factor markets. Both are based on rational optimizing behaviour. They consider imperfect as well as perfect markets since neo-classical thinking embraces many market varieties and disposes of a whole system for their classification. However, the authors believe that the issues arising from basic maximizing models have extensive implications for econometric methodology (Hollis and Nell, 1975, p. 2). In particular it is this class of models – rational behavior as maximizing behaviour – which provide support for specification and identification. And this, they argue, is where the flaw is to be found. Hollis and Nell (1975) argued that positivism (broadly conceived) has provided neo-classicism with important support, which they then show to be unfounded. They base their critique of neo-classicism not only on their critique of positivism but also on the alternative they propose, rationalism. Indeed, they argue that rationality is central to neo-classical economics – as rational choice – and that this conception of rationality is misused. Demands are made of it that it cannot fulfill.

In their 1994 work, "Pathologies of Rational Choice Theory", Donald P. Green and Ian Shapiro argue that the empirical outputs of rational choice theory have been limited. They contend that much of the applicable literature, at least in political science, was done with weak statistical methods and that when corrected many of the empirical outcomes no longer hold. When taken in this perspective, rational choice theory has provided very little to the overall understanding of political interaction - and is an amount certainly disproportionately weak relative to its appearance in the literature. Yet, they concede that cutting edge research, by scholars well-versed in the general scholarship of their fields (such as work on the U.S. Congress by Keith Krehbiel, Gary Cox, and Mat McCubbins) has generated valuable scientific progress.

Duncan K. Foley (2003, p. 1) has also provided an important criticism of the concept of "rationality" and its role in economics. He argued that“Rationality” has played a central role in shaping and establishing the hegemony of contemporary mainstream economics. As the specific claims of robust neoclassicism fade into the history of economic thought, an orientation toward situating explanations of economic phenomena in relation to rationality has increasingly become the touchstone by which mainstream economists identify themselves and recognize each other. This is not so much a question of adherence to any particular conception of rationality, but of taking rationality of individual behavior as the unquestioned starting point of economic analysis.

Foley (2003, p. 9) went on to argue thatThe concept of rationality, to use Hegelian language, represents the relations of modern capitalist society one-sidedly. The burden of rational-actor theory is the assertion that ‘naturally’ constituted individuals facing existential conflicts over scarce resources would rationally impose on themselves the institutional structures of modern capitalist society, or something approximating them. But this way of looking at matters systematically neglects the ways in which modern capitalist society and its social relations in fact constitute the ‘rational’, calculating individual. The well-known limitations of rational-actor theory, its static quality, its logical antinomies, its vulnerability to arguments of infinite regress, its failure to develop a progressive concrete research program, can all be traced to this starting-point.

Schram and Caterino (2006) contains a fundamental methodological criticism of rational choice theory for promoting the view that the natural science model is the only appropriate methodology in social science and that political science should follow this model, with its emphasis on quantification and mathematization. Schram and Caterino argue instead for methodological pluralism. The same argument is made by William E. Connolly, who in his work Neuropolitics shows that advances in neuroscience further illuminate some of the problematic practices of rational choice theory.

More recently Edward J. Nell and Karim Errouaki (2011, Ch. 1) argued that:The DNA of neoclassical economics is defective. Neither the induction problem nor the problems of methodological individualism can be solved within the framework of neoclassical assumptions. The neoclassical approach is to call on rational economic man to solve both. Economic relationships that reflect rational choice should be ‘projectible’. But that attributes a deductive power to ‘rational’ that it cannot have consistently with positivist (or even pragmatist) assumptions (which require deductions to be simply analytic). To make rational calculations projectible, the agents may be assumed to have idealized abilities, especially foresight; but then the induction problem is out of reach because the agents of the world do not resemble those of the model. The agents of the model can be abstract, but they cannot be endowed with powers actual agents could not have. This also undermines methodological individualism; if behaviour cannot be reliably predicted on the basis of the ‘rational choices of agents’, a social order cannot reliably follow from the choices of agents.

Furthermore, Pierre Bourdieu fiercely opposed rational choice theory as grounded in a misunderstanding of how social agents operate. Bourdieu argued that social agents do not continuously calculate according to explicit rational and economic criteria. According to Bourdieu, social agents operate according to an implicit practical logic—a practical sense—and bodily dispositions. Social agents act according to their "feel for the game" (the "feel" being, roughly, habitus, and the "game" being the field).

Other social scientists, inspired in part by Bourdieu's thinking have expressed concern about the inappropriate use of economic metaphors in other contexts, suggesting that this may have political implications. The argument they make is that by treating everything as a kind of "economy" they make a particular vision of the way an economy works seem more natural. Thus, they suggest, rational choice is as much ideological as it is scientific, which does not in and of itself negate its scientific utility.

An evolutionary psychology perspective is that many of the seeming contradictions and biases regarding rational choice can be explained as being rational in the context of maximizing biological fitness in the ancestral environment but not necessarily in the current one. Thus, when living at subsistence level where a reduction of resources may have meant death it may have been rational to place a greater value on losses than on gains. Proponents argue it may also explain differences between groups.

The rational choice approach allows preferences to be represented as real-valued utility functions. Economic decision making then becomes a problem of maximizing this utility function, subject to constraints (e.g. a budget). This has many advantages. It provides a compact theory that makes empirical predictions with a relatively sparse model - just a description of the agent's objectives and constraints. Furthermore, optimization theory is a well-developed field of mathematics. These two factors make rational choice models tractable compared to other approaches to choice. Most importantly, this approach is strikingly general. It has been used to analyze not only personal and household choices about
traditional economic matters like consumption and savings, but also choices about education, marriage, child-bearing, migration, crime and so on, as well as business decisions about output, investment, hiring, entry, exit, etc. with varying degrees of success.

Despite the empirical shortcomings of rational choice theory, the flexibility and tractability of rational choice models (and the lack of equally powerful alternatives) lead to them still being widely used.

The relationship between the rational choice theory and politics takes many forms, whether that be in voter behaviour, the actions of world leaders or even the way that important matters are dealt with.

Voter behaviour shifts significantly thanks to rational theory, which is ingrained in human nature, the most significant of which occurs when there are times of economic trouble. This was assessed in detail by Anthony Downs who concluded that voters were acting on thoughts of higher income as a person ‘votes for whatever party he believes would provide him with the highest utility income from government action’. This is a significant simplification of how the theory influences people's thoughts but makes up a core part of rational theory as a whole. In a more complex fashion, voters will react often radically in times of real economic strife, which can lead to an increase in extremism. The government will be made responsible by the voters and thus they see a need to make a change. Some of the most infamous extremist parties came to power on the back of economic recessions, the most significant being the far right Nazi Party in Germany, who used the hyperinflation at the time to gain power rapidly, as they promised a solution and a scapegoat for the blame. There is a trend to this, as a comprehensive study carried out by three political scientists concluded, as a ‘turn to the right’ occurs and it is clear that it is the work of the rational theory because within ten years the politics returns to a more common state.

The fear for many is that rational thinking does not allow for an efficient resolution to some of the most troubling world problems, such as the climate crisis. In this way, nationalism will not allow countries to work together and thus the criticisms of the theory should be noted very carefully.




</doc>
<doc id="25401" url="https://en.wikipedia.org/wiki?curid=25401" title="Romance languages">
Romance languages

The Romance languages (less commonly Latin languages, or Neo-Latin languages) are the modern languages that evolved from Vulgar Latin between the third and eighth centuries. They are a subgroup of the Italic languages in the Indo-European language family. The five most widely spoken Romance languages by number of native speakers are Spanish (480 million), Portuguese (255 million), French (77 million), Italian (65 million), and Romanian (24 million). Of those, Italian is the closest to Latin.

The more than 900 million native speakers of Romance languages are found worldwide, mainly in the Americas, Europe, and parts of Africa, as well as elsewhere. The major Romance languages also have many non-native speakers, and are in widespread use as lingua francas. This is especially true of French, which is in widespread use throughout Central and West Africa, Madagascar, Mauritius, Seychelles, Comoros, Djibouti, Lebanon, and North Africa (excluding Egypt, where it is a minority language).

Because it is difficult to assign rigid categories to phenomena such as languages which exist on a continuum, estimates of the number of modern Romance languages vary. For example, Dalby lists 23, based on the criterion of mutual intelligibility. The following includes those and additional current, living languages, and one extinct language, Dalmatian:


The term "Romance" comes from the Vulgar Latin adverb , "in Roman", derived from : for instance, in the expression , "to speak in Roman" (that is, the Latin vernacular), contrasted with , "to speak in Latin" (Medieval Latin, the conservative version of the language used in writing and formal contexts or as a lingua franca), and with , "to speak in Barbarian" (the non-Latin languages of the peoples living outside the Roman Empire). From this adverb the noun "romance" originated, which applied initially to anything written , or "in the Roman vernacular".

Lexical and grammatical similarities among the Romance languages, and between Latin and each of them, are apparent from the following examples having the same meaning in various Romance lects:

English: She always closes the window before she dines / before dining.

Romance-based creoles and pidgins

Some of the divergence comes from semantic change: where the same root words have developed different meanings. For example, the Portuguese word is descended from Latin "window" (and is thus cognate to French , Italian , Romanian and so on), but now means "skylight" and "slit". Cognates may exist but have become rare, such as in Spanish, or dropped out of use entirely. The Spanish and Portuguese terms meaning "to throw through a window" and meaning "replete with windows" also have the same root, but are later borrowings from Latin.

Likewise, Portuguese also has the word , a cognate of Italian and Spanish , but uses it in the sense of "to have a late supper" in most varieties, while the preferred word for "to dine" is (related to archaic Spanish "to eat") because of semantic changes in the 19th century. Galician has both (from medieval "fẽestra", the ancestor of standard Portuguese ) and the less frequently used and .

As an alternative to (originally the genitive form), Italian has the pronoun , a cognate of the other words for "she", but it is hardly ever used in speaking.

Spanish, Asturian, and Leonese and Mirandese and Sardinian come from Latin "wind" (cf. English "window", etymologically 'wind eye'), and Portuguese , Galician , Mirandese from Latin * "small opening", a derivative of "door".

Sardinian (alternative for /) comes from Old Italian and is similar to other Romance languages such as French (from Italian ), Portuguese , Romanian , Spanish , Catalan and Corsican (alternative for ).

The classification of the Romance languages is inherently difficult, because most of the linguistic area is a dialect continuum, and in some cases political biases can come into play. Along with Latin (which is not included among the Romance languages) and a few extinct languages of ancient Italy, they make up the Italic branch of the Indo-European family.

There are various schemes used to subdivide the Romance languages. Three of the most common schemes are as follows:

The main subfamilies that have been proposed by Ethnologue within the various classification schemes for Romance languages are:

This three-way division is made primarily based on the outcome of Vulgar Latin (Proto-Romance) vowels:
Italo-Western is in turn split along the so-called "La Spezia–Rimini Line" in northern Italy, which divides the central and southern Italian languages from the so-called Western Romance languages to the north and west. The primary characteristics dividing the two are:

The reality is somewhat more complex. All of the "southeast" characteristics apply to all languages southeast of the line, and all of the "northwest" characteristics apply to all languages in France and (most of) Spain. However, the Gallo-Italic languages are somewhere in between. All of these languages do have the "northwest" characteristics of lenition and loss of gemination. However:

On top of this, the ancient Mozarabic language in southern Spain, at the far end of the "northwest" group, had the "southeast" characteristics of lack of lenition and palatalization of /k/ to . Certain languages around the Pyrenees (e.g. some highland Aragonese dialects) also lack lenition, and northern French dialects such as Norman and Picard have palatalization of /k/ to (although this is possibly an independent, secondary development, since /k/ between vowels, i.e. when subject to lenition, developed to /dz/ rather than , as would be expected for a primary development).

The usual solution to these issues is to create various nested subgroups. Western Romance is split into the Gallo-Iberian languages, in which lenition happens and which include nearly all the Western Romance languages, and the Pyrenean-Mozarabic group, which includes the remaining languages without lenition (and is unlikely to be a valid clade; probably at least two clades, one for Mozarabic and one for Pyrenean). Gallo-Iberian is split in turn into the Iberian languages (e.g. Spanish and Portuguese), and the larger Gallo-Romance languages (stretching from eastern Spain to northeast Italy).

Probably a more accurate description, however, would be to say that there was a focal point of innovation located in central France, from which a series of innovations spread out as areal changes. The La Spezia–Rimini Line represents the farthest point to the southeast that these innovations reached, corresponding to the northern chain of the Apennine Mountains, which cuts straight across northern Italy and forms a major geographic barrier to further language spread.

This would explain why some of the "northwest" features (almost all of which can be characterized as innovations) end at differing points in northern Italy, and why some of the languages in geographically remote parts of Spain (in the south, and high in the Pyrenees) are lacking some of these features. It also explains why the languages in France (especially standard French) seem to have innovated earlier and more extensively than other Western Romance languages.

Many of the "southeast" features also apply to the Eastern Romance languages (particularly, Romanian), despite the geographic discontinuity. Examples are lack of lenition, maintenance of intertonic vowels, use of vowel-changing plurals, and palatalization of /k/ to . This has led some researchers to postulate a basic two-way East-West division, with the "Eastern" languages including Romanian and central and southern Italian, although this view is troubled by the contrast of numerous Romanian phonological developments with those found in Italy below the La Spezia-Rimini line. Among these features, in Romanian geminates reduced historically to single units — which may be an independent development or perhaps due to Slavic influence — and /kt/ developed into /pt/, whereas in central and southern Italy geminates are preserved and /kt/ underwent assimilation to /tt/.

Despite being the first Romance language to evolve from Vulgar Latin, Sardinian does not fit at all into this sort of division. It is clear that Sardinian became linguistically independent from the remainder of the Romance languages at an extremely early date, possibly already by the first century BC. Sardinian contains a large number of archaic features, including total lack of palatalization of /k/ and /g/ and a large amount of vocabulary preserved nowhere else, including some items already archaic by the time of Classical Latin (first century BC). Sardinian has plurals in /s/ but post-vocalic lenition of voiceless consonants is normally limited to the status of an allophonic rule (e.g. [k]"ane" 'dog' but "su" [g]"ane" or "su" [ɣ]"ane" 'the dog'), and there are a few innovations unseen elsewhere, such as a change of /au/ to /a/. Use of "su" < "ipsum" as an article is a retained archaic feature that also exists in the Catalan of the Balearic Islands and that used to be more widespread in Occitano-Romance, and is known as "" (literally the "salted article"), while Sardinian shares develarisation of earlier /kw/ and /gw/ with Romanian: Sard. "abba", Rum. "apă" 'water'; Sard. "limba", Rom. "limbă" 'language' (cf. Italian "acqua", "lingua").

Gallo-Romance can be divided into the following subgroups:
The following groups are also sometimes considered part of Gallo-Romance:

The Gallo-Romance languages are generally considered the most innovative (least conservative) among the Romance languages. Characteristic Gallo-Romance features generally developed earliest and appear in their most extreme manifestation in the Langue d'oïl, gradually spreading out along riverways and transalpine roads.

In some ways, however, the Gallo-Romance languages are conservative. The older stages of many of the languages preserved a two-case system consisting of nominative and oblique, fully marked on nouns, adjectives and determiners, inherited almost directly from the Latin nominative and accusative and preserving a number of different declensional classes and irregular forms. The languages closest to the oïl epicenter preserve the case system the best, while languages at the periphery lose it early.

Notable characteristics of the Gallo-Romance languages are:

Some Romance languages have developed varieties which seem dramatically restructured as to their grammars or to be mixtures with other languages. It is not always clear whether they should be classified as Romance, pidgins, creole languages, or mixed languages. Some other languages, such as Modern English, are sometimes thought of as creoles of semi-Romance ancestry. There are several dozens of creoles of French, Spanish, and Portuguese origin, some of them spoken as national languages in former European colonies.

Creoles of French:

Creoles of Spanish:

Creoles of Portuguese:

Latin and the Romance languages have also served as the inspiration and basis of numerous auxiliary and constructed languages, so-called "Neo-Romance languages".

The concept was first developed in 1903 by Italian mathematician Giuseppe Peano, under the title Latino sine flexione. He wanted to create a "naturalistic" international language, as opposed to an autonomous constructed language like Esperanto or Volapük which were designed for maximal simplicity of lexicon and derivation of words. Peano used Latin as the base of his language, because at the time of his flourishing it was the "de facto" international language of scientific communication.

Other languages developed since include Idiom Neutral, Interlingua and Lingua Franca Nova. The most famous and successful of these is Interlingua. Each of these languages has attempted to varying degrees to achieve a pseudo-Latin vocabulary as common as possible to living Romance languages. Some languages have been constructed specifically for communication among speakers of Romance languages, the Pan-Romance languages. 

There are also languages created for artistic purposes only, such as Talossan. Because Latin is a very well attested ancient language, some amateur linguists have even constructed Romance languages that mirror real languages that developed from other ancestral languages. These include Brithenig (which mirrors Welsh), Breathanach (mirrors Irish), Wenedyk (mirrors Polish), Þrjótrunn (mirrors Icelandic), and Helvetian (mirrors German).

The Romance language most widely spoken natively today is Spanish, followed by Portuguese, French, Italian and Romanian, which together cover a vast territory in Europe and beyond, and work as official and national languages in dozens of countries.

French, Italian, Portuguese, Spanish, and Romanian are also official languages of the European Union. Spanish, Portuguese, French, Italian, Romanian, and Catalan were the official languages of the defunct Latin Union; and French and Spanish are two of the six official languages of the United Nations. Outside Europe, French, Portuguese and Spanish are spoken and enjoy official status in various countries that emerged from the respective colonial empires.

Spanish is an official language in Spain and in nine countries of South America, home to about half that continent's population; in six countries of Central America (all except Belize); and in Mexico. In the Caribbean, it is official in Cuba, the Dominican Republic, and Puerto Rico. In all these countries, Latin American Spanish is the vernacular language of the majority of the population, giving Spanish the most native speakers of any Romance language. In Africa it is an official language of Equatorial Guinea.

Portuguese, in its original homeland, Portugal, is spoken by virtually the entire population of 10 million.
As the official language of Brazil, it is spoken by more than 200 million people in that country, as well as by neighboring residents of eastern Paraguay and northern Uruguay, accounting for a little more than half the population of South America, thus making Portuguese the most spoken official Romance language in a single country. It is the official language of six African countries (Angola, Cape Verde, Guinea-Bissau, Mozambique, Equatorial Guinea, and São Tomé and Príncipe), and is spoken as a first language by perhaps 30 million residents of that continent. In Asia, Portuguese is co-official with other languages in East Timor and Macau, while most Portuguese-speakers in Asia—some 400,000—are in Japan due to return immigration of Japanese Brazilians. In North America 1,000,000 people speak Portuguese as their home language.
In Oceania, Portuguese is the second most spoken Romance language, after French, due mainly to the number of speakers in East Timor. Its closest relative, Galician, has official status in the autonomous community of Galicia in Spain, together with Spanish.

Outside Europe, French is spoken natively most in the Canadian province of Quebec, and in parts of New Brunswick and Ontario. Canada is officially bilingual, with French and English being the official languages. In parts of the Caribbean, such as Haiti, French has official status, but most people speak creoles such as Haitian Creole as their native language. French also has official status in much of Africa, but relatively few native speakers. In France's overseas possessions, native use of French is increasing.

Although Italy also had some colonial possessions before World War II, its language did not remain official after the end of the colonial domination. As a result, Italian outside of Italy and Switzerland is now spoken only as a minority language by immigrant communities in North and South America and Australia. In some former Italian colonies in Africa—namely Libya, Eritrea and Somalia—it is spoken by a few educated people in commerce and government.

Romania did not establish a colonial empire, but beyond its native territory in southeastern Europe, the Romanian language is spoken as a minority language by autochthonous populations in Serbia, Bulgaria, and Hungary, and in some parts of the former Greater Romania (before 1945), as well as in Ukraine (Bukovina, Budjak) and in some villages between the Dniester and Bug rivers. The Aromanian language is spoken today by Aromanians in Bulgaria, Macedonia, Albania, Kosovo, and Greece. Romanian also spread to other countries on the Mediterranean (especially the other Romance-speaking countries, most notably Italy and Spain), and elsewhere such as Israel, where it is the native language of five percent of the population, and is spoken by many more as a secondary language. This is due to the large number of Romanian-born Jews who moved to Israel after World War II. And finally, some 2.6 million people in the former Soviet republic of Moldova speak a variety of Romanian, called variously Moldovan or Romanian by them.

The total native speakers of Romance languages are divided as follows (with their ranking within the languages of the world in brackets):

Catalan is the official language of Andorra. In Spain, it is co-official with Spanish in Catalonia, the Valencian Community, and the Balearic Islands, and it is recognized, but not official, in La Franja, and in Aragon. In addition, it is spoken by many residents of Alghero, on the island of Sardinia, and it is co-official in that city. Galician, with more than a million native speakers, is official together with Spanish in Galicia, and has legal recognition in neighbouring territories in Castilla y León. A few other languages have official recognition on a regional or otherwise limited level; for instance, Asturian and Aragonese in Spain; Mirandese in Portugal; Friulan, Sardinian and Franco-Provençal in Italy; and Romansh in Switzerland.

The remaining Romance languages survive mostly as spoken languages for informal contact. National governments have historically viewed linguistic diversity as an economic, administrative or military liability, as well as a potential source of separatist movements; therefore, they have generally fought to eliminate it, by extensively promoting the use of the official language, restricting the use of the other languages in the media, recognizing them as mere "dialects", or even persecuting them. As a result, all of these languages are considered endangered to varying degrees according to the UNESCO Red Book of Endangered Languages, ranging from "vulnerable" (e.g. Sicilian and Venetian) to "severely endangered" (Arpitan, most of the Occitan varieties). Since the late twentieth and early twenty-first centuries, increased sensitivity to the rights of minorities has allowed some of these languages to start recovering their prestige and lost rights. Yet it is unclear whether these political changes will be enough to reverse the decline of minority Romance languages.

Romance languages are the continuation of Vulgar Latin, the popular and colloquial sociolect of Latin spoken by soldiers, settlers, and merchants of the Roman Empire, as distinguished from the classical form of the language spoken by the Roman upper classes, the form in which the language was generally written. Between 350 BC and 150 AD, the expansion of the Empire, together with its administrative and educational policies, made Latin the dominant native language in continental Western Europe. Latin also exerted a strong influence in southeastern Britain, the Roman province of Africa, western Germany, Pannonia and the whole Balkans.

During the Empire's decline, and after its fragmentation and the collapse of Western half in the fifth and sixth centuries, the spoken varieties of Latin became more isolated from each other, with the western dialects coming under heavy Germanic influence (the Goths and Franks in particular) and the eastern dialects coming under Slavic influence. The dialects diverged from classical Latin at an accelerated rate and eventually evolved into a continuum of recognizably different typologies. The colonial empires established by Portugal, Spain, and France from the fifteenth century onward spread their languages to the other continents to such an extent that about two-thirds of all Romance language speakers today live outside Europe.

Despite other influences (e.g. "substratum" from pre-Roman languages, especially Continental Celtic languages; and "superstratum" from later Germanic or Slavic invasions), the phonology, morphology, and lexicon of all Romance languages consist mainly of evolved forms of Vulgar Latin. However, some notable differences occur between today's Romance languages and their Roman ancestor. With only one or two exceptions, Romance languages have lost the declension system of Latin and, as a result, have SVO sentence structure and make extensive use of prepositions.

Documentary evidence is limited about Vulgar Latin for the purposes of comprehensive research, and the literature is often hard to interpret or generalize. Many of its speakers were soldiers, slaves, displaced peoples, and forced resettlers, more likely to be natives of conquered lands than natives of Rome. In Western Europe, Latin gradually replaced Celtic and other Italic languages, which were related to it by a shared Indo-European origin. Commonalities in syntax and vocabulary facilitated the adoption of Latin.

Vulgar Latin is believed to have already had most of the features shared by all Romance languages, which distinguish them from Classical Latin, such as the almost complete loss of the Latin grammatical case system and its replacement by prepositions; the loss of the neuter grammatical gender and comparative inflections; replacement of some verb paradigms by innovations (e.g. the synthetic future gave way to an originally analytic strategy now typically formed by infinitive + evolved present indicative forms of 'have'); the use of articles; and the initial stages of the palatalization of the plosives /k/, /g/, and /t/.

To some scholars, this suggests the form of Vulgar Latin that evolved into the Romance languages was around during the time of the Roman Empire (from the end of the first century BC), and was spoken alongside the written Classical Latin which was reserved for official and formal occasions. Other scholars argue that the distinctions are more rightly viewed as indicative of sociolinguistic and register differences normally found within any language. Both were mutually intelligible as one and the same language, which was true until very approximately the second half of the 7th century. However, within two hundred years Latin became a dead language since "the Romanized people of Europe could no longer understand texts that were read aloud or recited to them," i.e. Latin had ceased to be a first language and became a foreign language that had to be learned, if the label Latin is constrained to refer to a state of the language frozen in past time and restricted to linguistic features for the most part typical of higher registers.

With the rise of the Roman Empire, Vulgar Latin spread first throughout Italy and then through southern, western, central, and southeast Europe, and northern Africa along parts of western Asia.

During the political decline of the Western Roman Empire in the fifth century, there were large-scale migrations into the empire, and the Latin-speaking world was fragmented into several independent states. Central Europe and the Balkans were occupied by Germanic and Slavic tribes, as well as by Huns. These incursions isolated the Vlachs from the rest of Romance-speaking Europe.

British and African Romance—the forms of Vulgar Latin used in Britain and the Roman province of Africa, where it had been spoken by much of the urban population—disappeared in the Middle Ages (as did Pannonian Romance in what is now Hungary, and Moselle Romance in Germany). But the Germanic tribes that had penetrated Roman Italy, Gaul, and Hispania eventually adopted Latin/Romance and the remnants of the culture of ancient Rome alongside existing inhabitants of those regions, and so Latin remained the dominant language there. In part due to regional dialects of the Latin language and local environments, several languages evolved from it.

Meanwhile, large-scale migrations into the Eastern Roman Empire started with the Goths and continued with Huns, Avars, Bulgars, Slavs, Pechenegs, Hungarians and Cumans. The invasions of Slavs were the most thoroughgoing, and they partially reduced the Romanic element in the Balkans. 
The invasion of the Turks and conquest of Constantinople in 1453 marked the end of the empire. The Slavs named the Romance-speaking population Vlachs, while the latter called themselves "Rumân" or "Român", from the Latin "Romanus" The Daco-Roman dialect became fully distinct from the three dialects spoken South of the Danube—Macedo-Romanian, Istro-Romanian, and Megleno-Romanian—during the ninth and tenth centuries, when the Romanians (sometimes called Vlachs or Wallachians) emerged as a people.

Over the course of the fourth to eighth centuries, local changes in phonology, morphology, syntax and lexicon accumulated to the point that the speech of any locale was noticeably different from another. In principle, differences between any two lects increased the more they were separated geographically, reducing easy mutual intelligibility between speakers of distant communities. Clear evidence of some levels of change is found in the "Reichenau Glosses", an eighth-century compilation of about 1,200 words from the fourth-century Vulgate of Jerome that had changed in phonological form or were no longer normally used, along with their eighth-century equivalents in proto-Franco-Provençal. The following are some examples with reflexes in several modern Romance languages for comparison:
In all of the above examples, the words appearing in the fourth century Vulgate are the same words as would have been used in Classical Latin of c. 50 BC. It is likely that some of these words had already disappeared from casual speech by the time of the "Glosses"; but if so, they may well have been still widely understood, as there is no recorded evidence that the common people of the time had difficulty understanding the language.

By the 8th century, the situation was very different. During the late 8th century, Charlemagne, holding that "Latin of his age was by classical standards intolerably corrupt", successfully imposed Classical Latin as an artificial written vernacular for Western Europe. Unfortunately, this meant that parishioners could no longer understand the sermons of their priests, forcing the Council of Tours in 813 to issue an edict that priests needed to translate their speeches into the "rustica romana lingua", an explicit acknowledgement of the reality of the Romance languages as separate languages from Latin.

By this time, and possibly as early as the 6th century according to Price (1984), the Romance lects had split apart enough to be able to speak of separate Gallo-Romance, Ibero-Romance, Italo-Romance and Eastern Romance languages. Some researchers have postulated that the major divergences in the spoken dialects began or accelerated considerably in the 5th century, as the formerly widespread and efficient communication networks of the Western Roman Empire rapidly broke down, leading to the total disappearance of the Western Roman Empire by the end of the century. The critical period between the 5th–10th centuries AD is poorly documented because little or no writing from the chaotic "Dark Ages" of the 5th–8th centuries has survived, and writing after that time was in consciously classicized Medieval Latin, with vernacular writing only beginning in earnest in the 11th or 12th centuries. An exception such as the Oaths of Strasbourg is evidence that by the ninth century effective communication with a non-learnèd audience was carried out in evolved Romance.

A language that was closely related to medieval Romanian was spoken during the Dark Ages by Vlachs in the Balkans, Herzegovina, Dalmatia (Morlachs), Ukraine (Hutsuls), Poland (Gorals), Slovakia, and Czech Moravia, but gradually these communities lost their maternal language.

Between the 10th and 13th centuries, some local vernaculars developed a written form and began to supplant Latin in many of its roles. In some countries, such as Portugal, this transition was expedited by force of law; whereas in others, such as Italy, many prominent poets and writers used the vernacular of their own accord – some of the most famous in Italy being Giacomo da Lentini and Dante Alighieri. Well before that, the vernacular was also used for practical purposes, such as the testimonies in the Placiti Cassinesi, written 960-963.

The invention of the printing press brought a tendency towards greater uniformity of standard languages within political boundaries, at the expense of other Romance languages and dialects less favored politically. In France, for instance, the dialect spoken in the region of Paris gradually spread to the entire country, and the Occitan of the south lost ground.

Significant sound changes affected the consonants of the Romance languages.

There was a tendency to eliminate final consonants in Vulgar Latin, either by dropping them (apocope) or adding a vowel after them (epenthesis).

Many final consonants were rare, occurring only in certain prepositions (e.g. "ad" "towards", "apud" "at, near (a person)"), conjunctions ("sed" "but"), demonstratives (e.g. "illud" "that (over there)", "hoc" "this"), and nominative singular noun forms, especially of neuter nouns (e.g. "lac" "milk", "mel" "honey", "cor" "heart"). Many of these prepositions and conjunctions were replaced by others, while the nouns were regularized into forms based on their oblique stems that avoided the final consonants (e.g. *"lacte", *"mele", *"core").

Final "-m" was dropped in Vulgar Latin. Even in Classical Latin, final "-am", "-em", "-um" (inflectional suffixes of the accusative case) were often elided in poetic meter, suggesting the "m" was weakly pronounced, probably marking the nasalisation of the vowel before it. This nasal vowel lost its nasalization in the Romance languages except in monosyllables, where it became e.g. Spanish "quien" < "quem" "whom", French "rien" "anything" < "rem" "thing"; note especially French and Catalan "mon" < "meum" "my (m.sg.)" pronounced as one syllable ( > *) but Spanish "mío" and Portuguese and Catalan "meu" < "meum" pronounced as two ( > *).

As a result, only the following final consonants occurred in Vulgar Latin:

Final "-t" was eventually dropped in many languages, although this often occurred several centuries after the Vulgar Latin period. For example, the reflex of "-t" was dropped in Old French and Old Spanish only around 1100. In Old French, this occurred only when a vowel still preceded the "t" (generally < Latin "a"). Hence "amat" "he loves" > Old French "aime" but "venit" "he comes" > Old French "vient": the was never dropped and survives into Modern French in liaison, e.g. "vient-il?" "is he coming?" (the corresponding in "aime-t-il?" is analogical, not inherited). Old French also kept the third-person plural ending "-nt" intact.

In Italo-Romance and the Eastern Romance languages, eventually "all" final consonants were either dropped or protected by an epenthetic vowel, except in clitic forms (e.g. prepositions "con", "per"). Modern Standard Italian still has almost no consonant-final words, although Romanian has resurfaced them through later loss of final and . For example, "amās" "you love" > "ame" > Italian "ami"; "amant" "they love" > *"aman" > Ital. "amano". On the evidence of "sloppily written" Lombardic language documents, however, the loss of final in Italy did not occur until the 7th or 8th century, after the Vulgar Latin period, and the presence of many former final consonants is betrayed by the syntactic gemination ("raddoppiamento sintattico") that they trigger. It is also thought that after a long vowel became rather than simply disappearing: "nōs" > "noi" "we", "se(d)ēs" > "sei" "you are", "crās" > "crai" "tomorrow" (southern Italian). In unstressed syllables, the resulting diphthongs were simplified: "canēs" > > "cani" "dogs"; "amīcās" > > "amiche" "(female) friends", where nominative "amīcae" should produce "**amice" rather than "amiche" (note masculine "amīcī" > "amici" not "**amichi").

Central Western Romance languages eventually regained a large number of final consonants through the general loss of final and , e.g. Catalan "llet" "milk" < "lactem", "foc" "fire" < "focum", "peix" "fish" < "piscem". In French, most of these secondary final consonants (as well as primary ones) were lost before around 1700, but tertiary final consonants later arose through the loss of < "-a". Hence masculine "frīgidum" "cold" > Old French "freit" > "froid" , feminine "frigidam" > Old French "freide" > "froide" .

Palatalization was one of the most important processes affecting consonants in Vulgar Latin. This eventually resulted in a whole series of "" and consonants in most Romance languages, e.g. Italian .

The following historical stages occurred:

Note how the environments become progressively less "palatal", and the languages affected become progressively fewer.

The outcomes of palatalization depended on the historical stage, the consonants involved, and the languages involved. The primary division is between the Western Romance languages, with resulting from palatalization of , and the remaining languages (Italo-Dalmatian and Eastern Romance), with resulting. It is often suggested that was the original result in all languages, with > a later innovation in the Western Romance languages. Evidence of this is the fact that Italian has both and as outcomes of palatalization in different environments, while Western Romance has only . Even more suggestive is the fact that the Mozarabic language in al-Andalus (modern southern Spain) had as the outcome despite being in the "Western Romance" area and geographically disconnected from the remaining areas; this suggests that Mozarabic was an outlying "relic" area where the change > failed to reach. (Northern French dialects, such as Norman and Picard, also had , but this may be a secondary development, i.e. due to a later sound change > .) Note that eventually became /s, z, ʒ/ in most Western Romance languages. Thus Latin "caelum" (sky, heaven), pronounced with an initial , became Italian "cielo" , Romanian "cer" , Spanish "cielo" /, French "ciel" , Catalan "cel" , and Portuguese "céu" .

The outcome of palatalized and is less clear:

This suggests that palatalized > > either or depending on location, while palatalized > ; after this, > in most areas, but Spanish and Gascon (originating from isolated districts behind the western Pyrenees) were relic areas unaffected by this change.

In French, the outcomes of palatalized by and by were different: "centum" "hundred" > "cent" but "cantum" "song" > "chant" . French also underwent palatalization of labials before : Vulgar Latin > Old French ("sēpia" "cuttlefish" > "seiche", "rubeus" "red" > "rouge", "sīmia" "monkey" > "singe").

The original outcomes of palatalization must have continued to be phonetically palatalized even after they had developed into //etc. consonants. This is clear from French, where all originally palatalized consonants triggered the development of a following glide in certain circumstances (most visible in the endings "-āre", "-ātum/ātam"). In some cases this came from a consonant palatalized by an adjoining consonant after the late loss of a separating vowel. For example, "mansiōnātam" > > > > early Old French "maisnieḍe" "household". Similarly, "mediētātem" > > > > early Old French "meitieḍ" > modern French "moitié" "half". In both cases, phonetic palatalization must have remained in primitive Old French at least through the time when unstressed intertonic vowels were lost (?8th century), well after the fragmentation of the Romance languages.

The effect of palatalization is indicated in the writing systems of almost all Romance languages, where the letters have the "hard" pronunciation in most situations, but a "soft" pronunciation (e.g. French/Portuguese , Italian/Romanian ) before . (This orthographic trait has passed into Modern English through Norman French-speaking scribes writing Middle English; this replaced the earlier system of Old English, which had developed its own hard-soft distinction with the soft representing .) This has the effect of keeping the modern spelling similar to the original Latin spelling, but complicates the relationship between sound and letter. In particular, the hard sounds must be written differently before (e.g. Italian , Portuguese ), and likewise for the soft sounds when not before these letters (e.g. Italian , Portuguese ). Furthermore, in Spanish, Catalan, Occitan and Brazilian Portuguese, the use of digraphs containing to signal the hard pronunciation before means that a different spelling is also needed to signal the sounds before these vowels (Spanish , Catalan, Occitan and Brazilian Portuguese ). This produces a number of orthographic alternations in verbs whose pronunciation is entirely regular. The following are examples of corresponding first-person plural indicative and subjunctive in a number of regular Portuguese verbs: "marcamos, marquemos" "we mark"; "caçamos, cacemos" "we hunt"; "chegamos, cheguemos" "we arrive"; "averiguamos, averigüemos" "we verify"; "adequamos, adeqüemos" "we adapt"; "oferecemos, ofereçamos" "we offer"; "dirigimos, dirijamos" "we drive" "erguemos, ergamos" "we raise"; "delinquimos, delincamos" "we commit a crime". In the case of Italian, the convention of digraphs <ch> and <gh> to represent /k/ and /g/ before written <e, i> results in similar orthographic alternations, such as "dimentico" 'I forget', "dimentichi" 'you forget', "baco" 'worm', "bachi" 'worms' with [k] or "pago" 'I pay', "paghi" 'you pay' and "lago" 'lake', "laghi" 'lakes' with [g]. The use in Italian of <ci> and <gi> to represent /tʃ/ or /dʒ/ before vowels written neatly distinguishes "dico" 'I say' with /k/ from "dici" 'you say' with /tʃ/ or "ghiro" 'dormouse' /g/ and "giro" 'turn, revolution' /dʒ/, but with orthographic <ci> and <gi> also representing the sequence of /tʃ/ or /dʒ/ and the actual vowel /i/ (/ditʃi/ "dici", /dʒiro/ "giro"), and no generally observed convention of indicating stress position, the status of "i" when followed by another vowel in spelling can be unrecognizable. For example, the written forms offer no indication that <cia> in "camicia" 'shirt' represents a single unstressed syllable /tʃa/ with no /i/ at any level (/kaˈmitʃa/ → [kaˈmiːtʃa] ~ [kaˈmiːʃa]), but that underlying the same spelling <cia> in "farmacia" 'pharmacy' is a bisyllabic sequence consisting of the stressed syllable /tʃi/ and syllabic /a/ (/farmaˈtʃia/ → [farmaˈtʃiːa] ~ [farmaˈʃiːa]).

Stop consonants shifted by lenition in Vulgar Latin in some areas.

The voiced labial consonants and (represented by and , respectively) both developed a fricative as an intervocalic allophone. This is clear from the orthography; in medieval times, the spelling of a consonantal is often used for what had been a in Classical Latin, or the two spellings were used interchangeably. In many Romance languages (Italian, French, Portuguese, Romanian, etc.), this fricative later developed into a ; but in others (Spanish, Galician, some Catalan and Occitan dialects, etc.) reflexes of and simply merged into a single phoneme.

Several other consonants were "softened" in intervocalic position in Western Romance (Spanish, Portuguese, French, Northern Italian), but normally not phonemically in the rest of Italy (except some cases of "elegant" or Ecclesiastical words), nor apparently at all in Romanian. The dividing line between the two sets of dialects is called the La Spezia–Rimini Line and is one of the most important isoglosses of the Romance dialects. The changes (instances of diachronic lenition resulting in phonological restructuring) are as follows:

Single voiceless plosives became voiced: "-p-, -t-, -c-" > "-b-, -d-, -g-". Subsequently, in some languages they were further weakened, either becoming fricatives or approximants, (as in Spanish) or disappearing entirely (as and , but not , in French). The following example shows progressive weakening of original /t/: e.g. "vītam" > Italian "vita" , Portuguese "vida" (European Portuguese ), Spanish "vida" (Southern Peninsular Spanish ), and French "vie" . Some scholars once speculated that these sound changes may be due in part to the influence of Continental Celtic languages, but scholarship of the past few decades challenges that hypothesis.

Consonant length is no longer phonemically distinctive in most Romance languages. However some languages of Italy (Italian, Sardinian, Sicilian, and numerous other varieties of central and southern Italy) do have long consonants like , , etc., where the doubling indicates either actual length or, in the case of plosives and affricates, a short hold before the consonant is released, in many cases with distinctive lexical value: e.g. "note" (notes) vs. "notte" (night), "cade" (s/he, it falls) vs. "cadde" (s/he, it fell), "caro" (dear, expensive) vs. "carro" (cart). They may even occur at the beginning of words in Romanesco, Neapolitan, Sicilian and other southern varieties, and are occasionally indicated in writing, e.g. Sicilian "cchiù" (more), and "ccà" (here). In general, the consonants , , and are long at the start of a word, while the archiphoneme is realised as a trill in the same position. In much of central and southern Italy, the affricates /t͡ʃ/ and /d͡ʒ/ weaken synchronically to fricative [ʃ] and [ʒ] between vowels, while their geminate congeners do not, e.g. "cacio" (cheese) vs. "caccio" (I chase).

A few languages have regained secondary geminate consonants. The double consonants of Piedmontese exist only after stressed , written "ë", and are not etymological: "vëdde" (Latin "vidēre", to see), "sëcca" (Latin "sicca", dry, feminine of "sech"). In standard Catalan and Occitan, there exists a geminate sound written "ŀl" (Catalan) or "ll" (Occitan), but it is usually pronounced as a simple sound in colloquial (and even some formal) speech in both languages.

In Late Latin a prosthetic vowel /i/ (lowered to /e/ in most languages) was inserted at the beginning of any word that began with (referred to as "s impura") and a voiceless consonant (#sC- > isC-): 
Prosthetic /i/ ~ /e/ in Romance languages may have been influenced by Continental Celtic languages, although the phenomenon exists or existed in some areas where Celtic was never present (e.g. Sardinia, southern Italy). While Western Romance words undergo prothesis, cognates in Balkan Romance and southern Italo-Romance do not, e.g. Italian "scrivere", "spada", "spirito", "Stefano", and "stato". In Italian, syllabification rules were preserved instead by vowel-final articles, thus feminine "spada" as "la spada", but instead of rendering the masculine "*il spaghetto", "lo spaghetto" came to be the norm. Though receding at present, Italian once had a prosthetic if a consonant preceded such clusters, so that 'in Switzerland' was "in" "Svizzera". Some speakers still use the prothetic productively, and it is fossilized in a few set locutions such as "in ispecie" 'especially' or "per iscritto" 'in writing' (although in this case its survival may be due partly to the influence of the separate word "iscritto" < Latin "īnscrīptus"). The association of /i/ ~ /j/ and /s/ also led to the vocalization of word-final -"s" in Italian, Romanian, certain Occitan dialects, and the Spanish dialect of Chocó in Colombia.

One profound change that affected Vulgar Latin was the reorganisation of its vowel system. Classical Latin had five short vowels, "ă, ĕ, ĭ, ŏ, ŭ", and five long vowels, "ā, ē, ī, ō, ū", each of which was an individual phoneme (see the table in the right, for their likely pronunciation in IPA), and four diphthongs, "ae", "oe", "au" and "eu" (five according to some authors, including "ui"). There were also long and short versions of "y", representing the rounded vowel in Greek borrowings, which however probably came to be pronounced even before Romance vowel changes started.

There is evidence that in the imperial period all the short vowels except "a" differed by quality as well as by length from their long counterparts. So, for example "ē" was pronounced close-mid while "ĕ" was pronounced open-mid , and "ī" was pronounced close while "ĭ" was pronounced near-close .

During the Proto-Romance period, phonemic length distinctions were lost. Vowels came to be automatically pronounced long in stressed, open syllables (i.e. when followed by only one consonant), and pronounced short everywhere else. This situation is still maintained in modern Italian: "cade" "he falls" vs. "cadde" "he fell".

The Proto-Romance loss of phonemic length originally produced a system with nine different quality distinctions in monophthongs, where only original had merged. Soon, however, many of these vowels coalesced:

The Proto-Romance allophonic vowel-length system was rephonemicized in the Gallo-Romance languages as a result of the loss of many final vowels. Some northern Italian languages (e.g. Friulan) still maintain this secondary phonemic length, but most languages dropped it by either diphthongizing or shortening the new long vowels.

French phonemicized a third vowel length system around AD 1300 as a result of the sound change /VsC/ > /VhC/ > (where "V" is any vowel and "C" any consonant). This vowel length was eventually lost by around AD 1700, but the former long vowels are still marked with a circumflex. A fourth vowel length system, still non-phonemic, has now arisen: All nasal vowels as well as the oral vowels (which mostly derive from former long vowels) are pronounced long in all stressed closed syllables, and all vowels are pronounced long in syllables closed by the voiced fricatives . This system in turn has been phonemicized in some non-standard dialects (e.g. Haitian Creole), as a result of the loss of final .

The Latin diphthongs "ae" and "oe", pronounced and in earlier Latin, were early on monophthongized.

"ae" became by the 1st century at the latest. Although this sound was still distinct from all existing vowels, the neutralization of Latin vowel length eventually caused its merger with < short "e": e.g. "caelum" "sky" > French "ciel", Spanish/Italian "cielo", Portuguese "céu" , with the same vowel as in "mele" "honey" > French/Spanish "miel", Italian "miele", Portuguese "mel" . Some words show an early merger of "ae" with , as in "praeda" "booty" > *"prēda" > French "proie" (vs. expected **"priée"), Italian "preda" (not **"prieda") "prey"; or "faenum" "hay" > *"fēnum" > Spanish "heno", French "foin" (but Italian "fieno" /fjɛno/).

"oe" generally merged with : "poenam" "punishment" > Romance * > Spanish/Italian "pena", French "peine"; "foedus" "ugly" > Romance * > Spanish "feo", Portuguese "feio". There are relatively few such outcomes, since "oe" was rare in Classical Latin (most original instances had become Classical "ū", as in Old Latin "oinos" "one" > Classical "ūnus") and so "oe" was mostly limited to Greek loanwords, which were typically learned (high-register) terms.

"au" merged with "ō" in the popular speech of Rome already by the 1st century . A number of authors remarked on this explicitly, e.g. Cicero's taunt that the populist politician Publius Clodius Pulcher had changed his name from "Claudius" to ingratiate himself with the masses. This change never penetrated far from Rome, however, and the pronunciation /au/ was maintained for centuries in the vast majority of Latin-speaking areas, although it eventually developed into some variety of "o" in many languages. For example, Italian and French have as the usual reflex, but this post-dates diphthongization of and the French-specific palatalization > (hence "causa" > French "chose", Italian "cosa" not **"cuosa"). Spanish has , but Portuguese spelling maintains , which has developed to (and still remains as in some dialects, and in others). Occitan, Romanian, southern Italian languages, and many other minority Romance languages still have . A few common words, however, show an early merger with "ō" , evidently reflecting a generalization of the popular Roman pronunciation: e.g. French "queue", Italian "coda" , Occitan "co(d)a", Romanian "coadă" (all meaning "tail") must all derive from "cōda" rather than Classical "cauda" (but notice Portuguese "cauda"). Similarly, Spanish "oreja", Portuguese "orelha", French "oreille", Romanian "ureche", and Sardinian "olícra", "orícla" "ear" must derive from "ōric(u)la" rather than Classical "auris" (Occitan "aurelha" was probably influenced by the unrelated "ausir" < "audīre" "to hear"), and the form "oricla" is in fact reflected in the Appendix Probi.

An early process that operated in all Romance languages to varying degrees was metaphony (vowel mutation), conceptually similar to the umlaut process so characteristic of the Germanic languages. Depending on the language, certain stressed vowels were raised (or sometimes diphthongized) either by a final /i/ or /u/ or by a directly following /j/. Metaphony is most extensive in the Italo-Romance languages, and applies to nearly all languages in Italy; however, it is absent from Tuscan, and hence from standard Italian. In many languages affected by metaphony, a distinction exists between final /u/ (from most cases of Latin "-um") and final /o/ (from Latin "-ō", "-ud" and some cases of "-um", esp. masculine "mass" nouns), and only the former triggers metaphony.

Some examples:

A number of languages diphthongized some of the free vowels, especially the open-mid vowels :

These diphthongizations had the effect of reducing or eliminating the distinctions between open-mid and close-mid vowels in many languages. In Spanish and Romanian, all open-mid vowels were diphthongized, and the distinction disappeared entirely. Portuguese is the most conservative in this respect, keeping the seven-vowel system more or less unchanged (but with changes in particular circumstances, e.g. due to metaphony). Other than before palatalized consonants, Catalan keeps intact, but split in a complex fashion into and then coalesced again in the standard dialect (Eastern Catalan) in such a way that most original have reversed their quality to become .

In French and Italian, the distinction between open-mid and close-mid vowels occurred only in closed syllables. Standard Italian more or less maintains this. In French, /e/ and merged by the twelfth century or so, and the distinction between and was eliminated without merging by the sound changes , . Generally this led to a situation where both and occur allophonically, with the close-mid vowels in open syllables and the open-mid vowels in closed syllables. This is still the situation in modern Spanish, for example. In French, however, both and were partly rephonemicized: Both and occur in open syllables as a result of , and both and occur in closed syllables as a result of .

Old French also had numerous falling diphthongs resulting from diphthongization before palatal consonants or from a fronted /j/ originally following palatal consonants in Proto-Romance or later: e.g. "pācem" /patsʲe/ "peace" > PWR */padzʲe/ (lenition) > OF "paiz" /pajts/; *"punctum" "point" > Gallo-Romance */ponʲto/ > */pojɲto/ (fronting) > OF "point" /põjnt/. During the Old French period, preconsonantal /l/ [ɫ] vocalized to /w/, producing many new falling diphthongs: e.g. "dulcem" "sweet" > PWR */doltsʲe/ > OF "dolz" /duɫts/ > "douz" /duts/; "fallet" "fails, is deficient" > OF "falt" > "faut" "is needed"; "bellus" "beautiful" > OF "bels" > "beaus" . By the end of the Middle French period, "all" falling diphthongs either monophthongized or switched to rising diphthongs: proto-OF > early OF > modern spelling > mod. French .

In both French and Portuguese, nasal vowels eventually developed from sequences of a vowel followed by a nasal consonant (/m/ or /n/). Originally, all vowels in both languages were nasalized before any nasal consonants, and nasal consonants not immediately followed by a vowel were eventually dropped. In French, nasal vowels before remaining nasal consonants were subsequently denasalized, but not before causing the vowels to lower somewhat, e.g. "dōnat" "he gives" > OF "dune" > "donne" , "fēminam" > "femme" . Other vowels remained diphthongized, and were dramatically lowered: "fīnem" "end" > "fin" (often pronounced ); "linguam" "tongue" > "langue" ; "ūnum" "one" > "un" .

In Portuguese, /n/ between vowels was dropped, and the resulting hiatus eliminated through vowel contraction of various sorts, often producing diphthongs: "manum, *manōs" > PWR *"manu, ˈmanos" "hand(s)" > "mão, mãos" ; "canem, canēs" "dog(s)" > PWR *"kane, ˈkanes" > *"can, ˈcanes" > "cão, cães" ; "ratiōnem, ratiōnēs" "reason(s)" > PWR *"raˈdʲzʲone, raˈdʲzʲones" > *"raˈdzon, raˈdzones" > "razão, razões" (Brazil), (Portugal). Sometimes the nasalization was eliminated: "lūna" "moon" > Galician-Portuguese "lũa" > "lua"; "vēna" "vein" > Galician-Portuguese "vẽa" > "veia". Nasal vowels that remained actually tend to be raised (rather than lowered, as in French): "fīnem" "end" > "fim" ; "centum" "hundred" > PWR "tʲsʲɛnto" > "cento" ; "pontem" "bridge" > PWR "pɔnte" > "ponte" (Brazil), (Portugal). In Portugal, vowels before a nasal consonant have become denasalized, but in Brazil they remain heavily nasalized.

Characteristic of the Gallo-Romance languages and Rhaeto-Romance languages are the front rounded vowels . All of these languages show an unconditional change /u/ > /y/, e.g. "lūnam" > French "lune" , Occitan . Many of the languages in Switzerland and Italy show the further change /y/ > /i/. Also very common is some variation of the French development (lengthened in open syllables) > > , with mid back vowels diphthongizing in some circumstances and then re-monophthongizing into mid-front rounded vowels. (French has both and , with developing from in certain circumstances.)

There was more variability in the result of the unstressed vowels. Originally in Proto-Romance, the same nine vowels developed in unstressed as stressed syllables, and in Sardinian, they coalesced into the same five vowels in the same way.

In Italo-Western Romance, however, vowels in unstressed syllables were significantly different from stressed vowels, with yet a third outcome for final unstressed syllables. In non-final unstressed syllables, the seven-vowel system of stressed syllables developed, but then the low-mid vowels merged into the high-mid vowels . This system is still preserved, largely or completely, in all of the conservative Romance languages (e.g. Italian, Spanish, Portuguese, Catalan).

In final unstressed syllables, results were somewhat complex. One of the more difficult issues is the development of final short "-u", which appears to have been raised to rather than lowered to , as happened in all other syllables. However, it is possible that in reality, final comes from "long" *"-ū" < "-um", where original final "-m" caused vowel lengthening as well as nasalization. Evidence of this comes from Rhaeto-Romance, in particular Sursilvan, which preserves reflexes of both final "-us" and "-um", and where the latter, but not the former, triggers metaphony. This suggests the development "-us" > > , but "-um" > > .

The original five-vowel system in final unstressed syllables was preserved as-is in some of the more conservative central Italian languages, but in most languages there was further coalescence:

Various later changes happened in individual languages, e.g.:

The so-called "intertonic vowels" are word-internal unstressed vowels, i.e. not in the initial, final, or "tonic" (i.e. stressed) syllable, hence intertonic. Intertonic vowels were the most subject to loss or modification. Already in Vulgar Latin intertonic vowels between a single consonant and a following /r/ or /l/ tended to drop: "vétulum" "old" > "veclum" > Dalmatian "vieklo", Sicilian "vecchiu", Portuguese "velho". But many languages ultimately dropped almost all intertonic vowels.

Generally, those languages south and east of the La Spezia–Rimini Line (Romanian and Central-Southern Italian) maintained intertonic vowels, while those to the north and west (Western Romance) dropped all except /a/. Standard Italian generally maintained intertonic vowels, but typically raised unstressed /e/ > /i/. Examples:
Portuguese is more conservative in maintaining some intertonic vowels other than /a/: e.g. *"offerḗscere" "to offer" > Portuguese "oferecer" vs. Spanish "ofrecer", French "offrir" (< *"offerīre"). French, on the other hand, drops even intertonic /a/ after the stress: "Stéphanum" "Stephen" > Spanish "Esteban" but Old French "Estievne" > French "Étienne". Many cases of /a/ before the stress also ultimately dropped in French: "sacraméntum" "sacrament" > Old French "sairement" > French "serment" "oath".

The Romance languages for the most part have kept the writing system of Latin, adapting it to their evolution.
One exception was Romanian before the nineteenth century, where, after the Roman retreat, literacy was reintroduced through the Romanian Cyrillic alphabet, a Slavic influence. A Cyrillic alphabet was also used for Romanian (Moldovan) in the USSR. The non-Christian populations of Spain also used the scripts of their religions (Arabic and Hebrew) to write Romance languages such as Ladino and Mozarabic in "aljamiado".

The Romance languages are written with the classical Latin alphabet of 23 letters – "A", "B", "C", "D", "E", "F", "G", "H", "I", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "V", "X", "Y", "Z" – subsequently modified and augmented in various ways. In particular, the single Latin letter "V" split into "V" (consonant) and "U" (vowel), and the letter "I" split into "I" and "J". The Latin letter "K" and the new letter "W", which came to be widely used in Germanic languages, are seldom used in most Romance languages – mostly for unassimilated foreign names and words. Indeed, in Italian prose is properly . Catalan eschews importation of "foreign" letters more than most languages. Thus Wikipedia is in Catalan but in Spanish.

While most of the 23 basic Latin letters have maintained their phonetic value, for some of them it has diverged considerably; and the new letters added since the Middle Ages have been put to different uses in different scripts. Some letters, notably "H" and "Q", have been variously combined in digraphs or trigraphs (see below) to represent phonetic phenomena that could not be recorded with the basic Latin alphabet, or to get around previously established spelling conventions. Most languages added auxiliary marks (diacritics) to some letters, for these and other purposes.

The spelling rules of most Romance languages are fairly simple, and consistent within any language. Since the spelling systems are based on phonemic structures rather than phonetics, however, the actual pronunciation of what is represented in standard orthography can be subject to considerable regional variation, as well as to allophonic differentiation by position in the word or utterance. Among the letters representing the most conspicuous phonological variations, between Romance languages or with respect to Latin, are the following:

Otherwise, letters that are not combined as digraphs generally represent the same phonemes as suggested by the International Phonetic Alphabet (IPA), whose design was, in fact, greatly influenced by Romance spelling systems.

Since most Romance languages have more sounds than can be accommodated in the Roman Latin alphabet they all resort to the use of digraphs and trigraphs – combinations of two or three letters with a single phonemic value. The concept (but not the actual combinations) is derived from Classical Latin, which used, for example, "TH", "PH", and "CH" when transliterating the Greek letters "θ", "ϕ" (later "φ"), and "χ". These were once aspirated sounds in Greek before changing to corresponding fricatives, and the "H" represented what sounded to the Romans like an following , , and respectively. Some of the digraphs used in modern scripts are:

While the digraphs "CH", "PH", "RH" and "TH" were at one time used in many words of Greek origin, most languages have now replaced them with "C/QU", "F", "R" and "T". Only French has kept these etymological spellings, which now represent or , , and , respectively.

Gemination, in the languages where it occurs, is usually indicated by doubling the consonant, except when it does not contrast phonemically with the corresponding short consonant, in which case gemination is not indicated. In Jèrriais, long consonants are marked with an apostrophe: is a long , is a long , and is a long . The phonemic contrast between geminate and single consonants is widespread in Italian, and normally indicated in the traditional orthography: 'done' vs. 'fate, destiny'; 's/he, it fell' vs. 's/he, it falls'. The double consonants in French orthography, however, are merely etymological. In Catalan, the gemination of is marked by a ("flying point"): .

Romance languages also introduced various marks (diacritics) that may be attached to some letters, for various purposes. In some cases, diacritics are used as an alternative to digraphs and trigraphs; namely to represent a larger number of sounds than would be possible with the basic alphabet, or to distinguish between sounds that were previously written the same. Diacritics are also used to mark word stress, to indicate exceptional pronunciation of letters in certain words, and to distinguish words with same pronunciation (homophones).

Depending on the language, some letter-diacritic combinations may be considered distinct letters, e.g. for the purposes of lexical sorting. This is the case, for example, of Romanian "ș" () and Spanish "ñ" ().

The following are the most common use of diacritics in Romance languages.


Most languages are written with a mixture of two distinct but phonetically identical variants or "cases" of the alphabet: majuscule ("uppercase" or "capital letters"), derived from Roman stone-carved letter shapes, and minuscule ("lowercase"), derived from Carolingian writing and Medieval quill pen handwriting which were later adapted by printers in the fifteenth and sixteenth centuries.

In particular, all Romance languages capitalize (use uppercase for the first letter of) the following words: the first word of each complete sentence, most words in names of people, places, and organizations, and most words in titles of books. The Romance languages do not follow the German practice of capitalizing all nouns including common ones. Unlike English, the names of months, days of the weeks, and derivatives of proper nouns are usually not capitalized: thus, in Italian one capitalizes "Francia" ("France") and "Francesco" ("Francis"), but not "francese" ("French") or "francescano" ("Franciscan"). However, each language has some exceptions to this general rule.

The tables below provide a vocabulary comparison that illustrates a number of examples of sound shifts that have occurred between Latin and Romance languages. Words are given in their conventional spellings. In addition, for French the actual pronunciation is given, due to the dramatic differences between spelling and pronunciation. (French spelling approximately reflects the pronunciation of Old French, c. 1200 AD.)


Overviews:

Phonology:

Lexicon:

French:

Portuguese:

Spanish:

Italian:

Rhaeto-Romance:



</doc>
<doc id="25402" url="https://en.wikipedia.org/wiki?curid=25402" title="Rugby football">
Rugby football

Rugby football is a collective name for the family of team sports of rugby union and rugby league, as well as the earlier forms of football from which both games, Association football, Australian rules football, and Gridiron football evolved. Canadian football (Grey Cup still has "Rugby Football" written on it), and to a lesser extent American football were also broadly considered forms of rugby football but are seldom now referred to as such.

Rugby football started about 1845 at Rugby School in Rugby, Warwickshire, England, although forms of football in which the ball was carried and tossed date to medieval times. Rugby split into two sports in 1895, when twenty-one clubs split from the Rugby Football Union to form the Northern Rugby Football Union (later renamed the Rugby Football League in 1922) in the George Hotel, Huddersfield, over broken-time payments to players who took time off from work to play the sport, thus making rugby league the first code to turn professional and pay players. Rugby union turned professional one hundred years later in 1995, following the 1995 Rugby World Cup in South Africa. The respective world governing bodies are World Rugby (rugby union) and the Rugby League International Federation (rugby league).

Rugby football was one of many versions of football played at English public schools in the 19th century. Although rugby league initially used rugby union rules, they are now wholly separate sports, with Rugby league being much more similar to Gridiron football. In addition to these two codes, both American and Canadian football evolved from rugby football in the beginning of the 20th century.

Following the 1895 split in rugby football, the two forms rugby league and rugby union differed in administration only. Soon the rules of rugby league were modified, resulting in two distinctly different forms of rugby. 100 years later, rugby union joined rugby league and most other forms of football as an openly professional sport.

The Olympic form of rugby is known as Rugby Sevens. In this form of the game, each team has seven players on the field at one time playing seven-minute halves. The rules and pitch size are the same as rugby union.

Although rugby football was codified at Rugby School, many rugby playing countries had pre-existing football games not dissimilar to rugby.

Forms of traditional football similar to rugby have been played throughout Europe and beyond. Many of these involved handling of the ball, and scrummaging formations. For example, New Zealand had Ki-o-rahi, Australia marn grook, Japan kemari, Georgia lelo burti, the Scottish Borders Jeddart Ba' and Cornwall Cornish hurling, Central Italy Calcio Fiorentino, South Wales cnapan, East Anglia Campball and Ireland had caid, an ancestor of Gaelic football.

In 1871, English clubs met to form the Rugby Football Union (RFU). In 1892, after charges of professionalism (compensation of team members) were made against some clubs for paying players for missing work, the Northern Rugby Football Union, usually called the Northern Union (NU), was formed. The existing rugby union authorities responded by issuing sanctions against the clubs, players, and officials involved in the new organization. After the schism, the separate clubs were named "rugby league" and "rugby union".

Rugby union is both a professional and amateur game, and is dominated by the first tier unions: New Zealand, Ireland, Wales, England, South Africa, Australia, Argentina, Scotland, Italy, France and Japan. Second and third tier unions include Belgium, Brazil, Canada, Chile, Fiji, Georgia, Germany, Hong Kong,Kenya, Namibia, the Netherlands, Portugal, Romania, Russia, Samoa, Spain, Tonga, the United States and Uruguay. Rugby Union is administered by World Rugby (WR), whose headquarters are located in Dublin, Ireland. It is the national sport in New Zealand, Wales, Fiji, Samoa, Tonga, Georgia and Madagascar, and is the most popular form of rugby globally. The Olympic Games have admitted the seven-a-side version of the game, known as Rugby sevens, into the programme from Rio de Janeiro in 2016 onwards. There was a possibility sevens would be a demonstration sport at the 2012 London Olympics but many sports including sevens were dropped.

In Canada and the United States, rugby developed into gridiron football. During the late 1800s (and even the early 1900s), the two forms of the game were very similar (to the point where the United States was able to win the gold medal for rugby union at the 1924 Summer Olympics), but numerous rule changes have differentiated the gridiron-based game from its rugby counterpart, introduced by Walter Camp in the United States and John Thrift Meldrum Burnside in Canada. Among unique features of the North American game are the separation of play into downs instead of releasing the ball immediately upon tackling, the requirement that the team with the ball set into a set formation for at least one second before resuming play after a tackle (and the allowance of up to 40 seconds to do so), the allowance for one forward pass from behind the site of the last tackle on each down, the evolution of hard plastic equipment (particularly the football helmet and shoulder pads), a smaller and pointier ball that is favorable to being passed but makes drop kicks impractical, a generally smaller and narrower field measured in customary units instead of metric (in some variants of the American game a field can be as short as 50 yards between end zones), and a distinctive field (shaped like a gridiron, from which the code's nickname is derived) with lines marked in five-yard intervals.

Rugby league is also both a professional and amateur game, administered on a global level by the Rugby League International Federation. In addition to amateur and semi-professional competitions in the United States, Russia, Lebanon, Serbia, Europe and Australasia, there are two major professional competitions—the Australasian National Rugby League and the Super League. International Rugby League is dominated by Australia, England and New Zealand. In Papua New Guinea and New Zealand, it is the national sport. Other nations from the South Pacific and Europe also play in the Pacific Cup and European Cup respectively.

Distinctive features common to both rugby codes include the oval ball and throwing the ball forward is not allowed so that players can gain ground only by running with the ball or by kicking it. As the sport of rugby league moved further away from its union counterpart, rule changes were implemented with the aim of making a faster-paced and more try-oriented game. Unlike American and Canadian football, the players do not wear any sort of protection or armour.

The main differences between the two games, besides league having teams of 13 players and union of 15, involve the tackle and its aftermath:

Set pieces of the union code include the "scrum", which occurs after a minor infringement of the rules (most often a knock-on, when a player knocks the ball forward), where packs of opposing players push against each other for possession, and the "line-out", in which parallel lines of players from each team, arranged perpendicular to the touch-line, attempt to catch the ball thrown from touch. A rule has been added to line-outs which allows the jumper to be pulled down once a players' feet are on the ground.

In the league code, the scrum still exists, but with greatly reduced importance as it involves fewer players and is rarely contested. Set pieces are generally started from the play-the-ball situation. Many of the rugby league positions have names and requirements similar to rugby union positions, but there are no flankers in rugby league.

In England, rugby union is widely regarded as an "establishment" sport, played mostly by members of the upper and middle classes. For example, many pupils at public schools and grammar schools play rugby union, although the game (which had a long history of being played at state schools until the 1980s) is becoming increasingly popular in comprehensive schools. Despite this stereotype, the game, particularly in the West Country is popular amongst all classes. In contrast, rugby league has traditionally been seen as a working-class pursuit. Another exception to rugby union's upper-class stereotype is in Wales, where it has been traditionally associated with small village teams made up of coal miners and other industrial workers who played on their days off. In Ireland, both rugby union and rugby league are unifying forces across the national and sectarian divide, with the Ireland international teams representing both political entities.

In Australia, support for both codes is concentrated in New South Wales, Queensland and the Australian Capital Territory. The same perceived class barrier as exists between the two games in England also occurs in these states, fostered by rugby union's prominence and support at private schools.

Exceptions to the above include New Zealand (although rugby league is still considered to be a lower class game by many or a game for 'westies' referring to lower class western suburbs of Auckland and more recently, southern Auckland where the game is also popular), Wales, France (except Paris), Cornwall, Gloucestershire, Somerset, Scottish Borders, County Limerick (see Munster) and the Pacific Islands, where rugby union is popular in working class communities. Nevertheless, rugby league is perceived as the game of the working-class people in northern England and in the Australian states of New South Wales and Queensland.

In the United Kingdom, rugby union fans sometimes used the term "rugger" as an alternative name for the sport, (see Oxford '-er'), although this archaic expression has not had currency since the 1950s or earlier. New Zealanders refer to rugby union simply as either "rugby" or "union", or even simply "football", and to rugby league as "rugby league" or "league". In the U.S., people who play rugby are sometimes called "ruggers", a term little used elsewhere except facetiously.

In France, rugby is widely played and has a strong tradition in the Basque, Occitan and Catalan areas along the border regions between Spain and France. The game is very popular in South Africa, having been introduced by English-speaking settlers in the 19th century. British colonists also brought the game with them to Australia and New Zealand, where the game is widely played. It has spread thence to much of Polynesia, having particularly strong followings in Fiji, Samoa, and Tonga. Rugby union continues to grow in the Americas and parts of Asia as well.

About a quarter of rugby players are injured in each season.

Being a high contact sport Rugby union has the highest announced rates of concussions and outside England also has the highest number of catastrophic injuries out of any team sport. Research finding that during match play, concussion was reported at a higher level, and during training at a lower level, but still at a higher level than most players of another sport to receive.

A rugby ball, originally called a quanco, is a diamond shape ball used for easier passing.
Richard Lindon and Bernardo Solano started making balls for Rugby school out of hand stitched, four-panel, leather casings and pigs' bladders. The rugby ball's distinctive shape is supposedly due to the pig's bladder, although early balls were more plum-shaped than oval. The balls varied in size in the beginning depending upon how large the pig's bladder was.

In rugby union, World Rugby regulates the size and shape of the ball under Law 2 (also known as Law E.R.B); an official rugby union ball is oval and made of four panels, has a length in-line of 280–300 millimetres, a circumference (end to end) of 740–770 millimetres, and a circumference (in width) of 580–620 millimetres. It is made of leather or suitable synthetic material and may be treated to make it water resistant and easier to grip. The rugby ball may not weigh more than 460 grams or less than 410 and has an air pressure of 65.71–68.75 kilopascals, or 0.67–0.70 kilograms per square centimetre, or 9.5–10.0 lbs per square inch. Spare balls are allowed under the condition that players or teams do not seek an advantage by changing the ball. Smaller sized balls may also be used in games between younger players.
Much larger versions of traditional balls are also available for purchase, but these are mainly for their novelty attraction.

The Rugby World Cup, which was first held in New Zealand and Australia in 1987, occurs every four years. It is an international tournament organized by World Rugby. The event is played in the union format and features the top 20 teams from around the world. The current world champions are South Africa, who won the 2019 Rugby World Cup, which was played in Japan.

The Rugby League World Cup was the first World Cup of either of the Rugby codes and was first held in France in 1954, and as of 2013 occurs on a 4-year cycle. It is an international tournament that is organized by the Rugby League International Federation. The event is played in the league format and features the top 14 teams from around the world. The current world champions are Australia, who won the World Cup in 2017, played in Australia, New Zealand and Papua New Guinea.

Rugby shirts were formerly made of leather but are now made of a cotton and polyester mix. This material has the advantage of not absorbing as much water or mud as cotton alone. Owing to the more aggressive nature of the game, rugby clothing, in general, is designed to be much more robust and hardwearing than that worn for association football.

The rugby jerseys are slightly different depending on the type of rugby game played. The shirts worn by rugby league footballers commonly have a large "V" around the neck. The players in rugby union wear jerseys with a more traditional design, sometimes completely white (Cahors Rugby in France). The number of the player and his or her surname are placed on the upper back of the jersey (often name above number, with the number being significantly larger and more central), and the logo of the team on the upper left chest.

With the popularity of rugby over the years, many betting establishments have made it possible for viewers of the game to place wagers on games. The various types of wagers that can be placed on games vary, however, the main types of bets that can be placed are as follows:

Like most team sports, both forms of rugby are vulnerable to match-fixing, particularly bets involving easily manipulated outcomes, such as conceding penalties and first point scorer. A recent example is a deliberate infringement by Ryan Tandy in order for the first points scored to be a penalty goal in a 2010 NRL match; the attempt backfired when instead of taking a shot at goal, a try was scored.




</doc>
<doc id="25403" url="https://en.wikipedia.org/wiki?curid=25403" title="Russian">
Russian

Russian refers to anything related to Russia, including:


Russian may also refer to:



</doc>
<doc id="25405" url="https://en.wikipedia.org/wiki?curid=25405" title="Rugby union">
Rugby union

Rugby union, widely known simply as rugby, is a full contact team sport that originated in England in the first half of the 19th century. One of the two codes of rugby football, it is based on running with the ball in hand. In its most common form, a game is played between two teams of 15 players using an oval-shaped ball on a rectangular field called a pitch. The field has H-shaped goalposts at both ends.

Rugby union is a popular sport around the world, played by male and female players of all ages. In 2014, there were more than 6 million people playing worldwide, of whom 2.36 million were registered players. World Rugby, previously called the International Rugby Football Board (IRFB) and the International Rugby Board (IRB), has been the governing body for rugby union since 1886, and currently has 101 countries as full members and 18 associate members.

In 1845, the first football laws were written by pupils at Rugby School; other significant events in the early development of rugby include the decision by Blackheath F.C. to leave the Football Association in 1863 and, in 1895, the acrimonious split between the then amateur rugby union and the professional rugby league. Historically rugby union was an amateur sport, but in 1995 formal restrictions on payments to players were removed, making the game openly professional at the highest level for the first time.

Rugby union spread from the Home Nations of Great Britain and Ireland, with other early exponents of the sport including Australia, New Zealand, South Africa and France. The sport is followed primarily in the British Isles, France, Australasia, Southern Africa, the Southern Cone and to a lesser extent Canada and Japan, its growth occurring during the expansion of the British Empire and through French proponents (Rugby Europe) in Europe. Countries that have adopted rugby union as their "de facto" national sport include Fiji, Georgia, Madagascar, New Zealand, Samoa, and Tonga. 

International matches have taken place since 1871 when the first game was played between Scotland and England at Raeburn Place in Edinburgh. The Rugby World Cup, first held in 1987, is contested every four years. The Six Nations Championship in Europe and The Rugby Championship in the Southern Hemisphere are other major international competitions that are held annually.

National club and provincial competitions include the Premiership in England, the Top 14 in France, the Mitre 10 Cup in New Zealand, the Top League in Japan, the Currie Cup in South Africa and the National Rugby Championship in Australia. Other transnational club competitions include the European Rugby Champions Cup, the Pro14 in Europe and South Africa, and Super Rugby and Global Rapid Rugby in the Southern Hemisphere.

The origin of rugby football is reputed to be an incident during a game of English school football at Rugby School in Warwickshire in 1823, when William Webb Ellis is said to have picked up the ball and run with it. Although the story may well be apocryphal, it was immortalised at the school with a commemorative plaque that was unveiled in 1895, and the Rugby World Cup trophy is named after Webb Ellis. Rugby football stems from the form of the game played at Rugby School, which former pupils then introduced to their universities.

Former Rugby School student Albert Pell is credited with having formed the first "football" team while a student at Cambridge University. Major private schools each used different rules during this early period, with former pupils from Rugby and Eton attempting to carry their preferred rules through to their universities. A significant event in the early development of rugby football was the production of a written set of rules at Rugby School in 1845, followed by the Cambridge Rules that were drawn up in 1848.

Formed in 1863, the national governing body The Football Association (FA) began codifying a set of universal football rules. These new rules specifically banned players from running with the ball in hand and also disallowed hacking (kicking players in the shins), both of which were legal and common tactics under the Rugby School's rules of the sport. In protest at the imposition of the new rules, the Blackheath Club left the FA followed by several other clubs that also favoured the "Rugby Rules". Although these clubs decided to ban hacking soon afterwards, the split was permanent, and the FA's codified rules became known as "association football" whilst the clubs that had favoured the Rugby Rules formed the Rugby Football Union in 1871, and their code became known as "rugby football".

In 1895, there was a major schism within rugby football in England in which numerous clubs from Northern England resigned from the RFU over the issue of reimbursing players for time lost from their workplaces. The split highlighted the social and class divisions in the sport in England, and led directly to the creation of the separate code of "rugby league". The existing sport thereafter took on the name "rugby union" to differentiate it from rugby league, but both versions of the sport are known simply as "rugby" throughout most of the world.

The first rugby football international was played on 27 March 1871 between Scotland and England in Edinburgh. Scotland won the game 1–0. By 1881 both Ireland and Wales had representative teams and in 1883 the first international competition, the Home Nations Championship had begun. 1883 is also the year of the first rugby sevens tournament, the Melrose Sevens, which is still held annually.

Two important overseas tours took place in 1888: a British Isles team visited Australia and New Zealand—although a private venture, it laid the foundations for future British and Irish Lions tours; and the 1888–89 New Zealand Native football team brought the first overseas team to British spectators.
During the early history of rugby union, a time before commercial air travel, teams from different continents rarely met. The first two notable tours both took place in 1888the British Isles team touring New Zealand and Australia, followed by the New Zealand team touring Europe. Traditionally the most prestigious tours were the Southern Hemisphere countries of Australia, New Zealand and South Africa making a tour of a Northern Hemisphere, and the return tours made by a joint British and Irish team. Tours would last for months, due to long traveling times and the number of games undertaken; the 1888 New Zealand team began their tour in Hawkes Bay in June and did not complete their schedule until August 1889, having played 107 rugby matches. Touring international sides would play Test matches against international opponents, including national, club and county sides in the case of Northern Hemisphere rugby, or provincial/state sides in the case of Southern Hemisphere rugby.

Between 1905 and 1908, all three major Southern Hemisphere rugby countries sent their first touring teams to the Northern Hemisphere: New Zealand in 1905, followed by South Africa in 1906 and Australia in 1908. All three teams brought new styles of play, fitness levels and tactics, and were far more successful than critics had expected.

The New Zealand 1905 touring team performed a haka before each match, leading Welsh Rugby Union administrator Tom Williams to suggest that Wales player Teddy Morgan lead the crowd in singing the Welsh National Anthem, "Hen Wlad Fy Nhadau", as a response. After Morgan began singing, the crowd joined in: the first time a national anthem was sung at the start of a sporting event. In 1905 France played England in its first international match.

Rugby union was included as an event in the Olympic Games four times during the early 20th century. No international rugby games and union-sponsored club matches were played during the First World War, but competitions continued through service teams such as the New Zealand Army team. During the Second World War no international matches were played by most countries, though Italy, Germany and Romania played a limited number of games, and Cambridge and Oxford continued their annual University Match.

The first officially sanctioned international rugby sevens tournament took place in 1973 at Murrayfield, one of Scotland's biggest stadiums, as part of the Scottish Rugby Union centenary celebrations.

In 1987 the first Rugby World Cup was held in Australia and New Zealand, and the inaugural winners were New Zealand. The first World Cup Sevens tournament was held at Murrayfield in 1993. Rugby Sevens was introduced into the Commonwealth Games in 1998 and was added to the Olympic Games of 2016. Both men and women's Sevens will again take place at the 2020 Olympic Games in Tokyo.

Rugby union was an amateur sport until the IRB declared the game "open" in August 1995 (shortly after the completion of the 1995 World Cup), removing restrictions on payments to players. However, the pre-1995 period of rugby union was marked by frequent accusations of "shamateurism", including an investigation in Britain by a House of Commons Select committee in early 1995. Following the introduction of professionalism trans-national club competitions were started, with the Heineken Cup in the Northern Hemisphere and Super Rugby in the Southern Hemisphere.

The Tri Nations, an annual international tournament involving Australia, New Zealand and South Africa, kicked off in 1996. In 2012, this competition was extended to include Argentina, a country whose impressive performances in international games (especially finishing in third place in the 2007 Rugby World Cup) was deemed to merit inclusion in the competition. As a result of the expansion to four teams, the tournament was renamed The Rugby Championship.

Each team starts the match with 15 players on the field and seven or eight substitutes. Players in a team are divided into eight forwards (two more than in rugby league) and seven backs.

The main responsibilities of the forward players are to gain and retain possession of the ball. Forwards play a vital role in tackling and rucking opposing players. Players in these positions are generally bigger and stronger and take part in the scrum and line-out. The forwards are often collectively referred to as the 'pack', especially when in the scrum formation.

The front row consists of three players: two props (the loosehead prop and the tighthead prop) and the hooker. The role of the two props is to support the hooker during scrums, to provide support for the jumpers during line-outs and to provide strength and power in rucks and mauls. The third position in the front row is the hooker. The hooker is a key position in attacking and defensive play and is responsible for winning the ball in the scrum. Hookers normally throw the ball in at line-outs.

The second row consists of two locks or lock forwards. Locks are usually the tallest players in the team, and specialise as line-out jumpers. The main role of the lock in line-outs is to make a standing jump, often supported by the other forwards, to either collect the thrown ball or ensure the ball comes down on their side. Locks also have an important role in the scrum, binding directly behind the three front row players and providing forward drive.
The back row, not to be confused with ‘Backs’, is the third and final row of the forward positions, who are often referred to as the loose forwards. The three positions in the back row are the two flankers and the number 8. The two flanker positions called the blindside flanker and openside flanker, are the final row in the scrum. They are usually the most mobile forwards in the game. Their main role is to win possession through 'turn overs'. The number 8 packs down between the two locks at the back of the scrum. The role of the number 8 in the scrum is to control the ball after it has been heeled back from the front of the pack, and the position provides a link between the forwards and backs during attacking phases.

The role of the backs is to create and convert point-scoring opportunities. They are generally smaller, faster and more agile than the forwards. Another distinction between the backs and the forwards is that the backs are expected to have superior kicking and ball-handling skills, especially the fly-half, scrum-half, and full-back.

The half-backs consist of two positions, the scrum-half and the fly-half. The fly-half is crucial to a team's game plan, orchestrating the team's performance. They are usually the first to receive the ball from the scrum-half following a breakdown, lineout, or scrum, and need to be decisive with what actions to take and be effective at communicating with the outside backs. Many fly-halves are also their team's goal kickers. The scrum-half is the link between the forwards and the backs. They receive the ball from the lineout and remove the ball from the back of the scrum, usually passing it to the fly-half. They also feed the scrum and sometimes have to act as a fourth loose forward.

There are four three quarter positions: two centres (inside and outside) and two wings (left and right). The centres will attempt to tackle attacking players; whilst in attack, they should employ speed and strength to breach opposition defences. The wings are generally positioned on the outside of the backline. Their primary function is to finish off moves and score tries. Wings are usually the fastest players in the team and are elusive runners who use their speed to avoid tackles.

The full-back is normally positioned several metres behind the back line. They often field opposition kicks and are usually the last line of defence should an opponent break through the back line. Two of the most important attributes of a good full-back are dependable catching skills and a good kicking game.

Rugby union is played between two teams – the one that scores more points wins the game. Points can be scored in several ways: a try, scored by grounding the ball in the in-goal area (between the goal line and the dead-ball line), is worth 5 points and a subsequent conversion kick scores 2 points; a successful penalty kick or a drop goal each score 3 points. The values of each of these scoring methods have been changed over the years. 
The field of play on a rugby pitch is as near as possible to a maximum of long by wide. In actual gameplay the length of a pitch can vary. There are typically between the two try-lines, but it can be as short as . Anywhere between behind each try line serves as the in-goal area. The pitch must be at least wide, up to a maximum of 

Rugby goalposts are H-shaped and are situated in the middle of the goal lines at each end of the field. They consist of two poles, apart, connected by a horizontal crossbar above the ground. The minimum height for posts is .

At the beginning of the game, the captains and the referee toss a coin to decide which team will kick off first. Play then starts with a dropkick, with the players chasing the ball into the opposition's territory, and the other side trying to retrieve the ball and advance it. The dropkick must make contact with the ground before kicked. If the ball does not reach the opponent's line 10 meters away, the opposing team has two choices: to have the ball kicked off again, or to have a scrum at the centre of the half-way line.
If the player with the ball is tackled, frequently a ruck will result.

Games are divided into 40-minute halves, with a break in the middle. The sides exchange ends of the field after the half-time break. Stoppages for injury or to allow the referee to take disciplinary action do not count as part of the playing time, so that the elapsed time is usually longer than 80 minutes. The referee is responsible for keeping time, even when—as in many professional tournaments—he is assisted by an official time-keeper. If time expires while the ball is in play, the game continues until the ball is "dead", and only then will the referee blow the whistle to signal half-time or full-time; but if the referee awards a penalty or free-kick, the game continues.

In the knockout stages of rugby competitions, most notably the Rugby World Cup, two extra time periods of 10 minutes periods are played (with an interval of 5 minutes in between) if the game is tied after full-time. If scores are level after 100 minutes then the rules call for 20 minutes of sudden-death extra time to be played. If the sudden-death extra time period results in no scoring a kicking competition is used to determine the winner. However, no match in the history of the Rugby World Cup has ever gone past 100 minutes into a sudden-death extra time period.

Forward passing (throwing the ball ahead to another player) is not allowed; the ball can be passed laterally or backwards. The ball tends to be moved forward in three ways — by kicking, by a player running with it or within a scrum or maul. Only the player with the ball may be tackled or rucked. A "knock-on" is committed when a player knocks the ball forward, and play is restarted with a scrum.

Any player may kick the ball forward in an attempt to gain territory. When a player anywhere in the playing area kicks indirectly into touch so that the ball first bounces in the field of play, the throw-in is taken where the ball went into touch. If the player kicks directly into touch (i.e. without bouncing in-field first) from within one's own line, the lineout is taken by the opposition where the ball went into touch, but if the ball is kicked into touch directly by a player outside the line, the lineout is taken level to where the kick was taken.

The aim of the defending side is to stop the player with the ball, either by bringing them to ground (a tackle, which is frequently followed by a ruck) or by contesting for possession with the ball-carrier on their feet (a maul). Such a circumstance is called a breakdown and each is governed by a specific law.

Tackling

A player may tackle an opposing player who has the ball by holding them while bringing them to ground. Tacklers cannot tackle above the shoulder (the neck and head are out of bounds), and the tackler has to attempt to wrap their arms around the player being tackled to complete the tackle. It is illegal to push, shoulder-charge, or to trip a player using feet or legs, but hands may be used (this being referred to as a tap-tackle or ankle-tap). Tacklers may not tackle an opponent who has jumped to catch a ball until the player has landed.

Rucking and Mauling

Mauls occur after a player with the ball has come into contact with an opponent but the handler remains on his feet; once any combination of at least three players have bound themselves a maul has been set. A ruck is similar to the maul, but in this case the ball has gone to ground with at least three attacking players binding themselves on the ground in an attempt to secure the ball.

When the ball leaves the side of the field, a line-out is awarded against the team which last touched the ball. Forward players from each team line up a metre apart, perpendicular to the touchline and between from the touchline. The ball is thrown from the touchline down the centre of the lines of forwards by a player (usually the hooker) from the team that did not play the ball into touch. The exception to this is when the ball went out from a penalty, in which case the side who gained the penalty throws the ball in.

Both sides compete for the ball and players may lift their teammates. A jumping player cannot be tackled until they stand and only shoulder-to-shoulder contact is allowed; deliberate infringement of this law is dangerous play, and results in a penalty kick.
A scrum is a way of restarting the game safely and fairly after a minor infringement. It is awarded when the ball has been knocked or passed forward, if a player takes the ball over their own try line and puts the ball down, when a player is accidentally offside or when the ball is trapped in a ruck or maul with no realistic chance of being retrieved. A team may also opt for a scrum if awarded a penalty.

A scrum is formed by the eight forwards from each team crouching down and binding together in three rows, before interlocking with the opposing team. For each team, the front row consists of two props (loosehead and tighthead) either side of the hooker. The two props are typically amongst the strongest players on the team. The second row consists of two locks and the two flankers. Behind the second row is the number 8. This formation is known as the 3–4–1 formation. Once a scrum is formed the scrum-half from the team awarded the "feed" rolls the ball into the gap between the two front-rows known as the "tunnel". The two hookers then compete for possession by hooking the ball backwards with their feet, while each pack tries to push the opposing pack backwards to help gain possession. The side that wins possession can either keep the ball under their feet while driving the opposition back, in order to gain ground, or transfer the ball to the back of the scrum where it can be picked up by the number 8 or by the scrum-half.

There are three match officials: a referee, and two assistant referees. The referees are commonly addressed as "Sir". The latter, formerly known as touch judges, had the primary function of indicating when the ball had gone into "touch"; their role has been expanded and they are now expected to assist the referee in a number of areas, such as watching for foul play and checking offside lines. In addition, for matches in high level competitions, there is often a television match official (TMO; popularly called the "video referee"), to assist with certain decisions, linked up to the referee by radio. The referees have a system of hand signals to indicate their decisions.

Common offences include tackling above the shoulders, collapsing a scrum, ruck or maul, not releasing the ball when on the ground, or being offside. The non-offending team has a number of options when awarded a penalty: a "tap" kick, when the ball is kicked a very short distance from hand, allowing the kicker to regather the ball and run with it; a punt, when the ball is kicked a long distance from hand, for field position; a place-kick, when the kicker will attempt to score a goal; or a scrum. Players may be sent off (signalled by a red card) or temporarily suspended ("sin-binned") for ten minutes (yellow card) for foul play or repeated infringements, and may not be replaced.

Occasionally, infringements are not caught by the referee during the match and these may be "cited" by the citing commissioner after the match and have punishments (usually suspension for a number of weeks) imposed on the infringing player.

During the match, players may be replaced (for injury) or substituted (for tactical reasons). A player who has been replaced may not rejoin play unless he was temporarily replaced to have bleeding controlled; a player who has been substituted may return temporarily, to replace a player who has a blood injury or has suffered a concussion, or permanently, if he is replacing a front-row forward. In international matches, eight replacements are allowed; in domestic or cross-border tournaments, at the discretion of the responsible national union(s), the number of replacements may be nominated to a maximum of eight, of whom three must be sufficiently trained and experienced to provide cover for the three front row positions.

Prior to 2016, all substitutions, no matter the cause, counted against the limit during a match. In 2016, World Rugby changed the law so that substitutions made to replace a player deemed unable to continue due to foul play by the opposition would no longer count against the match limit. This change was introduced in January of that year in the Southern Hemisphere and June in the Northern Hemisphere.

The most basic items of equipment for a game of rugby union are the ball itself, a rugby shirt (also known as a "jersey"), rugby shorts, socks, and boots. The rugby ball is oval in shape (technically a prolate spheroid), and is made up of four panels. The ball was historically made of leather, but in the modern era most games use a ball made from a synthetic material. World Rugby lays out specific dimensions for the ball, in length, in circumference of length and in circumference of width. Rugby boots have soles with studs to allow grip on the turf of the pitch. The studs may be either metal or plastic but must not have any sharp edges or ridges.

Protective equipment is optional and strictly regulated. The most common items are mouthguards, which are worn by almost all players, and are compulsory in some rugby-playing nations. Other protective items that are permitted include head gear; thin (not more than 10 mm thick), non-rigid shoulder pads and shin guards; which are worn underneath socks. Bandages or tape can be worn to support or protect injuries; some players wear tape around the head to protect the ears in scrums and rucks. Female players may also wear chest pads. Although not worn for protection, some types of fingerless mitts are allowed to aid grip.

It is the responsibility of the match officials to check players' clothing and equipment before a game to ensure that it conforms to the laws of the game.

The international governing body of rugby union (and associated games such as sevens) is World Rugby (WR). The WR headquarters are in Dublin, Ireland. WR, founded in 1886, governs the sport worldwide and publishes the game's laws and rankings. As of February 2014, WR (then known as the IRB, for International Rugby Board) recorded 119 unions in its membership, 101 full members and 18 associate member countries. According to WR, rugby union is played by men and women in over 100 countries. WR controls the Rugby World Cup, the Women's Rugby World Cup, Rugby World Cup Sevens, HSBC Sevens Series, HSBC Women's Sevens Series, World Under 20 Championship, World Under 20 Trophy, Nations Cup and the Pacific Nations Cup. WR holds votes to decide where each of these events are to be held, except in the case of the Sevens World Series for men and women, for which WR contracts with several national unions to hold individual events.

Six regional associations, which are members of WR, form the next level of administration; these are:

SANZAAR (South Africa, New Zealand, Australia and Argentina Rugby) is a joint venture of the South African Rugby Union, New Zealand Rugby, Rugby Australia and the Argentine Rugby Union (UAR) that operates Super Rugby and The Rugby Championship (formerly the Tri Nations before the entry of Argentina). Although UAR initially had no representation on the former SANZAR board, it was granted input into the organisation's issues, especially with regard to The Rugby Championship, and became a full SANZAAR member in 2016 (when the country entered Super Rugby).

National unions oversee rugby union within individual countries and are affiliated to WR. Since 2016, the WR Council has 40 seats. A total of 11 unions—the eight foundation unions of Scotland, Ireland, Wales, England, Australia, New Zealand, South Africa and France, plus Argentina, and —have two seats each. In addition, the six regional associations have two seats each. Four more unions—, , and the USA—have one seat each. Finally, the Chairman and Vice Chairman, who usually come from one of the eight foundation unions (although the current Vice Chairman, Agustín Pichot, is with the non-foundation Argentine union) have one vote each.

The earliest countries to adopt rugby union were England, the country of inception, and the other three Home Nations, Scotland, Ireland and Wales. The spread of rugby union as a global sport has its roots in the exporting of the game by British expatriates, military personnel, and overseas university students.
The first rugby club in France was formed by British residents in Le Havre in 1872, while the next year Argentina recorded its first game: 'Banks' v 'City' in Buenos Aires.

Seven countries have adopted rugby union as their de facto national sport; they are Fiji, Georgia, Madagascar, New Zealand, Samoa, Tonga and Wales.

A rugby club was formed in Sydney, New South Wales, Australia in 1864; while the sport was said to have been introduced to New Zealand by Charles Monro in 1870, who played rugby while a student at Christ's College, Finchley.

Several island nations have embraced the sport of rugby. Rugby was first played in Fiji circa 1884 by European and Fijian soldiers of the Native Constabulary at Ba on Viti Levu island. Fiji then sent their first overseas team to Samoa in 1924, who in turn set up their own union in 1927. Along with Tonga, other countries to have national rugby teams in Oceania include the Cook Islands, Niue, Papua New Guinea and Solomon Islands.

In North America a club formed in Montreal in 1868, Canada's first club. The city of Montreal also played its part in the introduction of the sport in the United States, when students of McGill University played against a team from Harvard University in 1874.

Although the exact date of arrival of rugby union in Trinidad and Tobago is unknown, their first club Northern RFC was formed in 1923, a national team was playing by 1927 and due to a cancelled tour to British Guiana in 1933, switched their venue to Barbados; introducing rugby to the island. Other Atlantic countries to play rugby union include Jamaica and Bermuda.

The growth of rugby union in Europe outside the 6 Nations countries in terms of playing numbers has been sporadic. Historically, British and Irish home teams played the Southern Hemisphere teams of Australia, New Zealand, and South Africa, as well as France. The rest of Europe were left to play amongst themselves. During a period when it had been isolated by the British and Irish Unions, France, lacking international competition, became the only European team from the top tier to regularly play the other European countries; mainly Belgium, the Netherlands, Germany, Spain, Romania, Poland, Italy and Czechoslovakia. In 1934, instigated by the French Rugby Federation, FIRA (Fédération Internationale de Rugby Amateur) was formed to organise rugby union outside the authority of the IRFB. The founding members were , , , , , and .

Other European rugby playing nations of note include Russia, whose first officially recorded match is marked by an encounter between Dynamo Moscow and the Moscow Institute of Physical Education in 1933. Rugby union in Portugal also took hold between the First and Second World Wars, with a Portuguese National XV set up in 1922 and an official championship started in 1927.

In 1999, FIRA agreed to place itself under the auspices of the IRB, transforming itself into a strictly European organising body. Accordingly, it changed its name to FIRA–AER (Fédération Internationale de Rugby Amateur – Association Européenne de Rugby). It adopted its current name of Rugby Europe in 2014.

Although Argentina is the best-known rugby playing nation in South America, founding the Argentine Rugby Union in 1899, several other countries on the continent have a long history. Rugby had been played in Brazil since the end of the 19th century, but the game was played regularly only from 1926, when São Paulo beat Santos in an inter-city match. It took Uruguay several aborted attempts to adapt to rugby, led mainly by the efforts of the Montevideo Cricket Club; these efforts succeeded in 1951 with the formation of a national league and four clubs. Other South American countries that formed a rugby union include Chile (1948), and Paraguay (1968).

Many Asian countries have a tradition of playing rugby dating from the British Empire. India began playing rugby in the early 1870s, the Calcutta Football Club forming in 1873. However, with the departure of a local British army regiment, interest in rugby diminished in the area. In 1878, The Calcutta Football Club was disbanded, and rugby in India faltered. Sri Lanka claims to have founded their union in 1878, and although little official information from the period is available, the team won the All-India cup in Madras in 1920. The first recorded match in Malaysia was in 1892, but the first confirmation of rugby is the existence of the "HMS Malaya Cup" which was first presented in 1922 and is still awarded to the winners of the Malay sevens.

Rugby union was introduced to Japan in 1899 by two Cambridge students: Ginnosuke Tanaka and Edward Bramwell Clarke. The Japan RFU was founded in 1926 and its place in rugby history was cemented with the news that Japan will host the 2019 World Cup. It will be the first country outside the Commonwealth, Ireland and France to host the event, and this is viewed by the IRB as an opportunity for rugby union to extend its reach, particularly in Asia. Other Asian playing countries of note include Singapore, South Korea, China and The Philippines, while the former British colony of Hong Kong is notable within rugby for its development of the rugby sevens game, especially the Hong Kong Sevens tournament which was founded in 1976.

Rugby in the Middle East and the Gulf States has its history in the 1950s, with clubs formed by British and French Services stationed in the region after the Second World War. When these servicemen left, the clubs and teams were kept alive by young professionals, mostly Europeans, working in these countries. The official union of Oman was formed in 1971. Bahrain founded its union a year later, while in 1975 the Dubai Sevens, the Gulf's leading rugby tournament, was created. Rugby remains a minority sport in the region with Israel and the United Arab Emirates, as of 2019, being the only member union from the Middle East to be included in the IRB World Rankings.

In 1875, rugby was introduced to South Africa by British soldiers garrisoned in Cape Town. The game spread quickly across the country, displacing Winchester College football as the sport of choice in South Africa and spreading to nearby Zimbabwe. South African settlers also brought the game with them to Namibia and competed against British administrators in British East Africa. During the late 19th and early 20th century, the sport in Africa was spread by settlers and colonials who often adopted a "whites-only" policy to playing the game. This resulted in rugby being viewed as a bourgeois sport by the indigenous people with limited appeal.. Despite this enclaves of black participation developed notably in the Eastern Cape and in Harare. The earliest countries to see the playing of competitive rugby include South Africa, and neighbouring Rhodesia (modern-day Zimbabwe), which formed the Rhodesia Rugby Football Union in 1895 and became a regular stop for touring British and New Zealand sides.

In more recent times the sport has been embraced by several African nations. In the early 21st century Madagascar has experienced crowds of 40,000 at national matches, while Namibia, whose history of rugby can be dated from 1915, have qualified for the final stages of the World Cup four times since 1999. Other African nations to be represented in the World Rugby Rankings as Member Unions include Côte d'Ivoire, Kenya, Uganda and Zambia. South Africa and Kenya are among the 15 "core teams" that participate in every event of the men's World Rugby Sevens Series.

Records of women's rugby football date from the late 19th century, with the first documented source being Emily Valentine's writings, in which she states that she set up a rugby team in Portora Royal School in Enniskillen, Ireland in 1887. Although there are reports of early women's matches in New Zealand and France, one of the first notable games to prove primary evidence was the 1917 war-time encounter between Cardiff Ladies and Newport Ladies; a photo of which shows the Cardiff team before the match at the Cardiff Arms Park. Since the 1980s, the game has grown in popularity among female athletes, and by 2010, according to World Rugby, women's rugby was being played in over 100 countries.

The English-based Women's Rugby Football Union (WRFU), responsible for women's rugby in England, Scotland, Ireland, and Wales, was founded in 1983, and is the oldest formally organised national governing body for women's rugby. This was replaced in 1994 by the Rugby Football Union for Women (RFUW) in England with each of the other Home Nations governing their own countries.

The premier international competition in rugby union for women is the Women's Rugby World Cup, first held in 1991; from 1994 through 2014, it was held every four years. After the 2014 event, the tournament was brought forward a year to 2017 to avoid clashing with other sporting cycles, in particular the Rugby World Cup Sevens competition. The Women's Rugby World Cup returned to a four-year cycle after 2017, with future competitions to be held in the middle year of the men's World Cup cycle.

The most important competition in rugby union is the Rugby World Cup, a men's tournament that has taken place every four years since the inaugural event in 1987. South Africa are the reigning champions, having defeated England in the final of the 2019 Rugby World Cup in Yokohama. New Zealand and South Africa have each won the title three times (New Zealand: 1987, 2011, 2015; South Africa: 1995, 2007, 2019), Australia have won twice (1991 and 1999), and England once (2003). England is the only team from the Northern Hemisphere to have won the Rugby World Cup.

The Rugby World Cup has continued to grow since its inception in 1987. The Rugby League World Cup dates from 1954 in contrast. The first tournament, in which 16 teams competed for the title, was broadcast to 17 countries with an accumulated total of 230 million television viewers. Ticket sales during the pool stages and finals of the same tournament was less than a million. The 2007 World Cup was contested by 94 countries with ticket sales of 3,850,000 over the pool and final stage. The accumulated television audience for the event, then broadcast to 200 countries, was a claimed 4.2 billion.

The 2019 Rugby World Cup took place in Japan between 20 September and 2 November. It was the ninth edition and the first time the tournament has been held in Asia.

Major international competitions are the Six Nations Championship and The Rugby Championship, held in Europe and the Southern Hemisphere respectively.

The Six Nations is an annual competition involving the European teams , , , , and . Each country plays the other five once. Following the first internationals between England and Scotland, Ireland and Wales began competing in the 1880s, forming the "Home International Championships". France joined the tournament in the 1900s and in 1910 the term "Five Nations" first appeared. However, the Home Nations (England, Ireland, Scotland, and Wales) excluded France in 1931 amid a run of poor results, allegations of professionalism and concerns over on-field violence. France then rejoined in 1939–1940, though World War II halted proceedings for a further eight years. France has played in all the tournaments since WWII, the first of which was played in 1947. In 2000, Italy became the sixth nation in the contest and Rome's Stadio Olimpico has replaced Stadio Flaminio as the venue for their home games since 2013. The current Six Nations champions are Wales.

The Rugby Championship is the Southern Hemisphere's annual international series for that region's top national teams. From its inception in 1996 through 2011, it was known as the Tri Nations, as it featured the hemisphere's traditional powers of Australia, New Zealand and South Africa. These teams have dominated world rankings in recent years, and many considered the Tri Nations to be the toughest competition in international rugby. The Tri Nations was initially played on a home and away basis with the three nations playing each other twice.

In 2006 a new system was introduced where each nation plays the others three times, though in 2007 and 2011 the teams played each other only twice, as both were World Cup years. Since Argentina's strong performances in the 2007 World Cup, after the 2009 Tri Nations tournament, SANZAR (South Africa, New Zealand and Australian Rugby) invited the Argentine Rugby Union (UAR) to join an expanded Four Nations tournament in 2012. The competition has been officially rechristened as The Rugby Championship beginning with the 2012 edition. The competition reverted to the Tri Nations' original home-and-away format, but now involving four teams. In World Cup years, an abbreviated tournament is held in which each team plays the others only once. 

Rugby union was played at the Olympic Games in 1900, 1908, 1920 and 1924. As per Olympic rules, the nations of Scotland, Wales and England were not allowed to play separately as they are not sovereign states. In 1900, France won the gold, beating Great Britain 27 points to 8 and defeating Germany 27 points to 17. In 1908, Australia defeated Great Britain, claiming the gold medal, the score being 32 points to three. In 1920, the United States, fielding a team with many players new to the sport of rugby, upset France in a shock win, eight points to zero. In 1924, the United States again defeated France 17 to 3, becoming the only team to win gold twice in the sport.

In 2009 the International Olympic Committee voted with a majority of 81 to 8 that rugby union be reinstated as an Olympic sport in at least the 2016 and 2020 games, but in the sevens, 4-day tournament format. This is something the rugby world has aspired to for a long time and Bernard Lapasset, president of the International Rugby Board, said the Olympic gold medal would be considered to be "the pinnacle of our sport" (Rugby Sevens).

Rugby sevens has been played at the Commonwealth Games since the 1998 Games in Kuala Lumpur. The most gold medal holders are New Zealand who have won the competition on four successive occasions until South Africa beat them in 2014. Rugby union has also been an Asian Games event since the 1998 games in Bangkok, Thailand. In the 1998 and 2002 editions of the games, both the usual fifteen-a-side variety and rugby sevens were played, but from 2006 onwards, only rugby sevens was retained. In 2010, the women's rugby sevens event was introduced. The event is likely to remain a permanent fixture of the Asian Games due to elevation of rugby sevens as an Olympic sport from the 2016 Olympics onwards. The present gold medal holders in the sevens tournament, held in 2014, are Japan in the men's event and China in the women's.

Women's international rugby union began in 1982, with a match between France and the Netherlands played in Utrecht. As of 2009 over six hundred women's internationals have been played by over forty different nations.

The first Women's Rugby World Cup was held in Wales in 1991, and was won by the United States. The second tournament took place in 1994, and from that time through 2014 was held every four years. The New Zealand Women's team then won four straight World Cups (1998, 2002, 2006, 2010) before England won in 2014. Following the 2014 event, World Rugby moved the next edition of the event to 2017, with a new four-year cycle from that point forward. New Zealand are the current World Cup holders.

As well as the Women's Rugby World Cup there are also other regular tournaments, including a Six Nations, run in parallel to the men's competition. The Women's Six Nations, first played in 1996 has been dominated by England, who have won the tournament on 14 occasions, including a run of seven consecutive wins from 2006 to 2012. However, since then, England have won only in 2017; reigning champion France have won in each even-numbered year (2014, 2016, 2018) whilst Ireland won in 2013 and 2015.

Rugby union has been professionalised since 1995. The following table shows professional and semi-professional rugby union competitions.

Rugby union has spawned several variants of the full-contact, 15-a-side game. The two most common differences in adapted versions are fewer players and reduced player contact.

The oldest variant is rugby sevens (sometimes 7s or VIIs), a fast-paced game which originated in Melrose, Scotland in 1883. In rugby sevens, there are only seven players per side, and each half is normally seven minutes. Major tournaments include the Hong Kong Sevens and Dubai Sevens, both held in areas not normally associated with the highest levels of the 15-a-side game.

A more recent variant of the sport is rugby tens (10s or Xs), a Malaysian invention with ten players per side.

Touch rugby, in which "tackles" are made by simply touching the ball carrier with two hands, is popular both as a training game and more formally as a mixed sex version of the sport played by both children and adults.

Several variants have been created to introduce the sport to children with a less physical contact. Mini rugby is a version aimed at fostering the sport in children. It is played with only eight players and on a smaller pitch.

Tag Rugby is a version in which the players wear a belt with two tags attached by velcro, the removal of either counting as a 'tackle'. Tag Rugby also varies in that kicking the ball is not allowed. Similar to Tag Rugby, American Flag Rugby, (AFR), is a mixed gender, non-contact imitation of rugby union designed for American children entering grades K-9. Both American Flag Rugby and Mini Rugby differ to Tag Rugby in that they introduce more advanced elements of rugby union as the participants age.

Other less formal variants include beach rugby and snow rugby.

Rugby league was formed after the Northern Union broke from the Rugby Football Union in a disagreement over payment to players. It went on to change its laws and became a football code in its own right. The two sports continue to influence each other to this day.

American football and Canadian football are derived from early forms of rugby football.

Australian rules football was influenced by rugby football and other games originating in English public schools.

James Naismith took aspects of many sports including rugby to invent basketball. The most obvious contribution is the jump ball's similarity to the line-out as well as the underhand shooting style that dominated the early years of the sport. Naismith played rugby at McGill University.

Swedish football was a code whose rules were a mix of Association and Rugby football rules.

Rugby lends its name to wheelchair rugby, a full-contact sport which contains elements of rugby such as crossing a try line with the ball to score.

According to a 2011 report by the Centre for the International Business of Sport, over four and a half million people play rugby union or one of its variants organised by the IRB. This is an increase of 19 percent since the previous report in 2007. The report also claimed that since 2007 participation has grown by 33 percent in Africa, 22 percent in South America and 18 percent in Asia and North America. In 2014 the IRB published a breakdown of the total number of players worldwide by national unions. It recorded a total of 6.6 million players globally, of those, 2.36 million were registered members playing for a club affiliated to their country's union. The 2016 World Rugby Year in Review reported 8.5 million players, of which 3.2 million were registered union players and 1.9 million were registered club players; 22% of all players were female.

The most capped international player from the tier 1 nations is former New Zealand openside flanker and captain Richie McCaw who has played in 148 internationals. While the top scoring tier 1 international player is New Zealand's Dan Carter, who has amassed 1442 points during his career. In April 2010 Lithuania which is a second tier rugby nation, broke the record of consecutive international wins for second tier rugby nations. In 2016, the All Blacks of New Zealand set the new record 18 consecutive test wins among tier 1 rugby nations, bettering their previous consecutive run of 17. This record was equalled by England on 11 March 2017 with a win over Scotland at Twickenham. The highest scoring international match between two recognised unions was Hong Kong's 164–13 victory over Singapore on 27 October 1994. While the largest winning margin of 152 points is held by two countries, Japan (a 155–3 win over Chinese Taipei) and Argentina (152–0 over Paraguay) both in 2002.

The record attendance for a rugby union game was set on 15 July 2000 in which New Zealand defeated Australia 39–35 in a Bledisloe Cup game at Stadium Australia in Sydney before 109,874 fans. The record attendance for a match in Europe of 104,000 (at the time a world record) was set on 1 March 1975 when Scotland defeated Wales 12–10 at Murrayfield in Edinburgh during the 1975 Five Nations Championship. The record attendance for a domestic club match is 99,124, set when Racing 92 defeated Toulon in the 2016 Top 14 final on 24 June at Camp Nou in Barcelona. The match had been moved from its normal site of Stade de France near Paris due to scheduling conflicts with France's hosting of UEFA Euro 2016.

Thomas Hughes 1857 novel "Tom Brown's Schooldays", set at Rugby School, includes a rugby football match, also portrayed in the 1940s film of the same name. James Joyce mentions Irish team Bective Rangers in several of his works, including "Ulysses" (1922) and "Finnegans Wake" (1939), while his 1916 semi-autobiographical work "A Portrait of the Artist as a Young Man" has an account of Ireland international James Magee. Sir Arthur Conan Doyle, in his 1924 Sherlock Holmes tale "The Adventure of the Sussex Vampire", mentions that Dr Watson played rugby for Blackheath.

Henri Rousseau's 1908 work "Joueurs de football" shows two pairs of rugby players competing. Other French artists to have represented the sport in their works include Albert Gleizes' "Les Joueurs de football" (1912), Robert Delaunay's "Football. L'Équipe de Cardiff" (1916) and André Lhote's "Partie de Rugby" (1917). The 1928 Gold Medal for Art at the Antwerp Olympics was won by Luxembourg's Jean Jacoby for his work "Rugby".

In film, Ealing Studios' 1949 comedy "A Run for Your Money" and the 1979 BBC Wales television film "Grand Slam" both centre on fans attending a match. Films that explore the sport in more detail include independent production "Old Scores" (1991) and "Forever Strong" (2008). "Invictus" (2009), based on John Carlin's book "Playing the Enemy", explores the events of the 1995 Rugby World Cup and Nelson Mandela's attempt to use the sport to connect South Africa's people post-apartheid.

In public art and sculpture there are many works dedicated to the sport. There is a 27 ft bronze statue of a rugby line-out by pop artist Gerald Laing at Twickenham and one of rugby administrator Sir Tasker Watkins at the Millennium Stadium. Rugby players to have been honoured with statues include Gareth Edwards in Cardiff and Danie Craven in Stellenbosch.






</doc>
<doc id="25406" url="https://en.wikipedia.org/wiki?curid=25406" title="Rugby World Cup">
Rugby World Cup

The Rugby World Cup is a men's rugby union tournament contested every four years between the top international teams. The tournament was first held in 1987, when the tournament was co-hosted by New Zealand and Australia.

The winners are awarded the Webb Ellis Cup, named after William Webb Ellis, the Rugby School pupil who, according to a popular legend, invented rugby by picking up the ball during a football game. Four countries have won the trophy; New Zealand and South Africa three times, Australia twice, and England once. South Africa are the current champions, having defeated England in the final of the 2019 tournament in Japan.

The tournament is administered by World Rugby, the sport's international governing body. Sixteen teams were invited to participate in the inaugural tournament in 1987, however since 1999 twenty teams have taken part. Japan hosted the 2019 Rugby World Cup and France will host the next in 2023.

On 21 August 2019, World Rugby announced that gender designations would be removed from the titles of the men's and women's World Cups. Accordingly, all future World Cups for men and women will officially bear the "Rugby World Cup" name. The first tournament to be affected by the new policy will be the next women's tournament to be held in New Zealand in 2021, which will officially be titled as "Rugby World Cup 2021".

Qualifying tournaments were introduced for the second tournament, where eight of the sixteen places were contested in a twenty-four-nation tournament. The inaugural World Cup in 1987, did not involve any qualifying process; instead, the 16 places were automatically filled by seven eligible International Rugby Football Board (IRFB, now World Rugby) member nations, and the rest by invitation.

In 2003 and 2007, the qualifying format allowed for eight of the twenty available positions to be filled by automatic qualification, as the eight quarter-finalists of the previous tournament enter its successor. The remaining twelve positions were filled by continental qualifying tournaments. Positions were filled by three teams from the Americas, one from Asia, one from Africa, three from Europe and two from Oceania. Another two places were allocated for repechage. The first repechage place was determined by a match between the runners-up from the Africa and Europe qualifying tournaments, with that winner then playing the Americas runner-up to determine the place. The second repechage position was determined between the runners-up from the Asia and Oceania qualifiers.

The current format allows for 12 of the 20 available positions to be filled by automatic qualification, as the teams who finish third or better in the group (pool) stages of the previous tournament enter its successor (where they will be seeded). The qualification system for the remaining eight places is region-based, with a total eight teams allocated for Europe, five for Oceania, three for the Americas, two for Africa, and one for Asia. The last place is determined by an intercontinental play-off.

The 2015 tournament involved twenty nations competing over six weeks. There were two stages, a pool and a knockout. Nations were divided into four pools, A through to D, of five nations each. The teams were seeded before the start of the tournament, with the seedings taken from the World Rankings in December 2012. The four highest-ranked teams were drawn into pools A to D. The next four highest-ranked teams were then drawn into pools A to D, followed by the next four. The remaining positions in each pool were filled by the qualifiers.

Nations play four pool games, playing their respective pool members once each. A bonus points system is used during pool play. If two or more teams are level on points, a system of criteria is used to determine the higher ranked; the sixth and final criterion decides the higher rank through the official World Rankings.

The winner and runner-up of each pool enter the knockout stage. The knockout stage consists of quarter- and semi-finals, and then the final. The winner of each pool is placed against a runner-up of a different pool in a quarter-final. The winner of each quarter-final goes on to the semi-finals, and the respective winners proceed to the final. Losers of the semi-finals contest for third place, called the 'Bronze Final'. If a match in the knockout stages ends in a draw, the winner is determined through extra time. If that fails, the match goes into sudden death and the next team to score any points is the winner. As a last resort, a kicking competition is used.

Prior to the Rugby World Cup, there was no truly global rugby union competition, but there were a number of other tournaments. One of the oldest is the annual Six Nations Championship, which started in 1883 as the Home Nations Championship, a tournament between England, Ireland, Scotland and Wales. It expanded to the Five Nations in 1910, when France joined the tournament. France did not participate from 1931 to 1939, during which period it reverted to a Home Nations championship. In 2000, Italy joined the competition, which became the Six Nations.

Rugby union was also played at the Summer Olympic Games, first appearing at the 1900 Paris games and subsequently at London in 1908, Antwerp in 1920, and Paris again in 1924. France won the first gold medal, then Australasia, with the last two being won by the United States. However rugby union ceased to be on Olympic program after 1924.

The idea of a Rugby World Cup had been suggested on numerous occasions going back to the 1950s, but met with opposition from most unions in the IRFB. The idea resurfaced several times in the early 1980s, with the Australian Rugby Union (ARU; now known as Rugby Australia) in 1983, and the New Zealand Rugby Union (NZRU; now known as New Zealand Rugby) in 1984 independently proposing the establishment of a world cup. A proposal was again put to the IRFB in 1985 and this time passed 10–6. The delegates from Australia, France, New Zealand and South Africa all voted for the proposal, and the delegates from Ireland and Scotland against; the English and Welsh delegates were split, with one from each country for and one against.

The inaugural tournament, jointly hosted by Australia and New Zealand, was held in May and June 1987, with sixteen nations taking part. New Zealand became the first-ever champions, defeating France 29–9 in the final. The subsequent 1991 tournament was hosted by England, with matches played throughout Britain, Ireland and France. This tournament saw the introduction of a qualifying tournament; eight places were allocated to the quarter-finalists from 1987, and the remaining eight decided by a thirty-five nation qualifying tournament. Australia won the second tournament, defeating England 12–6 in the final.

In 1992, eight years after their last official series, South Africa hosted New Zealand in a one-off test match. The resumption of international rugby in South Africa came after the dismantling of the apartheid system, and was only done with permission of the African National Congress. With their return to test rugby, South Africa were selected to host the 1995 Rugby World Cup. After upsetting Australia in the opening match, South Africa continued to advance through the tournament until they met New Zealand in the final. After a tense final that went into extra time, South Africa emerged 15–12 winners, with then President Nelson Mandela, wearing a Springbok jersey, presenting the trophy to South Africa's captain, Francois Pienaar.

The tournament in 1999 was hosted by Wales with matches also being held throughout the rest of the United Kingdom, Ireland and France. The tournament included a repechage system, alongside specific regional qualifying places, and an increase from sixteen to twenty participating nations. Australia claimed their second title, defeating France in the final.

The 2003 event was hosted by Australia, although it was originally intended to be held jointly with New Zealand. England emerged as champions defeating Australia in extra time. England's win was unique in that it broke the southern hemisphere's dominance in the event. Such was the celebration of England's victory that an estimated 750,000 people gathered in central London to greet the team, making the day the largest sporting celebration of its kind ever in the United Kingdom.

The 2007 competition was hosted by France, with matches also being held in Wales and Scotland. South Africa claimed their second title by defeating defending champions England 15–6. The 2011 tournament was awarded to New Zealand in November 2005, ahead of bids from Japan and South Africa. The All Blacks reclaimed their place atop the rugby world with a narrow 8–7 win over France in the 2011 final.

In the 2015 edition of tournament, hosted by England, New Zealand once again won the final, this time against established rivals, Australia. In doing so, they became the first team in World Cup history to win three titles, as well as the first to successfully defend a title. It was also New Zealand's first title victory on foreign soil.

The 2019 World Cup, hosted by Japan, saw South Africa claim their third trophy to match New Zealand for the most Rugby World Cup titles. South Africa defeated England 32–12 in the final.

The Webb Ellis Cup is the prize presented to winners of the Rugby World Cup, named after William Webb Ellis. The trophy is also referred to simply as the "Rugby World Cup". The trophy was chosen in 1987 as an appropriate cup for use in the competition, and was created in 1906 by Garrard's Crown Jewellers. The trophy is restored after each game by fellow Royal Warrant holder Thomas Lyte. The words 'The International Rugby Football Board' and 'The Webb Ellis Cup' are engraved on the face of the cup. It stands thirty-eight centimetres high and is silver gilded in gold, and supported by two cast scroll handles, one with the head of a satyr, and the other a head of a nymph. In Australia the trophy is colloquially known as "Bill" — a reference to William Webb Ellis.

Tournaments are organised by Rugby World Cup Ltd (RWCL), which is itself owned by World Rugby. The selection of host is decided by a vote of World Rugby Council members. The voting procedure is managed by a team of independent auditors, and the voting kept secret. The allocation of a tournament to a host nation is now made five or six years prior to the commencement of the event, for example New Zealand were awarded the 2011 event in late 2005.

The tournament has been hosted by multiple nations. For example, the 1987 tournament was co-hosted by Australia and New Zealand. World Rugby requires that the hosts must have a venue with a capacity of at least 60,000 spectators for the final. Host nations sometimes construct or upgrade stadia in preparation for the World Cup, such as Millennium Stadium – purpose built for the 1999 tournament – and Eden Park, upgraded for 2011. The first country outside of the traditional rugby nations of SANZAAR or the Six Nations to be awarded the hosting rights was 2019 host Japan. France will host the 2023 tournament.

Organizers of the Rugby World Cup, as well as the Global Sports Impact, state that the Rugby World Cup is the third largest sporting event in the world, behind only the FIFA World Cup and the Olympics, although other sources question whether this is accurate.

Reports emanating from World Rugby and its business partners have frequently touted the tournament's media growth, with cumulative worldwide television audiences of 300 million for the inaugural 1987 tournament, 1.75 billion in 1991, 2.67 billion in 1995, 3 billion in 1999, 3.5 billion in 2003, and 4 billion in 2007. The 4 billion figure was widely dismissed as the global audience for television is estimated to be about 4.2 billion.

However, independent reviews have called into question the methodology of those growth estimates, pointing to factual inconsistencies. The event's supposed drawing power outside of a handful of rugby strongholds was also downplayed significantly, with an estimated 97 percent of the 33 million average audience produced by the 2007 final coming from Australasia, South Africa, the British Isles and France. Other sports have been accused of exaggerating their television reach over the years; such claims are not exclusive to the Rugby World Cup.

While the event's global popularity remains a matter of dispute, high interest in traditional rugby nations is well documented. The 2003 final, between Australia and England, became the most watched rugby union match in the history of Australian television.

†Typhoon Hagibis caused 3 group stage matches to be cancelled. As a result, only 45 of the scheduled 48 matches were played in the 2019 Rugby World Cup.

Notes:

Twenty-five nations have participated at the Rugby World Cup (excluding qualifying tournaments). The only nations to host and win a tournament are New Zealand (1987 and 2011) and South Africa (1995). The performance of other host nations includes England (1991 final hosts) and Australia (2003 hosts) both finishing runners-up, while France (2007 hosts) finished fourth, and Wales (1999 hosts) and Japan (2019 hosts) reached the quarter-finals. Wales became the first host nation to be eliminated at the pool stages in 1991 while England became the first solo host nation to be eliminated at the pool stages in 2015. Of the twenty-five nations that have participated in at least one tournament, eleven of them have never missed a tournament.

 South Africa was excluded from the first two tournaments due to a sporting boycott during the apartheid era.

The record for most points overall is held by English player Jonny Wilkinson, who scored 277 during his World Cup career. New Zealand All Black Grant Fox holds the record for most points in one competition, with 126 in 1987; Jason Leonard of England holds the record for most World Cup matches: 22 between 1991 and 2003. All Black Simon Culhane holds the record for most points in a match by one player, 45, as well as the record for most conversions in a match, 20. All Black Marc Ellis holds the record for most tries in a match, six, which he scored against Japan in 1995.

New Zealand All Black Jonah Lomu is the youngest player to appear in a final – aged 20 years and 43 days at the 1995 Final. Lomu (playing in two tournaments) and South African Bryan Habana (playing in three tournaments) share the record for most total World Cup tournament tries, both scoring 15. Lomu (in 1999) and Habana (in 2007) also share the record, along with All Black Julian Savea (in 2015), for most tries in a tournament, with 8 each. South Africa's Jannie de Beer kicked five drop-goals against England in 1999 – an individual record for a single World Cup match. The record for most penalties in a match is 8, held by Australian Matt Burke, Argentinian Gonzalo Quesada, Scotland's Gavin Hastings and France's Thierry Lacroix, with Quesada also holding the record for most penalties in a tournament, with 31.

The most points scored in a game is 145, by the All Blacks against Japan in 1995, while the widest winning margin is 142, held by Australia in a match against Namibia in 2003.

A total of 16 players have been sent off (red carded) in the tournament. Welsh lock Huw Richards was the first, while playing against New Zealand in 1987. No player has been red carded more than once.





</doc>
<doc id="25407" url="https://en.wikipedia.org/wiki?curid=25407" title="Recursion">
Recursion

Recursion (adjective: "recursive") occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.

In mathematics and computer science, a class of objects or methods exhibits recursive behavior when it can be defined by two properties:

For example, the following is a recursive definition of a person's "ancestor". One's ancestor is either:

The Fibonacci sequence is another classic example of recursion:

formula_1

formula_2

formula_3

Many mathematical axioms are based upon recursive rules. For example, the formal definition of the natural numbers by the Peano axioms can be described as: "Zero is a natural number, and each natural number has a successor, which is also a natural number." By this base case and recursive rule, one can generate the set of all natural numbers.

Other recursively defined mathematical objects include factorials, functions (e.g., recurrence relations), sets (e.g., Cantor ternary set), and fractals.

There are various more tongue-in-cheek definitions of recursion; see recursive humor.

Recursion is the process a procedure goes through when one of the steps of the procedure involves invoking the procedure itself. A procedure that goes through recursion is said to be 'recursive'.

To understand recursion, one must recognize the distinction between a procedure and the running of a procedure. A procedure is a set of steps based on a set of rules, while the running of a procedure involves actually following the rules and performing the steps.

Recursion is related to, but not the same as, a reference within the specification of a procedure to the execution of some other procedure.

When a procedure is defined as such, this immediately creates the possibility of an endless loop; recursion can only be properly used in a definition if the step in question is skipped in certain cases so that the procedure can complete.

But even if it is properly defined, a recursive procedure is not easy for humans to perform, as it requires distinguishing the new from the old, partially executed invocation of the procedure; this requires some administration as to how far various simultaneous instances of the procedures have progressed. For this reason, recursive definitions are very rare in everyday situations.

Linguist Noam Chomsky, among many others, has argued that the lack of an upper bound on the number of grammatical sentences in a language, and the lack of an upper bound on grammatical sentence length (beyond practical constraints such as the time available to utter one), can be explained as the consequence of recursion in natural language.

This can be understood in terms of a recursive definition of a syntactic category, such as a sentence. A sentence can have a structure in which what follows the verb is another sentence: "Dorothy thinks witches are dangerous", in which the sentence "witches are dangerous" occurs in the larger one. So a sentence can be defined recursively (very roughly) as something with a structure that includes a noun phrase, a verb, and optionally another sentence. This is really just a special case of the mathematical definition of recursion.

This provides a way of understanding the creativity of language—the unbounded number of grammatical sentences—because it immediately predicts that sentences can be of arbitrary length: "Dorothy thinks that Toto suspects that Tin Man said that...". There are many structures apart from sentences that can be defined recursively, and therefore many ways in which a sentence can embed instances of one category inside another. Over the years, languages in general have proved amenable to this kind of analysis.

Recently, however, the generally accepted idea that recursion is an essential property of human language has been challenged by Daniel Everett on the basis of his claims about the Pirahã language. Andrew Nevins, David Pesetsky and Cilene Rodrigues are among many who have argued against this. Literary self-reference can in any case be argued to be different in kind from mathematical or logical recursion.

Recursion plays a crucial role not only in syntax, but also in natural language semantics. The word "and", for example, can be construed as a function that can apply to sentence meanings to create new sentences, and likewise for noun phrase meanings, verb phrase meanings, and others. It can also apply to intransitive verbs, transitive verbs, or ditransitive verbs. In order to provide a single denotation for it that is suitably flexible, "and" is typically defined so that it can take any of these different types of meanings as arguments. This can be done by defining it for a simple case in which it combines sentences, and then defining the other cases recursively in terms of the simple one.

A recursive grammar is a formal grammar that contains recursive production rules.

Recursion is sometimes used humorously in computer science, programming, philosophy, or mathematics textbooks, generally by giving a circular definition or self-reference, in which the putative recursive step does not get closer to a base case, but instead leads to an infinite regress. It is not unusual for such books to include a joke entry in their glossary along the lines of:

A variation is found on page 269 in the index of some editions of Brian Kernighan and Dennis Ritchie's book "The C Programming Language"; the index entry recursively references itself ("recursion 86, 139, 141, 182, 202, 269"). Early versions of this joke can be found in "Let's talk Lisp" by Laurent Siklóssy (published by Prentice Hall PTR on December 1, 1975 with a copyright date of 1976) and in "Software Tools" by Kernighan and Plauger (published by Addison-Wesley Professional on January 11, 1976). The joke also appears in "The UNIX Programming Environment" by Kernighan and Pike. It did not appear in the first edition of "The C Programming Language". The joke is part of the Functional programming folklore and was already widespread in the functional programming community before the publication of the aforementioned books.

Another joke is that "To understand recursion, you must understand recursion." In the English-language version of the Google web search engine, when a search for "recursion" is made, the site suggests "Did you mean: "recursion"." An alternative form is the following, from Andrew Plotkin: ""If you already know what recursion is, just remember the answer. Otherwise, find someone who is standing closer to Douglas Hofstadter than you are; then ask him or her what recursion is.""

Recursive acronyms are other examples of recursive humor. PHP, for example, stands for "PHP Hypertext Preprocessor", WINE stands for "WINE Is Not an Emulator" GNU stands for "GNU's not Unix", and SPARQL denotes the "SPARQL Protocol and RDF Query Language".

The canonical example of a recursively defined set is given by the natural numbers:

In mathematical logic, the Peano axioms (or Peano postulates or Dedekind–Peano axioms), are axioms for the natural numbers presented in the 19th century by the German mathematician Richard Dedekind and by the Italian mathematician Giuseppe Peano. The Peano Axioms define the natural numbers referring to a recursive successor function and addition and multiplication as recursive functions.

Another interesting example is the set of all "provable" propositions in an axiomatic system that are defined in terms of a proof procedure which is inductively (or recursively) defined as follows:


Finite subdivision rules are a geometric form of recursion, which can be used to create fractal-like images. A subdivision rule starts with a collection of polygons labelled by finitely many labels, and then each polygon is subdivided into smaller labelled polygons in a way that depends only on the labels of the original polygon. This process can be iterated. The standard `middle thirds' technique for creating the Cantor set is a subdivision rule, as is barycentric subdivision.

A function may be recursively defined in terms of itself. A familiar example is the Fibonacci number sequence: "F"("n") = "F"("n" − 1) + "F"("n" − 2). For such a definition to be useful, it must be reducible to non-recursively defined values: in this case "F"(0) = 0 and "F"(1) = 1.

A famous recursive function is the Ackermann function, which, unlike the Fibonacci sequence, cannot be expressed without recursion.

Applying the standard technique of proof by cases to recursively defined sets or functions, as in the preceding sections, yields structural induction — a powerful generalization of mathematical induction widely used to derive proofs in mathematical logic and computer science.

Dynamic programming is an approach to optimization that restates a multiperiod or multistep optimization problem in recursive form. The key result in dynamic programming is the Bellman equation, which writes the value of the optimization problem at an earlier time (or earlier step) in terms of its value at a later time (or later step).

In set theory, this is a theorem guaranteeing that recursively defined functions exist. Given a set "X", an element "a" of "X" and a function formula_7, the theorem states that there is a unique function formula_8 (where formula_4 denotes the set of natural numbers including zero) such that
for any natural number "n".

Take two functions formula_8 and formula_13 such that:

where "a" is an element of "X".

It can be proved by mathematical induction that formula_18 for all natural numbers "n":

By induction, formula_18 for all formula_25.

A common method of simplification is to divide a problem into subproblems of the same type. As a computer programming technique, this is called divide and conquer and is key to the design of many important algorithms. Divide and conquer serves as a top-down approach to problem solving, where problems are solved by solving smaller and smaller instances. A contrary approach is dynamic programming. This approach serves as a bottom-up approach, where problems are solved by solving larger and larger instances, until the desired size is reached.

A classic example of recursion is the definition of the factorial function, given here in C code:

unsigned int factorial(unsigned int n) {

The function calls itself recursively on a smaller version of the input and multiplies the result of the recursive call by , until reaching the base case, analogously to the mathematical definition of factorial.

Recursion in computer programming is exemplified when a function is defined in terms of simpler, often smaller versions of itself. The solution to the problem is then devised by combining the solutions obtained from the simpler versions of the problem. One example application of recursion is in parsers for programming languages. The great advantage of recursion is that an infinite set of possible sentences, designs or other data can be defined, parsed or produced by a finite computer program.

Recurrence relations are equations which define one or more sequences recursively. Some specific kinds of recurrence relation can be "solved" to obtain a non-recursive definition (e.g., a closed-form expression).

Use of recursion in an algorithm has both advantages and disadvantages. The main advantage is usually the simplicity of instructions. The main disadvantage is that the memory usage of recursive algorithms may grow very quickly, rendering them impractical for larger instances.

Shapes that seem to have been created by recursive processes sometimes appear in plants and animals, e.g. in branching structures where one large part branches out to two or more similar smaller parts. One example is Romanesco broccoli.

The Russian Doll or Matryoshka doll is a physical artistic example of the recursive concept.

Recursion has been used in paintings since Giotto's "Stefaneschi Triptych", made in 1320. Its central panel contains the kneeling figure of Cardinal Stefaneschi, holding up the triptych itself as an offering.

M. C. Escher's "Print Gallery" (1956) is a print which depicts a distorted city containing a gallery which recursively contains the picture, and so "ad infinitum".




</doc>
<doc id="25408" url="https://en.wikipedia.org/wiki?curid=25408" title="Robert Byrd">
Robert Byrd

Robert Carlyle Byrd (born Cornelius Calvin Sale Jr.; November 20, 1917June 28, 2010) was an American politician who served as a United States Senator from West Virginia for over 51 years, from 1959 until his death in 2010. A member of the Democratic Party, Byrd previously served as a U.S. Representative from 1953 until 1959. He is the longest-serving U.S. Senator in history, was the longest-serving member in the history of the United States Congress, until surpassed by Representative John Dingell of Michigan; the last remaining member of the U.S. Senate to have served during the presidency of Dwight Eisenhower; and the last remaining member of Congress to have served during the presidency of Harry S. Truman. Byrd is also the only West Virginian to have served in both chambers of the state legislature and both chambers of Congress.

Byrd served in the West Virginia House of Delegates from 1947 to 1950, and the West Virginia State Senate from 1950 to 1952. Initially elected to the United States House of Representatives in 1952, Byrd served there for six years before being elected to the Senate in 1958. He rose to become one of the Senate's most powerful members, serving as secretary of the Senate Democratic Caucus from 1967 to 1971 and—after defeating his longtime colleague, Ted Kennedy—as Senate Majority Whip from 1971 to 1977. Over the next three decades, Byrd led the Democratic caucus in numerous roles depending on whether his party held control of the Senate, including Senate Majority Leader, Senate Minority Leader, President pro tempore of the United States Senate and President pro tempore emeritus. As President pro tempore—a position he held four times in his career—he was third in the line of presidential succession, after the Vice President and the Speaker of the House of Representatives.

Serving three different tenures as Chairman of the United States Senate Committee on Appropriations enabled Byrd to steer a great deal of federal money toward projects in West Virginia. Critics derided his efforts as pork barrel spending, while Byrd argued that the many federal projects he worked to bring to West Virginia represented progress for the people of his state. He filibustered against the 1964 Civil Rights Act and supported the Vietnam War, but later renounced racism and segregation, and spoke in opposition to the Iraq War. Renowned for his knowledge of Senate precedent and parliamentary procedure, Byrd wrote a four-volume history of the Senate in later life.

Near the end of his life, Byrd was in declining health and was hospitalized several times. He died in office on June 28, 2010, at the age of 92. Byrd is the oldest member of Congress to die in office. He was buried at Columbia Gardens Cemetery in Arlington, Virginia.

Robert Byrd was born on November 20, 1917, as Cornelius Calvin Sale Jr. in North Wilkesboro, North Carolina, to Cornelius Calvin Sale and his wife Ada Mae (Kirby). When he was ten months old, his mother died in the 1918 flu pandemic. In accordance with his mother's wishes, his father dispersed their children among relatives. Calvin Jr. was adopted by his aunt and uncle, Titus and Vlurma Byrd, who changed his name to Robert Carlyle Byrd and raised him in the coal-mining region of southern West Virginia, primarily in the coal town of Stotesbury, West Virginia.

Byrd was educated in the public schools of Stotesbury. He was the valedictorian of his 1934 graduating class at Stotesbury's Mark Twain High School.

On May 29, 1936, Byrd married Erma Ora James (June 12, 1917 – March 25, 2006) who was born to a coal mining family in Floyd County, Virginia. Her family moved to Raleigh County, West Virginia, where she met Byrd when they attended the same high school.

Robert Byrd had two daughters (Mona Byrd Fatemi and Marjorie Byrd Moore), six grandchildren, and seven great-grandchildren.

In the early 1940s, Byrd recruited 150 of his friends and associates to create a new chapter of the Ku Klux Klan in Sophia, West Virginia.

As a young boy, Byrd had witnessed his adoptive father walk in a Klan parade in Matoaka, West Virginia. While growing up, Byrd had heard that "the Klan defended the American way of life against racemixers and communists". He then wrote to Joel L. Baskin, Grand Dragon of the Realm of Virginia, West Virginia, Maryland and Delaware, who responded that he would come and organize a chapter when Byrd had recruited 150 people. Byrd’s house couldn't fit 150 people, so he arranged to hold the ceremony at the home of C.M. “Clyde” Goodwin, a former law enforcement officer who lived in Crab Orchard, West Virginia.

It was Baskin who told Byrd, "You have a talent for leadership, Bob ... The country needs young men like you in the leadership of the nation." Byrd later recalled, "Suddenly lights flashed in my mind! Someone important had recognized my abilities! I was only 23 or 24 years old, and the thought of a political career had never really hit me. But strike me that night, it did." Byrd became a recruiter and leader of his chapter. When it came time to elect the top officer (Exalted Cyclops) in the local Klan unit, Byrd won unanimously.

In December 1944, Byrd wrote to segregationist Mississippi Senator Theodore G. Bilbo:

In 1946, Byrd wrote a letter to a Grand Wizard stating, "The Klan is needed today as never before, and I am anxious to see its rebirth here in West Virginia and in every state in the nation." However, when running for the United States House of Representatives in 1952, he announced "After about a year, I became disinterested, quit paying my dues, and dropped my membership in the organization. During the nine years that have followed, I have never been interested in the Klan." He said he had joined the Klan because he felt it offered excitement and was anti-communist.

Byrd later called joining the KKK "the greatest mistake I ever made." In 1997, he told an interviewer he would encourage young people to become involved in politics but also warned, "Be sure you avoid the Ku Klux Klan. Don't get that albatross around your neck. Once you've made that mistake, you inhibit your operations in the political arena." In his last autobiography, Byrd explained that he was a KKK member because he "was sorely afflicted with tunnel vision—a jejune and immature outlook—seeing only what I wanted to see because I thought the Klan could provide an outlet for my talents and ambitions." Byrd also said in 2005, "I know now I was wrong. Intolerance had no place in America. I apologized a thousand times ... and I don't mind apologizing over and over again. I can't erase what happened."

Byrd worked as a gas station attendant, a grocery store clerk, a shipyard welder during World War II, and a butcher before he won a seat in the West Virginia House of Delegates in 1946, representing Raleigh County from 1947 to 1950. Byrd became a local celebrity after a radio station in Beckley began broadcasting his "fiery fundamentalist lessons." In 1950, he was elected to the West Virginia Senate, where he served from December 1950 to December 1952.

In 1951, Byrd was among the official witnesses of the execution of Harry Burdette and Fred Painter, which was the first use of the electric chair in West Virginia. In 1965 the state abolished capital punishment, with the last execution having occurred in 1959.

Early in his career Byrd attended Beckley College, Concord College, Morris Harvey College, Marshall College, and George Washington University Law School, and joined the Tau Kappa Epsilon fraternity.

Byrd began night classes at American University Washington College of Law in 1953, while a member of the United States House of Representatives. He earned his J.D. "cum laude" a decade later, by which time he was a U.S. Senator. President John F. Kennedy spoke at the commencement ceremony on June 10, 1963 and presented the graduates their diplomas, including Byrd. Byrd completed law school in an era when undergraduate degrees were not a requirement. He later decided to complete his Bachelor of Arts degree in political science, and in 1994 he graduated "summa cum laude" from Marshall University.

In 1952, Byrd was elected to the United States House of Representatives for West Virginia's 6th congressional district, succeeding E. H. Hedrick, who retired from the House to make an unsuccessful run for the Democratic nomination for Governor. Byrd was re-elected twice from this district, anchored in Charleston and also including his home in Sophia, serving from January 3, 1953 to January 3, 1959. Byrd defeated Republican incumbent W. Chapman Revercomb for the United States Senate in 1958. Revercomb's record supporting civil rights had become an issue, playing in Byrd's favor. Byrd was re-elected to the Senate eight times. He was West Virginia's junior senator for his first four terms; his colleague from 1959 to 1985 was Jennings Randolph, who had been elected on the same day as Byrd's first election in a special election to fill the seat of the late Senator Matthew Neely.
While Byrd faced some vigorous Republican opposition in his career, his last serious electoral opposition occurred in 1982 when he was challenged by freshman Congressman Cleve Benedict. Despite his tremendous popularity in the state, Byrd ran unopposed only once, in 1976. On three other occasions – in 1970, 1994 and 2000 – he won all 55 of West Virginia's counties. In his re-election bid in 2000, he won all but seven precincts. Congresswoman Shelley Moore Capito, the daughter of one of Byrd's longtime foes, former governor Arch Moore Jr., briefly considered a challenge to Byrd in 2006 but decided against it. Capito's district covered much of the territory Byrd had represented in the U.S. House.

In the 1960 Democratic presidential election primaries, Byrd – a close Senate ally of Lyndon B. Johnson – endorsed and campaigned for Hubert Humphrey over front-runner John F. Kennedy in the state's crucial primary. However, Kennedy won the state's primary and eventually the general election.

Byrd was elected to a record ninth consecutive full Senate term on November 7, 2006. He became the longest-serving senator in American history on June 12, 2006, surpassing Strom Thurmond of South Carolina with 17,327 days of service. On November 18, 2009, Byrd became the longest-serving member in congressional history, with 56 years, 320 days of combined service in the House and Senate, passing Carl Hayden of Arizona. Previously, Byrd had held the record for the longest unbroken tenure in the Senate (Thurmond resigned during his first term and was re-elected seven months later). He is the only senator ever to serve more than 50 years. Including his tenure as a state legislator from 1947 to 1953, Byrd's service on the political front exceeded 60 continuous years. Byrd, who never lost an election, cast his 18,000th vote on June 21, 2007, the most of any senator in history. John Dingell broke Byrd's record as longest-serving member of Congress on June 7, 2013.

Upon the death of former Florida Senator George Smathers on January 20, 2007, Byrd became the last living United States Senator from the 1950s.

Having taken part in the admission of Alaska and Hawaii to the union, Byrd was the last surviving senator to have voted on a bill granting statehood to a U.S. territory. At the time of Byrd's death, fourteen sitting or former members of the Senate had not been born when Byrd's tenure in the Senate began, President Barack Obama among them.

These are the committee assignments for Sen. Byrd's 9th and final term.

Byrd was a member of the wing of the Democratic Party that opposed federally-mandated desegregation and civil rights. However, despite his early career in the KKK, Byrd was linked to such senators as John C. Stennis, J. William Fulbright and George Smathers, who based their segregationist positions on their view of states' rights in contrast to senators like James Eastland, who held a reputation as a committed racist.

Byrd joined with Democratic senators to filibuster the Civil Rights Act of 1964, personally filibustering the bill for 14 hours, a move he later said he regretted. Despite an 83-day filibuster in the Senate, both parties in Congress voted overwhelmingly in favor of the Act (Democrats 47–16, Republicans 30–2), and President Johnson signed the bill into law. Byrd cast no vote on the Voting Rights Act of 1965, and voted against the confirmation of Thurgood Marshall to the U.S. Supreme Court. He did not sign the 1956 Southern Manifesto and voted for the Civil Rights Acts of 1957, 1960, and 1968, as well as the 24th Amendment to the U.S. Constitution. In 2005, Byrd told "The Washington Post" that his membership in the Baptist church led to a change in his views. In the opinion of one reviewer, Byrd, like other Southern and border-state Democrats, came to realize that he would have to temper "his blatantly segregationist views" and move to the Democratic Party mainstream if he wanted to play a role nationally.

In February 1968, Byrd questioned General Earle Wheeler during the latter's testimony to the Senate Armed Services Committee. During a White House meeting between President Johnson and congressional Democratic leaders on February 6, Byrd stated his concern for the ongoing Vietnam War, citing the US's lack of intelligence, preparation, underestimating of the morale and vitality of the Viet Cong, and overestimated how backed Americans would be by South Vietnam.

President Johnson rejected Byrd's observations. "Anyone can kick a barn down. It takes a good carpenter to build one."

During the 1968 Democratic Party presidential primaries, Byrd supported the incumbent President Johnson. Of the challenging Robert F. Kennedy, Byrd said, "Bobby-come-lately has made a mistake. I won't even listen to him. There are many who liked his brother—as Bobby will find out—but who don't like him." Byrd praised Chicago Mayor Richard J. Daley's police response to protest activity at that year's Democratic National Convention, stating that the violence that resulted was the fault of the protesters, while the police only tried to restore order. Vice President Hubert Humphrey won the presidential nomination, and Byrd campaigned for him that fall.

Byrd served in the Senate Democratic leadership. He succeeded George Smathers as secretary of the Senate Democratic Conference from 1967 to 1971. He unseated Ted Kennedy in 1971 to become majority whip, or the second highest-ranking Democrat, until 1977. Smathers recalled that, "Ted was off playing. While Ted was away at Christmas, down in the islands, floating around having a good time with some of his friends, male and female, here was Bob up here calling on the phone. 'I want to do this, and would you help me?' He had it all committed so that when Teddy got back to town, Teddy didn't know what hit him, but it was already all over. That was Lyndon Johnson's style. Bob Byrd learned that from watching Lyndon Johnson." Byrd himself had told Smathers that " I have never in my life played a game of cards. I have never in my life had a golf club in my hand. I have never in life hit a tennis ball. I have—believe it or not—never thrown a line over to catch a fish. I don't do any of those things. I have only had to work all my life. And every time you told me about swimming, I don't know how to swim."

In 1976, Byrd was the "favorite son" Presidential candidate in West Virginia's primary. His easy victory gave him control of the delegation to the Democratic National Convention. Byrd had the inside track as majority whip but focused most of his time running for majority leader, more so than for re-election to the Senate, as he was virtually unopposed for his fourth term. By the time the vote for majority leader came, his lead was so secure that his lone rival, Minnesota's Hubert Humphrey, withdrew before the balloting took place. From 1977 to 1989 Byrd was the leader of the Senate Democrats, serving as majority leader from 1977 to 1981 and 1987 to 1989, and as minority leader from 1981 to 1987.

Byrd was known for steering federal dollars to West Virginia, one of the country's poorest states. He was called the "King of Pork" by Citizens Against Government Waste. After becoming chair of the Appropriations Committee in 1989, Byrd set a goal securing a total of for public works in the state. He passed that mark in 1991, and funds for highways, dams, educational institutions, and federal agency offices flowed unabated over the course of his membership. More than 30 existing or pending federal projects bear his name. He commented on his reputation for attaining funds for projects in West Virginia in August 2006, when he called himself "Big Daddy" at the dedication for the Robert C. Byrd Biotechnology Science Center. Examples of this ability to claim funds and projects for his state include the Federal Bureau of Investigation's repository for computerized fingerprint records as well as several United States Coast Guard computing and office facilities.

Byrd was also known for using his knowledge of parliamentary procedure. Byrd frustrated Republicans with his encyclopedic knowledge of the inner workings of the Senate, particularly prior to the Reagan Revolution. From 1977 to 1979 he was described as "performing a procedural tap dance around the minority, outmaneuvering Republicans with his mastery of the Senate's arcane rules." In 1988, majority leader Byrd moved a call of the Senate, which was adopted by the majority present, in order to have the Sergeant-at-Arms arrest members not in attendance. One member (Robert Packwood, R-Oregon) was escorted back to the chamber by the Sergeant-at-Arms in order to obtain a quorum.

As the longest-serving Democratic senator, Byrd served as President pro tempore four times when his party was in the majority: from 1989 until the Republicans won control of the Senate in 1995; for 17 days in early 2001, when the Senate was evenly split between parties and outgoing Vice President Al Gore broke the tie in favor of the Democrats; when the Democrats regained the majority in June 2001 after Senator Jim Jeffords of Vermont left the Republican Party to become an independent; and again from 2007 to his death in 2010, as a result of the 2006 Senate elections. In this capacity, Byrd was third in the line of presidential succession at the time of his death, behind Vice President Joe Biden and House Speaker Nancy Pelosi.

In 1969, Byrd launched a Scholastic Recognition Award; he also began to present a savings bond to valedictorians from high schools—public and private—in West Virginia. In 1985 Congress approved the nation's only merit-based scholarship program funded through the U.S. Department of Education, a program which Congress later named in Byrd's honor. The Robert C. Byrd Honors Scholarship Program initially comprised a one-year, $1,500 award to students with "outstanding academic achievement" who had been accepted at a college or university. In 1993, the program began providing four-year scholarships.

In 2002 Byrd secured unanimous approval for a major national initiative to strengthen the teaching of "traditional American history" in K-12 public schools. The Department of Education competitively awards $50 to a year to school districts (in amounts of about $500,000 to ). The money goes to teacher training programs that are geared to improving the knowledge of history teachers. The Continuing Appropriations Act, 2011 eliminated funding for the Robert C. Byrd Honors Scholarship Program.

Television cameras were first introduced to the House of Representatives on March 19, 1979, by C-SPAN. Unsatisfied that Americans only saw Congress as the House of Representatives, Byrd and others pushed to televise Senate proceedings to prevent the Senate from becoming the "invisible branch" of government, succeeding in June 1986.

To help introduce the public to the inner workings of the legislative process, Byrd launched a series of one hundred speeches based on his examination of the Roman Republic and the intent of the Framers. Byrd published a four-volume series on Senate history: "The Senate: 1789–1989: Addresses on the History of the Senate". The first volume won the Henry Adams Prize of the Society for History in the Federal Government as "an outstanding contribution to research in the history of the Federal Government." He also published "The Senate of the Roman Republic: Addresses on the History of Roman Constitutionalism".

In 2004, Byrd received the American Historical Association's first Theodore Roosevelt-Woodrow Wilson Award for Civil Service; in 2007, Byrd received the Friend of History Award from the Organization of American Historians. Both awards honor individuals outside the academy who have made a significant contribution to the writing and/or presentation of history. In 2014, The Byrd Center for Legislative Studies began assessing the archiving of Senator Byrd's electronic correspondence and floor speeches in order to preserve these documents and make them available to the wider community.

On July 19, 2007, Byrd gave a 25-minute speech in the Senate against dog fighting, in response to the indictment of football player Michael Vick.

For 2007, Byrd was deemed the fourteenth-most powerful senator, as well as the twelfth-most powerful Democratic senator.

On May 19, 2008, Byrd endorsed then-Senator Barack Obama for president. One week after the West Virginia Democratic Primary, in which Hillary Clinton defeated Obama by 67 to 25 percent, Byrd said, "Barack Obama is a noble-hearted patriot and humble Christian, and he has my full faith and support." When asked in October 2008 about the possibility that the issue of race would influence West Virginia voters, as Obama is African American, Byrd replied, "Those days are gone. Gone!" Obama lost West Virginia (by 13%) but won the election.

On January 26, 2009, Byrd was one of three Democrats to vote against the confirmation of Timothy Geithner as United States Secretary of the Treasury (along with Russ Feingold of Wisconsin and Tom Harkin of Iowa).

On February 26, 2009, Byrd was one of two Democrats to vote against the District of Columbia House Voting Rights Act of 2009, which if it had become law would have added a voting seat in the United States House of Representatives for the District of Columbia and add a seat for Utah, explaining that he supported the intent of the legislation, but regarded it as an attempt to solve with legislation an issue which required resolution with a Constitutional amendment. (Democrat Max Baucus of Montana also cast a "nay" vote.)

Although his health was poor, Byrd was present for every crucial vote during the December 2009 Senatorial healthcare debate; his vote was necessary so Democrats could obtain cloture to break a Republican filibuster. At the final vote on December 24, 2009, Byrd referenced recently deceased Senator Ted Kennedy, a devoted proponent, when casting his vote: "Mr. President, this is for my friend Ted Kennedy! Aye!"

Byrd initially compiled a mixed record on the subjects of race relations and desegregation. While he initially voted against civil rights legislation, in 1959 he hired one of the Capitol's first black congressional aides, and he also took steps to integrate the United States Capitol Police for the first time since Reconstruction. Beginning in the 1970s, Byrd explicitly renounced his earlier views in favor of racial segregation. Byrd said that he regretted filibustering and voting against the Civil Rights Act of 1964 and would change it if he had the opportunity. Byrd also said that his views changed dramatically after his teenage grandson was killed in a 1982 traffic accident, which put him in a deep emotional valley. "The death of my grandson caused me to stop and think," said Byrd, adding he came to realize that African Americans love their children as much as he does his. During debate in 1983 over the passage of the law creating the Martin Luther King Jr. Day holiday, Byrd grasped the symbolism of the day and its significance to his legacy, telling members of his staff "I'm the only one in the Senate who must vote for this bill".

Of the seven U.S. Senators to vote on the confirmations of both Thurgood Marshall and Clarence Thomas to the United States Supreme Court (the others being Daniel Inouye of Hawaii, Ted Kennedy of Massachusetts, Quentin Burdick of North Dakota, Mark Hatfield of Oregon, and Fritz Hollings and Strom Thurmond of South Carolina), Byrd was the only senator to vote against confirming both of the only two African American nominees to the Court in its history. In Marshall's case, Byrd asked FBI Director J. Edgar Hoover to look into the possibility that Marshall had either connections to communists or a communist past. With respect to Thomas, Byrd stated that he was offended by Thomas's use of the phrase "high-tech lynching of uppity blacks" in his defense and that he was "offended by the injection of racism" into the hearing. He called Thomas's comments a "diversionary tactic" and said, "I thought we were past that stage." Regarding Anita Hill's sexual harassment charges against Thomas, Byrd supported Hill. Byrd joined 45 other Democrats in voting against confirming Thomas to the Supreme Court.

On March 29, 1968, Byrd criticized a Memphis, Tennessee, protest: "It was a shameful and totally uncalled for outburst of lawfulness undoubtedly encouraged to some considerable degree, at least, by his [Dr. King's] words and actions, and his presence. There is no reason for us to believe that the same destructive rioting and violence cannot, or that it will not, happen here if King attempts his so-called Poor People's March, for what he plans in Washington appears to be something on a far greater scale than what he had indicated he planned to do in Memphis."

In a March 2, 2001, interview with Tony Snow, Byrd said of race relations:
Byrd's use of the term "white nigger" created immediate controversy. When asked about it, Byrd's office provided this in a written response,
For the 2003–2004 session, the National Association for the Advancement of Colored People (NAACP) rated Byrd's voting record as being 100% in line with the NAACP's position on the thirty-three Senate bills they evaluated. Sixteen other senators received that rating. In June 2005, Byrd proposed an additional $10,000,000 in federal funding for the Martin Luther King Jr. Memorial in Washington, D.C., remarking that, "With the passage of time, we have come to learn that his Dream was the American Dream, and few ever expressed it more eloquently." Upon news of his death, the NAACP released a statement praising Byrd, saying that he "became a champion for civil rights and liberties" and "came to consistently support the NAACP civil rights agenda".

Byrd initially said that the impeachment proceedings against Clinton should be taken seriously. Although he harshly criticized any attempt to make light of the allegations, he made the motion to dismiss the charges and effectively end the matter. Even though he voted against both articles of impeachment, he was the sole Democrat to vote to censure Clinton.

Byrd strongly opposed Clinton's 1993 efforts to allow gays to serve in the military and supported efforts to limit gay marriage. In 1996, before the passage of the Defense of Marriage Act, he said, "The drive for same-sex marriage is, in effect, an effort to make a sneak attack on society by encoding this aberrant behavior in legal form before society itself has decided it should be legal. [...] Let us defend the oldest institution, the institution of marriage between male and female as set forth in the Holy Bible."

Despite his previous position, he later stated his opposition to the Federal Marriage Amendment and argued that it was unnecessary because the states already had the power to ban gay marriages. However, when the amendment came to the Senate floor, he was one of the two Democratic senators who voted in favor of cloture.

On March 11, 1982, Byrd voted against a measure sponsored by Senator Orrin Hatch that sought to reverse "Roe v. Wade" and allow Congress and individual states to adopt laws banning abortions. Its passing was the first time a congressional committee supported an anti-abortion amendment.

In 1995, Byrd voted against a ban on intact dilation and extraction, a late-term abortion procedure typically referred to by its opponents as "partial-birth abortion". In 2003, however, he voted for the Partial-Birth Abortion Ban Act, which prohibits intact dilation and extraction. Byrd also voted against the 2004 Unborn Victims of Violence Act, which recognizes a "child in utero" as a legal victim if he or she is injured or killed during the commission of a crime of violence.

In April 1970, the Senate Judiciary Committee approved a plan to replace the Electoral College with direct elections of presidents. Byrd initially opposed direct elections on the key vote and was one of two senators to switch votes in favor of the proposal during later votes.

In April 1970, as the Senate Judiciary Committee delayed a vote on Supreme Court nominee Harry Blackmun, Byrd stated that "no nomination should be voted on within 24 hours after the hearing" after the previous two Supreme Court nominees had delays and was one of the 17 committee members who went on record of assuring Blackmun's nomination would be reported favorably to the full Senate.

In October 1970, Byrd sponsored an amendment protecting members of Congress and those elected that have not yet assumed office. Byrd mentioned the 88 political assassinations in the United States and said state law was not adequate to handle the increase in political violence.

In February 1971, after Fred R. Harris and Charles Mathias requested the Senate Rules Committee change the rules to permit selection of committee chairmen on a basis aside from seniority, Byrd indicated through his line of questioning that he saw considerable value in the seniority system.

In April 1971, after Representative Hale Boggs stated that he had been tapped by the Federal Bureau of Investigation and called on FBI Director J. Edgar Hoover to resign, Byrd opined that Boggs' imagination was involved and called on him to reveal any possible "good, substantial, bona fide evidence".

In April 1971, Byrd met with President Nixon, Hugh Scott, and Robert P. Griffin for a briefing that after which Byrd, Scott, and Griffin asserted they had been told by Nixon of his intent to withdraw American forces from Indochina by a specific date. White House Press Secretary Ronald L. Ziegler disputed their claims by stating that the three had not been told anything by Nixon he had not mentioned in his speech the same day as the meeting.

In April 1971, Jacob Javits, Fred R. Harris, and Charles H. Percy circulated letters to their fellow Senators in an attempt to gain cosponsors for a resolution to appoint the Senate's first girl pages. Byrd maintained that the Senate was ill-equipped for girl pages and was among those that cited the long hours of work, the carrying of sometimes heavy documents and the high crime rate in the Capitol area as among the reasons against it.

In September 1971, Representative Richard H. Poff was under consideration by President Nixon for a Supreme Court nomination, Byrd warning Poff that his nomination could be met with opposition by liberal senators and see a filibuster emerge. Within hours, Poff announced his declining of the nomination.

In April 1972, Senate Majority Leader Mansfield announced that he had authorized Byrd to present an amendment to the Senate for a fixed deadline for total troop withdrawal that the Nixon administration would be obligated to meet and that the measure would serve as an amendment to the State Department‐United States Information Agency authorization bill.

In April 1972, the Senate Judiciary Committee approved the nomination of Richard G. Kleindienst as United States Attorney General, Byrd being one of four Democrats to support the nomination. On June 7, Byrd announced that he would vote against Kleindienst, saying in a news release that this was Nixon's first nomination that he had not voted to confirm and that testimony at hearings investigating Kleindienst's tenure at the International Telephone and Telegraph Corporation displayed "a show of arrogance and deception and insensitivity to the people's right to know."

In a May 1972 luncheon speech, Byrd criticized American newspapers for "an increasing tendency toward shoddy technical production" and observed that there was "a greater schism between the Nixon Administration and the media, at least publicly, than at any previous time in our history."

In May 1972, Byrd introduced a proposal supported by the Nixon administration that would make cutting off all funding for American hostilities in Indochina conditional upon agreement on an internationally supervised cease‐fire. Byrd and Nixon supporters argued modification would bring the amendment more in line with President Nixon's proposal to withdraw all American forces from Vietnam the previous week and it was approved in the Senate by a vote of 47 to 43.

In September 1972, Edward Brooke attempted to reintroduce his war ending amendment that had been defeated earlier in the week as an addendum to a clean drinking water bill when he discovered that Byrd had arranged a unanimous consent free agreement prohibiting amendments that were not relevant to the subject. Brooke charged the Byrd agreements with impairing his senatorial prerogatives to introduce amendments.

During the 1972 general election campaign, Democratic nominee George McGovern advocated for partial amnesty for draft dodges. Byrd responded to the position in a November speech the day before the election without mentioning McGovern by name in saying, "How could we keep faith with the thousands of Americans we sent to Vietnam by giving a mere tap on the wrist to those who fled to Canada and Sweden?" Byrd said the welfare proposals were part of "pernicious doctrine that the Federal Government owes a living to people who don't want to work" and chastised individuals that had personal trips to Hanoi rather than official missions as "the Ramsey Clarks in our society who attempt to deal unilaterally with the enemy."

In January 1973, the Senate passed legislation containing an amendment Byrd offered requiring President Nixon to give Congress an accounting of all funds that he had impounded and appropriated by February 5. Byrd stated that President Nixon had been required to submit reports to Congress and that he had not done so since June, leaving Congress in the dark on the matter.

In February 1973, the Senate approved legislation requiring confirmation of the director and deputy director of the Office of Management and Budget in the White House in what was seen as "another battleground for the dispute between Congress and the White House over cuts in social spending programs in the current Federal budget and in the Nixon Administration's spending request for the fiscal year 1974, which begins next July 1". The legislation contained an amendment sponsored by Byrd limiting the budget officials to a maximum term of four years before having another confirmation proceeding. Byrd introduced another amendment that required all Cabinet officers be required to undergo reconfirmation by the Senate in the event that they are retained from one administration to another.

In March 1973, Byrd led Senate efforts to reject a proposal that would have made most critical committee meetings open to the public, arguing that tampering with "the rides of the Senate is to tamper with the Senate itself" and argued against changing "procedures which, over the long past, have contributed to stability and efficiency in the operation of the Senate." The Senate voted down the proposal 47 to 38 on March 7.

On May 2, 1973, the anniversary of FBI Director J. Edgar Hoover's death, Byrd called on President Nixon to appoint a permanent successor for Hoover.

In June 1973, Byrd sponsored a bill that would impose the first Tuesday in October as the date for all federal elections and mandate that states hold primary elections for federal elections between the first Tuesday in June and the first Tuesday in July. Senate Rules Committee approved the measure on June 13 and it was sent to the Senate floor for consideration.

In June 1973, along with Lloyd Bentsen, Mike Mansfield, John Tower, and Jennings Randolph, Byrd was one of five senators to switch their vote on the foreign military aid authorization bill to assure its passage after previously voting against it.

In October 1973, President Nixon vetoed the request of the United States Information Agency for 208 million for fiscal year 1974 on the grounds of a provision forcing the agency to provide any document or information demanded. Byrd introduced a bill identical to the one vetoed by Nixon the following month, differing in not containing the information provision as well as a ban on appropriating or spending more money than the annual budget called for, the Senate approving the legislation on November 13.

In November 1973, after the Senate rejected an amendment to the National Energy Emergency Act intending to direct President Nixon to put gasoline rationing into effect on January 15, Byrd indicated the final vote not coming for multiple days.

In June 1974, the Senate confirmed John C. Sawhill as Federal Energy Administrator only to rescind the confirmation hours later, the direct result of James Abourezk wanting to speak out and vote against the nomination due to the Nixon administration's refusal to roll back crude oil prices. Abourezk confirmed that he had asked Byrd for notice of when he could assume the Senate floor to deliver his remarks. Byrd was absent when present members passed the nomination as part of their efforts to clear the chamber's executive calendar and rescinded the confirmation.

In May 1974, the House Judiciary Committee opened impeachment hearings against President Nixon after the release of 1,200 pages of transcripts of White House conversations between him and his aides and the administration became engulfed in the scandal that would come to be known as Watergate. That month, Byrd delivered a speech on the Senate floor opposing Nixon's potential resignation, saying it would serve only to convince the President's supporters that his enemies had driven him out of office: "The question of guilt or innocence would never be fully resolved. The country would remain polarized — more so than it is today. And confidence in government would remain unrestored." Most of the members of the Senate in attendance for the address were conservatives from both parties that shared opposition to Nixon being removed from office. Byrd was among multiple conservative senators who stated that they would not ask Nixon to resign. Later that month, Republican Attorney General Elliot L. Richardson termed Nixon "a law and order President who says subpoenas must he answered by everyone except himself," the comment being echoed by Byrd who additionally charged President Nixon with reneging on his public pledge that the independence of the special prosecutor to pursue the Watergate investigation would not be limited without the prior approval of a majority of Congressional leaders.

On July 29, Byrd met with Senate Majority Leader Mike Mansfield, Minority Leader Hugh Scott, and Republican whip Robert P. Griffin in the first formality by Senate leaders on the matter of President Nixon's impeachment. Byrd opposed Nixon being granted immunity. "The New York Times" noted that as Chairman of the Republican National Committee George H. W. Bush issued a formal statement indicating no chance for the Nixon administration to be salvaged, Byrd was advocating for President Nixon to face some punishment for the illegal activities of the administration and that former Vice President Spiro Agnew should have been imprisoned. The Senate leadership met throughout August 7 to discuss Nixon's fate, the topic of immunity being mentioned in the office of Hugh Scott. Nixon announced his resignation the following day and resigned on August 9. The resignation led to Congress rearranging their intent from an impeachment to the confirmation of a new vice presidential nominee and the Senate scheduled a recess between August 23 to September 14, Byrd opining, "What the country needs is for all of us to get out of Washington and let the country have a breath of fresh air." By August 11, Hugh Scott announced he was finding fewer members of Congress from either party committed to criminally prosecuting former President Nixon over "Watergate", Byrd and Majority Leader Mansfield both indicating their favoring for Nixon's culpability being left in the consideration of Special Prosecutor Leon Jaworski and the "Watergate" grand jury.

On November 22, 1974, the Senate Rules Committee voted unanimously to recommend the nomination of Nelson Rockefeller as Vice President of the United States to the full Senate. Byrd admitted that he had preferred sending the nomination with no recommendation but was worried the act would apply prejudice to the nominee.

In January 1975, after President Ford requested $300 million in additional military aid for South Vietnam and $222 million more for Cambodia from Congress, Byrd said Ford and Secretary of State Henry Kissinger had described the aid as "imperative" and that congressional leaders had been told North Vietnam would take over Saigon "little by little" if additional ammunition and other aid were not provided by the US to Saigon. In February, along with Mike Mansfield, Hugh Scott, and Robert P. Griffin, Byrd was one of four senators to sponsor a compromise modification of the Senate's filibuster rule where three-fifths of the total Senate membership would be adequate in invoking closure on any measure except a change in the Senate's rules. In March, while the Senate voted on reforming its filibuster rule, James B. Allen and other senators used their allotted time to speak at length and also force a series of votes. In response, Byrd said the group was engaging in an "exercise in futility" and that the chamber had already made up its mind. In April, after President Ford and his administration's lawyers contended that Ford had authority as president to use troops under the War Powers Act, Byrd and Thomas F. Eagleton objected by charging that Ford was establishing a dangerous precedent. Byrd issued a statement on the Senate floor admitting his "serious reservations" pertaining to the Ford administration's intent to bring roughly 130,000 South Vietnamese refugees to the United States, citing cultural differences and unemployment as raising "grave doubts about the wisdom of bringing any sizable number of evacuees here." In May, after President Ford appealed for Americans to support the resettlement of 130,000 Vietnamese and Cambodians in the US, Byrd told reporters that he believed that President Ford's request for 507 million for refugee transport and resettlement would be reduced, citing its lack of political support in the United States. In September, Byrd sponsored an amendment to the appropriations bill that if enacted would bar the education department from ordering busing to the school nearest to a pupil's home and sought to hold the Senate floor until there was an agreement among colleagues on his proposal. This failed, as the time limit for debating various proposals ran out. On November 10, Byrd met with President Ford for a discussion on the New York loan guarantee bill.

In April 1976, Byrd was one of five members of the Senate Select Committee to vote for a requirement that the proposed oversight committee would share Its jurisdiction with four committees that had authority over intelligence operations. In June, after the Senate Judiciary Committee voted to send a bill breaking up 18 large oil companies into separate production, refining and refining‐marketing entities to the Senate floor, Byrd announced his opposition to divestiture and joined Republicans Hugh Scott and Charles Mathias in confirming their votes were to report the bill. In September, Congress overrode President Ford's veto of a 56 billion appropriations bill for social services, Ford afterward telling Byrd and House Speaker Carl Albert that he would sign two bills supported by the Democrats.

Byrd was elected Majority Leader on January 4, 1977. On January 14, President Ford met with congressional leadership to announce his proposals for pay increases of high government officials, Byrd afterward telling reporters that the president had also stated his intent to recommend that the raises be linked to a code of conduct. Days later, after the Senate established a special 15‐member committee to draw up a code of ethics for senators, Byrd told reporters that he was supportive of the measure and it would be composed of eight Democrats and seven Republicans who would have until March 1 to issue a draft code that would then be subject to change by the full Senate.

In January 1977, after President-elect Carter announced his nomination of Theodore C. Sorensen to be Director of Central Intelligence, Byrd admitted to reporters that there could be difficulty securing a Senate confirmation. Conservative opposition to Sorenson's nomination led Carter to conclude that he could not be confirmed, and Carter withdrawing it without the Senate taking action.

On January 18, 1977, after the Senate established a special 15‐member committee to draw up a code of ethics for senators, Byrd and Senate Minority Leader Howard Baker announced their support for the resolution, Byrd adding that knowledge of the code of ethics being enacted in the Senate would be privy to the public, press, and members of the Senate. While eight of Carter's secretaries were confirmed within the first hours of his presidency, Byrd made an unsuccessful effort to secure a date and time limit for debate on the confirmation of F. Ray Marshall, Carter's nominee for United States Secretary of Labor.

Between January and February 1979, Byrd proposed outlawing tactics frequently used to prevent him from bringing a bill to the floor for consideration. He stated the filibuster tactics gave the Senate a bad reputation and rendered it ineffective. His proposals initially earned the opposition of Republicans and conservative Democrats until there was a compromise for the reform package to be split and have the less objectionable part come up first for consideration. The Senate passed legislation curtailing tactics that had been used in the past to continue filibusters after cloture had been invoked on February 22. In March, Byrd negotiated an agreement that a proposed amendment was referred to the Judiciary Committee and would be reported by April 10. The arrangement stated that Byrd could call up the proposed amendment any time following June 1 and his action would not be subject to a filibuster while the resolution embodying the amendment will.

In October 1977, Byrd stated his refusal to authorize the Senate dropping consideration of the natural gas legislation under any circumstances, predicting the matter would be settled in the coming days as a result of conversations with colleagues he had the night before and a growing disillusion with filibusters in place of action on legislation. Byrd added that the deregulation bill would not become law due to it being identical to the Carter administration's proposal and President Carter's prior statement that he would veto deregulation bills.

In May 1978, Byrd announced that he would not move to end a filibuster against the Carter administration's labor law revision bill until after the Memorial Day recess. The decision was seen as allowing wavering senators to not be cornered on their votes as lobbying efforts for both business and labor commenced and various opponents of the bill viewed Byrd's call as a sign of weakness toward the Carter administration. Byrd stated that his decision to wait was "to give ample time for debate on the measure" and that he was expecting the first petition to end the filibuster to come sometime following the Senate returning in June.

In March 1979, after Attorney General Griffin B. Bell named a special counsel in the Carter warehouse investigation, Byrd stated his dissatisfaction with the move in a Senate floor speech, citing the existence of legislation approved by Congress the previous year that would allow the appointment of a special prosecutor. In June, Director of Public Citizens Congress Watch Mark Green stated that President Carter had told him that Majority Leader Byrd had threatened that he would personally lead a filibuster against any attempt to extend controls on domestic oil prices. In response, Byrd press secretary Mike Willard confirmed that Byrd told President Carter he would not vote for cloture in the event of a filibuster. Days later, after the Senate voted to grant President Carter authority to set energy conservation targets for each of the 50 states and allow Carter to impose mandatory measures on any state that failed to implement a plan to meet the targets he set, Byrd reaffirmed his opposition to attempts aimed at President Carter's decision to remove price controls from crude oil produced within the United States. In November, Byrd stated that the United States did not have an alternative to coal when attempting to meet its energy needs and that the technology needed to turn coal into liquid fuel at a lower cost than that of producing gasoline had already been made available, opining that doing this would solve most environmental problems. Weeks later, Sergeant at Arms of the United States Senate F. Nordy Hoffman sent a letter to Byrd warning him to take precautions against possible attacks by religious fanatics and nationalist terrorists and advocating for senators "vary their daily routines, take different routes to and from the Senate, exchange their personalized license plates for those that provide anonymity and be generally alert to the possibility of attack." Byrd distributed the letter to the other members of the chamber of Congress. In December, the Senate voted on a Republican proposal to limit overall Government tax revenue that would also yield an annual tax cut of 39 billion to 55 billion over the course of the following four years. Republican William Roth sponsored an amendment that Byrd moved to table Senator Roth's request for a budget waiver and won by five votes. The Senate narrowly blocked the proposal. By December, congressional leadership was aiming for President Carter to sign a new synthetic fuels bill before Christmas, with Byrd wanting the bill to contain a 185 billion revenue that was achieved in a minimum tax provision. Later that month, after the Senate approved a 1.5 billion in Federal loan guarantees for the Chrysler Corporation tonight after defeating a proposal to provide emergency, Byrd confirmed that he had spoken with United States Secretary of the Treasury G. William Miller about what Byrd called "excellent" chances that the Senate would complete work on a federal loans guarantees bill for Chrysler.

In August 1980, Byrd stated that Congress was unlikely to pass a tax cut before the November elections despite the Senate being in the mood for passing one.

In July 1978, Byrd introduced and endorsed a proposal by George McGovern for an amendment to repeal the 42‐month‐old embargo on American military assistance for Turkey that also linked any future aid for that country to progress on a negotiated settlement of the Cyprus problem. The Senate approved the amendment in a vote of 57 to 42 as part of a 2.9 billion international security assistance bill. Byrd stated that every government in the NATO alliance except Greece favored repeal of the embargo.

In May 1979, Byrd stated that giving Turkey a grant should not be construed as retaliation against Greece and that aid for Turkey would improve Turkey's security in addition to that of Greece, NATO, and of American allies in the Middle East. Byrd mentioned his encouragement from the report on the Greek and Turkish Cypriot communities agreeing to resume negotiations on the island's future as well as reports that progress was also being made on the reintegration of Greece into NATO. Byrd furthered that American military installations in Turkey were "of major importance in the monitoring of Soviet strategic activities" and would have "obvious significance" in the goal of verifying compliance by the Soviet Union with the strategic arms treaty. The Senate approved the Turkey grant, to Byrd's wishes, but against that of both President Carter and the Senate Foreign Relations Committee.

On February 2, 1978, Byrd and Minority Leader Baker invited all other senators to join them in sponsoring two amendments to the Panama Canal neutrality treaty, the two party leaders sending copies of amendments recommended by the Foreign Relations Committee the previous week.

In January 1979, Byrd met with Deputy Prime Minister of China Deng Xiaoping for assurances by Deng that China hoped to unite Taiwan to the mainland by peaceful means and would fully respect "the present realities" on the island. Byrd afterward stated that his concern on the Taiwan question had been allayed. In June, Byrd opined that a decision by President Carter to not proceed with the new missile system would kill the strategic arms limitation treaty in the Senate. Byrd held meetings with Soviet leaders between July 3 to July 4. Following their conclusion, Byrd said he was still undecided on supporting the arms pact and that there had been talks on "the need on both sides for avoidance of inflammatory rhetoric which can only be counterproductive." On September 23, Byrd stated that it was possible the Senate could complete the strategic arms limitation treaty that year but a delay until the following year could result in its defeat, adding that senators might have to remain in session during Christmas to ensure the treaty was voted on before 1979's end. Byrd noted that he was opposed to the treaty being "held hostage to the Cuban situation" as American interests could be harmed in the event the treaty was defeated solely due to Soviet troops being in Cuba. In November, Byrd admitted to complaining to President Carter about Senate leadership receiving only occasional briefings about the Iranian hostage crisis and that Carter had agreed to daily consultations for Minority Leader Howard Baker, Chairman of the Foreign Relations Committee Frank Church, and ranking Republican member of the Foreign Relations Committee Jacob Javits. Byrd added that he did not disagree with the move by the Carter administration to admit Mohammad Reza Pahlavi for hospitalization and that the same action would extend to "Ayatollah Khomeini himself if he were needing medical treatment and had a terminal illness." On December 3, Byrd told reporters that the Iranian hostage crisis was making the Senate uninhabitable for a debate on the strategic arms treaty, noting that a discussion could still occur before the Senate adjourned on December 21 but that he did not believe he would call up the opportunity even if granted the chance. Days later, Byrd announced there was no chance that the Senate would take up debate on the strategic arms treaty that year while speaking to reporters, adding that he would see no harm in having the discussion on the treaty begin in January of the following year.

In July 1979, Senators Henry M. Jackson and George McGovern made comments expressing doubt on President Carter being assured as the Democratic nominee in the 1980 Presidential election. When asked about their comments by a reporter, Byrd referred to Jackson and McGovern as "two very strong voices and not at all to be considered men who have little background in politics" but stated it was too early to participate in "writing the political obituary of the President at this point." Byrd added that the powers of the presidency made it possible that Carter could have a comeback and cited the events in November and December as being telling of his prospects of achieving higher popularity.

On May 10, 1980, Byrd called for President Carter to debate Senator Ted Kennedy, who he complimented as having done a service for the US by raising key issues in his presidential campaign. On August 2, Byrd advocated for an open Democratic National Convention where the delegates were not bound to a single candidate. The endorsement was seen as a break from President Carter.
In September, Byrd said that Republican presidential nominee Ronald Reagan had made comments on the war between Iran and Iraq that were a disservice to the United States and that he was exercising "reckless political posturing" in foreign policy.

In early 1990, Byrd proposed an amendment granting special aid to coal miners who would lose their jobs in the event that Congress passed clean air legislation. Byrd was initially confident in the number of votes he needed to secure its passage being made available but this was prevented by a vote from Democrat Joe Biden who said the measure's passage would mean an assured veto by President Bush. Speaking to reporters after its defeat, Byrd stated his content with the results: "I made the supreme effort. I did everything I could and, therefore, I don't feel badly about it." The Senate passed clean air legislation within weeks of the vote on Byrd's amendment with the intent of reduction in acid rain, urban smog and toxic chemicals in the air and meeting the request by President Bush for a measure that was less costly than the initial plan while still performing the same tasks of combating clean air issues. Byrd was one of eleven senators to vote against the bill and said he "cannot vote for legislation that can bring economic ruin to communities throughout the Appalachian region and the Midwest."

In August 1990, after the Senate passed its first major campaign finance reform bill since the Watergate era that would prevent political action committees from federal campaigns, lend public money into congressional campaigns and bestow candidates vouchers for television advertising, Byrd stated that he believed the bill would "end the money chase."

Byrd authored an amendment to the National Endowment for the Arts that would bar the endowment from funding projects considered obscene such as depictions of sadomasochism, homo-eroticism, the sexual exploitation of children, or individuals engaged in sex acts while also requiring grant recipients to sign a pledge swearing their compliance with the restrictions. The October 1990 measure approved in the Senate was a bipartisan measure loosening government restrictions on art project funding and leaving courts to judge what art could be considered obscene.

President Bush nominated Clarence Thomas for the Supreme Court. In October 1991, Byrd stated his support in the credibility of Anita Hill: "I believe what she said. I did not see on that face the knotted brow of satanic revenge. I did not see a face that was contorted with hate. I did not hear a voice that was tremulous with passion. I saw the face of a woman, one of 13 in a family of Southern blacks who grew up on the farm and who belonged to the church." Byrd questioned how members of the Senate could be convinced that Thomas would serve as an objective judge when he could refuse to watch Hill's testimony against him.

In February 1992, the Senate turned down a Republican attempt sponsored by John McCain and Dan Coats to grant President Bush line-item veto authority and thereby be authorized to kill projects that he was opposed to, Byrd delivering an address defending congressional power over spending for eight hours afterward. The speech had been written by Byrd two years prior and he had at this point steered 1.5 billion to his state.

In 1992, there was an effort made to pass a constitutional amendment to ensure a balanced federal budget. Byrd called the amendment "a smokescreen that will allow lawmakers to claim action against the deficit while still postponing hard budgetary decision" and spoke to reporters on his feelings against the amendment being passed: "Once members are really informed as to the mischief this amendment could do, and the damage it could do to the country and to the Constitution. I just have faith that enough members will take a courageous stand against the amendment." The sponsor of the amendment, Paul Simon, admitted that Byrd's predicton was not off and that other senators speak "when the chairman of the Senate Appropriations Committee talks".

In a June 1992 debate, Byrd argued in favor of the United States withdrawing accepting immigrants that did not speak English, the comment being a response to a plan from the Bush administration that would enable former Soviet states to receive American assistance and allow immigrants from a variety of countries to receive welfare benefits. Byrd soon afterward apologized for the comment and said they were due to his frustration over the federal government's inability to afford several essential services.

In February 1994, the Senate passed a $10 billion spending bill that would mostly be allocated to Los Angeles, California earthquake victims and military operations abroad. Bob Dole, John Kerry, John McCain, and Russ Feingold partnered together to persuade the Senate in favor of cutting back the deficit expense. Byrd raised a procedural point to derail an attempt by Dole that would approve 50 billion in spending cuts over the following five years. McCain proposed killing highway demonstration projects with a 203 million price tag, leading Byrd to produce letters written by McCain that the latter had sent to the Appropriations Committee in 1991 in an attempt to gather highway grants for his home state of Arizona. Byrd said that McCain "is very considerate of the taxpayers when it comes to financing projects in other states, but he supports such projects in his own state."

In May 2000, Byrd and John Warner sponsored a provision threatening to withdraw American troops from Kosovo, the legislation if enacted cutting off funds for troops in Kosovo after July 1, 2001, without Congressional consent. The language would have also withheld 25 percent of the money for Kosovo in the bill unless the assertion that European countries were living up to their promises to provide reconstruction money for the province was certified by President Clinton by July 15. Byrd argued that lawmakers had never approved nor debate whether American troops should be stationed in Kosovo. The Senate Appropriations Committee approved the legislation in a vote of 23-to-3 that was said to reflect "widespread concern among lawmakers about an open-ended deployment of American soldiers".

In November 2000, Congress passed an amendment sponsored by Byrd diverting tariff revenues from the Treasury Department and instead allocating them to the industry complaining, the amount involved ranging from between 40 million and 200 million a year. The following month, Japan and the European Union led a group of countries in filing a joint complaint with the World Trade Organization to the law.

Byrd praised the nomination of John G. Roberts to fill the vacancy on the Supreme Court created by the death of Chief Justice William Rehnquist. Likewise, Byrd was one of four Democrats who supported the confirmation of Samuel Alito to replace retiring Associate Justice Sandra Day O'Connor.

Like most Democrats, Byrd opposed Bush's tax cuts and his proposals to change the Social Security program.

Byrd opposed the 2002 Homeland Security Act, which created the Department of Homeland Security, stating that the bill ceded too much authority to the executive branch.

On May 2, 2002, Byrd charged the White House with engaging in "sophomoric political antics", citing Homeland Security Advisor Tom Ridge briefing senators in another location instead of the Senate on how safe he felt the US was.

He also led the opposition to Bush's bid to win back the power to negotiate trade deals that Congress cannot amend, but lost overwhelmingly. In the 108th Congress, however, Byrd won his party's top seat on the new Homeland Security Appropriations Subcommittee.

In July 2004, Byrd released The New York Times best-selling book "Losing America: Confronting a Reckless and Arrogant Presidency", which criticized the Bush presidency and the war in Iraq.

Byrd led a filibuster against the resolution granting President George W. Bush broad power to wage a "preemptive" war against Iraq, but he could not get even a majority of his own party to vote against cloture.

Byrd was one of the Senate's most outspoken critics of the 2003 invasion of Iraq.

Byrd anticipated the difficulty of fighting an insurgency in Iraq, stating on March 13, 2003,

On March 19, 2003, when Bush ordered the invasion after receiving congressional approval, Byrd said,

Byrd also criticized Bush for his speech declaring the "end of major combat operations" in Iraq, which Bush made on the U.S.S. "Abraham Lincoln". Byrd stated on the Senate floor,

On October 17, 2003, Byrd delivered a speech expressing his concerns about the future of the nation and his unequivocal antipathy to Bush's policies. Referencing the Hans Christian Andersen children's tale "The Emperor's New Clothes", Byrd said of the president: "the emperor has no clothes." Byrd further lamented the "sheep-like" behavior of the "cowed Members of this Senate" and called on them to oppose the continuation of a "war based on falsehoods."

In April 2004, Byrd mentioned the possibility of the Bush administration violating law by its failure to inform leadership in Congress midway through 2002 about its use of emergency anti-terror dollars to begin preparations for an invasion of Iraq. Byrd stated that he had never been told of a shift in money, a charge reported in the Bob Woodward book "Plan of Attack", and its validation would mean "the administration failed to abide by the law to consult with and fully inform Congress."

Byrd accused the Bush administration of stifling dissent:

Of the more than 18,000 votes he cast as a senator, Byrd said he was proudest of his vote against the Iraq war resolution. Byrd also voted to tie a timetable for troop withdrawal to war funding.

On May 23, 2005, Byrd was one of 14 senators (who became known as the "Gang of 14") to forge a compromise on the judicial filibuster, thus securing up and down votes for many judicial nominees and ending the threat of the so-called nuclear option that would have eliminated the filibuster entirely. Under the agreement, the senators retained the power to filibuster a judicial nominee in only an "extraordinary circumstance." It ensured that the appellate court nominees (Janice Rogers Brown, Priscilla Owen and William Pryor) would receive votes by the full Senate.

In 1977, Byrd was one of five Democrats to vote against the nomination of F. Ray Marshall as United States Secretary of Labor. Marshall was opposed by conservatives in both parties because of his pro-labor positions, including support for repealing right to work laws. Marshall was confirmed and served until the end of Carter's term in 1981.

In February 1981, as the Senate voted on giving final approval to the 50 billion increase in the debt limit, Democrats initially opposed the measure as part of an effort to elicit the highest number of Republicans in support of the measure. Byrd proceeded to give a signal for Democrats that saw caucus members switch their votes in support of the increase.

President Reagan was injured during an assassination attempt in March 1981. Following the shooting, Byrd opined that the aftermath of the attempt had proven there were "holes that need to be plugged" in the constitution's handling of the presidential line of succession after a president's disability and stated his intent to introduce legislation calling for a mandatory life sentence for anyone attempting to assassinate a president, vice president, or member of Congress.

In March 1981, during a Capitol Hill interview, Byrd stated that the Reagan administration was promoting an economic package with assumptions for the national economy that might take a year for the public to see its difficulties and thereby lead to a political backlash. Byrd contented that President Reagan would win approval by Congress of 35 billion to 40 billion of the 48 billion in proposed budget cuts while having more difficulty in passing his tax-cut package, asserting Democratic opposition and some Republicans having misgivings about the approach as the reason Congress would block the plan and furthering that he would be surprised if a one-year-cut in rates lasted more than year. Byrd opined that it was time for "some tax reform" that would see loopholes closed for the rich dropped to bring in revenues and expressed belief in the likelihood of the administration dismantling existing energy programs: "Energy programs are not as catchy now as budget cuts. But if the gas lines begin to form again, or the overseas oil gets cut off, we will have lost the time, the momentum, the money. Basically, they have a wholesale dismantlement of the energy programs we spent several years creating around here."

In March 1981, during a news conference, Byrd stated that the Reagan administration had not established a coherent foreign policy. He credited conflicting statements from administration officials with having contributed to confusion in Western European capitals. Byrd also said, "We've seen these statements, and backing and filling, and the secretary of state has been kept pretty busy explaining and denying assertions and pronouncements by others, which indeed indicate that the administration has not yet got its foreign policy act together."

In May 1981, Byrd announced his support for the Reagan administration's proposed budget for the fiscal year 1982 during a weekly news conference, citing that the "people want the President to be given a chance with his budget." Byrd added that he did not believe a balanced budget would be achieved by 1984, calling the budget "a balanced budget on paper only, made up of juggled figures produced out of thin air", and charged the administration with making assumptions, his comments being seen as an indication that little opposition would amount from the Democrats to the Reagan budget.

In November 1981, as Senate leaders rejected the request of Senator Harrison A. Williams Jr. to introduce new evidence during the Senate's consideration of whether to expel him for his involvement in the Abscam case, Byrd and Majority Leader Baker informed Williams that he could have a lawyer that would have to remain wordless.

On December 2, 1981, Byrd voted in favor of an amendment to President Reagan's MX missiles proposal that would divert the silo system by $334 million as well as earmark further research for other methods that would allow giant missiles to be based. The vote was seen as a rebuff of the Reagan administration.

In February 1982, Byrd wrote a letter to President Reagan urging him to "withdraw the Administration's proposed fiscal 1983 budget, and resubmit a budget that provides for much lower deficits and makes use of more realistic assumptions", recalling his previous appeal to President Carter in 1980 amid the rise of soaring inflation rates and Carter afterward consulting with Democrats in Congress. Byrd stated that he was in favor of "a document we in Congress can work with, one based on realistic assumptions, one which shows a much clearer trend toward a balanced budget." Byrd had cautious praise for a proposal by Democrat Fritz Hollings called for a freeze on all benefit programs with the exception of food stamps, Medicare and Medicaid in addition to a freeze on military spending while eliminating a pay increase for federal employees.

In March 1982, Byrd announced he would introduce an amendment to the War Powers Act that would bar the president from being able to send combat troops to El Salvador without the approval of Congress. Byrd described the proposal as only allowing the president to act with independence in the event that Americans needed to evacuate El Salvador or if the United States was attacked. "It is my view that if Americans are to be asked to shed their blood in the jungles of El Salvador, all Americans should first have an opportunity to debate and carefully evaluate that action."

By March 1982, along with Alan Cranston, Byrd was one of two senators supporting both the measure sponsored by Henry M. Jackson and John W. Warner calling upon the United States and the Soviet Union to freeze their nuclear arsenals at "equal and sharply reduced levels" and the bill sponsored by Ted Kennedy and Mark Hatfield calling upon the two countries first to negotiate a freeze on nuclear forces at existing levels before following atomic arms reduction.

In January 1983, after President Reagan said during his 1983 State of the Union Address that he hoped for the same bipartisan support that had produced the Social Security recommendations would lead Congress during the year on other issues, Byrd and House Majority Leader Jim Wright assailed the unfairness of a six-month delay in the cost-of-living increases for Social Security recipients during a period of letting the wealthy reap the benefits of the general income tax cut for a third year. Byrd stated that he did not "want a six-month delay in Social Security while leaving in place the third year of the tax cut for upper-income people" and stated that Reagan's speech had been "'rhetorically good, but substantively lacking in measures that would deal now with the crises that millions of people are experiencing."

At the beginning of February 1983, House Democrats committed themselves "to an emergency economic assistance program that would create public service jobs, provide shelter and soup kitchens for the destitute and avert foreclosures of homes and farms." Concurrently, Byrd pledged to work with the House Democrats in developing legislation concerning jobs, proposing 5 billion to 10 billion be spent and introducing legislation intended to form a national investment corporation that would assist with underwriting faltering basic industries and starting new ones in areas of high unemployment.

In March 1984, Byrd voted against a proposed constitutional amendment authorizing periods in public school for silent prayer, and in favor of President Reagan's unsuccessful proposal for a constitutional amendment permitting organized school prayer in public schools.

In June 1984, Byrd was one of five Democrats to vote against the Lawton Chiles proposal to cease MX production for a year during study in search of a smaller and single-warhead missile. The 48 to 48 tie was broken by then-Vice President George H. W. Bush.

In September 1986, Byrd endorsed the death penalty for some drug pushers in anti-drug legislation that would order President Reagan to end drug trafficking within 45 days through using the military as a means of intercepting smugglers, and imposing the death penalty on those pushers who intentionally cause a death as part of their operations while providing funding for prevention, drug abuse treatment, and anti-drug laws enforcement that was estimated to cost 3 billion to 4 billion over three years. Byrd admitted that calling for the death penalty seemed harsh, but cautioned that children in some cases had their entire lives destroyed through using drugs and that Congress had been soft for too long without seeing a change in results.

In December 1986, Byrd announced that the Senate would convene a Watergate-type select committee to investigate the Iran-Contra affair the following year and that he had reached an agreement with Bob Dole for the committee to have six Democrats and five Republicans. Byrd and Dole disagreed on whether it was a necessity for Congress to be launched into a special session that month for the purpose of getting the investigative process moving. Naming members during December enabled participants to informally move ahead by selecting the staff and be prepared before the 100th United States Congress began.

In September 1988, in response to charges by Vice President Bush's presidential campaign that Democratic nominee Michael Dukakis was weak on defense, Byrd delivered a Senate speech in which he said that the Reagan administration "is living in a glass house when it throws a stone at the Democratic Party for its so-called Disneyland defense policies" and that the U.S. land-based missiles had grown in vulnerability due to the administration being "unable to produce an acceptable solution to make our missiles survivable." Byrd furthered, "Indeed, the Fantasyland exhibits of this White House's Defense Disneyland are loaded with the rejected systems that have been developed and discarded. If anything deserves the names 'Goofy' and 'Daffy' and 'Mickey Mouse,' it is those' basing proposals."

In October 1990, Byrd and James A. McClure served as floor managers for the appropriation bill for the National Endowment of the Arts, accepting an amendment by Jesse Helms prohibiting NEA support of work denigrating objects or beliefs of religions.

In November 1993, when the Senate voted to seek federal court enforcement of a subpoena for the diaries of Bob Packwood, Byrd stated the possibility of Americans becoming convinced that the Senate was delaying taking action to protect one of its own members. Byrd also called for Packwood to resign. "None of us is without flaws. But when those flaws damage the institution of the Senate, it is time to have the grace to go!" Packwood resigned in 1995.

In October 1999, Byrd was the only senator to vote present on the Comprehensive Test Ban Treaty. The treaty was designed to ban underground nuclear testing and was the first major international security pact to be defeated in the Senate since the Treaty of Versailles.

Byrd opposed the Flag Desecration Amendment, saying that, while he wanted to protect the American flag, he believed that amending the Constitution "is not the most expeditious way to protect this revered symbol of our Republic." As an alternative, Byrd cosponsored the Flag Protection Act of 2005 (S. 1370), a bill to prohibit destruction or desecration of the flag by anyone trying to incite violence or causing a breach of the peace, or who steals, damages, or destroys a flag on federal property, whether owned by the federal government or a private group or individual—can be imprisoned, fined or both. The bill did not pass.

In 2009, Byrd was one of three Democrats to oppose the confirmation of Secretary of the Treasury Timothy Geithner. After missing nearly two months while in hospital, Byrd returned to the Senate floor on July 21 to vote against the elimination of funding for the F-22 fighter plane.

Byrd received a 65% vote rating from the League of Conservation Voters for his support of environmentally friendly legislation. Additionally, he received a "liberal" rating of 65.5% by the "National Journal"—higher than six other Democratic senators.

In 2010, Byrd received a 70 percent lifetime rating from the American Civil Liberties Union for supporting rights-related legislation.

Byrd had an essential tremor; he eventually used a wheelchair for mobility. His health declined through 2008, including several hospital admissions.

On January 20, 2009, Senator Ted Kennedy suffered a seizure during Barack Obama's inaugural luncheon and was taken away in an ambulance. Byrd, seated at the same table, became distraught and was himself removed to his office. Byrd's office reported that he was fine. On May 18, Byrd was admitted to the hospital after experiencing a fever due to a "minor infection", prolonged by a staphylococcus aureus infection. Byrd was released on June 30, 2009.

Byrd's final hospital stay began on June 27, 2010, at Inova Fairfax Hospital in Fairfax County, Virginia. He died at approximately EDT the next day at age 92 from natural causes.

Vice President Joe Biden recalled Byrd's standing in the rain with him as Biden buried his daughter when Biden had just been elected to the Senate. He called Byrd "a tough, compassionate, and outspoken leader and dedicated above all else to making life better for the people of the Mountain State." President Barack Obama said, "His profound passion for that body and its role and responsibilities was as evident behind closed doors as it was in the stemwinders he peppered with history. He held the deepest respect of members of both parties, and he was generous with his time and advice, something I appreciated greatly as a young senator." Senator Jay Rockefeller, who had served with Byrd since 1985, said, "I looked up to him, I fought next to him, and I am deeply saddened that he is gone." Former President Jimmy Carter noted, "He was my closest and most valuable adviser while I served as president. I respected him and attempted in every way to remain in his good graces. He was a giant among legislators, and was courageous in espousing controversial issues."

On July 1, 2010, Byrd lay in repose on the Lincoln Catafalque in the Senate chamber of the United States Capitol, becoming the first Senator to do so since 1957. He was then flown to Charleston, West Virginia, where he lay in repose in the Lower Rotunda of the West Virginia State Capitol.

A funeral was held on July 2, 2010, on the grounds of the State Capitol where Byrd was eulogized by President Barack Obama, Vice President Joe Biden, Governor Joe Manchin, Senate Majority Leader Harry Reid, Senate Minority Leader Mitch McConnell, Speaker of the House of Representatives Nancy Pelosi, Senator Jay Rockefeller, Congressman Nick Rahall, Victoria Reggie Kennedy, and former President Bill Clinton. After the funeral services in Charleston, his body was returned to Arlington, Virginia, for funeral services on July 6, 2010, at Memorial Baptist Church. After the funeral in Arlington, Byrd was buried next to his wife Erma at Columbia Gardens Cemetery in Arlington, although family members have stated that both the senator and Mrs. Byrd will be reinterred somewhere in West Virginia once a site is determined.

The song "Take Me Home, Country Roads" was played at the end of the funeral in a bluegrass fashion as his casket was being carried back up the stairs and into the West Virginia State Capitol Building.

On September 30, 2010, Congress appropriated $193,400 to be paid equally among Byrd's children and grandchildren, representing the salary he would have earned in the next fiscal year, a common practice when members of Congress die in office.
Multiple political figures issued statements following Byrd's death:

Byrd had a prominent role in the 2008 Warner Bros. documentary "Body of War" directed by Phil Donahue. The film chronicles the life of Tomas Young, paralyzed from the chest down after a sniper shot him as he was riding in a vehicle in Iraq. Several long clips of Byrd show him passionately arguing against authorizing the use of force in Iraq. Later in the movie, Byrd has a one-on-one interview with Tomas Young in Byrd's Senate office, followed by a shot of Byrd walking beside the Young as they leave the Capitol.

A fictionalized version of Byrd, then the Senate Majority Leader, was a character in the Jeffrey Archer novel "Shall We Tell the President?"

Byrd was an avid fiddle player for most of his life, starting in his teens when he played in various square dance bands. Once he entered politics, his fiddling skills attracted attention and won votes. In 1978 when Byrd was Majority Leader, he recorded an album called "U.S. Senator Robert Byrd: Mountain Fiddler" (County, 1978). Byrd was accompanied by Country Gentlemen Doyle Lawson, James Bailey, and Spider Gilliam. Most of the LP consists of bluegrass music. Byrd covers "Don't Let Your Sweet Love Die", a Zeke Manners song, and "Will the Circle Be Unbroken". He had performed at the Kennedy Center, on the Grand Ole Opry and on "Hee Haw". He occasionally took a break from Senate business to entertain audiences with his fiddle. He stopped playing in 1982 when the symptoms of a benign essential tremor had begun to affect the use of his hands.

Byrd appeared in the Civil War movie "Gods and Generals" in 2003 along with then-Virginia senator George Allen. Both played Confederate States officers.


In 2002, the Robert C. Byrd Center for Legislative Studies (CLS) was opened on the campus of Shepherd University. Adjoining the University's Ruth Scarborough Library, the CLS "advances representative democracy by promoting a better understanding of the United States Congress and the Constitution through programs and research that engage citizens." The CLS is an archival research facility, housing the papers of Senator Robert C. Byrd in addition to the papers of Congressmen Harley O. Staggers Sr. and Harley O. Staggers Jr. and Scot Faulkner, the first Chief Administrative Officer of the United States House of Representatives. The CLS is a founding institution of the Association of Centers for the Study of Congress, "an independent alliance of organizations and institutions which promote the study of the U.S. Congress." 





</doc>
<doc id="25409" url="https://en.wikipedia.org/wiki?curid=25409" title="Reptile">
Reptile

Reptiles are tetrapod animals in the class Reptilia, comprising today's turtles, crocodilians, snakes, amphisbaenians, lizards, tuatara, and their extinct relatives. The study of these traditional reptile orders, historically combined with that of modern amphibians, is called herpetology.

Because some reptiles are more closely related to birds than they are to other reptiles (e.g., crocodiles are more closely related to birds than they are to lizards), the traditional groups of "reptiles" listed above do not together constitute a monophyletic grouping or clade (consisting of all descendants of a common ancestor). For this reason, many modern scientists prefer to consider the birds part of Reptilia as well, thereby making Reptilia a monophyletic class, including all living diapsids. The term "reptiles" is sometimes used as shorthand for 'non-avian Reptilia'.

The earliest known proto-reptiles originated around 312 million years ago during the Carboniferous period, having evolved from advanced reptiliomorph tetrapods that became increasingly adapted to life on dry land. Some early examples include the lizard-like "Hylonomus" and "Casineria". In addition to the living reptiles, there are many diverse groups that are now extinct, in some cases due to mass extinction events. In particular, the Cretaceous–Paleogene extinction event wiped out the pterosaurs, plesiosaurs, ornithischians, and sauropods, alongside many species of theropods, crocodyliforms, and squamates (e.g., mosasaurs).

Modern non-avian reptiles inhabit all the continents except Antarctica, although some birds are found on the periphery of Antarctica. Several living subgroups are recognized: Testudines (turtles and tortoises), 350 species; Rhynchocephalia (tuatara from New Zealand), 1 species; Squamata (lizards, snakes, and worm lizards), over 10,200 species; and Crocodilia (crocodiles, gharials, caimans, and alligators), 24 species.

Reptiles are tetrapod vertebrates, creatures that either have four limbs or, like snakes, are descended from four-limbed ancestors. Unlike amphibians, reptiles do not have an aquatic larval stage. Most reptiles are oviparous, although several species of squamates are viviparous, as were some extinct aquatic clades – the fetus develops within the mother, contained in a placenta rather than an eggshell. As amniotes, reptile eggs are surrounded by membranes for protection and transport, which adapt them to reproduction on dry land. Many of the viviparous species feed their fetuses through various forms of placenta analogous to those of mammals, with some providing initial care for their hatchlings. Extant reptiles range in size from a tiny gecko, "Sphaerodactylus ariasae", which can grow up to to the saltwater crocodile, "Crocodylus porosus", which can reach in length and weigh over .

In the 13th century the category of "reptile" was recognized in Europe as consisting of a miscellany of egg-laying creatures, including "snakes, various fantastic monsters, lizards, assorted amphibians, and worms", as recorded by Vincent of Beauvais in his "Mirror of Nature".
In the 18th century, the reptiles were, from the outset of classification, grouped with the amphibians. Linnaeus, working from species-poor Sweden, where the common adder and grass snake are often found hunting in water, included all reptiles and amphibians in class "III – Amphibia" in his "Systema Naturæ".
The terms "reptile" and "amphibian" were largely interchangeable, "reptile" (from Latin "repere", 'to creep') being preferred by the French. Josephus Nicolaus Laurenti was the first to formally use the term "Reptilia" for an expanded selection of reptiles and amphibians basically similar to that of Linnaeus. Today, the two groups are still commonly treated under the single heading herpetology.

It was not until the beginning of the 19th century that it became clear that reptiles and amphibians are, in fact, quite different animals, and Pierre André Latreille erected the class "Batracia" (1825) for the latter, dividing the tetrapods into the four familiar classes of reptiles, amphibians, birds, and mammals. The British anatomist Thomas Henry Huxley made Latreille's definition popular and, together with Richard Owen, expanded Reptilia to include the various fossil "antediluvian monsters", including dinosaurs and the mammal-like (synapsid) "Dicynodon" he helped describe. This was not the only possible classification scheme: In the Hunterian lectures delivered at the Royal College of Surgeons in 1863, Huxley grouped the vertebrates into mammals, sauroids, and ichthyoids (the latter containing the fishes and amphibians). He subsequently proposed the names of Sauropsida and Ichthyopsida for the latter two groups. In 1866, Haeckel demonstrated that vertebrates could be divided based on their reproductive strategies, and that reptiles, birds, and mammals were united by the amniotic egg.

The terms "Sauropsida" ('lizard faces') and "Theropsida" ('beast faces') were used again in 1916 by E.S. Goodrich to distinguish between lizards, birds, and their relatives on the one hand (Sauropsida) and mammals and their extinct relatives (Theropsida) on the other. Goodrich supported this division by the nature of the hearts and blood vessels in each group, and other features, such as the structure of the forebrain. According to Goodrich, both lineages evolved from an earlier stem group, Protosauria ("first lizards") in which he included some animals today considered reptile-like amphibians, as well as early reptiles.

In 1956, D.M.S. Watson observed that the first two groups diverged very early in reptilian history, so he divided Goodrich's Protosauria between them. He also reinterpreted Sauropsida and Theropsida to exclude birds and mammals, respectively. Thus his Sauropsida included Procolophonia, Eosuchia, Millerosauria, Chelonia (turtles), Squamata (lizards and snakes), Rhynchocephalia, Crocodilia, "thecodonts" (paraphyletic basal Archosauria), non-avian dinosaurs, pterosaurs, ichthyosaurs, and sauropterygians.

In the late 19th century, a number of definitions of Reptilia were offered. The traits listed by Lydekker in 1896, for example, include a single occipital condyle, a jaw joint formed by the quadrate and articular bones, and certain characteristics of the vertebrae. The animals singled out by these formulations, the amniotes other than the mammals and the birds, are still those considered reptiles today.

The synapsid/sauropsid division supplemented another approach, one that split the reptiles into four subclasses based on the number and position of temporal fenestrae, openings in the sides of the skull behind the eyes. This classification was initiated by Henry Fairfield Osborn and elaborated and made popular by Romer's classic "Vertebrate Paleontology". Those four subclasses were:
The composition of Euryapsida was uncertain. Ichthyosaurs were, at times, considered to have arisen independently of the other euryapsids, and given the older name Parapsida. Parapsida was later discarded as a group for the most part (ichthyosaurs being classified as "incertae sedis" or with Euryapsida). However, four (or three if Euryapsida is merged into Diapsida) subclasses remained more or less universal for non-specialist work throughout the 20th century. It has largely been abandoned by recent researchers: In particular, the anapsid condition has been found to occur so variably among unrelated groups that it is not now considered a useful distinction.

By the early 21st century, vertebrate paleontologists were beginning to adopt phylogenetic taxonomy, in which all groups are defined in such a way as to be monophyletic; that is, groups which include all descendants of a particular ancestor. The reptiles as historically defined are paraphyletic, since they exclude both birds and mammals. These respectively evolved from dinosaurs and from early therapsids, which were both traditionally called reptiles. Birds are more closely related to crocodilians than the latter are to the rest of extant reptiles. Colin Tudge wrote:

Mammals are a clade, and therefore the cladists are happy to acknowledge the traditional taxon Mammalia; and birds, too, are a clade, universally ascribed to the formal taxon Aves. Mammalia and Aves are, in fact, subclades within the grand clade of the Amniota. But the traditional class Reptilia is not a clade. It is just a section of the clade Amniota: the section that is left after the Mammalia and Aves have been hived off. It cannot be defined by synapomorphies, as is the proper way. Instead, it is defined by a combination of the features it has and the features it lacks: reptiles are the amniotes that lack fur or feathers. At best, the cladists suggest, we could say that the traditional Reptilia are 'non-avian, non-mammalian amniotes'.

Despite the early proposals for replacing the paraphyletic Reptilia with a monophyletic Sauropsida, which includes birds, that term was never adopted widely or, when it was, was not applied consistently.
When Sauropsida was used, it often had the same content or even the same definition as Reptilia. In 1988, Jacques Gauthier proposed a cladistic definition of Reptilia as a monophyletic node-based crown group containing turtles, lizards and snakes, crocodilians, and birds, their common ancestor and all its descendants. While Gauthier's definition was close to the modern consensus, nonetheless, it became considered inadequate because the actual relationship of turtles to other reptiles was not yet well understood at this time. Major revisions since have included the reassignment of synapsids as non-reptiles, and classification of turtles as diapsids.

A variety of other definitions were proposed by other scientists in the years following Gauthier's paper. The first such new definition, which attempted to adhere to the standards of the PhyloCode, was published by Modesto and Anderson in 2004. Modesto and Anderson reviewed the many previous definitions and proposed a modified definition, which they intended to retain most traditional content of the group while keeping it stable and monophyletic. They defined Reptilia as all amniotes closer to "Lacerta agilis" and "Crocodylus niloticus" than to "Homo sapiens". This stem-based definition is equivalent to the more common definition of Sauropsida, which Modesto and Anderson synonymized with Reptilia, since the latter is better known and more frequently used. Unlike most previous definitions of Reptilia, however, Modesto and Anderson's definition includes birds, as they are within the clade that includes both lizards and crocodiles.

Classification to order level of the reptiles, after Benton, 2014.

The cladogram presented here illustrates the "family tree" of reptiles, and follows a simplified version of the relationships found by M.S. Lee, in 2013. All genetic studies have supported the hypothesis that turtles are diapsids; some have placed turtles within archosauriformes, though a few have recovered turtles as lepidosauriformes instead. The cladogram below used a combination of genetic (molecular) and fossil (morphological) data to obtain its results.

The placement of turtles has historically been highly variable. Classically, turtles were considered to be related to the primitive anapsid reptiles. Molecular work has usually placed turtles within the diapsids. As of 2013, three turtle genomes have been sequenced. The results place turtles as a sister clade to the archosaurs, the group that includes crocodiles, dinosaurs, and birds. However, in their comparative analysis of the timing of organogenesis, Werneburg and Sánchez-Villagra (2009) found support for the hypothesis that turtles belong to a separate clade within Sauropsida, outside the saurian clade altogether.

The origin of the reptiles lies about 310–320 million years ago, in the steaming swamps of the late Carboniferous period, when the first reptiles evolved from advanced reptiliomorphs.

The oldest known animal that may have been an amniote is "Casineria" (though it may have been a temnospondyl). A series of footprints from the fossil strata of Nova Scotia dated to show typical reptilian toes and imprints of scales. These tracks are attributed to "Hylonomus", the oldest unquestionable reptile known.
It was a small, lizard-like animal, about long, with numerous sharp teeth indicating an insectivorous diet. Other examples include "Westlothiana" (for the moment considered a reptiliomorph rather than a true amniote) and "Paleothyris", both of similar build and presumably similar habit.

The earliest amniotes, including stem-reptiles (those amniotes closer to modern reptiles than to mammals), were largely overshadowed by larger stem-tetrapods, such as "Cochleosaurus", and remained a small, inconspicuous part of the fauna until the Carboniferous Rainforest Collapse. This sudden collapse affected several large groups. Primitive tetrapods were particularly devastated, while stem-reptiles fared better, being ecologically adapted to the drier conditions that followed. Primitive tetrapods, like modern amphibians, need to return to water to lay eggs; in contrast, amniotes, like modern reptiles – whose eggs possess a shell that allows them to be laid on land – were better adapted to the new conditions. Amniotes acquired new niches at a faster rate than before the collapse and at a much faster rate than primitive tetrapods. They acquired new feeding strategies including herbivory and carnivory, previously only having been insectivores and piscivores. From this point forward, reptiles dominated communities and had a greater diversity than primitive tetrapods, setting the stage for the Mesozoic (known as the Age of Reptiles). One of the best known early stem-reptiles is "Mesosaurus", a genus from the Early Permian that had returned to water, feeding on fish.

It was traditionally assumed that the first reptiles retained an anapsid skull inherited from their ancestors. This type of skull has a skull roof with only holes for the nostrils, eyes and a pineal eye. The discoveries of synapsid-like openings (see below) in the skull roof of the skulls of several members of Parareptilia (the clade containing most of the amniotes traditionally referred to as "anapsids"), including lanthanosuchoids, millerettids, bolosaurids, some nycteroleterids, some procolophonoids and at least some mesosaurs made it more ambiguous and it's currently uncertain whether the ancestral amniote had an anapsid-like or synapsid-like skull. These animals are traditionally referred to as "anapsids", and form a paraphyletic basic stock from which other groups evolved. Very shortly after the first amniotes appeared, a lineage called Synapsida split off; this group was characterized by a temporal opening in the skull behind each eye to give room for the jaw muscle to move. These are the "mammal-like amniotes", or stem-mammals, that later gave rise to the true mammals. Soon after, another group evolved a similar trait, this time with a double opening behind each eye, earning them the name Diapsida ("two arches"). The function of the holes in these groups was to lighten the skull and give room for the jaw muscles to move, allowing for a more powerful bite.

Turtles have been traditionally believed to be surviving parareptiles, on the basis of their anapsid skull structure, which was assumed to be primitive trait. The rationale for this classification has been disputed, with some arguing that turtles are diapsids that evolved anapsid skulls in order to improve their armor. Later morphological phylogenetic studies with this in mind placed turtles firmly within Diapsida. All molecular studies have strongly upheld the placement of turtles within diapsids, most commonly as a sister group to extant archosaurs.

With the close of the Carboniferous, the amniotes became the dominant tetrapod fauna. While primitive, terrestrial reptiliomorphs still existed, the synapsid amniotes evolved the first truly terrestrial megafauna (giant animals) in the form of pelycosaurs, such as "Edaphosaurus" and the carnivorous "Dimetrodon". In the mid-Permian period, the climate became drier, resulting in a change of fauna: The pelycosaurs were replaced by the therapsids.

The parareptiles, whose massive skull roofs had no postorbital holes, continued and flourished throughout the Permian. The pareiasaurian parareptiles reached giant proportions in the late Permian, eventually disappearing at the close of the period (the turtles being possible survivors).

Early in the period, the modern reptiles, or crown-group reptiles, evolved and split into two main lineages: the Archosauromorpha (forebears of turtles, crocodiles, and dinosaurs) and the Lepidosauromorpha (predecessors of modern lizards and tuataras). Both groups remained lizard-like and relatively small and inconspicuous during the Permian.

The close of the Permian saw the greatest mass extinction known (see the Permian–Triassic extinction event), an event prolonged by the combination of two or more distinct extinction pulses. Most of the earlier parareptile and synapsid megafauna disappeared, being replaced by the true reptiles, particularly archosauromorphs. These were characterized by elongated hind legs and an erect pose, the early forms looking somewhat like long-legged crocodiles. The archosaurs became the dominant group during the Triassic period, though it took 30 million years before their diversity was as great as the animals that lived in the Permian. Archosaurs developed into the well-known dinosaurs and pterosaurs, as well as the ancestors of crocodiles. Since reptiles, first rauisuchians and then dinosaurs, dominated the Mesozoic era, the interval is popularly known as the "Age of Reptiles". The dinosaurs also developed smaller forms, including the feather-bearing smaller theropods. In the Cretaceous period, these gave rise to the first true birds.

The sister group to Archosauromorpha is Lepidosauromorpha, containing lizards and tuataras, as well as their fossil relatives. Lepidosauromorpha contained at least one major group of the Mesozoic sea reptiles: the mosasaurs, which lived during the Cretaceous period. The phylogenetic placement of other main groups of fossil sea reptiles – the ichthyopterygians (including ichthyosaurs) and the sauropterygians, which evolved in the early Triassic – is more controversial. Different authors linked these groups either to lepidosauromorphs or to archosauromorphs, and ichthyopterygians were also argued to be diapsids that did not belong to the least inclusive clade containing lepidosauromorphs and archosauromorphs.

The close of the Cretaceous period saw the demise of the Mesozoic era reptilian megafauna (see the Cretaceous–Paleogene extinction event). Of the large marine reptiles, only sea turtles were left; and of the non-marine large reptiles, only the semi-aquatic crocodiles and broadly similar choristoderes survived the extinction, with the latter becoming extinct in the Miocene. Of the great host of dinosaurs dominating the Mesozoic, only the small beaked birds survived. This dramatic extinction pattern at the end of the Mesozoic led into the Cenozoic. Mammals and birds filled the empty niches left behind by the reptilian megafauna and, while reptile diversification slowed, bird and mammal diversification took an exponential turn. However, reptiles were still important components of the megafauna, particularly in the form of large and giant tortoises.

After the extinction of most archosaur and marine reptile lines by the end of the Cretaceous, reptile diversification continued throughout the Cenozoic. Squamates took a massive hit during the KT-event, only recovering ten million years after it, but they underwent a great radiation event once they recovered, and today squamates make up the majority of living reptiles (> 95%). Approximately 10,000 extant species of traditional reptiles are known, with birds adding about 10,000 more, almost twice the number of mammals, represented by about 5,700 living species (excluding domesticated species).

All squamates and turtles have a three-chambered heart consisting of two atria, one variably partitioned ventricle, and two aortas that lead to the systemic circulation. The degree of mixing of oxygenated and deoxygenated blood in the three-chambered heart varies depending on the species and physiological state. Under different conditions, deoxygenated blood can be shunted back to the body or oxygenated blood can be shunted back to the lungs. This variation in blood flow has been hypothesized to allow more effective thermoregulation and longer diving times for aquatic species, but has not been shown to be a fitness advantage.
For example, Iguana hearts, like the majority of the squamates hearts, are composed of three chambers with two aorta and one ventricle, cardiac involuntary muscles. The main structures of the heart are the sinus venosus, the pacemaker, the left atrium, the right atruim, the atrioventricular valve, the cavum venosum, cavum arteriosum, the cavum pulmonale, the muscular ridge, the ventricular ridge, pulmonary veins, and paired aortic arches.

Some squamate species (e.g., pythons and monitor lizards) have three-chambered hearts that become functionally four-chambered hearts during contraction. This is made possible by a muscular ridge that subdivides the ventricle during ventricular diastole and completely divides it during ventricular systole. Because of this ridge, some of these squamates are capable of producing ventricular pressure differentials that are equivalent to those seen in mammalian and avian hearts.

Crocodilians have an anatomically four-chambered heart, similar to birds, but also have two systemic aortas and are therefore capable of bypassing their pulmonary circulation.

Modern non-avian reptiles exhibit some form of cold-bloodedness (i.e. some mix of poikilothermy, ectothermy, and bradymetabolism) so that they have limited physiological means of keeping the body temperature constant and often rely on external sources of heat. Due to a less stable core temperature than birds and mammals, reptilian biochemistry requires enzymes capable of maintaining efficiency over a greater range of temperatures than in the case for warm-blooded animals. The optimum body temperature range varies with species, but is typically below that of warm-blooded animals; for many lizards, it falls in the 24°–35 °C (75°–95 °F) range, while extreme heat-adapted species, like the American desert iguana "Dipsosaurus dorsalis", can have optimal physiological temperatures in the mammalian range, between 35° and 40 °C (95° and 104 °F). While the optimum temperature is often encountered when the animal is active, the low basal metabolism makes body temperature drop rapidly when the animal is inactive.

As in all animals, reptilian muscle action produces heat. In large reptiles, like leatherback turtles, the low surface-to-volume ratio allows this metabolically produced heat to keep the animals warmer than their environment even though they do not have a warm-blooded metabolism. This form of homeothermy is called gigantothermy; it has been suggested as having been common in large dinosaurs and other extinct large-bodied reptiles.

The benefit of a low resting metabolism is that it requires far less fuel to sustain bodily functions. By using temperature variations in their surroundings, or by remaining cold when they do not need to move, reptiles can save considerable amounts of energy compared to endothermic animals of the same size. A crocodile needs from a tenth to a fifth of the food necessary for a lion of the same weight and can live half a year without eating. Lower food requirements and adaptive metabolisms allow reptiles to dominate the animal life in regions where net calorie availability is too low to sustain large-bodied mammals and birds.

It is generally assumed that reptiles are unable to produce the sustained high energy output necessary for long distance chases or flying. Higher energetic capacity might have been responsible for the evolution of warm-bloodedness in birds and mammals. However, investigation of correlations between active capacity and thermophysiology show a weak relationship. Most extant reptiles are carnivores with a sit-and-wait feeding strategy; whether reptiles are cold blooded due to their ecology is not clear. Energetic studies on some reptiles have shown active capacities equal to or greater than similar sized warm-blooded animals.

All reptiles breathe using lungs. Aquatic turtles have developed more permeable skin, and some species have modified their cloaca to increase the area for gas exchange. Even with these adaptations, breathing is never fully accomplished without lungs. Lung ventilation is accomplished differently in each main reptile group. In squamates, the lungs are ventilated almost exclusively by the axial musculature. This is also the same musculature that is used during locomotion. Because of this constraint, most squamates are forced to hold their breath during intense runs. Some, however, have found a way around it. Varanids, and a few other lizard species, employ buccal pumping as a complement to their normal "axial breathing". This allows the animals to completely fill their lungs during intense locomotion, and thus remain aerobically active for a long time. Tegu lizards are known to possess a proto-diaphragm, which separates the pulmonary cavity from the visceral cavity. While not actually capable of movement, it does allow for greater lung inflation, by taking the weight of the viscera off the lungs.

Crocodilians actually have a muscular diaphragm that is analogous to the mammalian diaphragm. The difference is that the muscles for the crocodilian diaphragm pull the pubis (part of the pelvis, which is movable in crocodilians) back, which brings the liver down, thus freeing space for the lungs to expand. This type of diaphragmatic setup has been referred to as the "hepatic piston". The airways form a number of double tubular chambers within each lung. On inhalation and exhalation air moves through the airways in the same direction, thus creating a unidirectional airflow through the lungs. A similar system is found in birds, monitor lizards and iguanas.

Most reptiles lack a secondary palate, meaning that they must hold their breath while swallowing. Crocodilians have evolved a bony secondary palate that allows them to continue breathing while remaining submerged (and protect their brains against damage by struggling prey). Skinks (family Scincidae) also have evolved a bony secondary palate, to varying degrees. Snakes took a different approach and extended their trachea instead. Their tracheal extension sticks out like a fleshy straw, and allows these animals to swallow large prey without suffering from asphyxiation.

How turtles and tortoises breathe has been the subject of much study. To date, only a few species have been studied thoroughly enough to get an idea of how those turtles breathe. The varied results indicate that turtles and tortoises have found a variety of solutions to this problem.

The difficulty is that most turtle shells are rigid and do not allow for the type of expansion and contraction that other amniotes use to ventilate their lungs. Some turtles, such as the Indian flapshell ("Lissemys punctata"), have a sheet of muscle that envelops the lungs. When it contracts, the turtle can exhale. When at rest, the turtle can retract the limbs into the body cavity and force air out of the lungs. When the turtle protracts its limbs, the pressure inside the lungs is reduced, and the turtle can suck air in. Turtle lungs are attached to the inside of the top of the shell (carapace), with the bottom of the lungs attached (via connective tissue) to the rest of the viscera. By using a series of special muscles (roughly equivalent to a diaphragm), turtles are capable of pushing their viscera up and down, resulting in effective respiration, since many of these muscles have attachment points in conjunction with their forelimbs (indeed, many of the muscles expand into the limb pockets during contraction).

Breathing during locomotion has been studied in three species, and they show different patterns. Adult female green sea turtles do not breathe as they crutch along their nesting beaches. They hold their breath during terrestrial locomotion and breathe in bouts as they rest. North American box turtles breathe continuously during locomotion, and the ventilation cycle is not coordinated with the limb movements. This is because they use their abdominal muscles to breathe during locomotion. The last species to have been studied is the red-eared slider, which also breathes during locomotion, but takes smaller breaths during locomotion than during small pauses between locomotor bouts, indicating that there may be mechanical interference between the limb movements and the breathing apparatus. Box turtles have also been observed to breathe while completely sealed up inside their shells.

Reptilian skin is covered in a horny epidermis, making it watertight and enabling reptiles to live on dry land, in contrast to amphibians. Compared to mammalian skin, that of reptiles is rather thin and lacks the thick dermal layer that produces leather in mammals.
Exposed parts of reptiles are protected by scales or scutes, sometimes with a bony base (osteoderms), forming armor. In lepidosaurians, such as lizards and snakes, the whole skin is covered in overlapping epidermal scales. Such scales were once thought to be typical of the class Reptilia as a whole, but are now known to occur only in lepidosaurians. The scales found in turtles and crocodiles are of dermal, rather than epidermal, origin and are properly termed scutes. In turtles, the body is hidden inside a hard shell composed of fused scutes. 

Lacking a thick dermis, reptilian leather is not as strong as mammalian leather. It is used in leather-wares for decorative purposes for shoes, belts and handbags, particularly crocodile skin.

Reptiles shed their skin through a process called ecdysis which occurs continuously throughout their lifetime. In particular, younger reptiles tend to shed once every 5–6 weeks while adults shed 3–4 times a year. Younger reptiles shed more because of their rapid growth rate. Once full size, the frequency of shedding drastically decreases. The process of ecdysis involves forming a new layer of skin under the old one. Proteolytic enzymes and lymphatic fluid is secreted between the old and new layers of skin. Consequently, this lifts the old skin from the new one allowing shedding to occur. Snakes will shed from the head to the tail while lizards shed in a "patchy pattern". Dysecdysis, a common skin disease in snakes and lizards, will occur when ecdysis, or shedding, fails. There are numerous reasons why shedding fails and can be related to inadequate humidity and temperature, nutritional deficiencies, dehydration and traumatic injuries. Nutritional deficiencies decrease proteolytic enzymes while dehydration reduces lymphatic fluids to separate the skin layers. Traumatic injuries on the other hand, form scars that will not allow new scales to form and disrupt the process of ecdysis.

Excretion is performed mainly by two small kidneys. In diapsids, uric acid is the main nitrogenous waste product; turtles, like mammals, excrete mainly urea. Unlike the kidneys of mammals and birds, reptile kidneys are unable to produce liquid urine more concentrated than their body fluid. This is because they lack a specialized structure called a loop of Henle, which is present in the nephrons of birds and mammals. Because of this, many reptiles use the colon to aid in the reabsorption of water. Some are also able to take up water stored in the bladder. Excess salts are also excreted by nasal and lingual salt glands in some reptiles.

In all reptiles the urinogenital ducts and the anus both empty into an organ called a cloaca. In some reptiles, a midventral wall in the cloaca may open into a urinary bladder, but not all. It is present in all turtles and tortoises as well as most lizards, but is lacking in the monitor lizard, the legless lizards. It is absent in the snakes, alligators, and crocodiles.

Many turtles, tortoises, and lizards have proportionally very large bladders. Charles Darwin noted that the Galapagos tortoise had a bladder which could store up to 20% of its body weight. Such adaptations are the result of environments such as remote islands and deserts where water is very scarce. Other desert-dwelling reptiles have large bladders that can store a long-term reservoir of water for up to several months and aid in osmoregulation.

Turtles have two or more accessory urinary bladders, located lateral to the neck of the urinary bladder and dorsal to the pubis, occupying a significant portion of their body cavity. Their bladder is also usually bilobed with a left and right section. The right section is located under the liver, which prevents large stones from remaining in that side while the left section is more likely to have calculi.

Most reptiles are insectivorous or carnivorous and have simple and comparatively short digestive tracts due to meat being fairly simple to break down and digest. Digestion is slower than in mammals, reflecting their lower resting metabolism and their inability to divide and masticate their food. Their poikilotherm metabolism has very low energy requirements, allowing large reptiles like crocodiles and large constrictors to live from a single large meal for months, digesting it slowly.

While modern reptiles are predominantly carnivorous, during the early history of reptiles several groups produced some herbivorous megafauna: in the Paleozoic, the pareiasaurs; and in the Mesozoic several lines of dinosaurs. Today, turtles are the only predominantly herbivorous reptile group, but several lines of agamas and iguanas have evolved to live wholly or partly on plants.

Herbivorous reptiles face the same problems of mastication as herbivorous mammals but, lacking the complex teeth of mammals, many species swallow rocks and pebbles (so called gastroliths) to aid in digestion: The rocks are washed around in the stomach, helping to grind up plant matter. Fossil gastroliths have been found associated with both ornithopods and sauropods, though whether they actually functioned as a gastric mill in the latter is disputed. Salt water crocodiles also use gastroliths as ballast, stabilizing them in the water or helping them to dive. A dual function as both stabilizing ballast and digestion aid has been suggested for gastroliths found in plesiosaurs.

The reptilian nervous system contains the same basic part of the amphibian brain, but the reptile cerebrum and cerebellum are slightly larger. Most typical sense organs are well developed with certain exceptions, most notably the snake's lack of external ears (middle and inner ears are present). There are twelve pairs of cranial nerves. Due to their short cochlea, reptiles use electrical tuning to expand their range of audible frequencies.

Reptiles are generally considered less intelligent than mammals and birds. The size of their brain relative to their body is much less than that of mammals, the encephalization quotient being about one tenth of that of mammals, though larger reptiles can show more complex brain development. Larger lizards, like the monitors, are known to exhibit complex behavior, including cooperation and cognitive abilities allowing them to optimize their foraging and territoriality over time. Crocodiles have relatively larger brains and show a fairly complex social structure. The Komodo dragon is even known to engage in play, as are turtles, which are also considered to be social creatures, and sometimes switch between monogamy and promiscuity in their sexual behavior. One study found that wood turtles were better than white rats at learning to navigate mazes. Another study found that giant tortoises are capable of learning through operant conditioning, visual discrimination and retained learned behaviors with long-term memory. Sea turtles have been regarded as having simple brains, but their flippers are used for a variety of foraging tasks (holding, bracing, corralling) in common with marine mammals.

Most reptiles are diurnal animals. The vision is typically adapted to daylight conditions, with color vision and more advanced visual depth perception than in amphibians and most mammals.

Reptiles usually have excellent vision, allowing them to detect shapes and motions at long distances. They often have only a few Rod cells and have poor vision in low-light conditions. At the same time they have cells called “double cones” which give them sharp color vision and enable them to see ultraviolet wavelengths. In some species, such as blind snakes, vision is reduced.

Many lepidosaurs have a photosensory organ on the top of their heads called the parietal eye, which are also called third eye, pineal eye or pineal gland. This “eye” does not work the same way as a normal eye does as it has only a rudimentary retina and lens and thus, cannot form images. It is however sensitive to changes in light and dark and can detect movement.

Some snakes have extra sets of visual organs (in the loosest sense of the word) in the form of pits sensitive to infrared radiation (heat). Such heat-sensitive pits are particularly well developed in the pit vipers, but are also found in boas and pythons. These pits allow the snakes to sense the body heat of birds and mammals, enabling pit vipers to hunt rodents in the dark.

Most reptiles including birds possess a nictitating membrane, a translucent third eyelid which is drawn over the eye from the inner corner. Notably, it protects a crocodilian's eyeball surface while allowing a degree of vision underwater. However, many squamates, geckos and snakes in particular, lack eyelids, which are replaced by a transparent scale. This is called the brille, spectacle, or eyecap. The brille is usually not visible, except for when the snake molts, and it protects the eyes from dust and dirt.

Reptiles generally reproduce sexually, though some are capable of asexual reproduction. All reproductive activity occurs through the cloaca, the single exit/entrance at the base of the tail where waste is also eliminated. Most reptiles have copulatory organs, which are usually retracted or inverted and stored inside the body. In turtles and crocodilians, the male has a single median penis, while squamates, including snakes and lizards, possess a pair of hemipenes, only one of which is typically used in each session. Tuatara, however, lack copulatory organs, and so the male and female simply press their cloacas together as the male discharges sperm.

Most reptiles lay amniotic eggs covered with leathery or calcareous shells. An amnion, chorion, and allantois are present during embryonic life. The eggshell (1) protects the crocodile embryo (11) and keeps it from drying out, but it is flexible to allow gas exchange. The chorion (6) aids in gas exchange between the inside and outside of the egg. It allows carbon dioxide to exit the egg and oxygen gas to enter the egg. The albumin (9) further protects the embryo and serves as a reservoir for water and protein. The allantois (8) is a sac that collects the metabolic waste produced by the embryo. The amniotic sac (10) contains amniotic fluid (12) which protects and cushions the embryo. The amnion (5) aids in osmoregulation and serves as a saltwater reservoir. The yolk sac (2) surrounding the yolk (3) contains protein and fat rich nutrients that are absorbed by the embryo via vessels (4) that allow the embryo to grow and metabolize. The air space (7) provides the embryo with oxygen while it is hatching. This ensures that the embryo will not suffocate while it is hatching. There are no larval stages of development. Viviparity and ovoviviparity have evolved in many extinct clades of reptiles and in squamates. In the latter group, many species, including all boas and most vipers, utilize this mode of reproduction. The degree of viviparity varies; some species simply retain the eggs until just before hatching, others provide maternal nourishment to supplement the yolk, and yet others lack any yolk and provide all nutrients via a structure similar to the mammalian placenta. The earliest documented case of viviparity in reptiles is the Early Permian mesosaurs, although some individuals or taxa in that clade may also have been oviparous because a putative isolated egg has also been found. Several groups of Mesozoic marine reptiles also exhibited viviparity, such as mosasaurs, ichthyosaurs, and Sauropterygia, a group that include pachypleurosaurs and Plesiosauria.

Asexual reproduction has been identified in squamates in six families of lizards and one snake. In some species of squamates, a population of females is able to produce a unisexual diploid clone of the mother. This form of asexual reproduction, called parthenogenesis, occurs in several species of gecko, and is particularly widespread in the teiids (especially "Aspidocelis") and lacertids ("Lacerta"). In captivity, Komodo dragons (Varanidae) have reproduced by parthenogenesis.

Parthenogenetic species are suspected to occur among chameleons, agamids, xantusiids, and typhlopids.

Some reptiles exhibit temperature-dependent sex determination (TDSD), in which the incubation temperature determines whether a particular egg hatches as male or female. TDSD is most common in turtles and crocodiles, but also occurs in lizards and tuatara. To date, there has been no confirmation of whether TDSD occurs in snakes.

Many small reptiles, such as snakes and lizards that live on the ground or in the water, are vulnerable to being preyed on by all kinds of carnivorous animals. Thus avoidance is the most common form of defense in reptiles. At the first sign of danger, most snakes and lizards crawl away into the undergrowth, and turtles and crocodiles will plunge into water and sink out of sight.

Reptiles tend to avoid confrontation through camouflage. Two major groups of reptile predators are birds and other reptiles, both of which have well developed color vision. Thus the skins of many reptiles have cryptic coloration of plain or mottled gray, green, and brown to allow them to blend into the background of their natural environment. Aided by the reptiles' capacity for remaining motionless for long periods, the camouflage of many snakes is so effective that people or domestic animals are most typically bitten because they accidentally step on them.

When camouflage fails to protect them, blue-tongued skinks will try to ward off attackers by displaying their blue tongues, and the frill-necked lizard will display its brightly colored frill. These same displays are used in territorial disputes and during courtship. If danger arises so suddenly that flight is useless, crocodiles, turtles, some lizards, and some snakes hiss loudly when confronted by an enemy. Rattlesnakes rapidly vibrate the tip of the tail, which is composed of a series of nested, hollow beads to ward of approaching danger.

In contrast to the normal drab coloration of most reptiles, the lizards of the genus "Heloderma" (the Gila monster and the beaded lizard) and many of the coral snakes have high-contrast warning coloration, warning potential predators they are venomous. A number of non-venomous North American snake species have colorful markings similar to those of the coral snake, an oft cited example of Batesian mimicry.

Camouflage does not always fool a predator. When caught out, snake species adopt different defensive tactics and use a complicated set of behaviors when attacked. Some first elevate their head and spread out the skin of their neck in an effort to look large and threatening. Failure of this strategy may lead to other measures practiced particularly by cobras, vipers, and closely related species, which use venom to attack. The venom is modified saliva, delivered through fangs from a venom gland. Some non-venomous snakes, such as American hognose snakes or European grass snake, play dead when in danger; some, including the grass snake, exude a foul-smelling liquid to deter attackers.

When a crocodilian is concerned about its safety, it will gape to expose the teeth and yellow tongue. If this doesn't work, the crocodilian gets a little more agitated and typically begins to make hissing sounds. After this, the crocodilian will start to change its posture dramatically to make itself look more intimidating. The body is inflated to increase apparent size. If absolutely necessary it may decide to attack an enemy.

Some species try to bite immediately. Some will use their heads as sledgehammers and literally smash an opponent, some will rush or swim toward the threat from a distance, even chasing the opponent onto land or galloping after it. The main weapon in all crocodiles is the bite, which can generate very high bite force. Many species also possess canine-like teeth. These are used primarily for seizing prey, but are also used in fighting and display.

Geckos, skinks, and other lizards that are captured by the tail will shed part of the tail structure through a process called autotomy and thus be able to flee. The detached tail will continue to wiggle, creating a deceptive sense of continued struggle and distracting the predator's attention from the fleeing prey animal. The detached tails of leopard geckos can wiggle for up to 20 minutes. In many species the tails are of a separate and dramatically more intense color than the rest of the body so as to encourage potential predators to strike for the tail first. In the shingleback skink and some species of geckos, the tail is short and broad and resembles the head, so that the predators may attack it rather than the more vulnerable front part.

Reptiles that are capable of shedding their tails can partially regenerate them over a period of weeks. The new section will however contain cartilage rather than bone, and will never grow to the same length as the original tail. It is often also distinctly discolored compared to the rest of the body and may lack some of the external sculpting features seen in the original tail.

Dinosaurs have been widely depicted in culture since the English palaeontologist Richard Owen coined the name "dinosaur" in 1842. As soon as 1854, the Crystal Palace Dinosaurs were on display to the public in south London. One dinosaur appeared in literature even earlier, as Charles Dickens placed a "Megalosaurus" in the first chapter of his novel "Bleak House" in 1852. The dinosaurs featured in books, films, television programs, artwork, and other media have been used for both education and entertainment. The depictions range from the realistic, as in the television documentaries of the 1990s and first decade of the 21st century, or the fantastic, as in the monster movies of the 1950s and 1960s.

The snake or serpent has played a powerful symbolic role in different cultures. In Egyptian history, the Nile cobra adorned the crown of the pharaoh. It was worshipped as one of the gods and was also used for sinister purposes: murder of an adversary and ritual suicide (Cleopatra). In Greek mythology snakes are associated with deadly antagonists, as a chthonic symbol, roughly translated as "earthbound". The nine-headed Lernaean Hydra that Hercules defeated and the three Gorgon sisters are children of Gaia, the earth. Medusa was one of the three Gorgon sisters who Perseus defeated. Medusa is described as a hideous mortal, with snakes instead of hair and the power to turn men to stone with her gaze. After killing her, Perseus gave her head to Athena who fixed it to her shield called the Aegis. The Titans are depicted in art with their legs replaced by bodies of snakes for the same reason: They are children of Gaia, so they are bound to the earth. In Hinduism, snakes are worshipped as gods, with many women pouring milk on snake pits. The cobra is seen on the neck of Shiva, while Vishnu is depicted often as sleeping on a seven-headed snake or within the coils of a serpent. There are temples in India solely for cobras sometimes called "Nagraj" (King of Snakes), and it is believed that snakes are symbols of fertility. In the annual Hindu festival of Nag Panchami, snakes are venerated and prayed to. In religious terms, the snake and jaguar are arguably the most important animals in ancient Mesoamerica. "In states of ecstasy, lords dance a serpent dance; great descending snakes adorn and support buildings from Chichen Itza to Tenochtitlan, and the Nahuatl word "coatl" meaning serpent or twin, forms part of primary deities such as Mixcoatl, Quetzalcoatl, and Coatlicue." In Christianity and Judaism, a serpent appears in Genesis to tempt Adam and Eve with the forbidden fruit from the Tree of Knowledge of Good and Evil.

The turtle has a prominent position as a symbol of steadfastness and tranquility in religion, mythology, and folklore from around the world. A tortoise's longevity is suggested by its long lifespan and its shell, which was thought to protect it from any foe. In the cosmological myths of several cultures a "World Turtle" carries the world upon its back or supports the heavens.

Deaths from snakebites are uncommon in many parts of the world, but are still counted in tens of thousands per year in India. Snakebite can be treated with antivenom made from the venom of the snake. To produce antivenom, a mixture of the venoms of different species of snake is injected into the body of a horse in ever-increasing dosages until the horse is immunized. Blood is then extracted; the serum is separated, purified and freeze-dried. The cytotoxic effect of snake venom is being researched as a potential treatment for cancers.

Lizards such as the Gila monster produce toxins with medical applications. Gila toxin reduces plasma glucose; the substance is now synthesised for use in the anti-diabetes drug exenatide (Byetta). Another toxin from Gila monster saliva has been studied for use as an anti-Alzheimer's drug.

Geckos have also been used as medicine, especially in China. Turtles have been used in Chinese traditional medicine for thousands of years, with every part of the turtle believed to have medical benefits. There is a lack of scientific evidence that would correlate claimed medical benefits to turtle consumption. Growing demand for turtle meat has placed pressure on vulnerable wild populations of turtles.

Crocodiles are protected in many parts of the world, and are farmed commercially. Their hides are tanned and used to make leather goods such as shoes and handbags; crocodile meat is also considered a delicacy. The most commonly farmed species are the saltwater and Nile crocodiles. Farming has resulted in an increase in the saltwater crocodile population in Australia, as eggs are usually harvested from the wild, so landowners have an incentive to conserve their habitat. Crocodile leather is made into wallets, briefcases, purses, handbags, belts, hats, and shoes. Crocodile oil has been used for various purposes.

Snakes are also farmed, primarily in East and Southeast Asia, and their production has become more intensive in the last decade. Snake farming has been troubling for conservation in the past as it can lead to overexploitation of wild snakes and their natural prey to supply the farms. However, farming snakes can limit the hunting of wild snakes, while reducing the slaughter of higher-order vertebrates like cows. The energy efficiency of snakes is higher than expected for carnivores, due to their ectothermy and low metabolism. Waste protein from the poultry and pig industries is used as feed in snake farms. Snake farms produce meat, snake skin, and antivenom.

Turtle farming is another known but controversial practice. Turtles have been farmed for a variety of reasons, ranging from food to traditional medicine, the pet trade, and scientific conservation. Demand for turtle meat and medicinal products is one of the main threats to turtle conservation in Asia. Though commercial breeding would seem to insulate wild populations, it can stoke the demand for them and increase wild captures. Even the potentially appealing concept of raising turtles at a farm to release into the wild is questioned by some veterinarians who have had some experience with farm operations. They caution that this may introduce into the wild populations infectious diseases that occur on the farm, but have not (yet) been occurring in the wild.

In the Western world, some snakes (especially docile species such as the ball python and corn snake) are kept as pets. Numerous species of lizard are kept as pets, including bearded dragons, iguanas, anoles, and geckos (such as the popular leopard gecko).

Turtles and tortoises are an increasingly popular pet, but keeping them can be challenging due to particular requirements, such as temperature control and a varied diet, as well as the long lifespans of turtles, who can potentially outlive their owners. Good hygiene and significant maintenance is necessary when keeping reptiles, due to the risks of "Salmonella" and other pathogens.

A herpetarium is a zoological exhibition space for reptiles or amphibians.





</doc>
<doc id="25410" url="https://en.wikipedia.org/wiki?curid=25410" title="Rhode Island">
Rhode Island

Rhode Island (, like "road"), officially the State of Rhode Island and Providence Plantations, is a state in the New England region of the United States. It is the smallest U.S. state by area and the seventh least populous, but it is also the second most densely populated. The state takes its short name from Rhode Island; however, most of the state is located on the mainland. The state has land borders with Connecticut to the west, Massachusetts to the north and east, and the Atlantic Ocean to the south via Rhode Island Sound and Block Island Sound. It also shares a small maritime border with New York. Providence is the state capital and most populous city in Rhode Island.

On May 4, 1776, the Colony of Rhode Island was the first of the Thirteen Colonies to renounce its allegiance to the British Crown, and it was the fourth state to ratify the Articles of Confederation, doing so on February 9, 1778. The state boycotted the 1787 convention which drew up the United States Constitution and initially refused to ratify it; it was the last of the original 13 states to do so on May 29, 1790.

Rhode Island's official nickname is "The Ocean State", a reference to the large bays and inlets that amount to about 14 percent of its total area.

Despite its name, most of Rhode Island is located on the mainland of the United States. Its official name is "State of Rhode Island and Providence Plantations", which is derived from the merger of four Colonial settlements. The settlements of Newport and Portsmouth were situated on what is commonly called Aquidneck Island today but was called "Rhode Island" in Colonial times. "Providence Plantation" was the name of the colony founded by Roger Williams in the state's capital of Providence. This was adjoined by the settlement of Warwick; hence the plural Providence Plantations.

It is unclear how the island came to be named "Rhode Island", but two historical events may have been of influence:


The earliest documented use of the name "Rhode Island" for Aquidneck was in 1637 by Roger Williams. The name was officially applied to the island in 1644 with these words: "Aquethneck shall be henceforth called the Isle of Rodes or Rhode-Island." The name "Isle of Rodes" is used in a legal document as late as 1646. Dutch maps as early as 1659 call the island "Red Island" ("Roodt Eylant").

The first English settlement in Rhode Island was the town of Providence, which the Narragansett granted to Roger Williams in 1636. At that time, Williams obtained no permission from the English crown, as he believed that the English had no legitimate claim on Narragansett and Wampanoag territory. However, in 1643, he petitioned Charles I of England to grant Providence and neighboring towns a colonial patent, due to threats of invasion from the colonies of Boston and Plymouth. He used the name "Providence Plantations" in his petition, "plantation" being the English term for a colony. "Providence Plantations" was therefore the official name of the colony from 1643 to 1663, when a new charter was issued. Following the American Revolution, the new state incorporated both "Rhode Island" and "Providence Plantations" in its official name. The word "plantation" in the state's name has become a contested issue, and the Rhode Island General Assembly voted on June 25, 2009, to hold a general referendum determining whether "and Providence Plantations" would be dropped from the official name.

Advocates for excising "plantation" claimed that the word symbolized an alleged legacy of disenfranchisement for many Rhode Islanders, as well as the proliferation of slavery in the colonies and in the post-colonial United States. Advocates for retaining the name argued that "plantation" was simply an archaic synonym for "colony" and bore no relation to slavery. The referendum election was held on November 2, 2010, and the people voted overwhelmingly (78% to 22%) to retain the entire original name.

On June 18, 2020, State Senator Harold Metts sponsored a motion in the State for another ballot referendum on removing the words "and Providence Plantations" from the state's name. The motion was passed unanimously amidst the George Floyd protests and nationwide calls to address systemic racism. Metts said, "Whatever the meaning of the term 'plantations' in the context of Rhode Island's history, it carries a horrific connotation when considering the tragic and racist history of our nation." The resolution's companion legislation is expected to be voted on in the state House of Representatives in July 2020. If passed, the question would be decided as part of the 2020 United States elections. On June 22, 2020, Rhode Island Governor Gina Raimondo issued an executive order to remove "Providence Plantations" from a range of official documents and state websites.

In 1636, Roger Williams was banished from the Massachusetts Bay Colony for his religious views, and he settled at the top of Narragansett Bay on land sold or given to him by Narragansett sachem Canonicus. He named the site Providence, "having a sense of God's merciful providence unto me in my distress", and it became a place of religious freedom where all were welcome.

In 1638 (after conferring with Williams), Anne Hutchinson, William Coddington, John Clarke, Philip Sherman, and other religious dissenters settled on Aquidneck Island (then known as Rhode Island), which was purchased from the local tribes who called it Pocasset. This settlement was called Portsmouth and was governed by the Portsmouth Compact. The southern part of the island became the separate settlement of Newport after disagreements among the founders.

Samuel Gorton purchased lands at Shawomet in 1642 from the Narragansetts, precipitating a dispute with the Massachusetts Bay Colony. In 1644, Providence, Portsmouth, and Newport united for their common independence as the Colony of Rhode Island and Providence Plantations, governed by an elected council and "president". Gorton received a separate charter for his settlement in 1648 which he named Warwick after his patron.
Metacomet was the Wampanoag tribe's war leader, whom the colonists called King Philip. They invaded and burned down several of the towns in the area during King Philip's War (1675–1676), including Providence which was attacked twice. A force of Massachusetts, Connecticut, and Plymouth militia under General Josiah Winslow invaded and destroyed the fortified Narragansett Indian village in the Great Swamp in South Kingstown, Rhode Island on December 19, 1675. In one of the final actions of the war, an Indian associated with Benjamin Church killed King Philip in Bristol, Rhode Island.

The colony was amalgamated into the Dominion of New England in 1686, as King James II attempted to enforce royal authority over the autonomous colonies in British North America, but the colony regained its independence under the Royal Charter after the Glorious Revolution of 1688. Slaves were introduced in Rhode Island at this time, although there is no record of any law legalizing slave-holding. The colony later prospered under the slave trade, distilling rum to sell in Africa as part of a profitable triangular trade in slaves and sugar with the Caribbean. Rhode Island's legislative body passed an act in 1652 abolishing the holding of slaves (the first British colony to do so), but this edict was never enforced and Rhode Island continued to be heavily involved in the slave trade during the post-revolution era. In 1774, the slave population of Rhode Island was 6.3% of the total (nearly twice the ratio of other New England colonies).

Brown University was founded in 1764 as the College in the British Colony of Rhode Island and Providence Plantations. It was one of nine Colonial colleges granted charters before the American Revolution, but was the first college in America to accept students regardless of religious affiliation.

Rhode Island's tradition of independence and dissent gave it a prominent role in the American Revolution. At approximately 2 a.m. on June 10, 1772, a band of Providence residents attacked the grounded revenue schooner "Gaspee", burning it to the waterline for enforcing unpopular trade regulations within Narragansett Bay. Rhode Island was the first of the thirteen colonies to renounce its allegiance to the British Crown on May 4, 1776. It was also the last of the thirteen colonies to ratify the United States Constitution on May 29, 1790, and only under threat of heavy trade tariffs from the other former colonies and after assurances were made that a Bill of Rights would become part of the Constitution. During the Revolution, the British occupied Newport in December 1776. A combined Franco-American force fought to drive them off Aquidneck Island. Portsmouth was the site of the first African-American military unit, the 1st Rhode Island Regiment, to fight for the U.S. in the unsuccessful Battle of Rhode Island of August 29, 1778. A month earlier, the appearance of a French fleet off Newport caused the British to scuttle some of their own ships in an attempt to block the harbor. The British abandoned Newport in October 1779, concentrating their forces in New York City. An expedition of 5,500 French troops under Count Rochambeau arrived in Newport by sea on July 10, 1780. The celebrated march to Yorktown, Virginia, in 1781 ended with the defeat of the British at the Siege of Yorktown and the Battle of the Chesapeake.
Rhode Island was also heavily involved in the Industrial Revolution, which began in America in 1787 when Thomas Somers reproduced textile machine plans which he imported from England. He helped to produce the Beverly Cotton Manufactory, in which Moses Brown of Providence took an interest. Moses Brown teamed up with Samuel Slater and helped to create the second cotton mill in America, a water-powered textile mill. The Industrial Revolution moved large numbers of workers into the cities, creating a permanently landless class who were therefore, by the law of the time, also voteless. By 1829, 60% of the state's free white males were ineligible to vote. Several attempts were unsuccessfully made to address this problem, and a new state constitution was passed in 1843 allowing landless men to vote if they could pay a $1 poll tax.

For the first several decades of statehood, Rhode Island was governed in accordance with the 1663 colonial charter. Voting rights were restricted to landowners holding at least $134 in property, disenfranchising well over half of the state's male citizens. The charter apportioned legislative seats equally among the state's towns, over-representing rural areas and under-representing the growing industrial centers. Additionally, the charter disallowed landless citizens from filing civil suits without endorsement from a landowner. Bills were periodically introduced in the legislature to expand suffrage, but they were invariably defeated. In 1841, activists led by Thomas W. Dorr organized an extralegal convention to draft a state constitution, arguing that the charter government violated the Guarantee Clause in Article Four, Section Four of the United States Constitution. In 1842, the charter government and Dorr's supporters held separate elections, and two rival governments claimed sovereignty over the state. Dorr's supporters led an armed rebellion against the charter government, and Dorr was arrested and imprisoned for treason against the state. Later that year, the legislature drafted a state constitution, removing property requirements for American-born citizens but keeping them in place for immigrants, and retaining urban under-representation in the legislature.

In the early 19th century, Rhode Island was subject to a tuberculosis outbreak which led to public hysteria about vampirism.

During the American Civil War, Rhode Island was the first Union state to send troops in response to President Lincoln's request for help from the states. Rhode Island furnished 25,236 fighting men, of whom 1,685 died. On the home front, Rhode Island and the other northern states used their industrial capacity to supply the Union Army with the materials that it needed to win the war. The United States Naval Academy moved to Rhode Island temporarily during the war.

In 1866, Rhode Island abolished racial segregation in the public schools throughout the state.

The 50 years following the Civil War were a time of prosperity and affluence that author William G. McLoughlin calls "Rhode Island's halcyon era." Rhode Island was a center of the Gilded Age and provided a home or summer home to many of the country's most prominent industrialists. This was a time of growth in textile mills and manufacturing and brought an influx of immigrants to fill those jobs, bringing population growth and urbanization. In Newport, New York's wealthiest industrialists created a summer haven to socialize and build grand mansions. Thousands of French-Canadian, Italian, Irish, and Portuguese immigrants arrived to fill jobs in the textile and manufacturing mills in Providence, Pawtucket, Central Falls, and Woonsocket.

During World War I, Rhode Island furnished 28,817 soldiers, of whom 612 died. After the war, the state was hit hard by the Spanish Influenza.

In the 1920s and 1930s, rural Rhode Island saw a surge in Ku Klux Klan membership, largely in reaction to large waves of immigrants moving to the state. The Klan is believed to be responsible for burning the Watchman Industrial School in Scituate, which was a school for African-American children.

Since the Great Depression, the Rhode Island Democratic Party has dominated local politics. Rhode Island has comprehensive health insurance for low-income children and a large social safety net. Many urban areas still have a high rate of children in poverty. Due to an influx of residents from Boston, increasing housing costs have resulted in more homelessness in Rhode Island.

The 350th Anniversary of the founding of Rhode Island was celebrated with a free concert held on the tarmac of the Quonset State Airport on August 31, 1986. Performers included Chuck Berry, Tommy James, and headliner Bob Hope.

In 2003, a nightclub fire in West Warwick claimed 100 lives and resulted in nearly twice as many injured, catching national attention. The fire resulted in criminal sentences.

In March 2010, areas of the state received record flooding due to rising rivers from heavy rain. The first period of rainy weather in mid-March caused localized flooding and, two weeks later, more rain caused more widespread flooding in many towns, especially south of Providence. Rain totals on March 29–30, 2010 exceeded 14 inches (35.5 cm) in many locales, resulting in the inundation of area rivers—especially the Pawtuxet River which runs through central Rhode Island. The overflow of the Pawtuxet River, nearly above flood stage, submerged a sewage treatment plant and closed a five-mile (8 km) stretch of Interstate 95. In addition, it flooded two shopping malls, numerous businesses, and many homes in the towns of Warwick, West Warwick, Cranston, and Westerly. Amtrak service was also suspended between New York and Boston during this period. Following the flood, Rhode Island was in a state of emergency for two days. The Federal Emergency Management Agency (FEMA) was called in to help flood victims.

Rhode Island covers an area of located within the New England region and is bordered on the north and east by Massachusetts, on the west by Connecticut, and on the south by Rhode Island Sound and the Atlantic Ocean. It shares a narrow maritime border with New York State between Block Island and Long Island. The mean elevation of the state is . It is only wide and long, yet the state has a tidal shoreline on Narragansett Bay and the Atlantic Ocean of .

Rhode Island is nicknamed the Ocean State and has a number of oceanfront beaches. It is mostly flat with no real mountains, and the state's highest natural point is Jerimoth Hill, above sea level. The state has two distinct natural regions. Eastern Rhode Island contains the lowlands of the Narragansett Bay, while Western Rhode Island forms part of the New England upland. Rhode Island's forests are part of the Northeastern coastal forests ecoregion.

Narragansett Bay is a major feature of the state's topography. There are more than 30 islands within the bay; the largest is Aquidneck Island which holds the municipalities of Newport, Middletown, and Portsmouth. The second-largest island is Conanicut, and the third is Prudence. Block Island lies about off the southern coast of the mainland and separates Block Island Sound and the Atlantic Ocean proper.

A rare type of rock called Cumberlandite is found only in Rhode Island (specifically, in the town of Cumberland) and is the state rock. There were initially two known deposits of the mineral, but it is an ore of iron, and one of the deposits was extensively mined for its ferrous content.

Most of Rhode Island has a humid continental climate, with warm summers and cold winters. The southern coastal portions of the state are the broad transition zone into subtropical climates, with hot summers and cool winters with a mix of rain and snow. Block Island has an oceanic climate. The highest temperature recorded in Rhode Island was , recorded on August 2, 1975 in Providence. The lowest recorded temperature in Rhode Island was on February 5, 1996 in Greene. Monthly average temperatures range from a high of to a low of .

Rhode Island is vulnerable to tropical storms and hurricanes due to its location in New England, catching the brunt of many storms blowing up the eastern seaboard. Some hurricanes that have done significant damage in the state are the 1938 New England hurricane, Hurricane Carol (1954), Hurricane Donna (1960), and Hurricane Bob (1991).

The capital of Rhode Island is Providence. The state's current governor is Gina Raimondo (D), and the lieutenant governor is Daniel McKee (D). Raimondo became Rhode Island's first female governor with a plurality of the vote in the November 2014 state elections. Its United States senators are Jack Reed (D) and Sheldon Whitehouse (D). Rhode Island's two United States representatives are David Cicilline (D-1) and Jim Langevin (D-2). "See congressional districts map." Rhode Island is one of a few states that do not have an official governor's residence. "See List of Rhode Island Governors."

The state legislature is the Rhode Island General Assembly, consisting of the 75-member House of Representatives and the 38-member Senate. Both houses of the bicameral body are currently dominated by the Democratic Party; the presence of the Republican Party is minor in the state government, with Republicans holding a handful of seats in both the Senate and House of Representatives.

Rhode Island's population barely crosses the threshold beyond the minimum of three for additional votes in both the federal House of Representatives and Electoral College; it is well represented relative to its population, with the eighth-highest number of electoral votes and second-highest number of House Representatives per resident. Based on its area, Rhode Island even has the highest density of electoral votes.

Federally, Rhode Island is a reliably Democratic state during presidential elections, usually supporting the Democratic presidential nominee. The state voted for the Republican presidential candidate until 1908. Since then, it has voted for the Republican nominee for president seven times, and the Democratic nominee 17 times. The last 16 presidential elections in Rhode Island have resulted in the Democratic Party winning the Ocean State's Electoral College votes 12 times. In the 1980 presidential election, Rhode Island was one of six states to vote against Republican Ronald Reagan. Reagan was the last Republican to win any of the state's counties in a Presidential election until Donald Trump won Kent County in 2016. In 1988, George H. W. Bush won over 40% of the state's popular vote, something that no Republican has done since.
Rhode Island was the Democrats' leading state in 1988 and 2000, and second-best in 1968, 1996, and 2004. Rhode Island's most one-sided Presidential election result was in 1964, with over 80% of Rhode Island's votes going for Lyndon B. Johnson. In 2004, Rhode Island gave John Kerry more than a 20-percentage-point margin of victory (the third-highest of any state), with 59.4% of its vote. All but three of Rhode Island's 39 cities and towns voted for the Democratic candidate. The exceptions were East Greenwich, West Greenwich, and Scituate. In 2008, Rhode Island gave Barack Obama a 28-percentage-point margin of victory (the third-highest of any state), with 63% of its vote. All but one of Rhode Island's 39 cities and towns voted for the Democratic candidate (the exception being Scituate).

Rhode Island is one of 21 states that have abolished capital punishment; it was second do so, just after Michigan, and carried out its last execution in the 1840s. Rhode Island was the second to last state to make prostitution illegal. Until November 2009 Rhode Island law made prostitution legal provided it took place indoors. In a 2009 study Rhode Island was listed as the 9th safest state in the country.

In 2011, Rhode Island became the third state in the United States to pass legislation to allow the use of medical marijuana. Additionally, the Rhode Island General Assembly passed civil unions, and it was signed into law by Governor Lincoln Chafee on July 2, 2011. Rhode Island became the eighth state to fully recognize either same-sex marriage or civil unions. Same-sex marriage became legal on May 2, 2013, and took effect August 1.

Rhode Island has some of the highest taxes in the country, particularly its property taxes, ranking seventh in local and state taxes, and sixth in real estate taxes.

The United States Census Bureau estimates that the population of Rhode Island was 1,059,361 on July 1, 2019, a 0.65% increase since the 2010 United States Census. The center of population of Rhode Island is located in Providence County, in the city of Cranston. A corridor of population can be seen from the Providence area, stretching northwest following the Blackstone River to Woonsocket, where 19th-century mills drove industry and development.

According to the 2010 Census, 81.4% of the population was White (76.4% non-Hispanic white), 5.7% was Black or African American, 0.6% American Indian and Alaska Native, 2.9% Asian, 0.1% Native Hawaiian and other Pacific Islander, 3.3% from two or more races. 12.4% of the total population was of Hispanic or Latino origin (they may be of any race).

Of the people residing in Rhode Island, 58.7% were born in Rhode Island, 26.6% were born in a different state, 2.0% were born in Puerto Rico, U.S. Island areas or born abroad to American parent(s), and 12.6% were foreign born.

According to the U.S. Census Bureau, , Rhode Island had an estimated population of 1,056,298, which is an increase of 1,125, or 0.10%, from the prior year and an increase of 3,731, or 0.35%, since the year 2010. This includes a natural increase since the last census of 15,220 people (that is 66,973 births minus 51,753 deaths) and an increase due to net migration of 14,001 people into the state. Immigration from outside the United States resulted in a net increase of 18,965 people, and migration within the country produced a net decrease of 4,964 people.

Hispanics in the state make up 12.8% of the population, predominantly Dominican, Puerto Rican, and Guatemalan populations.

According to the 2000 U.S. Census, 84% of the population aged 5 and older spoke only American English, while 8.07% spoke Spanish at home, 3.80% Portuguese, 1.96% French, 1.39% Italian and 0.78% speak other languages at home accordingly.

The state's most populous ethnic group, non-Hispanic white, has declined from 96.1% in 1970 to 76.5% in 2011. In 2011, 40.3% of Rhode Island's children under the age of one belonged to racial or ethnic minority groups, meaning that they had at least one parent who was not non-Hispanic white.

6.1% of Rhode Island's population were reported as under 5, 23.6% under 18, and 14.5% were 65 or older. Females made up approximately 52% of the population.

According to the 2010–2015 American Community Survey, the largest ancestry groups were Irish (18.3%), Italian (18.0%), English (10.5%), French (10.4%), and Portuguese (9.3%).

Rhode Island has a higher percentage of Americans of Portuguese ancestry, including Portuguese Americans and Cape Verdean Americans than any other state in the nation. Additionally, the state also has the highest percentage of Liberian immigrants, with more than 15,000 residing in the state. Italian Americans make up a plurality in central and southern Providence County and French-Canadian Americans form a large part of northern Providence County. Irish Americans have a strong presence in Newport and Kent counties. Americans of English ancestry still have a presence in the state as well, especially in Washington County, and are often referred to as "Swamp Yankees." African immigrants, including Cape Verdean Americans, Liberian Americans, Nigerian Americans and Ghanaian Americans, form significant and growing communities in Rhode Island.

Although Rhode Island has the smallest land area of all 50 states, it has the second highest population density of any state in the Union, second to that of New Jersey.


A Pew survey of Rhode Island residents' religious self-identification showed the following distribution of affiliations: Roman Catholic 42%, Protestant 30%, Jewish 1%, Jehovah's Witnesses 2%, Buddhism 1%, Mormonism 1%, Hinduism 1%, and Non-religious 20%. The largest denominations are the Roman Catholic Church with 456,598 adherents, the Episcopal Church with 19,377, the American Baptist Churches USA with 15,220, and the United Methodist Church with 6,901 adherents.

Rhode Island has the highest proportion of Roman Catholic residents of any state, mainly due to large Irish, Italian, and French-Canadian immigration in the past; recently, significant Portuguese and various Hispanic communities have also been established in the state. Though it has the highest overall Catholic percentage of any state, none of Rhode Island's individual counties ranks among the 10 most Catholic in the United States, as Catholics are very evenly spread throughout the state.

The Jewish community of Rhode Island is centered in the Providence area, and emerged during a wave of Jewish immigration predominantly from Eastern Europeans shtetls between 1880 and 1920. The presence of the Touro Synagogue in Newport, the oldest existing synagogue in the United States, emphasizes that these second-wave immigrants did not create Rhode Island's first Jewish community; a comparatively smaller wave of Spanish and Portuguese Jews immigrated to Newport during the colonial era.

Rhode Island is divided into five counties but it has no county governments. The entire state is divided into municipalities, which handle all local government affairs.

There are 39 cities and towns in Rhode Island. Major population centers today result from historical factors; development took place predominantly along the Blackstone, Seekonk, and Providence Rivers with the advent of the water-powered mill. Providence is the base of a large metropolitan area.

The state's 18 largest municipalities ranked by population are :

Some of Rhode Island's cities and towns are further partitioned into villages, in common with many other New England states. Notable villages include Kingston in the town of South Kingstown, which houses the University of Rhode Island; Wickford in the town of North Kingstown, the site of an annual international art festival; and Wakefield where the Town Hall is located for the Town of South Kingstown.
The Rhode Island economy had a colonial base in fishing.

The Blackstone River Valley was a major contributor to the American Industrial Revolution. It was in Pawtucket that Samuel Slater set up Slater Mill in 1793, using the waterpower of the Blackstone River to power his cotton mill. For a while, Rhode Island was one of the leaders in textiles. However, with the Great Depression, most textile factories relocated to southern U.S. states. The textile industry still constitutes a part of the Rhode Island economy but does not have the same power that it once had.

Other important industries in Rhode Island's past included toolmaking, costume jewelry, and silverware. An interesting by-product of Rhode Island's industrial history is the number of abandoned factories, many of them now being used for condominiums, museums, offices, and low-income and elderly housing. Today, much of the economy of Rhode Island is based in services, particularly healthcare and education, and still manufacturing to some extent. The state's nautical history continues in the 21st century in the form of nuclear submarine construction.

Per the 2013 American Communities Survey, Rhode Island has the highest paid elementary school teachers in the country, with an average salary of $75,028 (adjusted to inflation).

The headquarters of Citizens Financial Group is located in Providence, the 14th largest bank in the United States. The Fortune 500 companies CVS Caremark and Textron are based in Woonsocket and Providence, respectively. FM Global, GTECH Corporation, Hasbro, American Power Conversion, Nortek, and Amica Mutual Insurance are all Fortune 1000 companies that are based in Rhode Island.

Rhode Island's 2000 total gross state production was $46.18 billion (adjusted to inflation), placing it 45th in the nation. Its 2000 "per capita" personal income was $41,484 (adjusted to inflation), 16th in the nation. Rhode Island has the lowest level of energy consumption per capita of any state. Additionally, Rhode Island is rated as the 5th most energy efficient state in the country. In December 2012, the state's unemployment rate was 10.2%.

Health services are Rhode Island's largest industry. Second is tourism, supporting 39,000 jobs, with tourism-related sales at $4.56 billion (adjusted to inflation) in the year 2000. The third-largest industry is manufacturing. Its industrial outputs are submarine construction, shipbuilding, costume jewelry, fabricated metal products, electrical equipment, machinery, and boatbuilding. Rhode Island's agricultural outputs are nursery stock, vegetables, dairy products, and eggs.

Rhode Island's taxes were appreciably higher than neighboring states, because Rhode Island's income tax was based on 25% of the payer's federal income tax payment. Former Governor Donald Carcieri claimed that the higher tax rate had an inhibitory effect on business growth in the state and called for reductions to increase the competitiveness of the state's business environment. In 2010, the Rhode Island General Assembly passed a new state income tax structure that was then signed into law on June 9, 2010, by Governor Carcieri. The income tax overhaul has now made Rhode Island competitive with other New England states by lowering its maximum tax rate to 5.99% and reducing the number of tax brackets to three. The state's first income tax was enacted in 1971.

, the largest employers in Rhode Island (excluding employees of municipalities) are the following:

The Rhode Island Public Transit Authority (RIPTA) operates statewide intra- and intercity bus transport from its hubs at Kennedy Plaza in Providence, Pawtucket, and Newport. RIPTA bus routes serve 38 of Rhode Island's 39 cities and towns. (New Shoreham on Block Island is not served). RIPTA currently operates 58 routes, including daytime trolley service (using trolley-style replica buses) in Providence and Newport.

From 2000 through 2008, RIPTA offered seasonal ferry service linking Providence and Newport (already connected by highway) funded by grant money from the United States Department of Transportation. Though the service was popular with residents and tourists, RIPTA was unable to continue on after the federal funding ended. Service was discontinued . The service was resumed in 2016 and has been successful. The privately run Block Island Ferry links Block Island with Newport and Narragansett with traditional and fast-ferry service, while the Prudence Island Ferry connects Bristol with Prudence Island. Private ferry services also link several Rhode Island communities with ports in Connecticut, Massachusetts, and New York. The Vineyard Fast Ferry offers seasonal service to Martha's Vineyard from Quonset Point with bus and train connections to Providence, Boston, and New York. Viking Fleet offers seasonal service from Block Island to New London, Connecticut, and Montauk, New York.

The MBTA Commuter Rail's Providence/Stoughton Line links Providence and T. F. Green Airport with Boston. The line was later extended southward to Wickford Junction, with service beginning April 23, 2012. The state hopes to extend the MBTA line to Kingston and Westerly, as well as explore the possibility of extending Connecticut's Shore Line East to T.F. Green Airport. Amtrak's Acela Express stops at Providence Station (the only Acela stop in Rhode Island), linking Providence to other cities in the Northeast Corridor. Amtrak's Northeast Regional service makes stops at Providence Station, Kingston, and Westerly.

Rhode Island's primary airport for passenger and cargo transport is T. F. Green Airport in Warwick, though most Rhode Islanders who wish to travel internationally on direct flights and those who seek a greater availability of flights and destinations often fly through Logan International Airport in Boston.

Interstate 95 (I-95) runs southwest to northeast across the state, linking Rhode Island with other states along the East Coast. I-295 functions as a partial beltway encircling Providence to the west. I-195 provides a limited-access highway connection from Providence (and Connecticut and New York via I-95) to Cape Cod. Initially built as the easternmost link in the (now cancelled) extension of I-84 from Hartford, Connecticut, a portion of U.S. Route 6 (US 6) through northern Rhode Island is limited-access and links I-295 with downtown Providence.

Several Rhode Island highways extend the state's limited-access highway network. Route 4 is a major north–south freeway linking Providence and Warwick (via I-95) with suburban and beach communities along Narragansett Bay. Route 10 is an urban connector linking downtown Providence with Cranston and Johnston. Route 37 is an important east–west freeway through Cranston and Warwick and links I-95 with I-295. Route 99 links Woonsocket with Providence (via Route 146). Route 146 travels through the Blackstone Valley, linking Providence and I-95 with Worcester, Massachusetts and the Massachusetts Turnpike. Route 403 links Route 4 with Quonset Point.

Several bridges cross Narragansett Bay connecting Aquidneck Island and Conanicut Island to the mainland, most notably the Claiborne Pell Newport Bridge and the Jamestown-Verrazano Bridge.

The East Bay Bike Path stretches from Providence to Bristol along the eastern shore of Narragansett Bay, while the Blackstone River Bikeway will eventually link Providence and Worcester. In 2011, Rhode Island completed work on a marked on-road bicycle path through Pawtucket and Providence, connecting the East Bay Bike Path with the Blackstone River Bikeway, completing a bicycle route through the eastern side of the state. The William C. O'Neill Bike Path (commonly known as the South County Bike Path) is an path through South Kingstown and Narragansett. The Washington Secondary Bike Path stretches from Cranston to Coventry, and the Ten Mile River Greenway path runs through East Providence and Pawtucket.

On May 29, 2014, Governor Lincoln D. Chafee announced that Rhode Island was one of eight states to release a collaborative Action Plan to put 3.3 million zero emission vehicles on the roads by 2025. The goal of the plan is to reduce greenhouse gas and smog-causing emissions. The Action Plan covers promoting zero emission vehicles and investing in the infrastructure to support them.

In 2014, Rhode Island received grants from the Environmental Protection Agency in the amount of $2,711,685 to clean up Brownfield sites in eight locations. The intent of the grants was to provide communities with the funding necessary to assess, clean up, and redevelop contaminated properties, boost local economies, and leverage jobs while protecting public health and the environment.

In 2013, the "Lots of Hope" program was established in the City of Providence to focus on increasing the city's green space and local food production, improve urban neighborhoods, promote healthy lifestyles and improve environmental sustainability. "Lots of Hope", supported by a $100,000 grant, will partner with the City of Providence, the Southside Community Land Trust and the Rhode Island Foundation to convert city-owned vacant lots into productive urban farms.

In 2012, Rhode Island passed bill S2277/H7412, "An act relating to Health and Safety – Environmental Cleanup Objectives for Schools", informally known as the "School Siting Bill." The bill, sponsored by Senator Juan Pichardo and Representative Scott Slater and signed into law by the Governor, made Rhode Island the first state in the US to prohibit school construction on Brownfield Sites where there is an ongoing potential for toxic vapors to negatively impact indoor air quality. It also creates a public participation process whenever a city or town considers building a school on any other kind of contaminated site.

Rhode Island has several colleges and universities:

Some Rhode Islanders speak with the distinctive, non-rhotic, traditional Rhode Island accent that many compare to a cross between the New York City and Boston accents (e.g., "water" sounds like "watuh"). Many Rhode Islanders distinguish a strong "aw" sound (i.e., do not exhibit the cot–caught merger) as one might hear in New Jersey or New York City; for example, the word "coffee" is pronounced . This type of accent may have been brought to the region by early settlers from eastern England in the Puritan migration to New England in the mid-17th century.

Rhode Islanders refer to a drinking fountain as a "bubbler" (sometimes pronounced "bubahluh") and sometimes call milkshakes "cabinets". A foot-long, overstuffed sandwich (of whatever kind) is called a "grinder."

Rhode Island, like the rest of New England, has a tradition of clam chowder. Both the white New England and the red Manhattan varieties are popular, but there is also a unique clear-broth chowder known as "Rhode Island Clam Chowder" available in many restaurants. A culinary tradition in Rhode Island is the "clam cake" (also known as a clam fritter outside of Rhode Island), a deep fried ball of buttery dough with chopped bits of clam inside. They are sold by the half-dozen or dozen in most seafood restaurants around the state, and the quintessential summer meal in Rhode Island is chowder and clam cakes.

The quahog is a large local clam usually used in a chowder. It is also ground and mixed with stuffing or spicy minced sausage, and then baked in its shell to form a "stuffie". Calamari (squid) is sliced into rings and fried as an appetizer in most Italian restaurants, typically served Sicilian-style with sliced banana peppers and marinara sauce on the side. (In 2014, calamari became the official state appetizer.) Clams Casino originated in Rhode Island, invented by Julius Keller, the maitre d' in the original Casino next to the seaside Towers in Narragansett. Clams Casino resemble the beloved stuffed quahog but are generally made with the smaller littleneck or cherrystone clam and are unique in their use of bacon as a topping.

The official state drink of Rhode Island is "coffee milk", a beverage created by mixing milk with coffee syrup. This unique syrup was invented in the state and is sold in almost all Rhode Island supermarkets, as well as its bordering states. Johnnycakes have been a Rhode Island staple since Colonial times, made with corn meal and water then pan-fried much like pancakes.

Submarine sandwiches are called "grinders" throughout Rhode Island, and the Italian grinder is especially popular, made with cold cuts such as ham, prosciutto, capicola, salami, and Provolone cheese. Linguiça or chouriço is a spicy Portuguese sausage, frequently served with peppers among the state's large Portuguese community and eaten with hearty bread.

The Farrelly brothers and Seth MacFarlane depict Rhode Island in popular culture, often making comedic parodies of the state. MacFarlane's television series "Family Guy" is based in a fictional Rhode Island city named Quahog, and notable local events and celebrities are regularly lampooned. Peter Griffin is seen working at the Pawtucket brewery, and other state locations are mentioned.

The movie "High Society" (starring Bing Crosby, Grace Kelly, and Frank Sinatra) was set in Newport, Rhode Island.

The 1974 film adaptation of "The Great Gatsby" was also filmed in Newport.

Jacqueline Bouvier and John F. Kennedy were married at St. Mary's church in Newport. Their reception was held at Hammersmith Farm, the Bouvier summer home in Newport.

Cartoonist Don Bousquet, a state icon, has made a career out of Rhode Island culture, drawing Rhode Island-themed gags in "The Providence Journal" and "Yankee" magazine. These cartoons have been reprinted in the "Quahog" series of paperbacks ("I Brake for Quahogs", "Beware of the Quahog", and "The Quahog Walks Among Us".) Bousquet has also collaborated with humorist and "Providence Journal" columnist Mark Patinkin on two books: "The Rhode Island Dictionary" and "The Rhode Island Handbook".

The 1998 film "Meet Joe Black" was filmed at Aldrich Mansion in the Warwick Neck area of Warwick.

"Body of Proof"s first season was filmed entirely in Rhode Island. The show premiered on March 29, 2011.

The 2007 Steve Carell and Dane Cook film "Dan in Real Life" was filmed in various coastal towns in the state. The sunset scene with the entire family on the beach takes place at Napatree Point.

"Jersey Shore" star Pauly D filmed part of his spin-off "The Pauly D Project" in his hometown of Johnston.

The Comedy Central cable television series "Another Period" is set in Newport during the Gilded Age.

Rhode Island has been the first in a number of initiatives. The Colony of Rhode Island and Providence Plantations enacted the first law prohibiting slavery in America on May 18, 1652.

The first act of armed rebellion in America against the British Crown was the boarding and burning of the Revenue Schooner "Gaspee" in Narragansett Bay on June 10, 1772. The idea of a Continental Congress was first proposed at a town meeting in Providence on May 17, 1774. Rhode Island elected the first delegates (Stephen Hopkins and Samuel Ward) to the Continental Congress on June 15, 1774. The Rhode Island General Assembly created the first standing army in the colonies (1,500 men) on April 22, 1775. On June 15, 1775, the first naval engagement took place in the American Revolution between an American sloop commanded by Capt. Abraham Whipple and an armed tender of the British Frigate "Rose". The tender was chased aground and captured. Later in June, the General Assembly created the American Navy when it commissioned the sloops "Katy" and , armed with 24 guns and commanded by Abraham Whipple who was promoted to Commodore. Rhode Island was the first Colony to declare independence from Britain on May 4, 1776.

Slater Mill in Pawtucket was the first commercially successful cotton-spinning mill with a fully mechanized power system in America and was the birthplace of the Industrial Revolution in the US. The oldest Fourth of July parade in the country is still held annually in Bristol, Rhode Island. The first Baptist church in America was founded in Providence in 1638. Ann Smith Franklin of the Newport "Mercury" was the first female newspaper editor in America (August 22, 1762). Touro Synagogue was the first synagogue in America, founded in Newport in 1763.

Pelham Street in Newport was the first in America to be illuminated by gaslight in 1806. The first strike in the United States in which women participated occurred in Pawtucket in 1824. Watch Hill has the nation's oldest flying horses carousel that has been in continuous operation since 1850. The motion picture machine was patented in Providence on April 23, 1867. The first lunch wagon in America was introduced in Providence in 1872. The first nine-hole golf course in America was completed in Newport in 1890. The first state health laboratory was established in Providence on September 1, 1894. The Rhode Island State House was the first building with an all-marble dome to be built in the United States (1895–1901). The first automobile race on a track was held in Cranston on September 7, 1896. The first automobile parade was held in Newport on September 7, 1899 on the grounds of Belcourt Castle.

Rhode Island is nicknamed "The Ocean State", and the nautical nature of Rhode Island's geography pervades its culture. Newport Harbor, in particular, holds many pleasure boats. In the lobby of T. F. Green, the state's main airport, is a large life-sized sailboat, and the state's license plates depict an ocean wave or a sailboat.

Additionally, the large number of beaches in Washington County lures many Rhode Islanders south for summer vacation.

The state was notorious for organized crime activity from the 1950s into the 1990s when the Patriarca crime family held sway over most of New England from its Providence headquarters.

Rhode Islanders developed a unique style of architecture in the 17th century called the stone-ender.

Rhode Island is the only state to still celebrate Victory over Japan Day which is officially named "Victory Day" but is sometimes referred to as "VJ Day." It is celebrated on the second Monday in August.

Nibbles Woodaway, more commonly referred to as "The Big Blue Bug", is a 58-foot-long termite mascot for a Providence extermination business. Since its construction in 1980, it has been featured in several movies and television shows, and has come to be recognized as a cultural landmark by many locals.

Rhode Island has two professional sports teams, both of which are top-level minor league affiliates for teams in Boston. The Pawtucket Red Sox baseball team of the Triple-A International League are an affiliate of the Boston Red Sox. They play at McCoy Stadium in Pawtucket and have won four league titles, the Governors' Cup, in 1973, 1984, 2012, and 2014. McCoy Stadium also has the distinction of being home to the longest professional baseball game ever played – 33 innings.

The other professional minor league team is the Providence Bruins ice hockey team of the American Hockey League, who are an affiliate of the Boston Bruins. They play in the Dunkin' Donuts Center in Providence and won the AHL's Calder Cup during the 1998–99 AHL season.

The Providence Reds were a hockey team that played in the Canadian-American Hockey League (CAHL) between 1926 and 1936 and the American Hockey League (AHL) from 1936 to 1977, the last season of which they played as the Rhode Island Reds. The team won the Calder Cup in 1938, 1940, 1949, and 1956. The Reds played at the Rhode Island Auditorium, located on North Main Street in Providence, Rhode Island from 1926 through 1972, when the team affiliated with the New York Rangers and moved into the newly built Providence Civic Center. The team name came from the rooster known as the Rhode Island Red. They moved to New York in 1977, then to Connecticut in 1997, and are now called the Hartford Wolf Pack.

The Reds are the oldest continuously operating minor-league hockey franchise in North America, having fielded a team in one form or another since 1926 in the CAHL. It is also the only AHL franchise to have never missed a season. The AHL returned to Providence in 1992 in the form of the Providence Bruins.
Before the great expansion of athletic teams all over the country, Providence and Rhode Island in general played a great role in supporting teams. The Providence Grays won the first World Championship in baseball history in 1884. The team played their home games at the old Messer Street Field in Providence. The Grays played in the National League from 1878 to 1885. They defeated the New York Metropolitans of the American Association in a best of five game series at the Polo Grounds in New York. Providence won three straight games to become the first champions in major league baseball history. Babe Ruth played for the minor league Providence Grays of 1914 and hit his only official minor league home run for that team before being recalled by the Grays' parent club, the Boston Red Stockings.

The now-defunct professional football team the Providence Steam Roller won the 1928 NFL title. They played in a 10,000 person stadium called the Cycledrome. The Providence Steamrollers played in the Basketball Association of America which became the National Basketball Association.

Rhode Island is also home to a top semi-professional soccer club, the Rhode Island Reds, which compete in the National premier soccer league, in the fourth division of U.S. Soccer.

Rhode Island is home to one top level non-minor league team, the Rhode Island Rebellion rugby league team, a semi-professional rugby league team that competes in the USA Rugby League, the Top Competition in the United States for the Sport of Rugby League. The Rebellion play their home games at Classical High School in Providence.

There are four NCAA Division I schools in Rhode Island. All four schools compete in different conferences. The Brown University Bears compete in the Ivy League, the Bryant University Bulldogs compete in the Northeast Conference, the Providence College Friars compete in the Big East Conference, and the University of Rhode Island Rams compete in the Atlantic-10 Conference. Three of the schools' football teams compete in the Football Championship Subdivision, the second-highest level of college football in the United States. Brown plays FCS football in the Ivy League, Bryant plays FCS football in the Northeast Conference, and Rhode Island plays FCS football in the Colonial Athletic Association. All four of the Division I schools in the state compete in an intrastate all-sports competition known as the Ocean State Cup, with Bryant winning the most recent cup in 2011–12 academic year.

From 1930 to 1983, America's Cup races were sailed off Newport, and the extreme-sport X Games and Gravity Games were founded and hosted in the state's capital city.

The International Tennis Hall of Fame is in Newport at the Newport Casino, site of the first U.S. National Championships in 1881. The Hall of Fame and Museum were established in 1954 by James Van Alen as "a shrine to the ideals of the game".

Rhode Island is also home to the headquarters of the governing body for youth rugby league in the United States, the American Youth Rugby League Association or AYRLA. The AYRLA has started the first-ever Rugby League youth competition in Providence Middle Schools, a program at the RI Training School, in addition to starting the first High School Competition in the US in Providence Public High School.

The state capitol building is made of white Georgian marble. On top is the world's fourth largest self-supported marble dome. It houses the Rhode Island Charter granted by King Charles II in 1663, the Brown University charter, and other state treasures.

The First Baptist Church of Providence is the oldest Baptist church in the Americas, founded by Roger Williams in 1638.

The first fully automated post office in the country is located in Providence. There are many historic mansions in the seaside city of Newport, including The Breakers, Marble House, and Belcourt Castle. Also located there is the Touro Synagogue, dedicated on December 2, 1763, considered by locals to be the first synagogue within the United States (see below for information on New York City's claim), and still serving. The synagogue showcases the religious freedoms that were established by Roger Williams, as well as impressive architecture in a mix of the classic colonial and Sephardic style. The Newport Casino is a National Historic Landmark building complex that presently houses the International Tennis Hall of Fame and features an active grass-court tennis club.

Scenic Route 1A (known locally as Ocean Road) is in Narragansett. "The Towers" is also located in Narragansett featuring a large stone arch. It was once the entrance to a famous Narragansett casino that burned down in 1900. The Towers now serve as an event venue and host the local Chamber of Commerce, which operates a tourist information center.

The Newport Tower has been hypothesized to be of Viking origin, although most experts believe that it was a Colonial-era windmill.






</doc>
<doc id="25412" url="https://en.wikipedia.org/wiki?curid=25412" title="Rock and roll">
Rock and roll

Rock and roll (often written as rock & roll, rock 'n' roll, or rock 'n roll) is a genre of popular music that originated and evolved in the United States during the late 1940s and early 1950s from musical styles such as gospel, jump blues, jazz, boogie woogie, rhythm and blues, and country music. While elements of what was to become rock and roll can be heard in blues records from the 1920s and in country records of the 1930s, the genre did not acquire its name until 1954.

According to journalist Greg Kot, "rock and roll" refers to a style of popular music originating in the U.S. in the 1950s prior to its development by the mid-1960s into "the more encompassing international style known as rock music, though the latter also continued to be known as rock and roll." For the purpose of differentiation, this article deals with the first definition.

In the earliest rock and roll styles, either the piano or saxophone was typically the lead instrument, but these instruments were generally replaced or supplemented by guitar in the middle to late 1950s. The beat is essentially a dance rhythm with an accentuated backbeat, which is almost always provided by a snare drum. Classic rock and roll is usually played with one or two electric guitars (one lead, one rhythm), a double bass (string bass) or after the mid-1950s an electric bass guitar, and a drum kit.

Beyond just a musical style, rock and roll, as depicted in movies, in fan magazines, and on television, influenced lifestyles, fashion, attitudes, and language. Rock and roll may have contributed to the civil rights movement because both African American and White American teenagers enjoyed the music.

The term "rock and roll" is defined by Greg Kot in "Encyclopædia Britannica" as the music that originated in the mid-1950s and later developed "into the more encompassing international style known as rock music". The term is sometimes also used as synonymous with "rock music" and is defined as such in some dictionaries.

The phrase "rocking and rolling" originally described the movement of a ship on the ocean, but by the early 20th century was used both to describe the spiritual fervor of black church rituals and as a sexual analogy. Various gospel, blues and swing recordings used the phrase before it became used more frequently – but still intermittently – in the 1940s, on recordings and in reviews of what became known as "rhythm and blues" music aimed at a black audience.

In 1934, the song "Rock and Roll" by the Boswell Sisters appeared in the film "Transatlantic Merry-Go-Round". In 1942, "Billboard" magazine columnist Maurie Orodenker started to use the term "rock-and-roll" to describe upbeat recordings such as "Rock Me" by Sister Rosetta Tharpe. By 1943, the "Rock and Roll Inn" in South Merchantville, New Jersey, was established as a music venue. In 1951, Cleveland, Ohio, disc jockey Alan Freed began playing this music style while popularizing the phrase to describe it.

The origins of rock and roll have been fiercely debated by commentators and historians of music. There is general agreement that it arose in the Southern United States – a region that would produce most of the major early rock and roll acts – through the meeting of various influences that embodied a merging of the African musical tradition with European instrumentation. The migration of many former slaves and their descendants to major urban centers such as St. Louis, Memphis, New York City, Detroit, Chicago, Cleveland, and Buffalo (See: Second Great Migration (African American)) meant that black and white residents were living in close proximity in larger numbers than ever before, and as a result heard each other's music and even began to emulate each other's fashions. Radio stations that made white and black forms of music available to both groups, the development and spread of the gramophone record, and African-American musical styles such as jazz and swing which were taken up by white musicians, aided this process of "cultural collision".
The immediate roots of rock and roll lay in the rhythm and blues, then called "race music", and country music of the 1940s and 1950s. Particularly significant influences were jazz, blues, gospel, country, and folk. Commentators differ in their views of which of these forms were most important and the degree to which the new music was a re-branding of African-American rhythm and blues for a white market, or a new hybrid of black and white forms.

In the 1930s, jazz, and particularly swing, both in urban-based dance bands and blues-influenced country swing (Jimmie Rodgers, Moon Mullican and other similar singers), were among the first music to present African-American sounds for a predominantly white audience. One particularly noteworthy example of a jazz song with recognizably rock and roll elements is Big Joe Turner with pianist Pete Johnson's 1939 single "Roll 'Em Pete", which is regarded as an important precursor of rock and roll. The 1940s saw the increased use of blaring horns (including saxophones), shouted lyrics and boogie woogie beats in jazz-based music. During and immediately after World War II, with shortages of fuel and limitations on audiences and available personnel, large jazz bands were less economical and tended to be replaced by smaller combos, using guitars, bass and drums. In the same period, particularly on the West Coast and in the Midwest, the development of jump blues, with its guitar riffs, prominent beats and shouted lyrics, prefigured many later developments. In the documentary film "Hail! Hail! Rock 'n' Roll", Keith Richards proposes that Chuck Berry developed his brand of rock and roll by transposing the familiar two-note lead line of jump blues piano directly to the electric guitar, creating what is instantly recognizable as rock guitar. Similarly, country boogie and Chicago electric blues supplied many of the elements that would be seen as characteristic of rock and roll. Inspired by electric blues, Chuck Berry introduced an aggressive guitar sound to rock and roll, and established the electric guitar as its centrepiece, adapting his rock band instrumentation from the basic blues band instrumentation of a lead guitar, second chord instrument, bass and drums. In 2017, Robert Christgau declared that "Chuck Berry did in fact invent rock 'n' roll", explaining that this artist "came the closest of any single figure to being the one who put all the essential pieces together".

Rock and roll arrived at a time of considerable technological change, soon after the development of the electric guitar, amplifier and microphone, and the 45 rpm record. There were also changes in the record industry, with the rise of independent labels like Atlantic, Sun and Chess servicing niche audiences and a similar rise of radio stations that played their music. It was the realization that relatively affluent white teenagers were listening to this music that led to the development of what was to be defined as rock and roll as a distinct genre. Because the development of rock and roll was an evolutionary process, no single record can be identified as unambiguously "the first" rock and roll record. Contenders for the title of "first rock and roll record" include Sister Rosetta Tharpe's "Strange Things Happening Every Day" (1944), "That's All Right" by Arthur Crudup (1946), "The Fat Man" by Fats Domino (1949), Goree Carter's "Rock Awhile" (1949), Jimmy Preston's "Rock the Joint" (1949), which was later covered by Bill Haley & His Comets in 1952, "Rocket 88" by Jackie Brenston and his Delta Cats (Ike Turner and his band The Kings of Rhythm), recorded by Sam Phillips for Sun Records in March 1951. In terms of its wide cultural impact across society in the US and elsewhere, Bill Haley's "Rock Around the Clock", recorded in April 1954 but not a commercial success until the following year, is generally recognized as an important milestone, but it was preceded by many recordings from earlier decades in which elements of rock and roll can be clearly discerned.

Other artists with early rock and roll hits included Chuck Berry, Bo Diddley, Little Richard, Jerry Lee Lewis, and Gene Vincent. Chuck Berry's 1955 classic "Maybellene" in particular features a distorted electric guitar solo with warm overtones created by his small valve amplifier. However, the use of distortion was predated by electric blues guitarists such as Joe Hill Louis, Guitar Slim, Willie Johnson of Howlin' Wolf's band, and Pat Hare; the latter two also made use of distorted power chords in the early 1950s. Also in 1955, Bo Diddley introduced the "Bo Diddley beat" and a unique electric guitar style, influenced by African and Afro-Cuban music and in turn influencing many later artists.

"Rockabilly" usually (but not exclusively) refers to the type of rock and roll music which was played and recorded in the mid-1950s primarily by white singers such as Elvis Presley, Carl Perkins, Johnny Cash, and Jerry Lee Lewis, who drew mainly on the country roots of the music. Elvis Presley was greatly influenced and incorporated his style of music with some of the greatest African American musicians like BB King, Chuck Berry and Fats Domino. His style of music combined with black influences created controversy during a turbulent time in history. Many other popular rock and roll singers of the time, such as Fats Domino and Little Richard, came out of the black rhythm and blues tradition, making the music attractive to white audiences, and are not usually classed as "rockabilly".

Bill Flagg who is a Connecticut resident, began referring to his mix of hillbilly and rock 'n' roll music as rockabilly around 1953. His song "Guitar Rock" is considered as classic rockabilly.

In July 1954, Elvis Presley recorded the regional hit "That's All Right" at Sam Phillips' Sun Studio in Memphis. Three months earlier, on April 12, 1954, Bill Haley & His Comets recorded "Rock Around the Clock". Although only a minor hit when first released, when used in the opening sequence of the movie "Blackboard Jungle" a year later, it set the rock and roll boom in motion. The song became one of the biggest hits in history, and frenzied teens flocked to see Haley and the Comets perform it, causing riots in some cities. "Rock Around the Clock" was a breakthrough for both the group and for all of rock and roll music. If everything that came before laid the groundwork, "Rock Around the Clock" introduced the music to a global audience.

In 1956, the arrival of rockabilly was underlined by the success of songs like "Folsom Prison Blues" by Johnny Cash, "Blue Suede Shoes" by Perkins and the No. 1 hit "Heartbreak Hotel" by Presley. For a few years it became the most commercially successful form of rock and roll. Later rockabilly acts, particularly performing songwriters like Buddy Holly, would be a major influence on British Invasion acts and particularly on the song writing of the Beatles and through them on the nature of later rock music.

Doo-wop was one of the most popular forms of 1950s rhythm and blues, often compared with rock and roll, with an emphasis on multi-part vocal harmonies and meaningless backing lyrics (from which the genre later gained its name), which were usually supported with light instrumentation. Its origins were in African-American vocal groups of the 1930s and 40s, such as the Ink Spots and the Mills Brothers, who had enjoyed considerable commercial success with arrangements based on close harmonies. They were followed by 1940s R&B vocal acts such as the Orioles, the Ravens and the Clovers, who injected a strong element of traditional gospel and, increasingly, the energy of jump blues. By 1954, as rock and roll was beginning to emerge, a number of similar acts began to cross over from the R&B charts to mainstream success, often with added honking brass and saxophone, with the Crows, the Penguins, the El Dorados and the Turbans all scoring major hits. Despite the subsequent explosion in records from doo wop acts in the later '50s, many failed to chart or were one-hit wonders. Exceptions included the Platters, with songs including "The Great Pretender" (1955) and the Coasters with humorous songs like "Yakety Yak" (1958), both of which ranked among the most successful rock and roll acts of the era. Towards the end of the decade there were increasing numbers of white, particularly Italian-American, singers taking up Doo Wop, creating all-white groups like the Mystics and Dion and the Belmonts and racially integrated groups like the Del-Vikings and the Impalas. Doo-wop would be a major influence on vocal surf music, soul and early Merseybeat, including the Beatles.

Many of the earliest white rock and roll hits were covers or partial re-writes of earlier black rhythm and blues or blues songs. Through the late 1940s and early 1950s, R&B music had been gaining a stronger beat and a wilder style, with artists such as Fats Domino and Johnny Otis speeding up the tempos and increasing the backbeat to great popularity on the juke joint circuit. Before the efforts of Freed and others, black music was taboo on many white-owned radio outlets, but artists and producers quickly recognized the potential of rock and roll. Some of Presley's early recordings were covers of black rhythm and blues or blues songs, such as "That's All Right" (a countrified arrangement of a blues number), "Baby Let's Play House", "Lawdy Miss Clawdy" and "Hound Dog". The racial lines, however, are rather more clouded by the fact that some of these R&B songs originally recorded by black artists had been written by white songwriters, such as the team of Jerry Leiber and Mike Stoller. Songwriting credits were often unreliable; many publishers, record executives, and even managers (both white and black) would insert their name as a composer in order to collect royalty checks.

Covers were customary in the music industry at the time; it was made particularly easy by the compulsory license provision of United States copyright law (still in effect). One of the first relevant successful covers was Wynonie Harris's transformation of Roy Brown's 1947 original jump blues hit "Good Rocking Tonight" into a more showy rocker and the Louis Prima rocker "Oh Babe" in 1950, as well as Amos Milburn's cover of what may have been the first white rock and roll record, Hardrock Gunter's "Birmingham Bounce" in 1949. The most notable trend, however, was white pop covers of black R&B numbers. The more familiar sound of these covers may have been more palatable to white audiences, there may have been an element of prejudice, but labels aimed at the white market also had much better distribution networks and were generally much more profitable. Famously, Pat Boone recorded sanitized versions of songs recorded by the likes of Fats Domino, Little Richard, the Flamingos and Ivory Joe Hunter. Later, as those songs became popular, the original artists' recordings received radio play as well.

The cover versions were not necessarily straightforward imitations. For example, Bill Haley's incompletely bowdlerized cover of "Shake, Rattle and Roll" transformed Big Joe Turner's humorous and racy tale of adult love into an energetic teen dance number, while Georgia Gibbs replaced Etta James's tough, sarcastic vocal in "Roll With Me, Henry" (covered as "Dance With Me, Henry") with a perkier vocal more appropriate for an audience unfamiliar with the song to which James's song was an answer, Hank Ballard's "Work With Me, Annie". Elvis' rock and roll version of "Hound Dog", taken mainly from a version recorded by the pop band Freddie Bell and the Bellboys, was very different from the blues shouter that Big Mama Thornton had recorded four years earlier. Other white artists who recorded cover versions of rhythm & blues songs included Gale Storm [Smiley Lewis' "I Hear You Knockin'"], the Diamonds [The Gladiolas' "Little Darlin'" and Frankie Lymon & the Teenagers' "Why Do Fools Fall in Love?"], the Crew Cuts [the Chords' "Sh-Boom" and Nappy Brown's "Don't Be Angry"], the Fountain Sisters [The Jewels' "Hearts of Stone"] and the Maguire Sisters [The Moonglows' "Sincerely"].

Some commentators have suggested a decline of rock and roll in the late 1950s and early 1960s. By 1959, the deaths of Buddy Holly, The Big Bopper and Ritchie Valens in a plane crash (February 1959), the departure of Elvis for service in the United States Army (March 1958), the retirement of Little Richard to become a preacher (October 1957), the scandal surrounding Jerry Lee Lewis' marriage to his thirteen-year-old cousin (May 1958), the arrest of Chuck Berry (December 1959), and the breaking of the Payola scandal implicating major figures, including Alan Freed, in bribery and corruption in promoting individual acts or songs (November 1959), gave a sense that the initial phase of rock and roll had come to an end.

During the late 1950s and early 1960s, the rawer sounds of Elvis Presley, Gene Vincent, Jerry Lee Lewis and Buddy Holly were commercially superseded by a more polished, commercial style of rock and roll. Marketing frequently emphasized the physical looks of the artist rather than the music, contributing to the successful careers of Ricky Nelson, Tommy Sands, Bobby Vee and the Philadelphia trio of Bobby Rydell, Frankie Avalon, Fabian, and Del Shannon, who all became "teen idols."

Some music historians have also pointed to important and innovative developments that built on rock and roll in this period, including multitrack recording, developed by Les Paul, the electronic treatment of sound by such innovators as Joe Meek, and the "Wall of Sound" productions of Phil Spector, continued desegregation of the charts, the rise of surf music, garage rock and the Twist dance craze. Surf rock in particular, noted for the use of reverb-drenched guitars, became one of the most popular forms of American rock of the 1960s.

In the 1950s, Britain was well placed to receive American rock and roll music and culture. It shared a common language, had been exposed to American culture through the stationing of troops in the country, and shared many social developments, including the emergence of distinct youth sub-cultures, which in Britain included the Teddy Boys and the rockers. Trad Jazz became popular, and many of its musicians were influenced by related American styles, including boogie woogie and the blues. The skiffle craze, led by Lonnie Donegan, utilised amateurish versions of American folk songs and encouraged many of the subsequent generation of rock and roll, folk, R&B and beat musicians to start performing. At the same time British audiences were beginning to encounter American rock and roll, initially through films including "Blackboard Jungle" (1955) and "Rock Around the Clock" (1956). Both movies contained the Bill Haley & His Comets hit "Rock Around the Clock", which first entered the British charts in early 1955 – four months before it reached the US pop charts – topped the British charts later that year and again in 1956, and helped identify rock and roll with teenage delinquency. American rock and roll acts such as Elvis Presley, Little Richard, Buddy Holly, Chuck Berry and Carl Perkins thereafter became major forces in the British charts.

The initial response of the British music industry was to attempt to produce copies of American records, recorded with session musicians and often fronted by teen idols. More grassroots British rock and rollers soon began to appear, including Wee Willie Harris and Tommy Steele. During this period American Rock and Roll remained dominant; however, in 1958 Britain produced its first "authentic" rock and roll song and star, when Cliff Richard reached number 2 in the charts with "Move It". At the same time, TV shows such as "Six-Five Special" and "Oh Boy!" promoted the careers of British rock and rollers like Marty Wilde and Adam Faith. Cliff Richard and his backing band, the Shadows, were the most successful home grown rock and roll based acts of the era. Other leading acts included Billy Fury, Joe Brown, and Johnny Kidd & the Pirates, whose 1960 hit song "Shakin' All Over" became a rock and roll standard.

As interest in rock and roll was beginning to subside in America in the late 1950s and early 1960s, it was taken up by groups in major British urban centres like Liverpool, Manchester, Birmingham, and London. About the same time, a British blues scene developed, initially led by purist blues followers such as Alexis Korner and Cyril Davies who were directly inspired by American musicians such as Robert Johnson, Muddy Waters and Howlin' Wolf. Many groups moved towards the beat music of rock and roll and rhythm and blues from skiffle, like the Quarrymen who became the Beatles, producing a form of rock and roll revivalism that carried them and many other groups to national success from about 1963 and to international success from 1964, known in America as the British Invasion. Groups that followed the Beatles included the beat-influenced Freddie and the Dreamers, Wayne Fontana and the Mindbenders, Herman's Hermits and the Dave Clark Five. Early British rhythm and blues groups with more blues influences include the Animals, the Rolling Stones, and the Yardbirds.

Rock and roll influenced lifestyles, fashion, attitudes, and language. In addition, rock and roll may have contributed to the civil rights movement because both African-American and white American teens enjoyed the music.

Many early rock and roll songs dealt with issues of cars, school, dating, and clothing. The lyrics of rock and roll songs described events and conflicts that most listeners could relate to through personal experience. Topics such as sex that had generally been considered taboo began to appear in rock and roll lyrics. This new music tried to break boundaries and express emotions that people were actually feeling but had not talked about. An awakening began to take place in American youth culture.

In the crossover of African-American "race music" to a growing white youth audience, the popularization of rock and roll involved both black performers reaching a white audience and white musicians performing African-American music. Rock and roll appeared at a time when racial tensions in the United States were entering a new phase, with the beginnings of the civil rights movement for desegregation, leading to the U.S. Supreme Court ruling that abolished the policy of "separate but equal" in 1954, but leaving a policy which would be extremely difficult to enforce in parts of the United States. The coming together of white youth audiences and black music in rock and roll inevitably provoked strong white racist reactions within the US, with many whites condemning its breaking down of barriers based on color. Many observers saw rock and roll as heralding the way for desegregation, in creating a new form of music that encouraged racial cooperation and shared experience. Many authors have argued that early rock and roll was instrumental in the way both white and black teenagers identified themselves.

Several rock historians have claimed that rock and roll was one of the first music genres to define an age group. It gave teenagers a sense of belonging, even when they were alone. Rock and roll is often identified with the emergence of teen culture among the first baby boomer generation, who had greater relative affluence and leisure time and adopted rock and roll as part of a distinct subculture. This involved not just music, absorbed via radio, record buying, jukeboxes and TV programs like "American Bandstand", but also extended to film, clothes, hair, cars and motorbikes, and distinctive language. The youth culture exemplified by rock and roll was a recurring source of concern for older generations, who worried about juvenile delinquency and social rebellion, particularly because to a large extent rock and roll culture was shared by different racial and social groups.

In America, that concern was conveyed even in youth cultural artifacts such as comic books. In "There's No Romance in Rock and Roll" from "True Life Romance" (1956), a defiant teen dates a rock and roll-loving boy but drops him for one who likes traditional adult music—to her parents' relief. In Britain, where postwar prosperity was more limited, rock and roll culture became attached to the pre-existing Teddy Boy movement, largely working class in origin, and eventually to the rockers. Rock and roll has been seen as reorienting popular music toward a youth market, as in Dion and the Belmonts' "A Teenager in Love" (1960).

From its early 1950s beginnings through the early 1960s, rock and roll spawned new dance crazes including the twist. Teenagers found the syncopated backbeat rhythm especially suited to reviving Big Band-era jitterbug dancing. Sock hops, school and church gym dances, and home basement dance parties became the rage, and American teens watched Dick Clark's "American Bandstand" to keep up on the latest dance and fashion styles. From the mid-1960s on, as "rock and roll" was rebranded as "rock," later dance genres followed, leading to funk, disco, house, techno, and hip hop.



</doc>
