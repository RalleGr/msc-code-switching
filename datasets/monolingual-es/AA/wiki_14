<doc id="2207" url="https://es.wikipedia.org/wiki?curid=2207" title="Partido Comunista de España">
Partido Comunista de España

El Partido Comunista de España (PCE) es un partido político español de ideología marxista-leninista formado el 14 de noviembre de 1921 a raíz de una escisión del Partido Socialista Obrero Español disconforme con la línea política socialdemócrata y cuya intención inicial era sumarse a la Internacional Comunista convocada por Lenin.

Fundado en sus orígenes por la unión del Partido Comunista Español y el Partido Comunista Obrero Español, se le reconoce especialmente su lucha durante el franquismo, desde la clandestinidad y la ilegalidad, por el restablecimiento de un marco parlamentario y democrático en España. Fue legalizado el 9 de abril de 1977, a partir de la aprobación de la Ley para la Reforma Política impulsada por el Gobierno del entonces presidente Suárez. Desde aquel entonces, el PCE será uno de los más importantes artífices de la Transición.

Desde 1986 forma parte de Izquierda Unida, siendo uno de los colectivos que impulsaron el proyecto de Refundación de la Izquierda. Fija en sus estatutos como objetivo «participar democráticamente en la transformación revolucionaria de la sociedad y de sus estructuras políticas, en la superación del sistema capitalista y la construcción del socialismo en el Estado Español, como contribución al tránsito hacia el socialismo en el plano mundial, con la perspectiva de la plena realización del ideal emancipador del comunismo». Ha sido descrito, ya dentro de Izquierda Unida, como un partido de izquierda o de izquierda radical y se define a sí mismo como un partido revolucionario, internacionalista y solidario, republicano, feminista y laico.

El 15 de abril de 1920, en la Casa del Pueblo de Madrid, fue fundado el Partido Comunista Español por la Federación de Juventudes Socialistas, que ya durante la Primera Guerra Mundial había apoyado a los internacionalistas encabezados por Lenin, y que acordó en su V Congreso, celebrado en diciembre de 1919, adherirse a la Internacional Comunista. Entre sus fundadores estaba Dolores Ibárruri, y lo conformaron desde el principio trabajadores jóvenes, estudiantes, obreros, intelectuales y campesinos. El órgano de expresión del Partido Comunista Español pasó a ser "El Comunista", en el que apareció el manifiesto de fundación del partido, en el cual se hacía hincapié en que era necesario apartarse del reformismo y defender la revolución como única vía para la consecución del socialismo.

El 13 de abril de 1921, después del III Congreso Extraordinario del PSOE, en el que se abrió la brecha entre partidarios de adherirse a la III Internacional y los que no, Antonio García Quejido, fundador del PSOE y de la UGT, y uno de los líderes más prestigiosos del movimiento obrero, declaró que los vocales de la Ejecutiva partidarios de la III Internacional se separaban del PSOE para constituir el Partido Comunista Obrero Español. Entre ellos estaba Daniel Anguiano, que estuvo en la Unión Soviética para observar la marcha de este Estado; su informe posterior, consideró conveniente la integración del PSOE en ésta, por valorar como un avance la III Internacional. El Comité Ejecutivo lo formaban Antonio García Quejido, Anguiano, Virginia González Polo, Evaristo Gil, Manuel Núñez de Arenas y Facundo Perezagua.

Siguiendo las indicaciones de la Internacional Comunista, el Partido Comunista Español y el Partido Comunista Obrero Español celebran en Madrid, del 7 al 14 de noviembre de 1921, una conferencia de fusión dando lugar al Partido Comunista de España.

El 15 de marzo de 1922, el Partido Comunista de España celebraba su I Congreso en Madrid. El primer secretario general fue Antonio García Quejido, que planteó la necesidad de lograr la unidad de la clase obrera en torno a la vanguardia constituida por el nuevo partido, con el objetivo de alcanzar el socialismo.

El II Congreso, celebrado el 8 de julio de 1923, eligió a César Rodríguez González —que había sido cofundador del PCOE en 1921— como secretario general del PCE. Ya en ese momento el PCE temía la involución y llamaba a la unidad de los trabajadores. El 13 de septiembre, el general Miguel Primo de Rivera, en connivencia con el rey Alfonso XIII daba un golpe de Estado y establecía la dictadura. Los locales del PCE son clausurados y las detenciones de militantes comunistas se suceden, incluido el secretario general.

Con un PCE ilegal, el III Congreso se celebra en París en agosto de 1925, que elige a José Bullejos como nuevo secretario general, cayendo el PCE en aquellos años en el sectarismo. En 1927 se adhiere buena parte de la CNT de Sevilla y en 1928 cae desmantelada toda la dirección. Tras la caída de la dictadura en enero de 1930 y la llegada de la "dictablanda", el gobierno del general Berenguer restablece algunas libertades y legaliza a algunos partidos políticos, entre los que no se encuentra el PCE. El 23 de agosto de ese año aparece el primer número semanal del órgano del partido, "Mundo Obrero", que en diciembre del año siguiente se transformó en diario.

El 14 de abril de 1931 se proclamaba la II República, que el PCE consideraba un "engaño para la clase trabajadora". El PCE proclamó: ""¡Abajo la república burguesa! ¡Vivan los soviets!""

Tras la proclamación de la República el PCE volvió a la luz en una situación muy precaria, después de estar prácticamente en la clandestinidad o casi en ella desde su fundación, pasaba a ser legal. Es elegido el primer alcalde comunista durante la Segunda República, Luis Cicuéndez, natural de La Villa de Don Fadrique, en la provincia de Toledo, de donde fue nombrado alcalde tras la segunda vuelta de las elecciones municipales de 1931. El PCE pasó de tener un millar de militantes a principios de 1931 y escasa influencia social e institucional, a 8800 a finales de ese año.
El 17 de marzo de 1932 se celebró el IV Congreso del PCE en Sevilla, que elige a José Díaz Ramos como secretario general, con el objetivo de construir un gran partido comunista de masas y a finales de ese año ya llega a 15 000 afiliados, abriéndose a alianzas con otras fuerzas políticas como el PSOE. El 3 de diciembre de 1933, es elegido el primer diputado comunista de la historia de España, Cayetano Bolívar Escribano, que salió de la cárcel para ocupar su escaño por la provincia de Málaga. También la Federación vasco-navarro del PCE, se constituyó en la Federación Comunista de Euskadi bajo la "Plataforma Revolucionaria del Partido Comunista para la Liberación Nacional y Social de Euskadi".

Posteriormente, en la llamada Revolución de 1934 contra la política del gobierno radical-cedista, el PCE desempeñó un papel menor que el del PSOE. Sin embargo tuvo que volver a la clandestinidad, teniendo ya 20 000 militantes. En 1935 la Federación Comunista de Euskadi, se constituye en partido nacional y de clase, Partido Comunista de Euskadi-Euskadiko Partidu Komunista (PCE-EPK). En ese momento el PCE se adhiere a la política de crear un Frente Popular que agrupe a todas las fuerzas de izquierda. Tras la victoria electoral del Frente Popular el 16 de febrero de 1936, el prestigio del Partido Comunista creció rápidamente: en cinco meses pasa de 30 000 a 100 000 afiliados.

Desde 1933 el PCE organizó una milicia paramilitar, las llamadas Milicias Antifascistas Obreras y Campesinas (MAOC), que de hecho fueron la única milicia de partido que antes de la Guerra civil habían recibido un auténtico entrenamiento militar. Las MAOC contaron con la instrucción de oficiales del Ejército como Francisco Galán o de militantes comunistas como Enrique Líster y Juan Modesto que habían recibido formación militar en la Unión Soviética durante los años 1930.

La expansión del PCE tuvo en los momentos previos a la Guerra Civil y en los inmediatamente posteriores dos grandes hitos:



Desde que estalló la Guerra Nacional Revolucionaria (denominación que le dio el partido a la Guerra Civil), la estrategia del PCE fue siempre el buscar la unidad contra la reacción golpista, formando un Frente Popular que aglutinara a todas las fuerzas leales a la República, incluyendo a la pequeña burguesía y a determinados sectores de la media burguesía. 

Así, prestó su apoyo para organizar la lucha contra el fascismo desde el primer gobierno constituido durante la guerra, el presidido por José Giral, de Izquierda Republicana (IR).

En Madrid, la mayoría de los militantes del PCE colaboraron en el cierre de los caminos de acceso a la ciudad. Al mismo tiempo, el partido realizó un denodado esfuerzo para incorporar a la lucha, encuadrando en los primeros batallones de milicias a millares de combatientes antifascistas. En Barcelona, tras derrotar a las guarniciones sublevadas, se constituía el Partido Socialista Unificado de Cataluña (PSUC).

En esa lucha, que se desarrollaba de un extremo a otro del país, tomaron parte desde el primer momento los dirigentes comunistas José Díaz, Dolores Ibárruri, Vicente Uribe, Pedro Checa, Evaristo Gil y Antonio Mije, además de los dirigentes de la Juventudes Socialistas Unificadas como Santiago Carrillo, Trifón Medrano, Fernando Claudín, José Cazorla, Federico Melchor, Ignacio Gallego, Andrés Martín y Lina Odena. Los dos últimos cayeron en los primeros combates.

El PCE también fue responsable de la creación de las Milicias Antifascistas Obreras y Campesinas (MAOC) y de la unidad sindical entre la Unión General de Trabajadores y la CGTU. Durante este período, el número de afiliados al PCE siguió en ascenso, así, a finales de junio de 1937 la militancia fue estimada en 301 000 afiliados, a los que habría que sumar a los 22 000 afiliados a la sección vasca y a los 60 000 de la sección catalana del partido. 

El gobierno republicano solo recibió ayuda de la URSS y México, escasa en comparación con el inmenso apoyo militar y armamentístico de la Alemania nazi y la Italia fascista a los sublevados.

El 29 de julio de 1936, a los diez días de haber empezado la guerra, Dolores Ibárruri hace un llamamiento internacional por radio para defender la República. Comenzaron a formarse las Brigadas Internacionales de voluntarios, representantes de más de medio centenar de países, integradas por comunistas, socialistas, gente de otros partidos, obreros, campesinos, intelectuales y antifascistas en general. 
El 4 de septiembre de 1936, el socialista Francisco Largo Caballero exigió la colaboración de los comunistas para aceptar las responsabilidades gubernamentales, ante lo cual el PCE accedió a incorporarse a la administración gubernamental y conformar un gobierno del Frente Popular. Se nombró a Vicente Uribe como ministro de Agricultura y a Jesús Hernández Tomás como , representantes del PCE en el gobierno.
En una asamblea pública celebrada en Madrid, el PCE expuso los puntos esenciales de su programa político que eran, en resumen, ganar la guerra, resolver los problemas de la revolución democrática y robustecer la unidad de todas las fuerzas populares, con lo cual las medidas revolucionarias quedarían aplazadas. 

Desde el partido pensaron que si se hubiese intentado establecer el comunismo, el Frente Popular se habría roto automáticamente y la continuación de la resistencia a la agresión militar fascista hubiera sido imposible, por lo que el PCE permaneció fiel a sus compromisos y fue el más abnegado defensor de la República democrática.

El Partido Comunista empezó a crear un Ejército Popular prácticamente apenas iniciada la guerra con las formaciones que constituyeron el Quinto Regimiento de Milicias Populares, que llegó a contar con 70 000 combatientes antifascistas y que echó los cimientos de la nueva organización militar. El 5.º Regimiento dotó de cuadros de mando al naciente Ejército del Pueblo.

Los postulados esenciales de esa política, fueron los siguientes: la utilización de los mandos que iban surgiendo del pueblo en los puestos a los que eran elevados por los propios combatientes; el desarrollo de un amplio trabajo de preparación y educación militar de nuevos cuadros, surgidos también del pueblo; la utilización simultánea en el nuevo Ejército de todos los antiguos militares fieles a la República; el nombramiento de comisarios políticos en todas las unidades de las fuerzas armadas...

Entre los combatientes populares figuraron muchos miembros del PCE como Santiago Aguado, Guillermo Ascanio, José Bobadilla Candón, Manuel Cristóbal Errandonea, Valentín Fernández, Eduardo García, Enrique García Vitorero, Enrique Líster, «Manolín» Álvarez, Pedro Mateo Merino, Rafael Menchaca, Juan Modesto, Pando, Vicente Pertegaz, Polanco, Puig, José Recalde, Joaquín Rodríguez López, Francisco Romero Marín, Alberto Sánchez, José Sánchez, Eugenio R. Sierra, Ramón Soliva, Etelvino Vega, Agustín Vilella, Matías Yagüe y otros. Al mismo tiempo, militares del antiguo Ejército ingresaron en el PCE para acabar convirtiéndose en cuadros y dirigentes del mismo, como Luis Barceló Jover, Bueno, Francisco Ciutat, Antonio Cordón, José María Galán, Rodrigo Gil, Ignacio Hidalgo de Cisneros, Manuel Márquez, Matz o Pedro Prado, entre otros. Este empleo de los oficiales del antiguo Ejército en puestos de mando y responsabilidad de las Fuerzas Armadas se consideró que se ajustaba a los principios del marxismo-leninismo.

Pero la política del Frente Popular le llevó a conflictos con los anarquistas de la CNT-FAI y los comunistas antiestalinistas del Partido Obrero de Unificación Marxista (POUM), los cuales no estaban de acuerdo con las alianzas con la pequeña burguesía ni en posponer la revolución a la guerra. Por otro lado, como artífice de la militarización de las milicias en el Ejército Popular Republicano, el PCE también fue acusado de deshumanizar el proceso revolucionario.

Durante la guerra civil, el partido alcanzó los 300 000 militantes, mientras que la JSU los 500 000 militantes.

Tras la victoria de Franco, los demócratas en general y los comunistas en particular pasan a vivir momentos duros. El régimen de Franco, de rasgos anticomunistas, demonizó al PCE, encarcelando, torturando y asesinando a sus miembros, sometiendo a algunos de ellos a juicios sumarísimos que carecían de garantías mínimas para que los encausados pudieran ser juzgados con equidad. El gobierno franquista aplicó la ley retroactivamente, calificando de insurgentes a los que se mantuvieron fieles a la legalidad constitucional. En esas durísimas condiciones, el PCE se tuvo que reorganizar en la clandestinidad (País Vasco, Galicia, Andalucía, Extremadura, Valencia, Navarra y Cataluña mantuvieron organización), en el exilio (México, Cuba, Chile, Uruguay, Francia y el norte de África, además de la Unión Soviética) y en las cárceles (en las que había dirigentes como Domingo Girón o Guillermo Ascanio).

En el interior de España, el partido se iba reorganizando poco a poco y en 1943, "Mundo Obrero", "Verdad", "Unidad", "El Obrero" y "Nuestra Bandera" se publicaban en diversas zonas de España. Fue durante mucho tiempo la principal, cuando no la única, fuerza organizada contra la dictadura de Franco. Al poco tiempo de iniciarse la Segunda Guerra Mundial, el secretario general José Díaz muere en Tiflis y es sustituido por Dolores Ibárruri, "Pasionaria".

En aquellos años, el PCE se unió al movimiento de resistencia contra el nazismo, integrándose la Agrupación de Guerrilleros Españoles en las Fuerzas Francesas del Interior (FFI), acabando muchos de sus militantes en los campos de concentración como Mauthausen. Al mismo tiempo impulsó la lucha guerrillera en España, el llamado maquis, ya que tenían la esperanza de que con la derrota de Hitler hubiese una intervención en España, cosa que finalmente no se produjo.

El partido decidió abandonar la vía guerrillera en 1948, tras la muerte de muchos militantes comunistas y una dura represión sobre la población civil de las zonas en las que actuaba esta guerrilla, que hizo perder apoyos en una población rural que pasaba además graves dificultades económicas.Algunos focos guerrilleros se mantuvieron hasta 1952.

El periodo de apogeo guerrillero fue el comprendido entre 1945 y 1947. En 1948, Stalin deja claro que había que desmantelar la guerrilla comunista en España. Pese al cambio de postura del PCE en junio de 1956, propuesto por Santiago Carrillo mediante el eslogan y objetivo político de la «reconciliación nacional», puede decirse que el final del maquis lo marcan las muertes de Ramón Vila en 1963 y de José Castro en 1965.

En 1947 se producen en el metal de Madrid y en las empresas textiles de Cataluña los primeros movimientos reivindicativos, durísimamente reprimidos por el franquismo. Siguiendo la táctica leninista, el partido opta por combinar la lucha clandestina con el aprovechamiento de los resquicios legales que el sistema permite: los comunistas participan en los sindicatos verticales y en todas las organizaciones de masas que existen (hermandades de acción católica, gremios...). En las elecciones de enlaces sindicales de 1950 ya son elegidos numerosos obreros comunistas y otros concienciados. Este movimiento dará origen a las Comisiones Obreras.

Pero la situación vuelve a empeorar para los comunistas, pues a la represión del régimen se une la guerra fría, durante la cual el gobierno dictatorial pasa a ser un importante auxilio en la política de los Estados Unidos frente a la Unión Soviética, que marca en gran medida la línea del PCE. En 1950 el ministro del Interior del gabinete socialista francés, Jules Moch, decreta la ilegalización del PCE en Francia y la detención de sus cuadros políticos. Mientras tanto, Radio España Independiente emite desde Europa del Este para España la visión del PCE.

El 12 de marzo de 1951 el PSUC llama a la huelga general en Cataluña. En Euskadi, Navarra y Madrid se producen otras huelgas. A los obreros se les van uniendo estudiantes e intelectuales, muchos de ellos ya pertenecientes a una nueva generación crecida durante el franquismo.

En septiembre de 1954 se celebra el V Congreso del PCE, en el que se establece la nueva táctica, en dos etapas. En la primera se propugnaba la creación de un frente amplio que liquidara la dictadura y formara un gobierno provisional. Este gobierno debería restablecer las libertades democráticas, amnistiar a presos y exiliados políticos, y adoptar medidas urgentes para mejorar las condiciones de vida de la población. Tras ello se deberían convocar elecciones y desarrollar la democracia. En junio de 1956 el PCE diseña su política de "Reconciliación Nacional" a la que también se suma el PSUC. En ese momento, los estudiantes cuentan con una fuerza creciente, el SEU se ha liquidado y empiezan a surgir movimientos democráticos burgueses en el interior de España, algunos de cuyos miembros procedían de sectores disconformes de la derecha, e incluso de la propia Falange. Cada vez más, la lucha del PCE representa con mayor claridad la lucha por las libertades democráticas. Para conseguir aliar a todas las fuerzas democráticas se entiende que hay que cancelar responsabilidades de la guerra civil y la posguerra:

Pero el régimen franquista había recibido un importantísimo espaldarazo en 1955: apoyado y avalado por los Estados Unidos entra a formar parte de la ONU. La lucha clandestina debe continuar, pues el régimen se siente fortalecido y acentúa la represión. En 1957 el PCE impulsa el movimiento pro-Amnistía y participa en los "boicots" que se producen en Madrid y Barcelona, así como en las luchas obreras que se producen en Sevilla, Alcoy, Valladolid y muy especialmente en las de los mineros de Asturias de marzo de 1958. Impulsa la Huelga Nacional Pacífica del 18 de junio de 1959.

En enero de 1960 se reunió el VI Congreso del PCE en Praga, que eligió a Santiago Carrillo como secretario general, que desplaza a Dolores Ibárruri a la presidencia del Partido. En plena crisis económica, con el salario real de los trabajadores cayendo en más de un 40% debido a la suspensión de las horas extra, primas y pluses, el Partido capitaliza la contestación a Franco creando las Comisiones Obreras (CCOO). El sindicato CCOO no fue creado por el PCE, sino por trabajadores que se configuraron en Comisiones de trabajadores en centros de trabajo y llamando a la "Huelga Nacional Pacífica". Los despidos eran cada vez más frecuentes y el paro avanzaba, dificultades que alcanzarían también a la pequeña burguesía y a los comerciantes, afectados también por la caída en el poder adquisitivo de la mayor parte de la población.

Entre 1961 y 1964 fueron detenidos 1500 comunistas. En 1962 es detenido y torturado Julián Grimau, recientemente elegido miembro del Comité Central del PCE, por la Brigada Político-Social del régimen. En 1963 fue condenado a muerte, hecho que desató una reacción internacional de protesta y presión sin precedentes, con manifestaciones multitudinarias en varias capitales europeas y latinoamericanas. El general Franco atribuyó esta presión a una "conspiración masónico-izquierdista con la clase política", mientras Manuel Fraga, ministro de Información y Turismo, inició una intensa campaña dirigida a la prensa internacional atribuyendo a Grimau los mayores crímenes. Fue fusilado la madrugada del 20 de abril de 1963.

A nivel interno, siguiendo la estela del Partido Comunista Italiano, el PCE va buscando una vía autónoma a la del PCUS y la Unión Soviética, esbozando lo que se denominará eurocomunismo. En este camino, la actitud a veces excesivamente personalista del nuevo secretario general irá apartando a cuantos discrepan de la línea de la dirección: en 1964 Fernando Claudín y Jorge Semprún son expulsados de modo sumarísimo del partido. Ese mismo año se produce la escisión de un sector en contra de la política de "reconciliación nacional" y el eurocomunismo, que pasa a constituir el Partido Comunista de España (marxista-leninista).

A mediados de 1965 se celebra el VII Congreso del PCE, en el que se defiende el avance al socialismo por una vía pacífica, parlamentaria y adecuada a los rasgos específicos de España, apostando por el no alineamiento en el plano internacional. Tras la condena de la invasión soviética de Checoslovaquia en 1968 se escindirá el Partido Comunista de España (VIII-IX Congresos).

Tras el VIII Congreso (1972), en el que se traza la línea definitiva que seguirá el partido, Enrique Líster funda el Partido Comunista Obrero Español, que se escinde del PCE. La nueva política de Carrillo se concreta en la constitución en París con otros partidos y personalidades independientes de la Junta Democrática de España el 30 de julio de 1974, organismo clave en la transición española y más adelante en Coordinación Democrática (la llamada ""platajunta""), unión entre la Junta y la Plataforma de Convergencia auspiciada por el PSOE.

Entre 1967 y 1976 el Tribunal Supremo condena a multitud de opositores, el 36% del PCE y el 25% de CCOO. En 1973 tiene lugar el Proceso 1001, en el que se juzga y condena a la dirección de CCOO por su vinculación con el PCE.

En diciembre de 1975 el rey Juan Carlos trasladó a Santiago Carrillo el mensaje de que pretendía democratizar el régimen, pidiendo paciencia y el fin de los ataques a la Monarquía. El PCE, que hasta entonces seguía impulsando una «ruptura democrática», en el comité ejecutivo de enero de 1976 dejaba ya a un lado las críticas al rey y baja el nivel de ofensiva y movilización. Esta posición se ratifica en el Comité Central celebrado en Roma el 28 de julio, donde se acuerda acabar con la estructura en células para impulsar agrupaciones territoriales, y donde el PCE rompe la Junta Democrática para acercarse a la Plataforma de Convergencia.
El 24 de enero de 1977 tuvo lugar lo que se conoce como la matanza de Atocha de 1977: un comando de ultraderecha entró en un despacho de abogados en derecho laboral de CCOO y el PCE en el centro de Madrid, asesinando a balazos a cinco de ellos y dejando a otros cuatro heridos. Al entierro asistieron más de cien mil personas y se convirtió en una multitudinaria manifestación, que transcurrió sin incidentes. Le siguieron importantes huelgas y muestras de solidaridad en todo el país, además de un paro general de trabajadores el día después del atentado.

El 11 de febrero de 1977 el PCE presenta la documentación para ser incluido en el Registro de Asociaciones y el 9 de abril de ese mismo año el PCE es legalizado, presentándose a las elecciones con Santiago Carrillo como candidato. La militancia del interior, muy próxima a la realidad española y representante de las posturas rupturistas con la dictadura, se veían como los custodios del Partido hasta que los "históricos" exiliados pudieran retornar, pero cuando así ocurrió, los exiliados retornados estaban tremendamente apartados de la realidad española debido a su larga ausencia.

En noviembre de 1977 Carrillo fue a Washington a dar unas conferencias y mantener un encuentro con el Departamento de Estado de los EE. UU.. En su intervención en la Universidad de Yale, Carrillo anunció que el PCE renunciaría al marxismo-leninismo en el siguiente Congreso. Ese mismo año un sector denominado Oposición de Izquierda (OPI), que había surgido tras el VIII Congreso, abandona el PCE y adopta el nombre de Partido Comunista de los Trabajadores (PCT).

En 1978, en el IX Congreso del PCE, primer Congreso que se celebraba en España desde 1932, salió reelegido Santiago Carrillo como secretario general, mientras que Dolores Ibárruri saldría elegida como presidenta del partido. Las divisiones que ya existían con anterioridad continuaron profundizándose cuando el PCE dejó de considerarse marxista-leninista para pasar a definirse como "marxista revolucionario", por 965 frente a 248 votos. Francisco Frutos, que luego sería secretario general del PCE, fue quien defendió las tesis leninistas en Madrid, que en el PSUC catalán (donde él militaba) eran mayoritarias. Además, se confirmó el cambio de una estructura celular a una territorial (rompiendo así con la estructura organizativa tradicional de los partidos comunistas) y se consolidó el distanciamiento de la URSS y el resto de países del Pacto de Varsovia, con el fin de otorgar un cierto aperturismo a la organización y aumentar el apoyo electoral, hecho que dio lugar a muchas convulsiones internas.

En 1979, el PCE pasó de 200 000 a 170 000 militantes.

En julio de 1981 se celebra el X Congreso del PCE, que en ese momento cuenta con 84.500 militantes más los 80.000 del PSUC en Cataluña. Se erigen dos tendencias contrarias a la dirección de Santiago Carrillo: los "leninistas" (también denominados "prosoviéticos") como Ignacio Gallego o Francisco García Salve defendían una postura más ortodoxa y cercana a la Unión Soviética; los "renovadores" defendían una postura más moderada y aperturista. Con el fin de armonizar las relaciones internas del PCE, Julio Anguita propuso una ponencia para dar lugar a diversas tendencias internas, propuesta tumbada por los "carrillistas". A su vez, Santiago Carrillo propuso la creación de la figura del vicesecretario general, dentro de la ejecutiva, siendo nombrado Nicolás Sartorius para tal cargo. 

Mientras tanto en la federación vasca del PCE, el secretario general del Partido Comunista de Euskadi (PCE-EPK) Roberto Lertxundi anunció la integración del mismo en Euskadiko Ezkerra (EE), de modo que Santiago Carrillo desautorizó tal decisión y expulsó del mismo a todo el PC vasco. Esto dio inicio a un debate interno acerca de si realmente se respetaba la independencia de las federaciones del PCE, debate que acentuaba las divisiones internas al expulsar a militantes que sí apoyaban la decisión de los vascos, como un grupo de concejales del Ayuntamiento de Madrid, entre los que se encontraba Cristina Almeida. Durante 1981 y 1983 las expulsiones continuaron por centenares de sectores diversos, como Francisco García Salve, Ramón Tamames o Carlos Alonso Zaldívar. También se expulsó del PSUC al sector "prosoviético" que formó años más tarde el Partit dels Comunistes de Catalunya (PCC) y el Partido Comunista de los Pueblos de España (PCPE).

Para octubre de 1982 se habían convocado elecciones generales en España, y Santiago Carrillo denunció que la convocatoria se celebraría con las normas electorales de 1977 y 1979, que consideró de "dudosa constitucionalidad", ya que beneficiaban a Alianza Popular (precursor del actual Partido Popular) y al PSOE. Además, aseguró que el entonces presidente Leopoldo Calvo-Sotelo había precipitado la convocatoria para provocar una bipolarización política en ambos partidos, con el objetivo de reducir al máximo la representación política del PCE, que era entonces el único partido que defendía un frente "democrático, marxista y revolucionario". Y así fue, pues en aquellos comicios continuó la caída electoral al concentrarse el voto de la izquierda en el PSOE, que ganó con mayoría absoluta, por lo que Carrillo dimitió como secretario general, siendo sustituido por Gerardo Iglesias.

Después de la dimisión de Carrillo y tras una ligera recuperación electoral en las elecciones municipales, el 20 de diciembre de 1983 se celebró el XI Congreso del PCE, en el que participaron 85 000 militantes, y que terminó totalmente dividido entre los "carrillistas", el sector "prosoviético" o leninista, liderado por Ignacio Gallego, y los "renovadores" que lograron la mayoría eligiendo a Gerardo Iglesias como secretario general, a Dolores Ibárruri como presidenta, así como a Enrique Curiel como vicesecretario general, acordando la construcción de un proyecto unitario de la izquierda, que buscase la convergencia social y política con otras fuerzas. En dicho Congreso, Santiago Carrillo y sus seguidores se declararon contrarios a la dirección de Gerardo Iglesias, proponiendo una secretaría general compartida entre los dos, sugerencia que sería rechazada por el secretario general. De este modo, Santiago Carrillo acusaría a los renovadores de no tener un programa propio, de ser dependientes del PSOE y de intentar destruir el PCE, en un contexto en el que los carrillistas se mostraban en clara minoría.

Por otra parte, entre el 13 y el 15 de enero de 1984, tuvo lugar en Madrid el Congreso celebrado por el sector prosoviético, en el que participaron también el Partido de los Comunistas de Cataluña, el Partido Comunista de España Unificado (PCEU), el Movimiento para la Recuperación del PCE y Células Comunistas. Este proceso, en el que participaron 10 000 militantes, dio lugar al Partido Comunista de los Pueblos de España (PCPE) y a su organización juvenil, los Colectivos de Jóvenes Comunistas (CJC). El nuevo partido fue también reconocido por el Partido Comunista de la Unión Soviética y los partidos comunistas de los países del Pacto de Varsovia.

En octubre de 1984, el gobierno de Felipe González, pactó una reforma electoral con Alianza Popular, y el entonces vicesecretario general del PCE, Enrique Curiel, criticó que dicha reforma mantenía la Ley de D'Hondt que perjudicaba especialmente a los comunistas y que favorecía al bipartidismo, ya que mantenía las grandes diferencias en el coste de votos que los partidos necesitan para obtener un lugar en el Parlamento.

En 1985, destacados miembros como Julio Anguita o Marcelino Camacho, incluso antiguos miembros de la dirección de Carrillo como Nicolás Sartorius, Simón Sánchez Montero o la misma Dolores Ibárruri y Gerardo Iglesias decidieron expulsar a Santiago Carrillo el 15 de abril, los "carrillistas" son destituidos de los órganos de dirección y estos se escinden para fundar el Partido de los Trabajadores de España-Unidad Comunista (PTE-UC). También en 1985 la dirección federal del PCE disolvió el comité regional de la UJCE en Madrid y expulsó a 6 de sus miembros, por su posición eurocomunista y contraria a la política de convergencia del partido. Ese año la afiliación del PCE había bajado a 67.000.

En 1986, y tal y como se había acordado en el último congreso, el PCE es el principal impulsor de la coalición Izquierda Unida (IU), constituida junto a otras fuerzas políticas como Izquierda Republicana, y en el que también participaría el PCPE.

De cara a las elecciones generales de 1986, Santiago Carrillo declaró que apoyar a IU suponía "enterrar el comunismo" y beneficiar "a la derecha" en detrimento del PSOE. Finalmente, IU obtuvo 7 escaños, correspondiendo 4 al PCE y 1 al PCPE, mientras el PTE-UC de Santiago Carrillo no obtuvo representación.

En febrero de 1988, durante el XII Congreso del PCE, Gerardo Iglesias renunció a todos sus cargos y Julio Anguita, conocido por haber sido alcalde de Córdoba, pasó a ser el secretario general del PCE. Bajo su dirección, el Partido recuperó buena parte de la ilusión y preceptos ideológicos anticapitalistas, hecho que hizo considerar al entonces secretario general del PCPE, Ignacio Gallego, el regreso al PCE. Así, en noviembre de ese año fue expulsado, y en enero de 1989, tras un Congreso Extraordinario, se reincorporaría al PCE junto a 8.000 militantes, 48 miembros del Comité Central y la mayoría de cargos públicos del PCPE.

Con todo, en el PCPE se quedó el sector encabezado por su nuevo secretario general, Juan Ramos Camarero, que abandonó la coalición, y en las siguientes elecciones generales de octubre de 1989, IU duplicaría su número de votos y obtendría 17 diputados, correspondiendo 13 al PCE y 1 al PSUC. Un mes después, el 12 de noviembre, murió Dolores Ibárruri, "Pasionaria".

Pero a pesar de los buenos resultados que estaba teniendo IU, en el XIII Congreso del PCE, celebrado en diciembre de 1991 y al que se llegó con 70.000 militantes, hubo un sector, encabezado por Francisco Palero y Juan Berga, que consideraba que el comunismo como ideología se había agotado, y que los partidos comunistas ya no eran instrumentos válidos, por lo que defendió la disolución del partido dentro de IU. Prevaleció por un 74,6% la postura defendida por el secretario general, Julio Anguita, de continuar con una IU construida como movimiento político y social, que contara con partidos en su seno a modo de corrientes, debido a la pluralidad ideológica de los componentes de la coalición. El PCE cedería parte de su soberanía para aumentar la actividad de IU, y para hacer sus decisiones vinculantes.

En las siguientes elecciones, IU consiguió casi 400 000 votos más, obteniendo un nuevo escaño. La estrategia política del PCE, en la que se insistiría más desde el XIV Congreso, en diciembre de 1995, era la de evitar cualquier tipo de pacto con el PSOE, en tanto este no abandonase sus políticas neoliberales y conservadoras. Para el PCE, los dirigentes del PSOE formaban parte de la élite dominante y les acusaban de haberse blindado en buenos sueldos y prebendas, y de ser parte de la aristocracia. Al mismo tiempo, aumentaban las tensiones con el PSUC, por la intención de su secretario general de disolverlo en Iniciativa per Catalunya, y con CCOO, a cuyo secretario general, Antonio Gutiérrez (luego diputado del PSOE entre 2004 y 2011), le acusaban de tener un pacto con el PSOE. El sindicato corría el riesgo de convertirse en "apéndice del Estado", alertó Julio Anguita, quien también anunció que no volvería a presentarse a la reelección como secretario general del PCE.

Fue durante esta etapa que IU logró sus mejores resultados, ya que en las elecciones generales de marzo de 1996, IU alcanzaría su techo histórico sobrepasando la barrera del 10% de votos y obteniendo 21 diputados, 12 del PCE y 2 del PSUC. En septiembre de 1997, en la Fiesta del PCE, Anguita anunciaba que iban a defender una España republicana y federal, y en la Fiesta del año siguiente, defendió el derecho de autodeterminación de los pueblos, y aclaraba que su partido sólo había aceptado la Monarquía de forma temporal, durante la Transición, para llegar al consenso, siempre y cuando se desarrollase la constitución. Sin embargo, el 17 de agosto de 1998, Anguita sufre un segundo infarto, que acelera su retirada de la primera línea política.

En diciembre de 1998, se celebra el XV Congreso del PCE, en el que Julio Anguita dejó la secretaría general del PCE, que pasó a ocupar Francisco Frutos. En su discurso, Anguita pidió a los militantes comunistas que reivindicasen los principios del anticapitalismo y la lucha por una sociedad igualitaria. Equiparó en lo político al PSOE y al PP y llamó a rebato a la militancia para recuperar la lucha en la calle.

Aquel congreso se celebró en medio de grandes tensiones con Comisiones Obreras, ya que los comunistas defendieron un sindicato "de clase y democrático", a la vez que acusaron a la dirección de "represora", y defendieron que los militantes luchasen democráticamente dentro del sindicato ""para cambiar políticas y a algunos dirigentes"", algo que el sindicato consideró una injerencia.

Con motivo de las elecciones generales de marzo de 2000, Frutos es nombrado como candidato de Izquierda Unida sin haber sido elegido coordinador general, y habiendo firmado un acuerdo pre-electoral de investidura con el PSOE. El resultado fue un fracaso para el PCE, por lo que a finales de ese mismo año, Gaspar Llamazares fue elegido coordinador federal en la VI Asamblea Federal de IU por un estrecho margen frente a Francisco Frutos, aunando los votos de diversas corrientes de IU críticas con la dirección saliente.

El nuevo coordinador de IU se apoyó en el PSOE, con una política de enfrentamiento con el PCE, que traería fuertes problemas internos y presiones que debilitaron al Partido.

Pese a las diferencias, en el XVI Congreso del PCE, celebrado en marzo de 2002, Frutos pactó con el entonces coordinador general de IU una lista, enfrentada a la de Ángeles Maestro (Corriente Roja), con el fin de lograr una mayoría en CCOO distinta a la de José María Fidalgo.

En diciembre de 2004 se celebró con carácter extraordinario la VIII Asamblea Federal de Izquierda Unida, tras la crisis abierta por las sucesivas derrotas electorales sufridas por la coalición y por la división en su dirección. El PCE presentó entonces como candidato a Enrique Santiago, pero volvió a salir elegido Gaspar Llamazares como coordinador federal, en un proceso muy polémico que algunos sectores calificaron de irregular debido a que la candidatura de Santiago (respaldada también por las Juventudes Comunistas) y la presentada por Sebastián Martín Recio (respaldada por los sectores más a la izquierda de IU), sumaron más de un 50% frente al 49% de la lista oficial de Llamazares.

La reelección de Gaspar Llamazares fue muy polémica porque se debió a un sistema aprobado en una reforma de los estatutos previa a la elección y durante la misma Asamblea, consistente en que votaran no solamente la mitad del Consejo Político Federal elegido en la propia Asamblea, órgano competente para elegir al coordinador federal, sino también los coordinadores de las federaciones. Así, el Consejo Político Federal al completo, ratificó la elección de Gaspar Llamazares por un 54%, algo que también generó polémica, ya que sus oponentes entendían que era necesario un mínimo del 60% para presentarse a una segunda reelección a coordinador, como disponen los estatutos. La Comisión de Garantías resolvió la cuestión a favor de Llamazares al entender que no se había agotado el segundo mandato, ya que la VIII Asamblea se había anticipado.

En el XVII Congreso del PCE, celebrado en junio de 2005 con 27 000 militantes, Francisco Frutos es reelegido secretario general, y como presidente se elige a Felipe Alcaraz, quien reuniría varias responsabilidades hasta ese momento del secretario general. Dolores Ibárruri, "La Pasionaria", es declarada Presidenta de Honor a perpetuidad. Se apuesta por la reconstrucción y el relanzamiento del partido, al que se considera que ha empezado a funcionar como corriente dentro de IU.

No se acepta para ello la salida de IU, como proponía Corriente Roja, y que llevaría a la organización a escindirse del PCE. En cambio, se apuesta por recuperar la soberanía dentro de IU, entendiendo que esta debería volver a funcionar como un movimiento político y social que permitiese que diferentes partidos confluyesen en criterios programáticos, superando así también al XIII Congreso del PCE.

También se debatió un documento presentado por Julio Anguita, en el que se reflexionaba acerca del Movimiento Comunista Internacional, y se llamaba a la refundación del partido. El documento señalaba el impacto negativo que trajo la caída de la Unión Soviética y el acriticismo y sumisión de los sindicatos y la Izquierda al orden capitalista establecido. Se acordó la creación de un equipo de trabajo que se reuniese con otras organizaciones comunistas y de izquierda, coordinase y recogiese propuestas y se debatiesen en una Conferencia, que redactaría y aprobaría un nuevo Manifiesto-Programa de cara al siguiente Congreso del Partido.

En 2006 se abrió una grave crisis interna en Asturias, donde el PCE anuló el último congreso del Partido Comunista de Asturias (PCA) por irregularidades en los censos. Llamazares, que ya había perdido el apoyo de las tres principales federaciones (Andalucía, Madrid y Comunidad Valenciana), perdería así también el control del último feudo que le quedaba en la organización comunista.

A finales de 2007, el PCE impulsa en el Consejo Político Federal de IU la celebración de elecciones primarias para designar al candidato a la Presidencia del Gobierno en las elecciones generales de 2008. Frente a Gaspar Llamazares, se presenta la secretaria general del Partit Comunista del País Valencià (PCPV), Marga Sanz. El referéndum se celebra en noviembre por un sistema de correo certificado, que el PCE criticó duramente. Finalmente, con una participación de en torno al 38%, Llamazares obtiene 13.626 votos (62,5%) y Sanz 8.169 (37,5%).

El 22 de abril de 2008, Julio Anguita remitió al Comité Federal del Partido Comunista de España (PCE) un documento en el que defendía también la necesidad de una 'refundación' de IU, que sólo sería posible desde el compromiso de comenzar desde cero. En su carta, defendió la democracia radical, la lucha por la III República y el federalismo, tanto para el modelo organizativo de la coalición como para el modelo de estado defendido. A su juicio, el debate debería abrirse en la siguiente asamblea federal de IU. Así, los días 28 y 29 de junio de 2008, en una Conferencia Política del PCE, se vinculó la reconstrucción del partido con la refundación de Izquierda Unida. Para ello, se acordó impulsar una serie de cambios en la próxima Asamblea Federal de IU.

En la IX Asamblea Federal de IU, celebrada los días 15 y 16 de noviembre de 2008, el PCE presentó su candidatura al Consejo Político Federal, con Cayo Lara como candidato de consenso de la lista «Otra IU es Posible». Su propuesta, titulada «Por una Izquierda Unida anticapitalista, republicana, federal y alternativa, organizada como movimiento político y social», obtuvo un 43% de los votos, pero la falta de acuerdo con otras corrientes hizo que la asamblea concluyese sin la elección de un nuevo Coordinador Federal de IU.
Finalmente, el Consejo Político Federal, convocado el 14 de diciembre, eligió a Cayo Lara como coordinador federal de la coalición con un 55.08% de los votos. Como nuevo coordinador general, Cayo Lara integró desde un principio en la nueva dirección a personas de todos los sectores y corrientes de la formación, e instó a abandonar las luchas internas y a preocuparse por lo que realmente pasa en el país. En su primera intervención pública, mencionó a los pobres, a los parados y a los hipotecados, e hizo un llamamiento a la huelga general, necesaria, a su juicio.

El 13 de abril de 2009, el partido reclama en un manifiesto con motivo del 78º aniversario de la II República que los trabajadores "no paguen" la actual situación de crisis y que se afronte la coyuntura económica a través de "la ruptura del pacto constitucional" y la apertura de un "proceso constituyente por la III República".

También en ese manifiesto se declara que el capitalismo ha fracasado y que no debe hacerse esfuerzos por "refundarlo", ya que no lo consideran una solución para los problemas de la humanidad y hay que hacer cambios revolucionarios. Declaran que hay que emprender, como ya han hecho otros países, el camino del socialismo del siglo XXI.

Con el propósito de renovar fuerzas ante los retos que el capitalismo posindustrial parece incapaz de resolver, y con el de afrontar el naciente proceso de Refundación de la Izquierda aprobado en la IX Asamblea Federal de IU en 2008, el partido celebra en noviembre de 2009 su XVIII Congreso, al que llega con 20.000 militantes. En el cónclave se aprueba su orientación hacia IU, con un 82% de votos favorables, el mantenimiento de Comisiones Obreras como referente sindical (69%) y se elige a José Luis Centella como secretario general, con un 85% de los votos, en sustitución de Francisco Frutos.

A fecha de abril de 2017, el PCE contaba con aproximadamente 10.500 afiliados, si bien 2.000 de estos se habían afiliado desde las elecciones generales de 2015..

A finales de 2017 se celebró la segunda fase del XX Congreso del PCE, en el que se aprobaron cambios significativos, siendo el de más calado la vuelta al marxismo-leninismo, que se abandonó en el IX Congreso de 1978. Esto provocó un cambio organizativo de envergadura, ya que se vuelve a adoptar el centralismo democrático, dejando de ser una organización federal, que correspondía esta a la división por comunidades autónomas de España, sustituyéndose por comités dependientes de un Comité Central. También se aprueba incluir una estrella roja de cinco puntas en la parte superior del logo del partido, en referencia al internacionalismo proletario.

En enero de 2020 el PCE vuelve a tener representantes en Consejo de Ministros, con el nombramiento de Alberto Garzón como ministro de consumo y a Yolanda Díaz como ministra de Trabajo y Economía Social.

 y 

Existen los siguientes Comités de Organización, los cuales son "Comités Regionales" o "Comités Nacionales". Hasta el XX Congreso, el PCE desde 1978 se organizaba en federaciones, pero al volver adoptar el marxismo-leninismo, también significó volver a un modelo centralista de organización.
Fuera de España, además, el PCE cuenta con federaciones activas en:


Aunque el Partido Socialista Unificado de Cataluña era un partido con personalidad propia y claramente diferenciado del PCE, durante muchos años estuvo representado en los órganos de dirección de este.








</doc>
<doc id="2208" url="https://es.wikipedia.org/wiki?curid=2208" title="Plata">
Plata

La plata es un elemento químico de número atómico 47 situado en el grupo 11 de la tabla periódica de los elementos. Su símbolo es Ag (procede del latín: "argentum", "blanco" o "brillante"). Es un metal de transición de color plateado (blanco metálico), brillante, blando, dúctil y maleable.

En la naturaleza se encuentra como parte de distintos minerales (generalmente en forma de sulfuro) o como plata libre. Es poco común en la naturaleza, de la que representa una parte en 1 mil de corteza terrestre. La mayor parte de su producción se obtiene como subproducto del tratamiento de las minas de cobre, zinc, plomo y oro.

Su nombre es una evolución de la palabra latina "*platus" (cf. chato), que significaba originalmente "plano" y posteriormente "lámina metálica". En las lenguas romances de la península ibérica el término específico referencia al metal: en catalán, aragonés y castellano "plata" y en gallego y portugués "prata".

El símbolo de la plata, Ag, proviene del latín "argentum" y el griego "ἄργυρος", nombres del metal en esos idiomas, derivados de una raíz indoeuropea que significa 'brillante'. Del vocablo latino derivan los nombres de la plata en la mayoría de las lenguas neolatinas, como el francés "argent", el italiano "argento" y el rumano "argint".

En español también existe el adjetivo "argentino", de uso exclusivamente literario, de donde surgió el nombre de Argentina.

La plata es un metal muy dúctil y maleable, algo más duro que el oro, y presenta un brillo blanco metálico susceptible al pulimento. Se mantiene en agua y aire, si bien su superficie se empaña en presencia de ozono, sulfuro de hidrógeno o aire con azufre. 

Posee la más alta conductividad eléctrica y conductividad térmica de todos los metales, pero su mayor precio ha impedido que se utilice de forma masiva en aplicaciones eléctricas. La plata pura también presenta el color más blanco y el mayor índice de reflexión.

Aproximadamente el 70% de la producción mundial de plata se utiliza con fines industriales, y el 30%, con fines monetarios; buena parte de este metal se emplea en orfebrería, pero sus usos más importantes se dan en la industria fotográfica y 
química.

Algunos usos de la plata se describen a continuación:


La plata es uno de los siete metales conocidos desde la antigüedad. Se menciona en el libro del Génesis; y los montones de escoria hallados en Asia Menor e islas del mar Egeo, indican que el metal comenzó a separarse del plomo al menos cuatro milenios antes de nuestra era.

No resulta complicado imaginar el efecto que hubo de producir en aquellos pobladores (que habían tallado y pulido la piedra, que encontraron y utilizaron el cobre y luego el estaño, llegando incluso a alear ambos por medio del fuego para obtener bronce) el descubrimiento de un metal raro y poco frecuente, de color blanco, brillo imperecedero e insensible al fuego que otros metales derretía. Tal asombro significó la atribución al metal de singulares propiedades, de las que los demás metales carecían, salvo el oro claro está; pues ambos no eran sino regalos de la naturaleza, formados uno por el influjo de la Luna, y el otro por el del Sol. Los demás, "viles metales", estaban sujetos a los cambios y transformaciones, que por los rudimentarios medios entonces disponibles podrían producirse; lejos, muy lejos, de la perfección de la plata y el oro. No es de extrañar que por ello surgiera la idea de la transmutación de los metales en un vano intento de perfeccionar aquellos viles metales y dando lugar a la aparición de las primeras doctrinas de la Alquimia. Particularmente adecuado parecía para tal propósito el mercurio, en el que se observaba el aspecto y color de la plata, hasta tal punto que se le dio el nombre de "hydrargyrum" (plata líquida) de donde proviene su símbolo químico (Hg).

La plata, como el resto de los metales, sirvió para la elaboración de armas de guerra y luego se empleó en la manufactura de utensilios y ornamentos, de donde se extendió al comercio al acuñarse las primeras monedas de plata y llegando a constituir la base del sistema monetario de muchos países. En 1516 Juan Díaz de Solís descubrió en Sudamérica el "mar Dulce" que posteriormente Sebastián Caboto denominó Río de la Plata, creyendo que allí abundaba el precioso metal, y de donde tomará el nombre la Argentina. Años más tarde, el hallazgo de grandes reservas de plata en el Nuevo Mundo en Zacatecas y Taxco en México; Potosí, en Bolivia; así como Paramillos de Uspallata, en Argentina: y su importación por Europa, provocó un largo periodo de inflación, que lejos de limitarse a España, se difundió por toda Europa; el fenómeno fue estudiado por Earl Jefferson Hamilton, que en 1934 publicó el libro "El tesoro americano y la revolución de los precios en España, 1501-1650".

El símbolo químico empleado por Dalton para la plata fue un círculo con la letra «S» en su centro

La plata se encuentra nativa, combinada con azufre (argentita, AgS), arsénico (proustita, AgAsS), antimonio (pirargirita, AgSbS) o cloro (plata córnea, AgCl), formando un numeroso grupo de . El metal se obtiene principalmente de minas de cobre, cobre-níquel, oro, plomo y plomo-cinc de México, Canadá, el Perú y los EE. UU..

La metalurgia a partir de sus minerales se realiza fundamentalmente por la cianuración:

La producción mundial de plata durante 2011 alcanzó un total de 23,800 toneladas métricas. Los principales países productores de plata son México y Perú que representan por sí solos 1/3 de la producción mundial de plata.

De acuerdo a información entregada en el informe anual del United States Geological Survey (USGS), las estimaciones señalan que las reservas conocidas de plata en 2011 a nivel mundial alcanzarían 530,000 toneladas métricas de plata fina. Y según las estimaciones de USGS, en Perú existirían del orden de 120,000 toneladas métricas económicamente explotables, equivalentes al 23% del total de reservas mundiales del mineral; seguido de Polonia con 85,000 toneladas métricas económicamente explotables, equivalentes al 16% del total de reservas mundiales del mineral.

La plata se alea fácilmente con casi todos los metales, aunque con el níquel lo hace con dificultad. Con el hierro y el cobalto no puede alearse. Incluso a temperatura ordinaria, la plata forma amalgamas con mercurio.

El metal de aleación por excelencia es el cobre, que endurece la plata si se añade a esta hasta contenidos del 5% (lo que se conoce como "plata de ley"), aunque se han utilizado platas con contenidos mayores de cobre. Las adiciones de cobre no alteran el color de la plata incluso aunque se llegue hasta contenidos del 50%, aunque en este caso el color se conserva en una capa superficial que al desgastarse mostrará una aleación de color rojizo, tanto más acusado cuanta mayor sea la cantidad de cobre. También se han usado aleaciones con cadmio en joyería, ya que este elemento le confiere a la aleación una ductilidad y maleabilidad adecuadas para el trabajo del metal.

Entre los compuestos de plata de importancia industrial destacan:

La plata natural se compone de dos isótopos estables Ag-107 y Ag-109, siendo el primero ligeramente más abundante (51,839%) que el segundo. Se han caracterizado veintiocho radioisótopos de los cuales los más estables son la Ag-105, Ag-111 y Ag-112, con periodos de semidesintegración de 41,29 días, 7,45 días y 3,13 horas respectivamente. Los demás isótopos tienen periodos de semidesintegración más cortos que una hora, y la mayoría menores que tres minutos. Se han identificado numerosos estados metaestables entre los cuales los más estables son Agm-108 (418 años), Agm-110 (249,79 días) y Agm-107 (8,28 días).

Los isótopos de la plata tienen pesos atómicos que varían entre las 93,943 uma de la Ag-94 y las 123,929 uma de la Ag-124. El modo de desintegración principal de los isótopos más ligeros que el estable más abundante es la captura electrónica resultando isótopos de paladio, mientras que los isótopos más pesados que el estable más abundante se desintegran sobre todo mediante emisión beta dando lugar a isótopos de cadmio.

El isótopo Pd-107 se desintegra mediante emisión beta produciendo Ag-107 y con un periodo de semidesintegración de 6,5 millones de años. Los meteoritos férreos son los únicos objetos conocidos con una razón Pd/Ag suficientemente alta para producir variaciones medibles en la abundancia natural del isótopo Ag-107. La Ag-107 radiogenética se descubrió en el meteorito de Santa Clara (California) en 1978.

La plata no es tóxica pero la mayoría de sus sales son venenosas y pueden ser carcinógenas. Los compuestos que contienen plata pueden ser absorbidos por el sistema circulatorio y depositarse en diversos tejidos provocando argiria, afección consistente en la coloración grisácea de piel y mucosas, que no es dañina.

Desde Hipócrates se conoce el efecto germicida de la plata y se han comercializado, y comercializan hoy día, diversos remedios para gran variedad de dolencias.

En junio de 2013 se ha publicado un estudio que ha demostrado en ratones su utilidad terapéutica como antibiótico. "Nuestro trabajo es el primero que descifra los mecanismos por los que la plata mata a los microorganismos. La plata es como un caballo de Troya que abre las puertas celulares a los antibióticos", dice el Dr. José Rubén Morones-Ramírez, investigador de la Universidad Autónoma de Nuevo León (México) y coautor del estudio. El Dr. Morones-Ramírez se encuentra actualmente en el Instituto Médico Howard Hughes, de la Universidad de Boston, en Estados Unidos.

Es reconocido que las sales solubles de plata, especialmente el nitrato de plata (AgNO3), son letales en concentraciones de hasta 2 gramos. Los compuestos de plata pueden ser absorbidos lentamente por los tejidos corporales, con la consecuente pigmentación azulada o negruzca de la piel, efecto conocido como argiria.

Adicionalmente:

La sobreexposición crónica a un componente o varios componentes de la plata tiene los siguientes efectos en los animales de laboratorio:

La sobreexposición crónica a un componente o varios componentes de la plata se supone que tiene los siguientes efectos en los humanos; efectos que aún deben ser corroborados mediante ulteriores investigaciones:




</doc>
<doc id="2217" url="https://es.wikipedia.org/wiki?curid=2217" title="Potamogetonaceae">
Potamogetonaceae

Las potamogetonáceas (nombre científico Potamogetonaceae) son una familia de hierbas perennes, de hábitats de agua dulce, con hojas laminares alargadas y flores sin perianto pero con falso perianto tetrámero formado por apéndices de los estambres, 4 estambres y 4 carpelos libres. Inflorescencias espiciformes.

Esta es una familia casi cosmopolita, integrada por alrededor de 50 especies. Fue reconocida por sistemas de clasificación modernos como el sistema de clasificación APG III del 2009 y el Angiosperm Phylogeny Website (2001 en adelante), si bien su circunscripción varía con los sistemas de clasificación, hoy está aceptado que para que Potamogetonaceae se mantenga monofilética, "Ruppia" debe ser escindida de esta familia conformando su propia familia Ruppiaceae, mientras que se mantienen en la familia los géneros que en otras clasificaciones fueron ubicados en Zannichelliaceae.

Hierbas acuáticas rizomatosas. Tallos con haces vasculares reducidos muchas veces en un anillo, con cavidades de aire. Taninos muchas veces presentes. Sin pelos.

Hojas alternas y espirales, u opuestas, lámina a veces bien desarrollada, simple, entera, con venación paralela o con sólo una vena media, envainadoras en la base, la vaina abierta, y más o menos separada de la lámina de forma que parece ser una estípula, las hojas a veces heteromórficas, con formas sumergidas y flotantes, 2 a muchas pequeñas escamas presentes en el nodo, dentro de las vainas de las hojas.

Inflorescencias indeterminadas, terminales y axilares, como espigas y elevadas por sobre o acostadas en la superficie del agua.

Flores bisexuales, radiales, no asociadas con brácteas a la madurez.

Sin tépalos.

Estambres 4, con apéndices bien desarrollados en la base de la antera que forma lo que parece un perianto más o menos carnoso.

Polen sin aperturas, globoso a elipsoide.

Carpelos usualmente 4, separados, ovario súpero, con placentación más o menos basal a apical, 1 estigma, truncado a capitado. 1 óvulo, más o menos anátropo a ortótropo.

Sin nectarios.

El fruto es un agregado de aquenios o drupas. Sin endosperma.

Cosmopolita. Hierbas de lagos, ríos, y otros hábitats acuáticos.

En "Potamogeton" y "Stuckenia" las flores usualmente crecen por sobre la superficie del agua y son polinizadas por el viento.

Los frutos son dispersados por animales o agua.

"Ruppia", un género de agua alcalina, salobre, u ocasionalmente salada, es muchas veces ubicado aquí, pero su inclusión hace a la familia bifilética (Les "et al." 1997a). Para que esta familia sea monofilética, es mejor instaurar "Ruppia" en su familia Ruppiaceae, que se caracteriza por flores con dos estambres que tienen apéndices diminutos, polen ligeramente elongado, y carpelos con tallos largos.

La familia fue reconocida por el APG III (2009), el Linear APG III (2009) le asignó el número de familia 319. La familia ya había sido reconocida por el APG II (2003).

6 géneros, 100 especies. El género más representado es "Potamogeton" , con más de 90 especies. 

Los géneros, conjuntamente con su publicación válida, distribución y número de especies se listan a continuación:



Sinónimos según el APWeb: Hydrogetonaceae Link, Zannichelliaceae Chevalier, "nom. cons."

Si bien la familia posee poco interés económico directo, muchas especies proveen alimento para la vida salvaje.




</doc>
<doc id="2218" url="https://es.wikipedia.org/wiki?curid=2218" title="Posidoniaceae">
Posidoniaceae

Las posidoniáceas (nombre científico Posidoniaceae) son una familia de plantas marinas extendidas por el mar Mediterráneo y en la costa sur de Australia. La familia es reconocida por sistemas de clasificación modernos como el sistema de clasificación APG III del 2009 y el APWeb (2001 en adelante). Pertenece al orden Alismatales y posee un único género, Posidonia, con 9 especies. Junto con algunas familias emparentadas forman lo que se conoce como "pastos marinos". Esta familia se reconoce por su rizoma monopodial y sus inflorescencias racimosas ramificadas.

Hierbas perennes, rizomatosas, acuáticas, sumergidas, marinas. Poseen aspecto de pastos (son "pastos marinos").

Hojas con forma de cinta, dísticas. 

Flores hermafroditas, aclamídeas, con 3 estambres y con gineceo unicarpelar. 

Inflorescencias cimosas espiciformes, provistas de brácteas foliosas. 

El fruto es una baya.

Distribuidas en el mar Mediterráneo y en la costa sur de Australia. La única especie endémica del Mediterráneo es "Posidonia oceanica".

En mayo de 2006 se halló en las cercanías de Formentera, Islas Baleares, un ejemplar de esa especie de 8 kilómetros de largo, que es la planta más grande conocida, y el mayor ser vivo sin considerar los hongos; su ritmo de crecimiento es de 2 centímetros por año y su edad se ha estimado en 100 000 años.

La familia fue reconocida por el APG III (2009), el Linear APG III (2009) le asignó el número de familia 40. La familia ya había sido reconocida por el APG II (2003).

La familia posee un único género, Posidonia, con 9 especies.

El nombre de "posidonia" deriva de Poseidón "(Ποσειδώνιος)", el dios griego de los océanos y de las aguas.

Esta especie esta en peligro por culpa de la Caulerpa Taxifolia ya que es venenosa y la pudre.




</doc>
<doc id="2219" url="https://es.wikipedia.org/wiki?curid=2219" title="Poaceae">
Poaceae

Las poáceas (Poaceae) o gramíneas son una familia de plantas herbáceas, o muy raramente leñosas, perteneciente al orden Poales de las monocotiledóneas. Con más de 820 géneros y cerca de 12 100 especies descritas, las gramíneas son la cuarta familia con mayor riqueza de especies luego de las compuestas, las orquídeas y las leguminosas; pero, definitivamente, es la primera en importancia económica mundial. De hecho, la mayor parte de la dieta de los seres humanos proviene de las gramíneas, tanto en forma directa —granos de cereales y sus derivados, como harinas y aceites— o indirecta —carne, leche y huevos que provienen del ganado y las aves de corral que se alimentan de pastos o granos—. Es una familia cosmopolita, que ha conquistado la mayoría de los nichos ecológicos del planeta, desde las zonas desérticas hasta los ecosistemas de agua salada, y desde las zonas deprimidas y anegadizas hasta los sistemas montañosos más altos. Esta incomparable capacidad de adaptación está sustentada en una enorme diversidad morfológica, fisiológica y reproductiva y en varias asociaciones mutualísticas con otros organismos, que convierten a las gramíneas en una fascinante familia, no solo por su importancia económica, sino también por su relevancia biológica.

Entre las especies más destacadas están la caña de azúcar, el trigo, el arroz, el maíz, el sorgo, la cebada, la avena, el centeno o el bambú.

En general son hierbas, si bien pueden ser leñosas —como los bambúes tropicales—, cespitosas, rizomatosas o estoloníferas. Por la duración de su ciclo de vida pueden ser anuales, bienal o perennes. Las gramíneas anuales, como es lógico suponer, se reproducen una sola vez durante su ciclo vital —el caso del trigo o de la avena, por ejemplo—. Las especies perennes, en cambio, pueden reproducirse varias veces —en general anualmente— o una sola vez. En el primer caso se denominan "iteróparas" —la mayoría de las especies de pastos, por ejemplo— y, en el segundo caso, "semélparas" —como es el caso de las diferentes especies bambúes—.

Tienen tallos cilíndricos a elípticos en su sección transversal, articulados, llamados ordinariamente cañas, en general con nudos macizos y entrenudos huecos (pero pueden ser totalmente macizos como en el caso del maíz y algunos bambúes). Los nudos son algo más gruesos que los entrenudos y en ellos nacen las hojas y las yemas. Los entrenudos son a veces algo achatados en la zona donde se desarrollan las ramificaciones. Un poco más arriba del nudo existe un meristema intercalar en forma de anillo que determina el alargamiento del tallo. En algunos géneros existen de dos a seis nudos muy próximos entre sí (los cuales se denominan "nudos compuestos"), cada uno de los cuales lleva su correspondiente hoja. En "Cynodon dactylon", por ejemplo, los nudos están en grupos de a dos por lo que las hojas parecen opuestas. En general los entrenudos basales son más cortos que los superiores; cuando hay varios nudos basales muy próximos, las hojas parecen "arrosetadas", es decir, se disponen de forma tal que simulan formar una roseta basal de hojas. Los principales tipos de tallos en las gramíneas son los siguientes:

Poseen hojas de disposición alterna, dísticas, compuestas típicamente de vaina, lígula y limbo. La vaina rodea apretadamente al tallo, sus márgenes se superponen pero no se fusionan entre sí (solo ocasionalmente pueden ser encontradas formando un tubo). La lígula es un pequeño apéndice membranoso, o raramente un grupo de pelos (tricomas), situado en la zona de unión del limbo con la vaina, en la parte adaxial. El limbo (o lámina) es simple, usualmente lineal, con nerviación paralela. Puede ser aplanado o a veces enrollado en un tubo, puede ser continuo con la vaina o poseer pecíolo. Además de esta descripción, es necesario abordar la variabilidad que se puede hallar para cada uno de estos órganos:

Los macollos o macollas son la unidad estructural de la mayoría de las especies de gramíneas. Se forman a partir de las yemas axilares o secundarias del meristema basal del eje principal. Cada uno de estos brotes secundarios o macollos inician su aparición cuando las plantas presentan entre dos y tres hojas. Cada uno de ellos, luego de producir sus primeras hojas, genera su propio sistema radicular. La suma o adición de macollos es lo que conforma la estructura y la forma de una planta de gramínea. Cuando las gramíneas se hallan en estado vegetativo producen continuamente nuevos macollos y hojas. Cada macollo, a su vez, comenzará en su momento a producir nuevos macollos.

La inflorescencia elemental de las gramíneas se llama espiguilla y consiste en una pequeña espiga formada por una o más flores sentadas o sésiles sobre un raquis articulado, a menudo brevísimo, llamado raquilla y protegido por brácteas estériles denominadas glumas, que se insertan sobre la raquilla, una más abajo que la otra.

Las flores pueden ser unisexuales o hermafroditas y presentan un perianto rudimentario de dos o tres piezas llamadas "lodículas" o "glumélulas", que son los órganos que determinan la apertura del antecio o casilla floral al ponerse turgentes durante la floración, permitiendo que se expongan los estigmas plumosos y los estambres. Los antecios están formados por dos "glumelas":

Todos estos elementos son muy variables por lo que es conveniente analizarlos por separado.

Las espiguillas, a su vez, se hallan reunidas o agrupadas en inflorescencias compuestas de tipo racimoso. Las más frecuentes son:

El fruto o "grano" de las gramíneas es una cariópside, fruto seco indehiscente, con una semilla cuya testa está soldada con el pericarpio formando una envoltura muy delgada. Esta envoltura encierra el embrión y el albumen o endosperma. Este fruto es básicamente una variante del aquenio, aunque se puede encontrar cierta variedad de frutos en la familia (ver por ejemplo Werker 1997). En algunos géneros como "Zizianopsis" o "Eleusine," el pericarpio no está soldado con la semilla, de modo que el fruto es un aquenio (o un utrículo según otros autores). En algunas Bambúseas el fruto es una nuez o una baya, mientras en el género "Sporobolus" el pericarpio es mucilaginoso y deja salir a la semilla cuando se embebe en agua. Muchos géneros, como "Aristida, Stipa, Piptochaetium," "Oryza" y casi todas las Paníceas poseen cariópsides que se desprenden de la planta envueltas por el lemma y por la pálea. En las Andropogóneas son las glumas las que persisten encerrando a la cariópside. En "Pennisetum" y "Cenchrus" se desprende toda la espiguilla rodeada de un involucro de cerdas o de espinas. La forma de la cariópside varía mucho según los géneros, pudiendo ser casi circular como en "Briza," oblonga como en "Hordeum," lanceolada como en "Poa" hasta casi linear, como en "Vulpia."
En la parte inferior de la cariópside, en visión dorsal, se aprecia el embrión más o menos elíptico cubierto por el pericarpio transparente. Por el otro lado, correspondiente al surco o sutura carpelar, se distingue, también por transparencia, la "mácula hilar" o "hilo" (o zona de unión de la semilla con el carpelo), que puede ser puntiforme, como en "Poa" y en las Paníceas, ovada, como en" Briza subaristata", o linear, como en "Hordeum, Vulpia" o "Festuca."

El embrión de las gramíneas es estructuralmente muy complejo y consta de la plántula unida a su cotiledón laminar, altamente modificado, llamado "escudete". El cotiledón es delgado, parenquimatoso, llevando en su parte exterior una capa de células epiteliales que durante la germinación segregan enzimas que hidrolizan las sustancias de reserva localizadas en el endosperma. La planta consta de un nudo cotiledonar, donde se inserta el cotiledón, una yémula cubierta de un capuchón o "coleoptilo" y una radícula envuelta por otro capuchón o "coleorriza". En muchos géneros en la parte externa del nudo cotiledonar hay una escama diminuta, el "epiblasto", que para algunos autores constituye un resto de un segundo cotiledón, mientras que otros consideran que se trata de un apéndice de la coleorriza.

El tamaño y el número de cromosomas tienen gran importancia en la sistemática de las gramíneas. Hay dos tipos cromosómicos extremos: el tipo "festucoide" caracterizado por presentar cromosomas grandes y número básico predominantemente x=7 y el tipo "panicoide" con cromosomas pequeños y números básicos predominantes x=9 y x=10. El tipo festucoide se encuentra en casi todas las tribus de la subfamilia de las poóideas, con algunas excepciones. Por ejemplo, la tribu Stipeae de esta subfamilia posee cromosomas pequeños y números básicos x=9, 10, 11, 12, 14, 16 y 17. Las restantes subfamilias de las gramíneas presentan el tipo cromosómico panicoide, con cromosomas pequeños y predominio del número básico x=9 y 10. En las bambusóideas, erartóideas y arundinóideas los cromosomas son pequeños y el número básico es x=12. En la subfamilia Danthonioideae se presentan cromosomas de tamaño intermedio y número básico x=6 y 7. La subfamilia Chloridoideae presenta cromosomas pequeños y varios números básicos, x=7, 8, 9, 10, 11, 12 y 14. Las panicóideas siempre tienen cromosomas pequeños, con números básicos x=9 o x=10, aunque existen especies con otros números básicos, que varían desde x=4 a x=19.

Las gramíneas son morfológicamente distintas de cualquier otra familia de plantas y, además, son muy diversas en cuanto a morfología y hábito de crecimiento. Las diferentes especies de gramíneas —como se ha descrito en la sección previa— difieren en sus tamaños y números cromosómicos. Asimismo, difieren en el tamaño (o contenido de ADN) de sus genomas.

El genoma del arroz, por ejemplo, es más de 11 veces más pequeño que el genoma de la cebada, a pesar de que ambas especies son diploides y aparentan tener la misma complejidad morfológica y fisiológica.

El contenido de genes de las diferentes especies de gramíneas, no obstante, no varia tan ampliamente como el contenido de ADN total. El arroz y la cebada, nuevamente, no difieren más que en dos veces en el número promedio de fragmentos de restricción que hibridan con las mismas sondas.

La mayor parte de las diferencias en el tamaño del genoma entre especies de gramíneas se deben a diferencias en el ADN repetitivo. Los genomas más grandes, como los de cebada o trigo, están compuestos en un 75 % de ADN repetitivo, mientras que los genomas más pequeños, como el del arroz, solo contienen menos del 50 % de ADN altamente repetitivo. Más aún, se ha determinado que buena parte de ese ADN repetitivo está compuesto de retrotransposones insertos entre los genes.

Los estudios de mapeo genómico en muchas especies de gramíneas utilizando las mismas sondas de ADN han demostrado que no solo el contenido de genes está muy conservado, sino también el orden de los genes dentro de los cromosomas.

La extensa conservación en el contenido de genes y en el orden de los mismos entre el maíz y el sorgo no es inesperada ya que ambas especies "sólo" cuentan con 15 a 20 millones de años de evolución independiente. No obstante, similares observaciones para el arroz y el maíz, las cuales divergieron hace 60 a 80 millones de años, indican que todas las especies de la familia provienen de un mismo antepasado común y que todas ellas conservan un mismo repertorio de genes en el mismo orden aproximado.

Los grandes rearreglos genómicos que diferencian entre sí a todas las gramíneas son el resultado de inversiones, translocaciones o duplicaciones cromosómicas que involucran la mayor parte de los brazos cromosómicos.

La mayoría, sino todas, las gramíneas son poliploides. Basados en el supuesto que todos los géneros y familias que presentan un número cromosómico básico x=12 son derivados de ancestros que sufrieron duplicaciones cromosómicas durante su evolución y que las subfamilias de gramíneas más primitivas ("Anomochlooideae", "Pharoideae", y "Puelioideae") tienen un número cromosómico básico x=12, se puede deducir que el ancestro de las gramíneas ya era un poliploide. Se sigue, además, que todas las gramíneas que se clasifican como diploides son, en realidad, "paleopoliploides" (o sea, poliploides antiguos que presentan herencia disómicay cuyos progenitores no pueden ser identificados mediante herramientas citogenéticas o marcadores moleculares).
La familia contiene más del 60 % de especies, distribuidas en todos los clados, que se clasifican como "neopoliploides", o sea que han sufrido un ciclo adicional de duplicación genómica. En estas especies, los genomas duplicados no han divergido mucho del genoma de sus ancestros y su número de cromosomas y comportamiento citológico durante la meiosis son indicativos de la duplicación cromosómica que los ha originado. La mayoría de estos neopoliploides (más del 65 %) han derivado de cruzamientos interespecíficos o intergenéricos por lo que se les clasifica como alopoliploides.

Las hemicelulosas y los polisacáridos de pectinas de la pared celular primaria de los pastos son muy diferentes de los de las demás espermatofitas, tanto en estructura como en las particularidades de la composición de los xiloglucanos. Los polisacáridos son menos ramificados que en todas las demás familias de plantas, si bien esta afirmación está basada en un muestreo todavía escaso de especies. Las poáceas pueden ser cianogenéticas o no. Cuando son cianogenéticas, los compuestos cianogenéticos son derivados de la tirosina. Pueden presentar alcaloides (a veces isoquinolina, pirrolizidina e indol). Raramente puede haber proantocianidinas y cianidinas, en cantidades traza, y sólo en representantes de las subfamilias Panicoideae y Chloridoideae. Los flavonoides se han hallado solo en algunos géneros, "Bouteloua" "Glyceria" y "Melica", cuando están presentes son quercetina, o kaempferol junto con quercetina. El ácido elágico y la arbutina no se han encontrado en ningún miembro de la familia. Raramente se encuentran saponinas y sapogeninas, así como también oxalatos libres (por ejemplo en "Setaria").

Una generalización acerca del modo de reproducción de las gramíneas es que los miembros de esta familia son plantas hermafroditas, que presentan fertilización cruzada (son alógamos) y se polinizan por el viento. Obviamente, una familia con cerca de 10.000 especies cuenta con muchas excepciones a esta regla, las cuales se describen a continuación.

Este tipo de sistema reproductivo, en el cual existen plantas femeninas y plantas masculinas, no es muy frecuente en las gramíneas. Solo 18 géneros son dioicos o presentan especies dioicas, siendo "Poa" el más conocido de ellos. De hecho, las especies dioicas de "Poa" se incluyen en un subgénero separado, "Dioicopoa".

Este sistema reproductivo describe el hecho de que en las poblaciones naturales de una especie coexisten individuos femeninos e individuos hermafroditas. Esta condición es bastante rara en las gramíneas. "Bouteloua chondrosioides" y algunas especies del subgénero "Andinae" de "Poa" son ginodioicas, si bien "Cortaderia" es el ejemplo más conspicuo.

En este sistema los sexos están separados espacialmente pero en el mismo individuo, o sea, cada planta presenta inflorescencias femeninas y masculinas. "Zea, Humbertochloa, Luziola, Ekmanochloa" y "Mniochloa" son ejemplos de géneros con especies monoicas. Mucho más común entre las gramíneas son las especies "andromonoicas", una condición muy común en las Andropogóneas y Paníceas. En las primeras, los dos sexos se presentan en espiguillas diferentes de pares heterógamos. Un par heterógamo de espiguillas consiste usualmente en una espiguilla sesil, con una flor neutra y otra hermafrodita, y una espiguilla pedicelada, con una flor neutra y otra masculina. En las espiguillas bifloras de las Paniceas, en cambio, la flor inferior es usualmente masculina o neutra, y la superior es hermafrodita. Algunos de los géneros que ejemplifican este tipo de sistema son "Alloteropsis,"
"Brachiaria," "Cenchrus," "Echinochloa," "Melinis," "Oplismenus,"
"Panicum," "Setaria," "Whiteochloa," y "Xyochlaena". Algunas especies dentro de estos géneros pueden tener solo flores hermafroditas ya que la flor inferior es siempre neutra, raramente ambas flores son hermafroditas. Aparte de las paníceas y las andropogóneas, "Arundinelleae" es otra tribu con especies andromonoicas. En el resto de la familia, las especies andromonoicas se encuentran muy esporádicamente, como por ejemplo en "Arrhenatherum,"
"Hierochloe" y "Holcus."

La gran mayoría de las especies de gramíneas son hermafroditas, no obstante, frecuentemente son incapaces de producir semillas cuando el polen de una planta poliniza sus propios estigmas. Esto se debe a que una gran parte de las especies de la familia presentan autoincompatibilidad, de tipo gametofítica y debido a la acción de dos genes independientes (llamados "S" y "Z") con varios alelos cada uno. Este sistema de autoincompatibilidad ha sido observado en varios géneros de la familia "(Festuca," "Secale," "Lolium," "Hordeum," "Dactylis," entre muchos otros) y no es perfectamente eficiente. De hecho, de la mayoría de las especies autoincompatibles puede obtenerse una proporción —si bien reducida— de semillas al autofecundar una planta.

La autopolinización y la autofecundación están muy distribuidas entre las gramíneas. En general, es un mecanismo más común entre las especies anuales que entre las perennes y, decididamente, mucho más frecuente entre las especies colonizadoras. Este mecanismo se ha determinado en aproximadamente 45 géneros de gramíneas, entre los cuales se hallan géneros económicamente muy importantes como "Triticum," "Oryza," "Secale," "Avena," "Agropyron" y "Lolium." Una condición de autogamia extrema es la "cleistogamia", en la cual se produce la polinización y la fecundación dentro del antecio sin que se produzca la antesis. Este último sistema está distribuido en más de 70 géneros pertenecientes a 20 tribus de gramíneas.

La apomixis se define como la reproducción asexual a través de semillas. En este sistema reproductivo los embriones se desarrollan por mitosis a partir de una oósfera no reducida sin que tenga lugar la fecundación. En otras palabras, cada embrión producido es genéticamente idéntico a la planta madre. En las gramíneas, la apomixis fue descrita por primera vez en 1933 en una especie de "Poa". Desde aquel momento se ha identificado este mecanismo en cientos de especies de poáceas, particularmente en las Paníceas y en las Andropogóneas. Algunos de los géneros que presentan especies apomícticas son "Apluda," "Capillipedium," "Heteropogon,"
"Themeda," "Sorghum, " "Bothriochloa," "Dichanthium, " "Cenchrus," "Setaria" y "Paspalum".

Las gramíneas son una familia cosmopolita que habita desde los desiertos hasta los hábitats de agua dulce o marinos, y todas las elevaciones salvo las más altas del planeta. En el mundo se han desarrollado extensos biomas nativos dominados por gramíneas donde hay sequías periódicas, topografía plana o inclinada, incendios frecuentes, y en algunas ocasiones donde hay pastoreo y bajo ciertas condiciones particulares de suelo. Las comunidades dominadas por los pastos suman el 24% de la vegetación del planeta, ejemplos son las praderas de Norteamérica, las pampas de Sudamérica, el "veldt" o la sabana en África, y las estepas euroasiáticas. Por fuera de las praderas herbáceas, los bambúes leñosos desempeñan un papel central en la ecología de los bosques de Asia tropical y templada.

Las gramíneas han sido ecológicamente exitosas y se han diversificado extensamente debido a muchas adaptaciones clave. La espiguilla protege a las flores pero, al mismo tiempo, permite la polinización cuando las lodículas abren el antecio. Asimismo, las espiguillas (lemmas con pelos o ganchos) poseen varias adaptaciones para la dispersión del fruto. La versatilidad en los sistemas de apareamiento, incluyendo la autofecundación y la apomixis, permitió a diversas especies de gramíneas ser colonizadoras exitosas de nuevos ambientes. La anatomía de la hoja, que puede ser C o C, permite a estas especies explorar y adaptarse a un amplio rango de hábitats. Los meristemas se hallan ubicados en la base de los entrenudos y en la base de las vainas, protegidos por toda la planta, lo que da como resultado una adaptación al pastoreo y al fuego sin igual entre todas las plantas. El desarrollo de las praderas durante el Mioceno (hace unos 25 a 5 millones de años) puede haber fomentado la evolución de los grandes herbívoros, además de representar una importante fuente de alimento y un estímulo para la evolución del "Homo sapiens".

Las gramíneas además han desarrollado ciertas características fisiológicas que les han permitido conquistar hábitats donde prevalecen condiciones subóptimas para el crecimiento de las plantas. Una de tales características es la capacidad de acumular betaínas de glicina y otros compuestos que se hallan asociados con la adaptación de las plantas al crecimiento en condiciones salinas. Por otro lado, las poóideas almacenan carbohidratos como fructanos, los que se hallan en mucha menor concentración en las restantes especies de la familia. Esta característica está asociada con la adaptación de tales especies a condiciones de estrés hídrico (sequías) y de bajas temperaturas (heladas). Finalmente, otro mecanismo fisiológico singular de las gramíneas es que aparentemente son la única familia de angiospermas que adquiere iones por quelación de iones férricos con sideróforos que son absorbidos por las raíces.

Las micorrizas y las micofilas son dos tipos de mutualismos en los cuales se hallan involucrados plantas superiores y hongos. Las micorrizas son mutualismos entre hongos y las raíces de las plantas. Las micofilas, entretanto, son mutualismos entre hongos endófitos (aquellos que crecen dentro de las plantas) y la parte aérea de las plantas.

La presencia de hongos endófitos puede modificar la supervivencia de las plantas de varios modos ya que se pueden producir tres tipos diferentes de asociaciones entre los simbiontes fúngicos y las plantas, las cuales varían de acuerdo con el grupo taxonómico del hospedante, con las estructuras fúngicas y vegetales involucradas y con las particularidades intrínsecas de la simbiosis. Así, el tipo I tiene como hospedantes a miembros de la familia de las juncáceas y a varias subfamilias de las gramíneas. Estos hongos colonizan la totalidad del hospedante y producen sus estructuras sexuales en estromas que suplantan a los frutos que deberían producir las plantas, por lo que éstas pierden la capacidad de reproducirse sexualmente. La interacción es sumamente agresiva para el hospedante y determina su paulatino decaimiento. El tipo II tiene como hospedantes a las gramíneas de la subfamilia de las poóideas. No todos los individuos de la población colonizada presentan estromas en lugar de cariópsides. En los ejemplares sin síntomas externos el micelio endofítico llega a colonizar las cariópsides sin perjudicar la reproducción sexual de la planta. De esta manera, el carácter patógeno de la interacción es menor que en el caso anterior. Por último, en el tipo III, que también se da en la subfamilia de las poóideas, nunca aparecen estromas en los hospedantes; la colonización del hongo es sistémica, alcanzando a las cariópsides por medio de las cuales se propaga dicha asociación. Este tipo de interacción es considerado una simbiosis mutualista debido a que los endófitos benefician a los hospedantes aumentando su crecimiento, biomasa, tasa fotosintética, tolerancia a las heladas y sequías, resistencia a nematodos e insectos, por lo que incrementan la competitividad de sus hospedantes. Asimismo, debido a que producen alcaloides protegen a las plantas del ataque de un amplio espectro de animales herbívoros, constituyendo así parte de su sistema de defensa. La presencia de endófitos afecta la palatabilidad de los pastos para los herbívoros y también la palatabilidad de las semillas para los pájaros granívoros, los animales que comen material infectado presentan diversos síntomas de intoxicación. El nivel de la infestación por áfidos y la de sus parásitos y parasitoides, y aún el patrón y la tasa de descomposición del pasto muerto, también son afectados por el mutualismo. Asimismo, las larvas de moscas del género "Phorbia "(o "Botanophila") viven en el estroma de los hongos endofitos del género "Epichloë", y los adultos transmiten los espermacios del hongo en una forma análoga a la polinización por insectos de las flores. A su vez, los endófitos se benefician recibiendo el aporte directo de hidratos de carbono producidos por sus hospedantes. De las 232 micofilas conocidas en el mundo, 209 tienen como hospedantes a miembros de todas las subfamilias de gramíneas y representan a los tres tipos de interacción. Las áreas de distribución de estas interacciones abarcan tanto zonas frías y templadas como tropicales. En el Hemisferio Norte son muy frecuentes las interacciones tipo I, II y III, mientras que en el Hemisferio Sur prevalecen las de tipo III.
Los hongos endofitos de la familia Clavicipitaceae están ampliamente distribuidos entre las gramíneas. Un género de esta familia de hongos, "Epichloë", es un endofito restringido a la subfamilia Pooideae, "Neotyphodium" es el estado asexual o imperfecto de "Epichloë". Más del 30% de las especies de poóideas están involucradas en tales asociaciones, y existe transmisión de estos hongos (la subfamilia "Balansiae" de las Clavicipitáceas) tanto en forma vertical como horizontal. Las micofilas son una asociación que parece datar de hace unos 40 millones de años y una de sus consecuencias es la producción de alcaloides como, por ejemplo, la lolina. Los alcaloides producidos, como la mencionada lolina, son activos principalmente en la defensa de las plantas contra insectos. Otras muchas especies endófitas, aparentemente asintomáticas, pueden crecer conjuntamente en las gramíneas, pero se conoce muy poco sobre sus relaciones. Márquez y colaboradores (2007), por ejemplo, informaron que la gramínea "Dicanthelium lanuginosum" solo puede crecer en suelos calentados por acción volcánica cuando el hongo endofítico "Curvularia", con el que está asociada, se halla infectado con un virus. Esto indica que las relaciones entre las gramíneas y los hongos endófitos pueden ser extremadamente complejas y sus efectos insospechados. Existen listados de especies de hongos endófitos asociados con innumerables especies de gramíneas. De hecho, solo en los bambúes hay al menos unas 1.933 especies de hongos descritas.

Los registros fósiles más antiguos indican que esta asociación tiene unos 400 millones de años, lo que indica la compleja coevolución entre las plantas y sus hongos asociados, que se manifiesta en la amplia distribución del fenómeno (se ha estimado que el 90% de las plantas terrestres están micorrizadas) y en la diversidad de mecanismos morfológicos, fisiológicos y ecológicos implicados. Durante la simbiosis, la planta hospedera recibe nutrientes minerales del suelo tomados por el hongo (principalmente fósforo), mientras que éste obtiene compuestos de carbono derivados de la fotosíntesis.
Los hongos formadores de micorrizas arbusculares constituyen micorrizas que colonizan el tejido interno de las raíces de la planta hospedera, donde desarrollan estructuras características de la simbiosis (arbúsculos y vesículas), así como micelio extrarradical, el cual interacciona con el ecosistema de la rizósfera y es el encargado de extraer nutrientes del suelo. En este sentido las gramíneas no son la excepción. Buena parte de las especies de esta familia forman micorrizas, lo que favorece y optimiza su adaptación a diversos tipos de ambientes.

El nitrógeno molecular (N) es la única reserva de nitrógeno accesible en la biósfera. Prácticamente ilimitada, esta reserva no es directamente utilizada por los vegetales y animales. El nitrógeno es un constituyente esencial de moléculas fundamentales de todos los seres vivos: aminoácidos, proteínas, ácidos nucleicos, vitaminas, entre las más importantes. Para que el nitrógeno atmosférico pueda ser asimilado, es necesario que sea reducido. Las gramíneas son capaces de asociarse con bacterias diazotróficas pertenecientes a los géneros "Azospirillum", "Azotobacter", "Azoarcus" y "Herbaspirillum" las cuales realizan la fijación biológica del nitrógeno atmosférico (N). Estas bacterias son organismos de vida libre capaces de fijar nitrógeno desde la rizósfera, o sea, desde el área circundante al sistema radicular de la planta e incluirlo en compuestos (como el amonio) fácilmente disponible y absorbible por las plantas. Además de fijar el nitrógeno atmosférico, las bacterias diazotróficas favorecen el desarrollo del sistema radicular de la planta con la cual conviven, al parecer a través de la producción de reguladores de crecimiento u hormonas. De este modo, favorecen una mayor absorción de nutrientes por parte de la planta. Se han informado incrementos del orden de 5 % hasta 30 % en los rendimientos de gramíneas como caña de azúcar, maíz, arroz, trigo y gramíneas forrajeras como resultado de esta asociación. Estas asociaciones no desarrollan estructuras diferenciadas en las que se alberguen los microorganismos, como ocurre en el caso de las leguminosas y las bacterias del género "Rhizobium". En 1998 se ha descrito otro tipo de asociación de diazótrofos en la cual la bacteria (llamada bacteria endófita) se localiza en el interior de la raíz, el tallo y las hojas de la planta. Esta asociación fue descubierta en aislamientos de diazótrofos de plantas forrajeras de Pakistán, en donde se identificó una nueva bacteria fijadora de nitrógeno llamada "Azoarcus". Este microorganismo se localiza en las capas externas del córtex; una vez en el interior de la planta se disemina a los tejidos aéreos probablemente por medio de los vasos del xilema.

Las gramíneas son anemófilas, es decir que el polen es transportado de una planta a otra por medio del viento para efectuar la polinización. Ninguna poácea tiene nectarios, si bien algunos pastos de bosques tropicales —especialmente pequeñas bambusóideas— son polinizadas por insectos. La dispersión de las semillas se produce principalmente por animales e incluso algunas especies presentan estructuras especializadas para atraerlos, como los elaiosomas. No obstante, la mayor parte de las especies presentan ganchos o agujas mediante los cuales los frutos o las diásporas se adhieren a los animales que pasan. Muchas especies se dispersn con el viento para lo cual presentan largos pelos en las aristas. Finalmente, "Spinifex" y algunos otros géneros son plantas rodadoras, las cuales son desarraigadas a la madurez y transportadas enteras por el viento, dispersando sus semillas mientras van rodando. Las aristas pueden ayudar tanto en la dispersión por viento como en la dispersión por animales; la microestructura de la superficie de las aristas puede dar como resultado que el cariopse sea directamente "plantado" en el suelo.

Las especies leñosas de bambúes son conocidas por florecer en forma sincronizada. Muchas de ellas, además, son perennes monocárpicas, es decir que vegetan muchísimos años, florecen una sola vez y mueren luego de la dar las semillas. Esta característica también se encuentra en algunos bambúes herbáceos. En plantas que muestran este tipo de reproducción, todos los miembros de un mismo clon florecen simultáneamente, sea donde fuere que hayan sido transportados en todo el globo terrestre, y todas las plantas, después de un período reproductivo que se puede decir retrasado, mueren.

En los tallos huecos de los bambúes muchas veces se acumula agua, y vive en ella una fauna distintiva. Las poáceas proveen alimento tanto para los adultos (el polen) como para las larvas (las raíces) de distintas especies de escarabajos de la subfamilia Galerucinae de los crisomélidos. Las orugas de mariposas de la familia Nymphalidae, en particular las marrones Satyrinae y las emparentadas Morphinae, son comunes en los miembros de esta familia (se encuentran en alrededor del 10 % de los censos). Los insectos del taxón Hemiptera-Lygaeidae-Blissinae se observan más comúnmente en las especies del clado llamado PACCMAD que en el clado BEP.

Son comunes en las poáceas los hongos parásitos del orden Uredinales y los de la clase Ustilaginomycetes. Los que atacan a las subfamilias Bambusoideae y Pooideae (incluyendo "Stipa" y parientes cercanos) son particularmente distintivos. Las dos terceras partes de los Ustilaginales (unas 600 especies) se encuentran en Poaceae.

Las gramíneas y sus parientes extintos datan de hace unos 89 millones de años, el grupo principal divergió hace unos 83 millones de años. A excepción de los clados basales de la familia Anomochlooideae, Pharoideae y Puelioideae las espiguillas de las gramíneas se conocen en el límite entre el Paleoceno y el Eoceno, hace unos 55 millones de años, y esta cifra está a grandes rasgos en línea con una estimación de la edad de una duplicación del genoma de las gramíneas, ocurrida hace unos 70-50 millones de años. Sin embargo, el fósil de una monocotiledónea ("Programinis burmitis") perteneciente al Cretácico temprano (hace unos 100-110 millones de años) es similar a una gramínea bambusóidea. Si bien este fósil tiene un número de caracteres vegetativos que son comunes entre las poáceas, su identidad todavía necesita confirmación.
Los tejidos vegetales silicificados (fitolitos) preservados en heces fosilizadas (coprolitos) de dinosaurios del Cretáceo tardío halladas en la India indican que por lo menos cinco taxones de gramíneas extinguidas estaban presentes en el subcontinente indio durante ese período geológico (hace unos 71-65 millones de años). Esta diversidad sugiere que el grupo basal de las gramíneas se habría diversificado y distribuido en Gondwana antes de que la India quedara geográficamente aislada.

La fotosíntesis C parece haber estado presente en las gramíneas del Mioceno temprano a medio, tanto en las Grandes Planicies de Norteamérica como en África, hace unos 25-12,5 millones de años. Quizás este tipo de fotosíntesis estuvo inicialmente asociada a cambios adaptativos en respuesta a una disminución en la concentración de CO en la atmósfera, si bien la gran expansión de este mecanismo fisiológico ocurrió hace solo unos 9-4 millones de años. Aún no está claro si este evento estuvo además favorecido por los incrementos en la temperatura, la disminución de las precipitaciones, el aumento de los vientos y el concomitante incremento de incendios, que habrían removido a los árboles de algunos hábitats en ese período. Los detalles de los mecanismos de la fotosíntesis C y las morfologías asociadas con ella son muy diversos y presentan una considerable variación, particularmente en el caso de la subfamilia de las panicóideas. De hecho, la fotosíntesis C aparentemente se originó y evolucionó independientemente hasta ocho veces en esta subfamilia. Asimismo, este mecanismo se originó de modo independiente en otras subfamilias, como Micrairoideae, Aristidoideae y Chloridoideae. Independientemente de su mayor eficiencia fotosintética, las gramíneas C presentan menor contenido de nitrógeno, mayor cantidad de fibras de esclerénquima y pueden ser menos palatables que las gramíneas C. A pesar de estas características, existió una radiación de mamíferos herbívoros en el Mioceno, que pudo estar asociada con la ampliación de las praderas y sabanas, dominadas por pastos. Sin embargo, cuando las especies de gramíneas de pradera se expandieron hacia Nebraska en el Mioceno temprano —hace unos 23 millones de años— los ungulados hipsodontos ya existían para entonces.

La inflorescencia de las gramíneas es una estructura nueva en el repertorio reproductivo de las plantas con flores. Es intrincada desde el punto de vista de la biología del desarrollo, tiene una importancia central en la agronomía y, finalmente, es una verdadera intriga evolutiva. Su arquitectura controla el tipo de polinización y la producción de semillas, por lo que es una diana importantísima tanto para la selección natural como para el mejoramiento genético y la biotecnología. Es de destacar que la diversidad de estructuras que presentan las espiguillas y espigas de las gramíneas estén controladas por genes que afectan el desarrollo y que no se hallan presentes en ninguna otra familia de plantas. Estos genes se han originado luego de extensas duplicaciones del genoma y la posterior diversificación funcional de los mismos.

La pálea (y aparentemente también la lemma) quizás sea derivada del cáliz, y las lodículas quizás sean derivadas de la corola (Ambrose "et al." 2000). Por otro lado un estudio de morfología comparativa sugiere que la lemma es una bráctea y que la pálea representa dos tépalos connados del verticilo más externo (Whipple y Schmidt 2006). Teniendo en cuenta las relaciones de parentesco cercano entre Ecdeicoleaceae y Joinvilleaceae recientemente encontradas por Marchant y Briggs (2007) y la probabilidad de que las flores de "Anomochloa" sean "sui generis", la morfología floral de "Streptochaeta" puede ser plesiomórfica (ancestral) en la familia. Es interesante que las flores de "Ecdeicolea" también son notablemente monosimétricas, con los dos tépalos adaxiales del verticilo externo más largos y aquillados, y si bien esto no es relevante en forma directa, una diferenciación comparable en el verticilo externo del perianto ocurre en Xyridaceae, todos estos probablemente sean paralelismos. Una interpretación más común de la pálea es que es de naturaleza profilar/bracteolar, las monocotiledóneas comúnmente tienen profilos bicarenados, sin embargo, parece que las bracteolas tuvieron que reaparecer en Poaceae, ya que los clados más emparentados pero externos a la familia ("outgroups" en análisis cladístico) no las poseen. Las lodículas parecen estar involucradas en la apertura de las flores estaminadas y las perfectas, mientras que pueden estar ausentes en las flores pistiladas (Sajo "et al." 2007).

Los análisis filogenéticos utilizando secuencias expresas del ADN y la estructura general de los genomas sugieren que las gramíneas difieren mucho más de otras monocotiledóneas de lo que se diferencian éstas de las dicotiledóneas. Tales conclusiones acerca de las relaciones de las poáceas con las otras familias de monocotiledóneas y con las dicotiledóneas no son sorprendentes. De hecho, las gramíneas son fácilmente reconocibles e identificables de cualquier otra familia, y su monofilia se halla sustentada tanto por la morfología como por los análisis moleculares de ADN. Los caracteres fenotípicos que apoyan la monofilia de la familia son la inflorescencia con brácteas, el perianto reducido, el tipo de fruto y los caracteres del embrión y de la pared del grano de polen. Las similitudes con las ciperáceas (Cyperaceae) en el hábito de cecimiento y en el tipo de espiguillas representan una evolución convergente, y no una sinapomorfía. De hecho, las ciperáceas están más emparentadas con los juncos (Juncaceae) que con las gramíneas, las cuales pertenecen al núcleo de los Poales.

La importancia económica y ecológica de la familia ha motivado la realización de una importante cantidad de estudios sistemáticos. A principios del siglo XIX, las diferencias entre las espiguillas de las poóideas y las panicóideas llevaron a Robert Brown a dividir a la familia en estos dos grupos básicos. A principios del siglo XX, los caracteres de la epidermis de las hojas y el número de cromosomas llevó a la separación de las clorídeas de las poóideas. A mediados del siglo XX, la anatomía interna de la hoja (en particular, la presencia o ausencia de la anatomía Kranz y los caracteres del embrión (presencia o ausencia de epiblasto y ciertas características del nudo cotiledonar), llevaron al reconocimiento de cinco a ocho subfamilias. Desde fines del Siglo XX, los estudios filogenéticos basados en varias secuencias de genes demostraron ser congruentes con muchas de las relaciones filogenéticas inferidas previamente a través de caracteres estructurales y fisiológicos. Los estudios moleculares más avanzados apoyan el reconocimiento de 13 subfamilias.Los tres primeros linajes que divergieron son Anomochlooideae (nativa de Brasil), Pharoideae (nativa de los trópicos del Viejo y Nuevo Mundo) y Puelioideae (nativa del oeste de África). Los integrantes de estos tres grupos solo son unas 25 de las casi 10.000 especies de la familia. El resto de las especies se distribuyen en dos grandes grupos. El primero, llamado clado BEP, agrupa a Bambusoideae, a Ehrharttoideae, y a Pooideae. El segundo, denominado clado PACCMAD, agrupa a Panicoideae, Arundinoideae, Chloridoideae, Centothecoideae, Micrairoideae, Aristidoideae y Danthonioideae. El clado PACCMAD está sustentado por un carácter del embrión: un largo entrenudo en el mesocótilo. La anatomía C es el estado plesiomórfico o ancestral en la familia. Todas las especies C se encuentran en el clado PACCMAD.

Todas las subfamilias mencionadas son monofiléticas, aunque sólo unas pocas poseen sinapomorfías morfológicas que caractericen a todos sus miembros. Más bien su monofilia se halla sostenida por grupos de caracteres morfológicos que deben observarse en conjunto. Las relaciones dentro de los grandes clados PACCMAD y BEP son en su mayor parte poco claras todavía, de hecho la posición de las poóideas es poco clara en algunos análisis.
El cladograma que muestra las relaciones entre los 13 clados mencionados es el siguiente:

La familia fue reconocida por todos los sistemas de clasificación de plantas, comenzando por el de Carlos Linneo en su obra "Systema naturae" pasando por los de Adolf Engler, Arthur Cronquist, Armen Takhtajan, entre muchos otros. Los sistemas modernos de clasificación (sistema de clasificación APG III, y los dos sistemas filogenéticos previos, APGI y APGII) también reconocen a esta familia.

Entre los caracteres diagnósticos de la familia se hallan varios de los órganos ya descriptos: las hojas que tienen vainas largas y abiertas y las lígulas en la unión entre la vaina y la lámina, los tallos redondos y usualmente huecos en los entrenudos, y las inflorescencias cuya unidad básica es la espiguilla. Las flores individuales son pequeñas, con perianto inconspicuo y con un gineceo que usualmente tiene dos estigmas plumosos y un único óvulo. En el fruto seco, de tipo aquenio (cariopse), el embrión —relativamente grande— ocupa una posición lateral y se ubica junto al tegumento de la semilla.

Son particularmente útiles para la identificación de los géneros los caracteres de la espiguilla, como el tamaño, el plano de compresión, la presencia o ausencia de glumas, el número de flores por espiguilla, la presencia de flores estériles o incompletas, el número de venas en las glumas y las glumelas, la presencia o ausencia de aristas, y la forma de las inflorescencias secundarias.

A continuación se brinda una descripción de las subfamilias de las gramíneas y de los géneros más importantes o representativos dentro de cada una.

Las anomoclóideas son plantas herbáceas y presentan inflorescencias con una morfología característica que no se parecen a las espiguillas típicas o usuales de las restantes gramíneas. Parecen ser el clado hermano de todo el resto de la familia, Soreng y Davis 1998 lo cual sugiere que la espiguilla característica de los pastos probablemente se originó luego de que las anomoclóideas divergieran del resto de las gramíneas. Los integrantes de esta subfamilia presentan pseudopecíolo con un pulvínulo apical, la lígula de las hojas transformada en un mechón de pelos y las ramas de la inflorescencia cimosas. En la inflorescencia presentan dos brácteas a lo largo de cada ramificación y dos más debajo de cada flor, o bien las flores se disponen en forma espiralada a lo largo de ejes racimosos, con varias brácteas debajo de cada flor. Las anteras son centrifijas (en "Anomochloa") o casi basifijas ("Streptochaeta"). Característicamente, la primera hoja de la plántula carece de lámina foliar. Los números cromosómicos básicos son x=11 y 18. Las especies pertenecientes a esta subfamilia fueron originalmente incluidas dentro de las bambusóideas, pero actualmente se reconoce que sólo poseen un parentesco distante con ellas. La subfamilia incluye a dos géneros —"Anomochloa" y "Streptochaeta"— con cuatro especies que habitan selvas y se distribuyen desde Centroamérica hasta el sudeste de Brasil.

Las faróideas se caracterizan por sus hojas resupinadas, es decir, con las láminas foliares con la cara abaxial hacia arriba. Las espiguillas son unifloras y presentan seis estambres con las anteras centrifijas. El coleoptile presenta lámina. Esta subfamilia incluye cuatro géneros y unas doce especies pantropicales, las cuales habitan en las selvas.
Esta subfamilia, junto con Anomochlooideae y Puelioideae, fue tradicionalmente incluida dentro de las bambusóideas, pero los estudios filogenéticos sobre datos moleculares han demostrado que las tres son los clados basales de toda la familia de las gramíneas. Las faróideas se diferencian de las puelióideas por sus espiguillas unifloras y sus estigmas trífidos.

Las puelióideas junto con las demás subfamilias de gramíneas, pero a excepción de las puelióideas y las anomoclóideas, presentan gineceos con dos estigmas y espiguillas que se desarticulan por encima de las glumas a la madurez. Se caracterizan además por su androceo con seis estambres. Incluye dos géneros —"Guaduella" y" Puelia"— y aproximadamente once especies distribuidas por África tropical.

Las bambusóideas, en sentido restringido, incluyen tanto especies herbáceas como leñosas y son casi exclusivamente tropicales en su distribución. Las hojas son pseudopecioladas. Las flores presentan tres lodículas y un androceo con seis estambres, raramente de dos a 14. El ovario lleva dos o tres estigmas, raramente uno solo. La primera hoja de las plántulas no presenta lámina. Los números cromosómicos básicos son x=7 y x=9 a 12. Incluye 112 géneros con aproximadamente 1647 especies tropicales a templadas.

La diversificación dentro de los bambúes ocurrió hace 30 a 40 millones de años. Los bambúes leñosos forman un grupo monofilético hermano del clado que contiene a las especies herbáceas. Los bambúes leñosos, con sus tallos de hasta 40 metros de altura, ciertamente no se parecen al césped. La floración en muchas de estas especies también es inusual, ya que ocurre en ciclos de hasta 120 años. Aun cuando los tallos individuales viven por solo una o unas pocas décadas, alguna forma de "reloj" hace que los tallos florezcan todos al mismo tiempo en todo el rango de distribución de la especie, causando a veces bruscos cambios ecológicos, como los que se asocian a las "ratadas". Algunos géneros de bambúes leñosos son "Bambusa" (120 especies), "Chusquea" (100 especies), "Arundinaria" (50 especies), "Sasa" (50 especies), y "Phyllostachys" (45 especies).

Los miembros de esta subfamilia presentan espiguillas con glumas muy reducidas y el androceo con seis estambres, raramente con uno solo. Los números cromosómicos básicos de esta subfamilia son x=10 y x=15. Comprende 21 géneros y 111 especies, entre las que se incluyen a los miembros de las tribus Ehrharteae del Hemisferio Sur, así como al cosmopolita Oryzeae. Este último es acuático o de tierras húmedas. El representante más conocido de la tribu Oryzeae es el arroz asiático "Oryza sativa", uno de los cultivos más importantes del mundo. En el norte de África también se cultiva otra de las 22 especies de "Oryza": "O. glaberrima". En Estados Unidos cobra importancia otra especie de la tribu, "Zizania aquatica", el arroz silvestre norteamericano.

Esta subfamilia es la más grande de las gramíneas. Consiste en 194 géneros que integran unas 4200 especies. Se distribuyen en las regiones de clima templado de todo el globo. Entre los géneros sobresalientes se incluyen importantes cereales, como el trigo, la cebada y la avena, y también al centeno ("Secale cereale"), a los pastos utilizados para césped (como "Poa", con 500 especies), para heno ("Festuca", 450 especies), para pasturas (como "Phleum", "Dactylis"), y algunas malas hierbas (como "Agrostis", con 220 especies, y "Poa"). Otros géneros importantes de esta subfamilia son "Stipa" (300 especies), "Calamagrostis" (270 especies), "Bromus" (150 especies), y "Elymus" (150 especies).Las poóideas se distinguen porque las ramificaciones principales de la inflorescencia son dísticas, la lemma usualmente consta de cinco nervios. Además, presentan oligosacáridos derivados de la fructosa en el tallo. Presentan cromosomas usualmente largos y el número cromosómico básico es x=7, más raramente x=2, 4, 5 o 6.

Los integrantes de las cloridoideas presentan espiguillas que se desarticulan por encima de las glumas y pelos bicelulares distintivos en la epidermis de las hojas. No obstante, este último carácter puede ser una sinapomorfía de sólo un subgrupo del clado. Todo el clado salvo dos especies muestra fotosíntesis por la vía del C. Los números cromosómicos básicos prevalecientes en la subfamilia son x=9 y x=19, aunque existen géneros con x=7 y 8. La subfamilia se distribuye principalmente en regiones tropicales áridas y semiáridas, donde se supone que la fotosíntesis C es ventajosa. Los centros de distribución ubicados en África y Australia sugieren un origen en el Hemisferio Norte. Algunos géneros importantes son "Eragrostis" (350 especies), "Muhlenbergia" (160 especies), "Sporobolus" (160 especies), "Chloris" (55 especies), "Spartina" (15 especies) y "Eustachys" (10 especies).

Las panicóideas han sido reconocidas taxonómicamente desde hace mucho tiempo, debido a sus espiguillas características. Las cañas son usualmente sólidas, las espiguillas se hallan comprimidas dorsalmente, no presentan raquilla y son bifloras. La desarticulación de la espiguilla a la madurez se produce por debajo de las glumas. El tipo de fisiología de la fotosíntesis prevaleciente es la C. Los gránulos de almidón en el endosperma son simples. Los números cromosómicos básicos más típicos son x=5, 9 y 10, aunque también se hallan especies con x=7, 12 y 14.
La subfamilia es principalmente tropical y contiene dos grandes tribus, Andropogoneae y Paniceae, junto con un número de grupos pequeños. Las andropogóneas son relativamente fáciles de reconocer debido a sus espiguillas dispuestas de a pares. Las paníceas no son tan homogéneas como los miembros de Andropogoneae. La subfamilia comprende 203 géneros y 3.600 especies. Entre los géneros más importantes se incluyen "Panicum" (470 especies, polifilético), "Paspalum" (330 especies), "Andropogon" (100 especies), "Setaria" (100 especies), "Sorghum" (20 especies), y "Zea" (4 especies). El sorgo y el maíz son dos cultivos de gran importancia económica y ambos se incluyen en esta subfamilia.

Las centotecoideas son una subfamilia pobremente estudiada. Está constituida por unas 30 especies distribuidas en 11 géneros que habitan selvas templadas cálidas a tropicales. Su característica más distintiva es la presencia de estilo en la flor y de epiblasto en el embrión. El número cromosómico básico más frecuente es x=12, aunque también hay géneros con x=11.

Con 14 géneros y entre 20 a 38 especies, las arundinoideas son una subfamilia cuya exacta delimitación todavía no es clara. Son gramíneas hidrofíticas a xerofíticas que habitan regiones templadas a tropicales. Sus números cromosómicos básicos son x=6, 9 y 12. "Arundo" (con 3 especies, "Arundo donax" es la caña de Castilla) y "Phragmites" (2 especies) son los géneros más conocidos de esta subfamilia.

Esta subfamilia monofilética perteneciente al clado PACCMAD ha sido reinstalada y circunscripta nuevamente en el año 2007. Los miembros de la misma presentan estomas con células subsidiarias en forma de domo, las lígulas con mechones de pelos, embriones pequeños, fotosíntesis C, granos de almidón simples en el endosperma. Comprende 8 géneros y unas 170 especies en su mayoría tropicales. Algunos de los géneros (como por ejemplo" Eriachne") no estaban asignados a ninguna familia hasta hace poco tiempo y otros estaban incluidos en otras subfamilias (ejemplo "Isachne" en las Panicoideas). Incluye a los géneros "Isachne" (100 especies), "Eriachne" (35 especies) y "Micraira" (8 especies).

Las aristoideas incluyen 3 géneros y de 300 a 385 especies de regiones templadas cálidas, con aristas con una columna basal y fotosíntesis del tipo C. Los números cromosómicos básicos son x=11 y x=12. Comprende al gran género "Aristida" (230 a 330 especies) y "Stipagrostis" (50 especies).

Danthonioideae es una subfamilia bastante distribuida en todo el globo, especialmente en el Hemisferio Sur. Presentan profilos bilobados y las sinérgidas del saco embrionario de tipo haustorial. Los números cromosómicos básicos son x=6, 7 y 9. Comprende 19 géneros y unas 270 especies. Entre los géneros con mayor número de especies se hallan "Danthonia" (100 especies) y "Rytidosperma" (90 especies). "Cortaderia selloana" es la popular "cortadera" una gramínea ornamental que se incluye dentro de esta subfamilia.

El listado de todos los géneros conocidos de gramíneas, ordenados alfabéticamente, se provee en el anexo denominado géneros de Poaceae. En muchos casos se listan sinónimos. Los enlaces a sinónimos llevan al nombre de género preferido.

La familia o alguna de sus subfamilias poseen los siguientes sinónimos:

Aegilopaceae, Agrostidaceae, Alopecuraceae, Andropogonaceae, Anomochloaceae, Arundinaceae, Arundinellaceae, Avenaceae, Bambusaceae, Chloridaceae, Eragrostidaceae, Festucaceae, Hordeaceae, Lepturaceae, Melicaceae, Miliaceae, Nardaceae, Oryzaceae, Panicaceae, Pappophoraceae, Parianaceae, Phalaridaceae, Pharaceae, Saccharaceae, Spartinaceae, Sporobolaceae, Stipaceae, Streptochaetaceae, Triticaceae, Zeaceae.

La familia de las gramíneas es probablemente la que mayor importancia tiene para la economía humana. De hecho, alrededor del 70 % de la superficie cultivable del mundo está sembrada con gramíneas y el 50 % de las calorías consumidas por la humanidad proviene de las numerosas especies de gramíneas que son utilizadas directamente en la alimentación, o bien, indirectamente como forrajes para los animales domésticos. En términos de la producción global, los 4 cultivos más importantes son gramíneas: caña de azúcar ("Saccharum officinarum"), trigo, arroz y maíz. La cebada y el sorgo están entre los primeros 12. Por otro lado, varias especies de gramíneas se utilizan en la industria.






Un glosario de términos generales y especializados en Glosario de Poaceae





</doc>
<doc id="2220" url="https://es.wikipedia.org/wiki?curid=2220" title="Prochlorales">
Prochlorales

Prochlorales es un grupo de cianobacterias verdes que son importantes componentes del picoplancton fotosintético. También fueron llamadas Prochlorophyta (Lewin, 1976) y Chloroxybacteria. Durante mucho tiempo sólo se conocieron las cepas del género "Prochloron", que proceden del océano Pacífico en su mayoría. Son formas cocoides (esféricas) que aparecen asociadas con tunicados y fuera de sus huéspedes han sido de difícil cultivo. Luego se encontraron dos géneros más de vida libre: "Prochlorococcus", también cocoide, y "Prochlorothrix", filamentosa. Son fisiológicamente como las algas verdes, puesto que realizan la fotosíntesis oxigénica, con los mismos pigmentos principales (clorofila "a" y clorofila "b", carotenoides, fundamentalmente β-caroteno) y carecen de ficobilinas; sin embargo, morfológicamente se parecen a las cianobacterias o algas verdeazuladas. 

Son semejantes a los cloroplastos de las plantas superiores, tanto fisiológica como morfológicamente. Algunos autores propusieron que los plastos de las algas verdes, y por tanto también los las plantas terrestres, sus descendientes, se originaron por la incorporación de células de este grupo (endosimbiosis). Estudios genéticos demostraron, por el contrario, que la condición verde podría haberse adquirido independientemente en los tres géneros y en los plastos verdes. También, que los plastos derivan de cianobacterias más típicas, próximas a "Synechococcus" (Synechococcales).


</doc>
<doc id="2223" url="https://es.wikipedia.org/wiki?curid=2223" title="Phacus">
Phacus

Phacus es un género de microalgas protistas unicelulares de forma aplanada y apuntada perteneciente al filo "Euglenozoa". Presenta un periplasma muy y rígido, con bandas espiraladas, numerosos plastos verdes y una mancha ocular roja muy visible próxima a la base flagelar. El movimiento solo es ejercido por el flagelo, sin los movimientos de la película superficial que son tan visibles en otros euglénidos.

El antiguo género "Hyalophacus", que carece de plastos, es considerado actualmente un sinónimo de la especie "Phacus ocellatus".


</doc>
<doc id="2224" url="https://es.wikipedia.org/wiki?curid=2224" title="Prorocentrum">
Prorocentrum

Prorocentrum es un género de protistas dinoflagelados de la clase "Dinophyceae", subclase "Desmophycidae". Presentan células comprimidas, lanceoladas, móviles que están desprovistas de cíngulo. Con una teca celulósica, bivalva, perforada y dos flagelos casi isocontos, con tricocistes, uno dirigido hacia adelante, mientras el otro tiende a enrollarse alrededor del polo anterior. Se presenta una escama anterior entre los dos flagelos. Probablemente son los dinoflagelados más primitivos.

El género fue descrito por el naturalista alemán Christian Gottfried Ehrenberg en 1834. Se reconocen como válidas 72 especies:


</doc>
<doc id="2225" url="https://es.wikipedia.org/wiki?curid=2225" title="Polykrikos">
Polykrikos

Polykrikos, organismos unicelulares de la división Dinophyta, clase Dinophyceae, subclase "Dinophyceae", orden Gymnodiniales. Con dos flagelos heterocontos en el sulco y el cíngulo. Los individuos presentan varios núcleos y varios pares de flagelos.


</doc>
<doc id="2227" url="https://es.wikipedia.org/wiki?curid=2227" title="Peridinium">
Peridinium

Peridinium es un género de protistas dinoflagelados de la clase "Dinophyceae", orden "Peridiniales", con dos flagelos heterocontos en el sulcus y el cíngulo. Son muy numerosos en el plancton. La división celular es oblicua, cada célula hija se lleva la mitad de la teca. Presentan forma globosa, bicónica, teca con placas ornamentadas.

Se ha encontrado que la especie "Peridinium quinquecorne" tiene una alga dorada (Chrysophyceae) endosimbionte que aún presenta plasto, mitocondria, dictiosoma y núcleo.

"Peridinium" consta de las siguientes especiesː



</doc>
<doc id="2228" url="https://es.wikipedia.org/wiki?curid=2228" title="PPP">
PPP




</doc>
<doc id="2229" url="https://es.wikipedia.org/wiki?curid=2229" title="Protocolo">
Protocolo

Protocolo hace referencia a varios artículos:












</doc>
<doc id="2231" url="https://es.wikipedia.org/wiki?curid=2231" title="Punto triple">
Punto triple

El punto triple es aquel en el cual coexisten en equilibrio el estado sólido, el estado líquido y el estado gaseoso de una sustancia. Se define con respecto a una temperatura y una presión de vapor.

El punto triple del agua, por ejemplo, está a 273.16 K (0.01 °C) y a una presión de 611.73 Pa ITS90. Esta temperatura, debido a que es un valor constante, sirve para calibrar las escalas Kelvin y Celsius de los termómetros de mayor precisión.

Es la combinación de presión y temperatura en la que los estados de agregación del agua, sólido, líquido y gaseoso (hielo, agua líquida y vapor, respectivamente) pueden coexistir en un equilibrio estable. Se produce exactamente a una temperatura de 273.16 K (0.0098 °C) y a una presión parcial de vapor de agua de 611.73 pascales (6.1173 milibares; 0.0060373057 atm). En esas condiciones, es posible cambiar el estado de toda la masa de agua a hielo, agua líquida o vapor, arbitrariamente, haciendo pequeños cambios en la presión y la temperatura. Se debe tener en cuenta que incluso si la presión total de un sistema está muy por encima de 611.73 pascales (es decir, un sistema con una presión atmosférica normal), si la presión parcial del vapor de agua es 611.73 pascales, entonces el sistema puede encontrarse aún en el punto triple del agua. Estrictamente hablando, las superficies que separan las distintas fases también debe ser perfectamente planas, para evitar los efectos de las tensiones de superficie.

El agua tiene un inusual y complejo diagrama de fase (aunque esto no afecta a las consideraciones generales expuestas sobre el punto triple). A altas temperaturas, incrementando la presión, primero se obtiene agua líquida y, a continuación, agua sólida. Por encima de 109 Pa aproximadamente se obtiene una forma cristalina de hielo. A temperaturas más bajas en virtud de la compresión, el estado líquido deja de aparecer y el agua pasa directamente de sólido a gas.

A presiones constantes por encima del punto triple, calentar hielo hace que se pase de sólido a líquido y de este a gas (o vapor). A presiones por debajo del punto triple, como las encontradas en el espacio exterior, donde la presión es cercana a cero, el agua líquida no puede existir y, al calentarse, el hielo se convierte directamente en vapor de agua sin pasar por el estado líquido, proceso conocido como sublimación.

La presión del punto triple del agua fue utilizada durante la misión Mariner 9 a Marte como un punto de referencia para definir "el nivel del mar". Misiones más recientes hacen uso de altimetría láser y gravimetría en lugar de la presión atmosférica para medir la elevación en Marte.

Se deben tener en consideración los grados de libertad que nos permite apreciar con mayor facilidad las variables de presión y temperatura, en que momento se pueden modificar y esta dada por la siguiente ecuación: 

L=C-F+2

Donde

L=Grados de libertad(variables que puedes modificar).

C=Número de componentes del sistema.

F= Número de fases del sistema.

Tabla pertinente para conocer los grados de libertad sobre un diagrama de fases. 
En esta tabla se incluyen los puntos triples de algunas sustancias comunes. Estos datos están basados en los proporcionados por la National Bureau of Standards (ahora NIST) de los EE.UU de América.



</doc>
<doc id="2233" url="https://es.wikipedia.org/wiki?curid=2233" title="Paramichelia">
Paramichelia

Paramichelia es un género de plantas de la familia de las magnoliaceas.



</doc>
<doc id="2234" url="https://es.wikipedia.org/wiki?curid=2234" title="Platycarya">
Platycarya

El género botánico "Platycarya pertenece a la familia de las "juglandáceas".

Son árboles u ocasionalmente arbustos, deciduos, monoicos. Brotes terminales subglobosos a ovoides, con escamas anchas, sobrepuestas. Hojas impares, pinnadas, raramente simples; folíolos (1-)7-15(-23), márgenes serrados. Inflorescencias terminales en crecimientos nuevos, erectas, panícula andrógina de espigas masculinas y femeninas, las laterales macho, las centrales hembras. Flores entomófilas. Flores masculinas sin sépalos, bráctea entera; bracteolas ausentes; estambres 4-15, anteras glabras. Flores femeninas subtendidas en una bráctea entera, ± libres del ovario; 2 bracteolas 2, 2-sépalos 2; estilo ausente; estigmas carinales, 2-lobuladas, cortas, plumosas. Espigas frutícolas cortas, erectas, brácteas rígidas, persistentes. Fruto pequeño, achatado, angostado, 2-cámaras en la base. Germinación epigea.



</doc>
<doc id="2235" url="https://es.wikipedia.org/wiki?curid=2235" title="Pterocarya">
Pterocarya

Pterocarya son un género de plantas de la familia de las juglandáceas. "Pterocarya" proviene del griego antiguo "πτερο-" "ala" + "κάρυον" "nuez", originario de Asia.

Son árboles caducifolios, de 10-40 metros de alto, con hojas pinnadas de 20-45 cm de largo, con 11-25 hojuelas. Las ramas tienen médulas compartimentadas, una característica que comparte el género "Juglans", pero no con el género "Carya", las pacanas, de la misma familia de Juglandáceas.

Las flores son monoicos, en amentos. Los amentos con semillas cuando maduran (alrededor de 6 meses después de la polinización) son colgantes, de 15-45 cm de largo, con 20-80 semillas colgadas a lo largo de ellos.

Las semillas son pequeñas nueces de 5-10 mm de ancho, con dos alas, una a cada lado. En algunas de las especies, las alas son cortas (5-10 mm) y anchas (5-10 mm), en otros más largos (10-25 mm) y más estrechas (2-5 mm).

Hay seis especies de pterocarias:

Otra especie de China, con un follaje similar y un anillo circular inusual justo alrededor de la nuez (en lugar de dos alas a los lados), previamente incluida como "Pterocarya paliurus", se ha transferido ahora a un nuevo género, como "Cyclocarya paliurus".

Las pterocarias son árboles grandes y de crecimiento rápido, ocasionalmente plantados en parques y grandes jardines. El más cultivado fuera de Asia es "P. fraxinifolia", junto con "P. rhoifolia". El híbrido "P. x rehderiana", un cruce entre "P. fraxinifolia" y "P. stenoptera", es incluso más rápido de crecimiento y ocasionalmente ha sido plantado para la producción de madera. La madera es de buena calidad, parecida a la del nogal, aunque no tan densa ni fuerte.



</doc>
<doc id="2237" url="https://es.wikipedia.org/wiki?curid=2237" title="Primeros auxilios">
Primeros auxilios

Los primeros auxilios consisten en la atención inmediata que se le da a una persona enferma, lesionada o accidentada en el lugar de los acontecimientos, antes de ser trasladada a un centro asistencial u hospitalario.

Son medidas terapéuticas urgentes que se aplican a las víctimas de accidentes o enfermedades repentinas. El propósito de los primeros auxilios es aliviar el dolor, la ansiedad del herido o paciente y evitar el agravamiento de su estado.

Por ejemplo, si se encuentra a una persona inconsciente o con sospecha de haber recibido una descarga eléctrica o electrocución, la persona que la atenderá debe estar segura que no le va a ocurrir lo mismo. Debe revisar que la zona sea segura. Si hay alguien más en el lugar del accidente, debe darle instrucciones para solicitar los servicios de emergencia, dando los siguientes datos:

A continuación se hace la valoración del paciente.


Según las nuevas pautas del European Resuscitation Council (ERC) que se publicaron en 2010, el pulso no es un criterio para decidir sobre si empezar la reanimación cardiopulmonar. En lugar de eso, la respiración es más importante porque es más fácil verificar si una persona respira. Además dicen estas Guías que los reanimadores entrenados deberían también proporcionar ventilaciones con una relación compresiones-ventilaciones (CV) de 30:2. Para los reanimadores no entrenados, se fomenta la RCP con solo compresiones torácicas guiada por teléfono.

<br>

Se considera como herida a toda pérdida de continuidad de la piel, de las mucosas o tejidos del organismo, producida por un traumatismo o accidente o por un acto quirúrgico. Como consecuencia de la agresión de los tejidos existe riesgo de infección y posibilidad de lesiones en órganos o partes adyacentes a la zona lesionada como: músculos, nervios, vasos sanguíneos, etc. En estas situaciones es necesario poseer conocimientos de primeros auxilios para así aplicar las medidas de atención necesarias según la situación lo requiera.

La sangre se encuentra circulando por el interior de los vasos sanguíneos (arterias, venas y capilares), que la transporta por todo el cuerpo. Cuando alguno de estos vasos sanguíneos se rompe, la sangre sale de su interior, originando una hemorragia, que puede ser interna o externa. Es importante conocer la atención que se debe entregar frente a una hemorragia ya que las consecuencias de un mal manejo pueden causar un Shock hipovolémico.

Las heridas son lesiones que producen pérdida de la integridad de los tejidos blandos. Son producidas por agentes externos, como un cuchillo o agentes internos como un hueso fracturado; pueden ser abiertas o cerradas, leves o complicadas. Los principales riesgos de una herida son la infección y el Shock hemorrágico.

Signos y Síntomas: dolor, hemorragia, destrucción, daño de los tejidos blandos, eritema de la zona.

<br>

Hemorragia se define como la salida incontrolada de sangre de cualquier vaso sanguíneo.

La cantidad de sangre que se pierde dependerá del vaso sanguíneo sangrante: vena o arteria. Esta última es evidentemente más grave, ya que en pocos minutos se puede perder un gran volumen de sangre, produciendo shock y riesgo de muerte.


Según el vaso dañado que produce la salida de sangre, podemos distinguir las siguientes características en una hemorragia o sangramiento.


La hemorragia arterial es la más peligrosa, porque si no se controla a tiempo, puede producir, Shock por pérdida importante de sangre en poco tiempo, debilitamiento agudo, muerte


Es la pérdida de grandes volúmenes afecta de manera significativa el transporte de oxígeno y nutrientes a todo el organismo, lo que lleva a esta emergencia médica, que si no es corregida la perdida de volumen puede producir un Paro Cardiorrespiratorio.

Signos y síntomas:



Son un tipo específico de lesión de los tejidos blandos producidas por agentes físicos, químicos, eléctricos o radiaciones.

Se producen por exposición al fuego, a metales calientes, a radiación, a sustancias químicas cáusticas, a la electricidad o, en general, a cualquier fuente de calor (por ejemplo el sol). Las quemaduras se clasifican según la profundidad del tejido dañado y según la extensión del área afectada. Una quemadura de primer grado, que sólo afecta a la capa superficial de la piel, se caracteriza por el enrojecimiento. Una quemadura de segundo grado presenta formación de flictenas (ampollas), y una de tercer grado afecta al tejido subcutáneo, músculo y hueso produciendo una necrosis. La gravedad de una quemadura también depende de su extensión. Esta se mide en porcentajes de la superficie corporal. Las quemaduras graves producen shock y gran pérdida de líquidos. Un paciente con quemaduras de tercer grado que ocupen más del 10 % de la superficie corporal debe ser hospitalizado lo antes posible.

En aquellos casos de emergencia, conviene tener presentes ciertas reglas de nemotecnias que permitan recordar fácilmente el orden de actuación. Un ejemplo de regla nemotecnia puede ser la siguiente - PAS:


Todo hogar, escuela, lugar público, centro de trabajo y automóvil debe contar con un botiquín que contenga lo necesario para salvar una vida y evitar complicaciones provocadas por un accidente.

Un botiquín debe contener material de curación y medicamentos que no tengan riesgo para las personas, sin embargo siempre debe preguntarse antes de administrarlos sobre una posible alergia o reacción negativa ante cualquier medicina o sustancia.

Es importante que el botiquín no esté al alcance de los niños, se conserve en un lugar fresco y seco y que se revise periódicamente la fecha de caducidad de los medicamentos para sustituirlos en caso necesario.

El botiquín debe incluir:





</doc>
<doc id="2238" url="https://es.wikipedia.org/wiki?curid=2238" title="Proyección cartográfica">
Proyección cartográfica

La representación cartográfica es un sistema de representación gráfica que establece una relación ordenada entre los puntos de la superficie curva de la Tierra y los de una superficie plana (mapa). Estos puntos se localizan auxiliándose en una red de meridianos y paralelos, en forma de malla. La única forma de evitar las distorsiones de esta proyección sería usando un mapa geodésico, aunque la distorsión es minimizada representada en un mapa esférico.

En un sistema de coordenadas proyectadas, los puntos se identifican por las coordenadas cartesianas ("x" e "y") en una malla cuyo origen depende de los casos. Este tipo de coordenadas se obtienen matemáticamente a partir de las coordenadas geográficas (longitud y latitud), que no son proyectadas.

Las representaciones planas de la esfera terrestre se llaman mapas, y los encargados de elaborarlos o especialistas en cartografía se denominan cartógrafos.

Se suelen establecer clasificaciones en función de su principal propiedad; el tipo de superficie sobre la que se realiza la proyección: cenital (un plano), cilíndrica (un cilindro) o cónica (un cono); así como la disposición relativa entre la superficie terrestre y la superficie de proyección (plano, cilindro o cono) pudiendo ser tangente, secante u oblicua. Según la propiedad que posea una proyección puede distinguirse entre: 


No es posible tener todas las propiedades anteriores a la vez, por lo que es necesario optar por soluciones de compromiso que dependerán de la utilidad a la que sea destinado el mapa.

Dependiendo de cuál sea el punto que se considere como centro del mapa, se distingue entre proyecciones polares, cuyo centro es uno de los polos; ecuatoriales, cuyo centro es la intersección entre la línea del Ecuador y un meridiano; y oblicuas o inclinadas, cuyo centro es cualquier otro punto. 

Se distinguen tres tipos de proyecciones básicas: cilíndricas, cónicas y acimutales.

La proyección de Mercator, que revolucionó la cartografía, es cilíndrica y conforme en ella, se proyecta el globo terrestre sobre una superficie cilíndrica. Es una de las más utilizadas, aunque por lo general en forma modificada, debido a 
las grandes distorsiones que ofrece en las zonas de latitud elevada, lo que impide apreciar a las regiones polares en su verdadera proporción. Es utilizada en la creación de algunos mapamundis. Para corregir las deformaciones en latitudes altas se usan proyecciones pseudocilíndricas, como la de Van der Grinten, que es policónica, con paralelos y meridianos circulares. Es esencialmente útil para ver la superficie de la Tierra completa.

La proyección cónica se obtiene proyectando los elementos de la superficie esférica terrestre sobre una superficie cónica tangente, situando el vértice en el eje que une los dos polos. Aunque las formas presentadas son de los polos, los cartógrafos utilizan este tipo de proyección para ver los países y continentes.
Hay diversos tipos de proyecciones cónicas:

En este caso se proyecta una porción de la Tierra sobre un plano tangente al globo en un punto seleccionado, obteniéndose una imagen similar a la visión de la Tierra desde un punto interior o exterior. Si la proyección es del primer tipo se llama proyección gnomónica; si es del segundo, ortográfica. Estas proyecciones ofrecen una mayor distorsión cuanto mayor sea la distancia al punto tangencial de la esfera y el plano. Este tipo de proyección se relaciona principalmente con los polos y hemisferios.
Tipos de proyecciones:


En la actualidad la mayoría de los mapas se hacen con base en proyecciones modificadas o combinación de las anteriores, a veces, con varios puntos focales, a fin de corregir en lo posible las distorsiones en ciertas áreas seleccionadas, aun cuando se produzcan otras nuevas en lugares a los que se concede importancia secundaria, como son por lo general las grandes extensiones de mar. Entre las más usuales figuran la proyección policónica de Lambert utilizada para fines educativos, y los mapamundis elaborados según las proyecciones Winkel-Tripel (adoptada por la National Geographic Society) y Mollweide, que tienen forma de elipse y menores distorsiones.

Las proyecciones convencionales generalmente fueron creadas para representar el mundo entero (mapamundi) y dan la idea de mantener las propiedades métricas, buscando un balance entre distorsiones, o simplemente hacer que el mapamundi "se vea bien". La mayor parte de este tipo de proyecciones distorsiona las formas en las regiones polares más que en el ecuador:




</doc>
<doc id="2239" url="https://es.wikipedia.org/wiki?curid=2239" title="Premio Ig Nobel">
Premio Ig Nobel

Los Premios Ig Nobel son una parodia estadounidense del Premio Nobel. Se entregan cada año a principios de octubre para reconocer los logros de diez grupos de científicos que «primero hacen reír a la gente, y luego la hacen pensar». Organizado por la revista de humor científico "Annals of Improbable Research" (AIR), los premios son presentados por una serie de colaboradores que incluye a auténticos Premios Nobel, en una ceremonia organizada en el Sanders Theatre, de la Universidad de Harvard. «Los premios pretenden celebrar lo inusual, honrar lo imaginativo y estimular el interés de todos por la ciencia, la medicina, y la tecnología».

El nombre, dado que los premios fueron creados por el estadounidense Marc Abrahams, es un juego con la palabra en inglés "ignoble", que en castellano significa “innoble”. Con los años, los organizadores han dado muchas explicaciones satíricas sobre el origen del nombre de los premios, incluyendo la afirmación de que Ig Nobel era el nombre de la persona que inventó la «Soda Pop».

Los primeros Premios Ig Nobel fueron adjudicados en 1991, aunque en aquella época eran premiados descubrimientos «que no podían, o no debían, ser reproducidos». Diez premios se otorgan cada año en varias categorías, incluyendo las cinco categorías del Premio Nobel (física, química, fisiología/medicina, literatura y de la paz), y además otras categorías como la salud pública, la ingeniería, la biología y la investigación interdisciplinaria. Con la excepción de tres premios en el primer año, los Premios Ig Nobel son para verdaderos logros. 

Los premios son a veces críticas veladas, como en los dos premios otorgados por la investigación sobre la homeopatía y los premios para la educación científica concedido a «las juntas estatales de la educación de Kansas y Colorado por su postura en relación con la enseñanza de la evolución, ya que abogan por el creacionismo y porque esta sea una teoría equiparable al electromagnetismo o a los estudios de Newton sobre la gravedad».
Otro ejemplo es el premio otorgado a la Social Text después del escándalo Sokal. Muy a menudo, sin embargo, llaman la atención los artículos científicos que tienen algún aspecto humorístico o inesperado. Los ejemplos van desde el descubrimiento de que la presencia de los humanos tiende a excitar sexualmente a las avestruces, a la afirmación de que los agujeros negros cumplen todos los requisitos técnicos para ser la ubicación del Infierno. Otro ejemplo es la investigación sobre la «regla de los cinco segundos», la creencia de que la comida que cae al suelo no se contamina si se recoge antes de transcurridos cinco segundos.

Por último, el hecho de recibir un Ig Nobel nada tiene que ver con la capacidad investigadora o la excelencia científica, ya que Andréy Gueim, ganador del Ig Nobel de física en 2000 ("Of Flying Frogs and Levitrons"), ha sido a su vez galardonado con el premio Nobel de física en 2010, por sus trabajos sobre el grafeno.

Los premios son presentados por auténticos ganadores de premios Nobel. Inicialmente fue una ceremonia celebrada en una sala de conferencias en el Instituto Tecnológico de Massachusetts (MIT) pero ahora se realiza en el Sanders Theater de la Universidad de Harvard. Contiene una serie seguida de chistes, como Miss Sweety Poo, una niña que grita repetidamente «Por favor, terminen. Estoy aburrida» en una voz aguda si los galardonados hablan durante demasiado tiempo. La entrega de premios se cierra tradicionalmente con las palabras: «Si no has ganado un premio - y especialmente si lo has hecho - ¡mejor suerte el próximo año!» 

La ceremonia es co-patrocinada por la Harvard Computer Society, la Harvard-Radcliffe Science Fiction Association y The Harvard-Radcliffe Society of Physics Students.

Arrojar aviones de papel al escenario es una larga tradición en los Ig Nobel. Sin embargo, desde la ceremonia de 2006, este juego fue suspendido debido a motivos de seguridad. En los últimos años, el profesor de física Roy Glauber se había encargado de barrer el escenario para limpiarlo de aviones. Sin embargo, Glauber no pudo asistir al evento del 2005, ya que se dirigió a Estocolmo para recibir un verdadero Premio Nobel de Física. 

El «Desfile de "Ignitaries"» (juego de palabras con "dignitaries", «dignatarios») atrae a diversos grupos al evento. En las ceremonias de 1997, un equipo de «investigadores sexuales criogénicos» distribuyó un folleto titulado "Sexo Seguro a Cuatro Kelvin". Los delegados del Museo del Mal Arte, de Massachusetts, a menudo exhiben algunas piezas de su colección, que muestran que el arte malo y la mala ciencia van de la mano.

La ceremonia es registrada y transmitida en directo tanto por la Radio Nacional Pública de EE.UU. como a través de Internet. La grabación se emite todos los años, el viernes después de la Acción de Gracias en EE.UU., en el programa de radio "Ciencia Viernes" de ese país. En reconocimiento a esta labor, el público corea en repetidas ocasiones el nombre del presentador del programa de radio, Ira Flatow.

Se han publicado dos libros acerca de los premios: "El Premio Ig Nobel" (2002) y "El Premio Ig Nobel 2" (2005), que fue retitulado más adelante como "El hombre que trató de Clonarse".

Un Ig Nobel Tour ha viajado por Reino Unido y Australia varias veces.

En 1995, Robert May, barón May de Oxford y principal asesor científico del Gobierno británico, pidió que los organizadores del Premio Ig Nobel no concedieran premios a los científicos británicos, alegando que estos se veían desprestigiados o ridiculizados al recibirlos. Sin embargo, varios científicos del país mencionado saltaron a defender los premios, desestimando las declaraciones de May, y la revista británica "Chemistry and Industry", en particular, publicó un artículo para refutar sus argumentos.

En septiembre de 2009 un artículo en "The National" titulado "El lado noble de los Ig Nobel" dice que, aunque los Premios Ig Nobel son una crítica velada de la investigación trivial, la historia ha demostrado que las investigaciones triviales a veces conducen a descubrimientos importantes. 

Por ejemplo, en 2006 un estudio que muestra que el mosquito que transporta la malaria ("Anopheles gambiae") se siente atraído igualmente por el olor del queso Limburger que por el olor de los pies humanos había ganado el Premio Ig Nobel en el área de Biología. Como resultado directo de estos hallazgos, este tipo de queso se coloca en lugares estratégicos de naciones africanas para combatir la epidemia de la malaria. La importante contribución que de forma inadvertida hizo este estudio hacia la preservación de la vida humana pone de relieve la importancia de compartir los resultados experimentales, independientemente de los usos previstos de dichos resultados.

Otro ejemplo es Andre Geim, quien en el año 2000 recibió el Premio lg Nobel de Física por hacer levitar una rana en un campo magnético, y posteriormente en el año 2010 el Premio Nobel de Física junto con Konstantín Novosiolov por el estudio del grafeno.




</doc>
<doc id="2244" url="https://es.wikipedia.org/wiki?curid=2244" title="Phalaris">
Phalaris

Phalaris, es un género de plantas herbáceass perteneciente a la familia de las poáceas. Varias especies de "Phalaris" crecen en todos los continentes, excepto la Antártida. El más conocido es el alpiste.
Son plantas herbáceas que llegan a alcanzar los 180 cm de altura, con varios tallos huecos y cilíndricos, semejantes al del trigo. Las flores se producen en densos racimos verdes que tornan levemente púrpura al madurar. Las semillas son de color marrón envueltas en una pequeña cáscara.

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 2: 1050. 1753. La especie tipo es: "Phalaris canariensis" L.
Phalaris: nombre genérico que podría derivar del griego "phalaros", que significa lustroso, aludiendo al brillo de las espigas. 
Tiene un número de cromosomas de: x = 6 y 7. 2n = 12, 14, 28, 35, 42, y 56. 2, 4, 6, y 8 ploidias. Cromosomas ‘grandes’. 





</doc>
<doc id="2245" url="https://es.wikipedia.org/wiki?curid=2245" title="Phragmites">
Phragmites

Phragmites, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es un género cosmopolita. Comprende 87 especies descritas y de estas, solo 4 aceptadas. 
Son plantas perennes, rizomatosas pedunculadas, comprimidas lateralmente, con 3-8 flores; la inferior y la superior masculinas o estériles, las demás hermafroditas; raquilla con pelos sedosos blanquecinos. Glumas 2, membranosas, más cortas que las flores, muy desiguales, con 1-3 nervios, papirácea. Pálea mucho más corta que la lema, membranosa, con 2 quillas, ciliada en los márgenes. Hilo oblongo.
El género fue descrito por Michel Adanson y publicado en "Familles des Plantes" 2: 34, 559. 1763. La especie tipo es: "Arundo phragmites" L.
Phragma: nombre genérico que deriva del griego "phragma", en alusión a su presencia en las cercanías de las vías fluviales.
El número cromosómico básico es x = 12, con números cromosómicos somáticos de 2n = 36, 44, 46, 48, 49, 50, 51, 52, 54 y 96, con series poliploides. Los cromosomas son relativamente pequeños.

A continuación se brinda un listado de las especies del género "Phragmites" aceptadas hasta noviembre de 2013, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 




</doc>
<doc id="2246" url="https://es.wikipedia.org/wiki?curid=2246" title="Piptatherum">
Piptatherum

Piptatherum es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del Viejo Mundo subtropical y de Norteamérica.

Algunos autores lo incluyen en el género "Oryzopsis".
Son plantas perennes, a menudo cespitosas. Hojas con vaina de márgenes libres; lígula truncada o aguda, más o menos pubescente en el dorso; limbo plano o convoluto. Inflorescencia en panícula laxa, con ramas verticiliadas, lisas o escábridas. Espiguillas comprimidas dorsalmente, unifloras. Glumas más largas que la flor, poco desiguales, membranosas, acuminadas. Lema lanceolado-elíptica, con 3-5 nervios poco marcados, obtusa, endurecida en la madurez, glabra, rara vez laxamente pubescente, con 1 arista terminal. Pálea del tamaño de la lema, endurecida en la madurez. Cariopsis oblongoidea.
El género fue descrito por Ambroise Marie François Joseph Palisot de Beauvois y publicado en "Essai d'une Nouvelle Agrostographie" 17, 173. 1812. La especie tipo es: "Aegilops triuncialis"
Piptatherum: nombre genérico derivado del griego "pipto" = "caer", y la palabra "therum" para "gluma", lo que significa "glumas caídas".
Tiene un número de cromosomas de: x = 12. 2n = 24. 2 ploidias. Cromosomas de ‘tamaño medio’. 



</doc>
<doc id="2247" url="https://es.wikipedia.org/wiki?curid=2247" title="Prolog">
Prolog

Prolog (o PROLOG), proveniente del francés "PROgrammation en LOGique", es un lenguaje de programación lógico e interpretado usado habitualmente en el campo de la Inteligencia artificial.

Se trata de un lenguaje de programación ideado a principios de los años 70 en la Universidad de Aix-Marseille I (Marsella, Francia) por Alain Colmerauer y Philippe Roussel. Nació de un proyecto que no tenía como objetivo la traducción de un lenguaje de programación, sino el tratamiento algorítmico de lenguajes naturales. Alain Colmerauer y Robert Pasero trabajaban en la parte del procesado del lenguaje natural y Jean Trudel y Philippe Roussel en la parte de deducción e inferencia del sistema. Interesado por el método de resolución SL, Philippe Roussel persuadió a su autor, Robert Kowalsk,i para que collaborara al proyecto, dando lugar a una versión preliminar del lenguaje Prolog a finales de 1971 y apareciendo la versión definitiva en 1972. Esta primera versión de Prolog fue programada en ALGOL W.

Inicialmente se trataba de un lenguaje totalmente interpretado hasta que, en 1983, David H.D. Warren desarrolló un compilador capaz de traducir Prolog en un conjunto de instrucciones de una máquina abstracta denominada Warren Abstract Machine, o abreviadamente, "WAM". Desde entonces Prolog es un lenguaje semi-interpretado.

Si bien en un principio se trataba de un lenguaje de uso reducido, la aparición de intérpretes del mismo para microordenadores de 8 bits (ej: "micro-PROLOG") y para ordenadores domésticos de 16 bits (ej: "Turbo Prolog" de Borland, entre otros muchos) a lo largo de la década de 1980 contribuyó notablemente a su popularización. Otro importante factor en su difusión fue la adopción del mismo para el desarrollo del proyecto de la quinta generación de computadoras a principios de la década de los 80,en cuyo contexto se desarrolló la implementación paralelizada del lenguaje llamada KL1 y del que deriva parte del desarrollo moderno de Prolog.

Las primeras versiones del lenguaje diferían, en sus diferentes implementaciones, en muchos aspectos de sus sintaxis, empleándose mayormente como forma normalizada el dialecto propuesto por la Universidad de Edimburgo, hasta que en 1995 se estableció un estándar ISO (ISO/IEC 13211-1), llamado ISO-Prolog.

Prolog se enmarca en el paradigma de los lenguajes lógicos y declarativos, lo que lo diferencia enormemente de otros lenguajes más populares tales como Fortran, Pascal, C o Java.

En los lenguajes de programación antes mencionados, las instrucciones se ejecutan normalmente en orden secuencial, es decir, una a continuación de otra, en el mismo orden en que están escritas, que sólo varía cuando se alcanza una instrucción de control (un bucle, una instrucción condicional o una transferencia).

Los programas en Prolog se componen de "cláusulas de Horn" que constituyen reglas del tipo "modus ponendo ponens", es decir, "Si es verdad el "antecedente", entonces es verdad el "consecuente"". No obstante, la forma de escribir las cláusulas de Horn es al contrario de lo habitual. Primero se escribe el consecuente y luego el antecedente. El antecedente puede ser una conjunción de condiciones que se denomina "secuencia de objetivos". Cada "objetivo" se separa con una coma y puede considerarse similar a una instrucción o llamada a procedimiento de los lenguajes imperativos.
En Prolog no existen instrucciones de control. Su ejecución se basa en dos conceptos: la unificación y el backtracking.

Gracias a la unificación, cada "objetivo" determina un subconjunto de cláusulas susceptibles de ser ejecutadas. Cada una de ellas se denomina "punto de elección".
Prolog selecciona el primer punto de elección y sigue ejecutando el programa hasta determinar si el objetivo es verdadero o falso.

En caso de ser falso entra en juego el "backtracking", que consiste en deshacer todo lo ejecutado situando el programa en el mismo estado en el que estaba justo antes de llegar al punto de elección. Entonces se toma el siguiente punto de elección que estaba pendiente y se repite de nuevo el proceso. Todos los objetivos terminan su ejecución bien en "éxito" ("verdadero"), bien en "fracaso" ("falso").

Existen dos tipos de cláusulas: Hechos y Reglas. Una regla es del tipo:
Cabeza :- Cuerpo.
y se lee como "La cabeza es verdad si el cuerpo es verdad". El cuerpo de una regla consiste en llamadas a predicados, que son llamados los objetivos de las reglas. El predicado codice_1 (es decir, un operador de paridad 2 (que recibe 2 argumentos) y de nombre codice_2 ) denota conjunción de objetivos, y el operador codice_3 denota disyunción. Conjunciones y disyunciones pueden sólo aparecer en el cuerpo, no en la cabeza de la regla.
En realidad la disyunción no es un operador básico o predefinido, sino que está meta-programado así:
';' (A,_) :- A.

';' (_,B) :- B.
Las cláusulas sin cuerpo (es decir, antecedente) son llamados hechos porque siempre son ciertos. Un ejemplo de un hecho es:
gato(tom).
que es equivalente a la regla:
gato(tom) :- true.
El predicado predefinido codice_4 siempre es verdad.

Dado el hecho anterior, se puede preguntar:

"¿Es Tom un gato?"

?- gato(Tom). 
Yes
"¿Qué cosas son gatos?"
?- gato(X). 
X = Tom
Debido a la naturaleza relacional de muchos predicados, pueden ser usados revertidos sus argumentos. Por ejemplo, codice_5 puede ser usado para determinar el tamaño (longitud) de una lista: length([a,b,c], L), así como para generar un esqueleto de lista para un largo dado (length(X, 5)). Similarmente, codice_6 puede ser usado también para unir o anexar dos listas: append([a,b], [c,d], X), así como para dividir una lista en dos partes: append(X, Y, [a,b,c,d]). Todo depende de qué argumentos sean variables libres y cuáles sean instanciados. En analogía con la programación imperativa, las variables libres son argumentos de salida y el resto son argumentos de entrada. Pero en Prolog, a diferencia de los lenguajes imperativos, dicho rol es intercambiable en la mayoría de los predicados. Esta característica se denomina reversibilidad, y las combinaciones válidas de argumentos de salida o entrada se denomina modos de uso. Por ejemplo, el predicado length/2 es reversible y tiene tres modos de uso: los dos argumentos instanciados, el primer argumento instanciado pero el otro no, y viceversa. El modo de uso con los
dos argumentos sin instanciar no tiene mucho sentido, pero podría ser admitido según algunas implementaciones, en tal caso, generaría todas los esqueletos de lista de todas las longitudes posibles...

Por esta razón, una biblioteca relativamente pequeña de predicados basta para muchos programas en Prolog. Todos los predicados pueden también ser usados para realizar pruebas unitarias: las consultas pueden ser incrustados en programas y permitir pruebas automáticas de regresión en tiempo de compilación.

Como un lenguaje de propósito general, Prolog también posee varios predicados predefinidos para interacción con el sistema operativo, como entrada/salida, gráficos y comunicaciones de datos. Estos predicados no tienen un significado relacional y son sólo útiles por los efectos laterales que exhiben en el sistema. Por ejemplo, el predicado write/1 muestra un término en la pantalla, pero no tiene relevancia su valor de verdad o falsedad.

Prolog cuenta con operadores para la unificación y comparación, sea con evaluación o sea simbólica, como los siguientes:
?- X is 3+5.

?- X = 3+5.

?- 3+5 =:= 2+6.

?- 3+5 == 2+6.

?- 3+5 == 3+5.
La representación de hechos simples no es lo común en la clasificación de elementos, sino que se agrupan los elementos de un mismo tipo en una lista.

Las listas son colecciones de elementos en Prolog. Una lista se divide en dos partes:
Cabeza. Es el primer elemento de la lista.
Cola. Es una lista con el resto de los elementos de la lista.
La cabeza y la cola de una lista se separan con el símbolo "|".

%% declaraciones
padrede('Juan', 'María'). % Juan es padre de María
padrede('Pablo', 'Juan'). % Pablo es padre de Juan
padrede('Pablo', 'Marcela'). % Pablo es padre de Marcela
padrede('Carlos', 'Débora'). % Carlos es padre de Débora

% A es hijo de B si B es padre de A
hijode(A,B) :- padrede(B,A).
% A es abuelo de B si A es padre de C y C es padre B
abuelode(A,B) :- 
% A y B son hermanos si el padre de A es también el padre de B y si A y B no son lo mismo
hermanode(A,B) :- 

% A y B son familiares si A es padre de B o A es hijo de B o A es hermano de B
familiarde(A,B) :- 
familiarde(A,B) :-
familiarde(A,B) :- 
%% consultas
% ¿Juan es hermano de Marcela?
?- hermanode('Juan', 'Marcela').
yes

% ¿Carlos es hermano de Juan?
?- hermanode('Carlos', 'Juan').
no

% ¿Pablo es abuelo de María?
?- abuelode('Pablo', 'María').
yes

% ¿María es abuela de Pablo?
?- abuelode('María', 'Pablo').
no

% La sintaxis es factorial(N, F) -> Factorial de N es F (el resultado se guarda en F)
factorial(0, 1).
factorial(1, 1).
factorial(N, F) :- N>0, N1 is N - 1, factorial(N1, F1), F is N * F1.

%el factorial se llama recursivamente dejando el resultado en F 

% La sintaxis es fibonacci(N, F) -> Término N de la sucesión (el resultado se guarda en F).
fibonacci(0, 0) :-!.
fibonacci(1, 1) :-!.
fibonacci(N ,F) :-N1 is N - 1, fibonacci(N1, F1),N2 is N - 2, fibonacci(N2, F2), F is F1 + F2.

%el fibonacci se llama recursivamente dejando el resultado en F. 

plantas([manzana, naranja, limón, espinaca, gardenia, alfalfa, pino]). 

lista([1,2,3]).

?-lista([H|T]).

?-lista([H,J|T]).
% Si queremos hallar la longitud de una lista.
% La longitud de una lista vacía es 0.
% La longitud de cualquier lista es la longitud de la cola + 1.

longitud([],0).
longitud([_|T],N):-longitud(T,N0), N is N0 + 1.

?- longitud([a,b,c],L).
?- longitud([a,b,c],4).

% Si queremos determinar si un elemento pertenece a una lista.
% El elemento pertenece a la lista si coincide con la cabeza de la lista.
% El elemento pertenece a la lista si se encuentra en la cola de la lista.

pertenece(X,[X|_]) .
pertenece(X,[_|R]):- pertenece(X,R). 

?- pertenece(b,[a,b,c]).
?- pertenece(b,[a,[b,c]]).
?- pertenece([b,c],[a,[b,c]]).
?- pertenece(X,[a,b]).

% Si queremos eliminar un elemento de la lista.
% Si X es la cabeza de la lista, la cola T es la lista sin X
% Si X no es la cabeza de la lista, conservamos la cabeza de la lista 
% como parte de la respuesta y continuamos eliminando X de la cola T.
elimina(X,[X|T],T).
elimina(X,[H|T],[H|T1]):- elimina(X,T,T1).

?- elimina(1,[1,2,3,4],R).
?- elimina(1,R,[2,3]).
?- elimina(X,[1,2,3],Y).

% Si queremos concatenar dos listas lista. 
% Concatenar una lista vacía con L es L.
% Concatenar X|L1 con L2 es poner el primer 
% elemento de la primera lista (X) más la 
% concatenación del resto de la lista (L1) con L2

concatenar([],L,L).
concatenar([X|L1],L2,[X|L3]):-concatenar(L1,L2,L3).

?- concatenar([1,2],[3,4],R).
?- concatenar(X,Y,[1,2]).

% Si queremos calcular la inversa de una lista. 
% La inversa de una lista vacía es una lista vacía.
% La inversa de H|T es la inversa de T concatenada con H.

inversa([],[]).
inversa([H|T],L):- inversa(T,R), concatenar(R,[H],L).

?- inversa([a,b,c,d],[d,c,b,a]).
% Utilizando un parámetro acumulador.

inver(L1,L2):-inver(L1,L2,[]).

inver([],L,L).
inver([H|T],L,S):-inver(T,L,[H|S]).

?- inver([a,b,c,d],[d,c,b,a]).



</doc>
<doc id="2249" url="https://es.wikipedia.org/wiki?curid=2249" title="Programación estructurada">
Programación estructurada

La programación estructurada es un paradigma de programación orientado a mejorar la claridad, calidad y tiempo de desarrollo de un programa de computadora recurriendo únicamente a subrutinas y tres estructuras básicas: secuencia, selección ("if" y "switch") e iteración (bucles "for" y "while"); asimismo, se considera innecesario y contraproducente el uso de la instrucción de transferencia incondicional (GOTO), que podría conducir a "código espagueti", mucho más difícil de seguir y de mantener, y fuente de numerosos errores de programación.

Surgió en la década de 1960, particularmente del trabajo de Böhm y Jacopini, y un famoso escrito de 1968: «La sentencia goto, considerada perjudicial», de Edsger Dijkstra. Sus postulados se verían reforzados, a nivel teórico, por el teorema del programa estructurado y, a nivel práctico, por la aparición de lenguajes como ALGOL, dotado de estructuras de control consistentes y bien formadas. 

A finales de los años 1970 surgió una nueva forma de programar que no solamente permitía desarrollar programas fiables y eficientes, sino que además estos estaban escritos de manera que se facilitaba su comprensión en fases de mejora posteriores.

El teorema del programa estructurado, propuesto por Böhm-Jacopini, demuestra que todo programa puede escribirse utilizando únicamente las tres instrucciones de control siguientes:


Solamente con estas tres estructuras se pueden escribir todos los programas y aplicaciones posibles. Si bien los lenguajes de programación tienen un mayor repertorio de estructuras de control, estas pueden ser construidas mediante las tres básicas citadas.

El teorema del programa estructurado proporciona la base teórica de la programación estructurada. Señala que la combinación de las tres estructuras básicas, secuencia, selección e iteración, son suficientes para expresar cualquier función computable. Esta observación no se originó con el movimiento de la programación estructurada. Estas estructuras son suficientes para describir el ciclo de instrucción de una unidad central de procesamiento, así como el funcionamiento de una máquina de Turing. Por lo tanto, un procesador siempre está ejecutando un «programa estructurado» en este sentido, incluso si las instrucciones que lee de la memoria no son parte de un programa estructurado. Sin embargo, los autores usualmente acreditan el resultado a un documento escrito en 1966 por Böhm y Jacopini, posiblemente porque Dijkstra había citado este escrito. El teorema del programa estructurado no responde a cómo escribir y analizar un programa estructurado de manera útil. Estos temas fueron abordados durante la década de 1960 y principio de los años 1970, con importantes contribuciones de Dijkstra, Robert W. Floyd, Tony Hoarey y David Gries.

P. J. Plauger, uno de los primeros en adoptar la programación estructurada, describió su reacción con el teorema del programa estructurado:

Nosotros los conversos ondeamos esta interesante pizca de noticias bajo las narices de los recalcitrantes programadores de lenguaje ensamblador que mantuvieron trotando adelante retorcidos bits de lógica y diciendo, 'Te apuesto que no puedes estructurar esto'. Ni la prueba por Böhm y Jacopini, ni nuestros repetidos éxitos en escribir código estructurado, los llevaron un día antes de lo que estaban listos para convencerse.

Donald Knuth aceptó el principio de que los programas deben adaptarse con asertividad, pero no estaba de acuerdo (y aún está en desacuerdo) con la supresión de la sentencia GOTO. En su escrito de 1974 «Programación estructurada con sentencias Goto», dio ejemplos donde creía que un salto directo conduce a código más claro y más eficiente sin sacrificar demostratividad. Knuth propuso una restricción estructural más flexible: debe ser posible establecer un diagrama de flujo del programa con todas las bifurcaciones hacia adelante a la izquierda, todas las bifurcaciones hacia atrás a la derecha, y sin bifurcaciones que se crucen entre sí. Muchos de los expertos en teoría de grafos y compiladores han abogado por permitir solo grafos de flujo reducible.

Los teóricos de la programación estructurada se ganaron un aliado importante en la década de 1970 después de que el investigador de IBM Harlan Mills aplicara su interpretación de la teoría de la programación estructurada para el desarrollo de un sistema de indexación para el archivo de investigación del "New York Times". El proyecto fue un gran éxito de la ingeniería, y los directivos de otras empresas lo citaron en apoyo de la adopción de la programación estructurada, aunque Dijkstra criticó las maneras en que la interpretación de Mills difería de la obra publicada.

Habría que esperar a 1987 para que la cuestión de la programación estructurada llamara la atención de una revista de ciencia de la computación. Frank Rubin lo hizo en ese año, con el escrito: «¿“La sentencia GOTO considerada dañina” se considera dañina?». A este le siguieron numerosas objeciones, como una respuesta del propio Dijkstra que criticaba duramente a Rubin y las concesiones que otros autores hicieron cuando le respondieron.

A finales del siglo XX, casi todos los científicos están convencidos de que es útil aprender y aplicar los conceptos de programación estructurada. Los lenguajes de programación de alto nivel que originalmente carecían de estructuras de programación, como FORTRAN, COBOL y BASIC, ahora las tienen.

Entre las ventajas de la programación estructurada sobre el modelo anterior (hoy llamado despectivamente código espagueti), cabe citar las siguientes:


Si bien es posible desarrollar la programación estructurada en cualquier lenguaje de programación, resulta más idóneo un lenguaje de programación procedimental. Algunos de los lenguajes utilizados inicialmente para programación estructurada incluyen ALGOL, Pascal, PL/I y Ada, pero la mayoría de los nuevos lenguajes de programación procedimentales desde entonces han incluido características para fomentar la programación estructurada y a veces, deliberadamente, omiten características en un esfuerzo para hacer más difícil la programación no estructurada.

Con posterioridad a la programación estructurada se han creado nuevos paradigmas tales como la programación modular, la programación orientada a objetos, la programación por capas y otras, así como nuevos entornos de programación que facilitan la programación de grandes aplicaciones y sistemas.





</doc>
<doc id="2251" url="https://es.wikipedia.org/wiki?curid=2251" title="Programación orientada a objetos">
Programación orientada a objetos

La Programación Orientada a Objetos (POO, en español; OOP, según sus siglas en inglés) es un paradigma de programación que viene a innovar la forma de obtener resultados. Los objetos manipulan los datos de entrada para la obtención de datos de salida específicos, donde cada objeto ofrece una funcionalidad especial.

Muchos de los objetos prediseñados de los lenguajes de programación actuales permiten la agrupación en bibliotecas o librerías, sin embargo, muchos de estos lenguajes permiten al usuario la creación de sus propias bibliotecas.

Está basada en varias técnicas del sexenio: herencia, cohesión, abstracción, polimorfismo, acoplamiento y encapsulamiento.

Su uso se popularizó a principios de la década de 1990. En la actualidad, existe una gran variedad de lenguajes de programación que soportan la orientación a objetos.

Los objetos son entidades que tienen un determinado "estado", "comportamiento (método)" e "identidad":


Los métodos (comportamiento) y atributos (estado) están estrechamente relacionados por la propiedad de conjunto. Esta propiedad destaca que una clase requiere de métodos para poder tratar los atributos con los que cuenta. El programador debe pensar indistintamente en ambos conceptos, sin separar ni darle mayor importancia a alguno de ellos. Hacerlo podría producir el hábito erróneo de crear clases contenedoras de información por un lado y clases con métodos que manejen a las primeras por el otro. De esta manera se estaría realizando una "programación estructurada camuflada" en un lenguaje de POO.

La programación orientada a objetos difiere de la programación estructurada tradicional, en la que los datos y los procedimientos están separados y sin relación, ya que lo único que se busca es el procesamiento de unos datos de entrada para obtener otros de salida. La programación estructurada anima al programador a pensar sobre todo en términos de procedimientos o funciones, y en segundo lugar en las estructuras de datos que esos procedimientos manejan. En la programación estructurada solo se escriben funciones que procesan datos. Los programadores que emplean POO, en cambio, primero definen objetos para luego enviarles mensajes solicitándoles que realicen sus métodos por sí mismos.

Los conceptos de la POO tienen origen en Simula 67, un lenguaje diseñado para hacer simulaciones, creado por Ole-Johan Dahl y Kristen Nygaard, del Centro de Cómputo Noruego en Oslo. En este centro se trabajaba en simulaciones de naves, que fueron confundidas por la explosión combinatoria de cómo las diversas cualidades de diferentes naves podían afectar unas a las otras. La idea surgió al agrupar los diversos tipos de naves en diversas clases de objetos, siendo responsable cada clase de objetos de definir sus "propios" datos y comportamientos. Fueron refinados más tarde en Smalltalk, desarrollado en Simula en Xerox PARC (cuya primera versión fue escrita sobre Basic) pero diseñado para ser un sistema completamente dinámico en el cual los objetos se podrían crear y modificar "sobre la marcha" (en tiempo de ejecución) en lugar de tener un sistema basado en programas estáticos.

La POO se fue convirtiendo en el estilo de programación dominante a mediados de los años 1980, en gran parte debido a la influencia de C++, una extensión del lenguaje de programación C. Su dominación fue consolidada gracias al auge de las interfaces gráficas de usuario, para las cuales la POO está particularmente bien adaptada. En este caso, se habla también de programación dirigida por eventos.

Las características de orientación a objetos fueron agregadas a muchos lenguajes existentes durante ese tiempo, incluyendo Ada, BASIC, Lisp más Pascal, entre otros. La adición de estas características a los lenguajes que no fueron diseñados inicialmente para ellas condujo a menudo a problemas de compatibilidad y en la capacidad de mantenimiento del código. Los lenguajes orientados a objetos "puros", por su parte, carecían de las características de las cuales muchos programadores habían venido a depender. Para saltar este obstáculo, se hicieron muchas tentativas para crear nuevos lenguajes basados en métodos orientados a objetos, pero permitiendo algunas características imperativas de maneras "seguras". El lenguaje de programación Eiffel de Bertrand Meyer fue un temprano y moderadamente acertado lenguaje con esos objetivos, pero ahora ha sido esencialmente reemplazado por Java, en gran parte debido a la aparición de Internet y a la implementación de la máquina virtual Java en la mayoría de navegadores web. PHP en su versión 5 se ha modificado; soporta una orientación completa a objetos, cumpliendo todas las características propias de la orientación a objetos.

La POO es una forma de programar que trata de encontrar una solución a estos problemas. Introduce nuevos conceptos, que superan y amplían conceptos antiguos ya conocidos. Entre ellos destacan los siguientes:



En comparación con un lenguaje imperativo, una "variable" no es más que un contenedor interno del atributo del objeto o de un estado interno, así como la "función" es un procedimiento interno del método del objeto.

Existe un acuerdo acerca de qué características contempla la "orientación a objetos". Las características siguientes son las más importantes:
El paradigma POO ha sido criticado por varias razones, incluyendo no cumplir con las metas de reusabilidad y modularidad, y por sobreenfatizar un aspecto de diseño y modelación de software (datos/objetos) a expensas de otros aspectos importantes (computación/algoritmos).

La POO es un paradigma surgido en los años 1970, que utiliza objetos como elementos fundamentales en la construcción de la solución. Un objeto es una abstracción de algún hecho o ente del mundo real, con atributos que representan sus características o propiedades, y métodos que emulan su comportamiento o actividad. Todas las propiedades y métodos comunes a los objetos se encapsulan o agrupan en clases. Una clase es una plantilla, un prototipo para crear objetos; en general, se dice que cada objeto es una instancia o ejemplar de una clase.

Para realizar programación orientada a objetos existen dos corrientes principales:



Simula (1967) es aceptado como el primer lenguaje que posee las características principales de un lenguaje orientado a objetos. Fue creado para hacer programas de simulación, en donde los "objetos" son la representación de la información más importante.

Smalltalk (1972 a 1980) es posiblemente el ejemplo canónico, y con el que gran parte de la teoría de la programación orientada a objetos se ha desarrollado.

Entre los lenguajes orientados a objetos se destacan los siguientes:
Muchos de estos lenguajes de programación no son puramente orientados a objetos, sino que son híbridos que combinan la POO con otros paradigmas.

Al igual que C++, otros lenguajes, como OOCOBOL, OOLisp, OOProlog y Object REXX, han sido creados añadiendo extensiones orientadas a objetos a un lenguaje de programación clásico.

Un nuevo paso en la abstracción de paradigmas de programación es la Programación Orientada a Aspectos (POA). Aunque es todavía una metodología en estado de maduración, cada vez atrae a más investigadores e incluso proyectos comerciales en todo el mundo.




</doc>
<doc id="2254" url="https://es.wikipedia.org/wiki?curid=2254" title="Paridad">
Paridad

El término paridad hace referencia a los siguientes conceptos: 






</doc>
<doc id="2255" url="https://es.wikipedia.org/wiki?curid=2255" title="Paridad horizontal y vertical">
Paridad horizontal y vertical

La paridad horizontal y vertical es utilizada en algunos códigos de bloque para una combinación de chequeo de (LRC / VRC) para detectar errores. El LRC: "Longitudinal Redundancy Checking" ("Chequeo de Redundancia Horizontal") y el VRC: "Vertical Redundancy Checking" ("Chequeo de Redundancia Vertical").

El proceso para calcular la paridad de bloque es el siguiente:


Los chequeos de paridad horizontal y vertical se usan para detectar y corregir los posibles errores que se puedan producir durante la transmisión de datos.

A continuación se muestra un ejemplo en el que se chequea la paridad de un bloque de 48 bits, distribuido en 6 filas de 8 bits cada una. Se usa paridad par.


</doc>
<doc id="2256" url="https://es.wikipedia.org/wiki?curid=2256" title="Prueba de Turing">
Prueba de Turing

La prueba de Turing o test de Turing es un examen de la capacidad de una máquina para exhibir un comportamiento inteligente similar al de un ser humano o indistinguible de este. Alan Turing propuso que un humano evaluara conversaciones en lenguaje natural entre un humano y una máquina diseñada para generar respuestas similares a las de un humano. El evaluador sabría que uno de los participantes de la conversación es una máquina y los intervinientes serían separados unos de otros. La conversación estaría limitada a un medio únicamente textual como un teclado de computadora y un monitor por lo que sería irrelevante la capacidad de la máquina de transformar texto en habla. En el caso de que el evaluador no pueda distinguir entre el humano y la máquina acertadamente (Turing originalmente sugirió que la máquina debía convencer a un evaluador, después de 5 minutos de conversación, el 70 % del tiempo), la máquina habría pasado la prueba. Esta prueba no evalúa el conocimiento de la máquina en cuanto a su capacidad de responder preguntas correctamente, solo se toma en cuenta la capacidad de esta de generar respuestas similares a las que daría un humano.

Turing propuso esta prueba en su ensayo “Computing Machinery and Intelligence” de 1950 mientras trabajaba en la Universidad de Mánchester (Turing, 1950; p. 460). Inicia con las palabras: “Propongo que se considere la siguiente pregunta, ‘¿Pueden pensar las máquinas?’”. Como es difícil definir la palabra “pensar”, Turing decide “reemplazar la pregunta con otra que está estrechamente relacionada y en palabras no ambiguas.”, la nueva pregunta de Turing es: “¿Existirán computadoras digitales imaginables que tengan un buen desempeño en el juego de imitación?". Turing creía que esta pregunta sí era posible de responder y en lo que resta de su ensayo se dedica a argumentar en contra de las objeciones principales a la idea de que “las máquinas pueden pensar”.

Desde que fue creada por Turing en 1950, la prueba ha demostrado ser altamente influyente y a la vez ampliamente criticada, además de transformarse en un concepto importante en la filosofía de la inteligencia artificial.

La incógnita sobre la capacidad de las máquinas de pensar tiene una larga historia, esta se divide entre las perspectivas materialista y dualista de la mente. René Descartes tuvo ideas similares a la prueba de Turing en su texto Discurso del Método (1637) donde escribió que los autómatas son capaces de reaccionar ante interacciones humanas pero argumenta que tal autómata carece de la capacidad de responder de manera adecuada ante lo que se diga en su presencia de la misma manera que un humano podría. Por lo tanto, Descartes abre las puertas para la prueba de Turing al identificar la insuficiencia de una respuesta lingüística apropiada lo que separa al humano del autómata. Descartes no llega a considerar que una respuesta lingüística apropiada puede ser producida por un autómata del futuro y por lo tanto no propone el test de Turing como tal aunque ya razonó los criterios y el marco conceptual.
Denis Diderot plantea en su "Pensées philosophiques" un criterio de la prueba de Turing:

“Si se encuentra un loro que puede responder a todo, se le consideraría un ser inteligente sin duda alguna."

Esto no significa que él esté de acuerdo pero muestra que ya era un argumento común usado por los materialistas de la época.

Según el dualismo, la mente no tiene un estado físico (o al menos cuenta con propiedades no físicas) y por lo tanto no se puede explicar en términos estrictamente físicos. De acuerdo con el materialismo, la mente puede ser explicada físicamente lo que abre la posibilidad a la creación de mentes artificiales.

En 1936, el filósofo Alfred Ayer consideraba la pregunta filosófica típica sobre otras mentes: ¿Cómo sabemos que otras personas experimentan el mismo nivel de conciencia que nosotros? En su libro "Lenguaje, Verdad y Lógica", Ayer propuso un método para distinguir entre un hombre consciente y una máquina inconsciente: “El único argumento que tengo para asegurar que lo que parece ser consciente no es un ser consciente sino un muñeco o una máquina, es el hecho de que falle en las pruebas empíricas por medio de las cuales se determina la presencia o ausencia de la conciencia" (Es una propuesta muy similar a la prueba de Turing pero esta se enfoca en la conciencia en vez de en la inteligencia). Además se desconoce si Turing estaba familiarizado con el clásico filosófico de Ayer. En otras palabras, algo no está consciente si reprueba una prueba de consciencia.

La “inteligencia maquinaria” ha sido un tema que investigadores del Reino Unido han seguido desde 10 años antes de que se fundara el campo de investigación de la inteligencia artificial (IA) en 1956. Era un tema comúnmente discutido por los miembros del “Club de la razón”, grupo informal de investigadores cibernéticos y electrónicos británicos que incluía a Alan Turing.

Turing, en particular, había estado trabajando con el concepto de la inteligencia maquinaria desde al menos 1941, una de las primeras menciones de la “inteligencia computacional” fue hecha por Turing en 1947. En el reporte de Turing llamado “maquinaria inteligente”, él investigó “la idea de si era, o no, posible para una máquina demostrar un comportamiento inteligente" y como parte de su investigación, propuso lo que se puede considerar como un predecesor de sus pruebas futuras:
El primer texto publicado escrito por Turing y enfocado completamente en la inteligencia de las máquinas fue “Computing Machinery and Intelligence”. Turing inicia este texto diciendo “Me propongo tomar en cuenta la pregunta ‘¿Pueden pensar las máquinas?’”. Turing menciona que el acercamiento tradicional es empezar con definiciones de los términos “máquina” e “inteligencia”, decide ignorar esto y empieza reemplazando la pregunta con una nueva, “que está estrechamente relacionada y en palabras no ambiguas”. Él propone, en esencia, cambiar la pregunta de “¿pueden las máquinas pensar?” a “¿Pueden las máquinas hacer, lo que nosotros (como entidades pensantes) hacemos?”. La ventaja de esta nueva pregunta es que “dibuja un límite entre las capacidades físicas e intelectuales del hombre."

Para demostrar este acercamiento, Turing propone una prueba inspirada en el “Juego de imitación”, en este entraban un hombre y una mujer a cuartos separados y el resto de los jugadores intentaría distinguir entre cada uno por medio de preguntas y leyendo las respuestas (escritas a máquina) en voz alta. El objetivo del juego es que los participantes que se encuentran en los cuartos deben convencer al resto que son el otro. (Huma Shah argumenta que Turing incluye la explicación de este juego para introducir al lector a la prueba de pregunta y respuesta entre humano y máquina) Turing describe su versión del juego de la siguiente manera:

Nos hacemos la pregunta, “¿Qué pasaría si una máquina toma el papel de A en este juego?” ¿Se equivocaría tan frecuentemente el interrogador en esta nueva versión del juego que cuando era jugado por un hombre y una mujer? Estas preguntas sustituyen la pregunta original “¿Pueden pensar las máquinas?”.

Más adelante en el texto se propone una versión similar en la que un juez conversa con una computadora y un hombre. A pesar de que ninguna de las versiones propuestas es la misma que conocemos hoy en día, Turing propuso una tercera opción, la cual discutió en una transmisión de radio de la BBC, donde un jurado le hace preguntas a una computadora y el objetivo de la computadora es engañar a la mayoría del jurado haciéndolo creer que es un humano.

El texto de Turing consideraba nueve objeciones putativas las cuales incluyen a todos los argumentos mayores, en contra de la inteligencia artificial, que habían surgido en los años posteriores a la publicación de su texto (ver “Computing Machinery and Intelligence”).

En 1966, Joseph Weizenbaum creó un programa que aseguraba pasar la prueba de Turing. Este programa era conocido como ELIZA y funcionaba a través del análisis de las palabras escritas por el usuario en busca de palabras clave. En el caso de encontrar una palabra clave, una regla que transformaba el comentario del usuario entra en acción y se regresaba una oración resultado. Si no se encontraba alguna palabra clave, ELIZA daba una respuesta genérica o repetía uno de los comentarios anteriores. Además, Weizenbaum desarrolló a ELIZA para replicar el comportamiento de un psicoterapeuta Rogeriano lo que permitía a ELIZA "asumir el rol de alguien que no conoce nada del mundo real”. El programa fue capaz de engañar a algunas personas haciéndolas creer que hablaban con una persona real e incluso algunos sujetos fueron “muy difíciles de convencerles de que ELIZA no era humana”. Como resultado, ELIZA es aclamado como uno de los programas (probablemente el primero) en pasar la prueba de Turing, aunque esto es muy controvertido (ver más adelante).

PARRY fue creado por Kenneth Colby en 1972. Era un programa descrito como “ELIZA con carácter”. Este intentaba simular el comportamiento de un esquizofrénico paranoico usando un acercamiento similar (probablemente más avanzado) al de Weizenbaum. PARRY fue examinado usando una variación de la prueba de Turing con tal de validar el trabajo. Un grupo de psiquiatras experimentados analizaba a un grupo de pacientes reales y computadoras ejecutando el programa PARRY a través de teletipos. A otro grupo de 33 psiquiatras se les enseñaban transcripciones de las conversaciones. A ambos grupos se les pedía indicar qué pacientes eran humanos y cuáles eran computadoras. Los psiquiatras fueron capaces de responder correctamente solo 48% de las veces, un valor consistente con respuestas aleatorias.

En el siglo XXI, versiones de estos programas (llamados “bots conversacionales”) siguieron engañando a la gente. “CyberLover”, un programa de malware, acechaba a usuarios convenciéndolos de “revelar información sobre sus identidades o de entrar a un sitio web que introduciría malware a sus equipos”. El programa surgió como un “riesgo de San Valentín”, coqueteando con la gente “buscando relaciones en línea para recabar información personal”.

El texto “Minds, Brains, and Programs” de 1980 escrito por John Searle, proponía el experimento de la “habitación china” y argumentaba que la prueba de Turing no podía usarse para determinar si una máquina podía pensar. Searle observó que software (como ELIZA) podía aprobar la prueba de Turing a través de la manipulación de caracteres que no había entendido. Sin la comprensión no se les puede clasificar realmente como “pensantes” de la misma manera que los humanos, por lo tanto, Searle concluyó que la prueba de Turing no puede probar que una máquina puede pensar. Al igual que la prueba de Turing, el argumento de Searle ha sido ampliamente criticado al igual que respaldado.

Argumentos como los de Searle en la filosofía de mente desataron un debate más intenso sobre la naturaleza de la inteligencia, la posibilidad de máquinas inteligentes y el valor de la prueba de Turing que continuó durante las décadas de los 80s y 90s. El filósofo fisicalista William Lycan reconoció el avance de las inteligencias artificiales, comenzando a comportarse como si tuvieran mentes. Lycan usa el experimento mental de un robot humanoide llamado Harry que puede conversar, jugar golf, tocar la viola, escribir poesía y por consiguiente consigue engañar a la gente como si fuera una persona con mente. Si Harry fuera humano, sería perfectamente natural pensar que tiene pensamientos o sentimientos, lo que sugeriría que en realidad Harry pueda tener pensamientos o sentimientos aún si es un robot. Para Lycan "no hay ningún problema ni objeción a la experiencia cualitativa en máquinas que no es igualmente un dilema para tal experiencia en humanos" (ver Problema de otras mentes).

El Premio Loebner proporciona una plataforma anual para pruebas de Turing prácticas, siendo la primera competición en noviembre de 1991. Creada por Hugh Loebner, el Centro de Estudios Conductuales de Cambridge en Massachusetts, Estados Unidos, organizó los premios hasta el año 2003 incluido. Loebner dijo que un motivo por el que se creó la competición era para avanzar el estado de la investigación de IA, al menos en parte, ya que no se había intentado implementar la prueba de Turing después de 40 años de discusión.

La primera competición del Premio Loebner en 1991, llevó a una discusión renovada sobre la viabilidad de la prueba de Turing y el valor de continuar persiguiéndola en la prensa y en la academia. El primer concurso lo ganó un programa inconsciente sin ninguna inteligencia identificable que logró engañar a interrogadores ingenuos. Esto sacó a la luz muchas de las deficiencias de la prueba de Turing (discutido más adelante): El programa ganó gracias a que fue capaz de “imitar errores humanos al escribir”; ”, los interrogadores poco sofisticados fueron fácilmente engañados y algunos investigados de IA sintieron que la prueba es una distracción de investigaciones más fructíferas.

Los premios de plata (únicamente textual) y de oro (visual y aural) no han sido ganados a la fecha, sin embargo, la competición ha entregado la medalla de bronce cada año al sistema computacional que, según la opinión de los jueces, demuestra el comportamiento conversacional “más humano” entre los otros participantes. A.L.I.C.E. (Artificial Linguistic Internet Computer Entitity) ha ganado el premio de bronce en 3 ocasiones recientes (2000, 2001 y 2004). La IA aprendiz Jabberwacky ganó en el año 2005 y nuevamente en el 2006.

El Premio Loebner evalúa la inteligencia conversacional, los ganadores son, típicamente, bots conversacionales. Las reglas de las primeras instancias de la competición, restringían las conversaciones: cada programa participante y un humano escondido conversaban de un solo tema y los interrogadores estaban limitados a una sola pregunta por cada interacción con la entidad. Las conversaciones restringidas fueron eliminadas en la competición de 1995. La interacción entre el evaluador y la entidad ha variado en las diferentes instancias del Premio Loebner. En el Premio Loebner de 2003 en la Universidad de Surrey, cada interrogador tenía permitido interactuar con la entidad por 5 minutos fuera humana o máquina. Entre 2004 y 2007, la interacción permitida era de más de 20 minutos. En 2008, la interacción permitida era de 5 minutos por par porque, el organizador Kevin Warwick, y el coordinador Huma Shah, consideraban que esta debía ser la duración de cualquier prueba por como Turing lo puso en su texto de 1950: “… la correcta identificación después de 5 minutos de cuestionamientos.”. Ellos sentían que las pruebas largas implementadas anteriormente eran inapropiadas para el estado de las tecnologías conversacionales artificiales. Es irónico que la competición del 2008 fue ganada por Elbot de Artificial Solutions, esta no simulaba una personalidad humana sino la de un robot y aun así logró engañar a tres jueces humanos de que era el humano actuando como robot.

Durante la competición del 2009. Con sede en Brighton, Reino Unido, la interacción permitida era de 10 minutos por ronda, 5 minutos para hablar con la máquina y 5 minutos para hablar con el humano. Se implementó de esta manera para probar la lectura alternativa sobre la predicción de Turing que la interacción de 5 minutos debía ser con la computadora. Para la competición del 2010, el patrocinador aumentó el tiempo de interacción entre interrogador y sistema a 25 minutos.

El 7 de junio de 2014 se realizó una competición de la prueba de Turing organizada por Kevin Warwick y Huma Shah para el 60 aniversario de la muerte de Turing y fue llevado a cabo en la Real Sociedad de Londres. Fue ganada por el robot conversacional ruso Eugene Goostman. El robot, durante una serie de conversaciones de 5 minutos de duración, logró convencer al 33% de los jueces del concurso de que era humano. John Sharkley se incluía entre los jueces, patrocinador del proyecto de ley que promueve el perdón gubernamental a Turing, al profesor de IA Aaron Sloman y Robert Llewellyn, actor de Enano Rojo.

Los organizadores de la competición creyeron que la prueba había sido “aprobada por primera vez” en el evento diciendo: “algunos dirán que la prueba ya ha sido pasada. Las palabras ‘Prueba de Turing’ han sido aplicadas a competiciones similares alrededor del mundo. Sin embargo, este evento involucra más pruebas simultáneas de comparación al mismo tiempo como nunca antes visto, fue independientemente verificado y, crucialmente, no se restringieron las conversaciones. Una prueba de Turing verdades no establece las preguntas ni los temas de las conversaciones.”

La competición ha enfrentado críticas, primero, solo un tercio de los jueces fue engañado por la computadora. Segundo, el personaje de computadora aparentaba ser una niña ucraniana de 13 años de edad que aprendió inglés como segundo lenguaje. El premio requería que un 30% de los jueces fuera engañado lo que concuerda con el texto de Turing "Computing Machinery and Intelligence". Joshua Tenenbaum, experto en psicología matemática en el MIT, calificó el resultado como nada impresionante.

Saul Traigner argumenta que hay al menos 3 versiones primarias de la prueba de Turing, de las cuales dos son propuestas en “"Computing Machinery and Intelligenc"e” y otra que el describe como “la interpretación estándar”. A pesar de que hay controversia en cuanto a si esta “interpretación estándar” fue descrita por Turing o si está basada en la mala interpretación del texto, estas tres versiones no se clasifican como equivalentes y sus fortalezas y debilidades son diferentes.

Huma Shah señala el hecho de que el mismo Turing estaba consternado con la posibilidad de que una máquina pudiera pensar y estaba proporcionando un método simple para examinar esto a través de sesiones de pregunta y respuesta entre humano y máquina. Shah argumenta que existe un juego de imitación que Turing pudo haber puesto en práctica de dos maneras diferentes: a) una prueba uno a uno entre el interrogador y la máquina o b) una comparación simultánea entre un humano y una máquina interrogados paralelamente por un mismo interrogador.Debido a que la prueba de Turing evalúa la indistinguibilidad en su capacidad de desempeño, la versión verbal naturalmente generaliza a toda la capacidad humana, verbal y no verbal (robótica).

El juego original descrito por Turing proponía un juego de fiesta que involucraba a tres jugadores. El jugador A es un hombre, el jugador B es una mujer y el jugador C (quien tiene el rol de interrogador) es de cualquier sexo. En el juego, el jugador C no tiene contacto visual con ninguno de los otros jugadores y se puede comunicar con ellos por medio de notas escritas. Al hacerles preguntas a los jugadores, el jugador C intenta determinar cuál de los dos es el hombre y cual la mujer. El jugador A intentará engañar al interrogador haciéndole escoger erróneamente mientras que el jugador B le auxiliará al interrogador en escoger al jugador correcto.

Sterret se refiere a este juego como “La Prueba del Juego Original De La Imitación”. Turing propuso que el rol del jugador A lo cumpliera una computadora para que esta tuviera que pretender ser mujer e intentara guiar al interrogador a la respuesta incorrecta. El éxito de la computadora seria determinado al comparar el resultado del juego cuando el jugador A es la computadora junto con el resultado del juego cuando el jugador A es un hombre. Turing afirmó que si “el interrogador decide erróneamente tan frecuentemente cuando el juego es jugado [con la computadora] como cuando el juego es jugado entre un hombre y una mujer”, se podrá argumentar que la computadora es inteligente.

La segunda versión apareció posteriormente en el texto de 1950 de Turing. Similarmente a la Prueba del Juego Original de la Imitación, el papel del jugador A seria realizado por una computadora. Sin embargo, el papel del jugador B sería realizado por un hombre y no una mujer.

“Dirijamos nuestra atención a una computadora digital en específico llamada C. ¿Será verdad que al modificar la computadora para que esta tenga un almacenamiento que aumentara su velocidad de reacción apropiadamente y proporcionándole un programa apropiado, C podrá realizar satisfactoriamente el rol de A en el juego de la imitación con el papel de B hecho por un hombre?”

En esta versión ambos, el jugador A (la computadora) y el jugador B intentarán guiar al interrogador hacia la respuesta incorrecta.

La comprensión general dicta que el propósito de la prueba de Turing no es determinar específicamente si una computadora podrá engañar al interrogador haciéndole creer que este es un humano sino su capacidad de "imitar" al humano. Aunque hay cierta disputa sobre cual interpretación es a la que Turing se refería, Sterret cree que era esta y por lo tanto combina la segunda versión con esta mientras que otros, como Traiger, no lo hacen sin embargo, esto no ha llevado a una interpretación estándar realmente. En esta versión el jugador A es una computadora y el sexo del jugador B es indiferente. El objetivo del interrogador no es determinar cuál de ellos es hombre y cual mujer sino cual es computadora y cual humano. El problema fundamental con la interpretación estándar es que el interrogador no puede diferenciar cual respondedor es humano y cual es una máquina. Hay otros problemas en cuanto a la duración pero la interpretación estándar generalmente considera esta limitación como algo que debería ser razonable.

Se ha creado controversia sobre cuál de las fórmulas alternativas es la que Turing planteo. Sterret argumenta que se pueden obtener dos pruebas de la lectura del texto de Turing de 1950 y que, según Turing, no son equivalentes. Se le refiere a la prueba que consiste del juego de fiesta y compara frecuencias de éxito como la “Prueba del Juego de la Imitación Original” mientras que a la prueba que consiste de un juez humano conversando con un humano y una máquina se le conoce “Prueba Estándar de Turing”, nótese que Sterret trata por igual a esta con la “interpretación estándar” en vez de tratarle como la segunda versión del juego de la imitación. Sterret concuerda que la Prueba Estándar de Turing (PET) tiene los problemas que sus críticas citan pero siente que, en contraste, la Prueba del Juego de la Imitación Original (Prueba JIO) mostró ser inmune a muchas de estas debido a una diferencia crucial: A diferencia de la PET, esta no hace similitud con el desempeño humano, aunque emplea al desempeño humano al definir criterio para la inteligencia de la máquina. Un humano puede reprobar la Prueba de JIO pero se argumenta que esta prueba es de inteligencia y el hecho de reprobar solo indica falta de creatividad. La Prueba JIO requiere la creatividad asociada con la inteligencia y no la sola “simulación del comportamiento conversacional humano. La estructura general de la Prueba JIO puede ser usada con versiones no verbales de juegos de imitación.

Otros escritores continúan interpretando la propuesta de Turing como el mismo juego de la imitación sin especificar como tomar en cuenta la declaración de Turing de que la prueba que el propuso usando la versión festiva del juego de la imitación está basada en un criterio de comparación de frecuencias de éxito en el juego en vez de la capacidad de ganar en tan solo una ronda.

Saygin sugirió que, probablemente, el juego original es una manera de proponer un diseño experimental menos parcial ya que esconde la participación de la computadora. El juego de la imitación incluye un “truco social” no encontrado en la interpretación estándar, ya que en el juego a ambos, la computadora y el hombre, se le requiere que pretendan ser alguien que no son.

Una pieza vital en cualquier prueba de laboratorio es la existencia de un control. Turing nunca aclara si el interrogador en sus pruebas está al tanto de que uno de los participantes es una computadora. Sin embargo, si hubiera una máquina que tuviera el potencial de pasar la prueba de Turing, sería mejor asumir que un control doble ciego es necesario.

Regresando al Juego Original de la Imitación, Turing establece que solo el jugador A será reemplazado con una máquina, no que el jugador C esté al tanto de este cambio. Cuando Colby, FD Hilf, S Weber y AD Kramer examinaron a PARRY, lo hicieron asumiendo que los interrogadores no necesitaban saber que uno o más de los interrogados era una computadora. Como Ayse Saygin, Peter Swirski y otros han resaltado, esto hace una gran diferencia en la implementación y el resultado de la prueba Un estudio experimental examinando violaciones a las máximas conversacionales de Grice usando transcripciones de los ganadores de la prueba (interlocutor escondido e interrogador) uno a uno entre 1994 y 1999, Ayse Saygin observó diferencias significativas entre las respuestas de los participante que sabían que había computadoras involucradas y los que no.

Huma Shah y Kevin Warwick, quienes organizaron el Premio Loebner del 2008 en la Universidad de Reading la cual fue sede de pruebas de comparativas simultáneas (un juez y dos interlocutores escondidos), demostraron que el conocimiento sobre los interlocutores no creó una diferencia significativa en la determinación de los jueces. A estos jueces no se les dijo explícitamente la naturaleza de las parejas de interlocutores escondidos que iban a interrogar. Los jueces fueron capaces de diferenciar entre humano y máquina, inclusivamente cuando estaban enfrentándose a parejas de control de dos máquinas o dos humanos infiltrados entre las parejas de máquina y humano. Los errores de ortografía delataban a los humanos escondidos, las máquinas eran identificadas por su velocidad de respuesta y expresiones de mayor tamaño.

El poder y atractivo de la prueba de Turing se deriva de su simplicidad. La filosofía de la mente, la psicología y la neurociencia moderna han sido incapaces de proporcionar definiciones para “inteligencia” y “pensamiento” que sean suficientemente precisas y generales como para ser aplicadas a máquinas. Sin estas definiciones, las incógnitas principales de la filosofía de la inteligencia artificial no pueden ser respondidas. La prueba de Turing, aunque imperfecta, al menos proporciona algo que puede ser medido y como tal, es una solución pragmática a una difícil pregunta filosófica.

El formato de la prueba permite al interrogador darle una gran variedad de tareas intelectuales a la máquina. Turing escribió que: “Los métodos de pregunta y respuesta parecen ser adecuados para introducir cualquiera de los campos de labor humana que queramos incluir.”. John Haugeland agregó: “la comprensión de las palabras no es suficiente, también se tiene que entender el tema.”.

Para aprobar una prueba de Turing diseñada correctamente, la máquina debe usar lenguaje natural, razón, tener conocimientos y aprender. La prueba puede ser extendida para incluir video como fuente de información junto con una “escotilla” por la que se pueden transferir objetos, esto forzaría a la máquina a probar su habilidad de visión y de robótica al mismo tiempo. En conjunto representan casi todos los problemas que la investigación de inteligencia artificial quisiera resolver.

La prueba de Feigenbaum está diseñada para usar a su ventaja el rango de temas disponibles para una prueba de Turing. Es una forma limitada del juego de respuesta y pregunta de Turing el cual compara a la máquina contra la habilidad de expertos en campos específicos como la literatura y la química. La máquina Watson de IBM alcanzó el éxito en un show de televisión que hacia preguntas de conocimiento humano a concursantes humanos y a la máquina por igual y al mismo tiempo llamado Jeopardy!.

Siendo un graduado de matemáticas con honores de Cambridge, se esperaba que Turing propusiera una prueba de inteligencia computacional que requiriera conocimiento experto de algún campo altamente técnico y como resultado necesitaría anticipar un acercamiento diferente. En vez de eso, la prueba que describió en su texto de 1950 solo requiere que la computadora sea capaz de competir exitosamente en un juego de fiestas común, con esto se refiere a que pueda compararse su desempeño con el de un humano típico al responder series de preguntas para aparentar ser la participante femenina.

Dado el estatus de dimorfismo sexual humano como uno de los temas más antiguos, se tiene implícito, en el escenario anterior, que las preguntas realizadas no pueden involucrar conocimientos factuales especializados ni técnicos de procesamiento de información. El reto para la computadora será exhibir empatía por el papel de la mujer al igual que demostrar una característica de sensibilidad estética, cualidades las cuales se muestran en este extracto imaginado por Turing:

Cuando Turing introduce algo de conocimiento especializado a sus diálogos imaginarios, el tema no es matemáticas ni electrónica sino poesía:

Turing, nuevamente, demuestra su interés en la empatía y en la sensibilidad estética como componente de la inteligencia artificial y en luz de una preocupación creciente de una IA en descontrol, se ha sugeridoque este enfoque puede representar una intuición crítica por parte de Turing, i.e. que la inteligencia y estética jugarán un rol clave en la creación de una “IA amigable”. Sin embargo, se ha observado que cualquiera que sea la dirección en la que Turing nos inspire, depende de la preservación de su visión original, o sea, la promulgación de una “Interpretación Estándar” de la prueba de Turing (i.e. una que se enfoque únicamente en la inteligencia discursiva) debe ser tomada con precaución.

Turing no afirmó explícitamente que la prueba de Turing podía ser usada como una medida de la inteligencia, o de cualquier otra cualidad humana. Él quería proporcionar una alternativa clara y comprensible a la palabra “pensar”, que posteriormente se pudiera usar para responder ante las críticas sobre la posibilidad de “máquinas pensantes”, y sugerir formas para que la investigación siga avanzando. Sin embargo, se ha propuesto el uso de la prueba de Turing como una medida de la “capacidad para pensar” o de la “inteligencia” de una máquina. Esta propuesta ha recibido las críticas de filósofos y científicos de la computación. Esta asume que un interrogador puede determinar si una máquina es “pensante” al comparar su comportamiento con el de un humano. Cada elemento de esta asunción ha sido cuestionado: la confiabilidad del juicio del interrogador, el valor de comparar únicamente el comportamiento y el valor de comparar a la máquina con el humano, es por estas asunciones y otras consideraciones que algunos investigadores de IA cuestionan la relevancia de la prueba de Turing en el campo.

La prueba de Turing no evalúa directamente si una computadora se comporta inteligentemente, solo si se comporta como un ser humano. Ya que el comportamiento humano y un comportamiento inteligente no son exactamente iguales, la prueba puede errar, al medir precisamente la inteligencia, de dos maneras:


La prueba de Turing evalúa única y estrictamente cómo se comporta el sujeto (o sea, el comportamiento externo de la máquina). En cuanto a esto, se toma una perspectiva conductista o fundamentalista al estudio de la inteligencia. El ejemplo de ELIZA sugiere que una máquina que pase la prueba podría simular el comportamiento conversacional humano siguiendo una simple (pero larga) lista de reglas mecánicas sin pensar o tener mente en lo absoluto.

John Searle ha argumentado que el comportamiento externo no puede ser usado para determinar si una máquina esta “realmente” pensando o simplemente “simulando el pensamiento”. Su habitación china pretende demostrar esto, aunque la prueba de Turing sea una buena definición operacional de la inteligencia, no indica si la máquina tiene una mente, conciencia o intencionalidad. (La intencionalidad es un término filosófico para el poder de los pensamientos de ser “sobre” algo.)

Turing anticipó esta crítica en su texto original escribiendo: 

En la práctica, los resultados de la prueba pueden ser fácilmente dominados, no por la inteligencia de la computadora sino por, las actitudes, la habilidad o la ingenuidad del interrogador.

Turing no especifica las habilidades ni el conocimiento requeridos del integrador en la descripción de su prueba pero si incluyó el término “interrogador promedio”: “[el] interrogador promedio no debe tener más del 70% de oportunidad de acertar en la identificación después de cinco minutos de cuestionamiento.”

Shah y Warwick (2009b) demostraron que los expertos son engañados y que la estrategia, “poder” vs “solidaridad” del interrogador influyen en la identificación con la última siendo más exitosa.

Los bots conversacionales como ELIZA han engañado, en repetidas ocasiones, a personas en creer que se comunican con seres humanos. En este caso, el interrogador no estaba al tanto de la posibilidad de que su interacción fuera con una computadora. Para aparentar ser un humano exitosamente, no hay necesidad de que la máquina tenga inteligencia alguna, solo se necesita una similitud superficial al comportamiento humano.

Las primeras competencias del Premio Loebner usaban interrogadores poco sofisticados que eran fácilmente engañados por las máquinas. Desde el 2004, los organizadores del Premio Loebner han implementado filósofos, científicos de la computación y periodistas entre los interrogadores. Sin embargo, algunos de estos expertos han sido engañados por las máquinas.

Michael Shermer señala que los seres humanos consistentemente consideran objetos no humanos como humanos siempre que tenga la oportunidad de hacerlo, un error llamado “falacia antropomórfica”: hablan con sus vehículos, atribuyen deseos e intenciones a fuerzas naturales (e.g. “la naturaleza odia el vacío”) y veneran al solo como un ser humano con inteligencia. Si la prueba de Turing se aplicara a objetos religiosos, entonces las estatuas inanimadas, rocas y lugares aprobarían consistentemente la prueba a lo largo de la historia según Shermer. Esta tendencia humana hacia el antropomorfismo, efectivamente reduce la exigencia a la prueba de Turing a menos que se le entrene a los interrogadores a evitarlo.

Una característica interesante de la prueba de Turing es la frecuencia con la que investigadores confunden a los participantes humanos con máquinas.Se ha sugerido que esto es porque los investigadores buscan respuestas humanas esperadas en vez de respuestas típicas. Esto resulta en la categorización incorrecta de algunos individuos como máquinas lo que puede favorecer a esta.

Los investigadores de IA famosos argumentan que el intentar pasar la prueba de Turing es una distracción de las investigaciones más fructíferas. La prueba no es un enfoque activo de investigación académica o de esfuerzo comercial, como Stuart Russel y Peter Norvig escribieron: “Los investigadores de IA han dedicado poca atención a pasar la prueba de Turing”. Hay varias razones para esto.

En primera, hay formas más fáciles de probar un programa. La mayoría de las investigaciones en los campos relacionados con la IA están dedicados a metas más específicas y modestas como la planificación automatizada, reconocimiento de objetos o logísticas. Para probar la inteligencia de los programas al realizar estas tareas, los investigadores simplemente les dan la tarea directamente. Russell y Norvig propusieron una analogía con la historia del vuelo: Los aviones son probados según su habilidad para volar, no comparándolos con aves. Textos de “Ingeniería Aeronáutica” mencionan: “no se debe definir la meta del campo como máquinas voladoras que vuelan tan parecidamente a las palomas que podrían engañar a estas.”.

En segunda, la creación de simulaciones de humanos es un problema difícil que no necesita resolverse para cumplir las metas básicas de la investigación de IA. Caracteres humanos creíbles son interesantes para una obra de arte, un juego o una interfaz de usuario sofisticada pero no tienen lugar en la ciencia de la creación de máquinas inteligentes que resuelven problemas con esta inteligencia.

Turing quería proporcionar un ejemplo claro y comprensible para ayudar en la discusión de la filosofía de la inteligencia artificial. John McCarthy menciona que la filosofía de IA es “poco probable que tenga más efecto en la práctica de la investigación de IA que la filosofía de la ciencia tienen en la práctica de la ciencia.”.

Numerosos versiones de la prueba de Turing, incluidas las mencionadas anteriormente, han sido debatidas a través de los años.

Una modificación de la prueba de Turing donde los objetivos entre las máquinas y los humanos es una prueba de Turing en reversa. Un ejemplo es empleado por el psicoanalista Wilfren Bion, quien tiene una fascinación particular por la “tormenta” que resultó del encuentro de una mente por otra. En su libro del 2000, entre otras ideas originales sobre la prueba de Turing, Swirski discute a detalle lo que define como la Prueba de Swirski (básicamente la prueba de Turing en reversa). Él señala que esta supera todas las objeciones comunes de la versión estándar.

R.D. Hinshelwood continuó con el desarrollo de esta idea al describir a la mente como un “aparato para reconocer mentes”. El reto sería que la computadora determine si está interactuando con un humano o con otra computadora. Esta es una extensión de la pregunta original que Turing intentaba responder y, probablemente, ofrece un estándar lo suficientemente alto para definir que una máquina puede “pensar” de la misma manera que nosotros describimos como humana.

CAPTCHA es una forma de la prueba de Turing en reversa. Antes de ser capaz de realizar una acción en un sitio web, se le presenta al usuario una serie de caracteres alfanuméricos en una imagen distorsionada y se le pide que lo ingrese en un campo de texto. Esto tiene como propósito la prevención de la entrada de sistemas automatizados comúnmente usados para el abuso del sitio web. La razón detrás de esto es que el software suficientemente sofisticado para leer y reproducir la imagen con precisión no existe aún (o no está disponible para el usuario promedio) por lo que cualquier sistema capaz de pasar la prueba debe ser humano.

El software capaz de solucionar CAPTCHA con precisión al analizar patrones en la plataforma generadora está siendo desarrollado activamente. Reconocimiento de Caracteres Ópticos o OCR (por sus siglas en inglés) se encuentra bajo en desarrollo como una solución para la inaccesibilidad de sistemas de CAPTCHA para humanos con discapacidades.

Otra variación es descrita como la variación de la Prueba de Turing Experta en la Materia en la cual no se puede distinguir entre la respuesta de una máquina de una respuesta dada por un experto en la materia. Se le conoce como la Prueba Feigenbaum y fue propuesta por Edward Feigenbaum en un texto del 2003.

La “Prueba de Turing Total” es una variación que añade requerimientos a la prueba tradicional. El interrogador también evalúa las capacidades de percepción del sujeto (requiriendo visión computacional) y la habilidad del sujeto de manipular objetos (requiriendo robótica).

La Prueba de la Señal de Inteligencia Mínima fue propuesta por Chris McKinstry como la “abstracción máxima de la prueba de Turing”, en la cual solo se permiten entradas en binario (verdadero/falso o si/no) con el objetivo de enfocarse en la capacidad de pensar. Se eliminan problemas de la conversación textual como la parcialidad antropomorfa, y no requiere la simulación de comportamientos humanos no inteligentes permitiéndole la entrada a sistemas que superan la inteligencia humana. Las preguntas no dependen de otras, sin embargo, esto es similar a una prueba de CI que una interrogación. Típicamente es usada para recolectar información estadística contra la cual se mide el rendimiento de los programas de IA.

Los organizadores del Premio Hutter creen que la compresión del lenguaje natural es un problema difícil para las inteligencias artificiales equivalente a pasar la prueba de Turing.

La prueba de compresión de información tiene ciertas ventajas sobre la mayoría de las versiones de la prueba de Turing incluyendo:


Las desventajas principales de esta prueba son:

Un acercamiento similar al del Premio Hutter que apareció mucho antes al final de la década de los noventa es la inclusión de problemas de compresión en una prueba de Turing extendida, o por pruebas completamente derivadas de la Complejidad de Kolmogórov Otras pruebas relacionadas son presentadas por Hernandez-Orallo y Dowe.

CI algorítmico, o CIA, es un intento de convertir la teórica Medida de la Inteligencia universal de Legg y Hutter (basada en la indiferencia inductiva de Solomonoff) en una prueba práctica funcional de la inteligencia de las máquinas.

Dos de las mayores ventajas de estas pruebas son su aplicabilidad a inteligencias no humanas y la ausencia de la necesidad de interrogadores humanos.

La prueba de Turing inspiró la prueba de Ebert propuesta en el 2011 por el crítico de cine Robert Ebert la cual evalúa si una voz sintetizada por computadora es capaz de producir las entonaciones, inflexiones, la sincronización entre otras cosas para hacer a la gente reír.

Turing predijo que las máquinas pasarían la prueba eventualmente, de hecho, el estimaba que para el año 2000, las máquinas con al menos 100 MB de almacenamiento podrían engañar a un 30% de los jueces humanos en una prueba de 5 minutos y que la gente no consideraría la frase “máquina pensante” como contradictoria. (En la práctica, desde 2009 a 2012, los robots conversacionales que participaron en el Premio Loebner solo lograron engañar una vez a un juez y esto era debido a que el participante humano pretendía ser un robot conversacional) Turing también predijo que el aprendizaje de las máquinas serían una parte esencial al construir máquinas poderosas, esta afirmación se considera posible en la actualidad por los investigadores en IA.

En un texto del 2008 enviado a la decimonovena Conferencia de Inteligencia Artificial y Ciencia Cognitiva del Medio Oeste, el doctor Shane T. Mueller predijo que una variante de la prueba de Turing llamada “Decatlón Cognitivo” sería completada en 5 años.

Al extrapolar el crecimiento exponencial de la tecnología a lo largo de varias décadas, el futurista Ray Kurzweil predijo que las máquinas que aprobaran la prueba de Turing serían fabricadas en el futuro próximo. En 1990, Kurzweil definió este futuro próximo alrededor del año 2020, para el 2005 cambió su estimado para el año 2029.

El proyecto “Long Bet Project Nr. 1” es una apuesta de $20,000 USD entre Mitch Kapor (pesimista) y Ray Kurzweil (optimista) sobre la posibilidad de que una máquina pase la prueba de Turing para el año 2029. Durante la prueba de Turing de Long Now, cada uno de los tres jueces realizara entrevistas a cada uno de los cuatro participantes (p.e. la computadora y tres humanos) durante dos horas para un total de 8 horas de entrevistas. La apuesta especifica condiciones a detalle.

El año 1990 marcó el 40 aniversario de la primera publicación del texto de Turing “Computing Machinery and Intelligence” por lo que se reavivó el interés en este. Dos eventos importantes se llevaron a cabo ese año, el primero fue el Coloquio de Turing con sede en la Universidad de Sussex en abril, este reunió a académicos e investigadores de disciplinas diferentes para discutir sobre la Prueba de Turing en cuanto a su pasado, presente y futuro; el segundo evento fue la competencia anual del Premio Loebner.

Blay Whitby en listo 4 puntos clave en la historia de la prueba de Turing siendo esto: la publicación de “Computing Machinery and Intelligence” en 1950, el anuncio de ELIZA en 1966 por Joseph Weizenbaum, la creación de PARRY por Kenneth Colby y el Coloquio de Turing de 1990.

En noviembre de 2005, la Universidad de Surrey fue sede de una junta de desarrolladores de entidades conversacionales artificiales a la que atendieron los ganadores del Premio Loebner: Robby Garner, Richard Wallace y Rollo Carpenter. Oradores invitados incluyeron a David Hamill, Hugh Loebner (patrocinador del Premio Loebner) Huma Shah.

En paralelo con el Premio Loebner del 2008 con sede en la Universidad de Reading la Sociedad para el Estudio de la Inteligencia Artificial y la Simulación del Comportamiento (IASC) fueron anfitriones de un simposio dedicado a discutir la Prueba de Turing, fue organizado por John Barnden, Mark Bishop, Huma Shah y Kevin Warwick Los oradores incluían al director de la Institución Real Susan Greenfield, Selmer Bringsjord, Andrew Hodges (biógrafo de Turing) y Owen Holland (científico de la conciencia). No se llegó a un acuerdo para una prueba de Turing canónica pero Bringsjord señaló que un premio mayor resultaría en la aprobación de la prueba de Turing más rápidamente.

60 años después de su introducción, el debate sobre el experimento de Turing sobre si “¿Pueden las máquinas pensar?” llevó a su reconsideración para la Convención de AISB del Siglo XXI, llevada a cabo del 29 de marzo al 1 de abril de 2010 en la Universidad De Monthfort, Reino unido. La IASC es la Sociedad para el Estudio de la Inteligencia Artificial y la Simulación del Comportamiento.

A lo largo del 2012, una cantidad considerable de eventos para celebrar la vida de Turing y su impacto científico se llevaron a cabo. El grupo Turing100 apoyó estos eventos y organizó una prueba de Turing especial en Bletchley Park el 23 de junio de 2012 para celebrar el aniversario número 100 del natalicio de Turing.

Las últimas discusiones sobre la Prueba de Turing en un simposio con 11 oradores, organizada por Vincent C. Müller (ACT y Oxford) y Aladdin Ayeshm (De Montfort) con Mark Bishop, John Barnden, Alessio Piebe y Pietro Perconti.




</doc>
<doc id="2257" url="https://es.wikipedia.org/wiki?curid=2257" title="Programación">
Programación

La programación es el proceso utilizado para idear y ordenar las acciones necesarias para realizar un proyecto, preparar ciertas máquinas o aparatos para que empiecen a funcionar en el momento y en la forma deseados o elaborar programas para su empleo en computadoras.

En la actualidad, la noción de programación se encuentra muy asociada a la creación de aplicaciones de informática y videojuegos. En este sentido, es el proceso por el cual una persona desarrolla un programa, valiéndose de una herramienta que le permita escribir el código (el cual puede estar en uno o varios lenguajes, como C++, Java y Python, entre otros) y de otra que sea capaz de “traducirlo” a lo que se conoce como lenguaje de máquina, que puede "comprender" el microprocesador.

Para crear un programa, y que la computadora lo interprete y ejecute, las instrucciones deben escribirse en un lenguaje de programación.
En sus comienzos las computadoras interpretaban solo instrucciones en un lenguaje específico, del más bajo nivel, conocido como código máquina, siendo éste excesivamente complicado para programar. De hecho solo consiste en cadenas de números 1 y 0 (sistema binario).
Para facilitar el trabajo de programación, los primeros científicos, que trabajaban en el área, decidieron reemplazar las instrucciones, secuencias de unos y ceros, por mnemónicos, que son abreviaturas en inglés de la función que cumple la instrucción; las codificaron y crearon así un lenguaje de mayor nivel, que se conoce como Assembly o lenguaje ensamblador. Por ejemplo, para sumar se podría usar la letra A de la palabra inglesa "add" (sumar). En realidad escribir en lenguaje ensamblador es básicamente lo mismo que hacerlo en lenguaje máquina, pero las letras y palabras son bastante más fáciles de recordar y entender que secuencias de números binarios.
A medida que la complejidad de las tareas que realizaban las computadoras aumentaba, se hizo necesario disponer de un método más sencillo para programar. Entonces, se crearon los lenguajes de alto nivel. Mientras que una tarea tan trivial como multiplicar dos números puede necesitar un conjunto de instrucciones en lenguaje ensamblador, en un lenguaje de alto nivel bastará con solo una.
Una vez que se termina de escribir un programa, sea en ensamblador o en algunos casos, lenguajes de alto nivel, es necesario compilarlo, es decir, traducirlo completo a lenguaje máquina.
Eventualmente será necesaria otra fase denominada comúnmente "link" o enlace, durante la cual se anexan al código, generado durante la compilación, los recursos necesarios de alguna biblioteca.
En algunos lenguajes de programación, puede no ser requerido el proceso de compilación y enlace, ya que se puede trabajar en modo intérprete. Esta modalidad de trabajo es equivalente pero se realiza instrucción por instrucción, se traduce a medida que es ejecutado el programa.

La programación se rige por reglas y un conjunto más o menos reducido de órdenes, expresiones, instrucciones y comandos que tienden a asemejarse a una lengua natural acotada (en inglés); y que además tienen la particularidad de una reducida ambigüedad. Cuanto menos ambiguo es un lenguaje de programación se dice que es más potente. Bajo esta premisa, y en el extremo, el lenguaje más potente existente es el binario, con ambigüedad nula (lo cual lleva a pensar así del lenguaje ensamblador).

En los lenguajes de programación de alto nivel se distinguen diversos elementos entre los que se incluyen el léxico propio del lenguaje y las reglas semánticas y sintácticas.

Un algoritmo es una secuencia no ambigua, finita y ordenada de instrucciones que han de seguirse para resolver un determinado problema. Un programa normalmente implementa y contiene uno o más algoritmos. Un algoritmo puede expresarse de distintas maneras: en forma gráfica, como un diagrama de flujo, en forma de código como en pseudocódigo o un lenguaje de programación, en forma explicativa.

Los programas suelen subdividirse en partes menores, llamadas módulos, de modo que la complejidad algorítmica de cada una de las partes sea menor que la del programa completo, lo cual ayuda a simplificar el desarrollo del programa. Esta es una práctica muy utilizada y se conoce como "refino progresivo".

Según Niklaus Wirth, un programa está formado por los algoritmos y la estructura de datos.

La programación puede seguir muchos enfoques, o paradigmas, es decir, diversas maneras de formular la resolución de un problema dado. Algunos de los principales paradigmas de programación son:

El programa escrito en un lenguaje de programación de alto nivel (fácilmente comprensible por el programador) es llamado "programa fuente" y no se puede ejecutar directamente en una computadora. La opción más común es compilar el programa obteniendo un módulo objeto, aunque también, si el lenguaje lo soporta, puede ejecutarse en forma directa pero solo a través de un intérprete. Algunos lenguajes, tal como BASIC, disponen de ambas formas de ejecución, lo cual facilita la tarea de depuración y prueba del programa.

El código fuente del programa se debe someter a un proceso de traducción para convertirlo a lenguaje máquina o bien a un código intermedio, generando así un módulo denominado "objeto". A este proceso se le llama "compilación".

Habitualmente la creación de un programa ejecutable (un típico .exe para Microsoft Windows o DOS) conlleva dos pasos: el primer paso se llama compilación (propiamente dicho) y traduce el código fuente, escrito en un lenguaje de programación y almacenado en un archivo de texto, a código en bajo nivel (normalmente a código objeto, no directamente a lenguaje máquina). El segundo paso se llama "enlazado" en el cual se enlaza el código de bajo nivel generado de todos los ficheros y subprogramas que se han mandado compilar y se añade el código de las funciones necesarias que reciden en bibliotecas externas, para que el ejecutable pueda comunicarse directamente con el sistema operativo, traduciendo así finalmente el código objeto a código máquina, y generando un módulo ejecutable.

Estos dos pasos se pueden hacer por separado, almacenando el resultado de la fase de compilación en archivos objetos (un típico .o para Unix, .obj para MS-Windows y DOS); para enlazarlos en fases posteriores, o crear directamente el ejecutable; con lo que la fase de compilación puede almacenarse de forma temporal.
Un programa podría tener partes escritas en varios lenguajes, por ejemplo, Java, C, C++ y ensamblador, que se podrían compilar de forma independiente y luego enlazar juntas para formar un único módulo ejecutable.

Existe una tendencia a identificar el proceso de creación de un programa informático con la programación, que es cierta cuando se trata de programas pequeños para uso personal, y que dista de la realidad cuando se trata de grandes proyectos.

El proceso de creación de software, desde el punto de vista de la ingeniería, incluye mínimamente los siguientes pasos:


La ingeniería del software se centra en los pasos de planificación y diseño del programa, mientras que antiguamente (programación artesanal) la realización de un programa consistía casi únicamente en escribir el código, bajo solo el conocimiento de los requisitos y con una modesta fase de análisis y diseño.

El trabajo de Ada Lovelace, hija de Anabella Milbanke Byron y Lord Byron, que realizó para la máquina de Babbage le hizo ganarse el título de "primera programadora de computadoras" del mundo, aunque Babbage nunca completó la construcción de la máquina.
El nombre del lenguaje de programación Ada fue escogido como homenaje a esta mujer programadora.

La programación debe perseguir la obtención de programas de calidad. Para ello se establece una serie de factores que determinan la calidad de un programa. Algunos de los factores de calidad más importantes son los siguientes:





El término ciclo de vida del software describe el desarrollo de software, desde la fase inicial hasta la fase final, incluyendo su estado funcional. El propósito es definir las distintas fases intermedias que se requieren para validar el desarrollo de la aplicación, es decir, para garantizar que el software cumpla los requisitos para la aplicación y verificación de los procedimientos de desarrollo: se asegura que los métodos utilizados son apropiados. Estos métodos se originan en el hecho de que es muy costoso corregir los errores que se detectan tarde dentro de la fase de implementación (programación propiamente dicha), o peor aún, durante la fase funcional. En el modelo de ciclo de vida se intenta que los errores se detecten lo antes posible y por lo tanto, permite a los desarrolladores concentrarse en la calidad del software, en los plazos de implementación y en los costos asociados. El ciclo de vida básico de un software consta de, al menos, los siguientes procedimientos:


El orden y la presencia de cada uno de estos procedimientos en el ciclo de vida de una aplicación dependen del tipo de modelo de ciclo de vida acordado entre el cliente y el equipo de desarrolladores. En el caso del software libre se tiene un ciclo de vida mucho más dinámico, puesto que muchos programadores trabajan en simultáneo desarrollando sus aportaciones.



</doc>
<doc id="2259" url="https://es.wikipedia.org/wiki?curid=2259" title="Parada y espera">
Parada y espera

El método de Parada y espera (Stop-and-wait) es un tipo de protocolo ARQ para el control de errores en la comunicación entre dos hosts basado en el envío de tramas o paquetes, de modo que una vez se envía un paquete no se envía el siguiente paquete hasta que no se recibe el correspondiente ACK (confirmación de la recepción) y en caso de recibir un NACK (rechazo de la recepción) se reenvía el paquete anterior.

Este protocolo asegura que la información no se pierde y que las tramas o paquetes se reciben en el orden correcto. Es el más simple de los métodos ARQ. En este, el emisor, después de enviar una sola trama, no envía las demás hasta que reciba una señal ACK (un acuse de recibo de que se recibió la trama) por parte del receptor. Por otro lado, el receptor, cuando recibe una trama válida (sin errores), envía la señal ACK.

Si el ACK no logra llegar al emisor antes de un cierto tiempo, llamado tiempo de espera, entonces el emisor, reenvía la trama otra vez. En caso de que el emisor sí reciba el ACK, entonces envía la siguiente trama.

El comportamiento anterior es la implementación más simple del método Parada-y-Espera. Sin embargo, en la implementación práctica de la vida real existen problemas que deben solucionarse.

Normalmente el emisor agrega un bit de redundancia al final de cada trama. El receptor utiliza dicho bit de redundancia para la búsqueda de posibles errores. Si el receptor encuentra que la trama es válida (no contiene errores), entonces envía el ACK. Si el receptor encuentra que la trama está dañada, entonces el receptor la deshecha y no envía el ACK -- pretendiendo que la trama se perdió por completo, no que fue solamente dañada.

Un problema surge cuando un ACK enviado por el receptor se daña o se pierde por completo en la red. En este caso, el emisor de la trama no recibe el ACK, se acaba el tiempo de espera y reenvía la trama de nuevo. Ahora el receptor tiene 2 copias de la misma trama y no sabe si la segunda es una trama duplicada o si es la siguiente trama de la secuencia que se enviará, que en realidad contiene datos idénticos a la primera.

Otro problema surge cuando el medio de transmisión tiene una latencia tan grande que el tiempo de espera del emisor se termina incluso antes de que la trama llegue al receptor. En este caso, el emisor reenvía la trama.
Eventualmente el receptor obtiene 2 copias de la misma trama y envía un ACK por cada una de ellas. Entonces, el emisor, que está a la espera de un solo ACK, recibe dos ACK's que pueden causar problemas si el emisor asume que el segundo ACK es para la siguiente trama en la secuencia.

Para evitar estos problemas, la solución más común es definir un número de secuencia de 1 bit en la cabecera de la trama. Este número de secuencia es alternado (de 0 a 1) en las tramas posteriores. Así, cuando el receptor envía un ACK, incluye el número de secuencia de la siguiente trama que espera recibir. 
De esta forma, el receptor puede identificar tramas duplicadas al checar si el número de secuencia de la trama fue alternado. Si dos tramas subsiguientes tienen el mismo número de secuencia, significa que son duplicados, y la segunda trama es deshechada. De igual forma, si dos ACK's subsiguientes hacen referencia al mismo número de secuencia, entonces significa que están acusando de recibo a la misma trama.

Como comentario, recordar que todo esto puede ser aplicado tanto a tramas, como a paquetes, ya que estos protocolos pueden ser implementados tanto en la capa de Enlace de Datos, como en la capa de Transporte del modelo OSI.

El método ARQ de Parada-y-Espera es ineficiente comparada con otros métodos ARQ porque el tiempo entre paquetes, en caso de que los ACK's y los datos sean recibidos satisfactoriamente, es el doble del tiempo de transmisión (suponiendo que el tiempo que tardan los hosts en procesar la información y responder es cero). El rendimiento en el canal es una fracción de lo que realmente podría ser. Para solucionar este problema, se puede enviar más de un paquete a la vez con un número de secuencia más grande y usar un solo ACK para dicho conjunto de paquetes. Esto es lo que se realiza con los métodos ARQ de Rechazo simple (Go-Back-N) y de Repetición Selectiva (Selective Repeat).



</doc>
<doc id="2260" url="https://es.wikipedia.org/wiki?curid=2260" title="Puccinellia">
Puccinellia

Puccinellia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones templas del Hemisferio Norte.
Son plantas anuales, bienales o perennes, cespitosas. Hojas con vaina de márgenes libres; lígula membranosa, aguda o truncada; limbo plano, plegado o convoluto. Inflorescencia en panícula laxa. Espiguillas subcilíndricas o ligeramente comprimidas, con 3-11 flores hermafroditas e imbricadas; raquilla glabra, desarticulándose en la fructificación. Glumas 2, desiguales, más corta que las flores, papiráceas; la inferior más corta que la superior, con 1 (-3) nervios; la superior con 3 nervios. Lema subcoriácea, con 3-5 nervios más o menos marcados. Pálea membranosa, con 2 quillas. Androceo con 3 estambres. Cariopsis oblonga, glabra.
El género fue descrito por Filippo Parlatore y publicado en "Flora italiana, ossia descrizione delle piante" ... 1: 366. 1848. La especie tipo es: "Puccinellia distans" (Jacq.) Parl.
El nombre del género fue otorgado en honor de Benedetto Puccinelli (1808–1850), profesor de Botánica en Lucca.

El número cromosómico básico del género es x = 7, con números cromosómicos somáticos de 2n = 14, 28, 35, 42, 49, 56, 70 y 77, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente «grandes».







</doc>
<doc id="2261" url="https://es.wikipedia.org/wiki?curid=2261" title="Psilurus incurvus">
Psilurus incurvus

Psilurus es un género monotípico de plantas herbáceas perteneciente a la familia de las poáceas. Su única especie Psilurus incurvus, es originaria de la región del Mediterráneo hasta Afganistán.

Es una planta anual con tallos de hasta 40 cm de altura, estriados, glabros o antrorso-escábridos sobre todo en la inflorescencia. Hojas con lígula de 0,1-0,2 mm, truncada; limbo de 10-40 x 0,2-0,5 mm, canaliculado, glabro. Espigas de 3-15 (-20) cm. Glumas más o menos escariosas, la inferior de la espiguilla terminal de 0,2-0,4 mm; la superior de 0,4-1 mm, triangular-ovada, más o menos escariosa. Lema de 3,7-5 mm, linear-lanceolada, con dorso antrorso-escábrido; arista de 3-6,5 mm, escábrida, más o menos violácea. Pálea de 3,7-4,5 (-5) mm, linear-lanceolada, anttorso-escábrida en el ápice. Cariopsis de c. de 4 mm, glabra. 2n = 28. Florece de marzo a junio.

"Psilurus incurvus" fue descrita por (Gouan) Schinz & Thell. y publicado en "Vierteljahrsschrift der Naturforschenden Gesellschaft in Zürich" 58: 40. 1913. 
El nombre del género deriva de las palabras griegas "psilos", delgado y "oura", cola, refiriéndose a la espiga delgada. 
Tiene un número de cromosomas de: x = 7. 2n = 14 y 28. 2 y 4 ploidias. Cromosomas ‘grandes’. 





</doc>
<doc id="2262" url="https://es.wikipedia.org/wiki?curid=2262" title="Pseudarrhenatherum">
Pseudarrhenatherum

Pseudarrhenatherum, es un género de plantas herbáceas, perteneciente a la familia de las poáceas. Es originario del oeste de Europa.

Algunos autores lo incluyen en el género "Arrhenatherum". 

Son plantas perennes, cespitosas o ligeramente estoloníferas. Hojas con vaina de márgenes ligeramente soldados en la base; lígula obtusa, membranosa; limbo convoluto, estriado por el haz, liso por el envés. Inflorescencia en panícula laxa y ramificada, con ramas escábridas. Espiguillas con 2 flores hermafroditas; la inferior articulada con la raquilla. Glumas desiguales, con 1-3 nervios. Lema con 5-7 nervios, de dorso redondeado, hírtulas; la de la flor inferior aristada; la de la superior generalmente mútica; arista inserta hacia la parte media dorsal de la lema, geniculada, con columna retorcida en hélice y de sección redondeada. Pálea con 2 quillas, ciliolada, más corta que la lema. Lodícula enteras. Ovario con ápice hirsuto. Cariopsis oblongo-elíptica, surcada ventralmente, con ápice peloso. Hilo linear.
El género fue descrito por Georges C.Chr. Rouy y publicado en "Bulletin de la Société Botanique de France" 68: 401. 1921. 
Pseudarrhenatherum: nombre genérico que significa "similar a "Arrhenatherum". 
Tiene un número de cromosomas de: 7. 2n = 14 (+/- 1B). 2 ploidias. 



</doc>
<doc id="2263" url="https://es.wikipedia.org/wiki?curid=2263" title="Polypogon">
Polypogon

Polypogon, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de la región del Mediterráneo hasta el oeste de Asia.
Son plantas anuales o perennes. Hojas con vaina de márgenes libres; lígula generalmente oblonga, escábrida, membranosa; limbo plano. Inflorescencia en panícula densa, con ramas lisas o escábridas y pedúnculos articulados. Espiguillas comprimidas lateralmente, desprendiéndose junto con la parte superior del pedúnculo, con 1 flor hermafrodita articulada con la raquilla. Glumas más largas que la flor, uninervadas, aquilladas. Raquilla no prolongada por encima de la flor. Lema con 5 nervios, truncado-dentada, mútica o con arista terminal, membranosa, glabra. Callo apenas marcado, glabro. Pálea un poco más corta que la lema, con 2 quillas. Lodículas enteras. Ovario glabro. Cariopsis surcada. Hilo elíptico.
El género fue descrito por René Louiche Desfontaines y publicado en "Flora Atlantica" 1: 66. 1798. La especie tipo es: "Polypogon monspeliensis" (L.) Desf.
El nombre del género deriva de las palabras griegas "polis" (muchos) y "pogon" (barba), refiriéndose a la panícula. 
Tiene un número de cromosomas de: x = 7. 2n = 14, 28, 42, 50, y 60 (no conocido en "P. tenellus"). 2, 4, 6, 7, y 8 ploidias. Cromosomas ‘grandes’. 




</doc>
<doc id="2264" url="https://es.wikipedia.org/wiki?curid=2264" title="Pollinia">
Pollinia

Pollinia es un género de insecto escama (Coccoidea) de la familia Asterolecaniidae.



</doc>
<doc id="2266" url="https://es.wikipedia.org/wiki?curid=2266" title="Poa">
Poa

Poa es un género cosmopolita de gramíneas (Poaceae) distribuidas en las regiones templadas y templado-cálidas de ambos hemisferios. Comprende dos subgéneros: "Poa" con flores hermafroditas y "Dioicopoa" con flores diclino dioicas. El nombre "Poa" deriva del griego y significa "hierba".

"Poa" comprende plantas anuales o perennes, cespitosas o rizomatosas, de talla variable, hasta de 1 m de altura. Presentan una inflorescencia en panoja laxa o contraída. Las vainas foliares se hallan dilatadas en la base o son más o menos comprimidas y son cerradas por lo menos en la parte inferior. La lígula es membranosa, truncada o acuminada. Las láminas foliares son planas, plegadas o convolutas, con la extremidad comúnmente obtusa. 

Las espiguillas son pauci o plurifloras, comprimidas, generalmente menores de 10 mm. Las glumas son membranosas, agudas, aquilladas, menores o iguales a los antecios contiguos. La gluma inferior es 1-3-nervada, la superior es 3-nervada. La raquilla está articulada por encima de las glumas y entre los antecios, es glabra o, más raramente, pilosa. La lema es aquillada, 5-nervada, aguda u obtusa, mútica, a menudo escariosa en el margen. La pálea es bidentada en el ápice, aquillada, estrecha y un poco más corta que la lemma. Las flores son hermafroditas o diclinas dioicas. Presenta 2 lodículas, generalmente 2-lobadas. Los estambres son 3, el ovario es oblongo con dos estilos cortos y estigmas plumosos. El cariopse es oblongo-elíptico, glabro, libre o más o menos adherente a la pálea, con hilo basal ovalado.

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum"1: 67–70. 1753. La especie tipo es: "Poa pratensis" L.
Poa: nombre genérico derivado del griego "poa" = (hierba, sobre todo como forraje). 
Tiene un número de cromosomas de: x = 7. 2n = 14, o 28, o 35, o 38, o 38–117, o 42, o 43, o 44, o 56, o 50–56, o 63, o 65, o 70–72, o 76 (etc). 2, 4, 5, 6, 7, 8, 9, 10, 11, y 12 ploidias (etc., y aneuploidias). Cromosomas ‘grandes’.




</doc>
<doc id="2267" url="https://es.wikipedia.org/wiki?curid=2267" title="Pleuropogon">
Pleuropogon

Pleuropogon, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia.
El género fue descrito por Robert Brown y publicado en "Chloris Melvilliana" 31–32, pl. D. 1823.
El nombre del género deriva del griego "pleura" (lado) y "pogon" (barba), aludiendo a cerdas en la base de la palea en la especie tipo. 
Número de la base del cromosoma, x = 9, o 10. 2n = 40 y 42. 4 ploide. Cromosomas relativamente «grandes». 




</doc>
<doc id="2270" url="https://es.wikipedia.org/wiki?curid=2270" title="Piresia">
Piresia

Piresia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Tiene cuatro especies nativas de Sudamérica.

Son plantas herbáceas perennes con hojas basales con peciolos, lanceoladas o oblongas o ovadas. Las flores masculinas y femeninas se encuentran en la misma inflorescencia, dispuestas en racimos terminales o axilares.
El género fue nombrado en honor de João Murça Pires, botánico brasileño.



</doc>
<doc id="2271" url="https://es.wikipedia.org/wiki?curid=2271" title="Phyllostachys">
Phyllostachys

Phyllostachys es un género de bambús de la familia de las Poáceas que tiene aproximadamente, 50 especies y 200 variedades. Es originario del Asia oriental.

El rizoma es de tipo monopodial, son los bambús que más se extienden. Se caracterizan por tener, nada menos que dos ramas en cada entrenudo y entrenudos más distanciados. Se encuentran en todos los continentes; en Europa consiguen alturas de más de 20 m, suelen resistir fríos de hasta -22 °C y son muy apreciados por su madera. 
El género fue descrito por Sieb. & Zucc. y publicado en "Abhandlungen der Mathematisch-Physikalischen Classe der Königlich Bayerischen Akademie der Wissenschaften" 3(3): 745, pl. 5, f. 3. 1843. La especie tipo es: "Phyllostachys bambusoides"
El número cromosómico básico es x = 12, con números cromosómicos somáticos de 2n = 24 (rara vez), o 48, o 72. Hay especies diploides y poliploides. Los cromosomas son relativamente pequeños.
Phyllostachys: nombvre genérico que deriva del griego antiguo y significa "en la punta de las hojas", refiriéndose a las inflorescencias.




</doc>
<doc id="2272" url="https://es.wikipedia.org/wiki?curid=2272" title="Phleum">
Phleum

Phleum es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia y Estados Unidos.

Son plantas perennes. Hojas con vaina de márgenes libres; lígula obtusa; limbo plano con haz estriado. Inflorescencia en panícula espiciforme, densa, con ramas casi completamente soldadas al raquis. Espiguillas muy comprimidas lateralmente, con 1 sola flor articulada con la raquilla. Glumas más largas que la flor, subiguales, trinervadas, marcadamente aquilladas. Raquilla no prolongada por encima de la flor, Lema con 5 nervios poco marcados, membranosa. Pálea tan larga como la lema, con 2 nervios. Lodículas bilobadas. Ovario glabro. Cariopsis ovoidea. Hilo elíptico.

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 1. 59. 1753. 
Phleum: nombre genérico que deriva de la palabra griega "phleos", una especie de caña o pasto. 
Tiene un número de cromosomas de: x = 7. 2n = 10 (raremente), o 14, o 28, o 42. 2, 4, y 6 ploidias. Cromosomas ‘grandes’. 



</doc>
<doc id="2273" url="https://es.wikipedia.org/wiki?curid=2273" title="Phippsia">
Phippsia

Phippsia es un género de plantas herbáceas perteneciente a la familia de las poáceas. Tiene 48 especies descritas y solo 3 aceptadas. Se distribuye por las regiones templadas del Hemisferio Norte.
Es una planta herbácea sin rizoma que alcanza hasta 25 cm de altura. Tiene las inflorescencias en panículas. 
El género fue descrito por Robert Brown y publicado en "Chloris Melvilliana" 27. 1823. La especie tipo es: "Phippsia algida" (Sol.) R.Br. 
El nombre del género fue otorgado en honor de Constantine John Phipps, explorador y botánico inglés.



</doc>
<doc id="2274" url="https://es.wikipedia.org/wiki?curid=2274" title="Pharus">
Pharus

Pharus es un género de plantas de la familia de las gramíneas o Poáceas. Comprende 23 especies distribuidas en América del Norte y Sudamérica.

Comprende plantas perennes, erectas o decumbentes. Son monoicas, con flores masculinas y femeninas en la misma inflorescencia. Presenta un androceo con 6 estambres y un gineceo con tres estigmas, plumosos o -más generalmente- pubescentes. La inflorescencia es una panoja.
El nombre del género deriva de la palabra griega"pharos" (tela o manto), quizá en alusión a sus hojas anchas.




</doc>
<doc id="2275" url="https://es.wikipedia.org/wiki?curid=2275" title="Perotis">
Perotis

Oxychloris, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de África, India, Ceilán, el este de Asia y Australia. Comprende 39 especies descritas y de estas, solo 13 aceptadas.

El género fue descrito por William Aiton y publicado en "Hortus Kewensis" 1: 85. 1789. La especie tipo es: "Perotis latifolia" Aiton.
A continuación se brinda un listado de las especies del género "Perotis" aceptadas hasta junio de 2015, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 


Ver "Chaetium Mosdenia Pennisetum Pogonatherum" 



</doc>
<doc id="2276" url="https://es.wikipedia.org/wiki?curid=2276" title="Periballia">
Periballia

Periballia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de la región del Mediterráneo.

Algunos autores lo incluyen en el género "Deschampsia". 
Es una planta herbácea caduca que alcanza los 25 cm de altura. Tiene las hojas lineales y estrechas de 0.5-2 mm de ancho. Las flores son hermafroditas y se encuentran en panículas.

Son plantas anuales. Hojas con vaina de márgenes libres; lígula lanceolada, aguda, membranosa; limbo delgado, convoluto en la desecación. Inflorescencia en panícula laxa. Espiguillas con 2 flores hermafroditas articuladas con la raquilla; raquilla hirsuta. Glumas más cortas que las flores, subiguales, agudas, la inferior uninervada, la superior trinervada. Raquilla no prolongada por encima de las flores. Lema elíptica, obtusa, con 5 nervios poco marcados, mútica o con arista subbasal recta, membranosa, con ápice escábrido. Callo orbicular, pubescente. Pálea tan larga o ligeramente más corta que la lema. Lodículas con 1 diente lateral. Ovario glabro. Cariopsis oblongoidea, ligeramente comprimida, libre. Hilo puntiforme.
El género fue descrito por Carl Bernhard von Trinius y publicado en "Fundamenta Agrostographiae" 133. 1820. 
Periballia: nombre genérico derivado del griego "peri" = (alrededor) y "ballo" = (lanzar), de alusión oscura. 
Tiene un número de cromosomas de: x = 4 and 7. 2n = 8, 14, y 18. 2 ploidias. Cromosomas ‘grandes’.




</doc>
<doc id="2277" url="https://es.wikipedia.org/wiki?curid=2277" title="Paspalum">
Paspalum

Paspalum, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es un género cosmopolita que se distribuye por las regiones templadas.
Son plantas perennes, cespitosas o estoloníferas. Vainas con márgenes glabros o pelosos. Hojas con limbo plano; lígula escariosa. Inflorescencia formada por racimos insertos a lo largo de un eje comprimido, o dispuesto subdigitadamente. Espiguillas cortamente pedunculadas, con flor inferior estéril y superior hermafrodita. Glumas muy desiguales; la inferior reducida a una escama membranosa triangular-ovada o ausente; la superior tan larga como las flores, submembranosa, con dorso convexo, con 4-7 nervios. Flor inferior con lema herbácea o submembranosa, tan larga como la gluma superior, sin pálea. Flor superior con lema sin nervios aparentes, de dorso convexo y liso, y pálea tan larga como la lema, con 2 quillas, coriáceas, aplanada. Cariopsis suborbicular.
El género fue descrito por Carlos Linneo y publicado en "Systema Naturae, Editio Decima" 2: 846, 855, 1359. 1759. La especie tipo es: "Paspalum dissectum" (L.) L., 1762 (sin.: "Paspalum dimidiatum" L., 1759 nom. illeg. ; "Panicum dissectum" L., 1753 ; "Paspalum membranaceum" Walter, 1788 ; "Paspalum walterianum" Schultes, 1824 nom. illeg.)
El nombre del género deriva del griego "paspalos" (una especie de mijo).

El número cromosómico básico del género es x = 10 y 12, con números cromosómicos somáticos de 2n = 20, 40, 48, 50, 60, 63 y 80, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente "pequeños". Nucléolos persistentes.




</doc>
<doc id="2278" url="https://es.wikipedia.org/wiki?curid=2278" title="Pariana">
Pariana

Pariana, es un género de plantas herbáceas perteneciente a la familia de las poáceas.



</doc>
<doc id="2279" url="https://es.wikipedia.org/wiki?curid=2279" title="Paratheria">
Paratheria

Paraneurachne es un género de plantas herbáceas, perteneciente a la familia de las poáceas. Es originario de África, Madagascar, Cuba, Brasil.



</doc>
<doc id="2280" url="https://es.wikipedia.org/wiki?curid=2280" title="Parapholis">
Parapholis

Parapholis es un género de plantas herbáceas, perteneciente a la familia de las poáceas. Tiene siete especies que se distribuyen por el sudoeste de Asia, norte de África y Europa donde puebla la región del Mediterráneo. También se ha introducido en América y Australia. Crece en terrenos moderadamente salobres y en margas húmedas desde los 140 hasta los 1000 metros de altitud.

Es una planta herbácea gramínea con una larga espiga que tiene las espiguillas muy muy unidas al eje, con lo que parece una caña alargada, delgada y rígida, esta puede ser poco curvada o muy curvada y hasta en espiral, en la especie "P. incurva". Florece en primavera tardía y principios de verano.

Son plantas anuales. Tallos frecuentemente ramificados. Hojas con vaina de márgenes libres; lígula corta, generalmente truncada, membranosa; limbo plano o plegado, rara vez convoluto. Inflorescencia en espiga, con eje marcadamente excavado, desarticulándose en la madurez. Espiguillas con 1 sola flor fértil y 2 glumas más largas que la flor, cubriendo cada excavación del eje, excepto en la antesis. Glumas coriáceas, con 5 nervios muy marcados y 1 surco transversal en la base. Lema membranosa, trinervada. Pálea tan larga como la lema, membranosa, bidentada, con 2 quillas poco marcadas, generalmente puberulentas en la parte superior. Androceo con 3 estambres. Ovario glabro.
El género fue descrito por Charles Edward Hubbard y publicado en "Blumea, Supplement" 3: 14. 1946. La especie tipo es: "Parapholis incurva" (L.) C.E. Hubb.
Parapholis: nombre genérico que deriva del griego "para" = (cercano) y "Pholiurus" (un género relacionado de hierbas); alternativamente, del griego "para" = (cercano) y "pholis" = (escama), en alusión a las glumas colaterales. 
Tiene un número de cromosomas de: x = 7, 9, y 19. 2n = 14, 28, 32, 36, 38, y 42. 2, 4, y 6 ploidias. Cromosomas ‘grandes’.




</doc>
<doc id="2281" url="https://es.wikipedia.org/wiki?curid=2281" title="Pappophorum">
Pappophorum

Pappophorum es un género de plantas herbáceas, perteneciente a la familia de las poáceas. Es originario de Estados Unidos y Sudamérica donde se distribuye desde Estados Unidos a Argentina y las Antillas.
Son plantas perennes cespitosas. Vainas redondeadas; lígula una hilera de cilios; láminas lineares, convolutas a aplanadas. Inflorescencia una panícula angosta. Espiguillas solitarias, con 3-6 flósculos, los 1-4 flósculos inferiores bisexuales, los flósculos superiores reducidos o estériles, el flósculo más inferior el mayor, los otros flósculos progresivamente más pequeños hacia arriba; desarticulación arriba de las glumas pero no entre los flósculos, o solo muy tardíamente; glumas subiguales, membranáceas, 1-nervias, glabras, persistentes, carinadas, agudas, tan largas como los cuerpos de las lemas; lemas redondeadas en el dorso, cartáceas, 7(-9)-nervias, con 11-25 aristas patentes, desiguales, escabrosas; pálea tan larga como el cuerpo de la lema o escasamente más larga, 2-nervia; callo escasamente oblicuo a truncado; lodículas 2; estambres 3; estigmas 2, plumosos. Fruto una cariopsis; embrión 1/2-2/3 la longitud de la cariopsis; hilo punteado.

El género fue descrito por Johann Christian Daniel von Schreber y publicado en "Genera Plantarum" 2: 787. 1791 La especie tipo es: "Dicrocaulon pearsonii" N.E. Br. 


ver "Bouteloua Bromus Enneapogon Pentameris Triodia" 




</doc>
<doc id="2282" url="https://es.wikipedia.org/wiki?curid=2282" title="Panicum">
Panicum

Panicum es un género de alrededor de 470 especiess de la familia de las poáceas. Son nativas de regiones tropicales del mundo, con pocas especies en las zonas templadas. Son pastos perennes, de 1 a 3 m de altura.

Las flores están en panículas bien desarrolladas, frecuentemente de más de 60 cm de longitud con numerosas semillas, de 3 a 6 mm de largo y 1 a 2 mm de ancho. Los frutos se desarrollan en racimos. Ambas glumas están presentes y bien desarrolladas.

Son plantas anuales o perennes y rizomatosas. Hojas con limbo plano o plegado; vaina pubescente; lígula membranosa, corta, truncada, largamente ciliada. Inflorescencia en panícula laxa, muy ramificada. Espiguillas cortamente pedunculadas, con flor inferior estéril o masculina y superior hermafrodita. Glumas 2, muy desiguales, membranosas; la inferior mucho más corta que las flores; la superior tan larga como las flores. Flor inferior con lema casi tan larga como la gluma superior, herbácea y pálea membranosa, tan larga como la lema y con 2 quillas, o mucho más corta que ésta y sin quillas. Flor superior con lema sin nervios aparentes y pálea tan larga como ésta, coriácea. Cariopsis oblongo-ovoidea.

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 1: 55. 1753. La especie tipo es: "Panicum miliaceum" L.


Panicum: nombre genérico que es un antiguo nombre de latín para el mijo común ("Setaria italica").


El número de cromosomas es de: x = 7, 9, y 10. 2n = 18, o 36, o 37, o 54, o 72. Cromosomas ‘pequeños’. Nucleolos persistentes.





</doc>
<doc id="2318" url="https://es.wikipedia.org/wiki?curid=2318" title="Premio Princesa de Asturias de Comunicación y Humanidades">
Premio Princesa de Asturias de Comunicación y Humanidades

Los Premios Princesa de Asturias de Comunicación y Humanidades (Premios Príncipe de Asturias, hasta 2014) son concedidos, desde 1981, a la persona, grupo de personas o institución cuya labor creadora o de investigación represente una aportación relevante a la cultura universal en esos campos.



</doc>
<doc id="2319" url="https://es.wikipedia.org/wiki?curid=2319" title="Premio Princesa de Asturias de las Letras">
Premio Princesa de Asturias de las Letras

El Premio Princesa de Asturias de las Letras (Premio Príncipe de Asturias de las Letras hasta 2014) son concedidos desde 1981, a la persona, grupo de personas o institución cuya labor creadora o de investigación represente una contribución relevante a la cultura universal en los campos de la Literatura o de la Lingüística.




</doc>
<doc id="2320" url="https://es.wikipedia.org/wiki?curid=2320" title="Premio Princesa de Asturias de los Deportes">
Premio Princesa de Asturias de los Deportes

Los Premios Princesa de Asturias de los Deportes (Premios Príncipe de Asturias de los Deportes hasta 2014) son concedidos, desde 1987, a aquella persona o institución que, además de la ejemplaridad de su vida y obra, haya conseguido nuevas metas en la lucha del hombre por superarse a sí mismo y contribuido con su esfuerzo, de manera extraordinaria, al perfeccionamiento, cultivo, promoción o difusión de los deportes.

En numerosas ocasiones se ha acusado al jurado de este premio de chovinista, pues la mitad de los galardonados han sido deportistas españoles (en 16 de 32 ediciones), y varios deportistas españoles lo han obtenido incluso en 2 ocasiones, caso de los hermanos Pau y Marc Gasol o de los futbolistas Iker Casillas y Xavi Hernández. Algunos premiados ni siquiera acudieron a recoger el galardón: Carl Lewis en 1996, Lance Armstrong en 2000, o los hermanos Pau y Marc Gasol en 2015.



</doc>
<doc id="2321" url="https://es.wikipedia.org/wiki?curid=2321" title="Premio Princesa de Asturias de las Artes">
Premio Princesa de Asturias de las Artes

Los Premios Princesa de Asturias de las Artes (Premios Príncipe de Asturias de las Artes hasta 2014) son concedidos a la persona, grupo de personas o institución cuya labor en la Arquitectura, Cinematografía, Danza, Escultura, Música, Pintura y demás expresiones artísticas constituya una aportación relevante al patrimonio cultural de la Humanidad.




</doc>
<doc id="2323" url="https://es.wikipedia.org/wiki?curid=2323" title="Portable Network Graphics">
Portable Network Graphics

PNG (siglas en inglés de Portable Network Graphics, Gráficos de Red Portátiles, pronunciadas ""ping"") es un formato gráfico basado en un algoritmo de compresión sin pérdida para bitmaps no sujeto a patentes. Este formato fue desarrollado en buena parte para solventar las deficiencias del formato GIF y permite almacenar imágenes con una mayor profundidad de contraste y otros importantes datos.

Las imágenes PNG usan la extensión .png y han obtenido un tipo MIME (image/png) aprobado el 14 de octubre de 1996.

Un archivo PNG empieza con una firma de 8 bytes, los valores en hexadecimal son: 89 50 4E 47 0D 0A 1A 0A, los valores decimales son: 137 80 78 71 13 10 26 10; cada valor está ahí por una razón específica.

Después de la cabecera se encuentran una serie de segmentos de los cuales cada uno guarda cierta información acerca de la imagen. Los segmentos se auto declaran como puntos críticos ("critical") o auxiliares ("ancillary") de modo que un programa que encuentre un segmento auxiliar y no lo entienda puede ignorarlo sin peligro. La estructura basada en segmentos está diseñada para poder ampliar el formato PNG manteniendo la compatibilidad con versiones antiguas.

Cada una de las secciones tiene una cabecera que específica su tamaño y tipo, inmediatamente seguido de los datos y el checksum de los datos. Las secciones tienen un nombre de 4 letras que es sensible a las mayúsculas. El uso de mayúsculas o minúsculas en dicho nombre provee a los decodificadores de información acerca de las secciones que no son reconocidas.

Si la primera letra es mayúscula esto indica que la sección es esencial, en caso contrario será auxiliar. Las secciones esenciales son necesarias para leer el fichero, si el decodificador encuentra una sección esencial que no reconoce debe abortar la lectura.

En caso de que la segunda letra sea mayúscula esto significará que la sección es pública en la especificación o el registro de secciones para propósitos especiales, en caso contrario será privada (no estandarizada). Este uso de mayúsculas y minúsculas asegura que nunca haya conflictos entre secciones públicas y privadas.

La tercera letra debe estar en mayúsculas para cumplir las especificaciones de PNG y está reservada para futuras expansiones.

La cuarta letra indica si es seguro copiar la sección en caso de que no sea reconocida, en caso de estar en minúsculas es seguro copiar la sección sin importar la cantidad de modificación que haya sufrido el fichero, si es mayúscula solo se deberán copiar si no hay secciones críticas que hayan sufrido modificaciones.

Un decodificador debe ser capaz de entender estas secciones para leer y renderizar un PNG:


Otros atributos que pueden ser guardados en una imagen PNG son: valores de gamma, color del fondo e información textual. PNG también soporta corrección de color con el uso de sistemas de manejo del color como sRGB.


La primera letra en minúsculas de estas secciones indica que no son necesarias en la especificación de PNG, la última letra en minúsculas indica que es seguro copiarlas incluso si la aplicación en cuestión no las entiende.

Otros atributos que pueden ser almacenados en un PNG incluyen valores de corrección gamma, color de fondo y metadatos. PNG además también utiliza la corrección de color que utilizan los sistemas de administración de color como el sRGB. Algunos programas como Adobe Photoshop disponen de este sistema.

Las imágenes en formato PNG pueden ser imágenes de paleta indexada o estar formadas por uno o varios canales. Si existe más de un canal, todos los canales tienen el mismo número de bits por píxel (también llamado profundidad de bits por canal). Aunque en la especificación oficial del PNG se nombra la profundidad de bits por canal, normalmente los programas de edición nombran solo la cantidad total de bits por píxel, es decir, la profundidad de color. 

El número de canales depende de si la imagen es en escala de grises o en color y si dispone de canal alfa (también llamado canal de transparencia). La combinaciones permitidas por PNG son:


Por otra parte, las imágenes indexadas disponen de un tope de 256 colores como máximo. Esta paleta de colores está almacenada con una profundidad de canal de 8 bits. La paleta no puede tener más colores que los marcados por la profundidad de bits, es decir 2=256 colores, aunque sí puede tener menos (por ejemplo, una imagen de 50 colores solo almacenará 50 entradas, evitando almacenar datos que no son utilizados).

La tabla expuesta a la derecha indican la profundidad de color para cada formato de imagen que soporta PNG. Esta se extrae de la profundidad de bits por canal y se multiplica por el número de canales. Las casillas en rojo representan combinaciones no soportadas. El estándar requiere que los decodificadores puedan leer todos los formatos disponibles, pero muchos editores de imagen solo pueden generar un pequeño subconjunto de ellos.

PNG ofrece una gran variedad de opciones de transparencia: con color verdadero o escala de grises, incluso un solo píxel puede ser declarado transparente o puede añadirse un canal alfa. Para imágenes que usan paletas se puede añadir un canal alfa en las entradas de la paleta. El número de dichos valores almacenados puede ser menor que el total de entradas en la paleta, de modo que el resto de las entradas se considerarán completamente opacas. La búsqueda de píxeles con transparencia binaria debe hacerse antes de cualquier reducción de color para evitar que algunos píxeles se conviertan en transparentes accidentalmente.

A diferencia de la transparencia ofrecida por GIF que solo puede tomar 2 valores (completamente transparente o completamente opaco), el canal alfa de PNG permite utilizar mayor profundidad de bits para lograr efectos de semi-transparencia, propios de objetos translúcidos. Por ejemplo, con una profundidad de 8 bits para transparencias se pueden conseguir 2 = 256 grados diferentes de transparencia, como si se tratara de un color.

El método de compresión utilizado por el PNG es conocido como deflación (en inglés ""). También existen métodos de filtrado. En la especificación 1.2 se define un único tipo de filtro, que incluye 5 modos de predicción del valor del píxel, que resulta muy útil para mejorar la compresión, donde se elige para cada línea de la imagen (scanline) un "método de filtrado" que predice el color de cada píxel basándose en los colores de los píxeles previos y resta al color del píxel actual, el color pronosticado. Los cinco métodos son: None, Sub, Up, Average y Paeth. 

Estos filtros pueden reducir notablemente el tamaño final del archivo, aunque depende en gran medida de la imagen de entrada. El algoritmo de compresión puede encargarse de la adecuada elección del método que mayor reducción ofrezca.

El tipo de media MIME para PNG es "image/png" (aprobado el 14 de octubre de 1996).

APNG es una extensión de PNG que soporta animación. Este formato soporta el visionado de una sola imagen en caso de que el decodificador no entienda el archivo. Es aceptado por múltiples navegadores y tiene extensión ".png". 

Por otro lado, MNG es un formato de imagen que soporta animación y está basado en las ideas y en algunas secciones de PNG, pero es un sistema complejo y no permite el visionado de una sola imagen en caso de no existir soporte completo en el visualizador de imágenes pertinente.





PNG y JPEG son formatos que están diseñados para funciones diferentes, por lo que únicamente se puede realizar una comparación generalista:




Algunas versiones de algunos navegadores web presentan los valores de corrección gamma incluso cuando no están especificados en el PNG. Navegadores conocidos con problemas de visualización de PNG:


El efecto final es que el color mostrado en el PNG no coincide con el esquema de color del resto de la página web. Una forma sencilla de evitar esto es volviendo a codificar el PNG truncando ciertos atributos. Algunas utilidades para tal fin:





</doc>
<doc id="2330" url="https://es.wikipedia.org/wiki?curid=2330" title="Python">
Python

Python es un lenguaje de programación interpretado cuya filosofía hace hincapié en la legibilidad de su código. Se trata de un lenguaje de programación multiparadigma, ya que soporta orientación a objetos, programación imperativa y, en menor medida, programación funcional. Es un lenguaje interpretado, dinámico y multiplataforma.

Es administrado por la Python Software Foundation. Posee una licencia de código abierto, denominada Python Software Foundation License,

Python fue creado a finales de los ochenta por Guido van Rossum en el Centro para las Matemáticas y la Informática (CWI, "Centrum Wiskunde & Informatica"), en los Países Bajos, como un sucesor del lenguaje de programación ABC, capaz de manejar excepciones e interactuar con el sistema operativo Amoeba.

El nombre del lenguaje proviene de la afición de su creador por los humoristas británicos Monty Python.

Van Rossum es el principal autor de Python, y su continuo rol central en decidir la dirección de Python es reconocido, refiriéndose a él como "Benevolente Dictador Vitalicio" (en inglés: "Benevolent Dictator for Life", BDFL); sin embargo el 12 de julio de 2018 declinó de dicha situación de honor sin dejar un sucesor o sucesora y con una declaración altisonante:

En 1991, van Rossum publicó el código de la versión 0.9.0 en alt.sources. En esta etapa del desarrollo ya estaban presentes clases con herencia, manejo de excepciones, funciones y los tipos modulares, como: codice_1, codice_2, codice_3, entre otros. Además en este lanzamiento inicial aparecía un sistema de módulos adoptado de Modula-3; van Rossum describe el módulo como «una de las mayores unidades de programación de Python». El modelo de excepciones en Python es parecido al de Modula-3, con la adición de una cláusula codice_4. En el año 1994 se formó comp.lang.python, el foro de discusión principal de Python, marcando un hito en el crecimiento del grupo de usuarios de este lenguaje.

Python alcanzó la versión 1.0 en enero de 1994. Una característica de este lanzamiento fueron las herramientas de la programación funcional: codice_5, codice_6, codice_7 y codice_8. Van Rossum explicó que «hace 12 años, Python adquirió lambda, reduce(), filter() y map(), cortesía de Amrit Perm, un hacker informático de Lisp que las implementó porque las extrañaba».

La última versión liberada proveniente de CWI fue Python 1.2. En 1995, van Rossum continuó su trabajo en Python en la (CNRI) en Reston, Virginia, donde lanzó varias versiones del software.

Durante su estancia en CNRI, van Rossum lanzó la iniciativa "Computer Programming for Everybody" (CP4E), con el fin de hacer la programación más accesible a más gente, con un nivel de 'alfabetización' básico en lenguajes de programación, similar a la alfabetización básica en inglés y habilidades matemáticas necesarias por muchos trabajadores. Python tuvo un papel crucial en este proceso: debido a su orientación hacia una sintaxis limpia, ya era idóneo, y las metas de CP4E presentaban similitudes con su predecesor, ABC. El proyecto fue patrocinado por DARPA. En el año 2007, el proyecto CP4E está inactivo, y mientras Python intenta ser fácil de aprender y no muy arcano en su sintaxis y semántica, alcanzando a los no-programadores, no es una preocupación activa.

En el año 2000, el equipo principal de desarrolladores de Python se cambió a BeOpen.com para formar el equipo BeOpen PythonLabs. CNRI pidió que la versión 1.6 fuera pública, continuando su desarrollo hasta que el equipo de desarrollo abandonó CNRI; su programa de lanzamiento y el de la versión 2.0 tenían una significativa cantidad de traslapo. Python 2.0 fue el primer y único lanzamiento de BeOpen.com. Después que Python 2.0 fuera publicado por BeOpen.com, Guido van Rossum y los otros desarrolladores de PythonLabs se unieron en Digital Creations.

Python 2.0 tomó una característica mayor del lenguaje de programación funcional Haskell: listas por comprensión. La sintaxis de Python para esta construcción es muy similar a la de Haskell, salvo por la preferencia de los caracteres de puntuación en Haskell, y la preferencia de Python por palabras claves alfabéticas. Python 2.0 introdujo además un sistema de recolección de basura capaz de recolectar referencias cíclicas.

Posterior a este doble lanzamiento, y después que van Rossum dejara CNRI para trabajar con desarrolladores de software comercial, quedó claro que la opción de usar Python con software disponible bajo GNU GPL era muy deseable. La licencia usada entonces, la Python License, incluía una cláusula estipulando que la licencia estaba gobernada por el estado de Virginia, por lo que, bajo la óptica de los abogados de Free Software Foundation (FSF), se hacía incompatible con GPL. Para las versiones 1.61 y 2.1, CNRI y FSF 
hicieron compatibles la licencia de Python con GPL, renombrandola Python Software Foundation License. En el año 2001, van Rossum fue premiado con FSF Award for the Advancement of Free Software.

Python 2.1 fue un trabajo derivado de las versiones 1.6.1 y 2.0. Es a partir de este momento que Python Software Foundation (PSF) pasa a ser dueño del proyecto, organizada como una organización sin ánimo de lucro fundada en el año 2001, tomando como modelo la Apache Software Foundation. Incluido en este lanzamiento fue una implementación del scoping más parecida a las reglas de static scoping (del cual Scheme es el originador).

Una innovación mayor en Python 2.2 fue la unificación de los tipos en Python (tipos escritos en C), y clases (tipos escritos en Python) dentro de una jerarquía. Esa unificación logró un modelo de objetos de Python puro y consistente. También fueron agregados los generadores que fueron inspirados por el lenguaje Icon.

Las adiciones a la biblioteca estándar de Python y las decisiones sintácticas fueron influenciadas fuertemente por Java en algunos casos: el package codice_9, introducido en la versión 2.3, está basado en log4j; el parser SAX, introducido en 2.0; el package codice_10, cuya clase "Thread" expone un subconjunto de la interfaz de la clase homónima en Java.

Python 2, es decir Python 2.7.x, fue oficialmente descontinuado el 1 de enero de 2020 (primero planeado para 2015) después de lo cual no se publicarán parches de seguridad y otras mejoras para él. Con el final del ciclo de vida de Python 2, solo tienen soporte la rama Python 3.5.x y posteriores.

En la actualidad, Python se aplica en los campos de inteligencia artificial y machine learning.

Python es un lenguaje de programación multiparadigma. Esto significa que más que forzar a los programadores a adoptar un estilo particular de programación, permite varios estilos: programación orientada a objetos, programación imperativa y programación funcional. Otros paradigmas están soportados mediante el uso de extensiones.

Python usa tipado dinámico y conteo de referencias para la administración de memoria.

Una característica importante de Python es la resolución dinámica de nombres; es decir, lo que enlaza un método y un nombre de variable durante la ejecución del programa (también llamado enlace dinámico de métodos).

Otro objetivo del diseño del lenguaje es la facilidad de extensión. Se pueden escribir nuevos módulos fácilmente en C o C++. Python puede incluirse en aplicaciones que necesitan una interfaz programable.

Aunque la programación en Python podría considerarse en algunas situaciones hostil a la programación funcional tradicional del Lisp, existen bastantes analogías entre Python y los lenguajes minimalistas de la familia Lisp como puede ser Scheme.

Los usuarios de Python se refieren a menudo a la filosofía de Python que es bastante análoga a la filosofía de Unix. El código que siga los principios de Python se dice que es "pythonico". Estos principios fueron descritos por el desarrollador de Python Tim Peters en El Zen de Python

Desde la versión 2.1.2, Python incluye estos puntos (en su versión original en inglés) como un huevo de pascua que se muestra al ejecutar codice_11.

El intérprete de Python estándar incluye un "modo interactivo" en el cual se escriben las instrucciones en una especie de intérprete de comandos: las expresiones pueden ser introducidas una a una, pudiendo verse el resultado de su evaluación inmediatamente, lo que da la posibilidad de probar porciones de código en el modo interactivo antes de integrarlo como parte de un programa. Esto resulta útil tanto para las personas que se están familiarizando con el lenguaje como para los programadores más avanzados.

Existen otros programas, tales como IDLE, bpython o IPython, que añaden funcionalidades extra al modo interactivo, como el autocompletado de código y el coloreado de la sintaxis del lenguaje.

Ejemplo del modo interactivo:
»> 1 + 1
2
»> a = range(10)
»> print(list(a))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

Python fue diseñado para ser leído con facilidad. Una de sus características es el uso de palabras donde otros lenguajes utilizarían símbolos. Por ejemplo, los operadores lógicos codice_12, codice_13 y codice_14 en Python se escriben codice_15, codice_16 y codice_17, respectivamente. Curiosamente el lenguaje Pascal es junto con COBOL uno de los lenguajes con muy clara sintaxis y ambos son de la década del 70. La idea del código claro y legible no es algo nuevo.

El contenido de los bloques de código (bucles, funciones, clases, etc.) es delimitado mediante espacios o tabuladores, conocidos como indentación, antes de cada línea de órdenes pertenecientes al bloque. Python se diferencia así de otros lenguajes de programación que mantienen como costumbre declarar los bloques mediante un conjunto de caracteres, normalmente entre llaves codice_18. Se pueden utilizar tanto espacios como tabuladores para indentar el código, pero se recomienda no mezclarlos. 

Debido al significado sintáctico de la indentación, cada instrucción debe estar contenida en una sola línea. No obstante, si por legibilidad se quiere dividir la instrucción en varias líneas, añadiendo una barra invertida codice_19 al final de una línea, se indica que la instrucción continúa en la siguiente.

Estas instrucciones son equivalentes:

Los "comentarios" se pueden poner de dos formas. La primera y más apropiada para comentarios largos es utilizando la notación <nowiki> comentario </nowiki>, tres apóstrofos de apertura y tres de cierre. La segunda notación utiliza el símbolo codice_20, y se extienden hasta el final de la línea. 

El intérprete no tiene en cuenta los "comentarios", lo cual es útil si deseamos poner información adicional en el código. Por ejemplo, una explicación sobre el comportamiento de una sección del programa.

Comentario más largo en una línea en Python

print ("Hola mundo") # También es posible añadir un comentario al final de una línea de código

Las "variables" se definen de forma dinámica, lo que significa que no se tiene que especificar cuál es su tipo de antemano y puede tomar distintos valores en otro momento, incluso de un tipo diferente al que tenía previamente. Se usa el símbolo codice_21 para asignar valores.
x = 1
x = "texto" # Esto es posible porque los tipos son asignados dinámicamente
Los nombres de variables pueden contener números y letras pero deben comenzar por una letra, además existen 28 palabras reservadas:



Los "tipos de datos" se pueden resumir en esta tabla:


Una sentencia condicional ("if") ejecuta su bloque de código interno solo si se cumple cierta condición. Se define usando la palabra clave codice_37 seguida de la condición, y el bloque de código. Condiciones adicionales, si las hay, se introducen usando codice_29 seguida de la condición y su bloque de código. Todas las condiciones se evalúan secuencialmente hasta encontrar la primera que sea verdadera, y su bloque de código asociado es el único que se ejecuta. Opcionalmente, puede haber un bloque final (la palabra clave codice_4 seguida de un bloque de código) que se ejecuta solo cuando todas las condiciones fueron falsas.
»> verdadero = True
»> if verdadero: # No es necesario poner "verdadero == True"
... print "Verdadero"
... else:
... print "Falso"
Verdadero
»> lenguaje = "Python"
»> if lenguaje == "C": # lenguaje no es "C", por lo que este bloque se obviará y evaluará la siguiente condición
... print "Lenguaje de programación: C"
... elif lenguaje == "Python": # Se pueden añadir tantos bloques "elif" como se quiera
... print "Lenguaje de programación: Python"
... else: # En caso de que ninguna de las anteriores condiciones fuera cierta, se ejecutaría este bloque
... print "Lenguaje de programación: indefinido"
Lenguaje de programación: Python
»> if verdadero and lenguaje == "Python": # Uso de "and" para comprobar que ambas condiciones son verdaderas
... print "Verdadero y Lenguaje de programación: Python"
Verdadero y Lenguaje de programación: Python

El bucle "for" es similar a "foreach" en otros lenguajes. Recorre un objeto iterable, como una "lista", una "tupla" o un generador, y por cada elemento del iterable ejecuta el bloque de código interno. Se define con la palabra clave codice_34 seguida de un nombre de variable, seguido de codice_39, seguido del iterable, y finalmente el bloque de código interno. En cada iteración, el elemento siguiente del iterable se asigna al nombre de variable especificado:
»> lista = ["a", "b", "c"]
»> for i in lista: # Iteramos sobre una lista, que es iterable
... print(i)
a
b
c
»> cadena = "abcdef"
»> for i in cadena: # Iteramos sobre una cadena, que también es iterable
... print(i, end=', ') # Añadiendo end=', ' al final hacemos que no introduzca un salto de línea, sino una coma y un espacio
a, b, c, d, e, f,

El bucle "while" evalúa una condición y, si es verdadera, ejecuta el bloque de código interno. Continúa evaluando y ejecutando mientras la condición sea verdadera. Se define con la palabra clave codice_49 seguida de la condición, y a continuación el bloque de código interno:
»> numero = 0
»> while numero < 10:
... print (numero),
... numero += 1 # Un buen programador modificará las variables de control al finalizar el ciclo while
0 1 2 3 4 5 6 7 8 9 



»> lista = ["abc", 42, 3.1415]
»> lista[0] # Acceder a un elemento por su índice
'abc'
»> lista[-1] # Acceder a un elemento usando un índice negativo
3.1415
»> lista.append(True) # Añadir un elemento al final de la lista
»> lista
['abc', 42, 3.1415, True]
»> del lista[3] # Borra un elemento de la lista usando un índice (en este caso: True)
»> lista[0] = "xyz" # Re-asignar el valor del primer elemento de la lista
»> lista[0:2] # Mostrar los elementos de la lista del índice "0" al "2" (sin incluir este último)
['xyz', 42]
»> lista_anidada = [lista, [True, 42L]] # Es posible anidar listas
»> lista_anidada
'xyz', 42, 3.1415], [True, 42L
»> lista_anidada[1][0] # Acceder a un elemento de una lista dentro de otra lista (del segundo elemento, mostrar el primer elemento)
True

»> tupla = ("abc", 42, 3.1415)
»> tupla[0] # Acceder a un elemento por su índice
'abc'
»> del tupla[0] # No es posible borrar (ni añadir) un elemento en una tupla, lo que provocará una excepción
»> tupla[0] = "xyz" # Tampoco es posible re-asignar el valor de un elemento en una tupla, lo que también provocará una excepción
»> tupla[0:2] # Mostrar los elementos de la tupla del índice "0" al "2" (sin incluir este último)
»> tupla_anidada = (tupla, (True, 3.1415)) # También es posible anidar tuplas
»> 1, 2, 3, "abc" # Esto también es una tupla, aunque es recomendable ponerla entre paréntesis (recuerda que requiere, al menos, una coma)
»> (1) # Aunque entre paréntesis, esto no es una tupla, ya que no posee al menos una coma, por lo que únicamente aparecerá el valor
1
»> (1,) # En cambio, en este otro caso, sí es una tupla
»> (1, 2) # Con más de un elemento no es necesaria la coma final
»> (1, 2,) # Aunque agregarla no modifica el resultado

»> diccionario = {"cadena": "abc", "numero": 42, "lista": [True, 42L]} # Diccionario que tiene diferentes valores por cada clave, incluso una lista
»> diccionario["cadena"] # Usando una clave, se accede a su valor
'abc'
»> diccionario["lista"][0] # Acceder a un elemento de una lista dentro de un valor (del valor de la clave "lista", mostrar el primer elemento)
True
»> diccionario["cadena"] = "xyz" # Re-asignar el valor de una clave
»> diccionario["cadena"]
'xyz'
»> diccionario["decimal"] = 3.1415927 # Insertar un nuevo elemento clave:valor
»> diccionario["decimal"]
3.1415927
»> diccionario_mixto = {"tupla": (True, 3.1415), "diccionario": diccionario} # También es posible que un valor sea un diccionario
»> diccionario_mixto["diccionario"]["lista"][1] # Acceder a un elemento dentro de una lista, que se encuentra dentro de un diccionario
42L
»> diccionario = {("abc",): 42} # Sí es posible que una clave sea una tupla, pues es inmutable
»> diccionario = {["abc"]: 42} # No es posible que una clave sea una lista, pues es mutable, lo que provocará una excepción
Si bien Python no tiene la estructura Switch, hay varias formas de realizar la operación típica que realizaríamos con una sentencia switch case.
Podemos usar la estructura de la siguiente manera:

»> if condicion1:
... hacer1
»> elif condicion2:
... hacer2
»> elif condicion3:
... hacer3
»> else:
... hacer

En esa estructura se ejecutara controlando la condicion1, si no se cumple pasara a la siguiente y así sucesivamente hasta entrar en el else.
Un ejemplo práctico seria:

»> def calculo(op,a,b):
... if 'sum' == op:
... return a + b
... elif 'rest' == op:
... return a - b
... elif 'mult' == op:
... return a * b
... elif 'div' == op:
... return a/b
... else:
... return None
»> print(calculo('sum',3,4))
7

Podriamos decir que el lado negativo de la sentencia armada con if, elif y else es que si la lista de posibles operaciones es muy larga, las tiene que recorrer una por una hasta llegar a la correcta.

Podemos usar un diccionario para el mismo ejemplo:
»> def calculo(op,a,b):
... return {
... 'sum': lambda: a + b,
... 'rest': lambda: a - b,
... 'mult': lambda: a * b,
... 'div': lambda: a/b
... }.get(op, lambda: None)()
»> print(calculo('sum',3,4))
7
De esta manera, si las opciones fueran muchas, no recorrería todas; solo iría directamente a la operación buscada en la última línea ".get(op, lambda: None)()" estamos dando la opción por defecto.

»> conjunto_inmutable = frozenset(["a", "b", "a"]) # Se utiliza una lista como objeto iterable
»> conjunto_inmutable
frozenset(['a', 'b'])
»> conjunto1 = set(["a", "b", "a"]) # Primer conjunto mutable
»> conjunto1
set(['a', 'b'])
»> conjunto2 = set(["a", "b", "c", "d"]) # Segundo conjunto mutable
»> conjunto2
set(['a', 'c', 'b', 'd']) # Recuerda, no mantienen el orden, como los diccionarios
»> conjunto1 & conjunto2 # Intersección
set(['a', 'b'])
»> conjunto1 | conjunto2 # Unión
set(['a', 'c', 'b', 'd'])
»> conjunto1 - conjunto2 # Diferencia (1)
set([])
»> conjunto2 - conjunto1 # Diferencia (2)
set(['c', 'd'])
»> conjunto1 ^ conjunto2 # Diferencia simétrica
set(['c', 'd'])
Una "lista por comprensión" (en inglés: list comprehension) es una expresión compacta para definir "listas". Al igual que codice_5, aparece en lenguajes funcionales. Ejemplos:
»> range(5) # La función "range" devuelve una lista, empezando en 0 y terminando con el número indicado menos uno
[0, 1, 2, 3, 4]
»> [i*i for i in range(5)] # Por cada elemento del rango, lo multiplica por sí mismo y lo agrega al resultado
[0, 1, 4, 9, 16]
»> lista = [(i, i + 2) for i in range(5)]
»> lista
[(0, 2), (1, 3), (2, 4), (3, 5), (4, 6)]


codice_27:

»> def suma(x, y = 2):
... return x + y # Retornar la suma del valor de la variable "x" y el valor de "y"
»> suma(4) # La variable "y" no se modifica, siendo su valor: 2
6
»> suma(4, 10) # La variable "y" sí se modifica, siendo su nuevo valor: 10
14
codice_5:

»> suma = lambda x, y = 2: x + y
»> suma(4) # La variable "y" no se modifica, siendo su valor: 2
6
»> suma(4, 10) # La variable "y" sí se modifica, siendo su nuevo valor: 10
14

»> class Persona():
... def __init__(self, nombre, edad):
... self.nombre = nombre # Un atributo cualquiera
... self.edad = edad # Otro atributo cualquiera
... def mostrar_edad(self): # Es necesario que, al menos, tenga un parámetro, generalmente: "self"
... print(self.edad) # mostrando un atributo
... def modificar_edad(self, edad): # Modificando Edad
... if edad < 0 or edad > 150: # Se comprueba que la edad no sea menor de 0 (algo imposible), ni mayor de 150 (algo realmente difícil)
... return False
... else: # Si está en el rango 0-150, entonces se modifica la variable
... self.edad = edad # Se modifica la edad
»> p = Persona('Alicia', 20) # Instanciar la clase, como se puede ver, no se especifica el valor de "self"
»> p.nombre # La variable "nombre" del objeto sí es accesible desde fuera
'Alicia'
»> p.nombre = 'Andrea' # Y por tanto, se puede cambiar su contenido
»> p.nombre
'Andrea'
»> p.mostrar_edad() # Se llama a un método de la clase
20
»> p.modificar_edad(21) # Es posible cambiar la edad usando el método específico que hemos hecho para hacerlo de forma controlada
»> p.mostrar_edad()
21

Existen muchas propiedades que se pueden agregar al lenguaje importando módulos, que son "minicódigos" (la mayoría escritos también en Python) que proveen de ciertas funciones y clases para realizar determinadas tareas. Un ejemplo es el módulo Tkinter, que permite crear interfaces gráficas basadas en la biblioteca Tk. Otro ejemplo es el módulo "os", que provee acceso a muchas funciones del sistema operativo. Los módulos se agregan a los códigos escribiendo codice_38 seguida del nombre del módulo que queramos usar.

La instalación de módulos en Python se puede realizar mediante la herramienta de software (suele venir incluida con las instalaciones de Python) Pip. Esta herramienta permite la gestión de los distintos paquetes o módulos instalables para Python, incluyendo así las siguientes características:


El módulo os provee funciones para interactuar con el sistema operativo:

»> import os # Módulo que provee funciones del sistema operativo
»> os.name # Devuelve el nombre del sistema operativo
'posix'
»> os.mkdir("/tmp/ejemplo") # Crea un directorio en la ruta especificada
»> import time # Módulo para trabajar con fechas y horas
»> time.strftime("%Y-%m-%d %H:%M:%S") # Dándole un cierto formato, devuelve la fecha y/u hora actual
'2010-08-10 18:01:17'
Para tareas de administración de archivos, el módulo shutil provee una interfaz de más alto nivel:

»> import shutil
»> shutil.copyfile('datos.db', 'informacion.db')
'informacion.db'
»> shutil.move('/build/programas', 'dir_progs')
'dir_progs'

El módulo glob provee una función para crear listas de archivos a partir de búsquedas con comodines en carpetas:

»> import glob
»> glob.glob('*.py')
['numeros.py', 'ejemplo.py', 'ejemplo2.py']

Los argumentos de línea de órdenes se almacenan en el atributo argv del módulo sys como una lista.

»> import sys
»> print(sys.argv)
['demostracion.py', 'uno', 'dos', 'tres']

El módulo math permite acceder a las funciones de matemática de punto flotante:

»> import math
»> math.cos(math.pi / 3)
0,494888338963
»> math.log(1024, 2)
10.0
El módulo random se utiliza para realizar selecciones al azar:

»> import random
»> random.choice(['durazno', 'manzana', 'frutilla'])
'durazno'
»> random.sample(range(100), 10) # elección sin reemplazo
[30, 23, 17, 24, 8, 81, 41, 80, 28, 13]
»> random.random() # un float al azar
0.23370387692726126
»> random.randrange(6) # un entero al azar tomado de range(6)
3
El módulo statistics se utiliza para estadística básica, por ejemplo: media, mediana, varianza, etc.:

»> import statistics
»> datos = [1.75, 2.75, 1.25, 0.5, 0.25, 1.25, 3.5]
»> statistics.mean(datos)
1.6071428571428572
»> statistics.median(datos)
1.25
»> statistics.variance(datos)
1.3720238095238095

El módulo datetime permite manejar fechas y tiempos:

»> from datetime import date
»> hoy = date.today()
»> hoy
datetime.date(2017, 8, 16)

En Python todo es un "objeto" (incluso las clases). Las "clases", al ser "objetos", son instancias de una metaclase. Python además soporta herencia múltiple y polimorfismo.
»> cadena = "abc" # Una cadena es un objeto de "str"
»> cadena.upper() # Al ser un objeto, posee sus propios métodos
'ABC'
»> lista = [True, 3.1415] # Una lista es un objeto de "list"
»> lista.append(42L) # Una lista también (al igual que todo) es un objeto, y también posee sus propios métodos
»> lista
[True, 3.1415, 42L]
Python tiene una gran biblioteca estándar, usada para una diversidad de tareas. Esto viene de la filosofía "pilas incluidas" (""batteries included"") en referencia a los módulos de Python. Los módulos de la biblioteca estándar pueden mejorarse por módulos personalizados escritos tanto en C como en Python. Debido a la gran variedad de herramientas incluidas en la biblioteca estándar, combinada con la habilidad de usar lenguajes de bajo nivel como C y C++, los cuales son capaces de interactuar con otras bibliotecas, Python es un lenguaje que combina su clara sintaxis con el inmenso poder de lenguajes menos elegantes.

Existen diversas implementaciones del lenguaje:

A lo largo de su historia, Python ha presentado una serie de incidencias, de las cuales las más importantes han sido las siguientes:








</doc>
<doc id="2331" url="https://es.wikipedia.org/wiki?curid=2331" title="Préstamo lingüístico">
Préstamo lingüístico

Un préstamo lingüístico es una palabra, morfema o expresión de un idioma que es adoptada por otro idioma. A menudo resulta de la influencia cultural de los hablantes del primer idioma sobre los del segundo. Cuando el elemento prestado es una palabra léxica, normalmente un adjetivo, un nombre o un verbo, hablamos de préstamo léxico. El préstamo léxico es de lejos el más frecuente de todos, pero también existe el préstamo gramatical, que sucede especialmente cuando un número importante de hablantes bilingües de las dos lenguas usan partículas, morfemas y elementos no léxicos de una lengua cuando se habla la otra. Otro fenómeno similar es el calco semántico, en el cual aunque no se toma una expresión literal de otro idioma, sí se toma su significado.

El préstamo léxico es lingüísticamente diferente de otro fenómeno de lenguas “en contacto” llamado calco léxico. En el primer caso, la forma fonética original es simplemente amoldada a la fonología de la lengua receptora sin interpretación o traducción. En el segundo caso existe una adaptación semántica, interpretación o traducción de los morfemas que componían la forma original.

El término "préstamo" está asimismo relacionado con el término "extranjerismo": un préstamo es un extranjerismo incorporado al sistema lingüístico de una lengua determinada.

También podemos hablar de "préstamo" para referirnos a palabras que dentro de una misma lengua pasan de una jerga especializada al registro estándar o viceversa.

En los préstamos lingüísticos se pueden distinguir cuatro tipos, según el grado de adaptación y necesidad de la palabra:


Las comunidades idiomáticas toman formas léxicas de otras lenguas, para describir realidades que en una cultura aparecen por primera vez, pero en la otra son frecuentes y ya poseen un término forjado, términos prestados o préstamos léxicos.
Así, por ejemplo, los conquistadores españoles entraron en contacto en América con fenómenos que bautizaron utilizando palabras indígenas como "hamaca", "patata", "maíz", "huracán", "cacique", "hule" o "tabaco". El espectáculo de desnudarse con gracia, que no tenía equivalente en las costumbres de los países de habla española, exigió el crudo anglicismo "strip-tease" e inversamente, los franceses usaron el calco semántico o traducción de la palabra española "olla podrida" para describir el plato de cocina de ese origen que llaman "pot-pourri", palabra que volvió a España como término popular para denominar lo que es variopinto y multiforme, "popurrí".

Suele producirse en comunidades lingüísticas con un alto grado de bilingüismo. Muchos de los rasgos comunes de las áreas lingüísticas podrían ser resultado del préstamo gramatical; por ejemplo, en el área lingüística balcánica varias lenguas de subfamilias diferentes e ininteligibles entre sí tienen artículos pospuestos y oraciones subordinadas con complementador.




</doc>
<doc id="2337" url="https://es.wikipedia.org/wiki?curid=2337" title="Perl">
Perl

Perl es un lenguaje de programación diseñado por Larry Wall en 1987. Perl toma características del lenguaje C, del lenguaje interpretado bourne shell (sh), AWK, sed, Lisp y, en un grado inferior, de muchos otros lenguajes de programación.

Estructuralmente, Perl está basado en un estilo de bloques como los del C o AWK, y fue ampliamente adoptado por su destreza en el procesado de texto y no tener ninguna de las limitaciones de los otros lenguajes de script.

Larry Wall comenzó a trabajar en Perl en 1987 mientras trabajaba como programador en Unisys y anunció la versión 1.0 en el grupo de noticias comp.sources.misc el 18 de diciembre de 1987. El lenguaje se expandió rápidamente en los siguientes años. Perl 2, publicado en 1988, aportó un mejor motor de expresiones regulares. Perl 3, publicado en 1989, añadió soporte para datos binarios.

Hasta 1991 la única documentación de Perl era una simple (y cada vez más larga) página de manual Unix. En 1991 se publicó 
"Programming Perl" (el libro del camello) y se convirtió en la referencia "de facto" del lenguaje. Al mismo tiempo, el número de versión de Perl saltó a 4, no por marcar un gran cambio en el lenguaje, sino por identificar a la versión que estaba documentada en el libro.

Perl 4 trajo consigo una serie de lanzamientos de mantenimiento, culminando en Perl 4.036 en 1993. En este punto, Larry Wall abandonó Perl 4 para comenzar a trabajar en Perl 5. Perl 4 se quedaría en esa versión hasta hoy.

El desarrollo de Perl 5 continuó en 1994. La lista de correo "perl5-porters" se estableció en mayo de 1994 para coordinar el trabajo de adaptación de Perl 5 a diferentes plataformas. Es el primer foro para desarrollo, mantenimiento y adaptación de Perl 5.

Perl 5 fue publicado el 17 de octubre de 1994. Fue casi una completa reescritura del intérprete y añadió muchas nuevas características al lenguaje, incluyendo objetos, referencias, paquetes y módulos. A destacar, los módulos proveen de un mecanismo para extender el lenguaje sin modificar el intérprete. Esto permitió estabilizar su núcleo principal, además de permitir a los programadores de Perl añadirle nuevas características.
El 26 de octubre de 1995, se creó el "Comprehensive Perl Archive Network" (CPAN). CPAN es una colección de sitios web que almacenan y distribuyen fuentes en Perl, binarios, documentación, scripts y módulos. Originalmente, cada sitio CPAN debía ser accedido a través de su propio URL; hoy en día, http://www.cpan.org redirige automáticamente a uno de los cientos de repositorios espejo de CPAN.

En 2008, Perl 5 continúa siendo mantenido. Características importantes y algunas construcciones esenciales han sido añadidas, incluyendo soporte Unicode, hilos, un soporte importante para la programación orientada a objetos y otras mejoras.

Perl se llamó originalmente "Pearl". Larry Wall quería darle al lenguaje un nombre corto con connotaciones positivas; asegura que miró (y rechazó) todas las combinaciones de tres y cuatro letras del diccionario. También consideró nombrarlo como su esposa Gloria. Wall descubrió antes del lanzamiento oficial que ya existía un lenguaje de programación llamado PEARL y cambió la ortografía del nombre.

El nombre normalmente comienza con mayúscula ("Perl") cuando se refiere al lenguaje y con minúscula ("perl") cuando se refiere al propio programa intérprete debido a que los sistemas de ficheros Unix distinguen mayúsculas y minúsculas. Antes del lanzamiento de la primera edición de "Programming Perl" era común referirse al lenguaje como "perl"; Randal L. Schwartz, sin embargo, forzó el nombre en mayúscula en el libro para que destacara mejor cuando fuera impreso. La distinción fue subsiguientemente adoptada por la comunidad.

El nombre es descrito ocasionalmente como "PERL" (por Practical Extraction and Report Language - "Lenguaje Práctico para la Extracción e Informe"). Aunque esta expansión ha prevalecido en muchos manuales actuales, incluyendo la página de manual de Perl, es un retroacrónimo y oficialmente el nombre no quiere decir nada. La ortografía de PERL en mayúsculas es por eso usada como jerga para detectar a individuos ajenos a la comunidad. Sin embargo, se han sugerido varios retroacrónimos, incluyendo el cómico "Pathologically Eclectic Rubbish Lister" (Contabilizador de Basura Patológicamente Ecléctico).

Perl se simboliza generalmente por un camello de una sola joroba (camello arábigo o dromedario), que fue la imagen elegida por el editor O'Reilly para la cubierta de "Programming Perl", que por consiguiente adquirió el nombre de "El Libro del Camello". O'Reilly es propietario de este símbolo como marca registrada, pero dice que usa sus derechos legales solo para proteger la ""integridad e impacto de este símbolo"".
O'Reilly permite el uso no comercial del símbolo, y ofrece logos Programming Republic of Perl y botones Powered by Perl.

Desde hace unos años, y para evitar este tipo de problemas con la licencia comercial, la Fundación Perl elaboró nuevos logotipos basados en una cebolla, a raíz de las conferencias anuales que Larry Wall ofrece con el título: "State of the Onion" ("Estado de la Cebolla") "Onion" se pronuncia muy parecido a "Union", por lo que suena parecido a "State of the Union" ("Estado de la Unión"), evento en el cual el Presidente de los Estados Unidos informa a los ciudadanos del estado en que se encuentra su país.

La página de manual Unix perlintro(1) dice:

La estructura completa de Perl deriva ampliamente del lenguaje C. Perl es un lenguaje imperativo, con variables, expresiones, asignaciones, bloques de código delimitados por llaves, estructuras de control y subrutinas.

Perl también toma características de la programación shell. Todas las variables son marcadas con un Sigilo precedente ("Sigil", en inglés). Los sigilos identifican inequívocamente los nombres de las variables, permitiendo a Perl tener una rica sintaxis. Notablemente, los sigilos permiten interpolar variables directamente dentro de las cadenas de caracteres ("string"). Como en los shell, Perl tiene muchas funciones integradas para tareas comunes y para acceder a los recursos del sistema.

Perl toma las listas del Lisp, "hash" (memoria asociativa) del AWK y expresiones regulares del sed. Todo esto simplifica y facilita todas las formas del análisis sintáctico, manejo de texto y tareas de gestión de datos.

En Perl 5, se añadieron características para soportar estructuras de datos complejas, funciones de primer orden (p. e. clausuras como valores) y un modelo de programación orientada a objetos. Estos incluyen referencias, paquetes y una ejecución de métodos basada en clases y la introducción de variables de ámbito léxico, que hizo más fácil escribir código robusto (junto con el pragma strict). Una característica principal introducida en Perl 5 fue la habilidad de empaquetar código reutilizable como módulos. Larry Wall indicó más adelante que "la intención del sistema de módulos de Perl 5 era apoyar el crecimiento de la cultura Perl en vez del núcleo de Perl".

Todas las versiones de Perl hacen el tipificado automático de datos y la gestión de memoria. El intérprete conoce el tipo y requerimientos de almacenamiento de cada objeto en el programa; reserva y libera espacio para ellos según sea necesario. Las conversiones legales de tipo se hacen de forma automática en tiempo de ejecución; las conversiones ilegales son consideradas errores fatales.

El diseño de Perl puede ser entendido como una respuesta a tres amplias tendencias de la industria informática: rebaja de los costes en el hardware, aumento de los costes laborales y las mejoras en la tecnología de compiladores. Anteriormente, muchos lenguajes de ordenador como Fortran y C, fueron diseñados para hacer un uso eficiente de un hardware caro. En contraste, Perl es diseñado para hacer un uso eficiente de los costosos programadores de ordenador.

Perl tiene muchas características que facilitan la tarea del programador a costa de unos requerimientos de CPU y memoria mayores. Estas incluyen gestión de memoria automática; tipo de dato dinámico; "strings", listas y "hashes"; expresiones regulares; introspección y una función codice_1.

Larry Wall fue adiestrado como lingüista y el diseño de Perl ha sido muy aleccionado con principios lingüísticos. Ejemplos incluyen la Codificación Huffman (las construcciones más comunes deben ser las más cortas), buena distribución (la información importante debe ir primero) y una larga colección de primitivas del lenguaje. Perl favorece las construcciones del lenguaje, tan naturales, como para los humanos son la lectura y la escritura, incluso si eso hace más complicado al intérprete Perl. 

La sintaxis de Perl refleja la idea de que "cosas que son diferentes deben parecer diferentes". Por ejemplo, escalares, "arrays" y "hashes" tienen diferente sigilo. Índices de "array" y claves "hash" usan diferentes clases de paréntesis. "Strings" y expresiones regulares tienen diferentes delimitadores estándar. Esta aproximación puede contrastarse con lenguajes como Lisp, donde la misma construcción "S-expresión" y sintaxis básica se usa para muchos y variados propósitos.

Perl tiene características que soportan una variedad de paradigmas de programación, como la imperativa, funcional y la orientada a objetos. Al mismo tiempo, Perl no obliga a seguir ningún paradigma en particular, ni obliga al programador a elegir alguna de ellas. 

Hay un amplio sentido de lo práctico, tanto en el lenguaje Perl como en la comunidad y la cultura que lo rodean. El prefacio de "Programming Perl" comienza con, "Perl es un lenguaje para tener tu trabajo terminado". Una consecuencia de esto es que Perl no es un lenguaje ordenado. Incluye características si la gente las usa, tolera excepciones a las reglas y emplea la heurística para resolver ambigüedades sintácticas. Debido a la naturaleza indulgente del compilador, a veces los errores pueden ser difíciles de encontrar. Hablando del variado comportamiento de las funciones internas en los contextos de lista y escalar, la página de manual de codice_2 dice "En general, hacen lo que tu quieras, siempre que quieras la coherencia."

Perl tiene varios lemas que transmiten aspectos de su diseño y uso. Uno es "There's more than one way to do it" (Hay más de una forma de hacerlo) (TMTOWTDI, usualmente pronunciado 'Tim Toady'). Otros son ""Perl: la motosierra del ejército Suizo de los lenguajes de programación"" y ""Límites imprecisos"". Una meta prefijada de Perl es hacer las cosas fáciles de forma fácil y las tareas difíciles, posibles. A Perl también se le ha llamado ""El esparadrapo de Internet"".

Perl tiene muchas y variadas aplicaciones, gracias a la disponibilidad de muchos módulos estándares y de terceras partes.

Se ha usado desde los primeros días del Web para escribir guiones ("scripts") CGI. Es una de las "tres Pes" (Perl, Python y PHP), que son los lenguajes más populares para la creación de aplicaciones Web, y es un componente integral de la popular solución LAMP para el desarrollo web. Grandes proyectos escritos en Perl son Slash, IMDb y UseModWiki, un motor de Wiki. Muchos sitios web con alto tráfico, como Amazon.com y Ticketmaster.com usan Perl extensamente.

Perl se usa a menudo como un "lenguaje pegamento", ligando sistemas e interfaces que no fueron diseñados específicamente para interoperar; y para el "escarbado de datos", convirtiendo o procesando grandes cantidades de datos para tareas como por ejemplo crear informes. De hecho, estas fortalezas están íntimamente unidas. Su combinación hace a Perl una popular herramienta de propósito general para los administradores de sistemas, especialmente en programas pequeños que pueden ser escritos y ejecutados en una sola línea de comandos.

Perl es también ampliamente usado en finanzas y bioinformática, donde es apreciado por su desarrollo rápido, tanto de aplicaciones como de despliegue, así como la habilidad de manejar grandes volúmenes de datos.

Perl está implementado como un intérprete, escrito en C, junto con una gran colección de módulos, escritos en Perl y C. La distribución fuente tiene, en 2005, 12 MB cuando se empaqueta y comprime en un fichero tar. El intérprete tiene 150.000 líneas de código C y se compila en un ejecutable de 1 MB en las arquitecturas de hardware más típicas. De forma alternativa, el intérprete puede ser compilado como una biblioteca y ser embebida en otros programas. Hay cerca de 500 módulos en la distribución, sumando 200.000 líneas de Perl y unas 350.000 líneas adicionales de código C. Mucho del código C en los módulos consiste en tablas de codificación de caracteres.

El intérprete tiene una arquitectura orientada a objetos. Todos los elementos del lenguaje Perl —escalares, listas, "hashes", referencias a código, manejadores de archivo— están representados en el intérprete como estructuras C. Las operaciones sobre estas estructuras están definidas como una numerosa colección de macros, typedef y funciones; esto constituye la API C de Perl. La API Perl puede ser desconcertante para el no iniciado, pero sus puntos de entrada siguen un esquema de nombres coherente, que ayuda a los que quieran utilizarla.

La ejecución de un programa Perl se puede dividir, generosamente, en dos fases: tiempo de compilación y tiempo de ejecución. En tiempo de compilación el intérprete "parsea" el texto del programa en un árbol sintáctico. En tiempo de ejecución, ejecuta el programa siguiendo el árbol. El texto es parseado solo una vez y el árbol sintáctico es optimizado antes de ser ejecutado, para que la fase de ejecución sea relativamente eficiente. Las optimizaciones del árbol sintáctico en tiempo de compilación incluyen , propagación del contexto y optimización en trozos sueltos de código. Sin embargo, las fases de compilación y ejecución pueden anidarse: un bloque codice_3 se ejecuta en tiempo de compilación, mientras que una función codice_4 inicia una compilación durante una ejecución. Ambas operaciones están implícitas en otras -de forma notable, la cláusula codice_5 que carga bibliotecas, conocidas en Perl como módulos, implica un bloque codice_3.

Perl es un lenguaje dinámico y tiene una gramática sensible al contexto que puede quedar afectada por el código ejecutado durante una fase de ejecución intermedia (Ver ejemplos.). Por eso Perl no puede ser parseado mediante una aplicación directa de analizadores sintácticos/parseadores Lex/Yacc. En cambio, el intérprete implementa su propio analizador léxico, que coordinado con un parseador modificado GNU bison resuelve las ambigüedades del lenguaje. Se ha dicho que "solo perl puede parsear Perl", queriendo decir que solo el intérprete Perl ("perl") puede parsear el lenguaje Perl ("Perl"). La razón de esto se atestigua por las persistentes imperfecciones de otros programas que emprenden la tarea de parsear Perl, como los analizadores de código y los auto-indentadores, que tienen que vérselas no solo con las muchas formas de expresar inequívocamente construcciones sintácticas, sino también con el hecho de que también Perl no puede, en general, ser parseado sin antes ser ejecutado.

El mantenimiento del intérprete Perl, a lo largo de los años, se ha vuelto cada vez más difícil. El núcleo ha estado en continuo desarrollo desde 1994. El código ha sido optimizado en rendimiento a expensas de la simplicidad, claridad y unas interfaces internas más fuertes. Nuevas características han sido añadidas, manteniendo todavía, compatibilidad virtualmente completa hacia atrás con las primeras versiones. El tamaño y la complejidad del intérprete son una barrera para los desarrolladores que desean trabajar en él.

Perl es distribuido con unos 120.000 test funcionales. Estos se ejecutan como parte del proceso normal de construcción y comprueban extensamente al intérprete y a sus módulos principales. Los desarrolladores Perl confían en los test funcionales para asegurarse que los cambios en el intérprete no introducen errores; recíprocamente, los usuarios Perl que vean al intérprete pasar los test funcionales en su sistema pueden tener un alto grado de confianza de que está funcionando adecuadamente.

No hay una especificación o estándar escrito para el lenguaje Perl y no hay planes de crear uno para la versión actual de Perl. Siempre ha existido solo una implementación del intérprete. Este intérprete, junto con los test funcionales, forman la especificación "de facto" del lenguaje.

Perl es software libre y está licenciado bajo la Licencia Artística y la GNU General Public License. Existen distribuciones disponibles para la mayoría de sistemas operativos. Está especialmente extendido en Unix y en sistemas similares, pero ha sido portado a las plataformas más modernas (y otras más obsoletas). Con solo seis excepciones confirmadas, puede ser compilado desde el código fuente en todos los Unix, compatibles POSIX o cualquier otra plataforma Unix compatible. Sin embargo, esto no es normalmente necesario, porque Perl está incluido por defecto en la instalación de los sistemas operativos más populares.

Debido a los cambios especiales necesarios para soportar al Mac OS Classic, existe una adaptación especial llamada MacPerl.

Perl está preinstalado en las distribuciones más populares de GNU/Linux incluyendo Gentoo, Slackware, Mandriva, Debian, RedHat y SUSE.

Los usuarios de Microsoft Windows normalmente instalan una distribución binaria de Perl. Compilar Perl desde el código fuente bajo Windows es posible, pero la mayoría de las instalaciones no disponen del necesario compilador de C.

La capa de emulación Cygwin proporciona otra forma de correr Perl bajo Windows. Cygwin proporciona en entorno parecido al Unix en Windows que incluye gcc, por lo que compilar Perl desde el código es una opción accesible para los usuarios que prefieren esta opción.

En junio de 2006, win32.perl.org fue lanzado por Adam Kennedy en nombre de la Fundación Perl. Es una comunidad web "para todo lo relacionado con Windows y Perl".

En Perl, el programa canónico "Hola mundo" es:
print "¡Hola mundo!\n";
La primera línea contiene el "shebang" (par de caracteres que identifica el texto que sigue), que le indica al sistema operativo dónde encontrar el intérprete de Perl. La segunda imprime el "string" "¡Hola mundo!" y un carácter de nueva línea.

El "shebang" es la forma normal para invocar al intérprete en los sistemas Unix. Los sistemas Windows pueden seguir utilizándolo o pueden asociar la extensión de archivo .pl con el intérprete Perl. Algunos editores de texto también usan la línea "shebang" como una pista sobre el modo de trabajo en que deben operar. Si el programa es ejecutado por perl y no invocado por el shell, la línea que empieza por el "shebang" es parseada para interpretar las opciones. En otro caso, es ignorada. Para ver los detalles de esto, consultar la página de manual perlrun.

Perl tiene tres tipos de datos: escalares, 
listas y hashes:

Todas las variables están precedidas por un sigilo, que identifica el tipo de dato que es accedido (no el tipo de dato de la misma variable). Se puede usar el mismo nombre para variables de diferentes tipos, sin que tengan conflictos.

$var # un escalar
@var # un array
%var # un hash

Los números se escriben de la forma usual; los "strings" están rodeados entre comillas de varias clases.

$n = 42;
$nombre = "juan";
$color = 'rojo';

Perl convertirá los "strings" en números y viceversa dependiendo del contexto en que sean usados. En el siguiente ejemplo los "strings" codice_7 y codice_8 son tratados como números cuando son argumentos del operador suma. Este código imprime el número '5', desechando cualquier información no numérica de la operación y dejando los valores de las variables intactos. (El operador de concatenación no es codice_9, sino codice_10.)

$n = "3 manzanas";
$m = "2 naranjas";
print $n + $m;

Perl también tiene un contexto booleano que utiliza en la evaluación de declaraciones condicionales. Los siguientes valores en Perl evalúan todos como falso:

$falso = 0; # el número cero
$falso = 0.0; # el número cero como flotante
$falso = '0'; # el string cero
$falso = ""; # el string vacío
$falso = undef; # el valor devuelto por undef

Todos los demás valores se evalúan a verdadero. Esto incluye el curioso "string" auto-descriptivo codice_11, que de hecho es 0 como número, pero verdadero como booleano. (Cualquier "string" no numérico también tendrá esta propiedad, pero este "string" en particular es ignorado por Perl en contextos numéricos). Las expresiones booleanas evaluadas también devuelven valores escalares. Aunque la documentación no indica qué valor "particular" se devuelve como verdadero o falso (y por lo tanto no fiable), muchos operadores booleanos devuelven 1 por verdadero y el "string" vacío para falso (que evalúa a cero en contexto numérico). La función "defined()" le dice si la variable tiene algún valor. En el ejemplo anterior "defined($falso)" será verdadero con cada uno de los valores anteriores, excepto "undef".
Si, específicamente, quiere asegurarse de tener un resultado 1/0 (como en C), necesita realizar el cálculo siguiente:

my $resultado_real = $resultado_booleano ? 1 : 0;

Una lista se define listando sus elementos, separados por comas y rodeados por paréntesis donde así sea requerido por la precedencia de los operadores.

@puntuaciones = (32, 45, 16, 5);

Un "hash" puede ser inicializado desde una lista de pares clave/valor.

%favorito = (

Los elementos individuales de una lista son accedidos utilizando un índice numérico, dentro de corchetes. Valores individuales en un hash son accedidos utilizando la correspondiente clave, dentro de llaves. El sigilo $ identifica que el elemento accedido es un escalar.

$puntuaciones[2] # un elemento de @puntuaciones
$favorito{joe} # un valor de %favorito

Múltiples elementos pueden ser accedidos usando en su lugar el sigilo @ (identificando el resultado como una lista).

@puntuaciones[2, 3, 1] # tres elementos de @puntuaciones
@favorito{'joe', 'sam'} # dos valores de %favorito

El número de elementos en un array puede ser obtenido evaluando el "array" en contexto escalar o con la ayuda del sigilo $#. Este último da el índice del último elemento dentro del "array", no el número de elementos.

$numero = @amigos;
$#amigos; # el índice del último elemento en @amigos
$#amigos+1; # normalmente el número de elementos en @amigos,

Hay unas pocas funciones que operan sobre "hashes" enteros.

@nombres_de_clientes = keys %direcciones; # guarda en @nombres_de_clientes todas las claves de %direcciones
@direcciones_de_email = values %direcciones; # guarda en @direcciones_de_email todos los valores de %direcciones

Perl tiene varias clases de estructuras de control.

Tiene estructuras de control orientado al bloque, similar a los de los lenguajes de programación C y Java. Las condiciones están rodeadas por paréntesis y los bloques subordinados por llaves:

Cuando se controla a una sola declaración, los modificadores de declaración proporcionan una sintaxis más ligera:

Los operadores lógicos "cortocircuito" son normalmente usados para controlar el flujo del programa a nivel de expresión:

Las palabras clave de control de flujo codice_12, codice_13, codice_14 y codice_15 son expresiones, por lo que pueden ser usadas con los operadores cortocircuito.

Perl también tiene dos construcciones implícitas para bucles:

codice_16 devuelve todos los elementos de "lista" en que el bloque subordinado evalúa a verdadero. codice_17 evalúa el bloque subordinado por cada elemento de "lista" y devuelve una lista de los valores resultantes. Estas construcciones permiten un estilo simple de programación funcional.

La declaración switch (llamada "given"/"when") existe desde la versión 5.10:

Perl incluye una declaración codice_18, pero es usada raramente. Las situaciones donde en otros lenguajes se utiliza codice_19 no ocurren tan a menudo en Perl debido a sus amplias opciones de control de flujo.

Existe también una declaración codice_20 que realiza una llamada 'final'. Termina la subrutina actual e inmediatamente llama a la codice_21 especificada. Esto se usa en situaciones donde una nueva subrutina puede realizar una gestión de la pila más eficiente que el propio Perl (porque típicamente no se requiere ningún cambio en la pila actual), y en una recursión muy profunda este tipo de llamadas puede tener un sustancial impacto positivo en el funcionamiento porque evita la sobrecarga de la gestión contexto/pila en el momento de retornar.

Las subrutinas se definen con la palabra clave codice_22 e invocadas simplemente nombrándolas. Si la subrutina en cuestión no ha sido todavía declarada, es necesario, para el proceso de análisis sintáctico, poner los paréntesis.

foo(); # paréntesis necesarios aquí...
foo; #... pero no aquí

Una lista de argumentos pueden ser indicados después del nombre de la subrutina. Los argumentos pueden ser escalares, listas o "hashes".

foo $x, @y, %z;

Los parámetros de una subrutina no necesitan ser declarados, ni en número ni en tipo; de hecho, pueden variar en cada llamada. Los "arrays" son expandidos a sus elementos, los "hashes" a una lista de pares clave/valor y todo el conjunto es pasado a la subrutina como una indiferenciada lista de escalares.

Cualesquiera de los argumentos pasados están disponibles para la subrutina en el "array" especial codice_23. Los elementos de codice_23 son asociados a los argumentos actuales; cambiando un elemento de codice_23 cambia el argumento correspondiente.

Los elementos de codice_23 pueden ser accedidos con los subíndices de la forma normal.

$_[0], $_[1]

Sin embargo, el código resultante puede ser difícil de leer y los parámetros tener una semántica de pase por referencia, que puede resultar algo no deseable.

Un modismo común es asignar codice_23 a una lista de variables con nombres.

my($x, $y, $z) = @_;

Esto afecta tanto a la mnemónica de los nombres de los parámetros como a la semántica de los valores pasados por valor. La palabra clave codice_28 indica que las siguientes variables están léxicamente embebidas en el bloque que las contienen.

Otro modismo es sacar los parámetros de codice_23. Esto es muy común cuando la subrutina toma un solo argumento.

my $x = shift; # Si no se dice nada, nos referimos a @_

Las subrutinas pueden devolver valores.

return 42, $x, @y, %z;

Si la subrutina no sale vía declaración codice_14, entonces devuelve la última expresión evaluada en el cuerpo de la subrutina. "Arrays" y "hashes" en el valor de retorno son expandidos a una lista de escalares, igual que si fueran argumentos de una función.

La expresión devuelta es evaluada en el contexto de la llamada de la subrutina; esto puede sorprender al desprevenido.

$x = lista; # devuelve 6 - último elemento de la lista
$x = array; # devuelve 3 - número de elementos de la lista
@x = lista; # devuelve (4, 5, 6)
@x = array; # devuelve (4, 5, 6)

Una subrutina puede descubrir su contexto de llamada con la función codice_31.

$x = cualquiera; # devuelve "Naranjas"
@x = cualquiera; # devuelve (1, 2)

El lenguaje Perl incluye una sintaxis especializada para escribir expresiones regulares y el intérprete contiene un motor para emparejar "strings" con expresiones regulares. El motor de expresiones regulares usa un algoritmo de Vuelta Atrás (), extendiendo sus capacidades desde el simple emparejamiento de patrones simples con la captura y sustitución de "strings". El motor de expresiones regulares se deriva de "regex", escrito por .

La sintaxis de expresiones regulares fue originalmente tomada de las expresiones regulares de Unix Versión 8. Sin embargo, se diferenció ya antes del primer lanzamiento de Perl y desde entonces ha ido incorporando muchas más características. Otros lenguajes y aplicaciones están adoptando las expresiones regulares de Perl (PCRE) en vez de las expresiones regulares POSIX, incluyendo PHP, Ruby, Java y el Servidor HTTP Apache.

El operador codice_32 (empareja) permite comprobar un emparejamiento por medio de una expresión regular. (Para abreviar, el precedente codice_33 puede ser omitido.) En el caso más simple, una expresión como:

$x =~ m/abc/

evalúa a verdadero si y solo si el "string" codice_34 empareja con la expresión regular codice_35.

Partes de la expresión regular pueden ser incluidas entre paréntesis: las partes correspondientes de un string emparejado son "capturadas". Los "strings" capturados son asignados de forma secuencial a las variables internas codice_36 y una lista de "strings" capturados se devuelve como valor del emparejamiento.

$x =~ m/a(.)c/; # captura el carácter entre 'a' y 'c' y lo guarda en $1

El operador codice_37 (sustitución) especifica una operación de búsqueda y reemplazo:

$x =~ s/abc/aBc/; # Convierte la b en mayúscula

Las expresiones regulares en Perl pueden tomar unos "modificadores". Son sufijos de una sola letra que modifican el significado de la expresión:

$x =~ m/abc/i; # emparejamiento independientemente de si están en mayúscula o minúscula
$x =~ s/abc/aBc/g; # búsqueda y reemplazo global (a lo largo de todo el "string") 

Las expresiones regulares pueden ser densas y crípticas. Esto es porque la sintaxis de las expresiones regulares es extremadamente compacta, generalmente usando caracteres sueltos o pares de caracteres que representan sus operaciones. Perl alivia un poco este problema con el modificador codice_38 que permite a los programadores poner espacio en blanco y comentarios "dentro" de las expresiones regulares:

$x =~ m/a # empareja una 'a'

Un uso común de las expresiones regulares es el de especificar delimitadores de campos al operador codice_39:

@palabras = split m/,/, $línea; # divide la $línea de valores separados por comas

El operador codice_39 complementa la captura de string. La captura de string devuelve las partes de un string que emparejan con una expresión regular; codice_39 devuelve las partes que no emparejan.

Perl está ampliamente favorecido para las aplicaciones de bases de datos. Sus facilidades de manejo de texto son buenas para generar consultas SQL; "arrays", "hashes" y la gestión de memoria automática hace fácil recoger y procesar los datos devueltos.

En las primeras versiones de Perl, las interfaces de bases de datos fueron creadas enlazando el intérprete con una biblioteca de base de datos desde el lado del cliente. Esto era algo torpe; un problema en particular fue que el ejecutable codice_42 resultante estaba restringido a usar solo una interfaz de base de datos, la que había sido enlazada. También, reenlazar el intérprete era lo suficientemente dificultoso que solo fue realizado para algunas de las más famosas e importantes bases de datos.

En Perl 5, las interfaces de bases de datos están implementadas por el módulo Perl DBI. El módulo codice_43 presenta una única interfaz a las aplicaciones Perl, independiente de las bases de datos, mientras que los módulos codice_44 (Controlador de base de datos) manejan los detalles de acceso a unas 50 bases de datos diferentes. Existen controladores codice_44 para la mayor parte de las bases de datos ANSI SQL.

El "Computer Language Shootout Benchmarks" compara el funcionamiento de implementaciones de problemas de programación típicos, en diversos lenguajes. Sus implementaciones Perl normalmente toman más memoria que las implementaciones en otros lenguajes, y esto varía los resultados de velocidad. Las prestaciones de Perl son similares a las de otros lenguajes como Python, PHP o Ruby, pero más lento que la mayor parte de lenguajes compilados.

Perl puede ser más lento que otros lenguajes haciendo lo mismo porque tiene que compilar el código fuente cada vez que corre el programa. En "A Timely Start", Jean-Louis Leroy encontró que sus scripts en Perl tardaban mucho más tiempo en correr que lo que él esperaba porque el intérprete perl perdía la mayor parte del tiempo buscando y compilando los módulos. Como Perl no puede salvar su compilación intermedia como lo hacen Java, Python y Ruby, los scripts Perl conllevan esta sobrecarga en cada ejecución. La sobrecarga no es un problema cuando la fase de ejecución es muy larga, pero puede sesgar significativamente en tiempos de ejecución muy cortos, como se encuentra a menudo en los benchmarks. Una vez que perl inicia la fase de ejecución, sin embargo, puede ser muy rápido y típicamente supera a otros lenguajes dinámicos. Tecnologías como mod perl superan esto guardando el programa compilado en memoria entre ejecuciones, o Class::Autouse que retrasa la compilación de partes del programa hasta que son necesarias.

Nicholas Clark, un desarrollador principal de Perl, diserta sobre algunas mejoras en el diseño con Perl y otras soluciones en "When perl is not quite fast enough". Las rutinas más críticas de un programa Perl pueden escribirse en C o incluso en lenguaje ensamblador con o Inline.

Optimizar Perl puede requerir un conocimiento íntimo de su funcionamiento en lugar de la destreza con el lenguaje y su sintaxis, significando que el problema está con la implementación de Perl en lugar de con el lenguaje mismo. Raku, la siguiente versión, tomará algunas de estas ideas como lecciones, que otros lenguajes ya han aprendido.

En el año 2000, en la conferencia Perl, Jon Orwant hizo una petición para una nueva versión del lenguaje. Esto condujo a una decisión para comenzar el trabajo del rediseño del lenguaje, que se llamaría Perl 6. Se pidieron propuestas a la comunidad Perl para las nuevas características del lenguaje, y se recibieron más de 300 RFC (del inglés "Request For Comments", petición de comentarios).

Larry Wall estuvo los siguientes años digiriendo los RFC y sintetizándolos en un entorno de trabajo coherente para Perl 6. Presentó su diseño de Perl 6 en una serie de documentos llamados Apocalipsis, que se numeran para corresponderse con los capítulos de "Programming Perl". La actual, inacabada especificación de Perl 6, está resumida en unos documentos de diseño llamados Sinopsis, que están numerados para corresponder a los Apocalipsis.

Perl 6 no tiene la intención de ser compatible para atrás, aunque existirá un modo de compatibilidad.

En 2001 se decidió que Perl 6 corriese en una máquina virtual llamada Parrot. Esto quiere decir que otros lenguajes que usen Parrot podrán ganar acceso nativo a CPAN y permitirá algún cierto nivel de desarrollo cruzado.

En 2005 Audrey Tang creó el proyecto pugs, una implementación de Perl 6 en Haskell. Fue y es una plataforma de testeo del lenguaje Perl 6 (separado del desarrollo actual de la implementación) permitiendo a los diseñadores explorar. El proyecto pugs engendró una activa comunidad Perl/Haskell centrada alrededor del canal irc #perl6 en Freenode.

Un cierto número de características en el lenguaje Perl 6 muestran una similitud con las de Haskell y Perl 6 ha sido acogido por la comunidad Haskell como un potencial lenguaje de scripting.

En el 2006, Perl 6, Parrot y pugs siguen estando bajo desarrollo y un nuevo módulo para Perl 5 llamado v6 permite a una parte del código Perl 6 correr directamente sobre Perl 5.

En octubre de 2019, ante la evidencia de que Perl 6 es un lenguaje muy diferente de Perl 5, y para que no se generara una confusión entre los dos lenguajes, se decidió renombrarlo a Raku.

Como en C, las competiciones de código ofuscado son un rasgo popular de la cultura Perl. El homenajea la virtud de la flexibilidad sintáctica de Perl. El programa siguiente imprime el texto "Just another Perl / Unix hacker", usando 32 procesos en paralelo coordinados con pipes. La explicación completa está disponible en el sitio web del autor.

@P=split//,".URRUU\c8R";@d=split//,"\nrekcah xinU / lreP rehtona tsuJ";sub p{
@p{"r$p","u$p"}=(P,P);pipe"r$p","u$p";++$p;($q*=2)+=$f=!fork;map{$P=$P[$f^ord
($p{$_})&6];$p{$_}=/ ^$P/ix?$P:close$_}keys%p}p;p;p;p;p;map{$p{$_}=~/^[P.]/&&
close$_}%p;wait until$?;map{/^r/&&<$_>}%p;$_=$d[$q];sleep rand(2)if/\S/;print

De forma similar al código ofuscado pero con un propósito diferente, "Poesía Perl" es la práctica de escribir poemas que puedan ser compilados como código legal de Perl (aunque generalmente sin sentido). Esta afición es más o menos única en Perl debido al gran número de palabras normales del Inglés que el lenguaje utiliza. Los nuevos poemas se publican de forma regular en el sitio Perl Monks, en la sección Poesía Perl. Parte del saber de Perl está en Black Perl, un ejemplo infame de la poética de Perl.

Otro pasatiempo es "Perl Golf". Como con el deporte real, la meta es reducir el número de golpes necesarios para completar un objetivo, pero aquí, los "golpes" se refieren a los pulsaciones de teclado en vez de a los recorridos en un club de golf. Se propone una tarea, como "escanear un string de entrada y devolver el palíndromo más grande que contenga" y los participantes intentan batir a sus oponentes escribiendo soluciones que requieran cada vez menos caracteres de código fuente Perl.

Otra tradición entre los "hacker" Perl es escribir JAPH, que es una especie de pequeños programas ofuscados que imprimen la frase "Just another Perl hacker,". El "canónico" JAPH incluye la coma al final, aunque a menudo se omite, y otras muchas variantes que se han creado (ejemplo, que imprime "Just Another Perl Pirate!").

Un módulo Perl interesante es Lingua::Romana::Perligata (en CPAN). Este módulo traduce el código fuente de un script escrito en Latín a Perl, permitiendo al programador escribir programas ejecutables en Latín.

La comunidad Perl ha reservado el espacio de nombres "Acme" para los módulos que sean divertidos o experimentales. Algunos de los módulos Acme están implementados de maneras muy entretenidas. Algunos ejemplos:


La comunidad Perl está constituida por grupos que a su vez conforman Perl Mongers.

En España hay tres grupos:





</doc>
<doc id="2341" url="https://es.wikipedia.org/wiki?curid=2341" title="Positivismo">
Positivismo

El positivismo o filosofía positiva es una corriente filosófica que afirma que el único conocimiento auténtico es el conocimiento científico y que tal conocimiento solo puede surgir del método científico, siendo el ejemplo ideal las ciencias físicas que triunfan claramente en el dominio de la naturaleza y en las aplicaciones técnicas que de ella se derivan.

Como consecuencia de esta postura, los positivistas critican la metafísica como pseudociencia por buscar lo que está más allá de la ciencia. Una línea de crítica fue a partir de lo que se llamó las «trampas del lenguaje», lo que supuso un interés en el estudio del lenguaje tanto en su dimensión formal, empirismo lógico, como en cuanto lenguaje natural, estudiando los «juegos del lenguaje», y dio lugar a la filosofía analítica.

El positivismo deriva del empirismo y de la epistemología que surge a inicios del siglo XIX de la mano de los pensadores franceses Henri de Saint-Simon y Auguste Comte, y el británico John Stuart Mill. Se extiende y desarrolla por el resto de Europa en la segunda mitad del siglo XIX. Desde un positivismo extremo hasta un positivismo casi idealista, el siglo XIX y comienzos del XX ofrecen un riquísimo panorama de autores y escuelas todas bajo denominación positivista. El rasgo común que caracteriza a todos ellos es la aceptación del conocimiento científico como única forma de conocimiento legítimo y el rechazo a la metafísica como pseudociencia.

El positivismo surgió como manera de legitimar el estudio científico naturalista del ser humano, tanto individual como colectivamente. Según distintas versiones, la necesidad de estudiar científicamente al ser humano nace debido a la experiencia sin parangón que fue la Revolución francesa, que obligó por primera vez a ver a la sociedad y al individuo como objetos.

La aplicación de la filosofía positiva a las diferentes ciencias y objetos de investigación, así como la rigidez en que se consideren los principios empiristas, dieron lugar a muy diversos tipos de empirismos y positivismos.

Estas corrientes tienen como características diferenciadoras la defensa de un monismo metodológico (teoría que afirma que hay un solo método aplicable en todas las ciencias). La explicación científica ha de tener la misma forma en cualquier ciencia si se aspira a ser ciencia, específicamente el método de estudio de las ciencias físico-naturales. A su vez, el objetivo del conocimiento para el positivismo es explicar causalmente los fenómenos por medio de leyes generales y universales, lo que le lleva a considerar a la razón como medio para otros fines (razón instrumental). La forma que tiene de conocer es inductiva, despreciando la creación de teorías a partir de principios que no han sido percibidos objetivamente. En metodología histórica, el positivismo prima fundamentalmente las pruebas documentadas, minusvalorando las interpretaciones generales, por lo que los trabajos de esta naturaleza suelen tener excesiva acumulación documental y escasa síntesis interpretativa.

Auguste Comte formuló a mediados del siglo XIX la idea de la creación de la sociología como ciencia que tiene a la sociedad como su objeto de estudio. La sociología sería un conocimiento libre de todas las relaciones con la filosofía y basada en datos empíricos en igual medida que las ciencias naturales.

Una de sus propuestas más destacadas es la de la investigación empírica para la comprensión de los fenómenos sociales, de la estructura y el cambio social (razón por la que se le considera padre de la sociología como disciplina científica).
Comte presenta a la historia humana en tres fases:


Además afirma que no es posible alcanzar un conocimiento de realidades que estén más allá de lo dado, de lo positivo, y niega que la filosofía pueda dar información acerca del mundo: esta tarea corresponde exclusivamente a las ciencias.

Dentro de esta, desde la perspectiva de Leopold Von Ranke, se dice que el historiador es imparcial, ya que es capaz de superar fobias, predilecciones o emociones.

De acuerdo al positivismo clásico: basta con reunir cierta cantidad de hechos documentados para que surja la ciencia de la historia.

El positivismo asume la cuantificación para que los historiadores puedan estar seguros de sus afirmaciones mediante la medición de los historiadores, aunque cuando ésta se convierte en la única solución aparece el problema de negar la veracidad a todo lo que no esté cuantificado o probado.

Durante el siglo XIX, a partir de los estudios de Bertrand Russell y otros, el filósofo Ludwig Wittgenstein elabora el texto "Tractatus Logico-Philosophicus", que sirve de inspiración para el surgimiento del Círculo de Viena, grupo de intelectuales que tuvo como objetivo el alejar definitivamente a la ciencia de la metafísica, a partir del desarrollo de la lógica de Russell. Esta propuesta plantea un método basado en la experimentación, observación y recolección objetiva de datos a fin de buscar explicaciones a las causas que originan los fenómenos.

Entre las corrientes positivistas se puede mentar el positivismo ideológico, empiriocriticismo, positivismo metodológico o conceptual, positivismo analítico, positivismo sociológico, positivismo realista, iuspositivismo y neopositivismo (empirismo lógico o neopositivismo lógico). Los enfoques sociológicos en filosofía de la ciencia y epistemología han sido tradicionalmente los principales críticos del positivismo, aunque ambas posturas no son necesariamente contradictorias.

En el campo del derecho, el denominado positivismo jurídico o iuspositivismo no guarda relación en su origen con el positivismo filosófico, sino con el concepto de derecho positivo (la consideración del derecho como creación del ser humano).

En el campo de la psicología se puede mentar el conductismo o psicología conductista, como pioneros en la aplicación de la metodología científica al estudio de la conducta humana. Actualmente, en la psicología conviven múltiples escuelas, muchas de las cuales se basan en el positivismo para el estudio del ser humano. Entre dichas escuelas o enfoques destaca el cognitivo-conductual. Cabe mencionar el avance de la neurociencia, que aborda temáticas mentales que antes parecían inescrutables desde un punto de vista naturalista.




</doc>
<doc id="2348" url="https://es.wikipedia.org/wiki?curid=2348" title="Partitura">
Partitura

Una partitura es un documento manuscrito o impreso que indica cómo debe interpretarse una composición musical, mediante un lenguaje propio formado por signos musicales y el llamado sistema de notación.
Como sus análogos los libros, los folletos, etc., el medio de la partitura generalmente es el papel o, en épocas anteriores, el pergamino. Aunque el acceso a la notación musical en los últimos años incluye también la presentación en pantallas de ordenador.

En música orquestal, se denomina "partitura" al documento que utiliza exclusivamente el director de orquesta y que contiene toda la obra que se ejecutará, además de algunas indicaciones particulares. En contraste, "particella" o parte es el nombre dado a cada una de las partituras que tienen los intérpretes de los diferentes instrumentos, la cual puede incluir dos instrumentos similares como un piccolo y una flauta, o un oboe y un corno inglés, y ser compartida por los intérpretes de estos.

La palabra «partitura» proviene del término italiano "partitura", que quiere decir literalmente "insieme di parti" que es «conjunto de piezas o partes». Un gran número de lenguas mantienen este mismo origen etimológico, como por ejemplo las acepciones "Partitur" en alemán, "partition" en francés o "partitura" en portugués, catalán y euskera. Por su parte, en lengua inglesa se utilizan para designar las partituras dos denominaciones de distinto origen, que son "sheet music" y "score".

La partitura se puede utilizar como un registro, una guía o un medio para interpretar una pieza de música. Aunque no sustituye al sonido de la ejecución musical, la partitura se puede estudiar para construir la interpretación y para dilucidar los aspectos de la música que pueden no ser evidentes a partir de la simple audición. Se puede obtener información fidedigna sobre una pieza musical mediante el estudio de los bocetos y las primeras versiones escritas de las obras que el compositor pudo haber conservado, así como la partitura final autógrafa y las anotaciones personales hechas en borradores y partituras impresas. 

La comprensión de las partituras requiere una forma especial de alfabetización, la capacidad de leer notación musical. No obstante, la capacidad de leer o escribir música no es un requisito imprescindible para componer música. Muchos compositores han sido capaces de crear música en formato impreso, sin la habilidad de leer o escribir en notación musical, siempre y cuando una copista de algún tipo estuviese disponible. Por ejemplo, el compositor ciego del John Stanley así como los compositores y letristas del Lionel Bart, Irving Berlin y Paul McCartney. 

La habilidad conocida como «lectura a primera vista» (o repentización) es la capacidad de un músico para interpretar una obra musical desconocida viendo la partitura de esta por primera vez. La facultad de leer a primera vista se espera tanto de los músicos profesionales como de los aficionados serios que tocan música clásica y otros géneros relacionados. Una habilidad aún más refinada es la capacidad de mirar una pieza musical desconocida y escuchar la mayoría o la totalidad de los sonidos (melodías, armonías, timbres, etc.) mentalmente sin tener que tocar la pieza. 

Con la excepción de las interpretaciones en solitario, donde se espera la memorización, los músicos clásicos suelen tener la partitura a mano cuando tocan. En la música de jazz, que es en su mayoría improvisada, la partitura —llamada "lead sheet" en este contexto— se utiliza para dar indicaciones básicas de las melodías, cambios de acordes y arreglos. 
La música manuscrita o impresa es menos importante en otras tradiciones de práctica musical. Aunque la mayor parte de la música popular se publica en notación de algún tipo, es muy común que la gente aprenda música «de oído». Este es también el caso en la mayoría de las formas occidentales de música folclórica, donde las canciones y las danzas se transmiten mediante la tradición oral y auditiva. La música de otras culturas, tanto folclórica como clásica, con frecuencia se transmite por vía oral, aunque algunas culturas no occidentales desarrollaron sus propias formas de notación musical en partituras. 

Aunque la partitura suele ser considerada como una plataforma para la nueva música y una ayuda a la composición (es decir, el compositor "pone por escrito" la música), también puede servir como un registro visual de la música ya existente. Los estudiosos y otras personas han hecho transcripciones para reproducir la música occidental y no occidental en un formato legible para el estudio, el análisis y la interpretación creativa. Esto se ha llevado a cabo con la música folclórica y tradicional. Por ejemplo, los volúmenes de música popular magiar y rumana escritos por Béla Bartók. Pero también se hizo con grabaciones sonoras de improvisaciones hechas por músicos como el piano de jazz, así como con actuaciones que pueden estar basadas solo parcialmente en notación musical. Un ejemplo exhaustivo y reciente de esto último es la colección "The Beatles: Complete Scores", que trata de transcribir en pentagramas y tablaturas todas las canciones tal y como fueron grabadas por los Beatles en detalle instrumental y vocal.

La partitura consta de un pentagrama, formado por cinco líneas y cuatro espacios, sobre el cual se ubican los símbolos que representan los componentes musicales de la obra escrita en ella. Estos signos musicales suelen indicar las notas musicales, las figuras, es decir la duración de las notas, la armadura de clave, tonalidad, alteraciones (como bemoles, sostenidos y becuadros), las ligaduras entre notas, la articulación y otras particularidades de la interpretación musical. Adicionalmente, las partituras suelen disponer fuera del pentagrama de información adicional sobre cómo interpretar las diferentes secciones de la obra, como el "tempo" y la dinámica, entre otros.

Para ampliar información sobre los elementos que pueden aparecer en una partitura véase el artículo dedicado a los signos musicales.

Las partituras mantienen un sistema consuetudinario de organizar los instrumentos musicales por familias y, a su vez, las familias se dividen en secciones. Dentro de cada familia, los instrumentos son ordenados en función de la tesitura de agudo a grave (por ejemplo, el piccolo precede a la flauta y el oboe precede al corno inglés).

Las partituras orquestales se organizan generalmente en el siguiente orden:

Las partituras modernas pueden presentarse en diferentes formatos. Si una pieza está compuesta para un solo instrumento o voz (por ejemplo, una pieza para un instrumento solista o para voz solista "a capella"), la obra completa puede ser escrita o impresa como una sola partitura. Si una pieza instrumental está diseñada para ser tocada por más de una persona, por lo general cada intérprete tendrá para tocar una partitura separada llamada parte o "particella". Este es precisamente el caso de la publicación de composiciones que requieren de cinco o más intérpretes. No obstante, invariablemente se publica la partitura completa también. Las voces cantadas en una obra vocal que hoy en día no suelen publicarse por separado, aunque así ha sido históricamente, sobre todo antes de que la impresión musical ampliase la disponibilidad de partituras.

Las partituras pueden ser editadas publicadas como individuales o de obras (por ejemplo, una canción popular o una sonata de Beethoven), en las colecciones (por ejemplo, trabajos de uno o varios compositores), como piezas realizadas por un determinado artista, etc.

Cuando las partes separadas de cada instrumento o voz se imprimen juntas, el texto resultante se denomina partitura. Convencionalmente, se compone de una veintena notación musical con cada parte instrumental o vocal, en alineación vertical (es decir, que los hechos concurrentes en la notación para cada parte se organizó ortográficamente). El término partitura también se ha utilizado para referirse a las partituras escritas por un solo intérprete. La distinción entre la partitura y una parte se aplica cuando hay más de una parte necesaria para el rendimiento.

Las partituras pueden presentarse en varios formatos, como los siguientes:








Si la partitura completa es muy compleja y grande, impidiendo así, la lectura de un instrumento en específico, entonces es impresa una parte solista de este mismo.

Antes del la música occidental era escrita a mano y conservada en manuscritos, normalmente recopilados en grandes volúmenes. Los ejemplos más conocidos son los manuscritos medievales de canto monódico. En el caso de la polifonía medieval, como el motete, las voces eran escritas en porciones separadas de páginas enfrentadas. Este proceso se vio favorecido por la llegada de la notación mensural para clarificar los aspectos rítmicos y fue acompañado por la práctica medieval de componer voces polifónicas de forma secuencial, en lugar de simultáneamente como en épocas posteriores. Los manuscritos que muestran todas las voces juntas en una sola partitura no eran comunes y se limitaban sobre todo al "organum", en especial el de la Escuela de Notre Dame.

Incluso después de la aparición de la música impresa, gran cantidad de música continuó existiendo únicamente en manuscritos hasta bien entrado el .

Existían varias dificultades para adaptar la nueva tecnología de impresión a la música. El primer libro impreso que incluía música, el Salterio de Maguncia (1457), tuvo que recoger la notación añadida a mano. Este caso es similar al espacio que se deja a la izquierda en otros incunables para las letras mayúsculas. El salterio fue impreso en Maguncia, Alemania por Johann Fust y Peter Schöffer. Actualmente un ejemplar está ubicado en el Castillo de Windsor y otro en la Biblioteca Británica. 

Más adelante las líneas del pentagrama fueron impresas, pero todavía los escribas añadían en el resto de la música a mano. La mayor dificultad en el uso de los tipos móviles para la impresión de música es que todos los elementos debían estar alineados; la cabeza de la nota debía estar correctamente alineada con el pentagrama o si no habría significado algo distinto de lo que debería. En la música vocal el texto debe estar alineado de forma adecuada con las notas, aunque en aquella época esta cuestión no era una prioridad, ni siquiera en los manuscritos. 

La primera música impresa a máquina apareció alrededor de 1473, aproximadamente veinte años después de que Gutenberg presentase la imprenta. En 1501 Ottaviano Petrucci publicó "Harmonice Musices Odhecaton", que contiene 96 piezas de música impresa. El método de impresión de Petrucci generaba una música limpia, legible y elegante, pero se trataba de un proceso largo y difícil que requería tres impresiones separadas. Más tarde Petrucci desarrolló un proceso que requería solo dos impresiones en la prensa, pero aún era costosa ya que cada paso exigía una alineación muy precisa para que el resultado fuera legible. Esta fue la primera música polifónica impresa bien distribuida. Petrucci también imprimió la primera tablatura con tipos móviles. El proceso de impresión consistente en una única impresión apareció por primera vez en Londres hacia 1520. Pierre Attaignant llevó esta técnica a ser utilizada ampliamente en 1528 y se mantuvo durante 200 años con pocos cambios. 
Un formato común para la publicar las obras a varias voces, la música polifónica del Renacimiento eran los "cuadernos de música". En este formato, la parte correspondiente a cada voz de una colección de madrigales a cinco voces, por ejemplo, se imprime por separado en su propio cuaderno, de tal manera que los cinco "cuadernos de música" serían necesarios para interpretar la música. Los mismos "cuadernos de música" podían ser utilizados por cantantes o instrumentistas. Las partituras de piezas a varias voces rara vez se imprimían en el Renacimiento. Si bien, el uso del formato partitura como medio para componer voces simultáneamente (en vez de sucesivamente, como a finales de la Edad Media) se le atribuye a Josquin Desprez.

El efecto de la música impresa fue similar al efecto de la palabra impresa, en que la información se propaga más rápido, de un modo más eficiente y a un mayor número de personas de lo que podría extenderse a través de manuscritos. Asimismo, tuvo el efecto adicional de proporcionar a los músicos aficionados, al menos aquellos que podían permitirse el lujo, los medios suficientes para interpretar. Esto afectó en muchos aspectos a toda la industria musical. Los compositores ahora podían escribir más música para los intérpretes aficionados, sabiendo que podía ser distribuida. Los músicos profesionales contaban con más música a su disposición. Se incrementó el número de aficionados, de los cuales los intérpretes profesionales también podían obtener ingresos a cambio de proporcionarles una instrucción musical. Sin embargo, en los primeros años el coste de la música impresa limitaba su difusión.

En muchos lugares, el derecho de imprimir la música era concedido por el monarca y sólo a aquellos con una dispensa especial se les permitía hacerlo. Esto suponía a menudo un honor (y un ""boom"" económico) que era otorgado a los músicos favoritos de la corte.
En el la industria de la música estaba dominada por los editores de partituras. En Estados Unidos la industria de las partituras ascendió en conjunto con un tipo de ministriles denominado "blackface". El grupo de editores y compositores asentados en Nueva York que dominaban la industria se conoce como ""Tin Pan Alley"". El final del fue testigo de la explosión masiva de la música de salón, con un piano "de rigor" en el hogar de clase media. 
Pero a comienzos del el fonógrafo y la música grabada crecieron enormemente en importancia. Esto, junto con el crecimiento de la popularidad de la radio de la década de 1920 en adelante, disminuyó la relevancia de los editores de partituras. La industria del disco reemplazó a los editores de partituras como la mayor fuerza de la industria musical.

Había algunas máquinas de escribir para partituras musicales cuya operación era compleja.
Entre las más conocidas, podemos citar la primera inventada en 1936 por Robert H. Keaton, la "Keaton Music Typewriter".

A finales del y en el se ha desarrollado un gran interés por la representación de las partituras en un formato legible para ordenador (ver software de notación musical), así como archivos descargables. Desde 1991 ha estado disponible el OMR ("Optical Music Recognition"), que es un OCR para música. Se trata de un "software" para «leer» partituras escaneadas de forma que los resultados puedan ser después manipulados. En 1998 las partituras virtuales se desarrollaron aún más en lo que iba a ser llamado "partitura digital", que por primera vez permitió a los editores autorizados hacer que las partituras con derechos de autor estuviesen disponibles para su compra en línea. A diferencia de su equivalente en papel, estos archivos permiten manipulaciones tales como cambios de instrumentos, transposición e incluso la reproducción MIDI ("Musical Instrument Digital Interface"). La popularidad de este sistema de entrega inmediata parece estar actuando entre los músicos como catalizador de un nuevo crecimiento de la industria en un futuro previsible.

Uno de los primeros programas disponible de notación informática para ordenadores domésticos fue "Music Construction Set", que fue desarrollado en 1984 y lanzado para varias plataformas diferentes. Al introducir conceptos en gran medida desconocidos para el usuario doméstico de la época, permitía la edición de las notas y signos musicales mediante un dispositivo señalador como un ratón. El usuario podía «agarrar» una nota o signo de una paleta y «arrastrarlo» en el lugar pertinente del pentagrama. El programa permitía reproducir la música creada a través de varias de las primeras tarjetas de sonido y se podía imprimir la partitura musical en una impresora de gráficos.

Muchos productos de "software" para estación de trabajo de audio digital o DAW soportan la generación de partituras a partir de archivos MIDI o bien por entrada manual. Entre los productos con esta característica se incluyen programas libres, de código abierto como "Aria Maestosa" y "MuseScore", así como los programas comerciales como "Cakewalk SONAR, Pro Tools" y "Logic Pro". 

En 1999 Harry Connick, Jr. inventó un sistema y método para coordinar la visualización de la música entre los intérpretes de una orquesta. La invención de Connick es un dispositivo con una pantalla que se emplea para mostrar la partitura a los músicos de una orquesta en lugar de la partitura de papel de uso más común. Connick utiliza este sistema cuando está de gira con su banda, por ejemplo.

De especial interés práctico para el público en general es el Proyecto Mutopia, un esfuerzo para crear una biblioteca de partituras de dominio público usando documentos abiertos de un programa de software libre (LilyPond), y que es comparable a la biblioteca de libros de dominio público del Proyecto Gutenberg. El "International Music Score Library Project" (IMSLP) también está tratando de crear una biblioteca virtual que contenga partituras musicales de dominio público, así como partituras de compositores que están dispuestos a compartir su música con el mundo de forma gratuita.

Además de los esfuerzos de dominio público como el proyecto Mutopia y el IMSLP, muchas obras musicales de dominio público originalmente compuestas para piano, violín o voz están encontrando su camino de regreso a la circulación comercial, ahora que se han vuelto a componer para otros instrumentos. Un ejemplo es WindsMusic, que reescribe piezas musicales de dominio público partiendo de las ediciones originales para diversos instrumentos de viento modernos y publica los arreglos con acompañamiento en archivos en formato Sibelius, Finale, MIDI y MP3.

Existe un gran número de aplicaciones para editar partituras:







</doc>
<doc id="2358" url="https://es.wikipedia.org/wiki?curid=2358" title="Programación dirigida por eventos">
Programación dirigida por eventos

La programación dirigida por eventos, es un paradigma de programación en el que tanto la estructura como la ejecución de los programas van determinados por los sucesos que ocurran en el sistema, definidos por el usuario o que ellos mismos provoquen.

Para entender la programación dirigida por eventos, podemos oponerla a lo que no es: mientras en la programación secuencial (o estructurada) es el programador el que define cuál va a ser el flujo del programa, en la programación dirigida por eventos será el propio usuario —o lo que sea que esté accionando el programa— el que dirija el flujo del programa. Aunque en la programación secuencial puede haber intervención de un agente externo al programa, estas intervenciones ocurrirán cuando el programador lo haya determinado, y no en cualquier momento como puede ser en el caso de la programación dirigida por eventos.

El creador de un programa dirigido por eventos debe definir los eventos que manejarán su programa y las acciones que se realizarán al producirse cada uno de ellos, lo que se conoce como el administrador de evento. Los eventos soportados estarán determinados por el lenguaje de programación utilizado, por el sistema operativo e incluso por eventos creados por el mismo programador.

En la programación dirigida por eventos, al comenzar la ejecución del programa se llevarán a cabo las inicializaciones y demás código inicial y a continuación el programa quedará bloqueado hasta que se produzca algún evento. Cuando alguno de los eventos esperados por el programa tenga lugar, el programa pasará a ejecutar el código del correspondiente administrador de evento. Por ejemplo, si el evento consiste en que el usuario ha hecho clic en el botón de play de un reproductor de películas, se ejecutará el código del administrador de evento, que será el que haga que la película se muestre por pantalla.

Un ejemplo claro lo tenemos en los sistemas de programación Léxico y Visual Basic, en los que a cada elemento del programa (objetos, controles, etcétera) se le asignan una serie de eventos que generará dicho elemento, como la pulsación de un botón del ratón sobre él o el redibujado del control. O en Javascript que asigna manejadores de eventos a los que responder a eventos en una web en el caso del navegador o a eventos producidos por objetos emisores en el caso de NodeJS.

La programación dirigida por eventos es la base de lo que llamamos interfaz de usuario, aunque puede emplearse también para desarrollar interfaces entre componentes de Software o módulos del núcleo.

En los primeros tiempos de la computación, los programas eran secuenciales, también llamados Batch. Un programa secuencial arranca, lee parámetros de entrada, procesa estos parámetros, y produce un resultado, todo de manera lineal y sin intervención del usuario mientras se ejecuta.

Con la aparición y popularización de los PC, el software empezó a ser demandado para usos alejados de los clásicos académicos y empresariales para los cuales era necesitado hasta entonces, y quedó patente que el paradigma clásico de programación no podía responder a las nuevas necesidades de interacción con el usuario que surgieron a raíz de este hecho.

En contraposición al modelo clásico, la programación orientada a eventos permite interactuar con el usuario en cualquier momento de la ejecución. Esto se consigue debido a que los programas creados bajo esta arquitectura se componen por un bucle exterior permanente encargado de recoger los eventos, y distintos procesos que se encargan de tratarlos. Habitualmente, este bucle externo permanece oculto al programador que simplemente se encarga de tratar los eventos, aunque en algunos entornos de desarrollo (IDE) será necesaria su construcción.

Ejemplo de programa orientado a eventos en pseudo lenguaje:

La programación orientada a eventos supone una complicación añadida con respecto a otros paradigmas de programación, debido a que el flujo de ejecución del software escapa al control del programador. En cierta manera podríamos decir que en la programación clásica el flujo estaba en poder del programador y era este quien decidía el orden de ejecución de los procesos, mientras que en programación orientada a eventos, es el usuario el que controla el flujo y decide.

Pongamos como ejemplo de la problemática existente, un menú con dos botones, botón 1 y botón 2. Cuando el usuario pulsa botón 1, el programa se encarga de recoger ciertos parámetros que están almacenados en un fichero y calcular algunas variables. Cuando el usuario pulsa el botón 2, se le muestran al usuario por pantalla dichas variables. Es sencillo darse cuenta de que la naturaleza indeterminada de las acciones del usuario y las características de este paradigma pueden fácilmente desembocar en el error fatal de que se pulse el botón 2 sin previamente haber sido pulsado el botón 1. Aunque esto no pasa si se tienen en cuenta las propiedades de dichos botones, haciendo inaccesible la pulsación sobre el botón 2 hasta que previamente se haya pulsado el botón 1.

Con la evolución de los lenguajes orientados a eventos, la interacción del software con el usuario ha mejorado enormemente permitiendo la aparición de interfaces que, aparte de ser la vía de comunicación del programa con el usuario, son la propia apariencia del mismo.
Estas interfaces, también llamadas GUI (Graphical User Interface), han sido la herramienta imprescindible para acercar la informática a los usuarios, permitiendo en muchos casos, a principiantes utilizar de manera intuitiva y sin necesidad de grandes conocimientos, el software que ha colaborado a mejorar la productividad en muchas tareas.

Uno de los periféricos que ha cobrado mayor importancia tras la aparición de los programas orientados a eventos ha sido el ratón, gracias también en parte a la aparición de los sistemas operativos modernos con sus interfaces gráficas. Estas suelen dirigir directamente al controlador interior que va entrelazado al algoritmo.

Con el paso del tiempo, han ido apareciendo una nueva generación de herramientas que incluyen código que automatiza parte de las tareas más comunes en la detección y tratamiento de eventos. 

Destacan particularmente los entornos de programación visual que conjugan una herramienta de diseño gráfica para la GUI y un lenguaje de alto nivel. Entre estas herramientas se encuentra la conocida Visual Basic, lenguaje altamente apreciado por principiantes debido a la facilidad para desarrollar software en poco tiempo y con pocos conocimientos, y denostado por tantos otros debido a su falta de eficiencia.










</doc>
<doc id="2359" url="https://es.wikipedia.org/wiki?curid=2359" title="Púrpura">
Púrpura

Púrpura es el color o coloraciones que se encuentran entre el rojo y el azul, o más específicamente es un color magenta oscuro que se encuentra entre el violeta y el carmesí. El término proviene de los caracoles marinos también llamados púrpuras ("Stramonita haemastoma", "Nucella lapillus"), y así se denominaba también al color producido por la oxidación de la tinta de estos caracoles, (tinta muy apreciada desde la antigüedad) y a las telas teñidas con ella. 

En colorimetría ha sido definido como el color complementario de los que se perciben como consecuencia de la fotorrecepción de una luz cuya longitud de onda dominante mide entre 555 y 565 nm. Se trata de un color estándar, es decir que se encuentra normalizado en catálogos de colores e inventarios cromáticos. La denominación de color «púrpura» incluye al conjunto de las coloraciones similares al color estándar, purpúreas. A la derecha se proporciona una muestra del púrpura estándar.

Popularmente, se usan como sinónimos nombres como morado, violeta, lila, malva, magenta o cárdeno. Sin embargo, estas denominaciones tienen diferente etimología, y aunque también son coloraciones situadas entre el rojo y el azul, han desarrollado en general su propio significado específico.

La palabra púrpura deriva del latín "purpŭra", ‘molusco que produce un tinte violáceo, tinte obtenido del molusco, tela o vestido teñido con ese tinte’, que procede del griego jónico πορφύρη (πορφύρα, "porphýrā", en koiné), con los mismos significados. El término griego, a su vez, se origina en una palabra semita que designaba al caracol marino con el que se elaboraba el tinte púrpura.

Se cree que los nombres de las antiguas regiones de Canaán y Fenicia pueden ser términos cognados para designar al color púrpura. Canaán podría derivar del vocablo acadio "kinahhu" (‘púrpura rojo’), mientras que Fenicia procedería del griego φοινός ("foinós", ‘rojo oscuro’) o φοῖνιξ ("foinix", ‘púrpura, escarlata’).

La primera aparición del término «púrpura» en idioma castellano data del año 1220.

Debajo se dan muestras de los colores estándar púrpuras y purpúreos que aparecen en un círculo cromático de veinticuatro colores, con el fin de facilitar su comparación entre sí. La terminología empleada en el presente artículo corresponde a estos colores.

Nótese que el azul púrpura equivale al violeta, y el rojo púrpura al magenta.


Al ser un púrpura azulado, el morado es técnicamente púrpura. La diferencia entre púrpura y morado es semántica, ya que la adjetivación de color «morado» se origina en la descripción de la pigmentación de las moras, frutos del árbol llamado moral ("Morus nigra").

La palabra «púrpura» alude a la coloración de ciertos tintes para teñir telas, al color que aquellos producen y a las telas teñidas con los mismos, no es el nombre de un color particular. Estos tintes se elaboraban con las secreciones de varias especies de caracoles marinos, y en tiempos antiguos se produjeron en diferentes regiones del globo, aunque el lenguaje y los usos occidentales fueron influidos en particular por la antigua industria del teñido con púrpura de la región del Mediterráneo.

Como los tintes eran difíciles de obtener, las telas teñidas de púrpura eran extremadamente costosas y su uso estaba restringido a quienes podían pagarlas. Eventualmente las prendas teñidas con púrpura adquirieron un valor simbólico, denotando nobleza: en tiempos de la Roma imperial, por ejemplo, solo el emperador tenía permitido llevarlas.

Vitruvio, en el siglo I a. C., relata que la coloración del tinte púrpura variaba entre azulada y rojiza, y atribuye esa variación al lugar de colecta de los moluscos con que se lo elaboraba; sin embargo, hoy se considera que la variación podía deberse al uso de diferentes especies de caracoles (cada una de las cuales produce un color algo distinto), a la mezcla de los tintes con el propósito de obtener variantes de color o a otras particularidades del procedimiento de teñido.

Durante la Edad Media la técnica para elaborar el tinte púrpura clásico se perdió, y este fue reemplazado por el rojo carmín que se obtenía del quermes, una especie de cochinilla. Las onerosas prendas de gala usadas por emperadores, nobles y altos funcionarios cambiaron así de color, pero no de nombre. Aún hoy, una de las acepciones de «púrpura» es «prenda de vestir, de este color o roja, que forma parte del traje característico de emperadores, reyes, cardenales, etc.»

De acuerdo con Philip Ball, durante toda la Antigüedad y la Edad Media el término latino "purpure" podía denotar un color que variaba del violeta a un rojo oscuro.

Michel Pastoureau, en su libro "Una historia simbólica de la Edad Media occidental", advierte a los historiadores acerca de las traducciones de los términos de color en textos antiguos:

Por otra parte, según el historiador de arte anglosajón antiguo Charles Reginald Dodwell, el término latino "púrpura" hacía referencia a una variedad de tela de seda gruesa, llamativa y cara, de más de un color. Dodwell consideraba que este tejido pudo haber sido similar al tafetán, con los hilos de la urdimbre de un color y los hilos de la trama de otro, de manera que con el movimiento, al incidir la luz sobre sus pliegues, cambiaba de color. Este tipo de tela, a pesar de su nombre, podía ser de cualquier color: las fuentes históricas textuales estudiadas por Dodwell mencionaban telas de "púrpura" que eran de color predominante rojo, blanco, negro y verde.

Desde que fue descubierto el modelo tricromático de coloración en el siglo XVII, el púrpura ha sido considerado el color secundario que puede producirse mezclando los primarios rojo y azul. Tal es el caso del sistema de coloración de D'Aguilon, que en 1613 escribió en latín el término ""purpureus"". Un tratado de pintura en francés de 1708 es más específico, colocando al púrpura ("pourpre") entre el violeta y el rojo carmesí. En el sistema de color en inglés de Moses Harris (1776), figura el círculo cromático con el púrpura ("purple") entre el azul púrpura y el rojo púrpura; y en la teoría del color en alemán de Wilhelm von Bezold (1874), figura el púrpura ("purpur") entre el rojo y el violeta.

En otros modelos tradicionales, el violeta figura como color secundario en lugar del púrpura. Por esta razón se considera por ejemplo que el rojo violeta es equivalente del rojo púrpura y, para los angloparlantes, el término purpúreo resulta sinónimo de violáceo o violado.

La definición del color púrpura puede variar según el idioma. No solo los traductores de textos antiguos deben ejercer prudencia, sino también aquellos que interpretan y traducen lenguas vivas.

Se ha considerado al púrpura como un color no espectral. Sin embargo, si se ve su composición cromática RGB, se notará que el valor del azul suele ser equivalente al del rojo, o puede ser también un poco superior o inferior de acuerdo a la coloración analizada. Cuando el rojo es mayor, como en el púrpura rojizo, será un color no espectral; pero sí el azul es mayor, como en el púrpura azulado, entonces sí es espectral, porque se encuentra en la gama más alejada de la luz violeta. Finalmente si azul y rojo tienen valores muy próximos, como en el púrpura estándar o el magenta, alcanza una longitud de onda en el mismo extremo del espectro visible con 380 nm, prácticamente imperceptible a la vista y en consecuencia se le considera no espectral.

Forma la parte externa de la gama del violeta espectral. Aquí algunos ejemplos:

Estos ejemplos abarcan aproximadamente una longitud de onda entre 388 y 397nm, por lo que forman parte de la región extrema del espectro visible.

Los colores luz púrpura y rojo púrpura no pueden ser generados por una sola longitud de onda lumínica, por lo cual no existe en el espectro de luz visible: son colores extra–espectrales. Para producirse es necesario una mezcla de longitudes de onda de los colores rojo y azul, o del rojo y del violeta.

Aunque el púrpura no se encuentra en el espectro de luz visible ni en el arco iris, sí aparece en la mayoría de los círculos cromáticos y en los espacios de color especialmente ideados para representar todos los colores posibles. En el diagrama cromático del espacio de color CIE 1931, por ejemplo, los púrpuras se localizan sobre una línea recta que conecta a los colores rojo y azul, y que cubre las cromaticidades que van desde el rojo purpúreo al púrpura azulado. Esta línea es conocida como «recta de los púrpuras».

La historia del color púrpura —al menos en Occidente— está marcada por el descubrimiento del tinte homónimo, cuyas sustancias precursoras se encuentran en la glándula hipobranquial de varias especies de caracoles marinos. Este tinte ya se conocía en épocas prerromanas; en la región del Mediterráneo hay rastros de su producción al menos desde alrededor del siglo XVIII a. C. en la isla de Creta, mientras que en Asia Menor se conocía desde el siglo XV a. C.

Es necesario destacar que esta industria fue global, ya que históricamente varias culturas en distintos lugares del mundo produjeron tinte púrpura a partir de caracoles de mar.

En toda la cuenca mediterránea se han encontrado antiguos sitios de explotación de caracoles de la púrpura, por lo general bajo la forma de montículos de conchas descartadas, como en Tiro, Sidón, Ugarit, Asdod y Shiqmona; en Tel Qison, localidad que formaba parte de la antigua Judea, y en el puerto de Acre se han hallado también restos de una industria de la púrpura. Los antiguos griegos probablemente aprendieron de los fenicios el arte del teñido con púrpura, mientras que en el Imperio Romano la púrpura se producía en las tintorerías imperiales. En la "Ilíada" de Homero y la "Eneida" de Virgilio se mencionan ropas teñídas con púrpura de Tiro.

En el libro de Ezequiel (27:7) se menciona a «Elisa» (tal vez Chipre, islas del Mar Egeo o las Canarias) como fuente de "těkēlet" y "’argāmān", términos bíblicos para el púrpura azulado y el púrpura rojizo, respectivamente.

En la Antigüedad el tinte púrpura estuvo asociado especialmente a los fenicios, que se dedicaban al comercio de telas teñidas con aquel. El mejor púrpura, y el más puro, era el que se extraía de los caracoles colectados en las costas del Mediterráneo, incluyendo al puerto fenicio de Tiro, de donde el tinte obtuvo su nombre.

El «púrpura de Tiro» o «púrpura tirio» se producía con la tinta de los caracoles murícidos denominados conchil ("Hexaplex trunculus") y cañadilla ("Murex brandaris"), del buccino ("Buccinum undatum"), Buccinidae) y, ocasionalmente, de "Stramonita haemastoma" (Rapaninae). El buccino da un color rojo púrpura, mientras que los murícidos producen un color similar al púrpura estándar.

En estos moluscos, el fluido precursor del tinte se encuentra en una glándula cercana a su cabeza, y es inicialmente blancuzco. Esta sustancia se extraía rompiendo o prensando los caracoles, y por efecto del aire y de la luz cambiaba de color, pasando a ser amarillo pálida, luego verde, luego azul y finalmente púrpura; el color final del tinte era inalterable ante la luz. Cada molusco no daba más que una gota de tinte, por lo que la obtención de treinta gramos de esta sustancia demandaba la muerte de unos 250 000 caracoles, justificando el elevadísimo precio de los paños teñidos de púrpura. En 1908, el químico austríaco Paul Friedländer, con el fin de obtener algo de tinte púrpura, compró a los vendedores de mariscos de Trieste 12 000 ejemplares de caracoles de la púrpura y obtuvo de ellos solamente 1,4 gramos de tinte, cantidad suficiente para teñir un pañuelo.

Plinio el Viejo, en su obra "Naturalis historia" (siglo I), relata que para la elaboración de púrpura de Tiro debían emplearse dos especies de caracoles, e indica cuál era el matiz más apreciado:

Bajo estas líneas se dan, a la izquierda, una muestra del color específico del púrpura de Tiro y, a la derecha, una muestra inespecífica más clara, que permite ver su matiz.

La púrpura no solo era prohibitiva debido a su precio, sino que a veces su uso estaba restringido por ley. En la Roma republicana los generales podían llevar túnicas completamente teñidas de púrpura, pero a los senadores, cónsules, pretores y otros personajes de menor rango solo se les permitía lucir franjas de púrpura en el borde de sus togas, tanto más angostas cuanto menor el rango. En la Roma imperial las restricciones sobre el uso de la púrpura se incrementaron, y hacia el siglo IV solamente el emperador podía usar púrpura, estando penado el uso de este color por cualquier otro romano, aunque se debiese a un tinte de imitación y no al púrpura auténtico.

En los mosaicos de la iglesia bizantina de San Vital de Rávena, que datan del siglo VI, puede apreciarse una relación jerárquica similar entre el uso de vestiduras de púrpura y el rango de las figuras retratadas: Jesús, el emperador Justiniano y la emperatriz Teodora aparecen vestidos completamente de púrpura, mientras que los personajes menores lucen superficies bastante más pequeñas de este color en sus ropajes.

La técnica de elaboración del tinte púrpura se perdió para Occidente tras la toma de Constantinopla por los turcos en 1453, y permaneció desconocida hasta mediados del siglo XIX.

Aunque el púrpura de Tiro se usó principalmente para el teñido de telas, Vitruvio recomendaba que en caso de usar el púrpura de Tiro —al que llamaba "ostrum"— para pintar, se le mezclara con miel, a fin de evitar que se secara demasiado pronto. Por otra parte, Plinio describió un procedimiento para preparar una laca púrpura de uso pictórico, que consistía en empapar paños teñidos de púrpura en una solución de agua con tiza.

Actualmente el púrpura de Tiro se comercializa como pigmento para uso artístico, aunque su precio sigue siendo elevado. A mediados de 2012, la firma Kremer Pigmente ofrecía 25 mg de púrpura de Tiro auténtico a alrededor de 130 dólares estadounidenses o 70 euros.

Antes de la proliferación de los colorantes sintéticos a mediados del siglo XIX, en Europa se conocieron varias fórmulas para obtener tintes purpúreos. Los colores morados o violáceos podían lograrse combinando un tinte azul con uno rojo (por ejemplo, glasto para el azul y rubia para el rojo), o con colorantes naturales derivados de plantas, hongos y líquenes. El llamado papiro de Estocolmo, texto griego que se remonta al siglo III, contiene la receta más antigua que se conoce para elaborar tinte púrpura de imitación. Además, es posible que el tinte de caracoles marinos se haya utilizado fuera de la región mediterránea, tanto para teñir telas como para pintar manuscritos iluminados; la especie empleada habría sido "Nucella lapillus".

Los fenicios no solo comerciaron con tintes purpúreos derivados de moluscos, sino que los obtuvieron también de líquenes, como los llamados orcela u orchilla, denominación que abarca a varias especies del género "Roccella". En las islas Púrpuras, frente a Esauira (Marruecos), se hallaron restos de una factoría que perteneció al rey Juba II y que alrededor del siglo I a. C. producía púrpura de moluscos, orchilla y orceína. Fue Plinio quien llamó «Purpurarias» a estas islas en razón de su producción; también se conocieron por su denominación portuguesa, Mogador.

Tradicionalmente, el primer paso para producir tinte de estos líquenes consiste en cosechar sus talos directamente de las rocas donde crecen y preparar una pasta tintórea, la orchilla u «orchilla de Mogador». El historiador, botánico y poeta canario José de Viera y Clavijo (1731–1813) da una descripción pormenorizada del procedimiento utilizado en Canarias, comenzando con los líquenes ya cosechados:

La explotación de los líquenes de las islas Canarias y el comercio de los tintes que producían constituyó una provechosa actividad, que estuvo a cargo de España y duró unos 450 años, desde mediados del siglo XV hasta principios del siglo XX, aunque ya había declinado durante el siglo XIX debido a la recolección abusiva de la orchilla y a la competencia de los colorantes sintéticos industriales.

Hirviendo la orchilla de Mogador en agua se podía aislar de ella un colorante, la orceína, cuyo principio químico es la orcina u orcinol, un difenol que se encuentra en los líquenes y que por sí solo se ha usado tradicionalmente para colorear papeles pintados. La presentación tradicional de la orceína era en forma de polvo, y con ella se lograban colores rojo púrpuras, rojo purpúreos y púrpura rojizos.

La orceína se usó desde la antigüedad cretense, fenicia, griega y romana, y los tintoreros tradicionales la emplearon para teñir lana y seda. Durante el siglo XIII, la familia de tintoreros Oricelli preparaba y vendía en Florencia un tinte rojo purpúreo derivado de la orceína, el "oricello". Además, con la orceína del «liquen de Canarias» ("Roccella phycopsis [= Roccella tinctoria]") se elaboró un tinte azul purpúreo muy oscuro denominado orcela u orcella.

Debajo se muestran las coloraciones específicas de la orceína y de la orcela.

En 1766, el Dr. Cuthbert Gordon patentó en Escocia un procedimiento similar al usado tradicionalmente en la obtención de la orchilla, para extraer tinte del liquen septentrional "Ochrolechia tartarea". El material resultante, llamado en inglés "cudbear", se almacenaba en forma de pequeños bollos y podía usarse para teñir textiles o como pigmento. Los colores proporcionados por esta sustancia iban del rojo al rosa, pasando por el magenta, hasta el púrpura.

De varias especies de líquenes se extrae también la sustancia denominada tornasol, que vira a rojiza o a violácea en contacto con compuestos ácidos o alcalinos, y que consecuentemente se ha usado como indicador de pH.

Con respecto a los colores y pigmentos para pintura artística y otros usos decorativos, Philip Ball, en su libro "La invención del color", menciona el uso del tinte rojo purpúreo de buccino de las costas de Francia o de Inglaterra para teñir pergaminos, y agrega que la mayoría de los pigmentos púrpuras o violáceos usados en la Edad Media para pintura sobre panel se hacían mezclando un azul —como el de la azurita, por ejemplo— con una laca roja.

En la Italia del siglo XIV era muy apreciado un colorante llamado "folium", elaborado con extractos de la planta llamada antiguamente "morella", "maurelle" o "torna ad solem", hoy identificada con la especie "Chrozophora tinctoria". Este colorante, al igual que otros de origen vegetal, vira de color según la acidez del medio, por lo que podía prepararse para que fuese rojo, púrpura o azul; al ser transparente, era particularmente adecuado para colorear manuscritos.

También se recurrió con fines pictóricos al tinte purpúreo extraído del liquen "Roccella phycopsis".

En el continente americano se utilizó tinte púrpura de moluscos desde épocas precolombinas. En 1990 se detectaron rastros de tinte del «caracol púrpura» ("Plicopurpura pansa") en textiles de la cultura Paracas (900 a. C. al 200 a. C.), y de la cultura Nazca (200 a. C. al 600 d. C.), que se desarrollaron en lo que hoy es Perú.

En cuanto a los testimonios históricos, diversos cronistas (Antoine de Jussieu, Thomas Gage, Antonio de Ulloa) mencionan que alrededor de 1700, tanto en la costa de Guayaquil (Ecuador) como de Guatemala, y en las cercanías de Panamá, se elaboraba de manera regular un tinte púrpura obtenido de una especie de caracol marino con miras al teñido de telas de algodón; hacia 1850 se documentó esta práctica también en la costa occidental de Nicaragua. Gonzalo Fernández de Oviedo ya había asentado, hacia 1515, una costumbre similar por parte de los nativos de la península de Nicoya, en Costa Rica. Ulloa refería que la secreción del caracol, al ser extraída, pasaba de un color lechoso al verde, y de este a un púrpura inalterable.

Actualmente esta tradición se conserva en contados lugares, como por ejemplo en el estado mexicano de Oaxaca, donde los tintoreros mixtecos se sirven de la secreción de "Plicopurpura pansa" para teñir hilados de algodón. Mientras que en otras partes del mundo el tinte se extraía a costa de la vida de los caracoles, en este caso los artesanos les extraen la sustancia precursora del tinte sin dañarlos; además seleccionan los caracoles a «ordeñar» por talla, evitan molestarlos durante su época de reproducción y rotan los bancos de explotación, técnica que parece coincidir con la referida por Fernández de Oviedo para Costa Rica.

A pesar del aprovechamiento sustentable de los caracoles, tanto esta tradición mixteca como los caracoles púrpura de la región se encuentran amenazados de desaparición por diversos factores socioambientales.

Alrededor del siglo VII, los valores morales tradicionales de Extremo Oriente estaban organizados en un sistema jerárquico, siendo el más importante de ellos la «moralidad» o «virtud», que se asociaba con el color púrpura.

En Japón, la denominación del color púrpura —紫, "murasaki"— es también la de la planta boraginácea "Lithospermum erythrorhizon", la fuente del tinte púrpura tradicional más apreciado de esa nación: el "shikon" (紫根), que se elaboraba con sus raíces y que también recibía la denominación "murasaki". La disponibilidad de este tinte era escasa debido a que cada planta rendía una cantidad pequeña y a que su cultivo no era sencillo. En el Japón antiguo se extraía asimismo un tinte púrpura del brasilere ("Caesalpinia sappan"), al que se llamaba «púrpura falso» (偽紫, "nise–murasaki") para distinguirlo del tinte de "Lithospermum", que gozaba de mayor prestigio.

Ya en el Nihonshoki, el libro más antiguo de Japón, que data del año 720, se menciona al color púrpura asociado a la nobleza y la elegancia. Las prendas púrpuras eran tenidas en gran estima y denotaban riqueza y alto nivel social en quien las portaba.

El desarrollo de diferentes tintes a lo largo del tiempo llevaron a que el púrpura tradicional japonés se clasificara en dos variedades: "asa–murasaki" (púrpura claro) y "fuka–murasaki" (púrpura oscuro). Este último —también llamado "hon–murasaki" (púrpura primario u original)— estaba reservado para la corte imperial y los monjes; para la gente común este color y sus matices asociados eran "kin–jiki", «colores prohibidos». Seiki Nagasaki, autor de libros sobre los colores tradicionales japoneses, describe al "fuka–murasaki" «ejemplar», o más apreciado. Abajo se compara con el murasaki y con colores similares como el purpúreo (púrpura agrisado) y el morado (púrpura oscuro):

En cuanto a los pigmentos púrpuras tradicionales (顔料, "ganryou"), solían prepararse mezclando rojo con azul. Por ejemplo, en el período Heian (siglos VIII a XII) se combinaba pigmento bermellón con índigo para obtener púrpura.

En China, el color púrpura (紫; pinyin: "zi") fue inicialmente poco estimado. En tiempos de las dinastías más antiguas, los colores que se asociaban a los cinco elementos tradicionales (azul, rojo, amarillo, blanco y negro) eran considerados sagrados y formaban parte del atuendo ceremonial de las clases dominantes, sirviendo como indicadores de rango. En este contexto el púrpura era un color secundario y profano, producto de la mezcla de azul con rojo.

Sin embargo, a partir del período de las Primaveras y Otoños (722 a. C. a 481 a. C.) el púrpura comenzó a ganar espacio entre la nobleza china, siendo usado primero por los oficiales de la corte, luego reservado a los oficiales de alto rango y finalmente, a partir de 1391, prohibido para todos los súbditos y nobles, excepto la realeza. Entretanto, durante la dinastía Tang (618–907) se había adoptado el taoísmo, sistema filosófico y religioso en el que el púrpura simbolizaba nobleza. En los escritos taoístas, los inmortales vestían de púrpura.

El púrpura quedó, entonces, fuertemente asociado con el emperador. Este, en cuanto Hijo del Cielo, era el representante terrenal del Emperador Celestial, cuya residencia se creía que estaba en el punto más alto del cielo, donde se encuentra Polaris, la estrella polar. Esta era la «Estrella Púrpura», también llamada «Gran Regente Imperial de los Cielos» y «Trono del Emperador». La región del firmamento que rodea a la estrella polar era el «Recinto Púrpura Sutil» o «Palacio Púrpura», donde vivían el Emperador Celestial y su corte, esta última representada por las estrellas circumpolares, que giran alrededor de la aparentemente fija Polaris. La morada del emperador terrestre, la Ciudad Prohibida, se consideraba análoga al palacio celestial que regía el orden de los cielos, lo que explica que en chino se le llame «Ciudad Púrpura Prohibida» (紫禁城; pinyin: "zǐ jìn chéng").

En idioma inglés, recibe el nombre de «púrpura Han» ("Han purple") o «púrpura chino» ("Chinese purple") un pigmento sintético compuesto de silicato de cobre–bario, cuya fórmula química es BaCuSiO. En las muestras antiguas no es púrpura sino violeta, pero las recreaciones modernas de este compuesto indican que según la cantidad de óxido de cobre (I) que se introduzca en su preparación, su color final puede variar entre el azul y el púrpura rojizo.

Este pigmento se halló en antiguos objetos pintados y cerámicas vidriadas de las dinastías chinas Zhou Oriental (800–221 a. C.), Qin (221–207 a. C.) y Han (206 a. C.–220 d. C.), aunque se cree que su uso podría remontarse al año 900 o 1000 a. C. aproximadamente. Sin ser particularmente estable, el púrpura Han requería sin embargo de sofisticados conocimientos de química y mineralogía para su preparación. El ejército de terracota encontrado en la tumba del emperador Qin Shi Huang está parcialmente pintado con este pigmento.

Actualmente, el compuesto BaCuSiO se emplea en la manufactura de materiales superconductores, siendo objeto de intensa investigación.

Se ha llamado «década malva» al período que, en la cultura occidental, va desde fines de la década de 1850 hasta principios de los años 1860, debido a la gran popularidad y demanda que experimentaron en ese tiempo los textiles de colores purpúreos.

Los últimos tintes púrpuras no derivados del alquitrán que tuvieron un uso masivo en Occidente fueron la murexida y el púrpura francés.

La murexida, que se fabricó desde 1835, era un tinte sintético que dependía de la recolección de un recurso de origen animal: se elaboraba con ácido úrico derivado del guano que se excavaba en las costas de Perú. Fue promocionado en Europa comparándosele con el púrpura tirio, llegándose a afirmar falsamente que tenía la misma composición que aquel.

El púrpura francés —llamado "mauve" (‘malva’) por los franceses— era un tinte natural derivado de líquenes que daba un color intenso, no precisando mordentado sobre lana y seda; hacia fines de los años 1850 de descubrió la manera de mordentarlo para que pudiese teñir algodón.

Hacia 1845, las investigaciones realizadas sobre el alquitrán de hulla comenzaron a sugerir su gran potencial para producir derivados con propiedades colorantes. El primero de estos en usarse en gran escala fue el ácido pícrico, que teñía de amarillo, aunque era poco resistente a la luz.

En 1856, el joven William Henry Perkin, estudiante del químico August Wilhelm Hofmann, intentaba sintetizar quinina a partir de derivados del alquitrán de hulla, cuando descubrió en forma accidental un colorante púrpura-malva. Al ensayar la oxidación de la anilina, obtuvo un precipitado negruzco que se disolvió parcialmente en alcohol. La sustancia daba un color púrpura que resultó ser un tinte estable y resistente a la luz. Perkin decidió encontrar la manera de industrializarlo. Una vez salvadas las dificultades iniciales, el tinte pudo producirse a gran escala, siendo ya muy solicitado para 1857 y desplazando en poco tiempo al muréxido y al púrpura francés.

Actualmente conocemos a este colorante como púrpura de Perkin, malveína, mauveína, malva o anilina morada, y en su tiempo fue patentado como púrpura de anilina (nomenclatura: 3-amino-2,9-dimetil-5-fenil-7-(p-tolilamino) acetato de fenazina).

El descubrimiento del púrpura de Perkin constituyó el puntapié inicial de una serie de investigaciones relacionadas con los colorantes derivados del alquitrán, a los que pronto se agregarían otros colores. Una vez establecida la producción de estos tintes a nivel industrial, fueron universalmente adoptados, y los antiguos tintes naturales cayeron gradualmente en desuso.

En adición a lo ya mencionado:


En heráldica, el púrpura o violado tiene la peculiaridad de comportarse como color y como metal, lo que le exceptúa de la prohibición heráldica de colocar color sobre color y metal sobre metal. Es un esmalte poco utilizado. El heraldista británico Arthur Fox–Davies menciona que las armas más antiguas donde tuvo noticia de que se usara el púrpura databan del año 1311.

Algunos heraldistas han sugerido que en un principio el púrpura era un color grisáceo y amarronado, quizás mezcla de los otros colores heráldicos: azur, gules, sable y sinople. En el siglo XIV pasó a tener un matiz violáceo, consolidando el septeto de colores de la heráldica.

En vexilología, el púrpura es también un color infrecuente. La mayoría de las banderas que lo emplean son regionales o municipales.

En los ejemplos bajo estas líneas: las armas del antiguo Reino de León, con un león de púrpura, que aún pueden verse en el escudo de España y otros; la bandera de Adjuntas, municipio de Puerto Rico, en la que el púrpura proviene del manto del santo patrono local, San Joaquín; la bandera de la Prefectura de Tokio, con fondo de «púrpura de Edo» ("Edo–murasaki"), color representativo de la región.





En el ámbito médico, la púrpura es una afección que se define como la salida de eritrocitos del torrente sanguíneo y su acumulación en la piel o en el tejido celular subcutáneo. Puede presentarse en forma de petequias (pequeñas lesiones rojas) o de equimosis (áreas de sangrado mayores de un centímetro de diámetro), que no desaparecen al aplicarles presión. Se clasifica en numerosas variedades.

Como se ha visto, la secreción de ciertos moluscos precisa oxidarse en contacto con el aire para volverse púrpura. El componente principal de esta sustancia es típicamente el 6,6’–dibromoíndigo, responsable del color.

Sin embargo, en la naturaleza son frecuentes ciertos colorantes que suelen ser de color púrpura en estado natural: las antocianinas, que se encuentran en abundancia en los frutos del arándano y de la zarzamora, en la piel de las berenjenas, en las hojas de las coles lombardas, en los pétalos de las violetas y en ciertas variedades de uvas, por citar unos pocos ejemplos. Estos son los colorantes purpúreos que se aprovecharon en el pasado extrayéndolos de líquenes y de plantas. Como las antocianinas viran al rojo o al azul dependiendo de la acidez o de la alcalinidad del medio, no todas las plantas u hongos que las contienen exhiben coloraciones púrpuras o violáceas; algunas veces son rojas o azules.

Algunos ejemplos adicionales de púrpura:

El purpúreo o en su forma latina "purpureus" es por lo general un color púrpura grisáceo y se ha usado en la descripción de especies biológicas, como por ejemplo en la planta "Lablab purpureus" (poroto de Egipto). Algunas muestras de su uso:

Los colores HTML establecidos por protocolos informáticos para su uso en páginas web incluyen al púrpura que se muestra debajo. Como se ve, se asemeja al púrpura estándar, y en programación se lo invoca con el nombre "purple" (púrpura).

Otras coloraciones web similares:




</doc>
<doc id="2365" url="https://es.wikipedia.org/wiki?curid=2365" title="Solas">
Solas

«Solas» puede referirse a:



</doc>
<doc id="2371" url="https://es.wikipedia.org/wiki?curid=2371" title="Muerte en Venecia (película)">
Muerte en Venecia (película)

Muerte en Venecia (título original: "Morte a Venezia") es una película franco-italiana dirigida por Luchino Visconti. Adapta la novela corta "La muerte en Venecia" del escritor alemán Thomas Mann. 

Esta cinta, una de las últimas obras del director de "Rocco y sus hermanos", "Senso" y "El gatopardo", fue candidata al Oscar al mejor vestuario. Se trata de una disquisición estético-filosófica sobre la pérdida de la juventud y la vida, encarnadas en el personaje de Tadzio, y el final de una era representada en la figura del protagonista.

A principios del siglo XX, el compositor de mediana edad Gustav von Aschenbach (Dirk Bogarde), que padece de una depresión severa debido a varios problemas tanto familiares como profesionales, se refugia en Venecia para descansar y huir del agobio de su vida en Múnich.

Poco después de instalarse en un lujoso hotel en la isla del Lido, se fija en un adolescente polaco, Tadzio, cliente del hotel con su familia. El interés del protagonista por este joven andrógino de belleza sobrecogedora, que encarna un ideal estético, se transformará en amor y obsesión. Visconti seleccionó el joven (Björn Andrésen, sueco de nacimiento) que encarna el papel de Tadzio entre centenares de jóvenes candidatos.

Los días de Aschenbach discurren en la playa del Lido y en excursiones al centro de Venecia; pero, sobre todo, se dedica a seguir, observar y espiar a Tadzio.

Paralelamente, Aschenbach va tomando consciencia de unos acontecimientos extraños en la ciudad (muertes repentinas, campañas de desinfección de las calles, explicaciones evasivas de los venecianos, etc.) y consigue descubrir que Venecia está aquejada de una epidemia de cólera, escondida por las autoridades para que los turistas no abandonen la ciudad. Atormentado por el conflicto interior que le producen sus sentimientos por el joven, Aschenbach toma la decisión de irse. Sin embargo, tras una serie de eventos desafortunados que aparentemente impiden su salida de Venecia ese mismo día, cambia de opinión y se queda en el hotel aliviado de no haber abandonado a Tadzio. Pocos días después, Aschenbach revela la información de la epidemia a la familia de Tadzio y les pide que se vayan. El día siguiente, Aschenbach, vestido de traje y maquillado, presencia desde lejos Tadzio y su familia abandonando la isla, luego estalla en llanto. 

Como era de esperar, Aschenbach, delicado de salud, enferma. Sale una última vez a la playa pensando en Tadzio (al cual nunca ha hablado) y el recuerdo vívido de Tadzio jugando con un amigo en la arena en esa misma playa le causa a tener un orgasmo antes de morirse momentos después a orillas del mar. La película termina con la figura de Tadzio alejandose de las orillas, mientras unos socorristas vienen a levantar el cuerpo de Aschenbach.

Tanto la novela original como la película constituyen, aparte de los sucesos acontecidos a Gustav durante su estancia en Venecia, una ilustración, oda, alegato y homenaje a la belleza perfecta, pura y plena de la que habla Platón en el "Fedro" y el "Banquete".

Gustav se encuentra frente a la belleza inalcanzable, bella por sí misma y reflejo de la verdad. Tadzio, su objeto de obsesión, no intercambia palabra alguna con él ya que el sentido de perfección no posee carácter mundano, va más allá (""Aquél que ha contemplado la belleza está condenado a seducirla o morir"").

El apellido alemán "Aschenbach" puede traducirse por "Arroyo de cenizas".

La trama se desarrolla en Venecia, símbolo del arte y el comercio entre Oriente y Occidente, en el fastuoso y decadente hotel del Lido veneciano (la estación balnearia que tuvo su mayor popularidad a fines del siglo XIX y principios del XX).

La descripción minuciosa y exacta del entorno aristocrático que logra Visconti (un legendario aristócrata milánes) es paradigmática. Incluso la ropa usada es original y fue planchada y almidonada a la manera de la época.

El personaje está basado vagamente en el compositor Gustav Mahler, el "Adagietto" de cuya "Quinta sinfonía" está presente a lo largo de la película, formando una unión indivisible entre imagen y sonido de gran presencia dramática. De hecho, Visconti es en gran medida responsable de la inmensa popularidad que cobró luego la música de Mahler, quien perdió una hija en circunstancias similares a las que se ven en la película, pero no era homosexual.

La popularidad de "Muerte en Venecia" y la obra de Gustav Mahler inspiraron un ballet del coreógrafo John Neumeier y la ópera homónima de Benjamin Britten.

Para el papel de Tadzio, Visconti escogió al desconocido Björn Andrésen tras un largo proceso de audiciones que se registraron en el documental "Alla ricerca di Tadzio" ("En busca de Tadzio"). El cantante español Miguel Bosé, entonces adolescente, fue un candidato a ese papel, pero su padre, el torero Luis Miguel Dominguín, se opuso.



</doc>
<doc id="2372" url="https://es.wikipedia.org/wiki?curid=2372" title="Ser o no ser (película)">
Ser o no ser (película)

Ser o no ser ("To Be or Not to Be") es una película cómica estadounidense de Ernst Lubitsch estrenada en . El guion, escrito por Lubitsch y Edwin Justus Mayer, adapta un relato de Menyhért Lengyel. En la película actuaron Carole Lombard, Jack Benny, Robert Stack, Felix Bressart, Lionel Atwill, Stanley Ridges y Sig Ruman.

El título hace referencia al monólogo de la tragedia "Hamlet" de William Shakespeare. La película se estrenó dos meses después de que su protagonista Carole Lombard muriera en un accidente de aviación.

Antes de la invasión de Polonia por la Alemania nazi en 1939, las estrellas de una compañía de teatro en Varsovia, el actor Josef Tura (Jack Benny) y su bella esposa, María (Carole Lombard), están ensayando, junto al resto de la compañía, la obra "Gestapo", en la que se satiriza a los nazis. Uno de los actores, Bronski (Tom Dugan), intenta incluso demostrar que la gente le tomaría por Hitler si saliera a la calle caracterizado como él. Los transeúntes quedan asombrados ante la aparición del dictador nazi en Varsovia, hasta que un joven le pide un autógrafo dirigiéndose a él como "Mr. Bronski".

Esa noche, cuando la compañía está representando la obra de Shakespeare "Hamlet", con Josef Tura en el papel principal, Bronski se lamenta junto a su amigo y colega Greenberg (Felix Bressart) de su triste papel como actores secundarios, y este último confiesa que siempre ha sido su sueño interpretar a Shylock en "El mercader de Venecia", en especial el famoso monólogo " "¿el judío no tiene ojos?" ", que comienza a recitar.

Mientras tanto, María ha recibido un ramo de flores de un joven y apuesto piloto, el teniente Stanislav Sobinski (Robert Stack). Ella acepta que la visite, diciéndole que abandone la sala y vaya a su camerino cuando su marido comience su monólogo con las famosas palabras " "Ser o no ser..." ". El joven así lo hace abiertamente, provocando en el actor un evidente disgusto ante la aparente ofensa. Poco después, un agente del gobierno anuncia a la compañía que deberán cancelar la representación de "Gestapo", temiendo que ésta ofenda a los alemanes y empeore las relaciones entre los dos países.

La noche siguiente, no obstante, citado otra vez por María para una nueva, y honesta, visita, Sobinski sale en el mismo momento de la representación, lo que contraría aún más a Josef. En el camerino de María, Sobinski le confiesa su amor por ella, dando por sentado que ésta va a abandonar a su marido y los escenarios para irse con él. Antes de que ella pueda sacarle de su error llega la noticia de que Alemania ha invadido Polonia. Sobinski, como piloto que es, deja a María para unirse al combate, y los actores se refugian en los sótanos del teatro cuando Varsovia comienza a ser bombardeada.

Hitler conquista Polonia mientras la División Polaca de la Real Fuerza Aérea británica (RAF) lucha para liberar a su patria. El teniente Sobinski y otros jóvenes pilotos de la división cantan juntos, teniendo como invitado al jefe de la Resistencia polaca, el Profesor Siletsky (Stanley Ridges). Éste les dice que volverá pronto a Varsovia, y los pilotos se apresuran a entregarle mensajes para sus seres queridos, pero Sobinski sospecha de Siletsky cuando al darle un mensaje para María Tura éste no sabe quién es la famosa actriz. Al comunicar Sobinski este hecho a sus superiores, los Aliados se dan cuenta de que Siletsky tiene entonces en su poder una lista con los nombres y direcciones de familiares de los pilotos polacos de la RAF, contra los cuales los alemanes pueden tomar represalias.

Sobinski regresa en avión a Varsovia para advertir a María, pero Siletsky llega antes. Inmediatamente después, ella es detenida por dos soldados encargados de llevarla al hotel en el que se encuentra Siletsky para que éste pueda enseñarle el mensaje de Sobinski y que le diga qué significa para ella "Ser o no ser". Siletsky invita a María a cenar con la esperanza de reclutarla como espía, pero también para poder apreciar sus encantos. Ella finge estar interesada y vuelve a su casa a cambiarse de ropa, pero antes de que llegue a su apartamento, Josef regresa y encuentra a Sobinski en su cama y con su albornoz. María y Sobinski deliberan sobre qué hacer con Siletsky, mientras que Josef trata de averiguar qué relación existe entre su esposa y el piloto. Al final, Josef proclama que "él" va a matar a Siletsky.

Por la noche, María regresa a la habitación de Siletsky y finge que él la atrae, pero cuando se están besando llaman a la puerta. Un oficial nazi (en realidad, uno de los integrantes de la compañía de teatro) convoca a Siletsky en un falso cuartel general de la Gestapo, preparado rápidamente en el teatro con el material que utilizan en las representaciones. En él, Josef finge ser el coronel Ehrhardt (Sig Ruman), jefe de la Gestapo, y Siletsky le da la lista con los nombres y direcciones de las familias de los pilotos polacos. También le dice que Sobinski le dio un mensaje para María, y que "Ser o no ser" era la señal convenida entre ellos para reunirse en su camerino. Ante la violenta reacción de Josef, devorado por los celos y que desvela su verdadera identidad, Siletsky descubre el engaño, saca una pistola y, encañonando a Josef, trata de escapar atravesando el patio de butacas del teatro, pero finalmente Sobinski le mata de un disparo.

Josef regresa al hotel disfrazado de Siletsky, poniéndose sus gafas y una barba postiza, para destruir la copia de los documentos acerca de la resistencia polaca, que Siletsky guardaba en su habitación, y para pedir explicaciones a María sobre su relación con Sobinski. Pero en el hotel es reconocido por el capitán Schultz (Henry Victor), ayudante del verdadero coronel Ehrhardt, que le acompaña para que se reúna con éste. Haciéndose entonces pasar por Siletsky, Josef le informa de la reciente (y supuesta) ejecución de los líderes de la resistencia, enterándose al mismo tiempo de que el propio Hitler visitará Varsovia al día siguiente.

Sin embargo, tras descubrirse en el teatro el cadáver del verdadero Siletsky, Ehrhardt convoca de nuevo al impostor, reunión a la que acude Josef haciéndose pasar todavía por aquel. Al llegar, Ehrhardt le hace esperar en una habitación en la que se encuentra el cadáver, suponiendo así que Josef confesará cuál es su verdadera identidad. Pensando rápidamente, Josef afeita la barba de Siletsky y luego le pega una falsa barba que él llevaba de repuesto en el bolsillo. Al cabo de un rato llama a Ehrhardt y consigue que éste tire de la falsa barba del verdadero Siletsky, arrancándola, lo que parece demostrar que el verdadero Siletsky era en realidad el impostor. Cuando Josef se dispone a abandonar el cuartel general de la Gestapo aparecen varios soldados nazis (actores de la compañía convenientemente disfrazados y enviados por María) que irrumpen en el despacho de Ehrhardt, arrancan la falsa barba de Josef y se llevan aparentemente detenido a éste, el cual no puede, sin embargo, abandonar el país con los demás, como tenían previsto, en el avión que Ehrhardt había preparado para Siletsky.

Cuando los nazis organizan un espectáculo en el teatro en honor a Hitler con motivo de su visita a Varsovia, a la compañía se le ocurre llevar a cabo un plan audaz: Sobinski, Josef, Bronski y los demás actores acceden al teatro disfrazados de nazis, ocultándose hasta la llegada de Hitler. Mientras los nazis cantan la "Deutschlandlied" (el himno nacional alemán) en el interior, Greenberg aparece súbitamente, lo que distrae a los soldados que vigilan el palco del Führer para que Bronski, como falso Hitler, salga de su escondite rodeado de sus acompañantes disfrazados sin que aquellos lo adviertan.
Josef, como aparente jefe de la guardia de Hitler, pregunta a Greenberg qué es lo que pretende, lo que proporciona al actor la oportunidad, tanto deseada, de recitar el famoso monólogo de Shylock en "El mercader de Venecia", concluyendo : "... si nos hacéis daño, ¿no hemos de vengarnos?". Josef ordena entonces a sus hombres que se lleven detenido a Greenberg, recomendando a Bronski/Hitler que abandone Polonia de inmediato, con lo que éste y sus acompañantes suben a los coches de Hitler y se alejan rápidamente.

De vuelta a su apartamento, María está esperando que los actores vayan a recogerla para regresar todos en el avión de Hitler, pero entonces aparece el Coronel Ehrhardt y trata de seducirla. Cuando la puerta se abre y entra Bronski disfrazado de Hitler, éste les mira, da media vuelta y sale sin decir una palabra, con lo que Ehrhardt, sin salir de su asombro, piensa inmediatamente, alarmado, que María está teniendo una aventura con Hitler y que él ha sido descubierto tratando de conquistar a su amante. María sale tras Bronski gritando: "¡Mein Führer, Mein Führer!"

El avión de Hitler despega entonces con toda la compañía, quienes se desembarazan fácilmente de los pilotos nazis. Sobinski vuela a Escocia, donde los actores son entrevistados por la prensa. Cuando se le pregunta a Josef qué recompensa desearía por el servicio prestado a los Aliados, él parece dudar mostrando una falsa modestia, pero María responde rápidamente en su lugar: "él querría interpretar "Hamlet" ", con lo cual puede verse de nuevo a Josef sobre el escenario representando la obra. Al llegar el momento de comenzar a recitar el monólogo de ""ser o no ser"" ve a Sobinski entre el público, pero ambos no salen de su asombro cuando observan que un nuevo joven oficial se levanta y se dirige abiertamente hacia los camerinos.

Lubitsch nunca había pensado dar el papel protagonista en la película a otro actor que no fuera Jack Benny, e incluso había elegido el personaje pensando en él. Encantado de que un director de la talla de Lubitsch se hubiera fijado en él mientras escribía el guion, Benny aceptó inmediatamente el papel. Benny estaba en una situación difícil, por extraño que parezca, por su éxito en la versión cinematográfica de "Charley's Aunt" (Archie Mayo, 1941), y pensaba que no era alguien interesante para que se le contratara.

Como coprotagonista de Benny, el estudio y Lubitsch se habían decidido por Miriam Hopkins, cuya carrera se estaba tambaleando en los últimos años. El papel fue diseñado como un regreso para la veterana actriz, pero Hopkins y Benny no congeniaban bien, y Hopkins salió de la producción.

Lubitsch se quedó sin protagonista femenina hasta que Carole Lombard, al tanto de la situación, pidió ser considerada como candidata. Lombard nunca había trabajado con el director y anhelaba tener una oportunidad. Lubitsch estuvo de acuerdo y Lombard entró en el reparto. La película también proporcionó a Lombard la oportunidad de trabajar con su amigo Robert Stack, a quien había conocido desde que era un adolescente. La película se rodó en la United Artists, lo que llevó a Lombard a decir que ella había trabajado en todos los grandes estudios de Hollywood.

"Ser o no ser," considerada en la actualidad como una de las mejores películas de Lubitsch y de la carrera artística de Benny y Lombard, no fue inicialmente muy bien recibida por el público, pues muchos no podían entender que se bromeara con una amenaza tan real como los nazis. Según las memorias incompletas de Jack Benny, publicadas en 1991, su padre salió del cine al principio de la película, disgustado de que su hijo vistiera un uniforme nazi, y se comprometió a no volver a verla. Benny le convenció de lo contrario y su padre terminó siendo un amante de la película, que vio hasta cuarenta y seis veces. 

No puede decirse lo mismo de todos los críticos. Si bien por lo general elogiaron a Lombard, algunos despreciaron a Benny y Lubitsch porque consideraban que la película era de mal gusto. Bosley Crowther, del "New York Times", escribió que era "difícil imaginar cómo alguien puede relatar, sin pestañear, una incursión aérea atacando Varsovia justo después de una secuencia cómica, o el espectáculo del Sr. Benny interpretando otra secuencia similar con un cadáver de la Gestapo. El señor Lubitsch mostró un extraño sentido del humor cuando dirigió esta película ". El "The Philadelphia Inquirer" era de la misma opinión, considerando que la película era "una obra de mal gusto al encontrar divertido el bombardeo de Varsovia". Algunos críticos se mostraron especialmente ofendidos por la afirmación del Coronel Ehrhardt: "Oh, sí que lo vi [a Benny] en "Hamlet", cuando hizo con Shakespeare lo mismo que estamos haciendo ahora con Polonia.". 

Sin embargo, otras críticas fueron positivas. "Variety" la consideró como "una de las mejores producciones de Lubitsch de los últimos años, una gran obra de entretenimiento". El "Harrison's Reports" la describió como "una interesante comedia dramática en tiempo de guerra, soberbiamente dirigida e interpretada. La acción mantiene el "suspense" en todo momento, y tanto los diálogos como las interpretaciones le hacen reir a uno casi constantemente." John Mosher, del "New Yorker", también alabó la película, escribiendo que "la comedia puede situarse en Varsovia en el momento de su invasión por los nazis y no parecer demasiado incongruente; es un triunfo Lubistch ". 

En 1943, el crítico Mildred Martin revisó otra de las películas de Lubitsch en "The Philadelphia Inquirer" y se refirió despectivamente a él por ser alemán de nacimiento y a su comedia sobre los nazis en Polonia, a lo que Lubitsch respondió con la publicación de una carta abierta al diario.

En los últimos tiempos, la película ha sido reconocida como un clásico de la comedia. "Ser o no ser" tiene un índice de aprobación del 98% en Rotten Tomatoes, con una calificación media de 8,8/10 basada en 42 comentarios resumidos así: "Una sátira compleja y oportuna que tiene tanto de solemnidad como de payasada, con un delicado equilibrio entre la ética y el humor". En una entrevista realizada en 2015, el crítico cultural y filósofo esloveno Slavoj Žižek la eligió como su comedia favorita, declarando: "es una locura, pienso que no se puede hacer una comedia mejor".

"Ser o no ser" fue candidata al .

En 1996 fue seleccionada para su conservación en el Registro Nacional de Cine de la Biblioteca del Congreso de Estados Unidos como siendo "cultural, histórica y estéticamente significativa". 

La película fue incluida, o fue candidata a su inclusión, por el American Film Institute en estas listas:





</doc>
<doc id="2376" url="https://es.wikipedia.org/wiki?curid=2376" title="The Untouchables">
The Untouchables

The Untouchables (conocida en español como Los Intocables de Eliot Ness en España, y como Los intocables en Hispanoamérica) es una película estadounidense de 1987 dirigida por Brian de Palma y protagonizada por Kevin Costner. Está inspirada en la novela "The Untouchables" (1957), escrita por Eliot Ness y Oscar Fraley. La película se estrenó el 3 de junio de 1987, siendo evaluada positivamente por la crítica especializada. Además, fue un éxito en el plano económico recaudando más de 76 millones de dólares en Estados Unidos y estuvo nominada a cuatro premios de la Academia, de los cuales Sean Connery recibió uno al .

Eliot Ness (Kevin Costner), un agente federal idealista, es el encargado de mantener el orden en el Chicago de la Ley seca. La película empieza con el atentado con bomba contra un bar en la guerra del alcohol.

En un principio Ness hace una redada en un almacén de Al Capone, pero cuando llega, en el almacén no encuentran alcohol. Ante esta perspectiva Ness sospecha que la policía de Chicago está sobornada. En un encuentro casual por la noche, Ness conoce a Malone (un policía de la vieja escuela), a quien pide que se una a su grupo para acabar con Capone. Poco después al equipo se unen Oscar Wallace (un agente del tesoro) y George Stone (un cadete de la policía). El nuevo equipo de los "intocables" limpia la ciudad y hasta hace una redada en la frontera canadiense. Capone al ver que su negocio corre peligro contrata a un asesino a sueldo llamado Frank Nitti. Poco después, Nitti recibe de Capone la orden de deshacerse del equipo de Ness y empieza a asesinarlos, empezando por Oscar Wallace a quién asesina dentro de un ascensor, luego asesina a Malone en la parte posterior de su casa, todo ello es informado personalmente a Capone. Pero como el equipo de Ness ya había ideado la forma de detener y atrapar a Capone, capturan a su contador tras un violento tiroteo entre los sicarios de Capone y los dos últimos integrantes del equipo de Ness (Stone y Ness). Pronto Capone es llevado a un tribunal, este espera tranquilamente el resultado, pero poco después Ness descubre que Nitti ingresó a la sala judicial armado y lo obliga a salir, para luego descubrir que se trata del asesino de Malone, Nitti enfrenta a Ness en el techo del Tribunal y este mofándose de la muerte de Malone provoca la ira de Ness quien lo arroja al vacío. Stone comunica a Ness que los miembros del jurado, están en una lista de sobornos de Capone, pero al mostrárselas al juez este las desestima, sin embargo Ness a solas le dice al juez que su nombre estaba en la lista de sobornos. Finalmente el juez del caso ordena el cambio de jurados, provocando no solo las protestas de Capone sino el cambio de opinión de su abogado quien lo acusa formalmente, Ness se retira de la Policía dejando a Stone como su reemplazo, el mismo día en los EE. UU. se derogaba la Prohibición al alcohol y un periodista se lo comunica a Ness, quien le dice que se tomará un trago.



</doc>
<doc id="2377" url="https://es.wikipedia.org/wiki?curid=2377" title="Dead Poets Society">
Dead Poets Society

Dead Poets Society (en España, El club de los poetas muertos; en Hispanoamérica, La sociedad de los poetas muertos) es una película del año 1989 dirigida por Peter Weir, con guion de Tom Schulman y protagonizada por Robin Williams. Narra el encuentro de un profesor de literatura con un grupo de alumnos durante 1959 en la Welton Academy (Vermont), institución señera y prestigiosa. 

Ganó un Óscar al mejor guion original. En 1991 se publicó una adaptación del guion original, en forma de novela, escrita por la editora Nancy H. Kleinbaum.

Cuando esperan la presentación del nuevo profesor, este les pide que salgan del salón y en el pasillo les cita un poema que Walt Whitman le dedicó al presidente Abraham Lincoln: "Oh capitán, mi capitán". De repente, les señala una orla de la primera generación de estudiantes del colegio y les dice que ellos no entendieron el concepto del "carpe diem" y que ahora, desde el más allá, piden a los nuevos estudiantes que no pierdan lo que no podrán volver a recuperar: el tiempo. En clase, el profesor les pide que observen el gráfico de coordenadas que la introducción del libro utiliza para definir la poesía, y él lo califica como «basura» y les dice que arranquen esa página, pues su concepción de poesía es que no tiene estructura, ni normas. Solo crea y piensa en algo, dale el énfasis que necesitas y rompe esquemas.
Con esta presentación, los cuatro amigos se interesan por saber quién es su extraño profesor y descubren, mediante el anuario de su promoción, que formó parte de la sociedad de los poetas muertos. Cuando le preguntan directamente en el patio del colegio, él les explica que el grupo se reunía en la cueva a la que llamaban India y escribían poesía, pensaban libremente y expresaban sus emociones a través de «una verborrea que fluía como la savia de un árbol herido». Los chicos deciden crear un nuevo Club de los poetas muertos y, encabezados por Neil Perry, una noche se escapan a la cueva y comienzan un ritual: el que se ve reflejado en que la cueva está libre de los prejuicios de la sociedad y no hay nadie que pueda oprimirlos. Les empieza a gustar la poesía y continúan reuniéndose en la cueva. Neil Perry (Robert Sean Leonard), que siempre ha querido ser actor y siempre ha permanecido bajo el yugo de su padre, pero a sus espaldas consigue el rol de protagonista en una obra de Shakespeare. Todd Anderson (Ethan Hawke) consigue perder la timidez con la poesía. Knox Overstreet (Josh Charles) se declarará a una joven sin importarle lo que pudiera suceder. Y Charles Dalton (Gale Hansen) invita a dos chicas a la cueva y firma un artículo «rebelde» en la revista de la academia planteando la entrada de mujeres en el colegio con «el club de los poetas muertos». Después de todo ello, surge su seudónimo: "Nuwanda".

Pero Neil no se anima a hablar con su padre, en lugar de ello, miente al profesor diciendo que su padre parece aceptar que protagonice "El sueño de una noche de verano" y que aproveche la oportunidad de ser actor. Pero, cuando llega el día del estreno, su padre aparece en el teatro y después de la obra no solo no lo felicita sino que además le dice que va a sacarlo de la academia y lo enviará a una institución militar para que luego estudie medicina y que recién entonces podrá elegir sobre su destino. Por ello, decide colocar en la ventana la corona que había utilizado en la obra de teatro y se suicida con el revólver de su padre. Tras la muerte del joven todos sus compañeros menos uno culpan a su padre, pero la institución decide inculpar al profesor «rebelde» de literatura John Keating (Robin Williams). Cuando el profesor se marcha, más de la mitad de sus alumnos se suben encima de sus bancos y le dicen "Oh capitán, mi capitán". Conmovido, el profesor responde con un «"gracias, chicos, gracias"».

El guion fue escrito por Tom Schulman, basado en sus experiencias en la Academia de Montgomery Bell en Nashville, Tennessee, donde fue particularmente inspirado en un maestro de inglés poco convencional llamado Samuel Pickering.

Inicialmente se quiso rodar el filme en la ciudad de Rome (en el estado de Georgia), pero finalmente se decidió cambiar el lugar de rodaje al estado de Delaware, ya que el clima de la zona ofrecía nieve natural y así se ahorraban el problema y el gasto de tener que fabricar la gran cantidad de nieve artificial necesaria para conseguir una buena recreación del campus estudiantil, aunque en realidad la obra estaba ambientada en el estado de Vermont.

La película tuvo un éxito arrasador. Tuvo además más éxito internacionalmente que en Estados Unidos, su país de producción, y también fue más apreciada por los críticos en el extranjero que en los Estados Unidos.




</doc>
<doc id="2379" url="https://es.wikipedia.org/wiki?curid=2379" title="The Sixth Sense">
The Sixth Sense

The Sixth Sense (titulada El sexto sentido en España y Sexto sentido en Hispanoamérica) es una película estadounidense del género de suspense de 1999 dirigida por el realizador indio
M. Night Shyamalan, sobre un guion original propio. Relata la experiencia de un psicólogo que intenta descubrir cómo ayudar a un niño a enfrentar la terrible verdad de sus poderes sobrenaturales. Su estreno provocó un fenómeno de público y de crítica, algo poco visto en ese entonces con respecto a las películas de su género, que mezcla el drama con el suspense y el terror. Su protagonista Haley Joel Osment cobró fama mundial por su interpretación del pequeño Cole, que se ve atormentado por extraños sucesos paranormales. Su papel le proporcionó una nominación al Premio Óscar al . La película obtuvo un total de seis nominaciones, incluida la de mejor película.

El Dr. Malcolm Crowe (Bruce Willis), un psicólogo infantil en Filadelfia (Estados Unidos), regresa a su casa una noche para reunirse con su esposa, Anna Crowe (Olivia Williams), después de haber sido homenajeado por su trabajo. Anna le reprocha a Malcolm que ella ha pasado a segundo plano por su trabajo. Los dos entonces descubren que no están solos: un joven aparece blandiendo una pistola. Dice que ya no quiere tener miedo y acusa a Malcolm de no haberlo ayudado. Malcolm lo reconoce como Vincent Grey (Donnie Wahlberg), un ex paciente a quien trataba cuando era un niño, porque sufría alucinaciones. Vincent le dispara a Malcolm en el abdomen, y se suicida de un disparo en la cabeza.

En el siguiente otoño, Malcolm comienza a trabajar con otro paciente, un niño de 9 años llamado Cole Sear (Haley Joel Osment), cuyo caso es similar al de Vincent. Malcolm se dedica al niño, a pesar de dudar acerca de su capacidad para ayudarlo, después de su fracaso con Vincent Grey. Mientras tanto, siente que se está distanciando de su esposa Anna, e incluso cree que ella podría estar contemplando un romance con un compañero de trabajo que sigue apareciendo en la casa.

Una vez que Malcolm gana la confianza del niño Cole, éste finalmente confiesa sus alucinaciones: «En ocasiones veo muertos, caminando como gente normal. Ellos no saben que están muertos». Le cuenta algunos casos que le han sucedido recientemente: Cole escucha una voz que le pide ayuda para dejarlo fuera de un armario oscuro, y le grita que él no robó «el caballo del maestro» y amenaza con atacar a Cole. También se le ha aparecido una mujer maltratada por su marido, que se ha cortado las muñecas. Otro «fantasma» es un niño con una gran salida de bala en la parte posterior de su cabeza, que invita a Cole a ver el revólver de su padre.

Malcolm piensa que Cole es delirante y planea dejar su tratamiento. Recordando a Vincent, el psicólogo escucha una cinta de audio que Malcolm había grabado de una sesión de psicoterapia entre él y Vincent (entonces un niño), en 1987. En la cinta, Malcolm sale un momento de la habitación, con Vincent riendo relajado; pero cuando regresa, Vincent está llorando. Al subir el volumen al máximo, Malcolm alcanza a oír la voz de un hombre llorando y suplicando angustiado, que es lo que aterroriza a Vincent. Malcolm asume entonces que Cole le dice la verdad y que puede tener la capacidad de ver y oír a las personas muertas. Malcolm le sugiere a Cole que en vez de concentrarse en el terror que le producen los fantasmas, trate de encontrar el propósito de su don de comunicarse con ellos; y tal vez ayudarles con sus asuntos sin terminar en la Tierra. Al principio, Cole no está dispuesto ya que los fantasmas lo aterrorizan, pero finalmente decide probarlo.

Así, Cole habla con uno de los fantasmas, una niña muy enferma ―Kyra Collins (Mischa Barton)― que aparece en su habitación y vomita. Él decide escuchar a la niña y ella le pide que vaya a su casa durante la recepción de su funeral. Él entonces va hasta allá, en compañía de Malcolm. Kyra había muerto después de una enfermedad prolongada y se escuchan los comentarios de los asistentes al funeral, que cuentan que la hermana menor de Kyra está empezando a enfermar también. Cole se introduce en el dormitorio de Kyra, y esta se le aparece y le da una caja, con una cinta de video. Cole entrega la caja al padre de Kyra, y al mirar el video ve a la madrastra de Kyra, que aparentemente la había cuidado a lo largo de toda su enfermedad, poniendo líquido limpiador de pisos en la comida de la niña, para envenenarla.

Al aprender a convivir con los fantasmas, Cole empieza a encajar en la escuela y obtiene un papel importante en una obra de teatro, a la que asiste Malcolm. El médico y el paciente aparecen muy contentos después la representación. Cole le sugiere a Malcolm que trate de hablar con Anna mientras ella está durmiendo. 

Luego, camino a casa, Cole confiesa su secreto a su madre, Lynn (Toni Collette). Como su madre en un principio no le cree, para convencerla, Cole le cuenta que ve a su abuela muerta y le dice a Lynn que ella fue a verla actuar en un recital de baile una noche, cuando era una niña, y que Lynn no se dio cuenta porque su madre se había quedado en la parte posterior de la audiencia, donde no podía ser vista. También le dice que la abuela le contó que después de morir, Lynn le hizo una pregunta cuando estaba sola ante su tumba: «¿Te sientes orgullosa de mí?». Y la respuesta era: «Todos los días». Entonces, Lynn acepta con lágrimas, la verdad de su hijo.

Malcolm regresa a su casa, donde encuentra a su mujer dormida en el sofá con la reproducción de video de la boda de la pareja, que Anna observa con frecuencia. Mientras ella duerme, le pregunta entre sueños a Malcolm por qué la abandonó. Malcolm siente que él no la dejó nunca, sino que su relación se ha ido enfriando y se han alejado a uno del otro. Entonces de la mano de Anna cae el anillo de bodas de Malcolm, quien sólo entonces descubre que no lo lleva puesto.

Aquí viene la revelación de la película: Malcolm recuerda que Cole le había dicho que de los fantasmas no saben que han muerto, y se da cuenta de que él realmente fue asesinado por Vincent hace un año, y que todo el tiempo que estuvo trabajando con Cole era un fantasma, sin percatarse. Gracias a los esfuerzos de Cole, Malcolm consigue terminar sus asuntos, rectifica su falta de comprensión y ayuda a Vincent a estar finalmente completo, a través de la ayuda que le proporciona a Cole. Recordando el consejo de Cole, Malcolm habla a su esposa dormida y cumple la segunda razón por la que permaneció, diciéndole que ella «nunca estuvo en segundo lugar», y que él la ama. Al dejarla vivir su propia vida, él es libre de dejar el mundo de los vivos.




</doc>
<doc id="2380" url="https://es.wikipedia.org/wiki?curid=2380" title="Zorba, el griego">
Zorba, el griego

Zorba, el griego ("Βίος και Πολιτεία του Αλέξη Ζορμπά", Vida y aventuras de Alexis Zorbas) es una novela escrita por el escritor griego Nikos Kazantzakis, publicada en 1946. Es la historia de un joven intelectual griego que escapa de su monótona y aburrida vida con la ayuda del bullicioso y misterioso Alexis Zorba. Fue adaptada en 1964 al cine en la versión dirigida por Michael Cacoyannis, del mismo nombre, así como a musical en 1968.

El libro se inicia en un café de El Pireo, justo antes del amanecer en una tormentosa mañana de otoño en la década de 1930. El narrador, un joven intelectual, se decide a dejar de lado sus libros por unos meses después de ser alentado por las palabras de un amigo, Stavridakis, que ha dejado los estudios para ir al Cáucaso con el fin de ayudar a griegos étnicos que están sufriendo persecución. Inicia un viaje a Creta con el fin de reabrir una antigua mina de lignito y sumergirse en el mundo de los campesinos y la gente de la clase trabajadora.

Está a punto de sumergirse en su ejemplar de "La Divina Comedia" cuando siente que está siendo observado; se da la vuelta y ve a un hombre de unos sesenta años mirándolo a través del cristal de la puerta. El hombre entra y de inmediato se le acerca a pedir trabajo. Dice ser experto como cocinero, minero y tañedor del "santur" y se presenta como Alexis Zorbas. El narrador se siente fascinado por el lenguaje lascivo y expresivo de Zorba y decide darle trabajo como capataz. En su camino hacia Creta, hablan de un gran número de temas y los soliloquios de Zorba establecen el tono para gran parte del libro.

A su llegada, rechazan la hospitalidad de Anagnostis Kondomanolious, el dueño del café local, y siguen la sugerencia de Zorba de ir al hotel de la señora Hortensia, que no es más que una fila de viejas casetas de baño. Se ven forzados por las circunstancias a compartir una caseta. El narrador pasa el día describiendo la isla, el paisaje de la cual le recuerda a ""la buena prosa, cuidadosamente ordenada, potente y sobria"", y leyendo a Dante. Al regresar al hotel para la cena, invitan a la señora Hortensia a su mesa y hacerla hablar de su pasado de cortesana. Zorba le da el mote de "Bouboulina" y, con la ayuda de su címbalo, la seduce. El espíritu del protagonista hierve en su habitación mientras escucha los sonidos de su amor apasionado.

Al día siguiente, la mina se abre y comienza a trabajar. El narrador, cuyos ideales socialistas le llevan a intentar confraternizar con los trabajadores, es advertido por Zorba de que debe mantener las distancia. "El hombre es una bestia. Si usted es cruel con él, le respetará y temerá; si es amable con él, le sacará los ojos". Por su parte, Zorba se sumerge en la tarea, que es una característica de su actitud: ser absorbido en lo que uno está haciendo o con quien se está en ese momento. Con bastante frecuencia, Zorba trabaja infinidad de horas y pide no ser interrumpido durante el trabajo. 

El narrador recupera su entusiasmo vital junto a Zorba y las personas que lo rodean, pero al final, la tragedia marcará su estancia en Creta y regresará a tierra firme completamente arruinado. Su despedida de Zorba, quizá por la falta de una explosión de sentimientos, resulta desgarradora, tanto para Zorba como para el narrador. Los dos se recordarán el uno al otro hasta su muerte.


"Zorba, el griego" fue llevada al cine en una película británica en 1964, escrita, producida, dirigida y montada por Michael Cacoyannis y con Anthony Quinn, Alan Bates, Irene Papas y Lilia Kédrova en los papeles principales. Fue galardonada con tres Premios Óscar en 1965, en las categorías de mejor actriz secundaria (Lila Kedrova), mejor fotografía (Walter Lassally), y mejor dirección artística (Vassilis Photopoulos). 

La música compuesta para el film por Mikis Theodorakis, conocida como Danza de Zorba, es un elemento ampliamente reconocido.

En el año 2003 se produjo, profesionalmente, la versión teatral de Zorba, el griego, producida por Alejandro Romay, con el siguiente reparto: Raúl Lavié, María Rosa Fugazzot, Miguel Habud, Julia Zenko, Rubén Ballester, Alejandro Viola (reemplazado, posteriormente, por Gustavo Monje), Marcelo Trepat, Andrea Mango y Roberto Fiore.


</doc>
<doc id="2384" url="https://es.wikipedia.org/wiki?curid=2384" title="País">
País

Un país (del francés "pays") es un territorio con características geográficas y culturales propias, que puede o no constituir un Estado soberano o una entidad política dentro de un Estado. Es utilizado también como sinónimo de Estado, conjunto de instituciones políticas dotadas de territorio, población y soberanía.

Nación tiene dos acepciones: la nación política, en la escena jurídico-política, es el sujeto político en el que reside el poder constituyente de un Estado; la nación cultural, concepto socio-ideológico más subjetivo y ambiguo que el anterior, puede definirse como una comunidad humana con ciertas características culturales comunes a las que se les dota de un sentido ético-político. La palabra nación se emplea en la vida cotidiana con múltiples significados: estado, país, territorio o habitantes de ellos, etnia y otros.

En España se da el caso de las nacionalidades históricas, realidad nacional, carácter nacional o, simplemente, nacionalidad; son términos acuñados "ad hoc" para la política de este país, usados para designar a aquellas comunidades autónomas con una identidad colectiva, lingüística y/o cultural diferenciada, según sus estatutos autonómicos, del resto del Estado. Además, también se utiliza la palabra país en el propio nombre de la comunidad autónoma del País Vasco. Por otro lado, las regiones catalanoparlantes (incluida Andorra) son a menudo denominadas Países Catalanes. Finalmente, tanto en el discurso político actual como en el habla popular en Galicia, es de uso extendido el término país para referirse a esta comunidad histórica. Un ejemplo de este uso se encuentra en el etiquetado de ciertos productos de origen gallego (e.g. queso del país, vino del país).

Un Estado es un conjunto de instituciones que poseen la autoridad para establecer las normas que regulan una sociedad, teniendo soberanía interna y externa sobre un territorio definido. El Estado incluye el control de instituciones tales como las Fuerzas armadas, administración pública, los tribunales y la policía.

Según algunas escuelas de ciencia política, un estado nación se caracteriza por tener un territorio claramente definido, una población constante, si bien no fija, y un gobierno. Otros atributos menores son un ejército permanente y un cuerpo de representación diplomática, es decir, una política exterior.

El Estado Nación se crea, históricamente, mediante el tratado de Westfalia, al finalizar la guerra de los 30 años (1648). Mediante este tratado finaliza el antiguo orden feudal y se da paso a organizaciones territoriales y poblacionales definidas en torno a un gobierno que reconoce sus límites espaciales, y por lo tanto, de poder.

La forma de gobierno es un término que se refiere al conjunto de las instituciones políticas mediante las cuales un estado se organiza para ejercer sus poderes sobre una comunidad política. Existen diferentes formas de gobernar un estado, como pueden ser una monarquía, república u otros como un sistema unipartidista o una dictadura militar.

Diversas organizaciones internacionales, es decir, agrupaciones formadas por diversos estados, se han ido creando a lo largo del siglo XX. Las organizaciones internacionales pueden ser de diversos tipos: 


País constituyente es un término a veces usado, normalmente por instituciones oficiales, en contextos en los cuales un número de países compone una larga entidad o agrupación; así la OCDE ha usado el término refiriéndose a la antigua Yugoslavia, y las instituciones europeas como el Consejo de Europa frecuentemente la usan en referencia a la Unión Europea.

Los territorios dependientes son territorios que por diferentes razones no poseen privilegios de total independencia o soberanía y, por lo tanto, son gobernadas por otros estados, llamados metrópoli. Muchos de estos territorios pueden ser considerados como colonias.

Los territorios dependientes cuentan con un sistema administrativo diferente al de la metrópoli o a las unidades que conforman la metrópoli. Por lo general gozan de menores derechos administrativos y políticos que una subunidad nacional. Este tipo de administración varía según el nivel de dependencia del territorio. Así existen territorios completamente deshabitados.

La mayoría de estos estados dependientes corresponden a islas de baja población que no pueden sostener un gobierno autónomo.

Las fronteras son líneas imaginarias que marcan el territorio de un país y que lo separan del o de los países colindantes. De esta forma se delimita el espacio en el que un país ejerce su soberanía.

Un territorio disputado es aquel territorio cuya soberanía es ambicionada por dos o más países. Normalmente la administración del territorio la lleva a cabo uno de los países que reclama la soberanía, mientras que el otro país no reconoce la soberanía sobre el territorio del otro país. Esto no suele ocurrir en áreas terrestres o marítimas sobre las que ninguno posee el control efectivo, como por ejemplo la Antártida, o solo lo tiene parcialmente. También se puede considerar como un territorio disputado a aquellas zonas que están administradas por dos gobiernos distintos, y por lo tanto están divididas; un ejemplo es la República Turca del Norte de Chipre y Chipre.



</doc>
<doc id="2385" url="https://es.wikipedia.org/wiki?curid=2385" title="Pueblo (desambiguación)">
Pueblo (desambiguación)

Pueblo, del latín "populus" hace referencia a los siguientes conceptos:






</doc>
<doc id="2386" url="https://es.wikipedia.org/wiki?curid=2386" title="Península ibérica">
Península ibérica

La península ibérica se encuentra situada en el sudoeste de Europa; está rodeada por el mar Mediterráneo y el océano Atlántico uniéndose al resto del continente por el noreste. Es una de las tres grandes penínsulas meridionales de Europa, junto a la itálica y la balcánica y mide aproximadamente 596 740 km². Tradicionalmente se ha establecido la frontera de la península en la cordillera pirenaica, si bien el istmo se encuentra situado en la línea recta que une el punto central de los golfos de Vizcaya y León —entre las ciudades de Bayona y Narbona—, quedando por tanto comprendida una franja meridional de territorio francés al sur del istmo.

Ocupan casi toda la superficie peninsular dos países, España y Portugal, además de Andorra, el territorio británico de Gibraltar y, de forma parcial, los departamentos franceses de la zona pirenaica, si bien estos últimos, convencionalmente, no se tienen en cuenta. Históricamente, se ha denominado «península ibérica», «Iberia», «península hispánica» o «península hespérica» al territorio continental situado «más allá» de los Pirineos. La expresión «península ibérica» fue acuñada por primera vez por el geógrafo francés Jean-Baptiste Bory de Saint-Vincent en su obra "Guide du voyageur en Espagne" del año 1823.

En la actualidad, en español recibe el nombre de «península ibérica». Distintos nombres de este accidente geográfico en otras lenguas y dialectos hablados en mayor o menor medida en el territorio serían el de «Península Ibérica» (portugués, gallego, asturiano y extremeño), «Península Ibèrica» (valenciano,catalán), «Iberian Peninsula» (inglés), «Péninsule Ibérique» (francés), «Peninsula Iberica» (aragonés y occitano) e «Iberiar penintsula» (euskera).

Antaño, historiadores y geógrafos de cultura griega, como Heródoto, Estrabón y Apiano, la denominaron «Iberia». Los escritores romanos a su vez le daban el nombre de «Hispania».

Su nombre proviene del río "Íber", probablemente el actual Ebro, aunque también pudiera ser otro río de la provincia de Huelva, donde textos muy antiguos citan un río "Iberus" y un pueblo al que llaman iberos. En un principio, en la Grecia arcaica los griegos pudieron llamarla «Hesperia», ya que Hesperia era descrita como la zona más occidental del Mediterráneo, aunque no se sabe con certeza si se refiere a la actual España, Marruecos o ambas. Más tarde los griegos pasarían a llamarla «Iberia».

Polibio, un historiador griego del que vivió un tiempo en la península, escribe:
Estrabón dedica el "Libro III" a la península ibérica.
Apiano de Alejandría (), en su "Historia romana", escribe:
La historiografía se refiere a ella también con otros vocablos, según el contexto histórico: «Celtaria», «Celtiberia», «Tierra de Tartessos», «Sefarad», «Al-Andalus», etc. Algunos eruditos, como Dámaso Alonso, estimaban que el nombre de Península Hispánica sería más adecuado.

Iberia fue el nombre dado por los griegos a la península, aunque la parte que más conocían era la zona meridional mediterránea, en torno al río Íber. Hispania era el nombre utilizado por los romanos para designar a la península ibérica, posiblemente de origen púnico (véase Origen del nombre de Hispania).

Tras la conquista musulmana recibió el nombre de al-Ándalus, pasando a ser parte de la provincia norteafricana del Califato Omeya (711 a 1492) para más tarde convertirse en el Emirato de Córdoba y posteriormente en el Califato de Córdoba independiente del Califato Abasí. Con la disolución del Califato de Córdoba en 1031, el territorio se dividió en los primeros reinos de taifas, periodo al que sucedió la etapa de los almorávides, los segundos reinos de taifas, la etapa de los almohades y los terceros reinos de taifas.

Más adelante, después de la unión dinástica de las Coronas de Corona de Castilla y Aragón y las conquistas de Granada y la mayor parte del Reino de Navarra, se empezó a llamar España a los territorios resultantes, por simplificación entre los no españoles, aunque la unificación jurídica de todos estos reinos no estuviera consolidada hasta el , con los Borbones.

Hasta finales del , inicios del , todos los pueblos de la "península ibérica" se consideraban españoles, como actualmente los diversos pueblos de Escandinavia se consideran escandinavos, o los de la península balcánica se consideran balcánicos. Con dificultad los portugueses se sintieron obligados a dejar de llamarse también españoles, a fin de no ser tomados por castellanos, a medida que se desarrollaba la castellanización de otros reinos de la antigua Hispania.

Por la manifiesta imposibilidad histórica, política y cultural demostrada de continuar llamando a los portugueses «españoles», sin que pudieran ser confundidos con los castellanos por otros pueblos que gobernaban dentro y fuera de la "península hispánica", se empezó, desde entonces, a utilizar la expresión "ibérico" para designar a los "dos pueblos" de la "península hispánica", ahora preferentemente llamada por el neologismo "península ibérica". Este proceso fue paralelo y similar al que surgió en el exterior de llamar español al idioma castellano, convertido en la única lengua oficial por el gobierno español, hasta que cambiaron la designación oficial del Estado, con la creación de la denominación oficial: Reino de España en el , y el cambio del título de los reyes de León, Castilla, Aragón, Sicilia, etc. para los reyes de España con fines simbólicos de unificación administrativa y para la nueva presentación internacional de la monarquía de la meseta.

La península tendría unos , incluyendo cerca de correspondientes a la parte al sur del istmo, pero al norte de los Pirineos, territorio francés. Si se excluye esa zona el área ascendería a unos .

Dentro de sus límites geográficos se encuentran España peninsular, Portugal continental, Andorra y Gibraltar, además de la mencionada franja meridional francesa. Por el sur, la península está separada de África (Marruecos y Argelia) por el mar Mediterráneo (que en esta zona se denomina mar de Alborán) y el océano Atlántico, siendo el estrecho de Gibraltar el límite entre ambos. El punto más alto es el Mulhacén de una altitud de 3478,6 m sobre el nivel del mar. El río más largo es el Tajo, con una longitud de 1007 km (731 km en España y 275 km en Portugal).

El geógrafo griego Estrabón, hablando de la península ibérica, la compara con una piel de toro:

Por su extensión, la península ibérica ocuparía, en caso de ser reunificada bajo un mismo Estado (como lo estuvo entre 1580 y 1640, con la excepción de Andorra), el lugar 48 como país más grande del mundo y por su población ocuparía el lugar número 24.

No hay un consenso científico sobre donde está situado el centro geográfico de la península ibérica. Tradicionalmente las localidades madrileñas de Getafe, con el cerro de los Ángeles como punto geodésico, y Pinto, en la confluencia de las calles Maestra María del Rosario y del Hospital, se disputan desde hace siglos dicho reconocimiento. Sin embargo más recientemente han salido otros estudios que situarían el centro geográfico en la provincia de Toledo.

Su topografía tiene como principal característica que la mayor parte de su superficie está configurada como una meseta, con ligera pendiente hacia poniente; esta tiene una altura media de seiscientos metros sobre el nivel del mar; el litoral es rocoso y con acantilados al norte, nordeste, noroeste y sureste, siendo más suave la mayor parte del litoral este y sur.


Los principales ríos de la península ibérica son, en orden descendiente de longitud, el Tajo, el Ebro, el Duero, el Guadiana y el Guadalquivir. Todos ellos tienen más de seiscientos kilómetros de longitud cada uno.

La geología de la península ibérica responde a una larga historia geológica, desde los tiempos proterozoicos hasta la actualidad, reflejando fusiones y roturas de continentes, apertura de océanos e importantes episodios orogénicos. Las huellas y cicatrices de esta historia configuran la corteza continental, la estructura y naturaleza de las rocas ígneas, metamórficas y sedimentarias que componen la península así como los actuales relieves.

Su aislamiento geográfico ha permitido el desarrollo de una flora y fauna características que incluyen un importante número de taxones endémicos. Como dato interesante hay que destacar que en España hay 17 804 millones de árboles y que cada año crecen una media de 284 millones más, según un estudio elaborado por la Sociedad Española de Ciencias Forestales en septiembre de 2009.

España es el segundo país de la Unión Europea con más superficie forestal, un total de 26,27 millones de hectáreas o el 57 % de su territorio, siendo la superficie arbolada, según el tercer inventario forestal, de 14,73 millones de ha y el resto de matorral mediterráneo.

La flora de la península, por sus condiciones biohistóricas, geográficas, geológicas, orográficas, etc., es una de las más ricas y variadas de toda Europa, comparable a la de países mediterráneos como Grecia e Italia e incluso de mayor diversidad; se calcula que incluye más de 8000 especies de plantas, muchas de ellas endémicas.

El Mediterráneo ha estado sometido en el pasado a grandes alteraciones de clima y vegetación, unido a unas variaciones, a veces muy grandes, en el nivel del mar y a variaciones en las posiciones relativas de las masas continentales (placas europea y africana). Con la entrada de plantas y el aislamiento, debido a las fluctuaciones marinas o a las periódicas glaciaciones, se puede encontrar una variada diversidad de especies vegetales.

La península ibérica, situada en una importante vía de paso entre África y Europa, se vio enriquecida con la llegada, según cambiaba el clima, de plantas esteparias, termófilas, xerófilas, orófilas y boreo-alpinas, muchas de las cuales lograron mantenerse después, gracias a la diversidad de medios que existen en las cadenas montañosas, que les permiten subir en altitud si el clima se va haciendo más cálido, o descender si se vuelve más frío. La complejidad geológica de la mayoría de las montañas ibéricas, especialmente de las Béticas, Sistema Ibérico y Pirineos, aumentó aún mucho más el número de nuevos medios a que adaptarse e hizo posible la diversidad y riqueza de la flora actual.

La región eurosiberiana está representada por la zona atlántica, que se extiende desde el norte de Portugal, la mayor parte de Galicia, Principado de Asturias, Cantabria, País Vasco, noroeste de Navarra y Pirineos occidentales y centrales. No obstante, su influencia en forma de comunidades o especies concretas se extiende en muchos puntos hacia el interior, especialmente en la las mitades norte y occidental. Se caracteriza por un clima húmedo, suavizado por la influencia oceánica, con inviernos templados-fríos y con una estación seca poco acentuada. 

La vegetación está representada por bosques caducifolios de robles ("Quercus petraea") y carballos ("Quercus robur"), con fresnedas de "Fraxinus excelsior" y avellanares en los suelos más frescos y profundos de fondo de valle. El piso montano se caracteriza por la presencia de hayedos y a veces, en los Pirineos, por abetales de "Abies alba"; estos hayedos y abetales ocupan las laderas frescas y con suelo profundo de las montañas no muy elevadas. La influencia mediterránea se siente en la presencia de encinares con laurel, que se sitúan en las crestas y laderas más cálidas, especialmente sobre suelos calizos, donde se acentúa la sequedad.

El aprovechamiento por el hombre a través de la historia ha transformado muchos de estos bosques en prados, que conservan en sus lindes restos de los setos o especies del primitivo bosque. La orla natural está formada por setos y espinares que se instalan en los calveros y partes aclaradas; están integrados por rosas silvestres, zarzas, endrinos, majuelos y otros arbustos más o menos espinosos; también pueden representar este papel, los piornales y retamares. Los siguientes son los principales bosques de esta zona.


La fauna de la península ibérica presenta una amplia diversidad que se debe en gran parte a dos factores, la posición geográfica de la península ibérica, entre el Atlántico y el Mediterráneo y entre África y Eurasia, y la gran diversidad de hábitats y biotopos, consecuencia de una variedad considerable de climas y regiones bien diferenciadas.

Entre los grandes carnívoros destacan dos especies desaparecidas de buena parte de Europa occidental: el oso pardo, que sobrevive en la cordillera Cantábrica y en ciertos enclaves pirenaicos, y el lobo ibérico, subespecie endémica de la península ibérica. Aunque el carnívoro más emblemático es sin duda el lince ibérico, el félido más amenazado de todo el continente europeo. Mucho más numerosas son las poblaciones de gato montés, de zorro rojo y las de algunos mustélidos: el tejón, el turón y la comadreja; algo menos numerosas son las de nutria, marta y garduña. Los vivérridos están representados por la jineta; y los herpéstidos, por el meloncillo.

Los herbívoros están representados por especies bastante extendidas, como algunos cérvidos: el ciervo común, el gamo y el corzo. Hay poblaciones endémicas de cabra montés y reductos pirenaicos y cantábricos de rebeco. También está ampliamente extendido el jabalí.

La península ibérica tenía una población en 2020 de , sumadas las poblaciones de la España peninsular, Portugal continental, Andorra y Gibraltar.


La península ibérica comprende gran parte de España, Portugal, Andorra y Gibraltar. Históricamente, se ha denominado península ibérica al territorio continental europeo que está situado al sur de los Pirineos.



</doc>
<doc id="2391" url="https://es.wikipedia.org/wiki?curid=2391" title="Tácito">
Tácito

Cornelio Tácito  (c. 55-c. 120) fue un político e historiador romano de época flavia y antonina. Escribió varias obras históricas, biográficas y etnográficas, entre las que destacan los "Anales" y las "Historias".

Se sabe poco de la biografía de Cornelio Tácito; ni siquiera se conocen las fechas y lugares de su nacimiento y muerte o su primer nombre o "praenomen", aunque se le han atribuido sin suficientes pruebas los de Cayo y Publio. La mayoría de las referencias sobre su vida que se poseen se han extraído de su correspondencia con Plinio el Joven o de sus propias obras.

Su fecha de nacimiento se conjetura a partir de la información que da Plinio en "Cartas", 7.20 cuando destaca la amistad excepcional que los une y el paralelismo de sus actividades, informando a su vez de cómo él era un jovenzuelo cuando ya Tácito disfrutaba de renombre. De ahí se ha deducido que son contemporáneos, aunque Tácito debió haber sido algo mayor. Y como se conoce la fecha de nacimiento de Plinio en el año 61 o 62, se puede estimar la fecha de nacimiento de Tácito en torno al año 55. En cuanto a la fecha de su fallecimiento se supone que, si como era su propósito llegó en su vejez a historiar el imperio de Trajano, tuvo que morir ya en tiempos de Adriano, por lo que se situaría en torno al año 120.

A veces se ha pretendido que nació en Interamnum, en Umbría (hoy Terni). La base de esta hipótesis es que Marco Claudio Tácito, emperador efímero que gobernó durante unos meses entre los años 275 y 276, había nacido allí y pretendía ser descendiente del historiador. Otras hipótesis, fundadas en la procedencia de algunos de sus íntimos, lo hacen originario del norte de la provincia de Italia o incluso de la Galia Narbonense; nada concluyente, en suma. Sin embargo, una anécdota que narra Plinio ("Cartas", 9.23) hace pensar que sus orígenes no eran itálicos, sino provinciales. 

Se cree que su familia era de origen ecuestre, pues se lo relaciona con un Cornelio Tácito de esa clase social al que menciona Plinio el Viejo (7.76) como procurador en la Galia Bélgica. Por su edad éste no podría ser el historiador, pero sí su padre o su tío.

Hacia el año 77 inicia su carrera política, que habría de ser muy regular. Él mismo ("Historias" 1.1) cuenta que la comenzó con Vespasiano y fue favorecida sucesivamente por Tito y Domiciano.

En el año 78 se casó con la hija de Cneo Julio Agrícola, el gobernador de Britannia, al que habría de dedicar tras su muerte una monografía. Siendo emperador Domiciano, en el año 88, fue pretor y quindecenviro responsable del culto y en ese mismo año participó en la celebración de los Juegos Seculares ("Ludi Saeculares"). El año 93 falleció Agrícola cuando Tácito y su esposa estaban ausentes de la ciudad y como Tácito ("Agricola", 45) afirma que la ausencia duró cuatro años, algunos piensan que desempeñaba algún cargo administrativo en provincias, en torno a lo cual se han hecho varias conjeturas carentes de solidez.

Fue "consul suffectus" en el año 97 bajo Nerva para sustituir al cónsul Lucio Verginio Rufo, muerto durante su mandato, cuyo encomio fúnebre se encargó de pronunciar; más tarde (112-113), ya bajo el imperio de Trajano, fue procónsul, es decir, gobernador de la provincia de Asia, según una inscripción hallada en Mylasa.

Su dedicación a la oratoria le ganó muy pronto un alto renombre gracias a su elocuencia; se había formado en contacto con los mejores abogados de su tiempo, pues él mismo afirmó en su "Diálogo sobre los oradores", 2, que en su juventud había escuchado con una pasión propia de la edad, y tanto en público como en privado, a Marco Apro y a Julio Segundo, luminarias del foro en esos momentos. No han faltado quienes piensen en la posibilidad de que de la misma manera que Plinio el Joven hubiera podido ser alumno de Quintiliano, pero no hay dato alguno que permita asegurarlo, si bien no cabe duda de que los rasgos del propio "Diálogo...", muy diferentes de los que él mismo cultivó en sus obras históricas, corresponden al pensamiento y estilo del gran rétor, cuya influencia, unida a la de Cicerón, es indudable. Pero la autoría de a Tácito sobre esta obra ha sido discutida. 

No se dedicó a la historia hasta después del año 97, cuando la muerte de Domiciano le permitió expresarse sin temor. Y esta aplicación al género en su madurez, tras culminar una importante carrera civil, así como el hecho de que su ideología política esté en el fundamento de su obra, lo aproximan al perfil de algunos historiadores republicanos como César o Salustio. Para un hombre noble había varias formas de servir al Estado: la actividad política y la milicia fundamentalmente y, una vez desempeñadas esas actividades, era beneficioso prestar servicios de otro tipo, como explicar los hechos y situaciones por los que había pasado Roma. Era lo que afirmaba Salustio ("Guerra de Catilina", 3): «Es hermoso obrar bien con el Estado, sin embargo no carece de sentido hablar bien de él además. Es lícito llegar a destacar en la guerra y en la paz». 
La "virtus", el conjunto de características que hacen bueno a un hombre, durante la guerra se basa en el valor. En la paz, escribir historia puede ser también manifestación de esa misma "virtus". Tácito, por su pensamiento y biografía, concuerda en gran medida con estos rasgos.

No se han conservado discursos de Tácito, por lo que es imposible conocer sus cualidades en el ámbito de la retórica. Existen algunas referencias indirectas. A propósito del discurso fúnebre en honor de Verginio Rufo que se ha citado más arriba, Plinio el Joven ("Cartas", 2.1.6) afirmaba que el hecho de que Tácito hubiera hecho muy elocuentemente su alabanza colmaba la fortuna del difunto. Por otra parte, en tiempos de Trajano se le encomendó junto a Plinio el Joven la acusación por concusión contra Mario Prisco, que había sido procónsul de África. En una sesión del Senado que presidía Trajano, en el desempeño de su tercer consulado, pronunció un discurso no solo elocuente sino además solemne (Plinio, "Cartas", 2.11.17).

Las "Historiæ" ("Historias") narran el periodo que va desde el inicio del segundo consulado de Galba (69) hasta la muerte de Domiciano (96). El término "historiæ" designa la obra historiográfica que relata acontecimientos de una época más o menos dilatada que acaba en los tiempos en que vive el propio autor. Desde los reinados justos y florecientes de Nerva y Trajano, tiempos «en que se permite pensar lo que quieras y decir lo que pienses» ("Historiæ", 1.1), se anima Tácito a pasar revista a una época ominosa llena de infamia. Sabemos que Tácito trabajaba en ellas durante la primera década del siglo II.

Probablemente constaban de catorce libros. Se han conservado los cuatro primeros y aproximadamente la mitad del quinto. Tienen su origen en el segundo consulado de Galba (1 de enero de 69), durante cuyo año el imperio pasa por las manos de tres emperadores, Galba, Otón y Vitelio, hasta que la victoria militar de Vespasiano estabiliza la situación con la inauguración de la dinastía Flavia. Lo conservado finaliza con las campañas de Tito contra Jerusalén.

Estos libros primeros parecen contener la base de pensamiento de toda la obra. Fija su atención en el intento de renovación de la libertad tras la muerte de Nerón, pero no se deja arrastrar por el optimismo al juzgar la actitud de las legiones. No cabe pensar que tomaran partido por convertir a sus generales en emperadores por limpio y desinteresado amor a la libertad, sino por afanes más materiales y bastardos. Presenta la influencia política de la corte de Nerón en los hechos que siguieron a su muerte y el empeño de ciertos personajes para no perder situaciones privilegiadas. Destaca la ceguera y crueldad de la lucha civil en este año, hasta el punto de que se violó la santidad del Capitolio que acabó destruido a manos de ciudadanos.

Vespasiano puso orden en ese fatídico año de los cuatro emperadores. Tácito revela cómo, tras la propaganda flavia, que justificaba su asalto al poder bajo el título de amor a la patria, se oculta en realidad una enorme ansia de poder. El autor es muy consciente de que el centro de gravedad del poder romano se ha desplazado ya fuera de la urbe y que «podía hacerse un príncipe en cualquier lugar distinto de Roma» ("Historias", 1.4.2). Todo ello gracias a que las legiones eran más propicias a servir a sus jefes, si ellos les dan posibilidad de obtener beneficios, que a asumir desinteresadamente las tarea de la defensa del estado. Por otra parte, en las provincias despierta un sentimiento el poder y ciertas ansias de libertad. Tácito trata de desenmascarar a las personalidades conductoras de la política y sus móviles para encontrar las causas reales de los acontecimientos.

Los "Anales" tienen como título completo "Ab Excessu divi Augusti Historiarum Libri" ("Libros de historias desde la muerte del divino Augusto"). San Jerónimo escribe de Tácito que «refirió la vida de los césares en treinta libros desde Augusto a Domiciano». De ello se desprende que las dos obras fundamentales, "Annales" e "Historiæ", formaron una secuencia sin solución de continuidad. Si las "Historiæ" cubrían desde Galba a Domiciano, los 16 libros de los "Annales" recogen la historia inmediatamente anterior, desde la muerte de Augusto a la de Nerón. Pero no ha de olvidarse que se trata de dos obras distintas en su planificación y desarrollo. En "Annales" 16 libros cubren 54 años, mientras que los 14 de "Historiæ" habían servido para historiar solo 27. Es evidente, pues, que la narración es mucho más detallada en las "Historiæ", quizá por la mayor proximidad de los hechos que en ellas se tratan. Es significativo que en ellas los cuatro primeros libros se dediquen a un solo año, el 68, aunque es muy cierto que la densidad de acontecimientos vivida en él exigía el uso de una escala mucho mayor que la que se precisaría en otros momentos.

Como siempre, los poquísimos datos de que disponemos son muy imprecisos. Hay un pasaje en la propia obra que da una pista. En 2.61 se hace mención de «...el imperio romano, que ahora se extiende hasta el Mar Rojo», donde con este nombre hay que entender que se refiere al Golfo Pérsico. De este dato podría inferirse que los "Anales" se comenzaron a escribir inmediatamente después de la conquista de Mesopotamia el año 114. La obra se acabaría ya en tiempos de Adriano en fecha próxima a la muerte del escritor.

De los "Anales" se conservan los cuatro primeros libros, el principio del quinto, el sexto, con excepción de su comienzo, y luego los libros XI a XVI con lagunas a principio y fin. Los seis primeros están dedicados al reinado de Tiberio. En la segunda parte conservada se incluyen los reinados de Claudio desde el año 47 y de Nerón hasta el 66.

Como género historiográfico, los Anales se caracterizaban por referirse a hechos alejados del tiempo vivido por su autor. Los hechos se disponían anualmente, de ahí su nombre. Aunque los "Anales" de Tácito se organicen de esta manera, trascienden el género analístico, pues se plantean miras muchos más amplias, relacionadas con las causas y efectos de los acontecimientos y la influencia en ellos de los rasgos de carácter y las pasiones de sus protagonistas. En este sentido, tienen mucho de biografía, ya que el retrato psicológico ocupa un espacio importante en la obra. La primera parte contiene un soberbio —y tendencioso— retrato de Tiberio. En la parte final los personajes de Nerón y Agripina compiten por el poder y crean una situación en la que ya no caben hombres como Lucio Anneo Séneca, quien con sus doctrinas estoicas tanto había contribuido a atemperar las conductas del emperador.

El "Dialogus de oratoribus" ("Diálogo sobre los oradores"), a pesar del pronunciamiento en contra de algunos estudiosos, se acepta generalmente como obra de Tácito. Es ciceroniano en su concepción y estilo, que se adapta aquí al género y es muy diferente del que el autor emplea en las obras históricas. El asunto tratado en él es la decadencia de la oratoria, que ya se había planteado también Quintiliano en un escrito perdido titulado "De causis corruptæ eloquentiæ" ("Sobre las causas de la corrupción de la oratoria").

Al comienzo de la obra, en casa de Curiacio Materno, poeta, aparecen reunidos con él otros dos personajes: el orador Marco Apro, y Vipstano Mesala, experto en retórica. La acción se sitúa claramente (capítulo 17) en el año 75. Esta fecha es el término "post quem" para la datación de la obra. Hay quienes tienden a considerar a partir de este dato que el "Diálogo..." es obra de juventud pocos años posterior. Sin embargo, por sus relaciones estilísticas y de contenido con las "Institutiones oratoriæ" de Quintiliano y con el "Panegírico de Trajano", no faltan quienes opten por una datación más tardía en los primeros años del siglo II.

Materno discute con Apro sobre la primacía de la poesía sobre la oratoria. Luego la discusión se centra exclusivamente sobre la oratoria. Apro defiende la modernidad y asegura que los oradores de su tiempo no tienen que hacer concesiones al antiguo estilo de la oratoria republicana, pues los tiempos han cambiado. Mesala, en cambio, cree en el valor imperecedero de Cicerón y sus contemporáneos. Según él, en el presente la oratoria está en decadencia a causa del abandono del estudio de los viejos oradores en la educación de los jóvenes.

El diálogo acaba con una intervención de Materno, el poeta, quien zanja la cuestión con un acertado criterio histórico: es la diferencia de régimen político la que determina la decadencia de la oratoria. En la República, una época más agitada, era precisa la elocuencia para hacer carrera política y conseguir apoyos en las actividades públicas. Desde que Roma vive en una larga paz y estabilidad gracias al gobierno de los emperadores, no hacen falta buenos oradores. No se puede asegurar que este fuera el punto de vista del propio Tácito, pero, si así fuera, estaría expresado a la vez con una buena dosis de ironía y de prudencia para no irritar al emperador. Lo que se dice entre líneas es que sin un régimen político libre la oratoria pierde su función.

"De vita Iulii Agricolæ" ("Sobre la vida de Julio Agrícola"), conocida también con el título abreviado de "Agrícola", es su primera obra con contenido histórico. Tácito asocia en ella la biografía y la monografía histórica. La parte biográfica en sentido estricto ocupa los primeros capítulos solamente. Dos tercios de la obra están dedicados a las campañas militares y el gobierno de Agrícola en Britania, probablemente lo más importante de las realizaciones del protagonista. Dedica también alguna atención a la etnografía y geografía del país.

La obra fue redactada tras la muerte de Agrícola a los 53 años de edad. Por ello sigue en gran medida la tradición del elogio fúnebre ("laudatio funebris") tradicional que pronunciaba un familiar en el entierro de los personajes destacados según la tradición romana. Pone su énfasis en las conductas y actuaciones personales de Agrícola que encajan en el marco de la vieja "virtus" aristocrática.

Tácito no se limita a tratar de la vida, cualidades y hazañas de su suegro. Siempre está presente su propio pensamiento, por lo que nos aporta un reflejo de sí mismo. También dedica su atención a lo que supuso el terrible periodo de gobierno de Domiciano, cuyas ignominias destaca. El final de la obra (cap. 43), en el que Tácito, aunque no lo suscriba, se hace eco del rumor según el cual la causa de la muerte de Agrícola había sido un envenenamiento que podía ser atribuido a Domiciano, sirve para completar la imagen perversa del emperador.

"De origine et situ Germanorum" ("Sobre el origen y territorio de los germanos"), conocido también como "Germania", describe a los germanos y su país. La monografía tuvo que escribirse muy poco después del primer año del reinado de Trajano (98), que fue también el de su segundo consulado, pues Tácito utiliza esta fecha como referencia para calcular cuánto tiempo había transcurrido desde los primeros ataques de los cimbros.

La obra es en general muy objetiva. De sus fuentes literarias Tácito solo menciona a César, pero hay que añadir a Plinio el Viejo y a otros historiadores y geógrafos. Además de la información literaria, Tácito, de quien no consta que tuviera conocimiento directo de los pueblos que habitaban Germania, debió de recopilar las narraciones orales de soldados, mercaderes y viajeros que regresaban del otro lado del Rin. Una primera parte del librito se dedica al estudio global de los germanos: geografía física, instituciones, vida privada y cotidiana, aspectos militares, etc. Luego, de forma más detallada, se describen las peculiaridades de cada etnia por separado. Pero no todo es objetividad en la obra.

Tácito no renuncia a reflejar su visión personal de los germanos y sus relaciones con Roma. Su intención es mostrar cómo entre aquellos se seguían cultivando virtudes que en otro tiempo imperaron en Roma. Creía reconocer en ellos los viejos valores de austeridad, dignidad y valor militar que en otro tiempo poseyeron los romanos, pero que habían venido a menos en tiempos posteriores. Tácito ve con simpatía ciertas características de estos pueblos: su primitivismo, proximidad a la naturaleza, pureza y rusticidad. La comparación con la Roma del momento está siempre presente de forma explícita o implícita. Y la vieja Roma no sale bien parada por su espíritu decadente. Sin embargo, no hay que pensar que el autor profesa una admiración acrítica por los germanos: es consciente de sus defectos principales, como eran la afición a la bebida y el juego, la tendencia a la inactividad en tiempos de paz y la tremenda indisciplina militar.

Además veía cómo los germanos constituían un peligro real para Roma, cuyo deterioro moral la incapacitaba para una defensa eficaz. Sus virtudes guerreras los hacían superiores a los ejércitos romanos, preocupados en muchas ocasiones por intereses que nada tenían que ver con la defensa del imperio. Así, en el capítulo 37, donde se ocupa de los cimbrios, revisa todos los contratiempos que Roma había sufrido por su causa desde los primeros ataques del año 113 a. C. No duda en expresar su admiración por ellos cuando los califica de «pueblo pequeño, pero enorme por su gloria»: el pueblo varias veces derrotado, pero nunca sometido.

Tácito es riguroso en el empleo de la documentación. Recoge la información que le proporcionan los historiadores anteriores (Aufidio Baso, Cluvio Rufo, Plinio el Viejo, Fabio Rústico y otros), memorias de personajes (las de Agripina, por ejemplo) y testimonios orales; recurrió también a los "Acta diuturna populi Romani" ("Crónicas del pueblo romano"), que constituían una especie de diario oficial de Roma, y a los archivos del senado. Aunque trate de usar sus fuentes con imparcialidad, su fuerte personalidad acaba imponiéndose, con lo que triunfa la subjetividad. Los componentes filosóficos (sobre todo estoicos) e ideológicos acaban siempre por teñir cuanto narra. Pero al principio de sus "Historias" declara cuál es su guía:

Casi toda su obra está dominada por el empeño de destacar las infamias cometidas por la mayoría de los emperadores desde la muerte de Augusto a la de Domiciano. Este recurso le sirve para resaltar más los méritos de Nerva y Trajano. Tácito no es un buen conocedor de la milicia, de la administración ni de la economía. En su carrera política, de hecho no le fueron nunca encomendadas actividades bélicas. Por ello su estudio es desigual: se interesa sobre todo por los aspectos psicológicos y dramáticos, y se ocupa de la corte imperial, que ofrece una rica materia para el análisis moral.

Su filosofía política presenta vacilaciones. No se decide a escoger entre la antigua noción romana del estado senatorial oligárquico, dirigido por «los mejores», y la idea helenística de un estado regido por un monarca. Con todo, sus tendencias estoicas parecen llevarlo a desconfiar de la solidez moral de un modelo político basado en las decisiones (y, por tanto, la arbitrariedad) de un solo hombre. En numerosas ocasiones parece añorar la vieja república y su concepto de libertad, aunque sus pronunciamientos en este sentido estén camuflados lo necesario para no resultar molestos al régimen imperial.

Es característico de Tácito el extremo cuidado del estilo. Su lenguaje es acerado, de construcción breve, muy sintético, dado a la braquilogía. Huye de los periodos cuidadosamente organizados y busca la asimetría. Todo ello hace muy densa su expresión, de un barroquismo conceptista en el que la agudeza de la idea prima sobre cualquier tendencia ornamental. No duda en emplear neologismos. Su principal modelo estilístico es Salustio, aunque, en contra de lo que hacía aquel, esquiva cualquier rasgo de arcaísmo: muy al contrario, su intención artística se canaliza en una consciente busca de la modernidad. Los rasgos del lenguaje de Tácito mencionados lo llevan en ocasiones a un tipo de narración de pincelada grande y suelta, donde se estimula la imaginación del lector para que supla lo no explicitado.

Tácito considera que los depositarios del poder son los protagonistas de la historia. En consecuencia da gran importancia al retrato, en el que destaca los componentes psicológicos y morales. Es poderosísimo, por ejemplo, el retrato de Tiberio contenido en la primera parte de los "Anales". Tácito ha sido capaz de imponer, a veces por encima de los propios hechos, su visión del personaje.

Siempre trata de crear un clima dramático, para lo que usa las acciones humanas individuales y los hechos producto del azar. Aunque trate de documentarse y en general respete los hechos, su interés siempre tiende a la creación de imágenes poderosas, en las que impone sus propias convicciones. No duda para lograr el efecto deseado en reproducir rumores que él mismo asegura que no tiene comprobados. Aunque establezca una duda sobre ciertos datos, el simple hecho de mencionarlos está influyendo en el lector, cuya posición se ahorma según las intenciones del autor. La imagen, pues, se instala por encima de los argumentos racionales y permanece. Por ejemplo, la que transmitió del incendio de Roma, la conducta de Nerón y la ulterior persecución de cristianos ("Anales", 15.44) ha creado la iconografía más arraigada para estos hechos: la que se ha instalado en la literatura y en el cine. Tácito no se entretiene en probar la perversidad de Nerón: bastan unas pocas pinceladas tremendistas, solamente media página, para cubrirlo de oprobio.

Tácito ha sido descrito como el «mejor historiador que haya producido el mundo romano». Su trabajo ha sido valorado por sus enseñanzas morales, su narrativa dramática y su estilo. Además del área de la historia, la influencia de Tácito es más prominente en el área de la teoría política. Las lecciones políticas de sus obras se pueden clasificar de dos maneras: los "Tacitistas rojos" utilizan su obra para apoyar los ideales republicanos y los "Tacitistas negros" lo leen como una lección en realpolitik maquiavélica.

Aunque su trabajo es nuestra fuente más fiable sobre la historia de su era, la precisión de los hechos que describe es cuestionada ocasionalmente. Los "Anales" se basan parcialmente en fuentes secundarias, y hay algunos errores obvios, por ejemplo la confusión de las dos hijas de Marco Antonio y Octavia Menor, llamadas ambas Antonia. Sin embargo, las "Historias" son escritas sobre la base de documentos primarios y conocimientos íntimos del período Flavio, y por lo tanto se cree que son más precisas.

Fuera de la traducción inédita y parcial de las "Historias" por Antonio de Toledo (1590), entre los traductores antiguos de Tácito al español el primero fue el caballero flamenco, de origen portugués, Emanuel Sueyro ("Las obras de Caio Cornelio Tácito", Amberes: por los herederos de Pedro Bellero, 1613, reimpresa en Madrid: Viuda de Alonso Martín, 1614 y 1619). Luego Baltasar Álamos de Barrientos ("Tácito Español illustrado con aforismos", 1614) tradujo todas sus obras, acompañándolas de comentarios a los pasajes difíciles; su versión estaba ya acabada, aunque no impresa, en 1594; posterior fue la muy alabada y difundida, reimpresa incluso en la actualidad, de Carlos Coloma (1629). Otras muchas fueron menos extensas u ocasionales, por ejemplo, la de "Los cinco libros primeros de los Annales de Cornelio Tacito: que comienzan desde el fin del Imperio de Augusto hasta la muerte de Tiberio..." (Madrid, 1615) de Antonio de Herrera y Tordesillas o, de Juan Alfonso de Lancina, "Comentarios políticos a los Anales de Tacito" (Madrid, 1687). Diego Clemencín publicó "Ensayo de traducciones..." (Madrid: Benito Cano, 1798) que incluye la "Germania", la "Vida de Agrícola" y algunos fragmentos de Tácito con un discurso preliminar, en todo lo cual le ayudó José Mor de Fuentes (aunque este pretendió tras la muerte de Clemencín que la mayor parte de las traducciones era suya, sin que a fecha actual se pueda dilucidar el problema). Sobre la calidad de estas versiones escribió Marcelino Menéndez Pelayo en el prólogo de su edición de los "Anales" (1890) para la "Biblioteca Clásica" (pp. 96-97):

En 1957 la Editorial Aguilar imprimió en español las "Obras completas" de Tácito (dirigida por V. Blanco García). En 1979 y 1980 la Editorial Gredos publicó la traducción de los "Anales" (libros I-XVI) realizada por José Luis Moralejo Álvarez (reeditada en 2001), autor asimismo de una traducción de las "Historias" publicada por Akal en 1990.








</doc>
<doc id="2392" url="https://es.wikipedia.org/wiki?curid=2392" title="Parque nacional del Manu">
Parque nacional del Manu

El parque nacional del Manu es un espacio natural protegido localizado en el sudeste del Perú, ubicado parcialmente en los departamentos de Madre de Dios y Cusco, en las provincias de Manu y Paucartambo. Con un área de 1 909 806 hectáreas o hectómetros cuadrados; se divide en tres grandes zonas: el Parque Nacional, con 1 532 806 ha, la Zona Reservada, con 257 000 ha y la Zona de Transición o Cultural, con 120 000 ha.

Se extiende desde los 300 msnm, en la confluencia del río Manu con el río Madre de Dios, hasta más de 4000 msnm en la cumbre de la montaña Apu Kañajhuay. Algunos investigadores creen que en las zonas vírgenes de esta reserva se halla el Paititi o "ciudad perdida de los Incas".

El parque nacional fue creado el 29 de mayo de 1973. En 1977, la Unesco reconoce al Parque como Reserva de Biosfera y en 1987 lo declara Patrimonio Natural de la Humanidad.

En la Reserva de Biosfera del Manu existen testimonios de antiguas culturas como los petroglifos de Pusharo, un conjunto de grabados de los que aún no se ha podido explicar su origen y significado, que fueron reportados por primera vez por el padre Vicente de Cenytagoya en 1921 y están ubicados en la margen derecha el río Shinkibenia, afluente del río Palotoa; otros petroglifos se encuentran en el río Q'eros, sobre el gran peñasco "Xinkiori", legendario para los huachipaeris. De igual manera se tiene conocimiento de un sitio arqueológico en la zona de Mameria, ubicado en las cabeceras del río Piñi Piñi y el Alto Tono.

La zona del Manu tiene una historia marcada por la llegada de gente foránea, desde los tiempos del imperio de los incas en que el Inca Pachacútec y Túpac Yupanqui anexaron esta zona a su imperio, hasta la llegada de los españoles que poco tiempo después de la invasión del Cuzco fundaron el pueblo de Paucartambo, lugar donde establecieron haciendas y encomiendas y donde además al rey Carlos III de España ordenó la construcción de un puente para facilitar el comercio de los productos de la zona; fue así como este valle empezó a abastecer al Cuzco de productos como coca, azúcar, algodón, ají, madera y otros.

En marzo de 1567, el español Juan Álvarez Maldonado a cargo de la provincia de Mojos emprendió un viaje de 37 días para hacer la primera expedición desde Paucartambo hasta la actual localidad de Pillcopata. En mayo del mismo año, Manuel de Escobar montó una segunda expedición que siguió el curso del río Madre de Dios hasta el río Manu.

En 1861, el coronel Faustino Maldonado emprendió una nueva expedición desde Paucartambo hacia el río Madre de Dios. Fue en honor a él que 30 años después, el barón del caucho, Carlos Fitzcarrald, bautizaría la desembocadura del río Tambopata como Puerto Maldonado, actual capital del departamento de Madre de Dios.

En la selva baja, las poblaciones indígenas se vieron afectadas por las actividades extractivas a fines del siglo XIX, la explotación del caucho marcó el inicio de intrépidas empresas como la de Fitzcarrald, uno de los caucheros más famosos de su época. No obstante, la zona del Manu fue parcialmente explotada. Las actividades del caucho cesaron en los años 20 cuando el recurso, incapaz de reponerse a la explotación intensiva y a la competencia de los prósperos y menos costosos cultivos en el continente asiático, empezó a disminuir.

Entre los años 50 y 60, la construcción del tramo final de la carretera (que hoy se conoce como Vía Interoceánica Sur) dio inicio la extracción maderera de cedro y de caoba, el trabajo de haciendas y posteriormente la extracción de pieles finas (otorongo, tigrillo y lobos de río). Más recientes son las actividades de exploración petrolera. En tanto, en la zona andina, las actividades agrícolas se vieron afectadas por la reforma agraria iniciada en 1969.

A partir del siglo XX, la presencia religiosa se hizo más significativa. En 1902 los padres dominicos fundaron su primera misión en Asunción. En 1908 instalaron el segundo puesto misional, San Luis del Manu, en la desembocadura del río Manu; luego de abandonar ésta se establecieron en la misión del río Palotoa (en la zona del Pantiacolla), la que luego de haber sido arrasada por una inundación se ubicó definitivamente en Shintuya en 1958.

En 1963 se creó el Bosque Nacional del Manu. A sugerencia de Flavio Bazan y de Paul Pierret, experto de la FAO, en 1965 fue propuesto crear allí un parque nacional. La importancia del lugar fue confirmada por el informe de 1966 del asesor británico Ian Grimwood. En 1968 se declaró el área como Reserva Nacional y posteriormente, el parque nacional del Manu fue establecido el 29 de mayo de 1973, mediante el "Decreto Supremo 0644-73-AG", con la finalidad de preservar su patrimonio natural y cultural en beneficio de las generaciones presentes y futuras; esa misma finalidad determinó para el reconocimiento por la Unesco de la reserva de la biosfera del Manu que hoy abarca un territorio de 1 881 200 ha en las provincias de Paucartambo en el Cuzco y Manu en Madre de Dios. Sus límites se trazaron aplicando el principio de los límites naturales y el dominio de cuencas. Sin embargo, el límite del Parque en el mismo río Manu tuvo que detenerse en la confluencia con el río Panagua debido a que existía una exploración petrolera.

Está destinada a la protección y sólo se permiten actividades de investigación antropológica y biológica, limitadas a la observación de la vida y los procesos ecológicos en su forma natural; en el parque se encuentra la Estación Biológica de Cocha Cashu, uno de los más importantes centros de investigación de los bosques tropicales. El lugar es intangible y para visitarlo hay que tener una autorización especial.

Se ubica en la parte baja del río Manu. En esta área están permitidas las actividades turísticas (organizadas por las agencias autorizadas) y la investigación con mínima manipulación. Es posible observar una gran riqueza paisajística y natural por la gran cantidad de flora y fauna visibles desde los ríos y las cochas (meandros del río que se cierran y quedan aislados del cauce principal, formando lagunas que mantienen una gran riqueza de fauna). Las visitas están controladas. Se extiende desde la quebrada del río Panagua hasta Boca Manu.

Está conformada por la ribera del río Madre de Dios y los territorios altoandinos que bordean la parte sur de la reserva, entre la línea divisoria del Parque Nacional y el río Mapacho. En esta zona predominan las poblaciones de colonos que desarrollan actividades agrícolas, pecuarias y forestales y que cuentan con servicios básicos de salud, educación y desarrollo, aunque de manera incipiente. 

Alrededor de la reserva de biósfera del Manu existen otras áreas como la Reserva Territorial del Estado a Favor de los Grupos Étnicos Kugapakori y Nahua, el Santuario Megantoni y la Reserva Comunal Amarakaeri; estos territorios y los de la cuenca del río Mapacho, además de la ampliación de la actual zona cultural (que luego se llamaría Zona de Uso Múltiple Andina y Amazónica) están considerados dentro de los estudios y propuestas para integrarlos a la reserva de biósfera del Manu.

En su vasto territorio, desde las altas punas a 3500 m, cubiertas de pasto, donde el aire seco y las temperaturas varían según se esté al sol o bajo la sombra, y dependiendo de los meses alcanzando -9 °C hasta 22 °C entre los meses de septiembre y abril y la época de lluvias nieve o granizo entre octubre y abril; se desciende por una extensa franja de transición denominada "matorral boscoso" que desciende hasta aproximadamente los 2600 msnm desde donde la vegetación va formando un bosque, zona en la que hasta la fecha se han identificado alrededor de 450 especies de plantas y la gorda donde la precipitación pluvial puede alcanzar entre 500 y 1000 mm.

Los bosques "Montano bajo" se desarrollan entre los 2200 y 1650 m; hallándose árboles de hasta 25 m de altura cubiertos de familias orquídeas y helechos crecen sobre las paredes de los profundos precipicios. Entre ellos se pueden encontrar aráceas de amplias hojas así como helechos arbóreos que caracterizan el lugar, en estos bosques cubiertos de neblina, existe un clima frío y un paisaje entre sombras y penumbras constantemente húmedo.

El bosque "Montano lluvioso", también conocido como "bosque de nubes", entre los 600 msnm y los 1650 m en la niebla es constante cubrir espléndido paisaje lleno de vegetación, aquí existen árboles hasta de 30 m de altura que están invadidos de busco orquídeas, promedios y helechos, formando un denso bosque interrumpido solamente por pequeños riachos y cascadas que nacen y se pierde entre la vegetación. En la actualidad se estima que esta zona contiene no menos de 200 diferentes especies de árboles con una densidad que puede superar las 700 por hectárea. Aquí la temperatura varía entre 20 y 25 °C pudiendo bajar por las noches a 16 o 18 °C. 

El "bosque húmedo tropical" o también llamado "selva baja" se extiende sobre el gran llano amazónico, desde los 300 hasta los 600 m. Éste es sin duda el paisaje más representativo y extenso de la Reserva. Aquí, en la zona del serpenteante río Manu, los árboles presentan una altura gigantesca; sobre las altas copas de éstos, algunas especies de árboles pueden llegar a emerger, como el shihuahuaco ("Dipteryx micrantha") y la lupuna ("Ceiba pentandra").
El parque está ubicado en las provincias de Manu y Paucartambo (Departamentos de Madre de Dios y Cusco, respectivamente), abarcando las laderas orientales de los Andes en la Amazonía Peruana. Los límites al norte son la cuenca que separa las cuencas de captación de Manu y los ríos de las Piedras (72°01'W, 11°17'S), a la zona donde el sur de la carretera de Paucartambo en el norte-oeste se convierte en Tres Cruces (71°30'W, 13°11'S), al este de la región en la margen izquierda del Alto Madre de Dios del río hasta el río Pilcopata, departamento del Cusco (71°10'W, 12°18'S), y para al oeste de la cuenca que separa las cuencas de captación de los ríos Manu y Camisea, también el límite entre los departamentos de Cusco y Madre de Dios (72°22'W, 11°45'S).

En el parque existen poblaciones humanas de nativos amazónicos pertenecientes a diferentes etnias que la habitan desde tiempos inmemoriales, cuyo número se calcula en unos 1000 indígenas; sin embargo, también existe una población quechua de aproximadamente 200 personas en la zona de Callanga.

La temporada de lluvias o temporada baja va de diciembre a marzo, pero durante todo el año pueden haber lluvias inesperadas; las temperaturas en las zonas bajas varían entre 35 °C durante el día y 20 °C durante la noche.

Es una de las regiones más biodiversas del mundo. En el Manu es posible encontrar toda la variedad de pisos ecológicos que existen en la Amazonia y esto la hace una de las áreas protegidas más apreciadas. En una sola hectárea se han llegado a encontrar hasta 250 especies de árboles. En la Reserva de la Biósfera de Manu se han registrado 223 especies de mamíferos y 1005 de aves. También tiene el récord mundial de diversidad de especies de anfibios (155) y reptiles (132) para un área protegida.



</doc>
<doc id="2394" url="https://es.wikipedia.org/wiki?curid=2394" title="Periodismo">
Periodismo

El periodismo es una actividad profesional que, en términos generales, consiste en la obtención, tratamiento, interpretación, redacción y difusión de informaciones, a través de los medios de comunicación social como la prensa, la radio, la televisión, el Internet, entre otros.

El propósito principal del periodismo es proporcionarle a los ciudadanos, información veraz y oportuna para hacer valer sus derechos ante la sociedad.

Dader (2012), señala que el periodismo además de ser un método para dar a conocer acontecimientos de relevancia para una sociedad, también "es una ciencia que combina la recopilación, verificación, síntesis y clarificación de la información acreditada como relevante y cierta, para servir desinteresadamente a los ciudadanos en su necesidad de un seguimiento preciso de los asuntos de interés público o potencialmente capaces de afectar sus vidas".

Como disciplina, el periodismo se ubica en general dentro de las ciencias de la comunicación, si bien en algunos países se adscribe a la sociología.

Podemos definir el periodismo atendiendo a sus tres grandes acepciones:


La base del periodismo es la noticia, si bien comprende otros géneros, muchos de los cuales se interrelacionan, como la entrevista, el reportaje, la crónica, el documental, el perfil y la opinión.

Álex Grijelmo, Director de la Escuela de Periodismo de El País, un referente obligado del periodismo en español, hace una diferenciación de los géneros periodísticos, en su libro "El estilo del periodista":

Mientras Grijelmo sólo distingue dos grandes géneros periodísticos, a partir de ellos podemos hacer subdivisiones de los mismos, como la siguiente que contiene tres grandes géneros periodísticos:


El periodismo puede ser clasificado según dos grandes criterios: el medio de comunicación que utilice y el tipo de información que analice.

La información puede ser difundida por medios o soportes técnicos, lo que da lugar al periodismo gráfico, la prensa escrita, el periodismo radiofónico, el audiovisual (a través del cine y la televisión) y el periodismo digital o multimedia.




El periodismo es considerado por algunos autores como el "cuarto poder" de las grandes democracias occidentales (los tres primeros son los que establecen las constituciones modernas: poder ejecutivo, legislativo y judicial). Como contraparte, el periodismo, en algunos casos es una profesión con riesgos; muchos periodistas han encontrado la muerte en el ejercicio de su profesión.

El periodismo creó, por sus necesidades de rápida lectura y comprensión y su supuesta neutralidad, un estilo redaccional que ha nutrido a numerosos escritores, los cuales formaron parte de sus planteles y se destacaron en sus columnas. Además ha creado prestigiosos y serios comentaristas de la vida social y política, vistió sus páginas con buenos humoristas y dibujantes; ha desarrollado desde el proyecto costumbrista hasta la investigación documentada.

Es un género periodístico que consiste en relatar vivencias, características y datos curiosos de un personaje en particular ya sea de personas, seres animados y también seres desanimados ayudándose de la atmósfera. Se aproxima muchas veces a la entrevista, la biografía y la crónica de personaje. El padre del perfil es Jon Lee Anderson, el cual ha perfilado a escritores, artistas y políticos entre otros.

El perfil periodístico necesita de una investigación profunda, por eso está dentro de los géneros del nuevo periodismo o periodismo narrativo. El propósito del perfil es reflejar la realidad de una forma diferente en donde exista narración, diálogo y descripción. Se evita llegar a una biografía o a que se vuelva una entrevista (pregunta-respuesta). A diferencia de estos, el perfil no solo recoge la voz del personaje sino que también se reúnen las voces de amigos, familiares hasta de enemigos. En su escritura se aplican los cinco sentidos, profundizándose en los detalles físicos y psicológicos desde la observación, la descripción de su vestuario, lo que proyecta (tranquilidad, nerviosismo, felicidad, armonía, entre otros), el aroma del sitio donde se encuentra, sabores y sensaciones. Además, se espera que a través del diálogo el personaje revele sus miedos, sus gustos, sus pasiones, sus tristezas, sus aventuras y sobre todo poder encontrar uno o más datos importantes dignos de publicar.

El perfil cumple con la estructura de cualquier relato: inicio, desarrollo y final. Los temas pueden ser históricos, sociales, políticos y culturales. Para escribir un perfil, citando a Jon Lee Anderson, se debe: ""descubrir la capacidad de sentir lo que está alrededor"".

El trabajo realizado por Berganza, Lavín y Piñeiro-Naval determinó una tipología de seis roles profesionales del periodismo a través de una encuesta a 390 periodistas españoles. Los roles identificados fueron:

El mayor porcentaje de periodistas se identifican con el rol "Altavoz de la ciudadanía", y el menor porcentaje de ellos con el rol "Favorecedor del "statu quo"".

En países de régimen democrático, el trabajo periodístico suele estar protegido por la ley o por la constitución. Esto incluye, muchas veces, el derecho del periodista a preservar en secreto la identidad de sus fuentes, incluso cuando sea interpelado judicialmente.

El artículo 19 de la Declaración Universal de los Derechos Humanos establece normas para la libertad de expresión y de prensa. Además de las normas jurídicas que regulan la profesión de los periodistas, estos mantienen un compromiso ético con la sociedad que se concreta en la llamada deontología profesional periodística. Se trata de una serie de normas recogidas en códigos deontológicos que cada empresa o asociación elabora según sus propios criterios.

La Comisión Investigadora de Atentados a Periodistas de la Felap posee como objetivo investigar, monitorear y denunciar los crímenes sobre periodistas en América Latina, en esta labor el secretario ejecutivo de esta Comisión es el periodista chileno Ernesto Carmona. En su informe del año 2012, entregó los antecedentes de 45 periodistas asesinados.

Además, según la organización Reporteros Sin Fronteras ("Reporters Sans Frontières"), en 2006 al menos a 81 periodistas fallecieron en el ejercicio de su trabajo o por expresar sus opiniones en veintiún países. Hay que remontarse a 1994 para encontrar una cifra más alta. Aquel año encontraron la muerte 103 periodistas, de los que casi la mitad murieron en el genocidio de Ruanda, cerca de una veintena en Argelia, víctimas de la guerra civil, y una decena en la antigua Yugoslavia. También destacan que murieron 32 colaboradores, al menos 871 periodistas fueron detenidos, 1472 agredidos o amenazados, 56 secuestrados y 912 medios de comunicación fueron censurados.

En España la formación reglada en periodismo se ofrece a través de titulaciones universitarias de grado y máster. El título de "Grado en Periodismo" tiene como objetivo impartir la formación básica y generalista de la labor periodística, mientras que las distintas titulaciones de másteres universitarios se centran en un tipo de periodismo en concreto así como en su investigación.

Las titulaciones universitarias en periodismo no son habilitantes, puesto que en España el periodismo sigue siendo todavía una profesión no regulada. No obstante, ostentar un título oficial de periodismo sí es requisito necesario para formar parte de cualquiera de las asociaciones profesionales de periodistas.

En España el código deontológico elaborado por la Federación de Asociaciones de Periodistas de España (FAPE) aplican para los profesionales integrados en este colectivo.

Un acontecimiento que marcó el inicio del periodismo en Venezuela, fue la aparición del primer periódico impreso "La Gazeta de Caracas" en el año 1808. En esa época, los periódicos utilizaban un estilo literario y el periodista ó redactor describía los acontecimientos, empleando técnicas del periodismo de opinión. Cabe destacar, que este medio de comunicación estaba destinado a la clase alta del país, ya que existía un alto nivel de analfabetismo en el grueso de la población en general.

Dos años después (1810), Francisco de Miranda se convertiría en el primer venezolano en ejercer funciones de periodista al publicar en Londres, en compañía del ecuatoriano José de Antepara, la hoja quincenal "El Colombiano". Este medio, circuló en América, propagando ideas de emancipación, influyendo en personajes importantes como Simón Bolívar, José de San Martín, entre otros.

Posteriormente, la prensa escrita daría un paso importante con la fundación de "El Correo del Orinoco" en 1818, de la mano de Simón Bolívar. Este medio, le serviría a Bolívar para difundir sus ideas libertarias de América.

El 20 de agosto de 1840, Antonio Leocadio Guzmán, funda el primer periódico popular y de opinión "El Venezolano". De esta manera, se marca el inicio de la prensa popular en Venezuela pues hasta entonces la prensa estaba destinada a la clase alta del país, alejada de un pueblo analfabeto en su gran mayoría. 

La prensa sufriría un duro golpe en los periodos de gobierno de Juan Vicente Gómez (1908-1935) y Marcos Pérez Jiménez (1953-1958).

No obstante, la peor crisis de la prensa y el periodismo ocurriría a partir de 1999 con la llegada de Hugo Chávez al poder; con el cierre de medios de comunicación, detención de periodistas, déficit de divisas para importar papel y otros insumos, además de los impedimentos para renovar concesiones.

El ejercicio profesional se rige por el Código de Ética del periodista en Venezuela. Las principales universidades donde se forman los periodistas en Venezuela son: Universidad Central de Venezuela (UCV), Universidad Católica Andrés Bello (UCAB), Universidad del Zulia (LUZ) y Universidad de Los Andes (ULA).

Hay dos antecedentes: previos al nacimiento del país, el "Telégrafo Mercantil, Rural, Político, Económico e Historiográfico del Río de la Plata", fundado en 1801, a instancias de Manuel Belgrano. Posterior a la liberación del 25 de mayo de 1810, la Gazeta de Buenos Ayres (sic), con el objetivo de publicitar los actos de gobierno de la Primera Junta, redactado por Mariano Moreno con Manuel Alberti y colaboraciones de Belgrano y Juan José Castelli.






</doc>
<doc id="2396" url="https://es.wikipedia.org/wiki?curid=2396" title="Periódico (desambiguación)">
Periódico (desambiguación)

Periódico es algo que ocurre de forma periódica, con periodicidad. Es decir, siguiendo un período que se repite a intervalos determinados. El término puede referirse a:





</doc>
<doc id="2398" url="https://es.wikipedia.org/wiki?curid=2398" title="Prokaryota">
Prokaryota

En biología, procarionte o procariota (taxón Prokaryota) es el superreino o dominio que incluye los microorganismos constituidos por células procariotas, es decir, células que presentan un ADN libre en el citoplasma, ya que no hay núcleo celular. El término deriva del griego: "πρό-"("pro-"), "antes de" + "κάρυον" ("carion"), "nuez" o "almendra", como referencia a la carencia del núcleo celular, Los procariontes u organismos procariotas han recibido diversas denominaciones tales como "Bacteria", "Monera" y "Schizophyta", dependiendo de los autores y los sistemas de clasificación. Otros términos usados fueron "Mychota", "Protophyta" y "Procaryotae". Está constituido a su vez por dos dominios bien diferenciados: "Archaea" y "Bacteria".

Los procariontes son unicelulares, salvo algunos casos como las mixobacterias, algunas de las cuales tienen etapas multicelulares en su ciclo de vida. En otros casos crean grandes colonias, como en las cianobacterias. Los procariontes se caracterizan por no presentar núcleo celular, mitocondrias ni otros orgánulos. La compartimentación, también es frecuente en el mundo procariota en la forma de compartimentos unos delimitados por proteínas y otros delimitados por lípidos. 

Los primeros microorganismos procariotas fueron observados por Anton van Leeuwenhoek en 1683 usando un microscopio de lente simple diseñado por él mismo y conjuntamente con los protozoos los denominó "animáculos". La invención del microscopio dejó atrás la "fase de la especulación" y se abre paso a la "era de la observación", la cual desembocó a mediados del en el "periodo de oro" de la microbiología.

Entre algunos de los momentos claves históricos, se puede mencionar que en 1859 Louis Pasteur, considerado el padre de la microbiología, define la fermentación bacteriana; en 1876 Robert Koch descubre la del carbunco o ántrax maligno, en 1884 se descubre la tinción de Gram y en 1910 Paul Ehrlich desarrolló el primer antibiótico para combatir al "Treponema" de la sífilis. En 1936 H.A Barker identificó a los metanógenos, en 1967 Thomas D. Brock descubre a los extremófilos y en 1977 un equipo de la Universidad de Illinois descubre la gran divergencia entre arqueas y bacterias gracias al estudio genético del ARN ribosómico (Balch "et al." 1977) lo que constituye parte de los inicios de la filogenia molecular, encontrando entonces las siguientes relaciones:

Ese mismo año (1977), Woese "et al." sienta las bases del sistema de tres dominios, el cual fue contradicho por la hipótesis del eocito de Lake "et al." (1984). En todo caso, ambos postulados forman parte de la moderna filogenia microbiana.

Los procariontes casi siempre son:

Los procariontes presentan enormes diferencias con los eucariontes, como la ausencia de organelos, la presencia de ribosomas más pequeños o diferencias en la reproducción. Pero la diferencia más importante radica en el origen mismo de los eucariontes (eucariogénesis), el cual tendría una historia evolutiva más tardía y compleja como resultado de la asociación simbiótica entre diferentes organismos procariotas. Mitocondrias y cloroplastos sintetizan sus propios ribosomas y éstos son además del mismo tamaño que el de los procariontes. Esto probaría el origen procariota de estos orgánulos por endosimbiosis seriada. Así pues, mientras los procariontes se originaron hace unos 3500 millones de años, los eucariontes aparecen mucho después, hace unos 1.400 millones de años y como descendientes de organismos procariotas. Bajo este punto de vista, podemos considerar a Prokaryota como un grupo parafilético.

Para una comparación con las características eucariotas, véase: Tabla comparativa.

Tienen típicamente entre 1 y 7 μm de longitud y 0,2–2,0 µm de diámetro, aunque pueden llegar a extremos como en las nanoarqueas y bacterias ultrapequeñas, las cuales suelen poseer 0,4 µm (400 nanómetros) de longitud y 0,25 µm de diámetro. Hay espiroquetas que pueden llegar a 500 μm de longitud, pero la bacteria más grande es la proteobacteria "Thiomargarita" con 750 μm.

Por lo general, los procariontes no presentan ningún orgánulo. Sin embargo hay excepciones, pues en ciertos casos hay cuerpos membranosos que demuestran la formación de compartimentos celulares procariotas y que este no es un fenómeno exclusivo de los eucariontes. 

Algunos ejemplos son los magnetosomas de bacterias magnetotácticas como "Magnetospirillum", los carboxisomas de algunas bacterias fijadores de CO, los clorosomas en algunas bacterias fotótrofas, los tilacoides en cianobacterias, y fundamentalmente varios tipos de compartimientos en los planctomicetos tales como el anammoxosoma, riboplasma, parifoplasma y un nucleoide que a veces está envuelto en doble membrana.

El metabolismo procariota tiene una gran diversificación. Mientras los eucariontes tienen solo dos (animal y vegetal), los procariontes han evolucionado en una gran variedad de ambientes por lo que dependen de los siguientes requerimientos:

Fuente de energía:
Fuente de carbono:
Fuente reductora o donadora de hidrógeno y electrones:

La combinación entre los diferentes factores metabólicos del párrafo anterior da lugar a los siguientes tipos de procariotas:







La respiración anaerobia es diversa y arcaica, en lugar del oxígeno usa generalmente como aceptor una sustancia inorgánica como nitrato, azufre, sulfato, tiosulfato, CO, Fe, Mn, seleniato, arseniato, y rara vez una sustancia orgánica como fumarato, DMSO, TMAO o clorobenzoato. Según su respiración los procariontes pueden ser:


Los organismos heterótrofos tienen generalmente metabolismo aerobio (que respiran oxígeno); y como la oxidación de la glucosa y otras sustancias libera mucha más energía que su utilización anaerobia, los seres aerobios pronto se convirtieron en los organismos dominantes en la Tierra por la mayor energía que se obtiene con este tipo de respiración.

A diferencia de los eucariontes, los procariontes tienen gran variabilidad de hábitats y de rangos de temperatura para su desarrollo. Según su temperatura óptima de desarrollo pueden ser:

La adaptación a los diferentes hábitats en la Tierra, ha permitido que los organismos procariotas evolucionen hasta en los ambientes más extremos. Según el ambiente en que se desarrollan se usan los siguientes términos:


Los primeros organismos vivos podrían haber sido procariontes relacionados con el origen de la vida (abiogénesis). El último antepasado común universal (LUCA) sería un organismo celular procariota evolucionado a partir de protobiontes (proto-células). 

Los modelos estadísticos confirman que todos los seres vivos descienden de un solo antepasado universal. Esto está respaldado por la evidencia que da la universalidad del código genético y de la célula como unidad básica biológica. Sin embargo, no hay un acuerdo sobre las características estructurales y/o metabólicas de este antepasado universal, ya que hay diversas hipótesis que sostienen que pudo haber sido un progenote (hipótesis del mundo de ARN ), una bacteria Gram positiva, una Gram negativa fotosintética, o, tal vez lo más probable, un organismo procariota tipo arquea, hipertermófilo y quimiosintético.

La evidencia paleontológica le da a la aparición de los primeros organismos procariotas una antigüedad de por lo menos 3500 millones de años (Ma), en la era Eoarcaica. Huellas fósiles revelan vida temprana en aguas termales terrestres encontradas en Pilbara (Australia), dando también 3500 Ma. El descubrimiento de grafito biogénico en rocas metasedimentarias al sudoeste de Groenlandia, constituye una evidencia de 3.700 Ma de antigüedad; aunque estos hallazgos han sido cuestionados. Microfósiles con forma de microfilamentos de óxido de hierro encontrados al norte de Canadá, serían evidencia de actividad procariota en respiraderos hidrotermales marinos con una antigüedad entre 3.770 y 4.280 Ma, lo que podría indicar que la vida apareció relativamente poco después de la formación de la Tierra.

Las teorías más aceptadas indican que los primeros seres vivos fueron procariotas que habitaron en un océano caliente (teoría del caldo primigenio) o en las fuentes hidrotermales volcánicas en la oscuridad del fondo del océano (teoría del mundo de hierro-azufre), en donde hay un medio caliente, de alta presión, anaerobio, con presencia de CO y compuestos de azufre, un medio adecuado para el metabolismo quimiosintético primigenio.

Evidencias al respecto se encuentran en la filogenia procariota: De acuerdo con la filogenia bacteriana sobre la base de ARNr 16S, 23S, así como a algunos árboles proteicos y enzimáticos, las bacterias más divergentes son termófilas como Thermotogae, Aquificae, Thermodesulfobacteria y Dictyoglomi. En arqueas es más notorio, pues la mayoría de filos tienen miembros termófilos. De acuerdo con la filogenia de los dos filos arqueanos principales, Crenarchaeota y Euryarchaeota, los subgrupos más divergentes son altamente hipertermófilos; en el primero caso son las Pyrodictiaceae, cuya temperatura óptima de crecimiento está por encima de los 100 °C, y en el segundo es "Methanopyrus", un metanógeno capaz de sobrevivir y reproducirse a 122 °C.

Los primeros seres vivos fueron procariontes y su aparición coincide aproximadamente con el inicio del periodo Arcaico. En esta época, el flujo de calor de la Tierra era casi tres veces superior al que es hoy, la actividad volcánica era considerablemente más alta, con numerosos puntos calientes, fosas tectónicas, dorsales oceánicas y lavas eruptivas muy calientes como la de komatita, inusual hoy en día. La luminosidad del Sol, era menor que la actual, pero hubo en esta época el mayor volumen de gases de efecto invernadero que acidificaron los océanos por la disolución de dióxido de carbono. Más del 90% de la superficie terrestre la ocupaban los océanos y sus aguas tenían una temperatura de 70 °C. La Tierra aún era presa del bombardeo intenso tardío de grandes meteoritos hasta hace 3.200 Ma. Todas estas condiciones hacen que solo sobrevivan los extremófilos.

Durante aquellos remotos tiempos, la atmósfera y océanos carecían de oxígeno, por lo que la respiración procariota predominante era anaerobia; y la fotosíntesis debió ser anoxigénica (sin producción de oxígeno) tal como actualmente lo hacen las bacterias verdes y púrpuras. Los estromatolitos más antiguos de comprobado origen microbiológico tienen 2.724 millones de años de antigüedad.

Paulatinamente la Tierra se fue enfriando, y un evento crucial y probablemente el más importante de la evolución procariota ocurre durante el Proterozoico hace 2450 millones de años, cuando se inicia la Gran Oxidación por acumulación de oxígeno en la atmósfera y los océanos, y la primera glaciación aparece hace 2.300 Ma. La oxigenación fue ocasionada por la proliferación de las cianobacterias (algas verdeazuladas), que son fotosintéticas oxigénicas y que producen estromatolitos con un máximo de desarrollo hace unos 1.200 millones de años. En este nuevo ambiente hace su aparición los primeros seres eucariotas hace unos 1.400 Ma, a partir de ancestros procariotas. Estos cambios debieron significar una extinción masiva procariota, en donde los termófilos solo sobrevivirían en las aguas termales o evolucionaron para adaptarse a los nuevos hábitats. A partir de entonces hasta hoy, las bacterias aerobias se convierten en los organismos más abundantes de la Tierra.

El amplio consenso divide a Prokaryota en dos grandes grupos: Archaea y Bacteria. Ambos son muy antiguos, vienen desde el Arcaico, en los albores de la historia de la Tierra hace más de 3500 millones de años. 

Sus principales diferencias son las siguientes:
De acuerdo con el sistema de tres dominios, Archaea y Bacteria son grupos comparables a Eukarya, sin embargo, se debe tomar en cuenta que el origen eucariota es muy posterior y se produjo por simbiogénesis entre una arquea y una bacteria, por lo que además de las características propias, los eucariontes han heredado de las arqueas características relacionadas con el ADN nuclear, histonas, ribosomas y genes informacionales, mientras que de las bacterias ha heredado características relacionadas con la membrana celular, el metabolismo, las mitocondrias y genes operacionales.

Sin embargo, en algunos árboles del proteoma la relación difiere de la siguiente manera:

Estudios sobre el origen eucariota (eucariogénesis), mostraron que la relación con los procariontes no es tan simple como en estos esquemas. En el origen y evolución eucariota estarían involucrados al menos tres organismos procariotas: una arquea habría sido la célula proto-eucariota anaerobia (hipótesis del eocito), mientras una proteobacteria habría dado origen a las mitocondrias y al metabolismo heterótrofo aerobio eucariota; adicionalmente una cianobacteria daría lugar a los cloroplastos y al metabolismo fotosintético de las plantas (endosimbiosis seriada). Esto implica que habría habido solo dos dominios primarios: Archaea y Bacteria.

Por otro lado, la filogenia procariota presenta una multitud de dificultades para la interpretación de árboles filogenéticos moleculares "(véase Filogenia bacteriana)". Esto es debido a la transferencia genética horizontal, en donde la herencia evolutiva resulta perturbada por el mobiloma (gran conjunto de virus, plásmidos y otros elementos); de tal manera que según el tipo de análisis, por ejemplo de una secuencia de genes, ARN o de proteínas específicas, se obtienen resultados con muchas diferencias. Aun así se puede mostrar una relación filogenética aproximada según algunos autores, entre los diferentes grupos y superfilos arqueanos, bacterianos y su relación con los eucariontes:

Visto de otro modo, la evolución procariota y su relación con los eucariontes, muestran que antes de que hablemos de un árbol filogenético de la vida, deberíamos hablar de un "anillo de la vida"; en donde se observaría que los ancestros procariotas que originaron al primer eucarionte, son una arquea del clado Asgard que le heredó los genes informacionales, mientras que una Alphaproteobacteria le legó los genes operacionales. Tal como podemos observar en la siguiente imagen:

Estudios recientes (2018) revelan la existencia de un importante grupo de nanobacterias denominado CPR, bacterias ultrapequeñas que serían parásitos o simbiontes de otros microorganismos. Estos aparecen en posición basal entre las bacterias, tal como se observa en la imagen adjunta. De modo análogo, las arqueas ultrapequeñas se presentan en posición basal bajo la denominación DPANN.

Durante los siglos XIX y XX se hicieron notorios avances en el conocimiento microbiológico. Sin embargo esto no significó avances en filogenia y clasificación natural de procariontes. La clasificación de plantas y animales se basaba en anatomía comparada y embriología, en cambio las bacterias carecen de complejidad morfológica, a la vez que tienen enorme diversidad fisiológica.

El manual de Bergey desde los años 60’s y 70’s, optó por dar clasificaciones no naturales, pero razonables, en lugar de especular filogenias que cambiasen continuamente. Muchos especialistas (Stanier, van Niel, Winogradsky) se resignaron a aceptar que una clasificación filogenética procariota era imposible, a pesar de la aceptación en general de que es un grupo monofilético y que está relacionada con el origen monofilético de la vida. Se concluyó entonces que debía evitarse el uso del sistema de Linneo con su terminología latina y sus implicancias filogenéticas, pues no tenía sustento, reconociéndose el desconocimiento a todo lo concerniente con la evolución bacteriana; excepto en la identificación de género/especie y se recomendó nombres comunes como bacterias del azufre, fotosintéticas, fijadoras de nitrógeno (Ninogvossky, van Niel) y propusieron cuatro grupos principales: cianofíceas, mixobacterias, espiroquetas y eubacterias (Stanier, Donderoff & Adelberg 1963).

El paso revolucionario en filogenética se da en los años 1970 gracias a los avances en biología molecular, los cuales permitieron elaborar árboles naturales más fiables mediante el análisis genético.

Para el análisis genético procariota se escogió el de la secuencia molecular del ARN ribosomal 16S, dando como resultado que las arqueas, un grupo procariota recién descubierto, estaba genéticamente distante de los demás procariontes, lo cual es atribuido a una antigua divergencia (Balch 1977). La comparación entre el análisis del ARNr 16S procariota con el ARNr 18S eucariota, dio lugar al postulado del Sistema de tres dominios o superreinos (Woese 1977), en donde Archaea, Bacteria y Eucarya son considerados dominios con la categoría taxonómica más alta.

Análisis genéticos posteriores a nivel del proteoma han robustecido la filogenia procariota confirmando la clara separación entre Archeae y Bacteria (Sicheritz 2001).

Aquí se muestra la relación entre algunas notables agrupaciones y sistemas de clasificación procariota:

Los organismos procariotas has sido considerados sucesivamente dentro del reino animal (Bacteria), vegetal (Schizophyta), protista (Moneres) y luego agrupados dentro de su propio reino (Monera o Procaryote).

Ehrenberg acuñó el término Bacteria en 1828 partiendo del griego βακτήριον (bacterion) que significa bastoncito. Su clasificación de 1838 es la primera de muchas que usaron la morfología bacteriana para definir los grupos. En ella agrupó a las bacterias dentro del reino Animal, distinguiendo 5 géneros:

Otras clasificaciones posteriores incluyen por ejemplo a "Micrococcus" (Cohn, 1872) para los cocos o bacterias esféricas y "Chlamydobacteriaceae" (Migula, 1895) para las bacterias filamentosas rodeadas por la vaina y conocidas hoy como proteobacterias.

En 1857, el botánico alemán Nageli rechazó la idea de que las bacterias fuesen animales y les dio el nombre de Schizomycetes (hongos de escisión), dentro del reino vegetal.

Una clasificación más coherente para estos organismos fue hecha por Ferdinand Cohn, que en 1875 juntó a las bacterias ("Schizomycetes") con las algas verdeazuladas ("Schizophyceae") en un grupo que denominó Schizophyta dentro del reino Planta. Schizophyta viene de "schizo"=partición y "phyta"=planta, en alusión a la forma de reproducción bacteriana por división binaria.

Este mismo criterio se mantiene en clasificaciones posteriores como la de Engler (1924), de Wettstein (1934) y de Krasilnikov (1958), este último usando el término Protophyta.

En 1866, Haeckel crea el orden Moneres (del griego μονήρης/moneres=simple), dentro del nivel más bajo del reino Protista para agrupar a las bacterias, pero sin incluir las algas azul-verdosas que estaban como Cyanophyceae entre las algas. Menciona que las bacterias son únicas pues “"...a diferencia de otros protistas, ellas no tienen núcleo y son tan diferentes como lo es la hidra de un vertebrado o un alga simple de una palma"”. En 1904 rectifica en su Die Lebenswunder (Las maravillas de la vida) reconociéndo que "Chromaceae" (algas azul-verdosas), al carecer de núcleo, deben agruparse en Moneres junto con las bacterias; además sugirió al observar los cloroplastos, que las plantas debían haber evolucionado por simbiosis entre una célula verde con otra célula fagótrofa no-verde. Ideas sobre simbiosis a fines del s.XIX no fueron poco comunes. Para Haeckel, la actividad de las moneras se reduce al proceso puramente químico de su metabolismo, de tal manera que la diferencia entre ellas y los demás seres cuyas células tienen núcleo, es la mayor en todos los aspectos, incluso mayor de la que hay entre una monera y un cristal inorgánico.

El término procariota (francés "procaryotes"), así como eucariota, fueron acuñados por Chatton en 1925 para diferenciar los microorganismos anucleados de los nucleados.

Por esta época, se vio la búsqueda de una clasificación natural para las bacterias. En 1927, el botánico Edwin Copeland argumentó que un reino vegetal que incluye a las bacterias "no es más natural que un reino de piedras". En 1938, su hijo Herbert Copeland propone para ellas un reino propio denominado Mychota con el argumento de que eran "los descendientes relativamente poco modificados de la vida que apareció en la Tierra, y que se distingue claramente de los protistas por la ausencia de núcleos".

Paralelamente en 1939, Barkley crea el reino Monera (forma neolatina del "moneres" de Haeckel) para agrupar a virus y procariontes, subdividiéndolo en dos grupos:"Archeophyta" para los virus (definidos como las partículas de la vida temprana primitiva) y "Schizophyta" para las algas azul-verdosas y bacterias.

Un reino formado solo por bacterias denominado Monera fue sustentado por van Niel en 1941, el manual de Bergey propone al reino Protophyta en 1948 y en sucesivas ediciones Monera o Procaryote. Otros autores como Whittaker (1969) y Margulis (1978-1996) también usaron el término Monera.

Si bien en los años 1940 los móneras se definían por acepciones negativas, como la carencia de núcleo, carencia de reproducción sexual, carencia de plástidos y organelas, ya para los 1960 con el desarrollo de la biología molecular y el microscopio electrónico, se redefine a los procariontes en citología comparada, bioquímica y fisiología, de tal manera que la divergencia en estructura celular que separa las bacterias y algas azul-verdosas de los demás organismos celulares (procariontes vs. eucariontes) se reconoce como la más grande discontinuidad evolutiva conocida en el mundo hasta ahora.

R.G.E. Murray, del Manual de Bergey, promovió su reconocimiento taxonómico filogenético en 1968 proponiendo a Procaryotae junto a "Eucaryotae" como taxones del más alto nivel. Al año siguiente A. Allsop les da el nivel de «superreino». Gunther Stent (1971) propone igualmente el superreino Prokaryota, Whittaker (1978) le da la categoría de «dominio», Margulis (1995) propone el término Prokarya y finalmente Mayr (1998) y Cavalier-Smith (2004) reconocen al «imperio» Prokaryota.

Actualmente no existe un sistema taxonómico oficial o que esté respaldado por todos los microbiólogos. Entre las instituciones dedicadas a la taxonomía procariota están el Comité Internacional de Sistemática de Procariotas (ICSP), la Lista de nombres procariotas del Manuel de Bergey (LPSN), el Centro Nacional de Información Tecnológica de EE.UU (NCBI) y el Catálogo de la Vida (CoL).

Según la taxonomía del NCBI hay dos superreinos procariotas que se subdividen en grupos del siguiente modo:

Una taxonomía reciente (sistema del CoL) que incluye grupos parafiléticos, clasifica al superreino Prokaryota en los siguientes reinos y subreinos:




</doc>
<doc id="2399" url="https://es.wikipedia.org/wiki?curid=2399" title="Ponferrada">
Ponferrada

Ponferrada es un municipio y ciudad española, capital de la leonesa de El Bierzo, situada en la confluencia de los ríos Sil y Boeza. Sus habitantes lo convierten en el municipio no capital de provincia más poblado de Castilla y León.

Aunque existen indicios de poblamiento tanto en el Neolítico (en las orillas del Sil), como en la Edad del Hierro y en la época romana, no es hasta el cuando tenemos constancia documental. Es al final de ese siglo cuando el obispo Osmundo de Astorga ordena la construcción de un puente, en torno al año 1082, con la colaboración del rey Alfonso VI de León, para facilitar el tránsito de los peregrinos del Camino de Santiago, debido a las dificultades que suponía el paso del río Sil en el anterior paso, a la altura del actual barrio de Compostilla. Este nuevo puente se reforzó con hierro, y esta circunstancia dio nombre, posteriormente, a la población que creció en sus alrededores, a las orillas del río Sil. Otra teoría sobre el nombre de Ponferrada proviene igualmente de Pons Ferrata pero con la traducción de puente fortificado.

Al poco se fundó la Iglesia de San Pedro, en el año 1086 y a su alrededor surgió "La Puebla de San Pedro" que es como se llamó primeramente a Ponferrada, para pasar a denominarse poco después Ponte Ferrato.

En el año 1180 el rey leonés Fernando II concedió a la villa los primeros fueros, siendo donada posteriormente a la Orden del Temple por el rey Alfonso IX de León. En este sentido, las crónicas cuentan que los primeros templarios que llegaron a Ponferrada lo hicieron con el maestre Guido de Garda a la cabeza, siendo Fray Helías el primer comendador de Ponferrada. Asimismo, la tradición ha atribuido diversas hazañas a los templarios en la localidad, entre las que destaca la del hallazgo de la imagen de la Virgen de la Encina, en torno al año 1200, en el hueco de una encina. No obstante, la disolución de dicha Orden a inicios del por orden papal, conllevó que Ponferrada dejase de depender de la misma en el año 1312.

Por otro lado, durante los siglos y , comienza a crecer y desarrollarse la localidad, ya amurallada, apareciendo tanto intramuros como en los alrededores, campesinos, comerciantes y artesanos, a la sombra del camino, que originó un crecimiento rápido y sostenido. Respecto a la muralla, cabe señalar que fue construida con cuatro puertas de entrada: El Cristo, Paraisín, Las Nieves y Las Eras, asentándose en las afueras de la villa una comunidad judía.

Ya en el , una vez disuelta la Orden del Temple, Ponferrada pasó a depender del linaje de los Castro. En este sentido, en la primera mitad de dicha centuria se recoge a Pedro Fernández de Castro como dueño de la villa y su fortaleza, que en 1343 pasan a manos de su hija, Juana de Castro, que la posee hasta 1374, cuando fallece sin descendencia, circunstancia que propició que Ponferrada pasase a manos de la Corona. 

De este modo, a finales del siglo XIV Enrique III otorga Ponferrada al conde de Trastámara y nieto de Alfonso XI, Pedro Enríquez, que contrae matrimonio con Isabel de Castro, mostrando la documentación de inicios del que Ponferrada se hallaba bajo el dominio del hijo de ambos, Fadrique Enríquez de Castro.

Sin embargo, las diversas fechorías cometidas por este (entre las que se hallaba la de apropiarse de los fondos que el concejo de Ponferrada había reunido para reparar las murallas de la villa), tuvieron como consecuencia que el rey Juan II le apresase y confiscase sus propiedades, entre ellas la villa de Ponferrada. No obstante, su esposa, Aldonza de Mendoza, reclamó la localidad en virtud de las arras matrimoniales, lo que tuvo como consecuencia que el rey reconociese a Doña Aldonza como señora de Ponferrada en 1431.

Posteriormente, Aldonza de Mendoza legó Ponferrada a su sobrino, Diego Manrique, si bien en 1440 éste traspasó la villa a su primo, Pedro Álvarez Osorio, que era señor de Cabrera y Ribera de León, y que recibió de manos del rey Enrique IV en 1456 el Condado de Lemos.

Por otro lado, en esta época, en el contexto del conflicto por la sucesión de Enrique IV, estalló la segunda Revuelta Irmandiña que, iniciada en Galicia, se expandió al Bierzo desde su parte occidental en 1467, atacando los irmandiños varias fortalezas leonesas, como Cornatel, Balboa o Sarracín, llegando hasta Ponferrada, donde se refugió buena parte de la nobleza gallega que huía de los irmandiños, y que se atrincheró en la fortaleza ponferradina, que sufrió daños por al ataque de las fuerzas irmandiñas, antes de ser sofocada la revuelta. Tras esta, en 1469, el conde de Lemos, Pedro Álvarez Osorio, mandó reconstruir y reparar sus castillos que habían sufrido daños.

Posteriormente, tras el fallecimiento en 1483 de Pedro Álvarez Osorio, estalló un cruento conflicto por su sucesión, que enfrentó, por un lado, al bando formado por la segunda mujer del Conde de Lemos, María de Bazán, y su hija Juana Osorio, casada con Luis Pimentel (hijo del conde de Benavente), y por otro lado a Rodrigo de Castro Osorio, nuevo Conde de Lemos. Para solucionar el conflicto, en 1486 los Reyes católicos decidieron actuar, creando el marquesado de Villafranca del Bierzo para Juana y su marido Luis Pimentel, quedando para Rodrigo de Castro Osorio el condado de Lemos, motivo por el cual los monarcas obligaron a éste a entregar el castillo de Ponferrada, que había tomado, que pasó a manos de la Corona. De esta manera, el castillo y la villa de Ponferrada pasaron a ser de realengo nuevamente, hecho que se prolongó hasta el fin del Antiguo Régimen, correspondiendo a los monarcas nombrar corregidor en la villa, siendo don Juan de Torres el primero que detentó este cargo tras la vuelta a la propiedad real.

Por otro lado, cabe destacar que en esta época, con la reducción de ciudades con voto en Cortes a partir de las Cortes de 1425, las localidades del actual municipio de Ponferrada pasaron a estar representadas por León, lo que les hizo formar parte de la provincia de León en la Edad Moderna, encabezando no obstante Ponferrada un partido propio dentro de esta, que coloquialmente era denominado "provincia del Vierzo". Asimismo, debido a la adscripción territorial desde la Alta Edad Media del territorio de Ponferrada al reino leonés, durante toda la Edad Moderna las localidades del municipio formaron parte de la jurisdicción del «Adelantamiento del reino de León».

Ya en la Edad Contemporánea, iniciado el siglo XIX, durante la Guerra de la Independencia Ponferrada destacó por haber llegado a albergar la sede de la Junta Superior de León en diversas fechas de septiembre de 1809, marzo de 1810, y agosto de 1811.

Más tarde, en 1821 Ponferrada y el resto de localidades del municipio pasaron a formar parte de la provincia de Villafranca, si bien al perder esta su estatus provincial al finalizar el Trienio Liberal, en la división de 1833 pasaron a estar adscritas a la provincia de León, dentro de la Región Leonesa. Un año después, en 1834, cuando se realizó en España la primera división en partidos judiciales, Ponferrada pasó a encabezar uno de ellos, incorporando al mismo el partido judicial de Villafranca en 1966.

Ya en el , cabe destacar que el 4 de septiembre de 1908 el rey Alfonso XIII concedió a Ponferrada el título de ciudad, coincidiendo con el centenario de la coronación de la Virgen de la Encina como patrona de la comarca. Finalmente, el descubrimiento y la explotación de las riquezas minerales, hierro y carbón, en esta época, así como la instalación en 1949 de la central térmica de Endesa, cambiaron definitivamente el carácter de la ciudad, que se ha asentado como el principal núcleo poblacional de la comarca y su centro económico y de servicios.

Ponferrada se encuentra en la confluencia de los ríos Sil y Boeza, en el extremo este de la llanura berciana, y en el noroeste de la provincia de León. La altitud media de la ciudad es de , la máxima altitud del casco urbano se sitúa en el monte Pajariel con . La máxima altitud del municipio se sitúa en el pico de Pico Tuerto, en la sierra de El Morredero, a 2051 msnm. El término municipal cuenta con una superficie total de .

Está rodeada por los siguientes municipios:

Ponferrada tiene un clima mediterráneo continental subhúmedo que se da en las zonas o regiones consideradas de transición entre el clima mediterráneo continental y el clima oceánico o de montaña, contando con precipitaciones relativamente abundantes, aunque con la sequía estival característica del clima mediterráneo y adquiriendo en las regiones interiores la nieve mucha importancia durante el invierno. La nieve es un fenómeno relativamente frecuente en invierno y se suele registrar entre diciembre y marzo. Los inviernos son fríos y largos siendo normales temperaturas de -3 °C, por otro lado el verano es corto y cálido con temperaturas que fácilmente superan los 32 °C. De acuerdo con la clasificación climática de Koppen, Ponferrada tiene un clima de transición entre los climas mediterráneo (Csa) y oceánico mediterráneo (Csb). Además, en las zonas montañosas del sur del municipio se da también el clima oceánico (Cfb), y el clima Dfb (Hemiboreal sin estación seca) en las montañas más altas del municipio, que en algunos casos llegan a superar los 2000 metros de altitud. 

El núcleo urbano de Ponferrada está bañado por dos ríos, el Sil y el Boeza.

El primero, atraviesa la ciudad de noreste a sur, hasta el barrio de la estación, donde une el caudal del Boeza, y el río comienza a seguir la falda del monte Pajariel, en dirección hacia el oeste. El río se encuentra canalizado a su paso por la ciudad, existiendo paseos y jardines junto a su ribera, que empiezan a la altura de "La Fuente del Azufre", una antigua central hidroeléctrica actualmente abandonada, y terminan en el barrio de Flores del Sil. Existen varios puentes y pasarelas para salvar el Sil a su paso por la ciudad, siendo los más recientes una pasarela que une al barrio de la estación con el monte Pajariel y el "Puente del Centenario", que permite una nueva unión entre la parte alta de Ponferrada y el barrio del Campo de los Judíos con la zona de Compostilla y las huertas del Sacramento. Hay que destacar que el Sil divide a la ciudad en dos partes, y que este hecho unido a la orografía ponferradina, hace distinguir a la zona antigua de la ciudad como la "Zona Alta".

El río Boeza delimita por el sur el núcleo urbano de la ciudad, separando al barrio del "Puente Boeza" del núcleo urbano. Este río es un afluente del Sil, y su desembocadura en él se produce en Ponferrada, en las faldas del Pajariel.

Además, el municipio es atravesado por otros ríos. Es el caso del río Oza, que nace en los Montes Aquilanos, atravesando varios pueblos del sur del municipio, que integran el Valle del Oza, como son Montes de Valdueza, San Clemente de Valdueza, Valdefrancos o San Esteban de Valdueza. El río vierte sus aguas al Sil tras pasar el pueblo de Toral de Merayo. Un afluente del río Oza es el río Valdueza, que pasa por el pueblo de Villanueva de Valdueza, desembocando en el Oza cerca de San Esteban.

De la "Fuente del Azufre" salen dos canales, uno de ellos destinado al riego, el "Canal Bajo del Bierzo", que lleva agua procedente del Sil hasta Carracedelo, pasando en Ponferrada por los barrios de Compostilla y Cuatrovientos. El otro canal, el "Canal de Cornatel", que lleva agua del Sil hasta la presa de Campañana, en Carucedo. El canal de Cornatel atraviesa la parte alta de Ponferrada soterrado, saliendo a la luz en un tramo muy próximo al casco antiguo, para luego volver a soterrarse, cruzar el Boeza y volver a desaparecer en las cercanías de Otero.

El embalse de Bárcena está ubicado también en el municipio de Ponferrada, además de otros como Congosto y Cubillos del Sil. Fue inaugurado en 1960 y ello supuso la inundación de pueblos como "Bárcena del Río" y "Posada del Río", siendo su población trasladada a pueblos de nueva creación (Bárcena del Bierzo, su nombre inicial fue "Bárcena del Caudillo" y Posada del Bierzo), o al barrio de Fuentesnuevas. Fue inaugurado en 1960, construido en el curso del Río Sil y tiene varios usos: Refrigerar la central de Compostilla II, principal motivo de su construcción; aprovechamiento para la generación de energía eléctrica mediante una central hidroeléctrica que posee, y recientemente, el abastecimiento de agua a Ponferrada, con la construcción e inauguración en 2009 de la nueva traída de agua desde el embalse, para evitar los problemas de suministro que sufría la ciudad debido a su crecimiento, ya que hasta 2009 se dependía del agua del río Oza y de la mancomunidad.

La Mancomunidad de Municipios del Agua del Bierzo es un ente público formado por los municipios de Arganza, Cabañas Raras, Cacabelos, Camponaraya, Carracedelo, Cubillos del Sil, y Sancedo (gran parte de los municipios que comprende el Bierzo Bajo excepto Villafranca del Bierzo, Toral de los Vados, Priaranza del Bierzo y Borrenes), pudiéndose adherirse otros municipios de acuerdo a lo recogido en sus Estatutos y al procedimiento indicado en la Ley 1/ 1998, del 4 de junio del Régimen Local de Castilla y León.

En el año 2008 el municipio de Ponferrada mostró su voluntad de abandonar la Mancomunidad, decisión que finalmente se llevó a cabo, procediéndose a cambiar la denominación de la entidad (fundada como Mancomunidad de municipios de la comarca de Ponferrada). A pesar de esta decisión se ha mantenido la sede de esta entidad en Ponferrada, situada en el barrio de Fuentesnuevas, avenida de Galicia número 369.

Ponferrada es un municipio trilingüe, ya que, además del español, se conservan en su término municipal las lenguas leonesa y gallega. En este sentido, la lengua tradicional de la mayor parte del municipio es el leonés, si bien el habla predominante de cuatro pedanías del extremo occidental del mismo (Fuentesnuevas, Dehesas, Toral de Merayo y Rimor) es el gallego, muy mezclado, no obstante, con el leonés (especialmente en el caso de las hablas locales de Toral de Merayo y Rimor). Por otro lado, lingüísticamente la toponimia histórica de estas cuatro localidades actualmente gallegohablantes sería propia del asturleonés, de lo que podría deducirse que el elemento lingüístico predominante en todo el municipio fue inicialmente el asturleonés, que habría sido desplazado posteriormente por el gallego en las pedanías más occidentales del municipio. No obstante, en el resto del municipio se ha mantenido como predominante el elemento lingüístico leonés, tanto en el habla como en la toponimia, aunque sufriendo una cada vez mayor castellanización. 

Ponferrada es el sexto municipio más poblado de la comunidad de Castilla y León, superando en número de habitantes a cuatro capitales de provincia y al resto de municipios de la comunidad autónoma.


Se observa el espectacular despegue poblacional entre 1940 y 1970, periodo en el que Ponferrada triplica su población, principalmente debido al crecimiento industrial del municipio y convertirse en el centro administrativo de la comarca, que vivía un auge minero sin precedentes. Esto dio lugar a numerosos problemas en su ordenación urbana debido al crecimiento desproporcionado y muchas veces caótico de las distintas barriadas de la ciudad, problema arrastrado hasta bien entrada la década de 1990-2000.

El casco urbano de Ponferrada, excluyendo Toral de Merayo, Flores del Sil, Cuatrovientos, Fuentesnuevas, Dehesas y otros barrios, tenía una población de 40.235 habitantes a 1 de enero de 2018, repartidos en 18.871 varones y 21.364 mujeres.

Las entidades de población que componen el término municipal de Ponferrada son las siguientes:

Actualmente, la Corporación Municipal de Ponferrada está integrada por 25 concejales, según lo dispuesto en la Ley del Régimen Electoral General, que establece el número de concejales elegibles en función de la población del municipio.

Hasta la fecha, todos los alcaldes de la ciudad de Ponferrada han pertenecido al Partido Popular (PP) y el Partido Socialista Obrero Español (PSOE). El 8 de marzo de 2013 se produce una moción de censura presentada por PSOE-IAP, que desbancó a Carlos López Riesco para colocar en la alcaldía a Samuel Folgueral Arias, del PSOE. Hecho polémico que planteó a Folgueral la disyuntiva de seguir en el PSOE o dejar su nuevo cargo de alcalde si no rompía el pacto con IAP, partido liderado por Ismael Álvarez. El 10 de marzo, Folgueral y los otros siete concejales del PSOE optaron por marcharse del partido. Por su parte, Álvarez (IAP) renunció al cargo.

El municipio de Ponferrada comprende, además de la ciudad de Ponferrada, a otras 33 entidades singulares de población. Algunas de ellas, 17 en total, son, a su vez, entidades locales menores. El municipio anexionó los términos municipales de San Esteban de Valdueza en 1974 y de Los Barrios de Salas en 1980, ambos integrados por muchos pueblos.

Los núcleos de población que forman parte del municipio de Ponferrada son los siguientes:



El Castillo "templario" de Ponferrada se sitúa sobre una colina en la confluencia de los ríos Boeza y Sil. Se emplaza en lo que, probablemente, en origen fue un castro celta, en una posición similar a la de otros de la comarca. Posteriormente se cree que fue un emplazamiento romano y visigodo.

Hacia 1178 Fernando II permite que los templarios establezcan una encomienda en la actual Ponferrada. En 1180 el rey expide fuero para la repoblación de la villa que había surgido un siglo antes, documentándose la primera fortificación en 1187. La fortaleza fue reconstruida en numerosas ocasiones a lo largo de las Edades Media y Moderna.
Actualmente, toda la zona palacial y ciertas torres de su recinto del han sido reformadas con el fin de instalar un centro cultural.

En estos momentos el castillo cuenta con la exposición permanente "templum libri" en la que se exponen libros facsímiles del medievo y del renacimiento, y la nueva "Biblioteca Templaria", que cuenta con 1380 volúmenes relacionados con la Orden del Temple y que la convierten en la mayor de sus características en todo el mundo.

De estilo renacentista, se comenzó a construir en 1572, bajo la dirección del arquitecto Juan de Alvear, que realizó la cabecera del templo, desapareciendo así la iglesia medieval. La esbelta torre que remata la basílica se construyó en 1614. Custodia en su interior la imagen de la Virgen de La Encina, patrona de El Bierzo.

Se intentó convertir en colegiata, pero la petición fue denegada el 6 de mayo de 1720 y posteriormente se denegó una nueva petición, el 21 de octubre de 1725.

Es una de las joyas arquitectónicas del Arte mozárabe junto al también leonés monasterio de San Miguel de Escalada. Está enclavada en el corazón del Valle del Oza a 14 kilómetros de Ponferrada. La zona, llena de monasterios e iglesias eremíticas desde el mereció en su época el sobrenombre de la "Tebaida berciana". Su situación en el centro de los Montes Aquilanos, facilitaba el aislamiento que buscaban los santos ascéticos de los siglos y , como San Fructuoso y San Genadio.

Fue construida en la primera mitad del por el abad Salomón, concretamente en el reinado de Ramiro II, Ramiro II hizo múltiples donaciones a la iglesia y al monasterio de la localidad. Entre ellas estuvo la llamada Cruz de Peñalba, símbolo del Bierzo. En esta cruz se puede ver la fuerte influencia de la orfebrería visigoda sobre los modelos mozárabes.

Situado en la Valdueza, fue junto con el Monasterio de Santa María de Carracedo, el más poderoso de los monasterios en cuanto a dominios de la comarca. Fundado por San Fructuoso hacia el año 635 a día de hoy solo se conservan ruinas y la iglesia del Monasterio debido a un incendio en el , la desamortización de Mendizábal y la desatención de las autoridades patrimoniales.

Actualmente, recoge diversos estilos: prerrománico, presente en alguno de los capiteles conservados; el románico de la torre, el claustro y la fachada de la iglesia, pertenecientes al siglo XVIII.

Situada en el pueblo de Santo Tomas de las Ollas construida , es una de las mejores muestras del estilo mozárabe. A destacar su bella capilla arqueada. El nombre de la ermita, tomado del pueblo Santo Tomás de las Ollas, proviene del oficio principal de esa localidad: la alfarería. Proveían de género a la zona del Valle del Oza. La ermita fue donada por el a la comunidad del Monasterio de San Pedro de Montes.

Su construcción original data del , después de la cual se efectuaron diversas modificaciones, siendo la última la Sacristía, del . La ermita consta de una nave rectangular con sólo un ábside el cual, a pesar de tener una planta ovalada, continúa la línea rectangular de la nave en sus muros exteriores. La techumbre es a dos aguas, de pizarra, y asentado sobre una armadura de madera.

Situada en el pueblo de Otero de Ponferrada. Consta de una sola nave. Fue construida en el , siendo la iglesia más antigua de estilo románico, también se pueden observar influencias del mozárabe. Destacan sus arcos de herradura y el bello testero, en el que se encuentra una ventana geminada de gran belleza.

En las estribaciones del municipio, dentro de los Montes Aquilanos, se encuentra la estación de esquí del Morredero.
Creada en el año 2002], actualmente es muy pequeña, con proyectos de ampliación, destacando la calidad de la nieve y el bajo coste del forfait. De su gestión se encarga la Asociación de Amigos del Morredero.

Recientemente se ha presentado un proyecto desde el ayuntamiento de Ponferrada para la construcción de equipamientos turísticos y la ampliación del dominio esquiable al máximo de las posibilidades del entorno; lo que daría lugar a una estación algo inferior a la vecina San Isidro. Está previsto que esté en funcionamiento en 2014 coincidiendo con el mundial de ciclismo en Ponferrada.


En los últimos años se ha incrementado notablemente el número de zonas verdes en la ciudad. El pulmón verde de Ponferrada es el monte Pajariel, aunque ha sufrido recientemente varios incendios forestales, existiendo un proyecto para convertirlo en parque forestal. Todos los barrios ponferradinos cuentan con alguna zona verde. Los principales parques de la ciudad son el del Plantío, parque del Temple, parque de la Concordia y parque del Oeste.


Este parque está ubicado en la zona alta de la ciudad, cercano a la universidad, contando con una senda de acceso desde el barrio de los Judíos. También es denominado "Parque Gil y Carrasco". La especie predominante en el arbolado del parque es el pino, aunque también existen otras especies de árboles de jardín. Entre los diferentes servicios que dispone el parque está una cancha deportiva de cemento en la que se puede practicar fútbol o baloncesto, una cafetería, un centro de día y una gran zona destinada a juegos infantiles, existiendo otro espacio para realizar ejercicios denominadas "Zonas de 1 a 100 años", que también hay en otros parques ponferradinos. Además cuenta con un estanque en el que hay patos, gansos y cisnes, un recinto cubierto donde se encuentran pavos reales y loros entre otras especies, y un quiosco o templete de música. Anteriormente, la estatua de "La Carrasca", el personaje de la novela El Señor de Bembibre de Gil y Carrasco, se encontraba en el parque, pero actualmente se ha trasladado a la glorieta homónima, que se encuentra junto a una de las entradas del parque.


Este parque está ubicado entre la zona centro de la ciudad y el barrio de Flores del Sil, rodeado en el norte por la barriada del temple, en el este por el barrio de la estación, en el sur por el auditorio municipal y la ribera del Sil, y en el oeste por el barrio de Flores del Sil. Es uno de los mayores parque de la ciudad. También es conocido como el "Parque del Belga" porque hasta hace unos años este parque era propiedad de la MSP, ya que era una finca particular, que estaba vallada, y donde residía el primero director general, y posteriormente vicepresidente de la MSP, el ingeniero Marcelo Jorissen, oriundo de Bruselas.

Este parque, perfectamente integrado en la ciudad, albergaba el conservatorio de Ponferrada, aunque debido a su traslado a un nuevo edificio en la Rosaleda, en noviembre de 2014 se hizo efectivo el traslado la sede de la policía local a este edificio, el auditorio municipal donde se ofrecen conciertos y espectáculos durante las fiestas de la encina, y celebrándose también en el parque unos días antes del comienzo de las fiestas, la denominada "ciudad mágica", más popularmente conocida como "CIMA" en la que se desarrollan numerosas actividades para los más pequeños, con talleres de artesanía, teatro, manualidades, hinchables..., existiendo el punto de información juvenil CIMA en uno de los edificios del parque.

La mayoría del arbolado está compuesto por pino. En el parque hay, además de los servicios expuestos anteriormente, una cafetería, un espacio destinado a la práctica de petanca, realizándose eventualmente torneos de la misma; canchas deportivas de cemento para la práctica de deportes como fútbol y baloncesto, y un circuito para el aprendizaje de seguridad vial, en que la policía municipal imparte clases de seguridad vial a los alumnos de los colegios de Ponferrada. También hay varias zonas destinadas a juegos y atracciones infantiles, además de una fuente y un estanque.

Recientemente se ha ampliado el parque para unirlo totalmente al barrio de flores del Sil, elevando la superficie a 60.000 m².


Ubicado en la zona de las huertas del sacramento, en la parte baja de ponferrada, es un parque que se desarrolla junto a una parte de la ribera del Sil, desde los campos de fútbol "Ramón Martínez" hasta el conocido "Puente de Cubelos".Hace unos años se acometió una gran reforma en el parque. Cuenta con todos los caminos asfaltados, y los dos antiguos quioscos han sido restaurados, albergando uno de ellos un aula de interpretación del río Sil. Tiene un estanque, un área infantil, y otra área para realizar ejercicios de gimnasia. La especie predominante es el chopo. Junto al parque se encuentran los juzgados, los campos de fútbol "Ramón Martínez", la escuela de idiomas de Ponferrada, el colegio "Peñalba" y el instituto "Álvaro de Mendaña". El parque es atravesado por la avenida "Gran vía del Reino de León".


Es el último parque construido en Ponferrada, ubicado junto al nuevo barrio de la Rosaleda, y que une este barrio con Cuatrovientos. Es un importante espacio verde, en el que predomina el césped. En él se encuentra el nuevo conservatorio de música de Ponferrada. Tiene un carril bici que bordea todo el parque, y cuenta con varias zonas de merenderos, así como una zona de juegos infantiles.


Además de estos parques, existen más zonas verdes distribuidas por toda la ciudad:

En toda la comarca se producen vinos de gran calidad (Denominación de Origen Bierzo), exportados a diversos sitios como Alemania o Estados Unidos, en el municipìo de Ponferrada, concretamente en el barrio del Puente Boeza tiene su sede la Cooperativa Vitivinícola "Cepas del Bierzo" la de mayor producción en tonelaje de uvas de El Bierzo, que recoge la uva procedente de las viñas de los pueblos de alrededor de la ciudad como son San Lorenzo, Otero o Campo, por lo que los últimos fines de semana del mes de septiembre o el primero de octubre, según hayan sido las precipitaciones y las condiciones climáticas durante el verano, se realiza la vendimia, en la que se procede a recolectar la uva de las viñas de estos pueblos ubicados en el sur y sureste de Ponferrada.

También tienen un lugar muy destacado en la agricultura ponferradina y berciana las frutas y hortalizas (Denominación de origen Manzana Reineta del Bierzo o Pimiento del Bierzo); castañas, pera Conferencia,cerezas etcétera.

Aún se explotan numerosas huertas, bien para el consumo familiar o bien para una posterior venta en el mercado de abastos de Ponferrada o fruterías locales. Estas huertas todavía se pueden encontrar en la ciudad, por la zona de la ribera del Sil en el barrio de los Judíos, que son el vestigio de la zona de "Las Huertas del Sacramento", ubicada donde actualmente está la avenida homónima, y donde numerosos ponferradinos tenían tierras en las que cultivaban diferentes tipos de hortalizas. Otros lugares aún próximos a la zona urbana son las zonas de huertas existentes entre los barrios de Flores del Sil y La Placa, o entre Cuatrovientos y Columbrianos, además de en las zonas rurales. En septiembre, las familias que han plantado pimientos en sus huertas, todavía continúan con la tradición del asado de pimientos, comenzada en el por las amas de casa bercianas, conocidas como "las pimenteras", y que consiste en recolecta los pimientos, almacenarlos y a los pocos días, se asan, y posteriormente se pelan y se eliminan las semillas, pasando a envasarlos en pequeños botes de vidrio.

La ganadería, aunque ha sufrido un retroceso en el municipio debido al crecimiento de la ciudad, sigue teniendo una cierta importancia tanto a nivel de explotaciones familiares en los pueblos del municipio, como en la cría de distintas especies para sacrificio.
Hasta 2009 Ponferrada contaba con un matadero municipal ubicado en la zona de La Martina, a la espera de la construcción de un matadero comarcal.

Sede de CUPA GROUP, líder mundial en la producción y comercialización de pizarras para cubiertas, con más de 2000 trabajadores y filiales en 11 países. Cuenta con la mayor planta de producción de palas (LM Wind Power) y fustes (Grupo Comonor) para aerogeneradores eléctricos de Europa que emplearon en su día a unos 1500 trabajadores encuadrándose por tanto dentro de la gran industria, aunque actualmente ha cesado la actividad en la segunda y se ha reducido la plantilla en la primera. Una empresa de siderurgia, Aceros Roldán S.A., que posee una planta de mediano tamaño dentro del grupo Acerinox y diversas pequeñas empresas. Otra empresa a destacar en la comarca es Vitro Cristalglass (transformación del vidrio) quien empleó a más 600 trabajadores y cuyas instalaciones son las de mayor fabricación de vidrio aislante de la península. Actualmente, debido a la crisis económica, esta empresa se encuentra en concurso de acreedores. Así mismo es la cuna tanto de la Minero Siderúrgica de Ponferrada, recientemente convertida en Coto Minero Cantábrico, la mayor empresa minera privada de España, como de ENDESA," Empresa Nacional de Electricidad", fundada en 1947, privatizada en el 2000 y que es la mayor eléctrica de España e Iberoamérica. "Compostilla I" fue su primera planta de producción, inaugurada a principio de los años 50. En la década de los 60 se crea la Central térmica de Compostilla II, en el cercano municipio de Cubillos del Sil (León).
A estas dos industrias, la MSP y ENDESA es a las que se les debe mayoritariamente el gran crecimiento de Ponferrada en el .

Dentro del municipio tienen su sede distintas empresas dedicadas a la fabricación de productos cárnicos con gran producción y volumen de ventas, como es el caso de "Embutidos Pajariel". Su zona de influencia también cuenta con industrias agroalimentarias y empresas que se dedican a la producción de cemento y también de suma importancia la manufacturación de la pizarra.

En la antigua "Central Térmica de Compostilla I", tiene su sede la fundación "Ciudad de la Energía (CIUDEN)". A finales de 2008 se aprueba el proyecto que convertirá la antigua "Central Térmica de la MSP" y parte de la "Central Térmica de Compostilla I" en el "Museo Nacional de la Energía".

En Ponferrada existen varios polígonos industriales:





Actualmente está en construcción el Cylog, un centro logístico que se ubica en la zona de "los muelles", en las cercanías de Cuatrovientos, y que actuará como un puerto seco que contará con acceso ferroviario y por carretera. Existen acuerdos para transportar mercancías de los puertos de La Coruña y Vigo.

El sector servicios, que sigue creciendo como en el resto de Europa, representa actualmente el sector probablemente más en auge y el que mayores ingresos produce aunque centrado en el comercio generando un nulo valor añadido y empleo con una escasa demanda de cualificación, como el resto del país.

Ponferrada ha sido un destacado nudo de comunicaciones desde la primera mitad del , formando parte del Camino de Santiago. Desde principios del en el que se asfaltan las primeras carreteras, forma parte del itinerario de la N-VI, una de las seis carreteras nacionales radiales que vertebran España. En la actualidad, la ciudad se encuentra plenamente integrada en la mediante la autovía A-6, que es la principal entrada a Galicia.

En 2011, Ponferrada contaba con un total de 46.816 vehículos de motor, lo que resulta a 680 automóviles por cada 1.000 habitantes.

Ponferrada cuenta con una estación de ferrocarril, gestionada por Adif que mantiene líneas con Vigo, La Coruña, Zaragoza, Madrid y Barcelona.

Está ubicada en el Barrio de la Estación y presta servicio "Renfe Operadora", mediante diversos trenes destinados a trayectos de Larga Distancia, como el Alvia, el Arco o Trenhotel, u orientados a relaciones de Media Distancia, en este caso, Intercity o Regional Exprés.

Permite numerosas conexiones a nivel nacional, puesto que enlaza diariamente con comunidades como Navarra, País Vasco, Aragón, Cataluña contando con varios trenes a lo largo del día.

Además de esta estación, destinada principalmente al transporte de viajeros, en el Polígono Industrial del Bierzo (PIB), existe una terminal para el transporte ferroviario de mercancías, gestionada por Adif y situada entre Ponferrada, Cuatrovientos y Flores del Sil. Es utilizada mayoritariamente para descargar trenes de carbón procedentes de los puertos de La Coruña, Avilés o de Gijón. También salen de ella trenes cargados con la ceniza producida por la central de Compostilla II. Dispone de 4 vías que están electrificadas en su cabecera, de las cuales dos tienen acceso al muelle de carga y descarga de la terminal.

Ponferrada era el origen del Ferrocarril Ponferrada - Villablino de vía métrica que conectaba las minas de Laciana con las centrales térmicas de Ponferrada y la línea de RENFE para trasbordar el carbón a los trenes de esta, siendo la estación ponferradina actualmente la sede del Museo del Ferrocarril de Ponferrada. En él hay restauradas varias locomotoras y vagones que prestaron servicio en la línea. En el municipio se conservan todavía algunos restos de este ferrocarril que hoy día parte de Cubillos del Sil, pudiéndose ver en algunas zonas los raíles de la vía.

La estación de autobuses está situada en la avenida de la Libertad, y desde ella parten autobuses con destinos comarcales como Toral de los Vados, Carracedelo, Bembibre, Villafranca...;regionales como León o Salamanca y destinos nacionales a Madrid, Santiago de Compostela, La Coruña...

La gran mayoría de los destinos regionales y nacionales están operados por Alsa, mientras que los comarcales son prestados por empresas locales (Aupsa, Pelines y González de la Riva) además de Alsa

Actualmente la ciudad de Ponferrada cuenta con un servicio de transporte urbano de 15 líneas de las que podemos destacar la línea Circular.
En los días laborables y sábados por la mañana existen 9 líneas de las cuales 8 son diametrales y conectan los diversos barrios de Ponferrada. Tienen una frecuencia de 1 hora:









También existe la línea Circular que conecta la zona alta de la ciudad con diferentes puntos administrativos o de interés como el Centro Comercial El Rosal, el Barrio de la Rosaleda, la estación de autobuses, Hospital de la Reina, el ayuntamiento o los Juzgados. Cuenta con una frecuencia de aproximadamente 15 minutos.

Los fines de semana y los días festivos, debido al menor uso del autobús circulan únicamente 5 líneas:





Estas líneas tienen una frecuencia de 1 hora y circulan los sábados por la tarde, domingos y festivos.
Los domingos y festivos el servicio comienza a las 11:00.
Además también circula el:


Hay que destacar que la totalidad de las líneas pasan por la parada del Intercambiador, para permitir una mayor intermodalidad entre ellas y una mayor variedad de destinos.

En el verano de 2009 para evitar que siguiese creciendo el Déficit del TUP se realizó un plan de Ajuste en el cual se suprimieron las Líneas L13, C2 y C9, así como se redujeron las frecuencias de paso de algunas líneas por los pueblos de sus extremos, como Bárcena, San Lorenzo, Santo Tomás o Dehesas, y que vieron aumentado su tiempo de espera de 1 a 2 horas. Posteriormente, también fue cancelada la Línea Búho.

Toda la flota fue renovada en el 2007, así como además de la creación de nuevas líneas y bonos se renovaron las marquesinas y paradas y se introdujeron los paneles electrónicos para saber el tiempo de espera de los buses en las paradas más importantes.
Al TUP se puede acceder con la tarjeta ciudadana de Ponferrada. En 2013 el billete sencillo del TUP cuesta 1,15€ y da derecho a transbordo a otras líneas durante 45 minutos.
Ponferrada es un municipio muy extenso, con una orografía complicada, donde mantener una línea regular en algunas poblaciones es inviable. Para paliar estas carencias, se puso en marcha hace años el Transporte a la Demanda. Es un servicio de Transporte Público de la Junta de Castilla y León, con reserva de plazas. No es una línea regular, solo funciona si se solicita. Permite acercarse de manera sencilla y muy económica a localidades con un indudable valor turístico como Peñalba, los pueblos de la Valdueza o Los Barrios de Salas. Además, 3 rutas de transporte escolar admiten su uso por el público en general: Ponferrrada a Fresnedo, Ponferrada a San Clemente de Valdueza y Ponferrada a Onamio. Empresas que prestan el servicio en el área de Ponferrada: AUPSA y Pelines.

Paradas

En Ponferrada hay 6 paradas para tomar el Transporte a la Demanda:

Parada del Transporte Urbano de Ponferrada (TUP) del Hospital de El Bierzo, parada del TUP frente al colegio Navaliegos,  Centro de Salud I, Centro de Salud II, Centro de Salud III y la parada del TUP de la avenida de la puebla, 44 (frente al hotel Alda, antiguo hotel Madrid).

Y, además, las paradas del Transporte Escolar:

Colegio San Andrés-La Borreca y el Instituto Virgen de La Encina.
Las rutas son en ambos sentidos. Por ejemplo: permiten ir de Ponferrada a Bouzas y de Bouzas a Ponferrada. Para simplificar, solo se muestran el orden de Ponferrada a la última población a la que llega el transporte.


"Martes y miércoles laborables"

Ponferrada, Villar de Los Barrios, Salas de Los Barrios, Lombillo de Los Barrios, Espinoso de Compludo, Manzanedo de Valdueza, San Cristóbal de Valdueza, Bouzas.

Ruta realizada por Autos Pelines.


"Miércoles y jueves laborables."

Ponferrada, Villalibre de La Jurisdicción, Priaranza del Bierzo, Santalla del Bierzo, Alto de Borrenes, Paradela de Muces, Villavieja.

Ruta realizada por AUPSA. 


"Miércoles y viernes laborables"

Ponferrada, Toral de Merayo, Rimor, Orbanajo, Ozuela.

Ruta realizada por AUPSA.


"Miércoles y sábados laborables".

Ponferrada, Otero, Puente Boeza, San Lorenzo, Valdecañada, San Esteban de Valdueza, Villanueva de Valdueza, Valdefrancos, San Clemente de Valdueza, Montes de Valdueza, Peñalba.

Ruta realizada por Autos Pelines.


"Martes y jueves laborables."

Ponferrada, Molinaseca, Riego de Ambrós, El Acebo, Carracedo de Compludo, Compludo, Palacios de Compludo (Cruce).

Ruta realizada por Autos Pelines.


"Lunes y viernes laborables."

Ponferrada, Bárcena del Bierzo, Cubillos del Sil, Posadina, Cabañas de la Dornilla, Cubillinos, Finolledo, Fresnedo.

También se puede realizar de lunes a viernes lectivos usando el Transporte Escolar.

Ruta realizada por Autos Pelines.


"Lunes a viernes lectivos."

La parada está en el Colegio San Andrés-La Borreca.

Ponferrada, Puente Boeza, Cooperativa, San Lorenzo, San Esteban De Valdueza, Villanueva de Valdueza, Valdefrancos, San Clemente de Valdueza.

Ruta realizada por Autos Pelines.

"Lunes a viernes lectivos."

La parada está en el Colegio San Andrés-La Borreca.

Ponferrada, Urbanización Patricia, Molinaseca, Poblado de Onamio, Onamio.

Ruta realizada por Autos Pelines.

Además del servicio de autobuses urbanos (TUP), Ponferrada cuenta con un sistema de préstamo de bicicletas, contando con 10 estaciones distribuidas por diferentes barrios y zonas de la ciudad. En los últimos años se ha comenzado a expandir la puesta en servicio de tramos de carril bici en algunas zonas de Ponferrada, bien sea integrándolos en la acera o en carreteras existentes, así como limitando la velocidad de algunas calles del centro urbano para poder hacer compatible el uso de la bici por ellas. También se han distribuido aparcabicis junto a lugares públicos o deportivos para poder dejar la bicicleta en ellos.

Ponferrada cuenta con tres zonas en las que se restringe el acceso al tráfico, que son el Casco Antiguo, la Avenida de España y la zona comercial del centro de la ciudad que está acotada por las calles Gómez Núñez, Camino de Santiago y Ramón y Cajal, a las que únicamente pueden entrar los residentes, quedando como zonas semipeatonales, fomentando el turismo y el comercio, estableciéndose varios aparcamientos disuasorios ubicados en la Avenida de la Libertad, el Museo del Ferrocarril y el Albergue de Peregrinos para estacionar los vehículos. También existen tres aparcamientos subterráneos en la ciudad, construidos en los últimos años. El primero se construyó bajo el bulevar de Pérez Colino, el segundo, bajo la Plaza del Ayuntamiento, y el tercero, en la Avenida de Compostilla. Entre todos suman un total de más de 800 plazas.

Debido a la orografía ponferradina, desde el año 2003 para acceder a la zona alta de la ciudad desde la calle General Vives, se ha instalado un ascensor público panorámico de uso gratuito y en funcionamiento las 24 horas del día, que facilita el acceso al casco histórico y a la popular "Calle Ancha", una de las principales arterias de la zona alta de Ponferrada. En el año 2011, y con el aval del uso masivo con el que cuenta el primer ascensor, se procedió a inaugurar el segundo ascensor público de similares características al primero, y permite una mejor unión del Barrio de los Judíos con la zona alta y el parque del Plantío.

El Aeropuerto de León, que entró en servicio en el año 1999, es el aeropuerto más cercano, encontrándose a 102 kilómetros de Ponferrada. Según las estadísticas de Aena, en 2008 el aeropuerto movió 122.809 pasajeros, 5700 operaciones y 15,9 toneladas de carga.

Mantiene vuelos con Madrid, Barcelona, Valencia, Tenerife y París todo el año, que se refuerzan en temporada estival con enlaces a Palma de Mallorca, Málaga-Costa del Sol, Ibiza, Gran Canaria y Menorca.

Ponferrada es la cabeza del Partido judicial de Ponferrada. Es el número 4 de los siete y su demarcación comprende al municipio de Ponferrada y el resto de municipios de El Bierzo a excepción del municipio de Palacios del Sil. El conjunto de organismos judiciales es el siguiente:


Ponferrada cuenta con los siguientes centros educativos:

Creado, según palabras del rector Ángel Penas de la universidad, como una «"apuesta de futuro"», el Campus de Ponferrada dependiente de la Universidad de León comenzó a impartir docencia en el curso 1996/1997 después de años de reivindicaciones para que Ponferrada se dotara de instituciones universitarias (ver: UNED). Para ello, se reaprovechó el edificio del antiguo Hospital "Camino de Santiago", vacío después de su traslado a un edificio de nueva planta.

La actividad en el Campus ponferradino se inició con solo 149 alumnos; hoy, supera los 1500, tras años en que el campus ha ido aumentando su abanico de titulaciones, al tiempo que se han ido dotando de nuevas y más modernas infraestructuras; entre ellas, nuevos edificios para servicios, institutos de investigación, cafetería, biblioteca universitaria, piscina climatizada...

En la actualidad, el abanico de titulaciones impartidas en Ponferrada son las siguientes:

Son diversas las fiestas universitarias a lo largo del año pero las de mayor importancia son sin duda la fiesta del magosto y la de Agrícolas (18 de marzo), adquiriendo también cierta importancia las fiesta de San Isidro, de Forestales.

La sanidad ponferradina está gestionada por el ente autonómico Sacyl (Sanidad de Castilla y León).La Ley 1/1993, de 6 de abril, de Ordenación del Sistema Sanitario, divide la atención sanitaria en tres niveles de atención: primaria, especializada y continuada.

La atención primaria está gestionada desde el Área de Salud de El Bierzo. Ponferrada cuenta con 4 centros de salud:
Además de estos centros, en los últimos años se están construyendo consultorios médicos en algunas pedanías para acercar la atención médica a los pueblos. En estos centros un médico especialista perteneciente generalmente a un centro de salud acude unos días determinados a la semana para prestar consulta. Actualmente tienen consultorio médico los pueblos de San Esteban de Valdueza, Campo, Fuentesnuevas, Dehesas, San Lorenzo, Toral de Merayo, Villanueva de Valdueza y San Andrés de Montejos.

En cuanto a la atención especializada, en Fuentesnuevas está ubicado el Hospital Comarcal de El Bierzo, que da servicio a todas las poblaciones del Bierzo así como a la comarca vecina de Laciana. El centro cuenta con un total de 450 camas y en él se imparten numerosas especialidades. Actualmente en algunas consultas concretas presenta índices de saturación. Este hospital se inauguró en 1994 en sustitución del antiguo "Hospital Camino de Santiago" ubicado en la zona alta que debido al aumento de la población se había quedado pequeño para atender a toda la demanda.

Además de las numerosas consultas privadas de especialistas que hay distribuidas por la ciudad (clínicas dentales, oftalmológicas, laboratorios de análisis, consultas de dermatólogos, cardiólogos, podólogos, fisioterapeutas...) existen dos grandes centros sanitarios privados en Ponferrada:

Actualmente Ponferrada cuenta con una amplia red de farmacias para acercar a los ponferradinos la compra de medicamentos. La distribución de farmacias es la siguiente:

Por la noche, hay una farmacia de guardia en la ciudad de manera rotatoria, es decir, cada noche le toca a una farmacia de Ponferrada abrir en horario nocturno. Además de esta farmacia de guardia rotatoria, existen dos farmacias, una ubicada en el centro comercial "El Rosal" y otra en el centro de la ciudad, que están abiertas las 24 horas

Ponferrada cuenta con un gran centro comercial ("C.C. El Rosal"), inaugurado en octubre de 2007, que integra un hipermercado Carrefour, 147 tiendas (Zara y otras tiendas del grupo Inditex, Cortefiel, Mango, Desigual, C&A, H&M, Benetton o Worten, The Phone House, Arenal, Tiger, entre otras), más de 15 restaurantes y multicine de 7 salas.

Ubicado en la zona de la Avenida de los Escritores/Aldama y cercano al Museo del Ferrocarril, está compuesto por establecimientos de grandes superficies como Decathlon Easy, Star Center (antes Darty), Sprinter, Lidl, Mercadona o el restaurante Mc Donalds.

En esta ubicación se encontraban también los 'Cines la Dehesa', trasladados en 2007 al Centro Comercial El Rosal.

Otro centro importante de comercio, es el remodelado Mercado de Abastos destinado a la alimentación, con productos autóctonos como carnes, frutas y hortalizas además de pescado. Actualmente se pretende potenciar con la apertura de gastrobares en las zonas que están en desuso del mismo. Destacan 5 gastrobares: Deleites Gourmet, Er Pescaíto, La crepería, Rosquilla Berciana y SaboreArte. 

A esta oferta hay que añadir la numerosa oferta comercial que se concentra principalmente en el entorno de la Plaza de Lazúrtegui donde se encuentran grandes firmas como: Zara, Bershka, Pull & Bear, Oysho, Stradivarius, Massimo Dutti, Springfield o Cortefiel entre otras, así como la presencia del grupo El Corte Inglés con tiendas Sfera, Telecor o viajes El Corte Inglés

Con la apertura del C.C. El Rosal en 2007 se creó el Centro Comercial Abierto La Cebra, más tarde, en 2015 Templarium toma el relevo, ampliando la zona de acción al resto del núcleo urbano, es una asociación de comerciantes, hosteleros, servicios de ocio y profesionales de Ponferrada que han decidido aunar fuerzas para poder competir con las grandes superficies. Entre otras iniciativas, realizan una feria de rebajas al finalizar el período de rebajas de invierno para dar salida al stock de las tiendas a unos precios muy económicos.

La zona alta de la ciudad podría decirse que se ha especializado más como zona de ronda de vinos y bares, concretamente los que están en el entorno de la plaza del Ayuntamiento o la plaza de la Encina.

La Virgen de la Encina es la patrona de Ponferrada, de El Bierzo. El Consejo Comarcal de El Bierzo pretende convertirla en festividad laboral en El Bierzo.

El día de la Encina es el 8 de septiembre, el 9 se celebra la Encinina. Los dos días festivos locales establecidos por decreto de la alcaldía. Trasladándose a fecha diferente si coincidiera en sábado o domingo. También se realiza una fiesta juvenil e infantil del 1 al 6 de septiembre llamada "Cima" en el Parque del temple, en el cual se instalan hinchables y un tren que recorre el parque así como talleres de manualidades y de aventura, representación de obras de teatro y actuaciones de baile etc... para los niños.

Las fiestas tienen lugar en los primeros días de septiembre, entre los días 5 y 9, según programa establecido por el Patronato de Fiestas, presidido por el Concejal de Cultura.
Según la tradición, la talla de la Virgen fue localizada en el interior de una encina por los Templarios en la Edad Media, donde habría sido depositada para protegerla de los musulmanes. Como curiosidad, se trata de una de las pocas vírgenes negras que existen, por lo que también es conocida popularmente como "La Morenica".

En estas fiestas se realiza una feria de Cerámica en el patio del CEIP Campo de la Cruz que cuenta con la representación de numerosos artistas y en la que también se imparten cursos sobre el manejo de la cerámica; exposiciones de pintores locales ubicadas en el casco antiguo y numerosas degustaciones gastronómicas ya sean de carácter local o nacional.

Otras actividades realizadas en estas fiestas son el Festival de la canción berciana, en el cual participan grupos corales de toda la comarca del Bierzo, así como diferentes competiciones deportivas y conciertos en el Auditorio Municipal, en el denominado ciclo de "Noches de la Encina", que ha traído a importantes artistas de nivel nacional. El día 8 de septiembre por la tarde se celebra el ya tradicional desfile de carrozas que cierra la reina de las fiestas con sus damas de honor. En 2012 a causa de la crisis económica se ha suspendido la elección de esta

En la explanada situada entre la Avenida de la Libertad y la Avenida de la Lealtad se instalan carpas donde se realizan degustaciones gastronómicas, y también es en ese lugar donde se instalan las orquestas que amenizan las noches de verbena de las fiestas.

Magostos de mayor o menor importancia y afluencia de público se celebran en muchos barrios de la ciudad, donde realmente se muestra el carácter hospitalario y acogedor de sus habitantes.

Consiste en una tarde/noche en la que se comen castañas asadas y otras viandas, como el "bollo preñao". Suelen estar animados por bandas o grupos de música. Un magosto muy popular es el del Campus universitario en el cual se concentran cientos de jóvenes alrededor de las hogueras.

También estos magostos se suelen celebrar en los colegios de Ponferrada, organizados por las asociaciones de padres, madres y alumnos de los respectivos colegios, o por las asociaciones vecinales de cada barrio.

Celebrada la primera luna llena del mes de julio, la Noche Templaria recrea un acontecimiento medieval lleno de fantasía.

Se trata de la representación de como Frey Guido de Garda, Maestre de la Orden de los Caballeros Templarios, vuelve a la ciudad del Puente de Hierro para sellar con ella un pacto de eterna amistad y entregarle la custodia de los símbolos hallados en la tierra sagrada de Jerusalén: la sagrada Arca de la Alianza y el Santo Grial.
La comitiva Templaria es recibida por miles de ponferradinos ataviados con ropajes medievales que, en desfile y custodiando el Arca de la Alianza y el Santo Grial, se dirigen hacia el Castillo.

Llegados hasta aquí se realiza un Juicio a la Orden Templaria. “Yo, Guido de Garda, Maestre de la fortaleza de Pons Ferrata, comprometo a todo el pueblo de Ponferrada para que vuelva cada año a renovar este compromiso festivo con su historia y su leyenda hasta que el tiempo llegue a borrar la línea del horizonte.

La Noche Templaria es una de las fiestas más animadas del verano ponferradino, todo ello amenizado con música, fuegos artificiales, degustaciones gastronómicas (consolidándose con gran popularidad la cena medieval) o animaciones de calles entre otras cosas, además de un gran desfile templario.

En Ponferrada también se celebra al igual que en toda España la Semana Santa. Es uno de los principales acontecimientos religiosos de la ciudad a lo larga del año, y la Semana Santa Ponferradina está declarada de Interés Turístico Regional, y desde 2015 también de Interés turístico Nacional, solicitada en 2012.
Cuenta con cuatro hermandades:
Se realizan diferentes actos procesionales en los días de la Semana Santa comprendidos entre el Viernes de Dolores y el Domingo de Resurrección
Además de las procesiones, otro personaje típico de la semana Santa ponferradina es el Nazareno Lambrión chupacandiles, que sale por las calles de Ponferrada el día antes del pregón de Semana Santa anunciando la llegada de esta.

Otras peculiaridades de la semana Santa en Ponferrada son la custodia de la llave de la basílica de la Encina por parte del Alcalde de Ponferrada el Jueves y Viernes Santo o la llamada ritual de timbales y clarinetes en la madrugada del Viernes Santo.

Gala que anualmente se celebra en Ponferrada un sábado de abril o mayo en la "Federación de Asociaciones de la Radio y la Televisión" entrega a diferentes personalidades del mundo del periodismo, política, cine, danza, tauromaquia, teatro, deporte o moda. Hasta 2012 se han realizado diez galas, siendo hasta 2010 celebrados en el "Pabellón del Toralín", donde se realizaba una gran cena-entrega de premios con más de 2000 invitados, pasándose a hacer desde la edición de 2011 la entrega de premios en el "Teatro Bérgidum" y una posterior cena en el Castillo de los Templarios. Como anécdota, los invitados llegan a la alfombra roja donde son recibidos por las autoridades en coches clásicos, realizando un recorrido por la ciudad.

Es patrocinada y organizado por el locutor de radio Luis del Olmo. A este acto han acudido diferentes personalidades como Vicente del Bosque, Antonio Banderas, Carmen Cervera o Rocío Jurado.

Conjuntamente a la celebración de la gala, durante esa semana se celebra la "semana de la radio", en la que se emiten programas radiofónicos nacionales desde Ponferrada y se realizan diferentes foros de debate sobre la situación y futuro de la radio española, entre otras actividades.

Se celebra del domingo al martes anterior al Miércoles de Ceniza. El domingo por la tarde se celebra el "Carnaval infantil", un desfile que se realiza por el centro de la ciudad y finaliza en el parque de Flores del Sil organizado por las AMPAS de los colegios de Ponferrada, disfrazándose cada colegio de un tema diferente. El lunes es el "Carnaval de los jóvenes", en el que se organiza por Cima un rompecabezas en el que los participantes integrados en grupos disfrazados tienen que buscar pistas distribuidas por toda la ciudad. El martes se realiza el tradicional desfile de carnaval, en el que participan tanto peñas como grupos, que optan por un premio económico por categorías. Este desfile comienza en Flores del Sil, atraviesa el centro de la ciudad y llega a la zona alta finalizando en la Plaza del Ayuntamiento, donde se realiza un baile popular a cargo de una orquesta.

El día 18 de julio se festeja el Corpus Christi, con solemne misa en la Ermita del Cristo (situada en el Camino de Santiago), en esta fiesta se suele merendar con ternera que se vende en el Bar del "Campo de la fiesta".

El 15 de agosto se celebra el Festival de la sardina. Donde se puede degustar una buena tapa de sardinas, acompañada de buen vino y amenizada con una orquesta al más puro estilo de una verbena. En honor de Nuestra Señora y San Roque. Hoy en día las nuevas actividades festivas han llevado a que los actos más concurridos sean La Procesión de la Luz (en plena noche y con alumbrado de velas) y la ronda de Bodegas (un acto popular de visitar la Calle Real, donde los vecinos sacan sus bodegas a la misma calle ofreciendo vino y comida gratis a todos los asistentes. Se ha puesto de moda entre los jóvenes del pueblo realizar, entre el grupo de "colegas", graciosas camisetas para animar el acto. Se espera que en años venideros la comisión de fiestas dedique un pequeño premio económico a las 3 mejores). En este barrio ponferradino se sitúan el hospital comarcal del Bierzo, el más grande de la comarca, un centro de asprona, un instituto, un colegio de educación especial y otro público -"la cogolla"- y un edificio del Proyecto Hombre.

Está comunicado con Ponferrada por las líneas , , , , , , y del TUP

Situado al Noroeste de Ponferrada, el barrio está dividido por el Canal Bajo del Bierzo, y dista 2,5 km del centro de la ciudad.

La creación del barrio surge por la llegada de obreros para trabajar en la construcción de la Central Térmica de Endesa de Compostilla I (hoy, ya desmantelada, sede del CIUDEN y futura sede del museo de la Energía) y también para trabajar en ella. Dicho barrio recibe la definición de barrio residencial debido a que la mayoría de sus edificaciones son unifamiliares.

Cuenta con instalaciones deportivas privadas, canchas de tenis, campos de baloncesto, campo de fútbol (usado, habitualmente, por la S.D.Ponferradina B) y canchas de pádel. También tiene unas piscinas de verano.

Este barrio está sufriendo cambios con la construcción en su entorno del nuevo Parque de Bomberos. Cuenta también con la sede de la Cruz Roja de la ciudad y el nuevo centro de Formación Profesional contiguo al Parque de Bomberos.

Podemos destacar la gran obra para la ciudad de Ponferrada que fue la retirada de la Montaña de Carbón que separaba el centro de la ciudad de dicho barrio. En este espacio está prevista la construcción de un proyecto para la juventud, que constará de un parque, zonas culturales y viviendas para jóvenes, así como el Museo de la Energía.

Está comunicado con Ponferrada por las líneas , y del TUP

Fiestas del barrio:


Este conocido barrio de Ponferrada al principio no era más que una hilera de casas bordeando la primera nacional VI que se construyó, allá por los años cincuenta. Los terrenos adyacentes, que básicamente eran zarzales o vegetación libre con alguna finca de manzanos, poco a poco fueron comprados por emigrantes, principalmente procedentes de Galicia pero también de muchos otros pueblos de El Bierzo, que en muchos casos se construyeron sus casas ellos mismos. Actualmente cuenta con un centro de salud el "Ponferrada IV" que posee una plantilla de seis médicos de familia, dos fisioterapeutas, un odontólogo, una matrona de área, un pediatra, un higienista dental y dos auxiliares de enfermería y servicios como el de Fisioterapia. También se encuentran en este barrio el colegio público Jesús Maestro y el centro concertado San José Obrero, además de un centro cívico y el parque Pablo Picasso.

Está comunicado con Ponferrada por las líneas , , , (Por el Canal), , y del TUP

En principio se llamaba San Dionisio. El nombre actual se debe a la ubicación de la placa donde giraban las locomotoras del ferrocarril, ya que en este barrio era donde estaban ubicados los talleres de RENFE en Ponferrada. Actualmente es un barrio en expansión. Se ubicará en un futuro la estación del AVE de Ponferrada, además de trasladarse aquí la estación de autobuses para configurar un enclave intermediario entre los diversos medios de transporte. Cuenta con su propio colegio, el CP Virgen del Carmen, además de con unas piscinas privadas y una iglesia, entre otros servicios.

La Placa está comunicada con Ponferrada por las líneas , y del TUP

Es una barrio de reciente creación, que se extiende desde los límites que marca la vía del ferrocarril, al este y norte; el río Sil al sur y la pedanía de La Martina al oeste. Los primeros pobladores se instalaron en el barrio en los años 20. Hasta entonces, los terrenos donde se ubica, eran en su mayoría campos de cultivo especialmente de cereal, pertenecientes a los vecinos del cercano pueblo de Toral de Merayo. El barrio fue surgiendo en torno a la carretera Nacional 120, Logroño-Vigo, llamada en Ponferrada, carretera de Orense.
La fiesta se celebra siempre el segundo domingo de mayo en honor a Jesús Divino Obrero. El obispo de Astorga eligió este patrono, por consejo de la gente, en el año 1936 ya que la mayor parte de la población era obrera. El primer templo del barrio, sin embargo, está dedicada a Santiago Apóstol.
En 2006 la población de este populoso barrio de Ponferrada era de 9072 habitantes.

Actualmente cuenta con el centro de Salud Ponferrada III y el IES Europa, ubicados en la zona del Temple, que se encuentra situada al este del barrio; delimitada al norte y al este por las vías del ferrocarril de la línea Palencia - La Coruña, al sur por el río Sil y al oeste por la "Avenida de La Cemba", se considera como un barrio dentro de Flores del Sil, donde están establecidos los servicios citados anteriormente además de una residencia de ancianos en el antiguo campo de la Minero, gestionada por la Junta de Castilla y León, un centro de día para enfermos de Alzheimer,y el parque homónimo, uno de los más grandes de la ciudad y donde entre otras actividades, se realiza en las fiestas de la Encina la denominada "Ciudad Mágica" CIMA, consistente en atracciones hinchables, juegos y talleres para los niños, o zonas dedicadas para practicar la petanca.

Es digno de nombrar que el otro parque grande de Flores del Sil fue adquirido en la década de los 70 por los propios vecinos, en "cuestación" popular. Para ello compraron parte de los terrenos de la finca del Trinitario.
Se ha inaugurado recientemente un centro cívico ubicado en la avenida de la Martina. Como centros educativos cuenta con el colegio Ponferrada XII que nace de la fusión de los 2 anteriores colegios existentes y el colegio bilingüe concertado La Asunción, perteneciente a las religiosas de la Asunción.

El barrio está conectado a Ponferrada mediante las líneas L1, , , , y del TUP

Su nombre es debido al puente medieval (cerca de él existen restos de uno anterior de construcción romana) que cruza el Río Boeza a su paso por este barrio.

Recientemente el puente medieval, que hasta bien poco aún daba servicio a la circulación de vehículos de motor, paso a ser peatonal al construirse a corta distancia otro puente de nueva fábrica.

Este barrio dividido en dos por el río Boeza, da paso a la parte sur del municipio de Ponferrada y a la comarca tradicional del Valle del Oza o Valdueza, así como la salida hacía los Montes Aquilanos, El Morredero y la parte más oriental de La Cabrera.

De él parte la carretera de Sanabria, proyectada en 1919 para comunicar El Bierzo con La Cabrera (Ponferrada-Santalavilla-Puebla de Sanabria) y que quedó sin terminar en lo alto del pico de la Aquiana en el llamado Campo de las Danzas.

Está conectado a Ponferrada por las líneas del TUP , , y 

En el barrio se ubica la cooperativa "Cepas del Bierzo" y cuenta con un gran prado en el que está delimitado un campo polivalente de Fútbol y Rugby. En la zona norte hay un polideportivo, conocido popularmente como "JT", y cuya denominación oficial es "Polideportivo Antonio Vecino".

Los eventos festivos de este barrio son:

Barrio ligado a la antigua carretera a Orense, situado entre el barrio de Flores del Sil y Dehesas. Dentro de sus límites se encuentran numerosas naves industriales. Las casas que lo forman suelen ser unifamiliares o de pocas alturas.

Está unido a Ponferrada por las líneas L1 y del TUP.

Nuevo barrio en constante crecimiento de la ciudad de Ponferrada cuenta con el edificio más alto de Castilla y León. La torre de la rosaleda "con cerca de 28 pisos", un centro comercial "El Rosal", un conservatorio y un barrio comercial con el bulevar Juan Carlos I como base. Se va a construir un centro de salud y un nuevo edificio para la junta de Castilla y León de 14 plantas. En él está ubicado el Parque del Oeste, que es el parque más grande de Ponferrada y que se prolongará con la creación del parque de la juventud, creando un gran pulmón verde para Ponferrada.
También se ha construido una nueva iglesia de diseño, la parroquia del buen Pastor, en el Bulevar Juan Carlos I, y próximamente se edificará una nueva residencia privada de mayores en el barrio.

Por el centro del barrio pasa la línea del TUP. Además, por los diferentes extremos de La Rosaleda se pueden coger otras líneas como la , , , , , , y en la avenida Galicia y la en la avenida de Asturias.

La Sociedad Deportiva Ponferradina fue fundada el 7 de junio de 1922. Milita actualmente en el Campeonato Nacional de Liga de Segunda División y disputa sus partidos como local en el Estadio El Toralín, de titularidad municipal, con aforo de 8.800 espectadores e inaugurado en el 2000.

La Deportiva ha disputado seis temporadas en Segunda División, a la que ha ascendido en cuatro ocasiones: en en el Rico Pérez ante el Alicante, en en Ponferrada ante el Sant Andreu de Barcelona, en en el Heliodoro ante el Tenerife y en en Ponferrada ante el Hércules de Alicante.

La Ponfe ha participado en cuarenta ediciones de la competición nacional de copa (Copa del Rey desde 1976), enfrentándose a históricos del fútbol nacional como el Zaragoza en 1990, el Sevilla en 2008 o el Real Madrid en los dieciseisavos de la edición 2011/12, con la presencia en la ida de la eliminatoria disputada en Ponferrada, de jugadores del cuadro de José Mourinho como Sergio Ramos o Cristiano Ronaldo, autor del segundo tanto madridista (0-2).

El Club Baloncesto Ciudad de Ponferrada fue fundado el 22 de junio de 2002. Milita actualmente en LEB Plata, tercera categoría del baloncesto nacional y disputa sus partidos como local en el «Pabellón Lydia Valentín», de titularidad municipal, con aforo de 3.500 espectadores e inaugurado en 2001.

El Ciudad de Ponferrada, es el club sucesor del histórico Club Jóvenes Trabajadores, J.T. Ponferrada (1975–2000), que disputaba sus encuentros en el «Pabellón de la Borreca», de titularidad municipal, con aforo de 700 espectadores e inaugurado en 1980.

La ciudad fue la sede del Campeonato Mundial de Ciclismo en Ruta en su edición de 2014. El Mundial, que se celebró entre los días 20 y 28 de septiembre de 2014, es el mayor hito deportivo de la historia de la comarca por su proyección nacional e internacional, y por la repercusión no sólo en imagen sino económica y social para la ciudad y El Bierzo, ya que movió miles de visitantes durante el tiempo que duró el evento.

El Mundial en cifras de la UCI: 350.000 espectadores "in situ" a lo largo de la semana de competición, 1.500 deportistas, 70 naciones, 6.000 acreditaciones oficiales, 73 cadenas de televisión, 400 horas de emisiones de televisión, 216 horas de emisión en vivo, una audiencia televisiva acumulada de 300 a 400 millones de espectadores.

La organización del mundial supuso preparar la ciudad para acoger el evento, con inversiones de las diferentes administraciones. Hasta ahora en España el Campeonato Mundial de Ciclismo en Ruta se había disputado en Madrid, Barcelona, San Sebastián y Benidorm.

La ciudad de Ponferrada participa en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. A partir de esta iniciativa se han establecido lazos con la siguiente localidad:

El elemento central del escudo de Ponferrada es un puente de oro, mazonado de sable (negro). El puente se encuentra situado sobre ondas de azur y plata. En los adornos exteriores figuran un pergamino heráldico y una corona real antigua o abierta.

Popularmente se ha aceptado la canción tradicional ""A Ponferrada me voy"" como himno de Ponferrada, siendo una de las canciones más conocidas y populares. En 1950 se dio a conocer el himno de Ponferrada compuesto por Pedro Fernández Matachana: ""En el Bierzo, jardín del amor, floreció esta hermosa ciudad, Ponferrada, hidalga y señorial..."".









</doc>
<doc id="2400" url="https://es.wikipedia.org/wiki?curid=2400" title="Punk">
Punk

El punk, también llamado punk rock, es un género musical que emergió a mediados de los años 1970. Este género se caracteriza en la industria musical por su actitud independiente y contracultural. En sus inicios, el punk era una música de estructura de composición minimalista, muy simple y cruda, a veces descuidada: un tipo de "rock" sencillo, con melodías agresivas de duraciones cortas, sonidos de guitarras amplificadas poco controlados y ruidosos cargados de mucha distorsión, pocos arreglos e instrumentos y, por lo general, de compases y tempos rápidos.

Las líneas de guitarra se caracterizan por su sencillez y la crudeza del sonido amplificado y muy distorsionado, generalmente creando un ambiente sonoro ruidoso o agresivo heredado del "garage rock". El bajo, en ocasiones, sigue solo la línea del acorde y no busca adornar con octavas ni arreglos la melodía pero, por lo general, en las primeras formaciones de punk (cosa que se repetiría más adelante con las bandas de post-punk) el bajo presenta arreglos sencillos pero constantes, luciendo, en muchos de los casos, más que la guitarra. La batería por su parte lleva un tempo acelerado, con ritmos sencillos de "rock and roll". Las voces varían desde expresiones fuertes e incluso violentas o desgarradas, mientras las letras se caracterizan por tratar temas como los problemas políticos y sociales (llevando con esto un mensaje de conciencia que se extiende, tratando de denunciar por medio de la música este tipo de problemas e incluso se llegan a tratar temas como el no a las drogas, amor, pacifismo, etc.). Aunque desde finales de los años 1970 e incluso antes (a excepción de la lírica) estos aspectos fueron cambiando gracias a que las bandas que se formaron en ese tiempo (por general bandas del estilo hardcore y otras bandas más tradicionalistas) fueron agregando cambios de acorde muy rápidos en la guitarra, arreglos (para mayor complejidad) y solos de guitarra (heredados directamente del "rock and roll" y "rockabilly"), en el bajo se introdujeron los arreglos con octava, y la batería aceleraba más el ritmo y el tempo, las voces eran muchas fuertes. 
El "punk rock" explotó en la corriente musical a finales de los años 1970 con bandas británicas como Sex Pistols, The Clash y The Damned, y bandas estadounidenses como Ramones, The Dead Boys y Blondie. Más adelante en la década de los años 1980, dentro de la escena independiente surgirían dos movimientos dentro del "punk rock" que se caracterizarían por seguir las ideas de la ética hazlo tú mismo, y con una fórmula musical mucho más agresiva, rápida, y veloz; en Estados Unidos el "hardcore" (con bandas como Black Flag, Minor Threat, Dead Kennedys, Circle Jerks, Bad Brains, etc, quienes ayudaron e influenciaron a músicos que más tarde conformarían la escena "hardcore" melódica, "skate punk", "nardcore" e incluso "straight edge/youth crew") y en el Reino Unido el UK '82 (representado por The Exploited, GBH, Chaos UK, Discharge, Varukers, estos últimos dos dieron continuación e influencia a formas más extremas del punk como "d-beat", "crust punk" y "grindcore"). Muchos músicos influenciados por el "punk rock" (pero con distintos intereses musicales) más tarde formarían bandas de post-punk, "heavy metal", "grunge", emo, "noise rock", ska, "new wave", "rock" alternativo y gótico. En los años 1990, aparece el "grunge" con bandas como Nirvana; más tarde bandas californianas de "skate punk" y "hardcore" melódico seguirían vendiendo millones de discos como Offspring, Rancid y NOFX. A mediados de los años 1990 el mercado musical promovería a bandas de "pop punk" como Green Day o Blink-182; asimismo, bandas de "punk hardcore" y "street punk" siguieron formándose como The Casualties, Unseen, Blanks 77, Agnostic Front, Violent Society, Restarts, First Step, Battery, Good Clean Fun, entre muchas otras.

Brotó gracias al Movimiento Punk, cuya intención por lo general es la protesta, suelen hacerlo presencialmente, así como también, utilizando como medio a la música, convirtiendo la suya en música protestante, manifestante o demandante. También es muy característico por gran parte de ellos su rechazo hacia el sexismo y el racismo, así como también hacia la prensa amarillista por ser esta sensacionalista, direccionalista, farandulera, manipuladora, opresora, constringente, lobbista, amarillista, conservadora, reservada, etc. Ya sea que cuente alguna, con una, varias, o todas esas características. La forma originaria del punk era un tipo de rock sencillo y algo ruidoso para expresarse con sus propios medios y conceptos. Entre las primeras bandas musicales representantes del punk están The Ramones, Dead Boys, The Voidoids, Blondie, The Heartbreakers, Tuff Darts, The Cramps, The Misfits, Black Flag, The Gun Club, The Bags, Cherry Vanilla y Wayne County & the Electric Chairs y en el proto-punk The New York Dolls, The Velvet Underground (la banda de Lou Reed), Iggy Pop y The Stooges, MC5, la cantante Patti Smith, Television y The Dictators entre otros, todo esto en Estados Unidos.

En el Reino Unido proliferarían agrupaciones como Sex Pistols, The Clash, The Damned, Buzzcocks, The Adicts, The Vibrators, Generation X, The Pretenders, The Jam, The Stranglers, U.K. Subs, Sham 69 y Adam & the Ants, además de David Bowie, Marc Bolan de T-Rex y The Who entre otros, también bandas y artistas glam fueron quienes influenciaron al punk igual que paralelamente lo hicieron las bandas "proto-punk" de garage rock de EE.UU.

Ramones se presentaban a sí mismos como banda de rock, sin pretensiones declaradas de mensaje directamente innovador o rompedor salvo en lo musical. Después surgiría la etiqueta punk y Ramones serían considerados los primeros en representarla. Por otro lado había una forma de transgresión, buscando liberarse de los estigmas sociales. Esta rama no daba explicaciones y buscaba incomodar a lo establecido chocando, ofendiendo y molestando al buen gusto, la moral y la tradición. Se buscaba básicamente la provocación a través de demostraciones de transgresión estética o giros de lenguaje contradictorios, absurdos o insolentes. Es el estilo que popularizaron Sex Pistols, relacionado ligeramente al nihilismo y otras formas de escepticismo. 

Más adelante, especialmente con la aparición del hardcore punk y marcado por la herencia de la actitud del colectivo Crass, se hizo presente todo un abanico de enfoques de crítica social, posicionamientos políticos y afinidad a campañas de protesta. El ejemplo musical más clásico son Crass y The Clash. La filosofía punk puede resumirse en: "Hazlo tú mismo" o "hazlo a tu manera". Rechaza los dogmas y cuestiona lo establecido. Desprecia las modas y la sociedad de masas (aunque su estética también puede considerarse una moda preestablecida por el punk). En sus canciones, estas bandas expresan un serio descontento con los sistemas e instituciones que organizan y controlan el mundo. En ocasiones también la música sirve de plataforma para propuestas filosóficas e ideológicas.

Durante la década de 1980, el punk en los Estados Unidos estuvo permeado de contenidos políticos, principalmente progresistas, en oposición al gobierno conservador de la época. Ejemplos de esta época son las bandas Dead Kennedys y Bad Religion. En Europa, el punk es una música especialmente utilizada como medio de difusión por gente afín a movimientos políticos y sociales "outsiders", mayoritariamente de izquierda, aunque existe una corriente de derecha que lo utiliza, siendo criticada y denostada por la mayoría de los movimientos punk.

A principios de los años 1990, las publicaciones amateur del punk y las canciones de los grupos también sirvieron como vehículo de los planteamientos y denuncias de los movimientos antiglobalización.

A comienzos y mediados de los años 1960, las bandas de garage rock, que son reconocidas como las progenitoras del punk rock, comenzaron a aparecer en diferentes lugares de América del Norte. The Kingsmen, una banda de "garage rock" de Portland, Oregon, tuvo un éxito con su versión de "Louie, Louie" de 1963, citada como "el "urtext" que define al punk rock". El sonido minimalista de muchas bandas de "garage rock" fue influenciado por el ala más dura de la invasión británica. Los éxitos de The Kinks de 1964, "You Really Got Me" y "All Day and All of the Night", se han descrito como "precursores de todo el género de tres acordes de Ramones; por ejemplo, "I Don't Want You" de 1978, fue puro Kinks". 

En 1964, The Who rápidamente progresó desde su sencillo debut, "I Can't Explain" a "My Generation". A pesar de que tuvo poco impacto en las listas estadounidenses, el himno de The Who presagiaba una mezcla más cerebral de ferocidad musical y postura rebelde que caracterizó a gran parte del primer punk rock británico: John Reed describe la aparición de The Clash como una "ajustada bola de energía que recuerda tanto en imagen como en retórica a un joven Pete Townshend con obsesión por la velocidad, ropa pop-art y ambición de escuela de arte". The Kinks, The Who y The Small Faces fueron algunos de los pocos viejos roqueros conocidos por los Sex Pistols. 

En 1966, el "mod" ya estaba en declive. Las bandas de "garage rock" de Estados Unidos comenzaron a perder fuerza en un par de años, pero el sonido crudo y la actitud desconocida de bandas de rock psicodélico como The Seeds presagiaba el estilo de bandas que se conocerían como las figuras arquetípicas del proto-punk.

Uno de los antecedentes del estilo punk, y para algunos la primera muestra del género, ocurrió en la década de los años 1960 en Perú con la banda Los Saicos formada por jóvenes de 19 años. Aunque la banda ha indicado que ellos solo tocaban rock and roll, aceptan ser llamados los precursores del punk.

En 1969, aparecieron los álbumes debut de dos bandas de Míchigan que son conocidos comúnmente como las grabaciones principales del proto-punk. En enero, MC5 de Detroit lanzó "Kick Out the Jams". "Musicalmente el grupo es intencionalmente crudo y agresivo", escribió el crítico Lester Bangs en "Rolling Stone":

En agosto, The Stooges de Ann Arbor, lanzaron su álbum homónimo. De acuerdo con el crítico Greil Marcus, la banda, liderada por el cantante Iggy Pop, creó "el sonido del "Airmobile" de Chuck Berry después de que los ladrones lo desarmaran". El álbum fue producido por John Cale, un exmiembro del grupo de rock experimental The Velvet Underground. Habiendo ganado una "reputación como la primera banda de rock under", The Velvet Underground inspiró, directa o indirectamente, a muchos de los músicos que estuvieron involucrados en la creación del punk rock y el heavy rock.

Son también etiquetados como proto-punk las bandas New York Dolls, Dictators o incluso The Who; además de temas de hard rock como "Come and Get It" de Blue Cheer, "Comunication Breakdown" de Led Zeppelin, "Paranoid" de Black Sabbath o "I´m Eighteen" de Alice Cooper.

A finales de los años 1960, una corriente de jóvenes del Reino Unido, Estados Unidos y otros países consideraban que el rock había pasado de ser un medio de expresión para los jóvenes, a una mera herramienta de mercado y escaparate para la grandilocuencia de los músicos de ese entonces, alejando la música de la gente común. El punk surgió como una burla a la rigidez de los convencionalismos que ocultaban formas de opresión social y cultural.

Las características del punk rock fueron precedidas por el garage rock (por ejemplo, "Pushin Too Hard" de 1966 de los californianos The Seeds), recrudeciendo más el sonido fuerte del rock y con composiciones menos profesionales, influenciados por el sonido de la invasión británica, como The Beatles, The Kinks o The Who, y cogiendo elementos del sonido ruidoso del "garage rock" de The Stooges o The Velvet Underground. También se recogen influencias del frenético surf rock. Estas variadas influencias se conocen ahora como proto-punk. Dentro de las mismas se puede incluir también entre las influencias tempranas a Bobby Fuller Four autor del sencillo "I Fought the Law", que fuera versionado por The Clash, y a Modern Lovers autores de "Roadrunner" dados sus ritmos acelerados, no tan cercanos al punk rock posterior pero si alejado del rock convencional y típico de la época, con acordes simples pero aún sin distorsión o volumen alto. The Dictators también fueron una banda crucial para el surgimiento del punk rock, empezaron como "garage rock" que posteriormente sería llamado "proto-punk" para luego emerger en el mismo punk rock que ellos mismos habían ayudado a formar. Dead Boys fue otra banda de punk estadounidense formada en 1976 y debutando en el mítico club CBGB.

The Ramones hicieron composiciones sencillas, cuyas ácidas letras trataban temas como la discriminación de otros jóvenes, la anti-moda y las drogas, una pauta para las bandas por venir.

La visita de estos a Londres (actuaron en el mítico "Roundhouse" el 4 de julio, día de independencia estadounidense, paradójicamente) hizo que grupos ya existentes, como Sex Pistols, comenzaran a usar sus instrumentos como medios de expresión y provocación para mostrar su descontento hacia lo que consideraban una sociedad de mentalidad estrecha y represora.
The Damned es otra de las emblemáticas primeras bandas de punk rock de Inglaterra. A diferencia de The Clash, sus acordes de guitarra eran más trabajados, siendo considerados precursores del hardcore punk que se caracteriza precisamente, por tener canciones de punk con acordes más trabajados como los del thrash metal por ejemplo; su vocalista y líder Dave Vanian llevaba una estética vampírica que luego influyó a la estética de la subcultura gótica y a las bandas de rock gótico.

Blondie fue otra banda que sobresalió en la primera oleada del punk rock estadounidense. Empezaron sus conciertos en el CBGB y se iniciaron junto a otras bandas como Ramones y Television. Fueron de crucial importancia para lo que posteriormente sería el new wave, derivado del punk rock. 

Con el tiempo, el género tomaría diferentes caminos y en su paso evolucionaría en muchos subgéneros y recogería influencias de otros estilos musicales. Los subgéneros del punk se definen a veces por características musicales, y en otros casos por el contenido del mensaje o la ideología que lo inspira.

Tal como después sucedería en muchos otros países, en el Reino Unido pronto los grupos tomaron influencias de otros géneros. Una de las primeras fusiones del punk fue con el reggae y el ska de los emigrantes jamaicanos en el país. Como primer y más representativo ejemplo, se puede mencionar a la banda The Clash y sus temas "Police and Thieves" (una versión del jamaicano Junior Murvin), "The Guns of Brixton" y "(White Man) In Hammersmith Palais". 

Otra fusión en el punk fue con otro estilo negro, el funk, el que se puede oír en trabajos como "The Idiot" de Iggy Pop, y bandas como James Chance & The Contortions, Ian Dury and The Blockheads, Gang Of Four, A Certain Ratio, como los mismos The Clash en su "Magnificient Seven".

En los inicios también surgió el estilo hardcore punk, que se caracterizó por ser una versión más rápida de la forma sucia de tocar el punk. The Damned fue la principal banda en influenciar a esta nueva rama del punk, seguidos por otras bandas como The Misfits, Bad Brains, Middle Class y Black Flag. Otra banda temprana en este estilo fueron Teen Idles.

El término inglés "punk" tiene un significado despectivo que suele variar, aplicándose a objetos (significando "basura", "suciedad") o a personas (significando "vago", "despreciable", "sucio" o, también, "basura" y "escoria"). Se utiliza de forma irónica como descripción del sustrato crítico o descontento que contiene esta música. Al utilizarlo como etiqueta propia, los "punkies" (o "punks") se desmarcan de la adecuación a los roles y estereotipos sociales. Debido al carácter de este significado, el punk a menudo se ha asociado a actitudes de descuido personal, se ha utilizado como medio de expresión de sentimientos de malestar y odio, y también ha dado cabida a comportamientos neuróticos o autodestructivos.

El término "punk" se utilizó como título de una revista fundada en 1976 en Nueva York por John Holmstrom, Ged Dunn y Legs McNeil que deseaban una revista que hablara de todo lo que les gustaba: las reposiciones por televisión, beber cerveza, el sexo, las hamburguesas con queso, los cómics, las películas de serie B, y el rock n' roll que sonaba en los garitos más mugrientos de la ciudad: The Velvet Underground, The Stooges y New York Dolls, entre otros.

Más tarde el significado también serviría para inspirar las corrientes izquierdistas del género, como etiqueta que deshace la condición de clase o rol social con deudas de reputación o apariencia.

Como movimiento creativo, el sonido punk dio aparición a numerosas vertientes. Muchos de los grupos se movían de uno a otro género, existiendo diferentes niveles de permeación, evolución y fusión, pudiendo hablarse de bandas que encajan en el perfil de más de dos subgéneros. Estos estilos individuales fueron popularizándose hasta formarse subgéneros musicales, categorías de obras y artistas que compartían un rasgo definido común.

Entre algunos de sus subgéneros se pueden identificar los siguientes:







</doc>
<doc id="2403" url="https://es.wikipedia.org/wiki?curid=2403" title="Provincias marítimas de Canadá">
Provincias marítimas de Canadá

Las llamadas Provincias Marítimas son una región de Canadá en la costa del océano Atlántico. Su economía tradicional tiene que ver con recursos naturales como la pesca, la agricultura y la minería.

Las provincias que componen esta región son:


</doc>
<doc id="2404" url="https://es.wikipedia.org/wiki?curid=2404" title="Palacio de los Guzmanes">
Palacio de los Guzmanes

El palacio de los Guzmanes es un palacio renacentista del situado en la plaza de San Marcelo junto a la Casa Botines en la ciudad de León, España. Su traza se debe al maestro Rodrigo Gil de Hontañón, si bien se ocupó de su ejecución Juan de Ribero Rada. A pesar de quedar inconcluso se convirtió en el palacio más destacado de la ciudad. Ya en el , la Diputación Provincial de León se hizo cargo de terminarlo para albergar sus nuevas oficinas, función que desempeña hasta la actualidad.

Fue declarado Monumento histórico (BIC) en 1963.

Fue mandado construir por D. Ramiro Núñez de Guzmán, antiguo líder comunero, sobre los solares que ocupaban las casas señoriales de su linaje. La familia de los Guzmanes era uno de los linajes leoneses más antiguos. A principios del eran señores de Guzmán, Aviados, Toral y Valle de Boñar. Asentados en la ciudad eran una de las casas nobles más influyentes y con mayor protagonismo en León donde estaban introducidos en los órganos de poder local.

D. Ramiro quiso aprovechar la situación de sus antiguas casas en una de las zonas principales de la ciudad para edificar un palacio de nueva planta que destacase y se diferenciase del entorno urbano por sus dimensiones y por adoptar la tipología y estética de la arquitectura “a lo romano” o renacentista. Para ello encargó el diseño de su traza a uno de los maestros más prestigiosos de Castilla en aquella época, Rodrigo Gil de Hontañón. Este diseñó un palacio rectangular con patio central, exento en sus cuatro costados, que precisaba estar enclavado entre calles alineadas de trazado regular y cuya fachada principal debía abrirse a la plaza existente para poder ser contemplado desde ella. En relación a su estructura y distribución interna el edificio tenía que conjugar el ámbito privado como residencia de la familia, con la esfera pública en la cual las partes nobles del palacio eran el reflejo de la categoría y nobleza de sus propietarios.

Inmediatamente el ambicioso proyecto tuvo que enfrentarse a las limitaciones existentes. Los solares disponibles eran de traza irregular al igual que las calles con las que limitaban, y su tamaño menor que el necesario para el nuevo edificio. Este necesitaba incorporar una zona ocupada por un tramo interior de la antigua muralla y por otras edificaciones. 

En el año 1559 el Consistorio leonés autorizó el derribo de los cubos y lienzo de la muralla y la ocupación de ese terreno. Asimismo, se le solicitó autorización para la ejecución de diversas obras encaminadas a conseguir la alineación de las calles. Este mismo año se iniciaron las obras. El encargado de su ejecución en calidad de aparejador fue el maestro Juan Ribero de Rada quien realizó aportaciones notables al diseño de Gil de Hontañón. 

En el año 1566 estaba levantada la fachada principal que da a la actual plaza de San Marcelo. En los años 1586 y 1587 se procedió a la adquisición y derribo de casas particulares para proseguir la obra y para ampliar el espacio de la plaza pública. Se buscaba que el palacio pudiese ser contemplado por entero desde ella como manifestación de la posición dominante que el linaje de los Guzmanes ocupaba en la ciudad. Con este fin se llegó a un acuerdo con el Consistorio para que esos terrenos quedasen en adelante libres de edificaciones y se incorporasen a la plaza existente.

A finales del se interrumpen las obras y el palacio queda incompleto. Se habían levantado dos de las cuatro alas, las que dan a la plaza y a la actual calle Ancha, y el patio central. A pesar de ello era la principal residencia de la ciudad y como tal hospedó en el año 1602, a Felipe III y a Margarita de Austria.

Pero en esta centuria el palacio dejará de estar habitado de forma regular al dejar de ser la residencia principal de la familia y comenzará su deterioro. Ya en los años 1654 y 1656 serán necesarias obras de reparación y reformas en los tejados, cornisas y en el patio entre otras. Sin uso continuado hubo que esperar al para que se empezase a limitar el proceso de decadencia en el que había entrado. En los años 40 de dicho siglo, el Gobierno Provincial alquiló parte del edificio para instalar sus oficinas llevándose a cabo reparaciones parciales. Posteriormente, en 1881, la Diputación Provincial de León compró el edificio a los propietarios de aquel entonces, los condes de Peñaranda de Bracamonte. 

Con su adquisición por parte de la diputación leonesa, se planteó una intervención en el edificio para adecuarlo a sus nuevas funciones y devolverle su primitivo aspecto. Anteriormente, en los años 1840 el arquitecto Miguel Echano había desmochado las torres quitándoles una planta, cerrado los balcones del segundo piso de estas y apuntalado las ventanas angulares del tercero para garantizar su estabilidad.

A lo largo de los siguientes años se sucedieron los proyectos y las reformas marcados siempre por las restricciones económicas. Al igual que ocurría en esa época con la catedral leonesa, la restauración del palacio fue objeto de debate entre las escuela conservadora, que abogaba por que las intervenciones se limitasen a la recuperación de las zonas dañadas produciendo “alteraciones mínimas”, y la escuela restauradora que proponía recuperar y completar el palacio bajo el criterio de “unidad de estilo”, tal y como se suponía lo hubiese concluido Gil de Hontañón.

Las intervenciones más destacadas fueron las siguientes:





El palacio tiene planta trapezoidal articulada en torno a un patio interior y esta torreado en sus cuatro esquinas. Su fachada principal tiene un marcado desarrollo horizontal, es de tres alturas separadas por impostas, la inferior tiene ventanas enrejadas, el cuerpo central balcones de los cuales los cercanos a la portada y los situados en las esquinas se coronas con frontones triangulares y semicirculares, y el superior presenta una galería que recorre la fachada hasta las torres formada por arcos de medio punto separados por pilastras corintias. Sobre estas y sobresaliendo de la cornisa se disponen un conjunto de gárgolas.

Las torres tienen una altura más, la última reconstruida en la restauración de 1975 buscando devolverles el aspecto que tuvieron antes de ser desmochadas en 1840. Tres de ellas lucen ventanas angulares y la suroeste lleva adosada una escalera de caracol.

La fachada sur que da a la calle Ancha es de estilo más clasicista. Se relaciona con Juan del Ribero Rada al que también se le atribuyen las ventanas angulares de la torre sudeste decoradas con pilastras dóricas y columnas jónicas y corintias, y la portada abierta a la calle del Cid.

La portada principal se abre descentrada siguiendo la tradición medieval hispana. Su diseño es característico del estilo de Rodrigo Gil de Hontañón. Formada por un arco de medio punto, está enmarcada por columnas jónicas sobre las que se apoya el entablamento que sustenta un balcón rematado por un frontón triangular decorado. A sus lados siguiendo la vertical de las columnas, dos guerreros portan los escudos de armas de la familia.

Atravesando el zaguán se accede al patio columnado. Es de dos plantas, la baja formada por arcos escarzanos apoyados en columnas jónicas que presentan la particularidad de que sus capiteles se muestran de perfil. La superior tiene arcos carpaneles sobre columnas corintias. Entre estas los antepechos están labrados con los escudos de los Guzmanes. Los huecos se cubren con vidrieras. Rematan el conjunto gárgolas al igual que en la fachada. En cuanto a su autoría su atribución es dudosa descartándose que se deba a Gil de Hontañón.

En la zona sur del patio se sitúa la escalera claustral de tres tramos sobre bóvedas rampantes. Aunque su estructura es de la época de construcción del edificio lo que podemos ver debe su aspecto a las restauraciones de los siglos y .

Respecto a las dependencias interiores han sido totalmente modificadas en las sucesivas obras llevadas a cabo en los pasados siglos. De los elementos originales ha sobrevivido la chimenea basada en modelos de Serlio que preside el salón principal. La decoración actual a base de cuadros, tapices y vidrieras de temas alegóricos de la historia leonesa corresponde a la etapa en que la diputación se ha hecho cargo del edificio.



</doc>
<doc id="2407" url="https://es.wikipedia.org/wiki?curid=2407" title="Piero della Francesca">
Piero della Francesca

Piero della Francesca ("Piero di Benedetto dei Franceschi"; llamado también "Pietro Borghese", Borgo del Santo Sepolcro, en el valle alto del Tíber, cerca de Arezzo, h. 1415 – Borgo del Santo Sepolcro, 12 de octubre de 1492) fue un pintor italiano del "Quattrocento" (siglo XV). Actualmente se le aprecia sobre todo como pintor especialista en frescos, pero en su época fue conocido también como un geómetra y matemático, maestro de la perspectiva y de la geometría euclidiana, temas en los que se concentró a partir del año 1470. Su pintura se caracterizó por su estilo sereno y el uso de las formas geométricas, particularmente en relación con la perspectiva y la luz. Es uno de los principales y fundamentales personajes del Renacimiento, aunque jamás trabajó para los Médicis y pasó poco tiempo en Florencia.

La reconstrucción biográfica de la vida de Piero es una empresa ardua a la que se han dedicado generaciones de estudiosos, confiando en los más débiles indicios, en la escasez general de documentos oficiales fiables que nos han llegado. Su propia obra ha llegado solo de forma fragmentaria, con numerosas pérdidas de extrema importancia, entre las que destacan los frescos ejecutados para el Palacio Apostólico, sustituidos en el siglo XVI por los frescos de Rafael.

Piero nació en un año no precisado entre el 1406 y el 1420, en Sansepolcro, que Vasari llama «Borgo San Sepolcro», región de la Toscana. Este territorio fronterizo, a mediados del siglo XV, cambió en diversas ocasiones de soberanía: en un principio estaba en manos de Rímini, después fue de la República de Florencia y más tarde pasó a la posesión del Papado. La fecha de nacimiento se desconoce, porque un incendio en los archivos comunales de Sansepolcro destruyó las actas de nacimiento del registro civil. Un primer documento que menciona a Piero como testigo es un testamento datado el 8 de octubre de 1436, del cual se deduce que el artista debía de tener ya al menos la edad prescrita de veinte años para un documento oficial. Según Giorgio Vasari en "Las vidas de los más excelentes arquitectos, pintores y escultores italianos desde Cimabue a nuestros tiempos", Piero, que murió en el año 1492, tenía 86 años en el momento de su muerte, lo que remontaría su fecha de nacimiento al año 1406 pero esta noticia se considera errónea, dado que sus padres se casaron en el año 1413.

Piero della Francesca procedía de una familia de mercaderes, de ahí que supiera matemáticas, cálculo, álgebra, geometría y contar con el ábaco. Su padre era el riquísimo comerciante de tejidos Benedetto de' Franceschi, y su madre Romana di Perino da Monterchi, noble de familia umbra. A esta aristocrática familia pertenecieron otros personajes famosos de la historia italiana; así, Francesco Franceschi (h. 1530-h.1599), importante editor literario y musical del Renacimiento; Angiolo Franceschi (1734 – 1806), arzobispo de Pisa y primado de Córcega y Cerdeña; y la escritora Caterina Franceschi Ferrucci (1803 – 1887), hija de Antonio Franceschi, médico y político, y de la condesa Maria Spada di Cesi.

Se ignora por qué, poco antes de su muerte, ya se le llamaba «della Francesca», en lugar de «di Benedetto» o «de' Franceschi», pero la conjetura de Vasari de que había tomado el apellido de su madre porque su marido murió cuando estaba ella embaraza y fue ella quien lo crio, no puede atenderse. Piero era el hijo primogénito de la pareja, que después tuvo otros cuatro hermanos (dos muertos a temprana edad) y una hermana.

Fue un artista itinerante, que trabajó en diversas localidades del centro y norte de Italia, en una actitud comparable a otros contemporáneos como la de Leon Battista Alberti.
Debió de tener una primera educación dentro del negocio familiar, para después formarse como pintor, si bien no se sabe con seguridad cómo, aunque probablemente fue en el mismo Sansepulcro, ciudad de frontera cultural, entre las influencias florentinas, sienesas y aportes umbros. Pudo haber aprendido su arte de uno de los varios artistas sieneses que trabajaban en Sansepulcro durante su juventud. También se ha apuntado la posibilidad de una formación en Umbría, de donde le provendría el gusto por la pintura de paisajes y el uso de colores delicados. El primer artista con el que colaboró fue Antonio de Anghiari, socio de su padre en la fabricación de estandartes, activo y residente en Sansepulcro, como atestigua el 27 de mayo de 1430 un documento de pago a Piero por la pintura de estandartes y banderas con las insignias de la Comuna y del gobierno papal, puestos por encima de una puerta de las murallas. Con Antonio de Anghiari colaboraría entre el año 1432 y 1436. En 1438 está de nuevo documentado en Sansepolcro, donde se le menciona entre los otros ayudantes de Antonio de Anghiari, a quien se confió, en un primer momento, el encargo para el retablo de la iglesia de San Francisco (luego realizada por Sassetta). Saber si Piero se formó con Antonio como maestro es difícil de decir, dado que de este último no se conserva ninguna obra cierta.

En 1439 está documentado por primera vez en Florencia, donde quizá recibió su verdadera formación, puede ser que estuviera allí ya en torno al año 1435. Para entonces, Masaccio ya llevaba muerto una década. Estuvo de aprendiz con Domenico Veneziano, y se le cita el 7 de septiembre de 1439 entre sus ayudantes en un ciclo de frescos dedicados la "Vida de la Virgen" en el coro de San Gil (actualmente Santa María la Nuova), hoy perdidos. Conoció a Fra Angélico, gracias al cual tuvo acceso a la obra del difunto Masaccio y también a otros maestros de la época como Brunelleschi. La maestría del arte de la perspectiva, la pintura luminosa y la paleta clarísima y suntuosa de Domenico Veneziano influyeron en Piero, pero también la moderna y vigorosa de Masaccio, lo que dio forma a algunas de las características fundamentales de su obra posterior. Piero conoció las diversas soluciones que el Prerrenacimiento florentino daba a los problemas de la representación del cuerpo humano y de cómo reflejar el espacio tridimensional sobre una superficie bidimensional. Por un lado seguía vigente el linearismo y el lirismo de Fra Angélico, Benozzo Gozzoli o Filippo Lippi, y por otro, estaba el realismo geométrico de Paolo Uccello. Piero aprendió cómo lograr representar una luz atmosférica, añadiendo una gran proporción de aceite en las mezclas de color.

Probablemente ya había colaborado con Domenico en Perugia en 1437-1438 y, según Vasari, los dos trabajaron también en Loreto, en la iglesia de Santa María, esta obra la dejaron inacabada y la terminó Luca Signorelli.

La primera obra que se conserva es la "Virgen con Niño", actualmente en la florentina Colección Contini Bonacossi, atribuida por vez primera a Piero en el año 1942 por Roberto Longhi, que data en los años 1435-1440, cuando Piero aún trabajaba como colaborador de Domenico Veneziano. En la parte posterior de la tabla está pintado un vaso, como ejercicio de perspectiva.

Para el año 1442 Piero estaba de vuelta en Sansepolcro donde fue nombrado uno de los «consiglieri popolari» del consejo comunal. El 11 de enero de 1445 recibió de la Cofradía de la Misericordia local el encargo de un retablo para el altar de su iglesia: el contrato preveía la realización de la obra en tres años y su completa autografía, si bien se dilató a lo largo de los quince años siguientes y parte del mismo se debe a colaboradores de su taller. Todavía en el año 1462 la cofradía de Sansepolcro realizaba un pago a Marco di Benedetto de' Franceschi, hermano de Piero y su representante en su ausencia, a cuenta de este retablo. La parte más conocida de este retablo es la tabla central, posiblemente la última en pintarse, que representa a la "Virgen de la Misericordia". La cofradía le exigió que el fondo del retablo fuera dorado, rasgo arcaizante e inusual en Piero.

De esta primera época es muy posible que sea una de sus más famosas obras, el "Bautismo de Cristo", originariamente el panel central de un gran tríptico. Su datación es controvertida, hasta el punto de que algunos la consideran la primera obra de Piero. Algunos elementos iconográficos, como la presencia de dignatarios bizantinos en el fondo, hacen que se sitúe la obra en torno a 1439, año del Concilio de Basilea-Ferrara-Florencia en el que se reunificaron efímeramente las iglesias de Occidente y Oriente. Otros datan la obra más tarde, en torno al 1460.

Pronto fue solicitado por diversos príncipes. En los años cuarenta estuvo en varias cortes italianas: Urbino, Ferrara y probablemente Bolonia, realizando frescos que se han perdido por completo. En Ferrara trabajó entre el 1447 y el 1448 para Lionello de Este, marqués de Ferrara. En 1449 ejecutó varios frescos en el Castillo de los Este y la iglesia de San Andrés de Ferrara, también perdidos. Quizá tuviera aquí un primer contacto con la pintura flamenca, encontrando a Rogier van der Weyden directamente o a través de las obras que había dejado en la corte. Esta influencia flamenca es particularmente evidente si se piensa en su precoz uso de la pintura al óleo. Piero influyó en el posterior pintor ferrarés Cosme Tura.

El 18 de marzo de 1450 está documentado en Ancona, como testimonia el testamento (recuperado recientemente por Matteo Mazzalupi) de la viuda del conde Giovanni di messer Francesco Ferretti. En el documento el notario especifica que los testigos son todos «ciudadanos y habitantes de Ancona», por lo que Piero fue probablemente huésped por cierto tiempo de la importante familia anconetana y quizá para ellos pintase la tablilla de "San Jerónimo penitente", datada precisamente en 1450. De los mismos años procede el muy parecido "San Jerónimo y el donante Girolamo Amadi". En ambos se registra un interés por el paisaje y por la adecuada representación de los detalles, por las variaciones de los materiales y del "«lustro»" (esto es de reflejos de luces), que pueden ser explicadas solamente a través de un conocimiento directo de la pintura flamenca. Vasari recuerda también unos "Desposorios de la Virgen" sobre el altar de San José en la catedral, ya desaparecido en el 1821.

En 1451 fue a Rímini, llamado por Segismundo Pandolfo Malatesta. Entonces ejecutó, para el famoso Templo Malatestiano, su conocido fresco votivo monumental de "Pandolfo Malatesta a los pies de su santo patrón", del año 1451, en el que la escena se enmarca en un trampantojo. También hizo un retrato del condotiero. Aquí probablemente pudo conocer a otro famoso matemático y arquitecto del Renacimiento, Leon Battista Alberti.

En el año 1452, Piero della Francesca fue llamado a realizar, en sustitución de Bicci di Lorenzo la que acabaría conociéndose como su obra maestra y una de las más significativas del Renacimiento: los frescos de la basílica de San Francisco en Arezzo, dedicados a la "Leyenda de la Santa Cruz". Fue la familia Bacci, la más rica de Arezzo, la que decidió decorar el coro o capilla mayor de la iglesia dedicada a San Francisco. En el año 1447 contrataron para ello a Bicci di Lorenzo, de tradición tardogótica, pero solamente consiguió acabar el fresco de la bóveda, antes de fallecer. Contrataron entonces a Piero della Francesca para acabarlo, datándose su realización entre el año 1452 y 1466, aunque también se ha considerado como posible que acabase antes del año 1459. Es muy posible que trabajara en dos fases, una primera entre 1452 y 1458, y una segunda a su regreso de Roma. A finales del 1466 la cofradía aretina de la Anunciada le encargó un estandarte con la "Anunciación", citando en el contrato el éxito de los frescos de San Francisco como motivo del encargo por lo tanto, para aquella fecha, el ciclo tenía que estar acabado. En esta obra se pueden apreciar características que hacen de Piero un precursor del Alto Renacimiento, como la composición clara que emplea magistralmente la perspectiva geométrica, el tratamiento rico y novedoso de la luz (tomado de Domenico Veneziano) y su cromatismo admirable, delicado y claro.

La realización de la obra de Arezzo fue simultánea con la de otras obras y con su estancia en otras localidades. Así, en 1453, regresó a Sansepolcro donde, al año siguiente, firmó un contrato para un retablo con destino al altar mayor de la iglesia agustiniana, conocido como Retablo o Políptico de San Agustín. Trabajó en este proyecto desde 1454 y no se acabó hasta 1469, como evidencia el pago realizado, quizás el último, el 14 de noviembre de ese año. En estos paneles se pone en evidencia, nuevamente, su profundo interés en el estudio teórico de la perspectiva y su enfoque contemplativo. La obra es muy innovadora, careciendo de fondo de oro, sustituido por un cielo abierto entre balaustres clasicistas, y con las figuras de los santos de una linealidad y monumentalidad acentuadas. Actualmente solamente quedan cuatro paneles.

También estuvo en Roma, en al menos dos ocasiones. Una primera vez, llamado por el papa Nicolás V (m. 1455), en la que ejecutó frescos en la Basílica de Santa María la Mayor, de los que solamente quedan restos, en concreto un "San Lucas" pintado probablemente por su taller, mientras que nada se ha conservado de las obras enteramente autógrafas. La segunda vez, fue cuando lo llamó el papa Pío II, que acababa de ser elegido. Antes de marcharse de Sansepolcro, designó a su hermano Marco como representante, en previsión de una larga ausencia. Pío II le encargó pintar su habitación en el Palacio Apostólico; esta obra fue destruida en el siglo XVI para dejar sitio a la primera de las "Estancias Vaticanas" de Rafael. La tesorería papal emitió un documento, datado el 12 de abril de 1459 para el pago de 140 florines por «ciertas pinturas» en la «cámara de Su Santidad Nuestro Señor»

Otras obras de madurez son la "Virgen del parto" (1455-1465) y "La resurrección de Cristo" (1450-1463). La "Virgen del Parto" la realizó en tan solamente siete jornadas, para la capilla de la antigua iglesia de Santa María de Nomentana del cementerio de Monterchi, aldea vecina de Sansepolcro y de la que era originaria su madre. El modelo iconográfico, la Virgen del Parto, no era muy frecuente. Empleó materiales de alta calidad, como una cantidad considerable de azul marino que se obtenía a partir de lapislázuli importado. En esta obra puede apreciarse la obsesión de Piero por la simetría, que le lleva a colocar a dos ángeles idénticos, uno a cada lado de la Virgen, utilizando el mismo cartón. "La resurrección de Cristo", por su parte, es obra notable al utilizar diversas perspectivas. Fue pintada en Arezzo, cerca de su ciudad natal, al tiempo que trabajaba en los frescos de la "Leyenda de la Santa Cruz".

El 6 de noviembre de 1459 murió la madre de Piero y el 20 de febrero de 1464 su padre. En el año 1460 se encontraba en Sansepolcro, donde firmó y dató el fresco de "San Luis de Tolosa". Hay que recordar que en 1462 le hicieron un pago por el "Políptico de la Misericordia". En 1466 Piero pintó el fresco de una "Magdalena" en la Catedral de Arezzo, y le encargaron, como ya se señaló, el estandarte para la cofradía de la Anunciada, que entregó en Arezzo en el año 1468.
En el año 1467 en Perugia ejecutó por cuenta de las hermanas terciarias del convento de San Antonio un retablo, conocido como Políptico de San Antonio. Le encargaron una obra de inspiración tardogótica, pero en la parte superior tiene lo más destacado: la "Anunciación" del gablete es de clara estampa renacentista, mostrando su maestría con la perspectiva.

En 1468 está documentado en Bastia Umbra, donde se había refugiado para huir de la peste. Allí realizó al menos otro gonfalón pintado perdido.

Para el año 1469, acabados ya los frescos de Arezzo y el retablo de San Agustín, Piero se encontraba en Urbino, al servicio de Federico de Montefeltro. No están claros los períodos de estancia en Urbino, parece que con seguridad estuvo allí entre el 1469 y 1472, pero algunos autores retrasan su marcha hasta 1480. Fue una época en la que produjo cuadros de notable calidad. Piero está considerado como uno de los protagonistas y promotores del renacimiento en Urbino, y su propio estilo alcanza en esta ciudad un equilibrio no superado entre el uso de las rigurosas reglas geométricas y el aire serenamente monumental. En la corte de Urbino profundizó en el conocimiento de la pintura flamenca, tanto a través de la colección del duque como por la presencia de Justo de Gante, quien entre el año 1471 y el 1472 se asentó en Italia, primero en Roma pero luego, invitado por Federico de Montefeltro, en la corte de Urbino, donde estuvo hasta octubre del año 1475. No sería el único artista destacado a quien conoció en Urbino, pues allí entró en contacto también con Melozzo da Forlì y Luca Pacioli.
Aquí pintó el famoso retrato doble de Federico de Montefeltro y su esposa Battista Sforza (h. 1465-1472), hoy en la Galería de los Uffizi de Florencia, titulado "Triunfo de la Castidad". En él se aprecia la influencia de la pintura flamenca en el tratamiento del paisaje y en la minuciosidad y amor por el detalle.

En 1469 Piero está documentado en Urbino, donde la Cofradía del Corpus le encargó que pintase un estandarte procesional. En aquella ocasión al maestro se le propuso también la pintura del "Retablo del Corpus Domini", ya encargada a Fra Carnevale, luego a Paolo Uccello (1467), que pintó solamente la predela, y al final terminada por Justo de Gante en 1473-1474. En el año 1470 se documenta a Federico da Montefeltro en Sansepolcro, quizá en compañía de Piero.

A esta época de Urbino pertenece "La flagelación" (h. 1470, aunque otros lo datan en 1452), una de sus pinturas más conocidas. Al parecer, fue una creación personal que no dependió de encargo alguno y que pone de manifiesto que Piero era consciente de las innovaciones arquitectónicas de la época; es controvertido en cuanto a su significado exacto (véanse las Interpretaciones icónicas de este cuadro).

Se cree que fue en Urbino donde pintó la "Natividad" (1470-1485), que se encuentra actualmente en Londres. Es una de las últimas obras de Piero, cuando ya se estaba quedando ciego, creyéndose que por este motivo quedó inacabada, aunque su estado puede deberse también a las restauraciones de siglos pasados. Fue un encargo de su sobrino, con motivo de su matrimonio. Algunos críticos elaboran la hipótesis que el rostro de la Virgen fuera realizado por otra mano «flamenca». A este período se atribuye también la "Virgen con Niño y cuatro ángeles" del Instituto de Arte Clark en Williamstown, Massachusetts.

En el año 1473 se registra un pago, quizá aún del "Políptico de San Agustín". En 1474 le corresponde el último pago de una pintura perdida, destinada a la capilla de la Virgen de la abadía de Sansepolcro. Desde el 1 de julio de 1477 y hasta 1480 vivió, con algunas interrupciones, en Sansepolcro, donde formó parte regularmente del consejo comunal. En el año 1478 pintó un fresco perdido para la Capilla de la Misericordia, siempre en Sansepolcro. Entre 1480 y 1482 estuvo al frente de la Cofradía de San Bartolomé en su ciudad natal.

Piero della Francesca está documentado en Rímini el 22 de abril de 1482, donde alquiló «una mansión con un pozo». Aquí se dedicó a la escritura del "Libellus de quinque corporibus regularibus", terminado en el año 1485 y dedicado a Guidobaldo da Montefeltro. Otorgó testamento el 5 de julio de 1487, declarándose «sano de espíritu, de mente y de cuerpo». En sus últimos años, pintores como Perugino y Luca Signorelli visitaron frecuentemente su taller.

Aunque actualmente su obra matemática es poco menos que ignorada por completo, Piero fue, en vida, un matemático reputado. Según Giorgio Vasari, «…los artistas le otorgaron el título del mejor geómetra de sus tiempos, porque seguramente sus perspectivas tienen una modernidad, un mejor diseño y una mayor gracia que ninguna otra». Es Vasari también quien dice que en estos últimos años se vio afectado por una grave enfermedad de los ojos que le impidió trabajar. Por ello abandonó la pintura y se dedicó exclusivamente a su obra teórica, que escribió dictándola.

Murió en Sansepolcro el 12 de octubre de 1492 , el mismo día en el que Cristóbal Colón pisó por vez primera América. Fue sepultado en la abadía de Sansepolcro, hoy el "Duomo".

Se conocen tres textos muy importantes escritos por Piero, de los más científicos del siglo XV: el "De prospectiva pingendi" («Sobre la perspectiva para la pintura»), "Libellus de quinque corporibus regularibus" («Librito de los cinco sólidos regulares») y un manual de cálculo titulado "Trattato dell’abaco" («Tratado del ábaco»).

Los temas tratados en estos escritos incluyen aritmética, álgebra, geometría y obras innovadoras tanto en geometría de los sólidos como perspectiva. En ellos se pone de manifiesto su contacto con Alberti. En estas tres obras matemáticas está presente una síntesis entre la geometría euclidiana, perteneciente a la escuela de los eruditos, y matemática con el ábaco, reservada a los técnicos.

La primera obra fue el "Libellus de quinque corporibus regularibus", un tratado dedicado a la geometría, que retomó temas antiguos de tradición platónico-pitagórica, estudiados siempre con la intención de que se puedan utilizar como elementos de diseño. Se inspira en las lecciones euclidianas para el orden lógico de las expresiones, para las referencias y el uso coordinado y complejo de los teoremas, mientras que se aproxima a las exigencias de los técnicos por la predictibilidad de las figuras tratadas, sólidas y poliédricas, y por la ausencia de demostraciones clásicas y por el uso de reglas aritméticas y algebraicas aplicadas a los cálculos. En el texto, en particular, por vez primera se dibujan los poliedros regulares y semiregulares, estudiando las relaciones que existen entre los cinco regulares.

En el segundo tratado, "De prospectiva pingendi" siguió en la misma línea de estudio, pero con notables novedades, hasta el punto de que se puede definir a Piero como uno de los padres del moderno dibujo técnico; de hecho, prefería la axonometría a la perspectiva, por considerarla más congruente con un modelo geométrico. Entre los problemas resueltos destaca el cálculo del volumen de la bóveda y la elaboración arquitectónica de las construcciones de las cúpulas.

El "Trattato d'abaco", sobre matemática aplicada (cálculo) fue escrito quizá ya en el año 1450, treinta años antes que el "Libellus". El título es de época moderna, porque el original carece de él. La parte geométrica y la algebraica resulta muy amplia en relación con las costumbres de su tiempo, así como la parte experimental sobre la que el autor ha explorado elementos no convencionales.

Gran parte de la obra de Piero fue posteriormente incluido en obras de otros, especialmente Luca Pacioli, un franciscano que era discípulo de Piero y a quien Vasari acusa directamente de copiar y plagiar a su maestro. La obra de Piero sobre geometría sólida aparece en la obra de Pacioli " De divina proportione (Divina Proporción)", un trabajo ilustrado por Leonardo da Vinci.

La crítica se encuentra dividida sobre la colaboración de varios artistas en su taller (entre otros Lorentino d'Arezzo, Luca Signorelli y el Perugino); por otro lado el único alumno que se ha documentado es Galeotto da Perugia. Entre sus colaboradores debe mencionarse a Giovanni da Piamonte, con el que trabajó en la ejecución de los frescos en San Francisco; es de dicho autor la tabla conservada cerca de la iglesia de Santa María de las Gracias de Città di Castello, en la que están seguramente presentes influencias de Piero della Francesca.

En vida fue muy famoso y su impacto se nota en las generaciones posteriores, aunque no fuera de pintores que trabajaran directamente con él. Dejó varios discípulos y seguidores: además de Luca Pacioli, Melozzo da Forli y Luca Signorelli.

Piero della Francesca es un pintor cuatrocentista, perteneciente a la segunda generación de pintores-renacentistas, intermedia entre Fra Angélico y Botticelli. Asumió los hallazgos de la primera escuela renacentista florentina de autores como Paolo Uccello, Masaccio y Domenico Veneziano. No viajó a Flandes, pero sí vio pintura flamenca, de manera que hizo una especie de simbiosis entre el Renacimiento italiano y la pintura flamenca.

Primó, como los otros grandes maestros de su tiempo, la creatividad. Trabajó técnicas nuevas, como el uso del lienzo como soporte pictórico y el óleo. Y también trató temas novedosos no solamente la omnipresente pintura religiosa, como el retrato y la representación de la Naturaleza. Tiene un estilo pictórico muy particular y por lo tanto es fácil de identificar. En su obra confluyen la perspectiva geométrica brunelleschiana, la plasticidad de Masaccio, la luz altísima que aclara las sombras y empapa los colores de Fra Angélico y Domenico Veneziano, así como la descripción precisa y atenta a la realidad de los flamencos. Otras características fundamentales de su expresión poética son la simplificación geométrica, tanto de la composición como de los volúmenes, la inmovilidad ceremonial de los gestos, la atención a la verdad humana.

Sus obras están admirablemente equilibradas entre el arte, la geometría y un complejo sistema de lectura a muchos niveles, donde se unen complejas cuestiones teológicas, filosóficas y de actualidad. Consiguió armonizar, tanto en su vida como en sus obras, los valores intelectuales y espirituales de su tiempo, condensando múltiples influencias y mediando entre la tradición y la modernidad, entre la religiosidad y las nuevas afirmaciones del Humanismo, entre la racionalidad y la estética.

Su actividad puede caracterizarse como un proceso que va de la práctica pictórica, a la matemática y a la especulación abstracta. Su producción artística, caracterizada por el extremo rigor de la búsqueda perspectivística, de la plástica monumentalidad de las figuras, del uso en función expresiva de la luz, influyó en el profundo la pintura renacentista de la Italia septentrional y, en particular, la escuela ferraresa y véneta.

Su obra se caracteriza por una dignidad clásica, similar a Masaccio. El término que mejor define su arte es el de «tranquilidad», lo que no impide que tenga un tratamiento técnico riguroso. Se percibe también la voluntad de construcción de un espacio racional y coherente. Piero se interesó mucho por los problemas del claroscuro y perspectiva, como su contemporáneo Melozzo da Forli. Piero della Francesca y Melozzo da Forlì son los más célebres maestros de la perspectiva del siglo XV, reconocidos como tales por Giorgio Vasari y Luca Pacioli. Destaca por sus conocimientos de perspectiva y composición, en lo que influyeron sus conocimientos matemáticos, fusionando el arte con la ciencia de la matemática, la geometría y la perspectiva. La perspectiva lineal era su característica principal a la hora de pintar, lo que se puede apreciar en todos sus cuadros, que se distinguen básicamente por sus coloridos luminosos y un suave pero firme trazo en las figuras. Sus composiciones son claras, equilibradas, reflejando con precisión matemática las arquitecturas. Sin ceder los efectos de trampantojo, Piero utilizó la perspectiva a fin de planificar las composiciones naturalistas grandiosas.

En estos paisajes serenos introducía las figuras de los personajes con un tratamiento muy volumétrico: se percibe un estudio anatómico, y una cierta monumentalidad. Ahora bien, son personajes muy estáticos, que permanecen como congelados y suspensos en sus propios movimientos, resultando un poco fríos, inexpresivos, monolíticos. Esta ausencia de nerviosismo es lo opuesto al resto de los pintores renacentistas de Florencia, que conforme avanzó el tiempo fueron haciendo figuras cada vez más dinámicas. Roberto Longhi, cuando habla de Piero della Francesca, dice que sus figuras son «columnas». El tratamiento de las figuras en volúmenes simples expresa un sentimiento de intemporalidad, lo mismo que la armonía de los tonos claros; todo ello expresa el sentido poético del arte de Piero della Francesca.

La luz atmosférica es otro de sus rasgos destacados, que adquirió de su maestro Domenico Veneziano, y que le servía para simbolizar la perfección de la Creación divina. Es muy diáfana, muy diurna, con un tratamiento uniforme, sin intensidades ni gradación lumínica (ligeramente arcaica, similar a la de Fra Angélico). Sus ensayos en este sentido llegan a dar la sensación de que sus figuras están modeladas en material dotado de luz propia, íntima, radiante. Los frescos como la "Leyenda de la Santa Cruz", en el ábside de la Iglesia de San Francisco, en Arezzo, son una obra de arte en luminosidad.

Lista de sus obras (pinturas sobre tabla y frescos) en orden cronológico.



Bohuslav Martinů escribió una obra en tres movimientos para gran orquesta titulado "Les Fresques de Piero della Francesca", H. 352 (1955). Dedicado a Rafael Kubelik, este lo estrenó, junto con la Filarmónica de Viena en el Festival de Salzburgo del año 1956.

El cantautor Javier Krahe le dedicó una canción satírica titulada "Piero Della Francesca" en su dísco Cábalas y Cicatrices del año 2002. En la presentación de la canción solía hacer un breve resumen de la vida del pintor destacando su faceta como geómetra.






</doc>
<doc id="2417" url="https://es.wikipedia.org/wiki?curid=2417" title="Panacea universal">
Panacea universal

La panacea es un mítico medicamento que cura todas las enfermedades o, incluso, prolonga indefinidamente la vida. Fue buscada por los alquimistas durante siglos, especialmente en la Edad Media.

La palabra panacea proviene de la voz griega "panakos" y significa 'remedio para todo' ("pan": todo y "akos": remedio).



</doc>
<doc id="2421" url="https://es.wikipedia.org/wiki?curid=2421" title="Pársec">
Pársec

El pársec o parsec (símbolo pc) es una unidad de longitud utilizada en astronomía. Su nombre se deriva del inglés "parallax of one arc second" (paralaje de un segundo de arco).

En sentido estricto, el pársec se define como la distancia a la que una unidad astronómica (ua) subtiende un ángulo de un segundo de arco (1″). En otras palabras, una estrella dista un pársec si su paralaje es igual a 1 segundo de arco entre el Sol y la Tierra.

De la definición resulta que:
Múltiplos del parsec:

La separación básica que usan los astrónomos para determinar el paralaje de las estrellas es el radio de la órbita de la Tierra. El paralaje se mide en segundos de arco (60 segundos de arco = 1 minuto de arco; 60 minutos de arco = 1 grado). Se basa en el método del paralaje trigonométrico, el más antiguo y extendido para determinar la distancia a las estrellas.

Puesto que el pársec es una distancia relacionada con la unidad astronómica, se relaciona con la tangente del ángulo en P (ver el diagrama). Ahora bien, siendo β (léase "beta") un ángulo muy pequeño, del orden de hasta la milésima de segundo de arco, se comportará como una función lineal de proporcionalidad inversa respecto a Δ (léase "delta"). Es decir, a Δ doble, π se hace la mitad; pero si Δ es la mitad, π será el doble, y así sucesivamente, de tal forma que la relación entre distancia y paralaje se vuelve muy sencilla:

donde Δ es la distancia en pársecs y β el paralaje en segundos de arco. Medido el paralaje de una estrella, no hay más que calcular su inversa para tener la distancia en pársecs.

Otra posibilidad es definir un pársec como la distancia a la que dos objetos, separados entre sí por una unidad astronómica, parecen estar separados por un ángulo de 1 segundo de arco. Entonces:

El valor adoptado por la Unión Astronómica Internacional es: 1 pc = 3,08567758149136727891 × 10 m = 3.261563777167434 años luz.

Por definición [IAU, 2015], 1 pc = 360/(2*π)*3600 ua = 648000/π ua; sustituyendo al valor anterior 1/tg[1/3600*2*π/360] ua, donde 1 ua = 149597870700 m.

Ejemplos de distancias en pársecs:

Los pársec se mencionan en diversas obras de ciencia ficción, como son libros, series de televisión y películas. En muchas de ellas, como son las novelas de Isaac Asimov o las series de televisión "Star Trek", se utiliza el término más o menos correctamente. Sin embargo, en ocasiones no es así.

En "La guerra de las galaxias" ("Episodio IV: Una nueva esperanza"), Han Solo se jacta de que su nave, el "Halcón Milenario", es «la nave que corrió la carrera Kessel en menos de 12 pársecs». Esto es repetido en el primer capítulo de la tercera trilogía ("Episodio VII: El despertar de la Fuerza"), esta bajo la dirección de J. J. Abrams. Popularmente se cree que George Lucas utilizó el pársec como medida de tiempo y no de distancia; sin embargo está usada correctamente: la carrera consiste en encontrar el camino más corto y a eso se refiere la frase. Esto ha dado lugar a que se hagan parodias del supuesto "error", como por ejemplo en el episodio "Blue Harvest" de "Padre de Familia".

Sin embargo, el mismo Lucas, en el comentario de la película aparecida en DVD de 2004, aclara que el tiempo dado en unidades de distancia significa, en el universo de "La guerra de las galaxias", una referencia a la manera en que el ordenador de una nave calcula el camino a recorrer entre dos puntos del espacio; una distancia menor (en pársecs) quiere decir que el ordenador ha encontrado un camino que se puede recorrer en menos tiempo. Al parecer, el "Halcón Milenario" era muy eficiente en este sentido, gracias a las "mejoras" introducidas por Solo. Por otro lado, en el Episodio 2 de la misma saga, la senadora Padme utiliza el pársec como una unidad de distancia. (Nota: en la versión original doblada en España del episodio 4 se utilizó "pársec" como la unidad de velocidad del "Halcón Milenario". En las siguientes versiones se tradujo la expresión por "parasegundo").

En el primer capítulo de la primera temporada de la serie "El Mandaloriano", relacionada con el universo Star Wars, «El Cliente» del trabajo especial dice de la misma: «Que eras el mejor en el pársec», refiriéndose a la habilidad del protagonista como cazarrecompensas.

El planeta Melmac, en la serie "ALF", estaba localizado seis pársecs más allá del Supercúmulo Hidra-Centauro, o a 19,56 años luz de allí.

En el videojuego para PC "Spore", al llegar al estado espacial, el pársec es utilizado para medir las distancias y como referencia para localizar un sistema en la galaxia utilizando el ángulo y la distancia al centro de la galaxia.

Es notable que el pársec se utilice con naturalidad y universalidad en novelas de Asimov como las pertenecientes a las sagas del "Imperio galáctico" o "Fundación," donde el origen de la humanidad ha sido olvidado y es un tema central de discusión (a niveles científico y popular), y por lo tanto la Tierra y nuestro sol tienen tanta relevancia como todos los demás (o incluso menos).



</doc>
<doc id="2422" url="https://es.wikipedia.org/wiki?curid=2422" title="Plancton">
Plancton

Se denomina plancton (del griego πλαγκτόν ["planctón"], ‘lo errante’ o ‘lo que va errante’) al conjunto de organismos, principalmente microscópicos, que flotan en aguas saladas o dulces, más abundantes hasta los 200 metros de profundidad, aproximadamente. Se distingue del necton, palabra que denomina a todos los nadadores activos y del neuston, los que viven en la interfase o límite con el aire, es decir, en la superficie. Plancton (organismos que viven en suspensión en el agua), bentos (del fondo de ecosistemas acuáticos) y edafón (de la comunidad que habita los suelos).

Aunque tradicionalmente se ha subdividido el plancton en fitoplancton y zooplancton, según las clasificaciones más recientes esta distinción no parece apropiada, ya que entre los organismos autótrofos se incluyen los vegetales, algunos protistas y bacterias, y entre los heterótrofos están los animales, otros protistas y bacterias. No obstante, esta clasificación sigue utilizándose extensamente.

Se puede hacer una primera división entre holoplancton, que son aquellos organismos que pasan todo su ciclo vital perteneciendo al plancton y meroplancton, formado por organismos que solo durante una parte de su vida integran la comunidad planctónica.

Constituido por todos los consumidores que constituyen en su gran mayoría a productores secundarios y terciarios. Este grupo está constituido por organismos generalmente microscópicos adultos y sus fases larvarias (holoplancton), y por las fases larvarias de otros organismos que en forma adulta habitan los fondos acuáticos o la columna de agua pero contrarrestando el movimiento de las corrientes. Algunos de los grupos de organismos más abundantes y característicos del zooplancton son los copépodos, cladóceros, rotíferos, cnidarios, quetognatos, eufáusidos y las larvas de los peces que por su relevancia socioeconómica de los organismos juveniles y adultos generalmente estudian y describen con el término “ictioplancton”. Al igual que el fitoplancton, dependiendo del ambiente en que se encuentren, ya sea dulceacuícola o marino, cada uno de los grupos o especies del zooplancton variará su diversidad y abundancia. Un componente del zooplancton relativamente menos estudiado son sus parásitos que constituyen una diversidad varios órdenes de magnitud mayor que los mismos organismos fitoplanctónicos y zooplanctónicos ya que cada organismos que existe en el planeta es propenso a infestarse o infectarse por múltiples parásitos.

El plancton vegetal, denominado fitoplancton, vocablo que deriva del griego φύτοπλαγκτον ["phytoplankton"] (φυτόν ["phyton"] significa planta), se desarrolla en las aguas costeras del mar con luz solar y sales minerales abundantes (aguas de hasta 30 m de profundidad), dado que elaboran su alimento por fotosíntesis.

Constituyen el alimento del zooplancton y producen el 50 % del oxígeno molecular necesario para la vida terrestre. Los organismos que más abundan en el fitoplancton son las cianobacterias y las diatomeas, unas algas doradas unicelulares. También encontramos a los dinoflagelados, responsables de las mareas rojas.

Base de la cadena trófica marina, el fitoplancton ha experimentado un significativo descenso debido al aumento de la radiación ultravioleta. Se ha observado que bajo el agujero de la capa de ozono en la Antártida la productividad del fitoplancton decreció entre el 6 % y el 12 %.

Diversos autores han realizado una clasificación del plancton por su tamaño, aunque es una división que puede considerarse “artificial”, pues en principio se basó en la luz de malla con la que se hacían las capturas, y no se ha llegado a un acuerdo definitivo. Una de las clasificaciones más utilizadas es la siguiente:

También se puede clasificar según su ubicación (horizontal o vertical)

El plancton vegetal está siempre cerca de la superficie del agua, pues necesita luz para realizar la fotosíntesis. En cambio el zooplancton está siempre en movimiento, de arriba hacia abajo, completando un ciclo diario con un recorrido de entre 100 a 500 metros, o más. Están casi siempre cerca de la superficie de noche para alimentarse, y más abajo durante el día para escapar de las fuertes radiaciones solares, aunque puede invertirse para algunos grupos.

La mayoría de las especies son transparentes con una cierta irisación, y presentan colores solo al microscopio. Las especies superficiales son azuladas, y las otras rojizas. Algunas emiten luminiscencia, como la noctiluca.

La mayoría de las especies del plancton mide menos de un milímetro, otras, en cambio, son más grandes, como los sifonóforos, ctenóforos y medusas acalefas.

El fitoplancton es el alimento del zooplancton. Este, sirve al mismo tiempo como alimento a equinodermos, crustáceos y peces en estado larvario. Estas larvas al crecer sirven como alimento a bancos de pequeños peces que a su vez alimentan a grandes planctívoros, como las ballenas o los tiburones ballena, y a peces mayores que alimentan, a veces, en varios eslabones sucesivos, a los grandes depredadores oceánicos, como son los cetáceos carnívoros, los tiburones, los atunes o los peces espada. En proporción, una tonelada de estos últimos habrá requerido, para su existencia y desarrollo, cinco mil toneladas de fitoplancton, como parte de lo que se denomina cadena trófica.

Conocidas normalmente como “mareas rojas” son las proliferaciones de dinoflagelados (fitoplancton) que crecen exponencialmente debido a las condiciones favorables para su desarrollo (temperaturas, calidad y cantidad de luz, nutrientes y pasividad de la columna de agua) . Su reproducción no para hasta que las condiciones sean desfavorables. Muchas veces estas floraciones algales pasan desapercibidas, mas es posible que la floración sea de algún tipo de fitoplancton tóxico, como "Alexandrium catenella", que provoca la muerte en vertebrados como los humanos. No todas las floraciones tornan el agua del color rojo que le da el nombre a este fenómeno, paradójicamente las floraciones más nocivas son incoloras, por lo que los expertos suelen referirse a ellas como «floraciones algales nocivas» (FAN).




</doc>
<doc id="2423" url="https://es.wikipedia.org/wiki?curid=2423" title="Pirekua">
Pirekua

La pirekua es uno de los géneros musicales propios del pueblo p'urhépecha, del estado de Michoacán, originada del sincretismo de la música y cantos religiosos de los evangelizadores españoles con las reminiscencias de la música indígena (sonecitos de la Tierra y sones del Costumbre). Actualmente representa un medio de expresión de la lengua p'urhépecha y constituyen una manera de exaltar su conciencia étnica a fin de salvaguardar la pindekua (tradición y costumbre). La pirekua, en sus ritmos abajeño (6/8) y son valseado (3/4), transmite mensajes de amor, de desamor, de la historia p'urhépecha y de Michoacán, de la geografía michoacana y de la vida social de la comunidad.

La palabra "pirekua", en lengua p'urhépecha o purépecha, significa "canción", y por lo general las pirekuas se cantan en esta lengua nativa o en castellano, algunas incluso intercalan ambos idiomas y otras tantas se interpretan instrumentalmente. Se destaca por su carácter noble, nostálgico y sentimental. Algunas de las regiones donde se cultiva la pirekua son el Quinceo, Zacán, San Lorenzo, Comachuen, Nurio, Cherán, Ichan, Angahuan, Pátzcuaro y otras comunidades. Aunque este género no es muy conocido en otras regiones del país, en Michoacán es una parte de la cultura de los purépechas, incorrectamente llamados tarascos por sus enemigos aztecas y más tarde por los conquistadores españoles.

Habitualmente se canta una o dos voces masculinas, aunque pueden ser femeninas o mixtas y acompañadas por guitarra sexta, contrabajo, vihuela o arpa y violín. A diferencia del son abajeño purépecha, se utilizan pocos instrumentos (a menudo uno, dos o hasta tres), pero en algunas ocasiones se utiliza la misma instrumentación, sobre todo cuando un grupo se dedica a interpretar ambos géneros tradicionales.

Algunas pirekuas tradicionales:




</doc>
<doc id="2426" url="https://es.wikipedia.org/wiki?curid=2426" title="Pandora">
Pandora

En la mitología griega, Pandora (en griego antiguo: Πανδώρα) fue la primera mujer, hecha por Hefesto debido a una orden de Zeus después de que Prometeo, yendo en contra de su voluntad, le otorgara el don del fuego a la humanidad. 

Según la versión del poeta Hesíodo, la creación de la primera mujer está ligada estrechamente con el incidente de Mecone. Cuando los mortales e inmortales se separaron, Prometeo urdió un engaño para que, en adelante, cuando los humanos sacrificaran para los dioses, solo les reservaran los huesos y pudieran aprovechar para sí mismos la carne y las vísceras. Zeus, irritado por el ardid, les negó el fuego a los humanos, pero Prometeo, hurtándolo, se lo restituyó.

Zeus ordenó que Hefesto modelara una imagen con arcilla con figura de encantadora doncella, semejante en belleza a las inmortales, y le infundiera vida. Pero, mientras que a Afrodita le ordenó otorgarle gracia y sensualidad, y a Atenea concederle el dominio de las artes relacionadas con el telar y adornarla, junto a las Gracias y las Horas con diversos atavíos, a Hermes le encargó sembrar en su ánimo mentiras, seducción y un carácter inconstante. Ello, con el fin de configurar un "bello mal", un don tal que los hombres se alegren al recibirlo, aceptando en realidad un sin número de desgracias.

Los poemas presentan de distinta forma la introducción de los males por Pandora. En la "Teogonía", el poeta la presenta como la primera de entre las mujeres, que en sí mismas traen el mal: en adelante, el hombre debe optar por huir del matrimonio, a cambio de una vida sin carencias materiales, pero sin descendencia que lo cuide y que mantenga después de su muerte su hacienda; o bien casarse, y vivir constantemente en la penuria, corriendo el riesgo incluso de encontrar a una mujer desvergonzada, mal sin remedio.

En "Trabajos y días", Hesíodo indica que los hombres habían vivido hasta entonces libres de fatigas y enfermedades, pero Pandora abrió un ánfora que contenía todos los males (la expresión «caja de Pandora» en lugar de jarra o ánfora es una deformación renacentista) liberando todas las desgracias humanas. El ánfora se cerró justo antes de que la esperanza fuera liberada.

En esta última versión es cuando se menciona por primera vez el nombre de "Pandora", y su vínculo con Epimeteo: Prometeo le había advertido no aceptar ningún regalo de Zeus, de lo contrario les sobrevendría una gran desgracia a los mortales. Tras un rechazo inicial que enfureció a Zeus, este encadena a Prometeo en las montañas del Cáucaso. Epimeteo termina casándose con Pandora, y se da cuenta muy tarde de la astucia del padre de los dioses.

Otras versiones del mito relatan que en realidad la jarra contenía bienes y no males. La apertura de la jarra ocasionó que los bienes volaran regresando a las mansiones de los dioses, sustrayéndose de la vida de los hombres, que en adelante solo viven afligidos por males. Lo único que pudieron conservar de aquellos bienes es la esperanza.

La "Biblioteca mitológica" (I, 7, 2) menciona que Epimeteo y Pandora fueron padres de Pirra, esposa de Deucalión, hijo de Prometeo. Deucalión y Pirra son considerados por el mito como antepasados de la mayor parte de los .

Etimológicamente se ha dado a la palabra «Pandora» un significado con distintos matices: Paul Mazon y Willem Jacob Verdenius la han interpretado como "el regalo de todos"; sin embargo, para Robert Graves significa "la que da todo" e indica que con ese nombre (Pandora) se adoraba en Atenas y otros lugares a Rea. Según Graves, se estaría ante la precursora griega de la Eva bíblica, puesto que Pandora es quien, como aquella, trae la desgracia a la humanidad.

Para Jean-Pierre Vernant, el rol de mito de Pandora en el texto hesiódico (sobre todo referido a "Trabajos y días") es el de la justificación teológica de la presencia de fuerzas oscuras en el mundo humano. Al intentar Prometeo obtener para los hombres más de lo que debían recibir, arrastra a la humanidad a la desgracia: Zeus da a los mortales un don ambiguo, mezcla de bien y mal, una peste difícil de tolerar pero de la que no se puede prescindir. Es el engaño mismo disfrazado de amante. Pandora es la responsable de comunicar al mundo humano los poderes representados por la estirpe de la Nyx: de ahora en adelante, toda abundancia convive con Ponos, a la juventud sigue Geras, y la justicia contrasta con Eris. La aparición de la mujer implica también la necesidad de un constante afán en las labores agrícolas, puesto que es presentada constantemente como un vientre hambriento, atenta a la hacienda de su prometido, al que acecha con encantos seductores (Apate), y una vez casada instala el hambre en el hogar.










</doc>
<doc id="2427" url="https://es.wikipedia.org/wiki?curid=2427" title="Prometeo">
Prometeo

En la mitología griega, Prometeo (en griego antiguo Προμηθεύς, ‘previsión’, ‘prospección’) es el Titán amigo de los mortales, honrado principalmente por robar el fuego de los dioses en el tallo de una cañaheja, darlo a los hombres para su uso y posteriormente ser castigado por Zeus por este motivo. Prometeo era padre de Deucalión y abuelo de Helén.

Como introductor del fuego e inventor del sacrificio, Prometeo es considerado como el Titán protector de la civilización humana. 

En Atenas, se había dedicado un altar a Prometeo en la Academia de Platón. Desde allí partía una carrera de antorchas celebrada en su honor por la ciudad, en la que ganaba el primero que alcanzaba la meta con la antorcha encendida.

De acuerdo con la mitología más aceptada, Prometeo era hijo de Jápeto y la oceánide Asia o de la también oceánide Clímene. Era hermano de Atlas, Epimeteo y Menecio, a los que superaba en astucia y engaños. No tenía miedo a ninguno de los dioses, y ridiculizó a Zeus y a su poca perspicacia. Sin embargo, Esquilo afirmaba en su "Prometeo encadenado" que era hijo de Gea o Temis. Según una versión minoritaria, el gigante Eurimedonte violó a Hera cuando esta era una adolescente y engendró a Prometeo, lo que causó la furia de Zeus.

Prometeo fue un gran benefactor de la humanidad . Urdió un primer engaño contra Zeus al realizar el sacrificio de un gran buey que dividió a continuación en dos partes: en una de ellas puso la piel, la carne y las vísceras, que ocultó en el vientre del buey y en la otra puso los huesos pero los cubrió de apetitosa grasa. Dejó entonces elegir a Zeus la parte que comerían los dioses. Zeus eligió la capa de grasa y se llenó de cólera cuando vio que en realidad había escogido los huesos. Desde entonces los hombres queman en los sacrificios los huesos para ofrecerlos a los dioses, y comen la carne.

Indignado por este engaño, Zeus prohibió a los hombres el fuego. Prometeo decidió robarlo, así que subió al monte Olimpo y lo cogió del carro de Helios o de la forja de Hefesto, y lo consiguió devolver a los hombres en el tallo de una cañaheja, que arde lentamente y resulta muy apropiado para este fin. De esta forma la humanidad pudo calentarse y utilizarlo para llevar a cabo sacrificios de animales. 

En otras versiones (notablemente, el "Protágoras" de Platón), Prometeo robaba las artes de Hefesto y Atenea, se llevaba también el fuego porque sin él no servían para nada, y proporcionaba de esta forma al hombre los medios con los que ganarse la vida.

Para vengarse por esta segunda ofensa, Zeus ordenó a Hefesto que hiciese una mujer de arcilla llamada Pandora. Zeus le infundió vida y la envió por medio de Hermes al hermano de Prometeo, Epimeteo, en cuya casa se encontraba la jarra que contenía todas las desgracias (plagas, dolor, pobreza, crimen, etcétera) con las que Zeus quería castigar a la humanidad. Epimeteo se casó con ella para aplacar la ira de Zeus por haberla rechazado una primera vez (a causa de las advertencias de su hermano de no aceptar ningún regalo de los dioses; en castigo Prometeo sería encadenado). Pandora terminaría abriendo el ánfora, tal y como Zeus había previsto.
Tras vengarse así de la humanidad, Zeus se vengó también de Prometeo e hizo que lo llevaran al Cáucaso, donde fue encadenado por Hefesto con la ayuda de Bía y Cratos. Zeus envió un águila (hija de los monstruos Tifón y Equidna) para que se comiera el hígado de Prometeo. Siendo este inmortal, su hígado volvía a crecer cada noche, y el águila volvía a comérselo cada día. Este castigo había de durar para siempre, pero Heracles pasó por el lugar de cautiverio de Prometeo de camino al jardín de las Hespérides y lo liberó disparando una flecha al águila. Esta vez no le importó a Zeus que Prometeo evitase de nuevo su castigo, ya que este acto de liberación y misericordia ayudaba a la glorificación del mito de Heracles, quien era hijo de Zeus. Prometeo fue así liberado, aunque debía llevar con él un anillo unido a un trozo de la roca a la que fue encadenado. 

Agradecido, Prometeo reveló a Heracles el modo de obtener las manzanas doradas de las Hespérides.

Sin embargo, en otra versión Prometeo fue liberado por Hefesto tras revelar a Zeus el destino de que si tenía un hijo con la nereida Tetis, este hijo llegaría a ser más poderoso que su padre, quien quiera que este fuera. Por ello Zeus evitó tener a Tetis como consorte y el hijo que tuvo esta con Peleo fue Aquiles quien, tal y como decía la profecía, llegó a ser más poderoso que su padre.

La "Biblioteca mitológica" recoge una versión según la cual Prometeo fue el creador de los hombres, modelándolos con barro. Prometeo se ofreció ante Zeus para cambiar su mortalidad por la inmortalidad de Quirón cuando este fue herido accidentalmente por Heracles, lo que le produjo una herida incurable.

En la mitografía, a Prometeo se le ha relacionado con Loki de la mitología nórdica, quien análogamente es un gigante más que un dios, está asociado con el fuego y es castigado a ser encadenado a una roca y atormentado por un águila, dicho gigante no se debe confundir con el muy representado Loki, originador del fraude.

La historia de Prometeo ha inspirado a muchos autores a lo largo de la historia para referirse a la osadía de los hombres de hacer o poseer las cosas divinas, y los románticos vieron en él un prototipo del demon o genio natural. Algunas de las obras de dichos autores son:














</doc>
<doc id="2429" url="https://es.wikipedia.org/wiki?curid=2429" title="Papa (desambiguación)">
Papa (desambiguación)

Papa hace referencia a varios artículos:

Se conoce como papa a varias especies de plantas comestibles originarias de América del Sur:






</doc>
<doc id="2442" url="https://es.wikipedia.org/wiki?curid=2442" title="Quake (videojuego)">
Quake (videojuego)

Quake es un videojuego de disparos en primera persona publicado por id Software el 22 de junio de 1996. Introdujo algunos de los mayores avances en el género de los videojuegos en 3D: utiliza modelos tridimensionales para los jugadores y los monstruos en vez de "sprites" bidimensionales; y el mundo donde el juego tiene lugar está creado como un verdadero espacio tridimensional, en vez de ser un mapa bidimensional con información sobre la altura representada en tres dimensiones. También incorporó la utilización de los mapas de luz y las fuentes de luz en tiempo real, descartando la iluminación estática basada en sectores de los juegos anteriores. Ofreció, en su tiempo, uno de los motores físicos más realistas programados para un videojuego hasta la fecha. Muchos creen que proporcionó la plataforma para la revolución de las tarjetas gráficas 3D independientes, «GLQuake» fue la primera aplicación que, en esos días, demostró la capacidad verdadera del chipset Voodoo Graphics de 3DFX. El impacto del motor Quake engine puede aún sentirse en la actualidad.

La mayoría de la programación del motor del Quake fue realizada por John Carmack. Michael Abrash, un especialista en optimización del rendimiento de los programas, fue contratado para ayudar a que el motor de representación por software fuera posible teniendo en cuenta la velocidad. La banda sonora y efectos de sonido fue compuesta por Trent Reznor.

Quake y sus secuelas "Quake II" y "Quake III Arena" han vendido más de 4 millones de copias juntos.

Además de que este juego se planifica lanzar una versión para PlayStation pero luego se canceló por razones que se desconocen.

En un futuro no lejano, científicos militares dan sus primeros pasos en la tecnología de la teletransportación, creando los Slipgates (Portales-Pasadizos), artefactos que permiten materializar organismos desde y hasta puntos espaciales distantes. Sin que ellos pudieran prevenirlo, estos aparatos eran capaces —además— de unir dimensiones, produciendo que una de estas instalaciones construyera el puente entre este universo y una realidad plagada de demonios (alusiva al infierno), iniciándose una invasión a gran escala que amenaza con arrasar el planeta. Pronto, los soldados de las bases militares terrestres caen derrotados ante las gigantescas hordas.

En este punto, y sin una introducción gráfica, el jugador toma el rol del último superviviente de un batallón, un anónimo soldado (posteriormente llamado Ranger, en reseña a las fuerzas especiales militares) quien cruza el primer portal e inicia la odisea de derrotar al mayor de estos demonios, un enemigo sumamente poderoso solo conocido por el nombre clave “Quake” (que posteriormente se revelará como Shub-Niggurath, oscura diosa pagana llena de maldad).

Armado solo con una escopeta y un hacha, este soldado iniciará numerosos viajes interdimensionales entre nuestro mundo y el demoníaco, buscando las salidas de los lugares en que aparezca teletransportado, a fin de acercarse cada vez más a Shub-Niggurath y cumplir su misión de exterminarlo.

Cabe destacar que esta línea argumental —similar a la utilizada para el videojuego Doom— fue desechada posteriormente por ID Software para la realización de las secuelas de la serie, particularmente Quake II y Quake IV, cambiando radicalmente la historia hacia una trama no paranormal, sino alienígena. Entre los fanáticos de Quake, aún se ven solicitudes a esta empresa de unir ambas historias en algún punto, o bien desarrollar una secuela que siga la campaña del juego original, esta vez con un motor gráfico de última generación.














Enemigos no incluidos en la versión final del juego.

"El 24 de febrero de 1996, ID Software presentó" Qtest, "la primera prueba pública que exhibiría la tecnología utilizada en el videojuego. En dicho lanzamiento fueron introducidos tres monstruos que, en definitiva, no vieron la luz en la edición final de Quake. Sin embargo, sus modelos y scripts han sido utilizados en las numerosas modificaciones que los aficionados han efectuado al juego mismo".




Las armas en "Quake" (excepto el hacha) requieren de munición, la cual se encuentra dispersa por todo el nivel de juego, en ocasiones ubicada en lugares secretos. A diferencia de muchos juegos FPS posteriores a Quake, el jugador no necesita recargar su arma, la cual puede disparar en forma continua hasta agotar la munición respectiva. Algunos enemigos son particularmente débiles frente a un determinado tipo de arma, y otros son resistentes a la misma, lo cual entrega un toque de estrategia para el jugador.









En cada mapa puede encontrarse distinta maquinaria que el jugador usará a fin de encontrar la salida de cada nivel, o bien elementos ambientales con que debe interactuar. Muchos de estos objetos, a pesar de ser considerados además en el videojuego Doom (serie)|Doom, formaron un estándar a seguir por los futuros FPS, incluso hasta el día de hoy. Los más recurrentes son:













"Quake" incluye un modo multijugador para jugar a través de una red de área local o de Internet con o contra otros humanos. El juego en red utiliza un modelo cliente/servidor, donde el juego actual solo se ejecuta en el servidor y todos los jugadores «acceden» a él para participar. Dependiendo de la ruta específica que el cliente tenga al servidor los diferentes clientes obtendrán diferentes tiempos de ping. Cuanto más baja sea la latencia (tiempo de ping), más suavemente será tus movimientos dentro del juego, y más fácil será apuntar correctamente y puntuar. Alguien jugando en el PC servidor obtiene una ventaja sustancial debido al lag prácticamente nulo.

Hay dos paquetes oficiales de expansión lanzados para Quake.

Los paquetes de expansión continúan donde el primer juego terminó, incluyen todas las mismas armas, power-ups, monstruos y arquitectura / atmósfera gótica, y continúan / terminan la historia del primer juego y su protagonista. Un tercer paquete no oficial de expansión, Abyss of Pandemonium, fue desarrollado por Impel Development Team, publicado por Perfect Publishing, y publicado el 14 de abril de 1998; Una versión actualizada, versión 2.0, titulada Abyss of Pandemonium - The Final Mission fue lanzado como freeware. En honor del quinto aniversario de Quake, MachineGames, un estudio de ZeniMax Media, propietarios actuales de Quake ,lanzó en línea un nuevo paquete de expansión gratuito llamado Episode 5: Dimensión of the past.

"Quake Mission Pack No. 1: Scourge of Armagon" es el primer mission pack, lanzado el 28 de febrero de 1997. Desarrollado por Hipnotic Interactive, presenta tres episodios divididos en diecisiete nuevos niveles de un solo jugador (tres de los cuales son secretos), un episodio Nuevo nivel multijugador, una nueva banda sonora compuesta por Jeehun Hwang, y características de juego no presentes originalmente en Quake, incluyendo estructuras giratorias y paredes rompibles. A diferencia del juego Quake principal y Mission Pack No. 2, Scourge elimina el cubo del episodio, requiriendo que los tres episodios se reproduzcan secuencialmente. Los tres nuevos enemigos que se incluyen son los Centroides, grandes escorpiones cibernéticos con pistolas de clavos; Gremlins, pequeños goblins que pueden robar armas y multiplicarse alimentándose de cadáveres enemigos; Y Spike Mines, orbes flotantes que detonan cuando están cerca del jugador. Las tres nuevas armas incluyen el Mjolnir, un gran martillo emisor de relámpagos; El cañón láser, que dispara rebotes de energía; Y el lanzador de minas de proximidad, que dispara granadas que se adhieren a las superficies y detonar cuando un oponente se acerca. Los tres nuevos power-ups incluyen el Cuerno de Conjuración, que convoca a un enemigo para proteger al jugador; El Escudo de la Empatía, que reduce a la mitad el daño recibido por el jugador entre el jugador y el enemigo atacante; Y el Wetsuit, que hace que el jugador invulnerable a la electricidad y permite al jugador permanecer bajo el agua durante un período de tiempo mayor. 

"Quake Mission Pack No. 2: Dissolution of Eternity" es el segundo mission pack, lanzado el 31 de marzo 1997. Desarrollado por Rogue Entertainment, presenta dos episodios divididos en quince nuevos niveles de un solo jugador, un nuevo nivel multijugador, una nueva banda sonora , Y varios nuevos enemigos y jefes. En particular, el paquete carece de niveles secretos. Los ocho nuevos enemigos incluyen las anguilas eléctricas, los espadachines fantasma, los ogros de la Multi-Granada, el infierno Spawn, los Wraths, los guardianes (guerreros egipcios egipcios resucitados), las momias y las estatuas de varios enemigos que pueden venir a la vida. Los cuatro nuevos tipos de jefes incluyen a hombres de lava, Overlords, Wraths grandes, y un dragón que guarda el "convertidor de energía temporal". Los dos nuevos power-ups incluyen el Anti Grav Belt, que permite al jugador saltar más alto; Y el Power Shield, que reduce el daño que el jugador recibe. En lugar de ofrecer nuevas armas, el paquete de misión le da al jugador cuatro nuevos tipos de munición para las armas existentes, como "clavos de lava" para el Nailgun, cluster grenades para el lanzador de granadas, cohetes que se dividen en cuatro en una línea horizontal para el cohete Lanzador, y las células plasmáticas para el rayo, así como un gancho para ayudar con moverse alrededor de los niveles..

A finales de 1996, id Software lanzó VQuake, un port official del motor Quake para soportar la renderización acelerada de hardware en tarjetas gráficas usando el chipset Rendition Vérité. Aparte del beneficio esperado de un rendimiento mejorado, VQuake ofreció numerosas mejoras visuales sobre el original Quake. Cuenta con un completo color de 16 bits, filtrado bilineal (reducción de la pixelación), iluminación dinámica mejorada, anti-aliasing opcional y claridad de código fuente mejorada. Como su nombre implicaba, VQuake era un puerto propietario específicamente para el chipset Vérité; La aceleración 3D del consumidor estaba en su infancia en ese momento, y no había una API 3D estándar para el mercado de consumo. Después de completar VQuake, John Carmack prometió nunca volver a escribir un puerto propietario, citando su frustración con la API Speedy3D de Rendition.

Ir al Artículo: QuakeWorld

Para mejorar la calidad del juego en línea, Id Software lanzó QuakeWorld el 17 de diciembre de 1996, una versión de Quake que incluía un código de red ampliamente mejorado, incluyendo la adición de la predicción del lado del cliente. El código de red de Quake original no mostraría al jugador los resultados de sus acciones hasta que el servidor enviara una respuesta reconociéndolas. Por ejemplo, si el jugador intentaba avanzar, su cliente enviaría la solicitud para avanzar al servidor y el servidor determinaría si el cliente era realmente capaz de avanzar o si se topaba con un obstáculo, como una pared O de otro jugador. El servidor respondería entonces al cliente, y solo entonces el cliente mostraría el movimiento al reproductor. Esto estaba bien para jugar en una LAN, con un ancho de banda alto, pero la latencia sobre una conexión a Internet de acceso telefónico es mucho mayor que en una LAN, y esto causó un retraso notable entre cuando un jugador trataba de actuar y Cuando esa acción era visible en la pantalla. Esto hizo que la jugabilidad fuera mucho más difícil, especialmente porque la naturaleza impredecible de Internet hacía que la cantidad de retraso variaba de un momento a otro.

El 22 de enero de 1997, id Software lanzó GLQuake. Esto fue diseñado para usar la API OpenGL 3D para acceder a las tarjetas de aceleración de gráficos 3D de hardware para rasterizar los gráficos(Modo Software), en lugar de tener la CPU de la computadora en cada píxel. Además de velocidades de fotogramas más altas para la mayoría de los jugadores, GLQuake proporciona modos de resolución más alta y filtrado de texturas. GLQuake también experimentó con reflejos, agua transparente e incluso sombras rudimentarias. GLQuake viene con un controlador que permite que el subconjunto de OpenGL utilizado por el juego funcione en la tarjeta gráfica 3Dfx Voodoo, la única tarjeta de nivel de consumidor en el momento capaz de ejecutar GLQuake. Anteriormente, John Carmack había experimentado con una versión de Quake específicamente escrita para el chip Rendition Vérité utilizado en la tarjeta PCI 3D Blaster de Creative Labs. Esta versión se había reunido con solo un éxito limitado, y Carmack decidió escribir para APIs genéricos en el futuro en lugar de la adaptación de hardware específico.

El 11 de marzo de 1997, id Software lanzó WinQuake, una versión del motor que no utilizaba OpenGL (sino que utilizaba el modo Software) diseñado para funcionar bajo Microsoft Windows; El Quake original se había escrito para el DOS, permitiendo el lanzamiento de Windows 95, pero no podía funcionar bajo sistemas operativos basados Windows NT porque requería el acceso directo al hardware. WinQuake accedió a hardware a través de APIs basadas en Win32 como DirectSound, DirectInput y DirectDraw que se admitían en Windows 95, Windows NT 4.0 y versiones posteriores. Al igual que GLQuake, WinQuake también permite modos de video de mayor resolución. Esto eliminó la última barrera a la popularidad generalizada del juego. En 1998, LBE Systems y Laser-Tron lanzaron Quake: Arcade Tournament Edition en las arcades en cantidades limitadas

El 24 de junio de 2016, para celebrar el 20 aniversario de Quake, MachineGames desarrolló un paquete de misiones. Cuenta con 10 nuevos niveles de un solo jugador y un nuevo nivel multijugador, pero no utiliza nuevas incorporaciones de Scourge of Armagon y Dissolution of Eternity. Considerado el episodio que tiene lugar entre el juego principal y la expansiónes

Después de la salida de Sandy Peterseny la de los empleados ID software restantes decidieron cambiar sustancialmente la dirección de la temática para Quake II, haciendo el diseño más tecnológico y futurista, en lugar de mantener el foco en la fantasía Lovecraftiana. Quake 4 siguió los temas de diseño de Quake II, mientras que Quake III Arena mezcló estos estilos o sea tenía un ajuste paralelo que albergaba varios "id all-stars" de varios juegos como personajes jugables. Los ajustes mixtos se produjeron porque Quake II originalmente comenzó como una línea de juego independiente aunque Los diseñadores de id se vieron obligados a recurrir al sobrenombre del proyecto de "Quake II" debido a la falta de obtener derechos sobre el título que querían. Ya que cualquier secuela del Quake original ya había sido vetada. En junio de 2011, John Carmack hizo un comentario extraño de que id Software estaba considerando un remake para el "... mezcló el mundo de "Cthulhu-ish" de Quake 1 y reinició en esa dirección". También hubo otro juego lanzado llamado "Quake Live", que es el último juego de la serie. En E3 2016, Quake Champions fue anunciado en la conferencia de prensa de Bethesda. El juego será un shooter multijugador en el estilo de Quake 3 Arena y será lanzado exclusivamente para Windows.

El 20 de julio de 2016, Axel Gneiting, un empleado de id Tech responsable de la implementación de la ruta de procesamiento de Vulkan al motor id Tech 6 utilizado en Doom (2016), lanzó un puerto con la api Vulkan para Quake 1 llamado vkQuake bajo la licencia GPLv2.

Cuando un cliente se conecta al servidor, este le envía el estado actual de todas las entidades que componen el juego: misiles, otros jugadores y objetos estáticos. Mientras el juego está corriendo, la máquina cliente suministra al servidor con la dirección en que está mirando el jugador, este computa esta dirección y ejecuta la lógica del juego, enviando al cliente una actualización delta de los cambios en las entidades. En cada cuadro se tienen que enviar todos los objetos que han cambiado con sus nuevos valores... así como los que no han cambiado con la información mínima. Como medida de optimización, en realidad solo se envían aquellas entidades que entrarán dentro del corte de visualización del cliente, de modo que no tenga que viajar por la red datos sobre entidades que de todos modos no van a ser vistas por el cliente.

El juego se realiza en realidad sobre el servidor, y que los clientes son meros espectadores del resultado.

Más tarde, cuando los clientes tienen todos los datos del cuadro, el programa se limita a recoger la información de los polígonos, y pasárselo a las bibliotecas OpenGL (o a los representadores por software en las versiones originales) que se encargan de dibujar estos polígonos sobre pantalla con las texturas dadas.

En la actualidad, se está desviando este modelo a uno con más cálculos en el cliente, pues se ha encontrado que dada las malas calidades de las redes informáticas es muy difícil que este sistema funcione sin saltos bruscos debidos a retrasos en los paquetes de datos. Para ello se está tratando de añadir capacidades de predicción en el cliente de modo que este se pueda “adelantar” al mensaje del servidor y realizar por el mismo muchas de las cosas que se prevé que el servidor va a pedir. Esta técnica se llama predicción de movimiento.

Se llama "mods" a los módulos que permiten cambiar la apariencia o la naturaleza de los juegos corriendo en el motor de Quake. Originalmente los "Mods" se limitaban a modificar la lógica del juegos, los gráficos y el sonido, pero tras el lanzamiento del código fuente de Quake, fue posible también modificar el motor del juego. 
La mayor parte de los "Mods" parten de los diseños originales del juego, y los utilizan para crear otra experiencia de juego. Pueden añadir nuevas formas de juego (como Capturar la Bandera, Asalto a la Fortaleza o rol) o añadir pequeñas modificaciones como un arma nueva o sonidos de distingo gusto. Una pequeña minoría modifica la totalidad de la experiencia de juego, son las "Total Conversion" donde el juego del "Mods" es de otro género al del juego original.

El primer mod de Quake importante fue Threewave Capture the Flag (CTF, Captura la Bandera), principalmente desarrollado por Dave 'Zoid' Kirsch. Se basa en el típico juego en el que dos equipos (rojo y azul) deben competir intentando capturar la bandera del equipo contrario, aunque algunos mapas estaban preparados para soportar hasta a cuatro equipos diferentes (rojo, azul, verde y amarillo). Captura la Bandera se convirtió en un modo estándar de juego incluido en la mayoría de los juegos multijugador aparecidos después de Quake, a parte del modo Combate a Muerte (Deathmatch) introducido por primera vez por Doom.

El popular mod «Asalto a la Fortaleza» TeamFortress para QuakeWorld consiste en un modo de juego Captura la Bandera, pero con un sistema de clases para los jugadores. Los jugadores adoptan un rol (la clase) y así obtienen unas habilidades (y restricciones) propias de ese papel que desempeña. Por ejemplo, la clase "Soldado" normal y corriente tiene una armadura media, velocidad media y una equilibrada selección de armas y granadas, mientras que la clase "Explorador" está débilmente protegido, es muy rápido, tiene un detector para los enemigos cercanos, pero tiene unas armas ofensivas muy débiles.

Junto con la gran capacidad de modificación de los parámetros de juego, se une la posibilidad de creación de mapas (escenarios del juego) para ser utilizados directamente como misiones individuales o unidos para formar una campaña, o bien escenarios multiplayer en que se desarrollan los llamados deathmatch (combates entre jugadores), ya sea en línea o contra los bots (jugadores virtuales controlados por la inteligencia artificial).
En un principio, crear mapas para Quake era tarea de gente especializada en el tema, al no existir entonces algún software que simplificara esta labor. Posteriormente, diversos programadores crearon utilidades que permitieron al público masivo dar rienda suelta a su imaginación, sin necesidad de conocimientos especializados de computación.

En este sentido, muchos programas obtuvieron popularidad entre los aficionados, como es el caso de Quark (Quake Army Knife). Sin embargo, el más conocido de todos es Worldcraft, creado por Ben Morris en el mes de septiembre de 1996. Este editor de mapas se convirtió en uno de los favoritos de los fanáticos no solo por ser de los primeros en ver la luz, sino además por su simplicidad, intuitiva interfaz, compatibilidad con elementos creados a medida y rapidez de compilación. Tal nivel de genialidad consiguió este programa, que en julio de 1997 su creador fue contratado por la compañía de videojuegos Valve –adquiriendo asimismo los derechos intelectuales sobre el editor– para desarrollar el editor de mapas oficial de su masivamente famoso juego "Half-Life", pasando ahora a llamarse Hammer (Worldcraft en su versión 3.0). A pesar que esta nueva versión del editor ya no era compatible con Quake, programadores independientes lanzaron un parche que permite adaptar este juego con todas las mejoras que le fueron introducidas al otrora Worldcraft.

Junto con este famoso programa, es destacable también la gran utilidad que hasta hoy presenta el editor gráfico Wally. Este programa permite crear texturas para Quake (junto a otros juegos más, como el mismo Half-Life), pudiendo soportar la creación directa de los gráficos al más puro estilo Paint, como también la opción de importar archivos de imágenes (en numerosos formatos, incluyendo tga) que el mismo editor se encarga de traducir a la paleta de colores de Quake. Con esto –considerando las limitaciones colorativas del motor- los creadores de mapas pueden incluir en sus escenarios cuadros famosos, afiches publicitarios, murallas o decorados realistas, hasta incluso implementar fotografías propias que serán visibles dentro del juego.

Finalmente, dentro de la edición de Quake se pueden mencionar programas como Qped II, que presenta una amplia gama de utilidades, tales como compilar trabajos en archivos ".pak" (paquetes básicos del juego, que contienen desde la configuración hasta los mapas), observar los modelos tridimensionales de los personajes y extraer las texturas de los mapas, simplificando con creces la labor de cualquier aficionado a la edición de Quake.

Un detalle poco conocido de la historia de los juegos de PC: La primera mención a Quake estaba en el primer juego de id Software, "Commander Keen" para PC, que fue lanzado en diciembre de 1990. El siguiente texto (el original estaba en inglés, se presenta aquí una versión traducida) estaba incluido en el archivo "preview.ck1", que tiene fecha del 10 de diciembre de 1990:

Quake fue elegido como el título en el que id Software estaba trabajando poco después del lanzamiento de Doom 2. Las primeras noticias describían a Quake como un personaje parecido a Thor que portaba un martillo gigante y era capaz de derribar a las personas tirando el martillo (junto con cinemáticas inversas en tiempo real). Las primeras capturas de pantalla mostraban ambientes medievales y dragones. El plan era que el juego tuviera más elementos de los JDR. Sin embargo el trabajo en el motor era muy lento, puesto que Carmack no solo desarrollaba un motor totalmente en 3D, sino también un sistema para redes TCP/IP (Carmack dijo posteriormente que debía haber desarrollado dos proyectos independientes para cada una de esas cosas). Al final el juego estaba muy lejos de sus intenciones originales, y presentaba un sistema de juego similar al del Doom 2. Adorado por la comunidad de jugadores, pronto destronó a los títulos FPS previos y revolucionó la manera en la que los juego multijugador fueron desarrollados. Las más importantes revistas de la época coincidieron en declarar que Quake marcaba "el año cero" en lo que se refiere a juegos asistidos por ordenador.

Antes del lanzamiento de Quake, el 24 de febrero de 1996, ID Software lanzó un Test solamente probado para DeathMatch llamado Qtest1 para así demostrar el magnífico motor gráfico 3D.
Aquel Qtest1 se pueden observar aquellas primitivas ideas del armamento de esta rara versión. La Shotgun, la SuperShotgun, o la RocketLauncher están intactas como la conoces en su versión final, pero las rarezas son la Nailgun y la SuperNailgun. Los niveles los clásicos DM: (DM1, DM2, DM3) aquí llamados: Test1, Test2, Test3. El Hud (barra de estado) con un diseño único,
enemigos los mismos pero con un diferentes "skin" y varios cambios como los ogros lanzaban Nails, el shalrath era un diferente modelo que caminaba en dos pies en vez de 3 de una araña, incluso 3 enemigos nuevos Vomitus (Monstruo que vomita tarbabys) Dragón (parecido a de Dissolution Of Eternity) Serpent (más que todo es como una serpiente voladora).

Un mes antes de la aparición definitiva de "Quake", apareció un prelanzamiento, casi todo era igual a excepción del último nivel que era diferente y los shalrath lanzaban bolas de lava.

Para mejorar la calidad del juego en línea, id Software lanzó "QuakeWorld" en 1996, una versión de Quake que incluía un código de red significativamente mejorado incluyendo predicción de movimiento. El código de red del Quake original fue diseñado para jugar en una LAN - una conexión de gran ancho de banda y baja latencia. Jugar a Quake en Internet a través de una conexión telefónica era terriblemente lento, con parones desconcertantes y retrasos mientras que el cliente esperaba por el servidor, y viceversa.

Con la ayuda de la predicción de movimiento, el código de red de QuakeWorld era mucho más adecuado para los jugadores con conexión telefónica con altos tiempos de ping. Los parámetros del código de red podía ser ajustados por el usuario, por lo que QuakeWorld ejecutaba bien para los usuarios con baja latencia (también conocidos como Low Ping Bastards (Bastardos con Ping Bajo) o LPB's) así como para jugadores con alta latencia (algunas veces llamados High Ping Weenies (salchichitas con Ping Alto) o HPB's). El popular mod Team Fortress está basado completamente es la plataforma QuakeWorld.

En 1996 apareció la conversión de Quake a Linux que incluyó un robo de código y algunos parches enviados a id Software antes de que se convirtiera en una conversión oficial. 1997 vio algunos esfuerzos de conversión más, con una conversión a IRIX, llamada SGI Quake realizada por Ed Hutchins en el SGI O2. SGI Quake tenía sistemas de representación mediante OpenGL y mediante software. También en 1997 aparecieron una conversión a MacOS, realizada por MacSoft, y una conversión a Solaris sobre Sparc. Muchas más conversiones fueron realizadas después de que se lanzara el código fuente.

El 4 de agosto de 2007, Quake fue puesto a la venta a través de la plataforma de distribución digital de videojuegos Steam, junto con una colección de otros títulos clásicos de id Software.

El código fuente de los motores de "Quake" y "QuakeWorld" fueron licenciados mediante la GPL en 1999. Los mapas, objetos, texturas, sonidos y otros trabajos creativos de id Software permanecieron bajo su licencia original. La versión shareware de Quake es todavía libremente redistribuible y utilizable por el código del motor liberado mediante la GPL. Debes comprar una copia original del Quake para obtener la versión registrada del juego que incluye más episodios de un solo jugador y los mapas combate a muerte.

Basados en el éxito del primer Quake, id publicó posteriormente "Quake II", "Quake III Arena" y "Quake IV", este último fue desarrollado por Raven Software utilizando el motor Id Tech 4.

Es también interesante destacar que "Quake" es el juego responsable de la aparición del fenómeno machinima en el cual las películas son realizadas mediante los motores gráficos de los juegos gracias a demos de Quake editadas como Ranger Gone Bad y Blahbalicious.

En Quake hay varias maneras de hacer que el personaje se mueva mediante un salto. Algunas de ellas aprovechan errores de software en el motor de física, en vez de utilizar las características del juego. Se debe destacar que algunas de estas "características" han sido incluidas en algunos juegos de acción en primera persona posteriores, específicamente aquellos que utilizan el motor del Quake, como Half-Life.

Para realizar un "rocket jump" (abreviado "RJ"), el jugador utiliza un lanzador de cohetes, apunta hacia abajo cerca de sus pies, salta e inmediatamente dispara un cohete. La explosión del cohete propulsa al jugador hasta alturas y distancias increíbles. El verdadero efecto del "rocket jump" solo se puede apreciar cuando el jugador no está en el suelo (es decir, que ha saltado antes de lanzar el cohete). Si el jugador estaba en el suelo cuando la explosión sucede el resultado es que el jugador no es impulsado tan lejos y además recibe bastante daño debido a la onda expansiva.

El "rocket jump" se puede realizar en cualquier juego de Quake. Los jugadores realizan el rocket jump para alcanzar los ítems más rápidamente, salvarse de la lava, evitar oponentes, o para encontrar lugares de camping. Algunos jugadores utilizan incluso el lanzagranadas (o el BFG en Quake II) para crear explosiones que intensifiquen el "rocket jump". Usar granadas para ayudar en el vuelo se denomina "grenade jumping".

Pueden realizarse "rocket jumps" increíblemente altos si el jugador está en posesión del "Quad Damage" (que provoca cuatro veces más daño) y el Pentagrama de Protección (que proporciona invulnerabilidad).

El "strafe jumping" permite que el jugador se mueva más rápido y salte más lejos. Se realiza saltando mientras uno se mueve hacia adelante (o hacia atrás) y desplazándose lateralmente a derecha o izquierda ("strafing").El "strafe jumping" se puede realizar en Quake, Quake II, Quake III Arena y Quake IV. Es un fallo relacionado con la aceleración aérea. IdSoftware no quiso integrarlo en Doom 3.

Para incrementar tu velocidad con el "strafe jumping" debes estar primero moviéndote hacia adelante o hacia atrás. Entonces, simultáneamente saltas y te desplazas lateralmente en una dirección, y giras la vista ligeramente en la misma dirección (para rotar avatar en el juego). Alternando entre los desplazamientos a la izquierda y a la derecha tiene como resultado un movimiento casi en línea recta a gran velocidad, y se ha convertido en una técnica utilizada ocasionalmente en los enfrentamientos de Quake.

Un lugar donde el "strafe jumping" puede ser útil es en el mapa de Quake dm2, donde puede hacer un "strafe jump" para coger la armadura roja a través de la lava. Normalmente, el jugador debe apretar un interruptor cercano para extender un puente sobre la lava, debido a que lava es exactamente una unidad más larga de lo que el jugador puede saltar normalmente. Sin embargo, con el aumento de velocidad proporcionado por el "strafe jump", un jugador experimentado puede saltar lo que supuestamente es una distancia imposible. Pero el "strafe jump" es de utilización limitada en el modo combate a muerte, ya que es más inseguro que correr y saltar y mucho menos efectivo que el "rocket jumping".

El "circle jumping" hace uso del hecho de que los jugadores pueden controlar su movimiento mientras están en el aire. Básicamente, un "circle jump" es simplemente un giro de 180 grados mientras estás en el aire. Este salto es utilizado principalmente en QuakeWorld, pero puede ser hecho en el Quake normal, a pesar de que es mucho más difícil.

Una versión diferente del circle jump es utilizado en Quake II donde los jugadores saltan en un arco manipulando la vista para saltar mayores distancias.

Un salto doble es un fallo que permite que el jugador salte dos veces seguidas en mitad del aire. Para realizar un doble salto el jugador debe haber saltado directamente desde un borde y entonces saltar de nuevo. El salto doble puede solo ser realizado en Quake II en las últimas versiones, y en las modificaciones para QuakeWorld que soporten el "jawnmode". En el mapa Q2DM1, puedes realizar este salto donde recoges el megahealth. Puedes alcanzar el sitio desde el backpack realizando un doble salto y luego saltando normalmente por el megahealth.

El salto doble ha sido incluido intencionalmente en juegos posteriores, incluyendo "Unreal Tournament".

El bunnyhopping es un método para incrementar la velocidad de movimento mediante saltos continuos. Funciona utilizando un fallo en el motor de Quake. Normalmente los jugadores está limitados a una velocidad máxima mientras andan por el suelo. Sin embargo, este límite impuesto no tiene efecto mientras el jugador está en el aire. Además, girar mientras estás en movimiento hace que la entidad del jugador adquiera una aceleración. Estos dos hechos permiten mantener e incrementar la velocidad aérea en saltos sucesivos mientras se gira suavemente. Cuando vuelves a andar por el suelo de nuevo deceleras a la velocidad máxima de correr.

El fallo es que el acto de saltar no es considerado «tocar el suelo». Para ser más preciso, es posible iniciar el próximo salto mientras aún estás en el aire, y por lo tanto el estado no-en-el-suelo del jugador nunca se apaga. Si el jugador salta continuamente el motor no registrará que el jugador toca el suelo, y el movimiento del jugador se verá gobernado por la aceleración aérea (sin límites en su velocidad máxima).

Para empezar un bunnyhopping, realiza un "strafe jump" y salta continuamente mientras te mueves hacia adelante. Empezarás a acelerar más allá de la velocidad de correr normal. El secreto para mantener un bunny hop es pulsar el botón de salto (normalmente la barra espaciadora) mientras aún estás en el aire. El juego te hará saltar nada más aterrizar, y por lo tanto mantendrás tu velocidad aérea y no serás considerado fuera del estado no-en-el-suelo. El bunnyhopping es posible en QuakeWorld, Quake II y Quake III Arena.

En QuakeWorld puedes hacer uso del control aéreo para girar en las esquinas muy rápido, es parecido al "circle jump". En vez de correr alrededor de la esquina en el suelo lentamente el jugador salta y utiliza sus teclas de movimiento para rotarse un cuarto de circunferencia alrededor de la esquina en mitad del aire. En Quake II no hay prácticamente control aéreo, por lo que solo puedes moverte hacia adelante. Es también útil en QuakeWorld cuando se realiza un "speed jump" (ver más abajo) para mantener tu velocidad de movimiento.

El speed jump es otro salto que permite al jugador moverse más rápidamente y, como el "rocket jump", se sirve de las fuerzas explosivas. Para realizar un speed jump, el jugador coge el lanzador de cohetes, se sitúa cerca de una pared, dispara el cohete hacia la pared, se gira rápidamente para acabar mirando en contra de la pared y salta hacia adelante con la ayuda de la onda expansiva del cohete. Muchos jugadores aderezan esto con un strafe jump o un bunnyhopping para mantener la velocidad ganada mediante esta acción. Los jugadores utilizan esta mejora de velocidad extrema para sorprender a sus oponentes o completar niveles de un solo jugador en tiempos récord. Este salto fue desarrollado por la comunidad de QuakeWorld y puede realizarse también en Quake II. Sin embargo, como no puedes controlar tu movimiento en el aire en Quake II, no puedes girar en las esquinas.

Este salto proviene del Quake III Arena. También es posible realizarlo en QuakeWorld bajo el "jawnmode" utilizando el "Super Nailgun". Dispara el SNG debajo de ti mientras estás muy cerca de una pared y salta para "trepar" por la pared.

En el juego Quake el "quad" proporciona cuatro veces más potencia de fuego. Esto te permite desmembrar a tus enemigos mucho más fácilmente. En Quake 3 el quad fue reducido a solo 3 veces la potencia de fuego normal, convirtiéndose en un mal uso del nombre.

Un grupo de jugadores expertos de Quake grabaron demos de los niveles de Quake completándolo en nuevas marcas de tiempo y las editaron en una demo de velocidad de Quake continua de 19 minutos y 49 segundos llamada "Quake done Quick" (QdQ, Quake hecho Rápido). El récord fue más tarde mejorado en "Quake done Quicker" (QdQr, Quake hecho más Rápido) hasta 16:35 y por último el increíble "Quake done Quick with a Vengeance" (QdQwav, Quake hecho Rápido con una Venganza, los títulos parafrasean los títulos originales de la trilogía de Jungla de cristal) hasta 12:23 en el nivel Nightmare (Pesadilla, el más difícil de todos). Se han realizado speed runs parecidos para los conjuntos de misiones de "Quake", "Quake II", Doom Ultimate Doom (16:05), Doom II (21:16) y Half-Life (31:00).



La popular «LAN party» estadounidense QuakeCon tiene sus raíces también en el juego. Esta convención para jugadores fue diseñada en un principio para que los fanes pudieran reunirse cada año y competir en una red de área local en condiciones igualadas sin los retrasos de las conexiones a Internet y las pérdidas de paquetes que dificultan el juego.

De acuerdo con David Kushner en Masters of Doom, id Software lanzó una versión de shareware minorista de Quake antes de la distribución completa del videojuego por parte de GT Interactive. Estas copias shareware se pueden convertir en versiones completas a través de contraseñas compradas por teléfono. Sin embargo, Kushner escribió que "los jugadores no perdieron el tiempo pirateando el shareware para desbloquear la versión completa del juego de forma gratuita". Este problema, combinado con la escala de la operación, llevó a id Software a cancelar el plan. Como resultado, la compañía se quedó con 150,000 copias shareware sin vender almacenadas. La empresa dañó Quake 'Las ventas iniciales de GT Interactive provocaron su venta minorista y se perdieron la temporada de compras navideñas. Tras el lanzamiento completo del juego, Kushner comentó que sus primeras "ventas fueron buenas, con 250,000 unidades vendidas, pero no un fenómeno como Doom II".

En los Estados Unidos, Quake ocupó el sexto puesto en las listas de ventas mensuales de juegos de computadora, según PC Data para noviembre y diciembre de 1996. Su edición de shareware fue el juego de computadora número seis más vendido del año 1996 en general, mientras que su SKU minorista obtuvo el lugar 20. Permaneció en el Top 10 mensual de PC Data de enero a abril de 1997, pero estuvo ausente en mayo. Durante sus primeros 12 meses, Quake vendió 373,000 copias al por menor y ganó $ 18 millones en los Estados Unidos, según PC Data. Sus ventas minoristas finales para 1997 fueron de 273,936 copias, lo que la convirtió en el 16º vendedor de juegos de computadora más grande del país en el año.

Las ventas de Quake llegaron a 550,000 unidades solo en los Estados Unidos en diciembre de 1999. A nivel mundial, vendió 1,1 millones de unidades para esa fecha.

Quake fue aclamado por la crítica en la PC. La crítica de los sitios web de revisión GameRankings y Metacritic dieron a la versión original de PC un 93.22% y 94/100, el port para Nintendo 64 un 76.14% y 74/100, y la versión de Sega Saturn un 64.50%. Un crítico de Next Generation elogió la física 3D realista del juego y los efectos de sonido realmente desconcertantes. Major Mike de GamePro dijo Quake había sido promocionado en exceso, sin embargo es excelente, particularmente su uso de su avanzado motor 3D. También elogió los efectos de sonido, la música atmosférica y los gráficos, aunque criticó que los polígonos utilizados para construir los enemigos son demasiado obvios a corta distancia.

Menos de un mes después del lanzamiento de Quake (y un mes antes de que realmente revisaran el videojuego), Next Generation lo incluyó como el número 9 en sus "100 mejores juegos de todos los tiempos", diciendo que es similar a Doom pero admite un máximo de ocho jugadores en lugar de cuatro. En 1996, Computer Gaming World enumeró "telefragged" como el #1 en su lista de "las 15 mejores maneras de morir en juegos de computadora". En 1997, en los Game Developers Choice Awards le otorgaron a Quake tres premios destacados por los mejores efectos de sonido, mejor música o banda sonora y el mejor juego en línea/Internet.



</doc>
<doc id="2445" url="https://es.wikipedia.org/wiki?curid=2445" title="Quetzalcóatl">
Quetzalcóatl

Quetzalcóatl (en nahuatl: "Quetzalcōhuātl" ‘Serpiente Emplumada’, de "quetzalli" ‘pluma’ y "cōhuātl" ‘serpiente’, forma honorífica: "Quetzalcōhuātzin") es uno de los más importantes dioses de la cultura mesoamericana, a veces considerado la principal divinidad del panteón mexica. Dios de la vida, la luz, la fertilidad, la civilización y el conocimiento. En ocasiones, también señor de los vientos y regidor del Oeste. Tercer Tezcatlipoca, asociado al color blanco.

Según Alfonso Caso, Quetzalcóatl era asociado al planeta Venus, como estrella matutina, y por ello denominado «el gemelo precioso» al considerárselo hermano del Xolotl, la estrella vespertina.

Quetzalcóatl, considerado como «la Serpiente Emplumada», la cual es una deidad que representa la dualidad inherente a la condición humana: la «serpiente» es cuerpo físico con sus limitaciones y las «plumas» son los principios espirituales. Otro nombre aplicado a esta deidad es Nahualpiltzintli, «príncipe de los nahuales». Quetzalcóatl es también el título de los sacerdotes supremos de la religión tolteca. Se lo identificó con al menos un personaje histórico, a saber: Ce Ácatl Topiltzin, rey de Tula, quien, según el "Memorial Breve de Colhuacan" y la "Historia de los Mexicanos por sus Pinturas", vivió entre los años 999 y 1051 de la era cristiana.

Las enseñanzas de Quetzalcóatl quedaron recogidas en ciertos documentos llamados "Huehuetlahtolli" («antiguas palabras»), transmitidos por tradición oral y puestos por escrito por los primeros cronistas españoles. Se han publicado traducciones parciales de los mismos.

Debido a que consideraban que todo el Universo tiene una naturaleza dual o polar, los toltecas creían que el Ser Supremo tiene una doble condición. Por un lado crea el mundo y por el otro lo destruye. La función destructora de Quetzalcóatl recibió el nombre de Tezcatlipoca, «espejo negro que humea», cuya etimología es la siguiente: "Tezcatl", «espejo»; "tliltic", «negro»; "Poca", «humo». Los informantes del padre Motolinía describieron a esta deidad del siguiente modo: «Tezcatlipoca era el que sabía todos los pensamientos y estaba en todo lugar y conocía los corazones; por eso le llamaban Moyocoya (ni), que quiere decir que es Todopoderoso o que hace todas las cosas; y no le sabían pintar sino como aire». (: "Teogonía e Historia de los Mexicanos")

Con un fin didáctico, el mito acentuaba la contradicción entre Quetzalcóatl y Tezcatlipoca. Sin embargo, su identidad esencial queda establecida en los códices y otros testimonios gráficos, donde ambas deidades comparten los mismos atributos.

Según la cosmogonía Náhuatl, el dios Iztauhqui-tezcatlipoca (Quetzalcóatl) es uno de los cuatro hijos de los dioses primordiales llamados Ometecuhtli y Omecíhuatl, bajo el relato de la creación del universo, de los cuales representan las esencia masculina y femenina de la creación, por lo que Quetzalcóatl simboliza la vida, la luz, la sabiduría, la fertilidad y el conocimiento, así como patrón de los vientos y del día, es el regidor del Oeste con el nombre de Tezcatlipoca Blanco. Con el tiempo, otros mitos se vinieron integrando para pasar de ser un dios creador de la humanidad hasta un rey histórico de la ciudad de Tula, o bien como otro dios solar al lado de su hermano Huitzilopochtli, interpretándose así con este mito, el traslado que realiza el Sol a través de los cielos, desde el amanecer hasta el atardecer por sus regidores y hermanos Tlahuizcalpantecuhtli y Xólotl, que junto con ellos, es hijo de Mixcóatl y Chimalma.

Para la cultura azteca y otras civilizaciones mesoamericanas, el dios era hermano de Tezcatlipoca. Para los toltecas, también eran rivales. Sea como sea, ambos eran considerados como el Ser Supremo. La combinación Quetzal-coatl contiene los siguientes significados, todos relativos a las funciones de Quetzalcóatl en la teología tolteca: «serpiente con plumas», «doble precioso», «ave de las edades», «gema de los ciclos», «ombligo o centro precioso», «serpiente acuática fecundadora», «el de las barbas de serpiente», «el precioso aconsejador», «divina dualidad», «femenino y masculino», «pecado y perfección», «movimiento y quietud». Quetzalcóatl era también importante para la civilización teotihuacana.

Tiene varias etapas, primero como deidad olmeca, tolteca, pipil, maya (como Kukulcán) y más tarde en el grupo de los dioses aztecas.

La cultura tolteca tomó la figura de este dios de la tradición religiosa de Teotihuacan, donde se encuentra una pirámide dedicada a la serpiente emplumada que data del siglo II después de Cristo. Sin embargo, tiene una raíz histórica más antigua. Los estudios recientes demuestran que este personaje se relaciona con la mitología olmeca y con su visión de la serpiente emplumada.

El arte y la iconografía de los olmecas demuestran claramente la importancia de la deidad de la Serpiente Emplumada en las cronologías de Mesoamérica, así como en el arte olmeca. En las grutas de Juxtlahuaca hay una representación de una serpiente emplumada de estilo olmeca. Incluso, desde lugares lejanos, como la Laguna de Asososca, en Managua, Nicaragua, se encuentran pinturas rupestres representativas de la Serpiente Emplumada, hasta Tula, hoy estado de Hidalgo, México.

El nombre de Quetzalcóatl se compone de dos palabras de origen náhuatl: "quetzal", que es un ave de hermoso plumaje que habita la selva centroamericana, y "cóatl", que significa «serpiente». Existe otra versión científica según la cual es posible que este dios tenga raíces chichimecas.

Sus influencias culturales abarcaron gran parte de Mesoamérica, incluyendo a las culturas maya y mixteca. Los mayas retomaron a Quetzalcóatl como Kukulkán o Gucumatz, aunque como se ha dicho antes es más conocida la versión de la cultura tolteca. Los aztecas incorporaron esta deidad a su llegada al valle de México.

Los mexicas relacionaban a Quetzalcóatl con el planeta Venus, que se puede observar como si fuera una estrella al lado del volcán Popocatépetl durante ocho meses al año, y desaparece otros tres meses. La profecía indica que este astro y los dos solsticios, donde se dice que Quetzalcóatl viene a la tierra dos veces al año a traer fertilidad y cosecha, sucederán hasta la segunda venida carnal de Quetzalcóatl. Una de las representaciones de esta deidad era la de un hombre barbado, por lo que durante la conquista de la Nueva España (Mesoamérica) algunos pueblos identificaron a Hernán Cortés con Quetzalcóatl. Tal afirmación nace desde las primeras Cartas de Relación que Cortés preparó para ser entregadas al rey español Carlos V. Se considera que dichas cartas fueron una estrategia legal, ya que las conquistas de tierras solamente podían ser aprobadas por el rey de acuerdo con las leyes españolas (Las Siete Partidas). Cortés carecía de dicho permiso, y por lo tanto tenía una orden de aprehensión. Posteriormente, defendió su postura al decir que los mexicas se rindieron al confundirlo con el dueño de las tierras y Cortés le entregaba esta posesión al rey, argumentando inocencia. Durante la colonia, la ilegalidad de la conquista se fue olvidando y el mito de que los españoles fueron confundidos por Quetzalcóatl se fortaleció, en parte por la aculturación oficial de los mexicas para reafirmar la jerarquía colonial.

En tiempos recientes las religiones de origen neotolteca hablan en sus tradiciones y leyendas urbanas del renacimiento de este personaje, idea que aparece en el llamado "Códice de Quetzalcóatl".

Quetzalcóatl es también el nombre de un personaje tolteca legendario, Ce Acatl Topiltzin Quetzalcóatl. Hijo de Mixcóatl y Chimalma, fue el último rey de Tollan o Toílan, ciudad que algunos estudios han identificado con la de Tula. El significado de su nombre es como sigue: "Ce Acatl": «Uno Caña», inicio de la trecena y último día del cuarto mes Huei Tozoztli (Perforación de la Gran Ave) dedicado al autosacrificio, "To": «Nuestro» y "Piltzin": «Joven Noble/Príncipe», el nombre con que se reconocía al gobernante. Su denominación como Quetzalcóatl se debe al culto al que pertenecía. Algunos autores creen que Tollan es hoy la ciudad de Tula, situada en el estado de Hidalgo, México. La leyenda dice que cayó por las tentaciones que los dioses presentaron al último rey de Tula y que están asociados a estados bélicos, no religiosos (precedentes al estado mexica). Teotihuacán, la ciudad de los dioses, es anterior a estas urbes.

La antropóloga Carmen Cook de Leonhardt promovió en los años ochenta la afirmación de que el pueblo María Magdalena Amatlán, o Amatlán de Quetzalcóatl (uno de los barrios de Tepoztlán), había sido la cuna del príncipe Ce Ácatl Quetzalcóatl. El presidente mexicano José López Portillo aceptó la propuesta y de alguna manera se «oficializó» la creencia de que el Quetzalcóatl histórico había nacido ahí. El novelista e investigador mexicano Fernando Zamora (respaldado por el Instituto de Investigaciones Estéticas de la UNAM) discute el hecho en la tesis: «Quetzalcóatl nació en Amatlán: Identidad y nación en un pueblo mesoamericano», publicado por la Universidad Iberoamericana.

La antropóloga basó su afirmación en tres estelas, en las que se le representaba respectivamente como serpiente emplumada y como el planeta Venus. De acuerdo con Cook, en dichas estelas y con base en la forma en que Venus se mueve por el cielo, encontró que el padre del dios serpiente fue el rey tolteca Mixcóatl (representado en la Vía Láctea) y que su madre se llamaba Chimalma. Dos de los cerros que rodean el lugar llevan dichos nombres desde tiempos prehispánicos, lo cual condujo a Carmen Cook a la convicción de que Amatlán era el lugar de nacimiento de Quetzalcóatl, hecho que si bien no ha recibido aceptación por parte de la comunidad científica, suele ser aceptado como verdadero por la gente del estado de Morelos, y particularmente por el pueblo de Amatlán.

Según la leyenda, Quetzalcoátl llegó a la zona maya (sureste del actual México) donde fue reconocido como un gran jefe guerrero, fundó la liga de Mayapán y conquistó la ciudad de Chichen Itzá donde fue conocido bajo el nombre de Kukulkán ("k'u uk'um", «pluma» y "kaan", «serpiente») y donde se encuentra el templo que lleva su nombre.



</doc>
<doc id="2447" url="https://es.wikipedia.org/wiki?curid=2447" title="Publio Sulpicio Quirinio">
Publio Sulpicio Quirinio

Publio Sulpicio Quirinio, a veces llamado también Publio Sulpicio Quirino o Cirenio (en griego Κυρήνιος, c. 51 a. C. - 21) fue un aristócrata del Imperio romano, miembro del Senado y cónsul.
Su periodo como gobernador de Siria es uno de los anclajes cronológicos del nacimiento de Jesús de Nazaret.

Nacido en el barrio de Lanuvio, población latina cercana a Roma, de familia nada distinguida, Quirinio recorrió el trayecto de servicio normal de un joven ambicioso de su clase social. Según el historiador romano Floro, Quirinio venció a los marmáridas, tribu de bandoleros del desierto procedente de Cirenaica, posiblemente cuando era gobernador de Creta y Cirene alrededor de 14 a. C., aunque no quiso aceptar el nombre honorífico que ameritaba por esa victoria militar, que habría sido Marmárico. En 12 a. C. fue nombrado cónsul, señal de que gozaba del favor de Augusto. Unos años más tarde encabezó una campaña contra los homonadenses, tribu sita en la región montañosa de Galacia y Cilicia, alrededor de 5 a. C. o 3 a. C., probablemente como legado de Galacia. Venció reduciendo los bastiones de su enemigo y matando de hambre a sus defensores. Esta victoria le valió un triunfo.

Para el año 1 d. C., Quirinio fue nombrado rector del nieto de Augusto, Gayo César, hasta que el joven murió de heridas que sufrió en campaña. Cuando el apoyo de Augusto pasó a su hijastro Tiberio, Quirinio se pasó al campo de seguidores de éste. Casado con Claudia Apia, de quien poco se sabe, se divorció de ella alrededor del año 3 d. C. y casó con Emilia Lépida, hija de Marco Emilio Lépido y hermana de Manio Emilio Lépido, que originalmente había estado comprometida con Lucio César. A los pocos años se divorciaron; en el año 20 d. C., Quirinio la acusó de alegar que era hijo del padre de ella y, más tarde, de intentar envenenarlo durante su matrimonio; Tácito afirma que Emilia gozaba de popularidad ante el pueblo, a cuyos ojos Quirinio la acusaba por despecho.

Tras la destitución de Arquelao, hijo de Herodes I el Grande, Quirinio llegó a Siria, enviado por César Augusto para hacer el censo de los bienes con vistas a establecer el impuesto. Con él fue enviado Coponio, para gobernar a los judíos. Como Judea había sido anexionada a Siria, Quirinio la incluyó en el censo.

El censo tuvo lugar "37 años después de que Octavio derrotó a Antonio en la batalla naval de Accio, el 2 de septiembre" (Flavio Josefo), lo que correspondería al año 6 d. C.

La "Biblia" menciona el censo de Quirinio como referente del nacimiento de Jesús de Nazaret:

Mientras que la cita anterior del Evangelio de Lucas menciona el censo de Quirinio como previo al nacimiento de Jesús, el Evangelio de Mateo afirma que Jesús nació durante el reinado de Herodes I el Grande. Una aparente contradicción resulta del hecho de que Herodes I el Grande fallece en el año 4 a. C., o sea, 10 años antes del censo de Quirinio. No se conoce de la existencia de otro censo en el período final del Reinado de Herodes, y el censo de Quirinio es llamado "el primero", por lo que se descartaría otro anterior. Sin embargo, para cuadrar ambos relatos algunos autores cristianos plantean si Quirinio podría haber estado ya antes en Siria, hacia el año 6 a. C., gobernando conjuntamente con Saturnino o con Quintilio Varo, y si podría haber realizado entonces un "primer" censo. Pero en ese entonces Judea no era parte de Siria y no tendría sentido censarla.

Según Flavio Josefo, este censo supuso una revuelta armada, dirigida por Sadoc y Judas el Galileo, natural de Gamala, y el propio Quirinio habría sofocado la revuelta, lo cual habría sido absurdo si aun viviera Herodes el Grande, pues como rey de Judea le habría correspondido sofocar ese levantamiento.

Quirinio se desempeñó como gobernador de Siria con autoridad nominal sobre Judea hasta 12 d. C., cuando volvió a Roma como allegado de Tiberio. Nueve años más tarde falleció, y Tiberio ordenó que se le diera funeral público.



</doc>
<doc id="2450" url="https://es.wikipedia.org/wiki?curid=2450" title="Quebec">
Quebec

Quebec (en inglés: "Quebec", y en francés: "Québec", pronunciado //) es una de las diez provincias que, junto con los tres territorios, conforman las trece entidades federales de Canadá. Su capital es la homónima, Quebec, y su ciudad más poblada es Montreal. Está ubicada al este del país, limitando al norte y noroeste con el estrecho de Hudson —que la separan de Nunavut— y la bahía de Hudson, respectivamente; al nordeste con la provincia de Terranova y Labrador; al este con el golfo de San Lorenzo y la provincia de Nuevo Brunswick; al sudeste con los Estados Unidos; y al sur y sudoeste con la provincia de Ontario. Con en 2008 es la segunda entidad más poblada —por detrás de Ontario— y con , la segunda más extensa, por detrás de Nunavut.

Por su idioma, su cultura y sus instituciones, forma una «nación dentro de Canadá». A diferencia de las demás provincias, Quebec tiene como única lengua oficial el francés, y es la única región mayoritariamente francófona de América del Norte. El idioma francés goza de protección legal e incluso la provincia cuenta con inspectores lingüísticos que revisan y controlan su uso. El celo de los quebequeses por su lengua y su estatus de minoría lingüística en América del Norte ha llegado a ciertos extremos políticos, pero también en su historia el pueblo quebequés sufrió periodos de represión y asimilación inglesa.

El Referéndum de independencia de Quebec de 1980 tuvo lugar el 20 de mayo de ese mismo año y los independentistas liderados por René Lévesque obtuvieron el 40,5 % de los sufragios. En el Referéndum de independencia de Quebec de 1995, los independentistas se quedaron a menos de un punto porcentual de conseguirlo con el 49,4 % de los votos.

El 27 de noviembre de 2006 el parlamento canadiense, con el apoyo del partido gobernante, reconoció a los quebequeses como una nación dentro de Canadá unida en un intento de aplacar los deseos secesionistas de los partidos independentistas, aunque fue tan solo en un sentido cultural y social, no en el legal.

En las elecciones generales de Quebec de 2012, el independentista Partido Quebequés, liderado por Pauline Marois, ganó la mayoría de los escaños de la Asamblea Nacional de Quebec, formando un gobierno minoritario. En el discurso del día de las elecciones, la ganadora planteó la posibilidad de convocar a un nuevo referéndum por la independencia al expresar su deseo de que Quebec se convirtiese en un país independiente y su convicción de que eso sucedería: «Queremos un país. Y lo tendremos.»

La provincia de Quebec se ubica al este de la provincia de Ontario y de la bahía de Hudson, al sur del territorio de Nunavut y del estrecho de Davis, al oeste de las Provincias Marítimas y de la provincia de Terranova Labrador y al norte de varios estados de los Estados Unidos (Nueva York, Vermont, Nuevo Hampshire y Maine). Más del 90% de la superficie de Quebec forma parte del llamado Escudo Canadiense.

Quebec comparte una frontera terrestre con cuatro estados en el nordeste de Estados Unidos (Nueva York, Vermont, Nuevo Hampshire y Maine) y tres provincias canadienses (Nuevo Brunswick, Ontario y Terranova y Labrador). En el golfo de San Lorenzo, la frontera es la línea de equidistancia entre las riberas de Quebec y la Isla del Príncipe Eduardo, Nuevo Brunswick, Nueva Escocia y Terranova y Labrador. Al norte y noroeste, la frontera marítima con el territorio de Nunavut sigue las orillas de la península del Labrador.

Una disputa fronteriza sigue en relación con la propiedad de península del Labrador —la frontera no está expresamente reconocida en Quebec—. Por otra parte, ya que los límites marítimos varían con las mareas, las islas costeras de la bahía de Hudson y la bahía de Ungava en Quebec solo lo son durante la marea baja. Así, más del 80% de las fronteras de Quebec siguen siendo inciertas.

El principal río es el San Lorenzo, el cual es navegable y comunica la región de los Grandes Lagos con el océano Atlántico. Atraviesa las ciudades de Montreal y Quebec, entre otras, y permanece helado desde noviembre hasta marzo. El clima es continental con temperaturas suaves en verano y muy frías en invierno, precipitaciones abundantes (en forma de nieve buena parte del año). En Montreal la temperatura media anual es de 6,1 °C (20,9 °C en julio, −10,4 °C en enero), en Quebec es de 4,0 °C (19,2 °C en julio, −12,8 °C en enero).

Quebec tiene tres regiones de clima principales:

Dentro de los mamíferos están el alce, lobo, puma, oso negro, venado, caribú, puercoespín, marmota, zorro, zorrillo, ardilla, carcayú. Aves como la guacharaca, lechuza montañera y colibrí. Reptiles como las serpientes son abundantes. Anfibios como ranas y sapos. Y entre los invertebrados abundan los insectos, arácnidos y escorpiones.

En el contorno de la bahía de Ungava y del estrecho de Hudson se encuentra la tundra, cuya flora se resume en una vegetación herbácea y arbustiva baja y de líquenes. Más al sur, el clima se vuelve propicio al crecimiento del bosque boreal, cuyo límite norte es la taiga.

La superficie del bosque quebequés se estima en . De Abitibi-Témiscamingue a la Côte-Nord, este bosque esencialmente está compuesto por coníferas como el abeto, el pino, el pinabeto blanco, el pinabeto negro y el 
alerce. Acercándose al río hacia el sur, se añaden gradualmente el abedul amarillo y otros hojosos. El valle del río San Lorenzo está compuesto por el bosque mixto con coníferas como el pino blanco de América y la tuya de Occidente (cedro), así como de hojosos.

En 2016 la provincia de Quebec contaba con habitantes, la mayoría de los cuales residían en el área metropolitana de Montreal, que cuenta con , segunda ciudad en habitantes de Canadá y la cuarta metrópoli francófona del mundo tras París, Kinsasa y Abiyán. Otras áreas metropolitanas de más de habitantes son las de Quebec (), Sherbrooke (), Saguenay () y Trois-Rivières (). El área de Gatineau (), frente a Ottawa, forma junto a esta un área metropolitana de habitantes. La población urbana en 2004 era de un 80,4 % y el porcentaje de inmigrantes se cifraba en un 12 % de la población total, destacando sobre todo la recepción de inmigración francófona, principalmente haitiana. La esperanza de vida es de 81,9 años para las mujeres y 76,3 años para los hombres.

El idioma oficial de la provincia es el francés. Es la única provincia canadiense donde la mayoría de la población es francófona, y el inglés no es reconocido como lengua oficial por las leyes de la provincia. Sin embargo, según la ley constitucional de Canadá de 1867, tanto el francés como el inglés pueden ser usados en la Asamblea Nacional de Quebec y sus cortes judiciales, y algunos documentos oficiales también han de estar escritos en ambos idiomas. Además, la minoría angloparlante tiene derecho a recibir la enseñanza en su idioma.

Según el censo de 2001, la lengua mayoritaria es el francés, hablada por el 81,2 % de la población. El 10 % habla una lengua no oficial —los llamados alófonos—, el 8 % es anglohablante y tan solo son bilingües el 0,8 % de la población. En el área metropolitana de Montreal el porcentaje de francófonos es del 68 %, siendo el 18,5 % alófonos, el 12,5 % anglohablantes, y bilingües el 1 % de la población. En las demás ciudades el porcentaje de francófonos supera el 90 %.

Quebec es una de las dos provincias canadienses cuya población es mayoritariamente católica, junto a Nuevo Brunswick. Este es un legado de la época colonial francesa, cuando solo a los católicos se les permitió establecerse en la Nueva Francia.

Los santos patronos de esta provincia son San Juan Bautista (cuya festividad es la Fiesta Nacional de Quebec) y Santa Ana.

El censo de 2001 mostró que la población era del 83,2 % cristianos católicos; el 4,7 % cristianos protestantes (incluidos 1,2 % anglicanos, 0,7 % de la Iglesia Unida, y el 0,5 % bautistas); el 1,4 % cristianos ortodoxos (incluyendo el 0,7 % de griegos ortodoxos), y el 0,8 % otros cristianos, así como el 1,5 % musulmanes, 1,3 % judíos; 0,6 % budistas, 0,3 % hindúes y sijs el 0,1 %. Un 5,8 % de la población dijo que no tenía ninguna afiliación religiosa (incluido el 5,6 % que dijo que no tenían ninguna religión en absoluto).

Cabe destacar que la mayoría de las expresiones vulgares de la lengua cotidiana utilizan términos habituales de la Iglesia católica y considerados sagrados por esta: "calise" (‘cáliz’), "tabarnac" (‘tabernáculo’), "ciboire" (‘copón’), "hostie" (‘hostia’). Dichas expresiones son a veces consideradas como parte de la identidad del dialecto quebequés frente al francés europeo.

La provincia de Quebec está altamente industrializada y en el territorio abundan los recursos naturales, entre los que destacan los minerales, grandes bosques de coníferas que nutren una importante industria maderera o los lagos, ríos y otras corrientes de agua que producen energía hidroeléctrica no solo para consumo interno sino también para su exportación a los Estados Unidos.

El valle del río San Lorenzo es una región agrícola muy fértil. Al contar con una gran cabaña ganadera, produce lácteos variados y carne, y en sus campos se cosechan excelentes frutas y verduras. Destaca en gran medida la producción de jarabe de arce, del cual la provincia de Quebec es el primer productor mundial.

El jefe de gobierno es el Primer Ministro, quien es el jefe del partido que más escaños ocupa en la Asamblea Nacional de Quebec. Los partidos más fuertes ahora son el nacionalista Coalición Futuro Quebec ("Coalition Avenir Québec" o CAQ) de centro-derecha y el federalista Partido Liberal de Quebec ("Parti libéral du Québec" o PLQ) de centro.
El teniente gobernador representa a la reina Isabel II del Reino Unido y actúa simbólicamente como jefe de Estado.

Antes de la llegada de los franceses, Quebec estaba habitado por diferentes pueblos aborígenes, entre los cuales destacan los inuits —antiguos esquimales—, los hurones, los algonquinos, los mohawks, los cree y los innus.

El primer explorador francés en Quebec fue Jacques Cartier, que en 1534 estableció en Gaspé una gran cruz de madera con tres flores de lis, tomando posesión de aquellas tierras en nombre de Francia. Cartier descubrió el río San Lorenzo.
En 1608, Samuel de Champlain dio nacimiento a la Nueva Francia fundando en la orilla norte del río San Lorenzo, en un lugar que los indios llamaban «kebek» (‘estrecho’), la ciudad de Quebec. La ciudad se volverá así el punto de partida de las exploraciones francesas en América del Norte. Después de 1627, el rey de Francia Luis XIII concedió el monopolio de la colonización a los católicos. La Nueva Francia se volvió una colonia real en 1663, bajo el reinado de Luis XIV.
Los franceses se alían con los indios hurones y otros contra los indios iroqueses, que eran los aliados de los británicos. La Guerra de los Siete Años (1756-1763), entre Gran Bretaña y Francia, toma un cambio decisivo en Norteamérica con la derrota en 1759 del ejército de Louis-Joseph de Montcalm a manos del ejército británico del general James Wolfe en la batalla de los Llanos de Abraham, a las puertas de la ciudad de Quebec.

Entre 1755 y 1762 los pobladores de la zona llamada Acadia en las actuales provincias marítimas de Nueva Escocia y Nuevo Brunswick sufrieron una deportación masiva de sus tierras, entregadas a inmigrantes de Nueva Inglaterra. Las familias, separadas en varios navíos y deportadas a otros lugares —Luisiana, Francia, Gran Bretaña— sufrieron una fuerte mortalidad.

El Reino Unido tomó posesión de la Nueva Francia con el Tratado de París en 1763, cuando el rey Luis XV de Francia y sus consejeros eligieron conservar Guadalupe, por su azúcar, en lugar de Quebec —considerado en ese entonces como un extenso territorio de hielo sin importancia—. A raíz de este Tratado, la mayoría de los aristócratas regresaron a Francia.

En 1774 con la Ley de Quebec, Londres daba reconocimiento oficial a los derechos del pueblo francés de Quebec: el uso de la lengua francesa, la práctica de la religión católica y el uso del Derecho Romano en lugar del Jurisprudencial anglosajón. Antes de esta fecha, la situación de la religión católica era muy frágil y las posibilidades para los católicos, muy limitadas.
En 1791 la Ley Constitucional de Canadá estableció dos provincias alrededor del río Ottawa: el Alto Canadá (la actual provincia de Ontario), de mayoría anglófona, y el Bajo Canadá (la actual provincia de Quebec), provincia mayoritariamente francófona.

En 1867, la firma de la Ley de América del Norte Británica consagró la federación de las provincias de Canadá, que constaba entonces de Quebec, Ontario, Nuevo Brunswick y Nueva Escocia.

La exclusión económica de los francoparlantes en Quebec fue considerada siempre un problema en Quebec hasta las reformas de los años 1960, la llamada «Revolución tranquila» ("Révolution tranquille"). El primer ministro de Quebec en ese entonces, Jean Lesage, propuso la nacionalización de la producción de electricidad. El gobierno creó empresas y bancas nacionales, y después impuso legislación para reconocer el derecho de trabajar en francés.

En 1948 se aprobó la actual bandera de Quebec como oficial, y la lengua francesa sería cooficial —junto al inglés— en Canadá desde 1968. En 1976 ganó las elecciones el nacionalista Partido Quebequés, de René Lévesque, que promulgaría la "Ley 101," por la que el francés sería la única lengua oficial de Quebec. En 1980, el referéndum de independencia arrojó resultado negativo, con un 59,6 % de votos en contra. De nuevo en el poder en 1995, el Partido Quebequés convocó a un nuevo referéndum el 30 de octubre de 1995, donde el "no" a la independencia ganó por tan solo votos y un 50,4 % de sufragios, con una participación que superó el 90 % del censo electoral. Según las encuestas, menos de la mitad de los quebequeses sigue deseando constituirse en estado independiente.

El 27 de noviembre de 2006, el parlamento canadiense, con el apoyo del partido en el gobierno, reconoció a los quebequeses ("Québécois", en francés) como «nación dentro de un Canadá unido», en un intento de aplacar los deseos secesionistas de los partidos independentistas, aunque en sentido cultural y social, no legal.

Asimismo, la provincia de Quebec tiene otras diez naciones de indios e inuit, reconocidos como tales por su Asamblea Nacional en la época de René Lévesque, y son también reconocidas por el gobierno federal (las Primeras Naciones). Otras naciones en la provincia de Quebec podrían eventualmente ser reconocidas, como los «métis» y los anglo-quebequeses.


La provincia de Quebec está dividida en 17 regiones administrativas:







</doc>
<doc id="2451" url="https://es.wikipedia.org/wiki?curid=2451" title="Quiromancia">
Quiromancia

La quiromancia, quiromancía, quirología o, en la cultura popular, lectura de la palma es el intento de adivinación a través de la lectura de las líneas de la mano. El término deriva del griego χείρ ("khéir", "mano") y μαντεία ("manteía", "adivinación"). Es una rama de la quirología y se centra en el estudio de las líneas y montes que se hallan en las palmas de las manos que, por medio de la observación, revelan supuestamente el perfil psicológico y fisiológico de una persona. Aunque suele ir íntimamente ligada a la adivinación y a las ciencias ocultas, siempre ha existido una cierta aceptación popular. La práctica se encuentra en todo el mundo, con numerosas variaciones culturales. Comúnmente, es denominada como "leer la mano" o "leer las manos", aunque también se le conoce como "echar", "leer" o "decir la buenaventura". Aquellos que la practican se denominan generalmente palmistas, lectores de palmas, analistas de la mano o quirologistas. 

Hay varias—a menudo en conflicto—interpretaciones de líneas y características a través de las distintas escuelas de palmistas. Estas contradicciones entre interpretaciones diferentes, así como la carencia de soporte empírico para las predicciones, contribuye a que científicamente se le considere una seudociencia donde históricamente se han utilizado técnicas de lectura en frío.

Desde hace algunas décadas, la comunidad científica ha corroborado la existencia de diversas relaciones químicas entre genes inconexos, vinculando así caracteres fenotípicos diferentes. Los quiromantes defienden de este modo la relación que pudiera existir entre los surcos y pliegues de las palmas de las manos con numerosos rasgos físicos y psíquicos, pudiendo así estudiar el perfil psicológico de una persona a través de su lectura palmar. Sin embargo, los defensores de esta hipótesis no presentan ningún estudio de relación entre los genes que determinan las líneas de la mano, actualmente desconocidos y los que determinan otros rasgos.

La quiromancia es una práctica común en muchos lugares diferentes de Eurasia; se ha practicado en las culturas de la India, Nepal, Tíbet, China, Persia, Sumeria, la Palestina histórica y Babilonia.

El acupunturista Yoshiaki Omura describe sus raíces en la astrología hindú (conocida en Sánscrito como "jyotish"), la China "Yijing" ("I Ching"), y adivinos de Roma. Varios miles de años atrás, el sabio hindú Valmiki se pensó que había escrito un libro que comprende de 567 estrofas, cuyo título se traduce en como "Las Enseñanzas de Valmiki Maharshi sobre Quiromancia Masculina". De la India, el arte de la quiromancia se extendió a China, Tíbet, Egipto, Persia y otros países de Europa. Desde esta región, la quiromancia avanzó hasta Grecia, donde Anaxágoras la practicó. Aristóteles (384-322 B. C. E.), descubrió un tratado sobre el tema en un altar de Hermes, posteriormente presentado a Alejandro magno (356-323 B. C. E.), quien tenía gran interés en examinar el carácter de sus oficiales, mediante el análisis de las líneas de las manos.

Durante la Edad Media el arte de leer las manos fue activamente suprimido por la Iglesia católica como superstición pagana. En el renacimiento de la magia, la quiromancia fue clasificada como una de las siete "artes prohibidas", junto con la nigromancia, geomancia, aeromancia, piromancia, hidromancia, y espatulamancia (escapulimancia).

La quiromancia ha experimentado un renacimiento en la era moderna, comenzando con la publicación de "La Chirognomie d"el Capitán Casimiro Stanislas D'Arpentigny en 1839.

La Sociedad de quiromancia de Gran Bretaña fue fundada en Londres por Katharine San Hill en 1889 con el objetivo declarado de promover y sistematizar el arte y para evitar que los charlatanes abusaran de la técnica. Edgar de Valcourt-Vermont (Comte de St Germain) fundó la Sociedad americana de quiromancia en 1897.

Una figura fundamental en el movimiento de la quiromancia moderna fue el Irlandés William John Warner, conocido por su nombre artístico, Cheiro. Después de estudiar con los gurús de la India, prácticó la quiromancia en Londres y disfrutó de una amplia lista de clientes famosos de todo el mundo, incluyendo a personajes como Mark Twain, W. T. Stead, Sarah Bernhardt, Mata Hari, Oscar Wilde, Grover Cleveland, Thomas Edison, el Príncipe de Gales, el General Kitchener, William Ewart Gladstone, y Joseph Chamberlain. Tan popular fue Cheiro como un "palmista de sociedad" que incluso aquellos que no eran creyentes en el ocultismo dejaban que él leyera sus manos. El escéptico Mark Twain escribió en el libro de visitas que Cheiro tenía "...expuesto mi personalidad con humillante exactitud."

Edward Heron-Allen, un inglés polimata, publicó varias obras, incluyendo el libro de 1883 "Quiromancia – Un Manual de Cheirosofía", que todavía es impreso. Hubo intentos de formular algún tipo de base científica para el arte, más concretamente en el año 1900, en la publicación "Las Leyes Científicas de la Lectura de la Mano" por William G. Benham.

Se suele decir que en las manos se puede conocer el destino de una persona y adivinar sucesos pasados, presentes y futuros.

La quiromancia consiste en la práctica de la evaluación del carácter o el futuro de una persona por la "lectura" de la palma de la mano. Varias "líneas" y "montes" a las que supuestamente se sugieren interpretaciones por su tamaño relativo, cualidades e intersecciones. En algunas tradiciones, los lectores también examinan las características de los dedos, uñas, huellas dactilares y patrones de la piel (dermatoglifos), textura de la piel, color, forma de la palma y flexibilidad de la mano. 

Un lector comienza con la lectura de la mano dominante (con la que él o ella escribe o utiliza mayormente, que a veces se considera que representa la mente consciente, mientras que el otro lado es el subconsciente) de la persona. En algunas tradiciones, la otra mano se cree que lleva los rasgos hereditarios o familiares, o, dependiendo de la las creencias cosmológicas del quiromano, transmite información sobre "vidas pasadas" o condiciones "kármicas".

Existen 3 líneas mayores y numerosas líneas menores que no siempre aparecen en su totalidad.
Antiguamente, la quiromancia era un rito pagano, como todos los ritos de adivinación. Los que la practicaban eran acusados de brujería y perseguidos por la Santa Inquisición.

En la actualidad, la práctica de la quiromancia, o de la quirología en general, suele estar acompañada de la lectura del tarot y otras prácticas esotéricas. Se pueden encontrar quiromantes con consultas privadas y altos precios por el estudio de la mano, así como quiromantes instalados en zonas céntricas de las ciudades con gran afluencia de peatones que practican la quiromancia y otras artes esotéricas a cambio del pago del cliente.

Dependiendo del tipo de quiromancia practicado, y el tipo de lectura que se realiza, los quiromantes pueden ver las distintas cualidades de la mano, incluyendo formas y líneas de la palma y los dedos; color y textura de la piel y las uñas; tamaño relativo de la palma y los dedos; protagonismo de los nudillos y varios otros atributos de las manos.

En la mayoría de las escuelas de quiromancia, la forma de las manos se divide en cuatro u once tipos principales, a veces correspondientes a los elementos clásicos o temperamentos. La forma se cree indicador de rasgos del carácter que corresponden al tipo indicado (es decir, una "mano de fuego" exhibe alta energía, creatividad, genio, ambición y todas las cualidades que se creen relacionadas con ese elemento).

A pesar de las variaciones, las clasificaciones más comunes usadas por los palmistas son:


El número y la calidad de las líneas también pueden ser incluidos en el análisis de la forma de la mano; en algunas tradiciones de la quiromancia, las manos de Tierra y Agua tienden a tener menos líneas profundas, mientras que las de Aire y Fuego son más propensas a mostrar las líneas con menos definición.

La quiromancia no es considerada como una ciencia por la comunidad científica al no cumplir con los requisitos básicos del método científico. Es decir, no ha superado las pruebas necesarias para ser considerada aceptable científicamente. Hay poca investigación que acepte la verificación de la precisión de la quiromancia como un sistema de análisis y mucha de ésta ha sido llevada a cabo por los propios quiromantes. Por otra parte, el mago y escéptico James Randi ofrece un premio de un millón de dólares a cualquiera que logre demostrar fehacientemente la existencia de un fenómeno o poderes paranormales —incluyendo el tipo de eventos como los que se ocupa la quiromancia— pero el premio está desierto desde que se ofreció.





</doc>
<doc id="2452" url="https://es.wikipedia.org/wiki?curid=2452" title="Quimera (mitología)">
Quimera (mitología)

En la mitología griega, Quimera (en griego antiguo Χίμαιρα "Khimaira" que significa "animal fabuloso"; en latín "Chimæra") era un monstruo híbrido, que aparece generalmente como hija de Tifón y de Equidna, aunque para el poeta Hesíodo la madre de la Quimera está designada por un pronombre que puede referirse tanto a Equidna como a la Hidra de Lerna. La quimera vagaba por las regiones de Asia Menor aterrorizando a las poblaciones y engullendo animales, y hasta rebaños enteros. Es posible que de su unión con Ortro nacieran la Esfinge y el León de Nemea.

Las descripciones varían desde las que decían que tenía el cuerpo de una cabra, la cola de una serpiente o un dragón y la cabeza de un león, hasta las que afirmaban que tenía tres cabezas: una de león, otra de macho cabrío, que le salía del lomo, y la última de dragón o serpiente, que nacía en la cola. Se dice que escupía fuego.

Quimera fue derrotada finalmente por Belerofonte con la ayuda de Pegaso, el caballo alado, a las órdenes del rey Yóbates de Licia. Hay varias descripciones de su muerte: algunas dicen simplemente que Belerofonte la atravesó con su lanza, mientras que otras sostienen que la mató cubriendo la punta de la lanza con plomo que se fundió al ser expuesto a la ardiente respiración de Quimera.

"La quimera de Arezzo", de origen etrusco conservado en el Museo Arqueológico de Florencia, es un buen ejemplo.

Algunos especialistas occidentales del arte chino, a partir de Victor Segalen, utilizan la palabra "quimera" para referirse genéricamente a cuadrúpedos alados híbridos de leones como los‎‎ "bìxié" (辟邪),‎‎ ‎‎ "‎‎tianlu" e incluso "qilin". A menudo se representan también con barbas de chivo.

Originalmente los denominados "Ruìshòu" (en chino 瑞兽) se utilizaban para proteger tumbas como animales propicios para dispersar los malos espíritus.




</doc>
<doc id="2456" url="https://es.wikipedia.org/wiki?curid=2456" title="Rugby">
Rugby

El rugby es un deporte de evasión y contacto en equipo nacido en Inglaterra. Fue en ese país donde tomó su nombre a partir de las reglas del fútbol elaboradas en el colegio de la ciudad de Rugby (Rugby School) en el siglo XIX. Sobre la forma de denominar en español al jugador practicante del deporte, el "Diccionario panhispánico de dudas" informa que «se usa con frecuencia en los países del Río de la Plata la forma "rugbier" con el sufijo -"er" propio del inglés para crear este tipo de derivados (aunque en inglés se usa, en este caso, la expresión "rugby player"). La Real Academia Española utiliza la palabra "rugbi", como "adaptación gráfica" del término inglés y recomienda la expresión "rugbista" para referirse al jugador, utilizando el sufijo -"ista" (como en "futbolista, golfista, tenista", etc)».

El "rugby" se practica a nivel internacional en todos los continentes, aunque ha alcanzado un importante grado de desarrollo en las naciones que conforman las islas británicas (Escocia, Gales, Inglaterra, Irlanda e Irlanda del Norte), así como en Australia, Fiyi, Nueva Zelanda, Papúa Nueva Guinea, Samoa, Sudáfrica, Tonga, Uruguay, Argentina y Francia.

En otros países tiene variados grados de popularidad y competitividad internacional. En África también es popular, por influencia inglesa, en Namibia, Kenia y Zimbabue, y por influencia francesa en Túnez, Costa de Marfil, Madagascar y Marruecos. En América se practica principalmente en Argentina—participante del Rugby Championship—, donde tiene gran arraigo y cuya selección ha logrado importantes logros internacionales; asimismo tiene cierta popularidad en otros países americanos, como Brasil, Canadá, Uruguay, Chile y Estados Unidos. En Asia, el equipo más destacado es el de Japón. En el resto de Europa, se destaca Italia, que participa en el Torneo de las Seis Naciones. Igualmente está difundido en otros países del continente europeo, sobre todo en Portugal, España y en países del este europeo, como Rumania, Georgia o Rusia. En Oceanía, por influencia australiana y neozelandesa, el "rugby" es un deporte muy popular en Fiyi, Tonga y Samoa, cuna de jugadores destacados en el ámbito internacional.

A nivel internacional, el rugby está regulado por World Rugby, asociación federativa que cuenta con 118 miembros (plenos y asociados). Luego de participar en cuatro ediciones de los Juegos Olímpicos a comienzos del siglo XX, el rugby fue reincorporado a los deportes olímpicos a partir de Río 2016 en su modalidad de "rugby 7".

Desde los orígenes mismos del "rugby" y el fútbol actual, a mediados del siglo XIX, se definieron como el "alter ego" del otro: fuerza contra habilidad; juego limpio contra juego desleal, etc. Un antiguo dicho británico dice que "el fútbol es un juego de caballeros jugado por villanos y el rugby es un juego de villanos jugado por caballeros". En el rugby es característico el respeto a las reglas que deben practicar tanto los jugadores como el público, y las decisiones del árbitro rara vez son discutidas por los jugadores. Además, se fomenta la sociabilidad, dándose generalmente entre compañeros de equipos y oponentes una cordial reunión después de los partidos, denominada "tercer tiempo", junto con los árbitros, entrenadores y parte del público, para hablar acerca del partido.

El rugby moderno, al igual que el fútbol moderno, es una evolución directa del "fútbol medieval británico", también llamado en español "fútbol de carnaval" (en inglés "mob football", equivalente a "fútbol multitudinario"), un juego de pelota violento y reiteradamente prohibido, de reglas sumamente variables, que se practicaba popularmente en las islas británicas durante el medievo europeo, en el que se usaban tanto las manos como los pies, así como la fuerza para detener a los competidores.

La tradición atribuye la invención del rugby a William Webb Ellis, un estudiante de teología del Colegio de Rugby y el trofeo que se entrega a los ganadores de la Copa del Mundo de Rugby lleva su nombre. El hecho es recordado en una lápida mural de la "public school" de Rugby.

Las "Leyes del juego de rugby" son dictadas por World Rugby ("International Rugby Board" o IRB hasta 2014). Su cuerpo central son las cuatro leyes que regulan el juego: el terreno, la pelota, número de jugadores, vestimenta, tiempo, oficiales, modo de jugar, ventaja, modo de marcar, juego sucio, "off side" (fuera de juego) y "on side" (en juego), pase forward (pase hacia delante o "avant"), salidas, pelota al suelo sin tackle ("placaje"), tackle ("placaje"): portador de la pelota derribado, "ruck", "maul", "mark", "touch" y "line-out", "scrum" ("melé"), penales y "free kicks" y "tries" (ensayos).

Las "Leyes del juego de rugby" también están integradas por un Prólogo, una lista de definiciones de los términos utilizados en las reglas, un apartado de variaciones para menores de 19 años, otro apartado para ""seven a side"" (variante de siete jugadores por bando), señales de los árbitros, y un extracto de la Regulación 35, sobre vestimenta de los jugadores. La publicación oficial de World Rugby, asimismo, está acompañada de un "Documento del juego", complementario de las leyes, que "cubre los principios básicos del Rugby".

En 2009, la IRB aprobó 16 modificaciones sustanciales, las "Experimental Law Variations" (Variaciones experimentales de las leyes), conocidas por su sigla en inglés, ELV al reglamento, y que fueron puestas en práctica en todos los torneos oficiales del mundo a partir del 8 de julio de 2002. En 2006, la IRB incorporó 13 de las 16 ELV a las "Leyes del juego", dejando sin efecto las otras tres.

En español existen dos traducciones del reglamento:

Como las dos traducciones son distintas, la terminología reglamentaria en español varía según se trate de algún país hispanoamericano o España. Algunos de los principales términos en que se registran diferencias son los siguientes, según se trate de Hispanoamérica o España: try/ensayo, conversión/transformación, penal/transformación de golpe, scrum/melé, tackle/placaje, hooker/talonador, fullback/zaguero, ala/flanker, wing/ala, etc.

En el rugby se enfrentan dos equipos de quince jugadores cada equipo (aunque hay una variación para un juego de siete). El campo de juego tiene forma rectangular y es de césped (aunque puede ser de arena, tierra, nieve o césped artificial). Sus medidas son de un máximo de 95 metros de largo y 65 de ancho. Al campo de juego se le suman dos áreas, la zona de anotación (o "in-goal"), en cada uno de los extremos, de no más de 22 metros cada una, destinada a apoyar la pelota para obtener el "try" o "ensayo", principal anotación del juego.
En los dos extremos del campo, en el centro de la "línea de anotación", se encuentran instalados dos postes separados entre sí por 5,6 metros y unidos por un travesaño situado a 3 metros de altura. Los postes deben tener un mínimo de 3,4 metros de alto, lo que le da al conjunto de los tres palos una forma de H.

El balón de rugby es de forma ovalada, está construida con cuatro gajos de cuero o material sintético parecido y pesa algo menos de medio kilo. Los partidos, en la modalidad de quince jugadores, duran ochenta minutos, divididos en dos tiempos iguales (setenta minutos para las categorías juveniles menores de 19 años).

Un campo de juego es rectangular, y no debe exceder de 95 metros de largo por 65 metros de ancho. Las líneas laterales (denominadas "líneas de "touch"") del campo de juego no forman parte de este. A continuación de cada uno de los lados menores del rectángulo hay una zona de anotación (o de ensayo), denominada ""in-goal"", con una longitud de entre 10 y 22 m. Entre el campo de juego y estas zonas de anotación hay una línea continua, denominada "línea de "goal"" (de marca, o de anotación, o de gol), que es parte de las últimas y en cuyo centro se ubican los postes de gol. Estos postes verticales están separados entre sí por una distancia de 5,6 m y unidos a 3 m de altura por un travesaño. La altura de los postes depende del gusto del equipo local, aunque en cualquier caso debe sobrepasar los 3,4 metros. El conjunto del campo de juego y las áreas de gol se denomina "área de juego". El área de juego, las líneas no incluidas en ella (las líneas de "touch" y las líneas laterales y finales que limitan el "in-goal", denominadas líneas de "touch in-goal" y líneas de pelota muerta respectivamente), y un área perimetral de 5 m de ancho alrededor del conjunto anterior, se denomina "terreno de juego".

En mitad del campo, paralela a las líneas de gol, se ubica una línea continua denominada "línea de mitad de cancha". En el centro de esta, una línea perpendicular marca el centro del campo. A 10 m a cada lado de la línea de mitad de campo existe una línea discontinua paralela, la cual se utiliza como referencia para las salidas, ya que el balón debe superar dicha línea para considerarse en juego. De cada lado hay otra línea continua entre ambas líneas de banda, paralela a la línea de gol y a 22 m de esta hacia el centro del campo. El espacio delimitado por esta línea y la de gol (excluyendo a esta) se denomina "las 22" o "zona de 22". Entre ambas líneas de banda, a 5 m de las líneas de gol y paralelas a esta hay líneas discontinuas. Finalmente, hay líneas discontinuas entre las anteriores, paralelas a las líneas laterales, a los 5 y 15 m de estas. Estas líneas señalan los límites para la posición del jugador más avanzado y más retrasado en los saques de banda.

En el rugby, los jugadores de cada equipo se dividen en dos grandes grupos: los "forwards" o delanteros y los "backs", zagueros o defensores.

Los "forwards" o delanteros, también referidos como "pack de forwards", son ocho jugadores, ubicados en la zona delantera del equipo. En general son los jugadores más grandes y pesados del equipo. Tienen como función específica disputar el scrum (melé) y los saques de lateral ("line out"). Los "forwards" se ubican en tres líneas: la primera línea está integrada por dos pilares (1 y 3) y un "hooker" o talonador (2) en el medio; la segunda línea está integrada por dos jugadores denominados con ese nombre (4 y 5); la tercera línea está integrada por tres jugadores, con el octavo en el medio, flanqueado por dos alas o "flankers" (6 y 7).

Los "backs", zagueros o defensores, son siete jugadores que se ubican en la zona posterior del equipo. En general son los jugadores más ágiles y rápidos del equipo. Cinco "backs" forman "la línea" o los "tres cuartos", ubicados en diagonal -con el fin de lograr velocidad en el avance- sucesivamente a partir del medio scrum o medio melé (9), el apertura (10), dos centros (12 y 13) y un "wing" -o ala en España- (14). En el extremo opuesto de la línea se ubica otro "wing" o "wing" ciego (11) y detrás al centro se ubica el "fullback" o zaguero (15).

El objetivo fundamental consiste en obtener una mayor cantidad de puntos que el adversario. Los puntos se pueden obtener del siguiente modo:

Un jugador, siempre que se encuentre en juego ("on side"), puede:

Una de las reglas fundamentales del rugby es el llamado "tackle" en el reglamento publicado por la WR, regulado en la ley 15:
Las leyes del juego hacen especial hincapié en evitar y sancionar severamente el juego peligroso, aun cuando no sea intencional. El tackle no puede realizarse mediante un golpe directo con el hombro o un brazo rígido. La Ley 10(4)(e), prohíbe explícitamente el tackle alto, que es aquel por el cual el jugador que lleva la pelota es tomado por encima de la línea de los hombros, aun cuando el tackle se haya iniciado por debajo. La World Rugby tiene una política de tolerancia cero respecto del contacto con la zona de la cabeza.

A partir del 3 de enero de 2017 la WR estableció una serie de medidas y definiciones para reducir la discrecionalidad y aumentar las sanciones ante los tackles altos. En las nuevas medidas la WR define dos tipos precisos de tackle alto: el "tackle temerario" ("reckless tackle") y el "tackle accidental". Se considera "tackle temerario" cuando el jugador sabía o debería haber sabido que existía el riesgo de contacto con la cabeza y aun así siguió adelante y debe ser sancionado como mínimo con tarjeta amarilla (exclusión de la cancha durante un tiempo). Cuando el contacto con la cabeza sea accidental (tackle accidental), corresponde como mínimo cobrar penal.

Simultáneamente la WR definió una serie de normas educativas a tener en cuenta por todas las personas involucradas en el juego:


El juego se inicia con un puntapié de salida, que debe efectuarse de sobrepique, realizado desde el centro del campo. Todos los jugadores del equipo que efectúa la salida deben ubicarse por detrás de la pelota hasta que esta haya sido pateada, y los rivales a diez metros de distancia. El balón debe superar la distancia de diez metros sin salir del campo y botar o ser atrapado dentro de él. El juego general ha comenzado, y continuará hasta que se produzca una interrupción. El juego se interrumpe cuando la pelota ha quedado “muerta”: la pelota ha salido de los límites del área de juego, se ha marcado un tanto, se ha producido una anulada, se ha producido una infracción sin ventaja para el equipo no infractor, un jugador ha pedido una marca ("mark"), o se ha producido otra interrupción en el juego.

El silbato del árbitro marca los puntapiés de salida (no los de reinicio) y las pelotas muertas. El árbitro hace sonar su silbato también para indicar que ha detenido la cuenta del tiempo, por ejemplo para que un jugador lesionado sea atendido o para dar indicaciones al capitán de un equipo, y que la ha reiniciado.

Cuando el tiempo se agota, el juego continúa hasta que se haya producido una pelota muerta, salvo que esto fuera como consecuencia de una infracción castigada con penal, en cuyo caso deberá continuar bajo el mismo principio. El reinicio del juego en el segundo tiempo se produce también con un puntapié de salida, a cargo del equipo que no efectuó el del comienzo del partido.


Cuando el balón, o el jugador que lo lleva, salen del campo por la línea de "touch", el juego se reinicia mediante un saque de banda llamado "line out" que debe arrojarse recto y superando la línea ubicada a cinco metros campo adentro del "touch" entre dos hileras de jugadores, una de cada equipo y separadas por una distancia de un metro. Los jugadores deben saltar para obtener la pelota, pudiendo ser impulsados y sostenidos por sus compañeros. El lanzamiento le corresponde al equipo que no la envió afuera, salvo que haya sido consecuencia de un penal, en cuyo caso debe lanzar el equipo que pateó. El equipo que lanza la pelota decide también cuántos jugadores va a haber en la hilera (de 2 a 14), mientras que el otro equipo puede tener menos pero no más. Todos los demás jugadores, excepto el lanzador, un opuesto al lanzador del equipo rival, y un receptor por cada equipo, deben alejarse diez metros hacia su campo de la línea perpendicular al "touch" por donde se arrojará la pelota.

La posición en la que se efectuará el tiro no es necesariamente aquella en que el balón cruzó la línea de "touch". Si el balón fue pateado por un jugador por delante de su línea de 22 metros, o por detrás de ella cuando es el equipo defensor lo introdujo en su zona de 22 metros, entonces el lanzamiento debe efectuarse en línea con el lugar desde donde se pateó. En cualquier caso, el equipo que debe reponer la pelota en juego puede decidir efectuar un “tiro rápido” en cualquier lugar entre su línea de "goal" y la línea en que debe formarse el "line out"; pero para ello deben cumplirse ciertas condiciones: que se utilice el mismo balón que salió del campo, que no haya sido tocado por nadie excepto el jugador que lanza (y eventualmente por el jugador rival que salió fuera del campo con el balón), y que no haya principiado la formación del "line out". En el tiro rápido no se requiere que el balón sea arrojado paralelamente a las líneas de "goal": puede enviarse oblicuamente hacia la línea de "goal" del lanzador, pero debe superar la línea de cinco metros paralela al "touch".

La pelota no debe arrojarse intencionalmente con las manos al "touch". Esta acción es considerada juego sucio y castigada con un penal.

El scrum o la melé, una de las formaciones más reconocibles del rugby, es una puja frente a frente, de un grupo de cada equipo formado por un máximo de ocho y un mínimo de tres jugadores en tres líneas, que se enfrentan agazapados y asidos entre sí, para comenzar a empujar con el fin de obtener el balón que ha sido lanzado en medio de ellos y sin tocarlo con la mano. El grupo que haya obtenido el balón, debe sacarlo sin tocarlo con la mano por detrás de la formación, donde lo tomará un jugador (usualmente, pero no siempre, el "medio melé" o "medio scrum") y continuará el juego.

Tanto en el saque de banda cuando el balón sale fuera("line out") como en el "scrum" (la melé), el sentido de las reglas es que exista disputa por la pelota. Esa es la diferencia con las infracciones mayores, que se penalizan con una patada de castigo (tiro a los postes, tiro afuera o puesta en juego), en la que el equipo infractor no puede intervenir.

Los "rucks" y los "mauls" son las formaciones grupales de lucha por la pelota que forman ambos equipos durante el desarrollo del juego. La diferencia entre ambos estriba en si la pelota se encuentra en poder de uno de los jugadores ("maul"), o si se encuentra en el suelo ("ruck").

El maul (ley 17) es una formación esencialmente ofensiva. Se produce cuando un jugador que lleva la pelota es asido por uno o más defensores, y hay uno o más compañeros del portador de la pelota asidos a este, todos ellos sobre sus pies (como mínimo deben ser dos atacantes y un defensor). Sus reglas son complejas, pero básicamente no debe dejar de moverse hacia la meta y con los defensores retrocediendo; si es detenido durante cinco segundos, algún jugador atacante debe abandonar el maul con la pelota o pasarla; en caso contrario, la acción del equipo atacante es castigada con un scrum a favor del defensor. Con un maul puede realizarse un try.

En la reglamentación vigente hasta agosto de 2008, esta formación no se podía derrumbar, por ser considerado juego peligroso, sancionándose en ese caso con penal. Entre el 1 de agosto de 2008 y el 1 de junio de 2009, se pusieron en vigencia trece Variaciones Experimentales Reglamentarias (ELVs), entre las que se incluyó una que permitía derrumbar el maul. Sin embargo, luego de ser examinadas durante la temporada 2008/2009 la IRB decidió no confirmar esta variación, volviendo a estar prohibido derribar el maul a partir del 1 de junio de 2009.
El ruck (ley 16) es una formación más orientada a la disputa de la pelota, pero cuando es ejecutada en serie, también se convierte en una herramienta ofensiva. El ruck se forma con la pelota en el suelo y con al menos un jugador de cada equipo chocando y pujando por la pelota, pero habitualmente son varios. El ruck lo forman los jugadores parados y enfrentados con sus contrincantes, que deben "ruquear" la pelota, esto es tratar de obtenerla. El ruck suele formarse cuando un jugador con la pelota es derribado; sus compañeros vienen entonces a proteger la posesión del balón, pasando un pie por encima de este, tomando así posesión de la pelota y obligando al equipo contrario a pasar completamente por arriba del jugador derribado y correr a los jugadores contrarios para tomar la posición de la pelota. No se puede entrar lateralmente a esta formación ya que seria sancionado con un penal.

Cuando se forman un ruck o un maul, se forman también dos líneas imaginarias de fuera de juego. Estas líneas, paralelas a las de gol, pasan por detrás del pie más retrasado del último jugador de cada bando en el ruck o maul y van de una línea lateral a otra línea lateral. Cualquier jugador que esté delante de su respectiva línea de fuera de juego y no forme parte del ruck o maul se considera fuera de juego y puede ser penalizado si interviene directa o indirectamente en el juego. Al ruck y al maul solo se puede ingresar desde atrás de dichas líneas imaginarias.

El fuera de juego ("offside") es la infracción más común durante un encuentro. Si la penalización se otorga a una distancia razonable para el pateador del equipo no infractor, este puede decidir por patear hacia los postes para obtener tres puntos. El equipo infractor tiene que ubicarse a 10 m de distancia del equipo que patea y no puede hacer ningún movimiento ni ruido, ni siquiera levantar los brazos. Si la falta es convertida (transformada), el juego se reinicia en la línea de centro con un saque del equipo que cometió la infracción. Por el contrario, si la penalización no es convertida (transformada), normalmente se reinicia el encuentro desde los 22 m con una patada de botepronto ("drop") del equipo que cometió la infracción; esta patada se llama "salida de 22 metros". Otras penalizaciones frecuentes incluyen juego peligroso, interferencia, no soltar el balón en el suelo, y lanzarse sobre un ruck (montonera en el suelo). El equipo al que se le otorga la falta (pateador) puede reiniciar el juego con un pequeño toque con el pie (pasando la marca) para iniciar una jugada o con una patada a la línea de banda ("touch") para obtener un saque de banda. En este saque de banda, el equipo que pateó el balón tiene el derecho de lanzar el balón nuevamente en el line-out. Para infracciones menores (tales como adelantar el pie en el scrum), se otorga un tiro libre o "free kick". A diferencia del golpe de castigo ("penal"), este no puede patearse directamente a los postes para ganar puntos. Además, el equipo infractor puede cargar hacia el balón una vez el pateador haya hecho algún movimiento para patear el balón.

El rugby se ha caracterizado por una evolución dinámica de las leyes del juego. Regularmente se introducen modificaciones que tienen por intención agilizar el juego, hacerlo más atractivo para los espectadores y más seguro para los jugadores, y reducir los márgenes de error en los fallos arbitrales: El rugby ha sido el primer deporte colectivo en adoptar la revisión en vídeo de las situaciones de difícil resolución, realizada por un cuarto árbitro a instancias del árbitro principal. En general, las modificaciones suelen probarse primero en un ámbito restringido (en los últimos años, la Universidad de Stellenbosch en Sudáfrica, para luego extenderse a un ámbito mayor (por ejemplo, algún torneo regional importante), para luego generalizarse como “variaciones experimentales”. La IRB analiza luego el resultado de este proceso, y finalmente se incorporan como leyes permanentes las variaciones experimentales que se hayan evaluado positivamente.

En 2008 la IRB aprobó una serie de modificaciones sustanciales al reglamento conocidas por su sigla en inglés, ELV (Experimental Law Variations), o Variaciones Reglamentarias Experimentales, que se pusieron en práctica en todos los torneos oficiales del mundo entre el 1 de agosto de 2008 y el 1 de junio de 2009. Tras de ser evaluadas globalmente durante la temporada 2008/2009, el IRB decidió confirmar 10 de las 13 variaciones e incorporarlas definitivamente a las Leyes del Juego, con excepción de las ELV 2, 3 y 6, que permitían derribar el "maul" y decidir libremente la cantidad de jugadores a colocar en el "line out" (saque de lateral).

Los cambios venían estudiándose desde 2004 y comenzaron a implementarse experimentalmente en 2006 en la universidad sudafricana de Stellenbosch, por lo que son referidas también como las "Reglas de Stellenbosch". De las muchas variaciones propuestas y ensayadas, la IRB decidió finalmente experimentar en todo el mundo trece reglas nuevas. De ellas, 10 fueran finalmente incluidas en las Leyes del Juego en 2009.

El 15 de mayo de 2012 la IRB sancionó once nuevas variaciones experimentales generales, a ser puestas en práctica a partir del 1º de septiembre de 2012 en los torneos del hemisferio norte, y del 1 de enero de 2013 en los del hemisferio sur, y una exclusivamente para la variante "seven a side" vigente a partir del 1 de junio de 2012.

Para la temporada 2013-2014 se produjo una modificación en lo referente a la entrada al "scrum" (ley 20.1 "Formación de un Scrum"), que comenzó a aplicarse al inicio de temporada en cada hemisferio. La variaciòn consiste en reemplazar el tiempo intermedio de la secuencia de entrada de tres tiempos ("cuclillas, tocar, ya") por "tomarse", de modo que los jugadores de las primeras lìneas estén efectivamente asidas entre sí y con la primera línea oponente, de modo de proporcionar mayor estabilidad a la formación antes de la introducciòn de la pelota.

La World Rugby decidió implementar a partir del 3 de enero de 2017 una serie de medidas con el fin de aumentar las precauciones para evitar el tackle alto, es decir el tackle en la que la persona tacleada es tomada o golpeada en la zona del cuello y la cabeza. Para ello la WR incorporó las figuras del "tackle atolondrado" ("reckless tackle") y del "tackle accidental" ("accidental tackle").

El 28 de mayo de 2020, durante la pandemia del Covid-19 la World Rugby elaboró un set de reglas opcionales para las uniones nacionales, con el fin de reducir los riesgos de contagio. Las reglas fueron elaboradas por un grupo de especialistas que incluyó médicos, bajo la guía de la Organización Mundial de la Salud (OMS). Las reglas son opcionales y temporarias para ser utilizadas por las uniones nacionales, que deben actuar siempre cumpliendo las normas sanitarias de cada país y actuar con asesoramiento médico.

Las diez reglas tienen como objetivo reducir el tiempo de contacto físico entre las personas competidoras y son las siguientes:


El rugby es un deporte de intenso contacto físico. Sin embargo, las reglas no permiten el uso de ninguna protección rígida, pues estas podrían causar lesiones a los jugadores. Solo se permiten protecciones acolchadas de hasta 5 mm de espesor en algunas zonas del cuerpo; estas protecciones deben ser aprobadas por la World Rugby. Normalmente se emplean un protector bucal de material siliconado; una camiseta elástica (usada por debajo de la camiseta del equipo) con protecciones para hombros y cuello, y a veces también para esternón, costillas, riñones, columna vertebral y bíceps; un casquete blando, destinado mayormente a reducir el efecto de los golpes en las orejas; y unas calzas cortas o medianas de contención. Se permite el uso de otras protecciones no rígidas y de espesor mínimo para prevenir lesiones, como rodilleras o tobilleras, o en algunos casos el uso de suspensorios para proteger los genitales de impactos dañinos.

La versión de este deporte más conocida es la del rugby jugado por equipos de quince jugadores, aunque no es la única. Es lo que se conoce en el mundo anglosajón como "rugby union", en referencia a la federación ("Union") de clubes que se rigen por unas mismas normas y que, tradicionalmente, habían sido universitarios o aficionados. Un partido dura 80 minutos, dividido en dos partes de 40 minutos, con un descanso de 15 minutos entre cada tiempo.

Sigue el modelo propuesto por William Webb Ellis. Por cada equipo juegan un total de 15 jugadores divididos en dos grupos: "forwards" o delanteros y "backs" o tres cuartos. Las denominaciones de los puestos, al igual que el resto de la terminología de juego, varía considerablemente entre España y los demás países hispanohablantes.

Los jugadores del 1 al 8 ("forwards") forman el "pack", la "delantera" o "paquete" para realizar el scrum (la melé):

"Primera línea": los jugadores que intentan llevar la pelota a su lado y que están en el choque; su función en los scrums es mantener el scrum estable, los pilares (números 1 y 3) suelen ser los más fuertes y pesados de entre todos los jugadores

"Segunda línea": generalmente los jugadores más altos del equipo y que se hacen cargo de empujar en los scrums, también suelen ser los encargados de ganar la pelota en los saques desde el lateral. ("touche", "line-out")

"Tercera línea": los jugadores que mantienen la formación equilibrada para que no se desarme cometiendo una falta.

"Línea de tres cuartos o "backs"": En los distintos países, estos jugadores reciben diferentes nombres de acuerdo con su propia tradición. Así, en Australia y Nueva Zelanda el apertura (n.º 10) y el primer centro (n.º 12) se denominan "first" y "second five eights", respectivamente.


Desde el siglo XIX existe en Inglaterra una variante cuyas reglas difieren en parte y en la que juegan equipos de 13 jugadores; éstos fueron profesionales prácticamente desde la implantación de esa modalidad. A ese juego se lo llamó "rugby league", en referencia al campeonato de liga en que se enfrentaban los clubes que remuneraban a sus jugadores. De Inglaterra pasó a algunos países de la esfera cultural y de influencia británica (Australia, Nueva Zelanda), así como a Francia.

Rugby League Football o Rugby a 13 es un deporte de equipo jugado por dos equipos de 13 jugadores, con 4 en el banco (reservas). El objetivo fundamental, como en el rugby de a 15, consiste en apoyar un balón ovalado en el suelo con las manos sobre o tras la línea de ensayo. Esto se denomina ensayo y tiene un valor, en Rugby League de 4 puntos. Tras el ensayo, el equipo anotador tiene el derecho de patear el balón hacia la portería adversaria, y si consigue pasarlo (transformación) entre los dos palos verticales y por encima del travesaño, anota 2 puntos más. También pueden conseguirse puntos tirando a palos tras un penalti, consistente en tirar a palos durante el juego abierto dejando previamente botar el balón en el suelo. En ambos casos su valor es de 1 punto. El equipo adversario intenta impedir al equipo de ataque realizar este gol obstaculizando al jugador con la pelota.

En áreas de Inglaterra donde el Rugby a 13 predomina - Yorkshire y el Noroeste - el uso del término rugby se refiere, por lo general, al rugby a 13, a diferencia de la mayor parte del país, donde este término se refiere al Rugby Union o Rugby a 15. En áreas de Australia y Nueva Zelanda donde predomina el Rugby a 13, el juego es comúnmente conocido como League o fútbol. En Francia, el juego es llamado el Rugby à Treize, que significa Rugby de a trece en francés. En Argentina, el nombre adoptado fue Rugby 13.

El Rugby a 13 fue jugado al principio por una facción que se escindió de la Federación Inglesa de Rugby (RFU) conocido como la Unión del Norte. Cuando se produjeron también escisiones similares en las federaciones de Rugby afiliadas a la RFU en Australia y Nueva Zelanda, en 1907 y 1908 formaron asociaciones conocidas como Rugby Leagues y usaron las reglas de la Unión del Norte modificadas. La Unión del Norte más tarde cambió su nombre a la Rugby Football League. Así, el juego se hizo conocido como la Rugby League.

El formato de rugby de 7 ("seven-a-side") se juega normalmente en torneos cortos (de un día o un fin de semana). Se utiliza el mismo campo que en la modalidad de 15 hombres, pero con solo 7 jugadores por equipo. Las variaciones respecto de las reglas del juego de quince son:
Actualmente existe un Campeonato del Mundo de Rugby a 7 y un circuito mundial, y ha sido aceptado como deporte olímpico para las Olimpiadas de 2016.

El rugby es un deporte en el que tradicionalmente se ha dado gran importancia a los valores morales. Las normas oficiales del juego están integradas por lo que se denomina "Documento del Juego", orientado a garantizar la conducta ética de todos los involucrados en el juego, "tanto dentro como fuera del campo". Una muestra de la importancia de los valores éticos en el rugby es la disposición referida al espíritu del juego que está incluida en el Documento:

Desde temprana edad a los jugadores de rugby se les enseñan una serie de cualidades positivas, como son el compañerismo, la honestidad, el respeto, la disciplina, la lealtad, el sacrificio y el altruismo.

A diferencia de otros deportes de equipo, en el rugby los jugadores no suelen discutir a los árbitros sus decisiones, ni tratan de engañarlos para sacar partido de sus decisiones. Los tantos son necesariamente consecuencia del esfuerzo de todos, por lo que no se producen las celebraciones individuales, tras la consecución de un try (ensayo) o una conversión (transformación), que se producen en otros deportes.

Al final del partido los jugadores de ambos equipos confraternizan juntos en el llamado «tercer tiempo», en el que beben y comen juntos por invitación del equipo local.

En el marco del cierre de la World Rugby Conference and Exhibition que se celebró en Londres el día 19 de noviembre de 2014, la IRB, la asociación madre del rugby mundial, presentó su nuevo programa de cambio de marca y pasó oficialmente a llamarse World Rugby.

En el corazón de la marca hay un posicionamiento distinto, expresado visualmente a través de un logo más moderno y progresista que encarna la misión de World Rugby para hacer crecer el juego en todo el mundo, manteniendo un vínculo con el patrimonio de la organización a través de su combinación de colores azul y verde.

El presidente de World Rugby, Bernard Lapasset, dijo: «El rugby ha crecido mucho en los últimos cuatro años, alcanzando una participación global de 6,6 millones de jugadores, impulsado por el éxito comercial de la Rugby World Cup, las estrategias de desarrollo de World Rugby y sus inversiones récord, la fortaleza de las Uniones y el regreso del rugby al Programa Olímpico».

El anuncio fue realizado en el cierre de una exitosa World Rugby Conference and Exhibition, que reunió a más de 700 delegados de 60 países durante más de dos días en los que participaron de talleres que invitaron a la reflexión y charlas que cubrieron los temas más importantes del rugby actual: el bienestar de jugador, la integridad para el futuro de la Copa del Mundo de Rugby y la realización de un excepcional evento de Rugby Seven en los Juegos Olímpicos de Río de Janeiro 2016.

Por lo general, y a diferencia de otros deportes, las selecciones nacionales de rugby tienen unos apodos afectivos por los que son conocidos sus equipos. Las de los diez primeros equipos según la clasificación del IRB a finales de octubre de 2007 son:


El rugby fue incluido como deporte olímpico a iniciativa del Barón Pierre de Coubertin, impulsor de las Olimpiadas modernas, quien había sido árbitro de la final de 1892, entre Stade Français y Racing Club de France. Estuvo presente en los Juegos Olímpicos de París 1900, Londres 1908, Amberes 1920 y París 1924. Las causas de su exclusión fueron la mínima cantidad de países participantes (solo 3 en 1924), el debut de las mujeres en los Juegos Olímpicos de Ámsterdam 1928 y el mayor énfasis del COI en los deportes individuales.


Rugby por país




</doc>
<doc id="2457" url="https://es.wikipedia.org/wiki?curid=2457" title="Red Hat">
Red Hat

Red Hat, Inc. es una multinacional estadounidense de software que provee software de código abierto principalmente a empresas. Fundada en 1993, Red Hat tiene su sede corporativa en Raleigh, North Carolina, con oficinas satélite en todo el mundo.

Red Hat es conocida en gran medida por su sistema operativo empresarial Red Hat Enterprise Linux y por la adquisición del proveedor de middleware empresarial de código abierto JBoss. Red Hat también ofrece Red Hat Virtualization (RHV), un producto de virtualización empresarial. Red Hat proporciona almacenamiento, plataformas de sistemas operativos, middleware, aplicaciones, productos de administración y servicios de soporte, capacitación y consultoría.

Red Hat crea, mantiene y contribuye a muchos proyectos de software libre. Ha adquirido muchos productos de software propietario, a través de fusiones y adquisiciones y ha liberado el código de estos aplicativos como código abierto. Red Hat es el segundo mayor contribuyente al núcleo Linux versión 4.14, tras Intel.

El 28 de octubre de 2018, IBM anunció su intención de adquirir Red Hat por $ 33,4 mil millones.

En 1993, Bob Young incorporó ACC Corporation, una empresa de catálogos que vendía accesorios de software de Linux y Unix. En 1994, Marc Ewing creó su propia distribución de Linux, a la que llamó Red Hat Linux. (Ewing había usado un sombrero rojo de lacrosse de la Universidad de Cornell, que le había dado su abuelo, mientras asistía a la Universidad Carnegie Mellon). Ewing lanzó el software en octubre, y se conoció como el lanzamiento de Halloween. Young compró el negocio de Ewing en 1995, y los dos se fusionaron para convertirse en Red Hat Software, con Young como director ejecutivo.

Red Hat salió a bolsa el 11 de agosto de 1999, logrando la octava ganancia de primer día en la historia de Wall Street. Matthew Szulik sucedió a Bob Young como CEO en diciembre de ese año. Bob Young fundó la empresa de impresión a pedido y autoedición en línea, Lulu, en 2002.

El 15 de noviembre de 1999, Red Hat adquirió Cygnus Solutions. Cygnus proporcionó soporte comercial para el software gratuito y alojó a los mantenedores de productos de software GNU como el Depurador GNU y Binutils. Uno de los fundadores de Cygnus, Michael Tiemann, se convirtió en el director técnico de Red Hat, y para 2008, en vicepresidente de asuntos de código abierto. Más tarde, Red Hat adquiere WireSpeed, C2Net y Hell's Kitchen Systems.

En febrero de 2000, InfoWorld otorgó a Red Hat su cuarto premio consecutivo de "Producto del sistema operativo del año" para Red Hat Linux 6.1. Red Hat adquirió Planning Technologies, Inc en 2001 y el directorio iPlanet de AOL y el software de servidor de certificados en 2004.

Red Hat trasladó su sede de Durham al Centennial Campus de North Carolina State University en Raleigh, Carolina del Norte, en febrero de 2002. En el mes siguiente, Red Hat presentó el servidor avanzado Red Hat Linux, posteriormente renombrado Red Hat Enterprise Linux (RHEL). Dell, IBM, HP y Oracle Corporation anunciaron su apoyo a la plataforma.

En diciembre de 2005, la revista "CIO Insight" realizó su "Encuesta de valor de proveedor" anual, en la que Red Hat se ubicó en el puesto número 1 en valor por segundo año consecutivo. El stock de Red Hat se convirtió en parte del NASDAQ-100 el 19 de diciembre de 2005.

Red Hat adquirió el proveedor de middleware de código abierto JBoss el 5 de junio de 2006, y JBoss se convirtió en una división de Red Hat. El 18 de septiembre de 2006, Red Hat lanzó la Red Hat Application Stack, que integraba la tecnología JBoss y que estaba certificada por otros proveedores de software conocidos. El 12 de diciembre de 2006, las acciones de Red Hat pasaron de cotizar en NASDAQ (RHAT) a la Bolsa de Nueva York (RHT). En 2007, Red Hat adquirió MetaMatrix e hizo un acuerdo con Exadel para distribuir su software.

El 15 de marzo de 2007, Red Hat lanzó Red Hat Enterprise Linux 5 y en junio adquirió Mobicents. El 13 de marzo de 2008, Red Hat adquirió Amentra, un proveedor de servicios de integración de sistemas para arquitectura orientada a servicios, administración de procesos de negocios, desarrollo de sistemas y servicios de datos empresariales.

El 27 de julio de 2009, Red Hat reemplazó a CIT Group en el índice de acciones Standard and Poor's 500, un índice diversificado de 500 compañías líderes de la economía de los EE. UU. Esto fue reportado como un hito importante para Linux.

El 15 de diciembre de 2009, se informó que Red Hat pagará US $ 8,8 millones para resolver una demanda colectiva relacionada con la reexpresión de los resultados financieros de julio de 2004. La demanda estaba pendiente en el Tribunal de Distrito de EE. UU. Para el Distrito Este de Carolina del Norte. Red Hat alcanzó el acuerdo de liquidación propuesto y registró un cargo único de US $ 8,8 millones por el trimestre que finalizó el 30 de noviembre..

El 10 de enero de 2011, Red Hat anunció que ampliaría su sede en dos fases, agregando 540 empleados a la operación de Raleigh e invirtiendo más de US $109 millones. El estado de Carolina del Norte ofrece hasta US$15 millones en incentivos. La segunda fase implica la "expansión a nuevas tecnologías, como la visualización de software y las ofertas de tecnología en la nube".

El 25 de agosto de 2011, Red Hat anunció que trasladaría a unos 600 empleados del campus centenario del estado de N. C. al centro de Two Progress Plaza. El 24 de junio de 2013 se llevó a cabo una ceremonia de corte de cinta en la sede de Red Hat.

En 2012, Red Hat se convirtió en la primera compañía de código abierto de un billón de dólares, alcanzando US $ 1,13 mil millones en ingresos anuales durante su año fiscal. Red Hat aprobó la referencia de $2 mil millones en 2015. A febrero de 2018, los ingresos anuales de la compañía fueron de casi $3 mil millones.

El 16 de octubre de 2015, Red Hat anunció la adquisición de la empresa de automatización de TI Ansible, que se rumorea por un estimado de $ 100 millones de dólares.

En mayo de 2018, Red Hat adquiere CoreOS.

El 28 de octubre de 2018, IBM anunció su intención de adquirir Red Hat por US $34 mil millones, en una de sus adquisiciones más grandes. La compañía operará fuera de la división Hybrid Cloud de IBM.

Sus principales productos son la distribución Red Hat Enterprise Linux, el servidor de aplicaciones libre JBoss, la herramienta de Mapeo objeto-relacional Hibernate y más soluciones en el ámbito de los servidores.

Por otra parte Red Hat patrocina y dirige la distribución Fedora, la cual usa para probar nuevas tecnologías. También participa en el proyecto "One Laptop per Child" y mantiene el sitio web Red Hat Magazine.

En septiembre de 2003, Red Hat decidió concentrar sus esfuerzos de desarrollo en la versión corporativa de su distribución y delegó la versión común a Fedora Core, un proyecto abierto independiente de Red Hat, pero patrocinado por la empresa. 

Los ingenieros de Red Hat trabajaron con la iniciativa One Laptop per Child (una organización sin fines de lucro establecida por miembros del MIT Media Lab) para diseñar y producir una computadora portátil de bajo costo e intentar proporcionar a todos los niños del mundo acceso a comunicación abierta y conocimiento abierto y aprendizaje abierto. La computadora portátil , la última máquina de este proyecto, ejecuta una versión reducida de Fedora 17 como su sistema operativo.

Red Hat es el mayor contribuyente al entorno de escritorio GNOME. Cuenta con varios empleados que trabajan a tiempo completo en , el personal de GNOME.

Dogtail, un marco de prueba de interfaz gráfica de usuario automatizada (GUI) de código abierto desarrollado inicialmente por Red Hat, consta de software gratuito publicado bajo la Licencia Pública General de GNU (GPL) y está escrito en Python. Permite a los desarrolladores construir y probar sus aplicaciones. Red Hat anunció el lanzamiento de Dogtail en la Cumbre Red Hat 2006. 

Red Hat MRG es un producto de clustering destinado a la informática integrada de alto rendimiento. El acrónimo MRG significa "Messaging Realtime Grid".

Red Hat Enterprise MRG reemplaza Red Hat Enterprise Linux RHEL, una distribución de Linux desarrollada por Red Hat, con el fin de proporcionar soporte adicional para la computación en tiempo real, la carga de trabajo de programación a máquinas virtuales locales o remotas, grid computing y cloud computing. 

A partir de 2011, Red Hat trabaja con la comunidad del Sistema de computación de alto rendimiento de y también brinda soporte para el software.

La herramienta de control de rendimiento de Tuna se ejecuta en el entorno MRG. 

Red Hat produce la publicación en línea Opensource.com. El sitio destaca formas en que los principios de código abierto se aplican en dominios distintos del desarrollo de software. El sitio rastrea la aplicación de la filosofía de código abierto a los negocios, la educación, el gobierno, la ley, la salud y la vida.

La compañía originalmente produjo un boletín llamado Under the Brim. La revista Wide Open apareció por primera vez en marzo de 2004, como un medio para que Red Hat comparta contenido técnico con suscriptores de manera regular. El boletín Under the Brim y la revista Wide Open se fusionaron en noviembre de 2004 para convertirse en Red Hat Magazine. En enero de 2010, la revista Red Hat se convirtió en Opensource.com. 

En 2007, Red Hat anunció que había llegado a un acuerdo con algunas compañías de software libre y de código abierto (FOSS) que le permitieron hacer un portal de distribución llamado Red Hat Exchange, revender el software FOSS con la marca original intacta. Sin embargo, para 2010, Red Hat había abandonado el programa de Exchange para centrar sus esfuerzos más en su Open Source Channel Alliance que comenzó en abril de 2009. 

Red Hat Single Sign On es un producto de software que permite el inicio de sesión único con Identity Management and Access Management dirigido a aplicaciones y servicios modernos. Hay un proyecto de código abierto en curso junto con Red Hat SSO, que es . Keycloak es básicamente la versión comunitaria de Red Hat SSO. Red Hat Single Sign On 7.3 es la última versión disponible.

Red Hat Subscription Management (RHSM) combina la entrega de contenido con la administración de suscripciones.

Red Hat opera OpenShift, una plataforma de cloud computing, que admite aplicaciones escritas en Node.js, PHP, Perl, Python, Ruby, JavaEE y más.

El 31 de julio de 2018, Red Hat anunció el lanzamiento de Istio 1.0, un programa de administración de microservicios que se utiliza junto con la plataforma Kubernetes. El software pretende proporcionar servicios de "gestión de tráfico, identidad y seguridad del servicio, aplicación de políticas y telemetría" para optimizar el uso de Kubernetes en los diversos sistemas operativos basados ​​en Fedora. Brian Redbeard Harring de Red Hat describió a Istio como "con el objetivo de ser un plano de control, similar al plano de control de Kubernetes, para configurar una serie de servidores proxy que se inyectan entre los componentes de la aplicación". 

Red Hat comercializa una versión de OpenStack que ayuda a administrar un centro de datos de manera que se realiza en cloud computing. 

Red Hat CloudForms proporciona administración de máquinas virtuales, instancias y contenedores basados ​​en VMware vSphere, Red Hat Virtualization, Microsoft Hyper-V, OpenStack, Amazon EC2, Google Cloud Platform, Microsoft Azure y Red Hat OpenShift. CloudForms se basa en el proyecto de fuente abierta de Red Hat. El código en ManageIQ proviene de la adquisición de ManageIQ por más de US $ 100 millones en 2012. 

Red Hat contribuye, con varios desarrolladores de software, a LibreOffice, una suite ofimática gratuita y de código abierto. 

Red Hat tiene algunos empleados que trabajan a tiempo completo en otros proyectos de software libre y de código abierto que no son productos de Red Hat, como dos empleados a tiempo completo que trabajan en el software gratuito radeon (David Airlie y Jerome Glisse ) y uno empleado a tiempo completo que trabaja en los controladores gráficos nouveau de software libre. Otro de estos proyectos es , un proyecto de código abierto que aporta experiencia en seguridad y desarrollo al desarrollo móvil empresarial multiplataforma. [Cita requerida]

Red Hat también organiza eventos de "Día de código abierto" donde múltiples socios muestran sus tecnologías de código abierto.

Los suscriptores tienen acceso a:


Más allá de los principales productos y adquisiciones de Red Hat, los programadores de Red Hat han producido herramientas y herramientas de programación de software para complementar el software estándar de Unix y Linux. Algunos de estos "productos" de Red Hat han encontrado su camino desde entornos operativos específicamente de Red Hat a través de canales de código abierto hasta una comunidad más amplia. Dichas utilidades incluyen:


El sitio web de Red Hat enumera las principales implicaciones de la organización en proyectos de software libre y de código abierto. 

Los proyectos comunitarios bajo los auspicios de Red Hat incluyen:




</doc>
<doc id="2459" url="https://es.wikipedia.org/wiki?curid=2459" title="Revisión por pares">
Revisión por pares

La revisión por pares es la evaluación del trabajo realizada por una o más personas con competencias similares a las de los productores del trabajo (pares). Funciona como una forma de autorregulación de miembros calificados de una profesión dentro del campo relevante. Los métodos de revisión por pares se utilizan para mantener los estándares de calidad, mejorar el rendimiento y proporcionar credibilidad. En el ámbito académico, la revisión por pares académicos a menudo se usa para determinar la idoneidad de un artículo académico para su publicación. La revisión por pares se puede clasificar por el tipo de actividad y por el campo o la profesión en la que se realiza la actividad, por ejemplo, la revisión médica por pares.

La revisión profesional por pares se centra en el desempeño de los profesionales, con miras a mejorar la calidad, mantener los estándares o proporcionar certificación. Henry Oldenburg (1619-1677) fue un filósofo británico nacido en Alemania que es visto como el "padre" de la revisión científica moderna por pares.

La revisión profesional por pares es común en el campo de la atención médica, donde generalmente se denomina revisión clínica por pares. Además, dado que la actividad de revisión por pares se segmenta comúnmente por disciplina clínica, también hay revisión por pares médicos, revisión por pares de enfermería, revisión por pares en odontología, etc. [6] Muchos otros campos profesionales tienen cierto nivel de proceso de revisión por pares: contabilidad, derecho, ingeniería (por ejemplo, revisión por pares de software, revisión técnica por pares), aviación e incluso gestión de incendios forestales.

Diferentes métodos de desarrollo de software incluyen etapas que comprenden arbitraje. Incluyen definición de requerimientos, diseño detallado y desarrollo de código. Uno de los enfoques muy rigurosos es el denominado inspección de software. En el movimiento del software libre se utiliza un procedimiento semejante al arbitraje, pues quien desee puede revisar, criticar y mejorar el software.

En este contexto, para la función del arbitraje existe una homóloga: la ley de Linus, que generalmente se expresa así: «Dados suficientes ojos, todo error es superficial». Esto se interpreta como «Con suficientes revisores, cualquier problema puede resolverse fácilmente». Eric S. Raymond, en su libro "La Catedral y el Bazar," reflexiona acerca de los beneficios de la aplicación del arbitraje en el desarrollo de software, en virtud de que permite encontrar defectos mucho más rápidamente que por "testing" o por informes de los usuarios acerca de errores. Esto minimiza tiempo, esfuerzo y los costos inherentes.





</doc>
<doc id="2463" url="https://es.wikipedia.org/wiki?curid=2463" title="Teoría de la relatividad especial">
Teoría de la relatividad especial

La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, es una teoría de la física publicada en 1905 por Albert Einstein. Surge de la observación de que la velocidad de la luz en el vacío es igual en todos los sistemas de referencia inerciales y de obtener todas las consecuencias del principio de relatividad de Galileo, según él, cualquier experimento realizado, en un sistema de referencia inercial, se desarrollará de manera idéntica en cualquier otro sistema inercial.

La teoría es "especial", ya que solo se aplica en el caso especial/particular donde la curvatura del espacio-tiempo producida por acción de la gravedad es irrelevante, es decir, en esta teoría Einstein no tuvo en cuenta a la gravedad como variable. Con el fin de incluir la gravedad, Einstein formuló la relatividad general en 1915. La relatividad general es capaz de manejar marcos de referencia acelerados, algo que no era posible con las teorías anteriores.

La Teoría de la relatividad especial estableció nuevas ecuaciones que facilitan pasar de un sistema de referencia inercial a otro. Las ecuaciones correspondientes conducen a fenómenos que chocan con el sentido común, como son la contracción espacial, la dilatación del tiempo, un límite universal a la velocidad, la equivalencia entre masa y energía o la relatividad de la simultaneidad entre otros, siendo la fórmula E=mc o la paradoja de los gemelos dos de los ejemplos más conocidos.

La relatividad especial tuvo también un impacto en la filosofía, eliminando toda posibilidad de existencia de un tiempo y de un espacio absoluto en el conjunto del universo.

A finales del siglo XIX los físicos pensaban que la mecánica clásica de Newton, basada en la llamada relatividad de Galileo Galilei (origen de las ecuaciones matemáticas conocidas como transformaciones de Galileo), describía los conceptos de velocidad y fuerza para todos los observadores (o sistemas de referencia). Sin embargo, Hendrik Lorentz y un poco antes Woldemar Voigt habían comprobado que las ecuaciones de Maxwell, que gobiernan el electromagnetismo, no cumplían las transformaciones de Galileo cuando el sistema de referencia inercial varía (por ejemplo, cuando se considera el mismo problema físico desde el punto de vista de dos observadores que se mueven uno respecto del otro). En particular las ecuaciones de Maxwell parecían requerir que la velocidad de la luz fuera constante (razón por la que se interpretó que esa velocidad se refería a la velocidad de la luz respecto al éter). Sin embargo, el experimento de Michelson y Morley sirvió para confirmar que la velocidad de la luz permanecía constante para cualquier velocidad y movimiento relativo al supuesto éter omnipresente y, además, independientemente del sistema de referencia en el cual se medía (contrariamente a lo esperado de aplicar las transformaciones de Galileo) . Por tanto la hipótesis del éter quedaba descartada y se abría un problema teórico grave asociado a las transformaciones de Galileo. Hendrik Lorentz ya había encontrado que las transformaciones correctas que garantizaban la invariancia no eran las de Galileo, sino las que actualmente se conocen como transformaciones de Lorentz.

Durante años las transformaciones de Lorentz y los trabajos de Henri Poincaré sobre el tema quedaron inexplicados hasta que Albert Einstein, un físico desconocido hasta 1905, sería capaz de darles una interpretación considerando el carácter relativo del tiempo y el espacio. Einstein también había sido influido por el físico y filósofo Ernst Mach. Einstein leyó a Ernst Mach cuando era estudiante y ya era seguidor suyo en 1902, cuando vivía en Zúrich y se reunía regularmente con sus amigos Conrad Habicht y Maurice Solovine (Véase Academia Olimpia). Einstein insistió para que el grupo leyese los dos libros que Mach había publicado hasta esa fecha: "El desarrollo de la mecánica" (título original, "Die Mechanik in ihrer Entwicklung", Leipzig, 1883) y "El análisis de las sensaciones" ("Die Analyse der Empfindungen und das Verhältnis des Physischen zum Psychischen", Jena, 1886). Einstein siempre creyó que Mach había estado en el camino correcto para descubrir la relatividad en parte de sus trabajos de juventud, y que la única razón por la que no lo había hecho fue porque la época no fue la propicia. El artículo de 1905 de Einstein, titulado "Zur Elektrodynamik bewegter Körper", cambió radicalmente la percepción del espacio y el tiempo que se tenía en ese entonces. En ese artículo Einstein introducía lo que ahora conocemos como teoría de la relatividad especial. Esta teoría se basaba en el principio de relatividad y en la constancia de la velocidad de la luz en cualquier sistema de referencia inercial. De ello Einstein dedujo las ecuaciones de Lorentz. También reescribió las relaciones del momento y de la energía cinética para que éstas también se mantuvieran invariantes.

La teoría permitió establecer la equivalencia entre masa y energía y una nueva definición del espacio-tiempo. De ella se derivaron predicciones y surgieron curiosidades. Como ejemplos, un observador atribuye a un cuerpo en movimiento una longitud más corta que la que tiene el cuerpo en reposo y la duración de los eventos que afecten al cuerpo en movimiento son más largos con respecto al mismo evento medido por un observador en el sistema de referencia del cuerpo en reposo.

En 1912, Wilhelm Wien, premio Nobel de Física de 1911, propuso a Lorentz y a Einstein para este galardón por la teoría de la relatividad, expresando Einstein no recibió el premio Nobel por la relatividad especial pues el comité, en principio, no otorgaba el premio a teorías puras. El Nobel no llegó hasta 1921, y fue por su trabajo sobre el efecto fotoeléctrico.


La fuerza del argumento de Einstein está en la forma en que se deducen de ella resultados sorprendentes y plausibles a partir de dos simples hipótesis y cómo estas predicciones las confirmaron las observaciones experimentales. Matemáticamente hablando, en ambos postulados, tomados en conjunto, implicaban que cualquier ley física debía ser invariante respecto a una transformación de Lorentz. Es decir, que en todos los sistemas inerciales la forma matemática de las ecuaciones debía ser forminvariante de Lorentz.

Cuando se aplican estos dos principios a las ecuaciones de Maxwell se ve que éstas solo son invariantes bajo las transformaciones de Lorentz, lo que implica que el intervalo de tiempo entre dos sucesos o la distancia entre dos puntos deben ser relativos al observador. Es decir, no todos los observadores medirán el mismo intervalo de tiempo entre dos sucesos o la misma longitud para un mismo objeto. Ese carácter no absoluto, sino relativo del espacio y el tiempo, que es una consecuencia de requerir que las medidas tomadas por diferentes observadores dejen invariantes las ecuaciones de Maxwell es la fuente de todos los resultados sorprendentes de la teoría de la relatividad. Cuando se examinan las leyes de Newton y otras leyes del movimiento de la mecánica clásica se aprecia que estas deben ser modificadas para ser también invariantes según las mismas transformaciones que las ecuaciones de Maxwell.

Henri Poincaré, matemático francés, sugirió a finales del siglo XIX que el principio de relatividad establecido desde Galileo (la invariancia galileana) se mantiene para todas las leyes de la naturaleza. Joseph Larmor y Hendrik Lorentz descubrieron que las ecuaciones de Maxwell, la piedra angular del electromagnetismo, eran invariantes solo por una variación en el tiempo y una cierta unidad longitudinal, lo que produjo mucha confusión en los físicos, que en aquel tiempo estaban tratando de argumentar las bases de la teoría del éter, la hipotética substancia sutil que llenaba el vacío y en la que se transmitía la luz. El problema es que este éter era incompatible con el principio de relatividad.

En su publicación de 1905 en electrodinámica, Albert Einstein explicó que, con las transformaciones hechas por Lorentz, este principio se mantenía perfectamente invariable. La contribución de Einstein fue el elevar este axioma a principio y proponer las transformadas de Lorentz como primer principio. Además descartó la noción de tiempo absoluto y requirió que la velocidad de la luz en el vacío sea la misma para todos los observadores, sin importar si estos se movían o no. Esto era fundamental para las ecuaciones de Maxwell, ya que éstas necesitan de una invarianza general de la velocidad de la luz en el vacío.

La teoría de la relatividad especial además busca formular todas las leyes físicas de forma que tengan validez para todos los observadores inerciales. Por lo que cualquier ley física debería tener una forma matemática invariante bajo unas transformaciones de Lorentz.

Como se ha mencionado, los físicos de la época habían encontrado una inconsistencia entre la completa descripción del electromagnetismo realizada por Maxwell y la mecánica clásica. Para ellos, la luz era una onda electromagnética transversal que se movía por un sistema de referencia privilegiado, al cual lo denominaban éter.

Hendrik Antoon Lorentz trabajó en resolver este problema y fue desarrollando unas transformaciones para las cuales las ecuaciones de Maxwell quedaban invariantes y sin necesidad de utilizar ese hipotético éter. La propuesta de Lorentz de 1899, conocida como la "Teoría electrónica de Lorentz", no excluía —sin embargo— al éter. En la misma, Lorentz proponía que la interacción eléctrica entre dos cuerpos cargados se realizaba por medio de unos corpúsculos a los que llamaba electrones y que se encontraban adheridos a la masa en cada uno de los cuerpos. Estos electrones interactuaban entre sí mediante el éter, el cual era contraído por los electrones acorde a transformaciones específicas, mientras estos se encontraban en movimiento relativo al mismo. Estas transformaciones se las conoce ahora como transformaciones de Lorentz. La formulación actual fue trabajo de Poincaré, el cual las presentó de una manera más consistente en 1905.

Se tiene un sistema S de coordenadas formula_1 y un sistema S' de coordenadas formula_2, de aquí las ecuaciones que describen la transformación de un sistema a otro son:
donde
es el llamado factor de Lorentz y formula_4 es la velocidad de la luz en el vacío.

Contrario a nuestro conocimiento actual, en aquel momento esto era una completa revolución, debido a que se planteaba una ecuación para transformar al tiempo, cosa que para la época era imposible. En la mecánica clásica, el tiempo era un invariante. Y para que las mismas leyes se puedan aplicar en cualquier sistema de referencia se obtiene otro tipo de invariante a grandes velocidades (ahora llamadas relativistas), la velocidad de la luz.

Directamente de los postulados expuestos arriba, y por supuesto de las transformaciones de Lorentz, se deduce el hecho de que no se puede decir con sentido absoluto que dos acontecimientos hayan ocurrido al mismo tiempo en diferentes lugares. Si dos sucesos ocurren simultáneamente en lugares separados espacialmente desde el punto de vista de un observador, cualquier otro observador inercial que se mueva respecto al primero los presencia en instantes distintos.

Matemáticamente, esto puede comprobarse en la primera ecuación de las transformaciones de Lorentz:

Dos eventos simultáneos verifican formula_5, pero si sucedieron en lugares distintos (con formula_6), otro observador con movimiento relativo obtiene formula_7. Solo en el caso formula_5 y formula_9 (sucesos simultáneos "en el mismo punto") no ocurre esto.

El concepto de simultaneidad puede definirse como sigue. Dados dos eventos puntuales "E" y "E", que ocurre respectivamente en instantes de tiempo "t" y "t", y en puntos del espacio "P" = ("x", "y", "z") y "P" = ("x", "y", "z"), todas las teorías físicas admiten que estos solo pueden darse una, de tres posibilidades mutuamente excluyentes:


Dado un evento cualquiera, el conjunto de eventos puede dividirse según esas tres categorías anteriores. Es decir, todas las teorías físicas permiten fijado un evento, clasificar a los demás eventos: en (1) pasado, (2) futuro y (3) resto de eventos (ni pasados ni futuros). En mecánica clásica esta última categoría está formada por los sucesos llamados simultáneos, y en mecánica relativista eventos no relacionados causalmente con el primer evento. Sin embargo, la mecánica clásica y la mecánica relativista difieren en el modo concreto en que esa división entre pasado, futuro y otros puede hacerse y en si dicho carácter es absoluto o relativo de dicha partición.

Como se dijo previamente, el tiempo en esta teoría deja de ser absoluto como se proponía en la mecánica clásica. O sea, el tiempo para todos los observadores del fenómeno deja de ser el mismo. Si tenemos un observador inmóvil haciendo una medición del tiempo de un acontecimiento y otro que se mueva a velocidades relativistas, los dos relojes no tendrán la misma medición de tiempo.

Mediante la transformación de Lorentz nuevamente llegamos a comprobar esto. Se coloca un reloj ligado al sistema S y otro al S', lo que nos indica que formula_10. Se tiene las transformaciones y sus inversas en términos de la diferencia de coordenadas:
y

Si despejamos las primeras ecuaciones obtenemos

De lo que obtenemos que los eventos que se realicen en el sistema en movimiento S' serán más largos que los del S. La relación entre ambos es esa formula_15. Este fenómeno se lo conoce como dilación del tiempo.

Si se dice que el tiempo varía a velocidades relativistas, la longitud también lo hace. Un ejemplo sería si tenemos a dos observadores inicialmente inmóviles, estos miden un vehículo en el cual solo uno de ellos "viajará" a grandes velocidades, ambos obtendrán el mismo resultado. Uno de ellos entra al vehículo y cuando adquiera la suficiente velocidad mide el vehículo obteniendo el resultado esperado, pero si el que esta inmóvil lo vuelve a medir, obtendrá un valor menor. Esto se debe a que la longitud también se contrae.

Volviendo a las ecuaciones de Lorentz, despejando ahora a x y condicionando a formula_16 se obtiene:

de lo cual podemos ver que existirá una disminución debido al cociente. Estos efectos solo pueden verse a grandes velocidades, por lo que en nuestra vida cotidiana las conclusiones obtenidas a partir de estos cálculos no tienen mucho sentido.

Un buen ejemplo de estas contracciones y dilataciones fue propuesto por Einstein en su paradoja de los gemelos, y verificado experimentalmente por la anomalía en el tiempo de vida de los muones, producidos por los rayos cósmicos.

La composición de velocidades es el cambio en la velocidad de un cuerpo al ser medida en diferentes sistemas de referencia inerciales. En la física pre-relativista se calculaba mediante

donde "v"′ es la velocidad del cuerpo con respecto al sistema "S"′, "u" la velocidad con la que este sistema se aleja del sistema "en reposo" "S", y "v" es la velocidad del cuerpo medida en "S".

Sin embargo, debido a las modificaciones del espacio y el tiempo, esta relación no es válida en Relatividad Especial. Mediante las transformadas de Lorentz puede obtenerse la fórmula correcta:
</math>
Al observar con cuidado esta fórmula se nota que si tomamos para el cuerpo una velocidad en el sistema "S" igual a la de la luz (el caso de un fotón, por ejemplo), su velocidad en "S"′ sigue siendo "v"′="c", como se espera debido al segundo postulado. Además, si las velocidades son muy pequeñas en comparación con la luz, se obtiene que esta fórmula se aproxima a la anterior dada por Galileo.

El concepto de masa en la teoría de la relatividad especial tiene dos bifurcaciones: la masa invariante y la masa relativista aparente. La masa relativista aparente es la masa aparente que va a depender del observador y se puede incrementar dependiendo de su velocidad, mientras que la invariante es independiente del observador e invariante.

Matemáticamente tenemos que: formula_17 donde formula_18 es la masa relativista aparente, formula_19 es la invariante y formula_20 es el factor de Lorentz. Notemos que si la velocidad relativa del factor de Lorentz es muy baja, la masa relativa tiene el mismo valor que la masa invariante pero si esta es comparable con la velocidad de la luz existe una variación entre ambas. Conforme la velocidad se vaya aproximando a la velocidad de la luz, la masa relativista tenderá a infinito.

Al existir una variación en la masa relativista aparente, la cantidad de movimiento de un cuerpo también debe ser redefinida. Según Newton, la cantidad de movimiento está definida por formula_21 donde formula_19 era la masa del cuerpo. Como esta masa ya no es invariante, nuestra nueva "cantidad de movimiento relativista" tiene el factor de Lorentz incluido así:

Sus consecuencias las veremos con más detenimiento en la sección posterior de fuerza.

La relatividad especial postula una ecuación para la energía, la cual llegó a ser la ecuación más famosa del planeta, "E" = "mc". A esta ecuación también se la conoce como la equivalencia entre masa y energía. En la relatividad, la energía y el momento de una partícula están relacionados mediante la ecuación:

Esta relación de energía-momento formulada en la relatividad nos permite observar la independencia del observador tanto de la energía como de la cantidad de momento. Para velocidades no relativistas, la energía puede ser aproximada mediante una expansión de una serie de Taylor así
encontrando así la energía cinética de la mecánica de Newton. Lo que nos indica que esa 
mecánica no era más que un caso particular de la actual relatividad. El primer término de esta aproximación es lo que se conoce como la energía en reposo (energía potencial), esta es la cantidad de energía que puede medir un observador en reposo de acuerdo con lo postulado por Einstein. Esta energía en reposo no causaba conflicto con lo establecido anteriormente por Newton, porque esta es constante y además persiste la energía en movimiento. Einstein lo describió de esta manera: 

En mecánica newtoniana la fuerza no relativista puede obtenerse simplemente como la derivada temporal del momento lineal:
Pero contrariamente postula la mecánica newtoniana, aquí el momento no es simplemente la masa en reposo por la velocidad. Por lo que la ecuación formula_23 ya no es válida en relatividad. Si introducimos la definición correcta del momento lineal, usando la masa aparente relativista entonces obtenemos la expresión relativista correcta:
m\frac {d\gamma}{dt}\mathbf{v} + \gamma m\frac {d\mathbf{v}}{dt} </math>
donde formula_24 es la masa relativista aparente. Calculando la fuerza anterior se observa el hecho que la fuerza podría no tener necesariamente la dirección de la aceleración, como se deduce desarrollando la ecuación anterior:
Introduciendo las :

Existen dos casos particulares de movimiento de una partícula donde la fuerza es siempre paralela a la aceleración, que son el movimiento rectilíneo uniformemente acelerado y el movimiento circular uniforme; en el primer caso el factor de proporcionalidad es formula_25 y el en segundo formula_26

La relatividad especial usa tensores y cuadrivectores para representar un espacio pseudo-euclídeo. Este espacio, sin embargo, es similar al espacio euclídeo tridimensional en muchos aspectos y es relativamente fácil trabajar en él. El tensor métrico que da la distancia elemental ("ds") en un espacio euclídeo se define como:

donde formula_27 son diferenciales de las tres coordenadas cartesianas espaciales. En la geometría de la relatividad especial, se añade una cuarta dimensión imaginaria dada por el producto "ict", donde "t" es el tiempo, "c" la velocidad de la luz e "i" la unidad imaginaria: quedando el intervalo relativista, en forma diferencial, como:

El factor imaginario se introduce para mostrar el carácter pseudoeuclídeo de la geometría espacio-tiemporal. Si se reducen las dimensiones espaciales a 2, se puede hacer una representación física en un espacio tridimensional,
Se puede ver que las geodésicas con medida cero forman un cono dual definido por la ecuación

La ecuación anterior es la de círculo con formula_28. Si se extiende lo anterior a las tres dimensiones espaciales, las geodésicas nulas son esferas concéntricas, con radio = distancia = c por tiempo.

Este doble cono de distancias nulas representa el "horizonte de visión" de un punto en el espacio. Esto es, cuando se mira a las estrellas y se dice: "La estrella de la que estoy recibiendo luz tiene X años", se está viendo a través de esa línea de visión: una geodésica de distancia nula. Se está viendo un suceso a formula_29 metros, y formula_30 segundos en el pasado. Por esta razón, el doble cono es también conocido como cono de luz (El punto inferior de la izquierda del diagrama inferior representa la estrella, el origen representa el observador y la línea representa la geodésica nula, el "horizonte de visión" o "cono de luz"). Es importante notar que solo los puntos interiores al cono de luz de un evento pueden estar en relación causal con ese evento.

Previo a esta teoría, el concepto de causalidad estaba determinado: "para una causa existe un efecto". Anteriormente, gracias a los postulados de Laplace, se creía que para todo acontecimiento se debía obtener un resultado que podía predecirse. La revolución en este concepto es que se "crea" un "cono de luz" de posibilidades (Véase gráfico adjunto).

Se observa este cono de luz y ahora un acontecimiento en el cono de luz del pasado no necesariamente nos conduce a un solo efecto en el cono de luz futuro. Desligando así la causa y el efecto. El observador que se sitúa en el vértice del cono ya no puede indicar qué causa del cono del pasado provocará el efecto en el cono del futuro.

Asumiendo el principio de causalidad e ingnorando ciertas posibilidades relacionadas con el movimiento superlumínico, obtenemos que ninguna partícula de masa positiva en reposo puede viajar más rápido que la luz. En particular, la relación entre la energía cinética "K" necesaria para acelerar rectilíneamente una partícula desde el reposo hasta una cierta velocidad "v" viene expresada por la ecuación:

Aquí puede verse claramente que para cualquier valor finito de "K" se cumplirá que "v" < "c". Otra manera de ver esta imposibilidad es usar el principio de causalidad, y aplicarlo al movimiento más rápido que el de la luz. Imagínese un cuerpo que experimenta una fuerza durante una cantidad infinita de tiempo. Tenemos entonces que para un movimiento rectilíneo:
</math>
De la expresión anterior se deduce que la "inercia efectiva", entendida como la resistencia que opone el cuerpo a ser acelerado "F / a", irá aumentando indefinidamente a medida que "v" se acerca a "c".

Por otra parte, esta conclusión depende críticamente de la asunción de causalidad. Así en mecánica cuántica esta asunción no se considera, por lo que algunas partículas virtuales no están sujetas a esa restricción. Además existen propuestas teóricas que postulan la existencia de partículas hipotéticas que podrían viajar más rápido que la luz, los taquiones, naturalmente en esas teorías no se asume el principio de causalidad en la forma planteada aquí.

La relatividad especial a pesar de poder ser descrita con facilidad por medio de la mecánica clásica y ser de fácil entendimiento, tiene una compleja matemática de por medio. Aquí se describe a la relatividad especial en la forma de la covariancia de Lorentz. La posición de un evento en el espacio-tiempo está dado por un vector contravariante cuatridimensional, sus componentes son:

esto es que formula_31, formula_32, formula_33 y formula_34. Los superíndices de esta sección describen contravarianza y no exponente a menos que sea un cuadrado o se diga lo contrario. Los superíndices son índices covariantes que tienen un rango de cero a tres como un gradiente del espacio tiempo del campo φ:

Habiendo reconocido la naturaleza cuatridimensional del espacio-tiempo, se puede empezar a emplear la métrica de Minkowski, η, dada en los componentes (válidos para cualquier sistema de referencia) así:
Su inversa es:
Luego se reconoce que las transformaciones coordenadas entre los sistemas de referencia inerciales están dadas por el tensor de transformación de Lorentz Λ. Para el caso especial de movimiento a través del eje x, se tiene:

que es simplemente la matriz de un boost (como una rotación) entre las coordenadas "x" y "t". Donde μ' indica la fila y ν la columna. También β y γ están definidos como:
</math>
Más generalmente, una transformación de un sistema inercial (ignorando la translación para simplificarlo) a otro debe satisfacer:

donde hay un sumatorio implícita de formula_35 y formula_36 de cero a tres en el lado derecho, de acuerdo con el Convenio de sumación de Einstein. El grupo de Poincaré es el grupo más general de transformaciones que preservan la métrica de Minkowski y esta es la simetría física subyacente a la relatividad especial.

Todas las propiedades físicas cuantitativas son dadas por tensores. Así para transformar de un sistema a otro, se usa la muy conocida ley de transformación tensorial

donde formula_37 es la matriz inversa de formula_38. Para observar como esto es útil, transformamos la posición de un evento de un sistema de coordenadas "S" a uno "S"', se calcula

que son las transformaciones de Lorentz dadas anteriormente. Todas las transformaciones de tensores siguen la misma regla. El cuadrado de la diferencia de la longitud de la posición del vector formula_39 construido usando

es un invariante. Ser invariante significa que toma el mismo valor en todos los sistemas inerciales porque es un escalar (tensor de rango 0), y así Λ no aparece en esta transformación trivial. Se nota que cuando el elemento línea formula_40 es negativo formula_41 es el diferencial del tiempo propio, mientras que cuando formula_42 es positivo, formula_43 es el diferencial de la distancia propia.

El principal valor de expresar las ecuaciones de la física en forma tensorial es que estas son luego manifestaciones invariantes bajo los grupos de Poincaré, así que no tenemos que hacer cálculos tediosos o especiales para confirmar ese hecho. También al construir tales ecuaciones encontramos usualmente que ecuaciones previas que no tienen relación, de hecho, están conectadas cercanamente al ser parte de la misma ecuación tensorial.

Ahora podemos definir igualmente la velocidad y la aceleración mediante simples leyes de transformación. La velocidad en el espacio-tiempo "U" está dada por

Reconociendo esto, podemos convertir buscando una ley sobre las composiciones de velocidades en un simple estado acerca de transformaciones de velocidades de cuatro dimensiones de una partícula de un sistema a otro. "U" también tiene una forma invariante:

Así la cuadrivelocidad tiene una magnitud de "c". Esta es una expresión del hecho que no hay tal cosa como la coordenada en reposo en relatividad: al menos, si se está siempre moviéndose a través del tiempo. Para la cuadriaceleración, esta viene dada por formula_44. Dado esto, diferenciando la ecuación para "τ" produce

formula_45

así en relatividad, la aceleración y la velocidad en el espacio-tiempo son ortogonales.

El momento lineal y la energía se combinan en un cuadrivector covariante:

donde "m" es la masa invariante.

La magnitud invariante del cuadrimomento es:

Podemos trabajar con que este es un invariante por el argumento de que este es primero un escalar, no interesa qué sistema de referencia se calcule y si la transformamos a un sistema donde el momento total sea cero.

Se observa que la energía en reposo es un invariante independiente. Una energía en reposo se puede calcular para partículas y sistemas en movimiento, por traslación de un sistema en que el momento es cero. La energía en reposo está relacionada con la masa de acuerdo con la ecuación antes discutida:

Nótese que la masa de un sistema de medida en su sistema de centro de momento (donde el momento total es cero) está dado por la energía total del sistema en ese marco de referencia. No debería ser igual a la suma de masas individuales del sistema medido en otros sistemas.

Al usar la tercera ley de Newton, ambas fuerzas deben estar definidas como la tasa de cambio del momentum respecto al mismo tiempo coordenado. Esto es, se requiere de las fuerzas definidas anteriormente. Desafortunadamente, no hay un tensor en cuatro dimensiones que contenga las componentes de un vector de fuerza en tres dimensiones entre sus componentes.

Si una partícula no está viajando a "c", se puede transformar en una fuerza de tres dimensiones del sistema de referencia de la partícula en movimiento entre los observadores de este sistema. A estos se los suele llamar fuerza de cuatro dimensiones. Es la tasa de cambio del anterior vector de cuatro dimensiones de energía momento con respecto al tiempo propio. La versión covariante de esta fuerza es:
-{\text{d} E}/{\text{d} \tau} \\ {\text{d} p_x}/{\text{d} \tau} \\ {\text{d} p_y}/{\text{d} \tau} \\ {\text{d} p_z}/{\text{d} \tau} \end{pmatrix} </math>
donde formula_46 es el tiempo propio.

En el sistema en reposo del objeto, la componente del tiempo de esta fuerza es cero a menos que la masa invariante del objeto este cambiando, en ese caso la tasa de cambio es negativa y es "c" veces. En general, se piensa que las componentes de la fuerza de cuatro dimensiones no son iguales a las componentes de la fuerza de tres porque esta de tres está definida por la tasa de cambio del momento con respecto al tiempo coordenado, así formula_47; mientras que la fuerza en cuatro dimensiones está definida por la tasa de cambio del momento respecto al tiempo propio, así formula_48.

En un medio continuo, la "densidad de fuerza" en tres dimensiones combinada con la "densidad de potencia" forma un vector de cuatro dimensiones covariante. La parte espacial es el resultado de dividir la fuerza en pequeñas células (en el espacio tridimensional) por el volumen de la célula. El componente del tiempo es negativo de la potencia transferida a la célula dividida para el volumen de la célula.

Investigaciones teóricas en el electromagnetismo clásico indicaron el camino para descubrir la propagación de onda. Las ecuaciones generalizando los efectos electromagnéticos encontraron que la velocidad de propagación finita de los campos E y B requiere comportamientos claros en partículas cargadas. El estudio general de cargas en movimiento forma un potencial de Liénard-Wiechert, que es un paso a través de la relatividad especial.

La transformación de Lorentz del campo eléctrico de una carga en movimiento por un observador en reposo en un sistema de referencia resulta en la aparición de un término matemático comúnmente llamado campo magnético. Al contrario, el campo magnético generado por las cargas en movimiento desaparece y se convierte en un campo electrostático en un sistema de referencia móvil. Las ecuaciones de Maxwell son entonces simplemente ajustes empíricos a los efectos de la relatividad especial en un modelo clásico del universo. Como los campos eléctricos y magnéticos son dependientes de los sistemas de referencia y así entrelazados, en el así llamado campo electromagnético. La relatividad especial provee las reglas de transformación de cómo los campos electromagnéticos en un sistema inercial aparecen en otro sistema inercial.

Las ecuaciones de Maxwell en la forma tridimensional son de por sí consistentes con el contenido físico de la relatividad especial. Pero debemos reescribirlas para hacerlas invariantes. La densidad de carga formula_49 y la densidad de corriente formula_50 son unificadas en el concepto de vector cuatridimensional:

La ley de conservación de la carga se vuelve:

El campo eléctrico formula_51 y la inducción magnética formula_52 son ahora unificadas en un tensor de campo electromagnético (de rango 2, antisimétrico covariante):

La densidad de la fuerza de Lorentz formula_53 ejercida en la materia por el campo electromagnético es:

La ley de Faraday de inducción y la ley de Gauss para el magnetismo se combinan en la forma:

A pesar de que se ven muchas ecuaciones, estas se pueden reducir a solo cuatro ecuaciones independientes. Usando la antisimetría del campo electromagnético se puede reducir a la identidad o redundar en todas las ecuaciones excepto las que λ, μ, ν = 1,2,3 o 2,3,0 o 3,0,1 o 0,1,2.

Existe cierta confusión sobre los límites de la teoría especial de la relatividad. Por ejemplo, con frecuencia en textos de divulgación se repite que dentro de esta teoría solo pueden tratarse sistemas de referencia inerciales, en los cuales la métrica toma la forma canónica. Sin embargo, como diversos autores se han encargado de demostrar la teoría puede tratar igualmente sistemas de referencia no inerciales.

Obviamente el tratamiento de sistemas no inerciales en la teoría de la relatividad especial resulta más complicado que el de los sistemas inerciales.

Einstein y otros autores consideraron antes del desarrollo de la relatividad general casi exclusivamente sistemas de coordenadas relacionados por transformaciones de Lorentz, razón por la cual se piensa que esta teoría es solo aplicable a sistemas inerciales.

Actualmente se considera como relatividad general el estudio del espacio-tiempo deformado por campos gravitatorios, dejando el estudio de los sistemas de referencia acelerados en espacios planos dentro de la relatividad especial. Igualmente la relatividad general es una de las teorías más relevantes para la construcción de modelos cosmológicos sobre el origen del universo.

La teoría general de la relatividad fue introducida históricamente en conexión con el principio de equivalencia y el intento de explicar la identidad entre la masa inercial y la masa gravitatoria. En esta teoría se usaban explícitamente sistemas de coordenadas no relacionados entre sí por transformaciones de Lorentz o similares, con lo cual claramente en la resolución de muchos problemas se hacía patente el uso de sistemas de referencia no inerciales. Estos hechos condujeron a la confusión en muchos textos de divulgación de que los sistemas no inerciales requieren del desarrollo de la teoría general de la relatividad.





</doc>
<doc id="2464" url="https://es.wikipedia.org/wiki?curid=2464" title="Resedaceae">
Resedaceae

Las resedáceas (Resedaceae) son una familia de plantas herbáceas o raramente subarbustivas. Hojas dispuestas helicoidalmente; enteras, tripartitas o pinnatífidas; estipulas glandulosas. Flores pequeñas, hermafroditas, cigomorfas; cáliz con 2-8 sépalos, a veces desiguales; corola con 2-8 pétalos (no tienen por qué tener el mismo número de piezas que el cáliz), a menudo laciniados, los posteriores mayores; androceo con 3 a muchos estambres; gineceo súpero, sincarpico, soldados siempre en la base y no siempre hasta arriba, con un número de carpelos variable entre 2 y 7. Inflorescencias abiertas, en racimo o en espiga. Fruto capsular (en "Reseda)" o polifolicular (en "Sesamoides"). 

Es una pequeña familia (c. 85 especies) cuyo conocimiento
taxonómico básico está relativamente bien establecido principalmente como fruto de dos
trabajos monográficos exhaustivos basados exclusivamente en datos morfológicos. Asimismo, el encuadramiento taxonómico general de la familia a nivel de orden es razonablemente consistente gracias a estudios recientes basados en caracteres morfológicos, bioquímicos y moleculares, si bien su relación con los grupos más próximos aún no está totalmente esclarecida.
Está compuesta por seis géneros (Caylusea A. St. Hil,
Ochradenus Del., Oligomeris Cambess., Randonia Coss., Reseda L. y Sesamoides All.; Fig.
1) y c. 85 especies, de las cuales más del 70% pertenecen al género Reseda. La familia se
distribuye principalmente por las regiones temperadas del Viejo Mundo, y presenta
su centro de diversidad en la cuenca mediterránea. Sus integrantes suelen encontrarse en
hábitats áridos y soleados, como estepas, desiertos y taludes, y muestran generalmente
preferencia por sustratos calcáreos. No obstante, cinco especies (Reseda alba L., R. lutea L.,
R. luteola L., R. odorata L. y R. phyteuma L.) son ruderales o arvenses en áreas alteradas por
la acción antrópica, y cuatro [R. attenuata (Ball) Ball, R. complicata Bory, R. glauca L. y R.
gredensis (Cutanda & Willk.) Müll. Arg.] se encuentran confinadas en zonas montañosas. En
cinco de los seis géneros de la familia, alguna de sus especies (en Caylusea y Reseda) o todas
ellas (en Ochradenus, Oligomeris y Randonia) habitan ambientes desérticos o subdesérticos.
Cuatro géneros están constituidos en su mayoría por hierbas anuales o perennes (Caylusea,
Oligomeris, Reseda y Sesamoides), mientras que los otros dos están formados por arbustos
(Ochradenus, Randonia). El hábito arbustivo también aparece en un pequeño grupo de unas
nueve especies del género Reseda, principalmente del cuerno de África.
Recientemente, se ha propuesto la primera hipótesis filogenética de la familia Resedaceae basada en regiones de ADN, con el principal objetivo de evaluar las clasificaciones taxonómicas tradicionales de la familia y discutir sus principales mecanismos evolutivos.



</doc>
<doc id="2467" url="https://es.wikipedia.org/wiki?curid=2467" title="Rosaceae">
Rosaceae

Las rosáceas (Rosaceae) son una familia de plantas dicotiledóneas pertenecientes al orden Rosales. Esta familia incluye la mayor parte de las especies de frutas de consumo masivo: manzana, pera, membrillo, melocotón (durazno), ciruela, cereza, fresa (frutilla), almendra, albaricoque, níspero, zarzamora, frambuesa, etc. También incluye muchas especies ornamentales, principalmente, las rosas, flores por excelencia, con importancia para la jardinería y la industria de la perfumería.

La familia de las rosáceas es muy amplia, con unos 90-130 géneros, en los que se reparten alrededor de 2000-2500 especies aceptadas, cuya distribución es casi mundial, originarias sobre todo de las regiones templadas y subtropicales del hemisferio boreal.

La familia Rosaceae incluye géneros con características muy heterogéneas, sin embargo, la característica común más importante es la presencia de un tálamo o receptáculo floral muy desarrollado, que varía desde una forma convexa (en "Rubus", "Fragaria"), hasta de forma cóncava (en "Rosa").

Estos individuos pueden poseer tallos leñosos o semileñosos. El hábito de estas plantas comprende: árboles, arbustos, trepadoras, sino hierbas perennes por rizomas o anuales.

Las hojas pueden ser simples o compuestas, (paripinnadas o imparipinnadas), casi siempre alternas y estipuladas, rara vez opuestas, con borde aserrado o dentado característico. Es frecuente la presencia de modificaciones: espinas, estípulas y aguijones, rara vez ausentes ("Spiraea").

Flores normalmente hermafroditas, actinomorfas (a veces zigomorfas por diferenciación de los sépalos en las plantas europeas, o también de los pétalos en las tropicales); periginas, epiginas o hipoginas; a menudo con un hipanto bien desarrollado; cáliz con cinco sépalos, a veces con epicáliz; corola generalmente con cinco pétalos libres; tetrámeras en "Sanguisorba"; androceo variable, con cuatro o cinco estambres, y más frecuentemente diplostémono o polistémono; gineceo variable, con de uno a muchos carpelos, libres o soldados y estilos generalmente libres. Flores solitarias o en inflorescencias variadas (racimos, espigas y corimbos).

Frutos muy variables (aquenio, poliaquenio con receptáculo abombado (eterio) o cóncavo (cinorrodón), drupa, polidrupa (sorosis), folículo, pomo); semillas pequeñas, sin endosperma (las substancias de reserva en los cotiledones).

Rosaceae es una familia en la cual la delimitación de sus géneros (y así de sus especies también) es una de sus mayores problemáticas para consistencia sistemática del grupo.

Se consideran caracteres importantes en la sistemática de la familia: la forma del receptáculo, el tipo de concrescencia de los carpelos y su posición, el número y disposición de los rudimentos seminales en el carpelo, el tipo de dehiscencia y la histología de los frutos, el número básico de cromosomas y la distribución de los metabolitos secundarios.

El nombre Rosaceae aparece publicado por el botánico escocés M. Adanson en ; y se conserva para el taxón. También citado posteriormente por Antoine L. de Jussieu, Genera Plantarum: 334. 1789. 
La clasificación de la familia Rosaceae es muy variada y diversa en taxones subfamiliares, y ha cambiado mucho con el tiempo y según los diferentes criterios de clasificación propuestos por los autores. Este es un esquema posible en el que se listan en primer lugar las subfamilias, en segundo lugar las tribus y en tercer lugar los géneros en sendos grupos; además de la referencia bibliográfica de cada taxón según el IPNI.

Fruto indehiscente: aquenios o drupas en su mayoría agregados. En el gineceo, los carpelos son numerosos y libres, el ovario medio o súpero. 

Frutos dehiscentes tipo folículo o cápsula.


Fruto indehiscente característico: el pomo. En el gineceo, los carpelos son 2-5 y están unidos; el ovario es ínfero.








Ver el anexo de .

Una clasificación práctica —y tradicional— para la familia de las rosáceas está dada por la división en cuatro subfamilias en función de las características de floración y fructificación de cada género. A su vez, es posible agrupar varios géneros en tribus y subtribus. Las cuatro subfamilias clásicas son:
Gineceo con un solo carpelo:

También conocida como Amygdaloideae, incluye especies leñosas y arbóreas, que poseen estípulas generalmente pequeñas y caducas. Las flores son periginas: el perianto (cáliz + corola) y el androceo se insertan sobre un receptáculo (o tálamo) acopado rodeando al gineceo; Gineceo con ovario súpero unicarpelar y uniovulado (monómero) que genera un fruto drupáceo.

"Prunus" - "Maddenia" - "Oemleria" - "Prinsepia" - "Exochorda".
Gineceo compuesto por más de un carpelo (policarpo):

+ n carpelos libres (policarpo apocárpico): Flores periginas, con gineceo apocárpico y ovario súpero a semi-ínfero. Fruto, frecuentemente múltiple: poliaquenio, polidrupa, eterio (drupas múltiples sobre receptáculo hinchado), cinorrodón (aquenios encerrados en una urna) o frutos complejos secos, indehiscente, semejantes a un aquenio. numerosos carpelos uniovulados que producen drupelas o aquenios. Estípulas bien desarrolladas y persistentes.
Colurieae - Crataegeae - Dryadeae - Exochordeae - Gillenieae - Kerrieae - Neillieae - Potentilleae - Roseae - Rubeae - Sanguisorbeae - Ulmarieae - Incertae sedis (lugar incierto)

"Acaena" "Acomastylis" "Agrimonia" "Alchemilla" "Aphanes" "Aremonia" "Bencomia" "Chamaebatia" "Cliffortia" "Coluria" "Comarum" "Cowania" "Dalibarda" "Dendriopoterium" "Dryas" "Duchesnea" "Erythrocoma" "Fallugia" "Filipendula" "Fragaria" "Geum" "Hagenia" "Horkelia" "Ivesia" "Kerria" "Leucosidea" "Marcetella" "Margyricarpus" "Novosieversia" "Oncostylus" "Polylepis" "Potentilla" "Rosa" "Rubus" "Sanguisorba" "Sarcopoterium" "Sibbaldia" "Sieversia" "Spenceria" "Taihangia" "Tetraglochin" "Waldsteinia ".-

+ 5 carpelos unidos al hipanto: Especies leñosas y arbóreas, de estípulas caducas. Flores epiginas con gineceo apocárpico (2-5 carpelos) que se fusionan y ovario ínfero; receptáculo floral muy desarrollado. Fruto de tipo complejo conocido como pomo.

"Mespilus" _ "Pyracantha" - "Amelanchier"

"Aria" - "Aronia" - "Chamaemeles" - "Chamaemespilus" - "Cotoneaster" - "Cormus" - "Crataegus" - "Cydonia" - "Dichotomanthes" - "Docynia" - "Docyniopsis" - "Eriobotrya" - "Eriolobus" - "Heteromeles" - "Kageneckia" - "Lindleya" - "Malacomeles" - "Malus" - "Osteomeles" - "Peraphyllum" -"Photinia" - "Pseudocydonia" - "Pyrus" - "Rhaphiolepis" - "Sorbus" - "Stranvaesia" - "Torminalis" - "Vauquelinia" - "x Crataemespilus".

Se propone como la subfamilia que reúne a los géneros más primitivos de las rosáceas. Especies principalmente leñosas. Estípulas ausentes o presentes, caducas o persistentes. Flores periginas con receptáculo plano; ovario súpero. Flor epigina, gineceo formado por varios (5) carpelos apocárpicos pluriovulados (cada ovario con varios primordios seminales) con placentación marginal (a veces soldados) que originan folículos o polifolículos, excepcionalmente un fruto capsular o poliaquenio. Ref.: 

"Aruncus" - "Holodiscus" - "Kelseya" - "Luetkea" - "Petrophytum" - "Sibiraea" - "Spiraea" - "Xerospiraea"




</doc>
<doc id="2468" url="https://es.wikipedia.org/wiki?curid=2468" title="Rhizophoraceae">
Rhizophoraceae

Rhizophoraceae es una familia constituida por plantas tropicales o subtropicales. Entre los miembros más conocidos se encuentran los mangles, árboles del género "Rhizophora". Consta de alrededor de 120 especies distribuidas en 16 géneros, la mayoría nativos del Viejo Mundo.

Raíces modificadas para la obtención de aire (neumatóforos) o para la sujeción: "raíces zanco" (viven en medios muy móviles: fango). Flores tetrameras, periantio más o menos desarrollado en las especies africanas, y menos desarrollado en las americanas y asiáticas. Frutos vivíparos. Acuáticas, forma manglares en algunas zonas tropicales costeras.

Estas especies son a menudo hermafroditas, más raramente polígamas. Las especies de manglar son normalmente vivíparas, a diferencia de las de tierra.


Algunas especies producen madera para construcciones bajo el agua o pilotes. De la corteza se extrae tanino.



</doc>
<doc id="2469" url="https://es.wikipedia.org/wiki?curid=2469" title="Rafflesiales">
Rafflesiales

Rafflesiales es un orden de plantas de la subclase Rosidae, clase Magnoliopsida.

Es un sinónimo de Malpighiales.

Pocas especies, una familia en península ibérica, con 1 especie, 2 especies importantes.


</doc>
<doc id="2470" url="https://es.wikipedia.org/wiki?curid=2470" title="Rafflesiaceae">
Rafflesiaceae

Rafflesiaceae, del orden Malpighiales, es una familia de hierbas perennes holoparásitas, monoicas, a veces con aspecto taloide. Tienen hojas reducidas a escamas o ausentes. Sus flores son unisexuales, homoclamídeas, generalmente tetrámeras, de ovario ínfero; dispuestas en espigas glomerulares o solitarias. Fruto en baya. Existen 9 géneros con unas 60 especies, sobre todo tropicales que se encuentran en el este sureste de Asia. Entre ellas, "Rafflesia arnoldii" es la especie con la flor de mayor tamaño entre todas las plantas.
La familia original Rafflesiaceae "sensu lato" se divide ahora en cuatro familias:


Las cuatro familias pueden distinguirse fácilmente por sus flores e inflorescencias:



</doc>
<doc id="2471" url="https://es.wikipedia.org/wiki?curid=2471" title="Rhamnales">
Rhamnales

En el sistema de Cronquist de clasificación científica de los vegetales, Rhamnales era un orden de plantas dicotiledóneas que incluía tres familias bien representadas en las zonas templadas. Generalmente leñosas, muchas trepadoras, algunas hierbas. Tiene siempre un disco nectarífero intraestaminal, que procede del verticilo interno de los estambres, los que quedan son epipétalos. La diferenciación entre familias por el número de óvulos por lóculo, Vitaceae, con 2 óvulos por lóculo y Rhamnaceae, con 1 óvulo por lóculo.

Las investigaciones filogenéticas modernas han separado a las ramnáceas, hoy consideradas parte del orden Rosales, de las dos familias restantes; estas últimas se consideran las únicas en el orden Vitales.


</doc>
<doc id="2472" url="https://es.wikipedia.org/wiki?curid=2472" title="Rhamnaceae">
Rhamnaceae

Las ramnáceas (Rhamnaceae) son una familia de plantas dicotiledóneas perteneciente al orden de las rosales. 

Se trata de árboles o arbustos, a veces trepadores con espinas, y matas. Hojas simples con frecuencia alternas, a veces opuestas, con estípulas caducas o transformadas en espinas; flores inconspicuas, hermafroditas o unisexuales (plantas dioicas o poliginas), pentámeras o tetrámeras, diclamideas o monoclamideas, de ovario súpero o ínfero, con un óvulo por lóculo. Inflorescencias en racimos o glomérulos. Frutos generalmente en drupa o secos. Unas 6000 especies cosmopolitas la mayoría de países cálidos y templados, algunos con aplicaciones medicinales.













</doc>
<doc id="2473" url="https://es.wikipedia.org/wiki?curid=2473" title="Rutaceae">
Rutaceae

Las rutáceas (Rutaceae) son una familia de plantas angiospermas perteneciente al orden Sapindales. Agrupa alrededor de 160 géneros y 1600 especies.

Plantas leñosas o raramente herbáceas, provistas de glándulas secretoras oleíferas. Hojas alternas u opuestas, simples o compuestas, sin estípulas, a veces con espinas axilares. Flores generalmente hermafroditas, actinomorfas o zigomorfas, pentámeras o tetrámeras, con piezas libres o soldadas en la base; androceo con un número variable de estambres, a menudo el mismo o el doble que el de pétalos, con disco nectarífero carnoso intraestaminal; gineceo súpero o semiínfero, pluricarpelar, generalmente sincárpico y plurilocular. Inflorescencias diversas. Frutos en cápsula, polifolículo, hesperidio, drupa o sámara.

Tiene las siguientes subfamilias: Citroideae, Dictyolomatoideae, Flindersioideae, Rutoideae, Spathelioideae, Toddalioideae. En total son unas 1600 especies, de las cuales la mayoría crecen en países tropicales y subtropicales.




</doc>
<doc id="2474" url="https://es.wikipedia.org/wiki?curid=2474" title="Rhoipteleaceae">
Rhoipteleaceae

Rhoipteleaceae es una familia de plantas dicotiledóneas que comprende una sola especie "Rhoiptelea sinensis", con un único género Rhoiptelea. Es nativa del sudoeste de China y norte de Vietnam donde vive en alturas de 700-1600 msnm en áreas montañosas.
Los árboles que alcanzan los 17 metros de altura y un tronco de 40 cm de diámetro, son polinizados por el viento y las flores se producen en panículas que alcanzan los 32 cm de longitud con parecido a una "cola de caballo". Su fruto es una pequeña nuez con alas redondeadas. Sus hojas son pinnadas.

"Rhoiptelea sinensis" fue descrita por Diels & Hand.-Mazz. y publicado en "Repertorium Specierum Novarum Regni Vegetabilis 30(791–798): 77–79, pl. 127 & 128", en el año 1932.



</doc>
<doc id="2475" url="https://es.wikipedia.org/wiki?curid=2475" title="Reyes Magos">
Reyes Magos

Los Reyes Magos de Oriente (o simplemente Reyes Magos) es el nombre por el que la tradición cristiana denomina a los «magos» —denominación que recibían los sacerdotes eruditos en el Antiguo Oriente— que, según el evangelio de Mateo, tras el nacimiento de Jesús de Nazaret, acudieron desde Oriente para rendirle homenaje y entregarle regalos de gran riqueza simbólica: oro, incienso y mirra.

En los evangelios canónicos solo el Evangelio de Mateo habla de estos «magos», sin precisar sus nombres, ni que fuesen reyes, ni que fueran tres. Fue en el cuando se estableció que pudieran ser reyes, ya que hasta entonces, por sus regalos y las iconografías que los representaban, tan solo se consideraba que eran personas pudientes. Fue también en ese siglo cuando se estableció su número en tres, uno por regalo, ya que hasta entonces había dibujos con dos, tres o cuatro magos, e incluso la Iglesia ortodoxa siria y la Iglesia apostólica armenia aseguraban que eran doce, como los apóstoles y las doce tribus de Israel.

Los nombres actuales de los tres reyes magos, Melchor, Gaspar y Baltasar, aparecen por primera vez en el conocido mosaico de San Apollinaire Nuovo (Rávena) que data del , en el que se distingue a los tres magos ataviados al modo persa con sus nombres encima y representando distintas edades. Aún tendrían que pasar varios siglos, hasta el , para que el rey Baltasar aparezca con la tez negra y los tres reyes, además de representar las edades, representen las tres razas de la Edad Media. Melchor encarnará a los europeos, Gaspar a los asiáticos y Baltasar a los africanos.

En España a partir del se inició la tradición de convertir la noche de Reyes (noche anterior a la Epifanía) en una fiesta infantil con regalos para los niños, a imitación de lo que se hacía en otros países el día de Navidad, en homenaje al santo oriental San Nicolás. Fue en 1866 cuando se celebró la primera cabalgata de Reyes Magos en Alcoy, tradición que se extendió al resto del país y posteriormente a otros países, especialmente a países de cultura hispana.

La palabra «mago», proviene del persa "ma-gu-u-sha", que significa sacerdote. Llegó al griego como μάγος ("magos", plural: μάγοι, "magoi"), refiriéndose a una casta de sacerdotes persas o babilonios, que estudiaban las estrellas en su deseo de buscar a Dios. Del griego pasó al latín como "magus", plural "magi", /mágui/ de donde llegó al español "mago".

La figura católica de los Reyes Magos tiene su origen en los relatos del nacimiento de Jesús, algunos, fueron integrados de los evangelios canónicos que hoy conforman el "Nuevo Testamento" de la "Biblia". Concretamente el "Evangelio de Mateo" es la única fuente bíblica que menciona a unos magos (aunque no especifica los nombres, el número ni el título de reyes) quienes, tras seguir una estrella, buscan al «rey de los judíos que ha nacido» en Jerusalén, guiándoles dicha estrella hasta Jesús nacido en Belén, y a quien presentan ofrendas de oro, incienso y mirra.

Si bien parece contradictorio que practicantes de la magia (severamente amonestada tanto en el "Antiguo" como en el "Nuevo Testamento") sean admitidos como adoradores de Jesús, hay que tener en cuenta que el término griego "magós" no era utilizado únicamente para referirse a los hechiceros. Se utiliza, en este caso, para referirse a ‘hombres sabios’ (así se los llama en diversas versiones de la "Biblia" en inglés) o, más específicamente, "hombres de ciencia". De hecho, también poseían conocimiento de las Escrituras y, desde antiguo se ha sostenido que pertenecían al mazdeísmo.

Mateo no explicita que sean astrólogos que conocieran con precisión el movimiento de alguna estrella (2:7) a pesar de ser ésta la creencia general. Aunque bien intencionados, su visita es causa de turbación general y despierta la desconfianza de Herodes (2:3), pues veía al nuevo Mesías como un rival. A pesar de ser anciano y de haber reinado ya por más de treinta años, Herodes les ruega que averigüen el sitio preciso del nacimiento del Mesías (2:8) con el fin de poder, así, acabar con su potencial competidor. Los sabios, que no sospechan eso, encuentran al Niño, lo adoran y obsequian oro, incienso y mirra (2:11). Un ángel previene a los magos de las intenciones que Herodes guardaba (2:12), así que no regresan donde él. Iracundo, el rey manda a matar a todos los niños menores de dos años. Para entonces, José ha sido avisado en sueños (2:13) de que debe huir a Egipto con los suyos.

A partir de ese relato, se han ido elaborando numerosas leyendas sobre los hechos y la personalidad de estas tres figuras.

Según la interpretación de José Luis Sicre, en el tiempo en que fue escrito el Evangelio de Mateo se estaba produciendo un incremento de conversiones paganas al cristianismo frente a las de los propios judíos. La incursión de estos fragmentos sobre los magos de Oriente en el Evangelio de Mateo subraya este hecho y lo utiliza como argumento de conversión: si los de fuera vienen y lo adoran (se convierten) ¿cómo no os dais cuenta los que lo tenéis entre vosotros?.

También existen otras interpretaciones astrológicas y cabalísticas sobre la figura de los Reyes Magos.

Según la interpretación ofrecida por Eric Rodríguez, se tiene lo siguiente:

Ya el término griego μάγος (literalmente “magos”) había caído en un uso peyorativo o deteriorado desde al menos el siglo III a. C. ("cf." versión Septuaginta) por la extracción de su origen y contexto cultural, y que es como se usa aún en la época del Nuevo Testamento ("cf." Hechos de los apóstoles, 8:9, 13:6, 19:13).
No obstante, en el texto original "koiné" (griego bíblico) de Mateo 2:1 dice:

A diferencia de los magos que ya se encontraban dispersos en tierra de Israel y todo el mundo helénico, el énfasis que se emplea al decir “de Oriente”, marca un cambio de connotación: el autor busca traer a la mente un personaje asociado con el Oriente, diferente a los sabios convencionales de Israel (rabinos), que conociera además las profecías mesiánicas y que fuera autoridad bíblica para el lector judío (ya que se acepta a nivel general que el Evangelio de Mateo fue escrito para hebreos y aún en lengua hebrea según el testimonio de casi todos los padres de la Iglesia).

Hay que tener en cuenta, además, que Oriente puede designar la región de Babilonia, y por lo tanto, para algunos autores, los magos, podrían corresponder a los llamados en arameo מדנחאי (Medinja’ey, “doctores babilónicos de la tradición oral”) cuya escuela perduraría hasta entrado el siglo octavo de nuestra era en Babilonia, y quienes conociendo la interpretación de lo dicho en Números 24:17 ("cf". tárgum de Onqlós/Onkelos sobre este pasaje), habrían sido guiados por Dios hasta el Mesías. En este caso la estrella simbolizaría al mismo Mesías según el lenguaje midrásico contemporáneo.

Las tradiciones antiguas que no fueron recogidas en la Biblia ―como por ejemplo el llamado "Evangelio del Pseudo Tomás", o "Evangelio de la infancia", del siglo II― son sin embargo más ricas en detalles. En ese mismo evangelio apócrifo se dice que tenían algún vínculo familiar, y también que llegaron con tres legiones de soldados: una de Persia, otra de Babilonia y otra de Asia ("sic").

En el último libro escrito por el papa Benedicto XVI sobre Jesús de Nazaret, «La infancia de Jesús», se menciona de tal modo a los Reyes Magos que algunos han sostenido que probablemente no venían de Oriente, sino de Tartessos, una zona que los historiadores ubican entre Huelva, Cádiz y Sevilla (Andalucía, España). El texto, sin embargo, dice: «Así como la tradición de la Iglesia ha leído con toda naturalidad el relato de la Navidad sobre el trasfondo de Isaías 1:3, y de este modo llegaron al pesebre el buey y el asno, así también ha leído la historia de los Magos a la luz del Salmo 72:10 e Isaías 60. Y, de esta manera, los hombres sabios de Oriente se han convertido en reyes, y con ellos han entrado en el pesebre los camellos y los dromedarios». Eso relata Benedicto XVI y continúa: «La promesa contenida en estos textos extiende la proveniencia de estos hombres hasta el extremo Occidente (Tarsis, Tartessos en España), pero la tradición ha desarrollado ulteriormente este anuncio de la universalidad de los reinos de aquellos soberanos, interpretándolos como reyes de los tres continentes entonces conocidos: África, Asia y Europa». Al respecto, el secretario general de la Conferencia Episcopal, monseñor Juan Antonio Martínez Camino, recordó que en ningún momento el Santo Padre dice que «los Reyes Magos fueran andaluces, lo que explica el Papa es que los magos no eran otra cosa que buscadores de la verdad. Representaban a todos los hombres buscadores de Dios de todos los tiempos y de todos los lugares y eso incluía a todo el mundo hasta entonces conocido y cuyo límite occidental era Tartessos, en la península ibérica», explica. Al mencionar a Tartessos, Benedicto XVI se refiere a este límite geográfico que tenía el mundo en el siglo I a. C, «los Magos son de Oriente pero que en esa inquietud por buscar a Dios están representados los hombres buscadores de Dios de todos los lugares y de todos los tiempos».

La historia narrada en el Evangelio de Mateo, cuenta que los magos vinieron de Oriente guiándose por una estrella, la cual los condujo hasta Belén (de ahí el nombre de estrella de Belén).

Antes de llegar, visitaron al rey Herodes el Grande en la ciudad de Jerusalén, a quien interrogaron por el nacimiento del «Rey de los judíos». El monarca, después de consultar a los escribas versados en la Biblia, les aseguró que el niño debía nacer en la pequeña ciudad de Belén, como establecía la profecìa de Miqueas. Agregó, astutamente que, de regreso, hablaran con él para darle noticia del sitio exacto donde se encontraba dicho niño y, así, poder ir él también a adorarle. En realidad, según el relato bíblico, su intención era darle muerte.

En Belén, los magos volvieron a ver la estrella, hallaron a Jesús recién nacido y lo adoraron; ofreciéndole oro (representando su naturaleza real, como presente conferido a los reyes), incienso (que representa su naturaleza divina, empleado en el culto ) y mirra (un compuesto embalsamador para los muertos, representando el sufrimiento y muerte futura de Jesús). Parece ser que por el hecho de traer tres dones, se dio por sentado que eran tres los personajes que los traían. Aunque también en algún momento las distintas tradiciones han señalado que eran cuatro, siete y hasta doce magos. Como antecedente, Seleuco I Nicátor ofrendó oro, incienso y mirra a Apolo en su santuario de Dídima, en el 288 a. C.

Al regreso, advertidos los magos por un sueño de las intenciones del rey, no volvieron a Jerusalén. Herodes, entonces, ordenó dar muerte a todos los niños menores de dos años residentes en Belén, episodio conocido como la matanza de los inocentes. Un nuevo mensaje celestial, advirtió a José de la amenaza y éste, llevando a María y a Jesús, huyó a Egipto.

La primera vez que surge el nombre con que hoy conocemos a los Reyes Magos es en la iglesia de San Apolinar Nuovo, en Rávena (Italia). El friso de la imagen está decorado con mosaicos de mediados del siglo VI que representan la procesión de las Vírgenes. Esta procesión está conducida por tres personajes vestidos a la moda persa, tocados con un gorro frigio y su actitud es la de ir a ofrecer lo que llevan en las manos a la Virgen que está sentada en un trono y tiene al Niño en la rodilla izquierda. Encima de sus cabezas se pueden leer tres nombres, de derecha a izquierda: "Gaspar, Melchior, Balthassar"...
Poco a poco la tradición ha ido añadiendo otros detalles a modo de simbología: se les ha hecho representantes de las tres razas conocidas en la antigüedad, representantes de las tres edades del hombre y representantes de los tres continentes (Asia, África y Europa).

La llegada de los Reyes Magos es un tema tratado también en los evangelios apócrifos. Según la tradición esotérica aplicada al cristianismo, estos personajes procedían del lugar donde se encontraba el Preste Juan.

Otra leyenda cuenta que, después de la resurrección de Jesús, el apóstol Tomás los halló en el reino de Saba, donde fueron por él bautizados y consagrados obispos. Después fueron martirizados en el año 70 y depositados en el mismo sarcófago. Allá fue Santa Elena a buscarlos, y halló tres cuerpos coronados, dando por sentado que se trataría de los Reyes Magos, por lo que los trasladó a Constantinopla. Posteriormente, Federico I Barbarroja, en el siglo XII, los trasladó a Colonia, Alemania, donde hoy reposan con las coronas que supuestamente llevaron durante su existencia (según la tradición, los relicarios con sus presentes se hallan en el monasterio de San Pablo del Monte Athos). Miles de peregrinos empezaron a llegar a Colonia, lo que propició que en 1248 se iniciara la construcción de la catedral de Colonia, que llevaría más de 600 años terminarla. Hoy día es uno de los monumentos góticos más impresionantes de Europa. Colonia se ha convertido junto con Roma y Santiago de Compostela en uno de los grandes centros de peregrinación. Igualmente, existen leyendas que hablan de un cuarto rey mago.

Según las diversas tradiciones de los reyes magos, el número de ellos varía; así se puede encontrar los siguientes reyes magos:



Con respecto a los nombres de los reyes (Melchor, Gaspar y Baltasar) las primeras referencias parecen remontarse al siglo V a través de dos textos, el primero titulado "Excerpta latina bárbari", en el que son llamados Melichior, Gathaspa y Bithisarea, y en otro evangelio apócrifo, el "Evangelio armenio de la infancia", donde se les llama Baltasar, Melkon y Gaspar.

Los nombres son además diferentes según la tradición siríaca: Larvandad, Gushnasaf y Hormisdas.

Los reyes magos son conocidos también como los Santos Reyes.

Con el tiempo, en países de tradición católica, se adoptó la costumbre de celebrar al mismo tiempo el día de la Epifanía (el 6 de enero) y la festividad de los Reyes Magos, conjugándose así la manifestación de Jesús al mundo no judío con la fiesta de estos personajes que representaban justamente ese mundo de gentiles. Poco a poco, se fue olvidando el significado verdadero de la palabra "epifanía" y la convirtió en un sinónimo de "adoración de los Magos".

El día 6 de enero es festivo en Cuba, España, México, Puerto Rico, República Dominicana, Paraguay, Uruguay, Colombia y Venezuela.

En algunos lugares, las autoridades organizan la llamada "Cabalgata de Reyes" el día 5 de enero, durante la cual los personajes suelen ir montados a caballo o en carrozas, vestidos con mantos y coronas, en lugar de la vestimenta frigia totalmente desconocida. En la mayoría de sitios donde sale la cabalgata, aparte de ir en ella los Reyes Magos, también hay carrozas de otros temas y distintos personajes, como pueden ser personajes infantiles y demás. El siguiente día, el 6 de enero, es festivo nacional. Ese día los niños disfrutan sus obsequios.
En España, la tradición dice que los regalos de Navidad a los niños los traen los Reyes Magos la noche del 5 al 6 de enero, compitiendo con la reciente introducción de Papá Noel en las costumbres navideñas debido a la influencia de otras culturas. Antes, los niños deben enviar una carta a los reyes enumerando los regalos que quieren y los méritos por los que merecen recibirlos. También es tradición que la noche del 5 de enero los niños dejen sus zapatos en algún lugar de la casa, junto a la puerta, en una ventana; incluso se dejan dulces para obsequiar a los Reyes Magos y agua o comida para los camellos. Al día siguiente se encuentran allí los regalos o, en el caso de haber sido malos, carbón en su lugar (se trata de un dulce de feo aspecto pero golosina, al fin y al cabo). El día 6 de enero es festivo en toda España. La escalada consumista ha conseguido que también reciban regalos los adultos, en ocasiones usando el juego del amigo invisible. Es típico desayunar el Roscón de Reyes que en muchos lugares puede comerse la víspera, para merendar o, como postre, en la cena; normalmente en familia. En España estos roscones suelen contener una figurilla, antiguamente un haba, popularmente conocida como "la sorpresa", y a quien le tocara en su trozo de roscón tenía que pagar este bollo. 

En varios países de Hispanoamérica existe la costumbre adoptada de los españoles de que los niños reciban regalos de los Reyes Magos, bien en la víspera, es decir, a la medianoche del 5 de enero, o en la mañana del 6 de enero (Argentina, México, República Dominicana, Puerto Rico, Paraguay y Uruguay). También se han heredado las costumbres de la carta a los Reyes y el carbón dulce en vez de regalos. La mayoría de los servicios postales aceptan estas cartas.
Al igual que la costumbre anglosajona en torno a Santa Claus, es frecuente que los reyes magos aparezcan en tiendas de regalos y centros comerciales, donde los niños tienen la oportunidad de tomar una foto sentados sobre las rodillas y entregar la carta con sus peticiones directamente. La representación consta normalmente de un escenario con tronos y los símbolos característicos, como figuras o dibujos de camellos, la estrella, un buzón y adornos de aspecto oriental. En los tronos es donde se sientan los reyes, habitualmente se trata de empleados caracterizados. En ocasiones se representan los tres reyes de la tradición, pero dependiendo de las circunstancias o el tamaño del escenario, puede incluirse únicamente uno. Además van acompañados de un paje, personaje característico que se encarga de conducir a los niños desde donde esperan con sus padres hasta los reyes y de recoger las cartas.
En México, el día 5 de enero por la noche se parte una Rosca de reyes que es tomada con chocolate, café o atole. Aquí se encuentra el segundo santuario más importante del mundo con respecto a los Tres Santos Reyes, ubicado en la ciudad de Tizimín, Yucatán; siendo visitado por millares de personas durante las fiestas religiosas en su honor, celebradas a finales de diciembre y principios de enero. Se trata además de la feria religiosa más antigua del sureste mexicano.

En Puerto Rico, la noche del 5 de enero los niños corren por el patio recogiendo grama. Ponen la grama en una caja de zapatos y colocan la caja junto a su cama. La grama es usada para alimentar a los camellos. Los reyes entonces dejan regalos en las cajas.

En Perú, ha caído en desuso el dar regalos a los niños en esta fecha. La celebración que se acostumbra es la llamada Bajada de Reyes, que consiste en que una familia o comunidad realiza una pequeña celebración mientras se va desmontando el "Nacimiento". Cuando se trata de una comunidad, es costumbre dejar dinero mientras se retiran los adornos y figuras. Esta tradición incluso ha llegado a empresas privadas, las cuales realizan dicha celebración entre los miembros de la misma.

Es interesante notar que, en tiempo de la colonización española, especialmente en Cuba, República Dominicana, Puerto Rico, México y Uruguay este día era de asueto para los esclavos negros que salían a las calles a bailar al ritmo de sus tamboras. Esto origina el nombre de "Pascua de los Negros" con que el día es aún conocido en algunos países como en Chile o Paraguay donde la comunidad Afro paraguaya celebra el día de su santo (San Baltasar).

Los países de habla inglesa dedican el día 6 de enero a desmontar los adornos de la Navidad. Esta costumbre también se ha extendido a países de América Latina, convirtiéndose el 6 de enero en el último día de la temporada navideña. Antiguamente se celebraban festejos con ese motivo y se cocinaba un pastel en el que se escondía un haba o una pequeña moneda de plata. La persona que encontraba el haba o la moneda era nombrada "rey judío" o "señor del desorden" y se veía obligada a encargarse de los festejos de esa noche. Con el tiempo, la fiesta fue evolucionando y se incluyeron bailes de máscaras y representaciones teatrales. Esta tradición dio origen en España al típico roscón de reyes (también llamado rosca de reyes en Hispanoamérica) que se toma en ese día y que esconde una pequeña sorpresa en su interior. En México, dicha rosca tiene en su interior varios muñecos pequeños de plástico los cuales representan al niño Jesús; aquella persona que en el momento de partir la rosca encuentra alguno de ellos, es encargado de hacer o invitar tamales y atole el 2 de febrero, día de la Candelaria.

En el año 1601 los abogados de Londres encargaron a Shakespeare una obra de teatro que se tituló "Noche de Reyes" y fue representada ante la reina Isabel I.

Desde la antigüedad, el tema de los Reyes Magos ha sido motivo de representación por artistas, pintores y escultores y también en la literatura. Han sido retratados habitualmente en número de tres; otras veces, cuatro; y, excepcionalmente, en número de dos. Es un tema abundantemente tratado durante la historia.

Hasta finales del siglo XIV no se comenzó a representar a uno de los magos de color negro, y solo a partir del siglo XVI fue cuando se generalizó esta forma de representarlos.

En las arquivoltas que enmarcan el tímpano de la portada románica de la Iglesia de Santo Domingo de Soria, del siglo XIII, se encuentra una de las representaciones más inusual de los Reyes Magos en el arte. Se trata del llamado "sueño de los Reyes Magos". En la representación, labrada en piedra, se representa a tres hombres barbados, de iguales rasgos físicos y sin corona real, tumbados hacia arriba en representación de su sueño y, junto a ellos, el ángel que según el Evangelio de Mateo les advierte en sueños de la intención de Herodes de matar a Jesús y que desencadenó la llamada matanza de los inocentes (que se representa en la siguiente arquivolta).

Entre los pintores que representan la escena de la Adoración de los magos, pueden citarse Andrea Mantegna, Botticelli, Giotto, Leonardo da Vinci, el Bosco, Velázquez, Rubens, Durero.

Por supuesto el cine no es ajeno a la figura de los Reyes Magos. Desde "Vida y pasión de Jesucristo" (1907), de Ferdinand Zecca, hasta "La Natividad" (2006), numerosas películas han incluido a estos personajes en alguna escena.

En la plaza de los Reyes Magos de la localidad alicantina de Ibi se encuentra un monumento dedicado a la figura de los tres Reyes Magos de Oriente, tan entroncados con la industria juguetera y de fuerte implantación en la localidad desde principios del siglo XX. Dicho monumento, de 5,8 toneladas, fue inaugurado el 5 de enero de 1974 y es una obra en piedra caliza del escultor granadino D. Aurelio López Azauste.

Otra localidad que también ha honrado a la figura de los tres Reyes Magos de Oriente es Juana Díaz, en la isla caribeña de Puerto Rico. Este pueblo es sede de las más destacadas devociones en honor a los tres Reyes Magos, y como homenaje al arraigo de esta tradición entre los puertorriqueños hay dos monumentos dedicados a los Reyes Magos:

En Logroño el 5 de enero de 2009 fue inaugurada por personajes que interpretaban a los Reyes Magos una escultura que los representa en la rotonda situada junto al Estadio Las Gaunas, en el cual estos mismos aterrizan en helicóptero cada 5 de enero.



</doc>
<doc id="2476" url="https://es.wikipedia.org/wiki?curid=2476" title="Robert Schuman">
Robert Schuman

Jean-Baptiste Nicolas Robert Schuman, más conocido como Robert Schuman (Luxemburgo, 29 de junio de 1886-Scy-Chazelles, 4 de septiembre de 1963), fue un político francés de origen germano-luxemburgués. Es considerado como uno de los «padres de Europa» en referencia a su determinante participación en la creación de las Comunidades Europeas.
Como miembro fundador del Movimiento Republicano Popular (MRP), fue uno de los principales dirigentes de la Cuarta República Francesa, siendo ministro de Finanzas, presidente del Consejo de Francia, ministro de Asuntos Exteriores y ministro de Justicia. También se desempeñó como diputado de Mosela entre 1919 y 1962, con una pausa entre 1942 y 1946.

Su cargo como ministro de Asuntos Exteriores (1948-1952), lo llevó a ser el principal negociador francés de los tratados firmados entre el final de la Segunda Guerra Mundial y el principio de la Guerra Fría (Consejo de Europa, OTAN, CECA, etc.). Además, fue él quien propuso por primera vez, el 9 de mayo de 1950, un proyecto de integración europea, que daría lugar a la Comunidad Europea del Carbón y del Acero. Fue también el primer presidente de la Asamblea Parlamentaria Europea (1958-1960), precedente del actual Parlamento Europeo.

Robert Schuman nació en Clausen, un barrio de Luxemburgo. Su casa natal es ahora la sede del Centro de Estudios Europeos Robert Schuman.

Su madre era luxemburguesa, por lo que es en este país donde realizó la mayor parte de su formación escolar. Su padre, oriundo de Mosela, sirvió en el ejército francés durante la Guerra franco-prusiana (1870-1871), tras la cual adoptó la nacionalidad alemana, luego de que Alsacia-Lorena fuese anexionada por el Imperio Alemán. Después se mudó a Luxemburgo, donde fue considerado como alemán al igual que su esposa, nacionalizada alemana a causa de su vínculo matrimonial. La pareja y su hijo único conformaban una familia de clase media. El padre era propietario de un terreno dedicado a la explotación agrícola propia y al alquiler de parcelas.

La educación familiar de Schuman estuvo enmarcada en la práctica del catolicismo que profesaban sus progenitores. En su infancia asistió a la escuela comunal de Clausen antes de continuar sus estudios en el Athénée Grand-Ducal, donde aprendió el francés. En 1900 murió su padre, cuatro años antes de que Robert Schuman hubiese finalizado sus estudios de secundaria en el Liceo Imperial de Metz.

Estudió Derecho en las universidades de Múnich, Bonn y Berlín. Finalmente se graduó en la Universidad de Estrasburgo, para luego abrir su propio bufete en Metz en junio de 1912, meses después de la muerte accidental de su madre, a quien lo unía una estrecha intimidad espiritual. En este periodo contempló la idea de iniciarse en el sacerdocio, pero finalmente optó por una vida a medio camino entre el clero y el trabajo como funcionario público. Gracias a la herencia dejada por sus padres, Schuman no tuvo dificultades económicas durante toda su vida.

En la universidad, Schuman formó parte de la Corporación Unitas, integrada principalmente por seminaristas y estudiantes de Teología. Sin embargo, su carácter reservado y su juventud hicieron que su paso por la organización no tuviera importancia significativa.

Schuman no prestó servicio militar por razones de salud. Pero al estallar la Primera Guerra Mundial, el reclutamiento se intensificó, por lo que fue empleado en la administración alemana debido a sus competencias jurídicas. Incorporado en el servicio auxiliar, fue radicado en Metz en una unidad de no combatientes. Allí prestó funciones de soldado secretario durante un año. Tras ser relevado del cargo, fue nombrado adjunto de la administración en Boulay, donde permaneció hasta el fin de la guerra en 1918, mientras continuaba paralelamente con su trabajo en el despacho de abogado de Metz. Hasta ese momento Schuman, quien nunca se casó y vivió siempre de manera austera, tenía una cultura esencialmente alemana.

Una vez terminada la guerra, Schuman se inscribió como abogado en Metz, estatuto que conservó durante toda su vida. Su formación de jurista y su conocimiento del francés hizo que las autoridades lo invitaran a participar en la reintegración de Alsacia-Mosela.

Los católicos de Lorena estaban preocupados por la integración al Estado francés, temerosos de esa república anticlerical. La disolución de las órdenes religiosas (1902-1904) en Francia y la separación de Estado e iglesia (1905), que suprimió toda subvención económica, no fueron aplicadas en Alsacia-Mosela en la época alemana. Estas regiones vivían aún bajo el concordato de 1801. Para defender esta particularidad, Schuman fue solicitado por los grupos católicos para presentarse como diputado y fue elegido para el cargo en 1919 como representante de Mosela por la circunscripción de Thionville, cargo que mantuvo durante toda su carrera.

En 1919 fue acusado en la asamblea de Lorena de haber formado parte del ejército alemán. Schuman respondió que esas acusaciones no iban a despertar aversiones por su parte y que los que las formulaban simplemente estaban manipulados. «Es mejor rezar por ellos que maldecirles», afirmó antes de salir de la cámara. En París, Schuman disfrutaba en ese momento de una relativa notoriedad en los círculos católicos. En esa época impulsó la formación de secciones departamentales de la CFTC (Confederación francesa de trabajadores cristianos) en Mosela. Espiritualmente se decía próximo a San Francisco de Asís.

Durante los años 1920 se asoció a los esfuerzos de paz y la acción del presidente Aristide Briand (1924-1932), que buscaba la aproximación entre la República de Weimar y la Francia del gobierno de Raymond Poincaré. En la década siguiente se mostró favorable a las sanciones impuestas a Italia, luego de que esta atacara Etiopía, pero por otra parte, se declaró partidario de los acuerdos de Múnich de 1938.

Su actividad parlamentaria fue modesta. De 1929 a 1939, fue miembro de la comisión de finanzas de la Asamblea nacional y permaneció en el Senado sin interrupción hasta 1940. Inicialmente estuvo inscrito en el partido Union Républicaine Lorraine, asociado al Bloc National (Poincaré), y después, en 1931, al Parti Démocrate Populaire, uno de los ancestros del Movimiento Republicano Popular. Schuman fue desde el principio adversario del Frente Popular (1936-1938), aunque personalmente apreciaba a Léon Blum.

En 1926 Schuman compró una casa en Scy-Chazelles en las afueras de Metz. Allí formó una colección de más de cuatro mil libros y autógrafos a partir de 1935 (entre ellos un autógrafo de Carlos I de España). Asimismo, durante varios años alquiló un apartamento en el 6.° piso sin ascensor en la rue du Bac de París.

Entre 1939 y 1945, Francia y Alemania se vieron enfrentadas en el marco de la Segunda Guerra Mundial. Schuman fue nombrado subsecretario de Estado para los refugiados en la administración de Paul Reynaud formada el 21 de marzo de 1940 —periodo de la drôle de guerre—, cargo que conservó durante el primer gobierno del mariscal Philippe Pétain. Más adelante votó a favor de otorgar plenos poderes a Petain, pero se negó a participar en dicho gobierno y regresó a Mosela que había sido nuevamente anexada a Alemania. Una vez en Metz, Schuman quemó su correspondencia misteriosamente. Después acudió a la policía para discutir la repatriación de los refugiados de Mosela y fue detenido por la Gestapo (policía secreta de la Alemania nazi) el 14 de septiembre de 1940, tras rechazar la solicitud de cooperación presentada por los nazis. Fue puesto bajo custodia en un hotel familiar de Neustadt (Renania-Palatinado) en abril de 1941. Allí le fue permitido recibir visitas y disfrutó de cierta libertad de desplazamiento, por lo que podía hacer trayectos en Alemania a excepción de Alsacia y Lorena que habían sido anexionadas por el gobierno nazi.

Persuadido de la posibilidad de ser trasladado a prisión o a un campo de prisioneros, Schuman logró evadirse en agosto de 1942 y llegó a la zona libre, para más tarde entrar en la clandestinidad el siguiente noviembre, cuando los nazis decidieron invadir el sur de Francia. Permaneció oculto en diferentes monasterios hasta que las fuerzas de ocupación fueron expulsadas.

No se conoce la reacción de Schuman al "Llamamiento del 18 de junio" en el que Charles de Gaulle convocaba al pueblo francés a mantener la resistencia ante el invasor, lo que suponía un rechazo a la petición de armisticio por parte del general Pétain. Por el contrario, Schuman fue claro en su respaldo a la posición inicial de Pétain durante la guerra y nunca formó parte de la Resistencia francesa.

Tras la invasión de Francia por los aliados y la consecuente expulsión de la Wehrmacht, el general De Lattre al mando del ejército francés, contactó a Schuman en septiembre de 1944 a fin de tener un consejero experimentado en asuntos de Alsacia-Lorena. Tres semanas después, el ministro de Guerra, André Diethelm, exigió que Schuman fuera arrestado. La sociedad de Metz acogió a Schuman, pero las autoridades lo trataron como a un exministro de Petain y como el parlamentario que votó a favor de los plenos poderes. Aunque fue considerado entonces como «indigno» e «ineligible», logró formar parte del comité departamental de liberación, donde trató de moderar la depuración.

Schuman escribió al general Charles de Gaulle el 24 de julio de 1945 y consiguió que hiciera archivar el expediente en su contra. Se proclamó su inocencia en el mes de septiembre siguiente, lo que le permitió retomar su lugar en la vida política del país. Rápidamente se convirtió en uno de los líderes principales de la Cuarta República Francesa y ocupó el cargo de ministro de Finanzas entre 1946 y 1947 en un momento crucial donde la inflación y el mercado negro tomaron fuerza.

El 24 de noviembre de 1947 asumió el cargo de presidente del Consejo de Francia (jefe de gobierno), cargo en el que estuvo hasta el 26 de julio de 1948, para volver brevemente en septiembre de ese mismo año durante apenas dos días, desde el día 5 hasta dimitir el 7 de septiembre. Durante su mandato nuevamente fue acusado de haber sido un oficial alemán, esta vez por el comunista Jacques Duclos. En julio de 1948, en el periodo que transcurrió entre sus dos gobiernos, fue nombrado ministro de Asuntos Exteriores, cargo que ocupó hasta 1952, a pesar de la inestabilidad que caracterizó al gobierno de la Cuarta República. Su bilingüismo le fue útil en las relaciones con Alemania, donde pronunció conferencias en el idioma local.
La opinión francesa vacilaba entre varias posibilidades con respecto al futuro inmediato de la Alemania derrotada. La mayoría prefería ejercer un control aliado en una Alemania descentralizada, un híbrido entre la Confederación Germánica (1815-1866) y la Confederación Alemana del Norte (1867-1871). Esta fue la posición de De Gaulle y sus sucesores hasta 1947. Otros políticos, entre los que se encontraba Schuman, prefirieron desmantelar las fuentes institucionales y culturales del militarismo alemán y fortalecer las relaciones intraeuropeas.

En marzo de 1948 fue creada la Unión Europea Occidental (Francia, Benelux y Reino Unido), como respuesta a la toma de poder de los comunistas en Praga. Esta unión se americanizó pronto y sirvió de base para la creación de la Organización del Tratado del Atlántico Norte (OTAN) en 1949.

La fusión de dos uniones aduaneras: el Benelux (surgida entre guerras) concretada en 1948 (sirvió de laboratorio a la CEE) y un proyecto de unión aduanera franco-italiana lanzado en 1947 que fracasó finalmente en 1951. Por su parte, el Consejo de Europa creado en 1949 fue destinado a preparar la confederación de estados europeos.

A comienzos de los años 1950, Schuman compró un apartamento en la rue de Verneuil, 6 en París, aunque siempre conservó su casa en Metz. A partir de 1953 no volvió a ser convocado para ningún cargo ministerial, a excepción de la cartera de Justicia en 1955 que ocupara durante la guerra de Algeria. Durante esta última, Schuman cursó órdenes para que los tribunales archivaran las denuncias por <nowiki>"las supuestas infracciones"</nowiki> cometidas por las fuerzas del orden. En 1962 no volvió a presentarse como candidato a diputado por Mosela.

Schuman no disponía de cualidades como orador. Ello no impidió que pronunciase uno de los discursos más trascendentales en la historia europea. El 9 de mayo de 1950, Schuman se dirigió a más de doscientos periodistas para presentar una declaración preparada junto a Jean Monnet (sentado a su derecha durante el discurso), que es considerada como la primera propuesta oficial para la construcción de una Europa integrada y que se conoce a partir de esa fecha como la "Declaración Schuman".

Ese día nació la Europa comunitaria, actualmente concretada en la Unión Europea. El plan de Schuman fue la base en la que se asentó la UE, una especie de primera piedra de las instituciones siguientes. En su discurso, Schuman proponía la creación de una comunidad franco-alemana para aprovechar conjuntamente el carbón y el acero de los dos países (en ese momento Alemania producía el doble de acero que Francia) bajo una Alta Autoridad común, independiente de los gobiernos y con poder para imponer sus decisiones. Una vez en funcionamiento, se ampliaría la comunidad a otros países europeos para formar un espacio de libre circulación de personas, mercancías y capital. Este sistema cruzado de intereses evitaría la posibilidad de una nueva guerra. Este proyecto de cooperación europea se presentó solo cinco años después de la capitulación de la Alemania nazi.

En la Cumbre de Milán de 1985 los jefes de Estado y de gobierno decidieron establecer el 9 de mayo como el "Día de Europa" en conmemoración de esta declaración.

Schuman firmó el Tratado de París, del 18 de abril de 1951, que constituyó la Comunidad Europea del Carbón y del Acero (CECA) entre Alemania, Bélgica, Francia, Italia, Luxemburgo y los Países Bajos. Así, con 160 millones de habitantes, 210 millones de toneladas de carbón y 33 millones de toneladas de acero producidas, la CECA se convirtió en un interlocutor de peso en las relaciones económicas internacionales.

Impulsó el plan para la formación de un ejército europeo denominado "Comunidad Europea de Defensa", que fue rechazado por los franceses en 1954.

Planeó junto a Adenauer el Estatuto del Sarre. Esta iniciativa buscaba dotar a este estado alemán, que se encontraba ocupado por el ejército francés tras el final de la guerra, de un estatuto europeo que haría del Sarre la sede de las instituciones europeas y lo convertiría la capital de la CECA. Los dos líderes decidieron aprobar la medida por referendo, confiados de su resultado positivo. Sin embargo, los ciudadanos del Sarre se pronunciaron en contra de dicho estatuto y a favor de la reintegración en la Alemania occidental.

Schuman presidió el Movimiento Europeo entre 1955 y 1961 y se convirtió así en el primer presidente de la Asamblea Parlamentaria Europea (1958-1960), que le da al fin de su mandato el título de «padre de Europa». En ese período presidió en conjunto la CEE-CECA-CEEA donde los miembros eran designados por sus estados de origen. Su función fue consultiva. Entre tanto, en 1958, le fue otorgado el Premio Carlomagno.

Entre 1955 y 1956, Schuman ocupó la cartera de Justicia que fue su último cargo ministerial. En octubre de 1959, durante una visita oficial a Italia, comenzó a manifestar los primeros síntomas de la enfermedad que afectó progresivamente su salud, cuando en medio de una conferencia de prensa perdió la lucidez.

Tras retirarse definitivamente de la Asamblea parlamentaria europea, Schuman inició la redacción de su único libro, "Por Europa", con la idea de hacer descubrir sus intuiciones europeas. Pocos meses después cedió el escrito a un amigo cercano y le dio la libertad de publicarlo después de su muerte.

En 1959 le fue diagnosticada esclerosis múltiple, por lo que se le prohibieron las caminatas y la lectura.

En enero de 1961, Schuman sufrió nuevamente ataque de esclerosis en Scy-Chazelles. Durante su caminata cotidiana, sufrió mareos y cayó al suelo sin perder totalmente el conocimiento. Su sirvienta pensaba que había sido invitado a cenar en casa de algún amigo y por ello no se inquietó de su tardanza. Al amanecer, Schuman fue encontrado inmóvil al borde de un camino por un guardia, bajo la lluvia.
En el verano de 1963, Schuman ya no podía hablar y solo era capaz de mirar y tomar la mano de su interlocutor. Algunos días antes de su muerte, el obispo de Metz, tras administrarle la unción de los enfermos, le hizo la lectura de una carta que el papa Pablo VI había escrito para Schuman. El 4 de septiembre de 1963, Robert Schuman falleció, tras una noche de agonía, a las nueve horas y treinta minutos.

Después de las exequias solemnes en la catedral de Saint-Etienne de Metz, su cuerpo fue inhumado en el cementerio municipal de Scy-Chazelles. Solamente un representante del Estado francés, el vicepresidente de la Asamblea nacional, asistió a la ceremonia. La radio y la televisión de Francia dedicaron una corta emisión a la cobertura de su fallecimiento. En 1966 los restos de Schuman fueron trasladados a la pequeña iglesia fortificada de St. Quentin de Scy-Chazelles, frente a la casa Maison de Robert Schuman, perteneciente al Consejo General del Departamento del Mosela.

La vida privada de Schuman, sus fuentes de inspiración y sus gustos siguen siendo en gran medida desconocidos.

Lo que distingue a Schuman de sus predecesores en el Ministerio de Relaciones exteriores es su enfoque de la cuestión alemana. Buscaba no repetir los errores del Tratado de Versalles de 1919, fundándose en que "la paz solamente puede basarse en la igualdad".No fue partidario del desmantelamiento de las fábricas alemanas, pero negoció los intereses de Francia en la industria del carbón. Se inclinó ante la necesidad de crear un estado alemán en el oeste, pero apoyó la creación de un estatuto del Sarre (separación de la Sarre y apego económico a Francia) y se opuso el rearme de Alemania. Schuman supo ganarse la confianza de los líderes belgas, británicos y estadounidenses; construyó una relación positiva con los dirigentes de la Democracia Cristiana italiana y cuidó sus vínculos con los líderes Luxemburgueses. Su actividad le valió que el canciller de Alemania Konrad Adenauer lo considerase como el "padre de la amistad entre los dos países".

La Declaración Schuman exponía que la reconciliación franco-alemana representaría el preludio de la integración europea. Desde entonces, la Unión Europea se ha construido por medio de sucesivas adiciones de funciones en un sistema que no corresponde a ningún modelo; es una especie de "federalismo a la inversa", en el que se ha realizado primero la transferencia de las competencias económicas y luego la de los poderes políticos, al contrario de lo que ocurre tradicionalmente en los estados federales. Más de medio siglo después de presentarse esta propuesta, la Declaración Schuman sigue siendo un patrón para medir los progresos de la Unión Europea. En este sentido, la orientación que Schuman dio a la integración europea se mantiene vigente.

La obra "Por Europa" (en francés "Pour l’Europe") es el único libro de la autoría de Robert Schuman y. Fue publicado originalmente en 1963. Este libro es su testamento político y en él profundiza la relación entre la formación de la democracia moderna y el cristianismo:

Durante la presidencia de François Mitterrand en la década de 1980, se planteó la posibilidad de trasladar los restos de Schuman al Panteón de París en reconocimiento a su obra, pero la propuesta fue rechazada por autoridades locales, que prefirieron conservarlos en Scy-Chazelles. Asimismo, desde hace varios años está abierta la causa de beatificación de Schuman. Una vez finalizada la fase diocesana, la documentación se encuentra en la Santa Sede. La causa fue apoyada por parlamentarios europeos, especialmente franceses y alemanes y está promovida por el "l'Institut Saint Benoît, Patron de l'Europe". Actualmente está considerado Siervo de Dios, la primera de las etapas para ser canonizado.




</doc>
<doc id="2477" url="https://es.wikipedia.org/wiki?curid=2477" title="Rodopsina">
Rodopsina

La rodopsina es una proteína transmembranal que, en humanos, se encuentra en los discos de los bastones de la retina. Consta de una parte proteica, opsina, y una no proteica que es un derivado de la vitamina A que es el 11-cis-retinal. Es inestable y se altera fácilmente con la energía lumínica, se decolora y descompone por exposición a la luz y se regenera con la oscuridad.

Una mayoría microorganismos marinos no fotosintéticos captan energía de la luz solar mediante rodopsina. La proteína permite a estos organismos utilizar la energía del sol para moverse, crecer y sobrevivir ante la falta de nutrientes. La rodopsina está altamente conservada y presente en los tres grandes dominios (arqueas, bacterias y eucariotas), lo que sugiere una aparición temprana y un papel fundamental en la evolución.

La opsina es una cadena polipeptídica formada por unos 348 aminoácidos. La opsina se distribuye en siete tramos de hélice alfa que se sitúan perpendiculares a la membrana unidos por partes proteicas sin estructura.

También se le llama púrpura visual, debido a su color y a que la encontró por primera vez en la retina de las ranas Franz Boll.

El carboxilo terminal se sitúa en la parte citosólica y el amino en posición intradiscal.

El 11-cis-retinal se sitúa unido a una de las hélices alfa en el centro de la molécula y colocado perpendicularmente. Esta colocación hace que cuando llegue luz incida en el 11-cis-retinal y este se transforme produciendo reacciones que llevan a un impulso nervioso.

Los bastones, que contienen rodopsina, son los responsables de la visión en condiciones de baja luminosidad. 


</doc>
<doc id="2479" url="https://es.wikipedia.org/wiki?curid=2479" title="Resina">
Resina

La resina es una secreción orgánica que producen muchas plantas, particularmente los árboles del tipo conífera. Sirve como un recubrimiento natural de defensa contra insectos u organismos patógenos. Es muy valorada por sus propiedades químicas y sus usos asociados, como por ejemplo la producción de barnices, adhesivos y aditivos alimenticios. También es un constituyente habitual de perfumes o incienso.
En muchos países, entre ellos España, es frecuente referirse a la "resina" como "resina de pino" ya que esta conífera es su principal fuente.

No existe acuerdo en la denominación de la resina y sus derivados. En este artículo se utilizará la aceptada por la Academia de la Lengua Española. Cuando pueda dar origen a confusión se incluyen los sinónimos utilizados con más frecuencia.

El término incluye también sustancias sintéticas con propiedades similares a las resinas naturales. De esta forma las resinas se dividen en: resinas naturales y resinas sintéticas.



La resina es una mezcla compleja de terpenos, ácidos resínicos, ácidos grasos y otros componentes complejos: alcoholes, ésteres...
La proporción de cada componente es función de la especie arbórea y el origen geográfico. Los valores típicos son:
Por destilación a presión ambiente, es posible separar dos fracciones:

El oficio de resinero era muy común entre los pueblos de montaña durante gran parte del siglo pasado. De los extensos pinares se extraía la resina que era vendida a buen precio en el mercado, puesto que su utilización en la industria era muy variada. Las nuevas técnicas de producción, y los nuevos materiales han relegado este oficio al olvido.

La provincia de Segovia, por estar encuadrada dentro de la comarca Tierra de Pinares, ha sido la mayor productora de resina en España, destacando la villa de Cuéllar, que su alta producción permitía el abastecimiento de parte de Castilla y Andalucía. Además, en el año 1958, la imagen de la Virgen del Henar, patrona de la Comunidad de Villa y Tierra de Cuéllar fue proclamada patrona de los resineros de España por el sumo pontífice Pío XII. Otro importante foco de producción en la provincia fue la villa de Coca, y ambas volvieron a restablecer la industria en la zona en el siglo XXI.

En Molinicos (Albacete) la industria resinera extraía grandes cantidades de este material de los extensos pinares existentes en el municipio. Aún hoy podemos observar la huella de esta industria en los troncos de los pinos.



</doc>
<doc id="2484" url="https://es.wikipedia.org/wiki?curid=2484" title="Ruppiaceae">
Ruppiaceae

Las rupiáceas (nombre científico Ruppiaceae) son una familia de plantas monocotiledóneas, herbáceas, perennes, acuáticas marinas sumergidas. La familia es reconocida por sistemas de clasificación modernos como el sistema de clasificación APG III (2009) y el APWeb (2001 en adelante), donde posee un solo género, Ruppia, con 7 especies. La familia puede ser reconocida por sus hojas dísticas, serruladas, con una vena media de fácil reconocimiento y base envainadora, y entrenudos bien desarrollados.

Las hojas no son rizomatosas, finas. Normalmente son anuales pero pueden ser perennes, no tienen agregaciones ni basales ni terminales de las hojas. El crecimiento del tallo puede ser spmpodial. Puede ser halófilo e hidrofítico, adaptados tanto a la vida marina como al agua dulce. Las hojas pueden estar sumergidas o no. No son heterófilas. Las hojas son pequeñas o medianas opuestas, alternadas o verticiladas. Las vainas foliares tiene márgenes libres. La lamina es entera, setosa o lineal. Presentan una vena, sin vénulas cruzadas. Se observan escamas axilares. No presentan estomas. El mesófilo carece de cristales de oxalato. 

La familia fue reconocida por el APG III (2009), el Linear APG III (2009) le asignó el número de familia 41. La familia ya había sido reconocida por el APG II (2003).

En los sistemas de clasificación modernos como el sistema de clasificación APG III (2009) y el APWeb (2001 en adelante) el género está en su propia familia Ruppiaceae. Anteriormente estaba en Potamogetonaceae, pero debe ser escindido para que Potamogetonaceae se mantenga monofilética.

Según el Real Jardín Botánico de Kew (visitado en enero de 2009), la familia posee 7 especies:
Cosmopolita.
México, Puerto Rico. 



</doc>
<doc id="2486" url="https://es.wikipedia.org/wiki?curid=2486" title="Rhodophyta">
Rhodophyta

Las algas rojas o rodófitas (filo Rhodophyta, del griego , «rosa», y , «planta») son un importante grupo de algas que comprende unas 7000 especies de una gran diversidad de formas y tamaños. Forman parte de Archaeplastida junto a Glaucophyta (glaucofitas) y Viridiplantae (plantas verdes), el cual es equivalente al reino Plantae en varios sistemas de clasificación.
Clásicamente en algunos sistemas de clasificación se agrupan en el reino Protista. Sin embargo hay que tener en cuenta que ese grupo no es válido ya que es un taxón cajón de sastre (un grupo conformado por los eucariotas que no encajaban en ninguno de los otros tres reinos eucarióticos).

Se caracterizan por su inmovilidad debido a la carencia o pérdida evolutiva de flagelos en todas las etapas de su ciclo vital. Sus plastos presentan dos membranas, clorofila "a" y pigmentos accesorios ficobiliproteínas y carotenoides, los cuales enmascaran el color de la clorofila y le dan el color rojo distintivo de estas algas. Están bien representadas en aguas profundas. 

Rhodophyta se origina en el Mesoproterozoico y se divide filogenéticamente en dos clados: Cyanidiophytina y Rhodophytina, los cuales divergieron hace unos 1200 millones de años.

Son un grupo primitivo de pequeñas algas unicelulares esféricas que presentan una coloración similar a las algas glaucófitas que va de verde a verde azulada (o verde cian). Se caracterizan por ser extremófilas debido a su condición termoacidófila (son termófilas e hiperacidófilas), habitando en aguas termales, calderas volcánicas y algunos entornos ácidos producto de la actividad humana, mostrando además resistencia a la presencia de metales tóxicos. 

Su reproducción es asexual por división mitótica, formando en ocasiones endosporas en número 2, 4, 8 y a veces hasta 16 células hijas. Son organismos muy simples, y en la mayoría de ocasiones poseen un solo cloroplasto y una sola mitocondria, los cuales se reproducen por división sincronizada entre la célula y estos mismos organelos.

La simplicidad de estas algas estaría en relación con la pérdida evolutiva de genes. Se estima que Cyanidiophytina habría perdido 3 veces más genes que Rhodophytina.

El clado Rhodophytina está conformado por las algas rojas propiamente dicho, con una coloración característica roja dada por el pigmento ficoeritrina. Son prácticamente marinas (solo 164 especies son de agua dulce) y se pueden encontrar en todo tipo de mares. Su ecosistema va desde zonas intermareales hasta zonas muy profundas, dependiendo de la transparencia del agua. Se pueden encontrar a 100 metros de profundidad, alcanzando hasta los 250 metros en casos excepcionales. Son las algas más abundantes en lugares profundos, pues sus pigmentos les permiten captar las longitudes de onda de la luz del Sol que penetran más profundamente en el agua. Estos pigmentos, que absorben la luz azul y reflejan la roja, les dan su característico color rojizo.

Algunas algas rojas, por ejemplo la dulse o el nori, son utilizadas como alimento y usadas para producir agar, carragenanos y otros aditivos alimenticios.

La extensa mayoría de las algas rojas son marinas, aunque existen algunas especies que viven en agua dulce o en el suelo. Son de vida libre, epífitas o epizoicas y se conocen algunas formas parásitas. Algunas especies crecen utilizando como sustrato las algas pardas (que suelen ser más grandes) o sobre las conchas de mejillones y gasterópodos. Algunas son simbiontes de foraminíferos bentónicos, mientras que otras tienen plastos vestigiales y se ven obligadas a parasitar otras rodofitas. En el grupo se incluye muchas macroalgas notables, entre las que se encuentran la mayoría de las algas coralinas que secretan carbonato de calcio y cumplen un papel crucial en la formación de los arrecifes de coral. Estas formas coralinas pueden ser difíciles de distinguir de los corales.

Las algas rojas suelen vivir en la zona litoral relativamente estrecha que bordea la placa continental. Como consecuencia de los pigmentos que presentan, pueden captar la luz a profundidades mayores que otros tipos de algas. Como ejemplo extremo se han encontrado rodofitas viviendo en las laderas de una montaña submarina a la profundidad de 268 m, donde solo es capaz de penetrar el 0,001% de la luz de la superficie. Se las encuentra en todas las latitudes, preferentemente en aguas tropicales y templadas, donde constituyen las algas más abundantes. En las aguas polares y subpolares hay pocas especies y dominan las algas verdes y pardas.

Algunas de las especies de algas rojas son unicelulares, pero la mayoría son pluricelulares creciendo en forma de filamentos o láminas membranosas, que suelen tener algún tipo de consolidación, alcanzado el grado de organización seudoparenquimatoso. Pueden existir como cilindros muy finos, ramificados en forma de arbolillo o como láminas enteras o divididas. El talo normalmente se construye mediante la agregación de numerosos filamentos, dando lugar a estructuras cilíndricas o laminares de hasta 1 m de longitud, pero que nunca alcanzan la complejidad de las algas pardas. Su tamaño es también menor al de las algas pardas más grandes. El crecimiento normalmente se produce mediante la división de la célula apical, que puede ser multinucleada. 

Adicionalmente como estructuras especializadas presentan rizoides que van desde filamentos simples hasta discos formados por la agrupación convergente de los filamentos en la base del alga. Los zarcillos son transformaciones de las ramas terminales cuya función es la adhesión a otras ramas de la misma alga, a algas mayores o a otros elementos. Como estructuras reproductoras se presentan esporangios (que forman esporas) y gametangios (que forman los gametos). Existen también células vesiculares o secretoras de sustancias que presentan una gran diversidad en su composición química.

En la mayoría de las algas rojas existen poros septales o de conexión (plasmodesmos) entre las células. Puesto que el crecimiento apical es la norma en las algas rojas, la mayoría de las células tienen dos poros primarios, uno a cada célula adyacente. Los poros que no se derivan de una división celular se denominan secundarias y se producen cuando se fusionan células adyacentes. Después de la formación del poro, la conexión citoplasmática es bloqueada mediante la formación de un tapón de proteínas que puede funcionar tanto como refuerzo estructural como de vía de comunicación de célula a célula. En el refuerzo estructural también tienen importancia los mucílagos que se acumulan en las paredes celulares, compactando tanto los filamentos axiales con los contiguos.

Las rodofitas son fotosintéticas y sus cloroplastos (llamados rodoplastos) presentan algunas características específicas de su grupo. Están rodeados por dos membranas y se supone procedentes de la endosimbiosis primaria de una cianobacteria, al igual que los de las plantas verdes y las glaucofitas. Sin embargo, presentan características diferenciadas, por lo que las rodofitas se clasifican como grupo aparte. 

Los rodoplastos contienen clorofila "a", además de los pigmentos accesorios ficobiliproteínas (ficoeritrina y ficocianina) y carotenoides. Las rodofitas son los únicos protistas fotosintéticos que presentan ficoeritrina. Antiguamente se creía que algunas especies contenían también clorofila "d", pero recientemente se ha descubierto que esta clorofila procede de una cianobacteria, "Acaryochloris marina", que vive epifita sobre estas algas. Los tilacoides son solitarios, sin apilar y algunas veces hay uno o dos tilacoides periféricos. Se caracterizan por la presencia de ficobilisomas que contienen ficobiliproteínas en la superficie de los tilacoides y ausencia de almidón en los cloroplastos, usando como material de reserva almidón de florídeas (de estructura semejante a la amilopectina) extraplastidial, en gránulos en el citoplasma, próximos al cloroplasto. Pueden presentar o no pirenoides, usualmente en los grupos basales en el primer caso. Los pigmentos les confieren un color rojo o rojizo, generalmente. 

Las rodofitas son organismos siempre inmóviles, pues carecen de células flageladas en todas las etapas de su ciclo vital. Sus células también carecen de centriolos, centrosomas y de cualquier otra estructura que implique una organización 9+2 de microtúbulos. Las células pueden ser multinucleadas. Las algas rojas tienen paredes celulares dobles. Las paredes externas contienen los polisacáridos agarosa y agaropectina que se puede extraer de las paredes celulares por ebullición para obtener agar. Las paredes internas, adheridas al citoplasma, son en su mayoría de celulosa.

Aunque en algunas especies puede faltar la sexualidad, usualmente presentan alternancia de generaciones, que pueden ser dos o tres. Son organismos en los que predomina la fase haploide, a la que se añaden una o dos fases diploides durante su ciclo vital. La reproducción sexual es por oogamia, con células especializadas, carpogonios y espermacios. Las células que hacen la función de espermatozoides carecen de flagelos y no puede nadar, por lo que son llevadas por las corrientes de agua.

El ciclo vital puede ser de dos tipos:



En la siguiente galería se muestran las tres generaciones de "Polysiphonia".

Los diversos eucariotas que componen las algas rojas han sido foco de numerosas investigaciones recientes y queda una rica fuente de especies pequeñas aún no descritas por la taxonomía tradicional. Los estudios moleculares ubican a las algas en Archaeplastida (= Primoplantae, Plantae sensu lato); sin embargo, la clasificación supraordinal se ha limitado al debate del nivel clase vs. subclase para los dos subgrupos reconocidos, uno de los cuales es ampliamente reconocido como parafilético. Este limitado foco generalmente ha ocultado en gran medida la necesidad de modificación de la clasificación algal.

Abajo hay dos taxonomías de algas rojas publicadas que son válidas, aunque necesariamente ninguna de las dos tiene que ser usada, ya que la taxonomía algal está todavía en continuo desarrollo. Nótese también que hay un debate científico continuo de si las rodofitas deberían ser incluidas en el reino Protista o en el reino Plantae. Estos dos sistemas de clasificación, en los cuales las algas se ubican en el reino Plantae, son mostrados en la tabla.


</doc>
<doc id="2487" url="https://es.wikipedia.org/wiki?curid=2487" title="Ratoncito Pérez">
Ratoncito Pérez

El Ratón o Ratoncito Pérez es un personaje fantástico que se encarga de recoger los dientes que se les caen a los niños y que colocan bajo la almohada. Mientras los niños duermen, el ratón lo cambia por dulces, monedas u otros regalos.

Se le reconoce como «Ratón» o «Ratoncito Pérez» en los países hispanohablantes, con la excepción de algunas regiones de México y Perú, donde se le llama simplemente «el Ratón de los dientes».

En Francia se le llama "la petite souris" («Ratoncito») y en Italia se le conoce como "Topolino", "Topino" («Ratoncito») o "Fatina" («Hadita»). En otros lugares hay otros seres fantásticos encargados de recoger los dientes, como "Tooth Fairy" («Hada de los dientes») en los países germanos, "l'Angelet" («el Angelito») o "La rateta" («la Ratita») en Cataluña, "Maritxu teilatukoa" («Mari la del tejado») en el País Vasco – sobre todo Vizcaya – o "L’Esquilu de los dientis" («La Ardilla de los dientes») en Cantabria. En algunos lugares es tradición tirar los dientes de los niños a los tejados de las casas.

El origen más probable del ratoncito y su enlace con un hada proviene de un cuento francés del siglo XVIII de la baronesa d'Aulnoy: "La Bonne Petite Souris" ("El buen ratoncito"). Habla de un hada que se transforma en un ratón para ayudar a derrotar a un malvado rey, ocultándose bajo la almohada del mismo, tras lo cual se le caen todos los dientes.

En España, su introducción a la mitología infantil se ha atribuido a Luis Coloma (autor también de "Pequeñeces" o "Jeromín"), cuando hacia 1894 pidieron al jesuita que escribiera un cuento para el futuro rey Alfonso XIII, que entonces tenía 8 años, y al que se le cayó un diente. Sin embargo, en la novela "La de Bringas" de Benito Pérez Galdós, escrita en 1884 y ambientada en 1868, el autor compara a un personaje, Francisco Bringas, avaro y tacaño, con el "ratoncito Pérez", luego debía ser popular para el público ya antes del cuento del padre Coloma.

Dentro del plan memoria de Madrid, el ayuntamiento de la Villa colocó una placa en el número 8 de la calle del Arenal, domicilio donde Luis Coloma situó la vivienda del roedor; en la placa puede leerse: «Aquí vivía, dentro de una caja de galletas en la confitería Prast el Ratón Pérez, según el cuento que el padre Coloma escribió para el niño rey Alfonso XIII.» En un local vecino se instaló un Museo del Ratón Pérez.

En algunos países asiáticos, como Corea, India, Japón y Vietnam, cuando un niño pierde un diente, es costumbre que lo lance al techo si viniera de la mandíbula inferior, o en el espacio debajo del piso si viniera de la mandíbula superior. Mientras se hace esto, el niño expresa un deseo de que el diente se sustituya por el diente de un ratón. Esta tradición se basa en el hecho de que los dientes de ratones crecen durante toda su vida, una característica de todos los roedores. En Japón, una variación indica que los dientes superiores se lancen directamente hacia abajo a la tierra y los dientes inferiores hacia arriba al aire, la idea es que los dientes entrantes crezcan derechos.

En países del Cercano Oriente (incluyendo Irak, Jordania, Palestina, Egipto y Sudán) existe una tradición de lanzar un diente de leche al cielo hacia el Sol o hacia Allah. Esta tradición puede tener su origen en una oferta pre-islámica que se remonta, al menos, al siglo XIII. También se menciona por Izz bin Hibat Allah Al Hadid en el siglo XIII.

En 2005 se estrenó en Buenos Aires "El Ratón Pérez, tu primer musical", de Cibrian Mahler (que se repondría en 2011). En 2007 se presenta un nuevo espectáculo teatral en el Teatro El Nacional "El Ratón Pérez y el cofre perdido", y en abril de 2010, en el Teatro Gran Rex, "El Ratón Pérez Superpoderoso".

En 2006 la historia fue llevada al cine en una coproducción hispano-argentina, bajo la dirección de Juan Pablo Buscarini, bajo el título de "El Ratón Perez" ("Pérez, el ratoncito de tus sueños" en España y México).

En enero de 2009, Disney estrenó en Hispanoamérica la película "El Ratón Pérez 2," bajo la dirección de Andrés G. Schaer. ("Pérez, el ratoncito de tus sueños 2" en España y México). 

En 2012 el Ratoncito Pérez tuvo un breve cameo en la película "El origen de los guardianes" de DreamWorks Animation. Durante la recolección de los dientes, una minihada de los dientes encuentra al ratón llevándose un diente y se pelea con él, pero ella le explica que es parte de la división latina mientras el ratón demuestra su enfado gritando y arrojando su sombrero; sin embargo, el ratón de la película parece ser un ratón ayudante que trabaja para el Ratoncito Pérez, parecido a las que trabajan para el Hada de los Dientes.

El Instituto Padre Luis Coloma de Educación Secundaria, de Jerez de la Frontera, España, para conmemorar el 180 aniversario de su fundación y en colaboración con el Aula Confucio que funciona en dicho centro, patrocinó una traducción adaptada al chino del cuento.



</doc>
<doc id="2488" url="https://es.wikipedia.org/wiki?curid=2488" title="Rhizodiniales">
Rhizodiniales

Rhizodiniales, organismos unicelulares de la división "Dinophyta", clase "Dinophyceae", subclase "Dinophycidae", con dos flagelos heterocontos en el sulco y el cíngulo. Son formas ameboides.



</doc>
<doc id="2490" url="https://es.wikipedia.org/wiki?curid=2490" title="Red punto a punto">
Red punto a punto

Los tipos de redes (multipunto) según tecnología:

Las redes punto a punto son aquellas que responden a un tipo de arquitectura de red en las que cada canal de datos se usa para comunicar únicamente dos computadoras, en clara oposición a las redes multipunto, en las cuales cada canal de datos se puede usar para comunicarse con diversos nodos.

En una red punto a punto, los dispositivos en red actúan como socios iguales, o pares entre sí. Como pares, cada dispositivo puede tomar el rol de emisor o la función de receptor. En un momento, el dispositivo A, por ejemplo, puede hacer una petición de un mensaje / dato del dispositivo B, y este es el que le responde enviando el mensaje / dato al dispositivo A. El dispositivo A funciona como receptor, mientras que B funciona como emisor. Un momento después los dispositivos A y B pueden revertir los roles: B, como receptor, hace una solicitud a A, y A, como emisor, responde a la solicitud de B. A y B permanecen en una relación recíproca o par entre ellos.








</doc>
<doc id="2491" url="https://es.wikipedia.org/wiki?curid=2491" title="RFC">
RFC

El término RFC puede referirse:


</doc>
<doc id="2494" url="https://es.wikipedia.org/wiki?curid=2494" title="Radio">
Radio

Radio hace referencia a varios artículos:










</doc>
<doc id="2502" url="https://es.wikipedia.org/wiki?curid=2502" title="Robótica">
Robótica

La robótica es la rama de la ingeniería mecánica, de la ingeniería eléctrica, de la ingeniería electrónica, de la ingeniería biomédica, y de las ciencias de la computación, que se ocupa del diseño, construcción, operación, estructura, manufactura, y aplicación de los robots. 

La robótica combina diversas disciplinas como la mecánica, la electrónica, la informática, la inteligencia artificial, la ingeniería de control y la física. Otras áreas importantes en robótica son el álgebra, los autómatas programables, la animatrónica y las máquinas de estados.

El término robot se popularizó con el éxito de la obra "R.U.R. (Robots Universales Rossum)", escrita por Karel Čapek en 1920. En la traducción al inglés de dicha obra la palabra checa "robota," que significa "trabajos forzados o trabajador," fue traducida al inglés como "robot".

La robótica va unida a la construcción de "artefactos" que trataban de materializar el deseo humano de crear seres a su semejanza y que al mismo tiempo lo descargasen de trabajos tediosos o peligrosos. El ingeniero español Leonardo Torres Quevedo (que construyó el primer mando a distancia para su automóvil mediante telegrafía, el ajedrecista automático, el primer transbordador aéreo y otros muchos ingenios), acuñó el término "automática" en relación con la teoría de la automatización de tareas tradicionalmente asociadas. 

Karel Čapek, un escritor checo, acuñó en 1920 el término "robot" en su obra dramática "Rossum's Universal Robots / R.U.R.", a partir de la palabra checa robota, que significa servidumbre o trabajo forzado. El término robótica es acuñado por Isaac Asimov, definiendo a la ciencia que estudia a los robots. Asimov creó también las tres leyes de la robótica. En la ciencia ficción el hombre ha imaginado a los robots visitando nuevos mundos, haciéndose con el poder o, simplemente, aliviando de las labores caseras.

La que a continuación se presenta es la clasificación más común:

Robots manipuladores. Son sistemas mecánicos multifuncionales con un sencillo sistema de control, bien manual, de secuencia fija o de secuencia variable.

Robots de aprendizaje. Repiten una secuencia de movimientos que ha sido ejecutada previamente por un operador humano. El modo de hacerlo es a través de un dispositivo mecánico. El operador realiza los movimientos requeridos mientras el robot le sigue y los memoriza.

Robots con control sensorizado. El controlador es un ordenador que ejecuta las órdenes de un programa y las envía al manipulador o robot para que realice los movimientos necesarios.

La estructura es definida por el tipo de configuración general del robot, puede ser metamórfica. El concepto de metamorfismo, de reciente aparición, se ha introducido para incrementar la flexibilidad funcional de un robot a través del cambio de su configuración por el propio robot. El metamorfismo admite diversos niveles, desde los más elementales (cambio de herramienta o de efecto terminal), hasta los más complejos como el cambio o alteración de algunos de sus elementos o subsistemas estructurales. Los dispositivos y mecanismos que pueden agruparse bajo la denominación genérica del robot, tal como se ha indicado, son muy diversos y es por tanto difícil establecer una clasificación coherente de los mismos que resista un análisis crítico y riguroso. La subdivisión de los robots, con base en su arquitectura, se hace en los siguientes grupos: poliarticulados, móviles, androides, zoomórficos e híbridos. 

En este grupo se encuentran los robots de muy diversa forma y configuración, cuya característica común es la de ser básicamente sedentarios (aunque excepcionalmente pueden ser guiados para efectuar desplazamientos limitados) y estar estructurados para mover sus elementos terminales en un determinado espacio de trabajo según uno o más sistemas de coordenadas, y con un número limitado de grados de libertad. En este grupo se encuentran los robots manipuladores, los robots industriales y los robots cartesianos, que se emplean cuando es preciso abarcar una zona de trabajo relativamente amplia o alargada, actuar sobre objetos con un plano de simetría vertical o reducir el espacio ocupado en el suelo. 

Son Robots con gran capacidad de desplazamiento, basados en carros o plataformas y dotados de un sistema locomotor de tipo rodante. Siguen su camino por telemando o guiándose por la información recibida de su en torno a través de sus sensores. Estos robots aseguran el transporte de piezas de un punto a otro de una cadena de fabricación. Guiados mediante pistas materializadas a través de la radiación electromagnética de circuitos empotrados en el suelo, o a través de bandas detectadas fotoeléctricamente, pueden incluso llegar a sortear obstáculos y están dotados de un nivel relativamente elevado de inteligencia. 


Son los tipos de robots que intentan reproducir total o parcialmente la forma y el comportamiento cinemático del ser humano. Actualmente, los androides son todavía dispositivos muy poco evolucionados y sin utilidad práctica, y destinados, fundamentalmente, al estudio y experimentación. 
Uno de los aspectos más complejos de estos robots, y sobre el que se centra la mayoría de los trabajos, es el de la locomoción bípeda. En este caso, el principal problema es controlar dinámica y coordinadamente en el tiempo real el proceso y mantener simultáneamente el equilibrio del Robot. Vulgarmente se los suele llamar "marionetas" cuando se les ven los cables que permiten ver cómo realiza sus procesos. 

Los robots zoomórficos, que considerados en sentido no restrictivo podrían incluir también a los androides, constituyen una clase caracterizada principalmente por sus sistemas de locomoción que imitan a los diversos seres vivos. 
A pesar de la disparidad morfológica de sus posibles sistemas de locomoción es conveniente agrupar a los Robots zoomórficos en dos categorías principales: caminadores y no caminadores. El grupo de los robots zoomórficos no caminadores está muy poco evolucionado. Los experimentos efectuados en Japón basados en segmentos cilíndricos biselados acoplados axialmente entre sí y dotados de un movimiento relativo de rotación. Los Robots zoomórficos caminadores multípedos son muy numerosos y están siendo objeto de experimentos en diversos laboratorios con vistas al desarrollo posterior de verdaderos vehículos terrenos, pilotados o autónomos, capaces de evolucionar en superficies muy accidentadas. Las aplicaciones de estos robots serán interesantes en el campo de la exploración espacial y en el estudio de los volcanes. 

Estos robots corresponden a aquellos de difícil clasificación, cuya estructura se sitúa en combinación con alguna de las anteriores ya expuestas, bien sea por conjunción o por yuxtaposición. Por ejemplo, un dispositivo segmentado articulado y con ruedas es, al mismo tiempo, uno de los atributos de los robots móviles y de los robots zoomórficos.




</doc>
<doc id="2505" url="https://es.wikipedia.org/wiki?curid=2505" title="Robot">
Robot

Un robot es una entidad virtual o mecánica artificial. En la práctica, esto es por lo general un sistema electromecánico que, por su apariencia o sus movimientos, ofrece la sensación de tener un propósito propio.
La independencia creada en sus movimientos hace que sus acciones sean la razón de un estudio razonable y profundo en el área de la ciencia y tecnología.
La palabra robot puede referirse tanto a mecanismos físicos como a sistemas virtuales de software, aunque suele aludirse a los segundos con el término de bots.

No hay un consenso sobre qué máquinas pueden ser consideradas robots, pero sí existe un acuerdo general entre los expertos y el público sobre que los robots tienden a hacer parte o todo lo que sigue: moverse, hacer funcionar un brazo mecánico, sentir y manipular su entorno y mostrar un comportamiento inteligente, especialmente si ese comportamiento imita al de los humanos o a otros animales. Actualmente podría considerarse que un robot es una computadora con la capacidad y el propósito de movimiento que en general es capaz de desarrollar múltiples tareas de manera flexible según su programación; así que podría diferenciarse de algún electrodoméstico específico. 

Aunque las historias sobre ayudantes y acompañantes artificiales, así como los intentos de crearlos, tienen una larga historia, las máquinas totalmente autónomas no aparecieron hasta el siglo XX. El primer robot programable y dirigido de forma digital, el Unimate, fue instalado en 1961 para levantar piezas calientes de metal de una máquina de tinte y colocarlas. 

Los robots domésticos para la limpieza y mantenimiento del hogar son cada vez más comunes. No obstante, existe una cierta ansiedad sobre el impacto económico de la automatización y la amenaza del armamento robótico, una ansiedad que se ve reflejada en el retrato a menudo perverso y malvado de robots presentes en obras de la cultura popular. Comparados con sus colegas de ficción, los robots reales siguen siendo limitados.

El gran público conoció la palabra robot a través de la obra "R.U.R. (Robots Universales Rossum)" del dramaturgo checo Karel Čapek, que se estrenó en 1920. La palabra se escribía como "robotnik".

Sin embargo, no fue este autor Čapek quien inventó la palabra. En una breve carta escrita a la editorial del Diccionario Oxford, atribuye a su hermano Josef la creación del término. En un artículo publicado en la revista checa "Lidové noviny" en 1933, explicó que originalmente le quiso llamar "laboři" (del latín "labor", trabajo). Sin embargo, no le gustaba la palabra y pidió consejo a su hermano Josef, que le sugirió "roboti". La palabra "robota" significa literalmente trabajo o labor y figuradamente "trabajo duro" en checo y muchas lenguas eslavas. Tradicionalmente robota era el periodo de trabajo que un siervo debía otorgar a su señor, generalmente 6 meses del año. La servidumbre se prohibió en 1848 en Bohemia, por lo que cuando Čapek escribió "R.U.R.", el uso del término "robota" ya se había extendido a varios tipos de trabajo, pero el significado obsoleto de "servidumbre" seguiría reconociéndose.

La palabra robótica, usada para describir este campo de estudio, fue acuñada por el escritor de ciencia ficción Isaac Asimov.
La robótica concentra 3 áreas de estudio: la mecatrónica, la física y las matemáticas como ciencias básicas.

En el siglo IV antes de Cristo, el matemático griego Arquitas de Tarento construyó un ave mecánica que funcionaba con vapor y a la que llamó «La paloma». También el ingeniero Herón de Alejandría (10-70 d. C.) creó numerosos dispositivos automáticos que los usuarios podían modificar, y describió máquinas accionadas por presión de aire, vapor y agua. Por su parte, el estudioso chino Su Sung levantó una torre de reloj en 1088 con figuras mecánicas que daban las campanadas de las horas.

Al Jazarí (1136–1206), un inventor musulmán de la dinastía Artuqid, diseñó y construyó una serie de máquinas automatizadas, entre las que había útiles de cocina, autómatas musicales que funcionaban con agua, y en 1206 los primeros robots humanoides programables. Las máquinas tenían el aspecto de cuatro músicos a bordo de un bote en un lago, entreteniendo a los invitados en las fiestas reales. Su mecanismo contenía un tambor programable con clavijas que chocaban con pequeñas palancas que accionaban instrumentos de percusión. Podían cambiarse los ritmos y patrones que tocaba el tamborilero moviendo las clavijas.

El artesano japonés Hisashige Tanaka (1799–1881), conocido como el «Edison japonés», creó una serie de juguetes mecánicos extremadamente complejos, algunos de los cuales servían té, disparaban flechas sacadas de un carcaj e incluso trazaban un "kanji" (caracteres utilizados en la escritura japonesa).

Por otra parte, desde la generalización del uso de la tecnología en procesos de producción con la Revolución Industrial se intentó la construcción de dispositivos automáticos que ayudasen o sustituyesen al hombre. Entre ellos destacaron los Jaquemarts, muñecos de dos o más posiciones que golpean campanas accionados por mecanismos de relojería china y japonesa.

Robots equipados con una sola rueda fueron utilizados para llevar a cabo investigaciones sobre conducta, navegación y planeo de ruta. Cuando estuvieron listos para intentar nuevamente con los robots caminantes, comenzaron con pequeños hexápodos y otros tipos de robots de múltiples patas. Estos robots imitaban insectos y artrópodos en funciones y forma. Como se ha mencionado anteriormente, la tendencia se dirige hacia ese tipo de cuerpos que ofrecen gran flexibilidad y han demostrado ser adaptables a cualquier ambiente. Con más de 4 piernas, estos robots son estáticamente estables, lo que hace que el trabajar con ellos sea más sencillo. Recientemente se han hecho progresos hacia los robots con locomoción bípeda.

En el sentido común de un autómata, el mayor robot en el mundo tendría que ser el "Maeslantkering", una barrera para tormentas del Plan Delta en los Países Bajos construida en los años 1990, la cual se cierra automáticamente cuando es necesario. Sin embargo, esta estructura no satisface los requerimientos de movilidad o generalidad.

En 2002 Honda y Sony comenzaron a vender comercialmente robots humanoides como «mascotas». Los robots con forma de perro o de serpiente se encuentran, sin embargo, en una fase de producción muy amplia; el ejemplo más notorio ha sido Aibo de Sony.

En la actualidad, los robots comerciales e industriales se utilizan ampliamente y realizan tareas de forma más exacta o más barata que los humanos. También se emplean en trabajos demasiado sucios, peligrosos o tediosos para los humanos. Los robots se usan en plantas de manufactura, montaje y embalaje, en transporte, en exploraciones en la Tierra y en el espacio, cirugía, armamento, investigación en laboratorios y en la producción en masa de bienes industriales o de consumo.

Otras aplicaciones incluyen la limpieza de residuos tóxicos, minería, búsqueda y rescate de personas y localización de minas terrestres.
Existe una gran esperanza, especialmente en Japón, de que el cuidado del hogar para la población de edad avanzada pueda ser desempeñado por robots.

Los robots parecen estar abaratándose y reduciendo su tamaño, una tendencia relacionada con la miniaturización de los componentes electrónicos que se utilizan para manejarlos. Además, muchos robots son diseñados en simuladores mucho antes de construirse y de que interactúen con ambientes físicos reales. Un buen ejemplo de esto es el equipo "Spiritual Machine", un equipo de 5 robots desarrollado totalmente en un ambiente virtual para jugar al fútbol en la liga mundial de la "F.I.R.A."

Además de los campos mencionados, hay modelos trabajando en el sector educativo, servicios (por ejemplo, en lugar de recepcionistas humanos o vigilancia) y tareas de búsqueda y rescate.

Recientemente se ha logrado un gran avance en los robots dedicados a la medicina, con dos compañías en particular, "Computer Motion" e "Intuitive Surgical", que han recibido la aprobación regulatoria en América del Norte, Europa y Asia para que sus robots sean utilizados en procedimientos de cirugía invasiva mínima. Desde la compra de Computer Motion (creador del robot Zeus) por Intuitive Surgical, se han desarrollado ya 6 modelos de robot Da Vinci por esta última, pasando por el primero modelo DaVinci, S, Si, Xi, X y el más reciente lanzado "SP". Actualmente hasta diciembre de 2017 se contabilizan en el mundo alrededor de 4409 sistemas Da Vinci, siendo Estados Unidos el país con más equipos disponibles, con un total de 2,862. Con aplicaciones en Urología, Ginecología, Cirugía general, Coloproctología, Cirugía Pediátrica, Cirugía Torácica, Cirugía Cardíaca y ORL.
También la automatización de laboratorios es un área en crecimiento. Aquí, los robots son utilizados para transportar muestras biológicas o químicas entre instrumentos tales como incubadoras, manejadores de líquidos y lectores. Otros lugares donde los robots están reemplazando a los humanos son la exploración del fondo oceánico y exploración espacial. Para esas tareas se suele recurrir a robots de tipo artrópodo.

Un impulsor muy significativo de este tipo de investigaciones es el desarrollo de equipos de espionaje militar.
A fin de proteger a aquellos que ponen su vida en peligro, los robots de seguridad y defensa aptos para el combate pueden realizar numerosas misiones para ayudar a los profesionales de la seguridad pública y del ejército.

Existen diferentes tipos y clases de robots, entre ellos con forma humana, de animales, de plantas o incluso de elementos arquitectónicos pero todos se diferencian por sus capacidades y se clasifican en 4 formas:

En esta última se puede clasificar según su morfología en: Robots angulares o antropomórficos, robots cilíndricos, robots esféricos o polares, robots tipo SCARA, robots paralelos, robots cartesianos, entre otros.

El robot de fabricación más común es el robot industrial y de entre los robots industriales, el más común es el brazo articulado también llamado brazo robótico. Un brazo robótico típico se compone de siete segmentos metálicos, unidos por seis articulaciones. Una computadora controla el robot girando motores de pasos individuales conectados a cada junta (los brazos más grandes utilizan la hidráulica o neumática). A diferencia de los motores eléctricos de movimiento continuo, los motores de pasos pueden moverse en incrementos exactos. Esto permite que el ordenador pueda mover el brazo de manera muy precisa, repitiendo exactamente el mismo movimiento una y otra vez. El robot utiliza sensores de movimiento para hacer que se mueva la cantidad justa.

Un robot industrial con seis articulaciones se asemeja mucho a un brazo humano - tiene el equivalente de un hombro, un codo y la muñeca. Típicamente, el hombro está montado en una estructura de base estacionaria en lugar de a un cuerpo móvil. Este tipo de robot tiene seis grados de libertad, lo que significa que puede pivotar en seis formas diferentes. Un brazo humano, en comparación, tiene siete grados de libertad.

El trabajo del brazo humano es mover la mano de un lugar a otro. Del mismo modo, el trabajo del brazo robótico es mover un efector final de un lugar a otro. Se puede equipar brazos robóticos con todo tipo de efectores de extremo, que están adaptados a una aplicación particular. Un efector final común es una versión simplificada de la mano, que puede captar y transportar objetos diferentes. Las manos robóticas a menudo han incorporado sensores de presión que le dicen a la computadora que tan fuerte el robot está sujetando un objeto en particular. Esto evita que el robot tire o rompa lo que lleva. Otros efectores finales incluyen sopletes, los soldadores por puntos, taladros y la pintura por
aire a presión entre otros.

Los robots industriales están diseñados para hacer exactamente lo mismo, en un ambiente controlado, una y otra vez. Por ejemplo, un robot podría cerrar las tapas de frascos de mantequilla que salen de una línea de montaje. Para enseñar a un robot cómo hacer su trabajo, el programador guía el brazo a través de los movimientos utilizando un controlador de mano ("teach pendant"). El robot almacena la secuencia exacta de los movimientos en su memoria, y lo hace una y otra vez cada vez que una nueva unidad viene por la línea de montaje.

Existen diferentes técnicas para programar robots industriales. Entre ellas se encuentran las técnicas de programación gestual y las de programación textual. En la programación gestual un operario guía al robot, manualmente o mediante controles remotos, enseñándole la tarea que este debe realizar. El robot va almacenando los pasos a seguir y luego puede repetirlos de manera autónoma. En la programación textual, en cambio, se realizan primero los cálculos de las posiciones y trayectorias que el robot debe recorrer y, con esta información, se crean las instrucciones del programa que el robot deberá ejecutar. Una vez transferido el programa al robot, este puede comenzar a realizar la tarea de manera autónoma.

La mayoría de los robots industriales trabajan en cadenas de montaje de automóviles, poniendo los coches juntos. Los robots pueden hacer este trabajo más eficientemente que los seres humanos gracias a su precisión, que les permite por ejemplo perforar siempre en el mismo lugar o apretar siempre los tornillos con la misma cantidad de fuerza, sin importar las horas que trabaje (cosa que no sucede con los humanos). Los robots de fabricación son también muy importantes en la industria electrónica, ya que se necesita un control increíblemente preciso para armar un microchip.


Muchas mitologías antiguas tratan la idea de los humanos artificiales.
En la mitología clásica, se dice que Cadmo sembró dientes de dragón que se convertían en soldados, y Galatea, la estatua de Pigmalión, cobró vida. También el dios griego de los herreros, Hefesto (Vulcano para los romanos) creó sirvientes mecánicos inteligentes, otros hechos de oro e incluso mesas que se podían mover por sí mismas.
Algunos de estos autómatas ayudan al dios a forjar la armadura de Aquiles, según la Ilíada
Aunque, por supuesto, no se describe a esas máquinas como "robots" o como "androides", son en cualquier caso dispositivos mecánicos de apariencia humana.

Una leyenda hebrea habla del Golem, una estatua animada por la magia cabalística. Por su parte, las leyendas de los Inuit describen al Tupilaq (o Tupilak), que un mago puede crear para cazar y asesinar a un enemigo. Sin embargo, emplear un Tupilaq para este fin puede ser una espada de doble filo, ya que la víctima puede detener el ataque del Tupilaq y reprogramarlo con magia para que busque y destruya a su creador.

Ya en 1817, en un cuento de Hoffmann llamado "El hombre de arena", aparece una mujer que parecía una muñeca mecánica, y en la obra de Edward S. Ellis de 1865 "El Hombre de Vapor de las Praderas" se expresa la fascinación americana por la industrialización.

Como se indicaba más arriba, la primera obra en utilizar la palabra "robot" fue la obra teatral R.U.R. de Čapek,(escrita en colaboración con su hermano Josef en 1920; representada por primera vez en 1921; escenificada en Nueva York en 1922. La edición en inglés se publicó en 1923).

La obra comienza en una fábrica que construye personas artificiales llamadas robots, pero están más cerca del concepto moderno de androide o clon, en el sentido de que se trata de criaturas que pueden confundirse con humanos. Pueden pensar por sí mismos, aunque parecen felices de servir. En cuestión está si los robos están siendo explotados, así como las consecuencias por su tratamiento.

El autor más prolífico de historias sobre robots fue Isaac Asimov (1920-1992), que colocó los robots y su interacción con la sociedad en el centro de muchos de sus libros. Este autor consideró seriamente la serie ideal de instrucciones que debería darse a los robots para reducir el peligro que estos representaban para los humanos. Así llegó a formular sus Tres Leyes de la Robótica: Ningún robot causará daño a un ser humano o permitirá, con su inacción, que un ser humano sufra daño; todo robot obedecerá las órdenes que le den los seres humanos, a menos que esas órdenes entren en conflicto con la primera ley; y todo robot debe proteger su propia existencia, siempre que esa protección no entre en conflicto con la primera o la segunda ley.

Esas tres leyes se introdujeron por primera vez en su relato corto de 1942 "Círculo Vicioso", aunque habían sido esbozadas en algunos textos anteriores. Más tarde, Asimov añadió la ley de Cero: "Ningún robot causará daño a la humanidad ni permitirá, con su inacción que la humanidad sufra daño". El resto de las leyes se modificaron para ajustarse a este añadido.

Según el Oxford English Dictionary, el principio del relato breve "¡Mentiroso!" de 1941 contiene el primer uso registrado de la palabra robótica. El autor no fue consciente de esto en un principio, y asumió que la palabra ya existía por su analogía con mecánica, hidráulica y otros términos similares que se refieren a ramas aplicadas del conocimiento.

El tono económico y filosófico iniciado por R.U.R. sería desarrollado más tarde por la película "Metrópolis", y las populares "Blade Runner" (1982) o "The Terminator" (1984).

Existen muchas películas sobre robots, entre las cuales cabe destacar:

En televisión, existen series muy populares como "Robot Wars" y "BattleBots". En la serie "Futurama" y Doraemon, de Matt Groening y Fujiko F. Fujio respectivamente, los robots poseen una identidad propia, como ciudadanos. También, en la serie "Almost Human" aparecen robots-policías con conciencia propia, llamados DRN, los cuales funcionan con un programa de “alma sintética”.

Existe la preocupación de que los robots puedan desplazar o competir con los humanos. Las leyes o reglas que pudieran o debieran ser aplicadas a los robots u otros “entes autónomos” en cooperación o competencia con humanos si algún día se logra alcanzar la tecnología suficiente como para hacerlos inteligentes y conscientes de sí mismos, han estimulado las investigaciones macroeconómicas de este tipo de competencia, notablemente construido por Alessandro Acquisti basándose en un trabajo anterior de John von Neumann.

Actualmente, no es posible aplicar las Tres leyes de la robótica, dado que los robots no tienen capacidad para comprender su significado, evaluar las situaciones de riesgo tanto para los humanos como para ellos mismos o resolver los conflictos que se podrían dar entre estas leyes.

Entender y aplicar lo anteriormente expuesto requeriría verdadera inteligencia y consciencia del medio circundante, así como de sí mismo, por parte del robot, algo que a pesar de los grandes avances tecnológicos de la era moderna no se ha alcanzado.

Muchas grandes empresas, como Intel, Sony, General Motors, Dell, han implementado en sus líneas de producción unidades robóticas para desempeñar tareas que por lo general hubiesen desempeñado trabajadores de carne y hueso en épocas anteriores.

Esto ha causado una agilización en los procesos realizados, así como un mayor ahorro de recursos, al disponer de máquinas que pueden desempeñar las funciones de cierta cantidad de empleados a un costo relativamente menor y con un grado mayor de eficiencia, mejorando notablemente el rendimiento general y las ganancias de la empresa, así como la calidad de los productos ofrecidos.

Pero, por otro lado, ha suscitado y mantenido inquietudes entre diversos grupos por su impacto en la tasa de empleos disponibles, así como su repercusión directa en las personas desplazadas. Dicha controversia ha abarcado el aspecto de la seguridad, llamando la atención de casos como el ocurrido en Jackson, Míchigan, el 21 de julio de 1984 donde un robot aplastó a un trabajador contra una barra de protección en la que aparentemente fue la primera muerte relacionada con un robot en los EE. UU.

Debido a esto se ha llamado la atención sobre la ética en el diseño y construcción de los robots, así como la necesidad de contar con lineamientos claros de seguridad que garanticen una correcta interacción entre humanos y máquinas.

El empleo de robots para labores de manufactura pudiera aún abaratar costos, ya que a diferencia de un operario humano no acarrearía pago de sueldos/salarios ni reivindicaciones laborales. No obstante, por tratarse de una máquina requeriría de servicio técnico (mantenimiento y reparación), lo cual conlleva un gasto monetario.

El relator especial de la ONU sobre ejecuciones extrajudiciales, sumarias o arbitrarias, Christof Heyns, está tratando de detener la creación y el esparcimiento de los robots autónomos letales (LAR), conocidos también como robots asesinos, hacia otros países de manera general. Heyns realizó un informe en el que se menciona de manera muy significativa la necesidad de realizar una legislación o protocolo mundial que describa el compromiso serio y significativo de poner un límite al desarrollo de esta tecnología que, probablemente en un futuro no muy lejano, a los robots se les confiera el poder y el permiso para matar a los seres humanos.

Asimismo, mencionó que, mientras los drones sigan teniendo a un ser humano como su controlador que les dirija a quién matar y a quién no, también sería muy probable que los robots asesinos, debido a su programación centrada en el ataque contra los seres humanos y a la destrucción de los mismos, puede ser un grave peligro. En su informe plantea su idea sobre las preocupaciones de las posibles consecuencias del uso de los LAR, y es que, según Heyns, los robots podrían desequilibrar la balanza entre la guerra y la paz, y no solo eso: tienen una estructura que les permite tener un largo alcance cuando son utilizados.

El relator propuso que el desarrollo de esta tecnología no dejaría nada bueno para nadie, quizá para las grandes potencias que tienen planeado una guerra o el aprovechamiento de algunos recursos. Pero la cuestión aquí es la programación que pueden tener estos robots, porque ¿será posible que un robot pueda distinguir entre los combatientes y los civiles?

Para finalizar su informe menciona que, a pesar de que su implementación, sería inaceptable. De igual modo, sería muy importante que se elaboren una serie de reglas que permitan un manejo en el desarrollo de esta tecnología que no beneficiaría para nada a la sociedad, y en cambio estaría representando un grave problema para todo el mundo, porque si los robots pudieran tomar por sí mismos algunas decisiones, estaríamos en un gran riesgo que no se debe permitir o por lo menos lograr controlar el manejo de estos robots.

"Así que tal vez es necesario formalizar de una vez por todas en las restricciones del uso y la construcción de esta tecnología, así como tener la capacidad moral y ética de realizar robots en beneficio de la sociedad, que contribuyan en todo momento con los seres humanos. Esta tecnología abrirá camino a un futuro prometedor donde resplandezca paz y una buena interacción entre robots y humanos. Solo pido a las grandes potencias que no tengan en mente destruir este hermoso planeta que nos ha dotado de mucha vida y felicidad a todos nosotros y sobre todo a las futuras generaciones."



</doc>
<doc id="2506" url="https://es.wikipedia.org/wiki?curid=2506" title="Rottboellia">
Rottboellia

Rottboellia es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones tropicales y subtropicales de Asia y África.

El nombre del género fue otorgado en honor de Christen Friis Rottbøll.

Muchas de las especies en esta lista han sido trasladadas a otros géneros: "Chasmopodium", "Coelorachis", "Elionurus", "Eremochloa", "Glyphochloa", "Hainardia", "Hemarthria", "Henrardia", "Heteropholis", "Ischaemum", "Lasiurus", "Lolium", "Loxodera", "Manisuris", "Mnesithea", "Muhlenbergia", "Ophiuros", "Oropetium", "Parapholis", "Phacelurus", "Pholiurus", "Psilurus", "Ratzeburgia", "Rhytachne", "Schizachyrium", "Spartina", "Stenotaphrum", "Thaumastochloa", "Urelytrum" y "Xerochloa".



</doc>
<doc id="2507" url="https://es.wikipedia.org/wiki?curid=2507" title="Richardsiella eruciformis">
Richardsiella eruciformis

Richardsiella es un género monotípico de plantas herbáceas, perteneciente a la familia de las poáceas. Su única especie: Richardsiella eruciformis Elffers & Kenn.-O'Byrne, es originaria de Zambia.

El nombre del género fue otorgado en honor de H.M.Richards, recolector de plantas.



</doc>
<doc id="2509" url="https://es.wikipedia.org/wiki?curid=2509" title="Rechazo múltiple">
Rechazo múltiple

Rechazo múltiple es un tipo de respuesta ARQ en la cual no se dejan de enviar paquetes hasta que se recibe un NACK. En ese momento se interrumpe la transmisión y se empieza la transmisión continua a partir del paquete que tenía errores desperdiciando así toda la información transmitida entre el primer envío y la detección del error.

Este tipo de ARQ exige una memoria en el transmisor que sea capaz de almacenar tantos datos como los que puedan enviarse en un timeout, ya que será el tiempo máximo de espera y esos datos deben reenviarse tras detectar un error.

Otra de las exigencias de este tipo de ARQ es la numeración de los ACK's para poder distinguir a qué paquete de información están asintiendo.


</doc>
<doc id="2511" url="https://es.wikipedia.org/wiki?curid=2511" title="Reduced instruction set computing">
Reduced instruction set computing

En arquitectura computacional, RISC (del inglés "Reduced Instruction Set Computer", en español "Computador con conjunto de instrucciones reducido") es un tipo de diseño de CPU generalmente utilizado en microprocesadores o microcontroladores con las siguientes características fundamentales:


Además estos procesadores suelen disponer de muchos registros de propósito general.

El objetivo de diseñar máquinas con esta arquitectura es posibilitar la segmentación y el paralelismo en la ejecución de instrucciones y reducir los accesos a memoria.
Las máquinas RISC protagonizan la tendencia actual de construcción de microprocesadores. PowerPC, DEC Alpha, MIPS, ARM, SPARC son ejemplos de algunos de ellos.

RISC es una filosofía de diseño de CPU para computadora que está a favor de conjuntos de instrucciones pequeñas y simples que toman menor tiempo para ejecutarse. El tipo de procesador más comúnmente utilizado en equipos de escritorio, el x86, está basado en CISC en lugar de RISC, aunque las versiones más nuevas traducen instrucciones basadas en CISC x86 a instrucciones más simples basadas en RISC para uso interno antes de su ejecución.

La idea fue inspirada por el hecho de que muchas de las características que eran incluidas en los diseños tradicionales de CPU para aumentar la velocidad estaban siendo ignoradas por los programas que eran ejecutados en ellas. Además, la velocidad del procesador en relación con la memoria de la computadora que accedía era cada vez más alta. Esto conllevó la aparición de numerosas técnicas para reducir el procesamiento dentro del CPU, así como de reducir el número total de accesos a memoria.

Terminología más moderna se refiere a esos diseños como arquitecturas de carga-almacenamiento.

Uno de los principios básicos de diseño para todos los procesadores es añadir velocidad al proveerles alguna memoria muy rápida para almacenar información temporalmente, estas memorias son conocidas como registros. Por ejemplo, cada CPU incluye una orden para sumar dos números. La operación básica de un CPU sería cargar esos dos números en los registros, sumarlos y almacenar el resultado en otro registro, finalmente, tomar el resultado del último registro y devolverlo a la memoria principal.

Sin embargo, los registros tienen el inconveniente de ser algo complejos para implementar. Cada uno está representado por transistores en el chip, en este aspecto la memoria principal tiende a ser mucho más simple y económica. Además, los registros le añaden complejidad al cableado, porque la unidad central de procesamiento necesita estar conectada a todos y cada uno de los registros para poder utilizarlos por igual.

Como resultado de esto, muchos diseños de CPU limitan el uso de registros de alguna u otra manera. Algunos incluyen pocos registros, aunque esto limita su velocidad. Otros dedican sus registros a tareas específicas para reducir la complejidad; por ejemplo, un registro podría ser capaz de hacer operaciones con uno o más de los otros registros, mientras que el resultado podría estar almacenado en cualquiera de ellos.

En el mundo de la microcomputación de los años setenta, este era un aspecto más de las CPU, ya que los procesadores eran entonces demasiado lentos –de hecho había una tendencia a que el procesador fuera más lento que la memoria con la que se comunicaba-. En esos casos tenía sentido eliminar casi todos los registros, y entonces proveer al programador de una buena cantidad de maneras de tratar con la memoria para facilitar su trabajo.

Dado el ejemplo de la suma, la mayoría de los diseños de CPU se enfocaron a crear una orden que pudiera hacer todo el trabajo automáticamente: llamar los dos números que serían sumados, sumarlos, y luego almacenarlos fuera directamente. Otra versión podría leer los dos números de la memoria, pero almacenaría el resultado en un registro. Otra versión podría leer uno de la memoria y otro desde un registro y almacenarlo en la memoria nuevamente. Y así sucesivamente.

La meta en general en aquel tiempo era proveer cada posible modo de direccionamiento para cada instrucción, un principio conocido como ortogonalidad. Esto llevó a un CPU complejo, pero en teoría capaz de configurar cada posible orden individualmente, haciendo el diseño más rápido en lugar de que el programador utilizara órdenes simples.

La última representación de este tipo de diseño puede ser vista en dos equipos, el MOS 6502 por un lado, y el VAX en el otro. El chip 6502 de $25 USD efectivamente tenía solamente un registro, y con la configuración cuidadosa de la interfaz de memoria fue capaz de sobrepasar diseños corriendo a velocidades mayores (como el Zilog Z80 a 4MHz). El VAX era un minicomputador que en una instalación inicial requería 3 gabinetes de equipo para un solo CPU, y era notable por la sorprendente variedad de estilos de acceso a memoria que soportaba, y el hecho de que cada uno de estos estaba disponible para cada instrucción.

A finales de los setenta, investigaciones en IBM (y otros proyectos similares en otros lugares), demostraron que la mayoría de esos modos de direccionamiento "ortogonal" eran ignorados por la mayoría de los programas. Esto fue un efecto colateral en el incremento en el uso de compiladores para generar los programas, algo opuesto a escribirlos en lenguaje ensamblador. Los compiladores tendían a ser demasiado tontos en términos de las características que usaban, un efecto colateral del intento por hacerlos pequeños. El mercado se estaba moviendo hacia un uso más generalizado de los compiladores, diluyendo aún más la utilidad de los modelos ortogonales.

Otro descubrimiento fue que debido a que esas operaciones eran escasamente utilizadas, de hecho tendían a ser más lentas que un número pequeño de operaciones haciendo lo mismo. Esta paradoja fue un efecto colateral del tiempo que se utilizaba diseñando los CPU, los diseñadores simplemente no tenían tiempo de optimizar cada instrucción posible, y en vez de esto solo optimizaban las más utilizadas. Un famoso ejemplo de esto era la instrucción codice_1, que se ejecutaba más lentamente que un bucle que implementara el mismo código.

Casi al mismo tiempo, las CPU comenzaron a correr a velocidades mayores que las de la memoria con la que se comunicaban. Aún a finales de los setenta, era aparente que esta disparidad continuaría incrementándose al menos durante la siguiente década, para entonces los CPU podrían ser cientos de veces más rápidos que la memoria. Esto significó que los avances para optimizar cualquier modo de direccionamiento serían completamente sobrepasados por las velocidades tan lentas en las que se llevaban a cabo.

Otra parte del diseño RISC llegó desde las medidas prácticas de los programas en el mundo real. Andrew Tanenbaum reunió muchos de estos, demostrando así que la mayoría de los procesadores estaban sobredimensionados. Por ejemplo, él demostró que el 98 % de todas las constantes en un programa podían acomodarse en 13 bits, aun cuando cada diseño de CPU dedicaba algunos múltiplos de 8 bits para almacenarlos, típicamente 8, 16 o 32, una palabra entera. Tomando este hecho en cuenta sugiere que una máquina debería permitir que las constantes fuesen almacenadas en los bits sin utilizar de otras instrucciones, disminuyendo el número de accesos a memoria. En lugar de cargar números desde la memoria o los registros, estos podrían estar "ahí mismo" para el momento en el que el CPU los necesitara, y por lo tanto el proceso sería mucho más rápido. Sin embargo, esto requería que la instrucción misma fuera muy pequeña, de otra manera no existiría suficiente espacio libre en los 32 bits para mantener constantes de un tamaño razonable.

Fue el pequeño número de modos y órdenes que dio lugar al término "conjunto reducido de instrucciones". Esta no es una definición correcta, ya que los diseños RISC cuentan con una vasta cantidad de conjuntos de instrucciones para ellos. La verdadera diferencia es la filosofía para hacer todo en registros y llamar y guardar los datos hacia ellos y en ellos mismos. Esta es la razón por la que la forma más correcta de denominar este diseño es "cargar-almacenar". Con el paso del tiempo las técnicas de diseño antiguas se dieron a conocer como "Computadora con Conjunto de Instrucciones Complejo", CISC por sus siglas en inglés, aunque esto fue solamente para darles un nombre diferente por razones de comparación.

Por esto la filosofía RISC fue crear instrucciones pequeñas, implicando que había pocas, de ahí el nombre "conjunto reducido de instrucciones". El código fue implementado como series de esas instrucciones simples, en vez de una sola instrucción compleja que diera el mismo resultado. Esto hizo posible tener más espacio dentro de la instrucción para transportar datos, resultando esto en la necesidad de menos registros en la memoria. Al mismo tiempo la interfaz con la memoria era considerablemente simple, permitiendo ser optimizada.

Sin embargo RISC también tenía sus desventajas. Debido a que una serie de instrucciones son necesarias para completar incluso las tareas más sencillas, el número total de instrucciones para la lectura de la memoria es más grande, y por lo tanto lleva más tiempo. Al mismo tiempo no estaba claro dónde habría o no una ganancia neta en el desempeño debido a esta limitación, y hubo una batalla casi continua en el mundo de la prensa y del diseño sobre los conceptos de RISC.

Debido a lo redundante de las microinstrucciones, los sistemas operativos diseñados para estos microprocesadores, contemplaban la capacidad de subdividir un microprocesador en varios, reduciendo el número de instrucciones redundantes por cada instancia del mismo. Con una arquitectura del software optimizada, los entornos visuales desarrollados para estas plataformas, contemplaban la posibilidad de ejecutar varias tareas en un mismo ciclo de reloj. Así mismo, la paginación de la memoria RAM era dinámica y se asignaba una cantidad suficiente a cada instancia, existiendo una especie de 'simbiosis' entre la potencia del microprocesador y la RAM dedicada a cada instancia del mismo.

La multitarea dentro de la arquitectura CISC nunca ha sido real, tal como en los RISC sí lo es. En CISC, el microprocesador en todo su conjunto está diseñado en tantas instrucciones complejas y diferentes, que la subdivisión no es posible, al menos a nivel lógico. Por lo tanto, la multitarea es aparente y por órdenes de prioridad. Cada ciclo de reloj trata de atender a una tarea instanciada en la RAM y pendiente de ser atendida. Con una cola de atención por tarea FIFO para los datos generados por el procesador, y LIFO para las interrupciones de usuario, trataban de dar prioridad a las tareas que el usuario desencadenara en el sistema. La apariencia de multitarea en un CISC tradicional, viene de la mano de los modelos escalares de datos, convirtiendo el flujo en un vector con distintas etapas y creando la tecnología pipeline.

Los microprocesadores actuales, al ser híbridos, permiten cierta parte de multitarea real. La capa final al usuario es como un CISC tradicional, mientras que las tareas que el usuario deja pendientes, dependiendo del tiempo de inactividad, el sistema traducirá las instrucciones (el software ha de ser compatible con esto) CISC a RISC, pasando la ejecución de la tarea a bajo nivel, en donde los recursos se procesan con la filosofía RISC. Dado que el usuario solo atiende una tarea por su capacidad de atención, el resto de tareas que deja pendientes y que no son compatibles con el modelo de traducción CISC/RISC, pasan a ser atendidas por el tradicional pipeline, o si son tareas de bajo nivel, tal como desfragmentaciones de disco, chequeo de la integridad de la información, formateos, tareas gráficas o tareas de cálculo matemático intenso.

En vez de tratar de subdividir a un solo microprocesador, se incorporó un segundo microprocesador gemelo, idéntico al primero. El inconveniente es que la RAM debía de ser tratada a nivel hardware y los módulos diseñados para plataformas monoprocesador no eran compatibles o con la misma eficiencia, que para las plataformas multiprocesador. Otro inconveniente, era la fragmentación del BYTE de palabra. En un RISC tradicional, se ocupan los BYTES de la siguiente forma: Si la palabra es de 32 BITS (4 BYTES de palabra de 8 BITS cada una, o dos de 16 o una de 32), dependiendo de la profundidad del dato portado, dentro del mismo BYTE, se incluían partes de otras instrucciones y datos. Ahora, al ser dos microprocesadores distintos, ambos usaban registros independientes, con accesos a la memoria propios (en estas plataformas, la relación de RAM por procesador es de 1/1). En sus orígenes, las soluciones se parecían a las típicas ñapas de albañil, cada placa base incorporaba una solución solamente homologada por la chip set usada y los drivers que la acompañaban. Si bien la fragmentación siempre ha sido como ese mosquito que zumba en el oído, pero que por pereza permitimos que nos pique, llegó un momento que era imposible evadir el zumbido. Esta época llegó con las plataformas de 64 BITS.

Mientras la filosofía de diseño RISC se estaba formando, nuevas ideas comenzaban a surgir con un único fin: incrementar drásticamente el rendimiento de la CPU.

Al principio de la década de los ochenta se pensaba que los diseños existentes estaban alcanzando sus límites teóricos. Las mejoras de la velocidad en el futuro serían hechas con base en "procesos" mejorados, esto es, pequeñas características en el chip. La complejidad del chip podría continuar como hasta entonces, pero un tamaño más pequeño podría resultar en un mejor rendimiento del mismo al operar a más altas velocidades de reloj. Se puso una gran cantidad de esfuerzo en diseñar chips para computación paralela, con vínculos de comunicación interconstruidos. En vez de hacer los chips más rápidos, una gran cantidad de chips serían utilizados, dividiendo la problemática entre estos. Sin embargo, la historia mostró que estos miedos no se convirtieron en realidad, y hubo un número de ideas que mejoraron drásticamente el rendimiento al final de la década de los ochenta.

Una idea era la de incluir un canal por el cual se pudieran dividir las instrucciones en pasos y trabajar en cada paso muchas instrucciones diferentes al mismo tiempo. Un procesador normal podría leer una instrucción, decodificarla, enviar a la memoria la instrucción de origen, realizar la operación y luego enviar los resultados. La clave de la canalización es que el procesador pueda comenzar a leer la siguiente instrucción tan pronto como termine la última instrucción, significando esto que ahora dos instrucciones se están trabajando (una está siendo leída, la otra está comenzando a ser decodificada), y en el siguiente ciclo habrá tres instrucciones. Mientras que una sola instrucción no se completaría más rápido, la "siguiente" instrucción sería completada enseguida. La ilusión era la de un sistema mucho más rápido. Esta técnica se conoce hoy en día como Segmentación de cauce.

Otra solución más era utilizar varios elementos de procesamiento dentro del procesador y ejecutarlos en paralelo. En vez de trabajar en una instrucción para sumar dos números, esos procesadores superescalares podrían ver la siguiente instrucción en el canal y tratar de ejecutarla al mismo tiempo en una unidad idéntica. Esto no era muy fácil de hacer, sin embargo, ya que algunas instrucciones dependían del resultado de otras instrucciones.

Ambas técnicas se basaban en incrementar la velocidad al añadir complejidad al diseño básico del CPU, todo lo opuesto a las instrucciones que se ejecutaban en el mismo. Siendo el espacio en el chip una cantidad finita, para poder incluir todas esas características algo más tendría que ser eliminado para hacer hueco. RISC se encargó de tomar ventaja de esas técnicas, esto debido a que su lógica para el CPU era considerablemente más simple que la de los diseños CISC. Aun con esto, los primeros diseños de RISC ofrecían una mejora de rendimiento muy pequeña, pero fueron capaces de añadir nuevas características y para finales de los ochenta habían dejado totalmente atrás a sus contrapartes CISC. Con el tiempo esto pudo ser dirigido como una mejora de proceso al punto en el que todo esto pudo ser añadido a los diseños CISC y aun así caber en un solo chip, pero esto tomó prácticamente una década entre finales de los ochenta y principios de los noventa.

En pocas palabras esto significa que para cualquier nivel de desempeño dado, un chip RISC típicamente tendrá menos transistores dedicados a la lógica principal. Esto permite a los diseñadores una flexibilidad considerable; así pueden, por ejemplo:


Las características que generalmente son encontradas en los diseños RISC son:

Los diseños RISC también prefieren utilizar como característica un modelo de memoria Harvard, donde los conjuntos de instrucciones y los conjuntos de datos están conceptualmente separados; esto significa que el modificar las direcciones donde el código se encuentra pudiera no tener efecto alguno en las instrucciones ejecutadas por el procesador (porque la CPU tiene separada la instrucción y el caché de datos, al menos mientras una instrucción especial de sincronización es utilizada). Por otra parte, esto permite que ambos cachés sean accedidos separadamente, lo que puede en algunas ocasiones mejorar el rendimiento.

Muchos de esos diseños RISC anteriores también compartían una característica no muy amable, el slot de salto retardado (Delay Slot). Un slot de salto retardado es un espacio de instrucción siguiendo inmediatamente un salto. La instrucción en este espacio es ejecutada independientemente de si el salto se produce o no (en otras palabra el salto es retardado). Esta instrucción mantiene la ALU de la CPU ocupada por el tiempo extra normalmente necesario para ejecutar una brecha. Para utilizarlo, recae en el compilador la responsabilidad de reordenar las instrucciones de manera que el código sea coherente para ejecutar con esta característica. En nuestros días el slot de salto retardado se considera un desafortunado efecto colateral de la estrategia particular por implementar algunos diseños RISC. Es por esto que los diseños modernos de RISC, tales como ARM, PowerPC, y versiones más recientes de SPARC y de MIPS, generalmente eliminan esta característica.

El primer sistema que pudiera ser considerado en nuestros días como RISC no lo era así en aquellos días; era la supercomputadora CDC 6600, diseñada en 1964 por Seymour Cray.

Cray la diseñó como un CPU para cálculos a gran escala (con 74 códigos, comparada con un 8086 400, además de 12 computadores simples para manejar los procesos de E/S (la mayor parte del sistema operativo se encontraba en uno de estos).

El CDC 6600 tenía una arquitectura de carga/almacenamiento con tan solo dos modos de direccionamiento. Había once unidades de canalización funcional para la aritmética y la lógica, además de cinco unidades de carga y dos unidades de almacenamiento (la memoria tenía múltiples bancos para que todas las unidades de carga/almacenamiento pudiesen operar al mismo tiempo). El nivel promedio de operación por ciclo/instrucción era 10 veces más rápido que el tiempo de acceso a memoria.

Los diseños RISC que más se dieron a conocer sin embargo, fueron aquellos donde los resultados de los programas de investigación de las universidades eran ejecutados con fondos del programa DARPA VLSI. El programa VLSI prácticamente desconocido hoy en día, llevó a un gran número de avances en el diseño de chips, la fabricación y aún en las gráficas asistidas por computadora.

Una de las primeras máquinas de carga/almacenamiento fue la minicomputadora Data General Nova, diseñado en 1968 por Edson de Castro. Había un conjunto de instrucciones RISC casi puro, muy similar a la de los procesadores ARM de hoy, sin embargo no ha sido citado como haber influido en los diseñadores del ARM, aunque estas máquinas estaban en uso en la Universidad de Cambridge ComputerLaboratory en la década de 1980.

El proyecto RISC de la Universidad de Berkeley comenzó en 1980 bajo la dirección de David A. Patterson, basándose en la obtención de rendimiento a través del uso de la canalización y un agresivo uso de los registros conocido como ventanas de registros. En una CPU normal se tienen un pequeño número de registros, un programa puede usar cualquier registro en cualquier momento. En una CPU con ventanas de registros, existen un gran número de registros (138 en el RISC-I), pero los programas solo pueden utilizar un pequeño número de estos (32 en el RISC-I) en cualquier momento.

Un programa que se limita asimismo a 32 registros por procedimiento puede hacer llamadas a procedimientos muy rápidas: la llamada, y el regreso, simplemente mueven la ventana de 32 registros actual para limpiar suficiente espacio de trabajo para la subrutina, y el regreso "restablece" esos valores.

El proyecto RISC entregó el procesador RISC-I en 1982. Consistiendo de solo 44.420 transistores (comparado con promedios de aproximadamente 100.000 en un diseño CISC de esa época) RISC-I solo tenía 32 instrucciones, y aun así sobrepasaba el desempeño de cualquier otro diseño de chip simple. Se continuó con esta tendencia y RISC-II en 1983 tenía 40.760 transistores y 39 instrucciones, con los cuales ejecutaba 3 veces más rápido que el RISC-I.

Casi al mismo tiempo, John Hennessy comenzó un proyecto similar llamado MIPS en la Universidad de Stanford en 1981. MIPS se centraba casi completamente en la segmentación, asegurándose de que ejecutara tan "lleno" como fuera posible. Aunque la segmentación ya había sido utilizada en otros diseños, varias características del chip MIPS hacían su segmentación mucho más rápida. Lo más importante, y quizá molesto de estas características era el requisito de que todas las instrucciones fueran capaces de completarse en un solo ciclo. Este requisito permitía al canal ser ejecutado a velocidades más altas (no había necesidad de retardos inducidos) y es la responsable de la mayoría de la velocidad del procesador. Sin embargo, también tenía un efecto colateral negativo al eliminar muchas de las instrucciones potencialmente utilizables, como una multiplicación o una división.

El primer intento por hacer una CPU basada en el concepto RISC fue hecho en IBM el cual comenzó en 1975, precediendo a los dos proyectos anteriores. Nombrado como proyecto RAN, el trabajo llevó a la creación de la familia de procesadores IBM 801, la cual fue utilizada ampliamente en los equipos de IBM. El 801 fue producido eventualmente en forma de un chip como "ROMP" en 1981, que es la abreviatura de "Research Office Products Division Mini Processor". Como implica el nombre, esta CPU fue diseñada para "tareas pequeñas", y cuando IBM lanzó el diseño basado en el IBM RT-PC en 1986, el rendimiento no era aceptable. A pesar de esto, el 801 inspiró varios proyectos de investigación, incluyendo algunos nuevos dentro de IBM que eventualmente llevarían a su sistema IBM POWER.

En los primeros años, todos los esfuerzos de RISC eran bien conocidos, pero muy confinados a los laboratorios de las universidades que los habían creado. El esfuerzo de Berkeley se dio a conocer tanto que eventualmente se convirtió en el nombre para el proyecto completo. Muchos en la industria de la computación criticaban el que los beneficios del rendimiento no se podían traducir en resultados en el mundo real debido a la eficiencia de la memoria de múltiples instrucciones, y esa fue la razón por la que nadie los estaba utilizando. Pero a comienzos de 1986, todos los proyectos de investigación RISC comenzaron a entregar productos. De hecho, casi todos los procesadores RISC modernos son copias directas del diseño RISC-II.

La investigación de Berkeley no fue comercializada directamente, pero el diseño RISC-II fue utilizado por Sun Microsystems para desarrollar el SPARC, por Pyramid Technology para desarrollar sus máquinas de multiprocesador de rango medio, y por casi todas las compañías unos años más tarde. Fue el uso de RISC por el chip de SUN en las nuevas máquinas el que demostró que los beneficios de RISC eran reales, y sus máquinas rápidamente desplazaron a la competencia y esencialmente se apoderaron de todo el mercado de estaciones de trabajo.

John Hennessy dejó Stanford para comercializar el diseño MIPS, comenzando una compañía conocida como MIPS Computer Systems Inc. Su primer diseño fue el chip de segunda generación MIPS-II conocido como el "R2000". Los diseños MIPS se convirtieron en uno de los chips más utilizados cuando fueron incluidos en las consolas de juego Nintendo 64 y PlayStation. Hoy son uno de los procesadores integrados más comúnmente utilizados en aplicaciones de alto nivel por Silicon Graphics.

IBM aprendió del fallo del RT-PC y tuvo que continuar con el diseño del RS/6000 basado en su entonces nueva arquitectura IBM POWER. Entonces movieron sus computadoras centrales S/370 a los chips basados en IBM POWER, y se sorprendieron al ver que aun el conjunto de instrucciones muy complejas (que era parte del S/360 desde 1964) corría considerablemente más rápido. El resultado fue la nueva serie System/390 que aún hoy en día es comercializada como zSeries. El diseño IBM POWER también se ha encontrado moviéndose hacia abajo en escala para producir el diseño PowerPC, el cual eliminó muchas de las instrucciones "solo IBM" y creó una implementación de chip único. El PowerPC fue utilizado en todas las computadoras Apple Macintosh hasta 2006, y está comenzando a ser utilizado en aplicaciones automotrices (algunos vehículos tienen más de 10 dentro de ellos), las consolas de videojuegos de última generación (PlayStation 3, Wii y Xbox 360) están basadas en PowerPC.

Casi todos los demás proveedores se unieron rápidamente. De los esfuerzos similares en el Reino Unido resultó el INMOS Trasputer, el Acorn Archimedes y la línea Advanced RISC Machine, la cual tiene un gran éxito hoy en día. Las compañías existentes con diseños CISC también se unieron a la revolución. Intel lanzó el i860 y el i960 a finales de los ochenta, aunque no fueron muy exitosos. Motorola construyó un nuevo diseño pero no le vio demasiado uso y eventualmente lo abandonó, uniéndose a IBM para producir el PowerPC. AMD lanzó su familia 29000 la cual se convirtió en el diseño RISC más popular a principios de los noventa.

Hoy en día los microcontroladores y CPU RISC representan a la vasta mayoría de todos los CPU utilizados. La técnica de diseño RISC ofrece poder incluso en medidas pequeñas, y esto ha venido a dominar completamente el mercado de CPU integrados de bajo consumo de energía. Los CPU integrados son por mucho los procesadores más comunes en el mercado: considera que una familia completa con una o dos computadoras personales puede poseer varias docenas de dispositivos con procesadores integrados. RISC se apoderó completamente del mercado de estación de trabajo. Después del lanzamiento de la SUN SPARCstation los otros proveedores se apuraron a competir con sus propias soluciones basadas en RISC. Aunque hacia 2006-2010 las estaciones de trabajo pasaron a la arquitectura x86-64 de Intel y AMD. Incluso el mundo de las computadoras centrales está ahora basado completamente en RISC.

Esto es sorprendente en vista del dominio del Intel x86 y x86 64 en el mercado de las computadoras personales de escritorio (ahora también en el de estaciones de trabajo), ordenadores portátiles y en servidores de la gama baja. Aunque RISC fue capaz de avanzar en velocidad muy rápida y económicamente.

Los diseños RISC han llevado a un gran número de plataformas y arquitecturas al éxito, algunas de las más grandes:









</doc>
<doc id="2512" url="https://es.wikipedia.org/wiki?curid=2512" title="Receta culinaria">
Receta culinaria

Una receta culinaria, receta de cocina o simplemente receta, en gastronomía, es una descripción ordenada de un procedimiento culinario. Suele consistir primero en una lista de ingredientes necesarios, seguido de una serie de instrucciones con la cual se elabora un plato o una bebida específicos. Suele incluir en algunos casos una lista de los utensilios de cocina adecuados para su realización. Ocasionalmente incluye una descripción social, histórica que motiva la receta.

Las recetas pueden transmitirse a lo largo de la historia de los pueblos, de generación en generación, mediante tradición oral, o escritas mediante su recopilación en libros de cocina o recetarios culinarios. Este conocimiento compilado forma parte importante de la cultura de un grupo humano, su evolución permite conocer los cambios a los que se ve sometida una cultura. Su empleo en estudios sociológicos y antropológicos, o en historia permite conocer las condiciones culinarias, los gustos, e influencias de un periodo. En el siglo XXI las recetas culinarias aparecen frecuentemente en medios de comunicación como programas de televisión, revistas, periódicos y blogs.

Una de las primeras evidencias documentales de recetas conocidas procede de 1600 en forma de tablilla de barro procedente del sur de Babilonia con escritura cuneiforme y expresada en idioma acadio. Los griegos tuvieron escritores culinarios dedicados como el poeta Arquestrato, el prolífico escritor culinario Timáquides de Rodas. Ninguno de sus recetarios ha llegado a nuestros días. Uno de los primeros libros de recetas conocidos en la cocina occidental fue de "De re coquinaria" escrito por el cocinero romano Marco Gavio Apicio.

El nombre «receta» proviene del latín "recipere" que indica por igual 'dar'/'recibir'. Inicialmente en los textos de recetas, los procesos culinarios se describían como una secuencia de instrucciones. El primer recetario medieval fue un manuscrito alemán del siglo XIII. La cocina española tiene en el "Libre del Sent Soví", 1324 uno de sus recetarios más antiguos.

En Europa los cocineros franceses Antonin Carême y Georges Auguste Escoffier comienzan a definir las técnicas de la cocina, y entre su tarea la de recopilar y sistematizar los procesos de cocina. En América del Norte Isabella Beeton escribe a finales del siglo XIX su "Book of Household Management" como uno de los primeros recetarios modernos. El fenómeno antropológico de las recetas culinarias, como transmisión de cultura fue estudiado por Claude Lévi-Strauss en su obra "Les mythologiques 3: L'origine des manières de table" (El origen de las maneras en la mesa). En los años sesenta aparecen los programas de televisión mostrando formas de cocinar.

Las recetas culinarias anteriores al siglo XX poseían más una estructura narrativa que permitía cierta creación literaria paralela. Es precisamente en las primeras décadas del siglo XX cuando aparece una estructura separada de ingredientes/procesos en la descripción de las recetas culinarias. Si la tradición oral se cristalizó en una literatura culinaria en forma de libros de cocina escritos desde el siglo XVIII, en el siglo XX las recetas se describen en programas de televisión, en revistas (especializadas o no), siendo además muy populares en diversos blogs especializados.
Las recetas tienen unas normas y reglas precisas para su escritura. Si la receta se dirige al público en general, debería estar escrita en lenguaje llano. En muchos casos presuponen un conocimiento básico de las técnicas de cocina. Las recetas aparecen generalmente categorizadas en familias que se agrupan por ingrediente principal, tipo de preparación, país, etc. forman parte de un libro de cocina.

Las recetas formales incluyen como elementos:


En recetas editadas en libros, o recetarios culinarios, usualmente se incluye una fotografía del plato ya montado, y generalmente ya decorado para su muestra a los comensales. A veces incluye una ilustración secuenciada de los procesos más notables. Para mejorar la didáctica de la receta suele incluirse indicaciones acerca de como elegir un buen ingrediente, detalles sobre la calidad de los mismos. Si el ingrediente indicado no es habitual al lector, proporcionar consejos acerca de donde poder encontrarlo. La inclusión sobre ciertos detalles empleados en las técnicas culinarias no habituales puede ayudar a un lector a reproducción con éxito la receta. En las recetas se incluye a veces el cómputo total de calorías que supone su ingesta, así como cualquier indicación nutricional. En las recetas culinarias generalmente es menos deseable conocer el origen histórico, sociológico del plato.
Y tiene propósito de enviar la receta a una persona para indicarle lo que tiene que hacer para cocinar

Históricamente no se ha considerado que las recetas culinarias posean derechos de autor, debido en parte a que se ha considerado a las preparaciones gastronómicas como un proceso generalmente útil a la humanidad bajo los principios del "Utilitarismo". Con el devenir de la innovación en la cocina, así como la industrialización de algunas preparaciones, a lo largo del siglo XXI se ha replanteado la cuestión. Existen casos de autoría confusa, de disputa en el reconocimiento. Algunas recetas poseen usos privados que producen beneficios a ciertos restaurantes y cocineros, y es por esta razón que algunas recetas (generalmente salsas) se han protegido con una marcas registradas y poseen derechos de autor. En algunas ocasiones se pretende proteger la preparación de un plato con el objeto de preservar una cultura, y es por esta razón por la que su elaboración se supervisa por un "organismo competente". En otras ocasiones las recetas culinarias están libres de licencias, y su elaboración no está regulada por organismo alguno. En Estados Unidos las leyes de Copyright (como la "Copyright Act of 1976") protegen invenciones, ideas y dispositivos, excepto las recetas culinarias.



</doc>
<doc id="2514" url="https://es.wikipedia.org/wiki?curid=2514" title="Rhytachne">
Rhytachne

Rhytachne es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Sudamérica tropical, Madagascar y África tropical.



</doc>
<doc id="2515" url="https://es.wikipedia.org/wiki?curid=2515" title="Rhombolytrum">
Rhombolytrum

Rhombolytrum es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Sudamérica. Comprende 5 especies descritas y de estas, solo 2 aceptadas. 

El género fue descrito por Heinrich Friedrich Link y publicado en "Hortus Regius Botanicus Berolinensis" 2: 296. 1833. La especie tipo es: "Rhombolytrum rhomboideum" Link
Rhombolytrum: nombre genérico 
A continuación se brinda un listado de las especies del género "Rhombolytrum" aceptadas hasta enero de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos.



</doc>
<doc id="2516" url="https://es.wikipedia.org/wiki?curid=2516" title="Rhipidocladum">
Rhipidocladum

Rhipidocladum es un género de bambúes nativos de América. Pertenece a la subfamilia de las Bambusóideas, dentro de la familia de las gramíneas o Poáceas.
Rizomas paquimorfos, culmos de 7 a 22 m de largo, generalmente decumbentes en la vegetación aledaña; 1 hasta 1,5 cm de diámetro. Las ramas por nudo pueden variar entre 30 y 200 según la especie, no verticiladas sino creciendo en forma de abanico en la base. Las ramas primarias de hasta 35 cm de largo. Cúlmeas u hojas caulinares lisas y brillantes, no persistentes en los entrenudos inferiores. Presentan de 12 a 32 espiguillas por racimo de acuerdo a la especie.

Ramas primarias creciendo en forma de abanico y en número que varía de 35 a 200 según la especie. Culmo hueco y quebradizo.
Ramas secundarias de hasta 35 cm de largo.

El género "Rhipidocladum" se distribuye naturalmente desde México hasta Argentina y Chile. 

Bosques de galería, riparia. Selvas medianas. Ecotonos. 200 a 900 msnm

El género fue descrito por Floyd Alonzo McClure y publicado en "Smithsonian Contributions to Botany" 9: 101, f. 42. 1973. La especie tipo es: "Rhipidocladum harmonicum" (Parodi) McClure

Dentro del género se reconocen tres secciones:

y un total de 19 especies: 




</doc>
<doc id="2520" url="https://es.wikipedia.org/wiki?curid=2520" title="Raddiella">
Raddiella

Raddiella, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de América tropical.




</doc>
<doc id="2521" url="https://es.wikipedia.org/wiki?curid=2521" title="Raddia">
Raddia

Raddia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del centro y sur de América.

Las especies descritas de "Raddia" son 9:



</doc>
<doc id="2525" url="https://es.wikipedia.org/wiki?curid=2525" title="Runrig">
Runrig

Runrig fue un grupo escocés de música rock con toques de música celta tradicional. Cantaban en inglés y en Gaélico escocés.

La en origen Banda de Baile Runrig, debutó como tal en el Glasgow Kelvin Hall en 1973. La formaban Rory MacDonald a la guitarra, Calum MacDonald a la batería y Blair Douglas al acordeón.

Donnie Munro se unió al año siguiente para darle más aporte vocal. Más tarde Blair se fue y se les unió como acordeonista un viejo amigo de la escuela, Robert MacDonald, quien falleció tristemente en 1986, tras una larga batalla contra el cáncer.

Hasta 1978 la banda había sido una ocupación parcial de estudiantes y fue en este formato como se grabó su primer álbum "Play Gaelic" en la discográfica escocesa Limor Recordigns.

Tras este primer paso sintieron que podrían y debían llevar adelante su propia discográfica para dotarles de libertad artística y financiera para grabar su más ambicioso segundo álbum. 

Eran tiempos de gran riesgo y retos, si bien ciertamente estos han rodeado toda la historia de Runrig. Ridge Records culminó con éxito y dotó a la banda de estatus profesional completo. Malcom fue persuadido para abandonar una brillante carrera universitaria, Rory diseñador gráfico, realizó su última obra maestra para las agencias publicitarias y Donnie and Calum, profesores, ya no podían salir de las clases lo suficientemente rápido como deseaban.
En 1979 entraron en el estudio y grabaron The HighLand Connection.

El sonido de la banda iba enriqueciéndose y había necesidad de extender sus parámetros musicales. Reforzaron la sección rítmica con la unión de un “Fifer” (fife:tipo de flauta), Iain Bayne, haciéndose cargo de la batería, pasando Calum a la percusión. Esta fue la formación que grabó el clásico "Recovery" en 1981.
Recovery fue un álbum conceptual, tratando de la historia social del Gaélico y les introdujo en la lucha por el idioma y culturas Gaélicos, parte de la cual la banda siempre ha sido. Es desde este ambiente desde el que el núcleo de la banda ha hecho su razón de ser musical, física, emocional y espiritual.

Tras "Recovery", la banda sintió la necesidad de extender sus alas desde las Tierras Altas Gaélicas para alcanzar más vastas audiencias que parecían tener interés a su música.

Aunque ciertamente, como banda de Folk, sus canciones reflejaban la tradición, era gratificante alcanzar a nuevas audiencias que se interesaban por la música de la banda. Parecía ahora que el Gaélico podía cruzar y ser aceptado por un entorno totalmente diferente.
Una vez más se dio la necesidad de extender sus parámetros musicales, para entonces convertidos ya en sexteto, empleando los servicios de un inglés, Richard Chern a los teclados, Tenían el dilema de permanecer en Ridge Records o salir en busca de contratos mayores.
Iniciaron, pues, este último camino pero no fue el momento adecuado y la compañía de grabación en cuestión no resultó ser lo más adecuado para la banda. Así experimentaron plenamente la hegemonía de los dictados comerciales sobre los artísticos con una compañía londinesa.

Finalmente la banda volvió a Ridge Records pero solo tras un prolongado retraso, tras el cual, en 1985, se grabó finalmente el 4º álbum, "Heartland". Richard se fue en 1986 para trabajar en el teatro y su vacante fue cogida por otro “Fifer”, el trabajador social ex BigCountry Peter Wishart; formación, la cual, perduró en la siguiente década. 
1987 fue claramente el año de la explosión. Los puntos más llamativos incluyen un exitoso viaje a Canadá, la primera salida al Telón de Acero para tocar en un festival en Berlín Este, un concierto en vivo emitido en ITV una colaboración con los recién erigidos reyes de Rock’n’Roll U2 en Murrayfield Stadium, Edimburgo y la publicación de "The Cutter and the Clan".
El álbum fue un increíble éxito para el sello Ridge, llevando a Runrig de la industria rural al ámbito nacional. Era tiempo de firmar con una de las compañías de discos en auge que más interés había mostrado por el grupo; Chrysalis Records con su carácter de independencia e integridad musical se convirtió en compañera inseparable. En el verano de 1987 firmó un importante contrato internacional con la banda y se convirtió en la estrella de un total acercamiento a su vida y trabajo.
"The Cutter and the Clan" fue inmediatamente reeditado con Chrysalis seguido rápidamente por el largamente esperado "Once in a Lifetime". 1989 vio la publicación de "Searchlight", el cual entró directamente en le puesto 11º de las listas nacionales, y tuvo su continuación con una gira internacional con 50 citas en Reino Unido y Europa, culminando en las Barrowlands Escocesas, con un concierto grabado por la STV para un futuro vídeo.

La nueva década se inició con un bombazo: apenas 30 escasos minutos tomados de la banda en el Glasgow George Square bastaron para liderar el Show de Hogmanay en la BBC. La cadena STV realizó un documental de una hora grabado en los conciertos de las BarrowLands que emitió en mayo. La respuesta fue abrumadora. La centralita de STV entonces se saturó durante horas y el programa tuvo cifras de audiencia improcedentes para la cadena. En 1990 vio también la luz el EP "Capture the Hearth", el cual entró directamente en las listas de singles con el número 49. La apertura en el Royal Concert Hall en Glasgow presentó la oportunidad de tocar en múltiples noches. El resultado fue de 5 conciertos con el cartel de “completo”. El esperado vídeo "City of Lights" fue publicado en noviembre, entrando el las listas nacionales con el número 7. Ese año se completó con el tour de "Alba". Un año más ocupado de lo esperado, pero aún quedaba más por venir.

Hay años llenos de trabajo y años “a tope”, y así fue 1991. Listar todos los eventos no sería suficiente para relatar todo el trabajo de todos aquellos conectados con la banda sino solo una ligera idea.
El octavo y más exitoso álbum, "The Big Wheel", entró directamente en las Listas Nacionales en el puesto 4º. El concierto al aire libre en el Balloch Country Park en Loch Lomond, seguido por 50,000 personas fue una inmejorable ocasión para probar la sin duda brillante carrera de Runrig. El tour de las HighLands y las Islas les llevó de nuevo a casa con una gigantesca carpa. El sencillo "Hearthammer" irrumpió en el Top 40 en el puesto 25. Dos conciertos más en la explanada del Castillo de Edimburgo atrajeron enorme interés de los medios, convirtiendo el Tour de "The Big Wheel" en internacional.

Otro single, "Flower of the West", fue publicado y el libro de Tom Morton "Going Home", acabó el año en la cima, convirtiéndose inmediatamente en un best seller escocés.
1992 vio a Runrig trabajando duro en los estudios de nuevo así como un buen número de apariciones en festivales por toda Europa. Los fanes vieron 2 veces a la banda talonear a Génesis; primero Hockenheim, Alemania y de nuevo en Roundhay Park, Leeds. Los fanes del otro lado del Atlántico, en Canadá tuvieron la oportunidad de verles tocar en Toronto y Montreal, y pisaron EE.UU., por primera vez para filmar en Nueva York un documental para la STV titulado "Air an Oir".
El 24 de agosto un nuevo video, "Wheel in Motion", fue publicado con imágenes en vivo del memorable concierto de Loch Lomond, del concierto de las HighLands y las Islas, el Castillo de Edimburgo y otras citas por Europa a lo largo de 1991.Al mismo tiempo, continuaron trabajando en Air an Oir con Graeme Strong para la Televisión Escocesa. Esta película fue emitida en Año Nuevo, despidiendo justamente 1993 en la dirección correcta.

Noviembre de 1992 vio a Runring retornar a los estudios Castle Sound, en Pencaitland, para grabar su siguiente álbum que estaba completado para enero de 1993. El sencillo "Wonderful", se publicó primero y a continuación el álbum "Amazing Things". El mismo alcanzó la más alta posición nunca conseguida en la Listas, entrando en la Clasificación Gallup con el número 2, a solo un punto de la cima. Wonderful y el segundo single "Greatest Flame", fueron ambos reproducidos en el Top of the Pops debido a su éxito en las Listas.
El resto de 1993 se empleó en promocionar el álbum retornando al directo. El tour de "Amazing Things" fue el más exitoso hecho nunca y para el último show en Barrowlands el 22 de diciembre, la banda había tocado 99 conciertos. Visitaron por segunda vez los teatros de Alemania e Inglaterra amen de otras notables actuaciones en Irlanda, Londres y las Fleadh Escocesas.

Finlandia se convirtió en un nuevo territorio cuando tocaron el en Festival de Turku en agosto de vuelta a casa entraron de nuevo en el Top e iniciaron un pequeño tour por Castillos y Lienzos culminando con un agradable retorno a la explanada del Castillo de Edimburgo en septiembre. Un punto álgido del año de "Amazing Things" fue cuando el álbum ganó el galardón de mejor disco del año por el Bristish Enviroment & Media.
Tras tomar un poco de aire y descansar, 1994 fue tomado como un año para apartarse un poco, escribir y planificar el nuevo álbum. Un álbum en vivo salió a la luz a finales de año y el siguiente álbum de estudio a principios de 1995. Para la grabación del álbum en directo la banda buscó sitios imaginativos al aire libre en los que tocar durante el verano, tales como Tarlair en el Nordeste de Escocia, Colonia en Alemania y 2 noches en el Castillo de Stirling cuyas actuaciones fueron cuidadosamente grabadas. La última noche del tour del "Amazing Things" fue también incluida.

Tras una breve visita a Canadá, "Transmitting Live" fue publicado en noviembre de 1994 y fue seguido por un tour escocés, culminado en diciembre con la transmisión en vivo en Hogmanay TV el show desde Princess Street Garden, en Edimburgo.
El nuevo año de 1995 vio a Runrig de nuevo en el entramado de las grabaciones. Tras un periodo de composición y ensayos, se recluyeron en los Studios Chapel de Lincolnshire para iniciar en abril la grabación de su siguiente disco. Las sesiones de Mara continuaron durante la primavera y el verano en los más cercanos estudios de Castle Sound, cerca de Edimburgo. En un pequeño interludio de las grabaciones, se publicó el sencillo "Uhbal As Airde". Usándose como banda sonora de una anuncio televisivo de Carlsberg, atrajo la atención del gran público, consiguiendo el más alto puesto conseguido nunca de las Listas de Singles, el puesto 18º y otra aparición en el Top of the Pops. El hecho más reconfortante del éxito de la canción fue el hecho que fue la primera canción Gaélica en hacer el puesto 20º.
En junio una escapada del estudio fue aprovechada para tocar en una serie de festivales y conciertos en Europa. En particular, el primer gran concierto al aire libre ante 20.000 personas en Alemania en Loreley, a las riberas de Rhin. 

Antes de los conciertos europeos la banda tocó para Rod Steward en el Pittodrie Stadium en Aberdeen, y hacia el final de agosto, les llegó por sorpresa una invitación para talonear a los Rolling Stones, en Schuttorf, Alemania. Ese día fue particularmente significativo puesto que fueron requeridos para tocar 2 veces en la misma tarde, viajando posteriormente para hacer su propio concierto en Jubek, apoyados por Mike and the Mechanics, lo que requirió una cuidada planificación amen de un rápido vuelo en avión.
El nuevo álbum "Mara" fue publicado en otoño seguido otra mini gira por Europa y Reino Unido. El primer single "Things that Are" entró en el Top 40 y volvió a su ya habitual puesto en el Tops of the Pops. El tour de "Mara" fue el más ambicioso hasta la fecha, a lo que a producción se refiere y para muchos de los fanes fueron los shows más fantásticos hechos nunca.
Culminado y aparcado el exitoso "Mara" todo el mundo pensaba que la banda estaba en una encrucijada y era tiempo de pensar en el futuro y en sus aspiraciones individuales y colectivas. Donnie cada vez más involucrado en la Política veía su cada vez más involucración en la mismo como un proceso delante de Runrig.
Aunque todo el mundo lo suponía, nadie estaba lo suficientemente preparado para el anuncio de abandonar la banda en breve. La banda entró en el más dudoso y discontinuo periodo de su historia. No quería hacer ninguna declaración pública durante cierto período por si había cambio de planes y hasta que todo el mundo lo hubiera reflexionado lo suficiente. De alguna manera, todos creían que estaban ante el fin de la formación que Runrig había tenido desde 1986 y cuando pusieran una fecha sería una decisión consensuada por todos.
Mientras tanto la práctica del trabajo diario tenía que continuar. No importaba que vendría por delante. Estaban llegando al final de una era de la banda y junto con la compañía discográfica decidieron que era hora de lanzar un grandes éxitos: "Best of Runrig".
Habría sido imposible para la banda decidir el listado final de canciones luego pensaron que sería mejor dejarlo en manos de los fanes, cosa que hicieron a través del club de fanes. Un fascinante ejercicio de mercado: un disco que era para los fanes decidido por ellos mismos. "Long Distance" fue publicado el 7 de octubre, entrando en el puesto 13 y dando pie a un largo y gustoso tour a lo largo del otoño de 1996 y la primavera de 1997, parando a tocar una vez más en el ya considerado como el mayor concierto callejero en Princess Street Garden, Edimburgo. El primer single del álbum era una versión de del "Rhythm of My Heart" de Rod Stewart, grabado hacia el final de la sesiones de "Mara" para una posible inclusión en el film Loch Ness.

El último show de "Long Distance" fue en Bielefeld el 12 de marzo y de ahí todo el mundo retornó a su casa para poner en marcha sus aspiraciones individuales y relejar sus individualidades pospuestas. Donnie estaba fuera en la campaña de las Elecciones Generales, habiendo aceptado la nominación por el Partido Liberal donde tomó el segundo puesto de Charles Kennedy, quien casualmente era fan de Runrig.
Malcom se tomó un respiro de la música de Runrig para enfrascarse en proyectos propios mientras Rory y Calum disfrutaban del relativo lujo de trabajar con nuevo material sin la presión de fechas y objetivos específicos.Todos estaban esperando hasta que se celebraran las Elecciones Generales y se hiciera público el futuro de Donnie y la banda sintió que los primeros en ser informados debían ser los fanes y así lo hicieron a través de la revista de su club de fanes. 
Desafortunadamente con todo el interés en la prensa que había acumulado Donnie durante la campaña electoral, los medios desbordaron de especulaciones y rumores y eventualmente el Castillo Aberdeen no tuvo otra opción que bajar la verja para refugiarse del acoso mediático y se hizo público.

Eran tiempos prácticos. La salida de Donnie tenía que marcar de alguna manera y todos estaban expectantes ante el largamente prometido concierto al aire libre en verano en Escocia. Stirling fue considerado como el sitio ideal, junto con una serie de conciertos para sus fanes europeos en Dinamarca y un par de noches para sus seguidores ingleses en Mánchester. El concierto de Stirling se alargó durante 3 noches y se añadió un concierto alemán de retorno a Tanzbrunnen, Colonia donde se grabó parte del álbum en vivo "Transmitting". Los shows finales con Donnie fueron emocionantes para todos y fue la mejor manera de celebrar el final de una era en la existencia de la banda y para que Donnie diera su personal adiós.
La segunda noche fue grabada en vivo para su publicación en vídeo a finales del otoño y aunque fue muy popular no pudo capturar toda la emoción y el espíritu del show. El último show del sábado a la noche fue sin duda el más emocionante y significativo concierto de la banda hasta entonces. El invierno de 1997 llegó con la banda reducida a 5 miembros y con la ardua tarea de audicionar a cantantes para la nueva vacante creada con la gira anual de Navidades en puertas y sin nadie que les emocionara particularmente para la siguiente etapa del viaje.

1998 fue el año que marcó el 25º aniversario de la banda, y para celebrarlo la ocasión comenzaron a trabajar en un proyecto que muchos fanes habían estado pidiendo durante mucho tiempo. Una colección de temas Gaélicos en CD. Las grabaciones fueron recopiladas, remasterizadas digitalmente y publicadas en "The Gaelic Colection" con el sello Ridge Records en mayo.
La primera parte de 1998 fue empleada en más audiciones aunque con constante decepción y sin ningún resultado positivo, lo que supuso que la banda se percatara que los resultados positivos tardarían en llegar. Para entonces ya habían comenzado la grabación del siguiente álbum de estudio cuyo trabajo, desde su inicio parecía controlado y excitante. Tras todo el estrés y las dificultades de todos los involucrados en el mismo las cosas iban retornando poco a poco a la normalidad e iban encarando los cambios de manera positiva.

La larga búsqueda de un nuevo cantante llegaba por fin a su final y para mediados de julio la banda pudo anunciar que Bruce Guthro, de la isla Cape Breton, en Nueva Escocia, Canadá, iba a ser el nuevo miembro de Runrig. Bruce había llamado la atención de todos hacia el final del proceso de audición y tras oírle por primera vez supieron instantáneamente que tenían algo especial entre manos. La nueva era Runrig había comenzado. Bruce cruzó de nuevo el Atlántico para grabar 6 canciones del álbum y aunque no fuera publicado hasta marzo de 1999, los fanes estaban desesperando por oír la nueva formación en vivo. El primer concierto se confirmó para ser en el Tonder Music Festival en Dinamarca a finales de agosto, en una corta aparición. El tour "Next Stage" se planeó para otoño e invierno. Increíblemente sonaba como si, tras mucha incertidumbre, la banda pudiera entrar en el milenio recargada, fresca y con mucho más entusiasmo que nunca antes.

El primer show de Bruce fue un poco nervioso pero su popularidad y aceptación sería completa durante los conciertos del corto "Next Stage". Guthro fue totalmente arrollado por el calor y el apoyo recibido por los fanes de Runrig. El álbum, "In Search of Angles" se publicó a primeros de marzo de 1999 y vio gustosamente el retorno de Runrig al sello Ridge Records en Reino Unido y copó el puesto 26 en las lista nacionales. El sencillo "The Message" se publicó una semana antes del álbum y aunque fue pinchado mucho fue relegado de las listas de singles porque el rejuvenecido sello Ridge Records no se había percatado que las reglas de la industria en cuanto al formato sencillo habían cambiado. Se habían incluido demasiadas caras-B.

El tour del álbum se inició con 2 noches en el Glasgow Royal Concert Hall antes de emprender el circuito habitual. El segundo single "Maymorning" se hizo coincidir con las elecciones generales de l nuevo parlamento escocés el 6 de mayo y fue elegido por la televisión escocesa para ilustrar la cobertura informativa a dichas elecciones durante la campaña. El tour del álbum continuó durante todo el verano con una serie de conciertos al aire libre y festivales en Dinamarca y Alemania.
Por entonces el milenio que se avecinaba parecía estar exclusivamente en las mentes de la gente. La banda parecía ser el candidato natural como amenizador de Hogmanay. Se consideraron muchas ofertas pero esta parecía ser la más apropiada de la audiencia de Inverness, capital de las Highland.

Enero vio a la banda tocar por primera vez en el ya prestigioso festival de Folk Celtic Connection". El show fue todo un éxito y fue grabado en vivo para publicarse en un CD: "Live at Celtic Connection. Esta, junto con la publicación de un vídeo directo en Bonn, dio a muchos fanes la ocasión de ver a Bruce con la banda en vivo y la oportunidad de ver y oír el nuevo sonido del grupo. Para entonces la nueva era de Bruce en Runrig ya se había establecido firmemente y la era previa se pudo evocar en diciembre con la publicación de "Flower of the West", "The Runrig Songbook": una completa colección de todas las canciones de Calum y Rory que habían aparecido en los álbumes de Runrig en los anteriores 25 años.

El camino estaba libre para la próxima grabación: el disco número 16, 9º en estudio. Puesto que Bruce había arribado a la mitad de la grabación del disco anterior, "Angels", era bueno comenzar un proyecto desde el principio. "The Stamping Ground", era un retorno a las raíces, un toque tradicional, reafirmando que la banda siempre intentaba tomar caminos musicales contemporáneos y que pudiera entrar en la radio del siglo XXI.
El proceso comenzó con una sesión de tambores étnicos en Ca.Va., en Glasgow seguido de una quincena de reclusión en los estudios Lundgaard, en Dinamarca, volviendo de nuevo a los estudios madre en Escocia en los siguientes meses.
Rompiendo el carácter del grupo aceptaron la entrada desde Brasil del fusionista céltico Paul Mounsey, y con el trabajo de producción del ingeniero de sonido danés Kristian Gislason. El álbum fue publicado en la primavera del 2001 con un éxito improcedente entre sus fanes; para la opinión de muchos, el mejor álbum de Runrig hecho nunca.

Fue aquí que la banda dio otra vuelta de tuerca en lo personal resultado de una nueva incursión en la Política. El teclista Peter Wishart, se presentó como candidato por el Partido Nacional Escocés por North Tayside en las Elecciones Generales. Tuvo éxito y dejó la banda una vez más enredada en audiciones para sustituirle. Afortunadamente no fue tan traumático en esta ocasión. Un joven y avezado músico llamado Brian Hurren fue descubierto justo al término de su graduación en la Perth Music College. 
El tour de "Stamping Ground" se inició con 2 noches en el marco de la Isla de Skye y continuó por 6 semanas. En Alemania la banda tocó en el legendario show televisivo Geld Oder Lieben atrayendo la que sin duda sería la más alta audiencia conseguida nunca por la banda ante 5 millones de espectadores. Como resultado de esta actuación, el tour alemán agotó todas sus entradas y el álbum alcanzó el puesto 20 en las listas alemanas.

De vuelta a Reino Unido, los sencillos "Book of Golden Stories" y "Wall of China" consiguieron más tiempo en la radio que cualquiera de sus anteriores grabaciones. A lo largo del verano el tour "Stamping Ground" continuó con varios festivales y volvió a Alemania en diciembre con el tour Whisky and Gluhwein Christmas donde se filmó un concierto en Colonia para la serie Rockpalast TV transmitido luego en televisión e Internet.
Con la nueva banda ganando confianza en vivo estaba claro que la nueva era de Runrig había evolucionado. Con la mente puesta en el 2003 y el 30 aniversario de la banda, se decidió entrar rápidamente en el estudio. Partiendo de la esencia del éxito de "Stamping Ground" con el ingeniero Kristian Gislason en el estudio Arhus, Dinamarca, comenzó el proceso en los inicios del 2002. Este iba a ser un pilar esencial y mucho se movía alrededor, máxime reconociendo los 30 años del grupo. Esto les remitió de nuevo a la influencia del músico brasileño Paul Monsey. Tras el éxito de "Running to the Light", en "Stamping Ground", Paul retornó para poner su sello en los arreglos y producción. Como parte del proyecto rearregló 2 canciones clásicas de la banda del "Recovery" de 1981.

"Proterra" se grabó en 12 estudios diferentes a los largo del mundo entre el 2002 y la primavera del 2003. Una excitante anécdota durante el proceso de grabación sucedió cuando en febrero de 2003 les fue presentado a la banda un CD recuperado intacto del accidente de la misión espacial Columbia. La tragedia conmocionó a los componentes de Runrig ya que la astronauta Dr. Lauren Clark había sido durante mucho tiempo fan del grupo y había contribuido al diario de la misión puesto que ella había usado la canción Runnig to the Light para despertar a sus compañeros en el centro de control de Houston.
Fue el marido de Lauren quien se presentó a la banda con el CD enmarcado. Fue una emocionante experiencia que apartó a todos por un rato de la música. 

Con el aniversario del 2003 en puertas, todas las esferas se concentraban en dos pilares: la publicación de "Proterra" y el concierto aniversario en la explanada del Castillo de Stirling, antes de finales de agosto. A lo largo de todo el circuito de festivales veraniegos la banda tocó un gran número de canciones antiguas: todo llevaba a Stirling. Iba a ser la más grande ocasión con significado personal para todo aquel involucrado en la marcha del grupo. Concierto que se grabaría en un nuevo disco "Day of Days", junto con un vídeo que se publicarían conjuntamente. Ültimo disco por el momento.




</doc>
