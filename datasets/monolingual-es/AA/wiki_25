<doc id="4532" url="https://es.wikipedia.org/wiki?curid=4532" title="Cerveza">
Cerveza

La cerveza (del latín "cerevisĭa") es una bebida alcohólica, no destilada, de sabor amargo, que se fabrica con granos de cebada germinados u otros cereales cuyo almidón se fermenta en agua con levadura (principalmente "Saccharomyces cerevisiae" o "Saccharomyces pastorianus") y se aromatiza a menudo con lúpulo, entre otras plantas. Es la bebida alcohólica más consumida del mundo, y una de las bebidas más consumidas, solo por detrás del agua y el té-

De ella se conocen varios tipos con una amplia gama de matices debidos a las diferentes formas de elaboración y a los ingredientes utilizados. Generalmente presenta un color ambarino con tonos que van del amarillo oro al negro pasando por los marrones rojizos. Se la considera «gaseosa» (contiene CO disuelto en saturación que se manifiesta en forma de burbujas a la presión ambiente) y suele estar coronada de una espuma más o menos persistente. Su aspecto puede ser cristalino o turbio. Su graduación alcohólica puede alcanzar hasta cerca de los 30 % vol., aunque principalmente se encuentra entre los 3 % y los 9 % vol.

Según Anderson y Hull, «el lúpulo da a la cerveza ese sabor límpido y amargo, sin el que malamente puede llamarse cerveza». En la Baja Edad Media se originó la costumbre de cocer el mosto con flores de lúpulo. A partir de entonces nació la bebida que hoy identificamos como cerveza, distinta del vino de malta. Tal costumbre se originó en Alemania hace unos mil años. El lúpulo sustituyó a los aromatizantes hasta entonces utilizados, dando a la cebada fermentada alcohólicamente su amargor característico. El lúpulo contribuye también decisivamente a su conservación. Además obra como eficaz antiséptico y estabilizador. También sirve para detener la fermentación acética y clarificar el líquido, causando la precipitación de las sustancias albuminosas. Los primeros testimonios que tenemos sobre el uso del lúpulo se remontan a la Alemania del siglo XI, con motivo de los impuestos por el uso del allí llamado "grut", que en inglés llaman "gruit" —el conjunto de yerbas utilizadas en la elaboración de la cerveza— que fue sustituido por el lúpulo.

Sin el uso del lúpulo, el fermentado proveniente de la cebada no pasa de ser un vino de malta —que no lleva lúpulo y, si lo lleva, no puede ser fresco—, que recuerda por su sabor más al vino que a la cerveza. Cuanto menos lúpulo se usa, la bebida resulta más vinosa. Si la malta está muy tostada no hace falta usar tanto lúpulo para evitar el sabor vinoso. En francés el vino de malta es llamado "vin d’orge", en inglés "barley wine", en alemán "Gerstenwein" y "Maltonwein" y en italiano "vino d’orzo". Sabe a vino, se sirve en copa de vino, tiene una graduación similar a la del vino y los mismos usos que el vino. Se distinguen incluso vinos de malta de mesa y de postre. No se los considera cerveza, aunque los famosos vinos de malta de Bélgica —aderezados con frutas— suelen ser incluidos al tratar de las cervezas, en calidad de «cervezas especiales». Tritton señala en su manual que, para elaborar cerveza en vez de vino de malta, basta añadir lúpulo y un fermento de los usados para elaborar cerveza. En el caso del vino de malta, se suprime el lúpulo y se utiliza fermento de vino en vez de fermento de cerveza. El lúpulo identifica tanto o más la individualidad de la cerveza, que la cebada u otros cereales. Tampoco tienen la consideración de cerveza, ni se llama cerveza, pues carece de lúpulo, el fermentado alcohólico, de unos 7 % vol. del que se extrae por destilación el whisky.

Además del vino de malta, existen otras bebidas alcohólicas con características o apariencia diferentes pero fabricadas también a base de almidón fermentado que, cuando no tienen un nombre específico —como es el caso del "sake"—, son asimiladas a cervezas. En este último caso se añade un complemento al nombre de «cerveza» a fin de evitar malentendidos —por ejemplo, cerveza de banana—. La cerveza sin alcohol es un caso especial ya que su contenido alcohólico es despreciable o nulo, aunque comparte las mismas características de base que el resto de las cervezas porque se ha desalcoholizado durante la elaboración.

Se podría clasificar el "sake" como cerveza de arroz —aunque hay varias diferencias— si se adoptase un criterio analógico. La cerveza es para los europeos lo que el "sake" para los japoneses. En sentido analógico, la cerveza también puede ser clasificada como un sake. Las clasificaciones analógicas suelen rechazarse científicamente por poco rigurosas, pues no distinguen adecuadamente el género de la especie. Si denominamos a todos los mamíferos «vacas», posteriormente hay que distinguir entre vacas «propiamente dichas» y otros animales que sólo son vacas por asimilación. No existe una palabra para designar a todas las bebidas provenientes de cereales alcohólicamente fermentados. Para el inglés, Harold J. Grossman ha propuesto "brews" y "malt beverages".

En Japón la cerveza, tal y como se conoce en Occidente, fue inicialmente un producto importado. Hoy en día existen fábricas de cerveza japonesas y para designar dicha bebida se adaptó la locución "bier" a dicho idioma como "biiru" (ビール). Aunque para hacer cerveza se utiliza muchas veces arroz, no solo la elaboración es distinta, sino también la fermentación. En la tradición oriental, en la fermentación alcohólica del arroz, el sorgo o el mijo, el fermento utilizado proviene de esos mismos cereales, y está basado en las esporas del "Aspergillus Orizae", un hongo asexuado. Produce la enzima llamada "takadiastasa". Ese fermento se llama "koji". Es palabra de origen japonés, pero que se utiliza en cualquier idioma, si se quiere designar ese fermento. El "koji" no incluye sólo el "Aspergillus Orizae", relativo al arroz, sino también otros como el "A. sojae" relativo a la soja. Tiene la virtud de hacer fermentar en alcohol no solo la sacarosa, sino también la lactosa. En la obtención de esas bebidas no se tuesta el cereal. También es distinta en consecuencia la preparación del "wort". En una cultura cervecera en la que se efectúan clasificaciones tan sutiles como la distinción entre "ale" y "beer" sería muy difícil clasificar el "sake" como una "ale" o como una "beer".

A diferencia de las bebidas obtenidas a partir de zumos de frutas fermentados, como los vinos, en la cerveza el cereal de base no contiene originalmente ni agua ni azúcar, caracterizando ambas carencias el proceso de elaboración. Para conseguir azúcar a partir del almidón del cereal, es necesario primero modificarlo mediante el malteado y sumergirlo en agua a la temperatura adecuada a fin de completar la conversión. El líquido resultante, compuesto de azúcares, proteínas y residuos procedentes del cereal, se filtra, se hierve vigorosamente y se le añade el lúpulo en caliente, aunque también existe la costumbre de lupular en frío —"dry hopping", «en seco», en inglés—, operación que consiste en añadir las flores al mosto ya frío, bien en las cubas de fermentación, bien en las cubas de almacenamiento. Una vez enfriado a una temperatura que permita el desarrollo de las levaduras, se añaden éstas y se inicia la fermentación que producirá el alcohol y el dióxido de carbono (CO).

Existen varias opiniones:

En todo caso, la raíz común es fácilmente apreciable en sus voces española «cerveza», portuguesa "cerveja", catalana "cervesa", gallega "cervexa", extremeña "cervécia" y retorrománica "gervosa". En otros idiomas europeos se emplean derivados de la misma raíz que la palabra germánica "bier", como es el caso del inglés "beer", francés "bière" e italiano "birra". En inglés también se utiliza la palabra "ale", equivalente a "öl", que es la palabra escandinava para cerveza. Charlie Papazian sostiene que "ale" significaba originariamente hidromiel —"mead" en inglés— muy rebajado con agua, mientras el hidromiel fuerte era denominado "biuza", de donde "beer".

Históricamente la cerveza fue desarrollada por los antiguos pueblos elamitas, egipcios y sumerios. Las evidencias más antiguas de la producción de cerveza datan de alrededor de IV milenio a. C. fueron halladas en Godin Tepe, en el antiguo Elam (actual Irán). Algunos la ubican conjuntamente con la aparición del pan entre 10 000 a. C. y 6000 a. C. ya que tiene una parecida preparación agregando más o menos agua. Parece ser que las cervezas primitivas eran más densas que las actuales, similares al actual "pombe" africano, de culturas igualmente primitivas. Según la receta más antigua conocida, el Papiro de Zósimo de Panópolis (siglo III), los egipcios elaboraban la cerveza a partir de panes de cebada poco cocidos que dejaban fermentar en agua. Su cerveza fue conocida como "zythum", que es palabra griega, pero en una fase más tardía. Antiguamente en Oriente se usaba arroz y también bambú. Del bambú, lo mismo que de la caña de azúcar, lo que se fermenta es su savia; pero no su fruto. Tal es el "ulanzi" propio de Tanzania. No puede ser considerado un fermentado alcohólico de cereal. Las bebidas alcohólicas más antiguas quizá sean derivadas de la leche.
Michael Jackson, en su "Michael Jackson's Beer Companion", recoge la opinión del profesor de la Universidad de Pensilvania Salomon Katz, que data la aparición de una bebida de cebada fermentada alcohólicamente en la Mesopotamia del año 4000 a. C. con el nombre de "sikaru", pero señala que se hacía con pan de cebada; es decir, se trataba de lo que hoy llamamos "kuas", que no es considerado propiamente cerveza, aunque es un fermentado alcohólico proveniente de cereal. La cerveza propiamente dicha aparece en Europa en el siglo XIII, en la medida en que el concepto de cerveza incluye el amargor propio del lúpulo. El malteado ya se había inventado antes. En el primer capítulo de sus "Études sur la bière", Pasteur hace notar que cuando se dice que en el siglo IV a. C. ya Teofrasto hablaba de «cerveza», en realidad no hablaba de cerveza, ni de "cervoise", ni de "beer", sino de vino de cebada, de οίνος εκ κριθεόν. Atribuir un origen muy antiguo a la cerveza se hace sobre la base de proporcionar un concepto muy amplio de lo que haya de entenderse por cerveza.

Los restos arqueológicos más antiguos de producción de cerveza en Europa fueron descubiertos en 1999 en el yacimiento de la Cova de Can Sadurní en el término municipal de Begas (Barcelona, España) los restos hallados eran del neolítico en una estratificación de entre 5500 a. C.-4000 a. C., por Manel Edo Benaiges, Pepa Villalba Ibáñez y Anna Blasco Olivares, de la Universidad de Barcelona (UB). Sin duda alguna este hallazgo desplazó el que hasta ese momento se creía como más antiguo descubrimiento de elaboración de cerveza en Europa en el yacimiento del valle de Ambrona, dentro del término municipal de Miño de Medinaceli, (Soria, España) y que databan de alrededor de siglo XXV a. C., según el trabajo arqueológico del equipo dirigido por el profesor Manuel Ángel Rojo Guerra, de la Universidad de Valladolid. También se han encontrado evidencias arqueológicas de elaboración de cerveza en el yacimiento de Genó, en Aitona (Lérida, España), tras los trabajos de investigación arqueológica, dirigidos por el profesor José Luis Maya González, que han establecido que estos restos arqueológicos databan de alrededor de siglo XII a. C.

Los celtas conocían la elaboración de la cerveza y llevaron consigo este conocimiento cuando se extendieron por la península ibérica, donde su uso y su elaboración se desarrolló muy pronto.

Con el paso de los siglos, sobre todo a partir de la romanización, la mediterránea se consolidó como una zona básicamente vinícola mientras que la cerveza se producía en el norte y centro de Europa y adquiría la forma de lo que entendemos hoy por cerveza. De esta manera, se extiende el uso de la malta como ingrediente principal y también se empieza a introducir el uso del lúpulo como aromatizante. Esta planta cannabáceas confiere a la cerveza su sabor amargo característico, a la vez que favorece la conservación.

El año 1516, el duque Guillermo IV de Baviera redactó la primera ley que fijaba qué se entendía por cerveza. Esta ley de pureza ("Reinheitsgebot") establecía que solamente podía utilizarse agua, malta de cebada y lúpulo para elaborar la cerveza. En cambio, en Inglaterra, Enrique VIII prohibió el uso del lúpulo, ante la presión del gremio de cerveceros; prohibición que levantó su hijo Eduardo VI, y que continuó por algún tiempo más en Escocia. Los cerveceros ingleses tardaron mucho en aceptar el uso del lúpulo. En su momento se llamó "ale" a la cerveza sin lúpulo y "beer" a la cerveza con lúpulo. Todavía hoy, para designar los vinos de malta sin lúpulo más que de "barley wine", que simplemente puede designar una cerveza de alta graduación, se habla de "gruit ale".

La cerveza empezó a recuperar su presencia social en España a partir del reinado del emperador Carlos I, que trajo consigo maestros cerveceros de Alemania. Todo ello queda reflejado entre las pertenencias del emperador a la muerte de éste en Yuste por su Secretario Martín de Gaztelu. Por aquel entonces, la cerveza era aún un producto de temporada. No se sabía conservar y con el calor perdía toda su fuerza. La cerveza llamada "lager", sin embargo, recibe ese nombre en razón de su posibilidad de almacenamiento. Se elaboraba en otoño, para ser consumida en primavera. La fermentación baja y a baja temperatura favorece la conservación. En realidad iba fermentando lentamente mientras estaba almacenada. Actualmente todas las cervezas, incluso las de alta fermentación, son almacenables y llevan fecha de caducidad que alcanza unos tres años. "Lager" ha sufrido un cambio semántico, y ha pasado a significar cerveza de fermentación baja.
La posibilidad de conservación de la cerveza se debe no tanto al invento de las neveras eléctricas, como al de conservantes distintos del lúpulo, y a la posibilidad de elaborar a gran escala y con facilidad envases herméticamente cerrados. Las botellas industriales hechas en serie aparecen en el siglo XIX. Antes se fabricaban a soplete. La cerveza enlatada comienza en 1933 en Estados Unidos, tras la abolición de la ley seca. Los barriles de cerveza en madera han desaparecido prácticamente. No se puede hablar de una verdadera industria cervecera hasta el siglo XIX, cuando empiezan a aparecer pequeñas fábricas más que artesanales ya industriales. 

La primera gran fábrica de cerveza en España fue abierta en 1864 por el alsaciano Louis Moritz en Barcelona. Y la siguieron marcas como La Salve Bilbao (1886), Mahou (1890), La Zaragozana (1900), Cruzcampo (1904) o Estrella Galicia (1906).

La elaboración de la cerveza se puede hacer con cualquier cereal que pueda producir azúcares fermentables. Para ello debe ser preparado para que la gran mayoría de sus azúcares sean fermentables. En algunos casos una simple cocción es suficiente (como en el caso del maíz) y en otros casos es preciso «maltear» el cereal. En la elaboración de la cerveza se utilizan numerosos cereales en su estado crudo o malteado, siendo la cebada el único que debe maltearse necesariamente y el más utilizado en la cervecería occidental.

Los azúcares que contiene el grano de cebada no son inmediatamente accesibles y, en una fase previa, es preciso activar unas enzimas presentes en el propio grano que reducirán las largas cadenas de almidón para liberar azúcares. Esta operación, también denominada malteo o malteado, consiste simplemente en hacer germinar los granos. Cuando se estima que la activación enzimática de la germinación se encuentra en su punto óptimo, se para el proceso reduciendo la humedad del grano hasta su mínimo. Este producto recibe el nombre de "malta verde". Después hay que hornearlo. A bajas temperaturas, el tostado es mínimo y se habla de "maltas claras" (llamadas también maltas "lager" o "pale" según el país en que se producen). A medida que se aumenta la temperatura del horno, la malta resultante es cada vez más oscura. Se puede llegar al punto de quemarla, produciendo «malta negra». El grado de tostado de la malta determina el color de la cerveza. Los demás cereales se pueden utilizar malteándolos previamente, aunque solamente es indispensable hacerlo en el caso de la cebada. Con los demás cereales, el malteado sirve para conseguir aromas diferenciados o efectos técnicos concretos.

Mezcla se refiere a la masa de grano que se utilizará para elaborar el mosto. Puede ser de un único tipo de malta o el resultado de una mezcla de maltas, o de maltas y grano crudo. Las proporciones y los componentes de esta mezcla son básicos para determinar el tipo o estilo de cerveza que se quiere producir.

Los diversos cereales que se utilizan para la cervecería presentan cada uno variedades botánicas que multiplican las posibilidades de elección del elaborador. Actualmente pueden encontrarse en el mercado hasta 60 tipos diferentes, cifra que aumenta considerablemente si tenemos en cuenta el malteo casero. Básicamente los cereales se distinguen en cuatro categorías:

La calidad de los cereales, sus variedades, y la calidad del proceso de malteo definen en gran medida la calidad de la cerveza. Las bebidas alcohólicas hechas de la fermentación de azúcares obtenidos de otras fuentes generalmente no se llaman cerveza, a pesar de ser producido por un proceso similar a la reacción bioquímica de la levadura. Como ejemplos, el zumo de manzana fermentado se llama sidra, el jugo fermentado de la pera se llama perada, y el jugo de uva fermentado se llama vino.

Actualmente, en la elaboración occidental de la cerveza, el aditivo principal que se utiliza para hacer de contrapeso (de equilibrante si se prefiere) al dulzor de la malta es el lúpulo ("Humulus lupulus"). De esta planta se utiliza sin fecundar la flor hembra, llamada “cono”, salvo en Inglaterra. Flores masculinas y femeninas crecen en plantas distintas, por lo que es usual suprimir las masculinas, con lo que se obtienen inflorescencias femeninas sin semillas. En Inglaterra, sin embargo, es costumbre tener un lúpulo masculino por cada doscientos femeninos, con lo que los “conos” tienen semillas. Ello parece proporcionar mayor resistencia a las plantas.

En la base de sus bracteolas, hay unas glándulas que contienen la "lupulina", que es el ingrediente que aportará a la cerveza su sabor amargo y los aromas propios. Del amargor son responsables los ácidos amargos y los aromas proceden de aceites elementales constituidos en especial por compuestos bastante volátiles y delicados a base de ésteres, y de resinas. Existen numerosas variedades botánicas del lúpulo que son objeto de investigaciones intensas. El lúpulo es la causa de la estimulación del apetito que produce la cerveza. Para su comprensión, también se clasifican en categorías:

Estos lúpulos son los que aportan más ácidos amargos que aromas. Los representantes más conocidos de esta categoría son el "brewer's gold" y el "northern brewer" o "nordbrauer".
Lógicamente, éstos aportan más elementos aromáticos que amargos. En este apartado se conocen especialmente el "saaz/zatec" que definen el estilo "pilsner" de cerveza, el "spalt" y el "tettnang" en el área alemana, y los "golding" y "fuggler" en el área anglófona.

El lúpulo es muy delicado, solamente se puede utilizar fresco durante los pocos meses de su cosecha, que coincide con la de la viña: finales de agosto a octubre según las variedades y el sitio. Fuera de este intervalo temporal se tiene que condicionar, de manera que el mercado presenta diversas formas que van desde el lúpulo deshidratado hasta extracto de lúpulo. Lógicamente, en cada manipulación se van perdiendo características y no es lo mismo utilizar un lúpulo fresco o congelado que un aceite de concentrado de lúpulo. El efecto organoléptico sobre la cerveza es muy diferente. La variedad y el frescor del lúpulo influyen muy sensiblemente en la calidad final de la cerveza.
Las formas de uso son en extracto, "pellet" o en polvo; aunque la forma más habitual es en "pellet".

El lúpulo puede adquirirse y usarse en forma de “pellets”, palabra inglesa utilizada en la jerga cervecera, que significa “pildorita”. Las hay de dos clases: para impartir amargor y para impartir aroma. Los pellets tienen la ventaja de evitar la rápida degradación propia de las flores. Las grandes fábricas utilizan extracto de lúpulo, que apenas tiene aroma; pero la gran masa de bebedores no es consciente de ello. También utilizan en ocasiones para dar aroma extractos de esencias de aceites. Tienen el inconveniente de que en mayor o menor cantidad contienen “mirceno”, que también proporciona olor desagradable. Esos extractos se añaden inmediatamente antes del embotellado.

Al margen del lúpulo, la historia recoge numerosos aditivos botánicos. Hoy en día podemos citar los siguientes:


Entre el 85 y 92 % de la cerveza es agua.

Aparte de las características bacteriológicas y minerales de potabilidad, cada tipo o estilo de cerveza requerirá una calidad diferente de agua. Algunas requieren de agua de baja mineralización, otras necesitan aguas duras con mucha cal. Actualmente, prácticamente ya no se hacen cervezas tal y como fluyen. Casi todas las cervecerías tratan las aguas de manera que siempre tenga las mismas características para una misma receta de cerveza.

Entre los minerales del agua que más interesan a los cerveceros están el calcio, los sulfatos y los cloruros. El calcio aumenta la extracción tanto de la malta como del lúpulo en la maceración y en la cocción y rebaja el color y la opacidad (o lo turbia que es) de la cerveza. El cobre, el manganeso y el zinc, inhiben la floculación de las levaduras. Los sulfatos refuerzan el amargor y la sequedad del lúpulo. Los cloruros dan una textura más llena y refuerzan la dulzura.

Actualmente, se consumen aproximadamente 3Hl de agua por cada Hl de cerveza producido. Por esta razón, la tendencia es reducir el consumo de agua.

La mayoría de los estilos de cerveza se hacen usando una de las dos especies unicelulares de microorganismos del tipo "Saccharomyces" comúnmente llamados levaduras, hongos que (como indica su nombre) consumen azúcar y producen alcohol y anhídrido carbónico. Existen dos tipos básicos diferentes de levadura que definen los dos grandes grupos estilísticos de cervezas:

En la elaboración de la cerveza, especialmente en las llamadas de fermentación espontánea, también pueden intervenir otras levaduras. En estas cervezas el elaborador no selecciona ninguna levadura sino que permite que todas las levaduras en suspensión en el aire se introduzcan en el mosto. Esas levaduras producen una fermentación tumultuosa similar a la del vino, no localizada particularmente ni en la parte alta ni en la baja del recipiente. Se instalan, aparte del "Saccharomyces", más de 50 fermentadores diferentes entre los cuales hay que citar el "Lactobacillus" (es una bacteria), que produce el ácido láctico, y el "Brettanomyces", que produce el ácido acético. Estas cervezas son pues ácidas por definición, y su elaboración requiere procedimientos especiales destinados a rebajar la acidez.

La tradición cervecera desapareció de la península ibérica probablemente con la introducción del cristianismo. De manera que el español no posee un lenguaje especializado de elaboración. Es por esto que en algunas ocasiones, se pondrá entre paréntesis la expresión alemana o inglesa para algún proceso o etapa.


Existen diversos criterios de clasificación de las cervezas. Las diversas asociaciones y los expertos se pusieron de acuerdo en los años 1970 para elaborar una clasificación de las cervezas basadas en estos criterios y en las descripciones de los propios elaboradores.

Habitualmente se suele indicar con qué grano se ha elaborado la cerveza cuando no ha sido elaborada exclusivamente con malta de cebada: cerveza de trigo, de avena, etcétera. En la mayoría de los casos se trata de una mezcla de malta de cebada y del grano indicado. No se suele indicar con qué lúpulo está hecha la cerveza, pero existe un estilo particular que se define por el uso de uno en particular: se trata de la cerveza "Pils" o "Pilsener", que originalmente tenía que hacerse con cebadas de Moravia y lúpulos de "Žatec" (o "Saaz") de Bohemia. También se pueden llamar "Pils" a algunas imitaciones históricas alemanas elaboradas con cebadas y lúpulos muy parecidos a la "Pils" original.

Muchas cervezas reciben el distintivo de su color: cerveza ámbar, roja, rubia, negra. Otras vienen definidas por su transparencia: cervezas turbias o translúcidas. Normalmente, la translucidez de una cerveza puede ser debida a las proteínas en suspensión, procedentes del grano (menos de cebada), o bien puede ser debida al hecho de ser poco o no haber sido filtrada y llevar levadura en suspensión. Las cervezas negras son llamadas así por el uso que se hace en la receta de maltas tostadas o quemadas. Algunas cervezas negras especialmente robustas son nombradas normalmente "stout" («robusto» en inglés).

Algunas cervezas se definen por algún procedimiento particular: la "Rauchbier" (cerveza ahumada) está hecha con maltas que se han tostado dejando que el humo de la leña impregne en grano. La "Dampfbier" o "Steambeer" vienen definidas por el uso de maquinaria de vapor en su elaboración. No son exactamente estilos pero se definen de esta forma. Algunas cervezas de Alemania, en invierno, eran servidas calientes y además se solía mojar una barrita de hierro ("Stachel") al rojo para aumentar la temperatura y caramelizar algunos azúcares: "Stachelbier". Este procedimiento también se ha descrito en Irlanda. La "Steinbier" es una especialidad en la que se calienta el mosto lanzándole piedras ("Stein") muy calientes.
Muchas cervezas se definen por su lugar de origen o por una denominación de origen controlada. Es preciso hablar en especial de las cervezas de abadía, que suelen recibir su nombre y su denominación por su relación, no siempre evidente ni directa con algún cenobio. El ejemplo más conocido es el de las cervezas "Trappistes" dependientes exclusivamente de monasterios de este orden. Estas cervezas suelen ser densas y con un notable contenido en alcohol. Existen dos denominaciones de origen: la "bière de garde" del Norte de Francia, y la Kölsch que sólo se puede elaborar en Colonia. También son muy características las cervezas regionales, como lo son las cervezas alemanas o las cervezas artesanales belgas (la Bush, La Binchoise, la De Coninck, etc.).

Sin embargo, a diferencia de lo que sucede con el vino, la comercialización de las cervezas no está basada en un sistema de “denominaciones de origen”, a las que se asigna una “región determinada” que es la única en la que se permite fabricar el correspondiente producto y la única que tiene derecho a usar esa “denominación de origen”. Las cervezas adoptan por nombre marcas comerciales registradas en los usuales registros de patentes y marcas. Muchas marcas de cerveza son famosas. Una gran fábrica encarga en distintos países la elaboración de una misma cerveza, sobre la base de exigirles una muy concreta receta. La producción no está ligada a una “región determinada”. El consumidor bebe “la misma cerveza” independientemente de que esté elaborada en Japón, en España o en Burdeos.

Esta es la clasificación más sencilla. Según el tipo de fermentación las cervezas se dividen en dos grandes grupos, "lager" de baja fermentación, y "ale" las de alta fermentación, en las que se incluyen también las de fermentación espontánea.

No obstante, se trata de una división demasiado genérica, por lo que normalmente se denominan "lager" las cervezas que no tienen ninguna otra característica especial. Por lo general, las cervezas "lager" son ligeras, claras, con bastante gas y una graduación moderada. También suelen ser muy refrescantes. Las "ale", en cambio, son menos habituales, al menos en el mediterráneo, aunque en Reino Unido y el centro de Europa son las más populares. Se trata de cervezas más oscuras, espesas y con poco gas. Suelen tener mayor graduación y un sabor mucho más intenso, en el que se nota más el cereal. El nombre "ale" suele aplicarse únicamente a las cervezas inglesas, mientras que el resto suelen adoptar su denominación en función de otras propiedades.

El ser humano comenzó a cultivar los cereales entre el milenio XI a. C. y el milenio VII a. C. en la zona de Mesopotamia. Es entonces bastante probable que tanto el pan como la cerveza fuesen descubiertas al mismo tiempo ("véase": Historia del pan). Solo es una cuestión de proporciones: si se ponía más harina que agua y se dejaba fermentar, se obtenía pan; si se invertía la proporción poniendo más agua que harina y se dejaba fermentar, se conseguía cerveza. Los rastros más antiguos que atestiguan la existencia de panificación y de cervecería aparecen en Mesopotamia, pero sería ocioso buscar una filiación con procedimientos idénticos descubiertos en el resto de Europa. Dadas las circunstancias climáticas que se estaban dando tras la recesión de la última glaciación conocida en la parte de la cuenca mediterránea así como en la desembocadura del Éufrates, el delta del Nilo y otros lugares, tendemos a creer que la cerveza se descubrió o inventó en muchos lugares del Mediterráneo y de Europa de forma bastante simultánea.

Originalmente es preciso concebir la cerveza como un alimento que ofrecía dos ventajas básicas. En primer lugar, permitía un uso más comedido de un ingrediente no muy fácil de cultivar al principio. En efecto, era más fácil hacer mucha cerveza con un poco de grano que mucho pan con la misma cantidad de grano. De hecho, muchas cervezas se hicieron remojando panes fermentados, cocidos en agua y dejando fermentar la mezcla. La cerveza se chupaba con cañas para evitar encontrarse con grumos de pan. En segundo lugar, la fermentación producía alcohol y desinfectaba el agua ofreciendo así una bebida limpia de contaminación bacteriana. No en vano, en sitios como la República Checa o Baviera, se llama a la cerveza hasta hoy "pan líquido".

El fenómeno de la fermentación era concebido como un acto procedente de las divinidades con fuerte carácter mágico. Así fue como la cerveza fue concebida como bebida sagrada y placiente a los dioses. Y no son raros los textos en los que se describe una ofrenda en la que figura la cerveza como alimento sagrado.

Cuando la cerveza se produjo en grandes cantidades, también bajó sensiblemente su calidad. Así es como en muchos lugares del Mediterráneo clásico apareció la cerveza como bebida de taberna. El único lugar donde parece que la cerveza no tuvo mucho papel fue en la antigua Grecia, donde dominaba el vino. Por todo el resto de la cuenca, la cerveza fue la bebida popular y a la vez sagrada. En concreto, en Roma, en los bajos fondos, se consumía en cantidades ingentes. Y para elaborarla se tuvieron que arrancar viñas, lo que creó un importante conflicto con los adeptos del vino.

Originalmente, las cervezas se solían hacer con un cereal antecesor del trigo llamado espelta. Pero rápidamente se impusieron el trigo y la cebada en la cervecería. El trigo, más agradable en su forma sólida, fue reservado a la panificación y la cebada destinada a la cerveza. Curiosamente, ya en épocas muy remotas, la cebada no se servía cruda. Se hacían unos panes, cocidos a diferentes niveles y que se conservaban muy bien. Para hacer la cerveza, se hacía trocitos el pan y se mezclaba con agua. Después de calentar y cocer la mezcla, se dejaba fermentar unos días. Existen muchos testimonios gráficos y documentales en la región de Mesopotamia que describen cómo los consumidores usaban una caña para beber la cerveza sin encontrarse con los trozos de pan. Los egipcios comenzaron su cervecería con panes como los sumerios, pero parece ser que fueron los inventores del malteo. Y tanto en la Mesopotamia como en Egipto, se hicieron grandes cantidades de cerveza de muchos tipos diferentes identificados por su color, cosa que indica que ya controlaban el grado de torrefacción de los panes o del grano.

La cerveza tuvo una gran importancia social hasta hace poco. La nutrición de un babilonio era constituida principalmente de cerveza, grano, frutas, verdura y cebolla, dieta poco diferente de la mayoría de la gente modesta de la antigüedad. Muchos salarios se cobraban en grano o directamente en cerveza. La gente con más poder adquisitivo no cambió el consumo aunque lo sofisticó: filtraban la cerveza, haciéndola más densa (más cara). Hasta se describe cómo los pobres bebían cerveza con cañitas del río, mientras que los ricos disponían de tubos en oro para hacer el mismo servicio. Otro indicio de la importancia social de la cerveza consiste en el hecho que en aquellos países, los elaboradores de cerveza no tenían la obligación de participar en guerras. En cambio eran obligados a seguir a los ejércitos con tal de asegurarles el avituallamiento de cerveza. Como era un alimento de primera necesidad, la cerveza, a lo largo de la historia, fue objeto de codicias diversas por parte de la gente poderosa que hizo en algún caso un monopolio. También cargó el comercio con importantes impuestos o bien se establecieron leyes de uso exclusivo de algún cereal para favorecer un monopolio de dicho cereal. Se describen algunos enfrentamientos y revueltas en diversos momentos y en diversos lugares cuando esta presión se reveló insoportable.

La cerveza posee un alto contenido en vitaminas, sales minerales, proteínas, fibras, micro nutrientes y carbohidratos. Según un estudio realizado en la Universidad de Cardiff (Reino Unido), la cerveza incrementa el colesterol «bueno», mejora la coagulación de la sangre, tiene un alto valor nutricional y favorece la digestión.

El consumo moderado de bebidas fermentadas, como la cerveza, puede formar parte de una alimentación saludable como la Dieta Mediterránea actual, por las propiedades que les confieren su baja graduación y las materias primas con las que están elaboradas. Por este motivo, la Sociedad Española de Nutrición Comunitaria (SENC), incluye en la Pirámide de la Alimentación Saludable —principal referencia en materia nutricional en España— las bebidas fermentadas (cerveza, vino, cava o sidra) de forma opcional y moderada en la dieta de adultos sanos.

Al norte del Pirineo, la Edad Media fue la edad de oro de la cerveza, y producirla fue un negocio favorable que extendió la práctica hasta incluso los frailes. Pronto, se estableció un conflicto de intereses entre los elaboradores laicos que tenían que pagar impuestos de todo tipo y los elaboradores monacales que disponían de materia prima en grandes cantidades y en condiciones muy ventajosas gracias a exenciones fiscales diversas, un caso flagrante de competencia desleal. Hacia el siglo XV, los elaboradores laicos tuvieron que inventarse un nuevo tipo de cerveza, más barata, que les permitiese sobrevivir a pesar de la competencia de los frailes. Aquí radica la diferencia histórica entre la "cerevisia" de los frailes, más densa, más aromatizada, y más cara, y la "bier/beer/bière" de los laicos, menos alimenticia, más refrescante y barata, aromatizada simplemente con lúpulo.

La historia de la cerveza se puede también analizar según el ángulo de la sanidad. En efecto, ya se ha hablado de que la presencia de alcohol permite desde siempre el consumo de una bebida sin algunas bacterias corrientes como la salmonela y otros. Pero también desde muy antes, los elaboradores han añadido numerosas cosas en la cerveza. Están documentadas incluso exageraciones como el hígado de ternera. Tanto es así que desde el siglo XIV, aparecen en Alemania e Inglaterra leyes para regular aquello que se añadía a la cerveza. La culminación de todas estas leyes es la ley de pureza bávara ("Reinheitsgebot") dictada por el rey Guillermo IV de Baviera el día de San Jorge de 1516. En ella el rey determinaba que la cerveza solamente podía hacerse con agua, malta de cebada y lúpulo. Esta ley hizo desaparecer muchas recetas particulares de cerveza de los territorios donde se aplicó, especialmente de las especialidades en las que era preciso añadir algún azúcar o variar los aromatizantes botánicos. En otros países, las leyes no fueron tan estrictas y se permitieron conservar recetas en las que figuraban algunos aditivos. La ley de pureza también contribuyó notablemente a aumentar la fortuna del rey, que tenía el monopolio de la producción de cebada.

Durante el siglo XIX los cerveceros checos y alemanes inventaron y desarrollaron una cerveza que tenía que tener buen aspecto, pues se empezaba a expandir el uso de los recipientes transparentes. Se inventaron formas diversas y más eficaces de filtrar la cerveza y la hicieron más clara. Una forma de clarificar las bebidas era la de alargar considerablemente la maduración a bajas temperaturas. Así apareció la cerveza "Lager" (en alemán, «almacén, bodega») y la propia levadura de baja fermentación que fue identificada "posteriori". Actualmente, la mayoría de las cervezas industriales están hechas según este sistema. Dentro de la categoría de las cervezas "Lager", las "Pils", originarias de la localidad checa de Pilsen, están hechas con maltas de Moravia y, sobre todo, lúpulo ("Hopfen", en alemán).

Precisamente desde finales del siglo XIX la historia de la cerveza se confunde con el desarrollo de métodos que permitían la elaboración masiva de la cerveza, en detrimento muchas veces de los criterios de calidad. Hasta bien entrados los años 1970, fueron desapareciendo grandes cantidades de recetas y se fue uniformizando mundialmente la producción, principalmente de cervezas "lager" de calidad mediana a baja, al mismo tiempo que se hacen y se consumen cada vez cantidades más grandes. Aun así, algunas asociaciones de productores y consumidores especialmente ingleses, alemanes y americanos siguen exigiendo cervezas de calidad.

Precisamente en los años 1940, se puede decir que vuelve a aparecer la idea de producir cerveza casera. De hecho, el 80 % de todas las cervezas históricas son caseras o artesanales. Las mujeres europeas fueron excelentes cerveceras pero, como se ha dicho, la costumbre de hacer cerveza casera desapareció, y volvió a brotar por el interés que tuvieron los elaboradores caseros americanos para reproducir las cervezas tradicionales europeas. Hasta el punto que importantes productores de talla mediana han apostado por producir cervezas históricas y por resucitar recetas perdidas. Las asociaciones de elaboradores y consumidores desarrollaron (o propiciaron) también la degustación y la apreciación científica o profesional de la cerveza. Esta corriente pasó de nuevo el Atlántico para llegar en los años 1980 primero a Inglaterra y después al resto de países de Europa.
En la actualidad existen diversos manuales y textos para la elaboración casera de cerveza

La cerveza no tuvo una producción en masa hasta finales del siglo XVIII, no adquiriendo una relativa importancia hasta mediados del XIX. Hasta 1914 los primeros productores fueron Alemania y Gran Bretaña, a partir de entonces el primer productor fue Estados Unidos. En el período de entreguerras la producción mundial alcanzó los 250 millones de hectolitros, siendo la URSS uno de los principales productores.

España es una potencia productora de cerveza, siendo el cuarto productor de cerveza de la Unión Europea, por detrás de Alemania, Reino Unido y Polonia, con una producción de 33 millones de hectolitros en 2012 y un consumo per cápita de 47,5 litros por persona y año. A nivel mundial, se sitúa en décima posición. En 2011, último año del que se tienen datos oficiales, la facturación por venta de cerveza en España fue de casi tres mil millones de euros. En México, a partir del siglo XXI, comenzó el auge de la "cerveza artesanal". Tijuana, fue considerada como "la capital de la cerveza artesanal en México", debido a la gran cantidad de reconocimientos en premios internacionales.

Los principales estilos de cerveza son:

Fermentación baja, también conocidas como "lager"

Fermentación alta, también conocidas como "ale"





Fermentación espontánea

Cerveza sin gluten

Sus ingredientes pueden ser: Agua, cereal o pseudo cereal malteado, jarabe de maíz, flor de lúpulo, extracto de lúpulo.
Los cereales o pseudo cereales más utilizados son el trigo sarraceno, el maíz, el sorgo, la Quinoa y el arroz.
Las cervezas elaboradas con esos componentes en forma cuidadosa para evitar las contaminaciones con otras, son consideradas seguras para quienes deben llevar una dieta libre de gluten.
Hay países que producen cervezas a base de cebada que aseguran que son libres de gluten.
La certificación necesaria para ser vendido como producto libre de gluten, depende de cada país (o región). Los métodos de análisis para determinar el posible contenido de gluten de una cerveza de estos tipos, son muy controvertidos, y se discute la validez del clásico ELISA R5 para estos fines. La comunidad científica no se ha puesto aún de acuerdo en como medir el gluten en algunas cervezas, porque los procesos enzimáticos de clarificación utilizados hoy en día, cortan las cadenas de proteínas en trozos más pequeños, que dificultan su detección por los métodos más tradicionales. Hay en general acuerdo en que el método más exacto es el PCR.

El gluten es una proteína que se encuentra en los granos de trigo, cebada,
centeno y posiblemente en la avena. Ciertas personas son alérgicas al gluten y no pueden tomar cerveza normal. Véase también: Celiaquía, artículo sobre la enfermedad.

Existen varios fabricantes que comercializan cerveza sin gluten, como por ejemplo Bi-Aglut o Damm con su cerveza Daura.






</doc>
<doc id="4533" url="https://es.wikipedia.org/wiki?curid=4533" title="Circuito integrado">
Circuito integrado

Un circuito integrado (CI), también conocido como chip o microchip, es una estructura de pequeñas dimensiones de material semiconductor, normalmente silicio, de algunos milímetros cuadrados de superficie (área), sobre la que se fabrican circuitos electrónicos generalmente mediante fotolitografía y que está protegida dentro de un encapsulado plástico o de cerámica. El encapsulado posee conductores metálicos apropiados para hacer conexión entre el circuito integrado y un circuito impreso.

Los CI se hicieron posibles gracias a descubrimientos experimentales que mostraban que artefactos semiconductores podían realizar las funciones de los tubos de vacío, así como a los avances científicos de la fabricación de semiconductores a mediados del siglo XX. La integración de grandes cantidades de pequeños transistores dentro de un pequeño espacio fue un gran avance en la elaboración manual de circuitos utilizando componentes electrónicos discretos. La capacidad de producción masiva de los circuitos integrados, así como la fiabilidad y acercamiento a la construcción de un diagrama a bloques en circuitos, aseguraba la rápida adopción de los circuitos integrados estandarizados en lugar de diseños utilizando transistores discretos.

Los CI tienen dos principales ventajas sobre los circuitos discretos: costo y rendimiento. El bajo costo es debido a los chips; ya que posee todos sus componentes impresos en una unidad de fotolitografía en lugar de ser construidos un transistor a la vez. Más aún, los CI empaquetados usan mucho menos material que los circuitos discretos. El rendimiento es alto ya que los componentes de los CI cambian rápidamente y consumen poco poder (comparado sus contrapartes discretas) como resultado de su pequeño tamaño y proximidad de todos sus componentes. Desde 2012, el intervalo de área de chips típicos es desde unos pocos milímetros cuadrados a alrededor de 450  mm, con hasta 9 millones de transistores por mm.

Los circuitos integrados son usados en prácticamente todos los equipos electrónicos hoy en día, y han revolucionado el mundo de la electrónica. Computadoras, teléfonos móviles, y otros dispositivos electrónicos que son parte indispensables de las sociedades modernas, son posibles gracias a los bajos costos de los circuitos integrados.

En abril de 1958, el ingeniero alemán Werner Jacobi (Siemens AG) completa la primera solicitud de patente para circuitos integrados con dispositivos amplificadores de semiconductores. Jacobi realizó una típica aplicación industrial para su patente, la cual no fue registrada.

Más tarde, la integración de circuitos fue conceptualizada por el científico de radares Geoffrey Dummer (1909-2002), que estaba trabajando para la Royal Radar Establishment del Ministerio de Defensa Británico, a finales de la década de 1940 y principios de la década de 1950.

Recién empleado por Texas Instruments, Jack S. Kilby registró sus ideas iniciales sobre el circuito integrado en julio de 1958, demostrando con éxito el primer ejemplo integrado de trabajo el 12 de septiembre de 1958. En su solicitud de patente del 6 de febrero de 1959, Kilby describió su nuevo dispositivo como "un cuerpo de material semiconductor ... en el que todos los componentes del circuito electrónico están completamente integrados". Se trataba de un dispositivo de germanio que integraba seis transistores en una misma base semiconductora para formar un oscilador de rotación de fase. El primer cliente de la nueva invención fue el Fuerza Aérea de los Estados Unidos.

En el año 2000 Kilby fue galardonado con el por la enorme contribución de su invento al desarrollo de la tecnología.

Robert Noyce desarrolló su propio circuito integrado, que patentó unos seis meses después. Además resolvió algunos problemas prácticos que poseía el circuito de Kilby, como el de la interconexión de todos los componentes; al simplificar la estructura del chip mediante la adición de metal en una capa final y la eliminación de algunas de las conexiones, el circuito integrado se hizo más adecuado para su producción en masa. Además de ser uno de los pioneros del circuito integrado, Robert Noyce también fue uno de los cofundadores de Intel Corporation, uno de los mayores fabricantes de circuitos integrados del mundo.

Los circuitos integrados se encuentran en todos los aparatos electrónicos modernos, tales como relojes, automóviles, televisores, reproductores MP3, teléfonos móviles, computadoras, equipos médicos, etc.

El desarrollo de los circuitos integrados fue posible gracias a descubrimientos experimentales que demostraron que los semiconductor, particularmente los transistores, pueden realizar algunas de las funciones de las válvulas de vacío.

La integración de grandes cantidades de diminutos transistores en pequeños chips fue un enorme avance sobre el ensamblaje manual de los tubos de vacío (válvulas) y en la fabricación de circuitos electrónicos utilizando componentes discretos.

La capacidad de producción masiva de circuitos integrados, su confiabilidad y la facilidad de agregarles complejidad, llevó a su estandarización, reemplazando circuitos completos con diseños que utilizaban transistores discretos, y además, llevando rápidamente a la obsolescencia a las válvulas o tubos de vacío.

Son tres las ventajas más importantes que tienen los circuitos integrados sobre los circuitos electrónicos construidos con componentes discretos: su menor costo; su mayor eficiencia energética y su reducido tamaño. El bajo costo es debido a que los CI son fabricados siendo impresos como una sola pieza por fotolitografía a partir de una oblea, generalmente de silicio, permitiendo la producción en cadena de grandes cantidades, con una muy baja tasa de defectos. La elevada eficiencia se debe a que, dada la miniaturización de todos sus componentes, el consumo de energía es considerablemente menor, a iguales condiciones de funcionamiento que un circuito electrónico homólogo fabricado con componentes discretos. Finalmente, el más notable atributo, es su reducido tamaño en relación a los circuitos discretos; para ilustrar esto: un circuito integrado puede contener desde miles hasta varios millones de transistores en unos pocos milímetros cuadrados.

Los avances que hicieron posible el circuito integrado han sido, fundamentalmente, los desarrollos en la fabricación de dispositivos semiconductores a mediados del siglo XX y los descubrimientos experimentales que mostraron que estos dispositivos podían reemplazar las funciones de las válvulas o tubos de vacío, que se volvieron rápidamente obsoletos al no poder competir con el pequeño tamaño, el consumo de energía moderado, los tiempos de conmutación mínimos, la confiabilidad, la capacidad de producción en masa y la versatilidad de los CI.

Entre los circuitos integrados más complejos y avanzados se encuentran los microprocesadores, que controlan numerosos aparatos, desde teléfonos móviles y horno de microondas hasta computadoras. Los chips de memorias digitales son otra familia de circuitos integrados, de importancia crucial para la moderna sociedad de la información. Mientras que el costo de diseñar y desarrollar un circuito integrado complejo es bastante alto, cuando se reparte entre millones de unidades de producción, el costo individual de los CI por lo general se reduce al mínimo. La eficiencia de los CI es alta debido a que el pequeño tamaño de los chips permite cortas conexiones que posibilitan la utilización de lógica de bajo consumo (como es el caso de CMOS), y con altas velocidades de conmutación.

A medida que transcurren los años, los circuitos integrados van evolucionando: se fabrican en tamaños cada vez más pequeños, con mejores características y prestaciones, mejoran su eficiencia y su eficacia, y se permite así que mayor cantidad de elementos sean empaquetados (integrados) en un mismo chip (véase la ley de Moore). Al tiempo que el tamaño se reduce, otras cualidades también mejoran (el costo y el consumo de energía disminuyen, y a la vez aumenta el rendimiento). Aunque estas ganancias son aparentemente para el usuario final, existe una feroz competencia entre los fabricantes para utilizar geometrías cada vez más delgadas. Este proceso, y lo esperado para los próximos años, está muy bien descrito por la International Technology Roadmap for Semiconductors.

Solo ha trascurrido medio siglo desde que se inició su desarrollo y los circuitos integrados se han vuelto casi omnipresentes. Computadoras, teléfonos móviles y otras aplicaciones digitales son ahora partes de las sociedades modernas. La informática, las comunicaciones, la manufactura y los sistemas de transporte, incluyendo Internet, todos dependen de la existencia de los circuitos integrados. De hecho, muchos estudiosos piensan que la revolución digital causada por los circuitos integrados es uno de los sucesos más significativos de la historia de la humanidad.

Existen al menos tres tipos de circuitos integrados:


Atendiendo al nivel de integración —número de componentes— los circuitos integrados se pueden clasificar en:


En cuanto a las funciones integradas, los circuitos se clasifican en dos grandes grupos:



Algunos son diseñados y fabricados para cumplir una función específica dentro de un sistema mayor y más complejo.

En general, la fabricación de los CI es compleja ya que tienen una alta integración de componentes en un espacio muy reducido, de forma que llegan a ser microscópicos. Sin embargo, permiten grandes simplificaciones con respecto a los antiguos circuitos, además de un montaje más eficaz y rápido.

Existen ciertos límites físicos y económicos al desarrollo de los circuitos integrados. Básicamente, son barreras que se van alejando al mejorar la tecnología, pero no desaparecen. Las principales son:

Los circuitos eléctricos disipan potencia. Cuando el número de componentes integrados en un volumen dado crece, las exigencias en cuanto a disipación de esta potencia, también crecen, calentando el sustrato y degradando el comportamiento del dispositivo. Además, en muchos casos es un sistema de realimentación positiva, de modo que cuanto mayor sea la temperatura, más corriente conducen, fenómeno que se suele llamar "embalamiento térmico" y, que si no se evita, llega a destruir el dispositivo. Los amplificadores de audio y los reguladores de tensión son proclives a este fenómeno, por lo que suelen incorporar protecciones térmicas.

Los circuitos de potencia, evidentemente, son los que más energía deben disipar. Para ello su cápsula contiene partes metálicas, en contacto con la parte inferior del chip, que sirven de conducto térmico para transferir el calor del chip al disipador o al ambiente. La reducción de resistividad térmica de este conducto, así como de las nuevas cápsulas de compuestos de silicona, permiten mayores disipaciones con cápsulas más pequeñas.

Los circuitos digitales resuelven el problema reduciendo la tensión de alimentación y utilizando tecnologías de bajo consumo, como CMOS. Aun así en los circuitos con más densidad de integración y elevadas velocidades, la disipación es uno de los mayores problemas, llegándose a utilizar experimentalmente ciertos tipos de criostatos. Precisamente la alta resistividad térmica del arseniuro de galio es su talón de Aquiles para realizar circuitos digitales con él.

Este efecto se refiere principalmente a las conexiones eléctricas entre el chip, la cápsula y el circuito donde va montada, limitando su frecuencia de funcionamiento. Con pastillas más pequeñas se reduce la capacidad y la autoinducción de ellas. En los circuitos digitales excitadores de buses, generadores de reloj, etc, es importante mantener la impedancia de las líneas y, todavía más, en los circuitos de radio y de microondas.

Los componentes disponibles para integrar tienen ciertas limitaciones, que difieren de sus contrapartidas discretas.


Durante el proceso de fabricación de los circuitos integrados se van acumulando los defectos, de modo que cierto número de componentes del circuito final no funcionan correctamente. Cuando el chip integra un número mayor de componentes, estos componentes defectuosos disminuyen la proporción de chips funcionales. Es por ello que en circuitos de memorias, por ejemplo, donde existen millones de transistores, se fabrican más de los necesarios, de manera que se puede variar la interconexión final para obtener la organización especificada.




</doc>
<doc id="4534" url="https://es.wikipedia.org/wiki?curid=4534" title="Melchor Pérez de Holguín">
Melchor Pérez de Holguín

Melchor Pérez de Holguín (Cochabamba, Virreinato del Perú, 1660-Potosí, 1732) fue un pintor altoperuano, considerado uno de los mejores artistas de la historia de Bolivia y de la pintura barroca española.

Hijo de Diego Pérez Holguín y de Esperanza Flores, Melchor Pérez de Holguín nació en la ciudad de Cochabamba y pasó la mayor parte de su vida en Villa Imperial de Potosí (Alto Perú, actual Bolivia), donde desarrolló en su arte y se casó el 25 de marzo de 1695 con la lugareña Micaela del Castillo. 

Se desconoce quién pudo ser su maestro. Su producción principal se centró en los encargos realizados para órdenes religiosas católicas, como la franciscana y la dominicana

Entre sus pinturas destacan: "El Juicio Final" (1706), "Triunfo de la Iglesia" (1708, parroquia de San Lorenzo, Potosí), "Entrada del virrey Morcillo en Potosí" (1716) "San Mateo" (1724), perteneciente a la serie de los evangelistas de la Casa de la Moneda de Bolivia; "Virgen de la Merced", "La peregrina", "San Francisco de Asís" (1693, Museo de la Casa de Moneda, Potosí) y "San Pedro de Alcántara en éxtasis" (1701, Museo Nacional de Arte).
Su obra se inscribe en una Potosí crédula y milagrosa, donde el arte barroco se fundía con el carácter religioso español. La fe de Pérez de Holguín y sus conocimientos de religión, combinados con ciertas creencias sobrenaturales (paganismos), le mantuvieron en numerosas ocasiones al borde de la herejía.
Aunque toda la obra de Pérez de Holguín se realizó en la Villa Imperial, algunas fuentes sostienen que pudo haber estudiado en Sevilla con Murillo y que recibió gran influencia de los grabados flamencos y de Zurbarán. Sea como fuere, hasta la fecha no se ha encontrado un solo cuadro firmado en otro lugar. Más tarde, sus obras fueron llevadas de Potosí a otras ciudades de Bolivia, y también al extranjero. 

El principal repositorio de obras de Pérez de Holguín es la Casa Nacional de Moneda en Potosí, entre las que sobresalen los bustos de Mateo el Evangelista y de san Pedro de Alcántara, los retratos de los santos Bernardo de Claraval, Juan de Dios y Luis Gonzaga, un Pentecostés, la excelente Sagrada Familia con san Luis (Luis IX de Francia) y el Nacimiento (fechado en 1701), un san Francisco de Asís (firmado en 1694, que hace par con otro Pedro de Alcántara), y la serie de los cuatro evangelistas de cuerpo entero, firmada en 1724.

Otras muchas obras suyas se encuentran en el Museo Nacional de Arte ubicado en La Paz, frente a la plaza Murillo o de Armas. 

Es difícil calcular el número de cuadros de Pérez de Holguín que salieron de Bolivia, pues su exportación fue clandestina. Se conoce solamente lo que se exhibe en museos públicos, por ejemplo, un San Francisco de Paula en el de Arte Hispanoamericano Isaac Fernández Blanco de Buenos Aires, una "Huida a Egipto", identificada por Cecilio Guzmán de Rojas en el Nacional de Bellas Artes de Santiago de Chile; esta institución posee, además, otro san Francisco de Paula (óleo sobre tela, 84x63cm); el O'Higginiano y de Bellas Artes de Talca (Chile) tiene un san Gerónimo; y en el Museo de América de Madrid, la citada en "Entrada del virrey Morcillo".

Aunque no se sabe cuánto produjo Pérez de Holguín, la gran cantidad de cuadros existentes en Bolivia después de un saqueo secular que sufre Potosí, nos da la pauta y permite concluir que fue bastante prolífico. Cobró fama desde la época virreinal, como lo demuestra el que su nombre sea el único que aparece en el inventario de la pinacoteca de los jesuitas de Potosí, levantado en 1769; que se conserven hasta el presente otros documentos de esa época que nos refieren de sus obras y que, finalmente, un informante de fines del virreinato, diga que fue un "pintor eminente", a quien llamaban "Brocha de Oro".



</doc>
<doc id="4535" url="https://es.wikipedia.org/wiki?curid=4535" title="Surrealismo">
Surrealismo

El surrealismo (del francés “surréalisme”) fue un movimiento cultural desarrollado en Europa tras la Primera guerra mundial, influenciado en gran medida por el dadaísmo. La RAE lo describe como "movimiento artístico y literario que intenta sobrepasar lo real impulsando lo irracional y onírico mediante la expresión automática del pensamiento o del subconsciente".

El movimiento es conocido por sus artes visuales y su escritura mezclado a una imaginación inusual. Los artistas pintaban imágenes desconcertantes e ilógicas, a menudo con una precisión fotográfica, creando extrañas criaturas de objetos cotidianos y desarrollando técnicas pictóricas que permitían desvelar el subconsciente. El objetivo era, según André Breton, "convertir las contradicciones de los sueños y la realidad en una realidad absoluta, una súper realidad".

Las obras surrealistas contienen elementos y yuxtaposiciones inesperados y "non sequitur"; sin embargo, muchos artistas y escritores surrealistas describen su obra primero como una expresión del movimiento filosófico y, lo que es más importante, concebidas como un artefacto. Breton afirmaba que el surrealismo era un movimiento revolucionario, siendo asociado a causas políticas como el comunismo y el anarquismo.

El término "surrealismo" fue acuñado por primera vez por Guillaume Apollinaire en 1917. No obstante, el movimiento surrealista no se estableció hasta el 15 de octubre de 1924, cuando el poeta y crítico francés André Breton publicó el "Manifiesto del surrealismo" en París. Esta ciudad sería la sede central del movimiento. Desde la década de 1920 en adelante, el movimiento se expandió por el mundo, influyendo las artes visuales, la literatura, el cine y la música de múltiples países e idiomas, así como pensamiento y práctica política, filosofía y teoría social.

El término proviene del francés: "surréalisme"; "sur" ['sobre o por encima'] más "réalisme" ['realismo']. Fue acuñado por el escritor francés Guillaume Apollinaire en 1917. En el programa de mano que escribió para el musical "Parade" (mayo de 1917) afirma que sus autores han conseguido:
La palabra "surrealista" aparece ya en junio de 1917, en el subtítulo de "Las tetas de Tiresias (drama surrealista)", para referirse a la reproducción creativa de un objeto, que lo transforma y enriquece. Como escribe Apollinaire en el prefacio al drama:

Los surrealistas señalaron como precedentes de la empresa surrealista a varios pensadores y artistas, como el pensador presocrático Heráclito, el marqués de Sade y Charles Fourier, entre otros. Las teorías psicoanalíticas de Sigmund Freud sobre el sueño y el subconsciente fueron sin duda uno de los pilares en la creación del pensamiento surrealista. 

En cuanto a las artes, la poesía surrealista bebe de la dialéctica y encuentra precursores en Arthur Rimbaud, Alfred Jarry o Lautréamont. En la pintura, el precedente más antiguo es el de Hieronymus Bosch "El Bosco", que en los siglos XV y XVI creó obras como "El jardín de las delicias" o "El carro de heno"; así como, a finales del siglo XIX, el más notable es Giorgio de Chirico y su pintura metafísica. El surrealismo retoma estos elementos y ofrece una formulación sistemática de los mismos. Sin embargo, su precedente más inmediato es el dadaísmo, corriente de la que retoma diferentes aspectos.

La primera fecha histórica del movimiento es 1916, año en que André Breton, precursor, líder y gran pensador del movimiento, descubre las teorías de Sigmund Freud y Alfred Jarry, además de conocer a Jacques Vaché y a Guillaume Apollinaire. Durante los siguientes años se da un confuso encuentro con el dadaísmo, movimiento artístico precedido por Tristan Tzara, en el cual se decantan las ideas de ambos movimientos. Estos, uno inclinado hacia la destrucción nihilista (dadá) y el otro a la construcción romántica (surrealismo) se sirvieron como catalizadores entre ellos durante su desarrollo.

En el año 1924 Breton escribe el primer "Manifiesto Surrealista" y en este incluye lo siguiente:

Tal fue la definición del término dada por los propios Breton y Soupault en el primer "Manifiesto Surrealista" fechado en 1924. Surgió por tanto como un movimiento poético, en el que pintura y escultura se conciben como consecuencias plásticas de la poesía.

En "El surrealismo y la pintura", de 1928, Breton expone la psicología surrealista: el inconsciente es la región del intelecto donde el ser humano no objetiva la realidad sino que forma un todo con ella. El arte, en esa esfera, no es representación sino comunicación vital directa del individuo con el todo. Esa conexión se expresa de forma privilegiada en las casualidades significativas (azar objetivo), en las que el deseo del individuo y el devenir ajeno a él convergen imprevisiblemente, y en el sueño, donde los elementos más dispares se revelan unidos por relaciones secretas. El surrealismo propone trasladar esas imágenes al mundo del arte por medio de una asociación mental libre, sin la intromisión censora de la conciencia. De ahí que elija como método el automatismo, recogiendo en buena medida el testigo de las prácticas mediúmnicas espiritistas, aunque cambiando radicalmente su interpretación: lo que habla a través del médium no son los espíritus, sino el inconsciente.

Durante unas sesiones febriles de automatismo, Breton y Soupault escriben "Los Campos Magnéticos", primera muestra de las posibilidades de la escritura automática, que publican en 1921. Más adelante Breton publica "Pez soluble". Dice así el final del séptimo cuento:
A partir de 1925, a raíz del estallido de la Guerra del Rif, el surrealismo se politiza; se producen entonces los primeros contactos con los comunistas, que culminarían ese mismo año con la adhesión al Partido Comunista por parte de Breton.

Entre 1925 y 1930 aparece un nuevo periódico titulado "El Surrealismo al servicio de la Revolución" en cuyo primer número Louis Aragón, Buñuel, Dalí, Paul Éluard, Max Ernst, Yves Tanguy y Tristan Tzara, entre otros, se declaran partidarios de Breton. Por su parte Jean Arp y Miró, aunque no compartían la decisión política tomada por Breton, continuaban participando con interés en las exposiciones surrealistas. Poco después se incorporaron Magritte (1930), Masson (1931), Giacometti y Brauner en 1933 y también Matta (que conoce a Breton en 1937 por mediación de Dalí) y Lam; el movimiento se hizo internacional apareciendo grupos surrealistas en los Estados Unidos, Dinamarca, Londres, Checoslovaquia y Japón. Desde este momento, se abrirá una disputa, a menudo agria, entre aquellos surrealistas que conciben el surrealismo como un movimiento puramente artístico, rechazando la supeditación al comunismo, y los que acompañan a Breton en su giro a la izquierda. 

En 1929 Breton publica el "Segundo Manifiesto Surrealista", en el que condena entre otros intelectuales a los artistas Masson y Francis Picabia. En 1936 expulsa a Dalí por querer mantenerse neutral frente a la politización del movimiento y no condenar el nazismo alemán, y a Paul Éluard. En 1938 Breton firma en México junto con León Trotski y Diego Rivera el "Manifiesto por un Arte Revolucionario Independiente".

Pese a esta escisión, existen numerosos artistas y obras de arte a los que se identifica y clasifica como surrealistas, sin que sus obras tengan sesgos políticos. El elemento más importante dentro del surrealismo sigue siendo la realidad aumentada, deformada o reinterpretada a partir de elementos oníricos y subconscientes.

El surrealismo tomó del dadaísmo algunas técnicas de fotografía y cinematografía así como la fabricación de objetos. Extendieron el principio del "collage" (el "objeto encontrado") al ensamblaje de objetos incongruentes, como en los poemas visibles de Max Ernst. Este último inventó el "frottage" (dibujos compuestos por el roce de superficies rugosas contra el papel o el lienzo) y lo aplicó en grandes obras como "Historia Natural", pintada en París en 1926.

Crearon el cadáver exquisito, en el cual varios artistas dibujaban las distintas partes de una figura o de un texto sin ver lo que el anterior había hecho pasándose el papel doblado. Las criaturas resultantes pudieron servir de inspiración a Miró.

En el terreno literario, el surrealismo supuso una gran revolución en el lenguaje y la aportación de nuevas técnicas de composición. Como no asumía tradición cultural alguna, ni desde el punto de vista temático ni formal, prescindió de la métrica y adoptó el tipo de expresión poética denominado como versículo: un verso de extensión indefinida sin rima que se sostiene únicamente por la cohesión interna de su ritmo. Igualmente, como no se asumía la temática consagrada, se fue a buscar en las fuentes de la represión psicológica (sueños, sexualidad) y social, con lo que la lírica se rehumanizó después de que los ismos intelectualizados de las Vanguardias la deshumanizaran, a excepción del Expresionismo. Para ello utilizaron los recursos de la transcripción de sueños y la escritura automática, y engendraron procedimientos metafóricos nuevos como la imagen visionaria. El lenguaje se renovó también desde el punto de vista del léxico dando cabida a campos semánticos nuevos y la retórica se enriqueció con nuevos procedimientos expresivos.

Masson adoptó enseguida las técnicas del automatismo, hacia 1923-1924, poco después de conocer a Breton. Hacia 1929 las abandonó para volver a un estilo cubista. Por su parte, Dalí utilizaba más la fijación de imágenes tomadas de los sueños, según Breton, «...abusando de ellas y poniendo en peligro la credibilidad del surrealismo...»; inventó lo que él mismo llamó método paranoico-crítico, una mezcla entre la técnica de observación de Leonardo da Vinci, por medio de la cual, observando una pared se podía ver cómo surgían formas y técnicas de "frottage"; fruto de esta técnica son las obras en las que se ven dos imágenes en una sola configuración. Óscar Domínguez inventó la decalcomanía (aplicar gouache negro sobre un papel el cual se coloca encima de otra hoja sobre la que se ejerce una ligera presión, luego se despegan antes de que se sequen). Además de las técnicas ya mencionadas de la decalcomanía y el "frottage", los surrealistas desarrollaron otros procedimientos que incluyen igualmente el azar: el raspado, el "fumage" y la distribución de arena sobre el lienzo encolado.

Miró fue para Breton el más surrealista de todos, por su automatismo psíquico puro. Su surrealismo se desenvuelve entre las primeras obras donde explora sus sueños y fantasías infantiles ("El Campo labrado"), las obras donde el automatismo es predominante ("Nacimiento del mundo") y las obras en que desarrolla su lenguaje de signos y formas biomorfas ("Personaje lanzando una piedra"). Arp combina las técnicas de automatismo y las oníricas en la misma obra desarrollando una iconografía de formas orgánicas que se ha dado en llamar "escultura biomórfica", en la que se trata de representar lo orgánico como principio formativo de la realidad.

René Magritte dotó al surrealismo de una carga conceptual basada en el juego de imágenes ambiguas y su significado denotado a través de palabras poniendo en cuestión la relación entre un objeto pintado y el real. Paul Delvaux carga a sus obras de un espeso erotismo basado en su carácter de extrañamiento en los espacios de Giorgio de Chirico.

El surrealismo penetró la actividad de muchos artistas europeos y americanos en distintas épocas. Pablo Picasso se alió con el movimiento surrealista en 1925; Breton declaraba este acercamiento de Picasso calificándolo de «...surrealista dentro del cubismo...». Se consideran surrealistas las obras del período Dinard (1928-1930), en que Picasso combina lo monstruoso y lo sublime en la composición de figuras medio máquinas medio monstruos de aspecto gigantesco y a veces terrorífico. Esta monumentalidad surrealista de Picasso puede ponerse en paralelo con la de Henry Moore y en la poesía y el teatro con la de Fernando Arrabal.

Otros movimientos pictóricos nacieron del surrealismo o lo prefiguran, como por ejemplo el Art brut.

En 1938 tuvo lugar en París la Exposición Internacional del Surrealismo que marcó el apogeo de este movimiento antes de la guerra. Participaron entre otros, Marcel Duchamp, Jean Arp, Dalí, Max Ernst, Masson, Man Ray, Óscar Domínguez y Meret Oppenheim. La exposición ofreció al público sobre todo una excelente muestra de lo que el surrealismo había producido en la fabricación de objetos.

Con el estallido de la Segunda Guerra Mundial, los surrealistas se dispersan, algunos de ellos (Dalí, Breton, Ernst, Masson) abandonan París y se trasladan a los Estados Unidos, donde siembran el germen para los futuros movimientos americanos de posguerra (expresionismo abstracto y Arte Pop).

En España el surrealismo aparece en torno a los años 1920, no en su vertiente puramente vanguardista, sino mezclado con acentos simbolistas y de la pintura popular. Además de Joan Miró y Salvador Dalí, el surrealismo español lo componen Maruja Mallo, Gregorio Prieto, José Moreno Villa, Benjamín Palencia y José Caballero, además de los neocubistas que se pasan al surrealismo (Alberto Sánchez y Ángel Ferrant).

Hubo un importante núcleo surrealista en las Islas Canarias, agrupado en torno a la "Gaceta de Arte" de Eduardo Westerdahl, del que un grupo de poetas invitaron a André Bretón a venir en 1935; allí compuso este el poema "Le chateau etoilé" y otras obras. Los máximos representantes de la pintura surrealista en el archipiélago fueron Óscar Domínguez, Juan Ismael y el propio Westerdahl.

En Latinoamérica se consideran surrealistas, además de los ya citados Roberto Matta (Chile) y Lam, a Remedios Varo y Leonora Carrington (ambas inmigrantes europeas posteriormente nacionalizadas mexicanas).

La que es considerada como la primera exposición surrealista en Latinoamérica se llevó a cabo en Lima, Perú en 1935 por iniciativa de los poetas y pintores surrealistas peruanos César Moro y Emilio Adolfo Westphalen. Posteriormente en México, en enero de 1940, el mismo César Moro con André Breton y Wolfgang Paalen logran presentar en la Galería de Arte Mexicano una selección de cuarenta obras tanto de representantes del movimiento surrealista como de americanos cuyo trabajo tenía afinidad con el movimiento.. Existe un debate sobre si la obra de Frida Kahlo pertenece a la corriente surrealista. Breton consideraba a México la esencia del surrealismo e interpretaba sus obras como surrealistas, si bien la propia Kahlo decía claramente "Yo no pinto sueños... pinto mi realidad".

Es de destacar el aporte al movimiento realizado desde Buenos Aires, Argentina, en ese entonces considerada como la capital latinoamericana de la cultura, de artistas y literatos como Aldo Pellegrini, Planas Casas y Batlle Planas.

El surrealismo fue seguido con interés por los intelectuales españoles de los años 30. Existía el precedente de Ramón Gómez de la Serna, quien utilizaba algunas fórmulas vinculables al surrealismo, como la greguería.

Varios poetas de la generación del 27 se interesaron por las posibilidades expresivas del surrealismo. El primero en adoptar sus métodos fue José María Hinojosa, autor de "La flor de Californía" (1928), libro pionero de prosas narrativas y oníricas. Su huella también es evidente en libros como en la sección tercera de "Sobre los ángeles" y en "Sermones y moradas" de Rafael Alberti; en "Poeta en Nueva York" de Federico García Lorca y "Un río, un amor" y "Los placeres prohibidos" de Luis Cernuda. Vicente Aleixandre se definió a sí mismo como "un poeta superrealista", aunque matizando que su poesía no era en modo alguno producto directo de la escritura automática. Miguel Hernández sufrió una efímera etapa surrealista y durante la posguerra la impronta surrealista se percibe en los poetas del Postismo y en Juan Eduardo Cirlot, y en la actualidad existe un cierto postsurrealismo en la obra de algunos poetas como Blanca Andreu.

Pero puede decirse que fue solo en Canarias donde la aventura surrealista tuvo, en el primer minuto del movimiento, auténtica expresión, esto es, declarada vinculación al movimiento pero sin instalarse en París: la Facción Surrealista de Tenerife, tal como la describiera Domingo Pérez Minik posteriormente. Todos sus componentes, liderados por Agustín Espinosa y vinculados a París por el pintor tinerfeño Óscar Domínguez, venían de la experiencia de la vanguardia insular con la revista "La Rosa de los Vientos", aparecida en 1926, y continuarían trabajando en la renovación artística y literaria de las islas en Gaceta de Arte, una de las más importantes revistas de la vanguardia hispánica, con diverso contenido de vanguardia internacional y con colaboradores no surrealistas como Domingo Pérez Minik y Eduardo Westerdahl. Aparte de Espinosa, Pedro García Cabrera, Emeterio Gutiérrez Albelo, Domingo López Torres y José María de la Rosa completan la nómina de escritores surrealistas con obras como "Crimen" (1934) -considerada por algunos como la mejor prosa surrealista en lengua castellana-, "Romanticismo y cuenta nueva" (1933), "Enigma del invitado" (1936), "Dársena con despertadores" (1936), "Lo imprevisto" (1937) y "Vértice de sombra" (1936). Juan Ismael se uniría a Óscar Domínguez en la plástica, pero desarrollando su actividad en las islas. Como en los demás casos, la Guerra Civil Española acabó con el grupo y con la vida de alguno de ellos, como López Torres -ahogado por los nacionales- o Espinosa, que murió poco después del golpe de Estado; García Cabrera, por su parte, sería detenido y huiría, uniéndose a las tropas republicanas. Sin embargo, la actividad había llegado a su culmen con la visita de André Breton y Benjamin Péret a Tenerife en 1935, organizando una exposición de pintura, firmando el Segundo Boletín Internacional del Surrealismo, intentando proyectar La Edad de Oro de Luis Buñuel -prohibida por el gobierno de la isla- y dejando en Breton un recuerdo que constituirá el contenido del capítulo V de su L'amour fou (1937).

Aunque no se le pueda considerar un surrealista estricto, el poeta y pensador Juan Larrea vivió de primera mano la eclosión del movimiento en París y reflexionó más tarde sobre su valor y trascendencia en obras como "Surrealismo entre viejo y nuevo mundo" (1944). En la actualidad existe una corriente de neosurrealismo en la poesía de Blanca Andreu. El español Fernando Arrabal tuvo una asistencia diaria al "café surrealista" La Promenade de Vénus de 1960 a 1963. André Breton publicó su teatro, su "Piedra de la locura" y algunos de sus cuadros.

En Hispanoamérica el surrealismo contó con la adhesión entusiasta de poetas como el chileno Braulio Arenas y los peruanos César Moro, Xavier Abril y Emilio Adolfo Westphalen, además de influir en la obra del escritor cubano Alejo Carpentier y de los poetas chilenos Pablo Neruda, Gonzalo Rojas y el peruano César Vallejo. En Argentina, pese al desdén de Jorge Luis Borges, el surrealismo sedujo aún al joven Julio Cortázar y produjo un fruto tardío en la obra de Alejandra Pizarnik. El poeta y pensador mexicano Octavio Paz ocupa un lugar particular en la historia del movimiento: amigo personal de Breton, dedicó al surrealismo varios ensayos esclarecedores.

En la literatura catalana - balear tenemos el surrealismo poético del mallorquín Llorenç Vidal sobre todo en sus obras "El cant de la balalaika" y "5 meditacions existencials".

El surrealismo tuvo como antecedente la patafísica de Alfred Jarry, y el movimiento dadaísta fundado en Zúrich en 1916 por T. Tzara, H. Ball y H. Arp. Animados por idéntico espíritu de provocación, André Breton, Louis Aragon y Philippe Soupault fundaron en París la revista "Littérature" (1919), mientras en EE. UU. manifestaban actitudes similares Man Ray, Marcel Duchamp y Francis Picabia, y en Alemania, Max Ernst y Hugo Ball.

A esta fase sucedió una actitud más metódica de investigación del inconsciente, emprendida por Breton, junto a Aragon, Paul Éluard, Soupault, Robert Desnos, Max Ernst, etc. La primera obra de esta tendencia, que cabe calificar de primera obra literaria surrealista, fue "Los campos magnéticos" (1921), escrita conjuntamente por Breton y Soupault. Tras la ruptura con Tzara, se adhirieron al movimiento Antonin Artaud, André Masson y Pierre Naville.

Breton redactó la primera definición del movimiento en su "Manifiesto del surrealismo" (1924), texto que dio cohesión a los postulados y propósitos del movimiento. Entre los autores que citaba como precursores del movimiento figuran Freud, Lautréamont, Edward Young, Matthew Lewis, Gérard de Nerval, Jonathan Swift, Marqués de Sade, François-René de Chateaubriand, Victor Hugo, Edgar Allan Poe, Charles Baudelaire, Arthur Rimbaud, Mallarmé y Jarry. En el mismo año se fundó el Bureau de recherches surréalistes y la revista "La Révolution Surréaliste", que sustituyó a "Littérature", de cuya dirección se hizo cargo el propio Breton en 1925 y que se convirtió en el órgano de expresión común del grupo.

La producción surrealista se caracterizó por una vocación libertaria sin límites y la exaltación de los procesos oníricos, del humor corrosivo y de la pasión erótica, concebidos como armas de lucha contra la tradición cultural burguesa. Las ideas del grupo se expresaron a través de técnicas literarias, como la «escritura automática», las provocaciones pictóricas y las ruidosas tomas de posición públicas. El acercamiento operado a fines de los años veinte con los comunistas produjo las primeras querellas y cismas en el movimiento.

En 1930 Breton publicó su "Segundo manifiesto del surrealismo", en el que "excomulgaba" a Joseph Delteil, Antonin Artaud, Philippe Soupault, Robert Desnos, Georges Limbour, André Masson, Roger Vitrac, Georges Ribemont-Dessaignes y Francis Picabia. El mismo año apareció el nuevo órgano del movimiento, la revista "Le Surréalisme au Service de la Révolution", que suplantó al anterior, "La Révolution Surréaliste", y paralelamente, Aragon (tras su viaje a la URSS), Éluard, Péret y Breton ingresaron en el Partido Comunista. A fines de 1933, Breton, Éluard y Crevel fueron expulsados del partido. En los años treinta se sumaron al movimiento Salvador Dalí, Luis Buñuel, Yves Tanguy, René Char y Georges Sadoul. Ya expulsado del grupo por Breton, Dalí publicó en 1942 "La Vida Secreta de Salvador Dalí", autobiografía que reúne muchos de los elementos propios del surrealismo y que constata las virtudes literarias de un Dalí en pleno auge creativo.

Tras los años previos a la II Guerra Mundial, marcados por la militancia activa de Breton, y los años de exilio neoyorquino de la mayoría de sus miembros, durante la ocupación alemana de Francia, el movimiento siguió manteniendo cierta cohesión y vitalidad, pero a partir de 1946, cuando Breton regresó a París, el surrealismo era ya parte de la historia.

Al principio el surrealismo era un movimiento fundamentalmente literario, y hasta un poco más tarde no produciría grandes resultados en las artes plásticas. Surge un concepto fundamental, el automatismo, basado en una suerte de dictado mágico, procedente del inconsciente, gracias al cual surgían poemas, ensayos, etc., y que más tarde sería recogido por pintores y escultores.

La primera exposición surrealista se celebró en la Galerie Pierre de París en 1925, y en ella, además de Jean Arp, Giorgio de Chirico y Max Ernst, participaron artistas como André Masson, Picasso, Man Ray, Pierre Roy, P. Klee y Joan Miró, que posteriormente se separarían del movimiento o se mantendrían unidos a él adoptando únicamente algunos de sus principios. A ellos se adhirieron Yves Tanguy, René Magritte, Salvador Dalí y Alberto Giacometti.

La rebelión del surrealismo contra la tradición cultural burguesa y el orden moral establecido tuvo su cariz político, y un sector del surrealismo, que no consideraba suficientes los tumultos de sus manifestaciones culturales, se afilió al Partido Comunista Francés. Sin embargo, nacieron violentas discrepancias en el seno del grupo a propósito del debate sobre la relación entre arte y política; se sucedieron manifiestos contradictorios y el movimiento tendió a disgregarse. Es significativo, a este respecto, que la revista «La révolution surréaliste» pase a llamarse, desde 1930, «Le surréalisme au service de la révolution». En los años 1930, el movimiento se extendió más allá de las fronteras francesas. Se celebró en 1938 en París la Exposición Surrealista Internacional.

La segunda guerra mundial paralizó toda actividad en Europa. Ello motivó que Breton, como muchos otros artistas, marchase a los EE. UU. Allí surgió una asociación de pintores surrealistas alemanes y franceses que se reunió en torno a la revista VVV. Estos surrealistas emigrados a EE. UU. influyeron en el arte estadounidense, en particular en el desarrollo del expresionismo abstracto en los años 1940. Cuando Breton regresó a Europa en 1946 el movimiento estaba ya definitivamente deteriorado.

Entre los artistas plásticos se manifiesta una dualidad en la interpretación del surrealismo: los surrealistas abstractos, que se decantan por la aplicación del automatismo puro, como André Masson o Joan Miró, e inventan universos figurativos propios; y los surrealistas figurativos, interesados por la vía onírica, entre ellos René Magritte, Paul Delvaux, o Salvador Dalí, que se sirven de un realismo minucioso y de medios técnicos tradicionales, pero que se apartan de la pintura tradicional por la inusitada asociación de objetos y las monstruosas deformaciones, así como por la atmósfera onírica y delirante que se desprende de sus obras. Max Ernst es uno de los pocos surrealistas que se mueve entre las dos vías. La obra de Ernst ha influido particularmente en un epígono tardío del surrealismo en Alemania que es Stefan von Reiswitz.

En la vertiente cinematográfica, el surrealismo dio lugar a varios intentos enmarcados en el cine de las vanguardias históricas, como "La Coquille et le clergyman" (1926, "La caracola y el clérigo"), de Germaine Dulac o "L'étoile de mer" (1928, "La estrella de mar"), de Man Ray y Robert Desnos, un cortometraje dadaísta.

También se puede considerar surrealista "Entr'acte" , corto de 22 minutos de duración escrito por René Clair y Francis Picabia, dirigido por Clair.

Luis Buñuel, en colaboración con Dalí, realizó las obras más revolucionarias: "Un perro andaluz" ("Un chien andalou", 1928) y "La edad de oro" ("L'âge d'or", 1930).

En 1931 Jean Cocteau escribió, dirigió y estrenó "La sangre de un poeta" , mediometraje surrealista de 50 minutos de duración.

En Estados Unidos la madre del surrealismo cinematográfico, desde 1940, fue Maya Deren. Su obra "Meshes of the Afternoon" (1943), de 14 minutos de duración, se considera la primera obra maestra surrealista estadounidense. Sin embargo, también se puede considerar surrealista un film anterior, "Rose Hobart" (1936), donde Joseph Cornell realiza un nuevo montaje en forma de collage a partir de celuloide de otro film anterior, "East of Borneo" (1931), cinta anterior a la entrada en vigor del Código Hays dirigida por George Melford.

Alfred Hitchcock y Salvador Dalí colaboraron cuando el primero encargó al artista catalán parte de la escenografía de "Recuerda" ("Spellbound", 1945).

Desde los años sesenta del siglo XX cineastas contemporáneos como Alejandro Jodorowsky, David Lynch, Jan Švankmajer, Fernando Arrabal y, en los años ochenta y noventa, Jean-Pierre Jeunet, Julio Médem, Stephen Sayadian, o Carlos Atanes, entre otros, muestran la influencia del surrealismo.

El cine escrito o dirigido por el Grupo Pánico, formado por Roland Topor, Arrabal y Jodorowsky, también se considera cine surrealista o post-surrealista, pues los tres formaron parte del Grupo Surrealista liderado por Breton en París entre 1960 y septiembre de 1962. 
De entre todos sus films "The Holy Mountain" (1973), de Jodorowsky, se considera el ejemplo más depurado de cine simbolista y surrealista.





</doc>
<doc id="4536" url="https://es.wikipedia.org/wiki?curid=4536" title="Puntillismo">
Puntillismo

El puntillismo es una técnica artística que consiste en hacer una obra mediante el uso de diminutos puntos. Aparece por primera vez en 1869, encabezada por el pintor neoimpresionista Georges Seurat, al que le siguieron artistas como Henri-Edmond Cross y Vlaho Bukovac. Este procedimiento consiste en poner puntos de colores puros en vez de pinceladas sobre la tela. Este fue el resultado de los estudios cromáticos llevados a cabo por Georges Seurat (1859-1891), pintor francés, quien en 1884 llegó a la división de tonos por la posición de toques de color que, mirados a cierta distancia, crean en la retina las combinaciones deseadas. Otro de los más importantes artistas puntillistas fue Paul Signac, participante junto con Seurat y otros neoimpresionistas en la Société des Artistes Indépendants (1884), todos ellos seguidores del puntillismo o divisionismo. Está relacionado con el divisionismo, una variante más técnica del método. El divisionismo se ocupa de la teoría del color, mientras que el puntillismo se centra más en el estilo específico de pincel utilizado para aplicar la pintura. Es una técnica con pocos practicantes serios hoy en día, y se observa notablemente en las obras de Seurat, Signac y Cross. Mediante la adopción de minúsculas pinceladas en forma de punto lograron acumular, incluso sobre superficies reducidas, una gran variedad de colores y tonos, cada uno de los cuales se correspondía con uno de los elementos que contribuía a la apariencia del objeto. A una distancia determinada esas partículas diminutas se mezclan ópticamente y el resultado tenía que producir una intensidad de colores mucho mayor que cualquier mezcla de pigmentos.

Este movimiento, dentro de las coordenadas del postimpresionismo, parte también de la imagen de la naturaleza, es decir, del mismo motivo que los impresionistas, pero para ellos serán unas leyes físicas y fisiológicas muy determinadas las que caractericen la esencia de la pintura. Su material de reflexión serán, sobre todo, los escritos de Charles Blanc y, de un modo más radical los impresionistas, los tratados científicos de Chevreul, Sutter, Rood y otros. 

Gracias a ellos, el puntillismo vio abierto ante sí un campo en el que su tarea habría de ser la aplicación metódica de sus conocimientos y la reconciliación de los rígidos principios del dibujo con los principios ópticos intuidos por los grandes coloristas. La mente lógica y reflexiva de estos pintores pedía la reducción del instinto al orden, del impulso al cálculo, reduciendo a lo esencial, no solo los temas de la vida moderna o el paisaje, sino también el método impresionista de presentarlo
De hecho, la declaración de Charles Blanc («El color, que está controlado por leyes fijas, se puede enseñar como la música»), publicada por primera vez en 1865 en su conocida "Gramatica Ades arts du dessin", resume perfectamente la actitud de los puntillistas ante las posibilidades expresivas del arte e indica su programa. Según esto, al igual que existen relaciones matemáticas entre los tonos musicales, hay relaciones físicas entre los colores, que pueden demostrarse en el laboratorio y llevarse a efecto en el estudio. Con el fin de estudiar con más detalle la interacción de los colores y sus complementarios, algunos puntillistas confeccionaron un disco en el que reunían todos los matices del arcoíris, unidos unos a otros mediante un número determinado de colores intermedios.

En su paleta también utilizaban el blanco mezclado con los colores primarios, lo que les permitía obtener una multitud de tonos que iban de un color con una ligera presencia de blanco hasta un blanco casi puro. El disco se completaba de manera que los matices puros se concentraban en torno al centro, desde donde iban desvaneciéndose hacia el blanco hasta llegar a la periferia.

Los experimentos físicos habían probado también que la mezcla de colores los ensucia y desemboca finalmente en el negro. Por ello, la única mezcla capaz de producir el efecto deseado es la mezcla óptica, que se convierte así en el factor predominante de su ejecución. Tras haber reunido por separado en sus telas los elementos individuales de color presentes en la naturaleza, el pintor asignaba a la retina del espectador la tarea de unirlos de nuevo. La técnica de pinceladas de los impresionistas no permitía la exactitud matemática que necesitaban los puntillistas para aplicar su sistema con pleno rendimiento. 

Mediante la adopción de minúsculas pinceladas en forma de punto lograron acumular, incluso sobre superficies reducidas, una gran variedad de colores y tonos, cada uno de los cuales se correspondía con uno de los elementos que contribuía a la apariencia del objeto. A una distancia determinada esas partículas diminutas se mezclan ópticamente y el resultado tenía que producir una intensidad de colores mucho mayor que cualquier mezcla de pigmentos. 

En este sentido, sus estudios de luz y color sobrepasan los realizados por cualquiera de los impresionistas, pero también se encontraron con mayores dificultades. Con más conocimientos y un ojo más disciplinado, tenían que hallar todos los matices del espectro luminoso, así como un modo de iluminar u oscurecer un matiz dado en relación con los contrastes simultáneos producidos por los colores que le rodeaban. Una de las obras más notables en ese sentido es "Tarde de domingo en la isla de la Grande Jatte" de Seurat.

A pesar de lo aparentemente acertado de la denominación de puntillismo, sobre todo en lo que se refiere a la técnica de este grupo, ni Seurat ni Signac la aceptaron nunca y ambos condenaron y evitaron rigurosamente este término a favor del de divisionismo, que abarcaba mejor todas sus innovaciones.

La técnica se basa en la capacidad del ojo y la mente del espectador para combinar las manchas de color en una gama más amplia de tonos. Está relacionado con el divisionismo, una variante más técnica del método. El divisionismo se ocupa de la teoría del color, mientras que el puntillismo se centra más en el estilo específico de pincel utilizado para aplicar la pintura. [1] Es una técnica con pocos practicantes serios hoy en día, y se observa notablemente en las obras de Seurat, Signac y Cross. Sin embargo, vea también las primeras obras de Andy Warhol, y Pop Art.


Algunos compositores trasladaron las conclusiones del método puntillista al campo musical. Así como el ojo compone colores que no están ahí, el oído hace lo propio: relaciona los sonidos separados y los interpreta como una melodía. Pero, al contrario que el movimiento pictórico, el puntillismo musical persigue la disociación.
Cabe notar que esta tendencia sirve como base para el desarrollo y creación de imágenes musicales tomando los pixeles musicales como estructura básica en la percepción visual de una imagen. teniendo en cuenta la teoría del cromatismo para la deficion del color dentro de la escala cromatica musical.


El puntillismo es una técnica artística que consiste en hacer una obra mediante el uso de diminutos puntos. Aparece por primera vez en 1884, encabezada por el pintor neoimpresionista Georges Seurat, al que le siguieron artistas como Henri-Edmond Cross y Vlaho Bukovac. Este procedimiento consiste en poner puntos de colores puros en vez de pinceladas sobre la tela. Este fue el resultado de los estudios cromáticos llevados a cabo por Georges Seurat (1859-1891), pintor francés, quien en 1884 llegó a la división de tonos por la posición de toques de color que, mirados a cierta distancia, crean en la retina las combinaciones deseadas. Otro de los más importantes artistas puntillistas fue Paul Signac, participante junto con Seurat y otros neoimpresionistas en la Société des Artistes Indépendants (1884), todos ellos seguidores del puntillismo o divisionismo.

Este movimiento, dentro de las coordenadas del postimpresionismo, parte también de la imagen de la naturaleza, es decir, del mismo motivo que los impresionistas, pero para ellos serán unas leyes físicas y fisiológicas muy determinadas las que caractericen la esencia de la pintura. Su material de reflexión serán, sobre todo, los escritos de Charles Blanc y, de un modo más radical los impresionistas, los tratados científicos de Chevreul, Sutter, Rood y otros. 

Gracias a ellos, el puntillismo vio abierto ante sí un campo en el que su tarea habría de ser la aplicación metódica de sus conocimientos y la reconciliación de los rígidos principios del dibujo con los principios ópticos intuidos por los grandes coloristas. La mente lógica y reflexiva de estos pintores pedía la reducción del instinto al orden, del impulso al cálculo, reduciendo a lo esencial, no solo los temas de la vida moderna o el paisaje, sino también el método impresionista de presentarlo
De hecho, la declaración de Charles Blanc («El color, que está controlado por leyes fijas, se puede enseñar como la música»), publicada por primera vez en 1865 en su conocida "Gramatica Ades arts du dessin", resume perfectamente la actitud de los puntillistas ante las posibilidades expresivas del arte e indica su programa. Según esto, al igual que existen relaciones matemáticas entre los tonos musicales, hay relaciones físicas entre los colores, que pueden demostrarse en el laboratorio y llevarse a efecto en el estudio. Con el fin de estudiar con más detalle la interacción de los colores y sus complementarios, algunos puntillistas confeccionaron un disco en el que reunían todos los matices del arcoíris, unidos unos a otros mediante un número determinado de colores intermedios.

En su paleta también utilizaban el blanco mezclado con los colores primarios, lo que les permitía obtener una multitud de tonos que iban de un color con una ligera presencia de blanco hasta un blanco casi puro. El disco se completaba de manera que los matices puros se concentraban en torno al centro, desde donde iban desvaneciéndose hacia el blanco hasta llegar a la periferia.

Los experimentos físicos habían probado también que la mezcla de colores los ensucia y desemboca finalmente en el negro. Por ello, la única mezcla capaz de producir el efecto deseado es la mezcla óptica, que se convierte así en el factor predominante de su ejecución. Tras haber reunido por separado en sus telas los elementos individuales de color presentes en la naturaleza, el pintor asignaba a la retina del espectador la tarea de unirlos de nuevo. La técnica de pinceladas de los impresionistas no permitía la exactitud matemática que necesitaban los puntillistas para aplicar su sistema con pleno rendimiento. 

Mediante la adopción de minúsculas pinceladas en forma de punto lograron acumular, incluso sobre superficies reducidas, una gran variedad de colores y tonos, cada uno de los cuales se correspondía con uno de los elementos que contribuía a la apariencia del objeto. A una distancia determinada esas partículas diminutas se mezclan ópticamente y el resultado tenía que producir una intensidad de colores mucho mayor que cualquier mezcla de pigmentos. 

En este sentido, sus estudios de luz y color sobrepasan los realizados por cualquiera de los impresionistas, pero también se encontraron con mayores dificultades. Con más conocimientos y un ojo más disciplinado, tenían que hallar todos los matices del espectro luminoso, así como un modo de iluminar u oscurecer un matiz dado en relación con los contrastes simultáneos producidos por los colores que le rodeaban. Una de las obras más notables en ese sentido es "Tarde de domingo en la isla de la Grande Jatte" de Seurat.

A pesar de lo aparentemente acertado de la denominación de puntillismo, sobre todo en lo que se refiere a la técnica de este grupo, ni Seurat ni Signac la aceptaron nunca y ambos condenaron y evitaron rigurosamente este término a favor del de divisionismo, que abarcaba mejor todas sus innovaciones.

La técnica se basa en la capacidad del ojo y la mente del espectador para combinar las manchas de color en una gama más amplia de tonos. Está relacionado con el divisionismo, una variante más técnica del método. El divisionismo se ocupa de la teoría del color, mientras que el puntillismo se centra más en el estilo específico de pincel utilizado para aplicar la pintura. [1] Es una técnica con pocos practicantes serios hoy en día, y se observa notablemente en las obras de Seurat, Signac y Cross. Sin embargo, vea también las primeras obras de Andy Warhol, y Pop Art.


Algunos compositores trasladaron las conclusiones del método puntillista al campo musical. Así como el ojo compone colores que no están ahí, el oído hace lo propio: relaciona los sonidos separados y los interpreta como una melodía. Pero, al contrario que el movimiento pictórico, el puntillismo musical persigue la disociación.
Cabe notar que esta tendencia sirve como base para el desarrollo y creación de imágenes musicales tomando los pixeles musicales como estructura básica en la percepción visual de una imagen. teniendo en cuenta la teoría del cromatismo para la deficion del color dentro de la escala cromatica musical.



</doc>
<doc id="4537" url="https://es.wikipedia.org/wiki?curid=4537" title="Manuel Pereira">
Manuel Pereira

Manuel Pereira (Oporto, 1588-Madrid, 29 de enero de 1683) fue un escultor barroco portugués avecindado en Madrid, donde realizó buena parte de su obra.

Nacido en Oporto en 1588, no se conoce otro dato de su vida y actividad hasta 1624, cuando ejecutó las estatuas en piedra de la iglesia de la Compañía de Jesús en Alcalá de Henares. Un año después se encontraba ya en Madrid, a donde se había trasladado en compañía de su madre y de su hermano Pantaleón Gómez, también escultor, que colaboró con él hasta su muerte en 1645. En 1625 contrajo matrimonio en Madrid con María González de Estrada, del que nacerán dos hijos, enviudando en 1639. En 1635 se encontraba en prisión por deudas, saliéndole fiadores el ensamblador Juan Bautista Garrido y el pintor Jusepe Leonardo, policromador de algunas de sus obras. 

En un curioso contrato, por el que el ensamblador y arquitecto Pedro de la Torre se comprometía en 1652 a realizar el retablo de la capilla del beato Simón de Rojas en la iglesia de la Trinidad de Madrid, se pone como condición que las esculturas han de ser de mano de Pereira o de Juan Sánchez Barba, «y no de ningún otro», condición que se repetiría en 1661 en el contrato de un retablo para el convento de la Merced con el ensamblador Juan de Ocaña. En ambos casos parece que el elegido fue Sánchez Barba, el único imaginero que podía competir en Madrid con Pereira en estos años. 

Obtuvo el nombramiento de Familiar del Santo Oficio, título que preferirá en su testamento al de escultor, para lo que hubo de presentar pruebas de limpieza de sangre. El mismo prurito nobiliario pondrá de manifiesto al casar a su hija Damiana con José de Mendieta, caballero de la Orden de Santiago, a la que también pertenecerán sus nietos, alegando un testigo «que él y sus ascendientes eran cavalleros fidalgos del Reyno de Portugal, donde havían exercido los oficios y ocupaciones que en aquel Reyno sólo pueden tener los cavalleros hixodalgos». Murió en Madrid en 1683, casi ciego y después de más de diez años de inactividad.

Discípulos o colaboradores fueron, además de su hermano Pantaleón ya citado, Manuel Correa, natural también de Oporto y doce años más joven, Manuel Delgado y el navarro José Martínez.

Aunque se supone que su formación tuvo lugar en Oporto Pereira se va a convertir en uno de los grandes representantes de la escuela madrileña de escultura. A excepción de un grupo de esculturas destinadas al convento portugués de Santo Domingo de Benfica, de las que se ocupó sin salir de Madrid en 1636 por encargo del conde de Figueiro, todas sus obras conocidas se distribuyen entre Madrid, Alcalá de Henares, Burgos, Segovia y otras localidades próximas. Pereira fue exclusivamente escultor, en piedra, alabastro o madera, no ocupándose nunca de la arquitectura de sus retablos ni del policromado. Tampoco se conocen relieves de su mano y su obra, aun trabajando para la corte, es casi exclusivamente religiosa, mencionándose tan sólo la ejecución de una escultura de "Neptuno" fuera de ese género.

Establecido en la corte desde joven, su obra revela un espíritu clásico. En sus figuras de canon alargado, expresión sobria y sereno patetismo, evitará siempre la crudeza y el gesto desgarrado. Su primera obra conocida es de 1624, para la fachada de la iglesia de la Compañía de Jesús de Alcalá de Henares, donde realizó varias figuras de santos. A la manera de la escuela castellana, las figuras son de volúmenes amplios y pliegues secos y quebrados, pero en el "San Bernardo" que realizó en fecha poco posterior para la fachada de las Bernardas de la misma ciudad se encuentran ya las características de su propio estilo, quizá influido por Alonso Cano. Estas obras hechas en Alcalá le proporcionarán de inmediato notable fama y los siguientes encargos irán en la misma línea: estatuas en madera para retablos y santos en piedra para ocupar las hornacinas de las fachadas de iglesias y otros edificios públicos, como la Cárcel de Corte, destacando entre las conservadas el "San Antonio de Padua" de la iglesia de San Antonio de los Portugueses en Madrid (1647) y, muy especialmente, el "San Bruno" de la Hospedería que la Cartuja de El Paular tenía en la calle de Alcalá de Madrid (1652), actualmente en la Real Academia de Bellas Artes de San Fernando, talla ante la que, según Antonio Palomino, acostumbraba a detenerse el rey Felipe IV.

En madera realizó una serie de imágenes de gran realismo y de extraordinaria intensidad expresiva, entre las que se pueden destacar el "San Marcos" de la parroquial de Martín Muñoz de las Posadas (Segovia), en actitud mística, el "San Antonio de Padua" del retablo mayor de la iglesia de San Antonio de los Portugueses en Madrid, 1631, o el "San Bruno" de la Cartuja de Miraflores, anterior a 1635. Muy notables son también una serie de Cristos crucificados, de cuerpo estilizado y rostro intensamente emotivo, encabezados, al parecer, por el "Crucifijo" de la parroquia del Sagrario de la catedral de Sevilla. Consta que en 1646 don Alonso de Aguilar, regidor de Segovia, encargó a Pereira otro Cristo que había de seguir el modelo del que anteriormente había realizado para el obispo de la misma ciudad castellana. Este segundo Crucificado ha sido identificado con el llamado "Cristo de Lozoya", actualmente localizado en la catedral de Segovia, que es, sin duda, el más célebre de la serie y aquel en el que Cristo se presenta con los brazos elevados en mayor tensión. Otro más, ricamente policromado, se encuentra en el Oratorio del Olivar de Madrid, diferente de los anteriores por la posición más abierta de los brazos. 

Suyas serán también las esculturas en madera que ocupan los altares de los machones en el madrileño Convento de San Plácido, con el ladeamiento de las cabezas y la estilización de los cuerpos que son características del maestro. Fueron célebres, además, algunas esculturas destruidas al estallar la guerra civil de 1936, entre ellas el "Cristo del Perdón" de los dominicos del Rosario de Madrid, según Palomino «cosa portentosa, a que ayudó mucho la encarnación, de mano de Camilo», del que existe una réplica, posiblemente del propio Pereira, en la capilla de los marqueses de Comillas en Cantabria, y la talla del santo titular en el retablo, labrado según trazas de Alonso Cano, de la iglesia de San Andrés. Para la capilla de San Isidro en la misma iglesia madrileña, iniciada su construcción en 1657, ejecutó una serie de santos labradores que, ya en el reinado de Carlos III, tras la expulsión de los jesuitas, pasaron a la iglesia del Colegio Imperial repintadas de blanco conforme a la moda neoclásica, resultando igualmente destruidas en 1936. 

Otras obras que se pueden relacionar con Pereira son una Inmaculada Concepción en el convento de Agustinas Recoletas de Pamplona, el "Ecce Homo" de las Carmelitas de Larrea (Vizcaya) y un crucifijo conservado en la iglesia de San Juan de Rabanera de Soria, lleno de tensión barroca y elevando su mirada hacia lo alto.



</doc>
<doc id="4538" url="https://es.wikipedia.org/wiki?curid=4538" title="Francisco Pérez Sierra">
Francisco Pérez Sierra

Francisco Pérez Sierra (c. 1627-1709) fue un pintor barroco español.

Nacido en Nápoles fue, según Antonio Palomino, hijo de un militar español casado con una hija del gobernador de Calabria. Según el mismo biógrafo, fue discípulo de Aniello Falcone en Nápoles y de Juan de Toledo en Madrid, al tiempo que servía de paje a don Diego de la Torre, secretario en la corte para los asuntos de Italia.

Poco se sabe de su pintura, aparte de lo que cuenta de él Palomino, quien asegura que fue experto en pintar batallas, países y "cabañas", además de realizar algunas obras religiosas, entre las que destacaba una de San Francisco de Paula para el desaparecido convento de la Victoria de Madrid. También pintó al fresco y al temple en colaboración con Francisco Rizi y Juan Carreño de Miranda en la Huerta de Sora, propiedad del marqués de Eliche y debió de alcanzar cierto prestigio como pintor de perspectivas fingidas para altares y otras arquitecturas efímeras destinadas a solemnidades festivas. De todo ello únicamente se conserva una "Inmaculada Concepción" firmada en 1655 en el convento de las Trinitarias Descalzas de Madrid, obra de composición compleja con fuertes resonancias de Rizi manifiestas también en dos óleos propiedad del Museo del Prado, procedentes del convento de Santa María de los Ángeles de Madrid: "Santa Ana conduciendo a la Virgen" (depositado en San Jerónimo el Real) y "San Joaquín" (Museo de Bellas Artes de Granada).

Palomino, que lo conoció, asegura que tras obtener por mediación de Diego de la Torre el cargo de agente general de los Presidios de España y disponiendo de una considerable hacienda, se aplicó por gusto «a pintar flores y frutas por el natural (con ocasión de un muy pulido jardín que tenía en su casa) que era en la calle de las Infantas». Un par de floreros, a él atribuidos, propiedad del Patrimonio Nacional, pueden servir de testimonio de su obra en este terreno, en el que mereció los elogios de sus contemporáneos. Murió a edad avanzada en Madrid, en 1709.




</doc>
<doc id="4540" url="https://es.wikipedia.org/wiki?curid=4540" title="Neocubismo">
Neocubismo

Neocubismo (o cubismo neoclásico) es un término de la historiografía del arte por el que se conoce el estilo de pintura en que empezaron a expresarse los artistas de la primera vanguardia en España en torno a los años veinte. Mientras que el cubismo reduce la naturaleza a las formas geométricas que el artista considera esenciales o más significativas, el neocubismo perfila enérgicamente los estilos y los modos que caracterizan a la vanguardia europea; mezcla de simbolismo, surrealismo y realismo.

La situación del arte en España en esos años era muy distinta a la del resto de Europa:
Los principales representantes en España de esta tendencia fueron, entre otros, los pintores Daniel Vázquez Díaz y Francisco Cossío, o los escultores Alberto y Ángel Ferrant. También se consideran neocubistas algunas etapas de la obra de Salvador Dalí y Joan Miró.

En los años cuarenta, esta tendencia neocubista se conoce también como figuración esquemática y sus elementos formales están tomados del cubismo y del "Art Decó". Además acusa la influencia de Henry Moore y de los figurativos italianos Marini, Campigli y Morandi. Esta tendencia es desplazada por el informalismo de los años cincuenta, y los artistas que la representaban optaron por distintas trayectorias, dentro de la pintura abstracta o la figurativa. Entre otros cabe señalar a Cristino de Vera, Adolfo Estrada, Gloria Merino, Carlos Pascual de Lara, Vaquero Turcios, Juan Méjica García, Manuel Hernández Mompó, Antoni Tàpies, Modesto Ciruelos y Hernando Viñes.


</doc>
<doc id="4541" url="https://es.wikipedia.org/wiki?curid=4541" title="Derecho informático">
Derecho informático

El derecho informático o derecho de la informática es un conjunto de principios y normas que regulan los efectos jurídicos de la relación entre el derecho y la informática.
Esta actividad involucra aquello referente a la contratación informática, delitos cometidos mediante su uso, relaciones laborales a que ella da lugar, litigios sobre la propiedad de programas o datos, etcétera.

El término Derecho Informático ("Rechtsinformatik") fue acuñado por el Dr. Wilhelm Steinmüller, académico de la Universidad de Ratisbona de Alemania, en los años 1970. Sin embargo, no es un término unívoco, pues también se han buscado una serie de términos como Derecho Telemático, Derecho de las Nuevas Tecnologías, Derecho de la Sociedad de la Información, Iuscibernética, Derecho Tecnológico, Derecho del Ciberespacio, Derecho de Internet, etcétera. En la actualidad, el término Derecho de las Tecnologías de la Información y Comunicación ha tomado fuerza en América Latina, llegando incluso a privilegiarse sobre el uso de Derecho Informático.

Los conceptos de tecnología y sociedad de información son antecedentes necesarios del derecho informático, con la finalidad de regular el comportamiento en un ámbito tecnológico. Actualmente el derecho informático no es muy específico en sí, sino que lo abordan las materias de derecho penal, derecho civil y derecho comercial. 

Desde la aparición de la computación como un fenómeno, ésta ha sido benéfica en las distintas áreas de la ciencia y la cultura, debido a las TICs quienes producen los intercambios de información. Sin embargo, la aparición de actos delictivos a través de la informática ha devenido en la creación de esta rama del derecho.

En derecho penal se afronta un reto en cuanto la sanción y clasificación de los delitos, ya que el delito se define como una conducta que es sancionada por las leyes de defensa social. No obstante, debido a su novedad, el derecho aún no prevé muchos actos informáticos ilegales como delitos o el castigo por la misma causa.

Según el Dr Zelarayan Juri, Federico: este derecho tiene surgimiento en el medioevo con el invento de la aguja de tejer, el nombre proviene de la aguja que significa recto, por lo tanto derecho e informático proviene de los puntos, la manera en que los hacemos, en su forma de transmitirlos, lo cual es información.

El derecho informático cuenta, al igual que las demás ramas de derecho, con sentencias de tribunales y razonamientos de teóricos del derecho. Las fuentes del derecho informático afectan a las ramas tradicionales del derecho:

En el Derecho Público

En derecho privado

Lo que aún se discute en la actualidad es si este derecho es una nueva disciplina o es una serie de normas dispersas que engloba a varias disciplinas.

El Derecho Informático afecta a distintas disciplinas dentro del Derecho. Este hecho, suscita un debate teórico sobre si estamos ante una nueva disciplina jurídica o si, por el contrario, se trata de un sector de normas dispersas pertenecientes a diferentes disciplinas jurídicas. 

Para poder hablar de autonomía de una rama del derecho se precisa la existencia de una legislación específica (campo normativo), un estudio particularizado de la materia (campo docente), investigaciones y doctrinas que traten la materia (campo científico) e instituciones propias que no se encuentren en otras áreas del derecho (campo institucional), con la finalidad de que se de un tratamiento específico de estos conocimientos determinados.

Una parte de los autores defienden que en el Derecho Informático existe legislación específica basada en leyes, tratados y convenios, que protegen al campo informático con la finalidad del control y aplicación lícita de los instrumentos informáticos (campo normativo). Dispone, además, de instituciones propias que no se encuentren en otras áreas del Derecho (campo institucional) tales como el contrato informático, el documento electrónico, el comercio electrónico, delitos informáticos, firmas digitales, habeas data, libertad informática, entre otras, que llevan a la necesidad de un estudio particularizado de la materia (campo docente), dando como resultado la investigaciones, doctrinas que traten la materia (campo científico). Es, por tanto, un Derecho autónomo con instituciones propias que se encarga de brindar soluciones legales a los problemas planteados por el avance científico en el ámbito de su competencia. 

De otro lado, se sostiene la postura que anula la autonomía del Derecho Informático desde el punto de vista de que en cada rama jurídica la actividad informática se encuentra presente, rechazando así la integración de normas en un cuerpo aislado. Niega la autonomía del Derecho Informático en virtud de que no existe claridad respecto a su área jurídica de influencia; es decir, como el Derecho Informático tiene relación con otras disciplinas jurídicas como el derecho civil, penal, laboral, administrativo, etc., y es a través del espectro normativo de estas la forma en que pueden incluirse las conductas y problemáticas jurídicas del impacto tecnológico. Por último, esta corriente argumenta la falta de autonomía del Derecho Informático en virtud de su constante y necesaria recurrencia a los principios jurídicos de otra rama para la solución de los casos concretos.








Publicaciones periódicas, por orden de aparición:

Difusiones periódicas, por orden de aparición:

En cuanto a estudios formales de la disciplina en Iberoamérica, el pionero fue el desaparecido Máster en Informática y Derecho (1994-2004) de la Universidad Complutense de Madrid, liderado por Emilio Suñe Llinás.

Desde más de 10 años, la Universidad París-Sur con el Instituto de Derecho del Espacio y de las Telecomunicaciones (IDEST) proponen un Máster 2 en Derecho de Actividades Espaciales y de Telecomunicaciones. Este Máster es sostenido por numerosas empresas del sector de las telecomunicaciones y espacial.

Más tarde, en 2007, la Universidad de Cuenca (Ecuador) inició una Maestría de Derecho Informático mención en Comercio Electrónico y la siguió la Universidad de la Américas-UDLA (Ecuador) y el CETID con la Universidad de Cuenca (Ecuador) con una Maestría en Derecho Informático, mención en Comercio Electrónico. 

En México el posgrado pionero en esta materia es el de INFOTEC, denominado "Maestría en Derecho de las Tecnologías de la Información y Comunicación (MDTIC)", creada con el apoyo del grupo consultor de Lex Informática Abogados, S.C. y el Instituto de Investigaciones Jurídicas de la UNAM. La Universidad Panamericana Campus Aguascalientes cuenta con un programa similar, la "Maestría en Derecho de la Propiedad Industrial, Derechos de Autor y Nuevas Tecnologías".

El Diplomado en Derecho de las Tecnologías de la Información y Comunicación que se imparte totalmente en línea con opción a obtener la Certificación de "Abogado Digital" es el que ofrece desde 2018 la Academia Mexicana de Derecho Informático, A.C. (AMDI).

Destacan a nivel internacional el "Advanced Master of Laws (LL.M.) in Information & Communications Technology Law" de la Universidad Católica de Lovaina (Bélgica), el "Máster Universitario en Derecho de las Nuevas Tecnologías" de la Universidad Pablo de Olavide, de Sevilla (España), el "Máster en Derecho de las Telecomunicaciones y Tecnologías de la Información" de la Universidad Carlos III de Madrid (España), el "Master in Diritto delle Nuove Tecnologie e Informática Giuridica" de la Universidad de Bolonia (Italia), el "Master of Laws in Information and Communication Technology Law" de la Universidad de Oslo (Noruega), el "Master of Law and IT" de la Universidad de Estocolmo (Suecia), entre otros.

Como programas de postítulo y/o especialización, la Pontificia Universidad Católica Argentina Santa María de los Buenos Aires (UCA) estableció en forma pionera en 1997 una Especialización en Derecho de Alta Tecnología; la Universidad de Chile ofrece desde 2003 un Magíster y Diplomados en Derecho de la Informática y de las Telecomunicaciones y la Universidad Externado de Colombia, en asociación con la Universidad Complutense de Madrid, ofrece desde el 2007 un programa de especialización y de Maestría en Derecho Informático y de las Nuevas Tecnologías desde el año 2015.

Para algunos ejemplos de situaciones en las que se haga uso del derecho informático son:

En Perú el derecho informático está en pleno desarrollo, actualmente a consecuencia de la utilización de las TIC y la aplicación del Gobierno Electrónico por parte de los organismos públicos, ha dado lugar a que muchos profesionales se interesen por especializare en esta rama del Derecho.

"Asimismo recientemente se ha incluido en el Código Penal Peruano la tipificación de los delitos informáticos."

Asimismo dentro de la estructura orgánica del Estado Peruano, existe La Secretaría de Gobierno Digital (SeGDi) quien es la encargada de: formular y proponer políticas nacionales y sectoriales, planes nacionales, normas, lineamientos y estrategias en materia de Informática y Gobierno Electrónico. Asimismo, es el órgano rector del Sistema Nacional de Informática y brinda asistencia técnica en la implementación de los procesos de innovación tecnológica para la modernización del Estado en coordinación con la Secretaría de Gestión Pública.





</doc>
<doc id="4549" url="https://es.wikipedia.org/wiki?curid=4549" title="25 de julio">
25 de julio

El 25 de julio es el 206.º (ducentésimo sexto) día del año en el calendario gregoriano y el 207.º en los años bisiestos. Quedan 159 días para finalizar el año.









</doc>
<doc id="4556" url="https://es.wikipedia.org/wiki?curid=4556" title="8 de julio">
8 de julio

El 8 de julio es el 189.º (centésimo octogésimo noveno) día del año en el calendario gregoriano y el 190.º en los años bisiestos. Quedan 176 días para finalizar el año.







</doc>
<doc id="4560" url="https://es.wikipedia.org/wiki?curid=4560" title="Giovanni Battista Piranesi">
Giovanni Battista Piranesi

Giovanni Battista Piranesi (Mogliano Veneto, cerca de Treviso, 4 de octubre de 1720 – Roma, 9 de noviembre de 1778) fue un arqueólogo, arquitecto, investigador y grabador italiano. Realizó más de 2000 grabados de edificios reales e imaginarios, estatuas y relieves de la época romana así como diseños originales para chimeneas y muebles.

Piranesi nació en Mogliano Veneto, que entonces pertenecía a la República de Venecia. Estudió Arquitectura en Venecia con su tío materno Matteo Lucchesi, que era "Magistrato delle Acque" en la ciudad. Allí descubrió las obras de Palladio, Vitruvio y algunos edificios de la antigüedad. Piranesi apenas llegó a ejercer como arquitecto (sólo se erigió un diseño suyo), si bien sus estudios le permitieron dibujar con mayor facilidad, e hizo gala de su formación firmando algunos grabados como "Piranesi architetto". 

Se trasladó en 1740 a Roma, junto a Marco Foscarini, enviado del papa en Venecia. Las ruinas del imperio romano encendieron su entusiasmo y la necesidad de representarlas. En aquella época, la arqueología no era aún una ciencia demasiado rigurosa, y en muchas ocasiones se trataba de simple saqueo. Combinando afán descriptivo y fantasía, Piranesi levantó acta de las ruinas romanas y de los hallazgos que se iban produciendo.

Conoció en Roma al erudito G. G. Bottari y aprendió la técnica del aguafuerte con Giuseppe Vasi, con quien firmó algunas imágenes. Sus primeros grabados fueron vistas de la ciudad, destinadas a guías ilustradas. En 1743 publicó su primera gran serie de estampas, "Prima Parte di Architettura e Prospettiva". Elaborada con apenas 23 años, desvela ya su maestría como grabador y su inventiva.

Abrió su taller frente a la Academia de Francia en Roma lo cual hizo que viviera en constante relación con los estudiosos de aquel país. Tuvo mucho éxito con sus grabados puesto que la mayoría de los visitantes que iban a Roma gustaban de volver con algún recuerdo, y sus grabados se imprimían en grandes tiradas que los hacían muy asequibles.

En 1761 se convirtió en miembro de la "Academia di San Luca". Murió en 1778 y fue enterrado en la única iglesia que construyó: Santa María del Priorato.

Sus entusiastas reproducciones e interpretaciones de antiguos monumentos romanos supusieron una importante contribución para la formación y desarrollo del neoclasicismo. En estos grabados se incluían imágenes fidedignas y exactas de las ruinas existentes, al igual que reproducciones imaginarias de antiguos edificios en las que la alteración de la escala y la yuxtaposición de elementos contribuyen a realzar el carácter de grandiosidad de los mismos.

Una de las primeras y más renombradas colecciones de grabados de Piranesi fueron sus Prisiones ("Carceri d'Invenzione", 1745-1760), en donde transformó las ruinas romanas en fantásticos y desmesurados calabozos dominados por enormes y oscuros pasadizos, empinadas escaleras a increíbles alturas y extrañas galerías que no conducen a ninguna parte. Estos grabados ejercieron una enorme influencia en el romanticismo del siglo XIX, jugando también un destacado papel en el desarrollo, ya en el siglo XX, del surrealismo e incluso en los decorados para el cine de terror.

También fue sumamente famosa su magna obra "Le Antichità Romane": más de 200 grabados en cuatro tomos, publicados en 1756. Incluye vistas de ruinas de monumentos funerarios de Roma y de sus alrededores así como detalladas ilustraciones del urbanismo romano, incluso del modo en que se adoquinaban las calles. Para crear esta obra, Piranesi exploró personalmente casi todos los yacimientos y ruinas, tomando con rigor medidas y apuntes para elaborar grabados muy precisos. Fue una labor titánica que requirió diez años de trabajo.

Los grabados de Piranesi, muchos de ellos de gran formato y ordenados en libros, se exportaron rápidamente a Inglaterra y otros países, a modo de souvenirs del "Grand Tour", antecedente del moderno turismo cultural. Esas láminas influyeron en la arquitectura palaciega, especialmente en las casas campestres inglesas.

Muchas planchas del artista se siguieron imprimiendo hasta principios del siglo XIX en París; primero las explotó su hijo Francesco Piranesi y a su muerte pasaron al taller de Firmín Didot. En 1839 estas matrices de cobre fueron adquiridas por emisarios del papa Gregorio XVI con destino a la "Calcografia Camerale" fundada por Clemente XII, antecesora de la actual "Calcografia Nazionale de Roma", dependiente del "Istituto Nazionale per la Grafica", donde aún se conservan.
Existen grabados de Piranesi en casi todas las bibliotecas antiguas de Europa. En España, destacan los fondos de la Biblioteca Nacional: más de 2000 estampas, tanto sueltas como en tomos encuadernados, las cuales han protagonizado una exposición en 2019 . También el Museo de Bellas Artes de Valencia cuenta con un rico repertorio: unas 880 estampas, casi todas adquiridas en el mismo siglo XVIII. 

Aún hoy los grabados de este artista son muy demandados y cotizados, en parte porque se siguen enmarcando y empleando como elemento decorativo en interiores de todos los estilos. 

Sus principales obras como grabador y teórico fueron:


Prianesi tan sólo vio construido uno de sus diseños arquitectónicos: la iglesia de Santa María del Priorato en Roma, sede de los caballeros de la Orden de Malta, así como la plaza que da acceso desde el Aventino.



</doc>
<doc id="4562" url="https://es.wikipedia.org/wiki?curid=4562" title="Fernando Fader">
Fernando Fader

Fernando Fader (Burdeos, Francia, 11 de abril de 1882-Loza Corral, Córdoba, Argentina, 28 de febrero de 1935) fue un pintor y dibujante argentino nacido en Francia, principal seguidor del impresionismo alemán en su país.

Fernando Fader nació el 11 de abril de 1882, en Burdeos, Francia. En 1898 realizó sus primeras obras pictóricas, entre las que destaca el óleo "El viejo piojoso".
Realiza estudios primarios en Francia y estudios secundarios en Alemania, en la Realschule del Palatinado del Rhin. Allí también estudia pintura con Heinrich von Zügel (1850-1941), un partidario de la pintura al aire libre, cuyos ejes temáticos eran los animales y la concepción naturalista del paisaje, derivados de la escuela de Barbizón. En 1900 ganó una medalla de oro por su pintura "detrás del arco iris". En 1904 vuelve al país y en 1906 realiza su primera muestra en Argentina, que no tuvo éxito. 'Participó del grupo Nexus -de temática localista y técnica que vacilaba entre el impresionismo y el academicismo- , con Collivadino, Ripamonte, Bernaldo de Quirós y, marginalmente, Emilio Caraffa. Nexus presentó tres exposiciones que abrieron el camino al Salón de Primavera de 1911. Impulsado por su otra pasión, la ingeniería, invirtió toda su fortuna en una empresa hidráulica que lo llevó a la quiebra. Este duro momento económico coincidió con los primeros síntomas de tuberculosis, que lo llevaron a buscar el clima suave de las sierras (de Córdoba) en 1917 (hay otra versión que sitúa esta mudanza en 1916). Pintó en las Sierras de Achala e Ischilín, en poblaciones como Candelaria, La Higuera, Pocho, San Pedro Norte y San Francisco del Chañar.'3'"/

Entre sus múltiples retratos, óleos y acuarelas se destacan "La mantilla", "La madre" y "La liga azul", estas últimas expuestas en el V salón Nacional de 1915, así como "La vida de un día", serie de ocho telas de (80x100 cm) en las que se representa el mismo paisaje con sus variantes de luz a lo largo del día, pintadas durante 1917. Esta serie está expuesta en el Museo Municipal de Bellas Artes de Rosario "Juan B. Castagnino". 

En su pintura se aprecian distintos periodos emocionales, como la etapa de interiores oscuros con predominio de los colores ocres y pardos y su otro momento de más luminosidad, donde la luz artificial cae sobre los objetos relacionando el color-luz con el objeto-luz. ""Sus paisajes serranos son uno de los momentos culminantes de la historia de la pintura en Córdoba. Su última obra es de 1931'
Sus obras pueden apreciarse entre otro lugares en:

En 1914 después de su quiebra, se instala en Buenos Aires y presenta dos obras en el Salón Nacional.Con "La mantilla" comparte el premio Adquisición con Ernesto de La Cárcova.
En la exposición Internacional de California gana el primer premio con "La comida de los cerdos"

3 "Los colores de un siglo- Grandes obras de la pintura de Córdoba” Fundación Benito Roggio, 1998, Edit. Arcángel Maggio, Bs.As. Arg.; p. 59/60

"Fader en el Fader" (1994) "200 obras en el Museo Emiliano Guiñazú de Mendoza" (catálogo).


</doc>
<doc id="4563" url="https://es.wikipedia.org/wiki?curid=4563" title="Winslow Homer">
Winslow Homer

Winslow Homer (Boston, 24 de febrero de 1836-Prouts Neck, 29 de septiembre de 1910) fue un pintor estadounidense.

Artista autodidacta, en 1857 empezó a trabajar como ilustrador de revistas, colaborador asiduo de "Harper's Weekly". Durante la Guerra Civil, Homer visitó en repetidas ocasiones el frente de Virginia donde habría de pintar su primer cuadro al óleo importante, "Los prisioneros del frente" (1866, Museo Metropolitano de Arte de Nueva York), obra notable por su fría objetividad y su vigoroso realismo.

En 1856 se trasladó a Francia durante un año pero, aunque su interés en las posibilidades pictóricas de la luz natural se desarrolla de forma paralela al de los primeros impresionistas, nunca sufrió la influencia directa del impresionismo o del arte francés.

En 1873 comenzó a utilizar la acuarela, medio de expresión tan importante en su obra como el óleo. Durante la década de 1870 los temas predominantes de sus obras fueron los de inspiración rural o idílica: escenas de la vida agrícola, niños jugando y escenas de lugares conocidos poblados de mujeres elegantes. De estas últimas el ejemplo más conocido es Long Branch, Nueva Jersey (1869, Museum of Fine Arts, Boston).

El año transcurrido en Inglaterra (entre 1881 y 1882), durante el cual Homer vivió en un pueblo de pescadores, provocó un cambio definitivo en la temática de su obra. A partir de entonces se concentró en escenas de la naturaleza a gran escala, en particular escenas marinas, de pescadores y sus familias. Después de fijar su residencia en solitario en Prout's Neck, en la costa de Maine (donde moriría el 29 de septiembre de 1910), produjo obras maestras del realismo tales como Eight Bells (1886, Addison Gallery, Andover, Massachusetts). En esta obra el dramatismo de la escena marina está imbuido de una cualidad épica y heroica que representa el tema dominante de su madurez: la lucha del hombre con las fuerzas de la naturaleza.

A partir de 1884, Homer pasó muchos inviernos en Florida, en las Bahamas y en Cuba, ejecutando gran parte de su obra plenairista, en especial acuarelas o incluso de pintura rápida, con un estilo muy avanzado para su época: fresco, suelto, espontáneo, casi impresionista, pero sin perder jamás su relación básica con el realismo americano. Falleció en Prouts Neck (Maine) el 29 de septiembre de 1910.



</doc>
<doc id="4564" url="https://es.wikipedia.org/wiki?curid=4564" title="Ricardo Yrarrázaval">
Ricardo Yrarrázaval

Ricardo Yrarrázaval (Santiago, 12 de octubre de 1931) es un pintor y ceramista chileno. El tema central de su obra es el hombre y su situación en la sociedad.

Nació en Santiago de Chile, 1931. Su vocación artística despertó a muy temprana edad. Después de enrolarse en un barco con destino a Egipto, viajó a Nápoles y a Roma, 

Más tarde, abandonó la capital italiana y marchó a Francia para finalmente instalarse en Vallauris. Allí entró en contacto con las técnicas de la cerámica, actividad a la que se dedicó durante varios años, sin nunca abandonar la pintura.

Su producción pictórica se caracteriza por su profunda e intensa búsqueda, tanto en lo formal como en lo temático. Después de una trascendental experiencia con la abstracción, que le valió el reconocimiento de una beca de la Fundación Guggenheim en 1966, (estadía de un año en Nueva York), en la década de 1970 su obra adquirió tintes surrealistas y comenzó a pintar en un estilo que algunos denominaron realismo subjetivo y que más tarde evolucionaría hacia lo objetivo. Los personajes de sus cuadros son figuras de volúmenes rotundos y formas distorsionadas que representan arquetipos de la sociedad moderna.

Su pintura más reciente se destaca por su irrenunciable búsqueda de nuevas técnicas, y una inconfundible relación con lo moderno y su vínculo con la sociedad actual, la figura del ser humano siempre al centro.

De los numerosos galardones que ha obtenido destacan el segundo premio de la Bienal de Lima (1968) y el primer premio de pintura del I Concurso de la Colocadora Nacional de Valores de Chile (1975).




</doc>
<doc id="4565" url="https://es.wikipedia.org/wiki?curid=4565" title="Jacob Jordaens">
Jacob Jordaens

Jacob Jordaens (Amberes, 19 de mayo de 1593-18 de octubre de 1678) fue un pintor barroco flamenco. Es el último gran maestro de la época en los Países Bajos, tras la muerte de Rubens (1640) y Van Dyck (1641). A diferencia de sus contemporáneos, nunca realizó un viaje formativo a Italia para conocer el arte clásico, y su carrera destaca por cierta indiferencia hacia las ambiciones cortesanas o intelectuales. Se le considera, por sus contados viajes fuera de los Países Bajos, como un pintor de considerable genio pese a su carácter local. 

Fue un pintor de éxito, grabador ocasional y notable diseñador de tapices. Al igual que Rubens, Jordaens fue un maestro de los tapices, las escenas mitológicas y las alegorías, y a partir de 1640 -año de la muerte de Rubens- fue el más notable pintor de Amberes, y como tal recibió numerosos encargos de cortesanos, familias adineradas y otros mecenas. Hoy, sin embargo, es más conocida su pintura de género, obras basadas en escenas costumbristas al modo de su contemporáneo Jan Brueghel el Viejo. Entre sus influencias se cuentan no solo pintores flamencos como Brueghel o el mencionado Rubens, sino también artistas del norte de Italia como Jacopo Bassano, Paolo Veronese o Caravaggio.

Jacob Jordaens nació en Amberes el 19 de mayo de 1593, primogénito entre los once hijos del rico mercader de lino Jacob Jordaens y Barbara van Wolschaten. Se sabe poco sobre su primera educación, pero se admite que debió recibir una formación aventajada como heredero de una rica familia. Esta idea se ve confirmada por su dominio de la gramática, su conocimiento del francés y su competencia en cuestiones mitológicas. Su familiaridad con los temas bíblicos se manifiesta en sus muchas pinturas de tema religioso, y su vinculación con estas cuestiones se ve confirmada por su tardía conversión del catolicismo al protestantismo. Al igual que Rubens estudió con Adam van Noort, quien fue su único maestro. Durante esta etapa Jordaens residió en la casa de Van Noort y estableció estrechos lazos con la familia. Después de ocho años de formación con Van Noort, se unió a la Guilda de San Lucas como "waterscilder" o "acuarelista". A la sazón éste era un medio que solía emplearse como base de los tapices y tal vez ello explica que no se conserven acuarelas del autor.
En 1616, el mismo año en que se le admitió en la hermandad, Jordaens contrajo matrimonio con la hija mayor de su maestro, llamada Anna Catharina van Noort, con la que tuvo tres hijos. En 1618 la pareja compró una vivienda en Hoogstraat, la calle donde se había criado. Hacia 1639 la amplió sobre la vivienda colindante, tal como había hecho Rubens dos décadas antes. Allí residió y trabajó hasta su muerte en 1678.

Jordaens nunca realizó el clásico viaje de aprendizaje a Italia para estudiar el arte clásico y renacentista. A pesar de esto, se esforzó notablemente para adquirir láminas de los grandes maestros italianos que podían encontrarse en aquel entonces en el norte de Europa. Así es como conoció a Tiziano, Veronese, Caravaggio y Bassano. Su obra, sin embargo, delata su fuerte arraigo como pintor local, y su apego a la pintura de género de artistas como Brueghel el Viejo, de carácter costumbrista y un tono más bien jocoso. La mayoría de sus encargos le llegaban de ricos mecenas flamencos y miembros del clero, si bien hacia el final de su carrera el maestro ya recibía encargos de cortesanos y gobiernos de toda Europa. Además de una considerable cantidad de pinturas al óleo, a lo largo de su carrera realizó numerosísimos tapices, una huella de su temprana vocación de acuarelista.

La importancia de Jordaens puede calibrarse por la cantidad de discípulos que tuvo: el registro de la Hermandad de San Lucas -el tradicional gremio de los pintores- apunta a unos quince entre 1621 y 1667, y a éstos hay que añadir otros seis aprendices que aparecen registrados como tales en documentos de la corte. Entre ellos se contaría su primo, y su propio hijo Jacob. Como Rubens y otros artistas de la época, el estudio de Jordaens dependía en gran medida de sus ayudantes para la producción de sus pinturas. Aunque no muchos de sus discípulos adquirieron fama, tener un cargo en el taller de Jordaens era una posición codiciada por artistas de toda Europa.

Pedro Pablo Rubens fue tal vez el pintor que más influyó en la obra de Jordaens. El maestro le encargó algunas reproducciones de sus bocetos, que tal vez explican su afinidad por una paleta cromática cálida, su común interés por el naturalismo y su similar adaptación del chiaroscuro y el tenebrismo italianos. Jordaens, que en vida de Rubens solo tuvo un moderado éxito como pintor de retratos, brillaba especialmente en la representación de personajes comunes de la vida cotidiana, tanto en sus pinturas de tema pastoril, de raíces profundamente clásicas, como en escenas morales similares a las que había popularizado Jan Steen. Aunque no fue un especialista, Jordaens solía repetir ciertos temas a lo largo de sus obras, con la intención de personalizar caracteres que van apareciendo a lo largo de su obra con diferentes edades, formando parte de la abigarrada multitud que rodea la mesa de un gran banquete. En cuanto a los temas mitológicos, Jordaens se ciñó a los motivos elaborados por Rubens, si bien filtrándolos a través de su tendencia a la personalización y el costumbrismo realista, así como un cierto aire burlesco que impregna incluso sus obras de temática mitológica o religiosa. El "Prometeo", de c. 1640 es un ejemplo claro de la influencia combinada de Rubens y Frans Snyders.

Además de ser conocido como notable retratista, Jordaens realizó obras de temática religiosa, obras de carácter alegórico y mitológico, así como un buen número de grabados. Aunque principalmente fue un pintor histórico, también trabajó sobre proverbios flamencos, del tipo ""el viejo canta, el joven parlotea"", o representaciones de festivales flamencos (""El Rey bebe""). Algunas de sus obras prueban su interés por la pintura de animales: en su obra figuran con frecuencia imágenes de caballos, vacas, aves de corral, gatos, perros y ovejas; generalmente eran modelos del natural. A lo largo de toda su vida realizó gran cantidad de apuntes del natural de animales y personas. Después de la muerte de Rubens, acaecida en 1640, Jordaens se convirtió en el pintor más popular de Amberes, y a partir de entonces empezó a recibir encargos de las principales cortes de Europa, especialmente de los reinos nórdicos. Los mismos herederos de Rubens solicitaron su ayuda para terminar un cuadro de "Hércules liberando a Andrómeda" (Museo del Prado) encargado por el rey Felipe IV de España.

Entre 1635 y 1640, aquejado Rubens por la gota, se concedió a Jordaens autorización para utilizar los bocetos del maestro flamenco para continuar la decoración de la entrada triunfal del Cardenal-Infante Ferdinando, el recién nombrado gobernador de las posesiones españolas en los Países Bajos. Aunque la obra se perdió finalmente, Jordaens también recibió el encargo de concluir las pinturas que decoraban la cámara de la Reina en Greenwich, encargo que igualmente había sido encomendado inicialmente a Rubens, quien rechazó el trabajo a causa de sus crecientes problemas de salud.

Por último debemos destacar que Jordaens también tuvo un papel importante en la decoración de la Torre de la Parada, erigida entre 1636 y 1641. Dos de las obras atribuidas a Jordaens son ""Apolo y Pan"" (1637, realizada según los esbozos de Rubens) y ""Vertumno y Pomona"" (1638). A esta producción se sumarían posteriormente una ""Caída de los Titanes"", el ""Matrimonio entre Peleo y Tetis"" y el ""Cadmo sembrando los dientes del dragón"". En 1661 recibió el encargo de elaborar tres tondos de considerable tamaño para el ayuntamiento de Ámsterdam.

Profesar el protestantismo estaba prohibido en Amberes, a la sazón territorio sujeto a la soberanía del rey de España. Sin embargo, en la última etapa de su vida, Jordaens se hizo protestante, si bien continuó recibiendo -y realizando- los encargos de las ricas iglesias católicas de su región. Por otra parte, la redacción de algunos textos heréticos publicados entre 1651 y 1658 le costaron pagar una multa de 200 libras y 15 chelines. En 1877 se levantó en Putte un monumento conmemorativo que contenía las lápidas del pintor y sus colegas Simon de Pape (I) y Adriaen van Stalbemt, en el lugar donde se encontraban la iglesia y el cementerio protestantes antes de su demolición. 

Jordaens falleció en octubre de 1678 a consecuencia de una misteriosa enfermedad endémica de Amberes -llamada 'zweetziekte' o 'polderkoorts', en neerlandés-, en el mismo día que se había llevado a su hija soltera Elizabeth, quien permanecía en la casa familiar. Sus cuerpos fueron enterrados juntos bajo una lápida del cementerio protestante de Putte, una aldea al norte de la frontera con Bélgica, donde ya descansaban los restos de su fallecida mujer Catharina. Un año después de su muerte, el hijo de Jordaens realizó una generosa donación de ""veinticinco libras flamencas a la Camer van den Huysarmen de Amberes"", a las que añadió "El lavatorio y unción del cuerpo de Cristo", que fue reubicado en un orfanato para niñas. Aparentemente esta donación se realizó en correspondencia con el testamento de Jordaens, aunque lamentablemente no se conserva este documento. Incluso sin haberse encontrado el testamento de Jordaens, su gentileza fue ampliamente reconocida por todos los que le conocían.

Hacia el final de su carrera (1652-1678) el talento creativo y artístico del pintor se fue recrudeciendo. De su brillante paleta juvenil pasó a tonos acres y terrosos, aplicando tan poca pintura que podía verse la tela debajo. Salvo contadas excepciones -como la "Historia de Psyche" que realizó para su propia casa-, su dedicación profesional se fue relajando y en cierto modo sus pinturas parecen afectadas de cierta fatiga.

"La adoración de los pastores" (1616 - 1618) presenta a la Virgen amamantando al niño Jesús mientras unos pastores, de aspecto flamenco, se postran en adoración. La escena muestra a cinco figuras que, a excepción del niño, aparecen recortadas a la altura de la cintura, subrayando la intimidad de la escena. 

Antes de 1616, Jordaens había estado interesado por la paleta cromática manierista, clara y brillante. Sin embargo, con esta pintura, empieza a utilizar la luz, y no el color, como el medio principal de modelar las figuras en el espacio, haciendo evidente su interés por Caravaggio. La principal fuente de luz en "La adoración..." es una vela sostenida por San José. El efecto puede ser una posible muestra de la influencia de pintores como Adam Elsheimer, conocido por situar la principal fuente de luz en el centro de sus composiciones. Otra posible prueba de la influencia de Caravaggio puede encontrarse en el enfoque "realista" que Jordaens aplica al tema: La Virgen y el Niño se presentan de manera sencilla, casi rústica, lejos de los arquetipos idealizados que solían seguirse en las representaciones de este mismo tema.

Jordaens realizó al menos otras seis versiones del tema de la "Adoración". Generalmente, la composición incluye a media docena de figuras agrupadas y recortadas de diversas maneras, que centran la atención del observador en la Sagrada Familia. Este tipo de composición pretendía intensificar el aspecto narrativo de la escena y acentuar la expresión individual de los personajes.

Esta escena en particular, de la que Jordaens realizó multitud de versiones, ilustra una fábula moralizante de Esopo. La historia comienza con el encuentro entre un hombre y un sátiro. Un frío día, mientras conversaban, el hombre se llevó los dedos a la boca y sopló. Cuando el sátiro le preguntó por ese gesto, el hombre le contestó que era para calentarse las manos. Después, cuando se sentaron a comer, el hombre se llevó el plato caliente a la boca y volvió a soplar. Cuando el sátiro le preguntó el motivo, le contestó que era para enfriar la comida. El sátiro entonces replicó, ""no puedo considerarte un amigo, si dices que el mismo soplo calienta y enfría"". La historia pretende ilustrar sobre la dualidad de la naturaleza humana, aunque algunos críticos sostienen que el interés del autor estaba más en el tema campesino que en la moraleja de la fábula.

El momento particular representado en la pintura es cuando el sátiro declara su desconfianza en el hombre. El hombre aún está comiendo, cuando el diablo se incorpora, alzando la mano, preparándose para salir de la casa. Jordaens escogió situar la escena en el interior de una granja abundante en animales: un toro, un perro, un gato y un gallo se acomodan sobre el mobiliario. Del mismo modo, alrededor de la mesa se reúnen figuras de todas las edades: hay un joven detrás de la silla del hombre, una vieja sosteniendo a un niño pequeño, y una joven que se asoma tras el hombro del sátiro.
Es característico del estilo pictórico de Jordaens el modo en que las figuras se empujan hacia el centro de la composición, amontonándose en un pequeño espacio. Jordaens sigue las técnicas tenebristas del claroscuro para definir una iluminación dramática, que resalta algunas figuras de la escena, como el bebé que descansa en el regazo de la anciana. Además, Jordaens recrea efectos naturalistas, como la suciedad en el pie del campesino sentado en primer plano, muy característico del estilo tenebrista flamenco de su época. Jordaens recreó otras versiones de este tema entre 1620 y 1621. En esta versión, parece haber situado a la niñera para "El sátiro y el campesino" del mismo modo en que aparecía en "La adoración de los pastores", y a juzgar por la gran cantidad de copias que se han encontrado —todas ellas sin el sello del maestro-, se cree que Jordaens había utilizado este cuadro como ejercicio de práctica para sus asistentes y alumnos en el taller.

En España existe un interesante conjunto de sus obras. El Museo del Prado conserva once obras de su pincel, ocho de las cuales son de carácter mitológico, dos costumbristas y una religiosa, además de un dibujo y una aguada. El Museo Thyssen-Bornemisza cuenta con una "Sagrada Familia con un ángel", con participación de su taller. La Real Academia de Bellas Artes de San Fernando posee una "Diana y Calisto" y en el Museo Lázaro Galdiano se le atribuye un lienzo con "Dos Angelotes abrazados". El Museo de Bellas Artes de Bilbao guarda un fragmento de "La infancia de Júpiter" titulado: "Sátiro tocando la flauta". y una "Cabeza de campesino"
Los diseños de tapices representan una parte muy significativa de su obra. Ricas tapicerías, que se contaban entre las obras más lucrativas realizadas entre el Renacimiento y el Barroco, cubrían los salones de las lujosas mansiones de la aristocracia europea desde el siglo XIV. Artistas como Jacob Jordaens, Pedro Pablo Rubens o Pietro da Cortona solían realizarlos por encargo, representando a sus ricos mecenas en escenas mitológicas o históricas cargadas de simbolismo y significación propagandística. Jordaens se convirtió en un especialista en este tipo de obras, y su dedicación y habilidad le ganaron un buen número de clientes. Se le consideró uno de los mayores diseñadores de tapices de su época.

El proceso de creación de tapices comenzaba con un boceto preliminar de la obra. El dibujo se transfería a un formato mayor, pintado al óleo sobre cartón, que era entregado al molino donde se transformaba en una pieza de tela. Jordaens también solía ejecutar pequeños bocetos acuarelados de sus composiciones, aunque hacia el final de su carrera sustituyó el cartón por el mismo lienzo como formato final. Sus tapices estaban concebidos para poder ser transportados por sus ricos propietarios, que solían llevarlos en sus viajes o campañas militares como símbolo de su categoría. El campo de temas a tratar era muy amplio, y pasaba desde la mitología o la vida rural hasta la historia de Carlomagno. Algunos expertos subrayan que un rasgo característico de la producción de Jordaens en esta área era la composición con masas de personajes, aplanados por el formato bidimensional de la tela, que enfatizaba fuertemente las tramas de la textura. De algún modo, Jordaens trasladó a los tapices su gusto por incluir multitud de caracteres en sus obras, ya fuesen tapices o pinturas.

El boceto para "interior de una cocina", que aquí presentamos, es un ejemplo de un estado de trabajo típico en la obra de Jordaens. Utilizando tinta ocre, aplicaba el color sobre una mancha de tiza negra, esbozando sobre el papel la composición final de la naturaleza muerta sobre la mesa y la disposición de las figuras. El resultado final difería ligeramente, pero como algunos autores han señalado la obra se inspiraba, en sus estados iniciales, en las naturalezas muertas del artista de Amberes Frans Snyders, muy afín a los intereses de Jordaens.

Algunos críticos han señalado que la obra de Jordaens continuó estilísticamente el "pictoricismo" característico de los dibujos de Rubens o Van Dyck en sus propios bocentos. Actualmente, los dibujos atribuidos al maestro rondan los 450, aunque lo parecido de sus técnicas hace que exista cierta disensión entre los expertos a la hora de distinguir su obra de la de Rubens. De cualquier modo, Jordaens y sus coetáneos fueron los principales exponentes de la tendencia flamenca hacia la realización de bocetos y apuntes preparatorios de menor escala que la imagen definitiva. Jordaens fue un consumado dibujante que no dudó en utilizar técnicas pictóricas -como el gouache o la acuarela- en la ejecución de sus bocetos. Su obra en papel también se caracteriza por su economía de medios, y no era raro que añadiese tiras de papel, recortase secciones del formato o pegase fragmentos aislados (al modo de un "collage") sobre el papel, hasta lograr el efecto deseado.

El tema de este dibujo (de fecha desconocida) ha sido objeto de cierta controversia. Este retrato de una mujer desnuda a lomos de un toro sería muy probablemente una representación del conocido tema mitológico de "el rapto de Europa", que presenta a Júpiter metamorfoseado en toro. No obstante, algunas teorías lo interpretan como una alegoría del mes de abril: según esa lectura, el toro representaría el signo zodiacal de Tauro, mientras que la mujer, que sostiene un ramo de flores, se identificaría con Flora, la diosa de la Primavera. Las figuras que la escoltan serían por tanto Ceres, la divinidad de la agricultura, y Sileno, instructor y consejero de Baco.

La faceta de Jordaens como grabador no es conocida entre el público no iniciado y apenas suele merecer atención en las monografías de arte, si exceptuamos los libros específicos sobre grabado antiguo. El experto Hollstein asignaba a Jordaens siete imágenes grabadas, varias de ellas con la fecha 1652; autoría que secunda en tiempos modernos (1993) el experto D'Hulst.

Los siete grabados ostentan inscrito el nombre de Jordaens como «"inventor"» (diseñador de la imagen), lo que da pie a pensar que el trabajo de grabar las matrices lo hizo otra mano; pero la convicción general es que él hizo todo el trabajo. Están hechos al aguafuerte con un resultado digno en cuanto a dibujo, si bien carecen de la destreza y expresividad de los aguafortistas más celebrados como Rembrandt y José de Ribera. 

Sus temas son variados y Jordaens ya los había plasmado casi todos en pinturas: "Cristo expulsando a los mercaderes del templo", "Lamentación ante Cristo muerto", "Juno sorprendiendo a Júpiter e Ío", "Mercurio matando a Argos", "Júpiter alimentado por la cabra Amaltea", "La huida a Egipto" y un tema rural, "Granjero agarrando a una vaca por la cola" (antes identificado como "Caco robando el rebaño de Gerión"). Ejemplares de estos grabados se conservan en museos de medio mundo: Museo Británico de Londres, National Gallery de Washington, Art Institute de Chicago...



</doc>
<doc id="4570" url="https://es.wikipedia.org/wiki?curid=4570" title="José Hierro">
José Hierro

José Hierro del Real (Madrid, 3 de abril de 1922 - ibídem, 21 de diciembre de 2002), conocido como José Hierro o Pepe Hierro, fue un poeta español. Pertenece a la llamada primera generación de la posguerra dentro de la llamada poesía desarraigada. 

En sus primeros libros, Hierro se mantuvo al margen de las tendencias dominantes y decidió continuar la obra de Juan Ramón Jiménez, Antonio Machado, Pedro Salinas, Gerardo Diego e, incluso, Rubén Darío. Posteriormente, cuando la poesía social estaba en boga en España, hizo poesía con numerosos elementos experimentales ("collage" lingüístico, monólogo dramático, culturalismo...).

Nació en Madrid en 1922, aunque la mayor parte de su juventud la pasó en Cantabria, puesto que su familia se trasladó a Santander cuando José contaba apenas dos años. Allí cursó la carrera de perito industrial, pero se vio obligado a interrumpirla en 1936, al comienzo de la Guerra Civil Española.

Al finalizar la guerra fue detenido y encarcelado por pertenecer a una "organización de ayuda a los presos políticos", uno de los cuales era su propio padre, Joaquín Hierro, un funcionario de Telégrafos que el 18 de julio de 1936 interceptó el cable con que la Capitanía Militar de Burgos quería sublevar a la guarnición de Santander, pagándolo con la cárcel. Su hijo también fue a prisión por sacar información de la misma cuando lo visitaba. Pasó cinco años encarcelado y fue liberado en enero de 1944 en Alcalá de Henares. Hasta 1946 vivió en Valencia. Allí, en el Café El Gato Negro, participó en una tertulia literaria a la que asistían [Ricardo Blasco], Angelina Gatell, Alejandro y Vicente Gaos, y Pedro Caba Landa, entre otros. Desempeñó entonces diversos oficios "pane lucrando". En 1948, en el "Diario Alerta" de Santander, hizo su primera crítica pictórica, sobre la obra del pintor burgalés Modesto Ciruelos (íntimo amigo que falleció también en 2002), labor que continuó ejerciendo en distintos medios de comunicación, especialmente en Radio Nacional de España y el "Diario Arriba" de Madrid. En 1949 contrajo matrimonio con María de los Ángeles Torres. Fundó la revista "Proel", junto con Carlos Salomón y hasta 1952 dirigió las publicaciones "Cámara de Comercio" y "Cámara Sindical Agraria", para instalarse al fin en Madrid, donde reanudó su carrera de escritor. Trabajó en el CSIC y en la Editora Nacional. Colaboró en las revistas poéticas "Corcel", "Espadaña", "Garcilaso. Juventud creadora", "Poesía de España" y "Poesía Española", entre otras. Participó en los Congresos de Poesía de Segovia, (del 17 al 24 de junio de 1952) y Salamanca (5 de julio de 1953). En 1995 es nombrado Doctor Honoris Causa por la Universidad Internacional Menéndez y Pelayo de Santander. Además en 1998, recibe, como reconocimiento final a su grandísima carrera, el Premio Cervantes. Fue elegido miembro de la Real Academia Española en 1999. En 2002 es nombrado también Doctor Honoris Causa de la Universidad de Turín. 

Fallece el 21 de diciembre de 2002 a los 80 años de edad en Madrid. Sus cenizas se honran en el Pabellón de Santanderinos Ilustres situado en la entrada del Cementerio de Ciriego de la capital cántabra desde el 28 de marzo de 2003. 

Poseía la curiosa superstición de no poder escribir nunca en su propia casa; era normal verlo en la cafetería de Avenida Ciudad de Barcelona, en Madrid; en ella y en otros cafés escribió toda su obra. Era sin embargo un trabajador lento y minucioso: algunos de sus poemas tardaron años en encontrar la forma definitiva.

José Hierro fue Premio Adonáis en 1947, Premio Nacional de Poesía (1953 y 1999), Premio de la Crítica (1958 y 1965), Premio de la Fundación Juan March (1959), Premio Príncipe de Asturias de las Letras en 1981, Premio Fundación Pablo Iglesias en 1986, Premio Nacional de las Letras Españolas en 1990, Premio Reina Sofía de Poesía Iberoamericana en 1995, Premio Cervantes y de nuevo el Premio de la Crítica en 1998, Premio Europeo de Literatura Aristeión, Premio Francisco de Quevedo y el Premio Ojo Crítico Especial por la belleza de su obra en 1999.

Fue declarado Hijo Adoptivo de Cantabria en 1982. En 2002 el Ayuntamiento de Madrid le concedió la Medalla de Oro de la ciudad. El 25 de abril de 2008 la ciudad de Santander le rindió homenaje colocando un busto del poeta en el Paseo Marítimo, junto a Puertochico, inspirado en los versos de uno de sus poemas sobre la bahía: "Si muero, que me pongan desnudo, desnudo junto al mar. Serán las aguas grises mi escudo y no habrá que luchar". 

En San Sebastián de los Reyes (Madrid) también existe un busto del poeta frente al edificio que alberga la Universidad Popular José Hierro. En esta localidad tiene lugar el Premio Nacional de Poesía José Hierro, organizado por la Universidad Popular José Hierro y dotado con un único premio de 9000 euros. En Cabezón de la Sal (Cantabria), lugar que visitaba cada año con motivo de la velada de la poesía en el Día de Cantabria, también se le rindió tributo dedicándole una calle y colocando otro busto en el Parque del Conde San Diego.

En 2003, con el impulso de la Comunidad de Madrid, el Ayuntamiento de Getafe y la familia Hierro se creó el Centro de Poesía José Hierro, un proyecto dedicado íntegramente al estudio, difusión y creación de su poesía y a perpetuar la memoria del poeta. En 2006 pasó a ser la Fundación Centro de Poesía José Hierro donde se imparten talleres y seminarios y se organizan recitales de poesía y otros eventos de carácter artístico y literario. Además se puede disfrutar allí de la única exposición de pintura de José Hierro con carácter permanente. Era una afición que muchos desconocen más allá de lo meramente anecdótico, pero a la que José Hierro dedicó gran parte de su vida, sobre todo en sus últimos años.

Sus primeros versos aparecen en distintas publicaciones del frente republicano. Acabada la contienda, pasa cuatro años en la cárcel, y esta experiencia lo marca indeleblemente. De ahí que, al reaparecer en el panorama lírico de los años cuarenta, con dos libros casi simultáneos, lo haga urgido por un amargo poso autobiográfico que dota a su poesía de una madurez poco frecuente en jóvenes poetas. Se titula el primero "Tierra sin nosotros" (1947), marbete que nos proporciona las desoladas claves donde arraiga, no ya sólo este libro, sino buena parte de la producción surgida de la guerra: la patria un día habitable aparece en ruinas. 

El libro siguiente, "Alegría" (1947) (Premio Adonáis), continúa la reflexión de "Tierra sin nosotros". 

"Con las piedras, con el viento" (1950), es el testimonio de una experiencia amorosa abocada, también, al fracaso. 

Con "Quinta del 42" (1953) comienza la exploración de la vía solidaria, nunca ajena a Hierro, pero, hasta ahora, sostenida en penumbra; no es, sin embargo, la suya una poesía social al uso, y esta diferencia desencadena, con anticipación de años, los mecanismos superadores de un realismo que por entonces amordazaba a la poesía española.

Antirrealista es, en efecto, "Cuanto sé de mí" (1957), libro que acentúa la preocupación verbal, reivindica ámbitos imaginativos y se aleja de la historia y del tiempo para acceder a la «sonora gruta del enigma». 

Estos elementos culminan en el "Libro de las alucinaciones" (1964). Marcado por una poderosa veta irracionalista que se canaliza con frecuencia en el versículo, este poemario rompe definitivamente con las categorías espacio-temporales. 

En 1974 publicará una nueva edición de "Cuanto sé de mí"; en 1991, un nuevo libro de poemas titulado "Agenda"; en 1995 "Emblemas neurorradiológicos" y a finales de los años 90 "Cuaderno de Nueva York", considerada esta última una obra maestra contemporánea.

Su poesía es poderosamente evocativa y ahonda en una intimidad erosionada por un tiempo implacable. Se percibe la influencia de Gerardo Diego. Se inició con una temática reivindicativa testimonial, la memoria de un niño de la guerra, si bien no es un poeta social al uso; poco a poco fue haciéndose más colectiva y existencial.

José Hierro produjo las siguientes obras, principalmente poéticas:






</doc>
<doc id="4571" url="https://es.wikipedia.org/wiki?curid=4571" title="Ángel González">
Ángel González

Ángel González Muñiz (6 de septiembre de 1925 en Oviedo, España - 12 de enero de 2008 en Madrid, España) fue un poeta español de la Generación del 50. Premio Príncipe de Asturias de las Letras en 1985 y académico y Premio Reina Sofía de Poesía Iberoamericana en 1996, publicó su primer libro de poemas en 1956.

Nació en Oviedo el 6 de septiembre de 1925. Su infancia se vio fuertemente marcada por la muerte de su padre, fallecido cuando apenas tenía dieciocho meses de edad. La disrupción del seno familiar continuó durante la Guerra Civil Española, cuando su hermano Manolo murió a manos del bando nacional en 1936. Posteriormente, su hermano Pedro se exilió por sus actividades republicanas y su hermana Maruja no pudo ejercer como maestra por el mismo motivo. En 1943 enfermó de tuberculosis, por lo que inició un lento proceso de recuperación en Páramo del Sil, donde se aficionó a leer poesía y empezó a escribirla él mismo. Tres años más tarde se halló ya por fin recuperado, aunque siempre arrastraría una insuficiencia respiratoria que al cabo le produciría la muerte. Decidió estudiar derecho en la Universidad de Oviedo y también magisterio; en 1950 se trasladó a Madrid para estudiar en la Escuela Oficial de Periodismo. El poeta Luis García Montero publicó en 2008 "Mañana no será lo que Dios Quiera", donde transcribe las memorias de Ángel González. Cuatro años después, en 1954, González opositó para Técnico de Administración Civil del Ministerio de Obras Públicas e ingresó en el Cuerpo Técnico; le destinaron a Sevilla, pero en 1955 pidió una excedencia y marchó a Barcelona durante un periodo en el que ejerció como corrector de estilo de algunas editoriales, entablando amistad con el círculo de poetas de Barcelona, formado por Carlos Barral, Jaime Gil de Biedma y José Agustín Goytisolo; en 1956 publicó su primer libro de poemas, "Áspero mundo", fruto de su experiencia como hijo de la guerra; con él obtuvo un accésit del "Premio Adonais". Volvió a Madrid para trabajar de nuevo en la Administración Pública y conoció al grupo madrileño de escritores de su generación, Juan García Hortelano, Gabriel Celaya, Caballero Bonald y algunos poetas más (luego conocida como Generación del 50 o del medio siglo). En 1959, participó en los actos del 20º aniversario de la muerte de Antonio Machado en la localidad francesa de Colliure.

Varios de sus poemas fueron seleccionados en la antología "Veinte años de poesía española (1939-1959)" de 1960 preparada por Josep María Castellet para la editorial Seix Barral. Tras su segundo libro, "Sin esperanza, con convencimiento" (1961), Ángel González pasó a ser adscrito al grupo de poetas conocido como "Generación del 50" o "Generación de medio siglo". En 1962 fue galardonado en Colliure con el "Premio Antonio Machado" de la editorial Ruedo Ibérico de París por su libro "Grado elemental". 

El año 1970 fue invitado a dar conferencias a la Universidad de Nuevo México en Albuquerque y luego extendieron su invitación para que enseñara durante un semestre; fijó su residencia en Estados Unidos y en 1973 pasó por las Universidades de Utah, Maryland y Texas bajo la misma condición de profesor invitado, regresando en 1974 a la Universidad de Nuevo México en Albuquerque como fijo de Literatura Española Contemporánea, cargo en que se jubiló en 1993. En 1979 viajó a Cuba para formar parte del jurado del Premio Casa de las Américas de Poesía. Ese mismo año conoció a Susana Rivera, con la que se casó en 1993. Tras su jubilación siguió residiendo en Nuevo México aunque a partir de 2006 las visitas a España eran cada vez más reiteradas. 

En 1985 le concedieron el "Premio Príncipe de Asturias de las Letras" y en 1991 el "Premio Internacional Salerno de Poesía". En enero de 1996 fue elegido miembro de la Real Academia Española en el sillón "P" sustituyendo al escritor Julio Caro Baroja. Fue propuesto por los académicos Gregorio Salvador, Miguel Delibes y Emilio Alarcos. El mismo año, además, obtuvo el "Premio Reina Sofía de Poesía Iberoamericana". En 2001 obtuvo el "Premio Julián Besteiro de las Artes y las Letras". En 2004 se convirtió en el primer ganador del Premio Internacional de Poesía Federico García Lorca.

Su obra es una mezcla de intimismo y poesía social, con un particular y característico toque irónico, y trata asuntos cotidianos con un lenguaje coloquial y urbano, nada neopopularista ni localista. El paso del tiempo y la temática amorosa y cívica son las tres obsesiones que se repiten a lo largo y ancho de sus poemas, de regusto melancólico pero optimistas. Guillermo Díaz-Plaja define así su poesía:

Su lenguaje es siempre puro, accesible y transparente; se destila en él un fondo ético de digna y humana fraternidad, que oscila entre la solidaridad y la libertad, al igual que el de otros colegas generacionales como José Ángel Valente, Jaime Gil de Biedma, Carlos Barral, José Agustín Goytisolo y José Manuel Caballero Bonald.

El profesor Enrique Baena Peña de la Universidad de Málaga en "Metáforas del compromiso. Configuraciones de la poética actual y creación de Ángel González" indaga en la estética literaria llamada realismo crítico o ético de los años cincuenta en España, de la que Ángel González es uno de los representantes más significativos. 

González colaboró con el cantautor Pedro Guerra en el libro-disco "La palabra en el aire" (2003) y también con el tenor Joaquín Pixán, el pianista Alejandro Zabala y el acordeonista Salvador Parada en el álbum "Voz que soledad sonando" (2004). El cantautor Joaquin Sabina le rendiría tributo con su canción "Menos dos alas".

En la madrugada del 12 de enero de 2008 falleció el poeta, a los 82 años, en Madrid, a causa de la insuficiencia respiratoria crónica que padecía.






</doc>
<doc id="4575" url="https://es.wikipedia.org/wiki?curid=4575" title="Videojuego de disparos en primera persona">
Videojuego de disparos en primera persona

Los videojuegos de disparos en primera persona (), más conocidos simplemente como «shooters», son un género de videojuegos centrado en las armas y otros combates basados ​​en armas desde una perspectiva en primera persona; es decir, el jugador experimenta la acción a través de los ojos del protagonista. El género comparte rasgos comunes con otros juegos de disparos, que a su vez hacen que caiga bajo el título de juego de acción. Desde el inicio del género, los gráficos avanzados en 3D y pseudo-3D han desafiado el desarrollo de hardware, y los juegos multijugador han sido integrales.

El género de disparos en primera persona se remonta a "Maze War", cuyo desarrollo comenzó en 1973, y en 1974 a "Spasim". Más tarde, y después de títulos más lúdicos como "MIDI Maze" en 1987, el género se convirtió en una forma más violenta con "Wolfenstein 3D" de 1992, a la que se le atribuye la creación del arquetipo básico del género en el que se basan los títulos posteriores. Uno de esos títulos, y el progenitor de la mayor aceptación y popularidad general del género, fue "Doom", uno de los juegos más influyentes en este género; durante algunos años, el término clon de Doom se usó para designar este género debido a la influencia de "Doom". «Corridor shooter» fue otro nombre común para el género en sus primeros años, ya que las limitaciones de procesamiento del hardware de la época significaban que la mayor parte de la acción en los juegos tenía que llevarse a cabo en áreas cerradas.

"Half-Life" de 1998 —junto con su secuela de 2004, "Half-Life 2"— mejoró la narrativa y los elementos del rompecabezas. En 1999, el mod de "Half-Life" "Counter-Strike" fue lanzado y, junto con "Doom", es quizás uno de los tiradores en primera persona más influyentes y paradigmáticos. Tras este éxito llegaron famosas sagas como "Quake", "Battlefield", "Medal of Honor", o "Unreal."

"GoldenEye 007", lanzado en 1997 para Nintendo 64, inició la tendencia de llevar los juegos de disparos en primera persona a las consolas domésticas, mientras que la serie "Halo" aumentó el atractivo comercial y crítico de la consola como plataforma para los títulos de juegos en primera persona. En el siglo XXI, el "shooter" en primera persona es el género de videojuegos más comercialmente viable y, en 2016, este género representó más del 27% de todas las ventas de videojuegos. Varios juegos de disparos en primera persona han juegos populares para deportes electrónicos y competiciones competitivas de juegos.

Los videojuegos de disparos en primera persona son un tipo de juego de disparos tridimensional, que presenta un punto de vista en primera persona con el que el jugador ve la acción a través de los ojos del personaje del jugador. Son diferentes a los shooters en tercera persona, en los cuales el jugador puede ver (generalmente desde atrás) el personaje que está controlando. El elemento principal del diseño es el combate, principalmente con armas de fuego.

Los juegos de disparos en primera persona también suelen clasificarse como distintos de los disparos con pistola de luz, un género similar con una perspectiva en primera persona que utiliza periféricos de pistola de luz, en contraste con los tiradores en primera persona que usan dispositivos de entrada convencionales para el movimiento. Otra diferencia es que los tiradores con pistola de luz en primera persona, como "Virtua Cop", a menudo cuentan con un movimiento "sobre rieles" (con guion), mientras que los tiradores en primera persona como "Doom" le dan al jugador más libertad para moverse.

El «shooter» en primera persona puede considerarse un género distinto en sí mismo, o un tipo de juego de disparos, a su vez, un subgénero del género de juegos de acción más amplio. Tras el lanzamiento de "Doom" en 1993, los juegos de este estilo se denominaron comúnmente «clones de Doom»; con el tiempo, este término ha sido reemplazado en gran parte por «shooter en primera persona». A "Wolfenstein 3D", lanzado en 1992, el año anterior a "Doom", se le atribuye la introducción del género, pero desde entonces los críticos han identificado juegos similares aunque menos avanzados desarrollados en 1973. Hay desacuerdos ocasionales con respecto a los elementos de diseño específicos que constituyen un «shooter» en primera persona. Por ejemplo, "Deus Ex" o "BioShock" pueden considerarse «shooter» en primera persona, pero también pueden considerarse videojuegos de rol a medida que toman prestado de este género ampliamente. Ciertos juegos de rompecabezas como "Portal" también se llaman «shooter» en primera persona, pero carecen de cualquier elemento de combate directo o de disparo, en su lugar utilizan la perspectiva en primera persona para ayudar a sumergir a los jugadores en el juego para ayudar a resolver los rompecabezas. Algunos comentaristas extienden la definición para incluir simuladores de vuelo de combate donde se lleva a cabo la cabina o el vehículo de las manos y las armas.

Como la mayoría de juegos de disparos, los de primera persona incluyen un avatar, una o más armas a distancia y un número variable de enemigos. Debido a que tienen lugar en un entorno 3D, estos juegos tienden a ser más realistas que los juegos de disparos en 2D, y tienen representaciones más precisas de la gravedad, la iluminación, el sonido y las colisiones. Los tiradores en primera persona que se juegan en computadoras personales se controlan con una combinación de teclado y ratón. Este sistema ha sido calificado como superior al que se encuentra en los juegos de consola, que con frecuencia utilizan dos sticks analógicos: uno usado para correr y esquivar, el otro para mirar y apuntar. Es común mostrar las manos y el armamento del personaje en la vista principal, con una pantalla de visualización que muestra los detalles de salud, municiones y ubicación. A menudo, es posible superponer un mapa de los alrededores.

Los «shooter» en primera persona a menudo se centran en el juego de acción, con tiroteos rápidos y sangrientos, aunque algunos ponen un mayor énfasis en la narrativa, la resolución de problemas y los rompecabezas de lógica. Además de disparar, el combate cuerpo a cuerpo también se puede usar ampliamente. En algunos juegos, las armas cuerpo a cuerpo son especialmente poderosas, una recompensa por el riesgo que el jugador debe tomar para maniobrar su personaje cerca del enemigo. En otras situaciones, un arma cuerpo a cuerpo puede ser menos efectiva, pero necesaria como último recurso. Los «shooter» tácticos son más realistas y requieren trabajo en equipo y estrategia para tener éxito; el jugador a menudo manda a un escuadrón de personajes, que puede ser controlado por el juego o por compañeros humanos.

Los «shooter» en primera persona suelen dar a los jugadores una selección de armas, que tienen un gran impacto en la forma en que el jugador se acercará al juego. Algunos diseños de juegos tienen modelos realistas de armas existentes o históricas reales, que incorporan su velocidad de disparo, tamaño de cargador, cantidad de munición, retroceso y precisión. Otros juegos de disparos en primera persona pueden incorporar variaciones imaginativas de armas, incluidos prototipos futuros, armas definidas en el escenario de "tecnología alienígena" y/o utilizar una amplia gama de proyectiles, desde herramientas de mano de obra industrial hasta lanzadores de láser, energía, plasma, cohetes y granadas o ballestas. Estas muchas variaciones también pueden aplicarse a las animaciones de lanzamiento de granadas, rocas, lanzas y similares. Además, se pueden emplear modos de destrucción más convencionales desde las manos de los usuarios visibles, como llamas, electricidad, telequinesia u otras construcciones sobrenaturales. Sin embargo, los diseñadores a menudo permiten que los personajes carguen múltiples múltiplos de armas con poca o ninguna reducción en la velocidad o la movilidad, o quizás de manera más realista, una pistola o un dispositivo más pequeño y un rifle largo o incluso limitando al jugador a solo una arma a la vez. A menudo hay opciones para cambiar, actualizar o cambiar en la mayoría de los juegos. Así, los estándares de realismo varían entre los elementos de diseño. En general, el protagonista puede ser curado y rearmado por medio de elementos tales como botiquines de primeros auxilios, simplemente caminando sobre ellos. Algunos juegos permiten a los jugadores acumular puntos de experiencia similares a los que se encuentran en los juegos de rol, que pueden desbloquear nuevas armas y habilidades.

Estos videojuegos pueden estar compuestos estructuralmente de niveles, o utilizar la técnica de una narrativa continua en la que el juego nunca abandona la perspectiva en primera persona. Otros cuentan con grandes entornos de mundo abierto, que no se dividen en niveles y se pueden explorar libremente. En los tiradores en primera persona, los protagonistas interactúan con el entorno en diversos grados, desde conceptos básicos como el uso de puertas, hasta rompecabezas de resolución de problemas basados ​​en una variedad de objetos interactivos. En algunos juegos, el jugador puede dañar el medio ambiente, también en diversos grados: un dispositivo común es el uso de barriles que contienen material explosivo que el jugador puede disparar, destruirse y dañar a los enemigos cercanos. Otros juegos cuentan con entornos que son ampliamente destructibles, lo que permite efectos visuales adicionales. El mundo del juego a menudo usará temas de ciencia ficción, históricos (particularmente la Segunda Guerra Mundial) o militares modernos, con antagonistas como extraterrestres, monstruos, terroristas y soldados de varios tipos. Los juegos tienen múltiples configuraciones de dificultad; en los modos más difíciles, los enemigos son más duros, más agresivos y hacen más daño, y los poderes son limitados. En modos más fáciles, el jugador puede tener éxito solo a través de tiempos de reacción; en configuraciones más difíciles, a menudo es necesario memorizar los niveles a través de prueba y error.

Los videojuegos de disparos en primera persona pueden ofrecer un modo multijugador, que tiene lugar en niveles especializados. Algunos juegos están diseñados específicamente para juegos de varios jugadores, y tienen modos para un solo jugador muy limitados en los que el jugador compite contra personajes controlados por el juego llamados "bots". Los juegos de disparos en primera persona y multijugador masivos en línea permiten que miles de jugadores compitan al mismo tiempo en un mundo persistente. Los juegos multijugador a gran escala permiten escuadrones múltiples, con líderes que emiten comandos y un comandante que controla la estrategia general del equipo. Los juegos multijugador tienen una variedad de diferentes estilos de juego.

Los tipos clásicos son el combate a muerte (y su variante basada en el equipo) en la que los jugadores obtienen puntos al matar a los personajes de otros jugadores; y captura la bandera, en la que los equipos intentan penetrar en la base opuesta, capturan una bandera y la devuelven a su propia base mientras evitan que el otro equipo haga lo mismo. Otros modos de juego pueden implicar intentar capturar bases o áreas enemigas del mapa, intentar agarrar un objeto durante el mayor tiempo posible mientras evade a otros jugadores, o variaciones de combate mortal que involucren vidas limitadas o en las que los jugadores luchan por un poder particularmente potenciador. Estos tipos de partidos también pueden ser personalizables, lo que permite a los jugadores variar las armas, la salud y los potenciadores que se encuentran en el mapa, así como los criterios de victoria. Los juegos pueden permitir a los jugadores elegir entre varias clases, cada una con sus propias fortalezas, debilidades, equipos y roles dentro de un equipo.

Hay muchos juegos de disparos en primera persona gratuitos en el mercado, incluyendo "", "Apex Legends", "Team Fortress" y "Planetside 2". Algunos juegos se lanzan como "free-to-play" como su modelo de negocio previsto y pueden ser altamente rentable ("League of Legends" ganó dos mil millones de dólares en 2017), pero otros, como "Eternal Crusade", comienzan su vida como juegos pagados y se convierten en "free-to-play" más tarde para llegar a un público más amplio después de una recepción inicialmente decepcionante. Algunas comunidades de jugadores se quejan de "freemium" en primera persona para los tiradores, por temor a que creen juegos desequilibrados, pero muchos diseñadores de juegos han ajustado los precios en respuesta a las críticas, y los jugadores generalmente pueden obtener los mismos beneficios jugando más tiempo que pagando.

Los dos primeros videojuegos de disparos en primera persona documentados son "Maze War" y "Spasim". "Maze War" fue desarrollado originalmente en 1973 por Greg Thompson, Steve Colley y Howard Palmer, estudiantes de secundaria en un programa de estudio y trabajo de la NASA que intentaba desarrollar un programa para ayudar a visualizar la dinámica de fluidos para los diseños de naves espaciales. El trabajo se convirtió en un juego de laberinto presentado al jugador en primera persona, y más tarde incluyó soporte para un segundo jugador y la capacidad de disparar al otro jugador para ganar el juego. Thompson llevó el código del juego con él al Instituto de Tecnología de Massachusetts, donde con la ayuda de Dave Lebling para crear una versión para ocho jugadores que se podría jugar con ARPANET, jugadores de computadora que utilizan inteligencia artificial, mapas personalizables, marcadores en línea y un modo de espectador. "Spasim" tuvo un debut documentado en la Universidad de Illinois en 1974. El juego fue un simulador de vuelo espacial rudimentario para hasta 32 jugadores, con una perspectiva en primera persona. Ambos juegos eran distintos de los «shooters» modernos en primera persona, e incluían movimientos simples basados ​​en fichas en los que el jugador solo podía moverse de una casilla a otra y girar en incrementos de 90 grados. Dichos juegos generaron juegos que usaban imágenes similares para mostrar al jugador como parte de un laberinto (como "Akalabeth: World of Doom" en 1979), y fueron llamados juegos de "vista de ojo de rata", ya que dieron la apariencia de una rata corriendo un laberinto. Otro juego crucial que influenció a los disparos en primera persona fue "Wayout", que, mientras presentaba al jugador que intentaba escapar de un laberinto, usó el lanzamiento de rayos para representar el laberinto, simulando visualmente cómo se representaría cada segmento de la pared en relación con la posición y el ángulo de orientación del jugador, permitiendo más movimiento de forma libre en comparación con "Maze War" y "Spasim".

"Spasim" condujo a simuladores de vuelo de combate más detallados y, finalmente, a un simulador de tanque, desarrollado para el Ejército de los Estados Unidos, a fines de los años setenta. Sin embargo, estos juegos no estaban disponibles para los consumidores, y no fue hasta 1980 cuando se lanzó un videojuego de tanque, "Battlezone", en salas de juego. Una versión del juego fue lanzada en 1983 para computadoras domésticas y se convirtió en el primer juego exitoso de mercado masivo que presenta un punto de vista en primera persona y gráficos en 3D "wireframe", presentados mediante una pantalla de gráficos vectoriales.

"MIDI Maze", uno de los primeros juegos de disparos en primera persona lanzado en 1987 para Atari ST, presentaba un juego basado en laberintos y diseños de personajes similares a "Pac-Man", pero mostrados en una perspectiva en primera persona. Posteriormente, se adaptó a varios sistemas, incluidos Game Boy y Super NES, bajo el título "Faceball 2000", y contó con la primera red de partidas a muerte para jugadores múltiples, utilizando una interfaz MIDI. Fue un juego relativamente menor, pero a pesar de la inconveniencia de conectar varias máquinas entre sí, su modo multijugador ganó un siguiente culto: 1UP.com lo llamó el "primer juego de disparos 3D para múltiples jugadores en un sistema general" y el primer "juego importante de acción en LAN".

El "Hovertank 3D" de Id Software fue pionero en la tecnología de "ray casting" en mayo de 1991 para permitir un juego más rápido que los simuladores de vehículos de la década de 1980; y "Catacomb 3-D" introdujo otro avance, el mapeo de texturas, en noviembre de 1991. El segundo juego que usó mapeo de texturas fue "", un videojuego de rol de acción en marzo de 1992 de Looking Glass Technologies que contó con un punto de vista en primera persona y un motor gráfico avanzado. En octubre de 1990, el desarrollador de identificación John Romero aprendió sobre el mapeo de texturas a través de una llamada telefónica a Paul Neurath. Romero describió la técnica de mapeo de texturas al programador de identificación John Carmack, quien comentó: "Yo puedo hacer eso", y se sentiría motivado por el ejemplo de Looking Glass para hacer lo mismo en "Catacomb 3-D". "Catacomb 3-D" también introdujo la visualización de la mano y el arma del protagonista (en este caso, hechizos mágicos) en la pantalla, mientras que anteriormente no se veían los aspectos del avatar del jugador. La experiencia de desarrollar "Ultima Underworld" permitiría a Looking Glass crear la serie "Thief" y "System Shock" años más tarde.

"Wolfenstein 3D" (creado por id Software y lanzado en 1992) fue un éxito instantáneo, impulsado en gran medida por su versión shareware, y se le atribuye la invención del género de disparos en primera persona. Se basó en la tecnología ray cast, pionera en juegos anteriores para crear una plantilla revolucionaria para el diseño de juegos de disparos, en la que todavía se basan los «shooter» en primera persona. A pesar de sus temas violentos, "Wolfenstein" escapó en gran parte de la controversia generada por el último "Doom", aunque fue prohibido en Alemania debido al uso de la iconografía nazi; y la versión Super NES reemplazó a los perros enemigos de ataque con ratas gigantes. Apogee Software, el editor de "Wolfenstein 3D", siguió su éxito con "" en 1993. Inicialmente, el juego fue bien recibido, pero las ventas disminuyeron rápidamente a raíz del éxito de id "Doom", lanzado una semana después.

"Doom", lanzado como shareware en 1993, refinó la plantilla de "Wolfenstein 3D" agregando texturas mejoradas, variaciones en altura (por ejemplo, escaleras que el personaje del jugador podría subir) y efectos como luces parpadeantes y parches de oscuridad total, creando un entorno 3D más creíble que "Wolfenstein 3D", más monótonos y simplistas. "Doom" permitió partidas competitivas entre múltiples jugadores, denominados «deathmatch» o partidas a muerte, y el juego fue responsable de la entrada posterior de la palabra en el léxico de los videojuegos. Según el creador John Romero, el concepto de combate a muerte del juego se inspiró en el multijugador competitivo de los juegos de lucha. "Doom" se hizo tan popular que sus características de multijugador comenzaron a causar problemas a las compañías cuyas redes se utilizaron para jugar el juego.

"Doom" ha sido considerado desde entonces el «shooter» en primera persona más importante jamás realizado. Fue muy influyente no solo en los juegos de disparos posteriores, sino también en los videojuegos en general, y ha estado disponible en casi todos los sistemas de videojuegos desde ese momento. El juego multijugador, que ahora es parte integral del género de disparos en primera persona, se logró primero con éxito en gran escala con "Doom". Si bien su combinación de violencia sangrienta, humor oscuro e imágenes infernales fueron aclamadas por los críticos, estos atributos también generaron críticas de grupos religiosos, y otros comentaristas calificaron el juego de "simulador de asesinato". Hubo más controversia cuando se supo que los perpetradores de la masacre de Columbine High School eran fanáticos del juego; posteriormente, las familias de varias víctimas intentaron sin éxito demandar a numerosas compañías de videojuegos, entre ellas id Software, que según las familias inspiraron la masacre. En 1994, Raven Software lanzó "Heretic", que usaba una versión modificada del motor de "Doom" que permitía la orientación vertical, un sistema de inventario para almacenar y seleccionar artículos y gibs.

"" se lanzó en 1995 después de que LucasArts decidiera que "Star Wars" haría el material apropiado para un juego al estilo de "Doom". Sin embargo, "Star Wars: Dark Forces" agregó varias características técnicas de las que carecía "Doom", como la capacidad de agacharse, saltar o mirar hacia arriba y hacia abajo. "Dark Forces" también fue uno de los primeros juegos en incorporar objetos diseñados en 3D en el motor 2D del juego. "Duke Nukem 3D" de Apogee, lanzado en 1996, fue "el último de los grandes «shooter» basados ​​en "sprites"" que fue aclamado por su humor basado en el machismo estilizado y en su modo de juego. Sin embargo, algunos consideraron que el tratamiento de las mujeres en el juego (y más tarde en toda la serie) era despectivo e insípido.

La mayoría de los «shooter» en este período fueron desarrollados para computadoras compatibles con PC de IBM. En el lado de Macintosh, Bungie lanzó su primer juego de acción, "Pathways into Darkness" en 1993, que presentaba más elementos narrativos y de aventuras junto con el juego de acción en primera persona. Los caminos se habían inspirado en "Wolfenstein 3D", y se llevaron a cabo un intento de convertir su anterior juego de exploración de mazmorras "Minotaur: The Labyrinths of Crete" en un entorno 3D. En el año siguiente, Bungie lanzó "Marathon", que simplificó los conceptos de "Pathways" al eliminar elementos de juego de roles en favor de la acción del tirador. "Marathon" tuvo éxito y condujo a dos secuelas para formar la trilogía de "Marathon", y "Marathon", el estándar para los tiradores en primera persona en esa plataforma. "Marathon" fue pionero o fue uno de los primeros en adoptar varias funciones nuevas como freelook, armas de doble uso y de doble función, modos multijugador versátiles (como King of the Hill, Kill the Man with the Ball, y juego cooperativo), y personajes no jugadores amistosos (NPC). Los juegos de "Marathon" también tenían un fuerte énfasis en contar historias además de la acción, que continuaría en los proyectos futuros de Bungie, "Halo" y "Destiny".

En 1994, Exact lanzó Geograph Seal para la computadora doméstica Sharp X68000 de Japón. Un oscuro título de importación en lo que se refería al mercado occidental, era, sin embargo, "un juego de disparos en primera persona poligonal en 3D" con una mecánica de juegos de plataformas innovadora y entornos al aire libre de "itinerancia libre". Al año siguiente, Exact lanzó su sucesor para la consola de PlayStation, "Jumping Flash!", que puso más énfasis en los elementos de su plataforma. "Descent" (lanzado por Parallax Software en 1995), un juego en el que el jugador pilotea una nave espacial alrededor de cuevas y ductos de fábrica, fue un verdadero juego de disparos en primera persona en tres dimensiones. Abandonó los "sprites" y el rayo a favor de polígonos y seis grados de libertad.

Poco después del lanzamiento de "Duke Nukem 3D" en 1996, id Software lanzó el muy esperado "Quake". Al igual que "Doom", "Quake" fue un título influyente y definió el género; presentaba un juego sangriento de ritmo rápido, pero usaba polígonos 3D en lugar de "sprites". Estaba centrado en los juegos en línea y presentaba múltiples tipos de partidos que aún se encuentran en los juegos de disparos en primera persona en la actualidad. Fue el primer juego de FPS en tener seguidores de clanes de jugadores (aunque el concepto había existido anteriormente en "MechWarrior 2" ("Netmech") con su tradición de "Battletech", así como entre los jugadores de MUD), e inspiraría populares fiestas LAN como QuakeCon. La popularidad del juego y el uso de gráficos poligonales en 3D también ayudaron a expandir el creciente mercado de hardware de tarjetas de video; y el apoyo adicional y el estímulo para las modificaciones del juego atrajeron a los jugadores que querían jugar con el juego y crear sus propios módulos. Según el creador John Romero, el mundo 3D de "Quake" se inspiró en el juego de lucha en 3D "Virtua Fighter". "Quake" también tenía la intención de expandir el género con "Virtua Fighter" influenciada en peleas cuerpo a cuerpo, pero esto finalmente se eliminó del juego final.

Basado en la película de James Bond, "GoldenEye 007" de Rare se lanzó en 1997, y en 2004 era el juego de Nintendo 64 más vendido en los Estados Unidos. Fue el primer juego de disparos en primera persona en consolas y fue muy aclamado por sus niveles atmosféricos para un solo jugador y sus mapas multijugador bien diseñados. Presentaba un rifle de francotirador, la capacidad de realizar disparos a la cabeza y la incorporación de elementos furtivos; (y todos estos aspectos también se usaron en la continuación espiritual del juego "Perfect Dark"), así como en las características inspiradas en "Virtua Cop", como la recarga, las animaciones de reacción a los golpes dependientes de la posición, las penalidades por matar inocentes y un sistema de puntería que permite a los jugadores apuntar a un punto preciso en la pantalla.

Aunque no es el primero de su tipo, "Tom Clancy's Rainbow Six" comenzó una tendencia popular de «shooters» tácticos en primera persona en 1998. Presentaba un diseño realista basado en el equipo y temas basados ​​en la lucha contra el terrorismo, que requieren que las misiones se planifiquen antes de la ejecución y en ella, un solo golpe era a veces suficiente para matar a un personaje. "Medal of Honor", lanzado en 1999, comenzó una proliferación de tiradores en primera persona durante la Segunda Guerra Mundial y supuso la entrada de Electronic Arts en el género.

Sin embargo, el punto de inflexión en los «shooters» lo protagonizó "Half-Life", de Valve, lanzado en 1998 y basado en la tecnología gráfica de "Quake". Inicialmente se encontró con una tibia acogida, pero se convirtió en un éxito comercial incontestable. Mientras que los anteriores tiradores en primera persona se habían centrado en el juego visceral con tramas comparativamente débiles, "Half-Life" tenía una narrativa sólida; el juego no mostraba cinemáticas, pero permanecía en la perspectiva de primera persona en todo momento. Presentaba innovaciones como personajes no enemigos (algo más temprano en títulos como "Strife") pero no empleaba potenciadores en el sentido tradicional. "Half-Life" fue elogiado por su inteligencia artificial, selección de armas y atención al detalle y "desde entonces ha sido reconocido como uno de los mejores juegos de todos los tiempos", según GameSpot. Su secuela, "Half-Life 2" (lanzada en 2004), fue menos influyente aunque "posiblemente un juego más impresionante".

"Starsiege: Tribes", también lanzado en 1998, fue un juego de disparos multijugador en línea que permite a más de 32 jugadores en un solo partido. Presentaba un juego basado en el equipo con una variedad de roles especializados y una característica inusual de "jet pack". El juego fue muy popular y luego fue imitado por juegos como la serie "Battlefield". Los títulos de id Software "Quake III Arena" y "Unreal Tournament" de Epic Games, ambos lanzados en 1999, fueron populares por sus frenéticos y accesibles modos multijugador en línea; ambos presentaban una jugabilidad muy limitada para un solo jugador. "Unreal" presentaba, a su vez, el motor Unreal Engine, que se convertiría en un motor muy influyente en los «shooters» del futuro.

"Counter-Strike", también lanzado en 1999, era una modificación de "Half-Life" con un tema de antiterrorismo. El juego se convirtió en un éxito sin precedentes y superó en popularidad y jugadores al propio "Half-Life", con más de 90 000 jugadores compitiendo en línea en cualquier momento durante su máxima actividad. Desde entonces, "Counter-Strike" se ha convertido en uno de los «shooters» más influyentes y de mayor tradición entre los jugadores de todo el mundo. Su secuela fue "" (2004).

En el Electronic Entertainment Expo (E3) de 1999, Bungie presentó un juego de estrategia en tiempo real llamado ""; en el siguiente E3, se mostró una versión revisada del «shooter» en tercera persona. En 2000, Bungie fue comprada por Microsoft y "Halo" fue renovado y lanzado como un juego de disparos en primera persona; fue uno de los títulos de lanzamiento para la consola Xbox. Fue un éxito crítico y comercial fuera de control, y se considera un primer juego de disparos en primera persona para consolas. Presentó una narrativa y una historia que recuerda a la anterior serie "Marathon" de Bungie, pero que ahora se cuenta en gran parte a través del diálogo en el juego y las escenas cinemáticas. También fue aclamado por sus personajes, tanto el protagonista, Master Chief y sus antagonistas alienígenas. La secuela, "Halo 2" (2004), llevó la popularidad de los juegos en línea al mercado de las consolas a través de Xbox Live, en el que fue el juego más jugado durante casi dos años.

"Deus Ex", lanzado por Ion Storm en 2000, presentaba un sistema de nivelación similar al que se encuentra en los juegos de rol; también tenía múltiples narraciones dependiendo de cómo el jugador completó las misiones y ganó reconocimiento por su estilo artístico y serio. Los juegos de "Resident Evil", "" en 2000 y "" en 2003, intentaron combinar los géneros de pistola de luz y de disparos en primera persona junto con elementos de "Horror de supervivencia". "Metroid Prime", lanzado en 2002 para Nintendo GameCube, un altamente elogiado juego de disparos en primera persona, incorporó elementos de aventuras de acción como saltar rompecabezas y se basó en la serie "Metroid" de aventuras de plataforma de desplazamiento lateral 2D. Dando un "paso masivo hacia los juegos en primera persona", el juego enfatizó sus elementos de aventura en lugar de disparar y fue acreditado por el periodista Chris Kohler por "liberar el género de las garras de "Doom"".

"World War II Online", lanzada en 2001, contó con un entorno multijugador persistente y "masivo", aunque IGN dijo que "la realización completa de ese entorno probablemente aún esté a unos pocos años". "Battlefield 1942", otro juego de disparos de la Segunda Guerra Mundial lanzado en 2002, presentó batallas a gran escala que incorporaban aviones, embarcaciones navales, vehículos terrestres y combate de infantería. En 2003, "PlanetSide" permitió a cientos de jugadores a la vez competir en un mundo persistente, y también fue promovido como el "primer juego de disparos en primera persona en línea para jugadores múltiples masivos". La serie "Serious Sam", lanzada por primera vez en 2001, y "Painkiller", lanzada en 2004, enfatizaron la lucha contra oleadas de enemigos en grandes estadios abiertos, en un intento por escuchar las raíces del género.

"Doom 3", lanzado en 2004, puso un mayor énfasis en el horror y el miedo del jugador que en los juegos anteriores de la serie y fue un éxito de ventas aclamado por la crítica, aunque algunos comentaristas sintieron que carecía de contenido y de innovación en el juego, poniendo demasiado énfasis en los gráficos impresionantes. En 2005, una película basada en "Doom" presentó una secuencia que emulaba el punto de vista y la acción del tirador en primera persona, pero se criticó críticamente como deliberadamente poco inteligente y gratuitamente violenta. En 2005, "F.E.A.R." fue aclamado por combinar con éxito el juego de disparos en primera persona con una atmósfera de terror japonés. Más tarde, en 2007, "BioShock" de Irrational Games sería aclamado por algunos comentaristas como el mejor juego de ese año por su innovación en arte, narrativa y diseño, y algunos lo llamaron el "sucesor espiritual" del anterior "System Shock 2" de Irrational.

Finalmente, los juegos de Crytek "Far Cry" (2004) y "Crysis" (2007), así como "Far Cry 2" (2008) de Ubisoft abrirán nuevos caminos en términos de gráficos y diseño de nivel abierto de gran tamaño, mientras que "" (2007), "" (2006) y su secuela "Resistance 2" (2008) presentaron niveles y narraciones lineales cada vez más refinados, con el ritmo rápido y la linealidad de los juegos de "Call of Duty" que se parecen a los shoot 'em up. En 2007, "Portal" popularizó el concepto de mecánica de rompecabezas en perspectiva de primera persona. En 2006, Gamasutra reportó al shooter en primera persona como uno de los géneros de videojuegos más grandes y de mayor crecimiento en términos de ingresos para los editores.

En 2010, investigadores de la Universidad de Leiden demostraron que jugar videojuegos de disparos en primera persona está asociado con una flexibilidad mental superior. En comparación con los no jugadores, se encontró que los jugadores de tales juegos requerían un tiempo de reacción significativamente más corto al cambiar entre tareas complejas, posiblemente porque se les exige desarrollar una mentalidad más receptiva para reaccionar rápidamente a estímulos visuales y auditivos de rápido movimiento, y cambio de ida y vuelta entre diferentes subdeberes. El uso de los controladores de juego de detección de movimiento, especialmente los de Wii, "prometió hacer que los controles de FPS sean más accesibles y precisos con una interfaz tan simple como apuntar literalmente al objetivo" y, por lo tanto, "remodelar drásticamente al tirador en primera persona". Sin embargo, las dificultades técnicas relacionadas con funciones distintas de la puntería, como la maniobra o la recarga, impidieron su uso generalizado entre los videojuegos de disparos en primera persona. La interfaz de usuario de Pointman combina un gamepad sensible al movimiento, un rastreador de cabeza y pedales deslizantes para aumentar la precisión y el nivel de control sobre el avatar en los juegos de disparos militares en primera persona.

La decimocuarta entrega de la serie "Medal of Honor" y la secuela directa del juego de 2010; "" fue lanzado para PlayStation 3, Xbox 360 y PC el 23 de octubre de 2012. "Warfighter" fue tanto un fracaso crítico como comercial. En enero de 2013, Electronic Arts anunció el cese de la serie "Medal of Honor" debido a la mala recepción y venta de "Medal of Honor: Warfighter", por lo que EA se centró en la saga "Battlefield". En 2016 "Battlefield 1" fue lanzado por EA DICE y Electronic Arts y fue un éxito comercial, vendiendo más de 15 millones de copias. La mayoría de los elogios se dirigieron hacia su tema poco recurrido de la Primera Guerra Mundial, las campañas para un jugador, su modo multijugador, la imágenes y el diseño de sonido.

Con la aparición de los videojuegos de modo "battle-royale" en 2017 a través de "PUBG" y "" —en tercera persona ambos—, algunas sagas de videojuegos han incluido este modo de juego "online" tan popular. "" (2018) incluyó su modo battle royale con "Blackout", mientras que "Battlefield V" (2018) lo hizo con "Firestorm", aunque sin el mismo éxito que los primeros.




</doc>
<doc id="4582" url="https://es.wikipedia.org/wiki?curid=4582" title="Vasili Kandinski">
Vasili Kandinski

Vasili Vasílievich Kandinski (Moscú, -Neuilly-sur-Seine, 13 de diciembre de 1944) fue un pintor ruso, precursor del arte abstracto en pintura y teórico del arte. Se considera que con él comienza la abstracción lírica y el expresionismo.

Pasó su niñez en Odessa, donde se graduó en la Escuela de Arte Grekov Odessa. Se matriculó en la Universidad Estatal de Moscú, donde estudió derecho y economía, tras lo cual empezó a ejercer con éxito su profesión e incluso le ofrecieron un puesto de profesor de derecho romano en la Universidad de Dorpat. Sin embargo, decidió comenzar a estudiar pintura cuando tenía 30 años.

En 1896 se estableció en Múnich, Alemania, donde se formó en la escuela privada de Anton Ažbe primero y en la Academia de Bellas Artes de Múnich después. Regresó a Moscú en 1914, después del estallido de la Primera Guerra Mundial. Tras la Revolución rusa, se convirtió en un gran conocedor de la administración cultural de Anatoli Lunacharski, el Comisario de Educación de la nueva administración soviética de Rusia, y ayudó a fundar el Museo de Cultura de la Pintura. Sin embargo, para entonces su «perspectiva espiritual... era ajena al materialismo argumentativo de la sociedad soviética», y las oportunidades le atrajeron de nuevo a Alemania, a donde retornó en 1920. Allí enseñó en la Escuela de la Bauhaus de arte y arquitectura desde 1922 hasta que la cerraron los nazis en 1933. Entonces se trasladó a Francia, donde residió el resto de su vida y donde adquirió la ciudadanía francesa en 1939. En este país creó algunas de sus mejores obras.

Kandinski nació el 16 de diciembre (4 de diciembre en el antiguo calendario ruso) de 1866, proveniente de una familia de clase media alta. Su padre Vasili Silvéstrovich Kandinski era un comerciante de té procedente de Kiajta, una población siberiana cercana a la frontera con Mongolia. La abuela de Vasili era una aristócrata mongola de la dinastía Gantimúrov. Lidia Ivánovna Tijéieva, la madre de Vasili, era de Moscú. Pasó su infancia y juventud entre Moscú y Odesa, donde se trasladó la familia en 1871. Tras el divorcio de sus padres vivió con su padre. Su tía Elizaveta Tijéieva también lo cuidó. Su abuela materna era alemana y le hablaba en alemán. En Odesa tomó clases de piano y de violonchelo. En 1886 Kandinski comenzó sus estudios de Derecho y Ciencias económicas en la Universidad de Moscú. También estudió etnografía. En 1892 se casó con su prima Anna Chemyákina, con quien vivió hasta 1904. En 1893 fue nombrado profesor asociado en la Facultad de Derecho. En 1896 la Universidad de Tartu le ofreció una plaza de profesor que rechazó para dedicarse por completo al arte. Esta decisión estuvo influida por la exposición de los impresionistas en Moscú en 1895, al ver las obras de Monet y la representación de "Lohengrin" de Richard Wagner en el teatro Bolshói.

Se trasladó a Múnich donde inicialmente no fue admitido en la Academia de Arte. Estudió un tiempo en la academia privada de Anton Ažbe hasta 1900, cuando fue admitido en la Academia. Su profesor fue Franz von Stuck y, como consideró que la paleta de Kandinski era demasiado brillante, le hizo pintar en una gama de grises durante un año.

En 1901 fundó el grupo Phalanx, cuyo propósito principal era introducir las vanguardias francesas en el provinciano ambiente muniqués, para lo que abrió una escuela en la que daba clases. Sus pinturas de los primeros años del siglo eran paisajes ejecutados con espátula, en un principio sombríos, para luego adquirir una intensidad casi "fauve". También pintó temas fantásticos basados en tradiciones rusas o en la Edad Media alemana. Este período estuvo marcado por la experimentación técnica, en particular en el uso del temple sobre un papel oscuro, para dar una impresión de superficie transparente, iluminada desde atrás. La consistencia tonal del claroscuro enfatiza el esquema, borrando la distinción entre las figuras y el fondo, resultando una composición casi abstracta.

En 1902 expuso por primera vez con la Secesion de Berlín y realizó sus primeras xilografías. En 1903 y 1904 viaja por Italia, Países Bajos, África y visita Rusia. En 1904 expuso en el Salón de Otoño de París. En 1903 se divorció de Anna Chemyákina y se casó con la joven artista Gabriele Münter. Durante cinco años viajó con su esposa por Europa pintando y participando en exposiciones. Volvió a Baviera y se asentó en Murnau am Staffelsee en la falda de los Alpes. En 1909 fue elegido presidente de la Nueva Asociación de Artistas de Múnich (NKVM). La primera exposición del grupo tuvo lugar en la galería Thannhauser de Múnich ese mismo año. Hacia el final de la década, las pinturas de Kandinski denotan una gran tendencia a la plenitud, por la equivalencia en intensidad de las áreas de color y la superficie reluciente, que destruye toda ilusión de profundidad. Las series de cuadros de jinetes en combate comenzaron en 1909, y en ellas, la línea del horizonte se va erradicando gradualmente, al igual que otras referencias espaciales. En 1913 escribió sus memorias y una colección de poesías.
En 1913 una obra suya se presentó en el Armory Show de Nueva York.

Al estallar la Primera Guerra Mundial abandonó Alemania y junto a su esposa Gabriele se mudaron a Suiza el 3 de agosto de 1914. En noviembre de 1914 Gabriele volvió a Múnich y Kandinski volvió a Moscú. En el otoño de 1916 conoció a Nina Andreiévskaya, que era hija de un general ruso. Se casó con ella en febrero de 1917.

A partir de la Revolución de octubre de 1917, Kandinski desarrolló un trabajo administrativo para el Comisariado del Pueblo, para la Educación; entre los proyectos de este organismo estaba la reforma del sistema educativo de las escuelas de arte. En 1920 fue uno de los fundadores en Moscú del INJUK (Instituto para la Cultura Artística), a lo largo de este año surgió el conflicto entre Kandinski, Malévich y otros pintores idealistas frente a los productivistas (o constructivistas), Vladímir Tatlin y Aleksandr Ródchenko, este último grupo encontró un fuerte apoyo en "el plan de propaganda monumental" ideado por las autoridades políticas de la Revolución. La situación de tensión propició la salida de Kandinski de Rusia.

En 1922 se trasladó a Weimar (Alemania), donde impartió clases teóricas para la Escuela de la Bauhaus. En 1926 se publicó su libro "Punto y línea sobre el plano. Contribución al análisis de los elementos pictóricos". Una continuación orgánica de su trabajo anterior "De lo espiritual en el arte".

Hacia 1931 los nacionalsocialistas iniciaron una campaña a gran escala contra la Bauhaus que llevó a su cierre en 1932. Kandinski y su esposa emigraron a Francia y fijaron su residencia en Neuilly-sur-Seine, suburbio de París.

Kandinsky recuerda la fascinación por el color como un niño. Su fascinación por el simbolismo del color y la psicología continuó a medida que crecía. En 1889, formó parte de un grupo de investigación etnográfica que viajó a Vólogda al norte de la región de Moscú. En "Mirada retrospectiva", relata que las casas e iglesias fueron decoradas con colores tan brillantes que al entrar en ellos, sentía que se movía en una pintura. Esta experiencia, y su estudio de arte popular de la región (en particular, el uso de colores brillantes sobre un fondo oscuro), se reflejó en gran parte de sus primeros trabajos. Unos años más tarde, primero comparó pintura para componer música de la manera por la cual se convertiría en señalar, escribiendo, "El color es la tecla. El ojo es el martillo. El alma es el piano. El artista es la mano que, con una u otra tecla hace vibrar el espíritu del ser humano".

En 1896, a la edad de 30 años, Kandinski abandonó una prometedora carrera docente en el mundo de la ley y la economía para inscribirse en la escuela de arte de Múnich. No se le concedió inmediatamente la admisión, y comenzó a aprender el arte por sí mismo. Ese mismo año, antes de salir de Moscú, vio una exposición de pinturas de Monet. Quedó impactado con el estilo impresionista de la serie de los , por su fuerte sentido de color casi independiente de los objetos mismos.

Kandinski fue influenciado de manera similar durante este período por Richard Wagner que, según él, empujó los límites de la música y la melodía más allá de lirismo estándar. Fue también espiritualmente influenciado por Helena Blavatsky (1831-1891), la mejor exponente conocida de la teosofía. La Teoría teosófica postula que la creación es una progresión geométrica, a partir de un solo punto. El aspecto creativo de la forma que se expresa mediante una serie descendente de círculos, triángulos y cuadrados. Los libros de Kandinski De lo espiritual en el arte (1910) y Punto y línea sobre el plano (1926) se hicieron eco de esta doctrina teosófica.

La escuela de arte, por lo general se considera difícil, pero era fácil para Kandinski. Fue durante este tiempo que comenzó a surgir como un teórico del arte, y como un pintor. El número de sus pinturas existentes aumentó a principios del siglo xx, queda mucho de los paisajes y pueblos que pintó, con amplios sectores de formas de color y reconocible. En su mayor parte, sin embargo, las pinturas de Kandinski no cuentan con ninguna figura humana, una excepción es "Domingo. (Rusia antigua)" (1904), en la que Kandinski recrea una vista muy colorida (y de fantasía) de los campesinos y los nobles frente a los muros de una ciudad. "Pareja a caballo" (1907) representa a un hombre a caballo con una mujer, con la ternura y el cuidado que cabalgan junto a un pueblo ruso con paredes luminosas a través de un río. El caballo está en silencio mientras las hojas de los árboles, la ciudad, y los reflejos en el río brillan con manchas de color. Este trabajo demuestra la influencia del puntillismo en la forma en que se pierde la profundidad de campo en una superficie plana y luminiscente. La influencia del fovismo es también evidente en estas primeras obras. Los colores se utilizan para expresar la experiencia de Kandinski con la materia, no para describir la naturaleza objetiva.

Quizás la más importante de sus pinturas de la primera década de 1900 fue "El Jinete Azul" (1903), la cual muestra una pequeña figura envuelta en un caballo veloz corriendo por un prado rocoso. La capa del jinete es azul medio y la sombra proyectada es de color azul oscuro. En el primer plano son sombras azules más amorfas, las traseras de los árboles en otoño, en el fondo. El jinete azul en la pintura destaca (aunque no claramente definido) y el caballo tiene un andar anormal (que Kandinski debía saber). Algunos historiadores del arte creen que una segunda figura (tal vez un niño) va agarrado por el piloto, y ñoño 
aunque puede tratarse de una sombra del jinete solitario. Esta disyunción intencional, que permite a los espectadores a participar en la compleción de la obra de arte, se convirtió en una técnica cada vez más consciente utilizada por Kandinski en los años siguientes, y culminó en las obras abstractas del período 1911-1914. En "El Jinete Azul", Kandinski muestra el jinete más como una serie de colores que en los detalles específicos. Esta pintura no es una excepción en este sentido en comparación con los pintores contemporáneos, pero muestra la dirección que Kandinski tomaría solo unos pocos años después.

Entre 1906 y 1908 Kandinski pasó mucho tiempo viajando por Europa (él era un asociado de grupo simbolista de Moscú), hasta que se instaló en la pequeña ciudad bávara de Murnau. El cuadro "Montaña Azul" (1908-1909) fue pintado en este momento, lo que demuestra su tendencia hacia la abstracción. Una montaña azul está flanqueada por dos árboles grandes, uno amarillo y uno rojo. Una procesión, con tres jinetes y varios figuras, se cruza en la parte inferior. Las caras, ropa y sillas de montar de los jinetes son cada una de un solo color y ni ellos ni las figuras caminando muestran cualquier detalle real. Los planos y los contornos también son indicativos de la influencia fauvista. El amplio uso del color en "Montaña Azul" ilustra la inclinación de Kandinski hacia un arte en el que el color se presenta independientemente de la forma, y en el que a cada color se le da la misma atención. La composición es más plana, el cuadro se divide en cuatro secciones: el cielo, el árbol rojo, el amarillo y el árbol de la montaña azul con los tres jinetes.

En 1911, Vasili Kandinski y Franz Marc y otros artistas, fundaron en Múnich un movimiento expresionista Der Blaue Reiter ("El Jinete Azul" en español) que transformó el expresionismo alemán.

De 1918 a 1921, Kandinski trabaja en la política cultural de Rusia y colabora en la educación artística y la reforma de los museos. Pintó poco durante este período dedicando su tiempo a la enseñanza artística con un programa basado en el análisis de la forma y del color. También ayudó a organizar el Instituto de Cultura Artística en Moscú. En 1916 conoció a Nina Andreiévskaya, con quien se casó al año siguiente. Su punto de vista espiritual y expresionista del arte fue finalmente rechazado por los miembros radicales del Instituto (Tatlin, Ródchenko) como demasiado individualista y burgués. En 1921, Kandinski fue invitado a ir a Alemania, para asistir a la Bauhaus de Weimar, por su fundador el arquitecto Walter Gropius.

Kandinski no solo enseñó en las clases de diseño básico para principiantes y el curso sobre teoría avanzada en el Bauhaus, sino que también llevó a cabo clases de pintura y un taller en el que aumentó su teoría del color con los nuevos elementos de la psicología de la forma. El desarrollo de sus trabajos sobre el estudio de las formas, en particular en los puntos y formas de línea, dio lugar a la publicación de su segundo libro teórico "Punto y línea sobre el plano" en 1926. Los elementos geométricos adquieren una importancia cada vez mayor tanto en su enseñanza y como en su pintura, especialmente el círculo, medio círculo, el ángulo, las líneas rectas y curvas. Este período fue sumamente productivo. Esta libertad se caracteriza en sus obras por el tratamiento rico en colores y matices —como en el amarillo-rojo-azul (1925), donde Kandinski ilustra su distancia desde el constructivismo y el suprematismo, movimientos influyentes de la época—.

Los dos metros de ancho amarillo- rojo-azul (1925) se componen de varias formas: un rectángulo vertical amarillo, una cruz inclinada roja y un gran círculo azul oscuro, una multitud de líneas negras rectas (o sinuosa), arcos circulares, círculos monocromáticas y dameros de colores dispersos, contribuyendo a su delicada complejidad. Esta simple identificación visual de las formas y de las masas principales de los colores presentes en el lienzo son solo una primera aproximación a la realidad interna de la obra, cuyo reconocimiento exige la observación más profunda, no solo de formas y colores que intervienen en la obra, sino su relación absoluta, las posiciones relativas en el lienzo y su armonía.

Kandinski fue uno de Die Blaue Vier (Blue Four), formada en 1924 con Paul Klee, Feininger y Alekséi von Jawlensky, que dio conferencias y exhibió en los Estados Unidos en 1924. Debido a la hostilidad de la derecha y a la izquierda de la Bauhaus de Weimar, se establecieron en Dessau en 1925. Después de una campaña de difamación nazi de la Bauhaus Dessau en 1932 se marcha a Berlín, hasta su disolución en julio de 1933. Kandinski luego abandonó Alemania y se estableció en París.

Al vivir en un pequeño apartamento en París, Kandinski creó su trabajo en un estudio de la sala. Con formas biomórficas, flexibles, no geométricas, los contornos de las formas que aparecen en sus pinturas sugieren organismos microscópicos que no hacen sino expresar la vida interior del artista. Kandinski utiliza composiciones originales en color, que evocan el arte popular eslavo. También en ocasiones mezcla con arena la pintura para dar a sus obras una textura granular y rústica.

Este período corresponde a una síntesis de los trabajos previos de Kandinski en el que utilizó todos los elementos, enriqueciéndolos. En 1936 y 1939 pintó sus dos últimas composiciones principales; el tipo de telas elaboradas no se había producido durante muchos años. "Composición IX" tiene mucho contraste y diagonales poderosas cuya principal forma da la impresión de un embrión en el útero. Pequeños cuadrados de colores y bandas de colores destacan sobre el fondo negro del cuadro "Composición X" como fragmentos de estrellas (o filamentos ), mientras enigmáticos jeroglíficos con tonos pastel cubren una gran masa marrón que parece flotar en la esquina superior izquierda del lienzo. En la obra de Kandinski, algunas características son evidentes, mientras que ciertos toques son más discretos y velados, y solo se revelan progresivamente tras profundizar en la relación con su trabajo. Tenía la intención de sus formas (sutilmente armonizados y colocados) para resonar con el alma del observador. En este período muchas de sus obras fueron adquiridas por Solomon Guggenheim, que fue uno de sus apoyos más entusiastas.

El desarrollo de Kandinski hacia la abstracción encuentra su justificación teórica en "Abstracción y empatía" de Wilhelm Worringer, que se había publicado en 1908. Argumenta que la jerarquía de valores al uso, basada en las leyes del Renacimiento, no es válida para considerar el arte de otras culturas; muchos artistas crean desde la realidad pero con un impulso abstracto, que hace que las últimas tendencias del arte se den en sociedades menos materialistas.

Kandinski, al igual que Piet Mondrian, estaba interesado también en la teosofía, entendida como la verdad fundamental que subyace detrás de doctrinas y rituales en todas las religiones del mundo; la creencia en una realidad esencial oculta tras las apariencias, proporciona una obvia racionalidad al arte abstracto.

En 1912 publicó "De lo Espiritual en el Arte", donde critica a las instituciones académicas tradicionalistas y la idea de arte en general. Es el primer libro que describe la fundación teórica del movimiento abstracto y habla de una nueva época de gran espiritualidad y de la contribución de la pintura a ella. El arte nuevo debe basarse en un lenguaje de color y Kandinski da las pautas sobre las propiedades emocionales de cada tono y de cada color, a diferencia de teorías sobre el color más antiguas, él no se interesa por el espectro sino solo en la "respuesta del alma".

Entre 1926 y 1933 pintó 159 óleos y 300 acuarelas. Muchos de ellos se perdieron después de que los nazis declararon "degeneradas" sus pinturas.
En 1939 se nacionalizó francés.

Al igual que en las teorías de los ensayos Der Blaue Reiter, Almanac y las indicaciones del compositor Arnold Schoenberg, Kandinski expresó también la comunión entre el artista y el espectador como puestas a disposición tanto de los sentidos como de la mente (sinestesia). Escuchando tonos y acordes mientras pintaba, Kandinski afirma que, por ejemplo, el amarillo es el color del centro en una trompeta de latón, negro es el color del cierre y el fin de las cosas, y que las combinaciones de colores producen frecuencias vibratorias, similares a los acordes tocados en un piano. Kandinski también desarrolló una teoría de las figuras geométricas y sus relaciones, afirmando, por ejemplo, que el círculo es la forma más pacífica y representa el alma humana. Estas teorías se explican en "Punto y línea sobre el plano" (ver más abajo).

Durante los estudios que Kandinski hizo en preparación para "Composición IV", se sintió agotado y se fue a dar un paseo. Mientras estaba fuera, Gabriele Münter arregló su estudio y sin querer le dio vuelta el lienzo. Al regresar y ver la tela (sin reconocerla) Kandinski cayó de rodillas y lloró, diciendo que era la pintura más hermosa que había visto nunca. Había sido liberado del apego a un objeto. Como la primera vez que vio "Pilas de Heno" de Monet, experiencia que cambiaría su vida.

En otro episodio con Münter durante los años expresionistas-abstractos bávaros, Kandinski estaba trabajando en su "Composición VI" que le tomó casi seis meses de estudio y preparación. La obra pretendía evocar una inundación, el bautismo, la destrucción y renacimiento al mismo tiempo. Después de describir el trabajo sobre un panel de madera de tamaño mural, se quedó bloqueado y no pudo continuar. Münter le dijo que él estaba atrapado en su intelecto y no alcanzaba el verdadero sujeto de la fotografía. Le sugirió que se limitara a repetir la palabra "uberflut" ("diluvio" o "inundación") y se centrara en su sonido y no su significado. Repitiendo esta palabra como un mantra, Kandinski pintó y completó la obra monumental en un lapso de tres días.

El análisis sobre las formas y los colores de Kandinski resulta, no de simples y arbitrarias asociaciones con ideas, sino de la experiencia interior del pintor. Pasó años creando pinturas abstractas, sensorialmente ricas, trabajando con formas y colores sin descanso, observando sus pinturas y las de otros artistas, teniendo en cuenta sus efectos sobre su sentido del color. Esta experiencia subjetiva es algo que el filósofo francés Michel Henry llama "subjetividad absoluta" o la "absoluta vida fenomenológica ".

Publicado en 1911, el libro de Kandinski compara lo espiritual en la vida de la humanidad a una pirámide: el artista tiene la misión de guiar a otros a la cima con su obra. La punta de la pirámide son esos pocos artistas, grandes. Se trata de una pirámide espiritual, avanzando y ascendiendo lentamente, incluso si a veces parece inmóvil. Durante los períodos decadentes, el alma se hunde hasta el fondo de la pirámide, la humanidad solo busca el éxito externo, haciendo caso omiso de las fuerzas espirituales.

La evolución del Arte y del mundo espiritual se produce en libertad, fuera de influencias, a veces se centra en el color y otras veces se centra en las formas.

La evolución en la pintura depende de las formas y los colores, de cómo se vayan utilizando y combinando. Una serie de figuras iguales pueden transmitir un mismo mensaje, sin embargo, si existe una variación de color y/o formas en la composición, el mensaje se distorsiona.

Al igual que los colores las formas tendrán sus efectos espirituales, los triángulos se relaciona más con los tonos cálidos (amarillo, rojo) esto por la agudeza de sus ángulos, en el caso de los colores profundos se relacionan con formas más cuadradas y redondas.

La espiritualidad humana va a reaccionar según como el artista utilice las formas y los colores; con un simple color se pueden transmitir diferentes sentimientos, como tristeza o alegría, dependiendo de su matiz.

Las propiedades obvias que podemos ver cuando miramos un color aislado y se le deja actuar solo, por un lado son la calidez o frialdad del tono de color y por el otro la claridad u oscuridad de ese tono. El calor es una tendencia hacia el amarillo, y la frialdad una tendencia hacia el azul; amarillo y azul, forman el primer gran contraste y dinámica. El amarillo tiene un movimiento excéntrico y el azul un movimiento concéntrico; una superficie amarilla parece moverse más cerca de nosotros, mientras que una superficie azul parece alejarse. El amarillo es un color típicamente terrestre, cuya violencia puede ser dolorosa y agresiva. El azul es un color celeste, que evoca una profunda calma. La combinación de los rendimientos de azul y amarillo da un resultado de inmovilidad y de calma, que es el verde.

La claridad es una tendencia hacia el blanco, y la oscuridad es una tendencia hacia el negro. Blanco y negro forman el gran contraste segundo, que es estático. El blanco es un silencio profundo, absoluto, lleno de posibilidades. El negro es la nada sin posibilidad, un silencio eterno sin esperanza, y se corresponde con la muerte. La mezcla de blanco con negro da gris, que no posee ninguna fuerza activa y cuya tonalidad es cercana a la de verde. El gris corresponde a la inmovilidad sin esperanza, pero tiende a la desesperación cuando se pone oscuro, recuperando un poco de esperanza cuando se ilumina.

El rojo es un color cálido, alegre y agitado, es contundente, un movimiento en sí mismo. Mezclado con negro se vuelve marrón, un color fuerte. Mezclado con amarillo, gana en calidez y se vuelve naranja, que imparte un movimiento de irradiación en sus alrededores. Cuando se mezcla con rojo y azul se aleja para convertirse en púrpura, que es un rojo fresco. Rojo y verde forman el gran contraste tercero, y naranja y púrpura el cuarto.

El ojo humano puede relacionar todo tipo de vivencias y sentimientos por medio de los colores y las formas. No necesariamente tiene que tener una representación exacta para identificarse con la misma. No es sino el uso correcto de los colores y/o formas lo que hace que una pintura pueda transmitir un mensaje e incluso armonizar con el alma humana.

En sus escritos Kandinski analizó los elementos geométricos que componen cada pintura: el punto y la línea. Llamó al soporte físico y la superficie del material en el que el artista dibuja o pinta el plano básico, o PB. Él no los analiza objetivamente, sino desde el punto de vista de su efecto sobre el observador interior.

Un punto es un elemento pequeño de color formulado por el artista en el lienzo. No es ni un punto geométrico ni una abstracción matemática, sino que es la extensión, forma y color. Esta forma puede ser un cuadrado, un triángulo, un círculo, una estrella o algo más complejo. El punto es la forma más concisa, aunque de acuerdo con su ubicación en el plano básico tomará una tonalidad diferente. Puede ser aislado o resuenan con otros puntos o líneas.

Una línea es el producto de una fuerza que se ha aplicado en una dirección dada: la fuerza ejercida sobre el lápiz o pincel por el artista. Las formas producidas lineales pueden ser de varios tipos: una línea recta, que resulta de una fuerza única aplicado en una sola dirección; una línea angular, como resultado de la alternancia de dos fuerzas en diferentes direcciones, o una curva (o en forma de onda) línea, producido por el efecto de dos fuerzas que actúan simultáneamente. Un avión se pueden obtener por condensación (desde unos girar alrededor de la línea de uno de sus extremos).

El efecto subjetivo producido por una línea depende de su orientación: una línea horizontal corresponde con el suelo en el que el hombre se apoya y se mueve, sino que posee una tonalidad afectiva oscura y fría similar a la de color negro o azul. Una línea vertical corresponde con la altura, y no ofrece ningún apoyo, sino que posee una tonalidad luminosa cálida próximo al blanco y amarillo. Una diagonal posee una más o menos caliente o frío tonalidad, de acuerdo con su inclinación hacia la horizontal o la vertical.
Una fuerza que se despliega, sin obstáculos, como la que produce una línea recta se corresponde con lirismo, varias fuerzas que se enfrentan (o molestar) sí forman un drama. El ángulo formado por la línea angular también tiene una sonoridad interior que es cálido y cercano al amarillo para un ángulo agudo (un triángulo), el frío y similar a azul por un ángulo obtuso (un círculo), y similar al rojo por un ángulo recto (un cuadrado).

El plano de base es, en general, rectangular o cuadrada. por lo tanto, que se compone de líneas horizontales y verticales que delimitan y definen como una entidad autónoma que soporta la pintura, comunicando su tonalidad afectiva. Esta tonalidad se determina por la importancia relativa de las líneas horizontales y verticales: las horizontales que dan una tonalidad calma, frío al plano de base, mientras que las verticales impartir una tonalidad calma, cálido. El artista intuye el efecto interior del formato de lienzo y dimensiones, que él elige de acuerdo a la tonalidad que quiere dar a su obra. Kandinski considera el plano básico de un ser vivo, que el artista "fecunda" y se siente "respirar".

Cada parte del plano básico posee una coloración afectiva, lo que influye en la tonalidad de los elementos pictóricos que redactará en él, y contribuye a la riqueza de la composición resultante de la yuxtaposición en el lienzo. Lo anterior del plano básico se corresponde con soltura y ligereza que, mientras que la continuación evoca la condensación y la pesadez. El trabajo del pintor es escuchar y conocer estos efectos para producir pinturas que no son solo el resultado de un proceso al azar, sino el fruto del trabajo auténtico y el resultado de un esfuerzo hacia la belleza interior.



</doc>
<doc id="4586" url="https://es.wikipedia.org/wiki?curid=4586" title="Enlace">
Enlace

Enlace hace referencia a varios artículos:






</doc>
<doc id="4589" url="https://es.wikipedia.org/wiki?curid=4589" title="Genealogía">
Genealogía

Genealogía (del latín "genealogia", "genos" , ': raza, nacimiento, generación, descendencia + "logos" , ': ciencia, estudio) también conocida como historia familiar, es el estudio y seguimiento de la ascendencia y descendencia de una persona o familia. También se llama así al documento que registra dicho estudio expresado como árbol genealógico. La genealogía es una de las Ciencias Auxiliares de la Historia y es trabajada por un genealogista. El objetivo principal en genealogía es identificar todos los ascendientes y descendientes en un particular árbol genealógico y recoger datos personales sobre ellos. Como mínimo, estos datos incluyen el nombre de la persona y la fecha y/o lugar de nacimiento, matrimonio y muerte. 

Lo primero al iniciar una investigación genealógica es recopilar la mayor cantidad de antecedentes a través de dos fuentes: orales y documentales. Estos antecedentes deben comprender nombres de personas, lugares y fechas. En caso que se desconozca la fecha exacta, se puede utilizar una aproximación.

Las fuentes orales son aquellas que se obtienen verbalmente de otra persona, generalmente dentro del núcleo familiar, padres, abuelos, tíos, primos, bisabuelos. Estas fuentes, dado que están nutridas de la tradición familiar, suelen ser inexactas en cuanto a fechas de nacimiento, bautizos, matrimonios y defunciones, profesiones y lugares de origen. Sin embargo, ofrecen un acervo de información que muchas veces no se encuentra documentada, además de permitir determinar el marco general familiar como punto de partida del trabajo posterior.

Lo mejor es consultar con aquellos miembros de mayor edad dentro de la familia extendida, cualquier antecedente por insignificante que parezca puede llegar a servir. Si también vive dentro de una comunidad pequeña, se debe consultar con las personas de mayor edad que vivan en ella o en sus inmediaciones.

Existen datos que pueden obtenerse exclusivamente de fuentes orales, bien sea por no existir documentación, por ejemplo el padre de un hijo natural no reconocido, o bien porque haya sido destruido el documento durante catástrofes naturales, accidentes o guerras, por lo que siempre es recomendable validar la información con personas y autores coetáneos, sin que ello implique despreciar la fuente primaria oral.

Es recomendable sistematizar siempre la información obtenida, creando fichas personales para cada persona que se está investigando, y dejando siempre bien definido quién fue la persona que informó de dichos datos. Estas fichas pueden tener un formato tanto físico como electrónico, utilizándose, generalmente, en este último caso programas o softwares genealógicos de tipo comercial (software propietario) o libres, algunos de ellos de gran calidad.

Las fuentes documentales son aquellas que se pueden encontrar en cualquier medio escrito (sea impreso o manuscrito). Quienes investigan una genealogía acuden a éstas una vez que han agotado todos los recursos que la memoria intrafamiliar pueda dar, tanto para corroborar la información verbal, como para ampliar la información y retroceder la búsqueda en el tiempo.

Son los documentos escritos que se hallan en posesión de una familia o comunidad y son traspasados de una generación a otra. Estos documentos generalmente son inéditos y son copias únicas de valiosa información y en sí constituyen un archivo. El contenido de estos archivos va desde cartas personales hasta documentos legales, como copias de expedientes, títulos de dominio, libretas de familia, etc. En algunos casos estos archivos, por estar en poder de particulares, no son custodiados bajo estándares bibliotecológicos que permitan su conservación en el tiempo, sea por manipulación o por almacenamiento. Por estas razones sus propietarios en algunos casos donan estos documentos a alguna institución seria, como los archivos nacionales, para evitar su destrucción o pérdida, mientras que en otros casos son tan ocultados que sólo se conocen hasta que muere el dueño de ellos y en la mayoría de las veces se encuentran en un estado de deterioro casi total.

Dependiendo del país se podrán rastrear antecedentes en las oficinas del Registro Civil hasta aproximadamente 1871 (si bien en Francia existen desde la época de la Revolución francesa, y en muchos lugares de España hay registros locales en los respectivos ayuntamientos, algunos desde los años 40 del siglo XIX). Los datos que manejan los registros civiles son nacimientos, defunciones, matrimonios, divorcios, condenas judiciales, nacionalizaciones.

Si se desea consultar por personas en fechas anteriores a la creación de los registros civiles, es aconsejable acudir a las parroquias que correspondan al domicilio de las personas investigadas. En ellas se encuentran libros de bautismos, defunciones y matrimonios. Todas las Parroquias creadas desde el siglo XVI en adelante tienen la obligación de llevar estos libros. 

En 1563 el Concilio de Trento instauró de forma oficial la obligación de registrar en los libros parroquiales las actas de bautismo, boda y defunción. A partir de ese momento los libros sacramentales registran los hechos vitales de cada individuo bautizado en la fe cristiana. De este modo, los registros parroquiales conservan una parte fundamental de la memoria histórica de algunos países, principalmente los colonizados por españoles, cuyo legado fue la enseñanza de la religión católica. Estos registros poblacionales eran realizados por el corregidor o máxima autoridad del recinto y luego eran entregados a un representante de la parroquia. Antes de 1563, no era obligación llevar registros, por lo que a veces la búsqueda de ancestros se detiene en esta fecha. Sin embargo, existen algunas parroquias donde es posible encontrar libros desde el siglo XIII en adelante. 

Hay que tener en cuenta que, en muchos países, la información contenida en los libros parroquiales es traspasada periódicamente a los Archivos Diocesanos, situados normalmente en la sede del obispado al que pertenece la parroquia. Esto es importante, porque muchas veces en la parroquias se han perdido por diversas razones (incendios, guerras, robos, mala conservación) los libros originales. Sin embargo, existe una copia de las inscripciones en los archivos diocesanos. Por otra parte, la La Iglesia de Jesucristo de los Santos de los Últimos Días o iglesia mormona ha hecho convenios con algunos obispados para microfilmar tanto los archivos diocesanos como los libros parroquiales, estando en algunos casos esta información disponible por internet en su base de datos. No obstante, en el caso de los microfilmes que no han sido digitalizados, esta información aparece incompleta, ya que no figuran nombres de testigos o padrinos, y a veces los nombres de las personas han sido mal transcritos, resultando imposible encontrarlos en su buscador, razón por la cual en algunos casos se debe consultar directamente estos registros en sus centros de documentación.

Otras fuentes importantes de datos son los Archivos notariales, que guardan información y documentos emanados por las actuales notarías y las antiguas escribanías, en los cuales se pueden hallar testamentos, cartas de dote, transacciones comerciales, ventas, arriendos, etc. y en general todos aquellos documentos suscritos entre particulares.

En casi todos los países existe un Fondo de Documentos Históricos o Archivo Histórico en el que se depositan cada cierta cantidad de años los documentos que generan los diversos organismos públicos o estatales durante su gestión, vale decir expedientes judiciales, expedientes militares, hojas de vida de los funcionarios públicos, nóminas de inmigrantes, censos, etc. También en estos archivos se reciben las donaciones de documentos de particulares, que pueden contener cartas, nóminas de empleados y una serie de documentos inéditos.

Muchos antecedentes que conciernen a los países hispanoamericanos se encuentran en el Archivo General de Indias, organismo que recibió la documentación generada por las colonias españolas hasta su independencia.

En países más descentralizados, como México y España, los estados o comunidades mantienen sus propios archivos donde se suelen encontrar archivos notariales y documentos relacionados con temas de tierras y aguas. 
Asimismo es aconsejable consultar las publicaciones que realizan periódicamente los institutos, asociaciones y academias de genealogía, historia y geografía de cada país donde se está realizando la investigación: siempre cabe la posibilidad de que ya se haya hecho un estudio sobre la familia o apellido que se desea investigar. Además algunas universidades y fundaciones mantienen guías de fuentes documentales.

Hay que destacar la existencia de listas de correo, en las cuales se suele encontrar la colaboración desinteresada de otras personas que realizan su propio árbol genealógico.

Estas listas son generalmente monográficas por ámbitos geográficos, aunque existen igualmente algunas dedicadas a algún apellido concreto.

Actualmente existen varias iniciativas para incorporar dentro de los proyectos de la Wikimedia Foundation. Todas ellas cumplen con la condición de ser gratuitas y de acceso libre para añadir, consultar y editar registros o fichas genealógicas y utilizan softwares del tipo "open-source" MediaWiki - del mismo tipo que se usa en otros proyectos de WikiMedia Foundation.

Listas de genealogía por tipo de licencia





</doc>
<doc id="4594" url="https://es.wikipedia.org/wiki?curid=4594" title="Francisco Nieva">
Francisco Nieva

Francisco Morales Nieva (Valdepeñas, Ciudad Real, 29 de diciembre de 1924-Madrid, 10 de noviembre de 2016) fue un dramaturgo, escenógrafo, director de escena, narrador, ensayista y dibujante español.

Académico de la Real Academia Española desde 1990, donde ocupó el , su producción teatral le valió el Premio Nacional de Teatro en dos ocasiones (1980 y 1992), el Premio Nacional de Literatura en la modalidad de Literatura Dramática y el Premio Valle-Inclán (2011), por la escritura y dirección de "Tórtolas, crepúsculo y... telón". Por su producción literaria en general, se le otorgó el Premio Príncipe de Asturias de las Letras en 1992.

El propio Francisco Nieva escribió un volumen de memorias titulado "Las cosas como fueron" (2002). Su familia descendía de conversos ricos emigrados a España en el siglo XVII y fue bisnieto del helenista y sacerdote Ciriaco Cruz. El padre, el oficial del Ayuntamiento de Valdepeñas Francisco Morales, fue gobernador civil de Toledo con la República en 1931, y allí se trasladaron a vivir hasta que estalló la guerra en 1936. Un tío, Cirilo del Río, fue ministro también durante los años de la República y el abuelo paterno fue durante muchos años Presidente de la Diputación Provincial. 

Sus padres amaban la cultura y el arte y llevaban a sus hijos al teatro, al cine y a la zarzuela. El abuelo materno era aficionado a la ópera y a la lectura y la abuela tocaba con gran destreza el piano. El hermano de Francisco, Ignacio, llegó a ser un notable compositor; en cuanto a él, se vio atraído por el arte muy tempranamente, en especial por el teatro, y se entretenía con escenografías, títeres y muñecos de cartón. La Guerra Civil les pilló en Valdepeñas y durante 1939 vivieron aislados en una casa de Sierra Morena, en el término de Venta de Cárdenas; de vuelta a su lugar natal recibió clases particulares de Juan Alcaide, un poeta relacionado con el "Postismo" y amigo de Carlos Edmundo de Ory, y con él lee obras clásicas y modernas españolas y europeas ("La Celestina", José Gutiérrez Solana, Alfred Jarry). Su familia decidió emigrar a Madrid en 1945, donde Nieva estudió pintura en la Real Academia de Bellas Artes de San Fernando y se hizo amigo de Eduardo Chicharro Briones y Ory, líderes "postistas", intentando abrirse paso como autor plástico dentro de ese movimiento de vanguardia de posguerra, que contaba en sus filas con otros representantes manchegos como Ángel Crespo. Mientras su hermano compositor se hacía pastor evangélico y emigraba a Estados Unidos y Puerto Rico, él residió entre 1953 y 1963 en París, donde asistió al estreno de "Esperando a Godot" de Samuel Beckett, disfrutando de una beca concedida en 1953 por el doctor Piterbag, un judío argentino del Instituto Francés de París, y allí trabajó como pintor y dibujante, en medio de un ambiente sumamente bohemio. En el entorno del fallecido Antonin Artaud, esboza su obra "El combate de Ópalos y Tasia" y recibe el premio Polignac por el conjunto de su obra artística (1963). En París se casa por el rito protestante con Geneviève Escande, que ocupa un alto cargo en el Centre National de la Recherche Scientifique, conoce al editor de los surrealistas José Corti y alterna con conocidos hispanistas franceses, publicando estudios pioneros sobre la influencia de Miguel de Cervantes en el teatro de García Lorca y la plástica en la obra de Valle-Inclán. Tras residir un año en Venecia, regresó a Madrid en 1964 y, salvo largas estancias en Berlín y Roma, permaneció afincado en esta ciudad, entregado a su trabajo como escenógrafo, autor dramático y colaborador de diversas publicaciones periódicas.

Como escenógrafo su labor empezó de la mano de José Luis Alonso Mañés, con quien colaboró realizando los escenarios de "El rey se muere" de Ionesco para el teatro María Guerrero. Trabajó después con Adolfo Marsillach en las escenografías de "Pigmalión" de George Bernard Shaw y "Después de la caída" de Arthur Miller. Estos trabajos lo transformaron en una figura de referencia en su campo, y a lo largo de los años sesenta se ocupó de "La dama duende" de Pedro Calderón de la Barca, "El zapato de raso" de Paul Claudel, "El burlador de Sevilla" de Tirso de Molina, "El señor Adrián" de Carlos Arniches y, por supuesto, "Marat-Sade" de Peter Weiss, de nuevo bajo la dirección de Adolfo Marsillach y Antonio Malonda.

Sin embargo, se mantuvo inédito como escritor teatral hasta que publicó en "Primer Acto" y representó privadamente "Es bueno no tener cabeza" en 1971. Sus ideas teatrales se expresaron en el texto conocido como "Breve poética teatral" en torno a los conceptos de "transgresión", "contravalor" y "culpa"; pretende exhibir escénicamente lo prohibido como si fuera lo más anodino, convencional y corriente en lo público ("contravalor") en busca de una liberación (catarsis) total. Esta poética bebe fundamentalmente del Artaud que conoció en París, pero también de Alfred Jarry, Ghelderode, Eugène Ionesco, Samuel Beckett y Jean Genet; lo original de Nieva es insertar conscientemente esta vanguardia en la tradición literaria española de lo grotesco y lo esperpéntico, otorgando a lo cómico un papel fundamental en lograr dicha inversión, prosiguiendo la tradición de Cervantes, Quevedo, José Gutiérrez Solana y Valle-Inclán.

Aunque clasificó su teatro inicialmente en "Teatro furioso", "Teatro de farsa y calamidad" y "Teatro de crónica y estampa", la publicación de su "Teatro completo" en 1991 le hizo distinguir otros grupos:


Esta clasificación se modificó en la edición de sus "Obras completas" (2007), donde distingue seis grupos: "Centón de teatro", "Teatro Furioso", "Teatro de Farsa y Calamidad", "Teatro de crónica y estampa", "Tres versiones libres" y "Varia teatral".

A todas estas obras hay que añadir sus adaptaciones de clásicos; fuera de la ya citada de Larra, están "La paz" de Aristófanes, "Los baños de Argel" de Cervantes, "Casandra" de Galdós, "Las aventuras de Tirante el Blanco" de Joanot Martorell, "El manuscrito encontrado en Zaragoza" de Jan Potocki, "Divinas palabras" de Valle-Inclán adaptada a la ópera con música de Antón García-Abril, "Don Álvaro o la fuerza del sino", del Duque de Rivas, "El desdén con el desdén", de Agustín Moreto y "Electra", de Benito Pérez Galdós.

Como director de escena montó óperas, zarzuelas y ballets como "Cinderella" de Sergei Prokofiev, "Capricho español" de Enrique Granados, "La vida breve" de Manuel de Falla, "L'heure espagnole" de Maurice Ravel, "Pepita Jiménez" de Isaac Albéniz, "Tosca" de Giacomo Puccini, "Curro Vargas" de Ruperto Chapí, "I due Foscari" de Giuseppe Verdi, "Don Giovanni" de Mozart y "La señorita Cristina" de Luis de Pablo.

Su última vertiente fue la de novelista: escribió "El viaje a Pantaélica" (1994), "La llama vestida de negro" (1995), "Granada de las mil noches" (1995), "Oceánida" (1996), "Carne de murciélago. Cuento de Madrid" (1998)...

Colaborador habitual del diario "La Razón", de Madrid, en 1990 ingresó en la Real Academia Española con un discurso titulado "Esencia y paradigma del género chico".

Ya desde 1949 había escrito teatro, pero solo empezó a estrenarlo y publicarlo a partir de 1971.

La obra dramática de Francisco Nieva puede dividirse en dos grandes grupos, llamados por el autor "Teatro furioso" y "Teatro de farsa y calamidad". Un modelo más imaginativo y vitalista que el teatro popular desarrollado de la posguerra española, con un lenguaje ‘valleinclanesco’ y una escenografía barroca.

Su obra narrativa es una prolongación de ese particular universo, con ejemplos como "El viaje a Pantaélica", "Oceánida" y "La llama vestida de negro: novela de misterios y sobrecogimiento" (1995), una especie de ciclo narrativo protagonizado por el caballero gallego Cambicio de Santiago, que se prolonga con "Granada de las mil noches" y "La mutación del primo mentiroso" (I Premio de Novela Ducado de Loeches en 2004).

Aunque ya existía una edición de su "Teatro completo" de 1991 en dos volúmenes, la "Obra completa" publicada en 2007 modifica bastante los textos, recopilando en dos tomos de unas 2500 páginas cada uno toda su producción. El primero está dedicado a su Teatro y el segundo recoge toda la Narrativa y una selección de artículos.









</doc>
<doc id="4595" url="https://es.wikipedia.org/wiki?curid=4595" title="La Fura dels Baus">
La Fura dels Baus

La Fura dels Baus es una compañía de teatro española creada en 1979 por Marcel·lí Antúnez Roca, Carlus Padrissa, Pere Tantinyà, Quico Palomar y Teresa Puig. Autodefinidos como un grupo de teatro «de fricción» que busca un espacio escénico distinto del tradicional, sus montajes y productos diversos han evolucionado mezclando imaginación, morbosidad, performance, mecatrónica e instalaciones de gran espectacularidad, en un contexto dramático de creación colectiva.

En la actualidad, la compañía funciona como una empresa artística de grandes espectáculos, que integra diversos registros del teatro de texto, el teatro digital, la ópera y el género cinematográfico. Su equipo está integrado por cientos de personas, incluyendo actores, atletas, funambulistas, técnicos, diseñadores, gestores y colaboradores, entre otros. Entre los miembros más reconocidos que han pasado por la compañía se encuentran, además de los mismos fundadores, Àlex Ollé, Miki Espuma, Jürgen Müller, Pep Gatell, Jordi Arús, Hansel Cereza y Michael Summers, Quico Palomar, Teresa Puig y Mireia Romero. Entre los colaboradores más cercanos, figura el actor Eduard Fernández, ganador de un Goya por su interpretación en "Fausto 5.0".

Su vasta trayectoria suma a la fecha cerca de tres mil representaciones y alrededor de tres millones de espectadores.

El nombre de la compañía, intraducible, hace alusión a un hurón («fura» en catalán) supuestamente endémico de una zona conocida como "Els Balçans" o "Els Baus", un torrente que cruza la localidad de Moyá, más tarde convertido en vertedero.

La Fura dels Baus, se inició como grupo de teatro de calle en la localidad catalana de Moyá a fines de los años 1970, haciendo pasacalles y participando en fiestas y «entoldados». En 1979, los directores de entonces, Marcel·lí Antúnez Roca, Carlus Padrissa y Pere Tantinyà, abandonaron su pueblo de origen para trasladarse a Barcelona.

Allí el grupo comenzó a integrar herramientas del teatro independiente, para luego desarrollar un estilo más personal, en el que se inscribe la mayoría de sus montajes del primer periodo, entre 1979 y 1989. Antúnez Roca abandonó el colectivo a fines de este periodo.

La compañía saltó a la fama durante los años 1990, luego de su participación en la ceremonia de apertura de los Juegos Olímpicos de Barcelona 1992, como compañía de teatro experimental, junto a la compañía Comediants, de teatro más tradicional, y a distinguidos músicos internacionales, tales como Ryūichi Sakamoto. A este evento le siguieron millonarias contrataciones para promocionar reconocidas marcas comerciales, como Pepsi, Mercedes-Benz, Peugeot, Volkswagen, Swatch, Airtel, Microsoft, Absolut Vodka, Columbia Pictures, Warner Bros., Puerto de Barcelona, Telecom Italia o Sun Microsystems, así como la contratación para la realización de masivas participaciones en eventos tales como los Juegos Mediterráneos de 2005, en Almería; el SummerTyne 2007, en Newcastle; o el Perth International Arts Festival 2010, en Perth.
Desde 1996 la compañía también se inició en la realización de óperas, con "Atlántida", de Manuel de Falla y Ernesto Halffter. A esta ópera le siguieron varias otras, entre ellas "El martirio de San Sebastián" de Claude Debussy, al año siguiente; "La condenación de Fausto", de Hector Berlioz, para el Festival de Salzburgo en 1999, o "La fábula de Orfeo", de Claudio Monteverdi, realizado en las bodegas de su barco «Naumon» en el Puerto Viejo de Barcelona por el 400 aniversario de su estreno en 1607, en la ciudad italiana de Mantua.

En cuanto a otros formatos, la compañía realizó en 1997 "Work in progress", un espectáculo de teatro digital donde a través de Internet se conectaban escenas que ocurrían simultáneamente en diversas ciudades. En 2001 también se adentraron en la industria cinematográfica, con la película "Fausto 5.0", en colaboración con el director Isidro Ortiz.

La compañía también ha realizado giras por América Latina. En 2011 abrió el Bicentenario de Uruguay, y más tarde realizó funciones en espacios como el Teatro Colón de Buenos Aires (2012) y el Teatro Municipal de Santiago de Chile.

Escrito por Andreu Morte a principios de los años ochenta para reivindicar el lenguaje furero, basado en la sensualización del medio escénico. Lo definen como un acto de rebelión contra la literatura dramática. Es una propuesta de la colectivización de la dramaturgia. La defensa de su teatro físico en intenso. Andreu Morte afirma en él los inicios del grupo, su proceso de cohesión, su entusiasmo vital y su deseo de transgredir la creación escénica tradicional. 

El manifiesto, escrito en 1983, es el siguiente:


Los trabajos de la compañía buscan estimular la imaginación y también provocar al espectador, a través de elementos de morbosidad, performance, mecatrónica e instalaciones de gran espectacularidad.

En sus creaciones más personales utilizan lo que ellos mismos denominan el «lenguaje furero», esto es, el uso de procesos de creación colectiva a partir de ejercicios de desinhibición actorales, que dan lugar a espectáculos en espacios no convencionales, donde los actores interactúan con el público, la música, el movimiento y una escenografía que recurre a diversos materiales orgánicos, industriales y tecnológicos.

El estilo de sus trabajos no ha estado exento de controversias. Algunos críticos han reprochado su cierta incapacidad para despertar sentimientos positivos que vayan más allá de las sensaciones elementales que genera la espectacularidad. Pese a lo anterior, la compañía ha presentado montajes como "Ascenso y caída de la ciudad de Mahagonny", que han generado una destacada recepción del público.

Caracterizados por la estética de la crueldad. Batallas, carne cruda, utilización violenta de las máquinas. Nacimiento, muerte, sexo, comida. Libertad para la provocación y la experimentación. Participación del espectador y desarrollo de las obras en lugares no convencionales. Equilibrio entre lo popular y lo sofisticado, lo atávico y lo tecnológico, lo corporal y las prótesis mecánicas;  carnalidad y misticismo; naturaleza y artificio; grosería y sofisticación; primitivismo y tecnología. Ambición y grandiosidad en sus macro espectáculos, la red humana, la máquina humana, el engranaje humano, la esfera humana. Gran productora de danza y espectáculo pero conservando su esencia, que les caracteriza desde que eran un grupo de colegas. «Su mayor aportación a la ópera ha sido en lo visual. Han unido la carnalidad de los cuerpos presentes y los virtuales con proyecciones hipersofisticadas de una calidad casi cinematográfica. Desde el inicio, La Fura ha sabido descubrir el aspecto performativo de lo digital», apunta Mercè Gatell. 

Todo esto son características, elementos del lenguaje “furero”, su esencia y evolución, su interior y su esqueleto, cuyas piezas ha ido uniéndose con los años en torno a un eje joven, transgresor, que busca libertad, que es ambicioso, que quiere divertirse, experimentar y aprender. La Fura ha ido durante 40 años cada vez más lejos, expandiéndose cada vez más, en cuanto a geografía, espacio escénico, número de participantes, narrativa y trascendencia. Desde el carro, la mula y los pasacalles de teatro improvisado en las calles de los pueblecitos de Barcelona; hasta los macro espectáculos, un referente en la ópera, los Juegos Olímpicos de Barcelona, un navío-teatro.

Muestra elementos que se consolidarán en el lenguaje propio de la formación. Sin un argumento definido. Encadena acciones que combinan la música y la performance. Las acciones se inician en alto pero caen al nivel del espectador. Poniendo un punto de atención en la evolución del actor. 

Música en directo y con sintetizadores. El desnudo, el barro, la suciedad, alimentos crudos. Destrucción de dos hombres trajeados de un coche. Efectos pirotécnicos. Pintura y peleas de comida. Sobre paredes blancas, sobre lonas, utilizando la arquitectura del espacio. 

Hombre primordial, juega, descubre, crea el mito y el lenguaje. Hombre violentado por la conciencia de sí mismo. Individuo que ha puesto la materia a su servicio. Los intérpretes juegan con todos lo elementos. Con ello aprenden y crea el lenguaje y el mito, superponiéndose a los dioses en este acto. El rito.

Aprovechando el espacio arquitectónico de la sala, las máquinas, carros, plataformas móviles, bañeras con ruedas, y la disposición del público, y la transformación de esta. 

Evolución de los dos espectáculos anteriores. Lenguaje consolidado. Un ciclo con momentos identificables. Circularidad inevitable del tiempo. Dramaturgia más elaborada. 

Tres personajes: el dios blanco, el inútil y el enano. Individuo, humanidad, poder, y sus relaciones. Enfrentados por la dominación de personajes que simbolizan la humanidad. Establecen todo un sistema de jerarquías dentro del espectáculo.

Ceremonia de inauguración de los Juegos Olímpicos de Barcelona en 1992. Macro espectáculo visto por millones de personas en todo el mundo.

Narraba el viaje de Jasón y los argonautas hasta el confín del mar Mediterráneo, hasta las columnas de Hércules, que según la mitología clásica eran la puerta al mundo desconocido. En el transcurso del viaje que se representó en el Estadio Olímpico, los argonautas se enfrentaron, en la mejor tradición clásica, a las furias que representaban la guerra, la contaminación, el hambre y la enfermedad: las furias de los primeros navegantes tienen su equivalente actual. El espectáculo culminaba cuando Hércules, después de cruzar el recinto, separaba las columnas y permitía que el mar, formado por cientos de personas, fluyera e inundara lo desconocido, en clara alegoría del encuentro entre culturas, razas y pueblos.

Revolución en la tradición que hasta ese momento había definido a los espectáculos de apertura de los Juegos Olímpicos, meras evoluciones de masas, coloristas y desprovistas de contenido, esteticistas y poco arriesgadas en lo conceptual.

Cantata escénica La Antártida, de Manuel de Falla. Partitura que se quedó a medias a la muerte del compositor, con fragmentos definidos y claros y otros solamente esbozados. 

Terminada por encargo muchos años después de su muerte. Basto oratorio que combina textos en catalán de la obra homónima con otros religiosos en latín y castellano. Rica simbología. 

La Fura dels Baus propone en su adaptación escénica un nuevo hilo argumental que tiene que ver con su propia peripecia, la de la obra, y con la reunión de tres grandes creadores, Falla, Verdaguer y Sert, que convenció al primero de la necesidad de darle a La Atlántida el carácter de cantata escénica y que se comprometió a crear la escenografía que debía tener en su estreno, pero que nunca consiguió terminar. Falla, Verdaguer y Sert aparecen en esta Atlántida como tres personajes que asisten a la representación de su obra mientras la desarrollan; que la contemplan mientras la crean, sin ser conscientes de que están atrapados en ella como el resto de los personajes.

Relectura de la obra de Eurípides. Con elementos clave del lenguaje de la Fura. Proyecciones. Elenco multitudinario. Respetando texto original. Elementos metálicos. Plataformas móviles. Agua. Reconocible de la Fura. 

Con este espectáculo retoman elementos de sus orígenes. Compartiendo espacio con el público, no hay escenario, sí diferencia de nivel, permiten a los espectadores interactuar con los intérpretes y viceversa. Estímulo constante para el público. 

Entre los espectáculos de lenguaje «furero» se pueden citar:
La Fura dels Baus ha protagonizado también algunos macroespectáculos y eventos especiales, entre ellos:

Han realizado también puestas en escena de óperas:

Entre los espectáculos concebidos para escenario de tipo teatral se cuentan:

Para cine, ha realizado:

También han editado música en formato discográfico:




</doc>
<doc id="4596" url="https://es.wikipedia.org/wiki?curid=4596" title="Física estadística">
Física estadística

La física estadística o mecánica estadística es una rama de la física que mediante la teoría de la probabilidad es capaz de deducir el comportamiento de los sistemas físicos macroscópicos constituidos por una cantidad estadísticamente significativa de componentes equivalentes a partir de ciertas hipótesis sobre los elementos o "partículas" que los conforman y sus interacciones mutuas.

Los sistemas macroscópicos son aquellos que tienen un número de partículas cercano a la constante de Avogadro, cuyo valor, de aproximadamente formula_1, es increíblemente grande, por lo que el tamaño de dichos sistemas suele ser de escalas cotidianas para el ser humano, aunque el tamaño de cada partícula constituyente sea de escala atómica. Un ejemplo de un sistema macroscópico sería, por ejemplo, un vaso de agua.

La importancia del uso de las técnicas estadísticas para estudiar estos sistemas radica en que, al tratarse de sistemas tan grandes es imposible, incluso para las más avanzadas computadoras, llevar un registro del estado físico de cada partícula y predecir el comportamiento del sistema mediante las leyes de la mecánica, además del hecho de que resulta impracticable el conocer tanta información de un sistema real.

La utilidad de la física estadística consiste en ligar el comportamiento microscópico de los sistemas con su comportamiento macroscópico o colectivo, de modo que, conociendo el comportamiento de uno, pueden averiguarse detalles del comportamiento del otro. Permite describir numerosos campos de naturaleza estocástica como las reacciones nucleares; los sistemas biológicos, químicos, neurológicos, entre otros.

Empíricamente, la termodinámica ha estudiado los gases y ha establecido su comportamiento macroscópico con alto grado de acierto. Gracias a la física estadística es posible deducir las leyes termodinámicas que rigen el comportamiento macroscópico de un gas, como la ecuación de estado del gas ideal o la ley de Boyle-Mariotte, a partir de la suposición de que las partículas en el gas no están sometidas a ningún potencial y se mueven libremente con una energía cinética igual a:

colisionando entre sí y con las paredes del recipiente de forma elástica (sin fuerzas disipativas). El comportamiento colectivo del gas depende de tan solo unas pocas variables macroscópicas (como la presión, el volumen y la temperatura). Este enfoque particular para estudiar el comportamiento de los gases se llama teoría cinética.

Para predecir el comportamiento de un gas, la mecánica exigiría calcular la trayectoria exacta de cada una de las partículas que lo componen (lo cual es un problema inabordable). La termodinámica hace algo radicalmente opuesto, establece unos principios cualitativamente diferentes a los mecánicos para estudiar una serie de propiedades macroscópicas sin preguntarse en absoluto por la naturaleza "real" de la materia de estudio. La mecánica estadística media entre ambas aproximaciones: ignora los comportamientos individuales de las partículas, preocupándose en vez de ello por promedios. De esta forma podemos calcular las propiedades termodinámicas de un gas a partir de nuestro conocimiento genérico de las moléculas que lo componen aplicando leyes mecánicas.

En el siglo XVIII Daniel Bernoulli aplica razonamientos estadísticos para explicar el comportamiento de sistemas de fluidos.

Los años cincuenta del siglo XIX marcaron un hito en el estudio de los sistemas térmicos. Por esos años la termodinámica, que había crecido básicamente mediante el estudio experimental del comportamiento macroscópico de los sistemas físicos a partir de los trabajos de Nicolas Léonard Sadi Carnot, James Prescott Joule, Clausius y Kelvin, era una disciplina estable de la física. Las conclusiones teóricas deducidas de las primeras dos leyes de la termodinámica coincidían con los resultados experimentales. Al mismo tiempo, la teoría cinética de los gases, que se había basado más en la especulación que en los cálculos, comenzó a emerger como una teoría matemática real. Sin embargo, fue hasta que Ludwig Boltzmann en 1872 desarrolló su teorema H y de este modo estableciera el enlace directo entre la entropía y la dinámica molecular. Prácticamente al mismo tiempo, la teoría cinética comenzó a dar a luz a su sofisticado sucesor: la teoría del ensamble.

El poder de las técnicas que finalmente emergieron redujo la categoría de la termodinámica de "esencial" a ser una consecuencia de tratar estadísticamente un gran número de partículas que actuaban bajo las leyes de la mecánica clásica. Fue natural, por tanto, que esta nueva disciplina terminara por denominarse mecánica estadística o física estadística.

La mecánica estadística puede construirse sobre las leyes de la mecánica clásica o la mecánica cuántica, según sea la naturaleza del problema a estudiar. Aunque, a decir verdad, las técnicas de la mecánica estadística pueden aplicarse a campos ajenos a la propia física, como por ejemplo en economía. Así, se ha usado la física estadística para deducir la "distribución de la renta", y la distribución de Pareto para las rentas altas puede deducirse mediante la mecánica estadística, suponiendo un estado de equilibrio estacionario para las mismas (ver econofísica).

La relación entre estados microscópicos y macroscópicos (es decir, la termodinámica) viene dada por la famosa fórmula de Ludwig Boltzmann de la entropía:

donde formula_4 es el número de estados microscópicos compatibles con una energía, volumen y número de partículas dado y formula_5 es la constante de Boltzmann.

En el término de la izquierda tenemos la termodinámica mediante la entropía definida en función de sus variables naturales, lo que da una información termodinámica completa del sistema. A la derecha tenemos las configuraciones microscópicas que definen la entropía mediante esta fórmula. Estas configuraciones se obtienen teniendo en cuenta el modelo que hagamos del sistema "real" a través de su hamiltoniano mecánico.

Esta relación, propuesta por Ludwig Boltzmann, no la aceptó inicialmente la comunidad científica, en parte debido a que contiene implícita la existencia de átomos, que no estaba demostrada hasta entonces. Esa respuesta del medio científico, dicen, hizo que Boltzmann, desahuciado, decidiera quitarse la vida.

Actualmente esta expresión no es la más apropiada para realizar cálculos reales. Ésta es la llamada ecuación puente en el Colectivo Micro Canónico. Existen otros colectivos, como el Colectivo Canónico o el Colectividad macrocanónica, que son de más interés práctico.

El postulado fundamental de la mecánica estadística, conocido también como "postulado de equiprobabilidad a priori", es el siguiente:

Este postulado fundamental es crucial para la mecánica estadística, y afirma que un sistema en equilibrio no tiene ninguna preferencia por ninguno de los microestados disponibles para ese equilibrio. Si Ω es el número de microestados disponibles para una cierta energía, entonces la probabilidad de encontrar el sistema en uno cualquiera de esos microestados es "p" = 1/Ω.

El postulado es necesario para poder afirmar que, dado un sistema en equilibrio, el estado termodinámico (macroestado) que está asociado a un mayor número de microestados es el macroestado más probable del sistema. Puede ligarse a la función de teoría de la información, dada por:

Cuando todas las rho son iguales, la función de información "I" alcanza un mínimo. Así, en el macroestado más probable además es siempre uno para el que existe una mínima información sobre el microestado del sistema. De eso se desprende que en un sistema aislado en equilibrio la entropía sea máxima (la entropía puede considerarse como una medida de desorden: a mayor desorden, mayor desinformación y, por tanto, un menor valor de "I").

En todos los libros de termodinámica se interpreta la entropía como una medida del desorden del sistema. De hecho, a veces se enuncia el segundo principio de la termodinámica diciendo: "El desorden de un sistema aislado sólo aumenta".

Es importante saber que esta relación viene, como acabamos de saber, de la mecánica estadística. La termodinámica no es capaz de establecer esta relación por sí misma, pues no se preocupa en absoluto por los estados microscópicos. En este sentido, la mecánica estadística es capaz de "demostrar" la termodinámica, ya que, partiendo de unos principios más elementales (a saber, los mecánicos), obtiene por deducción estadística el segundo principio. Fue esa la gran contribución matemática de Ludwig Boltzmann a la termodinámica.

La formulación moderna de esta teoría se basa en la descripción del sistema físico por un elenco de conjuntos o colectividad que representa la totalidad de configuraciones posibles y las probabilidades de realización de cada una de las configuraciones.

A cada colectividad se le asocia una función de partición que, por manipulaciones matemáticas, permite extraer los valores termodinámicos del sistema. Según la relación del sistema con el resto del Universo, se distinguen generalmente tres tipos de colectividades, en orden creciente de complejidad:

<noinclude>



</doc>
<doc id="4598" url="https://es.wikipedia.org/wiki?curid=4598" title="Relatividad general">
Relatividad general

La teoría general de la relatividad o relatividad general es una teoría del campo gravitatorio y de los sistemas de referencia generales, publicada por Albert Einstein en 1915 y 1916.

El nombre de la teoría se debe a que generaliza la llamada teoría especial de la relatividad y el principio de relatividad para un observador arbitrario. Los principios fundamentales introducidos en esta generalización son el principio de equivalencia, que describe la aceleración y la gravedad como aspectos distintos de la misma realidad, la noción de la curvatura del espacio-tiempo y el principio de covariancia generalizado. La teoría de la relatividad general propone que la propia geometría del espacio-tiempo se ve afectada por la presencia de materia, de lo cual resulta una teoría relativista del campo gravitatorio. De hecho la teoría de la relatividad general predice que el espacio-tiempo no será plano en presencia de materia y que la curvatura del espacio-tiempo será percibida como un campo gravitatorio.

La intuición básica de Einstein fue postular que en un punto concreto no se puede distinguir experimentalmente entre un cuerpo acelerado uniformemente y un campo gravitatorio uniforme. La teoría general de la relatividad permitió también reformular el campo de la cosmología.

Einstein expresó el propósito de la teoría de la relatividad general para aplicar plenamente el programa de Ernst Mach de la relativización de todos los efectos de inercia, incluso añadiendo la llamada "constante cosmológica" a sus ecuaciones de campo para este propósito. Este punto de contacto real de la influencia de Ernst Mach fue claramente identificado en 1918, cuando Einstein distingue lo que él bautizó como el "principio de Mach" (los efectos inerciales se derivan de la interacción de los cuerpos) del principio de la relatividad general, que se interpreta ahora como el principio de covariancia general.

El matemático alemán David Hilbert escribió e hizo públicas las ecuaciones de la covariancia antes que Einstein. Ello resultó en no pocas acusaciones de plagio contra Einstein, pero probablemente sea más, porque es una teoría (o perspectiva) geométrica. La misma postula que la presencia de masa o energía «curva» el espacio-tiempo, y esta curvatura afecta la trayectoria de los cuerpos móviles e incluso la trayectoria de la luz.

Poco después de la formulación de la teoría de la relatividad especial en 1905, Albert Einstein comenzó a elucubrar cómo describir los fenómenos gravitatorios con ayuda de la nueva mecánica. En 1907 se embarcó en la búsqueda de una nueva teoría relativista de la gravedad que duraría ocho años. Después de numerosos desvíos y falsos comienzos, su trabajo culminó el 25 noviembre de 1915 con la presentación a la Academia Prusiana de las Ciencias de su artículo, que contenía las que hoy son conocidas como "Ecuaciones de Campo de Einstein". Estas ecuaciones forman el núcleo de la teoría y especifican cómo la densidad local de materia y energía determina la geometría del espacio-tiempo. 

Las ecuaciones de campo de Einstein son no lineales y muy difíciles de resolver. Einstein utilizó los métodos de aproximación en la elaboración de las predicciones iniciales de la teoría. Pero ya en 1916, el astrofísico Karl Schwarzschild encontró la primera solución exacta no trivial de las Ecuaciones de Campo de Einstein, la llamada Métrica de Schwarzschild. Esta solución sentó las bases para la descripción de las etapas finales de un colapso gravitacional, y los objetos que hoy conocemos como agujeros negros. En el mismo año, se iniciaron los primeros pasos hacia la generalización de la solución de Schwarzschild a los objetos con carga eléctrica, obteniéndose así la solución de Reissner-Nordström, ahora asociada con la carga eléctrica de los agujeros negros.

En 1917, Einstein aplicó su teoría al universo en su conjunto, iniciando el campo de la cosmología relativista. En línea con el pensamiento contemporáneo, en el que se suponía que el universo era estático, agregó a sus ecuaciones una constante cosmológica para reproducir esa "observación". En 1929, sin embargo, el trabajo de Hubble y otros demostraron que nuestro universo se está expandiendo. Esto es fácilmente descrito por las soluciones encontradas por Friedmann en 1922 para la expansión cosmológica, que no requieren de una constante cosmológica. Lemaître utilizó estas soluciones para formular la primera versión de los modelos del Big Bang, en la que nuestro universo ha evolucionado desde un estado anterior extremadamente caliente y denso. Einstein declaró más tarde que agregar esa constante cosmológica a sus ecuaciones fue el mayor error de su vida. 

Durante ese período, la relatividad general se mantuvo como una especie de curiosidad entre las teorías físicas. Fue claramente superior a la gravedad newtoniana, siendo consistente con la relatividad especial y contestaba varios efectos no explicados por la teoría newtoniana. El mismo Einstein había demostrado en 1915 cómo su teoría lograba explicar el avance del perihelio anómalo del planeta Mercurio sin ningún parámetro arbitrario. Del mismo modo, en una expedición de 1919 liderada por Eddington confirmaron la predicción de la relatividad general para la desviación de la luz estelar por el Sol durante el eclipse total de Sol del 29 de mayo de 1919, haciendo famoso a Einstein instantáneamente. Sin embargo, esta teoría ha entrado en la corriente de la física teórica y la astrofísica desarrolladas aproximadamente entre 1960 y 1975, ahora conocido como la edad de oro de la relatividad general. Los físicos empezaron a comprender el concepto de agujero negro, y a identificar la manifestación de objetos astrofísicos como los cuásares. Cada vez más precisas, las pruebas del sistema solar confirmaron el poder predictivo de la teoría, y la cosmología relativista, también se volvió susceptible a encaminar pruebas observacionales.

Los éxitos explicativos de la teoría de la relatividad especial condujeron a la aceptación de la teoría prácticamente por la totalidad de los físicos. Eso llevó a que antes de la formulación de la relatividad general existieran dos teorías físicas incompatibles:

La necesidad de buscar una teoría que integrase, como casos límites particulares, las dos anteriores requería la búsqueda de una teoría de la gravedad que fuese compatible con los nuevos principios relativistas introducidos por Einstein. Además de incluir la gravitación en una teoría de formulación covariante, hubo otra razón adicional. Einstein había concebido la teoría especial de la relatividad como una teoría aplicable solo a sistemas de referencia inerciales, aunque realmente puede generalizarse a sistemas acelerados sin necesidad de introducir todo el aparato de la relatividad general. La insatisfacción de Einstein con su creencia de que la teoría era aplicable solo a sistemas inerciales le llevó a buscar una teoría que proporcionara descripciones físicas adecuadas para un sistema de referencia totalmente general.

Esta búsqueda era necesaria, ya que según la relatividad especial "ninguna información puede viajar a mayor velocidad que la luz", y por lo tanto no puede existir relación de causalidad entre dos eventos unidos por un intervalo de tipo espacio (space-like). Sin embargo, uno de los pilares fundamentales de la gravedad newtoniana, el principio de acción a distancia, supone que las alteraciones producidas en el campo gravitatorio se transmiten instantáneamente a través del espacio. La contradicción entre ambas teorías es evidente, puesto que asumir las tesis de Newton llevaría implícita la posibilidad de que un observador fuera afectado por las perturbaciones gravitatorias producidas fuera de su cono de luz.

Einstein resolvió este problema interpretando los fenómenos gravitatorios como simples alteraciones de la curvatura del espacio-tiempo producidas por la presencia de masas. De ello se deduce que el campo gravitatorio, al igual que el campo electromagnético, tiene una entidad física independiente y sus variaciones se transmiten a una velocidad finita en forma de ondas gravitacionales. La presencia de masa, energía o momentum en una determinada región de la variedad tetradimensional, provoca la alteración de los coeficientes de la métrica, en una forma cuyos detalles pormenorizados analizaremos en las secciones siguientes.

En esta visión, la gravitación solo sería una pseudo-fuerza (equivalente a la fuerza de Coriolis, o a la fuerza centrífuga) efecto de haber escogido un sistema de referencia no-inercial.

Las características esenciales de la teoría de la relatividad general son las siguientes:

El principio de covariancia es la generalización de la teoría de la relatividad especial, donde se busca que las leyes físicas tengan la misma forma en todos los sistemas de referencia. Esto último equivale a que todos los sistemas de referencia sean indistinguibles, y desde el punto de vista físico equivalentes. En otras palabras, que cualquiera que sea el movimiento de los observadores, las ecuaciones tendrán la misma forma matemática y contendrán los mismos términos. Ésta fue la principal motivación de Einstein para que estudiara y postulara la relatividad general. 

El principio de covariancia sugería que las leyes debían escribirse en términos de tensores, cuyas leyes de transformación covariantes y contravariantes podían proporcionar la "invarianza" de forma buscada, satisfaciéndose el principio físico de covariancia.

Un hito fundamental en el desarrollo de la teoría de la relatividad general lo constituye el principio de equivalencia, enunciado por Albert Einstein en el año 1912 y al que su autor calificó como «"la idea más feliz de mi vida"». Dicho principio supone que un sistema que se encuentra en caída libre y otro que se mueve en una región del espacio-tiempo sin gravedad se encuentran en un estado físico similar: en ambos casos se trata de sistemas inerciales.

Galileo distinguía entre cuerpos de movimiento inercial (en reposo o moviéndose a velocidad constante) y cuerpos de movimiento no inercial (sometidos a un movimiento acelerado). En virtud de la segunda ley de Newton (que se remonta a los trabajos del dominico español Domingo de Soto), toda aceleración estaba causada por la aplicación de una fuerza exterior. La relación entre fuerza y aceleración se expresaba mediante esta fórmula:

donde "a" es la aceleración, "F" la fuerza y "m" la masa. La fuerza podía ser de origen mecánico, electromagnético o, cómo no, gravitatorio. Según los cálculos de Galileo, la aceleración gravitatoria de los cuerpos era constante y equivalía a 9,8 m/s sobre la superficie terrestre. La fuerza con la que un cuerpo era atraído hacia el centro de la Tierra se denominaba peso. Evidentemente, según los principios de la mecánica clásica un cuerpo en caída libre no es un sistema inercial, puesto que se mueve aceleradamente dentro del campo gravitatorio en que se encuentra.

Sin embargo, la teoría de la relatividad considera que los efectos gravitatorios no son creados por fuerza alguna, sino que encuentran su causa en la curvatura del espacio-tiempo generada por la presencia de materia. Por ello, un cuerpo en caída libre es un sistema (localmente) inercial, ya que no está sometido a ninguna fuerza (porque la gravedad tiene este carácter en relatividad general). Un observador situado en un sistema inercial (como una nave en órbita) no experimenta ninguna aceleración y es incapaz de discernir si está atravesando o no, un campo gravitatorio. Como consecuencia de ello, las leyes de la física se comportan como si no existiera curvatura gravitatoria alguna. De ahí que el principio de equivalencia también reciba el nombre de Invariancia Local de Lorentz: En los sistemas inerciales rigen los principios y axiomas de la relatividad especial.

El principio de equivalencia implica asimismo que los observadores situados en reposo sobre la superficie de la tierra no son sistemas inerciales (experimentan una aceleración de origen gravitatorio de unos 9,8 metros por segundo al cuadrado, es decir, "sienten su peso").

Aunque la mecánica clásica tiene en cuenta la aceleración medida por un observador en reposo respecto al campo gravitatorio (p. ej. un astrónomo); el Principio de Equivalencia, contrariamente, toma en consideración la aceleración experimentada por un observador situado en el sistema en cuestión: cualquier cuerpo que se mueva sin restricciones por un campo gravitatorio puede ser considerado como un sistema inercial. Es el caso de los planetas que orbitan en torno del Sol y de los satélites que orbitan alrededor de los primeros: los habitantes de la Tierra no llegan a percibir si nos estamos acercando o alejando del Sol, ni si nos encontramos en el afelio o en el perihelio, a pesar de las enormes diferencias de la gravedad solar.

La gravedad se convierte, en virtud del Principio de Equivalencia, en una fuerza aparente, como la fuerza centrífuga y la fuerza de Coriolis: en estos dos últimos supuestos su aparición es debida a la elección de un marco de referencia acelerado (un observador situado en la superficie de una esfera en rotación). En el caso de la gravedad, únicamente percibimos la "fuerza aparente gravitatoria" cuando escogemos un sistema de referencia no inercial (en reposo sobre la superficie terrestre), pero no cuando nos situamos en otro que sí lo es (un cuerpo en caída libre).

Aunque el principio de equivalencia fue históricamente importante en el desarrollo de la teoría, no es un ingrediente necesario de una teoría de la gravedad, como prueba el hecho de que otras teorías métricas de la gravedad, como la teoría relativista de la gravitación prescindan del principio de equivalencia. Además conviene señalar que el principio de equivalencia no se cumple en presencia de campos electromagnéticos, por ejemplo una partícula cargada moviéndose a lo largo de una geodésica de un espacio-tiempo cualquiera en general emitirá radiación, a diferencia de una partícula cargada moviéndose a lo largo de una geodésica del espacio de Minkowski. Ese y otros hechos sugieren que el principio de equivalencia a pesar de su equivalencia histórica no es parte esencial de una teoría relativista de la gravitación.

La aceptación del principio de equivalencia por Albert Einstein le llevó a un descubrimiento ulterior: la contracción o curvatura del tiempo como consecuencia de la presencia de un campo gravitatorio, que quedó expresado en su artículo de 1911 ""Sobre la influencia de la gravedad en la propagación de la luz"".

Supongamos que un fotón emitido por una estrella cercana se aproxima a la Tierra. En virtud de la ley de conservación del tetramomentum la energía conservada del fotón permanece invariante. Por otro lado, el principio de equivalencia implica que un observador situado en el fotón (que es un sistema inercial, es decir, se halla en caída libre) no experimenta ninguno de los efectos originados por el campo gravitatorio terrestre. De ello se deduce que la energía conservada del fotón no se altera como consecuencia de la acción de la gravedad, y tampoco lo hace la frecuencia de la luz, ya que, según la conocida fórmula de la física cuántica, la energía de un fotón es igual a su frecuencia "v" multiplicada por la constante de Planck "h": "E = hν".
Ahora bien, si las observaciones las realizara un astrónomo situado en la superficie de la Tierra, esto es, en reposo respecto su campo gravitatorio, los resultados serían muy diferentes: el astrónomo podría comprobar cómo el fotón, por efecto de su caída hacia la Tierra, va absorbiendo progresivamente energía potencial gravitatoria y, como consecuencia de esto último, su frecuencia se corre hacia el azul. Los fenómenos de absorción de energía por los fotones en caída libre y corrimiento hacia el azul se expresan matemáticamente mediante las siguientes ecuaciones:

donde formula_4 es la energía medida por un observador en reposo respecto al campo gravitatorio (en este caso un astrónomo), formula_5 el potencial gravitatorio de la región donde se encuentra éste, formula_6 la energía conservada del fotón, formula_7 la frecuencia de emisión, formula_8 es la frecuencia percibida por el observador (y corrida hacia el azul) y formula_9 la constante de Planck.

Ahora bien, en el párrafo anterior hemos demostrado que la energía conservada del fotón permanece invariante. Por tanto, ¿cómo es posible que exista esta divergencia entre los resultados de la medición de la energía obtenidos por el astrónomo (formula_10) y la energía conservada del fotón (formula_11)? La única manera de resolver esta contradicción es considerando que el tiempo se ralentiza como consecuencia de la presencia de un campo gravitatorio. De este modo, la citada ecuación:

puede escribirse de este modo:

\frac{\mbox{ciclos}}{\Delta t_{em}} e^{-\Phi}</math>

Es decir, la frecuencia es igual al número de ciclos que tienen lugar en un determinado período (generalmente, un segundo). Donde formula_12 es el tiempo medido por un observador situado a una distancia infinita del cuerpo masivo (y por lo tanto no experimenta la atracción gravitatoria de éste), mientras que formula_13 es el tiempo medido por un observador bajo la influencia del campo gravitatorio y en reposo respecto a este (como, por ejemplo, una persona situada sobre la superficie terrestre). De ahí se deduce que cerca de un cuerpo masivo el tiempo se ralentiza, siguiendo estas reglas matemáticas:

En una singularidad espacio-temporal (como las que existen en el interior de los agujeros negros), la densidad de masa-materia y el campo gravitatorio tienden al infinito, lo que provoca la congelación del tiempo y por lo tanto la eliminación de todo tipo de procesos dinámicos:

La contracción del tiempo debido a la presencia de un campo gravitatorio fue confirmada experimentalmente en el año 1959 por el experimento Pound-Rebka-Snider, llevado a cabo en la universidad de Harvard. Se colocaron detectores electromagnéticos a una cierta altura y se procedió a emitir radiación desde el suelo. Todas las mediciones que se realizaron confirmaron que los fotones habían experimentado un corrimiento hacia el rojo durante su ascenso a través del campo gravitatorio terrestre.

Hoy en día, el fenómeno de la contracción del tiempo tiene cierta importancia en el marco del servicio localizador GPS, cuyas exigencias de exactitud requieren de una precisión extrema: Basta con que se produzca un retraso de 0.04 microsegundos en la señal para que se produzca un error de posicionamiento de unos 10 metros. De ahí que las ecuaciones de Einstein hayan de ser tenidas en cuenta al calcular la situación exacta de un determinado objeto sobre la superficie terrestre.

Desde un punto de vista teórico, el artículo de Einstein de 1911 tuvo una importancia aún mayor. Pues, la contracción del tiempo conllevaba también, en virtud de los principios de la relatividad especial, la contracción del espacio. De ahí que fuera inevitable a partir de este momento descartar la existencia de un espacio-tiempo llano, y fuera necesario asumir la curvatura de la variedad espacio-temporal como consecuencia de la presencia de masas.

En la relatividad general, fenómenos que la mecánica clásica atribuye a la acción de la fuerza de gravedad, tales como una caída libre, la órbita de un planeta o la trayectoria de una nave espacial, son interpretados como efectos geométricos del movimiento en un espacio-tiempo curvado. De hecho una partícula libre en un campo gravitatorio sigue líneas de curvatura mínima a través de este espacio tiempo-curvado.

Finalmente, podemos hacer referencia a la desviación de los rayos de la luz como consecuencia de la presencia de un cuerpo masivo, fenómeno que da lugar a efectos ópticos como las lentes gravitacionales o los anillos de Einstein.

Frente de onda desviado. Lente gravitacional. Experimento de Eddington.

Matemáticamente, Einstein conjeturó que la geometría del universo deja de ser euclidiana por la presencia de masas. Einstein modelizó que el universo era un tipo de espacio-tiempo curvo mediante una variedad pseudoriemanniana y sus ecuaciones de campo establecen que la curvatura seccional de esta variedad en un punto está relacionada directamente con el tensor de energía-momento en dicho punto.

Dicho tensor es una medida de la densidad de materia y energía. La curvatura "le dice a la materia como moverse", y de forma recíproca la "materia le dice al espacio como curvarse". En términos más precisos las trayectorias de las partículas se ven afectadas por la curvatura, y la presencia de muchas partículas en una región altera notoriamente la curvatura. La relatividad general se distingue de otras teorías alternativas de la gravedad por la simplicidad de acoplamiento entre materia y curvatura. 

Aunque todavía no existe una teoría cuántica de la gravedad que incorpore tanto a la mecánica cuántica como a la teoría de la relatividad general y que proponga una ecuación de campo gravitatorio que sustituya a la de Einstein, pocos físicos dudan que una teoría cuántica de la gravedad pondrá a la relatividad general en el límite apropiado, así como la relatividad general predice la ley de la gravedad en el límite no relativista.

Uno de los conceptos esenciales sobre el que gira toda la teoría de la relatividad general es el de derivada covariante (a veces impropiamente llamada conexión afín), que fue definida por primera vez por el matemático italiano Tullio Levi-Civita y que puede ser considerada tanto desde una perspectiva física como desde otra matemática. 

Desde un punto de vista físico, la derivada ordinaria de la velocidad es la aceleración de un cuerpo medida por un observador externo en reposo respecto a un campo gravitatorio (por ejemplo, un astrónomo situado sobre la superficie terrestre). En este caso el observador se mantiene a una distancia r constante del centro de masas, pero no así el objeto observado, que si consideramos que está en caída libre, progresivamente se irá aproximando al origen del campo gravitatorio, y el observador externo detectará que tiene una aceleración constante g.

Por el contrario, la "derivada covariante de la velocidad" formula_16 ó formula_17 es la aceleración medida por un "observador comóvil", es decir, que está en reposo respecto al cuerpo en caída libre (por ejemplo, el piloto de un avión en caída libre o los tripulantes de una nave espacial con sus motores apagados) y que a diferencia de la derivada ordinaria, no detectará ninguna aceleración, a menos que el piloto encienda los motores o que algún meteorito lo impacte.

En resumidas cuentas, la derivada ordinaria se utiliza para computar la aceleración ordinaria de un cuerpo, mientras que la derivada covariante es empleada para calcular su aceleración inercial. Según la mecánica galileana y newtoniana estos dos tipos de aceleración son idénticos, y sobre la base de este axioma se desarrollaron nuevos principios mecánicos como el Principio de d'Alembert. Sin embargo, del principio de equivalencia de Einstein se deduce que cuando un cuerpo está en caída libre tiene una aceleración ordinaria que depende de la masa del cuerpo sobre el cual está cayendo, pero su aceleración inercial es nula, a menos que se le aplique alguna otra fuerza. De ahí que para Einstein fuera absolutamente necesario introducir en su teoría el concepto de derivada covariante.

Desde un punto de vista estrictamente matemático, el cálculo de la derivada covariante tiene lugar a través de un sencillo procedimiento. Se procede en primer lugar al cómputo de la derivada parcial covariante y luego se generaliza ésta.

La derivada ordinaria se aplica exclusivamente sobre los componentes de un vector, mientras que la derivada covariante se aplica también sobre las bases del espacio vectorial, ya que la percepción del espacio-tiempo dependerá de la velocidad del "observador comóvil":

Sobre esta ecuación procedemos a aplicar la regla del producto (o de Leibniz),

Llegados a este punto introducimos una nueva notación, los símbolos de Christoffel, que pueden ser definidos como el componente formula_20 de la derivada parcial de formula_21 respecto a formula_22: formula_23. De este modo:

Realizamos un intercambio de índices (formula_20 por formula_26) en el último término del segundo miembro de la ecuación:

Y obtenemos con ello los componentes de la derivada parcial covariante de la velocidad, que equivalen a la expresión entre paréntesis:

Generalizamos dichos componentes multiplicándolos por el componente formula_22 de la tetravelocidad (formula_31) y obtenemos con ello la derivada covariante de la velocidad:

Puesto que para un observador inercial (p. ej. un cuerpo en caída libre) formula_34, esta última ecuación toma la siguiente forma:

Estas fórmulas reciben el nombre de ecuación de las líneas geodésicas, y se utilizan para calcular la aceleración gravitatoria de cualquier cuerpo.

A los lectores principiantes puede chocarles la propia definición de los símbolos de Christoffel. A fin de cuentas, en el espacio euclideo, la derivada de una base (por ejemplo formula_37) respecto a otra coordenada (pongamos formula_38) es siempre cero, por la simple razón de que las bases de ambas coordenadas son ortogonales. Sin embargo, esto no sucede así en las variedades curvas, como por ejemplo las superficies de un cilindro o de una esfera: En tales casos, los símbolos de Christoffel no son iguales a cero, sino que son funciones de las derivadas del tensor métrico. La relación matemática entre estas dos magnitudes matemáticas se expresa mediante la siguiente ecuación:

Los símbolos de Christoffel constituyen el parámetro principal que determina cuán grande es el grado de curvatura existente en una región determinada y con su ayuda podemos conocer cuál va a ser la trayectoria de una geodésica en un espacio curvo. En el caso de la variedad espacio-temporal, la Teoría de la Relatividad afirma que la curvatura viene originada por la presencia de tetramomentum y por ello, cuanta mayor sea la densidad de materia existente en una determinada región, mayores serán los valores de los símbolos de Christoffel.

En un espacio-tiempo curvo, las leyes de la física se modifican mediante el Principio de acoplamiento mínimo, que supone que las ecuaciones matemáticas en cuya virtud se expresan aquellas experimentan las siguientes modificaciones:


De este modo, la ecuación galileana de los sistemas inerciales se transforma en virtud de dicho principio en la ecuación relativista de las líneas geodésicas:

Ley de conservación de la energía:

Sin embargo, en virtud del principio de simetría de los símbolos de Christoffel, las leyes electromagnéticas en general no experimentan modificaciones debidas a la curvatura gravitatoria:


La medición de la curvatura de cualquier variedad (ya se trate del espacio-tiempo, de una esfera o de una silla de montar) viene determinada por el tensor de curvatura o tensor de Riemann, que es una función de los símbolos de Christoffel y sus derivadas de primer orden.

El tensor de Riemann tiene una importancia fundamental a la hora de calcular la desviación de dos líneas en origen paralelas cuando se desplazan a través de una superficie curva. Es bien sabido que en una variedad llana las líneas paralelas jamás se cortan, sin embargo esta regla no rige en el caso de las superficies curvas de geometría elíptica. Supongamos que dos viajeros salen del Ecuador en dirección norte. En ambos casos, el ángulo que la trayectoria de su barco forma con el Ecuador es inicialmente de 90º, por lo que se trata de dos líneas paralelas. Sin embargo, conforme los viajeros se van desplazando hacia el norte, su distancia recíproca se hace cada vez más pequeña hasta que se hace nula en el Polo Norte, que es donde se cortan sus trayectorias de viaje. Para calcular la tasa de aproximación entre las dos geodésicas utilizamos la siguiente ecuación:

donde formula_48 y formula_49 representan el recorrido desde el Ecuador de ambas líneas geodésicas y formula_50 la distancia de separación entre ellas.
En el espacio-tiempo, que también es una variedad curva, las cosas funcionan de un modo parecido: el tensor de Riemann determina la aceleración recíproca entre las líneas de universo de dos sistemas inerciales (p. ej. dos asteroides que se acercan progresivamente como consecuencia de su mutua atracción gravitatoria). Para calcular dicha aceleración, aplicamos de nuevo la conocida fórmula, modificándola ligeramente:
donde formula_51 es un parámetro afín (el tiempo local) y formula_52 y formula_53 son los vectores de cuadrivelocidad de ambos cuerpos que, según el esquema de Minkowski, equivalen geométricamente a campos vectoriales tangentes a ambas líneas de universo.
Todo esto nos conecta con lo que en física newtoniana se denominan "fuerzas de marea", responsables de múltiples fenómenos astronómicos y cuya base teórica reposa en el planteamiento siguiente: Supongamos que una determinada nave espacial está cayendo a un agujero negro. Es evidente que la proa de la nave experimenta una fuerza gravitatoria más intensa que la popa, por el simple hecho de que la primera está más próxima que la segunda al horizonte de sucesos. Si la diferencia de aceleraciones entre la proa y la popa es lo suficientemente intensa, la nave puede llegar a distorsionarse y quebrarse definitivamente.

El gradiente gravitatorio es también responsable del ciclo de mareas: Las zonas de la tierra más cercanas a la Luna, experimentan una mayor atracción gravitatoria que las más lejanas a ella, lo que provoca que el agua del mar se acumule en aquellas áreas de la superficie terrestre que están alineadas con la Luna.

En relatividad general, la aceleración de marea viene originada por el tensor de Riemann. Hay una correspondencia casi natural entre las ecuaciones newtonianas y las relativistas. En efecto, la ecuación newtoniana utilizada para computar las fuerzas de marea es la siguiente:

donde a es la aceleración de marea, formula_54 el potencial gravitatorio y formula_55 la distancia entre las dos partículas. Las fuerzas de marea vienen determinadas por las derivadas de segundo orden del potencial gravitatorio.

Desde el punto de vista relativista, las fuerzas de marea vienen determinadas por el tensor de Riemann y si la región del espacio tiene una escasa densidad de cuadrimomento y una distribución uniforme de la curvatura, los componentes toman aproximadamente los valores siguientes:

De ahí que sea muy simple deducir la ecuación clásica partir de la relativista:

Como se puede deducir de los párrafos anteriores, en relatividad general las fuerzas de marea están determinadas por el tensor de Riemann y las primeras derivadas de los símbolos de Christoffel. Si estas magnitudes tienen un valor no nulo, el diferencial de los símbolos de Christoffel provoca la dispersión de las geodésicas correspondientes a partículas de un fluido determinado.

Las geodésicas (trayectorias inerciales en el espacio-tiempo) vienen determinadas por los valores de los símbolos de Christoffel. Si éstos son constantes, las partículas de un fluido se mueven uniformemente, a una misma velocidad y aceleración, y no se altera su distancia entre sí. Pero si los componentes de los símbolos de Christoffel varían a lo largo de una determinada región, ello conlleva la divergencia de las líneas de universo de las partículas y la distorsión del fluido, en la medida en que cada una de sus partes constituyentes acelera distintamente.
Las fuerzas de marea y el tensor de Riemann tienen una importancia fundamental en la formación y configuración de los sistemas planetarios, así como en multitud de procesos astrofísicos y cosmológicos. Sirva de ejemplo nuestro propio Sistema Solar: Hace cerca de 4.500 millones de años, una nube molecular alcanzó la densidad y la compresión suficientes como para transformarse en un sistema planetario. La mayor parte del material de la nube se precipitó sobre en torno al núcleo, dando lugar al Sol. Sin embargo, ciertas cantidades de gas y de polvo continuaron rotando bajo la forma de un disco de acreción, y se aglutinaron para dar origen a planetesimales y posteriormente a planetas.
Sin embargo, en la zona situada entre Marte y Júpiter, los tensores de Riemann correspondientes a las masas del Sol y de Júpiter generaron unas intensas fuerzas de marea que dispersaron las líneas de universo de los planetesimales allí situados, impidiendo que se agregaran entre sí para dar lugar a un cuerpo masivo. Los planetesimales permanecieran dispersos bajo la forma de un cinturón de asteroides. Este fenómeno que acaba de describirse no es exclusivo de nuestro Sistema Solar, sino que ha sido observado en multitud de sistemas exoplanetarios descubiertos desde principios de los años noventa hasta la actualidad, como los mostrados en las ilustraciones de esta sección.

Las fuerzas de marea también poseen cierta importancia en el desarrollo de otros fenómenos astronómicos como las supernovas de tipo II, deflagraciones cósmicas que suelen tener lugar en el marco de sistemas estelares dobles. En efecto, en los sistemas binarios es frecuente que una estrella masiva orbite alrededor de una enana blanca. Si el tamaño de la primera sobrepasa el límite de Roche, el componente del tensor de Riemann formula_61 generado por la masa de la enana blanca extrae material de las capas exteriores de su compañera y lo precipita sobre la enana blanca, en torno a la cual dicho material orbita formando un disco de acreción. El plasma queda sometido a enormes temperaturas que provocan la emisión de rayos X y la aparición de explosiones periódicas conocidas con el nombre de supernovas de tipo II.

Según la teoría laplaciana-newtoniana de la gravitación universal, una masa esférica de gas reduce su volumen (como consecuencia de la atracción recíproca de sus moléculas) con una aceleración equivalente a formula_62:

Es evidente, que dicha ecuación no es compatible con la relatividad especial, por las razones reseñadas anteriormente:

En este sentido, cabe señalar que en un espacio-tiempo curvo la aceleración del volumen viene cuantificada por un objeto geométrico específico, el tensor de Ricci formula_65, que puede definirse como la aceleración coordenada del hipervolumen formula_66, normal al vector unitario formula_67. De este modo, el componente formula_68 expresa la aceleración temporal del volumen tridimensional:

La relación entre el tensor métrico y el tensor de Ricci se expresa a través de la llamada "ecuación de flujo de Ricci", que tiene la forma siguiente:

Según esta ecuación, la existencia de valores positivos del tensor de Ricci implica la disminución a lo largo del tiempo de los coeficientes del tensor métrico, y como consecuencia de ello la disminución de los volúmenes en esa región de la variedad. Por el contrario, la presencia de valores negativos en el tensor de Ricci lleva consigo una expansión progresiva de las distancias, las superficies y los volúmenes.

Por todo lo dicho, los tensores de energía-momentum y de Ricci permitían expresar de manera tensorial y covariante la "fórmula de Poisson", y de ahí que originalmente Einstein propusiera las siguientes "ecuaciones de universo":

En relatividad general, el tensor de Ricci tiene la virtualidad de representar aquellos efectos gravitatorios originados por la presencia inmediata y local de cuadrimomento, que son con gran diferencia los más importantes a pequeña y gran escala.

El tensor de Ricci rige, pues, la mayor parte de los procesos astrofísicos que tienen lugar en el Cosmos: constituye una medida de la contracción de nubes moleculares que dan lugar al nacimiento de estrellas y planetas; cuantifica el colapso de las grandes cuerpos estelares y su conversión en enanas blancas, estrellas de neutrones y agujeros negros; y proporciona una medida de la expansión del universo.

Del tensor de Ricci, particularmente de la forma que toma en los campos gravitatorios esféricos (como las estrellas estáticas), se deriva la llamada "Ley de equilibrio hidrostático", que regula el equilibrio entre la presión del fluido estelar (que tiende a expandir el volumen de la estrella) y la curvatura gravitatoria (que lo contrae). Este equilibrio se mantiene prácticamente durante toda la vida de la estrella y solo se rompe en dos ocasiones diferentes: 1) Cuando la estrella deviene en una gigante roja, en cuyo caso los efectos de la presión de radiación desbordan los del tensor de Ricci, y como resultado, el volumen de la estrella se expande hasta alcanzar una nueva situación de equilibrio. 2) Cuando la estrella agota su combustible. Se produce entonces un descenso en la presión del fluido, y la estrella, bien se transforma en una enana blanca, en una estrella de neutrones, o bien colapsa definitivamente convirtiéndose en un agujero negro.

Einstein tuvo pronto que modificar ligeramente sus ecuaciones de universo, pues estas no eran compatibles con la ley de la conservación de la energía [Demostración 1]. Esto constriñó a Einstein a modificar sus ecuaciones de Universo, que adquirieron su forma definitiva tras la publicación en 1915 del artículo "Aplicación de la teoría de la relatividad general al campo gravitatorio":

Donde formula_65 es el tensor de Ricci, formula_70 el tensor métrico, formula_71 el escalar de Ricci, formula_72 la constante de gravitación universal y formula_64 el tensor de energía-impulso. El miembro izquierdo de la ecuación recibe el nombre genérico de tensor de Einstein, se representa con la notación formula_74 y satisface las mismas relaciones de conservación que el tensor de tensión-energía:

Teniendo en cuenta que el escalar de curvatura formula_75 es proporcional a la traza del tensor de Einstein formula_76, las ecuaciones de universo de Einstein pueden reformularse de la manera siguiente:

En un fluido no relativista, como una nebulosa o una estrella de la secuencia principal, todos los componentes del tensor de energía-impulso son nulos o de muy poca importancia, salvo el elemento formula_77, que corresponde a la densidad de masa y que es el único que contribuye sensiblemente a la atracción gravitatoria y a la curvatura del espacio-tiempo. Si deseamos medir la contracción de volumen producida por la masa-energía presente en una determinada región, hemos de aplicar las ecuaciones de universo de Einstein:

Computemos ahora los valores de formula_78:

Tras ello obtenemos:

O bien:

Donde formula_80 es la presión del fluido, que en general es muy pequeña comparada con formula_81, por lo que tenemos es una ligera corrección de la anteriormente citada fórmula newtoniana. Como vemos, la atracción gravitatoria viene determinada nosolo por la masa-energía sino también por la presión, aunque la contribución de ésta es formula_82 inferior a la de la primera. Por eso, en las regiones del espacio-tiempo sometidas a bajas presiones y temperaturas, como las nebulosas o nuestro Sistema Solar, la masa es prácticamente la única fuente de atracción gravitatoria y por ello las ecuaciones de la gravitación universal newtonianas constituyen una muy buena aproximación de la realidad física. En cambio, en fluidos sometidos a altas presiones, como las estrellas que se colapsan, la materia que se precipita en los agujeros negros o los chorros que son expelidos de los centros de las galaxias; en todos ellos la presión puede tener cierta importancia a la hora de computar la atracción gravitatoria y la curvatura del espacio-tiempo.

En un fluido electromagnético, la traza del tensor de energía-impulso es nula. Como consecuencia de ello, las ecuaciones de universo de Einstein toman la siguiente forma.

Como vemos, los valores del tensor de Ricci son justo el doble de los calculados para las soluciones de polvo. Esto es lo que explica que la deflexión de los rayos de la luz sea dos veces superior en el ámbito relativista que en el newtoniano, y que la expansión de un universo cíclico de Tolman (dominado por la radiación) sea más lenta que la de un universo cíclico de Friedman (dominado por la materia).

Es importante notar que, puesto en un espacio-tiempo de cuatro dimensiones, el tensor pleno de curvatura contiene más información que la curvatura de Ricci. Eso significa que las ecuaciones del campo anterior, con Λ = 0, no especifican completamente el tensor de curvatura sino una parte del mismo, el tensor de Ricci. La parte de la curvatura no especificada por las ecuaciones de Einstein, coincide precisamente con el tensor de Weyl. Eso significa que las ecuaciones de Einstein no especifican por completo el tensor de curvatura, ni la forma global del universo.

Desde el principio Einstein apreció que matemáticamente el miembro derecho de su ecuación de campo podía incluir un término proporcional al tensor métrico sin que se violara el principio de conservación de la energía. Aunque inicialmente no incluyó dicho término, ya que no parecía tener una interpretación física razonable, más tarde lo incluyó. Esto se debió a que en sus primeros intentos de encontrar soluciones exactas a las ecuaciones de campo consideró que lo que hoy conocemos como modelo estacionario de Einstein. Einstein apreció que esa solución, explicaba adecuadamente los datos disponibles en su tiempo, y correspondía a un universo estático similar a los datos observados. Sin embargo, dicha solución era inestable matemáticamente lo cual no parecía corresponderse con la estabilidad física observable, y se dio cuenta de que con el término proporcional a la métrica la solución podía ser similar pero esta vez estable.

Por esa razón Einstein introdujo en sus ecuaciones un término proporcional al tensor métrico. Siendo la constante de proporcionalidad precisamente la constante cosmológica. El trabajo de varios científicos (FLRW): Alexander Friedman, Georges Lemaître, Howard Percy Robertson y Arthur Geoffrey Walker, probó que existían soluciones estables no estacionarios sin el término proporcional a la constante cosmológica. Y aunque Einstein inicialmente había rechazado el trabajo de Friedman por describir un universo en expansión que no parecía ser descriptivamente adecuado a un universo que él creía estacionario, los datos del corrimiento al rojo del astrónomo Edwin Hubblesolo parecían explicables mediante un modelo de universo en expansión. Esto convenció a Einstein de que la solución FLRW era de hecho correcta y descriptivamente adecuada y por tanto la constante cosmológica innecesaria.

Recientemente la evidencia de la aceleración de la expansión del Universo han llevado a reintroducir la constante cosmológica diferente de cero como una de las posibles explicaciones del fenómeno.

Matemáticamente las ecuaciones de campo de Einstein son complicadas porque constituyen un sistema de 10 ecuaciones diferenciales no lineales independientes. La complejidad de dicho sistema de ecuaciones y las dificultades asociadas para plantear el problema como un problema de valor inicial bien definido, hicieron que durante mucho tiempo solo se contara con un puñado de soluciones exactas caracterizadas por un alto grado de simetría. En la actualidad se conocen algunos centenares de soluciones exactas de las ecuaciones de Einstein.

Históricamente la primera solución importante fue obtenida por Karl Schwarzschild en 1915, esta solución conocida posteriormente como métrica de Schwarzschild, representa el campo creado por un astro estático y con simetría esférica. Dicha solución constituye una muy buena aproximación al campo gravitatorio dentro del sistema solar, lo cual permitió someter a confirmación experimental la teoría general de la relatividad explicándose hechos previamente no explicados como el avance del perihelio de Mercurio y prediciendo nuevos hechos más tarde observados como la deflexión de los rayos de luz de un campo gravitatorio. Además las peculiaridades de esta solución condujeron al descubrimiento teórico de la posibilidad de los agujeros negros, y se abrió todo una nueva área de la cosmología relacionada con ellos. Lamentablemente el estudio del colapso gravitatorio y los agujeros negros condujo a la predicción de las singularidades espaciotemporales, deficiencia que revela que la teoría de la relatividad general es incompleta.

Algunas otras soluciones físicamente interesantes de las ecuaciones de Einstein son:

Por otra parte, el espacio-tiempo empleado en la teoría especial de la relatividad, llamado espacio de Minkowski es en sí mismo una solución de las ecuaciones de Einstein, que representa un espacio-tiempo vacío totalmente de materia.

Fuera de las soluciones exactas y a efectos comparativos con la teoría de campo gravitatorio también es interesante la aproximación para campos gravitatorios débiles y las soluciones en forma de ondas gravitatorias.

Cuando Einstein formuló en 1915 las ecuaciones de universo de la Relatividad general, el científico alemán pensó, en un principio, que dichas ecuaciones eran irresolubles debido a su carácter no lineal, que se manifestaba tanto desde un punto de vista físico como desde otro matemático:



Para sorpresa de Albert Einstein, pocas semanas después de la publicación de sus ecuaciones de campo llegó a su despacho un correo de Karl Schwarzschild, un profesor universitario que en esos momentos se encontraba en el frente de la I guerra mundial, realizando trabajos de balística para las unidades de artillería del ejército alemán. En esa histórica carta se contenían las primeras soluciones exactas de las ecuaciones de la relatividad general, que serían conocidas por la posteridad con el nombre genérico de Solución de Schwarzschild.

El principio sobre el que pivotaba dicha solución era el siguiente: Dado que el Principio de la Covariancia General permitía hacer funcionar las ecuaciones de campo de la relatividad general en cualquier sistema de coordenadas, Schwarzschild procedió a calcular los valores de los tensores de energía-momento y de Einstein en coordenadas espacio-temporales esféricas formula_89. El alto grado de simetría proporcionado por dicho sistema de coordenadas, así como el carácter estático de la métrica, permitieron integrar directamente el conjunto de ecuaciones diferenciales. Siendo en el caso general el tensor métrico para un problema con simetría esférica de la forma:

Para el espacio la parte exterior de un astro esférica más concretamente se tenía:

Las comprobaciones experimentales mostraron que la métrica de Schwarzschild describe con enorme precisión lo que sucede en sistemas esféricos estáticos, similares al sistema solar.

 Las ecuaciones de un campo con simetría esférica permiten también estudiar la curvatura en el interior de las estrellas masivas. El resultado de ese análisis, es que para estrellas de la secuencia principal del diagrama de Hertzsprung-Russell, la curvatura originada por la gravedad es compensada por la presión de la materia estelar. Esa compensación conduce a una ley de equilibrio hidrostático que hace que la estrella, aún sometida a su propio campo gravitatorio, pueda mantener durante millones de años su volumen y su densidad a niveles constantes. Matemáticamente, el hecho de que la métrica tenga un carácter estático implica los valores del tensor formula_87 se mantengan estables en el tiempo. La ley de equilibrio hidrostático que relaciona la densidad y la presión en una estrella esférica viene dada por la ecuación de Tolman-Oppenheimer-Volkoff:

Donde:

La solución de Schwarzschild permitió aplicar los postulados de la relatividad general a disciplinas como la mecánica celeste y la astrofísica, lo cual supuso una verdadera revolución en el estudio de la cosmología: Apenas seis años después de la publicación de los trabajos de Einstein, el físico ruso Aleksander Fridman introdujo el concepto de singularidad espacio-temporal, definido como un punto del espacio-tiempo en el que confluyen todas las geodésicas de las partículas que habían atravesado el horizonte de sucesos de un agujero negro. En condiciones normales, la curvatura producida por la masa de los cuerpos y las partículas es compensada por la temperatura o la presión del fluido y por fuerzas de tipo electromagnético, cuyo estudio es objeto de la física de fluidos y del estado sólido. Sin embargo, cuando la materia alcanza cierta densidad, la presión de las moléculas no es capaz de compensar la intensa atracción gravitatoria. La curvatura del espacio-tiempo y la contracción del fluido aumentan cada vez a mayor velocidad: el final lógico de este proceso es el surgimiento de una singularidad, un punto del espacio-tiempo donde la curvatura y la densidad de tetramomentum son infinitas.

Ahora bien, el físico Subrahmanyan Chandrasekhar fue el primero en darse cuenta que la gravedad podía ser contenida nosolo por fuerzas de tipo mecánico, sino también por un fenómeno de origen cuántico al que llamó "presión de degeneración", derivado del principio de exclusión de Pauli y que era capaz de sostener a estrellas cuya masa no superase el "límite de Chandrasekhar". Estas ideas tan audaces le costaron caras a su autor, que fue ridiculizado en público por Sir Arthur Eddington durante un congreso de astrónomos. Sin embargo, los cálculos de Chandrasekhar se revelaron certeros, y sirvieron de base para la comprensión de un tipo estelar cuya naturaleza física hasta entonces era desconocida: la enana blanca.

Dado que para muchos sistemas físicos no resulta sencillo obtener las expresiones exactas de las soluciones de las ecuaciones de Einstein, los físicos teóricos han desarrollado aproximaciones bastante precisas empleando series de potencias. De entre ellas las más importantes funcionan en coordenadas armónicas y reciben los nombres de "aproximación posnewtoniana" y "aproximación para campos gravitatorios débiles".

En virtud del principio de la covariancia general, ya examinado en secciones anteriores, es posible hacer funcionar a las ecuaciones de universo de Einstein en cualquier tipo de coordenadas, incluidas las armónicas, que son aquellas en las que se cumple la relación formula_93 (como, por ejemplo, en el caso de las coordenadas cartesianas). Se hace necesario en este punto distinguir con claridad entre los conceptos de "planitud" del espacio-tiempo y "armonicidad" de un sistema de coordenadas: en una espacio-tiempo de curvatura nula, como el espacio-tiempo de Minkowski, es posible utilizar coordenadas no-armónicas como las esféricas o las cilíndricas, sin que ello implique que el espacio se curve, ya que la curvatura es una cualidad instrínseca de cualquier variedad e independiente de nuestro sistema de referencia.

Para campos gravitatorios poco intensos, como los existentes en el espacio interestelar, es recomendable utilizar la llamada "aproximación para campos débiles", que es, como veremos, muy similar en su estructura a la fórmula de Poisson newtoniana, si bien las diferencias con esta última son enormes.

La fórmula de Poisson afirma que el laplaciano del potencial gravitatorio formula_54 es igual formula_95:
Esta fórmula plantea un grave inconveniente, y es que presupone el principio de acción a distancia: No tiene en cuenta el retardo en la medición del campo gravitatorio realizada por un determinado observador (pongamos, un observador en la tierra) situado a cierta distancia a la masa del cuerpo que genera dicho campo gravitatorio (p. ej. el Sol, situado a 8 minutos luz de nuestro planeta).

De ahí que uno de los primeros intentos de compatibilizar la teoría de la Relatividad Especial y la Gravitación Universal consistiera en sustituir el laplaciano de la fórmula de Poisson por un d'Alembertiano, una de cuyas soluciones es, precisamente, un potencial retardado:

Como vemos, el potencial gravitatorio medido por el observador en el tiempo "t", es proporcional a la densidad de masa que tiene el cuerpo estelar observado en el tiempo "t - r/c", donde "c" es la velocidad de la luz, "r" es la distancia entre el observador y el objeto y "r/c" es el "retardo", es decir, el tiempo que la luz tarda en desplazarse desde la estrella en cuestión hasta el observador.

Ahora bien, la relatividad general es una teoría métrica de la gravedad, y explica los fenómenos gravitatorios en términos de perturbaciones de la métrica. Es conveniente, por tanto, introducir en nuestra ecuación el pseudotensor formula_97, que representa la desviación de los coeficientes del tensor métrico respecto a la métrica de Minkowski formula_98. Aplicando el límite newtoniano, en cuya virtud formula_86 es igual a formula_100, obtenemos el resultado siguiente:

A grandes rasgos, la sustitución del laplaciano formula_104 por el d'alembertiano formula_105 viene exigida por la obligada eliminación del principio de acción a distancia; el empleo del pseudotensor formula_97 en lugar del potencial formula_5 como elemento definitorio del campo gravitatorio es una consecuencia de la del carácter métrico de la teoría de la relatividad general; y finalmente, la eliminación, en el lado derecho de la ecuación, del parámetro formula_108 y su sustitución por la expresión tensorial formula_109 viene exigida por el principio de la covariancia general.
Sin embargo, en el análisis de la evolución de sistemas astronómicos como el solar o el formado por estrellas dobles o tripoles, la aproximación para campos débiles no es útil, ya que el uso de esta última se restringe a zonas del espacio-tiempo con poca densidad de tetramomentum. En estos casos es preferida la "aproximación posnewtoniana" que como su propio nombre indica prescinde del empleo de la compleja notación del cálculo tensorial y describe el movimiento de los cuerpos celestes utilizando los conceptos matemáticos que empleó el propio Newton a la hora describir las leyes de la mecánica y de la gravitación universal (vectores, gradientes, etc.).

En los siglos XVIII y XIX, astrónomos como Laplace y Le Verrier habían aplicado los postulados de la mecánica newtoniana al estudio de la evolución del Sistema Solar, obteniendo unos resultados muy fructuosos: La precisión de los cálculos astronómicos obtenidos había permitido incluso prever la existencia de un planeta hasta entonces nunca observado por los astrónomos, Neptuno. Por este motivo no es de extrañar que cuando la relatividad general obtuvo pleno reconocimiento, se desarrollase por parte de los astrofísicos una aproximación que siguiera en su estructura el modelo newtoniano y que fuese fácilmente aplicable tanto por los astrónomos como por los ordenadores.

De acuerdo con la teoría clásica de la gravitación, la aceleración de un cuerpo en caída libre es el gradiente negativo del potencial gravitatorio:

Como ya se ha avanzado en secciones anteriores, esta fórmula presupone la asunción del principio newtoniano de acción a distancia, contrario a los postulados de la Relatividad Especial, y además no tiene en cuenta los efectos gravitatorios generados por la energía y por el momentum. La "aproximación posnewtoniana" soslaya estos inconvenientes introduciendo otros dos nuevos potenciales: el potencial formula_111, que constituye una aproximación en segundo grado del potencial formula_112 y el potencial formula_113, derivado de la presencia de momentum en el fluido.

Las ecuaciones de movimiento quedarían reformuladas de la siguiente forma:

Existen un cierto número de soluciones exactas de las ecuaciones que describen un universo completo y por tanto pueden ser consideradas modelos cosmológicos entre ellas destacan:

Se considera que la teoría de la relatividad general fue comprobada por primera vez en la observación de un eclipse total de Sol en 1919, realizada por Sir Arthur Eddington, en la que se ponía de manifiesto que la luz proveniente de estrellas lejanas se curvaba al pasar cerca del campo gravitatorio solar, alterando la posición aparente de las estrellas cercanas al disco del Sol. Desde entonces muchos otros experimentos y aplicaciones han demostrado las predicciones de la relatividad general. Entre algunas de las predicciones se encuentran:


Esto implica el comportamiento del espacio-tiempo alrededor de un objeto masivo rotante.



La teoría de la relatividad general ha sido confirmada en numerosas formas desde su aparición. Por ejemplo, la teoría predice que la línea del universo de un rayo de luz se curva en las proximidades de un objeto masivo como el Sol. La primera comprobación empírica de la teoría de la relatividad fue a este respecto. Durante los eclipses de 1919 y 1922 se organizaron expediciones científicas para realizar esas observaciones, entre ellas la expedición de Arthur Eddington. Después se compararon las posiciones aparentes de las estrellas con sus posiciones aparentes algunos meses más tarde, cuando aparecían de noche, lejos del Sol. Einstein predijo un desplazamiento aparente de la posición de 1,745 segundos de arco para una estrella situada justo en el borde del Sol, y desplazamientos cada vez menores de las estrellas más distantes. Se demostró que sus cálculos sobre la curvatura de la luz en presencia de un campo gravitatorio eran exactos. En los últimos años se han llevado a cabo mediciones semejantes de la desviación de ondas de radio procedentes de quásares distantes, utilizando interferómetros de radio. Las medidas arrojaron unos resultados que coincidían con una precisión del 1% con los valores predichos por la relatividad general.

Otra confirmación de la relatividad general está relacionada con el perihelio del planeta Mercurio. Hacía años que se sabía que el perihelio (el punto en que Mercurio se encuentra más próximo al Sol) gira en torno al Sol una vez cada tres millones de años, y ese movimiento no podía explicarse totalmente con las teorías clásicas. En cambio, la teoría de la relatividad sí predice todos los aspectos del movimiento, y las medidas con radar efectuadas recientemente han confirmado la coincidencia de los datos reales con la teoría con una precisión de un 0,5%.

Se han realizado otras muchas comprobaciones de la teoría, y hasta ahora todas parecen confirmarla. Prácticamente con la más reciente prueba del satélite Gravity Probe B, se podría considerar a la teoría como una ley.

Los relojes en los satélites GPS requieren una sincronización con los situados en tierra para lo que hay que tener en cuenta la teoría general de la relatividad y la teoría especial de la relatividad. Si no se tuviese en cuenta el efecto que sobre el tiempo tiene la velocidad del satélite y su gravedad respecto a un observador en tierra, se produciría un adelanto de 38 microsegundos por día en el reloj del satélite (sin corrección, su reloj retrasaría al día 7 microsegundos como consecuencia de la velocidad y adelantaría 45 microsegundos por efecto de la gravedad), que a su vez provocarían errores de varios kilómetros en la determinación de la posición. Puede considerarse otra comprobación de ambas teorías.

En esta parte, la mecánica clásica y la relatividad especial están entrelazadas debido a que la relatividad general en muchos modos es intermediaria entre la relatividad especial y la mecánica cuántica.

Sujeto al principio de acoplamiento mínimo, las ecuaciones físicas de la relatividad especial pueden ser convertidas a su equivalente de la relatividad general al reemplazar la métrica de Minkowski ("η") con la relevante métrica del espacio-tiempo ("g") y reemplazando cualquier derivada normal con derivadas covariantes.

Tanto en mecánica cuántica como en relatividad se asumía que el espacio, y más tarde el espacio-tiempo, eran planos. En el lenguaje de cálculo tensorial, esto significaba que "R = 0", donde "R" es el tensor de curvatura de Riemann. Adicionalmente, se asumía que el sistema de coordenadas era un sistema de coordenadas cartesianas. Estas restricciones le permitían al movimiento inercial ser descrito matemáticamente como: 

formula_117 donde

Hay que notar que en la mecánica clásica, "x" es tridimensional y "τ ≡ t", donde "t" es una coordenada de tiempo. 

En la relatividad general, si estas restricciones son usadas en la forma de espacio-tiempo y en el sistema de coordenadas, éstas se perderán. Ésta fue la principal razón por la cual se necesitó una definición diferente de movimiento inercial. En relatividad especial, el movimiento inercial ocurre en el espacio de Minkowski como parametrizada por el tiempo propio. Esto se generaliza a espacios curvos matemáticamente mediante la ecuación de las geodésicas:

formula_119 donde

Como "x" es un tensor de rango uno, estas ecuaciones son cuatro y cada una está describiendo la segunda derivada de una coordenada con respecto al tiempo propio. (En la métrica de Minkowski de la relatividad especial, los valores de conexión son todos ceros. Esto es lo que convierte a las ecuaciones geodésicas de la relatividad general en formula_121 para el espacio plano de la relatividad especial).

En gravitación, la relación entre la teoría de la gravedad de Newton y la relatividad general son gobernadas por el principio de correspondencia: la relatividad general tiene que producir los mismos resultados, así como la gravedad lo hace en los casos donde la física newtoniana ha demostrado ser certera.

Alrededor de objetos simétricamente esféricos, la teoría de la gravedad de Newton predice que los otros objetos serán acelerados hacia el centro por la ley:
</math>
Donde: M: masa que genera el Campo gravitatorio, y m es la masa del cuerpo que es atraído.

En la aproximación de campo débil de la relatividad general tiene que existir una aceleración en coordenadas idénticas. En la solución de Schwarzschild, la misma aceleración de la fuerza de gravedad es obtenida cuando la constante de integración es igual a 2"m" (donde "m" = "GM"/"c").

El electromagnetismo planteó un obstáculo fundamental para la mecánica clásica, debido a que las ecuaciones de Maxwell no son invariantes según la relatividad galileana. Esto creaba un dilema que fue resuelto por el advenimiento de la relatividad especial. 
En forma tensorial, las ecuaciones de Maxwell son:

Donde:
El efecto de un campo electromagnético en un objeto cargado de masa "m" es entonces:

Donde
En la relatividad general, las ecuaciones de Maxwell se convierten en 

formula_127 , y
formula_128.

La ecuación para el efecto del campo electromagnético sigue siendo la misma, aunque el cambio de métrica modificará sus resultados. Nótese que al integrar esta ecuación para cargas aceleradas las hipótesis habituales no son válidas (ya que implican que una carga sujeta en un campo gravitato debe comportarse como si estuviera uniformemente acelerada, lo que muestra que una carga uniformemente acelerada no puede radiar).

En la mecánica clásica, la conservación de la energía y el momentum son manejados separadamente. En la relatividad especial, la energía y el momentum están unidos en el cuadrimomento y los tensores de energía. Para cualquier interacción física, el tensor de energía-impulso formula_129 satisface la ley local de conservación siguiente:

En la relatividad general, esta relación es modificada para justificar la curvatura, convirtiéndose en:

donde ∇ representa aquí la derivada covariante.

A diferencia de la mecánica clásica y la relatividad especial, en la relatividad general no es siempre posible definir claramente la energía total y el momentum. Esto a menudo causa confusión en espacio-tiempos dependientes del tiempo, en los que no existen temporales, los cuales no parecen conservar energía, aunque la ley local siempre se satisfaga (Ver energía de Arnowitt, Deser y Misner).

La teoría de la relatividad especial presenta covariancia de Lorentz esto significa que tal como fue formulada las leyes de la física se escriben del mismo modo para dos observadores que sean inerciales. Einstein estimó, inspirado por el principio de equivalencia que era necesaria una teoría que presentara una para la que valiera un principio de covariancia generalizado, es decir, en que las leyes de la física se escribieran de la misma forma para todos los posibles observadores fueron estos inerciales o no, eso le llevó a buscar una teoría general de la relatividad. Además el hecho de que la propia teoría de la relatividad fuera incompatible con el principio de acción a distancia le hizo comprender que necesitaba además que esta teoría general incorporase una descripción adecuada del campo gravitatorio.

Hoy sabemos que Einstein consideraba que la teoría de la relatividadsolo era aplicable a sistemas de referencia inerciales estrictamente, aunque Logunov ha probado en el marco de la teoría relativista de la gravitación que de hecho fijado un observador inercial o no, cualquier otro que se mueva con velocidad uniforme respecto al primero escribirá las leyes físicas de la misma forma. Probando así que la relatividad especial de hecho es más general de lo que Einstein creyó en su momento. Además el trabajo de Logunov prueba que siempre que el espacio-tiempo sea plano puede establecerse para cada observador existe un grupo decaparamétrico de transformaciones de coordenadas que generaliza las propiedades del grupo de Lorentz para observadores no inerciales.

El principio de geometrización y el principio de equivalencia fueron las piedras angulares en las que Einstein basó su búsqueda de una nueva teoría, tras haber fracasado en el intento de formular una teoría relativista de la gravitación a partir de un potencial gravitatorio. La teoría escalar de la gravitación de Nordström y la interpretación geométrica que extrajo de ella Adriaan Fokker (1914), el estudiante de doctorado de Hendrik Lorentz, llevaron a Einstein a poder relacionar el tensor de energía-impulso con la curvatura escalar de Ricci de un espacio-tiempo con métrica:

que involucraba la métrica del espacio-tiempo plano y un campo escalar relacionado con el campo gravitatorio. La superación de las deficiencias de la teoría de la gravitación escalar de Nordström llevaron a Einstein a formular las ecuaciones correctas de campo.





</doc>
<doc id="4599" url="https://es.wikipedia.org/wiki?curid=4599" title="Borís Spaski">
Borís Spaski

Borís Vasílievich Spasski (; Leningrado, 30 de enero de 1937) es un Gran Maestro Internacional de ajedrez ruso nacionalizado francés. Se proclamó décimo campeón del mundo de ajedrez en 1969 al derrotar al también soviético Tigrán Petrosián.

Fue un niño prodigio del ajedrez y su estilo era universal: ganaba a Mijaíl Tal con ataques al rey, a Tigrán Petrosián en profilaxis.

Aunque hizo méritos suficientes para pasar a la historia como un gran jugador, más que por su contribución al desarrollo del ajedrez, es conocido por haber sido el jugador que perdió con el estadounidense Robert James Fischer en el encuentro disputado en Reikiavik (Islandia) en 1972, al cual se denominó «encuentro del siglo». Se celebró en plena guerra fría y fue todo un símbolo del enfrentamiento entre las dos superpotencias. Hasta ese momento, Fischer no había ganado a Spasski, que además preparó el campeonato con Yefim Géller, el cual también había superado a Fischer en el pasado. Pero el estadounidense ganó con contundencia mediante una serie de salidas sorprendentes, que buscaban tirar por tierra la preparación de Spasski. El resultado final fue 12,5 - 8,5 a favor del estadounidense. Tras el encuentro, Spasski no volvió a ser el jugador dominante que había sido.
Pero ganaría el Torneo Internacional de Ajedrez Ciudad de Linares en 1983.
Cayó en desgracia en la Unión Soviética, nacionalizándose francés en 1984.

Posteriormente, participó en varios ciclos clasificatorios para el título mundial. En 1974, fue eliminado en semifinales por Anatoli Kárpov, futuro campeón del mundo, y en 1978 llegó hasta la final, donde fue derrotado por Víktor Korchnói.

En 1992, jugó un encuentro de revancha con Bobby Fischer en Sveti Stefan (en lo que hoy es Montenegro, entonces Yugoslavia), en el que volvió a ser derrotado. En esta última partida, el nivel de los contendientes distaba mucho del que los encumbró en el mundo del ajedrez; pero creó una gran expectación, ya que suponía el regreso de Fischer a los tableros después de veinte años de ausencia. Fue organizado por un empresario yugoslavo con una bolsa de 5 millones de dólares (3,35 millones para el ganador y 1,65 millones para el perdedor).



</doc>
<doc id="4600" url="https://es.wikipedia.org/wiki?curid=4600" title="Hidrodinámica">
Hidrodinámica

La hidrodinámica es la rama de la hidráulica que estudia la dinámica de los fluidos.

Para el estudio de la hidrodinámica se pueden considerar diferentes aproximaciones, dependiendo del problema que se vaya a abordar, como por ejemplo las siguientes: 

La hidrodinámica tiene aplicaciones en múltiples escalas que van desde la escala nanoscópica a la macroscópica. 

Daniel Bernoulli fue uno de los primeros matemáticos que realizó estudios de hidrodinámica, siendo precisamente él quien dio nombre a esta rama de la física con su obra de 1738, "Hydrodynamica".

La hidrodinámica o fluidos en movimientos presenta varias características que pueden ser descritas por ecuaciones matemáticas muy sencillas como:


donde formula_1 es la densidad, formula_2 la velocidad, formula_3 es el diámetro del cilindro y formula_4 es la viscosidad dinámica. Concretamente, este número indica si el fluido es laminar o turbulento, o si está en la zona de transición. formula_5 indica laminar, formula_6 turbulencia.

El caudal o gasto es una de las magnitudes principales en el estudio de la hidrodinámica. Se define como el volumen de líquido formula_7 que fluye por unidad de tiempo formula_8. Sus unidades en el Sistema Internacional son los m/s y su expresión matemática:

Esta fórmula nos permite saber la cantidad de líquido que pasa por un conducto en cierto intervalo de tiempo o determinar el tiempo que tardará en pasar cierta cantidad de líquido.

El principio de Bernoulli es una consecuencia de la conservación de la energía en los líquidos en movimiento. Establece que en un líquido incompresible y no viscoso, la suma de la presión hidrostática, la energía cinética por unidad de volumen y la energía potencial gravitatoria por unidad de volumen, es constante a lo largo de todo el circuito. Es decir, que dicha magnitud toma el mismo valor en cualquier par de puntos del circuito. Su expresión matemática es:

donde formula_9 es la presión hidrostática, formula_1 la densidad, formula_11 la aceleración de la gravedad, formula_12 la altura del punto y formula_13 la velocidad del fluido en ese punto. Los subíndices 1 y 2 se refieren a los dos puntos del circuito.

La otra ecuación que cumplen los fluidos no compresibles es la ecuación de continuidad, que establece que el caudal es constante a lo largo con todo el circuito hidráulico:

donde formula_14 es el área de la sección del conducto por donde circula el fluido y formula_13 su velocidad media.

En el caso de fluidos compresibles, donde la ecuación de Bernoulli no es válida, es necesario utilizar la formulación más completa de Navier y Stokes. Estas ecuaciones son la expresión matemática de la conservación de masa y de cantidad de movimiento. Para fluidos compresibles pero no viscosos, también llamados fluidos coloidales, se reducen a las ecuaciones de Euler.



</doc>
<doc id="4602" url="https://es.wikipedia.org/wiki?curid=4602" title="Pinyin">
Pinyin

El hànyǔ pīnyīn (), o deletreo Han, normalmente llamado pinyin, es un sistema de transcripción fonética del chino mandarín ("hànyǔ") y está reconocido oficialmente en la República Popular China. Cambia el uso de los caracteres tradicionales chinos de conceptual a fonético. Esto es, se usa la escritura latina para transcribir fonéticamente las 415 sílabas del idioma chino. Aunque es una ayuda para el aprendizaje, según sus críticos, obvia aspectos inherentes del mismo, que pueden distorsionar su comprensión. No hay duda de que este sistema ayuda a quienes no conocen la escritura china a pronunciar las palabras chinas y también se utiliza para introducir caracteres chinos en teclados QWERTY a través de programas específicos para ello (véase métodos informáticos para escritura china).

En 1979, la ISO adoptó el pinyin como el sistema de romanización estándar del chino.

El "hànyǔ pīnyīn" se ha consolidado como el principal sistema de transcripción del chino en la época actual, pero no es sino uno más de muchos sistemas existentes (véase sistemas de transcripción del chino).

El sistema pinyin posee un complejo sistema de diacríticos para marcar los tonos, que se describe más abajo. Para su correcta escritura no se debe omitir ningún signo. También hay reglas que determinan si dos o más palabras deben ser escritas juntas o separadas.

El pinyin fue creado por el intelectual chino Zhou Youguang, llamado «el padre del Pinyin», como parte de un programa iniciado en la década de 1950 por el gobierno de China para convertir el mandarín en la lengua nacional de China, simplificar los caracteres e idear un nuevo alfabeto fonético. Zhou estaba trabajando en un banco de Nueva York cuando volvió a China para ayudar a reconstruir el país después de la victoria del Partido Comunista de China (PCR) y su acceso al poder en 1949. Zhou fue profesor de economía en Shang'hái. En el año 1954 el Ministro de Educación de China creó el Comité para la Reforma de la lengua escrita china. Zhou trabajó en el desarrollo de un nuevo sistema caligráfico romanizado que permitió erradicar el analfabetismo en China; durante la Revolución Cultural, el pinyin fue abandonado y Zhou pasó dos años en un campo de labores forzosas. Zhou señaló años después que no era el padre del pinyin. «Soy el hijo del pinyin, que es el resultado de una larga tradición que se inicia en los últimos años de la dinastía Qing. Hemos estudiado el trabajo hecho y lo hemos mejorado».

La utilidad principal del pinyin es la transcripción del chino al alfabeto latino. Al mismo tiempo trata de ser un sistema de escritura fonémico (una letra, o un dígrafo, por fonema) del chino mandarín.

Debido a que el pinyin fue diseñado sobre todo para los angloparlantes, suele darse que "la pronunciación de una letra en pinyin no se corresponde con su sonido en español." El pinyin es de más fácil uso a los anglohablantes que a quienes hablamos español, a causa de sus orígenes en la lingüística inglesa.

Taiwán ha estado en proceso de adoptar el pinyin. En sus escuelas primarias se ha utilizado el sistema zhuyin, y no hay un sistema de romanización oficial, a pesar de los múltiples esfuerzos. A finales de los noventa, el gobierno de Taiwán decidió reemplazar el sistema zhuyin por el sistema pinyin. Esto ha originado una discusión de qué sistema utilizar, si el "hànyǔ pīnyīn" o el "tōngyòng pīnyīn".

Esta controversia es paralela con las tensiones políticas entre los partidarios de la independencia de Taiwán (que apoyan el uso del sistema "tōngyòng pīnyīn") y los de la reunificación con China o mantenimiento del "statu quo" (partidarios del "hànyǔ pīnyīn", el sistema utilizado en China continental).

En octubre de 2002, el gobierno de Taiwán eligió el sistema "tōngyòng pīnyīn" como oficial. Sin embargo, los gobiernos locales tienen derecho a elegir si aplican o no esta orden administrativa en su territorio, y las localidades bajo el mando del Kuomintang han elegido utilizar el sistema "hànyǔ pīnyīn".

El objetivo principal del pinyin en las escuelas chinas es enseñar la pronunciación del mandarín (lengua oficial de China) a hablantes de otras lenguas chinas. En Occidente hay quien cree que el pinyin se utiliza para que los niños asocien los caracteres chinos con las palabras que ya saben decir. Esto es falso. No todos los chinos hablan mandarín como lengua materna, ni remotamente. Algunos chinos aprenden su pronunciación en la escuela, con ayuda del "pinyin".

Cada carácter chino habitualmente representa una sílaba. Por ejemplo "Yo soy mexicano" se escribe con seis caracteres, es decir con seis sílabas (Wo3 shi4 mo4 xi1 ge1 ren2):

La sílaba en mandarín tiene dos partes, una inicial (en azul en el ejemplo) y una final (en rojo). El verde indica el tono (el mandarín tiene cuatro tonos y un tono neutro, que no se escribe).

El primer cuadro de abajo muestra las partes iniciales y el segundo cómo se leen las vocales y las lecturas especiales de algunas partes finales:

El chino mandarín es un idioma tonal. Los tonos se marcan mediante el uso de acentos gráficos sobre una vocal no medial.


Como muchos tipos de letra utilizados en un ordenador carecen de acentos como el macrón o el carón, una convención común es indicar el número correspondiente al tono justo después de cada sílaba (por ejemplo, "tóng" (tong con el tono ascendente) se escribiría "tong"). El dígito se numera en el orden que se indica arriba, con una excepción: el «quinto tono», además de tener el número 5, puede no indicarse o indicarse con un 0, como en la partícula interrogativa "ma0" (吗/嗎). 

Las vocales pinyin se ordenan en el siguiente orden: "a, o, e, i, u, ü". En general, la marca tonal se coloca en la vocal que aparece antes en el orden indicado. "Liú" es una excepción superficial cuya auténtica pronunciación es "lióu", y como la "o" precede a la "i", se marca la "óu" (que se contrae a "ú").




</doc>
<doc id="4607" url="https://es.wikipedia.org/wiki?curid=4607" title="Lenguas quechuas">
Lenguas quechuas

El quechua o quichua es una familia de idiomas originarios de los Andes peruanos que se extiende por la zona occidental de América del Sur a través de siete países. Para el año 2004 la cantidad de hablantes de lenguas quechuas se estimaba entre ocho y diez millones. Según datos estadísticos del censo de 2018, en el Perú la población de quechuahablantes ha aumentado, en comparación al año 2007.

Esta familia lingüística se habría originado en un territorio que correspondería con la región central y occidental de lo que actualmente es Perú. En el siglo, se separaron dos ramas de la familia; el quechua I hacia el norte y el quechua II hacia el sur. Hacia el siglo, la llamada "lengua general" se convirtió en una importante lengua vehicular y oficial por el Estado incaico. Esta variante fue la lengua más importante empleada para la catequesis de los indígenas durante la administración española. En el siglo, el castellano sobrepasó al quechua como lengua mayoritaria en el Perú. El quechua sureño, descendiente de la lengua general colonial, es la lengua quechua más extendida, seguido del quichua norteño (de Ecuador, Colombia y Loreto) y del quechua ancashino. En la década de 1960, estudios dialectológicos determinaron la existencia de lenguas separadas dentro del quechua.

Las lenguas quechuas tienen una morfología aglutinante, con raíces regulares y repertorios amplios de sufijos productivos, que permiten formar palabras nuevas de forma regular. Entre sus rasgos gramaticales, se distinguen la fuente de la información o evidencialidad, varios casos nominales, un "nosotros" inclusivo y otro excluyente, el beneficio o la actitud del hablante al respecto de una acción, y opcionalmente el tópico. Los verbos transitivos concuerdan con el sujeto y el objeto. Expresan predicaciones nominales yuxtaponiendo el sujeto y el atributo. A diferencia del español, el quechua funciona sin artículos o conjunciones y sin distinguir géneros gramaticales. 

Hay expresiones como: "Urqu mishi" (gato macho); "china mishi" (gato hembra); "china mulli" (molle hembra); "urqu mulli" (molle macho). "Urqun qucha" (laguna macho); "chinan qucha" (laguna hembra): para distinguir funcionalidades duales; no hay categoría similar a la de la gramática de las lenguas romances. Se antepone 'urqu', 'china' al nombre de animal o planta o accidente geográfico para indicar el género masculino o femenino que le corresponda. A pesar de que varias de estas características son mayormente conservadas, ciertas lenguas han perdido algunas de ellas en el transcurso del tiempo.

Las lenguas quechuas no tenían autoglotónimos o al menos no existen registros de que así haya podido ser. Por el contrario, es a partir de los estudios y de las crónicas de la época de la Conquista que se les da nombres a las lenguas del mosaico lingüístico que constituía el Virreinato del Perú del siglo XVI. Algunas frases se emplearon para designar a la lengua con la cual los gobernantes del Antiguo Perú se entendían con el Estado incaico: la más temprana registrada es la de "lengua general". Sin embargo, en la región andina no solo el quechua clásico recibió dicho epíteto, sino también más tarde el aimara, el puquina y el mochica.

El nombre de quechua fue empleado por primera vez por fray Domingo de Santo Tomás en su "Grammatica..." así como el origen de la expresión, también citado por Cieza de León y Bernabé Cobo: al ser preguntados los orejones por los cronistas por el origen de la llamada "lengua general", estos respondían ser originaria de la nación quichua, que habitaba en lo que es hoy la Provincia de Andahuaylas. El vocablo variante "quechua" comenzó a emplearse hacia mediados del siglo XVII. Tanto "quichua" como "quechua" provienen de algún cognado de la originaria ('valle templado'), que es empleada para aquellos valles de clima benigno.

En muchas variantes, como en el quechua sureño, este cognado muestra una consonante uvular que, cuando aparece delante de /i/, ya sea oclusiva o fricativa, provoca una alófono [e] en esta vocal. A las regiones que guardan esta alofonía suele corresponder el quechuismo "quechua". En algunas otras, la transformación de la original */q/ en consonantes no uvulares provoca la pérdida de la alofonía en las vocales, por lo que a estas variantes suele corresponder más bien el nombre de "quichua". Sin embargo, hay algunas salvedades, como en Santiago del Estero, donde se usa el nombre "quichua", y algunas zonas donde no se emplea el autónimo.

El autoglotónimo "runa simi" («lengua de gente») está extendido en algunas variedades del quechua sureño. Luego de la Conquista, el término "runa" sufrió una aculturación, ya que se tergiversó su sentido original de «ser humano» y se usó para designar a los nativos en contraposición a "wiraqucha", que se usó para designar a los españoles. Es así que "runa simi" se puede traducir como "lengua de indígenas", es decir, cualquier lengua nativa, para diferenciarlas del español ("kastilla simi"; "misu simi").

Otra interpretación posible es que la expresión "runa" haga referencia a categorías de la administración pública: el runa es el indio tributante, independientemente de si es quechua o no. Una razón potente en favor de esta hipótesis es la existencia de una expresión similar para las lenguas de la familia aimara: El glotónimo "jaqaru" procede de "jaqi" + "aru", con un significado idéntico.

No existen referencias tempranas ni tardías dentro de las crónicas españolas del uso de epíteto similar a "runa šimi" para designar a alguna lengua en particular, sino como referencia simplemente de que la lengua en mención era hablada por los indígenas. Una de las primeras referencias, citada por Cerrón-Palomino (2008), es la del quechuista Middendorf, apenas en 1891.

En ambos dialectos colombianos se le llama "inka shimi" («idioma de los incas») por ser los incas quienes lo llevaron a aquellas latitudes, mientras que en la periferia de Huancayo, el quechua huanca es llamado como "wanka shimi", es decir, "lengua de los huancas", y no se emplea por los vernáculos ni "nuna shimi" ni "qichwa shimi".

Los primeros estudios conocidos de la lingüística quechua se dieron a inicios del Virreinato del Perú. Los misioneros católicos emplearon este y otros idiomas locales para evangelizar a los indígenas, para lo cual se escribieron varios manuales ("artes") y diccionarios ("vocabularios") de estos idiomas, como el aimara, el mochica o el guaraní, así como catecismos.

Fray Domingo de Santo Tomás O.P., fraile dominico quien según su propio testimonio llegó al Perú en 1540, fue el primer misionero que aprendió la lengua de la región central de Perú durante su tarea evangelizadora; predicando luego en su propia lengua a los nativos de los actuales departamentos de La Libertad, Ancash, Lima, Ica, Apurímac, Huancavelica, Ayacucho, Junín y Huánuco. En 1560, como fruto del conocimiento de la lengua de los naturales, publicó en Valladolid las dos primeras obras en quechua, la "Gramática o arte de la lengua general de los indios de los reinos del Perú", y el "Lexicón o vocabulario de la lengua general del Perú".

El diputado limeño Juan de Balboa fue el primer catedrático de lengua quechua (lengua quichua), cuando se organizó la Universidad de San Marcos en 1576, y el primer peruano que en ella se graduó de doctor. Posteriormente, en 1608 Diego González Holguín (1552-1618) publicó el "Vocabvlario de la lengua general de todo el Perv llamada qquichua o del Inca".

En la segunda mitad del siglo XX, se dieron los primeros estudios científicos modernos del quechua. Los lingüistas Alfredo Torero y Gary Parker publicaron los primeros estudios sobre el tema, secundados por Rodolfo Cerrón Palomino, Félix Quesada, Antonio Cusihuamán, Clodoaldo Soto Ruiz, Amancio Chávez, Francisco Carranza, entre muchos otros, y el antropólogo y literato José María Arguedas. Entre los lingüistas extranjeros también se publicaron estudios importantes, como los de Willem Adelaar, Gerald Taylor, César Itier, Wolfgang Wolck, Pieter Muysken y otros más. Sin embargo, es también la época del progresismo en los Andes, donde las lenguas originarias, así como sus costumbres, eran vistas como derroteros del desarrollo de las naciones, por lo cual la incipiente educación rural se dirigió a la directa sustitución de las mismas por el castellano. El trabajo del Instituto de Estudios Peruanos y el impulso de Alberto Escobar y la publicación de sendos diccionarios de seis variedades del quechua y de sus respectivas gramáticas. Al respecto, Escobar dice 

El quechua no presenta vínculos genéticos demostrados con otras familias de lenguas. Anteriormente se vertieron algunas hipótesis que fueron posteriormente descartadas, como la propuesta de las familia amerindia de Joseph Greenberg (1987), que situaba al quechua dentro de la rama Andina del tronco andino-chibcha-paezano.

Aunque la tesis de una relación genética entre el quechua y las lenguas aimaras se halla también descartada, el consenso de los especialistas acepta una antigua relación de mutua influencia entre las protolenguas de estas familias. Parte importante del léxico de estas familias es compartido y se desconoce de cuál de ambas han provenido. De esta forma, tras un largo periodo de contacto, el protoquechua aparece a inicios del I milenio en la parte centro-occidental del Perú. El protoquechua divergió en dos ramas hacia el siglo V: el Quechua I inicia una nueva expansión en dirección norte a través de la vertiente oriental hasta el Callejón de Huailas y el Quechua II se expande en dirección sur por la sierra de la vertiente pacífica.

En el siglo XIII acontecía la expansión más reciente del quechua, impulsada a consecuencia del comercio del reino de Chincha, que produjo la adopción del quechua clásico como lengua franca en gran parte del Antiguo Perú y en lo que modernamente es la sierra ecuatoriana, empleada por los curacas de pueblos diversos para comunicarse entre gobernantes independientes para el intercambio de productos. Este avance condujo a la adopción del quechua en la sierra y la Amazonía ecuatoriales, por un lado, y hacia la sierra sur sobre territorio de habla aimara. Finalmente, la variante ecuatoriana divergió del habla del sur, produciéndose la última escisión de la familia quechua. Sin embargo, en varias regiones eran solo los curacas quienes conocían el quechua, mientras que el pueblo llano continuaba usando sus lenguas propias, como era el caso de la región mochicahablante. En medio de este proceso, cuando los incas iniciaron la conquista del Chinchaysuyo, adoptaron esta lengua para sus asuntos administrativos, si bien ellos también eran aimarahablantes, e impusieron su aprendizaje en las diversas provincias de su imperio, sin que esto significara que dejaran de lado las lenguas vernáculas. Algunos pueblos de la selva que mantuvieron contacto comercial con los incas resultaron también influenciados por el quechua.

Durante el Virreinato del Perú, los misioneros católicos emplearon este y otros idiomas locales para evangelizar a los indígenas; se escribieron varios manuales ("artes") y lexicones de este y otros idiomas importantes, como el aimara, el mochica o el guaraní, así como catecismos. Ello contribuyó a la expansión del quechua en otros pueblos andinos e amazónicos.

Los estudios dialectológicos seminales de los lingüistas Gary Parker (1963) y Alfredo Torero (1964) clasificaron las variedades de la familia lingüística quechua en dos subfamilias o ramas. Una de estas ramas es el llamado "Quechua I" en la nomenclatura de Torero o "Quechua B" según Parker. Esta rama comprende las variedades distribuidas en la sierra central y norcentral del Perú, por ambas vertientes de la cordillera de los Andes, dentro de las jurisdicciones de los departamentos peruanos de Lima, Junín, Pasco, Huánuco y Ancash. La otra rama es la denominada "Quechua II" (Torero) o "Quechua A" (Parker). Se expande por el norte entre el suroeste de Colombia, Ecuador y el norte de Perú, mientras que por el sur se expande entre el Perú austral, Bolivia y el noroeste argentino, con probables hablantes en la región próxima de Chile. Torero articuló en su trabajo una subdivisión tripartita del grupo Quechua I. 


En una reciente revisión, Adelaar recuerda que la posición taxonómica del grupo Quechua II A fue cuestionada por el propio autor y reconsiderada a la luz de posteriores investigaciones en la zona de Yauyos. El quechua de Pacaraos, por consideraciones principalmente morfológicas se considera como una rama del Quechua I, divergente del resto de quechuas centrales, mientras que las variedades restantes del II A inicial de Torero se consideraron como separaciones tempranas del proto-Quechua II, anterior a una probable bifurcación entre Quechua II B y Quechua II C.

En el subgrupo Periférico (II, B, Wampuy), encontramos zonas alta y medianamente definidas de dialectos inteligibles. Caso destacable es la subrama Chinchay austral, donde todas las variantes son inteligibles, caso similar al Chinchay septentrional. Dentro de las Yungay (QIIa), los dialectos de Cañaris y Cajamarca se intercomunican fácilmente; mientras que las otras dos variantes Laraos y Lincha se intercomunican con diferentes variedades de otras ramas, como se verá más adelante.

En la subfamilia Central (I, A, Waywash), el panorama es más complejo: las hablas del sur del departamento de Junín (Jauja y Huanca) son mutuamente inteligibles a pesar de la divergencia, mientras que las hablas al norte de este sector (incluida la de Pacaraos, del QIIa) conforman un enmarañado continuo dialectal, es decir, la intercomprensión de las variantes es relativo a la distancia entre las mismas. Las hablas de las provincias de Yauyos y Chincha (tanto Waywash como Yungay) son inteligibles a pesar de pertenecer a grupos tan distintos.

El lingüista Alfredo Torero, además, propuso una agrupación de las múltiples variedades empleadas en el Perú en siete "supralectos" o lenguas según su inteligibilidad mutua:

El quechua yauyino está compuesto por dialectos de ambas ramas del quechua que son mutuamente inteligibles a pesar de sus divergencias.

El Instituto Lingüístico de Verano ha catalogado la familia como "macrolengua", categoría creada por esta institución para describir aquellos linajes que por razones políticas o sociales son consideradas como si fueran un solo idioma en contra de la evidencia lingüística. Paralelamente, indexa 42 variantes como idiomas individuales, al margen del grado de inteligibilidad mutua.

No existe actualmente una lengua estándar (caso del árabe) o sistema escrito común (como en el chino) que utilicen los usuarios de lenguas ininteligibles para comunicarse: antes recurren al español, si lo conocen.

A nivel oficial, la constitución política del Perú habla del quechua como de un solo idioma; sin embargo, el Ministerio de Educación emite libros distintos para al menos seis variedades lingüísticas (Áncash, Ayacucho, Cajamarca-Cañaris, Cuzco, Junín, San Martín). En Bolivia se utiliza en la educación y en textos oficiales un solo "Quechua Normalizado" (sureño) y en Ecuador un "Kichwa Unificado". Todas las variedades habladas en estos dos países son mutuamente inteligibles.

Divergiendo del conceso de los especialistas, la llamada Academia Mayor de la Lengua Quechua afirma que el quechua es un solo idioma, con el quechua cuzqueño como dialecto estándar y las demás variantes como "deformaciones" de la misma.

Posteriormente a la convergencia formativa de las familias quechua y aimara, el quechua continuó teniendo una intensa relación de contacto lingüístico con la familia lingüística aimara, sobre todo las variedades australes. En muchas regiones el quechua llegó con el tiempo a sustituir al aimara. De hecho, muchas de las características del quechua IIC parecen deberse a que muchas de estas variedades se formaron sobre un substrato aimara.

Además, el quechua ha estado históricamente en contacto con lenguas amazónicas como el asháninka además de otras lenguas de las familias arawak y pano. En la cuenca del Marañón el quechua reemplazó completamente un número importante de lenguas preincaicas. En el sur el imperio incaico se extendió hasta el domino lingüístico del mapudungun, el cacán y las huarpe.

El quechua también influenció en el español, aportando muchos quechuismos para describir las nuevas realidades que conocieron los conquistadores. Análogamente, la lengua castellana ha dejado también préstamos en varias lenguas quechua. Posteriormente, el bilingüismo español-quechua en los Andes ha dado lugar a la incorporación de fonemas oclusivos sonoros en el Quechua II, y por otro lado a la formación del español andino.

Las lenguas quechuas se hablan en un amplio rango geográfico de forma discontinua en la zona occidental de América del Sur, desde el suroeste de Colombia hasta el Norte argentino.

Entre el suroeste de Colombia, Ecuador y el extremo norte de la Amazonía del Perú, predomina el llamado quichua norteño o ecuatoriano. Este conjunto diverso se extiende desde regiones discretas de los departamentos de Nariño, Putumayo y Cauca (Colombia) hasta las vertientes septentrinales del río Amazonas en el departamento de Loreto (Perú), pasando por gran parte de la Sierra y del Oriente ecuatorianos.

Dos variedades relacionadas al quichua norteño se hablan en los departamentos peruanos de Amazonas y San Martín. El quechua chachapoyano se emplea en la montaña amazonense mientras que el quechua lamista se emplea en las vertientes de los ríos Mayo y Sisa. Al oeste, el quechua cajamarquino se extiende por los alrededores de la ciudad de Cajamarca, en localidades como Chetilla y Porcón. La variedad de Incahuasi-Cañaris, inteligible con la variedad cajamarquina, se extiende el noreste por los distritos andinos de Incahuasi y Cañaris (Lambayeque) y cercanías en las provincias de Cutervo y Jaén (Cajamarca), además de un pueblo alejado en la vecina provincia de Huancabamba (Piura)

En la Sierra central del Perú se ubican principalmente lenguas de la rama Quechua I. Estas conforman un continuo dialectal esparcido entre los departamentos de Áncash y Huánuco por el norte, y los de Junín, Huancavelica e Ica por el sur, incluyendo los departamentos de Pasco y Lima.

La lengua más ampliamente hablada de estas regiones es el quechua ancashino, hablado en el extremo norte (Ancash y noroeste de Huánuco). El llamado quechua huanca o simplemente huanca se habla en las provincias de Huancayo, Chupaca y Concepción en el departamento de Junín. Al sur, en el departamento de Lima, dos dialectos Quechua II comparten su ámbito con las variedades de Quechua I de la provincia de Yauyos; una se ubica en el distrito de Laraos y el segundo se halla al sur de la provincia.

El "macrolecto" más extendido de la familia quechua, el quechua sureño, se habla entre el sur del Perú y el norte de Argentina formando tres regiones separadas. La primera incluye la Sierra sur del Perú entre el departamento de Huancavelica y los de Puno y Moquegua, proyectándose en una pequeña región al norte del departamento de La Paz. Esta región está separada de otra más al sur por el dominio lingüístico del aimara, segunda la cual se extiende en el centro y el suroeste de Bolivia por los departamentos de Cochabamba, Chuquisaca y Potosí además de partes limítrofes de otros departamentos, y en el norte argentino en las provincias argentinas de Jujuy, Salta y Tucumán. Por último, el quichua tiene una distribución "semi-aislada" en la región del centro-oeste de la Provincia de Santiago del Estero, que es el llamado "quichua santiagueño".

Las sílabas de las lenguas quechuas se componen como mínimo de una vocal como núcleo. Por regla general, aceptan una consonante en posición de ataque y coda (principio y fin de sílaba, respectivamente); no obstante, los préstamos más recientes pueden aceptar hasta dos consonantes en ataque, especialmente con consonantes líquidas. La entonación y la acentuación tienen roles menores.

Se distinguen tres fonemas vocálicos: una vocal abierta y las cerradas redondeada no redondeada . Además, los quechuas centrales distinguen dos cantidades vocálicas: vocales cortas y largas . La pronunciación precisa de estos fonemas vocálicos varía con su entorno fonético. La vecindad de una consonante uvular produce alófonos más centralizados como , , , , y la de la semiconsonante palatal también provoca un adelantamiento de a . Se produce la monoptongación de grupos como y en el quechua de Chachapoyas, así también en algunas variantes del quechua ancashino, donde también se afecta el grupo .

En cuanto a las consonantes, se presenta una alta diversificación producto de diversos cambios diacrónicos que han afectado este inventario original. El protoquechua habría contado con tres nasales cuatro oclusivas , dos africadas , tres fricativas , dos aproximantes y dos o tres líquidas . La fricativa retrofleja se hizo fricativa postalveolar sorda desde muy antiguo, conservándose solo en el huanca.

El inventario consonántico del protoquechua pasó por importantes reducciones más de una vez en su proceso de desarrollo. La glotal inicial desapareció en el Quechua I y en el quechua de Cajamarca e Incahuasi-Cañaris. Algunos consonantes se fundieron, como la oclusiva uvular con la velar en el QIIB y las sibilantes en el QIIC; ambos grupos además fundieron las africadas en una sola postalveolar, al igual que el quechua del Huallaga, con excepción del quechua de Chachapoyas y el del Pastaza. En esta última, la africada retrofleja, fue adelantada hasta la posición alveolar .

Por otro lado, se registran al menos dos expansiones o adiciones mayores del conjunto de consonantes. Por el contacto prolongado con el castellano, se han incorporado plosivas sonoras como , y , allí donde el quechua originalmente distinguía entre sonoras y sordas, además de la fricativa retrofleja entre los principales préstamos, como en "bindiy" (vender), "Diyus" (Dios) o "karru" (carro). En el quechua sureño, por muy probable influencia del aimara y salvo la variante ayacuchana, se añadieron eyectivas y aspiradas al repertorio fonémico de oclusivas y a la africada.

Un cambio reciente importante en el Quechua I ha afectado la articulación de las africadas. La postalveolar se adelantó hasta una alveolar en gran parte del norte y centro de este continuo dialectal. Posteriormente, algunas áreas adelantaron también la retrofleja a la posición postalveolar dejada por el cambio precedente. Algunas variantes, como el quechua de Cajatambo, pasaron inclusive por una desafricación de la nueva alveolar, coincidiendo con una previa glotalización de la sibilante alveolar .

Largamente se viene debatiendo acerca del empleo prehispánico de algún método de escritura andina. Se ha propuesto que los quipus y los tocapus podrían ser un sistema de escritura, pero existen demostraciones generalmente aceptadas. 

Los primeros españoles (principalmente cronistas y evangelizadores) así como los aborígenes buscaron graficar el(los) quechua(s), principalmente en quechua clásico y en formas tempranas de la variante cuzqueña, empleando el alfabeto latino; esta situación generó múltiples grafías para distintos fonemas y viceversa. Sin embargo, las lenguas quechua permanecieron como esencialmente orales hasta muy entrado el siglo veinte.

El 29 de octubre de 1939, se da uno de los primeros intentos de graficación del quechua aun bajo el paradigma de un solo idioma. En esta ocasión, es aprobado un alfabeto para las lenguas indígenas americanas que consta de 33 signos durante el XXVII Congreso Internacional de Americanistas, en Lima (Perú).

El 29 de octubre de 1946, el Ministerio de Educación del Perú aprueba el Alfabeto de las Lenguas Quechua y Aimara, con 40 signos utilizables en las cartillas de alfabetización rural que proyectaba dicha institución.

En la semana del 2 al 13 de agosto de 1954, durante el III Congreso Indigenista Interamericano, realizado en La Paz, se creó el "Alfabeto fonético para las lenguas quechua y aimara", basándose en los acuerdos de los dos congresos anteriores, realizados en Pátzcuaro (1940) y Cuzco (1949).

El 16 de octubre de 1975, a finales del gobierno militar de Juan Velasco Alvarado, el Ministerio de Educación peruano nombra una Comisión de Alto Nivel para implementar la Ley de Oficialización de la Lengua Quechua. Esta informa y recomienda el Alfabeto Básico General del Quechua, aprobado por el ministerio mediante la , cuyas letras eran a, aa, ch, e, h, i, ii, k, l, ll, m, n, ñ, o, p, q, r, s, sh, t, tr, ts, u, uu, w, y. Diez años más tarde, mediante Resolución Ministerial Nº 1218-85-ED, el alfabeto oficial suprimió las letras e y o; se usan solo tres vocales, a, i y u, que corresponde a la fonología del quechua. Sin embargo, la Academia Mayor de la Lengua Quechua en el Cuzco todavía promueve una versión del alfabeto quechua cusqueño con las cinco vocales del español.

Los números dígitos y el diez, no se señala el cero, en diferentes lenguas quechuas son:

En la tabla anterior se han empleado los símbolos del Alfabeto Fonético Internacional.

Las lenguas quechuas son aglutinantes y las reglas para la formación de palabras se conservan bastante bien. Los morfemas son altamente regulares, no suelen variar debido al entorno en donde se insertan. Las palabras se componen de tan solo dos tipos de morfemas: raíces y sufijos. Existen raíces independientes, que forman una palabra completa sin ser modificadas, y existen también las dependientes de sufijos para este fin. Los sufijos son de dos tipos: derivativos, que modifican el significado de los lexemas, y flexivos, que determinan los paradigmas de los rasgos gramaticales. Algunos sufijos son enclíticos, los cuales pueden unirse al final de cualquier palabra de la oración. Los sufijos son altamente productivos, pues conforman significados predictibles por el interlocutor.

Las lenguas quechuas se caracterizan por preferir un orden SOV variable, las palabras que cumplen una función adjetivos y las cláusula relativas anteceden siempre al nombre que modifican (lengua centrípeta). El alineamiento morfosintáctico suele ser de tipo acusativo, marcando el objeto directo con sufijos cognados de *"-kta". La frase posesiva completa se conforma anteponiendo el poseedor al poseído y marcando respectivamente con sufijos de caso genitivo y personal relativo.

La evidencialidad se conserva como rasgo gramatical en toda la familia. Así, se distingue siempre entre información presencial, reportada, conjeturada e inferida. Esta categoría se expresa en la forma de enclíticos o partículas que pueden ser libremente añadidas a virtualmente cualquier palabra del enunciado.

Por otro lado, el protoquechua habría contado con cuatro personas gramaticales definidas simultáneamente por la inclusión del hablante y la del oyente. El número no habría estado gramaticalizado inicialmente. Este sistema se mantiene en el quechua de Pacaraos y se trasluce en las demás variantes.
Posteriormente, aparecieron diversas marcas gramaticales verbales y nominales para los plurales, superponiéndose al esquema inicial. Con este cambio, el sistema pronominal vira a uno de siete personas: tres personas en singular, dos en primera persona plural (incluyente y omitente) y plurales de segunda y tercera persona. Además, la diferencia entre las dos primeras personas plurales ha desaparecido en el quichua norteño.

El número no parece haber tenido mayor relevancia hasta el advenimiento de la Conquista española. Otros rasgos gramaticales, como el género, no han ingresado a las lenguas quechuas. Solo la definitud se agregó al huanca mediante el sufijo -"kaq", derivado del agentivo del verbo "ka"- (‘haber’).

La morfología verbal es riquísima en esta familia. Las lenguas quechuas cuentran con repertorios amplios de sufijos derivativos. Estos se unen directamente a la raíz en cantidades virtualmente ilimitadas, formando nuevos temas. El protoquechua tuvo cuatro sufijos verbales que expresan dirección: "-rku-" (‘hacia arriba′), "-rpu-" (‘hacia abajo′), "-yku-" (‘hacia adentro’) y "-rqu-" (‘hacia afuera’). Solo en el quechua I y en el caso de los sufijos de dirección vertical se han conservado productivos, mientras que en otras instancias se presentan fosilizados o ausentes.

Los temas verbales son dependientes de sufijos flexivos de modo y tiempo, los cuales son específicos de la persona gramatical del sujeto de la oración. Los verbos quechuas concuerdan tanto con el sujeto como con el objeto directo cuando son transitivo, habiendo excepciones en el quichua ecuatoriano, región donde se ha perdido la conjugación binominal.

En cuanto al modo, se distingue la flexión del imperativo de la del indicativo con conjuntos distintos de sufijos. El quechua distingue típicamente dos tiempos: futuro y no futuro. Un verbo en el modo no futuro se puede especificar para el pasado mediante el sufijo *-"rqa". Muchas veces, el aspecto se marca mediante sufijos derivativos.

La gran mayoría de raíces nominales son morfológicamente independientes; esto es, no necesitan sufijos para formar una palabra completa. Ejemplos de excepciones son los pronombres relativos como "kiki"- (‘uno mismo’) o "llapa"- (‘todos’), que requieren sufijos posesivos para ser completos. Véase la forma ancashina "llapantsik" (‘todos nosotros’). Los sustantivos y adjetivos formados no presentan diferencias. Un nombre modifica a otro anteponiéndosele directamente. Juntos conforman una frase nominal que tiene su núcleo en la palabra final. Pueden anteponerse modificadores indefinidamente.

La flexión nominal admite sufijos posesivos específicos de cada persona gramatical, seguidos típicamente de un sufijo de plural opcional como -"kuna"; sin embargo, el orden se invierte en el quichua santiagueño. En tercer lugar van los sufijos de caso. Las frases nominales se flexionan añadiendo los sufijos solamente a su núcleo. Una frase sin sufijo de caso se considera nominativo. Los sufijos de caso acusativo (*-"kta"), lativo (-"man"), instrumental (-"wan"), comitativo (-"ntin"), genitivo (-"pa", salvo -"pi" en Laraos), benefactivo (-"paq") y causativo (-"rayku") son conservados en toda la familia quechua. Existen además sufijos de caso en los que se presentan variación, como el locativo (*-"ćhaw", -"pi", -"pa", "-man"), el ablativo (-"piq", -"pita", -"manta", -"paq", -"pa"), el prolativo (-"pa", -"nta"), el terminativo (*-"kama", -"yaq") y el comparativo (*-"naw", -"hina", -"yupay").

Actualmente el quechua es lengua nacional oficial en el Perú, Ecuador y Bolivia. También se habla sin ser oficial a nivel nacional en regiones limítrofes de Argentina y Chile. Las constituciones de Colombia, de Ecuador y del Perú estipulan a sus respectivas lenguas indígenas –entre ellas el quechua o quichua– como segundas lenguas oficiales después del español ("oficiales en las zonas donde predomina" u "oficiales en su territorio"). En Chile y en Argentina carecen de este reconocimiento oficial.









</doc>
<doc id="4610" url="https://es.wikipedia.org/wiki?curid=4610" title="Álvaro del Amo">
Álvaro del Amo

Álvaro del Amo y de Laiglesia (Madrid, 1942) es un guionista y dramaturgo, director de cine y de teatro, crítico musical y novelista español.
Su labor como autor teatral está en parte inédita, pero en sus montajes ("Geografía", 1985 y "Motor", 1988) se aprecia una traslación del lenguaje y la estética cinematográfica a la escena. Es el suyo un teatro que funde la realidad y la ficción, la vida y la apariencia, en un tono escéptico e irónico.

Como narrador, se inicia con "Mutis" (1980, La Gaya Ciencia). Otras obras suyas son "Libreto" (1985), "Contagio" (1991), "El horror" (finalista del premio Herralde en 1993), "Incandescencia" (colección de relatos, 1998) y "Los melómanos" (2000). Fue también guionista de "Amantes", de Vicente Aranda, obra que lleva a la escena en 2014 en el teatro Valle-Inclán de Madrid.

Enayística

Literaria

Como director




</doc>
<doc id="4611" url="https://es.wikipedia.org/wiki?curid=4611" title="Sergi Belbel">
Sergi Belbel

Sergi Belbel Coslado (Tarrasa, 29 de mayo de 1963) es un dramaturgo, director y traductor teatral español.

Licenciado en Filología Románica y Francesa por la Universidad Autónoma de Barcelona en 1986. Fue miembro fundador del Aula de Teatro de la Universidad Autónoma de Barcelona. En 1985 recibe el Premio Marqués de Bradomín por "Caleidoscopios y faros de hoy", que se estrena al año siguiente. A partir de ese momento se han sucedido los estrenos, y Belbel ha pasado a convertirse en uno de los valores jóvenes más firmes del país.

Su actividad teatral se amplía al campo de la dirección, y desde 1988 es profesor de Dramaturgia del Instituto del Teatro de Barcelona.

En 1996 obtuvo el Premio Nacional de Literatura en la modalidad de Literatura Dramática y en 2005 fue nombrado director del Teatro Nacional de Cataluña, cargo al que renunció en 2013.

Ha dirigido obras de autores clásicos y contemporáneos, entre otros, de Shakespeare, Calderón, Molière, Goldoni, Beckett, Koltés, Mamet y De Filippo.

Tiene escritas unas veinte obras teatrales escritas, entre las que se destacan "Caleidoscopios y faros de hoy", "Después de la lluvia", "Elsa Shneider", "Tálem", "Caricias" y "La sangre". Ha obtenido entre otros galardones el Premio Marqués de Bradomín (1985), el Premio Nacional Ignasi Iglesias (1987). Varias de sus obras han sido llevadas al cine por el director Ventura Pons.

Aparte de capítulos sueltos de otras series televisivas, escribió "Nissaga l'herència" para TV3. En 2011, participa en el guion de la película "Eva", guion por el que es candidato al Goya.

En 2014, dirige los guiones de la serie "Sin identidad" de Atresmedia.





</doc>
<doc id="4616" url="https://es.wikipedia.org/wiki?curid=4616" title="Bobby Fischer">
Bobby Fischer

Robert James Fischer, más conocido como Bobby Fischer (Chicago, Illinois; 9 de marzo de 1943-Reikiavik, Islandia; 17 de enero de 2008),
fue un gran maestro de ajedrez, campeón mundial entre 1972 y 1975. Obtuvo el título máximo del ajedrez mundial al vencer al soviético Borís Spasski en el llamado «Encuentro del Siglo». Sin embargo, después de lograr el título, no volvió a jugar nunca más en torneos internacionales. Su país dictó orden de busca y captura contra él en 1992 por haber jugado otro encuentro contra Borís Spasski en Sveti Stefan (Yugoslavia, país contra el cual Estados Unidos había decretado un bloqueo) y más tarde revocó su pasaporte. En julio de 2004, Fischer fue detenido en el aeropuerto Narita ―en Tokio (Japón)―, por intentar salir del país utilizando un pasaporte no válido; fue liberado ocho meses después y autorizado a viajar a Islandia, país que acababa de concederle la nacionalidad a pesar del malestar que ello generó en las autoridades de Estados Unidos. Falleció en Islandia tres años después.

Estrictamente hablando, Bobby Fischer no fue un niño prodigio como lo fueron José Raúl Capablanca, Samuel Reshevsky o Arturo Pomar. Su desarrollo al principio fue más bien lento. Hasta los trece años no comenzó a despuntar como un jugador de capacidad superior; antes de esa edad no se apreciaban en sus resultados y su calidad de juego signos de extraordinario talento ajedrecístico. Es exacta la aseveración del árbitro internacional español Pablo Morán en el sentido de que «Como "niño prodigio" no fue muy brillante; en cambio, como "adolescente prodigio" no ha tenido parangón en la historia del ajedrez».

Fue hijo de la enfermera suiza Regina Wender, inteligente y políglota, y del físico de origen alemán Hans-Gerhardt Fischer, aunque existe controversia respecto de si este último fue el padre biológico de Bobby, pues Regina y Hans-Gerhardt no vivían juntos desde 1939.
Se considera casi seguro que su padre biológico fue el físico húngaro Paul Nemenyi, dotado de asombrosa inteligencia de tipo matemático. En cualquier caso, Regina y Hans-Gerhardt no obtuvieron el divorcio hasta 1945; Bobby, que entonces tenía dos años, quedó, junto con su hermana mayor Joan, al cuidado de su madre. En 1949 Regina se trasladó con sus dos hijos a Nueva York, a un pequeño apartamento en Brooklyn. Fischer aprendió a jugar ajedrez por sí mismo a la edad de 6 años, a partir de las instrucciones que venían en un estuche con diversos juegos que le regaló su hermana. Su afición por el ajedrez fue aumentando hasta llegar a la obsesión; su madre, preocupada, le llevó a la consulta de un psiquiatra pero la actitud del chico no varió. En enero de 1951, gracias a un anuncio en el periódico, Bobby participó en una sesión de simultáneas contra el maestro Max Pavey; esa fue su primera aparición pública como ajedrecista, y aunque perdió le sirvió, según confesión propia, para seguir esmerándose en ajedrez. El presidente del Brooklyn Chess Club, Carmine Nigro, fue su mentor de ajedrez, le enseñó los fundamentos de la estrategia y le introdujo en el mundo del ajedrez de competición.

En 1955 ingresó en el Manhattan Chess Club y participó por primera vez en el Campeonato Junior de Estados Unidos, finalizando en décimo lugar. Un año después, en Filadelfia, conquistaría el título juvenil, ganando ocho partidas, empatando una y perdiendo otra. Poco después de esta victoria, Fischer abandonó la Erasmus Hall High School a los 16 años para dedicarse por completo al ajedrez; aducía que estudiar era una pérdida de tiempo. Sus profesores le recordaban como un muchacho difícil. Probablemente tenía un coeficiente intelectual alto (tal vez de 187), aunque era asocial. En 1956, John Collins, que había sido tutor de otros jugadores sobresalientes como Robert Byrne y William Lombardy, le aceptó como alumno. En algunas ocasiones se ha descrito a Collins como una figura paterna para Fischer.

Sobre su partida con Donald Byrne, conocida por algunos como la «partida del siglo» en 1956, el doctor Max Euwe, campeón del mundo entre 1935 y 1937, comentó: «Que un renombrado maestro se confíe demasiado ante un jugador joven en pleno progreso, y sufra por ello una seria derrota, no tiene en sí nada de particular, y en la historia del ajedrez se registran bastantes ejemplos. Mas lo que no sucede todos los días es que un escolar de trece años supere francamente en la combinación a uno de los mejores jugadores de Estados Unidos. Las combinaciones de Fischer no son particularmente profundas, aunque tampoco evidentes».

Cuando Bobby tenía 17 años su madre le abandonó, dejando solo a su hijo en el apartamento de Brooklyn, entregado totalmente al ajedrez.

Su carrera coincide con el encumbramiento de la escuela soviética de ajedrez que, subvencionada por el Estado, dominó la disciplina desde 1948 hasta la desintegración de la Unión Soviética en 1991, con el paréntesis de Fischer; y aun después de dicha desintegración, los jugadores formados en dicha escuela soviética estuvieron en la cima durante años. El campeonato de Estados Unidos de 1957 tuvo para la Federación Internacional de Ajedrez (FIDE) en el sistema de Candidatos al título mundial, categoría Zonal. Bobby, ya campeón juvenil de Estados Unidos y que había terminado noveno en la edición anterior del campeonato absoluto, se alzó con el primer lugar, y se clasificó para el Torneo Interzonal de Portoroz (hoy Eslovenia) del año siguiente, en el que obtuvo el sexto puesto. Un resultado magnífico que le permitió acceder al torneo de Candidatos y obtener de forma automática el título de gran maestro. Muchos jugadores han superado desde entonces el récord de precocidad de Fischer en obtener el título de gran maestro (lo hizo con quince años y medio); cabe señalar, sin embargo, que el estadounidense lo alcanzó con recursos muy limitados, en una época en la que la información ajedrecística, particularmente la que llegaba a Estados Unidos, era mínima; en solitario y sin entrenadores (mientras que los jugadores soviéticos recibían apoyo oficial), y sin el auxilio de potentes programas de juego y bases de datos disponibles para los jugadores actuales. Debieron pasar treinta y tres años para que la húngara Judit Polgár estableciera una nueva marca.

Disputó nueve veces el Torneo Rosenwald de Nueva York, en el que se dirimía el campeonato de Estados Unidos. En su primera participación solo pudo ganar un par de partidas, aunque una de ellas, su victoria ante Donald Byrne de la que ya hemos hablado, lo proyectó a la fama internacional pues se publicó en revistas especializadas prácticamente de todo el mundo. En dicho juego Fischer venció mediante un brillantísimo juego combinativo, aún más sorprendente si se toma en cuenta que apenas contaba con trece años de edad. En sus restantes ocho apariciones obtuvo en todas el título nacional con al menos un punto de ventaja sobre el segundo clasificado. En la edición de 1963 logró además la proeza de coronarse campeón venciendo en todas las partidas; una hazaña sin precedentes pues participaban en el certamen figuras de la talla de Reshevsky, Larry Evans, Pal Benko y Robert Byrne.

Bobby Fischer acudió a cuatro Olimpiadas de ajedrez con el equipo de Estados Unidos. En todas ellas consiguió resultados sobresalientes, incluyendo dos medallas de plata y una de bronce defendiendo el primer tablero de su país. Sus enfrentamientos contra el equipo de la Unión Soviética, cuyo primer tablero generalmente ocupaba el campeón del mundo, produjeron partidas extraordinarias que recogen las antologías. En Leipzig (Alemania), en 1960, empató espectacularmente con el soviético entonces campeón del mundo Mijaíl Tal; al término del juego, Fischer le dijo con sorna al campeón: «No juega usted mal», a lo que Tal respondió: «Es la primera vez que usted lo reconoce, y si me hubiera ganado afirmaría que jugué como un genio».

En Varna (Bulgaria), dos años después, se encontraría con el legendario Mijaíl Botvínnik, al que dominó durante toda la partida aunque este salvaría el empate gracias a la ayuda en el análisis de la posición aplazada de sus compañeros de equipo, especialmente de Efim Geller, alcanzando un final de tablas teóricas en desventaja material. En la Olimpiada de La Habana (Cuba), el equipo de la Unión Soviética reservó al campeón mundial Petrosián, por lo que Fischer se enfrentó al entonces subcampeón Borís Spasski con quien firmaría las tablas después de cincuenta y siete movimientos en una partida que comenzó con la Apertura Española o Ruy López. En su última presentación «olímpica», en Siegen (Alemania), Spasski, ya como campeón mundial, derrotaría brillantemente al gran maestro de Brooklyn. Fischer en total ganó 40 partidas, empató 18 y perdió 7 en la máxima competición por equipos del ajedrez, con un porcentaje de efectividad de 75,4 por ciento.

Aún con su enorme talento y dedicación al juego, el campeonato del mundo habría de esperar algunos años. En el maratoniano torneo de Candidatos 1959, en Yugoslavia (se jugó en tres ciudades: Bled, Zagreb y Belgrado), terminó en quinto lugar, empatado a puntos con Svetozar Gligorić, gran figura del ajedrez internacional; en esta ocasión Fischer perdió sus cuatro partidas con Tal. En 1962, triunfó en el Interzonal de Estocolmo (Suecia), con dos puntos de ventaja sobre Tigrán Petrosián (1929-1984), quien se coronaría campeón del mundo un año después, y Geller. En el torneo de Candidatos de Curaçao (Antillas Neerlandesas), sin embargo, Fischer terminaría sorprendentemente en un lejano cuarto lugar, detrás de Petrosián, Paul Keres y Geller, y denunciaría en un artículo de revista que los soviéticos jugaban en equipo, asistiéndose, y haciendo tablas fáciles entre ellos para repartirse los puntos y reservarse, con objeto de alejar de los puestos preferentes a otros jugadores. Desde luego, las acusaciones de Fischer no pudieron probarse, pero poco después la FIDE cambiaría las reglas del campeonato del mundo, sustituyendo el sistema del torneo de Candidatos por el de los enfrentamientos individuales.

Fischer se apartó temporalmente del ajedrez profesional durante algunos meses entre 1964 y 1965, se dedicó a dar exhibiciones y no participó en el ciclo de candidatos que culminó con el encuentro por el título mundial entre Petrosián y Borís Spasski en 1966, ni acudió a la Olimpiada de Tel Aviv (Israel). En 1967, no obstante, se presentaría al Interzonal de Sousse (Túnez) en una nueva acometida por el título mundial. Después de diez rondas, Fischer encabezaba la clasificación con un récord impresionante de siete victorias y tres empates, cuando decidió intempestivamente abandonar el torneo, alegando un calendario cargado. La crítica de Fischer parecía injusta pues el torneo se había estructurado, entre otras cosas, para respetar los días de descanso que sus creencias religiosas le imponían. De ese certamen es memorable su partida frente a Reshevsky, pues Fischer apareció en la sala de juego pocos minutos antes de perder por incomparecencia, y con la mitad del tiempo asignado en su reloj derrotó con relativa facilidad a su ilustre contrincante.

Bobby Fischer ganó todos los torneos en los que participó desde el mes de diciembre de 1962 hasta el Campeonato del Mundo de 1972, con solo dos excepciones: el Torneo Memorial Capablanca de 1965 (que se celebró en La Habana y Bobby jugó por teletipo desde Nueva York), en el que quedó empatado en segundo lugar con Borislav Ivkov y Geller, medio punto por detrás del ganador Smyslov; y la Copa Piatigorsky de 1966, en la que ocupó el segundo lugar, un punto y medio detrás de Spasski. En toda su carrera jamás perdió un enfrentamiento individual o partido, como se le conoce en la jerga ajedrecística. Derrotó al filipino Cardoso en 1957, y en 1961 dejó inconcluso un duelo con Reshevsky, que quedó en empate después de once partidas, a causa de desacuerdos con los organizadores; en su camino al campeonato del mundo se adjudicó tres victorias inapelables (ante el danés Bent Larsen y los soviéticos Mark Taimanov y Petrosián), y finalmente derrotó a Spasski en el ya mencionado y famoso Match del Siglo. Veinte años después, en 1992, disputó frente a su viejo rival Spasski un encuentro de exhibición, del que hablaremos.

Una de las características que distinguían a Fischer era la rapidez de su juego. En muy contadas ocasiones se veía en apuros de tiempo, pues casi siempre jugaba de manera ágil y muy correcta. No es de extrañar que con su excepcional talento se convirtiera en uno de los mejores jugadores de partidas rápidas (llamadas "blitz", donde cada jugador dispone de cinco minutos para toda la partida). En 1970 se disputó en Herceg Novi (Montenegro, antigua Yugoslavia), el torneo de partidas rápidas más importante celebrado hasta entonces. Fischer triunfó al lograr diecinueve de los veintidós puntos posibles contra rivales de primerísima fila, como los ex campeones mundiales Tal, Petrosián y Smyslov y los exaspirantes David Bronstein y Reshevsky. Solo Fischer y Tal fueron capaces de reproducir de memoria, una vez terminada la competencia, las partidas que habían jugado.

Ese mismo año se llevó a cabo en Belgrado (Serbia, antigua Yugoslavia) el entonces anual encuentro entre la Unión Soviética y el resto del mundo. Bobby Fischer accedió a jugar en el segundo tablero, cediendo el primero a Larsen, que había obtenido mejores resultados en los meses anteriores, pues el estadounidense había permanecido inactivo. Fischer tuvo que enfrentarse a Petrosián, entonces subcampeón mundial, a quien venció convincentemente 3 a 1 (dos victorias y dos tablas), a pesar de haber permanecido alejado de los tableros. En la edición 1971, el estadounidense ganaría por primera vez el Óscar del Ajedrez, distinción que repetiría los dos años siguientes.

En 1972, finalmente, alcanzó el derecho a disputar el Campeonato del Mundo. Obtuvo el primer lugar en el Torneo Interzonal de Palma de Mallorca (Islas Baleares, España) de 1970, en el que ganó quince de las veinticuatro partidas que disputó (las últimas siete del torneo de forma consecutiva), algo verdaderamente inusual tomando en consideración el nivel del torneo. Posteriormente, en el apogeo de su fuerza, arrolló en el ciclo de Candidatos disputado a lo largo de 1971 a los grandes maestros Mark Taimánov (soviético) y Bent Larsen (danés, el único que había logrado derrotarle en el Interzonal del año anterior), por idéntico resultado en sus respectivos enfrentamientos al mejor de 10 partidas: un sonrojante 6 a 0 que, en el caso de Taimánov, le supuso serios problemas con el aparato comunista soviético que lo acusó falta de carácter y de no haber sabido defender la honra patriótica. De hecho, ese resultado causó un enorme revuelo entre las autoridades ajedrecísticas de la Unión Soviética, que no solo acusaron a Taimánov, sino a todo el potente equipo de analistas que lo acompañó durante el encuentro.

Lo excepcional de estos resultados solamente se puede explicar diciendo que el gran talento de Fischer había llegado a su máximo esplendor. Para comprender la magnitud de la hazaña de Fischer, hay que tener en cuenta que, en el ajedrez de alto nivel, el empate es un resultado natural, pues lo normal es que a los contendientes les cueste trabajo romper el equilibrio. Hay que remontarse casi cien años atrás para hallar un resultado similar: en 1876, una época de ajedrez aún rudimentario, el primer campeón mundial Wilhelm Steinitz derrotó por 7 a 0 a Joseph Henry Blackburne, uno de los mejores jugadores de la época, aunque, en ese caso, Steinitz contaba con la gran ventaja de acabar de sentar las bases del ajedrez moderno que le proporcionaba una evidente superioridad sobre el resto de jugadores. En 1971 repetir ese resultado en la alta competición resultaba increíble, y más aún repetirlo dos veces consecutivas.

En la final de Candidatos, Fischer derrotó en Buenos Aires (Argentina) al ex campeón mundial, el soviético Tigrán Petrosián, por 6,5 a 2,5, ganando con ello el derecho a enfrentarse a Spasski con el título mundial en juego. Su cadena de 20 victorias consecutivas (las siete últimas del Interzonal, las de sus enfrentamientos con Taimánov y Larsen y la primera de su encuentro con Petrosian) constituye un auténtico hito en la historia del ajedrez de élite, como también lo es el haber cedido solo 2,5 puntos (una derrota y tres tablas) en las 21 partidas que disputó en las tres eliminatorias del ciclo de Candidatos. Algo que asombraba al mundo ajedrecístico y amedrentaba a sus rivales.

A partir de 1970, la Federación Internacional de Ajedrez adoptó la fórmula del científico húngaro Árpád Élő para estimar la fuerza de juego en el ajedrez. Robert Fischer, a la luz de este sistema, vigente en nuestros días, alcanzó la marca de 2785 puntos, registro que durante mucho tiempo se consideró el mejor rendimiento conseguido por un ajedrecista. Con el tiempo, varios jugadores notables han ido superando la barrera de los 2800 puntos, entre ellos, cinco campeones del mundo, Garri Kaspárov, Veselin Topálov, Vladímir Krámnik, Viswanathan Anand y Magnus Carlsen, así como los grandes maestros Levon Aronian, Alexander Grischuk y Fabiano Caruana. Este hecho por sí solo, sin embargo, no significa que su desempeño haya sido superior al logrado por Fischer años atrás, al menos desde el punto de vista estadístico. Esto se debe al fenómeno conocido como «inflación del Elo».
La clasificación de los jugadores han ido aumentando de manera imperceptible pero sostenida a través de los años, y aunque excede el propósito de este artículo referir las causas del fenómeno en cita, al que constantemente se le busca solución,
es cosa establecida que la evaluación Elo no resulta un criterio fiable para comparar el nivel de ajedrecistas pertenecientes a diferentes épocas. No obstante hay que reconocer, siendo justos, que el nivel general de los maestros de ajedrez en los tiempos modernos ha aumentado considerablemente, lo que hace más difícil ascender en el «escalafón».

Con independencia de cómo pueda medirse la potencia de un ajedrecista, Fischer fue, sin duda, un jugador excepcional. Su estilo no es fácil de definir, pero, según sus propios rivales, se basaba en una combinación de energía y ambición de victoria, precisión táctica, preparación teórica, firmeza estratégica y confianza en sí mismo.

El encuentro por el campeonato del mundo de 1972 fue singular por diversas razones, aunque algunas de ellas nada tenían que ver con el ajedrez. Reikiavik, capital de Islandia, representó el enfrentamiento de dos mitos del tablero. El primero era el propio Fischer, que nunca había ocultado su fobia deportiva hacia los grandes maestros soviéticos. Sus excentricidades, exigencias y reacciones eventualmente infantiles, para bien o para mal lograron interesar al gran público, de ordinario ajeno a las incidencias del ajedrez profesional. Lo excepcional del estadounidense, sin embargo, eran sus resultados. Su puntuación Elo era 125 puntos superior a la de Spasski. Si no se hubiera tratado del número uno y dos del escalafón mundial, la estadística indicaría solamente el enfrentamiento de dos ajedrecistas de diferente categoría. Tal era la distancia que Fischer llegó a tener con relación a sus contemporáneos.

El retador, en efecto, parecía invencible. No obstante, se enfrentaba a un rival temible, otro auténtico mito de invulnerabilidad. Ese rival no era solamente Spasski, un jugador de talento excepcional al que Fischer no había podido vencer antes de este encuentro, sino la poderosa estructura de ajedrez de la Unión Soviética, dirigida por el Comité de Educación Física y Deportes, que había producido a todos los campeones y subcampeones mundiales desde 1948, y había ganado todas y cada una las Olimpíadas que se habían efectuado desde entonces. Ningún campeonato del mundo desde 1951 se había disputado fuera de Moscú.

El ajedrez, en definitiva, era una cosa muy seria en la Unión Soviética, con importantes implicaciones políticas, pues sus frecuentes triunfos eran considerados una prueba de la superioridad del régimen; no podían permitirse, en consecuencia, perder el título a manos de un aspirante de Estados Unidos. El ex campeón mundial Mijaíl Botvínnik puso a disposición del equipo de Spasski un análisis exhaustivo de las partidas de Fischer; Ígor Bondarevski abordaría la parte técnica; Efim Geller el repertorio de aperturas; Nicolay Krogius, de la asistencia psicológica; e Ivo Ney se encargaría de la puesta a punto física del campeón.
El apoyo de Fischer lo componían Lombardy, el abogado Paul Marshall (que tuvo un papel destacado) y Fred Cramer, por parte de la Federación de Ajedrez de Estados Unidos. El partido no podía ser, por sus circunstancias particulares, un mero evento deportivo. Se enfrentaban dos maneras muy distintas de entender al mundo que aspiraban a la supremacía. Por unos meses la Guerra Fría se trasladó a un tablero de ajedrez.

Tras la jugada número 30 de la primera partida, los dos jugadores llegaron a una posición completamente simétrica (dos alfiles de casillas negras y seis peones repartidos de igual manera por ambos flancos). Fischer perdió cuando cometió un error amateur al comer un peón con su alfil que después del movimiento de un peón de Spasski queda sin escapatoria siendo una presa fácil para el rey que se encontraba cerca. No se presentó a la segunda partida alegando disconformidad con la organización. Parecía que Spasski retendría el título para el ajedrez soviético; pero Bobby Fischer venció en la tercera. La cuarta partida fue tablas y, desde la quinta, se impuso rotundamente el gran maestro estadounidense. Después de un tenso desarrollo, Fischer venció a su rival tras 21 partidas (Spasski abandonó por teléfono la última partida, que había quedado aplazada) y se coronó campeón mundial el 1 de septiembre de 1972 con un total de 7 partidas ganadas, 3 perdidas y 11 tablas. Ha sido el único estadounidense en conquistar el título.

Resultó incomprensible para todo el mundo que el momento culminante de la carrera de Bobby Fischer al conquistar el campeonato mundial significase también su abrupto y completo final, pues nunca más quiso volver a jugar una sola partida de competición oficial a pesar de tener solamente 29 años. La única explicación plausible para esta actitud es un temor insuperable a ser derrotado, lo cual se suma a los diversos indicios de obsesión y desequilibrio mental que hasta entonces había dado. Además de que al no volver a jugar frustró las expectativas de todos los aficionados y organizadores del mundo, hay que observar que la única fuente de futuros ingresos de Bobby sería el ajedrez o estaría en estrecha relación con este.

Cumplido el siguiente ciclo de clasificación tres años más tarde, en 1975, llegó una vez más la ocasión de que el campeón defendiera su título frente al nuevo aspirante, en este caso el joven soviético Anatoli Kárpov (n. 1951), de 24 años. Entonces Bobby planteó a la FIDE que no deseaba defender su título de la misma forma que lo había ganado, sino según otro esquema anterior a 1948, que consistía, entre otras cosas, en que la victoria sería para quien primero alcanzara 10 victorias (sin contar las tablas), reteniendo el título el campeón en caso de empate a 10. Hasta aquí puede decirse que es un planteamiento equitativo y razonable; de gustos personales, si se quiere, pero razonable. El gran inconveniente es que Fischer pretendía introducir además la condición de que él (Fischer) también retendría el título si se empataba a nueve.

Aunque la FIDE y la delegación soviética aceptaron las restantes exigencias de Fischer, la cuestión del empate a nueve no era razonable ni admisible. Para que se entienda mejor lo irracional de esta condición, podemos enunciarla así: «El campeón será Kárpov si gana diez partidas, y Fischer si gana nueve». Esta condición sería ridícula en otros deportes que se disputan a un tanteo prefijado, como el tenis, o cuando en fútbol hay que recurrir al lanzamiento de penaltis. Botvinnik calificó esta condición de " «unfair»" (injusta). La FIDE desautorizó esta pretensión, pero entonces Fischer se negó en redondo a jugar. No quedó otra opción que desposeer a Fisher de su título y proclamar campeón a Kárpov, quien, con sus resonantes triunfos en grandes torneos y matches por el campeonato mundial durante los diez años siguientes, se hizo merecedor indiscutible al título mundial y, con el paso del tiempo, ha demostrado ser uno de los jugadores más formidables de la historia del ajedrez, que ha ganado un casi increíble total de 160 torneos de ajedrez de élite.

Fischer, decepcionando profundamente a la afición mundial, continuó sin jugar e incluso desapareció de la vida pública. Kárpov, que dijo sentirse como un niño al que no le dan un juguete largo tiempo prometido, se entrevistó en 1976 con Bobby para concertar un encuentro, pero su intento no tuvo éxito. En 1981 Bobby, con aspecto de vagabundo, fue detenido en Pasadena (California) cuando la policía le confundió con el atracador de un banco.

Mucho después, en 1992, Fischer, a la sazón de 49 años, aceptó jugar un encuentro amistoso de exhibición contra su antiguo adversario Spasski, de entonces 55 años de edad. El partido comenzaría en Sveti Stefan, a orillas del Adriático, y acabaría en Belgrado, enclaves ambos de la República Federal de Yugoslavia, nación procedente del desmembramiento de la antigua Yugoslavia. Aunque tuvo notoriedad por ser la reaparición de Fischer después de veinte años, este encuentro estaba muy lejos de ser una repetición del famoso de 1972, pues la Unión Soviética se había disuelto y ya no había intereses ni tensiones internacionales; Spasski se había nacionalizado francés y ―esto es destacable― había retrocedido en la clasificación internacional Elo hasta el puesto 124; y, por último, no había en juego ningún título oficial ni extraoficial. Lo único realmente relevante era el apartado financiero, pues la exhibición estaba dotada con sustanciosos premios en metálico: 3,65 millones de dólares para el vencedor y 1,35 para el perdedor. El Gobierno de Estados Unidos prohibió a Fischer ―como a todos sus conciudadanos― involucrarse en el partido a causa de las restricciones en el comercio impuestas a la República Federal de Yugoslavia por su intervención en la reciente guerra de Bosnia. Ante las cámaras, Fischer (que jugaba con una bandera estadounidense en la mesa) escupió sobre la carta del gobierno de su país que le conminaba a desistir de jugar. El encuentro se celebró y acabó con la victoria del estadounidense, aunque la calidad de las partidas y el desarrollo general del acontecimiento despertaron escaso interés en el mundo del ajedrez. Las autoridades de Estados Unidos dictaron orden de búsqueda y captura contra Fischer, lo cual podía llegar a costarle hasta 10 años de cárcel.

A lo largo de años, al mismo tiempo que su salud mental comenzaba a deteriorarse, Bobby Fischer se había caracterizado por lanzar furibundos pronunciamientos antisemitas y antiestadounidenses. A pesar de ser él mismo de ascendencia judía por el lado materno, admiraba a Adolf Hitler y era un negacionista del Holocausto.
En al menos una oportunidad se había declarado a favor de un hipotético golpe militar derechista en su país, seguido de la destrucción de sinagogas y la ejecución de cientos de miles de judíos.

En una entrevista a una radio filipina el 12 de septiembre de 2001, Fischer proclamó su satisfacción por los ataques terroristas contra las Torres Gemelas y el Pentágono ocurridos el día anterior y se pronunció en durísimos términos contra Estados Unidos e Israel.
Sin embargo, cabe aclarar que su odio nunca se extrapoló al tablero pues durante toda su vida mantuvo una cordial relación con otros ajedrecistas judíos.

En julio de 2004 fue detenido en el aeropuerto de Narita, en Tokio (Japón) por utilizar un pasaporte no válido, pues Estados Unidos lo había anulado. Bobby permaneció ocho meses detenido hasta que en marzo de 2005, finalmente, Islandia le concedió la ciudadanía islandesa, con lo que las autoridades japonesas le autorizaron a que viajase a ese país. Islandia hizo este gesto por razones humanitarias, y sentimentales, pues el encuentro de 1972 hizo famosa su capital, Reikiavik, en todo el mundo. Las autoridades estadounidenses, sin embargo, expresaron su malestar por la concesión de dicha nacionalidad, pues reclamaban que el ajedrecista fuese extraditado a Estados Unidos para ser juzgado.

Tres años más tarde, el 17 de enero de 2008, Fischer falleció a los 64 años en Reikiavik (Islandia), a causa de una enfermedad renal y fue enterrado en una tumba sencilla en un cementerio cercano a Selfoss, pequeña localidad costera al sudoeste del país.

En junio de 2010 la Corte Suprema de Islandia determinó que el cuerpo de Fischer fuese exhumado para obtener una muestra de su ADN y poder así establecer si había sido el padre de Jinky Young, una niña filipina de nueve años cuya madre aseguraba haber tenido una relación con el excampeón. En julio de 2010 el cuerpo fue exhumado y, tras tomar muestra de su ADN, inhumado de nuevo. Magnus Skulason, íntimo amigo de Fischer, sostenía que el ajedrecista no era el padre de la niña. En agosto de 2010 se informó de que la prueba de ADN había revelado que Jinky Young no era hija del excampeón del mundo.





</doc>
<doc id="4617" url="https://es.wikipedia.org/wiki?curid=4617" title="Cisne">
Cisne

Cisne es el nombre común de varias aves anseriformes de la familia Anatidae. Son aves acuáticas de gran tamaño. La mayoría de las especies pertenecen al género "Cygnus".

Varias especies de aves son conocidas como cisne:


El cisne era un ave consagrada a Apolo, como dios de la música, porque se creía que el cisne poco antes de morir cantaba melodiosamente. Por esto dijo Pitágoras que esta ave se asemejaba a un alma que jamás muere y que su canto antes de morir viene de la alegría que experimenta porque va a ser librada de su cuerpo mortal. Platón parece ser de la misma opinión y algunos otros dicen que está consagrada a Apolo, porque goza del don de prever los bienes de la otra vida de los cuales espera gozar después de su muerte. 

Ovidio coloca a los cisnes en los Campos Elíseos. Estaban también consagrados a Venus, ya por su maravillosa blancura, ya por su temperamento bastante semejante al de la diosa del deleite. La carroza de Venus es tirada algunas veces por cisnes. Zeus se transformó en esta ave para engañar a Leda.



</doc>
<doc id="4618" url="https://es.wikipedia.org/wiki?curid=4618" title="Estación">
Estación

Estación hace referencia a varios artículos:









</doc>
<doc id="4627" url="https://es.wikipedia.org/wiki?curid=4627" title="Baltasar Gracián">
Baltasar Gracián

Baltasar Gracián y Morales (Belmonte de Gracián, 8 de enero de 1601-Zaragoza, 6 de diciembre de 1658) fue un jesuita, escritor español del Siglo de Oro que cultivó la prosa didáctica y filosófica. Entre sus obras destaca "El Criticón" —alegoría de la vida humana—, que constituye una de las novelas más importantes de la literatura española, comparable por su calidad al "Quijote" o "La Celestina".

Su producción se adscribe a la corriente literaria del conceptismo. Forjó un estilo construido a partir de sentencias breves muy personal, denso, concentrado y polisémico, en el que domina el juego de palabras y las asociaciones ingeniosas entre estas y las ideas. El resultado es un lenguaje lacónico, lleno de aforismos y capaz de expresar una gran riqueza de significados.

El pensamiento de Gracián es pesimista, como corresponde al periodo barroco. El mundo es un espacio hostil y engañoso, donde prevalecen las apariencias frente a la virtud y la verdad. El hombre es un ser débil, interesado y malicioso. Buena parte de sus obras se ocupan de dotar al lector de habilidades y recursos que le permitan desenvolverse entre las trampas de la vida. Para ello debe saber hacerse valer, ser prudente y aprovecharse de la sabiduría basada en la experiencia; incluso disimular, y comportarse según la ocasión.

Todo ello le ha valido a Gracián ser considerado un precursor del existencialismo y de la postmodernidad. Influyó en librepensadores franceses como La Rochefoucauld y más tarde en la filosofía de Schopenhauer y Nietzsche. Sin embargo, su pensamiento vital es inseparable de la conciencia de una España en decadencia, como se advierte en su máxima «floreció en el siglo de oro la llaneza, en este de yerro la malicia».

Nacido en Belmonte de Gracián, muy cerca de Calatayud, en 1601, las noticias sobre su infancia son muy escasas. Se sabe que su padre fue contratado por el concejo de Ateca para ejercer como médico en 1604, por lo que toda su familia tuvo que desplazarse hasta esa localidad. Todo indica que estudió letras desde los diez o doce años en Calatayud, quizá en el colegio de jesuitas de esta localidad. Hacia 1617 debió residir uno o dos años en Toledo con su tío Antonio Gracián, capellán de San Juan de los Reyes, donde aprendería lógica y profundizaría en el latín.

En 1619 ingresó en el noviciado de la provincia jesuítica de Aragón, situado en Tarragona, en el que se le dispensó de los dos años preceptivos de estudio de humanidades debido a su excelente formación anterior. En 1621 volvió a Calatayud, donde cursó dos años de Filosofía. De esta etapa data su aprecio por la ética, que influyó en toda su producción literaria. Otros cuatro cursos de Teología en la Universidad de Zaragoza completaron su formación religiosa.

Ordenado sacerdote en 1627, comenzó a impartir Humanidades en el Colegio de Calatayud. Parece ser que fue un periodo grato, pero pocos años más tarde tuvo graves enfrentamientos con los jesuitas de Valencia, adonde fue trasladado en 1630. De allí pasó a Lérida en 1631 para encargarse de las clases de Teología Moral. En 1633 viajó a Gandía para enseñar Filosofía en el colegio jesuita de la villa y se renovaron las enemistades con sus antiguos correligionarios valencianos.

En el verano de 1636 volvió a tierras aragonesas, a Huesca, como confesor y predicador. Esta ciudad tuvo una importancia capital en la vida del jesuita, puesto que con el apoyo del erudito mecenas Vincencio Juan de Lastanosa pudo publicar su primer libro: "El Héroe" (1637).

Lastanosa reunía en su casa-museo un importante cenáculo literario y artístico. El palacio del prócer oscense, que fue visitado por Felipe IV, era conocido por sus exquisitos jardines, por una estupenda armería, por la colección de medallas y por una magnífica biblioteca de cerca de siete mil volúmenes, una cantidad extraordinaria en esa época. En este propicio ambiente, Gracián traba contacto con la intelectualidad cultural aragonesa, entre la que se cuenta el poeta Manuel de Salinas o el historiador Juan Francisco Andrés de Uztarroz.

En 1639 llegó a Zaragoza, nombrado confesor del virrey de Aragón Francisco María Carrafa, duque de Nochera, con quien viaja a Madrid, donde predicó. No obstante, su estadía en la Corte fue desalentadora, pues, aunque aspiró a medrar entre la república literaria de la capital, sus ambiciones se saldaron con un franco desengaño. Con todo, publicó allí su segunda obra, "El Político" (1640) y ultimó la primera versión de su tratado teórico sobre estética literaria barroca, titulado "Arte de ingenio, tratado de la agudeza" (1642).

De 1642 a 1644 ejerció el cargo de vicerrector del Colegio de Tarragona, donde auxilió espiritualmente a los soldados que tomarían Lérida en la Sublevación de Cataluña (1640). De resultas de esta campaña, cayó enfermo, y fue enviado a Valencia para reponerse. Al calor de la magnífica biblioteca del hospital, preparó una nueva obra, "El Discreto" (1646), que verá la luz en Huesca. De nuevo en su refugio oscense, impartió clases de Teología Moral hasta 1650. Es en esta época cuando más activamente pudo dedicarse a la literatura. Aparecieron entonces el "Oráculo manual y arte de prudencia" (1647) y la segunda versión del tratado sobre el ingenio y el concepto "Agudeza y arte de ingenio" (1648).

Fue destinado a Zaragoza el verano de 1650 con el cargo de Maestro de Escritura, y al año siguiente publica la primera parte de su obra cumbre: "El Criticón". A excepción de "El Comulgatorio", Gracián publicó toda su obra sin el preceptivo permiso de la Compañía, lo que provocó protestas formales que fueron elevadas a las instancias rectoras de los jesuitas. Tales quejas no le disuadieron al punto de que apareciera en Huesca la segunda parte de esta obra. Algunos jesuitas valencianos, a consecuencia de viejas enemistades, interpretaron uno de sus pasajes como una ofensa a sus personas, lo que le granjeó nuevos ataques ante los superiores de la Compañía que apuntaban al contenido escasamente doctrinal de sus obras, impropias de un jesuita profeso, ya que, ocupándose todas ellas de la Filosofía Moral, esta se aborda desde una óptica profana. Quizá para contribuir a su descargo publicó, por primera vez con su auténtico nombre, "El Comulgatorio" (1655), un libro acerca de la preparación para la Eucaristía.

Pero la aparición en 1657 de la tercera parte de "El Criticón" determinó su caída en desgracia. El nuevo provincial de Aragón, el catalán Jacinto Piquer, recriminó públicamente a Gracián en el refectorio, le impuso como penitencia ayuno a pan y agua —prohibiéndole incluso disponer de tinta, pluma y papel—, y le privó de su cátedra de Escritura del Colegio Jesuita de Zaragoza. A comienzos de 1658 Gracián es enviado a Graus, un pueblo del prepirineo oscense.

Al poco tiempo, Gracián escribió al General de la Compañía para solicitar el ingreso en otra orden religiosa. Su demanda no fue atendida, pero se le atenuó la pena: en abril de 1658 ya fue enviado a desempeñar varios cargos menores al Colegio de Tarazona, hoy Hogar Doz. Los últimos contratiempos debieron acelerar su decadencia física, pues en junio no pudo asistir a la congregación provincial de Calatayud, falleciendo, poco más tarde, en Tarazona, el 6 de diciembre de 1658. Probablemente fue enterrado en la fosa común del colegio.

En Gracián todo lo absorbe la vida de la inteligencia, por lo cual lo afectivo queda prácticamente anulado: de ahí el tono duro y la falta de calor humano de que adolecen frecuentemente sus obras. Su concepto del hombre (el peor de los seres de la creación) y de la vida (perpetuo engaño y lucha constante) es negativo y puramente barroco: un desengaño. Todas las cosas tienen un doble valor de engañosa apariencia y de oculta realidad. Y se ajusta a reflejar la decadencia del Imperio español.

Su moral es una moral práctica, de combate y orientada al triunfo: hay que actuar sin descanso poniendo en tensión voluntad e inteligencia. La prudencia, y al mismo tiempo la desconfianza y el recelo son condiciones ineludibles para ello. Se expresa en las trescientas máximas incluidas en el "Oráculo manual y arte de Prudencia" (1647). Ya que la vida es lucha, nos apresta al combate proponiendo la defensa como ocultación y el ataque como descubrimiento del alma ajena; también una retirada a tiempo vale más que una brillante victoria.

Su estilo presenta la intensificación máxima del conceptismo, del que es incluso teorizador. Son constantes distintos tipos de antítesis (también las paradojas, las litotes, los oxímoros) y los juegos de palabras (anfibologías, etc.), así como distintos tipos de elipsis, en especial el zeugma, cuyo fin es conseguir una prosa densa, rápida y lacónica. Pero lo caracterizan peculiarmente tres recursos: la agudeza, la alegoría (especialmente la prosopopeya o personificación) y el humor.

Temáticamente, propone en sus opúsculos diversos arquetipos humanos de hombre superior: "El héroe" (1639), que establece las condiciones necesarias para sortear todo tipo de obstáculos prácticos; "El discreto" (1646), que hace lo mismo para el ámbito cortesano, y "El político" (1640), donde propone como modelo a Fernando el Católico.

Vista en conjunto la producción de Baltasar Gracián, podemos observar una estrecha relación con su biografía. Desde el juvenil entusiasmo por el triunfo y la gloria del hombre ejemplar, configurado en "El Héroe", se llegará al desengaño de la vejez y la muerte en los últimos capítulos de "El Criticón". Así se presenta como escritor en 1637 en el prólogo :Dos tratados más continuarían esta línea de delinear el hombre perfecto: "El Político", que extrae tales cualidades del rey Fernando el Católico, y "El Discreto", un manual de conducta para el hombre en sociedad, sea cual sea su posición en ella.

Por otro lado, Gracián dedicó grandes esfuerzos a elaborar un tratado de estética literaria barroca: la "Agudeza y arte de ingenio", que refunde una versión anterior titulada "Arte de ingenio, tratado de la agudeza". Allí teoriza sobre el «concepto» y propone una nueva retórica basada en la "praxis" barroca que se distancia, en parte, de la tradición aristotélica de la "Poética", pues su análisis está fundamentado en textos, que a su vez ejemplifican una clasificación de los distintos tipos de agudeza de su propia invención.

Toda la obra de Gracián, ocupada siempre de su aplicación práctica a la vida del hombre, tiene por objeto la Filosofía Moral. Las ideas acumuladas en tratados anteriores sobre el modo de conducirse en el mundo son sintetizadas y reunidas en el libro más lacónico y sentencioso de su producción, el "Oráculo manual y arte de prudencia". Con él culmina el proyecto de «manuales del vivir» para la persona cabal, y en él también se subsumen, probablemente, libros proyectados —en "El Discreto" se habla de los «doce gracianes», que se titularían «El Atento», «El Galante»— que no llegaron a ver la luz.

Fue admirado por moralistas franceses de los siglos XVII y XVIII, y en el XIX por Schopenhauer, quien recibió la influencia del pensamiento graciano y tradujo al alemán el "Oráculo manual y arte de prudencia". Esta versión, muy fiel al espíritu del aragonés, fue conocida por Nietzsche, que dijo en una de sus cartas: «Europa no ha producido nada más fino ni más complicado en materia de sutileza moral». Gracias a ellos la obra del filósofo español fue objeto de estudio en la universidad alemana.

Solo le quedaba ensayar la fabulación. Poner todo su trabajo de investigación retórica al servicio de una novela, que fuera a la vez tratado de filosofía moral, bajo el género que él mismo denominó «agudeza compuesta fingida», lo que viene a significar «alegoría novelada». Se concretó en las tres partes de "El Criticón", que recorre todo el ciclo de la vida de un hombre, que debe, además, vencer a las circunstancias del mundo en crisis de la sociedad del barroco.

El último libro que publicaría, quizá por hacer una concesión a los oficios propios de la orden jesuita, que no veía con buenos ojos su abordar la lucha por la vida siempre al margen de auxilio cristiano, fue "El Comulgatorio". Es el único que publicó con su auténtico nombre y cumplió con la preceptiva revisión por parte de los censores de su orden. Sin embargo, tras la aparición en 1657 de la tercera parte de "El Criticón" —de nuevo sin consentimiento de la Compañía y con su conocido (a estas alturas) seudónimo de Lorenzo Gracián—, el aragonés fue confinado a una celda y castigado a ayuno riguroso. Los tintes pesimistas que destila "El Criticón" corren parejas con su última peripecia vital.

El título de "El Héroe" remite a la cualidad máxima del hombre en la antigüedad clásica, esto es, la "virtus" latina o la "areté" (αρετη) griega. El primero de los libros publicados por Baltasar Gracián es un tratado en el que se describen las cualidades del hombre de excepción. Cada una de esas prendas relevantes de que está formado el héroe se autoriza con la mención de su presencia en un eminente personaje histórico, entroncando con los "Dicta et facta memorabilia" de la tradición latina de Valerio Máximo. Este ejemplo funciona como colofón de cada uno de los capítulos, llamados «primores». El término alude al sentido etimológico de esta palabra, derivándolo de "primus" como sustantivo, esto es, «el mejor», «el primero».

La obra remite también al "El Príncipe" de Maquiavelo, pues es un doctrinal de buen gobierno, si bien aplicado al ámbito de la rección de la propia persona. Pero, en contraste con el tratadista italiano, esta «razón de estado de uno mismo» no olvida conciliar la política y la moral, ya que en la España de la Contrarreforma al príncipe maquiavélico se opuso un príncipe cristiano, como pregona el título de la obra de su contemporáneo Diego Saavedra Fajardo "Idea de un príncipe político cristiano" (1640).

Por otro lado, "El Héroe" conecta con "El Cortesano" de Baltasar de Castiglione, aunque ya no basta con los modales corteses renacentistas. En Gracián el cortesano necesita también astucia, inteligencia, buen discernimiento e incluso disimulo.


En "El político don Fernando el Católico", bajo la forma de una tesis que defiende que Fernando el Católico fue el mayor rey de la monarquía española, se describen sus dotes políticas y sus virtudes como ejemplo de emulación para el hombre político. Se trata pues, no tanto de una biografía sino de otro tratado de moral práctica, solo que encarnada en el mayor príncipe, en un rey.

Construye este texto sobre la falsilla genérica del encomio biográfico, caracterizado como un discurso académico ante un auditorio, modelo que tuvo un importante cultivo en el Renacimiento y el Barroco. Se ofrece con él un modelo concreto de gobernante que destaca sobre todos los monarcas pasados, y que constituye un espejo en el que se deben reflejar los posteriores, incluido Felipe IV, en la línea de los conocidos «espejos de príncipes».


Gracián escribió dos tratados sobre el ingenio y la agudeza. El primero de ellos lo publicó en Madrid en 1642 con el título de "Arte de ingenio, tratado de la agudeza". El segundo apareció en 1648, con el título de "Agudeza y arte de ingenio". La teoría sobre el concepto que aborda en esta obra ilumina la producción literaria contemporánea a Gracián. Los géneros empleados en las distintas obras de Gracián se definen aquí de modo teórico. Posteriormente fue refundida, revisada y ampliada en una edición definitiva titulada "Agudeza y arte de ingenio", publicada en 1648.


Es su primer fruto de plena madurez. De nuevo estamos ante un tratado en el que se describe cómo ha de ser el hombre que quiere llegar a ser «persona»: un completo caballero prudente, sagaz, dotado de buen gusto y de buena educación. Por discreción entiende la capacidad de discernimiento, que es al fin y al cabo la inteligencia para elegir lo mejor y para distinguir y valorar aquello que el hombre necesita para ser un varón de todas las horas y todas las circunstancias.

Pero ahora se renuncia a conseguir la excelsitud heroica, contentándose con ayudar a mejorar al hombre de mundo para que destaque entre sus semejantes. El modelo propuesto ya no es un ser excepcional, un héroe de fama, gobernante o rey, como sucedía en sus dos anteriores tratados de parecida temática. Ahora se trata de adiestrar a un hombre prudente que no solo necesita muchas cualidades para gobernar, sino tan solo para desenvolverse en sociedad. Con el tiempo, el pesimismo de un Gracián que contempla la malicia del mundo se ha hecho más agudo. Su desengaño hace que el objetivo del triunfo del héroe planteado en el primer tratadito sea una utopía. Ahora basta con llegar a ser persona, es decir, ser, en el sentido clásico, un hombre virtuoso.

En los capítulos de este tratado —llamados ahora «realces» en consonancia con el destacar, y no ya ser el mejor, que revelaba el «primor»— se ensayan gran variedad de géneros: diálogo, apólogo, emblema, sátira, fábula, epístola, discurso académico, o panegírico, entre otros. En ellos utiliza por vez primera la fábula o la alegoría, creando ya un módulo de ficción que servirá a los propósitos de la «agudeza compuesta fingida», o novela alegórica, que será su empeño final. El último de sus «realces», que no lleva indicación de género, titulado , muestra un esquema de división de la vida del hombre en edades que preludia el de su novela "El Criticón".


El "Oráculo manual y arte de prudencia" (1647) supone la síntesis de los tratados didáctico-morales anteriores. El libro consta de trescientos aforismos comentados, y ofrece un conjunto de normas y orientaciones para guiarse en una sociedad compleja y en crisis.

No sólo ha interesado a aficionados a la literatura. A la obra se han acercado desde su publicación hasta la actualidad pensadores y filósofos. La admiración que por ella mostró Arthur Schopenhauer le llevó a traducirla al alemán y su versión fue la más difundida del "Oráculo" en esta lengua.

Este «arte de prudencia» escrito por Gracián ha tenido vigencia incluso en la actualidad, como demuestra el hecho de que una versión al inglés, titulada "The art of worldly wisdom: a pocket oracle" llegó a vender más de ciento cincuenta mil ejemplares en el ámbito anglosajón, al ser presentado como un manual de autoayuda para ejecutivos. En 1992, permaneció dieciocho semanas (dos en primera posición) en la lista de los más vendidos del Washington Post en el apartado "Nonfiction/General".

Se ha pensado que esta obra es una mera recopilación de sentencias de sus libros anteriores, pero esto solo es cierto, y en parte, en los cien primeros aforismos. El hecho de glosar apotegmas de obras propias era un proceder nuevo, pues hasta entonces estaba reservado a la autoridad de las citas extraídas los clásicos de la antigüedad, o al menos a autores de reconocido prestigio. El ser el "Oráculo" una antología de sus máximas indica que Gracián se eleva a sí mismo al rango de los autores que constituían el canon literario de la época.

El sintagma bimembre «oráculo manual y arte de prudencia», funciona como antítesis, pues oráculo tiene un sentido de «secreto emanado de la divinidad», y a este término se une el adjetivo «manual», esto es, «para un uso práctico y portátil». La palabra «arte» se usa en la acepción de «reglas y preceptos para hacer rectamente las cosas», como recoge el "Diccionario de Autoridades". Pero se le opone la prudencia, pues no hay normas ciertas y universales para la conducta del hombre. En conclusión, el libro sería un precioso y secreto manual de normas de uso práctico para la conducta del hombre en un mundo conflictivo.

El género que adopta el "Oráculo", a diferencia de los tratados anteriores, prescinde de la argumentación y la autoridad de ejemplos históricos que habían sido habituales en "El Héroe", "El Político" o "El Discreto". La observación del mundo y la aplicación de estos consejos en la práctica bastan para garantizar la validez y utilidad de estos oráculos mundanos.

Su estilo es la quintaesencia de la economía expresiva y la concisión graciana. En esta obra se da la mayor intensidad en cuanto a concentración semántica en frases breves y elípticas, que se suceden en un encadenado de sentencias. Este carácter hace del "Oráculo" su obra de más difícil lectura, pero también la de mayor contenido en ideas, constituyendo una "summa" de su pensamiento anterior.


Con la "Agudeza y arte de ingenio", Gracián escribe su definitiva estética literaria barroca. Se trata de un tratado de retórica en el que se analizan las figuras literarias dominantes en su época.

Esta obra supone el comentario definitivo acerca del concepto y también una teorización de su propia producción literaria anterior y posterior, y de la de sus contemporáneos. No es una retórica más, pues su análisis del hecho literario parte de los ejemplos extraídos de los textos, que en esta versión se amplían considerablemente, y no de una preceptiva previa.

En esta revisión de su trabajado "Arte de ingenio", en gran medida una reedición muy ampliada, incluyó más traducciones castellanas de textos latinos —sobre todo de Marcial—, debidas a Manuel de Salinas. Pero también reorganiza los materiales de 1642 y revisa, corrige y pule el estilo.

Es en este tratado donde aparece la definición que del concepto da Gracián:

No se trata, en puridad, de una obra sobre el conceptismo, tal y como lo concibió Menéndez Pelayo en su "Historia de las ideas estéticas en España", pues el concepto en Gracián es la expresión de una semejanza, desde un símil a una metáfora, desde una dilogía hasta la alegoría sostenida. Y estos tropos son utilizados tanto por escritores caracterizados como «conceptistas» como por los denominados «culteranistas». Tan es así que la mayor cantidad de ejemplos (que avalan las figuras que define como conceptos) son traídos de la poesía de Góngora. Además ejemplifica con escritores no solo del barroco español, sino de todos los tiempos. Y de ese modo encuentra conceptos ingeniosos en epigramas de Marcial, sentencias de Séneca, aforismos de Tácito, discursos de Cicerón o "exempla" de Juan Manuel.


"El Comulgatorio" se ocupa de la preparación del cristiano para recibir la comunión. Según desvela la portada, el tratado «contiene varias meditaciones para que los que frecuentan la sagrada Comunión puedan prepararse, comulgar y dar gracias». «Meditaciones» es el nombre que el jesuita asigna a los capítulos de este libro, en la línea de obras anteriores, donde se titulaban «primores» ("El Héroe"), «realces» ("El Discreto"), «discursos» ("Agudeza y arte de ingenio") o «crisi (s)» ("El Criticón"). El capítulo o meditación primera sirve a la preparación del cristiano para recibir la comunión, el segundo al acto de la comunión propiamente dicha, el tercero a los frutos que se obtienen de recibir el cuerpo de Cristo y el cuarto a dar gracias. Estas meditaciones están divididas en puntos o temas de reflexión y, a su vez, cada punto presenta dos partes separadas tipográficamente por un asterisco.

Con "El Comulgatorio" Gracián abandona el estudio del ingenio y se dedica al de las emociones, en línea con los escritores espirituales del Siglo de Oro. Es este un libro de carácter religioso, muy distinto de los hasta ahora escritos por el aragonés, tanto en temática como en estilo. Lo publica por primera vez con su verdadero nombre y no con el de su hermano «Lorenzo Gracián» o bajo un anagrama como el «García de Marlones» con el que ve la luz la primera parte de "El Criticón". "El Comulgatorio" es más discursivo y apela a los afectos. Está más cercano a la oratoria sagrada que a la sentenciosa filosofía moral.

En cuanto al género de "El Comulgatorio", la crítica se divide entre quienes piensan que es una pieza de oratoria sagrada, es decir, un sermón, y los que sostienen que la obra pertenece al género de los libros de devoción.


Las tres partes del "Criticón", publicadas en 1651, 1653 y 1657, constituyen, sin duda, la obra maestra de su autor y es una de las obras cumbres del Siglo de Oro español. Bajo la forma de una extensa novela alegórica de carácter filosófico, esta novela reúne en forma de ficción toda la trayectoria literaria de su autor. "El Criticón" conjuga la prosa didáctica y moral con la fabulación metafórica, y con ello, cada «crisi» (capítulo), alberga una doble lectura —si no más— en los planos real y filosófico. En ella se unen invención y didactismo, erudición y estilo personal, desengaño y sátira social.

La obra constituye una extensa alegoría de la vida del hombre, representado en sus dos facetas de impulsivo e inexperto (Andrenio) y prudente y experimentado (Critilo). Estos dos personajes simbólicos, persiguiendo la Felicidad (Felisinda, madre para Critilio y esposa para Andrenio), acaban recorriendo todo el mundo conocido persiguiendo el aprendizaje de la virtud que, pese al engaño que ofrece comúnmente el mundo, les llevará a ganar la inmortalidad por sus hechos al llegar la muerte al final de la novela. Es, por tanto, la culminación literaria de la visión filosófica del mundo de Gracián, donde prima el desengaño vital y el pesimismo, si bien la persona cabal consigue elevarse sobre este mundo de malicia.

La obra podría verse, desde el punto de vista del género empleado, como una gran epopeya moral: fábula menipea la llamó Fernando Lázaro Carreter. Además se ha relacionado con la novela bizantina por la multitud de peripecias y aventuras que sufren los personajes y con la novela picaresca por la visión satírica que de la sociedad se muestra a lo largo del peregrinaje de sus protagonistas Critilo y Andrenio.

Aunque "El Criticón" se plantea inicialmente como una novela bizantina, en la que los dos peregrinos tienen como fin la búsqueda de Felisinda, pronto se descubre esto como un imposible, y con ello la estructura de la novela se conforma como una serie de episodios ensartados, al modo de la novela itinerante habitual de la picaresca. Tras este desengaño, el verdadero objetivo de nuestros protagonistas es alcanzar la virtud y la sabiduría. Pronto se abandona, con ello, una tenue intriga para demorarse en sucesivos cuadros alegóricos que dan cauce a la reflexión filosófica partiendo de una óptica satírica del mundo.

En cuanto a la estructura externa, "El Criticón" apareció, como dijimos, en tres entregas. En la "Primera parte", subtitulada «En la primavera de la niñez y en el estío de la juventud», los protagonistas se encuentran en la isla de Santa Elena, se cuentan las peripecias vitales que les han llevado allí y emprenden el viaje a España, comenzando por la Corte. La "Segunda parte", que aparece con el epígrafe de «Juiciosa cortesana filosofía en el otoño de la varonil edad», transcurre por tierras de Aragón y Francia. En la "Tercera Parte", titulada más llanamente «En el invierno de la vejez», entran por tierras de Alemania y acaban en la meca del peregrino cristiano, Roma, para ser anunciados a la muerte y llegar a la inmortalidad cruzando las aguas de tinta de la fama. Los tres tomos ofrecen un equilibrio estructural en lo externo muy notable. Las dos primeras partes constan de trece «crisis» cada una y la tercera tiene doce.

El tiempo del relato se configura a través de un eje cronológico marcado por el ciclo vital del hombre y asociado a las estaciones del año, tal y como aparece esbozado en el último capítulo de "El Discreto". El tiempo de la ficción novelesca progresa de manera lineal, pero recorrido por constantes digresiones e interrupciones. En estos remansos se da cuenta de todo un mundo alegórico y supone una detención del tiempo, muy adecuada a la generalización filosófica y moral.

Parece seguro que había un plan de la obra preconcebido en "El Criticón", lo que se observa en el hecho de que el arranque y desenlace de la obra suceden en una isla, según apuntó Klaus Heger. La misma tesis recoge Ricardo Senabre, que señala también la existencia de principios estructurales basados sobre todo en la antítesis. Esta se hace presente ya en los dos protagonistas medulares, Andrenio-Critilo, y recorre toda la obra, desde los distintos comportamientos que ante determinadas situaciones tienen cada uno de los protagonistas, hasta la abundancia de periodos bimembres en frases e incluso en la figura literaria de la anfibología. Por otro lado, si nos atenemos a los temas que recorren la obra, encontramos una recurrente antinomia entre el engaño y el desengaño, eje temático que estructura toda la narración.

En fin, Correa Calderón, considera que "El Criticón" no es sino una serie de cuadros alegóricos yuxtapuestos, constituidos a modo de fantasías morales, y enlazados tan solo por la andadura de sus dos protagonistas, como ocurre en los libros satíricos de la época. Así lo hacían obras tal "El Diablo Cojuelo", de Luis Vélez de Guevara, que adoptaba una estructura de pequeños módulos alegóricos independientes, como son los ensartados en el hilo del camino de los dos peregrinos de Gracián.

El autor exhibe constantemente una técnica perspectivista que desdobla la visión de las cosas según los criterios o puntos de vista de cada uno de los personajes, pero de forma antitética, y no plural como en Cervantes. La novela refleja, con todo, una visión pesimista de la sociedad, con la que se identificó uno de sus mejores lectores, el filósofo alemán del XIX Arthur Schopenhauer. Se trata de una mirada amarga y desolada, aunque su pesimismo alberga una esperanza en los dos virtuosos protagonistas, que consiguen escapar a la mediocridad reinante alcanzando la fama eterna.







Se conservan 32 cartas completas de Gracián, dirigidas a Vincencio Juan de Lastanosa, Andrés de Uztarroz, Manuel de Salinas, o el tortosino Francisco de la Torre Sevil. También conservamos epístolas dirigidas a sus superiores y compañeros jesuitas.

Importan sobre todo las enviadas a un jesuita de Madrid en 1646, en las que se refiere a la batalla de Lérida, donde se muestra orgulloso de su valerosa intervención. Nos cuenta cómo muchos capellanes cayeron enfermos o prisioneros, y cómo hubo de multiplicar su trabajo para absolver y dar el jubileo a los soldados en la misma línea del frente, como un combatiente más.

En estas cartas, además de obtener jugosos datos sobre su biografía, se muestra escritor en un estilo natural, que dista mucho del que él mismo se forjó para vehicular su obra literaria. En cambio, las aprobaciones y prólogos citados, escritos en su peculiar estilo conceptista, no tienen tanto interés, pues además hay que tener en cuenta su tono laudatorio y obligado formulismo.



El estilo de Baltasar Gracián, el generalmente llamado «conceptismo», se caracteriza por la elipsis y la concentración de un máximo de significado en un mínimo de forma, procedimiento que Gracián lleva a su extremo en el "Oráculo manual y arte de prudencia", compuesto íntegramente de casi tres centenas de máximas comentadas. En ellas se juega constantemente con las palabras y cada frase se convierte en un acertijo por obra de los más diversos mecanismos de la retórica.

Si los manieristas, como Herrera o Góngora, tuvieron por modelo el estilo oratorio de Virgilio y Cicerón, Gracián —barroco— adopta el estilo lacónico de Tácito, Séneca y Marcial, su paisano. Ello no significa, sin embargo, que el suyo sea un estilo llano, al modo de Cervantes. La dificultad es patrimonio tanto de cultistas gongorinos como de conceptistas. La diferencia estriba en que el esfuerzo de comprensión del lector de estos últimos exige descifrar los múltiples significados ocultos tras cada expresión lingüística. La concisión sintáctica, además, obliga frecuentemente a suponer elementos elididos, ya sean palabras con significado léxico o conectores lógicos.

La prosa de Gracián está conformada por oraciones independientes y breves separadas por signos de puntuación (coma, punto y punto y coma) y no por nexos de subordinación. Predomina, pues, la yuxtaposición y la coordinación. La escasa presencia de oraciones subordinadas en periodos complejos, lejos de facilitar la comprensión, la hace ardua, se hace necesario suplir la lógica de las relaciones entre las sentencias, deduciéndola del sentido, de la idea que se expresa, lo que no siempre es fácil. La profundidad de Gracián, pues, está en el concepto y en la elusión, no en la sintaxis.

La concisión expresiva se manifiesta en la frecuente deixis de elementos con función anafórica que aparecen sobreentendidos por el contexto lingüístico que lo antecede o porque (como en el caso frecuente de los nexos) la relación lógica se da por supuesta y delegada a la inteligencia del lector. De modo que es habitual la elipsis del verbo «ser», como se aprecia en su conocida máxima «Lo bueno, si breve, dos veces bueno. Y aun lo malo, si poco, no tan malo» (), que además es una declaración de intenciones que se puede aplicar al laconismo de su elocución. Muy frecuente es, con este mismo objetivo, la utilización del zeugma. También se da la elipsis del sustantivo. Aquí vemos un ejemplo de sustantivo omitido en zeugma: «Dieron luego conmigo en un calabozo cargándome de hierros, que este fue el fruto de los míos» (mis yerros —de errar—, se entiende: ).

La riqueza semántica, casi siempre polisémica, ofrece en Gracián la mayor intensidad que se había dado hasta entonces en la literatura española. Nunca bastará con el principal significado denotativo, sino que se han de buscar todas las acepciones simultáneas. La dilogía, la ambivalencia semántica, los dobles y hasta triples sentidos son constantes en el quehacer de Gracián. No para crear ambigüedad, sino para ofrecer todas las posibilidades de conocimiento y percepción del mundo. La doble interpretación en el plano real y el alegórico o filosófico es lo que confiere una densidad extraordinaria a su obra. Y esto sucede tanto a nivel morfológico o léxico como oracional y textual. Así, ejemplos de dobles sentidos frecuentes en él son «río» (de ‘reír’ y ‘corriente de agua’) o «yerro» (‘metal’ y ‘error’).

En la lengua de Gracián domina el verbo y el sustantivo, en contraposición al escaso uso del epíteto, pues «el estilo lacónico los tiene desterrados en primera ley de atender a la intensión, no a la extensión» ("Agudeza y arte de ingenio", discurso LX). Muchas de sus sentencias, preferentemente en el "Oráculo manual y arte de prudencia", comienzan con un verbo, a veces precedido de la partícula negativa. En muchas ocasiones el verbo se convierte en el eje de la frase.

Por otro lado Gracián usa constantemente la antítesis, el contraste, la paradoja, reforzándolos en sintagmas y oraciones de estructura bimembre, que se oponen entre sí. Esta simetría conduce a un ritmo ágil, una prosa binaria, semejante a la utilizada por fray Antonio de Guevara en el siglo XVI. Destaca cómo construye oposiciones con el uso de la paronomasia, como se observa en los dobletes "cielo-cieno", "tálamo-túmulo", "joyas-hoyas", "vestal-bestial" o "gusto-gasto". El mismo retruécano sirve a menudo a la expresión del contraste: el hombre «no come ya para vivir, sino que vive para comer» ("Criticón", I, X); los españoles «tienen tales virtudes como si no tuviesen vicios, y tienen tales vicios como si no tuviesen virtudes» ("Criticón", II, III); «así que de todo hay en el mundo: unos que siendo viejos quieren parecer mozos, y otros que siendo mozos quieren parecer viejos.» ("Criticón", III, I).

Otro rasgo estilístico de la prosa de Gracián es la búsqueda de la precisión léxica, para la que en muchas ocasiones se recurre al neologismo de creación. Y en este terreno, es donde aparece el sustantivo, la verdadera piedra de toque del estilo de Gracián, en detrimento de adjetivos, adverbios y nexos de subordinación. Así aparecen términos como «conreyes», «descomido», «desañar», «despenado» o «reconsejo», nuevos en el acervo del léxico español.

Otras veces recurre a acepciones caídas en desuso y que él pone en primer plano (plausible=admirable, plático=práctico, brujulear=sondear el carácter, sindéresis=capacidad natural para el juicio correcto, etc.) o a cultismos traídos de nuevo a enriquecer el idioma, como «crisis» (estimación, juicio), «especiosidad» (perfección), «delecto» (capacidad de discernimiento), «deprecar» (pedir con insistencia), «exprimir» (expresar), «convicio» (ofensa), «intensión» (efectividad). Otras veces trae a colación nombres propios para crear vocablos comunes: «su minerva» (su inteligencia o sabiduría). Por último encontramos aragonesismos que concurren a aumentar el caudal del vocabulario español: «podrecer» (pudrir), «defecarse» (decantarse el vino de impurezas, y por extensión, lustrarse, perfeccionarse), entre otros.

Es muy característica la polisemia etimológica o falsamente etimológica en el nombre, a partir de peregrinas etimologías inventadas por él. Así, de Dios dirá «que del dar (...) tomó el Señor su Santísimo y Augustísimo remombre de "Dí-os" en nuestra lengua española» . En el mismo lugar ("Agudeza...", XXXII), nos aporta otro caso similar: «Ponderaba un varón grave y severo el tiempo que roban en España las comedias, y las llamaba come-día y come-días.» . Otras veces utiliza procedimientos de derivación y composición para crear neologismos nominales insólitos, como «espantaignorantes», «arrapaltares», «marivenido», llegando a extremos de monstruosidad lingüística, en casos como los de «serpihombre» o «monstrimujer». El proceso inverso a la composición se da también en ejemplos como «casa y miento» (perversa interpretación de las raíces léxicas de ‘casamiento’), o «cumplo y miento» (de ‘cumplimiento’).

Conocido es su uso de máximas de tradición grecolatina y humanística y también del refranero popular, pero siempre llevándolas unas y otro a su terreno, reinterpretando su sentido o acomodándolo a los tiempos y, en el caso de los dichos del folclore paremiológico, tergiversando su enunciación y manipulándolo a su gusto. Así, a la dificultad y concisión de su estilo le cuadra una de sus más acertadas manipulaciones del acervo común: su frase «A pocas palabras, buen entendedor», que otorga un sentido radicalmente distinto al proverbio popular, pues justifica su estilo elíptico por la inteligencia de los lectores que han de acercarse a la obra de Gracián.

La prosa de Gracián no es producto de la espontaneidad, pues el estudio del autógrafo de "El Héroe" realizado por Romera-Navarro demuestra que corregía y pulía constantemente su estilo. Elabora la forma tanto como cuida el contenido ideológico, lo que muestra su clara conciencia de escritor. La búsqueda de la originalidad y el rechazo del lenguaje manido hacen del suyo un arte minoritario, distinguido y elevado; pues como dice en el prólogo —aunque a nombre de Lastanosa, debido indudablemente a su pluma— :





</doc>
<doc id="4628" url="https://es.wikipedia.org/wiki?curid=4628" title="Siglo de Oro">
Siglo de Oro

El Siglo de Oro español es un periodo histórico en que florecieron el arte y las letras castellanos, y que coincidió con el auge político y militar del Imperio español y de la dinastía española de los Habsburgo. El Siglo de Oro no se enmarca en fechas concretas, aunque generalmente se considera que duró más de un siglo, entre 1492, año del fin de la Reconquista, el Descubrimiento de América, y la publicación de la "Gramática castellana" de Antonio de Nebrija, y el año 1659, en que España y Francia firmaron el Tratado de los Pirineos. El último gran escritor del Siglo de Oro, Pedro Calderón de la Barca, murió en 1681, año también considerado como fin del Siglo de Oro español.

Es Hesíodo el que habla por primera vez de las cinco épocas o Edades del hombre en "Los trabajos y los días": de oro, de plata, de bronce, heroica y la de hierro actual, cada vez más degradadas. El término, pues, "Siglo de Oro", se concibió a semejanza de este mito para celebrar una época de excelencia en todos los órdenes. Según Juan Manuel Rozas, la denominación surgió en el discurso de ingreso en la RAE de Alonso Verdugo (1736), e Ignacio de Luzán la tomó para el tercer capítulo de su muy difundida "Poética" (1737); es más, la usó al año siguiente el erudito Gregorio Mayáns y Siscar en la dedicatoria de su "Vida de Miguel de Cervantes Saavedra" (1738), con lo que ya quedó autorizada para que la utilizase el crítico literario dieciochesco Luis José Velázquez, marqués de Valdeflores (1722-1772) en 1754 en su obra "Orígenes de la poesía castellana", aunque para referirse exclusivamente al periodo comprendido entre los Reyes Católicos y la muerte de Felipe III, esto es, fines del XV, el siglo XVI entero y el XVII no más allá de 1621. Quedaban así excluidos Pedro Calderón de la Barca y otros importantes autores. Posteriormente la definición se amplió, abarcando toda la época clásica o de apogeo de la cultura española, esencialmente el Renacimiento del y el Barroco del . Para la historiografía y los teóricos modernos, pues, y ciñéndose a fechas concretas de acontecimientos clave, el Siglo de Oro abarca desde la publicación de la "Gramática castellana" de Nebrija en 1492 hasta la muerte de Calderón en 1681.

A finales del ya se había popularizado la expresión «Siglo de Oro» (creada en 1736, y que pronto prendió) que suscitaba la admiración de don Quijote en su famoso discurso sobre la "Edad de Oro". Pero la ampliación de sus límites cupo a Casiano Pellicer, quien en 1804 lo extendió a Calderón y su escuela en su "Tratado histórico sobre el origen y progreso de la comedia...". En el la terminó de consagrar el hispanista norteamericano George Ticknor en su "Historia de la literatura española"; faltaba sin embargo incluir a Luis de Góngora y sus seguidores, de lo cual se encargaron a principios del siglo XX Alfonso Reyes y la Generación del 27. Helmut Hatzfeld dividió, por otra parte, el Siglo de Oro literario en cuatro épocas estéticas: renacimiento (1530-1580), manierismo (1570-1600), barroco (1600-1630) y barroquismo (1630-1670). José Antonio Maravall interpreta el barroco como "un concepto histórico" y lo delimita entre 1600 y 1670-80. Ángel del Río y Fernando Rodríguez de la Flor, por su parte, estiman que el barroco abarcaría los cien años entre 1580 y 1680.

Con su unión dinástica, los Reyes Católicos habían esbozado un Estado políticamente fuerte, consolidado más adelante, cuyos éxitos envidiaron algunos intelectuales contemporáneos, como Nicolás Maquiavelo. Los judíos que no se cristianizaron fueron expulsados en 1492 y se dispersaron fundando colonias hispanas por toda Europa, Asia y Norte de África, donde siguieron cultivando su lengua y escribiendo literatura en castellano, de forma que produjeron también figuras notables como José Penso de la Vega, Miguel de Silveira, Jacob Uziel, Miguel de Barrios, Antonio Enríquez Gómez, Juan de Prado, Isaac Cardoso, Abraham Zacuto, Isaac Orobio de Castro, Juan Pinto Delgado, Rodrigo Méndez Silva o Manuel de Pina, entre otros. En enero de 1492 Castilla conquista Granada, con lo que finaliza la etapa política musulmana peninsular, aunque una minoría morisca habite más o menos tolerada hasta tiempos de Felipe III. Además, en octubre Colón llega a América y el afán guerrero cultivado durante las guerras medievales de la Reconquista se proyectará sobre las nuevas tierras. Conquistadores, misioneros y aventureros protagonizan, con sus arriesgadas expediciones y su "sed de oro y de evangelización", «la más extraordinaria epopeya de la historia humana» según escribe el historiador Pierre Vilar. Sin embargo, y sobre todo a mediados del , son perseguidos o tienen que emigrar los erasmistas y los protestantes españoles, entre ellos los traductores de la Biblia al castellano, como Francisco de Enzinas, Casiodoro de Reina y Cipriano de Valera, además de los humanistas protestantes Juan Pérez de Pineda, Antonio del Corro o Juan de Luna, entre otros.

Durante el apogeo cultural y económico de esta época, España alcanzó prestigio internacional en toda Europa. Cuanto provenía de España era a menudo imitado; y se extiende el aprendizaje y estudio del idioma (véase Hispanismo).

Las áreas culturales más cultivadas fueron literatura, las artes plásticas, la música y la arquitectura. El saber se acumula en las prestigiadas universidades de Salamanca y Alcalá de Henares.

Las ciudades más importantes de este periodo son: Sevilla, por recibir las riquezas coloniales y a los comerciantes y banqueros europeos más importantes, Madrid, como sede de la Corte, Toledo, Valencia, Valladolid (que fue capital del Reino a comienzos del ) y Zaragoza.

En el terreno de las humanidades su cultivo fue más extenso que profundo y de matiz más divulgativo que erudito, a pesar de que la filología ofreció testimonios eminentes como la "Biblia políglota complutense" (1520) o la "Políglota de Amberes" (1572), y las numerosas gramáticas y vocabularios de las lenguas indígenas recién descubiertas, obra de los numerosos frailes misioneros que evangelizaron el continente recién descubierto.

También en el campo científico hubo avances importantes que, por ejemplo, en la agronomía, llegaron a constituir una revolución. Pues si el Viejo Mundo aportó al Nuevo la caña de azúcar, el trigo y la vid; el Nuevo aportó al Viejo la patata, el maíz, el frijol, el cacao, el tomate, el pimiento y el tabaco. La lingüística se desarrolló notablemente con autores Francisco Sánchez de las Brozas ("Minerva"). Para la geografía y cartografía el cosmógrafo Martín Cortés de Albacar descubrió la declinación magnética de la brújula y el polo norte magnético, que situó entonces —se mueve a lo largo de la historia— en Groenlandia, y desarrolló el nocturlabio; su discípulo Alonso de Santa Cruz inventaría la carta esférica o proyección cilíndrica. En la antropología y las ciencias naturales (botánica, mineralogía, etc.) eldescubrimiento de América proporcionó información acerca de nuevos pueblos, especies y fenómenos. Hubo también figuras eminentes en matemáticas, como Sebastián Izquierdo y su cálculo de la combinación y la permutación; Juan Caramuel, responsable del cálculo de probabilidades; Pedro Nunes, descubridor de la loxodrómica e inventor del nonio; Antonio Hugo de Omerique, Pedro Ciruelo, Juan de Rojas y Sarmiento, Rodrigo Zamorano y otros. En el campo de la medicina y la farmacología cabe destacar al botánico Andrés Laguna; así como el descubrimiento por la condesa de Chinchón (1638) de las propiedades contra las fiebres y la malaria de la quina, antecesor de la quinina. En la psicología y la pedagogía cabe salientar a Juan Luis Vives y a Juan Huarte de San Juan ("Examen de ingenios para las sciencias", 1575); mientras que la filosofía vio la formulación del punto de partida racionalista con Francisco Sánchez el Escéptico y exponentes de la Escuela de Salamanca como Francisco Suárez. Igualmente se desarrollaron, a causa del gran impacto que tuvieron los descubrimientos de nuevos pueblos, el derecho natural y el derecho de gentes, con figuras como Bartolomé de las Casas, influyente precursor de los derechos humanos y defensor del iusnaturalismo en su "De regia potestate"; o Francisco de Vitoria.
El Siglo de Oro abarca dos periodos estéticos, que corresponden al Renacimiento del (Reyes Católicos, Carlos I y Felipe II), y al Barroco del (Felipe III, Felipe IV y Carlos II). El eje de estas dos épocas o fases puede ponerse en el Concilio de Trento y la Contrarreforma.

España produjo en su edad clásica algunas estéticas y géneros literarios característicos que fueron muy influyentes en el desarrollo ulterior de la literatura universal. Entre las estéticas, fue fundamental el desarrollo de una realista y popularizante, tal como se había venido fraguando durante toda la Edad Media peninsular como contrapartida crítica al excesivo, caballeresco y nobilizante idealismo del Renacimiento: se crean así géneros tan naturalistas como el celestinesco ("Tragicomedia de Calisto y Melibea", "Segunda Celestina", etc.), la novela picaresca ("La vida de Lazarillo de Tormes", "Guzmán de Alfarache", "La vida del Buscón" o "Estebanillo González"), o la proteica novela polifónica moderna ("Don Quijote de la Mancha"), que Cervantes definió como «escritura desatada».

A esta vulgarización literaria corresponde una subsecuente vulgarización de los saberes humanísticos mediante los populares géneros de las misceláneas o silvas de varia lección, harto leídas y traducidas en toda Europa, y entre cuyos autores más importantes se encontraban Pedro Mejía, Luis Zapata o Antonio de Torquemada.

A esta tendencia anticlásica corresponde también la fórmula de la comedia nueva creada por Lope de Vega y divulgada a través de su "Arte nuevo de hacer comedias en este tiempo" (1609): una explosión inigualable de creatividad dramática acompañó a Lope de Vega y sus discípulos (Juan Ruiz de Alarcón, Tirso de Molina, Guillén de Castro, Antonio Mira de Amescua, Luis Vélez de Guevara, Juan Pérez de Montalbán, entre otros), que quebrantaron como él las unidades aristotélicas de acción, tiempo y lugar. Todos los autores dramáticos de Europa acudieron luego al teatro clásico español del Siglo de Oro en busca de argumentos, una rica almoneda y cantera de temas y estructuras modernas cuyo pulimento les ofrecerá obras de carácter clásico.

A fines del se desarrolla notablemente la mística de mano de Juan de la Cruz, Juan Bautista de la Concepción, Juan de Ávila o Teresa de Jesús; y la ascética, con autores como Luis de León y Luis de Granada, para entrar decaer en el tras una última corriente innovadora, el quietismo de Miguel de Molinos.

Muchos de los temas literarios del provenían de la rica y pluricultural tradición medieval, árabe y hebrea, del romancero y de la impronta italianizante de la cultura española —a causa de la presencia política del reino español en la península itálica durante bastante tiempo—. Por otra parte, géneros dramáticos como el entremés y la novela cortesana introdujeron también la estética realista en los corrales de comedias, y aun la comedia de capa y espada tenía su representante popular en la figura del gracioso.

A esta corriente de realismo popularizador sucedió una reacción religiosa, nobiliaria y cortesana de signo Barroco que también hizo notables aportaciones estéticas, pero que ya correspondía a una época de crisis política, económica y social. Al lenguaje claro y popular del , el castellano vivo, creador y en perpetua ebullición de Bernal Díaz del Castillo y Santa Teresa sucederá la lengua más oscura, enigmática y cortesana del Barroco. Y así resulta la paradoja de que la literatura española del Renacimiento de hace cinco siglos es más clara, legible y entendible que la literatura del Barroco de hace tan solo cuatro.

En efecto, la lengua literaria del se enrarece con las estéticas del conceptismo y del culteranismo, cuyo fin era elevar lo noble sobre lo vulgar, intelectualizando el arte de la palabra; la literatura se transforma en una especie de escolástica, en un juego o un espectáculo cortesano, aunque las producciones moralizantes y por extremo ingeniosas de un Francisco de Quevedo y un Baltasar Gracián distorsionan la lengua, aportándole más flexibilidad expresiva y una nueva cantera de vocablos (cultismos). El lúcido Pedro Calderón de la Barca crea la fórmula del auto sacramental, que supone la vulgarización antipopular y esplendorosa de la teología, en deliberada antítesis con el entremés, que, sin embargo, todavía sigue teniendo curso; pues estos autores todavía son deudores y admiradores de los autores del , a los que imitan conscientemente, aunque para no repetirse refinan sus fórmulas y estilizan cortesanamente lo que otros ya crearon, de forma que se perfeccionan temas y fórmulas dramáticas ya usadas por otros autores anteriores. La escuela de Calderón proseguirá con este modelo, que continuarán y cerrarán definitivamente, a comienzos del , José de Cañizares y Antonio de Zamora.

España experimentó una gran ola de italianismo que invadió la literatura y las artes plásticas durante el , lo que constituye uno de los rasgos de identidad del Renacimiento: Garcilaso de la Vega, Juan Boscán y Diego Hurtado de Mendoza introdujeron el verso endecasílabo italiano y el estrofismo y los temas del petrarquismo; Boscán escribió el manifiesto de la nueva escuela en la "Epístola a la duquesa de Soma" y tradujo "El cortesano" de Baltasar de Castiglione, ideal del caballero renacentista, en perfecta prosa castellana. Contra estos se levantó una corriente nacionalista encabezada por el nostálgico Cristóbal de Castillejo, residente en Viena, o Ambrosio Montesino, partidarios ambos del octosílabo, de las coplas castellanas y de la inspiración popular; todos eran, sin embargo, renacentistas.

En la segunda mitad del la tendencia italiana y la autóctona castellana coexistieron y se desarrolló la ascética y la mística, alcanzándose cumbres como las que representan san Juan de la Cruz, santa Teresa de Jesús y Luis de León, entre muchas otras que merecerían larga reseña; Ignacio de Loyola crea la Compañía de Jesús, que instruirá a grandes eruditos por toda Europa en todos los órdenes del conocimiento y además fomentará el estudio de las lenguas clásicas. El petrarquismo siguió siendo cultivado por autores como Fernando de Herrera, y un grupo de jóvenes nuevos autores comenzó a desarrollar un "Romancero" nuevo, a veces de tema morisco: Lope de Vega, quien desarrollará además un culto casticismo a través de sus diversos cancioneros ("Rimas", "Rimas sacras", "La Circe", "La Filomela", "Rimas humanas y divinas"...) Luis de Góngora y Miguel de Cervantes, entre otros; el mejor poema de épica culta en español fue compuesto en esta época por Alonso de Ercilla: "La Araucana", que narra la conquista de Chile por los españoles. En 1584, año de publicación de "La Araucana", Francisco Hernández Blasco dio a luz otro extenso poema épico en estancias de asunto evangélico, la "Universal redención", que tendría numerosas ediciones posteriores y notable éxito. Entre las figuras excepcionales de la lírica aparecen poetas tan interesantes como Francisco de Aldana, Andrés Fernández de Andrada, autor de la serena y meditativa "Epístola moral a Fabio", los hermanos Bartolomé y Lupercio Leonardo de Argensola, Fernando de Herrera, Francisco de Medrano, Francisco de Rioja, Rodrigo Caro, Baltasar del Alcázar o Bernardo de Balbuena, quien en 1624 dará al mundo la segunda gran epopeya culta en español, "El Bernardo o Victoria de Roncesvalles".

Posteriormente, durante el , la expresión literaria fue dominada por los movimientos estéticos del conceptismo y del culteranismo, expresado el primero en la poesía de Francisco de Quevedo, principalmente satírica, moral y filosófico-existencial, y el segundo en la lírica de Luis de Góngora (los "Sonetos", la "Fábula de Polifemo y Galatea" y sobre todo sus "Soledades"). El conceptismo se distinguía por la economía en la forma, a fin de expresar el máximo significado en un mínimo de palabras; esta complejidad se expresaba sobre todo en paradojas y elipsis. El culteranismo, por el contrario, extendía la forma de un significado mínimo y se distinguía por la complejidad sintáctica, por el uso constante del hipérbaton, que hace muy difícil la lectura, y por la profusión de los elementos ornamentales y culturalistas en el poema, que debía descifrarse como un enigma. Ambos parecen sin embargo las caras de una misma moneda que intentaba aquilatar la expresión para hacerla más difícil y cortesana. Luis de Góngora atrajo a su estilo a poetas importantes de personalidad muy acusada, como el Conde de Villamediana, Gabriel Bocángel, sor Juana Inés de la Cruz o Juan de Jáuregui, mientras que el conceptismo tuvo a seguidores más templados, como el Conde de Salinas o imbuidos de un culto casticismo, como Lope de Vega o Bernardino de Rebolledo.

En el Siglo de Oro el «monstruo de la naturaleza», como lo llamó Cervantes, fue Lope de Vega, también conocido como «el Fénix de los Ingenios», autor de más de cuatrocientas obras teatrales, así como de novelas, poemas épicos, narrativos y varias colecciones de poesía lírica profana, religiosa y humorística. Lope destacó como consumado maestro del soneto. Su aportación al teatro universal fue principalmente una portentosa imaginación, de la que se aprovecharon sus contemporáneos, sucesores españoles y europeos extrayendo temas, argumentos, motivos y toda suerte de inspiración. Su teatro, polimétrico, rompe con las unidades de acción, lugar, tiempo, y también con la de estilo, mezclando lo trágico con lo cómico. Expuso su peculiar arte dramático en su "Arte nuevo de hacer comedias en este tiempo" (1609). Flexibilizó las normas clasicistas del aristotelismo para adecuarse a su tiempo y abrió con ello las puertas a la renovación del arte dramático. También creó el molde de la llamada comedia de capa y espada. En comedia palatina, fue el autor que más recurrió a la ambientación en el Reino de Hungría, recurso que se convertiría en frecuente en la literatura de la época. El ciclo de de Lope consta de alrededor de veinte obras.

Junto a él, destacan sus discípulos Guillén de Castro, que prescinde del personaje cómico del gracioso y elabora grandes dramas caballerescos sobre el honor junto a comedias de infelicidad conyugal o tragedias en las que se trata el tiranicidio; Juan Ruiz de Alarcón, que aportó su gran sentido ético de crítica de los defectos sociales y una gran maestría en la caracterización de los personajes; Luis Vélez de Guevara, al que se le daban muy bien los grandes dramas históricos y de honor; Antonio Mira de Amescua, muy culto y fecundo en ideas filosóficas, y Tirso de Molina, maestro en el arte de complicar diabólicamente la trama y crear caracteres como el de Don Juan en "El burlador de Sevilla y convidado de piedra".

Pueden citarse como obras maestras representativas del teatro áureo español la "Numancia" de Miguel de Cervantes, un sobrio drama heroico nacional; de Lope, "El caballero de Olmedo", drama poético al borde mismo de lo fantástico y lleno de resonancias celestinescas; "Peribáñez y el Comendador de Ocaña", antecedente del drama rural español; "El perro del hortelano", deliciosa comedia donde una mujer noble juguetea con las intenciones amorosas de su plebeyo secretario, "La dama boba", donde el amor perfecciona a los seres que martiriza, y "Fuenteovejuna", drama de honor colectivo, entre otras muchas piezas donde siempre hay alguna escena genial.

"Las mocedades del Cid" de Guillén de Castro, inspiración para el famoso «conflicto cornelliano» de "Le Cid" de Pierre Corneille; "Reinar después de morir" de Luis Vélez de Guevara, sobre el tema de Inés de Castro, que pasó con esta obra al drama europeo; "La verdad sospechosa" y "Las paredes oyen", de Juan Ruiz de Alarcón, que atacan los vicios de la hipocresía y la maledicencia y sirvieron de inspiración para Molière y otros comediógrafos franceses; "El esclavo del demonio" de Antonio Mira de Amescua, sobre el tema de Fausto; "La prudencia en la mujer", que explora el tema de la traición reiterada y donde aparece el recio carácter de la reina regente María de Molina, y "El burlador de Sevilla", de Tirso de Molina, sobre el tema del donjuán y la leyenda del convidado de piedra.

El otro gran dramaturgo áureo en crear una escuela propia fue Calderón de la Barca; sus personajes son fríos razonadores y con frecuencia obsesivos; su versificación reduce conscientemente el repertorio métrico de Lope de Vega y también el número de escenas, porque las estructuras dramáticas están más cuidadas y tienden a la síntesis; se preocupa también más que Lope por los elementos escenográficos y refunde comedias anteriores, corrigiendo, suprimiendo, añadiendo y perfeccionando; es un maestro en el arte del razonamiento silogístico y utiliza un lenguaje abstracto, retórico y elaborado que sin embargo supone una vulgarización comprensible del culteranismo; destaca en especial en el auto sacramental, género alegórico que se avenía con sus cualidades y llevó a su perfección, y también en la comedia.
De Calderón destacan obras maestras como "La vida es sueño", sobre los temas del libre albedrío y el destino; "El príncipe constante", donde aparece una concepción existencial de la vida; las dos partes de "La hija del aire", la gran tragedia de la ambición en la persona de la reina Semíramis; los grandes dramas de honor sobre personajes enloquecidos por los celos, como "El mayor monstruo del mundo", "El médico de su honra" o "El pintor de su deshonra". De entre sus comedias destacan "La dama duende", y cultivó asimismo dramas mitológicos como "Céfalo y Procris", de los que él mismo sacó la comedia burlesca del mismo título; también, autos sacramentales como "El gran teatro del mundo" o "El gran mercado del mundo" que sugestionaron la imaginación de los románticos ingleses y alemanes.

Tuvo por discípulos e imitadores de estas cualidades a una serie de autores que refundieron obras anteriores de Lope o sus discípulos puliéndolas y perfeccionándolas: Agustín Moreto, maestro del diálogo y la comicidad cortesana; Francisco de Rojas Zorrilla, tan dotado para la tragedia como para la comedia; Antonio de Solís, también historiador y propietario de una prosa que ya es neoclásica, o Francisco Bances Candamo, teorizador sobre el drama, entre otros no menos importantes.

Entre sus discípulos tenemos las comedias clásicas de Agustín Moreto, como la comedia palatina "El desdén, con el desdén", la de figurón "El lindo don Diego" y el drama religioso "San Franco de Sena", que remite a "El condenado por desconfiado" de Tirso de Molina; Francisco de Rojas Zorrilla con la comedia de figurón "Entre bobos anda el juego", el drama de honor "Del rey abajo ninguno" y la deliciosa y moderna comedia "Abre el ojo". De Antonio de Solís, "El amor al uso" y "Un bobo hace ciento"; de Francisco Bances Candamo, las tragedias políticas "El esclavo en grillos de oro" y "La piedra filosofal".

Otro género teatral importante, y a veces descuidado por la crítica, es el entremés, donde mejor y con más objetividad puede estudiarse la sociedad española durante el Siglo de Oro. Se trata de una pieza cómica en un acto, escrita en prosa o verso, que se intercalaba entre la primera y la segunda jornada de las comedias. Corresponde a la farsa europea, y en él destacaron autores como Luis Quiñones de Benavente y Miguel de Cervantes, entre otros.

La prosa en el Siglo de Oro ostenta géneros y autores que han pasado a la historia de la literatura universal. La conquista de América dio lugar al género de las "Crónicas", entre las que podemos encontrar algunas obras maestras, como las de Bartolomé de las Casas, el Inca Garcilaso de la Vega, Bernal Díaz del Castillo, Antonio de Herrera y Tordesillas y Antonio de Solís. También son espléndidas algunas autobiografías de soldados, como las de Alonso de Contreras o Diego Duque de Estrada. La primera obra maestra fue sin duda "La Celestina", pieza teatral irrepresentable y originalísima obra de un desconocido autor y de Fernando de Rojas, que, junto a sus continuaciones por parte de otros autores (el llamado género celestinesco) o sus imitaciones libres (entre ellas la portentosa "La Lozana andaluza" (1528), obra maestra de Francisco Delicado) marcó para siempre el Realismo en una parte esencial de la literatura española, cuya riqueza abona también ficciones caballerescas tan maravillosas y fantásticas como los libros de caballerías, menos leídos en la actualidad de lo que merecen, habida cuenta de que figuran entre sus piezas más destacadas novelas como "Tirante el Blanco", escrita en valenciano, "Amadís de Gaula" o el "Palmerín de Inglaterra"; un autor característico del género fue Feliciano de Silva.

La novela sentimental se abre y se cierra en medio siglo con dos obras maestras: "Cárcel de amor" (1492) de Diego de San Pedro y "Proceso de cartas de amores" (1548), una novela epistolar de Juan de Segura (1548). Junto a estas hay que hablar también de otras dos obras maestras del género de la novela morisca: la "Historia del Abencerraje y de la hermosa Jarifa (1565) y "Ozmín y Daraja" de Mateo Alemán (1599).

La novela picaresca tiene entre sus máximas creaciones, obras maestras como el anónimo "Lazarillo de Tormes" (1554), una sátira anticlerical y descarnada de las ínfulas de nobleza y el sentido de la honra de la clase alta; "Vida del pícaro Guzmán de Alfarache" (1599 y 1604) de Mateo Alemán, pesimista reflexión sobre el destino humano; la "Vida del escudero Marcos de Obregón" (1618) de Vicente Espinel, llena por el contrario de alegría de la vida; "La vida del Buscón" (1604-1620) de Francisco de Quevedo, una obra maestra del humor y del lenguaje conceptista, la anticlerical "Segunda parte de la vida de Lazarillo" (1620) del protestante Juan de Luna, y la obra de enigmática autoría Estebanillo González (1646), que ofrece una visión espléndida de la decadencia de España en el escenario europeo, y de la Guerra de los Treinta Años. La novela cortesana suministró las obras maestras que constituyen las "Novelas ejemplares" (1613) de Miguel de Cervantes, cada una en sí misma un experimento narrativo; su inmortal "Don Quijote de la Mancha" (1605 y 1615), de la que habría que escribir capítulo aparte a causa de la riqueza de los contenidos y cuestiones que plantea, que viene a ser la primera novela polifónica de la literatura europea. La novela pastoril cuenta con obras maestras como las "Dianas" de Jorge de Montemayor (1559 y 1604) y de Gaspar Gil Polo (1564), "La constante Amarilis" (1607) de Cristóbal Suárez de Figueroa o "Siglo de Oro en las selvas de Erifile" (1608) de Bernardo de Balbuena. La novela bizantina cuenta con ejemplos como "El peregrino en su patria" (1634) de Lope de Vega, quien realiza la hazaña de incluir todas sus aventuras en la Península, el "Persiles" (1617) de Cervantes o el "León prodigioso" (1634) de Cosme Gómez Tejada de los Reyes.

Novela filosófica emparentada con este género es el "Criticón" (1651, 1653 y 1657), de Baltasar Gracián, alegoría de la vida humana. La prosa doctrinal, en ciernes ensayística, tiene por autores modélicos a Pero Mexía, Luis Zapata, Antonio de Guevara ("Epístolas familiares", 1539, "Relox de príncipes", 1539), Luis de León ("De los nombres de Cristo"), San Juan de la Cruz ("Comentarios" al "Cántico espiritual" y otros poemas), Francisco de Quevedo ("Marco Bruto" y "Providencia de Dios") y Diego Saavedra Fajardo ("República literaria" y "Corona gótica").

Jean Rotrou (1609-1650) y Paul Scarron (1610-1660) alcanzaron grandes éxitos traduciendo o imitando a los autores españoles, y estos influyeron en los mayores dramaturgos galos, como por ejemplo Pierre Corneille y Molière, por no mencionar otros de menor importancia, como Thomas Corneille, Alain René Lesage, John Vanbrugh etc. Las obras de teatro españolas extendieron su influjo al ser traducidas, por ejemplo, en Holanda (por Theodore Rodenburg) e Inglaterra (John Webster, Fletcher, Dryden, etc.).

La filosofía del Siglo de Oro español abarca todo el pensamiento que va desde el primer humanismo hasta la instauración del racionalismo en el . A pesar de que en España convivían tres religiones (el judaísmo, el cristianismo y el islam), es cierto que se desarrolló una filosofía que llegaría a culminar en el período Barroco. De este modo, la filosofía del Siglo de Oro se divide en dos apartados: la del Renacimiento y la del Barroco.

Durante el Renacimiento encontramos al primer gran humanista de España, Antonio de Nebrija, con su gramática española. Nebrija consiguió crear las primeras reglas de la lengua que luego tanta difusión tendrían con la posterior fundación de la Real Academia Española (1713). Por otra parte, el gran mecenas durante el humanismo fue el cardenal Francisco Jiménez de Cisneros, quien puso su empeño en reformar las costumbres clericales. En 1499 fundó la Universidad de Alcalá de Henares, que superó en prestigio e influencia a todas las demás excepto la de Salamanca, su mayor rival.

Carlos I defendió las nuevas teorías de Erasmo de Róterdam y la nueva corriente humanista. Fiel seguidor del erasmismo fue Juan Luis Vives. Se convirtió en un reformador de la educación europea y en un filósofo moralista de talla universal, proponiendo el estudio de las obras de Aristóteles en su lengua original y adaptando sus libros destinados al estudio del latín a los estudiantes; substituyó los textos medievales por otros nuevos, con un vocabulario adaptado a su época y al modo de hablar del momento e hizo los primeros aportes a una ciencia en germen, la psicología.

Los nuevos descubrimientos en el Nuevo Mundo y la colonización española de las Indias llevaron a hacer reflexionar a algunos pensadores sobre el trato que los indígenas merecían. Las controversias fue suscitada por el dominico fray Bartolomé de las Casas en su "Brevísima relación de la destrucción de las Indias", donde describía con tintes horrorosos la colonización española de América y defendía el iusnaturalismo. El contenido del escrito hizo convocar una disputa entre 1550 y 1551 en Valladolid contra su principal contrincante, Juan Ginés de Sepúlveda, que defendía el consuetudinarismo, la bondad de la colonización española y el derecho de guerra. Esta disputa llegó a llamarse la «Junta de Valladolid».
La Universidad de Salamanca contribuyó al pensamiento político, económico y moral. El resurgimiento del nuevo espíritu se ve encarnado en la principal figura con Francisco de Vitoria, teólogo dominico, profesor de Salamanca, que rechazó toda argumentación basada en puras consideraciones metafísicas por estar a favor del estudio de los problemas reales que planteaba la vida política y social contemporánea. Fue el primero en establecer los conceptos básicos del derecho internacional moderno, basándose en la regla del derecho natural. Afirmaba así las libertades fundamentales como la palabra, de comunicación, comercio y tránsito por los mares, siempre que las naciones y razas no se perjudicaran mutuamente.

El cristianismo en España dio sus propios pensadores y teólogos, la mayoría ortodoxos mediante la Contrarreforma, pero también heterodoxos en una Reforma que sólo pudo cuajar en el extranjero. En cuanto a los ortodoxos, destaca san Ignacio de Loyola, que escribió sus "Ejercicios espirituales" y fundó la Compañía de Jesús, con la que se quería llegar a la unidad religiosa y que con su red de colegios renovó la enseñanza de las lenguas clásicas. En poesía se desarrollaron movimientos de ascética y mística muy profundos y personales. La lírica del Renacimiento se caracteriza por tener a un grupo de religiosos que transmitían su filosofía mediante la poesía. Cabe destacar a san Juan de la Cruz, santa Teresa de Jesús y a F como figuras eminentes entre un gran conjunto de figuras importantes.

La llegada del Barroco cambió por completo la mentalidad renacentista del humanismo. La visión de la vida se volvió pesimista y todas las perspectivas desembocaron en el desengaño. La prosa filosófica brilla con Luis de Molina, iluminado establecido en Roma. Su doctrina apodada molinismo tuvo una gran repercusión e influencia en los pensadores y escritores barrocos posteriores a él. Su pensamiento mezcla los principios de la religión con una elaborada filosofía moral. Molina combatió el determinismo con el libre albedrío. Sus obras acerca de la libertad fueron muy seguidas por los pensadores del siglo posterior.

El filósofo y médico Gómez Pereira rechaza los conceptos medievales para defender los métodos empíricos en que se basaría la ciencia de los dos siglos posteriores. Se le considera, junto con el escéptico Francisco Sánchez, uno de los precursores de René Descartes e influyó en sus trabajos posteriores, siendo el primero en sugerir el automatismo de las bestias, la teoría del conocimiento humano y la inmortalidad del alma.

La Universidad de Salamanca también aportó bastante al pensamiento del Barroco temprano. Melchor Cano escribió "De Locis Theologicis", obra en la que estableció las diez fuentes para la demostración teológica: la Sagrada Escritura, la tradición apostólica, la autoridad de la Iglesia católica, la autoridad de los concilios ecuménicos, la autoridad del Sumo Pontífice, la doctrina de los Padres de la Iglesia, la doctrina de los doctores escolásticos y canonistas, la verdad racional humana, la doctrina de los filósofos y la historia.

En la transición del Renacimiento al Barroco se encuentra Francisco Suárez, hombre de gran cultura y sabio en los aspectos clásicos. Continuó con la doctrina tomista. En su gran obra jurídica "De legibus ac Deo legislatore", muy fecunda para la doctrina del iusnaturalismo y el derecho internacional, se encuentra ya la idea del pacto social.

Con la antropología se hicieron grandes avances. La principal figura fue José de Acosta, que adelantó tres siglos la teoría de la evolución darwiniana.

En las artes plásticas destaca la pintura. A una primera fase corresponden Pedro Berruguete, Pedro Machuca, Luis de Morales, los leonardescos Juan de Juanes y Fernando Yáñez de la Almedina. A la segunda, Juan Fernández de Navarrete, Alonso Sánchez Coello y El Greco, principal exponente del manierismo pictórico en Castilla.

Al Barroco pertenecen Diego Velázquez, pintor de complejas composiciones intelectualizadas que ahonda en el misterio de la cruda e intensa luz y la perspectiva aérea; los tenebristas caravaggiescos Francisco de Zurbarán (gran pintor de frailes y bodegones), Francisco Ribalta y José de Ribera; en Sevilla cabe salientar a Francisco Herrera el Viejo y Francisco Herrera el Mozo, Bartolomé Esteban Murillo y Juan de Valdés Leal; mientras que en Córdoba destaca Antonio del Castillo.

Hay que citar también a Juan Bautista Maíno (pintor de alegorías políticas) Claudio Coello, Juan Carreño de Miranda, el florentino Vicente Carducho, el retratista Juan Pantoja de la Cruz, Luis Tristán (uno de los escasos discípulos del Greco, que añade al estilo del maestro elementos naturalistas), Juan Bautista Martínez del Mazo, Pedro Orrente, Bartolomé González y Serrano, el cartujo Juan Sánchez Cotán(famoso por sus místicos bodegones), Eugenio Cajés, Antonio Pereda; Mateo Cerezo, el paisajista Francisco Collantes, Juan Antonio Frías y Escalante, José Antolínez, el aragonés Jusepe Martínez y otros muchos.

En lo tocante a escultura tenemos ya en el Prerrenacimiento y primeros años del las figuras extranjeras que trabajaron en España: Domenico Fancelli, Pietro Torrigiano y Jacopo Florentino. La primera generación de escultores españoles del Renacimiento en Castilla estuvo compuesta por Vasco de la Zarza (trascoro de la catedral de Ávila), Felipe Vigarny (retablo mayor de la catedral de Toledo), Bartolomé Ordóñez (sillería del coro de la catedral de Barcelona) y Diego de Siloé (sepulcro de don Alonso de Fonseca y Acevedo en el Convento de las Úrsulas de Salamanca); en la Corona de Aragón destaca el trabajo de Damián Forment (retablo mayor de la Basílica del Pilar, 1509 y del monasterio de Poblet, 1527), Gil Morlanes el Viejo (portada de la iglesia de Santa Engracia de Zaragoza) y Gabriel Yoly, que talló en madera sin policromar el retablo mayor de la catedral de Teruel en 1536.

En el manierismo hay que nombrar por supuesto el correlato de la ascética y la mística de la segunda mitad del . El gran Alonso Berruguete, el gallego Gregorio Fernández , los escultores clasicistas italianos Leone Leoni y su hijo Pompeyo Leoni (que trabajaron para el Real Monasterio de San Lorenzo de El Escorial); los barrocos Francisco del Rincón y Pedro Vicálvaro, de la Escuela castellana, y Juan de Juni; de la Escuela andaluza Jerónimo Hernández, Andrés de Ocampo, Juan Martínez Montañés, Juan de Mesa, Francisco de Ocampo y Felguera, Alonso Cano. En el pleno Barroco desembocaron ya con escultores como Pedro de Mena, Pedro Roldán, su hija Luisa Roldán y su nieto Pedro Duque y Cornejo; Francisco Ruiz Gijón, José Risueño, Bernardo de Mora o su hijo José de Mora. De Guipúzcoa procedía Juan de Ancheta, de estilo clasicista romano, cuya obra se desarrolló fundamentalmente en Navarra, La Rioja y Aragón. La temática tratada es casi exclusivamente religiosa y sólo en el ámbito de la Corte se da escultura monumental; los temas mitológicos y profanos están ausentes. Se realizan retablos, donde aparecen figuras exentas y en bajorrelieve. Destaca con mucho la imaginería en madera de tradición hispana. En estas obras se pierde la técnica del estofado y posteriormente se usará la policromía. Las figuras son aisladas: para iglesias, conventos y para las procesiones de Semana Santa.

También para la música española fue este el siglo de oro. La labor de compositores cortesanos, que unían su labor de músico a la de dramaturgo y poeta, tiene un buen ejemplo en Juan del Encina en el y ; o en el Juan Hidalgo, que musicó las zarzuelas de Calderón de la Barca, como también hará Tomás de Torrejón y Velasco. En tiempos de Carlos V componen Mateo Flecha el Viejo, autor de "Las" "Ensaladas" (Praga, 1581), género que mezcla versos en diversas lenguas. Cristóbal de Morales estudió en Roma, donde publicó algunas misas en 1544. Otros músicos fueron Pedro de Pastrana, Juan Vázquez o Diego Ortiz.

A la época de Felipe II corresponden Francisco Gabriel Gálvez, Andrés de Torrentes, Juan Navarro o Rodrigo de Ceballos. En Sevilla trabajó Francisco Guerrero, que viajó a Italia y publicó su obra entre 1555 y 1589.

Pero más importante aún fue la labor de compositores o, como eran llamados a la sazón, "maestros de capilla" y organistas que, partiendo del motete y el madrigal italiano de Giovanni Pierluigi da Palestrina, desarrollaron una gran polifonía al servicio sobre todo de los oficios religiosos, con una gran carga emotiva que la distinguió de las otras tres grandes escuelas polifónicas de los siglos XV al XVII como la Escuela flamenca, la veneciana y la romana, y que se ha vinculado con el apasionamiento místico de escritores como Teresa de Ávila o Juan de la Cruz. Destacan las figuras ya mencionadas de Cristóbal de Morales, Francisco Guerrero, y otras anteriores como Francisco de Peñalosa, maestro de Morales, y posteriores, como Alonso Lobo pero sobre todo la del gran Tomás Luis de Victoria, majestuosa, inspirada y mística. Se ha comparado en su profundidad y emoción ascética a la pintura de el Greco, y hoy, gracias a la labor de estudiosos y difusores de su música como Jordi Savall, es reconocido como uno de los más grandes compositores de todos los tiempos. En Roma, que fue donde trabajó principalmente, publicó unas 170 obras —65 motetes, 34 misas, 37 oficios de Semana Santa, "Magnificat" y "Salmos"— desde 1572. A partir de 1587 trabaja para la Emperatriz, a cuya muerte compuso un famoso "Officium Defunctorum" (1605) para seis voces. Su policoralismo (composiciones para dos coros) y cuidado de la armonía, en la escritura de bemoles y sostenidos, lo señalan como precursor del Barroco.

Destaca la escuela de vihuela española del . Aparecieron grandes figuras, como Esteban Daza, Luys de Milán (autor de "El Maestro", 1536, que incluye fantasías, pavanas, tientos, villancicos, romances y obras originales en que la vihuela admite el canto), Alonso Mudarra (con sus "Tres libros de música en cifra para vihuela", Sevilla, 1546), Luis de Narváez ("El Delphín", 1538), Enríquez de Valderrábano ("Silva de Sirenas", 1547), Diego Pisador ("Libro de música de vihuela", 1552), Miguel de Fuenllana ("Orphénica Lyra") y Gaspar Sanz, ya en el último cuarto del , quien dio un impulso definitivo a la guitarra con su obra "Instrucción de música sobre la guitarra española".

Por su obra para teclado ganaron fama el burgalés Antonio de Cabezón en el , y Juan Bautista Cabanilles y Francisco Correa de Arauxo, en el . Las obras clásicas al respecto son las "Obras de música para tecla, harpa y vihuela" (1578) de Antonio de Cabezón, preparadas por su hijo, y "El Libro de Cifra Nueva para tecla, harpa y vihuela" (Alcalá de Henares, 1557) de Luis Venegas de Henestrosa: ambas muestran la versatilidad de estas composiciones para adaptarse a instrumentos o a voces humanas.

Todos ellos conformaron un periodo de esplendor para la música española, que, salvo figuras aisladas, no volvió a alcanzar las cotas a las que se llegó en esta época. Sin embargo, gran parte de este patrimonio musical se ha perdido y, por ejemplo, de la obra de Francisco de Salinas, que tanto deleitaba a fray Luis de León, no se ha conservado partitura alguna, sino sólo un tratado teórico.

En el se pasa del estilo plateresco del Renacimiento durante los Reyes Católicos al más plenamente renacentista durante el reinado de Carlos I; después, durante el de su hijo Felipe II, surge el Manierismo de Juan de Herrera, creador del Estilo herreriano y del monumental monasterio de San Lorenzo de El Escorial y de la espectacular y tristemente inacabada catedral de Valladolid, y durante el domina el Barroco y churrigueresco.

En España, el Renacimiento comenzó unido a las formas góticas en las últimas décadas del . El estilo comenzó a extenderse sobre todo a manos de arquitectos locales: es la razón de un estilo renacentista específicamente español, que reunió la influencia de la arquitectura del sur de Italia, a veces proveniente de libros ilustrados y pinturas, con la tradición gótica y la idiosincrasia local. El nuevo estilo se llama plateresco, debido a las fachadas decoradas en exceso, que recuerdan a los intrincados trabajos de los plateros. Órdenes clásicas y motivos de candeleros ("candelieri") se combinan con libertad en conjuntos simétricos.

En este contexto, el Palacio de Carlos V realizado por Pedro Machuca, en Granada, supuso un logro inesperado dentro del Renacimiento más avanzado de la época. El palacio puede ser definido como una anticipación al manierismo, debido a su dominio del lenguaje clásico y sus logros estéticos rupturistas. Fue construido antes de las principales obras de Miguel Ángel y Andrea Palladio. Su influencia fue muy limitada y mal entendida, las formas platerescas se imponían en el panorama general.

Según pasaban las décadas, la influencia gótica desaparece y la búsqueda de un clasicismo ortodoxo alcanzó niveles muy altos. Aunque el plateresco es un término usado habitualmente para definir a la mayoría de la producción arquitectónica de finales del y primera mitad del , algunos arquitectos adquirieron un gusto más sobrio, como Diego de Siloé, Rodrigo Gil de Hontañón y Gaspar de Vega. Ejemplos de plateresco son las fachadas de la Universidad de Salamanca, el Colegio Mayor Santa Cruz de Valladolid y del Hostal San Marcos de León.

La cumbre del Renacimiento español está representado por el Real Monasterio de El Escorial, realizado por Juan Bautista de Toledo y Juan de Herrera, en el que una adherencia excesiva al arte de la antigua Roma fue superado por el estilo extremadamente sobrio. La influencia de los techos flamencos, el simbolismo de la escasa decoración y el preciso corte del granito establecieron la base para un estilo nuevo, el herreriano.

Con un estilo más próximo al manierismo, el siglo se cierra con arquitectos como Andrés de Vandelvira (Catedral de Jaén).

Cuando las influencias barrocas italianas llegaron a España, gradualmente sustituyeron en el gusto popular al sobrio gusto clasicista que había estado de moda desde el . Tan pronto como en 1667, las fachadas de la catedral de Granada de Alonso Cano y la de Jaén de Eufrasio López de Rojas indican la facilidad de su interpretación a la manera barroca de los motivos tradicionales de las catedrales españolas.

El barroco local mantiene raíces en Herrera y en la construcción tradicional en ladrillo, desarrollada en Madrid a lo largo del (Plaza Mayor y Ayuntamiento de Madrid).





</doc>
<doc id="4629" url="https://es.wikipedia.org/wiki?curid=4629" title="Villamartín">
Villamartín

Villamartín es un municipio español de la provincia de Cádiz, Andalucía. Según el INE, en 2016 contaba con 12267 habitantes. Se encuentra al norte de la provincia, a una altitud de 169 metros y a 80 kilómetros de la capital, Cádiz.

Tan cerca de Sevilla (85,3 km) como de la propia capital provincial (85,7 km), su término municipal ocupa una extensión de 210 kilómetros cuadrados. Linda con los términos municipales sevillanos de Utrera y El Coronil, por el norte, y con los términos municipales gaditanos de Espera, Bornos, Arcos de la Frontera, Prado del Rey, Algodonales y Puerto Serrano, en el resto de su contorno, de oeste a este. A una altitud de 167 metros sobre el nivel del mar, su término es la transición de la Campiña a la Sierra.

Su estratégica situación, como nudo articulador de las comunicaciones entre las localidades de la comarca y, por ende, entre las provincias de Cádiz, Sevilla y Málaga, la han ido configurando como el centro de los servicios públicos comarcales.

La principal vía de comunicación es la carretera A-384 Arcos de la Frontera-Antequera, tramo de la antigua carretera nacional CN-342 Jerez-Cartagena. Formará parte, cuando se transforme, de la Autovía A-382 Jerez-Antequera que, actualmente, llega hasta Arcos, En Villamartín, a 22 km de Arcos de la Frontera y a 118 km de Antequera, se produce la distribución de las vías de comunicación con el entorno.

En efecto, las carreteras A-371 Villamartín-Las Cabezas de San Juan (Sevilla) y A-376 Sevilla-Ronda (Málaga) se cruzan aquí y comunican la capital andaluza y el sur de la provincia de Sevilla con la Sierra de Cádiz y el norte de la provincia de Málaga y la A-373 (Villamartín-Gaucín (Málaga) comunica la Sierra con la costa malagueña. Esta situación la convierte en la vía de entrada al parque natural de la Sierra de Grazalema por el noroeste y al parque natural de los Alcornocales, por el norte.

La autopista Jerez-Antequera, ahora en construcción, pasará por la localidad gaditana en los próximos años (proyecto anulado temporalmente).

Villamartín estuvo habitada desde la Prehistoria, como demuestra el Dolmen de Alberite, monumento megalítico situado en la finca homónima. En 1994 se descubrió un Yacimiento Tartésico en la cima de la colina en que se asienta la ciudad. En ella existió un conjunto de villas romanas dedicadas a la agricultura, cuyos restos se conservan aún en sus numerosos cortijos, que, después del breve dominio visigodo, se convirtieron en machares musulmanes.

De la época medieval se conserva el Castillo de Matrera, que se derrumbó en 2013 debido a la falta de mantenimiento. Este castillo guardaba la frontera sur del reino de Sevilla, de los ataques musulmanes procedentes del reino nazarí de Granada. La actual ciudad fue fundada el 4 de febrero de 1503, mediante Carta-Puebla otorgada por la ciudad de Sevilla a un grupo de 118 vecinos procedentes en su mayoría de los pueblos circundantes. Durante 2003 se celebran los actos del Quinto Centenario de su fundación


La economía de la zona se centra en agricultura y ganadería. Se está construyendo el mayor centro avícola del país

Son muy populares las sopas de tomate y de espárragos, protagonistas de las reuniones de familiares y de amigos.

También tiene una buena confitería, en la que destacan dulces como los Roscos Blancos, los Pitisús, los Cuernos de crema, los Palos de Nata y Torpedos, las Cañas y cordobesas, las sultanas de coco o de almendras, las Palmeras, los Piononos y un sinfín de pasteles artesanos que hacen las delicias de cualquier visitante. También se fabrican alfajores y turrones.

Por otro lado Villamartín también cuenta con pan artesano de buena calidad, fabricándose de varios estilos, además son muy apreciados los molletes, pieza imprescindible en los desayunos villamartinenses.

Desde hace unos años se ha consolidado quesería de prestigio internacional.

También posee uno de los mejores quesos de la comarca "Quesos Pajarete" el cual lleva muchísimos premios.

Villamartín cuenta en la actualidad con 4 Colegios públicos, de los cuales tres son de Educación Infantil y Primaria y uno específico de Educación Especial:


También cuenta con 2 institutos: el IES La Loma, situado en la calle Consolación y el IES Castillo de Matrera, en la avenida de Sevilla.

Villamartín cuenta además con varias organizaciones como Asparei-Asadifisa y AFANAS que enriquecen la atención que se ofrece al colectivo de discapacitados/as.

Y un centro de protección de menores (C.P.M. La Cañada), tutorizan a menores de edad sin familias en España

La localidad es sede de la Mancomunidad de municipios de la Sierra de Cádiz, que se ubica en la Alameda de la diputación, y en ella existe un juzgado de paz, situado en la Plaza del Ayuntamiento.

También cuenta con un Hospital Comarcal; el Hospital Virgen de las Montañas, que se ubica junto al IES Castillo de Matrera y a la oficina de empleo.

En la Avenida de la Feria está situado el centro de salud y centro de especialidades de la localidad, al que también acuden personas de localidades vecinas, al igual que al hospital, debido a su cartera de servicios.

También en Villamartín existe una oficina de atención de la Seguridad Social, una sede de ITV, una jefatura de policía local, un parque de bomberos y un cuartel de la guardia civil.

Está previsto que comience a funcionar el geriátrico comarcal instalado junto al hospital de Villamartín.

Villamartín cuenta con unas excelentes instalaciones deportivas, posiblemente las mejores de la Comarca, y la mayoría de ellas situadas en el Polideportivo Municipal, donde podemos encontrar instalaciones de: Baloncesto, Fútbol Sala (ambas contando con diferentes pistas tanto cubiertas como al descubierto), Tenis, Pádel (tanto de hormigón como cristaleras), Ping-Pong, un excelente estadio de fútbol con graderíos cubiertos, pista de atletismo, y césped artificial donde practicar Fútbol 7 o Fútbol 11, una piscina exterior, una piscina cubierta climatizada y un pabellón cubierto con pista polideportiva entre otras muchas instalaciones.

El equipo de fútbol de la localidad es la Unión Deportiva Villamartín. Fue fundado en 1957 con motivo de la unión entre los dos antiguos clubes de "Los Cazadores" y el "Guadalete". También Villamartín cuenta con el equipo A.C.U.DE de Baloncesto, el cual ha cosechado numerosos éxitos a lo largo de su aun reciente nacimiento.

Feria de ganado y fiestas de San Mateo. Es la Feria ganadera más antigua de Andalucía, celebrándose desde el siglo XVI. Del 20 al 24 de septiembre.





</doc>
<doc id="4640" url="https://es.wikipedia.org/wiki?curid=4640" title="Los Guayos">
Los Guayos

Los Guayos es una capital del Municipio Los Guayos del Estado Carabobo en la Región Central de Venezuela, al noroeste del Lago de Valencia. Forma parte del área urbana de la ciudad de Valencia. Tiene una población estimada para el 2011 de 179.568 habitantes.

Los Guayos debe su nombre a una alteración fonética del vocablo indígena "uayos", que significa goma o resina extraída de la corteza del "huayales". 

El pueblo de Los Guayos se halla en la parte norte del municipio homónimo. Actualmente se halla conectado con otras poblaciones que se han expandido con especial celeridad en las últimas décadas. Al Norte-Noreste de Los Guayos pasa la Autopista Caracas-Valencia. El río de Los Guayos fluye del Noreste y Este hacia el Sureste.

El clima es cálido como también intenso de 28·C a 30·C en la temporada de verano.

Los científicos no han llegado a una conclusión sobre el idioma y la clasificación de las etnias que vivían alrededor del Lago de Valencia y, por extensión, en la zona de Los Guayos. Por algunas narrativas como las de Oviedo y Baños y las de Pimentel se puede suponer que en general las etnias de la zona eran de la familia lingüística Caribe.
El primer encomendero conocido de los indios Guayo fue el capitán Gaspar Matute Villalobos, vecino de la Nueva Valencia del Rey. Al morir este y quedar vacante su encomienda se le otorga en 1642 a Domingo Vásques de Rojas y Alfaro, como hijo homónimo del maestre de campo Domingo Vásquez, mencionándose entonces a los indios Guayos como de nación Jiraharas, en cantidad de 'sesenta indios útiles' (no cuentan ni viejos ni infantes) y poblados en el pueblo de San Antonio de los Guayos ya para esa fecha.
El 20 de febrero de 1694 don Francisco Berroterán, gobernador de la Provincia de Venezuela, llama a Los Guayos "pueblo de indios". El territorio era parte de la tribu indígena de los Guayos.

El 6 de junio de 1710 "Los Guayos" se convierte en ayuda de parroquia o parroquia sufragánea de Guacara.

En 1751 los vecinos de Los Guayos se suman a otras poblaciones venezolanas en el levantamiento de Francisco de León contra la imposición de la Compañía Guipuzcoana.
En 1785 el gobernador Manuel Torres de Navarra declara a los Guayos como parroquia.
En mayo de 1812 Francisco de Miranda deja un ejército en el pueblo de Los Guayos para repeler a las fuerzas realistas mientras él continúa la guerra en otras partes de la región. Este grupo es desbaratado el 8 de mayo cuando uno de sus comandantes, un español, se pasa al bando de las fuerzas monárquicas.
En 1837 concluye la reconstrucción de la iglesia.



Templo colonial de San Antonio de Padua o iglesia de Los Guayos: La iglesia es una de las más antiguas de Venezuela. Su primera construcción data de 1650, cuando servía de iglesia para el pueblo de indios de la zona. El campanario actual, con dos cuerpos anexos, data de 1779.





</doc>
<doc id="4643" url="https://es.wikipedia.org/wiki?curid=4643" title="PH">
PH

El pH es una medida de acidez o alcalinidad de una disolución. El pH indica la concentración de iones de hidrógeno presentes en determinadas disoluciones. La sigla significa potencial de hidrógeno o potencial de hidrogeniones. El significado exacto de la p en «pH» no está claro, pero, de acuerdo con la Fundación Carlsberg, significa «poder de hidrógeno». Otra explicación es que la p representa los términos latinos "pondus hydrogenii" («cantidad de hidrógeno») o "potentia hydrogenii" («capacidad de hidrógeno»). También se sugiere que Sørensen usó las letras p y q (letras comúnmente emparejadas en matemáticas) simplemente para etiquetar la solución de prueba (p) y la solución de referencia (q). Actualmente en química, la p significa «cologaritmo decimal de» y también se usa en el término p"K", que se usa para las constantes de disociación ácida.

Este término fue acuñado por el bioquímico danés S. P. L. Sørensen (1868-1939), quien lo definió en 1909 como el opuesto del logaritmo de base 10 o el logaritmo negativo de la actividad de los iones de hidrógeno. Esto es:

Esta expresión es útil para disoluciones que no tienen comportamientos ideales, disoluciones no diluidas. En vez de utilizar la concentración de iones hidrógeno, se emplea la actividad formula_1, que representa la concentración efectiva.

El término pH se ha utilizado universalmente por lo práctico que resulta para evitar el manejo de cifras largas y complejas. En disoluciones diluidas, en lugar de utilizar la actividad del ion hidrógeno, se le puede aproximar empleando la concentración molar del ion hidrógeno.

Por ejemplo, una concentración de , lo que equivale a: 0.0000001 M y que finalmente es un pH de 7, ya que .

En disolución acuosa, la escala de pH varía, típicamente, de 0 a 14. Son ácidas las disoluciones con pH menores que 7 (el valor del exponente de la concentración es mayor, porque hay más iones hidrógeno en la disolución). Por otro lado, las disoluciones alcalinas tienen un pH superior a 7. La disolución se considera neutra cuando su pH es igual a 7, por ejemplo el agua.

El pH se define como el logaritmo negativo de base 10 de la actividad de los iones hidrógeno:

Se considera que "p" es un operador logarítmico sobre la concentración de una disolución p = –log[...]. También se define el pOH, como el logaritmo negativo de la concentración de iones hidróxido.

Puesto que el agua está adulterada en una pequeña extensión en iones OH y HO, se tiene:

formula_4
Donde:

Por lo tanto,

Por lo que se pueden relacionar directamente los valores del pH y del pOH.

En disoluciones no acuosas o fuera de condiciones normales de presión y temperatura, un pH de 7 puede no ser el neutro. El pH al cual la disolución es neutra está relacionado con la constante de disociación del disolvente en el que se trabaje.

El valor del pH se puede medir de forma precisa mediante un potenciómetro, también conocido como pH-metro (/pe achímetro/ o /pe ache metro/), un instrumento que mide la diferencia de potencial entre dos electrodos: un electrodo de referencia (generalmente de plata/cloruro de plata) y un electrodo de vidrio que es sensible al ion de hidrógeno.

El pH de una disolución se puede medir también de manera aproximada empleando "indicadores"":" ácidos o bases débiles que presentan diferente color según el pH. Generalmente se emplea un papel indicador, que consiste en papel impregnado con una mezcla de indicadores cualitativos para la determinación del pH. El indicador más conocido es el papel de litmus o papel tornasol. Otros indicadores usuales son la fenolftaleína y el naranja de metilo.


La determinación del pH es uno de los procedimientos analíticos más importantes y más utilizados en química y bioquímica. El pH determina muchas características notables de la estructura y de la actividad de las moléculas, por lo tanto, del comportamiento de células y organismos.

El pH que es medido en el laboratorio, generalmente no es el mismo que el calculado mediante la ecuación: formula_12, porque el valor numérico de la concentración de iones hidrógeno, no es igual al valor de su actividad, excepto, para las disoluciones diluidas.

Diversas reacciones químicas que se generan en disolución acuosa necesitan que el pH del sistema se mantenga constante, para evitar que ocurran otras reacciones no deseadas. Las disoluciones reguladoras, amortiguadoras o búfer, son capaces de mantener la acidez o basicidad de un sistema dentro de un intervalo reducido de pH.

En 1917 Hasselbalch propuso la ecuación pertinente para calcular el pH de disoluciones amortiguadoras. La ecuación que postuló es la siguiente: 

formula_13

Adicionalmente se debe establecer la concentración total del par conjugado, formula_14 para fijar un valor de pH determinado.

Estas disoluciones contienen como especies predominantes, un par ácido/base conjugado en concentraciones apreciables. La capacidad reguladora que posea la disolución depende de la cantidad presente del ácido débil y su base débil conjugada, mientras mayor sea esta cantidad, mayor será la efectividad de dicha disolución. El que sean ácidos y bases débiles significa que actúan como electrólitos débiles, en pocas palabras, no se ionizan por completo en agua. La reacción de neutralización es una reacción entre un ácido y una base. Generalmente en las reacciones acuosas ácido-base se generan agua y una sal.

El organismo dispone de tres recursos para mantener el pH en valores compatibles con la vida:


Las variaciones de pH en nuestro organismo pueden modificar ciertos procesos fisiológicos, tal es el caso de la reacción enzimática. Cada enzima de nuestro cuerpo tiene un intervalo de pH, que comúnmente se le conoce como "pH óptimo", en el cual la enzima desarrolla su máxima actividad. Si esta se encuentra en condiciones fuera del pH óptimo, puede reducir su velocidad de activación, modificar su estructura, o lo que es peor, dejar de funcionar.

Algo más cotidiano para nosotros son las inyecciones. Los fluidos que se emplean para preparar específicamente las inyecciones intravenosas, incluyen un sistema amortiguador para que la sangre mantenga su pH. Con todo esto se refleja la importancia de las disoluciones amortiguadoras, ya que sin estas, todas las reacciones químicas de los organismos, no podrían realizarse de manera eficaz.

Para obtener un indicador orgánico se puede utilizar col morada siguiendo estos pasos:


Si los resultados son los siguientes se extrajo el indicador con éxito:

El pH se relaciona mucho con la calidad del agua en las piscinas. Esto es así porque el cloro solo hace efecto si el pH del agua de la piscina está entre 6.5 y 8. Si el pH del agua es superior a 8 o inferior a 6.5, por más cloro que se añada este no actuará. Por ello es importante vigilar que el pH esté siempre entre 6.5 y 8. Esta previsión es clave para asegurar que la piscina permanezca en buen estado. Un pH de agua demasiado elevado (superior a 8) produce agua turbia, incrustaciones e irritación de ojos, orejas, nariz y garganta.




</doc>
<doc id="4644" url="https://es.wikipedia.org/wiki?curid=4644" title="Elemento">
Elemento

El término elemento puede referirse a:






</doc>
<doc id="4646" url="https://es.wikipedia.org/wiki?curid=4646" title="12 de enero">
12 de enero

El 12 de enero es el 12.º (duodécimo) día del año en el calendario gregoriano. Quedan 353 días para finalizar el año y 354 en los años bisiestos.








</doc>
<doc id="4647" url="https://es.wikipedia.org/wiki?curid=4647" title="13 de enero">
13 de enero

El 13 de enero es el 13.º (decimotercer) día del año en el calendario gregoriano. Quedan 352 días para finalizar el año y 353 en los años bisiestos.







</doc>
<doc id="4648" url="https://es.wikipedia.org/wiki?curid=4648" title="19 de enero">
19 de enero

El 19 de enero es el decimonoveno día del año del calendario gregoriano. Quedan 346 días para finalizar el año y 347 en los años bisiestos.








</doc>
<doc id="4649" url="https://es.wikipedia.org/wiki?curid=4649" title="20 de enero">
20 de enero

El 20 de enero es el vigésimo día del año en el calendario gregoriano. Quedan 345 días para finalizar el año y 346 en los años bisiestos.








</doc>
<doc id="4654" url="https://es.wikipedia.org/wiki?curid=4654" title="Oda (desambiguación)">
Oda (desambiguación)

Oda puede referirse a:

</doc>
<doc id="4658" url="https://es.wikipedia.org/wiki?curid=4658" title="Tragicomedia">
Tragicomedia

Una tragicomedia es una obra dramática en la que se mezclan los elementos trágicos y cómicos, aunque también hay lugar para el sarcasmo y parodia.

Una "pieza" es una obra literaria del tipo realista, en donde la situación y los personajes están claramente presentados. Si bien en lo que se relata y expone hay cambios inesperados, ellos son lógicos y explicables, y el suspenso va en continuo aumento, llegando por momentos a clímax con intensas emociones. La resolución o conclusión de la obra es consecuencia de los actos y de las situaciones planteadas con bastante claridad y sin ambigüedades, donde los actos de los personajes son verosímiles.

En la Grecia clásica, el drama satírico o la tragicomedia suele tratar un tema legendario, aunque con efectos cómicos protagonizados, fundamentalmente, por el coro. Los dioses no intervienen en la muerte de los hombres y puede haber más de una acción al mismo tiempo.

La tragicomedia principalmente va a mostrar la trayectoria del héroe tragicómico, que tiene un objetivo que perseguir (el amor, la justicia, la ambición, un trono, entre otras) y de cómo este lo consigue o no pasando por una serie de obstáculos para llegar a su fin. Si los obstáculos se presentan como positivos, es decir que parece que lo acercan cada vez más a su objetivo, más que obstáculos son como pruebas superadas. Por su parte el final será negativo, si los obstáculos son negativos, y parece que le impiden llegar a su objetivo, aunque por lo general el final será positivo y aunque sea a último minuto alcanzará su objetivo.

Hay antecedentes de la tragicomedia en el teatro renacentista italiano, por ejemplo en Ángelo Beolco (llamado “Ruzzante”). Tal tradición fue recogida por los renacentistas italianos de los teatros clásicos griego y romano. En España, el mayor exponente de la tragicomedia en el teatro fue Lope de Vega, rompiendo las estructuras del teatro aristotélico. Mientras que El Quijote, de Cervantes, es seguramente el mayor ejemplo de tragicomedia en el ámbito novelístico.

Aristóteles (384 a.C. - 322 a.C.), en el primer capítulo de "Poética" (1,6), hace una aproximación entre tragedia y comedia, mostrando que ambas se sirven de los mismos medios - mismos ritmos, mismos cantos y metros. Pero es probable que Plauto (254 a.C.-184 a.C.) haya sido el primero en emplear la palabra "tragicomedia", definiéndola como un género híbrido de comedia y tragedia, conforme explica a través del personaje Mercurio, en el prólogo de su pieza "Amphitryon o Amphitruo:"

Esas mezclas o alternancias de estilo ocurren en varias piezas griegas y romanas, como en "Alceste" de Eurípides (c. 485 a.C.-406 a.C.), que, en razón de su "final feliz", por el tono levemente humorístico de algunos pasajes, es vista por algunos eruditos como un drama satírico o una tragicomedia, mucho más que como una tragedia.

En Francia, el término fue introducido por el dramaturgo Robert Garnier (1545-1590).

En el inicio del siglo XVII, este tipo de teatro estaba de moda, más el estilo aún no esta claramente definido. Poco a poco, los autores fue sometiendo sus obras a las reglas del teatro clásico. Y entre los clásicos franceses del siglo XVII (Molière, Pierre Corneille, Jean Racine), el término en cuestión designaba una historia trágica con desenlace feliz.

El género no siempre agradó al público. ""El Cid"" de Corneille, por ejemplo, tuvo que ser reescrito para transformarlo en una tragedia, después que la primera versión recibió numerosas críticas desfavorables. No obstante, este caso fue algo peculiar, pues pudo haber estado contaminado por el cabale promovido por el cardenal Richelieu.

Un caso que también corresponde citar, es el de Victor Hugo, que con su drama romántico intentó imponer una escritura que se situaba entre lo "sublime" y lo "grotesco", pero que no tuvo mucho éxito. Solamente en el siglo XX, con el "Teatro del absurdo", el público comenzaría a aceptar que las risas no necesariamente excluyen la profundidad dramática.


</doc>
<doc id="4660" url="https://es.wikipedia.org/wiki?curid=4660" title="Reggae">
Reggae

El reggae (, /'rege/) es un género musical originado en Jamaica en los años 60. Esta se suele dividir en Ska (1960-1966), Rocksteady (1966-1968), Reggae (1969-1983) y Dancehall (desde mediados de los años 80 en adelante, aunque pueden considerarse su inicio a finales de los años 70 como un proceso gradual en el que los deejays ganaron popularidad a los cantantes tradicionales). 

En sentido estricto el Reggae es la música desarrollada entre 1969 y 1983, un período de mayor diversidad musical que las anteriores en la que el bajo eléctrico asumió un papel más central y conforme fue pasando el tiempo del período aumentó la influencia del movimiento Rasta en las letras y el sonido. 

La edición de 1977 del "Diccionario de inglés jamaiquino" incluía "reggae" como "una expresión recientemente establecida para "rege"", equivalente a "rege-rege", una palabra que podía significar tanto "trapos, ropa andrajosa" como "una pelea o riña".
El término "reggae" con un sentido musical apareció tanto en Desmond Dekker como en el hit "rocksteady" de 1968 "Do the Reggay" de The Maytals, pero ya se utilizaba en Kingston, Jamaica, para denominar una forma más lenta de bailar y tocar "rocksteady".
El artista reggae Derrick Morgan expresa en este sentido:

No nos gustaba el nombre "rocksteady", así que probé diferentes versiones de "Fat Man". Cambió el ritmo, el órgano se utilizó para sorprender. A Byron lee, el productor, le gustaba. Él creó el sonido con el órgano y la guitarra rítmica. Sonaba como 'reggae, reggae' y ese nombre simplemente despegó. Byron Lee comenzó a utilizar la palabra (sic) y pronto todos los músicos estaban diciendo 'reggae, reggae, reggae'.

El historiador del "reggae" Steve Barrow atribuye a Clancy Eccles la alteración del término patois "streggae" (mujer fácil), convirtiéndolo en "reggae". Sin embargo, según Toots Hibbert:

Hay una palabra que solíamos utilizar en Jamaica llamada 'streggae'. Si una chica pasa y los chicos la miran y dicen 'tío, ella es "streggae"', eso significa que no viste bien, que se ve reggay (andrajosa). Las chicas dirían lo mismo de un hombre también. Esta mañana yo y unos amigos estábamos jugando y yo dije, 'vale tío, hagamos el reggay'. Fue solo algo que me vino a la cabeza. Así que empezamos a cantar 'do the reggay, do the reggay' (haz el reggay, haz el reggay) y creamos un beat. La gente me dijo después que le habíamos dado al sonido ese nombre. Antes de eso la gente lo llamaba blue-beat y todo tipo de nombres. Ahora está en el "Libro Guinness de los Records".

Se dice que Bob Marley atribuía como origen de la palabra "reggae" un término del idioma español para referirse a la "música del rey".

Aunque poderosamente influenciada por el mento y el calipso tradicionales, el jazz estadounidense y el primer "rhythm and blues", el "reggae" es deudor directo en su origen de los diferentes desarrollos que tuvieron lugar en el "ska" y el "rocksteady" durante los años 1960 en Jamaica. Uno de los individuos que más contribuyeron a este desarrollo fue Count Ossie.

El "reggae" llega a Inglaterra por cuenta de los miles de inmigrantes que provienen de Jamaica, antigua colonia británica, y que empiezan a difundir su música y a relacionarse con obreros blancos que adoptan esa música. Incluso, el movimiento "skinhead" adoptó al 'ska", y posteriormente al "reggae", como su música propia y a esto también surge la "Gran guerra del reggae".

El "ska" surgió en los estudios de Jamaica alrededor de 1959. Se desarrolló a partir del mento. El "ska" se caracteriza por un tipo de línea de bajo llamado "walking bass" o "bajo galopante", ritmos de guitarra o piano acentuados en el "offbeat", y en ocasiones "riffs" de viento similares a los del jazz. Además de ser muy popular dentro de la subcultura jamaiquina de los "rude boys", hacia 1964 también había ganado una larga audiencia en la cultura mod inglesa.

Los "rude boys" comenzaron a poner deliberadamente los discos de "ska" a la mitad de velocidad, ya que preferían bailar más despacio como parte de su imagen de tipos duros. Hacia mediados de los años 1960, muchos músicos habían comenzado a tocar "ska" con ese tempo lento, al tiempo que ponían énfasis en el walking bass y los offbeats. El sonido más lento fue denominado rocksteady como consecuencia de un sencillo de Alton Ellis. Esta fase de la música jamaiquina duró hasta 1968, cuando los músicos comenzaron a aumentar la velocidad de la música de nuevo, añadiendo también otros efectos. Ello condujo a la creación del "reggae".

El "reggae" se desarrolló a partir del "rocksteady" en los años 1960. El cambio del "rocksteady" al "reggae" es ilustrada por el empleo del "shuffle" en el órgano, cuyo pionero fue Bob Marley. Este rasgo ya aparecía en algunos sencillos de transición como "Say What You're Saying" (1967) de Clancy Eccles o "People Funny Boy" (1968) de Lee "Scratch" Perry. El tema "Long Shot Bus' Me Bet" publicado por el grupo The Pioneers en 1967 es considerado como el ejemplo grabado más temprano del nuevo sonido que pronto sería conocido como reggae.

Es en los comienzos de 1968 cuando los primeros discos de reggae genuino fueron publicados: "Nanny Goat" de Larry Marshall y "No More Heartaches" de The Beltones. El hit "Hold Me Tight" del artista estadounidense Johnny Nash de 1968 ha sido reconocido como el primero en poner el "reggae" en las listas de éxitos de Estados Unidos. 

Otros pioneros del "reggae" incluyen a Prince Buster, Desmond Dekker y Jackie Mittoo y The Wailers, una banda formada por Bob Marley, Peter Tosh y Bunny Wailer. 

En el desarrollo que llevó el "ska" hacia el "rocksteady" y posteriormente al "reggae", fue fundamental la contribución de varios productores jamaiquinos. Entre los más importantes están Coxsone Dodd, Lee "Scratch" Perry, Leslie Kong, Duke Reid, Joe Gibbs y King Tubby. Chris Blackwell, que fundó el sello Island Records en Jamaica en 1960, se mudó a Inglaterra en 1962, donde continuó promocionando la música jamaiquina. Se alió a Trojan Records, fundado por Lee Gopthal en 1968. Trojan publicó discos de artistas reggae en UK hasta 1974, cuando Saga compró el sello.
En 1972, la película "The Harder They Come", en la que actuaba Jimmy Cliff, generó un considerable interés y popularidad para el "reggae" en Estados Unidos, y la versión de Eric Clapton en 1974 del tema de Bob Marley "I Shot the Sheriff" ayudó a llevar el "reggae" al "mainstream". Hacia mediados de los años 1970, el "reggae" recibía un considerable espacio en la radio inglesa, especialmente gracias al programa de John Peel. Lo que se conoció después como la "edad dorada del reggae" corresponde aproximadamente al apogeo del "roots reggae".

En la segunda mitad de los años 1970, la escena de "punk rock" de Gran Bretaña comenzaba a formarse, y el reggae fue una importante influencia para ello. Algunos DJs de "punk" ponían canciones de "reggae" durante sus sesiones y numerosas bandas de "punk" incorporaron estas influencias "reggae" en su música. En Inglaterra el "reggae" se expandió gracias a la nueva inclusión del género en la música de The Rolling Stones con mayor influencia desde 1974. Al mismo tiempo, el "reggae" comenzó una cierta recuperación en Inglaterra en los años 1980 abanderada por grupos como Steel Pulse, Aswad, UB40 y Musical Youth. Otros grupos que recibieron interés internacional a comienzos de los 1980 fueron Third World, Black Uhuru y Sugar Minott.

"Early reggae", también conocido como "skinhead reggae" debido a su popularidad dentro de esa subcultura inglesa de clase obrera, fue el primer reggae que existió después del Rocksteady, comenzó hacia finales de los años 60; a medida que la influencia de la música "funk" de sellos estadounidenses como Stax comenzó a penetrar en la forma de tocar de los músicos de "reggae". Lo que caracteriza al "early reggae" del "rocksteady" es el órgano Hammond "burbujeante", un estilo percusivo de tocar que atrajo mayor atención hacia la subdivisión en ocho octavas dentro del groove. Los "skanks" de la guitarra en la segunda y cuarta nota del compás eran frecuentemente "doblados" en estudio utilizando efectos electrónicos de eco, complementando de ese modo la sensación de doble-tiempo del órgano. En general se daba mayor énfasis al groove de la música. La creciente tendencia en la época de grabar una "versión" en la cara B del sencillo produjo, además, innumerables instrumentales lideradas por vientos o el órgano.

Entre los principales grupos de "skinhead reggae" se incluyen John Holt, Toots & the Maytals, The Pioneers y Symarip. Eran frecuentes las versiones de temas de "soul" de sellos como Motown, Stax y Atlantic Records, reflejando la popularidad de la música soul entre los "skinheads" y los "mods".

"Roots reggae" es un tipo de música espiritual cuyas letras se dedican predominantemente a enaltecer a Jah (Dios). Entre los temas más recurrentes se encuentran la pobreza y la resistencia al gobierno y a la opresión racial. Muchas de las canciones de Bob Marley y de Peter Tosh pueden considerarse "roots reggae". La cima creativa del "roots reggae" se dio hacia finales de los años 1970 con cantantes como Burning Spear, Gregory Isaacs, Freddie McGregor, Johnny Clarke, Horace Andy, Ijahman Levi, Barrington Levy, Big Youth y Linval Thompson, y bandas como Culture, Israel Vibration, The Meditations y Misty in Roots, mano a mano con productores como Lee 'Scratch' Perry y Coxsone Dodd. Musicalmente, en la canción "Roots, Rock, Reggae" Marley concibió un nuevo estilo de música "off beat" con un compás de seis "beats", donde el "skank" de la guitarra tiene lugar en el cuarto y sexto beat. Aunque totalmente separado de los ritmos del "ska", "rocksteady", "reggae", "skank", "flyers", "rockers" y otros estilos, este ritmo único está tan asociado a Marley que muy pocos otros lo adoptaron.

El "dub" es un género de reggae desarrollado en sus primeros tiempos por productores de estudio como King Tubby o Lee Perry. Se caracteriza por basarse en la remezcla (remix) de material previamente grabado, y por dar un particular énfasis a la batería y la línea de bajo. Las técnicas utilizadas provocaban en el oyente sensaciones viscerales, descritas por Tubby como "un volcán en tu cabeza". Augustus Pablo y Mikey Dread fueron otros importantes proponentes de este estilo.

El estilo "rockers" fue creado hacia mediados de los años 70 por Sly & Robbie. El "rockers" es descrito como un estilo fluido, mecánico y agresivo de tocar reggae. Un artículo describe el rockers como la "edad dorada del reggae".

El subgénero del "lovers rock" subgenre se originó en el sur de Londres a mediados de los años 1970. Las letras tratan normalmente sobre amor. Es similar al "rhythm and blues". Algunos artistas significativos de "lovers rock" incluyen Gregory Isaacs, Freddy McGregor, Dennis Brown, Maxi Priest y Beres Hammond. Bob Marley en su disco "Kaya", hizo un prototipo de "lovers rock" con sus baladas románticas "reggae" entre ellas "Waitin in Vain".

El Rap es un estilo de cantar o hablar sobre un disco instrumental que fue utilizado por primera vez en Jamaica en 1960, donde se le conocía como Toasting, por deejays como U-Roy y Dennis Alcapone. Este estilo influyó poderosamente sobre el DJ jamaiquino Kool Herc, quien utilizó este estilo en Nueva York, pero usando las partes instrumentales de discos de funk, a mediados de los años 1970, sentando el precedente directo del hip hop y el rap. El sonido de bajo y bombo saturado en la música dub también influyeron sobre el sonido de buena parte del hip hop.

El dancehall, primero conocido como rub-a-dub, fue desarrollado alrededor de 1980, por artistas como Yellowman, Super Cat y Shabba Ranks. El estilo se caracterizó por cantar como los "deejay" y rapear o hacer toasting sobre rhythms crudos y rápidos. Ragga (también conocido como "raggamuffin") y reggae fusion son subgéneros del dancehall donde actualmente la instrumentación principalmente es llevada a cabo mediante música electrónica y sampling (el primer ritmo 100 % digital fue "Under My Sleng Teng", de la mano del productor Prince Jammys y el cantante Wayne Smith por el año 1984 y que en su primera aparición hizo ganar al sound de Jammy's en un clash contra Black Scorpio). Entre los pioneros del ragga están Shinehead y Buju Banton.

El reggae fusion es una mezcla de reggae o dancehall con elementos de otros géneros, como hip-hop, R&B, jazz, rock, drum and bass, punk o polka.

Los principales intérpretes y grupos de reggae en Latinoamérica son: 

Todos los 1 de julio se celebra el Día Internacional del Reggae. Originalmente se instauró en 1994 de forma local, inspirado en el discurso que Winnie Mandela pronunció durante su visita a Kingston, la capital de Jamaica, en 1992, en el que alabó la capacidad que tenía la música Reggae de dar aliento, inspirar y unir al pueblo de Sudáfrica en su lucha contra el apartheid. La idea original del evento fue animar a las radios de jamaiquinas a que dieran a conocer las raíces y ramificaciones del género, promover normas de calidad en la música y los medios de comunicación, crear el Salón de la Fama de la Música Jamaicana y ayudar al pueblo del país a luchar por la igualdad social y mostrar lo mejor de la sociedad.

Pero a partir de 1996 se internacionalizó la celebración con el objetivo de "unir, inspirar y dar aliento" a la comunidad mundial del género mediante el poder de la música, los medios de comunicación y la tecnología, exhibir la ciudad de Kingston, capital de Jamaica, como la cuna de la música jamaicana, destacar la influencia de Jamaica, el Reggae y la religión Rastafari en la cultura pop mundial y y promover el apoyo a la educación musical y artística en la isla. 

El 29 de noviembre de 2018 la UNESCO declaró al Reggae de Jamaica Patrimonio Cultural Inmaterial de la Humanidad por su aporte a la reflexión internacional sobre cuestiones como la injusticia, la resistencia, el amor y la condición humana y poner de relieve la fuerza intelectual, sociopolítica, espiritual y sensual a la vez de conservar intactas toda una serie de funciones sociales básicas de la música como ser vehículo de opiniones sociales, práctica catártica y loa religiosa y ser un medio de expresión cultural del conjunto de la población jamaicana. 

La se compone de las expresiones que ilustran la diversidad del patrimonio inmaterial y contribuyen a una mayor consciencia de su importancia. 




</doc>
<doc id="4663" url="https://es.wikipedia.org/wiki?curid=4663" title="Geografía de Venezuela">
Geografía de Venezuela

El territorio de Venezuela está ubicado al norte de América del Sur. Su límite está muy cerca de la línea del Ecuador terrestre, por lo tanto, forma parte de la zona intertropical. Sus límites geográficos son: Mar Caribe (norte), Colombia y Brasil (sur), Guyana (este) y Colombia (oeste), además su Mar Patrimonial le otorga fronteras con los mares territoriales de: Estados Unidos de América (Puerto Rico e Islas Vírgenes de los EE.UU.), el Reino de los Países Bajos (Aruba y Antillas Neerlandesas: Bonaire, Curazao, Saba, y San Eustaquio), la República Dominicana, Francia (Guadalupe y Martinica), Trinidad y Tobago, Colombia, San Cristóbal y Nieves, el Reino Unido , Dominica, Santa Lucía, San Vicente y las Granadinas, Granada y Guyana. El territorio comprendido entre el límite oficial con Guyana (Río Cuyuní) y el río Esequibo comprende una extensa zona que Venezuela reclama como propia, conocida como la Guayana Esequiba.

Tomando como referencia el centro geográfico del territorio continental venezolano, el área que se encuentra exactamente al otro lado del globo terráqueo (antípoda) es la Península de Malaca, en el Sudeste Asiático, específicamente la región sur de Tailandia. Venezuela es un país rico en recursos tanto renovables como no renovables.

El territorio de Venezuela está formado por 1.075.945 km²; el territorio insular (islas), que abarca 1.270 km²; el espacio aéreo y submarinas se encuentran el mar territorial (el cual suma 71.295 km² al territorio general), la zona contigua (22.224 km²), la zona económica exclusiva (348.176 km² de extensión marina que incluyen la zona contigua), la plataforma continental (que corresponde al fondo marino, hasta la extensión de la zona económica exclusiva) y las aguas interiores, históricas y vitales. Visto así, el territorio (continental y marítimo) de Venezuela abarca 916.445 km², ya que de las áreas marinas y submarinas sólo el mar territorial suma extensión al territorio,aun cuando en todas ellas el estado ejerce soberanía.
Reclama 159.542 km² del territorio de lado a Guyana.Venezuela.

El país comprende muchas regiones geológicamente muy variadas. Al oeste se extienden la Cordillera de los Andes venezolanos. Estos se prolongan hacia el norte y se transforman allí en la Cordillera de la Costa (Venezuela). Al sur de esta cadena montañosa se encuentran los Llanos, con gran cantidad de ríos. Al sur de los Llanos corre el río Orinoco. Al sur del Orinoco está la región de las Guayanas, un escudo de la era precámbrica, una parte del cual se ubica en la cuenca del Río Negro e indirectamente en la del Río Amazonas y otra parte en la cuenca del Orinoco. La Guayana venezolana es la región más extensa del país y está formada, además del antiguo escudo guayanés, por amplias y elevadas mesetas que toman el nombre de Tepuyes, que le dan los pemones, indígenas que habitan en la Gran Sabana.

Por encontrarse en la zona intertropical, Venezuela posee un clima cálido y lluvioso en general, pero debido a la orografía, los vientos, la influencia del mar y la orientación de las cadenas montañosas, hay diferencias climáticas. La latitud ejerce cierta importancia en la estacionalidad y cantidad de las lluvias, pero su papel es mucho menor en cuanto al efecto que tiene en las temperaturas. La altitud, sin embargo, constituye un factor que cambia drásticamente el clima, sobre todo en lo que se refiere a la temperatura, alcanzando valores muy diferentes según la disposición del relieve en lo que se conoce como pisos térmicos, bióticos o ecológicos.

Durante el Pleistoceno tardío la extensión de los glaciares en la Cordillera de Mérida y Sierra de Perija era de más de 700 km² y el nivel del mar era 125 m. más bajo.
La medida anual de temperatura se reduce sólo con la altitud, como por ejemplo en Los Teques (situada a 1.300 metros) con sus 19,8;°C de promedio anual contrasta con los pueblos y ciudades en el nivel del mar que superan los 27 °C de medida anual, aunque la amplitud térmica es muy escasa en todo el país (nunca supera los 4 °C de diferencia). No existen las estaciones bien marcadas, como sucede con las zonas templadas de ambos hemisferios. Por el contrario, por influencia de la lengua castellana introducida por los españoles durante el período colonial, se le denomina invierno a la época de lluvias, aunque ésta coincide, aproximadamente, con el verano (térmico) en el Hemisferio Norte. Y, por el contrario, se le denomina verano a la época de sequía que corresponde, también aproximadamente, al invierno en el Hemisferio norte (noviembre a abril).
Los climas venezolanos están estructurados en "pisos térmicos", como se menciona a continuación:


La influencia del mar incide en cambios climáticos aunque en menor grado que la altitud, así en las zonas de costas las temperaturas máximas son altas, pero no tanta como Los Llanos, localizadas en el interior, además de esta región junto la Guayana los efectos de la continentalidad incide en amplitudes térmicas diarias más altas (de más de 0 °C), con respecto a la costa (no superior a 4 °C de amplitud media por lo general). Aunque en cualquier caso, en todo el territorio nacional las amplitudes térmicas anuales son insignificantes Con respecto a las precipitaciones hay variaciones en las distintas regiones venezolanas, en Los Llanos es tropical con una granpat estación seca (Clima intertropical de sabana), así en la zona costera del Mar Caribe es árido con escasas precipitaciones, exceptuando la vertiente del Atlántico donde llueve abundantemente. En las zonas montañosas de la Cordillera de la Costa (Venezuela), las lluvias varían según las disposición de las montañas, pero son suficientes y más regulares.El clima puede definirse como la resultante de las condiciones de la atmósfera y de sus efectos sobre la vida en la superficie terrestre, durante un período suficientemente largo. El conocimiento del clima en Venezuela es indispensable para comprender las características de los otros componentes del ambiente como los suelos, la vegetación y los recursos hidráulicos, así como para la evaluación de las potencialidades y limitaciones que ofrecen las diferentes regiones geográficas en el desarrollo del país.

Venezuela se encuentra ubicada en la zona intertropical del Hemisferio Norte, entre 0º 38’ 53” y 12º 11’ 22” de latitud norte, y 59º 48’ 10” y 73º 25’ 00” de longitud oeste. Como consecuencia de esta localización geográfica, las condiciones climatológicas dependen de los patrones de circulación atmosférica y de los sistemas atmosféricos planetarios y regionales que afectan el norte de Sur América y el sur del Mar Caribe.

El territorio venezolano está bajo la influencia directa de los vientos alisios del noreste, los cuales se originan en una zona del Atlántico norte donde la presión atmosférica es alta.

La franja o área donde convergen en la superficie los vientos alisios del noreste del Hemisferio Norte y los del sureste del Hemisferio Sur, recibe el nombre de Zona de Convergencia Intertropical (ZCIT). Ocasionalmente, esta convergencia es muy activa, dando lugar a la formación de grandes nubes de desarrollo vertical, que permiten claramente su identificación en las imágenes satelitales. Esta convergencia se centra, en promedio, alrededor de los 5º N, desplazándose hacia el norte y hacia el sur, en los períodos de los solsticios de verano correspondientes.

Las condiciones generales del clima pueden verse modificadas regionalmente por factores diversos, tales como la altitud del relieve, su exposición a los vientos, la continentalidad, e incluso por la presencia recurrente del llamado fenómeno El Niño-La Niña.

De estos sistemas atmosféricos, los más importantes debido a la amplitud geográfica y a su significado climático, son la Zona de Convergencia Intertropical, el Sistema de Alta Presión del Atlántico norte, las vaguadas, ondas del este y los relictos de frentes fríos del norte. Las ondas del este son perturbaciones que se propagan en la región del Atlántico-Caribe desde el este hacia el oeste, produciendo fuertes precipitaciones.

Por su situación latitudinal, el territorio nacional es esencialmente isotérmico, cuya oscilación térmica estacional es en general menor a 5 °C; en cambio, la amplitud térmica diaria (ATD) es más importante, alcanzando valores superiores a 8 °C y posiblemente aún más altos en situaciones de altitudes significativas, como en los casos de las zonas elevadas de Guayana y los Andes. Es bueno recalcar que los factores de altitud y distancia al mar (continentalidad) ejercen también influencia, pero de manera moderada.

La altitud del relieve es el factor que introduce la mayor variación de la temperatura en Venezuela. A la disminución de la temperatura con respecto a una determinada elevación se le conoce como Gradiente Térmico Vertical y su valor varía entre 0.63 °C/m cerca del ecuador y 0,62 ºC/m a 12° de Latitud Norte. De manera que en las regiones montañosas, la variación de la temperatura con la altitud es más marcada, generando una secuencia de pisos térmicos que en combinación con las otras variables atmosféricas da como resultado un patrón de pisos climáticos.

Cabe destacar que la variación estacional de los períodos secos y lluviosos es la característica más sobresaliente del clima de las regiones ubicadas al norte del Paralelo 6° N. La estación seca puede iniciarse en noviembre y se extiende hasta mediados de mayo cuando comienza la estación lluviosa, lo cual ocurre en casi todo el país, particularmente en los Llanos Orientales y Centrales, donde la precipitación promedio anual varía entre 800 a 1100 mm al año, así como en los Llanos Occidentales y Meridionales más húmedos con promedios anuales entre 1500 y 2400 mm. En las regiones semiáridas las lluvias son escasas, de alta intensidad y de carácter errático. Por lo general, las regiones de clima árido no presentan un período de humedad marcado, excepto en años excepcionalmente muy lluviosos.

En el territorio venezolano se pueden diferenciar los siguientes tipos climáticos relevantes:

I. Climas Cálidos de Tierras Bajas

Corresponde a las regiones ubicadas por debajo de 1000 m de altitud, con temperaturas medias anuales entre 22 y 26 °C para los tipos cálidos y superior a 26 °C para los tipos muy cálidos. Desde el punto de vista de la lluviosidad se distinguen seis tipos de climas cálidos de tierras bajas con diferentes subtipos; a saber:

Climas del Sur: Comprenden los tipos cálidos superhúmedo y muy húmedo de carácter ecuatorial.

Climas de los Llanos: Húmedo en las tierras bajas y muy húmedo en el piedemonte oeste de Barinas. En la generalidad de los casos, los climas de los llanos se caracterizan por tener una marcada estacionalidad de las lluvias. Así, en el período lluvioso se concentra más del 85% del total anual de precipitación, mientras que en los meses secos llueve muy poco, especialmente en enero, febrero, marzo y abril.

Climas de la Depresión del Lago de Maracaibo: En la Depresión del Lago de Maracaibo se pueden distinguir los siguientes tipos climáticos: Superhúmedo: en los Piedemontes de las cuencas de los ríos Tarra, Socuavo y Catatumbo. Muy Húmedo: en el sector oeste, sur y sureste del Lago. Húmedo: al suroeste, sur y este del Piedemonte Andino. Subhúmedo: en las costas Occidental y Oriental del Lago. Semiárido: en la Altiplanicie de Maracaibo, Laguna de Sinamaica y Paraguachón.

Climas del Sistema de Relieve Lara-Falcón-Yaracuy: En esta región conformada por una sucesión de serranías bajas, colinas y valles longitudinales, existe una variedad de climas que van desde muy húmedo hasta el árido.

Climas del Litoral Central: El Litoral Central está ubicado entre los meridianos 66°30’ y 68° W y se encuentra bajo los efectos del fenómeno de surgencia, resultante de la exposición de la línea de costa en sentido oeste-este y de la inversión de los vientos alisios. Tales situaciones originan en este sector un clima semiárido.

Climas de la Región Nororiental: En esta región se pueden encontrar climas muy húmedos. En el estado Nueva Esparta predomina el clima semiárido, exceptúan¬do el sector nororiental de la isla de Margarita que incluye el Cerro Copey, un poco más húmedo y más fresco por efecto de la altura.

II. Climas de Tierras Altas

Tierras Altas de Guayana: Están conformadas por una topografía escalonada y extensa en la que se intercalan los tepuyes, sierras y serranías, con altitudes superiores a los 1000 m, enclavadas en la extensa región de clima cálido superhúmedo de Guayana. Se presenta, básicamente dos tipos de clima: el mesotérmico superhúmedo y el mesotérmico muy húmedo.

Cordillera de la Costa Central y Oriental: En esta Cordillera, tanto en su parte central como oriental, los tipos climáticos que se destacan son el mesotérmico húmedo que cubre las franjas altitudinales entre 1000 y 2000 m, el mesotérmico subhúmedo ubicado en el mismo piso altitudinal y el templado subhúmedo locali¬zado por encima de los 2000 m. Este último presenta una temperatura media anual entre 15 °C en su parte inferior y 10 °C en sus partes más altas. El factor altitud se une con los factores marítimos y de continentalidad para producir alta diversidad climática.

Cordillera de Mérida: En la Cordillera de los Andes, es donde existe la mayor variabilidad climática de las tierras altas situadas por encima de los 1000 m de altitud. En efecto, la vigorosidad de su relieve expresada en diferentes franjas altitudinales que culminan casi a 5000 m, la masividad, su orientación e incluso su exposición a los vientos y a los rayos solares, así lo determinan. Los tipos y subtipos de climas que pueden encontrarse en esta Cordillera, van desde el Mesotérmico muy húmedo, hasta el Piso Gélido. La variabilidad climática andina determina, a su vez, una marcada biodiversidad, muy importante para la localización de los tipos de vegetación natural y el uso agrícola de la tierra. Es lo que se conoce como pisos bioclimáticos, cuyo escalonamiento en la Cordillera de Mérida es el más conspicuo de todo el territorio nacional.

Cordillera de Perijá: La Cordillera de Perijá presenta una gran diversidad climática, en estrecha relación con la altitud y la orientación de la exposición de las vertientes, lo cual origina importante variabilidad en la cantidad de lluvia media anual. Es así como en la cuenca del río Guasare, ubicada en el sector más hacia el norte de la cordillera, la precipitación varía entre 1600 mm/año en la zona montañosa baja hasta un poco más de 3000 mm/año en las estribaciones de la zona fronteriza con Colombia. Mientras que en extremo sur de la Cordillera, la cantidad de lluvia es aún mucho mayor, alcan¬zando promedios anuales entre 2200 mm/año en la zona montañosa baja hasta 4000 mm/año en los valles de los ríos de Oro e Intermedio. Asimismo, en las cumbres sobre los 3000 m de altura donde están las nacientes del río Apón y sus tributarios, se encuentra un zona de Páramo muy húmedo, bordeando la zona limítrofe con Colombia.

Fuente: Atlas de Venezuela - Instituto Geográfico de Venezuela Simón Bolívar.

Venezuela está conformada por tres vertientes hidrográficas: la del Mar Caribe, la del Océano Atlántico y la del Lago de Valencia, que forma una cuenca endorreica. La principal es la del Caribe por el número de ríos que la constituyen, aunque suelen ser de corto curso y de caudal escaso e irregular, con alguna excepción como es el caso del Río Catatumbo, que nace en Colombia y desagua en la cuenca del Lago de Maracaibo. Al Océano Atlántico drena la extensa cuenca del río Orinoco, cuya superficie es superior a la de toda Venezuela. La cuenca del Orinoco es la tercera de América del Sur por su superficie y da origen a un caudal de unos 33000 metros por segundo, lo que convierten al Orinoco en uno de los ríos más caudalosos del mundo y también en uno de los más valiosos desde el punto de vista de los recursos naturales renovables. Un río que constituye un caso único en el mundo es el Casiquiare, que constituye una derivación natural del Orinoco y que, después de unos 500 km de longitud, desagua en el río Negro el cual es afluente, a su vez, del Amazonas.

Los principales afluentes venezolanos del Orinoco son el Arauca y el Río Apure por la margen izquierda y el Ventuari el Caura y el Río Caroní por la margen derecha, entre otros.

Venezuela posee un relieve variado que va desde las cumbres de la Cordillera Andina en el oeste hasta las planicies deltaicas en el este pasando por los llanos en el centro-sur, la Cordillera de la Costa en el norte (considerada por muchos como continuación de la Cordillera Andina) y la amplia zona de mesetas del Macizo Guayanés al sur del Orinoco (la región más extensa, con el 50 % de la superficie total del país).

El país Venezuela es considerado como uno de los Países Mega diversos por poseer una gran cantidad y diversidad de especies, sobre todo en lo que se refiere a las especies vegetales y a las aves. Y la diversidad climática y, al mismo tiempo, la estabilidad de los elementos del clima, han hecho que muchas especies vegetales y animales de otras partes del mundo se hayan introducido y hayan encontrado un hábitat sumamente apropiado para su desarrollo: casi todos los cultivos y especies domesticadas por el hombre pueden cultivarse en Venezuela en condiciones muy favorables, un hecho ya señalado hace casi dos siglos por Andrés Bello en su "Silva a la agricultura de la Zona Tórrida".

Entre ellos destacan los minerales como petróleo, gas natural, hierro, bauxita, carbón, oro, diamantes, coltán, entre otros; los recursos pesqueros son abundantes en la fachada marítima caribeña y atlántica así como en los ríos de los Llanos; los recursos forestales y las vastas extensiones agrícolas y pecuarias están muy subutilizados y se hallan en Los Llanos y en las zonas andinas, así como en el norte del país. Además, el enorme potencial hidroeléctrico presente en la región Sur del país (Guayana) viene a complementar y hasta sustituir en su mayor parte, el potencial termoeléctrico de las plantas que consumen gas natural y gasóleo.
Existen varios tipos de relieve: relieve terrestre, relieve costa oriental y otros.

Caracas es la ciudad capital de la república, una metrópoli en donde se asientan los poderes públicos nacionales. Le siguen en importancia Maracaibo, Valencia, Barquisimeto ,Maracay y muchas otras.

Venezuela tiene siete regiones naturales continentales y una región natural marítima.



La población venezolana es bastante heterogénea. La mayoría de los habitantes tiene antepasados europeos, indígenas americanos y africanos, principalmente. Gran cantidad de inmigrantes han llegado a Venezuela en el siglo XX. Desde el punto de vista demográfico, Venezuela es un país relativamente joven, con una pirámide de población bastante ancha en la base, tasa de natalidad. A comienzos de los años 90 (siglo XX), la proporción de la población femenina comenzó progresivamente a sobrepasar a la masculina, lo que representa una tendencia consistente con la etapa de transición demográfica. La mayor parte de la población vive en el norte del país (más del 70 % de la población), con una amplia zona casi despoblada al sur del Orinoco (la mitad de la superficie del país sólo concentra el 5 % de sus habitantes) y la región de Los Llanos, con algo más del 20 % de la población total. Alrededor del 80 % de la población es urbana, la mayor parte de la cual se concentra en las grandes ciudades.




</doc>
<doc id="4664" url="https://es.wikipedia.org/wiki?curid=4664" title="Historia de Venezuela">
Historia de Venezuela

La historia de Venezuela se remonta al poblamiento del territorio, por las migraciones amerindias hace años. La historia escrita de Venezuela comienza con la llegada de los primeros españoles a finales del siglo XV Venezuela se conforma como estado en 1777 a partir de la Capitanía General de Venezuela, colonia del Imperio español que había sido fundada en 1527.

Se cree que el ser humano apareció en el territorio que hoy se conoce como Venezuela hace unos 30.000 años aproximadamente, proveniente de la Amazonia, los Andes y el Caribe. La época precolombina en Venezuela a partir de ese instante puede dividirse en cuatro períodos: Paleoindio (30.000 a. C-5000 a. C.), mesoindio (5000 a. C.-1000 a. C.), neoindio (1000 a. C.-1500 d. C.) e indohispano (1500 hasta el presente). Los períodos paleoindio y mesoindio se caracterizaron por la elaboración de instrumentos para cazar grandes animales como el megaterio, el mastodonte y el gliptodonte; así como el posterior desarrollo de artes de pesca y la navegación hacia a las islas del Caribe.
Grupos de personas que llegan durante el Pleistoceno Tardío, posiblemente desde el Norte, comienzan a ocupar la costa septentrional del territorio. Taima-Taima, Muaco y El Jobo son algunos de los lugares que presentan restos de esta población. La presencia de estos grupos se remonta al menos al año 13000 A.C. Los humanos que vivían en lo que es Falcón compartían su hábitat con una mega fauna como los megaterios, los glyptodontes y los toxodontes. La fauna de los años prehistóricos y precolombinos estaba formada en parte por dantas, tigres dientes de sable, armadillos gigantes, entre otros.

Los arqueólogos identifican un período Mesoindio entre el 7000-5000 A.C. y el 1000 A.C.. En este período los grupos de cazadores de mega animales pasan a formar estructuras tribales más organizadas.

El desarrollo que se produce aproximadamente a partir del 1000 A.C, pero muy diferente según las regiones, se conoce como el período Indígena. Se produce un desarrollo de la agricultura entre diversos grupos.

La población indígena al momento del primer contacto con los europeos aproximadamente eran medio millón de personas habitando lo que hoy es territorio venezolano, habría llegado, por el norte, desde la región del Calabozo; por el oeste, de los Andes, y por el norte, del Caribe. Los principales pueblos indígenas eran los chibchas en los Andes, los caribes, situados en casi todas las costas, y los arawakos, asentados en parte de las costas y más al sur, y los wayúu, o guajiros. Sin embargo, el territorio de la actual Venezuela era muy diverso lingüística y culturalmente durante el período precolombino, existe base para afirmar que los diferentes grupos indígenas pertenecían al menos a 16 grupos lingüísticos diferentes entre estas familias lingüísticas estarían presentes:

Y además existe un número de pueblos indígenas que hablaban lenguas aisladas o no clasificadas, cuya filiación no se conoce con precisión (maku, pumé, sapé, uruak, warao, guamo y otomaco).

Dentro de estos grupos existía también una notable diversidad, así las familias caribe y arahuaca ocupan un territorio muy extenso e incluían a pueblos que hablaban lenguas diferentes aunque emparentadas entre sí (dentro de cada familia). Las regiones de oriente, Guayana y centro del país así como también partes de Zulia y los llanos fueron habitados por tribus caribes que migraron de la cuenca del Amazonas en Brasil, aunque después a causa de guerras territoriales ocuparon la costa norte de Suramérica. Los arawakos provenientes de la Amazonía occidental poblaron regiones del que es hoy el estado Amazonas, en las planicies y buena parte del occidente y centro occidente del país, los waraos se encontraban en los caños de la desembocadura del río Orinoco, los Timoto-cuicas en las montañas de los Andes y también los yanomamis en las selvas del Amazonas. </ref>

Grupos chibchas provenientes de la zona hoy conocida como Colombia comienzan a entrar en territorio de los Andes venezolanos. Aparecen grupos Caquetíos de Paraguaná. También se producen pequeñas migraciones de grupos independientes que pueblan la cuenca del Orinoco y otras reducidas zonas del país. Al llegar los españoles existían en Venezuela numerosas etnias que hablaban idiomas caribes, arawakos, chibchas, tupí-guaraníes y de otras familias lingüísticas.
Los aborígenes usaban tecnologías rudimentarias para construir viviendas, terrazas, diques, canales de riego, etc. Habitaban comunidades nómadas, agricultores sedentarios, como los sembradores de maíz, cultivo que necesitaba de complejos sistemas de riego y embalses para controlar ríos, había cazadores de dantas y manatíes, recolectores de conchas marinas y pescadores, los cuales utilizaban embarcaciones fabricadas con base en un tronco de un árbol caído para transportarse, no derribaban una palma si no había necesidad de ello.
Unos adecuaban el terreno montañoso para la agricultura construyendo terrazas, otros construían muros de piedra en los valles para ordenar los sembradíos. En los Llanos, pueblos originarios construyeron una extensa red de calzadas, las cuales comunicaban a las aldeas, crearon los "campos elevados" que incrementaban la producción agrícola en las zonas anegadizas, con lo cual lograban dominar las inundaciones en las épocas de lluvias.

No solían traer materiales de regiones lejanas para construir sus viviendas o sus instrumentos, también otro tipos de materiales importantes. Casas de piedra unifamiliares en las regiones más frías, churuatas colectivas de madera y palmas agrupaban al grupo familiar extendido, palafitos de wayúus y waraos eran viviendas comunes apoyadas sobre pilotes en las lagunas y manglares. 

El trueque solía constituir en el intercambio de tubérculos de la montaña por frutas de tierras bajas, maíz por huevos de tortuga, pescado salado por yuca, y así sucesivamente. Los kariña lograron desarrollar amplias áreas de trueque, cultivaban algodón, yuca, árboles frutales y tabaco, los cuales cambiaban por canoas y hamacas. También producían cestería, cerámica, adornos corporales de plata, perlas, oro y carey de conchas de tortuga que encontraban en zonas distantes de su hábitat. La vestimenta variaba según la región, ya que se fabricaba con las fibras naturales que encontraban en su entorno, así, atavíos de lana para el frío de los Andes y guayucos para el calor. 

Se sancionaba fuertemente la acumulación de distintas riquezas en varias comunidades, ya que la propiedad era colectiva, la producción era social y no individual, en la mayoría de esas comunidades la comida solía prepararse para toda la población. Sin embargo, pretensiones territoriales de algunas poblaciones agresivas desembocaban en grandes guerras, ejércitos de hasta 40 mil hombres combatieron en la guerra entre catuches y teques.

En otras regiones, los warao, huyendo de los caribe dejaron su territorio ancestral, y encontraron un nuevo hogar dentro de los caños del delta del río Orinoco. La organización social variaba según la región, había algunos pueblos que se constituían en comunidades tribales, jerarquizadas, con caciques y autoridades de paz, y otras con una organización comunitaria donde sólo el chamán, curandero y guía espiritual tenía un rango superior, usaban las plantas con fines medicinales.

Lo que sería con el tiempo Venezuela fue avistada y recorrida inicialmente en agosto de 1498 por Cristóbal Colón quien se acerca a las bocas del río Orinoco yendo desde las islas Canarias, y recorre la costa desde la isla Trinidad hasta quizás el actual cabo de la Vela, en la península de la Guajira, al este de Colombia. Siendo ésta la primera vez que los europeos avistaban el continente, el almirante, al observar la variedad de flora y fauna, llamó a la zona "Tierra de Gracia", en clara alusión al Edén bíblico

Viajes subsiguientes como el de Alonso de Ojeda, Diego de Lepe, Cristóbal Guerra y Alonso Niño. Entre 1499 y 1502 delimitan rápidamente dos porciones de territorio para hacer de ellos gobernaciones, y ejercer jurisdicción: la una desde las bocas del Orinoco hasta el "morro de Maracapana", actualmente en la ciudad de Lechería, en la costa oriental de Venezuela, área que llega a ser conocida como la Gobernación de Cumaná, y de allí en adelante costeando hasta el cabo de la Vela sería luego hacia 1528 la Gobernación de Venezuela o Gobernación de Coquivacoa.

Hacia 1523 una ciudad castellana en el oriente de Venezuela, con el nombre de Nueva Cádiz, florece en la isla de Cubagua a base de la enorme extracción de perlas de sus aguas y luego con el comercio esclavista de indios de toda la costa firme cercana.

Esta efímera ciudad es, sin embargo, la más sólida de cuantas se construyen en ese siglo en Venezuela, pues toda ella está hecha de calicanto, tejas y piedra, por la riqueza que genera la explotación perlífera. Dura poco como establecimiento poblado castellano, hasta 1542, en que se la abandona en favor de la cercana Isla de Margarita, por la extinción final de los ostrales de sus aguas, y calamidades naturales como un posible terremoto y un seguido huracán en esos años. No obstante su influencia como ente irradiador de presencia castellana en el territorio y de base para expediciones al interior del mismo fue notable.

Carlos I le otorga la administración de Venezuela a la sociedad de los Welser de Augsburgo a cambio de fondos financieros. El rey prescribe que los Welser debían fundar una cierta cantidad de ciudades y promover la inmigración, pero estos se dedican ante todo a la búsqueda de El Dorado y la esclavización de los indios.

Ambrosio Alfinger (Ambrosius Ehinger) es el primer gobernador de la provincia. Llega a Coro en 1529 y desde allí marcha hacia el Occidente. En la entrada de un lago ataca a las tribus de la zona y funda la ciudad de Maracaibo.

Desde 1529 hasta 1538 los Welser registran la exportación de unos 1005 indígenas, aunque el rey ya había prohibido la esclavitud de indios en 1528. Las expediciones realizadas por los Welser y sus subalternos significaron un gran problema para la ciudad de Coro ya que siendo estos sus gobernantes prácticamente la dejaron abandonada con las personas que allí vivían desamparados y desalentados,debido a que al ser una zona costera no era apto el terreno para la siembra y los vecinos que ahí vivían todo lo adquirían en los almacenes de los welser a "precios abusivos". hasta el mes de enero de el año 1545 momento en que Juan de Carvajal se presenta como un funcionario real a la sociedad existente en la dicha ciudad. Animándolos a abandonar esa tierra por una 
donde fluye un gran río,y la cual es buena para la siembra, de la que había escuchado de algunos indios en la zona.Es así como logra salir la primera expedición con mujeres,niños y algunos animales.A pesar de que se presentaron múltiples opiniones negativas ya que muchos temían perder lo poco que les quedaba.llegaron así el día 7 de Diciembre de 1545 alrededor del Valle de El Tocuyo. De la cual se lee en años posteriores por Don José Luis Cisneros : " La ciudad está fundada en un espacioso valle que forman las dos cordilleras; es un poco melancólico su cielo;su temperamento es frío ;los aires son suaves y sanos;sus aguas son sanas y abundantes;la situación es hermosa..."(Cisneros, José Luis.'Descripción exacta de la provincia de Venezuela'.colecc.de libros raros o curiosos que tratan de América. Madrid, 1912,pp.105-108.)

Se destaca que Juan de Carvajal salió de Coro junto con Juan de Villegas que había llegado a Coro en septiembre del año 1544 a quien nombró teniente de Gobernador.
Luego de que llega fracasada la expedición de los welser se dirigen hacia El Tocuyo ya que la ciudad de Coro quedó despoblada y se le dijo a estos últimos sobre la acción que habían tomado los habitantes de la mano de Juan de Carvajal lo cual provocó la ira de Philipp von Hutten.Es así como llega al lugar donde se encontraba reunido Carvajal con algunos vecinos, y desde la llegada de los expedicionarios la conversación se tornó violenta desde el principio. Ya que esta acción los dejó "siendo los Gobernadores de una ciudad deshabitada",razón por la cual von Hutten manda a sus compañeros a desarmar a los partidarios de carvajal,Alegando a que la ciudad se mantendría con los que se quisieran quedar, pero ninguno de los hombres que allí se quedaran iban a poder poner resistencia nunca a las fuerzas de von hutten. Es así como tomaron las armas y en un momento Bartolomé Welser "el joven" reprende a Carvajal y éste cae en el barro.

Juan de Carvajal se levanta después de un breve letargo y reúne a los hombres que allí se encontraban,tomaron algunas armas y salieron tras los expedicionarios alemanes entre los que también habían españoles.

Es así como se vuelven a conseguir en medio del camino y vuelven de nuevo las discusiones pero esta vez más acaloradas. Es en ese momento cuando se torno todo en una batalla campal, dando como vencedor a los de Carvajal en la que murieron Bartolomé Welser, Philipp Von Hutten y tres españoles más. Se supo de lo ocurrido debido a las denuncias que hicieron los que escaparon.

Pronto llegó la noticia a los oídos del Rey, quien en un juicio parcializado y rápido manda a la detención y ejecución de Carvajal, el cual trató de defenderse diciendo que todo lo que hizo fue en beneficio de las familias que había sacado de la decadente Coro, y por el futuro del poblado recién creado.
Al cabo de un mes y medio después de lo sucedido el Juez y futuro Gobernador Juan Pérez de Tolosa captura a Juan de Carvajal. Preso, hasta el juicio el día 16 de septiembre de 1546. Después de ser condenado a ser llevado amarrado desde la cárcel en la que estaba hasta la plaza mayor a morir en la horca, según el escribano Juan Quincoces de la Llana. Al pronunciarse la sentencia, Carvajal tomó la palabra y se dirigió a las autoridades diciendo "que solo el fue responsable de las muertes de esas personas y que no se culpase a nadie más". Juan de Villegas en julio de 1546 debido al juicio de residencia que le iniciara el lic. Juan de Frías por el asesinato orquestado por Carvajal, estuvo encarcelado durante un mes. Pero finalmente no pudieron demostrar su culpa, en parte debido a que no hubo quien testificara en su contra.
El asesinato de Philipp von Hutten por el conquistador Juan de Carvajal en 1546 lleva al colapso de la administración de los Welser en Venezuela.

La abdicación de Carlos V en 1556 trae consigo la pérdida definitiva de los derechos de comercio para los alemanes.

En 1561 Venezuela ve la llegada de Lope de Aguirre y sus marañones provenientes del Perú. Este toma primero la isla de Margarita en 1561. De allí parte hacia Borburata, donde desembarca y continúa su camino a través de Valencia hacia Barquisimeto. En ese tiempo causa terror entre las poblaciones a las que llega con sus seguidores. El 27 de octubre de 1561 llega a Barquisimeto, donde es asesinado por sus propios expedicionarios.

El siglo XVI ve el nacimiento posteriormente, de forma más o menos espasmódica y con muchas vicisitudes de ciudades castellanas definitivas y estables, tales como Coro (1527), Maracaibo (1578), Barquisimeto (1552), Mérida (1558), Trujillo (1558), El Tocuyo (1545), Valencia (1553), Barinas (1597), Caracas (1568), Cumaná (1569), Carora, La Asunción y San Tomé, en la Guayana.

Los piratas y contrabandistas, ante todo grupos británicos y franceses, pero también holandeses, azotan las zonas costeras de Venezuela por más de dos siglos. Entre los ataques más importantes figuran los de John Hawkins y Francis Drake. John Hawkins desembarca en dos ocasiones en el pueblo costero de Borburata y vende allí esclavos que había apresado en Guinea.

A fines del siglo XVI ya el orden colonial está bien establecido y funcionan en debida forma las instituciones coloniales castellanas, como el Cabildo, la Iglesia, la Real Hacienda y el régimen de encomienda indígena. En 1576 el gobernador se establece en Caracas, por su buen clima y estar defendida de piratas por la serranía costera que la separa del litoral, ciudad donde residirá, haciendo a ella en adelante la capital del país. En 1584 se mudan a Caracas contadores de la Real Hacienda y para esa época ya reside allí el obispo.

El comercio del trigo florece, así como la ganadería, la minería de extracción aurífera y la costumbre de los cueros para la exportación. Se importan esclavos para las plantaciones y el servicio doméstico.

Durante la conquista y colonización del territorio venezolano se organizan varias gobernaciones o provincias, sin continuidad en el tiempo, como las de Nueva Andalucía o Cumaná, Coro, Venezuela (o Caracas), Trinidad, Gobernación de La Grita, Nueva Extremadura o Mérida, Guayana y la efímera de Barcelona, en 1636. Cabe señalar que las mismas funcionaban independientemente.

Las provincias de Caracas, Cumaná, Guayana y Maracaibo dependen inicialmente de la Real Audiencia de Santo Domingo y luego de la Real Audiencia de Santafé de Bogotá o del Virreinato de la Nueva Granada, en diversas ocasiones, alternándose en esta función, sobre todo en el ámbito judicial, con la Real Audiencia de Santo Domingo, dependiente del Virreinato de la Nueva España.

El siglo XVII ve el surgimiento del cacao (1615) como un gran producto de exportación, así como la caña de azúcar, el tabaco, la sal y los cueros. El trigo decae hacia el consumo interno, por aumento poblacional.

Se ordena la fundación hacia 1618 de pueblos de doctrina para recoger a los indios y nacen así pueblos como Turmero, Guarenas, Choroní, Petare, Baruta, La Victoria, Cagua, "San Mateo", "Santa Lucía", El Valle, "Antímano", etc., impulsados por orden real y localmente por acción del obispo y el gobernador, acatando dicha orden.

Las ciudades costeras se fortifican ante el auge pirata. Se construyen fortalezas como la de Araya en el oriente (1622-1646), Pampatar y Santa Rosa en Margarita, San Antonio en Cumaná o San Carlos de la Barra, en la entrada del Lago de Maracaibo, del Estado Zulia. Maracaibo es asaltada por piratas en 1642, y luego repetidamente en otras ocasiones, así como Gibraltar, en el propio lago, Trujillo, Cumaná y Margarita.

La Catedral del Obispado se muda en 1637 de Coro, en donde residía desde 1530, a Caracas y las misiones como institución de varias órdenes como la de los franciscanos y jesuitas comienzan a ejercer su labor pobladora, ordenadora y evangelizadora en todo el territorio, a partir de la segunda mitad del siglo XVII.

El así llamado "terremoto de San Bernabé" ocurrido en junio de 1641 destruye la mayor parte de las edificaciones de Caracas y poblaciones cercanas.
Enfermedades contagiosas tales como el cólera, el sarampión, la peste negra y la gripe, atacan en varias ocasiones las poblaciones castellanas, produciendo estragos entre los indios, esclavos y españoles. Una de las más graves ocurrida en 1657, que produce muchos fallecidos en Guarenas y otras ciudades.

Hacia 1780 se extingue por etapas la institución de la Encomienda de Indios.

En 1728 el escritor neogranadino José de Oviedo y Baños escribe "Historia de la Conquista y Población de la Provincia de Venezuela", que hasta hoy es un clásico de las letras y la historia nacional. José de Oviedo y Baños nació en Santa Fe de Bogotá en 1671 y murió en Caracas en 1738.

El siglo XVIII ve la llegada de la "Real Compañía Guipuzcoana", o Compañía de Caracas, que se establece en 1728 y deviene en un ente monopolizador del comercio del cacao y de la venta de productos importados directamente de España, tales como vinos, trigo, telas y hierro, eliminando tanto para los productores como para los consumidores locales la posibilidad de acceder a otro mercado, lo cual genera enormes fricciones sociales y animadversión de productores y comerciantes criollos en contra de dicha compañía, sus medidas y sobre todo, sus prácticas con respecto a la fijación de precios de las mercancías.

Sin embargo, el establecimiento de la Compañía trae también beneficios, impulsando -por su propio interés- el desarrollo o mejora de la infraestructura de puertos locales, tales como Puerto Cabello, Maracaibo, Coro y La Guaira, así como el resguardo de toda la costa desde el río Esequibo hasta la Goajira, al occidente, y su defensa en contra de contrabandistas que saboteaban su monopolio. Se requisan barcos, se revisan paquetes y caletas marinas y se crean alcabalas de aduana y control. Sus prácticas monopólicas y excluyentes produjeron varias revueltas, siendo una de ellas la liderada por el zambo Andresote, en San Felipe, en 1735. Sin embargo, la más relevante ocurrió en Barlovento, extendiéndose después hacia Caracas, entre 1748 y 1752, la cual estuvo liderada por el cosechero local de origen canario Juan Francisco de León y a la cual se plegaron todos los sectores marginados por las prácticas de la Compañía Guipuzcoana, incluyendo esclavos, pardos y canarios, por lo cual adquirió tintes de revolución social. Ambas fracasan por falta de apoyo de la élite criolla local, que decide plegarse a la Corona.

A mediados del siglo XVIII se fundan ciudades como Angostura (1764), en el Orinoco, y San Fernando de Apure (1788), y crecen otras como San Carlos, Calabozo y San Cristóbal, en los Andes.

Los jesuitas son expulsados hacia 1766, al igual que en el resto de América por orden real.

En 1777 se produce la integración de las varias provincias en la así llamada Gobernación de Venezuela y luego en la Capitanía General de Venezuela, que constituye esencialmente desde entonces el actual territorio de la nación.
El libre comercio se instaura y se extingue en esa década la Compañía Guipuzcoana.

A fines de siglo se crea la "Real Audiencia de Caracas", con jurisdicción judicial para conocer de los pleitos en segunda instancia, que sustituye en esa función a la antigua Audiencia de Santo Domingo.

Las provincias existentes para el momento de la creación y organización de la Capitanía General de Venezuela eran, aparte de la Provincia de Venezuela serían:

Provincia de Trinidad, creada en 1532, por el conquistador Antonio Sedeño, y originalmente bajo la jurisdicción de Santo Domingo, posteriormente a su incorporación a la Capitanía General, fue atacada por una flota inglesa, que obtuvo la rendición de la plaza, del gobernador de la isla, en el año 1797, y fue reconocida su ocupación por Tratado de Amiens en el año 1802.

Provincia de Nueva Andalucía, está reunió a las anteriores provincias o gobernaciones de Nueva Andalucía y Paria, en una única entidad, la misma fue originalmente dependiente de la Real Audiencia de Santo Domingo a partir del año 1569, hasta que fue sujeta a la jurisdicción del Virreinato de la Nueva Granada, de 1749 a 1777.

Provincia de Margarita, la isla fue una provincia hasta el año 1600, cuando pasa a depender directamente de la Corona Española hasta 1777.

Provincia de Guayana, también conocida como Provincia de Angostura, y fundada en el año 1591.

Provincia de Maracaibo, formada en 1740, con la unión de las anteriores provincias de La Grita y Mérida.

La Provincia de Venezuela o Caracas, depende siempre de la Real Audiencia de Santo Domingo, en la isla La Española, hasta 1718, cuando el nuevo régimen borbónico en España, por Real Cédula la hace depender en adelante del recién creado Virreinato de Nueva Granada. Se independiza de nuevo de este Virreinato de la Nueva Granada en el año 1742. Treinta años después se le anexan los territorios de las provincias de Maracaibo, Guayana, Cumaná, dependientes del Virreinato de la Nueva Granada, la provincia de Trinidad, dependiente de Santo Domingo y Margarita, dependiente de la Corona Española, para formar la Capitanía General de Venezuela, con capital en la ciudad de Santiago de León de Caracas, por Real Cédula emitida por el Rey Carlos III de España, el 8 de septiembre de 1777.

La autoridad de la Capitanía General abarca los asuntos de índole política, militar y económica, de todas las anteriormente señaladas provincias; sin embargo, las mismas continúan dependiendo judicialmente de la Real Audiencia de Santo Domingo, y sus gobernadores eran nombrados directamente por la Corona Española.

La influencia de Caracas como ciudad central de gobierno oficial, y residencia del Gobernador en un área que abarcaba económicamente varias otras gobernaciones como la de Nueva Andalucía, Mérida o Guayana, influye finalmente para integrar todo el conjunto de provincias y gobernaciones del área de Venezuela alrededor de la Gobernación de Caracas.

La economía colonial de Venezuela gira alrededor de la exportación de "cueros", trigo, tabaco y cacao, con auges en diferentes épocas, siendo este producto, el cacao tan apreciado en el exterior por su finura, aroma y calidad que impulsa durante los dos siglos finales de la etapa colonial el desarrollo económico, y genera una casta ilustrada de descendientes de los conquistadores, conocida como los mantuanos, que basa su riqueza y poder en este producto durante esos 2 siglos.

El Imperio Español descuida y limita la promoción de la educación en sus colonias. Venezuela, al ser una provincia particularmente pobre después del colapso de la explotación de las perlas en el siglo XVII, es particularmente olvidada. Los grupos de mulatos y otros no tienen acceso a la educación siquiera básica.

En 1727 se crea la primera universidad en Venezuela, siglos después de que se hubiera hecho en México o el Perú.

En 1760 el gobernador de la provincia de Caracas le otorga un permiso al coronel de ingenieros Nicolás de Castro para introducir los estudios de matemáticas con una Academia de Geometría y Fortificación exclusivamente para sus oficiales. Manuel Centurión crea en 1761 una Academia Militar de Matemáticas. En 1763 el maestro Lorenzo Campíns y Ballester introduce una Cátedra de Medicina.

La fuerza militar hispana es bastante reducida para la población. Para 1777 hay en teoría unos 12000 militares para una población de unas 800 mil personas. Es así como en 1797 las tropas inglesas del general Abercromby conquistan con facilidad las islas de Trinidad y Tobago: el gobernador José María Chacón y Sánchez de Soto apenas había conseguido movilizar unos quinientos soldados mal armados en contra de una armada con 59 buques y 6750 soldados de infantería.

Varias tímidas intentonas de emancipación se producen, una de ellas liderada por el ex esclavo José Leonardo Chirino en Coro, y otra por los criollos Manuel Gual y José María España y el español Juan Bautista Picornell influenciados por las ideas de la Revolución francesa, establecidos en La Guaira, denominada "la Conspiración de Gual y España". Sus cabecillas son presos y algunos ahorcados en la Plaza Mayor de Caracas en 1799.

Alexander von Humboldt informa que para el comienzo del siglo XIX Venezuela importaba productos por más de 35 millones de francos de la época y que cuatro quintas partes de esta mercancía viene de Europa. Dice que los cueros de Carora, las hamacas de Margarita y las mantas de algodón del Tocuyo son productos muy poco importantes "incluso para el mercado interno".

Dentro de las causas internas se destacaba el conocimiento de las ideas de la ilustración por los "criollos". Constituían un grupo social caracterizado por poseer un alto nivel educativo por lo que su preparación intelectual y contactos con el extranjero les permitieron conocer las ideas revolucionarias.

En referente a las causas externas que dieron lugar a las causas de la independencia de Venezuela destacamos las siguientes:
Las ideas de igualdad, libertad y fraternidad van a jugar una influencia decisiva en el ánimo de los criollos, además de las diversas independencias surgidas (independencia de Estados Unidos, independencia de Haití) y la Revolución Francesa.

A finales del siglo XVIII tienen lugar los primeros conatos independentistas en Venezuela. La primera de ellas es una rebelión armada en 1795 con José Leonardo Chirinos a la cabeza. La otra se trata de una conspiración por parte de Manuel Gual y José María España, en 1797, y es la primera de raíces populares. Ambas intentonas resultan fallidas, con sus respectivos líderes ejecutados. Francisco de Miranda, por su parte, intenta dos veces en 1806 invadir el territorio venezolano por La Vela de Coro con una expedición armada proveniente de Haití. Sus incursiones terminan en fracasos por la prédica religiosa en su contra y la indiferencia de la población.

La independencia de Venezuela se desarrolló entre 1810 y 1823. Fue marcada por dos importantes acontecimientos:

• La independencia de Estados Unidos de Inglaterra en 1776, abriendo camino a otras colonias como Venezuela.
• La revolución francesa (1789)

En 1806, el criollo Francisco de Miranda, precursor de la independencia, emprendió una expedición liberadora de Venezuela con una armada proveniente de Haití y apoyada por los británicos. En una primera ocasión el intento fue fallido. Miranda se refugió en Tobago, y pocos meses después volvió a intentarlo, logrando el éxito. 

La fecha del 19 de abril de 1810 marcó el inicio de la revolución venezolana y da inicio a la independencia de Venezuela. Vicente Emparan, para ese entonces era el Capitán General de Venezuela, fue destituido de su cargo por el Cabildo de Caracas. Ello dio paso a la formación de la Junta Suprema de Caracas, la primera forma de gobierno autónoma. La Junta gobernó hasta el 2 de marzo de 1811, día en que se instaló el Primer Congreso Nacional, ente que nombra un triunvirato compuesto por Cristóbal Mendoza, Juan Escalona y Baltasar Padrón. Meses después, el 5 de julio de ese año, se procede a declarar la independencia y el 7 de julio del mismo año, finalmente se firma el Acta de la Declaración de Independencia de Venezuela. 

Aun así, los ánimos estaban caldeados y muchos realistas planeaban una conspiración para regresar al estado anterior al 19 de abril de 1810, alzándose varias poblaciones con tal propósito, entre ellas Valencia, Caracas y Los Teques, con el apoyo de la guarnición de Puerto Cabello y varias tropas españolas procedentes de Maracaibo que aún permanecía en manos realistas. La ciudad de Valencia es declarada capital de la República por el Congreso Nacional el 9 de enero de 1812 luego de ser sofocada la rebelión, con el objetivo de asegurar el apego de la ciudad (al igual que el de otras importantes poblaciones dependientes, como Puerto Cabello) a los intereses independentistas. A pesar de ello, esta Primera República colapsa con la llegada de Domingo de Monteverde, quien recupera el control de la Provincia. El 25 de julio de 1812 Miranda, Comandante en Jefe del recién creado ejército, capituló en San Mateo; Simón Bolívar y otros militares entregaron a Miranda a los españoles liderados por Monteverde, quien les dio carta de salida del país.
La región occidental, junto con Atanasio Girardot y José Félix Ribas. Luego de hacer público el polémico Decreto de Guerra a Muerte, enfrentó a los realistas en cuatro batallas a lo largo de la ruta hacia Caracas. Al terminar la campaña, el 6 de agosto entró triunfalmente en Caracas, donde se le tituló como Libertador. Así se dio inicio a la Segunda República, aunque continuaron los combates en otros puntos del país. Sin embargo, al año siguiente estalló una rebelión leal a la Corona a cargo de José Tomás Boves. El violento empuje de sus tropas forzó a los seguidores de Bolívar a huir a oriente y a la expulsión de los patriotas de tierra firme, con lo que cayó la Segunda República.

Bolívar intentó una reedición de la Campaña Admirable para rescatar la república, pero por falta de apoyo se trasladó a Jamaica para conseguir apoyo británico, y luego a Haití. Allí se refugió el resto de los líderes patriotas. Estos planificaron una expedición a tierra firme, la cual zarpó en marzo de 1816. Luego de tomar la Isla de Margarita, los republicanos prosiguieron su gesta atacando Carúpano y Maracay. Bolívar huyó al poco tiempo. Se hizo una . Piar había conseguido liberar Guayana. Bolívar aprovechó esto para trasladarse allí junto con las tropas de mercenarios europeos - ante todo británicos - que llegaban a Venezuela a través de Oriente. Bolívar tomó el mando de las tropas republicanas acantonadas en Guayana y estableció la Tercera República. La rivalidad con Piar creció rápidamente y al final Bolívar mandó a aprehender a este. Al poco tiempo, Piar fue ejecutado. Por su parte, José Antonio Páez realizó importantísimas operaciones militares para liberar la región central del país al mando de sus "llaneros".

La guerra en el llano sigue hasta 1819. En febrero de ese año, Bolívar intentó la reorganización del Estado con la instalación del Congreso de Angostura, cuyo resultado es la creación de la Gran Colombia. En 1820, se firmó el Tratado de Armisticio y Regularización de la Guerra, poniendo fin a la guerra a muerte y cesando hostilidades hasta el 28 de abril de 1821. El 24 de junio de ese mismo año, Bolívar se enfrentó a Miguel de la Torre en la Batalla de Carabobo, que se salda con la victoria republicana. Esta victoria significó la liquidación de las tropas realistas en Venezuela, dejando remanentes que serían vencidos en la Batalla naval del Lago de Maracaibo en 1823.

La República de la Gran Colombia, según la ley fundamental que la crea, integra a Venezuela con el Virreinato de Nueva Granada y la Provincia Libre de Guayaquil, a la que luego se une la Audiencia de Quito. El congreso elegido en Angostura se mueve a Cúcuta, donde se sanciona la Constitución de Cúcuta en agosto de 1821, y en la que se define la organización política de este Estado. Bolívar es elegido presidente por mayoría, y Francisco de Paula Santander es hecho vicepresidente. Bolívar continúa sus campañas de liberación por el sur, en la que propicia la liberación del Perú y la creación de Bolivia.

El nuevo Estado reguló sobre el comercio y las instituciones públicas, y también decretó la abolición de la esclavitud. Pero la discrepancia entre bolivarianos (centralistas) y santanderistas (federalistas) tensionó el orden interno. Aunado a la crisis económica, la carente infraestructura, las diferencias idiosincráticas y de intereses, y el deseo de autonomía por parte de los venezolanos para con su territorio, germinó la secesión. La Cosiata de 1826, liderada por Páez, fraguó dicha inconformidad del departamento de Venezuela con el gobierno de Bogotá. Para aquietar la convulsión, Bolívar gobernó por decreto desde 1828, pero ello no impidió la separación de Venezuela, que se manifestó finalmente en noviembre de 1829. En mayo de 1830 se instaló el Congreso de Valencia en Valencia (capital provisional del país con motivo del congreso) para tomar decisiones con respecto a los pasos a seguir por el Departamento de Venezuela en vista el creciente y continuo distanciamiento con el Gobierno Central, lo cual terminó en la separación definitiva de Venezuela de la Gran Colombia y el nacimiento del Estado de Venezuela, adoptándose una nueva constitución.

El principal jefe político y hombre fuerte de Venezuela en sus albores como nación independiente es José Antonio Páez, quien se juramenta como Presidente el 11 de abril de 1831, y su Vicepresidente es Diego Bautista Urbaneja. En su persona se constituye el Partido Conservador, integrado en su mayoría por militares de alto rango que participaron en la Guerra de Independencia. En su mandato hay relativa paz y la economía muestra una recuperación estimulada por la Ley de Libertad de Contratos de 1834 y la masiva exportación de café. En 1835 delega el poder en José María Vargas, el primer civil en dirigir el país. Esto último no es de gusto para los militares de pensamiento liberal, encabezados por Santiago Mariño y Pedro Carujo que se levantan para exigir la reconstitución de la Gran Colombia y el fin del poderío de una minoría de comerciantes. Entre tales oficiales hay bolivarianos sobresalientes, como el edecán del Libertador, Luis Perú de Lacroix o el granadino José María Melo, así como también un enemigo de Bolívar, Pedro Carujo. Obtienen un efímero triunfo y designan como presidente provisional a Mariño, pero llaman al general Páez con el fin de que los respaldara; sin embargo éste restaura a Vargas en el gobierno y decreta amnistías a los oficiales de la revolución, muchos de los cuales sin embargo resultan desterrados.

Páez, tras haber derrotado una rebelión liberal, y luego de la presidencia de Carlos Soublette (1837-1839) vuelve a resultar electo en 1839. Afrontó la crisis económica mundial de ese año, que golpeó duramente a Venezuela, y a la creciente oposición liberal representada por Antonio Leocadio Guzmán, a la vez que iniciaba las disputas territoriales contra los británicos por la cuestión del Esequibo. Soublette fue nuevamente presidente en 1843, y en 1847 es elegido el general José Tadeo Monagas con gran apoyo, pero rompió luego con los conservadores. El intento de éstos en deponerlo desembocó en el atentado al Congreso de 1848. El General se aseguró de que su hermano José Gregorio Monagas fuese hecho presidente en 1851, quien proclamó la definitiva abolición de la esclavitud en 1854. José Tadeo volvió al poder en 1855, pero su régimen autoritario vio su fin en la Revolución de Marzo de 1858, comandada por Julián Castro; siendo este último nombrado como Presidente Provisional de la República en la Convención de Valencia y posteriormente en Presidente Interino, haciendo de la ciudad de Valencia nuevamente la capital provisional del país.

Los decretos del nuevo gobierno crearon descontento en liberales, y la inestabilidad hizo inminente el estallido de un conflicto armado conocido como la Guerra Federal.

El "" marca su inicio, y se desarrolla como una guerra de guerrillas. En las batallas iniciales, los federalistas liberales obtuvieron importantes triunfos, a pesar de la muerte en combate de su líder Ezequiel Zamora en 1860. Su mando es ocupado por Juan Crisóstomo Falcón. Los refuerzos y el apoyo conseguido por Falcón fortalecen a los liberales. Los enfrentamientos posteriores les dan ventaja y merman las fuerzas del gobierno centralista. Finalmente, en abril de 1863 se firma el Tratado de Coche, que significa la victoria de los liberales y su acceso al poder. No obstante este resultado, se conforman nuevos caudillismos regionales con ejército propio que mantiene el control de grandes porciones de tierra, cosa que contraria el anti-latifundismo liberal. Ese año, Falcón asume la presidencia y promulga su Decreto de Garantías que elimina la pena de muerte, cosa que es ratificada en la nueva constitución, y convirtiendo a Venezuela en el primer Estado moderno del mundo en llevarlo a práctica.

Las medidas de Falcón causaron rencor tanto entre los conservadores como en los disidentes de la facción liberal. Ambos bandos se unieron para derrocar al gobierno en 1867 en la llamada Revolución Azul. Un ejército dirigido por Miguel Antonio Rojas se alzó en la región central del país, mientras que el expresidente José Tadeo Monagas se alzó en la región oriental. Por la difícil situación, Falcón delegó el poder en Manuel Ezequiel Bruzual. A mediados de 1868 Rojas rodeó la capital, y firmó el Tratado de Antímano, reconociendo al gobierno y asumiendo el mando militar del país. Los orientales, considerando el tratado como una traición, prosiguieron su campaña hacia Caracas, a la que capturaron en junio de ese año, instaurando el gobierno de los "azules", Guillermo Tell Villegas y José Ruperto Monagas.

Antonio Guzmán Blanco, hijo de Antonio Leocadio Guzmán, había luchado en las filas del bando liberal durante la Guerra Federal y luego formó parte del gobierno de Falcón. Luego de iniciado el régimen de los azules, tramó junto con su padre el retorno al poder de los liberales. Al huir por el rechazo de turbas azuzadas por el gobierno, organizó una invasión que logró el apoyo de caudillos regionales federalistas, tales como Joaquín Crespo y Francisco Linares Alcántara. En febrero de 1870 desembarcó en Curamichate y tomó posiciones por el centro-occidente del país mientras engrosaba sus fuerzas. Tomó Caracas en abril de ese año, por lo que su acceso al poder se conoce como la Revolución de Abril.

Por haber vivido varios años en Europa, una vez hecho presidente implementó una serie de medidas tendientes a modernizar el país e instaurar el orden definitivo. En los decretos de ese año, creó el Conservatorio de Bellas Artes, reestructuró la Alta Corte Federal, dictó el Decreto de Instrucción Pública y Obligatoria promoviendo la educación, reorganizó la Universidad Central hizo del peso venezolano la moneda nacional, fomentó la agricultura, mejoró la infraestructura, e inició una ambiciosa transformación urbanística de Caracas, ciudad a la que según los historiadores se empeñó en darle cualidades parisinas, sin abandonar una tendencia centralista y autoritaria. También combatió los alzamientos en Apure, Guayana y Coro, logrando someter a los caudillos. Inició una promoción del culto a los héroes del pasado, especialmente a Simón Bolívar, como una estrategia para unir el país. Igualmente, debilitó el poder de la Iglesia Católica en Venezuela, al pasar al Estado funciones que tradicionalmente eran realizadas por ésta.

En 1877 viajó a Europa tras pasar el mando a Francisco Linares Alcántara, quien poco después comenzó un movimiento contra Guzmán Blanco. Ello, y la descontinuación de la línea progresista mantenida por su antecesor, provocó la Revolución Reivindicadora que le derrocó en 1879. Tras regresar al país, Guzmán Blanco inició un segundo gobierno en el que designó al bolívar como moneda nacional, y decretó el canto "Gloria al Bravo Pueblo" como himno nacional, además de continuar las medidas que habían tenido éxito en su anterior período, con la ganadería y el agro recuperándose de la caída en el pasado. Luego de cinco años pasó el mando a Joaquín Crespo. La introducción del positivismo y la creciente oposición del sector estudiantil condujeron al cierre de la universidad por parte del gobierno. Como resultado, el Congreso eligió a Guzmán Blanco para presidir entre 1886 y 1888, quien se retiró en 1887, dejando a Hermógenes López como presidente interino para la transición.

Le siguió Juan Pablo Rojas Paúl, quien se alejó de la línea centralista mantenida hasta el momento, creó la Academia Nacional de la Historia, y enfrentó disturbios y alzamientos anti-guzmancistas. En 1890 fue elegido Raimundo Andueza Palacio para el período constitucional de dos años, pero su intento por extender su mandato provocó la Revolución Legalista de 1892 encabezada por Joaquín Crespo, que le derrocó del poder. Crespo asumió la dirigencia como producto del movimiento en octubre de ese año, y aprobó una nueva constitución estableciendo la duración de la presidencia a cuatro años, y el voto directo. Mientras era jefe del país los recursos públicos fueron mal invertidos y se crearon nuevas deudas para el país, pero permaneció popular entre sus soldados. Su candidato a sucesor, Ignacio Andrade, venció en las elecciones de 1897, pero su contrincante José Manuel Hernández, desconoció los resultados acusando fraude, y se rebeló en Queipa, Valencia en 1898. Crespo, al mando de las tropas del gobierno, pereció en la Batalla de la Mata Carmelera, pero el alzamiento fue derrotado. El saldo al final del siglo XIX fue de recesión económica, pero de avances en la cultura, la tecnología y el urbanismo.

La Revolución Liberal Restauradora de 1899 organizada por Cipriano Castro y Juan Vicente Gómez hizo huir del país a Andrade, llevando al poder a Castro, quien sin embargo, ratificó en sus cargos a algunos ministros del derrotado gobierno, desvirtuando el lema principal de su campaña: «"Nuevos hombres, nuevos ideales, nuevos procedimientos"». En 1901, la Asamblea Nacional Constituyente lo eligió Presidente y como segundo vicepresidente a Gómez. Al igual que sus predecesores, por su autoritarismo combatió sediciones internas. La más sobresaliente de éstas fue la Revolución Libertadora, liderada por el banquero Manuel Antonio Matos, que culminó con el triunfo de Castro en 1903 tras las batallas de La Victoria y de Ciudad Bolívar, y cerrando el capítulo de las grandes rebeliones caudillistas. Además, su gestión siguió una fuerte línea anti-imperialista contra las grandes potencias extranjeras, negándose a cancelar la deuda nacional con el Reino Unido y Alemania. Debido a esto, debió encarar el bloqueo naval que impusieron estos países.

Debido a una enfermedad, en noviembre de 1908 Castro se dirigió a París con el propósito de someterse a tratamientos pertinentes. Días después, su vicepresidente y amigo Gómez perpetró un golpe de estado en diciembre de ese año, traicionando a Castro y prohibiendo su regreso a Venezuela. Gómez fue oficialmente presidente desde 1910, cuando el Congreso lo eligió para un término de cuatro años, pero decidió permanecer el poder, y para solventar la crisis posterior suspendió las elecciones. Gómez sería designado como Presidente Constitucional por períodos de siete años establecidos por una nueva constitución, con gobernantes títere presidiendo por poco tiempo y actuando de fachada a Gómez. Fue inmisericordioso tanto con opositores como con todo aquel que le cuestionase. Muchos prisioneros políticos cumplieron su condena realizando trabajos forzados para construir diversas carreteras por todo el país. Para resistir protestas del estudiantado, cerró la Universidad Central de Venezuela durante diez años, con lo cual sumió al país en un franco atraso educativo. También promulgó la primera Ley del Trabajo, creó bancos para obreros y agricultores, inició la explotación petrolera y logró la cancelación de la deuda externa en 1930. El movimiento opositor más recordado de su época fue protagonizada por los estudiantes universitarios en 1928, de donde surgirían nuevos líderes políticos. En 1929 también tuvo lugar un intento de golpe de estado en los cuarteles de Caracas tras los fallidos alzamientos de los generales, Emilio Arévalo Cedeño y José Rafael Gabaldón, así como la toma de Curazao por Rafael Simón Urbina y la invasión del "Falke" liderada por Román Delgado Chalbaud. La mayor contribución del general Gómez fue la pacificación definitiva del país, al exterminar a los caudillos importantes y crear la Academia Militar de Venezuela, como base de un Ejército Nacional consolidado. Su régimen es considerado como la dictadura más férrea que ha tenido Venezuela y Latinoamérica.

Gómez murió en 1935, y el General Eleazar López Contreras fue designado Encargado de la Presidencia hasta 1936, y luego Presidente Constitucional por siete años. Con él se inicia la transición a la democracia: decreta amnistía para los prisioneros políticos y restablece la libertad de prensa. En los Carnavales de este año una gran manifestación pública frente al Palacio de Miraflores demandando por mayores libertades civiles, a las que López accedió en parte con su "Programa de febrero". En julio reformó la constitución, reduciendo el período presidencial de 7 a 5 años, y focalizó sus políticas gubernamentales en la creación de programas asistenciales de salud pública. Además, concretó obras de suma importancia para la nación como la creación de la Guardia Nacional de Venezuela en 1937, la apertura del Museo de Bellas Artes y del Museo de Ciencias en 1938, y la creación del Banco Central de Venezuela en 1940.

Al término de su mandato en abril de 1941, el Congreso designó como Presidente a Isaías Medina Angarita, militar que promulgó una Ley de Hidrocarburos en 1943 que traería más dividendos monetarios al país y restringiría la participación de las empresas multinacionales. En su gestión se decretó la elección directa de los diputados, el sufragio femenino y la legalización de todos los partidos, se permitió el regreso de todos los exiliados políticos y la liberación de la totalidad de los presos políticos. También creó el primer plan de cedulación venezolana en 1944, activó una reforma agraria, e inició la modernización de las ciudades. Apoyó a los aliados en la Segunda Guerra Mundial e intentó la anexión de las Antillas Neerlandesas. El aspecto más negativo fue la firma del Tratado de Límites de 1941 entre Colombia y Venezuela. Aunque continuó con mayor rapidez el camino a la democracia, existían muchos adversarios políticos, como Rómulo Betancourt y su partido Acción Democrática. Desde su seno se fraguó en 1945 un golpe de estado con ayuda de un grupo de jóvenes militares dirigidos por los Tenientes Coroneles Marcos Pérez Jiménez, Luis Llovera Páez y Carlos Delgado Chalbaud, quienes disentían con el tipo de elección presidencial empleada y con muchas medidas de Medina.

Se instauró entonces una Junta Revolucionaria de Gobierno presidida por Betancourt. En breve tiempo la Junta llamó a comicios libres y directos. El famoso escritor Rómulo Gallegos resultó ser el primer presidente venezolano electo de esta forma, asumiendo en febrero de 1948. A pesar de eso, Gallegos no completó su período debido al golpe de estado del 24 de noviembre de ese año, en el que se hizo con el control del país una Junta Militar integrada por los mismos rebelados de hace tres años, y que derogó la constitución de 1947. De los triunviros, Carlos Delgado Chalbaud era candidato a presidir el país luego de que la Junta Militar convocara a elecciones, pero es secuestrado y asesinado por un grupo liderado por Rafael Simón Urbina y su sobrino Domingo Urbina el 13 de noviembre de 1950. Tras el incidente, Germán Suárez Flamerich fue designado presidente provisional. Aunque no se ha podido confirmar, es creencia popular que el autor intelectual del magnicidio fue Marcos Pérez Jiménez, el segundo triunviro que ejercía como Ministro de Defensa.

Pérez Jiménez permaneció en tal cartera hasta diciembre de 1952, fecha de las votaciones para una Asamblea Constituyente. Al observar que el partido opositor URD se estaba llevando el mayor porcentaje de votos, el oficialista Frente Electoral Independiente desconoció los resultados y suspendió las elecciones. Dos días más tarde, los poderes de la Junta fueron transferidos en su totalidad a Pérez Jiménez, quien en abril de 1953 es proclamado Presidente Constitucional por cinco años. Su gobierno, que en ese año impulsó una constitución, tuvo el formato de una dictadura personalista que no vaciló en proscribir a la oposición, coartar libertades civiles y censurar sistemáticamente a los medios de comunicación. Su principal organismo policial, la Dirección de Seguridad Nacional en su Sección Político-Social (f. 1949), tuvo la tarea de arrestar a opositores, recluirlos en el Campo de Concentración de Guasina, y también ejecutarlos. Tuvo especial apoyo del gobierno de los Estados Unidos por ser parte de la red de distribución petrolera y por su lucha contra el comunismo. Sin embargo, su régimen también se caracterizó por un progreso en infraestructura sin igual para el país, lo que posteriormente se conocería como la «dictadura desarrollista» de Venezuela. La explosión de la infraestructura visionaria y tecnológicamente puntera, el fomento especial a la inmigración europea que cambió a la sociedad venezolana, y la completación de ambiciosos y emblemáticos proyectos de obras públicas, se enmarcaron como la práctica de una corriente de pensamiento nacionalista denominada el Nuevo Ideal Nacional. A pesar de esto, la antipatía generada por sus actos represivos y sus intenciones de perpetuarse en el poder, incrementó el descontento en su contra.

En diciembre de 1957 se organizó un plebiscito para definir su permanencia para otro período en el poder. Los boletines oficiales le dieron la victoria, aunque era de sobre entendimiento en la población en general que se trató de un fraude orquestado. Esto produjo un fraccionamiento en las Fuerzas Armadas que lo habían apoyado hasta entonces, y que protagonizó una rebelión fallida en el día de Año Nuevo de 1958. La crisis política que se originó entonces desestabilizó las bases del régimen, concluyendo con su deposición por un movimiento cívico-militar en la madrugada del 23 de enero, lo que le obligó a huir hacia República Dominicana para posteriormente trasladarse a los Estados Unidos junto a su familia. Al día siguiente se organizó una Junta de Gobierno presidida por el contralmirante Wolfgang Larrazábal. Aunque se llamó a elecciones para ese año, la Junta rechazó varios conatos de golpe por parte de militares perezjimenistas. En octubre se procedió a la firma del Pacto de Puntofijo, que disponía de la alternancia en el poder de los partidos Acción Democrática, COPEI y URD, para encauzar la futura vida política del país y excluyendo a los partidos de izquierda. Larrazábal renunció a la junta en noviembre para participar en los comicios, siendo sustituido por Edgar Sanabria. La elección a Presidente se decantó finalmente por Rómulo Betancourt, quien asumió el cargo en febrero del año siguiente.

Las obras más perdurables de Pérez Jiménez se manifiestan en la construcción de gran parte de la infraestructura vial en el Distrito Federal. La Autopista Caracas-La Guaira, Autopista Tejerías-Valencia, Autopista Francisco Fajardo, el Paseo de los Próceres y otras muchas fueron obras del Gobierno Militar. Una Junta cívico - militar de Gobierno, presidida por el Contralmirante Wolfgang Larrazábal Ugueto se encarga del gobierno de transición hasta las nuevas elecciones presidenciales. Una medida populista de esta Junta de Gobierno, denominada Plan de Emergencia, por la cual se le daba una especie de salario mientras conseguía trabajo a todos los campesinos y obreros que lo solicitaran, dio origen a un masivo éxodo rural que se dirigió a las ciudades, especialmente, a Caracas, lo cual dio origen, a su vez, a una macrocefalia de la capital con respecto al resto del país, y al rápido y descontrolado incremento de las áreas de poblamiento marginal en las barriadas de las principales ciudades.

La nueva era democrática trajo consigo cambios a nivel político y económico. En su gobierno no se otorgó más concesiones petroleras a las empresas que operaban en el país, se constituyó la Corporación Venezolana del Petróleo, y se creó la OPEP en 1960, por iniciativa de Juan Pablo Pérez Alfonzo. Paralelamente se adelantó una ley de Reforma Agraria que redistribuiría los terrenos improductivos con el fin de detener el declive de la producción agrícola, debido al "boom" petrolero. Igualmente, se sancionó una nueva constitución en 1961. El nuevo orden tuvo sus antagonistas. Durante un desfile militar, el Presidente sufrió un atentado planeado por el dictador dominicano Rafael Leónidas Trujillo con el fin de reiniciar la dictadura en Venezuela. Los grupos izquierdistas excluidos del Pacto iniciaron una insurgencia armada, organizados en los focos guerrilleros de las Fuerzas Armadas de Liberación Nacional, auspiciadas por el Partido Comunista. En 1962, intentaron la desestabilización vía los cuerpos militares, protagonizando dos fallidas revueltas, una en Carúpano y otra en Puerto Cabello. Paralelo a esto, Betancourt promovió una doctrina internacional, en la que sólo reconocía a los gobiernos electos por votación popular y rompía con los regímenes dictatoriales.

En las siguientes elecciones de 1963 resultó electo Raúl Leoni. Su gobierno comenzó con una coalición de partidos a la que se denominó la "Amplia Base", integrando a AD, URD y el FND. Aunque su gobierno fue de concordia general y entendimiento entre los sectores de la población, tuvo que lidiar con numerosos ataques de la guerrilla. De entre estos destaca la invasión a las playas de Machurucuto en mayo de 1967. Viendo que rendía pocos frutos, la mayor parte de los guerrilleros abandonaron la lucha armada por la política electoral en ese año. El gobierno de Leoni también se destacó por la conclusión de obras públicas y el desarrollo cultural.

Rafael Caldera resultó vencedor en los siguientes comicios. Antes de tomar posesión, en 1969, estalló la insurrección de Rupununi en Guyana, que representó una oportunidad para anexar parte del Esequibo que reclamaba Venezuela. En este contexto, firmó el Protocolo de Puerto España en 1970, congelando las reclamaciones por 12 años. Pactó la tregua definitiva con la guerrilla y garantizó su integración a la vida política, legalizando el PCV. En 1974 asumió la presidencia Carlos Andrés Pérez. En su gobierno se hizo notable el profuso ingreso de divisas por concepto del petróleo y los altos estándares de vida que adquirió la población, llegándose a la acepción de la "Venezuela Saudita", en la que creció aceleradamente el Producto Interno Bruto. En 1975 nacionalizó la industria del hierro, y al año siguiente, la del petróleo, creando la empresa estatal PDVSA. Tanto Caldera como Pérez rompieron parcialmente con la Doctrina Betancourt.

En 1979, Luis Herrera Campíns es investido como Presidente. Inauguró múltiples instalaciones culturales y deportivas, así como el Metro de Caracas. Aunque los ingresos petroleros siguieron acrecentándose, ello no impidió que el país se endeudara en las finanzas internacionales, forzando el apego a los dictámenes del Fondo Monetario Internacional. 

En 1983 se produjo la devaluación del bolívar en el llamado "Viernes Negro", desatando una fuerte crisis económica. En el gobierno del próximo presidente, Jaime Lusinchi, se haría poco para contrarrestarla. Los índices de corrupción se vieron incrementados, y la política económica siguió manteniendo la línea rentista. Por otra parte, en 1987 se vivió el mayor momento de tensión militar internacional en los últimos años, cuando la corbeta colombiana "A.R.C. Caldas" ingresó clandestinamente en aguas del Golfo de Venezuela. Fue una crisis que se originó en la disputa por la soberanía en dicho golfo entre ambas naciones, y sobre la que no se había alcanzado acuerdo. Los medios hablaban de una posible guerra, pero el conflicto se resolvió por medio del diálogo y el retiro de la corbeta.

Carlos Andrés Pérez es nuevamente elegido en 1988. Buscando solventar la crisis, adoptó medidas que originaron grandes protestas como el Caracazo de 1989. Se produjeron dos intentos de golpe de Estado liderados el teniente coronel Hugo Chavez en febrero y en noviembre de 1992. Pérez fue finalmente destituido por el Congreso en 1993. Octavio Lepage fue Presidente provisional por pocos días, hasta que el historiador y parlamentario Ramón José Velázquez fue designado como interino por el Congreso Nacional.

Caldera llega al poder por segunda vez en 1994. Tuvo que manejar una fuerte crisis bancaria en 1994. El derrumbe e intervención de una decena de bancos culminó con la fuga de capitales, provocando también el quiebre de empresas. Para frenar la crisis, inició una política de privatizaciones, pero la grave situación económica continuaría. La situación catalizó el decaimiento de los partidos políticos que habían estado activos desde mediados del siglo XX.

Con la elección de Hugo Chávez como presidente de Venezuela en 1998, dando inicio a un proyecto ideológico y social que denominaron Revolución bolivariana. Chávez hizo un llamado a una Asamblea Nacional Constituyente al inicio de su gobierno para reformar la constitución. Se alargó el periodo presidencial de 5 a 6 años, se definió un nuevo límite de dos términos presidenciales consecutivos y se cambió el nombre de República de Venezuela a República Bolivariana de Venezuela tras la aprobación de la nueva constitución por referéndum en abril del mismo año. Lo primero que tuvo que afrontar como presidente fue la tragedia de Vargas a finales de 1999. Para el 2002 comenzaron grandes protestas en su contra. El 11 de abril del 2002 se consumó un golpe de estado contra Chávez que lo derrocó por dos días, el dirigente de Fedecámaras (cámara de empresas) Pedro Carmona se autoproclama presidente de Venezuela violando el hilo constitucional, y aprovecha junto a otro grupo de personas de disolver el Tribunal, los ministerios, la Asamblea y en general la constitución lo cual le dejaba pleno poder para gobernar la nación. Chávez fue restituido el 13 de abril.

La presidencia de Chávez se caracterizó por la creación de programas sociales, su fuerte política anti-estadounidense y la adopción del "Socialismo del siglo XXI".

Tras el fallecimiento de Hugo Chávez en 2013, el CNE convoca a elecciones presidenciales y es electo presidente Nicolás Maduro. Tras el mandato de Maduro, se agudiza la escasez en Venezuela, esta situación se da en productos con precios regulados, como la leche, diversos tipos de carne, pollo, café, arroz, aceite, harina pre-cocida, mantequilla; así como también, productos de primera necesidad como papel higiénico, productos de aseo personal, medicinas para tratar el cáncer, entre otros. Son frecuentes las filas de personas que quieren comprar productos básicos en supermercados y otros negocios. Esta situación ha llevado al gobierno venezolano a impulsar medidas como el "Sistema Biométrico de Abastecimiento".

Para las elecciones parlamentarias del 2015 la oposición obtiene 112 de los 167 diputados de la Asamblea Nacional (56,2 % de los votos), y la primera victoria electoral de peso para la oposición en 17 años.

En febrero de 2016, el presidente Nicolás Maduro, anunció el aumento de la gasolina, quedando en 1 Bs. para la de 91 octanos y en 6 Bs. para la de 95 octanos. Representando un 1328,57 % y 6085,56 % de incremento en el precio que se manejaba desde 1996. De igual forma, el salario mínimo se aumentó a 11.578 Bs.y el CestaTicket se incrementó a 13.275 Bs. Por su parte, el sistema marginal de divididas (SIMADI) pasa a ser "sistema complementario flotante", pasando de un dólar a 6,13 Bs., a 10 Bs. A finales de abril, es re-inaugurado el Teleférico Mukumbarí del estado Mérida. De igual forma, la Cervecería Polar paralizó la producción de malta y cerveza en el país, por no importar materia prima para su fabricación debido a la falta de divisas adjudicadas por el gobierno venezolano. Nicolás Maduro, anuncia el aumento del salario mínimo en un 30 % quedando en 15.051 bolívares y el cesta ticket a 3.5 UT ubicándose en 18.585 bolívares. El 1 de mayo del mismo año entra en vigencia el nuevo huso horario del sistema UTC -4:30 horas a UTC -4:00 horas en toda Venezuela.
Se realizan fuertes protestas y marchas en contra del gobierno de Nicolás Maduro entre abril y julio de 2017, exigiendo elecciones. Por su parte, Nicolás Maduro anunció llamar a una Asamblea Nacional Constituyente (ANC). Sectores opositores al gobierno rechazaron el anuncio y expresaron inconstitucional la medida. Para el 16 de julio, la oposición al gobierno de Maduro realizó una consulta popular donde 7.535.529 venezolanos rechazan la ANC y da posteas a la Asamblea Nacional (AN) de tomar decisiones. El gobierno desconoció esta consulta. De igual forma, la comunidad internacional manifiestan su descontento y desconocimiento de la ANC, entre los países que se expresaron, están Argentina, Brasil, Colombia, Estados Unidos, entre otros; así como organismos internacionales como la OEA en los cuales plantearon la suspensión de la ANC, por su parte Mercosur, anunció la posibilidad de expulsar a Venezuela de su organismo. Las elecciones de la Asamblea Nacional Constituyente se realizó el 30 de julio, donde el Consejo Nacional Electoral dio a conocer que 8.089.320 personas sufragaron. El mismo día, se reportaron al menos 15 muertos en las protestas que surgieron a raíz del rechazo a la ANC.

En la madrugada del 6 de agosto de 2017, un grupo de militares toman por asalto el Fuerte Paramacay, Municipio Naguanagua del Estado Carabobo, presuntamente comandados por el capitán Juan Caguaripano. En octubre se celebraron elecciones para elegir gobernadores, donde resultaron electas 28 del partidos del gobierno, mientras que cuatro fueron ganadas por AD y el estado Zulia, pese haber ganado Guanipa, ante la negativa de juramentarse con la ANC, el consejo legislativo del estado, declaró vació de poder y designó un nuevo gobernador. Para noviembre del mismo año, tras ajustes a los precios de la carne, este rubro comenzó a desaparecer de los locales dedicados a su venta. Después de los anuncios ofrecidos por el presidente Nicolás Maduro, a principios de noviembre de 2017, como el aumento salarial y la introducción del billete de 100 000 bolivares; economistas y medios de comunicación afirmarón que Venezuela, ha iniciado una hiperinflación, tras arrojar el pasado mes de octubre una inflación del 50,6 %. Luego el gobierno de Maduro, llamó a refinanciar la deuda externa. La Asamblea Nacional Constituyente promueve la Ley contra el Odio, por la Convivencia Pacífica y la Tolerancia, quien para algunos sectores limita la libre expresión dentro del territorio venezolano. Por su parte, desde el exterior se imponen fuertes sanciones económica contra partidarios afectos al gobierno, incluyendo al presidente Nicolás Maduro, por gobiernos como Estados Unidos y Canadá. A finales del año miembros del gobierno y fracciones de la oposición venezolana, realizaron diferentes encuentros con el fin de lograr acuerdos políticos. El 3 de diciembre de 2017, Nicolás Maduro dio a conocer la creación de la criptomoneda "petro", para evitar el bloqueo financiero.

A principio de enero de 2018, ocurrieron saqueos en diversas ciudades de Venezuela.
En la madrugada del 15 de enero de 2018, el área de El Junquito, fue acordonado por cuerpos de seguridad del estado, incluyendo militares. Luego se dio a conocer el paradero de Óscar Pérez en este sector, quien difundió por las redes sociales su situación, posteriormente manifestó su rendición, mientras las fuerzas de seguridad seguían disparando. En primera instancia resultó muerto Heiker Vásquez, quien ha estado relacionado con los grupos llamados "colectivos". La versión oficial de los cuerpos de seguridad, exclamaron que dos funcionarios resultaron muertos y al menos cinco heridos. El mismo día, Nicolás Maduro rindió su Memoria y cuenta del 2017, ante la Asamblea Nacional Constituyente. Al día siguiente, el ministro de Interior y Justicia, Néstor Reverol en rueda de prensa declaró la muerte de Óscar Pérez y otros seis miembros, quien calificó como una "célula terroristas". Por su parte, Luisa Ortega Díaz desde el exilio exclamó, que la muerte de Pérez, fue una violación de los derechos humanos. Por otro lado, la Asamblea Nacional investigará la muerte de Pérez, así también señaló que la presentación de memoria y cuenta por parte del presidente viola la Constitución de Venezuela, ya que está debió realizarse ante tal institución. El mismo día, algunos medios difundieron videos donde se observan funcionarios ejecutando un lanza cohete RPG-7, contra la vivienda donde se localizaba Óscar Pérez.

El 9 de abril de 2018, desde Colombia el Tribunal Supremo de Justicia venezolano en el exilio dicta prisión preventiva contra el presidente Nicolás Maduro, por corrupción. Tras la crisis, trabajadores del sector petrolero protestan por mejoras salariales, mientras tanto en PDVSA se prohíbe renunciar.
Para el 20 de mayo de 2018, se realizaron elecciones presidenciales dónde resultó electo nuevamente el presidente Nicolás Maduro, para el período presidencial 2019-2025, con una abstención de casi el 60 % en los centros electorales Evento desconocido por la comunidad internacional y la oposición venezolana. Tras las elecciones, el candidato a las elecciones Henri Falcón, anunció el desconocimiento de los resultados.

Para el 10 de enero de 2019, Nicolás Maduro es juramentado como presidente de Venezuela en el Tribunal Supremo de Justicia. Al siguiente día tras un cabildo abierto, Juan Guaidó, presidente de la Asamblea Nacional de Venezuela, anunció que se apegaría a los artículos 333, 350 y 233 de la constitución para, en sus palabras, ""lograr el cese de la usurpación y convocar elecciones libres con la unión del pueblo, FAN y comunidad internacional"", recibiendo apoyo de organismos internacionales como la OEA y países de la región, como Argentina, Brasil y Chile; sustentado en el artículo 233 de la Constitución de Venezuela. Juan Guaidó se juramenta el 23 de enero de 2019 en una multitudinaria marcha en la ciudad de Caracas como presidente interino de Venezuela, apegándose al artículo 233 de la constitución de Venezuela recibiendo el posterior respaldo de más de 50 países. Esto genera la Crisis presidencial de Venezuela de 2019. Maduro ordenó cerrar las fronteras e impedir cualquier ingreso. Desde el 23 de febrero de 2019, se realizaron fuertes protestas en las líneas fronterizas con Brasil y Colombia, para permitir la entrada de ayuda humanitaria proveniente principalmente de Estados Unidos, la cual el gobierno venezolano no accedió. Por su parte al menos 160 uniformados pertenecientes a organismos de seguridad desertaron a Colombia y unos 3 a Brasil, desconociendo el gobierno de Nicolás Maduro. Entre marzo y abril, ocurren masivos apagones a nivel nacional, haciendo crecer las manifestaciones por el suministro eléctrico y la falta de agua. El gobierno anunció racionamientos de luz en todo el país. Maduro realizó cambios en su gabinete ministerial.

En la mañana del 30 de abril de 2019, Juan Guaidó junto a Leopoldo López dan anuncio del inicio de la operación Libertad desde las inmediaciones de la Base Aérea La Carlota.



</doc>
<doc id="4665" url="https://es.wikipedia.org/wiki?curid=4665" title="Economía de Venezuela">
Economía de Venezuela

La economía de Venezuela está orientada a las exportaciones de materias primas. La principal actividad económica de Venezuela es la explotación y refinación del petróleo para la exportación, la extracción y refinación está a cargo la empresa estatal Petróleos de Venezuela.

La producción inicial data del año 1875, con la participación de la Compañía Petrolera del Táchira en la hacienda «La Petrolea» en los andes venezolanos; luego se construyó la primera refinería de la cual se obtenían productos como el queroseno y el gasóleo. El reventón del pozo Zumaque I en el Cerro La Estrella de Mene Grande en 1914 (aún en producción en junio de 2014) marca el comienzo de la explotación petrolera comercial a gran escala, accionando una gran cantidad de eventos que cambiaron drásticamente el rumbo del país. En 1960 y por medio de la iniciativa y participación de Venezuela dentro del mercado petrolero mundial es fundada la Organización de Países Exportadores de Petróleo (OPEP).

A partir de 1950 y durante más de 30 años, la economía venezolana experimentó un fuerte crecimiento y desarrollo económico constante, esto trajo como consecuencia que muchas personas de diferentes parte del mundo emigraran hacia Venezuela durante la Década de 1950, Década de 1960 y Década de 1970. Ya en la Década de 1980 y también parte de la Década de 1990, la economía venezolana se contrajo debido a la caída de los precios del petróleo, la inflación se disparó hasta alcanzar picos anuales de 84 % en 1989 y 99 % en 1996. Pero a pesar de estas elevadas tasas de inflación, la economía venezolana seguía siendo estable y figuraba como la cuarta economía más grande de América Latina.

Desde el final de la crisis de principios de los años 1990, la economía venezolana tuvo más de una década expansiva de crecimiento macroeconómico, por encima de la media del resto de América Latina. 

De acuerdo con el BCV, Venezuela recibió de 1998 a 2008 alrededor de 325 mil millones de dólares a través de la producción petrolera y la exportación en general, y de acuerdo con la OPEP para noviembre de 2018 la extracción petrolera cayó a 1 170 000 barriles diarios.

A pesar de las tensas relaciones con los Estados Unidos, este país es un vendedor de bienes y servicios a Venezuela. Las exportaciones estadounidenses a Venezuela incluyen maquinarias, productos agrícolas, instrumentos médicos y vehículos. Venezuela era uno de los principales proveedores de petróleo extranjero a los Estados Unidos, ubicándose en 2017 en cuarto lugar con apenas un 41 % de los barriles extraídos. Cerca de 500 empresas de Estados Unidos en diferentes áreas económicas están representadas en Venezuela.

Desde que Hugo Chávez Frías impuso estrictos controles de cambio en el año 2003, en un intento de implementar su sistema económico de nacionalización y expropiaciones y evitar la fuga de capitales, se han producido una serie de devaluaciones de la moneda.. En 2007 Venezuela venía presentando una inflación de 22.5% una de las más altas de la región y casi de los tres últimos años de la primera década del siglo. Para 2015, Venezuela tiene la tasa de inflación más alta del mundo, superando el 100 % interanual, convirtiéndose en la tasa más alta en la historia del país.. En noviembre de 2017 Venezuela entra por primera vez en hiperinflación con una inflación del 50,6 % solo en el mes de octubre. el 2018 es un año pésimo para la economía con retraso en el pago en sus Bonos, perdida del volumen de sus exportaciones de petróleo , perdida de la producción nacional de aluminio, cemento. hierro. al final de 2018 la inflación llega a 1.304.494% (más de un millón %) los pronósticos para este año no son alentadores según el FMI el PBI real proyectado para 2019 caerá un porcentaje negativo -25% y con una Tasa de inflación, precios promedio al consumidor de 10 millones por ciento. En un informe de la Asamblea Nacional de septiembre 2019 ubicó en 65,2% la inflación para el mes de agosto al igual que la variación interanual del índice inflacionario es de 135.379,8% (agosto 2018-2019), mientras que la inflación acumulada durante el año 2019, asciende a 7 374.4%.

Antes de convertirse en un país petrolero, Venezuela fue durante todo el Siglo XIX y principios del Siglo XX un país netamente agropecuario. El eje de la economía venezolana se basaba principalmente en la producción agropecuaria particularmente del café (del que llegó a ser segundo productor a nivel mundial, después de Brasil). Los principales productos que Venezuela exportaba en esa época eran el café, el cacao, el ganado vacuno, el azúcar, papelón, tabaco, balatá, cueros de res y caucho. 

Pero a pesar de todo esto, el PIB per cápita de Venezuela era notablemente inferior en comparación a otros países de América del Sur (Argentina, Chile, Uruguay), e incluso era inferior a la de países demográfica y geográficamente comparables como Perú, Colombia, Ecuador y Bolivia. 

Si bien en 1875 se había descubierto uno de los primeros pozos petrolíferos venezolanos, dicha producción petrolera seguiría siendo pequeña, teniendo una participación mínima en la economía del país, por lo menos hasta el año 1920. 

Cabe mencionar que 1920 constituye un punto de inflexión en la economía venezolana, a partir de ese entonces, las exportaciones agrícolas disminuirán exponencialmente en detrimento de las exportaciones petroleras. 

Para 1929, Venezuela fue el segundo mayor país productor de petróleo (solo por detrás de Estados Unidos) y el mayor exportador de petróleo del mundo. Con un espectacular desarrollo de la industria, el sector del petróleo había comenzado a dominar todos los demás sectores económicos del país.

Con la expansión petrolera vino el abandono del campo, debido a que la producción agrícola estaba primordialmente en manos de muy pocos terratenientes que ofrecían salarios minúsculos para las pésimas condiciones de vida que brindaba el campo. Por lo tanto no podían competir con los salarios que ofrecían las empresas petroleras en sus concesiones. El abandono del campo inundó al mercado laboral con un crecimiento abrumador de la oferta de trabajo.

A partir de 1925, gracias a la explotación del petróleo a gran escala Venezuela había superado el PIB per cápita de Perú, Colombia, Ecuador y Bolivia. A partir de 1926 experimentó un vertiginoso crecimiento que haría de Venezuela el país de América Latina de mayor renta per cápita lo que motivo la llegada de numerosos inmigrantes europeos y latinoamericanos. Entre 1950 y 1995 Venezuela siguió siendo el país de América Latina con mayor renta per cápita, aunque a partir de 1996 esta empezó a disminuir. La inflación en los años 90 fue entre 32 % (1992) y 100 % (1996).

La economía venezolana se aprovechó de los altos precios del petróleo durante la crisis petrolera de la década de 1970 y del superávit que esta le proveía; esto fue el detonante para que el Gobierno se endeudara con el exterior. Cuando la deuda externa se tornó impagable en 1983 se tuvo que devaluar la moneda en el episodio conocido como el Viernes Negro. A partir de entonces las políticas económicas de los gobiernos de Luis Herrera Campíns y Jaime Lusinchi no fueron capaces de frenar la espiral inflacionaria, generando desconfianza en las inversiones y pérdida de credibilidad en la moneda nacional. Algunas de las políticas que emplearon estos gobernantes para frenar los efectos estructurales fueron controles de cambio a través de RECADI (Luis Herrera Campins) y un control de precios (Jaime Lusinchi), medidas que devinieron en corrupción administrativa y mercado negro de divisas y bienes. Sin embargo la quiebra estructural del mercado interno, la falta de soberanía económica y alimentaria, generó una escasez gradual. En 1988 resulta electo presidente Carlos Andrés Pérez, apoyado en un discurso populista que apelaba a la justicia social. Con un gran respaldo electoral, el gobierno de Pérez, en lugar de buscar un cambio hacia la inclusión social, giró a liberar la economía, imponiendo su desregulación a través de un programa de ajustes macroeconómicos promovido por el Fondo Monetario Internacional (FMI), al que se le llamó "Paquete Económico", concebido para generar cambios sustanciales en la economía del país dentro del modelo neoliberal. Se anunciaron medidas de aplicación inmediata y otras de aplicación gradual en plazos breves. El paquete comprendía decisiones sobre política cambiaria, deuda externa, comercio exterior, sistema financiero, política fiscal, servicios públicos y política social. Sin embargo, la liberación de precios y la eliminación del control de cambio generó un reajuste sumamente brusco para las personas de menores ingresos, que eran la gran mayoría, lo que derivó en más hambre y desempleo. El descontento popular se manifestó en los trágicos sucesos del Caracazo (1989) lo cual no fue obstáculo para que se aplicaran con relativo éxito algunas de las medidas propuestas. Sin embargo dos intentos fallidos de golpe de estado (1992) liderados por el teniente coronel Hugo Chávez agravaron la crisis económica en una vorágine de sucesivas devaluaciones y una volatilidad inflacionaria, lo que llevó a que se perdieran miles de empleos y el país cayera en una grave situación de pobreza, de la cual algunos economistas y políticos creen que el país no se ha recuperado completamente.

En 2001 el crecimiento del Producto interno bruto o PIB fue del 3,4 %. Un aumento significativo de los precios internacionales del petróleo permitió recuperar la economía de una fuerte recesión sufrida durante el año 1999. Sin embargo, un sector no petrolero relativamente débil, una alta fuga de capitales y una caída temporal en los precios del petróleo evitó que la recuperación fuera mayor.

A principios de 2003 se estableció un control de cambio, de un esquema con tasa de cambio libre flotando en bandas a un esquema de precio fijo controlado por el gobierno, haciendo al bolívar considerablemente. En 2003, como consecuencia de la grave inestabilidad política, diversos conflictos sociales y la paralización de actividades de la principal empresa estatal petrolera PDVSA, la economía venezolana tuvo una caída de su PIB del 7,7 %.

El 6 de febrero de 2003, el gobierno venezolano implanta un sistema regulatorio de cambio en la compra/venta de divisas extranjeras. La institución gubernamental encargada en ese entonces, CADIVI, inicialmente estableció el cambio de 1600 bolívares por dólar para la venta. El 3 de marzo de 2005 se devaluó la moneda frente al dólar, pasando el cambio oficial de 1920 a 2150 bolívares por dólar.

Durante el año 2004 Venezuela experimentó un crecimiento del 17,9 % en su PIB, aunado a la realización del referéndum revocatorio presidencial con el triunfo del presidente Chávez con el 60 % de los votos, el ambiente político se mejoró y afectó positivamente a la economía. La inversión social del gobierno mediante las llamadas misiones en los campos educativos, alimenticios y de salud, lograron incrementar la calidad de vida de los ciudadanos momentáneamente con más bajos recursos (37 % de la población).

En 2005 Venezuela presentó un balance positivo en sus cuentas externas (31 000 millones de dólares) ya que las exportaciones alcanzaron 56 000 millones de dólares, representado el tercer lugar en importancia en América Latina detrás de México y Brasil. En tanto las importaciones totalizaron 25 000 millones de dólares.

Venezuela concluyó 2005 con un crecimiento de la economía del 9,4 % del Producto Interno Bruto, ubicándose en el primer lugar entre los países del continente por segundo año consecutivo. Además en 2005 Venezuela registró la inflación más baja de los últimos siete años cayendo hasta un 8,9 % según cifras del Banco Central de Venezuela (BCV) y de la CEPAL.

Según el informe anual del BCV durante 2006, el PIB venezolano tuvo un incremento del 10,3 %. Ese año el sector no petrolero de la economía tuvo un incremento anual de 11,4 % y las reservas internacionales alcanzaron la cifra de 37 299 millones de dólares.

El 7 de marzo de 2007 el Gobierno anunció un proceso de reconversión monetaria, y la moneda llevó el nombre transitorio de Bolívar Fuerte (BsF). Su emisión fue controlada por el BCV, ente que estableció un cambio de 2,15 bolívares fuertes por dólar, lo que supone dividir entre mil (correr tres ceros a la izquierda) el bolívar que circulaba desde 1879. La nueva escala monetaria venezolana fue aprobada mediante decreto presidencial con la publicación en la Gaceta Oficial N.º 38.638 por iniciativa del Presidente Hugo Chávez con la intención de reducir estéticamente la inflación y facilitar el sistema de pagos nacionales adecuándose a los estándares internacionales respecto a las cifras y el número de billetes que debería portar cada persona.

En 2007 en su informe Panorama social de América Latina de ese mismo año, la CEPAL reconoció que Venezuela entre 2002 y 2006, disminuyó en ese período sus tasas de pobreza en 18,4 % e indigencia en 12,3 %, pasando de una pobreza de 48,2 % y una indigencia de 22,2 % en 2002, a 37,9 % y 15,9 % respectivamente en 2005 y a 30,2 % y 9,9 % respectivamente en 2006.

Al cierre del año 2007 y según las cifras reportadas por el BCV la economía venezolana tuvo un crecimiento de 8,4 % impulsado por la expansión de la inversión y del consumo, con lo que se llegó a 17 trimestres de crecimiento consecutivo del PIB desde finales de 2003, registrándose desde ese mismo periodo un crecimiento interanual promedio de 11,8 %, el consumo registro la tasa de variación más alta desde 1997, al crecer 18,7 %, Los sectores o actividades económicas que registraron el mayor crecimiento fueron comunicaciones (21,7 %), actividad financiera y seguros (20,6 %) y construcción (10,2 %).
A finales de agosto, el ministro Rodríguez repasó sus cifras, estimando ahora una inflación anual de 26 % y un crecimiento del PIB cercano al 1 %. Sin embargo, el PIB venezolano experimentó finalmente una caída de 3,3 %;

El presupuesto nacional de 2009 fue calculado estimando el ingreso de 60 dólares por barril de petróleo, pero a finales de marzo se reformuló a 40 dólares, para ajustar la caída de los precios del petróleo a nivel global de 2009 y 2010, lo que desencadenó a su vez una crisis energética interna.

A inicios de 2010, el ministro de Finanzas Jorge Giordani estimó un crecimiento de 0,5 %, pero diversos especialistas calcularon una caída de entre 1,7 % y 3 %. A mediados de abril, el FMI estimó que Venezuela continuaría en recesión en 2010, con una caída de 2,6 %.

Finalmente, el PIB cayó 1,4 %; dentro del contexto regional, Venezuela queda detrás del resto de Latinoamérica y el Caribe, que experimentó en promedio un crecimiento de 6 %. Luego de la crisis energética, Venezuela sería la única nación petrolera y una de las dos naciones americanas aún en recesión en 2010. La otra nación es Haití, que a inicios de año experimentó un devastador terremoto.

Para expertos de la CEPAL, la crisis energética, y la caída en la exportación de petróleo venezolano estuvieron entre las razones para que Venezuela entrara en recesión, que duraría 18 meses desde segundo trimestre de 2009 hasta el tercer trimestre de 2010. El gobierno venezolano culpó a la lenta recuperación económica mundial de alargar la crisis, así como a la reducción de las cuotas de producción petrolera dictadas por la OPEP. De acuerdo a la oposición venezolana, las políticas del presidente Chávez para intentar aplicar el socialismo del siglo XXI estaban detrás de la crisis y estarían llevando "la economía a la ruina".

En septiembre de 2010, el bolívar fue devaluado nuevamente, pasando de 2,15 bolívares por dólar, a un sistema de cambio dual de 2,60 y 4,30 bolívares por dólar, dependiendo del tipo de transacciones a realizar con dichas divisas. Para aquel entonces, ya el dólar en el mercado negro se cotizaba por sobre los 9 bolívares.

Venezuela en el 2011 experimentó un crecimiento de 4,2 % de su PIB. El PIB no petrolero subió 4,3 % y el petrolero 0,6 %. Por segundo año consecutivo la economía venezolana siguió teniendo la inflación más alta del continente ya que los precios de los bienes y servicios subieron 27,6 %, un poco más que en 2010 cuando fue 27,2 %. Las exportaciones venezolanas al exterior, principalmente petróleo, subió 42,8 % en 2011. En total, Venezuela exportó mercancías por un total de 93 896 millones USD. Logrando así una balanza comercial supervitaria. Las importaciones se incrementaron 18 %, al cerrar el año 2011 con un monto de 45 615 millones USD. Las reservas internacionales del país cerraron el año en 29 899 millones USD, la cifra es 433 millones USD menor al cierre de 2010. El informe del presidente del BCV, señala que por la vía de Cadivi se liquidaron 35 394 millones USD en todo el año. En tanto, a través del Sitme se negoció un total de 8777 millones USD durante 2011.

En 2012 la economía venezolana cerró con un crecimiento de 5,5 %, una inflación de 20,1 % y un desempleo de 6,4 % ligeramente más bajo que en 2011. Los sectores que más crecieron fueron finanzas y entidades bancarias (32,9 %) y construcción 16,8 %.

Para 2013, el gobierno nacional anunció un aumento del 20 % en los precios controlados de la carne de res, pollo, leche y quesos. . El coeficiente de Gini habría alcanzado 0,435 puntos (1 es la desigualdad absoluta y cero la igualdad absoluta).

Según su tamaño, la economía venezolana en la actualidad es la décima tercera economía de América Latina con un PIB de 62 mil millones de dólares. Si se divide el PIB por la cantidad de población que tiene Venezuela (más de 30 millones de habitantes), el resultado sale un PIB per cápita de 2 457 dólares de riqueza promedio anual por cada venezolano, convirtiéndose de esta manera en el país más pobre del continente después de Nicaragua (1 859 dólares) y Haití (765 dólares). 

Durante toda la Década de 1960, la economía venezolana se ha posicionado como la cuarta economía más grande e importante de América Latina. En la Década de 1970, la economía de Venezuela seguía figurando como una de las principales economías del continente latinoamericano, ocupando el cuarto lugar.

Durante toda la Década de 1980, Venezuela continuaba figurando como una de las economías más importantes de la región. Cabe recordar que el año 1989 se originó el Caracazo que trajo como consecuencia la drástica caída del PIB de -8,9 %. Pero a pesar de los disturbios sociales y las consecuencias económicas que generó el Caracazo de 1989, pues para la Década de 1990, Venezuela todavía seguía siendo la cuarta economía más importante y grande de la región. El año 1998 seria el último año de un gran periodo de estabilidad económica en Venezuela. Al año siguiente (1999) ingresaría al poder Hugo Chávez Frías. 

A pesar de que en 1998 la economía de Venezuela había descendido un puesto (superada por la colombiana), aún seguía manteniéndose dentro de las 5 economías más grandes de América Latina. El año 2011, sería el último año del máximo crecimiento que alcanzaría la economía de Venezuela en toda su historia al mando de Hugo Chávez Frías, llegando a alcanzar un PIB de 334 mil millones de dólares ese año. 

A partir de ahí, el año 2012, el PIB del país ya no crecería y comenzaría a disminuir lentamente. Pero sería desde 2015, cuando la rebaja mundial del precio del barril de petróleo golpearía aún más a la economía venezolana, la cual empezaría a desmoronarse de una manera mucho más rápida, fuerte y drástica, con una acelerada caída del PIB, hasta retroceder 9 puestos. Según las previsiones del Fondo Monetario Internacional, si no se hace nada por recuperar la producción, entonces se espera que la economía venezolana siga retrocediendo muchos puestos más. 

En la actualidad (2019), con alrededor de 30 millones de habitantes, Venezuela solamente produce solamente un 68% de lo que producía en 1998 y un 18% de lo que llegó a producir en 2011. 

Durante el período de estabilidad de precios entre 1951 y 1973, Venezuela presentó una de las inflaciones más bajas del mundo, la interanual promedio fue de 1,6 % con una tasa de crecimiento del PIB de 5,7 %, caracterizado por disciplina fiscal y el tipo de cambio fijo.

Desde hace algunos años, Venezuela ha tenido una de las tasas de inflación más altas del mundo. En el último lustro supera en este parámetro a todos los países de la región, cosa que no pasaba en la década de los noventa, cuando países como Brasil, Perú y México tenían una tasa inflacionaria muy superior a la de Venezuela.

En 2007 la inflación superó con creces la meta gubernamental de 11 %. El gobierno venezolano había emprendido una serie de medidas para frenar la inflación, como la disminución del Impuesto al Valor Agregado (IVA) de 16 % a 14 % estableciéndose en el 2009 a 12 %, así como la emisión del bolívar fuerte. (a partir de agosto 2018 el IVA es de 16%)

Una de las causas principales de la elevada inflación en el país, según algunos economistas, es la política del Estado de imprimir dinero inorgánico en la economía del que correspondería según la producción del país: hay mucho más dinero líquido persiguiendo muy pocos productos.

El 11 de junio del 2018 La Asamblea Nacional, da reporte de la inflación en Venezuela en vista que el BCV tiene dos años que no reporta los indicadores económicos , y solo para el mes de mayo fue de 110 %, y la anualizada de 24,500 % una cifra alarmante y sin precedentes, que muestra una situación de hiperinflación en Venezuela y que esta se sigue agudizando.

Luego de 3 años, el Banco Central de Venezuela el 29 de mayo de 2019 admitió una hiperinflación de 53.798.500% entre 2016 y abril de 2019, al publicar los datos del Índice Nacional de Precios al Consumidor y que el los cuatro primeros meses del año 2019 el BCV indican que la inflación acumulada hasta abril es de 1,047% 

El viernes 8 de febrero de 2013 el gobierno del entonces vicepresidente Nicolás Maduro informa las medidas económicas y cambiarías que entrarían en vigencia en Venezuela el 13 de febrero. El ministro de Finanzas, Jorge Giordani, y el presidente del BCV, Nelson Merentes, informaron que el precio del dólar que distribuía CADIVI se devaluaría de 4,30 bolívares hasta 6,30 bolívares. Esto correspondía a un 46,5 % de diferencia entre una cotización y la otra. Según el gobierno esto permitiría incrementar los recursos con los que cuenta el estado para seguir impulsando el crecimiento de la economía. En enero de 2015, La medidora de riesgo internacional Moody's le bajó la calificación a Venezuela de "CAA1" a "CAA3", lo que significa que la nación incrementa el riesgo de incumplimiento de pagos debido a la dependencia y devaluación del petróleo.

Para 2014 el gobierno actual realizó otra devaluación al comenzar a vender dólares a dos tasas diferentes: en 6,30 la tasa CADIVI (estudiantes, casos especiales, jubilaciones y pensiones, gastos consulares y diplomáticos, salud y alimentación) y el 11,30 la tasa SICAD (cupos para viajeros, las remesas familiares y las divisas para las líneas aéreas). El presidente Nicolás Maduro anunció la adhesión de CADIVI al Centro Nacional de Comercio Exterior a finales de 2014.

Aunque durante gran parte del 2014 el precio de la cesta petrolera venezolana se mantuvo en el promedio de 103 dólares el barril, la deuda externa de la República continuó creciendo velozmente y registró un salto de 8 % respecto a 2011 para ubicarse en 105 779 millones de dólares, de acuerdo con las estadísticas del BCV. Al cierre del 2014, el PIB registro una caída de 3,9 % durante tres trimestres consecutivos entrando en una nueva recesión, con un aumento exorbitante de la inflación que para diciembre de 2014 se encontraba en 64 % acumulado.

Para el 10 de abril de 2015, una nueva providencia del CENCOEX, restringe dólares para viajeros y designa que la banca pública (Banco de Venezuela, Banco del Tesoro o el Banco Bicentenario), serán los únicos operadores cambiarios de divisas.

Para julio de 2017, Kansei Le Car filial de Kansei Motors, empezó operaciones en Venezuela, está empresa ofrece servicios a la marca Peugeot.

El 22 de marzo de 2018, el presidente Nicolás Maduro anunció que eliminará tres ceros a la moneda nacional, con un nuevo cono monetario. Del mismo modo que logró sacar el billete Bs. F. 100 de circulación.

El 25 de julio de 2018, el presidente Nicolás Maduro anuncia una nueva reconversion monetaria, pasando de eliminar tres ceros a eliminar cinco ceros a partir del 20 de agosto de 2018, además de promover la iniciativa de nuevas regulaciones económicas.Ese 20 de agosto se oficializa el incremento del IVA (Impuesto al valor agregado) a 16% 

La economía de Venezuela históricamente ha estado orientada a las exportaciones del petróleo y sus derivados, y ha sido dependiente de las importaciones de importantes rubros, razón por la cual la cotización histórica del bolívar venezolano expresada en unidades de moneda local por dólar estadounidense ha sido clave en la toma de decisiones de los agentes económicos. Desde mediados del siglo se mantuvo la estabilidad y fiabilidad que había caracterizado al bolívar como signo monetario, cuya última cotización libre el 18 de febrero de 1983 fue de 4,30 bolívares por dólar. Desde entonces la devaluación constante del bolívar, complicaciones con el pago de la deuda externa, el acelerado deterioro del poder adquisitivo y la implantación de un control de cambio llamado "Régimen de Cambio Diferencial" (RECADI) —que funcionó entre el 28 de febrero de 1983 y el 10 de febrero de 1989 y que tuvo graves casos de corrupción durante el gobierno de Jaime Lusinchi— hicieron desaparecer la estabilidad cambiaria de la moneda venezolana.

Mientras el gobierno en el 2003 aplica el control cambiario, el bolívar tiene un valor implícito que se oculta pero existe es el "valor de mercado negro" y es, con lo que venezolanos realizan sus importaciones y exportaciones en sus fronteras y tiene un valor en relación con el dólar estadounidense, el peso colombiano y el real brasileño diferente al valor controlado. En los primeros años de mandato de Chávez, sus programas sociales recién creados, requerían grandes inversiones a fin de realizar los cambios deseados en el país. El 5 de febrero de 2003, el gobierno creó CADIVI, un sistema de control de cambio encargado de los procedimientos de manejo de divisas. La razón de su creación fue controlar la fuga de capitales, estableciendo límites a los individuos y ofreciéndoles solamente una cantidad fijada de una moneda extranjera., tras ese sistema crecerá la corrupción gubernamental tal como ocurrió con RECADI. Este límite en moneda extranjera condujo a la creación de una economía de mercado negro de divisas, debido a que los comerciantes venezolanos necesitaban un flujo confiable y constante de divisas extranjeras para adquirir los productos importados que el estado no conseguía suplir. El Banco Central de Venezuela comenzó a imprimir más bolívares para cubrir sus programas sociales, así que el bolívar continuó devaluándose para el ciudadano común y comerciantes, ya que el gobierno se quedaba con la mayoría de las divisas.

Desde enero de 2014, el tipo de cambio oficial es de 1 USD a 6,3 BsF. VEF, mientras que la tasa de cambio del mercado negro es sesenta veces mayor; esto se debe a que el valor real del bolívar está sobrevaluado para el comercio venezolano. Desde que algunos comerciantes solo pueden recibir una cantidad fija de moneda extranjera de lo que necesitan para importar, por parte del gobierno, deben recurrir al mercado negro que a su vez aumenta los precios del comerciante para la venta al público. Las altas tasas en el mercado negro hacen que sea difícil para las empresas la compra de bienes necesarios ya que el gobierno a menudo obliga a estas empresas a hacer regulación de precios. Esto lleva a las empresas a vender sus productos con baja ganancia e incluso pérdida; por ejemplo, las franquicias venezolanas de McDonalds que ofrecen una comida de Big Mac por solo 1 USD. Dado que las empresas obtienen beneficios bajos, esto lleva a la escasez, debido a que son incapaces de importar la cantidad necesaria de bienes que Venezuela necesita y depende para funcionar. La compañía de Venezuela más grande de producción de alimentos, Empresas Polar, declaró que era posible que necesiten suspender parte de la producción durante casi todo el año de 2015, ya que les deben a los proveedores extranjeros 463 millones de dólares USD. El último informe de escasez en Venezuela mostró que 22,4 % de los productos necesarios no se encontraban en stock. Este fue el último informe del gobierno debido a que el banco central ya no publica el índice de escasez. Esto ha llevado especulación, y de esa forma un mecanismo del gobierno para ocultar su incapacidad para controlar la economía que podría crear dudas futuras sobre la veracidad de los datos económicos suministrados por el gobierno.

A partir de 2013 la economía venezolana ha sufrido una caída de sus índices macroeconómicos, dando paso a un período de recesión y crisis.
El origen de esta caída es una combinación de problemas estructurales propios en la economía venezolana y la fuerte influencia externa de la crisis financiera mundial con la caída de los precios del petróleo.
En 2014, el PIB tuvo una variación -3,9 %, en 2015 de -5,7 %. A finales de 2015 se vivió una ligera mejoría con una subida del 0,1 %, pero de nuevo en 2016 volvió a descender un 1,6 %. Con especial dureza la Crisis en Venezuela se ha manifestado, en un fuerte aumento del desempleo, con una tasa de desempleo del 14 % en el primer trimestre de 2015 según los datos del INE. Dañado el motor de la economía antes de la crisis enmarcado por el control de cambio (CADIVI), y una fuerte acumulación de deuda, se hace patente la debilidad estructural del modelo económico venezolano de los últimos años.

Además de todo, la inflación ha sido un hecho importante en este contexto, la cual en el 2014 llegó hasta 68.5 %. Esa cifra es una de las más altas que se han registrado en la historia económica del país y fue la más elevada en el mundo durante el 2013. Asimismo la inflación del año 2015 fue de 180.9 %, y para el 2016 el FMI pronosticó una inflación superior a 700 %.

La caída de los precios del petróleo ha sido una de las causas de la crisis económica. El Banco Central de Venezuela anuncia la caída de las Reservas Internacionales ubicándose en 13 501 millones de dólares. Tras la cancelación de la deuda externa en Bonos Global 2016 por 1 543 millones de dólares.

Para noviembre de 2017, el gobierno de Nicolás Maduro, llamó a refinanciar la deuda externa.

Uno de los fenómenos más particulares en la última década ha sido la escasez de productos de consumo diario, en particular de aquellos con precios regulados, como la leche, diversos tipos de carne, el aceite y otros. Los gobiernos de Chávez y Maduro han relacionado dicha escasez en primer lugar a un aumento en el consumo, que no puede ser rápidamente satisfecho por la producción, y cada vez más al acaparamiento y el contrabando. Los economistas en general consideran que el control de precios a un valor por debajo de los costes, el exceso de liquidez monetaria ante un sistema de poca producción nacional y la expropiación por parte del Estado de cerca de 1200 empresas privadas que abastecían el mercado nacional son las causas principales de tal escasez. Consideran que la economía de Venezuela padece los efectos típicos de una economía de escasez. El factor de contrabando es admitido por ambos grupos: varios productos son mucho más baratos en Venezuela que en Colombia, Brasil y otros países limítrofes. Para diciembre de 2013 el grado de escasez según el BCV indicaba que había una escasez de 22 %. Esto quiere decir que un 22 % de los productos que el consumidor buscaba en los negocios no se encontraba.

Para mayo de 2019 el precio de la canasta alimentaria familiar se ubicó en 1.924.265,03 bolívares para lo cual se requería 48.10 salarios mínimos (sueldo 40,000 Bolívares mensuales) todos los productos de la canasta suben mensual entre un 15% y un 33% y durante el mes de abril se presentó la escasez de veintiún productos entre ellos Leche en polvo, atún enlatado, aceite de maíz, mayonesa, mantequilla, huevos de gallina, café, pasta de fideos, azúcar, pan, queso, y otros adicionalmente antibióticos , una amplia gama de medicinas, productos básicos para la higiene personal y la limpieza 

Dada la envergadura histórica de esta crisis económica y su naturaleza global se han utilizado un amplio catálogo de medidas para combatirla. Durante los momentos de la crisis el gobierno practicó una política fiscal de estímulo de la demanda: la aprobación del Paquetazo Rojo del 17 de febrero de 2016 que englobaba todas las medidas económicas, financieras y fiscales que el Ejecutivo aplicaría para intentar recuperar la senda de crecimiento. El Plan se basaba en cuatro grandes ejes de actuación: medidas de apoyo a familias, medidas de fomento del empleo, medidas de apoyo al sistema financiero y medidas de modernización de la economía. El gobierno anunciaba recortes fiscales y nuevos gastos por valor de 227,6 millardos de bolívares, un 47,9 % del Producto Interior Bruto, en 2014.

No obstante, a los mercados de deuda pública les preocupaba el ritmo alarmante de deterioro de las cuentas públicas del país; en septiembre de 2014, la calificación de la deuda soberana de Venezuela fue rebajada por la agencia internacional Standard & Poor's. En 2013 la deuda del sector público rondaba el 52 % del PIB, cifra menor a la media de América Latina, pero a lo largo del año 2014 el déficit público se incrementó en un 14 %. Las principales causas son la pronunciada caída del PIB, la igualmente pronunciada subida del desempleo y la fuerte inversión en programas de ayudas como las contenidas en el Paquetazo Rojo. 

A raíz de esta situación en diciembre de 2014 el Gobierno aprueba un paquete de medidas de recorte que tratan de frenar el crecimiento del déficit público por 108 mil millones de bolívares.

En febrero de 2015, es incorporado un nuevo sistema de cambio que se ha denominado Sistema Marginal de Divisas (SIMADI). Al empezar a cotizar el día 13, el precio del dólar se ubicó a 170 bolívares. En marzo del 2016, se elaboran dos nuevos sistemas de divisas, Tipo de Cambio Protegido (DIPRO) y Tipo de Cambio Complementario (DICOM), el primero a un costo de 10 bs. por dólar y el otro empezó en 206 bs., el anuncio lo dio a conocer Miguel Pérez Abad. El 23 de mayo de 2017 el dólar DICOM estaba en 727,97 bolívares. Este día entró en vigor un nuevo sistema DICOM. La primera subasta fue convocada el 25 de mayo con una banda de posibles ofertas para la compra de dólares entre 1800 y 2000 Bs, equivalente a una devaluación de por lo menos 60 %.

En 2017 se produjo una recaída de la economía venezolana que ha producido una disminución aproximada del Producto interior bruto del 9,5 %. La causa de este deterioro fue el agravamiento de la crisis de la deuda pública, provocada por la desconfianza de los mercados financieros internacionales hacia las pérdidas ocultas de las entidades financieras venezolanas y sus posibles consecuencias en las arcas públicas. Esta desconfianza se tradujo en el cierre de los mercados financieros internacionales a la economía venezolana y la huida de los inversores extranjeros de los activos venezolanos. Ha sido un año peligroso desde el punto de vista financiero.

La destrucción de empleo ha sido muy fuerte, alcanzando una media anual del 25 %. Este desempleo junto al aumento impositivo efectuado para intentar controlar el déficit, ha provocado una importante erosión de las rentas de las familias y consecuentemente del consumo y la inversión.

El único sector que ha tenido un comportamiento positivo durante el año ha sido el exterior que ha traído un ligero incremento de las exportaciones y una disminución de las importaciones produciendo un saldo positivo de la balanza de bienes y servicios.

El 10 de abril no se cancela a tiempo el capital de los Bonos Elecar 2018 emitidos el 2008 los cuales se encuentran en Default y que podrían ser reclamados en cualquier momento, ya tenían retraso en el pago de intereses en octubre pasado 

Después de los anuncios ofrecidos por el presidente Nicolás Maduro, a principios de noviembre de 2017, como el aumento salarial y la puesta en circulación del billete de 100 000 bolívares; economistas y medios de comunicación afirmaron que Venezuela, ha iniciado una hiperinflación, tras arrojar el pasado mes de octubre una inflación del 50,6 %. Analistas del tema y medios de comunicación afirman, que para frenar la hiperinflación, primero deben detener la impresión de billetes, unificar el tipo de cambio, aumentar la producción nacional e importación bienes de consumo que sean necesarios, además de suprimir los controles de precios.

El 28 de enero inicia una nueva modalidad cambiaria en el país. Interbanex como una modalidad de cambio para empresas privadas y personas naturales sin la participación de Empresas del sector público por medio de una plataforma creada para tal fin con la participación del Banco Occidental de Descuento por el momento y un tipo de cambio referencial acorde al Mercado paralelo e informado por el Banco Central de Reserva de Venezuela

El 1 de mayo queda eliminado el sistema DICOM con lo cual se establece la liberación del Mercado de Divisas con algunos problemas dada las Sanciones emitidas contra los Bancos nacionales y sus corresponsales al momento de realizar ciertas operaciones 

El 7 de mayo cierra sus operaciones Interbanex a 90 días de su creación para dar paso a las mesas de dinero del sistema financiero y desaparecer el esquema del DICOM. 

El Banco Central de Venezuela informó al país que las Reservas internacionales para el día 24 de mayo cerraron en US $7,965 millones de dólares un 0.2% menos que la semana anterior que cerro en US $7,981 millones el día 17 de mayouna cifra de las más bajas en 30 años. Recordemos para 1998 las reservas estaban en US $14.849 millones de dólares .
Luego de 3 años, el Banco Central de Venezuela admitió una hiperinflación de 53.798.500% entre 2016 y abril de 2019, al publicar los datos del Índice Nacional de Precios al Consumidor y que el los cuatro primeros meses del año 2019 el BCV indican que la inflación acumulada hasta abril es de 1,047% 

La economía de Venezuela se centra en la exportación de petróleo. La dependencia del petróleo ha aumentado en los últimos años. Mientras que en 1999 las exportaciones de bienes y servicios petroleros representaban el 76 % de las exportaciones, en 2005 el porcentaje había pasado a 86 % y en 2012 se elevaba al 96 %.
El porcentaje de las exportaciones petroleras en las exportaciones totales había sido de 91,9 % en 1958, 92,8 % en 1968, 93,6 % en 1978 y 81,1 % en 1988.
A finales de 2013 Venezuela exportaba unos 1,7 millones de barriles diarios de petróleo.

Las cifras de producción real y exportación de petróleo han sido objeto de mucha polémica. El presidente de PDVSA, Rafael Ramírez, declaró en 2011 que Venezuela aumentaría su producción de petróleo y produciría unos 4,02 millones de barriles de petróleo para 2012. En 2012, Ramírez declaró que Venezuela produciría 4 millones de barriles para 2014. En diciembre de 2013 el presidente de PDVSA dijo que la producción petrolera en 2014 estaría en 3 millones 11 mil barriles de petróleo.

La compañía estatal PDVSA es la encargada de administrar los recursos petroleros. En 1998 trabajaban en esta empresa unas 36 000 personas, las cuales producían más 3.48 millones de barriles de petróleo diarios. En 2011 PDVSA contaba con 121 187 trabajadores, de los cuales 104 187 trabajaban en la producción de petróleo produciendo 2.76 millones de barriles de petróleo diarios, una baja significativa de la productividad.

En 2012, PDVSA produjo 2.91 millones de barriles de crudo diariamente. En 2013, esta cifra fue de 2.89 millones de barriles. Tras sanciones impuestas por el gobierno de Estados Unidos entre 2018 y 2019, la exportación ha descendido a 920.000 barriles por día (bpd) de crudo y combustible.

En 1998 el país exportaba bienes no petroleros por un valor de 5.529 millones de dólares mientras que en 2012 la cifra era de tan solo 3.771 millones de dólares.

En 2012 Venezuela importó bienes y servicios (CIF) por un valor total de 65 360 000 000 de dólares. En 1998 el total de importaciones de bienes y servicios se elevaba a 15 492 000 000 de dólares.

El país ha sido tradicionalmente importador de gran cantidad de productos manufacturados, pero en los últimos años esta tendencia se ha acentuado. En 2012-2013 los renglones más importantes de importación eran aquellos de maquinarias y repuestos para apartados y maquinarias mecánicos y eléctricos o electrónicos.

Según el ministro de Agricultura Yván Gil, Venezuela importaba un 50 % de los alimentos que consumía en 2013. Venezuela era en 2008 el principal importador mundial de leche en polvo.

El país actualmente importa productos que tradicionalmente exportara, como el café, el arroz y el maíz. En 2012 se importaron dichos productos por un monto aproximado de 1028 millones de dólares. También se han venido importando otros productos típicos de Venezuela como el azúcar.

Venezuela pasó a ocupar el puesto número 13 de los países que más gastaron en importaciones de armamento en 2012. Ocupaba el puesto 46 en 2002. El presidente Chávez había justificado estos gastos con el argumento de que era necesario reemplazar armamento obsoleto. Venezuela se ha convertido en el principal importador de armas en Sudamérica, por encima de Brasil. El principal vendedor de armas a Venezuela fue Rusia, con un 66 % de las importaciones, seguido de España, con un 12 % y de China, con un 6 %.

En Venezuela la minería no es muy alta, así que hay poca exportación en esta área.

Las reservas de hierro de Venezuela son unas de las más importantes en el mundo. Aun así, la extracción de hierro ha venido cayendo en los últimos años. SIDOR ha sido desde hace décadas la empresa estatal encargada de gerenciar la extracción y el procesamiento de este metal. En 1997 la empresa fue privatizada durante la ola de privatizaciones generadas por la falta de ingresos petroleros que permitiesen financiar la inversión en diversas industrias que se habían vuelto ineficientes. La empresa llegó a aumentar su producción hasta el 2007. A comienzos de 2008 fue nuevamente estatizada. A partir de ese momento ha vuelto a caer la producción. En 2013 la empresa producía tan solo un 45 % de su capacidad instalada.

Venezuela es uno de los principales países extractores de bauxita, la principal mena para extraer aluminio.
Como en el caso del hierro y del acero, desde la estatización de Alcasa ha habido una caída en la producción.
Tan solo entre 2012 y 2013 la producción disminuyó en un 28 %.
Al mismo tiempo, la cantidad de trabajadores aumentó en 18 %: pasó de 8606 a 10 169 trabajadores.

Actualmente hay dos federaciones rivales en el área de la producción ganadera: Fedenaga, la federación tradicional, y Fegaven, que está aliada al gobierno.

Los datos estadísticos sobre la producción agrícola en Venezuela son altamente disputados, con cifras bastante divergentes entre lo que dicen empresas privadas y el gobierno y asociaciones cercanas al gobierno.
Venezuela producía 1410 millones de litros de leche en 1998 según estudios de la Universidad de Los Andes.

Según el gobierno, en 2010 se producían unas 4 697 784 toneladas de carne en el país o lo que equivaldría a un 80 % del consumo nacional. El representante de Fedenaga calculaba para comienzos de 2013 que en realidad Venezuela estaría importando un 50 % de la carne que consumía.

Según Fevearroz, Venezuela producía 699 toneladas de arroz para 1998 y 1080 en 2008. La mayor parte de la producción se concentraba en Guárico y Portuguesa (para un 93 % de la producción).

Venezuela se abastecía a sí misma en el consumo de maíz hasta 2007. En 2012 se obtuvieron 772 853 toneladas de maíz, lo que equivalió a un 55 % de la demanda.

Venezuela posee una gran cantidad de paisajes con potencial turístico, pero la industria turística está mucho menos desarrollada que en otros países de América. En 1998, turistas extranjeros visitaron el país. En 2011, fueron los turistas que visitaron el país, en 2015 fueron . Regiones como Guatemala, Aruba y El Salvador, que en 1998 recibían menos visitantes extranjeros que Venezuela, en 2015 recibieron muchos más.




</doc>
<doc id="4670" url="https://es.wikipedia.org/wiki?curid=4670" title="SQL">
SQL

SQL (por sus siglas en inglés Structured Query Language; en español lenguaje de consulta estructurada) es un lenguaje de dominio específico utilizado en programación, diseñado para administrar, y recuperar información de sistemas de gestión de bases de datos relacionales. Una de sus principales características es el manejo del álgebra y el cálculo relacional para efectuar consultas con el fin de recuperar, de forma sencilla, información de bases de datos, así como realizar cambios en ellas.

Originalmente basado en el álgebra relacional y en el cálculo relacional, SQL consiste en un lenguaje de definición de datos, un lenguaje de manipulación de datos y un lenguaje de control de datos. El alcance de SQL incluye la inserción de datos, consultas, actualizaciones y borrado, la creación y modificación de esquemas y el control de acceso a los datos. También el SQL a veces se describe como un lenguaje declarativo, también incluye elementos procesales.

SQL fue uno de los primeros lenguajes comerciales para el modelo relacional de Edgar Frank Codd como se describió en su artículo de investigación de 1970 "El modelo relacional de datos para grandes bancos de datos compartidos". A pesar de no adherirse totalmente al modelo relacional descrito por Codd, pasó a ser el lenguaje de base de datos más usado.

SQL pasó a ser el estándar del Instituto Nacional Estadounidense de Estándares (ANSI) en 1986 y de la Organización Internacional de Normalización (ISO) en 1987. Desde entonces, el estándar ha sido revisado para incluir más características. A pesar de la existencia de ambos estándares, la mayoría de los códigos SQL no son completamente portables entre sistemas de bases de datos diferentes sin otros ajustes.

Los orígenes de SQL están ligados a las bases de datos relacionales, específicamente las que residían en máquinas IBM bajo el sistema de gestión System R, desarrollado por un grupo de la IBM en San José, California. 

En 1970, E. F. Codd propone el modelo relacional y asociado a este un sublenguaje de acceso a los datos basado en el cálculo de predicados. Basándose en estas ideas, los laboratorios de IBM definieron el lenguaje SEQUEL (Structured English Query Language) que más tarde fue ampliamente implementado por el sistema de gestión de bases de datos (SGBD) experimental System R, desarrollado en 1977 también por IBM. Sin embargo, fue Oracle quien lo introdujo por primera vez en 1979 en un producto comercial.

El SEQUEL terminó siendo el predecesor de SQL, que es una versión evolucionada del primero. SQL pasa a ser el lenguaje por excelencia de los diversos sistemas de gestión de bases de datos relacionales surgidos en los años siguientes y fue por fin estandarizado en 1986 por el ANSI, dando lugar a la primera versión estándar de este lenguaje, "SQL-86" o "SQL1". Al año siguiente este estándar es también adoptado por ISO.

Sin embargo, este primer estándar no cubría todas las necesidades de los desarrolladores e incluía funcionalidades de definición de almacenamiento que se consideró suprimirlas. Así que, en 1992, se lanzó un nuevo estándar ampliado y revisado de SQL llamado "SQL-92" o "SQL2".

En la actualidad SQL es el estándar "de facto" de la inmensa mayoría de los SGBD comerciales. Y, aunque la diversidad de añadidos particulares que incluyen las distintas implementaciones comerciales del lenguaje es amplia, el soporte al estándar SQL-92 es general y muy amplio.

El ANSI SQL sufrió varias revisiones y agregados a lo largo del tiempo:

SQL es un lenguaje de acceso a bases de datos que explota la flexibilidad y potencia de los sistemas relacionales y permite así gran variedad de operaciones.

Es un lenguaje declarativo de "alto nivel" o "de no procedimiento" que, gracias a su fuerte base teórica y su orientación al manejo de conjuntos de registros —y no a registros individuales— permite una alta productividad en codificación y la orientación a objetos. De esta forma, una sola sentencia puede equivaler a uno o más programas que se utilizarían en un lenguaje de bajo nivel orientado a registros.
SQL también tiene las siguientes características:


Algunos de los tipos de datos básicos de SQL's son:


Como ya se dijo antes, y suele ser común en los lenguajes de acceso a bases de datos de alto nivel, SQL es un lenguaje declarativo. O sea, que especifica qué es lo que se quiere y no cómo conseguirlo, por lo que una sentencia no establece explícitamente un orden de ejecución.

El orden de ejecución interno de una sentencia puede afectar seriamente a la eficiencia del SGBD, por lo que se hace necesario que éste lleve a cabo una optimización antes de su ejecución. Muchas veces, el uso de índices acelera una instrucción de consulta, pero ralentiza la actualización de los datos. Dependiendo del uso de la aplicación, se priorizará el acceso indexado o una rápida actualización de la información. La optimización difiere sensiblemente en cada motor de base de datos y depende de muchos factores.

Los sistemas de bases de datos modernos poseen un componente llamado optimizador de consultas. Este realiza un detallado análisis de los posibles planes de ejecución de una consulta SQL y elige aquel que sea más eficiente para llevar adelante la misma.

Existe una ampliación de SQL conocida como FSQL (Fuzzy SQL, SQL difuso) que permite el acceso a bases de datos difusas, usando la lógica difusa. Este lenguaje ha sido implementado a nivel experimental y está evolucionando rápidamente.

El lenguaje de definición de datos (en inglés "Data Definition Language", o "DDL"), es el que se encarga de la modificación de la estructura de los objetos de la base de datos. Incluye órdenes para modificar, borrar o definir las tablas en las que se almacenan los datos de la base de datos. Existen cuatro operaciones básicas: CREATE, ALTER, DROP y TRUNCATE.

Este comando permite crear objetos de datos, como nuevas bases de datos, tablas, vistas y procedimientos almacenados.

CREATE TABLE clientes;
Este comando permite modificar la estructura de una tabla u objeto. Se pueden agregar/quitar campos a una tabla, modificar el tipo de un campo, agregar/quitar índices a una tabla, modificar un trigger, etc.

ALTER TABLE alumnos ADD edad INT UNSIGNED;
Este comando elimina un objeto de la base de datos. Puede ser una tabla, vista, índice, trigger, función, procedimiento o cualquier objeto que el motor de la base de datos soporte. Se puede combinar con la sentencia ALTER.


DROP TABLE alumnos;
Este comando solo aplica a tablas y su función es borrar el contenido completo de la tabla especificada. La ventaja sobre el comando DELETE, es que si se quiere borrar todo el contenido de la tabla, es mucho más rápido, especialmente si la tabla es muy grande. La desventaja es que TRUNCATE solo sirve cuando se quiere eliminar absolutamente todos los registros, ya que no se permite la cláusula WHERE. Si bien, en un principio, esta sentencia parecería ser DML (Lenguaje de Manipulación de Datos), es en realidad una DDL, ya que internamente, el comando TRUNCATE borra la tabla y la vuelve a crear y no ejecuta ninguna transacción.


Un lenguaje de manipulación de datos ("Data Manipulation Language", o "DML" en inglés) es un lenguaje proporcionado por el sistema de gestión de base de datos que permite a los usuarios llevar a cabo las tareas de consulta o manipulación de los datos, organizados por el modelo de datos adecuado.

El lenguaje de manipulación de datos más popular hoy día es SQL, usado para recuperar y manipular datos en una base de datos relacional.

La sentencia SELECT nos permite consultar los datos almacenados en una tabla de la base de datos.

SELECT [{ALL|DISTINCT}]

FROM {<nombre_tabla>|<nombre_vista>}[,

[WHERE <condición> [{AND|OR} <condición>...]]

[GROUP BY <nombre_campo>[, <nombre_campo>...]]

[HAVING <condición> [{AND|OR} <condición>...]]

[ORDER BY {<nombre_campo>|<indice_campo>} [{ASC|DESC}][,

Ejemplo:

Para formular una consulta a la tabla coches y recuperar los campos matrícula, marca, modelo, color, número_kilómetros, num_plazas debemos ejecutar la siguiente consulta. Los datos serán devueltos ordenados por marca y por modelo en orden ascendente, de menor a mayor. La palabra clave FROM indica que los datos serán recuperados de la tabla Coches.
SELECT
FROM
ORDER BY
Ejemplo de consulta simplificada a través de un comodín de campos (*):

El uso del asterisco indica que queremos que la consulta devuelva todos los campos que existen en la tabla y los datos serán devueltos ordenados por marca y por modelo.
SELECT *
FROM
ORDER BY
La cláusula" WHERE" es la instrucción que nos permite filtrar el resultado de una sentencia SELECT. Habitualmente no deseamos obtener toda la información existente en la tabla, sino que queremos obtener solo la información que nos resulte útil en ese momento. La cláusula WHERE filtra los datos antes de ser devueltos por la consulta. Cuando en la Cláusula WHERE queremos incluir un tipo texto, debemos incluir el valor entre comillas simples.

Ejemplos:

En nuestro ejemplo, se desea consultar un coche en concreto, para esto se agregó una cláusula WHERE. Esta cláusula especifica una o varias condiciones que deben cumplirse para que la sentencia SELECT devuelva los datos. En este caso la consulta devolverá solo los datos del coche con matrícula para que la consulta devuelva solo los datos del coche con matrícula codice_1o bien la matrícula codice_2 . Se puede utilizar la cláusula WHERE solamente, ó en combinación con tantas condiciones como queramos.
SELECT
FROM
WHERE
Una condición WHERE puede ser negada a través del Operador Lógico NOT. La Siguiente consulta devolverá todos los datos de la tabla Coches, menos el que tenga la Matrícula codice_3.
SELECT
FROM
WHERE
La siguiente consulta utiliza la condicional DISTINCT, la cual nos devolverá todos los valores distintos formados por los campos "marca y modelo". de la tabla "coches".
SELECT DISTINCT marca, modelo FROM coches;
La cláusula" ORDER BY" es la instrucción que nos permite especificar el orden en el que serán devueltos los datos. Podemos especificar la ordenación ascendente o descendente a través de las palabras clave ASC y DESC. La ordenación depende del tipo de datos que este definido en la columna, de forma que un campo numérico será ordenado como tal, y un alfanumérico se ordenará de la A a la Z, aunque su contenido sea numérico. El valor predeterminado es ASC si no se especifica al hacer la consulta.

Ejemplos:
SELECT
FROM
ORDER BY
Este ejemplo, selecciona todos los campos matricula, marca, modelo, color, numero_kilometros y num_plazas de la tabla coches, ordenándolos por los campos marca y modelo, marca en forma ascendente y modelo en forma descendente.
SELECT
FROM
ORDER BY 2;
Este ejemplo, selecciona todos los campos matrícula, marca, modelo, color, numero_kilometros y num_plazas de la tabla coches, ordenándolos por el campo "marca", ya que aparece en segundo lugar dentro de la lista de campos que componen la SELECT.

Una subconsulta es una sentencia SELECT que está embebida en una cláusula de otra sentencia SQL. También pueden utilizarse subconsultas en los comandos INSERT, UPDATE, DELETE y en la cláusula FROM.

Las subconsultas pueden resultar útiles si necesitas seleccionar filas de una tabla con una condición que depende de los datos de la propia tabla o de otra tabla.

La subconsulta (consulta interna), se ejecuta antes de la consulta principal; el resultado de la subconsulta es utilizado por la consulta principal (consulta externa).
SELECT c.matricula, c.modelo
FROM coches AS c
WHERE c.matricula IN

En este ejemplo, se seleccionan las matrículas y los modelos de los coches cuyas multas superan los u$s 100.

Una sentencia "INSERT" de SQL agrega uno o más registros a una (y solo una) tabla en una base de datos relacional.

INSERT INTO
VALUES

-- O también se puede utilizar como:
INSERT INTO tablatura VALUES ('valor1', 'valor2');
Las cantidades de columnas y valores deben ser iguales. Si una columna no se especifica, le será asignado el valor por omisión. Los valores especificados (o implícitos) por la sentencia codice_4 deberán satisfacer todas las restricciones aplicables. Si ocurre un error de sintaxis o si alguna de las restricciones es violada, no se agrega la fila y se devuelve un error.

INSERT INTO agenda_telefonica (nombre, numero)
VALUES ('Roberto Jeldrez', 4886850);
Cuando se especifican todos los valores de una tabla, se puede utilizar la sentencia acortada:
INSERT INTO nombre_tabla VALUES ('valor1', ['valor2', ...]);
Ejemplo (asumiendo que 'nombre' y 'número' son las únicas columnas de la tabla 'agenda_telefonica'):
INSERT INTO agenda_telefonica
VALUES ('Jhonny Aguilar', 080473968);
Una característica de SQL (desde SQL-92) es el uso de "constructores de filas" para insertar múltiples filas a la vez, con una sola sentencia SQL:
INSERT INTO
VALUES
Esta característica es soportada por DB2, PostgreSQL (desde la versión 8.2), MySQL, y H2.

Ejemplo (asumiendo que nombre y número son las únicas columnas en la tabla agenda_telefonica):
INSERT INTO
VALUES
Que podía haber sido realizado por las sentencias
INSERT INTO agenda_telefonica VALUES ('Roberto Fernández', '4886850');
INSERT INTO agenda_telefonica VALUES ('Alejandro Sosa', '4556550');
Notar que las sentencias separadas pueden tener semántica diferente (especialmente con respecto a los triggers), y puede tener diferente rendimiento que la sentencia de inserción múltiple.

Para insertar varias filas en MS SQL puede utilizar esa construcción:
INSERT INTO phone_book
SELECT 'John Doe', '555-1212'
UNION ALL
SELECT 'Peter Doe', '555-2323';
Tenga en cuenta que no se trata de una sentencia SQL válida de acuerdo con el estándar SQL (), debido a la cláusula subselect incompleta.

Para hacer lo mismo en Oracle se usa la Tabla DUAL, siempre que se trate de solo una simple fila:
INSERT INTO phone_book
SELECT 'John Doe', '555-1212' FROM DUAL
UNION ALL
SELECT 'Peter Doe','555-2323' FROM DUAL
Una implementación conforme al estándar de esta lógica se muestra el siguiente ejemplo, o como se muestra arriba (no aplica en Oracle):
INSERT INTO phone_book
SELECT 'John Doe', '555-1212' FROM LATERAL ( VALUES (1) ) AS t(c)
UNION ALL
SELECT 'Peter Doe','555-2323' FROM LATERAL ( VALUES (1) ) AS t(c)
Un INSERT también puede utilizarse para recuperar datos de otros, modificarla si es necesario e insertarla directamente en la tabla. Todo esto se hace en una sola sentencia SQL que no implica ningún procesamiento intermedio en la aplicación cliente. Un SUBSELECT se utiliza en lugar de la cláusula VALUES. El SUBSELECT puede contener la sentencia JOIN, llamadas a funciones, y puede incluso consultar en la misma TABLA los datos que se inserta. Lógicamente, el SELECT se evalúa antes que la operación INSERT esté iniciada. Un ejemplo se da a continuación.
INSERT INTO phone_book2

SELECT *
FROM phone_book
WHERE name IN ('John Doe', 'Peter Doe');
Una variación es necesaria cuando algunos de los datos de la tabla fuente se está insertando en la nueva tabla, pero no todo el registro. (O cuando los esquemas de las tablas no son iguales.)
INSERT INTO phone_book2 ([name], [phoneNumber])

SELECT [name], [phoneNumber]
FROM phone_book
WHERE name IN ('John Doe', 'Peter Doe');
El SELECT produce una tabla (temporal), y el esquema de la tabla temporal debe coincidir con el esquema de la tabla donde los datos son insertados.

Una sentencia "UPDATE" de SQL es utilizada para modificar los valores de un conjunto de registros existentes en una tabla.

UPDATE My_table SET field1 = 'updated value' WHERE field2 = 'N';
Una sentencia "DELETE" de SQL borra uno o más registros existentes en una tabla.

DELETE FROM tabla WHERE columna1 = 'valor1';

DELETE FROM mi_tabla WHERE columna2 = 'N';
Los diseñadores de base de datos que usan una clave suplente como la clave principal para cada tabla, se ejecutará en el ocasional escenario en el que es necesario recuperar automáticamente la base de datos, generando una clave primaria de una sentencia SQL INSERT para su uso en otras sentencias SQL. La mayoría de los sistemas no permiten sentencias SQL INSERT para retornar fila de datos. Por lo tanto, se hace necesario aplicar una solución en tales escenarios.

Implementaciones comunes incluyen:

SELECT *
FROM NEW TABLE (
) AS t
INSERT INTO phone_book VALUES ('Cristobal Jeldrez', '0426.817.10.30')
RETURNING phone_book_id INTO v_pb_id

Set NoCount On;
INSERT INTO phone_book VALUES ('Cristobal Jeldrez', '0426.817.10.30');
Select @@Identity as id
Los disparadores, también conocidos como desencadenantes ("triggers" en inglés) son definidos sobre la tabla en la que opera la sentencia INSERT, y son evaluados en el contexto de la operación. Los desencadenantes BEFORE INSERT permiten la modificación de los valores que se insertarán en la tabla. Los desencadenantes AFTER INSERT no puede modificar los datos de ahora en adelante, pero se puede utilizar para iniciar acciones en otras tablas, por ejemplo para aplicar mecanismos de auditoría Excel.

Los sistemas de gestión de base de datos con soporte SQL más utilizados son, por orden alfabético:


El lenguaje de consultas de los diferentes sistemas de gestión de bases de datos son incompatibles entre ellos y no necesariamente siguen completamente el estándar. En particular, la sintaxis de fecha y tiempo, la concatenación de cadenas, nulas, y la comparación de textos en cuanto al tratamiento de mayúsculas y minúsculas varían de un proveedor a otro. Una excepción particular es PostgreSQL, que se esfuerza por lograr el cumplimiento del estándar.

Las implementaciones populares de SQL omiten comúnmente soporte para funciones básicas de SQL estándar, como la de los tipos de dato codice_5 o codice_6. Es el caso del manejador de bases de datos de Oracle (cuyo tipo codice_5 se comporta como codice_8, y carece de un tipo codice_6) y MS SQL Server (antes de la versión de 2008). Como resultado, el código SQL rara vez puede ser portado entre los sistemas de base de datos sin modificaciones.

Hay varias razones para esta falta de portabilidad entre sistemas de bases de datos:


El estándar ODBC (Open Database Connectivity) permite acceder a la información desde cualquier aplicación independientemente del sistema de gestión de base de datos (DBMS) en el que esté almacenada la información, desacoplando así la aplicación de la base de datos.


https://onlinedevtools.in/online/sqlformatter


</doc>
<doc id="4678" url="https://es.wikipedia.org/wiki?curid=4678" title="Inducción">
Inducción

El término inducción hace referencia a varios artículos:






</doc>
<doc id="4679" url="https://es.wikipedia.org/wiki?curid=4679" title="Numeración en base constante">
Numeración en base constante

Sea b un entero superior a uno. Escribir un entero n en la base b significa descomponerlo en las potencias de b, es decir determinar los coeficientes ( también llamados cifras) a tales que:


Bien es sabido que el sistema vigente por doquier es el decimal; es decir que se emplea la base diez: b = 10. La escritura de cualquier entero utiliza las potencias de 10 así:

1492 = 1000 + 400 + 90 + 2 = 1×1000 + 4×100 + 9×10 + 2×1 = 1x10 + 4x10 + 9x10 + 2x10 .

Para pasar de las unidades a las decenas, y de las decenas a las centenas, se multiplica por el mismo número, aquí diez, por eso se dice que el sistema es la numeración en base constante.

Se ha empleado la numeración en base constante, con otras bases que diez, principalmente las bases cinco (los Aztecas) y veinte. Quedan rastros del empleo de la base veinte en algunos idiomas occidentales, como el francés ("ochenta" se dice "quatre-vingts" es decir "cuatro veintes", ya que la palabra "huitante" se emplea en Suiza y Bélgica), en danés (para los números 50, 60 y 70), en inglés ("score", una veintena, "two-score", "three-score", "four scores" para "ochenta"), y en latín (donde "18" no se decía "10 + 8" sino "20 - 2").

Se cree que la elección de las bases 5, 10 ó 20 se debe a causas biológicas, pues el hombre siempre contó con los dedos (hasta de los pies).

Por la mitad del siglo XX, se descubrió un interés descomunal por la base dos o "base binaria", pues tiene la ventaja de necesitar solamente dos cifras, 0 y 1. Esto debido al desarrollo del cálculo electrónico y el procesamiento de datos. El sistema encaja bien con los dos estados de un circuito electrónico: sin corriente o con corriente. El sistema binario puro tiene la ventaja de ser sencillo, pero su principal inconveniente reside en que la expresión de un número en base 2 es muy extensa.

Reagrupando las cifras por cuatro o por cinco se obtiene la base hexadecimal (base dieciséis) y la base treinta y dos. Cuando se trabaja en una base superior a diez, se tiene que inventar nuevas cifras, para notar los números que van de diez a b-1 (b sigue siendo la base). Por ejemplo, cuando se emplea la base "doce", se añade las cifras alfa y beta para diez y once. Para la base hexadecimal, la costumbre es utilizar las letras mayúsculas A, B, ... F.

Han existido históricamente numeraciones en base variable, como la de los Sumerios, que empleaban una mezcla de base 60 y de base 10. Han legado al mundo actual el que una hora se divide en sesenta minutos, y no en diez o cien, la semana de siete días, la docena y el que el círculo se divide en 360 = 6×60 grados = 12x30 grados (en el Zodíaco), y no en cien o cuatrocientos (que también existe, pero no es tan común).

La desventaja de aquel sistema, era que multiplicar por 10 o 60 no resultaba fácil, pues no se puede sencillamente mover las cifras (a la izquierda) y añadir una casilla vacía (un cero, que no se había inventado todavía) a la derecha.

Cuando mayor sea la base, más complicado es calcular en ella, pues se necesita aprender tablas (de multiplicación) más largas (con b productos). 

Cuando menor sea la base, más largos se vuelven la escritura de los números: por ejemplo, la escritura de un número en base binaria es ln 10/ ln 2 veces más larga que su escritura en base decimal, o sea 3,3 veces más, en promedio (la longitud es proporcional al inverso del logaritmo de la base).

La cuestión de saber qué base es la más práctica no tiene respuesta sencilla. Sin embargo, se puede afirmar que la base decimal no tiene nada de excepcional, y que es superada con creces por la base seis, que ofrece la ventaja de tener criterios de divisibilidad sencillos para dividir por 2, 3, 4, ... hasta once; mientras que en base decimal, el 7 no tiene criterio asequible.

Todo número real se puede escribir en "base b", es decir, descomponer en las potencias de b, las b, con k entero positivo o negativo.
Por ejemplo, en base diez:

Si la descomposición necesita una infinidad de cifras, se dice que el número no es decimal. 1/3 = 0,333333333333... no es decimal, pero en base tres, un tercio es 1/10 = 0,1 que si lo es (habría que inventar una palabra como "triemal" para significar "decimal en base tres").
No hay unicidad de la escritura de un real en una base, como lo muestra la igualdad 1 = 0,999999999999... pero, si se decide que no se autoriza la sucesión infinita de dígitos "b-1" en base b, se demuestra que sí hay una única manera de escribir un real en base b



</doc>
<doc id="4680" url="https://es.wikipedia.org/wiki?curid=4680" title="E">
E

La e (en mayúscula E, nombre "e", plural "es" o "ees") es la quinta letra del alfabeto español y del alfabeto latino básico y su segunda vocal. Tiene dos formas para el plural: es o ees, siendo más recomendada la primera. 

Representa en español el sonido de una vocal media y anterior.

La "hê" semítica probablemente representó inicialmente una oración o figura humana que se llamaba ("hillul" festejar), y probablemente estaba basada en un jeroglífico egipcio similar que era pronunciado y utilizado en forma distinta. En semítico, la letra representaba (y en palabras extranjeras), en griego "hê" se convirtió en Εψιλον (Epsilon) con el valor . Los etruscos y romanos la empleaban de la misma forma. El uso en inglés puede ser distinto como consecuencia del "Great Vowel Shift", o sea (a partir de como en las palabras inglesas "me" o "bee"), mientras que en otras palabras, como por ejemplo "bed"; la pronunciación es similar al latín y otras lenguas en uso.

En español, antiguamente se usaba como conjunción copulativa, proveniente del latín "et". Actualmente se utiliza la semivocal "y" /i/, salvo cuando se encuentra antes del sonido /i/ formando diptongo con la sucesiva, para evitar el hiato. Aunque sí debe usarse "y" cuando comienza una frase empleándose de manera adverbial para expresar énfasis (por ejemplo ""¿Y Inés?"").
Ejemplos de palabras que empiezan con E: Elefante, Estambul, Estanco, Everest, Europa, Elisa, etc.
Se considera que es la letra que más se repite en los textos en español. También es la más frecuente en los idiomas checo, danés, neerlandés, inglés, francés, alemán, húngaro, latín, noruego y sueco.

En alfabeto fonético aeronáutico se le asigna la palabra Eco.
En código Morse es: codice_1

En Unicode la E mayúscula posee el código U+0045 y la e minúscula es U+0065.

El código ASCII para la E mayúscula es 69 y para la e minúscula es 101; o en sistema binario 01000101 y 01100101, respectivamente.

El código EBCDIC para la E mayúscula es 197 y para la e minúscula es 133.

Las referencias numéricas en HTML y XML son "E" y "e" para la mayúscula y minúscula, respectivamente.




</doc>
<doc id="4682" url="https://es.wikipedia.org/wiki?curid=4682" title="Función exponencial">
Función exponencial

En matemáticas, una función exponencial es una función de la formaformula_1 en el que el argumento x se presenta como un exponente. Una función de la forma formula_2 también es una función exponencial, ya que puede reescribirse como

Como funciones de una variable real, las funciones exponenciales se caracterizan únicamente por el hecho de que la tasa de crecimiento de dicha función (es decir, su derivada) es directamente proporcional al valor de la función. La constante de proporcionalidad de esta relación es el logaritmo natural de la base "b":formula_4 La constante es la base única para la cual la constante de proporcionalidad es 1, de modo que la derivada de la función es en sí misma:formula_5Dado que el cambio de la base de la función exponencial simplemente da como resultado la aparición de un factor constante adicional, es computacionalmente conveniente reducir el estudio de las funciones exponenciales en el análisis matemático al estudio de esta función particular, llamada convencionalmente la "función exponencial natural", o simplemente, "la función exponencial" y denotada porformula_6 o bienformula_7Si bien ambas notaciones son comunes, la primera se usa generalmente para los exponentes más simples, mientras que la última tiende a usarse cuando el exponente es una expresión complicada.

La función exponencial satisface la identidad multiplicativa fundamental formula_8 para todo formula_9Esta identidad se extiende a los exponentes de valores complejos. Se puede mostrar que cada solución continua, distinta de cero, de la ecuación funcional formula_10 es una función exponencial, formula_11 con La identidad multiplicativa fundamental, junto con la definición del número como , muestra que formula_12 para enteros positivos y relaciona la función exponencial con la noción elemental de exponenciación.

El argumento de la función exponencial puede ser cualquier número real o complejo o incluso un tipo de objeto matemático completamente diferente (por ejemplo, una matriz).

Su omnipresente aparición en matemáticas puras y aplicadas ha llevado al matemático W. Rudin a opinar que la función exponencial es ""la función más importante en matemáticas"". En los ajustes aplicados, las funciones exponenciales modelan una relación en la que un cambio constante en la variable independiente proporciona el mismo cambio proporcional (es decir, aumento o disminución de porcentaje) en la variable dependiente. Esto ocurre ampliamente en las ciencias naturales y sociales; por lo tanto, la función exponencial también aparece en una variedad de contextos dentro de la física, la química, la ingeniería, la biología matemática y la economía.

La gráfica de formula_13está inclinada hacia arriba, y aumenta más rápido a medida que aumenta. El gráfico siempre se encuentra por encima del eje , pero puede estar arbitrariamente cerca de él para negativo; Así, el eje es una asíntota horizontal. La pendiente de la tangente a la gráfica en cada punto es igual a su coordenada y en ese punto, como lo indica su función derivada. Su función inversa es el logaritmo natural, denotado formula_14 formula_15 o formula_16debido a esto, algunos textos antiguos se refiere a la función exponencial como el antilogaritmo.

La función exponencial real formula_17 se puede caracterizar de varias maneras equivalentes. Más comúnmente, se define por las siguientes series de potencias:

Como el radio de convergencia de esta serie de potencias es infinito, esta definición es, de hecho, aplicable a todos los números complejos formula_19. La constante puede definirse como

La diferenciación término por término de esta serie de potencias revela que formula_20 para todas las reales, lo que lleva a otra caracterización común de formula_21 como la única solución de la ecuación diferencial

satisfaciendo la condición inicial formula_23

Basándose en esta caracterización, la regla de la cadena muestra que su función inversa, el logaritmo natural, satisface formula_24 para formula_25 o formula_26 Esta relación lleva a una definición menos común de la función exponencial real formula_21como la solución formula_28 a la ecuación

Por medio del teorema del binomio y la definición de la serie de potencias, la función exponencial también se puede definir como el siguiente límite:

La función exponencial surge cuando una cantidad crece o decae a una tasa proporcional a su valor actual. Una de esas situaciones es el interés continuamente compuesto, y de hecho, fue esta observación la que llevó a Jacob Bernoulli en 1683 al número 

ahora conocido como "e". Más tarde, en 1697, Johann Bernoulli estudió el cálculo de la función exponencial.

Si una cantidad principal de 1 gana intereses a una tasa anual de capitalización mensual, entonces el interés ganado cada mes es veces el valor actual, por lo que cada mes el valor total se multiplica por , y el valor al final del año es . Si, en cambio, el interés se agrava diariamente, esto se convierte en . Dejar que el número de intervalos de tiempo por año crezca sin límite lleva a la definición límite de la función exponencial,

primero dado por Leonhard Euler. Esta es una de varias caracterizaciones de la función exponencial; Otros implican series o ecuaciones diferenciales.

De cualquiera de estas definiciones se puede mostrar que la función exponencial obedece a la identidad de exponenciación básica,

lo que justifica la notación .

La derivada (tasa de cambio) de la función exponencial es la función exponencial en sí misma. Más generalmente, una función con una tasa de cambio proporcional a la función en sí misma (en lugar de ser igual a ella) es expresable en términos de la función exponencial. Esta propiedad de función conduce a un crecimiento exponencial o decaimiento exponencial.

La función exponencial se extiende a una función completa en el plano complejo. La fórmula de Euler relaciona sus valores en argumentos puramente imaginarios con funciones trigonométricas. La función exponencial también tiene análogos para los cuales el argumento es una matriz, o incluso un elemento de un álgebra de Banach o un álgebra de Lie.

La importancia de la función exponencial en matemáticas y ciencias proviene principalmente de su definición como función única que es igual a su derivada y es igual a 1 cuando . Es decir, 

Las funciones de la forma para la constante son las únicas funciones que son iguales a su derivada (por el teorema de Picard-Lindelöf). Otras formas de decir lo mismo incluyen: 


Si la tasa de crecimiento o decaimiento de una variable es proporcional a su tamaño, como es el caso del crecimiento poblacional ilimitado (ver catástrofe maltusiana), interés compuesto continuamente o decaimiento radiactivo, entonces la variable puede escribirse como una función exponencial por el tiempo. Explícitamente para cualquier constante real k, una función satisface si y solo si f (x) = cekx para alguna constante c. , a function satisfies if and only if for some constant .

Además, para cualquier función diferenciable "f(x)", encontramos, por la regla de la cadena:

Una fracción continua para puede obtenerse a través de una identidad de Euler: 

La siguiente fracción continua generalizada para converge más rápidamente:

o bien, aplicando la sustitución. :

con un caso especial para :

Esta fórmula también converge, aunque más lentamente, para z> 2. Por ejemplo:

Como en el caso real, la función exponencial se puede definir en el plano complejo en varias formas equivalentes. La definición más común de la función exponencial compleja es paralela a la definición de la serie de potencias para los argumentos reales, donde la variable real se reemplaza por una compleja:

La multiplicación de dos copias de estas series de potencias en el sentido de Cauchy, permitida por el teorema de Mertens, muestra que la propiedad multiplicativa definitoria de las funciones exponenciales sigue siendo válida para todos los argumentos complejos:

La definición de la función exponencial compleja a su vez conduce a las definiciones apropiadas que extienden las funciones trigonométricas a argumentos complejos.

En particular, cuando formula_44 (formula_45real), la definición de la serie produce la expansión

En esta expansión, la reorganización de los términos en partes reales e imaginarias se justifica por la convergencia absoluta de la serie. Las partes reales e imaginarias de la expresión anterior de hecho corresponden a las expansiones de la serie de formula_47 y formula_48, respectivamente.

Esta correspondencia proporciona motivación para definir el coseno y el seno para todos los argumentos complejos en términos de formula_49y la serie de potencias equivalentes:

Las funciones exp, cos y sin, así definidas, tienen un radio infinito de convergencia por la prueba de relación y, por lo tanto, son funciones completas (es decir, holomorfas en formula_53). El rango de la función exponencial es formula_54, mientras que los rangos de las funciones complejas de seno y coseno son formula_53 en su totalidad, en de acuerdo con el teorema de Picard, que afirma que el rango de una función completa no constante es formula_53 o formula_53excluyendo un valor lacunario. 

Estas definiciones para las funciones exponenciales y trigonométricas conducen trivialmente a la fórmula de Euler:

Alternativamente, podríamos definir la función exponencial compleja basada en esta relación. Si formula_60, donde formula_61y formula_28son reales, podríamos definir su exponencial como

donde exp, cos y pecado en el lado derecho del signo de definición deben interpretarse como funciones de una variable real, previamente definida por otros medios.

Para formula_64, la relación formula_65se mantiene, por lo que formula_66para real formula_45 y formula_68mapea la línea real (mod formula_69) al círculo unitario. Sobre la base de la relación entre formula_70 y el círculo unitario, es fácil ver que, restringido a argumentos reales, las definiciones de seno y coseno dadas anteriormente coinciden con sus definiciones más elementales basadas en nociones geométricas.

La función exponencial compleja es periódica con el período formula_71 y formula_72 para todos formula_73. 

Cuando su dominio se extiende desde la línea real al plano complejo, la función exponencial conserva las siguientes propiedades:


Extender el logaritmo natural a argumentos complejos produce el logaritmo complejo , que es una función multivalor.

Podemos definir una exponenciación más general:

para todos los números complejos y w. Esta es también una función multivalor, incluso cuando es real. Esta distinción es problemática, ya que las funciones multivalor y se confunden fácilmente con sus equivalentes de un solo valor al sustituir un número real por . La regla sobre la multiplicación de exponentes para el caso de números reales positivos debe modificarse en un contexto multivalor:

La función exponencial mapea cualquier línea en el plano complejo a una espiral logarítmica en el plano complejo con el centro en el origen. Cabe señalar dos casos especiales: cuando la línea original es paralela al eje real, la espiral resultante nunca se cierra sobre sí misma; cuando la línea original es paralela al eje imaginario, la espiral resultante es un círculo de algún radio.

Considerando la función exponencial compleja como una función que involucra cuatro variables reales:

La gráfica de la función exponencial es una superficie bidimensional que se curva a través de cuatro dimensiones.

Comenzando con una parte codificada por colores del dominio formula_81, las siguientes son representaciones de la gráfica como se proyecta de manera diversa en dos o tres dimensiones. La segunda imagen muestra cómo se mapea el plano complejo de dominio en el plano complejo de rango:


La tercera y cuarta imágenes muestran cómo el gráfico en la segunda imagen se extiende en una de las otras dos dimensiones que no se muestran en la segunda imagen.

La tercera imagen muestra el gráfico extendido a lo largo del eje real formula_61. Muestra que la gráfica es una superficie de revolución sobre el eje formula_61de la gráfica de la función exponencial real, que produce una forma de bocina o embudo. 

La cuarta imagen muestra el gráfico extendido a lo largo del eje imaginario formula_28. Muestra que la superficie del gráfico para valores formula_28positivos y negativos realmente no coinciden con el eje real formula_83negativo, sino que forma una superficie en espiral alrededor del eje formula_28. Debido a que sus valores formula_28se han extendido a ± 2π, esta imagen también representa mejor la periodicidad 2π en el valor imaginario formula_28.

La exponenciación compleja se puede definir convirtiendo coordenadas polares y usando la identidad :

Sin embargo, cuando no es un número entero, esta función es multivalor, porque no es única.

Si se toma como base el número complejo "a" diferente de "e", y como variable el exponente "z", se tiene que la función exponencial general "w" = f("z")=formula_94, se define como: :Es una familia de funciones unívocas, no ligadas entre sí, que se distinguen por los factores exp(2kπi"z"), siendo "k" cualquier número entero. 

La definición de la serie de potencias de la función exponencial tiene sentido para las matrices cuadradas (para las cuales la función se denomina matriz exponencial) y más generalmente en cualquier álgebra de Banach. En esta configuración, , y es invertible con e inversa para cualquier x en . Si , entonces , pero esta identidad puede fallar para no conmutar e y. 

Algunas definiciones alternativas llevan a la misma función. Por ejemplo, puede definirse como:

O puede definirse como , donde es la solución a la ecuación diferencial con condición inicial . 

Dado un Grupo de Lie y su álgebra de Lie asociada formula_96, el mapa exponencial es un mapa formula_96↦ que satisface propiedades similares. De hecho, dado que es el álgebra de Lie del grupo de Lie de todos los números reales positivos bajo multiplicación, la función exponencial ordinaria para los argumentos reales es un caso especial de la situación del álgebra de Lie. De manera similar, como el grupo de Lie de matrices invertibles tiene como álgebra de Lie , el espacio de todas las matrices , la función exponencial para matrices cuadradas es un caso especial de Mapa exponencial de álgebra de Lie.

La identidad puede fallar para los elementos del álgebra de Lie y que no conmutan; La fórmula de Baker – Campbell – Hausdorff proporciona los términos de corrección necesarios.

La función no está en (es decir, no es el cociente de dos polinomios con coeficientes complejos).

Para números complejos distintos }, el conjunto } es linealmente independiente sobre .

La función es trascendental sobre 

Al computar (una aproximación de) la función exponencial, si el argumento está cerca de 0, el resultado será cercano a 1, y computar la diferencia formula_98 puede producir una pérdida de precisión.

Siguiendo una propuesta de William Kahan, puede ser útil tener una rutina dedicada, a menudo llamada codice_1, para calcular directamente, sin pasar por el cálculo de . Por ejemplo, si la exponencial se calcula utilizando su serie de Taylor

uno puede usar la serie de Taylor formula_100

Esto se implementó por primera vez en 1979 en la calculadora Hewlett-Packard HP-41C, y fue proporcionado por varias calculadoras, sistemas de álgebra computacional y lenguajes de programación (por ejemplo, C99). 

Se ha utilizado un enfoque similar para el logaritmo.(ver lnp1).

Una identidad en términos de la tangente hiperbólica,

proporciona un valor de alta precisión para valores pequeños de x en sistemas que no implementan .




</doc>
<doc id="4684" url="https://es.wikipedia.org/wiki?curid=4684" title="23 de enero">
23 de enero

El 23 de enero es el 23.º (vigesimotercer) día del año en el calendario gregoriano. Quedan 342 días para finalizar el año y 343 en años bisiestos.

















</doc>
<doc id="4686" url="https://es.wikipedia.org/wiki?curid=4686" title="24 de enero">
24 de enero

El 24 de enero es el vigesimocuarto día del año en el calendario gregoriano. Quedan 341 días para finalizar el año y 342 en los años bisiestos.

1875 se estrena la Danse Macabre de Camille Saint Saens












</doc>
<doc id="4687" url="https://es.wikipedia.org/wiki?curid=4687" title="26 de enero">
26 de enero

El 26 de enero es el 26.º (vigesimosexto) día del año en el calendario gregoriano. Quedan 339 días para finalizar el año y 340 en los años bisiestos.









</doc>
<doc id="4689" url="https://es.wikipedia.org/wiki?curid=4689" title="Ignacio Domeyko">
Ignacio Domeyko

Ignacio Domeyko Ancuta (; Niedźwiadka Wielka, Imperio ruso; -Santiago, Chile; ) fue un científico polaco a quien el gobierno chileno le concedió la nacionalidad chilena en 1848.

Nació el 31 de julio de 1802 en la localidad de Niedźwiadka Wielka, en ese entonces territorio polaco-lituano que recientemente había pasado a dominio del Imperio ruso. En la actualidad, el pueblo forma parte administrativamente del raión de Kareličy (Goradnia, Bielorrusia) con el nombre de Miadzviedka.

Perteneciente a la antigua nobleza polaca, Domeyko se consideró siempre polaco. Estudió en la Universidad de Vilna. Tuvo que exiliarse de su país tras la derrota de los patriotas polacos y lituanos en la insurrección de 1831 en contra de la dominación rusa. En París estudió en La Sorbona, el Colegio de Francia, el Jardín Botánico y la Escuela de Minas. En 1838 llega a Chile contratado por las autoridades de la provincia de Coquimbo para comenzar la enseñanza de mineralogía y química en el liceo San Bartolomé de La Serena, institución que también financia su viaje, instrumentos, materiales y colecciones, donde posteriormente revoluciona los métodos de enseñanza.

Entre 1840 y 1846 realizó viajes por gran parte del país. En estos viajes, describió la geología de extensas zonas. En 1847 fue contratado como profesor del Instituto Nacional de Chile, más tarde se le concedió la nacionalidad por gracia y contrajo matrimonio con una joven chilena, Enriqueta Sotomayor, con quien tuvo tres hijos.

Dándose cuenta de la enorme pero casi inexplorada riqueza minera de Chile impulsó presionando fuertemente a las autoridades chilenas para que se creasen las Escuelas de Minas de La Serena y Copiapó respectivamente; en la primera Domeyko se encargó personalmente de crearla, dirigirla y hacer clases, en tanto que en la escuela copiapina dirigió su creación, eligiendo con pinzas a sus académicos, varios de ellos egresados de la recién creada Universidad de Chile y la mayoría provenientes de Alemania y Francia, visitándola de vez en cuando para supervisarla.

Fue miembro del claustro académico de la Universidad de Chile y posteriormente fue elegido rector de la misma. En tal cargo desarrolló una extensa labor, siendo la principal de ella, la separación de las funciones de superintendencia de educación que ejercía sobre el sistema desde su creación, traspasando tales responsabilidades al recién creado Ministerio de Educación. En 1879 el Congreso Nacional dictó una ley por la que se separaban las funciones del Instituto Nacional, encomendándosele a la Universidad el trabajo de convertirse desde una unidad exclusivamente académica a una de docencia.

No retornó a su patria natal hasta el viaje que hizo entre 1884 y 1889 a Siria otomana (actual Israel), la Ciudad del Vaticano, Polonia (Cracovia y Varsovia), Lituania y la actual Bielorrusia. De su país natal trajo un saco de tierra que puso en su patio. Aún es conservado por sus descendientes.

Falleció en Santiago el 23 de enero de 1889 por causas naturales, a los 87 años.






</doc>
<doc id="4691" url="https://es.wikipedia.org/wiki?curid=4691" title="27 de enero">
27 de enero

El 27 de enero es el 27.º (vigesimoséptimo) día del año en el calendario gregoriano. Quedan 338 días para finalizar el año y 339 en los años bisiestos.









</doc>
<doc id="4692" url="https://es.wikipedia.org/wiki?curid=4692" title="28 de enero">
28 de enero

El 28 de enero es el 28.º (vigesimoctavo) día del año en el calendario gregoriano. Quedan 337 días para finalizar el año y 338 en los años bisiestos.









</doc>
<doc id="4693" url="https://es.wikipedia.org/wiki?curid=4693" title="Juan Carlos I de España">
Juan Carlos I de España

Juan Carlos I de España (Roma, 5 de enero de 1938) fue rey de España desde el 22 de noviembre de 1975 hasta el 19 de junio de 2014, fecha de su abdicación y del acceso a la Jefatura del Estado de su hijo Felipe VI. Tras su renuncia, Juan Carlos continuó usando el título de rey, con carácter honorífico, y pasó a ser capitán general de las Fuerzas Armadas en la reserva, aunque sin ejercer funciones constitucionales. En 2019 se retiró oficialmente de la vida pública e institucional y un año después se trasladó a Emiratos Árabes Unidos.

La solemne proclamación se celebró el 22 de noviembre de 1975, dos días después de la muerte de Francisco Franco y de acuerdo con la Ley de Sucesión en la Jefatura del Estado de 1947 y la Ley de 22 de julio de 1969. La Constitución española, ratificada por referéndum popular el 6 de diciembre de 1978 y promulgada el 27 de diciembre del mismo año, reconoce explícitamente su persona como rey de España y legítimo heredero de la dinastía histórica de Borbón, y le otorga la Jefatura del Estado. La carta magna confiere a su dignidad el rango de símbolo de la unidad nacional. Antes de su proclamación, debido a la delicada salud de Franco, había desempeñado intermitentemente funciones interinas en la Jefatura del Estado.

La positiva influencia del rey en la Transición española y su papel durante el intento de golpe de Estado de 1981, así como su apoyo a la unidad europea y su contribución a la hora de estrechar relaciones diplomáticas, le hicieron merecedor de múltiples homenajes, reconocimientos, premios y galardones internacionales. A ese respecto, la revista "Time" lo consideró «uno de los héroes más sorprendentes e inspiradores de la libertad del siglo XX, al desafiar un intento de golpe militar que pretendía subvertir la joven democracia posfranquista de España». 

Sin embargo, la segunda parte de su reinado fue más controvertida. Su imagen ante los medios de comunicación y ante la opinión pública empezó a deteriorarse a raíz del caso Nóos, un juicio por corrupción que implicaba directamente a una de sus hijas, la infanta Cristina, y que culminaría con el ingreso en prisión del esposo de esta, Iñaki Urdangarín. Después, en 2012, el propio monarca sufrió un accidente en Botsuana por el que hubo de ser evacuado a España; por ese percance se supo que había viajado al país africano para participar en una cacería de elefantes patrocinada por influyentes hombres de negocios saudíes y organizada por su entonces amante Corinna zu Sayn-Wittgenstein. 

En junio de 2014 abdicó en su hijo Felipe, que subió al trono como Felipe VI de España. Se decretó, sin embargo, que Juan Carlos conservara de forma vitalicia y honorífica el título de rey, el tratamiento de "Majestad" y honores análogos a los del heredero de la Corona. Cinco años después, en junio de 2019, comunicó que abandonaba definitivamente la vida institucional, y un año más tarde, debido a las crecientes sospechas de corrupción, fue despojado por Felipe VI de la asignación presupuestaria que venía percibiendo de la Casa del Rey. 

El 3 de agosto de 2020, la Casa del Rey hizo pública la voluntad de Juan Carlos de abandonar España ante la repercusión pública generada por «ciertos acontecimientos pasados» de su «vida privada», sin precisar el país de destino, aunque dos semanas después fue la propia Casa del Rey quien comunicó ante las crecientes especulaciones e informaciones que Juan Carlos se encontraba en Abu Dabi, capital de Emiratos Árabes Unidos.

Juan Carlos ("Juanito", o "don Juanito", entre los más allegados, para diferenciarlo de su padre) nació en Roma en el año 1938. Al poco de nacer, sus padres se trasladaron a Villa Gloria, una casa de cuatro pisos en el elegante barrio romano de Parioli. Fue bautizado el 26 de enero de 1938 en la capilla del Palacio Magistral de la Orden de Malta de Roma por el cardenal secretario de Estado de la Santa Sede, monseñor Eugenio Pacelli, futuro papa Pío XII. Su abuela paterna, la reina Victoria Eugenia, fue la madrina, y su abuelo materno, Carlos Tancredo de Borbón-Dos Sicilias, príncipe de las Dos Sicilias e infante de España, el padrino. En 1942 se trasladó junto con el resto de su familia a la ciudad suiza de Lausana.

En una entrevista celebrada el 25 de agosto de 1948 entre Franco y el conde de Barcelona en el golfo de Vizcaya, se acordó que el príncipe se trasladaría a España para cursar allí sus estudios. El 8 de noviembre de 1948, a los diez años de edad, Juan Carlos pisó por primera vez suelo español. Allí estudiaría durante ese año académico. Tras el verano de 1949, sin embargo, el deterioro de las relaciones entre Franco y don Juan llevarían a este último a decidir que su hijo no volviera por el momento a España.

Tras un año en Estoril (junto a Lisboa), Juan de Borbón accedió a que Juan Carlos regresara a España en el otoño de 1950 para continuar sus estudios, en esta ocasión acompañado de su hermano menor Alfonso. Para el verano de 1954, Juan Carlos había terminado el bachillerato. Posteriormente realizó su instrucción militar en la Academia General Militar de Zaragoza (1955-1957), en la Escuela Naval Militar de Marín en Pontevedra (1957-1958) y finalmente en la Academia General del Aire de San Javier en Murcia (1958-1959). Completó su formación en la Universidad de Madrid, donde cursó estudios de Derecho Político e Internacional, Economía y Hacienda Pública.

Durante las vacaciones de Semana Santa de 1956, el 29 de marzo, Jueves Santo, en la residencia familiar de Estoril, llamada todavía hoy "Villa Giralda", Juan Carlos, que ya tenía 18 años cumplidos, disparó accidentalmente un revólver mientras jugaba en el desván de la casa con su hermano menor, Alfonso, lo que causaría la muerte de Alfonso. El hermano mayor del conde de Barcelona y tío de Juan Carlos, Jaime de Borbón, solicitaría meses después una investigación judicial del suceso; petición calificada por el historiador Paul Preston como de inaudita «insensibilidad y pura malevolencia» y que seguramente fue motivada por procurarse beneficios políticos a su propia causa.

El 13 de septiembre de 1961 se anunció oficialmente el compromiso de Juan Carlos con la princesa Sofía de Grecia, su prima tercera. Ocho meses después, el 14 de mayo de 1962, la pareja contraía matrimonio en Atenas por los ritos ortodoxo y católico. Con anterioridad a su celebración, Franco había manifestado su interés en que Juan Carlos y Sofía vivieran en España, de modo que, a principios de 1963, y a pesar de la oposición inicial de Juan de Borbón, el matrimonio se trasladaba a Madrid para fijar su residencia en el Palacio de La Zarzuela.

El 5 de marzo de 1966, se celebró una reunión del Consejo Privado del Conde de Barcelona en Estoril para conmemorar el vigésimo quinto aniversario de la muerte de Alfonso XIII, a la que había sido invitado Juan Carlos. La reunión debía ser un acto de reafirmación de los derechos dinásticos de Juan de Borbón. Pese a que dos meses antes, Juan Carlos había declarado que «jamás» aceptaría la Corona mientras viviera su padre, decidió no asistir a la reunión a instancias de su esposa, Sofía de Grecia, utilizando como pretexto una indisposición. Juan de Borbón consideró aquel hecho como una ruptura de la unidad dinástica por parte de Juan Carlos.

De haberse cumplido las reglas dinásticas, la sucesión habría debido recaer en el padre de Juan Carlos, Juan de Borbón y Battenberg, tercer hijo y heredero de los derechos dinásticos de Alfonso XIII. Sin embargo, las no muy cordiales relaciones entre don Juan y Franco determinaron el salto en la línea de sucesión y el nombramiento de Juan Carlos como príncipe de España, título de nuevo cuño con el que Franco pretendía salvar distancias con respecto a la monarquía liberal. Dicho salto fue aceptado por el príncipe, lo que creó un conflicto interno en la Casa Real de Borbón. El instrumento jurídico del que se valió el dictador fue la Ley de Sucesión, aprobada en 1947. Finalmente, en julio de 1969 Franco designó a Juan Carlos como sucesor a título de rey, nombramiento ratificado por las Cortes Españolas el 22 de julio de 1969. Ante ellas, ese mismo día el joven príncipe juró guardar y hacer guardar las Leyes Fundamentales del Reino y los principios del Movimiento Nacional, es decir, el ideario franquista. En cualquier caso, el Conde de Barcelona no renunciaría oficialmente a sus derechos sucesorios hasta 1977.

Juan Carlos I asumió interinamente la jefatura del Estado entre el 19 de julio al 2 de septiembre de 1974, y después desde el 30 de octubre al 20 de noviembre de 1975 por enfermedades de Franco. El 9 de julio de 1974, Franco era ingresado por una flebitis en la pierna derecha. Antes de partir hacia el hospital, llamó al presidente del Gobierno, Carlos Arias Navarro, y al presidente de las Cortes franquistas, Alejandro Rodríguez de Valcárcel, para que prepararan el traspaso interino de poderes al príncipe. Con todo, dos días más tarde, Juan Carlos, que no quería un traspaso interino por parte de Franco, intentó persuadir a Arias para que hiciera ver al dictador que debía traspasarle el poder de manera definitiva. Ante la negativa del presidente del Gobierno, el príncipe pidió a Franco que no firmara el decreto de traspaso. El 19 de julio, el estado del dictador se agravó, por lo que Arias acudió al hospital para que aprobara el traspaso. El yerno de Franco, Cristóbal Martínez-Bordiú, intentó impedir que Arias entrara en la habitación del jefe del Estado. Finalmente consiguió acceder, tras lo cual convenció al dictador para que cediera el poder de manera interina, lo que provocó la furia del marqués de Villaverde y de la esposa del dictador, Carmen Polo. Juan Carlos asumía por primera vez la jefatura del Estado de manera interina.

Tras un nuevo empeoramiento de la salud de Franco, el 23 de octubre de 1975, Valcárcel y Arias Navarro acudieron a La Zarzuela para proponer al príncipe que asumiera de nuevo interinamente la jefatura del Estado. Juan Carlos se negó si la sustitución no era definitiva. El 30 de octubre, Franco padeció una peritonitis. Informado de la gravedad de su estado por el equipo médico que lo atendía, el dictador ordenó su sustitución por parte del príncipe Juan Carlos, lo que este aceptó, una vez tuvo la certeza de que la enfermedad del dictador era terminal.

Al anunciarse la muerte de Franco (20 de noviembre de 1975), Juan Carlos juró acatar los Principios del Movimiento Nacional, destinados a perpetuar el franquismo. Fue proclamado rey de España por las Cortes Españolas como Juan Carlos I de España el 22 de noviembre de 1975 y exaltado al trono el 27 de noviembre con una ceremonia de unción llamada Misa de Espíritu Santo (el equivalente a una coronación), celebrada en la histórica Iglesia de San Jerónimo el Real de Madrid. 

Fundándose en las facultades que las propias leyes franquistas le otorgaban —y a las que a la vez estaba sujeto—, impulsó el cambio de régimen en aras de facilitar el advenimiento de la democracia. Uno de sus hombres de confianza, el jurista Torcuato Fernández Miranda, se propuso buscar un respaldo jurídico para el cambio de rumbo político y proteger así al monarca de una posible acusación de perjuro. El resquicio legal lo iba a hallar en la propia Ley de Sucesión, que en su artículo décimo permitía reformar, "e incluso derogar", leyes fundamentales:

El «ardid» de Fernández-Miranda, la «llave maestra» que permitiría ir «de la ley a la ley», fue elaborar una nueva (la octava) que derogaba todas las anteriores. Así echó a andar la Ley para la Reforma Política y se iniciaba la Transición.

El 14 de mayo de 1977, su padre, el Conde de Barcelona, renunció definitivamente a sus derechos dinásticos históricos. En el solemne acto de abdicación estuvo presente, entre otros, Landelino Lavilla en calidad de Notario Mayor del Reino; tras la ceremonia Don Juan declaró que renunciaba «con mucho amor a España y cariño por mi hijo». Se reanudaba así en España la Casa de Borbón. Tras la proclamación de Juan Carlos I como rey de España, Felipe se convirtió en heredero de la Corona y el 1 de noviembre de 1977 asumió el título de Príncipe de Asturias.

El 22 de junio de 1977, Juan Carlos I envió una carta al sah de Irán, Reza Pahleví, en la que confirmaba su apuesta por la democracia, pero veía peligrar la monarquía, puesto que Adolfo Suárez, el candidato de su «plena confianza» y que consideraba soporte del sistema monárquico, carecía de las fuentes externas de financiación que disponían otras ideologías como la derecha, los comunistas y los socialistas, recalcando de estos últimos su ideología marxista (el PSOE se definió como tal hasta 1979). Finalmente, el rey solicitaba al sah «en nombre del partido político del presidente Suárez» un préstamo de diez millones de dólares como su «contribución personal al fortalecimiento de la monarquía española». La carta fue desvelada tras la publicación en 1991 del diario de Asadollah Alam, ministro del Interior y primer ministro del sah.

Durante su reinado se aprobó la Constitución española, que define las funciones del rey, suprimiendo toda participación política de la Corona y convirtiendo España en una monarquía parlamentaria; asimismo, el artículo 57 de la Constitución le reconoce como el heredero legítimo de la «dinastía histórica». La Constitución fue ratificada en referéndum del 6 de diciembre y el rey la sancionó el 27 de diciembre.

Uno de los momentos más graves a los que tuvo que hacer frente el rey Juan Carlos I fue el intento de golpe de Estado del 23 de febrero de 1981, el conocido como «23-F». Ese día, durante la segunda votación de la investidura del candidato a la Presidencia del Gobierno Leopoldo Calvo-Sotelo, se produjo la toma del Congreso de los Diputados por parte de fuerzas de la Guardia Civil al mando del teniente coronel Antonio Tejero. Simultáneamente en la Capitanía General de la III Región Militar (Valencia) el teniente general Jaime Miláns del Bosch ocupó las calles de la ciudad con tanques y hubo diversos conatos en otros puntos, tales como la toma de los estudios de Televisión Española en Prado del Rey (Madrid).

La intervención televisiva de Juan Carlos I desautorizando el golpe acabó con la insurrección, que pensaba contar con el apoyo de la Corona, y contribuyó a aumentar su carisma entre sectores políticos que hasta entonces no eran muy afines a la forma de gobierno monárquica. Después de este conflicto la monarquía quedó consolidada.

El 9 de febrero de 2012, el semanario alemán "Der Spiegel" publicó un cable diplomático desclasificado por Alemania según el cual el rey habría mostrado simpatía por los golpistas durante un encuentro con el entonces embajador de Alemania en España, Lothar Lahn. En respuesta, Rafael Spottorno, jefe de la Casa del Rey, desmintió esta atribuida simpatía y afirmó: «Ni su Majestad el Rey ni esta Casa acostumbran a valorar escritos u opiniones de terceros, que son responsabilidad exclusiva de sus autores, y que en este caso concreto no se compadecen con la realidad de unos hechos cuyo desarrollo y corolario final son de público conocimiento».

El 21 de septiembre de 1992, el entonces príncipe Salmán bin Abdulaziz de Arabia Saudí y Juan Carlos I de España inauguraron la Mezquita de la M-30, financiada con 2000 millones de pesetas del rey Fahd de Arabia Saudí.

En 1992, ante las especulaciones acerca de que Juan Carlos mantenía una relación sentimental con la mallorquina Marta Gayá, tanto el jefe de la Casa del Rey, Sabino Fernández Campo, como el presidente del Gobierno, Felipe González, manifestaron su preocupación sobre que se pudiera haber orquestado una campaña contra el rey.

La publicación en 1993 por el aristócrata José Luis de Vilallonga de "El Rey", última biografía autorizada hasta el momento por el rey Juan Carlos, suscitó controversia, por cuanto la edición española omitía comentarios de Juan Carlos I acerca del 23-F que sí aparecían en otras ediciones europeas del libro, del mismo modo que ponía en boca de Vilallonga comentarios que en otras ediciones se atribuían al propio Juan Carlos. Vilallonga había declarado meses antes en una entrevista que el rey le había pedido que, respecto del 23-F, en el libro, «dijese yo [por Vilallonga] casi todas las cosas».

El 12 de diciembre de 2011, tras las informaciones aparecidas en los medios de comunicación acerca de la probable imputación por malversación, fraude, prevaricación, falsedad y blanqueo de capitales del yerno del rey, Iñaki Urdangarin, duque consorte de Palma de Mallorca, La Zarzuela anunció que lo apartaba de todos los actos institucionales, por entender que su conducta no había sido «ejemplar». Además, durante su tradicional mensaje de Nochebuena, el rey insistió en la necesidad de un comportamiento ejemplar por parte de todas las personas con responsabilidades públicas, tras lo que afirmó que «la justicia es igual para todos», lo que se interpretó como una alusión a la probable imputación de su yerno. Con todo, tras su discurso en la solemne apertura de la X Legislatura, el 27 de diciembre, el rey Juan Carlos lamentó que se hubiera personalizado su mensaje de Navidad. Dos días más tarde, el juez instructor José Castro imputaba a Iñaki Urdangarin.

Durante su declaración ante el juez instructor en Palma, los días 25, 26 y 27 de febrero de 2012, Urdangarin manifestó que el rey le había pedido que abandonara sus negocios en marzo de 2006. Sin embargo, el 16 de abril de 2012, se hicieron públicos tres correos electrónicos escritos por Urdangarin y aportados al juez instructor por su exsocio, Diego Torres, que implicarían al rey en negocios a favor de su yerno con posterioridad a esa fecha.

El 14 de abril de 2012, Juan Carlos I sufrió una fractura de cadera durante una cacería de elefantes a la que había sido invitado en Botsuana, lo que levantó críticas desde distintos ámbitos debido a que ocurrió en la peor semana de la crisis económica y tras un discurso en el que el rey había pedido "rigor" y "sacrificios" a los españoles. Mientras que Partido Popular y Partido Socialista Obrero Español no quisieron valorar públicamente el percance, Izquierda Plural, Unión Progreso y Democracia y Esquerra Republicana de Catalunya anunciaron que preguntarían al Gobierno por este asunto en el Congreso de los Diputados. El lendakari, Patxi López, afirmó que «no estaría mal» una disculpa pública por parte del monarca. El 18 de abril, al salir del hospital donde fue intervenido, el rey se disculpó públicamente por esos hechos, situación sin precedentes desde que comenzara su reinado, calificada como un episodio absolutamente nuevo en toda la historia de la realeza.

En el año 2013, a raíz de salir a la luz la «estrecha relación» que el rey mantenía con la empresaria alemana Corinna zu Sayn-Wittgenstein, algunos medios de comunicación hicieron público que la Casa del Rey, utilizando dos millones de euros procedentes de fondos públicos de Patrimonio Nacional, remodeló profundamente la finca "La Angorrilla" —lugar muy cercano al Palacio de la Zarzuela, donde durante varios años habría vivido Corinna.
El 2 de junio de 2014, Juan Carlos I manifestó su deseo de abdicar en su hijo Felipe. Para ello, según el artículo 57 de la Constitución Española, que es el que regula la sucesión al trono, se precisaba la aprobación de una ley orgánica por parte de las Cortes Generales reunidas en sesión solemne. Así sería dictada dos semanas después la Ley Orgánica 3/2014, que en el Congreso de los Diputados obtuvo 299 votos a favor, 19 votos negativos y 23 abstenciones y en el Senado 233 votos a favor, 5 votos en contra y 20 abstenciones. 

El mismo día de hacerse público el anuncio, los principales partidos republicanos, como IU, BNG y ERC, así como movimientos sociales antimonárquicos y radicalistas,
como la Coordinadora 25-S y el Movimiento 15-M, convocaron manifestaciones en las principales capitales del país y en otros municipios, difundidas a través de las redes sociales, para reivindicar la república y la celebración de un referéndum sobre la forma de Estado, a las que asistieron decenas de miles de personas. En dichas manifestaciones se pudieron observar numerosas banderas tricolores republicanas. En Cataluña, las manifestaciones fueron principalmente convocadas por ERC, mediante un llamamiento para apoyar una república catalana independiente; en dichas manifestaciones, sumándose a la tricolor, se observaron banderas catalanas independentistas y pancartas a favor de la secesión de Cataluña.Similar situación se produjo en Galicia, cuyas manifestaciones fueron apoyadas, entre otros mentados, por el BNG y Nós-Unidade Popular, ambos a favor de la autodeterminación de Galicia; observándose banderas gallegas independentistas y consignas a favor de una república gallega independiente.El sábado 7 de junio, se impulsaron nuevamente, entre otros, por plataformas y partidos ya mencionados, manifestaciones en más de 40 ciudades españolas, reiterando la demanda anterior. La presencia en esta convocatoria fue bastante menor que la que precedió el día 2. La participación, en ambas manifestaciones, fue muy inferior a la de otras convocatorias a favor de la república desde la restauración de la monarquía.

En cualquier caso, bajo la consideración de «rey emérito» siguió desempeñando durante unos años cierto papel de representación institucional. Así, en diciembre de 2015 asistió como máxima representación del Estado a la toma de posesión como presidente de Argentina de Mauricio Macri. 

A finales de mayo de 2019, Juan Carlos comunicó oficialmente a Felipe VI que se retiraba definitivamente de la vida pública y que ya no participaría en actos oficiales.

En marzo de 2020, el Grupo Parlamentario de Esquerra Republicana y otros partidos registraron en el Congreso de los Diputados una iniciativa en la que solicitaban la creación de una comisión de investigación; el objetivo era determinar las responsabilidades «civiles, éticas y políticas» del exjefe de Estado en relación a una «donación» de 100 millones del año 2012 y relacionada con el entonces monarca, su amiga Corinna Larsen y la adjudicación de grandes obras en Arabia Saudita.

El mismo mes, fue el propio rey, Felipe VI, el que emitió un comunicado en el que renunciaba a la herencia de su padre, «así como a cualquier activo, inversión o estructura financiera cuyo origen, características o finalidad puedan no estar en consonancia con la legalidad y los criterios de rectitud e integridad que rigen su actividad institucional y privada». Según los estatutos de las sociedades Fundación Lucum y Fundación Zagazka, Felipe sería el beneficiario directo en caso de fallecimiento de su padre y encargado del sostenimiento del resto de la familia. Comunicó asimismo que retiraba a su padre la asignación que cobraba de los presupuestos de la Casa del Rey.

El 3 de agosto de 2020, la Casa del Rey hizo pública una carta en la que don Juan Carlos, dirigiéndose a su hijo el Rey, informaba de su voluntad de salir del país debido a la repercusión pública creciente de «ciertos acontecimientos pasados» de su «vida privada». Con ello parecía aludir a las investigaciones abiertas en Suiza y en España sobre los supuestos fondos de Juan Carlos I acumulados en paraísos fiscales. Por su parte, el Rey, a través del mismo comunicado, resaltaba la importancia histórica del reinado de su padre y le mostraba su «agradecimiento» por la decisión tomada. 

Tras conocerse la noticia, diversos medios especularon con la posibilidad de que cuando la carta se hizo pública Juan Carlos en realidad ya había abandonado el país.

En un primer momento, ni desde la Casa del Rey ni desde la presidencia del Gobierno se quiso desvelar el paradero del rey emérito. El Gobierno, a través del ministro del Interior Fernando Grande Marlaska, vino a reconocer que el Estado español seguía asumiendo el coste del dispositivo de seguridad del padre del Rey en el nuevo lugar de destino. Pero el 17 de agosto de 2020 finalmente la Casa Real española confirmó que Juan Carlos I se encontraba en los Emiratos Árabes Unidos desde el 3 de agosto, el mismo día que fue anunciada su decisión de abandonar España y fijar su residencia en otro país.

Coincidiendo con la aparición de estas informaciones, más de setenta ex altos cargos españoles suscribieron un manifiesto en apoyo al rey emérito por su reinado. Entre los firmantes se encontraban políticos retirados como Rodolfo Martín Villa, Alfonso Guerra, Matilde Fernández, Josep Piqué y Esperanza Aguirre o historiadores como Juan Pablo Fusi y Carmen Iglesias.

Según diversos sondeos de opinión, durante la mayor parte de su reinado el rey gozó de un nivel de popularidad muy elevado en España y en ciertas partes de Iberoamérica, donde llegó a ser considerado el líder más popular en 2008. Su figura, considerada una garantía de orden y estabilidad, siempre gozó de un elevado apoyo popular, incluso durante los primeros años de la crisis económica iniciada en 2008, mientras se producía un profundo desencanto ciudadano hacia el resto de instituciones del Estado.

Sin embargo, esta tendencia sufrió el primer cambio drástico en abril de 2012, tras saberse que había participado en una cacería llevada a cabo en Botsuana durante los peores momentos de la crisis económica. En aquel momento, el apoyo de la población, que se encontraba en el 74 %, cayó hasta el 52 %. A pesar de que el porcentaje de aprobación creció lentamente y se situó en diciembre del mismo año en el 58 %, en 2013 este porcentaje se desplomó. En abril de aquel año, por primera vez, y pese a seguir siendo la figura del sistema político español con mejor valoración —por encima de los Ayuntamientos, el Parlamento, el Gobierno, los partidos políticos y los representantes políticos—, la mayoría de la población (53 %) desaprobaba la forma en que el rey desempeñaba sus funciones, frente al 42 % que sí la aprobaba. No obstante, dos meses después de este dato, la confianza ciudadana subió ocho puntos porcentuales hasta situarse en el 50 % de aprobación. A pesar de situarse lejos de los datos obtenidos en años anteriores, el apoyo ciudadano seguía siendo superior al obtenido por el resto de instituciones del sistema político español y también superior al obtenido por otros jefes de Estado en sus respectivos países (como en Estados Unidos, Francia o Italia).

En un sondeo de opinión realizado en junio de 2014, pocos días después de anunciarse su abdicación, el rey Juan Carlos obtuvo un 6,9 sobre 10 a la hora de calificar el respeto que inspiraba su figura entre la ciudadanía. 

A pesar de la renuncia, la institución de la Corona no recuperó la popularidad perdida; de hecho, en el barómetro del Centro de Investigaciones Sociológicas de abril de 2015 apenas obtenía un 4,34 sobre 10. Desde ese año, el organismo ya no pregunta a los españoles sobre la Jefatura del Estado. 

Según José Álvarez Junco:
Según Santos Juliá:
Según Juan Pablo Fusi:

Según Victoria Prego, autora del libro "Así se hizo la Transición" (1995):
Según Charles Powell:
Según Paul Preston:

Algunas ONG y movimientos sociales sostuvieron que, en sus visitas a Marruecos, el rey actuaba como intermediario del Gobierno español en la venta de armas a este país que habrían sido utilizadas para reprimir al pueblo saharaui. También se le ha criticado su conocida amistad con las familias reales de países de Oriente medio como Arabia Saudí, Kuwait o Emiratos Árabes Unidos, países con regímenes autoritarios, destacando el caso de Arabia Saudí, cuya monarquía absoluta controla todos los organismos del Estado y ha sido durante años acusada de corrupción masiva y de constituir un régimen "feudal" y "no libre".

Dentro de las críticas al rey a menudo también se han incluido a los medios de comunicación españoles, que según sus críticos dan una imagen deliberadamente positiva de su figura, que incluso algunos medios extranjeros han señalado como un auténtico culto a la personalidad.

Otras críticas se refirieron a la irresponsabilidad penal del monarca, consagrada en la Constitución Española, que lo hacían inimputable por cualquier delito que pudiera cometer. Además, diversos autores han señalado el tabú existente en los medios de comunicación españoles en torno a la figura del rey. También ha sido criticado en algunos sectores su papel en el 23-F, el fallido golpe de Estado que tuvo lugar en 1981, pues el rey habría sabido previamente de su existencia o incluso podría haber sido partícipe. Del mismo modo, algunos autores consideraron inadecuado el "¿Por qué no te callas?" que el rey espetó al presidente venezolano Hugo Chávez en la XVII Cumbre Iberoamericana.

En el año 2007, "The Times", uno de los periódicos más importantes del Reino Unido, criticó el "lujoso estilo de vida" del rey y la "idealización" que se ha hecho de su figura durante 30 años, al tiempo que lo calificaba de "playboy".

Según una investigación periodística del diario Público, que tuvo acceso a documentos clasificados, publicada en 2014, el rey Juan Carlos I habría intermediado entre la dictadura militar de Jorge Rafael Videla en Argentina y el gobierno de España presidido por Adolfo Suárez desde 1976. Según la investigación y los documentos confidenciales, España habría proporcionado ayuda económica a través de acuerdos comerciales y diplomáticos. A su vez, el rey también habría hecho de intermediario entre la dictadura argentina y grandes empresarios y banqueros españoles, entre los que se encontraría Emilio Botín padre, propietario del Banco Santander. La necesidad de Argentina por obtener divisas provendría de los grandes gastos que suponían en esos años sus programas de represión política "(véase Vuelos de la muerte y Desaparecidos durante el Proceso de Reorganización Nacional"). La investigación también señalaba el intercambio de regalos y condecoraciones entre altos cargos de ambas naciones —por ejemplo el rey en 1978 le concedió a Videla la gran cruz de la Orden del Mérito Militar y el collar de la Orden de Isabel la Católica, mientras que el entonces príncipe Felipe (Felipe VI) fue nombrado por la Armada Argentina Guardiamarina Honoris Causa en 1981—. España también habría dado cursos a 33 militares argentinos entre 1976 y 1983 (ya en democracia) partícipes de la represión en su país.

En la madrugada del viernes 13 de abril de 2012, Juan Carlos sufrió una caída al tropezar en la oscuridad con un escalón, y este percance doméstico desencadenaría la mayor crisis de su reinado hasta entonces. La razón era que se había producido en un bungaló de lujo en Botsuana, adonde el rey se había desplazado con unos amigos para practicar la cacería de elefantes mientras que España se encontraba sumida en una grave crisis económica iniciada en 2008. 

Dada la gravedad de la caída y la avanzada edad del monarca (74 años), se decidió repatriarle a España. Fue ingresado en el hospital USP San José, de Madrid, donde hubo de ser sometido a dos operaciones quirúrgicas para sustituir una prótesis de cadera y reparar una triple fractura de fémur. Al salir de la habitación tras recibir el alta hospitalaria, se dirigió a una cámara de televisión que le aguardaba y pidió disculpas con estas palabras: «Lo siento mucho; me he equivocado y no volverá a ocurrir». Esta frase se convertiría con el tiempo en una de las más célebres de su reinado.

En un principio, ni la Casa del Rey ni el Gobierno informaron del viaje, y nunca se dio versión oficial fidedigna alguna de lo que había sucedido a lo largo de esos cinco días de estancia en el país africano. La información disponible proviene de investigaciones periodísticas. Así se supo que la comitiva había partido de Montecarlo en un "jet" privado que trece horas después aterrizó en Maun, al norte del país africano, donde tomó un helicóptero hasta el campamento Qorokwe, en el delta del río Okavango. Juan Carlos iba acompañado de su amante, Corinna zu Sayn-Wittgenstein, de su amigo Philip Adkins (primer marido de Corinna) y de Mohamed Eyad Kayali, magnate sirio de la construcción, representante de la Casa Real de Arabia Saudí en España y hombre de confianza del príncipe Mohamed bin Salmán. Con Corinna viajó también su hijo Alexander, de diez años de edad, al que Juan Carlos había invitado como regalo por su décimo cumpleaños.

El día 11, el monarca abatió a un elefante de 50 años tras dispararle siete veces con su rifle Rigby Express del calibre 470. El día 12, el magnate sirio dio cuenta de un segundo ejemplar y, al día siguiente (mientras el resto de la expedición retornaba de urgencia a España), de un tercero, que en principio estaba destinado a Adkins.

Según informaciones del diario digital "El Español", el coste por persona era de al menos 50 000 euros. Para "ABC", cazar un elefante "costaba" en 2012 unos 37 000 euros. Por su parte, Ana Romero, en su libro "Final de partida", sostiene que «entre elefantes, campamento, acompañamiento, helicópteros, el "jet" que los llevó desde Montecarlo a Maun y los extras de los escoltas y el médico, el safari de apenas seis días rondó los tres cuartos de millón de euros».

Todos los protagonistas aseguraron que fue Kayali quien corrió con todos los gastos excepto el avión, que fue pagado por Adkins. Pero esas declaraciones no mitigaron el malestar de ciertos medios: primero hacia el monarca, por aceptar regalos tan onerosos; y después hacia el Gobierno, por no informar del coste de haber desplazado a África a los escoltas y al médico intensivista que, como siempre, viajaron con él.

La fotografía del monarca y el director de Rann Safaris posando delante de un elefante muerto y con la trompa colocada contra un árbol fue recogida en portada por numerosos medios informativos y significó el comienzo del declive de su reinado ante la opinión pública española.

En aquellos días de abril, la situación política y económica en España era muy delicada. La ciudadanía estaba perdiendo poder adquisitivo debido a la subida de impuestos mientras asistía a la aplicación de una amnistía fiscal decretada por el Gobierno de Mariano Rajoy, el paro alcanzaba el 27 por ciento de la población activa y la economía española estaba a un paso de ser intervenida por la Unión Europea. Precisamente, el miércoles 11, el mismo día en que el monarca cobró su presa, la versión impresa del diario "El País" titulaba a cinco columnas: «Los mercados acentúan el ataque», y en subtítulos añadía: «El anuncio del Gobierno de nuevos recortes no consigue calmar a los mercados» y «El Ibex cae a su mínimo en tres años y la prima de riesgo se dispara a 433 puntos». 

Había disgusto, decepción y hasta enfado hacia el monarca, sobre todo si se recordaban algunas de sus últimas declaraciones: en un discurso pronunciado apenas un mes antes del viaje había manifestado: «El 50% de los jóvenes está en paro y eso es algo que a veces me quita el sueño»; y en su último mensaje de Navidad había declarado: «Sobre todo las personas con responsabilidades públicas tenemos el deber de observar un comportamiento adecuado, un comportamiento ejemplar».

En un editorial publicado el sábado 14, el diario "El Mundo" sentenciaba:

La imagen de la monarquía se desplomó a mínimos históricos. En el barómetro del CIS de abril de 2013, los españoles calificaron su confianza en la institución con un 3,68 sobre 10, en contraste con el 7,48 que había cosechado en noviembre de 1995.

El fondo soberano de la Saudi Arabian General Investment Authority, inicialmente bautizado Fondo Hispano-Saudí de Infraestructuras (SSIF, por sus siglas en inglés), se gestó durante la visita oficial de Juan Carlos a Arabia Saudí en abril de 2006 y quedó sellada dos meses después, cuando el monarca saudí devolvió la visita a España. Ideado en principio como un consorcio de empresas hispano-saudí, fue presentado en el Palacio de El Pardo en junio de 2007, aunque para entonces ya se había fijado su sede social en la isla de Guernsey, un paraíso fiscal ubicado en el canal de la Mancha. La invitación a los inversores españoles se había tramitado directamente desde el Palacio de la Zarzuela. El proyecto consistía en crear un fondo de inversión de 1000 millones de dólares «para el desarrollo de inversiones en infraestructuras de energía (en particular de energías renovables), transporte y telecomunicaciones», y como intermediaria de operaciones figuraba, entre otros consultores, Corinna Larsen.

Tres años después, los inversores españoles se quejaron a los gestores del fondo sobre los reiterados retrasos en las aportaciones por parte de sus socios saudíes. El consorcio español llevaba desembolsados más de 15 millones de dólares cuando decidió suscribir un acuerdo de cancelación: a cambio de liquidar completamente los compromisos contraídos, los empresarios defraudados renunciaban a las cantidades ya desembolsadas y, además, abonarían el 0,85 % de las aportaciones totales comprometidas. El montante total de las pérdidas ascendió a 21 millones de dólares. En esa cantidad se incluía la minuta de la empresaria germano-danesa, que ascendió a cerca de cinco millones de dólares.

En julio de 2018 los medios digitales "OkDiario" y "El Español" hicieron públicas las grabaciones de un encuentro que había tenido lugar tres años antes en Londres entre Corinna zu Sayn-Wittgenstein, el comisario José Manuel Villarejo y el expresidente de Telefónica Juan Villalonga, amigo común de ambos. En esos audios, Corinna acusa a Juan Carlos I de cobrar comisiones, que ascenderían a 100 millones de euros, por la adjudicación de las obras de construcción de la línea de tren de alta velocidad La Meca-Medina, así como de ocultar en Suiza su supuesta fortuna sirviéndose de testaferros y de sociedades pantalla.

Durante la conversación, la empresaria germano-danesa afirma que el rey la había utilizado para comprar numerosas propiedades (en Marruecos y en otros países), incluso a espaldas de ella, y que no lo hacía por generosidad, sino porque Corinna tenía su domicilio fiscal en Montecarlo. Una vez terminada la relación sentimental, el rey le habría exigido traspasar los bienes a su primo Álvaro de Orleans y Borbón, residente también en Mónaco. En los audios parece traslucirse el temor de la empresaria de que si efectuaba el traspaso se vería implicada en un delito de lavado de dinero.

El 3 de marzo de 2020 el diario ginebrino "Tribune de Genève" desveló que, a raíz de la difusión de las grabaciones del comisario Villarejo a Corinna Larsen en Londres en 2015 (caso de las "cintas de Corinna"), el fiscal jefe ("premier procureur") del cantón de Ginebra, Yves Bertossa, estaba llevando a cabo una investigación penal secreta por un posible delito de blanqueo de capitales agravado: la causa judicial P14783/2018, también conocida por la prensa transalpina como «los papeles secretos de Ginebra». Se trataba de averiguar una supuesta donación de 100 millones de dólares del entonces rey de Arabia Saudí, Abdalá Bin Abdelaziz, que había sido ingresada el 8 de agosto de 2008 en una cuenta de una sucursal bancaria panameña del banco privado ginebrino Mirabaud. La cuenta había sido abierta el día anterior por el gestor de fondos helvético Arturo Fasana a nombre de una entidad instrumental llamada Fundación Lucum, radicada también en Panamá. El beneficiario era el entonces rey de España, Juan Carlos I.

El fiscal suizo sospechaba que había una relación entre el «obsequio» del monarca saudí y la adjudicación de las obras del «AVE» a La Meca. Una de las hipótesis que se barajaban era que el importe fuera un porcentaje que el rey saudí hubiera «reservado» al monarca y la comisionista Larsen a cambio de convencer al consorcio de que redujera sustancialmente su propuesta. Como este finalmente accedió a la rebaja, la pareja recibió su comisión por parte de los saudíes. En palabras del periodista de "El País" José María Irujo en el documental «El virus de la corona»,

Para María Peral, de "El Español", se trataría de una sobrecomisión pagada por los adjudicatarios. Aunque la operación no se formalizó hasta octubre de 2011, la licitación, acordada en 6736 millones de euros, se produjo en 2006. 

Las investigaciones de Bertossa se encaminaron en primer lugar hacia los presuntos intermediarios utilizados para la gestión y disponibilidad del dinero. Además de la propia Corinna, en esas maniobras estaban implicados:



El caso puso en alerta a diversas instancias políticas y judiciales en España:

Según informaciones del diario "El Confidencial" del día 20 de julio de 2020, Juan Carlos I, para celebrar su 78 cumpleaños, disfrutó en enero de 2016 de un viaje a la Polinesia Francesa cuyos gastos del desplazamiento habían sido sufragados con dinero de dudosa procedencia. Dichos gastos, correspondientes a cinco billetes de avión de ida y vuelta para Juan Carlos y sus cuatro escoltas a la capital, Papeete, ascendieron a unos 32 900 euros.

En noviembre del año anterior, el entorno del rey emérito, a través del abogado Dante Canonica, se puso en contacto con la fundación Zagatka para que esta sociedad se hiciera cargo del desembolso. Credit Suisse, banco que gestionaba la cuenta asociada, pagó la cantidad estipulada a la sociedad "offshore" Fathomless Advisory Services Limited y, de esta, a otra llamada Cadenza Evening Limited. Esta última estaba administrada por Philip Adkins, primer marido de Corinna y amigo del rey, que fue quien un mes antes había comprado los billetes de avión.

De confirmarse el origen ilícito del capital de Zagatka, el rey emérito, que por aquellas fechas ya había perdido la condición de inviolable, podría enfrentarse a acusaciones de blanqueo de capitales.

En septiembre de 2012, el diario neoyorkino "The New York Times" publicó un artículo titulado «Un escarmentado rey busca la redención para España y su Monarquía». En el texto, difundido cinco días después de que el monarca visitara al periódico para explicar la situación española y mejorar la imagen del país, se indicaba, entre otros datos, que «la fortuna de la Familia Real española ha sido estimada en hasta 2.300 millones de dólares [casi 1.800 millones de euros]». Fuentes del diario neoyorquino indicaron posteriormente que el cálculo no había sido producto de una investigación propia, sino que se basaba en un promedio de cifras ya publicadas.

Las únicas publicaciones que, hasta esa fecha, habían incluido una cifra para la fortuna del rey de España, habían sido las revistas "Eurobusiness" (2000 y 2002) y "Forbes" (2003). Precisamente, esta última justificó la inclusión del monarca español en sus listas de 2003 por el dato que un año antes había publicado "Eurobusiness". "Eurobusiness" fue la primera en hablar de 1.790 millones de euros en la lista que publicó en 2002 con las 400 personas más ricas de Europa. Aunque en el suplemento anterior, publicado en el año 2000, el rey ya había aparecido con una fortuna estimada en unos 1.681 millones, el dato pasó desapercibido y las reacciones no llegaron hasta que se publicó el número del año 2002, donde se afirmaba:

En aquella ocasión el Gobierno y la Casa del Rey sí tuvieron conocimiento de la información y reaccionaron desmintiéndola. El embajador español en Reino Unido, país en el que se editaba la revista, envió una carta al director de la misma en la que le transmitía «el estupor de la Casa de Su Majestad el Rey de España» y calificaba de «disparatada» la estimación de "Eurobusiness", a lo que añadía la posible explicación al «erróneo» cálculo de la revista:

Sobre la cuestión de si los bienes inmuebles de Patrimonio Nacional fueron incluidos en la estimación de la fortuna, el artículo de "The New York Times" sentencia: «una suma [los 2.300 millones de dólares] que sus defensores afirman que fue inflada por la inclusión de propiedades del gobierno».

La prensa generalista española que analizó la información sobre la supuesta fortuna, alineó sus tesis con el dictamen del Gobierno, calificando el dato de «exorbitante» e «inverosímil», de «cálculo incorrecto», «cifra equivocada» e «inflada» o de «chocante». Sin embargo, en España, otras voces, como el economista y catedrático de la UPM, además de antiguo consejero delegado de Campsa, Roberto Centeno, que se presenta a sí mismo como asesor de la campaña de Donald Trump en España, dio por válida la cifra del "New York Times", y acusó al monarca y su antiguo administrador, Manuel Prado y Colón de Carvajal, de cobrar comisiones por el petróleo importado por el Estado procedente de países de Oriente Medio —de 1 a 2 dólares por barril, es decir, unos dos millones de euros por buque— desde finales de la década de 1970. En 2015 se filtró una conversación, grabada por el Centro Nacional de Inteligencia, donde el empresario Javier de la Rosa afirmaba que el bróker Arturo Fasana, implicado en varias tramas de corrupción y lavado de dinero, «guardó» en algún momento 300 millones a Juan Carlos I.

Con todo, uno de los más analistas más críticos respecto a las actividades de Juan Carlos es el ingeniero, economista y escritor español Roberto Centeno. Colaborador del diario "Alerta Digital", su trayectoria profesional había discurrido en diversas empresas públicas relacionadas con el sector energético. Centeno sostiene que, tras hacerse él responsable, en el ejercicio de sus responsabilidades, de la contratación de un cargamento de petróleo kuwaití, el entonces Ministro de Hacienda, Francisco Fernández Ordóñez, le hizo llegar una llamada de atención para que no volviera a formalizar contrato de suministro de petróleo alguno en Oriente Próximo porque —le advirtió— ese terreno estaba «reservado» para Manuel Prado y Colón de Carvajal. Según el economista,​ Fernández Ordóñez llegó a decirle: «Mira, ha estado aquí Manolo Prado, que se ha enterado de que estabas en Kuwait y me ha montado un pollo que no puedes imaginar; me ha dicho que Arabia Saudí y los Emiratos son exclusivamente suyos y nadie más que él puede negociar ni un barril, así que ni se te ocurra volver a hacer nada parecido». Y termina asegurando que Juan Carlos I, a través de su representante y administrador privado, Manuel de Prado, «tenía el monopolio de nuestros suministros extra durante la crisis del petróleo», y que «Hacienda pagaba por el petróleo lo que ponía en la factura, sin entrar en averiguación alguna y menos cometer la ordinariez de decir que se podía comprar más barato cuando el conseguidor era Prado».

El 14 de mayo de 1962 se casó en Atenas con la princesa Sofía de Grecia y Dinamarca, con la que tuvo tres hijos:




El rey Juan Carlos participó como regatista en los compitiendo en la clase Dragon con su embarcación "Fortuna". Sus dos tripulantes fueron Félix Gancedo y Gonzalo Fernández de Córdoba. Posteriormente formó parte del equipo Bribón. Tras varias décadas de alta competición, incluyendo un breve retiro entre 2011 y 2016, Juan Carlos I se proclamó campeón del mundo de vela en 2017, a los 79 años, en la categoría de embarcaciones clásicas de 6 metros en el Mundial de Vancouver (Canadá).

Otra de sus grandes pasiones ha sido el esquí, que le ha costado algunos percances.

Además de la vela y el esquí, su tercera gran afición ha sido la caza, aunque esta ha suscitado frecuentes polémicas: así, además de la desatada a raíz de su viaje a Botsuana en 2012, el 8 de octubre de 2004 participó en una cacería de osos en Rumanía; en 2004 pagó 7000 euros por matar en Polonia uno de los últimos bisontes vivos de Europa; y en 2006, distintos medios de Rusia lo acusaron de haber matado a un oso drogado, lo que llevó a la apertura de una investigación por parte de las autoridades rusas. La Casa del Rey calíficó de «ridículas» tales informaciones. A raíz de esas polémicas, el 21 de julio de 2012 la sección española del Fondo Mundial para la Naturaleza (WWF) decidió retirarle el cargo de Presidente de Honor, cargo que había ostentado el rey desde la fundación de la ONG.

Durante un tiempo fue radioaficionado.

El excoronel del Ejército español, diplomado de Estado Mayor, historiador militar y escritor Amadeo Martínez Inglés atribuía en 2008 a la monarquía juancarlista dos debilidades principales: por un lado, «el rápido (por no decir "meteórico"), incomprensible, y presuntamente delictivo, enriquecimiento de la Casa Real española»; y, por otro, «la escandalosa vida sentimental de su titular, el rey Juan Carlos I, que se ha traducido a lo largo de los años en multitud de turbias relaciones extramatrimoniales». También el íntimo amigo del monarca Manuel ("sic") Bouza, para justificar esta actitud de don Juan Carlos, comentó que un rey «está mucho más expuesto que cualquiera de nosotros a asedios y propuestas» y, además, «lo tenía muy fácil: la corona impresiona con su brillo». 

A la largo de todo su reinado, Juan Carlos ha mantenido diversas relaciones sentimentales fuera del matrimonio. A continuación se citan las más conocidas.  

En 1994 el rey trata de romper la relación, y entonces ella decide chantajearle con vídeos de sus encuentros íntimos que había grabado con ayuda de un familiar. La extorsión continuó hasta mayo de 1996, en que el presidente José María Aznar ordenó la paralización inmediata de los pagos. Bárbara Rey ofreció un acuerdo definitivo: cobrar de una sola vez un «generoso finiquito» y olvidarse para siempre del asunto. El Gobierno temía, más que el escándalo sexual, la revelación de secretos de Estado que el rey habría compartido con ella en la intimidad, como su papel en el 23-F. (Al parecer, según la periodista Rebeca Quintáns, una de las cintas había registrado una llamada telefónica en la que el monarca le habría advertido: «¡Oye, el lunes, 23, procura no salir de casa!, porque puede pasar algo…»). Ante el temor de que el escándalo forzara el fin de la legislatura y su recién estrenada mayoría absoluta, Aznar accedió a las pretensiones de la actriz. Según antiguos altos mandos del Cesid, los pagos se abonaban a través de una cuenta del Kredietbank de Luxemburgo y de otras cuentas opacas: «En Suiza habíamos abierto cuentas secretas a nombre de identidades falsas y de sociedades interpuestas. Tanto el dinero como las joyas y los regalos se pagaban en su mayoría desde la propia Presidencia del Gobierno con cargo a los fondos reservados». 

Mes y medio después, Manuel Prado y Colón de Carvajal, administrador de Zarzuela, fue encarcelado por delitos económicos y Corinna pasó a asumir algunas de sus funciones. «En seguida [Juan Carlos] se puso a encargarle cosas y, en poco tiempo, se había convertido en su consultora, ayudante personal, relaciones públicas de altura, mediadora… Como Prado, Corinna zu Sayn-Wittgenstein disfrutaba de pasaporte diplomático». El rey se hacía acompañar por su amante en sus encuentros con la alta sociedad española e, incluso, formando parte de su séquito en los viajes de Estado. En el año 2004 ayudó a Iñaki Urdangarín en el Valencia Summit, auspiciado por el instituto Nóos, y organizó la luna de miel del príncipe Felipe con Letizia Ortiz por varios países del mundo. Asimismo, participó en operaciones comerciales para grandes empresas españolas, como la OHL de los grandes amigos del rey, Juan Miguel Villar Mir, y del príncipe Felipe, Javier López Madrid. Cobraba comisiones de en torno al 3 por 100.

En 2010, a Juan Carlos se le detectó un nódulo en un pulmón y el monarca pensó que iba a morir de cáncer. Aunque el amor había decaído, la alemana continuó a su lado. Afortunadamente, el tumor era benigno. Luego la relación sentimental y de negocios repuntó. La reina Sofia ya residía en Londres cuando Corinna se instaló en La Angorrilla, una casa de campo dentro del complejo de La Zarzuela. La pareja también pasaba largas temporadas en un dúplex de lujo que había adquirido en Suiza. En 2012 sucedió el accidente de Botsuana.

La pareja puso punto final a su relación en noviembre del año 2014.

Juan Carlos I ha sufrido a lo largo de su vida diversos de problemas de salud y percances físicos que en numerosas ocasiones le han obligado a entrar en el quirófano: 


A lo largo de los años, la figura de Juan Carlos I se ha convertido en personaje de películas y telefilmes en España, pudiendo mencionarse los siguientes:



 


</doc>
<doc id="4694" url="https://es.wikipedia.org/wiki?curid=4694" title="29 de enero">
29 de enero

El 29 de enero es el 29.º (vigesimonoveno) día del año en el calendario gregoriano. Quedan 336 días para finalizar el año y 337 en los años bisiestos.












</doc>
<doc id="4696" url="https://es.wikipedia.org/wiki?curid=4696" title="Azúcar">
Azúcar

Se denomina azúcar (en árabe: السكر, al-sukar , persa:شکر ,šakar, sánscrito: शर्करा , śárkarā «arenilla») en el uso más extendido de la palabra, a la sacarosa, cuya fórmula química es CHO, también llamada «azúcar común» o «azúcar de mesa».

La sacarosa es un disacárido formado por una molécula de glucosa y una de fructosa, que se obtiene principalmente de la caña de azúcar o de la remolacha. El 27 % de la producción total mundial se realiza a partir de la remolacha y el 73 % a partir de la caña de azúcar.

La sacarosa se encuentra en todas las plantas, y en cantidades apreciables en otras plantas distintas de la caña de azúcar o la remolacha, como el sorgo y el arce azucarero.

En ámbitos industriales se usa la palabra azúcar o azúcares para designar los diferentes monosacáridos y disacáridos, que generalmente tienen sabor dulce, aunque por extensión se refiere a todos los hidratos de carbono.

Funde a los 160 °C y calentada a 210 °C se transforma en una masa de color pardo denominada "caramelo", utilizada en la elaboración de dulces y pasteles, así como para la saborización y coloración de líquidos.

Si se calienta por encima de 145 °C en presencia de compuestos amino (NH), derivados por ejemplo de proteínas, tiene lugar el complejo sistema de reacciones de Maillard, que genera colores, olores y sabores generalmente apetecibles, y también pequeñas cantidades de compuestos indeseables.

El azúcar es una importante fuente de calorías en la dieta alimenticia moderna, pero es frecuentemente asociada a calorías vacías, debido a la completa ausencia de vitaminas,minerales y sales.

En alimentos industrializados el porcentaje de azúcar puede llegar al 80 %. La Organización Mundial de la Salud recomienda que el azúcar no supere el 10 % de las calorías diarias consumidas.

El azúcar se ha producido en el subcontinente indio desde la antigüedad. No era abundante o barata en los primeros tiempos y la miel se utilizaba con más frecuencia para endulzar en casi todo el mundo. Originalmente, la gente masticaba la caña de azúcar en bruto para extraer su dulzura. La caña de azúcar era una especie nativa de los trópicos, en Asia meridional y en el sudeste asiático. Las diferentes especies de caña parecen tener su origen en diferentes lugares, siendo "Saccharum barberi" originaria de la India y "S. edule" y "S. officinarum" provenientes de Nueva Guinea. Una de las referencias históricas más tempranas a la caña de azúcar está en manuscritos chinos del siglo VIII a. C. que afirman que el uso de la caña de azúcar se originó en la India.

El azúcar no tuvo apenas importancia hasta que los indios descubrieron métodos para convertir el jugo de la caña de azúcar en cristales granulados que eran más fáciles de almacenar y transportar. Fueron descubiertos cristales de azúcar de la época de la Gupta Imperial, alrededor del siglo V d. C. En la lengua indígena local, estos cristales se llaman "khanda" (खण्ड, khaṇḍa).

Los marineros indios, quienes llevaban mantequilla y azúcar como suministros, introdujeron el conocimiento del azúcar en las diversas rutas comerciales por las que viajaban. Los monjes budistas, en sus viajes, llevaron los métodos de cristalización de azúcar a China. Durante el reinado de Harsha (606 a 647) en el norte de la India, los enviados de la India en la China Tang enseñaron métodos de cultivo de la caña de azúcar después del reinado de Li Shimin (que reinó de 626-649), que manifestó su interés por el azúcar. Posteriormente, China estableció sus primeras plantaciones de caña de azúcar en el siglo VII. Los documentos chinos confirman, al menos, dos expediciones a la India, iniciadas en el 647, para obtener la tecnología para el refinado del azúcar. En el sur de Asia, en Oriente Medio y en China, el azúcar se convirtió en un elemento básico de la cocina y de los postres.

Las conquistas de Alejandro Magno se detuvieron a orillas del río Indo por la negativa de sus tropas para ir más al este. Allí vieron a las personas en el subcontinente indio cultivando la caña y fabricando un dulce granulado, localmente llamado "sharkara" (शर्करा, "sarkara"), pronunciado como "saccharum" (ζάκχαρι). En su viaje de regreso, los soldados macedonios se llevaron "cañas de miel" con ellos. La caña de azúcar se mantuvo como un cultivo poco conocido en Europa durante más de un milenio. El azúcar era un bien escaso y los comerciantes de azúcar eran ricos.

Los cruzados trajeron con ellos el azúcar a Europa después de sus campañas en Tierra Santa, donde se encontraron con caravanas que transportaban esa "sal dulce". A principios del siglo XII, Venecia adquirió algunas aldeas cerca de Tiro y estableció fincas para producir azúcar para exportar a Europa, donde se complementaba con la miel, que anteriormente había sido el único edulcorante disponible. El cronista de las cruzadas Guillermo de Tiro, en un escrito de finales del siglo XII, describió el azúcar como un producto "muy necesario para el uso y la salud de la humanidad". En el siglo XV, Venecia era el principal centro de refinación y distribución de azúcar de Europa.

En agosto de 1492, Cristóbal Colón se detuvo en La Gomera, en las Islas Canarias para cargar vino y agua, con la intención de permanecer solo cuatro días. No obstante, tuvo una relación sentimental con Beatriz de Bobadilla y se quedó un mes. Cuando finalmente iba a partir, ella le dio unas cañas de azúcar, que fueron las primeras en llegar a América.

Los portugueses llevaron el azúcar a Brasil. En torno a 1540 había 800 fábricas de azúcar de caña en la isla de Santa Catarina y había otras 2000 en la costa norte de Brasil, en Demarara y en Surinam. La primera zafra tuvo lugar en la isla de La Española en 1501; y se construyeron muchos ingenios azucareros (fábricas) en Cuba y Jamaica en la década de 1520.

El azúcar fue un lujo en Europa hasta el siglo XVIII, en que se hizo más asequible. Luego se popularizó y en el siglo XIX el azúcar llegó a ser considerado una necesidad. Esta evolución del gusto y de la demanda de azúcar como ingrediente de alimentos esenciales desató grandes cambios económicos. Durante los siglos XVIII y XIX muchos europeos prosperaron con la industria azucarera en las Antillas y otros lugares de América. La demanda de mano de obra barata para realizar el duro trabajo necesario para su cultivo y procesamiento aumentó la demanda de la trata de esclavos del África subsahariana. También hubo una gran demanda de trabajadores semi-esclavos contratados en Asia. La mezcla étnica moderna de muchas regiones ha sido influenciada por la demanda de azúcar.

El azúcar también llevó a algunos industrialización de las antiguas colonias. Por ejemplo, el teniente J. Paterson, del establecimiento de Bengala, convenció al gobierno británico que la caña de azúcar podría ser cultivada en la India británica con muchas ventajas y a menor coste que en las Indias Occidentales. Como resultado, las fábricas de azúcar se establecieron en Bihar, al este de la India.

Durante las guerras napoleónicas, la producción de remolacha azucarera aumentó en la Europa continental debido a la dificultad de importar azúcar cuando el envío fue objeto de bloqueo. En 1880, la remolacha azucarera fue la principal fuente de azúcar en Europa. Esta se cultivaba en Lincolnshire y otras partes de Inglaterra, aunque el Reino Unido siguió importando la parte principal de su azúcar de sus colonias.

Hasta finales del siglo XIX, el azúcar fue comprado en panes de azúcar (en inglés, "sugarloafs", que significa hogazas de azúcar), que tenían que ser cortados. En años posteriores, el azúcar se vendió habitualmente granulado y en bolsas.

Los terrones de azúcar se produjeron en el siglo XIX. El primer inventor de un proceso para disponer el azúcar en forma de cubo fue el moravo Jakub Kryštof Rad, director de una empresa azucarera en Dačice. Comenzó la producción de terrones de azúcar después de haber adquirido una patente de cinco años, el 23 de enero de 1843. Henry Tate, de Tate & Lyle, fue otro de los primeros fabricantes de terrones de azúcar en sus refinerías en Liverpool y Londres. Tate adquirió una patente para la fabricación de terrones de azúcar del alemán Eugen Langen, quien en 1872 había inventado un método diferente de procesamiento de terrones de azúcar.

El azúcar es un endulzante de origen natural, sólido, cristalizado, constituido esencialmente por cristales sueltos de sacarosa, obtenidos a partir de la caña de azúcar ("saccharum officinarum L") o de la remolacha azucarera ("beta vulgaris L") mediante procedimientos industriales apropiados. Un grano de azúcar es entre 30 y 70 % menor que el grano de arroz.

El azúcar blanco se somete a un proceso de purificación química —llamado sulfitación— haciendo pasar a través del jugo de caña el gas SO obtenido por combustión de azufre.

La película de miel que rodea el cristal de azúcar moreno o rubio contiene sustancias como minerales y vitaminas. En el argot azucarero, a estas sustancias se les llama impurezas. Cabe aclarar que, durante el proceso de refinación, a todas las sustancias que no son sacarosa se consideran impurezas, pero son inofensivas para la salud. Y son estas las que le otorgan el color y sabor particular.

Cada día es mucho más frecuente en platos y dulces preparados encontrarse otros azúcares diferentes; glucosa, fructosa —básicamente de la planta de maíz, preferida por su asimilación más lenta— o combinados con edulcorantes artificiales.

La palabra azúcar viene del sánscrito "sharkara", que los persas transformaron en "sakar". Los griegos tomarían el término persa y lo llamarían "sakjar". El árabe clásico tomó el término griego y lo llamó "sukkar", y posteriormente el árabe hispano lo llamó "assúkar". El sánscrito tomó la palabra "sharkara" de "çarkara", que significa arenilla, ya que llamaban así al polvo blanquecino de la caña de azúcar.

Según la Real Academia Española, el azúcar tiene género ambiguo, pero cuando va sin especificativo es mayoritario su empleo en masculino.

A pesar de que no empieza con una letra "a" tónica, su artículo siempre se utiliza masculino.

El azúcar se puede clasificar por su origen (de caña de azúcar o remolacha), pero también por su grado de refinación o sus características. Normalmente, la refinación se expresa visualmente a través del color (azúcar moreno, azúcar rubio, blanco), que está dado principalmente por el porcentaje de sacarosa que contienen los cristales.

Los tipos de azúcar que se comercializan habitualmente son los siguientes:


El procesamiento del azúcar se puede dividir en las siguientes etapas:

En el mercado del azúcar se distinguen dos tipos de productos, el azúcar cruda y el azúcar refinada o blanca. Dentro de cada tipo existen diferentes categorías según sus diferentes calidades. El azúcar cruda se produce solamente de caña de azúcar, en tanto el azúcar refinada se produce tanto de caña de azúcar como de remolacha azucarera. En este sentido, se considera que la industria de la caña de azúcar tiene una mayor flexibilidad para responder a los cambios de precios relativos entre azúcar cruda y azúcar refinada.

El mercado mundial del azúcar es uno de los más distorsionados del mundo como resultado de un amplio conjunto de políticas de protección y de subsidio a la producción y exportaciones por parte de los principales países productores y consumidores del mundo. A nivel general, se pueden distinguir, básicamente, dos tipos de mercados de azúcar: el mercado protegido y el mercado libre.

El mercado protegido consiste en acuerdos preferenciales y contratos de largo plazo que incluyen el sistema de cuotas de los Estados Unidos, las cuotas de la Unión Europea, las exportaciones de Cuba a China y las exportaciones de Australia a Canadá.

En el mercado libre se transan los volúmenes no cubiertos por convenios especiales. Estas transacciones se realizan preferentemente en las diferentes bolsas azucareras, entre las cuales se encuentran las de Nueva York, Londres, París y Hong Kong. Además de transacciones spot, en el mercado libre de azúcar se utilizan instrumentos tales como forward, futuros y derivados.

Los principales productores de azúcar son:

En 2003, 16 países concentraban el 87,1 % de la producción mundial.

El consumo mundial se presenta del siguiente modo:

El alto consumo de azúcar demostró que aumenta significativamente la tensión sistólica y la presión arterial diastólica; las personas que consumen el 25% o más de calorías de azúcar tienen casi tres veces mayor riesgo de muerte por enfermedad cardiovascular.

Un metaanálisis reveló que el consumo de azúcar no mejora el estado de ánimo, pero que puede reducir el estado de alerta y aumentar la fatiga. Algunos estudios reportan relación causa efecto entre un alto consumo de azúcar refinado e hiperactividad , otros estudios sugieren que la cantidad de azúcar en la dieta no influye en el comportamiento infantil, sino que los padres que tienen prejuicios hacia los efectos de los dulces perciben erróneamente que sus hijos están más inquietos y nerviosos cuando comen golosinas.

Los alimentos ricos en azúcar suelen contener menor cantidad de vitaminas y minerales y pueden estar reemplazando a alimentos más nutritivos. Asimismo, contienen un exceso de calorías, lo cual puede ocasionar obesidad.. Esto es lo que se conoce como el argumento de la caloría vacía.

Diversos estudios de investigación indican que las células cancerosas consumen más azúcar (glucosa) que las células normales. No obstante, ningún estudio ha demostrado que consumir azúcar empeore el cáncer ni que eliminar su consumo lo haga disminuir o desaparecer y diferentes estudios evidencian que no existe asociación entre el consumo de azúcar y el cáncer. Solo existe evidencia posible de una relación entre la ingesta de monosacáridos (fructosa y glucosa) y el riesgo de desarrollar cáncer de páncreas, y entre el índice glucémico (IG) y el cáncer colorrectal.

No obstante, algunos autores señalan que una alimentación con un alto contenido de azúcar puede ocasionar un excesivo aumento de peso, y la obesidad está asociada a un riesgo elevado de padecer diversos tipos de cáncer. Otros autores señalan que la evidencia sobre la asociación entre la ingesta de azúcar añadido y el riesgo de desarrollar cáncer en adultos o en niños es insuficiente.

Actualmente, se conoce que las dietas ricas en azúcar pueden provocar un aumento excesivo de peso y resistencia a la insulina, lo cual predispone a padecer diabetes mellitus tipo 2 (DMT2). Esta enfermedad ha experimentado un drástico aumento de incidencia en las últimas décadas, principalmente debido a factores del estilo de vida occidental, como la falta de ejercicio y las dietas altas en calorías.

Se ha demostrado consistentemente que la DMT2 es un factor de riesgo para la enfermedad de Alzheimer. Por lo tanto, los cambios en la dieta pueden reducir significativamente el riesgo de desarrollar DMT2 y enfermedad de Alzheimer, y con ello aumentar la calidad de vida y mejorar la longevidad.

Una dieta alta en azúcar es la principal causa de la aparición de caries dental.. Un informe de la OMS afirmó que "el azúcar es indudablemente el factor dietético más importante en el desarrollo de caries" Una revisión de estudios en humanos mostró que la incidencia de caries es menor cuando la ingesta de azúcar es inferior al 10% de la energía total consumida.




</doc>
<doc id="4698" url="https://es.wikipedia.org/wiki?curid=4698" title="Glucosa">
Glucosa

La glucosa es un monosacárido con fórmula molecular CHO. Es una hexosa, es decir, contiene 6 átomos de carbono, y es una aldosa, esto es, el grupo carbonilo está en el extremo de la molécula (es un grupo aldehído). Es una forma de azúcar que se encuentra libre en las frutas y en la miel. Su rendimiento energético es de 3,75 Kcal/g en condiciones estándar. Es un isómero de la galactosa, con diferente posición relativa de los grupos -OH y =O. 

La aldohexosa glucosa posee dos enantiómeros, si bien la D-glucosa es predominante en la naturaleza. En terminología de la industria alimentaria suele denominarse dextrosa (término procedente de «glucosa dextrorrotatoria») a este compuesto.

El término «glucosa» procede del idioma griego "γλεῦκος" (gleûkos; "mosto", "vino dulce"), y el sufijo «-osa» indica que se trata de un azúcar. La palabra fue acuñada en francés como "glucose" (con anomalía fonética) por Jean-Baptiste Dumas en 1838; debería ser fonéticamente "gleucosa" (o "glicosa" si partimos de glykos, otro lexema de la misma raíz).

La glucosa, libre o combinada, es el compuesto orgánico más abundante de la naturaleza. Es la fuente primaria de síntesis de energía de las células, mediante su oxidación catabólica, y es el componente principal de polímeros de importancia estructural como la celulosa y de polímeros de almacenamiento energético como el almidón y el glucógeno.

A partir de su estructura lineal, la D-glucosa sufre una ciclación hacia su forma hemiacetálica para dar sus formas furano y pirano (D-glucofuranosa y D-glucopiranosa) que a su vez presentan anómeros alfa y beta. Estos anómeros no presentan diferencias de composición estructural, pero sí diferentes características físicas y químicas.

La glucosa es uno de los tres monosacáridos dietéticos, junto con fructosa y galactosa, que se absorben directamente al torrente sanguíneo durante la digestión. Las células lo utilizan como fuente primaria de energía y es un intermediario metabólico. La glucosa es uno de los principales productos de la fotosíntesis y combustible para la respiración celular.

Todas las frutas naturales tienen cierta cantidad de glucosa (a menudo con fructosa), que puede extraerse y concentrarse para preparar un azúcar alternativo. Sin embargo, a escala industrial tanto el jarabe de glucosa (disolución de glucosa) como la dextrosa (glucosa en polvo) se obtienen a partir de la hidrólisis enzimática de almidón de cereales (generalmente trigo o maíz).

Los organismos fotoautótrofos, como las plantas, sintetizan la glucosa en la fotosíntesis a partir de compuestos inorgánicos como agua y dióxido de carbono, según la reacción:

Los seres heterótrofos, como los animales, son incapaces de realizar este proceso y toman la glucosa de otros seres vivos o la sintetizan a partir de otros compuestos orgánicos. Puede obtenerse glucosa a partir de otros azúcares, como fructosa o galactosa. Otra posibilidad es la síntesis de glucosa a partir de moléculas no glucídicas, proceso conocido como gluconeogénesis. Hay diversas moléculas precursoras, como el lactato, el oxalacetato y el glicerol.

También existen ciertas bacterias anaerobias que utilizan la glucosa para generar dióxido de carbono y metano según esta reacción:

La glucosa es el constituyente básico de diversos polímeros de gran importancia biológica, como son los polisacáridos de reserva almidón y glucógeno, y los estructurales celulosa y quitina.

Celulosa. En su forma cíclica D-glucopiranosa, dos moléculas de glucosa se unen mediante un enlace β-glucosídico en el que reaccionan los -OH de sus carbonos 1 y 4, respectivamente, para formar el disacárido celobiosa; la unión de varias de estas moléculas forma celulosa, constituyente esencial de la pared celular de las células vegetales.

Quitina. Un derivado nitrogenado de la glucosa, la N-acetilglucosamina, también en su forma cíclica β-D-glucopiranosa, forma el disacárido quitocina, cuya repetición da lugar a la quitina, el componente del exoesqueleto de los artrópodos, el grupo animal con mayor éxito evolutivo.

Glucógeno y almidón. La unión de dos moléculas de D-glucopiranosa mediante enlace α-glucosídico da lugar a la maltosa y a la isomaltosa, disacáridos que son la base de los polisacáridos glucógeno (reserva energética propia de animales y hongos) y almidón (reserva típica de los vegetales y muchas algas).

En repostería se utiliza un derivado de la sacarosa, producido mediante hidrólisis ácida o enzimática, que se llama azúcar invertido, compuesto a partes iguales de fructosa y glucosa. Añadido a la mezcla o formado durante el proceso, se usa en la elaboración de bollería, caramelos y otros productos de confitería.

La mezcla cristaliza con más dificultad que la sacarosa, evita la desecación de los productos congelados y hace descender el punto de congelación de helados.




</doc>
<doc id="4699" url="https://es.wikipedia.org/wiki?curid=4699" title="Murcia">
Murcia

Murcia es una ciudad española, capital del municipio del mismo nombre y de la comunidad autónoma de la Región de Murcia. Es el centro de la comarca de la Huerta de Murcia y de su área metropolitana. Está situada en el sureste de la península ibérica a orillas del río Segura, en la denominada depresión prelitoral murciana, a 40 kilómetros del mar Mediterráneo. Con 453 258 habitantes (INE, 2019), Murcia es el .

El área urbana de la ciudad (o zona metropolitana), aunque no establecida oficialmente, comprendería a unos diez municipios de la Región de Murcia, contando con una población de 650 468 habitantes en 2018, repartidos en una superficie total de 1230,92 km, con una densidad de población de 528 hab/km. De este modo, el área urbana de Murcia ocuparía el 10º puesto en la lista de .

Murcia es un importante municipio de servicios en la que el sector terciario ha sucedido a su antigua condición de exportadora agrícola por antonomasia, gracias a su célebre y fértil huerta, por la cual era conocida con el sobrenombre de "la Huerta de Europa". Entre sus industrias más destacadas se encuentran la alimentaria, la textil, la química, la de destilación y la fabricación de muebles y materiales de construcción, estando muchas de ellas ubicadas en el Polígono Industrial Oeste, considerado uno de los más grandes de la península (compartido con el municipio de Alcantarilla).

Es también un importante centro de gran tradición universitaria desde que fuera fundada la primera universidad en 1272. Actualmente es sede de dos universidades: la pública Universidad de Murcia y la privada Universidad Católica San Antonio, que atraen alrededor de 50 000 estudiantes al municipio.

De orígenes inciertos, hay constancia de que fue fundada en el año 825 por orden de Abderramán II, probablemente, sobre un asentamiento anterior de origen romano. Durante la Edad Media, Murcia llegó a ser capital de la cora de Tudmir, posteriormente fue cabeza de distintos reinos de taifas de creciente importancia en los siglos XI, XII y XIII y entre 1243-1266 se incorporó a la Corona de Castilla como capital del Reino de Murcia, siendo además ciudad con voto en cortes y sede episcopal desde 1291.

De su patrimonio histórico-artístico destacan su célebre Catedral, de fachada barroca e interior principalmente gótico, el afamado Casino, de suntuosos interiores; el denso patrimonio escultórico de Francisco Salzillo, y un gran conjunto de edificios barrocos. En el ámbito cultural es conocida por su rico folclore, especialmente vistoso durante las Fiestas de Primavera y las procesiones de Semana Santa, declaradas de . El Consejo de Hombres Buenos de la Huerta de Murcia, ejemplo de tribunal consuetudinario de regantes del Mediterráneo español, está declarado Patrimonio cultural inmaterial de la Humanidad por la Unesco.

El origen del topónimo "Murcia" no está del todo claro y tanto historiadores como lingüistas sostienen varias hipótesis agrupadas en torno a dos orígenes básicos: el árabe y el latino. Según palabras de Menéndez Pidal: "el topónimo Murcia era azote de filólogos".

El origen pre-islámico, probablemente latino, parece el más lógico, aunque no se sabe con seguridad cuál es la raíz primera, y son muchas la hipótesis que se aventuran. La más extendida actualmente ya la enunció Francisco Cascales en sus "Discursos históricos de la muy noble y muy leal ciudad de Murcia" publicados en 1621,
Aunque la evolución de la palabra que propone Cascales está descartada, lo cierto es que el topónimo "Murcia" era usado por los romanos, siendo como dice el autor el nombre de una divinidad primitiva que tenía un templo en el valle situado entre las colinas del Aventino y el Palatino en la misma ciudad de Roma, creyéndose que la denominación de dicha diosa está relacionado con el latín "myrtus", con el significado de "mirto", evolucionando a "Myrtea"/"Murtea"/"Murcia".

Por lo tanto, parece que los estudios históricos han llegado a la conclusión de que -al igual que la mencionada divinidad- "Murcia" es un topónimo de origen latino que deriva muy probablemente de Myrtea o Murtea (“lugar de mirtos” o “lugar donde crecen los mirtos”) o de Murtia, y que de esa forma "Mursiya" -en árabe: مدينة مُرْسية- (primera denominación documentada ya en época islámica) no fue más que la adaptación árabe del término latino preexistente.

El escudo de la ciudad de Murcia tiene orígenes medievales, con diversos añadidos posteriores. Está compuesto por 7 coronas sobre fondo rojo. Debajo de la corona central se halla un corazón en cuyo interior se disponen un león rampante y una flor de lis rodeados por una leyenda ("Priscas novissima exaltat et amor"). Completa el escudo una orla con castillos y leones.

El origen de este emblema está en el rey Alfonso X, quien concedió un sello concejil con 5 coronas como representación legal y simbólica de la ciudad y su reino (conmemorando el hecho de que el Reino de Murcia era el quinto en ser reconquistado por la corona castellana).

Más tarde, en el 1361, Pedro I firmó un privilegio por el que se concedía a Murcia la sexta corona para que figurara en el sello y en el pendón municipal, añadiendo además una orla con los símbolos de la Corona de Castilla (en agradecimiento al papel murciano en la Guerra de los Dos Pedros).

Posteriormente, en 1575, el Concejo solicitó a Felipe II la inclusión de un corazón para conmemorar que las entrañas y el corazón de Alfonso X descansan en la ciudad como quedó establecido en el testamento del rey sabio (y que se encuentran en la Capilla Mayor de la Catedral de Murcia).

El actual escudo se completaría en 1709 por Felipe V. El monarca premió la fidelidad murciana en la Guerra de Sucesión concediendo otra corona real sobre un león y una flor de lis unida bajo el texto: "Priscas novissima exaltat et amor" (ensalzar y amar lo antiguo y lo nuevo).

La bandera del municipio es roja en su totalidad, con el escudo arriba descrito dispuesto en el centro de la misma. Así pues, el rojo es el color de la ciudad (como se puede comprobar, por ejemplo, en la indumentaria del Real Murcia Club de Fútbol).

El "Himno de Murcia" es obra del poeta y periodista murciano Pedro Jara Carrillo, quien puso la letra a la música del maestro Emilio Ramírez. Se estrenó el 9 de junio de 1922. El poeta también compuso el "Himno a la Virgen de la Fuensanta" con motivo de su coronación en 1927.

Un popular himno no oficial del municipio es "El Canto a Murcia" de La Parranda, zarzuela de ambiente murciano compuesta por el maestro Francisco Alonso, con libreto de Luis Fernández Ardavín y estrenada en 1928. El "Canto a Murcia" está considerado uno de los finales de acto más impresionantes de la historia de la zarzuela, y se le atribuyen características de himno regional.

Otros símbolos de Murcia son la Matrona o el León del Malecón.

El término municipal tiene una extensión de 881,86 km² y se divide de norte a sur en dos partes diferentes separadas por una serie de sierras que conforman la llamada "Cordillera Sur": Sierra de Carrascoy (1065 metros), del Puerto (531 metros), Cresta del Gallo (609 metros), Villares (487 metros), Columbares (647 metros), Altaona (534 metros) y Escalona (345 metros). Estas dos zonas se denominan: "Campo de Murcia" al sur, que geográficamente forma parte del Campo de Cartagena, y Huerta de Murcia al norte de la sierra, constituida por la vega segureña. Entre estas dos áreas, atravesando la sierra, se encuentran los pasos naturales del puerto de La Cadena, el puerto del Garruchal y el puerto de San Pedro.

La Vega del Segura, donde se encuentra la conocida huerta, es un llano de inundación depositado sobre una fosa tectónica que constituye la "depresión prelitoral murciana", a 40Km en línea recta del Mar Mediterráneo. Las elevaciones montañosas que la encajonan en sus flancos norte y sur están compuestas de materiales geológicos pertenecientes al dominio Bético. En la vertiente norte aparece el denominado "reborde interior de la depresión prelitoral", formado por una sucesión de suaves colinas, constituidas por areniscas y margas, restos de la sedimentación miocénica que queda en forma de resalte como consecuencia del hundimiento de la depresión del Segura. Sus alturas son modestas y aisladas, sin llegar a superar los 200 msnm, con los cabezos de Javali Viejo, Guadalupe, El Puntal, Churra, Cabezo de Torres, Monteagudo, Esparragal y Cobatillas (que hacen de límite con el término de Molina de Segura), prolongándose en la Comunidad Valenciana a través de la Sierra de Orihuela. El zócalo sur de la depresión está formado por las sierras de la referida Cordillera Sur, constituidas por materiales calizos, dolomías, esquistos, filitas y cuarcitas.
Los aportes y arrastres de estas colinas y montañas junto con las avenidas del Segura y el Guadalentín fueron rellenando y colmatando la depresión hasta formar una llanura aluvial de débiles pendientes. La ciudad de Murcia se encuentra en la parte central de la vega, a una altitud para el centro de la urbe de 42 msnm, mientras que la altitud del municipio varía desde los 25 metros en el último tramo del río Segura en el municipio, hasta los 1031 metros en el Morro de la Fuente, en la Sierra de Carrascoy.

La referida zona sur del municipio, llamada Campo de Murcia, no es sino la cabecera o parte norte de la llanura litoral del Campo de Cartagena, extendiéndose de forma descendente desde la Sierra de Carrascoy hasta los límites municipales de Fuente Álamo de Murcia, Torre Pacheco y San Javier. Un caso especial es el de la pedanía de Lobosillo que se sitúa como un enclave del municipio de Murcia en el centro del Campo de Cartagena.

La parte más occidental de la zona septentrional del municipio, la formada por las pedanías de Sangonera La Seca, Sangonera la Verde, Barqueros y Cañada Hermosa, constituye realmente la parte final del valle del Guadalentín, justo antes de su conexión con la vega del Segura, formando un valle encajonado entre la Sierra de Carrascoy al sur y las estribaciones montañosas del reborde norte de la depresión, valle al que también se le denomina "campo de Sangonera".

El río Segura es el principal eje hidrográfico del municipio. Discurre por la vega del mismo nombre y atraviesa la ciudad de Murcia con dirección oeste-este, siendo un río de régimen pluvial mediterráneo, de escaso caudal pero con fuertes crecidas, como las de 1946, 1948, 1973, 1987, 1989 o 2019 que inundaron diversas zonas del municipio.

El Segura entra en la "depresión prelitoral" procedente de la Vega Media (de los municipios de Las Torres de Cotillas y Molina de Segura), a la altura de las pedanías de Javalí Nuevo y Javalí Viejo, justo en donde se sitúa la denominada Contraparada. En este primer tramo todavía lleva una dirección norte-sur, que cambiará por la descrita "oeste-este" a su paso por la pedanía de Puebla de Soto y el límite municipal con Alcantarilla. Tras atravesar la ciudad, entre las pedanías de Santa Cruz y Alquerías el río adquiere una dirección "suroeste-noreste", abandonando el municipio a la altura de El Raal introduciéndose en el de Beniel y en el término de Orihuela, dentro ya de la Vega Baja.

El río transcurre a partir de la Contraparada a través de una canalización realizada en los años 90 que modificó el cauce anterior recortando los clásicos meandros y aumentando la capacidad de desagüe de cara a controlar las periódicas riadas. A su paso por la ciudad, el Segura cuenta con una amplia canalización en piedra realizada en los años 50 del siglo XX, sustituta de la anterior del siglo XVIII.

El río Guadalentín, el principal afluente del Segura por su margen derecha (también llamado "Sangonera" en su tramo final), discurre a través del Canal del Reguerón por la zona sur de la vega proveniente del valle del Guadalentín -que no es sino la misma depresión prelitoral antes de que el Segura acceda a ella-, concretamente de la comarca del Bajo Guadalentín (de los municipios de Librilla y Alhama de Murcia). Este río desemboca artificialmente en el Segura a la altura de la pedanía de Beniaján gracias al mencionado Canal del Reguerón, que fue realizado en el siglo XVIII para evitar que las riadas del Guadalentín confluyeran con las del Segura aguas arriba de la ciudad de Murcia.

También hay que destacar la presencia de numerosas ramblas, situadas principalmente en los piedemontes de los dos rebordes montañosos de la depresión prelitoral, destacando las ramblas de Espinardo y Churra en la zona norte, o la rambla del Garruchal en la zona sur. En la zona del Campo de Murcia también son típicos estos cauces, pero vierten sus aguas ocasionales hacia el Campo de Cartagena y el Mar Menor, destacando la rambla de La Murta, la rambla de Corvera o la rambla del Ciprés.

El Segura y su afluente el Guadalentín son famosos por sus furiosas crecidas y temidas inundaciones, teniéndose registro de algunas ya en la baja Edad Media, por lo que su control ha sido desde tiempo inmemorial motivo de construcción de obras de defensa tales como cortas, motas, canales de derivación y encauzamiento en algunos tramos. La propia muralla musulmana de la ciudad se pensó como una forma de protección, al igual que elementos tan característicos como el Paseo del Malecón o el Canal del Reguerón. Pese a la construcción de embalses en la cabecera, los desbordamientos continuaron afectando a la ciudad de Murcia y su huerta durante el siglo XX, por lo que se tuvo que ejecutar un definitivo plan integral contra las avenidas desarrollado entre 1987 y 1994.

Las crecidas del Segura están documentadas desde la baja Edad Media, siendo una de las primeras la de octubre de 1328, destacando la frecuencia de las mismas, con 17 episodios de importancia durante el siglo XV. El episodio más importante de ese siglo fue el de septiembre de 1452, lo que llevó a desarrollar mejoras en el cauce y varios proyectos de encauzamiento en la capital murciana.

En 1545 el desbordamiento del Segura inundó Murcia y su huerta siendo la más importante crecida hasta la fecha. En 1651 la "Riada de San Calixto" causó 1500 muertos en Murcia con un caudal de 1700 m³/s. En 1802 el Guadalentín rompió el Pantano de Puentes lo que provocó una riada que destruyó completamente la pedanía murciana de Buznegra. En 1879 la célebre "Riada de Santa Teresa" superó los 1800 m³/s a su paso por el Puente de los Peligros, marcando los registros históricos más altos de la historia y causando más de 1000 muertos y numerosos destrozos.

En el siglo XX las riadas de 1946, 1948, 1973, 1982, 1987 y 1989 han pasado a la historia superándose en muchas de ellas los 1000 m³/s de caudal máximo instantáneo. Gracias a las obras desarrolladas (encauzamiento total del tramo urbano en los años 50, encauzamiento y recorte de meandros en todo el municipio a finales de los 80 principios de los 90 y presas de contención en ríos y ramblas de toda la cuenca) se evitó el desbordamiento en las crecidas de 1997, 2000 y 2012, por lo que no se ha desbordado el río en el casco urbano desde octubre de 1982.

Murcia tiene un clima mediterráneo seco. De acuerdo con la clasificación climática de Köppen es en general un clima semiárido cálido de tipo "BSh". Con una temperatura media anual de 18,6 °C en Murcia (Centro Meteorológico) y de 18,2 °C en Murcia / Alcantarilla (Base Aérea), el área urbana y sus zonas más próximas se sitúan por encima de la barrera de los 18 °C que separa las variantes fría (BSk) y cálida (BSh) de este tipo de clima, si bien las medias son inferiores a los 18 °C en las zonas circundantes de huerta más expuestas a la inversión térmica, en las Sierras del Valle-Carrascoy y en el Campo de Murcia, en la zona sur del municipio, en donde la temperatura media se sitúa en torno a los 17-17.5 °C, dando lugar al clima "BSk" (semiárido frío).

Con inviernos suaves y veranos calurosos, las temperaturas oscilan entre los 16 °C y los 4 °C de enero y los 34 °C y los 21 °C de agosto, si bien las temperaturas extremas pueden superar los 40 °C en verano y descender de los 0 °C en invierno. Los valores extremos absolutos en las estaciones principales existentes en el municipio oscilan entre los 46,1 °C de máxima registrados en Murcia / Alcantarilla el día 4 de julio de 1994, y los -7,5 °C registrados en Murcia (Centro Meteorológico) el día 16 de enero de 1985. De la histórica estación meteorológica de Murcia / Instituto, puesta en marcha en 1866 en la azotea del actual Instituto Licenciado Cascales, y en funcionamiento hasta mediados del siglo XX, existen valores extremos de 47,8 °C de máxima registrados el día 29 de julio de 1876, y de -5,5 °C de mínima registrados el día 15 de enero de 1871. El valor de 47,8 °C es récord absoluto de temperatura máxima registrada en España en el siglo XIX, aunque hay que considerar que, el mismo día de ese registro, otras estaciones peninsulares alcanzaron registros superiores, aunque no fueron finalmente homologados por dudosos.

Respecto a las precipitaciones, los acumulados medios anuales se sitúan en el entorno de los 300 mm en gran parte del municipio, siendo superiores a los 350 mm en la cara norte de las Sierras del Valle-Carrascoy y zonas próximas. Las precipitaciones se concentran normalmente en pocos días, principalmente en invierno, primavera y sobre todo otoño, pudiendo ser torrenciales en situaciones de gota fría, con valores superiores a los 100 mm en menos de 24 horas, ocasionando riadas e inundaciones. La precipitación máxima en un día es de 136 mm registrados en Murcia / Alcantarilla el 10 de octubre de 1943. La nieve, extraordinariamente rara en la ciudad y el valle del Segura, puede caer en las cumbres y zonas altas de las Sierras del Valle-Carrascoy en episodios de entradas frías en invierno. En zonas bajas, la nevada más importante del siglo XX se produjo el 26 de diciembre de 1926, en donde según la prensa de la época, se llegó a acumular más de un metro de espesor en menos de 36 horas. Las dos últimas nevadas generalizadas en el municipio se produjeron el 12 de febrero de 1983 y el 18 de enero de 2017; esta fue la primera nevada generalizada del siglo XXI, cuajando en toda la Región de Murcia, incluyendo zonas de montaña, litoral y la huerta de Murcia.

El viento sopla normalmente de componente este-sureste desde los últimos meses de primavera, influenciado por la entrada de la brisa marina. Y gira a componente oeste a finales de otoño, durante el invierno y primeros meses de primavera. La máxima racha de viento, registrada el 4 de octubre de 1987 en Murcia (Centro Meteorológico), es de 108 km/h. Murcia / Alcantarilla tiene un registro máximo de 103 km/h. Sin embargo, es en las cumbres de las Sierras del Valle-Carrascoy donde el viento sopla con mayor intensidad, habiéndose llegado a medir rachas de hasta 141,6 km/h el día 24 de enero de 2013 en la estación meteorológica del Pico Relojero, a 609 msnm. En esta estación automática, en funcionamiento desde el año 2012, se alcanzan anualmente rachas máximas superiores a los 100 km/h, y tiene una media, para sus primeros 3 años de funcionamiento, de 44 días al año con rachas de viento superior a los 62 km/h, fuerza temporal según la definición establecida en la Escala de Beaufort.

Además de la huerta y las zonas urbanas, el término municipal cuenta por su gran tamaño con distintos paisajes: "tierras baldías", pinares de pino carrasco en las sierras de la Cordillera Sur y zonas de típico secano mediterráneo en el Campo de Murcia.
En el municipio se encuentra la mayor parte del de El Valle y Carrascoy, compartido con los municipios de Fuente Álamo de Murcia y Alhama de Murcia y que comprende gran parte de las sierras de la ya referida Cordillera Sur, siendo el pulmón verde de la ciudad. Dentro del parque, las sierras de Carrascoy, del Puerto y Cresta del Gallo están declaradas LIC, mientras que las sierras de la Cresta del Gallo, Villares, Columbares y Altaona, cuentan además con protección ZEPA.

El grupo faunístico más destacado en el ámbito del parque es el de las aves, y en especial las rapaces como "Águila perdicera", "Águila real", "Águila culebrera", "Águila calzada", "Ratonero" y "Halcón peregrino", destacando también la abundante presencia del "Búho real", especie que posibilitó la declaración de ZEPA al contar con una de las colonias más numerosas de España y con mayor densidad del Mundo. En cuanto a los mamíferos, está constatada la existencia de "Jabalí", "Zorro", "Gato montés" o distintas especies de mustélidos como "Garduña", "Tejón", "Comadreja", al igual que siete especies de "Murciélago".

La vegetación del parque está constituida principalmente por un bosque de "Pino carrasco", que en algunas zonas presenta "Pino piñonero" o manchas de "Carrasca". Hay que destacar los ejemplares relictos de "Alcornoque" presentes en el área denominada "Majal Blanco". El sotobosque mejor conservado cuenta con un matorral típicamente mediterráneo en el que el "lentisco", "acebuche", "palmito", "enebro", "espino negro" y "coscoja" son los más representativos.

Dentro de la fauna fluvial presente en el río Segura, destaca la recuperada presencia de la "Nutria" en el tramo inicial del río desde la Contraparada hasta la las proximidades de la ciudad. Igualmente se pueden encontrar "Ánades reales", "Garzas", "Fochas", "Garcetas", "Gallineta común", "Barbos" o "Carpas", ejemplos de especies antiguamente desaparecidas en el municipio y que han pasado a ser habituales de nuevo tras un largo proceso de recuperación ambiental y depuración de aguas. Incluso, en áreas del río alejadas de núcleos urbanos se pueden observar "Carriceros", "Martinetes", el "Martín pescador" o el "Avetorillo".

Asimismo, en la zona norte del término municipal, lindando con el de Santomera, se encuentra el paraje boscoso protegido llamado "Coto Cuadros", declarado "Monte de Utilidad Pública".

El paisaje más conocido y significativo del término municipal es la antiquísima Huerta de Murcia, espacio que dominaba gran parte de la vega segureña rodeando la ciudad, pero que desde hace décadas sufre la presión de la expansión urbana que junto a la terciarización de la economía y la ausencia de políticas de conservación ha reducido notablemente su extensión.

Entorno cultural

El paisaje huertano se muestra como un inmenso mosaico de poblamiento disperso fruto de la necesidad de los habitantes de vivir junto a sus cultivos. Entorno natural caracterizado por las acequias con sus mondas y su típica vegetación de cañas y árboles de ribera, además de los árboles frutales, donde destaca el limonero en un espacio parcelario alineado de hortalizas y con la abundante presencia de la morera.

El sistema de riegos de la huerta de Murcia se basa en una compleja red de acequias y demás canales de irrigación de antiquísimo origen. Los musulmanes fueron los que aprovecharon las áreas agrícolas romanas presentes en la vega segureña desde siglos antes, ya que la depresión aluvial tenía especiales características para el desarrollo de regadío. La auténtica transformación del valle tuvo lugar con la construcción del azud de la Contraparada, situado en el lugar en que el Segura hace su entrada en la "depresión prelitoral" y que se encarga de retener y elevar las aguas hacia las acequias mayores, la de "Aljufía" (al norte del río Segura) y "Alquibla" (al sur). Canales que el geógrafo árabe Al-Himyari describía como: 

El crecimiento demográfico impulsó la necesidad de colonizar tierras cada vez más lejanas de la Contraparada, aumentando la complejidad de todo el sistema. Algunas acequias se destinaron al servicio de la ciudad como la "Argualexa", proporcionando caudal necesario para el abastecimiento de los edificios públicos y de las industrias artesanas. La red de acequias surtía de energía a la industria murciana, ya que a su vera se desarrollaron molinos harineros, de batanes, de pimentón, fábricas de pólvora y salitre, fábricas de curtidos, de paños, de hilaturas de seda hasta llegar a finales del siglo XIX, cuando aparecen las primeras industrias conserveras.

La superficie de la huerta de Murcia ha vivido vaivenes a lo largo de la historia, desde un importante retroceso sufrido en el siglo XIV como consecuencia de la crisis e inseguridad reinante, hasta la expansión del siglo XVIII con motivo del auge del sector sericícola.A finales de los años 90 del siglo XX, tras varias décadas de terciarización económica, abandono de cultivos y expansión urbana, el espacio de regadío cubría una superficie próxima a las 12 500 hectáreas (menor hoy día tras el reciente boom urbanístico), que poco a poco ha ido cediendo sitio a la urbanización del suelo, cambiando radicalmente los usos tradicionales.

Medio físico

Así, la Huerta de Murcia se extiende por toda la vega del Segura desde el azud de la Contraparada al oeste hasta Orihuela, ya en la Vega Baja, al este, recorrida por más de 500 kilómetros de cauces.

Los "heredamientos" son las tierras que riega cada una de las acequias mayores, dividiéndose en dos grandes heredamientos generales subdivididos a su vez en particulares, destacando los del lado norte -margen izquierda del Segura- (Aljufía, Churra la Vieja, Alfatego, Beniscornia, Béndame, Arboleja, Caravija, Zaraiche, Santomera, Zaraichico, Casteliche, Nelva, Benetúcer, Raal Viejo, Aljada, Azarbe de Monteagudo, Azarbe Mayor, Pitarque y Raal Nueva).

Los del lado sur -margen derecha del Segura- (Alquibla, Barreras, Dava, Turbedal, Benialé, La Raya o Puxmarina, Almohajar, la Herrera, Condomina, Beniaján, Batán o Alcatel, Junco, Alguazas, Aljorabia, Alfande, Alarilla, Azarbe de Beniel, Riacho, Zeneta, las Parras y Carcanox).

La acequia de Churra la Nueva forma sin embargo un heredamiento independiente al resto, tomando sus aguas antes de la Contraparada.

Leyes y normas propias. El Consejo de Hombres Buenos

La existencia de la Huerta y su sistema de riego implicaba la cooperación de los huertanos mediante la regulación de los riegos. Para ello, desde los tiempos de los musulmanes y tras la reconquista, el concejo de la ciudad dictó una serie de leyes y normas encaminadas a proteger la Huerta y solucionar los conflictos que se generan. Como consecuencia de esto, aparecen una serie de instituciones y figuras jurídicas encargadas de velar por este espacio y sus riegos comunitarios dando lugar al desarrollo de una legislación local, en parte escrita y en parte consuetudinaria, sobre reparto, uso del agua y control de las infracciones. Todas ellas están recogidas en las "Ordenanzas y Costumbres de la Huerta de Murcia" que se recogen por escrito desde el siglo XIX, regulando también a la Junta de Hacendados de la Huerta de Murcia y el Consejo de Hombres Buenos.

Este consejo es una institución que se remonta a la Edad Media y cuya función es conocer y resolver las reclamaciones y pleitos, en un orden arbitral y extrajudicial, permitiendo resolver los litigios mediante actuaciones baratas, rápidas y especializadas haciendo posible una eficaz y pronta recuperacuón del orden quebrantado. Sus actuaciones eran verbales y no se comienzan a recoger por escrito hasta el siglo XVIII, estando sus decisiones reconocidas dentro del ordenamiento jurídico español.

En el año 2009, el Consejo de Hombres Buenos de la Huerta de Murcia fue declarado Patrimonio Cultural Inmaterial de la Humanidad por la Unesco como ejemplo de tribunal consuetudinario de regantes del Mediterráneo español.

En la siguiente tabla aparecen los municipios que limitan con el término municipal de Murcia, en la secuencia geográfica en la que están situados:

Además, el municipio de Alcantarilla está completamente rodeado por el término municipal de Murcia.

Existen muchas dudas sobre los orígenes de la ciudad de Murcia. Hay constancia de que fue mandada fundar el 25 de junio del año 825 por el emir de Al-Ándalus Abderramán II con el objetivo de sofocar las revueltas entre yemeníes y qaysíes que ensangrentaban las tierras de la Cora de Tudmir, para hacer así más fuerte el poder del Emirato de Córdoba sobre los particularismos tribales. Historiadores como Rodríguez Llopis defienden sin embargo que lo que se produjo en aquel año no fue la fundación sino el establecimiento de la capitalidad de Tudmir en una Murcia en cierto modo ya existente.

Todo parece indicar que ya existía un pequeño lugar poblado en esta misma zona, cuyos órígenes se remontarían a una villa romana denominada "Murtia", en clara referencia a la existencia de humedales y mirtos -arrayanes- en torno a ella, asentamiento del que la nueva ciudad acabaría tomando el nombre, aunque islamizado. De hecho, está arqueológicamente demostrado el desarrollo de un extenso complejo de villae romanas en el valle del Segura que aprovechaban la feracidad de las terrazas fluviales y la abundancia del agua del río.

Sin embargo, las evidencias humanas más antiguas en el actual territorio del municipio de Murcia pertenecen a la Cultura del Argar; cultura desarrollada durante la Edad del Bronce que tuvo su centro en el sureste ibérico con un avanzado concepto de urbanismo, además del dominio de la agricultura y la metalurgia del bronce.

En la época prehistórica, así como en la antigüedad, la mayoría de asentamientos humanos se concentraron en los rebordes montañosos de la "depresión prelitoral" o vega del Segura. Así, en el reborde sur destacan los yacimientos del "Puntarrón Chico" de Beniaján de época argárica, o "Santa Catalina del Monte" del Bronce Final. En el reborde norte destaca el yacimiento de la "Cuesta de San Cayetano" de Monteagudo, con una secuencia que va desde el Argar, pasando por el Bronce Tardío y el mundo íbero, finalizando en la Roma altoimperial.

Con la llegada de la Edad del Hierro, los íberos, concretamente los contestanos, tuvieron un especial desarrollo en el reborde sur con los yacimientos del "Verdolay", en donde aparece un importante poblado, con una necrópolis asociada (el "Cabecico del Tesoro") y un santuario (el "Santuario de la Luz") datados entre el 500 a. C. y la romanización.

Fue en plena época romana cuando comenzaron los asentamientos en el fondo del valle del Segura, zona de almarjales y aguas estancadas que fueron convertidas al cultivo a través de las primeras evidencias de aprovechamiento hídrico de la zona, comprobándose en yacimientos de época tardoantigua como el de "Senda de Granada". Como ya se ha comentado, el origen antiguo de Murcia estaría en una de esas villae que aparecieron en áreas más próximas al río Segura.

La referida zona de la Cordillera Sur vivió otro impulso poblacional en época tardorromana-visigoda, como parecen demostrar algunas infraestructuras que han llegado hasta nosotros. Es el caso de los yacimientos del "Martyrium de La Alberca" del siglo IV y la "Basílica del Llano del Olivar" de Algezares (siglo VI).

Aunque la explotación agraria y el aprovechamiento hídrico a gran escala del valle en donde se encuentra Murcia se remonta a tiempos romanos; fueron los árabes los que, valiéndose del curso del río Segura que atraviesa la depresión prelitoral, perfeccionaron y ampliaron una compleja red hidrológica formada por acequias, brazales y regaderas, dando impulso a la ciudad convirtiéndola en uno de los centros de producción agraria más importantes de Al-Andalus. Esto llevó a que a partir del siglo X Murcia se convirtiera en capital política y centro económico de la Cora de Tudmir.

No fue hasta la segunda mitad del siglo XI, tras el fin del Califato, cuando la ciudad de Murcia encabezó su primer reino taifa independiente bajo el mandato de Abu Abd al-Rahman Ibn Tahir. Conquistada por Al-Mutamid de Sevilla, fue epicentro del conflicto entre este y su visir Ibn Ammar.

La ciudad capitalizó un segundo reino taifa de la mano de Ibn Mardanis; conocido por los cristianos como Rey Lobo. Durante este periodo (1147-1172) Murcia vivió un momento de esplendor convertida en un centro político y cultural comparable a las principales capitales islámicas del momento, siendo cabeza de la resistencia andalusí frente al Imperio Almohade.

Tras la victoria cristiana en Las Navas de Tolosa (1212), Castilla se expandió hacia el sur, dirigiéndose hacia la taifa de Murcia, que en su tercer periodo estuvo regida por la dinastía de los Banu Hud, que tras 1228 se habían sublevado contra los almohades consiguiendo el control de casi toda Al-Andalus teniendo su capital en Murcia. Finalmente, el infante Alfonso de Castilla (futuro Alfonso X el Sabio) acordó con Ibn Hud al-Dawla el vasallaje de la ciudad en 1243 a través del Tratado de Alcaraz, incorporándola a la Corona de Castilla en forma de protectorado.

En 1264 los mudéjares murcianos se sublevaron contra los castellanos por el incumplimiento de lo pactado. Alfonso X, empleado entonces en sofocar la revuelta del sector andaluz, pidió ayuda urgente a su suegro Jaime I de Aragón. Tropas de la Corona aragonesa sofocaron la rebelión en 1266 y conquistaron Murcia definitivamente, eliminando los restos de autonomía musulmana al devolver la ciudad a la jurisdicción de Castilla en virtud del Tratado de Almizra. Jaime I de Aragón licenció a 10000 aragoneses para repoblar la zona, concediéndoles tierras, en algunos casos grandes extensiones.

Tras el fin del protectorado, Alfonso X el Sabio estableció las bases sociopolíticas del municipio al concederle el Fuero de Sevilla, convirtiéndola en capital del nuevo Reino de Murcia al ser la sede del "Adelantado Mayor" y tener voto en Cortes.

En el contexto de la Corona de Castilla, Murcia fue durante el reinado de Alfonso el Sabio una de las tres capitales en las que iba rotando la corte itinerante, junto a Toledo y Sevilla, creando un "studium arabicum et hebraicum". En ella quiso ser enterrado por disposición testamentaria, aunque finalmente acabaran por reposar su corazón y entrañas.

En el año 1291 Murcia se convirtió de manera oficial en la sede episcopal de la Diócesis de Cartagena tras el beneplácito de Sancho IV el Bravo.

En el contexto de la crisis dinástica en la corona castellana, Jaime II de Aragón ocupó la ciudad en el 1296, devolviéndola posteriormente a control castellano en virtud de la Sentencia Arbitral de Torrellas (1304).
Durante el siglo XIV se vivió una profunda crisis que afectó a la actividad agrícola de la huerta de Murcia y por ende a la ciudad, debido a las epidemias de peste y al contexto de inseguridad que se vivía en todo el Reino de Murcia, afectado como estaba por una triple frontera (con la corona de Aragón, con un Mediterráneo atestado de corsarios y sobre todo con los musulmanes granadinos).

A mediados del siglo XV comenzó una recuperación económica gracias al final de la amenaza granadina. En 1452 las tropas de la ciudad de Murcia junto con las de Lorca vencieron en la batalla de Los Alporchones a huestes musulmanas provenientes del reino nazarí. A partir de 1482, tanto Murcia como Lorca se convirtieron en la base de operaciones para las campañas militares que los Reyes Católicos lanzaron sobre la parte oriental del reino de Granada. La ciudad de Murcia sirvió de residencia a los monarcas en 1488.

En el 1520 Murcia se unió al levantamiento comunero aunque con unos matices totalmente distintos al resto de Castilla por su claro sentimiento antioligarquico que entroncaba con los conflictos que se vivían en la región a finales del siglo XV. Los comuneros murcianos implantaron una junta de síndicos con cierta representación popular y elegidos por parroquias.

En el reinado de Felipe II, tropas murcianas bajo mando de Luis Fajardo; II Marqués de los Vélez y adelantado del reino de Murcia, ayudaron a sofocar la rebelión morisca en el Reino de Granada. Este hecho hará que se le conceda a Murcia el título de "Muy noble y muy leal". El conflicto de las Alpujarras supondrá así mismo el hundimiento del sector sedero granadino, y en consecuencia, el auge de la seda murciana que permitirá a la ciudad y su reino esquivar los efectos de la crisis finisecular del siglo XVI a diferencia de Castilla. De hecho, la crisis no llegaría a Murcia hasta la tercera década del siglo XVII.

En el año 1613, Felipe III decidió la expulsión de los moriscos murcianos que todavía quedaban en las diseminadas aljamas de la huerta y que tan vitales fueron para la producción sericícola.

La crisis se precipitó sobre la ciudad con la epidemia de peste de 1648 y la posterior "Riada de San Calixto", que en 1651 arrasó Murcia con una avenida del río Segura que causó más de 1000 muertos. Sin embargo, en 1654 fue fundada la Real Fábrica del Salitre por orden de Felipe IV para revitalizar la ciudad.

En el año 1705 fue nombrado obispo de Cartagena Luis Belluga y Moncada. En el contexto de la Guerra de Sucesión Española fue el artífice del triunfo de la causa borbónica en la ciudad, organizando la defensa de Murcia ante el avance de la causa austracista en el sureste, venciendo en la batalla del Huerto de las Bombas, a las afueras de la ciudad. Esta victoria supuso un giro en la Guerra de Sucesión, comenzando el avance de la causa borbónica que culminaría en la batalla de Almansa.

Durante el siglo XVIII Murcia vivió una importante expansión económica. La base de este crecimiento se cimentó en un impulso agrícola basado así mismo en el aumento de la superficie cultivada. Las roturaciones provocaron una mayor extensión de la huerta de Murcia y de cultivos de secano en la zona de campo, algo que trajo consigo la aparición de asentamientos humanos en dichas áreas (el origen de muchas de las actuales pedanías). Como afirma el historiador Rodríguez Llopis, Murcia alcanzó a finales de siglo la cifra de 70 000 habitantes. En este contexto de riqueza continuó teniendo un importante papel el comercio de la seda, de hecho en 1770 se instaló en Murcia la Real Fábrica de Hilar Sedas a la Piamontesa.

La boyante coyuntura quedó reflejada en las artes y el urbanismo de la ciudad. Es la época de las iglesias y palacios barrocos y del escultor Francisco Salzillo. La expansión motivó que el primer asentamiento humano en la margen derecha del Segura se afianzara; el hoy conocido como Barrio del Carmen.

A finales del siglo XVIII, el murciano José Moñino Redondo, conde de Floridablanca fue nombrado ministro de Carlos III. Floridablanca favoreció notablemente a la tierra que le vio nacer a través de infraestructuras y medidas de carácter ilustrado.

Con el estallido de la Guerra de la Independencia española en 1808, en la ciudad de Murcia se creó una Junta Suprema presidida por el conde de Floridablanca que pretendió extender su autoridad en todo el reino de Murcia ante la ausencia del poder real.

En 1810 se produjo la entrada de las tropas francesas de Sebastiani, el día 24 de abril la ciudad fue saqueada brutalmente.
El 25 de enero de 1812 las tropas francesas del general Soult entraron también en la ciudad. En la calle de San Nicolás se produjo un encontronazo entre los soldados de Soult y las milicias del general Martín de la Carrera, que murió en dicho combate.

En febrero de 1820, tras el alzamiento de Riego que supuso el inicio del Trienio Liberal, el vizconde de Huertas orquestó con campesinos de la huerta y algunos militares el asalto a la prisión para liberar a los presos políticos, como el general Torrijos, proclamándose en la ciudad la Constitución de 1812.

Con la creación de las actuales provincias en 1833, Murcia se convirtió en capital de la de igual nombre, mientras que el antiguo reino de Murcia se dividió en las provincias de Murcia y Albacete.
En 1862 comenzaron a discurrir trenes entre Murcia y Cartagena en un viaje inaugural presidido por la reina Isabel II, y en 1865 la ciudad ya estaba conectada por ferrocarril con Albacete y Madrid. La llegada de este medio de transporte supuso una ampliación urbana hacia el sur, desarrollándose más aún el mencionado Barrio del Carmen.

Durante el Sexenio Democrático, se produjeron dos levantamientos en Murcia de carácter federal, el primero en 1869 y el segundo en 1872, dirigidos ambos por el revolucionario Antonio Gálvez Arce, conocido popularmente como "Antonete Gálvez". En el verano de 1873 la ciudad se unió al "Cantón Murciano" que se había proclamado en la sublevación cantonal de Cartagena, siendo uno de los principales conflictos a los que se tuvo que enfrentar la I República Española.

El 15 de octubre de 1879 acaeció la conocida como riada de Santa Teresa, una de las mayores de la historia de Murcia, la región murciana y toda la cuenca del Segura, que produjo cerca de 800 muertos en la ciudad y su huerta.

En los años de la II República, Murcia fue una ciudad con voto mayoritario de izquierdas en las sucesivas elecciones que tuvieron lugar. Durante la guerra civil, la ciudad permaneció fiel a la República hasta el 29 de marzo de 1939 cuando la 4.ª División de Navarra al mando de Camilo Alonso Vega tomó Murcia, apenas dos días antes del final de la contienda, en la llamada Ofensiva final.

Durante la dictadura franquista, tras la dura posguerra Murcia vivió una gran expansión urbana que le llevó a superar sus tradicionales límites bajo el sello del desarrollismo de la época, a costa de la huerta circundante y de parte del casco histórico.

Con la llegada de la Transición y la nueva organización territorial por autonomías, la ciudad se convirtió en capital de la comunidad autónoma de la Región de Murcia, siendo sede de la presidencia y las consejerías, no así del parlamento, sito en la ciudad de Cartagena.

El crecimiento económico de los años 60 y 70 vino acompañado de un auge demográfico que llevó a la ciudad a crecer a gran velocidad y crear infraestructuras viarias acordes con las nuevas necesidades. En la última década del siglo XX y la primera del siglo XXI vivió un nuevo periodo de crecimiento, convirtiéndose en el séptimo municipio por población de España y en un importante centro de negocios, a pesar de la Crisis económica de 2008-2015.

El municipio de Murcia contaba en 2019 (INE) con 453 258 habitantes, siendo el . Sin embargo, debido a la gran extensión del término municipal (881,86 km²), su densidad demográfica (513,98 hab./km²) está lejos de los primeros puestos.

Del total de la población del municipio, 169 744 personas residían en el distrito de la capital, lo que representa un 37% del total municipal. Con una superficie de 11,82 km², dicho distrito contaba con una densidad de 14 360 hab./km². 

El otro 63% de los habitantes (283 514 personas) se repartía entre las 54 pedanías en las que se divide el término. Algunas de éstas han sido anexionadas de facto por la expansión urbana de la ciudad, constituyendo auténticos barrios aunque administrativamente sigan siendo pedanías, por lo que su población no se computa junto a la del distrito de la capital, caso de Los Dolores, San Benito, Zarandona, Puente Tocinos, Santiago y Zaraiche o El Puntal. Si se sumara la población de esta conurbación al distrito de la capital, la ciudad estaría en torno a los 230 000 habitantes (el 51% del total municipal).

La mayoría de las pedanías tienen varios núcleos de población hasta alcanzar un total ampliamente superior a los 100 núcleos en el conjunto del municipio. Esto, sumado a la existencia de numerosos diseminados, indica una población muy dispersa. En el nomenclátor del INE aparecen 157 núcleos de población, pero en ese total están incluidos 28 que, en realidad, son barrios de la capital y uno más que aparece sin población, lo que dejaría el total en 128 núcleos habitados.

Dentro de las dos grandes zonas en las que se divide el municipio, el Campo de Murcia y la Huerta de Murcia, es en esta última donde se localiza la gran mayoría de las pedanías (45) y de la población (93,57% si incluimos la población de núcleo urbano), por lo que la densidad en la Huerta de Murcia es mucho mayor que la del Campo de Murcia o la del cómputo total del municipio (816 hab./km² frente a 75 hab./km²) siendo por tanto el poblamiento mucho más denso que en el Campo de Murcia. La tipología de poblamiento también es muy diferente: en las pedanías de la Huerta la densidad de población es alta con numerosos núcleos de población, algunos de ellos de gran tamaño, cercanos entre sí y muchas viviendas dispersas fuera de los núcleos, lo que produce un paisaje que, o bien es urbano, o tiene la constante presencia de viviendas; en las pedanías que no son de la Huerta (las del Campo de Murcia junto a las del Campo de Sangonera) la densidad de población es mucho más baja con pocos núcleos de población, todos de pequeño tamaño, y grandes zonas libres de población.

Fue a finales del siglo XVIII cuando se realizó el primer censo, ordenado por el Conde de Floridablanca. Murcia se componía entonces de 63 665 habitantes (1787). Tras un relativo estancamiento vivido durante el siglo XIX, en el siglo XX mantuvo un incremento poblacional constante, salvo en los años sesenta con un ligero descenso, disfrutando de importantes aumentos en la década de 1930 y a partir de 1970.

De acuerdo con los datos oficiales, en el año 2018 el 11,14% de la población del municipio era de nacionalidad extranjera, concretamente 49 834 personas, siendo las comunidades más importantes (de mayor a menor) las formadas por ciudadanos marroquíes (15 160), ucranianos (4 506), ecuatorianos (4 174), bolivianos (3 343), búlgaros (2 093), rumanos (2 092), chinos (1 801), colombianos (1 762), británicos (1 668) y argelinos (981).

Evolución demográfica del municipio de Murcia desde 1842:

Murcia es el centro de un área metropolitana que, si bien no está delimitada administrativamente, sí está reconocida como área urbana por el Ministerio de Fomento de España. La extensión y población de esta área dependen de cada estudio realizado al respecto, pero el del Ministerio incluye dentro de la misma a los municipios de Alcantarilla, Alguazas, Archena, Beniel, Ceutí, Lorquí, Molina de Segura, Santomera y Las Torres de Cotillas. Esta área estaría formada por 10 municipios, con una población de 650 468 habitantes en 2017 (siendo la décima más poblada de España), distribuidos en una superficie de 1230,9 km² y contando con una densidad de 528 hab/km².

El proyecto AUDES5 también define la conurbación de Murcia-Orihuela, la cual, integraría la aglomeración metropolitana anteriormente descrita de Murcia, Molina de Segura y Alcantarilla junto al área urbana de Orihuela. Esta área metropolitana suprarregional contaría con una población total de 776 784 habitantes (INE 2009), una superficie de 1787 km² y una densidad de 445,54 hab/km², por lo que sería la séptima de España.

De forma tradicional el municipio de Murcia fue un importante productor de materia prima agrícola gracias a su feraz y milenaria huerta. Durante la primera mitad del siglo XX su agricultura se convirtió en especializada e intensiva, permitiendo comercializar sus productos en mercados internacionales, algo que se intensificó en las décadas de 1950 y 1960. El municipio de Murcia participó en la exportación de tomates, lechuga y, especialmente limones y naranjas a toda Europa junto con otros muchos municipios de la Región de Murcia. 

Pese a que este sector fue antaño la base económica del municipio, su importancia es ahora mucho menor tras la terciarización vivida a partir de los años 60-70, la redistribución regional del suelo agrícola tras la llegada del Trasvase Tajo-Segura (concentrando la producción en otras comarcas anteriormente de secano y poco productivas, entre ellas el Campo de Murcia perteneciente al municipio) y la expansión urbana. Estos factores han generado la desaparición casi total en algunas zonas de la huerta de Murcia y su degradación paisajística actual.

El sector secundario industrial apareció con fuerza en el municipio de Murcia a comienzos del siglo XX a través de sectores derivados de su potente agricultura. En la antigüedad, el sector sericícola de tipo preindustrial tuvo una fuerza importante en el municipio, pero decayó durante el siglo XIX. Entre 1900 y 1930 se vivió la época dorada de la industria del pimentón, con firmas como "F.F." o "Albarracín". Tras la profunda crisis de la posguerra y la autarquía, en la década de 1960, los productores murcianos consiguieron el liderato en España, constituyendo la mitad de la oferta nacional. Las factorías pimentoneras capitalinas se concentraban fundamentalmente en el barrio de Espinardo, al norte de la ciudad. El sector de la conserva vegetal murciana alcanzó el liderato español hacia 1930, recuperándose de la crisis de posguerra en la década de 1950 y alcanzando un nuevo apogeo en la de 1960, hasta mediados de la década de 1970.

Actualmente, la actividad industrial del municipio se concentra en los polígonos industriales de Cabezo Cortao, Camposol y el polígono Oeste. Siendo este último el más grande de la Región de Murcia, compartido con el municipio de Alcantarilla. La potente industria conservera y del pimentón dio paso tras la crisis de principios de la década de 1990 a un sector industrial más diversificado en el que destaca el alimentario, con factorías como la cervecera "Estrella de Levante", los zumos de fruta de "Juver" o "AMC" o los gazpachos envasados "Alvalle" de "PepsiCo". En la confección textil destaca la central de "Liwe Española", en la pedanía de Puente Tocinos. También son destacables los sectores químicos (con la antigua Fábrica de la Pólvora, hoy gestionada por "Expal" en Javalí Viejo), de destilación y la fabricación de muebles y materiales de construcción.

El principal sector económico de Murcia es el sector servicios, de los que destacamos los administrativos, financieros, culturales y de otro tipo. Históricamente la ciudad de Murcia siempre ha sido un centro de intercambio comercial, ejerciendo de redistribuidor de la producción agrícola y artesanal del interior murciano y del sureste ibérico. A partir de finales de la década de 1960 y comienzos de la de 1970, la terciarización en el municipio comenzó a ser omnipresente. En 1991, la ciudad era junto con los municipios de la costa, donde más elevados porcentajes de empleo se daban en el sector servicios en el Levante meridional. 

Murcia actúa como punto de intercambio comercial de toda la Región de Murcia y la cuenca del Segura, extendiéndose su área de influencia a las provincias limítrofes de Alicante, Albacete y Almería. En torno al centro de Murcia se halla el núcleo tradicional del comercio de la ciudad, pudiéndose encontrar desde el pequeño comercio hasta las franquicias de grandes marcas. En el municipio existen tres centros de El Corte Inglés y uno de Ikea (el primero abierto en todo el levante español), además de otros centros comerciales como Atalayas, Nueva Condomina, Thader, Myrtea (antes denominado El Tiro), La Noria, Montevida y Parque Comercial Oeste.

A principios del siglo XXI emergió con fuerza en el municipio el turismo residencial, orientado a ciudadanos europeos, localizado principalmente en el "Campo de Murcia", actividad que tras verse frenada por el pinchazo de la burbuja inmobiliaria ha iniciado una cierta recuperación.

Como capital de la Región de Murcia la ciudad es sede de la Presidencia de la Comunidad Autónoma, del Consejo de Gobierno (ambos sitos en el Palacio de San Esteban), de las distintas Consejerías, del Tribunal Superior de Justicia de la Región de Murcia (sito en el Palacio de Justicia del Paseo de Garay), del Defensor del Pueblo de la Región de Murcia y el Consejo Jurídico de la Región de Murcia (ambos con sede en la calle Alejandro Séiquer) además del Consejo Económico y Social (CES).

Por parte del Gobierno de España también es sede de la Delegación del Gobierno en la comunidad autónoma, así como de la Confederación Hidrográfica del Segura.

Desde la restauración de los ayuntamientos democráticos en 1979, el gobierno de la ciudad estuvo en manos de diversos alcaldes del PSRM-PSOE hasta 1995, fecha en la que se hizo con la alcaldía el Partido Popular.

En la actualidad el alcalde es José Ballesta Germán (PP), quien gobierna en coalición con Ciudadanos desde junio de 2019.

(*)En 2015 la candidatura se presentó como Es Ahora Murcia, sin la participación de Equo

El territorio del municipio de Murcia se organiza administrativamente en el núcleo urbano de la capital y 54 pedanías.

El distrito de la capital ocupa 11,88 km² del total del término municipal y se divide en 28 , agrupados a su vez en 8 distritos.

Dichos barrios son: En la margen derecha del Segura, pertenecientes al distrito de "El Carmen", los barrios de El Carmen, Buenos Aires y Nuestra Señora de la Fuensanta, el distrito de "Infante Don Juan Manuel", con el polígono del mismo nombre, mientras que los barrios de Santiago el Mayor, San Pío X y La Purísima-Barriomar no forman parte de distrito alguno. 

En la margen izquierda se hallan los distritos de "La Flota-Vistalegre", formado por los barrios de Vistalegre y La Flota, el distrito "Este", formado por los barrios de La Paz, Vistabella y La Fama, el distrito "Santa María de Gracia-San Antonio", con la barriada de Santa María de Gracia. Los barrios de San Basilio, El Ranero y San Antón forman parte del distrito "Norte". El barrio de Espinardo no forma parte de ningún distrito.

También en la margen izquierda se encuentran los 11 barrios del centro histórico, que corresponden a las 11 parroquias originarias de la ciudad medieval y sus arrabales: San Andrés (que forma parte del distrito "Norte"), San Antolín, San Miguel, San Nicolás, San Pedro y Santa Catalina (que forman el distrito "Centro-Oeste"), San Juan, San Lorenzo, Santa Eulalia y San Bartolomé (que constituyen el distrito "Centro-Este" junto al barrio de La Catedral).

Mientras que cada distrito posee su respectiva Junta de Distrito, aquellos barrios que no forman parte de ningún distrito poseen en su lugar una Junta Municipal, de la misma forma que las pedanías del municipio.

Actualmente, la extensión del núcleo urbano de la ciudad de Murcia supera ampliamente su distrito administrativo, habiéndose extendido por la práctica totalidad de la pedanía de Santiago y Zaraiche, formando también un continuo urbano con los núcleos de San Benito (pedanía formada por el Barrio del Progreso y Patiño), Zarandona y El Puntal. La colmatación del distrito de la ciudad ha llevado a que los nuevos desarrollos invadan territorio que realmente pertenece a diversas pedanías cercanas como San Benito (parte de Ronda Sur), Los Dolores, Puente Tocinos, Zarandona (la expansión de La Flota hacia el este), Churra (avenida Juan de Borbón) o El Puntal (avenida Juan Carlos I).

A lo largo de los siglos se han producido diversos cambios en el mapa municipal. Varias pedanías se convirtieron en concejos independientes durante el Trienio Liberal (1820-1823), volviendo a integrarse a partir de los años 1830 en adelante ante su falta de sostenibilidad económica, mientras que los de la comarca del Mar Menor (San Pedro del Pinatar, San Javier y Torre-Pacheco) quedaron definitivamente segregados en 1836.

En 1960 y debido a la fuerte expansión urbana de la capital, la mayor parte de la antigua pedanía de Espinardo se incorporó al distrito de la ciudad como barrio. Lo que no se anexionó constituye actualmente la pedanía de El Puntal.

En 1978 se segregó la pedanía de Santomera y se constituyó en un nuevo municipio que incluía El Siscar y La Matanza.

En 1987, una superficie de 10,2 km² de la pedanía de Cañada Hermosa se incorporaron al municipio de Alcantarilla.

El edificio más emblemático de la ciudad es la Catedral de Santa María, sede de la diócesis de Cartagena que se encuentra en pleno casco antiguo, en la Plaza de Belluga. Comenzó a construirse sobre el edificio de la antigua mezquita mayor o "aljama" (convertido en templo cristiano en 1266) a finales del siglo XIV, y se consagró en 1467, aunque diversas partes fueron añadidas o reformadas hasta finales del siglo XVIII, cuando se terminó su famosa torre. Por este motivo presenta diferentes estilos arquitectónicos, especialmente gótico, renacentista y barroco.

Su ornamentada fachada principal (1737-1754), proyectada como un retablo al aire libre, es considerada a menudo una obra maestra del barroco levantino español. Destaca también su alto campanario, de 93 metros (98 con la veleta) siendo el segundo más alto de las catedrales de España tras la Giralda de Sevilla y dotado de veinte campanas. Este muestra una mezcla de estilos arquitectónicos: los dos primeros cuerpos son de estilo renacentista (1521-1555), el tercer cuerpo es barroco y el cuerpo del campanario y la cúpula son de influencias rococó y neoclásicas.

El interior del templo es mayoritariamente gótico. Destacan la "Capilla de los Vélez" y la "Capilla de Junterones" de un total de veintitrés. La primera es de estilo gótico flamígero, con una impresionante cúpula estrellada de diez puntas, y la otra es una de las grandes obras del renacimiento español. La capilla de los Vélez sobresale por el exterior de la catedral, destacando la cadena esculpida que la rodea y sobre la que pesa una famosa leyenda.
En la capilla mayor se encuentra el sepulcro con el corazón y las entrañas de Alfonso X el Sabio.

La Catedral cuenta con un renovado museo (Museo de la Catedral de Murcia) en el edificio que antaño fuera el claustro y en el que se exhibe el tesoro catedralicio.

Junto a la fachada de la catedral, en la misma Plaza de Belluga, se encuentran la Escuela Superior de Arte Dramático y Danza (antiguo Seminario Mayor de San Fulgencio) y el Palacio Episcopal, ambos del siglo XVIII. El majestuoso palacio se divide en dos partes: el cuerpo central articulado sobre un patio porticado y el denominado "Martillo", que fue el mirador de los obispos sobre el Segura y sus jardines y que constituye el cerramiento arquitectónico del contiguo "paseo del Arenal", actual Glorieta.

Esta plaza, abierta hacia el río Segura, ha sido tradicionalmente el centro político de la ciudad. Construida en el siglo XVIII, es un espacio ajardinado donde se encuentra la Casa Consistorial (siglo XIX), con un edificio anexo que da a la Plaza Belluga, obra señera de Rafael Moneo.

Aún es posible apreciar el antiguo entramado urbano medieval de época andalusí, antaño divisorio de religiones y ahora reconvertido en bellas calles peatonales, como la Platería y la famosa Trapería, la cual comunica la "Plaza de la Cruz" (donde se encuentra la torre de la Catedral) con la conocida Plaza de Santo Domingo, uno de los puntos de encuentro más apreciados por los murcianos. En la misma Trapería puede observarse la bella fachada ecléctica del Casino (fundado en 1847), con un interior suntuoso que aúna diferentes estilos, desde un patio árabe inspirado en los salones reales de La Alhambra y en los Reales Alcázares de Sevilla, pasando por un patio romano-pompeyano, una maravillosa biblioteca inglesa con más de 20 000 volúmenes y un bellísimo salón de baile neobarroco, entre otras estancias.

En la susodicha Plaza de Santo Domingo, podemos contemplar la Casa Cerdá; imponente inmueble de estilo ecléctico del primer tercio del siglo XX, además del bello conjunto formado por el Palacio Almodóvar (del siglo XVII pero reformado en 1908) y el arco que lo comunica con la Capilla del Rosario (siglo XVI) y la contigua Iglesia de Santo Domingo (siglo XVIII).

Otras plazas con encanto son la cercana Plaza de Julián Romea; donde además del teatro de igual nombre se asoman algunos palacetes como el Palacio Vinader (del siglo XVIII), la Plaza de las Flores, punto neurálgico del tapeo en Murcia, y su vecina la Plaza de Santa Catalina; que hasta el siglo XVIII era considerada como la plaza mayor de la ciudad.

Uno de los monumentos más importantes de Murcia es el Monasterio de Santa Clara la Real, construido entre los siglos XV y XVIII y en cuyo interior se encuentran los restos del al-Qasr al-Sagir (Alcázar Seguir), un palacio árabe del siglo XIII del que se han recuperado la alberca, los arriates y parte del salón norte (visitables a través del Museo de Santa Clara), destacando también un claustro gótico final.

Por todo el casco antiguo se alzan numerosas iglesias o conjuntos monásticos de gran valor. Además de las construcciones góticas ya señaladas como la Catedral o el Monasterio de Santa Clara, destaca la antigua Ermita de los Pasos de Santiago, con artesonado mudéjar.

Del renacimiento, además de la mencionada Capilla del Rosario, destaca el Colegio de San Esteban, primer colegio jesuita en España comenzado en 1555, sede actual del Gobierno autonómico bajo el nombre de "Palacio de San Esteban", del que destacan su iglesia y claustro. De principios del siglo XVII encontramos la Iglesia de San Pedro y el claustro del antiguo Convento de la Merced. De la misma centuria son la Iglesia de Jesús, sede de la Cofradía de "Los Salzillos", la iglesia del mencionado Monasterio de Santa Clara, el antiguo Convento de San Antonio y la "Capilla de la Arrixaca" de la Iglesia de San Andrés.

Dentro del barroco murciano desarrollado principalmente durante el siglo XVIII, hay que reseñar desde los primeros ejemplos de finales del XVII y principios de la siguiente centuria como la Iglesia de San Miguel, el Convento de las Agustinas del Corpus Christi o las iglesias de los ya referidos conventos de la Merced, Santo Domingo o Santa Ana; hasta las posteriores iglesias de influencia rococó (tras el impacto que supusieron las obras de la fachada principal de la Catedral en la ciudad) como el Carmen, San Nicolás de Bari, Santa Eulalia y San Juan de Dios (o también el Hospicio de Santa Florentina, el mencionado Seminario Mayor de San Fulgencio, el Seminario Menor de San Leandro, la portada y claustro del Colegio de la Anunciata -posterior Real Fábrica de Seda- o el Antiguo Colegio de Teólogos de San Isidoro). 

Las tendencias neoclásicas llegaron a la ciudad de la mano de la Iglesia de San Juan Bautista, además de las iglesias de San Lorenzo y San Bartolomé, adentrándose estas dos últimas en el siglo XIX, completándose San Bartolomé con fachada y nave historicista.

De la época musulmana en Murcia, además de los referidos restos del Alcázar Seguir, encontramos los restos del oratorio y el panteón real del Alcázar Mayor del siglo XII. También dispone de diferentes tramos de la muralla árabe, destacando el tramo de la "muralla de Verónicas" y el que se conserva dentro del "Centro de Interpretación de la muralla de Santa Eulalia". También hay que destacar el reciente hallazgo del arrabal de la Arrixaca en el antiguo jardín de San Esteban, del siglo XII y XIII, cuya musealización está en proyecto.

Dentro de los inmuebles de la ciudad, como ejemplos del renacimiento destacan el Palacio Pacheco y las portadas del derruido Palacio Riquelme. Del auge de la seda que vivió Murcia a comienzos del siglo XVII quedan, además del Palacio Almodóvar, las muestras del Palacio del Almudí, antiguo pósito municipal, hoy sala de exposiciones, y las portadas del derruido Contraste de la Seda. Avanzado el siglo se construyó el Palacio de los Saavedra, actual Colegio Mayor Azarbe.

Del siglo XVIII murciano hay que resaltar desde la inicial tipología barroca del Palacio de los Pérez-Calvillo a la posterior evolución rococó del mencionado Palacio Episcopal, el Palacio Fontes (sede de la Confederación Hidrográfica del Segura) y el referido Palacio Vinader. De finales de siglo, adquiriendo tintes neoclásicos encontramos el Palacio de Floridablanca, actual Hotel "Arco de San Juan", el Palacio Campuzano y el Palacio de la Inquisición, sede del Colegio de Arquitectos, ya del XIX, momento en el que se construyó la mencionada Casa Consistorial.

Del eclecticismo decimonónico la ciudad posee el ya referido Real Casino de Murcia, el Teatro Romea; inaugurado en 1862 pero reconstruido tras varios incendios por Justo Millán Espinosa, siendo el exterior de 1880 y el interior de 1899, la estación del Carmen de José Almazán (1863), el antiguo Hotel Victoria (1885) o la Plaza de toros de La Condomina, inaugurada en 1887 (también de Justo Millán).

Dentro de las corrientes modernistas que llegaron a la ciudad a principios del siglo XX habría que señalar la Casa Díaz-Cassou (1900-1906), la Casa Almansa (1903-1908) -actual sede de la Cámara de Comercio-, el Mercado de Verónicas (1912-1916) y la posterior Casa Guillamón (1920-24), obras de los arquitectos Pedro Cerdán -autor de la fachada principal del Casino (1902)- y José Antonio Rodríguez. Sin embargo, el eclecticismo continuó teniendo presencia transcurrido el siglo con La Convalecencia (1909-1915), la Alegría de la Huerta (1919-21), el Cuartel de Artillería (1921-26), la Sociedad de Cazadores (1927), la estación de Zaraiche (1921-1930) o la ya mencionada Casa Cerdá (1934-36), así como ejemplos de inspiración neoyorquina como la Casa de los Nueve Pisos (1914-41).

Dentro de la arquitectura de vanguardia desarrollada a partir de la década de 1930, también se hace necesario destacar el "Edificio de cinco plantas de la Calle Trapería" del famoso arquitecto Pedro Muguruza, construido en 1935 en una línea de regionalismo castizo, siendo del mismo autor el antiguo Edificio de Correos (1928-1931). Dentro del racionalismo también se encuentra la "Casa de seis plantas de la Calle Trapería" de José Luis de León y Díaz-Capilla (1934-1941) -del que también es el edificio llamado ""El Acorazado"" de la plaza Santo Domingo (1934-35)- o el Edificio Coy, de Gaspar Blein (1935).

Varios puentes de diversos estilos atraviesan el río Segura a su paso por Murcia, desde el más antiguo (del siglo XVIII) hasta varios de reciente creación de afamados autores contemporáneos.









También resulta de especial interés como elemento arquitectónico relacionado con el río Segura el popular Paseo del Malecón, antiguo muro de defensa contra las inundaciones y avenidas de agua, de origen medieval pero convertido en paseo durante el siglo XVIII.

Dentro de los principales parques y jardines de la ciudad, destacan:




En el resto del municipio son de reseñar el barroco Monasterio de los Jerónimos (en la pedanía de Guadalupe), y sobre todo el Santuario de la Fuensanta (en la pedanía de Algezares), donde se venera a la Virgen de la Fuensanta, patrona de la ciudad. Junto al Santuario, un amplio mirador nos permite contemplar una panorámica que abarca toda la ciudad, la vega del Segura y las sierras que la rodean. El Santuario está dentro del parque regional de Carrascoy y El Valle. Este parque, situado a menos de 5 km del casco urbano tiene una elevación máxima de 1065 metros. En El Valle se pueden practicar la escalada y el senderismo y realizar varias visitas tomando como punto de partida el Centro de Visitantes de La Luz. En él podremos conocer la flora y la fauna del lugar, la historia de los monasterios de la zona (San Antonio el Pobre, el Eremitorio de la Luz, Santa Catalina del Monte), así como de los yacimientos y restos arqueológicos de época íbera que dispone el parque: el Santuario ibérico de la Luz; visitable en la actualidad, y la necrópolis del Cabecico del Tesoro. En las mismas faldas de la sierra también se encuentran restos de época tardo-romana-visigoda, como el Martyrium de La Alberca o la Basílica del Llano del Olivar. Y también los restos de varios castillos musulmanes como el Castillo de la Luz, el Castillo de la Asomada, y el Castillo del Portazgo.

En la parte norte de la ciudad, en la pedanía de Monteagudo, se encuentra el Centro de Visitantes de San Cayetano, sito sobre el yacimiento del mismo nombre formado por un poblado de la Cultura del Argar y un posterior asentamiento íbero además de restos romanos.
También destacan en esta pedanía los restos del llamado Castillejo (palacio de Ibn Mardanis) y del Castillo de Monteagudo, también de época andalusí, coronado por un gran Cristo construido en 1951. De orígenes árabes es la noria hidráulica de La Ñora, muestra de la explotación milenaria de la huerta de Murcia, cuya red de riego comienza en la antiquísima Contraparada (situada entre las pedanías de Javalí Nuevo y Javalí Viejo).

En el ámbito cultural de la ciudad, destaca la existencia de dos universidades, una pública y otra privada; numerosos museos, cines y teatros, sus concurridas fiestas populares e importantes festivales.

En el término municipal de Murcia hay varios teatros, entre los que destaca el Teatro Romea. Situado en la plaza de Julián Romea, fue inaugurado en 1862 por la reina Isabel II. Llamado inicialmente "Teatro de los Infantes" y luego "Teatro de la Soberanía Popular", para finalmente adoptar su nombre actual en honor al actor murciano Julián Romea.

Otros espacios escénicos destacados son el Teatro Circo, inaugurado en 1892 y que tras décadas de abandono ha vuelto a abrir sus puertas tras una profunda rehabilitación; el Auditorio y Centro de Congresos Víctor Villegas inaugurado en 1995; y el Teatro Bernal situado en la pedanía de El Palmar.

La Red de Auditorios Municipales cuenta con el Auditorio de Beniaján, el Auditorio de Cabezo de Torres y el Auditorio de La Alberca, con una amplia oferta de teatro y música; a ellos hay que agregar otros dos auditorios de reciente construcción: el de Algezares y el de Guadalupe.

La ciudad ha visto cómo han ido desapareciendo los cines de barrio y los situados en el centro, hasta el punto de que desde 2006 sólo permanecían abiertos al público en esta zona el cine Rex, los cines Centrofama y la Filmoteca Regional de Murcia "Francisco Rabal" (antiguo cine Salzillo).

Años antes abrieron los primeros multicines, los de Atalayas y Zig Zag, ambos ya cerrados. En 2006 se inauguraron unos de última generación en los complejos comerciales Nueva Condomina y Thader, al norte de la ciudad. En este último se abrió la sala Xpand 6D, la primera en España que combinaba la proyección en 3D con butacas móviles SFX y efectos de lluvia, viento, luz, niebla y olor. Posteriormente se abrieron otros multicines en el centro comercial El Tiro.

La ciudad de Murcia tiene dos universidades: la Universidad de Murcia (UMU) y la Universidad Católica San Antonio de Murcia (UCAM).

La Universidad de Murcia es una universidad pública. Tiene su origen en 1272, aunque su fundación definitiva fue en 1914. Está formada por cinco campus de los cuales tres se encuentran en el municipio: el de la Merced, en el casco urbano; el de Espinardo; y el de Ciencias de la Salud en El Palmar. Hay un cuarto campus en el municipio de San Javier y un quinto en Lorca. En la institución estudian unos 38 000 alumnos.

La UCAM es una universidad privada católica fundada en 1996. Tiene su campus en el Monasterio de los Jerónimos en la pedanía de Guadalupe.

Dentro de la educación superior, la capital murciana alberga el Conservatorio Superior de Música de Murcia y la Escuela Superior de Arte Dramático y Danza de Murcia (ESAD).

Murcia cuenta con una gran cantidad de museos, muchos de ellos reformados recientemente, como el Salzillo, el Bellas Artes o el Museo Arqueológico. A destacar el Museo de Santa Clara, inaugurado en 2005 en el interior del conjunto monumental del Monasterio del mismo nombre.

Los diferentes museos de la ciudad de Murcia son:

Murcia acoge la celebración de varios festivales de distintos ámbitos culturales:





La Virgen de la Fuensanta es la patrona principal de la ciudad de Murcia. Constituye una de las advocaciones marianas más relevantes del levante peninsular. Es protagonista de dos romerías al año que trasladan a la imagen desde su Santuario a la Catedral y otras dos de vuelta que se desarrollan de cara a las festividades importantes que tienen lugar en la ciudad, en primavera (Semana Santa y Fiestas de Primavera) y septiembre (la Feria de Septiembre), siendo esta última la más multitudinaria.

El dialecto murciano es el dialecto romance tradicional e histórico de la Región de Murcia que tiene sus orígenes en el Reino de Murcia en los siglos XIII y XIV cuando diversas variantes lingüísticas (romance andalusí, árabe, castellano, catalán, aragonés, etc) se fundieron para dar lugar al dialecto murciano.

Algunos lingüistas lo clasifican como dialecto del español, por otro lado la RAE hasta hace poco consideraba al murciano como un dialecto del aragonés, mientras que fuentes catalanas sostienen que el murciano es un dialecto de transición entre el castellano y el catalán.

La variante comarcal del dialecto murciano, típica de los valles del Segura, es la que popularmente se ha venido denominando como "panocho", de la que participa el municipio de Murcia a pesar de la estandarización creciente del castellano normativo que se habla en él.

Los principales hospitales del municipio de Murcia son los siguientes:

Desde abril de 2015 existe MuyBici, un Sistema de Transporte Público en bicicleta que persigue fomentar el uso de la misma como medio de transporte eficiente y saludable en la ciudad, contribuyendo a que los desplazamientos sean más sostenibles. Este es un Plan Experimental puesto en marcha por el Ayuntamiento para lograr la reducción del uso del vehículo privado y potenciar otros medios de transporte no motorizados, como el transporte público, el desplazamiento a pie o la propia bicicleta. No es un sistema público de alquiler de bicicletas para uso turístico o recreativo. Es un complemento al transporte público. Su finalidad es cubrir los pequeños desplazamiento que a diario se producen por dentro de la ciudad.

El anterior Alcalde de Murcia, Miguel Ángel Cámara, firmó un Protocolo de Actuaciones con el presidente de Iberdrola, Ignacio Sánchez Galán, para la ejecución del proyecto piloto del vehículo eléctrico, que convertirá a Murcia en referente en la implantación de este medio de transporte. Asimismo, el Ayuntamiento, a través de ALEM, se encargará de elaborar un plan que contemple la previsión de la demanda de carga y la disponibilidad de puntos y tipos de recarga en los espacios públicos. La fiscalidad municipal recoge desde 2008 una bonificación del 30% en el impuesto de vehículos durante los tres primeros años para los coches eléctricos o con bajas emisiones contaminantes. El ayuntamiento está adquiriendo 8 coches eléctricos que se destinarán a varios servicios municipales. Por otra parte, la licitación del servicio de limpieza viaria y recogida de residuos recogerá en el pliego de condiciones la obligación de incorporar a la flota vehículos eléctricos

Existen diversos medios de transporte público disponibles en la ciudad y en el municipio de Murcia.

Murcia cuenta con una estación de autobuses situada en el barrio de San Andrés. Dispone conexiones a diversos destinos regionales, nacionales e internacionales.

Las líneas de autobús en el municipio están divididas en dos concesiones: una para las líneas urbanas (gestionada por el Ayuntamiento) y otra para líneas interurbanas (gestionada por el gobierno regional).

La empresa concesionaria de las líneas de transporte de viajeros por carretera en el ámbito de la ciudad es Transportes de Murcia, una UTE entre Martín, Ruiz y Fernanbús (Grupo Transvía). Los vehículos son de color grana, por lo que se les conoce coloquialmente como 'los coloraos'.

Actualmente prestan servicio 10 líneas:

La empresa concesionaria de las líneas interurbanas (hacia las pedanías o barriadas y otros municipios cercanos) es LAT. Opera 35 líneas y cuenta con una flota de más de 100 unidades.

En relación con la intermodalidad, el servicio público de alquiler de bicicletas de Murcia no posibilita la intermodalidad. Por otro lado, el Plan de Movilidad Urbana Sostenible del Ayuntamiento de Murcia no contempla la implantación de portabicicletas, la subida de bicicletas plegables, la conexión con el autobús y las tarjetas que integran los diversos servicios de movilidad (tren cercanías, autobús regional, autobús urbano, alquiler de bicicletas), como ya ocurre en otras comunidades autónomas como Asturias.

A diferencia de otros municipios, aún no se ha puesto en marcha ningún plan piloto de autobús eléctrico.

Hasta 2012 existió la Entidad Pública del Transporte (EPT). Fue pionera a nivel nacional al poner en marcha un nuevo sistema de pago en la flota pública de autobuses con el teléfono móvil que permitía sustituir al bonobús, y que empleaba la tecnología NFC (Near Field Communication) para poder, además, recargar o consultar el saldo, así como conocer los últimos movimientos.

Murcia cuenta con una línea de tranvía metropolitano y una lanzadera de la misma (línea 1B) ambas inauguradas en mayo de 2011, cuatro años después del estreno de un tramo experimental que recorría la Avenida Juan Carlos I.

La línea actualmente en servicio une la Plaza Circular con los campus de las universidades UMU y UCAM, en Espinardo y Guadalupe respectivamente, y varios centros comerciales y el Estadio Nueva Condomina situados entre las pedanías de Churra y Cabezo de Torres, formando una gran V en el norte de Murcia. Esta línea forma parte de un proyecto que incluye otras tres líneas por construir.

La lanzadera o línea 1B parte desde la parada de Los Rectores-Terra Natura y termina su recorrido en la parada UCAM-Los Jerónimos.

Murcia cuenta con una estación de ferrocarril, denominada "Murcia del Carmen" y situada en el barrio del mismo nombre.

Varias líneas de largo recorrido comunican la ciudad con Madrid, vía Albacete, así como con la Comunidad Valenciana, Cataluña y Aragón. Los trenes que prestan estos servicios son trenes Alvia, Altaria, Talgo, Media Distancia e Intercity. "Murcia del Carmen" es asimismo el centro de una red de Cercanías, denominada Cercanías Murcia/Alicante. La línea C-1 conecta la ciudad con Alicante a través de Orihuela y Elche, y la C-2 la une con Lorca y Águilas a través de Alcantarilla y Totana, atravesando una parte de la provincia de Almería en el ramal que lleva a la localidad costera de Águilas. 

También cuenta con una línea Regional que la enlaza con Cartagena, y otra de Media Distancia que llega hasta Valencia y Zaragoza.
Hasta el año 1985 existía una conexión directa con la ciudad de Granada y Andalucía. Era el llamado ferrocarril del Almanzora. En los últimos años debido al nuevo auge del transporte por ferrocarril existen planes para volver a poner en funcionamiento la línea, conectando así el arco mediterráneo con el puerto de Algeciras.

Están en fase de ejecución varias líneas ferroviarias de alta velocidad que integrarán a Murcia en dos grandes líneas de alta velocidad: el Corredor Mediterráneo que se extenderá desde la frontera francesa hasta Cádiz y Algeciras; y la Línea de alta velocidad Madrid-Levante que unirá la capital española con Valencia, Alicante, Murcia, Cartagena y Almería. El proyecto incluye el de las vías a su paso por la ciudad.

El municipio también cuenta con la Estación de Murcia-Mercancías, en la pedanía de Nonduermas, siendo la terminal de carga para mercancías de Renfe, contando también con unos talleres ferroviarios y con la Aduana de mercancías.

En la ciudad existía otra estación (esta de tipo término), la de Murcia-Zaraiche, también denominada "Estación de Caravaca", cabeza del antiguo ferrocarril Murcia-Caravaca, hoy desmantelado. El antiguo edificio de la estación es hoy la sede de la empresa municipal Aguas de Murcia, situado en la Plaza Circular.

El Aeropuerto Internacional de la Región de Murcia , también conocido como "Aeropuerto de Corvera", es el aeropuerto que da servicio al municipio y a la Región de Murcia, situado entre la pedanía del mismo nombre y la de Valladolises, a 26 kilómetros del casco urbano de la ciudad. Desde enero de 2019, fecha de su inauguración, es el único existente que opera vuelos civiles tras quedar para uso exclusivamente militar el Aeropuerto de Murcia-San Javier, operativo durante cincuenta años. El aeródromo posee varias rutas internacionales con diversos países de Europa tanto en vuelos regulares, ya sean estacionales o anuales, como de vuelos chárter, contando también con varias rutas nacionales. Su principal competidor es el Aeropuerto de Alicante-Elche, de la red de aeropuertos de Aena y situado a 72,4 kilómetros de Murcia. 

Al aeropuerto se accede desde la capital principalmente por la autovía A-30 dirección Cartagena, tomando la salida 161 dirección aeropuerto.

Las siguientes autovías tienen parte de su trazado en el municipio de Murcia:

Las siguientes carreteras nacionales tienen parte de su trazado en municipio de Murcia:

Murcia cuenta con una Agencia Local de la Energía y el Cambio Climático. En el marco de la labor que ejerce la Agencia, el Ayuntamiento va a desarrollar la Estrategia Local Contra el Cambio Climático del municipio.

La nueva Ordenanza Municipal de Captación Solar establece que los nuevos edificios, a aquellos edificios que se rehabiliten y a las piscinas de nueva construcción o aquellas ya construidas que deseen instalar un sistema de climatización de agua, estarán obligados a que el agua caliente provenga de energía solar y, como novedad, se incluye también a los bajos comerciales, algo que no recoge el Código Técnico de la Edificación. Uno de los artículos de la Ordenanza Municipal de Captación Solar trata sobre la protección del paisaje y obliga a adoptar medidas que atenúen al máximo el impacto visual de las placas solares, consiguiendo la adecuada integración al paisaje.

Murcia pertenece a la Red Española de Ciudades por el Clima.

Por otro lado, existe un proyecto para que las nuevas farolas instaladas en la ciudad funcionen con energía solar fotovoltaica se están instalando marquesinas con placas solares en su techo para su alumbrado nocturno sin consumo de la red de energía eléctrica

El Ayuntamiento ha sacado a concurso tres parcelas de propiedad municipal, con el objetivo de que los adjudicatarios instalen plantas de energía solar. Las dimensiones de las parcelas son: Gea y Truyols con 5,6 hectáreas, La Peraleja en Sucina cuenta con 43 hectáreas y la finca El Escobar en Jerónimo y Avileses con 100 hectáreas.


La ciudad cuenta con uno de los equipos históricos del fútbol español, el ya centenario Real Murcia. Juega en el estadio de fútbol de propiedad municipal de la Nueva Condomina.




En 2006, Murcia fue sede de dos campeonatos del mundo:

En 2001, Murcia fue sede del VI Festival Olímpico de la Juventud Europea, donde participaron 2500 jóvenes deportistas sub-18 de 46 países europeos y compitieron en diez modalidades deportivas diferentes. Murcia fue elegida por el Comité Olímpico Europeo en una reunión celebrada en Estocolmo, en noviembre de 1997. La ceremonia inaugural del Festival se celebró en la gran plaza central del antiguo Cuartel de Artillería el día 22 de julio de 2001.

En Murcia se encuentra el parque temático Terra Natura, que recrea el hábitat de África y la península ibérica. Abrió sus puertas en 2007 y cuenta con el primer parque acuático de la Región de Murcia.

Otro centro de ocio importante es el ZigZag City, un espacio al aire libre con una gran variedad de restaurantes, pubs y discotecas.

Entre las zonas de tapas de la ciudad cabe destacar la plaza de las Flores y su entorno, la plaza Cardenal Belluga, el área de la plaza de San Juan y alrededores, así como la zona de la plaza de Europa y calles adyacentes al campus de la Merced.

Murcia goza además de una amplia variedad de lugares de ocio nocturno. Son de destacar la zona de "Las Tascas", situada entre los céntricos barrios de San Lorenzo, Santa Eulalia y San Juan, así como "Atalayas" y "Mariano Rojas" en la periferia.

A lo largo de su historia son numerosas las personas ilustres que nacieron en Murcia o que por su especial vinculación con la ciudad pueden considerarse murcianos de adopción. De todos ellos la figura más relevante es probablemente Ibn Arabi (1165-1240), llamado también Abenarabi, místico sufí, poeta, filósofo, viajero y sabio andalusí. Ibn Arabi es una figura de nivel mundial en el ámbito del misticismo, especialmente musulmán.

En cuanto a las figuras de talla nacional, se puede considerar al rey Alfonso X de Castilla y León (1221-1284), llamado "el Sabio", como murciano de adopción ya que su vinculación con la ciudad llegó al extremo de pedir que su corazón y sus entrañas descansaran en la capital del reino de Murcia. Luis Antonio de Belluga y Moncada, más conocido como el cardenal Belluga (1662-1743), fue obispo de Cartagena y virrey de Murcia y Valencia durante el reinado de Felipe V. Aunque nacido en Motril, Belluga desarrolló la parte más intensa de su vida pública en Murcia, dejando una honda huella en la ciudad y sus alrededores. Murciano de nacimiento fue José Moñino y Redondo, conde de Floridablanca (1728-1808), prominente estadista que ocupó diversos cargos durante los reinados de Carlos III y Carlos IV, además de ser el primer presidente de la Junta Central Suprema durante la Guerra de la Independencia. Floridablanca fue un gran modernizador y benefactor de su ciudad natal. Bajo su dirección se realizó entre 1785 y 1787 el llamado "Censo de Floridablanca", primer censo de población realizado en España con técnicas estadísticas modernas.

Otros murcianos ilustres son: Ibn Sabin al-Mursí (1217-1270), maestro sufí y filósofo; Abu al-Abbas al-Mursi (1219-1287), maestro sufí que da nombre a la mezquita más importante de Alejandría (Egipto); Ibn Razin al-Tuyibi (1227-1293), poeta y gastrónomo; Luis Fajardo de la Cueva (1509-1574), noble, político y militar; Andrés de Claramonte (1560-1626), dramaturgo y actor; Pedro Orrente (1580-1645), pintor barroco difusor del naturalismo italiano en España; Diego de Saavedra Fajardo (1584-1648), escritor y diplomático de Felipe IV; Salvador Jacinto Polo de Medina (1603-1676), escritor y poeta; Diego Mateo Zapata (1664-1745), médico y filósofo; Francisco Salzillo (1707-1783), escultor e imaginero, el más representativo del XVIII español y creador de la escuela murciana de escultura; Roque López (1747-1811), escultor e imaginero, discípulo del anterior; Diego Clemencín (1765-1834), escritor y ministro liberal; Juan Palarea y Blanes (1780-1842), guerrillero y militar liberal; Julián Romea (1813-1868), actor teatral romántico; Mariano Benavente (1818-1885), médico pediatra; Antonete Gálvez (1819-1898), político y revolucionario cantonal; Manuel Fernández Caballero (1835-1906), compositor de zarzuelas; Mariano Padilla y Ramos (1842-1906), barítono; José Martínez Tornel (1845-1916), periodista y escritor; Antonio García Alix (1852-1911), político conservador y ministro; Fernando Díaz de Mendoza y Aguado, (1862-1930), actor y empresario teatral; Mariano Ruiz-Funes (1889-1953), jurista, político republicano y ministro; José Pérez Mateos (1884-1956) médico otorrinolaringólogo y Presidente de la Organización Médica Colegial de España; Juan de la Cierva y Codorníu (1895-1936), ingeniero e inventor, creador del autogiro y Pedro Flores (1897-1967), pintor vanguardista.

De los nacidos en el siglo XX la lista de murcianos ilustres se amplia con Juan González Moreno (1908-1996), escultor e imaginero; Ramón Gaya (1910-2005), pintor y escritor, Premio Nacional de Artes Plásticas; Jaime Campmany (1925-2005), periodista, novelista y poeta satírico; Eloy Sánchez Rosillo (1948-), poeta; Luis del Rivero (1950-), empresario presidente del grupo Sacyr Vallehermoso; Juan del Olmo (1958-), juez; Jerónimo Tristante (1969-), novelista; Alejandro Valverde (1980-), ciclista campeón del mundo 2018, vencedor en la general del circuito UCI ProTour 2006 y 2008 y de la Vuelta Ciclista a España 2009; Nicolás Almagro (1985-), tenista profesional, ganador de siete torneos de la ATP; Miguel Maldonado (1986-), comediante; Miguel Ángel López (1988-), campeón del mundo en en 2015; Laura Gil (1992-), jugadora de baloncesto, medallista olímpica en los , bronce en el Mundial 2018 y ganadora de dos Eurobasket, y los grupos musicales M Clan, Second, Maldita Nerea, Varry Brava, Funambulista, Klaus & Kinski, Viva Suecia o el cantante Muerdo.

Las ciudades hermanas de Murcia son:




</doc>
<doc id="4701" url="https://es.wikipedia.org/wiki?curid=4701" title="Turbocompresor">
Turbocompresor

Un turbocompresor o también llamado turbo es un sistema de sobrealimentación que usa una turbina centrífuga para accionar mediante un eje coaxial con ella, un compresor centrífugo para comprimir gases. Este tipo de sistemas se suele utilizar en motores de combustión interna alternativos, especialmente en los motores diésel.

En algunos países, la carga impositiva sobre los automóviles depende de la cilindrada del motor. Como un motor con turbocompresor tiene una mayor potencia máxima que otro de la misma cilindrada, un modelo turboalimentado pagaría menos impuestos que un motor sin turbocompresor de la misma potencia.


En los motores sobrealimentados mediante este sistema, el turbocompresor consiste en una turbina accionada por los gases de escape del motor de explosión, en cuyo eje se fija un compresor centrífugo que toma el aire a presión atmosférica después de pasar por el filtro de aire y lo comprime para introducirlo en los cilindros a mayor presión que la atmosférica.

Los gases de escape inciden radialmente en la turbina, saliendo axialmente, después de ceder gran parte de su energía interna (mecánica + térmica) a la misma.

El aire entra al compresor axialmente, saliendo radialmente, con el efecto secundario negativo de un aumento de la temperatura más o menos considerable. Este efecto se contrarresta en gran medida con un enfriador (intercooler).

Este aumento de la presión consigue introducir en el cilindro una mayor cantidad de oxígeno (masa) que la masa normal que el cilindro aspiraría a presión atmosférica, obteniéndose más par motor en cada carrera útil (carrera de expansión) y por lo tanto más potencia que un motor atmosférico de igual cilindrada, y con un incremento de consumo proporcional al aumento de masa de aire en el motor de gasolina. En los diésel la masa de aire no es proporcional al caudal de combustible, siempre entra aire en exceso al ser por inyección el suministro de combustible al cilindro, por ello es en este tipo de motores en donde se ha encontrado su máxima aplicación (motor turbodiésel).

Los turbocompresores más pequeños y de presión de soplado más baja ejercen una presión máxima de 0,25 bar (3,625 psi), mientras que los más grandes alcanzan los 1,5 bar (21,75 psi). En motores de competición se llega a presiones de 3 y 8 bares dependiendo de si el motor es gasolina o diésel.

Como la energía utilizada para comprimir el aire de admisión proviene de los gases de escape, que se desecharía en un motor atmosférico, no resta potencia al motor cuando el turbocompresor está trabajando, tampoco provoca pérdidas fuera del rango de trabajo del turbo, a diferencia de otros compresores de admisión, como los sistemas con compresor mecánico (volumétrico), en donde el compresor es accionado por una polea conectada al cigüeñal.

En los motores diésel el turbocompresor está más difundido debido a que un motor diésel trabaja con exceso de aire al no haber mariposa, por una parte; esto significa que a igual cilindrada unitaria e igual régimen motor (rpm) entra mucho más aire en un cilindro diésel.

Por otra parte, y esto es lo más importante, las presiones alcanzadas al final de la carrera de compresión y sobre todo durante la carrera de trabajo son mucho mayores (40 a 80 bares) que en el motor de ciclo Otto (motor de gasolina) (15-25 bares). Esta alta presión, necesaria para alcanzar la alta temperatura requerida para la auto-inflamación o auto-ignición del gasóleo, es el origen de que la fuerza de los gases de escape, a igual régimen, cilindrada unitaria y carga requerida al motor sea mucho mayor en el diésel que en la gasolina.

En épocas recientes la sobrealimentación en motores a gasolina se ha visto más difundida como una técnica para sacar provecho de los motores de baja cilindrada. Esto con el fin de no mermar el desempeño a raíz de las exigencias de consumos más reducidos. Casi siempre es similar el funcionamiento que en los motores diésel, sin embargo aquí la sobrealimentación juega un papel muy importante debido a que debe ser realizada de manera precisa con cantidades exactas con márgenes de error de +/- 0.50 cm/3 , en este caso al haber una mariposa en el múltiple de admisión de aire, se debe regular la proporción de aire y combustible en el sistema de inyección, así como calcular el valor de la relación de compresión con el fin de maximizar el desempeño y mejorar el consumo. Indirectamente estos motores pueden funcionar a mayor altitud sin tener una merma significativa de potencia.

Asimismo se requiere calibrar el momento de la actuación del turbocompresor debido al retardo de este mismo (turbo-lag). Generalmente esto se da porque la actuación del mismo depende de la velocidad a la que se expulsan los gases de escape, los cuales a su vez dependen de las RPM del mismo motor, casi siempre el mismo tendrá un desempeño óptimo en regímenes de rango medio (de 3000 a 5000 rpm), a su vez también esto depende de la presión de soplado del mismo, que en automóviles comunes casi siempre es calibrada en unos pocos bares o psi, mientras que en vehículos de competencia siempre dependerán de más PSI o Bares debido a las exigencias mayores las cuales pueden variar. Por ejemplo, los vehículos de rally en ocasiones deben depender de placas restrictoras en el mismo turbo para mantener una cifra de potencia pareja, además de mecanismos especiales que mantengan el mismo girando a tope sin importar el ralentí o la carrera del acelerador, con el fin de que se tenga la potencia necesaria tanto en HP, como en Torque (par) lo cual a su vez causa esas llamativas llamaradas y explosiones de los mismos vehículos así como su tono característico de motor.

Su funcionamiento se percibe con un silbido agudo que indica que la misma parte principal está girando de acuerdo a la velocidad de los gases de escape, a su vez en algunos motores al dejar de acelerar se puede distinguir un siseo similar al de los frenos de aire de un camión, indicación de que el turbo vuelve a un giro lento acorde al ralentí del motor.

Entre las primeras marcas que implementaron turbocompresores en motores de reducida cilindrada de manera más frecuente al principio del siglo XXI fueron las pertenecientes al Grupo Volkswagen posteriormente desarrollaron sistemas que implementarían la combinación de la carga estratificada de combustible y a su vez una combinación de turbocompresor y supercargador que permite obtener una potencia relativamente alta sin sacrificar el consumo de combustible, pues el segundo puede funcionar al principio ya que se impulsa por el mismo motor. 

Posteriormente, más marcas automotrices se sumaron al concepto, entre ellas Ford, quienes desarrollaron para la mayoría de sus motores tanto grandes como pequeños y en casi todos sus modelos los llamados Motores Ecoboost esto con el mismo fin de obtener más potencia sin gastar más combustible del necesario a la vez que se reducen las emisiones.

El aire, al ser comprimido, se calienta y pierde densidad; es decir, en un mismo volumen tenemos menos masa de aire, por lo que es capaz de quemar menos combustible y, en consecuencia, se genera menos potencia. Además, al aumentar la temperatura de admisión aumenta el peligro de detonación, picado, o autoencendido y se reduce la vida útil de muchos componentes por exceso de temperatura, y sobreesfuerzos del grupo térmico.

Para disminuir esta problemática se interpone entre el turbocompresor y la admisión un "intercambiador de calor" o "intercooler". Este sistema reduce la temperatura del aire, con lo que se aumenta la densidad de este, que se introduce en la cámara de combustión.

En el lado negativo, los intercambiadores de calor provocan una caída de presión, por lo que se disminuye la densidad del aire, aunque en muchos casos es necesario instalar uno para evitar la detonación o autoignición.

Los motores provistos de turbocompresor padecen de una demora del tiempo de respuesta del turbo (conocido como ""turbolag"" coloquialmente) mayor en la disposición de la potencia que los motores atmosféricos (NA-Naturally Aspirated o Aspiración Natural) o con compresor mecánico, debido a que el rendimiento del turbocompresor depende de la presión ejercida por este. En esta demora influyen la inercia del grupo (su diámetro y peso) y el volumen del colector entre la turbina y la salida de los gases de escape del cilindro.

Un turbocargador no funciona de igual manera en los distintos regímenes de motor. A bajas revoluciones, el turbocargador no ejerce presión ya que la escasa cantidad de gases no empuja con suficiente fuerza como para generar una cantidad de inercia considerable que se considere como respuesta de turbocarga. Un turbocompresor más pequeño evita la demora en la respuesta, pero ejerce menos fuerza a altas revoluciones. Distintos fabricantes de motores han diseñado soluciones a este problema.

También Mazda, tiene un prototipo de turbo eléctrico. El sistema eléctrico del coche no puede dar suficiente caudal para el motor a altas revoluciones, pero sí a bajas; así ambos se complementan. Con baja carga y revoluciones, la ayuda eléctrica permite un rápido aumento de presión y después la turbina puede suministrar toda la potencia para comprimir el aire. Este sistema ahorra mucha más energía que combinándolo con un compresor mecánico movido por el motor.

Fiat Auto, S.P.A., anteriormente, Fiat Group Automobiles (FGA) creó y desarrolló el sistema turbo + compresor mecánico durante la década de 1980. El vehículo en el cual se desarrolló y se implantó fue en el Lancia Delta (MKI), fabricado entre los años 1985 y 1990. Alcanzando su máximo exponencial y desarrollo en el Lancia Delta Integrale WRC.

Se conoce como Overboost el periodo durante el cual el sistema produce a plena carga una presión de sobrealimentación mayor a la normal, con objetivo de aumentar el par motor. 

Actualmente este sistema, con el control electrónico adecuado, puede tener en cuenta diferentes aplicaciones.

La filosofía de aplicación de los turbocompresores ha ido cambiando: desde priorizar la potencia a altas revoluciones a priorizar que el coche responda bien en todo el régimen de giro de uso.

La válvula llamada "waste-gate" evita presiones excesivas que dañen el motor. La "waste-gate" o válvula de descarga es la que regula que cantidad de gases de escape que se fugan del caracol de escape del turbo directamente hacia el escape del vehículo mediante la apertura de la válvula, de esa forma a más gases fugados menos presión de turbo, con la válvula cerrada se alcanza la máxima presión del turbo al pasar todos los gases de escape por el caracol.

La "dump valve" o válvula de alivio (también llamada blow off) abre una fuga en el conducto de admisión cuando se deja de acelerar para que la presión generada por la enorme inercia del turbo no sature estos conductos, evitando al mismo tiempo la brusca des-aceleración de la turbina, alargando su vida útil.

Normalmente el turbocompresor suele estar refrigerado con un sistema propio por aceite que circula mientras el motor está en marcha. Si se apaga bruscamente el motor después de un uso intensivo y el turbocompresor está muy caliente, el aceite que refrigera los cojinetes del turbocompresor se queda estancado y su temperatura aumenta, con lo que se puede empezar a carbonizar, disminuyendo su capacidad lubricante y acortando la vida útil del turbocompresor.

El turbo timer es un sistema que mantiene circulando el aceite en el turbocompresor durante un lapso de tiempo después del apagado del motor. Algunos modelos funcionan con sensores que detectan la intensidad en el uso del turbocompresor para permitir la lubricación forzada del mismo por un tiempo prudencial después del apagado del motor.




</doc>
<doc id="4702" url="https://es.wikipedia.org/wiki?curid=4702" title="Motor de explosión">
Motor de explosión

Un motor de explosión es un tipo de motor de combustión interna que utiliza la explosión de un combustible, encendido de manera provocada mediante una chispa, para expandir un gas que empuja un pistón, el cual esta sujeto al cigüeñal por una biela, esta hace las veces de manivela y transforma el movimiento lineal del pistón en rotativo en el cigüeñal. El ciclo termodinámico utilizado es conocido como ciclo Otto. Existen motores de explosión de dos tiempos y de cuatro tiempos.

Este motor, también llamado motor de gasolina o motor Otto, es junto al motor diésel, el más utilizado hoy en día para mover vehículos autónomos de transporte de mercancías y personas.
El combustible que se usa tradicionalmente en un motor de explosión es la gasolina. Actualmente, algunos motores de explosión pueden funcionar también con etanol, gas natural comprimido, gas licuado del petróleo o hidrógeno, además de gasolina.

En los países como Argentina (ejemplo) se utiliza más motores a gasolina para el uso de GNC (Gas Natural Comprimido), que además de ser económico daña menos al ecosistema.


El combustible se inyecta pulverizado y mezclado con el gas (habitualmente aire u oxígeno) dentro de un cilindro. La combustión total de 1 gramo de gasolina se realizaría teóricamente con 14,7 gramos de aire, pero como es imposible realizar una mezcla perfectamente homogénea de ambos elementos se suele introducir un 10% más de aire del necesario (relación en peso 1/16), a veces se suele inyectar más o menos combustible, esto lo determina la sonda lambda (o sonda de oxígeno) la cual envía una señal a la ECU. Una vez dentro del cilindro la mezcla es comprimida. Al llegar al punto de máxima compresión (punto muerto superior o PMS) se hace saltar una chispa, producida por una bujía, que genera la explosión del combustible. Los gases encerrados en el cilindro se expanden empujando un pistón que se desliza dentro del cilindro (expansión teóricamente adiabática de los gases). La energía liberada en esta explosión es transformada en movimiento lineal del pistón, el cual, a través de una biela y el cigüeñal, es convertido en movimiento giratorio. La inercia de este movimiento giratorio hace que el motor no se detenga y que el pistón vuelva a empujar el gas, expulsándolo por la válvula correspondiente, ahora abierta. Por último el pistón retrocede de nuevo permitiendo la entrada de una nueva mezcla de combustible.

La gasolina, la cual se obtiene mediante la destilación fraccionada del petróleo, fue descubierta en 1857. Más adelante, en 1860, Jean Joseph Etienne Lenoir creó el primer motor de combustión interna quemando gas dentro de un cilindro. Pero habría que esperar hasta 1876 para que Nikolaus August Otto construyera el primer motor de gasolina de la historia, de cuatro tiempos, que fue la base para todos los motores posteriores de combustión interna. En 1886 Karl Benz comienza a utilizar motores de gasolina en sus primeros prototipos de automóviles.


De interés relacionado:
Control de autoridades}}


</doc>
<doc id="4703" url="https://es.wikipedia.org/wiki?curid=4703" title="Rudolf Diesel">
Rudolf Diesel

Rudolf Christian Karl Diesel (París, 18 de marzo de 1858 – Canal de la Mancha, 29 o 30 de septiembre de 1913) fue un ingeniero alemán, inventor del carburante diésel y del motor de combustión de alto rendimiento que lleva su nombre, el motor diésel.

Diesel nació en París en 1858, segundo de los tres hijos de Elise Strobel y Theodor Diesel. Sus padres eran inmigrantes bávaros asentados en París. En 1870 la familia tuvo que abandonar Francia al estallar la guerra franco-prusiana, y Rudolf fue enviado a Augsburgo. Luego regresó a París como representante de la empresa de máquinas frigoríficas de su maestro.

Entre 1893 y 1897 construyó en los talleres de la compañía MAN AG, perteneciente al grupo empresarial alemán Krupp, el primer motor del mundo que quemaba aceite vegetal (aceite de palma) en condiciones de trabajo. Éste fue presentado en la feria internacional de París y posteriormente fue llamado con el apellido de su inventor.

El Instituto de Ingenieros Mecánicos le concedió la Orden del Mérito por sus investigaciones y desarrollos sobre los motores con aceite de maní (cacahuete), que posteriormente usaron petróleo por ser un combustible más económico.

Se consideraba así mismo como un filósofo social, aunque de su libro "Solidarismus", donde describe su visión de la empresa, solamente se vendieron 200 ejemplares.

Se supone que murió ahogado por noche del 29 al 30 de septiembre de 1913, pues desapareció del buque que cubría el trayecto de Amberes a Inglaterra en el que viajaba. Un par de días después, un bote de la guardia costera encontró su cuerpo. Como era lo común en aquel entonces, sólo se tomaron sus pertenencias (identificadas posteriormente por su hijo) y el cuerpo fue arrojado de nuevo al mar. La inexistencia de una nota o carta de suicidio ha inducido a pensar que podría haberse tratado de un accidente: Diesel, víctima de frecuentes dolores de cabeza, tal vez habría salido a pasear a cubierta, y caído al agua en un descuido. Sin embargo, también es cierto que no se puede descartar totalmente el suicidio, dado que su situación económica entonces era desesperada, pues se encontraba casi en quiebra. Existe la hipótesis de que agentes alemanes lo asesinaran para evitar la difusión de sus inventos, todavía no se ha comprobado que esto sea real, pero continúa siendo un misterio su muerte.

Después de la muerte de Diesel, el motor diésel experimentó mucho desarrollo y se convirtió en un reemplazo muy importante para el motor de pistón de vapor en muchas aplicaciones. Debido a que el motor diésel requería una construcción más pesada, más robusta que un motor de gasolina, no era ampliamente utilizado en la aviación. El motor diésel se generalizó en muchas otras aplicaciones, sin embargo, tales como motores estacionarios, máquinas agrícolas, submarinos, barcos, y mucho más tarde, locomotoras, camiones y en automóviles modernos.

Los motores diésel se encuentran con mayor frecuencia en aplicaciones en las que existe un alto requerimiento de par y un bajo requerimiento de RPM. Debido a su construcción generalmente más robusta y alto par, los motores diésel también se han convertido en los caballos de carga de la industria del camión. Recientemente, los motores diésel que han superado su peso han sido diseñados, certificados y volados en aviones ligeros. Estos motores están diseñados para funcionar con combustible diésel o más comúnmente con combustible para reactores.

El motor diésel tiene el beneficio de funcionar con más eficiencia de combustible que los motores de gasolina debido a relaciones de compresión mucho más altas y una mayor duración de la combustión, lo que significa que la temperatura aumenta más lentamente, permitiendo que más calor se convierta en trabajo mecánico. Diesel estaba interesado en utilizar el polvo de carbón o el aceite vegetal como combustible, y de hecho, su motor funcionaba con aceite de vegetal.

Aunque estos combustibles no fueron inmediatamente populares, durante 2008 los aumentos en los precios del combustible, junto con las preocupaciones sobre las reservas de petróleo, han llevado a un uso más extendido de aceite vegetal y biodiésel. La fuente primaria de combustible sigue siendo lo que se conoce como gasoil o gasóleo, un subproducto del petróleo derivado del su refinado, más seguro almacenar que la gasolina (su punto de inflamación es aproximadamente 175 grados más alto) y no explota.




</doc>
<doc id="4705" url="https://es.wikipedia.org/wiki?curid=4705" title="Telepatía">
Telepatía

La telepatía (del griego τῆλε "tēle", «lejos» y παθέειν "pathéein", 'sufrir, experimentar') consiste en la transmisión de contenidos psíquicos, entre individuos, a través de la mente sin el uso de agentes físicos conocidos. Es considerada como una forma de percepción extrasensorial o cognición anómala; además se piensa que esta es instantánea.

Aunque se han llevado a cabo muchos experimentos sobre la telepatía, su realidad no es aceptada por la gran mayoría de la comunidad científica, argumentando que las magnitudes de energía que el cerebro humano es capaz de producir resultan insuficientes para permitir la transmisión de información. No obstante, algunos investigadores señalan que, con la tecnología necesaria, en un futuro será posible interpretar las ondas cerebrales mediante algún dispositivo y enviar mensajes textuales a un receptor de manera inalámbrica. Sin embargo, descartan que este proceso pueda llevarse a cabo de cerebro a cerebro sin mediación tecnológica. Hasta la fecha, las únicas pruebas de la telepatía son las narraciones testimoniales, pues jamás se ha podido reproducir un fenómeno telepático en laboratorio. 

La telepatía es tratada frecuentemente en ufología, novelas y películas de ficción.

Se han encontrado muy pocas referencias a la telepatía en las culturas antiguas de las que se tienen registros escritos (a diferencia de, por ejemplo, la precognición, que sí aparece en muchos mitos). La noción de telepatía y las especulaciones relacionadas con ellas se hicieron frecuentes solo a partir del siglo XIX.

Se considera que la primera investigación sobre la telepatía fue la realizada por la , cuyos resultados fueron publicados en 1886 en la obra "Phantasms of the Living" (‘Fantasmas de los vivos’). Años antes, en 1882, , uno de los fundadores de la Sociedad de Investigaciones Psíquicas (SPR), introdujo, en un artículo publicado en "Proceedings of the Society for Psychical Research", el término «telepatía» (inspirado por la incipiente eclosión tecnológica de la época en que las técnicas electromagnéticas de telecomunicación reciben nombres como teléfono y telégrafo), para diferenciarlo de la falsa «lectura del pensamiento». Aunque gran parte de las investigaciones iniciales consistieron en la recopilación de relatos anecdóticos, también se llevaron a cabo experimentos con aquellos que afirmaban poseer habilidades telepáticas. Sin embargo, sus protocolos experimentales no eran muy estrictos.

En 1917 el psicólogo John E. Coover de la Universidad de Stanford dirigió una serie de pruebas sobre telepatía consistentes en transmitir y adivinar naipes. Los aciertos fueron levemente superiores a los esperados por azar, concluyéndose que el resultado había sido aleatorio.

Quizá los ejemplos más conocidos de experimentos sobre telepatía fueran los de Joseph Banks Rhine y sus asociados en la Universidad de Duke, que comenzaron en 1927 usando los distintivos «Naipes ESP» de Karl Zener (véase Cartas Zener). Estos experimentos incorporaron protocolos más rigurosos y sistemáticos que los anteriores, seleccionándose lo que se asumió que eran participantes «normales» y no aquellos que afirmaban tener habilidades excepcionales, y aplicando los nuevos avances en el campo de la estadística para evaluar los resultados. Estos y los de otros experimentos fueron publicados por Rhine en su conocido libro "Extra Sensory Perception" (‘Percepción extrasensorial’), que popularizó este término.

Otro libro influyente sobre la telepatía en su día fue "Mental Radio", publicada en 1930 por el ganador del premio Pulitzer Upton Sinclair (con prólogo de Albert Einstein). En él, Sinclair describe la aparente capacidad de su esposa de reproducir a veces los dibujos realizados por él y por otros, incluso cuando estaban separados por distancias de varias millas, en experimentos al parecer informales que recuerdan algunos de los usados por investigadores de la visión remota en épocas posteriores. En su libro, los Sinclair señalaban que los resultados podían también explicarse como una clarividencia más general, e hicieron algunos experimentos cuyos resultados sugerían que en realidad no hacía falta ningún emisor y algunos dibujos podían ser reproducidos precognitivamente.

En los años 1960, muchos parapsicólogos no estaban satisfechos con los experimentos de elección forzada de J. B. Rhine, debido en parte al aburrimiento de los participantes en las pruebas tras muchas repeticiones de adivinación monótona de naipes y al rechazo de la sugerencia de los magos de añadir naipes totalmente en blanco, y en parte por el «efecto de declive» por el que la precisión de la adivinación de cartas disminuía tras cierto tiempo para cada participante.

Algunos parapsicólogos recurrieron al formato de experimentos basados en «respuesta libre», donde el objetivo no estaba limitado a un pequeño conjunto finito predeterminado de respuestas (p. e. las cartas Zener), sino que podía consistir en su lugar en cualquier clase de cuadro, dibujo, fotografía, fragmento de película, composición musical, etcétera.

Como resultado de encuestas sobre experiencias psi espontáneas que concluían que más de la mitad de éstas sucedían en estado de sueño, los investigadores Montaque Ullman y Stanley Krippner de Maimonides Medical Center de Brooklyn (Nueva York) emprendieron una serie de experimentos para comprobar la telepatía durante el sueño. Un participante «receptor» en un cuarto insonorizado y electrónicamente blindado sería monitorizado mientras dormía en busca de patrones encefalográficos y movimientos oculares rápidos que caracterizan el estado de sueño. Un «emisor» en otra habitación intentaría entonces enviar una imagen, aleatoriamente seleccionada de un conjunto, al receptor concentrándose en dicha imagen durante los estados de sueño detectados. Cerca del final de dichos estados, el receptor sería despertado y se le pediría que describiese su sueño durante tal periodo. Los datos recogidos sugerían que algunas veces la imagen era incorporada de alguna forma en el contenido de los sueños del receptor.

Aunque los resultados de los experimentos de telepatía durante el sueño eran interesantes, llevarlos a cabo exigía muchos recursos (tiempo, esfuerzo, personal). Otros investigadores buscaron alternativas más económicas, como los llamados experimentos "ganzfeld". Hasta la fecha no ha habido ningún protocolo experimental satisfactorio diseñado para distinguir la telepatía de otras formas de percepción extrasensorial tales como la clarividencia.

La telepatía está considerada por la gran mayoría de la comunidad científica como una pseudociencia. Sus críticos objetan los experimentos con resultado positivo, diciendo que no han tenido el rigor científico adecuado. Por otro lado los miembros de los laboratorios de las universidades y asociaciones en donde sí se estudia sostienen que estos estudios tienen el rigor necesario, y que existen indicios favorables para continuar con las pruebas. Además existen argumentos evolutivos y físicos que hacen muy inverosímil la posibilidad de fenómenos telepáticos.

Un experimento típico procede como sigue:

Algunos defensores de la telepatía han usado conceptos científicos tomados de la psicología y la mecánica cuántica, de manera un tanto controvertida para explicar mecanismos reales que podrían hacer físicamente posible la telepatía. Sin embargo, al igual que sucede con la clarividencia, la telepatía presenta problemas de plausibilidad física similares. Si bien ciertas áreas de la psicología y la mecánica cuántica, siguen existiendo problemas abiertos para los que no existe una respuesta generalmente aceptada, eso no implica que dichos problemas puedan aportar ningún argumento prometedor para la plausibilidad física de la telepatía.

En un experimento realizado por investigadores de la Universidad de Mánchester se pretendía medir, mediante el uso de la realidad virtual, las capacidades telepáticas humanas. En el experimento, en el que participaron 100 voluntarios, se separaba a los participantes por parejas. Los dos miembros de cada pareja, equipados con un visor y un guante que les permite moverse e interactuar con los objetos del mundo virtual, entraban en salas separadas. A continuación se les mostraban una serie de objetos escogidos al azar (un teléfono, una trompeta, un paraguas...). Al primer participante solo se le enseñaba uno de los objetos y se le pedía que se concentrase e interactuase con él. En la segunda habitación, el otro participante ve el mismo objeto y otros tres más. Entonces debe señalar el objeto que cree que su compañero está intentando transmitirle telepáticamente. Los investigadores estaban especialmente interesados en observar en qué medida afectan los lazos familiares y otro tipo de relaciones a las capacidades telepáticas. Los responsables del experimento no creen que esta prueba sirva para demostrar la existencia o inexistencia de la telepatía, tan solo pretenden "crear un método experimental que facilite la investigación científica en esta área".

Varias de las razones, por las cuales muchos científicos han desechado la idea de la telepatía como un fenómeno viable, están en las dificultades para proponer un mecanismo físico de transmisión. Dada la escala y magnitud del cerebro, de existir señales telepáticas parece que deberían basarse, en la interacción electromagnética o más improbablemente en la interacción gravitatoria. Sin embargo, la anatomía no parece disponer de áreas diferenciadas u orgánulos capaces de producir de manera consistente ondas electromagnéticas, que pudieran ser recibidas e interpretadas por áreas anatómicas especializadas en cerebros vecinos. Todas esas razones indican que hay una ausencia de argumentos para pensar que los cerebros puedan, de alguna manera, producir señales telepáticas interceptables e interpretables por otros cerebros.

En algunas ocasiones, hay personas que se imaginan o, incluso, que se inventan transmisiones telepáticas. Creen poseer la facultad telepática sin ser verdaderamente así. En el caso de las personas que padecen de esquizofrenia en alguna ocasión pueden sentir pensamientos erróneos o sensaciones referidas a la telepatía.

La telepatía es un . Un buen número de superhéroes y supervillanos de varias novelas de ciencia ficción, etc, usan telepatía. Un notable ejemplo es la novela de Alfred Bester, "El hombre demolido" (1952), donde una comunidad de telépatas conviven con el resto de los seres humanos. Entre los telépatas más destacados se incluyen los jedis y los siths en el universo "Star Wars". Las habilidades telepáticas en la ficción varían considerablemente. Algunos telépatas ficticios solo pueden transmitir pensamientos con otros telépatas, o recibir pensamientos solo de otras personas específicas. Por ejemplo, en la novela de Robert A. Heinlein, "La hora de las estrellas" (1956), una pareja de gemelos pueden comunicarse telepáticamente, pero solo entre ellos. En la novela de ciencia ficción de A. E. van Vogt, "Slan" (1940), el héroe mutante Jommy Cross puede leer la mente de los humanos corrientes, pero no la de otros mutantes. Sookie Stackhouse, la camarera telépata de la serie de novelas "The Southern Vampire Mysteries" de Charlaine Harris, puede leer la mente de los humanos y de otros seres sobrenaturales, pero no la de los vampiros. Algunos telépatas pueden leer la mente solo si hay algún tipo de contacto físico, como los vulcanos en el universo de "Star Trek", Abe Sapien en las películas de Guillermo del Toro "Hellboy" (2004) o Aro, un vampiro de la novela "Luna nueva" de Stephenie Meyer (2006). El consultor y escritor del universo "Star Trek", André Bormanis, ha revelado que la telepatía en "Star Trek" es posible gracias a una especie de "campo psiónico"; según Bormanis, el campo psiónico es el medio por el cual los pensamientos y los sentimientos pueden ser transmitidos a través del espacio. Algunos humanoides pueden tener acceso perceptivo a dicho medio gracias a un órgano sensorial localizado en el cerebro; del mismo modo que el ojo humano puede percibir rangos dentro del campo electromagnético que los ojos de otras especies no pueden percibir, los telépatas pueden percibir el campo psiónico. Este campo es el equivalente al "plano astral" o "dimensión astral" en los cómics del Universo Marvel. En el libro "Eragon", de Christopher Paolini (2003), Eragon puede comunicarse telepáticamente con su dragona "Saphira" y con muchos otros, aunque puede bloquear los pensamientos con barreras psíquicas. En las series de novelas de "Harry Potter", de J. K. Rowling, la telepatía es una habilidad mágica conocida como legeremancia, la habilidad para bloquear los pensamientos ante hechizos de legeremancia se conoce como oclumancia. En la novela de John Wyndham "Las Crisálidas" (1955), el personaje principal y narrador, David Strorm, forma parte de un grupo de nueve telépatas, al igual que los sesenta niños de "Los cuclillos de Midwich" (1957) quienes poseen vastos poderes psíquicos y pueden comunicarse telepáticamente unos con otros, incluso con otros niños distantes y dispersos por todo el planeta. En la serie de novelas "Los guardianes" de Anthony Horowitz, los gemelos, Jamie y Scott Tyler, pueden leer y controlar las mentes de los demás, además de comunicarse entre ellos, por lo que siempre saben en qué está pensando el otro. 

Algunos escritores consideran la telepatía como un salto más en la evolución humana. En la novela de Tony Vigorito, "Just a Couple of Days" (2001), la telepatía se encuentra en todos los humanos gracias a un virus, el cual pasa inadvertido a causa de otras capacidades humanas. Por tanto, la telepatía es una habilidad latente que se puede desarrollar si se consigue eliminar otro tipo de distracciones, como la comunicación por el lenguaje.

En muchas obras de ficción, la telepatía está combinada con otra clase de poderes psíquicos, como en el caso de la novela "El Resplandor" (1977) de Stephen King, donde el niño Danny Torrance tiene poderes precognitivos y de mediumnidad además de habilidades telepáticas. Otros telépatas ficticios, poseen habilidades de control mental, incluyendo la capacidad de "implantar" pensamientos, sentimientos o visiones alucinatorias dentro de las mentes de los demás. Mediante ataques psíquicos pueden causar dolor, parálisis, desvanecimiento o incluso la muerte. Pueden alterar o borrar la memoria o controlar completamente la mente y el cuerpo de otros, similar a una "posesión espiritual". Ejemplos de este tipo de telépatas son Charles Xavier, Emma Frost, Jean Grey, Psylocke y, en general, casi todos los telépatas del universo Marvel. Otro ejemplo son los telépatas de la serie de televisión Héroes, como Matt Parkman entre otros. También los telépatas más poderosos dentro del mundo ficticio de Babylon 5 pueden desarrollar este tipo de habilidades, como Lyta Alexander o Al Bester.

El justiciero La Sombra tiene la habilidad de "nublar las mentes de los demás", el cual utiliza para ocultar su presencia frente a los demás.

La serie de películas de "Scanners" (Exploradores) trata de un grupo de personas que nacieron con vastos poderes telepáticos, al igual que ciertas habilidades psicoquinéticas. En el primer film de la serie, el Doctor Paul Ruth (interpretado por Patrick McGoohan) explica que el fenómeno telepático no es la transferencia de pensamiento, sino el encuentro entre los sistemas nerviosos, permitiendo a los telépatas (o "exploradores" como los definen en la película) acceder al sistema nervioso (y por tanto a los pensamientos) de los demás. Los exploradores más poderosos pueden, además, controlar y manipular el sistema nervioso ajeno. Estas habilidades pueden inhibirse mediante un fármaco llamado "Ephemerol" que altera la sinapsis cerebral de los exploradores bloqueando sus capacidades.

La obra "Devta" de Mohiuddin Nawab (1977), escrita en urdu, está basado en el personaje de Farhad Ali Taimur, un telépata implicado en la lucha entre el Bien y el Mal.

La película "Thoughtcrimes" de Breck Eisner (2003), narra la vida de la telépata Freya McAllister, desde sus problemáticos inicios hasta su inserción en la unidad especial de la ASN (Agencia de Seguridad Nacional).

La serie canadiense de televisión, "The Listener", narra las peripecias de Toby Logan, un paramédico telépata.

Mención especial merece la novela de Robert Silverberg "Muero por dentro" (1972), una novela introspectiva que narra la historia del telépata David Selig. Este nos cuenta los problemas y sinsabores que le ha causado su don, pero que después siente cómo "muere por dentro" al descubrir que va perdiendo progresivamente su capacidad telepática, aquella habilidad que le arruinó la vida pero que teme perder por darle una distinción especial frente a los demás.

Muchos pokémon, sobre todo legendarios, pueden hablar con los humanos mediante este método, como Lugia, Arceus, Mewtwo, , Jirachi, Shaymin, Darkrai, Reshiram, Zekrom, Keldeo, Cobalion, Terrakion, Virizion, Kyurem, Diancie o Xerneas.

En la serie de televisión estadounidense "The Tomorrow People" se sostiene que hay una nueva especie en la evolución humana que posee telequinesia y telepatía, además de tener la capacidad de teletransportarse.





</doc>
<doc id="4706" url="https://es.wikipedia.org/wiki?curid=4706" title="Pseudociencia">
Pseudociencia

La pseudociencia o seudociencia (en griego: Ψευδοεπιστήμη, romanización: Psevdoepistími; ‘falsa ciencia’) es aquella afirmación, creencia o práctica ('pseudoterapia o 'falso tratamiento') que es presentada como científica y fáctico, pero es incompatible con el método científico. A menudo se caracteriza por el uso de afirmaciones vagas, contradictorias, exageradas o infalsables; la dependencia en el sesgo de confirmación en lugar de pruebas rigurosas de refutación; poca o nula disposición por parte de sus seguidores a aceptar evaluaciones externas de expertos; y en general, la ausencia de procedimientos sistemáticos para el desarrollo racional de teorías.

Un área, práctica o cuerpo de conocimiento puede ser razonablemente llamada pseudocientífica cuando se presenta como congruente con los criterios de la investigación científica, pero manifiestamente no cumple con los requisitos de esta. La ciencia se diferencia de la revelación, la teología y la espiritualidad en que ofrece un entendimiento de la realidad mediante el conocimiento obtenido a través de la investigación y experimentación empíricas. La divulgación científica tendenciosa puede nublar las fronteras entre la ciencia y la pseudociencia del público general y puede además incluir ciencia ficción. Algunas creencias pseudocientíficas están ampliamente arraigadas, incluso entre periodistas y profesores de ciencia de escuelas laicas.

El problema de la demarcación entre ciencia y pseudociencia tiene implicaciones políticas, además de presentar problemas científicos y filosóficos. Distinguir entre ambas tiene importancia práctica en áreas como la asistencia médica, el peritaje judicial, las políticas ambientales y la educación en ciencias. Es parte de la educación y el alfabetismo científicos diferenciar los hechos y teorías científicos de las creencias pseudocientíficas, como las encontradas en la astrología, la alquimia, la charlatanería y las creencias ocultistas, que a menudo están unidas falazmente a conceptos científicos.

El término "pseudocientífico" a menudo se considera inherentemente peyorativo, debido a que sugiere que algo es presentado vaga o incluso embusteramente como ciencia cuando no lo es. En consecuencia, los seguidores de ideas categorizados como pseudocientíficas usualmente rechazan esta etiqueta.

El término "pseudociencia" se suele considerar como inherentemente negativo, ya que sugiere que algo está siendo incorrectamente presentado como ciencia, quizá incluso de forma intencionada. En consecuencia, aquellos de los que se afirma que practican o defienden pseudociencias normalmente discuten tal etiqueta pero por otro lado se encuentran miembros de la comunidad científica que cuestionan el uso peyorativo de la etiqueta como calificativo ante nuevas teorías, tesis o investigaciones.

El término "pseudociencia" o "seudociencia" es un neologismo formado a partir de la raíz griega "pseudo", «falso», y la palabra latina "ciencia", «conocimiento». Aunque el término como tal se emplea desde por lo menos finales del siglo XVIII, el concepto de "pseudociencia" como algo distinto de la ciencia real o auténtica parece haber surgido a mitad del siglo XIX. Uno de los primeros usos de la palabra "pseudociencia" proviene de 1844 en el "Northern Journal of Medicine". También se registra un uso anterior del término en 1843, en la obra del fisiólogo francés François Magendie.

Aunque los elementos que determinan si un cuerpo de conocimiento, metodología o práctica es científico pueden variar según el ámbito de actuación, existen ciertos principios generales con los que la comunidad científica se muestra en general de acuerdo. La noción básica es que todos los resultados experimentales deben ser reproducibles, y susceptibles de ser verificados por otros investigadores. Estos principios pretenden asegurar que los experimentos pueden ser reproducidos bajo las mismas condiciones, permitiendo mediante la investigación posterior determinar si una hipótesis o teoría acerca de un fenómeno es a la vez válida y fiable. Para ser considerado científico, un estudio debe aplicar el método científico en todos sus ámbitos, y el sesgo cognitivo debe ser controlado o eliminado mediante el muestreo al azar, técnicas específicas como el doble ciego, y otros métodos. Se espera que todos los datos recopilados, incluyendo especificaciones de las condiciones ambientales o experimentales, estén documentados y disponibles para su revisión por pares, permitiendo la realización de nuevos experimentos que confirmen o desmientan los resultados previos.

En general, y en la medida en que pueda resultar aplicable, la metodología científica exige que las teorías puedan someterse a pruebas empíricas rigurosas, mientras que a las pseudociencias, o bien no será posible aplicarles sistemas de refutación (por tratarse de formulaciones ambiguas), o bien sus partidarios protegerán la teoría (por ejemplo, con hipótesis auxiliares o "ad hoc", formuladas "a posteriori"), en lugar de someterla a ensayos que puedan refutarla.

Karl Popper introdujo a mediados del siglo XX el concepto de falsabilidad para distinguir la ciencia de la no-ciencia. Un resultado es "falsable" cuando puede ser demostrado como erróneo, es decir, cuando puede diseñarse un experimento teórico con el que demostrar si es falso. De este modo, las afirmaciones "falsables" pueden ser consideradas como ciencia, mientras que las no "falsables" se consideran no-ciencia. Por ejemplo, la afirmación de que "Dios creó el Universo" puede ser cierta o falsa, pero no puede diseñarse ningún experimento que demuestre una cosa u otra; simplemente está más allá de la capacidad de la ciencia, ergo, no es "falsable" y por tanto es no-ciencia. Popper usó la astrología y el psicoanálisis como ejemplos de pseudociencias, y la teoría de la relatividad de Einstein como ejemplo de ciencia. Luego clasificó las formulaciones no-científicas en las categorías filosófica, matemática, mitológica, religiosa y/o metafísica por un lado, y pseudocientífica por otro, aunque no dio criterios claros para definir cada una.

El término pseudociencia tiene connotaciones peyorativas, porque se usa para indicar que las materias así etiquetadas son errónea o engañosamente presentadas como científicas. Por este motivo, aquellos que cultivan determinada "pseudociencia", normalmente rechazan esta clasificación. El apelativo se ha aplicado a disciplinas como ciertas hipótesis de la física cuántica y otras ciencias que no utilizan el método científico rigurosamente como son las ciencias sociales, el psicoanálisis, la parapsicología y la criptozoología por la naturaleza de sus objetos de estudio difícil de aplicarle la misma rigurosidad científica que en otras disciplinas, no obstante esto es relativo y algunas de estas disciplinas acusadas de pseudocientíficas son aceptadas como científicas por universidades, asociaciones científicas, centros médicos, gobiernos, etc., por ejemplo, el psicoanálisis.

Muchas veces la discusión sobre un concepto o campo de conocimiento gira más alrededor de su consideración como ciencia o pseudociencia que acerca de los hechos y métodos reales. El filósofo de la ciencia Larry Laudan ha manifestado que el concepto "pseudociencia" no tiene significado científico y se usa mayoritariamente para describir una apreciación subjetiva: "Si quisiéramos permanecer firmes al lado de la razón, deberíamos deshacernos de términos como ‘pseudociencia’ y ‘acientífico’ de nuestro vocabulario; son solo palabras huecas que cumplen una función emotiva." Del mismo modo, Richard McNally afirma que "el término "pseudociencia" se ha convertido en poco más que una injuriosa palabra de moda para ningunear a los propios oponentes en las discusiones en los medios", y que "cuando los emprendedores terapéuticos hacen afirmaciones a favor de sus tratamientos, no deberíamos perder el tiempo intentando determinar si estos califican como pseudocientíficos. En su lugar se deberían hacer preguntas como: ¿Cómo sabe que su tratamiento funciona? ¿Cuáles son sus pruebas?".

Los autores que diferencian entre ciencias reales y pseudociencias señalan características cuya presencia simultánea, no necesariamente de todas a la vez (definición politética), ayuda a reconocer a las pseudociencias como tales:


Algunos autores afines al relativismo epistémico o al llamado «programa fuerte» (o «estándar») de la sociología de la ciencia (Barry Barnes, Steve Shapin y David Bloor), la Escuela de París, (Bruno Latour y Michael Callon), el grupo de Bath, (Harry Collins y Steven Yearley), el grupo de norteamericanos y su “Etnometodología”, (Harold Garfinkel y Michael Lynch), ponen en duda que sea posible diferenciar con rigor y objetividad el límite que demarca la "ciencia" de la "pseudociencia", respaldando en algunos casos posiciones abiertamente contrarias a determinadas concepciones de lo que es ciencia y criticando el método científico. Estas posiciones relativistas fueron contestadas por los científicos Alan Sokal y Jean Bricmont en su libro "Imposturas intelectuales" (1997).

Críticos de las pseudociencias como Richard Feynman, Richard Dawkins, Carl Sagan, Michael Shermer y Mario Bunge consideran que todas las formas de pseudociencia son dañinas, causen o no daños inmediatos a sus seguidores. Estos críticos generalmente consideran que la defensa de la pseudociencia puede suceder por varias razones, que van desde la simple candidez sobre la naturaleza de la ciencia y el método científico, hasta un engaño deliberado por beneficios económicos o políticos. No es apropiado tratar de pseudociencia cualquier cuerpo sistemático de creencias solo por no considerar veraces sus postulados, sino que solo tiene sentido hacerlo cuando desde la disciplina en cuestión se proclama sin fundamento su carácter científico. Algunos críticos de la pseudociencia consideran algunas o todas las formas de pseudociencia como pasatiempos inofensivos.

El pensamiento pseudocientífico se ha explicado en términos de psicología y psicología social. La tendencia humana a buscar confirmación en vez de refutación, la de mantenerse aferrado en las creencias confortables, y la de sobregeneralizar han sido mencionadas como razones comunes para la adherencia al pensamiento pseudocientífico. De acuerdo con Beyerstein (1991) los humanos son propensos a realizar asociaciones en función de la apariencia, y a menudo cometen errores en el pensamiento sobre causa y efecto.

Paul Feyerabend argumenta que una distinción entre ciencia y seudociencia no es ni posible ni deseable.Richard McNally, catedrático de Psicología de la universidad de Harvard, manifiesta: "El término 'pseudociencia' se ha convertido en poco más que una palabra de moda incendiaria para desacreditar rápidamente a un oponente a través de los medios de comunicación" y "Cuando los terapeutas manifiestan haber obtenido logros con sus prácticas, no deberíamos gastar nuestro tiempo en tratar de averiguar si sus prácticas se las pueden calificar de pseudocientíficas. En vez de eso, se le debería preguntar: ¿Cómo sabe usted que su práctica funciona? ¿Cuál es su evidencia?"

La protociencia engloba áreas de conocimiento en proceso de consolidación. Por ejemplo la alquimia en el siglo XVII entraba dentro de esta categoría. Cuando se descubrió que los principios en la que se basaban (como la influencia de los planetas en los metales) no tenían respaldo experimental, pasó a ser una pseudociencia. Lo mismo puede decirse de la parapsicología en el siglo XIX y principios del XX. No todas las protociencias desembocan en pseudociencias. Existen autores que consideran que la alquimia dio origen a la química y la astrología a la astronomía; aunque se debe tener en cuenta que otros historiadores de la ciencia rebaten este punto, considerando al ocultismo y a la ciencia como tradiciones paralelas.

No hay un acuerdo para la diferenciación entre protociencia, pseudociencia y ciencia. Hay ejemplos de teorías científicas vigentes que alguna vez fueron criticadas y etiquetadas como pseudocientíficas. La transición se caracteriza por una mayor investigación científica sobre el tema y el descubrimiento de más evidencias que sustenten la teoría. Así, la teoría de la deriva continental fue, en su momento, considerada pseudocientífica.

Se han hecho varios intentos para aplicar rigor filosófico a la demarcación de la ciencia con resultados diversos. Estos incluyen el criterio de falsabilidad de Karl Popper y la aproximación histórica de Imre Lakatos, quien lo critica en su "Methodology of scientific research programmes" "(Metodología de los programas de investigación científica)".
Historiadores y filósofos de la ciencia, principalmente Thomas Kuhn y Paul Feyerabend, sostienen desde otras perspectivas epistemológicas del conocimiento, que incluye la dimensión social, que no siempre es posible una distinción nítida y objetiva entre ciencia y pseudociencia.

Mario Bunge, filósofo de la ciencia, es conocido por su posición de incluir al psicoanálisis entre las pseudociencias. Críticas hacia la inconsistencia entre teoría y experiencia, o hacia el carácter especulativo del discurso se dirigen también a veces desde las ciencias naturales hacia ciertas ciencias sociales, como la economía o la psicopedagogía. El "escándalo Sokal", por el nombre del físico que lo puso en marcha, mostró que desde una cierta orientación de la Sociología de la Ciencia postmoderna también se ha recurrido a veces a usar inconsistentemente el lenguaje de las llamadas "ciencias duras", en lo que parece un intento irregular de legitimación científica, siendo esta una de las líneas de conducta frecuentemente reprochadas hacia las llamadas pseudociencias.

Para algunos sectores de la filosofía de la ciencia no existe un criterio de demarcación perfectamente delimitado, metodológico y objetivo para definir universalmente qué es ciencia y qué es pseudociencia,

Un campo en el que se usan frecuentemente alegaciones seudocientíficas es el de la curación de enfermedades.

Entre las pseudoterapias sin ningún resultado e incluso con contraindicaciones o efectos secundarios negativos que se han recomendado para curar el cáncer están la angeloterapia, biomagnetismo, bioneuroemoción o biodescodificación, constelaciones familiares, dianética, dieta alcalina, desensibilización por medio de movimientos oculares, flores de bach, limpieza de colon, homeopatía, iridología, iriogenética, homotoxicología, suplemento mineral milagroso, naturopatía, osteopatía, ozonoterapia, programación neurolingüística, psicoanálisis, quiropráctica, reflexología, acupresión, reiki, terapia gerson, terapia gestalt o terapia humanista, terapia neural, terapia ortomolecular o medicina ortomolecular, terapia quelante y el toque terapéutico. Se desaconsejan estas pseudoterapias apoyadas en pseudociencias para la cura del cáncer.

Entre las terapias pseudocientíficas más populares esta la acupuntura, a pesar de que el consenso científico es claro en mostrar la falta de efectividad por sobre el efecto placebo y la larga lista de efectos adversos publicados en revistas médicas y por la OMS incluyendo daños a órganos y tejidos e infecciones. 

Existe un importante mercado de métodos curativos y diagnósticos presentados como mecanismos curativos de validez demostrada por estudios, que en muchos casos utilizan métodos mágicos tradicionales, como la imposición de manos o procedimientos sin fundamento científico, como la radiestesia o el empleo de pirámides. La mayoría de estos curanderismos, cuya extensión creciente debe mucho a internet, busca la credibilidad y el prestigio que tiene la ciencia, alegando por ejemplo desconocidas propiedades del agua, la supuesta acción de fenómenos cuánticos, o presuntas energías de naturaleza difusa.

El cartílago de tiburón se ha promocionado falsamente como cura para el cáncer con base en una supuesta inexistencia de cánceres en tiburones. De acuerdo con Ostrander, esta práctica ha llevado a una continua disminución de las poblaciones de tiburones, y, lo que es más importante, ha alejado a los pacientes de terapias contra el cáncer que sí son efectivas. Los autores sugieren que "los mecanismos basados en la evidencia dada por la comunidad científica deberían añadirse al aprendizaje de los profesionales de los medios de comunicación y gubernamentales".

Un caso especial, por su extensión, es el de la homeopatía, cuya incongruencia con el conocimiento científico fue indicada ya en vida de su fundador, Samuel Hahnemann, y respecto a la que se han utilizado recientemente términos prestados de la mecánica cuántica (como el entrelazamiento) de manera admitidamente metafórica.

Lo mismo ocurre con la reflexología podal, llegándose incluso a impartir cursos que a veces están financiados por la administración pública sanitaria y dirigidos a matronas, personas con formación científica e inmersas en el ámbito sanitario, lo que puede confundir a la ciudadanía dando apariencia de estar avalado por la ciencia. Estos cursos, en España los imparte gente sin formación médica reglada, por lo que su credibilidad deja mucho que desear. Cualquier persona, independientemente de su formación, tiene acceso a cursos de reflexología y puede obtener un diploma que lo capacita para la práctica profesional de dicha disciplina.

Algunos tratamientos alternativos de carácter pseudocientífico han producido accidentes graves, incluso muertes; pero se admite en general que el mayor peligro para la salud de los pacientes ocurre cuando, confiando en un método ineficaz, renuncian a medidas más efectivas, como hábitos más saludables o un tratamiento médico de eficacia demostrada.

En 2018, en España, la "Asociación para Proteger al Enfermo de Terapias Pseudocientíficas" publicó una carta abierta dirigida a la ministra de Sanidad María Luisa Carceo y al público en general bajo el título "Seamos claros: las pseudociencias matan". En ella se señalaba que la muerte por el uso de terapias pseudocientíficas y por abandono de terapias "oficiales" se producían por la tolerancia legal existente hacia prácticas sanitarias ejercidas por no profesionales y fuera de los protocolos internacionales, la dejadez de las instituciones en el cumplimiento de leyes ya aprobadas, la tolerancia de los colegios médicos y la falta de información a los pacientes.

Las pseudociencias y paraciencias promueven la patologización y medicalización: intentan mostrar como problemas de salud susceptibles de tratamiento a características biológicas a comportamientos fisiológicos o vitales que no son problemas de salud.

Las pseudociencias extienden diagnósticos existentes a personas que no los padecen, establecen diagnósticos sin base científica probada (véase en psiquiatría la disputa de la controversia de la biopsiquiatría) y llegan a la creación de enfermedades sin ningún fundamento científico —pseudoenfermedades— (p.e. "síndrome del intestino permeable" o "permeabilidad intestinal aumentada") con el objeto de crear una clientela que compre sus libros y remedios.

Autores como Mario Bunge, Carl Sagan, Robert L. Park, James Randi, o Michael Shermer que defienden un criterio de demarcación estricto entre ciencia y pseudociencia, consideran que en los campos siguientes una parte significativa de sus practicantes presentan su disciplina como equivalente a campos o áreas de conocimiento riguroso, imitándolos a veces formalmente en el lenguaje o las formas de comunicación, y adoptando títulos personales científicamente prestigiosos ante el público como «doctor» o «profesor». 

Entre las consideradas pseudociencias están:



</doc>
<doc id="4707" url="https://es.wikipedia.org/wiki?curid=4707" title="Chamanismo">
Chamanismo

El chamanismo se refiere a una clase de creencias y prácticas tradicionales similares al animismo que aseguran la capacidad de diagnosticar y de curar el sufrimiento del ser humano, y en algunas sociedades, la capacidad de causarlo. Los chamanes creen lograrlo contactando con el mundo de los espíritus y formando una relación especial con ellos. Aseguran tener la capacidad de controlar el tiempo, profetizar, interpretar los sueños, usar la proyección astral y viajar a los mundos superior e inferior. Las tradiciones de chamanismo han existido en todo el mundo desde épocas prehistóricas.

Algunos especialistas en antropología definen al chamán como un intermediario entre el mundo natural y espiritual, que viaja entre los mundos en un estado de trance. Una vez en el mundo de los espíritus, se comunica con ellos para conseguir ayuda en la curación, la caza o el control del tiempo. Michael Ripinsky-Naxon describe a los chamanes como «personas que tienen fuerte ascendencia en su ambiente circundante y en la sociedad de la que forman parte».

Un segundo grupo de antropólogos discuten el término chamanismo, señalando que es una palabra para una institución cultural específica que, al incluir a cualquier sanador de cualquier sociedad tradicional, produce una uniformidad falsa entre estas culturas y crea la idea equívoca de la existencia de una religión anterior a todas los demás. Otros les acusan de ser incapaces de reconocer las concordancias entre las diversas sociedades tradicionales.

El chamanismo se basa en la premisa de que el mundo visible está impregnado por fuerzas y espíritus invisibles de dimensiones paralelas que coexisten simultáneamente con la nuestra, que afectan todas a las manifestaciones de la vida. En contraste con el animismo, en el que todos y cada uno de los miembros de la sociedad implicada lo practica, el chamanismo requiere conocimientos o capacidades especializados. Se podría decir que los chamanes son los expertos empleados por los animistas o las comunidades animistas. Sin embargo, los chamanes no se organizan en asociaciones rituales o espirituales, como hacen los sacerdotes.

Hay muchas variantes de chamanismo en el mundo; lo siguiente son creencias compartidas por todas las formas de chamanismo:
—hay que aclarar que el chamanismo proviene del «chamán», quien es propio de la región oriental de Siberia, aunque, como señala Mircea Eliade en su intento por hacer una historia general del chamanismo, hay una diversidad de chamanes esparcidos en todo el mundo, y les caracteriza el hecho de ser médicos y guías espirituales que realizan «ascensos hacia el cielo»—.


El chamanismo se basa en la premisa de que el mundo visible está dominado por fuerzas o espíritus invisibles que afectan las vidas de los vivientes. A diferencia de las religiones organizadas como el animismo o el animatismo que están lideradas por párrocos y que todos los miembros de una sociedad practican, el chamanismo requiere conocimientos individualizados y capacidades especiales. Los chamanes actúan fuera de religiones asentadas, y, tradicionalmente, actúan solos. Los chamanes pueden juntarse en asociaciones, como han hecho los practicantes tántricos indios.

La palabra «chamán» se refería originalmente a los sanadores tradicionales de las áreas túrquicas y mongolas del centro-norte de Asia (Siberia) y Mongolia. Chamán significa ‘médico’ en turco-tungus ―significa literalmente ‘el que sabe’―.
Otros académicos afirman que la palabra viene directamente del idioma manchú.
En turco fueron llamados "kam" y a veces "baksı".

La palabra tungusa "šamán" proviene de la china "sha men" tomada del palí, "śamana", y en última instancia del sánscrito "śramana:" ‘asceta’, que proviene de "śrama" ‘fatiga, esfuerzo’. La palabra pasó a través del ruso y el alemán antes de que fuera adoptada por el inglés, "shaman" (/sháman/), y llegara al español, donde «chamán» (plural, «chamanes») es correcto tanto en masculino como en femenino.

Otra explicación analiza el hecho de que esta palabra tungusa contiene la raíz "sha-", que significa ‘saber’. El "shamán" sería entonces ‘el o la que sabe’.

En su uso común, es equivalente al de brujo, un término que une las dos funciones del chamán: conocimiento del saber mágico y capacidad de curar a las personas y de reparar una situación problemática. Sin embargo, este último término se considera generalmente peyorativo y antropológicamente inexacto. La objeciones al uso de la palabra "chamán" vienen dadas por ser una palabra que viene de un lugar, de una gente, y de un sistema de prácticas específicas.

Ciertos antropólogos, como Alicia Kehoe,rechazan el término moderno por lo que implica de apropiación cultural. Se refieren a las formas occidentales modernas de chamanismo, que no solo falsifican y diluyen las prácticas indígenas genuinas, sino que lo hacen de tal forma que refuerzan ideas racistas, tales como la del "buen salvaje". Otra de las críticas al término es que se considera la forma en que se conduce la energía para la sanación del cuerpo, mente y espíritu, pero que esta era un ejercicio de mujeres sabias; y que al dividirse las tribus se desconoció el conocimiento que tenían las mujeres en el reino de los espíritus y lo usurparon los hombre. A partir de ese momento solo los hombres se denominan chamanes y a las mujeres se les denominó brujas. 

Kehoe es muy crítica con el trabajo de Mircea Eliade. Eliade, siendo historiador más que antropólogo, nunca había hecho ningún trabajo de campo ni había tenido contacto directo con los chamanes o las culturas que practican chamanismo. Según Kehoe, el chamanismo de Eliade es una invención sintetizada de varias fuentes sin apoyo de ninguna investigación directa. Opina que lo que este y otros estudiosos definen como propio del chamanismo, los trances, cánticos, comunicación con los espíritus, curaciones, son prácticas que existen en culturas no chamánicas como en algunos rituales judeocristianos. En su opinión, son propios de varias culturas que los utilizan, y no se pueden englobar en una religión general llamada chamanismo. Por lo mismo, rechaza que el chamanismo sea una antigua religión superviviente del Paleolítico.

Hoppál también discute si el término chamanismo es apropiado. Recomienda el usar «chamanidad» para marcar la diversidad y las características específicas de las culturas discutidas. Este es un término usado en viejos informes etnográficos, tanto rusos como alemanes, de principios del siglo XX. Cree que este término es menos general y permite marcar diferencias locales.

Los chamanes realizan una plétora de funciones dependiendo de la sociedad donde practican sus artes:
curación;
liderar un sacrificio;
conservar la tradición con historias y canciones;
videncia;
actuar como un psicopompo
En algunas culturas, un chamán puede cumplir varias funciones en una única persona.

El nigromante en la mitología griega puede ser considerado un chamán ya que el nigromante puede reunir espíritus y levantar a los muertos para utilizarlos como esclavos, soldados e instrumentos para la adivinación.

Los chamanes actúan como «mediadores» en su cultura.
El chamán es visto como un comunicador de la comunidad con los espíritus, incluyendo los espíritus de los muertos. En algunas culturas, esta función de mediador del chamán puede ser bien ilustrada por algunos de los objetos y símbolos del chamán. Por ejemplo, entre los selkups, un informe menciona a un pato marino como un animal-espíritu: los patos son capaces tanto de volar como de bucear bajo el agua, así se les considera pertenecientes tanto al mundo superior como al mundo inferior.

De modo parecido, el chamán y el jaguar son identificados en algunas culturas amazónicas: el jaguar es capaz de moverse libremente en la tierra, en el agua y trepando árboles (como el alma del chamán). En algunas culturas siberianas, son algunas especies de aves acuáticas las que están relacionadas con el chamán de una manera similar, y se cree que el chamán toma su forma.

«El árbol chamánico» es una imagen encontrada en varias culturas (yakutos, dolganos, evenkis), celtas, como un símbolo de mediación. El árbol es visto como un ser cuyas raíces pertenecen al mundo inferior; su tronco pertenece al medio, mundo habitado por humanos; y su copa se relaciona con el mundo superior.

En algunas culturas puede haber más tipos de chamanes, que realizan funciones más especializadas. Por ejemplo, entre el pueblo nanai, un tipo diferente de chamán actúa como un psicopompo.
Otros chamanes especializados pueden ser distinguidos según el tipo de espíritus, o reinos del mundo de los espíritus, con los cuales el chamán interacciona más comúnmente. Estos roles varían entre los chamanes nenets, enets y selkup (artículo; en línea). Entre los huicholes, hay dos categorías de chamán. Esto demuestra las diferencias entre los chamanes dentro de una misma tribu.

En los bosques tropicales, los recursos para el consumo humano son fácilmente agotables. En algunas culturas de los bosques tropicales, como los tucano, existe un sistema sofisticado para la gestión de los recursos, y para evitar el agotamiento de estos recursos a través de la sobreexplotación. Este sistema está conceptualizado en un contexto mitológico, involucrando simbolismo y, en algunos casos, la creencia de que la ruptura de las restricciones de caza puede causar enfermedades. Como principal maestro de simbolismo tribal, el chamán puede tener un papel principal en esta gestión ecológica, restringiendo activamente la caza y la pesca. El chamán es capaz de «sacar» los animales de caza (o sus almas) de sus ocultas moradas.

El chamán desana tiene que negociar con un ser mitológico por las almas de los animales de caza.
No solo los tucanos, sino también algunos otros indígenas de bosques tropicales tienen estas preocupaciones ecológicas relacionadas con su chamanismo, por ejemplo los piaroa.

Además de los tucanos y los piaroa, también muchos grupos esquimales piensan que el chamán es capaz de traer el espíritu de algunos animales de caza desde lugares remotos; o emprender un viaje del alma para promover suerte en la cacería, p. ej. pidiendo animales de caza a los seres mitológicos (Mujer del mar).

La plétora de funciones descritas en la sección anterior pueden parecer tareas bastante distintas, pero algunos conceptos subyacentes importantes les unen.

En algunos casos, en algunas culturas, el concepto de alma puede explicar más los fenómenos aparentemente no relacionados:
El aspecto ecológico de la práctica chamanística (y las creencias relacionadas) ya ha sido mencionado más arriba en el artículo.

La infertilidad de las mujeres puede curarse «obteniendo» el alma del niño que se espera que nazca.

También las creencias relacionadas con los espíritus pueden explicar muchos diferentes fenómenos.
Por ejemplo, la importancia de narrar historias, o actuar como un cantante, puede entenderse mejor si examinamos el sistema de creencias entero: una persona que es capaz de memorizar textos o canciones largas (y tocar un instrumento) puede considerarse como que ha logrado esta capacidad a través del contacto con los espíritus (por ejemplo entre el pueblo janty).

Como se ha mencionado, un enfoque (discutido) explica la etimología de la palabra «chamán» significando «uno que sabe».
Realmente, el chamán es una persona experta en mantener juntos los múltiples códigos a través de los cuales este complejo sistema de creencias aparece, y tiene una visión de conjunto de él en su mente con certeza de conocimiento.
El chamán usa (y el público entiende) múltiples códigos. El chamán expresa significados de muchas maneras: verbalmente, musicalmente, artísticamente y en baile. Los significados pueden manifestarse en objetos, como amuletos.

El chamán conoce bien la cultura de su comunidad, y actúa en consecuencia. Así, su público conoce los símbolos usados y los significados — esto es por lo que el chamanismo puede ser eficiente: la gente en el público confía en ello.
Estos sistemas de creencias pueden parecer para sus miembros con certeza de "conocimiento" ―esto explica la etimología descrita más arriba para la palabra «chamán»―.

Hay enfoques teóricos semióticos hacia el chamanismo, («etnosemiótica»). Los símbolos en el traje del chamán y el tambor pueden referirse a animales (como espíritus ayudantes), o al rango del chamán. Había también ejemplos de «símbolos mutuamente opuestos», distinguiendo chamanes «blancos» practicando de día contactando con espíritus celestes, y chamanes «negros» practicando de noche contactando con espíritus malignos para malos propósitos.

Series de estos símbolos opuestos se referían a una visión del mundo detrás de ellos. Análogamente a la manera que la gramática ordena las palabras para expresar significados y expresar un mundo, también esto formó un mapa cognitivo.
La tradición del chamán está arraigada en el folclore de la comunidad, que proporciona un «mapa mental mitológico».
Juha Pentikäinen usa el concepto «gramática de la mente».
Enlazando con un ejemplo sami, Kathleen Osgood Dana escribe:

Algunos enfoques se refieren a la hermenéutica, «etnohermenéutica», acuñada e introducida por Armin Geertz. El término puede ser extendido: Hoppál incluye no solo la interpretación de textos orales o escritos, sino la de los «textos visuales también (incluyendo movimientos, gestos y rituales más complejos, y ceremonias celebradas por ejemplo por chamanes)».
Esto puede no solo revelar las visiones animistas que se esconden detrás del chamanismo, sino también expresar su relevancia para el mundo reciente, donde los problemas ecológicos hacen los paradigmas sobre el equilibrio y la protección válidos.

Otros trabajos de campo usan conceptos de la teoría de sistemas y consideraciones ecológicas para entender la tradición del chamán. Los indígenas desana y tucano han desarrollado un sofisticado simbolismo y conceptos de «energía» fluyendo entre la gente y los animales en caminos cíclicos. Gerardo Reichel-Dolmatoff relaciona estos conceptos con los cambios en cómo la ciencia moderna (teoría de sistemas, ecología, algunos nuevos enfoques en antropología y arqueología) trata la causalidad de una manera menos lineal. También sugiere una cooperación de la ciencia moderna y la tradición indígena.

Según Vladimir Basilov y su obra "Chosen by the spirits", un chamán ha de estar en las mejores condiciones saludables para realizar sus funciones al máximo. La creencia del chamán es más popular para la gente situada en Asia Central y Kazajistán. Las tradiciones del chamanismo están también presentes en las regiones de tadzhikos y uzbekos. Los cuerpos de los chamanes han de estar formados por un tipo fuerte, alguien teniendo una complexión pequeña sería apartado en seguida. La edad es un requisito también, sin duda tener más de cincuenta años descalificaría a aquellos que quieren estar involucrados en servir a los espíritus. Los chamanes son siempre del más alto intelecto y se les mira desde una perspectiva diferente, tienen una forma que les hace rápidos con sus pies y con enfermedades curarán a aquellos necesitados.

Una de las cualidades más significativas y relevantes que separan a un chamán de otros líderes espirituales son sus comunicaciones con el mundo sobrenatural. Ya a principios de siglo la autohipnosis era muy considerada por aquellos que rendían culto. Otra característica del chamán es el talento para encontrar objetos y descubrir ladrones, impresionando a aquellos de su tribu y a aquellos otros también alrededor para presenciarlo. La creencia en los espíritus o lo sobrenatural es lo que atrae a aquellos que creen en el chamán. Aquellos que tienen hijos enfermos o están débiles de salud ellos mismos es lo que les lleva a las curaciones espirituales del chamán. Aunque los chamanes aún existen, la población está sin duda disminuyendo.

En las culturas chamánicas del mundo, el chamán juega un papel de párroco; no obstante, hay una diferencia esencial entre los dos, como Joseph Campbell describe:
Un chamán puede ser iniciado a través de una enfermedad grave, siendo alcanzado por un relámpago y soñando con un trueno para convertirse en un Heyoka, o por una experiencia cercana a la muerte (p. ej., el chamán Alce Negro), o uno puede seguir una «llamada» para convertirse en chamán. Hay normalmente un conjunto de imaginería cultural que se espera que se experimente durante la iniciación chamánica sin importar el método de inducción. Según Mircea Eliade, esta imaginería a menudo incluye ser transportado al mundo de los espíritus e interaccionar con seres que habitan el mundo distante de los espíritus, encontrar un guía espiritual, ser devorado por algún ser y aparecer transformado, o ser «desmontado» y «vuelto a montar» de nuevo, a menudo con amuletos implantados tales como cristales mágicos. La imaginería de la iniciación generalmente habla de transformación y de los poderes otorgados para trascender la muerte y el renacimiento.

En algunas sociedades chamánicas se considera que los poderes son heredados, mientras que en otros lugares del mundo se considera que el chamán ha sido «llamado» y requiere un entrenamiento largo. Entre los chukchis siberianos uno puede comportarse de maneras que los clínicos biomédicos «occidentales» caracterizarían tal vez como psicótico, pero que los pueblos siberianos pueden interpretarlo como una posesión por un espíritu que demanda que uno asuma la vocación chamánica. Entre los Tapirapé sudamericanos, los chamanes son llamados en sus sueños. En otras sociedades el chamán elige su carrera. En América del Norte, los pueblos de las Naciones Originarias buscarían la comunión con los espíritus a través de una «búsqueda de visión»; mientras que los shuar sudamericanos, buscando el poder para defender a su familia contra los enemigos, aprenden ellos mismos para lograr ser un chamán. Asimismo los urarina de la amazonia peruana tienen un elaborado sistema cosmológico basado en el consumo ritual de ayahuasca. Junto con los impulsos milenarios, el chamanismo ayahuasca de los urarina es una de las características clave de esta sociedad poco documentada.

Supuestamente también pueden observarse «tradiciones» chamánicas habituales entre los pueblos indígenas kuna de Panamá, que confían en poderes chamánicos y talismanes sagrados para curar. Por eso, gozan de una posición popular entre los pueblos locales.

La enfermedad chamánica, también llamada crisis iniciática chamanística, es una crisis psicoespiritual, normalmente involuntaria, o un rito de paso, observado entre aquellos que se convierten en chamán. El episodio a menudo marca el inicio de un episodio de confusión o comportamiento inquietante limitado en el tiempo donde el iniciado chamánico puede cantar o bailar de una manera poco convencional, o tener la experiencia de ser «perturbado por espíritus». Los síntomas no se consideran normalmente como signos de enfermedad mental por intérpretes de la cultura chamánica; más bien, son interpretados como señales indicadoras introductorias para el individuo que se supone que tomará el cargo de chamán (Lukoff et. al, 1992). Las similitudes de algunos síntomas de la enfermedad chamánica al proceso kundalinī han sido a menudo apuntadas.
El papel significativo de las enfermedades iniciáticas en la llamada de un chamán puede encontrarse en el detallado historial de Chuonnasuan, el último maestro chamán entre los pueblos tungus del nordeste de China.

El chamán juega el papel de curandero en las sociedades chamánicas; los chamanes adquieren conocimiento y poder atravesando el axis mundi y trayendo conocimiento de los cielos. Incluso en las sociedades occidentales, esta antigua práctica de curación está referenciada por el uso del caduceo como el símbolo de la medicina. A menudo el chamán tiene, o adquiere, una o más entidades familiares ayudantes en el mundo de los espíritus; estas son a menudo espíritus en forma de animal, espíritus de plantas medicinales, o (a veces) aquellas de los chamanes difuntos. En muchas sociedades chamánicas, la magia, la fuerza mágica y el conocimiento son todos denotados por una palabra, como el término quechua "«yachay»".

Aunque se considera que las causas de una enfermedad se encuentran en el mundo espiritual, siendo afecctadas por espíritus maliciosos o brujería, se utilizan tanto métodos espirituales como físicos para curar. Comúnmente, un chamán «entra en el cuerpo» del paciente para hacer frente al espíritu que pone enfermo al paciente, y cura el paciente desterrando el espíritu infeccioso. Muchos chamanes tienen conocimiento experto de la vida de las plantas en su área, y a menudo se receta un régimen de hierbas como tratamiento. En muchos lugares los chamanes afirman aprender directamente de las plantas, y ser capaces de aprovechar sus efectos y propiedades curativas solo después de obtener permiso de su espíritu permanente o patrón. En América del Sur, los espíritus individuales son llamados con el canto de canciones llamadas icaros; antes de que un espíritu pueda ser llamado el espíritu debe enseñar al chamán su canción. El uso de elementos totémicos como rocas es común; se cree que estos elementos tienen poderes especiales y un espíritu vivo. Estas prácticas son supuestamente muy antiguas; alrededor del 368 a. C., Platón escribió en el "Fedro" que «las primeras profecías fueron las palabras de un roble», y que todos los que vivieron en esa época encontraron suficientemente gratificante «escuchar a un roble o a una piedra, mientras dijera la verdad».

La creencia en la brujería, es frecuente en muchas sociedades chamánicas. Algunas sociedades distinguen los chamanes que curan de los hechiceros que hacen daño; otros creen que todos los chamanes tienen el poder tanto de curar como de matar; es decir, en algunas sociedades también se piensa que los chamanes son capaces de hacer daño. El chamán normalmente goza de un gran poder y prestigio en la comunidad, y es célebre por sus poderes y conocimientos; pero también pueden ser sospechosos de hacer daño a otros y por lo tanto son temidos.

Por dedicarse a este trabajo, el chamán se expone a un riesgo personal significativo, del mundo de los espíritus, de cualquier chamán enemigo, así como de los medios utilizados para cambiar su estado de conciencia. Ciertos materiales de las plantas usados pueden ser mortales, y el fallo de volver de un viaje extracorpóreo puede llevar a la muerte física. Los hechizos se usan a menudo para protegerse de estos peligros, y el uso de plantas más peligrosas está muy normalmente ritualizado.

Generalmente, el chamán atraviesa el "axis mundi" y entra en el mundo de los espíritus llevando a cabo una transición de conciencia, entrando en un trance extático, bien autohipnóticamente o bien a través del uso de enteógenos. Los métodos utilizados son diversos, y se usan a menudo juntos. Algunos de los métodos para llevar a cabo estos trances:
Los chamanes a menudo cumplen restricciones alimenticias o costumbres particulares de su tradición. A veces estas restricciones son más que solo culturales. Por ejemplo, la dieta seguida por los chamanes y aprendices antes de participar en una ceremonia ayahuasca incluye alimentos ricos en triptófano (un precursor biosintético de la serotonina) así como evita alimentos ricos en tiramina, que pueden inducir crisis hipertensivas si se ingieren con inhibidores de monoamino oxidasa como se encuentra en los brebajes de ayahuasca.

Justo como el propio chamanismo, la música y las canciones relacionadas con él en varias culturas son diversas, lejos de ser parecidas. En algunas culturas y en varios casos, algunas canciones relacionadas con el chamanismo intentan imitar también los sonidos naturales, a veces a través de onomatopeyas.

Por supuesto, en varias culturas, la imitación de sonidos naturales puede cumplir otras funciones, no necesariamente relacionadas con el chamanismo: objetivos prácticos como atraer animales en la caza; o entretenimiento (katajjaqs de los esquimales).

La música una de las arte más antiguas que conecta al ser humano con su yo espiritual pues a través de estas vibraciones el espíritu se abre camino en el mundo espiritual llegando a las puertas de su propio dios interno y de las entidades espirituales quien lo provee de fuerza y sabiduría para sanar o resolver conflictos terrenales.

La música es un medio muy importante en varias prácticas espiritistas no solo en el chamanismo.

Como se ha mencionado más arriba, las culturas calificadas como chamánicas puede ser muy diferentes. Por lo tanto, los chamanes pueden tener varios tipos de parafernalia.

El tambor se usa por los chamanes de varios pueblos de Siberia; lo mismo se aplica a muchos grupos esquimales, aunque puede carecer de uso chamánico entre los esquimales de Canadá.

El redoble del tambor permite al chamán lograr un estado alterado de conciencia o hacer un viaje. El tambor es por ejemplo referido como, «"caballo" o "puente del arco iris" entre los mundos físico y espiritual».
El viaje mencionado es uno en donde el chamán establece una conexión con uno o dos de los mundos de los espíritus. Con el redoble del tambor vienen efectos neurofisiológicos. Mucha fascinación rodea al papel que la acústica del tambor juega en el chamán. Los tambores de los chamanes siberianos son generalmente construidos con una piel de animal estirada sobre un aro de madera curvado, con un asa cruzando el aro.
Hay dos mundos diferentes, el superior y el inferior. En el mundo superior, imágenes como «subir una montaña, árbol, acantilado, arco iris o escalera; ascender al cielo con el humo; volar en un animal, alfombra, o limpiar y encontrar un maestro o guía», son típicamente vistas. El mundo inferior consta de imágenes que incluyen, «entrar en la tierra a través de una cueva, vaciar un tocón de árbol, un charco, un túnel, o un tubo».
Siendo capaz de relacionarse con un mundo diferente en un estado alterado y consciente, el chamán puede entonces intercambiar información entre el mundo en donde él vive y el que ha viajado.

Estas plumas se han visto usándose como un tipo de bisturí espiritual.

Encontrado generalmente entre los pueblos sudamericanos y africanos.
También usado en ceremonias entre los navajos y de manera tradicional en sus bendiciones y ceremonias.

También se sabe que entre algunas de las facultades que el chamán puede desarrollar, esta la de transformarse en alguna forma animal

A menudo encontrado entre los pueblos del Sureste Asiático y Extremo Oriente.

Encontrado principalmente entre los diferentes pueblos aborígenes de Australia.

Mientras que algunas culturas han tenido mayor número de chamanes hombres, otras como las culturas coreanas nativas han tenido una preferencia por las mujeres. La evidencia arqueológica reciente sugiere que los primeros chamanes conocidos —datados en la era del Paleolítico Superior en lo que es hoy la República Checa— eran mujeres.

En algunas sociedades, los chamanes muestran una identidad de dos espíritus, adoptando la vestimenta, los atributos, el rol o función del sexo opuesto, la fluidez del género o la orientación sexual hacia personas del mismo sexo. Esta práctica es común, y se encuentra entre los chukchis, los dayaks del mar, los patagones, los mapuches, los arapahos, los cheyennes, los navajos, los pawnees, los lakotas, y los utes, así como en muchas otras tribus nativas americanas. En efecto, estos chamanes de dos espíritus estuvieron tan extendidos como para sugerir un origen muy antiguo de la práctica. Véase, por ejemplo, el mapa de Joseph Campbell en "The Historical Atlas of World Mythology" (volumen I: «The Way of the Animal Powers», parte 2: pág. 174). Se cree que estos chamanes de dos espíritus son especialmente poderosos, y el chamanismo tan importante para las poblaciones ancestrales que puede haber contribuido al mantenimiento de los genes de los individuos transgénero en poblaciones de reproducción durante el tiempo evolucionario a través del mecanismo de «selección de parentesco».
Son muy respetados y buscados en sus tribus, ya que traerán un alto estatus a sus compañeros.

La dualidad y la bisexualidad también se encuentran en los chamanes del pueblo dogón de Malí (África). Se pueden encontrar referencias sobre esto en varios trabajos de Malidoma Somé, un escritor que nació y fue iniciado allí.

En algunas culturas, la frontera entre el chamán y la persona laica no es nítida:

La diferencia es que el chamán conoce más mitos y entiende mejor su significado, pero la mayoría de los hombres adultos también conocen muchos mitos.

Algo similar puede observarse entre algunos pueblos esquimales. Muchas personas laicas han sentido experiencias que son normalmente atribuidas a los chamanes de esos grupos esquimales: la experimentación de sueños despiertos, la ensoñación o el trance no están restringidos a los chamanes.
Es el control sobre los espíritus ayudantes lo que es principalmente característico de los chamanes, la gente laica usa amuletos, hechizos, fórmulas y canciones.
En Groenlandia entre algunos esquimales, hay personas laicas que pueden tener la capacidad de tener relaciones más cercanas que otros con seres del sistema de creencias. Estas gentes son chamanes aprendices que no consiguieron llevar a cabo su proceso de aprendizaje.

El ayudante de un chamán oroqen (llamado "jardalanin", o «segundo espíritu») sabe muchas cosas sobre las creencias asociadas: él/ella le acompaña en los rituales e interpreta el comportamiento del chamán.
A pesar de esto, el jardalanin no es un chamán. Por su rol interpretativo y de acompañamiento, sería incluso inoportuno entrar en trance.

La manera cómo los chamanes obtienen sustento y toman parte en la vida cotidiana varía entre culturas. En muchos grupos esquimales, proporcionan servicios para la comunidad y obtienen un «pago vencido» (algunas culturas creen que el pago se le da a los espíritus ayudantes), pero estos bienes son solo «añadidos bienvenidos». No son suficientes para permitir hacer de chamán como una actividad a tiempo completo. Los chamanes viven como cualquier otro miembro del grupo, como cazador o ama de casa.

El chamanismo es considerado por algunos como el antecedente de todas las religiones organizadas, ya que nació antes del Neolítico, durante el Paleolítico Superior. De este periodo datan algunos indicios en dibujos realizados en las paredes de las cuevas y en objetos de arte mobiliario, aunque no existen pruebas concluyentes.

Algunos de sus aspectos se mantienen en el fondo de las religiones, generalmente en sus prácticas místicas y simbólicas. El paganismo griego estaba influenciado por el chamanismo, como se refleja en las historias de Tántalo, Prometeo, Medea y Calipso entre otros, así como en los misterios, como los de Eleusis. Algunas de las prácticas chamánicas de la religión griega fueron copiadas más adelante por la religión romana.

Las prácticas chamánicas de muchas culturas fueron marginadas con la propagación del monoteísmo en Europa y el Oriente Medio. En Europa, comenzó alrededor del año 400, cuando la Iglesia católica consiguió la primacía sobre las religiones griega y romana. Los templos fueron destruidos sistemáticamente y sus ceremonias prohibidas o apropiadas. La caza de brujas fue la última persecución para acabar con el remanente del chamanismo europeo.

La represión continuó con la influencia católica en la colonización española. En el Caribe, y América Central y del Sur, los sacerdotes católicos seguían los pasos de los conquistadores y eran el instrumento de destrucción de las tradiciones locales, denunciando a sus practicantes como "representantes del diablo" y ejecutándolos. En Norteamérica, los puritanos ingleses realizaron campañas periódicas de ataque contra los pueblos indígenas a quienes consideraban como brujos. Más recientemente, ataques contra participantes de prácticas chamánicas han sido llevados a cabo por misioneros cristianos en países del Tercer Mundo. Una historia semejante de destrucción se puede contar entre budistas y chamanes, por ejemplo, en Mongolia.

Hoy, el chamanismo sobrevive sobre todo en pueblos indígenas. Su práctica continúa en las tundras, las selvas, los desiertos y otras áreas rurales, y también en ciudades, pueblos, suburbios, y aldeas de todo el mundo. Está especialmente extendido en Sudamérica, donde existe el llamado "chamanismo mestizo".

Aunque el chamanismo tenía una gran tradición en Europa antes de la llegada del monoteísmo cristiano, permaneció como una religión organizada y tradicional solamente en Mari-El y Udmurtia, dos provincias semiautónomas de Rusia cuya población era mayoritariamente finesa y húngara.

Entre las tribus húngaras, el centro de la religión era la adoración al ciervo sagrado y al águila celestial conocida como Turul. El universo se hallaba sobre un árbol titánico, el «árbol de la vida», hallándose el inframundo en sus raíces y el mundo superior de los dioses en la copa. A lo largo de su tronco y copa había tres bosques, el bosque de oro, el de cobre y el de plata, y esta era la región corpórea donde habitaban los seres humanos. En el tope del árbol, se sentaba el águila Turul y vigilaba el universo; cuidaba de las almas de los que nacerán, que existían en forma de pájaros, que habitaban en la copa del árbol.

Aquellos que eran chamanes nacían con cualidades físicas, como alguna deformidad o un par de dedos extra en sus manos, que legitimaban sus cualidades divinas y les permitirían comunicarse con los dioses. En el chamanismo húngaro se adoraba a los ríos, rocas, árboles y colinas, a los espíritus de los ancestros y a un dios superior, padre del universo, que se hallaba servido por una corte de dioses menores y otras entidades espirituales.

Un resto del chamanismo en Europa podría ser la brujería, ejercida sobre todo por mujeres que ayudaban en la curación o procuraban los deseos de sus vecinos por medio de hierbas y conjuros. La brujería europea fue perseguida masivamente desde fines del siglo XV, sobre todo en Alemania y Suiza. Eran acusadas de pactar con el diablo, realizar aquelarres o "sabbat", causar mal de ojo, causar todas las enfermedades que se producían, desde la peste a la muerte de niños, y por lo tanto quemadas vivas. La persecución acabó en el siglo XVIII, con la llegada de la Ilustración.

En las Islas Canarias (España), los aborígenes guanches tenían una clase de sacerdotes o chamanes llamados guadameñes.

Todavía se practica en algunas zonas, aunque en muchos otros casos el chamanismo ya estaba en decadencia a comienzos del siglo XX.

La región oriental de Rusia, conocida como Siberia, es un centro de chamanismo en donde muchas de las gentes que pueblan los Urales y Altái han mantenido estas prácticas vivas hasta épocas modernas. Variadas fuentes etnográficas han sido recogidas entre sus gentes.

Muchos grupos de cazadores y criadores de renos practicaron el chamanismo como tradición viva también en época moderna, especialmente los que han vivido aislados hasta tiempos recientes como los naganasan.

Cuando la República Popular China se creó en 1949 y la frontera con la Siberia rusa fue sellada formalmente, quedaron confinados grupos nómadas de tungus que practicaban el chamanismo en Manchuria y Mongolia. El último chamán conocido de los Oroqen, Chuonnasuan (Meng Jin Fu), murió en octubre del 2000.

El chamanismo todavía se practica en Corea del Sur, en donde el papel de chamán lo representan mujeres llamadas "mudang", mientras que los escasos varones son conocidos como "baksoo mudang". Ambos suelen ser miembros de clases bajas.

El título puede ser hereditario o deberse a una capacidad natural. En la sociedad contemporánea se les consulta para tomar decisiones tales como financieras y maritales.

El uso que las "mudang" y los "baksoo mudang" hacen de la "Amanita muscaria" era una práctica tradicional que se creía suprimida desde la dinastía Choseon. Otra seta (extremadamente venenosa) fue retitulada como la seta del chamán, "무당버섯". Los chamanes coreanos son conocidos también por utilizar arañas. Mantienen los trajes de colores, las danzas, los tambores y las armas rituales características.

Hay una gran influencia chamánica en la religión de Bön de Asia central, y en el budismo tibetano; el budismo llegó a ser popular entre los chamanes tibetanos, mongoles, y manchúes a principios de siglo VIII. Las formas rituales chamánicas impregnaron el budismo tibetano, y se institucionalizaron como religión de estado bajo las dinastías chinas Yuan y Qing. Un elemento común entre ambas religiones es la consecución de la realización espiritual, conseguido ocasionalmente por sustancias psicodélicas. De todas formas, la cultura chamánica todavía se practicó por varios grupos étnicos en áreas de Nepal y norte de India, donde no se considera extinguida actualmente, e incluso hay gentes que temen las maldiciones de los chamanes.

En Tíbet, la escuela de Nyingma en particular, mantenía la tradición tántrica de casar a sus sacerdotes, conocidos como Ngakpas (masc.) o Ngakmas/mos (fem.). El Ngakpas se ocupaba de librar a las aldeas de demonios o enfermedades, creando amuletos protectores, realizando los ritos oportunos, etc. Eran despreciados por la jerarquía de los monasterios, que, como en muchas instituciones religiosas convencionales, deseaban preservar sus propias tradiciones, a veces a expensas de otras: dependían de la liberalidad de mecenas que los ayudasen. Esta situación condujo a menudo a un choque entre los pueblos de carácter chamánicos con cultura Ngakpa y el sistema monástico más conservador.

También se practica en las islas de Ryukyu (Okinawa), donde se conocen a los chamanes como "nuru", y en algunas otras áreas rurales de Japón.

Muchos coreanos todavía creen que el sintoísmo es el resultado de la transformación del chamanismo en religión del estado.

El desarrollo de los cultos tribales en África, como en tantas partes del mundo, está adscrito muchas veces, si no a un brujo o chamán de la tribu, a una clase sacerdotal que adquiere particular desarrollo como institución. En multitud de comunidades se dan sacerdotes de distinta categoría y especialidad que cabe estudiar en dos grupos clásicos:

Además de estos chamanes, en África occidental existe la figura del djeli, un bardo cantante y músico ambulante, que es el depositario de las tradiciones orales, y a veces la única fuente que guarda los acontecimientos históricos. Es una figura que permanece en Malí, Gambia, Guinea y Senegal, entre los pueblos manden, fula, wólof, peul, serer entre otros.

Los "chamanes" americanos tienen creencias espirituales diversas. Nunca existió una religión o sistema espiritual común en las Américas siendo estas prácticas asociadas a cada etnia y su territorio. Es más, en el chamanismo amazónico, cada etnia que utiliza las diferentes técnicas chamánicas como el uso de enteógenos, la música y cantos repetitivos, y las dietas y aislamientos prologandos, entre otras prácticas, tienen diferentes cosmogonías asociadas a los mundos alternos que visitan los chamanes.

Algunas de estas religiones indígenas han sido falsificadas burdamente por los observadores y los antropólogos, tomando aspectos superficiales e incluso totalmente erróneos que eran tomados como "más auténticos" que los relatos de los miembros de esas culturas. Se contribuye al error al pensar que las religiones americanas son algo que existió solamente en el pasado, y que se pueden obviar las opiniones de las comunidades nativas.
No todas las comunidades indígenas tienen individuos con un papel específico de mediador con el mundo de los espíritus en nombre de su comunidad. Entre las que tienen esta estructura religiosa, métodos espirituales y creencias pueden tener algunas similitudes, aunque muchas de estas concordancias son debido a las relaciones entre naciones de la misma región o a que las que las políticas gubernamentales post-coloniales mezclaron naciones independientes en las mismas reservas. Esto puede dar la impresión de que hay más uniformidad entre creencias de las que realmente existieron en la antigüedad.

Entre el pueblo mapuche de América del Sur, sirve a la comunidad como chamán una mujer, llamada "machi", que realiza ceremonias y prepara hierbas para curar enfermedades, expulsar demonios e influenciar sobre el tiempo y la cosecha.

La etnia Aymara tiene como parte de la comunidad los Yatiris que son los médicos y los curanderos de la comunidad entre los aymaras de Bolivia, Chile y Perú, que utilizan en su práctica tanto en los símbolos y los materiales tales como hojas de coca. Sus curaciones no solo se restringen al cuerpo humano, si no sobre todo al "alma" o la que llaman AJAYU.

En el inmenso territorio compartido por Argentina (nordeste), Brasil (Estado do Paraná) y Paraguay (este), cerca de la confluencia de los ríos Iguazú y Paraná, habitan los mbyá (hombres del monte o de la selva), que son una etnia guaraní. Sus médicos-chamanes se denominan caraí opy´guá (señor del op´y o recinto ceremonial). Son avanzados curadores físicos y espirituales. Sus rituales de sanación en ocasiones son masivos con la confluencia de los chamanes de muchas comunidades regionales.

En la Amazonía colombiana, ecuatoriana, peruana y boliviana diferentes etnias utilizan plantas enteógenas como la ayahuasca, el yopo, el tabaco y la coca en rituales chamánicos dentro de sus prácticas de medicina tradicional. En ese sentido fue que el Gobierno del Perú declaró el 2008 a la ayahuasca como Patrimonio Cultural de la Nación en la categoría de Conocimientos, saberes y prácticas asociadas a la medicina tradicional. Hoy en día, pueblos como el Shipibo-conibo son un referente para la utilización chamánica de la enredadera de la ayahuasca en combinación con el arbusto chacruna.

Los «hombres medicina» navajos, conocidos como "hatalii", utilizan varios métodos para diagnosticar las dolencias del paciente. Usan herramientas especiales tales como rocas cristalinas, y habilidades tales como trances, acompañados a veces de cánticos. El "hatalii" selecciona un canto específico para cada tipo de dolencia. Los curadores navajos tienen que ser capaces de realizar correctamente la ceremonia de comienzo a fin, ya que en caso contrario no surtirá efecto. El entrenamiento de un "hatalii" es largo y difícil, casi como un sacerdocio. El aprendiz aprende observando a su maestro, memorizando las palabras de todos los cánticos. En ocasiones, un hombre medicina no puede aprender todas las ceremonias tradicionales, así que puede optar por especializarse en unas pocas.

En México es relevante la supervivencia de elementos y rituales de tipo mágico-religioso de los antiguos grupos indígenas, no solo en los indígenas actuales sino en los mestizos y blancos que conforman la sociedad mexicana rural y urbana.En zonas rurales el ó la son de mucha importancia para la vida de la comunidad rural al grado que estas personas pueden en cierta forma dirigir la vida de la gente, de forma muy discreta se acude a ellos y es muy difícil que estas personas acepten que visitan a un chamán, sin embargo, lo hacen muy frecuentemente siguiendo todos sus rituales, aunque existen muchos charlatanes, también hay mucha gente dedicada a la sanación del cuerpo, el alma y el espíritu, los que verdaderamente conocen los saberes de nuestros antepasados en herbolaría y demás si son capaces de sanar y mejorar la vida de las personas.

Hoy en día se mantiene viva la tradición chamánica en la costa y sierra norte del Perú incorporando elementos ancestrales, coloniales y contemporáneos. El cactus de San Pedro que contiene el alcaloide mescalina es un elemento central en esta tradición. Los especialistas rituales, andinos y mestizos, de la mesa curandera norteña en Cajamarca, La Libertad, Lambayeque y Piura utilizan rezos, cantos y música para entrar en trance para diagnósticas y tratar algunas enfermedades. Estas prácticas han sido estudiadas por antropólogos como Douglas Sharon, Luis Millones Santagadea y Alfredo Menacho, entre otros.

En las leyendas de la Tierra del Fuego, el "xon" tiene habilidades sobrenaturales, por ejemplo puede controlar el tiempo.

En las culturas chamánicas, los brujos desempeñan un papel similar al de los sacerdotes, aunque con una diferencia esencial:

Un chamán se puede iniciar a causa de una enfermedad grave, porque ha soñado con un rayo o un trueno, o por una experiencia cercana a la muerte, o bien porque se siente llamado a serlo. Hay todo un bagaje de imágenes culturales para experimentar en la iniciación, sin importar el método de inducción. Según Mircea Eliade, tales imágenes incluyen a menudo el viaje al mundo de los espíritus y el conocimiento de los seres que lo habitan, encontrando una guía espiritual, para emerger transformado, a veces con amuletos implantados, como cristales mágicos. Las imágenes de la iniciación hablan generalmente de la transformación y de los poderes concedidos para superar la muerte y renacer.

En algunas sociedades se considera que los poderes chamánicos son hereditarios, mientras que en otras deben ser "llamados" y necesitan un largo entrenamiento. Entre los Chukchis siberianos uno puede comportarse de forma tal que un médico "occidental" quizás caracterizaría como sicópata, pero que los siberianos interpretan como la prueba de la posesión por un espíritu, que le exige al poseso que asuma su vocación de chamán. Entre los Tapirapes suramericanos los chamanes son llamados en sus sueños. En otras sociedades eligen libremente su carrera. En Norteamérica, buscan la comunión con los espíritus a través de una visión, mientras que el shuar suramericano, busca el poder de defender a su familia contra enemigos aprendiendo de otros chamanes. El urarina de la Amazonía peruana tiene un elaborado sistema, afirmado en la consumición ritual de ayahuasca. Junto con impulsos milenarios, el chamanismo del ayahuasca de los urarinas es una característica dominante de esta mal documentada sociedad.

Estas supuestas tradiciones chamánicas también se pueden observar entre los indígenas kuna de Panamá, que confían en poderes y talismanes sagrados para sanar. Los chamanes gozan de una posición privilegiada entre la gente local.

La enfermedad del chamán, también llamada crisis iniciática chamánica, es una crisis sico-espiritual, o un rito del paso, observado entre los chamanes novicios. Marca a menudo el principio de un corto episodio de confusión o disturbios del comportamiento en que el iniciado puede cantar o bailar en una manera poco convencional, o tiene una experiencia de «ser molestado por espíritus». Los síntomas no son considerados como muestras de enfermedad mental por los intérpretes de la cultura chamánica; más bien se interpretan como indicaciones al individuo para que tome el oficio de chamán.
El papel significativo de las enfermedades iniciáticas, se puede encontrar en la historia detallada de Chuonnasuan, el último chamán de los tungus en el noreste de China.

El movimiento "New Age" se ha apropiado de algunas ideas del chamanismo, así como de creencias y prácticas de las religiones de oriente y de distintas culturas indígenas. Como con otras apropiaciones, los seguidores originales de estas tradiciones condenan su uso, considerándolo mal aprendido, superficialmente entendido y mal aplicado.

Hay un esfuerzo en algunos círculos ocultistas y esotéricos para reinventar el chamanismo en una forma moderna, partiendo de la base de un sistema de creencias y de prácticas sintetizadas por Michael Harner a partir de varias religiones indígenas. Harner ha hecho frente a muchas críticas por creer que partes de diversas religiones se pueden sacar de contexto para formar una cierta forma de tradición chamánica universal. Algunos de estos neochamanes también se centran en el uso ritual de enteógenos, así como en la magia del caos. Alegan que se basan en tradiciones investigadas (o imaginadas) de la Europa antigua, en donde creen que muchas prácticas y sistemas místicos fueron suprimidos por la iglesia cristiana.

Algunos de estos practicantes expresan su deseo de utilizar un sistema que se base sobre sus propias tradiciones ancestrales. Algunos antropólogos han discutido el impacto de tal neochamanismo en las tradiciones americanas indígenas, ya que estos "practicantes chamánicos" no se llaman a sí mismos chamanes, sino que usan nombres específicos derivados de las viejas tradiciones europeas; el völva (varón) o el seidkona (mujer) de las sagas son un ejemplo.










</doc>
<doc id="4708" url="https://es.wikipedia.org/wiki?curid=4708" title="Horóscopo">
Horóscopo

El horóscopo, y la carta natal, en astrología, son métodos de predicción no demostrada basados en la posición arbitraria de los astros en el momento del nacimiento. El término deriva del griego «ὥρα» ("hora", «hora»), y «σκοπέω» ("skopeo", «examinar»)

No existe ninguna prueba o estudio científico que apoye la validez de las predicciones obtenidas mediante cualquiera de las diferentes versiones de esta práctica.

Hay diversas explicaciones a la razón por la que la gente cree en esta pseudociencia, basados en fenómenos psicológicos. La creencia en la efectividad del horóscopo se ve potenciada por un fenómeno psicológico normal, la apofenia, basada en la búsqueda automática de patrones por parte del cerebro, por el que se recuerdan fácilmente las coincidencias y se olvidan las faltas de coincidencia. La vaguedad unida a la alta probabilidad de las supuestas predicciones permiten un índice de aciertos bajo, pero lo suficientemente alto para que funcione el mecanismo psicológico descrito. El Efecto Forer observa que las descripciones de personalidad vagas y generales, a las que las personas pueden entregarle su propio significado generan altas tasas de aprobación, siendo utilizado como técnica para la "adivinación" de personalidad del horóscopo.

Muchas culturas utilizaron formas de predicción similares basándose en sus propios calendarios en relación directa con los astros. La civilización Maya, por ejemplo.

Antes de que el horóscopo tuviera el carácter que le otorgó la cultura grecolatina, se ha descubierto que en Babilonia, bajo el reinado persa, como lo afirma Van der Waerden, nació la astronomía horoscópica, sin embargo existen datos relacionados con cuentas astronómicas y en esa etapa, astrológicas, desde la antigüedad:


El horóscopo es una representación gráfica de las posiciones planetarias en un momento especial; que normalmente es el de nacimiento de una persona, aunque también puede ser el momento en el que se inicia un proyecto empresarial, se tiene una idea especial, se realiza un trato, se realiza una boda, comienza un viaje, etc.

Esta representación de un horóscopo utiliza cálculos matemáticos y astronómicos que deberían ser idénticos independientemente de quien lo haga, siempre que se le faciliten los mismos datos iniciales.

La interpretación que se haga de ese horóscopo desde el punto de vista de la astrología pasa a ser una labor subjetiva, cuyas normas de interpretación varían dependiendo de la experiencia del astrólogo y de su formación como tal.

Para realizar un horóscopo es necesario conocer la fecha, la hora y el lugar de nacimiento.

Las posiciones planetarias y de los demás cuerpos celestes se consideran desde una visión geocéntrica.

En el horóscopo se representan la posición de los planetas, pero también las posiciones resultantes de hacer una división de la franja zodiacal en 12 partes, que es lo que se denominan las "casas astrológicas". La primera de esas 12 casas astrológicas comienza por lo que se denomina el "Ascendente".

En el horóscopo se representan los planetas astrológicos, que incluyen todos los Astros del Sistema Solar, la Luna y el Sol incluidos. La razón de ello es que esos cuerpos celestes, que no son planetas desde el punto de vista de la astronomía, son llamados astros en la astrología el Sol y la Luna sin embargo son llamados luminarias.

El horóscopo se suele representar como un círculo dividido en 12 partes y compuesto primero de la división zodiacal.

La segunda de las divisiones se corresponde con las "casas astrológicas", que raramente tienen 30 grados exactos y que dependen del espacio geográfico en el que se calcula el horóscopo. Así, si 2 personas nacen en el mismo momento, pero una en Australia y otra en Inglaterra, sus respectivos horóscopos serán iguales en cuanto a posiciones planetarias, pero muy diferentes en cuanto a las casas astrológicas; y eso determinaría una interpretación astrológica muy diferente en ambos casos.

Las divisiones del círculo por "casas" y por "signos" son independientes, de forma que el comienzo de las "casas" puede coincidir con cualquier posición en la división por "signos".

Los planetas astrológicos se distribuyen por el círculo según su posición en el cielo en el momento en el que se calcula el horóscopo. En ocasiones, están muy agrupados, en otras están más o menos dispersos.

También se suelen representar en el horóscopo los "aspectos", que son una selección de algunas de las distancias angulares entre los planetas y/o puntos sensibles, como podría ser el ascendente. De esta manera, si la posición de Saturno y la del Sol están separados por 120 grados, se diría que forman el aspecto llamado Trígono. Si su separación fuera de 180 grados, formarían un aspecto de Oposición, etc.

Teniendo en cuenta las posibles posiciones de los planetas en las "casas" y los "signos y sus aspectos", se dan muchísimas combinaciones que determinan diferencias en el momento de interpretar astrológicamente un horóscopo.

La tradición de la creación de horóscopos proviene sobre todo de las creencias de la historia antigua. Por aquel entonces se creía que la posición y movimiento de los cuerpos celestes podían predecir futuros acontecimientos o el desarrollo de la personalidad de un ser humano. Por el contrario, estas suposiciones han sido refutadas en numerosas publicaciones científicas. Por esta razón, horóscopos y la práctica general de su composición así como la astrología misma, están asignados al Esoterismo.


</doc>
<doc id="4711" url="https://es.wikipedia.org/wiki?curid=4711" title="Mallorquín">
Mallorquín

El mallorquín (en mallorquín: "mallorquí") es la variedad del catalán que se habla en Mallorca. Es similar a las otras variantes que se hablan en las otras islas del archipiélago balear: el ibicenco ("eivissenc") en Ibiza y Formentera; y el menorquín ("menorquí") en Menorca.

Hay indicios de que antes de la conquista se hablaba en la isla, además de árabe, un romance local o "mozárabe" sin relación con el catalán, del que pervivirían algunos topónimos como Muro o Campos.

La lengua catalana fue introducida en Mallorca por los repobladores, tras ser conquistada para la Corona de Aragón en 1229. Los repobladores procedían de diversos lugares y llegaron en diferentes proporciones. Según el "Llibre del Repartiment", las tierras conquistadas fueron repartidas entre gente proveniente de Cataluña (39,71 %), de Occitania (24,26 %), Italia (16,19 %), Aragón (7,35 %), Navarra (5,88 %), Francia (4,42 %), Castilla (1,47 %) y Flandes (0,73 %). En 1230 se dictaron las Franquezas de Mallorca, privilegios que atrajeron a más repobladores para cultivar el campo. La nueva población de Mallorca provenía mayoritariamente de Cataluña, más específicamente del Rosellón y del Ampurdán, por lo cual se conservan características dialectales emparentadas con variantes de dichas zonas, como el uso del "article salat". Quizá por este origen, la lengua propia de Mallorca es un dialecto oriental del catalán. Respecto de la denominación de mallorquín ya en el siglo XV, en torno a 1450, el humanista Ferran Valentí (1415-1476) usó ya el nombre de lengua mallorquina en el prólogo de su traducción de las "Paradoxa" de Cicerón. 

La posición estratégica del archipiélago balear ayudó a que se convirtiera en puente para la expansión de la Corona de Aragón y en un centro de comercio marítimo, así, tanto el mallorquín como otros dialectos baleares cuentan con numerosos préstamos léxicos tomados de otros idiomas, como el francés, el italiano, el provenzal y el griego. En el siglo XVIII el dominio británico sobre Menorca introdujo algunas palabras de origen inglés, como: “"xoc"” —de “"chalk"”: “tiza”—, “"escrú"” —“"screw"”: “tornillo”—, etc.










</doc>
<doc id="4712" url="https://es.wikipedia.org/wiki?curid=4712" title="Habitación china">
Habitación china

La habitación china es un experimento mental, propuesto originalmente por John Searle y popularizado por Roger Penrose, mediante el cual se trata de rebatir la validez del test de Turing y de la creencia de que el pensamiento es simplemente computación.

Searle se enfrenta a la analogía entre mente y ordenador cuando se trata de abordar la cuestión de la conciencia. La mente implica no solo la manipulación de símbolos (gramática o sintaxis), sino que además posee una capacidad semántica para darse cuenta, o estar consciente, de los significados de los símbolos.

En 1995, cuando Herbert Simon y Allen Newell Simon escribieron que «Ahora hay máquinas que leen, aprenden y pueden crear», se trataba de dar a entender que se había dado una solución al problema mente-cuerpo.

Pero Searle en su texto de "Mentes, cerebros y ciencia" ataca este pensamiento, y con el experimento de la habitación china muestra cómo una máquina puede realizar una acción sin siquiera entender lo que hace y el por qué lo hace. Por lo tanto según Searle la lógica usada por las computadoras es nada más que una que no busca el contenido en la acción como la usada por los seres humanos.

Supongamos que han pasado muchos años, y que el ser humano ha construido una máquina aparentemente capaz de entender el idioma chino, la cual recibe ciertos datos de entrada que le da un hablante natural de ese idioma, estas entradas serían los signos que se le introducen a la computadora, la cual más tarde proporciona una respuesta en su salida. Supóngase a su vez que esta computadora fácilmente supera la prueba de Turing, ya que convence al hablante del idioma chino de que sí entiende completamente el idioma, y por ello el chino dirá que la computadora entiende su idioma.

Ahora Searle nos pide que supongamos que él está dentro de ese computador completamente aislado del exterior, salvo por algún tipo de dispositivo (una ranura para hojas de papel, por ejemplo) por el que pueden entrar y salir textos escritos en chino.

Supongamos también que fuera de la sala o computador está el mismo chino que creyó que la computadora entendía su idioma y dentro de esta sala está Searle que no sabe ni una sola palabra en dicho idioma, pero está equipado con una serie de manuales y diccionarios que le indican las reglas que relacionan los caracteres chinos (algo parecido a «Si entran tal y tal caracteres, escribe tal y tal otros»).

De este modo Searle, que manipula esos textos, es capaz de responder a cualquier texto en chino que se le introduzca, ya que tiene el manual con las reglas del idioma, y así hacer creer a un observador externo que él sí entiende chino, aunque nunca haya hablado o leído ese idioma.

Dada esta situación cabe preguntarse:


De acuerdo a los creadores del experimento, los defensores de la inteligencia artificial fuerte —los que afirman que programas de ordenador adecuados pueden comprender el lenguaje natural o poseer otras propiedades de la mente humana, no simplemente simularlas— deben admitir que, o bien la sala comprende el idioma chino, o bien el pasar el test de Turing no es prueba suficiente de inteligencia. Para los creadores del experimento ninguno de los componentes del experimento comprende el chino, y por tanto, aunque el conjunto de componentes supere el test, el test no confirma que en realidad la persona entienda chino, ya que como sabemos Searle no conoce ese idioma.

Esto es así en el contexto de la siguiente argumentación:


Una puntualización importante: Searle no niega que las máquinas puedan pensar —el cerebro es una máquina y piensa—, niega que al hacerlo apliquen un programa.

Argumentos como los de Searle en la filosofía de mente desataron un debate más intenso sobre la naturaleza de la inteligencia, la posibilidad de máquinas inteligentes y el valor de la prueba de Turing que continuó durante las décadas de los 80 y 90. El experimento mental de la habitación china confirmaría la premisa 2, a juicio de sus defensores. A juicio de sus detractores, la premisa 2 basada en la inferencia a partir del experimento mental no es concluyente. Las objeciones suelen seguir una de las tres líneas siguientes:


No hay razón para decir que estos modelos solo exhiben una comprensión aparente, como en el caso de la habitación y su habitante, pero son modelos de inteligencia artificial.



El filósofo fisicalista William Lycan reconoció el avance de las inteligencias artificiales, comenzando a comportarse como si tuvieran mentes. Lycan usa el experimento mental de un robot humanoide llamado Harry que puede conversar, jugar golf, tocar la viola, escribir poesía y por consiguiente consigue engañar a la gente como si fuera una persona con mente. Si Harry fuera humano, sería perfectamente natural pensar que tiene pensamientos o sentimientos, lo que sugeriría que en realidad Harry pueda tener pensamientos o sentimientos aún si es un robot. Para Lycan "no hay ningún problema ni objeción a la experiencia cualitativa en máquinas que no es igualmente un dilema para tal experiencia en humanos".







</doc>
<doc id="4714" url="https://es.wikipedia.org/wiki?curid=4714" title="DocBook">
DocBook

DocBook es una aplicación del estándar SGML/XML e incluye una DTD propia y que se utiliza de manera más destacada en el área de la documentación técnica, especialmente para documentar todo tipo de material y programas informáticos. Existe un Comité Técnico de DocBook en OASIS (originalmente "SGML Open") que mantiene y actualiza este estándar. DocBook inicialmente comenzó como una DTD de SGML, pero a partir de la versión 4 existe un equivalente para XML.

Como lenguaje semántico que es, DocBook nos permite crear documentos en un formato neutro, independiente de la presentación. En este formato neutro se recogen tanto el contenido como la estructura lógica del mismo, permitiendo así que pueda ser publicado (presentado) automáticamente en multitud de formatos: HTML, XHTML, EPUB, PDF, man pages , HTML Help, etc., simplemente aplicando "plantillas" de presentación, sin que sea necesario ningún cambio sobre el documento original.

DocBook es un lenguaje XML. En su versión actual (5.0), está formalmente definido por un esquema RELAX NG con reglas Schematron integradas. (Existen también un XML Schema+Schematron y un DTD, pero actualmente se consideran no estándares.)

Los documentos DocBook no describen ni la apariencia ni la presentación de sus contenidos, sino únicamente el "sentido" de dichos contenidos. Por ejemplo, en lugar de indicar exactamente cómo ha de visualizarse una determinada frase que es el título de un capítulo, DocBook simplemente indica que dicha frase "es" un título de capítulo. Posteriormente, el decidir dónde y cómo se ha de mostrar dicho título dentro de la página será tarea de una herramienta procesadora externa o de la aplicación visualizadora que estemos manejando.

DocBook dispone de un gran número de etiquetas para describir elementos semánticos, englobándose estas en tres grandes categorías: estructurales, de bloque y de línea.

Las etiquetas estructurales especifican características generales de sus contenidos. Por ejemplo, el elemento "book" especifica que sus elementos hijo serán partes de un libro: "títulos", "capítulos", "glosarios", "apéndices", etc.
Algunas etiquetas estructurales son:
Los elementos estructurales pueden contener a otros elementos estructurales, pero han de ser siempre elementos de primer nivel dentro de un documento DocBook.

Las etiquetas de bloque representan elementos tales como "párrafos", "listas", etc., y no todos ellos han de contener necesariamente texto en su interior. Estos elementos de bloque suelen ir distribuidos secuencialmente, y serán visualizados uno "debajo" de otro. (Aunque "debajo" puede variar dependiendo del entorno cultural: en la mayoría de lenguajes occidentales "debajo" significará un sentido descendente en la página; pero en algunos lenguajes orientales, "debajo" significará un sentido en columnas de derecha a izquierda. La especificación DocBook es completamente neutral a ese tipo de conceptos intrínsecos de cada cultura.)

Las etiquetas de línea representan elementos tales como "letras enfatizadas", "hyper-enlaces", etc, y se suelen aplicar a porciones de texto en el interior de un elemento de bloque, provocando habitualmente que la herramienta procesadora de presentación aplique algún tipo de tratamiento tipográfico especial a dichas porciones. (La especificación de DocBook indica que se espera un tratamiento tipográfico especial, pero no indica exactamente qué tratamiento específico se debe aplicar. Por ejemplo, "letras enfatizadas" no implica necesariamente "letras en cursiva"; la herramienta de presentación puede optar por aumentarles el tamaño de letra o por cambiarles el color de fondo.)

<?xml version="1.0" standalone="no"?>

Desde un punto de vista semántico, este documento es un "artículo" (article), con su respectivo "título" (title). Se identifica claramente al "autor" (author), y se podría haber incluido también otro tipo de "información" adicional (articleinfo). Este artículo de ejemplo consta de una sola "sección", también con su respectivo "título"; y con "párrafos" (paragraph) de texto.

 <?xml version="1.0" standalone="no"?>

Desde un punto de vista semántico, este documento es un "libro" (book), con un "título" (title); consta de dos "capítulos" (chapter), cada uno de ellos con sus propio "título", y estos "capítulos" tienen "párrafos" (paragraph) de texto. Todo ello expresado en un formato fácilmente comprensible por humanos.

Tanto los distintos elementos que forman un DocBook, como las reglas para combinarlos (por ejemplo, que todo elemento "libro" ha de contener un elemento "título", previo a cualquier otro elemento estructural tal como "capítulo"), se definen formalmente en un esquema (schema), de tal forma que los programas informáticos pueden validar el documento contra dicho esquema, y determinar así, en todo momento, si el documento está "bien formado".

Como documentos XML que son, los documentos DocBook pueden ser escritos con cualquier editor de texto, aunque siempre será más sencillo escribirlos con un editor XML, o, mejor aún, con un editor XML que lleve integrados los esquemas específicos de DocBook. Por ejemplo Emacs, trabajando en modo nXML, o XML Copy Editor.

También existen editores más "visuales" (WYSIWYG), tales como XMLmind Editor ("XXE"), Oxygen XML Editor, capaz de representar los documentos DocBook formateándolos con CSS; o Syntext Serna, que realiza transformaciones XSL en tiempo real.

Asimismo, como documentos XML que son, los documentos DocBook pueden ser validados y procesados automáticamente por cualquier herramienta o lenguaje de programación que soporte XML.

Estas herramientas se suelen utilizar para crear documentos de salida en un amplio abanico de formatos, habitualmente haciendo uso de "hojas de estilo" DocBook XLS, un tipo de hojas XSLT que nos permiten transformar documentos DocBook a otros formatos tales como HTML, PDF, etc., permitiendo conversiones tan sofisticadas como para que contemplen la generación automática de tablas de contenido, de glosarios y/o de índices, o que permitan incluso filtrados previos de contenidos, extractando solo ciertas partes del DocBook original.

DocBook es muy utilizado en algunos contextos, entre los que destacan Linux Documentation Project (Proyecto de documentación Linux), las referencias de las APIs de GNOME y GTK+, así como la documentación del núcleo Linux. Las páginas "man" del Entorno Operativo Solaris se generan también a partir de documentos que utilizan las DTDs o los esquemas de DocBook.

DocBook nació en 1991, de un proyecto conjunto de HAL Computer Systems y O'Reilly & Associates, evolucionando posteriormente hasta tener su propia organización (el Grupo Davenport), para acabar, en 1998, siendo gestionado por el consorcio "SGML Open", que más tarde se convertiría en la organización OASIS, en cuyo seno existe actualmente el "DocBook Technical Committee", encargado del mantenimiento del estándar DocBook.

La especificación DocBook está disponible tanto en el formato SGML como en el formato XML, estando definida tanto por un documento DTD, como por un esquema RELAX NG - W3C XML Schema. A partir de la versión 5, el esquema RELAX NG es el "normativo", siendo el resto de formatos meras adaptaciones del mismo.

DocBook nació como una aplicación de SGML, pero actualmente su adaptación XML la ha sustituido en la mayoría de usos. (A partir de la versión 4 del DTD SGML, la versión DTD XML ha tomado su propio camino y su propio esquema de numeración.)

En un principio, el uso del formato DocBook estaba prácticamente reducido al grupo de compañías participantes en su diseño, pero en estos momentos ha sido ampliamente adoptado por toda la comunidad de software libre y por un amplio espectro de compañías, existiendo en el mercado multitud de herramientas que hacen uso de él.

Norman Walsh y el equipo de desarrollo del DocBook Open Repository mantienen un conjunto de hojas de estilo DSSSL y XSL para generar versiones PDF y HTML de documentos DocBook (así como para desarrollar otros formatos, incluyendo páginas de referencia "man" y de ayuda en HTML). Walsh es también el principal autor del libro "DocBook: The Definitive Guide" (DocBook: La Guía Definitiva), la documentación oficial de DocBook. Este libro se puede obtener bajo licencia GFDL o en su versión impresa (ISBN 1565925807), editada por O'Reilly & Associates."



</doc>
