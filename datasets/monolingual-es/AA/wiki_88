<doc id="19220" url="https://es.wikipedia.org/wiki?curid=19220" title="Ácido acetilsalicílico">
Ácido acetilsalicílico

El ácido acetilsalicílico o AAS (CHO), conocido popularmente como aspirina, nombre de una marca que pasó al uso común, es un fármaco de la familia de los salicilatos. Se utiliza como medicamento para tratar el dolor (analgésico), la fiebre (antipirético) y la inflamación (antiinflamatorio), debido a su efecto inhibitorio, no selectivo, de la ciclooxigenasa.

Se utiliza también para tratar inflamaciones específicas tales como la enfermedad de Kawasaki, la pericarditis o la fiebre reumática. La administración de aspirina poco después de un ataque al corazón disminuye el riesgo de muerte y su uso a largo plazo ayuda a prevenir ataques cardíacos, accidentes cerebrovasculares y coágulos de sangre en personas con alto nivel de riesgo. Puede disminuir el riesgo de padecer ciertos tipos de cáncer, en especial el cáncer colorrectal. En el tratamiento del dolor o la fiebre, sus efectos comienzan de forma típica a los treinta minutos. El ácido acetilsalicílico es el antiinflamatorio no esteroideo (AINE) por excelencia y funciona de forma similar a otros AINE, aunque bloquea el normal funcionamiento de las plaquetas (antiagregante plaquetario).

Entre los efectos secundarios comunes se encuentra la dispepsia y entre los efectos secundarios más importantes la úlcera péptica, la perforación del estómago y el empeoramiento del asma. El riesgo de hemorragia aumenta en personas mayores, consumidores de alcohol, de otros antiinflamatorios no esteroideos o de anticoagulantes. La aspirina no está recomendada en mujeres que se encuentren en la última fase del embarazo. En general, tampoco está recomendada en niños con infecciones, debido al riesgo de sufrir el síndrome de Reye, y en dosis altas puede provocar tinnitus (zumbido en los oídos).

El ácido salicílico, presente en las hojas del sauce ("Salix"), ha sido utilizado por la humanidad desde hace por lo menos 2400 años. El ácido acetilsalicílico fue sintetizado por primera vez por el químico francés Charles Frédéric Gerhardt en 1853, al combinar el salicilato de sodio con cloruro de acetilo. En la segunda mitad del siglo XIX otros químicos describieron su estructura química e idearon métodos más eficientes para su síntesis. En 1897, los científicos de Bayer comenzaron a estudiar la aspirina como un posible reemplazo menos irritante que los medicamentos de salicilato comunes. Aunque antes de 1899, Bayer había llamado al fármaco «Aspirina» y la comercializaba bajo esa marca en todo el mundo, los derechos de la empresa sobre la marca se perdieron o vendieron en muchos países. Su popularidad creció durante la primera mitad del siglo XX, lo que condujo a una fuerte competencia entre distintas marcas y productos cuyo principio activo era el ácido acetilsalicílico.

La aspirina es uno de los medicamentos más utilizados en el mundo, con un consumo estimado en 40 000 toneladas anuales, o lo que es lo mismo, entre 50 000 y 120 000 millones de pastillas. Está en la , donde se clasifican los medicamentos básicos que todo sistema de salud debería tener. La aspirina también está disponible como medicamento genérico y el coste al por mayor en los países desarrollados en 2014 era de entre 0,002 y 0,025 dólares (USD) por dosis. En el caso de Estados Unidos, en 2015 un mes de medicación tenía un coste de media menor a 25 dólares (USD).

Los antiguos egipcios pueden haber utilizado la corteza del sauce blanco (cuyo nombre latino es "Salix alba") para fines medicinales.

Los sumerios y los chinos usaban las hojas de sauce como analgésico antes del 1000 a. C.

La primera mención se encuentra en los textos de Hipócrates (460-370 a. C.), padre de la medicina griega, que usaba un brebaje extraído de hojas y corteza del sauce "Salix Latinum" para aliviar los dolores y la fiebre de sus pacientes,

También en alguna cultura amerindia (en el continente americano) se puede haber utilizado la corteza del sauce blanco para fines medicinales.

Existen evidencias de que entre los pueblos hispanorromanos se contaba con algún posible ejemplo del uso y sacralidad del sauce.

Los efectos medicinales del sauce blanco continuaron siendo mencionados por autores antiguos como el polígrafo y naturalista romano Plinio el Viejo (23-79), el médico y farmacéutico grecoturco Dioscórides (40-90) o el célebre médico griego Galeno (130-200).

Durante la Edad Media se hervía la corteza del sauce y la daban a beber a la gente que sufría de dolencias. Sin embargo este brebaje divino paso al olvido debido a una ley que restringió el descortezamiento y corte de hojas de este sauce, ya que eran utilizadas en la industria cestera.

En la época posrenacentista (1763) Edward Stone, reverendo de la Iglesia de Inglaterra, presentó un informe a lord Macclesfield, quien presidía la Royal Society, referente a estas propiedades terapéuticas de la corteza de sauce blanco destacando su efecto antipirético.
Stone describió en su trabajo que había administrado el extracto en forma de té o cerveza a 50 pacientes febriles, aliviándoles el síntoma.
Investigaciones posteriores condujeron al principio activo de esta planta, que los científicos llamaron "salicilina", un precursor del ácido salicílico y del ácido acetilsalicílico.

El principio activo de la corteza de sauce fue aislado en 1828 por Johann Buchner, profesor de Farmacia en la Universidad de Múnich, quien relató que se trataba de una sustancia amarga y amarillenta, en forma de agujas cristalinas que llamó "salicina".
Sin embargo, dos años antes, los italianos Brugnatelli y Fontana aislaron ese mismo extracto, pero en forma muy impura, y no lograron demostrar que la sustancia era la causante de los efectos farmacológicos de la corteza de sauce blanco.
En 1829 un farmacéutico francés, Henri Leroux, improvisó un procedimiento de extracción del que obtuvo 30 gramos de salicilina a partir de 1,5 kg de corteza. En 1838 Raffaele Piria (químico italiano), trabajando en La Sorbona de París logró separar la "salicina" en azúcar y un componente aromático llamado "salicilaldehído". A este último compuesto lo convirtió, por hidrólisis y oxidación, en cristales incoloros a los que puso por nombre ácido salicílico.

El ácido acetilsalicílico fue sintetizado por primera vez por el químico francés Charles Frédéric Gerhardt en 1853, queriendo mejorar el sabor amargo y otros efectos secundarios del ácido salicílico como la irritación de las paredes del estómago, al combinar el salicilato de sodio con cloruro de acetilo; y luego en forma de sal por Hermann Kolbe en 1859.
No obstante, hubo que esperar hasta 1897 para que el farmacéutico alemán Felix Hoffmann, investigador de los laboratorios Bayer y que, buscando un alivio eficaz contra los dolores que su padre sufría por un reumatismo crónico tratado con ácido salicílico además de importantes efectos secundarios, consiguiera sintetizar al ácido acetilsalicílico con gran pureza.
Sus propiedades terapéuticas como analgésico y antiinflamatorio fueron descritas en 1899 por el farmacólogo alemán Heinrich Dreser, lo que permitió su comercialización.

Muchos años después, en 1949, el que fuera jefe directo de Hoffmann, Arthur Eichengrün publica un artículo reivindicando el descubrimiento.
Se trataría de algo realmente sorprendente, que alguien reclame para sí un mérito 50 años después, cuando la aspirina ya llevaba décadas convertida en un fármaco famoso en todo el mundo. De hecho esta reivindicación fue ignorada por los historiadores científicos hasta 1999, fecha en la que el investigador de Walter Sneader de la Universidad de Strathclyde (en Glasgow), volvió a postular que fue Eichengrün quien tuvo la idea de sintetizar el ácido acetilsalicílico.
En todo caso, la casa Bayer, a la que importaría poco si los méritos deberían caer en uno u otro de sus empleados, y que lógicamente tiene todos los documentos que afectan al caso, refutó en un comunicado de prensa esta hipótesis, pero la controversia sigue abierta.

Aspirina fue el nombre comercial acuñado por los laboratorios Bayer para esta sustancia, convirtiéndose en el primer fármaco del grupo de los AINE (antiinflamatorios no esteroideos). Posteriormente, en 1971, el farmacólogo británico John Robert Vane, entonces empleado del Royal College of Surgeons (Colegio Real de Cirujanos) de Londres, pudo demostrar que el AAS suprime la producción de prostaglandinas y tromboxanos, lo que abrió la posibilidad de su uso en bajas dosis como antiagregante plaquetario, ampliando enormemente su campo comercial y compensando el hecho de que, en la actualidad, su uso como antiinflamatorio de elección haya sido desplazado por otros AINE más eficaces y seguros.
En 1985 la secretaria del Servicio de Salud de Estados Unidos, Margaret Heckler, anunció que la dosis de una aspirina diaria ayudaba en personas que habían sufrido un infarto de miocardio a prevenir nuevos ataques de isquemia coronaria.
Durante la Primera Guerra Mundial (1914-1919), la marca «aspirina» fue expropiada en los países ganadores, fundamentalmente Reino Unido, Estados Unidos y Francia; de tal manera que en estos países "aspirin" pasó a ser el nombre genérico de la sustancia.

La aspirina hoy es un fármaco registrado en más de 70 países del mundo.
Desde su comercialización se han consumido más de trescientos cincuenta billones de comprimidos y se estima que el consumo diario es de unos cien millones de aspirinas.
Consecuentemente, es uno de los fármacos más usados en el mundo, con un consumo estimado de más de 100 toneladas métricas diarias.
Actualmente, el 100 % de la producción mundial de ácido acetilsalicílico manufacturada por Bayer se realiza en Langreo, España, en una planta química de esta empresa multinacional.
Desde allí se envía a diferentes partes del mundo donde se preparan los comprimidos y diferentes formas farmacéuticas en las que se vende la Aspirina.

El ácido salicílico o salicilato, producto metabólico de la aspirina, es un ácido orgánico simple con un pK de 3,0. La aspirina, por su parte, tiene un pK de 3,5 a 25 °C.
Tanto la aspirina como el salicilato sódico son igualmente efectivos como antiinflamatorios, aunque la aspirina tiende a ser más eficaz como analgésico.

En la producción del ácido acetilsalicílico, se protona el oxígeno para obtener un electrófilo más fuerte.

La reacción química de la síntesis de la aspirina se considera una esterificación. El ácido salicílico es tratado con anhídrido acético, un compuesto derivado de un ácido, lo que hace que el grupo hidroxilo del salicilato se convierta en un grupo acetilo (salicilato-OH → salicilato-OCOCH). Este proceso produce aspirina y ácido acético, el cual se considera un subproducto de la reacción.
La producción de ácido acético es la razón por la que la aspirina con frecuencia huele como a vinagre.

Como catalizador casi siempre se usan pequeñas cantidades de ácido sulfúrico y ocasionalmente ácido fosfórico. El método es una de las reacciones más usadas en los laboratorios de química en universidades de pregrado.

El ácido acetilsalicílico se administra principalmente por vía oral, aunque también existe para uso por vía rectal, por vía intramuscular y por vía intravenosa.
Los comprimidos de aspirina para administración oral se hidrolizan con facilidad cuando se ven expuestos al agua o aire húmedo, de modo que deben permanecer almacenados en sus envoltorios hasta el momento de su administración. La aspirina que se ha hidrolizado así despide un olor a vinagre (en realidad es ácido acético) y no debe ingerirse. La aspirina también viene en preparados masticables para adultos. Los preparados efervescentes y saborizados son aptos para quienes prefieran la administración líquida del medicamento.

Es mayor la probabilidad de problemas severos del estómago con la aspirina que no tiene recubrimiento entérico.

La aspirina tiene muy escasa solubilidad en condiciones de pH bajo ―como dentro del estómago―, hecho que puede retardar la absorción de grandes dosis del medicamento unas 8-24 horas. Todos los salicilatos, incluyendo la aspirina, se absorben rápidamente por el tracto digestivo a la altura del duodeno y del intestino delgado, alcanzando la concentración máxima en el plasma sanguíneo al cabo de 1 a 2 horas.
Por ser un ácido débil, muy poco queda remanente en forma ionizada en el estómago después de la administración oral del ácido salicílico. Debido a su baja solubilidad, la aspirina se absorbe muy lentamente en casos de sobredosis, haciendo que las concentraciones plasmáticas aumenten de manera continua hasta 24 horas después de la ingesta.
La biodisponibilidad es muy elevada, aunque la absorción tiende a ser afectada por el contenido y el pH del estómago.

La unión del salicilato a las proteínas plasmáticas es muy elevada, superior al 99 %, y de dinámica lineal.
La saturación de los sitios de unión en las proteínas plasmáticas conduce a una mayor concentración de salicilatos libres, aumentando el riesgo de toxicidad. Presenta una amplia distribución tisular, atravesando las barreras hematoencefálica y placentaria. La vida media sérica es de aproximadamente 15 minutos. El volumen de distribución del ácido salicílico en el cuerpo es de 0,1-0,2l/kg. Los estados de acidosis tienden a incrementar el volumen de distribución porque facilitan la penetración de los salicilatos a los tejidos.

La aspirina se hidroliza parcialmente a ácido salicílico debido al "efecto de primer paso" en el hígado (gran parte del ácido acetilsalicílico no llega a la circulación sistémica ya que pasa del sistema digestivo al hígado gracias a la "vena porta)". Este metabolismo hepático está sujeto a mecanismos de saturación, por lo que al superarse el umbral, las concentraciones de la aspirina aumentan de manera desproporcionada en el organismo. También es hidrolizada a ácido acético y salicilato por esterasas en los tejidos y la sangre.

La aspirina pasa por el hígado, siendo después absorbida por el torrente sanguíneo ayudando así a calmar el dolor y malestar general.

Los mecanismos biológicos para la producción de la inflamación, dolor o fiebre son muy similares. En ellos intervienen una serie de sustancias que tienen un final común. En la zona de la lesión se generan unas sustancias conocidas con el nombre de prostaglandinas. Se las podría llamar también "mensajeros del dolor". Estas sustancias informan al sistema nervioso central de la agresión y se ponen en marcha los mecanismos biológicos de la inflamación, el dolor o la fiebre. En 1971 el farmacólogo británico John Robert Vane demostró que el ácido acetilsalicílico actúa interrumpiendo estos mecanismos de producción de las prostaglandinas y tromboxanos.
Así, gracias a la utilización de la aspirina, se restablece la temperatura normal del organismo y se alivia el dolor. La capacidad de la aspirina de suprimir la producción de prostaglandinas y tromboxanos se debe a la inactivación irreversible de la ciclooxigenasa (COX), enzima necesaria para la síntesis de esas moléculas proinflamatorias. La acción de la aspirina produce una acetilación (es decir, añade un grupo acetilo) en un residuo de serina del sitio activo de la COX.

La aspirina es un inhibidor no selectivo de ambas isoformas de la ciclooxigenasa, pero el salicilato, el producto metabólico normal de la aspirina en el cuerpo, es menos eficaz en la inhibición de ambas isoformas. Los salicilatos que no son acetilados pueden tener funciones en la eliminación de radicales del oxígeno. La aspirina inhibe irreversiblemente a la COX-1, modifica la actividad enzimática de la COX-2 e inhibe la agregación plaquetaria, no así las especies no acetiladas del salicilato.
Por lo general, la COX-2 produce los prostanoides, la mayoría de los cuales son proinflamatorios. Al ser modificada por la aspirina, la COX-2 produce en cambio lipoxinas, que tienden a ser antiinflamatorias. Los AINE (antiinflamatorios no esteroideos) más recientes se han desarrollado para inhibir la COX-2 solamente y así reducir los efectos secundarios gastrointestinales de la inhibición de la COX-1.

La aspirina también interfiere con los mediadores químicos del sistema calicreína-cinina, por lo que inhibe la adherencia de los granulocitos sobre la vasculatura que ha sido dañada, estabiliza los lisosomas evitando así la liberación de mediadores de la inflamación e inhibe la quimiotaxis de los leucocitos polimorfonucleares y macrófagos.

La aspirina es más eficaz reduciendo el dolor leve o de moderada intensidad por medio de sus efectos sobre la inflamación y porque es probable que pueda inhibir los estímulos del dolor a nivel cerebral subcortical. Es un ácido orgánico débil que tiene al mismo tiempo una función de ácido carboxílico y de fenol ya que también se le considera el orto fenol del ácido benzoico (su nombre es ortofenometiloico). Tiene características antiinflamatorias pero debido a que provoca irritaciones estomacales no se aplica como tal sino en forma de sus derivados, siendo los más conocidos el ácido acetilsalicílico ("Aspirina") y el salicilato de metilo (el éster con el alcohol metílico).

La aspirina reduce la fiebre, mientras que su administración solo afecta ligeramente a la temperatura normal del cuerpo. Los efectos antipiréticos de la aspirina probablemente están mediados tanto por la inhibición de la COX en el sistema nervioso central como por la inhibición de la interleucina-1, liberada por los macrófagos durante los episodios de inflamación.

Se ha demostrado que la aspirina interrumpe la fosforilación oxidativa en las mitocondrias de los cartílagos y del hígado al difundir al espacio que está entre las dos membranas de la mitocondria y actuar como transportador de los protones requeridos en los procesos de la respiración celular.
Con la administración de dosis elevadas de aspirina se observa la aparición de fiebre debido al calor liberado por la cadena de transporte de electrones que se encuentra en la membrana interna de las mitocondrias, contrariamente a la acción antipirética de la aspirina a dosis terapéuticas. Además, la aspirina induce la formación de radicales de óxido nítrico (NO) en el cuerpo, lo cual reduce la adhesión de los leucocitos, uno de los pasos importantes en la respuesta inmune a infecciones, aunque aún no hay evidencias concluyentes de que la aspirina sea capaz de combatir una infección.

Datos publicados recientemente sugieren que el ácido salicílico y otros derivados de la aspirina modulan sus acciones de señalización celular por medio del NF-κB, un complejo de factores de transcripción que juegan un papel importante en muchos procesos biológicos, incluida la inflamación.

Las dosis bajas de aspirina, de 81 mg diarios, producen una leve prolongación en el tiempo de sangrado, que se duplica si la administración de la aspirina continúa durante una semana. El cambio se debe a la inhibición irreversible de la COX de las plaquetas, por lo que se mantiene durante toda la vida de las mismas (entre 8 y 10 días).
Esa propiedad anti agregante plaquetaria hace que la aspirina sea útil en la reducción de la incidencia de infartos en algunos pacientes.
Son suficientes 40 mg de aspirina al día para inhibir una proporción adecuada de tromboxano A, sin que tenga efecto inhibitorio sobre la síntesis de prostaglandina I2, por lo que se requerirán mayores dosis para surtir efectos antiinflamatorios.

En el año 2008 un ensayo demostró que la aspirina no reduce el riesgo de aparición de un primer ataque cardiaco o accidente cerebrovascular, sino que reduce el riesgo de un segundo evento para quienes ya han sufrido un ataque cardiaco o un accidente cerebrovascular. En mujeres que toman dosis bajas de aspirina cada dos días se disminuye el riesgo de un accidente cerebrovascular, pero no es un tratamiento que pueda alterar sustancialmente el riesgo de un infarto o muerte cardiovascular.
En general, para un paciente que no tiene enfermedad cardíaca, el riesgo de sangrado supera cualquier beneficio de la aspirina.

Las interacciones farmacocinéticas van a cubrir prácticamente todo el espectro de posibilidades en cuanto al mecanismo de producción, aunque se muestran como más interesantes las de origen metabólico. En este sentido, parece ser independiente de la CYP3A4 y, al igual que otros AINE, estar ligada a la CYP2C9. No obstante, su abundante metabolismo al margen del hígado, hace que no sean fundamentales sus interacciones a nivel del citocromo P450. Las más interesantes se muestran en la siguiente tabla:

Finalmente, la siguiente tabla muestra las interacciones no farmacológicas, manifestadas como alteraciones de los resultados de las pruebas de laboratorio:

En pacientes con cardiopatías establecidas, el ibuprofeno puede interferir con los efectos cardioprotectores de la aspirina cuando se administran ambos medicamentos al mismo tiempo.

Sin embargo, no es efectiva para el dolor visceral severo. La aspirina y otros AINE (antiinflamatorios no esteroideos) se han combinado con analgésicos opioides para el tratamiento del dolor causado por el cáncer, donde los efectos antiinflamatorios actúan sinérgicamente con los opioides para aumentar la analgesia. La combinación de aspirina con oxicodona —una clase de analgésicos narcóticos— se usa para aliviar desde el dolor moderado al moderadamente intenso.

La aspirina disminuye la incidencia de ataques isquémicos, la angina inestable, trombosis de una arteria coronaria con infarto agudo de miocardio y la trombosis secundaria a un baipás coronario con una dosis diaria propuesta (en 2018) de 75-162mg.

Ciertos estudios epidemiológicos sugieren que el uso a largo plazo de la aspirina a bajas dosis se asocia con una reducción en la incidencia del cáncer colorrectal, así como el cáncer de pulmón, posiblemente por su asociación con efectos inhibitorios sobre la COX producida por adenocarcinomas, efectos supresores de prostaglandinas o incluso efectos directamente antimutagénicos.
También se ha estudiado el papel que juega la aspirina en reducir la incidencia de otras formas de cáncer. En varios estudios se demostró que la aspirina no reduce la aparición del cáncer de próstata.
Sus efectos en la prevención del cáncer de páncreas son mixtos, un estudio de 2004 encontró un aumento estadísticamente importante en el riesgo de contraer cáncer pancreático en mujeres que tomaban aspirina, mientras que un metaanálisis de varios estudios publicado en 2006 no encontró evidencias concluyentes de que la aspirina u otros AINE (antiinflamatorios no esteroideos) estuvieran asociados a un riesgo aumentado de esta forma de cáncer.
Es posible que la aspirina también tenga efectos positivos sobre el cáncer del tracto digestivo superior, pero las evidencias siguen aun siendo inconclusas.

Se ha hipotetizado que la aspirina es capaz de reducir la formación de cataratas en pacientes diabéticos, aunque en al menos un estudio se demostró que no es eficaz en esa acción.

Según científicos del Instituto de Neurociencia y Fisiología de la Universidad de Goteborg, en Suecia, una dosis diaria de aspirina puede ayudar a reducir el deterioro cerebral en personas de edad avanzada. No obstante Los expertos subrayan que «debido a los efectos secundarios peligrosos que puede tener la 'Aspirina', no se puede recomendar su uso masivo a fin de proteger la memoria».

Las dosis óptimas para alcanzar los efectos analgésicos o antipiréticos de la aspirina son menores de 0,6 a 0,65 gramos por vía oral. Dosis más elevadas pueden prolongar el efecto. La dosis habitual suele ser repetida cada 4 horas. La dosis antiinflamatoria en niños es de 50 a 75 mg por cada kg de peso del niño cada día, dividida en varias dosis durante el día. La dosis de entrada promedio para un adulto es de 45 mg/kg/día en dosis divididas.

A las dosis habituales, los efectos adversos más comunes de la aspirina son la irritación gástrica, náuseas, vómitos, úlcera gástrica o duodenal, mientras que la hepatotoxicidad, asma, cambios en la piel y nefrotoxicidad son menos frecuentes. Se ha reportado que ocurre adaptación de la mucosa en pacientes con úlceras asociadas a la administración de aspirina de modo que se ha demostrado el mejoramiento espontáneo con el tiempo sin descontinuar la toma de la aspirina.

Su ingesta abusiva produce nefritis, que son los procesos inflamatorios y/o disfuncionales del riñón y vasodilatación periférica por acción directa sobre el músculo liso. A dosis altas algunos pacientes han reportado vómitos, acúfenos, disminución de la audición, delirio, psicosis, estupor y vértigo los cuales son reversibles al reducir la dosis. A dosis aún mayores de salicilatos aparece una respiración profusa y coma resultado de un efecto directo sobre el bulbo raquídeo.

A niveles tóxicos de salicilatos se presenta alcalosis respiratoria seguida de acidosis metabólica por acumulación del salicilato, depresión respiratoria, cardiotoxicidad e intolerancia a la glucosa. Dos gramos o aún menos de aspirina al día pueden aumentar los niveles de ácido úrico en sangre, mientras que las dosis que sobrepasan los 4 gramos diarios disminuyen los uratos. Igual que el resto de los AINE (antiinflamatorios no esteroideos), la aspirina puede causar una elevación en las enzimas hepáticas, hepatitis, disfunción renal, sangrado y asma.

Algunas personas se ven menos afectadas que otras por el efecto antiagregante plaquetario de la aspirina. Un estudio ha sugerido que las mujeres son más resistentes a los efectos plaquetarios de la aspirina que los hombres y en otro estudio, cerca de un 30 % de los pacientes evaluados eran así de resistentes a la acción secundaria de la aspirina.

En ciertas ocasiones, puede aparecer un ligero sangrado gastrointestinal, por lo general causado por una gastritis erosiva que, con el tiempo, puede producir una deficiencia de hierro. En su forma comercial, no se debe administrar en niños menores de 12 años que padezcan gripe o varicela (por lo general se usa paracetamol en vez) y/o usar en conjunto con otros salicilatos, ya que puede conducir al Síndrome de Reye, enfermedad rara, pero muy grave.
La administración de aspirina durante un cuadro de dengue no se recomienda por razón de un aumento en el riesgo de hemorragias.

La administración de aspirina en madres antes del parto puede causar trastornos hemostásicos en recién nacidos, incluyendo petequias, hematuria, cefalohematoma, hemorragia conjuntival y sangrado durante o después de una circuncisión. Por su parte, las madres pueden presentar con sangrado confinado al período intraparto o puerperio. Por ello, la administración de aspirina debe ser evitada durante el embarazo y si se sospecha que la madre ha tomado aspirina en los cinco días antes del parto, el recién nacido debe ser evaluado para descartar sangrados.

Respecto a otras reacciones adversas (RAM), y a modo de resumen, se incluye una tabla siguiendo los criterios de la CIOSM.

De forma global, puede afirmarse que entre el 5 % y el 7 % de los pacientes experimenta algún tipo de efecto adverso.

Tomar más de 150mg/kg de aspirina puede provocar resultados graves e incluso mortales si no se recibe tratamiento. Para un adulto pequeño, eso equivale aproximadamente a tomar 20 tabletas que contienen 325mg de aspirina. Los niños pueden resultar afectados con niveles mucho más bajos.

Los pacientes con una sobredosis accidental o intencional de aspirina son sometidos a un lavado gástrico con carbón activado y se instala una producción abundante de orina alcalina. De presentar trastornos como hipertermia o desequilibrios en los electrolitos, deben ser restablecidos.
En las intoxicaciones severas, puede que sea necesaria la hemodiálisis o, rara vez, la ventilación asistida. A menudo se emplean infusiones de bicarbonato de sodio para alcalinizar la orina conllevando a un aumento en el salicilato excretado fuera del cuerpo.

La acción antiplaquetaria de la aspirina hace que esté contraindicada en pacientes con hemofilia.
Aunque en el pasado no se recomendaba el uso de la aspirina durante el embarazo, la aspirina puede tener utilidad en el tratamiento de la preeclampsia e incluso en la eclampsia.

La aspirina no debe administrarse en personas con antecedentes alérgicos al ibuprofeno o al naproxeno, o quienes sean de alguna forma intolerantes a los salicilatos o a los AINE (antiinflamatorios no esteroideos) y se debe ejercer moderación en la prescripción de aspirina a pacientes asmáticos o con broncoespasmo inducida por los AINE. Por motivo de su acción sobre la mucosa estomacal, se recomienda que los pacientes con enfermedad renal, úlceras pépticas, diabetes, gota o gastritis consulten con un profesional de la salud antes de tomar aspirina.
Aún en la ausencia de estas enfermedades, siempre existe el riesgo de sangrado gastrointestinal cuando se combina la aspirina con el licor o la warfarina.

Se ha demostrado que la aspirina puede causar anemia hemolítica en pacientes con deficiencia de glucosa-6-fosfato deshidrogenasa (G6PD), en especial a grandes dosis y dependiente de la severidad de la enfermedad.

Respecto a las dosis, habitualmente las más bajas (300mg o menos) se suelen utilizar como antiagregante plaquetario, ya que su uso en niños se ha restringido mucho. Las más elevadas se utilizan buscando el efecto analgésico/antipirético o el antirreumático. Respecto a las presentaciones se puede decir que prácticamente se han estudiado todas las posibles formas galénicas para el uso clínico del ácido acetilsalicílico. Las que a continuación se describen son las más comúnmente utilizadas:

Igualmente, existen multitud de presentaciones con otros fármacos asociados, habitualmente para el tratamiento de procesos catarrales. Es el caso de las asociaciones con fenilefrina, clorfenamina, cafeína, vitamina C (ácido ascórbico levógiro) o sustancias del complejo vitamínico B. Otras veces se asocia a otros fármacos para aumentar su efecto antiagregante (dipiridamol), o analgésico (paracetamol).

Respecto a los excipientes habituales se da una situación parecida. Tanto la forma galénica como las características organolépticas del fármaco obligan al uso de determinado tipos de excipientes. Sin embargo, el gran número de laboratorios que lo fabrican como fármaco genérico hace que los excipientes que podamos encontrarnos sean sumamente variables. No obstante a continuación se listan los que se suelen incluir en la forma comercial "Aspirina" (probablemente la más vendida en el mundo) y algunos de los más usuales en las formas genéricas:

Debido a los efectos secundarios menores de la aspirina, y a que algunas personas son alérgicas a ella, se han desarrollado varias sustancias sustitutas. La más común es la acetaminofén (Tylenol, Anacin-3), que reduce la fiebre y alivia el dolor de manera muy semejante a la aspirina, aunque no aminora la inflamación. El uso excesivo de acetaminofén puede ocasionar daño renal y hepático. Un segundo sustituto común es el Ibuprofeno (Advil, Nuprin y Motrin IB), que funciona justo como la aspirina para reducir dolor, fiebre e inflamación. Otra adición a la familia de los sucedáneos de la aspirina que se venden sin receta es el naproxeno (Aleve). Su principal ventaja es su efecto duradero. Mientras que casi todos los analgésicos requieren dosis cada 4-6 horas, una sola tableta de naproxeno dura 8-12 horas.



</doc>
<doc id="19223" url="https://es.wikipedia.org/wiki?curid=19223" title="Carl Maria von Weber">
Carl Maria von Weber

Carl Maria Friedrich Ernst von Weber (Eutin, 18 de noviembre de 1786 - Londres, 5 de junio de 1826) fue un compositor romántico alemán.

Nació en Eutin, una ciudad pequeña cercana a Lübeck en el norte de Alemania, con el nombre completo de "Carl Maria Friedrich Ernst George Look von Weber". Su padre era un oficial militar, que, sin embargo, se dedicaba a tocar el violín, y su madre había cantado en los escenarios. Sus cuatro primas, hijas del hermano de su padre, eran también cantantes conocidas. Una de ellas, Constanze, se convirtió en la esposa de Mozart, con lo cual se creó un vínculo familiar entre Mozart y Carl Maria Von Weber.

Weber tuvo de niño una salud frágil y sufrió en especial de una dolencia de caderas congénita. Acompañó a sus padres en los numerosos viajes que hacían, y en los que su padre daba conciertos de violín. Estos viajes le sirvieron para familiarizarse con los escenarios y el público. Su padre deseaba que Weber fuese un niño prodigio, al igual que había sido Mozart, quien en aquellos años estaba atravesando la etapa final de su vida. Así, pues, Weber aprendió a cantar y a tocar el piano desde muy pequeño, a pesar de que no pudo caminar hasta los cuatro años.

El hermano de Haydn, Michael Haydn, también músico, le dio en 1798 clases gratuitas en Salzburgo, ciudad con gran tradición musical en la que la familia se había instalado. Poco después murió su madre y la familia se trasladó a Múnich. Allí compuso Weber su primera obra que fue publicada. Tomó clases de composición y de canto y pronto comenzó a tocar el piano en público. Su padre le animó insistentemente a componer y Weber escribió varias obras que, sin embargo, no perduraron.

Tres años después, la familia volvió a Salzburgo, y Weber reanudó sus estudios con el hermano de Haydn. En 1803 fue nombrado director de orquesta de Breslau, una ciudad de tamaño mediano en el este de Alemania, cuando aún no había cumplido los 18 años. En este puesto adquirió grandes conocimientos escénicos, que le convirtieron con el tiempo en el compositor con mayor dominio de las técnicas teatrales. A raíz de una ingestión accidental de un ácido utilizado en los talleres de imprenta, se estropeó la voz de tal manera que ya no pudo cantar.

Durante los próximos años, Weber tuvo diversos empleos, en los que siguió formándose. Al mismo tiempo compuso varias obras, incluidas óperas, que, no obstante, no tuvieron el éxito deseado. En 1811 realizó una gira de conciertos, en la que estableció amistades con varias personas influyentes. En Múnich se interpretó la ópera "Abu Hassan" que había escrito en los años anteriores, y por fin una obra suya tuvo una acogida favorable por el público. Al año siguiente murió su padre y en 1813 aceptó el cargo de director de orquesta en Praga, donde permaneció tres años. Allí compuso algunas de sus mejores obras para piano y las interpretó con gran éxito.

De Praga se fue a Dresde, donde ocupó el mismo puesto de director de orquesta. Allí volvió hacia la música de ópera y realizó diversas composiciones dentro de este género. Más adelante se puso a trabajar en su ópera más famosa, "Der Freischütz" ("El cazador furtivo"), que terminó a mediados de 1820. Se estrenó con un éxito triunfal en Berlín al año siguiente. También en Viena y en Dresde obtuvo la obra un gran éxito. No obstante, las condiciones económicas de Weber en su puesto no mejoraron por ello. Aun así, no aceptó ofertas más ventajosas en otras ciudades, ya que quiso ser fiel a sus compromisos. Escribió una segunda ópera, "Euryanthe", que también cosechó un gran éxito (1823), y compuso otras muchas de sus obras. Durante estos años se fraguaron, además, los contactos con la élite intelectual del momento, sumergiéndose en los ideales sobre lo bello y lo sublime que caracterizarían a su obra.

Cuando ya se aproximaba el final de sus días y su salud comenzaba a empeorar, Weber recibió el encargo desde el Covent Garden de Londres de componer una ópera en inglés, que sería su tercera gran obra escénica: "Oberon". Se puso a trabajar de inmediato y aprendió el idioma con tal grado de perfección, que en toda la ópera se encontró un solo error de texto. Unos meses antes del estreno se trasladó a Londres donde su salud fue empeorando durante los ensayos. Poco después de estrenarse la ópera, que fue acogida con gran éxito, Weber murió en la casa de su anfitrión, por causa de una afección pulmonar el 5 de junio de 1826.

Además de sus tres óperas, consideradas por muchos como verdaderas obras maestras de la música escénica, Weber escribió otras muchas composiciones. Son especialmente conocidas sus obras para "piano solo" y para "piano y orquesta", sus dos "sinfonías", su "quinteto para clarinete" y sus célebres "conciertos para clarinete y orquesta" y su conocido Concertino para trompa y orquesta (Weber), el cual destaca porque usa elementos especiales: hay notas que deben ser tocadas y cantadas a la vez. Esto genera una tercera nota, formando así un acorde. Hay que tener en cuenta, además, que por aquel entonces la trompa no contaba con válvulas o pistones. 

También compuso dos "misas", ocho "cantatas", numerosas "canciones" y otras obras de diverso carácter.

Con "El cazador furtivo", basada en el folclore nacional alemán y en una combinación de elementos legendarios y sobrenaturales, Weber creó la escuela romántica de ópera alemana. Entre las innovaciones musicales de Weber cabe citar el empleo de leitmotivs y de recitativos cantados (como en Euryantha) en lugar del habitual diálogo hablado de la ópera alemana. Weber fue muy admirado por su brillante colorido orquestal. Ejerció una gran influencia en otros compositores, como Wagner, Mendelssohn o Berlioz, especialmente en el compositor alemán Richard Wagner que llegó a afirmar que jamás había existido un músico más alemán que Weber.
















http://www.epdlp.com/compclasico.php?id=943



</doc>
<doc id="19224" url="https://es.wikipedia.org/wiki?curid=19224" title="Elementos del periodo 5">
Elementos del periodo 5

Un elemento del periodo 5 es aquel elemento químico en la quinta fila (o periodo) de la tabla periódica.

Estos son:

Elementos del periodo 1 -
Elementos del periodo 2 -
Elementos del periodo 3 -
Elementos del periodo 4 -
Elementos del periodo 5 -
Elementos del periodo 6 -
Elementos del periodo 7


</doc>
<doc id="19225" url="https://es.wikipedia.org/wiki?curid=19225" title="Elementos del periodo 4">
Elementos del periodo 4

Un elemento del periodo 4 es aquel elemento químico en la cuarta fila (o periodo) de la tabla periódica.

Estos son:

Elementos del periodo 1 -
Elementos del periodo 2 -
Elementos del periodo 3 -
Elementos del periodo 4 -
Elementos del periodo 5 -
Elementos del periodo 6 -
Elementos del periodo 7


</doc>
<doc id="19228" url="https://es.wikipedia.org/wiki?curid=19228" title="Semimetal">
Semimetal

Junto con los metales y los no metales, los semimetales (también conocidos como metaloides) comprenden una de las tres categorías de elementos químicos siguiendo una clasificación de acuerdo con las propiedades de enlace e ionización. Se caracterizan por presentar un comportamiento intermedio entre los metales y los no metales, compartiendo características de ambos. Por norma general y en la mayoría de los casos, tienden a reaccionar químicamente con no metales, aunque hay ciertos compuestos formados por metal y semimetal como por ejemplo el boruro de magnesio. Pueden ser tanto brillantes como opacos, y su forma puede cambiar fácilmente. Generalmente, los metaloides son mejores conductores de calor y de electricidad que los no metales, pero no tanto como los metales. No hay una forma unívoca de distinguir los metaloides de los metales "verdaderos", pero generalmente se diferencian en que los metaloides son semiconductores antes que conductores. A diferencia de los metales, los cuales al aumentar la temperatura disminuye su conductividad eléctrica, en los semimetales aumentar la temperatura supone lo contrario, aumenta su conductividad eléctrica. Los no metales son opacos y de varios colores. Suelen ser utilizados en ocasiones para formar aleaciones. Pueden ser anfóteros o levemente ácidos. 

Son considerados metaloides los siguientes elementos:


Dentro de la tabla periódica los metaloides se encuentran en línea diagonal desde el boro al ástato (este último no está incluido). Los elementos que se encuentran encima a la derecha son no metales, y los que se encuentran debajo a la izquierda son metales.

Todos estos elementos poseen tres electrones de valencia o más en su última órbita (B 3, Si 4, Ge 4, As 5, Sb 5, Te 6, Po 6, At 7). El silicio, por ejemplo, es un metaloide ampliamente utilizado en la fabricación de elementos semiconductores para la industria electrónica, como rectificadores, diodos, transistores, circuitos integrados y microprocesadores.



</doc>
<doc id="19229" url="https://es.wikipedia.org/wiki?curid=19229" title="La Antigua (León)">
La Antigua (León)

La Antigua es un municipio y villa española, en la comarca del Páramo Leonés, provincia de León, comunidad autónoma de Castilla y León. 

Lo componen 5 núcleos de población: 

Es un municipio agrícola y cuenta con una industria conservera de pescado, principalmente salmón ahumado; llamada La Balinesa; construida en el antiguo molino de La Antigua.

Las fiestas se celebran los días 8 y 9 de septiembre en honor de la natividad de María.

Está dentro de las D.O. Alubia de La Bañeza, Lechazo de Castilla y León, Pimiento de Fresno, Cecina de León y Lenteja Pardina de Tierra de Campos.

Perteneció a la antigua Jurisdicción de Laguna de Negrillos.


La Antigua se encuentra en la comarca del Páramo, en el sur de la provincia de León, a una altitud de 750 msnm (la capital). Su término municipal limita al norte con Zotes del Páramo, Laguna de Negrillos y Roperuelos del Páramo , al sur con Pozuelo del Páramo, San Adrián del Valle, Matilla de Arzón (Zamora) y Villaquejida, al este con Laguna de Negrillos, Villamandos y Villaquejida, y al oeste con Pozuelo del Páramo y Roperuelos del Páramo.


Situado en el Páramo leonés, el relieve del municipio es horizontal, tan solo interrumpido por cerros aislados. La zona más alta del municipio se encuentra regada por las aguas del embalse de Barrios de Luna, mientras que la zona baja es cultivada en régimen de secano. En el término municipal se encuentra el vértice geodésico de Moriscos, a una altitud de 810 msnm, y situado en el centro del municipio.


La Antigua está bañada por varios arroyos y ríos menores que desembocan en el río Esla o en el río Órbigo.

El municipio se encuentra conectado a la red de carreteras mediante la carretera LE-412. Siguiendo esta carretera rumbo sureste se encuentra un enlace a la red de alta capacidad a la A-66.

El aeropuerto más cercano es el de Aeropuerto de León, que entró en servicio en el año 1999, situado a 51 kilómetros de La Antigua.




</doc>
<doc id="19230" url="https://es.wikipedia.org/wiki?curid=19230" title="Ardón (León)">
Ardón (León)

Ardón es un municipio y villa española, en la comarca del Páramo Leonés, provincia de León, comunidad autónoma de Castilla y León. Lo componen 6 núcleos de población: Ardón, Benazolve, Cillanueva, Fresnellino del Monte, San Cibrián y Villalobar.

Perteneció a la antigua Hermandad de Vega con Ardón.


Ardón se encuentra en la comarca de El Páramo, en el sur de la provincia de León, a una altitud de 810 msnm (la capital). Su término municipal limita al norte con Chozas de Abajo , al sur con Valdevimbre y Villamañán, al este con Campo de Villavidel y Cabreros del Río, y al oeste con Valdevimbre y Chozas de Abajo.


Situado en El Páramo, el relieve del municipio es horizontal, tan solo interrumpido por cerros aislados y valles labrados por arroyos que desembocan en el río Esla, situándose la capital del municipio en uno de estos valles, repitiéndose la misma situación con cada uno de los pueblos del municipio. En el término municipal se encuentra el vértice geodésico de Moriscos, a una altitud de 810 msnm, y situado en el centro del municipio.


Ardón está bañada por el río Esla, el cual recorre el municipio de norte a sur. En él desembocan varios arroyos y ríos menores.

Tradicionalmente predominó el cultivo de la vid de lo que quedan apenas algunos vestigios en forma de cepas abandonadas. Con la introducción del regadío, el cultivo de las viñas prácticamente desapareció y se sustituyó por maíz y otros cereales de regadío. Actualmente la mayoría del campo está en adil (debido a la emigración de su población a la ciudad), aunque últimamente se está introduciendo el cultivo de especies arbóreas (chopo y eucalipto) para su transformación posterior en biomasa.

Son típicas las bodegas destinadas antiguamente a la elaboración y conservación del vino. Actualmente están en desuso y algunas se han transformado en merenderos.
También eran típicos de esta localidad los palomares, aunque en la actualidad solo quedan íntegros dos. 

El municipio se encuentra plenamente integrado en la red de alta capacidad española mediante la A-66 a la cual tiene un enlace directo. Paralela a esta se encuentra la N-630. También se accede a la localidad de Ardón por la carretera comarcal LE-11 y por la autovía LE-5811.

El aeropuerto de León, que entró en servicio en el año 1999, es el aeropuerto más cercano, encontrándose a 20 kilómetros de Ardón. El único vuelo que se mantiene hoy en día es a Barcelona.





</doc>
<doc id="19231" url="https://es.wikipedia.org/wiki?curid=19231" title="Bercianos del Páramo">
Bercianos del Páramo

Bercianos del Páramo es un municipio y un lugar español, en la provincia de León, comunidad autónoma de Castilla y León. Se encuentra en la zona central del Páramo Leonés, una extensa plataforma interfluvial entre los valles de los ríos Órbigo y Esla que presenta un relieve prácticamente llano. Asimismo, forma parte del partido judicial de La Bañeza. Además de Bercianos, el municipio lo componen los núcleos de Villar del Yermo y Zuares del Páramo.

A pesar del hallazgo de algunos restos romanos en su entorno, los orígenes de Bercianos del Páramo se remontan, posiblemente, a la repoblación efectuada por el Conde Gatón en el siglo IX por encargo de Ordoño I. La primera mención histórica se constata en documentación eclesiástica del año 917, con motivo de unas donaciones a la iglesia de León.

A partir del siglo XIV, la nobleza se asentó en la zona, principalmente de la mano de los Osorio y los Quiñones, y son estos últimos quienes ejercieron su dominio sobre Bercianos hasta finales de la Edad Media. A principios del siglo XIX era lugar señorial y tras la caída del Antiguo Régimen quedó constituido en ayuntamiento constitucional. 

A partir de 1959, después de la construcción del embalse de Barrios de Luna, se generalizó el regadío en buena parte del Páramo, lo que tuvo una enorme trascendencia social y económica. Su economía se basa tradicionalmente en el sector primario, cuyos cultivos mayoritarios son de trigo y cebada en secano y maíz y remolacha en regadío. Las riberas de los arroyos se utilizan para la silvicultura, con la plantación intensiva de chopos.

Entre su patrimonio destaca la iglesia de San Vicente, construida en ladrillo entre los siglos XVI y XVII, y la ermita del Cristo de la Vera Cruz; esta es un referente tanto para Bercianos como para la vecina San Pedro Bercianos, debido a la costumbre de tocar la esquila de la ermita cuando había tormenta ya que, según la creencia popular, alejaría el granizo de las plantaciones. Entre las celebraciones que tienen lugar a lo largo del año sobresalen las fiestas patronales de San Vicente —en torno al 22 de enero—, la Santa Cruz —el 3 de mayo—, y el Cristo, el 14 de septiembre.

El topónimo de Bercianos del Páramo procede de, por un lado, la palabra «Bercianos», que haría referencia a su constitución como núcleo, en el contexto de la Reconquista, con gentes llegadas del Bierzo durante el reinado de Ordoño I. Por otro lado, el término «Páramo», referido a su ubicación. Este vocablo, por su parte, procede de la palabra hispano-latina "Paramus", de origen prerromano, que hace referencia a un campo expuesto a los vientos que no se cultiva ni habita.

El escudo heráldico municipal fue aprobado el 13 de diciembre de 1985 y oficializado el 7 de marzo de 1986. Su descripción es la siguiente:

El municipio se encuentra en la comarca del Páramo Leonés, al sur de la provincia de León. Su territorio está representado en la hoja MTN50 (escala 1:50 000) 194 del Mapa Topográfico Nacional.

Bercianos del Páramo se ubica en la zona central de la planicie del Páramo Leonés. Esta extensa plataforma —geológicamente un extenso interfluvio entre los valles de los ríos Órbigo y Esla— presenta un relieve prácticamente llano, de pendientes suaves. La altitud media del municipio está en los , entre los de la zona más alta situada en la parte norte y los de la zona más baja, en el extremo sur del término municipal. 

A nivel general, el municipio se sitúa en la zona noroccidental de la cuenca del Duero, depresión de origen terciario colmatada por materiales continentales que posteriormente fueron erosionados y recubiertos por sedimentos cuaternarios. A este último periodo pertenecen la mayoría de materiales que nos encontramos, principalmente áridos naturales: limos arcillosos —utilizados tradicionalmente para la elaboración de cerámica—, y arenas y gravas usadas para la construcción y las obras públicas.

El municipio se encuentra en la cuenca hidrográfica del Duero, cuyos cursos fluviales, a nivel general, se caracterizan por la irregularidad de su caudal, con estiajes en época estival y crecidas en otoño e invierno debido a la lluvia. La planicie del Páramo no es atravesada por ningún río de importancia, pero en el caso de Bercianos del Páramo su territorio es atravesado por dos cauces naturales, el arroyo de La Mata y el arroyo de La Magdalena. 

Asimismo, y debido al plan de regadío desarrollado en los años sesenta tras la construcción del embalse de Barrios de Luna, su terrazgo cuenta con el abastecimiento de dos canales principales —Trasvase del Páramo Bajo y Santa María— y numerosos canales secundarios. En aquellas zonas de drenaje deficiente aparecen pequeñas lagunas o zonas de encharcamiento, de carácter estacional, que desaparecen en época estival. La mayoría de ellas han sido desecadas por la acción del hombre e incorporadas al suelo cultivable.

El clima en el municipio se clasifica como mediterráneo continentalizado, de inviernos fríos con frecuentes heladas entre octubre y mayo y veranos cálidos y secos. La oscilación térmica anual ronda los 15 °C mientras que la diaria supera en ocasiones los 20 °C. Las precipitaciones se reparten de forma irregular a lo largo del año, con escasez de las mismas en verano, concentrándose al final del otoño, en los meses invernales y al principio de la primavera. Esta llega con retraso pero con gran dinamismo meteorológico; las temperaturas ascienden a finales de junio y se prologan así hasta mediados de agosto, cuando se registran de nuevo fuertes descensos térmicos.

Según la clasificación climática de Köppen, Bercianos del Páramo se encuadra en la variante Csb, es decir clima mediterráneo de veranos suaves, con la media del mes más cálido no superior a 22 °C pero superándose los 10 °C durante cinco o más meses. Se trata de un clima de transición entre el mediterráneo (Csa) y el oceánico (Cfb). Gracias a los datos de la estación meteorológica situada en Santa María del Páramo, a 5 kilómetros de distancia, los parámetros climáticos promedio aproximados del municipio son los siguientes:

El término municipal de Bercianos del Páramo se encuentra en el piso bioclimático supramediterráneo, por lo que su vegetación clímax son las especies marcescentes y las coníferas. Sin embargo, dicha vegetación, que debió cubrir toda la comarca, ha desaparecido de muchas zonas sustituida por los cultivos. En las zonas cercanas a los cursos fluviales aparecen bosques de ribera, con álamos, sauces o alisos, además de las plantaciones de chopos. A ellos se suman brezales y tomillares en los bordes de las vegas, pastizales y prados naturales.

El municipio cuenta con una fauna rica y variada debido a su situación de transición entre el mundo mediterráneo y el eurosiberiano. Así, en cuanto a los íctidos, las aguas de Bercianos del Páramo sustentan tres especies: el barbo común, la boga del Duero y la bermejuela, a los que acompañan mamíferos como la nutria. Entre las distintas especies de anfibios y reptiles están presentes el sapo común, el tritón jaspeado, la ranita de San Antón, el lagarto ocelado o la culebra viperina y culebra bastarda. En las zonas llanas del municipio se encuentran aves como el cernícalo, la avutarda o el gavilán común y pequeños mamíferos como el conejo o la liebre ibérica. En el entorno de los núcleos de población son comunes la cigüeña blanca, la golondrina, el vencejo común, la paloma torcaz, distintas especies de páridos, la graja o rapaces como el milano real. Por último, en las zonas de pastizales o de monte están presentes aves como la perdiz roja y mamíferos como el corzo, la comadreja, el zorro, el jabalí y, ocasionalmente, el lobo.

La mayor parte del Páramo tuvo una baja densidad de ocupación en tiempos prerromanos, localizándose los escasos asentamientos en altozanos y cerros próximos a los grandes valles y vegas de los ríos que enmarcan la meseta paramesa. Como ejemplos más significativos, encontramos los castros de «El Castillo», en un pequeño otero sobre el escarpe del río Esla, con ocupación de la edad del Bronce Final y de la edad del Hierro, reocupado en la alta y plena Edad Media, y «El Castro», cercano al anterior y con ocupación de la edad del Bronce Final, ambos en el municipio de Ardón.

El municipio estaba situado en territorio de los astures, que se extendían por Asturias, León y Zamora hasta el río Esla, la zona oriental de Lugo y Orense, y parte del distrito portugués de Braganza; en concreto, en el área del Páramo, estaban asentados los bedunienses, en el norte de la comarca, y los brigaecinos, en la zona sur. Posteriormente se produjo la conquista y asentamiento romano tras las guerras cántabras. De esta época data la que puede ser la primera referencia al territorio del Páramo; se trata de una lápida hallada en las murallas de León en la que Tulio, un ciudadano romano, dedica a la diosa Diana los cuernos de un ciervo.

En esos momentos las tierras altas del Páramo registran una escasa población; tan solo se ha localizado un asentamiento de época romana en Audanzas del Valle, unos kilómetros al sur de Laguna de Negrillos, cuyos escasos restos no permiten una atribución socioeconómica clara, aunque parecen indicar una orientación agraria. A ellos pueden sumarse algunos hallazgos epigráficos y numismáticos, como la inscripción supuestamente hallada en Banuncias y el miliario de Cillanueva, que documentan el trazado de la vía romana que comunicaba Asturica Augusta (Astorga) con Burdigala (Burdeos) a través del Páramo, camino que se documenta frecuentemente en la Edad Media. En Valdefuentes del Páramo fue hallada una moneda hispanorromana de la ceca de Calagurris, de época de Augusto, sin contexto arqueológico.

Así mismo, en el lugar llamado «Las Carbas», situado en el límite municipal entre Santa María del Páramo y Bercianos del Páramo, se encontró en 1974 una necrópolis formada por 32 tumbas, cuyas sepulturas de lajas y cantos tapadas con losas seguían la orientación de las líneas solsticiales. Había tres tipos de tumbas, en forma de ataúd, romboidal con vértices truncados y antropoide, y ninguna poseía restos de ajuar o material alguno.

Finalmente, se desconocen hallazgos materiales de época tardorromana y visigoda en las tierras paramesas, los más cercanos se encuentran de nuevo en la periferia de este espacio mesetario; en las ciudades de Legio y Asturica Augusta, en el castro de Valencia de Don Juan, el "Coviacense Castrum", donde la población hispanorromana resistió el asedio de las tropas godas de Teodorico II en 459, o en villas hispanorromanas ("villae") como la de La Milla del Río, todos ellos significativos centros de poder en esa época.

Tras la conquista musulmana de la península ibérica, la población emigraría hacia zonas más seguras lo que, en el caso del Páramo, se traduciría en un probable desierto demográfico en la comarca. Sin embargo, a mediados del siglo IX se produce la emigración mozárabe hacia el norte, hecho que aprovechó Ordoño I para consolidar la frontera; así, encargó al Conde Gatón la repoblación de estas tierras, momento en el que surgiría el núcleo de Bercianos, y es a partir del siglo X cuando se constata documentalmente el avance repoblador del Páramo por parte de los grupos más poderosos de la corte leonesa.

Así, el 26 de junio de 917 el obispo Frunimio otorgó a la iglesia de León «...otra villa en el Órbigo que llaman Verceianos...», que podría identificarse con Bercianos. Con la misma fecha (26 de junio de 917) el abad Iquila donó Bercianos al monasterio de Santiago de León junto con sus espacios agrarios y los hombres que allí residieran, que quedarían sometidos y obligados a realizar pagos en productos agrarios y metálicos. 

Al año siguiente tiene lugar otra mención por parte del monasterio de Santa María y Santiago de Valdevimbre, que a lo largo de la décima centuria va adquiriendo villas campesinas con tierras, prados, viñas, molinos, aguas y acueductos en su entorno, en las riberas de los ríos Bernesga, Torio y Esla. Se trata de una adquisición efectuada el 8 de enero de 918 tras la donación de Ordoño II al abad Belderedo el lugar de Busto, que limitaba, según el documento, con Bercianos y el camino de Coyanza. 

Además de ese avance monástico, se había ido intensificando la ocupación y organización agraria del Páramo por hombres y mujeres que no forman parte de grupos poderosos, quizás como continuación de un proceso iniciado en los tiempos precedentes al siglo X, anteriores a la dominación feudal. Parece tratarse de familias campesinas de diversa condición y riqueza, con plena capacidad jurídica de actuar, poseer, comprar, vender y organizarse en concejos aldeanos, al menos en los momentos iniciales del proceso colonizador, y que van siendo paulatinamente absorbidos (por presuras, compra de tierras, incautaciones) por dominios eclesiásticos y laicos, y entrando en dependencia y servidumbre de estos. En el caso de Bercianos del Páramo se conserva un documento en el cual dos personas, Cidi y su madre Goda, donaban a Momadonna una corte con sus pertenencias en 1021. 

El 27 de abril de 1157 Diego, obispo de León, otorgó a Bercianos del Páramo una de las seis cartas de behetría junto a las de Villagallegos, La Mata del Páramo, Bustillo, San Martín del Camino y Sardonedo. Estas cartas otorgaban ciertos derechos a estos lugares, como poder elegir el señor al que se subordinaban, ya que estaban poblados por hombres libres. Posteriormente, en 1188, la localidad aparece incluida en el Fuero de Valencia de Don Juan.

Debido, quizás, a la menor capacidad de producción de estas tierras de secano, frente a las fértiles propiedades en zonas de ribera, la mayor actividad económica se documenta en el límite oriental del Páramo, próximo al río Esla y a la ciudad de León. Un ejemplo sería Aparicio Johan, miembro del coro de la catedral de León y sacerdote en Bercianos que, entre 1335 y 1347 adquirió varios bienes en la zona; una viña y dos tierras en Bercianos y bienes y dos tierras en Villagallegos.

A partir de ese siglo XIV, la nobleza se asentará con fuerza en estas tierras. Hasta 1363 Pedro Álvarez Osorio tuvo derechos sobre una extensa porción del Páramo pero tras su muerte en 1365, el rey Pedro I amplió el alfoz de León con una cantidad importante de territorios que previamente poseía Pedro Álvarez Osorio. Esto chocó con los derechos eclesiásticos, como queda constatado en la polémica de Villar de Mazarife; aquí la iglesia de León tenía su dominio, pertenecía al alfoz de León y sus habitantes se consideraban de behetría, es decir, creían que podían elegir libremente a su señor. 

Por decisiones judiciales pasó a dominar el alfoz de León, territorio directamente dependiente de la ciudad de León en materia judicial y de impuestos. Sin embargo, un documento de 1366 ordena a los regidores leoneses devolver al obispado de León varios lugares que le habían sido arrebatados, lo que modificó la extensión del alfoz leonés. Este, a finales del siglo XV, abarcaba varios lugares del Páramo, entre ellos Bercianos.

Por su parte, el dominio de los Quiñones en el Páramo se remonta al siglo XIII, cuando Pedro Álvarez de Quiñones recibió las posesiones de Urdiales y Santa María del Páramo, patrimonio que se vería acrecentado en los siglos XIV y XV con, por ejemplo, Laguna de Negrillos. En 1462, cuando Diego Fernández de Quiñones recibió el título de Conde de Luna, el dominio de los Quiñones se extendía por Velilla de la Reina, Castrillo de San Pelayo, Celadilla, Santa María del Páramo, Grajal de Ribera, Urdiales, Vilecha, La Antigua, Conforcos, Villamorico, San Pedro Bercianos, Fojedo del Páramo y Bercianos.

En 1465 Enrique IV concedió a Alvar Pérez Osorio el marquesado de Astorga; el dominio de los Osorio en el Páramo debía ser ya muy amplio, a excepción de las posesiones de los Quiñones en torno a Laguna de Negrillos, principalmente, y de la iglesia de Astorga, por lo que tanto el marqués como sus descendientes se titularon señor del Páramo. En 1481 Isabel I de Castilla ordenó al conde de Valencia y al deán de León, Luis Osorio, que no perturbasen al conde de Luna en la posesión de Bercianos, por lo que la localidad perteneció a los Quiñones hasta el final de la Edad Media.

Durante esta época, la fisonomía y los tipos de cultivo prácticamente no tenían nada que ver con la realidad agraria actual. De esta manera, había un predominio del cultivo cerealícola, en el que el centeno era dominante, al acaparar en torno al 80% de la superficie cultivada, aunque sobre la base de la existencia de un sistema de rotación bienal, al aparecer alternando con el barbecho como práctica generalizada.

La ganadería adquiere relativa importancia a lo largo del siglo XVIII, de manera especial en los pueblos situados al norte y oeste de la comarca. Entre las especies con más implantación estaban la especie asnal aunque también estaban representadas la vacuna y la lanar y debía de existir bastante cantidad de ganado mular. La mención de numerosos vecinos de la localidad de Zuares del Páramo dedicados a la compra y venta de borricos, hace pensar en la existencia de una cuidada cabaña de garañones que se debían de exportar a la provincia leonesa y a las vecinas.

Son también reseñables los molinos de linaza (simiente del lino) que producían aceite de linaza, útil como alimento y para dar luz en el candil. El aceite que sobraba se vendía o cambiaba fuera del Páramo. Con los salvados de esta linaza fabricaban el pan de linaza, que era un alimento fundamental de los bueyes del Páramo, al no haber allí praderas en las que pudieran pacer. 

Se calcula que a mediados del siglo XVIII existían en todo el Páramo más de 200 molinos dedicados a la molturación de la linaza. Los beneficios estimativos representaban el 46% del total provincial. Además según el Catastro de Ensenada se constata la existencia en el municipio de telares de lienzos y paños y herrerías. 

Por otro lado, los únicos espacios que aparecían relativamente más concentrados y con mayores extensiones serían las praderías y espacios de monte de propiedad comunal. Todos los pueblos tenían sus correspondientes espacios comunales destinados al aprovechamiento ganadero y en beneficio de todos los vecinos. El uso de estos espacios aparecería regulado por parte de los gobiernos concejiles sobre la base de la costumbre transmitida a través de generaciones, normas vigentes que, en algunas localidades, aparecían también consignadas por escrito en ordenanzas concejiles. En algunos pueblos del centro y sur comarcal, también destacarían los quiñones o «senaras», es decir, lotes de tierra comunal que estaban distribuidos entre los vecinos con fines agrícolas. 

A partir de finales del siglo XVIII y primera parte del XIX, el crecimiento de la población daría lugar a la extensión de la superficie roturada a costa de la disminución de estos espacios, con la consiguiente transferencia de la propiedad a manos privadas. En algunos casos quedaría justificada esa reducción por una necesidad unánime manifestada por los concejos, aunque, en la gran mayoría de los casos, la alteración de la costumbre preestablecida y garante de la conservación de estos recursos se llevaría a cambio por mecanismos no tan legales.

Sebastián Miñano, en su "Diccionario geográfico y estadístico de España y Portugal" (1826-1829), señalaba que era lugar señorial perteneciente a la jurisdicción de Laguna de Negrillos, al partido de León y al obispado de León. Lo describe en un páramo seco y árido, presentaba un caserío mediano y contaba con una parroquia. Su población era de 420 habitantes, producía vino, trigo, cebada, centeno y avena, y existía una fábrica de aceite de linaza. A mediados del siglo XIX, Pascual Madoz, en su "Diccionario geográfico-estadístico-histórico de España y sus posesiones de Ultramar" (1845-1850), lo sitúa en la provincia de León, partido judicial de La Bañeza y ayuntamiento de San Pedro Bercianos.

Las primeras décadas del siglo XX aparecen marcadas por las malas comunicaciones y la pobreza del suelo, que provocan una fuerte emigración. Se sigue sembrando centeno, trigo, barbilla, avena y cebada, y se continúa la tradicional práctica del barbecho, aunque en el peor terreno se plantaba viñedo. Se empezaron a sembrar patatas en tierras regadas por el agua de pozos que era subida a través de un cigoñal.

Poco a poco, las tierras empezaron a ser de mediana y grande extensión, a través de las permutas entre vecinos de unas fincas por otras. Con este cambio se introdujeron las norias, sustituyendo a los cigoñales y ahondando los pozos. De esta forma se comenzó a sembrar trigo mocho, alubias, patatas y remolacha. La agricultura se completaba con las 10 ó 15 ovejas que solía tener cada familia y que pastoreaba un vecino dedicado a la ganadería ovina, al tener más ovejas que los demás. 

En 1959, tras la construcción del embalse de Barrios de Luna, llegó el agua a una buena parte del Páramo, lo que aceleró la transformación del secano en regadío, y el cambio de las estructuras agrarias que había comenzado años atrás. Los pozos, las norias, las viñas y los primitivos sistemas de explotación desaparecieron para dar paso a canales, acequias, concentraciones parcelarias y, sobre todo, agua. Las nuevas infraestructuras permitieron la mecanización y la incorporación de cultivos industriales. Al mismo tiempo, los animales de trabajo fueron sustituidos por una ganadería más rentable.

La puesta en regadío del Páramo tuvo una enorme trascendencia social y económica: aumento de las ganancias, reforma del medio natural, atenuación de la emigración, modificación de hábitos y costumbres tradicionales, desapareciendo, por ejemplo, la realización de trabajos comunales, "hacenderas" o "facenderas", que los concejos parameses realizaban desde tiempos inmemoriales.

Según el padrón municipal de habitantes de 2016 del INE, el municipio contaba con 605 habitantes, de los cuales 308 (50,90 %) eran hombres y 297 (49,09 %) eran mujeres.


</small>

Las entidades de población que componen el municipio de Bercianos del Páramo son las siguientes:


En cuanto al colectivo inmigrante, según el padrón municipal de 2016 del INE, en Bercianos del Páramo no residían personas procedentes de otros países.


La administración local del municipio se realiza a través de un ayuntamiento de gestión democrática, cuyos componentes se eligen cada cuatro años por sufragio universal. El censo electoral está compuesto por todos los residentes empadronados en Bercianos del Páramo, mayores de 18 años y con nacionalidad de cualquiera de los países miembros de la Unión Europea. Según lo dispuesto en la Ley del Régimen Electoral General, que establece el número de concejales elegibles en función de la población del municipio, la Corporación Municipal está formada por 7 ediles (9 hasta 1991), los cuales se han distribuido de la siguiente forma en los últimos años:


La gestión ejecutiva municipal está organizada en varias áreas al frente de las cuales hay un concejal del equipo de gobierno. Cada área de gobierno tiene varias delegaciones en función de las competencias que se le asignan y que son variables de unos gobiernos municipales a otros. El actual equipo de gobierno está encabezado por la alcaldesa María Milagros Benéitez Barragán.


En el municipio, además de la cabecera, se encuentran las localidades de Villar del Yermo y Zuares del Páramo, ambas integradas en la gestión municipal a través de sus respectivas Juntas Vecinales, cuyos componentes se eligen cada cuatro años por sufragio universal.


Bercianos del Páramo pertenece al partido judicial número 3 de la provincia de León, con sede en La Bañeza, cuya demarcación comprende dicha ciudad más otras poblaciones de las comarcas limítrofes, y cuenta con dos juzgados de primera instancia e instrucción.

El sector primario siempre ha tenido un peso importante en la economía del municipio. La riqueza de las tierras de regadío, gracias al agua procedente del embalse de Barrios de Luna, provoca que casi el 64% de los trabajadores del municipio se inscriban en este sector mientras que, por el contrario, las empresas del sector primario apenas suponen un 15,79%. Los cultivos mayoritarios son de trigo y cebada en secano y maíz y remolacha en regadío, aunque también se utiliza parte de las riberas de los arroyos para la silvicultura, con la plantación intensiva de chopos. La ganadería se centra sobre todo en la cabaña ovina, porcina y vacuna. Respecto a la distribución del suelo, los terrenos municipales se distribuyen de la siguiente forma: herbáceos (85,08%), pastos (5,62%), forestales (0,62%), leñosos (0,02%) y otros usos (8,66%).

El sector secundario es el que menor número de trabajadores emplea, tan sólo un 2%, aunque el número de empresas comprende el 10,53% del total. Respecto al sector de la construcción, ocupa un 12,16% de trabajadores y las empresas representan un 26,32%. Por último, el sector servicios emplea al 21,62% del total de trabajadores, y representan el 47,37% de empresas. Predomina un comercio tradicional, con un supermercado en el centro de la localidad.

En cuanto al desempleo, a raíz de la crisis económica existente desde 2008 el número de parados se ha incrementado y así, según el Servicio Público de Empleo Estatal, si en octubre de 2007 el paro registrado era de 4 personas, en septiembre de 2017 ascendía a 21 personas, de las cuales 10 eran hombres y 11 eran mujeres.


El municipio está integrado en la Comunidad de Regantes del Páramo Medio, cuya sede se encuentra en Bercianos. Esta comunidad se formó en 2009 por segregación de la anterior Comunidad de Regantes del Páramo Bajo, y forman parte de ella las localidades de Villar del Yermo, Bercianos del Páramo y Zuares del Páramo, todas ellas pertenecientes al municipio de Bercianos del Páramo, San Pedro Bercianos, dentro del municipio homónimo, Fontecha del Páramo, Palacios de Fontecha, Pobladura de Fontecha y Villagallegos, pertenecientes al municipio de Valdevimbre, y Villacalbiel, San Esteban de Villacalbiel y Villibañe dentro del municipio de Villamañán. 

La superficie total de esta comunidad es de 5600 hectáreas, de las cuales 4763 son tierras de regadío modernizadas, todas ellas abastecidas con el agua procedente del embalse de Barrios de Luna.

El municipio no cuenta con centros educativos por lo que, a nivel de educación infantil y primaria, sus alumnos acuden al CEIP «Benito León» en Santa María del Páramo. En cuanto a educación secundaria, sus estudiantes acuden al IES «Valles del Luna», también en Santa María del Páramo. Ambos centros están gestionados por la Consejería de Educación de la Junta de Castilla y León a través de la Dirección Provincial de Educación de León.

El sistema sanitario del municipio se presta a través del sistema público de salud, gestionado por Sacyl (Sanidad Castilla y León), mediante un consultorio médico que ofrece varios servicios a la semana, dependiente del centro de salud de Santa María del Páramo. Este cuenta con un servicio de guardia y en él se centraliza la zona básica de salud «Santa María del Páramo», que atiende a un total de diez municipios.

La localidad cuenta con una farmacia, y en relación a centros hospitalarios, sus habitantes acuden a los existentes en la capital provincial como el Hospital de León. En cuanto a servicios sociales, Bercianos del Páramo pertenece al Centro de Acción Social (CEAS) de Santa María del Páramo, con sede en el Ayuntamiento.

En 2014, el municipio contaba con un total de 752 vehículos de motor, que representa un índice de 723,7 automóviles por cada 1000 habitantes. Los puntos de Inspección Técnica de Vehículos más cercanos se encuentran en Cembranos y en Onzonilla.

Bercianos del Páramo está conectado con otras localidades de la comarca a través de dos viales de la red secundaria:


Para el transporte de viajeros por autobús, la compañía ALSA ofrece servicios por carretera entre el municipio y varios destinos provinciales y comarcales como por ejemplo León y Santa María del Páramo.


La estación de ferrocarril más cercana a Bercianos del Páramo es la estación de Veguellina, situada a 20 kilómetros de la localidad, siendo la estación de León la más importante cercana al municipio, a 29 kilómetros de la localidad, contando con servicios de alta velocidad desde 2015.


El aeropuerto de León, que entró en servicio en 1999, es el único aeropuerto ubicado en la provincia y el más cercano al municipio, encontrándose entre Valverde de la Virgen y San Andrés del Rabanedo, a 33 kilómetros de Bercianos del Páramo. Asimismo, las otras opciones más cercanas para el transporte aéreo son los aeropuertos de Valladolid y Asturias, situados a 113 y 187 kilómetros respectivamente.

Se trata de un templo católico, parroquia de la localidad, construido en ladrillo entre los siglos XVI y XVII en el mismo lugar en el que se levantaba otro más antiguo, de origen gótico. Presenta planta rectangular de una sola nave, torre cuadrada y porche lateral. En su interior, encalado en blanco, destaca el altar mayor; tanto su retablo como el sagrario y la talla central corresponden al estilo barroco castellano, del siglo XVII.

El retablo central, dedicado a San Vicente, se compone de banco, dos pisos distribuidos en cinco calles, entablamento y ático en tres calles. En él se aprovecharon varias tablas góticas de mediados del siglo XV que representan escenas de la vida de Cristo y del santo titular del templo. En el cuerpo superior también destacan dos tallas de los siglos XIV-XV, situadas a ambos lados del Crucificado, que representan a San Andrés y a Santiago.

En el lado derecho de la nave destaca la imagen de la Virgen negra con el Niño sentado en sus rodillas; se trata de una escultura de 81 cm de alto, asentada en trono sobre peana y datada entre los siglos XII-XIII. En el lado opuesto destaca, también sobre peana, una Virgen llamada del Carmen, datada en el siglo XVI y similar a la Virgen de la Encina de Ponferrada, así como las imágenes de la Asunción, Virgen del Rosario y Calvario del siglo XVII y las de Santiago y San Andrés, del siglo XIV. Manuel Gómez-Moreno, en su momento, no registró noticia alguna sobre dos tallas de un obispo y un clérigo del siglo XIII y un santo franciscano sin datación.

En 2000 la nave lateral derecha sufrió un derrumbe y fue restaurada dos años más tarde. Entre 2008 y 2009 la Fundación del Patrimonio Histórico de Castilla y León, en colaboración con la parroquia, financió la restauración del retablo mayor debido a los numerosos daños que presentaba.

Se trata de un edificio de planta rectangular, construido en ladrillo, que cuenta en su interior con un pequeño retablo donde descansa la imagen del Cristo de las Eras. Desde su construcción ha sido un referente para la localidad y para la vecina San Pedro Bercianos debido a la costumbre de tocar la esquila de la ermita cuando había tormenta para alejar el granizo de las plantaciones. Las puertas de la ermita se abren el 3 de mayo para celebrar la fiesta de la Santa Cruz y el 14 de septiembre con el Cristo.

A lo largo del año son varios los eventos festivos que tienen lugar en Bercianos del Páramo. Cronológicamente, el 22 de enero se celebra el patrón de la localidad, San Vicente, festividad durante la cual se organizan varias actividades de ocio y entretenimiento como actuaciones musicales y juegos populares.

El 3 de mayo tiene lugar la celebración de la Santa Cruz y el día 15 del mismo mes se celebra San Isidro Labrador, patrón de los agricultores, festividad tradicional en las localidades de tradición agrícola. Ya en septiembre, el día 14 se festeja el Cristo.

Durante la tercera semana de agosto tiene lugar la Semana Cultural de Bercianos del Páramo, en la cual se organiza un programa de actividades que incluye exposiciones, conferencias, juegos tradicionales, actuaciones teatrales y torneos deportivos, entre otras.

Una de las tradiciones más destacadas de la localidad corresponde al toque tradicional de la esquila en la ermita de la Vera Cruz, cuyo volteo continuado, según la creencia popular, ahuyentaría las tormentas que pudieran causar daños en los cultivos. Para ello, la Cofradía del Santo Cristo elegía el día de San Silvestre un alguacil-campanero que se encargaría de tocar la esquila cada vez que se observasen indicios de tormenta.

El toque de la esquila se podía escuchar también en la vecina localidad de San Pedro Bercianos, por lo que dicho toque servía a los habitantes de las dos localidades. Esta tradición se ha ido perdiendo en los últimos años, aunque todavía en la actualidad las personas de mayor edad siguen conservando la tradición de tocar la esquila en ocasiones destacadas. Además del toque de la esquila, existía la siguiente estrofa:

Para la práctica del deporte, Bercianos del Páramo cuenta con varias instalaciones como un frontón con gradas y una cancha para jugar a baloncesto, fútbol sala y voleibol. En cuanto a eventos deportivos, la localidad organiza el Torneo de Fútbol Sala Bercianos del Páramo, que en el año 2014 organizó su cuarta edición. Además, la localidad cuenta con un equipo de fútbol sala, el Bercianos Fútbol Sala, que compite en diferentes torneos dentro de la comarca, ganando en el año 2012 el XIX Torneo Paramés de Fútbol Sala.

La gastronomía en Bercianos del Páramo está basada, como en el resto de localidades de su entorno, en los alimentos que se pueden conseguir localmente. Entre aquellos de origen animal destacan los huevos y la carne, principalmente la obtenida de la matanza —con la que se elaboraban morcillas, chorizos, jijas y lomo—. Entre los alimentos de origen vegetal los más importantes eran las legumbres, el pan de trigo y frutas cultivadas como los higos. También el ajo era usado habitualmente para preparar las denominadas sopas de ajo, uno de los platos típicos de la comarca del Páramo Leonés. Entre la repostería tradicional son comunes los mazapanes, rosquillas, almendrados, orejas, hojaldres y sequillos, y la limonada en cuanto a bebidas.

Además, Bercianos del Páramo cuenta con varios productos con denominación de origen; las alubias, que en 2006 obtuvieron la Indicación Geográfica Protegida (IGP) de La Bañeza-León, la cecina de León, con Indicación Geográfica Protegida desde 1994, y el lechazo de Castilla y León, también con Indicación Geográfica Protegida desde 1997. Por último, y en relación a los vinos, Bercianos del Páramo se encuentra dentro de la Denominación de origen Tierra de León, protegido desde 2007.





</doc>
<doc id="19232" url="https://es.wikipedia.org/wiki?curid=19232" title="Estación Espacial Internacional">
Estación Espacial Internacional

La Estación Espacial Internacional (en inglés, International Space Station o ISS; en ruso, Междунаро́дная косми́ческая ста́нция, МКС) es un centro de investigación en la órbita terrestre, cuya administración, gestión y desarrollo están a cargo de la cooperación internacional. El proyecto funciona como una estación espacial permanentemente tripulada, en la que rotan equipos de astronautas e investigadores de las cinco agencias del espacio participantes: la Administración Nacional de la Aeronáutica y del Espacio (NASA), la Agencia Espacial Federal Rusa (FKA), la Agencia Japonesa de Exploración Espacial (JAXA), la Agencia Espacial Canadiense (CSA) y la Agencia Espacial Europea (ESA). Está considerada como uno de los logros más grandes de la humanidad.

La Agencia Espacial Brasileña participa a través de un contrato separado con la NASA. La Agencia Espacial Italiana tiene igualmente participación con contratos separados para atender tareas diferentes de las que ejecuta la ESA en la EEI (en los que ya participa Italia como miembro de pleno derecho).

La ISS es una evolución que aúna las capacidades de las anteriores estaciones espaciales: la Mir-2 rusa, la estadounidense" Freedom", el módulo europeo Columbus y el Kibo. Los primeros planes de montar una gran estación internacional remontan a los años 1980. La estación se planificó en ese momento con el nombre "Alpha". Actualmente está dividida en dos secciones, el segmento orbital ruso y el Segmento Orbital Estadounidense (United States Orbital Segment, USOS), ambos compartidos entre varias naciones. Ambas secciones cuentan con financiación hasta 2024. La Agencia Espacial Federal Rusa, sin embargo, ha propuesto la construcción de una nueva estación espacial, la OPSEK, usando componentes de la estación actual.

La ISS está en construcción desde 1998 y tras su conclusión es el objeto artificial más grande en órbita terrestre. Completa una vuelta cada 92 minutos aproximadamente y se encuentra a unos 408km de la superficie terrestre.Dichos datos corresponden a febrero de 2015, aunque su altura varia debido a la fricción atmosférica y a las repetidas propulsiones para recuperar una órbita segura. La inclinación es de 51,6°. La órbita de la ISS se encuentra dentro de la atmósfera terrestre, en la capa denominada termosfera.

La estación ha alcanzado unas dimensiones aproximadas de 110m×100m×30m, con un volumen habitable equivalente a un cubo de 10x10x10 aunque distribuidos en una red de módulos. Según los planes, debería mantenerse en operaciones al menos hasta el año 2024.

Gracias a la estación hay presencia humana permanente en el espacio, ya que al menos dos personas la han habitado desde el 2 de noviembre del año 2000 de forma continuada. La estación se mantiene principalmente por las lanzaderas rusas Soyuz y la nave espacial Progress. Anteriormente, el mantenimiento se hacía mediante los "transbordadores" estadounidenses, que operaron hasta el año 2011, momento en que el programa de transbordadores espaciales de Estados Unidos fue cancelado, debido a que sus exorbitantes costos no podían ser mantenidos con el recorte general de gastos acometido por el Gobierno de EE.UU.

En sus primeros tiempos la estación tenía una capacidad para una tripulación de tres astronautas, pero desde la llegada de la Expedición 20, aumentó hasta soportar una tripulación de hasta seis miembros.Antes de que llegara el astronauta alemán Thomas Reiter de la ESA, que se unió con el equipo de la Expedición 13 en julio de 2006, todos los astronautas permanentes pertenecían a los programas espaciales ruso, estadounidense o canadiense. Entretanto, la ISS ha sido visitada por 205 personas de 16 países y ha sido también el destino de los primeros turistas espaciales.

En líneas generales, se puede describir la Estación Espacial Internacional como un gigantesco mecano situado en órbita alrededor de la Tierra, a 400km de altura. Sus dimensiones son de aproximadamente 109m de longitud total y 88m de ancho, con una masa cercana a las 420 toneladas. El volumen presurizado alcanza unos 916m³, con lo que sobrepasa en amplitud y complejidad todo lo que existe hasta la fecha. Puede acoger hasta seis astronautas permanentemente, quienes se suceden según las exigencias de las misiones. Su energía es proporcionada por los paneles solares fotovoltaicos de 4046 metros cuadrados, con una potencia de 110kW.

(Datos de 2019) 


La estación ha progresado de manera sostenida, no solo en sus características técnicas, sino también en cuanto a la calidad de los espacios habitables, proporcionando mayor confort para las expediciones de larga duración. A la fecha octubre de 2017 tiene un espacio habitable comparable con una casa estándar de cinco dormitorios, tiene además dos baños y posee un gimnasio. Desde el 3 de octubre de 2019 se encuentra habitada por la Expedición 61.

La historia de la Estación Espacial Internacional comenzó el 20 de noviembre de 1998, cuando el cohete ruso Protón colocó en órbita el módulo ruso Zaryá, el módulo principal y más grande, diseñado para dotar a la estación espacial de la energía y capacidad de propulsión iniciales. El 4 de diciembre la NASA puso en órbita el nodo Unity a través de su transbordador espacial Endeavour.

El 12 de julio de 2000 se añadió el segundo módulo de servicio ruso Zvezdá (pronunciado "/zviozda/") que aportaba los sistemas de soporte vital a la Estación Espacial y la preparaba para recibir a sus primeros astronautas. El 11 de octubre de 2000 se añadió sobre el nodo Unity la estructura integrada ITS Z1 que permite comunicarse con la Tierra. El 2 de noviembre llegaron los primeros tripulantes a bordo de la Soyuz TM-31 lanzada el 31 de octubre de 2000. Un mes después se añadió el primer módulo fotovoltaico que proporcionaba energía solar a toda la estación.

En lo que se refiere a la primera tripulación permanente de la ISS, estaba compuesta por el astronauta estadounidense William Shepherd y los rusos Yuri Guidzenko y Serguéi Krikaliov, llegó a la Estación el 2 de noviembre de 2000. En abril de 2001, los países socios del proyecto de construcción de la Estación llevaron a cabo la autorizaron del primer viaje a la Estación como “turista” espacial, del financiero estadounidense Dennis Tito, quien permaneció seis días a bordo de la EEI a principios de mayo.

Al año siguiente llegó a la estación espacial el laboratorio más importante, el Destiny, de fabricación estadounidense. Fue acoplado a la estación el 7 de febrero de 2001 mediante el Transbordador espacial Atlantis. El 19 de abril de 2001 fue colocado el primer brazo de la 
EEI, de fabricación canadiense. Con el brazo SSRMS también llegaron un pequeño módulo italiano y una antena UHF. El 12 de julio de ese mismo año se añadió una cámara de descompresión para que los tripulantes pudieran salir de la estación espacial y dar los primeros paseos espaciales. El 14 de septiembre del 2001 se añadió un módulo de atraque ruso con una cámara de descompresión.

El 8 de abril de 2002 se acopló el segmento central ITS S0 del futuro armazón de 91 metros para soportar los paneles solares de gran tamaño en los extremos de la ISS. El brazo SSRMS canadiense que se había colocado en el módulo Destiny fue trasladado al segmento central ITS S0 el 5 de junio de ese mismo año. El 7 de octubre se colocó el segmento de estribor ITS S1 del armazón de la estación. El armazón principal se completó el 23 de noviembre de 2002 con el segmento de babor ITS P1.

El 27 de febrero de 2004, los tripulantes Michael Foale y Alexandr Kaleri realizaron el primer paseo espacial que involucraba a la totalidad de la tripulación. La mayoría de los objetivos del paseo, incluyendo la instalación de equipo externo, se lograron antes de que se abortara la misión debido a un problema de refrigeración en el traje de Kalery HL.

El 28 de julio de 2005 llegó a la estación el módulo italiano de carga Raffaello a través del transbordador Discovery de la NASA.

El 27 de junio de 2006 una pieza de basura espacial, que posteriormente fue identificada como el satélite militar estadounidense Hitch Hiker 1 lanzado en 1963, y ya fuera de servicio, pasó a aproximadamente 2 kilómetros de la ISS (esta se mueve a unos 7,7km/s). Este suceso provocó una situación de alarma y se iniciaron preparativos para una evacuación de emergencia en la Estación Espacial. Este acercamiento estuvo monitorizado por técnicos del CCVE ruso y el Centro de la NASA en Houston, y concluyó sin incidentes. Se estimó que la pieza de chatarra espacial tenía una masa de 79 kilos.

El 7 de julio de 2006 el transbordador Discovery se acopló a la ISS con éxito. Entre la tripulación del Discovery estaba el astronauta alemán Thomas Reiter que junto con el estadounidense Jeff Williams y el ruso Pavel Vinogradov formaron la tripulación permanente del complejo orbital. Con la llegada del astronauta de la ESA la estación pasa de una tripulación permanente de dos astronautas a tres.

El 8 de junio de 2007, el transbordador Atlantis (misión STS-117) parte para la Estación Espacial Internacional para instalar unos nuevos paneles solares tarea que realiza con éxito. El día 10 se detecta una grieta en la cubierta térmica del transbordador Atlantis que debe repararse en vuelo. El día 14 se produce un fallo informático grave que deja sin agua, luz y capacidad de orientación a la estación espacial. En el peor de los casos, esta tendría que haber sido desalojada, pero el fallo se solucionó y los sistemas volvieron a funcionar con normalidad.

El 17 de junio de 2007 la astronauta Sunita Williams se convierte en la mujer que más tiempo seguido ha estado en el espacio, al completar 188 días y 4 horas fuera de nuestro planeta.

El 23 de octubre de 2007 partió el módulo de fabricación italiana "Harmony" hacia la ISS con la misión STS-120 y se montó provisionalmente tres días más tarde en "Unitiy", tomando finalmente su posición definitiva en el extremo del laboratorio "Destiny". Con un peso cercano a las 15 toneladas, su objetivo es servir como puerto de enlace para los laboratorios europeos y japoneses.

En febrero de 2008 se añadió el módulo Columbus europeo y en junio el transbordador Discovery visitó nuevamente la Estación Espacial Internacional y añadió componentes nuevos, de los cuales destaca el módulo principal del esperado "Kibo Science Laboratory".

En marzo de 2009 se agregó el cuarto y último módulo de paneles solares (el S6) por la misión STS-119. En mayo de 2009 la Estación ya podía albergar a seis tripulantes dentro de ella.

El último elemento constructivo del módulo Kibo se instaló en junio por la misión STS-127. En noviembre de 2009, el módulo de acoplamiento ruso "Poisk" llegó a la estación. En febrero de 2010 se instaló el nodo de empalme "Tranquility" ("Node 3") con la cúpula de vista panorámica "Cupola". En mayo de 2010 le siguió el módulo ruso "Rassvet" y el MPLM "Leonardo" en marzo de 2011. El 23 de octubre de 2010 la ISS efectuó el relevo de la Mir, el vehículo espacial que había estado durante más tiempo (3644 días) ininterrumpidamente tripulado por seres humanos. Ese récord se ha extendido ahora a 4304 días. El experimento del AMS se instaló en mayo de 2011 con el penúltimo Transbordador STS. En el verano de 2013 la estación se completaria además con el módulo de laboratorio ruso "Naúka" o Módulo laboratorio multipropósito pero actualmente su lanzamiento sigue pendiente para mayo de 2021.

Tras el acuerdo de los países participantes de operar la estación en conjunto hasta por lo menos 2020, Rusia planeó la construcción de otros tres módulos que surgieron de una nueva concepción. En 2012 se instalaría, en primer lugar, un módulo de acoplamiento esférico en el extremo inferior del MLM "Nauka". Aquí se acoplarían en 2014 y 2015 dos grandes módulos nuevos (NEM 1 und 2), de investigación y de energía, respectivamente.

En diciembre de 2010, la masa de la estación bordeaba ya las 370 toneladas y su estructura tenía una longitud de 109 metros. Dado que la envergadura definitiva ya se había alcanzado desde la instalación de los primeros paneles solares, la ISS fue desde entonces, y continúa siendo hasta la fecha, la estación espacial más grande que se ha construido en la historia.

De acuerdo con el Memorando de Entendimiento original entre la NASA y Rosaviakosmos, la Estación Espacial Internacional estaba destinado a ser un laboratorio, observatorio y fábrica en la órbita terrestre baja. Además, estaba previsto para proporcionar el transporte, mantenimiento, y actuar como una base de ensayo para posibles futuras misiones a la Luna, Marte y los asteroides. En la Política Nacional del Espacio de Estados Unidos en 2010, la EEI se le dio papeles adicionales de servicio de propósito comercial, diplomáticas y educativas.

La ISS proporciona una plataforma para llevar a cabo la investigación científica que no se podría realizar de cualquier otra forma. Aunque una pequeña nave espacial no tripulada puede proporcionar plataformas de gravedad cero y exposición al espacio, las estaciones espaciales ofrecen un ambiente a largo plazo donde los estudios se pueden realizar potencialmente durante décadas, junto con un acceso inmediato a los investigadores humanos en períodos que exceden las capacidades de las naves espaciales tripuladas. La estación simplifica experimentos individuales, eliminando la necesidad de que los lanzamientos de cohetes y personal de investigación estén por separado. Los campos principales de investigación incluyen la astrobiología, la astronomía, la investigación humana, incluida la medicina espacial y ciencias de la vida, ciencias físicas, ciencias de los materiales, el clima espacial y el clima en la Tierra (meteorología). Los científicos de la Tierra tienen acceso a los datos de la tripulación y pueden modificar los experimentos o comenzar nuevos. Los beneficios generalmente no están disponibles en la nave espacial no tripulada. Las tripulaciones vuelan misiones de varios meses de duración, que proporcionan aproximadamente 160 horas-hombre a la semana de trabajo con una tripulación de seis personas.

Con el fin de detectar la materia oscura y ayudar a responder a otras preguntas fundamentales acerca de nuestro universo, ingenieros y científicos de todo el mundo construyeron el Espectrómetro Magnético Alpha (AMS), que la NASA compara con el telescopio espacial Hubble, y afirma que no se puede alojar en un vuelo de plataforma libre, debido en parte a sus requerimientos de energía y a las necesidades de ancho de banda de datos del satélite. El 3 de abril de 2013, científicos de la NASA informaron que podrían haber sido detectados indicios de materia oscura por el Espectrómetro. Según los científicos, "Los primeros resultados del Espectrómetro Magnético Alpha en el espacio transmitidos confirman un exceso inexplicable de positrones de alta energía en los rayos cósmicos con destino a la Tierra."
El ambiente espacial es hostil para la vida: presenta un intenso campo de radiación (conformado principalmente por protones y otras partículas subatómicas cargadas del viento solar, además de rayos cósmicos), gran vacío, temperaturas extremas, y la microgravedad. Algunas formas sencillas de la vida llamadas extremófilos, incluyendo pequeños invertebrados llamados tardígrados pueden sobrevivir en este ambiente en un estado extremadamente seco llamado desecación. La investigación médica mejora el conocimiento sobre los efectos de la exposición espacial a largo plazo en el cuerpo humano, incluyendo la atrofia muscular, pérdida de masa ósea y movimiento de fluidos. Estos datos se utilizan para determinar si largos vuelos espaciales y la colonización del espacio son factibles por el hombre. A partir de 2006, los datos sobre la pérdida ósea y la atrofia muscular sugieren que habría un riesgo significativo de fracturas y problemas de movimiento si los astronautas aterrizaran en un planeta después de un largo viaje interplanetario, como el intervalo de seis meses requerido para viajar a Marte. Se realizan estudios médicos a bordo de la EEI, en nombre del Instituto Nacional de Investigación Biomédica Espacial (NSBRI). Resalta entre estos el del Diagnóstico Avanzado por Ultrasonidos en el estudio de la microgravedad en los astronautas que realizan ecografías con la orientación de expertos a distancia. El estudio considera el diagnóstico y tratamiento de condiciones médicas en el espacio. Por lo general, no hay ningún médico a bordo de la ISS y el diagnóstico de las condiciones médicas es un reto. Se prevé que las ecografías guiadas remotamente tendrán aplicación en la Tierra en situaciones de emergencia y de atención rural, donde es difícil el acceso a un médico capacitado.

La gravedad en la ISS es solo un poco más débil que la gravedad que se siente en la superficie terrestre. Sin embargo, los objetos que están en órbita están en un continuo estado de caída libre, lo que resulta en un aparente estado de ingravidez. Esta ingravidez percibida se puede ver perturbada por cinco efectos diferentes:

Los investigadores están estudiando el efecto del medio ambiente casi ingrávido de la estación en la evolución, desarrollo, y crecimiento de los procesos internos de plantas y animales. En respuesta a algunos de estos datos, la NASA quiere investigar los efectos de la microgravedad en el crecimiento de tejidos tridimensionales, parecidos a los humanos, y los cristales de proteínas inusuales que se pueden formar en el espacio.

La investigación de la física de fluidos en condiciones de microgravedad permitirá a los investigadores modelar mejor el comportamiento de los fluidos. Debido a que los líquidos se pueden combinar casi por completo en condiciones de microgravedad, los físicos investigan líquidos inmiscibles en la Tierra. Además, un examen de las reacciones que se desaceleran por baja gravedad y temperatura, dará a los científicos una mejor comprensión de la superconductividad.

El estudio de la ciencia de los materiales es una importante actividad de investigación de la ISS, con el objetivo de obtener beneficios económicos a través de la mejora de las técnicas utilizadas en el suelo. Otras áreas de interés incluyen el efecto de la gravedad sobre el medio ambiente de baja combustión, a través del estudio de la eficiencia de la combustión y el control de las emisiones y contaminantes. Estos hallazgos podrían mejorar los conocimientos actuales sobre la producción de energía, y dar lugar a beneficios económicos y ambientales. Los planes futuros para los investigadores a bordo de la ISS son examinar los aerosoles, ozono, vapor de agua y óxidos en la atmósfera de la Tierra, así como los rayos cósmicos, el polvo cósmico la antimateria y la materia oscura en el universo.

La ISS ofrece una ubicación en la relativa seguridad de la órbita terrestre baja para probar sistemas de la nave que se requerirán para misiones de larga duración a la Luna y Marte. Esto proporciona experiencia en operaciones, mantenimiento, así como las actividades de reparación y reemplazo en órbita, que será habilidades esenciales en el funcionamiento de la nave espacial lejos de la Tierra, los riesgos de misión pueden reducirse y las capacidades de las naves espaciales interplanetarias avanzarían. En referencia al experimento MARS-500, la ESA afirma que "Mientras que la ISS es esencial para responder a las preguntas relativas a los posibles efectos de la ingravidez, la radiación y otros factores específicas del espacio, aspectos tales como el efecto de aislamiento y confinamiento a largo plazo puede ser abordado en forma adecuada a través de simulaciones basadas en tierra”. Sergey Krasnov, jefe de programas de vuelos espaciales humanos de la agencia espacial rusa, Roscosmos, en 2011 sugirió una "versión pequeña" de MARS-500, puede llevarse a cabo en la ISS.

En el año 2009, teniendo en cuenta el valor de la estructura de la alianza en sí, Sergey Krasnov escribió, "Cuando se compara con los socios actúan por separado, los socios de desarrollo de las habilidades y recursos complementarios podrían darnos mucho más aseguramiento del éxito y la seguridad de la exploración espacial. La ISS está ayudando aún más avanzar en la exploración del espacio cercano a la Tierra y la realización de programas futuros de investigación y exploración del sistema solar, incluso la Luna y Marte”. Una misión tripulada a Marte puede ser un esfuerzo multinacional que involucra organismos espaciales y los países fuera de la actual asociación de la EEI. En 2010, el Director General de la ESA, Jean-Jacques Dordain, declaró que su agencia está dispuesta a proponer a los otros cuatro socios que China, India y Corea del Sur serán invitados a unirse a la asociación de la ISS. Jefe de la NASA Charles Bolden declaró en febrero de 2011, "Cualquier misión a Marte es probable que sea un esfuerzo global". Actualmente, la legislación estadounidense impide la NASA cooperación con China en proyectos espaciales.

Estados Unidos mediante su agencia espacial gubernamental, la NASA, es la iniciadora del proyecto, y responsable de su buen desarrollo. La principal empresa constructora es el grupo Boeing Space, y su participación material incluye la estructura principal (el armazón que une la estación con los grandes paneles de los extremos), cuatro pares de paneles solares, tres módulos que forman el nodo 1 (Unity) de conexión que incluye las cámaras de acople para las naves espaciales y otros elementos menores. También fabrica los tanques de aire respirable que abastecen tanto los módulos de vivienda como los módulos de servicio tanto estadounidenses como rusos. La NASA proporciona también el módulo de vivienda, el laboratorio Destiny y el módulo de conexión a la centrifugadora. La logística bajo la responsabilidad de la NASA incluye la potencia eléctrica, las comunicaciones y el tratamiento de los datos, el control térmico, el control del medio ambiente habitable y el mantenimiento de la salud de la tripulación. Los giroscopios de la EEI están también bajo su responsabilidad y también esta SpaceX.

La Agencia Espacial Federal Rusa (Roscosmos) proporciona alrededor de un tercio de la masa de la ISS (el segmento orbital ruso), con la participación de sus principales empresas: Rocket Space Corporation-Energía y Krunitchev Space Center. La agencia rusa ha proporcionado un módulo de servicio habitable, que fue el primer elemento ocupado por una tripulación; un módulo de acople universal que permite el acople de naves tanto de Estados Unidos (transbordador espacial) como de Rusia (Soyuz); y varios módulos de investigación. Rusia también se implica bastante en el suministro de la estación así como para su mantenimiento en órbita, utilizando, en particular, naves de suministro de víveres Progress. El módulo de control ruso Zarya fue el primer elemento en ponerse en órbita.

Rusia también proporciona el sistema de aproximación KURS para la EEI, el cual fue usado exitosamente en la estación MIR.

La mayoría de los estados miembros de la ESA trabajan en la ISS, en particular, proporcionando el COF (Columbus Orbital Facility, simplemente llamado Columbus), módulo que puede recibir 10 paletas de instrumentos, la mitad europeas, y el ATV (Automated Transfer Vehicle) vehículo que llevará víveres al complejo orbital. La ESA es también responsable del brazo manipulador europeo, que se utilizará desde las plataformas científicas y logísticas rusas, así como sistemas de gestión de datos del módulo de servicio. Sin olvidar los lanzadores Ariane 5, que se utilizarán para el suministro de la EEI de combustible y material a través de los ATV.

La Agencia Espacial Canadiense asume la realización del brazo robótico SSRMS, también denominado Canadarm, un único dispositivo destinado a proporcionar el montaje y el mantenimiento de la estación. Canadá proporciona también el SVS (Space Vision System), un sistema de cámaras que ya se probó sobre el brazo manipulador del transbordador espacial estadounidense destinado a asistir a los astronautas encargados de su utilización y herramienta vital para el mantenimiento de la estación.

La JAXA (Agencia Japonesa de Exploración Aeroespacial) proporciona el JEM (Japanese Experiment Module), que alberga varios compartimentos a presión habitables, una plataforma donde 10 paletas de instrumentos pueden exponerse al vacío espacial y un brazo manipulador específico. El módulo a presión puede por su parte acoger también 10 paletas de instrumentos y otros.

Los siguientes países son meramente colaboradores:

Independientemente de su participación en la ESA, la ASI (Agencia Espacial Italiana) proporciona tres módulos logísticos polivalentes. Concebidos para poder integrar la bodega de la lanzadera estadounidense, implican compartimentos a presión y traerán distintos instrumentos y experimentos a bordo de la EEI. La concepción del módulo europeo Colombus se inspira de sobra en estos tres elementos. La ASI proporciona también los nodos 2 y 3 de la estación.

Bajo la dirección de la Agencia Espacial Brasileña, el Instituto Nacional de Pesquisas Espaciais proporciona un panel de instrumentos y su sistema de fijación que acogerá distintos experimentos de la estación. Transportado por un transbordador, el panel está destinado a exponerse al vacío espacial durante un largo período.

En general se distingue entre los módulos presurizados y los no presurizados. Todos los módulos que los astronautas utilizan para vivir y trabajar, están presurizados, puesto que los seres humanos no podrían sobrevivir en el vacío. El sistema de soporte vital a bordo se encarga de proveer una atmósfera que corresponde a la terrestre: 21% de oxígeno, 78% de nitrógeno y una presión de 1014 hectopascales. Entre los módulos presurizados se cuentan, por ejemplo, el laboratorio estadounidense Destiny o el módulo ruso Zarya. En cambio, los paneles solares o las estructuras del rack no están presurizadas.

El Nodo 1 (o nodo Unity) es una galería de una longitud de aproximadamente 6,5m y un diámetro de 5,5m que conecta las áreas de alojamiento y trabajo de la ISS. Además de su conexión a Zarya, el nodo sirve de conexión con el módulo estadounidense Destiny, el de alojamientos y al compartimiento estanco Pirs.

Los elementos esenciales tales como líquidos, así como el control del soporte vital, sistemas eléctricos y de datos, deben pasar por fuerza a través del nodo, ya que este conecta las áreas de trabajo y habitables. Se instalaron en total más de 50000 elementos mecánicos, 216 líneas de transporte de líquidos y gases y 121 cables eléctricos internos y externos, empleando más de 10 kilómetros de cable.

Se construyó en Hunstville, Alabama y la instalación principal de hardware en el Unity, se completó en junio de 1997 en el Centro de Vuelo Espacial Marshall de la NASA. Fue lanzado a bordo del transbordador Endeavour el 4 de diciembre de 1998. El Unity fue ensamblado al módulo de control Zarya en el transcurso de tres paseos espaciales llevados a cabo durante el séptimo día de misión del Endeavour.

El módulo Zarya, también llamado "Functional Cargo Block" y por las siglas rusas FGB, fue el primer componente lanzado de la estación espacial internacional. Este módulo fue diseñado para proporcionar la propulsión y la energía inicial del complejo orbital. El módulo presurizado de 19 323 kg fue lanzado en un cohete ruso "Protón" en noviembre de 1998.

El Zarya fue financiado por Estados Unidos y construido por Rusia. Su nombre significa «salida del sol» en ruso. Se considera un componente estadounidense de la estación, aunque fuese construido y lanzado por Rusia. El módulo fue construido en el Centro de Investigación y Producción Espacial y el Khrunichev State Research, conocido también como KhSC, localizado en Moscú bajo subcontrato de la compañía Boeing para la NASA.

El módulo Zarya tiene 12,6m de longitud y 4,1m en su punto más ancho. Tiene una estimación de vida operacional de por lo menos 15 años. Sus paneles solares y sus seis baterías de níquel-cadmio pueden proporcionar un promedio de 3kW de corriente eléctrica. Sus escotillas laterales permiten el acople de la naves rusas Soyuz y las naves de abastecimiento Progress.

El módulo de servicio Zvezda (debe leerse "/zviezdá/") ("estrella" en español) era la primera contribución completamente rusa a la Estación Espacial Internacional y sirvió como la temprana piedra angular para el primer habitáculo humano de la estación. El diseño de este módulo deriva del de las estaciones Saliut. El módulo proporciona los primeros habitáculos de la estación, los sistemas de soporte de vida, distribución de la corriente eléctrica, sistema de procesamiento de datos, sistema de mandos de vuelo y sistema de propulsión. También proporciona un sistema de comunicaciones que incluye capacidades de comando como regular el vuelo. Aunque muchos de estos sistemas están siendo sustituidos o suplidos por los componentes estadounidenses de la estación, el módulo de servicio Zvezda seguirá siendo siempre el centro estructural y funcional del segmento ruso de la estación espacial internacional.

El Destiny es el laboratorio de investigación primario. Soporta una amplia gama de experimentos y estudios que intentan contribuir a la salud, seguridad y calidad de vida de la gente de todo el mundo. El laboratorio de la estación ofrece a los investigadores una oportunidad sin par para probar procesos físicos en ausencia de gravedad. El objetivo de los experimentos de este laboratorio es permitir que los científicos entiendan mejor la Tierra y preparar misiones futuras a la Luna y a Marte.

El transbordador "Atlantis" acopló mediante su brazo este laboratorio espacial estadounidense a la estación el 8 de febrero de 2001. Se tuvieron que realizar tres paseos espaciales para activarlo.

El laboratorio fue diseñado para sostener sistemas de estantes modulares (racks) que pueden ser agregados, quitando o sustituyendo cuanto sea necesario. Pueden contener empalmes para fluidos y eléctricos, equipo de video, sensores, reguladores y humidificadores del movimiento para soportar cualquier experimento que se contenga en ellos.

Cuando llegó a la estación, el Destiny contenía cinco estantes eléctricos y los sistemas de soporte de vida. Las siguientes misiones del transbordador han entregado más estantes y experimentos a las instalaciones, incluyendo el Microgravity Science Glovebox, el Human Research Facility y cinco estantes para llevar a cabo varios experimentos científicos.

El Destiny aloja un total de 24 "racks", que se pueden utilizar tanto para experimentos, investigación de nuevos materiales, observaciones de la Tierra y usos comerciales, como también unidades de control y espacio de almacenamiento. Actualmente se desarrollan en el laboratorio experimentos en el área de microgravitación, estudios científicos sobre la vida humana, biología, ecología, ciencias de la tierra, investigación espacial y tecnología.

El Destiny, ensamblado con los módulos-laboratorios Kobi, de la NASA, y el Columbus de la ESA, además de su papel como laboratorio científico, también contiene el centro de control para las operaciones robóticas del brazo de la estación.

El compartimiento o cámara de descompresión Pirs posee dos escotillas para salidas extravehiculares, además de dos sistemas de acoplamiento, uno para su unión con el Zviozda, y otro, en el extremo opuesto, para naves Soyuz y Progress.

Fabricado por la empresa rusa S. P. Korolev RSC Energía, el Pirs se emplea como puerto de atraque complementario para vehículos Soyuz y Progress junto al módulo Zviozda. Igualmente sirve como esclusa estanca para permitir la salida de cosmonautas al exterior del complejo, de manera que se puedan realizar paseos espaciales desde la estación.

Una nave de carga rusa Progress modificada fue la que llevó el 17 de septiembre de 2001 el módulo Pirs a la ISS. El vehículo Progress usado transportó 870kg de propergoles y 800kg de cargas diversas, incluyendo el propio Pirs, así como materiales científicos y de otra índole.

Después de varios paseos espaciales el Pirs quedó perfectamente ensamblado al complejo orbital.

El módulo Harmony, anteriormente llamado Nodo 2, lanzado en la misión STS-120fue acoplado a la Estación Espacial Internacional el 14 de noviembre de 2007.<br>

Fue encargado a la empresa italiana Thales Alenia Space, y construido en Turín.
La ESA cedió su propiedad a la NASA en 2003.<br>
Es un módulo de soporte vital, ya que proporciona oxígeno, electricidad, agua y otros sistemas necesarios para el correcto desarrollo de la estancia de los astronautas. Además posee capacidad para albergar dos dormitorios para los seis posibles tripulantes de la EEI.Harmony servirá también como punto de conexión para el módulo europeo Columbus y el laboratorio japonés Kibo.

Este laboratorio es un módulo cilíndrico muy similar en forma al módulo logístico de funcionamientos múltiples. El módulo contiene 10 estantes ISPR (Estantes Internacionales Estándar de Carga Útil). Hay 4 de ellos en la parte delantera, 4 laterales y 2 en el techo. Los 3 restantes se equipan con los sistemas de soporte de vida. Hay 4 estantes que pueden colocarse con experimentos en los paneles externos para someterlos al vacío espacial. Estos paneles se encuentran arriba y abajo de la escotilla.

El laboratorio tiene una longitud de 6,87m, un diámetro de 4,49m y un peso bruto de 10,3 toneladas, que puede llegar hasta las 19,3t cuando el laboratorio esté a su máxima capacidad.

El Columbus se remonta a 1985 cuando la ESA aprobó el programa de mismo nombre. El programa pretendía crear una estación espacial europea, acompañada por el Hermes (un proyecto de mini-nave europea). El proyecto incluía una plataforma de experimentación de vuelos no tripulados, un módulo presurizado unido (APM) y un satélite de comunicaciones con disponibilidad para compartir datos entre él y la Tierra. La decisión final fue incluir el Columbus en la Estación Espacial Internacional debido a algunos recortes presupuestarios. De todo el proyecto creado para una estación espacial europea solo permaneció el APM, renombrado Columbus Orbital Facility o comúnmente conocido como Columbus.

Se prevé que su vida útil sea de 10 años.

Kibō está destinado a acelerar el progreso de Japón en la ciencia y la tecnología, adquirir nuevos conocimientos y aplicarlo a campos como la industria y la medicina.

El JEM (módulo japonés de experimentos) —llamado en japonés , que significa «esperanza»— es el primer complejo habitable espacial de Japón y realiza las capacidades únicas de investigación de la Estación Espacial Internacional.

En el Kibo se realizan experimentos en las áreas de medicina espacial, biología, observaciones de la Tierra, producción material, biotecnología e investigación de las comunicaciones. Los experimentos y los sistemas de Kibo funcionan en las operaciones de la estación espacial desde la sala de control de la misión, o SSOF, en el Space Center de Tsukuba en la prefectura de Ibaraki, Japón.

El módulo presurizado Kibo fue fabricado en Nagoya y tiene 11,2 metros de largo. Kibo está formado por varios componentes: dos instalaciones de investigación, un módulo presurizado y una instalación expuesta al espacio; llevarán un módulo de logística unido a cada uno de ellos; un sistema de manipulación alejado; y una unidad del sistema de comunicación de la inter-órbita.espacial.

Fue acoplado a la Estación Espacial Internacional a través de los vuelos STS-123 STS-124 y STS-127. El montaje se concluyó en junio de 2008. Su punto de conexión con la EEI es el módulo Harmony.

Lanzamiento: 10 de noviembre de 2009 con la Soyuz FG.

Este componente ruso para la ISS, MRM2 se utiliza para el atraque de buques de la Soyuz y de la Progress, como una esclusa para paseos espaciales, y como una interfaz para experimentos científicos.

El último de los nodos de la estación de Estados Unidos. El nodo Tranquility contiene un sistema de apoyo vital avanzado para reciclar las aguas residuales de la tripulación y generar oxígeno para que la tripulación respire. El módulo está provisto de seis posiciones de atraque, sin embargo cuatro de esas localizaciones están deshabilitadas ya que los módulos que estaban previstos añadirse en un inicio al Tranquility fueron cancelados. Al igual que con el módulo Harmony (Nodo 2) fue construido en Italia pero por un contrato de la ESA con la NASA, siendo propiedad de la última. Es utilizado como compartimento de carga, ya que su anterior cometido estaba relacionado con el módulo habitacional y con el de Crew Return Vehicle (vehículo de retorno de la tripulación), que fueron cancelados en 2001 y 2002 respectivamente. Fue lanzado en febrero de 2010 con el Transbordador Espacial Endeavour como parte de la misión STS-130.

El módulo Cúpula está concebido para ser un observatorio y área de control de la estación espacial. Llamado así por su forma de cúpula, cuenta con siete ventanas que proporcionan una visión panorámica a los tripulantes en el interior para observar y dirigir operaciones en el exterior de la estación.

El módulo cuenta con terminales de trabajo y puede controlar otro hardware, como el brazo robótico de la estación y se comunica con los otros miembros en otras partes de la estación o en el exterior durante los paseos espaciales. La cúpula también se utiliza como observatorio de la Tierra.

La cúpula es el resultado de un acuerdo de intercambio bilateral entre la Agencia Espacial Europea (ESA) y la NASA. La ESA, encargada de su construcción, contrató a la empresa Alenia Spazio como contratista principal y coordina a otras seis empresas europeas: APCO (Suiza), EADS Space Transportation (Alemania), CASA (España), SAAB Ericsson y Lindholmen Development (Suecia), y Verhaert (Bélgica)

Este módulo fue almacenado en el Centro Espacial Kennedy, hasta su lanzamiento en febrero de 2010 con el Transbordador Espacial Endeavour como parte de la misión STS-130.

El cuarto de baño para los astronautas está diseñado para hombres y mujeres, y aunque es similar en apariencia a uno terrestre, tiene una serie de características de diseño especiales. El escusado tiene un candado para las piernas y adaptaciones para mejor comodidad de los muslos. Tiene una potente bomba de aire para poder expulsar los residuos. El astronauta tiene que virar un resorte montado en el asiento del inodoro, y luego el sistema activa un potente ventilador y se abre un agujero de succión mediante el cual el flujo de aire retira todos los residuos. El aire de los inodoros antes de entrar en las instalaciones es cuidadosamente filtrado para eliminar las bacterias y el mal olor.

Este armazón de aluminio forma la espina dorsal de la Estación Espacial Internacional. El ITS (Integrated Truss Structure) soporta los radiadores de la ISS, los gigantescos paneles solares de sus extremos, la estructura móvil del brazo canadiense y otros equipos.

Inicialmente la NASA diseñó esta estructura como soporte de ocho paneles solares enormes, cuatro de menor tamaño y dos radiadores para la Estación Espacial Freedom. Dicha estación fue cancelada por falta de presupuesto. Una vez firmado el acuerdo para crear una estación internacional, la NASA aprovechó el diseño inicial de la estructura de la Freedom y lo aplicó al de la EEI con pequeñas modificaciones.

En 1991 se terminó el diseño de la estructura, dividiéndola para ser enviada por partes en la bodega del transbordador. Dividida en cinco segmentos, esta estructura se terminó de ensamblar en 2007.

Aparte de los paneles solares más pequeños al lado de los módulos rusos, que se utilizaron sobre todo en la fase inicial de la construcción, la ISS posee cuatro grandes paneles solares. Estos están fijados a los elementos P6 y P4 del lado izquierdo; S6 y S4 al lado derecho. Estos elementos pueden moverse sobre dos ejes, de modo tal que pueden dirigirse en todo momento de manera óptima hacia el sol.

El calor excesivo se evacúa a través de elementos de irradiación. Hay radiadores de tres filas en los elementos centrales del ITS, S1 y P1. Adicionalmente, en cada módulo solar hay un radiador más pequeño. Los radiadores impiden la acumulación de calor, constituyendo así la contraparte lógica de los paneles solares que proveen de energía a la estación.

Ordenados por orden cronológico previsto de lanzamiento.

Lanzamiento programado inicialmente: diciembre del 2011 con el cohete Protón-M. Rusia ha pospuesto este lanzamiento en varias ocasiones. 

En un anuncio de 2017, la Agencia Espacial Rusa anunciaó que lo lanzaría a finales de 2017, mediante un cohete del tipo Protón-M el MLM (Módulo Laboratorio Multipropósito), llamado Nauka. Este módulo será el más importante que Rusia ponga en órbita para fines científicos en la Estación Espacial Internacional. Cuando sea lanzado será cuarto modulo dedicado en la ISS para la investigación científica. Este módulo equipará un sistema de control de altitud que podrán usar en caso de necesitarlo los miembros de la Estación y será acoplado en el puerto de atraque del módulo Zvezda. El Brazo Robótico Europeo será lanzado junto a este laboratorio por el acuerdo que firmaron en 2005 la ESA y el Roskosmos.

Según los últimos planes de ROSCOSMOS, debido a los retrasos sufridos por las diferentes pruebas realizadas en el mantenimiento del modulo en tierra, como por los retrasos en el lanzamiento al espacio del ingeniero y cosmonauta Nikolai Tikhonov, encargado de la instalación del modulo en la Estación Espacial Internacional, que fue retirado en último momento de las misiones Soyuz MS-04 en 2017, Soyuz MS-10 en 2018 y este último año 2020 retirado, por una lesión fortuita en el ojo con una rama, de la misión Soyuz MS-16 a un mes del lanzamiento en febrero de 2020. Actualmente esta previsto su lanzamiento en mayo del año 2021, coincidiendo con el lanzamiento de la Soyuz MS-18, a la que esta asignado el cosmonauta Tikhonov.

El plan de ensamblado contemplaba un Módulo de investigación ruso o RM, pero este fue cancelado por problemas en 2007 y se decidió enviar en su lugar el Módulo portuario de carga que se ensamblará a la Estación Espacial Internacional mediante la misión STS-131 con fecha prevista para el año 2010.
Entre las funciones que realizará cabe destacar:


El Canadarm2 es un brazo de fabricación canadiense que tiene, además de un tamaño y peso excepcionales, características únicas que lo colocan muy por delante de su ya viejo hermano del Transbordador Espacial.

Tiene 17,6 metros de largo (2,6 metros más que el del transbordador) y es cuatro veces y media más ligero (1800kg contra 410). En realidad no es un brazo sino dos que cuenta con una mano inteligente en cada extremo.

El Canadarm2 puede contar o no con una base, según se requiera, y ella puede ser cualquiera de las dos manos. Cada una de estas manos puede sujetar unos peldaños especiales que se colocarán en puntos estratégicos de la EEI y que la proveerán de energía, datos y conexiones de vídeo. Agarrándose de estos peldaños y soltándose coordinadamente, tal como lo hace un mono para pasar de rama en rama, este robot será capaz de desplazarse de un extremo a otro de la EEI y llegar hasta donde se le requiera para tareas tan delicadas como enchufar conectores, o tan pesadas como ayudar a acoplarse a las naves de transporte.

Otra de sus virtudes es la fuerza bruta. El Canadarm2 será capaz de manejar volúmenes como vagones ferroviarios de hasta 116 toneladas.

El nuevo brazo fue estrenado en junio de 2001, cuando el Atlantis trajo la cámara de descompresión Quest para los paseos espaciales de la EEI, el Canadarm2 ayudó a colocar la cámara en su sitio.

Esta es solo la primera parte del Sistema de Servicio Móvil de la estación espacial (SSRMS). La segunda parte es el Sistema de Base Móvil, del tamaño de un camión, se desplazará sobre raíles para llevar al brazo canadiense más rápidamente de un extremo a otro de la estación espacial. La tercera y última parte, es el Manipulador Hábil para Propósitos Especiales. Es una mano inteligente equipada con luces, cámaras y pañol de herramientas que podrá instalar y reemplazar baterías, fuentes de energía y hasta delicadas computadoras.

El Canadarm2 se controla desde el laboratorio Destiny y los astronautas que lo operan serán apoyados por dos subcentros de control en la Tierra, uno en Houston (EE. UU.) y el otro en Quebec (Canadá), que están en condiciones de impartirle órdenes extras en caso de que sea necesario.

El Brazo Robótico Europeo (European Robotic Arm) se utilizará para instalar y sustituir placas solares, revisar y ensamblar módulos y para trasladar a los astronautas que realizan los paseos espaciales.

Mide unos 11,3 m de largo y pesa 630kg y es capaz de mover hasta 8000 kilogramos. En apariencia es casi como un brazo humano, con articulaciones y con la capacidad de coger, sujetar y girar como si de una verdadera mano se tratase. Es simétrico en su construcción.

El brazo se puede dirigir desde el exterior, a través de un panel, o desde una sala de control en el interior de la ISS denominada Cúpula por su forma y que a través de sus siete ventanas permitirá a los astronautas ver todos los movimientos del brazo robótico.

Será lanzado junto con el modulo multipropósito Nauka, el laboratorio ruso, a finales de 2021.

Para el transporte de astronautas y víveres y para la construcción de la propia EEI, cada agencia espacial participante cuenta con un vehículo de transporte. Estos vehículos se pueden dividir en tripulados y no tripulados.

Con la salida del transbordador espacial de la NASA del servicio, de 2011 a 2020 solo Rusia poseía un programa espacial tripulado con acceso a la ISS. Los astronautas de las demás nacionalidades se valían de los vehículos rusos Soyuz para llegar al complejo orbital.
EE. UU. reanudó en 2020 sus viajes propios a la ISS, con el lanzamiento de la Crew Dragon Demo-2, el día 30 de mayo 2020 de la nueva nave Dragon 2 y acoplamiento al día siguiente, en su versión tripulada de la nave Dragon 2 desarrollada dentro del programa COTS de la NASA, como parte del CREW, junto con la nave CST-100 Starliner de Boeing que se espera sea lanzada su primera nave tripulada en 2021.

El Transbordador Espacial estadounidense se encargó, hasta julio de 2011, del transporte de los componentes más grandes para su ensamblaje en la estación espacial y de transportar astronautas (hasta siete) para los trabajos de ensamblaje y mantenimiento de la estación, además de llevar a los componentes estadounidenses de las distintas expediciones de la EEI y cuantiosos víveres hasta ella.

La nave rusa Soyuz fue la nave que llevó a los primeros habitantes de la ISS. Se encarga de mantener la tripulación permanente de la estación espacial transportando hasta tres astronautas. Sirve como nave de emergencia por si la ISS debe ser evacuada dado que cada nave Soyuz permanece acoplada una media de seis meses en la estación. Desde 2002 se utilizan las Soyuz TMA diseñadas especialmente para la EEI. Tras el lanzamiento de la Soyuz TMA-22 en septiembre de 2011, se dejó de usar este tipo de nave en favor de la siguiente versión mejorada, Soyuz TMA-M.

El día 30 de mayo de 2020 fue lanzada la primera cápsula Crew Dragon con la misión DM-2, acoplandose el día 31 de mayo a la estación espacial, reanudandose la capacidad de poder transportar a sus astronautas por parte de los EE. UU., perdida en 2011 con la retirada del programa STS

Las agencias espaciales de Rusia, EE. UU. y Japón, mediante sus naves de abastecimiento no tripuladas se encargan de transportar víveres a la estación espacial. Hasta ahora lo han estado haciendo los rusos con el vehículo Progress, y de enero de 2008 a 2014 se añadió el europeo ATV, mucho más grande, con cinco misiones, que complementó en 2009 el programa japonés HTV y las cápsulas SpaceX Dragon y Cygnus desde 2012, producidas por capital privado en EE.UU por el programa COTS.

Las naves Progress rusas son utilizadas para llevar víveres y combustible a la EEI. Ya fueron utilizadas con las estaciones Salyut 6, Salyut 7 y Mir. Además de suministros y equipo, las Progress utilizan sus motores para elevar de forma regular la órbita de la estación. Su diseño está basado en la nave Soyuz.

En 2000 se introdujo el modelo Progress M1, ligera modificación de la Progress M con mayor capacidad de transporte de combustible. Pese a haber introducido la Progress M1, Rusia ha seguido lanzando también Progress M.

Retirado en 2014

Vehículo de Transporte Automático europeo, de un solo uso, se encargó de abastecer a la Estación Espacial Internacional y de evacuar los residuos de 2008 a 2014. El vehículo de carga no tripulado ATV-001 "Julio Verne", construido por la ESA, fue el primero de este tipo de naves, que poseen una mayor capacidad que las Progress, las utilizadas por la Agencia Espacial Rusa actualmente. Su primer lanzamiento se realizó el 9 de marzo del año 2008 en un cohete Ariane 5 y su último lanzamiento fue el 29 de julio de 2014, con la nave ATV-005 "Georges Lemaître", tras lo que fue el programa ATV se dio por finalizado. Aunque la base del modelo del vehículo ATV, se utilizara al menos en las dos primera misiones del programa Artemisa de la NASA de la futura nave Orión a la luna.

Es una aportación de la Agencia Espacial Japonesa al proyecto internacional. Transporta agua, suministros y experimentos a la Estación Espacial Internacional. Aunque es de tamaño mayor que las naves Progress, necesita ser acoplado manualmente usando el Canadarm2 porque no dispone de sistema de acoplamiento automatizado. En su configuración habitual el vehículo está separado en dos secciones: una presurizada que se conecta al puerto nadir del Harmony, y otra no presurizada, generalmente para el transporte de los experimentos de exposición espacial para el módulo Kibo. El primero fue lanzado el 11 de septiembre de 2009, manteniéndose actualmente en uso, y estando acoplado a la estación el HTV-9, desde el 25 de mayo de 2020. 

Vehículo privado desarrollado por la empresa SpaceX bajo el programa COTS de la NASA. aunque han desarrollado la versión tripulada en la dragon 2, la dragon solo se empleaba como vehículo no tripulado de carga. Está propulsado por el vehículo de lanzamiento Falcon 9. El primer lanzamiento de una cápsula SpaceX Dragon hacia la ISS se produjo el 22 de mayo de 2012. Actualmente el programa inicial CRS ha finalizado con el último lanzamiento de la SpaceX CRS-20 Dragon, y este año 2020 se espara el inicio de la segunda fase del programa CRS-2 con la versión de carga de la nave Dragon 2 del programa COTS de la NASA, a partir del lanzamiento de SpaceX CRS-21 en octubre de 2020.

Al igual que el SpaceX Dragon, la nave espacial Cygnus forma parte del programa COTS, por lo que es desarrollado por la compañía privada Orbital ATK. Su primer viaje fue realizado en septiembre de 2013 a bordo de un Antares (cohete), aunque en viajes posteriores también ha sido transportada en un Atlas V. La nave Cygnus se acopla a alguno de los nodos estadounidenses con la ayuda del brazo robótico Canadarm. En sus orígenes podía transportar cerca de tonelada y media de suministros, pero en uno de sus viajes (marzo de 2016), el Cygnus CRS OA-6, la nave llevó poco más de 3 toneladas de carga a la EEI. Tras unos días conectada a la Estación, la Cygnus se separa de la estación cargando basura y residuos para luego reintegrarse a la atmósfera y ser destruida.. La misión actual es Cygnus NG-13, desacoplada el 11 de mayo y pendiente de su desintegración en la atmósfera el 29 de mayo de 2020..

Es la tercera nave desarrollada dentro del programa COTS de la NASA para llevar cargas a la Estación Espacial Internacional por la empresa Sierra Nevada, en su segunda fase como parte del CRS-2 junto a la Dragon 2 de carga de 2020 a 2024. En un principio es como un transbordador en miniatura y también estaría diseñado para transportar a personal, aunque actualmente solo se desarrolla la versión de carga. Esta pendiente su primer lanzamiento en el año 2021 de la nave de prueba SNC DEMO-1 (DREAM CHASER)

La Estación Espacial Internacional es la infraestructura espacial más visitada en la historia de la astronáutica. A día de 5 de junio de 2020, ha tenido ya 240 visitantes (no distintos). La MIR tuvo 137 visitantes (no distintos). El número de visitantes distintos de la EEI es de 151.




La estimación de los costos totales de la ISS es de 100000 millones de dólares estadounidenses (USD). Dar una valoración de costos exacta para la ISS es, sin embargo, muy complicado, y difícilmente determinable qué costes se deben añadir realmente al programa de la ISS o cómo la contribución rusa debe ser medida, dado que la agencia rusa del espacio funciona con USD considerablemente más bajos que los otros socios.

La ISS ha sido descrita como el artículo individual más caro jamás construido. En 2010, el costo total fue de 150 000 millones de dólares estadounidenses. Esto incluye el presupuesto de la NASA de $ 58,7 mil millones (sin ajustar la inflación) para la estación de 1985 a 2015 ($ 72,4 mil millones en dólares de 2010), $ 12 mil millones de Rusia, $ 5 mil millones de Europa, $ 5 mil millones de Japón, $ 2 mil millones de Canadá y el costo de 36 vuelos de transbordador. para construir la estación, estimada en $ 1.4 mil millones cada una, o $ 50.4 mil millones en total. Suponiendo 20.000 días-persona de uso de 2000 a 2015 por tripulaciones de dos a seis personas, cada día-persona costaría $ 7.5 millones, menos de la mitad de los $ 19.6 millones ajustados por inflación ($ 5.5 millones antes de la inflación) por persona-día de Skylab.

En contraste con la creencia común, la mayoría de los costes de la NASA no se destinan inicialmente a construir los módulos de la ISS y la estructura externa en tierra o para los vuelos de tripulación y abastecimiento de la ISS. De hecho, el programa del transbordador espacial, que a fecha de 2006 cuesta casi 5000 millones de dólares anuales, normalmente no se considera parte del presupuesto del ISS, aunque los transbordadores se han utilizado casi exclusivamente para los vuelos a la ISS desde 1998.

La petición de presupuesto de la NASA al gobierno estadounidense correspondiente a 2007 enumera los costes para la ISS (sin costes del transbordador) como 25.600 millones de dólares desde 1994 a 2005. En 2005 y 2006 se asignaron al ISS entre 1700 y 1800 millones de dólares cada año. Esta suma se elevará en 2010, cuando se calcula que alcanzará los 2300 millones de dólares y entonces deberá permanecer en el mismo nivel, al menos hasta 2016 (fecha del final previsto del programa).

Los 1800 millones de dólares dados en 2005 se distribuyen en:






Proyecciones de la NASA que asume los costes medios de 2500 millones de dólares a partir del 2011 hasta el 2016 y el final del dinero destinado a la ISS en 2017 (entre 300 y 500 millones) después de la bajada en 2016, los costes totales del proyecto de la ISS para la NASA desde el comienzo del programa en 1993 hasta su final serán cerca de 53000 millones de dólares. Los 33 vuelos del transbordador (que, según lo mencionado arriba, normalmente no se consideran parte de los costes totales de la ISS) para la construcción y el mantenimiento de la ISS será alrededor de 35000 millones de dólares. También ha habido costes considerables para diseñar la Estación Espacial Freedom en los 1980 y los 1990, antes del programa de la ISS que comenzó en 1993. Por lo tanto, aunque los costes reales contribuidos a la ISS son solamente la mitad de los 100000 millones de dólares citados a menudo en los medios, si se une con los costes del transbordador y el diseño del proyecto precursor, alcanza casi los 100000 millones de dólares de gastos, solamente para la NASA.

Una parte considerable del presupuesto de la Agencia Espacial Federal Rusa se utiliza para la ISS. Desde 1998 ha habido unas dos docenas de vuelos de naves Soyuz y Progress. Desde el desastre del Columbia, ha sido la encargada de transportar a los relevos de la tripulación permanente y mantener el abastecimiento de la estación desde 2003 hasta 2006. La pregunta de cuánto tiempo puede soportar Rusia los costes de la estación es difícil de responder. Los dos módulos rusos en órbita son actualmente derivados del programa de la MIR y por lo tanto los costes del desarrollo son mucho más bajos que para otros módulos, además el cambio entre el rublo y el dólar no está mostrando adecuadamente una comparación verdadera de cuáles son los costes reales para Rusia.

Los 20 millones de dólares que cada turista espacial ha pagado por un asiento en la Soyuz a la EEI ha compensando solamente una parte muy pequeña de la contribución financiera de Rusia a la ISS.

La ESA calcula que su contribución sobre el curso de vida del proyecto (unos 30 años) será de 8000 millones de euros. Los costes para el laboratorio Columbus suman ya más de 1000 millones de euros, los costes para el desarrollo del ATV suman varios cientos de millones y el coste añadido de cada lanzamiento de Ariane 5 llega alrededor de los 125 millones de euros, cada lanzamiento de ATV sumará también costes considerables.

El laboratorio Kibo ha costado 2800 millones de dólaressegún un artículo reciente de este año. Además los costes anuales para el laboratorio Kibo sumarán alrededor de unos 350 a 400 millones de dólares estadounidenses.

Canadá, cuya contribución a la ISS es el Canadarm2 con el soporte móvil, se estima que pasados 20 años habrá contribuido con cerca de 1400 millones de dólares canadienses a la ISS

Desde 2008, siete turistas han visitado la ISS. Por alrededor de 20 millones de dólares, los «turistas» o «participantes» espaciales han comprado un pasaje en una nave Soyuz, junto a miembros de la tripulación rusa en los viajes destinados al cambio de turnos de la tripulación, permaneciendo en la estación por alrededor de una semana. Uno de estos turistas, Charles Simonyi, ha participado ya en dos ocasiones. Estos viajes se han gestionado a través de la empresa Space Adventures. Además, la ISS fue el lugar elegido para la primera boda espacial, en la que el cosmonauta ruso Yuri Malenchenko, de la Expedición 7, contrajo matrimonio con Ekaterina Dmitrieva, quien estaba en Texas en ese momento. La página web oficial de la EEI ofrece una lista alfabética actualizada de los visitantes (astronautas y turistas).

El jueves 12 de marzo de 2009 el objeto 25090 PAM-D estuvo en ruta de colisión con los desechos de la ISS, activando un plan de contingencia de último minuto debido a la tardanza en detectar el evento desde Houston. Como medida de precaución los astronautas abordaron la cápsula rusa Soyuz, cerrando las compuertas respectivas y activando el control automático de la ISS. La cápsula Soyuz permanece constantemente acoplada a la estación espacial como medida de protección, siendo el único medio de evacuación en este tipo de casos. El viernes 6 de noviembre de 2009 ocurrió un hecho similar con un objeto de menor tamaño que orbitó a tan solo 500 metros de la EEI.

La ISS ha experimentado dos fugas de aire, la primera en 2018 la segunda en 2020. La primera se debió a un taladro mal practicado.




</doc>
<doc id="19235" url="https://es.wikipedia.org/wiki?curid=19235" title="Alá">
Alá

«Alá» es la hispanización de la palabra árabe ("al-lāh"), que en español significa ‘el Dios’. Si bien el término es más conocido en Occidente debido a su utilización por parte de los musulmanes (chiíes y suníes) como referencia a Dios, este es utilizado por los hablantes del árabe de todas las religiones abrahámicas, incluidos los cristianos y judíos, se le adoraba agachándose y rezándole el Sala-ah (oración de los islámicos) El término fue utilizado también por los paganos de La Meca como referencia al Dios creador y perpetuo, posiblemente la deidad suprema en la Arabia preislámica.

En el islam, Dios es conocido por distintos nombres: el Justo, el Clemente, el Creador, entre otros. Sin embargo, según el Corán, «Alá» es el nombre en árabe que Dios ha preferido sobre los demás. La raíz lingüística de “Alá” indica que Él es el Único digno y merecedor de ser adorado y obedecido.

Los conceptos asociados con la palabra (como una deidad) difieren entre las tradiciones. En la Arabia preislámica, Alá no era la única divinidad, sino que le asociaban colaboradores y compañeros, hijos e hijas. En el islam, Alá es el supremo y comprende todo nombre divino. Todos los demás nombres divinos se cree que refieren a Alá. Alá es único, el único Dios, trascendente creador del universo y Omnipotente. Los árabes cristianos de hoy no tienen otra palabra para Dios más que Alá, usan términos tales como ("Al-lāh al-ab") para referirse a Dios padre. Hay similitudes y diferencias entre el concepto de Dios tal como es representado en el Corán árabe y la Biblia judeocristiana.

Unicode tiene un código reservado para Al-lāh, ﷲ = U + FDF2. Muchas fuentes árabes también tienen diferentes formas de escribirlo.

Respecto a la etimología de la palabra, hay opiniones muy diversas: ya entre los gramáticos árabes clásicos, esto es, medievales, se cuentan unas veinte opiniones diferentes. Las hipótesis más extendidas son las siguientes:

En cualquier caso la etimología de esta palabra remite a la misma raíz que ʾEl, Eloh y ʾElohīm (אֱלֹהִים), términos que designan a Dios en la Biblia y que forman parte de numerosos nombres propios de origen hebreo, como Samuel, Daniel, Rafael, Miguel, etc.

"“Alá”" es un nombre que, según la tradición islámica, abarca todos los nombres de Dios y es el escogido por Dios Mismo y expuesto en Su Revelación. Por eso los musulmanes dan preferencia a este nombre. Lingüísticamente, “Alá” es una palabra única, No tiene conjugación plural y no tiene género. Esta palabra es el reflejo de un concepto único de Dios en el islam.

"Al-lāh" se traduce como "Dios", con mayúscula, puesto que se refiere al dios único. La palabra "dios" con minúscula, es decir, referida a cualquier otra divinidad, es "ilāh", pl. "ilāhāt" (, pl. ). La complejidad y diferencia de estos conceptos con las lenguas indoeuropeas se explica en "¿Qué es Al-lāh para los musulmanes?" charla pronunciada por Abderrahman Mohamed Maanán, Doctor en filosofía por la Universidad de Sevilla. Los musulmanes no árabes usan siempre Al-lāh en lugar de Dios aduciendo que, puesto que Dios es quien habla en primera persona en el "Corán", Al-lāh, en árabe, es exactamente la palabra que emplea el Ser Supremo para referirse a sí mismo y, por tanto, la mejor para nombrarle. Los musulmanes que viven en un medio no musulmán, y especialmente los conversos, utilizan "Al-lāh" como forma de singularizarse como seguidores de una religión distinta a la mayoritaria.

El musulmán cree en un dios supremo, es decir; afirma la existencia de un Creador del universo y la Tierra, Dueño de todo lo existente, divinidad única, caracterizado de toda perfección, alejado de todo defecto, esto primeramente por guía del Creador a Su siervo, posteriormente por evidencias del instinto natural del ser humano, la razón, los sentidos, signos en la creación y como prueba principal; los textos sagrados.

Las cualidades que los musulmanes le atribuyen son básicamente las mismas que le atribuye el judaísmo, insistiendo en su unidad ("tawhid"), es decir, que es uno y no tiene diversas personas en su carácter incomparable e irrepresentable, es decir no es a imagen y semejanza del hombre.

Un teólogo reconocido en su famoso escrito "Aquidah At Tahawiah" describe a Alá de la siguiente manera:

El islam se refiere a Dios también con otros noventa y nueve nombres, que son otros tantos epítetos referidos a cualidades de Dios: El Clemente (Al-Rahmān), El Apreciadísimo (Al-'Azīz), El Creador (Al-Jāliq), etc. El conjunto de los 99 Nombres de Dios recibe en árabe el nombre de "al-asmā' al-husnà" o "los más bellos nombres", algunos de los cuales también han sido utilizados por cristianos y judíos o han designado a dioses de la Arabia preislámica. Algunas tradiciones afirman que existe un centésimo nombre que permanece incognoscible, que es objeto de especulaciones místicas, y que se define en ocasiones como el Nombre Inmenso ("ism al-'Azam"), o como el Nombre de la Esencia, figura que también existe en el judaísmo, y que ha tenido una gran importancia en el sufismo. Otras veces, se utiliza simplemente la palabra "Rabb" (señor).

La palabra "Al-lāh" está en el origen de algunas palabras españolas como "" ("w[a] shā-llāh": y quiera Alá o y quiera Dios), "olé" ("w[a]-llāh": por Alá o por Dios) o "hala" ("yā-llāh": oh Dios).




</doc>
<doc id="19237" url="https://es.wikipedia.org/wiki?curid=19237" title="George Boole">
George Boole

George Boole () (Lincoln, Lincolnshire, Inglaterra, 2 de noviembre de 1815 - Ballintemple, Condado de Cork, Irlanda, 8 de diciembre de 1864) fue un matemático y lógico británico.

Como inventor del álgebra de Boole, que marca los fundamentos de la moderna, Boole es considerado como uno de los fundadores del campo de las ciencias de la computación. En 1854 publicó "An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities", donde desarrolló un sistema de reglas que le permitían expresar, manipular y simplificar problemas lógicos y filosóficos cuyos argumentos admiten dos estados (verdadero o falso) por procedimientos matemáticos. Se podría decir que es el padre de los operadores lógicos simbólicos y que gracias a su álgebra hoy en día es posible operar simbólicamente para realizar operaciones lógicas.

El padre de George Boole, John Boole (1779-1848), fue un comerciante de escasos recursos. Estuvo especialmente interesado en las matemáticas y la lógica. John dio a su hijo sus primeras lecciones, pero el extraordinario talento matemático de George Boole no se manifestó durante la juventud, ya que al principio mostraba mayor interés por las humanidades; en su adolescencia, aprendió latín, griego, alemán, italiano y francés. Con estas lenguas, fue capaz de leer una gran variedad de teología cristiana.

La combinación de sus intereses por la teología y las matemáticas le llevó a comparar la trinidad cristiana del Padre, Hijo y Espíritu Santo con las tres dimensiones del espacio, y se sintió atraído por el concepto hebreo de Dios como una unidad absoluta. Boole consideró la adopción del judaísmo, pero al final optó por el unitarismo.

No fue hasta su establecimiento exitoso en una escuela en Lincoln, su traslado a Waddington, y más tarde su nombramiento en 1849 como el primer profesor de matemáticas del entonces Queen's College en Cork (en la actualidad, University College Cork) que sus habilidades matemáticas se realizaron plenamente.

En 1855, se casó con Mary Everest, sobrina de George Everest, que más tarde, como la señora de Boole, escribió varios trabajos educativos útiles en los inicios de su marido.

Pese a que Boole publicó poco, excepto su "Lógica" y obras matemáticas, su conocimiento de la literatura en general era amplio y profundo. Dante fue su poeta favorito, prefiriendo el "Paraíso" al "Infierno". La "Metafísica" de Aristóteles, la "Ética" de Spinoza, las obras filosóficas de Cicerón y muchas otras afines fueron también temas frecuentes de estudio. Sus reflexiones sobre cuestiones filosóficas y religiosas de carácter científico estaban orientadas en cuatro direcciones: el genio de sir Isaac Newton; el uso correcto del ocio; las demandas de la Ciencia; y el aspecto social de la cultura intelectual.

El carácter personal de Boole inspiró a todos sus amigos la estima más profunda. Se caracterizó por la modestia, y entregó su vida a la búsqueda de la verdad. Pese a que recibió una medalla de la Royal Society por sus memorias de 1844, y el título honorífico de doctor honoris causa en Derecho de la Universidad de Dublín, no solicitó ni recibió los beneficios ordinarios a los que sus descubrimientos le habrían dado derecho.

El 8 de diciembre de 1864, en pleno vigor de sus facultades intelectuales, murió de un ataque de fiebre, que terminó en un derrame pleural. Fue enterrado en el cementerio de la Iglesia de St. Michael, Church Road, Blackrock (un barrio de la ciudad de Cork, en Irlanda). Hay una placa conmemorativa en la iglesia contigua.

Para el público general Boole es conocido fundamentalmente como el autor de numerosos trabajos abstrusos en el campo matemático, y de distintas publicaciones que se han convertido en tratados. Su primer trabajo publicado fue «Investigaciones en la teoría de las transformaciones de análisis, con una aplicación especial a la reducción de la ecuación general de segundo orden», impreso en el "The Cambridge Mathematical Journal" en febrero de 1840 (volumen 2, n.º 8, pp. 64-73) y que llevó a propiciar la amistad entre Boole y D. F. Gregory, el editor de la revista, que duró hasta la muerte prematura de este último en 1844.

Una larga lista de las memorias y documentos de Boole, tanto en temas de lógica como de matemáticas, se encuentran en el "Catálogo de Memorias de la Ciencia" publicado por la Royal Society, y en el volumen suplementario sobre ecuaciones diferenciales, editado por Isaac Todhunter.

En 1841 Boole publicó un influyente artículo sobre la naciente teoría de invariantes. Recibió una medalla de la Royal Society por su memoria de 1844 titulada "On a General Method of Analysis", una contribución a las ecuaciones diferenciales lineales, partiendo del caso de los coeficientes constantes en los que ya había trabajado, para abordar el caso de los coeficientes variables. Su principal innovación en métodos operacionales consistió en admitir que las operaciones podían no ser conmutativas. En 1847 Boole publicó "The Mathematical Analysis of Logic", el primero de sus trabajos sobre lógica simbólica.

Boole publicaría veintidós artículos en "The Cambridge Mathematical Journal" y en su sucesor, "The Cambridge and Dublin Mathematical Journal". Asimismo, publicaría dieciséis artículos en la tercera y cuarta series del "Philosophical Magazine". La Royal Society tiene impresas seis memorias importantes en las "Philosophical Transactions", y las memorias de algunos otros trabajos se encuentran en las "Transactions of the Royal Society of Edinburgh" y de la Real Academia de Irlanda, en el "Bulletin de l'Académie de St-Pétersbourg" de 1862 (bajo el nombre de G. Boldt, vol. iv, pp. 198-215), y en la "Revista de Crelle". También se incluye un documento sobre la base matemática de la lógica, publicado en el "Mechanic's Magazine" en 1848.

Las obras de Boole figuran de manera dispersa entre unos cincuenta artículos y otras publicaciones independientes. Solo dos tratados sistemáticos sobre temas matemáticos fueron completados por Boole durante su vida. El conocido "Tratado sobre ecuaciones diferenciales" apareció en 1859, y fue seguido, al año siguiente, por un "Tratado sobre el cálculo de las diferencias finitas", diseñado para servir como una secuela de la obra anterior. Estos tratados son valiosas contribuciones a las ramas importantes de la matemática que se tratan en ellos. Hasta cierto punto, estas obras representan los más relevantes descubrimientos de su autor en el campo del cálculo. En los capítulos decimosexto y decimoséptimo de las "Ecuaciones diferenciales" pueden encontrarse, por ejemplo, el desarrollo del método simbólico general, con el hábil y audaz empleo del procedimiento que condujo a Boole hacia sus demás descubrimientos, y de un método general de análisis, descrito originalmente en su famosa memoria impresa en las "Philosophical Transactions" de 1844. Boole fue uno de los primeros y más eminentes matemáticos que percibieron que los símbolos de las operaciones podían ser separados de las cantidades sobre las que operan, y ser tratados como objetos distintos del propio cálculo. La principal característica de Boole fue su absoluta confianza en cualquier resultado obtenido por el tratamiento de los símbolos de conformidad con sus leyes primarias y condiciones, y una habilidad casi inigualable para poder localizar aplicaciones para estos resultados.

Durante los últimos años de su vida Boole se dedicó constantemente a la ampliación de sus investigaciones con el objeto de producir una segunda edición de sus ecuaciones diferenciales mucho más completa que la primera edición, y parte de sus últimas vacaciones las pasó en las bibliotecas de la Royal Society y del Museo Británico, pero esta nueva edición nunca se completó. Los manuscritos dejados a su muerte fueron tan incompletos que incluso Isaac Todhunter, a cargo de quien se dejaron, fue incapaz de completar una segunda edición del tratado original, y los publicó en 1865 en un volumen suplementario.

Con la excepción de Augustus De Morgan, Boole fue probablemente el primer matemático inglés desde los tiempos de John Wallis que escribió sobre lógica. Sus puntos de vista sobre la aplicación del método lógico se debían a la misma confianza profunda en el razonamiento simbólico con el que había irrumpido, con éxito, en la investigación matemática. Las especulaciones sobre un cálculo del razonamiento ocuparon los pensamientos de Boole, pero no fue hasta la primavera de 1847 cuando expresó sus ideas en el folleto titulado "Análisis matemático de la lógica". Consideró esta publicación como una precipitada e imperfecta exposición de su sistema lógico. Posteriormente, Boole manifestó que su trabajo más importante, su "Investigación sobre las Leyes del Pensamiento" (1854), en el que se sustentan sus teorías matemáticas sobre la Lógica y la Probabilidad, solo debía ser considerado como una declaración madurada de sus puntos de vista. Esta obra marcó el comienzo de un nuevo enfoque sobre la naturaleza de la validación de argumentos y pruebas. Sin embargo, es fácil apreciar un innegable encanto en la originalidad de su obra lógica anterior.

Boole no consideraba la lógica como una rama de las matemáticas, como podría interpretarse por el título de su folleto anterior, pero señaló una profunda analogía entre los símbolos del álgebra y la representación simbólica, en su opinión, necesaria para representar formas lógicas y silogismos, haciendo coincidir la lógica formal con la matemática limitada al uso de operaciones con ceros y unos. Para unificar distintos sistemas de operadores lógicos, Boole organizó el universo de todos estos objetos imaginables; creando una notación simbólica adecuada a sus propósitos, con símbolos tales como x, y, z, v, u, etc., que utiliza para caracterizar los atributos correspondientes a adjetivos y sustantivos comunes. Propuso que las proposiciones lógicas se deben expresar en forma de ecuaciones algebraicas, de forma que la manipulación algebraica de los símbolos en las ecuaciones proporciona un método a prueba de fallos de la deducción lógica, es decir, la lógica se reduce al álgebra. Mediante el uso de símbolos, tales proposiciones se podrían reducir a la forma de ecuaciones, y la conclusión silogística a partir de dos premisas se obtiene eliminando el término medio de acuerdo con las reglas ordinarias algebraicas.

Aún más original y notable, sin embargo, fue que parte de su sistema, totalmente basado en sus "Leyes del pensamiento", permitió estructurar un método simbólico general de la lógica de la inferencia. Dada una proposición que implique un número cualquiera de términos, Boole demostró cómo, por el tratamiento puramente simbólico de estas premisas, se podía deducir cualquier conclusión lógica contenida en dichas premisas. La segunda parte de sus "Leyes del pensamiento" contiene su correspondiente intento de descubrir un método general de las probabilidades, que como consecuencia, debe permitir determinar la probabilidad de cualquier evento lógicamente relacionado con un sistema de acontecimientos dados, a partir de las probabilidades del citado sistema de acontecimientos dados.

En 1921 el economista John Maynard Keynes publicó un libro que se ha convertido en un clásico en la teoría de la probabilidad, "A Treatise of Probability" ("Tratado de la probabilidad"). En su libro, Keynes comentaba la teoría de Boole sobre la probabilidad, y sostenía que Boole había cometido un error fundamental acerca del concepto de "independencia estocástica" que a su juicio viciaba la mayor parte del trabajo de su predecesor. En su libro, "The Last Challenge Problem: George Boole's Theory of Probability" (2009), David Miller proporciona un método general de acuerdo con el sistema de Boole, e intenta resolver los problemas reconocidos anteriormente por Keynes y otros autores.

En 1857, Boole publicó su tratado "On the Comparison of Transcendents, with Certain Applications to the Theory of Definite Integrals" ("Comparación de transcendentes, con ciertas aplicaciones a la teoría de integrales definidas"), donde estudiaba la suma de residuos de una función racional. Entre otros resultados, probó la conocida como "identidad de Boole":

para cualesquiera números reales "a" > 0, "b", y "t" > 0. La generalización de esta identidad juega un importante papel en la teoría de la transformada de Hilbert.


Boole tuvo cinco hijas:






</doc>
<doc id="19238" url="https://es.wikipedia.org/wiki?curid=19238" title="Muhammad (nombre)">
Muhammad (nombre)

Muhammad ( ), también transcrito Moham(m)ed, Moha(m)mad o de otras maneras que reflejan sus variantes de pronunciación con diferentes acentos, es un nombre propio de varón de origen árabe, aunque muy extendido en todo el mundo islámico debido a que es el nombre del profeta del islam Mahoma. Es catalogado como el nombre más utilizado de la tierra, debido a la cantidad de musulmanes que hay hoy en día. Se estima que más de 150 millones de personas en todo el mundo se llaman Muhammad o cualquiera de sus variantes. Significa «Alabado».

"Muhammad" es el participio pasivo masculino del verbo "hammada", que significa «alabar». Se relaciona semántica y etimológicamente con los nombres propios árabes Ahmad (m), Hamid (m), Hamida (f) y Mahmud (m).

Mahmad, Mahamed, Mohamad, Mohamed, Mohammad, Mohammed, Muhamad, Muhamed, Muhamet, Muhammed, Muhammet, Mahammud, Mehmet.



</doc>
<doc id="19239" url="https://es.wikipedia.org/wiki?curid=19239" title="Josemaría Escrivá de Balaguer">
Josemaría Escrivá de Balaguer

Josemaría Escrivá de Balaguer y Albás, bautizado con el nombre José María Julián Mariano (Barbastro, Huesca, Aragón, 9 de enero de 1902-Roma, 26 de junio de 1975) fue un sacerdote español, fundador en 1928 del Opus Dei y santo de la Iglesia católica, cuya fiesta se celebra el 26 de junio.

Escrivá obtuvo un doctorado en derecho civil por la Universidad Central de Madrid y otro en teología por la Pontificia Universidad Lateranense. Su obra principal fue la fundación, administración y expansión del Opus Dei, una institución perteneciente a la Iglesia católica. Su publicación más conocida es "Camino", obra traducida a decenas de idiomas y con varios millones de ejemplares vendidos.

José María Escrivá Albás(futuro Josemaría Escrivá de Balaguer y Albás) nació en Barbastro (Huesca, España) el 9 de enero de 1902. Sus padres se llamaban José Escrivá y Corzán y María de los Dolores Albás y Blanc. Fue el segundo de seis hermanos; sus tres hermanas pequeñas murieron siendo niñas. El último, Santiago, nació en 1919 y falleció el 25 de diciembre de 1994 a los 75 años de edad. Cuando Josemaría cumplió dos años, padeció una enfermedad grave en la que se temió por su vida. Tras su recuperación, sus padres lo llevaron en peregrinación a la ermita de Torreciudad en cumplimiento de una promesa a la Virgen María por su curación. En los años 1960, Escrivá impulsó la construcción de un santuario en Torreciudad, que se terminó a mediados de la década de 1970.

En 1914 quebró el negocio del padre, que era un comercio de tejidos, por lo que la familia quedó en la ruina. Tuvieron que trasladarse a Logroño, donde su padre encontró un trabajo como dependiente. Escrivá continuó estudiando hasta acabar el bachillerato. En las Navidades de 1917-18, al ver las huellas de pasos de un carmelita descalzo en la nieve, quedó impresionado, y decidió hacerse sacerdote, ingresando en el seminario de Logroño como alumno externo en el mes de octubre de 1918.

En septiembre de 1920, se trasladó a Zaragoza. Algunos de sus compañeros del seminario de Zaragoza lo recuerdan como un joven despierto, inteligente y alegre, a la vez que muy piadoso.

En las navidades de 1922 recibió los grados de ostiario y lector, junto con los de exorcista y acólito. Sus superiores apreciaron sus dotes por lo que lo nombraron Inspector del Seminario -encargado de mantener la disciplina entre los seminaristas, tanto en clase como en los paseos- siendo un hecho insólito que designaran a un seminarista y no a un sacerdote para este cargo. En 1923, siguiendo el consejo de su padre, comienza los estudios de Derecho en la Universidad de Zaragoza.

Su padre, José Escrivá, murió en 1924, y Josemaría quedó como cabeza de familia. Recibió la ordenación sacerdotal el 28 de marzo de 1925 y comenzó a ejercer el ministerio en varias parroquias rurales y luego en Zaragoza, con preferencia en la iglesia de san Pedro Nolasco, regida entonces por sacerdotes jesuitas.

En 1927 se trasladó a Madrid, con permiso de su obispo, para iniciar la tesis del doctorado en Derecho. Allí trabajó en una academia dando clases de Derecho romano y canónico para sostener a su familia. Ejerció su ministerio sacerdotal en el Patronato de Enfermos, institución benéfica dirigida por las Damas Apostólicas del Sagrado Corazón de Jesús.

Trataba sacerdotalmente a muchas personas de diversos ambientes sociales. Dedicó las mejores horas de su juventud, como capellán del Patronato de Enfermos, a la atención de numerosos enfermos y niños desvalidos de los barrios pobres de Madrid. Al mismo tiempo trataba con muchas otras personas: alumnos y profesores universitarios, obreros, dependientes de comercio, artistas, etc.

El 2 de octubre de 1928, según su propio testimonio, «vio» que Dios le pedía que difundiese en todo el mundo la llamada universal a la santidad, y que abriera un nuevo camino dentro de la Iglesia —el Opus Dei, en español «Obra de Dios»— para transmitir a todos los hombres que se pueden santificar a través del trabajo. Desde ese día, mientras continuaba con el ministerio pastoral que tenía encomendado en aquellos años, trabajaba en solitario en el desarrollo de la organización. Empezó a contactar con personas de diversas profesiones como artistas, profesores, obreros, sacerdotes, pequeños empresarios, a la vez ofrecía sus oraciones y mortificaciones.

Al principio Escrivá vino usando el término que él empleaba en el sentido que el Opus Dei estaba previsto solamente para hombres pero algunos años después, en 1930, según él mismo cuenta, Dios le había hecho ver que también estaba destinado a mujeres. En 1930, pidió la admisión en el Opus Dei un antiguo compañero de instituto de Escrivá, de origen argentino, Isidoro Zorzano, y en 1932 se unen un sacerdote asturiano, José María Somoano, una mujer cordobesa, María Ignacia García Escobar, y un joven empresario, Luis Gordon, aunque en un año fallecieron estos tres, y Josemaría tuvo que recomenzar.

Al año de la fundación del Opus Dei, el joven José María Escrivá y Albás consideró distintas posibilidades para sacar adelante a su familia, al margen de la vida consagrada, e incluso llegó a inscribirse en unas oposiciones convocadas en 1929 para cubrir plazas de auxiliar del ministerio de Asuntos Exteriores.

La caída de la monarquía trajo la llegada de la Segunda República en abril de 1931, iniciándose un período de gran tensión entre el nuevo régimen y la Iglesia católica, al aprobarse una nueva constitución laica. Al mismo tiempo, fueron atacados numerosos conventos e iglesias con la pasividad de las autoridades. En este contexto, Josemaría Escrivá prosiguió su tarea como capellán del Patronato de Enfermos, en el Patronato de Santa Isabel y el Opus Dei, manteniéndose al margen de las disputas políticas. 

No obstante, durante la época republicana hubo miembros de Opus Dei implicados en la política católica, como el caso del entonces estudiante Álvaro del Portillo, integrado en la Juventud Tradicionalista y en la Agrupación Escolar Tradicionalista. El propio Escrivá leía en esos años el diario tradicionalista "El Siglo Futuro", debido a su amistad con el sacerdote Antonio Sanz Cerrada, redactor de dicho diario. 

En 1933 contaba con un grupo de estudiantes universitarios y fundó la Academia DYA, en la que, además de impartirse clases de derecho y arquitectura, se organizaban charlas de formación cristiana. En 1934 publicó un pequeño libro llamado "Consideraciones Espirituales", que, ampliado durante los años siguientes, incluso durante la Guerra Civil, fue reeditado en 1939 con el título de "Camino".

En 1934 Josemaría fue nombrado rector del Patronato de Santa Isabel, lo que representa un pequeño alivio a sus dificultades económicas para mantener a su familia.

Como medio para alcanzar los fines de la institución, Escrivá concibe el llamado "plan de vida" que deben seguir los miembros, que por aquellos años se va perfilando e incluye prácticas como la misa diaria, comunión, el rezo del ángelus, la visita al sagrario, la lectura espiritual, el rezo del rosario y las mortificaciones, entre otras.
Hacia 1935/36, en la academia DyA (Derecho y Arquitectura) recién fundada en Madrid, los estudiantes comenzaron a practicar algunas de las ideas que el fundador concibió, y comenzaron a aparecer los signos distintivos de la futura Obra, y que serían consideradas en adelante muestra de "buen espíritu", como la corrección fraterna, ayunos y la mortificación corporal (ver citas de su libro Camino), por ejemplo dormir en el suelo, castigarse el cuerpo por medio de un cilicio apretado en el muslo durante dos horas al día y golpearse con unas "disciplinas" (latiguillo de cuerda) una vez a la semana. Según Escrivá, la finalidad de estas prácticas era unirse a la cruz de Cristo, domar las pasiones y obtener dones de Dios, castigando el cuerpo y refrenando la voluntad. Para servir de ejemplo, Escrivá se entregaba a todas estas mortificaciones, hasta el punto de dejar salpicaduras de sangre en las paredes cuando se azotaba, si bien no recomendó llegar hasta estos extremos a sus seguidores y aconsejaba también otro tipo de mortificaciones, relacionadas con la vida cotidiana.

Por aquella época sus seguidores empezaron a llamarle "el Padre". Jesús Ynfante critica que ello fue por deseo del propio Escrivá. No obstante, Escrivá solía rehusar cualquier otro trato, por ejemplo, el de Monseñor cuando le fue otorgado dicho título, así como el de Fundador.

Al estallar la Guerra Civil Española, en 1936, Josemaría se encuentra en Madrid, donde sigue ejerciendo su ministerio sacerdotal, con riesgo de su vida, clandestinamente. La persecución religiosa le obliga a refugiarse en diferentes lugares. Por ejemplo, fue hospitalizado de forma clandestina en una clínica psiquiátrica con la cobertura de estar aquejado fuertemente de reumatismo y durante 6 meses vive en el consulado hondureño. Finalmente, logró salir de Madrid en 1937 después de varias tentativas infructuosas usando documentación falsa. Después de una larga huida con algunos de sus seguidores por los Pirineos, pasó por el sur de Francia y se trasladó a la zona sublevada de España donde podía ejercer libremente su labor sacerdotal.

La Guerra Civil y las pruebas que había soportado en ella le habían marcado profundamente. El hecho de que el clero fuera objeto de persecución en la zona republicana dejó en él un recuerdo particularmente duradero.

Josemaría Escrivá regresa a Madrid el 28 de marzo de 1939, en un camión militar, y reemprende la expansión del Opus Dei por otras ciudades de España. El inicio de la Segunda Guerra Mundial impide el comienzo en otras naciones.

Cuando acaba la guerra civil en 1939, se produce un radical cambio en las estructuras del país y el Estado español se proclama como totalitario, confesional, ligado públicamente al Nacional-sindicalismo falangista y al Tradicionalismo carlista.

Las relaciones de Escrivá y Franco fueron complejas y son motivo de polémica, entre otras cosas porque años más tarde, el fundador le escribiría a Franco una carta para agradecerle que, entre los principios del Movimiento Nacional se declare "el acatamiento a la Ley de Dios, según la doctrina de la Santa Iglesia". Se trata de una carta fechada en Roma el 23 de mayo de 1958, cuya fotocopia, en unión de otras inéditas del mismo autor, se conserva en el archivo de la Fundación Nacional Francisco Franco.

Aunque también es conocido que, en una ocasión, el obispo de Madrid le pidió que predicara unos ejercicios espirituales a Franco y su familia en el Palacio de El Pardo y que durante aquellos ejercicios se produjeron ciertos malentendidos entre ambas personalidades.

En 1939, obtiene el título de doctor en Derecho. Recuperó también el puesto de rector del Real Patronato de Santa Isabel que obtuvo en 1934 por parte del Presidente de la República y le concedieron ese año el cargo de miembro del Consejo Nacional de Educación y el puesto de profesor de Ética y Deontología en la Escuela Oficial de Periodismo.

En los años posteriores a la guerra muchos obispos de toda España le llaman para dirigir ejercicios espirituales a sacerdotes de su diócesis. También predica a religiosos —-entre ellos a los agustinos de la comunidad del Monasterio de El Escorial—- por petición de los respectivos superiores, y a muchos laicos.

Desde el final de la guerra desarrolla la "Sección femenina" dentro de la Obra, prácticamente desde cero, con una estructura similar a la de los hombres, estrictamente separada de la sección masculina. Ese mismo año, el obispo de Madrid, Leopoldo Eijo y Garay, concede la primera aprobación diocesana del Opus Dei.

En 1943 Josemaría Escrivá encuentra una solución jurídica, la Sociedad Sacerdotal de la Santa Cruz, como medio para llevar el espíritu del Opus Dei a los sacerdotes seculares. Al año siguiente, el obispo de Madrid ordena a los tres primeros miembros del Opus Dei que acceden al sacerdocio: Álvaro del Portillo, José María Hernández Garnica y José Luis Múzquiz.

Después de finalizada la II Guerra Mundial, en 1946, Escrivá se traslada a Roma. Es decir: descubrió que las cuestiones de futuro para él y para el Opus Dei no estaban en Madrid sino en Roma. Según otros biógrafos, ese viaje se ha de ver en otra perspectiva: Ya en 1936, tenía proyectado comenzar la labor del Opus Dei en París, pero la Guerra Civil española, primero, y la II Guerra Mundial después habían impedido la expansión del Opus Dei en el extranjero. Su primer viaje a Roma tenía como finalidad inmediata conseguir de la Santa Sede una aprobación de derecho pontificio que asegurase la secularidad de los miembros del Opus Dei. Pero sus intenciones iban más allá: veía la ciudad de Roma como el enclave necesario para dirigir la expansión del Opus Dei por todo el mundo. En Roma recibió en 1947 el título de prelado doméstico de Su Santidad, lo cual le daba derecho al tratamiento de monseñor, y a utilizar sotana ribeteada de rojo y, sobre todo, dejaba claro que el Opus Dei no está relacionado con las órdenes religiosas, pues los miembros de éstas no pueden recibir esos títulos honoríficos. 

Por aquellos años se le diagnosticó una fuerte diabetes. Sus crisis de salud fueron muy frecuentes a partir de 1944. Como diabético insulinodependiente, Escrivá sufría constantemente cansancios, trastornos de la vista y se mantenía en pie gracias a las inyecciones y a una dieta estricta.

El ciclo fundacional parecía terminado. La primera fecha fundacional, la sección de varones, tuvo lugar en 1928; la segunda, la sección de mujeres, en 1930; la tercera, los sacerdotes, en 1943. La incorporación de supernumerarios, formada en su mayoría por hombres y mujeres casados, además de la admisión de cooperadores (que pueden ser no católicos, no cristianos y no creyentes), tuvo lugar entre 1947 y 1948. A partir de entonces, la organización iba a presentar su fisonomía definitiva.

Escrivá inició operaciones jurídicas para el reconocimiento del Opus Dei por parte de la Santa Sede. En 1947 y 1950, obtuvo la aprobación del Opus Dei como Instituto Secular de derecho pontificio, siendo aprobados sus estatutos en 1950, en los cuales los laicos hacían, si bien de forma privada los tres votos clásicos de obediencia, castidad y pobreza.

El nuevo estatus jurídico de la Obra como institución de derecho pontificio facilitó una nueva expansión internacional. En 1949 marcharon los primeros a Estados Unidos y México. Durante la década de 1950, el Opus Dei se estableció en Canadá y otros once países americanos, Alemania, Suiza, Austria, Holanda, Japón y Kenia.

En 1948 se erigió el Colegio Romano de la Santa Cruz, centro internacional de formación para los varones del Opus Dei. Y en 1952, el Colegio Romano de Santa María, para las mujeres. Estas dos instituciones permitieron que un buen número de miembros de la Obra recibieran formación espiritual y pastoral directamente de Escrivá, a la vez que obtenían la licenciatura o el doctorado en Filosofía, Teología, Derecho Canónico o Sagrada Escritura en alguna de las universidades pontificias de Roma. Muchos de los hombres y mujeres que empezarían la labor de la Obra por todo el mundo pasarían antes varios años en Roma.

Durante los últimos años de la década de 1950 y los primeros de 1960 Escrivá realizó diversos viajes a capitales europeas, para preparar el comienzo del Opus Dei en esos países.

En 1947 tuvo lugar la adquisición en Roma de una amplia casa, con jardín en el número 73 de la calle Bruno Buozzi para la construcción de la casa central de la Obra y sede del Colegio Romano del Opus Dei, que duraría trece años, hasta 1960. A partir de la casa originaria se levantaron ocho edificios. Todo ello dio a la construcción un aire imponente, al ser una estructura compleja e interconectada formada por los ocho edificios, con doce comedores y catorce oratorios, algunos de los cuales eran subterráneos, dando cabida el mayor de los oratorios a más de doscientas personas.

En la Casa de Roma, el sagrario del oratorio de la Trinidad fue el preferido de Escrivá y en donde rezaba con mayor devoción. Allí sus hijos colocaron -siguieron una antigua tradición- una sagrario con forma de columba, una "paloma eucarística". Se halla colgada del techo encima del altar y es una paloma fabricada de oro y piedras preciosas, en cuyo buche se abre un pequeño sagrario donde se guardan las hostias consagradas.

Escrivá también recibió el nombramiento de miembro honorario de la Pontificia Academia de Teología. Obtiene el doctorado en Teología por la Pontificia Universidad Lateranense. Es nombrado consultor de dos Congregaciones de la Curia Romana.

Sigue con atención los preparativos y las sesiones del Concilio Vaticano II (1962-1965), y busca un trato intenso con muchos de los padres conciliares. No obstante, Escrivá no participó en ninguna de las comisiones o sesiones conciliares, ya que -según algunos- no fue invitado por mucho que lo intentara. Sin embargo, el secretario general del Opus Dei, Álvaro del Portillo, desempeñó un papel relevante en los preparativos del Concilio.

Posiblemente uno de los episodios más controvertidos en la vida de Escrivá sucedió en 1968. Cuando solicita y le es concedido por el gobierno de Franco, en parte -según Jesús Ynfante - gracias a la colaboración de un miembro del Opus Dei en el Ministerio de Justicia el título de III marqués de Peralta, título que retuvo sin usar durante cuatro años, antes de renunciar a él en 1972 en favor de su hermano Santiago. Según la investigación de Ricardo de la Cierva, la concesión, aunque con buena intención, fue obtenida de forma irregular.

A pesar del deterioro de su salud, Josemaría Escrivá siguió estimulando y guiando durante los últimos años de su vida la difusión del Opus Dei por todo el mundo. Por ello, en los años setenta comenzó a recorrer el mundo en lo que él denominaba "correrías apostólicas".

En 1970 fue a México para rezar ante la Virgen de Guadalupe y visitar a las personas que participaban de la labor apostólica del Opus Dei allí.

Dos años después realizó un viaje por la península ibérica. En 1974, Escrivá se desplazó durante tres meses a Sudamérica. Comenzó su viaje en São Paulo (Brasil), donde estuvo del 22 de mayo al 7 de junio, manteniendo veinticinco encuentros con diversos grupos de personas. En Buenos Aires, permaneció del 7 al 28 de junio. Y tuvo encuentros con unas veinticinco mil personas. En el Teatro Coliseo mantuvo dos reuniones los días 23 y 26 de junio, a las que asistieron cinco mil personas en cada una de ellas. Del 28 de junio al 9 de julio acudió a Santiago de Chile, y aprovechó para visitar el santuario de la Virgen de Lo Vásquez. Después, prosegió su viaje por Perú, del 9 al 31 de julio. En Lima, enfermó y permaneció casi diez días convaleciente (15-24 de agosto). Su estancia en Quito, se desarrolló del 1 al 10 de agosto, pero debido al soroche, tuvo que limitar su actividad, y tan sólo pudo reunirse en cuatro ocasiones con grupos reducidos. El 15 de agosto llegó a Caracas enfermo, y su estado físico empeoró. Sus molestias le hicieron guardar reposo la mayor parte del tiempo, del 15 al 27 de agosto. Por ello decidieron acortar su largo viaje de catequesis y regresar a Roma el 31 de agosto.

En 1975 inició su último viaje de catequesis. El viaje comenzó el 4 de febrero en Caracas, donde permaneció hasta el 15 del mismo mes; y finalizó el 23 de febrero en Ciudad de Guatemala, desde donde regresó a Roma, despedido por el cardenal Mario Casariego.

De estos viajes se conserva abundante material audiovisual, sobre todo de sus reuniones con cientos de personas.

Fallece en Roma el 26 de junio de 1975, tras sufrir un infarto repentino. Tras su muerte, la Santa Sede recibió miles de cartas -entre ellas, las de un tercio del episcopado mundial y 41 superiores de órdenes religiosas- solicitando la apertura del proceso de beatificación y canonización. Finalmente, su causa se introdujo en 1981 y el 17 de mayo de 1992, Juan Pablo II beatifica a Josemaría Escrivá de Balaguer en la plaza de San Pedro, en Roma. «Con sobrenatural intuición», dijo el papa en su homilía, «el beato Josemaría predicó incansablemente la llamada universal a la santidad y al apostolado». El 6 de octubre de 2002, es canonizado por Juan Pablo II en Roma, apoyado por las cientos de miles de personas que asistieron a los actos. Durante la ceremonia de su canonización, Juan Pablo II animó a todos a buscar la santidad en medio del mundo, en el trabajo y la vida ordinaria, tal como lo enseñaba el nuevo santo y siguiendo su ejemplo.

Su rápido proceso a los altares no estuvo exento de polémica y oposición. Los detractores critican lo que ven como una canonización relámpago o "turbo santidad" de Escrivá, y afirman que el proceso entero estuvo plagado de irregularidades. Sin embargo, también obtuvo el apoyo de diversas figuras de la jerarquía eclesiástica.

Juan Pablo II, en la bula de canonización, le llamó «el santo de lo ordinario o de la vida ordinaria» y que San Josemaría «se contaba entre los grandes testigos del cristianismo».

Tras su canonización, en numerosos países ha recibido algún reconocimiento público: esculturas, imágenes, placas, murales, iglesias, calles, plazas, etc.

En la actualidad hay más de ochenta mil miembros del Opus Dei, como se indica en el Anuario Pontificio, que se actualiza periódicamente.

«La filiación divina es el fundamento del espíritu del Opus Dei», afirmó Josemaría en numerosas ocasiones. Desde el bautismo, un cristiano es un hijo de Dios. Escrivá se esforzó por vivir y difundir este mensaje como central para la vida de un cristiano.

Todas las biografías y los estudios sobre el pensamiento de Escrivá destacan, como uno de los
elementos fundamentales de su personalidad, de su vida y de sus obras, el valor que asigna a la
libertad como don de Dios.

Su enseñanza sobre la libertad no se limita a la acción social, política y de pensamiento
del cristiano. Es una realidad que influye sobre toda la vida cristiana en su unidad existencial y en su variedad de modos, en particular caracteriza toda la vida espiritual del cristiano, su relación con Dios, con los demás y con el mundo.

Escrivá de Balaguer enseñó a buscar la santidad en el trabajo, lo que significa esforzarse por realizarlo bien, con competencia profesional, y con sentido cristiano, es decir, por amor a Dios y para servir a los hombres. Así, decía, el trabajo ordinario se convierte en lugar de encuentro con Cristo.

El fundador del Opus Dei explicaba que el cristiano no debe «llevar como una doble vida: la vida interior, la vida de relación con Dios, de una parte; y de otra, distinta y separada, la vida familiar, profesional y social». Por el contrario, señalaba san Josemaría, «hay una única vida, hecha de carne y espíritu, y esa es la que tiene que ser —en el alma y en el cuerpo— santa y llena de Dios».

San Josemaría recordó la necesidad de cultivar la oración y la penitencia propias del espíritu cristiano. Recomendaba la asistencia, si puede ser diaria, a la Santa Misa, dedicar un tiempo a la lectura del Evangelio, acudir con frecuencia al sacramento de la confesión, fomentó la devoción a la Virgen. Para imitar a Jesucristo, recomendaba también ofrecer algunas pequeñas mortificaciones, especialmente aquellas que facilitan el cumplimiento del deber y hacen la vida más agradable a los demás, así como el ayuno y la limosna.

"Es en medio de las cosas más materiales de la tierra donde debemos santificarnos, sirviendo a Dios y a todos los hombres", decía san Josemaría. La familia, el matrimonio, el trabajo, la ocupación de cada momento son oportunidades habituales de tratar y de imitar a Jesucristo, procurando practicar la caridad, la paciencia, la humildad, la laboriosidad, la justicia, la alegría y en general las virtudes humanas y cristianas.


Es autor de libros de espiritualidad difundidos en los cinco continentes. El más conocido y popular es "Camino", que cuenta con cerca de cuatro millones y medio de ejemplares en 43 idiomas.

Algunos rasgos característicos de Escrivá fueron su profunda adhesión al papa y a la Iglesia; repetidas veces afirmaba que «el Opus Dei (que es “una parte de la Iglesia”) está para servir a la Iglesia como ella quiere ser servida».

Numerosas personalidades de la Iglesia consideran a Josemaría Escrivá como precursor del Concilio Vaticano II por su predicación sobre la santidad en medio del mundo, afirmando que las personas de cualquier condición y desde cualquier oficio honesto puede llegar a ser santos, sin necesidad de ser sacerdotes o religiosos.

Durante su vida, tanto su persona como el Opus Dei despertaron controversias, principalmente debido a las acusaciones de secretismo, elitismo y sectarismo de la organización y su fundador, además del apoyo de la organización a causas de ideología de derechas, como la conocida participación de los llamados «tecnócratas» del Opus Dei en los planes económicos de la dictadura de Francisco Franco. Descrita como «la fuerza más polémica de la Iglesia católica», en palabras del periodista John Allen, el Opus Dei fue visto por algunos teólogos como signo de contradicción y por otros como fuente de controversia.

Algunos autores, que nunca convivieron con Escrivá, como Ynfante y Carandell han presentado una imagen muy negativa de Escrivá acentuando su mal humor, e incluso su cólera. En el caso de Carandell, presenta un ensayo difamatorio, sin señalar ninguna fuente concreta que respalde sus afirmaciones. Por ello, no es de extrañar que Ricardo de la Cierva, concediera muy poco valor a la biografía de Carandell, a la que tachó de "jocosa y estúpida".Con todo, dichas acusaciones han sido negadas rotundamente por personas que convivieron largas años con Escrivá, como es el caso de Rafael Gómez Pérez, exmiembro de la Prelatura y muchos otras personas como Ángel Galíndez, Manuel Aznar, o José Antonio Giménez-Arnau, ninguno de los cuales perteneció al Opus Dei.

Después de su muerte, su canonización generó una considerable atención, tanto dentro de la Iglesia como en la prensa de todo el mundo. Varios periodistas que investigaron la historia del Opus Dei, entre ellos el vaticanista John Allen, argumentaron que muchas de las acusaciones a Escrivá no están probadas o proceden de los enemigos de Josemaría y su organización. El cardenal Albino Luciani —futuro papa Juan Pablo I—, Juan Pablo II, Benedicto XVI, apoyaron las enseñanzas de Escrivá «sobre la llamada universal a la santidad, el papel de los laicos y la santificación del trabajo».

Algunos libros fueron publicados en vida; otros son póstumos. El libro más conocido es "Camino", una colección de 999 aforismos, que ha tenido una importante recepción. Póstumamente, se publicaron otras dos colecciones de aforismos: "Surco" y "Forja".

"La Abadesa de las Huelgas" es un estudio teológico-jurídico, a partir de las fuentes y documentos originales, sobre el caso extraordinario de jurisdicción cuasiepiscopal por parte de la abadesa del famoso monasterio burgalés. La primera edición se publicó en 1944.

"Amar a la Iglesia" reúne tres homilías del fundador del Opus Dei: "Lealtad a la Iglesia", "El fin sobrenatural de la Iglesia" y "Sacerdote para la eternidad". El volumen incluye además, dos artículos de Mons. Álvaro del Portillo en torno a la figura del fundador del Opus Dei.

"Discursos sobre la Universidad" es un volumen elaborado por la Universidad de Navarra con motivo de la beatificación de su Fundador y Primer Gran Canciller, donde se recogen los diversos discursos académicos pronunciados por él ante la corporación universitaria, la homilía pronunciada en el campus de la Universidad de Navarra en octubre de 1967 y algunas otras declaraciones públicas suyas sobre temas universitarios.

Además, se publicaron dos colecciones de homilías, "Es Cristo que pasa", dedicado a los grandes momentos del año litúrgico, y "Amigos de Dios", en que glosa una serie de virtudes. "Santo Rosario" y "Via Crucis" (obra póstuma) están dedicados a estas dos formas tradicionales de la piedad católica. Finalmente, "Conversaciones con monseñor Escrivá de Balaguer" reúne en un volumen entrevistas concedidas a diversos medios de comunicación y una homilía pronunciada en el "campus" de la Universidad de Navarra en 1967.

Tanto de "Camino" como de "Santo Rosario" se ha publicado una edición crítico-histórica.

En 2011 se estrenó "Encontrarás dragones" (en inglés, There be Dragons), película protagonizada por Charlie Cox, Wes Bentley, Dougray Scott y Olga Kurylenko, en la que Charlie Cox da vida a Josemaría. A raíz del horror de la Guerra Civil Española, un candidato para la canonización es investigado por un periodista que descubre que su propio padre tenía una conexión profunda, oscura y devastadora.









</doc>
<doc id="19247" url="https://es.wikipedia.org/wiki?curid=19247" title="Núcleo Linux">
Núcleo Linux

"Para el sistema operativo libre compuesto principalmente por el núcleo Linux y GNU, véase GNU/Linux."

Linux es un núcleo mayormente libre semejante al núcleo de Unix. Linux es uno de los principales ejemplos de software libre y de código abierto. Linux está licenciado bajo la GPL v2 salvo el hecho que tiene blobs binarios no-libres y la mayor parte del software incluido en el paquete que se distribuye en su sitio web es software libre. Está desarrollado por colaboradores de todo el mundo. El desarrollo del día a día tiene lugar en la "Linux Kernel Mailing List Archive".

El núcleo Linux fue concebido por el entonces estudiante de ciencias de la computación finlandés Linus Torvalds en 1991. Linux consiguió rápidamente desarrolladores y usuarios que adoptaron códigos de otros proyectos de software libre para usarlos con el nuevo núcleo de sistema. A día de hoy miles de programadores de todo el mundo contribuyen en su desarrollo.

Linux es multiprogramado, dispone de memoria virtual, gestión de memoria, conectividad en red y permite bibliotecas compartidas. Linux es multiplataforma y es portable a cualquier arquitectura siempre y cuando esta disponga de una versión de GCC compatible.

En el archivo Léeme de Linux se indica que es un clon del sistema operativo Unix. Sin embargo Linux es un núcleo semejante al núcleo de un sistema operativo Unix. De hecho inicialmente se publicó como núcleo semejante a Minix, que a su vez era semejante a Unix pero con una concepción de micronúcleo en vez de monolítica.

Un micronúcleo contiene una funcionalidad mínima en comparación con un núcleo monolítico tradicional. Darwin y GNU Hurd tienen núcleos que son una versión de Mach. Minix, sistema operativo en el que se basó inicialmente Linux, también es micronúcleo. Todos ellos tienen en común el traslado de parte de la funcionalidad en espacio privilegiado a espacio de usuario.

La parte de un sistema operativo que se ejecuta sin privilegios o en espacio de usuario es la biblioteca del lenguaje C, que provee el entorno de tiempo de ejecución, y una serie de programas o herramientas que permiten la administración y uso del núcleo y proveer servicios al resto de programas en espacio de usuario, formando junto con el núcleo el sistema operativo.

En un sistema con núcleo monolítico como Linux la biblioteca de lenguaje C consiste en una abstracción de acceso al núcleo. Algunas bibliotecas como la biblioteca de GNU proveen funcionalidad adicional para facilitar la vida del programador y usuario o mejorar el rendimiento de los programas.

En un sistema con micronúcleo la biblioteca de lenguaje C puede gestionar sistemas de archivos o controladores además del acceso al núcleo del sistema.

A los sistemas operativos que llevan Linux se les llama de forma genérica distribuciones Linux. Estas consisten en una recopilación de software que incluyen el núcleo Linux y el resto de programas necesarios para completar un sistema operativo. Las distribuciones más comunes son de hecho distribuciones GNU/Linux o distribuciones Android. El hecho de que compartan núcleo no significa que sean compatibles entre sí. Una aplicación hecha para GNU/Linux no es compatible con Android sin la labor adicional necesaria para que sea multiplataforma.

Las distribuciones GNU/Linux usan Linux como núcleo junto con el entorno de tiempo de ejecución del Proyecto GNU y una serie de programas y herramientas del mismo que garantizan un sistema funcional mínimo. La mayoría de distribuciones GNU/Linux incluye software adicional como entornos gráficos o navegadores web así como los programas necesarios para permitirse instalar a sí mismas. Los programas de instalación son aportados por el desarrollador de la distribución. Se les conoce como gestores de paquetes. Los creadores de una distribución también se pueden encargar de añadir configuraciones iniciales de los distintos programas incluidos en la distribución.

Las distribuciones Android incluyen el núcleo Linux junto con el entorno de ejecución y herramientas del proyecto AOSP de Google. Cada fabricante de teléfonos dispone de su propia distribución de Android a la cual modifica, elimina o añade programas extra: interfaces gráficas, tiendas de aplicaciones y clientes de correo electrónico son algunos ejemplos de programas susceptibles de ser añadidos, modificados o eliminados. Además de las distribuciones de los fabricantes de teléfonos existen grupos de programadores independientes que también desarrollan distribuciones de Android. LineageOS y Replicant son dos ejemplos de distribuciones Android independientes.

En abril de 1991, Linus Torvalds, de 21 años, empezó a trabajar en unas simples ideas para un núcleo de un sistema operativo. Comenzó intentando obtener un núcleo de sistema operativo gratuito similar a Unix que funcionara con microprocesadores Intel 80386. Para ello tomó como base al sistema Minix (un clon de Unix) e hizo un núcleo monolítico compatible que inicialmente requería software de Minix para funcionar. El 26 de agosto de 1991 Torvalds escribió en el grupo de noticias "comp.os.minix":

Tras dicho mensaje, muchas personas ayudaron con el código. En septiembre de 1991 se lanzó la versión 0.01 de Linux. Tenía 10.239 líneas de código. En octubre de ese año (1991), salió la versión 0.02 de Linux; luego, en diciembre se lanzó la versión 0.11(1991). Esta versión fue la primera en ser "self-hosted" (autoalbergada). Es decir, Linux 0.11 podía ser compilado por una computadora que ejecutase Linux 0.11, mientras que las versiones anteriores de Linux se compilaban usando otros sistemas operativos. Cuando lanzó la siguiente versión, Torvalds adoptó la GPL como su propio boceto de licencia, la cual no permite su redistribución con otra licencia que no sea GPL. Antes de este cambio, se impedía el cobro por la distribución del código fuente.
Se inició un grupo de noticias llamado "alt.os.linux" y el 19 de enero de 1992 se publicó en ese grupo el primer "post". El 31 de marzo, "alt.os.linux " se convirtió en "comp.os.linux". XFree86, una implementación del X Window System, fue portada a Linux, la versión del núcleo 0.95 fue la primera en ser capaz de ejecutarla. Este gran salto de versiones (de 0.1x a 0.9x) fue por la sensación de que una versión 1.0 acabada no parecía estar lejos. Sin embargo, estas previsiones resultaron ser un poco optimistas: desde 1993 hasta principios de 1994 se desarrollaron 15 versiones diferentes de 0.99 (llegando a la versión 0.99r15).

El 14 de marzo de 1994, salió Linux 1.0.0, que constaba de 176.250 líneas de código. En marzo de 1995 se lanzó Linux 1.2.0, que ya estaba compuesto de 310.950 líneas de código.


Su código fuente está disponible para descarga en el sitio web oficial: http://www.kernel.org

Linux provee controladores, planificadores, gestores de memoria virtual, sistemas de archivos y protocolos de red como IPv4 e IPv6. Además está disponible para múltiples arquitecturas hardware y está diseñado de manera que se facilite su portabilidad a arquitecturas nuevas.

Actualmente Linux es un núcleo monolítico híbrido. Los controladores de dispositivos y las extensiones del núcleo normalmente se ejecutan en un espacio privilegiado conocido como anillo 0 ("ring 0"), con acceso irrestricto al hardware, aunque algunos se ejecutan en espacio de usuario. A diferencia de los núcleos monolíticos tradicionales, los controladores de dispositivos y las extensiones al núcleo se pueden cargar y descargar fácilmente como módulos, mientras el sistema continúa funcionando sin interrupciones. A diferencia de los núcleos monolíticos tradicionales, los controladores también pueden ser pre-volcados (detenidos momentáneamente por actividades más importantes) bajo ciertas condiciones. Esta habilidad fue agregada para gestionar correctamente interrupciones de hardware y para mejorar el soporte de multiprocesamiento simétrico.

El hecho de que Linux no fuera desarrollado siguiendo el diseño de un micronúcleo (diseño que, en aquella época, era considerado el más apropiado para un núcleo por muchos teóricos informáticos), fue motivo de una famosa y acalorada discusión entre Linus Torvalds y Andrew S. Tanenbaum. 

El núcleo Linux puede correr sobre muchas arquitecturas de máquina virtual, tanto como host del sistema operativo o como cliente. La máquina virtual usualmente emula la familia de procesadores Intel x86, aunque en algunos casos también son emulados procesadores de PowerPC o ARM.

Para verificar el correcto funcionamiento del núcleo este provee la posibilidad de compilarse contra la arquitectura ficticia bajo «User Mode Linux» (UML). Compilando Linux para UML el núcleo pasa a ejecutarse como un proceso más de usuario ejecutándose en el núcleo Linux del sistema anfitrión. También puede servir como virtualización o para aislar procesos, entre otros usos.

Linux 1.0 admitía solo el formato binario a.out. La siguiente serie estable (Linux 1.2) agregó la utilización del formato ELF, el cual simplifica la creación de bibliotecas compartidas (usadas de forma extensa por los actuales ambientes de escritorio como GNOME y KDE). ELF es el formato usado de forma predeterminada por el GCC desde alrededor de la versión 2.6.0. El formato a.out actualmente no es usado, convirtiendo a ELF en el formato binario utilizado por Linux en la actualidad.

Linux tiene la capacidad de permitir al usuario añadir el manejo de otros formatos binarios. También binfmt_misc permite correr el programa asociado a un archivo de datos.

En Linux existe un sistema de archivos que carga y contiene todos los directorios, redes, programas, particiones, dispositivos, etc. que el sistema sabe reconocer, o por lo menos, identificar. Este sistema de ficheros y directorios, tiene como base al carácter (/); ese mismo carácter sirve también para demarcar los directorios, como por ejemplo: ""/home/usuario/imagen.jpg"". El directorio especificado por una ruta consistente solo por este carácter contiene toda la jerarquía de los directorios que constituyen todo el sistema. A este directorio suele llamárselo directorio raíz. En Linux, a los discos no se les asigna una letra como en Windows (p.e. "C:"), sino que se les asigna un directorio de la jerarquía del directorio raíz (/), como por ejemplo: ""/media/floppy"". Es práctica común en el sistema de ficheros de Linux, utilizar varias "sub-jerarquías" de directorios, según las diferentes funciones y estilos de utilización de los archivos. Estos directorios pueden clasificarse en:

En Linux, un "panic" es un error casi siempre insalvable del sistema detectado por el núcleo en oposición a los errores similares detectados en el código del espacio de usuario. Es posible para el código del núcleo indicar estas condiciones mediante una llamada a la función de pánico situada en el archivo header sys/systm.h. Sin embargo, la mayoría de las alertas son el resultado de excepciones en el código del núcleo que el procesador no puede manejar, como referencias a direcciones de memorias inválidas. Generalmente esto es indicador de la existencia de un bug en algún lugar de la cadena de alerta. También pueden indicar un fallo en el hardware como un fallo de la RAM o errores en las funciones aritméticas en el procesador, o por un error en el software.
En muchas ocasiones es posible reiniciar o apagar adecuadamente el núcleo mediante una combinación de teclas como ALT+SysRq+REISUB.

Linux está escrito en el lenguaje de programación C, en la variante utilizada por el compilador GCC (que ha introducido un número de extensiones y cambios al C estándar), junto a unas pequeñas secciones de código escritas con el lenguaje ensamblador. Por el uso de sus extensiones al lenguaje, GCC fue durante mucho tiempo el único compilador capaz de construir correctamente Linux. Sin embargo, Intel afirmó haber modificado su compilador C de forma que permitiera compilarlo correctamente.

Asimismo se usan muchos otros lenguajes en alguna forma, básicamente en la conexión con el proceso de construcción del núcleo (el método a través del cual las imágenes arrancables son creadas desde el código fuente). Estos incluyen a Perl, Python y varios lenguajes shell scripting. Algunos drivers también pueden ser escritos en C++, Fortran, u otros lenguajes, pero esto no es aconsejable. El sistema de construcción de Linux oficialmente solo soporta GCC como núcleo y compilador de controlador.

Aún cuando Linus Torvalds no ideó originalmente Linux como un núcleo portable, ha evolucionado en esa dirección. Linux es ahora de hecho, uno de los núcleos más ampliamente portados, y funciona en sistemas muy diversos que van desde iPAQ (una handheld) hasta un zSeries (un mainframe masivo). Linux se ha convertido en el sistema operativo principal de las supercomputadoras de IBM, Blue Gene, lo cual ha reducido los costos e incrementado considerablemente el rendimiento.

De todos modos, es importante notar que los esfuerzos de Torvalds también estaban dirigidos a un tipo diferente de portabilidad. Según su punto de vista, la portabilidad es la habilidad de compilar fácilmente en un sistema aplicaciones de los orígenes más diversos; así, la popularidad original de Linux se debió en parte al poco esfuerzo necesario para tener funcionando las aplicaciones favoritas de todos, ya sean software libre o de código abierto.

Las arquitecturas principales soportadas por Linux son DEC Alpha, ARM, AVR32, Blackfin, ETRAX CRIS, FR-V, H8, IA64, M32R, m68k, MicroBlaze, MIPS, MN10300, PA-RISC, PowerPC, System/390, SuperH, SPARC, x86, x86 64 y Xtensa

Linux dispone de una arquitectura ficticia llamada «UserMode Linux» (UML). Esta arquitectura permite ejecutar Linux como un proceso más en espacio de usuario. Los procesos ejecutados bajo UML tienen la visión de una máquina propia disponible para ellos. 

Dispone de una interfaz para crear módulos de seguridad llamada "Linux Security Module". Con esta interfaz se pueden crear módulos para aplicar control de acceso obligatorio (MAC, "Mandatory Access Control").

Más allá de haber desarrollado su propio código y de integrar los cambios realizados por otros programas, Linus Torvalds continúa lanzando nuevas versiones del núcleo Linux. Estos son llamados núcleos “vanilla”, lo que significa que no han sido modificados por nadie.

La versión del núcleo Linux original constaba de cuatro números. Por ejemplo, asumamos que el número de la versión está compuesta de esta forma: A.B.C[.D] (ej.: 2.2.1, 2.4.13 ó 2.6.12.3).





También, algunas veces luego de las versiones puede haber algunas letras como “rc1” o “mm2”. El “rc” se refiere a release candidate e indica un lanzamiento no oficial. Otras letras usualmente (pero no siempre) hacen referencia a las iniciales de la persona. Esto indica una bifurcación en el desarrollo del núcleo realizado por esa persona, por ejemplo ck se refiere a Con Kolivas, ac a Alan Cox, mientras que mm se refiere a Andrew Morton.

El modelo de desarrollo para Linux 2.6 fue un cambio significativo desde el modelo de desarrollo de Linux 2.5. Previamente existía una rama estable (2.4) donde se habían producido cambios menores y seguros, y una rama inestable (2.5) donde estaban permitidos cambios mayores. Esto significó que los usuarios siempre tenían una versión 2.4 a prueba de fallos y con lo último en seguridad y casi libre de errores, aunque tuvieran que esperar por las características de la rama 2.5. La rama 2.5 fue eventualmente declarada estable y renombrada como 2.6. Pero en vez de abrir una rama 2.7 inestable, los desarrolladores de núcleos eligieron continuar agregando los cambios en la rama “estable” 2.6. De esta forma no había que seguir manteniendo una rama vieja pero estable y se podía hacer que las nuevas características estuvieran rápidamente disponibles y se pudieran realizar más pruebas con el último código.

Sin embargo, el modelo de desarrollo del nuevo 2.6 también significó que no había una rama estable para aquellos que esperaban seguridad y bug fixes sin necesitar las últimas características. Los arreglos solo estaban en la última versión, así que si un usuario quería una versión con todos los bug fixed conocidos también tendría las últimas características, las cuales no habían sido bien probadas. Una solución parcial para esto fue la versión ya mencionada de cuatro números (y en 2.6.x.y), la cual significaba lanzamientos puntuales creados por el equipo estable (Greg Kroah-Hartman, Chris Wright, y quizás otros). El "equipo estable" solo lanzaba actualizaciones para el núcleo más reciente, sin embargo esto no solucionó el problema del faltante de una serie estable de núcleo. Distribuidores de Linux, como Red Hat y Debian, mantienen los núcleos que salen con sus lanzamientos, de forma que una solución para algunas personas es seguir el núcleo de una distribución.

Como respuesta a la falta de un núcleo estable y de gente que coordinara la colección de corrección de errores, en Diciembre de 2005 Adrian Bunk anunció que continuaría lanzando núcleos 2.6.16 aun cuando el "equipo estable" lanzara 2.6.17. Además pensó en incluir actualizaciones de controladores, haciendo que el mantenimiento de la serie 2.6.16 sea muy parecido a las viejas reglas de mantenimiento para las serie estables como 2.4. El núcleo 2.6.16 será reemplazado próximamente por el 2.6.27 como núcleo estable en mantenimiento durante varios años.

Dado el nuevo modelo de desarrollo, que mantuvo fija la subversión de 2.6, tras la realización del Linux Kernel Summit de ese año, Linus Torvalds decidió modificar el sistema de numeración. Se han sustituido los dos primeros números por una única cifra, de forma que Linux 2.6.39 fue seguida por Linux 3.0 

Una distribución Linux es un conjunto de software acompañado del núcleo Linux que se enfoca a satisfacer las necesidades de un grupo específico de usuarios. De este modo hay distribuciones para hogares, empresas y servidores.

Las distribuciones son ensambladas por individuos, empresas u otros organismos. Cada distribución puede incluir cualquier número de software adicional, incluyendo software que facilite la instalación del sistema. La base del software incluido con cada distribución incluye el núcleo Linux, en la mayoría de los casos las herramientas GNU, al que suelen añadirse también multitud de paquetes de software. 

Las herramientas que suelen incluirse en la distribución de este sistema operativo se obtienen de diversas fuentes, y en especial de proyectos de software libre, como: GNU, GNOME (creado por GNU) y KDE. También se incluyen utilidades de otros proyectos como Mozilla, Perl, Ruby, Python, PostgreSQL, MySQL, Xorg, casi todas con licencia GPL o compatibles con esta (LGPL, MPL).

Usualmente se utiliza la plataforma X.Org Server, basada en la antigua XFree86, para sostener la interfaz gráfica.

Linux es mayormente software libre tal y como se distribuye desde su web y repositorio Git. Sin embargo hay fragmentos de código privativo que son empleados para hacer funcionar los dispositivos de un computador. Por ello Linux no ha podido ser integrado como un paquete de GNU. La Fundación del Software de América Latina decidió crear y mantener una bifurcación completamente libre llamada Linux-libre. Esta versión de Linux no incluye ningún complemento de código cerrado ni funciones ofuscadas que integran binarios en su código.
La inclusión de estos binarios en el código de Linux no incumple la licencia GPL Versión 2 que usa Linux. Dicha licencia fue actualizada para evitar este tipo de uso privativo de la licencia, parecido a lo que ocurre con los dispositivos TiVo. La nueva licencia es la GPL Versión 3.

Inicialmente, Torvalds distribuyó Linux bajo los términos de una licencia que prohibía la explotación comercial. Pero esta licencia fue reemplazada, poco tiempo después, por la GNU GPL (versión 2 exclusivamente). Los términos de esta última licencia permiten la distribución y venta de copias o incluso modificaciones, pero requiere que todas las copias del trabajo original y trabajos de autoría derivados del original sean publicados bajo los mismos términos, y que el código fuente siempre pueda obtenerse por el mismo medio que el programa licenciado.

Torvalds se ha referido a haber licenciado Linux bajo la GPL como ""la mejor cosa que he hecho"" (en inglés, ""the best thing I ever did"").

Sin embargo, la versión oficial del núcleo Linux contiene firmware de código cerrado, por ello, el Proyecto Linux-libre, auspiciado por la FSFLA, publica y mantiene versiones modificadas del núcleo Linux a las que se les ha quitado todo el software no libre.

A día de hoy, "Linux" es una marca registrada de Linus Torvalds en los Estados Unidos.

Hasta 1994 nadie registró la marca Linux en Estados Unidos. El 15 de agosto de 1994 cuando William R. Della Croce, Jr. registró la marca "Linux", pidió el pago de regalías a los distribuidores de Linux. En 1996, Torvalds y algunas organizaciones afectadas denunciaron a Della Croce y en 1997 el caso se cerró y la marca fue asignada a Torvalds.

Desde entonces, el Linux Mark Institute gestiona la marca. En 2005 el LMI envió algunas cartas a empresas distribuidoras de Linux exigiendo el pago de una cuota por el uso comercial del nombre. Esto es así porque la legislación estadounidense exige que el dueño de una marca la defienda, por lo que se tuvo que pedir dinero por usar la marca Linux, algunas compañías de forma totalmente voluntaria han cumplido con dicha exigencia, a sabiendas de que dicho dinero se iba a usar para caridad o defender la marca Linux.

El núcleo Linux ha sido criticado con frecuencia por falta de controladores para cierto hardware de computadoras de escritorio. Sin embargo, el progresivo incremento en la adopción de Linux en el escritorio ha mejorado el soporte de hardware por parte de terceros o de los propios fabricantes, provocando que, en los últimos años, los problemas de compatibilidad se reduzcan.

Empresas como IBM, Intel Corporation, Hewlett-Packard, Dell o MIPS Technologies tienen programadores en el equipo de desarrolladores del núcleo Linux que se encargan de mantener los controladores para el hardware que fabrican. Este grupo de programadores también se le suman los que provee grandes distribuidores de soluciones Linux como Novell o Red Hat.

Andy Tanenbaum escribió el 29 de enero de 1992:



</doc>
<doc id="19254" url="https://es.wikipedia.org/wiki?curid=19254" title="Gualeguay">
Gualeguay

Gualeguay es un municipio del departamento Gualeguay (del cual es cabecera) en la Provincia de Entre Ríos, República Argentina. El municipio comprende la localidad del mismo nombre, la localidad de Puerto Ruiz y un área rural. Es por su población la quinta ciudad más grande de la provincia, después de Paraná, Concordia, Gualeguaychú y Concepción del Uruguay.

El municipio de Gualeguay tenía 43.006 habitantes en el censo 2010 y un ejido de 117 km² que incluye a Puerto Ruiz.

Con la batalla del cerro de la Matanza (1749), encabezada por el teniente gobernador de Santa Fe, Francisco Antonio Vera y Mujica, se concretó la extinción de los aborígenes chanás y guaraníes.

A mediados del siglo XVIII, las tierras entrerrianas comenzaron a ser ocupadas por familias procedentes de Santa Fe y la Bajada, principalmente por españoles, criollos y algunos portugueses. La mayoría de los inmigrantes se situaron a orillas del arroyo Clé, integrando el primer agrupamiento social que puede considerarse remoto antecedente de Gualeguay. Pero las inundaciones frecuentes llevaron a varios pobladores a buscar ubicación en lugares más altos, situándose al norte de la actual ciudad.

Fue fundada el 19 de marzo de 1783 por Tomás de Rocamora quien la bautizó «Villa de San Antonio de Gualeguay Grande», en honor a quien sería su santo patrono, Antonio de Padua. Al momento de su fundación comprendía 56 manzanas donde se albergaban 150 vecinos.

En febrero de 1782, siendo Tomás de Rocamora Ayudante Mayor y prestando servicios en el Regimiento de Dragones de Almansa, el virrey Vértiz lo designó para que interviniera en los conflictos que se habían generado en la parroquia de Gualeguay, en la que pobladores del lugar y foráneos ejecutaban toda clase de delitos.

Instruir un sumario sobre los disturbios de Gualeguay y pacificar al vecindario constituyeron la primera misión de Rocamora en lo que él llamaría la provincia de Entre Ríos. Realizó el reconocimiento del territorio desde Paraná a Gualeguaychú, constatando la inexistencia de villas o poblados y, por primera vez, los pacíficos pobladores que sembraban la tierra y criaban animales encontraron un funcionario idóneo ante quien presentarse para plantear sus problemas y quejas.

El 11 de agosto de 1782, Rocamora dirigió un pliego al virrey Vértiz que adquiere trascendencia histórica: junto al rudimentario mapa hizo un amplio y descriptivo estudio de la geografía y la economía de la región, sus características más relevantes, ponderando la bondad de los campos, sus pastos, sus aguadas, el clima, sus tierras y bosques.

El primer cabildo estuvo integrado por el alcalde Vicente Navarro y los regidores Domingo Ruiz, Valentín Barrios y Pedro José Duarte, siendo capitán de milicias Gregorio Santa Cruz. 

La municipalidad de Gualeguay fue creada por ley promulgada el 28 de mayo de 1872, e instalada entre el 1 de enero y abril de 1873.

El ejido municipal fue fijado por ley n.º 8475 sancionada el 3 de abril de 1991 (promulgada el 25 de abril de 1991). Mediante la ley n.º 9735, sancionada el 30 de agosto de 2006 y promulgada el 7 de septiembre de ese año, fue incorporada el ejido municipal de Gualeguay y al departamento Gualeguay, la isla Gericke. Esta isla ubicada en el río Gualeguay pertenecía hasta entonces al departamento Islas del Ibicuy.

En el mes de diciembre se realiza el Encuentro Internacional de Coros "Gualeguay Coral" y el Encuentro Nacional de Batucadas y Pasistas.
El fin de semana coincidente con el feriado del 12 de octubre se realiza desde hace 75 años el Festival de Jineteada y Folklore de Sociedad Sportiva. Además, entre el 12 y el 18 de octubre de 2009, Gualeguay fue sede del VII Campeonato Mundial de Pelota Vasca sub 22 en Trinquete celebrado en el Club Pelota Gualeguay. En dicho campeonato, Argentina obtuvo tres medallas doradas al igual que Francia.
El 16 y 17 de noviembre se realiza la Fiesta del Asado y la Galleta en la Costanera por los festejos del Día de la Tradición.

Al comienzo de cada año, durante los meses de enero y febrero el carnaval desborda la ciudad de música, color y ritmo.
En la actualidad, las agrupaciones que dan brillo al carnaval son: SI-SI, K’rumbay y Samba Vera.
El corsódromo fue inaugurado el 15 de enero de 2005 en el predio de la antigua estación de ferrocarril.
La capacidad es de entre 15000 y 20000 personas ubicadas en tribunas, sillas, plateas, mesas y palcos.




</doc>
<doc id="19260" url="https://es.wikipedia.org/wiki?curid=19260" title="Cliff Robertson">
Cliff Robertson

Clifford Parker Robertson III (La Jolla, California, 9 de septiembre de 1923 – Nueva York, 10 de septiembre de 2011) fue un actor estadounidense ganador del premio Óscar al mejor actor en 1968.

Nació en La Jolla, población cercana a Los Ángeles, y recibió el nombre Clifford Parker Robertson III. Sus padres tenían un rancho y eran acomodados. Durante los años escolares, Robertson se apuntó a clases de teatro porque era la forma de evitar tener que asistir a varias de las clases en la escuela. No obstante, esta actividad le sirvió para desarrollar un interés por la interpretación. 

Durante la Segunda Guerra Mundial fue llamado a filas y a su regreso del servicio militar ingresó en la Universidad Antioch del estado de Ohio. Su primer trabajo fue como locutor de radio, pero pronto se orientó hacia el teatro, lo cual hizo viajando durante dos años con una compañía ambulante. Desde allí consiguió un papel en Broadway, donde debutó en 1952 en una obra dirigida por Joshua Logan.

Al cabo de tres años, Logan le ofreció un papel secundario en "Picnic", película protagonizada por William Holden y Kim Novak que significó el comienzo de su carrera cinematográfica. El mismo año ya obtuvo un papel principal en la película "Autumn Leaves", estrenada en 1956, en la que actuó junto a Joan Crawford. Las críticas de su actuación fueron positivas, lo que le permitió tener peso en la elección de sus siguientes películas.

En 1963 protagonizó la película bélica "PT 109", basada en las heroicas acciones del entonces teniente John F. Kennedy en el Pacífico durante la Segunda Guerra Mundial que le valdrían la Medalla de la Armada y del Cuerpo de Marines. Fue el propio presidente Kennedy quien eligió personalmente a Robertson para interpretar su personaje. Durante los años que siguieron Robertson siguió realizando películas que en general fueron taquilleras. Se puso de manifiesto que este actor de cara seria no sería una estrella del cine, pero sí un actor fiable, capaz de interpretar personajes muy dispares.

En 1968 Robertson actuó en "Charly", adaptación de la novela "Flores para Algernon", en la que interpretó de forma magistral a una persona con retraso mental que es sometida a un experimento científico, como consecuencia del cual su inteligencia se desarrolla hasta niveles de genio. Por esta actuación obtuvo un Óscar al mejor actor principal y una nominación al premio Globo de Oro. Desde entonces intervino en numerosas películas, en papeles que a medida que se ha hecho mayor han sido frecuentemente de carácter secundario, sin que ello mermase el reconocimiento de la crítica y del público. También fue participando en producciones para la televisión, medio en el que llegó a sentirse cómodo.
Su último papel importante fue la interpretación de Ben Parker en la trilogía de películas Spider-Man.

Robertson se casó dos veces. Su primer matrimonio duró sólo dos años, mientras que la segunda vez estuvo casado durante 20 años. Ambos matrimonios terminaron en divorcio. De cada uno de ellos Robertson tuvo una hija.

El actor murió por causas naturales en Nueva York, a los 88 años, el 10 de septiembre del 2011 justo un día después de su cumpleaños.


</doc>
<doc id="19268" url="https://es.wikipedia.org/wiki?curid=19268" title="Grado centesimal">
Grado centesimal

El grado centesimal o gon —también llamado gradián (plural: gradianes) y gonio— es una unidad de medida de ángulos planos, alternativa al grado sexagesimal y al radián. El valor de un grado centesimal se define como el ángulo central subtendido por un arco, cuya longitud es la cuadringentésima (1/400) parte de una circunferencia. Su símbolo es una ge minúscula, en superíndice, colocada tras la cifra en cuestión; por ejemplo, 12,4574. El grado centesimal, así como el grado sexagesimal, no pertenece al Sistema Internacional de Unidades.

Debido a que la circunferencia se divide en 400 gon, por ejemplo un ángulo recto equivale a 100 gon, lo que permite determinar que un grado centesimal equivale a nueve décimas partes del grado sexagesimal.

La denominación de gon suele restringirse a los ámbitos especializados de la topografía y la ingeniería civil, donde es muy utilizada esta unidad de medida para definir el valor de los ángulos. La denominación de gradián se emplea en las calculadoras, en las que suele representarse con la abreviatura "grad".

Existía una denominación anterior de esta unidad como grado centígrado. Para evitar confusiones, en 1948 la unidad homónima de temperatura del mismo nombre pasó a denominarse oficialmente grado Celsius, aunque popularmente el grado celsius se siga denominando centígrado. Esto es parcialmente incorrecto, ya que la escala Kelvin también es centígrada (es una escala que toma de referencia 100 partes iguales, en este caso, punto de congelación y ebullición del agua destilada) y el término sería ambiguo.

Atendiendo a la definición de metro utilizada en 1889, un kilómetro debería corresponder a la longitud de un arco de meridiano cuya amplitud es un minuto centesimal; aunque mediciones posteriores más precisas del tamaño de la Tierra mostraron que existen diferencias.

El grado centesimal surge de la división del plano cartesiano en cuatrocientos ángulos iguales, con vértice común. Cada cuadrante posee una amplitud 100 grados centesimales, y la suma de los cuatro cuadrantes mide 400 grados centesimales.


Los siguientes valores angulares son equivalentes:

Los minutos y segundos de gon se corresponden con la fracción decimal de gon, cosa que no ocurre con los grados sexagesimales. No deben confundirse los grados centesimales con el uso de fracciones decimales para expresar ángulos en grados sexagesimales.

Sus divisores son:

Los tres son unidades de medida de ángulos planos, y se diferencian así:



</doc>
<doc id="19270" url="https://es.wikipedia.org/wiki?curid=19270" title="Elephantidae">
Elephantidae

Los elefantes o elefántidos (Elephantidae) son una familia de mamíferos placentarios del orden Proboscidea. Antiguamente se clasificaban, junto con otros mamíferos de piel gruesa, en el orden, ahora inválido, de los paquidermos (Pachydermata). Existen hoy en día tres especies y diversas subespecies. Entre los géneros extintos de esta familia destacan los mamuts.
Los elefantes son los animales terrestres más grandes que existen en la actualidad. El periodo de gestación es de veintidós meses, el más largo en cualquier animal terrestre. El peso al nacer usualmente es 118 kg. Normalmente viven de cincuenta a setenta años, pero registros antiguos documentan edades máximas de ochenta y dos años. El elefante más grande que se ha cazado, de los que se tiene registro, pesó alrededor de 11 000 kg (Angola, 1956), alcanzando una altura en la cruz de 3,96 m, un metro más alto que el elefante africano promedio. El elefante más pequeño, de alrededor del tamaño de una cría o un cerdo grande, es una especie prehistórica que existió en la isla de Creta, "Elephas creticus", durante el Pleistoceno.

Con un peso de 5 kg, el cerebro del elefante es el más grande de los animales terrestres. Se le atribuyen una gran variedad de comportamientos asociados a la inteligencia como el duelo, altruismo, adopción, juego, uso de herramientas, compasión y autorreconocimiento. Los elefantes pueden estar a la par con otras especies inteligentes como los cetáceos y algunos primates. Las áreas más grandes en su cerebro están encargadas de la audición, el gusto y la movilidad.

Los elefantes actuales se clasifican en dos géneros distintos, "Loxodonta" (elefantes africanos) y "Elephas" (elefantes asiáticos), pertenecientes a dos tribus distintas. Clásicamente se reconocían dos especies, una en cada género, pero actualmente hay un debate entre los científicos sobre si las dos subespecies africanas son en realidad dos especies distintas, en cuyo caso estaríamos hablando en total de tres especies de elefantes. Se reconocen las siguientes especies y subespecies:



El elefante de Borneo ("Elephas maximus borneensis") y el elefante de Malasia ("Elephas maximus hirsutus") son clasificados actualmente como "Elephas maximus indicus".
Presentan una prolongación nasal muy desarrollada, denominada probóscide (comúnmente conocida como trompa), que gracias a su desarrollada musculatura (tiene 150 000 músculos) les da una gran movilidad y sensibilidad. La trompa es la fusión de la nariz y el labio superior del elefante, y le sirve para muchas cosas además de respirar y oler:
Los elefantes también poseen colmillos, que en realidad son incisivos; salen de su mandíbula superior y crecen curvos a los lados de la trompa. Les sirven para abrir camino, marcar árboles (una forma de señalar su territorio), escarbar y para atacar y defenderse en caso necesario. Los colmillos de elefante son una gran fuente de marfil, pero debido a la creciente rareza de los elefantes, casi toda la cacería y tráfico son ahora ilegales. Sin embargo, al no existir los recursos necesarios para conseguir que se cumpla la ley, se sigue comerciando con los colmillos de los elefantes en el mercado negro. Esto implica que la matanza de elefantes de forma desaforada sigue teniendo lugar en la actualidad para alcanzar semejante finalidad. Los colmillos del elefante pueden pesar hasta 120 kg y tener hasta 3 m de longitud, aunque lo normal es que midan menos de un metro. Estos colmillos no son dientes caninos, sino incisivos extremadamente largos y el marfil es la dentina que los forma.

Otra de las características principales de los elefantes es que poseen unos grandes pabellones auditivos (mayores en el elefante africano que en el asiático). La principal función de estas orejas es la termo regulación del animal. Al estar muy vascularizadas permiten un correcto enfriamiento de la sangre, que en animales de ese volumen sería difícil conseguir por otros medios. También es capaz de percibir sonidos infrasónicos, lo cual le permite comunicarse con individuos situados a varios kilómetros de distancia. Estos sonidos, con frecuencias de tan solo cinco hercios (imposibles de escuchar para el hombre), se transmiten por aire y tierra, pudiendo ser detectados mediante las patas antes de llegar al oído del animal, al ser la velocidad de propagación del sonido mayor en el suelo que en el aire. Este desfase en la recepción del sonido podría servir al elefante para estimar la distancia a la que se encuentra su congénere.
Se alimentan casi exclusivamente de hierbas, cortezas de árboles y algunos arbustos, de los que pueden llegar a ingerir doscientos kilogramos en un día. Son los mamíferos terrestres más grandes en la actualidad, en orden a su talla y peso. Un macho adulto africano puede llegar a pesar 7500 kg, aunque el récord conocido es de 11 000 kg. Viven generalmente hasta los sesenta, setenta años (en ocasiones superan los setenta años) aproximadamente. No se conoce exactamente un récord de edad para un elefante en libertad; se estima que en muy raras ocasiones han podido superar los noventa años de edad. En cautiverio el récord lo tiene el famosísimo elefante asiático Lin Wang, que sirvió para las Fuerzas Chinas Expedicionarias en la Segunda Guerra Sino-Japonesa además de participar en otras misiones militares y «conocer» a los altos cargos del ejército chino, como Sun Li-jen. Falleció con ochenta y seis años de edad en 2003.

El elefante produce una variada gama de sonidos, con los cuales expresa diversas emociones. El más conocido es el "barrito", que hace cuando está asustado.

Varios estudiantes de cognición de elefantes y neuroanatomía están convencidos de que los elefantes son muy inteligentes y conscientes de sí mismos. Otros impugnan esta opinión.

El elefante africano es el mamífero con el tiempo de gestación más largo, aproximadamente veintidós meses, y pesa unos 115 kg al nacer.

En general suele relacionarse al elefante con la buena memoria, y estudios realizados por la Universidad de Sussex en Kenia, dirigidos por la doctora Karen McComb, parecen confirmarlo. Estudiando las comunicaciones entre elefantes del parque nacional Amboseli, en Kenia, los investigadores llegaron a la conclusión de que estos animales eran capaces de reconocer la llamada de más de cien individuos diferentes. Al parecer, estos sonidos, similares a un gruñido agudo, pueden servir para identificar a los demás individuos y formar parte de una red social relativamente compleja.

Otros estudios, dirigidos también por Karen McComb, confirmaron la capacidad de los elefantes de reconocer los restos de cadáveres de su misma especie, prestando especial atención a los correspondientes a miembros de su manada, que al parecer distinguen por su olor. Cuando se encuentran con estos restos parecen rendirles un particular homenaje póstumo, tocándolos con sus trompas y pezuñas. Sin embargo, ante huesos de otras especies su indiferencia es total.

Mucha gente piensa que los elefantes tienen miedo a los ratones. En realidad, lo que ocurre es que los elefantes tienen una mala visión: sus ojos están a los lados de la cabeza, lo que hace que no puedan distinguir con claridad cualquier cosa pequeña que se mueva delante de ellos. Esto hace que no soporten las sorpresas o los movimientos bruscos y cuando se acerca un ratón se ponen nerviosos y un poco agresivos.

Se cree que existen cementerios de elefantes, ya que se han encontrado restos de elefantes en una misma zona, muy cerca uno de otro, lo cual es un mito. Lo que sí ocurre es que antes de morir, los elefantes, por instinto, buscan el agua, por lo que muchos mueren cerca de ella y próximos unos de otros.

La industria del hombre y el furor por hacer daño a sus enemigos hizo que emplease este enorme cuadrúpedo en la guerra, armándole de diferentes modos, entre ellos unos castilletes o torres de madera, desde donde cierto número de guerreros disparaban armas arrojadizas. Heliodoro fija el número de soldados que montaba la torre en seis. De todos modos, puede juzgarse el daño que haría esta especie de fortificación movible, pues además de las flechas y dardos que despedían sus defensores, el elefante hacía también uso de la trompa, puesto que según algunos historiadores, este animal se aficiona mucho a los ejercicios bélicos.

La primera vez que le vemos aparecer en escena en la historia militar es en la batalla de Arbela o Arbella (Siria) año de 331 a. C. en que Darío, rey de Persia, los presentó en número de 15 en el centro de su línea de batalla, contra Alejandro el Grande, el cual a pesar de esto, venció a su enemigo y le despojó del reino. El rey vencedor, como gran capitán, no dejó de aprovechar este elemento de guerra y los elefantes formaron en lo sucesivo parte de las falanges macedónicas. Heliano dice que los griegos organizaron militarmente el conjunto de elefantes de un ejército:

El caballero Armandi, coronel francés, es de opinión que la falange en el acto de ser atacada se formaba en cuadro sólido, de modo que pudiera formar con facilidad de frente y cuando atacaba iba en una sola fila. Pirro los hizo pasar a Italia y los romanos aprendieron de él y de Aníbal a utilizarlo en un día de batalla. Se sirvió de ellos por primera vez contra Filipo, y continuaron empleándolos en todas sus guerras durante 300 años, hasta los tiempos de César. Tanto se llegó a estimar al elefante, que se le cubría el cuerpo con planchas de hierro y el pecho con un peto, en medio del cual se fijaba una punta de acero. También llevaban estas puntas en las extremidades de los colmillos. En cambio, se inventaron corazas erizadas de púas aceradas para defender el cuerpo de los guerreros destinados a atacar a los elefantes para que estos se hiriesen al asirlos con la trompa.

El mejor modo de atacar al elefante era matar al "cornell" o conductor pues desorientado y sin guía marchaba a la ventura. No todos los elefantes tenían instinto guerrero y muchas veces, particularmente cuando eran nuevos, les espantaba el tumulto y confusión de los combates: los gritos y las heridas los irritaba y entonces, no encontrando lugar para la huida, porque se trataba de impedirla colocando un cuerpo de honderos a su espalda, embestían a las propias tropas, causando en ellas el destrozo que debía hacer en las enemigas. El conductor en este caso, no tenía otro remedio que clavarles en la cabeza un puñal muy afilado que llevaba al efecto y caían muertos en el instante. Este inconveniente, repetido con frecuencia, unido a las dificultades de su manutención, por la enorme cantidad de alimento que consumían, muchas veces imposible de proporcionar, hizo que se dejase de utilizar los elefantes como elemento de guerra.

La familia Elephantidae se subdivide en dos subfamilias y ocho géneros:




Los géneros "Anancus"†, "Tetralophodon"†, "Stegomastodon"† y "Paratetralophodon"† considerados antes como pertenecientes a esta familia son hoy clasificados en otros grupos.




</doc>
<doc id="19277" url="https://es.wikipedia.org/wiki?curid=19277" title="Potencia (física)">
Potencia (física)

En física, potencia (símbolo P) es la cantidad de trabajo efectuado por unidad de tiempo.

Si "W" es la cantidad de trabajo realizado durante un intervalo de tiempo de duración Δ"t", la potencia media durante ese intervalo está dada por la relación

La potencia instantánea es el valor límite de la potencia media cuando el intervalo de tiempo Δ"t" se aproxima a cero. En el caso de un cuerpo de pequeñas dimensiones
\mathbf{F}\cdot \mathbf{v}</math>
Donde

La potencia mecánica aplicada sobre un sólido rígido viene dada por el producto de la fuerza resultante aplicada por la velocidad:

Si además existe rotación del sólido y las fuerzas aplicadas están cambiando su velocidad angular:

donde:

Para un sólido deformable o un medio continuo general la expresión es más compleja y se expresa como producto del tensor tensión y el campo de velocidades. La variación de energía cinética viene dada por:
+ \int_V \sum_{ij} T_{ij}D_{ij}\ \mathrm{d}V</math>
donde:

La potencia eléctrica desarrollada en un cierto instante por un dispositivo viene dada por la expresión

Donde:
Si el componente es una resistencia, tenemos:

Donde:

La potencia calorífica de un dispositivo es la cantidad de calor que libera por la unidad de tiempo:

La potencia sonora, considerada como la cantidad de energía que transporta la onda sonora por unidad de tiempo a través de una superficie dada, depende de la intensidad de la onda sonora y de la superficie , viniendo dada, en el caso general, por:


Para una fuente aislada, el cálculo de la potencia sonora total emitida requiere que la integral anterior se extienda sobre una superficie cerrada.





</doc>
<doc id="19287" url="https://es.wikipedia.org/wiki?curid=19287" title="Amadeo I (desambiguación)">
Amadeo I (desambiguación)

Amadeo I puede referirse a:

</doc>
<doc id="19291" url="https://es.wikipedia.org/wiki?curid=19291" title="Bosque">
Bosque

Un bosque es un ecosistema donde la vegetación predominante la constituyen los árboles y matas. Estas comunidades de plantas cubren grandes áreas del globo terráqueo y funcionan como hábitats para los animales, moduladores de flujos hidrológicos y conservadores del suelo, constituyendo uno de los aspectos más importantes de la biosfera de la Tierra. Aunque a menudo se han considerado como consumidores de dióxido de carbono atmosférico, los bosques maduros son prácticamente neutros en cuanto al carbono, y son solamente los alterados y los jóvenes los que actúan como dichos consumidores. De cualquier manera, los bosques maduros juegan un importante papel en el ciclo global del carbono, como reservorios estables de carbono y su eliminación conlleva un incremento de los niveles de dióxido de carbono atmosférico.

Los bosques pueden hallarse en todas las regiones capaces de mantener el crecimiento de árboles, hasta la línea de árboles, excepto donde la frecuencia de fuego natural es demasiado alta, o donde el ambiente ha sido perjudicado por procesos naturales o por actividades humanas. Los bosques a veces contienen muchas especies de árboles dentro de una pequeña área (como la selva lluviosa tropical y el bosque templado caducifolio), o relativamente pocas especies en áreas grandes (por ejemplo, la taiga y bosques áridos montañosos de coníferas). Los bosques son a menudo hogar de muchos animales y especies de plantas, y la biomasa por área de unidad es alta comparada a otras comunidades de vegetación. La mayor parte de esta biomasa se halla en el subsuelo en los sistemas de raíces y como detritos de plantas parcialmente descompuestos. El componente leñoso de un bosque contiene lignina, cuya descomposición es relativamente lenta comparado con otros materiales orgánicos como la celulosa y otros carbohidratos.

El término "floresta" fue equivalente a bosque en los libros de caballerías, como corresponden a su origen (del latín "foresta"), pero el cruce fonético con flor le añadió después la idea de amenidad que hoy se le asocia. "Selva" fue equivalente a bosque según su origen etimológico, pero hoy se le asocia al bosque denso tropical y/o lluvioso. "Parque" es un bosque natural o artificial con un área delimitada. "Arboleda" es un área boscosa menor o sembrada.

Los bosques se diferencian de los "arbolados" por el grado de cobertura del dosel vegetal, en un ecosistema arbolado la presencia de árboles es minoritaria porque predominan las hierbas o matorrales; en un bosque las ramas y el follaje de los árboles distintos a menudo se encuentran o se entrelazan, aunque puedan haber huecos de distintos tamaños dentro de un bosque. Un arbolado tiene un dosel más abierto, con árboles notoriamente más espaciados, lo que permite que más luz solar llegue al suelo entre ellos; tal es el caso de la sabana arbolada y la pradera boscosa, en donde predominan los herbazales.

Una clasificación se establece por la composición predominante de los bosques según el tipo de hoja: hoja ancha, acicular (coníferas como el pino), o ambos.

Una forma de clasificación de los bosques es determinar la longevidad de las hojas de la mayoría de los árboles.



La fisionomía clasifica los bosques por su estructura física total o etapa de crecimiento. Los bosques pueden también ser clasificados más específicamente por las especies dominantes presentes en los mismos. Desde el punto de vista de su historia y grado de alteración, los bosques pueden ser clasificados en:


El WWF clasifica a los bosques dentro de los siguientes biomas:

El estudio científico de los bosques se denomina ecología forestal, mientras que su administración por lo general es conocida como silvicultura, normalmente con el fin de extracción de recursos sostenible. Los ecólogos forestales se especializan en los patrones y procesos del bosque, generalmente con el objetivo de aclarar las relaciones de causa y efecto. Los silvicultores por lo general se enfocan en extraer madera y en la silvicultura, incluyendo la regeneración y el proceso de crecimiento de los árboles.

Los bosques pueden ser alterados cuando suceden hechos como la tala de árboles, los incendios forestales, la lluvia ácida, los herbívoros, o las plagas, junto con otras cosas, provocando un daño. En los Estados Unidos, la mayoría de los bosques han sido históricamente "atacados" por los humanos hasta puntos muy altos, aunque en los últimos años las prácticas silvícolas han mejorado, ayudando así a regular el impacto. Pero de todos modos el Servicio Forestal estadounidense (United States Forest Service) estima que cada año se pierden cerca de 1,5 millones de acres (6000km²) de los 750 millones (3000000km²) que hay en la nación.

Los diez países con mayor riqueza forestal suman el 67% del área de bosque total. Rusia por sí sola tiene el 20% del total mundial.

El manejo de los bosques naturales puede tener varios objetivos:

Por ejemplo en Misiones, Argentina, casi dos tercios de su superficie está cubierta con bosques. Se explota el bosque nativo para diferentes usos. Las especies más valoradas son el cedro, el peteribi (muebles) y el guatambu (madera terciada). Hay extensas áreas de bosques implantados con pino (especie no nativa) y araucarias (especie nativa) principalmente en las márgenes del río Paraná. La producción forestal se destina a las fábricas de pastas celulósicas de Puerto Esperanza, Puerto Piray, y Puerto Mineral, a los aserraderos y otras industrias forestales existentes en la provincia.

Principales amenazas ambientales para los bosques

El cambio climático, la contaminación o las plagas, entre otros, son algunos de los factores que estresan a los bosques. En muchos casos, el interés de las compañías nómadas multinacionales por los recursos minerales, la construcción de presas que inundan amplias zonas selváticas o el crecimiento de las ciudades y las vías de comunicación (carreteras, canales, etc.) son otras tantas razones para la regresión o fragmentación del bosque.

Mientras en el mundo la superficie forestal disminuye, en Europa aumenta. Durante los sesenta y setenta, se levantó una gran preocupación por el decaimiento del bosque en el continente, cuando el 45 % de los bosques mostraban síntomas de enfermedad: defoliación, mortalidad de individuos, etc. La mayoría de estudios relacionaron el decaimiento forestal con la contaminación del aire. El proceso era particularmente grave en Europa Central, sobre suelos ácidos, donde las fuertes emisiones de dióxido de azufre hacía bajar el pH del agua de lluvia a valores cuyo promedio podía acercarse a 3.

Hay algunos factores externos que pueden causar el deterioro o destrucción del ecosistema del bosque, entre los que se incluyen la inundación del terreno de la represa para formar un reservorio (ver el capítulo sobre “Represas y Reservorios”), el desbroce del bosque para ganadería (ver el capítulo “Manejo de Ganado y Terrenos de Pastoreo”), la agricultura migratoria, y su conversión a la agricultura comercial (caucho, palma africana, café arroz y cacao).

Es motivo de preocupación mundial el deterioro rápido o destrucción completa de muchas áreas del bosque tropical húmedo de tierra baja, caracterizado por su gran diversidad de especies y complejidad ambiental, y las dificultades que se presentan al tratar de manejarlos de manera sostenible. Si bien la conservación de estas áreas forestales únicas, mediante el establecimiento de parques y reservas, es, potencialmente, la mejor manera de proteger su biodiversidad, los procesos ambientales, y los estilos de vida de sus moradores indígenas, sólo se puede proteger, en esta forma, algunas áreas limitadas. Las presiones económicas y el crecimiento de la población están intensificando el uso de la tierra que, anteriormente, era sustentable (agricultura migratoria), pero ahora alcanza niveles no sostenibles y destructivos, motivando la explotación forestal de desbroce, e impulsando la conversión en gran escala, de las tierras forestales a la agricultura y la ganadería, que, generalmente, son insostenibles y producen daños permanentes en el ecosistema forestal. Una de las maneras más adecuadas de proteger los bosques y prevenir su conversión a otras actividades orientadas a la producción, y preservar gran parte de sus valores ambientales, es la de manejar los bosques naturales para que su producción de madera y otros productos sea sustentables, y produzca resultados económicos importantes.

Las dos cuestiones críticas del manejo del bosque tropical húmedo para la producción de madera son:

En teoría, los bosques tropicales húmedos pueden proveer los productos forestales en forma indefinida. La realidad, sin embargo, es que existen pocos sistemas que han resultado ser sustentables, o que puedan ser aplicados a la mayoría de estos bosques naturales con un número limitado de especies. Por esta razón, y debido a las presiones económicas que exigen la generación de ingresos rápidos, solo una pequeña porción de los bosques tropicales húmedos de tierra baja que están siendo explotados, actualmente, se manejan de una manera sustentable.

El sistema de manejo forestal más adecuado para los bosques tropicales húmedos de tierra baja, por su gran diversidad de especies, es la explotación selectiva con la cual solo se extrae, un pequeño número de árboles por hectárea. Si esto se hace con cuidado, con un mínimo de deterioro del suelo y la vegetación circundante, se puede limitar los daños ambientales. Se reduce al mínimo los impactos sobre la biodiversidad del bosque y su capacidad para proveer servicios ambientales, porque no se crean grandes espacios en el bosque, como es el caso con el desbroce.

Casi en todas las iniciativas que tienen un impacto en los bosques naturales, sea la explotación comercial de la madera, las industrias de procesamiento, o su conversión a otros usos, para otras actividades (minería, construcción de represas, riego, desarrollo industrial), o la clausura de los bosques para su rehabilitación o conservación, surgen cuestiones sociales importantes. Los proyectos de desarrollo que desbrozan los bosques para otros usos pueden desplazar a la gente o reducir su acceso a los recursos forestales, de los cuales depende para subsistir. La explotación forestal comercial puede destruir los recursos que son importantes, localmente, para las economías de subsistencia, y pueden abrir las áreas a la colonización incontrolada, causando mayor degradación ambiental y conflicto social. Asimismo, la clausura de los bosques para su rehabilitación o conservación puede reducir los ingresos de las poblaciones a su alrededor, privándoles de los nutrientes importantes o productos que generan ingresos. Esta clausura puede causar mayor degradación. Si la presión sobre el área cerrada es demasiado grande, los esfuerzos de conservación y rehabilitación pueden fracasar.

Los moradores del bosque tienen mucho conocimiento acerca de las calidades, utilización potencial, y sostenibilidad de la flora, la fauna, y los recursos geológicos locales, basado, a menudo, en el conocimiento adquirido en siglos de uso sostenible.

En las áreas altas, áridas y semiáridas, donde las fuentes de forraje sean limitadas, usualmente, los bosques y los sistemas locales de producción ganadera, están vinculados estrechamente; los agricultores, con frecuencia, adoptan estrategias de subsistencia mixta, en las que la producción ganadera en el bosque juega un papel importante. Por ejemplo, en la región Himalaya, la productividad de la agricultura de "tierra alta" depende principalmente del compost, y el humus que se recolecta en los bosques.

La caza y la recolección, así como la agricultura migratoria, han sido practicadas durante ciento de años en los bosques tropicales húmedos.

La pesca artesanal en la zona aluvial es importante para muchos de los moradores de los bosques de "tierra baja".

Generalmente, la organización social de los grupos tradicionales está muy adaptada a las exigencias de los sistemas de producción. El conocimiento, tanto técnico, como administrativo, de estos recursos puede ser muy útil para los especialistas técnicos que buscan intensificar o modificar la producción de esta área u otra similar, es decir, para adaptar las recomendaciones agrícolas a las áreas donde, actualmente, se practica la agricultura migratoria, o para desarrollar modelos de gestión y utilización forestal para los bosques que serán rehabilitados. Al desplazarse los grupos que viven en los bosques, su conocimiento técnico aborigen del manejo y utilización del bosque, a menudo, se pierde. Se debe efectuar una evaluación cuidadosa, incluyendo un análisis económico real, antes de suponer que los usos actuales del bosque deban ser abandonados por algo «mejor».

Los aspectos de la tenencia de la tierra, casi siempre, son una preocupación en los proyectos forestales. A menudo, existen derechos sobrepuestos, que incluyen la tenencia reconocida por el Estado, y la tenencia de costumbre y/o sistemas de derechos concesionarios en cuanto a los productos. En el caso de las minorías étnicas que viven en los bosques, pueden haber derechos consuetudinarios muy fuertes sobre las tierras forestales, que sean válidos, constitucionalmente, a pesar de haberse transferido al gobierno, subsiguientemente, la autoridad sobre estas tierras.

En muchas sociedades, los derechos a la tierra y a los árboles pueden ser separados, con normas específicas para las diferentes especies. Los grupos que viven en el bosque, con frecuencia, tienen reglamentos complejos de propiedad en cuanto a los bosques y los productos. Por ejemplo, los derechos a los árboles frutales pueden ser distintos a los que permiten que los individuos den otro uso a la tierra forestal, incluyendo la agricultura migratoria. Los sistemas tradicionales de tenencia pueden ser más apropiados para el manejo de las tierras frágiles, que las opciones propiciadas por el Estado.

La clausura de los bosques, o restricción del acceso y uso de los recursos, afecta, de manera diferente, a muchos grupos de la población. Por ejemplo, los ganaderos sin tierras pueden ser los más perjudicados económicamente, por la clausura de estas áreas, porque ellos, a diferencia de los agricultores con tierras, no pueden obtener forraje de su propio terreno. Las mujeres pueden tener una carga de trabajo mucho mayor debido a la necesidad de viajar distancias mucho mayores para encontrar los recursos necesarios; sin embargo, la gente local posiblemente no identifique esta carga como un problema, debido al estado más bajo de la mujer en la sociedad. Si las rutas de los pastores migratorios son afectadas, estos pueden ser obligados a utilizar excesivamente otras tierras fuera del área del proyecto, que todavía estén disponibles, produciendo impactos negativos, tanto para esas tierras, como para los grupos sedentarios que dependen de ellas.

Los planificadores, cada vez más, están explorando las maneras de integrar las necesidades de la gente local a las iniciativas de conservación y rehabilitación de los bosques, a través de la promoción del manejo adecuado de los recursos de propiedad común o los sistemas de administración conjunta entre el gobierno y los usuarios. Es importante documentar los sistemas locales de administración existentes, incluidos los que han fallado debido al aumento de presión. En las áreas de biodiversidad única, otras medidas han incluido la creación de zonas de protección, que generan alternativas para la gente que depende, tradicionalmente, del área que va a ser conservada, o se han diseñado sistemas de conservación que permiten que la gente local utilice, en forma controlada, el área protegida. Ejemplos:

La expansión de la utilización de los productos forestales puede ayudar a intensificar el manejo del bosque. Muchas especies no se utilizan por falta de la infraestructura necesaria de procesamiento o comercialización. En los bosques tropicales, con su gran diversidad de especies, a menudo, las especies individuales que son comerciales están dispersas en un área grande, dificultando la cosecha, y, a menudo, volviéndola antieconómica. Posiblemente no sea rentable la explotación forestal en los bosques menos diversos, pero remotos, o de baja densidad.

Si los productos nuevos fueran de otras especies, o si fuera posible aprovechar muchos diferentes tamaños, gracias al mejoramiento del proceso o el desarrollo de nuevos mercados, se podría utilizar una mayor proporción del material forestal. Existe mucha amplitud, no solamente para desarrollar los nuevos productos, sino también para conservar las existencias actuales (p. ej. desarrollando chapas, madera terciada y aglomerado que sean más eficientes, utilizando los desperdicios de la explotación forestal y reciclando los desechos de las plantas de procesamiento) puede ayudar a equilibrar la oferta con la demanda, y quitar la presión que se aplica sobre los bosques naturales. Son obvios los beneficios de estos métodos, así también los peligros. El mayor uso de una selección más amplia de especies puede llevar al desbroce en gran escala, o a la «minería» del recurso forestal.

Las alternativas para el manejo de los bosques primarios y secundarios, para madera, los productos no igníferos y la producción agrícola y ganadera, amplia y de bajo impacto, son las siguientes:







</doc>
<doc id="19293" url="https://es.wikipedia.org/wiki?curid=19293" title="Océano Ártico">
Océano Ártico

El océano Ártico u océano Glacial Ártico es la parte del océano mundial más pequeña y más septentrional del planeta. Se encuentra principalmente al norte del círculo polar ártico, ocupando el área entre Europa, Asia y América del Norte. Abarca unos 14 056 000 km² de extensión y sus profundidades oscilan entre los 2000 m y 4000 m en la región central, y los 100 m en la plataforma continental. Su profundidad media es de 1205 m bajo el nivel del mar.

Este océano limita con la parte norte del Atlántico, recibiendo grandes masas de agua a través del estrecho de Fram y el mar de Barents. Está limitado por el estrecho de Bering, entre Chukotka (Rusia) y Alaska (EE. UU.), que lo separa del Pacífico; por la costa norte de Alaska y Canadá. También limita con el litoral septentrional de Europa y Asia.

Grandes masas de hielo protegen durante todo el año a este océano de las influencias atmosféricas. En su parte central pueden encontrarse casquetes de hielo de hasta cuatro metros de espesor. Las grandes capas de hielo suelen formarse por el deslizamiento de grandes paquetes de hielo uno sobre otro.

Las temperaturas en invierno suelen rondar los −50 °C debido a los fuertes vientos provenientes de Siberia (Rusia); mientras que en el verano apenas pueden superar el 0 °C; en tanto que en la plataforma continental pueden darse temperaturas de hasta 30 °C.

El océano Glacial Ártico ocupa una cuenca aproximadamente circular y se extiende por una superficie de alrededor de 14 056 000 kilómetros cuadrados, casi el tamaño de Rusia. La costa tiene 45 389 kilómetros de largo. Está rodeado por las masas terrestres de Eurasia, América del Norte, Groenlandia y por varias islas. Generalmente se considera que incluye la bahía de Baffin, el mar de Barents, el mar de Beaufort, el mar de Chukotka, el mar de Siberia Oriental, el mar de Groenlandia, la bahía de Hudson, el estrecho de Hudson, el mar de Kara, el mar de Laptev, el mar Blanco y otros conjuntos hídricos. Se conecta con el océano Pacífico a través del estrecho de Bering y con el océano Atlántico a través del mar de Groenlandia y el mar de Labrador.

Una dorsal oceánica, la dorsal de Lomonósov, separa la honda cuenca polar del Norte marino en dos cuencas oceánicas: la Euroasiática que tiene una profundidad de entre 4000 y 4500 metros, y la Asiático-americana (a veces llamada de Norteamérica o cuenca hiperbórea), de alrededor de 4000 metros de profundidad. La batimetría del fondo oceánico está marcado por dorsales de fallas, llanuras de la zona abisal, profundidades del océanos y cuencas. La profundidad media del océano Ártico es de 1038 metros. El punto más profundo está en la cuenca euroasiática, con 5450 metros.

Las dos grandes cuencas están subdivididas a su vez por dorsales en la cuenca canadiense (entre Alaska/Canadá y la dorsal Alpha), la cuenca de Makarov (entre las crestas Alpha y de Lomonósov), la cuenca del Fram (entre la dorsal de Lomonosov y la de Gakkel) y la cuenca de Nansen (cuenca de Amundsen) (entre la dorsal de Gakkel y la plataforma continental que incluye la Tierra de Francisco José).

Según estudios realizados por especialistas de la Universidad de Oxford (Reino Unido) y del Instituto Real de los Países Bajos para la Investigación Marina, el océano Ártico gozaba, hace unos setenta millones de años, de temperaturas similares a las que hoy día se encuentran en el Mar Mediterráneo, con mediciones de unos 15 °C; y temperaturas de unos 20 °C hace unos veinte millones de años.

Llegaron a esta conclusión los investigadores después de estudiar materiales orgánicos encontrados en el lodo de islotes de hielo del océano Ártico. No se sabe aún por qué se daban estas temperaturas en aquellos tiempos, pero se cree en que el responsable puede haber sido el efecto invernadero derivado de una fuerte concentración de dióxido de carbono en la atmósfera (el problema de esta hipótesis es el extraordinariamente mínimo efecto invernadero del gas carbónico).

El clima polar caracterizado por el frío persistente y variedades anuales de temperaturas relativamente estrechas; inviernos caracterizados por la oscuridad continua, condiciones frías y estables, y cielos despejados; los veranos caracterizados por la luz del día continua, húmedo y tiempo brumoso, con muchas nevadas y ciclones débiles con lluvia o nieve.°

Existen unas cuatrocientas especies animales en esta zona. De ellas, la más conocida es el oso polar, el mayor carnívoro del lugar. Llega a tener un peso de 800 kg y se alimenta de focas y peces, aunque si no logra atraparlos puede reemplazarlos momentáneamente por musgos y líquenes.

Seis especies de focas habitan este lugar, aunque su número ha ido decreciendo desde el siglo XIX debido a su depredador natural, el oso polar, y a la caza indiscriminada a que fue sometida por el hombre debido a lo preciado de su piel y su grasa. Otro poblador típico de la zona es la ballena, igualmente amenazada y que, actualmente, se halla protegida de la captura indiscriminada.

También se encuentra un diminuto pero importante habitante: el kril, que desempeña un papel importantísimo en la cadena alimenticia de la región.

La banquisa polar está adelgazando, y en muchos años habrá un agujero estacional en la capa de ozono.
La reducción de la superficie de hielo en el océano Ártico reduce el albedo medio del planeta, lo que posiblemente dé como resultado el calentamiento global en un mecanismo de retroalimentación positiva. La investigación muestra que el Ártico puede quedar libre de hielo por primera vez en la historia de la Humanidad entre el año 2013 y 2040. Muchos científicos están actualmente preocupados por el calentamiento de las temperaturas en el Ártico, porque podrían causar que grandes cantidades de agua fresca derretida entrase en el Atlántico norte, posiblemente perturbando los patrones de corrientes oceánicas globales. Potencialmente pueden ocurrir después drásticos cambios en el clima de la Tierra.

Los investigadores predicen que, en no más de cincuenta años, el océano Ártico será perfectamente navegable durante el verano. Es que el hielo que cubre esta masa oceánica se está haciendo cada vez más delgado, debido a que el tiempo de duración de altas temperaturas es cada vez mayor. Durante los pasados años se ha observado la fusión de la capa de hielos y, en agosto de 2004, científicos estadounidenses que navegaban en un buque y ruso, denunciaron la existencia de una laguna en el Polo Norte, que no pudo ser confirmada por imágenes satelitales, pero que en modo alguno sorprendió a la comunidad científica, quienes vienen alertando sobre el peligro del calentamiento global.

Se sabe, pues, que el espesor de la capa de hielos del océano Ártico ha disminuido un 40 % durante los pasados cincuenta años y los resultados indican que si esto continúa, la fusión de los hielos será más rápida cada vez, culminando con la desaparición de estos durante el verano, con serias consecuencias para el equilibrio ecológico de la zona y para el hábitat de ciertas especies, como el oso polar que necesita de esas capas de hielo para sobrevivir y cazar sus alimentos.

Otras preocupaciones medioambientales se refieren a la contaminación radiactiva del océano Ártico por, por ejemplo, los residuos radiactivos rusos en el mar de Kara y pruebas nucleares realizadas durante la época de la Guerra fría en lugares como Nueva Zembla.

El deshielo del Ártico abre nuevas posibilidades para explotar sus recursos naturales. En el lecho marino del Ártico se encuentra el 25 por ciento de las reservas mundiales de petróleo y gas natural. También el estaño, manganeso, oro, níquel, plomo y platino están presentes en cantidades importantes. Por ello y sumado a la importancia geoestratégica, el 2 de agosto de 2007 dos batiscafos rusos "Mir" realizaron una inmersión en el océano Glacial Ártico, en el Polo Norte, e instalaron en el fondo una bandera rusa, así como una cápsula con mensaje para generaciones venideras. Los Mir recogieron pruebas para demostrar que las cordilleras subacuáticas Lomonósov y Mendeléiev son la extensión natural de la plataforma continental de Rusia, hipótesis que, de ser confirmada, permitiría a Rusia reivindicar en el futuro derechos exclusivos sobre la explotación de los recursos minerales en esta zona.

Ocasionalmente se disgregan islas de hielo de la parte norte de la isla Ellesmere, y se forman icebergs a partir de los glaciares de la costa occidental de Groenlandia y el extremo noreste de Canadá. El permafrost se encuentra en la mayor parte de las islas. El océano está virtualmente cerrado por el hielo desde octubre hasta junio, y los barcos que lo naveguen están amenazados con quedar cubiertos de hielo desde octubre hasta mayo. Antes de que llegaran los modernos rompehielos, los barcos que zarpaban al océano Ártico se arriesgaban a quedar atrapados o aplastados por los hielos marinos (aunque el SS Baychimo vagó por el océano Ártico desatendido durante décadas a pesar de estos riesgos).




</doc>
<doc id="19294" url="https://es.wikipedia.org/wiki?curid=19294" title="Cereal">
Cereal

Los cereales (de "Ceres", el nombre en latín de la diosa de la agricultura) son plantas de la familia de las poáceas cultivadas por su grano (fruto de pared delgada adherida a la semilla, característico de la familia). Incluyen cereales mayores como el trigo, el arroz, el maíz, la cebada, la avena y el centeno, y cereales menores como el sorgo, el mijo, el teff, el triticale, el alpiste o la lágrima de Job. El tamaño del grano de algunos cereales, más grande que el de los demás pastos, fue producto de la domesticación a lo largo de miles de años. Muchos cereales en los inicios de su domesticación fomentaron la aparición de civilizaciones que se asociaron a ellos.

Los cereales contienen almidón. El germen de la semilla contiene lípidos en proporción variable que permite la extracción de aceite vegetal de ciertos cereales. La semilla está envuelta por una cáscara formada sobre todo por la celulosa, componente fundamental de la fibra dietética. Se emplean en la alimentación humana (especialmente el trigo, el arroz y el maíz) y del ganado, así como en la fabricación industrial de diversos productos.

Los cereales modernos, principalmente el trigo y el maíz, son el resultado de la selección efectuada durante la denominada revolución verde (segunda mitad del siglo XX), con el objetivo de conseguir variedades de alto rendimiento. Los procedimientos desarrollados obtuvieron un gran éxito en el aumento de la producción, pero no se dio suficiente relevancia a la calidad nutricional, resultando en cereales con proteínas de baja calidad y alto contenido en hidratos de carbono. Su consumo excesivo puede provocar el desarrollo de un gran número de enfermedades crónicas, incluyendo la diabetes tipo 2, la presión arterial alta, enfermedades del corazón, sobrepeso y obesidad. Algunas evidencias indican que consumidos sin refinar (cereales integrales) pueden ser beneficiosos en la prevención de la diabetes tipo 2, enfermedades cardiovasculares y el cáncer colorrectal.

Ciertos cereales contienen un conjunto de proteínas, el gluten, que ayuda a proporcionar elasticidad a las masas empleadas para la elaboración del pan y otros productos de repostería. Estos incluyen el trigo, la avena, la cebada y el centeno (T.A.C.C.) y cualquiera de sus variedades e híbridos tales como la espelta, la escanda, el kamut y el triticale. El consumo de estos cereales puede provocar el desarrollo de los denominados trastornos relacionados con el gluten, que incluyen la enfermedad celíaca, la sensibilidad al gluten no celíaca, la alergia al trigo, la dermatitis herpetiforme, la ataxia por gluten, y diversos trastornos neurológicos que pueden desarrollarse aunque no haya ningún tipo de daño o inflamación en el intestino, es decir, tanto en celíacos como en no celíacos. Prácticamente la totalidad de los casos reales continúa sin reconocer, sin diagnosticar y sin tratar. Asimismo, el gluten es uno de los factores más potentes que provocan un aumento de la permeabilidad intestinal, que está implicada en el desarrollo de enfermedades autoinmunes, cánceres, enfermedades del sistema nervioso, enfermedades inflamatorias, infecciones, alergias y asma.

Las evidencias históricas y arqueológicas muestran que previamente a la revolución agrícola del Neolítico (VIII milenio a. C.), los seres humanos en general no mostraban signos ni síntomas de enfermedades crónicas y que, coincidiendo con la inclusión de los cereales en la dieta, se produjo una serie de consecuencias negativas sobre la salud, muchas de las cuales continúan presentes en la actualidad. Entre ellas cabe destacar múltiples deficiencias nutricionales, tales como la anemia ferropénica, trastornos minerales que afectan tanto a los huesos (osteopenia, osteoporosis, raquitismo) como a los dientes (hipoplasias del esmalte dental, caries dentales), y una alta incidencia de trastornos neurológicos, enfermedades psiquiátricas, la obesidad, la diabetes tipo 2, la ateroesclerosis y otras enfermedades crónicas o degenerativas.

La humanidad existe desde hace unos 2,5 millones de años, pero los cereales se introdujeron en la dieta hace unos 10.000 años, durante la revolución neolítica y el desarrollo de la agricultura. El ser humano pasó de una alimentación basada en la caza y la recolección a una dieta con un alto contenido en cereales.

Este cambio de la alimentación se ha producido a un ritmo muy rápido en un plazo de tiempo muy corto desde el punto de vista evolutivo, con modificaciones mucho más marcadas durante las últimas décadas del siglo XX e inicios del siglo XXI, como consecuencia de la revolución verde y la progresiva difusión de los alimentos procesados y la comida rápida. No obstante, nuestro genoma y fisiología no se han modificado apenas durante los últimos 10.000 años y nada en absoluto en los últimos 40-100 años, dando como resultado una dieta "desadaptativa". Algunos autores opinan que esta hipótesis de la discordancia evolutiva ha proporcionado un marco teórico valioso, pero se trata de una visión incompleta que no refleja la flexibilidad, la variabilidad y la adaptabilidad en el comportamiento alimentario humano y la salud en el pasado y el presente.

A raíz de las dos guerras mundiales se hizo evidente la necesidad de aumentar la producción agrícola, para satisfacer la creciente demanda de alimentos de la población. Las estrategias puestas en práctica para solucionar este problema, durante la denominada revolución verde, fueron un éxito en cuanto a la producción pero no dieron suficiente relevancia a la calidad. Se desarrollaron las variedades de cereales que se cultivan en la actualidad, las cuales tienen un alto contenido en carbohidratos y una baja calidad nutricional, y que además desplazaron a los cultivos de legumbres. Estos cereales de alto rendimiento presentan deficiencias en aminoácidos esenciales y contenidos desequilibrados de ácidos grasos esenciales, vitaminas, minerales y otros factores de calidad nutricional. 

La "Nutrition Society", fundada en 1941 en Gran Bretaña, se centró en la mejora del cultivo del trigo. Las especies fueron seleccionadas para conseguir variedades resistentes a climas extremos y a las plagas, con alto contenido en gluten, cuyas propiedades viscoelásticas y adhesivas únicas son muy demandadas por la industria alimentaria, pues facilitan la preparación de masas, alimentos elaborados y diversos aditivos. El proyecto fue un éxito en relación a la producción, con tasas actuales que superan los 700 millones de toneladas por año, pero provocó un cambio drástico en la genética del trigo. El trigo moderno (aproximadamente el 95% del trigo cultivado en la actualidad) es una especie híbrida que contiene mayor cantidad de gluten (aproximadamente el 80-90% del total de proteínas), cuya capacidad inmunogénica y citotóxica es probablemente mayor, capaz de atravesar tanto la barrera intestinal como la barrera hematoencefálica y acceder al cerebro.

Se baraja la hipótesis de que esta modificación genética del trigo y el aumento del consumo de gluten, han sido demasiado altos y en un espacio de tiempo excesivamente corto para permitir la adaptación de nuestro sistema inmunitario, con el consiguiente aumento de los trastornos relacionados con el gluten, si bien esta teoría aún no está completamente aclarada.

La evidencia histórica y arqueológica muestra que, previamente a la revolución agrícola, los seres humanos en general no mostraban signos ni síntomas de enfermedades crónicas. Diversos estudios etnológicos y arqueológicos revelan que coincidiendo con la inclusión de los cereales en la dieta, se produjo una serie de consecuencias negativas sobre la salud, entre las que destacan reducciones de la estatura, disminución de la esperanza de vida, aumento de las enfermedades infecciosas, de la mortalidad infantil, las enfermedades neurológicas y psiquiátricas, múltiples deficiencias nutricionales, incluyendo anemia ferropénica, trastornos minerales que afectan tanto a los huesos (raquitismo, osteopenia, osteoporosis) como a los dientes (hipoplasias del esmalte dental, aumento de las caries dentales), y otras deficienicas de minerales y vitaminas. Parte de estos efectos negativos han sido compensados por el progreso de la higiene, el desarrollo de la Medicina y la complementación de las dietas basadas en cereales con otras fuentes de nutrientes, consiguiendo una reducción de la mortalidad infantil y una esperanza media de vida más larga. No obstante, la mayor parte de las consecuencias negativas continúa presente en la actualidad: el cambio de la alimentación basada en la caza y la recolección a las dietas con alto contenido en cereales y el estilo de vida occidental, está asociado a la alta incidencia de la obesidad, la diabetes tipo 2, la ateroesclerosis, las enfermedades psiquiátricas, los trastornos neurológicos y otras enfermedades crónicas o degenerativas.

Durante la Segunda Guerra Mundial, decayó el consumo de trigo y otros cereales como consecuencia de la escasez de suministros. El análisis de las admisiones en hospitales psiquiátricos de cinco países demostró un descenso en los ingresos por esquizofrenia, que se correlacionó con el porcentaje de disminución en el consumo de trigo. Por el contrario, en los Estados Unidos, donde hubo un incremento en el consumo de trigo, las admisiones por esquizofrenia aumentaron, por lo que se formuló la hipótesis de que la esquizofrenia es poco frecuente si el consumo de cereales es raro. Esta hipótesis fue corroborada posteriormente por un estudio antropológico en las Islas del Pacífico Sur, que demostró que la prevalencia de esquizofrenia era baja en países con bajo consumo de trigo y aumentó con la introducción del trigo, la cebada, la cerveza y el arroz en las dietas. Actualmente, se ha demostrado la relación de la esquizofrenia en una parte de pacientes con el consumo de gluten, independientemente de la predisposición genética, es decir, tanto en celíacos como en no celíacos.


Las especies que caben dentro de esta categoría agronómica pertenecen en su mayoría a la familia Poaceae (gramíneas), cuyo fruto es inseparable de la semilla; sin embargo también se incluye a veces a plantas con semillas semejantes a granos que son de otras familias, como la quinua, el alforfón, el amaranto, el huauzontle o el girasol. Algunos autores llaman a estas últimas especies "falsos cereales" o "pseudocereales".

Las principales especies son: arroz, maíz, trigo, avena, sorgo, centeno, cebada, mijo.

Los cereales más empleados en la alimentación humana son el trigo, el arroz y luego el maíz. La cebada se utiliza fundalmentalmente en la fabricación de la cerveza para hacer la malta.

Los cereales con gluten, especialmente el trigo, se emplean principalmente para elaborar panes y pastas. El gluten es muy apreciado por sus cualidades viscoelásticas únicas, que aportan elasticidad a la masa de harina, lo que permite que junto con la fermentación el pan obtenga volumen, así como la consistencia elástica y esponjosa de los panes y masas horneadas. Estas características físicas del gluten facilitan la producción de numerosos alimentos procesados, comida rápida y aditivos alimentarios, cuyo consumo se ha incrementado espectacularmente debido al proceso de industrialización global y a la occidentalización de la dieta. Como aditivo, se emplea para conferir viscosidad, espesor o volumen a una gran cantidad de productos alimenticios, lo que provoca que exista presencia de proteínas tóxicas para una parte de la población en los productos menos sospechosos. Más de la mitad de los alimentos que se comercializan actualmente contiene gluten de trigo, cebada, centeno o avena como espesante o aglutinante, en forma de contaminación cruzada o incluso por adulteración.

Tradicionalmente, los cereales con gluten se denominan "cereales panificables", sin bien también es posible elaborar panes sin gluten con harinas de otros cereales, como el maíz y el arroz, pseudocereales (tales como el amaranto, la quinua, el alforfón o el teff) y cereales menores (como el sorgo o el mijo).

Algunos cereales secundarios se han convertido al gusto de hoy día con la vuelta a una agricultura orgánica como la espelta, el centeno o la avena.

Determinados cereales, como la avena o el arroz, se emplean para elaborar las denominadas "leches vegetales".

Otras plantas como quinua, que se cultiva tradicionalmente en América del Sur, tienen un mercado en crecimiento, especialmente en el ámbito de la agricultura ecológica. Cabe aclarar que la quinua es un pseudocereal, perteneciente a la subfamilia Chenopodioideae de las amarantáceas.


Una gran parte de la producción mundial se destina a la alimentación animal del ganado: en los países desarrollados, el 56 por ciento del consumo de cereales se produce en la alimentación del ganado, el 23 por ciento en los países en desarrollo.A nivel mundial, el 37 por ciento de la producción de cereales se destina a alimentar a los animales de granja.

En alimentación animal se utilizan prácticamente todos los cereales, incluso el trigo, tradicionalmente reservado a los hombres, bajo diversas formas:

Además del grano, algunos cereales también proporcionan forrajes y paja.

Algunos de los usos de los cereales en la industria son los siguientes:


Los cereales pasan por diferentes etapas a través de una compleja y gran cadena, que se inicia en la cosecha y termina en el consumo. Este proceso está formado básicamente por tres áreas distintas. La primera cubre desde la cosecha hasta el almacenado del grano. La segunda —los métodos preliminares de procesamiento— involucra un tratamiento adicional del grano, pero los productos todavía no se encontrarán aptos para ser consumidos directamente. Antes de su consumo, éstos deberán pasar por una tercera etapa de procesamiento, como por ejemplo el humeado.

El pilado es el proceso por el cual se quita la cáscara al cereal, ya sea trigo, cebada, arroz, etc. Pulpa dentro de una cáscara.

La mayor parte de los granos comestibles cosechados en los trópicos se pierde debido a los inadecuados sistemas de manejo, almacenado y técnicas de procesamiento. Se estima que estas pérdidas oscilan entre el 10 y el 25% de la cosecha. Las causas más comunes por las cuales se producen estas pérdidas son:


El procesamiento de los cereales afecta a la composición química y al valor nutricional (esto quiere decir que su composición nutrimental es cambiada) de los productos preparados con cereales. Los nutrientes están distribuidos de modo heterogéneo en los distintos componentes del grano (germen, endospermo, revestimiento de la semilla y distintas capas que lo recubren). No existe un patrón uniforme para los distintos tipos de cereales. Los efectos más importantes del procesamiento sobre el valor nutricional de los cereales están relacionados con:


Los cereales por lo general contienen:

El refinado hace que se pierda la fibra, sales minerales y vitaminas.

Los cereales modernos de alto rendimiento han sido seleccionados dando prioridad a los que producen las semillas más grandes y regordetas. El almidón de estas semillas es del tipo ramificado (amilopectina), en contraposición a las semillas más pequeñas y arrugadas, que contienen almidón resistente.

Los almidones ramificados son fácilmente digeribles y se asimilan a gran velocidad. Esto provoca la liberación rápida de azúcar (alto índice glucémico). El consumo excesivo de alimentos de alto índice glucémico puede provocar el desarrollo de un gran número de enfermedades crónicas, incluyendo la diabetes tipo 2, la presión arterial alta y enfermedades del corazón. El organismo convierte en grasa el exceso de hidratos de carbono, lo que puede derivar en un exceso de peso y obesidad. Los alimentos con altos índices glucémicos generalmente no son recomendados para personas con diabetes (tanto tipo 1 como tipo 2).

Una revisión de los meta-análisis publicados hasta 2017 concluyó que existen algunas evidencias que indican que consumidos sin refinar (cereales integrales) podrían ser beneficiosos en la prevención de la diabetes tipo 2, enfermedades cardiovasculares y ciertos tipos de cáncer digestivo, que incluyen el cáncer colorrectal, el cáncer de páncreas y el cáncer de estómago. En comparación con los cereales refinados, en los que la cáscara, y por tanto la fibra, ha sido eliminada, los cereales integrales contienen fibra insoluble. Esta puede retrasar el tiempo de vaciamiento del estómago y disminuir la tasa de absorción de glucosa, lo que explica su menor índice glucémico. 

Un meta-análisis de 2018 concluyó que la relación entre la ingesta de cereales integrales y su efecto sobre el cáncer no está clara, puesto que depende de los subtipos de cáncer, el estilo de vida y factores dietéticos que deben ser investigados en estudios posteriores. Este meta-análisis demostró un descenso en la mortalidad por cáncer colorrectal del 16%, pero no confirmó otros hallazgos de meta-análisis previos y concluyó que la ingesta de cereales integrales no tiene ninguna influencia sobre el riesgo de desarrollar cáncer de páncreas, de próstata, de endometrio ni de mama. Los efectos protectores de los cereales integrales sobre el cáncer colorrectal se explican por su contenido en fibra, fitoestrógenos, vitaminas, antioxidantes y microelementos. Asimismo, la fibra ejerce un efecto beneficioso sobre la microbiota intestinal y el metabolismo, lo que explicaría su influencia beneficiosa sobre el riesgo de desarrollar obesidad, diabetes y enfermedades cardiovasculares.

El contenido de proteínas de los cereales es bajo en comparación con las legumbres y las plantas oleaginosas. Asimismo, son de bajo valor biológico y nutricional para el hombre por presentar deficiencias en aminoácidos esenciales, principalmente la lisina, aunque el arroz, la avena y la cebada contienen más lisina que el resto de cereales. El maíz tiene también bajo contenido de triptófano. Otros cereales a menudo contienen bajos niveles de treonina.

Hay diez aminoácidos que se consideran esenciales, puesto que los animales no pueden sintetizarlos y deben conseguirlos a través de la alimentación. Si los niveles de al menos uno de estos aminoácidos esenciales es deficiente, los demás son descompuestos y excretados, lo cual limita el crecimiento en los niños y hace que se pierda el nitrógeno de la dieta. Para compensar esta deficiencia, es preciso complementar con proteínas procedentes de otros alimentos, como pueden ser las legumbres. Otras posibilidades para mejorar el valor nutritivo de los cereales incluyen la fortificación con aminoácidos y otros nutrientes, la germinación y la fermentación.

En contraposición, los pseudocereales y los cereales menores poseen un elevado índice de valor nutricional y biológico, superior al del resto de los cereales tanto por su composición en aminoácidos esenciales, como por su biodisponibilidad o digestibilidad, no contienen gluten y representan una buena fuente de proteínas, fibra dietética, hidratos de carbono, vitaminas, minerales y ácidos grasos poliinsaturados. Destaca especialmente la quinua, que es el único alimento de origen vegetal que provee todos los aminoácidos esenciales, oligoelementos y vitaminas, equiparándose su calidad proteica a la de la leche.

Algunos cereales contienen un conjunto de proteínas de pequeño tamaño, el gluten, que es apreciado por su capacidad para proporcionar elasticidad a las masas empleadas para la elaboración del pan y otros productos de repostería. El gluten está presente exclusivamente en los cereales de secano, fundamentalmente el trigo, pero también la cebada, el centeno y la avena, o cualquiera de sus variedades e híbridos (tales como la espelta, la escanda, el kamut y el triticale).

Las proteínas de los cereales con gluten son deficientes en aminoácido esenciales como la lisina y el triptófano, por lo que tienen bajo valor biológico y nutricional. El gluten representa el 80-90% del total de las proteínas del trigo. No es indispensable para el ser humano y desde el punto de vista de la nutrición, su exclusión de la alimentación no representa ningún problema, pudiendo ser fácilmente sustituido por otras proteínas animales o vegetales cuando es preciso realizar una dieta libre de gluten.

El consumo de cereales con gluten puede provocar el desarrollo de los denomminados trastornos relacionados con el gluten. Estos incluyen:

Asimismo, el gluten y ciertas bacterias intestinales son los dos factores más potentes que provocan un aumento de la permeabilidad intestinal, independientemente de la predisposición genética, es decir, tanto en celíacos como en no celíacos. Este aumento de la permeabilidad intestinal provoca que pasen a la sangre sustancias que no deberían pasar (toxinas, químicos, microorganismos y alimentos incompletamente digeridos) y que, dependiendo de la predisposición genética de la persona, puedan desarrollarse diversos trastornos de salud. Actualmente, existen evidencias de que la alteración de la permeabilidad intestinal está implicada en el desarrollo de enfermedades autoinmunes, cánceres, enfermedades del sistema nervioso, enfermedades inflamatorias, infecciones, alergias y asma.

El número de personas afectadas por los trastornos relacionados con el gluten está aumentando de manera constante. No obstante, debido al escaso conocimiento sobre estos trastornos entre los profesionales de la salud, que tiende a perpetuarse, y pese a que se ha incrementado el número de diagnósticos en comparación con años anteriores, en la actualidad prácticamente la totalidad de los casos reales continúa sin reconocer, sin diagnosticar y sin tratar. La mayor parte de los afectados solo presenta síntomas digestivos leves, intermitentes o incluso ausentes, probablemente debido al efecto opioide del gluten, que enmascara el daño intestinal, aunque sí desarrollan otros trastornos asociados que pueden afectar prácticamente a cualquier órgano. Tras un dilatado historial de variadas molestias de salud y un largo peregrinaje por multitud de consultas de diversos especialistas durante años, sin recibir un apoyo médico adecuado, la mayoría de las personas afectadas acaba recurriendo a la dieta sin gluten y al autodiagnóstico, mientras que otras muchas son personas que se han acostumbrado a vivir con un estado de mala salud crónica como si fuera normal.

Las harinas de cereales sin gluten solo son aptas para el consumo de las personas afectadas cuando están libres de contaminación cruzada con gluten (también denominada "trazas"), que puede ocurrir durante los diferentes pasos de la recolección y elaboración, tanto en la cosecha de los granos, el transporte, la molienda, el almacenamiento, el procesamiento, la manipulación o el cocinado.
Con ciertos cereales, como el arroz o la avena, se elaboran las denominadas "leches" vegetales, extrayendo el material vegetal en agua, separando el líquido y formulando el producto final, generalmente con adición de ingredientes para permitir su conservación y mejorar su sabor y sus propiedades nutricionales. Aunque se publicitan como saludables y sanas, a fecha de 2018 no se han realizado suficientes investigaciones para comprender las implicaciones nutricionales de su consumo a corto y largo plazo. 

Los consumidores interpretan errónamente que las "leches" vegetales son un sustituto directo de la leche de vaca, pero la mayoría de estas bebidas carecen del equilibrio nutricional de la leche de origen animal, son bajas en proteínas, grasas, calorías y hierro, y algunas tienen contenidos de proteínas y calcio extremadamente bajos. No son un adecuado sustituto de la leche materna, de las fórmulas infantiles ni de la leche de vaca en los primeros dos años de vida. En el caso de niños mayores de dos años que por razones médicas no pueden consumir leche, la recomendación es elegir bebidas fortificadas y que contenga al menos 6 gramos de proteína por cada 250 mililitros.

El empleo de leche de arroz como alternativa a la leche de vaca, debido a la gran diferencia de valor nutricional entre ambas, puede causar desnutrición, especialmente en niños pequeños. Se han documentado varios casos de Kwashiorkor en países occidentales, una forma de malnutrición proteico-energética típica de zonas de hambruna, como consecuencia del uso de leche de arroz como alimento de destete y en niños a dieta vegana basada en arroz.

Con el objetivo de no confundir al consumidor, desde 2013 en los países de la Unión Europea se ha prohibido el uso de la palabra "leche" para referirse a las bebidas de origen vegetal, y solo las leches de origen animal pueden denominarse así. La Administración de Alimentos y Medicamentos de Estados Unidos (FDA) denomina las alternativas lácteas basadas en plantas como "leche de imitación" y "productos lácteos de imitación" y las define como «aquellas comidas que tienen las características físicas, como sabor, cuerpo, textura o apariencia, de la leche o productos lácteos, pero no entran dentro de la definición de "leche" o "productos lácteos" y son nutricionalmente inferiores al producto imitado». En los establecimientos de venta al público, se exige que estén físicamente separados de la leche o los productos lácteos.

El consumo de arroz blanco (arroz descascarillado) puede causar una deficiencia en vitamina B1 o tiamina, causante, en ausencia de un suplemento dietético, del beriberi.

El consumo excesivo de maíz, que no ha pasado por el proceso de nixtamalización, puede llevar a una deficiencia de vitamina PP, causa de la pelagra.

La cosecha mundial de cereales ascendió a 2,07 miles de millones de toneladas (2010). Esto representa un promedio bruto de 345 kg per cápita al año (6 miles de millones de personas en total), promedio que se situó en 155 kg de cereales para el consumo humano.

Los cereales que se cultivan en España han adelantado en las tres últimas décadas etapas de crecimiento que desarrollan en primavera como consecuencia de los efectos del cambio global, que en la Península se han manifestado con un incremento de la temperatura media y una ligera disminución pero mayor intensidad de las precipitaciones. El avance en sus estados fenológicos más significativo ha sido registrado en el trigo y en la avena, cuyas fases de aparición de la hoja bandera y de floración se han adelantado una media de tres y un día por año respectivamente. Las variaciones fenológicas pueden llegar a tener un gran impacto sobre la producción final de cultivo.



</doc>
<doc id="19296" url="https://es.wikipedia.org/wiki?curid=19296" title="Chiroptera">
Chiroptera

Los quirópteros (Chiroptera), conocidos comúnmente como murciélagos, son un orden de mamíferos placentarios cuyas extremidades superiores se desarrollaron como alas. Con unas 1100 especies actuales, representan aproximadamente un 20 % de todas las especies de mamíferos, lo que los convierte, tras los roedores, en el segundo orden más diverso de esta clase. Están presentes en todos los continentes, excepto en la Antártida.

Son los únicos mamíferos capaces de volar, se han extendido por casi todo el mundo y han ocupado una gran variedad de nichos ecológicos diferentes. Desempeñan un papel ecológico vital como polinizadores, como controladores de plagas de insectos y pequeños vertebrados y también desarrollan un importante papel en la dispersión de semillas; muchas plantas tropicales dependen por completo de los murciélagos. Tienen las patas anteriores transformadas en alas y más de la mitad de especies conocidas se orientan y cazan por medio de la ecolocalización. Cerca de un 70 % de las especies son insectívoras y la mayor parte del resto frugívoras; algunas se alimentan de pequeños vertebrados como ranas, roedores, aves, peces, otros murciélagos o, como en el caso de los vampiros (subfamilia Desmodontinae), de sangre.

Su tamaño varía desde los 29-33 mm de longitud y 2 g de peso del murciélago moscardón ("Craseonycteris thonglongyai"), a los más de 1,5 m de envergadura y 1,2 kg de peso del zorro volador filipino ("Acerodon jubatus").

A causa de los hábitos nocturnos de la mayoría de sus especies y la ancestral incomprensión sobre cómo podían «ver» en la oscuridad, se les consideraba y todavía se les considera a menudo como habitantes siniestros de la noche, y con pocas excepciones (como en China, donde son símbolo de felicidad y provecho) en la mayor parte del mundo los murciélagos han causado temor entre los humanos a lo largo de la historia; iconos imprescindibles en el cine de terror, aparecen en multitud de mitos y leyendas y, aunque en realidad solo tres especies son hematófagas, a menudo se les asocia a los vampiros mitológicos.

El nombre científico del orden, Chiroptera, proviene de dos vocablos griegos, "cheir" (χειρ), mano, y "pteron" (πτερον), ala. Aunque pocos quirópteros lo son completamente, antiguamente predominaba la creencia de que eran ciegos, como demuestra el origen de su nombre común, «murciélago», que es una metátesis histórica de «murciégalo», formada por la expresión del castellano antiguo "mur cego" 'ratón ciego', la cual deriva a su vez de la unión de los términos latinos "mus", ratón, "caecŭlus" (diminutivo de "caecus"), ciego, y "alatus", alado. En otras lenguas su nombre también es una palabra compuesta que alude a su parecido con estos roedores y a su capacidad de volar, como "ratpenat" (rata alada) en catalán, o "Fledermaus" (ratón que vuela) en alemán, en euskera se llama "sagu zahar" (ratón viejo), los chinos les llaman "sein-shii" (ratón celeste) y los aztecas les llamaban "quimich-papalotl", de "quimich", ratón, y "papalotl", mariposa.

El término «vampiro», que se utiliza como nombre común de los murciélagos hematófagos, proviene del francés "vampire", y este del alemán "Vampir". Esta palabra tiene su origen en los míticos vampiros, término que generalmente se acepta que tiene su origen en la lengua eslava y la magiar, aunque su origen etimológico es incierto.

Los murciélagos, junto con las aves y los ya extintos pterosaurios, son los únicos animales vertebrados capaces de volar. Para conseguirlo, han desarrollado una serie de caracteres destinados a permitir el vuelo; excepto el pulgar, todos los dedos de las manos están particularmente alargados y sostienen una fina membrana de piel, flexible y elástica, que garantiza la sustentación. Esta membrana, denominada patagio, está formada por dos capas de piel que recubren una capa central de tejidos inervados, vasos sanguíneos y fibras musculares.

Su pelaje varía según las especies, pero generalmente son pardos, grises, amarillos, rojos y negros. Su tamaño varía desde los 29-33 mm de longitud y 2 g de peso del murciélago moscardón ("Craseonycteris thonglongyai"), el mamífero más pequeño existente en la actualidad, a los más de 1,5 m de longitud y 1,2 kg de peso del gran zorro volador filipino ("Acerodon jubatus"). Otros murciélagos de gran tamaño son el zorro volador de la India ("Pteropus giganteus"), el zorro volador de Livingston ("Pteropus livingstonii") o el gran zorro volador ("Pteropus vampyrus"), el murciélago de mayor envergadura. A pesar de su nombre, los megaquirópteros no son siempre mayores que los microquirópteros, pues algunas especies tan solo miden seis centímetros de longitud y son más pequeños que los microquirópteros de mayor tamaño. 

La orientación de sus extremidades inferiores es única entre los mamíferos; la unión de la cadera está girada 90° de forma que las piernas se proyectan de lado y la cara de las rodillas casi hacia atrás. En parte debido a esta rotación, su movimiento al andar es generalmente torpe y los diferencia de otros tetrápodos. Este diseño de las extremidades inferiores da apoyo al patagio en el vuelo y les permite colgarse boca abajo, posición en la que pasan gran parte de su vida, pendiendo de las ramas de los árboles o de los techos de las cuevas donde habitan. También tienen el pulgar de los pies libre y está dotado de una uña que les ayuda a colgarse y trepar. Cuando están colgados, su peso ejerce una tracción sobre los tendones que mantiene las garras en posición de enganche, lo que les permite permanecer colgados incluso dormidos y no gastar energía aunque permanezcan en esa posición durante grandes periodos de tiempo.

La principal adaptación esquelética de este orden de mamíferos es un pronunciado alargamiento del quiridio, especialmente en sus huesos más separados de la línea media. El radio es largo y la ulna reducida y está fusionada con éste, y el húmero es también muy largo y con una cabeza grande, articulada con la escápula de una forma especial, pues los movimientos de vuelo se dan sobre todo a nivel del hombro, permaneciendo rígido el resto de la extremidad; el codo tan solo permite movimientos de flexión-extensión y el carpo realiza la flexión-extensión y el despliegue de los dedos. El primer dedo es robusto y externo al ala y siempre está provisto de uña (en el caso de los megaquirópteros también el segundo dedo); el resto de metacarpianos y falanges están especialmente alargados para sostener el patagio, con las falanges relativamente cortas. Las extremidades posteriores son débiles y cuentan con cinco dedos provistos de uñas, que utilizan para colgarse sin necesidad de contracción muscular.

El esternón forma una quilla donde se insertan sus potentes músculos pectorales y la ancha y vigorosa clavícula suele estar fusionada con el esternón y la escápula. Las costillas son anchas, tienen poca movilidad y pueden fusionarse entre sí y con las vértebras, lo que hace que su respiración sea predominantemente diafragmática. La pelvis ha evolucionado para adaptarse al vuelo; se halla desplazada de forma que el acetábulo queda situado dorsalmente y la pata se sostiene hacia afuera y hacia arriba, y generalmente carecen de sínfisis pubiana; en los microquirópteros, el ilion y el sacro están fusionados hasta el nivel del acetábulo, con lo que carecen de movilidad en la unión iliosacral, y en el caso de los zorros voladores los dos huesos están completamente fusionados. Las esternebras son robustas y están fusionadas, y las espinas neurales, la propia columna vertebral y especialmente la región lumbar son cortas. La columna está compuesta por siete vértebras cervicales, once vértebras torácicas y hasta diez vértebras caudales; para sustentar sus poderosos músculos de vuelo, las vértebras torácicas están fuertemente unidas entre sí formando una columna rígida.

Su cabeza difiere considerablemente de una especie a otra. La cabeza de muchos murciélagos recuerda la de otros animales como pueden ser los ratones, pero tienen estructuras diferenciales en los quirópteros. Muchos tienen láminas nasales u otras estructuras en la cara, que sirven para emitir o potenciar los ultrasonidos. Las orejas, que en muchas especies son de gran tamaño, a menudo están dotadas de surcos o arrugas, además del trago, un lóbulo de piel que mejora su capacidad de ecolocalización. 

Los microquirópteros tienen una visión en blanco y negro, mientras que los megaquirópteros ven en color. Aunque los ojos de la mayoría de los microquirópteros son pequeños y están poco desarrollados, con una baja agudeza visual, no se puede decir que sean ciegos. Utilizan la vista como ayuda en la navegación, especialmente en distancias largas, a las que no alcanza la ecolocalización. Según investigaciones recientes, algunas especies también perciben la luz ultravioleta reflejada por algunas flores, lo que les ayudaría a encontrar néctar. Algunos disponen de un sentido de magnetorrecepción, lo que les permite orientarse utilizando el campo magnético terrestre, de manera similar a las aves migratorias y otros animales, y les ayuda a orientarse durante sus vuelos nocturnos. Al carecer de ecolocalización, los ojos de los megaquirópteros están más desarrollados que los de los microquirópteros, y emplean el olfato y la vista para orientarse y localizar su alimento; su capacidad para captar la luz se intensifica por numerosas proyecciones de los bastoncillos de la retina.

Tienen generalmente entre 32 y 38 dientes, de los cuales están especialmente desarrollados los caninos. La evolución de diferentes modos de alimentación ha desarrollado múltiples configuraciones dentales, y en este orden de mamíferos se conocen unas 50 fórmulas dentales diferentes; el vampiro común ("Desmodus rotundus"), con veinte dientes, es una de las especies de quiróptero con menor número. Los dientes de los microquirópteros son similares a los de los animales insectívoros; están muy afilados con el fin de penetrar el duro exoesqueleto de los insectos o la piel de la fruta. Los de los megaquirópteros están adaptados para morder la dura piel de algunos frutos.

La especie "Anoura fistulata" tiene la lengua más larga en relación a la medida corporal de todos los mamíferos. Con su lengua larga y estrecha puede llegar al fondo de muchas flores con receptáculo cónico y alargado, y le ayuda a polinizar y alimentarse. Cuando retracta la lengua, se enrolla dentro su caja torácica.

Los quirópteros son un ejemplo de la gran variedad de posibilidades de desarrollo que pueden tener las patas de los tetrápodos. Excepto en el pulgar, todas las falanges de los dedos de sus patas anteriores están especialmente alargadas para sostener una extensa y fina membrana de piel, flexible y elástica, que recibe el nombre de patagio y que le permite la sustentación en el aire. El patagio está formado por una capa central de tejidos plagados de nervios, vasos sanguíneos y fibras musculares, recubierta por ambos lados con capas de piel. El patagio se divide en propatagio (la parte que va desde el cuello hasta el primer dedo), dactilopatagio (entre los dedos), plagiopatagio (entre el último dedo y las patas posteriores) y uropatagio (membrana caudal o interfemoral, que une ambas extremidades posteriores entre sí, incluyendo la cola en todo o en parte). Dependiendo de las especies y su estilo de vuelo, pueden tener un uropatagio muy largo, ser más reducido o incluso carecer de él; del mismo modo algunas como "Anoura" spp. y "Sturnira" spp. carecen de uropatagio evidente y los géneros "Desmodus", "Diphylla" y "Diaemus" carecen de cola pero tienen uropatagio, aunque muy estrecho.

Los huesos de los dedos son mucho más flexibles que los de otros mamíferos. Una de las razones es que el cartílago carece de calcio y otros minerales en su extremo, lo que les permite una gran torsión sin romperse. La sección de los huesos de los dedos es aplanada, en lugar de circular como por ejemplo en los humanos, lo que los hace todavía más flexibles. La piel de las membranas alares es muy elástica y se puede estirar mucho más de lo que es habitual en los mamíferos.

Como tienen alas mucho más delgadas que las de las aves, siendo más similares por evolución convergente a la de los pterosaurios, pueden maniobrar de una manera más rápida y precisa, aunque también son más delicadas y se rasgan con facilidad; sin embargo, el tejido de la membrana se repone en poco tiempo, por lo que estos pequeños rasgones pueden curarse con rapidez. La superficie de las alas está dotada de receptores sensibles al tacto en forma de pequeños bultos llamados células de Merkel, presentes en la mayoría de mamíferos pero que el caso de los murciélagos son diferentes, pues cada bulto tiene un pelo diminuto en el centro, lo que la hace aún más sensible y le permite detectar y recoger información sobre el aire que fluye sobre sus alas y así poder cambiar su forma para volar con mayor eficacia. Muchas especies, como "Myotis lucifugus", aprovechan esta destreza para volar cerca de la superficie del agua y beber mientras vuelan, y otras, como el zorro volador (género "Pteropus") o los megaquirópteros, rozan con su cuerpo la superficie del agua y toman tierra en los alrededores para lamer el agua adherida a la piel de su pecho. La membrana de las especies que utilizan las alas para cazar a sus presas tiene un tipo adicional de célula receptora, sensible al estiramiento de la membrana. Estas células se concentran en las partes de la membrana en que los insectos impactan con el ala cuando los murciélagos los capturan.

Los murciélagos piscívoros tienen un uropatagio poco desarrollado, para minimizar la resistencia aerodinámica y mejorar su estabilidad durante el vuelo raso. Los que se alimentan de insectos voladores tienen alas largas y estrechas que les permiten volar a más de 50 km/h. Cuando se encuentran grandes concentraciones de insectos, a veces solo tienen que volar con la boca abierta para capturar sus presas, de forma similar a como se alimentan de kril los cetáceos. En cambio, los que comen insectos situados en la corteza o las hojas de los árboles tienen unas alas de gran superficie que les permiten un vuelo lento y suave y volar entre la vegetación densa.
Sus alas también les sirven como protección cuando el animal está en reposo, además de como regulador térmico; aíslan el cuerpo del animal del ambiente exterior para conservar calor (a lo que también contribuye que la sangre circula por ellas), pero también sirven para reducir la temperatura del animal mientras vuela (esta sangre que circula por los capilares de sus finas alas se enfría con el movimiento de las mismas).

No todos los murciélagos tienen en el vuelo su único modo de locomoción. Algunos, como los mistacínidos de Nueva Zelanda tienen la capacidad de esconder sus alas bajo una resistente membrana, una adaptación que les permite desplazarse y alimentarse en tierra, e incluso escarbar madrigueras en el suelo.

Existía cierto consenso científico sobre que los murciélagos evolucionaron a partir de antepasados planeadores del tipo de las ardillas voladoras. Sin embargo, el vuelo de los murciélagos es un sistema funcional muy complejo desde una perspectiva morfológica, fisiológica y aerodinámica, y la transición desde el planeo al vuelo requiere una importante serie de adaptaciones, y estudios recientes apuntan a que la evolución de los murciélagos no tiene relación con los mamíferos planeadores.

Investigando la habilidad de los murciélagos para volar y capturar insectos en la oscuridad, Lazzaro Spallanzani descubrió en 1793 que se desorientaban si no podían oír, pero que evitaban obstáculos cuando estaban cegados. En 1920 el fisiólogo inglés Hartridge apuntaba la posibilidad de que localizaran y capturaran a sus presas con el oído. Ya en 1938, con el desarrollo de un micrófono que captaba las altas frecuencias, Donald Griffin descubrió que los murciélagos emitían ultrasonidos.

Los murciélagos, al igual que los delfines o los cachalotes, utilizan la ecolocalización, un sistema de percepción que consiste en la emisión de sonidos para producir ecos que a su retorno se transmiten al cerebro a través del sistema nervioso auditivo y les ayuda a orientarse, detectar obstáculos, localizar presas o con motivos sociales; se trata de una especie de «sonar» biológico. La utilizan fundamentalmente para la captura de sus presas y les proporciona información sobre su medida, velocidad y dirección.

Los microquirópteros emiten ultrasonidos mediante contracciones de la laringe, que es proporcionalmente más ancha que en otros mamíferos. Estos sonidos pueden variar en frecuencia, ritmo, duración e intensidad. Son emitidos por la boca o la nariz y son amplificados por unas «láminas nasales». Las distintas especies emiten frecuencias diferentes. Los humanos pueden percibir hasta 20 kHz, pero los murciélagos emiten desde 15 hasta 200 kHz. Las frecuencias pueden ser constantes (no cambian durante la duración de la señal) o moduladas (varían en mayor o menor medida). Los gritos pueden acabarse repentinamente o gradualmente, según la especie. Los megadermátidos, filostómidos, nictéridos y algunos vespertiliónidos utilizan intensidades débiles, mientras que el género "Nyctalus" tiene llamamientos muy potentes que se pueden sentir desde una distancia muy superior. En ocasiones dejan de emitir sonidos cuando se encuentran en lugares familiares o que conocen bien, quizás para evitar que determinados predadores los descubran.

Durante la búsqueda de presas emiten de media 4-12 señales de búsqueda por segundo en intervalos irregulares; cuando localizan una posible presa, durante la persecución el ritmo de las señales aumenta significativamente (hasta 40-50 por segundo), y justo antes de capturarlas emiten un «zumbido final» consistente en una secuencia de 10-15 pulsos cortos separados por un intervalo mínimo. La secuencia completa de localización, persecución y «zumbido final» dura menos de 1-2 segundos.

Utilizan las orejas para escuchar su propio eco, y las de algunos grupos, como por ejemplo los rinolófidos, pueden moverse independientemente la una de la otra. Calculan la distancia de la presa por la diferencia de tiempo entre la emisión del sonido y la recepción del eco, y la dirección la deducen por la diferencia entre la llegada del eco al oído derecho y al izquierdo. El pabellón auricular de los quirópteros está adaptado al tipo de vuelo de cada especie; cuanto más rápido vuelan, más cortas son las orejas. El pabellón auricular de las dos especies del género "Mormoops" es uno de los más sofisticados entre los mamíferos. 

Con muy pocas excepciones, como el género "Rousettus" que solo vive en cuevas y que es el único del suborden que produce auténticos sonidos de ecolocalización para poder desplazarse en su interior en la más completa oscuridad, o como "Rousettus aegyptiacus" que cuenta con una forma rudimentaria de la misma, los megaquirópteros (que se alimentan de jugo de frutas, néctar y polen) carecen de esta capacidad, y utilizan la vista y el olfato para orientarse.

Existen estudios que muestran que en vuelos a través obstáculos consistentes en alambres de diferentes diámetros estirados verticalmente, los murciélagos eran capaces de evitar alambres de un diámetro de 0,065 mm, y que incluso en los de 0,05 mm el porcentaje de vuelos sin colisiones era muy alto. Sin embargo, a pesar de esta precisión y de que la ecolocalización permite a los quirópteros desplazarse y cazar en situaciones de poca luz o incluso en total oscuridad, también supone importantes desventajas con respecto a la percepción visual, como son el coste energético para su producción, el tiempo de respuesta en la recepción del eco frente a la percepción continua de imágenes de la visión, un campo sonoro limitado comparado con el campo visual de los mamíferos, su limitado alcance (generalmente menor de 20 m y con un máximo de 50-60 m) o la baja resolución de las «imágenes» que produce.

La ecolocalización es un sistema que se basa en el análisis de los ecos; esto implica que los murciélagos posean adaptaciones tanto para emitir (en la laringe), como para recibir señales (en el sistema auditivo).

Adaptaciones en la laringe: En los murciélagos este órgano es una estructura muy rígida, ya que los cartílagos aritenoideos están fusionados en sus extremos superiores creando una sola estructura. La musculatura lateral del cricoaritenoideo puede rotar los cartílagos aritenoideos a lo largo de sus ejes hacia la línea media. La intensidad de los pulsos de ecolocalización que emite un murciélago puede alcanzar los 100 dB, y se originan mediante la acumulación de aire bajo la glotis, el cual crea una presión subglótica que incrementa la fuerza de salida del aire al expirar. Para minimizar el gasto de energía, el murciélago emite la señal ultrasónica en la espiración que acompaña al batido de las alas.

Adaptaciones en el sistema auditivo: en los quirópteros la audición funciona y está estructurada de una manera muy similar a la de los demás mamíferos; sin embargo, existen modificaciones en la forma y tamaño de la oreja y del trago que están relacionadas con la capacidad de los murciélagos de percibir los ecos de sus presas o de objetos. También destaca la alta sensibilidad del órgano de Corti en el oído interno, que recibe el estímulo de los ecos y manda la señal al cerebro para su posterior interpretación dependiendo de las condiciones ambientales y del tipo de sonido emitido.

El conocido como "buzz" de caza es un grupo de pulsos de ecolocalización cortos, de rápida emisión, que son emitidos por los murciélagos antes de hacer contacto físico con su presa. Al detectarla, el murciélago se aproxima a ella, conforme se va acercando disminuye el intervalo de tiempo de la emisión de pulsos con el fin de reducir el tiempo de retorno del eco. De esta manera el murciélago recibe información más detallada de la trayectoria de ésta, para poder seguirla o interceptarla. Los murciélagos que usan señales de FM-FC al localizar una presa pueden convertir gradualmente los pulsos por completo a frecuencia constante (FC) para recibir información detallada sobre ésta. Al principio, estos barridos (cambios) son de amplio intervalo de frecuencia o ancho de banda, pero a medida en que el murciélago se acerca a la presa, los barridos de FM (frecuencia modulada) se hacen más cortos, ya que el murciélago cada vez va obteniendo información más detallada de ésta.

Ocupan nichos en todos los hábitats, excepto en las regiones polares, los océanos o las montañas más altas. La mayor parte son insectívoros, pero tienen una amplísima variedad de dietas; algunos se especializan en una gama de alimentos relativamente estrecha y otros son omnívoros. Casi todos los murciélagos comen de noche y descansan de día, en sitios muy variados según las especies, como cuevas, edificaciones, agujeros, grietas o al aire libre. Algunas especies son solitarias, pero otras, como el murciélago cola de ratón ("Tadarida brasiliensis"), forman colonias de 20 y hasta 50 millones de individuos en algunas cuevas de Texas y el noroeste de Estados Unidos, que consumen entre 45 y 250 toneladas de insectos cada noche. Los murciélagos son vivíparos, y muchas especies han desarrollado una compleja fisiología reproductiva.

Suelen alcanzar la madurez sexual a los doce meses, y los sistemas de apareamiento varían de una especie a otra. Algunos murciélagos tienen un comportamiento promiscuo y se unen en grupos numerosos en uno o varios árboles y copulan con varios compañeros cercanos. Muchos microquirópteros neotropicales mantienen y defienden pequeños «harenes» de hembras. Aunque la mayoría de las especies son poliginias o promiscuas, algunas, como "Vampyrum spectrum", "Lavia frons", "Hipposideros galeritus", "Nycteris hispida" y varias del género "Kerivoula", son monógamas y, en estos casos, el macho, la hembra y su descendencia viven juntos en grupos familiares y los machos pueden colaborar en la protección y alimentación de los jóvenes. El comportamiento durante el cortejo es complejo en algunas especies, mientras en otras puede ser casi inexistente, llegando al caso de machos de algunas especies que se aparean con hembras en estado de hibernación que apenas reaccionan ante la cópula.

Un gran número de especies se reproduce estacionalmente; las de zonas templadas a menudo lo hacen antes de iniciar la hibernación. Todas las especies que no son criadoras estacionales se dan en la zona tropical, donde los recursos son a menudo relativamente constantes todo el año. La función de la cría estacional es coordinar la reproducción con la disponibilidad de recursos que permita la supervivencia de los recién nacidos. Los murciélagos vampiro pueden nacer en cualquier época del año.

Los murciélagos son vivíparos, por lo general con un desarrollo embrionario relativamente lento (3-6 meses), la duración de la gestación puede variar según la disponibilidad de alimentos y el clima, y de unas especies a otras puede variar desde los cuarenta días hasta los diez meses. Muchas especies han desarrollado una compleja fisiología reproductiva, como la ovulación retrasada, la implantación diferida, el almacenaje de esperma, el retraso de la fertilización o la diapausa embrionaria. La ovulación retrasada se da fundamentalmente en los murciélagos de zonas templadas, e implica que se apareen a finales del otoño y que la hembra almacene el semen durante todo el invierno; la ovulación se produce en primavera para que las crías nazcan en verano, cuando hay muchos insectos disponibles. En el caso de la implantación diferida el embrión empieza a desarrollarse inmediatamente pero se detiene poco después, esperando a que las condiciones vuelvan a ser favorables; este tipo de embriogénesis se produce en los megaquirópteros africanos y en el género "Miniopterus" y en otras especies, como "Macrotus californicus", el óvulo se implanta pero el feto no se desarrolla hasta la primavera. También pueden alargar la gestación para evitar el mal tiempo; en zonas tropicales, lo pueden hacer para esperar una época mejor en términos meteorológicos o de disponibilidad de alimento. La migración y la hibernación también limitan la temporada óptima de apareamiento.
Las hembras generalmente dan a luz a una cría por camada (aunque a veces pueden ser dos) y una camada por año, sin embargo algunas especies del género "Lasiurus", como el murciélago boreal rojizo ("L. borealis") de América del Norte, pueden llegar a tener tres o cuatro crías. En el norte de Europa los pipistrelos tienen una cría, pero en zonas más meridionales suelen tener dos, y teniendo en cuenta que los gemelos son más frecuentes entre ejemplares en cautividad, bien alimentados, que entre las mismas especies en estado silvestre, probablemente una mejor nutrición influya en el número de nacimientos.

Al nacer ya tienen entre el 10 y el 30 % del peso de sus madres, que necesitan de un gran aporte energético para producir leche para sus crías. Los recién nacidos son completamente dependientes de sus madres tanto para su protección como para su alimento, incluso hasta en el caso de los pteropódidos, cuyas crías ya nacen con piel peluda y con los ojos abiertos; los microquirópteros tienden a ser más altriciales al nacer. En algunas especies las crías nacen estando la madre colgada patas arriba y en otras vuelve la cabeza hacia arriba y recoge la cría con la membrana interfemoral (membrana cutánea que se extiende entre los miembros inferiores y la cola). En la mayoría de las especies las hembras disponen de dos mamas en el pecho, en algunas disponen de otro par de falsas mamas inguinales que sirven para que la cría se agarre y en otras, como en "Lasiurus", hay cuatro mamas funcionales; la lactancia puede empezar a los pocos minutos de nacer.

Las especies de zonas templadas forman generalmente colonias de maternidad, una especie de guarderías integradas casi exclusivamente por hembras adultas; estos hacinamientos reducen la pérdida de calor y el gasto energético de cada individuo. La mayoría de los murciélagos, sobre todos los insectívoros, que necesitan de la máxima maniobrabilidad posible, dejan a sus crías en las perchas mientras se alimentan y generalmente solo las llevan encima cuando cambian de percha. Los jóvenes de especies pequeñas se desarrollan con rapidez y vuelan a los 20 días, en cambio los zorros voladores, de mayor tamaño, pueden tardar tres meses en iniciar su primer vuelo; los vampiros son los que se desarrollan más lentamente, y se amamantan hasta los nueve meses. Aunque generalmente alcanzan su peso corporal máximo pocas semanas después del destete, algunas especies pueden tardar varios años en conseguirlo.

La longevidad media de los murciélagos suele ser de cuatro o cinco años, aunque a menudo alcanzan diez y hasta veinticinco años, y algunas especies pueden llegar a vivir treinta años de edad. La longevidad de los mamíferos generalmente está en relación con su tamaño, por lo que la vida de los murciélagos es sorprendentemente alta en proporción a su tamaño y por lo general es unas tres veces y media más larga que la de otros mamíferos de un tamaño similar.

Se encuentran por todo el mundo, excepto las regiones polares, las montañas más altas, las islas particularmente aisladas (principalmente del Pacífico oriental), los océanos o el centro de los desiertos más extensos; ocupan nichos en todos los hábitats y su capacidad de vuelo les permite colonizar nuevas zonas, si disponen de perchas de descanso y alimento. En Nueva Zelanda, Hawái, las Azores y muchas islas oceánicas, son los únicos mamíferos indígenas. Junto con los roedores, son el único taxón de euterios que colonizó el continente australiano sin contribución de los humanos, donde están representados por seis familias. Llegaron probablemente de Asia, y solo están presentes en el registro fósil desde hace quince millones de años. Aunque el 7 % de las especies de murciélagos del mundo viven en Australia, en este continente solo hay dos géneros endémicos.

Algunas especies son migratorias y aunque generalmente no suelen migrar grandes distancias, pueden llegar a recorrer trayectos tan largos como el que separa el norte de Canadá de México.

Han colonizado una gran variedad de hábitats. Viven en medios subterráneos, en grietas y fisuras de las paredes rocosas, entre la hojarasca, tras la corteza de los árboles o en sus cavidades. En cuanto a las construcciones humanas, los murciélagos también viven en sótanos, bodegas, puentes y construcciones militares.

Los hábitos alimenticios de los quirópteros son casi tan variados como los de todos los mamíferos en conjunto, y esta diversidad dietética es responsable en gran medida de la diversidad morfológica, fisiológica y ecológica que se aprecia en estos animales. Se alimentan de insectos y otros artrópodos, fruta, polen, néctar, flores, hojas, carroña, sangre, mamíferos, peces, reptiles, anfibios y aves.

Sus preferencias alimentarias varían mucho entre los dos subórdenes de quirópteros existentes y entre las distintas familias. Los megaquirópteros solo comen fruta, polen y néctar aunque en ocasiones complementan su dieta con carroña y pequeñas aves o peces, pero entre los microquirópteros existe una gran variedad de dietas. El peculiar murciélago neozelandés "Mystacina tuberculata" es una especie omnívora, como "Phyllostomus" o algunas especies neotropicales. La familia Phyllostomidae tiene una extensa variedad en hábitos de alimentación y ecología y cuenta por sí sola prácticamente con todas las dietas explotadas por los demás quirópteros, e incluye también a las únicas tres especies hematófagas (que se alimentan de sangre).

Aproximadamente dos tercios de las especies actuales, incluidas todas las de las latitudes templadas y frías, son únicamente insectívoras. La existencia de una gran cantidad de insectos los convierten en un alimento abundante y variado. Dados sus hábitos mayoritariamente nocturnos, cuando los pájaros insectívoros están inactivos los murciélagos no tienen competencia para cazar la gran cantidad de insectos que salen tras el ocaso. Casi todas las familias de insectos pueden ser sus presas y, aunque en mucha menor medida, también se alimentan otros tipos de artrópodos, como arañas, opiliones, crustáceos, escorpiones o ciempiés.

La gran mayoría de quirópteros insectívoros son de pequeño tamaño y capturan sus presas en vuelo; algunos utilizan las alas o las patas y muchos tienen una membrana entre sus extremidades inferiores (uropatagio) que utilizan para capturar los insectos y que a veces tiene forma de bolsa. Para capturar a sus presas durante el vuelo se valen principalmente de la ecolocalización, y para contrarrestar esta habilidad algunos grupos de polillas como los árctidos producen señales ultrasónicas que les advierten que están protegidas químicamente, o los noctúidos tienen un órgano en el oído que responde a la señal emitida por los murciélagos y que hace que los músculos de vuelo de la polilla se contraigan de forma errática, lo que hace que ejecute maniobras de evasión al azar, como dejarse caer o ejecutar una pirueta que despista a los murciélagos y dificulta su captura.

Los murciélagos no cazan sus presas únicamente en el aire, sino que a veces también lo hacen en tierra. Algunos insectívoros, como el murciélago grande de herradura ("Rhinolophus ferrumequinum"), preparan emboscadas a sus presas, esperándolos en un lugar fijo para lanzarse a su persecución. El falso vampiro australiano ("Macroderma gigas") captura grandes insectos y pequeños vertebrados atacándolos desde arriba y capturándolos con los pies para llevarlos después a lo alto de la rama de un árbol para comérselos, de manera similar a como lo hacen las aves de presa.


Aproximadamente el 25% de las especies de quirópteros son vegetarianas, y se reparten por las zonas tropicales y ecuatoriales del planeta. Su dieta se puede componer de frutos, de néctar o, en mucha menor medida, de hojas. El murciélago frugívoro "Eidolon helvum" se alimenta de treinta y cuatro géneros de frutos, diez géneros de flores y cuatro especies de hojas. "Hypsignathus monstrosus" se alimenta principalmente de jugos de frutas, aunque complementa su dieta con carroña y aves.

Sus preferencias se inclinan generalmente hacia frutas carnosas y dulces, pero no particularmente olorosas o de colores llamativos. Arrancan la fruta de los árboles con sus dientes y vuelan hacia una rama o saliente con la fruta en la boca y allí la consumen de un modo específico; comen hasta satisfacer su hambre y el resto de la fruta, las semillas y la pulpa caen a tierra y estas semillas echan raíces y se convierten en nuevos árboles frutales. Más de ciento cincuenta tipos de plantas dependen de los murciélagos para reproducirse.

En torno al 5 % son polinívoras (se alimentan de polen); estas especies tienen una musculatura masticatoria y una mandíbula atrofiadas en comparación con el resto de murciélagos, una nariz larga y puntiaguda (que les permite introducirla dentro de las flores con forma de cáliz) y una lengua larga y rasposa con la que lamen rápidamente el néctar. El olfato y el gusto están bien desarrollados en estos murciélagos. Como en el caso de los insectos, las plantas que son polinizadas por murciélagos han coevolucionado con ellos; algunas plantas tienen tallos resistentes para no romperse cuando se apoyan los murciélagos, mientras que otros quirópteros son más delicados y toman el néctar en pleno vuelo, como los colibrís.


Pocas especies han sido confirmadas como carnívoras estrictas. El término carnívoro se aplica a los murciélagos en los que los pequeños vertebrados (excluidos los peces) forman una parte significativa de su dieta, aunque parece haber especies que son carnívoras exclusivas, oportunistas u ocasionales. Así, "Vampyrum spectrum", "Trachops cirrhosus" o los megadermátidos se alimentan de artrópodos, otros murciélagos, pequeños roedores, aves, lagartos y ranas.

Algunos murciélagos son predominantemente piscívoros, aunque, como en el caso de los carnívoros, no suele ser su alimento exclusivo. Entre las pocas especies piscívoras existentes, como "Myotis vivesi" o "Myotis capaccinii", el murciélago pescador ("Noctilio leporinus") aunque también se alimenta de insectos y crustáceos, es uno de los murciélagos mejor adaptados para una alimentación a base de peces. Esta especie, la mayor de la familia Noctilionidae, cuenta con adaptaciones anatómicas como unas patas enormemente alargadas, garras y el espolón de sus miembros traseros, que le dotan de una gran eficacia en la captura de los peces que se encuentran cerca de la superficie del agua; con un sistema de ecolocalización extremadamente sensible, estos murciélagos detectan a sus presas por medio de las turbulencias producidas por los cardúmenes de peces en la superficie del agua. Aunque la mayoría captura peces de agua dulce, algunas especies, como "Pizonyx vivesi", se alimentan de crustáceos y peces marinos, llegando a experimentar adaptaciones que les permiten beber agua salada, algo muy poco común entre los mamíferos.


A pesar de la extensa visión popular de los murciélagos como animales que se alimentan de sangre, en realidad solo existen tres especies hematófagas, todas originarias de América e incluidas en la subfamilia Desmodontinae. Los murciélagos hematófagos se conocen con el nombre de vampiros.
El vampiro común ("Desmodus rotundus") es el más extendido; se alimenta de la sangre de ganado, perros, sapos, tapires, guanacos e incluso focas, mientras que el vampiro de patas peludas ("Diphylla ecaudata") se alimenta de la sangre de aves. El vampiro de alas blancas ("Diaemus youngi"), la más rara de estas especies, también se alimenta de la sangre de aves y la mayoría de la información que se tiene de ella ha sido obtenida a partir de ejemplares encontrados en gallineros.

Cuando se pone el sol, los vampiros salen en grupos de entre dos y seis animales. Una vez localizada su presa, como un mamífero dormido, aterriza sobre una zona desprovista de pelo, o bien cerca de su presa y se dirige a ella por tierra; elige un lugar conveniente para morder utilizando un sensor de calor situado en su nariz con el que localiza un área donde la sangre fluye cerca de la piel. No chupan o absorben la sangre, sino que la beben a lengüetadas, y su saliva tiene una función clave en el proceso de alimentarse de la herida pues contiene varios compuestos que prolongan el desangrado, como anticoagulantes que inhiben la coagulación de la sangre y compuestos que previenen el estrangulamiento de los vasos sanguíneos próximos a la herida.

La pérdida de sangre provocada por sus mordeduras es relativamente pequeña (unos 15-20 ml), por lo que el daño producido a las presas es también pequeño. El mayor riesgo en sus presas a causa de estas mordeduras está asociado a su exposición a infecciones secundarias, parásitos y el contagio de enfermedades transmitidas por virus como la rabia. La rabia se produce de forma natural en muchos animales salvajes, pero es mucho más frecuente en mofetas o zorros que en murciélagos, y dado que las mordeduras de vampiros a humanos son muy poco frecuentes, el contagio de esta enfermedad a los humanos es muy rara; aun así, teniendo en cuenta que los vampiros pueden ser portadores de este virus, deben ser manejados con precaución.

Los murciélagos que viven en las zonas templadas sufren en invierno, no solo por las bajas temperaturas, sino también por la escasez de sus presas (principalmente insectos). La mayoría no migran, sino que duermen hasta la primavera en un estado denominado hibernación. En este estado se producen una serie de cambios fisiológicos que permiten un descenso de la temperatura corporal y una disminución general de las funciones metabólicas para prolongar la duración de las reservas de energía; su duración es más larga cuanto más cerca estén de los polos (las más extremas duran hasta seis meses, mientras que las más suaves son cortas e intermitentes). Muchos otros mamíferos hibernan, como los osos (carnívoros), las ardillas y lirones (roedores) o los erizos (erinaceomorfos), pero ninguno en el grado de muchas especies de murciélagos, pues la mayoría de los mamíferos hibernadores disminuyen menos de 10 °C su temperatura corporal normal en activo, mientras que la de algunos murciélagos baja de los 0 °C (hasta -5 °C en el caso del murciélago boreal rojizo).

Durante el otoño o a finales del verano ingieren grandes cantidades de alimento para acumular reservas y aumentan rápidamente de peso, sobre todo en forma de grasa subcutánea que queda almacenada en los hombros, el cuello y los flancos, donde forma unos bultos visibles, y que puede representar hasta un tercio de su masa corporal; si no acumulan bastantes reservas, dado que no pueden volver a alimentarse hasta la primavera, podrían morir de hambre. El cambio de costumbres de verano a inverno es súbito, y está provocado por un factor o una combinación de factores como la disponibilidad de comida, la temperatura externa o la duración del día. Las funciones vitales van disminuyendo y baja el metabolismo; el corazón late tan solo diez veces por minuto, en contraste con las 600 pulsaciones durante la caza estival; la respiración es tan tenue que resulta casi imperceptible y tan solo supone el 1 % de la respiración en fase de actividad, llegando a permanecer varios minutos sin respirar; la temperatura corporal cae y se iguala con la temperatura ambiental (0-10 °C). Cada especie tiene una temperatura preferida, y cuanto más baja sea la temperatura corporal, más durarán sus reservas de energía; sin embargo deben evitar quedar congelados, y las bajas temperaturas pueden ser peligrosas y tienen efectos negativos como una menor resistencia a las enfermedades.

Los murciélagos en este estado despiertan periódicamente, para orinar y defecar a fin de eliminar el exceso de agua y productos de desecho, tóxicos para los tejidos, y restablecer el equilibrio fisiológico (homeostasis), o para trasladarse a otro lugar. Algunos murciélagos despiertan cada diez días, mientras que otros pueden tardar noventa; los períodos de adormecimiento suelen ser más largos al principio de la hibernación. Escogen lugares como cuevas, minas, oquedades de árboles, grietas o incluso en lugares expuestos; es importante que escojan lugares con humedad alta (en general por encima del 90 %), a fin de evitar el exceso de pérdidas por evaporación, que les obligaría a despertarse con más frecuencia para beber y para evitar que se le sequen las alas (es particularmente importante para los murciélagos de herradura, que cuelgan en lugares expuestos envueltos en sus alas). Algunas especies que duermen en cuevas lo hacen solos o en pequeños grupos, pero otras forman grupos de decenas y centenares de miles, o incluso de millones de individuos, con concentraciones de más de tres mil ejemplares por metro cuadrado. En algunos casos es posible encontrar numerosas especies compartiendo la misma cueva de manera permanente o temporal.

Los murciélagos que hibernan pueden aletargarse también en cualquier momento del verano, especialmente en climas fríos, cuando el alimento escasea. Este letargo estival no es tan extremo como la hibernación; también acumulan reservas alimentarias cuando la comida es abundante y entran en cierto letargo cuando escasea. Su temperatura corporal ronda los 30 °C, mucho mayor que durante la hibernación.

En general los murciélagos tienen pocos depredadores naturales, que se limitan a algunas aves rapaces, mamíferos carnívoros, serpientes y lagartos de gran tamaño.

Sobre todo en los trópicos, las boas y las culebras atacan a los zorros voladores que cuelgan de las ramas; estas serpientes suben a los árboles y los capturan por sorpresa mientras descansan, sobre todo a las crías. Cuando los ataques de estos reptiles son reiterados, pueden causar un gran impacto en algunas poblaciones al dejarlas sin jóvenes. En cambio las serpientes que cazan en las cuevas no parecen tener a los murciélagos entre sus presas habituales, y solo las atacan esporádicamente. Algunos lagartos tropicales de gran tamaño también comen murciélagos.

El milano murcielaguero es un ave de presa que caza murciélagos, atacándolos cuando salen durante el crepúsculo. El cernícalo común, el alcotán europeo y el halcón peregrino también los cazan, pero el mayor peligro aviar para los quirópteros son las rapaces nocturnas, como las lechuzas y los búhos, que esperan en el exterior de las cuevas a que llegue el anochecer para atrapar a los murciélagos que salen. Los búhos les cortan las alas antes de comérselos. Algunas aves han aprendido a adaptarse a las costumbres de los murciélagos, atacándolos cuando están buscando insectos. Sin embargo, en la gran mayoría de casos solo representan el 0,1-0,2 % de las presas de las aves rapaces.

Algunos carnívoros oportunistas, como el mapache boreal, las mofetas, el gato montés y los mustélidos cazan murciélagos activamente, mientras que el tejón europeo y el zorro solo se comen crías que se caen del techo de una caverna o que han optado por perchas a una altura demasiado baja; sin embargo, los quirópteros son presas poco habituales para estos animales. También algunos roedores, como el ratón de campo, se alimentan ocasionalmente de murciélagos, así como otros animales como las arañas migalomorfas, algunos peces carnívoros y algunos grandes anfibios como la rana toro.

Sin embargo las especies introducidas por los humanos sí que pueden diezmar sus poblaciones. Así, a causa de la introducción de la serpiente arborícola parda ("Boiga irregularis") en Guam, entre 1984 y 1988, todas las crías de algunas especies de murciélago fueron devoradas antes de llegar a adultos; una situación similar se produjo con la introducción de la serpiente lobo de la India ("Lycodon aulicus capucinus") en la Isla de Navidad. El gato, otra especie introducida, es uno de los predadores más peligrosos para los murciélagos; algunos gatos se vuelven salvajes y se especializan en la caza de murciélagos, pudiendo exterminar una colonia fácilmente accesible y no muy grande en cuestión de días. Algunos murciélagos, para defenderse, luchan o se hacen los muertos.

Las poblaciones de murciélagos están descendiendo con rapidez en todo el mundo, y varias especies se han extinguido recientemente. De las 1150 especies que se relacionan en la Lista Roja de la UICN algo más de la mitad figuran como especie bajo preocupación menor y de unas doscientas no se dispone de datos para su clasificación, pero setenta y siete figuran como especie casi amenazada, noventa y nueve como vulnerables, cincuenta y tres se encuentran en peligro de extinción, veinticinco en peligro crítico y cinco figuran como ya extintas ("Desmodus draculae" y cuatro miembros del género "Pteropus": "P. brunneus", "P. pilosus", "P. subniger" y "P. tokudae").
El síndrome de la nariz blanca ha provocado la muerte de más de un millón de murciélagos en el noreste de Estados Unidos en menos de cuatro años. La enfermedad recibe ese nombre a causa de un hongo blanco que se encontró desarrollándose en el hocico, oído y alas de algunos murciélagos, pero no se sabe si el hongo es la causa primaria de la enfermedad o es simplemente una infección oportunista. Se ha observado una tasa de mortalidad del 90-100 % en algunas cuevas. Al menos seis especies hibernantes se han visto afectadas, incluida "Myotis sodalis", que se encuentra en peligro de extinción. Debido a que las especies afectadas tienen una esperanza de vida larga y un índice de natalidad bajo (aproximadamente un descendiente por año), se cree que las poblaciones tardarán en recuperarse.

Entre las amenazas de origen antropogénico se encuentran los aerogeneradores, un medio de producción de energía limpia y renovable, pero que causan un elevado índice de mortalidad entre los murciélagos; teniendo en cuenta que a menudo no se aprecian señales de traumas externos, se supone que su alta mortalidad en las cercanías de estos dispositivos se debe a una mayor sensibilidad de sus pulmones frente a las repentinas fluctuaciones de presión del aire y que, a diferencia de los de las aves, los hace más propensos a romperse. Por otra parte, en la oscuridad confunden los aerogeneradores con árboles y son heridos o muertos por sus aspas, o quedan atrapados en los vórtices de aire generados por la rotación de las mismas. Aunque diversos estudios indican que algunas especies se han adaptado a una alimentación en espacios abiertos de insectos atraídos por las lámparas de vapor de mercurio, trabajos recientes muestran que la contaminación lumínica tiene un fuerte impacto negativo en especies como el murciélago pequeño de herradura ("Rhinolophus hipposideros") y otros murciélagos nocturnos de vuelo lento, pues les impide una selección de rutas adecuadas de vuelo hacia sus zonas alimenticias y que afectan de forma drástica al crecimiento de los jóvenes, y el volar en zonas iluminadas les hace más vulnerables ante sus depredadores. A todo lo anterior hay que añadir las muertes por atropellos de coches, camiones o trenes, una mortalidad todavía no cuantificada pero que probablemente es elevada. 

Durante siglos, en África, sudeste de Asia e islas de los océanos Índico y Pacífico, se han cazado en pequeñas cantidades los grandes zorros voladores frugívoros por su carne; sin embargo con el incremento de las armas de fuego y las facilidades para acceder a sus hábitats se ha sobreexplotado este recurso y muchas especies están en peligro de extinción, lo que, debido a su importancia en la polinización y dispersión de semillas, tiene una importante repercusión en las cosechas y un gran impacto en el desarrollo del bosque tropical y la sabana. Los murciélagos cavernícolas son particularmente vulnerables, pues a menudo las cuevas solo disponen de una reducida entrada a través de la cual deben pasar todos los animales y si los cazadores se apostan a su entrada pueden matar fácilmente gran cantidad de ejemplares; a ello se une el problema de las capturas accesorias (captura de especies distintas a la que se pretende cazar), como en el caso del pequeño murciélago insectívoro "Tadarida plicata" de Tailandia, que forma colonias de más de un millón de individuos en algunas cuevas y donde los habitantes locales han recolectado su guano durante doscientos años, pero que convive con murciélagos frugívoros a los que ahora los cazadores capturan para comercializar su carne montando redes en las entradas de las cuevas, y que al mismo tiempo capturan a "T.plicata", a los que matan en grandes cantidades y que después desechan, con lo que, además de disminuir la población de ambas especies, se ha visto afectada la producción de guano que aprovechaban los campesinos locales.

En las regiones templadas, las causas principales del descenso en las poblaciones de murciélagos son la pérdida de hábitat y las matanzas, deliberadas o accidentales. La recolección del guano de los murciélagos es un problema para estos animales, pues perturba sus lugares de reposo, y cuando son molestados regularmente durante su hibernación mueren de hambre y muchos lugares de descanso, como cuevas y árboles huecos, han sido obstruidos o derribados. La proliferación y el uso intensivo de insecticidas representan una amenaza para los murciélagos insectívoros. Además de una gran reducción en el tamaño de las poblaciones de insectos, que son su alimento, los plaguicidas pueden causar el envenenamiento indirecto de los murciélagos cuando comen presas intoxicadas. En el mundo desarrollado ya no se utilizan tanto los insecticidas, pero en los países en desarrollo todavía tienen un uso muy extendido, y la falta de regulación implica que a menudo se utilizan insecticidas muy tóxicos.

Algunas especies son perseguidas porque se alimentan de los cultivos agrícolas, o bien porque son vectores de enfermedades, y los Ministerios de Agricultura de algunos países consideran a algunos murciélagos como plagas y recomiendan su exterminio o la reducción de su número. Sin embargo, organizaciones como Bat Conservation International, un organismo que se dedica a reunir datos sobre la situación, la distribución y las amenazas que les acechan, organiza programas educativos y fomenta las investigaciones y la conservación. También los mitos, las supersticiones y los recelos hacia estos animales a menudo han impedido el desarrollo de programas de protección. Sin embargo actualmente se han firmado acuerdos internacionales para su conservación y los murciélagos se hallan protegidos por las leyes de la mayoría de los países europeos y de muchos otros, y se están designando lugares de descanso y hábitats alimentarios para potenciar su conservación.

Los murciélagos fueron agrupados con anterioridad en el superorden Archonta junto con los escandentios (Scandentia), los dermópteros (Dermoptera) y los primates (Primates), debido a las semejanzas aparentes entre los megaquirópteros y estos mamíferos. Actualmente los estudios genéticos han situado a los quirópteros en el superorden Laurasiatheria, junto a los carnívoros (Carnivora), los pangolines (Pholidota), los insectívoros eurasiáticos y americanos (Eulipotyphla), los perisodáctilos (Perissodactyla) y los cetartiodáctilos (Cetartiodactyla, orden de mamíferos placentarios que reúne a los antiguos órdenes de los cetáceos y de los artiodáctilos).

La clasificación de los quirópteros actuales según Simmons y Geisler (1998), con las modificaciones sugeridas por Kirsch "et al." (1998), los reparte en dos subórdenes con dieciocho familias:

Orden de los quirópteros (Chiroptera) 



En la obra "Mammal Species of the World" se citan también dieciocho familias, pero no incluyen Antrozoidae y en cambio citan Hipposideridae, considerada anteriormente subfamilia de Rhinolophidae, pero retornada como familia por Corbet y Hill (1992), Bates y Harrison (1997), Bogdanowicz y Owen (1998), Hand y Kirsch (1998) y otros muchos autores. En el Sistema Integrado de Información Taxonómica (ITIS) se reconocen diecisiete familias, las citadas anteriormente pero sin incluir ni Antrozoidae ni Hipposideridae.

Además, Simmons y Geisler describen en 1998 varias familias extintas de quirópteros, que corregidas y aumentadas por Smith "et al." en 2007 y Simmons "et al." en 2008 son:

Según estudios filogenéticos más recientes, dentro del clado Scrotifera, el grupo basal es el Chiroptera.

Respecto a su evolución, con base en motivos morfológicos, los quirópteros habían sido clasificados erróneamente durante mucho tiempo en el superorden Archonta (junto con los primates, dermópteros, y escandentios) hasta que la investigación genética mostró su estrecha relación con los laurasiaterios (el cual contiene al clado Scrotifera, que une al orden Chiroptera con los órdenes Carnivora, Artiodactyla, Perissodactyla y Pholidota); esto a pesar de que existen pocas similitudes anatómicas dentro de dicho clado y pareciera que no estuvieran relacionados.

Respecto a los fósiles, se han descubierto fósiles de quirópteros con características más primitivas, siendo "Onychonycteris finneyi" la especie más primitiva de los dos géneros monoespecíficos más antiguos de murciélagos de los que existe registro. Esta especie vivió en un área que hoy día es el estado de Wyoming durante en el período Eoceno, hace 52,5 millones de años y que sucede a "Icaronycteris index", anteriormente considerada la especie de murciélago más primitiva. Los primeros murciélagos que aparecen en el registro fósil ya volaban y se alimentaban de insectos. Su morfología era muy similar a la actual, pero todavía no contaban con la ecolocalización, como lo demuestra la cóclea poco desarrollada de "O. finneyi". Se cree que evolucionaron a partir de pequeños mamíferos arborícolas que saltaban de un árbol a otro, desarrollando en primera instancia membranas para planear y finalmente alas. Sin embargo no se ha descubierto ningún fósil que represente un estadio intermedio de esta evolución. Los dos subórdenes de quirópteros, los megaquirópteros y microquirópteros, divergieron casi al principio del Cenozoico.

Durante el Oligoceno, cuando la configuración de los continentes era diferente a la actual, Sudamérica estaba aislada del resto de masas terrestres y el continente australiano se encontraba más al el sur que hoy en día. En esta situación geográfica, su capacidad para volar permitió a los quirópteros una expansión mucho más importante que otros grupos de mamíferos. Hace veintitrés millones de años, colonizaron Indonesia y Australia.

En el Pleistoceno la temperatura global se desplomó, creando vastos casquetes polares en ambos hemisferios. Los que no migraron hacia latitudes más bajas murieron. La genética de poblaciones muestra que los murciélagos europeos se refugiaron en las penínsulas de Europa meridional (ibérica, itálica y balcánica).

Durante esta época compartían cuevas con los humanos primitivos, sin embargo no se conoce casi nada de este encuentro entre especies; solo se han encontrado pinturas rupestres de murciélagos en cuevas del norte de Australia.

Aristóteles no sabía si los murciélagos eran aves u otro tipo de animal. Tres siglos más tarde, Plinio el Viejo los consideró pájaros, un error que persistió para casi todos los naturalistas hasta el siglo XVI. Para Conrad von Gesner representaban una forma intermedia entre aves y mamíferos. Finalmente Linneo los acabó clasificando como mamíferos en su célebre "Systema naturae", ordenándolos en un tronco común con los primates y el hombre. Poco después, Daubenton ya había descrito cinco de las especies de quirópteros que habitan en Europa.

En las primeras obras, las leyendas se mezclaban con la ciencia, y estaban llenas de inexactitudes. Sin embargo Buffon ya descubrió la hibernación de los murciélagos, y durante su exploración de cuevas se encontró con cavernas llenas de guano de quirópteros; observando que en los excrementos había restos de moscas y mariposas, Buffon empezó a conocer la dieta de los murciélagos europeos.
A principios del siglo XIX ya se aceptaba mayoritariamente que los murciélagos formaban un orden propio. Los naturalistas europeos recibían ejemplares de todo el mundo, enviados por pioneros de la edad de la exploración. Muchos ejemplares llegaban de países exóticos, y no se conocía ni la distribución ni su comportamiento. A menudo los naturalistas recibían ejemplares sin ninguna indicación de su origen. A finales del siglo XIX se empezó a conocer de forma detallada el comportamiento de los murciélagos, y ya se estudiaba la hibernación y el despertar (Brehm) o la forma de sus alas (Blasius). Además del trabajo de campo se llevaban a cabo investigaciones en laboratorio, con murciélagos en jaulas alimentados con gusanos de la harina y moscas. De esta forma se descubrieron detalles sobre el apareamiento, el parto, o la ovulación diferida.

Con la llegada del siglo XX se comenzó a utilizar la técnica del anillamiento para su estudio. El auge de la espeleología también permitió conocer mejor a los murciélagos cavernícolas, aunque también implicó una perturbación de su hábitat. Algunas de las técnicas desarrolladas durante este siglo eran muy crueles; se cogían animales que estaban hibernando (despertándolos en el proceso), se los estudiaba, y entonces se los volvía a soltar al frío, donde morían. Los encargados de su captura también solían meter un gran número de especímenes en muy poco espacio, provocando que murieran ahogados o que se dañaran intentando huir, y la inexperiencia de algunos capturadores les causaba fracturas del antebrazo o de los dedos. Por ello, algunas campañas de captura y estudio causaron decenas de miles de muertos de murciélagos y la pérdida de colonias enteras. A principios de la década de 1960, algunos biólogos comenzaron a mostrar su oposición a esta metodología.

Los primeros naturalistas no podían comprender como era posible que los murciélagos «vieran» en la oscuridad. Investigando la posibilidad de que se trataba de un sentido diferente al de la vista, algunos naturalistas les tapaban los ojos y los soltaban en cuartos oscuros con muchos obstáculos y comprobaron que los animales no chocaban contra los obstáculos, sin embargo cuando les lesionaban los conductos auditivos o los tapaban con cera, los murciélagos se desorientaban. A finales del siglo XVIII, Spallanzani y Jurine comenzaron a investigar este fenómeno en laboratorio. Cuvier planteaba la hipótesis de que sus membranas auriculares y alares eran muy sensibles, y detectaban cambios en el aire. Boitard creía que esta percepción estaba relacionada con el oído, y Allen sospechaba que el trago, un lóbulo de piel situado frente al pabellón auricular de los murciélagos, captaba señales de retorno, al igual que el sonar.

En 1938, con el desarrollo de un micrófono que captaba las altas frecuencias, Donald Griffin y Robert Galambos, del Harvard Medical School Laboratory, llevaron a cabo experimentos para confirmar que los murciélagos utilizan la ecolocalización. Trabajando con distintas especies de murciélagos, descubrieron su capacidad de enviar y recibir ultrasonidos de hasta 50 kHz. En 1940 presentaron su descubrimiento.

El papel desempeñado por los murciélagos en el mantenimiento y regeneración de bosques, en la dispersión de semillas, o su actuación como polinizadores o como agentes de control de plagas se apoya cada vez más en argumentos aportados por numerosos estudios científicos.

Los murciélagos pueden resultar útiles como agentes de control biológico, reduciendo o limitando el crecimiento de poblaciones de insectos u otros artrópodos que de lo contrario se podrían convertir en una plaga. De esta forma protegen indirectamente a los humanos y a otros animales de enfermedades transmitidas por insectos, y evitan que su crecimiento descontrolado ponga en peligro las plantaciones vegetales. Existen estudios recientes que indican que contribuyen de manera decisiva al control de plagas, como el realizado por la Universidad Cornell, que recomienda a los agricultores que intenten aumentar las poblaciones locales de murciélagos y golondrinas entre mayo y julio, que es cuando más efecto pueden tener sobre las poblaciones de insectos. Otro estudio publicado en 2008 en la revista "Science" reveló que los murciélagos eran significativamente más eficientes que las aves en tareas de control biológico; las plantas en las que se impedía el acceso a las aves tenían un 65% de artrópodos más que las plantas control, mientras que las plantas en las que no se dejaba acceder a los murciélagos tenían un 153 % más. Según este estudio, los murciélagos también protegen en cierta medida a las plantas de los animales herbívoros.

Desempeñan un papel ecológico vital como polinizadores. Hay unas 750 especies de plantas polinizadas por distintos murciélagos en todo el mundo. Un murciélago puede visitar hasta 1000 flores en una noche. Un murciélago es capaz de transportar polen de una flor hasta otra a 30 kilómetros de distancia, y se han registrado vuelos de 65km en una dirección en una sola noche.

También desarrollan un importante papel en la dispersión de semillas. Cuando se comen un fruto, que más tarde excretan en otro lugar, contribuyen a que la planta se extienda a nuevas zonas. Muchas plantas tropicales dependen completamente de los murciélagos.

Los murciélagos son un reservorio natural para un gran número de patógenos zoonóticos como la rabia, el síndrome respiratorio agudo severo, "Henipavirus" y posiblemente el virus Ébola. Su gran movilidad, amplia distribución y comportamiento social convierten a los murciélagos en hospedadores y vectores de enfermedades. Muchas especies también parecen tener una alta tolerancia a la hora de albergar patógenos y a menudo no desarrollan la enfermedad mientras están infectadas.

En regiones donde la rabia es endémica, solo el 0,5 % de murciélagos porta la enfermedad, sin embargo, según un informe realizado en los Estados Unidos, 22 de los 31 casos de rabia en humanos que no fueron causados por perros entre los años 1980 y 2000, fueron provocados por mordeduras de murciélago.

Existe constancia de varias muertes de humanos en las selvas de Sudamérica tras el ataque de grupos numerosos de murciélagos a causa del virus de la rabia trasmitido por sus mordeduras, obligando a las autoridades sanitaria a aplicar medidas preventivas y de control, como vacunas y suero antirrábico, para proteger a las poblaciones nativas allí donde hubiera focos de esta enfermedad; estos ataques se producen por cambios en los ecosistemas locales, como la deforestación indiscriminada que provoca la extinción de algunas especies que son predadores naturales de los murciélagos y de otras que les sirven como fuente de alimento y, en el caso de los murciélagos hematófagos, la transformación de la selva tropical en pastizales para la pecuaria inicialmente aumenta su volumen de alimento, lo que facilita su reproducción y el aumento de las poblaciones, pero posteriormente el constante movimiento de las reses en busca de nuevos pastos los deja sin fuente de comida.

Los ejemplares rabiosos suelen ser torpes, desorientados e incapaces de volar, lo que aumenta la probabilidad de que entren en contacto con las personas. Aunque no se debe tener un miedo irracional a estos animales, es conveniente evitar manejarlos o tenerlos en lugares habitados, al igual que con cualquier animal salvaje. Si se encuentra un murciélago en una residencia cerca de una persona dormida, un bebé, una persona ebria o un animal doméstico, la persona o el animal doméstico deberían recibir asistencia médica inmediata para descartar la posibilidad de que hayan sido contagiados.

En Centroamérica se han encontrado representaciones de una divinidad murciélago de los mayas en columnas de piedra y recipientes de barro de unos 2000 años de antigüedad; esta deidad tenía cabeza de murciélago y las alas extendidas, y también aparece en pictogramas de esta cultura.

En multitud de mitos y leyendas, y en la mayor parte del mundo, los murciélagos han causado temor entre los humanos a lo largo de la historia. A causa de los hábitos nocturnos de la mayoría de sus especies y la ancestral incomprensión sobre como podían «ver» en la oscuridad, se les consideraba y todavía se les considera a menudo como habitantes siniestros de la noche. Además, aunque en realidad solo tres especies son hematófagas, a menudo se les asocia a los míticos vampiros.

Estos temores supersticiosos se manifiestan en casi todo el mundo con pocas excepciones, como en China, donde son símbolo de felicidad y provecho; este hecho se refleja en la palabra china "fu", que significa al mismo tiempo «felicidad» y «murciélago». Estos animales son utilizados a menudo, en grupos de cinco ("wu fu"), como un bordado en la ropa o como un talismán redondo. Los cinco murciélagos representan las cinco felicidades: salud, riqueza, larga vida, buena suerte y tranquilidad; este antiguo diseño a menudo es representado en color rojo, el color de alegría. 
En Europa los murciélagos han sido vistos de manera predominantemente negativa desde la antigüedad. Así, en "Las metamorfosis", Ovidio explica que las hijas del rey de Beocia fueron convertidas en murciélagos como castigo, porque se habían quedado a trabajar en el telar contando historias mitológicas, en lugar de participar en las festividades en honor de Baco. La Biblia también les asigna una condición negativa, incluyéndolos entre las «aves inmundas» (Lt 11.13, 19; Dt 14.11, 12, 18) e Isaías considera las cuevas donde descansan como lugares apropiados para arrojar los ídolos (Is 2.19ss).
Los demonios y las criaturas diabólicas (incluido el propio Satanás) a menudo son representados en las artes visuales clásicas con alas de murciélago, a diferencia de los ángeles. 

En el célebre grabado de Alberto Durero "Melancolía I" aparece una criatura similar a un murciélago que sostiene una cartela con el título del grabado. Durante el Barroco son uno de los atributos del Anticristo. El pintor español Francisco de Goya los utilizó, junto con los búhos, como símbolo de amenaza.

Los murciélagos también se asocian con la muerte y el alma; en algunas representaciones del siglo XIV, el alma abandona el cuerpo tras la muerte elevándose en forma de murciélago. Las leyendas europeas sobre vampiros también podrían tener sus orígenes en esta asociación, pues se remontan a épocas muy anteriores al descubrimiento de los auténticos murciélagos vampiros de América. Este mito de los vampiros ha perdurado hasta hoy en día en la cultura popular y se refleja sobre todo en la imaginación de los escritores y directores de cine; figuras como el Conde Drácula, que toma forma de murciélago durante la noche a la busca de víctimas, o en multitud de películas, como "El baile de los vampiros", de Roman Polanski, que también utilizan este mito.

En el cine de terror los murciélagos aparecen con frecuencia en las cuevas y casas abandonadas como inspiradores de pánico, e incluso son protagonistas en películas del género como "El murciélago diabólico" (1941), protagonizada por Bela Lugosi, o en "Bats" (1999), protagonizada por Lou Diamond Phillips y Dina Meyer.

La vida nocturna de estos animales también inspiró la creación del personaje de cómic y héroe de películas Batman, un superhéroe que se disfraza de murciélago para salir a la captura de criminales durante la noche y que eligió ese disfraz por el temor que infunden a las personas.

En algunas zonas del este de España es un símbolo heráldico, y figura en los escudos de ciudades como Valencia, Palma de Mallorca o Fraga. El uso heráldico del murciélago en Valencia, Cataluña y las Islas Baleares tiene sus orígenes en el dragón alado ("vibra" o "víbria"), de la cimera real del rey Pedro IV de Aragón; esta es la teoría más extensamente aceptada, aunque también existe una leyenda basada en el "Llibre dels feits" que narra que gracias a la intervención de un murciélago, el rey Jaime I de Aragón ganó una batalla crucial contra los sarracenos durante la Conquista de Valencia. Su uso como símbolo heráldico es frecuente en los territorios de la antigua Corona de Aragón y poco utilizado en otros lugares, aunque también se puede encontrar en el escudo de armas de la ciudad de Albacete, en España, o en el de la ciudad de Montchauvet, en Francia.






</doc>
<doc id="19298" url="https://es.wikipedia.org/wiki?curid=19298" title="Bôa">
Bôa

Bôa es una banda de rock formada en Londres en 1993 por Ed Herten (baterista), Paul Turrell (tecladista) y Steve Rodgers (guitarrista y voz principal) . Son conocidos por su tema Duvet, utilizado en la presentación de la serie de anime Serial Experiments Lain.

Alex Caird, quien había tocado con Ed otra banda llamada Draggin' Bones, fue llamado para tocar el bajo. La hermana menor de Steve, Jasmine Rodgers, fue invitada a cantar en una parte del coro en una de sus primeras canciones "Fran", ella luego se convertiría en la cantante principal. Ben Henderson, que había tocado con Alex en la banda "Doctor Sky", poco después fue contratado para tocar el saxo.

Su primera presentación fue en enero del 94 en el Foro de Londres como teloneros de Paul Rodgers, padre de Steve y Jasmine. En el verano de ese mismo año, Ed Herten decidió dejar la banda para concentrarse en sus estudios, por lo que ellos buscaron a otro baterista, Lee Sullivan. Él le dio al grupo más ritmo de rock que era el siguiente paso para complementar el ritmo funk que ya tenían y cambiar el saxo por las guitarras con Ben Henderson.

Bôa perfeccionó sus presentaciones a lo largo de todo el sur de Inglaterra y en 1996, aceptaron firmar por una disquera japonesa, Polystar. Aunque el álbum fue grabado y producido en Inglaterra, Jasmine y Steve viajaron a Japón en 1998 para promover su álbum debut "Race of a Thousand Camels", que fue lanzado solo en ese país.

Su primer single, "Duvet", se utilizó como la canción principal de una nueva serie de anime llamada "Serial Experiments Lain".

En el 2000, Ben Henderson deja la banda para concentrarse en su banda propia, Moth, junto a su esposa, cantante y compositora Tina. Luego, Bôa firmó con Geneon (exPioneer), distribuidores de Lain en USA, realizaron un concierto en vivo en la convención Otakon en el mismo año, siendo bien recibidos

En el 2001, Bôa lanza su álbum debut en USA "Twilight", bajo el sello de Pioneer. Pero ese mismo año tuvieron otra baja: Paul Turrell deja la banda por intereses propios.

La banda empieza a grabar su tercer álbum el 2003 pero fue lanzada hasta el 2005, "Get There".





</doc>
<doc id="19300" url="https://es.wikipedia.org/wiki?curid=19300" title="Radiohead">
Radiohead

Radiohead es una banda británica de rock alternativo y art rock originaria de Abingdon-on-Thames, Inglaterra, formada en 1985. Está integrada por Thom Yorke (voz, guitarra, piano), Jonny Greenwood (guitarra solista, teclados, otros instrumentos), Ed O'Brien (guitarra, segunda voz), Colin Greenwood (bajo, teclados) y Phil Selway (batería, percusión). 

Radiohead lanzó su primer sencillo, «Creep», en 1992. Si bien la canción fue en un comienzo un fracaso comercial, se convirtió en un éxito mundial tras el lanzamiento de su álbum debut, "Pablo Honey" (1993). La popularidad de Radiohead en el Reino Unido aumentó con su segundo álbum, "The Bends" (1995). El tercero, "OK Computer" (1997), con un sonido expansivo y temáticas como la alienación y la globalización, les dio fama mundial y ha sido aclamado como un disco histórico de la década de 1990 y uno de los mejores álbumes de todos los tiempos.

"Kid A" (2000) y "Amnesiac" (2001) significaron una evolución en su estilo musical, al incorporar música electrónica experimental, música clásica del siglo XX, krautrock y jazz. A pesar de la división inicial de fanes y crítica, "Kid A" fue nombrado mejor álbum de la década por "Rolling Stone", "Pitchfork" y " The Times". El álbum "Hail to the Thief" (2003), una mezcla de rock y música electrónica con letras inspiradas en la guerra al terror, fue el último de la banda con el sello discográfico EMI. Radiohead lanzó de manera independiente su séptimo álbum, "In Rainbows" (2007), que fue puesto a la venta en forma de descarga digital por la que los usuarios pagaban el precio que estimasen oportuno. Su octavo álbum, "The King of Limbs" (2011), fue una exploración del ritmo y de texturas más calmadas. En su noveno álbum, "A Moon Shaped Pool" (2016), predominan los arreglos orquestales de Jonny Greenwood. 

Han obtenido tres veces el Premio Grammy al , por "OK Computer", "Kid A" e "In Rainbows". El trabajo de la banda ha sido reconocido por los críticos en distintas listas y sondeos musicales. En 2005 se posicionaron en el puesto 73 en la lista de los 100 mejores artistas de todos los tiempos de la revista "Rolling Stone". En 2009 fue nombrada la mejor banda de la década del 2000 por "The Guardian". En 2010 ocuparon el puesto 29 en la lista de los 100 artistas más grandes de todos los tiempos según el canal VH1 y en 2011 se situaron en la posición número 3 de la lista de los mejores artistas británicos de la historia según "Paste Magazine", solo superados por The Beatles y The Rolling Stones. En 2014 el semanario inglés "NME" los ubicó como los músicos más influyentes del momento. Asimismo, su actuación en Glastonbury '97 fue elegida mejor concierto de la historia en una votación de la revista "Q" en 2004 y mejor concierto de un festival en una encuesta de Proud Galleries en 2005. El grupo ha vendido casi 40 millones de álbumes en todo el mundo. El 29 de marzo de 2019, Radiohead ingresó en el Salón de la Fama del Rock and Roll.

Aunque los primeros álbumes de la banda fueron especialmente influyentes en el rock y la música pop británica, su trabajo posterior ha influido a otros músicos de géneros que van desde el jazz y la música clásica al hip hop, la música electrónica y el R&B.

Radiohead se formó a mediados de la década de 1980 en la escuela Abingdon (un colegio privado para varones) en Abingdon-on-Thames, Oxfordshire en Inglaterra, a la cual acudían el baterista Phil Selway, el guitarrista Ed O'Brien, el vocalista Thom Yorke, el bajista Colin Greenwood y su hermano Jonny. Yorke y Colin Greenwood estaban en el mismo curso, O'Brien y Selway eran un año mayores que ellos y Jonny Greenwood, dos años menor que su hermano. Todos provenían de familias de clase media y empezaron a tocar en el salón de música de la escuela, tomando el nombre del único día de la semana en el que podían ensayar: On a Friday («el viernes»). La banda realizó su primera presentación en directo en la Taberna Jericho de Oxford a finales de 1986; al principio Jonny Greenwood tocaba la armónica y el teclado, pero pronto se convirtió en el guitarrista principal.

Aunque Yorke, O'Brien, Selway y Colin Greenwood dejaron Abingdon hacia 1987 para asistir a la universidad, la banda siguió practicando los fines de semana y en vacaciones. En 1991, cuando todos los miembros de la banda excepto Jonny Greenwood, habían completado sus estudios universitarios, On a Friday se reagrupó y la banda comenzó a grabar "demos", entre ellos "Manic Hedgehog" y reanudaron sus presentaciones en los alrededores de Oxford. La popularidad de la banda en la región de Oxfordshire creció hasta el punto de aparecer en la portada de "Curfew", una revista local de música. La música independiente había tenido mucha repercusión en Oxfordshire y en el valle del Támesis a finales de la década de 1980, pero solo estaba enfocada en bandas del shoegazing como Ride y Slowdive; On a Friday no pertenecía al género.

A medida que On a Friday comenzó a dar más conciertos, varias discográficas empezaron a mostrar interés por la banda. Chris Hufford, productor de Slowdive y Bryce Edge, productor y copropietario de los Estudios Courtyard de Oxford, asistieron a uno de los primeros conciertos del grupo en la Taberna de Jericho. Impresionados por la banda, produjeron una "demo" y se convirtieron en sus mánagers; de hecho, lo siguen siendo en la actualidad. Tras un encuentro casual entre Colin Greenwood y el representante de EMI, Keith Wozencroft en la tienda de discos donde trabajaba éste, el grupo firmó un contrato con la discográfica durante un lapso de seis álbumes. A petición de la compañía, la banda cambió su nombre inspirándose en una canción de Talking Heads del álbum "True Stories" (1986) llamada «Radio Head».

Radiohead grabó su primera publicación, el EP "Drill" en los Estudios Courtyard, con Chris Hufford y Bryce Edge como productores. Se lanzó en marzo de 1992 y su relevancia en las listas de ventas fue muy escasa. Posteriormente, la banda contrató a Paul Kolderie y Sean Slade —quienes habían previamente trabajado con las bandas estadounidenses de indie rock Pixies y Dinosaur Jr.— para que produjeran su álbum debut, grabado rápidamente en un estudio de Oxford en 1992. Tras el lanzamiento de su primer sencillo, «Creep» a fines de dicho año, Radiohead comenzó a llamar la atención de la prensa musical británica, aunque no siempre en forma favorable. "NME" los llamó «una imitación cobarde de una banda de rock», mientras que «Creep» se retiró de la BBC Radio 1 por ser considerada «demasiado depresiva».

La banda lanzó su álbum debut, "Pablo Honey", en febrero de 1993. El álbum alcanzó el número 22 de las listas del Reino Unido, mientras que «Creep» y otros sencillos del mismo como «Anyone Can Play Guitar» y «Stop Whispering» no lograron convertirse en éxitos. «Pop is Dead», un sencillo no incluido en el álbum y que sería desacreditado más tarde por la banda, tuvo unas ventas reducidas. Algunos críticos compararon el estilo primario de la banda con el grunge, popular a principios de la década de 1990, al extremo de compararla con Nirvana. "Pablo Honey" no tuvo éxito comercial ni recibió alabanzas de la crítica tras su lanzamiento, se le consideró un álbum con una presencia clara y determinante de las guitarras, cayendo dentro del clásico rock inglés: indie rock . La visión es bastante particular, existencialista y realista, dirigiendo sus críticas y lamentos a la sociedad contemporánea. Pese a algunas menciones al falsete de Yorke, la banda sólo tuvo una breve gira en universidades inglesas y discotecas.

En los primeros meses de 1993, Radiohead comenzó a atraer a oyentes de diferentes lugares. «Creep» se transmitió con mucha frecuencia en la radio israelí, y en marzo, después de que la canción se convirtiera en un éxito en las listas de ese país, Radiohead recibió una invitación a Tel Aviv para su primera actuación en el extranjero. Paralelamente, en San Francisco la estación de radio de música alternativa KITS, añadió la canción en su lista de reproducción. Pronto otras emisoras a lo largo de la costa oeste de los Estados Unidos siguieron el ejemplo. En el momento en el que Radiohead comenzó su gira por Norteamérica en junio de 1993, el video promocional de «Creep» se difundía ampliamente en MTV. La canción alcanzó el número dos en la lista "Billboard Modern Rocks" de Estados Unidos y alcanzó el número siete en las listas del Reino Unido cuando volvió a publicarse allí a fines de ese año.

El inesperado éxito del sencillo en Estados Unidos hizo que la banda tuviera que improvisar nuevas formas de promocionarse y que estuviera trasladándose de un continente a otro, llegando a tocar más de 150 conciertos en 1993. Radiohead estuvo a punto de separarse debido a la presión del repentino éxito de "Pablo Honey", con la gira extendiéndose hasta dos años. Los miembros del grupo manifestaron que fue una gira a la que era difícil adaptarse, mencionando que, hacia su final, acabaron «tocando las mismas canciones que grabamos hace dos años [...] como si estuviéramos atrapados en un túnel del tiempo» y también afirmaron que en aquel entonces estaban ansiosos por componer nuevos temas.

La banda comenzó a trabajar en su segundo álbum en 1994, junto al productor de los Estudios Abbey Road John Leckie. Existía mucha presión dentro del grupo, ya que se esperaba que igualaran o superaran el éxito de «Creep». Las canciones sonaban antinaturales en el estudio, debido a que los integrantes del grupo las habían ensayado en exceso. Buscaron disminuir la presión con una gira por Europa, el Lejano Oriente, Australasia y México y fue allí cuando comenzaron a sentir confianza en sus nuevas canciones. Sin embargo, confrontado otra vez por su fama nuevamente adquirida, Yorke se desilusionó por estar «en el filo del estilo de vida atractivo, descarado y acaramelado de MTV» y sintió que estaba ayudando al consumismo.

"My Iron Lung", un EP y sencillo lanzado en octubre de 1994 fue la reacción de Radiohead, marcando una transición hacia la profundidad que quisieron dar a su segundo álbum. Publicitado a través de emisoras radiofónicas, las ventas del sencillo fueron mejores de lo esperado y se sugirió que por primera vez la banda había creado una base de fieles seguidores por este éxito. Habiendo presentado más canciones nuevas en la gira, Radiohead terminó de grabar su segundo álbum hacia finales de ese año y lanzó "The Bends" el 13 de marzo de 1995. El disco recibió reseñas más positivas que el anterior por las letras y las interpretaciones. Coincidiendo con este lanzamiento, la banda publicó en VHS su primer video álbum, "Live at the Astoria", que recogía un concierto en el teatro londinense en mayo de 1994.

Aunque los miembros de Radiohead fueron vistos como extraños en el britpop, género predominante en aquel entonces, tuvieron finalmente éxito en su país natal con "The Bends"; los sencillos «Fake Plastic Trees», «High & Dry», «Just» y «Street Spirit (Fade Out)» ingresaron en la "UK Singles Chart" y este último tema estuvo en el "Top 5" de dicha lista. En 1995, Radiohead fue de gira a Estados Unidos y Europa junto a R.E.M., una de sus primeras influencias. «High and Dry» se convirtió en un éxito moderado, llegando al puesto 78 del "Billboard Hot 100" y "The Bends" llegó al puesto 88 del "Billboard 200", aunque su estilo influiría directamente a bandas como Muse y Coldplay. El grupo quedó satisfecho en cuanto a la recepción del álbum. Jonny Greenwood comentó: «Creo que el punto de inflexión para nosotros tuvo lugar nueve o doce meses después del lanzamiento de "The Bends", cuando comenzó a aparecer en las encuestas de 'lo mejor del año'. Fue entonces cuando sentimos que habíamos tomado la decisión correcta al haber elegido ser una banda».

A finales de 1995, Radiohead ya había grabado una canción que estaría presente en su próximo álbum. «Lucky», puesta a la venta como sencillo para promocionar el álbum de caridad de War Child "The Help Album", se creó durante una breve sesión con Nigel Godrich, un joven ingeniero de sonido que fue asistente de producción en "The Bends" y que también produjo uno de sus lados B, «Talk Show Host». La banda decidió producir su nuevo álbum junto a Godrich y comenzaron a trabajar a principios de 1996. Hacia julio, ya habían grabado cuatro canciones en su estudio de ensayos, Canned Applause, cerca de Didcot, Oxfordshire.

En agosto de 1996, Radiohead salió de gira como teloneros de Alanis Morissette, buscando perfeccionar sus canciones en directo antes de terminar la grabación. Luego la reanudaron fuera de un estudio tradicional, eligiendo en su lugar una mansión del siglo XV, St. Catherine's Court, cerca de Bath. Las sesiones de grabación fueron relajantes, con la banda tocando en todo momento, grabando canciones en diferentes salas y escuchando a The Beatles, DJ Shadow, Ennio Morricone y Miles Davis en busca de inspiración. Radiohead aportó «Talk Show Host» a la banda sonora de la adaptación realizada por Baz Luhrmann "Romeo + Juliet", así como «Exit Music (For a Film)» hacia fines de ese año. Gran parte del álbum estuvo terminado a fines de 1996 y hacia marzo del año siguiente ya estaba masterizado y mezclado.

Radiohead lanzó su tercer álbum, "OK Computer", en junio de 1997. Compuesto principalmente de canciones de rock melódico, el nuevo disco fue el primero donde el grupo experimentó con la estructura de las canciones y se incorporó un sonido ambiental, vanguardista y con influencias de música electrónica. Sus letras adquirieron un tono menos personal y más de observador que en "The Bends" y una revista llamó a las canciones «los blues del fin del milenio». "OK Computer" se recibió con elogios de la crítica y Yorke admitió que estaba «impresionado [debido a] la reacción que generó. Ninguno de nosotros ya sabía jodidamente si era bueno o malo. Lo que realmente me voló la cabeza fue el hecho de que la gente captó todas las cosas, todas las texturas y los sonidos y las atmósferas que estábamos tratando de crear».

"OK Computer" fue el debut del grupo en el primer puesto de las listas británicas, otorgándole a Radiohead éxito comercial global. Pese a haber ingresado en el puesto 21 en las listas estadounidenses, el álbum fue muy reconocido en dicho país y ganó el premio Grammy al y recibió una nominación en la categoría de . «Paranoid Android», «Karma Police» y «No Surprises» se pusieron a la venta como sencillos, siendo «Karma Police» el más exitoso a nivel internacional. «Let Down», cuyo lanzamiento como sencillo se canceló debido a que el video realizado resultó insatisfactorio, fue lanzada como sencillo promocional para estaciones de radios en algunos países.

Tras el lanzamiento de "OK Computer" tuvo lugar la gira internacional Against Demons, que contó con unas 104 fechas por todo el mundo, comenzando el 22 de mayo de 1997 en Barcelona y acabando el 18 de abril de 1998 en el Radio City Music Hall de Nueva York, con una presentación posterior el 14 de junio de 1998 en el Tibetan Freedom Concert de Washington D. C. Grant Gee, el director del video promocional de «No Surprises», acompañó y filmó a la banda, lanzando este material en el documental "Meeting People Is Easy", estrenado en noviembre de 1998. La película muestra la distante relación de la banda con la industria musical y la prensa, enseñando además su progreso desde la primera presentación de la gira en mayo de 1997 hasta las de abril de 1998 en Nueva York, casi un año después. Durante este tiempo, la banda también lanzó un compilado de videos promocionales llamado "7 Television Commercials" así como dos EP: "Airbag/How Am I Driving" (nominado en los premios Grammy en la categoría de mejor álbum de música alternativa), y "No Surprises/Running from Demons", una recopilación de los lados B de los sencillos de "OK Computer".

Radiohead estuvo largo tiempo inactivo como grupo tras su gira de 1997-1998; después de acabarla, su única presentación pública en 1998 fue en diciembre de 1998 en un concierto de Amnistía Internacional en París. Yorke admitió luego que durante ese período la banda estuvo a punto de separarse y que él tuvo una severa depresión. A principios de 1999, Radiohead comenzó a trabajar en una secuela para "OK Computer". Aunque ya no tenían presión ni un plazo impuesto por la discográfica, la tensión durante este tiempo fue alta. Los miembros de la banda tenían visiones diferentes sobre el futuro de Radiohead y Yorke sufría de bloqueo del escritor, lo que lo empujó a una forma de composición más abstracta y fragmentaria. El grupo se retiró a los estudios con el productor Nigel Godrich en París, Copenhague y Gloucester, además de en su estudio de Oxford. Finalmente, todos sus integrantes acordaron tomar una nueva dirección musical, redefiniendo sus nuevos roles instrumentales. Tras casi 18 meses, las sesiones de grabación finalizaron en abril de 2000.

El 2 de octubre de 2000 la banda lanzó su cuarto álbum, "Kid A", el primero de los dos discos grabados en dichas sesiones. En vez de ser una secuela de similar estilo a "OK Computer", "Kid A" presenta un estilo minimalista con menos secciones de guitarra eléctrica e instrumentación más diversa donde se incluyen las ondas Martenot, percusión programada e instrumentos de cuerda y viento-metal de jazz. Debutó en el primer puesto de las listas de muchos países incluyendo los Estados Unidos, donde su ingreso en la cima significó un hito en la historia de la banda y un éxito infrecuente de músicos del Reino Unido en dicho país.

Este éxito se acreditó a la campaña de mercadotecnia, la filtración del álbum a través de la red Napster unos meses antes de su lanzamiento y a la expectativa generada por "OK Computer". Aunque Radiohead no lanzó ningún sencillo extraído de "Kid A", se radiodifundieron copias promocionales de «Optimistic» e «Idioteque» y una serie de "blips" o videos breves de aproximadamente 30 segundos de duración se transmitieron en los canales musicales y se lanzaron gratuitamente en Internet. La banda había leído el libro antiglobalización de Naomi Klein "" durante la grabación y decidieron realizar una gira de verano por Europa dicho año en una carpa libre de publicidad. Además, en octubre promocionaron "Kid A" con tres conciertos en Norteamérica de los que se vendieron todas las entradas y una actuación en "Saturday Night Live".

"Kid A" ganó el premio Grammy en la categoría de mejor álbum de música alternativa y obtuvo una nominación en la de álbum del año a principios de 2001. Recibió alabanzas y duras críticas de los círculos de la música independiente por haber utilizado elementos de la música alternativa, mientras que algunos críticos británicos del "mainstream" vieron al álbum como «un suicidio comercial» y lo calificaron como «intencionalmente complicado», añorando el anterior estilo del grupo. Los seguidores también estuvieron divididos; junto a aquellos que quedaron desconcertados y sintieron rechazo hubo otros que lo consideraron el mejor trabajo de la banda. Yorke, sin embargo, negó que Radiohead buscara evitar expectativas comerciales diciendo: «Estaba realmente asombrado por lo mal que se estaba viendo [a "Kid A"], [...] la música no es tan difícil de comprender. No estábamos tratando de ser complicados, [...] en realidad, tratábamos de comunicarnos pero del otro lado de la línea, parecía que nos habíamos perdido entre mucha gente [...] —lo que hemos hecho no es tan radical».

"Amnesiac", lanzado en junio de 2001, contenía otras canciones extraídas de las mismas sesiones de grabación. Su estilo musical era similar en cuanto a su combinación de música electrónica e influencias de jazz, aunque había un mayor uso de las guitarras. El disco fue un éxito a nivel comercial y crítico en todo el mundo y llegó al primer puesto en las listas británicas y al segundo lugar en Estados Unidos, recibiendo nominaciones en los premios Grammy y en los premios Mercury. Tras el lanzamiento de "Amnesiac", la banda comenzó una gira mundial visitando Norteamérica, Europa y Japón. Mientras tanto, «Pyramid Song» y «Knives Out», los primeros sencillos de la banda desde 1998, tuvieron un éxito moderado y «I Might Be Wrong», inicialmente pensado como el tercer sencillo, se convirtió en el primer álbum en directo de la banda. "" presenta interpretaciones en directo de siete temas de "Kid A" y "Amnesiac" junto a la acústica «True Love Waits».

En julio y agosto de 2002, Radiohead estuvo de gira en Portugal y España, donde tocaron algunas canciones escritas recientemente. Luego grabaron el nuevo material en dos semanas en Los Ángeles con Godrich, añadiéndole muchas nuevas pistas en Oxford, donde continuaron su trabajo el año siguiente. Los miembros del grupo describieron el proceso de grabación como relajante, en contraste con las tensas sesiones para "Kid A" y "Amnesiac". Radiohead también compuso entonces junto a Sigur Rós la música para "Split Sides", una pieza de danza de Merce Cunningham Dance Company que debutó en octubre de 2003 en la Brooklyn Academy of Music. El sexto álbum del grupo, "Hail to the Thief", se lanzó en junio de 2003. Integrando los diferentes estilos en su carrera, "Hail to the Thief" combina el rock basado en guitarra con influencias de música electrónica y letras sobre la actualidad escritas por Yorke. Aunque el álbum recibió alabanzas, muchos críticos opinaron que Radiohead estaba vendiendo agua creativamente en vez de continuar la «redefinición de género» que había comenzado con "OK Computer". Sin embargo, el álbum gozó de éxito comercial, debutando en la primera posición en el Reino Unido y en la tercera en Estados Unidos, recibiendo un disco de platino en el primer país y uno de oro en el segundo. Los sencillos del álbum «There There», «Go to Sleep» y «2+2=5» fueron difundidos en las radios de rock. En la de los premios Grammy, Radiohead recibió una nominación en la categoría de mejor álbum alternativo por dicho trabajo, mientras que Godrich y el ingeniero de sonido Darrel Thorp recibieron el premio al mejor trabajo de ingeniería.

Yorke negó que el título sea un comentario sobre la controvertida elección presidencial estadounidense de 2000, explicando que escuchó por primera vez esas palabras en un debate de la BBC Radio 4 sobre la política estadounidense en el siglo XIX. Yorke comentó que sus letras recibieron influencias de las noticias sobre la guerra de 2001 a 2002 y «la sensación de que estamos entrando en una era de intolerancia y temor donde el poder de expresarnos en democracia y ser escuchados se nos está negando», pero también comentó que «[Radiohead] no escribió un disco de protesta [...] o político». Tras su lanzamiento, Radiohead se embarcó en mayo de 2003 en una gira mundial, que incluyó una aparición en el Festival de Glastonbury. La gira terminó un año después con una presentación en el Festival de Coachella. Durante la misma, la banda puso a la venta "COM LAG", un EP que recopilaba la mayoría de los lados B de aquel entonces. Después de esto, el grupo comenzó a componer y ensayar en su estudio de Oxford, pero posteriormente se tomaron un paréntesis. Libres de su contrato discográfico, el grupo pasó el resto del año descansando con su familia y trabajando en proyectos solistas. En diciembre de 2004, la banda publicó "The Most Gigantic Lying Mouth of All Time", un DVD que recoge los cuatro episodios del programa homónimo que fue transmitido a través de radiohead.tv. y que incluyen un total de veinticuatro cortometrajes animados, muchos de ellos por los propios seguidores del grupo.

Radiohead comenzó a trabajar en su séptimo álbum en febrero de 2005, aunque las sesiones regulares no comenzaron hasta agosto. En esta etapa serían contactados por Bob Geldof para participar en junio en la serie de conciertos Live 8, pero debido a la dispersión de la banda y su falta de actividad por la paternidad de Jonny, decidieron no acudir. En septiembre de 2005 la banda grabó una nueva canción, «I Want None of This», para el álbum de caridad de War Child: "Help: A Day in the Life", una secuela del compilado de 1995 para el que habían colaborado con «Lucky». El álbum se vendió por Internet, y la canción de Radiohead fue la más descargada del disco, aunque no se puso en venta como sencillo. Radiohead ya había comenzado a trabajar en su siguiente disco por su cuenta y luego con el productor Mark Stent. Sin embargo, a finales de 2006, tras haber salido de gira por Europa y Norteamérica y estrenado trece nuevas canciones en la misma, la banda reanudó el trabajo con Nigel Godrich en Londres, Oxford y varias localidades rurales en Somerset. El trabajo estuvo finalizado en junio de 2007 y se masterizó al mes siguiente.

El 1 de octubre de 2007, Radiohead anunció en su sitio web que su nuevo álbum, "In Rainbows", estaba terminado y que podía efectuarse una compra del mismo por el precio que el cliente considerara apropiado en formato digital o físico, con envío postal. También se incluyó la opción de descargarlo en forma gratuita, con la inscripción «Queda a su criterio». El formato físico del álbum comprendía dos CD y dos discos de vinilo, así como contenido adicional (fotografías digitales y librillo de letras). Tras el repentino anuncio de la banda solo diez días antes de activar la descarga, la estrategia inusual del grupo llamó la atención en la industria de la música. Más tarde, bandas como Nine Inch Nails y The Smashing Pumpkins imitaron esta tendencia, lanzándose a vender sus discos de manera independiente a través de Internet. Se informó de que se habían vendido 1,2 millones de copias el día de su lanzamiento, pero el mánager del grupo no mencionó las cifras oficiales, afirmando que la idea de lanzarlo por Internet tenía como objetivo incentivar las ventas físicas minoristas. Colin Greenwood explicó que el lanzamiento por Internet fue una forma de evitar las «listas de reproducción reguladas» y los «formatos rígidos» de la radio y la televisión, de asegurar que los seguidores de todo el mundo pudieran escuchar la música al mismo tiempo y de evitar una posible filtración antes de su lanzamiento físico.

"In Rainbows" se lanzó en forma física en el Reino Unido en diciembre de 2007 a través de XL Recordings y en Norteamérica en enero de 2008 a través de TBD Records, ubicándose en los primeros puestos de ambos países. El éxito comercial del disco en Estados Unidos fue el mayor de Radiohead en el país tras el lanzamiento de "Kid A", mientras que fue su quinto álbum número uno en el Reino Unido. "In Rainbows" vendió más de tres millones de copias en su primer año de lanzamiento. Además recibió críticas muy positivas y se lo destacó por poseer un sonido más accesible y unas letras de estilo más personal que los trabajos anteriores. El álbum recibió nominaciones a los premios Mercury y llegó a ganar en la de los premios Grammy el premio al . Su equipo de producción ganó en la categoría de mejor paquete de edición limitada, mientras que Radiohead obtuvo su tercera nominación en la categoría de álbum del año. Además de otras candidaturas de la banda, destacaron las nominaciones de la producción de Godrich y el video de «House of Cards».

Radiohead lanzó varios sencillos extraídos de "In Rainbows" para promocionar el álbum; «Jigsaw Falling into Place», el primero, se lanzó en el Reino Unido en enero de 2008. El segundo, «Nude» debutó en el puesto 37 del "Billboard Hot 100", convirtiéndose en la primera canción en ingresar a la lista desde «High and Dry» en 1995 y su primer sencillo en entrar al "Top 40" desde «Creep». Radiohead continuó lanzando temas del álbum como sencillos y videos promocionales; en junio de dicho año, se grabó uno de «House of Cards». «House of Cards», junto a «Bodysnatchers» fueron radiodifundidas y en septiembre la banda anunció un cuarto sencillo, «Reckoner». También se organizó una competición para crear una remezcla del mismo, similar a la de «Nude». EMI lanzó un álbum de grandes éxitos llamado "" en ese mismo mes. El compilado se hizo sin el consentimiento del grupo y no contiene ninguna canción de "In Rainbows", ya que por entonces la banda había dejado la discográfica. Yorke expresó su desaprobación en nombre de Radiohead: «No tenemos ningún éxito, así que, ¿cuál es exactamente el propósito de esto? [...] Si lo hubiéramos querido hacer, entonces estaría bien». 

El 24 de junio de 2008 se lanzó en formato digital el video álbum en directo "In Rainbows - From the Basement", que incluía los temas de "In Rainbows" tocados por la banda en el programa de televisión "From the Basement". Desde mayo hasta octubre de 2008, la banda salió de gira por Norteamérica, Europa y Japón. En marzo de 2009 se inició la segunda parte de la gira, en la que el grupo visitó Latinoamérica. Tras 15 años se presentaron en México y, por primera vez, en Brasil, Argentina y Chile. En agosto de 2009 volvió a salir de gira por Europa, actuando en el Festival de Reading, al que no iba desde 1994.

En mayo de 2009 la banda inició nuevas sesiones de grabación con el productor Nigel Godrich. Meses después, en agosto, la banda lanzó a través de su página web dos sencillos grabados durante esas sesiones. El primero de ellos, «Harry Patch (In Memory Of)» fue grabado en tributo a Harry Patch, el último soldado británico vivo que combatió durante la primera guerra mundial y que había fallecido recientemente. El tema se vendió al precio de 1 £ y lo recaudado fue donado a la Legión Británica. En «Harry Patch (In Memory Of)», Thom Yorke interpreta una letra basada en las declaraciones de Patch sobre su propia experiencia en la guerra, teniendo como telón de fondo una orquesta de cuerdas a cargo de Jonny Greenwood. Ese mismo mes, otro nuevo tema, «These Are My Twisted Words», se lanzó en forma de descarga gratuita. Jonny Greenwood explicó que la canción había sido una de las primeras composiciones de las últimas sesiones de estudio de la banda.

A mediados de 2009, en una entrevista para "NME", Yorke sugirió que Radiohead podría volver a enfocarse en los EP, incluyendo la posibilidad de lanzar un EP de música orquestal. En diciembre de ese año, O'Brien declaró en la web de la banda que comenzarían a trabajar en su próximo álbum en enero: «El ambiente es fantástico en la actualidad, y volveremos al estudio en enero para continuar el trabajo que iniciemos el pasado verano [...] hace 10 años estábamos todos juntos (lo que es la banda) en los tiempos de "Kid A" y, aunque estoy tremendamente orgulloso de ese disco, no era un lugar divertido en el que estar. [...] Lo que es tranquilizador ahora, es que somos definitivamente una banda diferente, lo que debería significar que la música también es diferente y ese es el objetivo del juego. [...] mantenerse en movimiento». En junio de 2010, Ed O'Brien manifestó comentarios similares: «Estamos en plena grabación ahora mismo». O'Brien agregó que esperaban poder tener listo el álbum para finales de 2010. En septiembre de 2010, Colin Greenwood mencionó que acababan de terminar un nuevo conjunto de canciones y que habían empezado a plantearse sobre como lo lanzarían «en un entorno digital que ha cambiado de nuevo». Phil Selway agregó ese mismo mes que la banda iba a «hacer balance» del nuevo material y dijo que todo estaba «en el aire».

En enero de 2010, mientras los miembros de Radiohead se encontraban en Los Ángeles para grabar, la banda tocó en su única actuación del año, un concierto a beneficio de Oxfam. Las entradas se subastaron al mejor postor, lo que permitió que en la presentación que tuvo lugar en el Henry Fonda Theater, se recaudaran más de medio millón de dólares para el trabajo de la ONG en Haití, que meses antes había sido golpeado por un devastador terremoto. Un grupo de aficionados editó un video del concierto con imágenes tomadas de diferentes cámaras digitales de los asistentes, que ofrecieron a través de YouTube y BitTorrent en diciembre de 2010, con el apoyo de la banda y con un enlace para donar dinero a Oxfam Internacional. En 2010, otro colectivo de seguidores hicieron un video del concierto de Radiohead en Praga en 2009 que fue distribuido por Internet gratuitamente, con el audio proporcionado por la propia banda. "Live in Praha" y "Radiohead for Haiti" tuvieron cierta repercusión en los principales medios y fueron descritos como ejemplos de la apertura de la banda a los fanes y su actitud positiva hacia las formas no comerciales de distribución por Internet. 

El 14 de febrero de 2011 Radiohead informó a través de su página web de que su nuevo disco se llamaría "The King of Limbs" y que podría descargarse a un precio fijado en menos de una semana. El 18 de febrero, un día antes de lo previsto, la banda mostró en su página el video de «Lotus Flower», uno de los nuevos temas, e informó que el nuevo álbum ya podía ser descargado. El 28 de marzo el disco se lanzó en versión CD y vinilo, y el 9 de mayo salió a la venta la edición "Newspaper", que incluye numerosos extras. El 16 de abril de 2011 la banda lanzó el sencillo «Supercollider/The Butcher» para celebrar el Record Store Day. El grupo reveló que trabajó en ambas canciones durante las sesiones para "The King of Limbs", pero finalmente decidieron no incluirlas en el álbum. «Supercollider», con más de 7 minutos de duración, es la canción de estudio más larga que la banda haya grabado nunca. Los archivos digitales de las canciones fueron puestos sin coste alguno a disposición de cualquier persona que ya había comprado el álbum en la página web de la banda, y además subieron el audio de las canciones a su canal de YouTube. 

El 6 de junio de 2011, la banda anunció el lanzamiento de una serie de remezclas de canciones de "The King of Limbs" realizadas por varios artistas, en vinilos de 12 pulgadas y de naturaleza limitada, que se irían editando de forma eventual en varias tandas a lo largo del verano. Radiohead publicó el audio de todas ellas en su canal de YouTube y en su web para ser escuchado en "streaming". El doble CD "TKOL RMX 1234567", que reunía las 19 remezclas realizadas, fue lanzado el 16 de septiembre en Japón y el 10 de octubre en el resto del mundo. Entre los artistas que colaboraron se encuentran Caribou, Modeselektor, Nathan Fake, Jacques Greene, Lone o Four Tet. El 21 de junio de 2011, un video de la banda tocando un tema nuevo, «Staircase», fue subido a su canal de YouTube como avance de la presentación en el programa "From the Basement". El 24 de junio de 2011, Radiohead actuó de manera sorpresa en el Festival de Glastonbury, donde mostró por primera vez en directo todos los temas de "The King of Limbs" y presentó además un nuevo tema anteriormente tocado por Yorke, «The Daily Mail». La banda actuó con un nuevo instrumentista, Clive Deamer, batería que había colaborado en varias giras con Portishead. En julio de 2011, varios canales de televisión de todo el mundo estrenaron la actuación en directo de la banda en el programa "From the Basement", donde interpretan, junto a Clive Deamer, los ocho temas de "The King of Limbs", así como «The Daily Mail» y «Staircase». La banda ya había aparecido en este programa en 2008 tras el lanzamiento de "In Rainbows". En septiembre los miembros de Radiohead, acompañados por Deamer, se presentaron en los programas estadounidenses "Saturday Night Live" y "The Colbert Report", y Thom y Jonny actuaron además en "Late Night with Jimmy Fallon". Además, la banda realizó dos presentaciones en directo en el Roseland Ballroom de Nueva York.

El 19 de diciembre de 2011 el grupo lanzó a través de descarga digital el video álbum en directo "", que incluía su actuación en el programa de televisión "From the Basement" ese año. Ese mismo día también se lanzó en formato digital el primer single del mismo, «The Daily Mail / Staircase». El 23 de enero de 2012 el video álbum se publicó además en formato físico, tanto en DVD como Blu-Ray, incluyéndose como material extra la interpretación de «Supercollider», que no fue mostrada cuando se emitió el programa por televisión.

En febrero de 2012, Radiohead comenzó la que fue su primera gira en Norteamérica en 4 años, incluyendo fechas en Estados Unidos, Canadá y México. El 16 de junio de 2012, una hora antes de que se abrieran las puertas del Downsview Park de Toronto para el concierto final de la gira norteamericana, el techo del escenario se derrumbó, causando la muerte del técnico de percusión de la banda Scott Johnson e hiriendo a otros tres miembros del equipo técnico de la banda. El derrumbe también destrozó el espectáculo de luces del grupo y gran parte de su equipo musical. Ninguno de los miembros de la banda estaban en el escenario. El concierto fue cancelado y las fechas de la gira europea fueron pospuestas. Radiohead rindió homenaje a Johnson y su equipo técnico en el primer concierto tras el derrumbe, que tuvo lugar en julio en Nimes (Francia). 
Yorke escribió más tarde que terminar la gira tras el colapso era su «mayor logro hasta el momento». En junio de 2013, el Ministerio de Trabajo de Ontario imputó a Live Nation Canada Inc, Live Nation Ontario Concerts GP Inc, Optex Staging & Services Inc y a un ingeniero con 13 cargos en virtud de la Ley de Seguridad y Salud Laboral. El caso comenzó el 27 de junio de 2013 en la Corte de Justicia de Ontario, ubicada en Toronto. La audiencia se inició en noviembre de 2015.

Mientras estaban de gira por EE.UU., a mediados de 2012, Radiohead pasó un día en el estudio de grabación de Jack White, guitarrista exmiembro de White Stripes, donde trabajó en dos nuevas canciones, una de ellas «Identikit». Yorke diría en abril de 2013 que esas grabaciones fueron «tarea inconclusa».

Tras la gira de "The King of Limbs", durante la que Radiohead interpretó varias canciones nuevas, la banda decidió realizar un nuevo paréntesis para trabajar en algunos proyectos independientes. El 25 de febrero de 2013, Atoms for Peace, el grupo de Yorke y Nigel Godrich, publicó su primer álbum, titulado "Amok". El 11 de febrero de 2014, Radiohead lanzó aplicación "Polyfauna" para Android y teléfonos iOS, una «colaboración experimental» entre la banda y el estudio de arte digital Universal Everything en la que se utilizan elementos musicales e imágenes de "The King of Limbs". El 26 de septiembre de 2014, Yorke publicó su segundo álbum en solitario, "Tomorrow's Modern Boxes". El 7 de octubre de 2014, Phil Selway también lanzó su segundo álbum solista, titulado "Weatherhouse", mientras que ese mismo mes se estrenó la banda sonora de la película "Inherent Vice", compuesta por Jonny. Esta última incluyó una reinterpretación de la canción «Spooks», tema de Radiohead inédito en estudio que debutó en directo en 2006.

Radiohead empezó a trabajar en su noveno álbum desde septiembre de 2014 hasta Navidad, retomando la grabación en marzo de 2015, según declaró Selway en una entrevista. Jonny Greenwood manifestó en otra entrevista en febrero de 2015 que: «Ciertamente hemos cambiado nuestro método de nuevo [...] Estamos como limitándonos a nosotros mismos; trabajando en límites [...] estamos intentando usar tecnología muy antigua y muy nueva simultáneamente para ver qué pasa». El 25 de diciembre de 2015, Thom Yorke anunció a través de Twitter la publicación en SoundCloud de «Spectre», tema que escribieron como encargo para la película homónima de James Bond pero que finalmente no fue incluido en la banda sonora.

El 1 de mayo la banda eliminó toda la información de sus cuentas en Twitter y Facebook, así como de su página oficial, la cual quedó completamente en blanco, generando numerosos rumores sobre la salida del álbum. El 3 de mayo publicaron un nuevo tema, «Burn the Witch», junto a su correspondiente videoclip. El 6 de mayo compartieron el segundo sencillo, «Daydreaming», así como el videoclip del mismo, dirigido por Paul Thomas Anderson. Ese mismo día anunciaron que el nuevo álbum se podría descargar el 8 de mayo a las 19:00, hora inglesa, y estaría disponible en formato físico el 17 de junio. El 8 de mayo fue publicado bajo el título "A Moon Shaped Pool". Incluye varias canciones escritas años antes, como «True Love Waits» (que data al menos de 1995), así como cuerdas y coros de la London Contemporary Orchestra. El 20 de mayo de 2016 comenzó la gira mundial del álbum, que duró de mayo a octubre y tuvo fechas en Europa, Norteamérica y Japón. En la misma contaron de nuevo con Clive Deamer como batería adicional. A finales de 2016, Radiohead anunció una segunda parte de la gira entre marzo y julio de 2017, con conciertos en diversos festivales estadounidenses y europeos, incluyendo presentaciones en Noruega, Inglaterra o Irlanda, entre otros. El 23 de junio actuaron en el Festival de Glastonbury, siendo la tercera vez que eran cabezas de cartel.

El 2 de mayo de 2017 Radiohead anunció una reedición especial de "OK Computer", celebrando así los 20 años desde su publicación. Su título es "OKNOTOK" e incluye las canciones originales remasterizadas además de 8 lados B y 3 canciones inéditas: «I Promise», «Man of War» (también conocida como «Big Boots») y «Lift». El álbum se lanzó en formato digital el 23 de junio, y en julio se pudo conseguir en formato CD y en una edición especial en vinilo y casete que incluiye además dibujos, letras y apuntes de Thom Yorke acerca de las canciones. El 2 de junio Radiohead lanzó «I Promise» como sencillo, siendo acompañado por su videoclip en YouTube. El 12 de septiembre se publicó el video para «Lift». El 29 de noviembre Radiohead anunció una nueva gira por Sudamérica en 2018, en la que compartieron cartel con Flying Lotus o Junun. La misma comenzó en Chile el 11 de abril, siguiendo por Argentina, Perú, Brasil y Colombia. En verano de 2018 realizaron varios conciertos en Estados Unidos y Canadá. El 13 de diciembre de 2018 el Salón de la Fama del Rock and Roll anunció la inclusión de Radiohead en una ceremonia que tendría lugar en el Barclays Center de Brooklyn el 29 de marzo de 2019.

El primer álbum solista de Thom Yorke, "The Eraser", se puso a la venta el 10 de julio de 2006 a través XL Recordings en Gran Bretaña y un día después en Estados Unidos. El disco fue producido por Godrich y Jonny Greenwood participó en la composición del tema que da nombre al álbum. "The Eraser" recibió reseñas en general positivas y estuvo nominado en la categoría de mejor álbum de música alternativa en los premios Grammy de . En septiembre de 2009, Yorke publicó dos temas editados en un sencillo: «Feeling Pulled Apart by Horses», que data de las sesiones del año 2001, y la inédita «The Hollow Earth». A finales de 2009 formó el supergrupo Atoms for Peace, con el que salió de gira en 2010, interpretando en directo canciones de su disco solista entre otros temas. 

El 6 de septiembre de 2012, Atoms for Peace lanzó el single «Default» en iTunes, al mismo tiempo que presentaron su página web. El 25 de febrero de 2013 Atoms for Peace publicó su primer álbum, titulado "Amok", bajo el sello XL Recording. Ese mismo año el grupo salió nuevamente de gira. El 26 de septiembre de 2014, Yorke publicó su segundo álbum en solitario, "Tomorrow's Modern Boxes", por medio del portal BitTorrent. El mismo fue producido por Nigel Godrich. En 2018, colaboró como compositor para el "thriller" de horror "Suspiria", del director Luca Guadagnino. Después de una publicidad críptica y confusa que consistía en la colocación de numerosos panfletos alrededor de Londres, los cuales anunciaban un servicio de «recolección de sueños» junto a un número telefónico, Yorke anunció su tercer álbum en solitario, "Anima" (2019), acompañado de una pieza audiovisual homónima lanzada en Netflix.

En 2003, Jonny Greenwood lanzó "Bodysong", un álbum instrumental con la música que compuso para el documental del mismo nombre dirigido por Simon Pummell. La banda sonora incluye canciones interpretadas por una orquesta, la mayoría procesadas electrónicamente, incluyendo desde cuartetos de cuerdas a piano y ondas Martenot. Este fue el primer álbum de un miembro de Radiohead como solista, aunque su hermano Colin también colaboró en el mismo. En 2005 participó junto a Phil Selway en la banda sonora de la película "Harry Potter y el cáliz de fuego" en tres temas compuestos por Jarvis Cocker (vocalista de Pulp), apareciendo ambos como parte de Weird Sisters, la banda que actúa en la fiesta del film. En 2007, Jonny Greenwood compuso la música de la película de Paul Thomas Anderson "There Will Be Blood", protagonizada por Daniel Day-Lewis. La banda sonora recibió nominaciones para los Óscar pero finalmente se la descalificó al contener partes no originales, incluidos algunos pasajes de su anterior álbum, "Bodysong". En 2010 trabajó también en la banda sonora de la película japonesa "Norwegian Wood". En 2011 realizó la música para la película "Tenemos que hablar de Kevin".

El 13 de marzo de 2012, Greenwood lanzó conjuntamente con Krzysztof Penderecki el álbum "Polymorphia", que incluyó una pieza nueva compuesta por él y titulada "48 Responses to Polymorphia", así como su antiguo tema "Popcorn Superhet Receiver". Ambos estaban inspirados en las composiciones de Penderecki "Polymorphia" y "Treno a las Víctimas de Hiroshima" respectivamente, también incluidas en el álbum. En 2012, Jonny realizó la banda sonora de "The Master" de Paul Thomas Anderson, y en 2014 repitió con el director por tercera vez al componer la música de la película "Inherent Vice". En 2015, junto a Shye Ben Tzur y Rajasthan Express, lanzó el álbum "Junun".

En 2017, Greenwood compuso la música de la película "Phantom Thread" de Paul Thomas Anderson. Esta cuarta colaboración con Anderson le valió a Greenwood su primera nominación al y al .

A finales de la década de 1990, Ed O'Brien compuso algunas partes de la banda sonora del programa de televisión británico "Eureka Street". La misma se lanzó en formato CD a través de la BBC. En 2003, O’Brien contribuyó tocando la guitarra en varias canciones de "Enemy of the Enemy", un álbum de la Asian Dub Foundation en el que también participa Sinéad O'Connor. Al igual que Phil Selway, Ed ha grabado y salido de gira con el supergrupo 7 Worlds Collide, un proyecto liderado por Neil Finn de Crowded House, participando en los álbumes "7 Worlds Collide" (2001) y "The Sun Came Out" (2009). 

En 2003, Colin Greenwood tocó el bajo en el tema «24 Hour Charleston», perteneciente a "Bodysong", el primer álbum solista de su hermano Jonny Greenwood. En 2008, en su primer proyecto en el que no estaba envuelto ningún otro miembro de Radiohead, Colin tocó el bajo en la banda sonora de James Lavino para la película "Woodpecker" de Alex Karpovsky. La misma también cuenta con la participación de Lee Sargent y Tyler Sargent, del grupo Clap Your Hands Say Yeah. En 2014, Greenwood contribuyó al álbum "Tomorrow's Modern Boxes" de Thom Yorke programando el "beat" de la canción «Guess Again!».

En 2005, Phil Selway participó junto a Jonny Greenwood en la banda sonora de la película "Harry Potter y el cáliz de fuego" en tres temas compuestos por Jarvis Cocker, vocalista de Pulp.Al igual que Ed O'Brien, Phil ha grabado y salido de gira con el supergrupo 7 Worlds Collide, un proyecto liderado por Neil Finn, participando en los álbumes "7 Worlds Collide" (2001) y "The Sun Came Out" (2009). El 30 de agosto de 2010, Selway lanzó "Familial", su primer álbum solista, en el que canta y toca diversos instrumentos, además de contar con la colaboración de diversos artistas. El 5 de julio de 2011 publicó el EP titulado "Running Blind". El 7 de octubre de 2014, Phil lanzó su segundo álbum solista, titulado "Weatherhouse".

Entre las primeras influencias de los miembros de Radiohead se encuentran Queen, Pink Floyd y Elvis Costello; bandas post-punk como Joy Division, Siouxsie and the Banshees y Magazine; y sobre todo grupos de rock alternativo de la década de 1980 como R.E.M, Pixies, The Smiths y Sonic Youth. Hacia mediados de la década de 1990, Radiohead comenzó a mostrar un interés por la música electrónica, especialmente la de DJ Shadow, al que la banda citó como una de sus influencias en "OK Computer". Otras influencias en el álbum fueron Miles Davis y Ennio Morricone, además de grupos de la década de 1960 como The Beatles y The Beach Boys. Jonny Greenwood también citó al compositor Krzysztof Penderecki como una inspiración para el sonido de "OK Computer". El estilo electrónico de "Kid A" y "Amnesiac" fue el resultado de la admiración de Thom Yorke por el "clicks and cuts", la música ambiental y la IDM, como así también artistas de Warp Records como Autechre y Aphex Twin. El jazz de Charles Mingus, John Coltrane y Miles Davis y bandas de krautrock de la década de 1970 como Can y Neu! fueron otras influencias importantes durante este periodo. El interés de Jonny Greenwood en la música clásica del siglo XX también fue importante, sobre todo la influencia de Krzysztof Penderecki y Olivier Messiaen, que es evidente en varias canciones de "OK Computer" y álbumes posteriores. Greenwood ha tocado las ondas de Martenot, un instrumento electrónico que popularizó Messiaen. En las sesiones de "Hail to the Thief", Radiohead volvió a poner énfasis en un rock más tradicional. The Beatles, The Rolling Stones y particularmente Neil Young, fueron las principales fuentes de inspiración de la banda durante esta etapa. Desde el comienzo de las grabaciones de "In Rainbows", los miembros de Radiohead han mencionado como influencias una gran variedad de músicos experimentales, de rock, electrónica y hip hop, tales como Björk, Liars, Modeselektor y Spank Rock.

Respecto a la influencia de Radiohead en otros músicos, los primeros álbumes de la banda fueron especialmente influyentes en el rock y la música pop británica, mientras que su trabajo posterior ha inspirado a músicos de géneros que van desde el jazz y la música clásica al hip hop, la música electrónica y el R&B. Por otro lado, algunos analistas y músicos, como miembros de Rush, Opeth, Dream Theater y Porcupine Tree, han atribuido a Radiohead la resurgencia del rock progresivo en la música popular debido a sus giros estilísticos e innovaciones. 
Desde su formación, Radiohead ha sido, lírica y musicalmente, liderada por Thom Yorke. Sin embargo, a pesar de que Yorke es el responsable de escribir casi todas las letras, la composición de las canciones es un esfuerzo colectivo, ya que se ha mencionado en varias entrevistas que todos los miembros de la banda tienen algún tipo de papel durante el proceso. Debido a esto, todas las canciones del grupo están oficialmente acreditadas a Radiohead. La sesiones de "Kid A" y "Amnesiac" produjeron un cambio en el estilo musical de Radiohead y un cambio aún más radical en el método de trabajo de la banda. Desde este cambio de una instrumentación de rock tradicional hacia un énfasis en el sonido electrónico, los miembros de la banda han tenido una mayor flexibilidad y ahora normalmente tocan distintos tipos de instrumentos en función de los requisitos particulares de cada canción. En "Kid A" y "Amnesiac", Yorke tocó el teclado y el bajo, mientras que Jonny Greenwood manipuló a menudo las ondas de Martenot en lugar de la guitarra, el bajista Colin Greenwood realizó "samples", y O'Brien y Selway se repartieron la caja de ritmos y la manipulación digital y además encontraron nuevas formas de incorporar sus principales instrumentos, guitarra y batería respectivamente, en el nuevo sonido. Las relajadas sesiones de 2003 para la grabación de "Hail to the Thief" dieron lugar a una dinámica diferente en el grupo. Yorke admitió en una entrevista que «[su] poder sobre la banda se desequilibró por completo y [él] quería quitar el poder al resto a toda costa. [...] Ciertamente es mucho mejor ahora, democracia racional, que es lo que solía haber».

La banda mantiene una estrecha relación con su productor Nigel Godrich, así como con el artista gráfico Stanley Donwood. Godrich alcanzó la fama con Radiohead, trabajando con la banda desde "The Bends", y como productor desde "OK Computer". En ocasiones, ha sido denominado el «sexto miembro» de la banda, en alusión a George Martin, que era llamado el «quinto Beatle». Donwood, otro colaborador habitual de la banda, ha diseñado todas las portadas de los álbumes de Radiohead y el arte visual del grupo desde 1994. Junto con Yorke, ganó un premio Grammy en 2002 en la categoría de mejor paquete por la edición especial por el libreto de "Amnesiac". Otros colaboradores de la banda son Dilly Gent y Peter Clements. Gent ha sido el responsable de la puesta en marcha de todos los videos de Radiohead desde "OK Computer", trabajando con la banda para encontrar al mejor director para cada proyecto. El técnico de sonido de la banda, Peter Clements, o «Plank», ha trabajado con el grupo desde "The Bends", preparando los instrumentos tanto para las grabaciones de estudio como para las actuaciones en directo.


Miembros adicionales en directo 

Fuente: Allmusic.

Álbumes de estudio:

Álbumes recopilatorios:






</doc>
<doc id="19303" url="https://es.wikipedia.org/wiki?curid=19303" title="Chinampa">
Chinampa

Una chinampa (del náhuatl "chinamitl", en la cerca de cañas) es un método mesoamericano antiguo de agricultura y expansión territorial que, a través de una especie de balsas cubiertas con tierra, sirvieron para cultivar flores y verduras, así como para ampliar el territorio en la superficie de lagos y lagunas del Valle de México; haciendo a México-Tenochtitlan una ciudad flotante. Las utilizaban para la agricultura y ganar terreno a las aguas lacustres.

Se trata de una balsa, de armazón hecha con troncos y varas, en ocasiones de considerables dimensiones, sobre la que se deposita tierra vegetal debidamente seleccionada con materias biodegradables como el pasto, hojarasca, cáscaras de diferentes frutas y vegetales, etc. En la chinampa se sembraban ahuejotes para que sus raíces crecieran desde el agua hasta la tierra firme en la ribera de lagunas y arroyos luego de que el sauce crecía, sembraban diferentes cultivos los cuales luego cosechaban.

Se trata de una técnica iniciada en tiempos de la cultura teotihuacana, aunque su máximo desarrollo se consiguió en el siglo XVI. Hacia 1519, esta técnica, por ejemplo, ocupaba casi todo el lago Xochimilco, y su combinación con otras técnicas como la irrigación por canales y la construcción de bancales, permitió sustentar una población muy densa.

A inicios del Formativo Tardío se hizo necesario introducir formas intensivas de producción de alimentos, en particular relacionadas con la agricultura. De este modo, los agricultores aprovecharon los márgenes de los pantanos y de las concentraciones de agua formadas durante la estación húmeda con el fin de obtener suelos mejor irrigados y más ricos, pudiendo conseguir en ocasiones tres cosechas anuales. También, como ocurrió en Río Bec, cultivaron jardines en torno a sus casas -cortijos-, donde plantaron otras plantas que requerían mayor cuidado y que diversificaban su dieta. En la misma región y en las montañas en torno al sitio de Caracol, fueron modificadas numerosas colinas con el fin de contener terrazas agrícolas que aumentaran la producción, a la vez que frenaran la erosión.

Sin embargo, el carácter verdaderamente intensivo de la agricultura vino de la mano de los drenajes y de las modificaciones realizadas en torno a las zonas acuáticas, dando lugar a un sistema que se ha denominado "de campos levantados", de gran similitud a las chinampas del centro de México.

Consisten estos en concentraciones artificiales de tierra, limitadas por canales de agua y situadas en márgenes de ríos y pantanos. Con este sistema, se asegura una suficiente cantidad de tierra fértil bien irrigada, de manera que no es necesario el barbecho en el trabajo de los campos, obteniéndose una producción abundante para alimentar a los ocupantes de los grandes núcleos urbanos. Los bajos de Belice, la región de Río Bec, los márgenes del Candelaria y otros lugares tuvieron este sistema intensivo en la agricultura.
La unidad mínima de residencia de los antiguos mexicas fue la casa, identificada por medio de pequeños montículos de tierra y piedras recubiertos de arcilla. Estas construcciones, de no más de 0,50 m de altura, sostuvieron en el pasado chozas rectangulares de carácter perecedero en las que habitó la población campesina, sea dispersa por el paisaje, sea en los centros urbanos. Esta unidad de habitación puede estar aislada o asociada a otras en torno a un patio, formando un conjunto residencial ocupado por familias extendidas. En ellos, no todos los edificios son viviendas, sino que existen almacenes, cocinas y residencias. Varios de tales conjuntos están ocupados por un linaje.

Este es el sistema básico de asentamiento de los centros mayas, con variaciones en tamaño y volumen, pero cuyos lazos de parentesco y la especialización en las funciones que jugaron en la sociedad fueron un factor de cohesión y de integración social. Cuando estos conjuntos residenciales alcanzan un mayor grado de complejidad, con espacios más amplios y edificios más elaborados, se forman pequeños centros cívicos, dirigidos por élites locales. Estos incluyen pirámides escalonadas y grandes edificios residenciales para los dirigentes del asentamiento.

La categoría más compleja de asentamiento corresponde a los centros cívico-ceremoniales o ciudades, que integraron social, política, económica e ideológicamente amplios territorios. En ellos se incluyen templos, palacios, estelas, juegos de pelota, altares, calzadas, plataformas, grandes depósitos de agua, fortificaciones, arcos, torres y una amplia gama de edificios y conjuntos, los cuales reproducen siempre los grupos residenciales. La diversificación en tamaño urbano y de los edificios que contienen, la cantidad de restos escritos y de elementos complejos de cultura material, manifiestan que algunos centros ejercieron un dominio político o económico sobre otros, siendo los más complejos capitales regionales

En la zona lacustre de la delegación Xochimilco y Tlahuac se encuentran los últimos relictos de agricultura de Chinampas; un antiguo sistema de agricultura de humedal cuyo origen se remonta a más de 900 años, cuando la sociedad Náhua florecía en la cuenca de México (Rojas, 2004), y que ha sido nombrado como uno de los sistemas más sustentables jamás logrados (Jiménez-Osorino et al., 1990; Ezcurra, 1991; Altieri, 2004). La zona de Chinampas en el suelo de conservación se encuentra principalmente ubicado en el área agrícola de tres poblados: Xochimilco, San Gregorio Atlapulco y de forma escasa en San Luis Tlaxialtemalco, lugar donde la agricultura de Invernaderos la ha sustituido.

En la actualidad la zona de Chinampas y demás sistemas agrícolas asociados a suelo lacustre, han sido superados por la urbanización convirtiéndose en una enorme isla de agricultura tradicional urbana en medio de la Ciudad de México. En esta zona se continúa cultivando una variedad de hortalizas y plantas de ornato, las actividades productivas se han diversificado creando condiciones para el desarrollo de la economía local así como provisión de bienes y servicios para la ciudad (Losada, et al., 2000). El turismo, el ganado estabulado, los cultivos de traspatio, la floricultura de Invernaderos y la horticultura de Chinampas, son las principales actividades asociadas al suelo de conservación. Los bienes producidos en estas localidades son vendidos en los centros de distribución de alimentos, mercados locales o exportados a otros estados de la República Mexicana, dada la férrea competencia que se vive en los principales centros de abasto de la ciudad. Otra parte de la producción es consumida en los hogares de los productores.

Asociada a la zona agrícola se encuentra una zona inundada; efecto de los constantes hundimientos del suelo lacustre, que se ha transformado después de varios años (aproximadamente 1990) en un humedal de tipo estacional y permanente, en el cual se realizan procesos ecológicos que tienen implicaciones en la limpieza del aire de la ciudad, remoción de sustancias tóxicas del agua, como nutrimentos de origen agrícola y urbano; así como la promoción de hábitat para una considerable cantidad de especies silvestres 6 de aves, 23 de mamíferos, 212 de aves, 10 de reptiles, 21 de peces y 146 especies de plantas (Ceballos y Galindo, 1984; CONABIO, 2000) y la prestación de servicios edónicos. Esta área de importancia ecológica se localiza en los Ejidos de San Gregorio A. y comprende un área aproximada de 441 ha. La vegetación se compone de especies halófilas, malezas y flotantes con predominancia del tule (Typha spp.), el huachinango (Eichhornia crassipes) y zacate cuadrado (Schoenoplectus americanus). El gradiente ambiental ha promovido que esta zona presente una alta heterogeneidad ambiental, por lo que se han generado una serie de hábitats como el tular, pastizal y popal (INECOL, 2002).

El hecho de estar inmersa en la Ciudad de México, ha significado una fuerte presión por modificar el uso de suelo y cobertura, como promotores han intervenido: la política pública, la agrícola, los grupos organizados en demanda de suelo habitacional, el bajo valor de la tierra agrícola, los asentamientos irregulares, la introducción de paquetes tecnológicos y la degradación ambiental. Sin embargo, se trata de una zona que, tanto por decreto oficial, al designarla Área Natural Protegida, como por la designación de Patrimonio Cultural de la Humanidad, ha sido protegida, mostrando el gran valor que este relicto de agricultura prehispánica y ecosistema lacustre, tiene para las sociedades presentes y futuras.




</doc>
<doc id="19304" url="https://es.wikipedia.org/wiki?curid=19304" title="Sigmund Freud">
Sigmund Freud

Sigmund Freud (Příbor, 6 de mayo de 1856-Londres, 23 de septiembre de 1939) fue un médico neurólogo austriaco de origen judío, padre del psicoanálisis y una de las mayores figuras intelectuales del siglo XX.

Su interés científico inicial como investigador se centró en el campo de la neurología, derivando progresivamente hacia la vertiente psicológica de las afecciones mentales, investigaciones de las que daría cuenta en la casuística de su consultorio privado. Estudió en París, con el neurólogo francés Jean-Martin Charcot, las aplicaciones de la hipnosis en el tratamiento de la histeria. De vuelta a la ciudad de Viena y en colaboración con Josef Breuer desarrolló el método catártico. Paulatinamente, reemplazó tanto la sugestión hipnótica como el método catártico por la asociación libre y la interpretación de los sueños. De igual modo, la búsqueda inicial centrada en la rememoración de los traumas psicógenos como productores de síntomas fue abriendo paso al desarrollo de una teoría etiológica de las neurosis más diferenciada. Todo esto se convirtió en el punto de partida del psicoanálisis, al que se dedicó ininterrumpidamente el resto de su vida.

Freud postuló la existencia de una sexualidad infantil perversa polimorfa, tesis que causó una intensa polémica en la sociedad puritana de la Viena de principios del siglo XX y por la cual fue acusado de "pansexualista". A pesar de la hostilidad que tuvo que afrontar con sus revolucionarias teorías e hipótesis, Freud acabaría por convertirse en una de las figuras más influyentes del siglo XX. Sus teorías, sin embargo, siguen siendo discutidas y criticadas, cuando no simplemente rechazadas. Muchos limitan su aporte al campo del pensamiento y de la cultura en general, existiendo un amplio debate acerca de si el psicoanálisis pertenece o no al ámbito de la ciencia.

La división de opiniones que la figura de Freud suscita podría resumirse del siguiente modo: unos le consideran más un gran científico en el campo de la medicina, que descubrió gran parte del funcionamiento psíquico humano; y otros lo ven especialmente como un filósofo que replanteó la naturaleza humana y ayudó a derribar tabúes, pero cuyas teorías, como ciencia, fallan en un examen riguroso.

El 28 de agosto de 1930, Freud fue galardonado con el Premio Goethe de la ciudad de Fráncfort del Meno por su actividad creativa. También en honor de Freud, al que frecuentemente se le denomina "el padre del psicoanálisis", se dio el nombre «Freud» a un pequeño cráter de impacto lunar que se encuentra en una meseta dentro de Oceanus Procellarum, en la parte noroccidental del lado visible de la Luna.

Sigismund Schlomo Freud nació el 6 de mayo de 1856 en Freiberg, Moravia (en la actualidad, Příbor en la República Checa) en el seno de una familia judía. Aunque el nombre que figura en su certificado de nacimiento es "Sigismund", su padre añadió un segundo nombre, de origen hebreo, "Schlomo" o "Shelomoh" (versiones de "Salomón") en una inscripción manuscrita en la biblia de familia. Un documento de 1871 se refiere a Freud como "Sigmund" aunque él mismo no comienza a firmar "Sigmund" hasta 1875 y nunca usó el segundo nombre. Fue el mayor de seis hermanos (cinco mujeres y un varón). Tenía además dos hermanastros de uno de los dos matrimonios anteriores de su padre. En 1860, cuando contaba con tres años de edad, su familia se trasladó a Viena, esperando el padre recobrar la prosperidad perdida de su negocio de lanas. Según sus propias palabras, «fue educado sin religión y permaneció incrédulo», de modo que sus lazos con el judaísmo no fueron ni religiosos, ni nacionalistas, aunque se identificó siempre con su cultura.

A pesar de que su familia atravesó grandes dificultades económicas, sus padres se esforzaron para que obtuviera una buena educación y en 1873, cuando contaba con 17 años, Freud ingresó en la Universidad de Viena como estudiante de medicina en un ambiente de antisemitismo creciente. En 1877 abrevió su nombre de "Sigismund Freud" a "Sigmund Freud". Estudiante poco convencional pero brillante, fue ayudante del profesor E. Brücke en el Instituto de Fisiología de Viena entre 1876 y 1882.
En 1880 conoció al que sería su mentor Joseph Breuer.

Según se desprende de numerosas cartas entre Freud y su amigo Eduard Silberstein, escritas entre 1871 y 1881, ambos aprendieron el español de manera autodidacta. Incluso formaron una especie de sociedad secreta a la que nombran «Academia Castellana» (AC) y usaron como pseudónimos los nombres de los dos perros protagonistas de "El coloquio de los perros" del "gran Cervantes"; solían firmar Freud como Cipion y Silberstein como Berganza. Publicadas en 1965, las cartas han sido traducidas al inglés, italiano, español y francés.
Las originales se encuentran en la Library of Congress.

En 1881 se graduó como médico.
Freud trabajó bajo la dirección de Theodor Meynert en el Hospital General de Viena entre los años 1883 y 1885. Como investigador médico, Freud fue un pionero al proponer el uso terapéutico de la cocaína como estimulante y analgésico. Entre 1884 y 1887 escribió muchos artículos sobre las propiedades de dicha droga. Sobre la base de las experimentaciones que él mismo realizaba en el laboratorio de neuroanatomía del notable patólogo austríaco y especialista en histología Salomon Stricker, logró demostrar las propiedades de la cocaína como anestésico local.

En 1884 Freud publicó su trabajo "Über Coca" ("Sobre la coca"), al que sucedieron varios artículos más sobre el tema. Aplicando los resultados de Freud, pero sin citarlo, Carl Koller utilizó con gran éxito la cocaína en cirugía e intervenciones oftalmológicas publicando al respecto y obteniendo por ello un gran reconocimiento científico.
Se ha podido determinar ―tras la publicación de las cartas a su entonces prometida y luego esposa, Martha Bernays― que Freud hizo un intento frustrado de curar con cocaína a su amigo Ernst von Fleischl-Marxow, quien era adicto a la morfina, pero el tratamiento solo le agregó una nueva adicción, hasta que finalmente falleció. Se le critica a Freud no haber admitido públicamente este fracaso, y a su biógrafo y amigo Ernest Jones que no haya informado de él. Es también conocido que el propio Freud consumió cocaína en algún período de su vida, según se puede leer en la versión completa de su correspondencia con Wilhelm Fliess.

En 1886, Freud se casó con Martha Bernays y abrió una clínica privada especializada en trastornos nerviosos. Comenzó su práctica para tratar neurosis como la histeria utilizando la hipnosis y el método catártico que su mentor Josef Breuer había aplicado con Bertha Pappenheim (Anna O.) obteniendo resultados que en aquel momento parecían sorprendentes, para posteriormente abandonar ambas técnicas en favor de la asociación libre, desarrollada por él entre los años 1895 y 1900, impulsado por las experiencias con sus pacientes histéricas. Freud notó que podía aliviar sus síntomas animándolas a que verbalizaran sin censura cualquier ocurrencia que pasara por su mente.

En 1899 se publicó la que es considerada como su obra más importante e influyente, "La interpretación de los sueños", inaugurando una nueva disciplina y modo de entender la mente humana, el psicoanálisis. Tras algunos años de aislamiento personal y profesional debido a la incomprensión e indignación que en general sus teorías e ideas provocaron, comenzó a formarse un grupo de adeptos en torno a él, el germen del futuro movimiento psicoanalítico. Sus ideas empezaron a interesar cada vez más al gran público y se fueron divulgando pese a la gran resistencia que suscitaban.

El primer reconocimiento oficial como creador del psicoanálisis fue en 1902 al recibir el nombramiento imperial como Profesor extraordinario, hecho que Freud comentaría en una carta a Wilhelm Fliess fechada en Viena el 11 de marzo de 1902, señalando sarcásticamente que esto era «...como si de pronto el papel de la sexualidad fuera reconocido oficialmente por su Majestad...»

Internacionalmente, obtuvo su primer reconocimiento oficial en 1909, cuando la Universidad de Clark, en Worcester, Massachusetts, le concedió el título honorífico "doctor honoris causa".
G. Stanley Hall lo invitó dar una serie de conferencias como parte de las celebraciones con motivo del vigésimo aniversario de la fundación de la universidad que presidía, con la intención de divulgar el psicoanálisis en los Estados Unidos.

Freud experimentó la primera disensión interna a su doctrina en octubre de 1911 cuando Alfred Adler y seis de sus partidarios se dieron de baja de la Asociación Psicoanalítica Vienesa.
Por esta época ya se gestaba la que Carl Gustav Jung protagonizaría en 1914, con más graves consecuencias y que amenazaría con desestabilizar todo el edificio psicoanalítico.

En 1923 se le diagnosticó un cáncer de paladar, probablemente a consecuencia de su intensa adicción a los puros, del que fue operado hasta 33 veces. Su enfermedad, aparte de provocarle un gran sufrimiento, una gran incapacidad y finalmente sordera del oído derecho, lo obligó a usar una serie de incómodas prótesis de paladar que le dificultaron mucho la capacidad del habla.
Nunca dejó de fumar, con las consecuencias que esto le acarreó. A pesar de su enfermedad, Freud continuó trabajando como psicoanalista y, hasta el fin de su vida, no cesó de escribir y publicar un gran número de artículos, ensayos y libros.

Toda la vida de Freud, con la excepción de sus tres primeros años, transcurrió en la ciudad de Viena. Sin embargo, en 1938, tras la anexión de Austria por parte de la Alemania nazi, Freud, en su condición de judío y fundador de la escuela psicoanalítica, fue considerado enemigo del Tercer Reich. Sus libros fueron quemados públicamente y tanto él como su familia sufrieron un intenso acoso. Reacio a abandonar Viena, se vio obligado a escapar del país al quedar claro el inminente peligro que corría su vida. En un allanamiento de la casa donde operaba la editorial psicoanalítica y de su vivienda, su hijo Martin fue detenido durante todo un día. Una semana más tarde, su hija Anna fue interrogada en el cuartel general de la Gestapo. Estos hechos lo llevaron a convencerse de la necesidad de partir.
El hecho de que sus hermanas (cuatro de ellas permanecieron en Viena) fueran apresadas más tarde y murieran en campos de concentración confirma "a posteriori" que el riesgo vital era cierto. Gracias a la intervención "in extremis" de Marie Bonaparte y Ernest Jones consiguió salir del país y refugiarse en Londres, Inglaterra. En el momento de partir se le exigió que firmara una declaración donde se aseguraba que había sido tratado con respeto por el régimen nazi.

El 23 de septiembre de 1939, muy deteriorado físicamente e incapaz de soportar el dolor que le producía la propagación del cáncer de paladar, le recordó a su médico personal, Max Schur, su promesa de sedación terminal para ahorrarle el sufrimiento agónico.
Freud murió después de serle suministradas tres inyecciones de morfina.
Fue incinerado en el crematorio laico de Golders Green, donde reposan sus cenizas junto a las de su esposa Martha.

A pesar de los implacables y a menudo apremiantes desafíos a los que sus ideas tuvieron que enfrentarse, tanto en vida como una vez desaparecido, Freud se convirtió y sigue siendo una de las figuras más influyentes del pensamiento contemporáneo.

Su hija Anna Freud fue una destacada psicoanalista, particularmente en el campo de la infancia y del desarrollo psicológico. Sigmund Freud fue abuelo del pintor Lucian Freud y del escritor Clement Freud. Fue bisabuelo de la periodista Emma Freud, de la diseñadora de moda Bella Freud y del relacionador público Matthew Freud. También fue tío de Edward Bernays, conocido como el padre de las relaciones públicas.

Freud innovó en dos campos. Desarrolló simultáneamente, por un lado, una teoría de la mente y de la conducta humana; y por otro, una técnica terapéutica para ayudar a personas con afecciones psíquicas. Algunos de sus seguidores afirman estar influidos por uno, pero no por otro campo.

Probablemente, la contribución más significativa que ha hecho al pensamiento moderno es la de intentar darle un estatus científico (no compartido por varias ramas de la ciencia y la psicología) al concepto de lo inconsciente (que tomó de Eduard von Hartmann, Schopenhauer y Nietzsche). Sus conceptos de «inconsciente», «deseo inconsciente» y «represión» fueron revolucionarios. Proponen una mente dividida en capas o niveles, dominada en cierta medida por una voluntad primitiva, más allá de la esfera consciente y que se manifiesta en «producciones» tales como chistes, lapsus, actos fallidos, sueños y síntomas.

En su obra más conocida, "La interpretación de los sueños" ("Die Traumdeutung", 1900), Freud explica el argumento para postular el nuevo modelo del inconsciente y desarrolla un método para conseguir acceder al mismo, tomando elementos de sus experiencias previas.
Como parte de su teoría, postula también la existencia de un preconsciente, que describe como la capa entre el consciente y el inconsciente (el término subconsciente es utilizado popularmente, pero no forma parte de la terminología psicoanalítica). La represión, por su parte, tiene gran importancia en el conocimiento de lo inconsciente. De acuerdo con Freud, las personas experimentan a menudo pensamientos y sentimientos tan dolorosos que no pueden soportarlos. Freud se refiere a esta idea a lo largo de toda su obra, principalmente en sus "Trabajos sobre metapsicología".
Según sostuvo, estos pensamientos y sentimientos (al igual que los recuerdos asociados) no pueden ser expulsados de la mente, pero sí del consciente para formar parte del inconsciente, manteniendo lo reprimido su efectividad psíquica y retornando en forma de alguna de sus producciones.

Aunque a lo largo de su carrera Freud intentó encontrar patrones de represión entre sus pacientes que derivasen en un modelo general para la mente, observó que sus distintos pacientes reprimían hechos diferentes. Además, advirtió que el proceso de la represión es en sí mismo un acto no consciente (es decir, no ocurriría a través de la intención de los pensamientos o sentimientos conscientes).

Freud buscó una explicación a la forma de operar de la mente. Propuso una estructura de la misma dividida en tres partes: el ello, el yo y el superyó (véase ello, yo y superyó):

Freud estaba especialmente interesado en la dinámica de estas tres partes de la mente. Argumentó que esa relación está influenciada por factores o energías innatos, que llamó pulsiones. Describió dos pulsiones antagónicas:

Freud también sostuvo que la libido madura en los individuos por medio del cambio de su objeto. Argumentó que la sexualidad infantil es «polimórficamente perversa», en el sentido de que una gran variedad de objetos pueden ser una fuente de placer. Conforme las personas se desarrollan, se fijan sobre diferentes objetos específicos en distintas fases:

El modelo psicosexual que desarrolló Freud se ha criticado desde diferentes frentes. Algunos han atacado su afirmación sobre la existencia de una "sexualidad infantil" (e implícitamente la expansión que hizo en la noción de sexualidad). Otros autores, en cambio, consideran que no amplió los conocimientos sobre sexualidad (que tenían antecedentes en la psiquiatría y la filosofía de autores como Schopenhauer), sino que «neurotizó» la sexualidad al relacionarla con conceptos como incesto, perversión y trastornos mentales. Ciencias como la antropología y la sociología argumentan que el patrón de desarrollo propuesto por Freud no es universal ni necesario en el desarrollo de la salud mental, calificándolo de etnocéntrico por omitir determinantes socio-culturales.

Freud esperaba probar que su modelo, basado en observaciones de la clase media austríaca, era válido universalmente. Utilizó la mitología griega y la etnografía contemporánea como modelos comparativos. Acudió al "Edipo Rey" de Sófocles para indicar que el ser humano desea el incesto de forma natural y cómo se reprime ese deseo. El complejo de Edipo fue descrito como una fase del desarrollo psicosexual y de madurez. También se fijó en los estudios antropológicos sobre totemismo, argumentando que reflejan una costumbre ritualizada del complejo de Edipo ("Tótem y tabú"). Incorporó en su teoría conceptos de la religión católica y judía, así como principios de la sociedad victoriana sobre represión, sexualidad y moral; y otros de la biología y la hidráulica.

Esperaba que su investigación proporcionara una sólida base científica para su método terapéutico. El objetivo de la terapia freudiana o psicoanálisis es, relacionando conceptos de la mente cartesiana y de la hidráulica, mover los pensamientos y sentimientos reprimidos (explicados como una forma de energía) hacia el consciente. Al inicio de sus trabajos con Breuer, Freud pensaba que esto podía realizarse a través de la catarsis, que conllevaría automáticamente la cura. Sin embargo, al poco tiempo Freud abandona ambas ideas en beneficio del método de la asociación libre y de la interpretación de los sueños. Con ello, también deja atrás la hipnosis y toda forma de técnica sugestiva. Así inaugura la técnica psicoanalítica propiamente dicha, a la que agrega otro elemento central: a través de la relativamente poca intervención del psicoanalista, que adopta una postura neutral y abstinente, el paciente puede proyectar sus pensamientos y sentimientos sobre él. A través de este proceso, llamado transferencia, el paciente puede reconstruir y resolver conflictos reprimidos (causantes de su enfermedad), especialmente conflictos de la infancia con sus padres.

Es menos conocido su interés inicial por la neurología. En los comienzos de su carrera había investigado la parálisis cerebral. Publicó numerosos artículos médicos en este campo. También mostró que la enfermedad existía mucho antes de que otros investigadores de su tiempo tuvieran noticia de ella y la estudiaran. Sugirió que era erróneo que esta enfermedad, que había descrito William Little (cirujano ortopédico británico), tuviera como causa una falta de oxígeno durante el nacimiento. En cambio, dijo que las complicaciones en el parto eran solo un síntoma del problema. No fue hasta la década de 1980 cuando sus especulaciones fueron confirmadas por investigadores más modernos.

Las hipótesis y métodos introducidos por Freud fueron polémicos durante su vida y lo siguen siendo en la actualidad, pero pocos discuten su enorme impacto en la psicología y la psiquiatría.

Freud desarrolló la llamada «cura del habla» que posibilitaría la mitigación y desaparición de los síntomas histéricos y neuróticos a través de un monólogo sin censura con el analista. Este, ubicado fuera de la vista del analizado, atendería con atención flotante y respetaría las reglas de la neutralidad y abstinencia, es decir, evitando juicios morales o de valor y no entregando satisfacciones sustitutas al analizado.

En momentos clave del trabajo asociativo, el analista haría intervenciones para interpretar el material expuesto. En la descripción inicial de la técnica, este proceso no tendría más finalidad que rememorar (hacer conscientes) ideas o recuerdos de eventos que, por ser dolorosos, humillantes o simplemente intolerables para el sujeto, fueron reprimidos en el inconsciente. Trayendo todo este material reprimido a la conciencia se le haría perder su poder patógeno y los síntomas desaparecerían. Este proceso, sencillo sobre el papel, supone un esfuerzo intenso para el analizado, ya que, las mismas fuerzas que otrora posibilitaron la represión hacia el inconsciente de las ideas y recuerdos traumáticos, se opondrían virulentamente a que sean traídos a la conciencia, es decir, a ser recordados. Estas fuerzas que se oponen al avance de la terapia y a la mejora del analizado se denominan resistencias.

En una época posterior de su trabajo, Freud descubriría que no basta con simplemente «hacer consciente lo inconsciente». En los "Nuevos consejos sobre la técnica del psicoanálisis" (1914), particularmente en el trabajo "Recordar, repetir y reelaborar", introduce el concepto de reelaboración ("durcharbeiten") de las resistencias, como otra pieza central del trabajo analítico «...que produce el máximo efecto alterador sobre el paciente y que distingue al tratamiento analítico de todo influjo sugestivo».

Los desarrollos teóricos tras la publicación de "Más allá del principio del placer" en 1920 tendrán nuevas implicaciones para la técnica terapéutica analítica. En esta obra, Freud realiza una redefinición de su primera teoría de las pulsiones e introduce la pulsión de muerte. La inercia del síntoma en la cura analítica queda explicada a partir de entonces a través de la compulsión de repetición movilizada por la pulsión de muerte.

Finalmente, Freud retoma el tema de la técnica en 1937 en los textos "Análisis terminable e interminable" (1937) y "Construcciones en el análisis" (1937) ambos trabajos de tono menos entusiasta (según apunta James Strachey en el prólogo) en los que describe de manera más realista los alcances y limitaciones de su técnica.

La obra de Freud tuvo un enorme impacto en las ciencias sociales, especialmente en la Escuela de Frankfurt y la teoría crítica.
Además, muchos filósofos han discutido sus teorías y sus implicaciones en el contexto del pensamiento occidental. El modelo de la mente de Freud se considera a menudo un desafío para la filosofía moderna.

El freudomarxismo es un intento de hacer compatibles y complementarias las teorías de Sigmund Freud y Karl Marx.

Freud también ha tenido una influencia duradera y de gran alcance en la cultura popular. Muchas de sus ideas generales ganaron su lugar en el pensamiento cotidiano: el «lapsus freudiano», el «complejo de Edipo», entre otras.


Freud utilizó seudónimos en sus historias clínicas. Muchas de las personas identificadas por seudónimos fueron rastreadas hasta su verdadera identidad por Peter Swales. Algunos pacientes conocidos por seudónimos fueron:

Otros pacientes famosos son:

Entre las personas que no fueron pacientes pero cuyas observaciones psicoanalíticas fueron publicadas, están:



<br>


</doc>
<doc id="19319" url="https://es.wikipedia.org/wiki?curid=19319" title="Escorbuto">
Escorbuto

El escorbuto es una avitaminosis producida por el déficit de vitamina C, necesaria para la síntesis de colágeno en los humanos. El nombre químico de la vitamina C, ácido ascórbico, proviene de la raíz latina "scorbutus". Era común en los marinos que subsistían con dietas en las que no figuraban fruta fresca ni hortalizas (reemplazando estas con granos secos y carne salada). Fue reconocida hace más de dos siglos por el médico naval escocés James Lind, que la prevenía o curaba añadiendo cítricos a la dieta.

La palabra llegó al español a través del francés «"SCORBUT"», desde su origen en los Países Bajos, del bajo sajón medio «"schorbûk"» o el temprano neerlandés moderno «"schorbuyck"», aproximadamente «ruptura de vientre».

La síntesis normal del colágeno depende de la hidroxilación correcta de la lisina y la prolina (para obtener hidroxiprolina e hidroxilisina) en el retículo endoplasmático. Dicha hidroxilación la llevan a cabo la lisil y prolil hidroxilasa, enzimas que necesitan el ácido ascórbico (vitamina C) como coenzima. La deficiencia de ácido ascórbico impide la correcta hidroxilación de estos, por tanto se obtienen cadenas de procolágeno defectuosas y la síntesis no puede finalizarse correctamente.

Las características de la enfermedad consisten en pápulas perifoliculares hiperqueratósicas en las que los pelos se fragmentan y caen; hemorragias perifoliculares; púrpura que se inicia en la parte posterior de las extremidades inferiores y acaba confluyendo y formando equimosis; hemorragias en los músculos de los brazos y las piernas con flebotrombosis secundarias; hemorragias intraarticulares; hemorragias en astilla en los lechos ungueales; afectación de las encías, sobre todo en personas con dientes que comprenden hinchazón, friabilidad, hemorragias, infecciones secundarias y aflojamiento de los dientes; mala cicatrización de las heridas y reapertura de las recientemente cicatrizadas; hemorragias petequiales en las vísceras; y alteraciones emocionales. Pueden aparecer síntomas similares a los del síndrome de Sjögren. En estados terminales son frecuentes la ictericia, el edema y la fiebre, y pueden producirse súbitamente convulsiones, shock y muerte. La enfermedad se cura solamente comiendo fruta en buen estado.

Es frecuente la anemia normocrómica y normocítica, que se debe a las hemorragias tisulares. La anemia puede ser macrocítica o megaloblástica en la quinta parte de los pacientes. Muchos de los alimentos que contienen vitamina C también contienen folatos y las dietas que provocan escorbuto también pueden inducir el déficit de estos. Sin embargo, el déficit de ácido ascórbico produce además un aumento de la oxidación del ácido formil tetrahidrofólico a metabolitos inativos de folatos. No se conoce con exactitud si en la patogenia de la anemia interviene también una alteración en la distribución y almacenamiento del hierro. La anemia se corrige con el aporte de vitamina C y con la instauración de una dieta equilibrada.

En algunos hospitales se utiliza la determinación de los niveles de ácido ascórbico en las plaquetas para establecer el diagnóstico de escorbuto, pues en esta enfermedad su valor suele ser inferior a la cuarta parte de la cifra normal. Los niveles plasmáticos de la vitamina guardan peor correlación con el estado clínico. En los lactantes, las alteraciones radiológicas óseas pueden ser diagnosticadas. La bilirrubina está a menudo elevada. La fragilidad capilar es normal.

Las dosis habituales de vitamina C en los adultos es de 100 mg tres a cinco veces al día por vía oral hasta que se hayan administrado 4 gramos, siguiendo después con 100 mg/día. En los lactantes y niños pequeños, la posología adecuada es de 10 a 25 mg tres veces al día. A la vez se establece una dieta rica en vitamina C. Las hemorragias espontáneas suelen cesar en 24 horas, los dolores musculares y óseos ceden con rapidez, y las encías comienzan a curar en dos a tres días. Incluso los grandes hematomas o equimosis regresan en diez a doce días, aunque las alteraciones pigmentarias en las zonas de grandes hemorragias pueden persistir durante meses. La bilirrubina sérica se normaliza en tres a cinco días y la anemia se suele corregir en dos a cuatro semanas o meses.

La mayor parte de las especies animales son capaces de sintetizar la vitamina C, al poseer la enzima , (GULO, por sus siglas en inglés). Como destacadas excepciones podemos citar a la mayor parte de los quirópteros, y a primates superiores muy relacionados taxonómicamente con los humanos, en concreto el suborden Anthropoidea (Haplorrhini) que incluye tarsiformes y simios. Tampoco sintetizan ácido ascórbico dos especies de Caviidae, el capibara y el conejillo de indias. Variadas fuentes se contradicen sobre el escorbuto en animales de compañía, como perros y gatos.



</doc>
<doc id="19320" url="https://es.wikipedia.org/wiki?curid=19320" title="Econometría">
Econometría

La econometría (del griego οἰκονόμος "oikonómos" 'regla para la administración doméstica' y μετρία "metría", 'relativo a la medida') es la rama de la economía que hace un uso extensivo de modelos matemáticos y estadísticos así como de la programación lineal y la teoría de juegos para analizar, interpretar y hacer predicciones sobre sistemas económicos, prediciendo variables como el precio de bienes y servicios, tasas de interés, tipos de cambio, las reacciones del mercado, el coste de producción, la tendencia de los negocios y las consecuencias de la política económica.

La economía, perteneciente a las ciencias sociales, trata de explicar el funcionamiento del sistema económico en sus distintos aspectos, como producción, consumo, dinero, distribución del ingreso, etc. La herramienta más utilizada por los economistas es la construcción de modelos económicos teóricos y matemáticos que describan el comportamiento de los agentes económicos. Sin embargo, esos modelos deben contrastarse con los datos disponibles para saber si éstos tienen capacidad explicativa y predictiva, y poder en definitiva optar entre unas u otras opciones. La construcción de tales modelos es la finalidad de la econometría. 

Los econometristas, econometras o económetras (economistas cuantitativos) han tratado de emular a las ciencias naturales (física, química) con mejor o peor resultado a través del tiempo. Hay que considerar que tratan con uno de los fenómenos más complejos que conocemos, el comportamiento de las personas y su interacción. Actualmente, la econometría no necesariamente requiere o presupone una teoría económica subyacente al análisis econométrico. Más aún: la econometría moderna se precia de prescindir voluntariamente de la teoría económica por considerarla un obstáculo si se quiere realizar un análisis riguroso (ésta es, por ejemplo, la filosofía del método de Vector Autorregresivos - VAR o recientemente el "data mining").

En la elaboración de la econometría se unen la matemática, la estadística, la investigación social y la teoría económica. El mayor problema con el que se enfrentan los económetras en su investigación es la escasez de datos, los sesgos que pueden presentar los datos existentes, los sesgos del propio investigador y la ausencia o insuficiencia de una teoría económica adecuada. Aun así, la econometría es la única aproximación científica al entendimiento de los fenómenos económicos.

Entre las definiciones de econometría que los economistas relevantes han formulado a lo largo de la historia, podemos destacar las siguientes: 











La econometría se ocupa de obtener, a partir de los valores reales de variables económicas y a través del análisis estadístico y matemático (mas no de la teoría económica, como si se usa en las ciencias naturales, como la física), los "valores" que tendrían los "parámetros" (en el caso concreto de la estimación paramétrica) de los modelos en los que esas variables económicas aparecieran, así como de "comprobar el grado de validez" de esos modelos, y ver en qué medida estos modelos pueden usarse para explicar la economía de un agente económico (como una empresa o un consumidor), o la de un agregado de agentes económicos, como podría ser un sector del mercado, o una zona de un país, o todo un país, o cualquier otra zona económica; su evolución en el tiempo (por ejemplo, decir si ha habido o no cambio estructural), poder predecir valores futuros de la variables, y sugerir medidas de política económica conforme a objetivos deseados (por ejemplo, para poder aplicar técnicas de optimización matemática para racionalizar el uso de recursos dentro de una empresa, o bien para decidir qué valores debería adoptar la política fiscal de un gobierno para conseguir ciertos niveles de recaudación impositiva).

La econometría, igual que la economía, tiene como objetivo explicar una variable en función de otras. Esto implica que el punto de partida para el análisis econométrico es el modelo económico y este se transformará en modelo econométrico cuando se han añadido las especificaciones necesarias para su aplicación empírica. Es decir, cuando se han definido las variables (endógenas, exógenas) que explican y determinan el modelo, los parámetros estructurales que acompañan a las variables, las ecuaciones y su formulación en forma matemática, la perturbación aleatoria que explica la parte no sistemática del modelo, y los datos estadísticos.

A partir del modelo econométrico especificado, en una segunda etapa se procede a la estimación, fase estadística que asigna valores numéricos a los parámetros de las ecuaciones del modelo. Para ello se utilizan métodos estadísticos como pueden ser: mínimos cuadrados ordinarios, máxima verosimilitud, mínimos cuadrados bietápicos, etc. Al recibir los parámetros el valor numérico definen el concepto de estructura que ha de tener valor estable en el tiempo especificado.

La tercera etapa en la elaboración del modelo es la verificación y contrastación, donde se someten los parámetros y la variable aleatoria a unos contrastes estadísticos para cuantificar en términos probabilísticos la validez del modelo estimado. 

La cuarta etapa consiste en la aplicación del modelo conforme al objetivo del mismo. En general los modelos econométricos son útiles para:


También se conoce como teoría de la regresión lineal, y estará más desarrollado en la parte estadística. No obstante, aquí se dará un resumen general sobre la aplicación del método de mínimos cuadrados.

Se parte de representar las relaciones entre una variable económica endógena y una o más variables exógenas de forma lineal, de la siguiente manera:

o bien:
"Y" es la variable endógena, cuyo valor es determinado por las exógenas, formula_1 hasta formula_2. Cuales son las variables elegidas depende de la teoría económica que se tenga en mente, y también de análisis estadísticos y económicos previos. El objetivo buscado sería obtener los valores de los parámetros desde formula_3 hasta formula_4. A menudo este modelo se suele completar añadiendo un término más a la suma, llamado término independiente, que es un parámetro más a buscar. Así:

o bien:
En el que formula_5 es una constante, que también hay que averiguar. A veces resulta útil, por motivos estadísticos, suponer que siempre hay una constante en el modelo, y contrastar la hipótesis de si es distinta, o no, de cero para reescribirlo de acuerdo con ello.

Además, se supone que esta relación no es del todo determinista, esto es, existirá siempre un cierto grado de error aleatorio (en realidad, se entiende que encubre a todas aquellas variables y factores que no se hayan podido incluir en el modelo) que se suele representar añadiendo a la suma una letra representa una variable aleatoria. Así:

o bien:
Se suele suponer que formula_6 es una variable aleatoria normal, con media cero y varianza constante en todas las muestras (aunque sea desconocida), representado de forma matemática como formula_7

Se toma una muestra estadística, que corresponda a observaciones de los valores que hayan tomado esas variables en distintos momentos del tiempo (o, dependiendo del tipo de modelo, los valores que hayan tomado en distintas áreas o zonas o agentes económicos a considerar). 

Por ejemplo, en un determinado modelo podemos estar interesados en averiguar como la renta ha dependido de los niveles de precios, de empleo y de tipos de interés a "lo largo de los años" en "cierto país", mientras que en otro podemos estar interesados en ver como, a "lo largo de un mismo año", ha dependido la renta "de distintos países" de esas mismas variables. Por lo que tendríamos que observar, en el primer caso, la renta, niveles de empleo, precios y tipos de interés del año 1, lo mismo, pero del año 2, etcétera, para obtener la muestra a lo largo de varios años, mientras que en el segundo caso tendríamos que tener en cuenta los valores de cada uno de los países para obtener la muestra. Cada una de esas observaciones para cada año, o país, se llamaría observación muestral. Nótese que aún se podría hacer un análisis más ambicioso teniendo en cuenta "país y año".

Una vez tomada la muestra, se aplica un método, que tiene su justificación matemática y estadística, llamado método de mínimos cuadrados. Este consiste en, básicamente, minimizar la suma de los errores (elevados al cuadrado) que se tendrían, suponiendo distintos valores posibles para los parámetros, al estimar los valores de la variable endógena a partir de los de las variables exógenas en cada una de las observaciones muestrales, usando el modelo propuesto, y comparar esos valores con los que realmente tomó la variable endógena. Los parámetros que lograran ese mínimo, el de las suma de los errores cuadráticos, se acepta que son los que estamos buscando, de acuerdo con criterios estadísticos.

También, este método nos proporcionará información (en forma de ciertos valores estadísticos adicionales, que se obtienen además de los parámetros) para ver en qué medida los valores de los parámetros que hemos obtenido resultan fiables, por ejemplo, para hacer contrastes de hipótesis, esto es, ver si ciertas suposiciones que se habían hecho acerca del modelo resultan, o no, ciertas. Se puede usar también esta información adicional para comprobar si se pueden prescindir de algunas de esas variables, para ver si es posible que los valores de los parámetros hayan cambiado con el tiempo (o si los valores de los parámetros son diferentes en una zona económica de los de otra, por ejemplo), o para ver en qué grado son válidas predicciones acerca del futuro valor de la variable endógena si se supone que las variables exógenas adoptarán nuevos valores.

El método de los mínimos cuadrados tiene toda una serie de problemas, cuya solución, en muchas ocasiones aproximada, ha estado ocupando el trabajo de los investigadores en el campo de la econometría.

De entrada, el método presupone que la relación entre las variables es lineal y está bien especificada. Para los casos de no linealidad se recurre, bien a métodos para obtener una relación lineal que sea equivalente, bien a aproximaciones lineales, o bien a métodos de optimización que absorban la relación no lineal para obtener también unos valores de los parámetros que minimicen el error cuadrático.

Otro supuesto del modelo es el de normalidad de los errores del modelo, que es importante de cara a los contrastes de hipótesis con muestras pequeñas. No obstante, en muestras grandes el teorema del límite central justifica el suponer una distribución normal para el estimador de mínimos cuadrados.

No obstante, el problema se complica considerablemente, sobre todo a la hora de hacer contrastes de hipótesis, si se cree que la varianza de los errores del modelo cambia con el tiempo. Es el fenómeno conocido como heterocedasticidad (el fenómeno contrario es la homocedasticidad). Este fenómeno se puede detectar con ciertas técnicas estadísticas. Para resolverlo hay que usar métodos que intenten estimar el cambiante valor de la varianza y usar lo obtenido para corregir los valores de la muestra. Esto nos llevaría al método conocido como mínimos cuadrados generalizados. Una versión más complicada de este problema es cuando se supone que, además, no solo cambia la varianza del error sino que también los errores de distintos periodos están correlacionados, lo que se llama autocorrelación. También hay métodos para detectar este problema y para corregirlo en cierta medida modificando los valores de la muestra, que también son parte del método de los mínimos cuadrados generalizados.

Otro problema que se da es el de la multicolinealidad, que generalmente sucede cuando alguna de las variables exógenas en realidad depende, también de forma estadística, de otra variable exógena del mismo modelo considerado, lo que introduce un sesgo en la información aportada a la variable endógena y puede hacer que el método de mínimos cuadrados no se pueda aplicar correctamente. Generalmente la solución suele ser averiguar qué variables están causando la multicolinealidad y reescribir el modelo de acuerdo con ello.

También hay que tener en cuenta que en ciertos modelos puede haber relaciones dinámicas, esto es, que una variable exógena dependa, además, de los valores que ella misma y/u otras variables tomaron en tiempos anteriores. Para resolver estos problemas se estudian lo que se llama modelos de series temporales.

Entre los programas más empleados se encuentran SAS, Stata, RATS, TSP, SPSS, Limdep y WinBugs. Para más detalles, se pueden observar las siguientes referencias.

R como tal es un lenguaje de programación a la vez que es una herramienta para aplicar este la econometría de forma muy poderosa. Por otro lado, la econometría con ayuda de programas o lenguajes de programación y en un sentido estricto, no requiere que sea especializado. Los análisis de corte econométrico puede hacerse en Java, J, C, C++, C#, Python, Perl, Scheme, K, S (la base principal de R junto con Scheme) y los derivados de estos lenguajes también, entre otra cantidad importante de dialectos o lenguajes de programación.

Como ejemplo del párrafo anterior, SPSS es un software inicialmente creado para análisis estadísticos en ciencias sociales (ver artículo en Wikipedia). R inicialmente como un proyecto derivado de S y con finalidad más bien estadística. Otor ejemplo al respecto, Stata es un programa estadístico, pero permite poderosos análisis en econometría.

Gretl está enfocado a hacer la interfaz muy amigable con el econometra, además de servir con eficiencia para las series de tiempo. Eviews, que debe el nombre a " Econometrical Views " ("Vistas econométricas"), tiene como fin netamente inicial, la econometría; por esta razón, despliega una cantidad apropiada, pero poco personalizable, de información altamente útil para estos análisis.

Incluso las calculadoras científicas más avanzadas pueden llegar a tener algunos elementos básicos para la elaboración y comprobación de modelos econométricos. Basta con que pueda graficar y en las regresiones se logre calcular, por cualquier medio, que formula_6 es una variable aleatoria normal (formula_9). En caso de no serlo, se requerirían más pasos en la calculadora. Incluso, sin ser calculadores, puede hacerse análisis econmétricos, como lo son MATLAB, Maple, Scilab. Claramente, los programas matemáticos que se acaban de mencionar tienen limitaciones, como la cantidad de observaciones que pueden soportar (por ejemplo, la versión de Scilab 5.5.1 apenas soportaba una matriz que entre columnas y filas llegaba a cinco mil).

No obstante los beneficios de unos y otros software, depende en general, sobre los dispositivos en los que se vaya a usar tal herramienta. Si, por ejemplo, se prefiere Windows como sistema operativo, puede usarse una cantidad importante de programas de licencia y libres; no así en GNU Linux. En esta última distribución y sistema operativo, no se podrán usar muchas distribuciones de licencia, aunque sí otras formas igualmente poderosas. En Mac OS se tiene problemas también con algunos programas de paga u Open Source, Licencia Libre o Software Libre.

Los fines del análisis econométrico también influirá de forma determinante para usar cierto programa. Por ejemplo, si lo que se desea es algo completamente personalizado, con niveles de profesionalismo muy adecuado para publicaciones internacionales, los lenguajes de programación son adecuados. Estos permiten que se exponga la información de una forma propia más fácilmente que en otros ya con interfaces predeterminadas.





</doc>
<doc id="19321" url="https://es.wikipedia.org/wiki?curid=19321" title="Hans Adolf Krebs">
Hans Adolf Krebs

Hans Adolf Krebs (Hildesheim, Alemania, 25 de agosto de 1900 - Oxford, Inglaterra, 22 de noviembre de 1981) fue un bioquímico alemán, ganador del en el año 1953.

Cursó estudios de Medicina, Biología y Química en la universidad de Gotinga. Friburgo de Brisgovia, Múnich y Berlín; en esta última trabajó con Otto Heinrich Warburg, Premio Nobel de Medicina en 1931. Obtuvo la cátedra de Medicina Interna de la Universidad de Friburgo. En 1931, emigró a Inglaterra, país del que obtuvo la nacionalización. Actividad docente en las universidades de Sheffield y Cambridge. Profesor de bioquímica en Whitley y "Fellow" del Trinity College, en Oxford.

Sus principales trabajos de investigación giran alrededor del análisis del metabolismo de la célula, fundamentalmente en la trasformación de los nutrientes en energía. Descubrió que todas las reacciones conocidas dentro de las células estaban relacionadas entre sí, nombrando a esta sucesión de reacciones ciclo del ácido cítrico (1937), más tarde conocido como ciclo de Krebs. Estos estudios le valieron para ganar el Premio Nobel.

El ciclo del ácido cítrico es el conjunto de reacciones energéticas que se producen en los tejidos de los mamíferos, traducidas por la formación y descomposición repetidas del ácido cítrico con eliminación de anhídrido carbónico.

Otras investigaciones desarrolladas por Krebs incluyen aspectos fundamentales de la urogénesis (1932), y el descubrimiento de la importancia de los ácidos tricarboxílicos (ácido cítrico, ácido isocítrico, ácido aconítico etc.), en la respiración aerobia.

Obtiene el en el año 1953, compartido con Fritz Lipmann, co-descubridor del coenzima A.



</doc>
<doc id="19322" url="https://es.wikipedia.org/wiki?curid=19322" title="Galeopterus variegatus">
Galeopterus variegatus

El galeopiteco, kaguang o colugo (Galeopterus variegatus) es un curioso animal para el que se ha creado un orden independiente, el de los Dermópteros, que cuenta con un reducido número de especies.

Es un mamífero capaz de volar pero no en vuelo activo, a la manera de las aves o los murciélagos, sino planeando a manera de cómo lo hacen las ardillas voladoras; modo con el cual puede recorrer por el aire distancias de hasta ciento cuarenta y cinco metros de un árbol a otro.

Es la criatura terrestre actual que mayor distancia planea.

Su tamaño es aproximadamente el de un gato; tiene el esqueleto grácil, los miembros muy alargados, cabeza pequeña y la cola corta. Su pelaje es de color rojo, más o menos monocromo en contraste con el medio en que vive y en la cara dorsal presenta numerosas manchas blancas.

El rasgo más sobresaliente de su morfología es una amplia membrana llamada patagio, constituida por un repliegue tegumentario que engloba totalmente los miembros del animal, y se extiende, además, por delante, entre los miembros anteriores y el cuello y, por detrás, entre los miembros posteriores y la cola quedando todo el animal rodeado por la membrana cuya superficie, bastante considerable, permite su deslizamiento aéreo actuando a modo de paracaídas.

Desde las ramas altas se lanzan al espacio con los miembros separados y mantenidos en un mismo plano horizontal, quedando de este modo desplegado el patagio. Las hembras saltan de un árbol a otro, e incluso planea, llevando a su cría (siempre una sola) asida al pecho.

De comportamiento ágil, es un gran trepador gracias a sus afiladas uñas. El galeopiteco marcha con dificultad por el suelo. Es por consiguiente un animal arborícola, herbívoro y de actividad nocturna, durante el día duerme suspendido de alguna rama asido con pies y manos que permanecen juntos, con el dorso fuertemente curvado, y la cabeza y la cola replegadas sobre el pecho y el abdomen, respectivamente.

Emite un grito agudo, que se percibe desde lejos.

Es un animal característico de las regiones tropicales del Extremo Oriente, Cochinchina, Siam, Birmania, archipiélago malayo, Borneo, Sumatra, Bangka y norte de Java.


</doc>
<doc id="19324" url="https://es.wikipedia.org/wiki?curid=19324" title="Alimentación">
Alimentación

Alimentación es la ingestión de alimento por parte de los organismos para proveerse de sus necesidades alimenticias, fundamentalmente para conseguir energía y desarrollarse.

En el ser humano, la alimentación de cada persona varía según numerosos factores: gustos, edad, actividad física, medios económicos o disponibilidad de productos en la región en la que habita. La cultura también influye en la alimentación, ya que, para productos similares, en cada lugar existen diferentes costumbres y tradiciones.

Los animales y otros heterótrofos deben comer para poder sobrevivir, como los carnívoros, que comen a otros animales, los herbívoros comen plantas, los omnívoros consumen tanto plantas como animales, o los detritívoros, que se alimentan de detritos. Los hongos realizan una digestión externa de sus alimentos, secretando enzimas, y que absorben luego las moléculas disueltas resultantes de la digestión, a diferencia de los animales, que realizan una digestión interna.

Las reacciones químicas necesarias para la vida dependen de la aportación de nutrientes. En los organismos superiores estos nutrientes son sintetizados por fotosíntesis (vegetales), o elaborados a partir de compuestos orgánicos (animales y setas). Existen otras fuentes energéticas para los microorganismos: por ejemplo, algunas arqueas obtienen su energía produciendo metano o por oxidación de ácido sulfhídrico o azufre.

Las plantas son en su mayoría organismos autótrofos. Son capaces de sintetizar compuestos orgánicos a partir de sales minerales y de la energía solar a través de la función clorofílica o fotosíntesis.

Los animales son organismos heterótrofos. Dependen de una o más especies distintas para su nutrición. Los alimentos son transformados en nutrientes mediante la digestión. El régimen alimentario, ya sea carnívoro o herbívoro, tiene una gran influencia en el comportamiento animal, y determina su condición de depredador o presa en la cadena trófica. Pueden tener un comportamiento alimentario omnívoro o más específico, como folívoro, piscívoro, carroñero, nectarívoro, saprófago, etc. Tal como otros animales, el hombre depende de su ambiente para asegurar sus necesidades fundamentales de alimento.



</doc>
<doc id="19325" url="https://es.wikipedia.org/wiki?curid=19325" title="Colesterol">
Colesterol

El colesterol es un lípido (del tipo esterol) que se encuentra en la membrana plasmática eucariota, los tejidos corporales de todos los animales y en el plasma sanguíneo de los vertebrados. Pese a que las cifras elevadas de colesterol en la sangre tienen consecuencias perjudiciales para la salud, es una sustancia estructural esencial para la membrana plasmática, ya que regula la entrada y salida de sustancias en la célula. Abundan en las grasas de origen animal.

François Poulletier de la Salle identificó por primera vez el colesterol en forma sólida en los cálculos de la vesícula biliar en 1769. Sin embargo, fue en 1815 cuando el químico Michel Eugène Chevreul nombró el compuesto «colesterina», del griego χολή, "kolé", ‘bilis’ y στερεος, "stereos", ‘sólido’.

La fórmula química del colesterol se representa de dos formas: CHO / CHOH.

Es un lípido esteroide, derivado del ciclopentanoperhidrofenantreno (o esterano), constituido por cuatro carbociclos condensados o fusionados, denominados A, B, C y D, que presentan varias sustituciones:


En la molécula de colesterol se puede distinguir una cabeza polar constituida por el grupo hidroxilo y una cola o porción apolar formada por el carbociclo de núcleos condensados y los sustituyentes alifáticos. Así, el colesterol es una molécula tan hidrófoba que la solubilidad de colesterol libre en agua es de 10 M y, al igual que los otros lípidos, es bastante soluble en disolventes apolares como el cloroformo (CHCl).

La biosíntesis del colesterol tiene lugar en el retículo endoplasmático liso de prácticamente todas las células de los animales vertebrados. Mediante estudios de marcaje isotópico, Rittenberg y Bloch demostraron que todos los átomos de carbono del colesterol proceden, en última instancia, del acetato, en forma de acetil coenzima A. Se requirieron aproximadamente otros 30 años de investigación para describir las líneas generales de la biosíntesis del colesterol, desconociéndose, sin embargo, muchos detalles enzimáticos y mecanísticos a la fecha. Los pasos principales de la síntesis de colesterol son:

Resumidamente, estas reacciones pueden agruparse de la siguiente manera:

El ser humano no puede metabolizar la estructura del colesterol hasta CO y HO. El núcleo intacto de esterol se elimina del cuerpo convirtiéndose en ácidos y sales biliares las cuales son secretadas en la bilis hacia el intestino para desecharse por heces fecales. Parte de colesterol intacto es secretado en la bilis hacia el intestino el cual es convertido por las bacterias en esteroides neutros como coprostanol y colestanol.

En ciertas bacterias sí se produce la degradación total del colesterol y sus derivados; sin embargo, la ruta metabólica es aún desconocida.

La producción en el humano del colesterol es regulada directamente por la concentración del colesterol presente en el retículo endoplásmico de las células, habiendo una relación indirecta con los niveles plasmáticos de colesterol presente en las lipoproteínas de baja densidad (LDL por su acrónimo inglés). Una alta ingesta de colesterol en los alimentos conduce a una disminución neta de la producción endógena y viceversa. El principal mecanismo regulador de la homeostasis de colesterol celular aparentemente reside en un complejo sistema molecular centrado en las proteínas SREBPs ("Sterol Regulatory Element Binding Proteins 1 y 2": proteínas que se unen a elementos reguladores de esteroles). En presencia de una concentración crítica de colesterol en la membrana del retículo endoplásmico, las SREBPs establecen complejos con otras dos importantes proteínas reguladoras: SCAP ("SREBP-cleavage activating protein": proteína activadora a través del clivaje de SREBP) e Insig (insulin induced gene) 1 y 2. Cuando disminuye la concentración del colesterol en el retículo endoplásmico, las Insigs se disocian del complejo SREBP-SCAP, permitiendo que el complejo migre al aparato de Golgi, donde SREBP es escindido secuencialmente por S1P y S2P (site 1 and 2 proteases: proteasas del sitio 1 y 2 respectivamente). El SREBP escindido migra al núcleo celular donde actúa como factor de transcripción uniéndose al SRE ("Sterol Regulatory Element": elemento regulador de esteroles) de una serie de genes relevantes en la homeostasis celular y corporal de esteroles, regulando su transcripción. Entre los genes regulados por el sistema Insig-SCAP-SREBP destacan los del receptor de lipoproteínas de baja densidad (LDLR) y la hidroxi-metil-glutaril CoA-reductasa (HMG-CoA-reductasa), la enzima limitante en la vía biosintética del colesterol.
El siguiente diagrama muestra de forma gráfica los conceptos anteriores:

Tras dilucidar los mecanismos celulares de captación endocítica de colesterol lipoproteico, trabajo por el cual fueron galardonados con el en el año 1985, Michael S. Brown y Joseph L. Goldstein han participado directamente en el descubrimiento y caracterización de la vía de los SREBPs de regulación del colesterol corporal. Estos avances han sido la base del mejor entendimiento de la fisiopatología de diversas enfermedades humanas, fundamentalmente la enfermedad vascular aterosclerótica, principal causa de muerte en el mundo occidental a través del infarto agudo al miocardio y los accidentes cerebrovasculares y el fundamento de la farmacología de las drogas hipocolesteromiantes más potentes: las estatinas. 
Es importante reseñar que el tratamiento hipolipemiante se ha relacionado consistentemente con una reducción de la mortalidad por todas las causas, la mortalidad cardiovascular y el riesgo de ictus. Al tratamiento con estatinas se le ha atribuido clásicamente una elevada frecuencia de efectos adversos, fundamentalmente a nivel muscular en forma de mialgias. Estudios aleatorizados, doble ciego, comparando estatinas frente a placebo, muestran una frecuencia similar de mialgias en pacientes tomadores de estatinas y aquellos que solo estaban tomando placebo, demostrando la influencia de la sugestión en la percepción de dicho efecto adverso. Esto se denomina efecto nocebo. 

El colesterol es imprescindible para la vida animal por sus numerosas funciones:


La concentración actualmente aceptada como normal de colesterol en el plasma sanguíneo (colesterolemia) de individuos sanos es de 120 a 200 mg/dL. Sin embargo, debe tenerse presente que la concentración total de colesterol plasmático tiene un valor predictivo muy limitado respecto del riesgo cardiovascular global (ver más abajo). Cuando esta concentración aumenta se habla de hipercolesterolemia.

Dado que el colesterol es insoluble en agua, el colesterol plasmático solo existe en la forma de complejos macromoleculares llamados lipoproteínas, principalmente LDL, HDL y VLDL, que tienen la capacidad de fijar y transportar grandes cantidades de colesterol. La mayor parte de dicho colesterol se encuentra en forma de ésteres de colesterol, en los que algún ácido graso, especialmente el ácido linoleico (un ácido graso de la serie omega-6), esterifica al grupo hidroxilo del colesterol.

Habitualmente se afirma que la existencia sostenida de niveles elevados de colesterol LDL (popularmente conocido como "colesterol malo") por encima de los valores recomendados, incrementa el riesgo de sufrir eventos cardiovasculares (principalmente infarto de miocardio agudo) hasta diez años después de su determinación, según indicaba el estudio de Framingham iniciado en 1948.

En sentido estricto, el nivel deseable de colesterol LDL debe definirse clínicamente para cada sujeto en función de su riesgo cardiovascular individual, el cual está determinado por la presencia de diversos factores de riesgo, entre los que destacan:

Es preferible que el LDL sea bajo. En general, el nivel de LDL se considera demasiado elevado si es de 190 mg/dL o mayor. Recientemente se han actualizado los niveles de colesterol que se consideran elevados. Las guías de práctica clínica de la Sociedad Europea de Cardilogía de 2019 definen nuevos niveles de riesgo cardiovascular y establecen niveles objetivos de colesterol menores a los que se establecían en las guías de 2016. 

Los niveles entre 79 y 189 mg/dL suelen considerarse excesivamente altos en pacientes diabéticos con edades comprendidas entre 40 y 75 años, pacientes diabéticos con riesgo alto de desarrollar enfermedades cardíacas y personas con riesgo de medio a alto de padecer enfermedades cardíacas.

En relación al colesterol total, pueden darse las siguientes cifras orientativas, aunque el riesgo es muy variable, dependiendo de otros factores asociados, como tabaquismo, diabetes mellitus e hipertensión arterial.

Hipercolesterolemia



</doc>
<doc id="19327" url="https://es.wikipedia.org/wiki?curid=19327" title="Eos">
Eos

En la mitología griega, Eos (en griego antiguo Ἠώς "Ēós" o Έως "Eos", ‘aurora’) era la diosa titánide de la aurora, que salía de su hogar al borde del océano que rodeaba el mundo para anunciar a su hermano Helios, el Sol.

Se cree que la adoración griega de la aurora como diosa fue heredada de la época indoeuropea. El nombre «Eos» es un cognado del latín Aurora y del sánscrito védico Ushas.

Como diosa de la aurora, Eos abría las puertas del cielo con «sonrosados dedos» para que Helios pudiera conducir su carro por el cielo cada día. En la "Ilíada", su toga de color azafrán está bordada o tejida con flores; con dedos sonrosados y brazos dorados, era representada en vasijas áticas como una mujer sobrenaturalmente hermosa, coronada con una tiara o diadema y con largas alas con plumas blancas de pájaro:

Quinto de Esmirna la representaba exultante en su corazón sobre los resplandecientes caballos (Lampo y Faetonte) que tiraban de su carro, entre las Horas de brillante pelo, subiendo el arco del cielo y esparciendo chispas de fuego.

Con frecuencia se la asocia con el epíteto homérico Rododáctila (ῥοδοδάκτυλος: ‘de sonrosados dedos’), si bien Homero también la llama Eos Erigenia: 

También emplea ese epíteto Hesíodo: 

Por tanto Eos, precedida por el lucero del alba (Venus), es considerada el origen de todas las estrellas y planetas, siendo sus lágrimas las creadoras del rocío matutino, personificado por Ersa o Herse.

Eos es la hija de Hiperión y Tea (o Palas y Estigia) y hermana de Helios (el Sol) y Selene (la Luna), «que brilla sobre todos los que están en la tierra y sobre los inmortales dioses que viven en el ancho cielo», según Hesíodo.

Eos era libre con sus favores y tuvo muchos consortes, tanto entre la generación de los titanes como entre los mortales más hermosos. Con Astreo tuvo a todos los vientos y estrellas. Su pasión por el gigante Orión no fue correspondida. Eos secuestró a Céfalo, Clito, Ganimedes y Titono para que fueran sus amantes.

En el más restrictivo mundo helénico, el poeta Apolodoro afirmaba, en una anécdota más que un mito, que su vergonzosa despreocupación era un tormento para Afrodita, quien la halló en la cama con Ares.

El consorte más fiel de Eos fue Titono, de cuyo diván la imaginaban levantándose los poetas. Cuando Zeus le robó a Ganimedes para que fuese su copero, Eos le pidió que hiciese inmortal a Titono, pero olvidó pedir la eterna juventud. Titono vivió por tanto para siempre pero se hizo más y más anciano, convirtiéndose finalmente en un grillo.

Según Hesíodo, Titono y Eos tuvieron dos hijos, Memnón y Ematión. Memnón luchó junto a los troyanos en la Guerra de Troya y murió a causa de ello. Su imagen con Memnón muerto sobre sus rodillas, como Tetis con Aquiles e Isis con Osiris, fue el icono que inspiró la "Pietà" cristiana.

El rapto de Céfalo tenía un atractivo especial para el público ateniense debido a que este era un muchacho de la ciudad, por lo que este elemento mítico apareció frecuentemente en las vasijas pintadas áticas y fue exportado con ellas. En los mitos literarios, Eos raptó a Céfalo cuando este estaba cazando y lo llevó a Siria. Pausanias fue informado de que la secuestradora de Céfalo fue Hemera, la diosa del día. Aunque Céfalo ya estaba casado con Procris, Eos tuvo tres hijos con él, incluidos Faetonte y Héspero. Pero entonces Céfalo empezó a añorar a su esposa, provocando que una contrariada Eos lo devolviese con ella y lo maldijese. En el relato de Higino, se cuenta que Céfalo mató accidentalmente a Procris algún tiempo después al confundirla con un animal mientras cazaba. En "Las metamorfosis" de Ovidio, Procris, celosa, espía a Céfalo, le oye cantar al viento (Aura) y lo interpreta como una serenata para Aurora, que había sido amante de él.

Entre los etruscos, la diosa generativa de la aurora era Thesan. Las representaciones de la diosa-aurora con un joven amante se hicieron populares en Etruria en el siglo V, probablemente inspiradas por las vasijas pintadas griegas importadas. Aunque los etruscos preferían representar a la diosa como una criadora (curótrofa) más que como una abductora de hombres jóvenes, las acroteras escultóricas arcaicas tardías de la Caere etrusca, actualmente en Berlín, muestran a una diosa corriendo en una pose arcaica adaptada de los griegos, y llevando a un muchacho en sus brazos, que ha sido normalmente identificada con Eos y Céfalo. En un espejo etrusco Thesan aparece llevándose a un joven, cuyo nombre es inscrito .

La aurora pasó a ser asociada en la religión romana con Matuta, más tarde conocida como Mater Matuta y también asociada con los puertos marítimos. Tenía un templo en el Foro Boario. El 11 de junio se celebraba la Matralia en ese templo en honor de Mater Matuta, festival éste sólo para mujeres en su primer matrimonio.










</doc>
<doc id="19328" url="https://es.wikipedia.org/wiki?curid=19328" title="Arteriosclerosis">
Arteriosclerosis

La arteriosclerosis (del gr. ἀρτηρία 'tubo' y σκλήρωσις 'endurecimiento patológico') es un término general utilizado en medicina humana y veterinaria, que se refiere a un endurecimiento de arterias de mediano y gran calibre. La arteriosclerosis por lo general causa estrechamiento (estenosis) de las arterias que puede progresar hasta la oclusión del vaso impidiendo el flujo de la sangre por la arteria así afectada.

Los términos arteriosclerosis, arteriolosclerosis y aterosclerosis son similares tanto en escritura como en significado, aunque son, sin duda, diferentes. La arteriosclerosis es un término generalizado para cualquier endurecimiento con pérdida de la elasticidad de las arterias, la palabra viene el griego "arterio", que significa «arteria» y "sclerosis" que significa «cicatriz, rigidez». La arteriolosclerosis se usa para el endurecimiento de las arteriolas o arterias de pequeño calibre. La aterosclerosis es una induración causada específicamente por placas de ateromas.

El término fue introducido en 1833 por el patólogo franco-alemán Jean Lobstein (1777– 1835) en una obra en cuatro volúmenes inconclusa titulada "Traité d'Anatomie pathologique", en su segundo volumen, en una sección sobre la enfermedad arterial.

Los factores de riesgo más comunes son los siguientes: edad, tabaquismo, hipertensión tanto los valores sistólicos como los diastólicos influyen a la elevación del riesgo, hiperglucemia [PMID 15522466], [PMID 22324971], dislipemias, como la hipercolesterolemia. Se consideraba que con una mayor edad, el límite alto de lo normal en la presión arterial aumentaba, debido a la pérdida de elasticidad de los vasos, hoy se tiende aconsiderar anormal toda presión arterial sistólica de 140 mm/Hg o más, o presión arterial diastólica de 90 mm Hg o más (primer grado), pero hay polémica sobre si los límites para empezar a intervenir deberían ser más bajos. 

El consumo de cigarrillos, tóxico además por otros mecanismos, aumentaría la presión debido a la afección de la microvasculatura generalizada, añadido y peor, a la dislipemia, las alteraciones del metabolismo hidrocarbonado, la predisposición familiar a la arterosclerosis, la obesidad, IMC igual o superior a 30 kg/m², sobre todo la obesidad central, perímetro abdominal de más de 101 cm en los hombres, más de 89 cm en las mujeres; se suele considerar como marcador de riesgo cardiovascular el aumento de homocisteína en plasma, factores relacionados con la hemostasia y trombosis, y por supuesto los antecedentes familiares.

Son posibles indicadores de riesgo los niveles elevados de la proteína C reactiva (PCR -CRP en inglés) en sangre que pueden señalar el riesgo de aterosclerosis y de ataques al corazón; altos niveles de PCR son un indicio de inflamación en el organismo, al ser la respuesta general a lesiones o infecciones. Las lesiones en la parte interna de las paredes de las arterias desencadenarían la inflamación y promoverían el crecimiento de la placa.

Las personas con bajos niveles de PCR pueden tener aterosclerosis de evolución más lenta que cuando se tienen niveles elevados de PCR, hay investigaciones en curso para establecer si la reducción de la inflamación y la disminución de los niveles de PCR también puede reducir el riesgo de la aterosclerosis. Las hormonas sexuales, los estrógenos son protectores de la aterosclerosis; las mujeres son más afectadas después de la menopausia.

El pilar sobre el que se fundamenta el origen de la lesión arteriosclerótica es la disfunción endotelial. Se estima que ciertos trastornos del tejido conjuntivo puedan ser factores de iniciación que, sumados a factores de riesgo como la hipertensión, promuevan la más frecuente aparición de arteriosclerosis en algunos grupos de individuos.


No existe tratamiento médico alguno demostrado para la arteriosclerosis en su conjunto, pese a ser el fármaco probablemente más buscado por la industria farmacéutica.

El tratamiento farmacológico (antihiperlipidémicos, antiagregantes o anticoagulantes) sirve para disminuir sus causas o sus consecuencias.

El tratamiento quirúrgico es muy resolutivo en la cardiopatía isquémica y también en otras localizaciones.

El tratamiento profiláctico consiste en evitar los factores predisponentes de la enfermedad y a las complicaciones de ésta: obesidad, hipertensión, sedentarismo, hiperglucemia, hipercolesterolemia, tabaquismo, etc.

Para ello lo ideal es practicar ejercicio suave, una dieta equilibrada como la mediterránea, baja en grasas, técnicas de relajación para evitar el estrés, dejar de fumar, etc.

Investigadores en China encontraron que el hongo reishi mejora el flujo sanguíneo y baja el consumo de oxígeno del músculo cardíaco. Resultados similares fueron también encontrados por científicos japoneses, quienes evidenciaron que el reishi contiene ácido ganodérico, el cual reduce la presión sanguínea y el colesterol e inhibe la agregación plaquetaria, la cual puede conducir a ataque cardíaco y otros problemas circulatorios.



</doc>
<doc id="19337" url="https://es.wikipedia.org/wiki?curid=19337" title="Crimen">
Crimen

En lenguaje ordinario, un crimen es una acción indebida o reprensible, en general un delito grave como la acción voluntaria de matar o herir gravemente a alguien. El término crimen no tiene, en el derecho penal moderno, una definición simple y universalmente aceptada, aunque se han proporcionado definiciones legales para ciertos fines. La opinión más popular es que el crimen es una categoría creada por la ley; en otras palabras, algo es un delito si así lo declara la ley pertinente y aplicable. Una definición propuesta es que un crimen (o delito penal) es un acto perjudicial no solo para una víctima sino también para una comunidad, sociedad o Estado ("un error público"). Dichos actos están prohibidos y punibles por ley.

Diferentes países tienen diferentes ideas de lo que son los crímenes, y cuáles son los peores. Algunas cosas que son crímenes en un país no lo son en otros países. Muchos países se hacen una idea de lo que son los crímenes de las religiones o de los acontecimientos polémicos que hacen que se cree rápidamente una ley. Por ejemplo, una religión puede considerar que comer un alimento en particular es un crimen. Cuando los automóviles se multiplicaron, mataron o hirieron a muchas personas en accidentes de tráfico, por lo que se crearon nuevas leyes para ellos.

En muchos países, si la gente dice que hizo o escribió un libro, una película, una canción o una página web que realmente no hizo o escribió, es un delito contra las leyes de derecho de autor. En muchos países, ayudar a crecer, fabricar, mover o vender drogas ilegales es un delito. 

En la mayoría de los países, la policía intenta detener los delitos y encontrar a los delincuentes. Cuando la policía encuentra a alguien que cree que puede ser un criminal, generalmente lo mantiene en la cárcel. Luego, por lo general, un tribunal o un juez decide si la persona realmente cometió un delito. Si el tribunal o el juez decide que la persona realmente lo hizo, es posible que tenga que pagar una multa o ir a prisión. Algunas veces el juez puede decidir que el criminal debe ser ejecutado. Esto se llama pena capital (o "pena de muerte"). Hay países en el mundo que ejecutan criminales y otros que no. 

En muchos países, deben darse dos condiciones para que un acto sea delictivo:


Ambos deben estar presentes para que el acto sea considerado un crimen.

Los asesinatos rituales o los crímenes rituales han existido y todavía existen hoy y se cometen individualmente o colectivamente contra grupos minoritarios vulnerables, el albino constituye en África un grupo particularmente impactado y vulnerable.



</doc>
<doc id="19338" url="https://es.wikipedia.org/wiki?curid=19338" title="Jennifer Connelly">
Jennifer Connelly

Jennifer Lynn Connelly (Cairo, Nueva York, 12 de diciembre de 1970) es una actriz estadounidense ganadora del Óscar y el Globo de Oro por su papel en "A Beautiful Mind". Asimismo, es reconocida por sus interpretaciones en películas como "Labyrinth" (1986), "Requiem for a Dream" (2000), "Dark City" (1998), "Hulk" (2003), "Casa de arena y niebla" (2003) y "Diamante de sangre" (2006).

Hija única de un comerciante textil y de una mercader de antigüedades, tiene ascendencia irlandesa, noruega, rusa y polaca. Creció en el barrio de Brooklyn, en la ciudad de Nueva York. Estudió en el colegio privado Saint Ann's School.

A los diez años, unos amigos de la familia sugirieron a sus padres enviarla a una selección de modelos infantiles. Fue así como Connelly inició una carrera como modelo publicitaria. Pronto comenzó a aparecer en las portadas de revistas y más tarde en anuncios de televisión, la mayoría de ellos en Japón.

En 1979 le ofrecieron intervenir en una serie de televisión británica. A partir de entonces fue alternando su actividad como modelo con algún papel en producciones televisivas.

Unos años más tarde, en 1984, tuvo su debut en el cine. Un director de "casting" la presentó al director Sergio Leone, que buscaba una actriz joven que supiera bailar para su película "Érase una vez en América". A pesar de que Connelly no apareció más que unos minutos en pantalla, fueron suficientes para poner de manifiesto su talento como actriz.

En 1985 consiguió su primer papel principal en "Phenomena" o "Creepers", de Dario Argento; la película cosechó mucho éxito de taquilla en Europa, pero fue distribuida en Estados Unidos en una versión censurada.

En 1986, con 16 años, protagonizó la que es una de sus películas más emblemáticas y por la que muchos la recuerdan, "Labyrinth", junto al enigmático David Bowie, dirigida por Jim Henson. En 1990 filmó "Labios ardientes", film de corte erótico con Don Johnson, con quien se asegura que tuvieron una relación y vivieron un tiempo juntos. En 1991 trabajó en "The Rocketeer", junto a Timothy Dalton, donde destacó su belleza y capacidad escénica y donde conoció al que sería su pareja durante cinco años, Billy Campbell.

En los años siguientes continuó obteniendo papeles en el cine, algunos en películas poco relevantes, pero otros en películas que tuvieron buena acogida por el público. De esta forma, Connelly se fue convirtiendo en una actriz conocida y apreciada por los directores.

En 2000, el joven director Darren Aronofsky y la eligió para coprotagonizar su segunda película, "Requiem for a Dream" ("Requiem por un sueño"), donde daba vida a Marion, una joven drogadicta, convirtiéndose con el tiempo en un filme de culto sobre la temática tratada.

En 2001 recibió el reconocimiento por su trabajo cuando ganó el , un Globo de Oro y un premio de la Academia Británica de Cine por su interpretación como Alicia Nash en "A Beautiful Mind" (en español "Una mente maravillosa" o "Una mente brillante").

Su trabajo en la película que protagonizó junto a Ben Kingsley, "Casa de arena y niebla" (2003) fue uno de los más aclamados de su carrera y el que terminó de consagrarla, a pesar de no obtener la nominación al Óscar como se esperaba.

Participó en la película "He's Just Not That Into You" (conocida como "¿Qué les pasa a los hombres?" en España y "Simplemente no te quiere" en Hispanoamérica), una comedia romántica donde interviene un elenco de actores famosos como Drew Barrymore, Jennifer Aniston y Scarlett Johansson. Puso su voz al personaje femenino de la película de animación "9", y volvió a trabajar junto a su marido, Paul Bettany, en "Creation", dando vida a la esposa de Charles Darwin. Se la pudo volver a ver a las órdenes de Ron Howard en la comedia "The Dilemma". En "Virginia" cambió su imagen personal tiñéndose de rubia para interpretar el papel principal de la película, dando vida a una madre soltera que mantiene una relación oculta con el sheriff de la ciudad, papel interpretado por Ed Harris.
También trabajó en títulos como la película de Darren Aronofsky, "Noah", compartiendo cartel con Russell Crowe, entre otros film de corte dramático.

En 2014 Jennifer trabajó con Claudia Llosa en la película "No llores, vuela".

Su primer hijo, Kai, nació el 6 de julio de 1997 fruto de su relación con el fotógrafo David Dugan.
Desde enero de 2003 está casada con el también actor Paul Bettany, a quien conoció en el rodaje de "A Beautiful Mind", con el que ha tenido dos hijos: el 5 de agosto de 2003 nació Stellan Bettany (nombrado así por el actor Stellan Skarsgård, amigo de Paul Bettany). El 31 de mayo de 2011 dio a luz a una niña, que recibió el nombre de Agnes Lark.



</doc>
<doc id="19345" url="https://es.wikipedia.org/wiki?curid=19345" title="Milla náutica">
Milla náutica

La milla náutica o milla marina es una unidad de longitud empleada en navegación marítima y aérea. En la actualidad, la definición internacional, adoptada en 1929, es el valor convencional de 1852 m, que es aproximadamente la longitud de un arco de 1' (un minuto de arco, la sesentava parte de un grado sexagesimal) de latitud terrestre. Ha sido adoptada, con muy ligeras variaciones, por todos los países occidentales. Esta unidad de longitud no pertenece al Sistema Internacional de Unidades (SI).
La milla náutica deriva de la longitud sobre la superficie terrestre de un minuto de arco de latitud. Sesenta millas náuticas de latitud equivalen entonces a una diferencia de latitud de un grado. De ahí se deriva el uso de la milla náutica en navegación. Para distancias menores a una milla náutica, lo usual en el mundo náutico es utilizar décimas de milla náutica.

De la milla náutica se deriva también la medida de velocidad usada en el campo náutico, el nudo. Un nudo es una velocidad igual a una milla náutica por hora.

No existe un solo símbolo aceptado de forma universal. El SI da preferencia a M, pero también se usan mn, nmi, NM y nm (del inglés: "nautical mile"). No debe confundirse con la milla terrestre, "estatutaria" o inglesa, que todavía se emplea en algunos países anglosajones y equivale a 1609,344 m.

La milla náutica y el nudo son prácticamente las únicas medidas de distancia y velocidad usadas en navegación marítima y aérea, ya que simplifican los cálculos de posición del observador. Esta posición se mide mediante las coordenadas geográficas de latitud (Norte o Sur) y longitud (Este u Oeste) a partir del ecuador y de un meridiano de referencia, usando grados sexagesimales. El problema del navegante es conocer la posición en grados y minutos de latitud y longitud tras haber recorrido una cierta distancia, o al revés (sabiendo las coordenadas actuales y del punto de destino, calcular la distancia a la que se encuentra). 

Mediante el uso de la trigonometría esférica es sencillo determinar la distancia esférica entre dos puntos conocidas sus coordenadas de latitud y longitud. Esta distancia esférica se obtiene en grados sexagesimales de modo que reduciendo el ángulo a minutos sexagesimales se obtiene, directamente, la distancia en millas náuticas.

Punto A: Latitud = 44º36'N; Longitud = 4º55'10"W. 
Punto B: Latitud = 40º10'20"N; Longitud = 12º10'E. 
Distancia esférica entre A y B = 12º46'05" = 766' = 766 NM (millas náuticas). 

La obtención directa de la distancia esférica expresada en minutos sexagesimales explica el éxito de esta medida de distancia desde su utilización inicial en navegación marítima, normalmente grandes distancias y sobre la superficie del globo terrestre. Obviamente se utiliza la simplificación de considerar el globo terrestre esférico (realmente es un elipsoide).

Las cartas y los derroteros —también muchos mapas terrestres en proyección Mercator— permiten conocer las coordenadas de faros, cabos, islas, puntos de referencia (landmarks), etc, e incluso medir con un compás de puntas distancias esféricas (sobre los meridianos). Antes de la época actual que ha visto la popularización de sistemas de posicionamiento global por satélite (GPS, GLONAS, Galileo), en mar abierto se usaba el sextante para medir la altura observada de los astros respecto al horizonte, lo que permitía determinar la latitud del lugar. La medición de la longitud requería el uso de cronómetros y la observación del sol. La precisión en la determinación de la latitud y longitud se ha visto significativamente incrementada al usar sistemas de posicionamiento global por satélite.





</doc>
<doc id="19348" url="https://es.wikipedia.org/wiki?curid=19348" title="Metal del bloque p">
Metal del bloque p

Los elementos metálicos situados en la tabla periódica junto a los metaloides (o semimetales), dentro del bloque "p" se distinguen de los metales de otros bloques de la tabla; en algunos casos son denominados "otros metales". Tienden a ser blandos y a tener puntos de fusión bajos.

Estos elementos son los siguientes:
Aunque la división entre metales y no metales puede variar.

Los elementos con número atómico desde el 113 al 116 pueden incluirse en este grupo, pero no suelen ser considerados. Estos elementos tienen un nombre sistemático


</doc>
<doc id="19349" url="https://es.wikipedia.org/wiki?curid=19349" title="Beato de Liébana">
Beato de Liébana

Beato de Liébana (Reino de Galicia, c. 730-Liébana, c. 798), también llamado san Beato, fue un monje del monasterio de San Martín de Turieno (actualmente monasterio de Santo Toribio de Liébana), en la comarca de Liébana (Cantabria, España), en las estribaciones de los Picos de Europa. Su obra más conocida es el "Comentario al Apocalipsis de San Juan" ("Commentarium in Apocalypsin") en doce libros, de gran difusión durante la Alta Edad Media, debido a su trabajo en el campo de la teología, política y geografía.

Algunas fuentes no del todo fiables aseguran que el Beato se retiraría posteriormente al monasterio de Valcavado en Palencia, donde sería nombrado abad —según Alcuino de York— y finalmente encontraría la muerte.

En Cantabria, así como en Asturias, sienten una gran devoción por esta figura. Inscrito en el santoral católico, su festividad se celebra el 19 de febrero.

Sin duda no es originario de los Montes Cantábricos. Algunos historiadores piensan que viene más bien de Toledo, o incluso de Andalucía. Quizá eligió este monasterio de Liébana debido a la proximidad de Covadonga y Cosgaya, lugares que los cristianos de la época daban por milagrosos.

El Beato adquiere rápidamente una reputación de gran erudición. Pasa a ser incluso durante algún tiempo preceptor y confesor de la hija de Alfonso I, la futura reina Adosinda, que se casaría con el rey Silo de Asturias, monarca desde 775 a 783.

Su notoriedad tenía también otras causas que su Comentario al Apocalipsis. Pensador militante y enérgico, combate a los que se comprometen con los invasores musulmanes, comenzando por el arzobispo de Toledo, a quien acusa de herejía por defender el adopcionismo.

Como ha destacado Eduardo Manzano Moreno, su "Comentario sobre el Libro del Apocalipsis" es una «interpretación de la difícil obra del apóstol San Juan. Uno de los temas que se desarrollaban en ese comentario era el de la Jerusalén celeste». «Su comentario sobre el Apocalipsis adquirió una enorme resonancia y fueron numerosos los manuscritos que se hicieron de él; manuscritos muchas veces iluminados con fantásticas imágenes que componen todo un ciclo —los llamados "Beatos"— que constituye una de las cumbres de la iconografía medieval».

A finales del siglo VIII Elipando de Toledo, arzobispo de Toledo, entonces bajo dominio del Emirato de Córdoba, defendió el adopcionismo que afirmaba que el Padre había "adoptado" al Hijo, ya que la «naturaleza» de este no era divina sino humana al haber sido concebido por una mujer. Como el prestigio de la sede toledana todavía se mantenía en toda la península ibérica, a pesar de que estaba sometida al emir de Córdoba, la propuesta «adopcionista» de Elipando provocó una feroz respuesta en el Reino de Asturias que estuvo encabezada por el monje Beato de Liébana, posiblemente abad de un monasterio y muy bien relacionado con la reina Adosinda. Beato de Liébana acusó a Elipando de locura, herejía e ignorancia y llegó a llamarle «testículo del Anticristo». Según Eduardo Manzano Moreno, la polémica entre Elipando y Beato de Liébana fue «espoleada por la fuerte pugna entre una iglesia septentrional, cada vez más independiente, y la antigua iglesia visigoda, cuyos principales episcopados habían caído en territorio andalusí». En el año 794 el concilio de Frankfurt presidido por Carlomagno condenó el adopcionismo. En uno de sus cánones se decía que esta «herejía debería ser radicalmente extirpada de la Santa Iglesia». 

De indudable autoría del Beato es el "Apologeticum adversus Elipandum", una obra en dos volúmenes, que escribió con Eterio de Osma, para enfrentarse a la herejía adopcionista del arzobispo de Toledo, Elipando.

En cambio, se discute su autoría del himno "O Dei Verbum", que está formado por frases y conceptos tomados del "Comentario" para ensalzar y promocionar el patronazgo de Santiago sobre la Hispania cristiana septentrional, necesitada de la ayuda divina. Pocos años después, sería descubierta la tumba del apóstol en Santiago de Compostela.

Otra obra atribuida al Beato sin certeza y que se conserva en un manuscrito fragmentario del siglo X (en Santillana del Mar), es un "Liber Homiliarum" de uso litúrgico. Son homilías que siguen las lecturas de la misa o el oficio de maitines, de acuerdo con el calendario mozárabe.

Se conocen como «Los Beatos» los manuscritos de los siglos X al XIII, más o menos abundantemente ilustrados, donde se copian el Apocalipsis de San Juan y los "Comentarios" sobre este texto redactados en el siglo VIII por el Beato de Liébana. Escribió los Comentarios al Apocalipsis de San Juan (Commentarium in Apocalypsin), en el año 776. Diez años después, en el 786, redacta la versión definitiva. En esta versión pretende hacer frente a la crisis por la que pasaba la Iglesia en aquellos años e intenta demostrar que está en posesión de la "traditio" sobre la llegada y predicación del Apóstol Santiago en Hispania. Para ello se basa en ciertos escritos del libro Breviario de los Apóstoles.

Estos "Comentarios" contienen también uno de los más antiguos mapamundis del mundo cristiano.

En el año 379, el emperador Graciano escogió a un general hispano, Teodosio (o Teodosio I, también conocido como Teodosio el Grande) para ocupar el trono del Imperio de Oriente. Éste, después de eliminar a un usurpador del trono de Occidente en 388, reinó sobre todo el Imperio romano.
Tras convertirse al cristianismo en 380, hizo de ésta la religión oficial del Imperio, prohibiendo la herejía arriana, los cultos paganos y el maniqueísmo.

La unificación del Imperio duró poco: a la muerte de Teodosio, el Imperio se dividió entre sus dos hijos.

El 31 de diciembre del año 406, diversos pueblos germánicos atravesaron la Galia. Los Suevos se establecieron en Galicia (al noroeste de España), los vándalos y los alanos se establecieron en Andalucía.

Durante ese tiempo, Ataúlfo, jefe de los visigodos, y sucesor de Alarico I, se casó en el 414 con Gala Placidia, la hija del emperador Teodosio I el Grande. Pero, empujado por el gobierno de Rávena, se trasladó a Hispania, lo que causaría que las guerras entre los bárbaros se multiplicaran en la península ibérica.

Al mismo tiempo, estos pueblos sufrieron una "romanización". Así los visigodos se unen a una coalición romana encabezada por el general Flavio Aecio, siendo rey de los visigodos Teodorico I, para enfrentarse a la alianza de los hunos mandada por su rey Atila. La batalla de los Campos Cataláunicos, en la que Teodorico murió, se desarrolló cerca de Orleans, en el 451.

Poco después, el rey de los Visigodos, Eurico, viajó a Hispania y se proclamó su primer rey independiente. El Imperio romano había dejado de existir. Eurico, fiel a las doctrinas de Arrio, instó al rey Suevo a convertirse al arrianismo.

En el 325, el obispo Osio de Córdoba fue convocado por el emperador Constantino al primer concilio ecuménico en Nicea para condenar las doctrinas de Arrio (Jesucristo siervo de Dios).
Los Visigodos, también arrianos, contribuyeron a extender la herejía por la península. Pese a que sólo el 5% de la población la practicaba, hicieron del arrianismo la religión oficial del estado.

Tras esto, los clérigos católicos se refugiaron en los ámbitos rurales.

A comienzos del siglo VI, el rey visigodo Leovigildo contrajo matrimonio con Teodosia, la hermana de Isidoro de Sevilla. Isidoro intentó reconciliar a los Visigodos con los "hispano-romanos" y se muestra de acuerdo con el Símbolo niceno en lo que concierne a la naturaleza de Cristo. Uno de sus hijos casó con una nieta cristiana de Clodoveo. El otro, Recaredo, se convirtió al cristianismo en el 587 y abjura oficialmente del arrianismo en el concilio de Toledo (589), arrastrando con él a la reina, la corte y a los obispos visigodos herejes.

El arzobispo de Toledo queda como primado de Hispania, y la Iglesia es sostenida por los soberanos. Estos nombran a los obispos que, a cambio, ejercen un control sobre la administración real.

Pero los visigodos deben hacer frente a epidemias, hambres, incursiones de francos. Las guerras de sucesión devastan el país. Un aspirante al trono de Toledo, Agila II, refugiado en Ceuta (Mauritania en la antigüedad), para garantizarse la victoria sobre su enemigo Rodrigo, pide la ayuda de tropas de el Magreb. Así pues, en 711, Táriq ibn Ziyad cruza el estrecho cuyo nombre en adelante se asocia al suyo

Los 7000 hombres de Tariq no eran árabes sino bereberes, de los que los árabes, que ocuparán poco después África del Norte, eran aliados, si bien esta alianza conoció momentos bajos debido a que los bereberes eran tratados como musulmanes 'de segunda'. Y este antagonismo entre las dos etnias, debilitará a los nuevos amos musulmanes de Hispania, que ahora recibe el nombre de Al-Ándalus.

En tres años fue ocupada la Península, a excepción de una parte de la Cordillera Cantábrica (futuro reino de Asturias) que forma en el noroeste del país una especie de fortaleza cuyas cumbres alcanzan a menudo más de 2000 metros, e incluso más de 2600 metros en los Picos de Europa. Muy pronto acuden los cristianos a refugiarse allí, en particular de Toledo. Pelayo, se hace elegir jefe de los rebeldes con los que atacará las guarniciones berberiscas.

Tranquilamente instaladas en Córdoba, o estando ocupadas en razzias en el sur de la Galia, las fuerzas musulmanas no se preocupan de la rebelión al principio. Sin embargo, se envía una expedición a Asturias. Pero allí, Pelayo, fingiendo huir, atrae a bereberes y árabes a las gargantas de Covadonga, los divide en grupos, y según la leyenda acaba de exterminarlos en gran número cerca de Liébana. El reino de Asturias se consolida a partir de entonces en torno a Cangas de Onís, con Pelayo como rey.

Mientras que Asturias se refuerza cada vez más y se puebla, los cristianos que viven bajo el dominio musulmán se encuentran en la misma situación que antes bajo la dominación de los visigodos arrianos. Sujetos a impuestos que sólo se les aumentan a ellos, no tienen derecho a construir nuevas iglesias ni a fundar nuevos conventos. Una vez más, son numerosos los que se refugian en el campo, mientras los invasores permanecen en las ciudades. De nuevo las ermitas aparecen en lugares remotos. Los cristianos que viven en tierra musulmana no pueden practicar su religión excepto si antes juraron lealtad a un jefe moro.

En 791, el rey asturiano Alfonso II "el Casto" traslada la capital del reino a Oviedo.

En este contexto histórico, y en una región donde los huidos del Islam estaban aportando una cultura muy rica, particularmente en el ámbito artístico, es en el que Beatus (Beato para los hispanos), monje en un convento del valle de Liébana, escribe su comentario del Apocalipsis.

Es una obra de erudición pero sin gran originalidad, hecha sobre todo de compilaciones. El Beato toma extractos más o menos largos de los textos de los Padres y Doctores de la Iglesia; en particular, San Agustín, San Ambrosio, San Ireneo, San Isidoro. También está el comentario del Libro de Daniel por San Jerónimo.

La organización de la obra es juzgada por algunos como torpe, y el texto a veces redundante o contradictorio. Lejos de una obra que emana una fuerte y profunda personalidad, estamos en presencia de una producción un tanto timorata, no dando pruebas de un gran espíritu de innovación. ¿Cómo tal libro, escrito en 776 y alterado diez años más tarde, ha tenido tal impacto durante cuatro siglos?. Si la parte del Beato es muy reducida, la obra contiene por el contrario una traducción latina íntegra del Apocalipsis de Juan, lo que puede en parte explicar su notoriedad.

El Apocalipsis de Juan es el último libro del corpus bíblico cristiano. La clase literaria apocalíptica (del griego "apocalupteïn", revelar) que florece en el período intertestamentario (entre el II y el I siglo a. C.) encuentra sus raíces, no en el Nuevo Testamento, sino en los últimos libros del Antiguo, en particular, algunas partes del Libro de Daniel (escrito hacia 167 antes del J.C.): el Apocalipsis tiene entonces más relaciones conceptuales y de contexto con la cultura semítica del Antiguo Testamento que con el mundo de los Evangelios. El Apocalipsis de Juan se redactó en el último tercio del siglo I, durante las persecuciones de Néron, después de la de Domiciano contra los cristianos que se negaban a rendir culto el Emperador.

Un apocalipsis es un "descubrimiento" del futuro, revelado a un alma y transcrita bajo una forma poética más o menos criptada. Es un discurso escatológico. Se calificaron los Apocalipsis de "Evangelios de la Esperanza", ya que anuncian a poblaciones martirizadas que el mal histórico consigue una felicidad eterna. El texto parece generalmente oscuro a los que no están penetrados de la cultura bíblica: destinado a los creyentes y a ellos sólo, hace referencia a la Historia Santa y a libros proféticos del Antiguo Testamento. Por eso su alcance "político" escapa a los perseguidores. Es pues una concepción de la Historia (una "Teología de la Historia", escribía Henri-Irénée Marrou) destinada a mostrar a los que sufren cómo el Bien Supremo se encontrará al término de una marcha históricamente necesaria a través del Mal.

El Apocalipsis se presenta como el libro de la resistencia cristiana. Los grandes símbolos toman un nuevo sentido. El Animal, que designaba al Imperio, se convierte en el nombre del emirato (convertido más tarde en califato) - Babilonia no es ya Roma sino Córdoba, etc.

El Apocalipsis, que se había interpretado como una profecía del final de las persecuciones romanas, se convierte en el anuncio de la Reconquista. Es una promesa de entrega y castigo. El desciframiento es sencillo para las masas que creen, y este libro termina por adquirir, en Al-Ándalus, más importancia que los Evangelios.

El Apocalipsis, que los arrianos se negaban a considerar como un libro revelado, y que se centra en la divinidad del Cristo, se convierte, a partir del siglo VIII, en el texto "faro" de los cristianos que resistían. El Apocalipsis es pues una obra de combate, verdadera arma teológica, contra todos los que no veían en Cristo una persona divina en el mismo plano que Dios Padre. El clero de Asturias reanuda la prescripción del IV Concilio de Toledo (633): so pena de excomunión, el "Apocalipsis debe considerarse como un libro canónico; se leerá en la "Misa" entre la Pascua y Pentecostés". Tengamos en cuenta que tal obligación solo se refería, de la Biblia entera, a este único texto.

El Comentario del Apocalipsis menciona que Santiago es el evangelizador de Hispania. Algunos historiadores piensan incluso que Beato es el autor del himno "O dei verbum", en el cual se califica a San Yago de santo patrón de España.

Al principio del siglo IX "se descubre" la tumba de Santiago en el "Campo de Estrella" (es decir, Compostela), que se convertirá en Santiago de Compostela. Allí se habrían transportado las reliquias del hermano de San Juan Evangelista un siglo antes, desde Mérida, para sustraerlos a los profanadores musulmanes. Dado que en la época se asignó el Apocalipsis a San Juan, Beato quiso quizá honrar también a su hermano Santiago el Mayor, y hacer de los dos hijos de Zebedeo los vectores de los valores de la España martirizada, resistente y gloriosa.

Beato muere en 798, antes de la primera mención a la tumba del "Matamoros".

En 1924 tiene lugar en Madrid una exposición de manuscritos españoles con miniaturas (" Exposición de códices miniados españoles"). Y si los "Beatos" son especialmente estudiados, es porque impusieron nuevas formas en el ámbito artístico.

La fascinación por estos libros tiene pues una dimensión doblemente visionaria, como si las formas hubieran, ellas también, profetizado... ha parecido a muchos que los "Beatos" contenían la complejidad misma de lo que anunciaban, ofreciendo insuperables respuestas a cuestiones que aún balbuceaban en la época de su redescubrimiento.

Por supuesto, el arte mozárabe no nació de la nada: las raíces se encuentran en las corrientes visigóticas, carolingias, árabes, y hasta en el arte "copto" cuyas estilizaciones particulares son bien identificables.

Y si los especialistas detectan incluso contribuciones más alejadas, mesopotámicas, sasánidas, eso no significa que el arte mozárabe no sea más que la producción de artistas de segundo orden, sin gran personalidad, diluyendo su falta de imaginación en el eclecticismo. Lejos ser una simple ilustración que no añadiría nada al texto, o incluso que desviaría al lector, la coloración mozárabe, a menudo en toda la página e incluso en doble página, como lo reconoce Jacques Fontaine, conducía el alma desde la lectura del texto hasta la profundización de su sentido en una visión.

Entre las obras más notables (si se excluye el "Beato"), es necesario citar la Biblia miniada en 920 por el diácono Juan en el monasterio de Santa María y San Martín de Albares (dicha "Biblia de Juan de Albares", se conserva en los archivos de la catedral de León.

Al observar estas imágenes, no tenemos el sentimiento de que más de diez siglos nos separan.

Quedan una treintena de manuscritos miniados del "Comentario del Apocalipsis" redactado por Beato en la Abadía de Santo Toribio de Liébana en 776, 784 y 786. De ellos, veinticinco están completos, veintidós tienen imágenes, pero solamente una decena pueden considerarse como antiguos. Según algunas hipótesis, el manuscrito se decoró desde el principio, como hacen pensar las partes insertadas en el texto, que hacen referencia a una imagen. Pero no se ha conservado ninguno de estos "proto-Beatos".

Las imágenes impactan incluso a los que conocen bien el "Apocalipsis". Pero no es subestimar el genio de los miniaturistas, reconocer la relación de muchos elementos con la realidad que los rodeaba. Si los decorados, los muebles, las actitudes parecen ser puros productos de la imaginación, es porque la liturgia que los suscitaba no nos es familiar; por eso atribuimos a la invención lo que estaba incluido en la observación. Una vez más conviene atender al talento literario y la precisión de Jacques Fontaine:

Esta liturgia, estos objetos, estas luces deslumbraban a los propios árabes, como ese canciller musulmán que había asistido a una ceremonia nocturna en una iglesia de Córdoba, como lo informa su cronista, también musulmán:

Si se excluyen algunas visiones trágicas de condenación eterna, algunas posturas de desesperación, así como lo observa Jacques Fontaine, "lo dominante en estas obras es una contemplación serena" (citada Obra, p. 361).

La humanidad e incluso el humor están presentes en los colofones. Así pues, en el Beato de Tábara, el pintor Emeterio, en un dibujo, representa la torre de la biblioteca y el propio scriptorium Contiguo, se representa él mismo y añade estas palabras: "Ô torre de Tábara, alta torre de piedra, tan alto que, Emeterio permaneció sentado, muy curvado sobre su tarea, durante tres meses, y que tuvo todos sus miembros baldados por el trabajo del cálamo. Este libro se terminó del 6 de las calendas de agosto el año 1008 de nuestra era a la octava hora." (en Jacques Fontaine, Obra citada, p. 361).

Estos colofones no son en ninguna parte tan abundantes como en las obras mozárabes. Los Beatos pueden así asignarse y datarse con una gran precisión, lo que permite un estudio serio de las cuestiones de filiaciones estilísticas. Sabemos así como Magius realizó las pinturas del Beato de Pierpont Morgan Library, que una pintora llamada Ende ayudó a su alumno Emeterio en la realización del Beato de Gerona.

El soporte es generalmente el pergamino, y también el papel (presente en la península a partir del siglo XI ).

El texto estaba escrito en tinta color pardo (o que se volvía parda). Los títulos están a menudo en rojo. Este color servía también para dibujar el contorno de los elementos de la página. Los pintores seguían en eso las recomendaciones de Isidoro de Sevilla extraídas de las Etimologías: se trazan en primer lugar los contornos, luego se procede al relleno de las figuras con ayuda del color.

Los colores de las pinturas son el rojo (más o menos oscuro), el ocre, el verde oscuro, el rosa-malva, el azul oscuro, púrpura, anaranjado, y sobre todo el amarillo huevo muy luminoso, muy intenso, consustancial a la pintura mozárabe. Se emplea el negro también. El azul claro y el gris son raros.

Los colores "calientes" son los predominantes: rojo, anaranjado, amarillo. Aquí aún, los pintores siguen la enseñanza de Isidoro de Sevilla que hace una aproximación etimológica (pues, para él, fundado en la esencia de las cosas) entre las palabras color (latín color) y calor (latín calor): "Se nombran los colores así porque se elevan a su terminación (perfección) por el calor del fuego o el sol" (Etimologías, XIX, capítulo XVI).

El oro (metal) es muy raro. Se encuentra presente, o previsto, en el Beato de Gerona y en el Beato de Urgel.

Algunos manuscritos están inacabados, lo que, por otra parte, nos informa sobre las etapas de su elaboración. En el Beato de Urgell (ms 26, f°233) o en el de la Real Academia de la Historia de Madrid (ms 33, f°53), el dibujo solo está parcialmente coloreado.

Los colores son puros, sin medias tintas, sin mezclas, sin transiciones de uno a otro.

Mientras que en el primer Beato eran bastante mates, o, al menos, discretos, los Beatos de segundo estilo (mediados del siglo X) llaman la atención por la brillantez de sus colores. Sin duda se debe a la utilización, sobre un fondo barnizado a la cera, de nuevos ligantes., como el huevo o la miel que permiten la obtención de transparencias y tonos vivos, luminosos.

Si se excluyen los refinamientos de los tonos del Beato de Pierpont Library (y, por supuesto el único exotismo del Beato de Saint Sever), los colores están distribuidos más bien en oposiciones intensas, y utilizados para contribuir a la irrealidad de las escenas.

Por supuesto, cuando Isidoro de Sevilla habla de verdad, la entiende como la conformidad con la realidad sensible. Pero como vimos con el problema del espacio, los pintores de los Beatos no buscan una adecuación con el mundo de la percepción. La realidad que dan a conocer es de carácter espiritual.

Los colores ni son mezclados, ni rebajados. El modelado, la sombra, el rebaje, solo aparecen en el Beato de Saint-Sever.

En los Beatos españoles, la vivacidad de los colores, sus contrastes, la violencia misma de algunas yuxtaposiciones, conducen extrañamente la mirada a no detenerse en una percepción global, sino hacia los elementos constitutivos de la página.

Aquí también, como con el tratamiento del espacio, el objetivo del pintor parece ser distraer al espíritu de las tentaciones de lo accidental para atraerlo a la esencia del relato ofrecida en la contemplación estética.

Una de las originalidades de muchas páginas de los Beatos, es la presentación de las escenas sobre un fondo de amplias bandas pintadas, horizontales, que no corresponden a ninguna realidad exterior. No se trata del cielo, el agua, el horizonte o de efectos de aproximación o alejamiento. Se habló con mucha razón de un "desrealización" del espacio por el color.

"Como más tarde en El Greco, la pintura resulta aquí método espiritual", escribe Jacques Fontaine (Obra citada, p. 363). El mundo sensible se purifica de sus elementos anecdóticos solo para dejar sitio a la parte fundamental. Se trata de poner de manifiesto que pasa algo sin distraernos describiéndo el lugar dónde eso pasa. Los protagonistas del drama apocalíptico exploran aún más lo que pasa en su alma (o en la del lector), con "esta fijeza huraña que llega, a veces, hasta el éxtasis, y a la desmesura", para recoger la fórmula de Mireille Mentré.

Las formas son geométricas, y la esquematización llega a veces hasta la abstracción. Así la representación de las montañas por círculos superpuestos. Estas convenciones son comunes a varios manuscritos.

Sin embargo lo decorativo nunca triunfa sobre lo simbólico: a pesar de la simplificación de las formas, de la multiplicidad de los ángulos de visión en una misma escena, las imágenes siguen siendo eminente y claramente la referencia. El esquematismo y la ornamentación nunca predominan sobre la legibilidad

Algunos manuscritos llevan "páginas-tapices", páginas completas situadas generalmente al principio del libro, dónde se encuentra, entre motivos geométricos y laberínticos, la información sobre el escriba, el pintor o el destinatario del manuscrito. Estas páginas imitan las encuadernaciones (contemporáneas del libro, pero también parece coptas), y se asemejan a veces a alfombras persas o turcas

En el Beato de Saint-Sever, al cual se reservará una parte especial, se encuentran páginas-tapices donde los entrelazados parecen de inspiración irlandesa.

Es necesario volver de nuevo sobre la importante cuestión de los ángulos de visión. No hay perspectiva en la pintura mozárabe ni, en particular, en el Beato. Además, la bidimensionalidad de las figuras conduce a representarlas simultáneamente bajo varias caras, -lo que es también una particularidad del arte copto. Pero, mientras que el recortado y el tres-cuartos están presentes en las representaciones coptas, los manuscritos mozárabes rechazan todo lo que podría bosquejar una tridimensionalidad. No sólo una figura puede componerse de una cara y de un perfil, sino establecer los detalles de cada uno de estos dos aspectos de un elemento y pueden establecerse de manera al parecer incoherente de cara o perfil.

El Beato de Urgell presenta una imagen similar.

A veces una página muestra una ciudad cuyas murallas se ven de frente y arriba se ve lo que se encuentra dentro del recinto. Lo que importa al pintor, es representar todos los elementos esenciales de una visión, como si el espectador se encontrara al mismo nivel con cada uno ellos.

El autor de esta tesis sobre la pintura mozárabe destaca que lo que importa realmente para el artista, es la cohesión conceptual y no la cohesión perceptiva. Cada elemento está en relación directa con el espectador, pero no mantiene relación estructural con los otros elementos. La imagen no es el lugar donde se organizan los conjuntos de objetos para ofrecer la representación de una escena real; es la disposición de los elementos del relato, tomado uno a uno, lo que debe afectarnos por su dimensión simbólica.

El Beato ofrece así una audaz desmultiplicación de las escenas destinada a favorecer la lectura espiritual.

El arte, aquí, se hace auxiliar del sentido profundo de un texto. La visión apocalíptica no es la simple obra de arte: es tentador decir que el viaje místico en el Beato es indispensable para completar y purificar nuestra intelección de la palabra de San Juan.

Todos los artistas de talento no son necesariamente genios creativos. Estos últimos fueron los que produjeron obras originales, no en el sentido vulgar del término, sino en que ellas fueron el origen de otras obras y de nuevas maneras de plantear y solucionar problemas estéticos. Así determinados Beatos proceden de un pensamiento fundador, mientras que otros, solo son suntuosos ejercicios de escuela.

Es el caso del Beato pintado por Facundo para Fernando I de Castilla y León y la reina consorte Sancha de Pamplona, y acabado en 1047. Las miniaturas no tienen originalidad en la composición. La obra nos encanta por sus colores brillantes debidos sobre todo a un notable estado de conservación, y también por la elegancia de las formas. No obstante, es necesario reconocer que Facundo sigue meticulosamente, en cuanto a la estructura, las miniaturas del Beato de Urgell realizado en La Rioja o León en el año 975.

Basta con comparar el f° 19 de Urgell (el Cristo portador del Libro de Vida) con el f° 43 de Facundo; la doble página 140v 141 de Urgell (la Mujer y el Dragón) con la doble página 186v 187 de Facundo; la doble página 198v 199 de Urgell (la Nueva Jerusalén) con la doble página 253v 254 de Facundo. Podríamos enumerar bastantes más páginas aún.
Facundo se inspira también en gran medida en el Beato de Valladolid, terminado por el pintor Oveco en 970. Se compara el f° 93 del Beato de Valladolid y el f° 135 de Facundo; el f° 120 del Beato de Valladolid comparado al f° 171 de Facundo.
Facundo también está muy influido por el arte de Magius (Beatus de Pierpont, acabado en 960), todas estas obras presentan una evidente filiación con el "Codex biblicus legionensis", biblia mozárabe de 960 pintada por Florentius y conservada en la colegiata de San Isidoro de León.

Facundo no inventa. Suaviza las líneas, confiere más finura a sus personajes y propone imágenes que nos parecen más seductoras. Pero la seducción no es el fin del arte, y algunos especialistas dirán que su obra indica una pérdida de sabor con relación a la estética de los Beatos previos.

Ya mencionamos este manuscrito cuya especificidad es necesario destacar. Se conserva en la Biblioteca Nacional de Francia (sigla Ms Lat. 8878).

Es el único Beato conocido copiado en la época románica al norte de los Pirineos. Comprende el Comentario sobre el Apocalipsis de Beato de Liébana, así como el Comentario sobre el libro de Daniel de San Jerónimo. El programa de iluminación se distribuye así:


En los 292 folios, hay 108 miniaturas, 84 de las cuales historiadas (entre ellas, 73 páginas completas y 5 en doble página). Las páginas miden 365 x 280 mm.

Se realizó durante el mandato de Gregorio de Montaner que duró 44 años (de 1028 a 1072), por lo tanto hacia mediados del siglo XI. Conocemos el nombre de un escriba, que era quizá también pintor: Stephanius Garsia. Las diferencias estilísticas inclinan a pensar que hay varios escribas y pintores. Pero a pesar de eso las imágenes presentan una determinada unidad:


Este carácter se ve, por ejemplo, en la representación de la Nueva Jerusalén: como en todos los manuscritos mozárabes, está constituida por un cuadrado, pero en Saint Sever, las arquerías son románicas, de curva entera, y no visigóticas con arcos de herradura.

Los artistas "del Beato" quisieron evitar un exceso de imágenes redundantes con relación al texto, recalcadas sobre las palabras, las reemplazan en nuestra percepción. Los artistas de los siglos X y XI, lo vimos, solucionaron el problema irrealizando las escenas, renunciando a todo elemento de decorado inútil con el fin de no sumergir la mirada del lector en todo lo que descartaría el espíritu de la parte fundamental. Entonces las miniaturas se liberan, purificadas de todo lo que puede darse por anecdótico.

El artista puede también agregar a la figuración de una visión datos resultantes del comentario de Beato. Es lo que se puede ver en el "Beato" de Osma (f° 117v) y en el "Beato" de Facundo (f° 187), donde al Diablo se le presenta encadenado en el infierno y donde los Ángeles reúnen a los que barrió la cola del Dragón. Aquí la miniatura se apropia el comentario de Beato que, con motivo del libro XII del Apocalipsis, se anticipa al libro XX donde se menciona que se encadenó a Satanás.

El trabajo del pintor puede ser más complejo aun cuando, en una misma miniatura, procede a una audaz síntesis de varios pasajes. Debe entonces renunciar a la transcripción literal. Si los 24 Ancianos (o Sabios, o Viejos) corren el riesgo, en un espacio reducido, de causar un bullicio que encubriría otros datos esenciales... ¡se representan 12! Poco importa: ¡sabemos que son 24, puesto que los textos lo dicen y que otras páginas los muestran al completo! Se aclaran un poco las filas, se suprimen algunas alas en otra parte, y así se tiene el lugar suficiente para ofrecer una visión global extraída de dos capítulos. Es lo que se puede ver en la admirable miniatura del f° 117v del "Beato" de Facundo:

"La gran Teofanía se continúa con esta miniatura del Beato de Facundo (folio 117 reverso, formato 300 x 245 mm., diámetro del círculo 215 mm.) que une dos pasajes del texto en una única imagen (Apoc. IV 2 y 6b-8a, así como V 6a y 8) para constituir la visión del Cordero místico." Pero el ilustrador se toma libertades con el texto. Así los cuatro Seres del Tetramorfos, que simbolizan a los cuatro Evangelistas (llevan cada uno un libro) no lleva cada una seis alas, sino un solo par, cubierta de ojos; por otra parte, están encima de una especie de disco inspirado en las famosas ruedas del carro de Yahvé, en Ezequiel (I 15), según una fórmula muy antigua que es frecuente en la iconografía del Tetramorfos. En cuanto a los Veinticuatro Sabios, se reducen a doce solamente, que realizan las acciones descritas (Apoc. V 8): cuatro de ellos "se arrodillan", otros cuatro "tienen cítaras" (siempre de tipo árabe), y los cuatro últimos tienen en su mano "copas de oro llenas de perfumes". En el centro, finalmente el Cordero, portador de la cruz asturiana está en posesión de un relicario que simboliza el Arca de la Alianza. Sobre el círculo figura la puerta abierta al cielo, un arco en herradura contiene el trono divino (Apoc. IV 2) "con el Que se sienta sobre este trono".

La síntesis de los pasajes IV-4 y V-2 del Apocalipsis es muy frecuente. Se la encuentra incluso en a los folios 121v y 122 del "Beatus" de Saint Sever.

El gran historiador del arte medieval francés Émile Mâle creía ver la influencia del "Beato" en los capiteles de la torre-soportal de Saint-Benoît-sur-Loire (antiguamente Fleury-sur-Loire), y María- Madalena Davy concede un cierto crédito a esta tesis Pero Eliane Vergnolle, en su obra principal sobre Saint-Benoît-sur-Loire pone de manifiesto de manera completamente convincente que las capiteles historiados de la torre del abad Gauzlin, se inscribían en la tradición carolingia -algunos recuerdan incluso las formas de las miniaturas "del Apocalipsis de Tréveris", o del "Comentario sobre el Apocalipsis de Aymon de Auxerre " (este último manuscrito conservado en la Bodleian Library en Oxford).

Sabemos también que Gauzlin extendió la influencia de la abadía de Fleury hasta Italia, de ahí hizo venir a un pintor llamado Nivard para representar escenas del Apocalipsis en las paredes de la iglesia -lo que confirma la orientación iconográfica carolingia, más bien que mozárabe, de los decorados de la abadía. La cuestión sería más propensa a controversia por lo que concierne al segundo gran edificio al que Émile Mâle se refiere: el Tímpano de San Pedro de Moissac. Como tantos otros, Margarita Vidal sigue con determinación la lección de Émile Mâle y piensa que este tímpano ofrece indicios fiables de la presencia de un manuscrito iluminado del "Comentario sobre el Apocalipsis" de Beato de Liébana en la biblioteca de la Abadía. Sin embargo, por lo que se refiere al tema de este artículo, se imponen reservas:





No obstante, es necesario reconocer algunas analogías estilísticas inquietantes entre la doble página 121v-122 del "Beato" de Saint Sever] y el Tímpano de Moissac. Hay, por ejemplo, en las dos obras el audaz giro de la cabeza del toro en tensión adorante en dirección al Cristo.

Sin embargo, si hay algunas semejanzas entre los Veinticuatro Sabios del Beato (misma doble página) y los del tímpano (peinados, cítaras, cortes), estos últimos ofrecen una alegre animación no sin nobleza, -mientras que los del "Beato " parecen una banda de graciosos que alzan sus copas durante una canción tabernaria: la majestad de Moissac no debe nada al alboroto de Saint Sever... Lo que no priva de nada a la innegable belleza de tantas otras páginas de ese mismo manuscrito. En cuanto a los "Beatos " mozárabes, no se deben despreciar porque no hagan de modelos para otras artes. No tienen la amplitud de su difusión y de su posible influencia. Y aunque no tuvieran ninguna posteridad, seguirían siendo, en nuestra percepción estética, monumentos tan grandiosos que, como las enigmáticas estatuas de la isla de Pascua, tienen el poder de hacer brotar en nosotros los sueños de otro universo.

Para Hegel, la filosofía es la más alta actividad del espíritu, ya que traduce a conceptos lo que la religión dice en relatos, que, ellos mismos, tenían en palabras lo que el arte presentaba en imágenes. Ciertamente, para él la verdad se hace perceptible en la belleza de una forma sensible; no obstante el espíritu solo reconquista el ser en su totalidad comprendiendo que la Naturaleza no es más que el espíritu que se exilia de sí y que hay una consubstancialidad de lo real y lo racional. Todo es comprensible por el espíritu porque, en el fondo, todo es espíritu.

El examen de la pintura mozárabe trastorna esta jerarquía. Viajando en las páginas de los "Beatos", no estamos ante realidades sensibles aún próximas a las realidades naturales. Estamos en un mundo de imágenes que hablan mejor al alma que lo harían las palabras apoyando los conceptos, y que, al contrario, facilitan por su abstracción el acceso a la verdad del relato, sin por ello favorecer una pura seducción estética por la preponderancia de la ornamentación. Como si estallara en colores de fuego el momento mudo de un éxtasis, el inefable sentido del texto encontrándose cristalizado en formas y colores "surreales".

El término de ilustración no conviene absolutamente para nombrar producciones artísticas que son obras de arte de pleno derecho. En convento San Marco de Florencia, Fra Angélico no ilustra los Evangelios: al mismo tiempo que nos da la belleza de sus frescos, ofrece a nuestra inteligencia el fruto de su meditación sobre los textos.

Los "Beatos" no son una inútil paráfrasis del Apocalipsis (o de su comentario por el monje de Liébana): son visiones nacidas de una visión, de nuevas capas de verdad añadidas al texto profético. Así la Belleza no es más que una etapa en la ruta que conduce a la Verdad: el fuego de los colores se mezcla en el brasero de las palabras para lanzar en nuestros almas deslumbradas nuevas gavillas de significados.

Entre los 31 "Beatos " (de algunos de los cuales no quedan más que fragmentos), es necesario distinguir:


Beato de Liébana, es reconocido internacionalmente por los historiadores de geografía y cartografía. Se considera que su trabajo influyó de gran manera en este campo durante siglos con el tipo de Mapa de T en O





</doc>
<doc id="19350" url="https://es.wikipedia.org/wiki?curid=19350" title="Premios Princesa de Asturias">
Premios Princesa de Asturias

Los Premios Princesa de Asturias (Premios Príncipe de Asturias hasta 2014) están destinados a galardonar la labor científica, técnica, cultural, social y humana realizada por personas, instituciones, grupos de personas o de instituciones en el ámbito internacional, aunque con especial atención en el ámbito hispánico.

En octubre de 2014, el patronato de la Fundación Príncipe de Asturias aprobó que tanto la institución como los premios pasaran a denominarse «Princesa de Asturias», en alusión a la heredera de la Corona, la princesa Leonor de Borbón. En 2015, Wikipedia se hizo acreedora del premio en la categoría Cooperación Internacional.

Fueron declarados de "excepcional aportación al patrimonio cultural de la Humanidad" por la Unesco en el año 2005.

Fue el periodista Graciano García quien impulsó la idea de la puesta en marcha de una fundación que llevaría en primer lugar el nombre de Fundación Principado de Asturias, para luego pasar a llamarse Fundación Príncipe de Asturias; según sus propias palabras:""Para los asturianos [la Carta Magna de 1978] tenía un significado especial porque se recuperaban las instituciones más significativas de nuestra historia: el título de Príncipe de Asturias para el heredero de la Corona y el de Principado para nuestra comunidad. En estas circunstancias, surgió mi idea de crear una Fundación que estableciera vínculos firmes entre el Príncipe y su Principado y vertebrara esa relación a través del fomento de la cultura, el aliento de la concordia y la cooperación entre los pueblos.""El 24 de septiembre de 1980 se firmó la carta constitucional de la Fundación. El acto de la firma se celebró en el Salón Covadonga del Hotel de la Reconquista de Oviedo, en una ceremonia presidida por el entonces príncipe de Asturias, Felipe de Borbón y Grecia, y con la presencia de los reyes de España.

El entonces presidente de la Caja de Ahorros de Asturias, Adolfo Barthe Aza, que presidía la Comisión Gestora de la Fundación, abrió el solemne acto agradeciendo la presencia de los miembros de la familia real, para quien deseó que «la Fundación sea, desde ahora, su segunda casa».

Los Premios Príncipe de Asturias se otorgan en la ciudad de Oviedo, capital del Principado de Asturias, en una solemne ceremonia que se realiza anualmente en el Teatro Campoamor. A esta ceremonia asisten personalidades del mundo cultural, empresarial y deportivo de España, así como autoridades políticas del Gobierno regional y nacional.

La ceremonia fue presidida entre los años 1981 y 2013 por el entonces príncipe Felipe de Borbón que solamente faltó a la cita en el año 1984 por causa de sus estudios; en principio acompañado en el escenario del Teatro Campoamor por sus padres, los reyes de España, y ya a partir de su mayoría de edad y hasta su enlace con la ahora reina Letizia, presidirá unipersonalmente la mesa.

Su madre, la reina Sofía ha estado presente en la ceremonia todos los años, presenciando desde la falta de su marido, el rey Juan Carlos I, el acto desde el palco de honor del Teatro Campoamor, en la mayoría de ocasiones en solitario.

En la edición de 2014 vuelve a presidir Felipe de Borbón y Grecia pero, en esta ocasión, convertido en rey Felipe VI, en nombre de su hija, la ya princesa de Asturias, Leonor de Borbón, quien previsiblemente se hará cargo de la ceremonia en un futuro no determinado, cuando sus padres, los reyes, lo consideren adecuado.

Cada premio consta de un diploma, una escultura de Joan Miró representativa del galardón, una insignia con el escudo de la Fundación Príncipe de Asturias, y una dotación económica de  euros. Si el premio fuera compartido, correspondería a cada galardonado la parte proporcional de su cuantía.

Si el galardonado no acude a la ceremonia de entrega de los premios no recibe ni la dotación económica ni la escultura. Y ello incluso en el caso de que la ausencia se deba a razones de fuerza mayor. Fue sonado el caso de Bob Dylan en 2007, quien rehusó acudir a Oviedo y solicitó infructuosamente que le fuera enviada la escultura. El escritor Philip Roth tampoco pudo asistir en 2012, pero debido a razones médicas justificadas. Y Pau y Marc Gasol no fueron autorizados en 2015 por sus respectivos equipos de baloncesto. El resultado es el mismo en todos los casos.

Según las bases, pueden presentar propuesta razonada de candidatos a los Premios Príncipe de Asturias todos los galardonados en ediciones anteriores, aquellas personalidades e instituciones a quienes la fundación invite, las embajadas españolas, las representaciones diplomáticas en España, los integrantes de cada uno de los jurados respecto de los otros premios, así como personalidades e instituciones de reconocido prestigio.

El fallo de todos los jurados de los premios se realiza entre los meses de abril a junio, salvo los Premios de Concordia, que se falla en el mes de septiembre. Dichas reuniones de jurados y fallos se realizan en el Hotel de la Reconquista, precisamente estos últimos en el Salón Covadonga, en homenaje y recuerdo a ese primer acto de constitución de la Fundación Príncipe de Asturias.

En sus primeras ediciones estos galardones se concedían en exclusiva a personalidades e instituciones iberoamericanas, es a partir de 1990 y coincidiendo con el décimo aniversario de la creación de la Fundación Príncipe de Asturias, cuando tras un extenso debate, se acordó la ampliación de candidaturas al ámbito universal, contribuyendo así a su mayor proyección internacional.

La primera ceremonia de entrega de los Premios Príncipe de Asturias se celebró un sábado, 3 de octubre de 1981, en el Teatro Campoamor de Oviedo; esta fecha venía en cada edición determinada por las agendas de los reyes de España y el príncipe de Asturias, sin embargo se fue haciendo patente la necesidad de establecer una fecha exacta, al menos con un año de antelación, a fin de que los diferentes agentes interventores pudieran hacer frente a sus compromisos.

Finalmente es en el año 1994 cuando se decide de manera no oficial fijar la fecha el último viernes de octubre, siempre y cuando no coincida con un puente debido a la cercana festividad de Todos los Santos, que cae el 1 de noviembre.

En estas primeras ediciones y a falta de precedentes, la imagen y protocolo de la ceremonia de entrega de los Premios va sufriendo múltiples variaciones hasta llegar a su imagen y ceremonial actual a partir del año 1998.Fue en octubre de 2014, el patronato de la Fundación Príncipe de Asturias aprobó que tanto la institución como los premios pasaran a denominarse «Princesa de Asturias», en alusión a la heredera de la Corona, la princesa Leonor de Borbón. En el año 2005, la Unesco consideró los Premios Princesa de Asturias de "excepcional aportación al patrimonio cultural de la humanidad".

En la edición de 2017 los Premios Princesa de Asturias estuvieron fuertemente marcados por el independentismo catalán. Es por ello que además de la manifestación por la república que se celebraba desde años atrás en la plaza de la Escandalera, también se produjo una manifestación por la unidad de España en frente del Teatro Campoamor que contó con centenares de asistentes. Además de los propios manifestantes, se pudo apreciar la gran cantidad de banderas españolas que la gente portaba. En esta edición, la ganadora del Premio Princesa de Asturias a la Concordia fue la Unión Europea, lo que contó con la presencia de Donald Tusk, presidente del Consejo Europeo; el presidente de la Comisión Europea, Jean-Claude Juncker y el presidente del Parlamento Europeo, Antonio Tajani. Tusk, en su discurso, apoyó sin cisuras al gobierno central ante el desafío catalán diciendo que "la ley tiene que ser cumplida por todos". Aunque ya se sabía de días atrás, en está edición de los Premios Princesa de Asturias se pudo confirmar el apoyo total de la Unión Europea hacia el Gobierno de España. Finalmente, esta edición cerró con el célebre discurso del rey (uno de los más importantes del año) argumentando que "el conflicto de secesión se resolverá por medio de la Constitución". Este fue el primer año que los Premios Princesa de Asturias contaron con la presencia del presidente del Gobierno, Mariano Rajoy, desde el mandato de Leopoldo Calvo Sotelo.

Los Premios Príncipe de Asturias fueron establecidos en 1981, en principio en seis categorías: Artes, Ciencias Sociales, Comunicación y Humanidades, Cooperación Internacional (anteriormente denominado Cooperación Iberoamericana), Investigación Científica y Técnica y Letras. El Premio a la Concordia fue establecido en el año 1986, y el último en incorporarse fue el de Deportes, que se concede desde el año 1987.

En la actualidad los Premios Princesa de Asturias tienen ocho categorías:




</doc>
<doc id="19355" url="https://es.wikipedia.org/wiki?curid=19355" title="INTA">
INTA

La sigla INTA puede referirse a:

</doc>
<doc id="19356" url="https://es.wikipedia.org/wiki?curid=19356" title="Distrito del Ensanche">
Distrito del Ensanche

El Ensanche (en catalán "Eixample") es el nombre que recibe el distrito segundo de la ciudad de Barcelona, que ocupa la parte central de la ciudad, en una amplia zona de 7,46 km² que fue diseñada por Ildefonso Cerdá.

Es el distrito más poblado de Barcelona en términos absolutos (262.485 habitantes) y el segundo en términos relativos (35.586 hab/km²). 

En el distrito del Ensanche es donde se pueden encontrar algunas de las vías y plazas más conocidas de Barcelona, como el paseo de Gracia, la rambla de Cataluña, la plaza de Cataluña, la avenida Diagonal, la calle de Aragón, la Gran Vía de las Cortes Catalanas, la calle de Balmes, la ronda de San Antonio, la ronda de San Pedro, el paseo de San Juan, la plaza de la Sagrada Familia, la plaza de Gaudí, y en sus extremos, la plaza de las Glorias Catalanas y la plaza Francesc Macià.

Asimismo, en el Ensanche se encuentran numerosos puntos de interés turístico y ciudadano como la Basílica de la Sagrada Familia, la Casa Milà, la Casa Batlló, el Teatro Nacional de Cataluña, el Auditorio de Barcelona, la Plaza de toros Monumental, la Casa Terradas, así como numerosos cines, teatros, restaurantes, hoteles y otros lugares de ocio, como parques.

Durante la primera mitad del siglo XIX, en pleno auge de la Revolución Industrial, las ciudades que hasta entonces continuaban teniendo un urbanismo medieval, muchas de ellas rodeadas de murallas, se ven colapsadas por la instalación de las recién nacidas industrias y la expansión demográfica.

La ciudad de Barcelona, al igual que muchas otras ciudades europeas, no es ajena a esta situación; pero en su caso, las propias murallas, la situación política y el hecho de que todos los terrenos exteriores a la muralla se considerasen zona militar, impiden que se puedan instalar en sus alrededores las nuevas industrias debido a la prohibición de construir en ese gran espacio llano, teniendo un uso exclusivamente agrícola por parte de los payeses (campesinos) de Barcelona y las poblaciones cercanas.

La expansión demográfica y las industrias se trasladan a zonas que en la época eran municipios independientes, hoy barrios de la ciudad, tales como: Sants, Sarriá, Gracia, San Andrés o San Martín. La necesidad de comunicarse con estas poblaciones da origen a una serie de vías que hoy siguen formando parte del tramado urbano. Entre ellas es claramente reconocible el actual paseo de Gracia, que comunica Barcelona con Gracia y que durante esa época constituyó no solo una vía de comunicación, sino un verdadero lugar de encuentro, paseo y esparcimiento, creándose a los lados de la vía jardines y zonas de recreo utilizadas tanto por los habitantes de Barcelona como por los de Gracia, llegando a existir una línea regular de transporte de viajeros en carruajes de caballos, precursora de las actuales líneas de autobuses.

La apremiante necesidad de expansión y la existencia de un corto periodo de gobierno progresista, entre 1854 y 1856, dan lugar a la demolición de las murallas, quedando con ello abierto el camino que llevaría a la Barcelona actual.

En 1855 el Ayuntamiento de Barcelona, pese a que no había intervenido directamente en él, considera inicialmente el proyecto de ensanche diseñado por el ingeniero de caminos, canales y puertos Ildefonso Cerdá. El plan define una ciudad jardín con grandes espacios abiertos, los edificios, de solo tres plantas están muy distantes entre sí separados con anchas calles y no existe diferenciación entre clases sociales al ser todas sus calles iguales. Este cúmulo de circunstancias provoca que la burguesía de la época considerara su propuesta como un despilfarro de terreno, existiendo un claro conflicto de intereses entre las partes. Las protestas de esta burguesía y su indudable influencia política hace que el ayuntamiento dé marcha atrás y rechace el plan inicialmente aprobado.

Vista la necesidad real de elaborar un proyecto que permita la expansión de la ciudad, el ayuntamiento convoca en 1859 un concurso de proyectos urbanísticos del que resulta ganador el proyecto del arquitecto Rovira i Trias. El proyecto es, naturalmente, más acorde a las pretensiones de la burguesía que el plan de Cerdá: las calles tan solo tienen 12 m de ancho, se considera la posibilidad de sobrepasar las alturas propuestas por Cerdá, existe una clara separación de clases sociales y las edificaciones presentan una mayor densidad.

Ante la aprobación del proyecto de Rovira i Trias en 1860, el gobierno central de Madrid impone pocos meses después, por Real Decreto, el plan de Ildefonso Cerdá, comenzándose casi de inmediato su ejecución, no sin las protestas del pueblo barcelonés. No obstante, y fruto tal vez de las fuertes presiones, el propio Cerdá en 1863, retoca ligeramente su plan para aumentar la superficie edificable. 

Pese a que durante varias décadas hubo resentimiento por parte del pueblo barcelonés y que el resultado final que conocemos hoy del Ensanche de Barcelona ha sufrido muchas modificaciones sobre el que inicialmente propuso Cerdá, nadie duda hoy que el plan impuesto por decreto fue mejor que el aprobado en el concurso y que el resto de los presentados.

En “Monografía estadística de la clase obrera”, Cerdá escribe su preocupación por el crecimiento de la densidad de las  industrias y de la población. Estos estudios lo llevan a realizar un estudio comparativo con otras ciudades importantes de la época, tales como Boston, Turín, San Petersburgo y Buenos Aires, publicado en el atlas de la Teoría donde busca estudiar los métodos utilizados para aplicarlos es su proyecto de ensanche.

Cerdá propone en su proyecto un “ensanche ilimitado”, desarrollar toda la planicie hasta sus límites naturales: Collserola al norte, el río Besós al este, el Montjuic al oeste y el mar Mediterráneo al sur.  Elige para su proyecto un plan hipodámico o damero, que ya había sido utilizado en ciudades coloniales como Filadelfia y Buenos Aires, siendo la opción más eficiente para la explotación intensa del terreno, otorgando además un valor equitativo al espacio.

Entiende además que hay dos conductas fundamentales en la vida del ser humano, el traslado y el reposo. La nueva ciudad está basada en dos elementos, los que favorecen el desplazamiento, que son las vías, y los espacios pensados para la quietud y el reposo, las intervías.

Las intersecciones se convirtieron con el tiempo en la imagen más fuerte de Barcelona. Los chaflanes a 45° responden a la necesidad de dar respuesta, no solo a condiciones de mayor visibilidad e iluminación, sino a las distintas situaciones de circulación, tanto vehicular como peatonal.

La generosa proporción entre la cantidad de suelo edificado proviene de las bases del concurso de 1859 que establecen que el espacio edificado sea el mismo al destinado a jardines. En los planos originales, Cerdá diseña los edificios en el perímetro de la manzana liberando el interior de manzana, dicho espacio interior debe estar destinado a jardín para favorecer las ventilaciones y permitir un mayor grado de iluminación. De este modo los edificios del eixample poseen una fachada principal a la calle y otra hacia el pulmón de manzana.

La gran extensión de terreno que corresponde al Ensanche de Cerdá, desde Montjuic hasta el río Besós y desde los límites de la ciudad medieval hasta las antiguas poblaciones vecinas, está concebida como una cuadrícula regular formada por los ejes longitudinales de sus calles, separados entre sí por una distancia de 133,3 m, la regularidad de esta cuadrícula es imperturbable a lo largo de todo el trazado urbano y está justificada, una vez más, en términos de igualdad, ya no tan solo entre clases sociales, sino en la comodidad del tránsito de personas y vehículos, ya que de este modo, tanto si se circula por una vía como si se hace por sus transversales, los cruces entre ellas se encuentran a igual distancia, y al no existir unas vías más cómodas que otras el valor de las viviendas tenderá a igualarse.

Por lo que respecta a la orientación, las vías discurren en dirección paralela al mar unas, y en perpendicular las otras, esto hace que la orientación de los vértices de los cuadrados coincida con los puntos cardinales y que por lo tanto todos sus lados tengan luz directa del sol a lo largo del día, denotando una vez más la importancia que el diseñador concede al fenómeno solar.

Las calles tienen por lo general una anchura de 20 m de los cuales en la actualidad los 10 m centrales están destinados a calzada y 5 m a cada lado destinados a aceras, no obstante y debido a variadas necesidades, diseñó algunas vías más anchas, sin que ello perturbe la cuadrícula regular de 133,3 m, sino que para conseguirlo redujo adecuadamente las dimensiones de las manzanas afectadas por el ensanchamiento de las vías, así podemos hablar de la Gran Vía de las Cortes Catalanas bajo la cual circulan el metro y el tren, la calle de Aragón por la que durante muchos años transitó el ferrocarril al aire libre hasta que finalmente fue soterrado, la calle Urgel y otras.

Mención especial merece el diseño del Paseo de Gracia y la Rambla de Cataluña, donde con el fin de respetar el antiguo camino de Gracia y la vertiente natural de las aguas, de ahí el nombre de rambla, trazó solo dos vías consecutivas de anchura especial donde en realidad atendiendo al tramado de 133,3 m debería haber tres calles. Además el paseo de Gracia, por respetar el antiguo trazado, no es exactamente paralelo al resto de las calles lo que hace que las manzanas existentes entre las dos vías citadas, si bien son cuadradas y con chaflanes, presentan irregularidades que les dan forma de trapecios.

A todo ello hay que añadir la presencia de algunas calles de carácter especial que no siguen el trazado reticular sino que lo atraviesan en diagonal, tales como la propia Avenida Diagonal, la Avenida Meridiana, la calle Pedro IV, y otras que fueron trazadas respetando la existencia de antiguas vías de comunicación con los pueblos vecinos.

Las dimensiones de las manzanas vienen dadas por las distancias antes mencionadas entre los ejes longitudinales de las calles y la propia anchura de estas vías, de modo que al establecer una anchura estándar de las vías en 20 m, las manzanas están formadas por cuadriláteros de 113,3 m, truncados sus vértices en forma de chaflán de 15 m, lo que da una superficie de manzana de 1,24 ha, contrariamente a la creencia popular de que las manzanas tienen una superficie exacta de 1 hectárea.

Cerdà justificó el chaflán de los vértices de las manzanas desde el punto de vista de la visibilidad que ello da a la circulación rodada y en una visión de futuro en la que no se equivocó más que en el término empleado para definir el vehículo, hablaba de las locomotoras particulares que un día circularían por las calles y de la necesidad de crear un espacio más amplio en cada cruce para favorecer la parada de estas locomotoras.

Dentro del espacio de cada manzana, Cerdá concibió dos formas básicas para situar los edificios, una presentaba dos bloques paralelos situados en los lados opuestos, dejando en su interior un gran espacio rectangular destinado a jardín y la otra presentaba dos bloques unidos en forma de “L” situados en dos lados contiguos de la manzana, quedando en el resto un gran espacio cuadrado también destinado a jardín.

La sucesión de manzanas del primer tipo daba como resultado un gran jardín longitudinal que atravesaba las calles y la agrupación de 4 manzanas del segundo tipo, convenientemente dispuestas, formaba un gran cuadrado edificado atravesado por dos calles perpendiculares y con sus cuatro jardines unidos en uno.

En este estado del proyecto, y habida cuenta de las dificultades que tenía en cuanto a la oposición a él por parte del pueblo barcelonés, no tardaron en aparecer actividades especulativas y argumentos que trataban de conseguir un mayor espacio construido, el primero de ellos fue que si las calles tenían 20 m de ancho, bien podía aumentarse el ancho de los edificios a esa misma distancia, se ocupó posteriormente la zona central de las manzanas con edificaciones más bajas, destinadas en la mayoría de los casos a talleres y pequeñas industrias familiares, desapareciendo con ello la mayor parte de los jardines centrales, con lo que como último recurso para aumentar el suelo construido se unieron los dos laterales ya construidos con edificios que los unían, cerrando por completo las manzanas.

Parecía que aquí iba a acabar el proceso especulativo, pero un nuevo argumento se sumó a él. Si las calles tenían 20 m de ancho, no habría inconveniente en que los edificios tuvieran una altura de 20 m en lugar de los 16 m proyectados, ya que aun con esta altura, estando el sol a 45º, iluminaría cualquier edificio en su totalidad sin que ningún edificio vecino le hiciera sombra, este argumento unido a la construcción de techos más bajos dio como resultado que se ganaran dos pisos de altura.

Por último, teniendo en cuenta una parte de la teoría anterior. Si se construyen sobre el edificio actual un piso más, pero con la fachada retirada hacia el interior del edificio tanto como la altura de este piso, se conseguiría aumentar el espacio construido sin que la sombra del edificio afecte a los edificios vecinos estando el sol a 45ª, naciendo de este modo el piso ático y por la misma teoría se construyó el sobreático, retirando la fachada otro tanto hacia atrás.

Pese a todo o gracias a ello, ya que Cerdá concibió una ciudad utópica, el Ensanche actual tiene plena vigencia, después de 150 años. En pleno inicio de Siglo XXI el Ensanche sigue siendo el corazón de la Barcelona actual, y continúa su construcción ya que si bien la parte central representada por el distrito del mismo nombre presenta la fisonomía típica del diseño inicial, durante muchos años permaneció parada la evolución de la cuadrícula de Cerdà hacia el río Besós, el distrito de San Martín de Provensals, cuyos terrenos formaban parte del proyecto de ensanche, se ocupó por industrias que por sus características no podían ser ubicadas en la parte central, la necesidad de espacios superiores al de una manzana impidió abrir muchas de las calles proyectadas así durante décadas.

El último gran impulso del ensanche se produce a partir de 1986, cuando Barcelona es nombrada sede de los Juegos Olímpicos de 1992, y se construye la Vila Olímpica del Poblenou respetando el trazado del ensanche. Ello permitió que después de más de un siglo el Ensanche y la Avenida Diagonal lleguen al mar. Las grandes industrias se desplazaron fuera de Barcelona, quedando como testimonio ornamental de la actividad industrial de esta zona, muchas de las chimeneas de las fábricas. Todo ello permite en la actualidad abrir vías y urbanizar zonas nuevas, que en algunos casos se están construyendo rodeados de zonas ajardinadas, muy similares a los diseños originales.

El Ensanche Derecho (de la calle Balmes hacia la derecha), es donde está la gente más adinerada de este distrito. A partir de Paseo San Juan vuelve a bajar de valor. Las cuatro calles más caras son: Paseo de Gracia (actualmente es la segunda calle más cara de Europa), la Avenida Diagonal (calle más importante de Barcelona), la Rambla de Cataluña y la calle Balmes. Se ha de hacer hincapié en que las calles cercanas al Paseo de Gracia tienen un precio muy elevado. Cerca de esta calle vive la gente proveniente de la alta burguesía (al menos eso era antes) además de gente con un poder adquisitivo muy alto a la que le gusta estar cerca del centro y gozar de sus privilegios (buenas comunicaciones: metros, buses, estación de tren...). De forma general, la parte norte del Ensanche Derecho es la más valorada aunque se ha de tener en cuenta que la Plaza de Cataluña es muy conocida y, por consiguiente, de coste elevado. También hay zonas destinadas a clase media-alta en los extremos del barrio (zonas no tan consideradas aunque gozan también de prestigio).




</doc>
<doc id="19360" url="https://es.wikipedia.org/wiki?curid=19360" title="Batalla de Stalingrado">
Batalla de Stalingrado

La batalla de Stalingrado fue un conflicto bélico entre el Ejército Rojo de la Unión Soviética y la Wehrmacht de la Alemania nazi y sus aliados del Eje por el control de la ciudad soviética de Stalingrado, actual Volgogrado, entre el 23 de agosto de 1942 y el 2 de febrero de 1943. La batalla se desarrolló en el transcurso de la invasión alemana de la Unión Soviética, en el marco de la Segunda Guerra Mundial. Con bajas estimadas en más de dos millones de personas entre soldados de ambos bandos y civiles soviéticos, la batalla de Stalingrado es considerada la más sangrienta de la historia de la humanidad. La grave derrota de la Alemania nazi y sus aliados en esta ciudad significó un punto clave y de severa inflexión en los resultados finales de la guerra; representando el principio del fin del nazismo en Europa,pues la Wehrmacht nunca recuperaría su capacidad ofensiva ni obtendría más victorias estratégicas en el Frente Oriental.

La ofensiva alemana para capturar Stalingrado comenzó a finales del verano de 1942 en el marco de la Operación Azul o "Fall Blau", un intento por parte de Alemania de tomar los pozos petrolíferos del Cáucaso. El 23 de agosto, el 6.° Ejército, apoyado por el 4.° Ejército Panzer, logran cruzar la curva del Río Don. Un masivo bombardeo redujo buena parte de la ciudad; mientras las tropas terrestres del 6° Ejército debían tomar la ciudad calle por calle, casa por casa y edificio por edificio, en lo que ellos denominaron «Rattenkrieg» ('guerra de ratas'). A pesar de lograr controlar la mayor parte de la ciudad, la Wehrmacht nunca fue capaz de derrotar a los últimos defensores soviéticos que se aferraban tenazmente a la orilla oeste del río Volga, que dividía la ciudad en dos. En noviembre de 1942, una gran contraofensiva soviética embolsó al 6º Ejército Alemán del general Paulus y parte de 4° Ejército Panzer dentro de Stalingrado, incapaz de escapar del cerco por la negativa de Hitler a renunciar a la toma de la ciudad. Este cerco, llamado por los alemanes «Der Kessel» ('el caldero'), significó el embolsamiento de 250 000 soldados, debilitados rápidamente a causa del hambre, el frío, los continuos ataques soviéticos; y los constantes fracasos del general Von Manstein por intentar romper el cerco, harían, desobedeciendo las órdenes de Hitler, que Friedrich Paulus rindiera su 6º ejército en febrero de 1943.

La derrota alemana en Stalingrado confirmó lo que muchos expertos militares sospechaban: la capacidad logística de las fuerzas alemanas era insuficiente para abastecer y mantener una ofensiva en un frente que se extendía desde el mar Negro hasta el océano Ártico. Esto se confirmaría poco después en la nueva derrota que Alemania sufriría en la batalla de Kursk. El fracaso militar convenció a muchos oficiales de que Hitler estaba llevando a Alemania al desastre, acelerándose los planes para su derrocamiento y dando como resultado el atentado contra Hitler de 1944. La ciudad de Stalingrado recibiría el título de "Ciudad Heroica".

Influido por el geopolítico Karl Haushofer, Adolf Hitler pensaba convertir las tierras de la Unión Soviética en colonias alemanas a las que denominaría Germania. Entre 1939 y 1941, la Alemania nazi estuvo ocupada luchando con sus históricos enemigos de Occidente: Francia y el Reino Unido (véase Batalla de Francia y Batalla de Inglaterra); no obstante, Hitler nunca perdió de vista su verdadero objetivo: invadir el este de Europa y aniquilar a los eslavos.

El 22 de junio de 1941, Alemania invadió la Unión Soviética, incluso cuando Inglaterra no había sido derrotada. Hitler, convencido de la debilidad del Estado soviético, a quien consideraba como un gigante con los pies de barro, creía que sus pueblos se volverían contra Iósif Stalin, permitiéndole concluir la invasión antes del invierno. Sus generales recibieron órdenes de ceñirse al plan, desdeñando sus opiniones. De esta forma, un día antes de la invasión, unos tres millones de soldados alemanes esperarían el inicio de la mayor operación militar hasta la fecha, distribuidos desde Finlandia hasta el mar Negro. Unos 950 000 soldados de otras naciones aliadas de Alemania, peor entrenadas, coordinadas y equipadas, acompañaban a los alemanes.

En diciembre de 1941, la guerra en la Unión Soviética no se había desarrollado tal como el Alto Mando Alemán había planeado. Leningrado y Sebastopol continuaban resistiendo el cerco en el norte y el sur respectivamente, y la ofensiva contra Moscú había llegado a un punto muerto. Entonces, inesperadamente, los alemanes se encontraron con una gran contraofensiva soviética desde la capital rusa y tuvieron que afrontar el hecho de que, a pesar de haber aniquilado y capturado a cientos de miles de soldados del Ejército Rojo en los últimos meses, pactando la no agresión con Tokio, el Alto Mando Soviético había logrado desplegar reservas suficientes, además de las divisiones siberianas dirigidas por el general Georgi Zhúkov, hasta entonces ubicadas en la frontera con Manchuko, para emprender una gran contraofensiva. Tardíamente, y tal como se ha creído durante décadas, los invasores comprenderían que aparentemente las reservas enemigas eran «inagotables». Estudios recientes revelan que las reservas soviéticas tenían preocupado al Alto Mando Soviético en una proporción más grande de la esperada.

Habiendo fracasado en capturar Moscú, Hitler —con sus generales en contra— decidió dirigirse hacia los pozos petrolíferos del Cáucaso, pues el petróleo era el elemento fundamental, del que apenas disponía, para poder sostener la guerra y, además, debilitar verdaderamente a su enemigo. La Operación Azul, como se denominó la campaña alemana en el sur de la Unión Soviética, tenía como objetivo la captura de puntos fuertes en el Volga primero y, posteriormente, el avance sobre el Cáucaso.

El 5 de abril de 1942, Hitler emitió la "Directiva" fundamental "41" con la que definió el desarrollo planificado de la nueva gran ofensiva en detalles tácticos y describió, en realidad de una manera bastante nebulosa, los objetivos geoestratégicos de la operación Azul ("Fall Blau" en alemán) a partir de los cuales esperaba un éxito decisivo. La ofensiva alemana, que habría involucrado a dos grupos de ejércitos, más de 1 millón de soldados con alrededor de 2.500 tanques, apoyada por cuatro ejércitos rumanos, italianos y húngaros (unos 600.000 hombres más). se habría desatado en el sur de Rusia con el objetivo de conquistar las cuencas del Don y el Volga, destruya las industrias importantes de Stalingrado (nodo de comunicación ferroviario y fluvial y centro de producción mecánica muy importante) y luego apunte a los pozos petroleros del Cáucaso, asegurando a Alemania con suficientes recursos energéticos para continuar la guerra Esta ambiciosa directiva se basó principalmente en la suposición errónea de Hitler de un supuesto agotamiento irreversible material y moral del Ejército Rojo después de las enormes pérdidas sufridas en la campaña de 1941-42.

La operación, inicialmente programada para principios de mayo, sufrió retrasos considerables debido a la dura resistencia soviética en el asedio de Sebastopol, la necesidad de llevar a cabo algunas operaciones preliminares de rectificación del frente y oponerse a algunos intentos prematuros e ineficaces. Ofensivas soviéticas (segunda batalla de Jarkov). De hecho, después de estos éxitos alemanes que le costaron a los soviéticos menos de un cuarto millón de pérdidas, favorecieron en gran medida el éxito inicial de la Operación "Blau." 

El 10 de mayo, el general Friedrich Paulus, comandante del 6.º Ejército Alemán, presentó al Mariscal de Campo Fedor von Bock un esbozo de la «Operación Federico». Paulus había tomado el mando del 6.º Ejército recientemente tras el fallecimiento de su anterior comandante, Walter von Reichenau, a consecuencia de un ataque cardíaco sufrido después de hacer ejercicio en la campiña rusa a temperaturas bajo cero.

La Operación Federico significaba la consolidación del frente delante de Járkov, recién capturada por Alemania. No obstante, el mariscal Semión Timoshenko se adelantó a Paulus, emprendiendo el 12 de mayo una contraofensiva desde Vorónezh, cuyo objetivo era precisamente la liberación de Járkov, rodeando al 6.º Ejército en un movimiento de pinza. Cuando 640 000 soviéticos con 1200 tanques se lanzaron contra las fuerzas de Paulus, este se encontró al borde del colapso. Solamente la oportuna llegada del 1.º Ejército Panzer de Ewald von Kleist permitió revertir la situación de la ofensiva y, en lugar de ser capturados, los hombres de Paulus ayudaron a los de Von Kleist a capturar los Ejércitos soviéticos 6.º y 57.º en Barvenkovo. El 28 de mayo unos 240 000 soldados soviéticos fueron embolsados y capturados, e incautan 1.250 carros y más de 2.000 cañones. Es la peor derrota soviética de la guerra.terminando la contraofensiva de Timoshenko. 

El 1 de junio, Adolf Hitler y el mariscal Fedor von Bock presentaron a los generales del Grupo de Ejércitos Sur la Operación Azul en los cuarteles generales de esta unidad, ubicados en Poltava. Al 6.º Ejército de Paulus se le encargó la tarea de limpiar Vorónezh, y dirigirse luego a Stalingrado acompañado del 4.º Ejército Panzer de Hermann Hoth. Una vez allí, se encargarían de destruir los complejos industriales y de proteger el Cáucaso desde el Norte. Aun tratándose del centro administrativo de la costa del mar Caspio y el Delta del Volga, Hitler no priorizó la captura de la ciudad; probablemente debido al deterioro de la línea de ferrocarril que conectaba Stalingrado con, entre otros, el Donbass, el área de Stavropol y el distrito de Daguestán, la cual fue sustituida por carreteras pavimentadas. 

Cómo un adelanto de la ofensiva, el 10 de junio la 1ª Panzerarmee y el 6.°Ejercito alemán, compuestos por 33 divisiones, 5 de ellas Panzerdivisionen y 2 motorizadas, inician una contraofensiva en los sectores de y Kupians; las fuerzas acorazadas se despliegan entre el flanco derecho del Grupo de Ejércitos meridional y el sector Smolensk-Slaviansk. 

En occidente, en Washington el Congreso aprueba la Ley de Prestamos y Arriendos en favor de la Unión Soviética. La ayuda material comenzara a efectuarse a fines de año. 

En cuento a la Operación Azul, se prohibió tajantemente la transcripción de órdenes; todo debía comunicarse de manera verbal. Sin embargo, el 19 de junio, un avión alemán que llevaba anotaciones personales del general Georg Stumme acerca de la operación fue derribado detrás de las líneas enemigas, y los papeles fueron capturados por los soviéticos. No obstante, después de que el general Filipp Gólikov los entregara directamente a Stalin, este los rechazó como falsos, convencido de que Moscú seguía siendo el principal objetivo alemán.

El 26 de junio, la 1.º Panzearmee y el 6.º Ejército alemán, tras 16 días de combates, rechazan el ala izquierda del Frente Sudoccidental soviético, empujando a los rusos hasta las orillas del Oskol, donde se atrincheran.

Caída de Sebastopol, el 11.º Ejército alemán entran en las ruinas de la fortaleza tras meses de resistencia soviética. El general del 11º Ejército de Von Mansteinm es ascendido a mariscal de campo por su brillante campaña de Crimea, que culmina con la toma del fuerte de Sebastopol

El 28 de junio se inició la gran ofensiva alemana en dirección a Vorónezh, y el 30 de junio en la región del Donetsk hacia el sur de Rusia, el Grupo de Ejércitos Sur comenzó bien su ofensiva. Las fuerzas soviéticas ofrecieron poca resistencia en las vastas estepas vacías y comenzaron a fluir hacia el este. Varios intentos de restablecer una línea defensiva fallaron cuando las unidades alemanas los flanquearon. Se formaron y destruyeron dos grandes focos: el primero, al noreste de Jarkov, el 2 de julio, y un segundo, alrededor de , el Óblast de Rostov, una semana después. El avance inicial del 6º Ejército y otros ejércitos aliados fue un éxito. Mientras tanto, desde hace días se combate en las proximidades y en la periferia de la importante ciudad meridional soviética de Voronezh. Von Bock esperaba que los alemanes pudieran tomarla pronto, pero Timoshenko había reforzado su guarnición. Hitler dio la orden de detener el ataque y proseguir la Ofensiva Fall Blau hacia el sur. El 6 de julio, el 4.º Ejército Panzer alemán han entrabado intensos combates con los soviéticos que defienden Voronezh y no pueden retirarse como lo había ordenado Hitler. Pero capturan parcialmente la ciudad. Como los rusos comienzan a retroceder, el Führer ordena conquistar la ciudad. El 4.º Ejército Blindado estuvo completamente involucrado en la batalla de Voronezh durante dos días y los alemanes tardaron un tiempo antes de que pudieran abandonar la línea hasta la llegada del 2.º Ejército húngaro que continuó luchando por el resto de la ciudad. Como parte de la segunda fase de la operación, el cuarto panzer se dirigiría en dirección al Don y el Volga, Sin embargo, fue sometido a un contraataque potente por parte del Ejército Rojo hasta el 13 de julio en la zona del Don y el Donietsk. Meses después, Hitler afirmaría que esos dos días de retraso en Voronezh, junto a otras demoras evitables, permitieron al mariscal Semión Timoshenko reforzar el Don y su gran meandro, evitando la toma de Stalingrado por parte del 4º Ejército Panzer, los cuáles tendrían futuras consecuencias. 

Como parte de la segunda fase de la operación, el 9 de julio, Hitler había ordenado dividir el Grupo de Ejércitos Sur en dos fuerzas. El Grupo de Ejércitos A, comandado por el mariscal Wilhelm List, debía continuar la ofensiva en el Cáucaso. El Grupo de Ejércitos B, incluido el 6º Ejército de Friedrich Paulus y el 4º Ejército Panzer de Hermann Goth, comandados por el mariscalMaximilian von Weichs se trasladarían hacia el Don y el Volga. Para llevar a cabo los dos objetivos simultáneamente, Hitler, no tuvo en cuenta las reservas alemanas de combustible, eran alarmantemente escasas, asumió que el enemigo había agotado en gran medida las masas de sus reservas en el primer invierno de la guerra.

En un informe de Halder al Führer de fecha 13 de julio; los ejércitos alemanes de Von Bock, empeñados en la Ofensiva Fall Blau en el sur de Rusia, no pueden aniquilar a las tropas soviéticas del mariscal Timoshenko, que se repliegan en perfecto orden hacia el este para evitar las maniobras de tenaza germanas. Hitler cree que es una desbandada y cambia los planes de la ofensiva, ordenando a la 4ª Panzerarmee y al que abandonen el objetivo en el Don meandro, dejando dirigirse allí al 6.º Ejército en solitario. 

El 15 de Julio, Hitler y von Bock, comandante del Grupo de Ejércitos Sur, discutieron sobre los próximos pasos en la operación. El acalorado debate y los continuos contraataques soviéticos, que ataron al 4°. Ejército Panzer hasta el 13 de julio, hicieron que Hitler perdiera los estribos y despidiera a von Bock. 

El 4.º Ejército Panzer de Hoth, proseguiría su avance según lo acordado por el alto mando alemán, hacia el sur y se uniría al Grupo de Ejércitos A, debido en más por los lentos progresos en la campaña del Cáucaso, y con ello ayudar en la captura del resto de las fuerzas de Timoshenko, que se esperaba tendría lugar cerca de Rostov del Don, sin lograrlo plenamente. En el avance se produjo un atasco masivo cuando el cuarto panzer y el primer panzer requirieron las pocas carreteras en el área. Ambos ejércitos fueron detenidos mientras intentaban limpiar el desorden resultante de miles de vehículos. La demora fue larga y se cree que le costó el avance al menos una semana. A pesar de los inconvenientes. Rostov fue atacada y reconquistada por el 17.º Ejército y el 1.º Ejército Panzer el 23 de julio. Los alemanes del coronel Reinhardt han ocupado las calles de la ciudad de Rostov, pero los soviéticos continúan la resistencia desde algunos edificios y aún dominan los puentes y los muelles sobre el Don. Las fuerzas alemanas del coronel Reinhardt acaban con toda la resistencia de Rostov, tras 3 días de combates casa por casa.  A mediodía, unidades de vanguardia de la atraviesan el río Don y continúan avanzando hacia el Cáucaso, al sur.

La ciudad tenía una importante industria militar (Stalingrado tenía las fábricas de tractores Octubre Rojo y de cañones Barricady), y poseía el nudo ferroviario crucial de la línea que unía Moscú, el mar Negro y el Cáucaso, existiendo igualmente un puerto fluvial en servicio para la navegación por el Volga. La urbe se extendía unos 24 kilómetros a lo largo de la orilla occidental del Volga pero con menos de diez kilómetros de anchura. No existía ningún puente cruzando el río, empleándose grandes barcazas para comunicar ambas orillas. La orilla oriental apenas estaba poblada. Es importante considerar que la temperatura en el cáucaso es muy extrema tanto en verano como en invierno, durante el cual el frío es tal que el Volga se congela con una capa suficientemente gruesa de hielo como para permitir el paso de vehículos pesados.

Stalin había previsto la rápida caída de Rostov. Por esta razón, el 19 de julio había ordenado que Stalingrado quedase en estado de sitio total y comenzasen los preparativos para resistir ante los alemanes que se acercaban. No se permitió a los civiles abandonar la ciudad, queriendo alentar a la milicia soviética con la permanencia de sus familiares entre los habitantes. No obstante, trabajadores especializados considerados claves de las industria armamentista fueron enviados a los Urales, para seguir trabajando allí.

El 17 de julio, se había dado inicio la ofensiva alemana hacia el Don, a cargo del 6.º Ejército, en cuanto a la defensa; Vasili Chuikov llegaría al Frente de Stalingrado, allí quedaría a cargo del 64.º Ejército Soviético, cuyas principales unidades todavía no habían llegado. Chuikov encontró a sus tropas con la moral muy baja, y fue muy poco lo que pudo hacer para evitar ser obligado a cruzar el Don. La llegada de la aviación rusa, que mantuvo ocupados a los Messerschmitt 109 alemanes hasta inicios de agosto, aliviaron a las castigadas fuerzas terrestres. 

A mediados de julio, los alemanes habían empujado a las tropas soviéticas de vuelta hacia el margen del río Don, a pesar de la escasez de combustible. En este punto, los ríos Don y Volga están a solo 65 km (40 millas) de distancia, y los alemanes dejaron sus principales depósitos de suministros al oeste del Don, lo que tuvo implicaciones importantes más adelante en el curso de la batalla. Los alemanes comenzaron a usar los ejércitos de sus aliados italianos, húngaros y rumanos para proteger su flanco izquierdo (norte). Ocasionalmente, las acciones italianas se mencionaban en comunicados oficiales alemanes. Las fuerzas italianas generalmente eran tenidas en poca consideración por los alemanes, y fueron acusadas de tener baja moral: en realidad, las divisiones italianas lucharon relativamente bien, según un oficial de enlace alemán, la y la demostraron tener buena moral, y se vieron obligadas a retirarse solo después de un ataque blindado masivo en el que los refuerzos alemanes no habían llegado a tiempo, según un historiador alemán. De hecho, los italianos se distinguieron en numerosas batallas, como la . 

El 24 de julio de 1942, Hitler reescribiría personalmente los objetivos operativos para la campaña de 1942, ampliándolos en gran medida para incluir la ocupación de la ciudad de Stalingrado. Ambas partes comenzaron a atribuir valor propagandístico a la ciudad, basándose en que llevaba el nombre del líder de la Unión Soviética. Hitler proclamó que después de la captura de Stalingrado, matarían a sus ciudadanos varones y deportarían a todas las mujeres y niños porque su población era "completamente comunista" y "especialmente peligrosa". Se suponía que la caída de la ciudad también aseguraría firmemente los flancos norte y oeste de los ejércitos alemanes a medida que avanzaban en Bakú, con el objetivo de asegurar estos recursos petroleros estratégicos para Alemania. La expansión de los objetivos fue un factor significativo en el fracaso de Alemania en Stalingrado, causado por el exceso de confianza alemán y una subestimación de las reservas soviéticas. 

El 25 de julio, los alemanes enfrentaron una fuerte resistencia con una cabeza de puente soviética al oeste de Kalach. "Tuvimos que pagar un alto costo en hombres y material ... en el campo de batalla de Kalach quedaron numerosos tanques alemanes quemados o disparados. Ese día el grueso de la 1.ª Panzerarmee de Kleist cruza el rio Don, pero algunas unidades rezagadas no lo harían hasta pasado mañana.

El 28 de julio, preocupado por el avance alemán hacia el Volga, que amenazaba con dividir a la Unión Soviética en dos, Stalin prohibió la rendición sin importar las razones, y ordenó la formación de una línea en la retaguardia de la infantería que fusilara a todo soldado soviético que retrocediese sin permiso. Esta orden de Stalin, la 227, muy pronto fue conocida popularmente como la orden «¡Ni un paso atrás!». Asimismo, se obligó a combatir también a las mujeres a gran escala. Además, el Ejército Rojo practicaba el envío de ataques masivos frontales a distancias mínimas, convirtiendo la batalla en una masacre. 

Por su parte, confiado en el derrumbe del Ejército Rojo en el sur de Rusia, Hitler, ignorará el estado real de sus tropas en el Cáucaso y de los planes enemigos de posicionarse fuertemente en las montañas, ordenaría la inmediata captura de los pozos petrolíferos por el reforzado Grupo de Ejércitos A, que se empeñaba por avanzar lo más rápido posible, hasta situarse a 100 km del mar Caspio, los cuáles nunca llegaran. El 9 de agosto cae el primer yacimiento petrolífero de Maikop, pero lo encuentran completamente destruidos. Las unidades germanas carecen de suministros y se encuentran agotadas; las compañías raramente tenían más de 60 hombres, y las Panzerdivisionen, 80 tanques. Sin más refuerzos y de combustible, muy lejos de su alcance de los principales yacimientos petrolíferos de Baku. Hitler exasperado, comienza a prestar su atención en el frente Stalingrado que todavía no había sido tomada por el 6.ºEjército, debido a la feroz resistencia soviética que se dio en la curva del río Don y en los suburbios de la ciudad. Finalmente, el 9 de septiembre Hitler destituirá a Von List jefe del Grupo de Ejércitos A, y asumirá personalmente el mando de sus tropas.

A inicios de agosto, Hitler enfurecido, debido a los lentos progresos del general Paulus en el Don, ordenó al 4.° Ejército Panzer de Hoth, que se dirigieran de nuevo a Stalingrado para apoyar al 6.° Ejército y aplastar definitivamente la resistencia soviética en el Don meandro, por el sur. El general Hoth obedeció preocupado, debido a las pocas reservas de combustible restantes tras el descenso hacia el cáucaso. El 8 de agosto, las 16ª y 24ª Panzerdivisionen del 6º Ejército de Von Paulus, que avanza con el objetivo de llegar a Stalingrado, terminan de cercar a las tropas del 62º Ejército soviético del general al oeste de Kalach, a 60 km de la capital del Volga. Quedan embolsadas 7 divisiones, 2 brigadas motorizadas, y 2 acorazadas con unos 1.000 carros de combate y 750 piezas de artillería. Al siguiente día, Stalin nombró a Andréi Yeriómenko comandante del Frente de Stalingrado, harto de las continuas derrotas del mariscal Timoshenko. Para el 10 de agosto, el 6º Ejército alemán del general Von Paulus derrota a las tropas del 62º Ejército soviético del general Kolpakchi, que oponían una fiera resistencia en la curva del rió Don, al oeste de Kelach. Los germanos hacen unos 35.000 prisioneros rusos e incautan 270 carros y unos 560 cañones. Queda abierto para las fuerzas del Eje el camino a Stalingrado, a unos 60 km al este; pero antes los germanos tendrán que acabar con los reductos soviéticos en la zona: tardarán unos once días. El 22 de agosto tras acabar con los últimos reductos de resistencia soviéticos en Kalach, el 4º Panzerkorps penetra en las líneas rusas de Vertiachi, al noreste de Stalingrado. El del general abre una brecha en el frente ruso con la que podrán alcanzar la orilla del Volga; por la brecha penetrará el 51º Cuerpo de Ejército de Seydlitz. De esa forma las primeras unidades alemanas cruzan la curva del río Don y establecen una cabeza de puente, 
El 23 de agosto, Stalingrado recibió su primer bombardeo proveniente de los Heinkel 111 y Junkers 88, unos 600 aviones del general Wolfram von Richthofen, comandante de la Legión Cóndor durante el bombardeo de Guernica. Se lanzaron 1000 toneladas de bombas y se perdieron tan sólo tres aeroplanos. Murieron no menos de 5000 personas ese día. En esa semana morirían 40 000 de los 600 000 habitantes de la ciudad, dañando o destruyendo unos 4.000 edificios. La Luswafe perdería, en total, 90 aeroplanos. Ese mismo día, la vanguardia del 6.º Ejército alemán alcanzó el Volga. Los soldados estaban emocionados por haber avanzado con tantos sacrificios desde el Don meandro (gracias en parte al resultado del Combate de Isbucensky y al apoyo de la Lutfwaffe), confiando en una caída rápida de Stalingrado. La germana al mando del general Hube, continuo cruzando la curva del rió Don sobre un pontón montado en Vertiachi, al noreste de Stalingrado. Por la tarde, la componía de transmisiones llega a la vista de la ciudad, a unos 40 km, mientras está siendo bombardeada por Stukas. Prosigue por los suburbios de Spartakovka, Hinok y Latashinika, entra en los arrabales de la ciudad y se atrinchera en la ribera del Volga.

Por el sur, el avance de Hoth era más lento, ya que Yeremenko había colocado la mayor parte de sus fuerzas contra el 4.º Ejército Panzer; además, Hitler le había quitado al general Hoth un Cuerpo Blindado para integrarlo al 6.°Ejército de Paulus. 

Comienza la Batalla de Stalingrado

El 24 de agosto unidades de la 16ª Panzerdivisionen al mando de Hube, avanzan por los arrabales industriales de Spartakovka, al noroeste de la capital, entablando una dura lucha con tropas del 62º Ejército soviético que emplea algunos T 34 recién fabricados y son ayudados por ciudadanos armados, que luchan tras barricadas. Los germanos atacan el ferrocarril, con su artillería dominan el Volga y la Luftwaffe bombardea la zona soviética de la capital. La 35ª División soviética aísla a los germanos, que forman en erizo aguardando la llegada de más unidades alemanas. algunas divisiones no podrán llegar, debido a una inesperada contraofensiva soviética de grandes proporciones, que durarían varias semanas en ser derrotadas, el contraataque se llevo a cabo en el sector de al norte de la ciudad, con ejércitos recién formados: el 4.º de Tanques, el 24.º y el 66.º ejércitos y el 1.º de Guardias soviéticos. Estos nuevos ejércitos lanzarían contraataques costosos sobre las fuerzas alemanas, por lo que se tuvo que desviar divisiones completas del 6.º Ejército próximas a Stalingrado, hacia el norte para contener la arremetida soviética. Otros dos ejércitos soviéticos frescos, el 57.º y el 51.º, hicieron lo propio desde el sur, donde se encontraban las fuerzas de Hoth, relegando nuevamente el avance de Paulus y sus fuerzas a una toma rápida de Stalingrado.
El mariscal Zhúkov, quien recientemente había sido nombrado Vicecomandante en Jefe, segundo después de Stalin, llegó a Stalingrado el 29 de agosto.Parte de la infantería alemana llegaría a los suburbios el 1 de septiembre con escaso apoyo maquinizado, debido a los acontecimientos recientes al norte de la ciudad. En aquel momento convergían sobre Stalingrado, por el sur, las 29.ª y 14.ª Divisiones motorizadas; por el oeste se acercaban la 24.ª, 94.ª, 71.ª, 76.ª y 295.ª Divisiones de infantería blindada; por el norte y hacia el centro de la ciudad, la 100.ª División de cazadores, la 389.ª y 60.ª División de infantería motorizada. Mientras que en la ciudad era defendida en ese momento solo por unos 56 000 soldados. El comando soviético podría proporcionar a sus tropas en Stalingrado solo transbordadores arriesgados a través del Volga. En medio de las ruinas de la ciudad ya destruida, el 62 Ejército soviético construyó posiciones defensivas con puntos de disparo ubicados en edificios y fábricas. Al día siguiente tropas del 6º Ejército alemán y el 4.ª Panzerarmee llegan a las colinas que dominan Stalingrado, cortando las comunicaciones terrestres de la urbe; su guarnición solo puede aprovisionarse por el Volga. Francotiradores y grupos de asalto detuvieron al enemigo lo mejor que pudieron. Los alemanes, que se trasladaron a Stalingrado, sufrieron grandes pérdidas. Los refuerzos soviéticos cruzaron el Volga desde la costa este bajo bombardeos constantes y fuego de artillería. Con el transcurso de la batalla, todo el 6.º Ejército y parte del 4.º Ejército Panzer se encontrarían combatiendo en la ciudad. Estas tropas desconocían (en parte por motivos de seguridad) que el Ejército Rojo preparaba una ofensiva a gran escala contra el 6.º Ejército alemán en los próximos meses.

Stalin, que instaba a Zhúkov a salirles al encuentro e interceptar dichas fuerzas enemigas, replicaba:

Las ofensivas de a finales de agosto y septiembre, lograrían aliviar en parte la situación respecto del norte de la ciudad. La orden de Zhúkov era terminante: «¡No entreguen Stalingrado!».

Las fuerzas alemanas atenazaron Stalingrado. Hitler, que no había deseado la Guerra de guerrillas en Moscú y Leningrado, ahora bramaba por la conquista de la ciudad bajo esa premisa: eso implicaba la guerra calle por calle, casa por casa, un tipo de combate para el cual ni la Wehrmacht ni las Waffen-SS estaban preparadas.

El fracaso en la toma del Cáucaso llevó a Hitler a repensar drásticamente sus objetivos. Sin el ansiado petróleo, se convenció que al conquistar la ciudad, además de tapar su derrota estratégica con una victoria simbólica, tendría de nuevo posibilidades de virar hacia el sur.

En el frente, el 6.º Ejército alemán de Von Paulus inició un ataque cuyo objetivo es completar la conquista de Stalingrado. Para ello, la 71ª, 76ª y 295ª divisiones de infantería avanzaría desde la estación de Gumrak hacia el hospital principal, para luego tomar Mamáyev Kurgan; por otro lado la 94ª división de infantería y otra división motorizada atacan la zona del suburbio apoyadas por las 14ª y 24ª Panzerdivisionen, el comandante del 62º Ejército soviético, Lopatin, da por perdida la ciudad, y pide autorización para huir por el río. Stalin se lo niega. El 12 de septiembre, Zhúkov destituyó deshonrosamente al comandante a cargo de las defensas de Stalingrado, Anton Lopatin por demostrar cobardía ante el enemigo al no poder contenerlo con el 62.º Ejército, y fue reemplazado por el granítico e inflexible general Vasili Chuikov, un soldado eficiente y decidido, hasta entonces a cargo del , desplegado al sur de la ciudad y que había estado resistiendo los embates del 4.° Ejercito Panzer de Hoth y el Panzergruppe de Kleist.

Cuando Chuikov llegó al escenario de la batalla, Yeriómenko y Jrushchov le preguntaron: «"—¿Cuál es el objetivo de su misión, camarada? —Defender la ciudad o morir en el intento"», contestó firmemente Chuikov. Yeriómenko observó a Jrushchov, y tuvo la certeza de que Chuikov había entendido perfectamente lo que se esperaba de él.

El nuevo comandante se encontró con menos de 20 000 hombres y 60 tanques, así como unas deficientes defensas. Chuikov reforzó las defensas antiaéreas (servidas por mujeres militares) de la ciudad y, asimismo, fortificó aquellos lugares donde fuese posible contener al enemigo, en especial la colina de Mamáev Kurgán y el barranco del río Tsaritsa. Además retiró la mayor parte de su artillería a la ribera oriental del Volga y fomentó el despliegue de francotiradores, entre ellos el famoso Vasili Záitsev.

El mismo día que Chuikov tomó el mando del 62º Ejército, Paulus se encontraba en Vinnitsa, en el Wehrwolf con Hitler, que quería saber cuándo caería la ciudad. Paulus se encontraba preocupado por los flancos de su 6.º Ejército, que estaban desprovistos de unidades mecanizadas de consistencia y eran resguardados por ejércitos sin armamento pesado de varias nacionalidades: rumanos, italianos, húngaros. Estas fuerzas de inferior calidad resultarían sobrepasados, incapaces de asegurar los flancos de las fuerzas alemanas en Stalingrado, unos 20 000 soldados en aquel momento. No obstante, Hitler minimizó esta debilidad, convencido de que el frente soviético estaba al borde del colapso, una falsa confianza que fue contagiada a Paulus.

El 14 de septiembre, se inició el otro intento alemán de tomar la ciudad —que se pensaba sería el único intento— y la 71ª y 76ª División alemana llegaron al control de Stalingrado, acercándose peligrosamente al embarcadero principal, la terminal de llegada de refuerzos soviéticos, y abriendo una brecha en el sector central de las posiciones rusas, llegando algunas avanzadillas a 200 metros del bunker de Chuikov, que desplaza la totalidad de sus tanques para detener el ataque, y emplea la táctica de dejar pasar a los carros enemigos hasta sus posiciones de cañones anti-tanque. Las tropas del Eje pierden hoy 8.000 hombres; los soviéticos pierden 2.000 soldados y evacuan por el Volga a 3.500 heridos. Los alemanes hacen 5.000 prisioneros.

En estos combates cae abatido el teniente Rubén Ruiz Ibárruri, el único hijo de la Pasionaria, en la estación central de la ciudad.especialmente en la conquista de la colina de Mamaev Kurgan y en las fábricas en el centro de la ciudad, duró más de dos meses y se convirtió en una enconada lucha en que las banderas de ambos bandos ondearon alternadamente, ya que si los alemanes controlaban esta colina, su artillería dominaría el Volga. Las batallas por la fábrica de Krasny Oktyabr, la fábrica de tractores y la fábrica de artillería de Barricadas se dieron a conocer en todo el mundo. Mientras que los soldados soviéticos continuaron defendiendo sus posiciones disparando contra los alemanes, los trabajadores de las fábricas y las fábricas estaban reparando tanques y armas soviéticas dañadas en las inmediaciones del campo de batalla, y algunas veces en el campo de batalla. Los detalles específicos de la lucha en las empresas era el uso limitado de armas de fuego debido al riesgo de rebote: las peleas se peleaban con objetos que perforaban, cortaban y aplastaban, así como la lucha cuerpo a cuerpo. Los alemanes desplegaron todo un sistema de altavoces incitando a la deserción de los soviéticos. Muchos se pasaron y se convirtieron en hiwis, muchos otros fueron fusilados por acción u omisión frente a la deserción.

Para las fuerzas soviéticas de Stalingrado fue, probablemente, el momento más crítico de la batalla. Los alemanes asaltaron al 62º Ejército en estado crítico, siendo salvado del desastre gracias a la intervención de la 13ª División de Fusileros de la Guardia del general Rodimtsev (si bien esto fue reconocido después) y a la reactivación de la 8ª Fuerza Aérea Soviética, donde servía un hijo de Stalin. Las operaciones soviéticas en tierra fueron constantemente obstaculizadas por la "Luftwaffe". 

El 19 de septiembre, la 1ª Guardia Soviética y el 24º Ejército lanzaron otra ofensiva contra el VIII Cuerpo de Ejército del general Walter Heitz en Kotluban. "VIII Fliegerkorps" envió ola tras ola de bombarderos de buceo Stuka para evitar un gran avance. La ofensiva fue repelida. Los Stukas afirmaron que 41 de los 106 tanques soviéticos noqueados esa mañana, mientras escoltaban los Bf 109 destruyeron 77 aviones soviéticos. En medio de los escombros de la ciudad destruida, los ejércitos 62 y 64 soviéticos, que incluían la , anclaron sus líneas de defensa con puntos fuertes en casas y fábricas.

El 20 de septiembre las tropas alemanas dominan las orillas del Tsaritsa y tienen artillería a pocos metros del muelle principal. El general Chuikov se vio obligado a trasladar su amenazado cuartel general del bunker de Tsaritsin a Mamaeiev Kurgan. La zona central de la ciudad está estancada, ambos ejércitos están agotados. Los soviéticos aún podían traer refuerzos empleando los transbordadores del extremo septentrional de la ciudad y los subterráneos, donde tienen sus cuarteles, hospitales y refugios, inalcanzables para la artillería germana. La ciudad ya es un montón de escombros. 

Pelear dentro de la ciudad en ruinas era feroz y desesperado. El teniente general Alexander Rodimtsev estaba a cargo de la 13ª División de Fusileros de la Guardia, y recibió a uno de los dos Héroes de la Unión Soviética premiados durante la batalla por sus acciones. La Orden No. 227 de Stalin del 27 de julio de 1942 decretó que todos los comandantes que ordenaron retiradas no autorizadas estarían sujetos a un tribunal militar. Los desertores y presuntos simuladores fueron capturados o ejecutados después de la lucha. Durante la batalla, el 62º ejército tuvo la mayor cantidad de arrestos y ejecuciones: 203 en total, de los cuales 49 fueron ejecutados, mientras que 139 fueron enviados a compañías penales y batallones. Los alemanes que avanzaban hacia Stalingrado sufrieron grandes bajas.

La doctrina militar alemana se basaba en la interacción de las ramas militares en general y en la interacción particularmente estrecha de infantería, zapadores, artillería y bombarderos de buceo . En respuesta, los combatientes soviéticos trataron de ubicarse a docenas de metros de las posiciones enemigas, en cuyo caso la artillería y los aviones alemanes no podían operar sin el riesgo de ir por su cuenta. A menudo, los oponentes estaban divididos por una pared, piso o escalera. En este caso, la infantería alemana tuvo que luchar en igualdad de condiciones con la infantería soviética: rifles, granadas, bayonetas y cuchillos. La lucha fue por cada calle, cada fábrica, cada casa, sótano o escalera. Incluso edificios individuales se pusieron en las tarjetas y obtuvieron los nombres: Casa de Pavlov, Molino, Grandes almacenes, prisión, Casa Zabolotny, Casa de productos lácteos, Casa de especialistas, Casa en forma de L y otros. El Ejército Rojo constantemente realizaba contraataques, tratando de recuperar las posiciones previamente perdidas. Varias veces pasaron de mano en mano Mamaev Kurgan, estación de ferrocarril Stalingrado-I. Los grupos de asalto en ambos lados trataron de usar cualquier pasaje al enemigo: alcantarillas, sótanos, socavamientos.

Para mediados de septiembre, ocho de las veinte divisiones del 6.º Ejército alemán se encontraban luchando dentro de la ciudad; no obstante, los soviéticos no dejaban de alimentar el frente con refuerzos de Siberia y Mongolia. El general Paulus, enfermo de disentería, estaba sobre tal presión para que informara de la fecha en que caería Stalingrado que acabó por desarrollar un 'tic' en el ojo izquierdo, que luego se extendió por el lado izquierdo de su cara.

En este momento, las estadísticas de bajas alemanas se dispararon dada la inexperiencia en combate urbano del soldado alemán. Aunque Paulus sabía que las bajas soviéticas era por lo menos el doble que las alemanas, sus recursos humanos se disipaban rápidamente ya que nada más contaba con una división en la reserva. Eran habituales los destacamentos de comandos alemanes enviados al combate callejero que perdían entre el 50 y el 70 % de efectivos.

En este campo de batalla, los alemanes estaban bajo constante tensión ya que el soldado soviético se había convertido en un maestro del camuflaje y las emboscadas eran comunes. La noche no ofrecía descanso, ya que los defensores de la ciudad preferían atacar de noche, neutralizando el peligro de los bombarderos alemanes. Sin embargo, no era una limitación para los bombarderos soviéticos, que pasaban sobre la ciudad arrojando pequeñas bombas de 400 kilogramos. Finalmente, el 6.º Ejército solicitó a la Luftwaffe que mantuviera la presión sobre la aviación soviética durante la noche, porque «las tropas no tienen descanso». Si los bombardeos nocturnos, las minas antipersonales y las emboscadas de la infantería enemiga no eran suficientes para mantener alerta a los alemanes en Stalingrado, los francotiradores sí lograron captar la atención de los oficiales germanos. Los francotiradores soviéticos, utilizando las ruinas como refugios, también infligieron grandes daños a los alemanes. El francotirador Vasily Grigorievich Zaitsev durante la batalla destruyó 225 soldados y oficiales del enemigo (incluidos 11 francotiradores). El número de oficiales muertos por francotiradores, especialmente los observadores, también se disparó y muy pronto se tuvo que recurrir a realizar promociones prematuras, con el fin de reemplazar a los caídos.

La neurosis que un soldado podría desarrollar por estar sometido constantemente al grado de tensión de la llamada "Rattenkrieg" ('Guerra de ratas') no era excusa para abandonar el campo de batalla, ya que tanto alemanes como soviéticos no reconocían esta condición y la calificaban de cobardía, que usualmente era solucionada con la ejecución sumaria inmediata.

La artillería pesada se volvió inútil en este ambiente de lucha urbana, ya que debido a la falta de precisión de la misma, no se podía atacar una casa ocupada por el enemigo, porque las casas vecinas estaban ocupadas por tropas amigas. Una gran cantidad de baterías de artillería apoyaron a ambos lados de la lucha (artillería soviética de gran calibre operada desde la costa oriental del Volga), morteros de hasta 600 mm. Hubo el famoso caso de la llamada Casa de Pávlov en que el dominio de los pisos se alternaban cruentamente entre los bandos. 

Vasili Chuikov ordenó que la artillería fuera trasladada a la orilla oriental del Volga y que atacase detrás de las líneas alemanas con el objetivo de destruir las líneas de comunicación y las formaciones de infantería en la retaguardia. Para saber hacia dónde disparar, un oficial de observación debía asomarse por la azotea de un edificio en la ciudad, lo que en muchos casos significaba la muerte a manos de un francotirador alemán. Solamente los Katiusha fueron dejados en Stalingrado, ocultos en el banco de arena del Volga.

A diferencia de los puestos de mando alemanes, los puestos de mando soviéticos se encontraban en la ciudad y, por lo tanto, expuestos a ser atacados. En una ocasión, un tanque alemán se situó en la entrada del búnker del comandante de artillería del 62º Ejército y éste, junto con su personal, tuvo que cavar para salvarse.Pese a que la iniciativa, la razón de bajas enemigas "per cápita" y los mejores medios técnicos correspondían a las tropas alemanas, el ejército invasor tuvo grandes dificultades en conquistar una ciudad que, al haber sido salvajemente bombardeada, disponía de condiciones ideales para una defensa calle por calle. Los ataques combinados de infantería y blindados resultaban inútiles en el caos de la lucha urbana.

Para desgastar al oponente, las medidas impuestas por Chuikov fueron extremas: por ejemplo, se envió a miles de soldados sin experiencia para apoderarse de las trincheras alemanas, asumiendo muchas bajas. Pronto la ciudad se cubrió de una atmósfera repulsiva y pútrida. La razón era obvia: los cadáveres de ambos bandos se descomponían bajo los escombros. A su vez. en el bando alemán, y bajo tal ambiente, se prosiguió la política antisemita nazi. La "Feldgendarmerie" (Policía Militar alemana) había estado capturando judíos y haciendo cautivos a civiles que fueran aptos para el trabajo y se ejecutó a unos 3000 civiles judíos de todas las edades por parte de los "Sonderkommandos" de los "Einsatzgruppen". Otros 60 000 fueron enviados a Alemania para trabajos forzados. Los "Sonderkommandos" se retiraron de Stalingrado el 15 de septiembre, habiendo ejecutado cerca de 4000 civiles.

Sabiendo que el invierno se aproximaba, Paulus decidió acelerar la toma de la ciudad y preparó una ofensiva que se ejecutó el 27 de septiembre. La principal fuerza alemana atacó al norte del Mamaev Kurgan, cerca de los asentamientos obreros de las fábricas Octubre Rojo y Barrikady. Los alemanes observaron atónitos cómo los civiles que huían de los asentamientos para buscar refugio en las líneas alemanas eran derribados por sus propios soldados. Desde ahí, una división escogida de soldados alemanes capturó la «Casa de los Especialistas», donde se hicieron fuertes y comenzaran a disparar contra las lanchas que iban y venían por el Volga trayendo soldados. Los cañones de 88 mm, los Stukas y la artillería alemana competían en hundir las barcazas que traían soldados del otro lado del Volga.
Entre el primer y segundo día de combate los alemanes tuvieron cerca de 2500 bajas, los soviéticos cerca de 6000. Para los soviéticos las pérdidas sobrepasaban las ya de por sí altas bajas diarias: casi 3000 soldados morían por día (a razón de un centenar cada hora). Aunque las tropas alemanas lograron penetrar en la ciudad o lo que quedaba de ella, nunca se hicieron completamente con la totalidad (el muelle y la colina), puesto que los primeros no pudieron ser alcanzados, y mientras permanecieran en manos soviéticas, los refuerzos y suministros necesarios para proseguir la batalla podrían afluir con regularidad. Batallones y brigadas de comandos alemanes que intentaron llegar a los muelles fueron reducidas al 50 % de sus efectivos.

En Berlin, el 30 de septiembre, en un discurso en el Palacio de deportes, con motivo del inicio de la 4ª campaña del Socorro de Invierno, Hitler dirá que "Stalingrado ha sido conquistada" y alega que "...nadie conseguirá expulsarnos jamás de esta posición". 

Tanto para Stalin como para Hitler, la batalla de Stalingrado se convirtió en una cuestión de prestigio. además de la importancia estratégica de la ciudad. El comando soviético trasladó las de Moscú al Volga, y también transfirió fuerzas aéreas de casi todo el país a la región de Stalingrado.

Pero aquí, desde la reserva de la Sede, el Frente Don recibe 7 divisiones de fusil totalmente equipadas (277, 62, 252, 212, 262, 331, 293 sd ). El el general soviético Konstantin Rokossovski, hasta hace poco comandante del Frente de Briansk, es nombrado comandante del Grupo de Ejércitos o Frente del Don, decide usar nuevas fuerzas para una nueva ofensiva. 

El 4 de octubre, las tropas del 6º Ejército realizan un cuarto ataque contra las posiciones soviéticas en Stalingrado, dándose durísimos combates. Ese día Rokossovsky instruyó para desarrollar un plan de operación ofensiva, y el 6 de octubre el plan estaba listo. La operación estaba programada para el 9 de octubre. Pero en esos momento varios eventos esteban teniendo lugar.

El 5 de octubre de 1942, Stalin criticó duramente el liderazgo del Frente de Stalingrado en una conversación telefónica con A. I. Eremenko y exigió que se tomaran medidas inmediatas para estabilizar el frente y luego derrotar al enemigo. El 6 de octubre, Eremenko hace un informe a Stalin, en el que propone llevar a cabo una operación para rodear y destruir unidades alemanas cerca de Stalingrado. Allí se propone por primera vez rodear al 6º Ejército con ataques laterales contra las unidades rumanas, y después de atravesar los frentes, unirse en la región de Kalach-on-Don. La sede consideró el plan de A.I. Eremenko, pero luego lo consideró imposible (la profundidad de la operación fue demasiado grande, etc.). Sin embargo, la idea misma de una contraofensiva fue discutida por Stalin, Zhukov y Vasilevsky el 12 de septiembre.

En efecto, el 4 de julio los generales Zukov y Vassilievksi, del Stavka o estado mayor del Ejército Rojo, habían acordado con los comandantes de los 3 Frentes soviéticos en la zona de Stalingrado las operaciones para cercar al 6.º Ejército alemán de von Paulus dentro de la ciudad. Pero los comandantes que resistían en el frente Stalingrado no les fueron detallados verídicamente.

El 9 de octubre, un decreto del Presidium del Soviet Supremo devuelve a los oficiales del Ejército Rojo el cuidado de la disciplina militar, suprimiendo su Cuerpo de Comisarios. Se les ordena disparar sobre grupos de combatientes soviéticos en retirada. Se hace efectiva la Orden N"227. Y en Occidente se acuerdan los detalles para el envío de armamento, materia prima y municiones a Rusia. 

En la mañana del 14 de octubre, el sexto ejército alemán lanzó un nuevo ataque decisivo contra las cabezas de puente soviéticas cerca del Volga. Fue apoyada por más de mil aviones de la 4ta Flota Aérea de la Luftwaffe. La concentración de tropas alemanas no tenía precedentes: en el frente, a unos 4 km de distancia, tres divisiones de infantería y dos tanques atacaron la planta de tractores y la planta de Barricadas. Las unidades soviéticas se defendieron obstinadamente, apoyadas por fuego de artillería desde la costa oriental del Volga y desde los barcos de la flotilla militar del Volga, deteniendo de esa forma el avance alemán. Sin embargo, la artillería en la orilla izquierda del Volga comenzó a experimentar una escasez de municiones en relación con la preparación de la contraofensiva soviética.Para el 15 de octubre, las tropas alemanes logran llegar a la margen del río por el centro de la ciudad, partiendo al 62.° Ejército por la mitad. Los generales soviéticos solicitaron refuerzos desesperados ante el inminente peligro de que sean empujados a la otra orilla de río Volga. Los refuerzos llegarían al día siguiente por parte de la del coronel , que había cruzado el río por el lado norte de la ciudad, cerca a la fabrica de Barricadas (Barricady), y de inmediato se pusieron en acción, con grandes perdidas, los alemanes fueron rechazados una vez más. 

Hitler ordenó a Von Paulus que sus tropas del 6.º Ejército deben "mantener a toda costa las líneas ya alcanzadas, punto de partida de una ofensiva prevista para 1943" Según el Führer, los soldados germanos se hallan "mejor preparados y dispuestos" para afrontar este invierno que lo estuvieron en el pasado, y considera que el Ejército Rojo está "debilitado tras los últimos combates." En definitiva, se debe resistir en Stalingrado hasta el ultimo hombre.

El 31 de octubre, las tropas del Ejército Rojo de Chuikov, hostigadas por el 6º Ejército alemán de Von Paulus y parte de la 4ª Panzerarmee, tan solo dominaran las ruinas de 2 fábricas al norte de la ciudad y una franja de 2 km de ribera portuaria del Volga, por la que recibe refuerzos, suministros y pertrechos. Durante ese mes, las tropas del Eje han perdido 400 tanques y unos 40.000 soldados, gastando miles de toneladas de munición. 

El 31 de octubre, era obvio que los alemanes no habían conquistado la totalidad de la ciudad, pero si habían ocupado el 80 % de ella. las tropas del Ejército Rojo de Chuikov, hostigadas por el 6º Ejército alemán de Von Paulus y parte de la 4ª Panzerarmee, tan solo dominaban las ruinas de 2 fábricas al norte de la ciudad y una franja de 2 km de ribera portuaria del Volga, por la que recibe refuerzos, suministros y pertrechos. Durante ese mes, las tropas del Eje han perdido 400 tanques y unos 40.000 soldados, gastando miles de toneladas de munición. Pero los alemanes continuaban asediando esas fábricas; la de tractores Octubre Rojo y la fabrica de cañones Barricady, incluso tenían posición una parte de una de esas fabricas. Las bajas rusas se incrementarían a razón de 4000 soldados diarios. Los heridos soviéticos se arrastraban a la orilla del Volga con la efímera esperanza de poder ser auxiliados, y miles murieron congelados. El hecho de cruzar el río no constituía ninguna garantía de recibir atención médica, ya que debido a la falta de recursos, muchos soldados eran dejados a su suerte. Lo que los soviéticos no podían notar era que los alemanes estaban al borde de su capacidad ofensiva; de hecho, no tenían las suficientes fuerzas para conquistar la ciudad, pues su línea de abastecimientos era insuficiente. 

En ese octubre, Hitler y sus comandantes cayeron en la cuenta de que no podrían tomar la ciudad en otoño. El invierno se aproximaba, por tanto se hicieron todos los arreglos para pasar allí, en recuerdo del terrible invierno anterior. Y a finales de octubre se dejaron sentir las enfermedades en ambos bandos: fiebre paratifoidea, tifus, disentería, y los alemanes ya tenían conocimiento por medio de prisioneros de que los soviéticos preparaban una gigantesca contraofensiva. Ellos mismos habían notado los movimientos en sus flancos. Para protegerse, Paulus había levantado una barrera en su flanco izquierdo para prevenir los ataques procedentes por el norte, sirviéndose de las unidades rumanas. Pero el Alto Mando alemán en Berlin, seguiría ignorando esos informes. 

En efecto, el alto mando soviético, alertado por la Orquesta Roja, la red de espías soviéticos en el estado mayor alemán, se enteró de la debilidad de los flancos del ejército enemigo, formado por soldados inexpertos rumanos, y equipados con cañones franceses sin repuestos y con solo dos obuses cada uno, y preparó una gran ofensiva dirigida contra esos flancos norte y sur. Se estaban acumulando cerca de 1 700 000 hombres, es decir, cerca de 200 divisiones, la mayoría siberianas, además de carros de combate y cañones procedentes de Moscú y los Urales. El plan consistía en una maniobra de pinza para cercar, copar y embolsar al 6.º Ejército entero, irrumpiendo en la retaguardia alemana por los flancos norte y sur, atacando allí donde las fuerzas del Eje fueran más débiles. Si bien en un primer momento Stalin se negaba a desviar recursos del propio combate urbano, vio en estos planes la mejor oportunidad de cambiar el frente sur y de revertir toda la situación de Stalingrado, por lo cual apoyó la idea del cerco; aunque esto significara reducir el cupo de municiones del 62º ejército, que defendía por sí solo la ciudad. La idea de rodear a un ejército alemán en estas condiciones era en todo osada, pero no había otra posibilidad viable luego de los constantes errores en las ofensivas soviéticas de comienzo del 42.

El 9 de noviembre, cayeron las primeras nevadas, había llegado el invierno y la ciudad quedó sumida en un manto blanco con temperaturas que rondaban los -18 °C. Los combates callejeros cesaron casi por completo durante la noche. De noche, los grupos enfrentados hacían señales de tregua temporales con banderas que asomaban en los orificios de las ruinas, permitiéndose retirar algunos caídos con vida en tierra de nadie; realizando, además. un intercambio no oficial de abastos entre pequeños grupos de ambos bandos, realizado muy a escondidas en treguas concertadas espontáneamente. De ser descubiertos, la pena era la ejecución inmediata por confraternizar con el enemigo. De día, la lucha se reanudaba sin cuartel.

Cruzar el Volga se volvió extremadamente difícil debido a los témpanos de hielo en el río, las tropas del 62 Ejército experimentaron una aguda escasez de municiones y alimentos. Al final del día, el 11 de noviembre, las tropas alemanas completaron la captura de la fabrica de cañones "Barrikady, en le avance," logran llegar a la margen del río por el norte de la ciudad rodeando la división de Lyudnikov, y cortando su enlace con el 62.° Ejército. La , o como se la conoció durante el furor de la batalla la ", se aferro en un tramo de territorio de 500 m ancho × 200 m largo a las orillas del Volga, con tal amenaza, la artillería divisional soviética tuvo que ser evacuada hacia la orilla este. Pero la 138a todavía se sostenía con una fuerza cada vez más reducida de los feroces asaltos alemanes, hasta el fin de la contienda. 

Como resultado, después de tres meses de sangrientos combates y lentos avances, los alemanes solo logran capturar el 90% de la ciudad en ruinas y dividiendo las fuerzas soviéticas restantes en tres bolsillos estrechos. Los témpanos de hielo en el Volga impedían que los botes y remolcadores abastecieran a los defensores soviéticos. Sin embargo, la lucha continuó, especialmente en las laderas de Mamayev Kurgan, fabrica de tractores y la ""isla de lyudnikov"". Del 21 de agosto al 20 de noviembre, el 6.º Ejército alemán perdió 60.548 hombres, incluidos 12.782 muertos, 45.545 heridos y 2.221 desaparecidos.El 19 de noviembre de 1942, los 3500 cañones soviéticos comenzaron a disparar sin descanso sobre las líneas enemigas más débiles entre Serafimovih y Klestkaya. Estas formaciones consistían en tropas rumanas con escaso material antitanque. Después de una hora de fuego de artillería, los batallones de fusileros avanzaron sobre las filas rumanas.

Los rumanos del II y IV Cuerpos lograron contener brevemente las primeras oleadas de infantería, pero fueron arrasados por carros de combate T-34 hacia el mediodía. Cuando los fortines fueron demolidos, los rumanos huyeron en desbandada por la planicie blanca, siendo perseguidos por las oleadas soviéticas. Si bien hubo algunos intentos de responder al ataque, los comandantes del 6.º Ejército infravaloraron el ataque hasta que fue demasiado tarde. Los combates en la misma ciudad de Stalingrado no se detuvieron durante varios días una vez comenzado el ataque soviético. Los Stukas acudieron a dar apoyo a las unidades del eje, pero el avance soviético era por entonces imparable.

Si bien el ataque del sur fue, por muchos factores, más débil, funcionó, y las columnas de la trampa avanzaron sin grandes reveses, salvo contraataques aislados que apenas produjeron momentáneas detenciones. El objetivo donde convergían las tenazas de la ofensiva era el pequeño pueblo de Kalach y su puente, donde los alemanes no poseían una fuerza para afrontar la amenaza y donde quedaban expuestos sus talleres y depósitos de suministros. El desastre era total, el VIº Ejército de Paulus quedó encerrado en Stalingrado con unos 250 000 hombres y sin suministros mayores.

El OKW alemán ordenó retirar el grueso del 6.º Ejército desde Stalingrado por el sudoeste hacia el Don, y así evitar el encierro. Tal proyecto aún podía ejecutarse ya que había brechas importantes que aún no estaban cerradas, pero Hitler se negó a aceptar semejante solución y exigió a Paulus y sus hombres mantenerse en la ciudad conquistada mediante una contraorden directa, retirando las vanguardias enviadas en dirección sudoeste para tratar de superar el cerco.

Hitler consideraba que la situación no estaba del todo perdida y confiaba en poder repetir la situación producida en febrero de ese mismo año en la Bolsa de Demyansk, donde una gran masa de soldados alemanes pudieron resistir un prolongado cerco soviético mediante un puente aéreo. Tal idea llegó a oídos del jefe máximo de la Luftwaffe, Hermann Goering, quien sin consultar a sus asesores técnicos prometió a Hitler que sus aviones podrían realizar un vasto abastecimiento desde el aire. La promesa de Goering exasperó al general de aviación Von Richtofen, pues el tiempo nublado con tormentas de nieve impediría volar a los aviones de forma sostenida e incluso haría imposible siquiera que despegasen. En estas condiciones Paulus radió un mensaje directo a Hitler:

Las tenazas soviéticas se cerraron en menos de 4 días de lucha. El 24 de noviembre Stalingrado estaba bajo asedio soviético. La División 94º al mando del general Walther von Seydlitz-Kurzbach, al ver que Paulus carecía de iniciativa ordenó a su tropa evacuar su sector y forzar el bloqueo, esperando con ello que las demás divisiones le siguieran en su retirada no autorizada. Apenas dejó su posición, le cayó encima el 62º Ejército Soviético y muchos de sus batallones fueron aniquilados sin contemplaciones, no hubo prisioneros.

Goering, de manera irresponsable, ante los informes advirtiéndole lo imposible de la misión —que recibió e ignoró—, prometió abastecer al "Kessel" con 500 toneladas diarias de pertrechos, pero los aviones apenas lograron llevar 130 toneladas en tres días de operaciones a horizonte raso y en medio de tempestades de nieve. Esto causaba que los vuelos nunca fueran realmente permanentes (como debía corresponder a un eficaz puente aéreo) sino que por causa del mal clima durante varios días los aviones no podían despegar de sus bases, o simplemente despegaban pero no podían aterrizar en Stalingrado. Para aumentar los males, los soviéticos atacaron de manera audaz la principal base aérea de suministros, el aeródromo de Pitomnik, llegando a colapsar las bases de reaprovisionamiento y acentuando la escasez de aviones de carga para las operaciones del puente aéreo. Sumado a las inclemencias climatológicas perjudiciales para los alemanes, los soviéticos lanzaban bengalas desde posiciones recién tomadas para hacer creer a los aviones de abastecimiento que en ese emplazamiento todavía quedaban soldados alemanes que solicitaban suministros. Hitler, obsesionado, dijo a Von Richtofen: «Si Paulus sale de Stalingrado, jamás volveremos a tomar la plaza».

A principios de diciembre, surgieron las primeras bajas por inanición. A pesar de todo, los alemanes trataron de conservar la disciplina y la organización funcionó regularmente.

Stalingrado se convirtió en un caldero ("Der Kessel") donde, sin agua ni alimentos suficientes, atacados por las epidemias y en medio del pútrido olor a descomposición, los alemanes se aprestaron a sufrir un largo asedio en medio de las mayores penurias. De este modo, unos 250 000 soldados quedaron atrapados en una bolsa con la orden, por parte de Hitler, de no retroceder ni rendirse. Pese a que Göring, mariscal del aire y jefe supremo de la Luftwaffe, prometió abastecer a las tropas desde el aire, la llegada de recursos a las tropas alemanas fue casi imposible y apenas se realizaron algunos vuelos.

Los alemanes pudieron utilizar el aeródromo de Pitomnik pero éste se hallaba sujeto a continuos ataques soviéticos, los Junkers Ju 52 llegaron con abastecimientos e inmediatamente partían de vuelta evacuando heridos, aun así los pocos aviones no daban abasto y los afortunados que podían subir escapaban del infierno, los heridos colgaban de las puertas y algunos desesperados se aventuraban a volar asiéndose en las alas, donde ninguno logró sobrevivir. Tras la caída de Pitomnik el 16 de enero sólo quedaba el improvisado aeródromo de Gumrak, más pequeño y en peores condiciones que el de Pitomnik, pero Gumrak también cayó en manos soviéticas el 23 de enero. A partir de ese día las hambrientas tropas alemanas sólo pudieron recibir provisiones mediante cajas lanzadas en paracaídas por la Luftwaffe, lo cual no aseguraba que la carga llegase a destino: soldados soviéticos a veces se quedaban con las provisiones, éstas caían al río Volga, o simplemente las tropas germanas estaban muy agotadas y hambrientas para buscar dichos suministros entre las ruinas de la ciudad.

Además, unos 10 000 civiles soviéticos también quedaron atrapados en la bolsa, de los cuales nunca se volvió a tener noticia.

En diciembre, los soldados alemanes cercados tuvieron una leve esperanza: Erich von Manstein venía en su auxilio. Manstein, que acababa de asumir el mando del Grupo de Ejércitos Don, planeó la Operación Tormenta de Invierno, que incluía dos amplias operaciones con un punto de partida diferente. Una vendría de Chirsk y la otra de , a 160 km de Stalingrado. Incluso para los generales más incrédulos del régimen nazi, el hecho de que Hitler abandonara al 6.º Ejército era algo impensable, por lo cual sentían esperanzas de un posible rescate. De esta manera la Wehrmacht se aseguraba de hacer todo lo posible por rescatar a este ejército de élite cercado lejos de Alemania.

La ofensiva empezó el 12 de diciembre, para el 13 de diciembre, habiendo cruzado Aksai, la 6.ª División Panzer llegó al pueblo de Las unidades sovieticas de la 235ª brigada de tanques separada con el 234º destacamento adjunto se adelantaron para recibirla. Regimiento de tanques, 20ª brigada de artillería antitanques, , bajo el mando del Teniente Coronel y el 2º Ejército de Guardias Frente de Stalingrado. Las batallas por Verkhne-Kumsky continuaron con éxito variable del 14 al 19 de diciembre. Solo el 19 de diciembre, el fortalecimiento del grupo alemán por parte de la 17 División Panzer y la amenaza de cerco obligaron a las tropas soviéticas a retirarse, hacia una nueva linea defensiva en el río . El retraso de cinco días de los alemanes en Verkhne-Kumsky fue un éxito indiscutible para las tropas soviéticas, ya que ganó tiempo para traer el 2º Ejército de la Guardia.

El 20 de diciembre, las tropas alemanas llegaron al río . La distancia al 6.º Ejército de Paulus, rodeado en Stalingrado, era de 35 a 50 km, pero las grandes pérdidas (hasta un 60% de infantería motorizada y 230 tanques) socavaron significativamente el potencial ofensivo del grupo Hoth. La situación exigía comenzar inmediatamente una ruptura del ejército de Paulus desde el cerco hacia el 4º Ejército Panzer, ya que Goth ya no tenía la oportunidad de atravesar el "corredor" por su cuenta. El avance fue comenzar con "la" señal de código "Thunderbolt". Pero Manstein no se atrevió a utilizar debido a que no estaba seguro de que el comandante del 6º Ejército Friedrich Paulus lo llevara a cabo. Primero, de acuerdo con la orden de Hitler, Paulus tuvo que mantener""Fortaleza de Stalingrado"", y romper el cerco significaba abandonar la ciudad. En segundo lugar, el mando del VI Ejército requirió 6 días para preparar un avance, ya que el combustible disponible sería suficiente para superar solo 30 km.

El mismo día, se desarrolló una situación crítica en el flanco izquierdo del grupo de ejércitos de Hollidt. Bajo la presión de las tropas soviéticas, dos divisiones italianas del Grupo de Ejércitos B se retiraron y el flanco izquierdo del grupo de Hollidt quedó expuesto. Al final del día, la abandonó sus posiciones sin autorización. Los destacamentos de vanguardia del Ejército Rojo llegaron al cruce a través del Seversky Donets cerca de la ciudad de Kamensk-Shakhtinsky. La intención de las tropas soviéticas de abrirse paso en dirección a Rostov se hizo evidente. La tarea principal del grupo de Hollidt y del 3.º Ejército rumano era ahora proteger los aeródromos de y que eran muy necesarios para los suministros del 6º ejército cercado, así como la retención de importantes cruces a través del Donets en Forkhstadt (Belaya Kalitva) y Kamensk-Shakhtinsky. La detención significó que los soviéticos le atacaran con todo y lo hicieran retroceder 200 km. El ataque, que fue llevado a cabo por la sexta división blindada, de manera implacable al comienzo, se vio amenazado por otro contraataque soviético en la retaguardia, por lo cual se decidió retroceder de manera definitiva. A todo esto, el aeródromo de , el principal de los Ju-52 para reaprovisionamiento, cayó en poder soviético. Era evidente un nuevo cerco en las alturas de Rostov. 

En los días siguientes, la situación en el frente de Chirsk se deterioró tanto que el 23 de diciembre Manstein ordenó a la 6.a División Panzer retirarse de sus posiciones y dirigirse hacia . Al amanecer del 24 de diciembre, una columna de tanques y vehículos se dirigió hacia su nuevo destino. Con la retirada de la columna, el 2.º Ejército de Guardias de R. Ya. Malinovsky pasó a la ofensiva contra el flanco extendido del 57º Cuerpo Panzer alemán. A las 16:30 del 24 de diciembre, las tropas soviéticas volvieron a capturar . El Frente de Stalingrado, con las fuerzas del 2º Ejército de la Guardia con tres cuerpos mecanizados, lanzó una ofensiva sobre . Ante esta situación, el general Hoth dio la orden de retirada general ese mismo día, eliminando así toda opción seria de salvar a las tropas sitiadas en Stalingrado.

En el bando alemán se impuso un riguroso racionamiento para intentar pasar el invierno. Paulus, asqueado ante lo absurdo de las órdenes de Hitler, se dio cuenta de que, para el Führer, el 6.º Ejército, o lo que quedaba de él, era poco menos que una pieza sacrificable en el juego de la guerra. La vida de los soldados no tenía la menor importancia para Hitler. El 25 de diciembre, en el "Kessel", murieron 1280 soldados de frío y hambre. Para el año nuevo, los soviéticos montaron una serie de cocinas y realizaron fiestas en la orilla sur del Volga con el doble objetivo de celebrar el año y mortificar a los alemanes cercados.

El 8 de enero, los soviéticos realizaron un estrechamiento del perímetro y capturaron el único aeródromo que servía de conexión con el mundo exterior, Pitomnik: los alemanes tuvieron que reconstruir el de Gumrak, gravemente dañado por ellos mismos, para poder seguir recibiendo noticias. El 9 de enero se presentaron dos oficiales del Ejército Rojo en la línea occidental del frente alemán con un ultimátum de la Stavka para Paulus. Si dicho ultimátum no se aceptaba, los soviéticos lanzarían una ofensiva final contra el "Kessel" al día siguiente. El ultimátum fue rechazado. Las penurias se multiplicaron en el 6.º Ejército Alemán: las epidemias diezmaban los soldados, la disciplina había desaparecido y el hambre era tan atroz que los alemanes sacrificaron sus caballos, además de perros y ratas para poder alimentarse. Cabe destacar que aun en estas penosas condiciones, la resistencia del 6.º Ejército continuaba, ya que las líneas del frente se retiraban combatiendo e infligiendo bajas a los soviéticos que ejecutaban el plan anillo para acabar con los alemanes.

El 28 de enero, Paulus trasladó el cuartel general hacia los sótanos del "Univermag" y allí se hacinaron unos 3000 heridos de diversa consideración, enfermos de tifus, paratifoidea y disentería. Los casos graves o que requerían cirugía prolongada eran colocados afuera para que murieran de frío.

El 30 de enero, el general Paulus fue promovido a "Generalfeldmarschall", «Mariscal de Campo». Hasta entonces ningún Mariscal de Campo alemán había sido capturado, y Paulus recibió esta promoción como una orden de suicidio. Paulus declaró entonces: «No tengo intenciones de dispararme por este cabo bohemio», en referencia a Hitler, e informó a otros generales (como Arthur Schmidt, Seydlitz, Jaenecke, y Strecker) que él no se suicidaría y se prohibía hacerlo a los demás oficiales para "seguir la suerte de sus soldados".

Un tanque soviético se acercó al cuartel general de Paulus, en el que venía un intérprete que había sido enviado por Paulus, el mayor Winrich Behr. El 31 de enero por la mañana, Paulus se rendía con más de 91 000 soldados, los restos de un ejército de 270 000 hombres, convirtiéndose en el primer mariscal que capitulara en la historia alemana, desobedeciendo así a Hitler, atenazado por las tropas soviéticas, la falta de alimentos y el frío polar de la estepa rusa, para el que sus tropas no tenían material suficiente en un gesto sin precedentes en la Wehrmacht. La rendición oficial se produjo el 2 de febrero pero unos 11 000 soldados alemanes no acataron la rendición y siguieron luchando hasta el final, a principios de marzo los soviéticos acabaron con los últimos reductos de resistencia.

El III Reich perdió en Stalingrado a su mejor ejército, con el cual Hitler se jactaba que este ejército "podía asaltar los cielos". Las perdidas también incluyen parte del 4.º Ejército Panzer y el Grupo de Ejércitos Don e incontables recursos materiales que no se pudieron reemplazar con la misma facilidad de la que disponía la URSS. De hecho, entre muertos, heridos, desaparecidos o caídos prisioneros, la Wehrmacht había perdido desde el 21 de agosto hasta el final de la batalla, a más de 400 000 combatientes, muchos de ellos experimentados, tropas de élite que sólo podían ser reemplazados mayormente por reclutas. Si se incluyen las pérdidas del Grupo de Ejércitos A, el Grupo de Ejércitos Don y unidades alemanas del Grupo de Ejércitos B durante el período del 28 de junio de 1942 al 2 de febrero de 1943, las bajas alemanas fueron más de 600 000. Por otra parte, los ejércitos aliados del Eje, sufrieron similares perdidas devastadoras, siendo el punto de quiebre en las relaciones de los satélites con Alemania. 
Los soviéticos, aparte de haber asegurado una ciudad prácticamente destruida, habían sufrido más de un millón de bajas. De estos, unos 13 000 habían muerto ejecutados por sus propios compatriotas, acusados de cobardía, deserción, colaboracionismo, etc.. Eso si se tiene en cuenta que miles de soldados soviéticos se pasaron al bando alemán. Se estima que más de 50 000 hiwis (soldados soviéticos vestidos de uniforme alemán), murieron o desaparecieron en la batalla de Stalingrado. Cabe destacar que no fue hasta la caída de la URSS que los historiadores soviéticos pudieron discutir abiertamente las cifras de bajas de la batalla, por temor a reconocer que el sacrificio de vidas fue excesivo., si bien éstas nunca serán exactas (debido a la ausencia de registros fiables y la proliferación de fosas comunes no contabilizadas), se cree que fueron muy altas, quizá más de las consideradas, haciéndose eco aquella frase de los generales soviéticos «El tiempo es sangre». Según el cálculo más alto, si se incluye a todas las fuerzas que pelearon en el Volga y el Don, murieron, desaparecieron y fueron heridos 747 000 soldados del Eje y 102 000 fueron capturados, cerca de 1 130 000 soldados soviéticos (incluyendo prisioneros muertos en cautiverio, muertos en combates, heridos tras ser evacuados, desaparecidos o capturados) y más de 300 000 civiles desaparecieron o encontraron su fin (incluyendo refugiados y gentes que vivían en pueblos y ciudades donde también se combatió). Cabe resaltar que un cuarto de millón de civiles fueron evacuados al este del país.

Cuando el 6.º Ejército Alemán se rindió con más de 91 000 soldados, estos fueron condenados a andar sobre la nieve en la denominada “marcha de la muerte” pereciendo 40 000 a causa de la caminata y las palizas. Al resto se les recluyó en los campos de concentración de Lunovo, Suzdal, Krasnogorsk, Yelabuga, Bekedal, Usman, Astrakán, Basianovski, Oranki y Karaganda, e incluso a 3500 de ellos en la misma Stalingrado para que reconstruyeran la ciudad. La mayoría de ellos, con temperaturas de -25 y -30 ºC grados bajo cero enfermó de tifus, disentería, ictericia, difteria, escorbuto, tuberculosis, hidropesía y malaria. De los 91 000 prisioneros sólo lograrían sobrevivir 5000.

Las consecuencias de esta catástrofe fueron inmensas y de gran alcance. La tragedia no pudo ocultarse al pueblo alemán, decretando tres días de duelo nacional. Por primera vez, Alemania perdía la iniciativa de la guerra y tenía que pasar a la defensiva. De hecho, la Wehrmacht carecía ya de los elementos logísticos necesarios para avanzar más hacia el este, siendo las orillas del Volga el punto más oriental alcanzado por tropas alemanas en Europa. Después de esta batalla, la Unión Soviética surgió engrandecida y con la iniciativa de la guerra en manos de sus líderes. Además, el comandante de la Luftwaffe, Hermann Göring, cayó en desgracia ante Hitler, perdiendo crédito entre la élite del régimen nazi, así como prestigio entre los militares, al no poder cumplir la orden de abastecer por aire a las fuerzas alemanas cercadas, como había prometido. 

En cuanto al "Führer," la rendición de Von Paulus en Stalingrado y la gran brecha abierta en el Frente del Este causan en Adolf Hitler una aguda crisis depresiva. Tomará somníferos todas las noches, y tendrá pesadillas sobre el cerco, hasta casi el final de la guerra. 

El triunfo de esta batalla trascendió los límites de la Unión Soviética e inspiró a todos los aliados. El 62° Ejército, comandado por Vasili Chuikov, fue incentivando la resistencia en todas partes. El rey Jorge VI de Inglaterra le regaló a la ciudad una espada forjada especialmente en su honor, y hasta el poeta chileno Pablo Neruda escribió el poema «Canto de amor a Stalingrado», recitado por primera vez el 30 de septiembre de 1942, y el poema «Nuevo canto de amor a Stalingrado» en 1943, celebrando la victoria, lo cual transformó esta lucha en un símbolo y en un punto de inflexión para toda la guerra.

El mariscal Paulus sobrevivió a la guerra y volvió a Alemania en 1952, viviendo en la "zona de ocupación soviética" y luego en la RDA. Zhúkov reclamó para sí el éxito de Stalingrado, pero se le concedieron todos los créditos a Vasili Chuikov, que fue ascendido a capitán general y puesto a cargo de un ejército que marcharía luego a Berlín. Sin embargo, la batalla de Stalingrado supuso para los nazis una auténtica catástrofe militar y una de sus principales derrotas en la Segunda Guerra Mundial, marcando además el punto de inflexión en la guerra, tras el cual ya no pararían de retroceder ante los soviéticos hasta rendirse ante Zhúkov, en el mismo Berlín, dos años y medio después. Hoy en día los historiadores occidentales consideran la Batalla de Stalingrado como la segunda Verdún de Alemania.

La Medalla "por la Defensa de Stalingrado" fue otorgada a todos los miembros de las fuerzas armadas soviéticas y también a los civiles que estuvieron directamente involucrados en la defensa de Stalingrado del 12 de julio al 19 de noviembre de 1942. A partir del 1 de enero de 1995, esta medalla había sido otorgada 759,561 veces. En el edificio del personal de la unidad nº 22220 en Volgogrado, el enorme mural está determinado por la representación de la medalla. Muestra a un grupo de soldados con fusiles apuntando hacia adelante y bayonetas plantadas bajo una bandera ondeando. A la izquierda se puede ver el contorno de los tanques y un escuadrón de aviones, sobre él la estrella soviética de cinco puntas.

Con motivo del 50 aniversario del final de la batalla, se emitió una moneda conmemorativa en honor a la ciudad de Stalingrado en 1993 con un valor nominal de 3 rublos de cobre / níquel.

Con motivo de las celebraciones en el 55 aniversario del fin de la guerra, una moneda en honor a la heroica ciudad de Stalingrado también se lanzó en 2000 como parte de la serie "Heldenstädte". La moneda con la inscripción СТАЛИНГРАД - Stalingrado muestra soldados atacantes y un pesado tanque rodante frente a las ruinas de las casas.

En el , el monumento central alemán fue inaugurado el 18 de octubre de 1964 para conmemorar a todos los soldados que murieron en Stalingrado y murieron en cautiverio. En 1988, la ciudad de Limburg se hizo cargo de la "Fundación de Combatientes de Stalingrado", asegurando así el mantenimiento y cuidado del Sitio Memorial de Stalingrado a través de la existencia de los "Antiguos Combatientes de Stalingrado". V. Alemania". El gobierno federal decidió disolverse en 2004.

Para muchas personas, una imagen permanece asociada con la Batalla de Stalingrado: la de la Virgen de Stalingrado. La imagen pintada en 1942 por el pastor protestante, médico y artista en un refugio en Stalingrado con carbón en la parte posterior de un mapa soviético lleva la inscripción "1942 Navidad en la caldera - Fortaleza de Stalingrado - Luz, vida, amor". Si bien el propio Reuber no sobrevivió al cautiverio, la imagen llegó a manos de la familia con uno de los últimos aviones, que el presidente federal Karl Carstens sugirió a la lIglesia Memorial Kaiser Wilhelm en 1983.en Berlín para conmemorar a los caídos y recordar la paz. En la iglesia (en la pared detrás de las filas de sillas correctas) cuelga una imagen de María que alienta el recuerdo y la oración. La Madonna es el motivo en el escudo de armas del Regimiento Médico 2 del Servicio Médico de la Bundeswehr.

Cada febrero en Austria, las misas conmemorativas de Stalingrado tienen lugar en muchas iglesias, que generalmente son organizadas por la u otras asociaciones tradicionales. Además, numerosos objetos de la batalla se exhiben en el Museo de Historia Militar de Viena, que incluyen: a. también reliquias de guerra como cascos de acero, botas y equipos que se recuperaron en el campo de batalla de Stalingrado.

Hay una estación de metro de Stalingrado en París. Se encuentra en la "Place de la Bataille-de-Stalingrad".

En Italia, las calles se llaman "" en varias ciudades. 

75 años después del final de la Batalla de Stalingrado, el Ayuntamiento de Volgogrado decidió a fines de enero de 2013 que la ciudad debería volver a su antiguo nombre de "Stalingrado" seis días al año. Los veteranos de guerra habían solicitado esto. La decisión provocó acaloradas discusiones en Rusia. El oficial de derechos humanos, , condenó el cambio de nombre temporal y lo calificó de "insulto a los caídos de Stalingrado". Merecen una apreciación, "pero no de esta forma". Los comunistas en Rusia están pidiendo un retorno permanente al antiguo nombre de la ciudad.





</doc>
<doc id="19364" url="https://es.wikipedia.org/wiki?curid=19364" title="Montañas Rocosas">
Montañas Rocosas

Las Montañas Rocosas o Rocallosas ("Rocky Mountains" o "Rockies" en inglés) es un sistema de cordilleras montañosas situado en el sector occidental de Norteamérica y que corre paralelo a la costa occidental, desde Columbia Británica en el noroeste, pasando por la frontera entre Alberta y Columbia Británica y llegando hasta el suroeste de Estados Unidos, en Nuevo México. El pico más alto es el monte Elbert en Colorado, con .

Las Montañas Rocosas se formaron durante la orogénesis cenozoica y están constituidas por un núcleo central de rocas cristalinas rodeado de formaciones laterales de rocas sedimentarias; el sistema ha sido marcado profundamente por la glaciación cuaternaria y la erosión atmosférica, y presenta ejemplos de fenómenos volcánicos. Tienen importantes reservas de minerales, como oro, plata, plomo, cinc, cobre y en las regiones marginales petróleo y carbón.

En sus zonas altas se extienden prados de alta montaña; en los valles se dan cultivos agrícolas cereales y patatas; y la ganadería ovina en las regiones septentrionales del sector estadounidense.
Atravesadas por muchos ferrocarriles y autopistas que dan valor a sus bellezas naturales (tuteladas por muchos parques nacionales), las Montañas Rocosas constituyen también un notable elemento de atracción turística con muchas localidades de vacaciones y de deportes de invierno.

Desde la última gran edad de hielo, las Montañas Rocosas han sido hogar de pueblos indígenas americanos, como apaches, arapahos, Bannocks, pies negros, cheyennes, crows, flathead (cabezas lisas), shoshones, sioux, utes, kutenai (ktunaxa en Canadá), sekanis, dunne-za y otros. Los paleoindios cazaban el ahora extinto mamut y bisontes antiguos (un animal 20 % más grande que el bisonte moderno) en las estribaciones y valles de las montañas. Al igual que las tribus modernas que les siguieron, los paleoindios probablemente emigraban a la en el otoño y el invierno para la caza de bisontes y a las montañas en la primavera y el verano para peces, ciervos, alces, raíces y bayas. En Colorado, a lo largo de la cresta de la divisoria continental, los muretes de roca que los nativos construyeron para conducir la caza han sido fechados hace 5400-5800 años. Hay crecientes evidencias científicas que indican que los indígenas tuvieron efectos significativos sobre las poblaciones de mamíferos por la práctica de la caza y en los patrones de vegetación por la quema deliberada.

La reciente historia humana de las Montañas Rocosas es una historia de rápidos cambios. El explorador español Francisco Vázquez de Coronado —con un grupo de soldados, misioneros y esclavos africanos— entró en la región de las Montañas Rocosas desde el sur en 1540. La introducción del caballo, herramientas de metal, rifles, nuevas enfermedades, y diferentes culturas cambiaron profundamente las culturas nativas americanas. Las poblaciones nativas americanas fueron expulsadas de la mayoría de sus áreas de distribución históricas por la enfermedad, la guerra, la pérdida de hábitat (erradicación del bisonte), y los continuados ataques contra su cultura.

En 1739, los comerciantes de pieles franceses Pierre y Paul Mallet, mientras viajaban a través de las Grandes Llanuras, descubrieron una cadena de montañas en las cabeceras del río Platte, que las tribus indias estadounidenses locales llamaban las «Rockies», convirtiéndose en los primeros europeos en informar sobre esta cordillera inexplorada.
Sir Alexander MacKenzie (1764-1820) se convirtió en el primer europeo en cruzar las Montañas Rocosas en 1793. Encontró los tramos superiores del río Fraser y llegó a la costa del Pacífico de lo que hoy es Canadá el 20 de julio de ese año, completando la primera travesía transcontinental registrada de Norteamérica al norte de México. Llegó a Bella Coola, Columbia Británica, donde por primera vez alcanzó el agua salada en South Bentinck Arm, un entrante del océano Pacífico.

La expedición de Lewis y Clark (1804-1806) fue la primera expedición que realizó un reconocimiento científico de las Montañas Rocosas. Se recolectaron especímenes para los botánicos, zoólogos y geólogos contemporáneos. La expedición se decía que había allanado el camino a (y a través de) las Montañas Rocosas a los euro-estadounidenses desde el Este, aunque Lewis y Clark durante su transcurso ya se reunieron al menos con 11 hombres de las montañas euro-estadounidense.

Los hombres de las montañas, sobre todo franceses, españoles y británicos, habían recorrido las Montañas Rocosas desde 1720 a 1800 en busca de yacimientos minerales y pieles. Los comerciantes de pieles de la North West Company establecieron en 1799 un puesto comercial en lo que ahora son las estribaciones de las Montañas Rocosas de la actual Alberta, la Rocky Mountain House, y su negocio rivalizó con el que estableció la Compañía de la Bahía de Hudson en la cercana Acton House. Estos puestos sirvieron como base para la mayoría de la actividad europea en las Montañas Rocosas de Canadá a principios del siglo XIX. Entre las más notables son las expediciones de David Thompson (cartógrafo) que siguió el río Columbia hasta el océano Pacífico. En su expedición de 1811, acampó en el cruce del río Columbia y el río Snake y erigió un poste y notificación reclamando el área para el Reino Unido e indicando la intención de la Compañía del Noroeste para construir un fuerte en el sitio.

Por la Convención Anglo-Estadounidense de 1818, que estableció el paralelo 49º Norte como frontera internacional al oeste del lago de los Bosques a las Montañas pedregosas "Stony Mountains"; el Reino Unido y los EE. UU. acordaron lo que ya se ha descrito como «ocupación conjunta» de las tierras más al oeste hasta el océano Pacífico. La resolución de los problemas territoriales y de los tratados, la disputa de Oregón, se aplazó hasta un momento posterior.

En 1819, España cedió sus derechos al norte del paralelo 42º a los Estados Unidos, a pesar de que estos derechos no incluían la posesión y también incluían obligaciones de Gran Bretaña y Rusia respecto a sus reclamaciones en la misma región.

Después de 1802, los comerciantes de pieles y exploradores estadounidenses fueron los primeros caucásicos muy extendida en las Montañas Rocosas al sur del paralelo 49. Los más famoso de estos fueron los estadounidenses William Henry Ashley, Jim Bridger, Kit Carson, John Colter, Thomas Fitzpatrick, Andrew Henry y Jedediah Smith. El 24 de julio de 1832, Benjamin Bonneville llevó la primera caravana de carromatos a través de las Montañas Rocosas utilizando Pass Sur en el actual estado de Wyoming. Del mismo modo, siguiendo a la expedición de Mckenzie de 1793, se establecieron puestos comerciales al oeste de las Northern Rockies en una región de la meseta interior septentrional de la Columbia Británica, que llegó a ser conocida como Nueva Caledonia, comenzando en Fort McLeod (actual comunidad de McLeod Lake) y Fort Fraser, pero al final se centró en el Stuart Lake Post (actual Fort St. James).

Las negociaciones entre el Reino Unido y los Estados Unidos en las siguientes décadas no consiguieron resolver el problema de límites y la disputa de Oregón llegó a ser importante en la diplomacia geopolítica entre el Imperio británico y la nueva república estadounidense. En 1841, James Sinclair, factor jefe de Compañía de la Bahía de Hudson, guio a unos 200 pobladores de la Colonia del Río Rojo al oeste para reforzar el asentamiento alrededor de Fort Vancouver en un intento de retener el Distrito de Columbia para Gran Bretaña. La partida cruzó las Montañas Rocosas en el Valle de Columbia, una región de la fosa de las Montañas Rocosas cerca de la actual Radium Hot Springs, Columbia Británica, y luego viajó al sur. A pesar de estos esfuerzos, en 1846 Gran Bretaña cedió todos sus derechos sobre las tierras del Distrito de Columbia al sur del paralelo 49º a los Estados Unidos; y la resolución del conflicto de la frontera de Oregón se acordó en el Tratado de Oregon.

Miles de personas pasaron a través de las Montañas Rocosas siguiendo la ruta de Oregón a partir de la década de 1840. Los mormones comenzaron a establecerse cerca del Gran Lago Salado en 1847. Entre 1859 y 1864, se descubrió oro en Colorado, Idaho, Montana, y Columbia Británica, lo que provocó varias fiebres del oro con lo que miles de buscadores de oro y mineros exploraron cada montaña y cañón y para crear la primera gran industria de las Montañas Rocosas. La fiebre del oro de Idaho, por sí sola, produjo más oro que las fiebres de California y Alaska juntas y fue importante en la financiación del Ejército de la Unión durante la Guerra Civil Americana. El ferrocarril transcontinental fue terminado en 1869, y el parque nacional de Yellowstone se estableció siendo el primer parque nacional del mundo en 1872. Mientras tanto, en Canadá se prometió originalmente en 1871 un ferrocarril transcontinental. A pesar de las complicaciones políticas, la Canadian Pacific Railway se finalizó en 1885, atravesando la cordillera por los pasos de Kicking Horse y Rogers hasta el océano Pacífico. Los funcionarios ferroviarios canadienses también convencieron al Parlamento para que dejase al margen vastas áreas de las Montañas Rocosas canadienses que luegon se convertirán en los actuales parques nacionales de Jasper, Banff, Yoho y Waterton Lakes, sentando las bases para una industria del turismo que se desarrolla en la actualidad. El estadounidense parque nacional de los Glaciares (Montana) se estableció con una relación similar a las promociones de turismo por la Great Northern Railway. Mientras los colonos llenaban los valles y los pueblos mineros, la ética de la conservación y la preservación comenzaron a tomar fuerza. El presidente de los EE. UU. Harrison estableció varias reservas forestales en las Montañas Rocosas en 1891-92.+5491168069732

Situadas a lo largo de la frontera entre Alberta y la Columbia Británica, las Rocosas (o Rocallosas) Canadienses están situadas dentro de dos parques nacionales muy grandes: el Banff, al sur, y el Jasper, al norte. El Parque Nacional Banff fue el primer santuario oficial de la vida salvaje en Canadá, y hoy en día la ciudad que le dio nombre se ha convertido en el primer centro turístico del país, tanto en verano como en invierno, aunque el Parque Nacional Jasper sea más extenso e inexplorado.
Las precipitaciones que recibe son moderadas, lo que lleva a concluir que los ríos son de un caudal irregular. Su población es muy escasa y el número de ciudades es realmente muy pobre. Entre sus principales actividades económicas se encuentra la maderera y la obtención de minerales como oro, plata, molibdeno o zinc y de petróleo y gas. Su vegetación se caracteriza principalmente por la presencia de las coníferas. Ya en Alaska y no perteneciente a las Montañas Rocosas, se encuentra el monte Denali que con sus 6194 metros sobre el nivel del mar, es la mayor altura de América del Norte.

Tradicionalmente, las Montañas Rocosas se han considerado divididas, geográfica y geológicamente, en varios sectores o tramos. De norte a sur, las divisiones son las siguientes:



</doc>
<doc id="19366" url="https://es.wikipedia.org/wiki?curid=19366" title="Kevin Smith">
Kevin Smith

Kevin Patrick Smith (Red Bank, Nueva Jersey; 2 de agosto de 1970) es un guionista y director de cine estadounidense y el fundador de View Askew Productions. También es conocido como escritor de cómics.

Las películas de Smith suelen estar protagonizadas por los mismos actores, que incluyen a Jason Lee, Ben Affleck, Ali Larter, Jason Mewes y Matt Damon. También han participado En sus películas también han participado otros actores famosos como Salma Hayek, Chris Rock, Jason Biggs, Shannen Doherty, George Carlin, Shannon Elizabeth, Ethan Suplee, Joe Quesada, Eliza Dushku, Stan Lee y Alanis Morissette. Kevin Smith aparece en la mayor parte de sus películas (a excepción de "Una chica de Jersey" y "Zack and Miri Make a Porno") y siempre haciendo el mismo papel, el de Bob el Silencioso ("Silent Bob").

Fue coproductor ejecutivo de la película "El indomable Will Hunting", en 1998, ayudando a sus amigos Matt Damon y Ben Affleck a realizar su película y con el marketing de la misma. Después Damon y Affleck recibieron un premio Óscar por su guion, aunque algunos críticos alegan que Smith fue el responsable del mismo, rumor que este niega vehementemente. En la cinta "Jay y Bob el Silencioso contraatacan" hay una referencia al hecho de que Smith les haya ayudado a promocionar la película.

Es un conocido autor de cómics. Escribió para Marvel Comics historias de Daredevil y Spider-Man / Black Cat, y para DC Comics, y además tiene un cameo en la película "Daredevil" (2003), protagonizada por Ben Affleck. En dicha película interpreta a un forense que se llama Kirby, por Jack Kirby, una afamado dibujante de cómics. Joe Quesada es el dibujante que le acompañó en su etapa de guionista del cómic de Daredevil. Quesada actualmente es redactor jefe de Marvel Comics.

También ha hecho cómics sobre sus propias películas, incluyendo tres cómics: "Clerks", "Persiguiendo a Dogma", que tiene lugar entre las películas "Persiguiendo a Amy" y "Dogma", y Bluntman y Chronic, el cómic creado por Banky y Holden en "Persiguiendo a Amy". Kevin ha dicho durante años que escribiría otros cómics basados en sus películas, incluyendo "Mallrats 2", una secuela de "Mallrats", y "Bartleby y Loki", basada en los personajes de "Dogma", aunque no han sido publicados. En 1999 ganó un premio Harvey por sus logros en los cómics. Ha recibido numerosas críticas sobre la tardanza con la que completa sus historietas.

Es conocido por reescribir guiones, incluyendo un borrador de la película "Bar Coyote". Aunque de acuerdo con sus declaraciones, ningún diálogo de su guion apareció finalmente en dicha cinta.

Kevin Smith nació y se crio en Chapatales Highlands, Nueva Jersey, ciudad de la que se siente muy orgulloso, lo cual se puede constatar en todas sus películas. Su primera película, "Clerks", fue filmada en la tienda en la cual trabajaba y para hacerla tuvo que vender su colección de cómics que recuperó tras el éxito de la misma. Ganó el premio más importante en el festival de Sundance, en 1994, y fue distribuida posteriormente por los estudios Miramax. Es una de las películas que más recaudación obtuvo respecto a lo que costó tras "The Blair Witch Project" La película caló tan bien que los estudios firmaron a Smith para que hiciera más películas; la siguiente fue "Mallrats". Durante el rodaje, Smith reunió a sus amigos y estrellas cercanas de su próxima película, Ben Affleck y Jason Lee y su nueva novia, Joey Lauren Adams. Smith ha dicho que su relación con Adams fue una inspiración para su siguiente película, "Persiguiendo a Amy", el drama de la comedia de Smith que ganó unos pocos premios independientes. Por la misma época de "Persiguiendo a Amy", conoció a la que sería su esposa, Jennifer Schwalbach.

Después de "Persiguiendo a Amy", Smith dirigió "Dogma", una película polémica acerca de la cristiandad. La esposa de Smith dio a luz a su primera hija, Harley Quinn Smith (en honor al personaje de Batman y extraña pareja sentimental del Joker, la villana Harley Quinn). Harley Quinn y Jennifer tienen dos pequeños papeles en la que sería su próxima película, "Jay y Bob el Silencioso contraatacan". En esta comedia, los héroes de culto Jay y Bob el Silencioso quieren parar la producción de una película que se hace acerca de ellos, encontrar el amor verdadero, y salvar a su mono.

Smith ha escrito también guiones para "Daredevil" o "Green Arrow". Escribió también un guion para una película nueva de Superman, pero le hicieron dejar el proyecto. Smith tiene tres películas más en las que trabajar para los próximos años y ha abierto una tienda de cómics, tiene una compañía de producción, escribe artículos para la revista "Arena", y elabora los cortometrajes del "show" de Jay Leno.
Sus últimas películas hasta el momento han sido la segunda parte de su exitoso primer filme, llamada "Clerks II" y la película "Zack and Miri make porno". Es consultor creativo y dirigió el primer episodio de la serie "Reaper".

Kevin fue contratado por New Line para reescribir "Overnight Delivery" (1998), de la cual solo se esperaba que se convirtiera en otra mala película americana de videoclub. La novia de por aquel entonces de Kevin Smith, Joey Lauren Adams, casi toma el papel de Ivy en la película en lugar de la protagonista en Persiguiendo a Amy. Finalmente el papel recayó sobre Reese Witherspoon y "Overnight Delivery" pasó directamente a Video. Su participación en la película fue revelada cuando Kevin escribió una columna en internet www.viewaske.com.




</doc>
<doc id="19367" url="https://es.wikipedia.org/wiki?curid=19367" title="Graham Chapman">
Graham Chapman

Graham Chapman (Leicester, Inglaterra, 8 de enero de 1941 – Maidstone, Inglaterra, 4 de octubre de 1989) fue un miembro del grupo humorístico Monty Python. Graham Chapman nació en Leicester, hijo de un policía. Abandonó sus estudios de medicina en la Universidad de Cambridge.

Conocido por protagonizar a personajes autoritarios, como el coronel famoso que interrumpía los sketches, también interpretó varias veces los papeles de doctor, para el que su formación habrá contribuido mucho, entre otros tantísimos papeles. Realizó los papeles principales en "La vida de Brian", protagonizando el papel de Brian, y "Los caballeros de la mesa cuadrada", como Arturo. Con el tiempo, el alcoholismo perturbó su desempeño como actor.

Chapman nació en la Stoneygate Nursing Home, en Stoneygate, Leicester. Fue educado en la escuela Melton Mowbray Grammar School, estudió medicina en la institución educativa Emmanuel College y más tarde en la St Bartholomew's Medical College de la universidad Queen Mary, University of London. Fue un ávido fanático de la comedia en la radio desde temprana edad, estaba especialmente atraído por el programa "The Goon Show". En la introducción a su biografía póstuma (2005/2006), Jim Yoakum apunta que los programas de radio no le hacían reír necesariamente. Solo unos pocos consiguieron una risa de Chapman, incluyendo a Frankie Howerd, el equipo de Jimmy Jewel y Ben Warriss, "It's That Man Again", "Educating Archie", "Take It From Here" y "Much-Binding-in-the-Marsh". «Me gustó especialmente Robert Moreton, aunque parecía que a nadie más le gustaba demasiado. Él hacía cosas como contar chistes mal a propósito y cambiar el orden de las frases. Era obviamente un buen comediante avanzado a su tiempo. La aparente incompetencia que mostraba era maravillosa. Fue uno de mis héroes.» Pero el programa que realmente sorprendió a Graham, y que se convertiría en una mayor influencia en su carrera fue "The Goon Show" (p.xvii). Chapman dijo que «desde los siete u ocho años solía ser un ávido oyente del programa "The Goon Show". De hecho, en esa época deseaba "ser" un "goon"». (p. 23).

Entre los amigos más cercanos de Chapman se encontraban Keith Moon de The Who, el cantante Harry Nilsson, y el "beatle" Ringo Starr. Chapman fue alcohólico en los años 1970, y mantuvo en secreto su homosexualidad hasta mediados de esa década (aunque sus compañeros en Monty Python ya sabían de su orientación sexual), cuando la confesó en un programa de entrevistas presentado por el músico de jazz George Melly, convirtiéndose así en una de los primeros famosos en hacerlo. Unos días después reveló su orientación a un grupo de amigos en una fiesta celebrada en su casa de Belsize Park, donde les presentó oficialmente a su compañero David Sherlock. Posteriormente, Chapman se convertiría en un defensor de los derechos de los homosexuales.

Vivió durante veinte años con David Sherlock, con el que adoptó un hijo, John Tomiczeck (quien murió en 1991), un adolescente huido del hogar que Graham encontró en una calle de Londres.

En 1969 Chapman y Cleese se unieron a Michael Palin, Terry Jones, Eric Idle y al artista estadounidense Terry Gilliam para crear "Monty Python's Flying Circus". Uno de los personajes más recordados de Chapman era "El Coronel", un estirado oficial británico que aparecía de repente en medio de las actuaciones para ordenar el fin del sketch, por ser este demasiado estúpido.

Después de que Cleese abandonase la serie en 1973, Chapman siguió escribiendo en solitario la cuarta y última temporada de la serie, así como con la colaboración de Neil Innes y Douglas Adams. Más tarde desarrolló varios proyectos para cine y televisión, a destacar entre ellos: "Out of the Trees", "The Odd Job" y "Yellowbeard", en la cual actuaba junto a Cleese, Peter Cook, Cheech y Chong y Marty Feldman (quien murió en los últimos días de rodaje).

Graham Chapman, quien fue un gran fumador, murió el 4 de octubre de 1989 a raíz de un cáncer. Como parte de la elegía de su funeral, Eric Idle cantó un fragmento de "Always Look On The Bright Side Of Life", canción compuesta por él mismo, con la que termina "La vida de Brian". El mensaje de dicha canción es muy apropiado dado el optimismo, gran sentido del humor y generosidad de Graham Chapman.

John Cleese fue quien pronunció el discurso principal del funeral, entre cuyas palabras se decía lo siguiente:

Cuando murió, el grupo se preparaba para celebrar su 20º aniversario, por lo que Terry Jones dijo: "Es el mayor aguafiestas que he conocido. Ahora en serio, lo echamos mucho de menos, lo queríamos mucho".





</doc>
<doc id="19368" url="https://es.wikipedia.org/wiki?curid=19368" title="Led">
Led

Un diodo emisor de luz o led (también conocido por la sigla LED, del inglés "light-emitting diode") es una fuente de luz constituida por un material semiconductor dotado de dos terminales. Se trata de un diodo de unión p-n, que emite luz cuando está activado. Si se aplica una tensión adecuada a los terminales, los electrones se recombinan con los huecos en la región de la unión p-n del dispositivo, liberando energía en forma de fotones. Este efecto se denomina electroluminiscencia, y el color de la luz generada (que depende de la energía de los fotones emitidos) viene determinado por la anchura de la banda prohibida del semiconductor. Los ledes son normalmente pequeños (menos de 1 mm) y se les asocian algunos componentes ópticos para configurar un patrón de radiación.

Los primeros ledes fueron fabricados como componentes electrónicos para su uso práctico en 1962 y emitían luz infrarroja de baja intensidad. Estos ledes infrarrojos se siguen empleando como elementos transmisores en circuitos de control remoto, como son los mandos a distancia utilizados dentro de una amplia variedad de productos de electrónica de consumo. Los primeros ledes de luz visible también eran de baja intensidad y se limitaban al espectro rojo. Los ledes modernos pueden abarcar longitudes de onda dentro de los espectros visible, ultravioleta e infrarrojo, y alcanzar luminosidades muy elevadas.

Los primeros ledes se emplearon en los equipos electrónicos como lámparas indicadoras en sustitución de las bombillas incandescentes. Pronto se asociaron para las presentaciones numéricas en forma de indicadores alfanuméricos de siete segmentos, al mismo tiempo que se incorporaron en los relojes digitales. Los recientes desarrollos ya permiten emplear los ledes para la iluminación ambiental en sus diferentes aplicaciones. Los ledes han permitido el desarrollo de nuevas pantallas de visualización y sensores, y sus altas velocidades de conmutación permiten utilizarlos también para tecnologías avanzadas de comunicaciones.

Hoy en día, los ledes ofrecen muchas ventajas sobre las fuentes convencionales de luces incandescentes o fluorescentes, destacando un menor consumo de energía, una vida útil más larga, una robustez física mejorada, un tamaño más pequeño así como la posibilidad de fabricarlos en muy diversos colores del espectro visible de manera mucho más definida y controlada; en el caso de ledes multicolores, con una frecuencia de conmutación rápida. 

Estos diodos se utilizan ahora en aplicaciones tan variadas que abarcan todas las áreas tecnológicas actuales, desde la Bioingeniería, la Medicina y la Sanidad, pasando por la nanotecnología y la computación cuántica, los dispositivos electrónicos o la iluminación en la ingeniería de Minas; entre los más populares están la RETROILUMINción de pantallas de TV y ordenador, así como de dispositivos móviles la luz de navegación de los aviones, los faros delanteros de los vehículos, los anuncios publicitarios, la iluminación en general, los semáforos, las lámparas de destellos y los papeles luminosos de pared. Desde el comienzo de 2017, las lámparas led para la iluminación de las viviendas son tan baratas o más que las lámparas fluorescentes compacta de comportamiento similar al de los ledes. También son más eficientes energéticamente y, posiblemente, su eliminación como desecho provoque menos problemas ambientales.

El fenómeno de la electroluminiscencia fue descubierto en 1907 por el experimentador británico Henry Joseph Round, de los laboratorios Marconi, usando un cristal de carburo de silicio y un detector de bigotes de gato. El inventor soviético Oleg Lósev informó de la construcción del primer led en 1927. Su investigación apareció en revistas científicas soviéticas, alemanas y británicas, pero el descubrimiento no se llevó a la práctica hasta varias décadas más tarde. Kurt Lehovec, Carl Accardo y Edward Jamgochian interpretaron el mecanismo de estos primeros diodos led en 1951, utilizando un aparato que empleaba cristales de carburo de silicio, con un generador de impulsos y con una fuente de alimentación de corriente, y en 1953 con una variante pura del cristal.

Rubin Braunstein, de la RCA, informó en 1955 sobre la emisión infrarroja del arseniuro de galio (GaAs) y de otras aleaciones de semiconductores. Braunstein observó que esta emisión se generaba en diodos construidos a partir de aleaciones de antimoniuro de galio (GaSb), arseniuro de galio (GaAs), fosfuro de indio (InP) y silicio-germanio (SiGe) a temperatura ambiente y a 77 kelvin.

En 1957, Braunstein también demostró que estos dispositivos rudimentarios podían utilizarse para establecer una comunicación no radiofónica a corta distancia. Como señala Kroemer, Braunstein estableció una línea de comunicaciones ópticas muy simple: tomó la música procedente de un tocadiscos y la procesó mediante la adecuada electrónica para modular la corriente directa producida por un diodo de GaAs Arseniuro de Galio. La luz emitida por el diodo de GaAS fue capaz de sensibilizar un diodo de PbS Sulfuro de Plomo situado a una cierta distancia. La señal así generada por el diodo de PbS fue introducida en un amplificador de audio y se trasmitió por un altavoz. Cuando se interceptaba el rayo luminoso entre los dos ledes, cesaba la música. Este montaje ya presagiaba el empleo de los ledes para las comunicaciones ópticas.
En septiembre de 1961, James R. Biard y Gary Pittman, que trabajaban en Texas Instruments (TI) de Dallas (Texas), descubrieron una radiación infrarroja (de 900 nm) procedente de un diodo túnel que habían construido empleando un sustrato de arseniuro de galio (GaAs). En octubre de 1961 demostraron la existencia de emisiones de luz eficientes y el acoplamiento de las señales entre la unión p-n de arseniuro de galio emisora de luz y un fotodetector aislado eléctricamente y construido con un material semiconductor. Con base en sus descubrimientos, el 8 de agosto de 1962 Biard y Pittman produjeron una patente de título “Semiconductor Radiant Diode” (Diodo radiante semiconductor) que describía cómo una aleación de zinc difundida durante el crecimiento del cristal que forma el sustrato de una unión p-n led con un contacto del cátodo lo suficientemente separado, permitía la emisión de luz infrarroja de manera eficiente en polarización directa. 

A la vista de la importancia de sus investigaciones, tal como figuraban en sus cuadernos de notas de ingeniería y antes incluso de comunicar sus resultados procedentes de los laboratorios de General Electric, Radio Corporation of America, IBM, Laboratorios Bell o las del Laboratorio Lincoln del Instituto Tecnológico de Massachusetts, la Oficina de Patentes y Marcas de Estados Unidos les concedió una patente por la invención de los diodos emisores de luz infrarroja de arseniuro de galio (patente US3293513A de los EE. UU.), que son considerados como los primeros ledes de uso práctico. Inmediatamente después de la presentación de la patente, la TI inició un proyecto para la fabricación de los diodos infrarrojos. En octubre de 1962, Texas Instruments desarrolló el primer led comercial (el SNX-100), que empleaba un cristal puro de arseniuro de galio para la emisión de luz de 890 nm. En octubre de 1963, TI sacó al mercado el primer led semiesférico comercial, el SNX-110.

El primer led con emisión en el espectro visible (rojo) fue desarrollado en 1962 por Nick Holonyak.Jr cuando trabajaba en la General Electric. Holonyak presentó un informe en la revista Applied Physics Letters el 1 de diciembre de 1962. En 1972 M. George Craford, un estudiante de grado de Holonyak, inventó el primer led amarillo y mejoró la luminosidad de los ledes rojo y rojo-naranja en un factor de diez. En 1976, T. P. Pearsall construyó los primeros ledes de alto brillo y alta eficiencia para las telecomunicaciones a través de fibras ópticas. Para ello descubrió nuevos materiales semiconductores expresamente adaptados a las longitudes de onda propias de la citada transmisión por fibras ópticas.

Los primeros ledes comerciales fueron generalmente usados para sustituir a las lámparas incandescentes y las lámparas indicadoras de neón así como en los visualizadores de siete segmentos. Primero en equipos costosos tales como equipos electrónicos y de ensayo de laboratorio, y más tarde en otros dispositivos eléctricos como televisores, radios, teléfonos, calculadoras, así como relojes de pulsera. Hasta 1968, los ledes visibles e infrarrojos eran extremadamente costosos, del orden de 200 dólares por unidad, por lo que tuvieron poca utilidad práctica. La empresa Monsanto Company fue la primera que produjo de manera masiva ledes visibles, utilizando fosfuro de arseniuro de galio (GaAsP) en 1968 para producir ledes rojos destinados a los indicadores.

Hewlett-Packard (HP) introdujo los ledes en 1968, inicialmente utilizando GaAsP suministrado por Monsanto. Estos ledes rojos eran lo suficientemente brillantes como para ser utilizados como indicadores, puesto que la luz emitida no era suficiente para iluminar una zona. Las lecturas en las calculadoras eran tan débiles que sobre cada dígito se depositaron lentes de plástico para que resultaran legibles. Más tarde, aparecieron otros colores que se usaron ampliamente en aparatos y equipos. En la década de los 70 Fairchild Optoelectrónics fabricó con éxito comercial dispositivos led a menos de cinco centavos cada uno. Estos dispositivos emplearon chips de semiconductores compuestos fabricados mediante el proceso planar inventado por Jean Hoerni de Fairchild Semiconductor. El procesado planar para la fabricación de chips combinado con los métodos innovadores de encapsulamiento permitió al equipo dirigido por el pionero en optoelectrónica, Thomas Brandt, lograr las reducciones de coste necesarias en Fairchild. Estos métodos siguen siendo utilizados por los fabricantes de los ledes.
La mayoría de los ledes se fabricaron en los encapsulamientos típicos T1¾ de 5mm y T1 de 3mm, pero con el aumento de la potencia de salida, se ha vuelto cada vez más necesario eliminar el exceso de calor para mantener la fiabilidad. Por tanto ha sido necesario diseñar encapsulamientos más complejos ideados para conseguir una eficiente disipación de calor. Los encapsulamientos empleados actualmente para los ledes de alta potencia tienen poca semejanza con los de los primeros ledes.

Los ledes azules fueron desarrollados por primera vez por Henry Paul Maruska de RCA en 1972 utilizando nitruro de Galio (GaN) sobre un substrato de zafiro.
Se empezaron a comercializar los de tipo SiC (fabricados con carburo de silicio) por la casa Cree, Inc., Estados Unidos en 1989. Sin embargo, ninguno de estos ledes azules era muy brillante.

El primer led azul de alto brillo fue presentado por Shuji Nakamura de la Nichia Corp. en 1994 partiendo del material Nitruro de Galio-Indio (InGaN). Isamu Akasaki y Hiroshi Amano en Nagoya trabajaban en paralelo, en la nucleación cristalina del Nitruro de Galio sobre substratos de zafiro, obteniendo así el dopaje tipo-p con dicho material. Como consecuencia de sus investigaciones, Nakamura, Akasaki y Amano fueron galardonados con el . En 1995, Alberto Barbieri del laboratorio de la Universidad de Cardiff (RU) investigaba la eficiencia y fiabilidad de los ledes de alto brillo y como consecuencia de la investigación obtuvo un led con el electrodo de contacto transparente utilizando óxido de indio y estaño (ITO) sobre fosfuro de aluminio-galio-indio y arseniuro de galio.

En 2001 y 2002 se llevaron a cabo procesos para hacer crecer ledes de nitruro de galio en silicio. Como consecuencia de estas investigaciones, en enero de 2012 Osram lanzó al mercado ledes de alta potencia de nitruro de galio-indio crecidos sobre sustrato de silicio.

El logro de una alta eficiencia en los ledes azules fue rápidamente seguido por el desarrollo del primer led blanco. En tal dispositivo un “fósforo” (material fluorescente) de recubrimiento Y Al O:Ce (conocido como YAG o granate de itrio y aluminio) absorbe algo de la emisión azul y genera luz amarilla por fluorescencia. De forma similar es posible introducir otros “fósforos” que generen luz verde o roja por fluorescencia. La mezcla resultante de rojo, verde y azul se percibe por el ojo humano como blanco; por otro lado, no sería posible apreciar los objetos de color rojo o verde iluminándolos con el fósforo YAG puesto que genera solo luz amarilla junto con un remanente de luz azul.

Los primeros ledes blancos eran caros e ineficientes. Sin embargo, la intensidad de la luz producida por los ledes se ha incrementado exponencialmente, con un tiempo de duplicación que ocurre aproximadamente cada 36 meses desde la década de los 1960 (de acuerdo con la ley de Moore). Esta tendencia se atribuye generalmente a un desarrollo paralelo de otras tecnologías de semiconductores y a los avances de la óptica y de la ciencia de los materiales, y se ha convenido en llamar la ley de Haitz en honor a Roland Haitz.

La emisión luminosa y la eficiencia de los ledes azul y ultravioleta cercano aumentaron a la vez que bajó el coste de los dispositivos de iluminación con ellos fabricados, lo que condujo a la utilización de los ledes de luz blanca para iluminación. El hecho es que están sustituyendo a la iluminación incandescente y la fluorescente.

Los ledes blancos pueden producir 300 lúmenes por vatio eléctrico a la vez que pueden durar hasta 100000 horas. Comparado con las bombillas de incandescencia esto supone no solo un incremento enorme de la eficiencia eléctrica sino también un gasto similar o más bajo por cada bombilla.

Una unión P-N puede proporcionar una corriente eléctrica al ser iluminada. Análogamente una unión P-N recorrida por una corriente directa puede emitir fotones luminosos. Son dos formas de considerar el fenómeno de la electroluminiscencia. En el segundo caso esta podría definirse como la emisión de luz por un semiconductor cuando está sometido a un campo eléctrico. Los portadores de carga se recombinan en una unión P-N dispuesta en polarización directa. En concreto, los electrones de la región N cruzan la barrera de potencial y se recombinan con los huecos de la región P. Los electrones libres se encuentran en la banda de conducción mientras que los huecos están en la banda de valencia. De esta forma, el nivel de energía de los huecos es inferior al de los electrones. Al recombinarse los electrones y los huecos una fracción de la energía se emite en forma de calor y otra fracción en forma de luz.

El fenómeno físico que tiene lugar en una unión PN al paso de la corriente en polarización directa, por tanto, consiste en una sucesión de recombinaciones electrón-hueco. El fenómeno de la recombinación viene acompañado de la emisión de energía. En los diodos ordinarios de Germanio o de Silicio se producen fonones o vibraciones de la estructura cristalina del semiconductor que contribuyen, simplemente, a su calentamiento. En el caso de los diodos led, los materiales semiconductores son diferentes de los anteriores tratándose, por ejemplo, de aleaciones varias del tipo III-V como son el arseniuro de galio ( AsGa ), el (PGa) o el fosfoarseniuro de galio (PAsGa ). 

En estos semiconductores, las recombinaciones que se desarrollan en las uniones PN eliminan el exceso de energía emitiendo fotones luminosos. El color de la luz emitida depende directamente de su longitud de onda y es característico de cada aleación concreta. En la actualidad se fabrican aleaciones que producen fotones luminosos con longitudes de onda en un amplio rango del espectro electromagnético dentro del visible, infrarrojo cercano y ultravioleta cercano. Lo que se consigue con estos materiales es modificar la anchura en energías de la banda prohibida, modificando así la longitud de onda del fotón emitido. Si el diodo led se polariza inversamente no se producirá el fenómeno de la recombinación por lo que no emitirá luz. La polarización inversa puede llegar a dañar al diodo.

El comportamiento eléctrico del diodo led en polarización directa es como sigue. Si se va incrementando la tensión de polarización, a partir de un cierto valor (que depende del tipo de material semiconductor), el led comienza a emitir fotones, se ha alcanzado la tensión de encendido. Los electrones se pueden desplazar a través de la unión al aplicar a los electrodos diferentes tensiones; se inicia así la emisión de fotones y conforme se va incrementando la tensión de polarización, aumenta la intensidad de luz emitida. Este aumento de intensidad luminosa viene emparejado al aumento de la intensidad de la corriente y puede verse disminuida por la recombinación Auger. Durante el proceso de recombinación, el electrón salta de la banda de conducción a la de valencia emitiendo un fotón y accediendo, por conservación de la energía y momento, a un nivel más bajo de energía, por debajo del nivel de Fermi del material. El proceso de emisión se llama recombinación radiativa, que corresponde al fenómeno de la emisión espontánea. Así, en cada recombinación radiativa electrón-hueco se emite un fotón de energía igual a la anchura en energías de la banda prohibida:

formula_1siendo c la velocidad de la luz y f y λ la frecuencia y la longitud de onda, respectivamente, de la luz que emite. Esta descripción del fundamento de la emisión de radiación electromagnética por el diodo led se puede apreciar en la figura donde se hace una representación esquemática de la unión PN del material semiconductor junto con el diagrama de energías, implicado en el proceso de recombinación y emisión de luz, en la parte baja del dibujo. La longitud de onda de la luz emitida, y por lo tanto su color, depende de la anchura de la banda prohibida de energía. Los substratos más importantes disponibles para su aplicación en emisión de luz son el GaAs y el InP. Los diodos led pueden disminuir su eficiencia si sus picos de absorción y emisión espectral en función de su longitud de onda están muy próximos, como ocurre con los ledes de GaAs:Zn (arseniuro de galio dopado con zinc) ya que parte de la luz que emiten la absorben internamente. 

Los materiales utilizados para los ledes tienen una banda prohibida en polarización directa cuya anchura en energías varía desde la luz infrarroja, al visible o incluso al ultravioleta próximo. La evolución de los ledes comenzó con dispositivos infrarrojos y rojos de arseniuro de galio. Los avances de la ciencia de materiales han permitido fabricar dispositivos con longitudes de onda cada vez más cortas, emitiendo luz en una amplia gama de colores. Los ledes se fabrican generalmente sobre un sustrato de tipo N, con un electrodo conectado a la capa de tipo P depositada en su superficie. Los sustratos de tipo P, aunque son menos comunes, también se fabrican.

Un led comienza a emitir cuando se le aplica una tensión de 2-3 voltios. En polarización inversa se utiliza un eje vertical diferente al de la polarización directa para mostrar que la corriente absorbida es prácticamente constante con la tensión hasta que se produce la ruptura.

El led es un diodo formado por un chip semiconductor dopado con impurezas que crean una unión PN. Como en otros diodos, la corriente fluye fácilmente del lado p, o ánodo, al n, o cátodo, pero no en el sentido opuesto. Los portadores de carga (electrones yhuecos) fluyen a la unión desde dos electrodos puestos a distintos voltajes. Cuando un electrón se recombina con un hueco, desciende su nivel de energía y el exceso de energía se desprende en forma de un fotón. La longitud de onda de la luz emitida, y por tanto el color del led, depende de la anchura en energía de la banda prohibida correspondiente a los materiales que constituyen la unión pn. 

En los diodos de silicio o de germanio los electrones y los huecos se recombinan generando una transición no radiativa, la cual no produce ninguna emisión luminosa ya que son materiales semiconductores con una banda prohibida indirecta. Los materiales empleados en los ledes presentan una banda prohibida directa con una anchura en energía que corresponde al espectro luminoso del infrarrojo-cercano (800 nm - 2500 nm), el visible y el ultravioleta-cercano (200-400nm). 
El desarrollo de los ledes dio comienzo con dispositivos de luz roja e infrarroja, fabricados con arseniuro de galio (GaAs). Los avances en la ciencia de materiales han permitido construir dispositivos con longitudes de onda cada vez más pequeñas, emitiendo luz dentro de una amplia gama de colores.

Los ledes se suelen fabricar a partir de un sustrato de tipo n, con uno de los electrodos unido a la capa de tipo p depositada sobre su superficie. Los sustratos de tipo p también se utilizan, aunque son menos comunes. Muchos ledes comerciales, en especial los de GaN/InGaN, utilizan también el zafiro (óxido de aluminio) como sustrato.

La mayoría de los materiales semiconductores usados en la fabricación de los ledes presentan un índice de refracción muy alto. Esto implica que la mayoría de la luz emitida en el interior del semiconductor se refleja al llegar a la superficie exterior que se encuentra en contacto con el aire por un fenómeno de reflexión total interna. La extracción de la luz constituye, por tanto, un aspecto muy importante y en constante investigación y desarrollo a tomar en consideración en la producción de ledes. 

La mayoría de los materiales semiconductores usados en la fabricación de los ledes presentan un índice de refracción muy elevado con respecto al aire. Esto implica que la mayoría de la luz emitida en el interior del semiconductor se va a reflejar al llegar a la superficie exterior que se encuentra en contacto con el aire por un fenómeno de reflexión total interna.

Este fenómeno afecta tanto a la eficiencia en la emisión luminosa de los ledes como a la eficiencia en la absorción de la luz de las células fotovoltaicas. El índice de refracción del silicio es 3.96 (a 590nm), mientras que el del aire es 1,0002926. La extracción de la luz constituye, por tanto, un aspecto muy importante y en constante investigación y desarrollo a tomar en consideración en la producción de ledes.

En general, un chip semiconductor led de superficie plana sin revestir emitirá luz solamente en la dirección perpendicular a la superficie del semiconductor y en unas direcciones muy próximas, formando un cono llamado cono de luz o cono de escape. El máximo ángulo de incidencia que permite escapar a los fotones del semiconductor se conoce como ángulo crítico. Cuando se sobrepasa este ángulo, los fotones ya no se escapan del semiconductor pero en cambio son reflejados dentro del cristal del semiconductor como si existiese un espejo en la superficie exterior.

Debido a la reflexión interna, la luz que ha sido reflejada internamente en una cara puede escaparse a través de otras caras cristalinas si el ángulo de incidencia llega a ser ahora suficientemente bajo y el cristal es suficientemente transparente para no reflejar nuevamente la emisión de fotones hacia el interior. Sin embargo, en un simple led cúbico con superficies externas a 90 grados, todas las caras actúan como espejos angulares iguales. En este caso, la mayor parte de la luz no puede escapar y se pierde en forma de calor dentro del cristal semiconductor.

Un chip que presente en su superficie facetas anguladas similares a las de una joya tallada o a una lente fresnel puede aumentar la salida de la luz al permitir su emisión en las orientaciones que sean perpendiculares a las facetas exteriores del chip, normalmente más numerosas que las seis únicas de una muestra cúbica.

La forma ideal de un semiconductor para obtener la máxima salida de luz sería la de una microesfera con la emisión de los fotones situada exactamente en el centro de la misma, y dotada de electrodos que penetraran hasta el centro para conectar con el punto de emisión. Todos los rayos de luz que partieran del centro serían perpendiculares a la superficie de la esfera, lo que daría lugar a que no hubiera reflexiones internas. Un semiconductor semiesférico también funcionaría correctamente puesto que la parte plana actuaría como un espejo para reflejar los fotones de forma que toda la luz se podría emitir completamente a través de la semiesfera.

Después de construir una oblea de material semiconductor, se corta en pequeños fragmentos. Cada fragmento se denomina chip y pasa a constituir la pequeña parte activa de un diodo led emisor de luz.

Muchos chips semiconductores led se encapsulan o se incorporan en el interior en carcasas de plástico moldeado. La carcasa de plástico pretende conseguir tres propósitos:

La tercera característica contribuye a aumentar la emisión de luz desde el semiconductor actuando como una lente difusora, permitiendo que la luz sea emitida al exterior con un ángulo de incidencia sobre la pared exterior mucho mayor que la del estrecho cono de luz procedente del chip sin recubrir.

Los ledes están diseñados para funcionar con una potencia eléctrica no superior a 30-60 milivatios (mW). En torno a 1999, Philips Lumileds introdujo ledes más potentes capaces de trabajar de forma continua a una potencia de un vatio. Estos ledes utilizaban semiconductores de troquelados mucho más grandes con el fin de aceptar potencias de alimentación mayores. Además, se montaban sobre varillas de metal para facilitar la eliminación de calor.

Una de las principales ventajas de las fuentes de iluminación a base de ledes es la alta eficiencia luminosa. Los ledes blancos igualaron enseguida e incluso superaron la eficiencia de los sistemas de iluminación incandescentes estándar. En 2002, Lumileds fabricó ledes de cinco vatios, con una eficiencia luminosa de 18-22 lúmenes por vatio (lm/W). A modo de comparación, una bombilla incandescente convencional de 60-100 vatios emite alrededor de 15lm/W, y las lámparas fluorescentes estándar emiten hasta 100lm/W.

A partir de 2012, Future Lighting Solutions había alcanzado las siguientes eficiencias para algunos colores. Los valores de la eficiencia muestran la potencia luminosa de salida por cada vatio de potencia eléctrica de entrada. Los valores de la eficiencia luminosa incluyen las características del ojo humano y se han deducido a partir de la función de luminosidad.

En septiembre de 2003, Cree Inc. fabricó un nuevo tipo de led azul que consumía 24milivatios (mW) a 20miliamperios (mA). Esto permitió un nuevo encapsulamiento de luz blanca que producía 65lm/W a 20 miliamperios, convirtiéndose en el led blanco más brillante disponible en el mercado; además resultaba ser más de cuatro veces más eficiente que las bombillas incandescentes estándar. En 2006 presentaron un prototipo de led blanco con una eficiencia luminosa récord de 131lm/W para una corriente de 20 miliamperios. Nichia Corporation ha desarrollado un led blanco con una eficiencia luminosa de 150lm/W y una corriente directa de 20mA. Los ledes de la empresa Cree Inc. denominados xlamp xm-L, salieron al mercado en 2011, produciendo 100lm/W a la potencia máxima de 10W, y hasta 160lm/W con una potencia eléctrica de entrada de unos 2W. En 2012, Cree Inc. presentó un led blanco capaz de producir 254lm/W, y 303lm/W en marzo de 2014. Las necesidades de iluminación general en la práctica requieren ledes de alta potencia, de un vatio o más. Funcionan con corrientes superiores a 350 miliamperios.

Estas eficiencias se refieren a la luz emitida por el diodo mantenido a baja temperatura en el laboratorio. Dado que los ledes, una vez instalados, operan a altas temperaturas y con pérdidas de conducción, la eficiencia en realidad es mucho menor. El Departamento de Energía de los Estados Unidos (DOE) ha realizado pruebas para sustituir las lámparas incandescentes o los LFC por las lámparas led, mostrando que la eficiencia media conseguida es de unos 46lm/W en 2009 (el comportamiento durante las pruebas se mantuvo en un margen de 17lm/W a 79lm/W).

Cuando la corriente eléctrica suministrada a un led sobrepasa unas decenas de miliamperios, disminuye la eficiencia luminosa a causa de un efecto denominado pérdida de eficiencia.

Al principio, se buscó una explicación atribuyéndolo a las altas temperaturas. Sin embargo, los científicos pudieron demostrar lo contrario, que si bien la vida del led puede acortarse, la caída de la eficiencia es menos severa a temperaturas elevadas. En 2007, la causa del descenso en la eficiencia se atribuyó a la recombinación Auger la cual da origen a una reacción mixta. Finalmente, un estudio de 2013 confirmó definitivamente esta teoría para justificar la pérdida de eficiencia.

Además de disminuir la eficiencia, los ledes que trabajan con corrientes eléctricas más altas generan más calor lo que compromete el tiempo de vida del led. A causa de este incremento de calor a corrientes altas, los ledes de alta luminosidad presentan un valor patrón industrial de tan solo 350 mA, corriente para la que existe un equilibrio entre luminosidad, eficiencia y durabilidad. 

Ante la necesidad de aumentar la luminosidad de los ledes, esta no se consigue a base de incrementar los niveles de corriente sino mediante el empleo de varios ledes en una sola lámpara. Por ello, resolver el problema de la pérdida de eficiencia de las lámparas led domésticas consiste en el empleo del menor número posible de ledes en cada lámpara, lo que contribuye a reducir significativamente los costes.

Miembros del Laboratorio de Investigación Naval de los Estados Unidos han encontrado una forma de disminuir la caída de la eficiencia. Descubrieron que dicha caída proviene de la recombinación Auger no radiativa producida con los portadores inyectados. Para resolverlo, crearon unos pozos cuánticos con un potencial de confinamiento suave para disminuir los procesos Auger no radiativos. 

Investigadores de la Universidad Central Nacional de Taiwán y de Epistar Corp están desarrollando un método para disminuir la pérdida de eficiencia mediante el uso de sustratos de cerámica de nitruro de aluminio, que presentan una conductividad térmica más alta que la del zafiro usado comercialmente. Los efectos de calentamiento se ven reducidos debido a la elevada conductividad térmica de los nuevos sustratos.

Los dispositivos de estado sólido tales como los ledes presentan una obsolescencia muy limitada si se opera a bajas corrientes y a bajas temperaturas. Los tiempos de vida son de 25000 a 100000 horas, pero la influencia del calor y de la corriente pueden aumentar o disminuir este tiempo de manera significativa.

El fallo más común de los ledes (y de los diodos láser) es la reducción gradual de la emisión de luz y la pérdida de eficiencia. Los primeros ledes rojos destacaron por su corta vida. Con el desarrollo de los ledes de alta potencia, los dispositivos están sometidos a temperaturas de unión más altas y densidades de corriente más elevadas que los dispositivos tradicionales. Esto provoca estrés en el material y puede causar una degradación temprana de la emisión de luz. Para clasificar cuantitativamente la vida útil de una manera estandarizada, se ha sugerido utilizar los parámetros L70 o L50, que representan los tiempos de vida (expresados en miles de horas) en los que un led determinado alcanza el 70% y el 50% de la emisión de luz inicial, respectivamente.

Así como en la mayoría de las fuentes de luz anteriores (lámparas incandescentes, lámparas de descarga, y aquellas que queman un combustible, por ejemplo las velas y las lámparas de aceite) la luz se generaba por un procedimiento térmico, los ledes solo funcionan correctamente si se mantienen suficientemente fríos. El fabricante específica normalmente una temperatura máxima de la unión entre 125 y 150°C, y las temperaturas inferiores son recomendables en interés de alcanzar una larga vida para los ledes. A estas temperaturas, se pierde relativamente poco calor por radiación, lo que significa que el haz de luz generado por un led se considera frío.

El calor residual en un led de alta potencia (que a partir de 2015 puede considerarse inferior a la mitad de la potencia eléctrica que consume) es transportado por conducción a través del sustrato y el encapsulamiento hasta un disipador de calor, que elimina el calor en el ambiente por convección. Es por tanto esencial realizar un diseño térmico cuidadoso, teniendo en cuenta las resistencias térmicas del encapsulamiento del led, el disipador de calor y la interfaz entre ambos. Los ledes de potencia media están diseñados normalmente para ser soldados directamente a una placa de circuito impreso que dispone de una capa de metal térmicamente conductora. Los ledes de alta potencia se encapsulan en paquetes cerámicos de gran superficie diseñados para ser conectados a un disipador de calor metálico, siendo la interfaz un material de una alta conductividad térmica (pasta térmica, material de cambio de fase, almohadilla térmica conductora o pegamento termofusible).

Si se instala una lámpara de ledes en un aparato luminoso sin ventilación, o el ambiente carece de una circulación de aire fresco, es probable que los ledes se sobrecalienten, lo que reduce su vida útil o, incluso, produzca el deterioro anticipado del aparato luminoso. El diseño térmico se suele proyectar para una temperatura ambiente de 25 °C (77 °F). Los ledes utilizados en las aplicaciones al aire libre, como las señales de tráfico o las luces de señalización en el pavimento, y en climas donde la temperatura dentro del aparato de iluminación es muy alta, pueden experimentar desde una reducción de la emisión luminosa hasta un fallo completo. 

Puesto que la eficiencia de los ledes es más alta a temperaturas bajas, esta tecnología es idónea para la iluminación de los congeladores de supermercado. Debido a que los ledes producen menos calor residual que las lámparas incandescentes, su uso en congeladores también puede ahorrar costes de refrigeración. Sin embargo, pueden ser más susceptibles a la helada y a la acumulación de escarcha que las lámparas incandescentes, por lo que algunos sistemas de iluminación led han sido dotados de un circuito de calefacción. Además, se han desarrollado las técnicas de los disipadores de calor de manera que pueden transferir el calor producido en la unión a las partes de los equipos de iluminación que puedan interesar.

Los ledes convencionales están fabricados a partir de una gran variedad de materiales semiconductores inorgánicos. En la siguiente tabla se muestran los colores disponibles con su margen de longitudes de onda, diferencias de potencial de trabajo y materiales empleados.

El primer led azul-violeta utilizaba cloro dopado con magnesio y lo desarrollaron Herb Maruska y Wally Rhines en la Universidad de Standford en 1972, estudiantes de doctorado en ciencia de materiales e ingeniería. En aquel entonces Maruska estaba trabajando en los laboratorios de RCA, donde colaboraba con Jacques Pankove. En 1971, un año después de que Maruska se fuera a Standford, sus compañeros de RCA Pankove y Ed Miller demostraron la primera electroluminiscencia azul procedente del zinc dopado con nitruro de galio; sin embargo el dispositivo que construyeron Pankove y Miller, el primer diodo emisor de luz de nitruro de galio real, emitía luz verde. En 1974 la Oficina de Patentes Estadounidense concedió a Maruska, Rhines y al profesor de Stanford David Stevenson una patente (patente US3819974 A de los EE. UU.) de su trabajo de 1972 sobre el dopaje de nitruro de galio con magnesio que hoy sigue siendo la base de todos los ledes azules comerciales y de los diodos láser. Estos dispositivos construidos en los 70 no tenían suficiente rendimiento luminoso para su uso práctico, por lo que la investigación de los diodos de nitruro de galio se ralentizó. En agosto de 1989 Cree introdujo el primer led azul comercial con una transición indirecta a través de la banda prohibida en un semiconductor de carburo de silicio (SiC). Los ledes de SiC tienen una eficiencia luminosa muy baja, no superior al 0,03%, pero emiten en la región del azul visible.

A finales de los 80, los grandes avances en crecimiento epitaxial y en dopaje tipo-p en GaN marcaron el comienzo de la era moderna de los dispositivos opto-electrónicos de GaN. Basado en lo anterior, Theodore Moustakas patentó un método de producción de ledes azules en la Universidad de Boston utilizando un novedoso proceso de dos pasos. Dos años más tarde, en 1993, los ledes azules de alta intensidad fueron retomados por Shuji Nakamura de la Nichia Corporation utilizando procesos de síntesis de GaN similares al de Moustakas. A Moustakas y a Nakamura se les asignaron patentes separadas, lo que generó conflictos legales entre Nichia y la Universidad de Boston (sobre todo porque, pese a que Moustakas inventó su proceso primero, Nakamura registró el suyo antes). Este nuevo desarrollo revolucionó la iluminación con ledes, rentabilizando la fabricación de las fuentes de luz azul de alta-potencia, conduciendo al desarrollo de tecnologías como el Blu-ray, y propiciando las pantallas brillantes de alta resolución de las tabletas y teléfonos modernos.

Nakamura fue laureado con el Premio de Tecnología del Milenio por su contribución a la tecnología de los ledes de alta potencia y su alto rendimiento. Además se le concedió, junto a Hiroshi Amano y Isamu Akasaki, el Premio Nobel de Física en 2014 por su decisiva contribución a los ledes de alto rendimiento y al led azul. En 2015 un juzgado estadounidense dictaminó que tres empresas (o sea las mismas compañías demandantes que no habían resuelto sus disputas previamente) y que disponían de las patentes de Nakamura para la producción en EE.UU., habían vulnerado la patente previa de Moustakas y les ordenó pagar unos derechos de licencia por un valor de 13 millones de dólares.

A finales de los 90 ya se disponía de los ledes azules. Estos presentan una región activa que consta de uno o más pozos cuánticos de InGaN intercalados entre láminas más gruesas de GaN, llamadas vainas. Variando la fracción de In/Ga en los pozos cuánticos de InGaN, la emisión de luz puede, en teoría, modificarse desde el violeta hasta el ámbar. El nitruro de aluminio y galio AlGaN con un contenido variable de la fracción de Al/Ga se puede usar para fabricar la vaina y las láminas de los pozos cuánticos para los diodos ultravioletas, pero estos dispositivos aún no han alcanzado el nivel de eficiencia ni la madurez tecnológica de los dispositivos de InGaN/GaN azul/verde. Si el GaN se usa sin dopar, para formar las capas activas de los pozos cuánticos el dispositivo emite luz próxima al ultravioleta con un pico centrado en una longitud de onda alrededor de los 365 nm. Los ledes verdes fabricados en la modalidad InGaN/GaN son mucho más eficientes y brillantes que los ledes producidos con sistemas sin nitruro, pero estos dispositivos todavía presentan una eficiencia demasiado baja para las aplicaciones de alto brillo.

Utilizando nitruros de aluminio, como AlGaN y AlGaInN, se consiguen longitudes de ondas aún más cortas. Una gama de ledes ultravioletas para diferentes longitudes de onda están empezando a encontrarse disponibles en el mercado. Los ledes emisores próximos al UV con longitudes de onda en torno a 375-395 nm ya resultan suficientemente baratos y se pueden encontrar con facilidad, por ejemplo para sustituir las lámparas de luz negra en la inspección de las marcas de agua anti-falsificación UV en algunos documentos y en papel moneda. Los diodos de longitudes de onda más cortas (hasta 240 nm), están actualmente en el mercado, aunque son notablemente más caros.

Como la fotosensibilidad de los microorganismos coincide aproximadamente con el espectro de absorción del ADN (con un pico en torno a los 260 nm) se espera utilizar los ledes UV con emisión en la región de 250-270 nm en los equipos de desinfección y esterilización. Investigaciones recientes han demostrado que los ledes UV disponibles en el mercado (365 nm) son eficaces en los dispositivos de desinfección y esterilización. Las longitudes de onda UV-C se obtuvieron en los laboratorios utilizando nitruro de aluminio (210nm), nitruro de boro (215nm) y diamante (235nm).

Los ledes RGB consisten en un led rojo, uno azul y otro verde. Ajustando independientemente cada uno de ellos, los ledes RGB son capaces de producir una amplia gama de colores. A diferencia de los ledes dedicados a un solo color, los ledes RGB no producen longitudes de ondas puras. Además, los módulos disponibles comercialmente no suelen estar optimizados para hacer mezclas suaves de color.

Los sistemas RGB
Existen dos formas básicas para producir luz blanca. Una consiste en utilizar ledes individuales que emitan los tres colores primarios (rojo, verde y azul) y luego mezclar los colores para formar la luz blanca. La otra forma consiste en utilizar un fósforo para convertir la luz monocromática de un led azul o UV en un amplio espectro de luz blanca. Es importante tener en cuenta que la blancura de la luz producida se diseña esencialmente para satisfacer al ojo humano y dependiendo de cada caso que no siempre puede ser apropiado pensar que se trata de luz estrictamente blanca. Sirva como punto de referencia la gran variedad de blancos que se consiguen con los tubos fluorescentes.

Hay tres métodos principales para producir luz blanca con los ledes.


Debido al metamerismo, es posible disponer de diferentes espectros que parezcan blancos. Sin embargo, la apariencia de los objetos iluminados por esa luz puede modificarse a medida que el espectro varía. Este fenómeno óptico se conoce como ejecución del color, es diferente a la temperatura del color, y que hace que un objeto realmente naranja o cian pueda parecer de otro color y mucho más oscuro como el led o el fósforo asociado no emiten esas longitudes de onda. La mejor reproducción de color con CFL y led se consigue utilizando una mezcla de fósforos, lo que proporciona una menor eficiencia pero una mejor calidad de luz. Aunque el halógeno con mayor temperatura de color es el naranja, sigue siendo la mejor fuente de luz artificial disponible en términos de ejecución de color.

La luz blanca se puede producir mediante la adición de luces de diferentes colores; el método más común es el uso de rojo, verde y azul (RGB). De ahí que el método se denomine ledes de blanco multicolor (a veces conocido como ledes RGB). Debido a que necesitan circuitos electrónicos para controlar la mezcla y la difusión de los diferentes colores, y porque los ledes de color individuales presentan patrones de emisión ligeramente diferentes (lo que conduce a la variación del color en función de la dirección de observación), incluso si se fabrican en una sola unidad, rara vez se utilizan para producir luz blanca. Sin embargo, este método tiene muchas aplicaciones por la flexibilidad que presenta para producir la mezcla de colores y, en principio, por ofrecer una mayor eficiencia cuántica en la producción de luz blanca.

Hay varios tipos de ledes blancos multicolor: ledes blancos di- , tri- y tetracromático. Varios factores clave influyen en estas diferentes realizaciones, como son la estabilidad del color, el índice de reproducción del color natural y la eficiencia luminosa. Con frecuencia, una mayor eficiencia luminosa implicará una menor naturalidad del color, surgiendo así una compensación entre la eficiencia luminosa y la naturalidad de los colores. Por ejemplo, los ledes blancos dicromáticos presentan la mejor eficiencia luminosa (120 lm / W), pero la capacidad de representación cromática más baja. Por otro lado, los ledes blancos tetracromáticos ofrecen una excelente capacidad de representación de color pero a menudo se acompañan de una pobre eficiencia luminosa. Los ledes blancos tricromáticos se encuentran en una posición intermedia, poseen una buena eficiencia luminosa (> 70 lm / W) y una razonable capacidad para la reproducción de color.

Uno de los desafíos pendientes de resolver consiste en el desarrollo de ledes verdes más eficientes. El máximo teórico para los ledes verdes es de 683 lúmenes por vatio, pero a partir de 2010 tan solo unos pocos ledes verdes superaron los 100 lúmenes por vatio. Los ledes azul y rojo, sin embargo, se están acercando a sus límites teóricos.

Los ledes multicolores ofrecen la posibilidad no solo de producir luz blanca sino también de generar luces de diferentes colores. La mayoría de los colores perceptibles se pueden formar mezclando diferentes proporciones de los tres colores primarios. Esto permite un control dinámico preciso del color. A medida que se dedica más esfuerzo en investigación el método de los ledes multicolor presenta una mayor influencia como método fundamental utilizado para producir y controlar el color de la luz.

Si bien este tipo de ledes puede jugar un buen papel en el mercado, antes hay que resolver algunos problemas técnicos. Por ejemplo, la potencia de emisión de estos ledes disminuye exponencialmente al aumentar la temperatura, produciendo un cambio sustancial de la estabilidad del color. Estos problemas pueden imposibilitar su empleo en la industria. Por ello, se han efectuado muchos diseños nuevos de encapsulamientos y sus resultados se encuentran en fase de estudio por los investigadores. Evidentemente, los ledes multicolores sin fósforos nunca pueden proporcionar una buena iluminación debido a que cada uno de ellos emite una banda muy estrecha de color. Así como los ledes sin fósforos constituyen una solución muy pobre para iluminación, ofrecen la mejor solución para pantallas de iluminación de fondo para LCD o de iluminación directa con pixeles de ledes.

En la tecnología Led, la disminución de la temperatura de color correlacionada (CCT) es una realidad difícil de evitar debido a que, junto con la vida útil y los efectos de la variación de la temperatura de los ledes, se acaba modificando el color real definitivo de los mismos. Para corregirlo, se utilizan sistemas con bucle de realimentación provistos, por ejemplo, de sensores de color y así supervisar, controlar y mantener el color resultante de la superposición de los ledes monocolor.

Este método implica el recubrimiento de los ledes de un color (principalmente ledes azules de InGaN) con fósforos de diferentes colores para producir luz blanca; los ledes resultantes de la combinación se llaman ledes blancos basados en fósforos o ledes blancos con un convertidor de fósforo (PCLED). Una fracción de la luz azul experimenta el desplazamiento de Stokes que transforma las longitudes de onda más cortas en longitudes de onda más largas. Dependiendo del color del led original, se pueden emplear fósforos de diversos colores. Si se aplican varias capas de fósforos de colores distintos se ensancha el espectro de emisión, incrementándose efectivamente el valor del índice de reproducción cromática (IRC) de un led dado.

Las pérdidas de eficiencia de los ledes basados en fósforos (con sustancias fluorescentes) se deben a las pérdidas de calor generadas por el desplazamiento de Stokes y también a otros problemas de degradación relacionados con dichas sustancias fluorescentes. En comparación con los ledes normales sus eficiencias luminosas dependen de la distribución espectral de la salida de luz resultante y de la longitud de onda original del propio led. Por ejemplo, la eficiencia luminosa de un fósforo amarillo YAG típico de un led blanco de 3 a 5 veces la eficiencia luminosa del led azul original, debido a la mayor sensibilidad del ojo humano para el color amarillo que para el color azul (según el modelo de la función de luminosidad). Debido a la simplicidad de su fabricación, el método de fósforo (material fluorescente) sigue siendo el más popular para conseguir una alta intensidad en los ledes blancos. El diseño y la producción de una fuente de luz o lámpara utilizando un emisor monocromático con la conversión de fósforo fluorescente es más simple y más barato que un sistema complejo RGB, y la mayoría de los ledes blancos de alta intensidad existentes actualmente en el mercado se fabrican utilizando la conversión de la luz mediante fluorescencia.

Entre los retos que surgen para mejorar la eficiencia de las fuentes de luz blanca a base de ledes se encuentra el desarrollo de sustancias fluorescentes (fósforos) más eficientes. A partir de 2010, el fósforo amarillo más eficiente continúa siendo el fósforo YAG, que presenta una pérdida por el desplazamiento de Stokes inferior al 10%. Las pérdidas ópticas internas debidas a la reabsorción en el propio chip del led y en el encapsulamiento del led constituyen del 10% al 30% de la pérdida de eficiencia. Actualmente, en el ámbito del desarrollo con fósforo, se dedica un gran esfuerzo en su optimización con el fin de conseguir una mayor producción de luz y unas temperaturas de operación más elevadas. Por ejemplo, la eficiencia se puede aumentar con un mejor diseño del encapsulamiento o mediante el uso de un del tipo más adecuado de fósforo. El proceso de revestimiento de ajuste se suele utilizar con el fin de poder regular el espesor variable del fósforo.

Algunos ledes blancos dotados de fósforos consisten en ledes azules de InGaN encapsulados en una resina epoxi recubierta por un fósforo. Otra opción consiste en asociar el led con un fósforo separado, una pieza prefabricada de policarbonato preformado y revestida con el material del fósforo. Los fósforos separados proporcionan una luz más difusa, lo cual es favorable para muchas aplicaciones. Los diseños con fósforos separados son también más tolerantes con las variaciones del espectro de emisión del led. Un material de fósforo amarillo muy común es el aluminio granate de itrio y aluminio dopado con cerio (Ce 3+ :YAG).

Los ledes blancos también se pueden fabricar con ledes del ultravioleta próximo (NUV) recubiertos con una mezcla de fósforos de europio de alta eficiencia que emiten rojo y azul, más sulfuro de zinc dopado con cobre y aluminio ( ZnS:Cu, Al ) que emite verde. Este procedimiento es análogo al de funcionamiento de las lámparas fluorescentes. El procedimiento es menos eficiente que el de los ledes de color azul con fósforo YAG:Ce, puesto que el desplazamiento de Stokes es más importante, por lo que una mayor fracción de la energía se convierte en calor, aun así se genera una luz con mejores características espectrales y, por tanto, con una mejor reproducción de color. 

Dado que los ledes ultravioleta presentan una mayor radiación de salida que los azules, ambos métodos ofrecen, en definitiva, un brillo similar. Un inconveniente de los últimos es que una posible fuga de la luz UV procedente de una fuente luminosa que funcione incorrectamente puede causar daño a los ojos o a la piel humana.

Otro método utilizado para producir ledes experimentales de luz blanca sin el empleo de fósforos se basa en la epitaxia de crecimiento del seleniuro de zinc (ZnSe) sobre un substrato de ZnSe que de forma simultánea emite luz azul procedente de su región activa y luz amarilla procedente del sustrato.

Una nueva forma para producir ledes blancos consiste en utilizar obleas compuestas de nitruro de galio sobre silicio a partir de obleas de silicio de 200 mm. Esto evita la costosa fabricación de sustratos de zafiro a partir de obleas de tamaños relativamente pequeños, o sea de 100 o 150mm. El aparato de zafiro debe estar acoplado a un colector similar a un espejo para reflejar la luz, que de otro modo se perdería. Se predice que para 2020 el 40% de todos los ledes de GaN se fabricarán sobre silicio. La fabricación de zafiro de gran tamaño es difícil, mientras que el material de silicio grande es barato y más abundante. Por otro lado, los fabricantes de ledes que cambien del zafiro al silicio deben de hacer una inversión mínima.

En un diodo emisor de luz orgánico (OLED), el material electroluminiscente que constituye la capa emisora del diodo es un compuesto orgánico. El material orgánico es conductor debido a deslocalización electrónica de los enlaces pi causados por el sistema conjugado en toda o en parte de la molécula; en consecuencia el material funciona como un semiconductor orgánico. Los materiales orgánicos pueden ser pequeñas moléculas orgánicas en fase cristalina, o polímeros.

Una de las ventajas que posibilitan los OLED son las pantallas delgadas y de bajo costo con una tensión de alimentación baja, un amplio ángulo de visión, un alto contraste y una extensa gama de colores. Los ledes de polímero presentan la ventaja añadida de propiciar las pantallas imprimibles y flexibles. Los OLED se han utilizado en la fabricación de pantallas visuales para las dispositivos electrónicos portátiles, como son los teléfonos móviles, las cámaras digitales y los reproductores de MP3, y se considera que los posibles usos futuros también inlcuirán la iluminación y la televisión.

A inicio de los años 60 comenzó una década de revolución tecnológica con el nacimiento de Internet y el descubrimiento del led en el espectro visible. En 1959 el premio nobel de física Richard P. Feynman, en su célebre conferencia dada en la reunión anual de la Asociación Física de los Estados Unidos titulada: "Hay mucho espacio en el fondo: una invitación para entrar en un nuevo campo de la física", ya adelantaba la revolución tecnológica y los importantes descubrimientos que podían suponer la manipulación de los materiales hasta reducirlos a tamaños o escalas atómicas o moleculares. Pero no es hasta la década siguiente de 1970 que el conocimiento de numerosas aplicaciones de la mecánica cuántica (a unos 70 años de su invención) unido al avance de las técnicas de crecimiento y síntesis de materiales, llegan a suponer un cambio importante en las líneas de investigación de numerosos grupos. 

Ya en esta década se unía la capacidad de diseñar estructuras teniendo nuevas propiedades ópticas y electrónicas a la búsqueda de nuevas aplicaciones tecnológicas a los materiales ya existentes en la naturaleza. De hecho, en 1969, L. Esaki et al. propusieron la implementación de heteroestructuras formadas por capas muy delgadas de distintos materiales, dando lugar a lo que se conoce como ingeniería y diseño de bandas de energía en materiales semiconductores. La heteroestructura de pequeñas dimensiones más básica es el pozo cuántico (Quantum Well, QW). Consiste en una capa delgada de un determinado semiconductor, del orden de 100Å, confinada entre dos capas de otro material semiconductor caracterizado por una mayor anchura de la banda de energía prohibida (bandgap, BG). Debido a las pequeñas dimensiones del pozo de potencial asociado a esta estructura, los portadores ven restringido su movimiento a un plano perpendicular a la dirección de crecimiento. Los diodos láser con QWs en la zona activa suponían grandes ventajas, como por ejemplo la capacidad de seleccionar la longitud de onda de emisión en función de la anchura del pozo o la disminución de la corriente umbral, esto último relacionado con la densidad de estados resultado del confinamiento en un plano. 

A todos estos avances se fueron sucediendo de manera natural otros como el estudio de los sistemas con confinamiento en tres dimensiones, es decir los puntos cuánticos (QDs). Así, los QDs se pueden definir como sistemas artificiales de tamaño muy pequeño, desde algunas decenas de nanómetros a algunas micras en los que los portadores se encuentran confinados en las tres direcciones del espacio tridimensional (por eso se llama cero-dimensional), en una región del espacio más pequeña que su longitud de onda de Broglie. 

Cuando el tamaño del material semiconductor que constituye el punto cuántico se encuentra dentro de la escala nanométrica, este material presenta un comportamiento que difiere del observado para el mismo a escala macroscópica o para los átomos individuales que los conforman. Los electrones en el nanomaterial se encuentran restringidos a moverse en una región muy pequeña del espacio y se dice que están confinados. Cuando esta región es tan pequeña que es comparable a la longitud de onda asociada al electrón (la longitud de De Broglie), entonces comienza a observarse lo que se denomina comportamiento cuántico. En estos sistemas, sus propiedades físicas no se explican con conceptos clásicos sino que se explican mediante los conceptos de la mecánica cuántica. Por ejemplo, la energía potencial mínima de un electrón confinado dentro de una nanoparticula es mayor que la esperada en física clásica y los niveles de energía de sus diferentes estados electrónicos son discretos. Debido al confinamiento cuántico, el tamaño de la partícula tiene un efecto fundamental sobre la densidad de estados electrónicos y por ello, sobre su respuesta óptica. El confinamiento cuántico se produce cuando el tamaño de las partículas se ha reducido hasta aproximarse al radio del excitón de Bohr (generándose en el material semiconductor un par electrón-hueco o excitón) quedando confinado en un espacio muy reducido. Como consecuencia, la estructura de los niveles energéticos y las propiedades ópticas y eléctricas del material se modifican considerablemente. Los niveles de energía pasan a ser discretos y finitos, y dependen fuertemente del tamaño de la nanopartícula. 

Usualmente están fabricados con material semiconductor y pueden albergar desde ninguno a varios miles de electrones. Los electrones que están dentro del punto cuántico se repelen, cuesta energía introducir electrones adicionales, y obedecen el principio de exclusión de Pauli, que prohíbe que dos electrones ocupen el mismo estado cuántico simultáneamente. En consecuencia, los electrones en un punto cuántico forman órbitas de una manera muy similar a las de los átomos y en algunos casos se los denomina átomos artificiales. También presentan comportamientos electrónicos y ópticos similares a los átomos. Su aplicación puede resultar muy diversa, además de en optoelectrónica y óptica, en la computación cuántica, en el almacenamiento de información para computadoras tradicionales, en biología y en medicina.

Las propiedades ópticas y de confinamiento cuántico del punto cuántico permiten que su color de emisión se pueda ajustar desde el visible al infrarrojo. Los ledes de puntos cuánticos pueden producir casi todos los colores del diagramaCIE. Además, proporcionan más opciones de color y una mejor representación del mismo que los ledes blancos comentados en las secciones anteriores, ya que el espectro de emisión es mucho más estrecho, lo que es característico de los estados cuánticos confinados.

Existen dos procedimientos para la excitación de los QD. Uno utiliza la fotoexcitación con una fuente de luz primaria de led (para ello se utilizan habitualmente los ledes azules o UV). El otro procedimiento utiliza la excitación eléctrica directa demostrada por primera vez por Alivisatos et al.

Un ejemplo del procedimiento de fotoexcitación es el desarrollado por Michael Bowers, en la Universidad Vanderbilt de Nashville, realizando un prototipo que consistía en el recubrimiento de un led azul con puntos cuánticos que emitían luz blanca en respuesta a la azul del led. El led modificado emitía una luz cálida de color blanco amarillento similar a la de las lámparas incandescentes. En 2009 se iniciaron investigaciones con los diodos emisores de luz utilizando QD en aplicaciones a los televisores con pantalla de cristal líquido (LCD).

En febrero de 2011 científicos del PlasmaChem GmbH fueron capaces de sintetizar puntos cuánticos para las aplicaciones de los ledes realizando un convertidor de luz que conseguía transformar eficazmente la luz azul en luz de cualquier otro color durante muchos cientos de horas. Estos puntos cuánticos pueden también ser utilizados para emitir luz visible o cercana al infrarrojo al excitarlos con luz de una longitud de onda menor.

La estructura de los ledes de puntos cuánticos (QD-LED) utilizados para la excitación eléctrica del material, poseen un diseño básico similar al de los OLED. Una capa de puntos cuánticos se encuentra situada entre dos capas de un material capaz de transportar electrones y huecos. Al aplicar un campo eléctrico, los electrones y los huecos se mueven hacia la capa de puntos cuánticos y se recombinan formando excitones; cada excitón produce un par electrón-hueco, emitiendo luz. Este esquema es el habitualmente considerado para las pantallas de puntos cuánticos. La gran diferencia con los OLED reside en su tamaño de dimensiones muy pequeñas y como consecuencia, generan los efectos y propiedades ópticas del confinamiento cuántico.

Los QD resultan también muy útiles como fuentes de excitación para producir imágenes por fluorescencia debido al estrecho margen de longitudes de onda emitidas por el QD que se manifiesta en el estrecho ancho de banda del pico en el espectro de emisión (propiedad debida al confinamiento cuántico). Por ello se ha mostrado eficiente el uso de ledes de puntos cuánticos (QD-LED) en la técnica de microscopía óptica de campo cercano. 

En cuanto a la eficiencia energética, en febrero de 2008 se consiguió una emisión de luz cálida con una eficiencia luminosa de 300 lúmenes de luz visible por cada vatio de radiación (no por vatio eléctrico) mediante el uso de nanocristales.

Los ledes se fabrican en una gran variedad de formas y tamaños. El color de la lente de plástico suele coincidir con el de la luz emitida por el led aunque no siempre es así. Por ejemplo, el plástico de color púrpura se emplea para los ledes infrarrojos y la mayoría de los ledes azules presentan encapsulamientos incoloros. Los ledes modernos de alta potencia como los empleados para iluminación directa o para retroiluminación aparecen normalmente en montajes de tecnología de superficie (SMT).

Los ledes miniatura se suelen usar como indicadores. En la tecnología de agujeros pasantes y en los montajes superficiales su tamaño varía desde 2 mm a 8 mm. Normalmente no disponen de un disipador de calor independiente. La corriente máxima se sitúa entre 1mA y 20mA. Su pequeño tamaño constituye una limitación a efectos de la potencia consumida debido a su alta densidad de potencia y a la ausencia de un disipador. A menudo se conectan en cadena margarita para formar tiras de luz led.

Las formas de la cubierta de plástico más típicas son redonda, plana, triangular y cuadrada con la parte de arriba plana. El encapsulamiento también puede ser transparente o coloreado para poder mejorar el contraste y los ángulos de visión.

Investigadores de la Universidad de Washington han inventado el led más delgado. Está formado por materiales de dos dimensiones (2-D). Su anchura son 3 átomos, o sea entre 10 y 20 veces más fino que los ledes tridimensionales (3-D) y 10000 veces más delgado que un pelo humano. Estos ledes 2-D permitirán las comunicaciones ópticas y los nano láseres más pequeños y más eficientes en energía.

Hay tres categorías principales de ledes miniatura de un único color:


Preparados para una corriente de 2mA con unos 2V (consumo de más o menos 4 mW).


Para una corriente de 20mA y con 2 o 4-5V, diseñadas para poder ver con luz solar directa. Los ledes de 5V y 12V son ledes miniatura normales que incorporan un resistencia en serie para la conexión directa a una alimentación de 5 o 12V.

"Ver también: Iluminación de estado sólido, Lámpara led, ledes de Alta Potencia o HP-LED"

Los ledes de alta potencia HP-LED (High-power LED) o de alta emisión HO-LED (del inglés High-Output LED) pueden controlarse con corrientes desde cientos de mA hasta de más de 1 Amperio, mientras que otros ledes solo llegan a las decenas de miliAmperios. Algunos pueden emitir más de mil lúmenes. 

También se han alcanzado densidades de potencia de hasta 300 W/(cm). Como el sobrecalentamiento de los ledes puede destruirlos, se tienen que montar sobre un disipador. Si el calor de un HP-LED no se transfiriera al medio, el aparato fallaría en unos pocos segundos. Un HP-LED puede sustituir a una bombilla incandescente en una linterna o varios de ellos pueden asociarse para constituir una lámpara led de potencia. Algunos HP-LED bien conocidos en esta categoría son los de la serie Nichia 19, Lumileds Rebel Led, Osram Opto Semiconductors Golden Dragon y Cree X-Lamp. Desde septiembre de 2009, existen ledes manufacturados por Cree que superan los 105lm/W.

Ejemplos de la ley de Haitz, que predice un aumento exponencial con el tiempo de la emisión luminosa y de la eficiencia de un led, son los de la serie CREE XP-GE que alcanzó en 2009 los 105lm/W y la serie Nichia 19 con una eficiencia media de 140lm/W que fue lanzado en 2010.

Semiconductor Seúl ha desarrollado ledes que puede funcionar con corriente alterna sin necesidad de un conversor DC. En un semiciclo, una parte del led emite luz y la otra parte es oscura, y esto sucede al contrario durante el siguiente semiciclo. La eficiencia normal de este tipo de HP-LED es 40lm/W. Un gran número de elementos led en serie pueden funcionar directamente con la tensión de la red. En 2009, Semiconductor Seúl lanzó un led de alto voltaje, llamado 'Acrich MJT', capaz de ser gobernado por AC mediante un simple circuito de control. La baja potencia disipada por estos ledes les proporciona una mayor flexibilidad que a otros diseños originales de ledes AC. 

Los ledes intermitentes se utilizan como indicadores de atención sin necesidad de ningún tipo de electrónica externa. Los ledes intermitentes se parecen a los ledes estándar, pero contienen un circuito multivibrador integrado que hace que los ledes parpadeen con un período característico de un segundo. En los ledes provistos de lente de difusión, este circuito es visible (un pequeño punto negro). La mayoría de los ledes intermitentes emiten luz de un solo color, pero los dispositivos más sofisticados pueden parpadear con varios colores e incluso desvanecerse mediante una secuencia de colores a partir de la mezcla de colores RGB.

Los ledes bicolor contienen dos ledes diferentes en un solo conjunto. Los hay de dos tipos; el primero consiste en dos troqueles conectados a dos conductores paralelos entre sí con la circulación de la corriente en sentidos opuestos. Con el flujo de corriente en un sentido se emite un color y con la corriente en el sentido opuesto se emite el otro color. En el segundo tipo, en cambio, los dos troqueles tienen los terminales separados y existe un terminal para cada cátodo o para cada ánodo, de modo que pueden ser controlados independientemente. La combinación de colores más común es la de rojo / verde tradicional, sin embargo, existen otras combinaciones disponibles como el verde tradicional/ámbar, el rojo/verde puro, el rojo/azul o el azul/verde puro.

Los ledes tricolores contienen tres ledes emisores diferentes en un solo bastidor. Cada emisor está conectado a un terminal separado para que pueda ser controlado independientemente de los otros. Es muy característica una disposición en la que aparecen cuatro terminales, un terminal común (los tres ánodos o los tres cátodos unidos) más un terminal adicional para cada color.

Los ledes RGB son ledes tricolor con emisores rojo, verde y azul, que usan generalmente una conexión de cuatro hilos y un terminal común (ánodo o cátodo). Este tipo de ledes pueden presentar como común tanto el terminal positivo como el terminal negativo. Otros modelos, sin embargo, solo tienen dos terminales (positivo y negativo) y una pequeña unidad de control electrónico incorporada.

Este tipo de ledes posee emisores de diferentes colores y están dotados de dos únicos terminales de salida. Los colores se conmutan internamente variando la tensión de alimentación.

Los ledes alfanuméricos están disponibles como visualizadores de siete segmentos, como visualizadores de catorce segmentos o como pantallas de matrices de puntos. Los visualizadores (displays) de siete segmentos pueden representar todos los números y un conjunto limitado de letras mientras que los de catorce segmentos pueden visualizar todas las letras. Las pantallas de matrices de puntos usan habitualmente 5x7 píxeles por carácter. El uso de los ledes de siete segmentos se generalizó en la década de 1970 y 1980 pero el uso creciente de las pantallas cristal líquido ha reducido la popularidad de los ledes numéricos y alfanuméricos por su menor requerimiento de potencia y mayor flexibilidad para la visualización.

Son ledes RGB que contienen su propia electrónica de control "inteligente". Además de la alimentación y la conexión a tierra, disponen de conexiones para la entrada y la salida de datos y, a veces, para señales de reloj o estroboscópicas. Se encuentran conectados en una cadena margarita, con la entrada de datos al primer led dotada de un microprocesador que puede controlar el brillo y el color de cada uno de ellos, independientemente de los demás. Se usan donde sea necesaria una combinación que proporcione un control máximo y una vista mínima de la electrónica, como sucede en las cadenas luminosas navideñas o en las matrices de led. Algunos incluso presentan tasas de refresco en el margen de los kHz, lo que los hace aptos para aplicaciones básicas de vídeo.

Un filamento led consta de varios chips led conectados en serie sobre un sustrato longitudinal formando una barra delgada que recuerda al filamento incandescente de una bombilla tradicional. Los filamentos se están utilizando como una alternativa decorativa de bajo coste a las bombillas tradicionales que están siendo eliminadas en muchos países. Los filamentos requieren una tensión de alimentación bastante alta para iluminar con un brillo normal, pudiendo trabajar de manera eficiente y sencilla a las tensiones de la red. Con frecuencia un simple rectificador y un limitador capacitivo de corriente se emplean como una sustitución de bajo coste de la bombilla incandescente tradicional sin el inconveniente de tener que construir un convertidor de baja tensión y corriente elevada, tal como lo requieren los diodos led individuales. Normalmente se montan en el interior de un recinto hermético al que se le da una forma similar a la de las lámparas que sustituyen (en forma de bombilla, por ejemplo) y se rellenan con un gas inerte como nitrógeno o dióxido de carbono para eliminar el calor de forma eficiente. Los principales tipos de ledes son: miniatura, dispositivos de alta potencia y diseños habituales como los alfanuméricos o los multicolor.

"Artículo principal: Circuito con led" 

La curva característica corriente-tensión de un led es similar a la de otros diodos, en los que la intensidad de corriente (o brevemente, corriente) crece exponencialmente con la tensión (ver la ecuación de Shockley). Esto significa que un pequeño cambio en la tensión puede provocar un gran cambio en la corriente. Si la tensión aplicada sobrepasa la caída de la tensión umbral en polarización directa del led, en una pequeña cantidad, el límite de corriente que el diodo puede soportar puede superarse ampliamente, pudiendo dañar o destruir el led. La solución que se puede adoptar para evitarlo consiste en utilizar fuentes de alimentación de intensidad de corriente constante (brevemente, fuente de corriente constante) capaces de mantener la corriente por debajo del valor máximo de la corriente que puede atravesar el led o, por lo menos, si se usa una fuente de tensión constante convencional o batería, añadir en el circuito de iluminación del Led una resistencia limitadora en serie con el Led. Dado que las fuentes normales de alimentación (baterías, red eléctrica) son normalmente fuentes de tensión constante, la mayoría de los aparatos led deben incluir un convertidor de potencia o, al menos, una resistencia limitadora de corriente. Sin embargo, la alta resistencia de las pilas de botón de tres voltios combinada con la alta resistencia diferencial de los ledes derivados de nitruros hace posible alimentar tales ledes con una pila de botón sin necesidad de incorporar una resistencia externa.

"Artículo principal: Polaridad eléctrica de los Ledes"

Al igual que sucede con todos los diodos, la corriente fluye fácilmente del material de tipo p al material de tipo n. Sin embargo, si se aplica un voltaje pequeño en el sentido inverso la corriente no fluye y no se emite luz. Si el voltaje inverso crece lo suficiente como para exceder la tensión de ruptura, fluye una corriente elevada y el led puede quedar dañado. Si la corriente inversa está lo suficientemente limitada como para evitar daños, el led de conducción inversa puede ser utilizado como un diodo avalancha.

La inmensa mayoría de los dispositivos que contienen ledes son "seguros en condiciones de uso normal", y por lo tanto se clasifican como "Producto de riesgo 1 RG1 (riesgo bajo)" / "LED Class 1". En la actualidad, solo unos pocos ledes -los ledes extremadamente luminosos que presentan un ángulo de visión muy pequeño de una apertura de 8° o menos- podrían, en teoría, causar una ceguera temporal y, por lo tanto, se clasifican como de "Riesgo 2 RG2 (riesgo moderado)". La opinión de la Agencia Francesa de Seguridad Alimentaria, Medioambiental y de Salud y Seguridad Ocupacional (ANSES) al abordar en 2010 las cuestiones sanitarias relacionadas con los ledes, sugirió prohibir el uso público de las lámparas que se encontraban en el Grupo 2 o de Riesgo Moderado, especialmente aquellas con un alto componente azul, en los lugares frecuentados por los niños.

En general, los reglamentos de seguridad en la utilización de la luz laser —y los dispositivos de Riesgo 1, Riesgo 2, etc.— son también aplicables a los ledes.

Así como los ledes presentan la ventaja, sobre las lámparas fluorescentes, de que no contienen mercurio, sin embargo, pueden contener otros metales peligrosos tales como plomo y arsénico. En cuanto a la toxicidad de los ledes cuando se tratan como residuos, un estudio publicado en 2011 declaró: "De acuerdo con las normas federales, los ledes no son peligrosos, excepto los ledes rojos de baja intensidad, ya que al principio de su comercialización contenían Pb (plomo) en concentraciones superiores a los límites reglamentarios (186 mg/L; límite reglamentario: 5). Sin embargo, de acuerdo con las reglamentaciones de California, los niveles excesivos de cobre (hasta 3892 mg/kg; límite: 2500), plomo (hasta 8103 mg/kg, límite: 1000), níquel (hasta 4797 mg/kg, límite: 2000), o plata (hasta 721 mg/kg, límite: 500) ocasionan que todos los ledes, excepto los amarillos de baja intensidad, sean peligrosos ".

El bajo consumo de energía, la poca necesidad de mantenimiento y el tamaño pequeño de los ledes ha propiciado su uso como indicadores de estado y visualización en una gran variedad de equipos e instalaciones. Las pantallas led de gran superficie se utilizan para retransmitir el juego en los estadios, como pantallas decorativas dinámicas y como señales de mensajes dinámicos en las autopistas. Las pantallas ligeras y delgadas con mensajes se utilizan en los aeropuertos y estaciones de ferrocarril y como en los trenes, autobuses, tranvías y transbordadores.

Las luces de un solo color son adecuadas para los semáforos, las señales de tráfico, los letreros de salida, la iluminación de emergencia de los vehículos, las luces de navegación, los faros (los índices estándar de cromaticidad y de luminancia fueron establecidos en el Convenio Internacional de Prevención de Colisiones en el Mar de 1972 Anexo 1 y por la Comisión Internacional de Iluminación o CIE) y las luces de Navidad compuestas de ledes. En regiones de climas fríos, los semáforos led pueden permanecer cubiertos de nieve. Se usan ledes rojos o amarillos en indicadores y pantallas alfanuméricas, en ambientes donde se debe mantener una visión nocturna: cabinas de aviones, puentes submarinos y de buques, observatorios astronómicos y en el campo por ejemplo para la observación de animales durante la noche y aplicaciones militares del campo.

Dada su larga vida útil, sus tiempos de conmutación rápidos y su capacidad para ser vistos a plena luz del día debido a su alta intensidad y concentración, desde hace algún tiempo se vienen utilizado ledes para las luces de freno de automóviles, camiones y autobuses, y en las señales de cambio de dirección; muchos vehículos usan actualmente los ledes en sus conjuntos de luminosas traseras. El uso en los frenos mejora la seguridad debido a la gran reducción en el tiempo requerido para un encendido completo, es decir por el hecho de presentar un tiempo de subida más corto, hasta 0.5 segundos más rápido que una bombilla incandescente. Esto proporciona más tiempo de reacción para los conductores de atrás. En un circuito de dos intensidades (luces de posición traseras y frenos) si los ledes no son accionados con una frecuencia suficientemente rápida, pueden crear una matriz fantasma, donde las imágenes fantasma del led aparecerán si los ojos se desplazan rápidamente por la disposición de luces. Los faros provistos de ledes blancos están empezando a utilizarse. El uso de los ledes tiene ventajas de estilo porque pueden formar haces de luz mucho más delgados que las lámparas incandescentes provistas de reflectores parabólicos.

Los ledes de baja potencia resultan relativamente muy económicos y permiten su utilización en objetos luminosos de vida corta como son los autoadhesivos luminosos, los objetos de usar y tirar y el tejido fotónico Lumalive. Los artistas también usan los ledes para el llamado arte led. Los receptores de radio meteorológicos y de socorro con mensajes de área codificados (SAME) disponen de tres ledes: rojo para alarmas, naranja para atención y amarillo para avisos, indicaciones e informes.

Para alentar el cambio a las lámparas de ledes, el Departamento de Energía de los Estados Unidos ha creado el premio L. La bombilla led Philips Lighting North America ganó el primer premio el 3 de agosto de 2011 después de completar con éxito 18 meses de pruebas intensivas de campo, laboratorio y producto.

Los ledes se utilizan como luces de la calle y en iluminación arquitectónica. La robustez mecánica y la vida útil larga se utilizan en la iluminación automotriz en los coches, las motocicletas y las luces de la bicicleta. La emisión de luz led puede controlarse eficazmente mediante el uso de principios ópticos de no imagen.

En 2007, el pueblo italiano de Torraca fue el primer lugar en convertir todo su sistema de iluminación en led. Los ledes se utilizan también en la aviación, Airbus ha utilizado la iluminación led en su Airbus A320 desde 2007, y Boeing utiliza la iluminación led en el 787. Los ledes también se utilizan ahora en el aeropuerto y la iluminación del helipuerto. Los aparatos de aeropuerto de ledes incluyen actualmente luces de pista de media intensidad, luces en la línea central de la pista, en la línea central de la calle de rodaje y luces en el borde.

Los ledes también se utilizan como fuente de luz para proyectores DLP y para iluminar los televisores LCD (conocidos como televisores led) y las pantallas para ordenadores portátiles. Los ledes RGB elevan la gama de colores hasta en un 45%. Las pantallas para TV y pantallas de ordenador pueden ser más delgadas usando ledes para retroiluminación . La falta de radiación infrarroja o térmica hace que los ledes sean ideales para luces de escenario con bancos de ledes RGB que pueden cambiar fácilmente de color y disminuir el calentamiento de la iluminación, así como la iluminación médica donde la radiación IR puede ser dañina. En la conservación de la energía, hay una menor producción de calor al utilizar ledes.

Además son pequeños, duraderos y necesitan poca potencia, por lo que se utilizan en dispositivos portátiles como linternas. Las luces estroboscópicas led o los flashes de la cámara funcionan a una tensión segura y baja, en lugar de los 250+ voltios que se encuentran comúnmente en la iluminación basada en flash de xenón. Esto es especialmente útil en las cámaras de teléfonos móviles. Los ledes se utilizan para la iluminación infrarroja en los usos de la visión nocturna incluyendo cámaras de seguridad. Un anillo de ledes alrededor de una cámara de vídeo dirigido hacia adelante en un fondo retrorreflectante, permite la codificación de croma en producciones de video.

Los ledes se utilizan en las operaciones mineras, como lámparas de tapa para proporcionar luz a los mineros. Se han realizado investigaciones para mejorar los ledes de la minería, reducir el deslumbramiento y aumentar la iluminación, reduciendo el riesgo de lesiones a los mineros. 

Los ledes se usan ahora comúnmente en todas las áreas de mercado, desde el uso comercial hasta el uso doméstico: iluminación estándar, teatral, arquitectónico , instalaciones públicas, y donde se utilice luz artificial.

Los ledes están encontrando cada vez más usos en aplicaciones médicas y educativas, por ejemplo como mejora del estado de ánimo, y nuevas tecnologías tales como AmBX, explotando la versatilidad del led. La NASA ha patrocinado incluso la investigación para el uso de ledes para promover salud para los astronautas.

La luz puede utilizarse para transmitir datos y señales analógicas. Por ejemplo, los ledes blancos pueden ser utilizados en sistemas para ayudar a la gente a orientarse en espacios cerrados con el objetivo de localizar disposiciones u objetos.

Los dispositivos de audición asistida de muchos teatros y espacios similares utilizan matrices de ledes infrarrojos para enviar el sonido a los receptores de los espectadores. Los ledes (y también los láseres de semiconductor) se utilizan para enviar datos a través de muchos tipos de cable de fibra óptica. Desde los cables TOSLINK para la transmisión de audio digital hasta a los enlaces de fibra de ancho de banda muy elevado que constituyen la espina dorsal de Internet. Durante algún tiempo los ordenadores estuvieron equipados con interfaces IrDA, que les permitían enviar y recibir datos de los equipos próximos mediante radiación infrarroja.

Debido a que los ledes pueden encenderse y apagarse millones de veces por segundo, requieren disponer de un ancho de banda muy alto para la transmisión de datos.

La eficiencia en la iluminación es algo necesario para la arquitectura sostenible. En 2009, las pruebas realizadas con bombillas led por el Departamento de Energía de los Estados Unidos mostraban una eficiencia media desde 35 lm/W, por debajo, por tanto, de la eficiencia de las LFC, hasta valores tan bajos como 9 lm/W, peores que las bombillas incandescentes. Una bombilla led típica de 13 vatios emitía de 450 a 650 lúmenes, que equivalía a una bombilla incandescente estándar de 40 vatios.

En cualquier caso, en 2011 existían bombillas led con una eficiencia de 150 lm/W, e incluso los modelos de gama baja llegaban a exceder los 50 lm/W, por lo que un led de 6 vatios podía alcanzar los mismos resultados que una bombilla incandescente estándar de 40 vatios. Estas últimas tienen una durabilidad de 1000 horas mientras que un led puede seguir operando a una menor eficiencia durante más de 50.000 horas .

Tabla comparativa de led-LFC-bombilla incandescente:

La reducción en el consumo de energía eléctrica que se consigue con una iluminación basada en led es importante cuando se compara con la iluminación por incandescencia. Además, esta reducción también se manifiesta como una notable disminución de daño al medio ambiente. Cada país presenta un panorama energético diferente y, por tanto, aunque la repercusión en el consumo energético sea el mismo, la producción de gases nocivos para el medio ambiente puede fluctuar algo de unos a otros. En lo que respecta al consumo se puede tomar como muestra una bombilla incandescente convencional de 40 vatios. Una producción luminosa equivalente se puede obtener con un sistema de ledes de 6 vatios de potencia. Utilizando, pues, el sistema de ledes en lugar de bombillas incandescentes, se puede reducir el consumo energético en más de un 85%. En cuanto al ahorro en el impacto ambiental es posible cuantificarlo para cualquier país si se conoce la producción de CO por cada kW por hora. En el caso concreto de España se sabe que el mix energético de la red eléctrica española ha producido unos 308 g de CO/kWh en 2016. Se supone para el cálculo que tanto la bombilla como el conjunto led han funcionado durante 10 horas al día a lo largo de todo el año 2016. Las energías consumidas han sido de 146 kW-hora por parte de la bombilla incandescente y de 21.6 kW-hora por parte del conjunto led. La energía eléctrica consumida se puede traducir a kg de CO producidos al año. En el primer caso se ha llevado a cabo la generación de unos 45 kg de CO mientras que en el segundo caso la producción de CO ha quedado reducida a 6.75 kg.

Los sistemas de visión industriales suelen requerir una iluminación homogénea para poder enfocar sobre rasgos de la imagen de interés. Este es uno de los usos más frecuentes de las luces led, y seguramente se mantenga así haciendo bajar los precios de los sistemas basados en la señalización lumínica. Los escáneres de código de barras son el ejemplo más común de sistemas de visión, muchos de estos productos de bajo coste utilizan ledes en vez de láseres. Los ratones de ordenador ópticos también utilizan ledes para su sistema de visión, ya que proporcionan una fuente de luz uniforme sobre la superficie para la cámara en miniatura dentro del ratón. De hecho, los ledes constituyen una fuente de luz casi ideal para los sistemas de visión por los siguientes motivos: 

La sanidad se ha hecho eco de las ventajas de los ledes frente a otros tipos de iluminación y los ha incorporado en su equipamiento de última generación. Las ventajas ofrecidas por los ledes en su estado de desarrollo actual han propiciado su rápida difusión en el mundo del instrumental para el diagnóstico y apoyo en los procedimientos médicos y quirúrgicos. Las ventajas apreciadas por los profesionales de la medicina son las siguientes:

Con base en las ideas anteriores, los endoscopios actuales están dotados de iluminación led. La técnica endoscópica abarca muchas especialidades médicas, por ejemplo gastroscopia, colonoscopia, laringoscopia, otoscopia o artroscopia. Todas estas técnicas permiten la observación de órganos y sistemas del cuerpo humano mediante el uso de cámaras miniatura de video. Se pueden también emplear en las intervenciones quirúrgicas o para efectuar diagnósticos. Los equipos también se conocen como videoscopios o videoendoscopios. Los hay rígidos o flexibles según las necesidades. La fibra la óptica se adapta a cada caso en particular. Por otro lado las luminarias de los quirófanos y clínicasodontológicas son actualmente de ledes. Satisfacen a la perfección todos los requerimientos técnicos y sanitarios para su utilización. Se aprecia especialmente la obtención de una iluminación blanca, natural, brillante (más de ciento cincuenta mil candelas a un metro de distancia del campo de la operación), sin sombras y sin emisiones infrarrojas o ultravioleta que podían afectar tanto al paciente como al personal médico que participa en la intervención. 

Otro tanto sucede con las lámparas frontales de los cirujanos y odontólogos dotadas de ledes, con las lámparas para exámenes médicos, para exploraciones e intervenciones oftalmológicas o par acirugía menor con lo que se puede afirmar que los ledes han llegado a abarcar todas las especialidades médicas. Las empresas ópticas dedicadas a la medicina han incorporado los ledes en sus equipos de observación, por ejemplo en los microscopios, obteniendo con ello muchas ventajas para el estudio de imágenes empleando las distintas técnicas (campo claro, contraste, fluorescencia), lo que pone de manifiesto en los campos publicitario y comercial. Los ledes se utilizan con éxito como sensores en pulsímetros o tensiómetros de oxígeno para medir la saturación de oxígeno.

La luz led se emplea en una técnica de tratamiento de la piel denominada fototerapia. Recordemos que la luz emitida por las diferentes aleaciones de semiconductores es muy monocromática. A cada uno de los colores (azul, amarillo, rojo, etc.) se le atribuye actividad prioritaria en un determinado proceso terapéutico, por ejemplo, favorecer la cicatrización (luz azul), atacar a determinada cepa de bacterias (varios colores), aclarar las manchas dérmicas (luz roja), etc.
Muchos materiales y sistemas biológicos son sensibles o dependientes de la luz. Las luces de crecimiento emplean ledes para aumentar la fotosíntesis en las plantas. Las bacterias y los virus pueden eliminarse del agua y de otras sustancias mediante una esterilización con ledes UV.

La industria ha adaptado los modelos de observación empleados en medicina para sus propias necesidades y los equipos reciben el nombre de endoscopios industriales o también boroscopios, flexoscopios o videoendoscopios. Puede observarse con ellos el interior de máquinas, motores, conductos, cavidades o armas sin necesidad de desmontarlos.

La luz de los ledes puede ser modulada muy rápidamente por lo que se utilizan mucho en la fibra óptica y la comunicación óptica por el espacio libre. Esto incluye los controles remotos utilizados en televisiones, videograbadoras y ordenadores led. Los aisladores ópticos utilizan un led combinado con un fotodiodo o fototransistor para proporcionar una vía de señal con aislamiento eléctrico entre dos circuitos. Esto es especialmente útil en equipos médicos donde las señales de un circuito de sensores de baja tensión (normalmente alimentados por baterías) en contacto con un organismo vivo deben estar aisladas eléctricamente de cualquier posible fallo eléctrico en un dispositivo de monitorización que funcione a voltajes potencialmente peligrosos. Un optoisolador también permite que la información se transfiera entre circuitos que no comparten un potencial de tierra común.

Muchos sistemas de sensores dependen de la luz como fuente de señal. Los ledes suelen ser ideales como una fuente de luz debido a los requisitos de los sensores. Los ledes se utilizan como sensores de movimiento, por ejemplo en ratones ópticos de ordenadores. La barra de sensores de la Nintendo Wii utiliza ledes infrarrojos. Los oxímetros de pulso los utilizan para medir la saturación de oxígeno. Algunos escáneres de mesa utilizan matrices de led RGB en lugar de la típica lámpara fluorescente de cátodo frío como fuente de luz. Tener el control de forma independiente de tres colores iluminados permite que el escáner se calibre para un balance de color más preciso y no hay necesidad de calentamiento. Además, sus sensores solo necesitan ser monocromáticos, ya que en cualquier momento la página escaneada solo se ilumina con un color de luz. Dado que los LED también pueden utilizarse como fotodiodos, se pueden usar también para la emisión de fotografías o para la detección. Esto podría ser utilizado, por ejemplo, en una pantalla táctil que registra la luz reflejada desde un dedo o un estilete. Muchos materiales y sistemas biológicos son sensibles o dependen de la luz. Las luces para cultivo usan led para estimular la fotosíntesis en las plantas, y las bacterias y los virus pueden ser eliminados del agua y otras sustancias que usan ledes UV para la esterilización.

Los ledes también se han utilizado como referencia de voltaje de calidad en circuitos electrónicos. En lugar de un diodo Zener en reguladores de baja tensión, se puede usar la caída de tensión directa (por ejemplo, aproximadamente 1,7 V para un led rojo normal). Los ledes rojos tienen la curva I / V más plana. Aunque la tensión directa del led es mucho más dependiente de la corriente que un diodo Zener, los diodos Zener con tensiones de ruptura por debajo de 3 V no están ampliamente disponibles.

La miniaturización progresiva de la tecnología de iluminación de bajo voltaje, como los ledes y los OLED, adecuados para incorporarse a materiales de bajo espesor, ha fomentado la experimentación en la combinación de fuentes de luz y superficies de revestimiento de paredes interiores. Las nuevas posibilidades ofrecidas por estos desarrollos han llevado a algunos diseñadores y compañías, como Meystyle, Ingo Maurer, Lomox y Philips a investigar y desarrollar tecnologías propietarias de papel tapiz led, algunas de las cuales están actualmente disponibles para la compra comercial. Otras soluciones existen principalmente como prototipos o están en proceso de ser refinadas.



</doc>
<doc id="19373" url="https://es.wikipedia.org/wiki?curid=19373" title="Pixies">
Pixies

Pixies es una banda de rock alternativo formada en 1986 en la ciudad de Boston, Estados Unidos. El grupo se desintegró en 1993 debido a tensiones internas, pero se reunió nuevamente en 2004. La banda ha estado integrada desde su creación por Black Francis, Joey Santiago, Kim Deal y David Lovering; sin embargo Deal dejó la banda en el 2013, siendo reemplazada por Kim Shattuck por unos meses, quien fue reemplazada a su vez en diciembre de ese año por Paz Lenchantin, para el tour mundial 2014. Si bien Pixies solo pudo conseguir un modesto éxito en su país, alcanzó mucho mayor reconocimiento en Europa, en especial en el Reino Unido y a pesar de que ninguno de sus álbumes tuvo un gran éxito comercial, fue una de las bandas más influyentes del rock alternativo

La música de Pixies está influida por el post-punk y el surf rock. A pesar de que su estilo es muy melódico, son capaces de ser cáusticos. Esto se ve reflejado en las letras de Francis, cantante y compositor principal del grupo, que interpreta con un estilo de voz desesperado. Los temas de sus letras suelen girar en torno a fenómenos de difícil explicación, como el fenómeno OVNI o el surrealismo. Las referencias a temas como la inestabilidad mental, imágenes violentas tomadas de la Biblia, el incesto o daños físicos también son muy frecuentes en las letras de la banda.

Al grupo frecuentemente se le ve como el antecesor inmediato del "boom" del rock alternativo de principios de la década de 1990, a pesar de que su disolución se dio antes de que pudieran cosechar beneficios de ello. Kurt Cobain, fan confeso de Pixies, reconoció la gran influencia que el grupo ejerció en Nirvana. Esto aseguraría el legado de Pixies y provocaría que su influencia fuera creciendo sustancialmente en los años posteriores a su separación.

Al igual que ocurrió con otros grupos de noise rock, su estilo singular ejerció gran influencia sobre muchas de las bandas de rock de la escena grunge y alternativa de principios de la década de 1990, especialmente sobre Nirvana, Pavement y Weezer, quienes popularizaron su particular uso de melodías suaves durante las estrofas y explosiones, gritos y guitarras distorsionadas en los estribillos. Fueron de gran influencia para todas las bandas que posteriormente incluirían el "noise" en canciones de estructura pop al igual que ellos.

La historia de Pixies comenzó cuando Joey Santiago y Black Francis (cuyo nombre de pila es Charles Thompson IV), estudiantes de antropología de la Universidad de Massachusetts Amherst, coincidieron como compañeros de habitación. Santiago pronto introdujo a Francis en la música de David Bowie y el punk rock de la década de 1970, y comenzaron a componer e interpretar juntos. En esta época Francis realizó un viaje de intercambio estudiantil a la ciudad de San Juan, Puerto Rico, aunque tuvo problemas para desenvolverse por culpa del idioma. Después de seis meses de compartir habitación con un "compañero loco, psicótico y gay", regresó a Boston y abandonó la universidad. Ambos compañeros pasaron el año 1984 trabajando en un almacén, mientras Francis se dedicaba a componer canciones con su guitarra acústica y a escribir letras de canciones mientras viajaba en el metro.

En 1986 Francis publicó en un periódico un anuncio clasificado donde buscaba "bajista a quien le gustara tanto la música del grupo de folk Peter, Paul and Mary como la banda de hardcore punk Hüsker Dü." De esta forma, Kim Deal (la única persona que respondió al anuncio) se incorporó al grupo, si bien se presentó a la audición sin un bajo, ya que nunca antes había tocado dicho instrumento. Explicó que su hermana gemela Kelley Deal tenía un bajo en su casa en Dayton, pero que no tenía dinero para ir a buscarlo. Francis le prestó cincuenta dólares para el billete de avión y Deal regresó con el bajo. El trío comenzó a ensayar en el piso de Deal, ya que la "anciana que vivía en el piso de arriba no podía oír".

Después de reclutar a Kim Deal, la banda intentó sin resultados convencer a su hermana Kelley para que se uniese a ellos como batería. El esposo de Kim, John Murphy, les sugirió que contratasen al baterista David Lovering, al que Deal había conocido en la celebración de su boda. Después de la llegada de Lovering, el grupo adquirió su nombre cuando Santiago eligió al azar una palabra de un diccionario, encontrando la palabra "Pixie" ("duendecillo" en inglés), y le gustó la definición que daban de ella ("pequeños elfos maliciosos"). El grupo aceptó la sugerencia y comenzaron llamándose "Pixies In Panoply" (también consideraron llamarse "Things on Fire"), pero poco tiempo después lo acortarían a "Pixies". Ya con un nombre y con una formación estable, el grupo trasladó su lugar de ensayo al garaje de los padres de Lovering en el verano de 1986. Dieron su primer concierto — que la banda recuerda como "posiblemente el peor concierto en la historia del rock" — en The Rathskeller en Boston, donde interpretaron las primeras versiones de "Here Comes Your Man", "Dig for Fire" y "Build High".

Durante un concierto de Pixies junto a la banda Throwing Muses, los vio el productor discográfico Gary Smith de Fort Apache Studios. Smith dijo a la banda que "no podría dormir hasta que fueran mundialmente famosos". Poco después, la banda produjo en los estudios Fort Apache una demo de diecisiete canciones, conocida por los seguidores como "The Purple Tape" debido al color lila de la funda del casete. La grabación se hizo en tres días y los mil dólares que costó los aportó el padre de Francis. El casete fue lanzado como demo exclusivamente para gente del gremio, entre ellos Ivo Watts-Russell de 4AD y el promotor local Ken Goes, que se convertiría en mánager de la banda. En un comienzo, Watts-Russell no mostró interés por el grupo, al que encontraba demasiado normal, "demasiado rock n' roll", pero les ofreció su primer contrato discográfico debido a la insistencia de su novia.

Ya con un contrato discográfico con 4AD (compañía de la que la banda dijo: "la mejor a la hora de pagar a tiempo"), se seleccionaron ocho canciones de "The Purple Tape" para conformar el EP "Come On Pilgrim", su primer álbum. El título provenía de la letra de la canción "Levitate Me", que a su vez venía de una frase que utilizaba en sus conciertos de los años 1970 el cantante de rock cristiano Larry Norman, al cual Francis había visto actuar una vez en un campamento de verano: "Come on Pilgrim, you know He loves you" (""Venga peregrino, sabes que Él te quiere""). Black Francis grabaría más adelante una canción de Norman en su carrera en solitario, además de compartir escenario con él en una ocasión.

En el EP, Francis habla de sus experiencias en Puerto Rico y de la pobreza del país, sobre todo en las canciones "Vamos" e "Isla de Encanta". Las letras de temática religiosa de "Come On Pilgrim" y álbumes posteriores proceden de su época como cristiano renacido en la iglesia pentecostal.

"Come On Pilgrim" es una muestra de lo que serían Pixies y sentó las bases de su estilo musical. Incluye dos canciones cantadas parcialmente en español ("Vamos" e "Isla de Encanta") y dos canciones que hacen referencia al incesto ("Nimrod's Son" y "The Holiday Song"). "I've Been Tired" habla de la cultura del rock and roll y sexo con un retorcido sentido del humor. Además, hay cuatro canciones de temática religiosa: "Caribou", "Nimrod's Son", "I've Been Tired" y "The Holiday Song". Musicalmente, "Come On Pilgrim" mostraba la errática forma de tocar de Santiago (muy visible en "Vamos"), las dulces armonías de Kim Deal (que utilizaba el pseudónimo de "Mrs. John Murphy" en las primeras grabaciones de la banda, en forma de broma feminista), y los increíbles cambios tonales en la voz de Frank Black, que pasaba de gritar a cantar de forma melódica sencilla y tradicional.

A "Come On Pilgrim" le siguió el primer álbum LP, "Surfer Rosa". Fue producido por Steve Albini, grabado en dos semanas y lanzado a principios de 1988. Posteriormente, Albini se haría famoso por producir el álbum "In Utero" de Nirvana a petición de Kurt Cobain, que citó a "Surfer Rosa" como una de sus mayores influencias musicales, admirando en particular la potencia y naturalidad del sonido de la batería, que se debían en gran medida a la producción de Albini en el álbum. "Surfer Rosa" les trajo críticas positivas por todo el mundo; las revistas "Melody Maker" y "Sounds" consideraron al disco "álbum del año". El éxito de "Surfer Rosa" les llevó a firmar con la discográfica multinacional Elektra para su segundo álbum.

Al igual que con "Come On Pilgrim", la banda ofrecía una amplia gama de estilos de canciones. De cualquiera de las maneras, en sonido y temas, "Surfer Rosa" era similar a "Come On Pilgrim", desde la canción "Bone Machine", dominada por la batería y que mostraba una propensión a la temática surrealista que sería sello de la banda, hasta canciones pop de guitarras como "Broken Face", "Break My Body" y "Brick is Red".

La banda incluyó material más duro, como "Something Against You", con los gritos distorsionados de Black Francis. La revista "Q" incluyó a "Surfer Rosa" en su lista de los ""50 álbumes más duros de todos los tiempos"". En el álbum aparece una versión regrabada de "Vamos", canción que aparecía ya en el EP "Come On Pilgrim". La pista "You Fuckin' Die! (I Said)" (referida como "pista adicional" o "pista escondida" en la mayoría de las ediciones del álbum) que aparece al final del álbum es una grabación accidental de Francis y Deal hablando y bromeando, y en contra de lo que sugiere el título no hay rastro de la tensión que luego les llevaría a disolver la banda.

"Surfer Rosa" contiene algunas de sus canciones más populares, como "Gigantic", que fue su primer sencillo y una de las pocas canciones donde la voz principal es de Kim Deal (el sencillo no entró en el Billboard, y solo llegó al número 93 en el Reino Unido), "River Euphrates" o "Where Is My Mind?", que aparece en la banda sonora de la película "Fight Club".

Después de su aclamado primer álbum, la banda viajó al Reino Unido con la banda Throwing Muses en una gira europea llamada ""Sex and Death tour"", que dio comienzo en Londres. La gira les llevó a los Países Bajos, donde Pixies ya había recibido suficiente atención de los medios como para encabezar la gira. Francis después recordaría: "El primer sitio donde conseguí algo con Pixies fue en los Países Bajos". La lista de canciones del concierto incluía nuevos temas como "In Heaven", "Hey", y "Wild Honey Pie", y la banda a veces tocaba todo el "setlist" en orden alfabético, entre otras bromas privadas que caracterizaron la gira. Las canciones mencionadas se escogieron para grabar una "Peel session" en julio para la BBC y pronto volvieron a los mismos estudios para grabar otra sesión con Peel, en esta ocasión escogiendo "Dead", "Tame", "There Goes My Gun", y "Manta Ray". En total, la banda grabó seis "Peel sessions" y lanzó un álbum con algunas de estas grabaciones, "Pixies at the BBC".

En esta época, la banda conoció al productor británico Gil Norton, que se haría cargo de la grabación de su segundo álbum, "Doolittle" (provisionalmente llamado "Whore"), que se grabó en las últimas seis semanas de 1988 y se considera como un cambio del sonido crudo de "Come On Pilgrim" y "Surfer Rosa". "Doolittle" tiene un sonido mucho más limpio, en gran medida por Norton y los cuarenta mil dólares que se invirtieron en la grabación, cuadruplicando el dinero invertido en "Surfer Rosa". Mucha de la temática del álbum es similar a la de los dos álbumes previos; varias canciones parecen evocar imágenes sangrientas y de mutilaciones, como "I Bleed", "Wave of Mutilation", y "Gouge Away".

"Doolittle" comienza con "Debaser", una oda a la película surrealista de Luis Buñuel y Salvador Dalí, "Un perro andaluz". "Debaser" es quizá su canción más aclamada; la revista "Q" la puso en el puesto número 21 de su lista de ""Las 100 mejores pistas de guitarra"". "Doolittle" contiene el sencillo "Here Comes Your Man"; una canción de pop inusualmente jovial para la banda. Francis después expresaría su sorpresa ante el hecho de que el riff de guitarra inicial y la parte cantada fuese exactamente igual a la canción "Never My Love" de la banda The Association, escrita veinte años antes. "Monkey Gone to Heaven", la única canción de Pixies con una sección de cuerda, se convirtió en un éxito, entrando en el Top 10 de la lista de radio de rock moderno en los Estados Unidos y en el Top 100 británico de sencillos. La única contribución de Deal como compositora se encuentra en la canción "Silver" (coescrita con Francis), en la cual Deal toca la guitarra slide y Lovering el bajo. Lovering puso la voz principal en "La La Love You", una canción de amor atípica para la banda.

Al igual que "Surfer Rosa", "Doolittle" fue aclamado por el público y la crítica, y es su álbum más vendido, siendo certificado disco de oro por la RIAA el 10 de noviembre de 1995. En 2003, el álbum se situó en el número 226 de la lista de "Los 500 grandes álbumes de todos los tiempos" de la revista "Rolling Stone". También la revista "Q" la incluyó en su lista de "Los 100 mejores álbumes de todos los tiempos".

Fue después de "Doolittle" cuando las tensiones entre Deal y Francis se volvieron insoportables (por ejemplo, Francis le tiró una guitarra a Deal durante un concierto en Stuttgart) y casi fue expulsada de la banda. Santiago, en una entrevista a la revista "Mojo", dijo:

Durante la gira de "Fuck or Fight" por Estados Unidos, hecha para promocionar el álbum "Doolittle", la saturación de trabajo pasó factura a la banda; Pixies habían sacado tres álbumes en dos años, además de estar constantemente de gira. Casi al final de la gira de 1989, durante su concierto en Boston, Deal estaba borracha, siendo tal el enfado de Santiago que destrozó sus instrumentos para después marcharse del concierto. Después del último concierto en Nueva York, la banda estaba demasiado cansada para asistir a la fiesta del final de la gira y anunciaron que iban a tomarse un descanso.

Durante esta pausa, Santiago viajó por el Gran Cañón para "encontrarse a sí mismo", y Lovering se marchó a Jamaica. Francis se compró un Cadillac amarillo y atravesó Estados Unidos con su novia (por una fobia a volar), mientras hacía conciertos en solitario para conseguir dinero. Deal formó The Breeders, banda llamada igual que la que había formado con su hermana en la adolescencia, formando el nuevo grupo junto a Tanya Donelly de Throwing Muses y la bajista Josephine Wiggs de Perfect Disaster. Su álbum debut, "Pod", se lanzó a finales de ese mismo año.

Cuando la banda retomó el trabajo, Francis comenzó a limitar las contribuciones de Deal y a controlar más todo lo concerniente a la banda; los tres primeros álbumes habían sido compuestos en parte por Deal, mientras que el nuevo "Bossanova" solo contiene canciones de Francis. Deal no estaba contenta y anunció unilateralmente una aparente disolución de la banda durante un concierto de la gira de apoyo de "Doolittle". Pixies estaban en la cumbre de su popularidad, encabezando festivales como el Reading Festival de 1990.

La temática del álbum cambió con respecto a los álbumes anteriores, hacia un enfoque más surrealista, de ciencia ficción. El estilo musical está inspirado en el surf rock. El álbum comienza con una versión de "Cecilia Ann" de The Surftones. Canciones como "Havalina" y "Ana" mostraban un lado soñador de la banda, y la voz de Francis mucho más entonada (aunque en "Rock Music", muestra sus inconfundibles gritos). "Dig For Fire" es, según Francis, un tributo a Talking Heads. La guitarra de Santiago es menos prominente, sin ningún solo alocado como en "Come On Pilgrim" o "Surfer Rosa". La canción "Allison" es un tributo a uno de los ídolos musicales de Francis, el artista estadounidense de jazz y blues Mose Allison. La canción habla del espacio y el universo, temática habitual en la música de Mose.

La banda siguió de gira, y la disolución de la que tanto se había hablado no terminaba de ocurrir, por lo que siguió el álbum "Trompe le Monde", lanzado en 1991 y que tampoco contaba con mucho material compuesto por Deal. El álbum tuvo una acogida buena, aunque no comparable a los primeros. Antes de su lanzamiento, se rumoreaba que estaba inspirado en el heavy metal, y el prelanzamiento del sencillo "Planet of Sound", más duro de lo que venía siendo habitual, no ayudó a acallar los rumores.

"Trompe Le Monde" sigue la temática de los OVNIs y la ciencia ficción (incluyendo una canción sobre vuelos espaciales, "Planet of Sound", mientras que "Motorway to Roswell" toca el fenómeno alienígena). Canciones como "Bird Dream of the Olympus Mons" y "Lovely Day" son similares en estilo a las del álbum "Bossanova" (como "Havalina"). El álbum hizo a la banda conseguir cierta popularidad con canciones como "Palace of the Brine" y "Trompe Le Monde". Las canciones "U-Mass" y "Alec Eiffel" incluyen al teclista Eric Drew Feldman, algo que habría sido impensable en la época de "Come On Pilgrim" y "Surfer Rosa". El álbum también incluye una versión de la canción "Head On" de The Jesus and Mary Chain. "Trompe Le Monde" fue el último álbum antes de la disolución de la banda.

Siguiendo al lanzamiento de "Trompe Le Monde", la banda contribuyó al álbum tributo a Leonard Cohen, "I'm Your Fan", con una versión de la canción "I Can't Forget", para después embarcarse en una gira por Estados Unidos, que culminó con una actuación en el programa de televisión "The Tonight Show Starring Johnny Carson". Después comenzaron una incómoda gira como teloneros de U2 (en su gira Zoo TV) en 1992. La tensión entre los miembros de la banda creció, y, a finales de año, se separaron para concentrarse en sus proyectos en solitario.

A principios de 1993, Francis anunció en una entrevista a BBC Radio 5 que la banda se había disuelto y no dio más explicaciones, ni siquiera al resto de los miembros de la banda. Después, telefoneó a Santiago y subsecuentemente avisó a Deal y Lovering por fax. Francis después se arrepentiría de deshacer la banda de esta manera, ya que no dio oportunidad al resto de la banda de discutirlo.

Black Francis se hizo llamar Frank Black y editó tres álbumes en solitario ("Frank Black", "Teenager of the Year" y "The Cult of Ray"). Después, formó una banda junto a los ex miembros de Miracle Legion Scott Boutier y David McCaffrey, y el músico de sesión Lyle Workman, haciéndose llamar Frank Black and the Catholics. Para el segundo álbum con the Catholics, Workman fue sustituido por Dave Gilbert, y para el tercer álbum se sumó al proyecto un tercer guitarrista, Dave Phillips. Aunque había un fuerte elemento de rock en la banda, la adición de steel guitar le daba un aire country. En 2005 The Catholics se disolvieron y Black editó su cuarto álbum en solitario, "Honeycomb", con un estilo más maduro, de rhythm and blues, con el apoyo de músicos de Nashville, Tennessee. El 19 de julio de 2006, lanzó al mercado un doble álbum también con extractos de estas grabaciones, "Fastman Raiderman". Volviendo a su "nombre de guerra" de la época de Pixies, Black Francis lanzó un LP ("Bluefinger") y un EP ("Svn Fngrs"). En mayo de 2008, Black Francis y su banda (incluyendo a Eric Drew Feldman) actuaron en el Festival de cine de San Francisco para promocionar la película de terror "The Golem". A principios de 2009 formó junto a su esposa, Violet Clark, la banda Grand Duchy. Su álbum debut, "Petits Fours", se puso a la venta el 14 de abril con una recepción bastante buena por parte de la crítica.

Deal volvió a The Breeders teniendo cierto éxito con el sencillo «Cannonball» del álbum de 1993, "Last Splash", que RIAA certificó disco de platino. Tardaron un tiempo en grabar otro álbum, en parte por la adicción de su hermana Kelley Deal a la heroína. Durante el parón de The Breeders, Deal formó la banda The Amps, con el que grabó un único álbum, "Pacer", en 1995. Un nuevo álbum de Breeders, "Title TK", finalmente salió al mercado en 2002, con solo Kim y Kelley de los miembros iniciales de la banda.

Lovering trabajó como mago actuando con el nombre artístico de "The Scientific Phenomenalist", haciendo experimentos en el escenario y ocasionalmente abriendo los espectáculos de Frank Black o The Breeders. Lovering continuó tocando la batería, tocando en uno de los álbumes en solitario de Tanya Donelly, "Lovesongs for Underdogs" de 1997.

Santiago tocó la guitarra en los álbumes en solitario de Frank Black, y en otros álbumes como "Statecraft", del músico de indie rock Charles Douglas. Santiago también compuso la música para la cadena de televisión Fox, además de formar una banda con su esposa, Linda Mallari, llamada The Martinis. Lanzaron su álbum debut, "Smitten", en 2004.

Después de la disolución de la banda, 4AD y Elektra Records lanzaron varios álbumes recopilatorios como "Death to the Pixies" y "Complete B-Sides", junto con "Pixies (The Purple Tape)" y "Pixies at the BBC".

En los once años posteriores a la disolución de la banda, los rumores sobre una reunión fueron frecuentes. Aunque Frank Black los negaba con celeridad, es cierto que comenzó a incluir una creciente cantidad de temas de Pixies en sus conciertos con the Catholics, y ocasionalmente incluía a Santiago y Lovering en sus proyectos en solitario. A finales de 2003 la prensa confirmó una reunión para primavera de 2004. Pixies hicieron su primer concierto de regreso el 13 de abril de 2004 en The Fine Line Music Cafe en Minneapolis, Minnesota, y una gira de calentamiento por los Estados Unidos y Canadá, seguida de un concierto en el Festival de Música y Artes de Coachella Valley. La banda pasó el resto de 2004 de gira por Brasil, Europa, Japón, y nuevamente los Estados Unidos.

Esta reunión de 2004 fue plasmada en la película documental "LoudQUIETloud".

En junio de 2004, la banda lanzó un nuevo sencillo, "Bam Thwok" exclusiva para iTunes Music Store. La canción llegó al número 1 de la lista de música más descargada en el Reino Unido, la UK Official Download Chart. 4AD lanzó "", junto a un DVD. La banda también contribuyó con una versión de "Ain't That Pretty At All" para el álbum homenaje al músico Warren Zevon, "".

En 2005 la banda tocó en Lollapalooza, "T on the Fringe", y el Newport Folk Festival. Continuaron tocando a lo largo de 2006 y 2007, culminando con su única aparición en Australia. Desde 2005, Francis ha comentado en varias ocasiones la posibilidad de que Pixies grabase un nuevo álbum de estudio, o las pocas probabilidades de ello, siendo Kim Deal el mayor obstáculo para que ocurra.

Para la celebración del vigésimo aniversario del lanzamiento de "Doolittle", Pixies comenzó una nueva gira en octubre de 2009, en la que tocaban todas las canciones del disco y las caras B de sus sencillos. La gira comenzó en Europa, continuó por Estados Unidos en noviembre, siguiendo con más fechas en Australia, Nueva Zelanda y nuevamente Europa en primavera de 2010, para terminar en Estados Unidos en otoño de 2010 hasta la primavera de 2011.

El 14 de junio de 2013 el perfil de Twitter de la banda anunció que Kim Deal había dejado el grupo. Desde entonces Deal ha publicado música en solitario en su página web y los restantes Pixies la han acogido de vuelta cuando su calendario con las Breeders lo permite. Dos semanas más tarde, la banda publicó una nueva canción, "Bagboy", como descarga gratuita en su página web. El canción cuenta con la participación de Jeremy Dubs del grupo Bunnies a las voces, en sustitución de Deal.

El 1 de julio de 2013 los Pixies anunciaban la incorporación de la guitarrista y vocalista de The Muffs y The Pandoras, Kim Shattuck, para sustituir a Deal en la gira europea 2013 de la banda. El 3 de septiembre de 2013 los Pixies lanzaron un EP de nuevas canciones titulado "EP1". El 29 de noviembre de 2013 Shattuck anunció que había sido despedida de la banda ese día. En diciembre de 2013 se anunció que la bajista de A Perfect Circle y The Entrance Band, Paz Lenchantin, se unía a los Pixies para la gira 2014. Más material nuevo apareció cuando los Pixies publicaron su segundo EP, "EP2", el 3 de enero de 2014. El sencillo estrenado en radio fue "Blue Eyed Hexe". Otro EP, "EP3", fue lanzado el 24 de marzo de 2014. Todos los EP estuvieron disponibles exclusivamente como descargas y vinilos de edición limitada. Los tres EP fueron recopilados en formato LP y publicados como el álbum "Indie Cindy" en abril de 2014. El álbum fue el primer lanzamiento de la banda en más de dos décadas, habiendo sido el último "Trompe le Monde" en 1991.

En 2015 se anunció que los Pixies girarían como teloneros del exlíder de Led Zeppelin, Robert Plant, en una serie de conciertos a lo largo de Norte América.

El 6 de julio de 2016 la banda anunció el lanzamiento de su sexto álbum, "Head Carrier", y que Lenchantin se convertía en miembro permanente del grupo. El álbum fue lanzado el 30 de septiembre de 2016.

Aunque el estilo musical de Pixies ha cambiado a lo largo de los años, la banda se define como de rock alternativo similar a bandas contemporáneas como Throwing Muses. Pixies exploró varios estilos en sus canciones, aunque la mayoría se caracterizan por la distintiva voz y gritos de Francis, los suaves coros de Deal (como en "I Bleed" y "Debaser") y la errática forma de tocar la guitarra de Santiago. El sonido de la banda ha evolucionado del indie rock de los álbumes "Come On Pilgrim" y "Surfer Rosa", hacia un rock de ciencia ficción en "Bossanova" y "Trompe le Monde". De cualquiera de las maneras, han experimentado con varios géneros, como el surf rock ("Cecilla Ann" de "Bossanova"), rock ("U-Mass") y pseudo metal ("Planet of Sound" y "The Sad Punk", de "Trompe le Monde").

Pixies han sido influidos por un gran número de artistas y géneros distintos; cada miembro viene de un pasado musical muy distinto. Cuando Francis comenzó a componer para Pixies, dice que escuchaba sobre todo Hüsker Dü, Captain Beefheart e Iggy Pop (incluyendo "New Values" y el bootleg "I'm Sick of You"); en una entrevista a la revista "Mojo Magazine" citó a Iggy como su mayor influencia . Durante la elaboración de "Doolittle" escuchaba mucho el "White Album" de The Beatles. Citó a Buddy Holly como modelo a seguir para su forma de componer canciones de forma comprimida.

Francis dijo:

Santiago escuchaba punk de los años 1970 y 1980 (incluyendo Black Flag) y a David Bowie. Entre los guitarristas que le influyeron se incluyen Jimi Hendrix, Les Paul, Wes Montgomery y George Harrison.

Las influencias de Deal provenían sobre todo de la música country; había formado una banda de country con su hermana en la adolescencia, también llamada The Breeders.

Además, la música folk también fue una influencia para Pixies; Francis escuchaba al cantante de rock cristiano Larry Norman. De hecho, cuando la banda buscaba bajista, el único requisito era que le gustara Hüsker Dü y el trío folk Peter, Paul and Mary. Francis también menciona a Lou Reed en la canción "I've Been Tired" de "Come On Pilgrim".

Otras formas de arte también influyeron a Pixies, como el cine; Francis cita películas surrealistas como "Eraserhead" y "Un perro andaluz" como influencias. Comentó sobre estas influencias (homenajeadas sobre todo en el álbum "Doolittle") diciendo que "no tenía la paciencia para sentarme a leer novelas surrealistas, era más fácil ver películas de veinte minutos de duración". Llegó a decir que los miembros de la banda eran surrealistas en una entrevista a "Melody Maker": "Tal vez la vanguardia atrae a la gente que viene del mismo origen económico que nosotros, porque rechazamos, al modo típico, los antiguos y profundos valores cristianos; pero seguimos estando endemoniadamente confusos.

La mayoría de las canciones de Pixies las componía y cantaba Black Francis, cuya temática en las letras estaba enfocada en la violencia bíblica ("Dead", "Gouge Away") y el incesto ("The Holiday Song", "Nimrod's Son"). Más adelante, comentó en una entrevista a "Melody Maker": "Son todos esos personajes del Antiguo Testamento. Estoy obsesionado con ellos. No sé porqué son tan recurrentes".

También escribió sobre temas poco habituales, como los suicidios de salarymen en el Japón ("Wave of Mutilation") o terremotos ("Here Comes Your Man"), y en su primera época incluía temas cristianos, como en "Levitate Me". Más adelante, comenzó a interesarse en temas de ciencia ficción como los alienígenas ("Motorway to Roswell") o los OVNIs ("The Happening").

Deal puso su voz a "Gigantic" y a la última composición de la banda, "Bam Thwok", ambas canciones compuestos por ella, además de en "Silver," coescrita con Francis; también cantó en la canción escrita por Francis "Into the White" y la versión de "I've Been Waiting For You" de Neil Young. Lovering puso la voz principal a "La La Love You" y "Make Believe"; ambas compuestas por Francis.

La banda ha grabado varias versiones: "Hang on to You Ego" (The Beach Boys), "Wild Honey Pie" (The Beatles), "Ain't That Pretty At All" (Warren Zevon), "Winterlong" y "I've Been Waiting for You" (Neil Young), "I Can't Forget" (Leonard Cohen), una versión en español de "Evil Hearted You" (The Yardbirds), "Head On" (The Jesus and Mary Chain), "Cecilia Ann" (The Surftones), "Born in Chicago" (The Paul Butterfield Blues Band), "In Heaven (The Lady in the Radiator Song)" (de la película "Eraserhead"), y "Theme from NARC" (del videojuego NARC).

En términos de instrumentación, Pixies son una banda de rock de cuatro piezas. Francis, el cantante de la banda, toca guitarra rítmica, ya sea Fender Telecaster, Acústica Martin & co Gibson SG, Gibson SG Junior, Fender Mustang, o Fender Jaguar, empleando Marshall JCM 800 o Vox AC30 como amplificación. Santiago, el guitarrista principal, siempre usa Les Paul o Gibson ES-335 y un amplificador Pearce GR-8. Deal, la bajista, toca con bajos Fender Precision o Music Man Stingray. Lovering, el batería, usa una batería Pro Prestige de cinco piezas.

A medida que progresaba su carrera, comenzando con "Gigantic" (de "Surfer Rosa"), la banda ha incorporado otros instrumentos, a veces inusuales, y ha experimentado con su sonido. Por ejemplo, "Monkey Gone to Heaven" cuenta con una sección de cuerda. En "Velouria" (de "Bossanova") se hace uso de un theremín. La gran mayoría de las canciones de "Trompe le Monde" contienen teclados y sintetizadores, de la mano de Eric Drew Feldman, y "Bam Thwok", su último sencillo, cuenta con un solo de órgano.

Aunque Pixies editaron pocos álbumes, tuvieron una gran influencia en la explosión del rock alternativo de la década de 1990 que comenzó con la canción "Smells Like Teen Spirit" de Nirvana. Gary Smith, productor del primer álbum de Pixies, "Come On Pilgrim", habló de su influencia en 1997.

A Pixies, en lo que a sonido se refiere, se les atribuye haber popularizado las formas de composición que luego se convertirían en un estándar dentro del rock alternativo; las canciones de Pixies normalmente se componen de estrofas suaves y reprimidas junto a estribillos explosivos. Las versiones de sus canciones y comentarios de diversos artistas como David Bowie, Radiohead, U2, Weezer, Nirvana, The Cure y críticas como las de Graham Linehan atestiguan el aprecio por la banda de otros músicos y críticos. Bob Mould (de Hüsker Dü, a quienes Pixies citan como influencia) dijo que era un gran fan de Pixies, al igual que Thom Yorke de Radiohead, que después de informarse de que Pixies había decidido tocar antes que ellos en el Coachella Valley Music and Arts Festival, exclamó:

Yorke dijo en ese mismo festival que, mientras estaba en el colegio, "los Pixies cambiaron mi vida". Otros miembros de Radiohead han citado a la banda como influencia, y Yorke comentó, "si todos fuéramos seguidores de los Pixies y nada más, está claro a lo que sonaría la banda".

Durante la gira con U2 en 1992, Pixies recibieron una nota de la banda que decía: "Seguid buscando fuego. Os queremos". David Bowie, cuya música había inspirado a Francis y Santiago en su época universitaria, habló de la ruptura de la banda: "Me sentí muy deprimido el día en que me enteré de la separación de los Pixies. Vaya desperdicio... Los veía llegando a ser gigantescos". Esta aseveración se hacía eco de muchos artistas de la época que pensaban que los Pixies merecían haber tenido más éxito comercial.

La cita más célebre fue la de Kurt Cobain sobre la influencia de la banda en "Smells Like Teen Spirit", admitiendo que fue un intento consciente de seguir el estilo de Pixies. En enero de 1994, en una entrevista a "Rolling Stone", dijo:

Weezer (que después harían una versión de "Velouria" en el álbum de homenaje a Pixies "") han citado a Pixies como influencia en su música, y su cantante Rivers Cuomo, en una entrevista a "Addicted To Noise", dijo que la banda le hizo "explotar la cabeza cuando fui a Los Ángeles la primera vez y comencé a descubrir nueva música". Damon Albarn de Blur dijo: "Cuando comenzamos queríamos sonar como los Pixies".

Pixies hicieron apariciones en varios programas televisivos en su primera etapa, incluyendo "The Tonight Show" y "120 Minutes" en Estados Unidos; "Snub TV" y "The Word" en el Reino Unido.

Ya que la banda tenía contrato con una pequeña discográfica independiente, 4AD, en la época de "Come On Pilgrim" y "Surfer Rosa" no se editó ningún videoclip. Desde "Doolittle", su primer álbum con Elektra Records, el grupo comenzó a lanzar videoclips con cada uno de sus sencillos, aunque eran bastante simples. Por ejemplo, en las canciones "Monkey Gone To Heaven", "Head On" y "Debaser", eran simples videos de la banda tocando sus instrumentos.

Para "Bossanova", la banda ya tenía animadversión a la grabación de videoclips, ya que Francis se negaba a utilizar la sincronía de labios. Por ejemplo, en el video de "Here Comes Your Man", tanto Black como Deal abren la boca de forma exagerada en vez de intentar seguir la letra. Según la discográfica, este fue uno de los motivos por los cuales Pixies no consiguieron gran éxito en la MTV.

Cuando "Velouria" (el primer sencillo extraído de "Bossanova") comenzó a escalar en las listas del Reino Unido, el grupo recibió una oferta para aparecer en el programa "Top of the Pops". Pero las reglas de la BBC eran que solo los sencillos con videoclip podían formar parte del programa, por lo que se hizo un video barato con la banda corriendo por una cantera. El video consta de veintitrés segundos de metraje (el tiempo que tardó la banda en llegar hasta la cámara), que se ralentizó para que durara lo mismo que la canción. De cualquiera de las maneras, la grabación del video fue en vano, ya que Pixies nunca llegaron a tocar "Velouria" en "Top of The Pops" mientras duró el sencillo en listas.

Aunque Pixies nunca ganaron premios de renombre, la banda ganó varios premios de revistas musicales y premios menores. Por ejemplo, ganaron el "Act of the Year" ("Mejor directo del año") en 2004 en los Premios de la música de Boston. Además, varias revistas musicales les han premiado:
Además, los álbumes "Surfer Rosa" y "Doolittle" aparecen en la lista de la revista Rolling Stone, "Los 500 mejores álbumes de todos los tiempos", en los puestos 315 y 226, respectivamente.







</doc>
<doc id="19375" url="https://es.wikipedia.org/wiki?curid=19375" title="Louis Daguerre">
Louis Daguerre

Louis-Jacques-Mandé Daguerre, más conocido como Louis Daguerre (Cormeilles-en-Parisis, Valle del Oise, Francia, 18 de noviembre de 1787-Bry-sur-Marne, Valle del Marne, Francia, 10 de julio de 1851), fue el primer divulgador de la fotografía, tras inventar el daguerrotipo, y trabajó además como pintor y decorador teatral.

Educado en el seno de una familia acomodada, desde su juventud demostró una gran inclinación por el estudio de las letras y las artes. Daguerre recibió una educación muy elemental que terminó a los catorce años. A esta edad tuvo que empezar a ganarse la vida. De inteligencia natural y con una extraordinaria facilidad para el dibujo, Daguerre se empleó como aprendiz de arquitecto. Ahí aprendió a trazar planos y a hacer dibujo en perspectiva. Estas enseñanzas fueron de gran valor para su segunda ocupación, pues empezó a trabajar como aprendiz del célebre y famoso -en aquel tiempo- diseñador de escenarios para teatro y ópera Degoti. Tres años permaneció en este trabajo, antes de abandonarlo para ingresar como ayudante del escenógrafo más destacado del París de la época, Prevost. En esa ocupación Daguerre empezó a darse a conocer por sus trabajos, consagrándose entre los hombres más importantes del teatro. 
Daguerre era un pintor de segunda fila en el París de la primera mitad del siglo XIX. No obstante, logró una de sus creaciones más espectaculares con la obra "Misa del Gallo en Saint-Etienne-du Mont", por el realismo de su perspectiva.

Louis Daguerre pasará a la historia por inventar el diorama, instalación mediante la que se proporciona sensación de profundidad a las imágenes. Este invento despertó la atención del público parisino en un espectáculo que consistía en crear la ilusión al espectador de que se encontraba en otro lugar a través de imágenes enormes, que se podían mover y que se combinaban con un juego de luces y sonidos, etc., para que pareciese que el espectador estaba en situaciones como una batalla, una tempestad, etc. Para que todo esto fuera creíble, las pinturas debían ser muy realistas y por esta razón, a Daguerre le interesaba la aplicación del principio de la cámara oscura al Diorama.

Sus instalaciones llegaron a la Ópera de París y su éxito fue tal que fue condecorado con la Legión de honor (Francia).

El diorama fue un espectáculo visual diseñado por Daguerre en el que se mostraban unas imágenes de paisajes naturales, interiores de capillas u otras vistas mediante elaboradas técnicas escenográficas que incluían movimientos como el de las nubes o el de un sol que al pasar cambia las tonalidades del paisaje. Así, con juegos de luces, transparencias, efectos sonoros, elementos en relieve y otros efectos, se conseguía recrear con gran realismo distintos entornos. Daguerre patentó su diorama en 1823, un año después de poner en marcha su primer espectáculo en París, en el que se mostraban dos paisajes recreados detalladamente en imágenes de 21,3 x 13,7 metros, visibles a través de un marco de 7,3 X 6,4 metros a 12 metros de distancia del espectador. Este espectáculo que cosechó éxitos durante casi veinte años, se presentaba en un edificio especialmente creado para la ocasión, con una palco de más de 300 espectadores. La zona del público estaba compuesta por unos asientos sobre una plataforma giratoria que, tras el visionado de la primera imagen, giraba hacia la segunda. El invento, como ocurriera con su Daguerrotipo (basado en el invento de Joseph Nicéphore Niépce), no es realmente suyo, sino que supo ver los deseos de un público que ya empezaba a reclamar espectáculos como ese, que realizaban desde antes, aunque en escala menor, otros diseñadores escénicos como Philippe-Jacques Loutherbourg (1740-1812) con su "Eidophusikon" o Franz Niklas König (1765-1832) con su “Diaphanorama”.


Actualmente, se le llama diorama al modelo tridimensional de paisaje que muestra acontecimientos históricos, naturaleza, ciudades, etc, usado para la educación o el entretenimiento, confeccionado con materiales o elementos en tres dimensiones, que conforman una escena de la vida real. Se ubican delante de un fondo curvo, pintado de manera tal que simule un entorno real y se completa la escena con efectos de iluminación. Se pueden representar animales, plantas, batallas, paisajes, etc.

Su segundo invento fue el daguerrotipo, el primer procedimiento fotográfico dado a conocer públicamente, en el año 1839, en París.

Daguerre seguía con sumo interés los descubrimientos que acerca de la fotografía se realizaban en aquella época. Utilizaba la cámara oscura para hacer maquetas de sus vastas composiciones, y empezó a ocuparse seriamente en reproducir sus trabajos. Hizo algunos ensayos con sustancias fosforescentes, pero la imagen era fugaz y visible tan solo en la oscuridad. Daguerre trabajó en numerosas ocasiones con el óptico Charles Chevalier quien lo puso en contacto con Joseph Nicéphore Niépce, al conocer los experimentos que éste estaba realizando en la fijación de imágenes de la cámara oscura. 
El 5 de diciembre de 1829 firmaron un contrato de sociedad, en el que Daguerre reconocía que Niepce "había encontrado un nuevo procedimiento para fijar, sin necesidad de recurrir al dibujo, las vistas que ofrece la naturaleza".
Fueron varios días los que Daguerre y Niepce estuvieron trabajando juntos. Cada uno informaba al otro sobre sus trabajos, a veces con recelo, otras veces con más espontaneidad. Trabajaban con placas sensibles de plata, cobre y cristal. Hacían uso de vapores para ennegrecer la imagen.

Sin volver a verse, al morir Niépce en 1833, Daguerre continuó investigando. Más tarde, en 1835, hizo un descubrimiento importante por accidente. Puso una placa expuesta en su armario químico y encontró después de unos días, que se había convertido en una imagen latente, por efectos del mercurio que se evaporaba y actuaba como revelador.
Daguerre perfeccionó el daguerrotipo hasta 1838.
El daguerrotipo no permitía obtener copias, ya que se trata de una imagen positiva única. Además, los tiempos de exposición eran largos y el vapor de mercurio tenía efectos tóxicos para la salud.

En 1833 fallece Joseph Niépce sin que el invento se hiciera público y dos años más tarde, en 1835, Daguerre aprovecha los problemas económicos de Isidore, hijo de Joseph Niépce, para modificar el contrato suscrito, lo que supone que el nombre de Daguerre pase a aparecer por delante del nombre de Niépce, a cambio de que los derechos económicos del padre le sean reconocidos al hijo Isidore.

En ese mismo año se produce una tercera modificación del contrato que supone la desaparición del nombre de Niépce y que el procedimiento pase a llamarse «Daguerrotipo». Unos pocos años después, en 1838, Louis Daguerre tomaba en el Boulevard du Temple la primera fotografía, en la que aparece una persona. 

Daguerre perfeccionó el procedimiento fotográfico ensayado por Niépce. Utilizó placas de cobre plateado, sensibilizadas en vapores de yodo. Consiguió buenos revelados a partir de vapores de mercurio. Y fijó las imágenes en agua salada muy caliente. Estas fueron las tres grandes innovaciones de Daguerre. Como resultado obtuvo imágenes muy nítidas y de calidad permanente.

Rápidamente en la ciudad de París en un año se hicieron 500 000 daguerrotipos. Daguerre, ayudado por su cuñado, consigue sacar al mercado la cámara llamada "Daguerrotype", la cual era numerada y llevaba la firma de Daguerre. El manual explicativo del procedimiento del daguerrotipo fue traducido a los principales idiomas.

En 1838 se tomó la que se cree es la primera fotografía de personas vivas. La imagen muestra una calle muy concurrida (el Bulevar del Temple parisino). Sin embargo, debido al largo tiempo de exposición para impresionar la imagen –alrededor de quince minutos en las horas de máxima irradiación–, no aparece el tráfico u otros transeúntes, pues se mueven demasiado rápido. Las únicas excepciones son un hombre y un chiquillo que limpiaba sus botas, que permanecieron en la misma posición durante el tiempo que tardó la exposición del daguerrotipo. Según la investigación de la historiadora Shelley Rice, el limpiabotas y su cliente son actores ubicados allí por Daguerre, quien previamente habría tomado otra fotografía del mismo lugar, notando la incapacidad de la técnica fotográfica de aquel momento para dejar registro de la intensa actividad humana de ese lugar.

El 7 de enero de 1839, en la Academia de las Ciencias en París presentó públicamente el invento.
Posteriormente, el Estado francés compró el invento por una pensión vitalicia anual de 6000 francos para Daguerre y otra de 4000 francos para el hijo de Joseph Niépce, con el objetivo de poner a disposición de la ciudadanía el invento, lo cual permitió que el uso del daguerrotipo se extendiera por toda Europa y los Estados Unidos.

Con la aportación de Daguerre, se consiguió reducir a un período comprendido entre los cinco y los cuarenta minutos el tiempo necesario para la toma de imágenes, frente a las dos horas necesarias con el procedimiento de Niépce, lo cual suponía un salto enorme en quince años.

A partir de este momento Daguerre comienza a trabajar en la mejora del procedimiento químico con el empleo del yoduro de plata y el vapor de mercurio, así como con la disolución del yoduro residual en una solución caliente a base de sal común.

De este mismo año es el daguerrotipo más antiguo conocido. Bajo el nombre de "Composición" nos encontramos ante un bodegón de diversos objetos que presenta una imagen más volumétrica, con mayor profundidad y mejores relieves.

Durante los años 1838 y 1839 se dedicó a promocionar el invento por diversos medios como su intento de crear una sociedad de explotación por suscripción pública que fracasó o las operaciones de tomas de vistas realizadas por las calles de París. Gracias a sus actuaciones logró contactar con François Aragó, científico y político liberal, quien en el año 1839 presentó ante la Academia de Ciencias Francesa públicamente el invento.

Daguerre logró un reconocimiento unánime por todo el mundo, recibiendo nombramientos de academias extranjeras y condecoraciones francesas y extranjeras, ocultando los verdaderos logros de Joseph Nicéphore Niépce como predecesor de sus investigaciones. Poco a poco la verdad se fue conociendo y finalmente acabó reconociendo las aportaciones de Niépce.

Hasta la fecha de su muerte, el 10 de julio de 1851, en Bry sur Marne, se dedicó a la fabricación en serie de material fotográfico, junto a su cuñado Giroux, y a la organización de demostraciones en público del invento.



 


</doc>
<doc id="19378" url="https://es.wikipedia.org/wiki?curid=19378" title="Ananas comosus">
Ananas comosus

Ananas comosus (ananá, ananás, matzatli o piña) es una especie de la familia de las bromeliáceas, nativa de América del Sur. Planta de escaso porte y con hojas duras y lanceoladas de hasta 1m de largo, fructifica una vez al año produciendo un único fruto fragante y dulce, muy apreciado en gastronomía.

Aunque la mayoría de las bromeliáceas son epifitas, "A.comosus" es una planta vivaz, terrestre, aparentemente acaule, con una roseta basal de hojas rígidas, sésiles, lanceoladas, estrechamente imbricadas, con los márgenes dotados de espinas de puntas cortas, de 30 a 100cm de largo; son ligeramente cóncavas, para conducir el agua de lluvia hacia la roseta. El tallo, rojizo, se hace visible alrededor de los tres años, creciendo longitudinalmente hasta alcanzar entre 1 y 1,5m. De las axilas foliares aparecen pequeños que los cultivadores cortan para la reproducción, aunque si se dejan pueden producir más frutos.

Del tallo brotan inflorescencias en forma de espiga, con el tallo engrosado, formadas por varias docenas de flores trímeras de color violáceo, que aparecen al final de un escapo en las axilas de las brácteas. Las flores son hermafroditas, sésiles, con brácteas inconspicuas, los tépalos externos apenas asimétricos y libres, de ovario súpero. El período de floración se extiende por un mes o más; la planta es autoestéril, un rasgo seleccionado por los criadores para favorecer la reproducción vegetativa. La polinización está a cargo, en su entorno natural, de los colibríes.

El fruto es una pequeña baya, que se fusiona tempranamente con las adyacentes en un sincarpio o infrutescencia, grande y de forma ovoide. El corazón del sincarpo más fibroso se forma a partir del tallo axial engrosado, y las paredes del ovario, la base de la bráctea y los sépalos se transforman en una pulpa amarilla, apenas fibrosa, dulce y ácida, muy fragante, que no guarda rastro de los frutos que la compusieron. La flor propiamente dicha se transforma en un escudete octogonal de cubierta dura, formada por la fusión del ápice de la bráctea y los tres sépalos, que formará la dura piel cerúlea y espinosa del fruto. La cavidad de la flor endurece sus paredes; según el cultivar aparece como una celdilla vacía junto a la piel, en la que se conservan los restos duros y filiformes de los estambres, o se reduce a unas ranuras. Más hacia el interior, las celdas del ovario, que contienen las semilla en el raro caso de fertilización, también se estrechan considerablemente. Estas últimas son de tamaño bimilimétrico, arrugadas, de forma amigdaloide y de color pardo más o menos oscuro.

Su aroma se debe al butirato de etilo.

El ananá es un cultivo claramente tropical. Acepta cualquier tipo de suelo, siempre que cuente con buen drenaje; los suelos anegados pueden causar la podredumbre de las raíces. Es ligeramente acidófilo, prefiriendo un pH entre 5,5 y 6; exige buenas concentraciones de nitrógeno y potasio, algo de magnesio y cantidades limitadas de calcio y fósforo. No tolera las heladas ni las inundaciones, y requiere de altas temperaturas para fructificar, alrededor de los 24°C; los excesos de calor, superando los 30°C, perjudican la calidad del fruto al exacerbar el ciclo metabólico; el régimen de lluvias debe estar entre los 1000 y 1500mm anuales. No crece normalmente por encima de los 800(metros sobre el nivel del mar), aunque existen plantaciones aisladas en Colombia, Kenia y Malasia en zonas de altitud.

Originario de algún lugar no especificado de Sudamérica, probablemente provenga del Cerrado, específicamente del Altiplano Goiaseño. Los estudios de diversidad sugieren que se originaría entre Brasil, Paraguay y Argentina (es decir, la zona de nacimiento de la cuenca del Plata), desde donde se difundió al curso superior del Amazonas y la zona de Venezuela y las Guayanas. Hacia el 200d.C. fue cultivado en Perú por los mochica, quienes lo representaron en su cerámica. En el siglo XVI se propagó hacia Europa y las zonas tropicales de África y Asia.

El fruto para su consumo puede estar fresco y en conserva. En Occidente se usa habitualmente como postre y en ensaladas, aunque cada vez más como ingrediente dulce en preparaciones de comida oriental. Cuando la piña está madura, la pulpa es firme pero flexible, las hojas se pueden arrancar de un fuerte tirón y el aroma es más intenso en la parte inferior. Debido al coste del transporte del fruto fresco y la concentración del consumo, se producen numerosos subproductos industrializados, en especial zumos, yogures, helados y mermeladas. Del jugo se produce un vinagre excelente y muy aromático.

Es el ingrediente principal de algunos cócteles, como la piña colada. En México se elabora el tepache, una bebida refrescante fermentada que utiliza como base la cáscara de la piña.

Aunque la enzima proteolíctica llamada bromelina se concentra en los tallos, si el jugo la contiene en cantidad suficiente, se puede usar como un ablandador de carnes.

Entre las propiedades medicinales del fruto, la más notable es la de la bromelina, que ayuda a metabolizar los alimentos. Es también diurético, ligeramente antiséptico, desintoxicante, antiácido y vermífugo. Se ha estudiado su uso como auxiliar en el tratamiento de la artritis reumatoide, la ciática, y el control de la obesidad.

La alta concentración de bromelina en la cáscara y otras partes ha llevado a su uso en decocto para aliviar infecciones laríngeas y faríngeas, así como en uso tópico para la cistitis y otras infecciones.

Según algunos estudios, la bromelina produce autofagia en células del carcinoma mamario, lo que promueve el proceso celular de la apoptosis.

La piña puede plantarse en cualquier momento del año en suelos húmedos, aunque la mejor época es el otoño. Es rara la reproducción a partir de semilla. Más frecuentemente se utilizan los retoños del tallo central; los mejores proceden de la parte basal del mismo, aunque también pueden usarse las yemas del tallo distal o la corona de brácteas de la fruta. Naturalmente, los brotes basales se desarrollan, fructifican y dan a su vez origen a nuevos tallos. Los distintos tipos de retoños se conocen como "corona" (el meristemo apical), "gallo" (las yemas pedunculares) y "clavos" (vástagos de la yema peduncular).

Los vástagos se plantan en línea, dejando 40-45cm entre plantas y algo más entre hileras, o más si se aplicará pulverización mecánica con herbicidas, con una densidad total de 37500 a 50000 plantas por hectárea. Las plantaciones de fruta con destino industrial son más apretadas, de hasta 80000 plantas. Se desmaleza dos veces al año; la alternativa es el rociado con herbicidas, en especial ametrina, diuron e incluso uracilos como el caso del bromacil. Se fertiliza tri o bianualmente con nitrógeno, potasio y fósforo, de 5 a 6g por planta, a los que se añade a veces magnesio. En zonas de heladas, la planta debe cubrirse durante la temporada de frío.

La cosecha principal se efectúa normalmente desde principios de verano hasta comienzos de otoño. Es un fruto no climatérico, es decir, que hay que cosecharlo ya maduro, pues una vez cortado, la maduración se detiene por completo y empieza a deteriorarse. La piña es poco sensible a la presencia de etileno, y tiene baja producción de esta fitohormona. Las condiciones más apropiadas para su conservación son temperaturas de 7 a 13°C y humedad de 85-90%. La vida en postcosecha en condiciones de conservación óptimas alcanza entre 2 y 4 semanas.

El rendimiento del 30% se considera aceptable, es decir, de 12000 a 18000 frutos de entre 1 y 2,5kg por hectárea. Normalmente las plantas se renuevan cada dos ciclos de cosecha para evitar la disminución del rendimiento. Con el uso de etefón puede inducirse la floración para regular el ciclo productivo.

Algunos cultivares han sido seleccionados para mejorar el rendimiento de la fruta para envasado (piñas peroleras): generalmente el fruto es cilíndrico y alargado.


Hoy la piña es el segundo cultivo tropical en volumen, sólo superado por el plátano ("Musa paradisiaca"), y conforma más del 20% de la producción comercial de este tipo de frutos, de la cual el 70% se consume fresca en el país de origen. El resto se destina al enlatado en almíbar, una práctica iniciada en Hawái en el siglo XVIII, que es la forma más consumida en los países templados.

Los principales productores son Costa Rica, Brasil, Filipinas, Indonesia, India, que concentran el 50% de la producción. Otros productores de relieve son Kenia, México y Nigeria, Tailandia y China. El cultivar más importante es el llamado 'smooth Cayenne', originario de la Guayana Francesa.

"Ananas comosus" fue descrita primero por Carlos Linneo como "Bromelia comosa" y publicado en "Herbarium Amboinenese", vol.21, 1754 y, posteriormente, atribuido al género "Ananas" por Elmer Drew Merrill y publicado en "An Interpretation of Rumphius's Herbarium Amboinense", 133, 1917.

El término «piña» se adoptó por su semejanza con el cono de una conífera. La palabra "ananá" es de origen guaraní, del vocablo "naná naná," que significa ‘perfume de los perfumes’.




</doc>
<doc id="19379" url="https://es.wikipedia.org/wiki?curid=19379" title="Ribavirina">
Ribavirina

La ribavirina también conocida como virazole es un nucleósido sintético en el que la base nitrogenada es la triazolcarboxamida, que actúa como antiviral. La ribavirina se puede administrar por vía oral, vía tópica y vía inhalatoria.

La ribavirina inhibe "in vitro" el crecimiento de virus tanto de ADN como de ARN, tales como mixovirus, paramixovirus, arenavirus, bunyavirus, virus del herpes, adenovirus y poxvirus.

La ribavirina sufre un proceso de fosforilación en las célula infectada utilizando enzimas tisulares como la adenosin kinasa.

La ribavirina monofosfato inhibe la síntesis de guanosín monofosfato, reduciendo sus niveles intracelulares. La ribavirina trifosfato inhibe la enzima mRNA-guanililtransferasa inhibiendo la síntesis de ARN mensajero vírico y también ARN polimerasa. A altas concentraciones in vitro también inhibe la transcriptasa inversa del VIH. En el caso de Hepatitis C ejerce su afecto antiviral a través diversos mecanismos, entre ellos mutagénesis letal.

La ribavirina puede producir anemia macrocítica, alteraciones neurológicas y gastrointestinales. Por vía inhalatoria puede producir irritación conjuntival y erupciones cutáneas. Por vía intravenosa anemia, aumento de la bilirrubina, hierro y ácido úrico.

No se recomienda durante el embarazo por su capacidad teratogénica.



</doc>
<doc id="19380" url="https://es.wikipedia.org/wiki?curid=19380" title="Emperador">
Emperador

Un emperador (del término latino "imperator") es el monarca soberano de un imperio o un monarca que tiene como vasallos a otros reyes. Es el título de mayor dignidad, por encima del rey, y su equivalente femenino es emperatriz para referirse a la esposa de un emperador (emperatriz consorte), a la madre (emperatriz viuda) o a una mujer que gobierna por derecho propio (emperatriz titular o reinante).

El Emperador de Japón es el único monarca reinante en la actualidad cuyo título se traduce como "emperador".

En el año 27 a. C., Octavio Augusto unificó el mundo romano y estableció la entidad política conocida generalmente como Imperio romano por oposición a la República Romana, pero no se atrevió a asumir poderes absolutos y quebrar de esta manera el sistema político de la Roma, debido al ejemplo que representaba el asesinato de Julio César el año 44 a. C., precisamente acusado por los senadores de querer acabar con las libertades civiles republicanas. De esta manera creó el principado, un régimen político en el cual se mantenían todos los cargos y formas republicanas, pero todos los grandes cargos públicos eran asumidos por Octavio. De esta manera, Octavio se garantizaba el control efectivo del "Imperium". Aunque este aceptó para sí tan solo el título de "princeps civium" (esto es, "el primero de los ciudadanos"), en la práctica el título más importante que quizá tenía era el de "Imperator" o jefe del ejército, porque era este el último garante de la paz romana después de las cruentas guerras civiles libradas en el último siglo. Esto, como se ha visto, no impidió que el Senado "saludara" a otros "imperator".

Durante los dos siglos siguientes, los emperadores romanos eran usualmente referidos como "princeps", esto es, "príncipes", dado que el clima político y de paz favorecía el predominio de la función civil del emperador. Sin embargo, a raíz de la Crisis del siglo III, cuando el mando del Imperio pasó a estar en manos de caudillos militares, el monarca romano fue adquiriendo un cariz mucho más militar, hasta el punto de que, dado el clima de inestabilidad, su única garantía para mantenerse en el poder era su fortaleza como caudillo militar. De esta manera, el uso del título "Imperator" se generalizó, y con el paso del tiempo se fue identificando el título de emperador con el de amo y señor absoluto de un imperio.

Una vez derrumbado el Imperio romano de Occidente, el Imperio bizantino se consideró continuador de la tradición romana, aunque abandonó la lengua y la cultura latina por la helénica que predominaba en los territorios orientales del Imperio romano. Tras la muerte de Justiniano I, la pretensión de continuidad con los emperadores romanos fue abandonada poco a poco, y se sustituyó el latín, hasta entonces la lengua administrativa del Imperio bizantino, por el griego. Así, a partir de Heraclio, los emperadores bizantinos se hicieron llamar con el término griego "basileus", que significa "rey". Sin embargo, en el ámbito oriental comenzó a cobrar fuerza también el título de zar, derivado del nombre latino César, y que se aplicó después al zar de Bulgaria, para ser tomado en su momento por los zares del Imperio ruso, una vez caída Constantinopla en manos de los otomanos.

En Occidente no existieron emperadores desde el año 476, pero la Iglesia de Roma se consideraba continuadora del Imperio en el campo espiritual. Inicialmente los papas romanos reconocieron a los emperadores bizantinos como continuadores de la tradición imperial romana, pero las crecientes desaveniencias entre ambos, debidas a las continuas injerencias de los emperadores bizantinos para forzar las elecciones papales y al desinterés que mostraron por la defensa de Roma ante las invasiones bárbaras, hicieron que el papado dirigiera la mirada hacia el creciente poder político de los francos. De esta manera los papas llamaron a Pipino el Breve en su ayuda para que acabara con la amenaza de los longobardos, entronizándolo como rey de los francos en recompensa.
Su hijo Carlomagno fue coronado Emperador de Occidente el año 800 en Roma, en un sorpresivo gesto del papa León III; oficialmente se sostiene que por agradecimiento ante su intervención durante una revuelta en Roma, pero es posible que haya sido motivado por el creciente acercamiento entre Carlomagno e Irene, a la sazón emperatriz de Bizancio, lo que iba en contra de sus intereses. En el año 812, el emperador bizantino Miguel I Rangabé reconoció a Carlomagno como emperador de Occidente a través de un tratado firmado en Aquisgrán, aunque esta aceptación fue endeble, ya que Bizancio consideraba que la nueva realeza germana no tenía lazo jurídico alguno con el Imperio romano, mientras que el Imperio bizantino sí que era (en el papel, al menos) sucesor legal de este, en Oriente.

El uso moderno del título emperador nace realmente en ese momento. Cuando Carlomagno es coronado en Roma, rescata el título militar romano de "Imperator", pero olvidando su cariz militar pasa a tomarlo como sinónimo de "rey". De esta manera, el monarca de un territorio extenso como el de Carlomagno investido con alguna legitimación divina viene a ser tratado como un emperador.

Después de que los nietos de Carlomagno se repartieran su imperio en el Tratado de Verdún (843), hubo varios advenedizos que con mayor o menor fortuna intentaron hacerse reconocer como emperadores, tratando de forzar al papa mediante el envío de expediciones militares a Italia. Finalmente el año 962 el rey germano Otón el Grande consiguió ser coronado legítimo emperador de Occidente, lo que le proclamaba como heredero de Carlomagno, fundándose así el Sacro Imperio Romano Germánico. Sus sucesores conservarían el título hasta el año 1806, aunque el propio Sacro Imperio Romano Germánico fue prácticamente desmantelado en 1648, después del Tratado de Westfalia.

Durante siglos se admitió en Occidente que el papa, como legítimo custodio de la tradición romana, era el único capaz de designar emperadores. Supuestamente, en la concepción medieval, el mundo estaba bajo la tutela temporal del emperador, y la espiritual del papa, como señores conjuntos del mundo cristiano (y del mundo, en definitiva, por cuanto el papa se consideraba Vicario de Cristo para toda la Humanidad). De esta manera solo podía haber un único emperador, con jurisdicción sobre todos los reyes cristianos. 

Sin embargo, este sueño estuvo lejos de cumplirse, ya que nunca el emperador de Occidente consiguió imponerse a todos los reyes cristianos; quien llevó más lejos este sueño universalista fue el emperador Carlos V del Sacro Imperio, rey de España, precisamente en una época (el siglo XVI) en que el universalismo medieval estaba desapareciendo en beneficio del nacionalismo moderno. Sin embargo, aunque los reyes medievales se trenzaran en múltiples guerras, e incluso muchos de ellos combatieran por las armas al emperador de turno, jamás intentaron tomar para sí el título por no contar con las bases legales para ello.

Se denomina Imperio español a la unión de territorios conquistados, heredados y reclamados por España o por las dinastías reinantes en España; aunque en algunos de ellos, tales como las grandes praderas de América del Norte o la parte más austral de América del Sur, la presencia estable española fue muchas veces más teórica que real. Alcanzó casi los 20 millones de kilómetros cuadrados a finales del siglo XVIII. Durante los siglos XVI y XVII creó una estructura propia y no fue llamado «imperio colonial» hasta el año 1768, siendo en el siglo XIX cuando adquirió estructura puramente colonial.

No existe una postura unánime entre los historiadores sobre los territorios concretos poseídos por España porque, en ocasiones, resulta difícil delimitar si determinado lugar era parte de España o formaba parte de las posesiones del rey de España, especialmente en una época en la que no estaba clara la diferencia entre las posesiones del rey y las del país donde residía, como tampoco lo estaba la hacienda o la herencia. Así, tradicionalmente se considera a los Países Bajos como parte del mismo (tesis mayoritaria en España y los Países Bajos entre otros); pero existen autores como Henry Kamen que proclaman que esos territorios nunca se integraron en el Imperio español, sino en las posesiones personales de los Austrias.

El Imperio español fue el primer imperio global, porque por primera vez un imperio abarcaba posesiones en todos los continentes, las cuales, a diferencia de lo que ocurría en el Imperio romano o en el Carolingio, no se comunicaban por tierra las unas con las otras. Sin embargo, su mandatario no asumió el título de emperador (salvo el rey Carlos I que lo era de Alemania) sino el de rey de España.

Sin embargo, la llegada de Napoleón Bonaparte cambiaría las cosas. Durante el siglo XVIII se había producido un fuerte renacimiento del clasicismo romano (Neoclasicismo), que se había vinculado a la idea de que la Razón iba a superar el oscurantismo que se identificaba con la Edad Media. En lo político (y también en lo artístico), Napoleón trató de regresar al modelo imperial romano, por lo que se transformó en cabecilla de un gobierno directorial a la manera romana: el Consulado. Napoleón finalmente mandó llamar mediante presiones y amenazas al papa Pío VII para coronarse emperador en la catedral Notre Dame de París, en el año 1804. Cambió la tradición al acordar previamente con Pío que él mismo se pondría la corona en la cabeza, lo cual fue aceptado por el Papa, que se limitó a dar su bendición. Esto significaba que el papa ya no era fuente de legitimación del emperador, que lo era por sus méritos y no por derecho divino.

Sin embargo, todavía seguía existiendo el emperador del Sacro Imperio Romano Germánico, a la sazón Francisco II, quien optó por renunciar a su título en 1806 y adoptar el de Emperador de Austria con el nombre de Francisco I. El Imperio austríaco seguiría siendo, tras el Congreso de Viena de 1815, el heredero del Sacro Imperio y, tras la derrota contra Prusia en la guerra austro-prusiana de 1866, pasaría a llamarse Imperio austrohúngaro. En 1871 el rey Guillermo de Prusia, después de la Guerra franco-prusiana, y considerándose legítimo heredero del Sacro Imperio Romano Germánico, al haber derrotado previamente a Austria-Hungría, se proclamó emperador de Alemania. Ambos imperios, austrohúngaro y alemán, serían abolidos en 1918 y con ellos se extinguiría la línea del Imperio romano de Occidente. La del Imperio romano de Oriente habría desaparecido el año anterior, en 1917, con la caída de los zares en Rusia.

No obstante, el gesto de Napoleón no solo fue calificado como una usurpación por parte de un advenedizo sin títulos legales ni jurídicos para su acción, sino que además abrió la espita para otros, que acto seguido se proclamaron emperadores en otros lugares. De esta manera, en Haití reinaron los emperadores Jacobo I de Haití (1804-1806) y Faustino I (1847-1859). En México, el general Agustín de Iturbide se proclamó como emperador Agustín I en 1821, aunque sería derrocado al año siguiente; en 1864 asumiría Maximiliano I, entronizado por los ejércitos de Napoleón III, y duraría en funciones hasta ser fusilado en 1867. Al año siguiente de la entronización de Agustín Iturbide, en 1822, se proclamó el Imperio de Brasil, con Pedro I de Brasil como emperador y después su hijo Pedro II de Brasil; este duraría hasta la proclamación de la República en 1889. 

En Vietnam en 1925 llegó al poder, Bao Dai y en 1926 en Japón, Hirohito.

En 1935, el líder de Italia, Benito Mussolini invadió Etiopía y derrocaron al Emperador Haile Selassie, quien retorno de su Exilio en 1941 en el Reino Unido.

La reina Victoria de Inglaterra se proclamaría, a su vez, Emperatriz de la India. Y en 1976 el general africano Jean Bédel Bokassa (admirador de Napoleón Bonaparte) transformó la República Centroafricana en Imperio Centroafricano, y él mismo se proclamó emperador Bokassa I en una desmesurada ceremonia; este dictador duraría hasta 1979, año en que fue derrocado por una sublevación popular.

Sin embargo, uno de los imperios injustamente ignorados - por los accidentes históricos que forzaron su disolución - es el Reino del Congo, que desde el año 1395 al 1885, constituía un estado altamente desarrollado situado en el centro de una extensa red de intercambios comerciales que abarcaba todos los territorios en lo que actualmente constituye la zona norte de Angola, el enclave de Cabinda, la República del Congo y la parte occidental de la República Democrática del Congo,y en su época de mayor expansión, se extendía entre el océano Atlántico y los ríos Kwango al este, Congo al norte y Loje al sur; es decir, principalmente lo que son los países africanos de habla francesa y portuguesa,(incluidos los territorios de la actual República de Guinea Ecuatorial). Además de sus recursos naturales y marfil, este imperio, gobernado bajo un único emperador, el Manikongo, fabricaba y comerciaba con toda clase de objetos de cobre, tejidos de rafia y cerámica. El pueblo congo hablaba el idioma kikongo. El Reino del Congo - que, al igual que se observa del ejemplo del rey del imperio español Carlos V, del Sacro Imperio Español, no utilizaron el título de ´´emperador´´, pues tenían su propia denominación (Manikongo), fue la principal víctima de las dos principales fenómenos que han definido la historia reciente de la humanidad: la esclavitud y la colonización

Algunos títulos de monarcas han sido traducidos a las lenguas europeas como "emperador", pese a que no guardan relación con el Imperio romano ni sus estados sucesores. Así, los soberanos de Persia o Irán han recibido el título de emperadores desde la creación del Imperio persa hasta su disolución en 1979. Puesto que "Sah" se tradujo como emperador, el término "Sahbanu" que solamente utilizó Farah Diba ha sido traducido como emperatriz. En China el título del monarca era el de "Wáng", traducido como rey, y cuando se unificó el país pasó a ser "Huángdì" lo que se ha traducido como emperador y duró hasta 1912 con la deposición de Puyi. Si bien los soberanos de los grandes estados islámicos no han recibido por la historiografía este título, sino el de califa o sultán, dichos estados si que han sido llamados imperios, por lo que se habla del Imperio abásida, el Imperio Omeya o, más tarde, del Imperio turco u otomano.

En la actualidad, el único gobernante del mundo que conserva el título de emperador es el de Japón, si bien no tiene ninguna relación con el título de origen romano y es la traducción al castellano de la palabra "tenno", que también puede ser entendida como "rey" o "monarca".

Hay pretendientes a los tronos que de varios países, que de ser restaurada la monarquía se convertirían presumiblemente en emperadores, como es el caso de la Gran duquesa María Vladímirovna de Rusia, Luis de Orleans-Braganza de Brasil y Carlos Felipe de Habsburgo de México.



</doc>
<doc id="19381" url="https://es.wikipedia.org/wiki?curid=19381" title="Jornada del foso de Toledo">
Jornada del foso de Toledo

La jornada del foso fue un hecho histórico supuestamente acaecido en la ciudad de Toledo (España), en el siglo , que viene recogido en los cronicones toledanos de los siglos y , como "Historia o Descripción de la Imperial Ciudad de Toledo" (1554) de Pedro de Alcocer. Sin embargo, algunos expertos, como el filólogo y arabista Álvaro Galmés de Fuentes, sobrino nieto de Ramón Menéndez Pidal, cuestionan la base histórica y sugieren que se trata de narraciones que perviven de una leyenda de época preislámica.

En el año 797 gobernaba en la España musulmana el emir árabe Alhakén I. Toledo era una ciudad sometida al emir, pero con autonomía propia. Su población estaba formada por visigodos, hispanorromanos (muladíes la mayoría), árabes y judíos (estos establecidos en el campo). Alhakén quiso terminar de una vez con la independencia y autonomía de que gozaba la ciudad y dispuso una trampa. Mandó como nuevo gobernador de Toledo a un muladí de su confianza, Amrus ben Yusuf (Jiménez de Rada le llama Ambroz). Para celebrar el nombramiento, el muladí invitó a su palacio a las personas más destacadas, ricas e influyentes, en total más de 400. Durante el banquete las degolló a todas y mandó arrojar sus cabezas a un foso preparado de antemano para el desenlace. Otras fuentes señalan que el motivo de Amrús era vengarse de la ejecución de su hijo, Yusuf, por la nobleza de la ciudad.

La frase "pasar una "noche toledana"", para indicar que no se ha dormido, puede referirse a estos sucesos narrados, si bien otras fuentes hacen referencia al calor «agobiante». Por otra parte, Sebastián de Covarrubias, en su "Tesoro de la lengua castellana o española" (1611), afirmaba que la noche toledana era aquella que «se pasa de claro en claro, sin poder dormir, porque los mosquitos persiguen a los forasteros que no están prevenidos de remedios como los demás». 

Numerosos autores han hecho referencia a la expresión «noche toledana» en sus obras, aunque con distintos temas, incluyendo Lope de Vega, que escribió una sátira de enredo y simulaciones, "La noche toledana" (1605), con motivo del nacimiento del príncipe Felipe, así como "Noche toledana" (1841) de Ventura de la Vega o "Una noche toledana" (1870) de Enrique Pérez Escrich.


</doc>
<doc id="19383" url="https://es.wikipedia.org/wiki?curid=19383" title="Mikado">
Mikado

Mikado puede referirse a:


</doc>
<doc id="19392" url="https://es.wikipedia.org/wiki?curid=19392" title="Onofre">
Onofre

San Onofre (, del egipcio: "Wnn-nfr", que significa «el que es continuamente bueno») (* alrededor de 320 en Etiopía, † en torno al año 400 quizás en Siria) fue un eremita y anacoreta que alcanzó la santidad y es muy venerado en la actualidad por las iglesias copta y católica.

Al parecer San Onofre era hijo de un rey egipcio, persa o abisinio (Abisinia era el antiguo nombre de la actual Etiopía). Se cree que nació hacia el año 320. Sus padres no podían tener hijos, pero la madre rogó tanto al cielo que al final concibió. Pero el diablo le hizo creer al rey que el niño era producto de una relación adulterina de su esposa. «Apenas nazca, tíralo al fuego», le dijo el maligno. Así lo hizo el rey, pero el niño resultó ileso gracias a la intervención de su ángel custodio. Este milagro hizo que sus padres se convirtieran a la fe de Cristo y lo bautizaran con el nombre egipcio de "Wnn-nfr" (en griego Ὀνούφριος), que significa «el que es continuamente bueno».

Desde niño estuvo rodeado de lujos y comodidades, pero siendo un adolescente salió un día de su palacio y conoció la pobreza, la angustia y la enfermedad, los males que agobian al pueblo. Esto le conmovió tanto que abandonó su vida principesca y solicitó ser admitido en un convento de Abage, en la región de Eremopolites, o Hermópolis, en medio del desierto de la Tebaida egipcíaca. 

En el convento convivió con cerca de cien monjes que, como él, habían abandonado los placeres mundanos para vivir en solitario una experiencia mística y espiritual, compartiendo los alimentos en común y llevando una vida en paz, dedicados en silencio a labores sencillas y a vencer en las luchas interiores. Allí se hizo muy devoto del profeta San Elías y de San Juan Bautista, el santo de los eremitas. Durante una hambruna que asoló al convento, la Virgen María salvó a Onofre de morir de hambre.

Cuando se hizo adulto abandonó el cenobio y se marchó a vivir como ermitaño, deseoso de alcanzar la unión con el amor de Dios. La tradición relata que una luminaria le acompañó por cerca de siete millas de camino en el itinerario hasta una choza. Al llamar a la puerta, le abrió un venerable anciano que era ermitaño desde hacía muchos años. Onofre cayó de rodillas, penetrado de admiración, y el anciano le dijo: «Te aguardaba, Onofre, que como ves, sabía de antemano tu nombre; ni me son desconocidos tus deseos, ni ignoro para lo que el cielo te reserva: persevera, pues, hijo, en tu propósito, y entra en mi choza a descansar algunos días».

Onofre estuvo algunos días con el anciano, quien le instruyó en las reglas de vida de los ermitaños. Después lo llevó al desierto y a unos cuatro días de camino llegaron a la región de Calidiomea, donde encontraron una palmera que daba sombra a una pequeña choza. El anciano le dijo: «Este es el lugar que Dios te señala», y permaneció con Onofre durante treinta días, luego de los cuales partió. Onofre se quedó a vivir allí y una vez al año salía para visitar a su maestro y aprender de sus sabias enseñanzas. Otras versiones afirman que la ermita de Onofre fue una cueva entre acantilados cerca de Göreme (Capadocia), en la actual Turquía.

Onofre solo comía dátiles y agua que sacaba de la palma que crecía junto a su choza. Otras versiones afirman que también comía hierbas del desierto, insectos y en algunas ocasiones miel. Como vestimenta solo tenía sus propios cabellos, de considerable longitud, y hojas de palma o hierbas del desierto entretejidas. Por las noches tiritaba de frío y en el día era abrasado por los rayos inclementes del sol. Un ángel le daba pan y vino los domingos, y de esta forma comulgaba. Sobrevivió así durante 60 años, dedicado a la oración, las mortificaciones y las austeridades. Muy pocas veces salió de su retiro para administrar el sacramento del bautismo. Cuando llevaba treinta años viviendo como ermitaño, murió su maestro espiritual, a quien Onofre enterró y le rindió honores.

En cierta ocasión, el abad san Pafnucio se internó en el desierto, en busca de los famosos monjes eremitas que seguían las enseñanzas de San Antonio Abad. Luego de cuatro días de camino, durante los cuales se alimentó solo con pan y agua, enfermó y se le acabaron las provisiones; a punto estuvo de morir, pero siguió caminando por otros cuatro días con sus noches, hasta que un ángel se le apareció y le dio fuerzas. 

Luego caminó durante diecisiete días más hasta que se encontró con un hombre de aspecto desagradable, cuyo cabello erizado le cubría todo el cuerpo, al estilo de las fieras; alrededor de la cintura se ceñía hierbas secas del desierto, y su barba era tan larga como su cabello. Pafnucio sintió miedo y trató de huir, pero el hombre, quien no era otro que Onofre, lo llamó diciéndole: «Sígueme, que yo estoy con Dios». Pafnucio se arrodilló a sus pies, pero Onofre le dijo: «Levántate, hijo mío, porque tú también eres siervo de Dios y de los Santos Padres». Onofre estaba en un estado lamentable de salud, su cuerpo se había deformado y las canas reflejaban su vejez.

Pafnucio le hizo compañía y le pidió que le contara la historia de su vida. Onofre así lo hizo, y poco después falleció, un 12 de junio del año 400. Pafnucio puso por escrito la vida y obras de san Onofre, y la tradición añade que cuando murió, un coro angélico le rindió honores y alabanzas, y unos leones mansos acompañaron a Pafnucio a enterrar el cuerpo del venerable anacoreta.

Se le representa como un santo provecto de luengas barbas y envuelto en sus propios cabellos.
También puede aparecer situado en el desierto y con una corona y un cetro tirados a sus pies, como símbolo del rechazo a las vanaglorias de este mundo. En ocasiones aparecen a su lado la Regla de Antonio Abad, el cráneo y la cruz que presidían sus meditaciones, la palmera de cuyos dátiles se alimentaba e incluso una alforja (símbolo de las raciones que nunca le faltaron).

San Onofre fue venerado primero en Constantinopla, y de allí pasó su culto a Occidente en la época de las cruzadas. En la actualidad es venerado en varias ciudades de España, en algunas regiones de Venezuela, incluyendo Caracas, en algunas zonas de México y Cuba y en la región Caribe de Colombia. Se le suelen ofrecer exvotos de agradecimiento por los favores recibidos. El 12 de junio de cada año se celebran fiestas, procesiones y peregrinaciones en su honor.

Es patrono de las ciudades de Algemesí, Alguazas, Cuart de Poblet, L'Alcúdia de Crespins, el principado de Mónaco, la ciudad alemana de Múnich, en donde se conserva una reliquia de su cabeza, además de Salobreña, La Lapa (Badajoz, Extremadura), Fuentes de León (Badajoz, Extremadura) y San Onofre (Sucre, Colombia). En Brunswick se conservan algunos de sus huesos.

Muchos fieles lo consideran el patrón de los desempleados; por eso le encienden una vela amarilla para pedir por el trabajo y para que nunca haya escasez de comida en las casas. También se le considera protector de los tejedores, de los que quieren conseguir casa propia, de los viudos, de los peluqueros, de los solitarios, de los penitentes, de los que pierden un ser querido y de los que quieren superar una adicción. En algunas regiones del mundo también se le invoca contra las enfermedades del ganado.

Uno de los grandes peligros de las devociones de muchos de los santos es que se toman como justificativos para la santería. La devoción de Santa Bárbara, por ejemplo, ha ido adquiriendo mayor fuerza entre los que hacen este tipo de práctica mágico-religiosa. San Onofre, igualmente, ha ido tomando mucho arraigo entre los santeros, y es utilizado para invocarlo, y por su intercesión buscar y pedir trabajo (véase uno de los rituales).

Otra de las tendencias es el alto valor de carácter apócrifo que tienen este tipo de literaturas. 
Entiéndase por apócrifo el afán de resaltar la santidad como intervención divina, alejándose muchas veces de una vida sencilla, ordinaria y cotidiana. El hecho mismo de comer con lobos, leones, y otras tipologías literarias parecidas, llevan a la exageración y se apartan de un estilo de vida totalmente humano. Así se cuenta en las pinceladas biográficas de San Onofre, que "Un ángel le daba pan a diario y los domingos también la comunión. Sobrevivió de esta guisa durante 60 años."

En el municipio de San José del Rincón existe una hacienda a su nombre, donde durante el siglo XIX fue representada una pintura en honor a este santo valuada en alrededor de 15 millones de pesos, desapareció durante el año 2013. 


</doc>
<doc id="19393" url="https://es.wikipedia.org/wiki?curid=19393" title="Pragmatismo">
Pragmatismo

El pragmatismo es una tradición filosófica centrada en la vinculación de la práctica y la teoría. Describe un proceso en el que la teoría se extrae de la práctica y se aplica de nuevo a la práctica para formar lo que se denomina "práctica inteligente". Posiciones importantes características del pragmatismo incluyen el instrumentalismo, el empirismo radical, el verificacionismo, la relatividad conceptual y el falibilismo. Existe un consenso general entre los pragmatistas de que la filosofía debe tener en cuenta los métodos y los conocimientos de la ciencia moderna.

La piedra angular del pragmatismo es la redención de la idea de verdad (y otras nociones como el bien y la belleza) en la filosofía post-kantiana. Aunque según los pragmatistas el conocimiento objetivo podría ser imposible, se puede redefinir la verdad como aquello que funciona desde nuestra limitada forma de experimentar la realidad.

El pragmatismo fue creado en los Estados Unidos a finales del siglo XIX. Charles Sanders Peirce (y su máxima pragmática) merece la mayor parte del crédito por el pragmatismo, junto con William James y John Dewey, contribuidores de finales del siglo XIX.

Pierce describió el pensamiento de la escuela con la siguiente máxima pragmática: «Considera los efectos prácticos de los objetos de tu concepción. Luego, tu concepción de esos efectos es la totalidad de tu concepción del objeto».

El pragmatismo valora la insistencia en las consecuencias como manera de caracterizar la verdad o significado de las cosas. El pragmatismo se opone a la visión de que los conceptos humanos y el intelecto representan el significado real de las cosas, y por lo tanto se contrapone a las escuelas filosóficas del formalismo y el racionalismo. También el pragmatismo sostiene que solo en el debate entre organismos dotados de inteligencia y con el ambiente que los rodea es donde, las teorías y datos adquieren su significado. Rechaza la existencia de verdades absolutas, las ideas son provisionales y están sujetas al cambio, a la luz de la investigación futura.

El pragmatismo, como corriente filosófica, se divide e interpreta de muchas formas, lo que ha dado lugar a ideas opuestas entre sí que dicen pertenecer a la idea original de lo que es el pragmatismo. Un ejemplo de esto es la noción de practicidad: determinados pragmáticos se oponen a la practicidad y otros interpretan que la practicidad deriva del pragmatismo. Esta división surge de las nociones elementales del término "pragmatismo" y su utilización. Básicamente se puede decir que, ya que el pragmatismo se basa en establecer un significado a las cosas a través de las consecuencias, se basa en juicios a posterioridad y evita todo prejuicio. Lo que se considere práctico o no, depende del considerar la relación entre utilidad y practicidad.

Una mala comprensión del pragmatismo da lugar a generar prejuicios cuando es todo lo contrario. En política se suele hablar de pragmatismo cuando en verdad el pragmatismo político se basa en prejuicios y apenas observa las consecuencias que no encajen con los prejuicios de base, que es muchas veces lo opuesto al sentido original del pragmatismo filosófico.

Para los pragmatistas, la verdad y la bondad deben ser medidas de acuerdo con el éxito que tengan en la práctica. En otras palabras, el pragmatismo se basa en la "utilidad", siendo la utilidad la base de todo significado.

Los principales rasgos del pragmatismo son:


La palabra pragmatismo proviene del vocablo griego "pragma" que significa "práctica" o "asunto" (situación concreta).

El pragmatismo como movimiento filosófico comenzó en los Estados Unidos en la década de 1870. Charles Sanders Peirce (y su "Máxima Pragmática") se le atribuye el mérito de su desarrollo, junto con los contribuyentes de finales del siglo XIX, William James y John Dewey. Su dirección fue determinada por los miembros del Club Metafísico, Charles Sanders Peirce, William James y Chauncey Wright, así como por John Dewey y George Herbert Mead.

El primer uso impreso del nombre de pragmatismo fue en 1898 por James, quien atribuyó a Peirce el haber acuñado el término a principios de la década de 1870. James consideró la serie "Ilustraciones de la lógica de la ciencia" de Peirce (incluida "La fijación de la creencia" (1877), y especialmente "Cómo hacer que nuestras ideas sean claras" (1878), como la base del pragmatismo.

A su vez, Peirce escribió en 1906 que Nicholas St. John Green había sido instrumental al enfatizar la importancia de aplicar la definición de creencia de Alexander Bain, que era "aquello sobre lo que un hombre está dispuesto a actuar". Peirce escribió que "de esta definición, el pragmatismo es poco más que un corolario, de modo que estoy dispuesto a pensar en él como el abuelo del pragmatismo ". John Shook ha dicho:" Chauncey Wright también merece un crédito considerable, ya que tanto Peirce como James recuerdan que fue Wright quien exigió un empirismo fenomenalista y falibilista como alternativa a la especulación racionalista".

Peirce desarrolló la idea de que la investigación depende de la duda real, no de la mera duda verbal o hiperbólica, y dijo que para entender una concepción de una manera fructífera: "Considere los efectos prácticos de los objetos de su concepción, ya que la concepción de esos efectos es la totalidad de su concepción del objeto", que luego llamó la máxima pragmática. Equivale a cualquier concepción de un objeto hasta el alcance general de las implicaciones concebibles para la práctica informada de los efectos de ese objeto. Este es el corazón de su pragmatismo como un método de reflexión mental experimental que llega a las concepciones en términos de circunstancias confirmatorias y confirmatorias imaginables, un método hospitalario para la generación de hipótesis explicativas, y propicio para el empleo y la mejora de la verificación. Típica de Peirce es su preocupación por la inferencia de las hipótesis explicativas como fuera de la alternativa fundamental habitual entre el racionalismo deductivista y el empirismo inductivista, aunque era un lógico matemático y uno de los fundadores de la estadística.

Peirce dio una conferencia y escribió sobre el pragmatismo para aclarar su propia interpretación. Al encuadrar el significado de una concepción en términos de pruebas imaginables, Peirce enfatizó que, dado que una concepción es general, su significado, su significado intelectual, equivale a las implicaciones de su aceptación para la práctica general, más que a cualquier conjunto definido de efectos reales (o resultados de pruebas). El significado clarificado de una concepción apunta hacia sus verificaciones concebibles, pero los resultados no son significados, sino logros individuales.

Peirce en 1905 acuñó el nuevo nombre pragmaticismo "con el propósito preciso de expresar la definición original", diciendo que "todo fue feliz" con los usos variados de James y Schiller del antiguo nombre "pragmatismo" y que, sin embargo, acuñó el nuevo nombre debido al uso creciente del viejo nombre en "revistas literarias, donde se abusa". Sin embargo, en un manuscrito de 1906 citó como causa sus diferencias con James y Schiller. Y en una publicación de 1908, sus diferencias con James y el autor literario Giovanni Papini. Peirce, en cualquier caso, consideró sus puntos de vista de que la verdad es inmutable y que el infinito es real, ya que los otros pragmáticos se oponen, pero se mantuvo aliado con ellos en otros asuntos.

El pragmatismo disfrutó de una atención renovada después de que Willard Van Orman Quine y Wilfrid Sellars utilizaran un pragmatismo revisado para criticar el positivismo lógico en la década de 1960. Inspirado por el trabajo de Quine y Sellars, una clase de pragmatismo conocido a veces como neopragmatismo ganó influencia a través de Richard Rorty, el más influyente de los pragmáticos de finales del siglo XX junto con Hilary Putnam y Robert Brandom. El pragmatismo contemporáneo puede dividirse ampliamente en una estricta tradición analítica y un pragmatismo "neoclásico" (como Susan Haack) que se adhiere al trabajo de Peirce, James y Dewey.

Algunos de los pensadores que sirvieron de inspiración para varios pragmatistas son los siguientes:


Algunas de las diversas posiciones interrelacionadas que a menudo son características de los filósofos que trabajan desde un enfoque pragmático incluyen:


Dewey, en "The Quest for Certainty", criticó lo que llamó "la falacia filosófica": - los filósofos a menudo dan por sentadas categorías (como la mental y la física) porque no se dan cuenta de que estos son conceptos meramente nominales que fueron inventados para ayuda a resolver problemas específicos. Esto causa confusión metafísica y conceptual. Varios ejemplos son el "Ser último" de los filósofos hegelianos, la creencia en un "reino del valor", la idea de que la lógica, porque es una abstracción del pensamiento concreto, no tiene nada que ver con el acto del pensamiento concreto, y así sucesivamente. David L. Hildebrand resume el problema: "La falta de atención a las funciones específicas que comprende la investigación llevó a realistas e idealistas a formular relatos de conocimiento que proyectan los productos de la abstracción extensa de vuelta a la experiencia." (Hildebrand 2003)

Desde el principio, los pragmáticos quisieron reformar la filosofía y ponerla más en línea con el método científico tal como lo entendieron. Argumentaban que las filosofías idealista y realista tenían tendencia a presentar el conocimiento humano como algo más allá de lo que la ciencia podía comprender. Sostenían que estas filosofías recurrían entonces a una fenomenología inspirada por Kant de las teorías de la correspondencia del conocimiento y la verdad. Los pragmatistas criticaban a la primera por su apriorismo, y a la segunda porque toma la correspondencia como un hecho inanalizable. El pragmatismo en cambio trata de explicar la relación entre el conocedor y el conocido.

En 1868, C.S. Peirce argumentó que no hay poder de intuición en el sentido de una cognición incondicionada por inferencia y ningún poder de introspección, intuitiva o de otro tipo, y que la conciencia de un mundo interno es por inferencia hipotética de hechos externos. La introspección y la intuición eran herramientas filosóficas básicas al menos desde Descartes. Argumentó que no hay una cognición absolutamente primera en un proceso cognitivo; tal proceso tiene su comienzo, pero siempre se puede analizar en etapas cognitivas más finas. Aquello que llamamos introspección no da acceso privilegiado al conocimiento sobre la mente: el yo es un concepto que se deriva de nuestra interacción con el mundo externo y no al revés (De Waal 2005, pp. 7-10). Al mismo tiempo, sostenía persistentemente que el pragmatismo y la epistemología en general no podían derivarse de los principios de la psicología entendidos como ciencia especial: lo que pensamos es demasiado diferente de lo que deberíamos pensar; en su serie ""Ilustraciones de la Lógica de la Ciencia"", Peirce formuló tanto el pragmatismo como los principios de la estadística como aspectos del método científico en general. Este es un punto importante de desacuerdo con la mayoría de los otros pragmáticos, que defienden un naturalismo y un psicologismo más profundos.

Richard Rorty amplió estos y otros argumentos en "Philosophy and the Mirror of Nature" en los que criticaba los intentos de muchos filósofos de la ciencia de crear un espacio para la epistemología que no tiene relación alguna con las ciencias empíricas y que a veces se considera como superior a ellas. W. V. Quine en su ensayo "Epistemology Naturalized" (Quine 1969), también criticó la epistemología "tradicional" y su "sueño cartesiano" de certeza absoluta. El sueño, argumentó, era imposible en la práctica, así como equivocado en teoría, porque separa la epistemología de la investigación científica.

Hilary Putnam ha sugerido que la reconciliación del antiescepticismo y el falibilismo es el objetivo central del pragmatismo estadounidense. Aunque todo el conocimiento humano es parcial, sin la capacidad de tener una "visión superior", esto no requiere una actitud escéptica globalizada, un escepticismo filosófico radical (a diferencia de lo que se llama escepticismo científico).

Peirce insistió en que (1) en el razonamiento, existe la presuposición, y al menos la esperanza, de que la verdad y lo real son descubribles y serían descubiertos, tarde o temprano, pero aún inevitablemente, por una investigación suficiente, y (2) contrariamente a la famosa e influyente metodología de Descartes en las Meditaciones sobre la Primera Filosofía, la duda no puede ser fingida o creada por mandato verbal para motivar una investigación fructífera, y mucho menos puede comenzar la filosofía en la duda universal. La duda, como la creencia, requiere justificación. La duda genuina irrita e inhibe, en el sentido de que la creencia es aquella sobre la cual uno está preparado para actuar. Surge de la confrontación con alguna cuestión de hecho recalcitrante específica (que Dewey llamó una "situación"), que desestabiliza nuestra creencia en alguna proposición específica. La indagación es entonces el proceso racionalmente autocontrolado de intentar regresar a un estado establecido de creencia sobre el asunto. Hay que tener en cuenta que el antiescepticismo es una reacción al escepticismo académico moderno a raíz de Descartes. La insistencia pragmática en que todo conocimiento es tentativo es bastante compatible con la tradición escéptica anterior.

Las teorías pragmáticas de la verdad afirman que una proposición es verdadera si resulta útil o funciona en la práctica. Así, la proposición «en verano hace calor» es verdadera si constituye una buena guía para la acción, esto es, si resulta útil para cualquier persona que la considere verdadera. Hay que entender el criterio de utilidad como una apelación a comprobar en la práctica la verdad de las proposiciones, sin caricaturizar la premisa básica. Si sucede tal y como la proposición indica, entonces es verdadera. Así pues, según la teoría de la utilidad, sólo podremos establecer la verdad de una proposición cuando la comprobamos en la práctica. Esta exigencia no se produce en la teoría de la correspondencia, en la que una proposición es verdadera si se corresponde con los hechos, aunque éstos no puedan comprobarse. Como es obvio, la comprobación de una proposición está sujeta a ciertas limitaciones: primero ha de ser verificable, y además la verificación no es infalible.

El papel de la creencia en representar la realidad es ampliamente debatido en el pragmatismo. ¿Es válida una creencia cuando representa la realidad? Copiar es uno (y solo uno) modo genuino de conocimiento. ¿Las disposiciones de creencias que califican como verdaderas o falsas dependen de qué tan útiles sean en la investigación y en la acción? ¿Es solo en la lucha de los organismos inteligentes con el entorno que las creencias adquieren significado? ¿Una creencia solo se vuelve verdadera cuando tiene éxito en esta lucha? En el pragmatismo, nada práctico o útil se considera necesariamente verdadero, ni nada que ayude a sobrevivir meramente a corto plazo. Por ejemplo, creer que mi cónyuge infiel es fiel puede ayudarme a sentirme mejor ahora, pero ciertamente no es útil desde una perspectiva a más largo plazo porque no concuerda con los hechos (y por lo tanto no es verdad).

El pragmatismo no fue el primero en aplicar la evolución a las teorías del conocimiento: Schopenhauer abogó por un idealismo biológico ya que lo que es útil para un organismo es que puede diferir enormemente de lo que es verdadero. Aquí el conocimiento y la acción se representan como dos esferas separadas con una verdad absoluta o trascendental por encima y más allá de cualquier tipo de organismos de investigación utilizados para hacer frente a la vida. El pragmatismo desafía este idealismo al proporcionar una explicación "ecológica" del conocimiento: la investigación se refiere a cómo los organismos pueden controlar su medio ambiente. Lo real y lo verdadero son etiquetas funcionales en la investigación y no pueden entenderse fuera de este contexto. No es realista en un sentido tradicionalmente robusto de realismo (lo que Hilary Putnam llamaría más tarde realismo metafísico), pero es realista en la forma en que reconoce un mundo externo que debe ser tratado.

Mientras que el pragmatismo comenzó simplemente como un criterio de significado, rápidamente se expandió para convertirse en una epistemología completa con implicaciones de amplio alcance para todo el campo filosófico. Los pragmáticos que trabajan en estos campos comparten una inspiración común, pero su trabajo es diverso.

En la filosofía de la ciencia, el instrumentalismo es la opinión de que los conceptos y las teorías son meramente instrumentos útiles y el progreso en la ciencia no puede expresarse en términos de conceptos y teorías que de algún modo reflejen la realidad. Los filósofos instrumentalistas a menudo definen el progreso científico como nada más que una mejora en la explicación y predicción de fenómenos. El Instrumentalismo no afirma que la verdad no importe, sino que proporciona una respuesta específica a la pregunta sobre qué significan la verdad y la falsedad y cómo funcionan en la ciencia.

Uno de los principales argumentos de C. I. Lewis en "Mind and the World Order es": "El esquema de una teoría del conocimiento es que la ciencia no solo proporciona una copia de la realidad, sino que debe trabajar con sistemas conceptuales y que se eligen por razones pragmáticas, es decir, porque ayudan a la investigación. El propio desarrollo de Lewis de lógicas modales múltiples es un buen ejemplo. Lewis a veces se llama un "pragmatista conceptual" debido a esto." (Lewis 1929)

Otro desarrollo es la cooperación del positivismo lógico y el pragmatismo en las obras de Charles W. Morris y Rudolf Carnap. La influencia del pragmatismo en estos escritores se limita principalmente a la incorporación de la máxima pragmática en su epistemología. Los pragmáticos con una concepción más amplia del movimiento a menudo no se refieren a ellos.

El documento de W. V. Quine ""Dos dogmas del empirismo"", publicado en 1951, es uno de los artículos más célebres de la filosofía del siglo XX en la tradición analítica. El documento es un ataque a dos principios centrales de la filosofía de los positivistas lógicos. Una es la distinción entre enunciados analíticos (tautologías y contradicciones) cuya verdad (o falsedad) es una función de los significados de las palabras en el enunciado ("todos los solteros no están casados") y enunciados sintéticos, cuya verdad (o falsedad) es una función de estados de cosas (contingentes). El otro es el reduccionismo, la teoría de que cada enunciado significativo obtiene su significado de una construcción lógica de términos que se refiere exclusivamente a la experiencia inmediata. El argumento de Quine trae a la mente la insistencia de Peirce de que los axiomas no son verdades a priori sino declaraciones sintéticas.

Más adelante en su vida, F.C.S. Schiller se hizo famoso por sus ataques a la lógica en su libro de texto, "Formal Logic". Para entonces, el pragmatismo de Schiller se había convertido en el más cercano de cualquiera de los pragmáticos clásicos que a una filosofía del lenguaje ordinario. Schiller buscó socavar la posibilidad misma de la lógica formal, al mostrar que las palabras solo tenían significado cuando se usaban en contexto. El menos famoso de los trabajos principales de Schiller fue la secuela constructiva de su destructivo libro "Lógica Formal". En esta secuela, "Logic for Use", Schiller intentó construir una nueva lógica para reemplazar la lógica formal que había criticado en "Formal Logic". Lo que él ofrece es algo que los filósofos reconocerían hoy como una lógica que cubre el contexto del descubrimiento y el método hipotético-deductivo.

Considerando que F.C.S. Schiller descartó la posibilidad de la lógica formal, la mayoría de los pragmáticos son más críticos que su pretensión de validez última y ven la lógica como una herramienta entre otras, o quizás, considerando la multitud de lógicas formales, un conjunto de herramientas entre otras. C.S. Peirce desarrolló múltiples métodos para hacer una lógica formal.

Los usos del argumento de Stephen Toulmin inspiraron a los estudiosos en lógica informal y estudios retóricos (aunque es un trabajo epistemológico).

James y Dewey eran pensadores empíricos de la manera más directa: la experiencia es la prueba definitiva y la experiencia es lo que necesita ser explicado. No estaban satisfechos con el empirismo ordinario porque, según la tradición que data de Hume, los empiristas tendían a pensar que la experiencia no era más que sensaciones individuales. Para los pragmáticos, esto va en contra del espíritu de empirismo: debemos tratar de explicar todo lo que se da en la experiencia, incluidas las conexiones y el significado, en lugar de explicarlos y postular los datos sensoriales como la realidad última. El empirismo radical, o empirismo inmediato en las palabras de Dewey, quiere dar un lugar al significado y al valor en lugar de explicarlos como adiciones subjetivas a un mundo de átomos que zumban.

William James ofrece un ejemplo interesante de esta deficiencia filosófica:
El primer libro de F. C. S. Schiller, "Riddles of the Sphinx", fue publicado antes de que se diera cuenta del creciente movimiento pragmático que estaba teniendo lugar en Estados Unidos. En él, Schiller aboga por un término medio entre el materialismo y la metafísica absoluta. Estos opuestos son comparables a lo que William James denominó empirismo de mentalidad dura y racionalismo de mentalidad sensible. Schiller sostiene, por una parte, que el naturalismo mecanicista no puede dar sentido a los aspectos "superiores" de nuestro mundo. Estos incluyen el libre albedrío, la conciencia, el propósito, universales y algunos agregarían a Dios. Por otro lado, la metafísica abstracta no puede dar sentido a los aspectos "inferiores" de nuestro mundo (por ejemplo, lo imperfecto, el cambio, la fisicalidad). Si bien Schiller es vago sobre el tipo exacto de terreno intermedio que intenta establecer, sugiere que la metafísica es una herramienta que puede ayudar a la investigación, pero que solo es valiosa en la medida en que ayuda en la explicación.

En la segunda mitad del siglo XX, Stephen Toulmin argumentó que la necesidad de distinguir entre realidad y apariencia solo surge dentro de un esquema explicativo y, por lo tanto, que no tiene sentido preguntar en qué consiste la "realidad última". Más recientemente, una idea similar ha sido sugerida por el filósofo postanalítico Daniel Dennett, quien argumenta que cualquiera que quiera entender el mundo debe reconocer tanto los aspectos "sintácticos" de la realidad (es decir, los átomos zumbando) como sus propiedades emergentes o "semánticas" (es decir, significado y valor).

El empirismo radical da respuestas interesantes a las preguntas sobre los límites de la ciencia si los hay, la naturaleza del significado y el valor y la viabilidad del reduccionismo. Estas preguntas ocupan un lugar destacado en los debates actuales sobre la relación entre religión y ciencia, donde a menudo se supone -aunque la mayoría de los pragmáticos estarían en desacuerdo- que la ciencia degrada todo lo que es significativo en fenómenos "meramente" físicos.

Tanto John Dewey en "Experience and Nature" (1929) como medio siglo después Richard Rorty en su "Philosophy and the Mirror of Nature" (1979) argumentaron que gran parte del debate sobre la relación de la mente con el cuerpo resulta de confusiones conceptuales. En cambio, argumentan que no hay necesidad de colocar la mente como una categoría ontológica.

Los pragmatistas no están de acuerdo sobre si los filósofos deberían adoptar una postura quietista o naturalista hacia el problema mente-cuerpo. Los primeros (Rorty, entre ellos) quieren acabar con el problema porque creen que es un pseudoproblema, mientras que los segundos creen que es una pregunta empírica significativa.

El pragmatismo no ve una diferencia fundamental entre la razón práctica y la teórica, ni ninguna diferencia ontológica entre hechos y valores. Tanto los hechos como los valores tienen un contenido cognitivo: el conocimiento es lo que debemos creer; los valores son hipótesis sobre lo que es bueno en acción. La ética pragmática es ampliamente humanista porque no ve una prueba definitiva de moralidad más allá de lo que nos importa como humanos. Los buenos valores son aquellos para los cuales tenemos buenas razones. La formulación pragmática es anterior a las de otros filósofos que han subrayado importantes similitudes entre valores y hechos como Jerome Schneewind y John Searle.

William James intentó mostrar la significación de (algunos tipos de) espiritualidad, pero, como otros pragmáticos, no vio la religión como la base del significado o la moralidad.

La contribución de William James a la ética, tal como se presenta en su ensayo "The Will to Believe (La voluntad de creer)" a menudo ha sido malentendida como una súplica a favor del relativismo o la irracionalidad. En sus propios términos, argumenta que la ética siempre implica un cierto grado de confianza o fe y que no siempre podemos esperar pruebas adecuadas al tomar decisiones morales.

De los pragmáticos clásicos, John Dewey escribió más ampliamente sobre moralidad y democracia. (Edel 1993) En su artículo clásico "Tres factores independientes en la moral" (Dewey 1930), trató de integrar tres perspectivas filosóficas básicas sobre la moralidad: el derecho, la virtud y el bien. Sostuvo que si bien las tres proporcionan formas significativas de pensar sobre cuestiones morales, la posibilidad de conflicto entre los tres elementos no siempre se puede resolver fácilmente. (Anderson, SEP)

Dewey también criticó la dicotomía entre los medios y los fines que consideraba responsables de la degradación de nuestra vida laboral y educativa cotidiana. Hizo hincapié en la necesidad de un trabajo significativo y una concepción de la educación que la considerara no como una preparación para la vida sino como la vida misma. (Dewey 2004 [1910] capítulo 7, Dewey 1997 [1938], p.47)

Dewey se oponía a otras filosofías éticas de su época, especialmente el emotivismo de Alfred Ayer. Dewey vislumbró la posibilidad de la ética como una disciplina experimental, y los valores del pensamiento podrían caracterizarse mejor no como sentimientos o imperativos, sino como hipótesis sobre qué acciones conducirán a resultados satisfactorios o qué él denominó experiencia consumatoria. Una implicación adicional de este punto de vista es que la ética es una empresa falible, ya que los seres humanos a menudo no pueden saber qué los satisfaría.

Durante la transición del siglo XX al XXI, muchos aceptaron el pragmatismo en el campo de la bioética dirigido por los filósofos John Lachs y su alumno Glenn McGee, cuyo libro de 1997 ""El bebé perfecto: un enfoque pragmático de la ingeniería genética"" ( ver bebé de diseño) obtuvo alabanzas dentro de la filosofía clásica estadounidense y la crítica de la bioética para el desarrollo de una teoría de la bioética pragmática y su rechazo de la teoría del principalismo entonces en boga en la ética médica. Una antología publicada por The MIT Press, ""Pragmatic Bioethics"" incluyó las respuestas de los filósofos a ese debate, incluidos Micah Hester, Griffin Trotter y otros, muchos de los cuales desarrollaron sus propias teorías basadas en el trabajo de Dewey, Peirce, Royce y otros. El propio Lachs desarrolló varias aplicaciones del pragmatismo a la bioética independientemente de la obra de Dewey y James, pero ampliándola.

Una contribución pragmática reciente a la meta-ética es ""Making Morality"" de Todd Lekan (Lekan 2003). Lekan argumenta que la moralidad es una práctica falible pero racional y que tradicionalmente ha sido erróneamente basada en teoría o principios. En cambio, argumenta, la teoría y las reglas surgen como herramientas para hacer que la práctica sea más inteligente.

"El Arte como experiencia" de John Dewey, basado en las conferencias William James que pronunció en Harvard, fue un intento de mostrar la integridad del arte, la cultura y la experiencia cotidiana. El Arte, para Dewey, es o debería ser parte de la vida creativa de todos y no solo el privilegio de un selecto grupo de artistas. También enfatiza que la audiencia es más que un destinatario pasivo. El tratamiento de Dewey del arte fue un alejamiento del enfoque trascendental de la estética a raíz de Immanuel Kant, quien enfatizó el carácter único del arte y la naturaleza desinteresada de la apreciación estética.

Un destacado esteticista pragmático contemporáneo es Joseph Margolis. Él define una obra de arte como "una entidad físicamente incorporada, emergente culturalmente", una "expresión" humana que no es un capricho ontológico sino que está en línea con otra actividad humana y cultura en general. Enfatiza que las obras de arte son complejas y difíciles de comprender, y que no se puede dar ninguna interpretación determinada.

Tanto Dewey como James investigaron el papel que la religión puede seguir desempeñando en la sociedad contemporánea, el primero en "A Common Faith" y el último en "The Varieties of Religious Experience".

Desde un punto de vista general, para William James, algo es verdadero solo en la medida en que funciona. Por lo tanto, la afirmación, por ejemplo, de que la oración se escucha puede funcionar en un nivel psicológico pero (a) puede no ayudar a lograr las cosas por las que oras, y (b) puede explicarse mejor refiriéndose a su efecto sedante que el que las oraciones son escuchadas.

Como tal, el pragmatismo no es antitético a la religión, pero tampoco es una apología de la fe. Sin embargo, la posición metafísica de James deja abierta la posibilidad de que las afirmaciones ontológicas de las religiones sean verdaderas. Como observó al final de las "Variedades", su posición no equivale a negar la existencia de realidades trascendentes. Por el contrario, defendió el derecho epistémico legítimo de creer en tales realidades, ya que tales creencias sí marcan una diferencia en la vida de un individuo y se refieren a afirmaciones que no pueden ser verificadas o falsificadas ni por motivos sensoriales intelectuales ni comunes.

Joseph Margolis, en "Historied Thought, Construted World" (California, 1995), hace una distinción entre "existencia" y "realidad". Sugiere usar el término "existe" solo para aquellas cosas que exhiben adecuadamente la alteridad de Peirce: cosas que ofrecen una resistencia física bruta a nuestros movimientos. De esta manera, las cosas que nos afectan, como los números, pueden decirse que son "reales", aunque no "existen". Margolis sugiere que Dios, en tal uso lingüístico, bien podría ser "real", haciendo que los creyentes actúen de tal o cual manera, pero podría no "existir".

El neopragmatismo es una amplia categoría contemporánea utilizada por varios pensadores que incorporan ideas importantes y, sin embargo, divergen significativamente de los pragmáticos clásicos. Esta divergencia puede ocurrir ya sea en su metodología filosófica (muchos de ellos son leales a la tradición analítica) o en la formación conceptual (C. I. Lewis fue muy crítico con Dewey, a Richard Rorty no le gusta a Peirce). Importantes neopragmáticos analíticos incluyen a Lewis, W. V. O. Quine, Donald Davidson, Hilary Putnam y Richard Rorty. El pensador social brasileño Roberto Unger aboga por un "pragmatismo radical", que "desnaturaliza" la sociedad y la cultura, y por lo tanto insiste en que podemos "transformar el carácter de nuestra relación en los mundos sociales y culturales que habitamos en lugar de solo cambiar, poco a poco, el contenido de los arreglos y creencias que los componen ". [21] Stanley Fish, el último Rorty y Jürgen Habermas están más cerca del pensamiento analítico continental.

El pragmatismo neoclásico denota aquellos pensadores que se consideran herederos del proyecto de los pragmáticos clásicos. Sidney Hook y Susan Haack (conocidos por la teoría del fundherentismo) son ejemplos bien conocidos. Muchas ideas pragmáticas (especialmente las de Peirce) encuentran una expresión natural en la reconstrucción de la teoría de la decisión de la epistemología perseguida en el trabajo de Isaac Levi. Nicholas Rescher defiende su versión del "pragmatismo metódico" basado en interpretar la eficacia pragmática no como un reemplazo de las verdades sino como un medio para su demostración.

No todos los pragmáticos se caracterizan fácilmente. Es probable, considerando el advenimiento de la filosofía postanalítica y la diversificación de la filosofía angloamericana, que más filósofos estarán influenciados por el pensamiento pragmático sin necesariamente comprometerse públicamente con esa escuela filosófica. Daniel Dennett, un alumno de Quine, entra en esta categoría, al igual que Stephen Toulmin, quien llegó a su posición filosófica a través de Wittgenstein, a quien llama "un pragmático de tipo sofisticado" (prólogo de Dewey 1929 en la edición de 1988, p. XIII). Otro ejemplo es Mark Johnson cuya filosofía incorporada (Lakoff y Johnson 1999) comparte el psicologismo, realismo directo y anticartesianismo con el pragmatismo.

El pragmatismo conceptual es una teoría del conocimiento que se origina con el trabajo del filósofo y lógico Clarence Irving Lewis. La epistemología del pragmatismo conceptual se formuló por primera vez en el libro de 1929, "La mente y el orden mundial: Esquema de una teoría del conocimiento."

El "pragmatismo francés" cuenta con teóricos como Bruno Latour, Michel Crozier, Luc Boltanski y Laurent Thévenot. A menudo se ve como opuesto a los problemas estructurales relacionados con la teoría crítica francesa de Pierre Bourdieu.

En el siglo XX, los movimientos del positivismo lógico y la filosofía del lenguaje ordinario tienen similitudes con el pragmatismo. Al igual que el pragmatismo, el positivismo lógico proporciona un criterio de verificación de significado que se supone que nos libera de la metafísica sin sentido, sin embargo, el positivismo lógico no acentúa la acción como lo hace el pragmatismo. Los pragmatistas raramente usaban su máxima de significado para descartar toda metafísica como una tontería. Por lo general, el pragmatismo se planteó para corregir las doctrinas metafísicas o para construir las empíricamente verificables en lugar de proporcionar un rechazo total.

La filosofía del lenguaje ordinario está más cerca del pragmatismo que otras filosofías del lenguaje debido a su carácter nominalista y porque considera el funcionamiento más amplio del lenguaje en un entorno como su foco en lugar de investigar las relaciones abstractas entre el lenguaje y el mundo.

El pragmatismo tiene lazos para procesar la filosofía. Gran parte de su trabajo se desarrolló en diálogo con filósofos como Henri Bergson y Alfred North Whitehead, quienes generalmente no son considerados pragmáticos porque difieren tanto en otros puntos. (Douglas Browning y otros, 1998; Rescher, SEP)

El conductismo y el funcionalismo en psicología y sociología también tienen vínculos con el pragmatismo, lo que no es sorprendente si se tiene en cuenta que James y Dewey eran ambos estudiosos de la psicología y que Mead se convirtió en sociólogo.

El utilitarismo tiene algunos paralelismos significativos con el pragmatismo y John Stuart Mill defendió valores similares.

El pragmatismo enfatiza la conexión entre el pensamiento y la acción. Los campos aplicados como administración pública, ciencias políticas, estudios de liderazgo, relaciones internacionales, resolución de conflictos, y metodología de investigación han incorporado los principios del pragmatismo en su campo. A menudo, esta conexión se realiza utilizando la noción expansiva de democracia de Dewey y Addams.

El interaccionismo simbólico, una perspectiva principal dentro de la psicología social sociológica, se derivó del pragmatismo a principios del siglo XX, especialmente el trabajo de George Herbert Mead y Charles Cooley, así como el de Peirce y William James.

Se está prestando cada vez más atención a la epistemología pragmatista en otras ramas de las ciencias sociales, que han luchado con debates divisivos sobre el estado del conocimiento científico social.

Los partidarios sugieren que el pragmatismo ofrece un enfoque que es a la vez pluralista y práctico.

El pragmatismo clásico de John Dewey, William James y Charles Sanders Peirce ha influido en la investigación en el campo de la Administración Pública. Los eruditos afirman que el pragmatismo clásico tuvo una profunda influencia en el origen del campo de la administración pública. En el nivel más básico, los administradores públicos son responsables de hacer que los programas "funcionen" en un entorno plural y orientado a los problemas. Los administradores públicos también son responsables del trabajo diario con los ciudadanos. La democracia participativa de Dewey se puede aplicar en este entorno. La noción de Teoría de Dewey y James como herramienta, ayuda a los administradores a elaborar teorías para resolver problemas administrativos y políticos. Además, el nacimiento de la administración pública estadounidense coincide estrechamente con el período de mayor influencia de los pragmáticos clásicos.

Qué pragmatismo (pragmatismo clásico o neo-pragmatismo) tiene más sentido en la administración pública ha sido una fuente de debate. El debate comenzó cuando Patricia M. Shields presentó la noción de Dewey de la Comunidad de Investigación. Hugh Miller se opuso a algunos elementos de la comunidad de investigación (situación problemática, actitud científica, democracia participativa). A esto siguió un debate que incluía respuestas de un profesional, un economista, un planificador, otros académicos de la administración pública, y destacados filósofos. Miller y Shields también respondieron.

Además, becas aplicadas de la administración pública que evalúan las escuelas autónomas, la subcontratación, la gestión financiera, la medición del desempeño, las iniciativas de calidad de vida urbana, y la planificación urbana en parte se basan en las ideas del pragmatismo clásico en el desarrollo del marco conceptual y el enfoque del análisis.

Sin embargo, el uso del pragmatismo por parte de los administradores del sector de la salud ha sido criticado como incompleto, según los pragmáticos clásicos, el conocimiento siempre está conformado por los intereses humanos. El enfoque del administrador en los "resultados" simplemente promueve su propio interés, y este enfoque en los resultados a menudo socava los intereses de sus ciudadanos, que a menudo están más interesados ​​en el proceso. Por otro lado, David Brendel argumenta que la capacidad del pragmatismo de puentear dualismos, enfocarse en problemas prácticos, incluir perspectivas múltiples, incorporar la participación de partes interesadas (paciente, familia, equipo de salud), y su naturaleza provisional lo hace adecuado para abordar problemas en esta área.

Desde mediados de la década de 1990, las filósofas feministas han redescubierto el pragmatismo clásico como fuente de teorías feministas. Los trabajos de Seigfried, Duran, Keith, y Whipps exploran los vínculos históricos y filosóficos entre el feminismo y el pragmatismo. La conexión entre el pragmatismo y el feminismo tardó tanto tiempo en redescubrirse porque el pragmatismo mismo fue eclipsado por el positivismo lógico durante las décadas centrales del siglo XX. Como resultado, se perdió del discurso feminista. Las mismas características del pragmatismo que llevaron a su declive son las características que las feministas ahora consideran su mayor fortaleza. Estas son "críticas persistentes y tempranas de las interpretaciones positivistas de la metodología científica; revelación de la dimensión de valor de las afirmaciones fácticas"; ver la estética como información de la experiencia cotidiana; subordinar el análisis lógico a cuestiones políticas, culturales y sociales; unir los discursos dominantes con la dominación; "realinear la teoría con la praxis y resistir el giro hacia la epistemología y, en cambio, enfatizar la experiencia concreta". Estas filósofas feministas apuntan a Jane Addams como fundadora del pragmatismo clásico. Además, las ideas de Dewey, Mead y James son consistentes con muchos principios feministas. Jane Addams, John Dewey y George Herbert Mead desarrollaron sus filosofías cuando los tres se hicieron amigos, se influyeron mutuamente y participaron en la experiencia de Hull House y las causas de los derechos de las mujeres.

El pragmatismo valora y evalúa los efectos de un diseño sobre la transformación urbana, y los efectos de un concepto o diseño alteran la comprensión general del concepto. Richard Rorty menciona que está ocurriendo "un cambio radical" en el pensamiento filosófico reciente: "un cambio tan profundo que quizás no reconozcamos que está ocurriendo". Mientras que el mundo en el que está enraizado el movimiento ha tenido muchos cambios, como marco para percibir el mundo, el pragmatismo también ha experimentado diferentes niveles de modificaciones. Esos cambios son muy relevantes para el desarrollo de las ciudades y los temas básicos, como el antifundamentalismo, el falibilismo, cuestionar la clara distinción entre teoría y práctica, el pluralismo y la democracia, el pragmatismo se puede aplicar al urbanismo aún más fuertemente.

Vincent di Norcia argumenta que un enfoque pragmático es adecuado con respecto a los problemas sociales porque requiere una conducta que resuelva los problemas a medida que evalúa continuamente los efectos prácticos de un proyecto. Esto asegura el interés de los interesados ​​y Norcia subraya la importancia del pluralismo social y cognitivo. El pluralismo social significa que debemos reconocer los intereses de todos los interesados ​​que se ven afectados por una determinada decisión, sin tener en cuenta los intereses de los grupos políticos o económicos de élite. Como complemento, Norcia también enfatiza el pluralismo cognitivo, que indica que uno debe incluir todo tipo de conocimiento que sea relevante para un problema.




</doc>
<doc id="19409" url="https://es.wikipedia.org/wiki?curid=19409" title="Relieve de España">
Relieve de España

El relieve de España se caracteriza por ser bastante elevado, con una altitud media de 660 metros sobre el nivel del mar, bastante montañoso si lo comparamos con el resto de países de Europa y solo superado por Suiza, Austria, Grecia y los microestados de Andorra y Liechtenstein. En la España peninsular, el relieve se articula en torno a una gran Meseta Central que ocupa la mayor parte del centro de la península ibérica. Fuera de la meseta, está la depresión del río Guadalquivir, situada en el sudoeste de la península, y la del río Ebro, en el nordeste de la misma.

Los sistemas montañosos de España son muy numerosos y ocupan casi la mitad del territorio nacional. Los Pirineos (en el límite nordeste) y el sistema Bético (en el sudeste) son las cordilleras más elevadas y se sitúan fuera de la Meseta Central. Rodeando esta, está la cordillera Cantábrica en el norte, el sistema Ibérico en el este, y Sierra Morena en el sur. Dentro de la Meseta Central está el sistema Central y los montes de Toledo.

A España pertenecen dos archipiélagos de interés geográfico: las islas Baleares, situadas en el mar Mediterráneo, con una latitud similar a la de Castilla-La Mancha; y las islas Canarias, siete islas de origen volcánico ubicadas en el océano Atlántico, próximas a la costa del Sahara Occidental; y con menos importancia: la isla de Alborán entre España y Marruecos y las islas Columbretes en Castellón. De España también son algunos pequeños enclaves costeros del norte de África: las ciudades de Ceuta y Melilla, las islas Chafarinas, y los peñones de Alhucemas y de Vélez de la Gomera.

La costa española, bañada por el océano Atlántico, y los mares Cantábrico y Mediterráneo, presenta una gran diversidad de playas, acantilados y rías. La costa alta (presencia de acantilados y rasas) y articulada (presencia de rías y cabos) es la más predominante en el norte y en las islas Canarias, mientras que la costa baja (presencia de playas y calas) es propia del sur, del Mediterráneo y Baleares.

España tiene gran variedad paisajística, con la existencia de grandes montañas y depresiones, las montañas pueden ser abruptas o suaves. En el relieve también influye la forma maciza y poco articulada de la península, las costas carecen de salientes, tienen elevada altitud media. Hay un cinturón montañoso que rodea la península y dificulta el acceso al interior. 


Desde principios de la Era Primaria existía el continente que los geólogos han denominado Gondwana, de contornos distintos al del continente africano actual, pero del que en realidad deriva este continente. Por el norte se extendía al mismo tiempo el continente que podemos llamar Paleoeuropa del que después derivaría la actual Europa. Y entre ambos continentes un mar mucho más ancho y profundo que el actual Mediterráneo, el antiguo Tetis de los geólogos. 

A finales de la Era Primaria se produjeron movimientos tectónicos y orogénicos llamados en conjunto orogenia hercínica (o plegamiento herciniano), de gran intensidad. Tras ellos, los territorios occidentales de la Península adquirieron una fisonomía semejante a la actual. Por el norte, este y sur se extendía el mar de Tetis. El relieve así formado tomó la dirección armoricana (nombre de la antigua Bretaña francesa) de NO-SE.

El plegamiento herciniano afectó a grandes masas de sedimentos que se transformaron en pizarras, cuarcitas y formaciones graníticas. Toda esta actividad magmática dio lugar también a filones de minerales como plomo, mercurio, pirita, etc., que son la base principal de la riqueza minera de la península. Este movimiento afectó a toda Europa y dio lugar, entre otros, al Macizo Central y la Selva Negra.

En esta superficie (territorios occidentales de la Península), conocida como "zócalo paleozoico", predomina actualmente la sílice, cuya expresión más común es el cuarzo. El conjunto forma la llamada España silícea.

El periodo Secundario fue de calma orogénica, caracterizado por la erosión de lo ya existente, y sedimentación de materiales en las diferentes fosas marinas.

La etapa del plegamiento alpino se da en el Terciario, con fuertes presiones que pliegan los materiales; las barreras que se habían creado en la orogenia herciniana van a tener un efecto de tope sobre estas fuerzas. Estos empujes van a plegar los materiales más modernos que son de naturaleza blanda y los materiales más antiguos van a romperse. Con estas fuertes presiones se formaron los Pirineos, se fracturó la Meseta y dio lugar a Sierra Morena, la cordillera Cantábrica y la Ibérica, transcurridos varios millones de años se formaron los Sistemas Béticos y surgieron las islas Baleares. También se formaron las prefosas alpinas que son depresiones que anteceden a las cordilleras y se van a ir rellenando de materiales.

Al final del Terciario se acabará casi de configurar la actual península. En el periodo post-alpino se darán deformaciones que darán lugar a agrupamientos, consecuencia de la orogenia alpina. Las masas continentales intentan llegar a un equilibrio y liberar las tensiones acumuladas. Estos procesos posteriores y los asociados reciben el nombre de tectónica morfológica y son movimientos de tipo vertical.

A partir del Neógeno ha habido tres movimientos póstumos: el primero es el abombamiento de la meseta que se bascula hacia occidente, derivados de este basculamiento se producen una serie de empujes desde oriente y provocan que el zócalo también se bascule hacia el Atlántico, y la tercera fase es una serie de movimientos de origen vertical que elevan las cordilleras alpinas (sistema Central y Montes de Toledo).

En el Cuaternario se dan cambios en el paisaje, son movimientos eustáticos que afectan al nivel del mar. Se producen tanto subidas como bajadas, con origen en el glaciarismo. También aparecen fenómenos volcánicos en áreas fragmentadas o fallas como Olot y La Mancha. Además los ríos toman la configuración actual y comienza su erosión.

Por último están los sistemas morfogenéticos que dan lugar al relieve actual, hacen referencia a los aspectos climáticos, erosivos, químicos y mecánicos que afectan al relieve y están ligados a la tectónica. También tuvieron influencia las glaciaciones (en las partes más elevadas), junto al agua y al viento.

Una forma es el circo glaciar, una especie de depresión circular por influencia de los hielos que se da en las mayores altitudes y da lugar a una serie de lenguas de hielo llamadas morrenas que surgen de la parte más elevada de la montaña y discurren por todo el valle hasta la base; si se encaja entre montañas y labra un valle en forma de "U" se le llama artesa. Esto es muy común en los Pirineos y las zonas más elevadas.

En las zonas menos elevadas se da otro tipo de modelado, el dominio periglaciar, muy común en los periodos interglaciares. Las formas de modelado más importantes son la gelifracción y la solifluxión, que a su vez alteran los procesos de erosión fluvial: en las fases más frías los ríos tendrán menos caudal que en el deshielo cuando aumentan su fuerza erosiva y configuran los acantilados.

La España peninsular tiene una superficie de 493 458 km² (el 97,53 % del territorio nacional) y sus costas miden un total de 4600 km aproximadamente. La altitud media es de 660 metros sobre el nivel del mar , y la anchura máxima de la península es de 1094 km. En el relieve destaca la abundancia de sistemas montañosos, puesto que casi la mitad de la superficie es accidentada. La Meseta Central es el elemento principal del relieve porque está situada en el centro del país, ocupa una gran extensión y en torno a ella se articulan las cordilleras y depresiones. 

Las cordilleras más elevadas son el sistema Bético, los Pirineos, la cordillera Cantábrica y el sistema Central. El sistema Ibérico, las cordilleras Costeras Catalanas, los montes de Toledo y Sierra Morena conforman zonas de media montaña, las más abundantes en las zonas accidentadas. Las áreas llanas las componen la Meseta Central, las depresiones del Ebro y del Guadalquivir, y las llanuras litorales de la costa mediterránea. Los mares que bañan el litoral español son el mar Mediterráneo por el nordeste, este, sudeste y sur, el mar Cantábrico por el norte, y el océano Atlántico por el noroeste y sudoeste.

El territorio español además presenta una gran diversidad natural y humana, que viene dada por la variedad del relieve y por los contrastes climáticos propiciados por el mismo, que determinan diferentes tipos de vegetación, de aguas y de suelos. Esta variedad del medio físico supone un reparto desigual de los recursos naturales en el espacio y, por consiguiente, de las actividades económicas humanas, dando lugar a una gran pluralidad de paisajes humanos.

El relieve de la península ibérica se articula alrededor de una gran unidad central, la Meseta Central, de una altitud media de 650 metros sobre el nivel del mar. Esta se ubica en el centro de la península ibérica, en las comunidades autónomas de Castilla y León, Comunidad de Madrid, Castilla-La Mancha, la mitad este de Extremadura y el suroeste de Aragón, y está ligeramente inclinada al océano Atlántico. Los principales ríos que discurren por la meseta son el río Duero, el Tajo y el Guadiana, todos ellos dirigidos al oeste. El sistema Central divide la Meseta Central en dos submesetas: la Submeseta Norte y la Submeseta Sur.

La Submeseta Norte se ubica exclusivamente en Castilla y León y tiene una altitud media de 700 metros sobre el nivel del mar. Limita en el sur con las sierras de Gata y Gredos, y en el sureste con las sierras de Guadarrama y Ayllón, todas ellas pertenecientes al sistema Central. En su límite este-noreste está el sistema Ibérico, y en el norte limita con la cordillera Cantábrica. Toda la submeseta pertenece a la cuenca del río Duero, el cual transcurre de este a oeste.

La Submeseta Sur se ubica en las comunidades autónomas de Madrid, Castilla-La Mancha, en la mitad este de Extremadura y en la provincia aragonesa de Teruel. Tiene una altitud media de 670 metros sobre el nivel del mar y está limitada por sucesivas cadenas montañosas. En el límite noroeste están las sierras de Gredos y Guadarrama, y en el norte la de Ayllón, todas ellas pertenecientes al sistema Central. En el límite noreste y este está el sistema Ibérico, y en el sur se extiende Sierra Morena. La Submeseta Sur se encuentra dividida en dos mitades, norte y sur, por los montes de Toledo, una pequeña cordillera que se orienta de oeste a este y que se ubica en el norte de La Mancha. El río Tajo transcurre de este a oeste en la mitad norte de la submeseta, y el río Guadiana transcurre con la misma orientación en la mitad sur. Ambas cuencas están separadas por los montes de Toledo.

Las principales cordilleras de la península pueden considerarse, en relación con la Meseta Central, organizadas en tres grupos:

Dentro de la Meseta Central hay dos sistemas montañosos: el sistema Central y los montes de Toledo. El sistema Central, ubicado en el centro de la meseta, la divide en dos submesetas (norte y sur) y es la frontera natural entre las comunidades autónomas de Castilla y León por un lado, y de la Comunidad de Madrid y Castilla-La Mancha por otro. La cordillera se extiende de oeste a este a lo largo de 700km, y su pico más alto es el Almanzor, con 2592m s. n. m. Algunas de sus sierras son la de Gata y de Gredos en su mitad oeste, y las de Guadarrama y de Ayllón en su mitad este.

El otro sistema montañoso es el de los montes de Toledo, una pequeña cordillera de 350km de longitud y 100km de anchura que se extiende de oeste a este en las provincias de Toledo y Cáceres. Estas montañas no son especialmente elevadas puesto que el pico más alto, el de la Villuerca Alta, mide 1603 m s. n. m. A esta cordillera pertenece la sierra de Guadalupe, ubicada en el centro de Extremadura.

Las cordilleras interiores a la Meseta Central son:


Rodeando la Meseta Central está la cordillera Cantábrica, el sistema Ibérico y Sierra Morena. La cordillera Cantábrica se extiende de oeste a este a lo largo de 480km por todo el límite norte de la meseta, haciendo de límite natural entre Castilla y León y las comunidades autónomas cantábricas (Galicia, Asturias, Cantabria y País Vasco). La máxima altitud de la cordillera es el pico Torre Cerredo con sus 2648m s. n. m. Así, la cordillera Cantábrica separa la meseta de la costa del mar Cantábrico.

En el límite noreste y este de la Meseta Central está el sistema Ibérico, un sistema montañoso con orientación sureste-noroeste y una longitud cercana a los 600km que hace de límite natural entre las dos Castillas y Aragón. Comienza en La Rioja y acaba en la provincia de Albacete. A la cordillera pertenecen sierras como los Picos de Urbión en La Rioja y la provincia de Soria, la sierra de Albarracín en la provincia de Guadalajara, la serranía de Cuenca (en la provincia de Cuenca), y la del Rayo en la provincia aragonesa de Teruel. El pico más alto es el Moncayo con sus 2313 m s. n. m. Por tanto, el sistema Ibérico separa la meseta de la depresión del Ebro y de la costa levantina del mar Mediterráneo.

En el límite sur de la meseta está Sierra Morena, un sistema montañoso que se extiende de oeste a este con una longitud de 400km, haciendo de límite natural de Castilla-La Mancha y Extremadura con Andalucía. Su máxima elevación excede levemente los 1000 metros sobre el nivel del mar, por lo que no es una cordillera especialmente elevada. Sierra Morena incluye sierras como Sierra Madrona, la sierra de Aracena y la sierra de Hornachuelos. Así, Sierra Morena separa la meseta de la depresión del Guadalquivir.

En el noroeste, junto a Galicia, están los montes de León

Las cordilleras que rodean la Meseta Central son:




Existen cordilleras que no limitan con la Meseta Central. Una de ellas es los Pirineos, una de las cordilleras más elevadas de España y una de las más extensas con sus 415 km de longitud y sus 150 km de anchura media. La cordillera está ubicada en la frontera con Francia, en el extremo noreste del país, en el istmo de la península ibérica. Pertenecen a las comunidades autónomas del País Vasco, Navarra, Aragón y Cataluña. Por su adscripción política, se pueden diferenciar los Pirineos españoles, los franceses y los andorranos. Los Pirineos españoles albergan los Prepirineos, una cordillera con picos más bajos que los de los Pirineos, ubicada en el sur de ésta; los Pirineos Navarros, con picos que no exceden de los 3000 metros; los Pirineos Aragoneses, que tienen los picos más altos (muchos de ellos superan los 3000m); y los Pirineos Catalanes, que también tienen picos superiores a los 3000 metros. La mayor elevación de la cordillera es el pico del Aneto con sus 3404m, siendo el segundo pico más alto de la península ibérica. Al sur de los Pirineos se extiende la depresión del Ebro.

En el extremo noreste de la península ibérica están las Cordilleras Costero Catalanas, un conjunto de sierras ubicadas en la comunidad autónoma de Cataluña. Tienen una orientación suroeste-noreste y se extienden a lo largo de 250 km paralelos a la costa mediterránea, y van desde la provincia de Tarragona hasta el golfo de Rosas. El sistema lo forman dos cordilleras: la Cordillera Litoral, situada junto a la costa, y la cordillera Prelitoral, ubicada más al noroeste.

En el sureste de la península ibérica están las cordilleras Béticas, un grupo de cordilleras y sierras que conforman una unidad geográfica. Los sistemas montañosos béticos se dividen en dos grandes conjuntos: cordillera Penibética, ubicada en la zona sur, junto a la costa con el mar Mediterráneo; la cordillera Subbética, situada más al norte y limitando con el este de Sierra Morena y el sur del sistema Ibérico; y los sistemas Prebéticos, situados al este de la cordillera Penibética. Algunas sierras béticas son Sierra Nevada, la Sierra de Cazorla y la Sierra de Grazalema. El pico más elevado de la cordillera y de la península ibérica es el Mulhacén (3478m).
En el noroeste de la Península, detrás de los Montes de León, dentro de Galicia, está el Macizo Galaico.

Las cordilleras exteriores a la Meseta Central son:




Las dos principales depresiones de la España peninsular son la del río Ebro y la del Guadalquivir. Estas son exteriores de la Meseta y fueron cuencas o fosas prealpinas que, tras la orogénesis terciaria, quedaron entre las cordilleras alpinas y los macizos antiguos. Tienen forma triangular y fueron rellenadas por grandes espesores de sedimentos terciarios y cuaternarios.

Las costas de la Península son poco recortadas, curvas y con un contorno rectilíneo. Abundan las costas altas y rocosas en el norte, y las costas bajas y arenosas en el sureste. Las costas de las islas Baleares presentan tramos rocosos, y las costas de las islas Canarias presentan acantilados.

Los casi 4600 km de costas de la España peninsular pertenecen al mar Mediterráneo en el noreste, este, sureste y sur, al Cantábrico en el norte, y al océano Atlántico en el noroeste y suroeste.












A España pertenecen dos archipiélagos de islas. Uno de ellos es el de las islas Baleares, ubicado en el mar Mediterráneo y a 90 km al este del cabo de la Nao (Alicante). El otro es el de las Islas Canarias, situado en el océano Atlántico, a 1.050 km al suroeste de Cádiz y a 100 km al oeste de la costa africana.

Las islas Baleares es un archipiélago situado en el mar Mediterráneo, a 80 km al este de la península ibérica. Tiene una latitud media de 39º 30' N. 270 km separan los límites occidental y oriental del archipiélago, y 160 km de los extremos norte y sur. Las tres islas más grandes e importantes que componen Baleares son Ibiza, Mallorca y Menorca. Las tres islas principales están alineadas en ese orden orientadas de suroeste a noreste. Aparte de estas tres, hay otras islas de menor tamaño como son Formentera y la Cabrera. En el relieve de las islas Baleares predominan las zonas llanas y de escasa altitud, exceptuando la sierra de la Tramontana, situada en Mallorca.

El relieve del archipiélago balear tiene relación con las Cordilleras Béticas y las Cordilleras Costeras Catalanas, ya que Mallorca e Ibiza están unidas bajo el agua a través de un estrecho; y Menorca está unida con las Cordilleras Costeras Catalanas.






Las costas de las islas Baleares son altas, ya que en muchos lugares las montañas llegan hasta el mar. Si el mar baña una llanura, la costa es baja y arenosa.

El archipiélago de las islas Canarias está situado en el océano Atlántico norte, a 1050 km al suroeste de la costa de Cádiz y a 100 km al oeste de la costa africana. 460 km separan los extremos occidental y oriental del archipiélago y 190km los límites norte y sur del mismo. Canarias se compone de siete islas principales dispuestas de oeste a este y de dos islotes, Alegranza y Graciosa. Es de origen volcánico debido a su formación mediante la acumulación de sedimentos procedentes de las erupciones, que a su vez procedían del fondo atlántico. El relieve de las islas es montañoso, con una importante presencia de volcanes y con costas altas. El más alto de ellos es el Teide (3718 m), situado en la isla de Tenerife, siendo también el más alto del territorio español y el tercer volcán más grande del mundo desde su base.






Las costas de las islas Canarias son algunas costas altas, con acantilados; y otras de costas bajas con playas de arena o de piedra, donde se forman las dunas. Si es de origen volcánico, la playa tiene arena de color negro.

Las plazas de soberanía es el conjunto de posesiones españolas en la costa norte de África.





El accidentado y complejo relieve que tiene España ha influido directamente en la historia de este país, y en las batallas y guerras que en él se han librado. Hay que tener en cuenta que, hasta hace poco más de doscientos años, el acceso a muchos puntos de la península ibérica era complicado porque había que superar cordilleras montañosas. Por ejemplo, para acceder a la Meseta Central (donde están Madrid y Toledo) saliendo de Europa, hay que atravesar los Pirineos y el sistema Ibérico. Los antiguos romanos, los visigodos, los árabes y posteriormente los cristianos tuvieron dificultades en la conquista de territorios debido a que los pobladores de zonas montañosas conocían bien la orografía de su tierra, mientras que los invasores no. También por ese motivo se retrasó la conquista de las Canarias. En las zonas llanas, especialmente en la Meseta Central, los castillos se construían en lo alto de los cerros para poder avistar al enemigo a tiempo. Historiadores y escritores han comparado a la Meseta Central con un castillo, siendo las cordilleras que la rodean sus murallas. Por tanto, España nunca ha sido un país fácil de conquistar debido, en parte, a su relieve. Por otro lado, esta geografía fue una de las causas por las que España no tuvo una red de ferrocarril suficientemente extensa hasta que las tecnologías permitieron la construcción de rutas montañosas. Esta falta de medios de transporte modernos supuso para el país un retraso en el desarrollo de la Revolución industrial.





</doc>
<doc id="19413" url="https://es.wikipedia.org/wiki?curid=19413" title="Puerto">
Puerto

Un puerto es un lugar en la costa que, por sus características naturales o artificiales, resulta resguardada de gran oleaje, y donde las embarcaciones pueden realizar diversas maniobras: como fondeo, atraque, estadía y desatraque; y operaciones como transferencia de cargas, embarque y desembarque de pasajeros. Suelen contar, además, con servicios para las embarcaciones, pasajeros y cargas. 

Un puerto natural (en inglés, "harbor") es un accidente costero constituido por una entrada de agua lo suficientemente profunda como para permitir el anclaje de embarcaciones, en la que la propia configuración de la costa proporciona protección frente a las grandes olas. Ejemplos de estos son la bahía de Guantánamo, en Cuba o el puerto de Kingston en Jamaica, protegido por una barra llamado "Palisadoes" 

En la mayoría de los casos, se realizan obras de ingeniería para construirlos y mantenerlos. La protección de las olas se logra con rompeolas (también llamadas escolleras) y la profundidad, mediante el dragado. 

Los puertos se pueden clasificar por el cuerpo de agua en el que se ubican (marítimos, lacustres y fluviales), por la actividad principal que se realiza (comerciales, deportivos, pesqueros, militares) o por el calado del que disponen. En un "puerto de aguas profundas" el calado debe ser superior a los 45 pies (13,72 m).

Desde el punto de vista funcional, las obras y las instalaciones de un puerto se pueden clasificar por su ubicación. Así, se distinguen cuatro zonas diferentes:

El conjunto de servicios que presta un puerto se pueden clasificar en función del ámbito al que van destinados.

Es el tipo más común de puertos, estos están dedicados a la carga y descarga, ya sea de petróleos, graneles líquidos no petrolíferos, graneles sólidos, pasajeros, etcétera. Algunos ejemplos pueden ser Long Beach, Osaka , Barcelona, Shanghái, Callao, Valparaíso, Ciudad del Cabo, Hong Kong, Barranquilla, Génova, Estambul, Dubái entre otros

En ocasiones las mercancías descargadas son tratadas en el mismo puerto, lo que da paso a una variación del mismo llamada "puerto-fábrica".

Los puertos deportivos son aquellos especialmente dirigidos a abrigar durante estancias más o menos prolongadas o servir de base a las embarcaciones de recreo, que por su uso irregular deben pasar estancias prolongadas en zona de amarre o en dique seco. Por las necesidades a cubrir de estos puertos, suelen presentar características diferenciadas respecto a los puertos mercantes o tradicionales como zona de varadero, dique seco, atarazanas o la existencia de restaurantes, tiendas y otros servicios enfocados a una clientela de cierto poder adquisitivo.

Los puertos o partes de los puertos que se encargan especialmente de la construcción o reparación de buques son los astilleros con instalaciones particulares de este tipo. Suelen ser representativos de los astilleros la existencia de grandes grúas, diques secos o diversas zonas de botadura para buques de distinto tamaño.

Aquellos encargados del manejo de mercancías perecederas y especialmente los destinados a la descarga del pescado, los puertos pesqueros, contienen en sus instalaciones edificios orientados a la compraventa de estas mercancías, las lonjas. Estos puertos, al ser lugar de origen para la entrada en el mercado de estos productos deben dotarse de la infraestructura logística y mercantil para distribuirlos a las zonas de consumo.

Entre museos portuarios y museos navales existe una amplia representación internacional de este tipo de museos, llamando la atención museos navales en lugares alejados del mar como el museo Naval de Madrid u otros que por su tradición secular merecen mención como las Atarazanas Reales de Barcelona o el Museo Marítimo Nacional de Greenwich.



Enlaces externos


</doc>
<doc id="19414" url="https://es.wikipedia.org/wiki?curid=19414" title="Moby-Dick">
Moby-Dick

Moby Dick es una novela del escritor Herman Melville publicada en 1851. Narra la travesía del barco ballenero "Pequod", comandado por el capitán Ahab, junto a Ismael y el arponero Queequog en la obsesiva y autodestructiva persecución de un gran cachalote blanco.

Al margen de la persecución y evolución de sus personajes, el tema de la novela es eminentemente enciclopédico al incluir detalladas y extensas descripciones de la caza de las ballenas en el siglo XIX y multitud de otros detalles sobre la vida marinera de la época. Quizá por ello la novela no tuvo ningún éxito comercial en su primera publicación, aunque con posterioridad haya servido para cimentar la reputación del autor y situarlo entre los mejores escritores estadounidenses.

El libro inicia con 80 epígrafes, empezando con uno del Génesis y terminando con una canción ballenera. Todos los epígrafes están relacionados con el mar y las ballenas o el Leviatán. Melville también presenta al inicio definiciones directas de diccionarios para la ballena y luego una tabla donde se muestra la palabra ballena en diferentes idiomas. 

La frase inicial del narrador —«"Call me Ishmael"» en inglés, traducido al español a veces como «Llamadme Ismael», otras veces como «Pueden ustedes llamarme Ismael»—, se ha convertido en una de las citas más conocidas de la literatura en lengua inglesa.

El narrador, Ismael, un joven con experiencia en la marina mercante, decide que su siguiente viaje será en un ballenero. De igual forma se convence de que su travesía debe comenzar en Nantucket, Massachusetts, isla prestigiosa por su industria ballenera. Antes de alcanzar su destino, o el origen de su aventura, entabla una estrecha amistad con el experimentado arponero polinesio "Queequeg", con quien acuerda compartir la empresa.

Ambos se enrolan en el ballenero "Pequod", con una tripulación conformada por las más diversas nacionalidades y razas; precisamente sus arponeros son el caníbal Queequeg, el piel roja "Tashtego" y el «negro salvaje» "Daggoo". El "Pequod" es dirigido por el misterioso y autoritario capitán Ahab, un viejo lobo de mar con una pierna construida con la mandíbula de un cachalote. Ahab revelará a su tripulación que el objetivo primordial del viaje, más allá de la caza de ballenas en general, es la persecución tenaz a Moby Dick, enorme leviatán que lo privó de su pierna y que había ganado fama de causar estragos a todos y cada uno de los balleneros que, osada o imprudentemente, habían intentado darle caza.

"Moby Dick" es una obra de profundo simbolismo. Se suele considerar que comparte características de la alegoría y de la épica. Incluye referencias a temas tan diversos como biología, idealismo, jerarquía, obsesión, política, pragmatismo, racismo, religión y venganza.

Los tripulantes del "Pequod" tienen orígenes tan variados como Chile, Colombia, China, Dinamarca, España, Francia, Holanda, India, Inglaterra, Irlanda, Islandia, Italia, Malta, Portugal y Tahití, lo que sugiere que el "Pequod" es una representación de la humanidad.

Las alusiones bíblicas de los nombres de los personajes o el significado del cachalote blanco han intrigado a lectores y críticos durante más de un siglo.

Además de haber estado basada en las experiencias personales de Melville como marinero, "Moby Dick" está inspirada en dos casos reales:




</doc>
<doc id="19415" url="https://es.wikipedia.org/wiki?curid=19415" title="Nag Hammadi">
Nag Hammadi

Nag Hammadi (árabe نجع حمادي; transliteración: Naj' Hammādi) (), es un pueblo situado en la ribera del río Nilo, en Egipto, llamado "Jenoboskion" (griego Χηνοβόσκιον) en la antigüedad, donde en el año 320, San Pacomio, fundó el primer monasterio cristiano.

El pueblo es famoso porque en 1945 apareció una amplia colección de códices antiguos. En 367, los monjes del lugar copiaron unos 45 escritos religiosos (incluidos los evangelios gnósticos de Tomás, Felipe y Valentín) en una docena de códices. Esos fueron cuidadosamente guardados en un recipiente sellado y escondidos en unas grutas próximas, donde permanecieron ocultos durante casi 1600 años. 

En diciembre del año 1945 dos campesinos egipcios encontraron más de 1100 páginas de antiguos manuscritos en papiro, enterrados junto al acantilado oriental del valle del Nilo. Los textos eran traducciones de originales griegos al idioma copto, que surgió alrededor del siglo III. 



</doc>
<doc id="19416" url="https://es.wikipedia.org/wiki?curid=19416" title="Ninjutsu">
Ninjutsu

El , también conocido como shinobi-jutsu, y como , es el arte marcial japonés del espionaje y la guerrilla.

Este arte marcial, se basa en grupos de técnicas y tácticas (consideradas clásicamente 20, mencionadas más adelante) que han utilizado supuestamente los ninja durante siglos. Los primeros datos que se tienen de la utilización de ninjas en el campo de batalla data del siglo V, lo que nos da una idea de la antigüedad de este estilo de lucha, que se complementaba con el aprendizaje de muchas habilidades útiles para el espionaje, como la caracterización o falsificación de documentos, así como ciertas prácticas esotéricas derivadas del Mikkyo, budismo esotérico japonés.

Con la llegada de Oda Nobunaga, los ninja fueron perseguidos en un intento de detener su creciente influencia y poder. Aunque esto provocó que algunos clanes ninja se extendiesen por todo Japón al tener que huir de Iga. Ya en el siglo XVII se utilizaron por última vez de forma masiva en la revuelta cristiana de Kyushu en 1637. A mediados y fines del período Edo, comienza el declive en el uso de los shinobi, dado el largo período de paz establecido por la familia Tokugawa.
Entre los siglos XVII y XIX se prohibió legalmente el uso de los mercenarios ninja, lo que hizo que se utilizaran de forma clandestina y a escala pequeña.

En el siglo XX Japón utilizó el ninjutsu como forma de entrenamiento de sus tropas de élite. No obstante eran tropas regulares dotadas de un entrenamiento especial, sin que se pueda llegar a considerarlos verdaderos ninjas. El último registro real sobre el empleo de los ninja por parte del gobierno japonés data de la segunda guerra mundial (1939-1945).

La internacionalización del Ninjutsu viene de manos del maestro Masaaki Hatsumi, heredero de nueve tradiciones marciales antiguas del Japón (Ryu), entre ellas tres de origen ninja, y seis de origen samurai. Dentro de su organización, la llamada Bujinkan.

En la actualidad el ninjutsu se limita al uso de golpes, luxaciones articulares, lanzamientos, derribos y uso de armas tradicionales; buscando formar al individuo, de forma similar al conjunto de las artes marciales tradicionales modernas o gendai budo actual (como el Judo, el Aikido, el karate-Do, el Kendo, etc.) Aunque en los niveles más altos de esta disciplina se realizan seminarios muy exclusivos en Japón acerca de su faceta psicológica, esotérica, uso de venenos y de explosivos.

En realidad, no puede considerarse al antiguo ninjutsu como un arte marcial tradicional más; en el sentido clásico del término, ya que las disciplinas que el ninja debía conocer iban mucho más allá de las técnicas de lucha o de combate con y sin armas. Como ya se ha dicho, la práctica del Ninpo Mikkyo, o prácticas esotéricas, y del Kuji-kiri (corte de nueve sílabas, posiciones místicas con los dedos que canalizan la energía), el cual legendariamente proporcionaba al ninja poderes asombrosos, eran de estudio obligado para los clanes ninja, quienes preferían tácticas de terror y espionaje, mucho más sutiles que el clásico bujutsu o arte marcial del samurái.

Sin embargo, es un frecuente error histórico el considerar separadas conceptualmente las técnicas de combate del ninja y del guerrero samurái, dado que aquellas son una evolución o adaptación de éstas (según ciertos autores). Quizá por culpa de la industria cinematográfica y documentales erróneos, se tiende a considerar al ninja como el enemigo del samurái, cuando la realidad apunta a una posible simbiosis que los situaría en más estrecha comunión. Remarquemos que muchos líderes ninja eran a la vez samurái de renombre, que ocultaban su condición clandestina como indicaba la tradición; e inclusive muchos ninja servían como espías, e informantes a diferentes clanes feudales; sirviendo en contadas ocasiones como asesinos.

Hay quienes dicen que fue el príncipe Shotoku Taishi quien nombró "shinobi" a un gran estratega en la recolección de información. "Shi" significa hacedor, "No" que es experto y "Bi" que se lee como información y de ahí parece haber derivado el término. 

El entrenamiento ninja clásico contempla, al menos a nivel histórico, el aprendizaje de veinte disciplinas.:


En cambio, las dieciocho habilidades del bujutsu samurái eran las siguientes:


Como se puede ver, las habilidades del ninja son un refinamiento, o si se quiere una especialización de la forma de comprender el arte de la guerra del samurái. Esto nos acerca a una interrelación entre el ninja y el samurái, que lejos de ser similar a la que presenta la cinematografía, apunta quizá hacia una ósmosis, tanto a nivel de conocimiento, como de táctica y estrategia, e incluso de seres humanos. No en vano, famosos samurái fueron a la vez destacados ninja de clanes famosos, y viceversa. Incluso varias escuelas antiguas de tradición nítidamente noble, es decir, samurái, incluían el ninjutsu dentro de su programa, tanto en torno a la técnica como a las citadas táctica y estrategia.

El Arte Ninja se complementaría con otros conocimientos propios de modernos agentes de campo, como los primeros auxilios, la orientación, conocimientos de alimentación muy particulares, técnicas para andar y desplazarse en silencio o transportando heridos, el empleo de armas ocultas o camufladas (las llamadas "Kakushi Buki", y en definitiva todo aquello que fuese útil para su labor).

Las diversas escuelas de Nin Jutsu que existen en la actualidad emplean mayoritariamente un keikogi de color negro o azul oscuro. Pese a la creencia popular, el tradicional traje negro con capucha del ninja del folclore es solamente un concepto popular, derivado de las ropas que empleaban los tramoyistas del teatro kabuki, para confundirse con el fondo de color oscuro. La utilidad de ese ropaje para pasar inadvertido se asimiló sincréticamente como parte del atuendo del ninja en el folclore japonés.

Entre las escuelas o «Ryu» más conocidos (se cree que en Japón perduran unos pocos más reservados sólo a japoneses) a nivel mundial está la asociación Bujinkan del Soke Dr. Maasaki Hatsumi, y el estilo Genbukan del maestro Shoto Tanemura.





</doc>
<doc id="19418" url="https://es.wikipedia.org/wiki?curid=19418" title="Tejido adiposo">
Tejido adiposo

El tejido adiposo o tejido graso es el tejido de origen mesenquimal (un tipo de tejido conjuntivo) conformado por la asociación de células que acumulan lípidos en su citoplasma: los adipocitos.

El tejido adiposo, por un lado, cumple funciones mecánicas: servir como amortiguador, proteger y mantener en su lugar tanto a los órganos internos así como a otras estructuras externas del cuerpo, y funciones metabólicas: generar grasas para el organismo.

Existen dos tipos de tejido adiposo, el tejido adiposo blanco (o unilocular) y la grasa parda (o multilocular).

El citosol y el núcleo quedan reducidos a una pequeña área cerca de la membrana. El resto es ocupado por una gran gota de grasa. El tejido adiposo, que carece de sustancia fundamental, se halla dividido por finas trabéculas de tejido fascicular en lóbulos.

La grasa de las células se encuentra en estado semilíquido y también está compuesta fundamentalmente por triglicéridos. Se acumula de preferencia en el tejido subcutáneo, la capa más profunda de la piel. Sus células, lipocitos, están especializadas en formar y almacenar grasa. Esta capa se denomina panículo adiposo y es un aislante del frío y del calor. Actúa como una almohadilla y también como un almacén de reservas nutritivas.

Este tipo de tejido cumple funciones de relleno y de amortiguación, especialmente en las áreas subcutáneas. También sirve de soporte estructural y una función de reserva energética. La grasa varía de consistencia, es decir puede ser encontrada tanto en estado líquido como sólido.

El crecimiento de este tejido se puede producir por proliferación celular (crecimiento hiperplásico), por acumulación de una mayor cantidad de lípidos en las células ya existentes (crecimiento hipertrófico) pero nunca aumenta el número de adipocitos por división mitótica. Durante la adolescencia el crecimiento es, generalmente, rápido, lo mismo que en el adulto hipertrófico.

En los humanos, el tejido adiposo está localizado debajo de la piel (grasa subcutánea), alrededor de los órganos internos (grasa visceral), en la médula ósea (médula ósea amarilla) y en las mamas. El tejido adiposo está localizado en regiones específicas, las cuales se conocen como depósitos de adipocitos. Además de los adipocitos, que conforman el porcentaje más alto de células en el tejido adiposo, existen otras células que están presentes de manera colectiva denominadas fracción de estroma visceral (SVF). Este estroma está formado por preparatoria-adipocitos, fibroblastos, macrófagos de tejido adiposo, y células endoteliales. El tejido adiposo contiene pequeños vasos sanguíneos.

En el sistema tegumentario, el cual incluye la piel, el tejido adiposo se almacena en la capa más profunda de la piel regulando la temperatura del cuerpo.

Alrededor de los órganos, éste tejido brinda protección. Sin embargo su función principal es ser una reserva de lípidos, los cuales pueden ser utilizados para generar la energía necesaria para el cuerpo y protegernos del exceso de glucosa. Bajo condiciones normales, brinda estímulo de hambre y saciedad al cerebro.

Los ratones tienen ocho grandes depósitos de tejido adiposo, cuatro de ellos se encuentran en la cavidad abdominal. Los pares de depósitos gonadales están unidos al útero y a los ovarios en hembras y al epidídimo y a los testículos en los machos. De todos los depósitos en los ratones, los depósitos gonadales son los más grandes y los más fáciles de diseccionar, ya que comprende aproximadamente 30% de grasa.
Los depósitos pareados del retroperitoneo, se encuentran a lo largo de la pared dorsal del abdomen, rodeando al riñón y a veces se extiende hasta la pelvis.

El depósito del mesenterio forma una red de tejido que sirve de sostén para los intestinos; y por último, el depósito omental, el cual se origina cerca del estómago y el bazo, cuando crece anómalamente se extiende hasta el abdomen. Ambos depósitos almacenan tanto tejido linfoide como ganglios linfáticos y glóbulos blancos respectivamente.

Los dos depósitos superficiales son los depósitos pareados inguinales, los cuales se encuentran anteriores a las extremidades superiores (debajo de la piel); incluye el grupo de nódulos linfáticos de la ingle.

Los depósitos subescapulares no son una mezcla de tejido adiposo café y tejido adiposo blanco, que se encuentran debajo de la piel entre las crestas dorsales y las escápulas. La capa de tejido adiposos café en este depósito está normalmente recubierto por un “glaseado” de tejido adiposo blanco; algunas veces estos dos tipos de grasa (café y blanca) son difíciles de diferenciarse. Depósitos menores incluyen el pericardio, el cual rodea al corazón y los depósitos pareados poplíteos, ubicado en el músculo poplíteo (detrás de la rodilla), cada depósito poplíteo contiene un gran nódulo linfático.

En personas con obesidad mórbida, el exceso de tejido adiposo que cuelga hacia abajo desde el abdomen es referido como panículo. El panículo complica la cirugía de la obesidad mórbida, este permanece literalmente como un “delantal de piel” si una persona con obesidad grave pierde grandes cantidades de grasa muy rápido (resultado común de la cirugía del bypass gástrico). Esta condición no puede ser efectivamente corregida tan solo con la dieta y el ejercicio, ya que el panículo está conformado por adipocitos y otros tipos de células de soporte menguadas a su volumen y diámetro mínimos. La cirugía reconstructiva es una de las formas más viables de tratarlo.

De acuerdo con la International Agency for Research on Cancer (Agencia Internacional de Investigación del Cáncer), y con base en estudios epidemiológicos, las personas con obesidad o sobrepeso presentan mayor riesgo de desarrollar varios tipos de cáncer, como adenocarcinoma en el esófago, cáncer en el colon, cáncer de mama (en mujeres postmenopáusicas), cáncer endometrial y cáncer en los riñones.

La grasa visceral o abdominaltambién conocida como grasa intra-abdominal, es localizada dentro de la cavidad abdominal, almacenada entre los órganos (estómago, hígado, intestinos, riñones, etc.). La grasa visceral es distinta a la grasa subcutánea, ubicada debajo de la piel, o la grasa intramuscular, que esta dispersa en los músculos esqueléticos. La grasa en la parte inferior del cuerpo, como en los muslos y los glúteos, es subcutánea y no es un tejido consiente del espacio, mientras que la grasa en el abdomen es más visceral y con un estado semilíquido.La grasa visceral está compuesta por depósitos adiposos, incluyendo tejido mesentérico y tejido blanco adiposo del epidídimo, y depósitos perirrenales. La grasa visceral es considerada como tejido adiposo mientras que la grasa subcutánea no se le considera como tal.

Un exceso de grasa visceral es conocido como obesidad central, la cual sobresale del abdomen. También se le ha relacionado con diabetes tipo 2,resistencia a la insulina, enfermedades inflamatorias, y otra enfermedades relacionadas con obesidad.

La hormona del sexo femenino provoca que la grasa sea almacenada en muslos, glúteos y caderas de las mujeres.Los hombres son más propensos a tener grasa almacenada en el vientre debido a la diferencia hormonal que existe. Cuando las mujeres llegan a la menopausia y la producción de estrógeno en los ovarios disminuye, la grasa emigra de los muslos, glúteos y caderas a sus cinturas;que después será almacenada en su vientre.

Los ejercicios de alta intensidad es una forma efectiva en la cual la grasa abdominal puede ser reducida.Otro estudio demuestra que al menos 10 horas a la semana de gasto energético por medio de ejercicios aeróbicos es requerido para la reducción de grasa abdominal.

El tejido adiposo epicardial (EAT, Epicardial Adipose Tissue) es una forma particular de grasa visceral depositada alrededor del corazón y reconocida como un órgano activo del metabolismo que genera varias moléculas bioactivas, las cuales pueden afectar de forma significativa a la función cardiaca. Se han observado marcadas diferencias de componentes entre la grasa epicardial y la grasa subcutánea, sugiriendo así la existencia de un depósito de almacenamiento de ácidos grasos que impactan específicamente en las funciones y el metabolismo de adipocitos.

Mucha de la grasa restante no-visceral se encuentra justo debajo de la piel en una región llamada hipodermis. Esta grasa subcutánea no está relacionada con algunas de las patologías clásicas relacionadas de la obesidad, así como enfermedades del corazón, cáncer, y accidente cerebrovascular (CVA); e incluso, algunas evidencias sugieren que puede tener función protectora. El patrón típico de la distribución de grasa corporal femenina (o pelvis) alrededor de las caderas, muslos y piernas es grasa subcutánea, y es por tal que representa un menor riesgo de salud en comparación a la grasa visceral.

Así como otros órganos de grasa, la grasa subcutánea es parte activa del sistema endocrino, ya que secreta las hormonas leptina y resistina.

La relación entre la capa de tejido adiposo subcutáneo y la grasa corporal total en una persona es normalmente modelada al usar ecuaciones de regresión. La más popular de estas ecuaciones fue formulada por Durnin y Wormersley, quienes rigurosamente probaron muchos tipos de plegamiento de la piel y que, como resultado, crearon dos fórmulas para calcular la densidad del cuerpo de las mujeres y de los hombres. Estas ecuaciones presentan una correlación inversa entre los pliegues de la piel y la densidad corporal – al aumentar la suma de los pliegues de la piel, la densidad corporal disminuye.

Factores como el sexo, la edad, el tamaño de la población, economía, entre otras variables pueden invalidar y hacer no usables estas ecuaciones; hasta el 2012, la ecuación de Durnin y Wormersley permanece solo para estimar el verdadero nivel de grasa en una persona. Nuevas fórmulas aún se siguen creando.

Los ácidos grasos libres son liberados de la lipoproteína por una enzima llamada lipasa lipoproteíca; éstos ácidos grasos libres entran al adipocito, donde son reensamblados en triglicéridos. Aproximadamente el 87% del tejido graso de los humanos está compuesto por lípidos.

Existe un constante flujo de ácidos grasos libres. Dichos fluidos son controlados por la insulina y la leptina. Si tenemos una concentración elevada de insulina existe un incremento en el flujo de ácidos grasos libres, cuando la insulina baja, los ácidos grasos pueden ser liberados del tejido adiposo. La secreción de insulina es estimulada por la concentración elevada de azúcar o glucosa en sangre debido al consumo de carbohidratos.

En humanos, la lipólisis (hidrólisis de triglicéridos en ácidos grasos) es regulada por el balance controlado de los receptores andrógeno-B lipolítico y el receptor androgénico a2A, mediando la anti-lipólisis.
Los adipocitos tienen un papel fisiológico importante en la regulación de los niveles de los triglicéridos y los ácidos grasos libres, así mismo determinan la resistencia a la insulina.

La grasa abdominal tiene un metabolismo diferente, siendo más propenso a inducir la resistencia a la insulina. Esto explica porque la obesidad central es un precursor de la intolerancia a la glucosa siendo un factor independiente a enfermedades cardiovasculares (aún en la ausencia de diabetes mellitus e hipertensión).

En 2009 se realizaron estudios a monos hembra, en la Universidad de Wake Forest, en los cuales se descubrió que los individuos que sufrían de niveles altos de estrés tenían niveles más altos de grasa visceral en el cuerpo. Esto sugiere una posible causa-efecto donde el estrés promueve la acumulación de grasa visceral, lo cual se convierte en la causa de cambios hormonales y metabólicos que contribuyen a la aparición de enfermedades cardiovasculares y otros problemas de salud.
Recientes avances en biotecnología han permitido la cosecha de tejido adiposo de células adultas, permitiendo la estimulación del tejido utilizando las propias células del paciente. Aunado a eso se sabe que las células diferenciadas en adipocitos tanto de humanos como de animales, pueden ser reprogramadas a ser células pluripotenciales sin la necesidad de células de cultivo.El uso de células propias del paciente reduce el riesgo de rechazo y evita problemas éticos asociados al uso de células madre de un embrión.

El tejido adiposo es una gran fuente de aromatasa, que tanto en hombres como en mujeres contribuye a la producción de estradiol.
Las hormonas derivadas de adipocitos incluyen:

El tejido adiposo también secreta un tipo de citosinas, llamadas adipocinas, las cuales actúan en las complicaciones asociadas a la obesidad.

El tejido adiposo marrón es una forma especializada de tejido adiposo en humanos, roedores, mamíferos pequeños y en algunos animales hibernadores. Se encuentra principalmente alrededor del cuello y en grandes vasos sanguíneos del tórax. Este tejido especializado puede generar calor por "desacoplamiento" de la cadena respiratoria de la fosforilación oxidativa dentro de la mitocondria. El proceso de desacoplamiento se refiere a cuando los protones por el gradiente electroquímico pasan a través de la membrana mitocondrial interna, la energía de este proceso se libera en forma de calor en lugar de ser utilizada para generar ATP. Este proceso termogénico puede ser vital en los recién nacidos expuestos al frío, que a su vez requieren de esta termogénesis para mantener el calor, ya que son incapaces de temblar, o realizar otras acciones para mantenerse calientes.

Los intentos de simular este proceso de manera farmacológica hasta ahora han sido insatisfactorios (e incluso letales).)Las técnicas para manipular la diferenciación de la “grasa marrón” se han convertido en un mecanismo para una terapia de pérdida de peso en un futuro, fomentando el crecimiento de tejido con este metabolismo especializado sin inducirlo en otros órganos.

Hasta hace poco tiempo se pensaba que el tejido adiposo marrón estaba limitado principalmente a la etapa infantil en los seres humanos, pero la nueva evidencia ha anulado esa creencia. El tejido metabólicamente activo presenta respuestas de temperatura similares a las del tejido adiposo marrón, esto se registró por primera vez en el cuello y el tronco de algunos adultos humanos en 2007,y la presencia de tejido adiposo marrón en los adultos humanos se verificó posteriormente histológicamente en las mismas zonas anatómicas.

La hipótesis del gen ahorrador (también conocida como la hipótesis de hambre) afirma que en algunas poblaciones, el cuerpo sería más eficaz en la retención de grasa en tiempos de abundancia, con lo que dotarían una mayor resistencia a la inanición en tiempos de escasez de alimentos. Esta hipótesis ha sido desacreditada por los antropólogos físicos, fisiólogos y el autor de la propuesta original.

En 1995, Jeffrey Friedman, en su residencia en la Universidad Rockefeller, descubrió la leptina, proteína que al ratón genéticamente obeso le faltaba. La leptina es producida en el tejido adiposo blanco y por señalización al hipotálamo. Cuando los niveles de leptina bajan, el cuerpo lo interpreta como pérdida de energía, y el hambre aumenta. Los ratones que carecen de esta proteína comen hasta cuatro veces su tamaño normal.

Sin embargo la leptina, desempeña una función diferente en la obesidad por dieta en roedores y humanos. Debido a que los adipocitos producen leptina. Los niveles de leptina son elevados en la obesidad. Sin embargo, sigue siendo el hambre, y, cuando los niveles de leptina caen debido a la pérdida de peso, aumento de hambre. La caída de la leptina es mejor visto como una señal de hambre que el aumento de la leptina como una señal de saciedad. Sin embargo, la elevación de leptina en la obesidad se conoce como resistencia a la leptina. Los cambios que ocurren en el hipotálamo para dar lugar a resistencia a la leptina en la obesidad son actualmente el foco de investigación de la obesidad.

Los defectos genéticos en el gen de la leptina (ob) son poco frecuentes en la obesidad humana.
A partir de julio de 2010, solo han sido identificadas en todo el mundo 14 personas de cinco familias que llevan un gen mutado ob (una de las cuales fue la primera causa identificada de obesidad genética en los seres humanos): dos familias de origen pakistaní que viven en el Reino Unido, una familia que vive en Turquía, uno en Egipto y otro en Austria.- y otras dos familias se han encontrado que llevan un obreceptor mutado. otros han sido identificados genéticamente como parcialmente deficientes en leptina, y, en estos individuos, los niveles de leptina en el extremo inferior del rango normal se puede predecir la obesidad.
Varias mutaciones de genes que implican las melanocortinas (utilizados en la señalización del cerebro asociada con el apetito) y sus receptores también han sido identificados como causantes de la obesidad en una porción más grande de la población que las mutaciones de leptina.
En 2007, los investigadores aislaron el gen adiposo, los investigadores establecieron como hipótesis que este gen sirve para mantener a los animales delgados durante tiempos de abundancia. En ese estudio, el aumento de la actividad del gen adiposo se asoció con animales más delgados.Aunque sus descubridores denominaron a este gen el gen adiposo, este no es un gen responsable de la creación de tejido adiposo.

Un medidor de grasa corporal es una herramienta ampliamente disponible usada para medir el porcentaje de grasa en el cuerpo humano. Diferentes medidores utilizan varios métodos para determinar la grasa corporal, en relación con el peso. Estos tienden a subestimar el porcentaje de grasa del cuerpo.

En contraste con las herramientas clínicas, un tipo relativamente económico de medidor de grasa corporal utiliza el principio de análisis de impedancia bioeléctrica (BIA) para determinar el porcentaje de un individuo de grasa corporal. Para lograr esto, se pasa una pequeña corriente eléctrica inofensiva a través del cuerpo y mide la resistencia, a continuación, utiliza la información sobre el peso de la persona, altura, edad y sexo, para calcular un valor aproximado para el porcentaje de la grasa corporal de la persona. El cálculo mide el volumen total de agua en el cuerpo (tejido muscular magro y contienen un porcentaje más alto de agua que la grasa), y se calcula el porcentaje de grasa basado en esta información. El resultado puede fluctuar varios puntos porcentuales dependiendo de lo que se ha comido y la cantidad de agua se ha consumido antes del análisis.

Genneser Finn. Histología. (2666). Editorial Médica Panamericana. Tercera edición. Buenos Aires, Argentina, Colombia 1999 y México D.F Chile, la serena (2011)



</doc>
<doc id="19419" url="https://es.wikipedia.org/wiki?curid=19419" title="Frederik Pohl">
Frederik Pohl

Frederik Pohl (Nueva York, 26 de noviembre de 1919-Palatine, Illinois, 2 de septiembre de 2013) fue un escritor y editor estadounidense de ciencia ficción. Su carrera dentro del género se extendió durante más de 75 años y abarcó todo tipo de actividades: escritor, editor de libros, revistas y colecciones, agente literario, crítico, pero sobre todo fue reconocido como un destacado aficionado y promotor de la ciencia ficción.

Fuera de este campo destacó también como conferenciante y profesor en el área de la prospectiva. Fue también autoridad sobre el emperador Tiberio en la "Enciclopedia Británica".

Hijo de un comerciante, pasó su infancia viviendo en lugares tan dispares como Texas, California, Nuevo México o la zona del Canal de Panamá. Cuando tenía siete años su familia finalmente se asentó en el barrio de Brooklyn, en Nueva York. Allí estudió en el Brooklyn Technical High School, pero abandonó los estudios a los 17 años.

En 1936 sus ideas políticas sindicalistas, antirracistas y antifascistas le llevaron a unirse a las Juventudes Comunistas, en las que llegó a presidir una sección local, pero en 1939 las abandonó decepcionado por el pacto nazi-soviético. Durante la II Guerra Mundial sirvió en las fuerzas aéreas entre 1943 y 1945, y fue destinado a Italia.

Pohl se casó cinco veces. Su primer matrimonio fue con la también Futuriana Leslie Perri entre 1940 y 1944. En 1948 se casó, por tercera vez, con la también escritora de ciencia ficción Judith Merril con la que tuvo una hija, Ann. Pohl y Merril se divorciaron en 1952. Su cuarto matrimonio con Carol M. Ulf Stanton (1953-1983) le proporcionó 3 hijos. Desde 1984 hasta su muerte Pohl estuvo casado con la experta en ciencia ficción Elizabeth Anne Hull. La escritora canadiense Emily Pohl-Weary es nieta de Pohl y Merril.

En 1984 trasladó su residencia a Palatine, Illinois, cerca de Chicago.

Ya desde tierna edad fue un lector compulsivo de literatura popular, sobre todo de ciencia ficción, y escribió desde los once años en "fanzines" que él mismo organizaba y distribuía por Nueva York. Activo participante en el naciente "fandom" (grupo de seguidores), entra a formar parte de Brooklyn Science Fiction League (BSFL), una sección local de la "Science Fiction League" fundada por Hugo Gernsback en 1934. En ella conoce a Donald A. Wollheim, John Michel y Robert A. W. Lowndes, y juntos los cuatro pasan por sucesivos clubs de ciencia ficción hasta que en 1937 fundan su propio grupo, conocido como los Futurianos ("Futurians"). El grupo adquirió gran prominencia entre la masa de aficionados, pero la marcada ideología política de muchos de sus miembros les llevo a chocar con otra parte los seguidores de Nueva York. Esta situación cristalizó cuando a Pohl y otros Futurianos les fue prohibida la entrada en la 1.ª Worldcon. Con el tiempo el grupo se disolvió, pero las amistades y los contactos permanecieron, y un buen número de sus miembros terminaron convirtiéndose en importantes escritores y editores del género. De ese periodo también viene la amistad de Pohl con Cyril M. Kornbluth, Isaac Asimov o Larry T. Shaw por poner algunos ejemplos.

A los 19 años Pohl ya editaba dos revistas pulp de ciencia ficción: "Astonishing Stories" y "Super Science Stories". También escribía relatos en estas revistas, pero siempre bajo pseudónimos. Los trabajos escritos junto a Cyril M. Kornbluth aparecen bajo los nombres de S. D. Gottesman o Scott Mariner; otras colaboraciones las firma como Paul Dennis Lavond y su trabajo en solitario como James MacCreigh (y en una ocasión como Warren F. Howard). En su autobiografía, Pohl comenta que dejó de editar ambas revistas alrededor de 1941.

Tras la II Guerra Mundial, Pohl empezó a publicar material bajo su propio nombre, mucho de él en colaboración con su amigo Kornbluth. No obstante, siguió usando ocasionalmente pseudónimos como Charles Satterfield, Paul Flehr, Ernst Mason, Jordan Park (dos novelas escritas con Kornbluth) y Edson McCann (una con Lester del Rey). Es de este periodo la publicación de su clásico junto con Kornbluth "Mercaderes del espacio.

Su carrera como agente literario se remonta a 1937, pero no es hasta después de la II Guerra Mundial cuando se convierte en un trabajo a tiempo completo. Terminó "representando más de la mitad de de los escritores exitosos de ciencia ficción". Por un corto periodo de tiempo fue el agente literario de Isaac Asimov (de hecho, el único que este autor tuvo). Pese a todo ello, los resultados financieros no acompañaron a su agencia literaria, y tuvo que cerrarla a principios de la década de 1950.

A finales de la década de 1950, Pohl ayudó a un enfermo H. L. Gold en la dirección de las prestigiosas revistas de ciencia ficción "Galaxy Science Fiction" e "If", aunque no sería hasta de diciembre de 1961 cuando oficialmente se hiciera cargo de las mismas. Pohl revitalizó ambas revistas y consiguió durante su periodo la participación de los escritores más prestigiosos del momento. Fruto de este trabajo son los tres premios Hugo a la mejor revista profesional que "If" recibió en 1966, 1967 y 1968. También se encargó de dirigir la nueva revista "Worlds of Tomorrow" desde su primer número en 1963 hasta su fusión con "If" en 1967. En 1969 abandonó la dirección de ambas revistas.

En la década de 1970 Pohl reemerge como escritor de novelas, esta vez en solitario. "Homo Plus" le valió el premio Nébula en 1976, y "Pórtico", el primer volumen de la saga de los Heeche, se alzó con la victoria tanto en los Nébula de 1977 como en los Hugo de 1978. Por otro lado "Jem" (1980) se haría con el prestigioso Premio Nacional del Libro.

A mediados de la década de 1970 Pohl adquirió y editó novelas para Bantam Books, publicadas bajo el epígrafe "Selecciones Frederik Pohl"; entre ellas destacaron "Dhalgren" de Samuel R. Delany y "El hombre hembra" de Joanna Russ. También editó una serie de antologías de ciencia ficción.En 1987 publicó una novela titulada Chernobyl donde relata de forma novelada los hechos acontecidos en el accidente de la central nuclear de dicha ciudad Ucraniana.

En 1994 recibió el premio Gran Maestro Damon Knight Memorial a toda su carrera y en 1998 fue incluido en el Salón de la Fama de la Ciencia Ficción.

A partir de 1995 formó parte del jurado del premio Theodore Sturgeon Memorial, inicialmente junto a James Gunn y Judith Merril, y desde entonces y hasta su retiro en 2013 con varios otros. Pohl se había asociado con James Gunn desde la década de 1940, en lo que luego se convertiría en el Centro para el Estudio de la Ciencia Ficción en la Universidad de Kansas. En él presentó varias charlas y conferencias y participó en el taller literario de escritura de ciencia ficción.

Pohl recibió el segundo premio anual J. W. Eaton al trabajo de toda una vida en el campo de la ciencia ficción por la Biblioteca de la Universidad de California en Riverside, en el marco de la Conferencia Eaton de Ciencia Ficción: "Extraordinary Voyages: Jules Verne and Beyond" de 2009.

La última novela de Pohl, "All the Lives He Led" salió a la luz el 12 de abril de 2011. Estaba preparando una segunda edición de sus memorias "The way the future was" cuando le sorprendió la muerte en 2013.

Su primera obra maestra fue la novela "Mercaderes del espacio" (1953), una antiutopía satírica de un mundo gobernado por las agencias de publicidad escrita junto a su amigo y colaborador habitual C.M. Kornbluth. Otras novelas destacadas son "Homo Plus" (1976), que narra los esfuerzos por colonizar Marte adaptando el cuerpo humano al ambiente de ese mundo, y la "Saga de los Heechee", iniciada en 1977 con su novela "Pórtico", en la que se describen los restos de una civilización desaparecida cuyas vías de comunicación y tecnologías usan torpemente sin entenderlas los humanos. Igualmente destacó como un buen escritor de relatos, en los que se percibe el sesgo satírico contra el consumismo y la publicidad. En otra serie de novelas colaboró con especial asiduidad con Jack Williamson. Las obras más tardías, como la destacan por su imaginación y frescura.




</doc>
<doc id="19421" url="https://es.wikipedia.org/wiki?curid=19421" title="Accidente isquémico transitorio">
Accidente isquémico transitorio

El accidente isquémico transitorio (TIA, por sus siglas en inglés) es un accidente cerebrovascular de tipo isquémico en un episodio breve y pasajero de disfunción neurológica. Es causado por un disturbio focal por isquemia cerebral, retiniana o medular, con síntomas que duran menos de 1 hora y sin evidencia de infarto agudo. Se produce por la falta de aporte sanguíneo a una parte del cerebro, de forma transitoria, desapareciendo los síntomas, antes de 1 hora. Durante un AIT, la interrupción temporal del suministro sanguíneo a un área del cerebro ocasiona una reducción breve y repentina en la función cerebral. 

Cuando se afectan las arterias que son ramas de las arterias vertebrales (localizadas en la parte posterior del cuello), son frecuentes el mareo, la visión doble y la debilidad generalizada. 

Sin embargo, los AIT pueden manifestarse con muchos síntomas diferentes, tales como:

Aunque los síntomas son semejantes a los de un ictus (ACV), los AIT son transitorios y reversibles. Sin embargo, los episodios de AIT a menudo son recidivantes. La persona puede sufrir varias crisis diarias o sólo 2 o 3 episodios a lo largo de varios años. En el 35 por ciento de los casos un AIT se sigue de un ictus. Aproximadamente la mitad de estos ictus ocurren durante el año posterior al AIT.

Un AIT es de inicio súbito, y por lo general dura entre 2 y 30 minutos; algunas veces se prolonga más, de 1 a 2 horas.<br>
La mayoría de los signos y síntomas desaparecen en dos horas, aunque según la Mayo Clinic en raras ocasiones los síntomas pueden durar hasta 24 horas.

Existe una escala del NIH, para la determinación de una puntuación para guiar el diagnóstico del ACV isquémico, llamada NIHSS (National Institutes of Health Stroke Scale)   

La pérdida de circulación de sangre al cerebro puede ser causada por:

En un AIT, el flujo de sangre sólo se bloquea temporalmente. Por ejemplo, un coágulo sanguíneo puede disolverse y permitir que la sangre fluya de nuevo de manera normal.

La ateroesclerosis ("endurecimiento de las arterias") es una disfunción fisiológica por la que se presentan depósitos adiposos en el revestimiento interno de las arterias, lo que incrementa mucho el riesgo de AIT y de accidente cerebrovascular. La placa ateroesclerotica se forma cuando ocurre daño al revestimiento de una arteria. Las plaquetas se aglutinan alrededor del área de la lesión como parte normal del proceso de coagulación y cicatrización.

El colesterol y otras grasas también se acumulan en este sitio, formando una masa en el revestimiento de la arteria. Se puede formar un coágulo (trombo) en el sitio de la placa, desencadenado por flujo sanguíneo irregular en este lugar, y el trombo luego puede obstruir los vasos sanguíneos en el cerebro.

Se pueden desprender fragmentos de la placa o de los coágulos y viajar a través del torrente sanguíneo desde lugares distantes, formando un émbolo que puede obstruir las arterias pequeñas, causando AIT.

Casi una tercera parte de las personas con diagnóstico de AIT presentan posteriormente un accidente cerebrovascular. Sin embargo, alrededor de un 80 ó 90% de las personas que presentan accidente cerebrovascular secundario a la arterosclerosis tuvieron episodios AIT antes de presentarse dicho accidente. Aproximadamente, una tercera parte de las personas que sufren un AIT, presentarán otro AIT, mientras que una tercera parte presentan sólo un episodio de esta enfermedad. 
La edad en que se inicia varía, pero la incidencia aumenta significativamente después de los 50 años. El AIT es más común en los hombres y en afroamericanos.

Entre las causas menos comunes de AIT se encuentran trastornos sanguíneos (incluyendo policitemia, anemia drepanocítica y síndromes de hiperviscosidad, en los que la sangre es muy espesa), espasmos de las arterias pequeñas en el cerebro, anomalías de los vasos sanguíneos causados por trastornos como displasia fibromuscular, inflamación de las arterias (arteritis, poliarteritis, angiitis granulomatosa), lupus eritematoso sistémico y sífilis.

La hipotensión (presión sanguínea baja) puede precipitar los síntomas en individuos con una lesión vascular preexistente. Otros riesgos de AIT incluyen presión sanguínea alta (hipertensión), enfermedad cardíaca, migrañas, tabaquismo, diabetes mellitus y edad avanzada.

La escala ABCD es una puntuación clínica para determinar el riesgo de accidente cerebrovascular, dentro de los primeros dos días después de un accidente isquémico transitorio. Sus parámetros son: edad (Age), presión arterial (Blood pressure), las características Clínicas, y la Duración del AIT, y la presencia de la Diabetes.



El tratamiento agudo está dirigido fundamentalmente a salvar la isquemia del tejido comprometido. Este tejido puede ser rescatado si se restituye el flujo sanguíneo dentro de un lapso de tiempo corto. Una de las terapias medicamentosas puede ser la trombolisis endovenosa.
El manejo a mediano y largo plazo, se centra en el control de los factores de riesgo, en el manejo de la patología concomitante y en la rehabilitación.



</doc>
