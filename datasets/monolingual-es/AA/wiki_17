<doc id="2922" url="https://es.wikipedia.org/wiki?curid=2922" title="Vacío">
Vacío

El vacío (del latín "vacīvus") es la ausencia total de material en los elementos (materia) en un determinado espacio o lugar, o la falta de contenido en el interior de un recipiente. Por extensión, se denomina también vacío a la condición de una región donde la densidad de partículas es muy baja, como por ejemplo el espacio interestelar; o la de una cavidad cerrada donde la presión del aire u otros gases es menor que la atmosférica.

Puede existir naturalmente o ser provocado en forma artificial, puede ser para usos tecnológicos o científicos, o en la vida diaria. Se aprovecha en diversas industrias, como la alimentaria, la automovilística o la farmacéutica.

De acuerdo con la definición de la Sociedad Estadounidense del Vacío o AVS (1958), el término se refiere a cierto espacio lleno con gases a una presión total menor que la presión atmosférica, por lo que el grado de vacío se incrementa en relación directa con la disminución de presión del gas residual. Esto significa que cuanto más se disminuya la presión, mayor vacío se obtendrá, lo que permite clasificar el grado de vacío en correspondencia con intervalos de presiones cada vez menores. Cada intervalo tiene características propias.

La presión atmosférica es la que ejerce la atmósfera o aire sobre la Tierra. A temperatura ambiente y presión atmosférica normal, un metro cúbico de aire contiene aproximadamente en movimiento a una velocidad promedio de . Una manera de medir la presión atmosférica es con un barómetro de mercurio; su valor se expresa en términos de la altura de la columna de mercurio de sección transversal unitaria y de alto. Con base en esto, se dice que una atmósfera estándar es igual a . Se utilizara por conveniencia la unidad torricelli (símbolo, Torr) como medida de presión; = 1 mmHg, por lo que = ; por lo tanto = 1/760 de una atmósfera estándar, o sea = .

Uno de los métodos más conocidos para medir bajas presiones es el método desarrollado por Pirani.
Consiste en un puente de Wheatstone donde una resistencia del puente se encuentra expuesta al vacío a medir. La resistencia de ese elemento sensor variará según cambie la presión, debido a que a vacíos cerca de presión atmosférica el filamento estará en contacto con más moléculas, generando una baja de temperatura y por consiguiente una baja en su valor resistivo.
A medida que mejora el vacío este filamento ira encontrando menos moléculas para disipar su calor, por consiguiente aumentara su temperatura. Este aumento de temperatura producirá un aumento de su valor resistivo generando un desequilibrio en el puente de Wheatstone. Este desequilibrio se mide con un microamperimetro. Luego solo queda interpolar los microamperes generados por el puente de Wheatstone con los valores de vacío. Estos valores se vuelcan en una tabla con la que se dibuja una escala, donde por ejemplo en los vacuómetros CINDELVAC, se tendrá 0 microamperios cuando el sensor esté en alto vacío y 50 microamperios a presión atmosférica. La tabla de respuesta del puente de Wheatstone CINDELVAC es la siguiente:
Tienen el mismo fundamento que las bombas de ionización, hasta el punto que estas pueden considerarse como una consecuencia de aquellas. Cuando se trata de medir presiones de vacío muy bajas, se utilizan las variantes propuestas por Bayard-Alpert de aquellos aparatos capaces de suministrar con gran exactitud presiones de hasta .

El aire está compuesto por varios gases; los más importantes son el nitrógeno y el oxígeno, pero también contiene en menores concentraciones gases como dióxido de carbono, argón, neón, helio, criptón, xenón, hidrógeno, metano, óxido nitroso y vapor de agua.

Durante toda la Antigüedad y hasta el Renacimiento se desconocía la existencia de la presión atmosférica. No podían por tanto dar una explicación de los fenómenos debidos al vacío. En Grecia se enfrentaron por ello dos teorías. Para Epicuro y sobre todo para Demócrito (420 a. C.) y su escuela, la materia no era un todo continuo sino que estaba compuesta por pequeñas partículas indivisibles (átomos) que se movían en un espacio vacío y que con su distinto ordenamiento daban lugar a los distintos estados físicos. Por el contrario, Aristóteles excluía la noción de vacío y para justificar los fenómenos que su propia Física no podía explicar recurría al célebre aforismo según el cual «la Naturaleza siente horror al vacío» (teoría que resultó dominante durante la Edad Media y hasta el descubrimiento de la presión).

Este término de "horror vacui" fue el utilizado incluso por el propio Galileo a comienzos del al no poder explicar ante sus discípulos el hecho de que una columna de agua en un tubo cerrado por su extremo no se desprenda, si el tubo ha sido invertido estando sumergido el extremo libre del mismo dentro de agua. Sin embargo, supo transmitir a sus discípulos la inquietud por explicar el hecho anterior y asociado a él, por qué las bombas aspirantes-impelentes (órgano hidráulico inventado por el alejandrino Ctesibio, contemporáneo de Arquímedes) no podían hacer subir el agua de los pozos a una altura superior a los .

En 1630 Giovanni Battista Baliani envió una carta a Galileo Galilei donde le notificaba que no lograba que el agua en los sifones subiera más allá de . Galileo le propuso que la explicación era que el vacío no tenía fuerza suficiente nada más que para levantar esa cantidad de agua. En 1640 el italiano Gasparo Berti tratando de explicar lo que ocurría con los sifones realizó el primer experimento con el vacío. Creó lo que constituye, primordialmente, un barómetro de agua, el cual resultó capaz de producir vacío.

Al analizar el informe experimental de Berti, Evangelista Torricelli captó con claridad el concepto de presión de aire, por lo que diseñó, en 1644, un dispositivo para demostrar los cambios de presión en el aire. Construyó un barómetro que en lugar de agua empleaba mercurio, y de esta manera, sin proponérselo, comprobó la existencia del vacío.

El barómetro de Torricelli constaba de un recipiente y un tubo lleno de mercurio (Hg) cerrado en uno de sus extremos. Al invertir el tubo dentro del recipiente se formaba vacío en la parte superior del tubo. Esto era algo difícil de entender en su época, por lo que se intentó explicarlo diciendo que esa región del tubo contenía vapor de mercurio, argumento poco aceptable ya que el nivel de mercurio en el tubo era independiente del volumen del mismo utilizado en el experimento.

La aceptación del concepto de vacío se dio cuando en 1648, Blaise Pascal subió un barómetro con de mercurio a una montaña a . Sorprendentemente, cuando el barómetro estaba en la cima, el nivel de la columna de Hg en el tubo era mucho menor que al pie de la montaña. Torricelli aseguraba la existencia de la presión de aire y decía que debido a ella el nivel de Hg en el recipiente no descendía, lo cual hacía que el tamaño de la columna de mercurio permaneciera constante dentro del tubo. Así pues, al disminuir la presión del aire en la cima de la montaña, el nivel de Hg en el recipiente subió y en la columna dentro del tubo bajó inmediatamente (se vació de manera parcial).

El paso final que dio Torricelli fue la construcción de un barómetro de mercurio que contenía en la parte vacía del tubo otro barómetro para medir la presión de aire en esa región. Se hicieron muchas mediciones y el resultado fue que no había una columna de mercurio en el tubo del barómetro pequeño porque no se tenía presión de aire. Esto aclaró que no existía vapor de mercurio en la parte vacía del tubo. Así, se puso en evidencia la presión del aire y, lo más importante, la producción y existencia del vacío.

Entonces, después de varios experimentos se puede explicar bien el funcionamiento del barómetro de Torricelli: la atmósfera ejerce una presión, lo cual impide que el mercurio salga del tubo y del recipiente; es decir, cuando la presión atmosférica se iguale a la presión ejercida por la columna de mercurio, el mercurio no podrá salir del tubo. Cuando el aire pesa más, soporta una columna mayor de mercurio; y cuando pesa menos, no es capaz de resistir la misma columna de mercurio, así que se escapa un poco de mercurio del tubo.

En muchas ocasiones, en los laboratorios modernos, ocurren situaciones donde un contenedor lleno de un gas debe ser vaciado. La evacuación debe ser el primer paso para crear un nuevo ambiente gaseoso. Durante el proceso de destilación, se debe de remover de manera continua el gas a medida que se desarrolla el proceso. Algunas veces es necesario evacuar el contenedor para prevenir que el aire contamine alguna superficie limpia o que interfiera con alguna reacción química. Haces de partículas atómicas deben ser tratadas al vacío para prevenir la pérdida de momentum a través de las colisiones con las moléculas de aire. Muchas formas de radiación son absorbidas por el aire y por lo tanto solamente pueden ser propagadas sobre largas distancias en el vacío. Un sistema de vacío es una parte esencial para los instrumentos de laboratorio, tales como el espectómetro de masa y los microscopios electrónicos. Sistemas de vacío simples son utilizados para la deshidratación al vacío y la congelación al vacío. Aceleradores de partículas nucleares y dispositivos termonucleares requieren de sistemas de vacío muy sofisticados y de enormes proporciones. En procesos industriales modernos, dentro de los más notables la fabricación de semiconductores, se requieren de ambientes cuidadosamente controlados al vacío.

La presión y composición de los gases residuales en un sistema de vacío varía considerablemente con su diseño e historia. Para algunas aplicaciones una densidad de gas residual de decenas de miles de millones de moléculas por centímetro cúbico es tolerable. En otros casos, no más de unos cientos de miles de moléculas por centímetro cúbico constituyen un vacío aceptable.

Para presiones por debajo de la atmosférica se suele categorizar el vacío de la siguiente forma:

La composición del gas en un sistema de vacío se modifica a la vez que el sistema evacua debido a que la eficiencia de las bombas de vacío es diferente para diferentes gases. A bajas presiones las moléculas de las paredes del contenedor comienzan ser des absorbidas y se conforma el gas residual. Inicialmente, el grueso del gas que deja las paredes es vapor de agua y dióxido de carbono; a muy bajas presiones, en contenedores que han sido horneados, se tiene hidrógeno.




</doc>
<doc id="2923" url="https://es.wikipedia.org/wiki?curid=2923" title="Verrucariaceae">
Verrucariaceae

Verrucariaceae es una familia de líquenes que se caracteriza por tener el talo crustáceo y los gonidios de color verde intenso, además de no presentar nunca como gonidios clorófitas del orden Trentepohliales. La familia se divide en 21 géneros, los más conocidos de los cuales son "Verrucaria", "Polyblastia" y "Staurothele" que se diferencian en la configuración de las esporas y en la presencia o ausencia de gonidios himeniales.

Los géneros de esta familia son:
https://web.archive.org/web/20060426075352/http://www.biologie.uni-hamburg.de/b-online/ibc99/botanica/botanica/liquen2.htm

http://davesgarden.com


</doc>
<doc id="2924" url="https://es.wikipedia.org/wiki?curid=2924" title="VPN (desambiguación)">
VPN (desambiguación)

La sigla VPN puede referirse a:



</doc>
<doc id="2926" url="https://es.wikipedia.org/wiki?curid=2926" title="Vulcanismo del Campo de Calatrava">
Vulcanismo del Campo de Calatrava

La región volcánica del Campo de Calatrava (también llamada Provincia Volcánica de Calatrava) constituye, junto con la de Olot, en Gerona, y la de Cabo de Gata, en Almería, una de las tres zonas de vulcanismo reciente más importantes de la península ibérica. Su actividad se desarrolló entre hace 8,7 y 1,75 millones de años, es decir, durante el Plioceno y el Cuaternario. Es, por tanto, una actividad bastante reciente, lo que ha permitido que los edificios volcánicos conserven en buena parte su morfología original, y sus productos se hayan preservado en buenas condiciones de observación hasta la actualidad. 

La región volcánica tiene una extensión total de unos 5.000 km², e incluye unos 240 edificios volcánicos diferenciados. Algunas de las principales localidades que quedan incluidas dentro del área son Ciudad Real, Miguelturra, Almagro, Daimiel y Bolaños. Puertollano se sitúa próxima a su extremo Sur, mientras que los edificios volcánicos más próximos a Almadén son los de La Bienvenida y Cabezarados.

El vulcanismo estromboliano originó pequeños volcanes cónicos, actualmente degradados a cerros redondeados, de formas troncocónicas a semiesféricas, dependiendo del grado de erosión. Sus diámetros van desde los 100 m a los 2 km, y sus alturas, desde 20 a 120 m. Sólo ocasionalmente se identifican depresiones tipo cráter. De estos volcanes suelen partir coladas de lavas de diferente importancia, que pueden llevar a alcanzar los 6-7 km de longitud. Algunos de los mejores ejemplos de este tipo de volcán son los de La Yezosa, entre Bolaños de Calatrava y Almagro, y Cerro Gordo, en Valenzuela de Calatrava.

El vulcanismo hidromagmático es el más frecuente en la región, y da origen a unas depresiones volcánicas muy características, pero a menudo difíciles de identificar como tales en el terreno: se trata de los denominados "maares", que llegan a alcanzar diámetros de 1-1.5 km. Uno de los ejemplos más típicos puede ser la Hoya del Mortero, en Poblete. 

Por otra parte, es relativamente frecuente que se sucedan momentos de actividad hidromagmática y estromboliana a partir del mismo centro emisor.

En el relleno sedimentario de uno de los antiguos maares, en Alcolea de Calatrava, se encuentra el yacimiento de Las Higueruelas, un yacimiento paleontológico del Plioceno Superior, que ha proporcionado una abundante y variada fauna de mamíferos terrestres, aves, anfibios y reptiles.

Los materiales volcánicos que aparecen en el Campo de Calatrava son variados, tanto explosivos como efusivos, presentando cenizas, lapillis, escorias, bloques lávicos y bombas; y por otro lado, coladas de variada morfología, se manifiestan en fondos de valle y en vertientes. Los magmas son siempre básicos, por lo que la existencia de actividad explosiva que se ha registrado en ocasiones no parece muy acorde, esto se puede explicar por la existencia de acuíferos subterráneos, que con el calor del magma generaron una potente explosión freática o hidromagmática; o sobre todo por un exceso de gas carbónico en algunos magmas, que dieron lugar a la apertura de diatremas.

Las rocas volcánicas emitidas por estos volcanes corresponden a basaltos en sentido amplio: se pueden diferenciar una serie de variedades, tanto composicionales: melilititas olivínicas, limburgitas, nefelinitas olivínicas, basaltos y basanitas o leucititas olivínicas, como texturales: rocas porfídicas masivas, piroclastos escoriáceos, y depósitos hidromagmáticos. 

Las variedades porfídicas masivas presentan textura porfídica, y están constituidas por fenocristales de olivino o de olivino y piroxeno en matriz microcristalina a vítrea, formada por microcristales de augita, óxidos de hierro y titanio (magnetita-ilmenita) y olivino. Además pueden presentar plagioclasa, feldespatoides, melilita y vidrio, en proporciones variables, lo que permite la clasificación petrográfica más fina antes mencionada. 

En lo que se refiere a sus aplicaciones, estas variedades masivas se han empleado hasta fechas recientes en la obtención de adoquines para la pavimentación de calles. Su principal aplicación actual es la obtención de áridos de trituración y, en especial, para la obtención de balasto para el Tren de Alta Velocidad. Una de las principales canteras existentes sobre este tipo de materiales es la del Morrón de Villamayor. También tienen utilidad como rocas de construcción.

Las variedades piroclásticas escoriáceas son rocas muy vacuolares, de tipo "piedra pómez", que aparecen formando masas constituidas por fragmentos de estas rocas de tamaños muy variables: desde acúmulos de material de grano muy fino, pulverulento (cenizas), hasta acúmulos de grandes bloques, pasando por acumulaciones muy heterométricas de fragmentos de tamaño medio centi- a decimétrico (lapilli), con presencia ocasional de fragmentos de tamaño muy superior (bombas). A continuación se muestran algunos ejemplos de este tipo de rocas: 

Estos materiales se explotan en varias canteras de la región para la obtención de puzolanas, lo que constituye su principal aplicación industrial. Hay que indicar, por otra parte, que han sido también utilizados como piedra de construcción, en monumentos tan significados como el Castillo de Calatrava La Nueva, o la ermita visigótica de la Virgen de Zuqueca, en Oreto, antigua e importante ciudad romana (Granátula de Calatrava).

Los depósitos hidromagmáticos constituyen normalmente depósitos bien estratificados, en los que se suelen diferenciar facies planares, con laminación/estratificación paralela, y facies con estratificación cruzada. Además, suelen presentar grandes bombas de material no volcánico (cuarcitas, fundamentalmente). 

Corresponden a tobas líticas o lítico-cristalinas, poco consolidadas y heterométricas, formadas mayoritariamente por fragmentos de rocas paleozoicas (cuarcitas, pizarras) o terciarias, siendo poco abundantes los componentes volcánicos cogenéticos (fragmentos basálticos, cristales de olivino, piroxenos, etc.). 

No presentan utilidad industrial, más que para la obtención de áridos clasificados.

Desde el punto de vista geoquímico, las rocas volcánicas de la región del Campo de Calatrava corresponden a un magmatismo alcalino de intraplaca, generado a partir de bajas tasas de fusión parcial del manto superior. Los magmas serían líquidos primarios, como indican los altos contenidos en Ni y el alto valor del parámetro #Mg (=MgO/MgO+FeO). 

La tabla adjunta muestra la composición química media y la norma CIPW calculada de las diferentes variedades petrográficas porfídicas. 

Estos caracteres geoquímicos, y el estudio de su evolución espacial y temporal, permiten establecer que el magmatismo de la región del Campo de Calatrava podría estar relacionado con la existencia de un punto caliente asociado a un proceso de elevación cortical y posiblemente de "rifting" abortado. 

Asociado a este magmatismo encontramos una serie de yacimientos minerales, de escasa importancia minera, ya que por lo general presentan escaso tonelaje, pero que constituyen un tipo único a nivel mundial. Se trata de mineralizaciones de óxidos de Fe y de Mn, las segundas con el interés añadido de que presentan contenidos relativamente elevados en Co, lo que ha hecho que hasta fecha reciente hayan sido objeto de prospección minera, con ánimo de localizar alguna masa de suficiente volumen como para permitir su explotación. 

Son yacimientos de origen sedimentario, que encontramos como niveles dentro de las secuencias del Plioceno y Cuaternario, constituyendo masas lenticulares de cierta potencia (hasta varios metros) y extensión lateral (varios cientos de metros, en los mejores casos). Su origen parece estar relacionado con el de otras manifestaciones características del área y mucho más conocidas: los manantiales de "agua agria" o "hervideros", el más conocido de los cuales podría ser la "Fuente Agria" de Puertollano. El nexo genético sería que ambos, mineralizaciones y manantiales, serían manifestaciones de actividad hidrotermal póstuma ligada al magmatismo. 

En lo que se refiere a las mineralizaciones, se pueden establecer dos grandes tipos: 

Desde el punto de vista mineralógico, los minerales que podemos encontrar en estos yacimientos son óxidos e hidróxidos complejos de Mn (criptomelana y litioforita, fundamentalmente). Son minerales de hábito terroso, micro- o criptocristalinos, sin apenas interés para coleccionismo. 

Las costras de óxidos de Fe-Mn son formaciones lenticulares de algunos metros de espesor por varios centenares de metros cuadrados de extensión, en general asociadas a alguna surgencia de aguas agrias. Las costras están formadas por nódulos de óxidos de Mn cobaltífero recubiertos por una corteza de 1-1.5 cm de espesor de óxidos e hidróxidos de hierro. Uno de los yacimientos más representativos de esta tipología es el de la mina de La Zarza, localizada a unos 2 km al SSO de Pozuelo de Calatrava. 

Las capas de "canutillos" constituyen acumulaciones de pequeñas estructuras vegetales reemplazadas por óxidos de Mn cobaltífero, que aparecen formando niveles de hasta 2-3 m de potencia entre materiales de tipo aluvial. La mina de El Chorrillo, situada en proximidad de la de La Zarza, es uno de los mejores ejemplos de este tipo de mineralizaciones.

Las capas con pisolitos de óxidos de Mn corresponden a mineralizaciones que han sufrido un cierto transporte con respecto a los focos hidrotermales. Están formadas por niveles lenticulares en los que son muy abundantes las estructuras pisolíticas, de diámetro centimétrico, constituidas por los óxidos e hidróxidos de Mn. El yacimiento de Los Ardales se puede considerar representativo de esta tipología. 






</doc>
<doc id="2927" url="https://es.wikipedia.org/wiki?curid=2927" title="Encefalopatía espongiforme bovina">
Encefalopatía espongiforme bovina

La encefalopatía espongiforme bovina, también conocida popularmente como la enfermedad de las vacas locas, es una enfermedad causada por priones, y que se puede transmitir a los seres humanos a través del consumo de partes de animales infectados, sobre todo tejidos nerviosos.

Ángel encefalopatía espongiforme bovina (EEB) o enfermedad de las vacas locas es una enfermedad que pertenece a una misteriosa familia de enfermedades emparentadas, muy raras en su mayoría. Los primeros casos de animales enfermos se declararon en el Reino Unido en 1986. En 1996 se detectó en el ser humano una nueva enfermedad, una variante de la Enfermedad de Creutzfeldt-Jakob, que se relacionó con la epidemia de EEB en el ganado vacuno.

Los síntomas no se observan inmediatamente en el ganado debido al período de incubación extremadamente largo de la enfermedad. Se ha observado que algunos bovinos tienen un modo de andar anormal, cambios en el comportamiento, temblores e hiperactividad ante ciertos estímulos. La ataxia de la extremidad trasera afecta la marcha del animal y ocurre cuando se pierde el control muscular. Esto da como resultado un equilibrio y una coordinación pobres. Los cambios de comportamiento pueden incluir agresión, ansiedad relacionada con ciertas situaciones, nerviosismo, frenesí o un cambio general en el temperamento. Algunos síntomas raros, pero observados previamente también incluyen el caminar de un lado para otro, el roce o la lamedura persistentes. Además, también se han observado síntomas inespecíficos que incluyen pérdida de peso, disminución de la producción de leche, cojera, infecciones del oído y rechinamiento de los dientes debido al dolor. Algunos animales pueden mostrar una combinación de estos síntomas, mientras que otros solo se observan demostrando uno de los muchos reportados. Una vez que surgen los síntomas clínicos, generalmente empeoran durante las próximas semanas y meses, lo que finalmente conduce a la recumbencia, el coma y la muerte.

El diagnóstico de EEB sigue siendo un problema práctico. Tiene un período de incubación de meses a años, durante el cual no se notan síntomas, aunque se ha iniciado la vía para convertir la proteína prion cerebral normal (PrP) en la forma de PrPSc tóxica relacionada con la enfermedad. En la actualidad, prácticamente no se conoce ninguna forma de detectar PrPSc de manera confiable excepto mediante el examen del tejido cerebral post mortem usando métodos neuropatológicos e inmunohistoquímicos. La acumulación de PrPSc en forma de PrP anormalmente plegada es una característica de la enfermedad, pero está presente a niveles muy bajos en fluidos corporales fácilmente accesibles tales como sangre u orina. Los investigadores han intentado desarrollar métodos para medir PrPSc, pero no se han aceptado completamente los métodos para su uso en materiales como la sangre.

El método tradicional de diagnóstico se basa en el examen histopatológico de la médula oblonga del cerebro y otros tejidos, post mortem. La inmunohistoquímica puede usarse para demostrar la acumulación de proteína priónica.

En 2010, un equipo de Nueva York describió la detección de PrPSc incluso cuando inicialmente estaba presente en solo una parte en cien mil millones (10-11) en tejido cerebral. El método combina la amplificación con una nueva tecnología llamada inmunoensayo de fibra óptica envolvente y algunos anticuerpos específicos contra PrPSc. Después de amplificar y luego concentrar cualquier PrPSc, las muestras se etiquetan con un colorante fluorescente usando un anticuerpo para especificidad y luego se cargan en un tubo microcapilar. Este tubo se coloca en un aparato especialmente construido, por lo que está totalmente rodeado de fibras ópticas para capturar toda la luz emitida una vez que el tinte se excita utilizando un láser. La técnica permitió la detección de PrPSc después de muchos menos ciclos de conversión que otros lograron, reduciendo sustancialmente la posibilidad de artefactos, así como la aceleración del ensayo. Los investigadores también probaron su método en muestras de sangre de ovejas aparentemente sanas que desarrollaron tembladera. Los cerebros de los animales se analizaron una vez que los síntomas se hicieron evidentes. Los investigadores podrían, por lo tanto, comparar los resultados del tejido cerebral y la sangre tomada una vez que los animales exhibieron los síntomas de las enfermedades, con sangre obtenida antes en la vida de los animales, y de animales no infectados. Los resultados mostraron muy claramente que PrPSc podría detectarse en la sangre de los animales mucho antes de que aparecieran los síntomas. Después de un mayor desarrollo y pruebas, este método podría ser de gran valor en la vigilancia como una prueba de detección basada en sangre u orina para la EEB.

Es una enfermedad degenerativa del sistema nervioso central de los bovinos, que se caracteriza por la aparición de síntomas nerviosos en los animales adultos que, progresivamente, finaliza con la muerte del animal.

La enfermedad está causada por una proteína que ha modificado su estructura tridimensional (en Bioquímica, se denominan estructuras secundaria y terciarias de las proteínas), debido un proceso denominado cambio conformacional, y que las convierte en un agente patológico. Estas proteínas infecciosas se denominan priones. El periodo de incubación de la enfermedad es de 4 o 5 años. Esta proteína es la Prp, que en su variante normal (conformación Nativa) es c, pero al entrar en contacto con la proteína en la conformación no nativa pasa a ser Prp (Sc) y en cadena. Ésta, al entrar en contacto con la proteína normal (c) del organismo le induce un cambio conformacional y provoca el paso a la Sc. Es una proteína fisiológica y no se ha podido eliminar del organismo.

Los síntomas que se observan están motivados por la acumulación del prion en las células neuronales originando la muerte celular. Un análisis microscópico revela lesiones como vacuolas que dan al tejido nervioso un aspecto de esponja.

La vía de transmisión de esta enfermedad conocida hasta la fecha es la ingestión de alimentos contaminados con el prion, la administración de fármacos de origen bovino y provenientes de animales enfermos (típicamente hormona del crecimiento) y posiblemente de madre a hijo. El único método disponible para detectar la infección en fase terminal es la inoculación parenteral de tejido encefálico en ratones. No obstante, esta técnica no es utilizable en la práctica ya que los períodos de incubación son de unos 300 días.

La enfermedad se acumula sobre todo en el cráneo (incluidos encéfalo y ojos), la amígdala, la médula espinal, el intestino (del duodeno al recto) y el bazo.

Alan Colchester de la Universidad de Kent propuso en septiembre de 2005 en la revista médica "The Lancet" que la enfermedad pudo haberse originado a través de alimento para ganado procedente de la India, contaminado con restos humanos. El gobierno de la India lo negó rotundamente, calificando a la investigación de "engañosa, maliciosa; producto de la imaginación; absurda," añadiendo que la India mantiene controles constantes y que no han tenido ningún caso de EEB o vECJ. La mayoría de los científicos piensan que la enfermedad se originó en los propios animales y en el consumo de restos no humanos.

Los científicos han aceptado que la aparición de esta enfermedad estuvo determinada por la alimentación suplementaria del ganado bovino con restos de ganado ovino y caprino (que ya presentaban la enfermedad, pero no se trasmitía a humanos, denominada scrapie), lo que conllevó a que en 1998 en Reino Unido se sacrificaran e incineraran a los animales sospechosos de haber adquirido la enfermedad.

La patogénesis de la EEB no se entiende bien ni se documenta como otras enfermedades de esta naturaleza. A pesar de que la EEB es una enfermedad que produce defectos neurológicos, su patogénesis ocurre en áreas que residen fuera del sistema nervioso. Hubo una fuerte deposición de PrPSc inicialmente localizada en las placas del intestino delgado de Ileal Peyer. El sistema linfático ha sido identificado en la patogénesis de los recortes. Sin embargo, no se ha determinado que sea una parte esencial de la patogénesis de la EEB. Los parches de Ileal Peyer han sido el único órgano de este sistema que se ha encontrado que desempeña un papel importante en la patogénesis. La infectividad de los parches de Ileal Peyer se ha observado a los 4 meses de la inoculación. Se encontró que la acumulación de PrPSc ocurre principalmente en macrófagos corporales tangibles de los parches de Ileal Peyer. Se cree que los macrófagos corporales tangibles implicados en la eliminación de PrPSc juegan un papel en la acumulación de PrPSc en los parches de Peyer. La acumulación de PrPSc también se encontró en células dendríticas foliculares; sin embargo, fue en menor grado. Seis meses después de la inoculación, no había infectividad en ningún tejido, solo el del íleon. Esto llevó a los investigadores a creer que el agente de la enfermedad se reproduce aquí. En casos naturalmente confirmados, no ha habido informes de infectividad en los parches de Ileal Peyer. Generalmente, en experimentos clínicos, se administran altas dosis de la enfermedad. En casos naturales, se formuló la hipótesis de que estaban presentes dosis bajas del agente y, por lo tanto, no se pudo observar la infectividad.

Hasta 2007, inclusive, se declararon 336.799 reses enfermas de EEB en la Unión Europea y 516 más en el resto del mundo, la inmensa mayoría en el Reino Unido: el 98,38%. Solo en Gran Bretaña fueron sacrificadas más de 2 millones de reses.

Por otra parte, hasta junio de 2010 se diagnosticaron 220 pacientes humanos afectados por la nueva variante de la Enfermedad de Creutzfeldt-Jakob, 217 casos primarios y 3 secundarios (por una transfusión de sangre).

El 18 de octubre de 2018 el gobierno de Escocia confirmó la identificación de un caso de EEB en una granja de Aberdeenshire.




</doc>
<doc id="2928" url="https://es.wikipedia.org/wiki?curid=2928" title="Integración a muy gran escala">
Integración a muy gran escala

La integración a escala muy grande o VLSI (sigla en inglés de "very large-scale integration") es el proceso de crear un circuito integrado compuesto por millones de transistores en un único chip. VLSI comenzó a usarse en los años 70, como parte de las tecnologías de semiconductores y comunicación que se estaban desarrollando.

Los primeros chip semiconductores contenían solo un transistor cada uno. A medida que la tecnología de fabricación fue avanzando, se añadieron más y más transistores y, debido a ello, más y más funciones fueron integradas en un mismo chip. El microprocesador es un dispositivo VLSI.

La primera generación de computadoras dependía de válvulas de vacío. Luego vinieron los semiconductores discretos, seguidos de circuitos integrados. Los primeros circuitos integrados contenían un pequeño número de dispositivos, como diodos, transistores, resistencias y condensadores (aunque no inductores), haciendo posible la fabricación de compuertas lógicas en un solo chip. La cuarta generación (LSI) consistía de sistemas con al menos mil compuertas lógicas. El sucesor natural del LSI fue VLSI (varias decenas de miles de compuertas en un solo chip). Hoy en día, los microprocesadores tienen varios millones de compuertas en el mismo chip.

Hacia principios de 2006 se comercializaban microprocesadores con tecnología de hasta 65 nm, en 2010 se comercializan chips con tecnología de 32 nm.





</doc>
<doc id="2929" url="https://es.wikipedia.org/wiki?curid=2929" title="Ventenata">
Ventenata

Ventenata, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia.
Son plantas anuales con culmos de 10-70 cm de alto; herbácea. Los culmos con nodos glabros. Los entrenudos huecos. Los brotes no aromáticos. Hojas no agregadas basalmente; no auriculadas. Las láminas de las hojas son lineales; estrechas; de 1-2.5 de ancho, dobladas o enrolladas (convolutas); sin venación. Lígula una membrana no truncada (aguda, a menudo lacerada); 2-4 mm de largo. Inflorescencia paniculada, o un solo racimo; abierto, o comprimido.
El género fue descrito por Georg Ludwig Koeler y publicado en "Descriptio Graminum in Gallia et Germania" 272. 1802. 


</doc>
<doc id="2930" url="https://es.wikipedia.org/wiki?curid=2930" title="Vulpia">
Vulpia

Vulpia es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del norte de Asia y Norteamérica.
Son plantas anuales, a veces cespitosas o estoloníferas. Hoja con vaina con márgenes libres; lígula truncada, a menudo lacerada; limbo setáceo y convoluto, al menos en la desecación, rara vez plano. Inflorescencia en panícula o en racimo, generalmente unilateral. Espiguillas con pedúnculos generalmente dilatados, cuneiformes, marcadamente comprimidas, con 3-10 flores, las superiores frecuentemente estériles; raquilla escábrida o pelosa. Espiguillas desarticulándose por debajo de las flores o por la base de los pedúnculos. Glumas 2, muy desiguales, generalmente más cortas que las flores, herbáceas, agudas, múticas o aristadas; la inferior sin nervios o uninervadas, rara vez trinervadas; la superior más larga que la inferior, trinervada. Lema con (3-) 5 nervios, lanceoladas, papiráceas, generalmente con 1 arista apical, antrorso-escábrida. Pálea algo más corta que la lema, membranosa, bidentada, con 2 quillas antrorso-escábrida o ciliadas. Androceo con 1-3 estambres fértiles. Ovario glabro. Cariopsis estrechamente elipsoidea, soldadas a la pálea.
El género fue descrito por Carl Christian Gmelin y publicado en "Flora Badensis Alsatica" 1: 8. 1805. La especie tipo es: "Vulpia myuros"
El nombre del género fue nombrado en honor de J.S.Vulpius (1760–1840) 
Tiene un número de cromosomas de: x = 7. 2n = 14, 28, y 42. 2, 4, y 6 ploidias. Cromosomas ‘grandes’.

Existen híbridos intergenéricos entre especies de "Vulpia" y de "Festuca" formando - "×Festulpia" Melderis ex Stace & R.Cotton.




</doc>
<doc id="2931" url="https://es.wikipedia.org/wiki?curid=2931" title="Videojuego">
Videojuego

Un videojuego es un juego electrónico en el que una o más personas interactúan, por medio de un controlador, con un dispositivo que muestra imágenes de video. Este dispositivo electrónico, conocido genéricamente como «plataforma», puede ser una computadora, una máquina "arcade", una videoconsola o un dispositivo portátil, como por ejemplo un teléfono móvil. Los videojuegos son, año por año, una de las principales industrias del arte y el entretenimiento.

Al dispositivo de entrada, usado para manipular un videojuego se le conoce como controlador de videojuego, o mando, y varía dependiendo de la plataforma. Por ejemplo, un controlador podría únicamente consistir de un botón y una palanca de mando o "joystick", mientras otro podría presentar una docena de botones y una o más palancas (mando). Los primeros juegos informáticos solían hacer uso de un teclado para llevar a cabo la interacción, o bien requerían que el usuario adquiriera un "joystick" con un botón como mínimo. Muchos juegos de computadora modernos permiten o exigen que el usuario utilice un teclado y un ratón de forma simultánea. Entre los controladores más típicos están los "gamepads", "joysticks", teclados, ratones y pantallas táctiles.

Generalmente, los videojuegos hacen uso de otras maneras, aparte de la imagen, de proveer la interactividad e información al jugador. El audio es casi universal, usándose dispositivos de reproducción de sonido, tales como altavoces y auriculares. Otro tipo de realimentación se hace a través de periféricos hápticos que producen vibración o retroalimentación de fuerza, usándose a veces la vibración para simular la retroalimentación de fuerza.

Los orígenes del videojuego se remontan a la década de 1950, cuando poco después de la aparición de las primeras computadoras electrónicas tras el fin de la Segunda Guerra Mundial, se llevaron a cabo los primeros intentos por implementar programas de carácter lúdico. Así, fueron creados el "Nim" (1951) o el "Oxo" (1952), juegos electrónicos pero que aún no son realmente videojuegos, y el "Tennis for Two" (1958) o el "Spacewar!" (1962), auténticos pioneros del género. Todos ellos eran todavía prototipos, juegos muy simples y de carácter experimental que no llegaron a comercializarse, entre otras cosas, porque funcionaban en unas máquinas que solo estaban disponibles en universidades o en institutos de investigación.
No sería hasta la década de los 70 en que, con el descenso de los costes de fabricación, aparecieron las primeras máquinas y los primeros videojuegos dirigidos al gran público. Títulos como "Computer Space" (1971) o "Pong" (1972), de Atari, inauguraron las primeras máquinas recreativas construidas al efecto, que funcionaban con monedas. Poco después llegarían los videojuegos a los hogares gracias a las consolas domésticas, la primera de las cuales fue la Magnavox Odyssey (1972), y más tarde la exitosa Atari 2600 o VCS (de 1977), con su sistema de cartuchos intercambiables. Por aquel entonces las máquinas arcade empezaron a hacerse comunes en bares y salones recreativos, una expansión debida en parte a un matamarcianos que alcanzó gran popularidad, el "Space Invaders" (1978). Otros juegos que marcaron esta primera época fueron "Galaxian" (1979), "Asteroids" (1979) o "Pac-Man" (1980).
En los años 1980, la empresa norteamericana Atari tuvo que compartir su dominio en la industria del videojuego con dos compañías llegadas de Japón: Nintendo (con su famosa consola NES) y SEGA (con la Master System). Paralelamente, surgió una generación de ordenadores personales asequibles y con capacidades gráficas que llegaron a los hogares de millones de familias, como fueron el Spectrum, el Amstrad CPC, el Commodore 64 o el MSX. A partir de entonces, los videojuegos empezaron a convertirse en una poderosa industria. Fue además una época muy creativa para los desarrolladores de videojuegos; muchos de los principales géneros que existen hoy en día (conducción, lucha, plataformas, estrategia, aventura...) tomaron forma en esta década. Por otra parte, aparecieron también las primeras consolas de bolsillo, también conocidas como «maquinitas», que aunque hasta la llegada de la Game Boy de Nintendo (1989) solo ejecutaban un juego cada una, alcanzaron gran popularidad entre los más jóvenes.

Los años 1990 traen el salto a la tecnología de 16-bit (como la SNES y la Mega Drive), lo que significa importantes mejoras gráficas. Entra en escena el gigante Sony con su primera PlayStation (1994), mientras Nintendo y Sega actualizan sus máquinas (Nintendo 64 y Sega Saturn). En cuanto a las computadoras, el progreso de los PC termina por barrer del mapa a los demás sistemas salvo el de Apple. Aparecen juegos cada vez más avanzados tecnológicamente, como los shooters en 3D. En el año 2002 entra Microsoft en el sector de las videoconsolas con su Xbox, y en el 2006 Nintendo lanza su innovadora Wii. Entretanto, Sony actualiza su exitosa PlayStation (versiones II y III), y en los PC, gracias a la expansión de internet, cobran protagonismo los juegos en línea y multijugador.

Por último, en la década de 2010 emergen como plataformas de juegos los dispositivos táctiles portátiles, como los teléfonos inteligentes y las tabletas, llegando a un público muy amplio. Por otro lado, varias empresas tecnológicas empiezan a desarrollar cascos de realidad virtual, que prometen traer nuevas experiencias al mundo del entretenimiento electrónico.

Típicamente, los videojuegos recrean entornos y situaciones virtuales en los que el videojugador puede controlar a uno o varios personajes (o cualquier otro elemento de dicho entorno), para conseguir uno o varios objetivos dentro de unas reglas determinadas.

Dependiendo del videojuego, una partida puede disputarla una sola persona contra la máquina, dos o más personas en la misma máquina, o bien múltiples jugadores a través de una red LAN o en línea vía Internet, compitiendo colaborativamente contra la máquina o entre sí.

Existen videojuegos de muchos tipos. Algunos de los géneros más representativos son los videojuegos de acción, rol, estrategia, simulación, deportes o aventura.

Un videojuego se ejecuta gracias a un programa de "software "(el videojuego en sí) que es procesado por una máquina (el "hardware") que cuenta con dispositivos de entrada y de salida.

El programa de "software" o soporte lógico contiene toda la información, instrucciones, imágenes y audio que componen el videojuego. Va grabado en cartuchos, discos ópticos, discos magnéticos, tarjetas de memoria especiales para videojuegos, o bien se descarga directamente al aparato a través de Internet.

En la década de 1980 el soporte habitual para el "software" era el cartucho en las videoconsolas, y el disco magnético o la cinta de casete en los ordenadores. Posteriormente se usó el CD-ROM, pues tenía más capacidad que los cartuchos ya que estos habían llegado a su tope tecnológico y además resultaba más económico para producir en masa. Actualmente se usa el sistema DVD de alta capacidad y, en las consolas de sobremesa como PlayStation 4 y Xbox One, el Blu-Ray, de capacidad muy alta. Sin embargo desde hace unos años está creciendo la descarga desde Internet, al ser una tecnología extendida masivamente, de fácil acceso y menos costosa que la distribución física de discos, aparte de las ventajas de seguridad al evitar pérdidas por daños o extravío de discos (ya que el videojuego estará virtualmente siempre disponible).
Los dispositivos de entrada son los que permiten al jugador manejar el juego. Si bien es habitual el uso de un dispositivo de entrada externo —como son los clásicos teclado y ratón, el mando, o el "joystick"—, las plataformas portátiles de hoy en día ("smartphones", "tablets", videoconsolas de bolsillo...) permiten jugar mediante su pantalla táctil o mediante el movimiento del propio aparato (gracias al uso de giroscopios y acelerómetros). Otros dispositivos de entrada son los detectores de movimiento, entre los que destacan los dispositivos de mano (por ejemplo el Wiimote de Wii), los de presión (alfombras o soportes con sensores), los de dispositivos de realidad virtual como la PlayStation VR y los de captura de imágenes, caso del Kinect de Xbox. También se puede emplear la voz en aquellos videojuegos que la soporten a través de procesadores de voz.

Los dispositivos de salida son aquellos que muestran las imágenes y los sonidos del videojuego: un televisor, un monitor o un proyector para el vídeo, y unos altavoces o auriculares para el audio. Los equipos más modernos utilizan sonido digital con Dolby Surround con efectos EAX y efectos visuales modernos por medio de las últimas tecnologías en motores de videojuego y unidades de procesamiento gráfico.

La pieza central del hardware lo constituye la CPU o unidad central de procesamiento, que interpreta las instrucciones contenidas en los programas y procesa los datos. Su capacidad de procesamiento, mayor en cada nueva generación de dispositivos, marca el límite de las posibilidades técnicas y gráficas de los videojuegos.

Todos estos dispositivos (de entrada, de salida, de procesamiento...) pueden constituir unidades físicamente separadas pero conectadas entre sí —como es el caso de los PC o las videoconsolas de sobremesa—, o bien estar integradas en un solo aparato —como sucede en los teléfonos móviles y otros dispositivos portátiles—.

El 21 de agosto Nvidia presentó la nueva serie de procesadores gráficos GeForce RTX 20, los cuales contarán con arquitectura de Turing y desarrollan el trazado de rayos a tiempo real para llevar la experiencia de juego hasta una reproducción de vídeo en 8K.

Los distintos tipos de dispositivo en los que se ejecutan los videojuegos se conocen como plataformas. Los cuatro tipos de plataformas más populares son el PC, las videoconsolas, los dispositivos portátiles y las máquinas arcade.
Las videoconsolas o consolas de videojuegos son aparatos electrónicos domésticos destinados exclusivamente a reproducir videojuegos. Creadas por diversas empresas desde los años 70, han generado un inmenso negocio de trascendencia histórica en la industria del entretenimiento. La videoconsola por antonomasia es un aparato de sobremesa que se conecta a un televisor para la visualización de sus imágenes, pero existen también modelos de bolsillo con pantalla incluida, conocidos como videoconsolas portátiles.

El PC u ordenador personal es también una plataforma de videojuegos, pero como su función no es solo ejecutar videojuegos, no se considera como videoconsola. El PC no entra en ninguna generación, ya que cada pocos meses salen nuevas piezas que modifican sus prestaciones. El PC por regla general puede ser mucho más potente que cualquier consola del mercado. Los más potentes soportan modos gráficos con resoluciones y efectos de postprocesamiento gráfico muy superiores a cualquier consola.

Las máquinas recreativas de videojuegos están disponibles en lugares públicos de diversión, centros comerciales, restaurantes, bares, o salones recreativos especializados. En los años 1980 y 90 del siglo pasado disfrutaron de un alto grado de popularidad al ser entonces el tipo de plataforma más avanzado técnicamente. Los progresos tecnológicos en las plataformas domésticas han supuesto a comienzos del siglo XXI una cierta decadencia en el uso de las máquinas arcade.

Las videoconsolas portátiles y otros aparatos de bolsillo cuentan con la capacidad para reproducir videojuegos. Entre estos últimos destacan hoy en día los teléfonos móviles (en particular los teléfonos inteligentes) que, sin ser los videojuegos su función primaria, los han ido incorporando a medida que se han ido incrementando sus prestaciones técnicas. Otro dispositivo portátil de creciente popularidad en los últimos años son las tabletas.

Los videojuegos se pueden clasificar en géneros atendiendo a factores como el sistema de juego, el tipo de interactividad con el jugador, sus objetivos, etc. La evolución de los videojuegos desde sus comienzos ha dado lugar a una variedad creciente y cambiante de géneros, muchas veces en relación con lo que los avances en la tecnología han ido haciendo posible. Entre los géneros de videojuegos más populares están los de acción, estrategia, rol, aventura, rompecabezas, simulación, deportes o carreras, cada uno de ellos con varios subgéneros. Por otro lado, hoy en día son habituales los videojuegos que toman elementos de más de un género, lo que ha dado lugar a géneros mixtos (por ejemplo rol-acción, aventura-acción, etc.).

Junto a los géneros, existen otras formas de clasificar o caracterizar los juegos como puede ser por su temática (fantástico-medieval, futurista, de guerra...), su complejidad (juegos AAA, juegos casuales...), su finalidad (educativos, promocionales, artísticos...), tipo de desarrollo, etc.

Por otra parte, también se distingue a unos juegos de otros, incluso dentro de un mismo género, por la perspectiva visual que adoptan (o dicho de otra manera, la posición de la cámara). Así, hay juegos con perspectiva 2D (ya sea con proyección paralela, vista lateral o vista cenital), 2.5D (mediante proyección isométrica, oblicua, o parallax scrolling, entre otras), y 3D (en perspectiva fija, en primera persona, o en tercera persona).

En muchos juegos se puede encontrar la opción de multijugador, es decir, que varias personas puedan participar simultáneamente en la misma partida, ya sea empleando todos la misma máquina (como suele ocurrir con las videoconsolas) o bien usando cada uno su propio dispositivo (el caso habitual en los PC o dispositivos portátiles, en lo que se conoce como videojuegos en línea). Existen juegos en que un jugador humano se enfrenta contra otros jugadores controlados por la máquina, mediante inteligencia artificial, pero en este caso no se considera que sea un videojuego multijugador. Por último, hay videojuegos que están pensados para congregar a un gran número de jugadores de todo el mundo conectados a través de Internet, son los conocidos como videojuegos MMO (de "massive multiplayer online").

La creación de videojuegos es una actividad llevada a cabo por empresas conocidas como desarrolladoras de videojuegos. Estas se encargan de diseñar y programar el videojuego, desde el concepto inicial hasta el videojuego en su versión final. Esta es una actividad multidisciplinaria, que involucra profesionales de la informática, el diseño, el sonido, la actuación, etc. El proceso es similar a la creación de "software" en general, aunque difiere en la gran cantidad de aportes creativos (música, historia, diseño de personajes, niveles, etc.) necesarios. El desarrollo también varía en función de la plataforma objetivo (PC, móviles, consolas), el género (estrategia en tiempo real, RPG, aventura gráfica, plataformas, etc.) y la forma de visualización (2d, 2.5d y 3d). Algunas de las más importantes desarrolladoras de videojuegos a nivel internacional son: Blizzard Entertainment, Valve, Rockstar North, Bungie, Microsoft, Nintendo, BioWare, Sega , Sierra Entertainment o Zynga, a las cuales hay que añadir los estudios internos (a menudo homónimos) de las principales distribuidoras.

La comercialización de los juegos creados por las desarrolladoras es labor de las distribuidoras de videojuegos. Estas se ocupan de su distribución (ya sea a través de tiendas físicas o por internet), publicidad, presentación, traducción... pero también ejercen a menudo un papel fundamental antes y durante el desarrollo del juego, como es su concepción, su financiación, los estudios de mercado, el control de calidad, etc. Muchas distribuidoras tienen uno o varios estudios de desarrollo propio, al margen de que puedan o no trabajar con desarrolladoras externas. Algunas de las más importantes distribuidoras de videojuegos son: Electronic Arts, Ubisoft, Activision Blizzard, Nintendo, Sony Computer Entertainment, Microsoft Studios, Take-Two Interactive, Capcom, Konami, Bethesda Softworks, Square Enix, Bandai Namco, Valve, Rockstar Games, Gameloft y SEGA.

Nacida con la aparición de la primera máquina recreativa a monedas en 1971, la industria del videojuego ha pasado en unas pocas décadas de ser una mera curiosidad tecnológica a convertirse en una de las mayores industrias del entretenimiento por volumen de facturación. Se estima este en 81.500 millones de dólares en 2014 a nivel mundial, llegando a duplicar el de la industria del cine en el mismo año. Los ingresos proceden fundamentalmente de la venta de videojuegos, de videoconsolas, de accesorios y de máquinas recreativas. Los principales países en ingresos por videojuegos son EE. UU., China y Japón, seguidos de Alemania y Reino Unido. España se sitúa en la décima posición, facturando anualmente cerca de 1 500 000 000 de dólares.

La industria del videojuego da trabajo a más de 100 000 personas en todo el mundo, gente de muy diversas disciplinas entre las que se incluye la programación, el diseño, la ingeniería, la interpretación, las finanzas, la mercadotecnia, la música, la comunicación o el comercio. La cadena de valor en la industria del videojuego se puede dividir en 6 partes: los inversores, los desarrolladores de videojuegos, los creadores del "software" empleado por los desarrolladores, los fabricantes de hardware, las distribuidoras de videojuegos, y los consumidores.

Los costes de desarrollo de un videojuego comercial varían enormemente desde los pocos miles de dólares que puede representar un título pequeño, desarrollado por una sola persona, hasta los más de 100 000 000 de dólares de algunos videojuegos AAA, en los que intervienen equipos de hasta un centenar de trabajadores. El videojuego con mayor coste de desarrollo hasta la fecha es "Grand Theft Auto V", de la desarrolladora Rockstar Games, con 167 millones de dólares, seguido por "Destiny", de Bungie, con 154 millones de dólares. Las cifras son aún bastante mayores si se suma la inversión en mercadotecnia.

Las constituyen uno de los principales escaparates donde la industria presenta sus más recientes creaciones cada año. Las más conocidas mundialmente son la E3 en Los Ángeles (EE. UU.), la Gamescom en Colonia (Alemania) y la Tokyo Games Show (Japón). En Francia la feria más importante es la Paris Games Week y en España destacan la Madrid Games Week, GameLab y GameFest. En Chile destaca sobre todo la Festigame, siendo la más importante de Latinoamérica. Los consumidores se informan de las novedades del sector principalmente a través de medios de comunicación especializados. Entre los pertenecientes al ámbito hispano se pueden destacar revistas en papel como Micromanía, New Superjuegos o Hobbyconsolas, y revistas en internet como Meristation, Vandal, Eurogamer, Gamercafe, LagZero o Niubie. Existen también numerosos blogs y canales de Youtube centrados esta temática. Es menor sin embargo su presencia en televisión o en radio.

La venta de videojuegos se ha realizado tradicionalmente en grandes almacenes o en tiendas físicas especializadas; en España las dos principales cadenas de tiendas de videojuegos son Game y, hasta su cierre en 2014, Gamestop. En Chile las más grandes son Zmart, MicroPlay y TodoJuegos. Sin embargo, la tendencia en estos últimos años en todo el mundo es hacia la venta por internet mediante descarga, tanto en PC como en consolas. En dispositivos móviles, la venta por internet —a través de las tiendas de aplicaciones— es de hecho el único canal disponible.

Las principales asociaciones sectoriales en España son AEVI (Asociación Española de Videojuegos), que engloba a compañías que facturan el 90 % de los ingresos totales del sector, y DEV (Desarrollo Español de Videojuegos), que agrupa a las principales compañías desarrolladoras del país. En Chile está el grupo VGChile, donde se agrupan los desarrolladores chilenos.

En un mercado dominado por las grandes distribuidoras de videojuegos, cabe destacar en esta última década el auge de los videojuegos independientes (también conocidos como "indie"), que han llegado a constituir un apartado propio en tiendas y en plataformas de distribución como Steam o Gameloft. Estos juegos son desarrollados por pequeños grupos de no más de 20 personas, sin la ayuda financiera de ninguna distribuidora. Se caracterizan habitualmente por un desarrollo artístico variado y particular, tanto en gráficos como en banda sonora, con historias de tramas innovadoras y que generalmente no tienen una continuidad o no están diseñados para crear una saga. La originalidad de sus planteamientos, alejándose muchas veces de los estereotipos establecidos, le ha valido el interés de una parte de los aficionados.

La ciencia que estudia los videojuegos se llama «ludología» o estudio de los juegos. Uno de los aspectos principales estudiados por esta ciencia y la información que recolecta son los impactos positivos y negativos de los videojuegos en las personas.

La ludología se ocupa del estudio crítico de los juegos, de su diseño, de los jugadores y de la interacción entre ambos, así como su papel en la sociedad y la cultura. Los métodos usados para recolectar información van desde encuestas e investigaciones etnográficas, hasta experimentos controlados de laboratorio.

Una encuesta en línea realizada por la ISFE entre europeos con edades comprendidas entre 16 y 64 años reveló que el 48 % de ellos juega a videojuegos, ya sea de manera habitual (1 o más veces por semana, 25 %) u ocasional (23 %). En España dicho porcentaje se situaría en el 40 %, y en EE. UU. (según la ESA) en el 59 %. Por edades, el 51 % de los videojugadores europeos son menores de 35 años y el 49 % mayores, situándose la edad promedio en 34 años (el estudio realizado en E.E.U.U la sitúa en 31 años). Por sexos, el 55 % son hombres, el 45 % mujeres.

Cabe señalar que la demografía de los videojuegos no ha sido siempre la misma. En el pasado los videojuegos eran un tipo de entretenimiento casi exclusivamente para personas jóvenes, pero esta situación ha ido cambiando con el paso de los años como demuestran diversos estudios recientes. Además de los ya señalados, un estudio de Parra, David et al. (2009) en el que se realizaron 974 encuestas a españoles mayores de 35 años concluyó que "los videojuegos se están implantando con singular intensidad en el conjunto de la población española. Más de la mitad de los españoles mayores de 35 años (53,5 por ciento) juega con videojuegos (bien de manera esporádica o bien de manera habitual)".

Los efectos que pueda tener el uso habitual de videojuegos en las personas, y en especial en los niños, han sido objeto de interés y de controversia.

Entre los efectos positivos que se les atribuyen están capacidades tales como: «coordinación ojos-manos, capacidad lógica, capacidad espacial, resolución de problemas, desarrollo de estrategias, concentración, atención, colaboración, cooperación y selección de información relevante, estimulación auditiva, entre otras». Según un estudio, el niño desarrolla habilidades mentales y su capacidad de razonamiento es más activa en comparación a un niño de hace 20 años que no contaba con esta tecnología. En adultos pueden funcionar como un liberador de estrés, contribuyendo a una buena salud. Otros afirman que mejoran la salud visual e incluso ciertas habilidades como por ejemplo las necesarias para práctica de la cirugía. Hay que señalar también que los efectos varían según el tipo de juego.  Un catedrático de la Universidad de Nottingham también ha afirmado que pueden tener el efecto de atenuar el dolor. Según un estudio, la exposición a corto plazo tiene un efecto positivo en la atención en unos niños sin problemas psiquiátricos, estos obtuvieron una mejor puntuación en la prueba de Stroop después de estar expuestos durante una hora a un videojuego que jugaron por primera vez.

En cuanto a los aspectos negativos de los videojuegos, cabe señalar factores como la adicción. El fácil acceso a ordenadores, "smartphones "y consolas, sumado a una falta de control por parte de los padres o el ambiente de un hogar disfuncional, puede dar lugar a que niños o adolescentes hagan un uso abusivo de los videojuegos. Ello tiene efectos negativos como es el ser más propensos a la agresividad, falta de asertividad y bajo rendimiento académico.

Otro aspecto controvertido de los videojuegos en los niños es que pueden frenar algunos aspectos de su desarrollo motriz, y conducir a una falta de socialización, aunque esto último está rebatido por otros estudios que apuntan a todo lo contrario, a que los videojuegos aumentan su sociabilidad. "Los videojuegos son un entretenimiento que se adecua bien a la realidad del niño nacido en la era de la informática ya que suponen una socialización en la cultura de simulación que caracteriza a las sociedades avanzadas contemporáneas” (Turkle 1997). 

En un estudio se asocia la salud mental y jugar videojuegos, se encontró que los jugadores que jugaban de forma moderada tenían la mejor salud mental, los que jugaban de forma excesiva tenían un leve incremento en comportamientos problemáticos y los que no jugaban videojuegos tuvieron la peor salud mental.

Diversos expertos han señalado el valor de los videojuegos como herramientas para inculcar conocimientos. Gros, B. y sus colaboradores (1997) escriben: «Pensamos que los juegos de ordenador constituyen un material informático de gran valor pedagógico», y enumeran una serie de características:

Los juegos educativos se presentan en los últimos tiempos como una alternativa a los videojuegos violentos. Incluso existe una colección de juegos cuya carátula versa "«la alternativa inteligente a los videojuegos violentos»."

A pesar de las cuestiones positivas, se deben tener en cuenta todos aquellos aspectos negativos, como el uso ilimitado y no vigilado, así como la falta de compromiso, responsabilidad o esfuerzo con actividades que no estén relacionadas con el juego. Por ello, lo ideal es no perder de vista que aunque los videojuegos están en función del entretenimiento, son utilizados como herramientas para posibilitar o potencializar el aprendizaje, lo cual se logrará siempre y cuando exista un buen uso y control por parte de los usuarios o los responsables de estos.

Gee (2004) establece que “la teoría de aprendizaje incorporada a los buenos videojuegos se acerca más a lo que yo creo que son las mejores teorías del aprendizaje planteadas por la ciencia cognitiva”. De esta forma, establece una similitud entre las teorías del aprendizaje que se utilizan diariamente en el ámbito educativo con las teorías incorporadas en los videojuegos, encontrando entre ellas pocos puntos en los que diverjan.

Los videojuegos, como otras formas de expresión audiovisual, han despertado controversia entre personas o colectivos que consideran que tienen o pueden tener efectos perniciosos sobre los jugadores. Entre estos se arguyen por ejemplo los efectos que puede tener en el desarrollo emocional el hecho de pasar demasiado tiempo ante la pantalla e inhibirse por completo en un universo de fantasía. Existen asimismo casos de ludopatía y de ciberadicción. También se argumenta un posible fomento de la violencia y lujuria, gráficamente presentes en muchos videojuegos. Por otro lado, se ha comprobado que la rapidez con que se mueven los gráficos puede provocar ataques en las personas que padecen diversos tipos de epilepsia.

Estudios científicos demuestran que, en general, los videojuegos enriquecen la vida del jugador, le enseñan a resolver problemas técnicos, y estimulan sus habilidades neuro-cinéticas, reflejos visuales y enfoque de múltiples puntos de visión (objetivos). Incluso mejoran la comunicación cuando se juega en familia o en línea.

Diferentes estudios con niños y adolescentes (Castells y Bofarull, 2002; Bringas, Rodríguez y Herrero, 2008) demuestran que el rendimiento escolar puede verse afectado debido al uso de los videojuegos. Ahora bien, existen diferentes artículos que establecen que niveles moderados de juego no se asocian con un bajo rendimiento escolar (Ferguson, 2011); incluso, podrían relacionarse con un mejor rendimiento (Llorca, Bueno, Villar y Díez, 2010). Esto se debe a que los jugadores que utilizan videojuegos adquieren mejores estrategias de conocimiento, estrategias de resolución de problemas, y sus capacidades espaciales, su precisión y capacidad de reacción se ven mejoradas gradualmente (McFarlane, 2002).

En España, la imposibilidad de registrar el videojuego como tal, obedece a que no está reconocido jurídicamente por no estar contemplado en el marco del artículo 145.1 del Real Decreto Legislativo 1/1996, de 12 de abril (en adelante, Ley de Propiedad Intelectual o LPI), como creación intelectual original y unitaria, obliga a sus creadores y titulares a desistir a registrarlo como videojuego, al tener que pasar por separar cada obra de acuerdo a su propia naturaleza, artística, científica o literaria, lo que se evidencia como un obstáculo ante la falta de una regulación específica que permita a la industria proteger, exportar e internacionalizar estos activos intangibles en constante crecimiento socioeconómico, bajo un marco legal estable en el ámbito de la propiedad intelectual de sus creadores.

En el año 2009, la «Proposición No de Ley (PNL)» del Congreso de los Diputados, fue aprobada por unanimidad con dos enmiendas del Partido Popular y CiU: «La Comisión de Cultura del Congreso de los Diputados establece que el videojuego constituye un ámbito fundamental de la creación y la industria cultural de España. En consecuencia, insta al Gobierno a reconocer a sus creadores y emprendedores como protagonistas de nuestra cultura. Asimismo, en el marco de sus competencias y en coordinación con las administraciones autonómicas competentes, se insta al Gobierno a facilitar su acceso a todas las ayudas factibles para la promoción de su actividad, la financiación como industria cultural y la internacionalización de sus iniciativas».

Lo más relevante de la PNL, es que se acuerda por unanimidad que «no debe ser válido contemplarlos como obra audiovisual y, por otro, tampoco se les debe asimilar simplemente a creadores de "software"».




</doc>
<doc id="2934" url="https://es.wikipedia.org/wiki?curid=2934" title="Vallenato">
Vallenato

El vallenato es un género musical autóctono de la Región Caribe de Colombia con su origen en la antigua provincia de Padilla (actuales sur de La Guajira, norte del Cesar y oriente del Magdalena). Tiene notable influencia de la inmigración europea, ya que el acordeón fue traído por pobladores alemanes a Riohacha, La Guajira, a finales del siglo XIX, y tanto la organización estrófica como la métrica se valen de la tradición española; por otra parte, el componente de los esclavos afrocolombianos hace presencia con la caja vallenata, una especie de tambor que en gran medida le da el ritmo a la melodía del acordeón, y por último lo indígena se evidencia con la guacharaca. Su popularidad se ha extendido hoy a todas las regiones de Colombia, a países vecinos como Ecuador, Panamá, Venezuela e incluso países de Europa.
Se interpreta tradicionalmente con tres instrumentos: el acordeón diatónico, la guacharaca y la caja vallenata. Los ritmos o aires musicales del vallenato son el paseo, el merengue, la puya, el son y la tambora. El vallenato también se interpreta con guitarra y con la instrumentación de la cumbia en cumbiambas y grupos de millo. 

El 29 de noviembre de 2013, el vallenato tradicional fue declarado Patrimonio Cultural Inmaterial de la Nación por el Consejo Nacional de Patrimonio del Ministerio de Cultura. El 1 de diciembre de 2015 fue incluido en la lista de Patrimonio Cultural Inmaterial de la Humanidad, en la lista de salvaguardia urgente por la Unesco.

No se sabe con exactitud de dónde proviene la palabra "vallenato", a pesar de las muchas hipótesis que han sido expuestas. Sin embargo, a principios del siglo XX, tenía una connotación despectiva y a los propios habitantes de Valledupar no les gustaba. Por tal motivo, en 1915 don Miguel Vence, educador de primaria, fundó una Academia de la Lengua de Valledupar, la cual sesionó una sola vez y determinó que el gentilicio de los nacidos en Valledupar fuera "valduparense".

Generalmente se define al vallenato como un género musical de la Costa Caribe colombiana, más precisamente del área de influencia de Valledupar, capital del departamento de Cesar. Se sostiene que el nombre proviene del gentilicio anterior de los nacidos en la ciudad donde tiene mayor arraigo este género. Según algunos, se trata de un neologismo que nació con los nativos viajeros en mulas, que cuando se les preguntaba en otras tierras de dónde eran, en su decir campesino respondían "Soy nato del Valle", que es como decir "Soy del Valle nato".

No obstante que el término "vallenato" puede referirse a los nacidos o a las cosas que se originan en Valledupar (Valle De Upar, El Valle Del Cacique Upar, cacique indígena legendario de la región), existen otras versiones de la denominación: según Barrameda Morán, el vocablo "ballenato" pasó a designar a todas las personas que padecieran la contaminación sanguínea producida por el jején, fueran oriundos o no de Valledupar y dice: "La tendencia popular a confundir V con B en su pronunciación, terminó por generar el nuevo vocablo: Vallenato".

De manera similar, otra versión con poco fundamento sostiene que en las áreas rurales de los bancos del río Cesar, muchos de los habitantes extremadamente pobres sufrieron de una enfermedad producida por un mosquito que les dejó la piel seca y escamosa, con parches descoloridos. La gente asoció la enfermedad con las ballenas recién nacidas (ballenatos), también llamadas "pintaos", que tienen un color manchado de blanco y rosado, parecido a la enfermedad dérmica llamada "carate" o "jovero", por lo cual se identificaba a quienes la padecían como "caratejos" o "ballenatos". De tal forma que "vallenatos" llegó a ser un nombre para menospreciar a la gente pobre del río.

Las melodías de estos cantos se interpretaron primero con la flauta de caña de millo o carrizo, abierta en sus dos extremos con cuatro orificios en su longitud y una lengüeta que forma la embocadura y pisa un hilo, sostenido por los dientes, para modular el sonido; a ella se sumaron la caja, tambor pequeño hecho artesanalmente del tronco hueco de los árboles secos y sellado en uno de sus extremos con un pedazo de cuero templado, y la guacharaca, instrumento ancestral indígena que se fabrica utilizando un pedazo de cañabrava a la que se le hacen pequeñas ranuras sucesivas para producir un sonido raspativo al ser frotadas con un hueso (originalmente).

A finales del siglo XIX, décadas después de su invención, el acordeón llegó a Colombia por el puerto de Riohacha; los vaqueros y campesinos lo incorporaron a sus expresiones musicales, y paulatinamente fue sustituyendo al carrizo hasta convertirse en el instrumento principal del conjunto típico de música vallenata.

Además de estos tres instrumentos, caja, guacharaca y acordeón, que representan la trietnia que dio origen a la raza y cultura de la Costa Caribe colombiana, el conjunto típico vallenato presenta un cuarto elemento básico que es el cantante, de más o menos reciente incorporación a raíz de los festivales vallenatos, ya que hasta los años 1960 la costumbre era que el acordeonero llevaba la voz cantante e interpretara él mismo la letra de las canciones que tocaba.

Desde los años 90´s, cuando el "vallenato romántico" empezó a ser popular en Colombia, la guitarra acústica se convirtió, extraoficialmente, en un cuarto instrumento fundamental de la música vallenata, pese a no ser utilizada en los festivales vallenatos. El bajo eléctrico se ha venido convirtiendo en parte importante, aunque no fundamental, para interpretar música vallenata en parrandas y conciertos.

El vallenato o la música vallenata hace parte de la música folclórica de la Costa Caribe colombiana. Es el ritmo musical colombiano que ha alcanzado más popularidad, tanto a nivel nacional como internacional.

Lo que hace característico al vallenato tradicional es ser interpretado solo con tres instrumentos que no requieren de amplificación alguna: dos de percusión (la caja y la guacharaca), que marcan el ritmo, y el acordeón diatónico (de origen europeo) con el que se interpreta la melodía. No obstante, en algunas ocasiones las canciones se componen o interpretan con otros instrumentos: la guitarra, la flauta, la gaita, el acordeón cromático y la armónica. Por otra parte, para el vallenato comercial es común no solo la incorporación de estos instrumentos, sino también del bajo eléctrico y otros de percusión, como las congas y los timbales.

La importancia que adquirió el vallenato en las últimas décadas del siglo XX llevó a la organización de festivales en los que los acordeoneros compiten por el honor de ser declarado el más hábil ejecutor de cada uno de los aires tradicionales (a excepción, inexplicablemente, de la tambora). El más célebre de estos festivales es el Festival de la Leyenda Vallenata, que se celebra anualmente a fines de abril en Valledupar, y cuya primera versión se disputó en 1968. Desde 1979 se disputa el Festival Cuna de Acordeones de Villanueva, Guajira, pasando a ser el segundo de mayor importancia y, desde 1983, el Festival de Río Grande de la Magdalena en Barrancabermeja (Santander), que sería el tercero en importancia a nivel nacional para los intérpretes del acordeón.

En el vallenato, el modo de uso del acordeón diatónico requiere usar simultáneamente ambos lados del acordeón. Lo anterior caracteriza al acordeonero colombiano y diferencia al vallenato de los otros géneros musicales con acordeón, donde generalmente se suprime o subutiliza la parte de los bajos (ejecutados con la mano izquierda): en Colombia, la forma armónica y rítmica con que el acordeonero maneja los bajos es un factor relevante de calificación en los festivales vallenatos.

El vallenato nace en una vasta región enmarcada por los ríos Magdalena, Cesar y Ranchería, el mar Caribe, la Sierra Nevada de Santa Marta y las estribaciones de la serranía del Perijá, en la primera mitad del Siglo XIX.

Los cantos de vaquería con que los peones de las grandes haciendas acompañaban sus jornadas vespertinas para recoger y encerrar el ganado, fueron la base de lo que más tarde se convertiría en las historias cantadas que derivaron en las canciones vallenatas.

Los primeros acordeoneros de que se tiene memoria fueron a la vez autores de los cantos que interpretaban; cantos que ya tenían una clara diferencia rítmica y una estructura musical propia que les valieron ser clasificados como "paseos", "merengues", "puyas", "tamboras" y "sones". Entonces no había, como hoy, una persona especializada únicamente en componer el canto, otra en ejecutar la melodía en el acordeón y una tercera que los cantara. El acordeonero era un músico integral que con igual destreza hacía sonar el acordeón como interpretaba cantos de su propia inspiración o, en ocasiones, de un tercero. Y hechos los primeros cantos, los acordeoneros se convirtieron en correos cantados, en periodistas musicales, juglares, que iban de pueblo en pueblo y de vereda en vereda llevando la información de los últimos sucesos narrados en los merengues, paseos, puyas, sones y tamboras que cantaban cuando se reunían a descansar y, en ocasiones especiales, a bailar en cumbiambas que se formaban con motivo de las fiestas patronales, entre otras ocasiones.

En relación con los cantares de vaquería como uno de los orígenes del vallenato, el investigador cultural y musical Ciro Quiroz anota sobre la cumbia:
En cuanto al sitio de origen del vallenato, Quiroz anota:

Sobre la transición de pitos y flautas a los instrumentos actuales del vallenato, el mismo autor dice sobre la primitiva denominación de los aires:

A diferencia de todos los demás aires de este folclor, el paseo vallenato tiene una cuadratura de compás de cuatro tiempos. La marcación de los bajos es de uno por tres y a veces, de acuerdo con la pieza, de dos por uno.

Musicalmente hablando, el merengue vallenato tradicional tiene una cuadratura de compás de seis por ocho, un compás derivado, ya que los compases originales son el de cuatro tiempos, el de tres y el de dos.

En Valledupar y demás pueblos del antiguo departamento del Magdalena Grande, el ritmo más antiguo era llamado "puya". Su nombre deriva del verbo puyar, y tiene un compás de seis por ocho. El ritmo, en su forma indígena, nunca tuvo canto y consistía en la imitación hecha por el carricero –pitero o caña sillero-, en ritmo rápido, del canto de algunos pájaros; se bailaba en hileras, llevando cada persona las dos manos cerradas a la altura del pecho con los dedos apuntando hacia delante y simulando que se puyaba repetidamente a quien danzaba adelante. Posteriormente, a través del tiempo, se fueron fusionando los distintos elementos triétnicos típicos de la cultura costeña y ribereña colombiana, logrando sumarse la puya negroide, género cantado, a la puya indígena, dándose como resultado la puya vallenata con su actual equilibrio entre el canto, la melodía y el ritmo.

La puya y el merengue en su patrón rítmico y armónico son iguales. La diferencia está marcada en su concepción melódica: en el ritmo, en la música y naturalmente en la interpretación que se haga, propia de cada pieza. Así, la puya tiene una marcación en los bajos de dos por dos y, a veces, de dos por uno en ciertos pasajes de la interpretación, aunque no en todas las piezas. La velocidad que se le imprima no supone una diferencia, porque el intérprete la toca a su gusto.

La puya se destaca por ser el aire más rápido, y el que exige más habilidad en el intérprete del acordeón. Se utiliza más comúnmente en las contiendas y competencias de acordeonistas en los festivales vallenatos de Colombia.

El son vallenato tiene una cuadratura de compás de dos por cuatro. Una característica esencial en la ejecución de este aire es la prominente utilización de los bajos del acordeón en la interpretación de cada pieza, tanto que los bajos pueden ser más notorios que la misma melodía emitida por el teclado, principalmente en los acordeoneros de las nuevas generaciones.

El son tiene una marcación en los bajos de uno por uno muy marcada, sobre todo en intérpretes sabaneros o de influencia bajera – viejo Bolívar -; a diferencia de los acordeoneros de la provincia, quienes interpretan el son más fluido, menos marcado, más sutil y le dan una marcación de bajo de uno por dos y de dos por uno, en ocasiones.

Como el paseo, los sones son una especie de crónica en donde la singular narrativa del cantor deja plasmados los acontecimientos de su existencia, particularmente en esta especie se representan dramas nostálgicos que han constituido parte importante en la vida del autor.

La tambora es un ritmo que tiende a desaparecer. Tomó denominación femenina debido al predominio de voces de mujeres cuando estos aires eran solo cantados.

Unas son politemáticas, en las que cada verso expresa una circunstancia diferente a la del otro, pero existe uno que es constante. Algunas tienen la particularidad de intercalar el inmodificable verso fijo cada dos versos, y otras mantienen la unidad de escritura de un tema, pero sin tener en cuenta concordancia y armonía en las frases poéticas.

En general, todas tienen condición satírica, lograda en la descoordinación que resalta más el contraste. Todavía existen algunas puramente instrumentales, interpretadas únicamente con tambores. De ahí su designación.

Ejemplos de tamboras: "La candela viva" (de Alejandro Durán), "Mi compadre se cayó", "La perra".

La tambora tradicional es de conformación triétnica (negro, blanco, indio) y que su entorno geográfico está centrado a orillas del río Grande de la Magdalena en la sub-región denominada Depresión Momposina. Los pueblos del departamento del Cesar que han tenido la tambora como identidad cultural son, entre otros: Tamalameque, La Gloria, Gamarra, Chimichagua, Chiriguaná, El Paso.

Antecedido de una grandes polémica en el mundo vallenato, un quinto aire para concurso fue institucionalizado en Villanueva durante la versión 29 del Festival Cuna de Acordeones de 2007. El llamado "quinto aire" fue bautizado como "Romanza Vallenata", en este mismo festival en el año 2006, y fue aceptado como tal con el respaldo de autoridades del vallenato como Rafael Escalona, Francisco Zumaqué, Hernán Urbina Joiro, Rosendo Romero y el expresidente Alfonso López Michelsen.

De esta manera se aceptó que el llamado "paseo" que comercialmente se escucha hoy, lo dejó de ser hace algún tiempo. Así como en su momento del "son" surgió el "paseo", hoy surge un nuevo aire de este. Las romanzas vallenatas, por su carácter lírico o poético, son un canto al amor, al desamor, al perdón y a la mujer, distinto del paseo clásico que se interpreta en los festivales, por eso se decidió darle un espacio en ellos. Además, se tuvo en cuenta que este aire ha sido motor trascendental para la internacionalización del vallenato. Este aire, hijo del paseo, adquirió independencia gracias a su aceptación mundial y después de voces en contrario que no admiten la evolución del género musical. Igualmente no se determina esta denominación como un ritmo oficial vallenato.

La piqueria (de "pique", enfrentamiento) es una competencia usualmente entre dos verseadores improvisadores y repentistas, en la que gana quien produzca mejores versos y se equivoque menos, a juicio de un jurado. Existen las modalidades de versos de cuatro palabras, décima de tema libre y pie forzado. Al momento de elegir al ganador se tienen en cuenta factores como la capacidad para improvisar con agilidad, gracia y exactitud métrica y rítmica versos de cuatro palabras (cuartetas) o de diez (décimas) para desafiar o responder el requerimiento musical de un contrincante en iguales condiciones. A juicio del jurado, el pique puede tener como punto de partida un solo verso de cuatro palabras con un tema determinado, una décima de tema libre o un pie forzado. El jurado puede imponer cualquiera de estas tres modalidades o imponerlas todas si así lo considera.

La vallenatóloga Consuelo Araújo identificó tres escuelas en el vallenato:

Esta clasificación no es aceptada generalmente y ha sido criticada por músicos y juglares. Los músicos sabaneros en particular hablan de música de acordeones o música sabanera de acordeón, la cual fue desarrollada en paralelo a la escuela vallenata, de ejecución distinta, no comparte completamente su organología, y no incluye la puya como ritmo, pero adiciona otros como cumbia, paseaíto, porro y chandé.

De carácter eminentemente folclórico, es ejecutado en los festivales como el de la Leyenda Vallenata, el Cuna de Acordeones y el Festival del Río Grande de la Magdalena. Comprende cuatro (4) de los cinco (5) ritmos tradicionales: la puya, el paseo, el son y el merengue. Su temática abarca hechos de la vida cotidiana, la amistad, la parranda, la tierra y el amor. Es la música que cultivaron juglares como Juancho Polo Valencia, Alejandro Durán, Abel Antonio Villa, Luis Enrique Martínez, "Toño" Salas, Lorenzo Morales, Leandro Díaz, "Pacho" Rada, "Colacho" Mendoza, Rafael Escalona, Emiliano Zuleta, entre otros. También entrarían en este selecto grupo acordeoneros que fueron parte de la agrupación "Los Corraleros de Majagual" como Calixto Ochoa, Lisandro Meza y Alfredo Gutierrez, los dos últimos aún en actividad.

Es la primera corriente vallenata que se hace comercial en las emisoras del país, siendo conocida como "vallenato yuca". Se empezó a escuchar a principios de los años 70´s, extendiéndose su popularidad en los 80´s. Sus principales representantes son: Otto Serge y Rafael Ricardo, Jorge Oñate, Los Hermanos Zuleta, Diomedes Díaz, Binomio de Oro (con Rafael Orozco), Los Betos, Farid Ortiz, Iván Villazón, Daniel Celedón e Ismael Rudas, entre otros. Predomina el paseo y, sin tanta importancia, el merengue y la puya.

Estilo influido por otros ritmos como la balada, impulsado por Iván Calderón a finales de los años 80´s y comienzos de los 90´s, se basa principalmente en el paseo y, en la denominada décadas después como "romanza vallenata". Su principal característica radica en la letra, donde exclusivamente se le canta al amor. Sus temas incluyen amores, despechos, distanciamientos y reconciliaciones. Es uno de los subgéneros más escuchados de Colombia y el más escuchado en el extranjero (Monterrey y Saltillo en México, Iquitos en Perú, Venezuela, Panamá, Ecuador, Paraguay, Noreste de Argentina y las colonias colombianas y latinoamericanas de Estados Unidos y Europa). Algunos de sus representantes más importantes son: Binomio de Oro de América, Estrellas Vallenatas, Los Pechichones, Dúo Sensacional, Patricia Teherán, Las Musas, Adriana Lucía (en su época de intérprete vallenata), Los Diablitos, Los Gigantes, Los Inquietos, Los Chiches, Los Embajadores, Miguel Morales, Jesús Manuel y Álex Manga (los tres ex-Diablitos), Fabián Corrales, Luis Mateus, Nelson Velásquez (ex-Inquietos), Jean Carlo Centeno, Junior Santiago y Jorge Celedón (los tres ex-Binomio de Oro de América), Amín Martínez (ex-Chiches), Luis Miguel Fuentes y Heberth Vargas (ambos ex-Gigantes), entre otros.

Una corriente vallenata que empezó a ser aceptada por el público colombiano desde comienzos de los años 2000, impulsada por Kaleth Morales, hijo del cantautor vallenato Miguel Morales, el cual combina elementos y arreglos de corte carnavalesco y electrónico con instrumentos de percusión y de viento de otras zonas de la región Caribe (bombardino, redoblante, tamboras, tuba, etc.). Sus artistas más destacados son: Silvestre Dangond, Peter Manjarrés, Martín Elías (hijo de Diomedes Díaz), Luifer Cuello, Penchy Castro, Kvrass, Mono Zabaleta, Churo Díaz, Elder Díaz (hijo de Diomedes Díaz), Daniel Calderón, Felipe Peláez, La Gente de Omar Geles, Los K Morales (sucesores del legado de Kaleth Morales), Orlando Liñan, Kbto Zuleta, Cayito Dangond (hermano de Silvestre Dangond), Los Comandantes del Vallenato, entre otros.

En la época de los juglares, la música vallenata ya venía manejando su "Nueva Ola": Alejo Durán compuso e interpretó en 1960 una canción en aire de paseo llamada "La Ola del Vallenato", como crítica a artistas de la época que hacían innovaciones en el género como Aníbal 'Sensación' Velásquez, quien grabó e impuso un nuevo ritmo al vallenato llamándolo "Guaracha".

Los verdaderos juglares vallenatos se han perdido entre la historia y la leyenda. Entre ellos se encuentran desde la figura legendaria de Francisco el Hombre, pasando por Pedro Nolasco Martínez, Emiliano Zuleta, Guillermo Buitrago, Lorenzo Morales, Leandro Díaz, Luis Enrique Martínez, Tobías Enrique Pumarejo, Juancho Polo Valencia, Diomedes Díaz, Abel Antonio Villa, Rafael Escalona, y el que ha sido el más grande icono del folclor vallenato, el primer Rey Vallenato, Alejandro Durán. Muchos de ellos murieron en la pobreza a pesar de que sus cantos se escuchaban en toda América Latina y de que dieron fisonomía al vallenato mucho antes de que se convirtiera en un fenómeno de ventas.

A pesar de existir compositores e intérpretes del vallenato tradicional de gran popularidad en Colombia, el máximo "embajador" de esta música en el mundo es el cantante samario Carlos Vives, que lo ha dado a conocer a través de una variante que se podría denominar vallenato-pop-rock, también conocido como vallenato alternativo. Hoy por hoy, se hace una diferenciación entre el vallenato tradicional y un vallenato más comercial, en el que se han destacado cantantes como Silvestre Dangond, Kaleth Morales, Jorge Celedón e Iván Villazón y agrupaciones como el El Binomio de Oro de América. Otros intérpretes como Diomedes Díaz lograron que el vallenato ganara popularidad entre los colombianos, sin distinción social ni cultural. En Venezuela, Rafael Orozco es considerado un ídolo musical, aun muchos años después de su muerte; tal fue el cariño expresado por el público venezolano que uno de sus mayores éxitos está dedicado a este país: "Recorriendo a Venezuela".

El festival de música vallenata más importante de Colombia es el Festival de la Leyenda Vallenata, que se celebra desde 1968 en Valledupar, Cesar. En él se premia al mejor ejecutante del acordeón con el título de Rey Vallenato. El ganador del primer festival fue Alejandro Durán, quien derrotó en la tarima "Francisco el Hombre" a Emiliano Zuleta. Cabe resaltar que desde 1987 se realiza, cada 10 años, el torneo "Rey de Reyes", donde participan únicamente quienes han sido coronados como reyes vallenatos en el festival.

El segundo festival en importancia para la música vallenata es el Festival Cuna de Acordeones de Villanueva, Guajira, población fuente de intérpretes del acordeón, que se realiza desde 1979. El Festival Cuna de Acordeones fue nombrado Patrimonio Cultural y Artístico de Colombia por el Congreso Nacional mediante la Ley 1052 de 2006. Al igual que el festival de Valledupar, el Festival Cuna de Acordeones corona desde 2003 y cada 10 años al "Rey de Reyes". 

El tercero en importancia es el Festival del Río Grande de la Magdalena, que se realiza en el municipio santandereano de Barrancabermeja desde el año 1983, el cual también fue declarado Patrimonio Cultural de la Nación mediante la Ley 1007 de 2006 por el Congreso de la República. Igual que en Valledupar y Villanueva, este festival corona su "Rey de Reyes" desde 1992 pero cada 11 años.

En Riohacha se realiza desde el año 2009 el Festival Francisco El Hombre. A diferencia de los anteriores festivales, este premia a los mejores grupos y cantantes vallenatos.

Fuera de Colombia, se organizaron festivales vallenatos como el Festival Internacional Vallenato de Monterrey (México) que se realizó entre 2007 y 2017, logrando coronar diez reyes vallenatos del país azteca, teniendo invitados especiales a intérpretes y compositores de Colombia como Sergio Moya Molina, Adolfo Pacheco, Fernando Meneses Romero, Isaac Carrillo "Tijito", Los Hermanos Lora, Roy Rodríguez, Alberto Rada, El Dúo Sensacional, Jorge Luis Ortiz, entre otros. En el año 2016, este festival coronó su primer y único "Rey de Reyes". Este festival fue reemplazado desde 2018 por el Festival Vallenato de México, realizado en la ciudad regiomontana. Otro festival importante que se realiza en esta misma ciudad es el Festival Voz de Acordeones, de mayor tradición (se realiza desde 1998) e importancia porque el ganador del festival viaja a Valledupar, representando a México en el Festival de la Leyenda Vallenata del año siguiente.




</doc>
<doc id="2935" url="https://es.wikipedia.org/wiki?curid=2935" title="Vertebrata">
Vertebrata

Los vertebrados (Vertebrata) son un subfilo muy diverso de cordados que comprende a los animales con espina dorsal o columna vertebral, compuesta de vértebras. Incluye unas 72.327 especies actuales y muchos fósiles.

Los vertebrados han logrado adaptarse a diferentes ambientes, incluidos los más difíciles e inhóspitos. Aunque proceden inicialmente del medio dulceacuícola, una gran diversidad de formas evolucionó en el mar y más tarde, en el medio terrestre.

El término «Vertebrata», usado en sentido amplio, es sinónimo de "Craniata", e incluye a los mixinos, que no poseen auténticas vértebras, y a las lampreas, que poseen esbozos de vértebras denominados arcualia, junto a los gnatostomados que por lo general poseen vértebras. Según los estudios genéticos, las lampreas y mixinos forman un grupo llamado Cyclostomata dentro de Vertebrata. Evidencias fósiles recientes también apoyan la inclusión de los mixinos en los vertebrados, esto sugiere que los mixinos son descendientes de vertebrados sin mandíbula que durante su evolución perdieron las vértebras. Esta agrupación exige excluir a las lampreas del clado Cephalaspidomorphi el cual incluye a los peces sin mandíbulas más emparentados con los gnatostomados.

Los vertebrados tienen simetría bilateral y están provistos de un cráneo que protege el cerebro, y esqueleto cartilaginoso u óseo, que comprende una parte axial metamerizada (columna vertebral). Según los autores, se conocen entre 50 000 y casi 62 000 especies actuales.

Los vertebrados típicos tienen el cuerpo dividido en tres regiones: cabeza, tronco y cola; en los mamíferos, el tronco está a su vez subdividido en tórax y abdomen. En las formas acuáticas, existen aletas en posición media, generalmente diferenciadas en aleta dorsal, caudal y anal); en los vertebrados con quijadas, del tronco sobresalen las extremidades pares. Presentan notocordio en la fase de embrión, que es sustituido por la columna vertebral en estado adulto; la cabeza está bien diferenciada, y en ella se agrupan y centralizan la mayoría de órganos sensoriales y nerviosos. La estructura esqueletaria de los vertebrados fosiliza con facilidad. lo cual ha sido fundamental para conocer su evolución.

Durante el desarrollo embrionario, las paredes del cuerpo de los vertebrados desarrollan unos orificios o hendiduras faríngeas, que dan lugar a las branquias (en los peces) y a diferentes estructuras. El esqueleto puede ser óseo, cartilaginoso, y en ocasiones presentar dermoesqueleto, consistente en unas formaciones óseas de origen cutáneo.

El tegumento adquiere notable importancia en los vertebrados por los múltiples papeles que desempeña, y puede presentar variadas diferenciaciones córneas. En el tegumento se distinguen formaciones de estructuras protectoras y sensoriales, glándulas con funciones excretoras, aislamiento del medio, etc. Consta de tres capas: epidermis, dermis e hipodermis. Por su parte, la coloración del tegumento es debida sobre todo a los cromatóforos o células pigmentarias ramificadas de la piel.

La piel origina dos formaciones importantes, epidérmicas y dérmicas:


El aparato locomotor de los vertebrados se ha adaptado de su función inicial (la natación), a otras acciones múltiples que permiten movimientos complejos según las condiciones registradas por los órganos sensitivos.

Los peces, habitantes del medio primigenio, sufrieron cambios evolutivos importantes a partir de la aparición de las aletas pares, que posteriormente se convirtieron en quiridios o extremidades locomotoras pentadáctilas (de cinco dedos) cuando comenzaron la conquista del medio terrestre, y que sufrirían posteriormente adaptaciones específicas, tales como las manos prensoras de los primates, las manos desgarradoras de los felinos, o las alas de sustentación aérea de las aves.

En los vertebrados el aparato circulatorio es cerrado, mediante el cual se transporta oxígeno y nutrientes a los distintos tejidos y células (presentan glóbulos rojos que transportan el oxígeno mediante la hemoglobina). Consta de un sistema sanguíneo y sistema linfático. Está dotado de un corazón dividido en cámaras, arterias, arteriolas, venas, vénulas y capilares. En los peces hay un circuito sistémico y otro branquial. En muchos vertebrados terrestres el sistema sanguíneo es doble (circulación mayor o general, y circulación menor o pulmonar), es decir no se mezclan la sangre arterial y venosa. El corazón de los peces presenta dos cámaras, una aurícula y un ventrículo (dos aurículas y un ventrículo en los anfibios y reptiles). En las aves y mamíferos es tetracameral (dos aurículas y dos ventrículos), y con una serie de válvulas cardíacas. En los vertebrados existe además un sistema linfático, encargado de recoger el líquido intersticial.

El aparato respiratorio de los vertebrados es branquial en los animales acuáticos (ciclóstomos, peces y larvas de anfibios), y pulmonar en los terrestres, parte de los acuáticos y también los anfibios que tienen dos tipos de respiración: la pulmonar y a través de la piel.

Las branquias son un órgano o apéndice filiforme (en forma de laminillas vascularizadas), externa o interna según se disponga en el cuerpo. Tienen una función respiratoria, y están especializadas para el intercambio gaseoso en el medio acuático. Todas las branquias presentan en común una amplia superficie de contacto con el medio, y en ellas la irrigación sanguínea se encuentra mucho más desarrollada que en otras partes del cuerpo.

En las aves, el aparato respiratorio es sumamente eficaz; proporciona el oxígeno necesario para generar la energía que el cuerpo demanda por el esfuerzo desarrollado durante el vuelo. Consta de un sistema de bronquios que están conectados a unos sacos aéreos; los pulmones están divididos en alvéolos y lobulillos.

El sistema nervioso de los vertebrados comprende el sistema nervioso central, que a su vez consta de encéfalo y médula espinal; y el sistema nervioso periférico, que consta de numerosos ganglios y nervios (raquídeos o espinales); existe además un sistema nervioso autónomo que inerva las vísceras (sistema simpático y parasimpático). Los órganos sensitivos, así como las funciones motoras, son muy perfeccionados y desarrollados. Los nervios raquídeos se ramifican a diferentes niveles de la médula, e inervan los distintos músculos, glándulas y órganos. En el caso de los tetrápodos, aparecen dos engrosamientos en la médula, las intumescencias cervicales y lumbar, como consecuencia del desarrollo de las patas.

Los sentidos incluyen: ojos, dispuestos en cámara de visión lateral, salvo en algunas aves y mamíferos primates, que es binocular; tangorreceptores, que incluyen los órganos táctiles de los mamíferos y la línea lateral (captadoras de ondas de presión) de los ciclóstomos, peces y algunos anfibios acuáticos; órganos auditivos, en los tetrápodos consta de oído interno y oído medio, ventanas oval y redonda, membrana timpánica y huesecillos, los cuales transmite la vibración del tímpano a la cóclea o caracol. El oído medio comunica con la faringe a través de la trompa de Eustaquio; los mamíferos disponen además de un oído externo. En los peces solo hay oído interno.

El sistema endocrino de los vertebrados está muy perfeccionado; mediante las hormonas regula múltiples funciones del organismo. Está controlado por el hipotálamo y la hipófisis, que mediante la elaboración de mensajes bioquímicos ejercen su acción sobre las gónadas, páncreas, glándulas suprarrenales, etc.

El aparato digestivo de los vertebrados evolucionó a partir de las primeras formas que se alimentaban mediante sistemas filtradores, hasta los vertebrados macrofágicos, que supuso una serie de adaptaciones de los diferentes elementos intervinientes: dentales, masticadores, musculares, e incluso de las propias cavidades internas, tales como los componentes enzimáticos necesarios para realizar la digestión.

El aparato digestivo de los vertebrados consiste en una cavidad oral, faringe, esófago, estómago, intestino y ano. Estos órganos están asociados a otras formaciones glandulares anexas, tales como las salivales, hígado y páncreas. En los tetrápodos, la cavidad bucal es de complejidad creciente; en ella se desarrollan un conjunto de estructuras auxiliares, tales como labios, lengua, paladar y dientes.

El estómago está típicamente dividido en tres regiones; en el caso de los rumiantes (por su adaptación a dietas herbívoras) presentan un estómago de cuatro cavidades. En las aves se distingue un proventrículo y una molleja trituradora; y en el esófago un divertículo o buche.

El intestino está compuesto de una porción estrecha (el intestino delgado), y otras más corta y ancha (el intestino grueso). En el primero se vierten la bilis del hígado y el jugo pancreático, que realizan una función proteolítica (hidrólisis de las proteínas), y se absorben los nutrientes a través de las microvellosidades. En el intestino grueso se absorbe el agua y se forman los desechos o heces.

Inicialmente, los vertebrados primitivos se alimentaban mediante sistemas de filtración, los cuales pronto fueron reemplazados por otros más evolucionados. El resultado fue una reducción del tamaño de la faringe y del número de hendiduras branquiales. Excepto en los agnatos, que son los vertebrados más primitivos, los dos primeros arcos branquiales del resto de vertebrados evolucionaron hasta transformarse en las mandíbulas, que se han especializado en la "captura" del alimento. Su aparato digestivo es completo.

El aparato excretor de los vertebrados está formado por el aparato renal y las glándulas sudoríparas. Está muy perfeccionado en comparación con los cordados inferiores. Mediante estructuras especializadas se consigue filtrar los líquidos internos al margen del medio externo, a la vez que mantiene en equilibrio el nivel de todos ellos dentro del cuerpo.

La reproducción de los vertebrados es sexual salvo excepciones (ejemplo de algunos peces con casos de hermafroditismo), esto ocurre habitualmente mediante sexos separados, con fecundación interna o externa, y tanto vivíparos como ovíparos. Los mamíferos presentan la mayor complejidad, en los cuales el embrión se desarrolla en el interior de la madre recibiendo el alimento a través de la placenta en los mamíferos placentarios y del marsupio en los mamíferos marsupiales. Después de nacidas las crías la administración del alimento se efectúa mediante la leche segregada por las glándulas mamarias.

Los vertebrados se originaron durante la explosión cámbrica, a principios del Paleozoico, junto con otros muchos grupos de animales. El vertebrado más antiguo que se conoce es "Haikouichthys", con una antigüedad de 525 millones de años. Se asemejaban a los mixinos actuales, ya que carecían de mandíbulas (agnato), y tanto su cráneo como su esqueleto eran cartilaginosos. Otro vertebrado ancestral es "Myllokunmingia". Ambos proceden de Chengjiang (China).

Los primeros peces con mandíbulas (gnatóstomos) aparecieron en el Ordovícico, y se hicieron abundantes durante el Devónico, que por ello se denomina a veces la "edad de los peces"; durante este periodo desaparecieron muchos de los agnatos ancestrales y aparecieron los laberintodontos, formas transicionales entre peces y anfibios.

Los primeros reptiles hicieron su aparición en el siguiente periodo, el Carbonífero. Los reptiles anápsidos y sinápsidos abundaron durante el Pérmico, en el tramo final del Paleozoico, mientras que los diápsidos fueron los vertebrados dominantes durante el Mesozoico. Los dinosaurios originaron a las aves en el Jurásico. La extinción de los dinosaurios al final del Cretácico propició la expansión de los mamíferos, que se habían originado hacía ya mucho tiempo a partir de reptiles sinápsidos, pero que habían permanecido en un segundo plano durante el Mesozoico.

El número de especies de vertebrados descritas se divide en tetrápodos y peces. La siguiente tabla enumera el número de especies existentes descritas para cada clase de vertebrados según la UICN.

Los vertebrados se han venido clasificando durante décadas en diez clases vivientes, agrupadas de la siguiente manera:

Subfilo Vertebrata

Intensos estudios basados en métodos cladísticos, sobre todo a partir de la década de los 80, han producido una revolución en la clasificación de los vertebrados. El debate sigue abierto y las clasificaciones que sigan no deben considerarse definitivas. El árbol ha ido cambiando un poco desde las primeras clasificaciones en 1980. A continuación se muestra la filogenia de los vertebrados existentes según estudios genéticos recientes:

Nótese que diversos taxones de la clasificación linneana tradicional son parafiléticos: Agnatha (Cyclostomata + Cephalaspidomorphi), Osteichthyes (Actinopterygii + Sarcopterygii), Reptilia (Testudines + Lepidosauria + Crocodilia), y que las aves son un clado más dentro de los "reptiles"; si se tienen en cuenta las formas fósiles, grupos como los anfibios aparecen también como parafiléticos. Los "reptiles", en sentido cladista, incluyen las aves y no incluye a los "reptiles" que condujeron a los mamíferos (Sinápsidos).




</doc>
<doc id="2936" url="https://es.wikipedia.org/wiki?curid=2936" title="Vértebra">
Vértebra

Se denomina vértebra a cada uno de los huesos que conforman la columna vertebral. En los seres humanos hay 33 vértebras durante la etapa fetal y en la niñez (7 cervicales + 12 torácicas + 5 lumbares + 5 sacras + 4 del cóccix), y durante la etapa adulta, solo hay 24 debido a que los huesos del sacro y el cóccix se unen convirtiéndose en un hueso cada uno. Cada una de ellas se encuentra separada de la inmediata inferior por medio de un disco vertebral, exceptuando las 5 vértebras del sacro y las 4 del cóccix, debido a su unión. 

Las vértebras se alinean entre sí por los llamados cuerpos vertebrales y por sus apófisis articulares. Entre una vértebra y otra existen núcleos de tejido conectivo laxo que se denominan discos intervertebrales.

Con excepción de la primera y segunda vértebra cervical, las llamadas vértebras verdaderas o movibles (pertenecientes a las citadas tres regiones superiores) presentan ciertos rasgos comunes que son mejor reconocidos examinando una vértebra de en medio de la región torácica.

Excepto la primera y la segunda vértebras cervicales, que tienen una configuración algo especial, el resto de las vértebras muestra una estructura similar: un cuerpo, dos láminas vertebrales, dos pedículos, una apófisis espinosa, dos apófisis transversas y cuatro apófisis articulares.

Son generalmente pequeñas y delicadas. Sus apófisis espinosas son pequeñas y bífidas (la C7 es la primera vértebra, cuya apófisis espinosa puede ser palpada). Se las puede diferenciar por tener un agujero en la base de las apófisis transversas (agujero para la arteria vertebral). Numeradas de arriba abajo como C1 hasta C7, son las vértebras que permiten la rotación del cuello. Específicamente el atlas (C1) permite al cráneo subir y bajar, y el axis (C2) es el responsable de que la parte superior del cuello gire de izquierda a derecha, luego está la vértebra de rixi (C3) que es la vértebra patrón, a partir de ella todas las vértebras son prácticamente iguales. La vértebra C6 posee lo que se conoce como "Tubérculo de Chassaignac". La vértebra C7 se conoce como "Vértebra Prominente". Además, poseen un canal raquídeo muy ancho, porque coincide con el comienzo de la médula espinal.
Los discos intervertebrales de la región cervical crean lo que se llama la lordosis cervical (curvatura cóncava dorsal) de la columna.

Sus procesos espinosos apuntan hacia abajo en forma casi vertical, y son más pequeñas en relación con las de las otras regiones. Poseen en sus caras laterales unas facetas articulares (fositas costales), que articulan con la cabeza de las costillas, y otra carilla articular en sus procesos transversos destinadas a articular con el tubérculo costal. Tienen un pequeño grado de rotación entre ellas pero, al estar articuladas con la caja torácica, se vuelven casi inmóviles. Los discos intervertebrales de la región torácica crean lo que se llama la cifosis torácica (curvatura convexa dorsal) de la columna.

Son vértebras mucho más robustas que las anteriores ya que han de soportar pesos mayores. Tiene un agujero vertebral de forma triangular, sus apófisis son largas y delgadas. Permiten una considerable flexión y extensión, una moderada flexión lateral y un pequeño grado de rotación (5º). Los discos intervertebrales de la región lumbar crean lo que se llama la lordosis lumbar (curvatura cóncava dorsal) de la columna. Además, su apófisis espinosa es cuadrilátera y se presenta casi horizontalmente.

Durante los cuatro primeros meses de desarrollo embrionario el esclerotomo cambia su posición para rodear la médula espinal y el notocordio. El esclerotomo es formado a partir del mesodermo
y se origina en la parte ventromedial de los somitas. Esta columna de tejido tiene una apariencia segmentada, con partes alternadadas de áreas densas y menos densas.
A medida que el esclerotomo se desarrolla, se condensa más y se empieza a conformar lo que se llama el cuerpo vertebral. El desarrollo de las formas adecuadas de los cuerpos vertebrales está regulado por los genes de tipo HOX. La parte menos densa que se separa del esclerotomo durante el desarrollo acaba convirtiéndose en los discos intervertebrales
El notocordio desaparece en los segmentos del esclorotomo (cuerpo vertebral), pero persistirá en la región de los discos intervertebrales como el núcleo pulposo. El núcleo pulposo y las fibras del llamado annulus fibrosus (anillo fibroso) conformaran el disco intervertebral
Las curvas primarias de la columnas (torácica y sacral) se conforman durante el desarrollo fetal. Las curvas secundarias se forman después del nacimiento. La curvatura cervical se forma como resultado de la elevación de la cabeza y la curvatura lumbar como resultado del proceso natural de caminar.
Existen varias patologías asociadas al desarrollo vertebral. La escoliosis puede ser el resultado de una fusión anómala de las vértebras. En el síndrome de klippel-feil, los pacientes tienen menos vértebras de las normales junto con otros defectos de nacimiento. Un defecto serio durante la gestación puede ser el cierre incompleto del arco vertebral que da lugar a la llamada espina bífida. Hay diferentes tipos de espina bífida que son un reflejo, de la mayor o menor gravedad del problema.



</doc>
<doc id="2937" url="https://es.wikipedia.org/wiki?curid=2937" title="Ciudad del Vaticano">
Ciudad del Vaticano

La Ciudad del Vaticano, oficialmente Estado de la Ciudad del Vaticano (en latín: "Status Civitatis Vaticanæ"; en italiano: "Stato della Città del Vaticano"), o simplemente el Vaticano, es un Estado soberano sin salida al mar, cuyo territorio consta de un enclave dentro de la ciudad de Roma, en la península itálica. Es uno de los seis microestados europeos, y también es el país más pequeño en extensión y población del mundo.

La Ciudad del Vaticano tiene una extensión de 0,44 km² (44 hectáreas) y una población de aproximadamente 800 habitantes, por lo que resulta un híbrido de ciudad elevada al rango de Estado independiente, siendo además el país más pequeño del mundo. Es tan pequeño que solo la basílica de San Pedro es un 7 % de su superficie; la basílica y la plaza de San Pedro ocupan un 20 % del territorio, lo que lo convierte en el territorio independiente más urbanizado del mundo. La Ciudad del Vaticano comenzó su existencia como Estado independiente en 1929 tras la firma de los Pactos de Letrán celebrados entre la Santa Sede y el entonces Reino de Italia, que en 1870 había conquistado los Estados Pontificios.

La Ciudad del Vaticano alberga la Santa Sede, máxima institución de la Iglesia católica. Aunque los dos nombres, «Ciudad del Vaticano» y «Santa Sede», se utilizan a menudo como si fueran equivalentes, el primero se refiere a la ciudad y a su territorio, mientras que el segundo se refiere a la institución que dirige la Iglesia y que tiene personalidad jurídica propia como sujeto de Derecho internacional. En rigor, es la Santa Sede y no el Estado del Vaticano la que mantiene relaciones diplomáticas con los demás países del mundo. Por otro lado, el Vaticano es quien da el soporte temporal y soberano (sustrato territorial) para la actividad de la Santa Sede.

La máxima autoridad del Vaticano y jefe de Estado del mismo es el papa de la Iglesia católica, por lo que puede considerarse la única teocracia y la última monarquía absoluta de Europa. El sumo pontífice delega las funciones de gobierno en el secretario de Estado.

El conjunto arquitectónico e histórico-artístico que conforma la Ciudad del Vaticano fue declarado Patrimonio de la Humanidad por la Unesco en 1984.

Su nombre viene del monte Vaticano (probablemente del latín "vaticinĭum": predicción, vaticinio; o "vāticinātio": profecía, vaticinio, pues antiguamente la colina era la sede de un oráculo etrusco o tal vez del nombre de un poblado del mismo origen, Vaticum).

En italiano la denominación completa es "Stato della Città del Vaticano". En latín, idioma oficial de la Santa Sede, se traduce como "Status Civitatis Vaticanæ".


El Estado de la Ciudad del Vaticano nació con el objeto de un ser instrumento de la independencia de la Santa Sede y de la Iglesia católica respecto a cualquier otro poder externo. El papa, que es cabeza suprema de la Iglesia católica, es también soberano de la Ciudad del Vaticano y ostenta la plenitud de los poderes ejecutivo, legislativo y judicial,por lo que se puede considerar a este país como una teocracia en forma de monarquía absoluta.

El papa administra el Estado mediante la Pontificia Comisión para el Estado de la Ciudad del Vaticano, salvo en los casos que entienda reservarse a sí mismo o a otras instancias. Equivale al poder legislativo y está compuesta por cardenales nombrados por el papa para un quinquenio. El papa delega el poder ejecutivo en el presidente de la Comisión, coadyuvado por el secretario y el vicesecretario generales. El presidente de la Comisión tiene también facultad legislativa: puede emitir ordenanzas, y en casos de urgente necesidad puede adoptar disposiciones con carácter de ley, siempre que la Comisión las confirme en los 3 meses siguientes. Asume también la representación diplomática del Estado excepto ante los Estados extranjeros, función que es reservada al papa. Actualmente el presidente de la Pontificia Comisión para el Estado de la Ciudad del Vaticano y de la Gobernación del Estado de la Ciudad del Vaticano es el cardenal Giuseppe Bertello.

El cargo de gobernador del Estado de la Ciudad del Vaticano fue, en una época, unipersonal y ejercido por el marqués y conocido numismático Camillo Serafini, desde 1929, año de la fundación del Estado, hasta la muerte de este en 1952. Posteriormente, no fue designado sucesor de Serafini, y el cargo propiamente tal tampoco fue mencionado en la Ley Fundamental del Estado, emitida por el papa Juan Pablo II el 26 de noviembre de 2000, y que entró en vigor el 22 de febrero de 2001. El presidente de la Comisión Pontificia para el Estado de la Ciudad del Vaticano ha ejercido desde 1952 las funciones que antes eran atribuidas al gobernador, y desde 2001 también recibe el título de presidente de la Gobernación del Estado de la Ciudad del Vaticano.

Durante el periodo de sede vacante, producido tras la muerte o renuncia del papa, los poderes recaen en el Colegio Cardenalicio, aunque este únicamente podrá dictar leyes en caso de urgencia y con su duración limitada a dicho espacio de tiempo. Será tarea de este colegio de cardenales elegir a un nuevo pontífice en cónclave.

El idioma más hablado es el italiano. La moneda, según un acuerdo suscrito con la Unión Europea (UE), es el euro.

En enero de 2014 eran 180 Estados los que mantenían relaciones diplomáticas con la Santa Sede, reconociendo la existencia del microestado. Entre los países que no tienen relaciones diplomáticas con la Santa Sede se encuentran China, Corea del Norte, Vietnam y Arabia Saudita.

Es el único país del mundo en donde no hay votaciones para elegir cargos de gobierno.

La Guardia Suiza es el cuerpo militar encargado de la seguridad de la Ciudad del Vaticano. Está compuesta por unos 100 soldados (todos varones): cuatro oficiales, 23 mandos intermedios, 70 alabarderos, 2 tamborileros y un capellán. Se les entrena en procedimientos y manejo de armas modernas (como el fusil suizo Sig 550), aunque también se enseña a manejar la espada y la alabarda.

La Guardia Suiza tiene su cuartel frente al Palacio Apostólico Pontificio.
Según el Tratado de Letrán, se ha establecido que la Policía italiana custodie, junto con la Guardia Suiza y los Servicios Vaticanos de Seguridad, la plaza de San Pedro.

La defensa de la Ciudad del Vaticano es proporcionada por Italia.

El Estado de la Ciudad del Vaticano consta de la ciudad vaticana propiamente dicha, cuya extensión aproximada es de unas 44 hectáreas y sobre la que ejerce total soberanía, y de otros edificios y lugares, tanto en la ciudad de Roma como en el resto de Italia, que gozan del derecho de extraterritorialidad. Se encuentra a la orilla derecha del río Tíber y la colina vaticana, lugar donde se establecieron los primeros asentamientos en tiempos del cristianismo primitivo.

Entre ellos, cabe destacar la residencia estival de los papas, el palacio de Castel Gandolfo con sus jardines, cuya extensión ronda las 55 ha, y que dista unos 34 km de la Urbe; las basílicas patriarcales de San Juan de Letrán, Santa María la Mayor y San Pablo Extramuros, varios edificios más en la ciudad de Roma: la Cancillería Apostólica, el palacio de San Calixto en el Trastévere, la Curia General de los Jesuitas, el Vicariato y el palacio de Propaganda Fide, entre otros, así como el Centro Televisivo de Santa María de Galería.

Durante el siglo XV, debido a que la basílica paleocristiana se encontraba bastante deteriorada y amenazaba con poder derrumbarse, el papa Nicolás V encargó su reconstrucción a Bernardo Rosellino en 1452, pero los trabajos se interrumpieron tres años después a la muerte del papa y los muros tan solo alcanzaban a levantarse un metro del suelo.

Es con Julio II en 1506 cuando se reinician las obras acogiendo el diseño propuesto por Bramante, y se terminan con Paulo V en 1626, gracias a las ventas de indulgencias. El nombre de este papa aparece en la fachada de la basílica.

Hubo dos proyectos iniciales, realizados por Bramante y Rafael, respectivamente. El primero es un proyecto de cruz griega y el segundo de cruz latina. Posteriormente, Miguel Ángel retoma el proyecto de cruz griega de Bramante, diseñando también la cúpula de la basílica. El último arquitecto que intervino en la construcción fue Gian Lorenzo Bernini. En la cúpula, con letras de dos metros de alto está escrito "Tu est Petrus, et super hanc petram aedificabo eclessia meam", es decir: "Tú eres Pedro, y sobre esta piedra edificaré mi iglesia". En ese mismo lugar se construyó unos mil años antes otra basílica de tres naves longitudinales, paleocristiana. Esta, en el siglo XV, amenazaba con derrumbarse y fue sustituida.

En la actualidad está permitida su visita, incluida la cúpula, siempre teniendo en cuenta que hay que vestir con recato. Está prohibida la entrada con tirantes y pantalones cortos tanto a hombres como a mujeres.

Desde 1277, está conectada con el Castillo Sant'Angelo por un corredor fortificado, llamado "Passetto", de unos 800 metros de longitud.

En el año 1939, siendo papa Pío XII, y cuando se llevaban a cabo las excavaciones para preparar la tumba de Pío XI, se descubrió un mosaico. Existía una tradición que decía que debajo del altar papal, debajo del baldaquino de Bernini, bajo la cúpula de Miguel Ángel, había una necrópolis, un cementerio, donde había sido enterrado San Pedro, pero de esto todavía no había certeza. Pío XII mandó que siguieran excavando y apareció la necrópolis.

Constantino I el Grande, para agradecer a Cristo que, según él, le había dado la victoria en la batalla de Puente Milvio sobre Majencio, se convierte al cristianismo; en Roma, hay un obelisco en el que se lee: «Aquí fue bautizado Constantino por el papa Silvestre.» Después de ello, Constantino comenzó a construir una serie de templos cristianos; uno de ellos fue la basílica en honor de San Pedro, que según él, estaría edificada sobre la tumba del Apóstol. Hay indicios que llevan a pensar que él estaba seguro de la localización exacta de la tumba: por ejemplo, empieza a edificar su basílica en la ladera de un monte que tiene mucho desnivel (11 metros), lo que hace realizar un gran trabajo de movimiento de tierra para lograr una explanada (sin maquinarias), aunque no tan lejos tenía un sitio que parecía ideal: la explanada del circo de Nerón, que medía trescientos metros de largo y unos cien de ancho. Construyendo en este sitio se hubiera evitado grandes costos y trabajo. Otras dificultades que se deben haber presentado, además de las técnicas, serían las morales y jurídicas, ya que bajo esta construcción quedaba enterrada una necrópolis que era muy importante en Roma y en la que estaban enterrados personajes importantes de aquella sociedad, como los Flavios y los Valerios.

El papa Pío XII anunció por radio en el tiempo de Navidad de 1950 que se había encontrado la tumba de San Pedro. Una vez culminada la investigación sobre dicha tumba en 1952, la profesora Margherita Guarducci, autoridad en epigrafía griega, comenzó a descifrar los grafitos que hay en uno de los muros contiguos a esa tumba. Algunos de ellos, que estaban casi escritos unos sobre otros son: «Pedro, ruega por los cristianos que estamos sepultados junto a tu cuerpo.» También se consiguió el logotipo de San Pedro, que era como una P y en el palo vertical tres rayas horizontales en forma de llave. Esta profesora concluyó que por allí está la tumba de San Pedro, pues entre los grafitos plasmados en el muro denominado G (de color blanco) y en el adyacente (de color rojo), descifró un grafito que significa: «Pedro está aquí». Al excavar descubrieron un nicho forrado de mármol blanco, que contenía huesos.

La responsabilidad de estudiar estos huesos recayó en Venerato Correnti, profesor y catedrático de Antropología de la Universidad de Palermo. En el estudio definió que en el nicho había huesos humanos y el de un ratón. Con respecto al animal, indicó que se coló por alguna rendija y al no poder salir murió allí. Un detalle interesante es que los huesos de los pies no aparecieron entre los restos hallados, y se puede recordar, que quien era crucificado cabeza abajo (entre los diferentes modos que existían en la crucifixión), se le descolgaba cortando los pies y así el cuerpo caía al suelo.

El papa comunicó al mundo tal hallazgo, en junio de 1968, asegurando que se habían encontrado los restos (reliquias) de San Pedro Apóstol.
La editorial Vaticana publicó un libro escrito por la profesora Guarducci con toda la información y que se titula "Las Reliquias de San Pedro".

Dentro del territorio de la Ciudad del Vaticano están los Jardines Vaticanos (en italiano: "Giardini Vaticani"), que representan cerca de la mitad de su territorio. Los jardines, establecidos durante el Renacimiento y el Barroco, están decorados con fuentes y esculturas.

Los jardines cubren aproximadamente 23 hectáreas (0,23 kilómetros cuadrados). El punto más alto está a 60 metros (197 pies) sobre el nivel medio del mar. Los muros de piedra delimitan la zona en el norte, sur y oeste.

Los jardines datan de la época medieval, cuando los huertos y los viñedos se extendían al norte del Palacio Apostólico Papal. En 1279, el Papa Nicolás III (Giovanni Gaetano Orsini, 1277-1280) trasladó su residencia al Vaticano desde el Palacio de Letrán y cerró esta zona con muros. Plantó un huerto (pomerium), un césped (pratellum) y un jardín (viridarium).

El clima de la Ciudad del Vaticano, como el de Roma, es un clima templado, mediterráneo con inviernos moderados y lluviosos desde septiembre a mitades de mayo y veranos calurosos y secos que van de mayo a agosto.

En julio de 2007, el Vaticano aceptó una propuesta de dos empresas con sede en San Francisco y Budapest respectivamente, por la que se convertiría en el primer estado neutral en cuanto a emisiones de carbono, compensando sus emisiones de dióxido de carbono con la creación de un Bosque Climático del Vaticano en Hungría, como un gesto puramente simbólico para animar a los católicos a hacer más para salvaguardar el planeta.

El 26 de noviembre de 2008, el propio Vaticano puso en marcha un plan anunciado en mayo de 2007 para cubrir el techo de la Sala de Audiencias Pablo VI con paneles solares.

La nacionalidad vaticana no se obtiene por nacimiento, sino por concesión. Es la única en ese tipo. Son ciudadanos de nacionalidad vaticana todos los diplomáticos empleados en las nunciaturas como las embajadas vaticanas de todo el mundo y aquellas personas que ejercen funciones para el Estado de la Ciudad. La nacionalidad vaticana se añade a la nacionalidad de origen y se pierde cuando las personas dejan de ejercer estas funciones.

En el Vaticano residen el sumo pontífice, los cardenales —que viven dentro de los muros, o en Roma—, los miembros del cuerpo diplomático, los sacerdotes y hermanos religiosos, los guardias suizos y algunos hombres y mujeres seglares, en su mayoría empleados en el Estado, junto con sus respectivos cónyuges e hijos.

El Vaticano no puede mantenerse a merced de la actividad productiva de su propio territorio, limitada a la venta de recuerdos turísticos, libros, sellos y entradas a museos. Pero cuenta con los ingresos de la organización católica en todo el mundo, provenientes de: las aportaciones económicas de los Estados donde cuenta con acuerdos (llamados Concordatos) de financiación (por su tradición católica); las donaciones de los católicos (a nivel personal o empresarial); y los beneficios de las empresas, escuelas, universidades y bancos propiedad de la Iglesia.

La economía estaba seriamente dañada hacia 1979, y tres años más tarde se produjo la quiebra de uno de los bancos más ilustres de Italia, el Banco Ambrosiano, que llevaba las finanzas internacionales del Vaticano, y el asesinato de su director Roberto Calvi: las investigaciones consiguientes revelarían que el banco se dedicaba al blanqueo del dinero de la mafia. Más tarde, el papa Juan Pablo II trasladó la responsabilidad de la economía vaticana a partir de 1984. Cinco años más tarde, el papa lleva a cabo una reestructuración de la organización económica y la dirección de la economía había sido encargada a cinco financieros reconocidos internacionalmente (bajo la supervisión de una comisión de cinco cardenales). La Administración del Patrimonio de la Sede Apostólica se encarga de estos controles.

La lira vaticana estaba a la par con la lira italiana, que también circulaba como moneda válida dentro de la Ciudad del Vaticano. Estas unidades monetarias cambiaron con la entrada en vigor de la moneda única europea. Por acuerdo con Italia, en representación de la Unión Europea, la moneda vaticana es el euro, con diseño propio y aceptación en toda la zona euro. Dado que el Vaticano no tiene casa de moneda propia, ha establecido un acuerdo con Italia para su acuñación. El valor total de las monedas acuñadas no puede exceder de un millón de euros anuales.

La Ciudad del Vaticano emite sus propios sellos postales, tiene su propio periódico ("L'Osservatore Romano"), una emisora de radio (Radio Vaticano) y una televisión (Centro Televisivo Vaticano).

Además tiene distintas fundaciones, academias y universidades pontificias.

El Estado Vaticano cuenta con un servicio de teléfonos que dispone de modernas instalaciones para la comunicación tanto interna como externa de la ciudad y personal altamente capacitado. Téngase en cuenta que no existe el derecho a la confidencialidad de las comunicaciones telefónicas o de otro tipo. La ciudad-Estado consta de una compleja infraestructura de redes por la que consta de una completa autonomía.

La red ferroviaria de la Ciudad del Vaticano conecta la estación Ciudad del Vaticano, con la red ferroviaria de Italia, en la estación Roma San Pietro.

La ciudad posee también un helipuerto. Los servicios de ómnibus desde la ciudad de Roma son frecuentes.

El Vaticano otorga a través del registro de vehículos vaticanos una patente a los automóviles de dicha ciudad-Estado. Los automóviles que son del gobierno llevan las siglas SCV y los que son de los ciudadanos CV.

La cultura del Vaticano es obviamente correspondiente a la cultura de la religión católica, aunque se abre también al arte de otras culturas, y su mayor exponente son las obras de arquitectura, como la Basílica de San Pedro, la plaza de San Pedro, la Capilla Sixtina y los Museos Vaticanos.

Entre los Museos Vaticanos se encuentran: el Museo Gregoriano de arte egipcio y de arte etrusco, el Museo Pío Clementino, el Museo Chiaramonti y la Pinacoteca Vaticana.

Muchos artistas y arquitectos famosos como Bramante, Miguel Ángel, Rafael y Bernini trabajaron en importantísimas obras artísticas que hoy se pueden admirar en los edificios vaticanos.

En noviembre de 2006 se publicó un libro que revela la cocina del Vaticano, y que realiza un recorrido por la gastronomía histórica desde el primer papa hasta nuestros días, aderezado con una colección de recetas que incluyen menús tan representativos como el de la Última Cena o los platos favoritos de muchos de los papas. La autora del libro comenta que la gastronomía de este país es «una de las más complejas y ricas del mundo, mucho más que la de cualquier Casa Real».

El libro contiene además datos curiosos sobre los orígenes de algunas de las numerosas recetas que se inventaron en esta Ciudad, como por ejemplo la salsa verde (también llamada salsa vaticana), la salsa carmelita o la cocción al baño maría, además de información sobre protocolo y numerosas referencias que muestran la indisoluble unión entre la historia y la gastronomía de los papas.

El Vaticano cuenta con la Selección de fútbol de la Ciudad del Vaticano, que no está asociada a la UEFA ni a la FIFA ni al COI. El equipo está compuesto por voluntarios de la Guardia Suiza, miembros del Consejo Papal y por guardias de los museos. Desde 2007, la ciudad cuenta con un campeonato; además los seminaristas que, mientras estudian en Roma, viven en los diversos colegios pontificios participan en la Copa Clerical, un campeonato formado por religiosos que se empezó a jugar desde febrero del mismo año.

La relación de la Santa Sede con los organismos deportivos está encomendada al Consejo Pontificio para los Laicos, donde el papa Juan Pablo II creó una sección llamada "Iglesia y Deporte" en 2004.

Hasta la fecha, es el único país que no ha participado en los Juegos Olímpicos.





</doc>
<doc id="2942" url="https://es.wikipedia.org/wiki?curid=2942" title="Medicina veterinaria">
Medicina veterinaria

La medicina veterinaria es la rama de la medicina que se ocupa de la prevención, diagnóstico y tratamiento de enfermedades, trastornos y lesiones en los animales. El ámbito de la medicina veterinaria es amplio, cubriendo todas las especies, tanto domésticas como silvestres.

El profesional universitario que pone en práctica esta ciencia es llamado veterinario, médico veterinario o médico cirujano veterinario, mientras que en algunos países de Latinoamérica, el profesional que se dedica a la productividad agropecuaria es llamado zootecnista. El profesional técnico es llamado técnico veterinario o enfermero veterinario.

Denominación oficial para los médicos veterinarios en algunos países 
Esta palabra tiene varios orígenes posibles. La más comúnmente reconocida es que esta palabra proviene del idioma latín culto. Veterinarius, según el escritor Catón, era el conocedor y practicante del arte de curar las veterinae o veterina, es decir, las bestias de carga. El nombre de estos animales parece proceder de vetus (viejo), porque se trataría de animales envejecidos, y por ende no aptos ya para las carreras ni para los carros de guerra y sólo útiles para el transporte. 

Otras fuentes afirman que veterina pudo nacer del verbo veho, vehere, de donde se derivaría vehículo, que significa precisamente transportar. 

Para los árabes está la palabra "albéitar" que hace referencia a la persona encargada de curar las patologías de los caballos, animales tan importantes para la cultura árabe. En Europa, los humanistas del siglo XVI recuperaron las palabras «veterinaria» y «veterinario», que se denominaban en el mundo hispanohablante, respectivamente, "albeitería" y "albeitar" ("alveitar" en portugués), palabras obsoletas en la actualidad.

Los papiros egipcios de Lahun (1900 a.C.) y la literatura vedas de la antigua India ofrecen uno de los primeros registros escritos sobre la medicina veterinaria. El emperador budista de la India, Aśoka, ordenó lo siguiente: ""En todas partes del reino se harán dos tipos de medicamentos (चिकित्सा), medicina para las personas y la medicina para animales. Cuando no hubiese hierbas curativas para las personas y animales, se ordena comprarlas y sembrarlas"".

Los primeros intentos de organizar y regular la práctica veterinaria tienden a centrarse en los caballos, debido a su importancia como medio de transporte y arma de guerra. Durante la Edad Media (año 1356), el alcalde de Londres, Henry Picard, preocupado por la mala calidad de la atención prestada a los caballos en la ciudad, pidió que todos los herradores que operan dentro de un radio de siete millas de la ciudad forman una "beca" para regular y mejorar sus prácticas. Esta última instancia condujo a la creación del Gremio de herradores en 1674.

El primer tratado completo sobre la anatomía de una especie no humana corresponde al libro "Anatomia del Cavallo" (Anatomía del caballo), publicado por el italiano Carlo Ruini en el año 1598.

La primera facultad veterinaria data del año 1761, siendo fundada por Claude Bourgelat como Escuela Nacional Veterinaria de la Universidad de Lyon. Poco tiempo después se fundan otras siguiendo este modelo en otros países de Europa, como la de Padua en 1765, Viena en 1768 o la de Turín en 1769.

En Alemania destacan las contribuciones de Johann Christian Erxleben a la medicina veterinaria moderna en Gotinga.

La Sociedad Agrícola Odiham fue fundada en 1783 en Inglaterra para promover la agricultura y la industria, jugó un papel importante en la fundación de la profesión veterinaria en Gran Bretaña. Thomas Burgess, miembro fundador de la sociedad, comenzó a asumir la causa del bienestar animal y tratamiento más humanitario de los animales enfermos. En una de las reunión de la Sociedad en 1785, se resolvió "promover el estudio de herraje con principios científicos racionales". El Real Colegio de Veterinarios del Reino Unido fue establecido por carta real en 1844.

La ciencia veterinaria alcanzaría "mayoría de edad" a finales del siglo XIX, con notables contribuciones de Sir John McFadyean, acreditados por muchos como el fundador de la investigación veterinaria moderna. En Estados Unidos, las primeras escuelas fueron establecidas en el siglo XIX en Boston, Nueva York y Filadelfia.

Para el año 2019, la publicación QS Top Universities, indicaba que las diez mejores universidades del mundo en esta disciplina eran:

En el año 1900 el rumor dentro del Ministerio de Agricultura inició a aparecer: una epizootia que afectaba a los bovinos se había declarado en el sur de la provincia de Buenos Aires. Por muchos días la aftosa ocupó la atención de los principales periódicos y, además de afectar gravemente desde entonces y por casi un siglo la economía nacional, dejó al desnudo las carencias en legislación, en policía sanitaria y en la pobre atención brindada a la formación de profesionales de una ciencia hasta entonces no muy valorada, la veterinaria.

Cuatro años más tarde, el panorama había mejorado en cierto aspecto gracias a la sanción de una ley de Policía Sanitaria, pero los controles sanitarios poco o nada habían cambiado. Consciente de tal situación, el ministro Wenceslao Escalante decide cambiar el propósito de la ley 4.174 (de 1903) que creaba una Estación Agronómica, Granja Modelo y Escuela Práctica de Agricultura en los terrenos de la "Chacarita de los Colegiales", ubicados en el oeste de la ciudad de Buenos Aires, para dar nacimiento ahora -por decreto del 19 de agosto de 1904- al Instituto Superior de Agronomía y Veterinaria.

Inaugurado el 25 de septiembre de 1904, el Instituto se debatió en sus primeros tiempos en un mar de incertidumbres creado principalmente por la prédica negativa de la prensa opositora al Gral. Roca. Este, justo es recordarlo, había ya inaugurado en 1883 el primer instituto de formación superior de veterinarios: la Escuela de Agronomía y Veterinaria y Haras de la provincia de Buenos Aires en Santa Catalina, más tarde trasladada a la ciudad de La Plata y transformada en Facultad de Agronomía y Veterinaria. 

El Instituto de la Chacarita, como se lo denominaba vulgarmente, había sido proyectado sobre terrenos inundables y que habían sido asiento de fábricas de ladrillos lo que lo hacían inapropiado para los cultivos y para colmo estaba alejado del centro de la ciudad siendo su acceso sumamente difícil.

No era poco para los adversarios políticos, que además se quejaban de que se había malgastado los fondos del erario en crear una escuela para formar veterinarios.

Su primer rector fue el químico y médico Pedro N. Arata, notable exponente intelectual de la generación del 80. Gran parte del plantel docente fue contratado en Europa, así por ejemplo vinieron, entre otros, de Italia los Dres. Godofredo Cassai, Angel Baldoni y Salvatore Baldasarre; de Alemania, Kurt Wolffhugel; de Francia, Julio Lesage. Solamente un veterinario argentino integró el cuerpo de profesores en los primeros años: el Dr. Joaquín Zabala, quien más tarde fuera considerado el padre de los veterinarios argentinos.

A los dos años de funcionamiento la escuela contaba ya con una revista propia: los Anales del Instituto Superior de Agronomía y Veterinaria de la Nación. También había un Centro de Estudiantes, cuyo presidente era el futuro veterinario José Morales Bustamante.

A fines de 1908 egresó la primera promoción de graduados; más de treinta veterinarios se incorporaban así al quehacer nacional. Este fruto permitió apreciar la eficiencia y calidad de los estudios, por lo que el 10 de mayo de 1909 el Poder Ejecutivo incorporó al Instituto a la Universidad de Buenos Aires, dejando entonces de ser una dependencia del Ministerio de Agricultura. Al poco tiempo la Universidad dio su conformidad a este paso, pero dándole al Instituto categoría de Facultad. El rector Arata pasó a ser así el primer decano de la naciente Facultad de Agronomía y Veterinaria.

Hubo que esperar quince años hasta que se designara un decano que fuera un profesional de alguna de las dos carreras de la casa. Correspondió ese honor al veterinario Daniel Inchausti que ejerció el decanato en el período 1924-1927.

Hasta 1912 los ingresantes a la carrera de veterinaria siempre fueron mayoría sobre los de agronomía, pero a partir de 1913 la tendencia cambió abruptamente, llegando en algunos años los inscriptos en agronomía a cuadruplicar a los de veterinaria. Este fuerte desnivel fue fermento de ciertos conflictos en el manejo del presupuesto, los que se agudizaron desde mediados del siglo. Así las cosas en 1972 la situación se tornó insostenible para la carrera de veterinaria y después de un traumático proceso -que incluyó marchas por la ciudad, la suspensión de las clases y una huelga de hambre- se logró la sanción de la ley 19.908 del 23 de octubre de 1972 que dio nacimiento a la Facultad de Ciencias Veterinarias. La separación se efectivizó desde el primer día del año siguiente y fue el primer decano el Dr. Guillermo C. Lucas

La escuela veterinaria en México surge el 17 de agosto de 1853 por el decreto 4001, expedido por el entonces presidente de la República Mexicana, Antonio López de Santa Anna . Esta escuela pertenecía al Colegio Nacional de Agricultura, siendo la primera escuela de medicina veterinaria de su género en México y en el continente . Durante esa época el Rector del Colegio Nacional de Agricultura era José María Arreola .

En el decreto del presidente Santa Anna se estableció además que después de seis años desde la creación de la carrera no se permitiría la práctica veterinaria a quien no hubiese obtenido el título correspondiente (Artículo 13º) , el cual era otorgado por el Colegio Nacional de Agricultura, aunque tuvieron que pasar diez años desde la creación de la escuela hasta el egreso de los primeros estudiantes debido a diversas vicisitudes .

La duración de la carrera de veterinario sería de cuatro años, los cuales serían cursados después de la educación secundaria . Durante aquella época la educación veterinaria en México y en el mundo estaba casi totalmente encaminada hacia la medicina equina, debido a la importancia de este animal tanto en el aspecto militar como en el económico, de tal forma que no es una sorpresa que las asignaturas del primer plan de estudios estuvieran encaminadas hacia esta especie.

En la elaboración del plan de estudios de la carrera, la mayor parte se le atribuye al ilustre militar y veterinario francés Pascal Eugène Bergeyre, egresado de la Escuela Nacional de Veterinaria de Tolsa, el cual además fungía como el médico encargado de las caballerizas del Presidente de la República . De acuerdo con Eugène Bergeyre ""las cátedras se imparten basándose en textos extranjeros correspondientes a las enseñanzas de las escuelas europeas de medicina veterinaria y principalmente las francesas"" . Principalmente, el libro de Philippe Etinne Lafosse "Cours d' Hippiatrique, ou Traité de la médicine des" "chevaux "fue la piedra angular en el principio de la educación veterinaria en México , de la que se conserva un ejemplar original en la Biblioteca MV José de la Luz Gómez en la actual Facultad de Medicina Veterinaria y Zootecnia de la UNAM.

El plan de estudios para la carrera de veterinaria en 1853 estaba compuesto por las siguientes materias: En el primer año, lección alternada de zoología y dibujo anatómico, lección diaria de química, lección diaria de inglés, ejercicios de equitación y manipulaciones químicas; en el segundo año, lección diaria de anatomía y fisiología hipiátricas y a fin de año, curso comprendido de higiene hipiátrica, natación e inglés; en el tercer año, lección diaria de patología interna y externa hipiátricas, lección diaria de clínica interna y externa hipiátricas, práctica anatómica y patológica hipiátrica, lección diaria de idioma alemán; en el cuarto año, lecciones diarias de operaciones terapéuticas, lecciones alternadas de los principios de economía rural y práctica de herrajes, lección diaria de idioma alemán .

Las labores académicas preparativas iniciaron el 22 de febrero de 1854, en el antiguo hospicio de San Jacinto, el cual se encuentra ubicado en la actual Calzada México - Tacuba .

Los profesores del colegio de San Jacinto de acuerdo a un siguiente decreto establecido el 4 de enero de 1856 por el entonces presidente Ignacio Comonfort propusieron como director del Colegio Nacional de Agricultura al eminente médico cirujano y científico Leopoldo Río de la Loza, el cual había sido presidente de la Academia de Medicina, para iniciar finalmente las actividades académicas .

Además, este decreto declaraba que la enseñanza veterinaria estuviera orientada a la formación de mariscales veterinarios, para la cual se cursaban tres años de estudios y por otra parte, la formación de los profesores de veterinaria duraría cinco años .

Para el inicio de las clases, las cátedras de anatomía y fisiología serían impartidas por Ignacio Alvarado (considerado el iniciador de la fisiología experimental en México y médico de cabecera Don Benito Juárez) mientras que las relacionadas con la Medicina Veterinaria eran impartidas por Eugène Bergeyre .

Será hasta el año de 1857 que ingresarían a la carrera de medicina veterinaria los primeros siete alumnos, iniciando los cursos formalmente el 9 de abril de 1858 .

Estos alumnos fueron José de la Luz Gómez, José E. Mota, Manuel y Mariano G. Aragón, José María Lugo, Narciso Aguirre e Ignacio Salazar . Desgraciadamente se suscitaba la Guerra de Reforma desde el 17 de diciembre de 1857, la cual causaría complicaciones para la enseñanza veterinaria y para el colegio de San Jacinto, ya que este fue utilizado como cuartel .

Sería durante enero 1861, que el presidente Benito Juárez entraría a la Ciudad de México, y a los pocos días retiró de su cargo al Dr. Río de la Loza, mismo que fue sustituido por Juan N. Navarro, un médico militar originario de Michoacán. El Sr. Navarro dirigiría la escuela desde 1861 a 1867 .

Su enseñanza en Chile comienza de forma estable y definitiva el 1 de mayo de 1898 por parte del Ejército de Chile, mediante los cursos de Veterinaria Militar, con una duración de 3 años. Luego, en 1905 se creó la Escuela Militar de Veterinaria

La enseñanza de la profesión paso de lo militar a lo civil el 10 de noviembre de 1915, mediante decreto Supremo N° 1853 que crea en la Escuela de Medicina Veterinaria Civil, en dependencias de la Quinta Normal de Agricultura.

La Universidad de Chile fue la primera casa de estudios superior en dictar la carrera en dicho país, el 12 de abril de 1928, al fundar la primera Facultad de Ciencias Veterinarias y Pecuarias del país.

En el año 2019 trece universidades chilenas impartían la carrera, nombradas desde la más antigua a la más reciente en ofrecer la carreraː Universidad de Chile (1928), Universidad Austral (1955), Universidad de Concepción (1972), Universidad Santo Tomás (1990), Universidad Iberoamericana de Ciencias y Tecnología (1991), Universidad Mayor (1991), Universidad Católica de Temuco (1993), Universidad de las Américas (2000), Universidad de Viña del Mar (2002), Universidad San Sebastián (2002), Universidad Andrés Bello (2004), Universidad Pedro de Valdivia (2004), Universidad del Pacífico (2006) y Universidad de O'Higgins (2018). La Pontificia Universidad Católica de Chile dictará la carrera a partir de 2020. 

La extinta Universidad Regional San Marcos dictó la carrera entre los años 2004 y 2011.
La Universidad Iberoamericana de Ciencias y Tecnología se encuentra en proceso de cierre programado para el año 2020. La La Universidad del Pacífico se encuentra en proceso de cierre programado para el año 2022.

En las legiones romanas de la provincia de Hispania existían profesionales de la medicina animal llamados "medicus equarius" («médicos de caballos»), que atendían los "veterinarium" («hospitales veterinarios»), en los "castrum" («campamentos romanos»).

En 1489, a Fernando de Palencia las cortes le nombran herrador y albéitar de la casa del príncipe don Juan, con la misma categoría que los oficiales de escuadrones y tercios. Fernando de Palencia es considerado el primer veterinario militar oficial de España y del mundo.

En la época de Felipe II, los mariscales («veterinarios») acompañaban a los Tercios reales. Consta que en 1661 los mariscales tenían un texto excelente que indicaba cómo tratar y curar las heridas de arcabuz de los caballos.

La Facultad de veterinaria de Madrid fue fundada en el siglo XIX por Real Orden de 6 de agosto de 1835, a partir de la Escuela de Veterinaria de Madrid, que había sido creada en 1792.

Argentina

La enseñanza de la Veterinaria en Argentina, comienza en el Partido de Lomas de Zamora, de la Provincia de Buenos Aires, por decreto provincial de la Legislatura de la provincia de Buenos Aires sancionó el 13 de septiembre de 1881 la Ley 1.424, en la que disponía la creación de una Casa de Monta y Escuela de Veterinaria que se establecería junto a la Escuela Práctica de Agricultura de Santa Catalina. Esta escuela dio origen a la Facultad que, de este modo, fue la primera en su tipo de todo América del Sur.

En agosto de 1882, había cinco profesores belgas -el ingeniero civil Camilo Gillet, el ingeniero agrónomo Gustavo André y los veterinarios Carlos Lambert, Carlos Tombeur y Desiderio Bernier- y uno francés -el ingeniero agrónomo Julio Frommel-, que serían los fundadores del Instituto. La Escuela de Agronomía y Veterinaria y Haras de la Provincia de Buenos Aires abrió sus puertas el 6 de agosto de 1883 (día en que se conmemora en nuestro país el inicio de los estudios veterinarios) y posteriormente tomó la denominación de Instituto Agronómico Veterinario de Santa Catalina. Dependiendo del Ministerio de Gobierno de la Provincia de Buenos Aires.

Fue así como, el 25 de septiembre 1905, que el Doctor Joaquín Víctor González, gracias a la ley 4.699, concibió la idea de organizar una nueva universidad, de carácter científico y experimental, en la que se desarrollara ampliamente la investigación científica y se impartieran las enseñanzas primaria y secundaria, también de carácter experimental. La iniciativa del entonces responsable de la cartera de Justicia e Instrucción Pública respondía a su preocupación por los problemas de organización universitaria. Abocado a la búsqueda de un lugar para impulsar su proyecto, Joaquín González centró su mirada en la Universidad provincial que funcionaba en La Plata, observando las principales casas de estudio que existían en esa época y que pudieran dar lugar a la institución soñada durante la presidencia de Manuel Quintana. Cabe destacar que el 12 de agosto del mismo año se firmó un convenio ad referéndum entre la Nación y la Provincia para la creación de la Universidad Nacional.

El 1° de enero de 1921 queda constituir la Facultad de Veterinaria como entidad autónoma. Esta medida significaba la separación de la de Agronomía tomando en cuenta la misión híbrida de ambas instituciones, las que persiguen fines completamente diversos.

En Argentina se encuentran en las diferentes Universidades Nacionales que dictan la carrera de Veterinaria, Universidad Nacional de la Plata (UNLP), Universidad de Buenos Aires (UBA), Universidad Nacional de La Pampa (UNLPam), Universidad Nacional de La Rioja (UNLaR), Universidad Nacional de Río Cuarto (UNRC), Universidad Nacional de Río Negro (UNRN), Universidad Nacional de Rosario (UNR), Universidad Nacional de Tucumán (UNT), Universidad Nacional de Villa María (UNVM), Universidad Nacional del Centro de la Provincia de Buenos Aires (UNCen), Universidad Nacional del Litoral (UNL), Universidad Nacional del Nordeste (UNNE), Universidad Nacional de Entre Ríos (UNER) 

Dentro de la formación de la medicina veterinaria en la República Argentina se encuentran además de las Universidades Nacionales, varias Universidades privadas que dictan el curso de medicina veterinaria como: Universidad del Salvador (USAL), Universidad Católica de Córdoba (UCCOR), Universidad Católica de Cuyo - Sede San Luis (UCCYYOSL), Universidad Católica de Salta (UCASAL), Universidad Juan Agustín Maza (UMAZA), 

El veterinario o médico veterinario (conocido también como médico cirujano veterinario), es aquel profesional universitario encargado de la salud animal, con estudios equivalentes a una licenciatura o grado (en España). El rol profesional es el equivalente de un médico en humanos. En la lengua española (de manera coloquial), también se denomina doctor a estos profesionales de la salud, aunque no hayan obtenido el grado académico de doctorado.

Entre las funciones que realiza el médico cirujano veterinario se destacan el diagnóstico y tratamiento de la patología de los animales, mejorar el rendimiento animal y la ganadería productiva, vigilar la fabricación y puesta en circulación, así como su estado, de los productos alimenticios de origen animal destinados al consumo humano (bromatologia), la epidemiología y salud pública, la investigación y la docencia. Cada casa de estudios sugiere un plan de estudio particular de la Universidad que dicta la carrera es por ello que es posible encontrar diversas orientaciones, descripción de roles y alcances del título según la Universidad que se consulte.

El médico veterinario puede necesitar la asistencia de ayudantes o técnicos veterinarios para llevar a cabo tareas inherentes a la profesión, ya que son ellos los encargados de ejecutar las indicaciones médicas de la forma correcta para con los pacientes en tratamiento y generar la relación profesional con los propietarios de los pacientes.

El médico veterinario desempeña varios campos ocupacionales en la salud pública y animal. Entre los que se detallan son: Diagnosticar, prevenir y tratar enfermedades de los animales de compañía (gato, perro, equinos entre otros), de producción (bovinos, ovinos, caprinos, suinos, etc) y de la fauna silvestre (lobos marinos, focas, tortugas, etc). Además de dirigir, organizar, planificar y asesorar establecimientos parques zoológicos, ecoparques, reservas privadas, municipales, provinciales o nacionales, en la preservación de fauna autóctona y exótica, en repuesta de preservar las especies que se encuentran en extinción. 

Además el médico veterinario tiene que desempeñarse profesionalmente ante sus pacientes, respetando los marcos éticos, socio culturales de las sociedades, siempre pensando en el bienestar animal, desempeñarse dentro de las normas legales. Con los conocimientos adquiridos durante el desarrollo de la carrera de medicina veterinaria, el alumno de la carrera durante la enseñanza / aprendizaje, incorporara habilidades para poder desempeñarse también en los conocimientos agroalimentos manufacturados y terminados para el consumo humano, diseñando, gestionando, realizando controles de calidad, en las diferentes industria de la producción alimenticia (frigoríficos de vacunos, pollos, etc., además de la producción avícola) y fundamentalmente en la determinación higiénico sanitario. 

Uno de los objetivos principales es tener una formación de grado de alta calidad, poder trabajar en equipos multidisciplinarios, conocimientos científicos y realizar servicios de extensión universitaria.UNRN

El técnico veterinario, es el profesional calificado para colaborar en la administración de clínicas de mascotas y planteles pecuarios a pequeña y mediana escala, siempre bajo la supervisión de un veterinario o médico veterinario. Su papel es fundamental para el desarrollo de la profesión; ya que sobre ellos recae la responsabilidad de asistir y apoyar los procedimientos médicos. Para obtener este título es necesario cursar la carrera de Técnico de nivel Superior en Veterinaria con al menos 3 años de estudio.



</doc>
<doc id="2945" url="https://es.wikipedia.org/wiki?curid=2945" title="Vilvestre">
Vilvestre

Vilvestre es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Se integra dentro de la comarca de Vitigudino y la subcomarca de La Ribera (Las Arribes). Pertenece al partido judicial de Vitigudino.

El mirador del cerro del castillo, el monte gudín, la iglesia, el rollo de justicia, el molino de la Luisa y el paraje del muelle de La Barca son los lugares más destacados de este municipio del parque natural de Arribes del Duero, que a su vez forma parte de la reserva de la biosfera transfronteriza denominada como Meseta Ibérica. Está formado por un único núcleo de población, ocupa una superficie total de 46,52 km² y según datos del padrón municipal elaborado por el INE en , cuenta con una población de habitantes.

El escudo heráldico que representa al municipio fue aprobado el 11 de abril de 1997 con el siguiente blasón:

La bandera municipal fue aprobada con la siguiente descripción textual:

Vilvestre se encuentra situado en el parque natural de Arribes del Duero y al noroeste de la provincia de Salamanca. Limita al oeste con el río Duero, que actúa como frontera natural separándolo de Portugal. Dista 96 km de Salamanca capital.

Se integra dentro de la comarca de La Ribera. Pertenece a la Mancomunidad Centro Duero y al partido judicial de Vitigudino.

Su término municipal se encuentra dentro del parque natural de Arribes del Duero, un espacio natural protegido de gran atractivo turístico.

A Vilvestre se accede desde la capital charra por la carretera autonómica CL-517, de Salamanca a La Fregeneda. Llegando a Vitigudino, desviación por la carretera Provincial SA-320, de Vitigudino a Mieza hasta El Milano. Desde El Milano bien podemos desviarnos por la DSA-572, de la red secundaria de la Diputación Provincial, hasta Barruecopardo y de aquí por la DSA-575 hasta Vilvestre, o bien continuar desde El Milano, por la SA-320, hasta Cerezal de Peñahorcada y desviación por la DSA-576, de la red secundaria de la Diputación Provincial, de Cerezal a Vilvestre, recorriéndose en ambos casos una distancia total de 96 km. desde Salamanca. 


La topografía del municipio de Vilvestre podría definirse en general como suavemente ondulada en la parte norte y este del municipio, aumentando las pendientes hacia la parte sur y oeste. 

La altitud media de la zona es de 592 metros sobre el nivel del mar, destacando como punto más elevado el cerro de Homomula con 791 metros, descendiendo en el río Duero hasta los 190 metros, que es la cota del nivel máximo normal del embalse de Saucelle. 

Vilvestre tiene un clima mediterráneo continentalizado suavizado por la cercanía del río Duero y la relativa baja altitud del municipio en el contexto de la meseta norte. Se trata de un "Csa" (templado con verano seco y caluroso) según la clasificación climática de Köppen. Debido a estas condiciones climáticas se pueden encontrar naranjos, chumberas, olivos y melocotoneros.

Vilvestre pertenece a la cuenca hidrográfica del Duero, siendo éste el principal accidente hidrográfico del término municipal. Los arroyos son de escasa importancia, con un caudal muy variable después de encauzar las aguas de escorrentía y de manantiales según la estación y pluviometría, destacando el arroyo de la Nava, el arroyo de Fuentesalsa, el arroyo de Las Payitas y el arroyo de Los Lagares. Estos arroyos se suelen secar habitualmente en verano debido a la falta de precipitaciones.

Los recursos hídricos de aguas subterráneas no son demasiado relevantes y se asocian con acuíferos someros, como fuentes, pozos y charcas, que se aprovechan, tanto para el abastecimiento de la población, si bien este es complementado mediante una red de abastecimiento procedente de la presa de Almendra, como para abrevar a la ganadería en régimen extensivo. 

Con respecto al transporte de viajeros en autobús, la compañía Arribes Bus S.L. ofrece servicios por carretera entre el municipio y distintos destinos provinciales y comarcales, como por ejemplo Salamanca o Vitigudino. Los lunes, miércoles y viernes sale de Salamanca a las 13:15 h. y las 18:00 h. Los martes y jueves sólo a las 13:15 h. Desde Vilvestre sale los lunes, miércoles y viernes a las 07:30 h y las 15:00 h. Los martes y jueves sólo a las 07:30 h. Desde Vilvestre sale los martes otra línea con destino Vitigudino a las 11:00 h.

La línea de ferrocarril más próxima es la de Salamanca-Valladolid.

En lo que concierne al transporte aéreo, el aeropuerto más cercano es el de Salamanca, localizado entre las localidades de Machacón, Calvarrasa de Abajo y Villagonzalo de Tormes, a 116 kilómetros de Vilvestre. Asimismo, las otras opciones a menor distancia son los aeropuertos de Valladolid y Madrid, situados a 223 y 330 kilómetros respectivamente.

En 2011, el municipio contaba con un total de 296 vehículos de motor, de los cuales 198 son turismos, 77 camiones y 14 motocicletas. Representan 404,1 automóviles por cada 1000 habitantes. Los puntos de Inspección Técnica de Vehículos más cercanos se encuentran en la capital provincial y en Vitigudino.

Su economía está basada principalmente en la ganadería, la agricultura y la industria transformadora (Fábrica de Quesos). Además se está desarrollando también el turismo con la apertura en los últimos años de varias casas rurales además de un centro de turismo rural, y el fomento de las actividades turísticas con la construcción de nuevos miradores, el marcado de nuevas rutas de senderismo, rutas fluviales por el río Duero, y la oferta municipal de dos museos, y un albergue.

La concentración parcelaria de la zona de Vilvestre (Salamanca), fue declarada de Utilidad Pública y Urgente Ejecución por Acuerdo 18/2008, de 13 de marzo, de la Junta de Castilla y León. 
Las Bases Provisionales se publicaron con fecha 31 de marzo de 2012, y el plazo para presentar Alegaciones finalizó el día 10 de mayo de 2012.

Las Bases Definitivas fueron aprobadas por Resolución de la Dirección General de Producción Agropecuaria y Desarrollo Rural de fecha 2 de junio de 2014, sometiéndose a encuesta pública por un periodo de 30 días hábiles, espacio de tiempo que finalizó el día 2 de agosto de 2014, plazo en el que fue interpuesto un único recurso de alzada contra las mismas, el cual fue resuelto por Orden de la Consejería de Agricultura y Ganadería de la Junta de Castilla y León de fecha 4 de mayo de 2016. Las Bases Definitivas fueron declaradas firmes con fecha 5 de mayo del mismo año.

El Proyecto de Concentración Parcelaria fue aprobado por el Jefe del Servicio Territorial de Agricultura y Ganadería de Salamanca el 13 de septiembre de 2016. La exposición del Proyecto de Concentración comenzó el 14 de septiembre de 2016 con la inserción del correspondiente aviso en el tablón de anuncios del Ayuntamiento, prolongándose el plazo de presentación de alegaciones durante un plazo de 30 días hábiles, finalizando dicho período el 25 de octubre de 2016.

Actualmente se encuentra en fase de Acuerdo habiéndose publicado con fecha 15 de febrero de 2019. El plazo para presentar Recurso finalizó el día 29 de marzo de 2019.

El perímetro de la zona de concentración parcelaria afecta a una superficie de 4282 hectáreas, aportadas por 536 propietarios, en 6009 parcelas. Se han atribuido 1155 fincas de reemplazo. Esta en ejecución la obra civil de los nuevos caminos, para dar acceso a las parcelas, así como la colocación de los hitos que delimitarán el perímetro de las nuevas parcelas.

La fundación de Vilvestre se sitúa en el siglo XII, dentro del proceso repoblador emprendido en la zona por los reyes de León, siendo probablemente repoblado con gentes provenientes de Vilviestre del Pinar, en la Tierra de Lara, actual provincia de Burgos, según sostiene el medievalista Ángel Barrios. En la provincia de Soria, en el actual municipio de Royo, encontramos otro Vilviestre, éste y el anterior de la cercana Tierra de Lara, se encuentran en el área del Sistema Ibérico, en torno a la Sierra de la Demanda y los Picos de Urbión. Asimismo, en el alfoz de la ciudad de Burgos se ubica un tercer Vilviestre (de Muñó), que forma parte del municipio de Estépar.

Sea como fuere su repoblación, lo que si está documentado es la donación de Vilvestre con su término al Arzobispado de Santiago en 1192 por parte del rey Alfonso IX de León, pasando a pertenecer la localidad a la diócesis de León de Santiago, la cual agrupaba los territorios leoneses de dicha Orden, no pasando a pertenecer a la diócesis de Salamanca hasta el siglo XVI.

Por otro lado, la zona de Las Arribes del Duero fue fortificada entre la segunda mitad del siglo XII y principios del XIII una vez que Portugal se independizó del Reino de León en 1143, siendo Vilvestre una de las localidades fortificadas para defender la frontera leonesa. Asimismo, junto al de Vilvestre se erigieron otros castillos y fortalezas en Barruecopardo (anterior a 1212), Mieza, Aldeadávila (se conserva la mayor parte de la torre del Homenaje, aunque recrecida), Masueco y Pereña, en algunos casos reutilizadas parte de ellos en las iglesias parroquiales. Parte de ellos pasaron posteriormente al infantado de don Pedro de Molina.

Posteriormente, en el siglo XV, Vilvestre, como la mayor parte del territorio histórico leonés, tomó partido por Juana la Beltraneja, en la guerra civil abierta entre ésta e Isabel la Católica por los tronos de León y de Castilla, estando defendido su castillo por las tropas juanistas hasta la derrota de dicho bando, siendo en el siglo XVII arruinado el castillo de Vilvestre en el ataque que las tropas portuguesas hicieron a la localidad dentro de la Guerra de Restauración portuguesa (1640-1668).

En otro orden de cosas, la expulsión de los judíos dictada por los Reyes Católicos tuvo en estas tierras una de sus fases señeras: "Igualmente, los justicias de Ledesma recibieron orden de ir a Pereña, o donde fuera necesario, para prender a Pedro de Miranda, que era pasador de los judíos fuera de los caminos señalados, y a García de Ledesma, y a Pedro Herrero, quienes habían intentado matar a Alonso de Sejas, encargado de que se cumpliera la salida de los judíos por los caminos estipulados para tal cosa en el término de la mencionada villa -Ledesma- y en Vilvestre". Y la Reina Isabel la Católica mandó detenerle y ajusticiarle en 1481, y nos dice el cronista Pulgar: "Cuarenta y seis fortalezas fueron derrribadas entonces, y veinte más tarde: ajusticiados como principales malhechores Pedro de Miranda y el mariscal Pero Pardo...". Hechos que prueban la existencia de una comunidad importante de judíos en Vilvestre.

Durante el pontificado de Clemente VII se resolvió a favor de la sede salmantina la posesión de las villas de Vilvestre, Yecla de Yeltes, Vitigudino y Palacios del Arzobispo; siendo D. Francisco de Bobadilla (1510-1529) el primer obispo que las poseyó.

Vilvestre siempre ha estado vinculado al Duero. El Catastro de la Ensenada lo sitúa a poniente de este término y Madoz (Diccionario Geográfico, Estadístico e Histórico) explica cómo los naturales mediado el siglo pasado, lo atravesaban en barca. Y a este río aluden las cuatro bandas azules colocadas en punta. Dado el microclima de este término municipal, siempre han abundado los árboles frutales, dedicando gran parte del territorio al cultivo de olivos. Hay una zona bastante extensa conocida por “Los Olivares”. Por ello existe un olivo en el escudo.

Este pueblo, según afirma el Padre Morán (reseña Histórico-artística de la provincia de Salamanca), juntamente con Yecla, Moronta y Palacios de Arzobispo, Perteneció al Arzobispado Compostelano. Este es el motivo de que aparezca la Cruz-Espada de Santiago en su escudo. Fue famoso por su castillo fronterizo con Portugal, y parece que fue gobernado por Ruy Gómez de Silva, príncipe de Éboli. La respuesta 23 de la generales del catastro de ensenada dice textualmente "que esta Villa y su concejo goza en calidad de propios un fuerte arruinado que llaman el castillo" (fecha de 28 de junio de 1752). En el siglo XVIII era ya una defensa inservible, seguramente quedaría fuera de combate en la Guerra de Secesión. Un dicho popular, coetáneo al tiempo de la fortificación, cuando estaban levantados muros y cortinas con piedra rosada. Lo más seguro es que la construcción fuera de líneas abaluartadas y una estancia estuviera destinada a capilla de la Virgen. Nuestra Señora del Castillo, que en 1757 tuvo ermita propia, seguramente procediera de la fortaleza.

Con la creación de las actuales provincias en 1833, Vilvestre quedó encuadrado en la provincia de Salamanca, dentro de la Región Leonesa.


 

Los «miradores del Castillo» son un conjunto de dos miradores situados en el entorno de la Ermita de Nuestra Señora del Castillo. Se sitúan en lo alto del cerro desde el que se vislumbra todo Vilvestre, cerca de los restos del antiguo castillo de la localidad. Se ha habilitado una barrera de seguridad para observar las arribes del Duero y dispone de un punto de información sobre los distintos puntos de interés turístico que hay en esa zona. En el cerro se encuentra la ermita, los restos del castillo que todavía se conservan, un conjunto rupestre y un palomar que lo corona.

El «mirador de El Reventón de La Barca» es el más moderno y equipado. Se encuentra bajando por el puerto hacia el paraje de La Barca desde Vilvestre, en punto kilométrico 1,5 de la carretera VLV-424. El mirador está situado en el interior de una curva de 180 grados más empinada que el resto de tramos de la carretera en este lugar. En él se ha habilitado una zona con asientos en la que se puede observar el paisaje y en la que se han colocado unos murales explicativos del entorno. Es completamente accesible en coche y para personas en sillas de ruedas.

El «mirador del Monte Gudín» se encuentra en lo alto del cerro del mismo nombre, desde el que se obtiene una amplia vista del Duero. No dispone de murales ni de ningún punto de información. Para acceder a él hay que seguir por el camino indicado que sale desde el frontón de pelota. En ocasiones está cerrado el acceso en coche porque el mirador está una finca comunal privada. Cuando esto ocurre hay que aparcar el coche a la entrada de la finca del Monte Gudín situado a 0,7 km del mirador y continuar andando. Si el acceso a la finca estuviese abierto se continuaría por el camino hasta las casas del monte. A partir de ahí, en esos últimos metros, no se puede acceder a él en ningún vehículo, por lo que el último tramo se debe hacer a pie.

En el municipio de Vilvestre existen varias rutas de senderismo marcadas para que aquellas personas que lo deseen puedan realizar senderismo por ellas.

Además el Ayuntamiento de Vilvestre organiza anualmente la Marcha de Senderismo Arribes del Duero el segundo domingo de marzo para dar a conocer algunos rincones que habitualmente no son recorridos por los senderos ya prefijados.

Esta estructura de la población se debe a la emigración que tuvo lugar desde finales del siglo XIX, cuando el número de habitantes alcanzó su máximo histórico. A partir de entonces la población empezó a desplazarse mayoritariamente a América y, más tarde a Europa y a otras regiones de España y de la provincia de Salamanca. El proceso se aceleró a partir de los años 60 debido a la baja rentabilidad de las explotaciones agrícolas en una economía moderna y la escasez de alternativas en la zona, entre otras por el cierre de la mina de wolframio de Barruecopardo. La emigración no solo causó un importante despoblamiento, sino también el envejecimiento de la población tal y como se aprecia en la pirámide de población.
Además se está produciendo una gran despoblación, con una pérdida del 28,93% de la población durante el siglo XXI; que a diferencia de lo ocurrido entre los años 1960 y 1981, en los que se produjo un descenso histórico de más del 50% de la población por la emigración, en este caso la bajada de población se debe a la mortalidad, principalmente causada por el envejecimiento de la población.

De los datos de la pirámide de población de 2015 se pueden observar que la población menor de 20 años es el 4,92 % del total, la comprendida entre 20-40 años es el 17,23 %, la comprendida entre 40-60 años es el 23,27 % y la mayor de 60 años es el 54,59 %.
La actual Corporación Municipal de Vilvestre fue elegida en las elecciones municipales celebradas el 24 de mayo de 2015, tomando posesión de sus cargos el día 13 de junio de 2015, y está dirigida por segunda legislatura consecutiva por D.Manuel Domínguez Hernández, miembro del Partido Popular que gobierna con mayoría absoluta, al contar con cuatro de los 7 miembros de la Corporación. Los otros tres concejales pertenecen a Ciudadanos.

Los anteriores alcaldes desde 1979 fueron Casimiro Hernández Calvo (1979-1995) y José Manuel Guarido Mateos (1995-2007).

El alcalde de Vilvestre no recibe ningún tipo de prestación económica por su trabajo al frente del ayuntamiento (2017).








</doc>
<doc id="2947" url="https://es.wikipedia.org/wiki?curid=2947" title="VRML">
VRML

El lenguaje de modelado de realidad virtual o VRML (sigla del inglés "Virtual Reality Modeling Language") es un formato de archivo normalizado que tiene como objetivo la representación de escenas u objetos interactivos tridimensionales diseñado particularmente para web. Se usa por medio de comandos en inglés, los cuales agregan y determinan las características.

El lenguaje VRML posibilita la descripción de una escena compuesta por objetos 3D a partir de prototipos basados en formas geométricas básicas o de estructuras en las que se especifican los vértices y las aristas de cada polígono tridimensional y el color de su superficie. VRML permite también definir objetos 3D multimedia, a los cuales se puede asociar un enlace de manera que el usuario pueda acceder a una página web, imágenes, vídeos u otro fichero VRML de Internet cada vez que haga clic en el componente gráfico en cuestión.

El Consorcio Web3D fue creado para desarrollar este formato. Su primera especificación fue publicada en 1995; la versión actual funcionalmente completa es la VRML 97 (ISO/IEC DIS 14772-1). 

VRML es la base en la que se ha desarrollado X3D (Extensible 3D Graphics).



</doc>
<doc id="2951" url="https://es.wikipedia.org/wiki?curid=2951" title="Wiki">
Wiki

El término wiki (palabra que proviene del hawaiano "wiki", «rápido») alude al nombre que recibe una comunidad virtual, cuyas páginas son editadas directamente desde el navegador, donde los mismos usuarios crean, modifican, corrigen o eliminan contenidos que, habitualmente, comparten. No tiene por qué ser necesariamente un sitio en la web, puesto que hay wikis instalables para uso en el escritorio de un computador personal o que pueden portarse en un llavero USB que lleven un entorno LAMP como, por ejemplo, XAMPP.

Los textos o «páginas "wiki"» tienen títulos únicos. Si se escribe el título de una página "wiki" en algún sitio de la "wiki" entre dobles corchetes ("Título de la página"<nowiki>), esta palabra se convierte en un «enlace web» a la página correspondiente. De este modo, en una página sobre «alpinismo» puede haber una palabra como «piolet» o «brújula» que esté marcada como palabra perteneciente a un título de página </nowiki>"wiki". La mayor parte de las implementaciones de "wikis" indican en el localizador de recursos uniforme (URL) de la página el propio título de la página "wiki" (en Wikipedia, ocurre así: «nowiki>https://es.wikipedia.org/wiki/Alpinismo</nowiki» es el URL de la página "wiki" "Alpinismo"), lo que facilita el uso y la aplicación general del enlace fuera del propio sitio web. Además, esto permite formar en muchas ocasiones una coherencia terminológica, y genera una ordenación "natural" del contenido.

Su principal tarea, a la que le debe su fama hasta el momento, ha sido la creación de enciclopedias colectivas, género al que pertenece Wikipedia. Existen muchas otras aplicaciones más cercanas a la coordinación de informaciones y acciones, o la puesta en común de conocimientos o textos dentro de grupos. La mayor parte de las "wikis" actuales conservan un historial de cambios que permite recuperar fácilmente cualquier estado anterior y ver qué usuario hizo cada cambio, lo cual facilita el mantenimiento conjunto y el control de usuarios nocivos. Habitualmente, sin necesidad de una revisión previa, se actualiza el contenido que muestra la página "wiki" editada.

La primera WikiWikiWeb fue creada por Ward Cunningham, quien inventó y dio nombre al concepto "wiki", y produjo la primera implementación de un servidor WikiWiki para el repositorio de patrones del Portland (Portland Pattern Repository) en 1995. En palabras del propio Cunningham, una "wiki" es «la base de datos en línea más simple que pueda funcionar» ("the simplest online database that could possibly work"). 

En enero de 2001, los fundadores del proyecto de enciclopedia Nupedia, Jimmy Wales y Larry Sanger, decidieron utilizar un "wiki" como base para el proyecto de enciclopedia Wikipedia. Originalmente se usó el software UseMod, pero luego crearon un software propio, MediaWiki, que ha sido adoptado después por muchos otros wikis.

La "wiki" más grande que existe es la versión en inglés de Wikipedia, seguida por varias otras versiones del proyecto. Las "wikis" que no pertenecen a Wikipedia son mucho más pequeños y con menor participación de usuarios, generalmente debido al hecho de ser mucho más especializadas. Es muy frecuente, por ejemplo, la creación de "wikis" para proveer de documentación a programas informáticos, especialmente los desarrollados en software libre.

Según su creador una "wiki" es "la base de datos en línea más simple que pueda funcionar". Se trata de un tipo de página web que brinda la posibilidad de que multitud de usuarios puedan editar sus contenidos a través del navegador web, con ciertas restricciones mínimas. De esta forma permite que múltiples autores puedan crear, modificar o eliminar los contenidos. Se puede identificar a cada usuario que realiza un cambio y recuperar los contenidos modificados, volviendo a un estado anterior. 
Estas características facilitan el trabajo en colaboración así como la coordinación de acciones e intercambio de información sin necesidad de estar presentes físicamente ni conectados de forma simultánea. El ejemplo más conocido y de mayor tamaño de este tipo de páginas web es la enciclopedia colaborativa Wikipedia (www.wikipedia.org).
A favor: Es una fuente de información y bibliográfica de construcción colectiva.
Problemas: La información publicada puede provenir de fuentes erróneas o no válidas. 
Solución/recomendaciones: Es recomendable trabajar criterios sobre el empleo de fuentes de información confiables y formas de validar los contenidos.

Una "wiki" permite que se escriban artículos colectivamente (coautoría) por medio de un lenguaje de wikitexto editado mediante un navegador. Una página "wiki" singular es llamada «página "wiki"», mientras que el conjunto de páginas (normalmente interconectadas mediante hipervínculos) es «la "wiki"». Es mucho más sencillo y fácil de usar que una base de datos.

Una característica que define la tecnología "wiki" es la facilidad con que las páginas pueden ser creadas y actualizadas. En general no hace falta revisión para que los cambios sean aceptados. La mayoría de las "wikis" están abiertos al público sin la necesidad de registrar una cuenta de usuario. A veces se requiere conectarse para obtener una cookie de «wiki-firma», para autofirmar las ediciones propias. Otras "wikis" más privadas requieren autenticación de usuario. 

Por lo explicado, las "wikis" son una muy buena opción pedagógica para realizar actividades educativas, ya que como explica Mariana Maggio, se pueden generar propuestas que los alumnos puedan integrar en las "wikis" a partir de la reconstrucción de las mismas en un sentido didáctico. En la actualidad los documentos Web, como lo es el ejemplo de las "wikis", crean tendencias y cuando estas configuran los usos de los niños y los jóvenes, es importante que los educadores las reconozcan y se preocupen por entenderlas a partir de su exploración.

Para Maggio, «un proyecto didáctico maravilloso puede ser, cuando el tema lo justifique, generar contenidos para Wikipedia o revisar los publicados allí: entender el tema de un modo profundo, verificar los contenidos, transparentar y discutir los criterios, ampliar lo publicado, ofrecer versiones y especificaciones de alto valor local».

Una "wiki" también puede ser un espacio usado para seguimiento individual de los alumnos, donde ellos puedan crear sus proyectos independientemente y el profesor pueda intervenir guiando y corrigiendo. Se ha utilizado también en procesos de formación docente ayudando al mejoramiento de sus habilidades tecnológicas, pero también los procesos de colaboración entre pares.
Permite la creación colectiva de documentos en un lenguaje simple de marcas utilizando un navegador web.
Generalmente no se hacen revisiones previas antes de aceptar las modificaciones y la mayoría de las "wikis" están abiertas.
Permite a los participantes trabajar juntos en páginas web, para añadir o modificar su contenido.
Las versiones antiguas nunca se eliminan y pueden restaurarse.
Se puede seleccionar diferentes tipos de "wiki", profesor, grupo, alumno.

En una "wiki" tradicional, existen tres representaciones por cada página:


El código fuente es potenciado mediante un lenguaje de marcado simplificado para hacer varias convenciones visuales y estructurales. Por ejemplo, el uso del asterisco «*» al empezar una línea de texto significa que se generará una lista desordenada de elementos ("bullet-list"). El estilo y la sintaxis pueden variar en función de la implementación, alguno de las cuales también permite etiquetas HTML.

La razón de este diseño es que el HTML, con muchas de sus etiquetas crípticas, es difícil de leer para usuarios no habituados a la tecnología. Hacer visibles las etiquetas de HTML provoca que el texto en sí sea difícil de leer y editar para la mayoría de usuarios. Por lo tanto, se promueve el uso de edición en texto llano con convenciones para la estructura y el estilo fáciles de comprender.

A veces es deseable que los usuarios no puedan usar ciertas funcionalidades que el HTML permite, tales como JavaScript, CSS y XML. Se consigue consistencia en la visualización, así como seguridad adicional para el usuario. En muchas inserciones de "wiki", un hipervínculo es exactamente tal como se muestra, al contrario de lo que ocurre en el HTML.

Durante años el estándar de facto fue la sintaxis del WikiWikiWeb original. Actualmente las instrucciones de formateo son diferentes dependiendo del motor de la "wiki". Las "wikis" simples permiten solo formateo de texto básico, mientras que otros más complejos tienen soporte para cuadros, imágenes, fórmulas e incluso otros elementos más interactivos tales como encuestas y juegos. Debido a la dificultad de usar varias sintaxis, se están haciendo esfuerzos para definir un estándar de marcado (ver esfuerzos de Meatball y Tikiwiki).

Los wikis son un auténtico medio de hipertexto, con estructuras de navegación no lineal. Cada página contiene un gran número de vínculos a otras páginas. En grandes "wikis" existen las páginas de navegación jerárquica, normalmente como consecuencia del proceso de creación original, pero no es necesario usarlas. Los vínculos se usan con una sintaxis específica, el «patrón de vínculos».

Originalmente, la mayoría de las "wikis" usaban CamelCase como patrón de vínculos, poniendo frases sin espacios y poniendo la primera letra de cada palabra en mayúscula (por ejemplo, la palabra «CamelCase»). Este método es muy fácil, pero hace que los hiperenlaces se escriban de una manera que se desvía de la escritura estándar. Las "wikis" basadas en CamelCase se distinguen instantáneamente por los enlaces con nombres como: «TablaDeContenidos», «PreguntasFrecuentes». Por consiguiente, comenzaron a desarrollarse otras soluciones.

Los «vínculos libres», usados por primera vez por CLiki, usan un formato tipo _(vínculo). Por ejemplo, _(Tabla de contenidos), _(Preguntas frecuentes). Otros motores de "wiki" usan distintos signos de puntuación.

Interwiki permite vínculos entre distintas comunidades "wiki". Las nuevas páginas se crean simplemente creando un vínculo apropiado. Si el vínculo no existe, se acostumbra a destacar como «vínculo roto». Siguiendo el vínculo se abre una página de edición, que permite al usuario introducir el texto para la nueva página "wiki". Este mecanismo asegura que casi no se generen páginas huérfanas (es decir, páginas que no tienen ningún vínculo apuntando a ellas). Además se mantiene un nivel alto de conectividad.

La mayoría de las "wikis" permite al menos una búsqueda por títulos, a veces incluso una búsqueda por texto completo. La escalabilidad de la búsqueda depende totalmente del hecho de que el motor de la "wiki" disponga de una base de datos o no: es necesario el acceso a una base de datos indexada para hacer búsquedas rápidas en "wikis" grandes. En Wikipedia, el botón «Ir» permite a los lectores ir directamente a una página que concuerde con los criterios de búsqueda. El motor de MetaWiki se creó para habilitar búsquedas en múltiples "wikis".

Las "wikis" suelen diseñarse con la filosofía de aumentar la facilidad de corrección de los errores, y no la de reducir la dificultad de cometerlos. Las "wikis" son muy abiertas, pero incluso así proporcionan maneras de verificar la validez de los últimos cambios al contenido de las páginas. En casi todos las "wikis" hay una página específica, «Cambios recientes», que enumera las ediciones más recientes de artículos, o una lista con los cambios hechos durante un período. Algunas "wikis" pueden filtrar la lista para deshacer cambios hechos por vandalismo.

Desde el registro de cambios suele haber otras funciones: el «Historial de revisión» muestra versiones anteriores de la página, y la característica «diff» destaca los cambios entre dos revisiones. Usando el historial, un editor puede ver y restaurar una versión anterior del artículo, y la característica «diff» se puede usar para decidir cuándo eso es necesario. Un usuario normal de la "wiki" puede ver el «diff» de una edición listada en «Cambios recientes» y, si es una edición inaceptable, consultar el historial y restaurar una versión anterior. Este proceso es más o menos complicado, según el software que use la "wiki".

En caso de que las ediciones inaceptables se pasen por alto en «Cambios recientes», algunos motores de "wiki" proporcionan control de contenido adicional. Se pueden monitorizar para asegurar que una página o un conjunto de páginas mantienen la calidad. A un usuario dispuesto a mantener esas páginas se le avisará en caso de modificaciones, y así se le permitirá verificar rápidamente la validez de las nuevas ediciones.

Consiste en realizar ediciones (generalmente hechas por desconocidos o gente mal intencionada) que borran contenido importante, introducen errores, agregan contenido inapropiado u ofensivo (por ejemplo, insultos) o simplemente incumplen flagrantemente las normas de la "wiki". También son frecuentes los intentos de spam, por ejemplo:


Algunas soluciones que se utilizan para luchar contra el vandalismo son:


Existen varios programas, generalmente scripts de servidor en Perl o PHP, que implementan una "wiki". Con frecuencia, suelen utilizar una base de datos, como MySQL. 

Suelen distinguirse por:

Algunos de los más utilizados son:





</doc>
<doc id="2968" url="https://es.wikipedia.org/wiki?curid=2968" title="Red de área amplia">
Red de área amplia

Una red de área amplia, o WAN ("Wide Area Network" en inglés), es una red de computadoras que une varias redes locales, aunque sus miembros no estén todos en una misma ubicación física. 
Muchas WAN son construidas por organizaciones o empresas para su uso privado, otras son instaladas por los proveedores de Internet (ISP) para proveer conexión a sus clientes. 

Hoy en día, internet brinda conexiones de alta velocidad, de manera que un alto porcentaje de las redes WAN se basan en ese medio, reduciendo la necesidad de redes privadas WAN, mientras que las redes privadas virtuales que utilizan cifrado y otras técnicas para generar una red dedicada sobre comunicaciones en Internet, aumentan continuamente.

Las WAN no necesariamente tienen que estar conectadas a las LAN. Por ejemplo, puede tener un esqueleto localizado de una tecnología WAN, que conecta diferentes LANs dentro de un campus. Esta podría ser la de facilitar las aplicaciones de ancho de banda más altas, o proporcionar una mejor funcionalidad para los usuarios. 

Las WAN se utilizan para conectar redes LAN y otros tipos de redes. Así los usuarios se pueden comunicar con los usuarios y equipos de otros lugares. Muchas WAN son construidas por una organización en particular y son privadas. Otras, construidas por los proveedores de servicios de Internet, que proporcionan conexiones LAN a una organización de Internet. WAN a menudo se construyen utilizando líneas arrendadas. En cada extremo de la línea arrendada, un enrutador conecta la LAN en un lado con un segundo enrutador dentro de la LAN en el otro. Las líneas arrendadas pueden ser muy costosas. En lugar de utilizar líneas arrendadas, WAN también se puede construir utilizando métodos menos costosos de conmutación de circuitos o conmutación de paquetes. La red de protocolos incluyendo TCP/IP tiene la función de entrega de transporte y funciones de direccionamiento. Los protocolos, incluyendo paquetes como SONET/SDH, MPLS, ATM y Frame Relay son utilizados a menudo por los proveedores de servicios que ofrecen los vínculos que se usan en redes WAN. X.25 fue pronto un protocolo WAN importante, y es a menudo considerado como el "abuelo" de Frame Relay ya que muchos de los protocolos subyacentes y funciones de X.25 todavía están en uso hoy en día (con actualizaciones) por Frame Relay. 

La investigación académica en redes de área amplia puede ser dividido en tres áreas: modelos matemáticos, emulación de red y de simulación de red. 

Mejoras en el rendimiento a veces se entregan a través de los servicios de archivos de área extensa o por servicios de optimización de la WAN.

Hay varias opciones disponibles para la conectividad WAN:

Las tasas de transmisión han aumentado con el tiempo, y seguirán aumentando. Alrededor de 1960 a 110 bits/s (bits por segundo) de la línea fue normal en el borde de la WAN, mientras que los enlaces centrales de 56 kbit/s a 64 kbit/s se consideraron "rápida". En este momento (2016) los hogares están conectados a Internet con ADSL o Fibra óptica a velocidades que van desde 1 Mbit/s hasta 600 Mbit/s, y las conexiones en el núcleo de una WAN puede variar de 1 Gbit/s de 300 Gbit/s.

Recientemente, con la proliferación del bajo coste de conexión a Internet muchas empresas y organizaciones han recurrido a las VPN para interconectar sus redes, creando una red WAN de esa manera. Empresas como Citrix, Cisco, New Edge Networks y Check Point ofrecen soluciones para crear redes VPN.




Existen varios tipos de red WAN, y tres de ellos se agrupan bajo la clasificación de red conmutada (en física, la conmutación consiste en el cambio del destino de una señal o de una corriente eléctrica):

Por circuitos

Son redes de marcación de (dial-up), como la red de telefonía básica (RTB) y RDSI. Durante el tiempo que dura la llamada, el ancho de banda es dedicado.

Por mensaje

Sus conmutadores suelen ser ordenadores que cumplen la tarea de aceptar el tráfico de cada terminal que se encuentre conectado a ellas. Dichos equipos evalúan la dirección que se encuentra en la cabecera de los mensajes y pueden almacenarla para utilizarla más adelante. Cabe mencionar que también es posible borrar, redirigir y responder los mensajes en forma automática.

Por paquetes

Se fracciona cada mensaje enviado por los usuarios y se transforman en un número de pequeñas partes denominadas paquetes,que se vuelven a unir una vez llegan al equipo de destino, para reconstruir los datos iniciales. Dichos paquetes se mueven por la red independientemente, y esto repercute positivamente en el tráfico, además de facilitar la corrección de errores, ya que en caso de fallos solo se deberán reenviar las partes afectadas. El ancho de banda es compartido entre todos los usuarios que usan la red.

Topologías de los enrutadores en una red de área amplia:


otros bancos e incluso con bancos en el extranjero. Cada una de estas redes es una WAN que
permite a un usuario extraer dinero en una ATM del otro lado del país, o incluso en un país
diferente.
mantienen a sus trabajadores comunicados mediante una WAN exclusiva de la empresa, de
modo que puedan intercambiar información y mantenerse en permanente contacto a pesar de
estar en países diferentes.
tecnológicos a lo largo de distancias enormes, incluso de un lado del mundo al otro. 




</doc>
<doc id="2970" url="https://es.wikipedia.org/wiki?curid=2970" title="Wangenheimia lima">
Wangenheimia lima

Wangenheimia lima es la única especie de planta herbácea perteneciente a la familia de las poáceas que contiene el género monotípico Wangenheimia. Es originaria de la región del Mediterráneo meridional.
Es una pequeña gramínea anual de hojas finas y acintadas. El tallo es corto y rígido y está terminado en una única espiga terminal corta y unilateral (aspecto pectinado) con espiguillas más o menos sésiles.
Es originaria de la región del Mediterráneo meridional. En España se distribuye por Alicante, Castellón, Lérida, Tarragona y Valencia, donde se encuentra en terrenos baldíos y pastizales secos muy transitados en áreas de montaña. 
"Wangenheimia lima" fue descrita por (L.) Trin. y publicado en "Fundamenta Agrostographiae" 132. 1820. 



</doc>
<doc id="2971" url="https://es.wikipedia.org/wiki?curid=2971" title="Web (desambiguación)">
Web (desambiguación)

La palabra web (del inglés: red, malla, telaraña, entramado) puede referirse a:



</doc>
<doc id="2977" url="https://es.wikipedia.org/wiki?curid=2977" title="WWW (desambiguación)">
WWW (desambiguación)

El término WWW, una sigla, puede referirse, en esta enciclopedia:


</doc>
<doc id="2984" url="https://es.wikipedia.org/wiki?curid=2984" title="Weber (unidad)">
Weber (unidad)

El weber (símbolo: Wb) es la unidad de flujo magnético o flujo de inducción magnética en el Sistema Internacional de Unidades equivalente al flujo magnético que al atravesar un circuito de una sola espira produce en la misma una fuerza electromotriz de 1 voltio si se anula dicho flujo en 1 segundo por decrecimiento uniforme. Es representado simbólicamente por Wb. El nombre de esta unidad fue dado en honor al físico alemán Wilhelm Eduard Weber.

Su equivalente en el Sistema Cegesimal de Unidades (CGS) es el máxwell. 1 máxwell = 10 Wb.

A continuación una tabla de los múltiplos y submúltiplos del Sistema Internacional de Unidades:



</doc>
<doc id="2987" url="https://es.wikipedia.org/wiki?curid=2987" title="XHTML">
XHTML

XHTML ("eXtensible HyperText Markup Language") es, básicamente, HTML expresado como XML válido. Es más estricto a nivel técnico, pero esto permite que posteriormente sea más fácil al hacer cambios o buscar errores entre otros. En su versión 1.0, XHTML es solamente la versión XML de HTML, por lo que tiene, básicamente, las mismas funcionalidades, pero cumple las especificaciones, más estrictas, de XML. Su objetivo es avanzar en el proyecto del World Wide Web Consortium de lograr una web semántica, donde la información, y la forma de presentarla estén claramente separadas. La versión 1.1 es similar, pero parte a la especificación en módulos. En sucesivas versiones la W3C planea romper con los tags clásicos traídos de HTML.

Las principales ventajas del XHTML sobre el HTML son:

El estándar XHTML indica en un apéndice informativo una manera de escribir XHTML de modo tal que los navegadores actuales que sólo entienden HTML, lo procesen como si fuera este. Para esto se deberá crear un documento con algunas restricciones y consideraciones, y servirlo con el «content-type» text/html, en vez del correcto para XHTML.

Algunas de las reglas propuestas para que XHTML «parezca» HTML son:


Para algunos autores, la inclusión de este apéndice en el estándar fue un error y consideran que es un error usar XHTML de esta manera.

La siguiente lista muestra algunas reglas de XHTML 1.0 que lo diferencian de HTML 4.01. Muchas de estas diferencias vienen con el cambio de ser una aplicación SGML a ser una aplicación del más estricto XML:





</doc>
<doc id="2988" url="https://es.wikipedia.org/wiki?curid=2988" title="XPath">
XPath

XPath ("XML Path Language") es un lenguaje que permite construir expresiones que recorren y procesan un documento XML. La idea es parecida a las expresiones regulares para seleccionar partes de un texto sin atributos ("plain text"). XPath permite buscar y seleccionar teniendo en cuenta la estructura jerárquica del XML. XPath fue creado para su uso en el estándar XSLT, en el que se usa para seleccionar y examinar la estructura del documento de entrada de la transformación. XPath fue definido por el consorcio W3C. 

Todo el procesamiento realizado con un fichero XML está basado en la posibilidad de direccionar o acceder a cada una de las partes que lo componen, de modo que podamos tratar cada uno de los elementos de forma diferenciada. 

El tratamiento del fichero XML comienza por la localización del mismo a lo largo del conjunto de documentos existentes en el mundo. Para llevar a cabo esta localización de forma unívoca, se utilizan los URI (Uniform Resource Identifiers), de los cuales los URL (Uniform Resource Locators) son sin duda los más conocidos. 

Una vez localizado el documento XML, la forma de seleccionar información dentro de él es mediante el uso de XPath, que es la abreviación de lo que se conoce como XML Path Language. Con XPath podremos seleccionar y hacer referencia a texto, elementos, atributos y cualquier otra información contenida dentro de un fichero 
XML.

XPath en sí es un lenguaje sofisticado y complejo, pero distinto de los lenguajes procedurales que solemos usar (C, C++, Basic, Java...). Además, como casi todo en el mundo de XML, aún está en estado de desarrollo, por lo que no es fácil encontrar herramientas que incorporen todas sus funcionalidades. 

XPath es a su vez la base sobre la que se han especificado nuevas herramientas que aprovechan para el tratamiento de documentos XML. Herramientas tales como XPointer, XLink y XQuery (el lenguaje que maneja los documentos XML como si de una base de datos se tratase). Así, XPath sirve para decir cómo una hoja de estilo debe procesar el contenido de una página XML, pero también para poder poner enlaces o cargar en un navegador zonas determinadas de una página XML, en vez de toda la página.

Un documento XML es procesado por un analizador (o parser) construyendo un árbol de nodos. Este árbol comienza con un elemento raíz, que se diversifica a lo largo de los elementos que cuelgan de él y acaba en nodos hoja, que contienen solo texto, comentarios, instrucciones de proceso o incluso que están vacíos y solo tienen atributos.

La forma en que XPath selecciona partes del documento XML se basa precisamente en la representación arbórea que se genera del documento. De hecho, los "operadores" de que consta este lenguaje nos recordarán la terminología que se utiliza a la hora de hablar de árboles en informática: raíz, hijo, ancestro, descendiente, etc. 

Un caso especial de nodo son los nodos atributo. Un nodo puede tener tantos atributos como desee, y para cada uno se le creará un nodo atributo. No obstante, dichos nodos atributo NO se consideran como hijos suyos, sino más bien como etiquetas añadidas al nodo elemento. 

A continuación se muestra un ejemplo de cómo se convierte en árbol un documento XML. Este mismo ejemplo será usado a lo largo de todo el tutorial. En primer lugar se muestra el documento XML y a continuación el árbol que genera. 

Documento XML :
Árbol generado :

Existen distintos tipos de nodos en un árbol a partir de un documento XML, a saber: raíz, elemento, atributo, texto, comentario e instrucción de procesamiento (respectivamente; root, elements, attribute, text, comment y processing instruction). Todo esto es muy beneficioso.

Se identifica por /. No se debe confundir el nodo raíz con el elemento raíz del documento. Así, si el documento XML de nuestro ejemplo tiene por elemento raíz a libro, éste será el primer nodo que cuelgue del nodo raíz del árbol, el cual es: /. 

Insisto: / hace referencia al nodo raíz del árbol, pero no al elemento raíz del documento XML, por más que un documento XML solo pueda tener un elemento raíz. De hecho, podemos afirmar que el nodo raíz del árbol contiene al elemento raíz del documento. 

Cualquier elemento de un documento XML se convierte en un nodo elemento dentro del árbol. Cada elemento tiene su nodo padre. El nodo padre de cualquier elemento es, a su vez, un elemento, excepto el elemento raíz, cuyo padre es el nodo raíz. Los nodos elemento tienen a su vez hijos, que son: nodos elemento, nodos texto, nodos comentario y nodos de instrucciones de proceso. Los nodos elemento también tienen propiedades tales como su nombre, sus atributos e información sobre los "espacios de nombre" que tiene activos. 

Una propiedad interesante de los nodos elemento es que pueden tener identificadores únicos (para ello deben ir acompañados de un DTD que especifique que dichos atributos toman valores únicos), esto permite referenciar a dichos elementos de una forma mucho más directa.

Por texto vamos a hacer referencia a todos los caracteres del documento que no están marcados con alguna etiqueta. Un nodo texto no tiene hijos, es decir, los distintos caracteres que lo forman no se consideran hijos suyos.

Como ya hemos indicado, los nodos atributo no son tanto hijos del nodo elemento que los contiene como etiquetas añadidas a dicho nodo elemento. Cada nodo atributo consta de un nombre, un valor (que es siempre una cadena) y un posible "espacio de nombres". 

Aquellos atributos que tienen por valor el valor por defecto asignado en el DTD se tratarán como si el valor se les hubiese asignado al escribir el documento XML. Al contrario, no se crea nodo para atributos no especificados en el documento XML, y con la propiedad "#IMPLIED" definida en su DTD. Tampoco se crean nodos atributo para las definiciones de los espacios de nombre. Todo esto es normal si tenemos en cuenta que no es necesario tener un DTD para procesar un documento XML.

Aparte de los nodos indicados, en el árbol también se generan nodos para cada nodo con comentarios y con instrucciones de proceso. Al contenido de estos nodos se puede acceder con la propiedad "string-value".



</doc>
<doc id="2989" url="https://es.wikipedia.org/wiki?curid=2989" title="Yellow Dog Linux">
Yellow Dog Linux

Yellow Dog Linux (abreviado, "YDL") es una distribución Linux creada para tener soporte para el procesador PowerPC. Se lanzó por primera vez en 1999 para el Apple Macintosh Inc. Yellow Dog Linux es un producto de Terra Soft Solutions, Colorado (EE. UU.), una compañía especializada en software basado en Linux para la arquitectura Power, y está basada en CentOS y Fedora.

El 11 de noviembre de 2008 Fixstars compra Terrasoft, haciéndose con todos los derechos de "Yellow Dog Linux".

Yellow Dog Linux está principalmente dirigida a los ordenadores Macintosh de Apple, IBM BladeCenter JS2x y servidores Series P5, Mercury XR9, PlayStation 3, y varios otros sistemas y plataformas en torno a la arquitectura Power PC, como los comercializados por la propia Fixstars (como el YDL PowerStation).

La última versión estable es la 6.2 del 29 de junio de 2009 y posiblemente sea la última para PowerPC. La anterior fue la 6.1, que fue publicada el 19 de noviembre de 2008.

Yellow Dog Linux es un derivado de Fedora Core y se basa en el gestor de paquetes RPM. A través de las sucesivas versiones de Yellow Dog Linux, Terra Soft Solutions ha invertido mucho en la aplicación de apoyo específicamente en el párrafo hardware de IBM y Apple. Como resultado de ello, Yellow Dog Linux soporta aceleración de gráfica y de audio de serie, aunque algunos otros componentes de hardware, como las tarjetas inalámbricas IEEE 802.11g AirPort Extreme de Apple 802.11g (presentes en PowerBooks e iBooks) no funcionan adecuadamente sin modificaciones en el núcleo. 

Desde v5.0, Yellow Dog Linux incorpora por defecto un gestor de ventanas Enlightenment como escritorio predeterminado, aunque soporta e incorpora igualmente otros entornos como KDE, GNOME y Xfce.

Gcalctool: es una poderosa calculadora gráfica con modos financiero, lógico y científico. Utiliza un paquete de precisión múltiple para hacer la aritmética, proporcionando un alto grado de precisión.

Xpad: editor de textos con la posibilidad de poner notas en la pantalla

Evolution: gestor personal de contenidos con cliente de correo.

Gnome-dictionary: es un programa que permite acceder de forma cómoda a los diversos servidores de diccionarios en formato DICT disponibles en internet, como Freedict.de o bien hacer uso de servidores y diccionarios instalados localmente.

Gnome-terminal: es un emulador de terminal para XFree86 escrito por Havoc Pennington y otros.

Vncviewer: es una herramienta para tu dispositivo con la cual podrás visualizar y controlar el escritorio de tu PC desde tu propio dispositivo.

Screen Capture: aplicación para hacer capturas

Gedit: editor de textos se caracteriza principalmente por su facilidad de uso, conseguida en gran parte gracias a una interfaz gráfica clara y limpia, mostrando únicamente las funcionalidades principales que suelen requerir la mayoría de usuarios.

Funciones:


Distribuida en dos DVD (uno de instalación y otro de fuentes), una YDL 4.1 tiene más de 1000 paquetes.
Yellow Dog Linux 5,0 es una de las primeras distribuciones de Linux para funcionar en PlayStation 3. Se ha diseñado específicamente para HDTV SDTV con lo que los usuarios tendrán que utilizar los comandos 'installtext ' y 'ydl480i' para ser capaz de instalar y ejecutar.

Yellow Dog Linux está disponible en dos ediciones que ofrecen dos DVD (de instalación y de fuentes) por un precio entre $50 y $100 (sin o con documentación). Parte del dinero producto de estas distribuciones financia el desarrollo del sistema operativo. Los envases están diseñados para hacer coincidir el recubrimiento de policarbonato blanco de la última PowerPC, iMacs e iBooks, equipos en los que la versión de escritorio es probable que se ejecute.

Como con la mayoría de las distribuciones Linux, Terra Soft Solutions también hace Yellow Dog Linux disponible como una descarga gratuita desde FTP público.




</doc>
<doc id="2991" url="https://es.wikipedia.org/wiki?curid=2991" title="Yves Saint Laurent">
Yves Saint Laurent

Yves Henri Donat Mathieu-Saint-Laurent, más conocido como Yves Saint Laurent (Orán, Argelia, 1 de agosto de 1936-París, Francia, 1 de junio de 2008), fue un diseñador de moda y empresario francés, considerado uno de los principales de la segunda mitad del siglo XX. Se lo considera el creador del esmoquin femenino en los años 1960, el impulsor del resurgir de la alta costura en los años 1980 y de los primeros en incluir en sus desfiles modelos no caucásicas.

Nació en Orán, por entonces colonia francesa de Argelia, en el seno de una de las más ricas familias de la ciudad. Su padre, descendiente de un barón francés, era presidente de una compañía de seguros y propietario de varias salas de cine. Su abuela materna era española. 

En Argelia, la Segunda Guerra Mundial y la ocupación nazi de Francia parecían sucesos lejanos, y no incidieron demasiado en la vida de Yves Saint Laurent y su familia. Siendo niño le gustaba interpretar personajes de Molière y leía con avidez la revista "Vogue". Le atraía el mundo de los diseños para teatro y confeccionaba vestidos para su madre y sus hermanas. Por su carácter "peculiar" sufrió acoso escolar, que él intentaba superar prometiéndose: «Algún día seré famoso».

En 1950, Saint Laurent envió tres diseños a París, a un concurso convocado por el "Secretariado Internacional de la Lana". Quedó en tercera posición, y acudió a recibir el premio acompañado de su madre. Sus diseños sorprendieron a Michel de Brunhoff, redactor jefe de "Vogue", que le recomendó que estudiase en la "Chambre Syndicale de la Couture". Saint Laurent le hizo caso y tras graduarse en Orán se mudó a París. 

En 1951 volvió a participar en el concurso del "Secretariado Internacional", y esta vez resultó ganador, derrotando a un joven Karl Lagerfeld. Remitió más diseños a De Brunhoff, quien vio en ellos similitudes con un diseñador consagrado: Christian Dior. El responsable de "Vogue" envió estos diseños a Dior, que vio al instante el talento de Saint Laurent y decidió sumarlo a su taller.

Con 18 años entró a trabajar en "Dior", si bien sus tareas iniciales fueron más bien prosaicas: decorar el estudio y diseñar algunos accesorios. Sorprendentemente, Christian Dior le eligió como su sucesor en el cargo de diseñador jefe de la casa. Saint Laurent y su madre se extrañaron por la decisión de Dior, quien parecía demasiado joven para jubilarse. Moriría de un infarto ese mismo año.
En 1957, con 21 años, Saint Laurent se convirtió en el modista más joven de la alta costura francesa. Su colección de primavera de 1958 alcanzó resonante éxito, al prolongar el estilo "New Look" acuñado por Dior. Este éxito contribuyó a "rescatar" la firma de una quiebra que parecía segura. En 1959, fue elegido por Farah Diba para que diseñase el vestido de su boda con el Sha de Irán. Pero las creaciones posteriores de Saint Laurent cosecharon duras críticas, y su carrera en "Dior" se interrumpió en el año 1960, cuando fue llamado para cumplir con el servicio militar francés, coincidiendo con la guerra de independencia de Argelia. Saint Laurent había eludido la milicia hasta entonces gracias a las influencias del propietario de "Dior", Marcel Boussac, y se ha conjeturado que cuando Boussac quiso prescindir de él, movió los "hilos" necesarios para que le llamasen a filas.

Saint Laurent duró apenas 20 días en el ejército. Debido a las humillaciones infligidas por unos compañeros sufrió un ataque de estrés y fue ingresado en un hospital militar. Allí supo que la casa "Dior" no le reservaba el empleo y que más bien había prescindido de él; esta noticia empeoró su estado emocional y fue ingresado en el psiquiátrico de Val-de-Grâce, un centro tristemente conocido por sus terapias agresivas. Saint Laurent sufrió electroshocks y le administraron sedantes y otros fármacos, una etapa sombría que ayuda a explicar sus posteriores problemas emocionales y adicciones. 

A finales de 1960 Saint Laurent abandonó el psiquiátrico y, al volver a París, vio que había sido sustituto en la casa "Dior" por Marc Bohan, diseñador que se acercaba más al estilo "ladylike" (femenino a la antigua usanza) que se buscaba. Saint Laurent demandó a la empresa por daños morales con la ayuda de su amigo Pierre Bergé, y con el dinero recibido, sumado al apoyo financiero del empresario J. Mack Robinson de Atlanta, creó su propia casa de costura.

La primera colección de ese mismo año, "Ligne Trapéze" (‘línea trapecio’ en francés), se convirtió en un éxito instantáneo. La imagen y el logotipo de la empresa (un monograma con las iniciales YSL superpuestas) se encargaron al diseñador gráfico francés Cassandre en 1961, y siguen en uso hoy en día.

La colaboración de Pierre Bergé fue sustancial para que Saint Laurent llegase a erigir una empresa sólida. Aunque interrumpieron su relación sentimental en 1976, siguieron conviviendo en la misma casa y colaborando, y Bergé fue el apoyo imprescindible que permitió a Saint Laurent seguir creando y superar sus crisis emocionales.

Sus colecciones en los años 60 destacaron por la incorporación del esmoquin al vestuario femenino y por la implantación del "prêt-à-porter" como una línea comercial completa; de hecho fue el primer creador de alta costura que presentó una línea de esta nueva categoría de moda. En 1966 inauguró su primer local que comercializaba "prêt-à-porter", situado en el nº 21 de la Rue de Tournon: Saint Laurent Rive Gauche. También fue el primer diseñador que incorporó mujeres de color como modelos en sus desfiles.

Diseñó decorados y vestuario para filmes y obras teatrales como "Cyrano de Bergerac" y "La Pantera Rosa", colaborando con Roland Petit, Claude Régy, Jean-Louis Barrault, Luis Buñuel, François Truffaut, Alain Resnais ("Stavisky", 1974), Jean Marais, Zizi Jeanmaire, Arletty...

Vistió a divas del cine como Jeanne Moreau, Claudia Cardinale ("La panthère rose", 1963) e Isabelle Adjani, y convirtió a Catherine Deneuve en icono de estilo y musa personal.

Sus diseños nunca dejaban indiferentes a los críticos. El desfile de otoño de 1966, inspirado en Mondrian, causó sensación, pero otras propuestas no se libraron de críticas negativas. En 1971 Saint Laurent lanzó una colección inspirada en los colaboracionistas franceses durante la ocupación nazi de Francia en los años 1940 que fue "masacrada" porque se entendió que enaltecía los tiempos de la ocupación nazi («que él no conoció») y el «feo utilitarismo de la posguerra». 

Las exigencias de producción en alta costura y "prêt-à-porter" (dos colecciones al año de cada categoría) le acarrearon un estrés creciente, que combatía con alcohol y drogas. En 1987 sufrió un "traspié" en críticas a raíz de un fallido desfile en Nueva York; exhibió chaquetas con aplicaciones de joyas de 100 000 dólares pocos días después de que un "crack" financiero sacudiese la ciudad. Desde entonces, fue delegando el diseño del "prêt-à-porter" en ayudantes, y esta gama de su producción apenas retuvo pujanza entre sus fanes.

La marca "Saint Laurent" suscitó otro escándalo en la última etapa, cuando para promocionar un perfume masculino se recurrió a una fotografía de desnudo frontal, donde un modelo posaba con los genitales visibles; fue la primera (y única) imagen de este tipo que se recuerda dentro de la publicidad de alcance global.

Saint Laurent fue también conocido por su faceta mundana; acudía a discotecas como "Studio 54", y era consumidor habitual de cocaína. Cuando dejó las drogas, sumó otra adicción más inocua: bebía al día varios litros de Coca-Cola. Su vida íntima daba que hablar, aunque su compañero Pierre Bergé siempre le apoyó y contribuyó a que la empresa no naufragase ni en los peores momentos del diseñador.

Saint Laurent y Bergé reunieron una importante colección de arte en su mansión del número 55 de la calle Babylon de París, gracias en gran medida al éxito económico del perfume "Opium" (1977), el más vendido del mundo. Adquirieron obras como un importante retrato de Goya que perteneció a los Rockefeller ("El niño don Luis María de Cistué") y una escultura de madera de Constantin Brancusi (actualmente quedan solo tres en manos privadas). Sumaron pinturas de otros muchos artistas como Frans Hals, Cornelis de Vos, Ingres, Géricault, Picasso, Fernand Léger, Giorgio de Chirico, Édouard Vuillard, Edvard Munch, Matisse y Mondrian, así como dibujos y acuarelas de Manet, Paul Klee, Paul Gauguin, Degas, Toulouse-Lautrec, Alberto Giacometti y Cézanne. En fechas posteriores la pareja declaró que «con los precios actuales, ya no podemos seguir comprando».

Su afición por el arte le llevó a ‘homenajear’ a maestros como Piet Mondrian, Picasso y Braque, con vestidos que reproducen sus motivos. Una exposición en La Coruña (febrero de 2008) ilustró esta influencia en su trabajo, mostrando sus diseños junto con obras de arte que los inspiraron.

Yves Saint Laurent anunció su retirada del diseño de moda y las pasarelas en enero de 2002. Se mostró decepcionado por la moda predominante, que a su juicio arrinconaba la ambición artística a favor del simple lucro «como si fuese hacer cortinas para ventanas».

Yves Saint-Laurent falleció en París el 1 de junio de 2008, a la edad de 71 años, tras padecer cáncer. Pocos días antes, formalizó su prolongada relación con Pierre Bergé mediante una unión civil, seguramente para solucionar cuestiones de herencia. Habían mantenido su relación amistosa hasta el final, si bien viviendo separados desde 1992. 

Al funeral de Saint Laurent acudieron el entonces presidente de Francia Nicolas Sarkozy y su esposa Carla Bruni, que había trabajado como modelo durante muchos años con la firma; así como por importantes personalidades vinculadas a la moda como Valentino, John Galliano, Jean Paul Gaultier, Claudia Schiffer, Naomi Campbell , Catherine Deneuve y Farah Diba. 

Tras la muerte del diseñador, su colección de arte se dispersaba; su compañero Bergé comentó que tal conjunto se había formado como un proyecto de los dos que, al fallecer Saint Laurent, había perdido sentido. Se celebró una subasta de tres días, entre el 23 y el 25 de febrero en el Grand Palais de París a cargo de la firma "Christie's". De los 733 lotes se vendieron 730 por un total de 373 millones de euros, dinero que se destinó a una fundación y a la lucha contra el sida. El citado retrato de Goya se excluyó de la venta para ser donado al Museo del Louvre, y Bergé se quedó con algunas obras de Andy Warhol (retratos de Saint Laurent y de su perro favorito). Curiosamente, uno de los pocos lotes que no encontraron comprador fue el cuadro de Picasso "Instrumentos de música sobre una mesa", la pintura con valoración más alta. Por el contrario, un cuadro de Mondrian fue adquirido para el futuro museo Louvre Abu Dhabi.

En mayo de 2009, se ha rumoreado que la antigua vivienda de Saint Laurent y Bergé en la calle Babylon podría ser adquirida por el presidente Nicolas Sarkozy y Carla Bruni como su nueva residencia.

En 2010, se hallaron diversos dibujos hechos por Saint Laurent que databan de los años 1970 y 1980, entre estos cuadros se encuentra un retrato desnudo del líder de Queen, Freddie Mercury, posiblemente hecho en Múnich, Alemania en 1984.

Saint Laurent ha pasado a la historia como el primer diseñador de moda que ha expuesto en un museo, el Metropolitan Museum de Nueva York. Creó, con Pierre Bergé, una fundación para la custodia y difusión de su legado creativo.




</doc>
<doc id="2994" url="https://es.wikipedia.org/wiki?curid=2994" title="Yukón">
Yukón

Yukón (en inglés y en francés: "Yukon") es uno de los tres territorios que, junto con las diez provincias, conforman las trece entidades federales de Canadá. Su capital es Whitehorse. Está ubicado en el extremo noroeste del país, limitando al norte con el océano Ártico, al este con Territorios del Noroeste, al sur con Columbia Británica y al oeste con Alaska (Estados Unidos). Con 31 530 habs. en 2008 es la segunda entidad menos poblada —por delante de Nunavut— y con 0,06 hab/km², la tercera menos densamente poblada, por delante de Territorios del Noroeste y Nunavut, la menos densamente poblada. 

La cresta de los montes Mackenzie da forma a gran parte de la frontera oriental. 

La etimología de su nombre proviene de una lengua aborigen local, el gwichʼin, y quiere decir «río grande». El territorio es famoso entre otras cosas por haber sido el escenario de la Fiebre del oro de Klondike, un hecho histórico que ocurrió en 1897 y que fue de gran trascendencia para la región.

Se han encontrado restos humanos de la que fuera la población más antigua y primitiva de Norteamérica, aunque su datación es bastante discutida. Una gran cantidad de huesos modificados y de rasgos humanoides que fueron descubiertos en la región de Old Crow, al norte del Yukón, tienen entre 25 000 o 40 000 años de antigüedad, según el estudio por el método del carbono 14. El centro y norte del Yukón no fueron afectados por las glaciaciones, como sí lo fue parte de Beringia.

Una erupción volcánica en Mount Churchill, próximo a la frontera con Alaska, cubrió de cenizas el sur de Yukón. Aquel depósito de brasas y restos de magma puede apreciarse aún a lo largo de la autopista de Klondike. Las historias de las Primeras Naciones del Yukón hablan sobre la muerte de los animales y peces a consecuencia de este suceso, al igual que otras provenientes de las tribus de lenguas atabasca, navajo y apache, lo que ha llevado a muchos antropólogos a la conclusión de que la emigración de pueblos athabaskanos a lo que es hoy conocido como el suroeste de Estados Unidos pudo haber sido consecuencia de dicha erupción. Poco después, las innovaciones en la tecnología de la caza favorecieron la sustitución de los atlatles por el arco y la flecha.

Se desarrollaron redes extensivas de comercio e intercambio entre los tlingits de la costa y las Primeras Naciones del interior. Se cree que los primeros intercambiaban aceite de eulacón y otros productos de su entorno por cobre y pieles de las Primeras Naciones.

Los primeros europeos en visitar este territorio fueron los españoles, quienes realizaron sus exploraciones iniciales a partir del siglo XVI. La parte oeste de las costas del Pacífico fueron territorios en disputa, explorados y de escasos asentamientos del imperio español, Yukón junto con la provincia de Columbia Británica, pasó a formar también parte del Virreinato de la Nueva España. Más adelante fue cedido al imperio británico.

Las incursiones europeas en lo que más tarde se conocería como el Yukón dieron comienzo en la primera mitad del siglo XIX. Los exploradores y comerciantes de la Compañía de la Bahía de Hudson, que llegaron desde los puestos comerciales del río Mackenzie, emplearon dos rutas diferentes para penetrar en el territorio, creando puestos comerciales por toda la ruta. La ruta del norte nacía en Fort McPherson, Territorios del Noroeste, a orillas del Mackenzie, cruzaba las montañas por los ríos Bell y Porcupine y llegaba hasta el río Yukón. La del sur comenzaba, en cambio, en Fort Liard (Territorios del Noroeste), y se dirigía hacia el oeste por el río Liard hasta el lago Frances, seguía hacia el oeste siguiendo el curso del río Liard hasta el lago Frances, y luego por el río Pelly hasta que éste desembocaba en el Yukón.

Tras fundar Fort McPherson, John Bell cruzó las montañas para llegar a la cuenca del Yukón en 1845, y descendió por el río Rat (hoy conocido como Bell) hasta su confluencia con el Porcupine. Tras organizar el comercio de pieles en Fort McPherson regresó al Bell, y siguió río abajo por el Porcupine hasta llegar de nuevo al río Yukón, en el lugar en que más adelante se levantaría el fuerte del mismo nombre (Fort Yukón). No mucho tiempo después, Alexander Hunter Murray estableció puestos comerciales en Lapierre House (1846), y en Fort Yukon (1847), en la confluencia de los ríos Porcupine y Yukón. Murray hizo varios dibujos de las tiendas de venta de piel y de los habitantes de la zona, y escribió el "Journal of the Yukon, 1847–48" (Diario del Yukón), una valiosa fuente de información acerca de la cultura local de los gwich’in en la época. Como el puesto estaba en realidad en la Alaska rusa, la Compañía de la Bahía de Hudson continuó su actividad comercial allí hasta que fue expulsada tras la adquisición de Alaska por Estados Unidos en 1869. Un nuevo puesto comercial, Rumpert House, fue establecido aguas arriba del Porcupine, pero se demostró que se hallaba también dentro de Alaska. Los gwich’in, especialmente bajo el liderazgo de Sahneuti, enfrentaron a la Compañía de la Bahía de Hudson con los comerciantes estadounidenses de la Compañía Comercial de Alaska.

Por esa misma fecha, Robert Campbell, procedente de Fort Simpson, exploró buena parte del sur del Yukón y fundó Fort Frances (1842) sobre el lago homónimo en la cuenca del río Liard, y Fort Selkirk (1848) en la confluencia de los ríos Yukón y Pelly. En 1852, Fort Selkirk fue saqueado por guerreros tlingit de la costa, que se opusieron violentamente a esta injerencia en su actividad comercial. A raíz de este incidente, Fort Selkirk quedó abandonado y no se restableció hasta 1889.

Los misioneros anglicanos y católicos siguieron la estela del comercio de pieles, siendo digno de mención William Carpenter Bompas, que se convirtió en el primer obispo anglicano del Yukón. Por su parte, los misioneros católicos pertenecían a la orden de los Oblatos de María Inmaculada, aún presente en el territorio. 

En 1859, Robert Kennicott emprendió una expedición para recolectar especímenes de historia natural en los valles del hoy llamado río Mackenzie y del río Yukón, y en la tundra ártica. Kenicott adquirió popularidad entre los traficantes de la Compañía de la Bahía de Hudson, y los incentivó a buscar especímenes de historia natural y objetos manufacturados por las Primeras Naciones y a enviar lo recabado al Instituto Smithsoniano. En 1865, se organizó la Expedición del Telégrafo de Western Union, con el fin de encontrar alguna ruta posible para establecer una línea de telégrafo entre Norteamérica y Rusia a través del mar de Bering. Kennicott fue el jefe científico de la expedición, y entre el grupo de naturalistas que la integraban se encontraba W. H. Dall. Lamentablemente, Kennicott falleció de un ataque cardíaco cuando remontaba el río Yukón. Sin embargo, sus esfuerzos dieron a conocer al mundo este territorio canadiense. 

A pesar de los rumores sobre la presunta presencia de oro en la región yukoniana, no se procedió a grandes investigaciones. Tras la compra de Alaska por parte del gobierno de Estados Unidos y el consecuente abandono de Rampart House, los comerciantes de la Compañía Comercial de Alaska empezaron a trabajar en el curso superior del río Yukón. Tres mineros —Alfred Mayo, Jack McQuesten y Arthur Harper— habiendo oído de estos rumores, se unieron a los trabajos de la compañía, si bien su principal interés radicaba en la búsqueda de oro. En 1874, Mayo y McQuesten fundaron Fort Reliance, a unos kilómetros río abajo de lo que sería más tarde Dawson City. Otros mineros y buscadores se adhirieron pronto a la empresa, y se encontró oro en muchas áreas, aunque raramente en cantidades suficientes como para que supusiera un buen negocio. Hacia 1885, una buena cantidad de este metal fue hallada en el río Stewart, y McQuesten logró convencer a la compañía de que empleara a los mineros en lugar de centrar su actividad en el comercio de pieles. Al año siguiente, se encontraron cantidades rentables de oro en bruto en el río Fortymile, y se fundó un nuevo puesto comercial, llamado también Fortymile, en la confluencia de este río con el Yukón. 

Paralelamente, el Ejército de los Estados Unidos envió al teniente Frederick Schwatka para reconocer el río Yukón para el ejército estadounidense. Atravesando el Paso de Chilkoot, la expedición construyó balsas y navegó por el Yukón hasta su estuario en el mar de Bering, dando nombre a muchas zonas geográficas durante ese tramo. La expedición de Schwatka alarmó al gobierno canadiense, que envió a su propio grupo de expedicionarios al mando de George Mercer Dawson en 1887. William Ogilvie, un agrimensor que saltaría a la fama durante la fiebre del oro de Klondike, midió los terrenos para fijar con precisión la frontera natural con Alaska.

En 1894, preocupado por la afluencia de mineros estadounidenses y el tráfico de licor, el gobierno canadiense encargó al inspector Charles Constantine de la Policía montada del Canadá investigar las condiciones bajo las que se encontraba el distrito del Yukón. Constantine declaró que se acercaba una fiebre del oro y reclamó, de forma urgente, la presencia de una fuerza policial que fuera capaz de controlar la zona. Un año después, regresó al Yukón en compañía de 20 hombres que se encontraban allí cuando comenzó la "Fiebre del oro de Klondike", en 1897.

La "Fiebre del oro de Klondike" constituye un hecho crucial en la historia del Yukón. Un grupo comandado por Skookum Jim Mason descubrió oro en un afluente del río Klondike en agosto de 1896. Entre 30.000 y 40.000 personas desafiaron un sinfín de dificultades para alcanzar el yacimiento de oro de Klondike en el otoño, invierno y la primavera de 1897-1898, después de que el hallazgo se hiciera oficial en 1897. Con la afluencia de inmigrantes estadounidenses, el gobierno canadiense decidió crear un territorio separado para controlar mejor la situación. En 1901, tras el regreso de muchos a sus hogares, el censo arrojaba una población de 27.219 habitantes, una cifra que no volvería a alcanzarse hasta 1991. La masiva afluencia de inmigrantes en la región estimuló la exploración minera en otras partes del Yukón y propició dos "fiebres del oro" de menor importancia en Atlin (Columbia Británica) y Nome (Alaska), así como varias pequeñas incursiones. La necesidad de transporte hacia los campos de oro llevó a la construcción del ferrocarril de White Pass y Yukón.

El escritor estadounidense Jack London reflejó la vida de los buscadores de oro en varias de sus novelas y relatos. Fue seducido, como tantos otros, por la idea de hacerse rico en poco tiempo, pero después de pasar varios meses allí enfermó de escorbuto y regresó con las manos vacías. Algunos de sus mejores cuentos ambientados en la dura vida del Norte son "La hoguera" ("To buil a fire", 1908), "El silencio blanco" ("The white silence", 1899), "El filón de oro" ("All gold canyon", 1905), "El amor a la vida" ("Love of life", 1905), y las novelas "La llamada de la selva" y "Colmillo Blanco".

Al término de la fiebre de oro, la población del territorio declinó rápidamente, alcanzando un mínimo de 4157 en 1921 y permaneciendo bastante estable hasta los años 1940. Esto a pesar del desarrollo de otras áreas mineras, incluyendo yacimientos de plata en Conrad y especialmente en mayo, de oro en la región del lago Kluane, y de cobre cerca de Whitehorse. En Klondike, los derechos de varios mineros particulares fueron adquiridos y consolidados, con apoyo del gobierno, por un reducido número de compañías, entre ellas la "Yukon Gold Corporation" de Solomon R. Guggenheim, que utilizaba dragas flotantes. La "Yukon Consolidated Gold Company" («Compañía del Oro Consolidada del Yukón») continuó dragando en busca de oro hasta la década de 1960, disfrutando de un breve período de prosperidad durante los años 1930 con la subida del precio del oro.

Hacia 1920 el consejo territorial electo había quedado reducido a tres miembros, y Yukón pasó a ser gobernado por un Comisionado del Oro, funcionario federal dependiente del Ministerio del Interior de Canadá.

El siguiente hecho importante en la historia del Yukón fue la construcción, durante la Segunda Guerra Mundial, de la autopista Alaska, la cual, tras su renovación llevada a cabo por el gobierno canadiense a fines de la década de 1940, abrió el territorio al tráfico por carretera. La guerra también fue testigo de la construcción de varios aeródromos como parte de la plataforma de la ruta del Noroeste. No obstante, la afluencia de trabajadores para las obras de la autovía del sur tuvo efectos devastadores para algunas de las Primeras Naciones, que sufrieron un gran número de muertes al verse expuestas a enfermedades a las que no eran inmunes.

En las décadas de 1950 y 1960 se construyeron otras autopistas, lo que tuvo como consecuencia la decadencia y consecuente desaparición de los barcos fluviales, que habían sido hasta entonces el principal medio de transporte en la zona. En la segunda mitad del siglo XX, la "White Pass & Yukon Route" (Ruta de White Pass y Yukón) inició los fletes de transporte intermodal en contenedores. La minería también resucitó, incluyendo la explotación de cobre en Whitehorse, plata y plomo en Keno City y Elsa, y asbesto en Clinton Creek. La mayor mina de cinc a cielo abierto de todo el mundo se abrió en Faro a comienzos de los años 1970. La minería del oro regresó a Klondike y a otras zonas con la importante subida de los precios de este metal en los años 1970.
Entre las décadas de 1980 y 1990, la minería decayó y el papel del gobierno aumentó considerablemente, con transferencias económicas que fueron incrementando su importancia. En 1978, se consiguió establecer un gobierno responsable y partidos políticos que lo sustentaran. Por otro lado, las Primeras Naciones comenzaron a organizarse e iniciaron negociaciones para hacer valer sus derechos territoriales, que culminaron con la firma del "Umbrella Final Agreement" (Acuerdo Final de Umbrella) en 1992. Aunque la mayoría de las Primeras Naciones firmaron acuerdos, sus reivindicaciones territoriales y de autogobierno continúan en la actualidad. Las Primeras Naciones son consideradas actualmente un cuarto nivel de gobierno, y la naturaleza específica de las relaciones intergubernamentales es un aspecto en el que se sigue trabajando.

El territorio del Yukón se localiza en el extremo noroeste de Canadá. Escasamente poblado, destaca por su paisaje natural de lagos de hielo derretido y montañas perennemente nevadas, entre las que se encuentran muchas de las más altas de Canadá. El clima es ártico, subártico y muy seco, con largos inviernos y breves veranos. Sin embargo, las prolongadas horas de sol en el estío son suficientes para el florecimiento de brotes y frutos comestibles. La mayor parte del territorio se encuentra cubierta de bosques y matorrales boreales, siendo la tundra el tipo más común de paisaje sólo en el extremo septentrional y sobre elevaciones altas. El campo de hielo no polar más grande del mundo, el Kluane, se sitúa mayoritariamente en el Yukón.

La región yukoniana posee una forma similar a la de un triángulo rectángulo, limitando con el estado de Alaska al oeste, con los Territorios del Noroeste al este, y con la provincia de Columbia Británica al sur. Comprende una superficie aproximada de , de los cuales son tierra y 8052km, agua. 

El Yukón está delimitado por el paralelo 60º de latitud en el sur. Su costa norte se halla sobre el mar de Beaufort, y su ribera occidental se circunscribe a los 141 grados oeste de longitud. Su límite este (algo desparejo) sigue la división del curso fluvial que subyace entre la cuenca del río Yukón y el estuario del Mackenzie hasta la cadena montañosa del mismo nombre. La totalidad del Yukón se ubica al oeste de Vancúver, por lo que alberga a las comunidades más occidentales de Canadá. 
Excepto por la llanura costera del mar de Beaufort (océano Ártico), el resto del territorio forma parte de las Montañas Rocosas. El terreno incluye cadenas montañosas, mesetas y valles fluviales. 

El suroeste se encuentra dominado por los campos nevados de Kluane ("Kluane National Park and Reserve"). Las montañas de Saint Elias, también en esta zona, representan a cinco de las más altas elevaciones canadienses. Un buen número de glaciares emergen de estos campos, entre los que destacan el Logan, el Hubbard y el Kaskawulsh.

Asimismo, el permafrost es muy frecuente, sobre todo en el norte y en el centro (donde se halla más extendido). El sur, en cambio, carece de grandes concentraciones de este fenómeno, sólo viéndose en escarchas heladas bastante aisladas. 

Dos grandes fallas, la de Denali y la de Tintina, han sido las responsables de la creación de vastos valles denominados trincheras: Shakwak y Tintina. La primera separa a las cordilleras de Kluane de otras montañas del norte. Tanto la vía de Haines como la de Alaska, en el punto septentrional de la intersección Haines, fueron construidas sobre esta trinchera. La de Tintina rodea al Yukón de noroeste a sureste, y sus orillas son ricas en depósitos minerales, como los yacimientos de oro de Klondike o los de plomo y zinc cercanos a Faro. 

Fuentes: Precisiones geológicas del Yukón en "Yukon Geoprocess File User Guide" (archivo PDF, 1.2MB)
Las montañas de Saint Elias forman parte del relieve costero que se extiende desde el sur de la Columbia Británica hasta Alaska, abarcando al sureste del Yukón. Mientras que éstas son las más elevadas, existe otra buena cantidad de montes como, por ejemplo, las Montañas Británicas bien al norte, que componen la cordillera de Brooks; las Montañas Mackenzie y Richardson al este; las de Cassiar al sureste; las de Pelly en el centro; y las Montañas Ogilvie en el Yukón nórdico, pasando Dawson City por la vía de Dempster. 

Las cordilleras del Yukón incluyen:


La mayor parte del territorio se encuentra en la cuenca del río homónimo, que desemboca en el Mar de Bering. El sur del Yukón es abundante en lagos glaciares, angostos y alpinos, que fluyen, en gran medida, hacia el sistema del Yukón. Los más grandes son: Teslin, Atlin, Tagish, Marsh, Laberge, Kusawa, y Kluane. El lago Bennett, donde se dio la fiebre del oro, es de menor tamaño y muere en el Tagish.

Otra gama de ríos y riachuelos desembocan en el océano Pacífico o directa o indirectamente en el Ártico. El drenaje del Alsek-Tatshenshini sigue su curso hacia el Pacífico desde el suroeste territorial, y los dos ríos principales del Yukón, el Liard y el Peel, fluyen hacia el Mackenzie y terminan en los Territorios del Noroeste (sureste y noreste respectivamente).

El clima predominante es el subártico, caracterizado por inviernos profundos y breves veranos templados. La pista de aterrizaje de Snag, 25 kilómetros al este de Beaver Creek, sobre la frontera con Alaska, experimentó la temperatura más baja registrada en la historia de Norteamérica, -63 °C el 3 de febrero de 1947. La costa del océano Ártico posee un clima polar. El promedio yukoniano es el de una constante seca, de pocas precipitaciones, y ligeramente húmedo en el sureste. Las lluvias son más abundantes en las montañas, y el hielo comienza a derretirse en verano, propiciando las caídas intensas de agua en los meses de julio y agosto. 

La tundra predomina en casi todo el Yukón. De acuerdo con las definiciones de la ecozona de Environment Canada, el sur y centro del territorio representan la "Ecozona de la Cordillera Boreal", mientras que el bosque del norte constituye la "Ecozona de la Cordillera de Taiga". El sector del río Peel en el noreste forma parte de la llanura taigana, y el litoral ártico, de la "Ecozona del Ártico meridional".

La pícea negra ("Picea mariana") y la blanca ("Picea glauca"), el aspa ("Populus tremuloides") y el álamo balsámico ("Populus balsamifera") están presentes a lo largo de todo el territorio. Aunque poco común, el abedul de Alaska ("Betula neoalaskana") puede encontrarse también en el panorama vegetal del Yukón. Una variedad de conífera ("Pinus contorta") alcanza su extremo septentrional y central-meridional, mientras que el tamarack ("Larix laricina") se remonta al sureste, y el abeto subalpino ("Abies lasiocarpa") a las elevadas alturas de la región sur.

Los mamíferos más grandes son: el caribú ("Rangifer tarandus", de baldío y de bosque), el alce ("Alces alces"), el lobo ("Canis lupus"), el oso pardo ("Ursus arctos horribilis") y el oso negro americano ("Ursus americanus"). En la altura se pueden ver ovejas ("Ovis dalli") y, en sur, a la cabra de las Rocosas ("Oreamnos americanus"). Los osos polares ("Ursus maritimus") se adentran en la costa ártica. El ciervo mulo ("Odocoileus hermionus") y su predador, el puma ("Puma concolor"), se han vuelto muy populares en el sur, y los coyotes ("Canis latrans") han ganado terreno en el norte. El uapití y el bisonte fueron introducidos tardíamente.

Hay un gran número de roedores, incluyendo ardillas, ardillas de tierra, lemmings, pikas, castores, varias ratas de campo, puercoespines, ratas almizcleras, etc. Los mustélidos son también muy representativos: glotón ("Gulo gulo"), marta ("Martes americana"), mustela erminea ("Mustela erminea"), comadreja ("Mustela nivalis"), visón americano ("Mustela vison"), y nutria de río ("Lontra canadensis"). Otros pequeños carnívoros son: el lince canadiense ("Lynx canadensis"), el zorro rojo ("Vulpes vulpes") y el zorro polar ("Alopex lagopus") en el litoral norteño. 

Más de 250 especies de aves sobrevuelan el Yukón. La grajilla común ("Corvus corax") es el ave más extendida. Entre otros pájaros autóctonos encontramos el águila calva ("Haliaeetus leucocephalus"), el águila real ("Aquila chrysaetos"), el halcón rústico ("Falco rusticolus") y el halcón peregrino ("Falco peregrinus"), y cinco tipos de urogallo (de la pícea, azul, gorguera, ptarmigano, y el ptarmigano de cola blanca). Muchas aves migratorias se aparean y crían en el Yukón, como ocurre en el perímetro norte del Pacífico.

Además del burbot y el lucio norteño, muchos de los peces que habitan las aguas yukonianas son salmónidos. Cuatro especies de salmón viven en los ríos y cuencas del Yukón, y en los lagos del Pacífico. El río Yukón tiene la corriente de agua más fresca para el hábitat de cualquier salmón; el chinook nada río arriba unos 3000kilómetros desde su boca en el Mar de Bering hasta Whitehorse, donde deposita sus huevas. También hay salmones sockeye y truchas arcoíris en lagunas interiores.

Los salvenilus se componen de truchas de lago, presentes en gran parte de las lagunas del Yukón, así como de truchas autóctonas (Dolly Varden, toro y del Ártico). El pez grisáceo polar es ubicuo, mientras que los lagos poseen varios corégonos e inconos. 

No hay reptiles en el territorio. En cuanto a anfibios, se encuentran unas pocas ranas.

El Yukón posee abundantes fuentes minerales, siendo la minería el principal pilar de su economía hasta hace poco tiempo. No en vano, el oro encontrado en Klondike provocó la fiebre de este metal en 1897. Actualmente se encuentra oro en muchos arroyos y ríos, habiendo industrias que se dedican a su explotación activa.

Otros minerales que se han hallado en mayor o menor medida son: cobre en la región de Whitehorse, plomo y zinc, en Faro, y estos dos últimos más algunos agregados de plata, en mayo y Kenik. Se ha descubierto además asbesto en Clinton Creek, y cobre, oro, y carbón en la zona de Carmacks. El yacimiento de tungsteno más grande del mundo se encuentra en Macmillan Pass, en las montañas Mackenzie, próximo a la frontera con los Territorios del Noroeste. Los minerales no metálicos incluyen al jade y a la baritina.

La venta de pieles había sido el sostén económico de las tribus de las Primeras Naciones, pero la baja de precios y el crecimiento de las críticas por parte del sector defensor de los animales terminaron por poner fin a dicha actividad.

El Yukón dispone de tres centrales hidroeléctricas: una en Schwatka Lake, en Whitehorse, otra cerca de Mayo y una tercera en Aishihik Lake.

Mientras que los bosques predominan en el paisaje, la mayoría de los árboles son pequeños y de lento crecimiento a consecuencia del clima frío seco. Se practica la silvicultura a pequeña escala, siendo en el sur donde se perciben los aportes industriales más fructíferos debido a su ligera humedad. Sin embargo, la distancia que existe respecto al mercado y los elevados precios han resultado en una empresa poco provechosa. 

Una pequeña cantidad de gas natural se produce en el sureste, si bien poco se ha explorado en otros puntos del territorio. Se cree que puede haber grandes reservas de gas en el área de Eagle Plains sobre la vía de Dempster Highway, y posiblemente en zonas allegadas a Whitehorse, pero, una vez más, la distancia desde el gasoducto ha obstaculizado la investigación.

El calentamiento global está afectando más al norte que a cualquier otro punto del planeta, y el Yukón no es la excepción. Si bien es cierto que sus residentes recibirían con entusiasmo una buena temporada de calor, los efectos colaterales de tal fenómeno son desconocidos. La subida de las temperaturas incurriría en el aumento de la evaporación y en la sequía de un ambiente que es de por sí árido, provocando incendios forestales y reduciendo la productividad biológica de los bosques boreales, cuyo crecimiento se ve más limitado a la falta de humedad que a la de una temperatura favorable. 

El territorio es además el blanco de la contaminación proveniente de otros sectores del mundo, sobre todo de origen orgánico, por lo que el consumo de animales salvajes y de pescado ya no es aconsejable. 

Localmente, la demanda minera y su correspondiente explotación son causantes de la aparición de ácido en los confines de su campo laboral, costando cientos de millones de dólares en reparos y limpieza.

En un intento por incentivar la investigación de recursos naturales, en 2005 el Partido del Yukón, liderado por Dennis Fentie, ha suspendido la protección de áreas que se hallaban anteriormente respaldadas por el gobierno del Partido Demócrata, y ha señalado su intención de no crear parques restringidos adicionales. 

La tribu gwichʼin de Old Crow depende del caribú de Porcupine para autoabastecerse con comida y abrigo, como otras tantas del entorno. Esta especie se traslada a las llanuras de la costa para aparearse en el "Refugio Salvaje Nacional del Ártico" (Arctic National Wildlife Refuge) en Alaska. Esa manada suele ser seriamente castigada por la actividad petrolera del ambiente.

De escasa población, y con cerca de en un territorio casi tan extenso como España o Suecia, Yukón tiene una densidad de por km². Cerca de tres cuartas partes de la población se concentran en la zona de Whitehorse, y el resto vive en otras pequeñas comunidades. Todas ellas, excepto Old Crow, son accesibles por carretera. 

La capital, Whitehorse, es también su ciudad más grande y poblada; la segunda en importancia es Dawson City (1800hab.), que fue la primera en población hasta 1952. 

Tradicionalmente, el Yukón estaba habitado por tribus atabascas de las Primeras Naciones, que habían establecido fuertes redes de comercio con los tinglits del Pacífico. Se estima que el 20% de su población actual es de origen indígena. Los inuit que habitaban en las costas del Ártico se extinguieron a causa de una epidemia. 

El siguiente cuadro presenta la población de muchas comunidades territoriales. Es importante especificar que el censo de 2001 comprende a todos aquellos que residen entre los límites comunitarios, mientras que el Departamento de Estadísticas del Yukón ("Yukon Bureau of Statistics" YBS) incluye a todo ciudadano con dirección de correo postal. Comúnmente, muchas personas viven en las afueras de los polos urbanos, de ahí que haya más registros en el YBS. 
 

Población del Yukón desde 1901:
"Fuente: Statistics Canada" 

Al ser un territorio federal, los idiomas oficiales del Yukón son el inglés y el francés, aunque de acuerdo al censo de 2011 y al igual que en el resto de Canadá, a excepción de Quebec (francés mayoritario) y Nunavut (inuktitut mayoritario), el idioma más hablado es el inglés ya que el 82,9% de los habitantes del territorio la tienen como lengua materna.

La economía está basada en los recursos minerales (plomo, zinc, plata, oro, asbesto, cobre, tungsteno, jade y baritina). La industria de fabricación, incluyendo los muebles, ropa y artesanías, le sigue en importancia, junto con la hidroelectricidad. En la actualidad, el sector público es, de lejos el mayor empleador en el territorio, dando empleo directo a aproximadamente 5000 personas de una fuerza laboral de 12500.

El mayor atractivo del Yukón es su naturaleza casi virgen y el turismo del lugar depende en gran medida de esto, hay muchos proveedores de equipo organizado y guías a disposición de los cazadores y pescadores y amantes de la naturaleza de todo tipo. Los amantes del deporte pueden remar en los lagos y ríos con canoas y kayaks, tomar rutas de viaje o de paseo, esquiar o hacer snowboarding en un entorno organizado o acceder en moto de nieve, escalar las cumbres más altas de Canadá o tener una pequeña caminata por las montañas, o tratar de escalada en hielo y trineos tirados por perros.

Yukón también tiene una amplia gama de actividades culturales y deportivas que atraen a artistas, participantes y turistas de todas partes del mundo.

Curiosamente es uno de los 50 países que aparecen en el juego TEG.

Antiguamente, el principal medio de transporte era la red fluvial del río Yukón, tanto antes como después de la Fiebre del Oro. Los tinglits de la costa comerciaban con la gente atabasca empleando caminos montañosos.

Desde la fiebre del oro hasta los años 1950, las embarcaciones navegaron por el Yukón, principalmente entre Whitehorse como punto de concentración y Dawson City, algunos incluso llegando a Alaska y al mar de Bering, mientras que otros lo hacían por el río principal (el Yukón) y sus afluentes (río Stewart, etc.).

Muchos barcos pertenecían a la compañía de navegación del Yukón británico, una extensión de las rutas del White Pass (y Yukón), que operaba asimismo en un área reducida entre Skagway (Alaska) y Whitehorse. El ferrocarril dejó de funcionar en la década de 1980 con la primera clausura de la mina de Faro. Hoy en día sólo es utilizado como medio de transporte y traslado de turistas durante el verano, y no en la totalidad del territorio.

En tiempos actuales, el principal medio de comunicación terrestre lo representa, sin duda alguna, la autopista Alaska, que atraviesa Whitehorse. La ruta de Klondike comprende el tramo que va desde Skagway, pasando por la capital yukoniana hasta llegar a Dawson City; la de Haines se extiende desde la ciudad del mismo nombre en Alaska hasta Haines Junction, Yukón; y la de Dempster lo hace desde la ruta de Klondike hasta Inuvik, en los Territorios del Noroeste. Todas estas carreteras, a excepción de la última, se hallan pavimentadas. Otros caminos menos transitados son el Campbell, que comunica Camaracks con Watson Lake, en la vía de Alaska; y el "Silver Trail" que interseca con la ruta de Klondike sobre el puente del río Stewart para unirse a las viejas comunidades mineras de plata en mayo, Elsa y Keno City. La casi totalidad de los municipios del Yukón son accesibles por carretera, siendo el transporte aéreo la única forma de llegar a la remota comunidad de Old Crow en el extremo norte.

El aeropuerto internacional de Whitehorse sirve de conexión con otras regiones cercanas, entre ellas Vancúver, Calgary, Edmonton, Fairbanks, Juneau y Frankfurt (en verano). Cada comunidad posee un aeropuerto, y la empresa de vuelo está esencialmente al servicio del turismo y las prospecciones mineras.

En el siglo XIX, Yukón fue parte primero del Territorio Noroeste administrado por la Compañía de la Bahía de Hudson, y luego de los Territorios del Noroeste gobernados por Canadá. Consiguió cierto nivel de autogobierno solo en 1895, cuando se convirtió en un distrito separado de los Territorios del Noroeste. En 1898 se convirtió en un territorio separado, con su propio Comisionado y Consejo Territorial. 

Antes de 1979, Yukón era administrado por un Comisionado nombrado por el Ministro federal de Asuntos Indígenas y Desarrollo del Norte. Este comisionado presidía y jugaba un cierto papel en el nombramiento de un Consejo ejecutivo, cuyas competencias eran únicamente consultivas. En 1979, el gobierno federal y el comisionado delegaron parte de su poder en una asamblea territorial que, en ese año, adoptó un sistema de partidos de gobierno responsable. Este trámite fue efectuado a través de una carta acreditativa expedida por el ministro Jake Epp, más que por un procedimiento legislativo.

La "Yukon Act", aprobada el 1 de abril de 2003, formalizó los poderes del gobierno del Yukón y añadió una serie de competencias adicionales de índole territorial al gobierno del territorio (por ejemplo, el control sobre la tierra y los recursos naturales). Desde 2003, excepto en materia de persecuciones criminales, el Yukón dispone casi de los mismos poderes que los gobiernos provinciales, algo que también están buscando los otros dos territorios (Noroeste y Nunavut). Hoy, el papel del Comisionado es análogo al de un Teniente-Gobernador provincial; no obstante, a diferencia de estos últimos, los comisionados no representan a la Reina de Canadá, sino que son empleados del gobierno federal. 

En previsión del gobierno responsable, se organizaron partidos políticos y se postularon candidatos para la Asamblea Legislativa del Yukón por primera vez en 1978. Los Conservadores Progresistas ganaron las elecciones y constituyeron el primer gobierno de partido en enero de 1979. El Nuevo Partido Democrático del Yukón se mantuvo en el poder desde 1985 hasta 1992, bajo el mandato de Tony Penikett, y de nuevo en 1996, con Piers McDonald, hasta su derrota en el 2000. Los conservadores regresaron al poder en 1992 de la mano de John Ostashek, tras cambiar su nombre por el de Partido del Yukón. El gobierno de Pat Duncan, del Partido Liberal del Yukón, fue derrotado en las elecciones de noviembre de 2002 frente a Dennis Fentie del Partido del Yukón, quien fue nombrado "Premier/Premier ministre". 

Si bien se han presentado debates en torno a si el Yukón pudiera ser considerado como la undécima provincia de Canadá, se suele poner de relieve su escasez poblacional, razón más que suficiente para desmerecer dicha categorización. Como resultado, el gobierno de la Columbia Británica ha propuesto en un sinfín de ocasiones el hacerse cargo de su administración. 

A nivel federal, el territorio está representado en el Parlamento de Canadá por un diputado y un senador. A diferencia de lo que sucede con Estados Unidos, todos los miembros del Parlamento tienen el mismo valor, y los residentes en Yukón gozan de los mismos derechos que otros ciudadanos canadienses. Uno de los representantes del Yukón en el Parlamento —Eric Nielsen— fue el diputado primer ministro durante el gobierno de Brian Mulroney, mientras que otro —Audrey McLaughlin— fue el líder del Nuevo Partido Demócrata. 

El Yukón fue una de las nueve jurisdicciones de Canadá en proponer el matrimonio entre homosexuales antes de la aprobación de la Ley de Matrimonios Civiles canadiense, junto con Ontario, Columbia Británica, Quebec, Manitoba, Nueva Escocia, Saskatchewan, Terranova y Labrador y Nuevo Brunswick.

La gran mayoría de la población forma parte de las Primeras Naciones. En 1991 se firmó un acuerdo territorial entre 7000 representantes de catorce pueblos indígenas diferentes y el gobierno federal; desde entonces, cada «nación» de forma particular debe negociar sus reclamaciones específicas de tierras y de autogobierno. En noviembre de 2005, 11 de 14 Primeras Naciones han firmado acuerdos con el gobierno. A continuación se ofrece una lista de las 14 Primeras Naciones: 

El territorio tuvo una vez un establecimiento inuit, localizado en Herschel Island sobre la costa del Ártico. El mismo fue desmantelado en 1987 y sus habitantes trasladados a los Territorios del Noroeste. Como consecuencia del Acuerdo Final Inulaviut, la isla es hoy un parque territorial y se conoce oficialmente como Qikiqtaruk, el nombre de la isla en idioma inuktitut.



En inglés:



</doc>
<doc id="2999" url="https://es.wikipedia.org/wiki?curid=2999" title="Zygophyllaceae">
Zygophyllaceae

Las zigofiláceas (Zygophyllaceae) son plantas herbáceas o raramente leñosas, a menudo xerófilas y halófilas (plantas que crecen en ambientes salinos). 
Son hierbas, arbustos o árboles, anuales o perennes; ramas usualmente divaricadas, con nudos angulados o abultados, simpódicas; plantas hermafroditas. Hojas opuestas u ocasionalmente alternas, usualmente paripinnadas, a veces simples o 2-folioladas, raramente 3–7-folioladas, persistentes, frecuentemente carnosas a coriáceas, pecioladas a subsésiles; folíolos enteros u ocasionalmente lobados, inequiláteros, peciolulados a subsésiles; estípulas apareadas, libres, persistentes o raramente caducas, foliáceas o carnosas o espinescentes. Flores (4) 5-meras, hipóginas, regulares o a veces ligeramente irregulares; pedúnculos terminales o pseudoaxilares, con 1 flor, solitarios u ocasionalmente varios juntos; sépalos (4) 5, libres o ligeramente connados en la base, imbricados cuando en yema, persistentes o a veces deciduos; pétalos (4) 5, libres o raramente connados en la base, frecuentemente unguiculados, a veces retorcidos e imbricados o convolutos, deciduos, raramente marcescentes; disco glandular extrastaminal y/o intrastaminal usualmente presente y conspicuo; estambres en (1) 2 verticilos de 5 cada uno, el verticilo más externo usualmente opuesto a los pétalos, con frecuencia verticilos alternativamente desiguales o estériles, filamentos libres, subulados a filiformes o raramente alados, frecuentemente glandulosos o con apéndices en la base, los del verticilo exterior ocasionalmente adnados a los pétalos, insertos en el disco o debajo de este, las anteras ditecas, subbasifijas a versátiles, introrsas, longitudinalmente dehiscentes; gineceo (2–) 5-carpelar, sincárpico, ovario súpero, (2–) 5 (–10)-lobado, (2–) 5 (–10)-locular, sésil o raramente en un ginóforo corto, los óvulos (1) 2–numerosos por lóculo, péndulos o ascendentes, anátropos, placentación axial o raramente basal, estilo terminal, usualmente simple, estigma menuda e inconspicuamente lobado a obviamente acostillado. Frutos cápsulas (2–) 5-lobadas, loculicidas o septicidas, o esquizocarpos que se separan longitudinalmente en 5 ó 10 mericarpos duros, tuberculados a espinosos o alados, o raramente drupas o bayas; semillas 1 (–numerosas) por lóculo, endosperma presente o ausente, embrión con cotiledones aplanados.

Existen más de 280 especies de los países cálidos. Algunas especies son:




</doc>
<doc id="3000" url="https://es.wikipedia.org/wiki?curid=3000" title="Zoología">
Zoología

Zoología (del griego «ζωον» "zoon" = "animal", y «-λογία» "-logía", tratado, estudio, ciencia) es la disciplina biológica que se encarga del estudio de los animales. Esta ciencia estudia diversos ámbitos como la biología, fisiología, Morfología, etología, distribución y ecología de cada una de las especies.

El interés del hombre por los animales y por la gran diversidad de sus formas comenzó en la antigüedad. En Grecia, en el siglo IV a.C., Aristóteles describió numerosas especies y realizó un esbozo de clasificación del reino animal; pero muchas de sus conclusiones carecían de rigor científico, pues no estaban basadas en experimento.

Con el Renacimiento, las investigaciones zoológicas adoptaron carácter verdaderamente científico y se desecharon algunas teorías aristotélicas y muchos conceptos fantasiosos sostenidos hasta entonces. La invención del microscopio por el neerlandés Anton van Leeuwenhoek permitió abordar el estudio de los tejidos de los animales y de seres hasta entonces desconocidos porque eran demasiado pequeños para ser observados a simple vista: los microbios o microorganismos.

Ya avanzado el siglo XVIII, el sueco Carlos Linneo fue el primero en encarar una clasificación sistemática de los animales y las plantas. Su obra fue continuada por el naturalista francés Georges Cuvier. En 1859 Charles Darwin dio a conocer su teoría de la evolución de las especies, que significó un gran aporte a los estudios zoológicos.

Se encarga de todos los aspectos genéricos y comunes que poseen los animales antes de proceder a una descripción taxonómica.

Una vez que se ha estudiado el mundo animal en los aspectos embriológicos, histológicos, funcionales, etc. Cabe describir un prototipo para cada una de las especies, pero previamente es imprescindible proceder a la exposición de una serie de múltiples consideraciones relativas de la historia denominada clasificación sistemática.

En el estudio de las plantas y en el de los animales, los especialistas se interesan tanto por las semejanzas como por las diferencias que presentan las especies para lograr la agrupación lógica y sistemática de las mismas. Admitido este procedimiento clasificatorio, es evidente la necesidad de adoptar una nomenclatura que sea universalmente comprometida para superar de este modo la limitación que supondrían las denominaciones locales o nacionales.

La taxonomía abarca la exploración y tabulación sistemática de los hechos concernientes al reconocimiento de todas las especies existentes y extintas de animales y su distribución en el espacio y el tiempo.

Las principales variedades de trabajadores zoológicos situados bajo este encabezado son:


Gradualmente, desde los tiempos de Hunter y Cuvier, el estudio anatómico se ha ido disociando cada vez de la morfografía, hasta que al día de hoy nadie considera de valor un estudio animal que no incluya en su enfoque la estructura interna, la histología y la embriología.

El auténtico surgimiento de la zoología después del período legendario de la Edad Media está ligado al nombre de un inglés, Edward Edward Wotton, nacido en Oxford en 1492, quien ejerció como médico en Londres y murió en 1555. Publicó un tratado titulado "De differentiis animalium" en París en 1552. En muchos sentidos Wotton era simplemente un exponente de Aristóteles, cuya doctrina (con varias adiciones imaginarias), constituyera la verdadera base de conocimiento zoológico a lo largo de la Edad Media. El mérito de Wotton fue el rechazó de los argumentos legendarios y fantásticos, y su regreso a Aristóteles y a la observación de la naturaleza.

El método más efectivo para notar el progreso de la zoología durante los siglos XVI, XVII y XVIII es comparar las concepciones clasificatorias de Aristóteles con las de los sucesivos naturalistas, aquellos que pueden ser encontrados en las obras de Caldon.




</doc>
<doc id="3004" url="https://es.wikipedia.org/wiki?curid=3004" title="Zosteraceae">
Zosteraceae

Las zosteráceas (nombre científico Zosteraceae) son una familia de plantas monocotiledóneas marinas sumergidas, con aspecto de hierbas, perennes, rizomatosas, monoicas, distribuidas en los mares templados a subtropicales de todo el mundo. La familia es reconocida por sistemas de clasificación modernos como el sistema de clasificación APG III (2009) y el APWeb (2001 en adelante). Es una de las familias a las que se llama de "pastos marinos", debido a su aspecto de pastos, sus espádices son aplanados, sus flores están en la superficie adaxial, las inflorescencias están encerradas por una espata. La mayoría de los taxones posee unas estructuras opuestas a los estambres a las que se llamó "retináculos", que quizás sean tépalos o brácteas.
Son hierbas perennes, acuáticas, sumergidas, marinas, glabras, con hojas dispuestas en braquiblastos y macroblastos, con polinización en el interior del agua –hipohidrofilia. Hojas alternas, diferenciadas en vaina y limbo, liguladas, lineares, obtusas, paralelinervias. Inflorescencias en espádice. Flores hermafroditas o unisexuales –en especies extrapeninsulares–, monómeras, sésiles, sin perianto, o con perianto muy reducido (retináculo) situado junto a una de las tecas. Androceo con 1 estambre, con el conectivo poco desarrollado entre las dos tecas. Gineceo súpero, con 1 carpelo; carpelos con un rudimento seminal, péndulo, de placentación apical. Fruto aqueniforme, con pico diferenciado, sésil, dehiscente por rotura del pericarpo membranáceo. Semillas ± estriadas, con el embrión curvado, sin endosperma.

La familia fue reconocida por el APG III (2009), el Linear APG III (2009) le asignó el número de familia 38. La familia ya había sido reconocida por el APG II (2003).

Hay dos géneros:



</doc>
<doc id="3005" url="https://es.wikipedia.org/wiki?curid=3005" title="Zannichelliaceae">
Zannichelliaceae

Las zaniqueliáceas (nombre científico Zannichelliaceae) son una familia de hierbas perennes, rizomatosas, monoicas, que habitan aguas dulces o saladas. Hojas alternas u opuestas, filiformes. Flores pequeñas, unisexuales, solitarias o dispuestas en inflorescencias falciformes, aclamídeas o de perianto cupuliforme; las masculinas con 13 estambres; las femeninas de gineceo con 1 hasta 9 carpelos libres. Frutos aqueniformes o nuciformes. Se conocen unas 7 especies, extendidas por una gran parte del mundo. Esta familia era reconocida en las clasificaciones
"clásicas" de las angiospermas, y aun hoy algunos botánicos lo continúan haciendo. En los sistemas de clasificación modernos como el sistema de clasificación APG II del 2003, el APG III (2009
), y el APWeb (2001 en adelante), los géneros que pertenecían a esta familia ("Althenia", "Lepilaena", "Pseudalthenia", "Vleisia", "Zannichellia") se disponen dentro de las Potamogetonáceas. 



</doc>
<doc id="3007" url="https://es.wikipedia.org/wiki?curid=3007" title="Zea">
Zea

El género Zea comprende varias especies de poáceas o gramíneas de origen americano, de las cuales la única que cuenta con valor económico es "Z. mays" ssp. "mays", conocida como "maíz", un cereal de alto valor energético cultivado para el consumo humano y animal. Todas las otras especies y subespecies reciben el nombre común de teosinte. 
Son plantas anuales robustas o perennes, cespitosas o rizomatosas; tallos con muchos entrenudos, sólidos, a menudo con raíces fúlcreas; plantas monoicas. Hojas en su mayoría caulinares; lígula una membrana; láminas grandes, lineares, aplanadas. Inflorescencias unisexuales; inflorescencia estaminada una panícula de racimos, terminal, entrenudos del raquis no articulados, delgados; espiguillas estaminadas pareadas, unilaterales, una espiguilla de cada par sésil o subsésil, la otra pedicelada, los pedicelos libres, glumas herbáceas, multinervias, flósculos superiores e inferiores similares, ambos estaminados, lema y pálea hialinas, lodículas 3, estambres 3; inflorescencia pistilada una espiga solitaria, axilar, delgada, envuelta en 1–numerosas espatas, entrenudos del raquis desarticulándose, hinchados, espiguillas pistiladas sésiles, solitarias, dísticas en 2 hileras, profundamente hundidas y casi envueltas por el entrenudo del raquis (cúpula), callo oblicuo, truncado o aplanado, gluma inferior endurecida, lisa, inconspicuamente alada en la punta, gluma superior membranácea, flósculo inferior estéril, lema inferior pequeña, hialina, pálea inferior pequeña, hialina, flósculo superior pistilado, lodículas ausentes, estilo y estigma solitarios, muy largos, las puntas extendiéndose más allá de las espatas envolventes. Fruto una cariopsis; hilo punteado. En Zea mays ssp. mays la inflorescencia pistilada es una mazorca masiva, dura, fibrosa, entrenudos del raquis no desarticulándose, espiguillas pareadas, sésiles, polísticas en 4–36 hileras, insertadas superficialmente en la mazorca, callo agudo, glumas membranáceas, flósculo inferior generalmente estéril o raramente pistilado, flósculo superior pistilado, lemas y páleas membranáceas.

Varias otras especies del género, conocidas colectivamente como teosintes, han desarrollado un aspecto similar al del maíz como contramedida a su erradicación selectiva por los granjeros.

Las especies del género "Zea" presentan por lo general un tallo hueco, similar al del bambú, del que, según la especie, pueden derivar o no ramificaciones. Si bien "Z. nicaraguensis" y "Z. perennis" son perennes, la mayoría de las especies son anuales. Pese a su breve ciclo vital, alcanzan varios metros de altura.

Virtualmente todas las poblaciones de teosinte están amenazadas o en riesgo: "Zea diploperennis" existe en un área de solo unos pocos km²; "Zea nicaraguensis" sobrevive en aproximadamente una sola subpoblación de 6000 plantas en un área de 200×150 m. Los gobiernos de México y de Nicaragua han reaccionado recientemente para proteger las poblaciones silvestres de teosinte, usando tanto métodos de conservación "in-situ" y "ex-situ". Hay mucho interés científico en estos trabajos benéficos, y en otros como resistencia a insectos, perennialismo, tolerancia a inundación. 

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 2: 971. 1753. 
Zea: nombre genérico que sería una voz de origen griego, derivada de "zeo" que significa "vivir". Pero Plinio el Viejo ("Historia naturalis","18, 81") emplea el término "Zĕa, æ" para referirse a "Triticum spelta" (espelta, también conocido como escaña mayor o escanda mayor).
Comprende 40 especies descritas y de estas, solo 6 aceptadas.




</doc>
<doc id="3008" url="https://es.wikipedia.org/wiki?curid=3008" title="Zaragoza">
Zaragoza

Zaragoza es una ciudad y un municipio de España, capital de la provincia homónima y de la comunidad autónoma de Aragón. Perteneciente a la Comarca Central, tiene un régimen legal especial como capital de Aragón. Con una población de 674 997 habitantes (INE, 2019) es la quinta ciudad más poblada del país, tras Madrid, Barcelona, Valencia y Sevilla. 

La ciudad cuenta con un PIB nominal de millones USD y un PIB per cápita nominal de , lo que representa un PIB PPA per cápita de USD, siendo la 5.ª ciudad española en actividad económica.

Está situada a orillas de los ríos Ebro, Huerva y Gállego y del Canal Imperial de Aragón, en el centro de un amplio valle. Su privilegiada situación geográfica la convierte en un importante nudo logístico y de comunicaciones; se encuentra a unos 300 km de Madrid, Barcelona, Valencia, Bilbao y Toulouse. Parte de su término municipal está ocupado por la reserva natural dirigida de los Sotos y Galachos del Ebro. 

La ciudad ostenta los títulos de Muy Noble, Muy Leal, Muy Heroica, Muy Benéfica, Siempre Heroica e Inmortal, otorgados en su mayoría tras su resistencia frente al ejército napoleónico en los Sitios de Zaragoza durante la Guerra de la Independencia. Todos estos títulos quedan reflejados en su escudo, mediante las iniciales de cada uno de ellos. Celebra su fiesta mayor en honor a la Virgen del Pilar el 12 de octubre. El patrón de la ciudad es san Valero (29 de enero). Entre el 14 de junio y el 14 de septiembre de 2008, año del bicentenario de los Sitios de Zaragoza y centenario de la Exposición Hispano-Francesa de 1908, Zaragoza acogió la exposición internacional Expo Zaragoza 2008 dedicada al agua y el desarrollo sostenible. Además, la ciudad de Zaragoza fue sede del Secretariado de Naciones Unidas para la Década del Agua 2005-2015.

Su nombre actual procede del antiguo topónimo romano, Caesaraugusta, que recibió en honor al emperador César Augusto en el 14 a. C. El origen de la ciudad se remonta a Salduie, que fue el nombre de la ciudad ibérica sedetana situada en el solar de la actual Zaragoza desde la segunda mitad del siglo III a. C. Está documentado en monedas ibéricas y con el nombre de «Salduvia» en un texto de Plinio el Viejo. Con la fundación de Caesar Augusta, la ciudad-estado íbera pasó a convertirse en colonia inmune de ciudadanos romanos. Su denominación romana fue evolucionando, pasando quizá por el árabe Saraqusta.

Se asienta en mitad del valle del Ebro, en la ribera media del Ebro, en el punto en el que desembocan los ríos Huerva y Gállego, los cuales también atraviesan la ciudad. 

En la ribera y en el área urbana, el terreno es llano por lo general, sobre todo en la parte norte de la ciudad asentada en la margen derecha de la desembocadura del río Gállego, mientras que la sur tiene una inclinación más pronunciada conforme se aleja del Ebro. La altura del río Ebro a su paso por Zaragoza es de 199 m s. n. m., aunque gran parte de la ciudad está por encima de los 210 m s.n.m. y los barrios del sur como Torrero y La Paz, se encuentran a más de 250 m s.n.m. La ribera está rodeada de escarpes, cornisas o cárcavas dando lugar a relieves abruptos, en ocasiones con fuertes desniveles. En contraste altitudinal respecto al área metropolitana, dentro del municipio, destacan la Plana de Moses: un segmento meridional de los Montes de Castejón con 680 m s.n.m.; y las Planas de María con 645 m s.n.m., resultado de la dureza de sus materiales geológicos a la erosión. 

Los materiales que afloran en la ciudad y su entorno son fundamentalmente gravas, arenas y arcillas producto de la propia sedimentación del Ebro durante el Cuaternario, que se disponen en terrazas fluviales de varios metros de espesor sobre las unidades de yesos y limos depositadas durante el Terciario. Estos materiales se consideran evaporitas, al formarse por evaporación de las aguas en extensas lagunas de una depresión endorreica que delimitaban las cordilleras pirenaica, ibérica y costero catalana. Esta zona endorreica se abrió al mediterráneo por la erosión remontante de uno o varios barrancos, precursores del actual río Ebro.

La naturaleza salina de las evaporitas, unida a una pluviometría escasa y un fuerte coeficiente de evaporación, han favorecido el desarrollo de una singular vegetación esteparia en el entorno de Zaragoza, que constituye una rareza botánica y paisajística a nivel europeo, si bien es poco valorada en general por sus habitantes. El contraste entre este paisaje y la vegetación exuberante de los sotos fluviales es uno de los atractivos de los descensos en piragua que pueden realizarse aguas arriba y aguas abajo de la ciudad.

De acuerdo con la clasificación climática de Köppen, Zaragoza tiene un clima semiárido frío (BSk), propio de la depresión del Ebro. Los inviernos son ligeramente fríos,con mínimas medias alrededor de los 2 grados positivos, siendo normales las heladas nocturnas (23 días de heladas de media al año),mientras que las máximas suelen situarse alrededor de los 10 grados de media aunque durante los meses de diciembre y enero debido a la inversión térmica producida por las nieblas, las máximas pueden desplomarse bastante incluso llegando a ser más bajas que las zonas cercanas a Zaragoza con más altitud donde hay sol, en estos eventos las máximas y las mínimas son prácticamente las mismas y las máximas pueden bajar fácilmente a los 2 grados. Los veranos son cálidos,las máximas suelen superar casi siempre los 30 °C, incluso pasando los 35 °C algunos días y ocasionalmente se ha llegado y superando ligeramente la barrera de los 40 grados en algunos eventos cálidos; mientras que las mínimas debido a la continentalidad de Zaragoza suelen bajar de los 20 grados aunque a veces esta marca se supere en eventos cálidos provocando que sea difícil conciliar el sueño al tener altas temperaturas tanto por el día como por la noche.

Las escasas luvias se concentran en primavera y en otoño en menor medida (6 o 7 días de lluvia por mes), ocurriendo sequías estivales prácticamente todos los años (1 o 2 días de lluvia por mes), aunque en ocasiones en verano puedan ocurrir fuertes tormentas a veces incluso con granizo debido al calor y que hacen amainar un poco la sequía. En invierno las precipitaciones también son bastantes escasas tan solo un poco más comunes que en verano, lo que provoca que a pesar de las bajas temperaturas apenas nieve, alrededor de dos días en el invierno y tan solo 4 días de lluvia por mes. El promedio anual es bastante escaso, de unos 315 mm influenciado sobre todo por el efecto foehn o efecto adiabático. 

Las temperaturas más altas desde que existen registros históricos son los 44,5 °C del 7 de julio de 2015; los 43,2 °C del 29 de junio de 2019; los 42,8 °C del 26 de agosto de 2010 y los 42,6 °C del 17 de julio de 1978 y las más bajas -15,2 °C registrada tanto el 1 como el 18 de enero de 1918, -14,9 °C, del 31 de diciembre de 1887 y -11,4 °C registrada el 5 de febrero de 1963. Zaragoza tiene de media 2,4 días de nieve al hallarse situada a poca altitud.

Según la Agencia Estatal de Meteorología, la velocidad media del viento es de 19 km/h. El cierzo sopla con frecuencia durante el invierno y a comienzos de la primavera.

La ciudad de Zaragoza cuenta con más de dos mil años de historia. La población más antigua documentada data del siglo VII a. C., en los restos de unos poblamientos del final de la Edad del Bronce. Las primeras noticias de un asentamiento urbano datan de la segunda mitad del siglo III a. C. y nos hablan de una ciudad ibérica llamada Salduie que se identifica con el nombre de «Salduvia» en un texto de Plinio el Viejo. 

La ciudad romana de Caesaraugusta fue una colonia inmune refundada sobre la ciudad ibera por Octavio Augusto con veteranos de las guerras cántabras entre el año 25 y el 12 a. C., muy probablemente el 14 a. C. Entonces tenía una planta rectangular y una extensión de 47 hectáreas, coincidente con el trazado urbanístico del actual casco antiguo, y su perímetro estaba delimitado por la calle del Coso al sur y al este, la avenida de César Augusto al oeste y el río Ebro al norte. Poco después se convirtió en el centro urbano más importante del valle medio del Ebro. La ciudad no decayó durante el Bajo Imperio romano de Occidente.

El año 452 fue conquistada por los suevos y el 466 por los visigodos, quienes la incorporaron al reino de Tolosa. En 541, fue asediada por los francos, aunque la ciudad no llegó a caer en su poder.

En el siglo VII su sede episcopal conoció un periodo de esplendor con las figuras de los obispos Braulio de Zaragoza y Tajón. El año 714 fue ocupada por el sarraceno Musa ibn Nusair y se convirtió en un centro musulmán importante llamado «Medina al-Baida Saraqusta» (Zaragoza la Blanca), que Carlomagno intentó ocupar sin éxito el año 788.

En el siglo IX los Banu Qasi, familia noble de origen visigodo convertida al islam al poco de la invasión, la designaron capital de sus extensos dominios poco después del 852 con Musa ibn Musa, conocido en la tradición cristiana como el «Moro Muza». Más tarde, el emir Mohámed I de Córdoba se la compró en 884 por 15 000 dinares de oro. En 890, obtuvieron la hegemonía los tuyibíes, yemeníes provenientes de las zonas de Calatayud y Daroca, y Muhammad Alanqar pasó a ser el gobernador de Zaragoza.

Capital de la frontera superior con los reinos cristianos bajo el califato Omeya, gozó de cierta autonomía respecto de Córdoba. La Zaragoza musulmana del siglo X acogió a comunidades de las otras religiones del Libro en la judería y el barrio mozárabe. Tras la descomposición del califato andalusí, se erigió en la capital de un importante reino, la Taifa de Saraqusta, en 1018, con el reinado del tuyibí Mundir I.
El periodo de esplendor de la ciudad islámica se dio en el siglo XI, especialmente con el reinado de Al-Muqtadir (1046-1081), ya perteneciente a la dinastía de los Banu Hud, quien amplió su reino con la anexión de la taifa de Tortosa y la taifa de Denia y sometió a vasallaje a la de taifa de Valencia. Construyó un espléndido palacio fortificado de recreo: la Aljafería, cuyas obras comenzaron en 1065. La dinastía hudí consiguió mantener su independencia frente al Imperio almorávide y a la presión de un joven reino de Aragón, hasta que en 1110 la ciudad tuvo que ser entregada al poder morabita, que puso al frente de la gobernación de la urbe al exregidor de Valencia Muhammad ibn al-Hayy. En 1115 le sustituyó Ibn Tifilwit, quien nombró visir al gran filósofo Avempace.

Con la ayuda de sus aliados occitanos y cruzados francos, y su ejército de aragoneses, Alfonso I el Batallador pudo conquistar Zaragoza en 1118, que se convertiría pronto en la capital del Reino de Aragón, y fue la sede en la que se coronaron los reyes de la Corona de Aragón. La población musulmana se tuvo que trasladar fuera de los muros de la ciudad, donde fundó el nuevo barrio de la morería, mientras que el núcleo urbano era repoblado por francos y dado en feudo a Gastón IV de Bearne.

Desde el final del siglo XIII fue el centro de la Unión Aragonesa (asociación de nobles para limitar el poder real y mantener sus privilegios), hasta que esta fue derrotada por Pedro el Ceremonioso el año 1384. La unión dinástica de la Corona de Castilla y la de Aragón la transformó en una ciudad más de la monarquía de los Austrias. El establecimiento de la Inquisición fue causa de importantes revueltas y del asesinato del inquisidor Pedro Arbués en 1485. En el se incorporaron a la ciudad los arrabales de labradores de San Pablo y de pescadores de las Tenerías. Durante el reinado de Fernando el Católico se fundó la universidad y se construyó la Lonja. La expulsión de los judíos en 1492 y de los moriscos en 1609 provocaron un cierto estancamiento en su crecimiento, pero a pesar de eso, no dejó de ser una ciudad importante (con 25 000 habitantes en 1548).

Fue escenario de revueltas a causa del encarcelamiento de Antonio Pérez, secretario de Felipe II, que procesado por orden del rey, se acogió a la protección de los Fueros Aragoneses en el año 1591. Los disturbios acabaron con la ejecución del Justicia Juan de Lanuza y la introducción de algunas restricciones en sus privilegios. Durante la Guerra de Sucesión, la ciudad, en defensa de las libertades y soberanía de Aragón, de sus instituciones y del Derecho aragonés, se declaró partidaria del archiduque Carlos de Austria. Al ser conquistada por las tropas borbónicas, perdió la autonomía de la que había disfrutado hasta aquel momento (1707), y que solo pudo recuperar brevemente en 1710, al derogarse sus fueros por los Decretos de Nueva Planta, con lo que la ciudad dejó de ser sede de importantes instituciones del Reino de Aragón.

Durante el la población pasó de 30 000 habitantes en 1725 a 43 000 en 1787. En 1760 se produjo un motín paralelo de Esquilache, y en 1776 se fundó la Sociedad Económica de Amigos del País.

Durante la Guerra de la Independencia Española (1808-1814), Zaragoza resistió los enfrentamientos con las tropas francesas. En la guerra contra Napoleón se hizo famosa por toda Europa por sus asedios, siendo un símbolo de la resistencia a Napoleón. En el primer asedio (junio-agosto de 1808), el general Verdier tuvo que desistir de tomarla. En el segundo asedio (final diciembre de 1808-21 de enero de 1809) capituló después de una serie de combates violentísimos, donde la población colaboró de forma heroica con las tropas de los defensores, a las órdenes de José de Palafox, que se encerró con 30 000 hombres. Moncey y después Lannes dirigieron el segundo asedio. Se calcula que murieron 8000 franceses y 40 000 defensores, ya que dentro de la ciudad se propagó una epidemia de tifus. 

Durante las Guerras Carlistas el general carlista Juan Cabañero intentó ocupar la ciudad la madrugada del 5 de marzo de 1838, pero fue rechazado por la guarnición. El 2 de enero de 1854 hubo un intento frustrado de pronunciamiento. 

El cólera de 1885 causó muchas víctimas. Sin embargo, el año 1900 la ciudad tenía unos 100 000 habitantes. También en el se produjeron las primeras transformaciones importantes que han configurado la ciudad actual: el emplazamiento de la estación de ferrocarril (estación del Norte), que generó un núcleo residencial e industrial, y la construcción paulatina del paseo de la Independencia (iniciado en 1815), con sus porches, que creó un eje que iba desde el Coso hasta la Huerta de Santa Engracia y articulaba el crecimiento hacia lo que constituiría el ensanchamiento de principios del siglo XX, con la Gran Vía y el paseo de Sagasta como calles principales. A finales del se convirtió en el foco de una fuerte inmigración rural atraída por el reciente proceso de industrialización de la ciudad.

Tras 1898, el azúcar de caña que suministraban las colonias perdidas comenzó a ser sustituido por el derivado de la remolacha, lo que originó en Zaragoza el auge de las industrias azucareras y la aparición de una burguesía que animó los movimientos regeneracionistas de la Liga Nacional de Productores (1899) y la Unión Nacional (1900) de Joaquín Costa y Basilio Paraíso. La acumulación de capital provocada por el fuerte aumento de los beneficios dio origen a la creación de varias entidades financieras regionales, como el Banco Zaragozano o el Banco de Aragón. Se produjo una inmigración rural, la aparición de un proletariado y el crecimiento urbanístico de la ciudad. La expansión económica trajo consigo períodos de carestía e inflación y provocaron las consecuentes reivindicaciones obreras. Estas fueron desatendidas por una burguesía local intransigente amparada en la represión violenta de las fuerzas del orden. Todo ello propició el fracaso del reformismo obrero y el crecimiento sostenido de la afiliación a la Confederación Nacional del Trabajo. Zaragoza fue durante las primeras décadas del siglo XX, tras Barcelona, la segunda ciudad cenetista de España.

Al inicio de la Guerra Civil Española, el 19 de julio de 1936 los sublevados, mandados por el general Miguel Cabanellas, tomaron fácilmente el control de la ciudad. En 1937, los republicanos emprendieron la ofensiva de Zaragoza para tratar de recuperar el control de la ciudad, sin éxito. 

Durante la dictadura franquista se reabrió la Academia General Militar y se instaló la Confederación Hidrográfica del Ebro. Después de diversos planes urbanísticos que completaron el trazado del siglo XIX, se produjo en los últimos treinta años del siglo un enorme crecimiento del casco urbano con la superación de la barrera natural que constituye el Ebro, y que ha llevado a la construcción de populosos nuevos barrios. Desde la segunda mitad del hasta nuestros días, Zaragoza ha seguido pujante, siendo actualmente la quinta ciudad de España en términos demográficos.

Desde la fundación de Caesaraugusta como colonia inmune romana sus límites acogieron una población que se podría situar en torno a los 20 000 habitantes. Durante el periodo visigodo su pujanza como ciudad clave en el norte de la Península no decayó, si acaso la ciudad pudo disminuir en algo su población.
La llegada de los musulmanes a la península y la creación de la marca y posterior Taifa de Zaragoza hizo de Saraqusta una de las capitales más importantes de los primeros reinos de taifas. En su época de máximo esplendor (hacia 1080) dominaba, como centro de un reino islámico fundamentalmente urbano, gran parte del Levante peninsular, incluyendo la costa de Tortosa y Denia. Por aquel tiempo Zaragoza pudo llegar a contar con 25 000 habitantes, e incluso, los cálculos más optimistas, llevarían a la ciudad a los cerca de 50 000.

Es difícil establecer un cálculo preciso sobre las variaciones experimentadas durante la etapa medieval cristiana. Tras la reconquista de Alfonso I de Aragón en 1118 la población islámica fue obligada a habitar en los arrabales, quedando la antigua medina para los nuevos pobladores. Parte de la población, sobre todo la que pertenecía a estratos sociales más elevados, marchó hacia al-Ándalus. Se sabe que el siglo XII fue un periodo de crisis, lo que unido a la emigración comentada, pudo hacer descender notablemente la población, pese a los privilegios jurídicos que los reyes aragoneses se esforzaron en promulgar para estimular su asentamiento en la capital del reino. En todo caso, a fines del Zaragoza recuperó la población que tuvo en el máximo apogeo de la capital del reino taifal con 20 000 habitantes aproximadamente.

La primera mitad del contempla un Renacimiento ciudadano, con la construcción de La Lonja y la presencia de un paisaje urbano dominado por numerosas torres de estilo mudéjar, que valieron a Zaragoza el apelativo de «la de las cien torres» o la «Florencia española». Así, en 1548 la ciudad cuenta con 25 000 habitantes. No debió de haber cambios significativos en la demografía durante el siglo XVII, etapa de crisis demográfica en general en España y en particular en Aragón, donde a pestes, hambrunas y crisis económica se unió la expulsión definitiva de los moriscos en 1609.

En cambio, durante el (siglo en que comienzan las primeras estimaciones censales de la demografía) la población experimentó un importante auge, y evolucionó de unos 30 000 habitantes en 1725 a los 43 000 en 1787.

Pero la Guerra de la Independencia y los dos sitios que sufrió por parte de las tropas napoleónicas asolaron Zaragoza que, de los 55 000 ciudadanos que la habitaban en 1808, pasó a 12 000 supervivientes. A pesar de todo ello, el dinamismo de la capital del valle medio del Ebro le permitió recuperarse y hacia 1850 ya tenía 60 000 habitantes; muy notablemente continuó este ascenso demográfico en la segunda mitad del en que se vio favorecida por la actividad industrial (azucareras de remolacha e industria alimentaria fundamentalmente) y comercial que llevó su población a superar los 100 000 habitantes a comienzos del siglo XX.

A la llegada de la Segunda República, la ciudad frisa los 200 000 habitantes y en 1960 alcanza los 300 000. Pero el verdadero despegue demográfico se produce entre 1960 y 1980, un intervalo de veinte años en los que Zaragoza casi duplica su población, rebasando a mediados de los ochenta el medio millón de almas y a finales las 600 000. En la primera década del la urbe siguió creciendo, en parte por su condición de imán de la población aragonesa, que emigra desde las zonas rurales más empobrecidas de la región, y según el padrón municipal en 2018 tenía una población de 697 895 habitantes.

Desglose de población según el Padrón Continuo por Unidad Poblacional del INE.

En 2013 vivían en Zaragoza 107 864 extranjeros, lo que suponía el 15% del total. Desde 2004 hasta 2013 dicha cifra pasó de 43 355 a 107 864, es decir, aumentó casi un 150%. Los barrios con mayor población extranjera eran Delicias (25 428 habitantes, el 23% del total del distrito) y el Casco Histórico (11 881, el 25%).

A raíz de las elecciones de 2019 fue elegido alcalde Jorge Azcón, del Partido Popular, quien contó además con el respaldo de los concejales de Ciudadanos y Vox.

Zaragoza se divide en 16 distritos, de los cuales 15 son distritos urbanos, correspondientes al núcleo principal de población; el decimosexto, el Distrito Rural, comprende catorce barrios rurales:

El concepto de deuda viva contempla solo las deudas con cajas y bancos relativas a créditos financieros, valores de renta fija y préstamos o créditos transferidos a terceros, excluyéndose, por tanto, la deuda comercial.

La deuda viva municipal por habitante en 2015 ascendía a 1608,65 €.

Zaragoza es la quinta ciudad de España según su Índice de Actividad Económica. Los sectores estratégicos de la economía zaragozana son la industria del automóvil, la logística y los transportes, las energías renovables, los servicios a empresas, la agroindustria y el turismo. En la economía de la ciudad ocupa un lugar destacado la fábrica de Opel (Grupo PSA) en Figueruelas, una localidad del área metropolitana, alrededor de la cual se ha desarrollado un entramado de industrias auxiliares del motor. En el terreno industrial también sobresalen:

Destaca el transporte de mercancías del aeropuerto de Zaragoza, que en 2017 cargó 117 000 000 kg, lo hace colocarse el tercer mejor aeropuerto español (en cuanto a mercancías) tan solo por detrás de Madrid y disputándose el segundo puesto con Barcelona 

Proyectos como la Plataforma Logística de Zaragoza (PLAZA), que con 12 500 000 m² es la mayor del sur de Europa, han impulsado el sector logístico en estos últimos años. Además, la apertura de Puerto Venecia, el mayor centro comercial de Europa, ha creado cerca de 4000 puestos de trabajo, siendo un importante centro de atención turística nacional e internacional.

A esta clase de grandes iniciativas comerciales van unidas otras que estimulan la implantación de oficinas en la ciudad, como por ejemplo, el edificio World Trade Center Zaragoza o el espacio de la Exposición Internacional de 2008, convertido en un complejo empresarial y la Ciudad de la Justicia.

Empresas del área de influencia de Zaragoza por facturación:

El Servicio Aragonés de Salud es el organismo que integra todos los centros de la sanidad pública en Aragón. Está estructurado en ocho sectores sanitarios y, dentro de esta estructura, Zaragoza contiene tres de ellos, denominados Zaragoza I, Zaragoza II y Zaragoza III. Cada uno de estos sectores cuenta con sus correspondientes centros de atención primaria, centros de especialidades, hospitales y centros de salud mental.

La relación de hospitales, tanto del sector público como del privado, existentes en Zaragoza es la siguiente: 

El nombre oficial es Aeropuerto de Zaragoza (código IATA: ZAZ). Su inauguración se remonta a septiembre de 1947, cuando aún contaba con el nombre de Aeropuerto General Sanjurjo y está situado en el barrio de Garrapinillos, a 10 kilómetros del centro de la ciudad. El aeropuerto está conectado por una línea de autobús con una frecuencia de 30 minutos. El traslado en taxi tiene un coste aproximado de 20 euros. Cuenta con servicios de alquiler de coches (Atesa, Europcar, Hertz, Sixt). Además de cajeros automáticos, cafeterías, restaurantes, tiendas de viaje, de compras, tiene servicio de internet WIFI. En el año 2011 transportó a 751 097 pasajeros (según AENA). En marzo de 2008, con motivo de la Exposición Internacional del Agua, se abrió una nueva terminal. El moderno edificio tiene capacidad para un millón de pasajeros por año aunque es ampliable hasta el millón y medio. Actualmente, en el aeropuerto operan seis compañías aéreas (Ryanair, Air Europa, Air Nostrum, Volotea, Vueling y Wizz Air) y existen conexiones permanentes con nueve destinos, seis europeos (Londres, París, Bruselas, Milán, Cluj-Napoca y Bucarest) y tres nacionales (Mallorca, Tenerife y Lanzarote), a los que se le suman otros cuatro enlaces veraniegos (Gran Canaria, Fuerteventura, Ibiza y Menorca). 

En lo referente al transporte de mercancías, el aeropuerto de Zaragoza está experimentando un fuerte crecimiento que lo ha situado como el tercero de España desde el año 2009, posición consolidada en 2011 con 48 647 toneladas de carga, un 14,3% más que en 2010.
En 2016 el aeropuerto puso en servicio el nuevo sistema de aterrizaje instrumental, ILS (Instrumental Landing System) categoría II/III, que permite operar en condiciones de baja visibilidad, prácticamente nula, a aeronaves certificadas para este tipo de operaciones. Esta categoría, la máxima existente, permite que las aeronaves aterricen con un alcance visual de pista inferior a 200 metros y un techo de nubes entre 0 y 30 metros.

El 11 de octubre de 2003 se inauguró la línea de alta velocidad (AVE) Madrid-Zaragoza-Lérida que asegura la conexión de Zaragoza con Madrid en 90 minutos. La extensión de la línea de alta velocidad hasta Barcelona se inauguró el 20 de febrero de 2008. Desde mayo de 2007, la nueva Estación Intermodal de Zaragoza-Delicias, situada en el distrito de La Almozara, alberga también la estación central de autobuses de la ciudad.

La ciudad cuenta con otras tres estaciones ferroviarias: la estación de El Portillo, la de Goya, y la de Miraflores. Todas ellas dan servicio a la línea de cercanías de Zaragoza y a varias de media distancia de Renfe.

Entre 2008 y 2010 comenzaron a operar el puerto seco y el área logística intermodal ferroviaria de PLAZA (Plataforma Logística de Zaragoza) que suman, entre ambas superficies, 2,6 millones de metros cuadrados. Además, la ciudad también cuenta con el puerto seco de «MercaZaragoza» y con los muelles ferroviarios de varias empresas como SAICA o la Montañanesa.

Excepto con Soria, Zaragoza está comunicada mediante autovías y autopistas con todas las capitales de las provincias limítrofes, además de con Barcelona, Bilbao, Madrid y Valencia. 

Zaragoza dispone de varias rondas circunvalatorias, de las cuales adquieren mayor protagonismo: 

Zaragoza dispone, desde el 11 de junio de 2008, de su primera línea de Cercanías: Casetas-Utebo-Delicias-Portillo-Goya-Miraflores. 

El 4 de abril de 2012 se abrió al público la estación de Goya, situada entre el Portillo y Miraflores. Este apeadero está llamado a ser la piedra angular del sistema de Cercanías dada su centralidad y su correspondencia con la Línea 1 del tranvía. En el futuro, la C-1 deberá extenderse hacia el oeste y hacia el este llegando a Alagón y a El Burgo de Ebro, respectivamente.

La segunda línea de cercanías, todavía en proyecto, tendrá una disposición Norte-Sur, perpendicular a la C-1 a escala metropolitana, aunque compartiendo vías en el casco urbano de Zaragoza. A largo plazo se espera que llegue desde Zuera hasta María de Huerva, si bien, en una primera fase, podría cubrir el trayecto El Portillo-Estación Intermodal-Plataforma Logística (PLAZA).

El servicio de buses periurbanos de Zaragoza sirve a una treintena de municipios situados en un radio de 30 km con respecto a Zaragoza. Existen seis corredores principales:

El servicio es financiado por los diversos ayuntamientos, la DPZ, la DGA y por los propios usuarios. Es servido por varias compañías de transporte, aunque todas aglutinadas en el Consorcio de Transportes del Área de Zaragoza. El color corporativo habitual es el rojo con franjas naranjas ondulantes y en todas las líneas del consorcio está disponible la "tarjeta Interbús", que es compatible con los Autobuses Urbanos de Zaragoza y con el tranvía. Además, existen cinco intercambiadores de transporte en la ciudad de Zaragoza a los que llegan los buses metropolitanos.

La primera fase de la línea 1 del tranvía fue inaugurada el 19 de abril de 2011 inaugurándose completamente el 26 de marzo de 2013. La línea 1, con una disposición Norte-centro-Suroeste, mide 12,8 km de longitud interconectando los barrios de Valdespartera, Casablanca, Romareda, Universidad, Centro, Casco Histórico, Actur, Juslibol y Parque Goya así como conectando los dos campus principales de la Universidad de Zaragoza. Las paradas de Emperador Carlos V, Fernando el Católico-Goya y León Felipe/Rosalía de Castro sirven de intercambiador con los autobuses metropolitanos de CTAZ o con el Cercanías de Zaragoza La sociedad concesionaria del servicio de Tranvía es TRAZA. En el año 2016 prestó servicio a 27,9 millones usuarios.

Se encuentra en estudio la creación de una segunda línea con disposición Oeste-centro-Este. La Línea 2 conectará los barrios de Valdefierro, Oliver, Delicias, La Bombarda, Centro y por medio de dos ramales los barrios de Las Fuentes y San José. Además, esta línea permitirá conectar con la Estación Zaragoza-Delicias por tranvía. 

Asimismo se ha planteado la creación de una tercera línea, la cual todavía no se encuentra en fase de estudio, que uniría Torrero y La Jota pasando por el centro urbano. También se ha planteado la ampliación de la Línea 1 por el suroeste conectando de esta manera con el nuevo barrio de Arcosur.
El transporte urbano de Zaragoza se basa primordialmente en el transporte en autobús realizado por la empresa concesionaria AUZSA (Autobuses Urbanos de Zaragoza S.A), anteriormente conocida como TUZSA (Transportes Urbanos de Zaragoza S.A), que posee una flota de cerca de 350 vehículos, una plantilla cercana a los 750 empleados, 47 líneas urbanas (aunque llegó a haber 60 líneas urbanas pero se fueron retirando algunas con la implantación del tranvía) y que realiza cerca de 115 millones de viajes anuales. Esta empresa cuenta también con el funcionamiento del Bus Turístico, uno de los primeros servicios emergentes de la ciudad de Zaragoza.

El promedio de tiempo que las personas pasan en transporte público en Zaragoza, por ejemplo desde y hacia el trabajo, en un día de la semana es de 48 min, mientras que el 9% de las personas pasan más de 2 horas todos los días. El promedio de tiempo que las personas esperan en una parada o estación es de 11 min, mientras que el 12% de las personas esperan más de 20 minutos cada día. La distancia promedio que la gente suele recorrer en un solo viaje es de 4,2 km, mientras que el 5% viaja por más de 12 km en una sola dirección.

En mayo de 2008, poco antes de la inauguración de la Exposición Internacional, se implantó en la ciudad un sistema público de bicicletas de alquiler llamado Bizi Zaragoza. Al término de la primera fase, el sistema ponía a disposición de los ciudadanos 340 bicicletas distribuidas en 29 estaciones repartidas, principalmente, por las riberas del Ebro, el centro de la ciudad y las vías de acceso al recinto de la Exposición. En mayo de 2009 el número de estaciones pasó a 49 y en octubre de ese mismo año, con varios meses de antelación a lo previsto, el sistema pasó a contar con 1000 bicicletas en 100 puntos de acceso, alcanzándose en abril de 2010 la cifra de 29 034 abonados. Tras la última ampliación, con 130 estaciones y 1300 bicicletas, el número de usuarios de Bizi Zaragoza llegó a un máximo de 37 500 a fines de 2013, registrándose 35 500 en diciembre de 2014. Uno de los factores que más influyeron en la disminución de los abonados fue la ejecución de una sentencia del Tribunal Superior de Justicia de Aragón que obligó al Ayuntamiento a anular la ordenanza municipal que permitía la circulación de las bicicletas por las aceras.

Existe un servicio turístico de transporte en barco, el cual realiza el trayecto fluvial comprendido entre el puerto de Vadorrey y la zona Expo, haciendo parada obligatoria en el Club Náutico (al pie de la Basílica del Pilar). Dicho servicio presenta problemas con el calado de los barcos y nunca ha estado a pleno rendimiento.

También hay varios trenecitos turísticos que en verano unen el casco urbano con los espacios naturales periurbanos del galacho de Juslibol y los galachos de La Alfranca.

Zaragoza es una ciudad bimilenaria por la que han pasado la práctica totalidad de las civilizaciones que han dominado la península ibérica y de las que quedan restos y monumentos, a pesar del destructivo efecto que tuvieron para el patrimonio arquitectónico los sitios que padeció durante la Guerra de la Independencia.

Los tres principales lugares de interés son:



Véase también:

Dentro de la arquitectura de la ciudad de fines del y principios del XXI se pueden destacar: 

Además, para la Exposición Internacional de 2008 se realizaron diversas infraestructuras, entre las que cabe destacar el Pabellón de España, el Pabellón Puente, la Torre del Agua, el Palacio de Congresos de Zaragoza, el Pabellón de Aragón, el Puente del Tercer Milenio y la Pasarela del Voluntariado.

Entre las calles y plazas más emblemáticas y concurridas se encuentran:

Antes del año 2008, Zaragoza contaba con once grandes parques urbanos de extensión superior a 10 hectáreas, entre los que se pueden citar los siguientes: 


A causa de la celebración de la EXPO 2008 se aumentaron significativamente las zonas verdes de la ciudad. En particular se reacondicionó parte del meandro de Ranillas para crear el Parque Metropolitano del Agua y se realizó un plan para la recuperación de las riberas del Ebro, Huerva y Gállego.

Entre las 204 especies de árboles existentes en las calles de Zaragoza en el año 2007, las más difundidas son: el plátano de sombra, el pino carrasco, el aligustre, el ciruelo rojo, el ailanto, la robinia, el álamo y la catalpa.

El entorno natural de Zaragoza tiene como eje fundamental a sus ríos, el Ebro, el Gállego y el Huerva, donde se encuentran diferentes ecosistemas como varios bosques de ribera o sotos. En este contexto destacan los galachos, que son meandros abandonados del río Ebro que conservan parte del agua y forman singulares espacios naturales: El galacho de La Alfranca y el galacho de La Cartuja forman, junto al galacho de El Burgo de Ebro, la Reserva natural dirigida de los Sotos y Galachos del Ebro. Otra área protegida es el Galacho de Juslibol. También se pueden destacar los humedales de la balsa del Ojo del Cura, balsa del Ojo del Fraile, balsa de la Consejera (las tres en Casetas) y la balsa de Larralde (en Garrapinillos), aunque esta última fue en su origen una balsa artificial para el riego. El Anillo Verde de Zaragoza es un itinerario natural de unos 60 km de longitud que conecta diversos espacios naturales, parques y paseos urbanos. Se divide en el Anillo Verde Norte, que tiene gran parte del trayecto junto a los ríos Ebro y Gállego, y el Anillo Verde Sur, cuyo recorrido discurre en buena parte por el corredor que está a lo largo del Canal Imperial de Aragón y también junto al Ebro. Otro espacio natural es el Vedado de Peñaflor.

La ciudad cuenta con la Universidad de Zaragoza, en funcionamiento desde 1583, tras haber sido aprobada por Carlos V en 1542 y por el papa en 1554. Sus orígenes se sitúan en el «Estudio General de Artes» creado en 1474 sobre una escuela anterior. Actualmente, se reparte entre varios campus, de los que dos se encuentran en la ciudad (Campus de San Francisco y Campus Río Ebro).

En Zaragoza se celebró la Exposición Hispano-Francesa de 1908, que conmemoró el primer centenario de los Sitios de Zaragoza y también la Exposición internacional de 2008, con el tema «Agua y desarrollo sostenible». Así mismo fue elegida como sede de una Oficina de la Organización de Naciones Unidas para la Década del Agua 2005-2015.

Véase también: 

En Zaragoza hay una variada red de museos y salas de exposiciones: 



Zaragoza cuenta con un sistema bibliotecario públicocompuesto por un centro coordinador y veinticuatro bibliotecas municipales,con 277.598 volúmenes [2006] en su mayoría abiertas entre 1983 y 2003.Cuenta también con la Biblioteca de Aragón, dependiente del Gobierno de Aragón, y la Biblioteca de la Universidad de Zaragoza.

En Zaragoza también tienen su sede importantes archivos, entre ellos el Archivo de Casa de Ganaderos, el Archivo de las Escuelas Pías de Aragón, el Archivo de la Fundación Bernardo Aladrén, el Archivo Histórico Provincial de Zaragoza, el Archivo Municipal de Zaragoza y el Archivo de las Catedrales de Zaragoza (eclasiástico).

Entre los autores de obras literarias vinculados de algún modo con Zaragoza figura en época romana Prudencio, que publicó odas de alabanza a los mártires de Zaragoza. En la época visigoda una obra destacada fue la "Chronica Caesaraugustana", atribuida al obispo Máximo y sobresalió también el obispo Braulio, autor de obras litúrgicas. En la Taifa de Zaragoza escribieron filósofos de la talla de Avempace y Avicebrón. En 1475 se estableció la primera imprenta en Zaragoza y en esos años iniciales sobresalieron las imprentas dirigidas por Pablo Hurus y Jorge Coci. En el renacimiento, Antonio Agustín y Juan Verzosa fueron dos humanistas que escribieron en latín la mayor parte de sus obras, y como historiador brilló Jerónimo Zurita. En el Siglo de Oro, Zaragoza fue uno de los lugares donde Baltasar Gracián redactó su obra cumbre: "El Criticón", mientras en poesía destacaron los hermanos Bartolomé Leonardo de Argensola y Lupercio Leonardo de Argensola. A fines del siglo XVIII, Félix Latassa recogió en tres volúmenes una historia literaria de Aragón que comprendía 2866 escritores aragoneses. Entre el y XIX escribió el científico, filólogo, jurista y economista Jordán de Asso y, en la misma época, el poeta, novelista y comediógrafo José Mor de Fuentes. La Aljafería fue el escenario del drama romántico del "El trovador", de Antonio García Gutiérrez, basado en acontecimientos históricos del siglo XV. En 1844 se imprimió en Zaragoza la "Vida de Pedro Saputo" de Braulio Foz. Por otra parte, los sitios sufridos por Zaragoza generaron numerosas obras literarias, entre ellas "Zaragoza", uno de los "Episodios nacionales" de Benito Pérez Galdós. Dos novelistas zaragozanos que nacieron en el fueron María del Pilar Sinués —cuyas obras de temática femenina fueron muy populares en su tiempo— y José María Matheu. 

Como periodista sobresalió Mariano de Cavia. Ya dentro del modernismo figuran autores como Mariano Miguel de Val y Eduardo de Ory y entre los escritores del posteriores merecen destacarse autores como Ildefonso Manuel Gil, de la generación de 1936; Miguel Labordeta, poeta de la generación de la posguerra y el vanguardista Tomás Seral. Entre los años 1930 y 1960 aparecieron las revistas literarias "Cierzo", "Noreste", "Doncel", "Proa", "Pilar", "Ansí", "Orejudín", "Papageno" y "Despacho Literario", algunas de las cuales tuvieron una efímera existencia. Durante los años cuarenta, publica novelas la autora Rosa María Aranda.
En la actualidad, destacan narradores como Ignacio Martínez de Pisón, Soledad Puértolas, Mariano Gistaín, José María Conget, José Luis Melero, Antón Castro, Miguel Mena, Cristina Grande, Ismael Grasa, Daniel Gascón, Rodolfo Notivol, José Antonio Labordeta, Ángela Labordeta, Vital Citores, Ignacio García-Valiño, José Giménez Corbatón, Javier Delgado, Santiago Gascón, Adolfo Ayuso, Ana Alcolea, Javier Barreiro, Ramón Acín Fanlo, Ángeles de Irisarri, Magdalena Lasala, Félix Romeo, Julio José Ordovás... Poetas como Fernando Ferreró, Fernando Sanmartín, Manuel Peláez González, Manuel Vilas, Fernando Andú, Ángel Guinda, Emilio Gastón, Joaquín Sánchez Vallés, Ana María Navales, David Mayor, Ignacio Escuín, Fernando Mombiela, Paco Rubio Sesé... Autores de teatro, como Alfonso Plou, Mariano Anós y Rafael Campos. Autores de literatura infantil como Daniel Nesquens, Fernando Lalana, Francis Meléndez...

La gran figura del arte plástico es Francisco de Goya, cuyo alcance es universal. En Zaragoza se pueden encontrar obras suyas en la Basílica del Pilar, la Cartuja del Aula Dei, el Museo de Zaragoza, el Museo Camón Aznar y el Museo Diocesano, aparte de en colecciones privadas. 

Sin embargo, no han faltado grandes artistas desde el maestro gótico Blasco de Grañén.

En el siglo XVI, pintores como Jerónimo Cósida, Pedro Morone, Roland de Mois o Francisco Lupicini introdujeron el estilo renacentista en la ciudad.

Su influjo consolidaría en el siglo siguiente una importante escuela pictórica barroca con autores como Jerónimo de Mora, Miguel Jerónimo Lorieri, Pablo Rabiella y Díez de Aux, Juan de Orcoyen, Francisco del Plano, Rafael Pertús y, sobre todos, el ejeano Vicente Berdusán y el tratadista y pintor Jusepe Martínez.
Del data José Luzán, el primer maestro de Goya, pero el más reconocido junto con el pintor de Fuendetodos, fue su cuñado Francisco Bayeu, cabeza de una saga que incluyó a sus hermanos Manuel y Ramón.

En el de nuevo surgen grandes figuras, como Bernardino Montañés, Marcelino de Unceta, Mariano Barbasán y, especialmente, Francisco Pradilla, nacido en Villanueva de Gállego, a trece kilómetros de la capital.

En el destaca la obra informalista de Manuel Viola, quien estuvo adscrito al influyente grupo «El Paso», así como la de Ruizanglada, Santiago Lagunas, José Manuel Broto Gimeno, Francisco Marín Bagüés o Fermín Aguayo. Pepe Cerdá, Ángel, Vicente Pascual Rodrigo, Fernando Sinaga, Requena Nozal, Lina Vila, Jorge Gay o Dino Valls son artistas que están dejando su impronta en el siglo XXI.

En escultura cabe citar a Gil Morlanes «el Joven» —quien terminó la portada renacentista de la Iglesia de Santa Engracia, emprendida por su padre—, Jerónimo Secano en el barroco y Ponciano Ponzano en el siglo XIX. Del siglo pasado destacan Félix Burriel y Carlos Palao, que fue director de la Academia de Bellas Artes.

En la fotografía, ya en 1837 (dos años antes del reconocimiento oficial del invento a Louis Daguerre), José Ramos Zapetti fijaba en Zaragoza una imagen sobre una plancha de cobre con el procedimiento de la cámara oscura. Entre 1856 y 1874 establece su gabinete fotográfico en el n.º 33 del Coso el renombrado Mariano Júdez, quien traspasó su negocio a Anselmo María Coyne, fundador del Estudio Coyne. Posteriormente, su hijo Ignacio Coyne Lapetra, amplió su actividad al cinematógrafo, creando el Cine parlante Coyne. Otro importante fotógrafo, que desarrolló su labor en la capital aragonesa a fines del siglo XIX, fue Enrique Beltrán. Entre otros fotógrafos actuales se puede mencionar a Rafael Navarro y Pedro Avellaned.

El más antiguo teatro de la ciudad es el Teatro Principal. También cuenta con el Teatro del Mercado, el Teatro de la Estación, el Teatro de las Esquinas en el barrio de las Delicias y el Teatro Arbolé, dedicado exclusivamente a la programación infantil. El Teatro Fleta se encuentra desde hace varios años en obras paralizadas pendiente de un proyecto para el mismo.

Además del Centro de Arte Dramático de Aragón, cuya sede se encuentra en Zaragoza, y de la Escuela Municipal de Teatro, dependiente del Ayuntamiento de Zaragoza y con una larga trayectoria, sobresalen las compañías independientes que han animado la escena local desde los años 50. Siguen en completa actividad el Teatro de la Ribera, el Teatro Imaginario, Caleidoscopio, Muac, Belladona teatro y compañías de gran proyección como Teatro del Temple.

Zaragoza fue pionero en la producción cinematográfica. Eduardo Jimeno Correas rodó una de las primeras películas españolas "Salida de misa del Pilar", en 1896, y desde entonces la ciudad ha dado un buen número de directores, actores, críticos, guionistas y técnicos. Entre los directores, se deben citar a Florián Rey, José Luis Borau, José María Forqué, Fernando Palacios, Antonio Artero, Santos Alcocer, Alfredo Castellón, José Antonio Maenza, Raúl Artigot, José Antonio Duce, José Luis Gonzalvo, Alberto Sánchez y, más recientemente, Miguel Ángel Lamata y Luis Alegre.
Entre los guionistas se cita a Santiago Aguilar Oliver.

En Zaragoza han nacido reconocidos autores de cómic, como Adolfo Buylla, Tran, Carlos Ezquerra, Antonio Altarriba, Calpurnio, Fernando de Felipe, Nacho Casanova o Furillo, aunque nunca ha tenido una industria local importante, más allá de los fanzines. Sí que celebra desde 2002 el "Salón del Cómic de Zaragoza".

El Auditorio de Zaragoza, cuya "Sala Mozart" ha recibido elogios por la calidad de su acústica, acapara gran parte de los eventos de música clásica. En este Auditorio tienen su sede el Grupo Enigma-Orquesta de Cámara, el coro Amici Musicae y la Orquesta barroca Al Ayre Español. También organizan conciertos las entidades de ahorro Ibercaja y CAI, así como Juventudes Musicales de Zaragoza, pero todas estas de un modo más esporádico. La pianista Pilar Bayona fue una de las grandes figuras de la música clásica zaragozana del siglo XX.
En cuanto a música popular, Zaragoza cuenta con una nutrida escena musical. Hay cantautores, como los que comenzaron en torno a la Nueva Canción Aragonesa, como José Antonio Labordeta, Joaquín Carbonell y La Bullonera o como los más jóvenes Ángel Petisme, Carmen París o María José Hernández. También es de Zaragoza la banda Héroes del Silencio, con su líder Enrique Bunbury. Singular es la obra de Santiago Auserón, que fue durante años líder de Radio Futura, así como Amaral. Hay muchos grupos de pop independiente vinculados a la ciudad, como los desaparecidos El niño gusano, que han sido el embrión de otros proyectos en este estilo musical, como Tachenko, La Costa Brava o Da. Sin duda, el rap y el hip hop ocupan un lugar muy destacado en la escena musical, fundamentalmente por el grupo Violadores del verso, también cabe mencionar a raperos que trabajan en solitario como Rapsusklei o Sharif. En el género del punk destaca Manolo Kabezabolo. Finalmente, en el género del blues destaca la voz de Ana Midón. Otros grupos y solistas destacados de la escena musical zaragozana son Los Especialistas, Niños del Brasil, Las Novias, Más Birras, Gabriel Sopeña, Mauricio Aznar, Bigott, Ixo Rai y Comando Cucaracha.
Dos locales emblemáticos de Zaragoza desde principios del han sido el «Oasis» y «El Plata», que ofrecían espectáculos de variedades donde destacaban algunas formas de música popular como el cuplé. El Oasis fue convertido en discoteca y sala de conciertos mientras El Plata abrió de nuevo sus puertas en 2008 con espectáculos de cabaré.

A lo largo del año hay numerosos eventos relacionados con la jota, principal manifestación de música folclórica de Zaragoza. El más importante, celebrado anualmente desde 1886, es el Certamen Oficial de Jota Aragonesa.

Entre las revistas musicales de la ciudad destaca Zona de obras, especialmente interesada en la música y la cultura iberoamericana. En cuanto a salas de conciertos destacan, además de la Sala Oasis, La Casa del Loco, el Auditorio, Sala Reset y diversos centros cívicos dependientes del ayuntamiento que albergan numerosos conciertos. 


En la gastronomía tradicional de Zaragoza son fundamentales muchos de los productos de huerta del entorno de la ciudad y de otras zonas de la ribera del Ebro dentro de los que sobresale por su calidad la borraja, pero también el cardo, la alcachofa, el bisalto, el tomate, la acelga, la coliflor, el calabacín, el puerro, la patata, la lechuga romana, la escarola rizada, el pimiento, la espinaca, la judía verde, la cebolla o el ajo. Con algunos de estos y otros ingredientes se elaboran gran diversidad de platos como la "ensalada aragonesa" o la "fritada". Legumbres tales como lentejas, garbanzos y boliches conforman la materia prima de potajes o estofados. 

En Zaragoza también confluyen muchos de los productos alimenticios más valorados de otras partes de Aragón, como las olivas negras, las almendras, los vinos, las trufas, el aceite de oliva y el azafrán. En cuanto a carnes, abundan las recetas que tienen como protagonista el cordero o el pollo, así como el conejo o la carne de caza, sin olvidar los productos derivados del cerdo entre los que destacan el jamón, la longaniza, el chorizo y la morcilla. También son diversos los guisos donde figuran como ingredientes principales los caracoles. Algunas de las recetas tradicionales son las "migas", las "magras con tomate", el "arroz a la zaragozana", el "pollo a la chilindrón" o el "ternasco asado". El pescado de referencia es el bacalao que, por su versatilidad, es el ingrediente principal de muchos platos. 

Frutas destacadas son las manzanas, peras, cerezas, ciruelas, melocotones, alberges e higos. En cuanto a repostería y dulces, se elaboran magdalenas, tortas, rosquillas, frutas de Aragón, piedras de río, chocolate, adoquines del Pilar, roscón en San Valero y guirlaches en Navidad.

En Zaragoza hay una gran variedad de oferta de ocio. Abundan los bares de tapas y terrazas, que se concentran especialmente en diversos entornos dentro del Casco histórico, zona centro y Universidad. En El Tubo conviven establecimientos populares y restaurantes como Bal d´Onsera, la primera estrella Michelin de la capital aragonesa. Dentro de El Tubo el café cantante El Plata mantiene el cabaret en Zaragoza. 

Dentro del casco histórico, La Magdalena es hoy en día la zona más alternativa e intercultural de la ciudad. Allí se pueden encontrar teterías árabes, bares con música reggae, galerías de arte, comercios alternativos, escuelas de aragonés y centros sociales con una intensa programación cultural y musical.

La mayor aglomeración de discobares y pubs de la ciudad se encuentra también dentro del casco histórico, en torno a la calle del Temple y adyacentes. Cabe destacar también dentro del ocio nocturno la cercana discoteca Oasis. Otras zonas de ocio nocturno son la de la calle María Lostal y adyacentes, la de la zona de la calle Bretón o el entorno de la Plaza de Salamero. Los adolescentes suelen acudir a «El Rollo», zona situada en las calles Moncasi y Maestro Marquina. Cerca de la Puerta del Carmen abren sus puertas varios pubs "gay-friendly".

En el centro sobreviven algunas salas de cine, aunque otras, de mítica presencia en la ciudad, han cerrado sus puertas recientemente. El Casino de Zaragoza también se encuentra en este entorno. 

Numerosos centros comerciales abiertos en los últimos años (Plaza Imperial, Puerto Venecia, GranCasa, Augusta, Aragonia o los Porches del Audiorama) también ofertan espacios de ocio con cines, restaurantes, boleras y parques infantiles, entre otros. Dentro de los espacios de ocio infantil, otro de los lugares destacables es el parque de atracciones de Zaragoza, inaugurado en 1974.

A raíz de las inversiones realizadas para la Expo 2008 se habilitaron en torno al río Ebro numerosos espacios en los que se podía practicar gran variedad de actividades. Dentro de las principales infraestructuras, algunas de ellas siguen siendo parte de la oferta de ocio de Zaragoza en determinadas épocas del año, como los paseos en barca con origen en el azud de Vadorrey, o el descenso del canal de aguas bravas pero otras como la telecabina dejaron de tener uso desde el año 2011.

Zaragoza posee varios equipos que juegan en las principales ligas españolas. En fútbol masculino, el Real Zaragoza, que juega en el estadio de La Romareda, y en el femenino, el Zaragoza Club de Fútbol Femenino. En baloncesto, el equipo masculino Basket Zaragoza y el femenino Club Deportivo Basket Zaragoza. En rugby, el Universidad San Jorge Fénix Rugby Zaragoza (USJ Fénix) compite en la temporada 2019/2020 en la División de Honor B, categoría de plata del rugby nacional. Zaragoza también está representada por el Íbero Club de Rugby y por el Club Deportivo Universitario Rugby Zaragoza.

El balonmano en Zaragoza está representado por el Club Deportivo Básico Balonmano Aragón, el fútbol sala por la Agrupación Deportiva Sala 10, el voleibol por el Club Voleibol Zaragoza y el atletismo por el Scorpio-71. También participa a nivel nacional el Escuela Waterpolo Zaragoza en waterpolo femenino. En gimnasia rítmica destaca el Club Deportivo Zaragozano de Gimnasia y el C.E.G.R. Zaragoza. 

En cuanto a las principales instalaciones donde se celebran competiciones de élite pueden citarse los pabellones Príncipe Felipe y Siglo XXI, el Palacio de los Deportes de Zaragoza y el Estadio de Atletismo del Centro Aragonés del Deporte.

En Zaragoza se editan los periódicos "Heraldo de Aragón" y "El Periódico de Aragón", el deportivo digital "Equipo", y el gratuito "20 minutos". 

Hay varios canales de televisión que emiten desde la ciudad: el canal autonómico Aragón TV y los canales locales Canal 44 y LaTele TV. Además, Televisión Española cuenta en Zaragoza con un centro territorial para Aragón desde 1979.

Gran parte de las emisoras de radio de Zaragoza están controladas por los grandes grupos radiofónicos de España: Radio Nacional de España, Cadena SER, COPE y Onda Cero. Al margen de ellas, operan en Zaragoza otras emisoras de ámbito autonómico, como Aragón Radio y numerosas emisoras locales.

Las ciudades hermanadas con Zaragoza son:

Además, tiene firmados acuerdos de colaboración con:





</doc>
<doc id="3009" url="https://es.wikipedia.org/wiki?curid=3009" title="Zizania">
Zizania

Zizania, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia y América del Norte.
Aunque conocido comúnmente como arroz silvestre o arroz salvaje, no está directamente relacionado con el arroz asiático ("Oryza sativa"), aunque ambos comparten la misma tribu. Se trata de hierbas acuáticas o palustres, robustas y erguidas. Tienen raíces delgadas y fibrosas que no penetran mucho, algunas adventicias; cañas hasta de 3 metros de altura; las hojas de 1 m de longitud por 4 cm de ancho; flores en panojas terminales. Los granos alargados contienen más de 13% de proteína y además carbohidratos, vitamina B, potasio y fósforo.

Las especies nortemericanas, llamadas ahora arroz indígena, fueron recolectadas para la alimentación por los aborígenes, desde hace por lo menos 10 mil años. Hoy en día son cultivadas.

"Zizania palustris" es una planta anual, originaria de la región de los Grandes Lagos, crece a temperaturas entre los 6° y 13° C. Los principales productores son los estados de Saskatchewan y Manitoba en Canadá y Minesota y California en Estados Unidos.

"Zizania aquatica", también anual, es originaria de la cuenca del río San Lorenzo, Florida y el golfo de México. "Zizania texana", de la cuenca del Río San Marcos, se encuentra en peligro de extinción por la reducción de su hábitat.

"Zizania texana" es una planta perenne que se localiza únicamente en una pequeña área a lo largo del Río Saint Laurence y en las costas del Atlántico de los EEUU. Actualmente se encuentra en peligro de extinción debido a la polución y a la pérdida progresiva de terreno en el que es capaz de crecer, dado que su ecosistema es muy limitado.

"Zizania latifolia", el arroz silvestre de Manchuria, es una planta perenne. Aunque la producción y el consumo del grano han disminuido al ser sustituidos por arroz común, la planta se sigue utilizando en la alimentación, especialmente como verdura.

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 2: 991. 1753. La especie tipo es: "Zizania aquatica" L.
Zizania: nombre genérico que deriva de la palabra griega: "zizanion", un antiguo nombre de un grano de maleza silvestre que normalmente crecía entre cultivos de trigo.



</doc>
<doc id="3010" url="https://es.wikipedia.org/wiki?curid=3010" title="Zeugites">
Zeugites

Zeugites es el nombre botánico de un género de plantas de la familia de las Poáceas. Comprende unas 19 especies distribuidas en Centroamérica, el norte de Sudamérica y su costa oeste desde Colombia hasta Bolivia. Se trata de plantas bisexuales, con tallo de 30 a 100 cm de altura, hojas perennes de lanceoladas a ovales. Flores abiertas, capilares con ramillas, sin inflorescencias parciales y órganos foliares.

Son plantas perennes; con tallos delgados y rastreros o robustos y erectos; plantas monoicas. Hojas generalmente caulinares; lígula una membrana; pseudopecíolos generalmente bien desarrollados; láminas lanceoladas a ovadas, aplanadas, generalmente delgadas, nervadas transversalmente. Inflorescencia una panícula terminal, abierta, eje principal y ramas a menudo víscidos; espiguillas bisexuales, pediceladas, solitarias, comprimidas lateralmente, con 3–15 flósculos dimorfos, unisexuales; glumas más cortas que la espiguilla, anchas, obtusas o truncadas, a menudo irregularmente dentadas o lobadas, con nervaduras transversales conspicuas, la superior generalmente más angosta que la inferior; unión de la raquilla entre los flósculos pistilados y estaminados generalmente alargada; desarticulación por debajo de las glumas y la base del flósculo estaminado más inferior, los flósculos estaminados caedizos como una unidad; flósculo más inferior pistilado; lema ancha, generalmente obtusa, pálea un poco más larga o un poco más corta que la lema; lodículas 2, truncadas, vascularizadas; estilos 2, los estigmas plumosos; flósculos superiores estaminados 2–14, generalmente más angostos que el flósculo pistilado, agudos o subobtusos, pálea casi tan larga como la lema, estambres 3. Fruto una cariopsis; embrión 1/4–1/3 la longitud de la cariopsis, hilo punteado.




</doc>
<doc id="3012" url="https://es.wikipedia.org/wiki?curid=3012" title="Zenkeria">
Zenkeria

Zenkeria, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de la India, Ceilán, Birmania. Comprende 5 especies descritas y aceptadas.
Son plantas perennes; cespitosas de 60-130 cm de alto; herbáceas. Las láminas amplias o estrechas, de 5-25 mm de ancho, planas, o laminadas; pseudopeciolada. Lígula con una franja de pelos. Plantas bisexuales, con espiguillas bisexuales; con flores hermafroditas. La inflorescencia paniculada; abierta. 
El género fue descrito por Carl Bernhard von Trinius y publicado en "Linnaea" 11(2): 150. 1837. La especie tipo es: "Zenkeria elegans" Trin. 
A continuación se brinda un listado de las especies del género "Zenkeria" aceptadas hasta noviembre de 2013, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 



</doc>
<doc id="3014" url="https://es.wikipedia.org/wiki?curid=3014" title="Zacatecas">
Zacatecas

Zacatecas, oficialmente llamado Estado Libre y Soberano de Zacatecas, es uno de los treinta y un estados que, junto con la Ciudad de México, conforman México. Fue fundado el 23 de diciembre de 1588. Su capital y ciudad más poblada es la homónima Zacatecas. Está ubicado en la región centronorte del país, limitando al norte con Coahuila, al noreste con Nuevo León, al este con San Luis Potosí, al sur con Guanajuato, Jalisco y Aguascalientes, al suroeste con Nayarit y al oeste con Durango. Con 75,539 km² es el octavo estado más extenso, con 1,579,209 habs. en 2015, el octavo menos poblado y con 19.73 hab/km² y el sexto menos densamente poblado. 

Se divide en 58 municipios. La capital es la ciudad homónima, Zacatecas. Esta ciudad ostenta los títulos de "La Muy Noble y Leal Ciudad de Nuestra Señora de los Zacatecas", otorgados por el rey Felipe II de España el día 20 de junio de 1588 en San Lorenzo de El Escorial, Madrid. Así mismo, le concedió el Escudo de Armas, emblema en el que fue incluido el cerro de la Bufa. Esta ciudad fue importante para la colonización, puesto que era un punto importante en la ruta hacia los territorios del norte de la Nueva España. Sus principales actividades económicas son la minería, la agricultura y el turismo. Es conocido por sus grandes depósitos de plata y otros minerales, su arquitectura colonial y su importancia durante la Revolución mexicana. Entre sus localidades más importantes están Jerez de García Salinas, Fresnillo de González Echeverría, Río Grande, Guadalupe, Sombrerete, Nochistlán y Calera.

Del náhuatl, "zacatl", y co; locativo: «lugar donde abunda el zacate». El nombre significa personas que viven en el lugar del zacate. Antes de su conquista el lugar era habitada por indígenas llamados zacatecas, de donde precisamente se deriva el nombre de Zacatecas.

El escudo de armas de Zacatecas fue otorgado el 20 de junio de 1588 por el rey Felipe II, mediante una Cédula Real. Según la cédula real original, debe tener la forma de un escudo español. En un único campo, predomina una elevación que representa al emblemático cerro de la Bufa, en cuyos pies nace la ciudad en 1546, como producto del descubrimiento de las ricas minas de plata. En la parte más eminente del cerro aparece una imagen de la Virgen María, por haberse descubierto este cerro y las minas el día en el que la iglesia católica celebra la fiesta de la Natividad de la Virgen; abajo, el monograma del Felipe II, como testimonio de quien otorgó el escudo de armas a la ciudad. En los dos extremos superiores del escudo flotan el sol y la luna en un cielo de color azul intenso. En la falda del cerro hay cuatro retratos de personas en campo: el capitán Cristóbal de Oñate, Juan de Tolosa, Diego de Ibarra y Baltazar Temiño de Bañuelos, siendo los principales fundadores; debajo de ellos aparece el lema "Labor Vincit Omnia" (el trabajo lo vence todo); y en la orla, cinco manojos de flechas y entremetidos con otros cinco arcos, que son las armas de que usaban los referidos indios chichimecas.

La Marcha Zacatecas es obra del compositor zacatecano Genaro Codina. Fue compuesta en el año de 1892 y tocada por primera vez en público en la primavera de 1893, por la Banda del Estado, que era dirigida por Fernando Villalpando y reforzada por la Banda de Niños del Hospicio, además de una banda de guerra. El mérito de la instrumentación de la marcha corresponde a Fernando Villalpando.

Por su aceptación y frecuente interpretación en actos oficiales, está considerada como el segundo Himno Nacional Mexicano, así como el himno nacional de la Charrería Mexicana.

En el siglo XVI, los españoles llamaron «La Gran Chichimeca«» al norte de la mesa central de México, territorio que nunca fue conquistada por los mexicas. Esto ahora es compuesto por los estados de Jalisco, Aguascalientes, Nayarit, Guanajuato, San Luis Potosí, Durango, Coahuila y Zacatecas. Los mexicas llamaron con el etnónimo "chichimeca" a los pobladores de esta gran región aunque fueran de distintas civilizaciones, lenguajes o tribus. Se reportó que en lo que ahora es el estado zacatecano habitaban cuatro etnias primigenias: los caxcanes, guachichiles, tepehuanes y zacatecos, siendo de estos últimos que el estado recibe su nombre moderno.

La mayoría de los pobladores eran nómadas dedicados a la caza, con pocos asentamientos humanos permanentes. El territorio sureño del estado estuvo bajo la influencia mesoamericana, mientras que la mayor parte del estado formaba parte de la región denominada Aridoamérica. Actualmente en Zacatecas se encuentran zonas arqueológicas como La Quemada, localizada en el municipio de Villanueva, y Altavista localizada en el municipio de Chalchihuites. En estas zonas se encuentran edificaciones ceremoniales y pirámides con rasgos arquitectónicos de las culturas mesoamericanas.

En 1531, Pedro Almíndez Chirino junto a sus tropas llegaron de lo que ahora es el noreste de Jalisco, guiados por "Xiconaque," cacique zacateco cuya tierra se encontraba en la actual Lagos de Moreno, Xiconaque los llevó a lo que se llamaba el «pueblo grande de los zacatecos» o Tlacuitlapán donde Chirino no pudo encontrar riquezas considerables, solo describió encontrar en la cima de un cerro con crestón (La Bufa) una aldea indígena cuyas casas eran circulares con techo de paja y hierba. Sin embargo, en 1546 cuando Juan de Tolosa encabezaba una exploración en la parte sureña del estado, en lo que hoy es Tlatenango, se le acercaron unos indígenas y le mostraron piedras brillantes que contenían plata. De Tolosa fue al cerro de La Bufa, donde se encontraba una aldea zacateca, y se llevaron varias cargas de metal a lo que hoy es Nochistlan. Según se cree, en enero de 1548 Juan de Tolosa, Diego de Ibarra, Cristóbal de Oñate y Baltazar Temiño de Bañuelos se reunieron y fundaron las primeras casas, aunque no hicieron la fundación formal de la ciudad que sería conocida como Minas de los Zacatecas ya que a su llegada ya existían indígenas que habitaban dicha zona, conocida después como la «Civilizadora del Norte». Se supone que la fundación ocurrió el 8 de septiembre de 1546 fecha aproximada en que De Tolosa exploraba el cerro de la Bufa. Zacatecas llegó a formar parte de Nueva Galicia, el nombre que se le dio a un territorio del virreinato de Nueva España. Debido a su riqueza mineral y los fuertes ingresos a la Corona Española, en 1585 Minas de los Zacatecas recibió el título de «Muy Noble y Leal Ciudad de Nuestra Señora de los Zacatecas» y su correspondiente escudo por parte del Rey de España Felipe II.

En Zacatecas como en la mayor parte de México se suscitaron levantamientos y batallas con el fin de independizarse de España, uno de ellos fue la Toma de Zacatecas de 1811. Cuando se conoció captura de los comandantes insurgentes en Acatita de Baján, López Rayón huyó de Coahuila el 26 de marzo, dirigiéndose a Zacatecas; siendo este seguido por el jefe realista José Manuel Ochoa, con el que libró la Batalla del Puerto de Piñones, derrotando a Ochoa y obteniendo armamento. Luego de varios combates, el 15 de abril López Rayón tomó Zacatecas, ahí fundió artillería, fabricó pólvora y dio uniforme a sus tropas.

Un personaje zacatecano que destacó durante la independencia fue José María Cos quien participó en la guerra independentista de México casi desde el principio. Se le atribuye, entre otros méritos, el haber impedido que Zacatecas fuera arrasada por la violencia de la guerra y facilitar su incorporación como plaza simpatizante de los insurgentes con el mínimo de sangre derramada. Cuando se instituyó el Congreso de Chilpancingo, Cos participó en él como diputado por la provincia de Zacatecas. Participó en la redacción de la Constitución de Apatzingán, primera ley que rigió el gobierno de la naciente República Mexicana.

El coronel don Manuel Orive y Novales fue el último intendente de la Provincia de Zacatecas, estando en el poder hasta el 18 de octubre de 1823, fecha en que la diputación provincial declaró el «Estado Libre y Federado de Zacateca» nombrándose un nuevo cuerpo legislativo que nombró el primer gobernador interino y provisional al Coronel don Juan Peredo que tomó posesión el mismo 18 de octubre de 1823.

La diputación provincial declaró a Zacatecas estado libre y federado el 17 de junio, y el 19 de octubre de 1823 quedó instalado el primer Congreso estatal. Los tres poderes constituidos —Ejecutivo, Legislativo y Judicial— defendieron la autonomía del estado como condición para conservar la integridad nacional. Su postura se identificó con un confederalismo, opuesto a toda actitud separatista, y mantuvo la tradición constitucionalista y legalista que se había arraigado en Zacatecas entre 1810 y 1813. Don José María Hoyos, fue nombrado por la Junta Auxiliar como Gobernador Provisional del Nuevo Estado Federado de Zacatecas, y el mismo día 18 de marzo de 1824, tomó posesión del cargo, mientras se convocaba a los Municipios a que presentaran una terna para elegir al nuevo Gobernador. Las tareas más urgentes del primer Congreso fueron elaborar la constitución y mantener el estado a salvo de las pretensiones centralizadoras del gobierno nacional y, paradójicamente, de Guadalajara, que estaba interesada en conservar su antigua jurisdicción sobre Zacatecas. Dos de los principales temas de discusión de la legislatura fueron el manejo de los recursos económicos del estado, como garantía de su independencia y soberanía, y el equilibrio de los tres poderes.

José María García Rojas fue el primer gobernador elegido en Zacatecas conforme a la nueva constitución. Como gobernador García Rojas estableció la milicia cívica, que sería muy importante en la defensa del federalismo y apoyó la autoridad de poder civil sobre los militares y los religiosos. A finales de 1829, Francisco García Salinas fue elegido gobernador de Zacatecas que defendía un modelo federal. Los conservadores zacatecanos favorecían un sistema de gobierno representativo. En dos ocasiones, los conservadores se rebelaron contra el gobierno federal. En la rebelión de 1835, las fuerzas federales del general Santa Anna saquearon la ciudad de Zacatecas y las minas de plata de Fresnillo.

En enero de 1825 se promulgó la Constitución política del estado libre de Zacatecas. Para su preparación, los legisladores analizaron las constituciones más avanzadas de la época, pero tuvieron una clara inspiración en la de Cádiz. La Constitución fue esencialmente un documento ideológico que estableció como forma de gobierno la república representativa popular federal y la división de los poderes; asimismo, definió los vínculos entre el estado y el resto de la nación. El territorio estatal quedó dividido en los partidos de Zacatecas, Fresnillo, Sombrerete, Aguascalientes, Juchipila, Nieves, Mazapil, Pinos, Jerez, Tlaltenango y Villanueva. A los ayuntamientos se les concedió mayor autonomía, con lo que ampliaron su participación en la vida política del estado.

A raíz del fracaso del sistema federal, el centralismo ganó terreno y el Congreso modificó la Constitución de 1824 a fin de crear una república centralista, limitando el poder de los estados y reduciendo el número de tropas militares. Tales acontecimientos provocaron una rebelión en Zacatecas, donde el propio gobernador, Francisco García Salinas, encabezó un ejército de unos cuatro mil hombres en contra del gobierno. Para poner fin a los sublevados, el presidente Santa Anna, en persona se dirigió a combatirlo, dejando como encargado de la presidencia al general Miguel Barragán. García Salinas, fue derrotado en la Batalla de Zacatecas de 1835, y en castigo por su rebeldía, fue obligado el estado de Zacatecas a perder parte de su territorio, con la que se formó el estado de Aguascalientes.

Durante la Guerra de Reforma (1858—1861) Zacatecas se convirtió en campo de batalla entre liberales y conservadores. En 1859, Jesús González Ortega puso fin a los ideales conservadores del estado cuando decretó una ley en contra de ello.

Una de las batallas más importantes de la revolución ocurrió en la ciudad de Zacatecas el 23 de junio de 1914. Se le conoce como la Toma de Zacatecas. En esta batalla, Francisco Villa —asistido por Felipe Ángeles y Pánfilo Natera— y sus dorados tomaron control de la ciudad de Zacatecas, asegurando la seguridad financiera de la revolución. En honor a Pancho Villa, se erigó una estatua en el Cerro de la Bufa y el estadio olímpico de la capital recibió su nombre, no obstante la economía estatal sufrió las consecuencias de estos hechos durante décadas de acentuada recesión económica.

Para conmemorar ese hecho desde 2004 se realiza anualmente en el mes de junio en la capital zacatecana una cabalgata que reúne cientos de jinetes de diferentes localidades del estado.

Zacatecas tiene una extensión territorial de 75,040 km², esto representa el 3.83% del territorio nacional. Sus coordenadas extremas son 25°09' al norte, 21°04' al sur de latitud norte; al este 100°49' y al oeste 104°19' de longitud oeste. Limita al norte con Coahuila, al noroeste con Durango, al oeste con Nayarit, al este con San Luis Potosí y Nuevo León, y al sur con Jalisco, Aguascalientes y Guanajuato.

El estado se encuentra en el norte de México específicamente en la Meseta Central de México, que abarca los estados de Zacatecas, Durango, Chihuahua y parte de Coahuila; entre la Sierra Madre Oriental y la Sierra Madre Occidental, al occidente y suroccidente existen algunas mesetas con una altitud máxima de 2,850 metros sobre el nivel del mar (msnm) como el cerro La Aguililla.

En uno de los valles, existe el Cañón de Juchipila, con una altura mínima en el estado de 1,000 metros. La Sierra Madre Occidental es la principal cadena montañosa que atraviesa el estado. La elevación más alta es la Sierra El Astillero con una altitud de 3,200 msnm, seguido por la Sierra de Sombrerete con 3,100 y la Sierra Fría con 3,030. El 38.82 % de la superficie estatal es matorral, el 27.38 % de la superficie se usa para la agricultura, el 15.67 % es pastizal, el 12.66 % es bosque, el 1.94 % selva y el resto tiene otros usos. La mayor parte del estado forma parte del desierto Chihuahuense, caracterizándose por escasa precipitación pluvial y una gran diversidad cactácea.

La entidad carece de ríos importantes; los que hay, en su mayor parte son temporales y se forman al escurrir el agua de las montañas en la época de lluvias. El sistema hidrográfico está formado por dos cuencas: la cuenca del Pacífico a través de otros estados son: San Pedro, Juchipila, Jerez, Tlaltenango, San Andrés, Atengo, Valparaíso. Los ríos de la Cuenca Interior no tienen salida al mar y los principales son: Calabacillas, Zaragoza, Los Lazos, San Francisco y Aguanaval que desemboca en Torreón, Coahuila. En cuanto a agua subterránea, existen 20 zonas geohidrológicas en el estado. El estado cuenta con un total de 80 presas con una capacidad total de 595 337 millones de metros cúbicos destacándose las presas de: Leobardo Reynoso (Fresnillo), Miguel Alemán (Tlaltenango) y El Chique (Tabasco).

Existen 20 zonas geohidrológicas en el estado en las cuales se localizan 5891 pozos profundos con fines agrícolas con gastos hidráulicos que oscilan entre 15 y 60 l/s. Con profundidades de 150 a 250 metros. y niveles dinámicos promedio de 80 metros. Además se tienen 2441 norias o pozos a cielo abierto de poca profundidad y bajo costo de 5 a 10 l por segundo. Igualmente, en diversas regiones del estado se localizan 483 pozos de bajo gasto con fines de abrevadero para ganado. El principal problema que enfrenta el agricultor que extrae agua para usos agrícolas es el costo de la electricidad.

La vegetación de Zacatecas es muy variada. En las sierras existen bosques mixtos de pinos y encinos; los árboles se mantienen verdes todo el año. También hay regiones áridas y semidesérticas que albergan gran cantidad de plantas como las cactáceas. En llanos y valles abundan los mezquites, gobernadoras, huisaches, nopales, lechuguillas, guayules y pastizales.

La fauna de las sierras incluye es, venados cola blanca y liebres; en llanos y valles suelen encontrarse coyotes, tejones, codornices y patos. Otros animales de la región son la víbora de cascabel, chirrioneros, alicantre, rata canguro, ratón de campo, gato montés, murciélagos, águila, guajolote silvestre, topo, tuza, guacamaya enana y la guacamaya verde. Zacatecas es la entidad del país en la que se encuentran más ejemplares de águila real, el símbolo nacional mexicano.
Los matorrales abarcan la tercera parte de la superficie del estado; le siguen en extensión los pastizales y en las partes más elevadas los bosques de coníferas y encinos. Las zonas agrícolas abarcan 25% del territorio.

El estado de Zacatecas existen áreas que cubren varios municipios en las que el suelo, la vegetación, el clima, la fauna es característica solo de esa parte del territorio se encuentra dividido en 4 zonas


La zona norte es caracterizada por el clima seco desértico, el suelo es duro y salado, dificulta la agricultura; solo se dan el maíz y el frijol. En cambio, las posibilidades del uso pecuario del suelo de esta región son mayores. La zona centro tiene climas semidesértico y templado semidesértico, algunas partes de la región tienen posibilidades para la agricultura. La zona sur es la región más grande del estado y cuenta con climas como elseco semidesértico, el templado semidesértico y el templado subhúmedo, abundan los bosques pero también en varias partes crecen matorrales y pastizales; también existes varios ríos, por lo que la región tiene grandes posibilidades para la explotación agrícola.

A nivel nacional, el estado tiene tres representantes en el Senado Mexicano: Carlos Alberto Puente Salas (PVEM), José Marco Antonio Olvera Acevedo (PRI) y Héctor Adrián Menchaca Medrano (PT). Zacatecas tiene ocho representantes en la Cámara de Diputados | LXI Legislatura representantes en la Cámara de Diputados: cuatro diputados por mayoría relativa del PRI, uno por representación proporcional del PAN, uno por representación proporcional del (PT) y uno más por representación proporcional del (PVEM) | LXI Legislatura.

Los poderes gubernamentales del estado tienen sus instalaciones en la Ciudad de Zacatecas.

El actual gobernador de Zacatecas es Alejandro Tello del PRI, quien ocupara el cargo durante el periodo 2016-2021. Los miembros principales de su gabinete son:

La organización general del Poder Ejecutivo

El Poder Legislativo del estado, está compuesto por 30 diputados; 18 de mayoría relativa y 12 de representación proporcional, de entre estos últimos, por mandato constitucional y desde la quincuagésima octava legislatura hay dos diputados migrantes o binacionales, destacando a Zacatecas como una entidad que ha sabido dar su representación y lugar a los migrantes zacatecanos en puestos de elección popular.

Distritos Federales
Distritos Locales

La organización general de poder legislativo.


El Poder Judicial del estado está compuesto por 13 magistrados. Los miembros del congreso estatal y los ayuntamientos son elegidos por un periodo de 3 años.

Distritos Judiciales del estado

El estado de Zacatecas cuenta relaciones internacionales en las que se busca entre otras cosas atraer inversiones para el estado o hermanamientos, actualmente se cuenta con varios hermanamientos en varias ciudades del estado como las siguientes:


De acuerdo con la Secretaría de Relaciones Exteriores, el estado ha tenido 7 convenios ó acuerdos internacionales estos son:

"Vea: Municipios de Zacatecas."

Zacatecas está dividido en 58 municipios. El municipio de Mazapil es el más grande en el estado. Ocupa alrededor del 36% del área estatal y es más de dos veces la superficie del estado de Aguascalientes. El municipio de Momax es el más pequeño con solo 159 km². El estado tenía 56 municipios, cantidad que se incrementó cuando en 2000 se creó el municipio de Trancoso y en 2005 el municipio de Santa María de la Paz y totalizar los 58 actuales.

Según el Instituto Nacional de Estadística, Geografía e Informática, en el 2010, el Estado de Zacatecas tenía una población de 1,690,750 habitantes con una densidad de 57 habitantes/km². Esto ubica al estado en el 25º lugar de población en la nación y representa el 1.3% del total nacional. El 51.3% (863,771) de la población zacatecana fue compuesta por mujeres mientras que los hombres representan el 48.7% (826,897). Al igual que la nación, la población promedio de Zacatecas es bastante joven, pues es solo de 23 años, lo que lo ubica en el lugar 19°. En los últimos 10 años, la población de Zacatecas aumentó 6%. La densidad fue de 18.13 habitantes por km².

Según el último censo disponible (1921) étnicamente la entidad estaba formada por un 8.55% indígenas, 86.1% mestizos y 5.35% blancos. la población mestiza cuenta con un porcentaje de genes europeos superiores al resto del país.
Estas cantidades se han mantenido hasta la actualidad solo estimando una disminución del porcentaje indígena y un ligero aumento del sector mestizo. Actualmente solo 1,837 personas hablaban un idioma indígena.

Las ciudades consideradas como más importantes del estado, debido a su población son Zacatecas, Guadalupe, Fresnillo, Jerez y Río Grande.

En el estudio más reciente sobre zonas metropolitanas (ZM), publicado en 2010, por el Consejo Nacional de Población (CONAPO), el Instituto Nacional de Estadística y Geografía (INEGI) y la Secretaría de Desarrollo Social (SEDESOL), se estableció que en el Estado de Zacatecas existe solo una zona Metropolitana.

La Zona Metropolitana de Zacatecas-Guadalupe se conforma por los municipios de Zacatecas, Guadalupe y Morelos. Donde la población asciende a 309 660 personas, una densidad de población de 88.1 habitantes por kilómetro cuadrado. La ZM de Zacatecas-Guadalupe tuvo una tasa de crecimiento media anual de 1.1 % de 2000 a 2010.

En cuanto a urbanismo, de acuerdo a los resultados que presentó el Censo General de Población y Vivienda de 2010, en el estado cuentan con un total de 372,662 viviendas en el estado.

La información más reciente ubica a Zacatecas entre los estados con desarrollo humano medio (IDH de 0.7057). Su posición en la clasificación nacional se ha bajado un lugar (lugar 27) en comparación de 2000 y 2005 cuando estaba ubicado en el lugar 26. En términos relativos, para el año 2005 el índice de desarrollo humano (IDH) estatal fue de 0.7872, valor menor al nacional (0.8200), aunque creció más rápidamente pues mientras el indicador nacional aumentó 1.57%, el del estado lo hizo en 3.19%.

Al interior de la entidad se tiene que el IDH en 2005, para los municipios de Zacatecas (IHD de 0.8900) y Guadalupe (IHD de 0.8799) registraron el mayor nivel de IDH. En contraparte, las demarcaciones de El Salvador (IHD de 0.6792) y Jiménez del Teúl (IHD de 0.6583) tuvieron los menores niveles de desarrollo humano.

La emigración de Zacatecas hacia Estados Unidos ha representado, históricamente, uno de los flujos más intensos a escala nacional, debido a las fluctuaciones de la minería y a las condiciones agrícolas, entre otros motivos. En 1956, 1957 y 1958, como consecuencia de la crisis agrícola severa originada por la sequía, las salidas de zacatecanos fueron de 9.7, 11.1 y 10.4 por ciento en cada año respectivamente, de las totales nacionales; en 1957, respecto de la población equivalió a 6 por ciento y 21 a la mano de obra (Padilla 2000). Navarro y Vargas (2000) identifican que de 1990 a 1995 cerca de 26 mil zacatecanos abandonaron anualmente la entidad y emigraron a otros estados del país y en especial hacia Estados Unidos.

El estado de Zacatecas inició su incursión como expulsor de fuerza de trabajo desde finales del siglo XIX. En trabajos de antaño como el de Gamio (1930), y en contemporáneos como el de Durand (2005), se reconoce que desde principios del siglo XX, Zacatecas junto con Jalisco, Michoacán y Guanajuato, ya conformaban la región expulsora de fuerza de trabajo.

En este proceso migratorio han influido diversos factores, como la precariedad y el carácter excluyente de la estructura productiva de Zacatecas, caracterizada, entre otras cosas, por un sector industrial limitado, una actividad agrícola poco tecnificada y orientada a la subsistencia familiar; una ganadería de corte extensivo especializada en la cría de bovinos en pie y un sector minero, que casi no incide en el empleo y la economía regional. Estas características sitúan al estado como uno de los de menor capacidad para generar empleo en el país, tal como lo señala el Plan Estatal de Desarrollo (1999–2004). Resulta interesante notar que, aunque la economía zacatecana ha tenido periodos de expansión importantes, en la actualidad se sigue ubicando como uno de los estados con mayor pobreza y marginación del país.

El flujo migratorio sigue siendo negativo, aunque menor que en décadas pasadas. Se estima que la mitad de los zacatecanos viven fuera del estado. En Estados Unidos viven entre 800,000 y 1,000,000. La mayoría reside en Chicago, Denver, Dallas, Houston, Los Ángeles y Phoenix.

La mayoría de los zacatecanos son católicos, a pesar de que el porcentaje de población católica en toda la entidad disminuyó 0.7 %.

En el año 2010, según el más reciente Censo de Población y Vivienda del INEGI, el 94.4 % de los zacatecanos profesaban la religión católica, con lo que superó a Guanajuato que una década atrás encabezaba la estadística. Del 1,490,000 zacatecanos en el estado, 1,394,000 son católicos. Con excepción de la iglesia católica, prácticamente todas las religiones y cultos principales registraron aumento en su porcentaje de adeptos.

En lo que respecta a protestantes y evangélicos (históricas, pentecostales, neopentecostales) el INEGI reportó 41,878, lo que equivale al 2.8 % de la población, es decir, casi un punto más que lo registrado una década atrás. Grupos religiosos como los Adventistas, Testigos de Jehová, Iglesia de Jesucristo de los Santos de los Últimos Días) llegaron a 15,581 personas, lo que habla de un aumento de apenas 0.1 %. En todo el estado, solamente fueron contabilizados 94 judíos.

En Zacatecas los templos católicos juegan un papel importante en la cultura que, junto con las plazas, se consideran como el centro de las localidades. El templo más importante del estado es la Catedral Basílica de Zacatecas, considerada como el máximo exponente del barroco en toda América. El Obispo de la Diócesis de Zacatecas es el Monseñor Sigifredo Noriega Barceló, nombrado obispo de Zacatecas por el Papa Benedicto XVI el 2 de agosto de 2012.

Los servicios públicos son aquellos que cumplen una función económica o social (o ambas) y satisfacen primordialmente las necesidades de la comunidad o sociedad donde estos se llevan a cabo.

La tasa de natalidad del estado en 2012 se ubica en el 17.79 %, en 2010 se registraron 36,323 nacimientos. Además de tener una población muy joven, en el año 2009, poco más de la cuarta parte de la población en el estado (26.1%) es joven (15 a 29 años). La esperanza de vida al nacer es de 74.8 a nivel estatal, 72.1 años para los hombres y 77.7 años para las mujeres. En 2010 fueron registrados 8,202 defunciones, de lo anterior, las principales causas de muerte son las enfermedades del corazón, tumores malignos y diabetes mellitus.

Zacatecas cuenta con: Instituto Mexicano del Seguro Social, Instituto de Seguridad y Servicios Sociales de los Trabajadores del Estado, Cruz Roja Mexicana Delegación Zacatecas. La población de los servicios de salud de Zacatecas es estimada en 553,839 habitantes, mientras que la Delegación Estatal del ISSSTE tiene una población de 144,659 derechohabientes y en cuanto afiliados a oportunidades son en total 253,498. Cuenta con unidades de primer nivel de atención (426 unidades de atención y 16 unidades de especialidad); unidades de segundo nivel de atención (5 Hospitales generales, 15 unidades de atención y 11 hospitales comunitarios); El ISSSTE tiene un total de 35 unidades y 688 personas en recursos humanos.

El IMSS tiene 33 unidades de primer nivel y 2 de segundo nivel. Existen un total de 3,240 recursos humanos.

Los Servicios de Salud cuentan con 142 Unidades de primer nivel, 6 de segundo nivel y 75 unidades móviles. Los principales hospitales del estado son: Hospital General de Zacatecas, Hospital General de Fresnillo, Hospital General de Jerez, Hospital General de Loreto y el Hospital de la Mujer Zacatecana.

El estado de Zacatecas tiene una biblioteca por cada 6,250 personas. En el 2005, el 92.7% de la población de 15 años o más fueron alfabetos. El 98% de la población de 8-14 tienen la aptitud de leer y escribir. En el estado existen 1,350 escuelas de preescolar, 2,031 primarias, 1,159 secundarias y 185 bachilleratos.

Cronología de la Educación Superior en Zacatecas

1759 – Fundación del Colegio de San Luis Gonzaga en el lugar que ocupa actualmente la Unidad Académica Preparatoria no. 1 de la UAZ.

1826 – Fundación de la Escuela Normal de Enseñanza Mutua.

1832 – Inicia labores el Instituto Literario de Jerez.

1868 - El Instituto Literario de Jerez cambia de nombre a Instituto Literario de García, se establece la “Junta de Instrucción Pública, Industria y Fomento”, buscando el perfeccionamiento de los escuelas en la ciudad.

1877 - Bajo el gobierno de Trinidad García de la Cadena, se promueve la enseñanza superior para mujeres.

1912 – Durante la breve administración del Lic. José Guadalupe González, se construyó en la población de Guadalupe el edificio de la Escuela de Agricultura quedando abandonado al morir el gobernante, el 29 de diciembre de 2012.

1927 – Comenzó a funcionar la imprenta de Enrique García, el primer linotipo del Estado.

1935 – Son fusionadas la Escuela de Artes y Oficios de Guadalupe y el asilo de niñas, debido a dificultades económicas, quedando administrada por la Secretaría de Educación Pública.

1937 – Reabre sus puertas el Instituto de Ciencias, habiendo siendo clausurada años antes junto con las escuelas normales de Zacatecas y Río Grande.

1940 – Reabre sus puertas la Escuela Normal de Zacatecas.

1947 – Se iniciaron los Juegos Florales Ramón López Velarde, siendo reprimidos en el año de 1972.

1948 – Se inauguraron los cursos de primavera, con la finalidad de difundir la cultura en todo el Estado. Siendo el mismo año que tuvo la novena sesión del Congreso Mexicano de Historia.

1950 – 1956 - Durante el periodo de la administración de José Minero Roque, se enriqueció la Biblioteca Pública del Estado.

1957 – 1972 Estuvo en funcionamiento la Escuela Normal del Colegio del Centro.

1968 – El Instituto de Ciencias Autónomo se convirtió en lo que ahora es la Universidad Autónoma de Zacatecas, siendo su primer rector Magdaleno Varela Luján.

En el Estado de Zacatecas las decisiones que el gobierno toma en materia de política pública pero dirigida a la cuestión educativa, son los lineamientos que han de conducir y acompañar al modelo educativo vigente en cada sociedad. Son las reglas a seguir, las disposiciones que el Estado establece y que se aplica al Sistema Educativo. México es un país con una extensa diversidad de cultura, lenguas, tradiciones y costumbres por lo que nos caracteriza como país en lo cual hace énfasis en los planes y programas de estudio, y en el mejoramiento de la educación en México.

El estado de Zacatecas cuenta con las siguientes universidades, centros de Investigación e institutos:

Públicas

Privadas

Escuelas Normales

En esta entidad se encuentran una gran cantidad de monumentos, reconocidos internacionalmente por sus estilos barroco, gótico, churriguresco y colonial. La conquista religiosa fue llevada a cabo prioritariamente por la orden Franciscana los que fundaron el hospicio en 1558; en 1567 tenían ya un gran convento y 1603 se creó la custodia de provincia de Zacatecas. Posteriormente llegaron los Agustinos, Dominicos y Jesuitas quienes no solo evangelizaron a los indígenas sino coadyuvaron a volverlos sedentarios, por lo que alrededor de sus capillas y conventos se fueron fundando poblaciones que en la actualidad son ciudades muy populosas.
El Acueducto el cubo hacía llegar el vital líquido

hasta el sitio donde se ubica el monumento al general Jesús González Ortega y de ahí hacia una pila de agua que se ubicaba en la Plaza Independencia, partiendo del tiro de la mina del Cubo, de ahí su nombre popular. Durante los siglos XVII y XVIII también se desarrollaron interesantes muestras de arquitectura barroca, además en este periodo también se construyeron varias haciendas en varios municipios del estado como la hacienda del Condado de San Mateo en Valparaíso. Para el siglo XIX durante el porfiriato se construyeron en la entidad varios
edificios, principalmente en la capital, de los que destacan el teatro Calderon y el mercado González Ortega ubicados en el centro de la ciudad de Zacatecas.

De las construcciones religiosas se destacan la Catedral Basílica de Zacatecas, el ex convento de San Francisco, templo de Fátima, templo de Santo Domingo, ex templo de San Agustín, Santuario de Plateros entre otros.

Zacatecas cuenta con un gran número de museos, la mayoría ubicados en la ciudad de Zacatecas, tiene grandes acervos artísticos y con diversidad temática. Dichas colecciones son exhibidas en espacios históricos y artísticos (ex conventos, ex templos, edificios coloniales, señoriales residencias, ex reclusorios, centros de enseñanza, modernos recintos, etc), que para tal efecto fueron acondicionados adecuándolos a las exposiciones. Entre los museos del estado se encuentran el museo Rafael Coronel
donde se exhibe interesantes colecciones de arte popular mexicano, las cuales están encabezadas por la denominada "El rostro de México" y que está integrada por más de diez mil máscaras mexicanas; el Museo Pedro Coronel donde se encuentra una colección de arte universal con más de 1300 piezas de diversas culturas, incluyendo obras del propio artista y de otros como Pablo Picasso, Marc Chagall, Salvador Dalí y Kandinsky; el museo Zacatecano de cultura, el toma de Zacatecas, Museo Francisco Goitia, el museo Mina el Edén y el museo de arte religioso ubicado en la ciudad de Guadalupe, en este museo se encuentra una de las colecciones de pintura virreinal más importantes de México, así como el museo de arte abstracto Manuel Felguérez.

Otros museos del estado son: el museo de ciencias de la UAZ, la galería episcopal, el museo de Historia del Transporte y el Museo de las migraciones. De los museos arqueológicos los existentes son el museo arqueológico La Quemada y el museo arqueológico sitio Altavista; de los museos interactivos de la entidad son el museo interactivo Zig Zag, único museo en el estado de Cuarta Generación y el museo interactivo Casa Ramón López Velarde ubicado en la ciudad de Jerez. A lo largo del estado existen también varios museos regionales.

En la entidad se celebran las festividades de , también se destacan las conmemoraciones oficiales como el aniversario de la Independencia de México (16 de septiembre), Día de Muertos, aniversario de la Revolución Mexicana, etc. En la Semana Santa católica se llevan a cabo representaciones del viacrucis y crucifixión.

También ocurre el Internacional Festival de Teatro de Calle (la tercera semana de octubre), que presenta a compañías teatrales de México y el extranjero que ofrecen funciones en plazas, museos y otros espacios al aire libre de la ciudad de Zacatecas. Surgió en el año 2001, con el propósito de llevar el teatro a su recinto original, la calle, así lograr un mayor acercamiento de esta disciplina artística al público en general, especialmente a los niños y jóvenes.

En la entidad se celebran las festividades de , también se destacan las conmemoraciones oficiales como el Aniversario de la Independencia de México (16 de septiembre), Día de Muertos, Aniversario de la Revolución Mexicana, etc. En Semana Santa se llevan a cabo representaciones del viacrucis y crucifixión.

Las festividades en Zacatecas tienen características y matices muy especiales, aunque durante el año se realiza una programación de eventos, las galas festivas se muestran durante los grandes festivales como: el Festival Cultural Zacatecas (en las semanas Santa y Pascua); el Internacional del Folclor (celebrado en mes de julio); las tradicionales Morismas de Bracho (en la última semana de agosto); la Feria Nacional de Zacatecas (las dos primeras semanas de septiembre)

o el también Internacional Festival de Teatro de Calle (la tercera semana de octubre), que presenta a compañías teatrales de México y el extranjero que ofrecen funciones en plazas, museos y otros espacios al aire libre de la ciudad de Zacatecas. Surgió en el año 2001, con el propósito de llevar el teatro a su recinto original, la calle, así lograr un mayor acercamiento de esta disciplina artística al público en general, especialmente a los niños y jóvenes.

Las fiestas y ferias en el interior del estado son principalmente para festejar a un santo patrón determinado. Entre las festividades tradicionales que más destacan son las «callejoneadas», en donde se recorren los escondidos callejones de la ciudad, saboreando un exquisito mezcal de Huitzila y degustando una muestra de la gastronomía regional.

Otras fiestas del estado son: la jerezada, la feria de primavera de Jerez, el festival barroco del museo de Guadalupe y el festival de la poesía zacatecana. Las fiestas patronales de algunos municipios del estado son:


En el estado de Zacatecas, al igual que en otras regiones de México, el desarrollo económico o la adopción de nuevas pautas culturales provocaron en muchas comunidades urbanas o rurales, un progresivo abandono de la producción en diversas ramas del artesanado tradicional.

Sin embargo, en los últimos años se vienen realizando intentos desde algunos organismos oficiales para preservar la influencia de aquellos maestros artesanos que aún subsisten y propagar sus conocimientos. Una encuesta reciente del IDEAZ (Instituto de Desarrollo Artesanal de Zacatecas) ha permitido elevar el censo de artesanos de 150 a 1500 en toda la entidad. Entre otras iniciativas, se pretende crear un sello de alta calidad para la platería, ya que la plata zacatecana es de las mejores del mundo.

En Zacatecas las principales ramas artesanales actuales son la platería, la cerámica y alfarería, el tallado de cantera, los textiles, la talabartería y los trabajos con pita y lechuguilla. Se producen también artículos de madera tallada, cerámica, herrería artística, cestería, vidrio soplado, resinas, papel maché, y obras en rocas, piedras semipreciosas y metales como el cobre, el hierro y el oro. Objetos de cuero con pirograbados, muebles de madera confeccionados a mano, trabajos en piel y peluche, en macramé, muebles en miniatura, figuras de yeso y en chaquira. Se elaboran además productos artesanales comestibles, como bebidas a partir del agave, dulces típicos y comidas o platillos tradicionales.

Los trabajos de labrado en cantera, siempre han sido muy destacados en este estado. Esta actividad se ha preservado sobre todo en Fresnillo. Los textiles de Guadalupe, Jerez, Villa García y Zacatecas tienen son destacan. Se realizan allí sarapes (especie de manta campesina) y jorongos (con abertura a modo de poncho), chamarras (camperas) y otros artículos de lana, elaborados en telar de pedales.

El territorio huichol abarca zonas de la Sierra Madre Occidental principalmente en los estados de Jalisco y Nayarit, y menor en Durango y Zacatecas. Los huicholes producen collares, pulseras y colgantes elaborados con chaquiras (mostacillas), fajas y morrales de lana tejidos y bordados con motivos abstractos o naturistas, ropa y sombreros ceremoniales, etc. Estos objetos son originalmente valorados por su capacidad para brindar protección tanto física como espiritual para el usuario.

Por ser un estado ganadero, son muy valorados aquí los artículos en piel, sobre todo aquellos relacionados con la charrería, como ser monturas, cintos, botas, fundas de pistolas y arneses para caballos. En Jerez se elaboran sillas de montar y sus diferentes accesorios, al igual que productos utilitarios como bolsas de mujer (carteras) y monederos. Todos ellos son profusamente decorados (piteados) con hilos de pita que permiten resistir un uso intensivo. El trabajo con la fibra de pita es característico del suelo zacatecano.

En la región semiárida zacatecana, se produce una fibra conocida como lechuguilla, con la que se producen diversos objetos tejidos. Salvador Mazapil, Concepción del Oro y Melchor Ocampo son los tres municipios donde numerosos pobladores se sostienen del trabajo con este material El tallado de piedras preciosas o semipreciosas se hacen en la ciudad minera de Zacatecas, sobresaliendo las turquesas engarzadas en metal. A lo largo del estado existen varios museos regionales donde se exhiben artículos propios de la región.

Existen bailes muy reconocidos y recordados entre la población zacatecana como es el baile de mexicapan que representa un propósito vivido y que refleja el carácter del zacatecano: Recio, galante y fanfarrón. Existe un conjunto de melodías La Jesusa, Las Barracas, La Varsovina, El Barretero, La Botella, y el Diablo Verde; indiscutiblemente que con influencia europea, como resultado del auge minero de Zacatecas. En el sur de Zacatecas es famosa "la Danza de los Tastuanes" en los municipios de Apozol, Juchipila y Moyahua de Estrada.

En todo el estado zacatecano es famoso el corrido, en sus múltiples manifestaciones. En el sur del altiplano y norte de la sierra suelen encontrarse las bandas de aliento, mejor conocido como tamborazo. "La Marcha Zacatecas" de Genaro Codina, se ha oído por años en toda la república y el extranjero; es el himno de las asociaciones charras y está considerado, por su frecuente interpretación en actos oficiales como el "segundo himno nacional". Asimismo se puede escuchar por todo el territorio estatal la música norteña identificada principalmente por el acordeón.

Han sido célebres las interpretaciones de la internacional Banda de música del estado de Zacatecas, dirigida por Octaviano Sigala, Juan Pablo García y Salvador García; así como de la Orquesta Típica de señoritas, dirigida por Fernando Villalpando. Es importante destacar el tamborazo (proveniente de Jerez) que no puede faltar en las fiestas zacatecanas. Los jaraberos de Nochistlán, música considerada de las más antiguas del estado.

A partir de 2004, el gobierno del estado otorga la medalla al mérito musical "Candelario Huízar" a personajes destacados por su trayectoria en favor del desarrollo musical en Zacatecas. Los músicos galardonados han sido:

La entrega de la presea se realiza el 2 de febrero de cada año en conmemoración del natalicio de Candelario Huízar.

La música coral ha sido una de las principales manifestaciones musicales en el estado. Han sido localizados manuscritos de obra vocal para agrupaciones musicales en la Catedral Basílica de Zacatecas, el Convento de Guadalupe y la biblioteca de la Undad Académica de Artes de la Universidad Autónoma de Zacatecas cuenta con un acervo importante de manuscritos de compositores zacatecanos de los siglo XIX y XX, entre otros: Fernando Villalpando, Francisco Aguilar y Urizar, Manuel Barrón y Soto, Severiano González, Isauro Félix, Candelario Huízar, Samuel de la Trinidad Herrera, Luis G. Araujo y Octaviano Sigala.

Las agrupaciones corales más destacadas en el siglo XX fueron

La Sociedad Coral de Zacatecas y el Ensamble Vocal Polifonía mantuvieron actividad constante durante más de una década realizando conciertos, grabaciones, giras y participaciones en puestas en escena de ópera. Ambas agrupaciones participaron en varias ediciones del Festival Cultural de Zacatecas. A su vez, el Ensamble Vocal Polifonía obtuvo en dos ocasiones (1985 y 1987) el primer lugar nacional del Concurso Nacional de Conjuntos Musicales del Magisterio, convocado por la Secretaría de Educación Pública y realizado en el Conservatorio Nacional de Música.

A inicios del siglo XXI surgió en la ciudad capital el proyecto "Coro Monumental de Zacatecas" conformado por un coro de cámara y varios coros de niños, jóvenes y adultos mayores. El proyecto tuvo corta duración con actuaciones trascendentes en los actos oficiales del Ayuntamiento de Zacatecas.

A partir de 2009 se instituye el Coro del Estado de Zacatecas, única agrupación coral profesional activa en la actualidad. 

En 2007 se establece el Taller de Ópera de Zacatecas, que posteriormente derivó en lo que hoy es la Compañía de Ópera de Zacatecas. 

El estado cuenta con estas agrupaciones orquestales:
Entre otras

Es clara la diferencia en la indumentaria regional para la mujer y el hombre.



La gastronomía de Zacatecas es amplia y variada, es una cocina rica en sabores y aromas, se basa principalmente en productos como las carnes, el maíz, el chile, el jitomate, frutas y los frijoles, birria y tacos de canasta, cómo dijo Delia.

Entre los platillos principales destaca el asado de bodas, el cual está hecho a base de carne de puerco con una salsa de chiles y chocolate, y que, como su nombre lo dice, tradicionalmente se servía durante las bodas. Otros de los platillos principales de Zacatecas son la birria de carnero, el popular menudo, las enchiladas, las gorditas rellenas de guisados y la carne adobada. Las gorditas con chile y manteca nacen durante la Revolución, y eran preparadas por las adelitas para sus hombres, y las hacían básicamente con esos dos ingredientes. Hoy en día, existe una gran variedad de gorditas y panecillos que juegan un importante papel en la alimentación popular, se hacen de maíz o de trigo, dulces o salados, en comal o en el horno. Las gorditas pueden ser rellenadas de una gran variedad de guisos (mole con arroz, nopalitos con huevo, chicharrón con chile, rajas con papas, lengua, hígado, alambre, yesca, entre otros. También se acostumbran las panuchos y las semitas, que se elaboran con harina de trigo, leche, canela y azúcar, y decoradas con pasitas, coco o nuez, los condoches que son gorditas de maíz tierno que se cuecen sobre las hojas de elote.

En lo referente a los postres destacan las cocadas jerezanas, melcochas, ates (pasta dulce) de guayaba y membrillo, jamoncillos de leche, miel de tuna, así como los tradicionales dulces de camote, biznaga, chilacayote y calabaza. En época de cuaresma se consume la capirotada como postre principal.

Zacatecas cuenta con la tradición de producción de mezcal y el estado es parte de la lista de entidades con denominación de origen para la producción de mezcal junto con otros 7 estados. Además esta región produce vinos de mesa reconocidos internacionalmente, excelentes vinos tintos, blancos y rosados, ideales como acompañamiento en las comidas, dentro de otras bebidas se deben mencionar el pulque y el aguamiel, junto con el aguardiente de caña y el colonche la bebida regional por excelencia, que se obtiene de la fermentación del jugo de la tuna cardona.

Como uno de los productores de uva más importantes de México el estado desarrolló una cultura vitivinícola importante a partir de la década de 1970. En los años ochentas se instaló Pedro Domecq en el municipio de Ojocaliente donde inició la producción de brandy y vino de mesa. En 1986 Jesus López López, expresidente de la Asociación Mexicana de Vitivinicultores, comenzó a producir el vino Cacholá en el Valle de las Arsinas, el cual ganó la primera medalla para un vino zacatecano en el Challenge International Du Vin de Burdeos. En 1987 el empresario Isauro López López funda Viñedos Campo Real, ubicados en el municipio de Trancoso y en 1988 produce su primer vino, Reserva del Patrón, de uva Ruby Cabernet. Después de un hito varios años, en 2010 los hijos de Isauro López López resumen la producción de vino en Viñedos Campo Real bajo el nombre de Tierra Adentro. La vinícola se ha convertido en la más prolífica del estado y ha conseguido ganar más de doce galardones por sus vinos. En 2018, Isauro López Muñoz introduce al mercado el Vino López Rosso, producido bajo la dirección del Dr. Joaquín Madero, y en 2019 obtiene su primer galardón en el Concours Mondial de Bruxelles Edición México.

El Estado de Zacatecas puede considerarse que se encuentra bien comunicado, dado que se encuentra en la zona centro-norte. Existe una red ferroviaria que actualmente solo se utiliza para transporte de carga con una longitud de 671 km. Asimismo se cuenta con varias centrales de autobuses a lo largo del estado donde las más importantes son la de Zacatecas y la de Fresnillo, cuanta además con un Aeropuerto Internacional, que tiene servicio nacional e internacional con vuelos nacionales a Tijuana e internacionales a Chicago-Midway y Los Ángeles; cuenta con 5 aeródromos en los municipios de Mazapil, Tlaltenango y Huanusco.

En cuanto a caminos, la red carretera federal, estatal y rural tiene una longitud de 11,842 km y cuenta con una red de carreteras pavimentadas que cruzan su territorio, asimismo cuenta con una amplia red que comunica la capital con todos los .

Las principales rutas que cruzan el estado son: carreteras federales, la Carretera Federal 23 Fresnillo, ZAC - Chapala, JAL; la Carretera Federal 25, Aguascalientes, AGS - Loreto, ZAC; la Carretera Federal 44 Fresnillo - Valparaíso; la Carretera Federal 54 Saltillo - Zacatecas.

El estado cuenta con 188,438 líneas de teléfono fijas, 851 oficinas postales, 30 oficinas de telégrafos, operan en el estado 19 radiodifusoras (13 de amplitud modulada y 6 de frecuencia modulada) así como 16 estaciones televisoras. La cobertura de la radiodifusión de amplitud y frecuencia modulada.

En la entidad se publican cinco periódicos, Imagen, Página 24, El Sol de Zacatecas (fundado en 1965, el cual es uno de los diarios con mayor difusión y ventas en el estado con un tiraje de 17,217 ejemplares, de periodicidad diaria y miembro de la Organización Editorial Mexicana), La Jornada Zacatecas y el Diario NTR; dos canales de señal abierta de televisión con 12 repetidoras de Televisa y TV Azteca, así como una gran variedad de programas locales por cable y publicaciones impresas en municipios.

Debido a su ubicación geográfica, Zacatecas está conectado con los principales puertos y centros económicos del país. El Estado de Zacatecas tradicionalmente ha tenido muy pequeña su aportación al producto interno bruto (PIB). En la actualidad su participación en el total nacional es tan solo de 0.9 %.

Zacatecas recibió 5.9 millones de dólares por concepto de inversión extranjera directa (IED) en 2011. La industria manufacturera fue el principal destino de la inversión extranjera directa recibida por el estado en el año de referencia.
Zacatecas forma parte del recién creado Corredor Económico del Norte de México, integrado por los estados de Chihuahua, Coahuila, Durango, Nuevo León, Sinaloa, Tamaulipas y Zacatecas.

La PEA de 12 años en adelante es de 358,449 personas, esto es el 37.5%, ubicando a Zacatecas 11.8 puntos por debajo de la media nacional, que es de 49.3%. Sobresale la población económicamente inactiva, donde 62 de cada cien personas no trabajan, lo que nos ubica 12 puntos arriba el parámetro nacional. La población ocupada, es de 353,628 personas (98.66%) de las cuales el 53.2% son empleados y obreros, el resto trabaja por su cuenta o está en otra situación. En esta misma categoría sobresalen las mujeres, donde 68 de cada cien trabajan como empleadas y obreras. La población ocupada se encuentra distribuida de la manera siguiente: 73,126 personas en el sector primario, 94,549 en el sector secundario y 174,981 en el sector terciario; en este último se concentra casi el 50% de la población ocupada.

Están clasificadas como primarias: la agricultura, la ganadería, la silvicultura, la pesca, la minería, etc.

Agricultura

Los productos agrícolas que se cosechan son cereales (la producción de los cuales depende de la intensidad de las precipitaciones), y el maguey, el cual depende de la irrigación de los valles bajos, y que se desenvuelve con facilidad en climas secos.

De acuerdo a las cifras del Censo del 2000, se sembraron en el Estado 1’303,564 hectáreas, de las cuales a los cultivos cíclicos corresponden 1’241,824 hectáreas, y a los perennes apenas 61,735. Dentro de los cíclicos la mayor superficie se siembra de fríjol 755,615 hectáreas pero dado la baja o depresión del precio de esta leguminosa se espera que se sembrara una menor superficie en el futuro próximo. Cabe mencionar que en la siembra de maíz fue de 356,166 hectáreas, 64,177 de avena forrajera; 34,150 de chile, 5,248 de cebada, 4,025 de avena, además de siembra de sorgo, cebolla, ajo, etc. Por lo que respecta a los perennes se siembran 22,012 hectáreas, de durazno, 14,181 de nopal, 8,605 de alfalfa, 5,246 de guayaba etc.

Los cultivos cíclicos que más se siembran son: fríjol, maíz, avena forrajera, chile y cebada. Además se siembran avena, sorgo, cebolla, ajo, durazno, nopal, alfalfa y guayaba. En total, se usan 1,303,564 hectáreas para la agricultura.

Ganadería y pesca

La ganadería es también prioritaria en la economía pues el estado de Zacatecas cuenta con grandes extensiones de agostadero 5’388,434 hectáreas susceptibles de aprovecharse en actividades ganaderas. De acuerdo al Censo del año 2000, se producen anualmente 1’037,287 cabezas de ganado bovino, entre: producción de carne, leche,245,762 cabezas de porcino, 310,023 de cabezas de ganado ovino; 546,414 caprinos; 209,707 equinos entre caballos, asnos y acémilas; 1,862,726 aves gallináceas y 30,442 guajolotes y 46,426 colmenas entre rústicas y modernas. Cabe destacar que a pesar de que el Estado carece de litorales, el volumen de captura de productos acuícola es de 5,095 toneladas destacando la tilapias, la carpa, el bagre y la lobina.

Silvicultura
Con respecto a la silvicultura se obtienen 58,344 m en rollo de productos forestales maderables, siendo los principales el encino y el pino.

Minería

La minería es por mucho una de las actividades más antiguas realizadas en Zacatecas, así como una de las más importantes, destacan principalmente la extracción de plata, oro, mercurio, hierro, zinc, plomo, bismuto, antimonio, sal, cobre, cuarzo, caolín, ónix, cantera, cadmio y Wollastonita. Las riquezas minerales del estado fueron descubiertas poco después de la conquista, y algunas de las minas (y las más famosas de México) datan de 1546. Las más productivas son las minas de plata de Alvarado. Solo de esta mina se extrajeron más de 800 millones de dólares durante 1548 y 1867, según los registros que se han conservado. Hoy en día, México es el primer productor de plata del mundo.

Actualmente la zonas mineralizadas más importantes, contienen principalmente agregados minerales en forma de óxidos y sulfuros complejos de plomo, zinc y cobre, con pequeñas cantidades de plata y oro. Tales zonas están localizados principalmente en 13 distritos mineros, entre los que destacan por su importancia los de Fresnillo, Zacatecas, Concepción del Oro, Mazapil, Sombrerete y Chalchihuites, siendo más importante en estos últimos años Noria de Ángeles.

Existen 86 unidades económicas en la actividad minera. La manufactura es un sector de la economía en crecimiento, de estas la industria alimenticia y de bebidas es la más grande. Cabe destacar que en las minas citadas también existen importantes yacimientos de minerales no metálicos de uso industrial como: caolín, ónix, cantera, wallastonita y cuarzo, entre otros.

Este sector se refiere a las actividades industriales, aquellas que transforman los recursos del sector primario.

La manufactura es un sector de la economía en crecimiento, de estas la industria alimenticia y de bebidas es la más grande; de la industria manufacturera zacatecana sobresale la elaboración de cerveza, que además de aparecer como la actividad industrial más importante al aportar el 24.8% de la producción bruta total de la entidad y 26.6% de los activos fijos, también ocupa el primer lugar en la generación del valor agregado. asimismo destacan por su aportación al Producto, la actividad de Alimentos Bebidas y Tabaco con el 55.93%, seguido de Productos Metálicos, Maquinaria y Equipo con el 22.41%. Los productos que tuvieron mayor dinamismo en Zacatecas por su tasa de crecimiento anual fueron: los productos de minerales no metálicos, seguidos de los productos metálicos, maquinaria y equipo.

Actualmente se trata de consolidar el Parque Industrial de Fresnillo, al cual se dotó de infraestructura en su área de reserva territorial, con la urbanización de 15 hectáreas (ampliación del Parque Industrial Fresnillo), donde se obtuvo facilidad de acceso y conectividad para el establecimiento de nuevas empresas, también se han creado 10 nuevas naves industriales, de 500m2 cada una, totalmente acondicionadas para ese fin.

En el Parque Industrial Calera se realizan trabajos de nueva infraestructura, la cual se ejecuta en sus vialidades respecto a pavimentos, alumbrado público, cerco perimetral, módulos de acceso y parador de autobuses. La industria aeroespacial se instaló en Zacatecas. La empresa Grupo Everest, del sector de proyectos industriales de manufactura en México, ha consolidado la instalación de una empresa ancla, triumph group, dedicada a la manufactura de partes para aeronaves, ultimadamente como resultado de la inversión japonesa se inauguró otra empresa japonesa en la entidad llamada Koide Kokan México.

Comercio

Cuenta con 48,257 unidades económicas, el 1.3 % del país, en cuando a ocupación emplea 174,368 personas, el 0.9% del personal ocupado de México. Del total del personal ocupado en la entidad, el 60 % (103 894) son hombres y el 40 % (70 474) son mujeres, en promedio, las remuneraciones que recibe cada trabajador al año en Zacatecas son de 72,211 pesos mexicanos, el promedio nacional es de $ 99,114.

En la entidad, se registra la existencia de 265 hoteles y 6,815 habitaciones, 604 establecimientos de preparación en servicios de alimentos, bebidas, recorridos nocturnos de leyendas, tours en tranvías, servicio de guías por hora, entre otros servicios turísticos. El estado de Zacatecas posee comunidades coloniales con reconocimiento internacional como Zacatecas, capital del estado; Guadalupe, Fresnillo, Sombrerete y Jerez de García Salinas.

Las localidades más visitadas son:

Mercado y la Plaza de Toros San Pedro (hoy convertida en hotel, uno de los más bellos de Latinoamérica), también es famosa por su arquitectura barroca y churrigueresca, aunque en realidad el único edificio puramente barroco es la fachada principal de la catedral y sus dos torres, de las cuales la torre norte se terminó hasta 1904, así como los 8 retablos bañados en oro de los altares laterales del templo de Santo Domingo, existe una iglesia de estilo gótico que es el templo de Fátima, construido a mitad del siglo XX única construcción de su estilo en el estado. Ciudad declarada Patrimonio Cultural de la Humanidad por la UNESCO en 1993. En Zacatecas se puede admirar el pueblo desde un teleférico que cruza el centro desde el Cerro de la Bufa hasta el Cerro del Grillo.

Se realiza cada año durante la Semana Santa una Semana Cultural, donde todas las expresiones del arte se manifiestan en teatros, y escenarios naturales como plazas, plazuelas, calles y callejones, Esta semana cultural recibe artistas de todas partes del mundo y es considerado uno de los festivales culturales más importantes de América, solo después del Festival Cervantino.

El último jueves, viernes, sábado y domingo del mes de agosto se celebran las Morismas de Bracho, una representación de la batalla de Lepanto, moros contra cristianos sucedida en 1571, es una de las 3 representaciones del mundo junto con la de Granada y Alicante en España, siendo esta, la de Zacatecas, la más monumental, con cerca de 15,000 actores y que tiene más de 100 años de tradición.
Del último domingo de julio al primer domingo de agosto se realiza el festival zacatecas del folclor internacional "Gustavo Vaquera Contreras" donde nos visitan grupos de danza de los cinco continentes, así como de los estados de México, es considerado uno de los mejores 5 festivales del mundo, siendo el más importante del continente americano, los bailables son en plazuelas, plazas y lugares especiales, las presentaciones de gala que se hacen en el Teatro Fernando Calderón y Ramón López Velarde.

En septiembre se realiza la Feria Nacional de Zacatecas (FENAZA) durante las tres primeras semanas del mes, con eventos como corridas de toros, peleas de gallos, exposiciones, variedades artísticas y diversiones, considerada una de las 3 ferias más importantes de México y es una de las pocas ferias que no cobran el acceso a sus instalaciones.

También cada año en el mes de octubre, la ciudad de Zacatecas mantiene vigente su cita internacional con el festival de teatro de calle, siendo el único festival en su género en todo México, abriendo las puertas de sus plazuelas y callejones a las compañías que han hecho del espacio abierto, el escenario de su creatividad artística, donde invaden todos los espacios posibles para adentrarse en los sentidos de cada uno de los espectadores, para eliminar la barrera que divide la vida cotidiana del mundo de los sueños.

De esta forma, en el transcurso de estos ocho años, el Festival Internacional de Teatro de Calle se ha consolidado como uno de los proyectos culturales que han logrado alcanzar un importante objetivo: movilizar e integrar al público con los actores, en el marco incomparable de un espacio urbano patrimonio de la humanidad.

Todo el año Zacatecas realiza con eventos culturales diversos, y se cuenta con una gran infraestructura hotelera, siendo una de las mejores opciones para el turismo nacional e internacional.

Es el lugar del famoso Santo Niño de Atocha una imagen romana comprada por México a España

Centro Histórico de Zacatecas

En 1972 se celebró la Convención del Patrimonio Mundial parte de la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura (UNESCO) y la Organización de las Naciones Unidas (ONU) en la ciudad de París en la que el Centro histórico de Zacatecas fue declarado Patrimonio de la Humanidad por la UNESCO en el año de 1993. Notable por su arquitectura y sus numerosos museos, la Catedral Basílica de Zacatecas, construida entre el 1730 y 1760, el Teatro Fernando Calderón, el Acueducto El Cubo, el Palacio de Gobierno, la Plaza de Armas, o el Museo de la Toma de Zacatecas, son solo algunos ejemplos.

Camino Real de Tierra Adentro

En el marco de la 34.ª reunión del Comité de Patrimonio Mundial de la UNESCO que se efectúa del 25 de julio al 3 de agosto en la capital brasileña de Brasilia, el comité votó y declaró al Camino Real de Tierra Adentro, también conocido como El Camino de la Plata como patrimonio mundial.

Dicho Camino es el más antiguo de América abarca una extensión de 2,900 km que parte desde la Ciudad de México hasta Santa Fe, Nuevo México, Estados Unidos. Dicha ruta fue trazada en el siglo XVI por los conquistadores españoles para desarrollar el comercio, facilitar las campañas militares, apoyar la colonización y evangelización en la Nueva España. Representa además uno de los puentes culturales más relevantes que unen a ambas naciones.

En Zacatecas los sitios que recibieron esta distinción son el ex Colegio Apostólico de Nuestra Señora de Guadalupe, en Guadalupe, la Cueva de Ávalos en Ojocaliente; el Santuario de Plateros en Fresnillo; el templo de San Nicolás Tolentino y el Centro Histórico de Pinos; los templos de Nuestra Señora de los Ángeles y Nuestra Señora de los Dolores, en Noria de ángeles y Villa González Ortega, el templo de Noria de San Pantaleón, Conjunto Histórico y Sierra de Órganos en Sombrerete; Conjunto Histórico de Chalchihuites, así como el Camino Real de Palmillas en Ojocaliente.

La UNESCO determinó que una carreta que forma parte de los murales prehispánicos de la Cueva de Ávalos será considerada el emblema de los 60 sitios que conforman el itinerario cultural declarado Patrimonio de la Humanidad. En este lugar fue plasmado en pintura rupestre el contacto con los colonizadores.

Las características de estos pueblos incluyen el estar ubicados en zonas cercanas a sitios turísticos o grandes ciudades, tener accesos fáciles por carretera. El Estado de Zacatecas cuenta con seis Pueblos Mágicos que son:

Son más de 500 zonas arqueológicas los que se encuentran en todo el territorio zacatecano, de los cuales destacan La Quemada y Altavista.

La Quemada

La Quemada es una zona arqueológica singular en el mosaico de los sitios mesoamericanos. En 1615, Fray Juan Torquemada la identificó como uno de los lugares visitados por los Aztecas en su migración hacia la cuenca de México.

Altavista

La zona arqueológica Altavista fue un centro ceremonial y astronómico producto de la rama súchil de la cultura Chalchihuites, cuya ocupación y desarrollo tuvo un período de aproximadamente 800 años. Esta zona es considerada como un importante centro ceremonial-astronómico de la cultura Chalchihuites, cuyos vestigios arqueológicos son: la Plaza de la Luna (o salón de columnas), la pirámide votiva, la escalera de Gamio y el Laberinto. En esta última se pueden apreciar con puntualidad y precisión los respectivos equinoccios de las estaciones.

Son considerados parques nacionales aquellas áreas que están protegidas mediante un decreto oficial, generalmente por el presidente, son regiones que cuentan con uno o más ecosistemas. En el estado de Zacatecas, se cuenta con un parque nacional en el municipio de Sombrerete.

Parque Nacional Sierra de Órganos

Es un área que corresponde al municipio de Sombrerete, en el estado mexicano de Zacatecas. El lugar cuenta con varias elevaciones escarpadas y formaciones rocosas caprichosas originadas por diversos factores climáticos y geológicos, con forma similar a la de las cactáceas o a la de las pípas de aquellos instrumentos musicales de los cuales toma el nombre.

Aunado a la presencia de bosques de coníferas en las zonas altas y matorral y plantas xerófilas en las bajas, es refugio de algunas especies de animales endémicas de la región y otras en peligro de extinción. Es un lugar concurrido para la práctica de los deportes extremos como el ciclismo, el senderismo y también por la belleza escénica de sus paisajes. Además ser escenario desde los años 1950’s de numerosas películas de talla internacional.

El estado participa dentro del Sistema Nacional de Cultura Física y Deporte en la región II conformada por los estados de Chihuahua, Durango y Zacatecas.

Igualmente en el Consejo Nacional del Deporte de la Educación participa en la Región Norte, zona integrada por las universidades de los estados de Chihuahua, Durango y Zacatecas. Por Zacatecas compiten las siguientes universidades:
- Universidad Autónoma de Zacatecas
- Universidad Autónoma de Durango. Campus Zacatecas
- Escuela Normal Manuel Ávila Camacho
- Escuela Normal Rural Matias Ramos

El profesionalismo se encuentra representado por el equipo de fútbol Mineros de Zacatecas
que compite en Liga de Expansión MX, la segunda categoría más alta en el balompié mexicano
, y el equipo homónimo y los Plateros de Fresnillo de Basquetbol de la Liga Nacional de Baloncesto Profesional de México.

En tiempos pasados el estado solía contar con equipos muy competitivos como los Mineros de Zacatecas del desaparecido Circuito Mexicano de Basquetbol del que fue campeón en 2003, los Tuzos de la UAZ equipo de béisbol profesional de los años ochenta, y la Real Sociedad de Zacatecas finalista en 1997 de la Primera A.

Fundadores y conquistadores


Artistas

Política y Militar

Religiosos


Deportistas

Intelectuales




</doc>
<doc id="3015" url="https://es.wikipedia.org/wiki?curid=3015" title="Zootecnia">
Zootecnia

La zootecnia es una ciencia que estudia diversos parámetros para el mejor aprovechamiento de los animales domésticos y silvestres, pero siempre teniendo en cuenta el bienestar animal ante todo y si estos serán útiles al hombre con la finalidad de obtener el máximo rendimiento, administrando los recursos adecuadamente bajo criterios de sostenibilidad del ambiente.

Se ocupa del estudio de la producción de animales, así como de sus derivados (carne, huevo, leche, piel, etc.), teniendo en cuenta el bienestar animal; fijándose como objetivo la obtención del óptimo rendimiento de las explotaciones pecuaria de tu abono

En la mayor parte de los países latinos existen dos palabras que en los anglosajones se confunden. En castellano se distingue perfectamente entre zootecnia y ganadería, arte u objeto práctico de esta ciencia. Lo mismo sucede en francés entre "zootechnie" y "elevage", y en italiano entre "zootecnica" y "allevamento". En inglés, por el contrario, el vocablo "zootechnics" constituye un neologismo técnico poco empleado, y está más generalizada la voz "animal breeding" para designar tanto a la ciencia como al arte aplicado a la cría animal.

La ingeniería en zootecnia es una rama de la ingeniería que tiene formación integral de diversas ciencias como la matemática, química, bioquímica, biología, ecología, microbiología, cultivos, anatomía, fisiología vegetal y animal, biotecnología reproductiva, mejoramiento genético, sanidad, tecnología de carnes y leches,tecnología y gestión de crianza animal, gestión ambiental, además de economía, desarrollo empresarial y marketing.

Los ingenieros zootecnistas o zootecnistas son personas con capacidad de observar y analizar holísticamente todos los fenómenos involucrados con la producción animal, mejoramiento genético, pastos y forrajes, reproducción animal, sanidad preventiva, nutrición animal y economía animal. Tiene también injerencia a la administración de empresas agropecuarias, industrias, ecología y conservación del medio ambiente.

Los campos de actividad del ingeniero zootecnista se encuentran en el sector público y privado relacionados con el planeamiento y conducción de industria pecuarias y la administración de empresas agropecuarias pueden desempeñarse como:




</doc>
<doc id="3019" url="https://es.wikipedia.org/wiki?curid=3019" title="Épica">
Épica

La épica (del adjetivo: "ἐπικός", "epikós"; de "ἔπος", "épos", "palabra, historia, poema")es un género narrativo en el que se presentan hechos legendarios o ficticios relativos a las hazañas de uno o más héroes y a las luchas reales o imaginarias en las que han participado.

Su forma de expresión tradicional fue la narración en verso,cuya finalidad última era la exaltación o engrandecimiento de un pueblo. En algunos casos, la épica no tenía forma escrita, sino que era contada oralmente por los rapsodas. Con posterioridad la épica adoptó también la forma narrativa en prosa, incorporando elementos de descripción y diálogo y dando lugar, en primera instancia, a la novela de caballerías, a la novela picaresca o antiheroica y posteriormente a la novela realista o burguesa, entre otros géneros menos cultivados.

Alternancia de discursos que tiene como origen la observación aristotélica de la diferencia entre mímesis y diégesis, es decir, entre narración y descripción.

El género épico se encuentra en todas las literaturas, pues es un género esencial, y se puede dar y se ha dado históricamente en formas muy diferentes.
Los sumerios ("Epopeya de Gilgamesh"), griegos ("Ilíada", "Odisea"), romanos ("Eneida"), hinduistas ("Majabhárata") y persas ("Shahnameh") compusieron epopeyas en torno a las hazañas de un héroe arquetípico, que representaba los valores tradicionales colectivos de una nación, y otros personajes como dioses y hombres, incluyendo además elementos fantásticos. 

Edad antigua: Sus personajes principales eran dioses, los cuales tenían capacidades completamente superiores a las de un humano. En este tipo de relatos habían batallas sobrenaturales y hazañas increíbles 

En la Edad Media la epopeya se denominó cantar de gesta, y en ella empezaron a escasear los elementos divinos y fantásticos. En Francia se compusieron la mayoría de ellas, y la más influyente fue la "Chanson de Roland" o "Cantar de Roldán". En España se compuso el "Cantar de Mío Cid", entre otros. Los alemanes compusieron el "Cantar de los Nibelungos", y los sajones el "Beowulf". En Inglaterra, no llegaron a reunirse leyendas dispersas en torno a Robin Hood, pero se escribieron en prosa historias sobre un hipotético rey llamado Artus o Arturo. En Islandia, las sagas, aunque tienen un marcado carácter histórico, se emparentan con esta tradición narrativa, sobre todo en las sagas arcaicas como la "Volsunga Saga". 

Con el paso a los tiempos modernos, la epopeya empezó a estar protagonizada no por héroes y dioses, sino únicamente por personas vulgares y corrientes, cuya única hazaña era la supervivencia o conseguir una mejor condición social; de igual manera, las hazañas fantásticas fueron sustituidas por una tendencia realista. Esa fue la gran contribución de novelas como la anónima novela picaresca española "El lazarillo de Tormes" y, sobre todo, las dos partes del "El ingenioso hidalgo don Quijote de La Mancha" de Cervantes, que desacreditaron por completo los restos de epopeya que venían de la Edad Media, encarnados por los llamados libros de caballerías. El "Quijote" supone, pues, el nacimiento de la novela moderna realista y polifónica, escrita en prosa, y cuyos protagonistas son personas vulgares y corrientes que se mueven en ambiente realista, sin hechos sobrenaturales y sin que intervengan los dioses. Este tipo de novela se desarrolló extraordinariamente en el siglo XIX, cuando la burguesía lo tomó como modelo para exponer sus 
inquietudes y como espejo de su nueva ideología materialista. La novela realista del siglo XIX es la epopeya de la clase media o burguesía.

El poema épico intenta reactualizar en los tiempos modernos la epopeya griega y romana, sus antecedentes, en un estilo generalmente lleno de reminiscencias y en rima consonante. A este género pertenecen, por ejemplo:

El cuento tradicional es una narración anónima de carácter oral que sirve para pasar el tiempo y se suele contar a los niños. En el siglo XVIII y XIX empezaron a recogerse y estudiarse. Colecciones de cuentos populares son las de los hermanos Jacob y Wilhelm Grimm en Alemania, o Charles Perrault en Francia.

La leyenda, escrita en verso o en prosa, es característica del siglo XIX y narra hechos con alguna base histórica de verdad, pero fabulándose mucho en ellos libremente. Cabe destacar, por ejemplo, las Leyendas de Gustavo Adolfo Bécquer.

El mito es una narración corta que tiene una función cognoscitiva o explicativa, etiológica, frecuentemente de carácter alegórico. Es por esto que su estudio está más relacionado con la mitología. En ella se habla a cerca de personajes divinos o extraordinarios, que forman parte de las creencias de una cultura.

El relato es una narración escrita de autor conocido, con pocos personajes y sin la complicación y meandros de que hace gala la novela clásica.

El romance o, en los países nórdicos, balada, es una narración corta en verso, casi siempre de carácter anónimo, surgida en general de la descomposición de los cantares de gesta medievales, aunque pronto fueron compuestos algunos romances y baladas por nuevos autores imitando los romances viejos ("La Balada del Caballo Blanco", de G. K. Chesterton).

La fantasía heroica o fantasía épica, es un subgénero del género fantástico, principalmente de la literatura, aunque también presente en la historieta, el cine fantástico y los juegos de rol, caracterizado por la presencia de seres mitológicos o fantásticos, la ambientación ficticia de carácter medieval, antiguo, indefinido o, en cualquier caso, sobre la base de sociedades tecnológicamente atrasadas, y un fuerte componente mágico y épico.



</doc>
<doc id="3020" url="https://es.wikipedia.org/wiki?curid=3020" title="Égloga">
Égloga

La égloga es un subgénero de la poesía lírica que se dialoga a veces como una pequeña pieza teatral en un acto. De tema amoroso, uno o varios pastores lo desarrollan contándolo en un ambiente campesino donde la naturaleza es paradisíaca y tiene un gran protagonismo la música. Como subgénero lírico se desarrolla a veces mediante un monólogo pastoril o, más frecuentemente, con un diálogo. 

La égloga es una composición en la que el poeta, sacrificado en uno o varios pastores, expresa su amor en un marco idealizado, lleno de belleza y amor.

Las primeras églogas fueron los "Idilios" (en griego, "poemitas" o "pequeños cantos") de Teócrito; luego los escribieron Mosco, Bión de Esmirna y otros autores bajo su influencia. El escritor latino Virgilio (siglo I a. C.) con sus "Églogas" (en griego, "selecciones") o "Bucólicas" añadió elementos autobiográficos, haciendo de cada pastor un personaje imaginario que encubría a un personaje real: Cayo Mecenas, Augusto, etc. Algunas de ellas llegaron a escenificarse en Roma. Otros autores latinos escribieron también églogas, como Nemesiano, Calpurnio Sículo o Ausonio.

Esta innovación pasó a la bucólica posterior, de forma que algunas veces los personajes de las églogas representaban personajes reales. A través de Giovanni Boccaccio y con el Renacimiento y la "Arcadia" de Jacopo Sannazaro el género se volvió a recuperar mezclándose las composiciones en verso en un marco narrativo en prosa, y se difundió por todo el mundo occidental, bien en verso, bien como églogas intercaladas en una novela pastoril cualquiera. En la literatura castellana, escribieron églogas Juan del Encina, Lucas Fernández, Garcilaso de la Vega, Juan Boscán, Lope de Vega, Pedro Soto de Rojas, Bernardo de Balbuena y Juan Meléndez Valdés.



</doc>
<doc id="3022" url="https://es.wikipedia.org/wiki?curid=3022" title="África">
África

África es el tercer continente más extenso, tras Asia y América. Está situado entre los océanos Atlántico, al oeste, e Índico, al este. El mar Mediterráneo lo separa al norte del continente europeo; el punto en el que los dos continentes se hallan más cercanos es el estrecho de Gibraltar de 14,4km de ancho. El mar Rojo lo separa al este de la península arábiga y queda unido a Asia a través del istmo de Suez, en territorio egipcio. Posee una superficie total de 30 272 922 km² (621 600 km² en masa insular), que representa el 20,4 % del total de las tierras emergidas del planeta. La población es de mil millones de habitantes, menos del 15 % del total mundial. El continente se divide en 54 estados soberanos siendo uno de ellos, Egipto, , además de dos estados con reconocimiento limitado y dos territorios dependientes.

Los primeros habitantes del continente con los cuales los romanos tomaron contacto fueron los Afri; con este término se aludia a todos aquellos que moraban al oeste del Nilo y que no eran ni griegos, como los de Cirene, ni púnicos, como los cartagineses. En ese sentido era el equivalente del griego Libia. La tribu de los "Afri" parece derivar su nombre o del fenicio "'afar", que significa, "polvoriento" o del bereber "ifri" (plural "ifran") que quiere decir: "caverna", siendo los "Afri" habitantes de las cavernas o trogloditas

El término se expandió bajo la dominación romana a toda una provincia: África proconsular y luego al continente.

Numerosas etimologías se han propuesto para este nombre; desde aquellas que lo hacen provenir de algún epónimo, como Flavio Josefo que lo relaciona con un nieto de Abraham de nombre Efer ("Ant. 1.15"), o de alguna expresión descriptiva, siendo la más divulgada la de Isidoro de Sevilla ("Etymologiae" XIV.5.2) donde la vincula a la palabra latina "aprica"; soleada. El escritor y explorador andalusí, León el Africano traza el origen del nombre a un supuesto e inexistente compuesto griego: "a-phrike": "sin frío".

Se cree que África es la cuna de la humanidad y que de allí proceden las sucesivas especies de homínidos y antropoides que dieron lugar a los seres humanos. La teoría explica que allí se originó el "Homo sapiens" hace cerca de 300 000 años para luego expandirse por el resto de los continentes.

Según el historiador griego Heródoto (484 a. C.), una expedición fenicia auspiciada por el faraón Necao II (616 a. C.) circunnavegó el continente africano por primera vez.

Los orígenes del tráfico comercial entre el oeste y el centro de África y la cuenca mediterránea se pierden en la prehistoria. Los primeros relatos históricos datan de la antigüedad y versan sobre los nómadas que organizaban el comercio entre Leptis Magna y el Chad. Este comercio vivió su primer auge en el siglo I a. C. con el ascenso del Imperio romano. Sobre todo se comerciaba con oro, esclavos, marfil y animales exóticos para los juegos de circo en Roma a cambio de bienes de lujo romanos. De hecho es en esta época en la que se gesta el propio nombre de África. Tras la derrota de Cartago por Roma en la tercera guerra púnica se establece la provincia romana de África que abarcaría aproximadamente el Túnez actual. Fue una generalización territorial de la provincia lo que dio nombre a todo el continente. Una importancia crucial tuvo también la mayor utilización del camello a partir del siglo I en el norte de África.

A partir del siglo VII los árabes invaden el África del norte. El comercio caravanero y la expansión islámica alimentan el establecimiento de nuevas relaciones entre las «dos Áfricas».

El Imperio Kanem-Bornu existió en África entre el siglo XIII y la década de 1840. En su momento de mayor esplendor abarcó el área de lo que actualmente es el sur de Libia, Chad, noreste de Nigeria, este de Níger y norte de Camerún.

El Reino del Congo fue un estado situado en lo que actualmente constituye la zona norte de Angola, el enclave de Cabinda, Congo-Brazzaville y la parte occidental de Congo-Kinsasa. Su área de influencia abarcaba también los estados vecinos.

La repartición colonial de África por las potencias europeas, iniciada a partir del siglo XVII, tuvo lugar aproximadamente en 1885, con la conferencia de Berlín y el comienzo de la Primera Guerra Mundial, época en la que los imperios coloniales se extendieron más rápidamente en África que en cualquier otro lugar del mundo, si bien dos países, Liberia y Etiopía, consiguieron mantener su independencia. Es un ejemplo del Nuevo Imperialismo generado por la necesidad de los países europeos de obtener materias primas para el rápido crecimiento de su producción manufacturera después de la Revolución Industrial, iniciada en Inglaterra a fines del siglo XVIII.

Al final de la Segunda Guerra Mundial los aliados no logran ponerse de acuerdo sobre el futuro de la antigua colonia italiana de Libia. En ese momento era un territorio más de cinco veces mayor que la propia Italia. Sin embargo, la población no sobrepasaba el millón de habitantes, por lo que representaba un destino apropiado para la población desplazada de Italia por la guerra, que empezó a buscar lugares a donde emigrar. Los recelos entre Occidente y la Unión de Repúblicas Socialistas Soviéticas (URSS) hacen que finalmente la Organización de las Naciones Unidas (ONU) decida dar la independencia al país dejándolo en manos del rey Idris.

Aunque ya había cuatro países independientes en África (Liberia en 1847, Sudáfrica en 1910, Egipto en 1922 y Etiopía en 1941) Libia se convierte así en la primera colonia africana en lograr su independencia en 1951, a la que seguirá la de Ghana en 1957. Más adelante las potencias europeas lamentarían este hecho, pues contribuyó a desencadenar las diferentes luchas por la independencia africana.

En su mayor parte, África es una enorme y antigua plataforma continental maciza y compacta, elevada entre 600 y 800 msnm, surcada por grandes ríos (aunque pocos) y escasa en penínsulas. Destaca por su regularidad orográfica y considerable altitud media.

Tres franjas climáticas sucesivas se repiten al norte y al sur del ecuador, abarcando los climas mediterráneo, desértico, subtropical e intertropical lluvioso, este último, en sus dos tipos principales, tanto de sabana como de selva. África es el continente con mayor índice de insolación anual, lo cual podría haber dado origen a su nombre (África, del griego "a-phrike", ‘sin frío’).

Los suelos son excepcionalmente ricos en minerales y muy aptos para pastos. Debido al clima es allí donde evolucionó la mosca tsetsé y donde prolifera actualmente. Las principales áreas cultivadas se encuentran en las tierras altas orientales y la zona de los Grandes Lagos, algunos deltas y riberas e incluso en el Sahel.- Situación Astronómica Continental: Norte: Cabo Blanco, Túnez (37°20′ Norte) Sur: Cabo de las agujas, Rep. Sudafricana (35° Sur) Este: Cabo Hafún, Somalia (51°24′ Este) Oeste: Cabo Verde, Senegal (18° Oeste)



El continente africano está compuesto de 54 Estados soberanos, tres territorios dependientes y varios territorios integrados en Estados no africanos como Francia, España o Portugal. Las entidades políticas africanas anteriores al colonialismo desaparecieron con la expansión europea por el continente a finales del siglo XIX. Solo Abisinia, que se mantuvo independiente gracias a su victoria sobre los italianos en 1896 en la batalla de Adua, y Liberia, que fundada por el Gobierno estadounidense con esclavos liberados de su país en 1847, se mantuvieron independientes.
La mayoría de los países africanos lograron la independencia en el siglo XX a partir del proceso de descolonización tras la Segunda Guerra Mundial y que alcanzó su plenitud en los años 60. En 2011 surgió, hasta el momento, el último Estado en el continente, Sudán del Sur, tras conseguir la independencia de Sudán tras dos largas guerras civiles (1955-1972 y 1983-2005).

Todos los Estados africanos soberanos son miembros de pleno derecho de la ONU, contando entre ellos con cuatro Estados fundadores como fueron Egipto, Sudáfrica, Liberia y Etiopía.

En materia económica 52 Estados son miembros de la Organización Mundial del Comercio (ocho son observadores) mientras que Sudán del Sur y Eritrea no pertenecen a ella. Asimismo, la totalidad del continente se incluye en el Fondo Monetario Internacional aunque ocho Estados no cumplen el artículo VIII de la organización.

Es importante destacar la presencia del continente en la OPEP, ya que Argelia, Angola, Gabón, Libia y Nigeria son productores de petróleo.

En materia de justicia y seguridad todos los países africanos están integrados en la Interpol, sin embargo en el caso de la Corte Penal Internacional nueve países no han firmado ni ratificado el Estatuto de Roma, mientras que son diez los firmantes que aún no lo han ratificado. El resto de países acepta la jurisdicción del Corte Penal Internacional para juzgar casos de crímenes contra la humanidad.

También esta presente la Liga Árabe, que engloba a los países musulmanes del continente: Marruecos, Argelia, Túnez, Libia, Egipto, Sudán, Mauritania, Somalia y Yibuti.

Los Estados africanos (salvo Sudán del Sur) están adscritos al Movimiento de Países No Alineados.

En cuanto a las organizaciones transcontinentales, el continente africano esta presente en Asociación ribereña del Océano Índico para la cooperación regional (1995) de cooperación entre países asiáticos, Australia y nueve Estados africanos (Somalia, Tanzania, Madagascar, Seychelles, Mauricio, Mozambique, Kenia, Sudáfrica y Comoras). Durante la Guerra Fría (1986) se creó la Zona de Paz y Cooperación del Atlántico Sur, bajo el auspicio de las Naciones Unidas, con el objetivo de mantener la seguridad y la paz en el Atlántico Sur. Por iniciativa de Brasil se unieron 21 Estados africanos más tres sudamericanos.

En 1975 se creó Estados de África, del Caribe y del Pacífico (ACP) para, a través de varios acuerdos (el más reciente Acuerdo de Cotonú del año 2000) luchar contra la pobreza junto a la Unión Europea, que trabaja por medio del Fondo Europeo de Desarrollo. Forman parte de esta organización los 47 Estados africanos. La Unión Europea trabaja a través de la firma de acuerdos económicos con los cinco bloques regionales.

La principal organización política regional del continente es la Unión Africana (UA), heredera de varios intentos previos de unir políticamente al continente, a semejanza de la Unión Europea en Europa. Sus predecesoras son la Unión de Estados Africanos, creada por el ghanés Kwame Nkrumah en 1958, y la Organización para la Unidad Africana de 1963. Desde 1984 hasta el año 2017 Marruecos no formó parte de la UA como protesta por la admisión de la República Árabe Saharaui Democrática, con la que mantiene un contencioso por el Sáhara Occidental.

La principal organización económica es la Comunidad Económica Africana (CEA), fundada en 1981. El objetivo de la CEA es fomentar la integración y el desarrollo a través de la cooperación entre los estados africanos. Para ello utiliza un sistema de agrupaciones regionales como pilares básicos:
Fuera del paraguas de la CEA existen otras organizaciones de tipo económico, como la Comisión del Océano Índico, Autoridad de Liptako-Gourma, Unión del Río Mano o la Comunidad Económica de los Países de los Grandes Lagos.

Datos de superficie y población consultados en actualizados 1 junio de 2016.

De los 54 estados soberanos de África, 51 de ellos están constituidos como república, y tan solo 3 tienen forma de monarquía: Lesoto, Marruecos y Suazilandia.

Varios estados alcanzaron su independencia como monarquías pero a lo largo de los años se han ido convirtiendo en repúblicas: Egipto (1952), Ghana (1960), Kenia (1964), Burundi (1966), Malawi (1966), Libia (1969), Gambia (1970), Etiopía (1975) y Mauricio (1992).

Uno de los grandes males del continente africano es la proliferación de regímenes políticos autoritarios desde que los distintos estados fueron obteniendo su independencia. Atendiendo al Índice de democracia publicado anualmente por la Economist Intelligence Unit, en 2018 solamente una pequeña parte de los países africanos podían ser considerados como democráticos, mientras que más de la mitad se mantenían bajo formas políticas autoritarias o dictatoriales:
Desde que comenzó a publicarse este índice de democracia en 2006, la situación ha mejorado muy ligeramente. Han pasado a tener la consideración de países democráticos Ghana (2010), Senegal (2012) y Túnez (2014); por contra la han perdido Malí (2012) y Benín (2013). Temporalmente fueron considerados democráticos los regímenes de Zambia (2011-2015) y Malawi (2012-2013).

Han pasado de estar calificados como regímenes autoritarios a ser considerados como híbridos Sierra Leona (2008), Marruecos (2012), Burkina Faso (2013), Nigeria (2016) y Costa de Marfil (2018). En el sentido contrario, Burundi (2012) y Mozambique (2018) han pasado a ser catalogados como regímenes autoritarios. Temporalmente fueron considerados híbridos los regímenes de Níger (2011-2015), Mauritania (2011-2014), Egipto (2011-2012) y Libia (2012-2013), regresando todos ellos a la condición de autoritarios.

Dos territorios africanos aún tienen un reconocimiento limitado debido a conflictos territoriales. Así, la República Árabe Saharaui Democrática surgió como resultado del proceso de descolonización del Sáhara Español en 1976, siendo reconocido por 48 estados soberanos. El resto del territorio está ocupado por Marruecos que no reconoce la independencia y lo reclama como territorio propio.
Somalilandia se autoproclamó estado durante la crisis política somalí que desembocó en una guerra civil todavía latente. Somalilandia no es reconocida por ningún estado.
Datos de superficie y población consultados en actualizados 1 junio de 2016.

La dependencia francesa está compuesta por una serie de islas conocidas como Islas Dispersas del Océano Índico de las que la mayoría están deshabitadas y son reclamadas por países soberanos como Mauricio, Madagascar o Seychelles. Por su parte el territorio de ultramar de Santa Helena está incluido en el Comité de Descolonización de la ONU. Ambos territorios forman parte de los países y territorios de ultramar (o PTU) son las dependencias y territorios de ultramar de los Estados miembros de la Unión Europea que no forman parte de la Unión, sino que tiene un estatuto de asociados a los Estados miembros desde el Tratado de Lisboa.
Datos de superficie y población consultados en actualizados 1 junio de 2016.

Está formada por territorios integrados como parte de otros estados. Los territorios de las Islas Canarias, Reunión, Mayotte y Madeira forman parte de la Región Ultraperiférica de la Unión Europea, que, aun estando geográficamente alejados del continente europeo, forman parte indivisible de alguno de los veintiocho Estados miembros de la Unión.

Datos de superficie y población consultados en actualizados 1 junio de 2016.


En su condición de excolonias, la mayoría de los países africanos mantienen estrechas relaciones económicas con la Unión Europea (UE).
Existe una organización supranacional, tomando como referencia a la Unión Europea, llamada Unión Africana(UA), de la que forman parte todos los países del continente, incluida la República Árabe Saharaui Democrática. La mayor parte de los países africanos están subdesarrollados o en vías de desarrollo.

El 36,2% de la población, unos 350 millones de personas, viven con menos de un dólar al día. África paga cerca de 20 000 millones de dólares en pagos de deuda cada año, aun pese a las condonaciones de deuda de los años 1990.

Durante el régimen colonial los europeos explotaron los productos más fáciles y más provechosos de extraer, como el oro, el marfil, maderas y fibras textiles. Tras la emancipación de las colonias los más codiciados pasaron a ser el petróleo, los diamantes y la minería en general, pero estos productos mencionados se hallan en pocos países.

La carencia de buena tecnología y de medios de comunicación eficientes dificultan la explotación de dichas materias primas. El 60% de los trabajadores africanos realiza actividades rurales, y el 80% de lo que África exporta son materias primas, siendo a su vez los productos industrializados los que representan la casi totalidad de sus importaciones. Solo el 15% está empleado en el sector industrial, siendo Egipto, Sudáfrica, Túnez y Marruecos los que poseen casi el total de dicha actividad. El resultado es que África es el continente más pobre del planeta: su PBI representa tan solo el 2,6% del total mundial.

La ayuda exterior llega a los cincuenta millones de dólares cada año, y en los últimos 60 años esa ayuda ha sido de al menos mil millones. Sin embargo, esto ha empobrecido más a los países, ha ralentizado el crecimiento, los ha endeudado más, los ha hecho más propensos a la inflación y vulnerables a los vaivenes de las divisas, ha reducido el atractivo para la inversión y ha aumentado el riesgo de conflictos civiles. La ayuda exterior se transforma en deuda, que se paga a expensas de la educación y los servicios médicos africanos. Aun cuando se termina de pagar una deuda, los países vuelven a pedir más ayuda. A fin de paliar este círculo vicioso, la tendencia actual consiste en condonar la deuda externa a los países que demuestran un compromiso con el sistema democrático y con el desarrollo.

La asistencia ha estado afectada por corrupción, y los flujos han acabado beneficiando a las burocracias gubernamentales y ciertas ONG financiadas por algunos gobiernos. La corrupción le cuesta a África 150 millones de dólares al año. No existen incentivos para que los gobiernos busquen formas más transparentes para recaudar fondos para el desarrollo, solo se les pide a las agencias de donación una infusión de capital.

En contraposición, en otros países la ayuda ha servido para resolver problemas como las epidemias que diezman la salud y las vidas de la población activa (sida, malaria), la falta de infraestructuras básicas, el rendimiento agrícola, el analfabetismo y la carencia de educación primaria universal. Existen ejemplos de países, como Ghana, que demuestran emplear correctamente la ayuda.

El flujo de capital ayuda a que los gobiernos ineficientes sigan en el poder, ya que el presidente no tiene que hacer nada pues la ayuda sigue llegando, siempre y cuando pague al ejército. No tiene que subir los impuestos, ni preocuparse del descontento de los ciudadanos ni de la representación de estos. Los choques civiles a menudo son motivados por el conocimiento de que al hacerse con el poder, el ganador obtiene un acceso virtualmente completo al paquete de ayuda.

La ayuda hace que la burocracia se vuelva clientelista y envuelva a los ciudadanos con trámites innecesarios. En Camerún se tardan 426 días en hacer un procedimiento comercial y 119 días en Angola.

La ayuda alimentaria que compra comida cultivada en Estados Unidos quiebra a los agricultores locales. Se ha hecho poco para ayudar a los agricultores y se gastan millones de dólares en el programa.

La gran cantidad de dinero crea la "enfermedad neerlandesa": los grandes flujos de dinero hacen que la moneda local se fortalezca incrementando además los precios internos. Esto crea además inflación, por lo que los países deben emitir bonos. Uganda fue obligada a emitirlos en 2005, pagando intereses de $110 millones anuales.

China está presente en países con grandes recursos, por ejemplo petróleo, en Angola, que es su principal proveedor, y en otros países como son Guinea Ecuatorial, Nigeria, Chad, Sudán, Gabón, Zambia y República Democrática del Congo, estos dos últimos países productores de minerales.

Después de Estados Unidos y de la Unión Europea, China es el tercer socio más importante del continente, con inversiones en industrias de la construcción que están haciendo carreteras, embalses, viviendas, hospitales, y en la explotación de hidrocarburos y minerales. China tiene estrecha relación con Zimbabue, y Sudán, cuyos gobiernos son cuestionados.

Estados Unidos tiene interés en África por el petróleo.

Según el Programa de las Naciones Unidas para el Desarrollo (PNUD), África cuenta con ciudades subdesarrolladas, la mayoría de ciudades de África que poseen mayor desarrollo están en Sudáfrica y Egipto.

Hay aproximadamente 41 monedas oficiales distintas. La moneda oficial más extendida es el franco CFA en sus dos versiones: el Franco CFA de África Occidental moneda nacional en ocho países y el Franco CFA de África Central en seis. También esta presente el euro a través de los territorios españoles y franceses en el continente, así como la libra de Santa Elena, par a la libra esterlina, pero con sus propios modelos de monedas y billetes con respecto a la moneda británica.

Las estimaciones sobre la población no son precisas debido a lo obsoleto de gran número de censos nacionales. Se calcula, sin embargo, que viven en África no menos de mil millones de personas.

En África predomina la raza negra, cerca de un 80% del total de la población, a excepción de la franja costera mediterránea donde son mayoritarios, aunque no exclusivos, tipos humanos arabo-bereberes y caucasoides-mediterráneos. Entre el trópico de Capricornio y el trópico de Cáncer la población es casi en su totalidad negra, y suele ser subdividida en cuatro grupos principales, aunque siempre han existido en las zonas limítrofes entre estos grandes grupos pueblos más o menos mixtos en todas sus combinaciones. Tales grupos principales son sudanés, (Sahel y países del golfo de Guinea), nilótico, (Nilo, desde Sudán hasta los Grandes Lagos), cusita (Macizo etíope y Cuerno de África) y bantú, siendo este el más extendido, ya que ocupa toda el área a partir del cinturón selvático ecuatorial. Es además un tipo mixto relacionado con dos tipos antaño muy extendidos (hoy en día minoritarios), los twa y otros grupos mal denominados pigmeos, habitantes de los bosques, y los kung-san, mal denominados bosquimanos, de las zonas áridas del extremo sur.
Migrantes de origen francés se hallan establecidos en el Magreb y escasamente en las grandes ciudades de África Occidental, los de origen español habitan Marruecos y el Sáhara Occidental, mientras que en Angola y algunas ciudades costeras de África Occidental hay un número minoritario de grupos mixtos de origen africano-portugués. En el sur de África hay una significante cantidad (seis millones) de africanos blancos o afrikáneres, descendientes de neerlandeses y británicos.

La mayoría de los africanos mantienen un estilo de vida rural, pero la urbanización aumenta, ya que la gente abandona el campo para buscar trabajo en las ciudades. Las mayores densidades de población se encuentran donde el agua es más accesible, como en el valle del Nilo, las costas del norte y oeste, a lo largo del Níger, en las regiones montañosas del este y en Sudáfrica.

Aumento de la población desde el año 1 d. C. hasta el 2010 y proyecciones de población para los años 2050 y 2100.

En África las características de la población y su esperanza de vida varían según las condiciones. En África del Norte o en el desierto del Sahara, la mayor parte de sus habitantes son adultos y superan a la población juvenil, aunque no se da tampoco un envejecimiento progresivo. En el África subsahariana la mayor parte de sus habitantes son jóvenes, aunque en las últimas décadas se ha experimentado un crecimiento en la población adulta y un progresivo envejecimiento. Esto se da principalmente en países como Etiopía y Somalia, aunque en Sudáfrica también se experimenta un crecimiento de población adulta pero no es tan común el envejecimiento.

La población por sexo varía en el continente; al sur del Sahara, conocido también como el África negra, predominan las personas de sexo femenino, excepto en países como Angola, Mozambique, Etiopía, Somalia y Yibuti, entre otros. En cambio, en la mayor parte de los países del África del Norte predominan las personas de sexo masculino, excepto Marruecos, Sáhara Occidental, Mauritania y Chad.

Además de lenguas alóctonas como el árabe, el francés o el inglés (entre otras) cuya presencia en África se debe a procesos de conquista y dominación política. Se estima que en África actualmente existen unas 1700 lenguas autóctonas.

Demográficamente el árabe y el francés son las lenguas con más hablantes potenciales y las más extendidas en el continente. Las lenguas autóctonas de África con el mayor número de hablantes son el suahili (90 millones de hablantes), el oromo (70 millones), el hausa (40 millones) y el amhárico, todas ellas con un buen número de hablantes para los cuales es su segunda lengua y no su lengua materna (estas cuatro lenguas se usan ampliamente como "lingua franca" en sus respectivas áreas de influencia). Las lenguas europeas más extendidas son el francés, el inglés y el portugués, generalmente utilizados por las administraciones postcoloniales y las clases urbanas. A continuación existe un grupo de cerca de 20 idiomas étnicos con entre 1 y 20 millones de hablantes como: (de norte a sur) el wólof, manding (mandé), ewe, fon, yoruba, igbo, lingala, shona, setsuana, xhosa, malgache, etc. Otros idiomas minoritarios son el afrikáans y el español, de origen europeo, y otros autóctonos como el bereber. Los idiomas africanos y oficiales en sus respectivos estados son: el amárico hablado en Etiopía, el somalí en Somalia, el suajili en Kenia y Tanzania, el setsuana en Botsuana, el afrikáans en Sudáfrica y Namibia (junto con el inglés), y el malgache en la República de Madagascar (junto con el francés).

Las lenguas africanas autóctonas pertenecen a cuatro grandes grupos:

La mayor parte del continente profesa religiones tradicionales africanas, englobadas dentro del impreciso grupo conocido como animista. Esto significa que creen que los espíritus habitan objetos animados o inanimados. Dicho así mismo suele persistir bajo la apariencia de religiones universalistas como el islam o el cristianismo. También hay creyentes del rastafarismo.

El islam tiene una presencia dominante en el norte y destacada en el Sáhara, el Sahel, África Occidental y África Oriental. El cristianismo monofisita, aunque más antiguo que el islam, quedó confinado a Etiopía. A partir del siglo XX adquirirán una creciente importancia el catolicismo y protestantismo.

Sin embargo tanto islam como el cristianismo se encuentran en África con sincretismos más o menos sectarizados como el kimbanguismo o la Iglesia "Cita con la Vida", que persisten y se reproducen gracias a la fortaleza implícita de los conceptos de las religiones tradicionales. Las religiones tradicionales africanas tienen una presencia destacada en América, especialmente el vudú en Haití, la religión yoruba y las religiones del antiguo Reino del Congo en el Caribe y en Brasil principalmente.

Existen asimismo minorías hinduistas.

Los antiguos la personificaban bajo la figura de una mujer y con la de un escorpión. En una medalla del emperador Adriano lleva por casco o morrión la cabeza de un elefante. En otras varias medallas se observa que tiene en la mano derecha un escorpión y en la izquierda el cuerno de la abundancia; a sus pies un cesto lleno de flores y de frutos. El caballo y la palma son los símbolos de la parte de África vecina a Cartago.

En una medalla de la reina Cristina se ve una alegoría menos conocida: Atlas cubierto con la piel de la cabeza de un elefante guarnecida con su trompa y sus colmillos, contemplando los signos del zodíaco, para indicar que este rey, considerado por algunos como el inventor de la astronomía, reinó en África. Los modernos aprovechando de todas estas ideas han representado el África bajo los rasgos de una mujer mora, casi desnuda teniendo los cabellos rizados, llevando por casco una cabeza de elefante, un collar de coral, un cuerno lleno de espigas en una mano, un escorpión en la otra o un colmillo de elefante y acompañada de un león y de varias serpientes. Lebrun la ha pintado bajo la figura de una mora desnuda hasta la cintura, sentada sobre un elefante y en la cabeza un parasol que la pone enteramente a la sombra. Sus cabellos son negros, cortos y rizados: lleva por pendientes dos grandes perlas y sus brazos adornados con ricos brazaletes.

El teatro africano, entre tradición e historia, se está encauzando actualmente por nuevas vías. Todo predispone en África al teatro. El sentido del ritmo y de la mímica, la afición por la palabra y la verborrea son cualidades que todos los africanos comparten en mayor o menor medida y que hacen de ellos actores natos. La vida cotidiana de los africanos transcurre al ritmo de variadas ceremonias, rituales o religiosas, concebidas y vividas generalmente como verdaderos espectáculos. No obstante, aunque África ha conocido desde siempre este tipo de ceremonias, cabe preguntarse si se trataba realmente de teatro; a los ojos de muchos, estos espectáculos están demasiado cargados de significado religioso para que puedan considerarse como tal. Otros estiman que los tipos de teatro africanos guardan cierto parecido, como en otros tiempos la tragedia griega, como un preteatro que nunca llegará totalmente a ser teatro si no se desacraliza. La fuerza y las posibilidades de supervivencia del teatro negro residirán, por lo tanto, en su capacidad para conservar su especificidad. en el África independiente está tomando forma un nuevo teatro.

Nuevo Teatro: Se trata de un teatro comprometido, incluso militante, concebido para defender la identidad de un pueblo que ha logrado su independencia

Teatro de Vanguardia: Se orienta actualmente hacia una investigación sobre el papel de actor, próxima a la de Jerzy Grotowski y su teatro laboratorio. Así, en Libreville (Gabón), se formó en 1970 un teatro vanguardista que realizó dos espectáculos que dejaron una huella perdurable en las jóvenes generaciones de comediantes. Otra vía de investigación es el teatro de silencio, creado por François Rosira, cuyo fin era realizar espectáculos en los que el canto, el recitado, la música y el baile se complementen en perfecta armonía.

Asociaciones como Ndjembé promovían el carácter teatral en África.




</doc>
<doc id="3026" url="https://es.wikipedia.org/wiki?curid=3026" title="2000">
2000

2000 () fue un en el calendario gregoriano. Fue también el número 2000 anno Dómini o de la designación de Era Cristiana, además el último año del siglo XX y del segundo milenio y también el primer año de la década 2000.

El año 2000 a veces se abrevia como «Y2K» (la «Y» significa «año» —del inglés "year"—, y la «K» significa «kilo», es decir «mil»). El año 2000 fue el tema del Problema del año 2000, que temían que las computadoras no cambien de 1999 a 2000 correctamente. Sin embargo, para fines de 1999, muchas empresas ya se habían convertido a software existente nuevo o mejorado. Algunos incluso obtuvieron la certificación Y2K. Como resultado de un esfuerzo masivo, ocurrieron relativamente pocos problemas.

El año 2000 fue declarado:

























































Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.







</doc>
<doc id="3028" url="https://es.wikipedia.org/wiki?curid=3028" title="1978">
1978

1978 () fue un año normal que comenzó en domingo en el calendario gregoriano.

Según el horóscopo chino, 1978 fue Año de Caballo.






































10 de julio: Martina Navratilova llega a ser la número 1 del mundo.













</doc>
<doc id="3029" url="https://es.wikipedia.org/wiki?curid=3029" title="2002">
2002

2002 () fue un año común comenzado en martes según el calendario gregoriano. Fue también el número 2002 anno Dómini o de la designación de Era Cristiana, además del segundo del tercer milenio y del siglo XXI, el tercero de la década de 2000.
















































Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.





</doc>
<doc id="3030" url="https://es.wikipedia.org/wiki?curid=3030" title="1 de enero">
1 de enero

El 1 de enero es el primer día del año en el calendario gregoriano. Quedan 364 días para finalizar el año y 365 en los años bisiestos. En el calendario juliano empezó a ser el primer día del año en el 153 a. C.

Durante la Edad Media, bajo la influencia de la Iglesia Católica, muchos países de Europa occidental decidieron trasladar el comienzo del año a una de las varias fiestas cristianas importantes: el 25 de diciembre (la Natividad de Jesús), el 1 de marzo, el 25 de marzo (la Anunciación), o incluso la Pascua. El Imperio Bizantino comenzaba el año el 1 de septiembre.

En Inglaterra, el 1 de enero se celebraba como el festival de Año Nuevo, pero desde el siglo XII hasta 1752 el año en Inglaterra comenzaba el 25 de marzo (Día de la Dama). Así, por ejemplo, el registro parlamentario señala que la ejecución de Carlos I ocurrió el 30 de enero de 1648 (ya que el año no terminó hasta el 24 de marzo), aunque las historias modernas ajustan el comienzo del año al 1 de enero y registran la ejecución como si ocurriera en 1649.

La mayoría de los países de Europa occidental cambiaron el comienzo del año al 1 de enero antes de adoptar el calendario gregoriano. Por ejemplo, Escocia cambió el comienzo del Año Nuevo Escocés al 1 de enero en 1600. Inglaterra, Irlanda y las colonias británicas cambiaron el comienzo del año al 1 de enero en 1752. Más tarde ese año en septiembre, el calendario gregoriano se introdujo en toda Gran Bretaña y las colonias británicas. Estas dos reformas fueron implementadas por el Calendario (Nuevo Estilo) Ley de 1750. 

























</doc>
<doc id="3031" url="https://es.wikipedia.org/wiki?curid=3031" title="2001">
2001

2001 () fue un año común comenzado en lunes según el calendario gregoriano. Fue también el número 2001 anno Dómini o de la designación de Era Cristiana, segundo del decenio de los años 2000, primer año del siglo XXI, marca el inicio del y del tercer milenio. 

Fue declarado:





















































Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.







</doc>
<doc id="3034" url="https://es.wikipedia.org/wiki?curid=3034" title="8 de enero">
8 de enero

El 8 de enero es el octavo día del año en el calendario gregoriano. Quedan 357 días para finalizar el año y 358 en los años bisiestos.

















</doc>
<doc id="3035" url="https://es.wikipedia.org/wiki?curid=3035" title="6 de enero">
6 de enero

El 6 de enero es el sexto día del año en el calendario gregoriano. Quedan 359 días para finalizar el año y 360 en los años bisiestos.










</doc>
<doc id="3036" url="https://es.wikipedia.org/wiki?curid=3036" title="9 de enero">
9 de enero

El 9 de enero es el noveno día del año en el calendario gregoriano. Quedan 356 días para finalizar el año y 357 en los años bisiestos.























</doc>
<doc id="3037" url="https://es.wikipedia.org/wiki?curid=3037" title="10 de enero">
10 de enero

El 10 de enero es el 10.º (décimo) día del año en el calendario gregoriano. Quedan 355 días para finalizar el año y 356 en los años bisiestos.








</doc>
<doc id="3038" url="https://es.wikipedia.org/wiki?curid=3038" title="11 de enero">
11 de enero

El 11 de enero es el undécimo día del año en el calendario gregoriano. Quedan 354 días para finalizar el año y 355 en los años bisiestos.








</doc>
<doc id="3039" url="https://es.wikipedia.org/wiki?curid=3039" title="15 de enero">
15 de enero

El 15 de enero es el 15.º (decimoquinto) día del año en el calendario gregoriano. Quedan 350 días para finalizar el año y 351 en los años bisiestos.


















</doc>
<doc id="3040" url="https://es.wikipedia.org/wiki?curid=3040" title="14 de enero">
14 de enero

El 14 de enero es el 14.º (decimocuarto) día del año del calendario gregoriano.
Quedan 351 días para finalizar el año y 352 en los años bisiestos.







</doc>
<doc id="3041" url="https://es.wikipedia.org/wiki?curid=3041" title="16 de enero">
16 de enero

El 16 de enero es el 16.º (decimosexto) día del año en el calendario gregoriano. Quedan 349 días para finalizar el año y 350 en los años bisiestos.








</doc>
<doc id="3042" url="https://es.wikipedia.org/wiki?curid=3042" title="17 de enero">
17 de enero

El 17 de enero es el 17.º (decimoséptimo) día del año en el calendario gregoriano. Quedan 348 días para finalizar el año y 349 en los años bisiestos.









</doc>
<doc id="3044" url="https://es.wikipedia.org/wiki?curid=3044" title="18 de enero">
18 de enero

El 18 de enero es el decimoctavo día del año en el calendario gregoriano. Quedan 347 días para finalizar el año y 348 en los años bisiestos.








</doc>
<doc id="3045" url="https://es.wikipedia.org/wiki?curid=3045" title="21 de enero">
21 de enero

El 21 de enero es el 21.º (vigesimoprimer) día del año en el calendario gregoriano. Quedan 344 días para finalizar el año y 345 en los años bisiestos.









</doc>
<doc id="3046" url="https://es.wikipedia.org/wiki?curid=3046" title="22 de enero">
22 de enero

El 22 de enero es el 22.º (vigesimosegundo) día del año en el calendario gregoriano. Quedan 343 días para finalizar el año y 344 en los años bisiestos.










</doc>
<doc id="3049" url="https://es.wikipedia.org/wiki?curid=3049" title="Óptica">
Óptica

La óptica (del latín medieval "opticus, relativo a la visión", proveniente del griego clásico , "optikós") es la rama de la física que involucra el estudio del comportamiento y las propiedades de la luz, incluidas sus interacciones con la materia, así como la construcción de instrumentos que se sirven de ella o la detectan. La óptica generalmente describe el comportamiento de la luz visible, de la radiación ultravioleta y de la radiación infrarroja. Al ser una radiación electromagnética, otras formas de radiación del mismo tipo como los rayos X, las microondas y las ondas de radio muestran propiedades similares.

La mayoría de los fenómenos ópticos pueden explicarse utilizando la descripción electrodinámica clásica de la luz. Sin embargo, la óptica práctica generalmente utiliza modelos simplificados. El más común de estos modelos, la óptica geométrica, trata la luz como una colección de rayos que viajan en línea recta y se desvían cuando atraviesan o se reflejan en las superficies. La óptica física es un modelo de la luz más completo, que incluye efectos ondulatorios como la difracción y la interferencia, que no se pueden abordar mediante la óptica geométrica. 

Algunos fenómenos dependen del hecho de que la luz muestra indistintamente propiedades como onda y partícula. La explicación de estos efectos requiere acudir a la mecánica cuántica. Al considerar las propiedades de la luz similares a las de las partículas, se puede modelar como un conjunto de "fotones" individuales. La óptica cuántica se ocupa de la aplicación de la mecánica cuántica a los sistemas ópticos.

La óptica como ciencia es un campo muy relevante, y es estudiada en muchas disciplinas con las que está íntimamente relacionada, como la astronomía, varios campos de la ingeniería, la fotografía y la medicina (particularmente la oftalmología y la optometría). Las aplicaciones prácticas de la óptica se encuentran en una gran variedad de tecnologías, incluidos espejos, lentes, telescopios, microscopios, equipos láser y sistemas de fibra óptica.

Las primeras aplicaciones de la óptica muy probablemente comenzaron con el desarrollo de lentes en el antiguo Egipto y en Mesopotamia. Las primeras lentes conocidas, hechas de cristal pulido, a menudo cuarzo, datan ya del año 700 a. C., como la lente de Nimrud, descubierta en Asiria. También se conocen esferas de cristal rellenas de agua utilizadas como lentes en la antigua Roma y en la antigua Grecia. La invención de estos objetos fue seguida por la aparición de teorías sobre la luz y la visión planteadas por los antiguos filósofos griegos y de la India, y por el desarrollo de la óptica geométrica en el mundo grecorromano. La palabra "óptica" proviene de la palabra griega ("optikē"), que significa "aspecto, apariencia".

La filosofía griega sobre la óptica se dividió en dos ideas opuestas sobre cómo funcionaba la vista: la "teoría de la visión" y la "teoría de la emisión". Un enfoque consideraba que la visión provenía de los propios objetos, que emitían copias de sí mismos (llamadas "eidola") que eran captadas por el ojo. Con muchos propagadores, entre ellos Demócrito, Epicuro, Aristóteles y sus seguidores.

Platón fue el primero que articuló la teoría de la emisión, la idea de que la visión se logra mediante rayos emitidos por los ojos. También habló sobre la inversión en los espejos (de la paridad entre un objeto y su imagen reflejada) en el "Timaeus". Unos cien años después, Euclides escribió un tratado titulado "Óptica", donde vinculó la visión a la geometría, creando la "óptica geométrica". En su trabajo sobre la teoría de la emisión de Platón describió las reglas matemáticas de la perspectiva y describió los efectos de la refracción cualitativamente, aunque cuestionó que un rayo de luz emitido desde un ojo iluminara instantáneamente las estrellas cada vez que alguien parpadeaba. Claudio Ptolomeo, en su tratado sobre "Óptica", introdujo una teoría de la visión que combinaba las dos anteriores: los rayos (o el flujo emitido) del ojo formaban un cono, el vértice estaba dentro del ojo y la base definía el campo visual. Los rayos eran sensibles y transmitían información al intelecto del observador sobre la distancia y la orientación de las superficies. Resumió gran parte del trabajo de Euclides y describió una forma de medir los efectos de la ley de Snell, aunque no se dio cuenta de la relación empírica existente entre los ángulos.

Durante la Edad Media, las ideas griegas sobre la óptica fueron resucitadas y ampliadas por varios escritores en el mundo islámico. Uno de los primeros fue Al-Kindi (c 801-73), que escribió sobre los méritos de las ideas aristotélicas y euclidianas de la óptica, favoreciendo la teoría de la emisión, ya que podía cuantificar mejor los fenómenos ópticos. En 984, el matemático iraní Ibn Sahl escribió el tratado "Sobre espejos y lentes incendiarios", describiendo correctamente una ley de refracción equivalente a la ley de Snell. Utilizó esta ley para calcular formas óptimas para lentes y espejos curvos. A principios del siglo XI, Alhacén, considerado uno de los padres de la óptica, escribió el "Libro de Óptica" ("Kitab al-manazir") en el que exploró la reflexión y la refracción y propuso un nuevo sistema para explicar la visión y la luz basado en la observación y la experimentación. Rechazó la "teoría de emisión" de la óptica ptolemaica con sus rayos emitidos por el ojo, y planteó la idea de que la luz se refleja en todas las direcciones en líneas rectas desde todos los puntos de los objetos vistos y luego entra en el ojo, aunque no fue capaz de explicar correctamente cómo el ojo captaba los rayos. El trabajo de Alhacén fue ignorado en gran medida en el mundo árabe, pero fue traducido anónimamente al latín alrededor del año 1200 y más tarde resumido y expandido por el monje polaco Witelo, convirtiéndose en un texto estándar sobre óptica en Europa durante los 400 años siguientes.

En la Europa medieval del siglo XIII, el obispo inglés Roberto Grosseteste escribió sobre una amplia gama de temas científicos y discutió la luz desde cuatro perspectivas diferentes: una epistemología de la luz, una metafísica o cosmogonía de la luz, una etiología o física de la luz y un teología de la luz, basándose en las obras de Aristóteles y el platonismo. El discípulo más famoso de Grosseteste, Roger Bacon, escribió obras que citan una amplia gama de trabajos ópticos y filosóficos por entonces traducidos, incluidos los de Alhacén, Aristóteles, Avicena, Averroes, Euclides, al-Kindi, Ptolomeo, Tideus y Constantino el Africano. Bacon pudo usar partes de esferas de vidrio como lupas para demostrar que la luz se refleja en los objetos en lugar de liberarse de ellos.

Los primeros anteojos prácticos fueron inventados en Italia alrededor de 1286. Este fue el comienzo de la industria óptica del pulido de lentes para estos oculares, primero en Venecia y Florencia en el siglo XIII, y más tarde en los centros de fabricación de gafas en los Países Bajos y Alemania. Los fabricantes de gafas crearon tipos mejorados de lentes para la corrección de la visión, basados más en el conocimiento empírico obtenido al observar los efectos de las lentes que en utilizar la rudimentaria teoría óptica de la época (teoría que ni siquiera podía explicar adecuadamente cómo funcionaban las gafas). La práctica del desarrollo, el dominio y la experimentación con lentes condujo directamente a la invención del microscopio óptico compuesto alrededor de 1595 y del telescopio refractor en 1608. Ambos aparecieron en los centros de fabricación de gafas en los Países Bajos.

Hacia el año 1600, Galileo Galilei dirigió su primitivo telescopio refractor hacia el firmamento, dando origen a la astronomía moderna, que podía servirse de instrumentos de aumento para ver los detalles de los cuerpos celestes. Siguiendo su estela, a principios del siglo XVII Johannes Kepler amplió la óptica geométrica en sus escritos, cubriendo las lentes, los reflejos de espejos planos y curvos, los principios de la cámara estenopeica, las leyes de los cuadrados inversos que rigen la intensidad de la luz y las explicaciones ópticas de fenómenos astronómicos como los eclipses lunares y solares y el paralaje astronómico. También fue capaz de deducir correctamente el papel de la retina como el órgano real que percibe las imágenes, y finalmente fue capaz de cuantificar científicamente los efectos de los diferentes tipos de lentes que los fabricantes de gafas habían estado observando durante los últimos 300 años. Después de que se inventara el telescopio, Kepler estableció las bases teóricas sobre cómo funcionaba y describió una versión mejorada, conocida como "telescopio kepleriano", utilizando dos lentes convexas para producir una mayor ampliación.

La teoría óptica progresó a mediados del siglo XVII con los tratados escritos por el filósofo René Descartes, en los que explicaba una gran variedad de fenómenos ópticos, incluyendo la reflexión y la refracción al asumir que la luz era emitida por los objetos que la producían. Esta interpretación difería sustancialmente de la antigua teoría de emisión griega. A finales de la década de 1660 y principios de la de 1670, Isaac Newton expandió las ideas de Descartes en una teoría corpuscular de la luz, y determinó que la luz blanca era una mezcla de colores que se puede separar en sus partes componentes con un prisma. En 1690, Christiaan Huygens propuso una explicación ondulatoria para la luz, basándose en las sugerencias que había hecho Robert Hooke en 1664. El propio Hooke criticó públicamente las teorías de la luz de Newton y la disputa entre los dos duró hasta la muerte de Hooke. En 1704, Newton publicó "Opticks" y, en ese momento, en parte debido a su éxito en otras áreas de la física, generalmente se le consideraba el vencedor en el debate sobre la naturaleza de la luz.

Entretanto, los instrumentos ópticos empezaron a experimentar considerables mejoras técnicas, que permitieron a la ciencia adentrarse en campos hasta entonces inaccesibles, desde lo extremadamente pequeño (representado por el descubrimiento de los microbios) hasta lo inconcebiblemente grande (con un conocimiento cada vez mayor del sistema solar). El microscopio, considerablemente evolucionado desde el primitivo modelo de Anton van Leeuwenhoek (1650), permitió iniciar el estudio de las células gracias a los trabajos pioneros de Robert Hooke, recogidos en su tratado "Micrographia". Por otro lado, los telescopios refractores habían alcanzado su límite teórico de resolución, limitado por la aberración cromática, lo que en parte contribuyó al nacimiento de un nuevo tipo de instrumento: el telescopio reflector. Fue Isaac Newton quien construyó el primero de estos instrumentos en 1668. Este fue el inicio de una enconada carrera, que duró dos siglos y medio, entre los dos tipos de telescopios: refractores (lentes) y reflectores (espejos). La invención de las lentes acromáticas hacia 1750, permitió solucionar el problema de la aberración cromática, lo que dio inicialmente la primacía a los telescopios refractores sobre los primitivos telescopios reflectores, lastrados por la escasa luminancia y la poca durabilidad de los espejos de speculum, una aleación de bronce que se oxidaba con relativa facilidad. En esta época se sentaron las bases del desarrollo de los grandes refractores, que con Joseph von Fraunhofer adquirieron su madurez funcional a finales del siglo XVIII, convirtiéndose en la técnica dominante en el siglo XIX. También fue Fraunhofer quien sentaría las bases de una nueva ciencia que forma parte de la óptica: la espectroscopia. Los avances en la fabricación de lentes permitieron a su vez el desarrollo de los instrumentos utilizados en geodesia, permitiendo completar con una precisión hasta entonces impensable la medición del arco de meridiano de París en 1798, lo que permitiría establecer la unidad de longitud del sistema internacional: el metro.

La óptica newtoniana fue generalmente aceptada hasta principios del siglo XIX, cuando Thomas Young y Augustin Fresnel llevaron a cabo experimentos sobre la interferencia de la luz, que establecieron firmemente su naturaleza ondulatoria. El famoso experimento de la doble rendija de Young, con el que se hacía patente el fenómeno de la interferencia, demostró que la luz seguía el principio de la superposición de estratos, que es una propiedad ondulatoria no prevista por la teoría corpuscular de Newton. Este trabajo condujo a una teoría de la difracción de la luz y abrió un área completa de estudio en la óptica física. La óptica ondulatoria se unificó con éxito con el electromagnetismo gracias a James Clerk Maxwell en los años 1860.

La segunda mitad del siglo XIX contempló una serie de descubrimientos que sentarían las bases del desarrollo de instrumentos ópticos a lo largo del siglo XX. En el campo de los telescopios, la posibilidad de depositar una película de aluminio sobre una base de vidrio, decantó de forma ya definitiva la carrera entre los dos tipos de telescopios, decidiéndose a favor de los de espejos, que han seguido aumentando de tamaño sin cesar desde entonces. Así mismo, se descubrió la base de la fotografía con los trabajos de Niépce, que a su vez propiciaría la aparición del cine unas décadas después. Otro invento de finales del siglo XIX, el tubo de rayos catódicos, permitiría desarrollar unos años después las pantallas de televisión. En este período también vio la luz otro tipo de instrumento científico, el interferómetro, que sirvió para dar un inesperado soporte a la teoría de la relatividad y que con el paso del tiempo ha pasado a formar parte de equipos de medición de altísima precisión, como el LIGO, que ha permitido confirmar la existencia de ondas gravitatorias a comienzos del siglo XXI. 

La aparente confirmación de la naturaleza ondulatoria de la luz debido a su carácter de radiación electromagnética, llevó a un callejón sin salida, generando un intenso debate a lo largo de medio siglo acerca de la existencia del éter, un medio hipotético que se consideraba imprescindible para posibilitar la propagación de las ondas de luz. Se realizaron sin éxito numerosos experimentos para demostrar su existencia (como el famoso experimento de Michelson y Morley de 1887), y no sería hasta 1905 cuando Albert Einstein, con su Teoría de la relatividad especial, estableció el papel clave de la velocidad de la luz como una de las constantes fundamentales de la naturaleza, resolviendo de una vez por todas la cuestión del éter, descartando definitivamente su existencia.

El siguiente desarrollo en la teoría óptica llegó en 1899, cuando Max Planck modeló correctamente la radiación del cuerpo negro, al asumir que el intercambio de energía entre la luz y la materia solo ocurría en cantidades discretas que denominó "cuantos". En 1905 Albert Einstein publicó la teoría del efecto fotoeléctrico que estableció firmemente la cuantificación de la luz en sí misma. En 1913 Niels Bohr demostró que los átomos solo podían emitir cantidades discretas de energía, lo que explica las líneas discretas observadas en los espectros de emisión y de absorción. La comprensión de la interacción entre la luz y la materia que siguió a estos desarrollos no solo formó la base de la óptica cuántica, sino que también fue crucial para el desarrollo de la mecánica cuántica en su conjunto. La última culminación, la teoría electrodinámica cuántica, explica todos los procesos ópticos y electromagnéticos en general como resultado del intercambio de partículas reales y de fotones virtuales.

La óptica cuántica adquirió importancia práctica con las invenciones del máser en 1953 y del láser en 1960. Siguiendo el trabajo de Paul Dirac en la teoría cuántica de campos, George Sudarshan, Roy Jay Glauber y Leonard Mandel aplicaron la teoría cuántica al campo electromagnético en los años 1950 y 1960 para obtener una comprensión más detallada de la fotodetección y del comportamiento estadístico de la luz.

Otro hito importante en el campo de la aplicación práctica de dispositivos ópticos son los LED, cuyo principio de funcionamiento (la electroluminiscencia) fue descubierto en 1903. Se empezaron a producir industrialmente en la década de 1950, hasta hacerse omnipresentes en las pantallas de todo tipo de aparatos de consumo de masas, como teléfonos móviles o televisores.

La óptica clásica se divide en dos ramas principales: la óptica geométrica (o de rayos) y la óptica física (u ondulatoria). En la óptica geométrica, se considera que la luz viaja en línea recta, mientras que en la óptica física, la luz se considera como una onda electromagnética.

La óptica geométrica se puede ver como una aproximación a la óptica física que se aplica cuando la longitud de onda de la luz utilizada es mucho menor que el tamaño de los elementos ópticos en el sistema que se está analizando. la "óptica geométrica", u "óptica de rayos", describe la propagación de la luz en términos de "rayos" que viajan en línea recta, y cuyos caminos se rigen por las leyes de la reflexión y la refracción en los cambios de fase entre diferentes medios. Estas leyes descubiertas empíricamente se han utilizado de forma generalizada en el diseño de componentes e instrumentos ópticos. 

Las leyes de reflexión y refracción pueden derivarse del principio de Fermat, que establece que "el camino recorrido entre dos puntos por un rayo de luz es el camino que se puede atravesar en el menor tiempo posible".

La óptica geométrica a menudo se simplifica haciendo una aproximación paraxial o "aproximación de ángulos pequeños". El comportamiento matemático se vuelve lineal, permitiendo que los componentes ópticos y los sistemas se describan mediante matrices simples. Esto lleva a las técnicas de la óptica gaussiana y del "trazado de rayos paraxial", que se utilizan para determinar las propiedades básicas de los sistemas ópticos, como las imágenes y posiciones aproximadas de objetos y el correspondiente aumento óptico.

La reflexión se puede dividir en dos tipos: imagen especular y reflexión difusa. La reflexión especular describe el brillo de superficies como los espejos, que reflejan la luz de una manera simple y predecible. Esto permite la producción de imágenes reflejadas que pueden asociarse con una ubicación real (real) o extrapolada (virtual) en el espacio. La reflexión difusa describe materiales no brillantes, como papel o las rocas. Los reflejos de estas superficies solo se pueden describir estadísticamente, con la distribución exacta de la luz reflejada dependiendo de la estructura microscópica del material. Muchos reflectores difusos se describen o se pueden aproximar mediante la ley de Lambert, que describe superficies que tienen igual luminancia cuando se ven desde cualquier ángulo. Las superficies brillantes pueden dar una reflexión tanto especular como difusa.

En la reflexión especular, la dirección del rayo reflejado está determinada por el ángulo que forma el rayo incidente con el vector normal, una línea perpendicular a la superficie en el punto donde incide el rayo. Los rayos incidentes y reflejados y la normal se encuentran en un solo plano, y el ángulo entre el rayo reflejado y la superficie normal es el mismo que entre el rayo incidente y la normal. Este fenómeno físico se conoce como imagen especular.

Para espejos planos, la ley de la reflexión implica que las imágenes de los objetos están en posición vertical y a la misma distancia detrás del espejo que los objetos frente al espejo. El tamaño de la imagen es el mismo que el tamaño del objeto. La ley también implica que las imágenes especulares presentan una paridad invertida, que se percibe como una inversión izquierda-derecha. Las imágenes formadas a partir de la reflexión en dos (o cualquier cantidad par de) espejos no presentan paridad invertida. Un reflector de esquina es un retrorreflector que produce rayos reflejados que viajan en la misma dirección (y distinto sentido) desde la que vinieron los rayos incidentes.

Los espejos curvos pueden ser modelizados utilizando el trazado de rayos y usando la ley de reflexión en cada punto de la superficie. En los espejos parabólicos, los rayos paralelos al eje incidentes en el espejo producen rayos reflejados que convergen en un foco común. Otras superficies curvas también pueden enfocar la luz, pero con aberraciones debidas a la forma divergente que hace que el foco se disperse en el espacio. En particular, los espejos esféricos exhiben aberración esférica. Los espejos curvados pueden formar imágenes con una ampliación mayor o menor que uno, y la ampliación puede ser negativa, lo que indica que la imagen está invertida. Una imagen vertical formada por reflejo en un espejo siempre es virtual, mientras que una imagen invertida es real y puede proyectarse en una pantalla.

La refracción se produce cuando la luz viaja a través de un área del espacio que tiene un índice de refracción cambiante; este principio permite construir lentes capaces de enfocar la luz. El caso más simple de refracción ocurre cuando se tiene una interfaz entre un medio uniforme con índice de refracción formula_1 y otro medio con índice de refracción formula_2. En tales situaciones, la ley de Snell describe la deflexión resultante del rayo de luz:

donde formula_4 y formula_5 son los ángulos entre la normal (a la interfaz) y los rayos incidentes y refractados, respectivamente.

El índice de refracción de un medio está relacionado con la velocidad , de la luz en ese medio por
donde es la velocidad de la luz.

La ley de Snell se puede utilizar para predecir la deflexión de los rayos de luz a medida que pasan a través de medios lineales, siempre que se conozcan los índices de refracción y la geometría de los medios. Por ejemplo, la propagación de la luz a través de un prisma da como resultado que el rayo de luz se desvíe dependiendo de la forma y orientación del prisma. En la mayoría de los materiales, el índice de refracción varía con la frecuencia de la luz. Teniendo esto en cuenta, la ley de Snell se puede utilizar para predecir cómo un prisma dispersará la luz en un espectro.

Algunos medios tienen un índice de refracción que varía gradualmente con la posición y, por lo tanto, los rayos de luz en el medio son curvos. Este efecto es responsable de los espejismos vistos en días calurosos: un cambio en el índice de refracción del aire en altura hace que los rayos de luz se curven, creando la apariencia de reflejos especulares en la distancia (como si estuvieran en la superficie de una extensión de agua). Los materiales ópticos con índice de refracción variable se denominan materiales de gradiente de índice de refracción ("GRIN" según su acrónimo en inglés). Dichos materiales se utilizan para hacer instrumentos de acuerdo con los principios de la óptica de gradiente de índice.

Para los rayos de luz que viajan desde un material con un alto índice de refracción a un material con un índice de refracción bajo, la ley de Snell predice que formula_5 desaparece cuando formula_4 es grande. En este caso, no ocurre transmisión; toda la luz se refleja. Este fenómeno se llama reflexión interna total y permite la tecnología de la fibra óptica. A medida que la luz viaja por una fibra óptica, se somete a una reflexión interna total que permite que prácticamente no se pierda luz en el cable.

Un dispositivo que produce rayos de luz convergentes o divergentes debido a la refracción se conoce como "lente". Las lentes se caracterizan por su distancia focal: una lente convergente tiene una distancia focal positiva, mientras que una lente divergente tiene una distancia focal negativa. Una distancia focal más pequeña indica que la lente tiene un efecto convergente o divergente más fuerte. La distancia focal de una lente simple en el aire viene dada por la configuración de la propia lente.

El trazado de rayos se puede usar para mostrar cómo se forman las imágenes con una lente. Para una lente delgada en el aire, la ubicación de la imagen viene dada por la simple ecuación:

donde formula_10 es la distancia desde el objeto a la lente, formula_11 es la distancia desde la lente a la imagen, y formula_12 es la distancia focal de la lente. Con la convención de signos utilizada, las distancias entre el objeto y la imagen son positivas si el objeto y la imagen están en lados opuestos de la lente.

Los rayos paralelos entrantes se enfocan mediante una lente convergente en un punto a una distancia focal de la lente, en el lado más alejado de la lente. Esto se llama punto focal trasero de la lente. Los rayos de un objeto a distancia finita se enfocan más lejos de la lente que la distancia focal; cuanto más cerca esté el objeto de la lente, más lejos estará la imagen de la lente.

Con lentes divergentes, los rayos paralelos entrantes divergen después de atravesar la lente, de tal manera que parecen haberse originado en un punto a una distancia focal enfrente de la lente. Este es el punto focal frontal de la lente. Los rayos de un objeto a una distancia finita se asocian con una imagen virtual que está más cerca de la lente que el punto focal, y en el mismo lado de la lente que el objeto. Cuanto más cerca esté el objeto de la lente, más cerca estará la imagen virtual de la lente. Al igual que con los espejos, las imágenes verticales producidas por una sola lente son virtuales, mientras que las imágenes invertidas son reales.

Las lentes sufren de aberraciones que distorsionan las imágenes. Las "aberraciones monocromáticas" ocurren porque la geometría de la lente no dirige los rayos desde cada punto del objeto a un solo punto en la imagen, mientras que la aberración cromática ocurre porque el índice de refracción de la lente varía con la longitud de onda de la luz.

En óptica física, se considera que la luz se propaga como una onda. Este modelo predice fenómenos como la interferencia y la difracción, que no se explican por la óptica geométrica. Las ondas se propagan en la atmósfera terrestre casi a la misma velocidad de la luz en el vacío, aproximadamente a 3,0×10 m/s (exactamente 299,792,458 m/s en el vacío). La longitud de onda de las ondas de luz visible varía entre 400 y 700 nm, pero el término "luz" también se aplica con frecuencia a la radiación infrarroja (0.7-300 μm) y a la radiación ultravioleta (10-400 nm).
El modelo de onda se puede usar para hacer predicciones sobre cómo se comportará un sistema óptico sin requerir una explicación de sobre qué medio se están "agitando" las ondas. Hasta mediados del siglo XIX, la mayoría de los físicos creían en un medio "etéreo" en el que se propagaba la perturbación lumínica. La existencia de ondas electromagnéticas fue predicha en 1865 por las ecuaciones de Maxwell. Estas ondas se propagan a la velocidad de la luz y manifiestan campos eléctricos y magnéticos variables que son ortogonales entre sí, y también a la dirección de propagación de las ondas. Actualmente, las ondas de luz se tratan como ondas electromagnéticas, excepto cuando se deben considerar efectos de mecánica cuántica.

Muchas aproximaciones simplificadas están disponibles para analizar y diseñar sistemas ópticos. La mayoría usan una sola cantidad escalar para representar el campo eléctrico de la onda de luz, en lugar de usar un modelo vectorial con vectores eléctricos y magnéticos ortogonales.

El principio de Fresnel - Huygens es uno de esos modelos, deducido empíricamente por Fresnel en 1815, basándose en la hipótesis de Huygens de que cada punto en un frente de onda genera un frente de onda esférico secundario, que Fresnel combinaba con el principio de superposición de ondas. La fórmula de la difracción de Kirchhoff, que se deduce a partir de las ecuaciones de Maxwell, coloca la ecuación de Huygens-Fresnel sobre una base física más firme. Los ejemplos de la aplicación del principio de Huygens-Fresnel se pueden encontrar en las secciones de "difracción" y "difracción de Fraunhofer".

Se requieren modelos más rigurosos, que impliquen el modelado de campos eléctricos y magnéticos de la onda de luz, cuando se trata de la interacción detallada de la luz con materiales en los que la interacción depende de sus propiedades eléctricas y magnéticas. Por ejemplo, el comportamiento de una onda de luz interactuando con una superficie de metal es bastante diferente de lo que sucede cuando interactúa con un material dieléctrico. También se debe usar un modelo vectorial para modelizar la luz polarizada.

Las técnicas de simulación numérica, como el método de los elementos finitos, el método de elementos de frontera y el método de transmisión lineal matricial, se pueden usar para modelar la propagación de la luz en sistemas que no se pueden resolver analíticamente. Dichos modelos son computacionalmente exigentes y normalmente solo se utilizan para resolver problemas de pequeña escala que requieren una precisión superior a la que se puede lograr con soluciones analíticas.

Todos los resultados de la óptica geométrica se pueden reproducir utilizando las técnicas de la óptica de Fourier, que aplican muchas de las mismas técnicas matemáticas y analíticas utilizadas en ingeniería acústica y procesamiento de señales.

El haz de propagación gaussiano es un modelo de óptica física paraxial simple para abordar la propagación de radiación coherente, como los rayos láser. Esta técnica explica parcialmente la difracción, permitiendo cálculos precisos de la velocidad a la que un rayo láser se expande con la distancia y el tamaño mínimo al que se puede enfocar el rayo. La propagación del haz gaussiano cierra la brecha entre la óptica geométrica y la física.

En ausencia de efectos no lineales, el principio de superposición puede usarse para predecir la configuración de las formas de onda que interactúan mediante la simple suma de las perturbaciones. Esta interacción de ondas para producir un patrón resultante generalmente se denomina "interferencia" y puede dar como resultado una gran variedad de resultados. Si dos ondas de la misma longitud de onda y frecuencia están "en fase", las crestas y los valles de las ondas se alinean. Esto da como resultado una interferencia, con un aumento en la amplitud de la onda, que para la luz se asocia con un brillo de la forma de onda en esa ubicación. Alternativamente, si las dos ondas de la misma longitud de onda y frecuencia están desfasadas, las crestas de onda se alinearán con los valles de cada onda y viceversa. Esto da como resultado una interferencia con una disminución en la amplitud de la onda, que para la luz se asocia con un oscurecimiento de la forma de onda en esa ubicación. Véase a continuación una ilustración de este efecto.

Como el Principio de Fresnel - Huygens establece que cada punto de un frente de onda está asociado con la producción de una nueva perturbación, es posible que un frente de onda se interfiera de manera constructiva o destructiva en diferentes ubicaciones, produciendo franjas brillantes y oscuras en patrones regulares y predecibles. La interferometría es la ciencia que mide estos patrones, generalmente como un medio para hacer determinaciones precisas de distancias o resoluciones ópticas. El interferómetro de Michelson es un instrumento famoso que usaba efectos de interferencia para medir con precisión la velocidad de la luz.

La apariencia de películas finas y revestimientos se ve directamente afectada por los efectos de interferencia. La supresión de reflejos utiliza la interferencia destructiva para reducir la reflectividad de las superficies que recubren, y se pueden usar para minimizar el deslumbramiento y los reflejos no deseados. El caso más simple es una sola capa con un grosor de un cuarto de la longitud de onda de la luz incidente. La onda reflejada desde la parte superior de la película y la onda reflejada desde la interfaz película/material están exactamente desfasadas 180°, lo que causa interferencia destructiva. Las ondas solo están exactamente desfasadas para una longitud de onda determinada, que normalmente se elige para estar cerca del centro del espectro visible, alrededor de 550 nm. Los diseños más complejos que utilizan capas múltiples pueden lograr una baja reflectividad en una banda ancha o una reflectividad extremadamente baja en una sola longitud de onda.

La interferencia constructiva en películas delgadas puede crear un fuerte reflejo de la luz en un rango de longitudes de onda, que puede ser estrecho o amplio dependiendo del diseño del recubrimiento. Estas películas se usan para hacer espejos dieléctricos, filtros de interferencia, reflectores de calor y filtros para separación de colores en cámaras de televisión en color. Este efecto de interferencia también es lo que causa los coloridos patrones del arco iris que se ven en las manchas de petróleo y en las pompas de jabón.

La difracción es el proceso por el que la interferencia de la luz se observa más comúnmente. El efecto fue descrito por primera vez en 1665 por Francesco Maria Grimaldi, quien también acuñó el término del latín "diffringere", para "romperse en trozos". Posteriormente en ese mismo siglo, Robert Hooke e Isaac Newton también describieron fenómenos que ahora se conocen como difracción en anillos de Newton, mientras que James Gregory registró sus observaciones sobre los patrones de difracción de las plumas de ave.

El primer modelo de la difracción utilizando la óptica física se basó en el principio de Fresnel - Huygens, y fue desarrollado en 1803 por Thomas Young mediante su experimento de la doble rendija, analizando los patrones de interferencia de dos ranuras estrechamente espaciadas. Demostró que sus resultados solo podían explicarse si las dos ranuras actuaban como dos únicas fuentes de ondas en lugar de corpúsculos. En 1815 y 1818, Augustin Fresnel estableció firmemente las matemáticas de cómo la interferencia de ondas puede explicar la difracción.

Los modelos físicos más simples de difracción usan ecuaciones que describen la separación angular de franjas claras y oscuras debido a la luz de una longitud de onda particular (λ). En general, la ecuación toma la forma

donde formula_14 es la separación entre dos fuentes de frente de onda (en el caso de los experimentos de Young, fueron dos ranuras), formula_15 es la separación angular entre la franja central y la franja de orden formula_16, donde el máximo central es formula_17.

Esta ecuación se modifica ligeramente para tener en cuenta una variedad de situaciones tales como la difracción a través de un espacio único, la difracción a través de rendijas múltiples o la difracción a través de una red de difracción que contiene un gran número de rendijas a igual espaciado. Los modelos de difracción más complicados requieren trabajar con las matemáticas de Fresnel o de Fraunhofer.

La cristalografía de rayos X hace uso del hecho de que los átomos en un cristal tienen un espaciado regular a distancias que están en el orden de un ángstrom. Para ver los patrones de difracción, se hacen pasar rayos X con longitudes de onda similares a ese espaciado a través del cristal. Dado que los cristales son objetos tridimensionales en lugar de rejillas bidimensionales, el patrón de difracción asociado varía en dos direcciones según la ley de Bragg, y los puntos brillantes asociados se producen en patrones únicos y formula_14 es el doble del espaciado entre átomos.

Los efectos de la difracción limitan la sensibilidad de un detector óptico a la separación entre dos fuentes de luz, determinando su resolución óptica. En general, la luz que pasa por una apertura experimentará difracción, y las mejores imágenes que se pueden crear a través de esta apertura (como se describe en un "sistema limitado por la difracción") aparecen como un punto central con anillos brillantes circundantes, separados por franjas oscuras; este patrón se conoce como disco de Airy. El tamaño de dicho disco viene dado por

donde "θ" es la resolución angular, "λ" es la longitud de onda de la luz, y "D" es el diámetro de la apertura del objetivo. Si la separación angular de los dos puntos es significativamente menor que el radio angular del disco de Airy, entonces los dos puntos no se pueden resolver en la imagen, pero si su separación angular es mucho mayor que esta, se forman imágenes distintas de los dos puntos y por lo tanto, se pueden resolver. Rayleigh definió la "resolución óptica" de forma arbitraria como los dos puntos cuya separación angular es igual al radio del disco de Airy (medido al primer anillo nulo, es decir, al primer lugar donde no se ve luz) que pueden considerarse resueltos. Se puede ver que cuanto mayor es el diámetro de la lente o su apertura, más fina es la resolución. La interferometría astronómica, con su capacidad para emular aperturas de línea de base extremadamente grandes, permite la mayor resolución angular posible.

Para imágenes astronómicas, la atmósfera impide que se logre una resolución óptima en el espectro visible debido a la dispersión atmosférica que provoca el titilado de las estrellas. Los astrónomos se refieren a este efecto como la calidad de visualización. Las técnicas conocidas como óptica adaptativa se han utilizado para eliminar la alteración atmosférica de las imágenes y lograr resultados que se acercan al límite de difracción.

Los procesos de refracción tienen lugar en el límite de la óptica física, donde la longitud de onda de la luz es similar a otras distancias, como en un fenómeno del tipo de la dispersión. El tipo más simple de dispersión es la dispersión de Thomson, que ocurre cuando las ondas electromagnéticas son desviadas por partículas individuales. En el límite de la dispersión de Thomson, en la que la naturaleza ondulatoria de la luz es evidente, la luz se dispersa independientemente de la frecuencia, en contraste con el efecto Compton, que depende de la frecuencia y es estrictamente un proceso de mecánica cuántica, que involucra la naturaleza de la luz como un haz de partículas. En un sentido estadístico, la dispersión elástica de la luz provocada por numerosas partículas mucho más pequeñas que la longitud de onda de la luz es un proceso conocido como dispersión de Rayleigh, mientras que el proceso similar para dispersar partículas similares o mayores que la longitud de onda se conoce como difusión de Mie siendo el efecto Tyndall un resultado comúnmente observado. Una pequeña proporción de la dispersión de la luz producida por átomos o moléculas puede sufrir efecto Raman, donde la frecuencia cambia debido a la excitación de los átomos y las moléculas. La dispersión de Brillouin se produce cuando la frecuencia de la luz varía debido a los cambios locales con el tiempo y los movimientos de un material denso.

La dispersión tiene lugar cuando diferentes frecuencias de luz tienen velocidades de fase diferentes, debido a las propiedades del material ("dispersión del material") o a la geometría de una guía de onda óptica ("dispersión de guía de onda"). La forma más familiar de dispersión es una disminución en el índice de refracción con el aumento de la longitud de onda, que se observa en la mayoría de los materiales transparentes. Esto se llama "dispersión normal". Ocurre en todos los materiales dieléctricos, en rangos de longitud de onda donde el material no absorbe la luz. En los rangos de longitud de onda donde un medio tiene una absorción significativa, el índice de refracción puede aumentar con la longitud de onda. Este fenómeno se denomina "dispersión anómala".

La separación de colores por un prisma es un ejemplo de dispersión normal. En las superficies del prisma, la ley de Snell predice que la luz incidente en un ángulo θ a la normal se refracta en un ángulo igual al [arco seno (sin (θ)/"n")]. Por lo tanto, la luz azul, con su índice de refracción más alto, se desvía con más fuerza que la luz roja, lo que da como resultado el conocido patrón del arcoíris.

La dispersión de un material a menudo se caracteriza por el número de Abbe, que proporciona una medida de la dispersión simple basada en el índice de refracción en tres longitudes de onda específicas. La dispersión de conducción de onda depende de la constante de propagación. Ambos tipos de dispersión provocan cambios en las características del grupo de la onda y en las características del paquete de onda, que cambian con la misma frecuencia que la amplitud de la propia onda electromagnética. La "dispersión de la velocidad de grupo" se manifiesta como una dispersión de la "envolvente" de la señal de la radiación y se puede cuantificar con un parámetro de retardo de la dispersión de grupo:

donde formula_21 es la velocidad de grupo. Para un medio uniforme, la velocidad del grupo es

donde "n" es el índice de refracción y "c" es la velocidad de la luz en el vacío. Esto proporciona una forma más simple para el parámetro de retardo de la dispersión:

Si "D" es menor que cero, se dice que el medio tiene "dispersión positiva" o dispersión normal. Si "D" es mayor que cero, el medio tiene "dispersión negativa". Si un pulso de luz se propaga a través de un medio normalmente dispersivo, el resultado es que los componentes de frecuencia más alta se ralentizan más que los componentes de frecuencia más baja. Por lo tanto, el pulso se convierte en "positivamente pulsante", aumentando su frecuencia con el tiempo. Esto hace que el espectro que sale de un prisma aparezca con la luz roja menos refractada y la luz azul/violeta más refractada. Por el contrario, si un pulso viaja a través de un medio dispersivo anómalo (negativo), los componentes de alta frecuencia viajan más rápido que los de baja frecuencia y el pulso se vuelve "negativamente pulsante", disminuyendo en frecuencia con el tiempo.

El resultado de la dispersión de la velocidad del grupo, ya sea negativa o positiva, es la dispersión temporal del pulso. Esto hace que la gestión de la dispersión sea extremadamente importante en los sistemas de comunicaciones ópticas basados en la fibra óptica, ya que si la dispersión es demasiado alta, un grupo de pulsos que codifican una información binaria se dispersarán en el tiempo y se fusionarán,lo que hará imposible extraer la señal.

La polarización es una propiedad general de las ondas que describe la orientación de sus oscilaciones. Para ondas transversales, como muchas ondas electromagnéticas, describe la orientación de las oscilaciones en el plano perpendicular a la dirección de desplazamiento de la onda. Las oscilaciones pueden orientarse en una sola dirección (polarización lineal), o la dirección de oscilación puede rotar a medida que la onda se desplaza (circular o elíptica). Las ondas polarizadas circularmente pueden girar hacia la derecha o hacia la izquierda respecto a la dirección de desplazamiento, y cuál de esas dos rotaciones está presente en una onda se denomina quiralidad de la onda.

La forma típica de considerar la polarización es realizar un seguimiento de la orientación del vector del campo eléctrico a medida que la onda electromagnética se propaga. El vector de campo eléctrico de una onda plana se puede dividir arbitrariamente en dos componentes perpendiculares con las denominaciones "x" e "y" (con z indicando la dirección de propagación). La forma proyectada en el plano xy por el vector del campo eléctrico es una figura de Lissajous que describe el "estado de polarización". Las figuras anteriores muestran algunos ejemplos de la evolución del vector del campo eléctrico (azul), con el tiempo (el eje vertical), en un punto particular en el espacio, junto con sus componentes "x" e "y" (rojo/izquierda y verde/derecha), y la ruta trazada por el vector en el plano (violeta): la misma evolución ocurriría si se observa el campo eléctrico en la dirección opuesta a la propagación en un momento particular mientras el punto evoluciona en el espacio.

En la figura anterior, los componentes "x" e "y" de la onda de luz están en fase. En este caso, la relación de sus amplitudes es constante, por lo que la dirección del vector eléctrico (el vector suma de estos dos componentes) es constante. Como la punta del vector traza una sola línea en el plano, este caso especial se llama polarización lineal. La dirección de esta línea depende de las amplitudes relativas de los dos componentes.

En la figura central, las dos componentes ortogonales tienen las mismas amplitudes y están desfasadas 90°. En este caso, un componente es cero cuando el otro componente está en amplitud máxima o mínima. Hay dos posibles relaciones de fase que satisfacen este requisito: el componente "x" puede estar 90° por delante del componente "y" o puede estar 90° por detrás del componente "y". En este caso especial, el vector eléctrico traza un círculo en el plano, por lo que esta polarización se denomina polarización circular. La dirección de rotación en el círculo depende de cuál de las dos relaciones de fase existe y corresponden a la "polarización circular dextrógira" y a la "polarización circular levógira".

En todos los demás casos, cuando los dos componentes no tienen las mismas amplitudes y/o su diferencia de fase no es cero ni múltiplo de 90°, la polarización se llama polarización elíptica porque el vector eléctrico traza una elipse en el plano (la "elipse de polarización"). Esto se muestra en la figura de arriba a la derecha. Las matemáticas detalladas de la polarización utilizan el cálculo de Jones y se caracterizan por los parámetros de Stokes.

Los medios que tienen diferentes índices de refracción para diferentes modos de polarización se llaman "birrefringentes". Manifestaciones bien conocidas de este efecto aparecen en láminas de onda/retardadores ópticos (modos lineales) y en el efecto Faraday/actividad óptica (modos circulares). Si la longitud de la ruta en el medio birrefringente es suficiente, las ondas de polarización plana saldrán del material con una dirección de propagación significativamente diferente, debido a la refracción. Por ejemplo, este es el caso de los cristales macroscópicos de calcita, que presentan al espectador dos imágenes desplazadas, ortogonalmente polarizadas, de lo que se ve a través de ellos. Fue este efecto el que proporcionó el primer descubrimiento de un fenómeno de polarización por Rasmus Bartholin en 1669. Además, el cambio de fase, y por lo tanto, el cambio en el estado de polarización, generalmente depende de la frecuencia, lo que, en combinación con el dicroísmo, a menudo da lugar a colores brillantes y efectos tipo arcoíris. En mineralogía, dichas propiedades, conocidas como pleocroísmo, se explotan con frecuencia con el fin de identificar minerales utilizando microscopios con luz polarizada. Además, muchos plásticos que normalmente no son birrefringentes llegan a serlo cuando están sujetos a tensión mecánica, un fenómeno que es la base de los métodos de fotoelasticidad. Para hacer rotar la polarización lineal de haces de luz, además del polarizador rotativo, existen prismáticos que usan la reflexión interna total en un conjunto de prismas diseñado para obtener una transmisión colineal eficiente.

Los medios que reducen la amplitud de ciertos modos de polarización se llaman "dicróicos", con dispositivos que bloquean casi toda la radiación en unos dispositivos conocidos como "filtros polarizadores" o simplemente "polarizadores". La ley de Malus, que lleva el nombre de Étienne-Louis Malus, dice que cuando se coloca un polarizador perfecto en un haz de luz polarizado lineal, la intensidad, "I", de la luz que lo atraviesa viene dada por

donde

Se puede pensar que un haz de luz no polarizada contiene una mezcla uniforme de polarizaciones lineales en todos los ángulos posibles. Dado que el valor promedio de formula_25 es 1/2, el coeficiente de transmisión se convierte en

En la práctica, se pierde algo de luz en el polarizador y la transmisión real de luz no polarizada será algo menor, alrededor del 38% para los polarizadores de tipo Polaroid pero considerablemente mayor (>49.9%) para algunos tipos de prismas birrefringentes.

Además de la birrefringencia y el dicroísmo en medios extensos, los efectos de la polarización también pueden ocurrir en la interfaz (reflectante) entre dos materiales de diferente índice de refracción. Estos efectos son tratados por las ecuaciones de Fresnel. Parte de la onda se transmite y parte se refleja, y la relación depende del ángulo de incidencia y del ángulo de refracción. De esta manera, la óptica física se relaciona con la física ondulatoria a través del parámetro denominado ángulo de Brewster. Cuando la luz se refleja desde una película delgada en una superficie, la interferencia entre las reflexiones de las superficies de la película puede producir polarización en la luz reflejada y en la transmitida.

La mayoría de las fuentes de radiación electromagnética contienen una gran cantidad de átomos o moléculas que emiten luz. La orientación de los campos eléctricos producidos por estos emisores puede no estar correlacionada, en cuyo caso se dice que la luz está "no polarizada". Si hay una correlación parcial entre los emisores, la luz está "parcialmente polarizada". Si la polarización es constante en todo el espectro de la fuente, la luz parcialmente polarizada se puede describir como una superposición de un componente completamente no polarizado, y uno completamente polarizado. La luz puede describirse en términos de su grado de polarización y según los parámetros de la elipse de polarización.

Cuando es reflejada por materiales transparentes y brillantes, está parcial o totalmente polarizada, excepto si la luz es normal (perpendicular) a la superficie. Fue este efecto el que permitió al matemático Étienne-Louis Malus realizar las mediciones que permitieron desarrollar los primeros modelos matemáticos de la luz polarizada. La polarización se produce cuando la luz se dispersa en la atmósfera terrestre. La luz dispersa produce el brillo y el color del cielo despejado. Esta polarización parcial de la luz dispersada se puede aprovechar al usar filtros polarizadores para oscurecer el cielo en determinadas fotografías. La polarización óptica es principalmente importante en química, debido al dicroísmo circular y a la actividad óptica (""birrefringencia circular"") exhibida por moléculas quirales ópticamente activas.
La "óptica moderna" abarca áreas de la ciencia óptica y de la ingeniería que se hicieron populares en el siglo XX. Estas áreas de la ciencia óptica se relacionan típicamente con las propiedades electromagnéticas o cuánticas de la luz, pero incluyen otros temas. Un importante subcampo de la óptica moderna, la óptica cuántica, trata específicamente de las propiedades de la luz según la mecánica cuántica. La óptica cuántica no es solo teórica; algunos dispositivos modernos, como los láseres, tienen principios de funcionamiento que describe la mecánica cuántica. Los detectores de luz, como fotomultiplicadores y canaltrones, responden a fotones individuales. Los sensores de imagen electrónicos, como los CCDs, exhiben un ruido de disparo correspondiente a las estadísticas de eventos de fotones individuales. Los LEDs y las células fotoeléctricas tampoco se pueden entender sin la mecánica cuántica. En el estudio de estos dispositivos, la electrónica cuántica a menudo se superpone con la óptica cuántica.

Las áreas de especialidad de investigación óptica incluyen el estudio de cómo la luz interactúa con materiales específicos como en la óptica de cristales y en metamateriales. Otra línea de investigación se centra en los fenómenos asociados a las ondas electromagnéticas como en las singularidades ópticas, la óptica sin imagen, la óptica no lineal, la óptica estadística y la radiometría. Además, la ingeniería en computación se ha interesado en la óptica integrada, las máquinas de visión y las computadoras ópticas como posibles componentes de la "próxima generación" de ordenadores.

En la actualidad, la ciencia pura de la óptica se llama ciencia óptica o física óptica para distinguirla de las ciencias ópticas aplicadas, que se conocen como ingeniería óptica. Los subcampos destacados de la ingeniería óptica incluyen la ingeniería de la iluminación, la fotónica y la optoelectrónica, con aplicaciones prácticas como el diseño óptico de lentes, la fabricación y prueba de componentes ópticos y el procesamiento digital de imágenes. Algunos de estos campos se superponen, con límites nebulosos entre los términos que describen las respectivas disciplinas, que significan cosas ligeramente diferentes en diferentes partes del mundo y en diferentes áreas de la industria. En las últimas décadas se ha desarrollado una comunidad profesional de investigadores en óptica no lineal, gracias a los avances en tecnología láser.

Un láser es un dispositivo que emite luz (radiación electromagnética) a través de un proceso llamado "emisión estimulada". El término "láser" es un acrónimo de la expresión inglesa ""Light Amplification by Stimulated Emission of Radiation"" ("Amplificación de luz por emisión estimulada de radiación"). La luz láser es generalmente coherente, lo que significa que se emite en un estrecho haz de baja divergencia o que puede convertirse en uno de estos haces con la ayuda de componentes ópticos como las lentes. Debido a que el equivalente en microondas del láser, el "máser", se desarrolló primero, los dispositivos que emiten frecuencias de microondas y de radio generalmente se llaman "másers".

El primer láser en funcionamiento fue presentado el 16 de mayo de 1960 por Theodore Harold Maiman en los Hughes Research Laboratories. Cuando se inventaron por primera vez, se los llamó "una solución que busca un problema". Desde entonces, los láseres se han convertido en una industria multimillonaria, encontrando utilidad en miles de aplicaciones muy variadas. La primera aplicación de láser visible en la vida cotidiana de la población general fue el escáner de código de barras de los supermercados, introducido en 1974. El reproductor laserdisc, presentado en 1978, fue el primer producto de consumo exitoso en incluir un láser, pero el reproductor de disco compacto fue el primer dispositivo equipado con láser verdaderamente común en los hogares de los consumidores, comenzando en 1982. Estos dispositivos de almacenamiento óptico usan un diodo láser de menos de un milímetro de ancho con el que escanean la superficie del disco para la recuperación de datos. Las comunicaciones por fibra óptica dependen de los láseres para transmitir grandes cantidades de información a la velocidad de la luz. Otras aplicaciones comunes de los láseres incluyen las impresoras láser y los punteros láser. También se usan en medicina en áreas como la cirugía general, la cirugía refractiva y la microdisección láser; así como en aplicaciones militares como sistemas antimisil, contramedidas electro-ópticas y sistemas lIDAR. Los láseres también se usan en holografía, grabados 3D, pantallas láser y depilación láser.

En construcción, se utilizan como herramientas de corte de planchas metálicas; en geodesia y topografía los telémetros láser sirven para la medida precisa de distancias (como en el caso extremo de la medición de la distancia entre la Tierra y la Luna, utilizando los espejos situados en la superficie del satélite por distintas misiones espaciales); y en la navegación aeronáutica son la base de los giróscopos láser de anillo.

Así mismo, en algunos tipos de reactores de fusión nuclear se utilizan rayos láser de gran potencia para alcanzar las elevadas temperaturas que requieren este tipo de reacciones.

El efecto Kapitsa-Dirac hace que los haces de partículas se difracten como resultado de encontrarse con una onda estacionaria de luz. La luz se puede usar para manipular fragmentos atómicos o moleculares de materia, aprovechando las propiedades de este fenómeno (véase "pinza óptica").

La óptica es parte de la vida cotidiana. La ubicuidad de los sistemas visuales en biología indica el papel central que juega la óptica como ciencia de uno de los cinco sentidos. Muchas personas se benefician de gafas o lente de contacto, y la óptica es esencial para el funcionamiento de muchos bienes de consumo, incluidas cámaras fotográficas, de cine o de televisión. El arco iris y los espejismos son ejemplos de fenómenos ópticos. La fibra óptica proporciona la red troncal tanto para Internet como para la telefonía moderna.

El ojo humano funciona enfocando la luz sobre una capa de fotorreceptores llamada retina, que forma el revestimiento interior de la parte posterior del ojo. El enfoque se logra mediante una serie de medios transparentes. La luz que entra al ojo pasa primero a través del córnea, que proporciona gran parte de la potencia óptica del ojo. Luego continúa a través del fluido contenido justo detrás de la córnea, en la cámara anterior, y pasa a través de la pupila. A continuación atraviesa el cristalino, que enfoca más la luz y permite el ajuste del enfoque, y pasa a través del cuerpo principal de fluido interior del ojo, el humor vítreo, y alcanza la retina. Las células fotosensibles de la retina recubren la parte posterior del ojo, excepto donde sale el nervio óptico; esto da como resultado un punto ciego.

Hay dos tipos de células fotorreceptoras, bastones y conos, que son sensibles a diferentes aspectos de la luz. Las células Los conos son sensibles a la intensidad de la luz en un amplio rango de frecuencia, por lo tanto son responsables de la visión en blanco y negro. Los bastones no están presentes en la fóvea, el área de la retina responsable de la visión central, y no son tan sensibles como los conos a los cambios espaciales y temporales de la luz. Sin embargo, hay veinte veces más bastones que conos en la retina, porque los primeros están presentes en un área más amplia. Debido a su distribución más amplia, los bastones son responsables de la visión periférica.

Por el contrario, los conos son menos sensibles a la intensidad general de la luz, pero se presentan en tres variedades que son sensibles a diferentes rangos de frecuencia y, por lo tanto, se utilizan en la percepción del color y en la visión fotópica. Las células cónicas están altamente concentradas en la fóvea y tienen una agudeza visual alta, lo que significa que son mejores para la resolución espacial que las células bastón. Dado que los conos no son tan sensibles a la luz tenue como los bastones, la mayor parte de la visión nocturna se limita a los bastones. Del mismo modo, como los conos se encuentran en la fóvea, la visión central (incluida la visión necesaria para realizar la mayoría de las tareas de detalle fino, como la lecturas, la costura o el examen cuidadoso de los objetos) se realiza mediante los células conos.

Los músculos ciliares alrededor del cristalino permiten ajustar el enfoque del ojo. Este proceso se conoce como acomodación. La presbicia y el punto remoto definen las distancias más cercana y más lejana al ojo en las que un objeto puede enfocarse con nitidez. Para una persona con visión normal, el punto lejano se encuentra en el infinito. La ubicación del punto cercano depende de cuánto pueden aumentar los músculos la curvatura del cristalino y de su pérdida de flexibidad con la edad. Optometristas, oftalmólogoss y ópticos generalmente consideran que un punto cercano apropiado está más cerca que la distancia de lectura normal, aproximadamente 25 cm.

Los defectos en la visión pueden explicarse utilizando principios ópticos. A medida que las personas envejecen, el cristalino se vuelve menos flexible y el punto cercano se aleja del ojo, una situación conocida como presbicia. Del mismo modo, las personas que sufren de hipermetropía no pueden disminuir la distancia focal de su cristalino lo suficiente como para permitir que los objetos cercanos se vean en su retina. Por el contrario, la miopía se produce cuando el punto lejano está considerablemente más cercano que el infinito. Un problema conocido como astigmatismo se produce cuando la córnea no es esférica, sino que es más curva en una determinada dirección. Esto hace que los objetos extendidos horizontalmente se enfoquen en la retina de diferente forma que los objetos extendidos verticalmente, y da como resultado imágenes distorsionadas.

Todas estas deficiecias funcionales se pueden corregir con lentes correctivas. Para la presbicia y la hipermetropía, una lente acerca el punto cercano al ojo, mientras que para la miopía, envía el punto lejano al infinito. El astigmatismo se corrige con una lente de superficie cilíndrica que se curva más fuertemente en una dirección que en otra, lo que compensa la falta de uniformidad de la córnea.

La potencia óptica de las lentes correctoras se mide en dioptrías, un valor igual al inverso de la distancia focal medida en metros. Una distancia focal positiva corresponde a una lente convergente y una distancia focal negativa correspondiente a una divergente. Para las lentes que también corrigen el astigmatismo, se dan tres números: uno para la potencia esférica, otro para la potencia cilíndrica y el tercero para el ángulo de orientación del astigmatismo.

Las ilusiones ópticas (también llamadas ilusiones visuales) se caracterizan por ser imágenes visualmente percibidas que difieren de la realidad objetiva. La información recopilada por los ojos se procesa en el cerebro para dar una percepción que difiere del objeto que se está observando. Las ilusiones ópticas pueden ser el resultado de variados fenómenos, que incluyen los efectos físicos que crean imágenes que son diferentes de los objetos que los producen, los efectos fisiológicos en los ojos y en el cerebro de una estimulación excesiva (por ejemplo, brillo, inclinación, color o movimiento) y las ilusiones cognitivas en las que el ojo y el cerebro producen inferencias subconscientes.

Las ilusiones cognitivas incluyen algunas que resultan de la mala aplicación inconsciente de ciertos principios ópticos. Por ejemplo, efectos como la habitación de Ames, la ilusión de Hering, las de Müller-Lyer, Orbison, Ponzo, Sander y de Wundt, se basan en crear la sensación de distancia mediante el uso de líneas convergentes y divergentes, de la misma manera que los rayos de luz paralelos (o de hecho, cualquier conjunto de líneas paralelas) parecen converger en un punto de fuga situado en el horizonte cuando se representa una perspectiva en dos dimensiones. Esta sugestión es también la responsable de la famosa "ilusión lunar", en la que la luna, a pesar de tener esencialmente el mismo diámetro angular, parece mucho más grande cerca del horizonte que en el cenit. Esta ilusión confundió a Ptolomeo, que incorrectamente la atribuyó a la refracción atmosférica cuando la describió en su tratado de óptica.

Otro tipo de ilusión óptica explota patrones descompuestos para engañar a la mente, de forma que perciba simetrías o asimetrías que no están realmente presentes. Los ejemplos incluyen las ilusiones de la pared de la cafetería, de Ehrenstein, de la espiral de Fraser, de Poggendorff y la ilusión de Zöllner. Relacionados, pero no siendo estrictamente ilusiones, están los patrones producidos por la superposición de estructuras periódicas. Por ejemplo, los tejidos transparentes con una estructura de cuadrícula producen formas conocidas como patrón de Moiré, mientras que la superposición de patrones transparentes periódicos que comprenden líneas o curvas opacas paralelas produce patrones líneales de Moiré.

Las lentes simples tienen una gran variedad de aplicaciones que incluyen objetivos fotográficos, lentes correctivas y lupas, mientras que los espejos simples se usan en reflectores parabólicos y espejos retrovisores. La combinación de varios espejos, prismas y lentes produce instrumentos ópticos compuestos que tienen diversos usos prácticos. Por ejemplo, un periscopio está formado simplemente por dos espejos planos alineados para permitir ver evitando un obstáculo. Los instrumentos ópticos compuestos más famosos de la ciencia son el microscopio y el telescopio, que fueron ideados por los holandeses a finales del siglo XVI.

Los microscopios se desarrollaron primero con solo dos lentes: un objetivo y un ocular. La lente del objetivo es esencialmente una lupa, y se diseñó con una distancia focal muy pequeña, mientras que el ocular generalmente tiene una distancia focal más larga. Esto tiene el efecto de producir imágenes ampliadas de objetos cercanos. En general, se utiliza una fuente de iluminación adicional, ya que las imágenes ampliadas son más débiles debido al principio de conservación de la energía y a la dispersión de los rayos de luz sobre un área de superficie más grande. Los microscopios modernos, conocidos como "microscopios compuestos" tienen muchas lentes (generalmente cuatro) para optimizar su funcionalidad y mejorar la estabilidad de la imagen. Una variedad ligeramente diferente de microscopio, el microscopio estereoscópico, permite obtener dos imágenes de las muestras examinadas, que se perciben en tres dimensiones gracias al uso de un sistema binocular.

Los primeros telescopios, los denominados "telescopios refractores" también se desarrollaron con un solo objetivo y una lente ocular. En contraste con el microscopio, la lente del objetivo del telescopio se diseñó con una gran distancia focal para evitar aberraciones ópticas. El objetivo enfoca una imagen de un objeto distante en su punto focal, que se ajusta para localizarse a su vez en el punto focal de un ocular con una distancia focal mucho más pequeña. El objetivo principal de un telescopio no es necesariamente la ampliación, sino más bien la recolección de luz, que viene determinada por el tamaño físico de la lente del objetivo. Por lo tanto, los telescopios se denominan normalmente por los diámetros de sus objetivos más que por la ampliación que se puede obtener cambiando los oculares. Debido a que la ampliación de un telescopio es igual a la distancia focal del objetivo dividida por la distancia focal del ocular, los oculares de longitud focal más pequeña producen una mayor ampliación.

Como fabricar lentes grandes es mucho más difícil que crear grandes espejos, la mayoría de los telescopios modernos son "telescopios reflectores", es decir, telescopios que usan un espejo primario en lugar de un objetivo. Las mismas consideraciones ópticas generales que se aplican a los telescopios reflectores, se aplican a los telescopios de refracción, a saber, que cuanto mayor es el espejo primario, más luz se recoge, y la ampliación es igual a la distancia focal del espejo primario dividida por la distancia focal del ocular. Los telescopios profesionales generalmente no tienen oculares y en su lugar se coloca un sistema de captación de imágenes electrónico (a menudo un dispositivo de carga acoplada) en el punto focal.

La óptica de la fotografía involucra tanto el uso de lentes como el medio en el que se registra la radiación electromagnética, ya sea una placa, una película o un dispositivo de carga acoplada. Los fotógrafos deben considerar la relación de reciprocidad entre la cámara y la toma, que se resume mediante la igualdad

En otras palabras, cuanto menor sea la abertura (proporcionando una mayor profundidad de enfoque), menor será la cantidad de luz que entrará, por lo que deberá aumentarse el tiempo de exposición (lo que puede provocar una imagen borrosa si se produce movimiento). Un ejemplo del uso de la ley de reciprocidad es la regla 16/f, que proporciona una referencia aproximada de la configuración necesaria para estimar la exposición adecuada durante el día.

La apertura de una cámara se mide con un número sin unidades denominado "f", f/#, a menudo anotado como formula_27, y dado por
donde formula_12 es la distancia focal y formula_30 es el diámetro del orificio del diafragma de entrada. Por convención, "f/#" se trata como un símbolo único, y los valores específicos de f/# se escriben reemplazando la almohadilla por un valor numérico. Las dos formas de aumentar el límite de la focal son disminuir el diámetro del obturador o cambiar a una distancia focal más larga (en el caso de un zum, esto se puede hacer simplemente ajustando la lente). Los números f más altos también tienen una profundidad de campo más grande debido a que el objetivo se acerca al límite de una cámara estenopeica que puede enfocar todas las imágenes perfectamente, independientemente de la distancia, pero requiere tiempos de exposición muy largos.

El campo de visión que proporcionará la lente cambia con la distancia focal de la lente. Hay tres clasificaciones básicas basadas en la relación con el tamaño diagonal de la película o el tamaño del sensor de la cámara con respecto a la distancia focal de la lente:


Los zum modernos pueden tener algunos o todos estos atributos.

El valor absoluto para el tiempo de exposición requerido depende de la sensibilidad lumínica del medio utilizado (medida según una escala de sensibilidad fotográfica o, para medios digitales, por su eficiencia cuántica). Las primeras fotografías usaban medios que tenían muy poca sensibilidad a la luz, y por lo tanto los tiempos de exposición tenían que ser largos, incluso para tomas muy brillantes. A medida que la tecnología ha mejorado, también lo ha hecho la sensibilidad gracias al desarrollo de películas cada vez más versátiles y de cámaras digitales con mejores prestaciones.

Otros resultados de la óptica física y geométrica se aplican a la óptica de la cámara. Por ejemplo, la capacidad de resolución máxima de una configuración particular de la cámara está determinada por el límite de difracción asociado con el tamaño del obturador y, aproximadamente, por el criterio de Rayleigh.

Las propiedades ópticas únicas de la atmósfera causan una amplia gama de fenómenos ópticos espectaculares. El color azul del cielo es un resultado directo de la dispersión de Rayleigh que redirige la luz solar de mayor frecuencia (azul) al campo de visión del observador. Debido a que la luz azul se dispersa más fácilmente que la luz roja, el sol adquiere un tono rojizo cuando se observa a través de una atmósfera espesa, como durante un orto u ocaso. Las partículas suspendidas en el cielo puede dispersar diferentes colores en diferentes ángulos creando coloridos cielos brillantes al anochecer y al amanecer. La dispersión de cristales de hielo y otras partículas en la atmósfera es responsable de los halos, arreboles, coronas, rayos crepusculares y parhelios. La variación en este tipo de fenómenos se debe a los diferentes tamaños y geometrías de las partículas.

Los espejismos son fenómenos ópticos en los que los rayos de luz se curvan debido a variaciones térmicas que modifican el índice de refracción del aire, produciendo imágenes desplazadas o muy distorsionadas de objetos distantes. Otros espectaculares fenómenos ópticos asociados con este efecto incluyen el efecto Nueva Zembla donde el sol parece elevarse antes de lo previsto con una forma distorsionada. Otra forma llamativa de refracción que se produce en condiciones de inversión térmica es el fenómeno llamado Fata Morgana, en el que los objetos en el horizonte o incluso más allá del horizonte, como islas, acantilados, barcos o icebergs, aparecen alargados y elevados, como "castillos de cuento de hadas".

Los arcoíris son el resultado de una combinación de reflexión interna y refracción dispersiva de la luz en las gotas de lluvia. Una sola reflexión en la parte posterior de una serie de gotas de lluvia produce un arcoíris con un tamaño angular en el cielo que varía de 40° a 42°, con el color rojo en el exterior. Los dos tipos de arcoíris dobles son producidos por dos reflejos internos con un tamaño angular de 50,5° a 54°, con el color violeta en el exterior. Debido a que los arcos iris se ven con el sol a 180° del centro del arcoíris, son más prominentes cuanto más cerca está el sol del horizonte.




</doc>
<doc id="3050" url="https://es.wikipedia.org/wiki?curid=3050" title="25 de enero">
25 de enero

El 25 de enero es el 25.º (vigesimoquinto) día del año del calendario gregoriano. Quedan 340 días para finalizar el año y 341 en los años bisiestos.









</doc>
<doc id="3053" url="https://es.wikipedia.org/wiki?curid=3053" title="7 de febrero">
7 de febrero

El 7 de febrero es el trigésimo octavo día del año en el calendario gregoriano. Quedan 327 días para finalizar el año y 328 en los años bisiestos.






















</doc>
<doc id="3054" url="https://es.wikipedia.org/wiki?curid=3054" title="8 de febrero">
8 de febrero

El 8 de febrero es el 39.º (trigésimo noveno) día del año en el calendario gregoriano. Quedan 326 días para finalizar el año y 327 en los años bisiestos.









</doc>
<doc id="3055" url="https://es.wikipedia.org/wiki?curid=3055" title="Álgebra de Boole">
Álgebra de Boole

El álgebra de Boole, también llamada álgebra booleana, en electrónica digital, informática y matemática es una estructura algebraica que esquematiza las operaciones lógicas.

Se denomina así en honor a George Boole (2 de noviembre de 1815 a 8 de diciembre de 1864), matemático inglés autodidacta, que fue el primero en definirla como parte de un sistema lógico, inicialmente en un pequeño folleto, "The Mathematical Analysis of Logic", publicado en 1847, en respuesta a una controversia en curso entre Augustus De Morgan y "sir" William Rowan Hamilton. El álgebra de Boole fue un intento de utilizar las técnicas algebraicas para tratar expresiones de la lógica proposicional. Más tarde fue extendido como un libro más importante: "An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities" (también conocido como "An Investigation of the Laws of Thought" o simplemente "The Laws of Thought"), publicado en 1854.

En la actualidad, el álgebra de Boole se aplica de forma generalizada en el ámbito del diseño electrónico. Claude Shannon fue el primero en aplicarla en el diseño de circuitos de conmutación eléctrica biestables, en 1948. Esta lógica se puede aplicar a dos campos:


Dado un conjunto formula_1 en el que se han definido dos leyes de composición interna formula_2. La estructura formula_3es un álgebra de Boole si y solo si formula_3es un Retículo distributivo, esto es:



Basándose en esta definición se determina lo siguiente.

Dado un conjunto: formula_1 formado cuando menos por los elementos: formula_12 en el que se ha definido:




Dada la definición del álgebra de Boole como una estructura algebraica genérica, según el caso concreto de que se trate, la simbología y los nombres de las operaciones pueden variar.

Diremos que este conjunto y las operaciones así definidas: formula_19 son un álgebra de boole, si cumple las siguientes axiomas:











Partiendo de los cinco axiomas anteriores, se pueden deducir y demostrar los siguientes teoremas fundamentales:










Sea: formula_19 un álgebra de Boole, sean a, b dos elementos del conjunto, podremos decir entonces que a antecede a b y lo denotamos:

si se cumple alguna de las siguientes condiciones:

Estas cuatro condiciones se consideran equivalentes y el cumplimiento de una de ellas implica necesariamente el cumplimiento de las demás. Definiendo un conjunto parcialmente ordenado.

Si se cumple que:

Para los valores a, b de formula_1, que cumplen que a antecede a b, o que b antecede a a, se dice que a y b son "comparables".

Si se cumple que:

Para los valores a, b de formula_1, que cumplen que a no antecede a b, y que b no antecede a a, se dice que a y b son "no comparables".

El concepto de dualidad permite formalizar este hecho: a toda relación o ley lógica le corresponderá su dual, formada mediante el intercambio de los operadores suma con los de producto, y de los formula_51 con los formula_52.

En Lógica binaria se suele emplear la notación formula_53, común en la tecnología digital, siendo la forma más usual y la más cómoda de representar.

Por ejemplo las leyes de De Morgan se representan así:

Cuando el álgebra de Boole se emplea en electrónica, suele emplearse la misma denominación que para las puerta lógica AND (Y), OR (O) y NOT (NO), ampliándose en ocasiones con X-OR (O exclusiva) y su negadas NAND (NO Y), NOR (NO O) y X-NOR (equivalencia). las variables pueden representarse con letras mayúsculas o minúsculas, y pueden tomar los valores {0, 1}.

Empleando esta notación las leyes de De Morgan se representan:

Con la notación lógica las leyes de De Morgan serían así:

En el formato de Teoría de conjuntos el Álgebra de Boole toma el aspecto: formula_61

En esta notación las leyes de De Morgan serían así:

Otra forma en la álgebra de conjuntos del Álgebra de Boole, las leyes de De Morgan serían así:

Desde el punto de vista práctico existe una forma simplificada de representar expresiones booleanas. Se emplean apóstrofos (') para indicar la negación, la operación suma (+) se representa de la forma normal en álgebra, y para el producto no se emplea ningún signo, las variables se representan, normalmente con una letra mayúscula, la sucesión de dos variables indica el producto entre ellas, no una variable nombrada con dos letras.

La representación de las leyes de De Morgan con este sistema quedaría así, con letra minúsculas para las variables:

y así, empleando letras mayúsculas para representar las variables:

Todas estas formas de representación son correctas, se utilizan de hecho, y pueden verse al consultar bibliografía. La utilización de una u otra notación no modifica el álgebra de Boole, solo su aspecto, y depende de la rama de las matemáticas o la tecnología en la que se esté utilizando para emplear una u otra notación.

Hay numerosos casos de distintos análisis de estructuras algebraicas que corresponden al álgebra de Boole, aunque en apariencia son muy diferentes, su estructura es la misma. Vamos a ver algunos de ellos, con el propósito de hacer palpable las similitudes en la estructura y los distintos ámbitos de aplicación y distinta terminología para referirse a las operaciones o a las variables.

Una serie de temas, aparentemente tan distintos, tiene dos cosas en común, la lógica binaria basada en los ceros y los unos y el álgebra de Boole, posiblemente la forma más conocida de esta álgebra, que en ocasiones da lugar a la interpretación que el álgebra de Boole es la lógica binaria exclusivamente, así el conjunto formula_1 en este caso está formado por dos elementos {0,1}, o {F, V}, o {no, sí}, dos valores contrapuestos, que son las dos posibles alternativas entre dos situaciones posibles, aquí, sin pérdida de la generalidad, tomaremos el conjunto: {0,1} como ya hemos dicho:

Donde:


La operación unaria interna negación, definimos una aplicación que a cada elemento a de {0,1}, le asigna un b de {0,1}.

Para todo elemento a en {0,1}, se cumple que existe un único b en {0,1}, tal que b es la negación de a. Como se ve en la tabla.


Con la operación suma definimos una aplicación que, a cada par ordenado (a, b) de B por B, le asigna un c de B.

Para todo par ordenado (a,b) en B por B, se cumple que existe un único c en B, tal que c es el resultado de sumar a con b.


Con la operación producto definimos una aplicación que, a cada par ordenado (a, b) de B por B, le asigna un c de B.

Para todo par ordenado (a, b) en B por B, se cumple que existe un único c en B, tal que c es el resultado del producto a y b. Como se puede ver en la tabla.

Así formula_80 es un álgebra de Boole al cumplir los siguientes axiomas:











Luego formula_80 es álgebra de Boole.

Partiendo de estos axiomas se puede demostrar los siguientes teoremas:










Partiendo de formula_103 álgebra de Boole, dadas dos variables binarias: a, b, que cumplen alguna de estas condiciones:

entonces a es menor o igual que b. Dados los valores binarios 0 y 1, podemos ver:

Estas cuatro condiciones son equivalentes y el cumplimiento de una de ellas supone el cumplimiento de las otras, en este caso es sencillo comprobarlas todas. Luego podemos decir que 0 antecede a 1 y lo denotamos:

Si además sabemos que 0 y 1 son valores distintos:

El valor binario 0 es menor que el valor binario 1.

Dado cualquier conjunto U, se llama conjunto potencia de U, al conjunto de todos los subconjuntos posibles de U y lo denotamos formula_111.

A título de ejemplo podemos considerar:

Que tiene como conjunto potencia:

El conjunto vacio es el que no tiene elementos y se representa:

Podemos definir:

Y como es obvio:


En esta operación definimos una aplicación que, a cada elemento A de P(U), le asigna un B de P(U).

Para todo elemento A en P(U), se cumple que existe un único B en P(U), tal que B es el complemento A.

Definiendo el complemento de un conjunto así:

B es el complemento de A, si se cumple que para todo x que pertenezca a B, x pertenece a U y x no pertenece a A.


Con esta operación binaria interna definimos una aplicación que, a cada par ordenado (A, B) de P(U) por P(U), le asigna un C de P(U).

Para todo par ordenado (A,B) en P(U) por P(U), se cumple que existe un único C en P(U), tal que C es la unión A y B.

Definiendo la unión de dos conjuntos como:
El conjunto C es la unión de A y B, si para todo elemento x de C, se cumple que x es elemento de A o de B


Con lo que definimos una aplicación que, a cada par ordenado (A, B) de P(U) por P(U), le asigna un C de P(U).

Para todo par ordenado (A,B) en P(U) por P(U), se cumple que existe un único C en P(U), tal que C es la intersección A y B.

Definiendo la intersección de dos conjuntos como:

El conjunto C es la intersección de A y B, si para todo elemento x de C, se cumple que x es elemento de A y de B.

Con lo que podemos plantear: formula_127, para un U conocido, como álgebra de Boole si cumple las siguientes axiomas:











Concluyendo que formula_138 es un álgebra de boole.

Partiendo de estos axiomas se puede demostrar los siguientes teoremas:







 \forall A \in \mathcal{P}(U)


</doc>
<doc id="3057" url="https://es.wikipedia.org/wiki?curid=3057" title="9 de febrero">
9 de febrero

El 9 de febrero es el 40.º (cuadragésimo) día del año en el calendario gregoriano. Quedan 325 días para finalizar el año y 326 en los años bisiestos.









</doc>
<doc id="3059" url="https://es.wikipedia.org/wiki?curid=3059" title="1990">
1990

1990 () fue un año común comenzado en lunes según el calendario gregoriano, y fue designado como:






































En el Mundial disputado en Argentina, Yugoeslavia vence a la Unión Soviética en la final, iniciando la despedida de estas dos potencias europeas, de las competencias internacionales, puesto que ambos países se desintegrarían a la brevedad.

Campeonato Del Mundo de Velocidad:
125 c.c.: Loris Capirossi, ITA, Honda RS 125
250 c.c.: John Kocinski, USA. Yamaha YZR 250
500 c.c.: Waine Rainey, USA, Yamaha YZR 500













</doc>
<doc id="3062" url="https://es.wikipedia.org/wiki?curid=3062" title="Órgano">
Órgano

Órgano hace referencia a varios artículos:








</doc>
<doc id="3064" url="https://es.wikipedia.org/wiki?curid=3064" title="Ética">
Ética

La ética, o filosofía moral, es la rama de la filosofía que estudia la conducta humana, lo correcto y lo incorrecto, lo bueno y lo malo, la moral, el buen vivir, la virtud, la felicidad y el deber. La ética contemporánea se suele dividir en tres ramas o niveles: la metaética estudia el origen, naturaleza y significado de los conceptos éticos, la ética normativa busca normas o estándares para regular la conducta humana, y la ética aplicada examina controversias éticas específicas.

Ética y moral son conceptos muy relacionados que a veces se usan como sinónimos, pero tradicionalmente se diferencian en que la ética es la disciplina académica que estudia la moral. La ética no inventa los problemas morales, sino que reflexiona sobre ellos. Las acciones relevantes para la ética son las acciones morales, que son aquellas realizadas de manera libre, ya sean privadas, interpersonales o políticas. La ética no se limita a observar y describir esas acciones, sino que busca determinar si son buenas o malas, emitir juicio sobre ellas y así ayudar a encauzar la conducta humana.

El estudio de la ética se remonta a los orígenes mismos de la filosofía en la Antigua Grecia, y su desarrollo histórico ha sido amplio y variado. A lo largo de la historia ha habido diversas maneras de entender la ética y distintas propuestas morales orientadoras de la vida humana.

Aunque la ética siempre fue una rama de la filosofía, su amplio alcance la conecta con muchas otras disciplinas, incluyendo la antropología, biología, economía, historia, política, sociología y teología.

Desde el inicio de la reflexión filosófica ha estado presente la consideración sobre la ética. Platón afronta la temática ética en diversos lugares y desde contextos diferentes. Así, por ejemplo, en el "Gorgias" busca superar el hedonismo y la ley del más fuerte. En el "Fedón" evidencia la importancia de lo que exista tras la muerte para regular el propio comportamiento. En "La República" aborda juntamente la ética individual (desde la perspectiva de una justicia dentro del alma) y la ética pública, con una compleja teoría del Estado, que encuentra complementos y puntos de vista diferentes en otras dos obras, el "Político" y las "Leyes". En la segunda mitad de la obra "Fedro", uno de los temas principales es la ética.

La "Ética nicomáquea", seguramente el más importante tratado de ética de Aristóteles, se basa en la premisa de que todo ser humano busca la felicidad (ética eudemónica). Para Aristóteles todos los seres naturales tienden a cumplir la función que les es propia y están orientados a realizar completamente sus potencialidades. El bien, que es lo mismo que la perfección de un ser o la realización de las capacidades es cumplir su función propia, aquello a que solo él puede realizar. También los seres humanos están orientados a la realización plena de la función que les es propia. El problema que se suscita, entonces, es cuál es la función propia del hombre. Y si acaso hay más de un bien propio del hombre, ¿cuál es el bien más alto y más perfecto de los que puede alcanzar el ser humano?

Como en otras de sus obras, Aristóteles releva las opiniones de sus contemporáneos al respecto y comprueba que todas parecen estar de acuerdo en que el objetivo supremo del hombre es vivir bien y ser feliz, aunque hay muchos desacuerdos respecto de en qué consiste la felicidad y el buen vivir. Para Aristóteles la vida feliz (plena) es la que permite realizar la actividad superior (contemplación), con una suficiente autonomía (bienes materiales, salud), y en compañía de un número suficiente de amigos (cf. "Ética nicomáquea" I).

Solo son morales las acciones en las que se puede elegir y decidir qué hacer. En cambio, no son morales ni inmorales las acciones padecidas, compulsivas o forzosas. Lo que es moral es la acción que depende de la voluntad, si se actúa de modo correcto. ¿Cuándo se actúa correctamente? La forma correcta de actuar depende del ámbito de acción (dianoético o intelectual, ético o moral) y en parte está pautada por las costumbres de la comunidad a la que se pertenece (si la comunidad es éticamente sana, algo que supone Aristóteles para el mundo griego quizá de modo acrítico) y se aprende con la educación. Cuando se actúa de acuerdo con estas pautas, se vive bien y se es virtuoso.

Por otra parte, los filósofos estoicos y epicúreos propusieron teorías morales basadas en principios opuestos: la virtud y la vida con moderación (estoicismo), y la búsqueda del placer (epicureísmo).

Es un momento en el que la ética asume elementos de las doctrinas clásicas de la felicidad (el fin del actuar humano consiste en obtener el bien que nos hace felices) y los une a la doctrina cristiana (vista como Revelación divina), especialmente según la normativa que recogen los mandamientos. El fin último del actuar humano es la caridad, que se consigue al vivir desde el Evangelio, y que permite al hombre acceder a la visión de Dios (en el cielo), donde el ser humano alcanza su máxima plenitud y el bien supremo.

Diversos autores hablan de ética y según perspectivas diferentes. Es oportuno recordar dos grandes nombres, san Agustín de Hipona y santo Tomás de Aquino (especialmente en la segunda parte de la "Suma de teología", en la que se recogen numerosos elementos de la ética de Aristóteles).

Posteriormente, y tras las huellas de las ideas de Tomás de Aquino, se desarrolla en el ámbito católico lo que luego será conocido como principio de doble efecto.

Los filósofos éticos modernos trabajan con la mirada puesta, sobre todo, en el mundo antiguo (estoicos, epicúreos, Platón, Aristóteles), si bien con algunos elementos heredados de la Escolástica medieval. Descartes tiene algunos elementos de ética en su famoso "Discurso del método". Dentro del racionalismo, es Baruch Spinoza quien elaboró de modo más amplio y sistemático una propuesta ética. En el ámbito del empirismo, David Hume trabajó en diversos momentos para comprender los motivos profundos de las acciones humanas.

La gran revolución ética moderna se realiza a través de Immanuel Kant, que rechaza una fundamentación de la ética en otra cosa que no sea imperativo moral mismo (deontologismo formal), pues si la moral se orienta a buscar la felicidad no podría dar ninguna norma categórica ni universal. Los filósofos idealistas desarrollaron esta moral del imperativo categórico. Hacen frente así al utilitarismo, al afirmar que el principio de utilidad no es el único criterio de corrección de las acciones.

La ética del siglo XX ha conocido aportes muy importantes por parte de numerosos autores: los vitalistas y existencialistas desarrollan el sentido de la opción y de la responsabilidad, Max Scheler elabora una fenomenología de los valores. Autores como Alain Badiou han intentado demostrar que esta principal tendencia (en las opiniones y en las instituciones), la cuestión de «la ética» en el siglo XX, es en realidad un «verdadero nihilismo» y «una amenazante denegación de todo pensamiento».

Recientemente, y desarrollando un análisis en profundidad de los orígenes y fundamentos de la ética, han aparecido diversos estudios sobre el papel de las emociones en el desarrollo de un pensamiento ético antifundacionalista, como ha indicado Richard Rorty. En las últimas dos décadas, el filósofo escocés MacIntyre establece nuevas herramientas de análisis histórico-filosófico de distintas versiones rivales de la ética.

La palabra «ética» proviene del latín "ethĭcus", y este del griego antiguo ἠθικός transliterado como "ēthikós". La forma femenina proviene del latín tardío "ethĭca," y este del griego "ēthikḗ". Según algunos autores, se debe diferenciar "êthos", que significa «carácter», de "ethos", que significa «costumbre», pues «ética» se sigue de aquel sentido y no es este.

Etimológicamente «ética» y «moral» tienen el mismo significado, ya que la palabra «moral» viene de latín "mos" que significa «hábito» o «costumbre».





</doc>
<doc id="3065" url="https://es.wikipedia.org/wiki?curid=3065" title="1969">
1969

1969 () fue un año normal comenzado en miércoles, según el calendario gregoriano.













































El XIV Festival se celebró en la ciudad de Madrid (España) el sábado 29 de marzo, en el Teatro Real.
La presentadora fue Laura Valenzuela y Ramón Díez, el director.
La dirección musical corrió a cargo de Augusto Algueró.
Concurrieron un total de 16 países.


En Estados Unidos se realiza el primer Festival de Woodstock al que asisten bandas de diversas categorías, como Canned Heat, The Who, Creedence Clearwater Revival, Grateful Dead y Jimi Hendrix, entre otras. Asiste una cantidad importante de personas. El Festival de Woodstock se realizó nuevamente en varias ocasiones.




</doc>
<doc id="3066" url="https://es.wikipedia.org/wiki?curid=3066" title="1975">
1975

1975 () fue un año normal comenzado en miércoles y fue designado como:



































































</doc>
<doc id="3080" url="https://es.wikipedia.org/wiki?curid=3080" title="1985">
1985

1985 () fue un año normal comenzado en martes en el calendario gregoriano.
Corresponde al año del Buey en el horóscopo chino. La Organización de las Naciones Unidas lo declaró "Año Internacional de la Juventud", en tanto la UNESCO lo declaró "Año Internacional de la Música".











































Campeón: Bolívar de La Paz; 
Subcampeón: Wilstermann de Cochabamba.




























</doc>
<doc id="3082" url="https://es.wikipedia.org/wiki?curid=3082" title="1616">
1616

1616 (MDCXVI) fue un año bisiesto comenzado en viernes.





</doc>
<doc id="3083" url="https://es.wikipedia.org/wiki?curid=3083" title="29 de agosto">
29 de agosto

El 29 de agosto es el 241.º (ducentésimo cuadragésimo primer) día del año en el calendario gregoriano y el 242.º en los años bisiestos. Quedan 124 días para finalizar el año.








</doc>
<doc id="3103" url="https://es.wikipedia.org/wiki?curid=3103" title="Ciencias humanas">
Ciencias humanas

Ciencias humanas es un concepto epistemológico que designa a un extenso grupo de ciencias y disciplinas cuyo objeto es el ser humano en el aspecto de sus manifestaciones inherentemente humanas, esto es el lenguaje verbal en primer término, el arte y el pensamiento y, en general, la cultura y sus formaciones históricas. El término de Ciencias humanas se opone y, por otra parte, complementa al de Ciencias naturales o físico-naturales. El término de Humanidades no es en realidad sino una abreviatura, de preferencia anglosajona, frente al uso más tradicional germánico y románico de Ciencias humanas, directamente establecido sobre la tradición humanística.

Las modernamente denominadas Ciencias humanas constituyen una entidad fundada en la antigüedad clásica, con posterioridad humanísticamente delimitada, tras el régimen medieval del , mediante la designación secular de Studia humanitatis (es decir, característica y centralmente Gramática, Retórica, Dialéctica, Poética, Poesía o Literatura como disciplina y lectura del canon clásico, Historia, Filosofía, especialmente Ética o Filosofía moral). A finales del siglo XIX y comienzos del XX surgieron las denominaciones de Ciencia de la Cultura y Ciencias del Espíritu, esta última preconizada por Wilhelm Dilthey, el más importante teórico moderno sobre la materia, las cuales designan teorías fundamentales de la epistemología de las Ciencias humanas y, generalizada y permanentemente, han sido consideradas como términos equivalentes al de estas.

Entre las Ciencias humanas y las Ciencias naturales existe, a partir del siglo XIX, tras la crisis de la metafísica idealista y la irrupción de la Sociología, la serie intermedia ya estable designada Ciencias sociales, de definición sin duda menos nítida en virtud de su carácter interrelacionado. Fuera de los campos humanísticos, existe en nuestro tiempo la frecuente tendencia a omitir o aminorar la presencia de las Ciencias humanas en favor de una sobrexposición de las Ciencias sociales como consecuencia, entre otros factores, del incremento de la tendencia occidental, ahora también extendida a Asia, de predominio de las razones económicas de mercado frente a las clásicas y actualmente secundarias de cultura humanística, así como de la extraordinaria influencia desempeñada por los medios de comunicación y sus potentes capacidades de inserción política y social.

La historia de las ciencias humanas asienta en una antigüedad primigenia por principio fundada en saberes profundos pero indiferenciados cuya referencia indiscutible se encuentra en Pitágoras. Las Ciencias humanas se remontan evidentemente a época tan antigua como la de cualquier rama del conocimiento humano. En el pensamiento socrático y en el pensamiento más técnico de los sofistas queda constituido plenamente el saber de la ciencia humanística, ya en la "enciclopedia" aristotélica configurado en el orden más general de las ciencias, es decir, por ejemplo, Retórica y Poética, Ética y Política, o Biología.

Dilthey, heredero de la hermenéutica de Friedrich Schleiermacher, asume el concepto de "comprensión" ("Verstehen") como principio cognoscitivo de las Ciencias humanas. Esto representa la oposición del par "explicación" / "comprensión", mantenido por Droysen, en tanto oposición Ciencia natural / Ciencia histórica o humana. Dice Dilthey: "La comprensión cae bajo el concepto general del conocer, entediéndose por "conocer", en el sentido más amplio, aquel proceso en el cual se busca un saber de validez universal". "Llamamos "comprender" al proceso en el cual se llega a conocer la vida psíquica partiendo de sus manifestaciones sensiblemente dadas". "Denominamos "interpretación" la comprensión técnica de manifestaciones de vida fijadas por escrito" .

En la "Introducción a las Ciencias del Espíritu", afirma Dilthey que el estudio de las ciencias humanas o “ciencias del espíritu” es la interpretación de la experiencia personal en un entendimiento reflexivo de la experiencia y una expresión natural de los gestos, las palabras y el arte. También indica que todo saber debe analizarse a la luz de la historia. Sin esta lógica, el conocimiento solo puede ser parcial. En "El mundo histórico", que ofrece el desarrollo epistemológico por antonomasia de la ciencia del espíritu como humanística, dice Dilthey a propósito de "los métodos en los que se nos presenta el mundo espiritual": "La conexión de las ciencias del espíritu se halla determinada por su fundamento en la vivencia y en la comprensión, y en ambas encontramos diferencias tajantes con respecto a las ciencias de la naturaleza, que prestan su carácter propio al edificio de las ciencias del espíritu".

El objeto de las Ciencias humanas, que se define, frente al de las físico-naturales, en virtud de su singularidad, irrepetibilidad e historicidad, estatuye una gama metodológica que alcanza desde el método filosófico y dialéctico, el hermenéutico y el histórico-crítico hasta el comparatista.
Los métodos cuantitativos y estadísticos, si bien pueden ejercer subsidiariamente alguna función en la investigación científico-humanística, según en sana lógica cabe comprender, en ningún caso son susceptibles de desempeñar alguna función decisoria ni constante en Ciencias humanas, a diferencia de las Ciencias sociales, en las cuales desempeñan a menudo un procedimiento característico o imprescindible.

Existen taxonomías de criterio tanto en Ciencias humanas como naturales y sociales. Alguna de ellas incluso se quiere transversal entre humanas y sociales, pero de hecho, al presentar múltiples insuficiencias e indeterminaciones, ofrece resultados señaladamente antieconómicos. Existe una discriminación que divide en ontológicas, metodológicas y epistemológicas, pero cuyos solapamientos devienen insostenible desajuste. Como es evidente, la clasificación de las ciencias, humanas o cualesquiera otras es cambiante y responde a la cultura académica y epistemológica de cada época.

Suele afirmarse que la Filosofía es la primera de las Ciencias humanas por cuanto en origen fue matriz de parte de éstas y asimismo atañe de algún modo a la organización del conjunto. Esta relación, que es extensible a las ciencias en general, es de reconocer que modernamente se ha debilitado. En criterio asimismo general, se entiende con frecuencia la Filosofía como fundamento del conocimiento e incluso a veces como ciencia de ciencias. Sin embargo, ya no es frecuente considerar la Gnoseología o Teoría del Conocimiento, la específica disciplina filosófica de determinación cognoscitiva, como primera ciencia. Con todo, la Gnoseología, por una parte, y la Epistemología, que sin embargo actualmente ya cabe ser adscrita de manera sectorial a cada una de las disciplinas humanísticas por sí, puede decirse que continúan señalando los diferentes y respectivos límites de la actividad cognoscitiva y disciplinar.

Desde su fundamento platónico, y tras el eje socrático, que decidió una filosofía del hombre frente a una filosofía de la naturaleza, es de notar la existencia secular de un doble lineamiento, el de una filosofía contemplativa, a veces neoplatónica, y una filosofía sectorialmente disciplinar y más característicamente aristotélica y académica.
Entre las tradicionales ramas disciplinares de la Filosofía se cuentan fundamentalmente la Metafísica, la Ontología, la Gnoseología, la Lógica, la Ética y la Axiología.
Entre las delimitaciones modernas se encuentran la Antropología y la Estética, ya considerables con un alto grado de autonomía y vinculación a otras disciplinas contiguas. Por su parte, una disciplina como sobre todo la Psicología, ya se da por definitivamente escindida.
La Filosofía, a lo largo de su desarrollo histórico y en función del avance del conocimiento, ha ido diversificándose en distintas ramas a fin de aproximarse de forma adecuada a su objeto.
Existe establecida una serie de ramas especiales, así Filosofía del Lenguaje, Filosofía de la Historia, Filosofía de la Ciencia, Filosofía de la Religión, Filosofía del Derecho, Filosofía de la Educación. Por otro lado, al margen de dichas ramas y aparte ciertos usos más o menos justificables se ha fomentado la tendencia anglosajona a aducir distinciones que a veces se multiplican casi indiscriminadamente. En cualquier caso, se trata de distinciones que no constituyen disciplina y, referidas al saber o a la actividad que fuere, han de mantener cuando menos el sentido propio respecto de primeros principios, normas reguladoras y finalidades.

La gran serie científico-humanística configurada por la Filología delinea tanto la concatenación más extensa de campos de las Ciencias humanas como el trazado más técnico de su metodología. Según afirmaba Johan Huizinga, en Ciencias humanas casi todo es Filología. El marco extensísimo de la Filología permite discernir, siguiendo la "ciencia real", un ámbito general y un ámbito particular relativo al mundo concreto de las lenguas naturales y sus familias y culturas, en el bien entendido de que este último es requisito y objeto presupuesto en el primero.

El ámbito general de la Filología se encuentra organizado sobre la dicotomía de dos grandes dominios: Ciencia del lenguaje o Lingüística y su paralela Ciencia de la literatura o literaria. Ambos dominios han devenido, cada uno por su parte, un organismo disciplinar tripartito organizado sobre la base de tres criterios: histórico, teórico y aplicativo cuya disposición en tanto que ciencia real consiste en la subsiguiente doble serie de Lingüística histórica, Lingüística general o teórico-descriptiva y Lingüística aplicada, y por otra parte Historia de la literatura, Teoría de la literatura y Crítica literaria. Es preciso tener en cuenta que la Teoría de la literatura secularmente y desde la antigüedad configuró dos disciplinas clave de la enciclopedia aristotélica y actualmente vigentes y decisorias: la Retórica o ciencia del discurso general y la Poética o ciencia de la construcción de la obra literaria. Por demás, a estos campos disciplinares, tan autónomos como simétricamente interrelacionados, se suman otros verdaderos campos de naturaleza metodológica transversal de primer orden, así por ejemplo y eminentemente la Ecdótica o Crítica textual, la Traductología, la Dialectología, la Literatura comparada, la Lingüística comparada y, aún más allá, en su sentido completo pluridisciplinar y globalizador, la Comparatística, que en último término atañe al conjunto de las ciencias, sobre todo humanas, pero también sociales y naturales.

Bajo la denominación reciente de Biblioteconomía, o Biblioteconomía y Archivística y Documentación, tiende actualmente a discriminarse una disciplina auxiliar autónoma respecto de las tradicionales metodologías filológicas nacidas en la Escuela de Alejandría. No tanto sucede con las no menos tradicionales Paleografía y, la más general, Bibliografía. Ambas de hecho, pero sobre todo esta última, atañen instrumentalmente a todos los dominios del saber.

El ámbito particular de la Filología es relativo a las múltiples lenguas naturales concretas y sus mundos de cultura. Las áreas mayores de este ámbito a su vez se organizan escalonadamente en sucesivos dominios disciplinares cada vez más concretos y con más específica determinación, por tanto, de lengua concreta. Asimismo, la Filología general, sus series disciplinares, se realizan en las Filologías particulares. La clasificación de las áreas o campos disciplinares mayores de las Filologías particulares es muy nutrida; incluye, principalmente: Egiptología, Indología, Sinología, Niponología, Coreanología, así como Arabismo o Filología Árabe o Filología Semítica, Africanismo, Filología Bíblica o Escriturística, y Filología Clásica o griega y latina antiguas, fundamento de la cultura occidental. El ingente desarrollo de las filologías particulares hace prescindible la distinción de Filología Moderna frente a la referida Clásica y opta por la necesaria distinción sucesiva de particulares, entre ellas, sobre todo, Filología Alemana o Germanística, Filología Inglesa o Angloamericana, Filología Eslava o Eslavística, Filología Románica o Romanística, que a su vez incluye la completa familia neolatina: Filología Francesa, Filología italiana, Filología Rumana, Filología Portuguesa (y sus variantes brasileña y africana) o Galaicoportuguesa.
Dentro de la Filología Románica posee especial dimensión la Filología Española en tanto que Filología Hispánica, de extraordinaria expansión americana, y sus múltiples subcampos, extremadamente desde la Iberística originaria hasta el sefardí o el Filipinismo asiático e incluso un dominio peninsular ibérico originalmente no románico como el vascuence, además de sus variantes románicas peninsulares como la gallega, la valenciana y la catalana.

La Estética se refiere tanto a la Naturaleza, y a la vida en general, como al arte en tanto Filosofía del arte en concepto hegeliano. La Estética tiene dos grandes épocas o ciclos. La Estética antigua y clasicista define un saber entremezclado, como prototípicamente se observa en la obra de Platón; la Estética moderna, especialmente a partir del Empirismo inglés, Baumgarten y, sobre todo la "Crítica del Juicio" de Kant, se configura como disciplina autónoma desligada de la Ética, cosa esta última que fue discutida de inmediato, y reelaborada de hecho por Friedrich Schiller mediante sus argumentos estéticos acerca de la libertad antropológicamente fundados. A partir de Kant la Estética propiamente idealista pasa a desempeñar el lugar clave de resolución para el pensamiento moderno. Con posterioridad es de constatar, entre otras cosas, una estética tanto de la empatía o proyección como formalista en Alemania. A principios del siglo XX propuso Benedetto Croce un barrido de la "techne" epistemológicamente reordenador. La segunda mitad del siglo XX ha estado determinado especialmente por la teoría problemática de Theodor Adorno, en nuestro tiempo sometida a crítica.

El centro teórico de la disciplina está formado, principalmente, por la estimativa y la teoría del valor, por la teoría del efecto estético y, acaso en lo más característico, por las categorías estéticas, esto es fundamentalmente la Belleza y lo Sublime, pero también lo Humorístico y lo Trágico, por otra parte lo Feo, que son las distinciones mejor asentadas. Existe, cuando menos, otro tipo de categorías estéticas modernamente reconocido, las histórico-estilísticas, de inserción periodológica.

Las Ciencias del arte configuran una serie análoga a la general de la Filología, siguiendo los tres criterios de intervención histórica, teórica y aplicativa, esto es Historia o historiografía del Arte, Teoría del Arte y Crítica artística. Este régimen de la ciencia real se ha extendido con naturalidad y eficacia al conjunto de objetos que definen estos campos tradicionales pero también los de nueva creación contemporánea. Estos campos y objetos, definitoriamente, se refieren a artes plásticas, visuales y auditiva musical. Se trata de disciplinas que historizan, analizan y critican el arte. A esta serie disciplinar se ha de sumar la instrumentalización procurada por la Museografía. Se trata de campos y objetos que actualmente poseen importante proyección a través de los medios de comunicación. El crítico de arte analiza, observa y aprecia las obras de arte desde una perspectiva cuyo grado de objetividad constituye uno de los problemas básicos de estas especializaciones, particularmente en los medios de carácter publicístico o de actualidad.

Las tradicionalmente llamadas Bellas Artes han sido las plásticas, es decir Pintura, Escultura y Arquitectura, esto es las artes particulares en concepto hegeliano, pero también las subespecialidades como la de la estampación calcográfica y el grabado en sus distintas gamas, desde la xilografía hasta la serigrafía. Asimismo, la Numismática, y la serie de artes "menores", sea la cerámica y las artes decorativas, o la caligrafía. Por otra parte, son de distinguir campos de estudio, como especialmente la Iconología.

A ello se ha de sumar, ya sistemática y establemente desde la Poética aristotélica, la danza. El teatro es considerable tanto en su vertiente de arte literario o poesía como en tanto artes escénicas. A esta gran serie añadió el siglo XX géneros especiales híbridos como el de la instalación y, por otra parte, el cine sobre todo, también el vídeo. Pero antecede a éste la fotografía, quizás mejor incluible entre las plásticas tradicionales. Por su parte, la ópera, con frecuencia tenida por "arte total", en tanto compuesta, remite a las artes escénicas en conjunto, pero eminentemnete a la coral y la música, a un género de ésta. Todas estas artes son, pues, objeto de estudio histórico, teórico y crítico.

Es de entender, en primer lugar, siguiendo la clasificación hegeliana, desde una Historia inmediata que conceptualiza lo que ha sucedido y ha sido visto, una historia reflexionada y una historia por conceptos. Esta Historia por concepto es aquella que se refiere, y aquí es cuestión decisiva, a todas y cada una de las ciencias y disciplinas humanísticas. La historia como materia política es parcialmente una ciencia social que estudia el pasado de la humanidad. En su especialización, se centra en el desarrollo de ciertos sistemas (la sociedad, las poblaciones, etc.), a través del tiempo; en algunos casos insistiendo en su capacidad de cuantificación. Desde otro punto de vista, sistematiza y analiza las acciones humanas (para Habermas acción comunicativa) en periodos de tiempo definidos.

La Historiografía ha evolucionado con frecuencia durante el siglo XX desde los objetos generales civiles, políticos y socioeconómicos hacia preferencias de la vida privada, material y de las mentalidades.

La Historia de la cultura y la Historia de las ideas configuran dos ramas historiográficas modernas y especiales en virtud de la compleja historicidad de sus objetos. Ambas se refieren a objetos constitutivamente diferentes al tiempo que pueden ser reintegrados como parte. Característicamente definen formas del pensamiento contemporáneo, largamente maduradas y que culminan estableciéndose durante la segunda mitad del siglo XIX, sobre todo a manos, respectivamente, de Jacob Burckhardt y Marcelino Menéndez Pelayo.

Mucha menor entidad han adquirido las delimitaciones de "Historia de las mentalidades" e "Historia intelectual", de perfiles menos nítidos o menos eficaces.
La Historia de las ideas, habitualmente relacionada con la Comparatística, ha tenido en los campos del pensamiento estético y político sus dos ámbitos mayores de desarrollo.

Existen fundamentalmente una Antropología filosófica y una Antropología etnográfica o Etnografía. Ha sido concebida una Antropología general pero también es un hecho que el objeto de estudio antropológico no ha sido especificado establemente. Es de asumir que la Antropología estudia el comportamiento humano desde un criterio parcial o bien holístico, así como las relaciones humanas, o sea los grupos humanos en tanto culturales y según qué relaciones interpersonales determinan, las jerarquías de estos grupos, sus conflictos y su evolución. El enfoque de esta disciplina, tradicionalmente, se ha aplicado al estudio de la evolución y el comportamiento de aspectos paradigmáticos del individuo, bien de grupos humanos, sean ágrafos (sin escritura) o aislados, pero también en convivencia con otras líneas de estudio relativas a la vida y las sociedades modernas y sus derivaciones (sobre todo las occidentales); así por ejemplo, las aplicaciones relativas a antropología de la empresa, ya posteriores a la relación de objeto establecida entre antropología rural y urbana, entre otras. Durante el siglo XX tanto el Funcionalismo como sobre todo el Estructuralismo, e incluso la Lingüística de esta inclinación, afectaron grandemente a los estudios antropológicos llegando a concebirse un antes y un después. Actualmente parece superado esa perspectiva de cosas.

La Geografía constituye la plural y paradigmática serie científica que atañe, según sus partes, tanto a las Ciencias humanas, así la tradicionalmente llamada Geografía humana, fronteriza con la Historia y la Antropología, como a las físico-naturales, esto es la Geografía física, y también sociales en el caso de la Geografía de la población.
Las ciencias auxiliares de la Geografía son múltiples, a partir de la Ciencias de la Tierra y hasta concreciones aplicativas como las referentes a biología, cultura o turismo..., pero poseen especial estatus la Cartografía y la Paisajística, ambas de gran relieve humanístico.

Si la Geografía ofrece una entidad organizada por principio como pluralidad científica, las Ciencias jurídicas ofrecen por su parte un perfil de transición humana / social paradigmático. No puede olvidarse que el ámbito jurídico atiende desde una Filosofía del Derecho hasta una práctica puramente política o meramente administrativa aplicada en lo fundamental a métodos de trámite oficial o institucional en todas las diversas instancias. Puede decirse que la Sociología ocupa el centro definitorio de las Ciencias sociales, mientras que las ciencias políticas y la Economía o las Ciencias económicas configuran sus grandes dominios de expansión en el cuerpo social, ya con independencia del aspecto humano esencial. La Sociología estudia una parte específica o, más bien, una perspectiva específica de la totalidad de la vida social que incluye fundamentalmente el intercambio material, de actitudes y emociones, especialmente costumbres, comportamientos individuales e interpersonales de los miembros de una sociedad. Su mayor objetivo es entender al ser humano como parte de un grupo social y sus relaciones. Los estudios sociales son en gran medida, a diferencia de los humanísticos, de base cuantitativa y estadística. Existe, por otra parte, una tendencia llamada "filosofía social".

Si bien la Política nace de completa planta en la enciclopedia aristotélica como humana relación Ética y Retórica, modernamente representa, al menos en cierta medida relevante, el mejor ejemplo de desgajamiento del tradicional saber humanístico y aproximación al ámbito de la Sociología. Las ciencias políticas son el grupo de ciencias sociales dedicado, al menos en parte, a la toma decisiones. Estas son relativas a la teoría política.

Las Ciencias económicas son sociales y estudian las relaciones del individuo o el ciudadano regidas por instrumentos de cuantificación "naturales": precios, salario, cantidades de bienes producidos, ritmo de producción, etc. La Economía estudia con medios cuantitativos el funcionamiento que provee el hombre económico, el cual tiene a su disposición una gran diversidad de instrumentos y persigue una multiplicidad de fines. Este individuo es por tanto considerado en el marco del régimen social de producción, de la producción de bienes materiales. Más allá de la Contabilidad, en el marco de la Contaduría, existe una ciencia económica matematizada en tanto disciplina teorética y no sólo estadística. Aunque estas técnicas y saberes se relacionan con la actividad de la persona, muy poco tienen que ver con las humanidades. Es de saber que la teoría económica moderna fue creada por los filósofos de la Escuela de Salamanca.

Finalmente, cabe decir que la Biología Humana y, en general por otra parte, las actualmente denominadas Ciencias de la salud, ejercen una vinculación humana y humanitarista ajena al núcleo de los objetos humanísticos. La biología humana, fundándose en la anatomía y fisiología, explica el funcionamiento del cuerpo humano. Su enfoque primario recae en la descripción interna de los órganos que lo componen y las relaciones que mantienen entre sí. El término de Ciencias de la salud tiende a englobar la Medicina en sus diferentes especializaciones y niveles tanto técnicos y terapéuticos como asistenciales. Ha existido tradicionalmente una fuerte vinculación, aun epistemológicamente por completo externa, entre medicina y cultura humanística.

Entrado el siglo XXI, toda ciencia humanística queda referida al curso y al problema de la Globalización.
Por otra parte, este cuerpo de conocimiento y disciplinas, redefinido en el siglo XIX es fundamental para el desarrollo de las variantes epistemológicas que se pudieran producir en lo sucesivo. La Ciencias humanas han de desarrollar su propia epistemología con vigor e independencia frente a frecuentes e indisimuladas agresiones. El crecimiento de las Ciencias sociales, especialmente gracias al ingente aparato político organizativo de las sociedades occidentales, sobre todo como aplicación del procedimiento de la "encuesta" y la estadística, ha conducido a un retraimiento académico de las Ciencias humanas y la cultura humanística, que han de reubicar su programa y finalidades irrenunciables en un mundo regido, para bien y para mal, por el ámbito público. Misión actual de las Ciencias humanas es construir la relación entre civilizaciones, la cual ha de asentarse en el aspecto humano permanente, inherente y no sustituible, de pensamiento, lenguaje, religiones y artes, ahora integrado en un mundo abocado a la globalización y la multitud de problemas culturales, humanísticos y humanitarios que ésta suscita.




</doc>
<doc id="3104" url="https://es.wikipedia.org/wiki?curid=3104" title="Ciencias naturales">
Ciencias naturales

Las ciencias naturales, ciencias de la naturaleza, ciencias físico-naturales o ciencias experimentales son aquellas ciencias que tienen por objeto el estudio de la naturaleza, siguiendo la modalidad del método científico conocida como método empírico-analítico. Son a veces referidas con el término de historia natural.

Las ciencias naturales se apoyan en el razonamiento lógico y el aparato metodológico de las ciencias formales, especialmente de la matemática y la lógica, cuya relación con la realidad de la naturaleza es indirecta. A diferencia de las ciencias aplicadas, las ciencias naturales son parte de la ciencia básica, pero tienen en ellas sus desarrollos prácticos, e interactúan con ellas y con el sistema productivo en los sistemas denominados de "investigación y desarrollo" o "investigación, desarrollo e innovación" (I+D e I+D+I). 

No se deben confundir con el concepto más restringido de ciencias de la Tierra o geociencias.

Las diferencias entre las distintas ciencias naturales no siempre son marcadas, y estas «ciencias cruzadas» comparten un gran número de campos. La física juega un papel significativo en las otras ciencias naturales, dando origen, por ejemplo, a la astrofísica, la geofísica, la química física y la biofísica. Asimismo, la química está representada por varios campos, como la bioquímica, la geoquímica y la astroquímica.

Un ejemplo particular de disciplina científica que abarca múltiples ciencias naturales es la ciencia del medio ambiente. Esta materia estudia las interacciones de los componentes físicos, químicos y biológicos del medio, con particular atención a los efectos de la actividad humana y su impacto sobre la biodiversidad y la sostenibilidad. Esta ciencia también afecta a expertos de otros campos.

Una disciplina comparable a la anterior es la oceanografía, que se relaciona con una amplia gama de disciplinas científicas. La oceanografía se subdivide, a su vez, en otras disciplinas cruzadas, como la biología marina. Como el ecosistema marino es muy grande y diverso, la biología marina también se bifurca en muchas subdivisiones, incluyendo especializaciones en especies particulares.

Hay también un grupo de campos con disciplinas cruzadas en los que, por la naturaleza de los problemas que abarcan, hay fuertes corrientes contrarias a la especialización. Por otro lado, en algunos campos de aplicaciones integrales, los especialistas, en más de un campo, tienen un papel clave en el diálogo entre ellos. Tales campos integrales, por ejemplo, pueden incluir la nanociencia, la astrobiología y complejos sistemas informáticos.



</doc>
<doc id="3222" url="https://es.wikipedia.org/wiki?curid=3222" title="Teoría de la relatividad">
Teoría de la relatividad

La teoría de la relatividad incluye tanto a la teoría de la relatividad especial como la de relatividad general, formuladas principalmente por Albert Einstein a principios del siglo XX, que pretendían resolver la incompatibilidad existente entre la mecánica newtoniana y el electromagnetismo. 
La teoría de la relatividad especial, publicada en 1905, trata de la física del movimiento de los cuerpos en ausencia de fuerzas gravitatorias, en el que se hacían compatibles las ecuaciones de Maxwell del electromagnetismo con una reformulación de las leyes del movimiento. En la teoría de la relatividad especial, Einstein, Lorentz y Minkowski, entre otros, unificaron los conceptos de espacio y tiempo, en un ramado tetradimensional al que se le denominó espacio-tiempo. La relatividad especial fue una teoría revolucionaria para su época, con la que el tiempo absoluto de Newton quedó relegado y conceptos como la invariabilidad en la velocidad de la luz, la dilatación del tiempo, la contracción de la longitud y la equivalencia entre masa y energía fueron introducidos. Además, con las formulaciones de la relatividad especial, las leyes de la Física son invariantes en todos los sistemas de referencia inerciales; como consecuencia matemática, se encuentra como límite superior de velocidad a la de la luz y se elimina la causalidad determinista que tenía la física hasta entonces. Hay que indicar que las leyes del movimiento de Newton son un caso particular de esta teoría donde la masa, al viajar a velocidades muy pequeñas, no experimenta variación alguna en longitud ni se transforma en energía y al tiempo se le puede considerar absoluto.

La teoría de la relatividad general, publicada en 1915, es una teoría de la gravedad que reemplaza a la gravedad newtoniana, aunque coincide numéricamente con ella para campos gravitatorios débiles y "pequeñas" velocidades. La teoría general se reduce a la teoría especial en presencia de campos gravitatorios. La relatividad general estudia la interacción gravitatoria como una deformación en la geometría del espacio-tiempo. En esta teoría se introducen los conceptos de la curvatura del espacio-tiempo como la causa de la interacción gravitatoria, el principio de equivalencia que dice que para todos los observadores locales inerciales las leyes de la relatividad especial son invariantes y la introducción del movimiento de una partícula por líneas geodésicas. La relatividad general no es la única teoría que describe la atracción gravitatoria, pero es la que más datos relevantes comprobables ha encontrado. Anteriormente, a la interacción gravitatoria se la describía matemáticamente por medio de una distribución de masas, pero en esta teoría no solo la masa percibe esta interacción, sino también la energía, mediante la curvatura del espacio-tiempo y por eso se necesita otro lenguaje matemático para poder describirla, el cálculo tensorial. Muchos fenómenos, como la curvatura de la luz por acción de la gravedad y la desviación en la órbita de Mercurio, son perfectamente predichos por esta formulación. La relatividad general también abrió otro campo de investigación en la física, conocido como cosmología y es ampliamente utilizado en la astrofísica.

El 7 de marzo de 2010, la Academia Israelí de Ciencias exhibió públicamente los manuscritos originales de Einstein (redactados en 1905). El documento, que contiene 46 páginas de textos y fórmulas matemáticas escritas a mano, fue donado por Einstein a la Universidad Hebrea de Jerusalén en 1925 con motivo de su inauguración.

El supuesto básico de la teoría de la relatividad es que la localización de los sucesos físicos, tanto en el tiempo como en el espacio, son relativos al estado de movimiento del observador: así, la longitud de un objeto en movimiento o el instante en que algo sucede, a diferencia de lo que sucede en mecánica newtoniana, no son invariantes absolutos, y diferentes observadores en movimiento relativo entre sí diferirán respecto a ellos (las longitudes y los intervalos temporales, en relatividad son relativos y no absolutos).

La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, fue publicada por Albert Einstein en 1905 y describe la física del movimiento en el marco de un espacio-tiempo plano. Esta teoría describe correctamente el movimiento de los cuerpos incluso a grandes velocidades y sus interacciones electromagnéticas, se usa básicamente para estudiar sistemas de referencia inerciales (no es aplicable para problemas astrofísicos donde el campo gravitatorio desempeña un papel importante).

Estos conceptos fueron presentados anteriormente por Poincaré y Lorentz, que son considerados como precursores de la teoría. Si bien la teoría resolvía un buen número de problemas del electromagnetismo y daba una explicación del experimento de Michelson-Morley, no proporciona una descripción relativista adecuada del campo gravitatorio.

Tras la publicación del artículo de Einstein, la nueva teoría de la relatividad especial fue aceptada en unos pocos años por prácticamente la totalidad de los físicos y los matemáticos. De hecho, Poincaré o Lorentz habían estado muy cerca de llegar al mismo resultado que Einstein. La forma geométrica definitiva de la teoría se debe a Hermann Minkowski, antiguo profesor de Einstein en la Politécnica de Zúrich; acuñó el término "espacio-tiempo" ("Raumzeit") y le dio la forma matemática adecuada. El espacio-tiempo de Minkowski es una variedad tetradimensional en la que se entrelazaban de una manera indisoluble las tres dimensiones espaciales y el tiempo. En este espacio-tiempo de Minkowski, el movimiento de una partícula se representa mediante su línea de universo ("Weltlinie"), una curva cuyos puntos vienen determinados por cuatro variables distintas: las tres dimensiones espaciales (formula_1,formula_2,formula_3) y el tiempo (formula_4). El nuevo esquema de Minkowski obligó a reinterpretar los conceptos de la métrica existentes hasta entonces. El concepto tridimensional de punto fue sustituido por el de suceso. La magnitud de distancia se reemplaza por la magnitud de intervalo.

En la teoría de la relatividad una partícula puntual queda representada por un par formula_5, donde formula_6 es una curva diferenciable, llamada línea de universo de la partícula, y "m" es un escalar que representa la masa en reposo. El vector tangente a esta curva es un llamado cuadrivelocidad, el producto de este vector por la masa en reposo de la partícula es precisamente el cuadrimomento. Este cuadrimomento es un vector de cuatro componentes, tres de estas componentes se denominan espaciales y representan el análogo relativista del momento lineal de la mecánica clásica, la otra componente denominada componente temporal representa la generalización relativista de la energía cinética. Además, dada una curva arbitraria en el espacio-tiempo, puede definirse a lo largo de ella el llamado "intervalo relativista", que se obtiene a partir del tensor métrico. El intervalo relativista medido a lo largo de la trayectoria de una partícula es proporcional al intervalo de tiempo propio o intervalo de tiempo percibido por dicha partícula.

Cuando se consideran campos o distribuciones continuas de masa, se necesita algún tipo de generalización para la noción de partícula. Un campo físico posee momentum y energía distribuidos en el espacio-tiempo, el concepto de cuadrimomento se generaliza mediante el llamado tensor de energía-impulso que representa la distribución en el espacio-tiempo tanto de energía como de momento lineal. A su vez un campo dependiendo de su naturaleza puede representarse por un escalar, un vector o un tensor. Por ejemplo el campo electromagnético se representa por un tensor de segundo orden totalmente antisimétrico o 2-forma. Si se conoce la variación de un campo o una distribución de materia, en el espacio y en el tiempo entonces existen procedimientos para construir su tensor de energía-impulso.

En relatividad, estas magnitudes físicas son representadas por vectores 4-dimensionales o bien por objetos matemáticos llamados tensores, que generalizan los vectores, definidos sobre un espacio de cuatro dimensiones. Matemáticamente estos 4-vectores y 4-tensores son elementos definidos del espacio vectorial tangente al espacio-tiempo (y los tensores se definen y se construyen a partir del fibrado tangente o cotangente de la variedad que representa el espacio-tiempo).

Igualmente además de cuadrivectores, se definen cuadritensores (tensores ordinarios definidos sobre el fibrado tangente del espacio-tiempo concebido como variedad lorentziana). La curvatura del espacio-tiempo se representa por un 4-tensor (tensor de cuarto orden), mientras que la energía y el momento de un medio continuo o el campo electromagnético se representan mediante 2-tensores (simétrico el tensor energía-impulso, antisimétrico el de campo electromagnético). Los cuadrivectores son de hecho 1-tensores, en esta terminología. En este contexto se dice que una magnitud es un invariante relativista si tiene el mismo valor para todos los observadores, obviamente todos los invariantes relativistas son escalares (0-tensores), frecuentemente formados por la contracción de magnitudes tensoriales.

El intervalo relativista puede definirse en cualquier espacio-tiempo, sea este plano como en la relatividad especial, o curvo como en relatividad general. Sin embargo, por simplicidad, discutiremos inicialmente el concepto de intervalo para el caso de un espacio-tiempo plano. El tensor métrico del espacio-tiempo plano de Minkowski se designa con la letra formula_7, y en coordenadas galileanas o inerciales toma la siguiente forma:

El intervalo, la distancia tetradimensional, se representa mediante la expresión formula_8, que se calcula del siguiente modo:

Los intervalos pueden ser clasificados en tres categorías: Intervalos espaciales (cuando formula_9 es negativo), temporales (si formula_9 es positivo) y nulos (cuando formula_11). Como el lector habrá podido comprobar, los intervalos nulos son aquellos que corresponden a partículas que se mueven a la velocidad de la luz, como los fotones: La distancia formula_12 recorrida por el fotón es igual a su velocidad ("c") multiplicada por el tiempo formula_13 y por lo tanto el intervalo formula_14 se hace nulo.

Los intervalos nulos pueden ser representados en forma de cono de luz, popularizados por el celebérrimo libro de Stephen Hawking, "Historia del Tiempo". Sea un observador situado en el origen, el "futuro absoluto" (los sucesos que serán percibidos por el individuo) se despliega en la parte superior del eje de ordenadas, el "pasado absoluto" (los sucesos que ya han sido percibidos por el individuo) en la parte inferior, y el presente percibido por el observador en el punto 0. Los sucesos que están fuera del cono de luz no nos afectan, y por lo tanto se dice de ellos que están situados en zonas del espacio-tiempo que no tienen relación de causalidad con la nuestra.

Imaginemos, por un momento, que en la galaxia Andrómeda, situada a 2,5 millones de años luz de nosotros, sucedió un cataclismo cósmico hace 100 000 años. Dado que, primero: la luz de Andrómeda tarda dos millones de años en llegar hasta nosotros y segundo: nada puede viajar a una velocidad superior a la de los fotones, es evidente, que no tenemos manera de enterarnos de lo que sucedió en dicha Galaxia hace tan solo 100 000 años. Se dice por lo tanto que el intervalo existente entre dicha hipotética catástrofe cósmica y nosotros, observadores del presente, es un intervalo espacial (formula_15), y por lo tanto, no puede afectar a los individuos que en el presente viven en la Tierra: Es decir, no existe relación de causalidad entre ese evento y nosotros.

El único problema con esta hipótesis, es que al entrar en un agujero negro, se anula el espacio tiempo, y como ya sabemos, algo que contenga algún volumen o masa, debe tener como mínimo un espacio donde ubicarse, el tiempo en ese caso, no tiene mayor importancia, pero el espacio juega un rol muy importante en la ubicación de volúmenes, por lo que esto resulta muy improbable, pero no imposible para la tecnología.

Podemos escoger otro episodio histórico todavía más ilustrativo: El de la estrella de Belén, tal y como fue interpretada por Johannes Kepler. Este astrónomo alemán consideraba que dicha estrella se identificaba con una supernova que tuvo lugar el año 5 a. C., cuya luz fue observada por los astrónomos chinos contemporáneos, y que vino precedida en los años anteriores por varias conjunciones planetarias en la constelación de Piscis. Esa supernova probablemente estalló hace miles de años atrás, pero su luz no llegó a la tierra hasta el año 5 a. C. De ahí que el intervalo existente entre dicho evento y las observaciones de los astrónomos egipcios y megalíticos (que tuvieron lugar varios siglos antes de Cristo) sea un "intervalo espacial", pues la radiación de la supernova nunca pudo llegarles. Por el contrario, la explosión de la supernova por un lado, y las observaciones realizadas por los tres magos en Babilonia y por los astrónomos chinos en el año 5 a. C. por el otro, están unidas entre sí por un "intervalo temporal", ya que la luz sí pudo alcanzar a dichos observadores.

El tiempo propio y el intervalo se relacionan mediante la siguiente equivalencia: formula_16, es decir, el intervalo es igual al tiempo local multiplicado por la velocidad de la luz. Una de las características tanto del tiempo local como del intervalo es su invarianza ante las transformaciones de coordenadas. Sea cual sea nuestro punto de referencia, sea cual sea nuestra velocidad, el intervalo entre un determinado evento y nosotros permanece invariante.

Esta invarianza se expresa a través de la llamada geometría hiperbólica: La ecuación del intervalo formula_17 tiene la estructura de una hipérbola sobre cuatro dimensiones, cuyo "término independiente" coincide con el valor del cuadrado del intervalo (formula_18), que como se acaba de decir en el párrafo anterior, es constante. Las "asíntotas" de la hipérbola vendrían a coincidir con el cono de luz.

En el espacio-tiempo de Minkowski, las propiedades cinemáticas de las partículas se representan fundamentalmente por tres magnitudes: La cuadrivelocidad (o tetravelocidad) , la cuadriaceleración y el cuadrimomentum (o tetramomentum).

La cuadrivelocidad es un cuadrivector tangente a la línea de universo de la partícula, relacionada con la velocidad coordenada de un cuerpo medida por un observador en reposo cualquiera, esta velocidad coordenada se define con la expresión newtoniana formula_19, donde formula_20 son el tiempo coordenado y las coordenadas espaciales medidas por el observador, para el cual la velocidad newtoniana ampliada vendría dada por formula_21. Sin embargo, esta medida newtoniana de la velocidad no resulta útil en teoría de la relatividad, porque las velocidades newtonianas medidas por diferentes observadores no son fácilmente relacionables por no ser magnitudes covariantes. Así en relatividad se introduce una modificación en las expresiones que dan cuenta de la velocidad, introduciendo un invariante relativista. Este invariante es precisamente el tiempo propio de la partícula que es fácilmente relacionable con el tiempo coordenado de diferentes observadores. Usando la relación entre tiempo propio y tiempo coordenado: formula_22 se define la cuadrivelocidad [propia] multiplicando por formula_23 las de la velocidad coordenada: formula_24.

La velocidad coordenada de un cuerpo con masa depende caprichosamente del sistema de referencia que escojamos, mientras que la cuadrivelocidad propia es una magnitud que se transforma de acuerdo con el principio de covariancia y tiene un valor siempre constante equivalente al intervalo dividido entre el tiempo propio (formula_25), o lo que es lo mismo, a la velocidad de la luz "c". Para partículas sin masa, como los fotones, el procedimiento anterior no se puede aplicar, y la cuadrivelocidad puede definirse simplemente como vector tangente a la trayectoria seguida por los mismos.

La cuadriaceleración puede ser definida como la derivada temporal de la cuadrivelocidad (formula_26). Su magnitud es igual a cero en los sistemas inerciales, cuyas líneas del mundo son geodésicas, rectas en el espacio-tiempo llano de Minkowski. Por el contrario, las líneas del mundo curvadas corresponden a partículas con aceleración diferente de cero, a sistemas no inerciales.

Junto con los principios de invarianza del intervalo y la cuadrivelocidad, juega un papel fundamental la ley de conservación del cuadrimomentum. Es aplicable aquí la definición newtoniana del momentum (formula_27) como la masa (en este caso conservada, formula_28) multiplicada por la velocidad (en este caso, la cuadrivelocidad), y por lo tanto sus componentes son los siguientes: formula_29, teniendo en cuenta que formula_30. La cantidad de momentum conservado es definida como la raíz cuadrada de la norma del vector de cuadrimomentum. El momentum conservado, al igual que el intervalo y la cuadrivelocidad propia, "permanece invariante ante las transformaciones de coordenadas", aunque también aquí hay que distinguir entre los cuerpos con masa y los fotones. En los primeros, la magnitud del cuadriomentum es igual a la "masa multiplicada por la velocidad de la luz" (formula_31). Por el contrario, el cuadrimomentum conservado de los fotones es igual a la magnitud de su "momentum tridimensional" (formula_32).

Como tanto la velocidad de la luz como el cuadrimomentum son magnitudes conservadas, también lo es su producto, al que se le da el nombre de energía conservada (formula_33), que en los cuerpos con masa equivale a la masa multiplicada por la velocidad de la luz al cuadrado (formula_34, la famosa fórmula de Einstein) y en los fotones al momentum multiplicado por la velocidad de la luz (formula_35)
Componentes formula_36
Magnitud del cuadrimomentum formula_37

Magnitud en cuerpos con masa formula_38
Magnitud en fotones (masa = 0) formula_39
Energía formula_40

Energía en cuerpos con masa (cuerpos en reposo, p=0) formula_41
Energía en fotones (masa en reposo = 0) formula_42
La aparición de la Relatividad Especial puso fin a la secular disputa que mantenían en el seno de la mecánica clásica las escuelas de los mecanicistas y los energetistas. Los primeros sostenían, siguiendo a Descartes y Huygens, que la magnitud conservada en todo movimiento venía constituida por el momentum total del sistema, mientras que los energetistas -que tomaban por base los estudios de Leibniz- consideraban que la magnitud conservada venía conformada por la suma de dos cantidades: La "fuerza viva", equivalente a la mitad de la masa multiplicada por la velocidad al cuadrado (formula_43) a la que hoy denominaríamos "energía cinética", y la "fuerza muerta", equivalente a la altura por la constante "g" (formula_44), que correspondería a la "energía potencial". Fue el físico alemán Hermann von Helmholtz el que primero dio a la "fuerzas leibnizianas" la denominación genérica de energía y el que formuló la "Ley de conservación de la energía", que no se restringe a la mecánica, que se extiende también a otras disciplinas físicas como la termodinámica.

La mecánica newtoniana dio la razón a ambos postulados, afirmando que tanto el momentum como la energía son magnitudes conservadas en todo movimiento sometido a fuerzas conservativas. Sin embargo, la Relatividad Especial dio un paso más allá, por cuanto a partir de los trabajos de Einstein y Minkowski el momentum y la energía dejaron de ser considerados como entidades independientes y se les pasó a considerar como dos aspectos, dos facetas de una única magnitud conservada: el cuadrimomentum.

Tres son las ecuaciones fundamentales que en física newtoniana describen el fenómeno de la "gravitación universal": la primera, afirma que la fuerza gravitatoria entre dos cuerpos es proporcional al producto de sus masas e inversamente proporcional al cuadrado de su distancia (1); la segunda, que el potencial gravitatorio (formula_45) en un determinado punto es igual a la masa multiplicada por la constante "G" y dividida por la distancia "r" (2); y la tercera, finalmente, es la llamada ecuación de Poisson (3), que indica que el laplaciano del potencial gravitatorio es igual a formula_46, donde formula_47 es la densidad de masa en una determinada región esférica.

Sin embargo, estas ecuaciones no son compatibles con la Relatividad Especial por dos razones:

Por todo ello, resulta necesario prescindir del término formula_47, situado en el lado derecho de la fórmula de Poisson y sustituirlo por un objeto geométrico-matemático que permanezca invariante ante las transformaciones de Lorentz: Dicho objeto fue definido por Einstein en sus ecuaciones de universo y recibe el nombre de "tensor de energía-momentum" (formula_50). Sus coeficientes describen la cantidad de tetramomentum formula_51 que atraviesa una hipersuperficie formula_52, normal al vector unitario formula_53. De este modo, el tensor de energía momentum puede expresarse mediante la siguiente ecuación:

O lo que es lo mismo: El componente formula_51 del tetramomentum es igual a la integral de hipersuperficie formula_55 del tensor de tensión-energía. En un fluido ideal, del que están ausentes tanto la viscosidad como la conducción de calor, los componentes del tetramomentum se calculan de la siguiente forma:

donde formula_47 es la densidad de masa-energía (masa por unidad de volumen tridimensional), formula_57 es la presión hidrostática, formula_58 es la cuadrivelocidad del fluido, y formula_59 es la matriz inversa del tensor métrico de la variedad.

Además, si los componentes del tensor se miden por un observador en reposo relativo respecto al fluido, entonces, el tensor métrico viene constituido simplemente por la métrica de Minkowski:

Puesto que además la tetravelocidad del fluido respecto al observador en reposo es:

como consecuencia de ello, los coeficientes del tensor de tensión-energía son los siguientes:

Donde formula_47 es la densidad de masa, y formula_61 son los componentes tridimensionales de la presión hidrostática. Como vemos, el campo gravitatorio tiene dos fuentes diferentes: La masa y el momentum del fluido en cuestión. Los efectos gravitatorios originados por la masa se denominan efectos gravitoeléctricos, mientras que aquellos que se deben al momentum reciben el nombre de efectos gravitomagnéticos. Los primeros tienen una intensidad formula_62 superior a los segundos, que solo se manifiestan en aquellos casos en los que las partículas del fluido se mueven con una velocidad cercana a la de la luz (se habla entonces de "fluidos relativistas"): Es el caso de los chorros ("jets") que emanan del centro de la galaxia y que se propulsan en las dos direcciones marcadas por el eje de rotación de este cuerpo cósmico; de la materia que se precipita hacia un agujero negro; y del fluido estelar que se dirige hacia el centro de la estrella cuando esta entra en colapso. En este último caso, durante las fases finales del proceso de contracción de la estrella, la presión hidrostática puede llegar a ser tan fuerte como para llegar a acelerar el colapso, en lugar de ralentizarlo.

Podemos, a partir del tensor de tensión-energía, calcular cuánta masa contiene un determinado volumen del fluido: Retomando la definición de este tensor expuesta unas líneas más arriba, se puede definir al coeficiente formula_63 como la cantidad de momentum formula_64 (esto es, la masa) que atraviesa la hipersuperficie formula_65. En el espacio-tiempo de Minkowski, la hipersuperficie formula_65 es aquella región que se define por las tres bases vectoriales normales al vector formula_67: formula_68 es, por tanto, un volumen tridimensional, definido por los vectores base formula_69 (eje "x"), formula_70 (eje "y"), y formula_71 (eje "z"). Podemos por tanto escribir:

Del mismo modo, es posible deducir matemáticamente a partir del tensor de tensión-energía la definición newtoniana de presión, introduciendo en la mentada ecuación cualquier par de índices que sean diferentes de cero:

La hipersuperficie formula_75 es aquella región del espacio-tiempo definida por los tres vectores unitarios normales a formula_76 (se trata de los dos vectores espaciales, formula_70 y formula_78, correspondientes a los ejes "y" y "z"; y del vector temporal formula_79 —o formula_80, como se prefiera—). Esta definición nos permite descomponer la integral de hipersuperficie en una integral temporal (cuyo integrando viene definido por formula_80) y otra de superficie (esta vez bidimensional, formula_82):

Finalmente, derivamos parcialmente ambos miembros de la ecuación respecto al tiempo, y teniendo en cuenta que la fuerza no es más que la tasa de incremento temporal del momentum obtenemos el resultado siguiente:

Que contiene la definición newtoniana de la presión como fuerza ejercida por unidad de superficie.

Las ecuaciones deducidas por el físico escocés James Clerk Maxwell demostraron que electricidad y magnetismo no son más que dos manifestaciones de un mismo fenómeno físico: el campo electromagnético. Ahora bien, para describir las propiedades de este campo los físicos de finales del siglo XIX debían utilizar dos vectores diferentes, los correspondientes los campos eléctrico y magnético.

Fue la llegada de la Relatividad Especial la que permitió describir las propiedades del electromagnetismo con un solo objeto geométrico, el "vector cuadripotencial", cuyo componente temporal se correspondía con el potencial eléctrico, mientras que sus componentes espaciales eran los mismos que los del potencial magnético.

De este modo, el campo eléctrico puede ser entendido como la suma del gradiente del potencial eléctrico más la derivada temporal del potencial magnético:

y el campo magnético, como el rotacional del potencial magnético:

Las propiedades del campo electromagnético pueden también expresarse utilizando un tensor de segundo orden denominado tensor de Faraday y que se obtiene diferenciando exteriormente al vector cuadripotencial formula_88

formula_90

La fuerza de Lorentz puede deducirse a partir de la siguiente expresión:

Donde "q" es la carga y formula_93 la cuadrivelocidad de la partícula.





</doc>
<doc id="3246" url="https://es.wikipedia.org/wiki?curid=3246" title="Historia de la biología">
Historia de la biología

La historia de la biología narra y analiza la historia del estudio de los seres vivos, desde la Antigüedad hasta la época actual. Aunque la biología moderna es un desarrollo relativamente reciente (siglo XIX), las ciencias relacionadas e incluidas en ella se han estudiado como filosofía natural desde la antigüedad —antiguas civilizaciones de Mesopotamia, Egipto, subcontinente indio, China—, pero los orígenes de la biología moderna y su enfoque del estudio de la naturaleza se quieren originados en la antigua Grecia. Si bien el estudio formal de la medicina se remonta al Egipto faraónico —ver: "Āyurveda" y medicina en el Antiguo Egipto—, fue Aristóteles (384-) quien contribuyó más ampliamente al desarrollo de la biología. Especialmente importantes son su "Historia de los animales" y otras obras donde mostró inclinaciones naturalistas, y luego obras más empíricas que se enfocaron en la causalidad biológica y la diversidad de la vida. El sucesor de Aristóteles en el Liceo, Teofrasto, escribió una serie de libros sobre botánica ("De historia plantarum") que sobrevivieron como la contribución más importante de la antigüedad a las ciencias de las plantas, incluso hasta la Edad Media.

La decadencia del Imperio romano llevó a la desaparición o la destrucción de gran cantidad de conocimiento, aunque los médicos todavía conservaron la tradición griega en formación y práctica. En Bizancio y el mundo islámico, muchos de los trabajos griegos fueron traducidos al árabe y muchos de los trabajos de Aristóteles fueron preservados. La historia natural se basó en gran medida en el pensamiento aristotélico, especialmente en la defensa de una jerarquía de vida fija, destacando la obra de algunos eruditos que escribieron sobre biología, como al-Jahiz (781-869), Al-Dīnawarī (828-896), que escribió sobre botánica, y Rhazes (865-925) que escribió sobre anatomía y fisiología. Avicena (980-1037)fue el gran médico que continuo las tradiciones grecorromanas e introdujo los ensayos clínicos y la farmacología clínica en su enciclopedia "El canon de medicina", que se utilizó como texto de referencia para la enseñanza médica europea hasta el siglo XVII.

Durante el Renacimiento y principios de la Edad Moderna —beneficiándose del desarrollo de la impresión por Gutenberg alrededor de 1450, con la creciente impresión de libros dedicados a la historia natural profusamente ilustrados con grabados— el pensamiento biológico experimentó una revolución en Europa, con un renovado interés hacia el empirismo y por el descubrimiento de gran cantidad de nuevos organismos. Figuras prominentes de este movimiento fueron Vesalio y Harvey, que utilizaron la experimentación y la observación cuidadosa de la fisiología. Pero la biología comenzó a desarrollarse y crecer rápidamente con la espectacular mejora del microscopio de Anton van Leeuwenhoek. Fue entonces cuando los estudiosos descubrieron los espermatozoides, las bacterias, los infusorios y la diversidad de la vida microscópica, todo un mundo antes desconocido. Las investigaciones de Jan Swammerdam llevaron a un nuevo interés en la entomología y ayudaron a desarrollar las técnicas básicas de disección microscópica y tinción.
Los avances en microscopía también tuvieron un profundo impacto en el pensamiento biológico. A principios del siglo XIX, varios biólogos señalaron la importancia central de la célula. Luego, en 1838, Schleiden y Schwann comenzaron a promover las ideas ahora universales de que (1) la unidad básica de los organismos era la célula y (2) que las células individuales tenían todas las características de la vida, aunque se oponían a la idea de que (3) todos las células proviniesen de la división de otras células. Sin embargo, gracias al trabajo de Robert Remak y Rudolf Virchow, en la década de 1860 la mayoría de los biólogos ya aceptaban los tres principios de lo que llegó a conocerse como teoría celular, que proporcionaba una nueva perspectiva sobre los fundamentos de la vida.

A lo largo de los siglos XVIII y XIX algunas ciencias biológicas, como la botánica y la zoología, se convirtieron en disciplinas científicas cada vez más profesionales. Lavoisier y otros científicos físicos comenzaron a unir los mundos animados e inanimados a través de la física y química. Los exploradores-naturalistas, como Alexander von Humboldt investigaron la interacción entre organismos y su entorno, y los modos en que esta relación depende de la situación geográfica, iniciando así la biogeografía, la ecología y la etología. Los naturalistas, a partir de los resultados obtenidos en los campos de la embriología y la paleontología, comenzaron a rechazar el esencialismo y a considerar la importancia de la extinción y la mutabilidad de las especies. La importancia creciente de la teología natural, en parte una respuesta al alza de la filosofía mecánica, y la pérdida de fuerza del argumento teleológico impulsó el crecimiento de la historia natural.
Mientras tanto, la taxonomía y la clasificación de la diversidad de la vida y el registro fósil se convirtieron en el centro de atención de los historiadores naturales, así como el desarrollo y el comportamiento de los organismos. Carl Linnaeus publicó una taxonomía básica para el mundo natural en 1735 (cuyas variaciones se han utilizado desde entonces), y en la década de 1750 introdujo nombres científicos para todas sus especies. Georges-Louis Leclerc, conde de Buffon, trató las especies como categorías artificiales y las formas vivas como maleables, sugiriendo incluso la posibilidad de una descendencia común. Aunque se opuso a la evolución, Buffon es una figura clave en la historia del pensamiento evolucionista; su trabajo influyó en las teorías evolutivas tanto de Lamarck como de Darwin.

El pensamiento evolutivo serio se originó con las obras de Jean-Baptiste Lamarck, quien fue el primero en presentar una teoría coherente de la evolución. Postuló que la evolución era el resultado del estrés ambiental sobre las propiedades de los animales, lo que significaba que cuanto más frecuente y rigurosamente se usaba un órgano, más complejo y eficiente se volvería, adaptando así al animal a su entorno. Lamarck creía que estos rasgos adquiridos podrían luego transmitirse a la descendencia del animal, que los desarrollaría y perfeccionaría aún más. Sin embargo, fue el naturalista británico Charles Darwin, que combinando el enfoque biogeográfico de Humboldt, la geología uniformista de Lyell, los escritos de Malthus sobre el crecimiento de la población y su propia experiencia morfológica y extensas observaciones naturales, quien forjó una teoría evolutiva más exitosa basada en la selección natural; un razonamiento y pruebas similares llevaron a Alfred Russel Wallace a llegar de forma independiente a las mismas conclusiones. Aunque fue objeto de controversia (que continúa hasta el día de hoy), la teoría de Darwin se extendió rápidamente a través de la comunidad científica y pronto se convirtió en un axioma central de la ciencia de la biología en rápido desarrollo. El final del siglo XIX vio la caída de la teoría de la generación espontánea y el nacimiento de la teoría microbiana de la enfermedad, aunque el mecanismo de la herencia genética fuera todavía un misterio.

A principios del siglo XX, el redescubrimiento del trabajo de Mendel sobre la representación física de la herencia condujo al rápido desarrollo de la genética por parte de Thomas Hunt Morgan y sus discípulos y la combinación de la genética de poblaciones y la selección natural en la síntesis evolutiva moderna durante los años 1930. En la década de 1940 y principios de la de 1950, los experimentos señalaron que el ADN era el componente de los cromosomas que contenía las unidades portadoras de rasgos que se conoceran como genes. Un enfoque en nuevos tipos de organismos modelo como virus y bacterias, junto con el descubrimiento de Watson y Crick de la estructura de doble hélice del ADN en 1953, marcó la transición a la era de la genética molecular. Desde la década de 1950 hasta la actualidad, la biología se ha extendido enormemente en el dominio molecular. El código genético fue descifrado por Har Gobind Khorana, Robert W. Holley y Marshall Warren Nirenberg después de que se entendiera que el ADN contenía codones. Finalmente, en 1990 se lanzó el Proyecto Genoma Humano con el objetivo de mapear el genoma humano general. Este proyecto se completó esencialmente en 2003, y aún se están publicando análisis adicionales. El Proyecto Genoma Humano fue el primer paso en un esfuerzo globalizado para incorporar el conocimiento acumulado de la biología en una definición funcional y molecular del cuerpo humano y de los cuerpos de otros organismos.

La biología, que tras el establecimiento del dogma central de la biología molecular y del descifrado del código genético, se había dividido fundamentalmente entre la biología orgánica —los campos que trabajan con organismos completos y grupos de organismos— y los campos relacionados con la biología molecular y celular, a finales del siglo XX, con la aparición de nuevos campos como la genómica y la proteómica, invertía esa tendencia, con biólogos orgánicos usando técnicas moleculares, y biólogos moleculares y celulares investigando la interacción entre genes y el entorno, así como la genética de poblaciones naturales de organismos.

La palabra biología está formada por la combinación de los términos griegos βίος "bios", vida, y el sufijo -λογία , ciencia, tratado, estudio, basado en el verbo griego λέγειν ("legein"), seleccionar, reunir ("cf." el nombre λόγος "logos", palabra). El término «biología» en su sentido actual se cree que fue introducido de forma independiente por Karl Friedrich Burdach (en 1800), Gottfried Reinhold Treviranus ("Biologie oder Philosophie der lebenden Natur", 1802) y Jean-Baptiste Lamarck ("Hydrogéologie", 1802). La palabra en sí misma ya aparece en el título del volumen 3 de "Philosophiae naturalis sive physicae dogmaticae": «Geologia, biologia, phytologia generalis et dendrologia», de Michael Christoph Hanow, publicado en 1766.

Con anterioridad se utilizaron distintos términos para el estudio de animales y plantas. "Historia natural" se utilizó para referirse a los aspectos descriptivos de la biología, aunque también incluía la mineralogía y otros campos no biológicos; de la Edad Media al Renacimiento, el marco de unificación de la historia natural era la "scala naturae" o cadena de los seres. Filosofía natural y teología natural englobaban la base conceptual y metafísica de planta y vida animal, tratando con problemas como por qué los organismos existen y se comportan del modo en que lo hacen, aunque estas materias también incluían lo que es en la actualidad la geología, la física, la química y la astronomía. La fisiología y la farmacología botánica eran de la incumbencia de la medicina. "Botánica", "zoología" y (en el caso de los fósiles) "geología" sustituyeron a la historia natural y a la filosofía natural en los siglos XVIII y XIX antes de que "biología" se adoptara mayoritariamente.En la actualidad "botánica" y "zoología" son términos utilizados de forma generalizada, aunque se les han añadido otras subdisciplinas de la biología, como la micología y la biología molecular.

Los primeros humanos deben haber tenido y transmitido el conocimiento sobre plantas y animales para aumentar sus posibilidades de supervivencia y probablemente tendrían también conocimientos sobre anatomía humana y animal y sobre algunos aspectos del comportamiento animal (como modelos de migración). Sin embargo, el primer paso decisivo en el conocimiento biológico vino con la revolución neolítica hace aproximadamente 10 000 años. Los humanos primero cultivaron plantas para la agricultura y posteriormente animales como ganado para acompañar a las sociedades sedentarias resultantes.

Las antiguas culturas de Mesopotamia, Egipto, el subcontinente indio y China, entre otras, dieron pie al nacimiento de renombrados cirujanos y estudiosos de las ciencias naturales como Sushruta o Zhang Zhong Jing, que reflejaron sofisticados sistemas independientes de la filosofía natural. Sin embargo, generalmente las raíces de la biología moderna se remontan a la tradición secular de la filosofía griega antigua.

Uno de los sistemas organizados más antiguos de la medicina se sitúa en el subcontinente indio en la forma del Āyurveda, proveniente del "Átharva Vedá" (uno de los cuatro libros más antiguos de conocimiento y cultura india) alrededor del 1500 a. C. Otros textos médicos antiguos surgen del Antiguo Egipto, como el papiro Edwin Smith; esta cultura también es conocida por desarrollar el proceso de embalsamamiento, que se utilizaba para la momificación, a fin de conservar el cuerpo humano y prevenir la descomposición. En la antigua China se pueden encontrar temas biológicos dispersos a través de varias disciplinas diferentes, como los trabajos de herbólogos, médicos, alquimistas y filósofos. La tradición taoísta de la alquimia china, por ejemplo, puede considerarse parte de las ciencias de la vida debido a su énfasis en la salud (con el objetivo último de obtener el «elixir de la vida»). El sistema de la medicina china clásica por lo general giraba en torno a la teoría del yin y yang y de los cinco elementos. Los filósofos taoístas, como Zhuangzi en el siglo IV a. C., también expresan ideas relacionadas con la evolución, como negar la persistencia o continuidad de las especies biológicas y especulando que las especies habían desarrollado atributos diferenciadores en respuesta a distintos ambientes.

La antigua tradición india del Ayurveda desarrolló independientemente el concepto de los tres humores, que se asemejaba al de los cuatro humores de la medicina en la Antigua Grecia, aunque el sistema ayurvédico incluía complejidades adicionales, como que el cuerpo estaba formado por cinco elementos y siete tejidos básicos. Los escritores de esta tradición también clasificaron a las criaturas en cuatro categorías basadas en el método utilizado para su nacimiento (útero, huevo, calor/humedad y semilla) y explicaron la concepción de un feto de forma detallada; también progresaron en el campo de cirugía, a menudo sin la utilización de la disección de humanos o la vivisección de animales. Uno de los tratados ayurvédicos más antiguos fue el "Sushruta Samhita", atribuido a Sushruta, en el siglo VI a. C., que también fue una temprana farmacopea y describía 700 plantas medicinales, 64 preparaciones de fuentes minerales y 57 preparaciones de origen animal.

Los filósofos presocráticos se hicieron muchas preguntas sobre la vida, si bien produjeron poco conocimiento sistemático en torno a temas específicamente biológicos; no obstante, los intentos de los atomistas para explicar la vida en términos puramente físicos aparecerán recurrentemente a lo largo de toda la historia de la biología. Sin embargo, las teorías médicas de Hipócrates y sus discípulos, especialmente el humorismo, tuvieron un gran impacto.

El filósofo Aristóteles fue el estudioso del mundo orgánico más influyente de la Antigüedad. Aunque sus primeros trabajos en la filosofía natural fueron especulativos, las escrituras biológicas posteriores de Aristóteles eran más empíricas, centrándose en la causalidad biológica y la diversidad de la vida. Hizo innumerables observaciones de la naturaleza, sobre todo sobre los hábitos y los atributos de las plantas y animales de su alrededor, con una especial atención a la categorización. En total Aristóteles clasificó 540 especies de animales y diseccionó al menos 50. Creía que los objetivos intelectuales y las causas formales dirigían todos los procesos naturales. (Véase: Biología de Aristóteles)

Aristóteles y casi todos los eruditos occidentales posteriores a él hasta el siglo XVIII, creían que las criaturas se organizaban en una escala graduada de perfección que se eleva desde las plantas hasta los humanos: la "scala naturae" (escala natural) o cadena de los seres. El sucesor de Aristóteles en el Liceo, Teofrasto, escribió una serie de libros sobre la botánica ("De historia plantarum"), que sobrevivió como la contribución más importante de la Antigüedad a la botánica hasta la Edad Media. Muchos de los nombres de Teofrasto sobreviven en la actualidad, como "carpos" para la fruta, y "pericarpio" para la parte del fruto que recubre su semilla. Dioscórides escribió una pionera farmacopea enciclopédica, "De materia medica", que incorporaba descripciones de unas 600 plantas y sus usos en la medicina. Plinio el Viejo también fue reconocido por su conocimiento de las plantas y la naturaleza con obras como "Naturalis historia", y fue un prolífico compilador de descripciones zoológicas.

Algunos eruditos del período helenístico bajo la Dinastía Ptolemaica (en especial Herófilo de Calcedonia y Erasístrato) corrigieron el trabajo fisiológico de Aristóteles, realizando incluso disecciones y vivisecciones. Galeno de Pérgamo se convirtió en la autoridad más importante en medicina y anatomía. Aunque algunos atomistas antiguos como Lucrecio desafiaran el punto de vista teleológico aristotélico de que todos los aspectos de la vida son el resultado de un diseño u objetivo, la teleología y la teología natural permanecerían en el centro del pensamiento biológico hasta los siglos XVIII y XIX. Ernst Mayr manifestó que «Nada realmente importante pasó en la biología después de Lucrecio y Galeno hasta el Renacimiento». Las ideas de las tradiciones griegas sobre la historia natural y la medicina sobrevivieron, y por lo general no fueron cuestionadas en la Europa medieval.

La decadencia del Imperio romano llevó a la desaparición o la destrucción de gran cantidad de conocimiento, aunque los médicos todavía incorporaban muchos aspectos de la tradición griega en formación y práctica. En Bizancio y el mundo islámico, muchos de los trabajos griegos fueron traducidos al árabe y muchos de los trabajos de Aristóteles fueron preservados.

Los médicos, los científicos y los filósofos musulmanes medievales hicieron contribuciones significativas al conocimiento biológico entre los siglos VIII y XIII, durante lo que se conoce como la «Edad de Oro del islam». En zoología, por ejemplo, el erudito afroárabe Al-Jahiz (781-869) describió algunas de las primeras ideas evolutivas, como la lucha por la existencia. También introdujo la idea de una cadena alimentaria, y fue un temprano partidario del determinismo geográfico. El biólogo kurdo Al-Dinawari (828-896) está considerado el fundador de la botánica árabe por su "Libro de las plantas", en el que describió al menos 637 especies y trató sobre el desarrollo de las plantas desde la germinación hasta la muerte, describiendo las fases de su crecimiento y la producción de flores y frutos. Al-Biruni describió el concepto de la selección artificial y sostuvo que la naturaleza trabaja más o menos de la misma forma, una idea que ha sido comparada con la selección natural.

En medicina experimental, el médico persa Avicena (980-1037) introdujo los ensayos clínicos y la farmacología clínica en su enciclopedia "El canon de medicina", que se utilizó como texto de referencia para la enseñanza médica europea hasta el siglo XVII. El médico andalusí Avenzoar (1091-1161) fue un temprano partidario de la disección experimental y la autopsia, que utilizó para demostrar que la enfermedad de la piel conocida como sarna era causada por un parásito, un descubrimiento que desestabilizaba la teoría del humorismo. También introdujo la cirugía experimental, y utilizó la experimentación con animales para probar técnicas quirúrgicas antes de su utilización con humanos. Durante una hambruna en Egipto en 1200, Abd al-Latif al-Baghdadi observó y examinó un gran número de esqueletos, y descubrió que Galeno había hecho una descripción incorrecta de la formación de los huesos de la mandíbula y el sacro.

A principios del siglo XIII el biólogo andalusí Abu al-Abbas al-Nabati fue uno de los primeros en utilizar el método científico en la botánica, introduciendo técnicas empíricas y experimentales en las pruebas, descripción e identificación de elementos de farmacopea, y separación de informes no verificados de aquellos apoyados por pruebas y observaciones. Su alumno Ibn al-Baitar (1190?-1248) escribió una enciclopedia farmacéutica que describía 1400 plantas, alimentos y medicinas, 300 de las cuales eran descubrimientos realizados por él mismo; una traducción al latín de su trabajo fue utilizada por biólogos y farmacéuticos europeos durante los siglos XVIII y XIX.

El médico árabe Ibn Nafis (1213-1288) fue otro de los primeros partidarios de la disección experimental y la autopsia, quien en 1242 descubrió la circulación pulmonar y la circulación coronaria, que forman la base del sistema circulatorio; también describió el concepto de metabolismo, pulso, huesos, músculos, intestinos, órganos sensoriales, bilis, esófago y estómago.

Durante la Alta Edad Media algunos eruditos europeos, como Hildegarda de Bingen, Alberto Magno y Federico II, ampliaron el catálogo de la historia natural. El nacimiento de las universidades europeas, aunque importante para el desarrollo de la física y la filosofía, tuvo poco impacto en el estudio de la biología.

El Renacimiento europeo trajo consigo un nuevo interés por la historia natural y la fisiología empíricas. En 1543 Andrés Vesalio iniciaba una nueva era en la medicina occidental con la publicación de su seminal tratado de anatomía humana "De humani corporis fabrica", que estaba basado en la disección de cadáveres. Vesalio fue el primero de una serie de anatomistas que gradualmente reemplazó la escolástica por el empirismo en la fisiología y la medicina, basándose en la experiencia propia y no en la autoridad y el razonamiento abstracto. A través del herbalismo, la medicina se convirtió en una fuente indirecta para el estudio empírico de las plantas. Otto Brunfels, Hieronymus Tragus y Leonhart Fuchs fueron prolíficos escritores sobre plantas silvestres, el principio de un acercamiento basado en la naturaleza a la gran variedad de la vida vegetal. Los bestiarios, un género que combinaba el conocimiento natural y figurativo sobre los animales, también se hicieron más sofisticados, especialmente gracias al trabajo de William Turner, Pierre Belon, Guillaume Rondelet, Conrad von Gesner y Ulisse Aldrovandi.

Artistas como Alberto Durero y Leonardo da Vinci, que a menudo trabajaron con naturalistas, también estuvieron interesados en el cuerpo de animales y humanos, estudiando la fisiología en detalle y contribuyendo así al progreso del conocimiento anatómico. La alquimia, especialmente en la obra de Paracelso, también contribuyó al conocimiento de los seres vivos; los alquimistas sometieron la materia orgánica al análisis químico y experimentaron profusamente tanto con la farmacología biológica como mineral. Estos estudios formaban parte de una transición más importante en la visión del mundo (el nacimiento de la filosofía mecánica) que continuó hasta el siglo XVII, cuando la metáfora tradicional de la «naturaleza como organismo» fue remplazada por la «naturaleza como máquina».

A principios del siglo XVII, el micromundo de la biología comenzaba a ampliarse. Algunos fabricantes de lentes y filósofos naturales habían estado creando rudimentarios microscopios desde finales del siglo XVI, y Robert Hooke publicó el seminal "Micrographia" basado en observaciones realizadas con su propio microscopio realizado en 1665. Pero no fue hasta las significativas mejoras en la fabricación de lentes introducidas por Anton van Leeuwenhoek a finales de los años 1670 (que consiguieron una ampliación de 200 aumentos de con una única lente), cuando los eruditos descubrieron los espermatozoides, las bacterias, los infusorios y la compleja diversidad de la vida microscópica. Investigaciones similares por parte de Jan Swammerdam conllevaron un nuevo interés hacia la entomología y establecieron las técnicas básicas de la disección microscópica y la tinción.

Mientras que el mundo microscópico se ampliaba, el mundo macroscópico se reducía. Botánicos como John Ray trabajaron para incluir la avalancha de nuevos organismos recién descubiertos provenientes de todo el globo en una taxonomía coherente y en una teología racional. El debate sobre el Diluvio universal catalizó el desarrollo de la paleontología; en 1669 Niels Stensen publicó un ensayo sobre como los restos de organismos vivos podrían quedar atrapados en capas de sedimento y mineralizarse para producir fósiles. Aunque las ideas de Stensen sobre la fosilización fueran conocidas y ampliamente debatidas entre filósofos naturales, un origen orgánico de los fósiles no sería aceptado por todos los naturalistas hasta finales del siglo XVIII debido al debate filosófico y teológico sobre cuestiones como la edad de la Tierra y la extinción.

La sistematización, descripción y clasificación dominó la historia natural a lo largo de la mayor parte de los siglos XVII y XVIII. Carlos Linneo publicó una taxonomía básica para el mundo natural en 1736 (variaciones de la misma se han seguido utilizando hasta la actualidad), y en los años 1750 introdujo la nomenclatura binominal para todas sus especies. Mientras que Linneo concebía las especies como partes invariables de una jerarquía diseñada, el otro gran naturalista del siglo XVIII, Georges Louis Leclerc, conde de Buffon, trató a las especies como categorías artificiales y a las formas vivas como maleables (incluso la posibilidad de un origen común). Aunque estaba en contra de la evolución, Buffon fue una figura clave en la historia del pensamiento evolutivo; su trabajo influiría en las teorías evolutivas tanto de Lamarck como de Darwin.
El descubrimiento y la descripción de nuevas especies y la recogida de especímenes se convirtieron en una pasión de caballeros científicos y un lucrativo negocio para empresarios; muchos naturalistas viajaron por todo el mundo en busca de conocimiento científico y aventuras.

Ampliando el trabajo de Vesalio en experimentos en cuerpos todavía vivos (tanto de personas como de animales), William Harvey y otros filósofos naturales investigaron el papel de la sangre, las venas y las arterias. En 1628 el "Exercitatio anatomica de motu cordis et sanguinis in animalibus" (Ejercicio anatómico sobre el movimiento del corazón y de la sangre en animales) de Harvey fue el principio del fin para la teoría galénica, que junto a los estudios sobre el metabolismo de Santorio Santorio, sirvió como modelo de acercamiento cuantitativo a fisiología.

Durante el siglo XIX, el ámbito de biología estaba dividido fundamentalmente entre la medicina, que investigaba sobre cuestiones de forma y función, e historia natural, que estudiaba la diversidad de la vida y las interacciones entre distintas formas de vida y entre la vida y la no vida. Hacia 1900, la mayor parte de estas áreas se superpuso, mientras la historia natural (y su equivalente filosofía natural) había cedido el paso en gran parte a disciplinas científicas especializadas, como la bacteriología, la morfología, la embriología, la geografía y la geología.

Los numerosos viajes emprendidos por naturalistas a principios y mediados del siglo XIX produjeron una gran cantidad de información novedosa sobre la diversidad y la distribución de los organismos vivos. De particular importancia fue el trabajo de Alexander von Humboldt, que analizó la relación entre organismos y su ambiente (el campo de la historia natural) utilizando los métodos cuantitativos de la filosofía natural (es decir, física y química). El trabajo de Humboldt estableció las bases de la biogeografía e inspiró a varias generaciones de científicos.

La emergente disciplina de la geología acercó a la historia natural y a la filosofía natural; el establecimiento de la columna estratigráfica unió la distribución espacial de los organismos a su distribución temporal, un precursor clave para la noción de la evolución. Georges Cuvier y otros dieron un gran paso en anatomía comparada y paleontología a finales de los años 1790 y principios de los años 1800. En una serie de conferencias y ensayos que hacían comparaciones detalladas entre mamíferos vivientes y fósiles, Cuvier fue capaz de establecer que los fósiles eran restos de especies que se habían extinguido, en lugar de corresponder a restos de especies todavía vivas en otras partes del mundo, tal como se creía por entonces. Los fósiles descubiertos y descritos por Gideon Mantell, William Buckland, Mary Anning y Richard Owen, entre otros, ayudaron a establecer que existió una «edad de los reptiles» y que éstos habían precedido incluso a los mamíferos prehistóricos. Estos descubrimientos captaron el interés público y dirigieron la atención hacia la historia de la vida en la Tierra. La mayor parte de estos geólogos sostenían la teoría del catastrofismo, pero el influyente "Principles of Geology" (1830) de Charles Lyell popularizó el uniformismo de Hutton, una teoría que explicaba en igualdad de términos el pasado y el presente geológico.

La teoría evolutiva más significativa antes de Darwin fue la de Jean-Baptiste Lamarck; basada en la transmisión de caracteres adquiridos (un mecanismo de herencia que fue ampliamente aceptado hasta el siglo XX), describió una cadena de desarrollo que se extiende desde el más ínfimo microbio hasta los seres humanos. El naturalista británico Charles Darwin, combinando la metodología de la biogeografía de Humboldt, la geología uniformista de Lyell, los trabajos de Thomas Malthus sobre el crecimiento demográfico y su propio conocimiento morfológico, crearon una teoría evolutiva más acertada basada en la selección natural; pruebas similares realizadas de forma independiente llevaron a Alfred Russel Wallace a alcanzar las mismas conclusiones.

La publicación en 1859 de la teoría de Darwin en "El origen de las especies" está considerado como el principal acontecimiento en la historia de la biología moderna. La credibilidad establecida de Darwin como naturalista, el tono sobrio del trabajo, y sobre todo la depurada fuerza y volumen de pruebas presentado, permitió a "El origen" tener éxito donde los trabajos evolutivos anteriores habían fallado, como el libro de Robert Chambers "Vestiges of the Natural History of Creation". La mayor parte de científicos aceptaron la evolución y el origen común hacia finales del siglo XIX, sin embargo, la selección natural no sería aceptada como el mecanismo primario de la evolución hasta bien entrado el siglo XX, cuando la mayoría de las teorías contemporáneas sobre la herencia parecieron incompatibles con la herencia de la variación aleatoria.

Wallace, siguiendo los trabajos anteriores de de Candolle, Humboldt y Darwin, realizó importantes contribuciones a la zoogeografía. Debido a su interés en la hipótesis de la transmutación, prestó particular atención a la distribución geográfica de las especies estrechamente relacionadas durante su trabajo de campo primero en América del Sur y después en el archipiélago malayo. Durante su estancia en el archipiélago identificó la llamada línea de Wallace, que discurre a través de las Molucas dividiendo la fauna del archipiélago entre una zona asiática y una zona nuevoguineana/australiana. Su pregunta clave, en cuanto a porqué la fauna de las islas con climas similares puede llegar a ser tan diferente, solo podía responderse considerando su origen. En 1876 escribió "The Geographical Distribution of Animals", que se convirtió en el trabajo de referencia estándar durante medio siglo, y una secuela, "Island Life", en 1880 que se centraba en la biogeografía insular. Amplió el sistema de seis regiones desarrollado por Philip Sclater para describir la distribución geográfica de las aves a los animales en general. Su método de tabular datos sobre los grupos animales en zonas geográficas destacó las discontinuidades y su apreciación sobre la evolución permitió que propusiera explicaciones racionales que no habían sido realizadas con anterioridad.

El estudio científico de la herencia genética creció rápidamente como consecuencia del "Origen de las especies" de Darwin con los trabajos de Francis Galton y los biométricos. El origen de la genética generalmente se asocia al trabajo de 1866 del monje agustino Gregor Mendel que sería conocido posteriormente como las Leyes de Mendel. Sin embargo, su trabajo no fue reconocido como significativo hasta 35 años después. Mientras tanto, una variedad de teorías de la herencia (basadas en la pangénesis, ortogénesis y otros mecanismos) fue debatida e investigada enérgicamente. La embriología y la ecología también se convirtieron en importantes campos biológicos, especialmente unidos a la evolución y popularizados por el trabajo de Ernst Haeckel. Sin embargo la mayor parte del trabajo del siglo XIX sobre la herencia no estaba en la esfera de la historia natural, sino en la de la fisiología experimental.

A lo largo del siglo XIX el alcance de fisiología se amplió en gran medida, de un campo fundamentalmente orientado a la medicina a una amplia investigación de los procesos físicos y químicos de la vida, incluidas plantas, animales e incluso microorganismo, además del hombre. "Seres vivos como máquinas" se convirtió en una metáfora dominante en el pensamiento biológico y social.

El desarrollo de la microscopía tuvo un profundo impacto en el pensamiento biológico. A principios del siglo, varios biólogos señalaron a la importancia fundamental de la célula. En 1838 y 1839, Schleiden y Schwann empezaron a promover la teoría según la cual (1) la unidad básica de los organismos es la célula, (2) las células individuales tienen todas las características de la vida, aunque se opusieran a la idea que (3) todas las células proceden de otras células. Gracias al trabajo de Robert Remak y Rudolf Virchow se aceptaron definitivamente entre la comunidad científica todas las tesis de la teoría celular.
La teoría celular obligó a los biólogos a volver a imaginar a los organismos individuales como conjuntos interdependientes de células individuales. Los científicos del emergente campo de la citología, armados con microscopios cada vez más potentes y con los nuevos métodos de tinción, pronto descubrieron que incluso las células individuales eran mucho más complejas que las cámaras llenas de fluido homogéneo descritas anteriormente por los microscopistas. Robert Brown había descrito el núcleo celular en 1831, y a finales del siglo XIX los citólogos ya habían identificado muchos de los componentes fundamentales de las células: cromosomas, centrosomas, mitocondrias, cloroplastos y otras estructuras se hacen visibles a través de la tinción. Entre 1874 y 1884 Walther Flemming describió las distintas fases de la mitosis, demostrando que no eran artefactos de la tinción, sino que ocurrían en las células vivas, y además que los cromosomas se duplicaban en número justo antes de la división celular y de la producción de una célula hija. Gran parte de la investigación sobre la reproducción celular se reunió en la teoría de August Weismann de la herencia: identificó el núcleo como el material hereditario, propuso la distinción entre células somáticas y células germinales (argumentando que el número de cromosomas se debe reducir a la mitad para las células germinales, un precursor del concepto de la meiosis), y adoptó la teoría de Hugo de Vries sobre la pangénesis. El weismannismo fue muy influyente, especialmente en el nuevo campo de la embriología experimental.
A mediados de 1850 la teoría miasmática de la enfermedad fue ampliamente superada por la teoría microbiana, creando un gran interés en los microorganismos y sus interacciones con otras formas de vida. En la década de 1880 la bacteriología se estaba convirtiendo en una disciplina coherente, especialmente a través de la obra de Robert Koch, quien introdujo métodos para el crecimiento de cultivos puros en placas de Petri con nutrientes específicos en gelatina de agar. La antigua idea de que los organismos vivos podrían originarse a partir de materia inanimada (generación espontánea) fue embestida por una serie de experimentos realizados por Louis Pasteur, mientras que los debates del vitalismo frente al mecanicismo (un tema perenne desde la época de Aristóteles y los atomistas griegos) continuaban con vehemencia.

En el campo de la química una cuestión fundamental era la distinción entre sustancias orgánicas e inorgánicas, sobre todo en el contexto de transformaciones orgánicas como la fermentación y la putrefacción. Desde Aristóteles, estos habían sido considerados procesos esencialmente biológicos ("vitales"), sin embargo, Friedrich Wöhler, Justus Liebig y otros pioneros del ascendente campo de la química orgánica (a partir de los trabajos de Lavoisier) demostraron que el mundo orgánico a menudo puede ser analizado por métodos físicos y químicos. En 1828 Wöhler demostró que una sustancia orgánica como la urea puede ser creada por medios químicos que no tienen que ver con la vida, poniendo en tela de juicio al vitalismo. Comenzando con la diastasa en 1833, se descubrieron extractos de célula («fermentos») que podría afectar las transformaciones químicas. A finales del siglo XIX se estableció el concepto de las enzimas, aunque las ecuaciones de la cinética química no se aplicarían a las reacciones enzimáticas hasta principios del siglo XX.

Fisiólogos como Claude Bernard exploraron (a través de la vivisección y otros métodos experimentales) las funciones físicas y químicas de los cuerpos vivos en un grado sin precedentes, sentando las bases para la endocrinología (un campo que se desarrolló rápidamente después del descubrimiento de la primera hormona, la secretina, en 1902), la biomecánica y el estudio de la nutrición y la digestión. La importancia y diversidad de los métodos de la fisiología experimental, en el seno de la medicina y la biología, creció de forma drástica durante la segunda mitad del siglo XIX. El control y la manipulación de los procesos de la vida se convirtió en una preocupación fundamental, y el experimento se situó en el centro de la educación biológica.

A principios del siglo XX la investigación biológica era en gran medida una tarea profesional. La mayor parte del trabajo todavía se realizaba al modo de la historia natural, que enfatizaba al análisis morfológico y filogenético por sobre las explicaciones causales basadas en experimentos. Sin embargo, los fisiólogos experimentales y embriólogos antivitalistas, especialmente en Europa, fueron cada vez más influyentes. El gran éxito de los enfoques experimentales hacia el desarrollo, la herencia y el metabolismo en las décadas de 1900 y 1910 demostró el poder de la experimentación en la biología. En las décadas siguientes, el trabajo experimental sustituyó a la historia natural como el método dominante de investigación.

A principios del siglo XX, los naturalistas se enfrentaron a una creciente presión para añadir rigor y preferentemente experimentación a sus métodos, tal como las nuevas y prominentes disciplinas biológicas basadas en el laboratorio habían hecho. La ecología había nacido como una combinación de la biogeografía con el ciclo biogeoquímico, concepto promovido por los químicos; los biólogos de campo desarrollaron métodos cuantitativos como el cuadrado de muestreo ("quadrat") y adaptaron instrumentos de laboratorio y cámaras para su utilización en el campo con tal de separar sus trabajos de la historia natural tradicional. Los zoólogos y botánicos hicieron lo posible para mitigar el carácter impredecible de los seres vivos, llevando a cabo experimentos de laboratorio y estudiando entornos naturales semicontrolados tales como jardines; nuevas instituciones como la Estación Carnegie para la Evolución Experimental y el Laboratorio de Biología Marina proporcionaron entornos más controlados para estudiar organismos a través de sus ciclos de vida completos.

El concepto de sucesión ecológica, promovido en las décadas de 1900 y 1910 por Henry Chandler Cowles y Frederic Clements, fue importante en los inicios de ecología de las plantas. Las ecuaciones presa-depredador de Alfred J. Lotka, los estudios de la biogeografía y la estructura bioquímica de los lagos y ríos (limnología) de G. Evelyn Hutchinson y los estudios sobre la cadena alimenticia animal de Charles Elton fueron pioneros entre la serie de métodos cuantitativos que colonizaron las especialidades ecológicas en desarrollo. La ecología se convirtió en una disciplina independiente en las décadas de 1940 y 1950 después de que Eugene P. Odum sintetizara muchos de los conceptos de la ecología de los ecosistemas, poniendo a las relaciones entre grupos de organismos (especialmente relaciones de materia y energía) en el centro del campo.

En la década de 1960, debido a que los teóricos evolutivos exploraron la posibilidad de múltiples unidades de selección, los ecologistas se volvieron hacia enfoques evolutivos. En la ecología de poblaciones, el debate sobre la selección de grupos fue breve pero intenso; durante la década de 1970, la mayoría de los biólogos concordaban en que la selección natural era rara vez efectiva a nivel de organismos individuales. La evolución de los ecosistemas, sin embargo, se convirtió en un foco de investigación permanente. La ecología se expandió rápidamente con el aumento del movimiento ambientalista; el Programa Biológico Internacional trató de aplicar los métodos de la gran ciencia (que había tenido mucho éxito en las ciencias físicas) a la ecología de ecosistemas y a los problemas ambientales apremiantes, mientras que los esfuerzos independientes de menor escala, tales como la biogeografía de islas y el Bosque Experimental de Hubbard Brook ayudaron a redefinir el ámbito de una disciplina cada vez más diversa.

1900 marcó el llamado "redescubrimiento de Mendel": Hugo de Vries, Carl Correns y Erich von Tschermak llegaron independiente a las leyes de Mendel (que en realidad no están presentes en el trabajo de Mendel). Poco después, los citólogos (biólogos celulares) propusieron que los cromosomas eran el material hereditario. Entre 1910 y 1915, Thomas Hunt Morgan y los «drosofilistas» con su mosca de laboratorio forjaron estas dos ideas —ambas controversiales— dentro de la «teoría cromosómica mendeliana» de la herencia. Ellos cuantificaron el fenómeno de ligamiento genético y postularon que los genes residen en los cromosomas como las cuentas de una cadena; plantearon la hipótesis del entrecruzamiento cromosómico para explicar el ligamiento y la construcción de mapas genéticos de la mosca de la fruta "Drosophila melanogaster", que se convirtió en un organismo modelo ampliamente utilizado.

Hugo de Vries trató de vincular a la nueva genética con la evolución; basándose en su trabajo sobre la herencia y la hibridación, propuso una teoría de mutacionismo, que fue ampliamente aceptada en el siglo XX. El lamarckismo también tuvo muchos adeptos. El darwinismo era visto como incompatible con los rasgos continuamente variables estudiados por la biometría, que parecían sólo parcialmente hereditarios. En la década de 1920 y 1930 —tras la aceptación de la teoría cromosómica mendeliana— el surgimiento de la disciplina de la genética de poblaciones, con el trabajo de R. A. Fisher, J. B. S. Haldane y Sewall Wright, unificó la idea de la evolución por selección natural con la genética mendeliana, produciendo la síntesis moderna. La herencia de caracteres adquiridos fue rechazada, mientras que el mutacionismo dio lugar a la maduración de teorías genéticas.

En la segunda mitad del siglo, las ideas sobre genética de poblaciones comenzaron a aplicarse en las nuevas disciplinas de la genética del comportamiento, la sociobiología, y especialmente en seres humanos, la psicología evolutiva. En la década de 1960 W. D. Hamilton entre otros desarrollaron la teoría de juegos enfocada en explicar el altruismo desde una perspectiva evolutiva a través de la selección de parentesco. El posible origen de los organismos superiores a través de la endosimbiosis, en contrastante con los enfoques de la evolución molecular desde una visión centrada en el gen (que tiene a la selección como la causa predominante de la evolución) y la teoría neutralista (que hace de la deriva genética un factor clave) dio lugar a debates permanentes sobre el equilibrio adecuado entre adaptacionismo y contingencia en la teoría evolutiva.
En la década de 1970, Stephen Jay Gould y Niles Eldredge propusieron la teoría del equilibrio puntuado, que sostiene que la inmutabilidad es la característica más destacada del registro fósil, y que la mayoría de los cambios evolutivos se producen rápidamente durante periodos relativamente cortos de tiempo. En 1980, Luis Álvarez y Walter Alvarez propusieron la hipótesis de que un impacto astronómico fue el responsable de la extinción masiva del Cretácico-Terciario. También en la década de 1980, el análisis estadístico en los registros fósiles de organismos marinos publicado por Jack Sepkoski y David M. Raup, llevó a una mejor apreciación de la importancia de los eventos de extinción masiva en la historia de la vida en la Tierra.

A finales del siglo XIX todas las principales rutas en el metabolismo de fármacos habían sido descubiertas, gracias a la comprensión del metabolismo de proteínas y ácidos grasos y de la síntesis de urea. En las primeras décadas del siglo XX, los componentes menores en los alimentos de la nutrición humana, las vitaminas, comenzaron a ser aislados y sintetizados. Las mejoras en técnicas de laboratorio como la cromatografía y la electroforesis llevaron a los rápidos avances en la química fisiológica, que —como "bioquímica"— comenzó a adquirir independencia de sus orígenes médicos. En las décadas de 1920 y 1930, los bioquímicos —dirigidos por Hans Krebs y Carl y Gerty Cori— comenzaron a trazar muchas de las rutas metabólicas centrales para la vida: el ciclo del ácido cítrico, la glucogénesis, la glucólisis y la síntesis de esteroides y porfirinas. Entre los años 1930 y 1950, Fritz Lipmann entre otros establecieron el papel del ATP como el portador universal de energía en la célula, y de la mitocondria como el centro energético de la célula. Tales trabajos tradicionalmente bioquímicos, continuaron siendo activamente perseguidos durante todo el siglo XX y en el siglo XXI.

Tras el ascenso de la genética clásica, muchos biólogos, —incluyendo una nueva ola de físicos en la biología— persiguieron la interrogante del gen y su naturaleza física. Warren Weaver, jefe de la división científica de la Fundación Rockefeller, distribuyó subvenciones para promover la investigación que aplicara los métodos de la física y la química a los problemas biológicos básicos, acuñando el término de "biología molecular" para este enfoque en 1938, muchos de los avances biológicos significativos de las décadas de 1930 y 1940 fueron financiados por la Fundación Rockefeller.

Como en la bioquímica, la superposición de las disciplinas de la bacteriología y la virología (más tarde combinadas como "microbiología"), situadas entre la ciencia y la medicina, se desarrolló rápidamente en el siglo XX. El aislamiento del bacteriófago por Félix d'Herelle durante la Primera Guerra Mundial inició una larga línea de investigación que se centró en los virus bacteriófagos y las bacterias que infectan.

El desarrollo del estándar, organismos genéticamente uniformes que pudieran producir resultados experimentales repetibles, fue esencial para el desarrollo de la genética molecular. Después de los primeros trabajos con la mosca "Drosophila" y el maíz, la adopción de sistemas modelo más simples como el moho del pan "Neurospora crassa" hizo posible la conexión entre la genética y la bioquímica, y más importante, con la hipótesis «un gen, una enzima» de Beadle y Tatum en 1941. Experimentos genéticos en sistemas aún más simples como el virus del mosaico del tabaco y el bacteriófago, ayudado por las nuevas tecnologías de la microscopía electrónica y la ultracentrifugación, obligó a los científicos a volver a evaluar el significado literal de "vida"; la herencia del virus y la reproducción de las estructuras celulares nucleoproteicas fuera del núcleo («plasmagenes») complicaron la teoría cromosómica mendeliana aceptada.

Oswald Avery mostró en 1943 que el ADN era probablemente el material genético de los cromosomas, y no sus proteínas; la cuestión se resolvió decisivamente con el experimento de Hershey y Chase en 1952, una de las muchas contribuciones del llamado grupo del fago centrado en torno al físico y biólogo Max Delbrück. En 1953 James D. Watson y Francis Crick, basándose en el trabajo de Maurice Wilkins y Rosalind Franklin, sugirieron que la estructura del ADN era una doble hélice. En su famoso artículo «"Estructura molecular de los ácidos nucleicos"», Watson y Crick observaron tímidamente: «No se nos escapa que el emparejamiento específico que hemos postulado sugiere inmediatamente un posible mecanismo de copiado del material genético». Después de 1958 el experimento de Meselson-Stahl confirmó la replicación semiconservativa del ADN, con lo que era evidente para la mayoría de los biólogos que la secuencia de ácido nucleico de alguna manera debía determinar la secuencia de aminoácidos en las proteínas; el físico George Gamow propuso que un código genético fijo relacionaba las proteínas y el ADN. Entre 1953 y 1961, había pocos secuencias biológicas conocidas, —ni siquiera el ADN o las proteínas— pero sí una gran cantidad de sistemas de código propuestos, una situación aún más complicada por el incremento en el conocimiento de la función intermediaria del ARN. Para realmente descifrar el código, se realizaron una extensa serie de experimentos en la bioquímica y la genética bacteriana, entre 1961 y 1966 —muy importantemente el trabajo de Nirenberg y Khorana.

Además de la División de Biología en el Instituto de Tecnología de California (Caltech), el Laboratorio de Biología Molecular (y sus precursores) en Cambridge, y un puñado de otras instituciones, el Instituto Pasteur se convirtió en un importante centro de investigación de la biología molecular a finales de la década de 1950. Los científicos de Cambridge, dirigidos por Max Perutz y John Kendrew, se centraron en el campo de rápido desarrollo de la biología estructural, combinando la cristalografía de rayos X con el modelado molecular y las nuevas posibilidades de cálculo de la computación digital (ambos beneficiados directa e indirectamente con la financiación militar de la ciencia). Más tarde, un número de bioquímicos dirigidos por Fred Sanger se unió al laboratorio de Cambridge, reuniendo así el estudio de la estructura y función macromolecular. En el Instituto Pasteur, François Jacob y Jacques Monod continuaron el experimento PaJaMo de 1959 con una serie de publicaciones sobre el operón lac que estableció el concepto de regulación genética e identificaron lo que llegó a ser conocido como ARN mensajero. A mediados de la década de 1960, el núcleo intelectual de la biología molecular —un modelo para las bases moleculares del metabolismo y la reproducción— estuvo en gran parte completo.

Entre finales de la década de 1950 hasta principios de la década de 1970 fue un período de intensa investigación y expansión institucional para la biología molecular, que se ha convertido en una disciplina coherente sólo recientemente. Los métodos y profesionales en biología molecular crecen con rapidez en lo que el biólogo organísmico E. O. Wilson ha llamado «la guerra molecular», a menudo llegando a dominar departamentos e incluso disciplinas enteras. La molecularización fue particularmente importante para la genética, la inmunología, la embriología y la neurobiología, mientras que la idea de que la vida es controlada por un «programa genético» —una metáfora que Jacob y Monod introdujeron desde los campos emergentes de la cibernética y las ciencias de la computación— se convirtió en un punto de vista influyente en toda la biología. La inmunología en particular, se vinculó con la biología molecular, fluyendo la innovación en ambos sentidos: la teoría de la selección clonal desarrollada por Niels Kai Jerne y Frank Macfarlane Burnet a mediados de 1950 ayudó a arrojar luz sobre los mecanismos generales de la síntesis de proteínas. 

La resistencia a la creciente influencia de la biología molecular fue especialmente evidente en la biología evolutiva. La secuenciación de proteínas tuvo un gran potencial para el estudio cuantitativo de la evolución (a través de la hipótesis del reloj molecular), pero importantes biólogos evolutivos cuestionaron la relevancia de la biología molecular para responder a las grandes preguntas de la causalidad evolutiva. Departamentos y disciplinas fracturadas, así como biólogos organicistas afirmaron su importancia e independencia: Theodosius Dobzhansky hizo la famosa declaración de que «nada en biología tiene sentido excepto a la luz de la evolución» como una respuesta al desafío molecular. El problema se hizo aún más crítico a partir de 1968; la teoría neutralista de la evolución molecular de Motoo Kimura sugiere que la selección natural no fue la causa de la evolución en todas partes, por lo menos a nivel molecular, y que la evolución molecular podría ser un proceso fundamentalmente diferente de la evolución morfológica. La resolución de esta «paradoja molecular/morfológica» ha sido un tema central de la investigación de la evolución molecular desde la década de 1960.

La biotecnología, en un sentido general ha sido una parte importante de la biología desde finales del siglo XIX. Con la industrialización en la elaboración de cerveza y la agricultura, los químicos y biólogos se dieron cuenta del gran potencial de los procesos biológicos controlados por humanos. En particular, la fermentación resultó ser de gran ayuda para las industrias químicas. Para inicios de la década de 1970, una amplia gama de biotecnologías fueron desarrolladas, desde drogas como la penicilina y los esteroides, hasta alimentos como "Chlorella" y proteína de origen unicelular para gasohol, así como una amplia gama de cultivos de alto rendimiento híbridos y tecnologías agrícolas, la base de la Revolución Verde.

La biotecnología en el sentido moderno de la ingeniería genética comenzó en la década de 1970 con la invención de técnicas de ADN recombinante. Las enzimas de restricción fueron descubiertas y caracterizadas a finales de la década de 1960, siguiendo los pasos de aislamiento, luego duplicación y luego síntesis de genes virales. Comenzando con el laboratorio de Paul Berg en 1972 (ayudado por la "Eco"RI del laboratorio Herbert Boyer basándose en el trabajo con la ligasa del laboratoria Arthur Kornberg), los biólogos moleculares pusieron todas estas piezas juntas para producir el primer organismo transgénico. Poco después, otros comenzaron a usar vectores plásmidos y a añadir genes para la resistencia a antibióticos, incrementando considerablemente el alcance de las técnicas de recombinación. 

Cautelosa ante los peligros potenciales (particularmente la posibilidad de una bacteria prolífica con un gen viral causante de cáncer), la comunidad científica, así como una amplia gama de científicos independientes reaccionaron hacia estos desarrollos tanto con entusiasmo como con reservas temerosas. Prominentes biólogos moleculares conducidos por Berg, sugirieron una moratoria temporal sobre las investigaciones con ADN recombinante hasta que los peligros pudiesen ser juzgados y las políticas pudiesen ser creadas. Esta moratoria fue largamente respetada, hasta que los participantes de la Conferencia de Asilomar sobre ADN Recombinante crearon recomendaciones políticas y concluyeron que la tecnología podía ser utilizada con seguridad.

Después de Asilomar, nuevas técnicas y aplicaciones de la ingeniería genética se desarrollaron rápidamente. Los métodos de secuenciación de ADN mejoraron mucho (iniciados por Fred Sanger y Walter Gilbert), al igual que la síntesis de oligonucleótidos y las técnicas de transfección. Los investigadores aprendieron a controlar la expresión de los transgenes, y pronto se apresuraron —tanto en el contexto académico como en el industrial— a crear organismos capaces de expresar genes humanos para la producción de hormonas humanas. Sin embargo, esta fue una tarea de mayores proporciones de las que los biólogos moleculares habían esperado; los desarrollos entre 1977 y 1980 mostraron que, debido a los fenómenos de división y empalme de los genes, los organismos superiores tienen un sistema de expresión genética mucho más complejo que el de las bacterias modelo usadas en estudios anteriores. El primer puesto en la carrera por la síntesis de la insulina humana fue ganado por Genentech. Esto marcó el inicio de la explosión biotecnológica (y con ella, la era de las patentes genéticas) con un nivel de solapamiento sin precedentes entre la biotecnología, la industria y la ley.

Durante la década de 1980, la secuenciación de proteínas había ya transformado los métodos de clasificación científica de los organismos (especialmente la cladística) pero los biólogos pronto comenzaron a usar las secuencias de ARN y ADN como caracteres; esto incrementó la significatividad de la evolución molecular dentro de la biología evolutiva, como resultado la sistemática molecular podría ser comparada con los árboles evolutivos tradicionales basados en la morfología. Siguiendo las ideas pioneras de Lynn Margulis sobre la teoría endosimbiótica, que sostiene que algunos de los orgánulos de las células eucariotas se originaron a partir de organismos procariotas sin vida a través de relaciones simbióticas, incluso la división global del árbol de la vida ha sido revisado. En la década de 1990, los cinco dominios (plantas, animales, hongos, protistas, y moneras) se convirtieron en tres (Archaea, Bacteria, y Eukarya) con base en el trabajo pionero sobre sistemática molecular de Carl Woese con la secuenciación del ARN ribosomal 16S.

El desarrollo y la popularización de la reacción en cadena de la polimerasa (PCR) a mediados de 1980 (por Kary Mullis y otros científicos de Cetus Corporation) marcó otro hito en la historia de la biotecnología moderna, incrementando considerablemente la facilidad y rapidez del análisis genético. Junto con el uso de los marcadores de secuencia expresada, la PCR condujo al descubrimiento de muchos más genes que pueden encontrarse a través de bioquímicos tradicionales o métodos genéticos y abrió la posibilidad de secuenciar genomas completos.

La unidad de gran parte de la morfogénesis de los organismos desde el huevo fertilizado hasta el adulto, empezó a ser descifrada tras el descubrimiento de los genes homeobox, primero en moscas de la fruta y luego en otros insectos y animales, incluyendo a seres humanos. Estos desarrollos dieron lugar a avances en el campo de la biología evolutiva del desarrollo hacia la comprensión de cómo los diversos planes corporales de los filos animales han evolucionado y cómo se relacionan entre sí.

El Proyecto Genoma Humano —el más grande y más costoso estudio biológico único jamás realizado— se inició en 1988 bajo la dirección de James D. Watson, después del trabajo preliminar con organismos modelo genéticamente más simples, tales como "E. coli", "S. cerevisiae" y "C. elegans". La secuenciación aleatoria y los métodos de descubrimiento de genes iniciados por Craig Venter —y alimentados por la promesa financiera de las patentes genéticas con Celera Genomics—, condujo a un concurso de secuenciación en los sectores público y privado, que terminó en un compromiso con el primer borrador de la secuencia del ADN humano anunciado en el año 2000.

A principios del siglo XXI, las ciencias biológicas convergieron con disciplinas nuevas y clásicas anteriormente diferenciadas como la física en campos de investigación como la biofísica. Se hicieron avances en química analítica e instrumentación física, incluidas las mejoras en sensores, componentes ópticos, marcadores, instrumentación, procesamiento de señales, redes, robots, satélites y poder de cómputo para la recopilación, almacenamiento, análisis, modelado, visualización y simulación de datos. Estos avances tecnológicos permitieron la investigación teórica y experimental, incluida la publicación en Internet de la bioquímica molecular, los sistemas biológicos y la ciencia de ecosistemas. Esto hizo posible el acceso mundial para mejorar las mediciones, los modelos teóricos, las simulaciones complejas, la teoría de experimentación con modelos predictivos, el análisis, el reporte observacional de datos por Internet, la libre revisión por pares, la colaboración y la publicación en Internet. Nuevos campos de investigación en ciencias biológicas surgieron como la bioinformática, la biología teórica, la genómica computacional, la astrobiología y la biología sintética.





</doc>
<doc id="3251" url="https://es.wikipedia.org/wiki?curid=3251" title="Protoplasma">
Protoplasma

El protoplasma es el material viviente de la célula, es decir, todo el interior de la célula (también el núcleo y el citoplasma).

Está formado por los elementos y sustancias químicas que se encuentran en la naturaleza, formando los cuerpos o estructuras no vivientes. 

En estado coloidal el protoplasma está formado por las siguientes sustancias:

El protoplasma tiene 3 propiedades fisiológicas fundamentales, "la irritabilidad, el metabolismo y la reproducción".


</doc>
<doc id="3252" url="https://es.wikipedia.org/wiki?curid=3252" title="Eukaryota">
Eukaryota

En biología y taxonomía, Eukaryota o Eukarya (palabras con etimología del griego: εὖ "eu" —‘bueno’, ‘bien’— y κάρυον "karyon" —‘nuez’, ‘carozo’, ‘núcleo’—) es el dominio (o imperio) que incluye los organismos formados por células con núcleo verdadero. La castellanización adecuada del término es eucariota o eucarionte. Estos organismos constan de una o más células eucariotas, abarcando desde organismos unicelulares hasta verdaderos pluricelulares en los que las diferentes células se especializan para diferentes tareas y que, en general, no pueden sobrevivir de forma aislada. 

Pertenecen al dominio o imperio eucariota los reinos de los animales, plantas y hongos, así como varios grupos incluidos en el parafilético reino Protista. Todos ellos presentan semejanzas a nivel molecular (estructura de los lípidos, proteínas y genoma), comparten un origen común, y principalmente, comparten el plan corporal de los eucariotas, muy diferente al de procariotas.

Con excepción de algunos organismos unicelulares, el ciclo de vida eucariota alterna una fase haplonte y otra diplonte, que se consigue mediante la alternancia de meiosis y fecundación, procesos que dan células haplontes y diplontes respectivamente.

Las células eucariotas son generalmente mucho más grandes que las procariotas y están mucho más compartimentadas. Poseen una gran variedad de membranas con núcleo rodeado de la envoltura nuclear, retículo endoplasmático y aparato de Golgi, además de mecanismos para la gemación y fusión de vesículas, incluida la exocitosis y endocitosis. Estructuras internas llamadas orgánulos se encargan de realizar funciones especializadas dentro de la célula. Presencia de lisosomas, peroxisomas y mitocondrias.

También caracteriza a todos los eucariotas un esqueleto interno o endoesqueleto, en este caso llamado citoesqueleto, formado por dos entramados de proteínas: el sistema de microtúbulos y el sistema contráctil de actina/miosina, que desempeñan un papel importante en la definición de la organización y forma de la célula, en el tráfico intracelular (por ejemplo, los movimientos de vesículas y orgánulos) y en la división celular. El característico flagelo eucariota y sus motores moleculares asociados se encuentran anclados al citoesqueleto.

El ADN de las células eucariotas está contenido en un núcleo celular separado del resto de la célula por una doble membrana permeable. El material genético se divide en varios bloques lineales llamados cromosomas, que son separados por un huso microtubular durante la división nuclear. Los cromosomas contienen histonas, varios replicones, centrómeros y telómeros. Hay un característico ciclo celular con segregación mitótica y reproducción sexual por meiosis. Se incluye un complejo de poros nucleares, transporte trans-membranal de ARN y proteínas a través de la envoltura nuclear, intrones y nuevos patrones de procesamiento del ARN utilizando espliceosomas. 

Para una comparación con las características procariotas, véase: Tabla comparativa.

La célula eucariota debe en gran parte su forma y capacidad de movimiento al citoesqueleto, ya que le otorga rigidez y flexibilidad. En los organismos flagelados ancla los flagelos al resto de la célula y permite su batido durante la locomoción o para la creación de corrientes de agua que le lleven el alimento. En los organismos ameboides permite la extensión de "pies" o seudópodos para la locomoción o la alimentación. También fija los surcos de alimentación de los excavados y el complejo apical que permite a los apicomplejos entrar en las células parasitadas.

Solo después de desarrollar su citoesqueleto pudo el eucariota ancestral realizar la fagocitosis, ya que es este el que, mediante crecimiento diferencial de sus fibras, logra que la célula se deforme para que la fagocitosis ocurra. La fagocitosis es también una propiedad ancestral de los eucariotas, si bien se ha perdido en grupos que se adaptaron a otras formas de alimentación. Hongos y plantas perdieron esta capacidad al desarrollar una pared celular rígida externa a la célula, pero ya contaban con otros modos de nutrición, la saprotrofia o el parasitismo en hongos y la fotosíntesis en plantas.

La mitocondria, derivada de la fagocitosis y posterior simbiogénesis de una proteobacteria, permitió al eucariota ancestral la respiración aerobia y con ello aprovechar al máximo la energía contenida en la materia orgánica. Como no es sorprendente en la evolución de un carácter tan antiguo, en varios grupos la mitocondria ha perdido esa capacidad ancestral y a cambio se ha modificado para cumplir otras funciones. También proceden de un evento de endosimbiosis los cloroplastos, en este caso con una cianobacteria, que permiten a las plantas realizar la fotosíntesis. Posteriormente otros grupos de eucariotas consiguieron sus cloroplastos mediante la endosimbiosis secundaria con un alga verde o roja.

Además de la división asexual de las células (mitosis), la mayoría de los eucariontes tiene algún proceso de reproducción sexual basado en la meiosis que no se encuentra entre los procariontes. La reproducción de los eucariontes típicamente implica la existencia de una fase haploide, donde está presente solamente una copia de cada cromosoma en las células, y diploide, donde están presentes dos. Las células diploides surgen por fusión nuclear (fecundación) y las haploides, por meiosis. En los organismos multicelulares, se distinguen tres tipos de ciclos biológicos:

Los organismos unicelulares pueden reproducirse asexualmente por bipartición, gemación o esporulación y sexualmente mediante gametos o por conjugación.

En los eucariontes, la relación de superficie frente a volumen es más pequeña que los procariontes, y así tienen tasas metabólicas más bajas y tiempos de generación más largos.

El origen de la célula eucariota es el proceso biológico más revolucionario desde el origen de la vida desde varios puntos de vista, como es el caso de la morfología, desarrollo evolutivo, estructura genética, relaciones simbióticas y ecología. Todas las células complejas son de este tipo y constituyen la base de casi todos los organismos pluricelulares. Aunque no hay acuerdo sobre cuándo se han originado los eucariotas, en general se sugerido hace unos 2309 millones de años. Aunque también se ha propuesto en 1800 Ma, sin embargo es controversial. Hasta ahora los fósiles más antiguos que pueden considerarse eucariotas son de 2100 millones de años y se les conoce como biota francevillense los cuales representan los primeros indicios de vida pluricelular. La separación entre los eucariotas y su grupo hermano las arqueas Asgard se estimó a finales del Arcaico (periodo Neoarcaico).

Eukarya se relaciona con Archaea desde el punto de vista del ADN nuclear y de la maquinaria genética, y ambos grupos son clasificados a veces juntos en el clado Neomura. Desde otros puntos de vista, tales como por la composición de la membrana, se asemejan más a Bacteria. Se han propuesto para ello tres posibles explicaciones principales:


Cada vez son mayores las evidencias que parecen demostrar que el origen eucariota es producto de la fusión de una arquea y una bacteria. Mientras el núcleo celular tiene elementos genéticos relacionados con las arqueas, las mitocondrias y la membrana celular tienen características bacterianas. La fusión genética es más evidente al constatar que los genes informativos parecen de origen arqueano y los genes operacionales de origen bacteriano. En todo caso también es cierto que un cierto número de rasgos presentes exclusivamente en los eucariontes son difíciles de explicar por medio de un evento de fusión.

Tampoco está claro el retraso de mil millones de años entre el origen de los eucariotas y su diversificación, pues las bacterias dominaron la biosfera hasta hace unos 800 millones de años. Este intervalo de estabilidad ambiental, litosférica y evolutiva se conoce con el nombre de "boring billion" (aburridos mil millones de años). Este retraso quizá se debiera simplemente a la dificultad de introducción en una biosfera ocupada enteramente por los procariotas. Otras explicaciones están relacionadas con la lenta evolución de los eucariotas o con el aumento del oxígeno, que no alcanzó los niveles ideales para los eucariotas hasta el final de dicho período. El diseño compartimentado de la célula eucariota es el más adecuado para el metabolismo aerobio y es comúnmente aceptado que todos los eucariotas actuales, incluidos los anaerobios, descienden de antecesores aerobios con mitocondrias.

Algún tiempo después de que surgiera la primera célula eucariota se produjo una radiación explosiva que las llevó a ocupar la mayoría de los nichos ecológicos disponibles.

La primera célula eucariota era probablemente flagelada, aunque con tendencias ameboides al no tener una cubierta rígida. Desde el antecesor flagelado, algunos grupos perdieron ulteriormente los flagelos, mientras que otros se convirtieron en multiflagelados o ciliados. Cilios y flagelos (incluidos los que tienen los espermatozoides) son estructuras homólogas con nueve dobletes de microtúbulos que se originan a partir de los centriolos.

El carácter ameboide surgió varias veces a lo largo de la evolución de los protistas dando lugar a los diversos tipos de seudópodos de los distintos grupos. El que los ameboides procedan de los flagelados y no al revés, como se pensaba en el pasado, tiene como base estudios moleculares (fusión, partición o duplicación de genes, inserción o borrado de intrones, etc.).
Está generalmente aceptado que los cloroplastos se originaron por endosimbiosis de una
cianobacteria y que todas las algas eucariotas evolucionaron en última instancia de antepasados heterótrofos. Se piensa que la diversificación primaria de la célula eucariota tuvo lugar entre los zooflagelados: células predadoras no fotosintéticas con uno o más flagelos para nadar, y a menudo también para generar corrientes de agua con las que capturar a las presas.

En la actualidad hay discrepancia en dónde debe ponerse la raíz del árbol de Eukarya. La posibilidad más aceptada es situarlo entre o próximo a los excavados, que serían el grupo basal de los eucariontes.

Durante la primera parte de su historia los eucariontes permanecieron unicelulares. A partir del período Ediacárico los pluricelulares comienzan a profilerar, aunque el proceso con seguridad comenzó bastante antes. Los organismos unicelulares de vida colonial comenzaron a cumplir funciones específicas en una zona del colectivo. Se formaron así los primeros tejidos y órganos. La pluricelularidad se desarrolló independientemente en varios grupos de eucariontes: plantas, hongos, animales, algas pardas y algas rojas. A pesar de su pluricelularidad, estos dos últimos grupos se siguen clasificando en el reino Protista. 

Las algas verdes, las primeras plantas, se desarrollaron para formar las primeras hojas. En el Silúrico surgen las primeras plantas terrestres y de ellas las plantas vasculares o cormófitas.

Los hongos unicelulares constituyeron filas de células o hifas que agrupadas se convirtieron en organismos pluricelulares absortivos con un marcado micelio. Inicialmente, los hongos fueron acuáticos y probablemente en el período Silúrico apareció el primer hongo terrestre, justo después de la aparición de las primeras plantas terrestres. Estudios moleculares sugieren que los hongos están más relacionados con los animales que con las plantas.

El reino animal comenzó con organismos similares a los actuales poríferos que carecen de verdaderos tejidos. Posteriormente se diversifican para dar lugar a los distintos grupos de invertebrados y vertebrados.

Los eucariontes se dividen tradicionalmente en cuatro reinos: Protista, Plantae, Animalia y Fungi (aunque Cavalier-Smith 2004, 2015 reemplaza Protista por dos nuevos reinos, Protozoa y Chromista). Esta clasificación es el punto de vista generalmente aceptado en actualidad, aunque ha de tenerse en cuenta que el reino Protista, definido como los eucariontes que no encajan en ninguno de los otros tres grupos, es parafilético. Por esta razón, la diversidad de los protistas coincide con la diversidad fundamental de los eucariontes.

La reciente clasificación de Adl "et al." (2018) evita la clasificación en reinos, sustituyéndola por una acorde con la filogenia actualmente conocida, en la que por otra parte a los clados o taxones no se les atribuye ya categoría alguna, para evitar los inconvenientes que suponen éstas para su posterior actualización. Los principales grupos de esta clasificación (equivalentes a reinos en clasificaciones anteriores) es como sigue:

Algunos grupos de protistas tienen una clasificación dudosa, en particular Cryptista (criptofitas) y Haptista (haptofitas), mientras que otros parecen situarse fuera de los grandes grupos, en particular CRuMs, Ancyromonadida y Hemimastigophora.

Adicionalmente se reconocen dos agrupaciones más grandes. Diaphoretickes (o Corticata) engloba a Archaeplastida y Sar, mientras que Amorphea (o Podiata) agrupa a Amoebozoa y Opisthokonta. Nótese que una forma ameboide o flagelar no indica la pertenencia a un grupo taxonómico concreto, como se creía en clasificaciones tradicionales, creando grupos artificiales desde el punto de visto evolutivo (ver polifilia).

El siguiente árbol filogenético muestras las relaciones entre los principales grupos de Eukarya de acuerdo con Adl "et al." 2018:



</doc>
<doc id="3253" url="https://es.wikipedia.org/wiki?curid=3253" title="Pinocitosis">
Pinocitosis

La pinocitosis (del griego: πίνειν gr 'beber', kyto- κύτος gr. cient. 'célula' y -ō-sis gr. 'proceso') es un tipo de endocitosis que consiste en la captación de material del espacio extracelular por invaginación de la membrana citoplasmática eucariota. Con desprendimiento hacia el interior celular de una vesícula que contiene líquido con posibles moléculas disueltas o partículas sólidas en suspensión.

La pinocitosis puede describirse como la fagocitosis de moléculas solubles. En esta la membrana se repliega creando una "vesícula pinocítica" y es de esta manera como las grasas, que son insolubles, pasan de la luz del intestino al torrente sanguíneo.

Un tipo de célula en la cual se la ha observado frecuentemente es el óvulo humano. Cuando el óvulo madura en el ovario de la mujer, se rodea de "células nodrizas". Aparentemente, estas células ceden alimentos disueltos al óvulo, que los incorpora por pinocitosis.

En la pinocitosis, la membrana celular se invagina formando una vesícula alrededor del líquido del medio externo que será incorporado a la célula

Junto con la fagocitosis, constituyen los dos tipos principales de endocitosis. A diferencia de la fagocitosis, la pinocitosis consiste en el ingreso de fluidos a través de la membrana celular mediante la formación de vesículas especiales, que se denominan pinosomas o vesículas pinocíticas. En el ser humano, este fenómeno se observa en células de la mucosa intestinal, cuando éstas permiten el ingreso de vesículas de grasa durante la absorción de nutrientes.

Las vesículas se originan en la superficie cubierta por clatrina, la vesícula revestida de clatrina pasa al citoplasma mediante invaginación, una vez que la vesícula esta en el citoplasma el revestimiento de clatrina desaparece, los trisqueliones (formados por moléculas de clatrina) quedan libres en el citoplasma. 

Dicha vesícula se fusiona con el endosoma temprano en la que intervienen dos proteínas:

a) V -SNAREs

b) T - SNARES

Posteriormente entra en juego el endosoma tardío y la formación del lisosoma en la que se produce la digestión.

En la pinocitosis se produce una acidificación gradual desde el endosoma temprano con un pH de 6, endosoma tardío pH 5,5 y el lisosoma pH 3,5.

Hay que recalcar que hay células en las que la pinocitosis no precisa moléculas de clatrina ni trisqueliones, sino que están revestidas de otras proteínas llamadas caveolinas.


</doc>
<doc id="3255" url="https://es.wikipedia.org/wiki?curid=3255" title="Nutrición autótrofa">
Nutrición autótrofa

Los organismos autotrófos es la capacidad de ciertos organismos de sintetizar todas las sustancias esenciales para su metabolismo a partir de sustancias inorgánicas, de manera que para su nutrición no necesitan de otros seres vivos. Organismos autótrofos son las plantas, las algas y algunas bacterias y arqueas.

Se denominan así porque generan su propio alimento, a través de sustancias inorgánicas para su metabolismo. Los organismos autótrofos producen su masa celular y materia orgánica a partir del dióxido de carbono, que es inorgánico, como única fuente de carbono, usando la luz o sustancias químicas como fuente de energía.

Los seres autótrofos pueden clasificarse en fotosintéticos y quimiosintéticos. Las plantas y otros organismos que usan la fotosíntesis se llaman fotolitoautótrofos; las bacterias que utilizan la oxidación de compuestos inorgánicos, como el anhídrido sulfuroso o compuestos ferrosos, para producir energía se llaman quimiolitotróficos.

Los seres heterótrofos, como los animales, los hongos, los protozoos, los mohos mucilaginosos y la mayoría de las bacterias y arqueas, dependen de los autótrofos, ya que aprovechan la materia que estos contienen para fabricar moléculas orgánicas complejas.

Los seres vivos basan su composición en compuestos en los que el elemento químico definitorio es el carbono (compuestos orgánicos), y los autótrofos obtienen todo el carbono a través de un proceso metabólico de fijación del carbono llamado ciclo de Calvin.

Los organismos autótrofos forman el primer eslabón en las cadenas tróficas como productores primarios de la materia orgánica que circula a través de ellas. Son necesariamente los organismos más abundantes, ya que —dada la eficiencia limitada de los procesos metabólicos— cada eslabón está mucho menos representado que los anteriores.

Los seres autótrofos son una parte esencial en la cadena alimenticia, ya que benefician a otros seres vivos, llamados heterótrofos, que utilizan a los autótrofos como alimento. Los autótrofos obtienen los átomos y la energía que necesitan de fuentes abióticas, como la luz solar (por medio de la fotosíntesis) o las reacciones químicas entre sustancias minerales (por medio de la quimiosíntesis), así como de fuentes inorgánicas, como el dióxido de carbono, y los convierten en moléculas orgánicas que utilizan para desarrollar funciones biológicas, como su propio crecimiento celular, además de servir de alimento a los heterótrofos.



</doc>
<doc id="3256" url="https://es.wikipedia.org/wiki?curid=3256" title="Lípido">
Lípido

Los lípidos son un conjunto de moléculas orgánicas (la mayoría biomoléculas), que están constituidas principalmente por carbono e hidrógeno y en menor medida por oxígeno que integran cadenas hidrocarbonadas alifáticas o aromáticas, aunque, también pueden contener fósforo, azufre y nitrógeno.

Debido a su estructura, son moléculas hidrófobas (insolubles en agua), pero son solubles en disolventes orgánicos no polares como la bencina, el benceno y el cloroformo lo que permite su extracción mediante este tipo de disolventes. A los lípidos se les llama incorrectamente grasas, ya que las grasas son solo un tipo de lípidos procedentes de animales y son los más ampliamente distribuidos en los organismos vivos.

Los lípidos cumplen diversas funciones en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), estructural (como los fosfolípidos de las bicapas) y reguladora (como las hormonas esteroides). Además, se les atribuye la capacidad de ser aislantes naturales, ya que son malos conductores del calor.

La palabra lípido está compuesta con los siguientes lexemas:

Los lípidos son moléculas diversas en el cuerpo ; unos están formados por cadenas alifáticas saturadas o insaturadas, en general lineales, pero algunos tienen anillos (aromáticos). Algunos son flexibles, mientras que otros son rígidos o semiflexibles hasta alcanzar casi una total Flexibilidad mecánica molecular; algunos comparten carbonos libres y otros forman puentes de hidrógeno.

La mayoría de los lípidos tienen algún tipo de carácter no polar, es decir, poseen una gran parte de apolar o de hidrofóbico ("que le teme al agua" o "rechaza el agua"), lo que significa que no interactúa bien con solventes polares como el agua, pero sí con la gasolina, el éter o el cloroformo. Otra parte de su estructura es polar o hidrofílica ("que tiene afinidad por el agua") y tenderá a asociarse con solventes polares como el agua; cuando una molécula tiene una región hidrófoba y otra hidrófila se dice que tiene carácter de anfipático. La región hidrófoba de los lípidos es la que presenta solo átomos de carbono unidos a átomos de hidrógeno, como la larga "cola" alifática de los ácidos grasos o los anillos de esterano del colesterol; la región hidrófila es la que posee grupos polares o con cargas eléctricas, como el hidroxilo (–OH) del colesterol, el carboxilo (–COOH) de los ácidos grasos, el fosfato (–PO) de los fosfolípidos.

Los lípidos son hidrofóbicos, esto se debe a que la molécula de agua está compuesta por un átomo de oxígeno y dos de hidrógeno a su alrededor, unidos entre sí por un enlace de hidrógeno. El núcleo de oxígeno es más grande que el del hidrógeno, presentando mayor electronegatividad. Como los electrones tienen mayor carga negativa, la transacción de un átomo de oxígeno tiene una carga suficiente como para atraer a los de hidrógeno con carga opuesta, uniéndose así el hidrógeno y el agua en una estructura molecular polar.

Por otra parte, los lípidos son largas cadenas de hidrocarburos y pueden tomar ambas formas: cadenas alifáticas saturadas (un enlace simple entre diferentes enlaces de carbono) o insaturadas (unidos por enlaces dobles o triples). Esta estructura molecular es no polar.

Los lípidos son un grupo muy heterogéneo que usualmente se subdivide en dos, atendiendo a que posean en su composición ácidos grasos (lípidos saponificables) o no los posean (lípidos insaponificables):


Para que los ácidos grasos puedan ser utilizados a nivel celular se transportan en forma de triglicéridos, que consisten en una molécula de glicerol unida a tres ácidos grasos por lo que también es llamado triester de glicerilo.
Son las unidades básicas de los lípidos saponificables, y consisten en moléculas formadas por una larga cadena hidrocarbonada(CH2) con un número par de átomos de carbono (2-24) y un grupo carboxilo(COOH) terminal. La presencia de dobles enlaces en el ácido graso reduce el punto de fusión.
Los ácidos grasos se dividen en saturados e insaturados.
Los denominados ácidos grasos esenciales no pueden ser sintetizados por el organismo humano y son el ácido linoleico, el ácido linolénico y el ácido araquidónico, que deben ingerirse en la dieta.


Los acilglicéridos o acilgliceroles son ésteres de ácidos grasos con glicerol (glicerina), formados mediante una reacción de condensación llamada esterificación. Una molécula de glicerol puede reaccionar con hasta tres moléculas de ácidos grasos, puesto que tiene tres grupos hidroxilo.

Según el número de ácidos grasos que se unan a la molécula de glicerina, existen tres tipos de acilgliceroles:

Los triglicéridos constituyen la principal reserva energética de los animales, en los que constituyen las grasas; en los vegetales constituyen los aceites. El exceso de lípidos es almacenado en grandes depósitos en el tejido adiposo de los animales.

Las ceras son esteres de un alcohol monohidroxilado de cadena larga como un ácido graso. Por ejemplo la cera de abeja. Son sustancias altamente insolubles en medios acuosos y a temperatura ambiente se presentan sólidas y duras. En los animales las podemos encontrar en la superficie del cuerpo, piel, plumas, cutícula, etc. En los vegetales, las ceras recubren en la epidermis de frutos, tallos, junto con la cutícula o la suberina, que evitan la pérdida de agua por evaporación.

Los complejos además de contener carbono, hidrógeno y oxígeno, pueden tener azufre, fosfato y nitrógeno e inclusive glúcido.

Los fosfolípidos se caracterizan por poseer un grupo de naturaleza de fosfato que les otorga una marcada polaridad. Se clasifican en dos grupos, según posean glicerol o esfingosina.

Los fosfoglicéridos están compuestos por ácido fosfatídico,que contienen ácido fosfórico en lugar de ácido graso, combinado con una base de hidrógeno. Es una molécula compleja compuesta por glicerol, al que se unen dos ácidos grasos (uno saturado y otro insaturado) y un grupo fosfato; el grupo fosfato posee un alcohol o un aminoalcohol, y el conjunto posee una marcada polaridad y forma lo que se denomina la "cabeza" polar del fosfoglicérido; los dos ácidos grasos forman las dos "colas" hidrófobas; por tanto, los fosfoglicéridos son moléculas con un fuerte carácter anfipático que les permite formar bicapas, que son la arquitectura básica de todas las membranas biológicas.

Los principales alcoholes y aminos de los fosfoglicéridos que se encuentran en las membranas biológicas son la colina (para formar la fosfatidilcolina o lecitina), la etanolamina (fosfatidiletanolamina o cefalina), serina (fosfatidilserina) y el inositol (fosfatidilinositol).

Los fosfoesfingolípidos son esfingolípidos con un grupo fosfato, tienen una arquitectura molecular y unas propiedades similares a los fosfoglicéridos. No obstante, no contienen glicerol, sino esfingosina, un aminoalcohol de cadena larga al que se unen un ácido graso, conjunto conocido con el nombre de ceramida; a dicho conjunto se le une un grupo fosfato y a este un aminoalcohol; el más abundante es la esfingomielina, en la que el ácido graso es el ácido lignocérico y el aminoalcohol la colina; es el componente principal de la vaina de mielina que recubre los axones de las neuronas.

Los glucolípidos son esfingolípidos formados por una ceramida (aminoalcohol + ácido graso) unida a un glúcido, careciendo, por tanto, de grupo fosfato. Al igual que los fosfoesfingolípidos poseen ceramida, pero a diferencia de ellos, no tienen fosfato ni alcohol. Se hallan en las bicapas lipídicas de todas las membranas celulares, y son especialmente abundantes en el tejido nervioso; el nombre de los dos tipos principales de glucolípidos alude a este hecho:

Los glucolípidos se localizan en la cara externa de la bicapa de las membranas celulares donde actúan de receptores.

Los terpenos, terpenoides o isoprenoides, son lípidos derivados del hidrocarburo isopreno (o 2-metil-1,3-butadieno). Los terpenos biológicos constan, como mínimo de dos moléculas de isopreno. Algunos terpenos importantes son los aceites esenciales (mentol, limoneno, geraniol), el fitol (que forma parte de la molécula de clorofila), las vitaminas A, K y E, los carotenoides (que son pigmentos fotosintéticos) y el caucho (que se obtiene del árbol "Hevea brasiliensis"). Desde el punto de vista farmacéutico, los grupos de principios activos de naturaleza terpénica más interesantes son: monoterpenos y sesquiterpenos constituyentes de los aceites esenciales, derivados de monoterpenos correspondientes a los iridoides, lactonas sesquiterpénicas que forman parte de los principios amargos, algunos diterpenos que poseen actividades farmacológicas de aplicación a la terapéutica y por último, triterpenos y esteroides entre los cuales se encuentran las saponinas y los heterósidos cardiotónicos.

Los esteroides son lípidos derivados del núcleo del hidrocarburo esterano (o ciclopentanoperhidrofenantreno), esto es, se componen de cuatro anillos fusionados de carbono que posee diversos grupos funcionales (carbonilo, hidroxilo) por lo que la molécula tiene carácter anfipático.

Entre los esteroides más destacados se encuentran los ácidos biliares, las hormonas sexuales, las corticosteroides, la vitamina D y el colesterol. El colesterol es el precursor de numerosos esteroides y es un componente más de la bicapa de las membranas celulares. Esteroides Anabólicos es la forma como se conoce a las substancias sintéticas basadas en hormonas sexuales masculinas (andrógenos). Estas hormonas promueven el crecimiento de músculos (efecto anabólico) así como también en desarrollo de las características sexuales masculinas (efecto andrógeno).

Los esteroides anabólicos fueron desarrollados a finales de 1930 principalmente para tratar el Hipogonadismo, una condición en la cual los testículos no producen suficiente testosterona para garantizar un crecimiento, desarrollo y función sexual normal del individuo. Precisamente a finales de 1930 los científicos también descubrieron que estos esteroides facilitaban el crecimiento de músculos en los animales de laboratorio, lo cual llevó al uso de estas sustancias por parte de físicoculturistas y levantadores de pesas y después por atletas de otras especialidades.

El abuso de los esteroides se ha diseminado tanto que hoy en día afecta el resultado de los eventos deportivos.

Los eicosanoides o prostaglandinas son lípidos derivados de los ácidos grasos esenciales de 20 carbonos tipo omega-3 y omega-6. Los principales precursores de los eicosanoides son el ácido araquidónico, el ácido linoleico y el ácido linolénico. Todos los eicosanoides son moléculas de 20 átomos de carbono y pueden clasificarse en tres tipos: prostaglandinas, tromboxanos y leucotrienos.

Cumplen amplias funciones como mediadores para el sistema nervioso central, los procesos de la inflamación y de la respuesta inmune tanto de vertebrados como invertebrados. Constituyen las moléculas involucradas en las redes de comunicación celular más complejas del organismo animal, incluyendo el hombre.

Los lípidos desempeñan diferentes tipos de funciones biológicas:

Los lípidos (generalmente en forma de triacilgiceroles) constituyen la reserva energética de uso tardío o diferido del organismo. Su contenido calórico es muy alto (10 Kcal/gramo), y representan una forma compacta y anhidra de almacenamiento de energía.

A diferencia de los hidratos de carbono, que pueden metabolizarse en presencia o en ausencia de oxígeno, los lípidos sólo pueden metabolizarse aeróbicamente.

Aunque parezca paradójico, los lípidos representan una importante reserva de agua. Al poseer un grado de reducción mucho mayor el de los hidratos de carbono, la combustión aerobia de los lípidos produce una gran cantidad de agua (agua metabólica). Así, la combustión de un mol de ácido palmítico puede producir hasta 146 moles de agua (32 por la combustión directa del palmítico, y el resto por la fosforilación oxidativa acoplada a la respiración). En animales desérticos, las reservas grasas se utilizan principalmente para producir agua (es el caso de la reserva grasa de la joroba de camellos y dromedarios).

Las vitaminas A, D, E y K son liposolubles, lo que significa que solo pueden ser digeridas, absorbidas y transportadas junto con las grasas. Las grasas juegan un papel vital en el mantenimiento de una piel y cabellos saludables, en el aislamiento de los órganos corporales contra el shock, en el mantenimiento de la temperatura corporal y promoviendo la función celular saludable. Además, sirven como reserva energética para el organismo. Las grasas son degradadas en el organismo para liberar glicerol y ácidos grasos libres.

El contenido de grasas de los alimentos puede ser analizado por extracción. El método exacto varía según el tipo de grasa a analizar. Por ejemplo, las grasas poliinsaturadas y monoinsaturadas son analizadas de forma muy diferente. Las grasas y los aceites son los principales lípidos que se encuentran en los alimentos, además contribuyen a la textura y a las propiedades sensoriales de nutrición. 

Las grasas también pueden servir como un tampón muy útil de una gran cantidad de sustancias extrañas. Cuando una sustancia particular, sea química o biótica, alcanza niveles no seguros en el torrente sanguíneo, el organismo puede efectivamente diluir (o al menos mantener un equilibrio) estas sustancias dañinas almacenándolas en nuevo tejido adiposo. Esto ayuda a proteger órganos vitales, hasta que la sustancia dañina pueda ser metabolizada o retirada de la sangre a través de la excreción, orina, desangramiento accidental o intencional, excreción de sebo y crecimiento del pelo.

Es prácticamente imposible eliminar completamente las grasas de la dieta, y, además, sería equivocado hacerlo. Algunos ácidos grasos son nutrientes esenciales, significando esto que ellos no pueden ser producidos en el organismo a partir de otros componentes y por lo tanto necesitan ser consumidos mediante la dieta. Todas las demás grasas requeridas por el organismo no son esenciales y pueden ser producidas en el organismo a partir de otros componentes.

El tejido adiposo o graso es el medio utilizado por el organismo humano para almacenar energía a lo largo de extensos períodos de tiempo. Dependiendo de las condiciones fisiológicas actuales, los adipocitos almacenan triglicéridos derivadas de la dieta y el metabolismo hepático o degrada las grasas almacenadas para proveer ácidos grasos y glicerol a la circulación. Estas actividades metabólicas son reguladas por varias hormonas (insulina, glucagón y epinefrina). La localización del tejido determina su perfil metabólico: la grasa visceral está localizada dentro de la pared abdominal (debajo de los músculos de la pared abdominal) mientras que la grasa subcutánea está localizada debajo de la piel (incluye la grasa que está localizada en el área abdominal debajo de la piel pero por encima de los músculos de la pared abdominal).




</doc>
<doc id="3258" url="https://es.wikipedia.org/wiki?curid=3258" title="Ácido ribonucleico">
Ácido ribonucleico

El ácido ribonucleico (ARN o RNA) es un ácido nucleico formado por una cadena de ribonucleótidos. Está presente tanto en las células procariotas como en las eucariotas, y es el único material genético de ciertos virus (los virus ARN).

El ARN se puede definir como la molécula formada por una cadena simple de ribonucleótidos, cada uno de ellos formado por ribosa, un fosfato y una de las cuatro bases nitrogenadas (adenina, guanina, citosina y uracilo). El ARN celular es lineal y monocatenario (de una sola cadena), pero en el genoma de algunos virus es de doble hebra.

En los organismos celulares desempeña diversas funciones. Es la molécula que dirige las etapas intermedias de la síntesis proteica; el ADN no puede actuar solo, y se vale del ARN para transferir esta información vital durante la síntesis de proteínas (producción de las proteínas que necesita la célula para sus actividades y su desarrollo). Varios tipos de ARN regulan la expresión génica, mientras que otros tienen actividad catalítica. El ARN es, pues, mucho más versátil que el ADN.

Los ácidos nucleicos fueron descubiertos en 1867 por Friedrich Miescher, que los llamó nucleína ya que los aisló del núcleo celular. Más tarde, se comprobó que las células procariotas, que carecen de núcleo, también contenían ácidos nucleicos. El papel del ARN en la síntesis de proteínas fue sospechado en 1939. Severo Ochoa ganó el Premio Nobel de Medicina en 1959 tras descubrir cómo se sintetizaba el ARN.

En 1965 Robert W. Holley halló la secuencia de 77 nucleótidos de un ARN de transferencia de una levadura, con lo que obtuvo el Premio Nobel de Medicina en 1968. En 1967, Carl Woese comprobó las propiedades catalíticas de algunos ARN y sugirió que las primeras formas de vida usaron ARN como portador de la información genética tanto como catalizador de sus reacciones metabólicas (hipótesis del mundo de ARN). En 1976, Walter Fiers y sus colaboradores determinaron la secuencia completa del ARN del genoma de un virus ARN (bacteriófago MS2).

En 1990 se descubrió en "Petunia" que genes introducidos pueden silenciar genes similares de la misma planta, lo que condujo al descubrimiento del ARN interferente. Aproximadamente al mismo tiempo se hallaron los micro ARN, pequeñas moléculas de 22 nucleótidos que tenían algún papel en el desarrollo de "Caenorhabditis elegans". El descubrimiento de ARN que regulan la expresión génica ha permitido el desarrollo de medicamentos hechos de ARN, como los ARN pequeños de interferencia que silencian genes.
En el año 2016 se tiene prácticamente por comprobado que las moléculas de ARN fueron la primera forma de vida propiamente dicha en habitar el planeta Tierra (Hipótesis del mundo de ARN).

Como el ADN, el ARN está formado por una cadena de monómeros repetitivos llamados nucleótidos. Los nucleótidos se unen uno tras otro mediante enlaces fosfodiéster cargados negativamente.

Cada nucleótido está formado por tres componentes:

Los carbonos de la ribosa se numeran de 1' a 5' en sentido horario. La base nitrogenada se une al carbono 1'; el grupo fosfato se une al carbono 5' y al carbono 3' de la ribosa del siguiente nucleótido. El pico tiene una carga negativa a pH fisiológico lo que confiere al ARN carácter polianiónico. Las bases púricas (adenina y guanina) pueden formar puentes de hidrógeno con las pirimidínicas (uracilo y citosina) según el esquema C=G y A=U. Además, son posibles otras interacciones, como el apilamiento de bases o tetrabucles con apareamientos G=A.
Muchos ARN contienen además de los nucleótidos habituales, nucleótidos modificados, que se originan por transformación de los nucleótidos típicos; son característicos de los ARN de transferencia (ARNt) y el ARN ribosómico (ARNr); también se encuentran nucleótidos metilados en el ARN mensajero eucariótico.

La interacción por puentes de hidrógeno descrita por Watson y Crick forma pares de bases entre una purina y una pirimidina. A este patrón se le conoce como apareamiento Watson y Crick. En este, la adenina se aparea con el uracilo (timina, en ADN) y la citosina con la guanina. Sin embargo en el ARN se presentan muchas otras formas de apareamiento, de las cuales la más ubicua es el apareamiento wobble (también apareamiento por balanceo o apareamiento titubeante) para la pareja G-U. Este fue propuesto por primera vez por Crick para explicar el apareamiento codón-anticodón en los tRNAs y ha sido confirmado en casi todas las clases de RNA en los tres dominios filogenéticos.

Se refiere a la secuencia lineal de nucleótidos en la molécula de ARN. Los siguientes niveles estructurales (estructura secundaria, terciaria) son consecuencia de la estructura primaria. Además, la secuencia misma puede ser información funcional; esta puede traducirse para sintetizar proteínas (en el caso del mRNA) o funcionar como región de reconocimiento, región catalítica, entre otras. <br>
<br>
<br>

El ARN se pliega como resultado de la presencia de regiones cortas con apareamiento intramolecular de bases, es decir, pares de bases formados por secuencias complementarias más o menos distantes dentro de la misma hebra. La estructura secundaria se refiere, entonces, a las relaciones de apareamiento de bases: «El término ‘estructura secundaria’ denota cualquier patrón plano de contactos por apareamiento de bases. Es un concepto topológico y no debe ser confundido con algún tipo de estructura bidimensional». La estructura secundaria puede ser descrita a partir de "motivos estructurales" que se suelen clasificar de la siguiente manera:

La estructura terciaria es el resultado de las interacciones en el espacio entre los átomos que conforman la molécula. Algunas interacciones de este tipo incluyen el apilamiento de bases y los apareamientos de bases distintos a los propuestos por Watson y Crick, como el apareamiento Hoogsteen, los apareamientos triples y los zíperes de ribosa. 

A diferencia del ADN las moléculas de ARN suelen ser de cadena simple y no forman dobles hélices extensas, no obstante, en las regiones con bases apareadas sí forma hélices como motivo estructural terciario. Una importante característica estructural del ARN que lo distingue del ADN es la presencia de un grupo hidroxil en posición 2' de la ribosa, que causa que las dobles hélices de ARN adopten una conformación A, en vez de la conformación B que es la más común en el ADN. Esta hélice A tiene un surco mayor muy profundo y estrecho y un surco menor amplio y superficial. Una segunda consecuencia de la presencia de dicho hidroxilo es que los enlaces fosfodiéster del ARN de las regiones en que no se forma doble hélice son más susceptibles de hidrólisis química que los del ADN; los enlaces fosfodiéster del ARN se hidrolizan rápidamente en disolución alcalina, mientras que los enlaces del ADN son estables. La vida media de las moléculas de ARN es mucho más corta que las del ADN, de unos minutos en algunos ARN bacterianos o de unos días en los ARNt humanos.

La biosíntesis de ARN está catalizada normalmente por la enzima ARN polimerasa que usa una hebra de ADN como molde, proceso conocido con el nombre de transcripción. Por tanto, todos los ARN celulares provienen de copias de genes presentes en el ADN.

La transcripción comienza con el reconocimiento por parte de la enzima de un promotor, una secuencia característica de nucleótidos en el ADN situada antes del segmento que va a transcribirse; la doble hélice del ADN es abierta por la actividad helicasa de la propia enzima. A continuación, la ARN polimerasa progresa a lo largo de la hebra de ADN en sentido 3' → 5', sintetizando una molécula complementaria de ARN; este proceso se conoce como elongación, y el crecimiento de la molécula de ARN se produce en sentido 5' → 3'. La secuencia de nucleótidos del ADN determina también dónde acaba la síntesis del ARN, gracias a que posee secuencias características que la ARN polimerasa reconoce como señales de terminación.

Tras la transcripción, la mayoría de los ARN son modificados por enzimas. Por ejemplo, al pre-ARN mensajero eucariota recién transcrito se le añade un nucleótido de guanina modificado (7-Metilguanosina) en el extremo 5' por medio de un puente de trifosfato formando un enlace 5'→ 5' único, también conocido como "capucha" o "caperuza", y una larga secuencia de nucleótidos de adenina en el extremo 3' (cola poli-A); posteriormente se le eliminan los intrones (segmentos no codificantes) en un proceso conocido como "splicing".

En virus, hay también varias ARN polimerasas ARN-dependientes que usan ARN como molde para la síntesis de nuevas moléculas de ARN. Por ejemplo, varios virus ARN, como los poliovirus, usan este tipo de enzimas para replicar su genoma.

El ARN mensajero (ARNm) es el tipo de ARN que lleva la información del ADN a los ribosomas, el lugar de la síntesis de proteínas. La secuencia de nucleótidos del ARNm determina la secuencia de aminoácidos de la proteína. Por ello, el ARNm es denominado ARN codificante.

No obstante, muchos ARN no codifican proteínas, y reciben el nombre de ARN no codificantes; se originan a partir de genes propios (genes ARN), o son los intrones rechazados durante el proceso de "splicing". Son ARN no codificantes el ARN de transferencia (ARNt) y el ARN ribosómico (ARNr), que son elementos fundamentales en el proceso de traducción, y diversos tipos de ARN reguladores.

Ciertos ARN no codificantes, denominados ribozimas, son capaces de catalizar reacciones químicas como cortar y unir otras moléculas de ARN, o formar enlaces peptídicos entre aminoácidos en el ribosoma durante la síntesis de proteínas.

El ARN mensajero (ARNm o RNAm) es el que lleva la información sobre la secuencia de aminoácidos de la proteína desde el ADN, lugar en que está inscrita, hasta el ribosoma, lugar en que se sintetizan las proteínas de la célula. Es por lo tanto, una molécula intermediaria entre el ADN y la proteína, y el apelativo de "mensajero", es del todo descriptivo. En los eucariotas, el ARNm se sintetiza en el nucleoplasma del núcleo celular y donde es procesado antes de acceder al citosol, donde se hallan los ribosomas, a través de los poros de la envoltura nuclear.

Los ARN de transferencia (ARNt o tRNA) son cortos polímeros de unos 80 nucleótidos, que transfiere un aminoácido específico al polipéptido en crecimiento; se unen a lugares específicos del ribosoma durante la traducción. Tienen un sitio específico para la fijación del aminoácido (extremo 3') y un anticodón formado por un triplete de nucleótidos que se une al codón complementario del ARNm mediante puentes de hidrógeno. Estos ARNt, al igual que otros tipos de ARN, pueden ser modificados post-transcripcionalmente por enzimas. La modificación de alguna de sus bases es crucial para la descodificación de ARNm y para mantener la estructura tridimensional del ARNt.

El ARN ribosómico o ribosomal (ARNr o RNAr) se halla combinado con proteínas para formar los ribosomas, donde representa unas 2/3 partes de los mismos. En procariotas, la subunidad mayor del ribosoma contiene dos moléculas de ARNr y la subunidad menor, una. En los eucariotas, la subunidad mayor contiene tres moléculas de ARNr y la menor, una. En ambos casos, sobre el armazón constituido por los ARNm se asocian proteínas específicas. El ARNr es muy abundante y representa el 80 % del ARN hallado en el citoplasma de las células eucariotas. Los ARN ribosómicos son el componente catalítico de los ribosomas; se encargan de crear los enlaces peptídicos entre los aminoácidos del polipéptido en formación durante la síntesis de proteínas; actúan, pues, como ribozimas.

Muchos tipos de ARN regulan la expresión génica gracias a que son complementarios de regiones específicas del ARNm o de genes del ADN.

Los ARN interferentes (ARNi o iRNA) son moléculas de ARN que suprimen la expresión de genes específicos mediante mecanismos conocidos globalmente como ribointerferencia o interferencia por ARN. Los ARN interferentes son moléculas pequeñas (de 20 a 25 nucléotidos) que se generan por fragmentación de precursores más largos. Se pueden clasificar en tres grandes grupos:

Los micro ARN (miARN o RNAmi) son cadenas cortas de 21 o 22 nucleótidos hallados en células eucariotas que se generan a partir de precursores específicos codificados en el genoma. Al transcribirse, se pliegan en horquillas intramoleculares y luego se unen a enzimas formando un complejo efector que puede bloquear la traducción del ARNm o acelerar su degradación comenzando por la eliminación enzimática de la cola poli A.

Los ARN interferentes pequeños (ARNip o siARN), formados por 20-25 nucleótidos, se producen con frecuencia por rotura de ARN virales, pero pueden ser también de origen endógeno. Tras la transcripción se ensambla en un complejo proteico denominado RISC ("RNA-induced silencing complex") que identifica el ARNm complementario que es cortado en dos mitades que son degradadas por la maquinaria celular, bloquean así la expresión del gen.

Los ARN asociados a Piwi son cadenas de 29-30 nucleótidos, propias de animales; se generan a partir de precursores largos monocatenarios (formados por una sola cadena), en un proceso que es independiente de Drosha y Dicer. Estos ARN pequeños se asocian con una subfamilia de las proteínas "Argonauta" denominada proteínas Piwi. Son activos las células de la línea germinal; se cree que son un sistema defensivo contra los transposones y que juegan algún papel en la gametogénesis.

Un ARN antisentido es la hebra complementaria (no codificadora) de un hebra ARNm (codificadora). La mayoría inhiben genes, pero unos pocos activan la transcripción. El ARN antisentido se aparea con su ARNm complementario formando una molécula de doble hebra que no puede traducirse y es degradada enzimáticamente. La introducción de un transgen codificante para un ARNm antisentido es una técnica usada para bloquear la expresión de un gen de interés. Un mARN antisentido marcado radioactivamente puede usarse para mostrar el nivel de transcripción de genes en varios tipos de células. Algunos tipos estructurales antisentidos son experimentales, ya que se usan como terapia antisentido.

Muchos ARN largos no codificantes (ARNnc largo o long ncARN) regulan la expresión génica en eucariotas; uno de ellos es el Xist que recubre uno de los dos cromosomas X en las hembras de los mamíferos inactivándolo (corpúsculo de Barr).
Diversos estudios revelan que es activo a bajos niveles. En determinadas poblaciones celulares, una cuarta parte de los genes que codifican para proteínas y el 80 % de los lncRNA detectados en el genoma humano están presentes en una o ninguna copia por célula, ya que existe una restricción en determinados ARN.

Un riboswitch es una parte del ARNm (ácido ribonucleico mensajero) al cual pueden unirse pequeñas moléculas que afectan la actividad del gen. Por tanto, un ARNm que contenga un riboswitch está directamente implicado en la regulación de su propia actividad que depende de la presencia o ausencia de la molécula señalizadora. Tales riboswitchs se hallan en la región no traducida 5' (5'-UTR), situada antes del codón de inicio (AUG), y/o en la región no traducida 3' (3'-UTR), también llamada secuencia de arrastre, situada entre el codón de terminación (UAG, UAA o UGA) y la cola poli A.

El ARN puede actuar como biocatalizador. Ciertos ARN se asocian a proteínas formando ribonucleoproteínas y se ha comprobado que es la subunidad de ARN la que lleva a cabo las reacciones catalíticas; estos ARN realizan las reacciones "in vitro" en ausencia de proteína. Se conocen cinco tipos de ribozimas; tres de ellos llevan a cabo reacciones de automodificación, como eliminación de intrones o autocorte, mientras que los otros (ribonucleasa P y ARN ribosómico) actúan sobre substratos distintos. Así, la ribonucleasa P corta un ARN precursor en moléculas de ARNt, mientras que el ARN ribosómico realiza el enlace peptídico durante la síntesis proteica ribosomal.

Los intrones son separados del pre-ARNm durante el proceso conocido como "splicing" por los espliceosomas, que contienen numerosos ARN pequeños nucleares (ARNpn o snRNA). En otros casos, los propios intrones actúan como ribozimas y se separan a sí mismos de los exones.

Los ARN pequeños nucleolares (ARNpno o snoRNA), hallados en el nucléolo y en los cuerpos de Cajal, dirigen la modificación de nucleótidos de otros ARN; el proceso consiste en transformar alguna de las cuatro bases nitrogenadas típicas (A, C, U, G) en otras. Los ARNpno se asocian con enzimas y los guían apareándose con secuencias específicas del ARN al que modificarán. Los ARNr y los ARNt contienen muchos nucleótidos modificados.

La mitocondrias tienen su propio aparato de síntesis proteica, que incluye ARNr (en los ribosomas), ARNt y ARNm.
Los ARN mitocondriales (ARNmt o mtARN) representan el 4 % del ARN celular total. Son transcritos por una ARN polimerasa mitocondrial específica.

El ADN es la molécula portadora de la información genética en todos los organismos celulares, pero, al igual que el ADN, el ARN puede guardar información genética. Los virus ARN carecen por completo de ADN y su genoma está formado por ARN, el cual codifica las proteínas del virus, como las de la cápside y algunos enzimas. Dichos enzimas realizan la replicación del genoma vírico. Los viroides son otro tipo de patógenos que consisten exclusivamente en una molécula de ARN que no codifica ninguna proteína y que es replicado por la maquinaria de la célula hospedadora.

La hipótesis del mundo de ARN propone que el ARN fue el primer ácido nucleico que apareció en la Tierra precediendo posteriormente al ADN y a su vez estos ácidos nucleicos junto con proteínas al unirse con liposomas formados espontáneamente originarían las primeras células. Se basa en la comprobación de que el ARN puede contener información genética, de un modo análogo a como lo hace el ADN, y que algunos tipos son capaces de llevar a cabo reacciones metabólicas, como autocorte o formación de enlaces peptídicos.

Durante años se especuló en qué fue primero, el ADN o las enzimas, ya que las enzimas se sintetizan a partir del ADN y la síntesis de ADN es llevada a cabo por enzimas. Si se supone que las primeras formas de vida usaron el ARN tanto para almacenar su información genética como realizar su metabolismo, se supera este escollo. Experimentos con los ribozimas básicos, como el ARN viral Q-beta, han demostrado que las estructuras de ARN autorreplicantes sencillas pueden resistir incluso a fuertes presiones selectivas (como los terminadores de cadena de quiralidad opuesta).



</doc>
<doc id="3260" url="https://es.wikipedia.org/wiki?curid=3260" title="Ribosoma">
Ribosoma

Los ribosomas son complejos supramoleculares de ácido ribonucleico (ARNr) y proteínas ribosómicas, constituyendo una máquina molecular que está presente en todas las células (excepto en los espermatozoides). Son los centros celulares de traducción que hacen posible la expresión de los genes. Es decir, se encargan de sintetizar proteínas a partir de la información contenida en el ADN, que llega transcrita a los ribosomas en forma específicamente de ARN mensajero (ARNm).

Los ribosomas son responsables de la síntesis de proteínas, en un proceso conocido como traducción. La información necesaria para esa síntesis se encuentra en el ARN mensajero (ARNm), cuya secuencia de nucleótidos, determina la secuencia de aminoácidos de la proteína. A su vez, la secuencia del ARNm proviene de la transcripción de un gen del ADN. El ARN de transferencia lleva los aminoácidos a los ribosomas donde se incorporan al polipéptido en crecimiento.

El ribosoma lee el ARN mensajero y ensambla los aminoácidos suministrados por los ARN de transferencia a la proteína en crecimiento, proceso conocido como traducción o síntesis de proteínas.

Todas las proteínas están formadas por aminoácidos. Entre los seres vivos se han descubierto hasta ahora 20 aminoácidos. En el código genético, cada aminoácido está codificado por uno o varios codones. En total hay 64 codones que codifican 20 aminoácidos y 3 señales de parada de la traducción. Esto hace que el código sea degenerado y que haya varios codones diferentes para un mismo aminoácido.

La traducción comienza, en general, con el codón AUG que codifica el aminoácido metionina. Al final de la secuencia se ubica un codón que indica el final de la proteína; es el codón de terminación. El código genético es "universal" porque cada codón codifica el mismo aminoácido para la mayoría de los organismos (no todos).

El ribosoma consta de dos partes, la subunidad mayor y una menor, estas salen del núcleo celular por separado. Las subunidades se mantienen unidas por cargas. Al disminuir experimentalmente la concentración de Mg, las subunidades tienden a separarse.

Por ejemplo, en el citoplasma de una célula eucariota, el proceso con la siguiente secuencia de ARN mensajero sería este:


Por tanto, la cadena polipeptídica ensamblada ha sido: Alanina-Asparagina-Glicina-Metionina-Prolina-Treonina.

Se les encuentra en el citosol, en las mitocondrias, en el retículo endoplasmático rugoso y en los cloroplastos. Solo son visibles al microscopio electrónico, debido a su reducido tamaño (29 nm en células procariotas y 32 nm en eucariotas). Bajo el microscopio electrónico se observan como estructuras redondeadas, densas a los electrones. Bajo el microscopio óptico se observa que son los responsables de la basofilia que presentan algunas células. Los "ribosomas" están considerados en muchos textos como orgánulos no membranosos, ya que no existen endomembranas en su estructura, aunque otros biólogos no los consideran orgánulos propiamente por esta misma razón.

Están formados por ARN ribosómico (ARNr) y por proteínas ribosómicas. Estructuralmente, tienen siempre dos subunidades: la mayor o grande y la menor o pequeña. En las células, estas macromoléculas aparecen en diferentes estados de disociación. Cuando están completas, pueden estar aisladas o formando grupos (polisomas). En células eucariotas, los ribosomas se elaboran en el núcleo pero desempeñan su función de síntesis en el citosol. Las proteínas sintetizadas por los ribosomas actúan principalmente en el citosol; también pueden aparecer asociados al retículo endoplasmático rugoso o a la membrana nuclear externa, y las proteínas que sintetizan son sobre todo para la secreción.

Tanto el ARNr como las subunidades de los ribosomas se suelen nombrar por su coeficiente de sedimentación en unidades Svedberg. En las células eucariotas, los ribosomas del citoplasma alcanzan 80 S. En plastos de eucariotas, así como en procariotas, son 70 S. Los ribosomas mitocondriales son de tamaño variado, entre 55 y 70 S.

En la célula procariota, tanto de bacterias como de arqueas, los ribosomas tienen un coeficiente de sedimentación de 70 S. Contienen un 66% de ARNr y se dividen en dos subunidades de distinto tamaño:



En la célula eucariota, los ribosomas tienen un coeficiente de sedimentación de 80 S. Su peso molecular es de 4.194 Kd. Contienen un 60% de ARNr y 40% de proteínas. Al igual que los procariotas se dividen en dos subunidades de distinto tamaño:



Los ribosomas mitocondriales o "mitorribosomas" junto con ARNt y ARNm, son parte del aparato propio de síntesis proteica que tienen las mitocondrias. Son de tamaño variable, desde los 50S de Leishmania hasta 72S en Candida. Los mitorribosomas de las células animales son 55S y sus dos tipos de ARN ribosómicos, el 12S y 16S, se transcriben a partir de genes del ADN mitocondrial, y son transcritos por una ARN polimerasa mitocondrial específica. Todas las proteínas que forman parte de los ribosomas mitocondriales están codificadas por genes del núcleo celular, que son traducidos en el citosol y transportados hasta las mitocondrias.

Los ribosomas que aparecen en plastos o "plastorribosomas" son similares a los procariotas. Son, al igual que los procariotas, 70 S, pero en la subunidad mayor hay un ARNr de 4 S que es equivalente al 5 S procariota.

La subunidad mayor 50S tiene unas 33 proteínas y la subunidad menor 30S tiene unas 25 proteínas. La gran mayoría de estas proteínas son homólogas (ortólogas) a las proteínas ribosomales bacterianas y unas pocas son específicas de los cloroplastos.

Los ribosomas fueron observados por primera ocasión por el biólogo celular rumano George Emil Palade a mediados de la década de 1950, para lo cual usó el microscopio electrónico. El término "ribosoma" fue propuesto por el científico Richard B. Roberts en 1958.

En 1974 Albert Claude, Christian de Duve y George Emil Palade recibieron el premio Nobel en la categoría de fisiología o medicina por su descubrimiento. El premio Nobel de química de 2009 fue entregado a Venkatraman Ramakrishnan, Thomas A. Steitz y Ada E. Yonath por motivo de la especificación detallada de la estructura y mecanismo de operación del ribosoma.

El ribosoma podría haber aparecido en un mundo de ARN, primero como un complejo autoreplicante que después evolucionó con la habilidad para encadenar aminoácidos. Estudios sugieren que un ribosoma compuesto únicamente de ARNr sería capaz de propiciar la formación de enlaces peptídicos. Adicionalmente, otras evidencias aducen la autosuficiencia genética de los ribosomas, característica ausente en el complejo aparato de replicación del ADN. Presummiblemente, el ARNr surgió como catalizador de su propia replicación, haciendo uso de su capacidad para luego codificar y sintetizar ARNt y proteínas para llevarla a cabo. Conforme fueron apareciendo aminoácidos en las condiciones prebióticas del mundo de ARN, sus interacciones con el ARN autocatalítico habrían conferido a este último con mayor alcance y eficiencia. Bajo esta rúbrica, la fuerza motora para la evolución del ribosoma actual a partir de una máquina autoreplicante arcaica podría haber sido la presión selectiva por incorporar proteínas a la maquinaria, de manera que aumentara su capacidad de autoreplicación.



</doc>
<doc id="3261" url="https://es.wikipedia.org/wiki?curid=3261" title="Célula eucariota">
Célula eucariota

Se llama eucariota —del griego "eu",'verdadero', y "karyon", ‘nuez’ o ‘núcleo’— a aquellas células que tienen un citoplasma compartimentado por membranas, y donde destaca la existencia de un núcleo celular organizado, cubierto por una envoltura nuclear, en el cual está conteniendo el material hereditario, que incluye al ADN que es la base de la herencia; se distinguen así de las células procariotas que carecen de núcleo definido, por lo que el material genético se encuentra disperso en su citoplasma. A los organismos formados por células eucariotas se les denomina eucariontes.

El paso de procariotas a eucariotas significó el gran salto en complejidad de la vida y uno de los más importantes de su evolución. Sin la complejidad que adquirieron las células eucariotas no habrían sido posibles ulteriores pasos como la aparición de los organismos pluricelulares; la vida, probablemente, se habría limitado a constituirse en un conglomerado de bacterias. De hecho, a excepción de procariontes (del que proceden), los cuatro reinos restantes (animales, plantas, hongos y protistas) son el resultado de ese salto cualitativo. El éxito de estas células eucariotas posibilitó las posteriores radiaciones adaptativas de la vida que han desembocado en la gran variedad de especies que existe en la actualidad.

Las células eucariotas presentan un citoplasma organizado en compartimentos, con orgánulos (semimembranosos) separados o interconectados, limitados por membranas biológicas que tienen la misma naturaleza que la membrana plasmática. El núcleo es el más notable y característico de los compartimentos en que se divide el protoplasma, es decir, la parte activa de la célula. En el núcleo se encuentra el material genético, el ADN. El ADN se encuentra distribuido en múltiples cromosomas y unido a proteínas, principalmente a proteínas cromosómicas llamadas histonas y porta toda la información necesaria para que se lleve a cabo todos los procesos tanto intracelulares como fuera de la célula, es decir, en el organismo en sí.

En el protoplasma se distinguen tres componentes principales a conocer: la membrana plasmática, el núcleo celular y el citoplasma, constituido por todo lo demás. Las células eucariotas están dotadas en su citoplasma de un citoesqueleto complejo, muy estructurado y dinámico, formado por microtúbulos y diversos filamentos proteicos. Además puede haber pared celular, que es lo típico de plantas, hongos y protistas pluricelulares, o algún otro tipo de recubrimiento externo al protoplasma.

Para su comparación con la célula procariota, véase la "Tabla comparativa"

Aunque las células eucariotas demuestran una diversidad increíble en su forma, comparten las características fundamentales de su organización celular, arriba resumidas, y una gran catálisis homogénea en lo relativo a su bioquímica (composición), y metabolismo, que contrasta con la inmensa heterogeneidad que en este terreno presentan los procariontes (bacteria en sentido amplio).

Las células eucariotas contienen en principio mitocondrias, orgánulos que habrían adquirido por endosimbiosis de ciertas bacterias primitivas, lo que les dota de la capacidad de desarrollar un metabolismo aerobio. Sin embargo, en algunas eucariotas del reino protistas las mitocondrias han desaparecido secundariamente en el curso de la evolución, en general derivando a otros orgánulos, como los hidrogenosomas.

Algunos eucariontes realizan la fotosíntesis, a diferencia de la célula animal, gracias a la presencia en su citoplasma de orgánulos llamados plastos, los cuales derivan por endosimbiosis de bacterias del grupo denominado cianobacterias (algas azules).

El origen de los eucariontes es un complejo proceso que tiene un origen procariota. Si bien hay varias teorías que explican este proceso, según la mayoría de estudios se produjo por endosimbiosis entre varios organismos procariotas, en donde el ancestro principal protoeucariota es de tipo arqueano y las mitocondrias y cloroplastos son de origen bacteriano. Es discutible la incorporación de otros organismos procariotas. La teoría más difundida al respecto es la endosimbiosis seriada, postulada por Lynn Margulis.
La teoría endosimbiótica (endo significa interno y simbionte se refiere a la relación de beneficio mutuo entre dos organismos). Esta interpretación no es extensiva al origen de la membrana nuclear, la cual se habría establecido a partir de una invaginación de la membrana celular.

Los organismos eucariotas forman el dominio Eukaryota que incluye a los organismos más conocidos, repartidos en cuatro reinos: Animalia (animales), Plantae (plantas), Fungi (Hongos) y Protista (que no pueden clasificarse dentro de los tres primeros reinos). Incluyen a la gran mayoría de los organismos extintos morfológicamente reconocibles que estudian los paleontólogos. Los ejemplos de la disparidad eucariótica van desde un dinoflagelado (un protista unicelular fotosintetizador), un árbol como la sequoia, un calamar, o un racimo de setas (órganos reproductivos de hongos), cada uno con células distintas y, en el caso de los pluricelulares, a menudo muy variadas.

Existen diversos tipos de células eucariotas entre las que destacan las células de animales y plantas. Los hongos y muchos protistas tienen, sin embargo, algunas diferencias substanciales.

Las células animales componen los tejidos de los animales y se distinguen de las células vegetales en que carecen de paredes celulares y de cloroplastos y poseen centriolos y vacuolas más pequeñas y, generalmente, más abundantes. Debido a la carencia de pared celular rígida, las células animales pueden adoptar variedad de formas e incluso pueden fagocitar otras estructuras.

Las características distintivas de las células de las plantas son:


Las células de los hongos, en su mayor parte, son similares a las células animales, con las excepciones siguientes:


Algunos organismos Protistas, son conformados por una única célula que pueden alcanzar tamaños macroscópicos (el organismo unicelular "Syringammina fragilissima" alcanza los 20 cm de diámetro).

Las células eucariotas se pueden reproducir de tres maneras distintas, principalmente:






</doc>
<doc id="3263" url="https://es.wikipedia.org/wiki?curid=3263" title="Codón">
Codón

La información genética, en el ARN, se escribe a partir de cuatro letras, que corresponden a las bases nitrogenadas (A, C, G y U), formando largas sucesiones de tripletes (conjunto de tres nucleótidos adyacentes). En el ARN, cada uno de estos tripletes consecutivos no solapados se denomina codón, que durante el proceso de traducción sufre una unión transitoria con el aminoacil-tRNA complementario dentro de los sitios de inserción del ribosoma, para establecer las fases de iniciación, elongación y terminación de la formación polipeptídica, además de un símbolo de puntuación (Comienzo, parada).

La estructura celular, de la que cada célula tiene muchas, que sintetiza las proteínas a partir de aminoácidos con la información contenida en el ARNm, leyendo los codones, es un agregado molecular complejo llamado ribosoma.

Un codón es un triplete de nucleótidos. En el código genético, cada aminoácido está codificado por uno o varios codones. El codón es la unidad de información básica en el proceso de traducción del ARNm. Cada uno de los codones codifica un aminoácido y esta correlación es la base del código genético que permite la traducción de la secuencia de ARNm a la secuencia de aminoácidos que compone la proteína. A toda la secuencia de codones de un gen, desde el codón de inicio hasta el último codón antes del de terminación, se le conoce como «Marco de Lectura Abierto» (ORF, por sus siglas en inglés), debido a que esta es la secuencia que se va a "leer" para dar lugar a un polipéptido.

Cada codón porta la información para pasar la secuencia de nucleótidos del ARNm a la secuencia de aminoácidos de la proteína en el proceso de traducción. Dado que cada codón codifica un aminoácido, hay 64 codones diferentes por combinación de los 4 nucleótidos en cada una de las 3 posiciones del triplete (ver tabla más abajo), de los cuales se codifican 20 aminoácidos,3 codones de terminación de la traducción y un codón de inicio de la traducción, el AUG, que codifica la metionina.
Salvo la metionina y el triptófano que están codificados por un único codón, los aminoácidos pueden estar codificados por 2, 3, 4 ó 6 codones diferentes. Esto hace que el código sea redundante, lo que se denomina código degenerado, porque hay varios codones diferentes que codifican para un solo aminoácido.
Los 3 codones de terminación conocidos como codón de terminación, codón de parada o codón stop llamados ocre (UAA), ámbar (UAG) y ópalo (UGA) son los tres tripletes que al no codificar ningún aminoácido ocasionan el cese de la síntesis proteica.
Hay un codón de inicio de la traducción, el AUG, que codifica la metionina, es el primer codón de una transcripción de ARNm traducido por un ribosoma.

Los codones que codifican un mismo aminoácido muchas veces tienen los dos primeros nucleótidos iguales, cambiando sólo el tercero. Así, cambios en el nucleótido de la tercera posición no suponen cambios en el aminoácido (mutaciones silenciosas). De este modo se minimiza el impacto de mutaciones puntuales cuando éstas ocurren en la tercera posición del codón. En cambio las mutaciones en la primera y segunda posición del codón suelen suponer un cambio de aminoácido (mutaciones "missense"). "." Normalmente los aminoácidos con las mismas características físico-químicas presentan el mismo nucleótido en la segunda posición del codón. Así los aminoácidos polares presentan adenina mientras que los apolares presentan uracilo. Mutaciones puntuales en la primera posición dan lugar a aminoácidos similares mientras que cambios en la segunda posición del codón, dan lugar a la incorporación de aminoácidos de propiedades muy diferentes. Mutaciones en cualquiera de las tres posiciones del codón pueden dar lugar a la aparición de codones stop provocando una terminación de la traducción prematura lo que ocasiona que se traduzca una proteína incompleta y, en la mayoría de los casos, no funcional (mutaciones "nonsense)."



</doc>
<doc id="3264" url="https://es.wikipedia.org/wiki?curid=3264" title="Ácido aspártico">
Ácido aspártico

El ácido aspártico o su forma ionizada, el aspartato (símbolos Asp y D) es uno de los veinte aminoácidos con los que las células forman las proteínas. En el ARN se encuentra codificado por los codones GAU o GAC.
Presenta un grupo carboxilo (-COOH) en el extremo de la cadena lateral. Su fórmula química es .

A pH fisiológico, tiene una carga negativa (es ácido); pertenece al grupo de aminoácidos con cadenas laterales polares cargadas. No es un aminoácido esencial ya que puede ser sintetizado por el organismo humano.
Su biosíntesis tiene lugar por transaminación del ácido oxalacético, un metabolito intermediario del ciclo de Krebs.

El ácido aspártico fue descubierto en 1827 por los químicos franceses Auguste-Arthur Plisson y Étienne Ossian Henry, derivado de la asparagina, que había sido aislada a partir de jugo de espárragos en 1806, por ebullición con una base.

El aspartato no es esencial en mamíferos, siendo producido a partir del oxalacetato por una reacción de transaminación. También se sintetiza del dietil sodio eftalimidomalonato, ().

El aspartato participa en la formación de glutamato a través de la glutamato-aspartato transaminasa citosólica.

El aspartato es también un metabolito del ciclo de la urea y participa en la gluconeogénesis. 

El mecanismo de inactivación es la recaptación. Se han descrito distintos sistemas de transporte en las membranas neuronales y gliales. En la neurona está el EGAC1, que transporta glutamato y aspartato. En la célula glial está el GLAST (aspartato-glutamato). Estos sistemas de transporte son dependientes de sodio e independiente de cloro.
y relaciones.

Los receptores para aspartato son un mundo muy complejo. Los hay ionotrópicos y metabotrópicos. Estimula los receptores NMDA, aunque no tan fuertemente como la hace el glutamato.

El aspartato es uno de los aminoácidos que actúan como neurotransmisores. Su función como neurotrasmisor es de carácter excitatorio del SNC.





</doc>
<doc id="3265" url="https://es.wikipedia.org/wiki?curid=3265" title="Catálisis">
Catálisis

La catálisis es el proceso por el cual se aumenta la velocidad de una reacción química, debido a la participación de una sustancia llamada catalizador y aquellas que desactivan la catálisis son denominados inhibidores. Una característica importante es que la masa de catalizador no se modifica durante la reacción química, lo que lo diferencia de un reactivo, cuya masa va disminuyendo a lo largo de la reacción.

En la síntesis de muchos de los productos químicos industriales más importantes existe una catálisis, ya que esta puede disminuir el tiempo que requiere. El envenenamiento de los catalizadores, que generalmente es un proceso no deseado, también es utilizado en la industria química. Por ejemplo, en la reducción del etino a eteno, el catalizador paladio (Pd) es "envenenado" parcialmente con acetato de plomo (II), Pb(CHCOO). Sin la desactivación del catalizador, el eteno producido se reduciría posteriormente a etano.

La catálisis interviene en muchos procesos industriales. Así mismo, la mayoría de los procesos “biológicamente” significativos son catalizados. La investigación en catálisis es uno de los principales campos en ciencia aplicada e involucra muchas áreas de la química, especialmente en química organometálica y ciencia de materiales. La catálisis es importante para muchos aspectos de las ciencias ambientales, por ejemplo, el convertidor catalítico de los automóviles y la dinámica del agujero de ozono. Las reacciones catalíticas son las preferidas en la química verde para un medioambiente amigable debido a la reducida cantidad de residuos que genera en lugar de las reacciones estequiométricas en las que se consumen todos los reactivos y se forman más productos secundarios. El catalizador más común es el protón (H). Muchos metales de transición y los complejos de los metales de transición se utilizan en la catálisis. Los catalizadores llamados enzimas son importantes en Biología.

El catalizador funciona proporcionando un camino de reacción alternativo al producto de reacción. La velocidad de la reacción aumenta a medida que esta ruta alternativa tiene una menor energía de activación que la ruta de reacción no mediada por el catalizador. La dismutación del peróxido de hidrógeno para dar agua y oxígeno es una reacción que está fuertemente afectada por los catalizadores:
Esta reacción está favorecida, en el sentido de que los productos de reacción son más estables que el material de partida, sin embargo, la reacción no catalizada es lenta. La descomposición del peróxido de hidrógeno es de hecho tan lenta que las soluciones de peróxido de hidrógeno están disponibles comercialmente. Tras la adición de una pequeña cantidad de dióxido de manganeso, el peróxido de hidrógeno reacciona rápidamente de acuerdo a la ecuación anterior. Este efecto se ve fácilmente por la efervescencia del oxígeno. El dióxido de manganeso puede ser recuperado sin cambios, y volver a utilizarse de forma indefinida, y por lo tanto no se consume en la reacción. En consecuencia, el dióxido de manganeso "cataliza esta reacción".

La característica general de la catálisis es que la reacción catalítica tiene un menor cambio de energía libre de la etapa limitante hasta el estado de transición que la reacción no catalizada correspondiente, resultando en una mayor velocidad de reacción a la misma temperatura. Sin embargo, el origen mecánico de la catálisis es complejo.

Los catalizadores pueden afectar favorablemente al entorno de reacción, por ejemplo, los catalizadores ácidos para las reacciones de los compuestos carbonílicos forman compuestos intermedios específicos que no se producen naturalmente, tales como los ésteres de Osmio en la dihidroxilación de alquenos catalizadas por el tetróxido de osmio, o hacer la ruptura de los reactivos a formas reactivas, como el hidrógeno atómico en la hidrogenación catalítica.

Cinéticamente, las reacciones catalíticas se comportan como las reacciones químicas típicas, es decir, la velocidad de reacción depende de la frecuencia de contacto de los reactivos en la etapa determinante de velocidad (ver ecuación de Arrhenius). Normalmente, el catalizador participa en esta etapa lenta, y las velocidades están limitadas por la cantidad de catalizador. En catálisis heterogénea, la difusión de los reactivos a la superficie de contacto y la difusión de los productos desde dicha superficie puede ser la etapa determinante de la velocidad. Eventos similares relacionados con la unión del sustrato y la disociación del producto se aplican en la catálisis homogénea.

Aunque los catalizadores no son consumidos por la propia reacción, pueden resultar inhibidos, desactivados o destruidos por procesos secundarios. En la catálisis heterogénea, procesos secundarios típicos incluyen el coqueo, donde el catalizador se cubre por productos secundarios poliméricos. Además, los catalizadores heterogéneos pueden disolverse en la solución en un sistema sólido-líquido o evaporarse en un sistema sólido-gas.

Los catalizadores generalmente reaccionan con uno o más de los reactivos para formar productos intermedios que, posteriormente, conducen al producto final de reacción. En el proceso se regenera el catalizador. El siguiente esquema es típico de una reacción catalítica, donde C representa el catalizador, X e Y son los reactivos, y Z es el producto de la reacción de X con Y:

Aunque el catalizador es consumido por la reacción 1, posteriormente es producido por la reacción 4, por lo que la reacción global es:
Como el catalizador se regenera en una reacción, a menudo bastan pequeñas cantidades del catalizador para incrementar la velocidad de una reacción. Sin embargo, en la práctica los catalizadores son algunas veces consumidos en procesos secundarios.

Como ejemplo de este proceso, en 2008, investigadores daneses revelaron por primera vez la secuencia de sucesos cuando el oxígeno y el hidrógeno se combinan en la superficie del dióxido de titanio (TiO, o "titania") para producir agua. Con una serie de imágenes de microscopía de efecto túnel a intervalos, determinaron que las moléculas sufren adsorción, disociación y difusión antes de reaccionar. Los estados intermedios de reacción fueron: HO, HO, luego HO y el producto final de la reacción (dímeros de la molécula de agua), tras lo cual la molécula de agua se desorbe de la superficie del catalizador.

Los catalizadores funcionan proporcionando un mecanismo (alternativo) que involucra un estado de transición diferente y una menor energía de activación. Por lo tanto, más colisiones moleculares tienen la energía necesaria para alcanzar el estado de transición. En consecuencia, los catalizadores permiten reacciones que de otro modo estarían bloqueadas o ralentizadas por una barrera cinética. El catalizador puede aumentar la velocidad de reacción o de la selectividad, o permitir que la reacción ocurra a menores temperaturas. Este efecto puede ser ilustrado con una distribución de Boltzmann y un diagrama de perfil de energía.

Los catalizadores no cambian el rendimiento de una reacción: no tienen efecto en el equilibrio químico de una reacción, debido a que la velocidad, tanto de la reacción directa como de la inversa, se ven afectadas (véase también termodinámica). El hecho de que un catalizador no cambie el equilibrio es una consecuencia de la segunda ley de la termodinámica. Supongamos que hay un catalizador que modifica el equilibrio. La introducción del catalizador en el sistema daría lugar a la reacción para ir de nuevo al equilibrio, produciendo energía. La producción de energía es un resultado necesario, puesto que las reacciones son espontáneas sí y solo sí se produce energía libre de Gibbs, y si no hay una barrera energética no hay necesidad de un catalizador. En consecuencia, la eliminación del catalizador también resultaría en una reacción, produciendo energía; esto es, tanto la adición, como su proceso inverso, la eliminación, producirían energía. Así, un catalizador que pudiera cambiar el equilibrio sería un móvil perpetuo, en contradicción con las leyes de la termodinámica.

Si un catalizador cambia el equilibrio, entonces debe consumirse a medida que avanza la reacción, y por lo tanto también es un reactivo. Algunos ejemplos ilustrativos son la hidrólisis de los ésteres catalizada por bases, donde el ácido carboxílico producido reacciona inmediatamente con el catalizador básico, y así el equilibrio de la reacción se desplaza hacia la hidrólisis.

La unidad derivada SI para medir la actividad catalítica de un catalizador es el katal, que es igual a moles por segundo. La actividad de un catalizador puede ser descrita por el número de conversiones, o TON (del inglés "turn over number"), y la eficiencia catalítica por la "frecuencia de conversiones", TOF (del inglés "turn over frequency"). El equivalente bioquímico es la unidad de actividad enzimática. Para más información sobre la eficiencia de la catálisis enzimática, ver el artículo de Catálisis enzimática.

El catalizador estabiliza el estado de transición más que de los que estabiliza el material inicial. Disminuye la barrera cinética al disminuir la "diferencia" de energía entre el material inicial y el estado de transición.

La naturaleza química de los catalizadores es tan diversa como la catálisis misma, aunque pueden hacerse algunas generalizaciones. Los ácidos próticos son probablemente los catalizadores más ampliamente usados, especialmente para muchas reacciones que involucran agua, incluyendo la hidrólisis y su inversa. Los sólidos multifuncionales a menudo suelen ser catalíticamente activos, por ejemplo las zeolitas, la alúmina y ciertas formas de carbono grafítico. Los metales de transición son utilizados a menudo para catalizar reacciones redox (oxigenación, hidrogenación). Muchos procesos catalíticos, especialmente los que involucran hidrógeno, requieren metales del grupo del platino.

Algunos de los llamados catalizadores son, en realidad, precatalizadores. Los precatalizadores se convierten en el catalizador en el transcurso de la reacción. Por ejemplo, el catalizador de Wilkinson RhCl(PPh) pierde un ligando trifenilfosfina antes de entrar en el verdadero ciclo catalítico. Los precatalizadores son más fáciles de almacenar, pero son fácilmente activados "in situ". Debido a esta etapa de preactivación, muchas reacciones catalíticas involucran un período de inducción.

Las especies químicas que mejoran la actividad catalítica son denominadas co-catalizadores o promotores, en la catálisis cooperativa.

Los catalizadores pueden ser homogéneos o heterogéneos, dependiendo de si existe un catalizador en la misma fase que el sustrato. Los biocatalizadores son vistos a menudo como un grupo separado.
Son aquellos que realizan procesos mediante enzimas, estos son más empleados en las áreas industriales, una de ellas es la farmacéutica, en la cual un ejemplo es para la producción de insulina, la cual se obtiene al emplear la enzima de la bacteria "E. coli."

Los catalizadores heterogéneos son aquellos que actúan en una fase diferente que los reactivos. La mayoría de los catalizadores heterogéneos son sólidos que actúan sobre sustratos en una mezcla de reacción líquida o gaseosa. Se conocen diversos mecanismos para las reacciones en superficies, dependiendo de cómo se lleva a cabo la adsorción (Langmuir-Hinshelwood, Eley -Rideal, y Mars-van Krevelen). El área superficial total del sólido tiene un efecto importante en la velocidad de reacción. Cuanto menor sea el tamaño de partícula del catalizador, mayor es el área superficial para una masa dada de partículas.

Por ejemplo, en el proceso de Haber, el hierro finamente dividido sirve como un catalizador para la síntesis de amoníaco a partir de nitrógeno e hidrógeno. Los gases reactantes se adsorben en los "sitios activos" de las partículas de hierro. Una vez adsorbidos, los enlaces dentro de las moléculas reaccionantes se resienten, y se forman nuevos enlaces entre los fragmentos generados, en parte debido a su proximidad. De esta manera el particularmente fuerte triple enlace en el nitrógeno se debilita y los átomos de hidrógeno y nitrógeno se combinan más rápido de lo que lo harían el caso en la fase gaseosa, por lo que la velocidad de reacción aumenta.

Los catalizadores heterogéneos suelen estar "soportados", que significa que el catalizador se encuentra disperso en un segundo material que mejora la eficacia o minimiza su costo. A veces el soporte es más que una superficie sobre la que se transmite el catalizador para aumentar el área superficial. Más a menudo, el soporte y el catalizador interactúan, afectando a la reacción catalítica.

Normalmente los catalizadores homogéneos están disueltos en un disolvente con los sustratos. Un ejemplo de catálisis homogénea implica la influencia de H en la esterificación de los ésteres, por ejemplo, acetato de metilo a partir del ácido acético y el metanol. Para los químicos inorgánicos, la catálisis homogénea es a menudo sinónimo de catalizadores organometálicos.

En el contexto de la electroquímica, específicamente en la ingeniería de las pilas de combustible, que contienen varios metales los catalizadores se utilizan para mejorar las velocidades de las semirreacciones que conforman la pila de combustible. Un tipo común de electrocatalizador de pila de combustible se basa en nanopartículas de platino que están soportadas en partículas un poco mayores de carbón. Cuando este electrocatalizador de platino está en contacto con uno de los electrodos en una pila de combustible, aumenta la velocidad de reducción del oxígeno a agua (o hidróxido o peróxido de hidrógeno).

Mientras que los metales de transición a veces atraen más la atención en el estudio de la catálisis, las moléculas orgánicas que no contengan metales también pueden poseer propiedades catalíticas. Normalmente, los catalizadores orgánicos requieren una mayor carga (o cantidad de catalizador por unidad de cantidad de reactivo) que los catalizadores basados en metales de transición, pero estos catalizadores suelen estar disponibles comercialmente en grandes cantidades, ayudando a reducir los costos. A principios de los 2000, los organocatalizadores fueron considerados una "nueva generación" y eran competidores de los tradicionales catalizadores que contenían metales. Las reacciones enzimáticas operan a través de los principios de la catálisis orgánica.

El principio de la nanocatálisis se basa en la premisa de que los materiales catalíticos aplicados en la nanoescala tienen mejores propiedades, en comparación con lo que exhiben en una macroescala.
Se estima que el 90% de todos los productos químicos producidos comercialmente involucran catalizadores en alguna etapa del proceso de su fabricación. En 2005, los procesos catalíticos generaron cerca de 900.000 millones de dólares en productos de todo el mundo. La catálisis es tan penetrante que las subáreas no son fácilmente clasificables. Algunas áreas de particular concentración 

El refinado de petróleo hace un uso intensivo de la catálisis para la alquilación, craqueo catalítico (rotura de hidrocarburos de cadena larga en trozos más pequeños), reformado de nafta y el reformado con vapor (conversión de hidrocarburos en gas de síntesis). Incluso los gases de combustión de la quema de combustibles fósiles es tratada a través de la catálisis: convertidores catalíticos, normalmente compuestos de platino y rodio, rompen algunos de los subproductos más nocivos de los gases de escape de los automóviles.
Con respecto a los combustibles sintéticos, un viejo pero importante proceso es el síntesis de Fischer-Tropsch de hidrocarburos a partir del gas de síntesis, que a su vez se procesa a través de la reacción de cambio agua-gas, catalizada por el hierro. El Biodiésel y los biocombustibles relacionados requieren un procesamiento tanto a través de los catalizadores inorgánicos como de los biocatalizadores.

Las pilas de combustible se basan en catalizadores de las reacciones tanto anódicas como catódicas.

Algunos de los productos químicos obtenidos a gran escala se producen a través de la oxidación catalítica, a menudo usando oxígeno. Algunos ejemplos son el ácido nítrico (a partir de amoníaco), el ácido sulfúrico (a partir de dióxido de azufre a trióxido de azufre por el proceso de las cámaras de plomo), el ácido tereftálico a partir de p-xileno, el ácido acrílico a partir de propileno o propano, y el acrilonitrilo a partir de propano y amoníaco.

Muchos otros productos químicos son generados por reducción a gran escala, a menudo a través de hidrogenación. El ejemplo a mayor escala es el amoníaco, que se prepara a través del proceso de Haber a partir de nitrógeno. El Metanol es preparado a partir de monóxido de carbono.

Los polímeros a granel derivados de etileno y propileno se preparan a menudo a través de la catálisis Ziegler-Natta. Los poliésteres, las poliamidas, y los isocianatos se obtienen a través de la catálisis ácido-base.

La mayoría de los procesos de carbonilación requieren catalizadores metálicos, los ejemplos incluyen la síntesis de ácido acético mediante el proceso Monsanto y la hidroformilación.

Muchos productos de química fina se preparan a través de la catálisis, los métodos incluyen a los de la industria pesada, así como procesos más especializados que serían prohibitivamente caros a gran escala. Algunos ejemplos son la metátesis de olefinas usando el catalizador de Grubbs, la reacción de Heck, y la reacción de Friedel-Crafts.

Debido a que la mayoría de los compuestos bioactivos son quirales, muchos productos farmacéuticos son producidos por catálisis enantioselectiva.

Una de las aplicaciones más obvias de la catálisis es la hidrogenación (reacción con el hidrógeno gas) de las grasas usando níquel como catalizador para producir la margarina. Muchos otros productos alimenticios se preparan a través de biocatálisis (véase más abajo).

En la naturaleza, las enzimas son catalizadores en el metabolismo y el catabolismo. La mayoría de biocatalizadores están basados en proteínas, es decir, enzimas, pero otras clases de biomoléculas también exhiben propiedades catalíticas incluyendo las ribozimas, y de desoxirribozimas sintéticas.

Los biocatalizadores se pueden considerar como intermedio entre los catalizadores homogéneos y los heterogéneos, aunque estrictamente hablando las enzimas solubles son catalizadores homogéneos y las enzimas enlazadas a membrana son heterogéneas. Varios factores afectan la actividad de las enzimas (y otros catalizadores), incluyendo la temperatura, el pH, la concentración de la enzima, el sustrato y los productos. Un reactivo particularmente importante en las reacciones enzimáticas es el agua, que es el producto de muchas de las reacciones en que se forman enlaces y un reactivo en muchos procesos en que se rompen enlaces.

Las enzimas se emplean para preparar los productos químicos básicos, incluyendo el jarabe de maíz y la acrilamida.

La catálisis tiene un impacto en el medio ambiente mediante el aumento de la eficiencia de los procesos industriales, pero la catálisis también juega un papel directo en el medio ambiente. Un ejemplo notable es el papel catalítico de los radicales libres en la destrucción del ozono. Estos radicales se forman por la acción de la radiación ultravioleta sobre los clorofluorocarburos (CFC)

En un sentido general, cualquier cosa que aumenta la velocidad de un proceso es un "catalizador", un término derivado del griego, que significa "anular", "desatar", o "recoger". La frase "procesos catalizados" fue acuñada por Jöns Jakob Berzelius en 1836 para describir las reacciones que son aceleradas por sustancias que permanecen sin cambios después de la reacción. Otro de los primeros químicos involucrados en la catálisis fue Alexander Mitscherlich quien se refirió a los "procesos de contacto" y Johann Wolfgang Döbereiner que habló de "acción de contacto" y cuyo encendedor basado en hidrógeno y una esponja de platino se convirtieron en un gran éxito comercial en la década de 1820. Humphry Davy descubrió el uso de platino en la catálisis. En la década de 1880, Wilhelm Ostwald en la Universidad de Leipzig inició una investigación sistemática de las reacciones que eran catalizadas por la presencia de los ácidos y las bases, y encontró que las reacciones químicas ocurren a una velocidad finita y que estas velocidades pueden utilizarse para determinar la fuerza de ácidos y bases. Por este trabajo, Ostwald fue galardonado en 1909 con el Premio Nobel de Química.

Las sustancias que reducen la acción de los catalizadores son llamadas inhibidores catalíticos si son reversibles, y venenos catalíticos si son irreversibles. Los promotores son sustancias que aumentan la actividad catalítica, en particular cuando no son catalizadores en sí mismos.

El inhibidor puede modificar la selectividad además de la velocidad. Por ejemplo, en la reducción del etino a eteno, el catalizador es paladio (Pd), parcialmente "envenenado" con acetato de plomo (II) (Pb(CHCOO)). Sin la desactivación del catalizador, el etileno producido se reducirá aún más, hasta etano.

El inhibidor puede producir este efecto por ejemplo, envenenando selectivamente solo a ciertos tipos de sitios activos. Otro mecanismo es la modificación de la geometría de la superficie. Por ejemplo, en las operaciones de hidrogenación, grandes planchas de superficie metálica funcionan como lugares de catálisis hidrogenolítica mientras que los sitios que catalizan la hidrogenación de los insaturados son menores. Así, un veneno que cubre la superficie al azar tienden a reducir el número de grandes planchas no contaminada, pero dejan proporcionalmente más sitios pequeños libres, así se cambia la hidrogenación frente a la hidrogenolisis selectiva. También son posibles otros muchos mecanismos.

La figura muestra el diagrama de una reacción catalizada, mostrando como varía la energía (E) de las moléculas que participan en la reacción durante el proceso de reacción (tiempo, t). Todas las moléculas contienen una cantidad determinada de energía, que depende del número y del tipo de enlaces presentes en ella. Los sustratos o reactivos (A y B) tienen una energía determinada, y el o los productos (AB en el gráfico), otra.

Si la energía total de los sustratos es mayor que la de los productos (por ejemplo como se muestra en el diagrama), una reacción exotérmica, y el exceso de energía se desprende en forma de calor. Por el contrario, si la energía total de los sustratos es menor que la de los productos, se necesita tomar energía del exterior para que la reacción tenga lugar, lo que se denomina reacción endotérmica.

Cuando las moléculas de los sustratos se van acercando para reaccionar, pierden estabilidad (usando una analogía antropomórfica, a las moléculas "les gusta" mantener su espacio vital, y las intromisiones no son bienvenidas). La inestabilidad se manifiesta como un aumento de la energía del sistema (es el pico de energía que se ve en el diagrama). Cuando los sustratos se convierten en productos, las moléculas se separan y se relajan de nuevo, y el conjunto se estabiliza.

Las enzimas catalizan las reacciones estabilizando el intermedio de la reacción, de manera que el "pico" de energía necesario para pasar de los sustratos a los productos es menor. El resultado final es que hay muchas más moléculas de sustrato que chocan y reaccionan para dar lugar a los productos, y la reacción transcurre en general más deprisa. Un catalizador puede catalizar tanto reacciones endotérmicas como exotérmicas, porque en los dos casos es necesario superar una barrera energética. El catalizador (E) crea un microambiente en el que A y B pueden alcanzar el estado intermedio (A...E...B) más fácilmente, reduciendo la cantidad de energía necesaria (E2). Como resultado, la reacción es más fácil, optimizando la velocidad de dicha reacción.

Los catalizadores no alteran el equilibrio químico propio de la reacción en ningún caso.




</doc>
<doc id="3266" url="https://es.wikipedia.org/wiki?curid=3266" title="Enzima">
Enzima

Las enzimas

Debido a que las enzimas son extremadamente selectivas con sus sustratos y su velocidad crece solo con algunas reacciones, el conjunto ("set") de enzimas presentes en una célula determina el tipo de metabolismo que tiene esa célula. A su vez, esta presencia depende de la regulación de la expresión génica correspondiente a la enzima.

Como todos los catalizadores, las enzimas funcionan disminuyendo la energía de activación (ΔG) de una reacción, de forma que la presencia de la enzima acelera sustancialmente la tasa de reacción. Las enzimas no alteran el balance energético de las reacciones en que intervienen, ni modifican, por lo tanto, el equilibrio de la reacción, pero consiguen acelerar el proceso incluso en escalas de millones de veces. Una reacción que se produce bajo el control de una enzima, o de un catalizador en general, alcanza el equilibrio mucho más deprisa que la correspondiente reacción no catalizada.

Al igual que ocurre con otros catalizadores, las enzimas no son consumidas en las reacciones que catalizan, ni alteran su equilibrio químico. Sin embargo, las enzimas difieren de otros catalizadores por ser más específicas. La gran diversidad de enzimas existentes catalizan alrededor de 4000 reacciones bioquímicas distintas. No todos los catalizadores bioquímicos son proteínas, pues algunas moléculas de ARN son capaces de catalizar reacciones (como la subunidad 16S de los ribosomas en la que reside la actividad peptidil transferasa). También cabe nombrar unas moléculas sintéticas denominadas enzimas artificiales capaces de catalizar reacciones químicas como las enzimas clásicas.

La actividad de las enzimas puede ser afectada por otras moléculas. Los inhibidores enzimáticos son moléculas que disminuyen o impiden la actividad de las enzimas, mientras que los activadores son moléculas que incrementan dicha actividad. Asimismo, gran cantidad de enzimas requieren de cofactores para su actividad. Muchas drogas o fármacos son moléculas inhibidoras. Igualmente, la actividad es afectada por la temperatura, el pH, la concentración de la propia enzima y del sustrato, y otros factores físico-químicos.

Muchas enzimas son usadas comercialmente, por ejemplo, en la síntesis de antibióticos o de productos domésticos de limpieza. Además, son ampliamente utilizadas en diversos procesos industriales, como son la fabricación de alimentos, destinción de vaqueros o producción de biocombustibles.

Desde finales del siglo XVIII y principios del siglo XIX, se conocía la digestión de la carne por las secreciones del estómago y la conversión del almidón en azúcar por los extractos de plantas y la saliva. Sin embargo, no había sido identificado el mecanismo subyacente. La primera enzima fue descubierta por Anselme Payen y Jean-François Persoz en 1833.

En el siglo XIX, cuando se estaba estudiando la fermentación del azúcar en el alcohol con levaduras, Louis Pasteur llegó a la conclusión de que esta fermentación era catalizada por una fuerza vital contenida en las células de la levadura, llamadas fermentos, e inicialmente se pensó que solo funcionaban con organismos vivos. Escribió que ""la fermentación del alcohol es un acto relacionado con la vida y la organización de las células de las levaduras, y no con la muerte y la putrefacción de las células"". Por el contrario, otros científicos de la época como Justus von Liebig, se mantuvieron en la posición que defendía el carácter puramente químico de la reacción de fermentación.

En 1878 el fisiólogo Wilhelm Kühne (1837-1900) acuñó el término "enzima", que viene del griego "ενζυμον" "en levadura", para describir este proceso. La palabra enzima fue usada después para referirse a sustancias inertes como la pepsina. Por otro lado, la palabra "fermento" solía referirse a la actividad química producida por organismos vivientes.

En 1897 Eduard Buchner comenzó a estudiar la capacidad de los extractos de levadura para fermentar azúcar a pesar de la ausencia de células vivientes de levadura. En una serie de experimentos en la Universidad Humboldt de Berlín, encontró que el azúcar era fermentado inclusive cuando no había elementos vivos en los cultivos de células de levaduras. Llamó a la enzima que causa la fermentación de la sacarosa, “zimasa”. En 1907 recibió el ""por sus investigaciones bioquímicas y el haber descubierto la fermentación libre de células"". Siguiendo el ejemplo de Buchner, las enzimas son usualmente nombradas de acuerdo a la reacción que producen. Normalmente, el sufijo "-asa" es agregado al nombre del sustrato (p. ej., la lactasa es la enzima que degrada lactosa) o al tipo de reacción (p. ej., la ADN polimerasa forma polímeros de ADN).

Tras haber mostrado que las enzimas pueden funcionar fuera de una célula viva, el próximo paso era determinar su naturaleza bioquímica. En muchos de los trabajos iniciales se notó que la actividad enzimática estaba asociada con proteínas, pero algunos científicos (como el premio Nobel Richard Willstätter) argumentaban que las proteínas eran simplemente el transporte para las verdaderas enzimas y que las proteínas "per se" no eran capaces de realizar catálisis. Sin embargo, en 1926, James B. Sumner demostró que la enzima ureasa era una proteína pura y la cristalizó. Summer hizo lo mismo con la enzima catalasa en 1937. La conclusión de que las proteínas puras podían ser enzimas fue definitivamente probada por John Howard Northrop y Wendell Meredith Stanley, quienes trabajaron con diversas enzimas digestivas como la pepsina (1930), la tripsina y la quimotripsina. Estos tres científicos recibieron el Premio Nobel de Química en 1946.

El descubrimiento de que las enzimas podían ser cristalizadas permitía que sus estructuras fuesen resueltas mediante técnicas de cristalografía y difracción de rayos X. Esto se llevó a cabo en primer lugar con la lisozima, una enzima encontrada en las lágrimas, la saliva y los huevos, capaces de digerir la pared de algunas bacterias. La estructura fue resuelta por un grupo liderado por David Chilton Phillips y publicada en 1965. Esta estructura de alta resolución de las lisozimas, marcó el comienzo en el campo de la biología estructural y el esfuerzo por entender cómo las enzimas trabajan en el orden molecular.

Las enzimas son generalmente proteínas globulares que pueden presentar tamaños muy variables, desde 62 aminoácidos como en el caso del monómero de la 4-oxalocrotonato tautomerasa, hasta los 2500 presentes en la sintasa de ácidos grasos.

Las actividades de las enzimas vienen determinadas por su estructura tridimensional, la cual viene a su vez determinada por la secuencia de aminoácidos. Sin embargo, aunque la estructura determina la función, predecir una nueva actividad enzimática basándose únicamente en la estructura de una proteína es muy difícil, y un problema aún no resuelto.

Casi todas las enzimas son mucho más grandes que los sustratos sobre los que actúan, y solo una pequeña parte de la enzima (alrededor de 3 a 4 aminoácidos) está directamente involucrada en la catálisis. La región que contiene estos residuos encargados de catalizar la reacción es denominada "centro activo". Las enzimas también pueden contener sitios con la capacidad de unir cofactores, necesarios a veces en el proceso de catálisis, o de unir pequeñas moléculas, como los sustratos o productos (directos o indirectos) de la reacción catalizada. Estas uniones de la enzima con sus propios sustratos o productos pueden incrementar o disminuir la actividad enzimática, dando lugar así a una regulación por retroalimentación positiva o negativa, según el caso.

Al igual que las demás proteínas, las enzimas se componen de una cadena lineal de aminoácidos que se pliegan durante el proceso de traducción para dar lugar a una estructura terciaria tridimensional de la enzima, susceptible de presentar actividad. Cada secuencia de aminoácidos es única y por tanto da lugar a una estructura única, con propiedades únicas. En ocasiones, proteínas individuales pueden unirse a otras proteínas para formar complejos, en lo que se denomina estructura cuaternaria de las proteínas.

La mayoría de las enzimas, al igual que el resto de las proteínas, pueden ser desnaturalizadas si se ven sometidas a agentes desnaturalizantes como el calor, los pHs extremos o ciertos compuestos como el SDS. Estos agentes destruyen la estructura terciaria de las proteínas de forma reversible o irreversible, dependiendo de la enzima y de la condición. Una consecuencia de la desnaturalización es la pérdida o merma de la función, de la capacidad enzimática.

Las enzimas suelen ser muy específicas tanto del tipo de reacción que catalizan como del sustrato involucrado en la reacción. La forma, la carga y las características hidrofílicas/hidrofóbicas de las enzimas y los sustratos son los responsables de dicha especificidad. La constante de especificidad, es una medida de la eficiencia de una enzima, ya que la velocidad de la reacción se encuentra directamente relacionada con la frecuencia con la que se encuentran las moléculas de enzima y sustrato. Las enzimas también pueden mostrar un elevado grado de estereoespecificidad, regioselectividad y quimioselectividad.

Algunas de estas enzimas que muestran una elevada especificidad y precisión en su actividad son aquellas involucrados en la replicación y expresión del genoma. Estas enzimas tienen eficientes sistemas de comprobación y corrección de errores, como en el caso de la ADN polimerasa, que cataliza una reacción de replicación en un primer paso, para comprobar posteriormente si el producto obtenido es el correcto. Este proceso, que tiene lugar en dos pasos, da como resultado una media de tasa de error increíblemente baja, en torno a 1 error cada 100 millones de reacciones en determinadas polimerasas de mamíferos. Este tipo de mecanismos de comprobación también han sido observados en la ARN polimerasa, en la ARNt aminoacil sintetasa y en la actividad de selección de los aminoacil-tRNAs.

Aquellas enzimas que producen metabolitos secundarios son denominadas promiscuas, ya que pueden actuar sobre una gran variedad de sustratos. Por ello, se ha sugerido que esta amplia especificidad de sustrato podría ser clave en la evolución y diseño de nuevas rutas biosintéticas.

Las enzimas son muy específicas, como sugirió Emil Fischer en 1894. Con base en sus resultados dedujo que ambas moléculas, la enzima y su sustrato, poseen complementariedad geométrica, es decir, sus estructuras encajan exactamente una en la otra, por lo que este modelo ha sido denominado como modelo de la «llave-cerradura», refiriéndose a la enzima como a una especie de cerradura y al sustrato como a una llave que encaja de forma perfecta en dicha cerradura. Una llave sólo funciona en su cerradura y no en otras cerraduras. Sin embargo, si bien este modelo explica la especificidad de las enzimas, falla al intentar explicar la estabilización del estado de transición que logran adquirir las enzimas.

En 1958, Daniel Koshland sugiere una modificación al modelo de la llave-cerradura: las enzimas son estructuras bastante flexibles y así el sitio activo podría cambiar su conformación estructural por la interacción con el sustrato. Como resultado de ello, la cadena aminoacídica que compone el sitio activo es moldeada en posiciones precisas, lo que permite a la enzima llevar a cabo su función catalítica. En algunos casos, como en las glicosidasas, el sustrato cambia ligeramente de forma para entrar en el sitio activo. El sitio activo continua dicho cambio hasta que el sustrato está completamente unido, momento en el cual queda determinada la forma y la carga final.

Las enzimas pueden actuar de diversas formas, como se verá a continuación, siempre dando lugar a una disminución del valor de ΔG:

Cabe destacar que este efecto entrópico implica la desestabilización del estado basal, y su contribución a la catálisis es relativamente pequeña.

La comprensión del origen de la reducción del valor de ΔG en una reacción enzimática requiere elucidar previamente cómo las enzimas pueden estabilizar su estado de transición, más que el estado de transición de la reacción. Aparentemente, la forma más efectiva para alcanzar la estabilización es la utilización de fuerzas electrostáticas, concretamente, poseyendo un ambiente polar relativamente fijado que pueda orientarse hacia la distribución de carga del estado de transición. Ese tipo de ambientes no existen ni se generan en ausencia de enzimas.

La dinámica interna de las enzimas está relacionada con sus mecanismos de catálisis. La dinámica interna se define como el movimiento de diferentes partes de la estructura de la enzima, desde residuos individuales de aminoácidos, hasta grupos de aminoácidos o incluso un dominio proteico entero. Estos movimientos se producen a diferentes escalas de tiempo que van desde femtosegundos hasta segundos. Casi cualquier residuo de la estructura de la enzima puede contribuir en el proceso de catálisis por medio de movimientos dinámicos. Los movimientos de las proteínas son vitales en muchas enzimas. Dichos movimientos podrán ser más o menos importantes según si los cambios conformacionales se producen por vibraciones pequeñas y rápidas o grandes y lentas, y dicha importancia dependerá del tipo de reacción que lleve a cabo la enzima. Sin embargo, aunque estos movimientos son importantes en el proceso de unión y liberación de sustratos y productos, aún no está claro si estos movimientos ayudan a acelerar los pasos químicos de las reacciones enzimáticas. Estos nuevos avances también tienen implicaciones en la comprensión de los efectos alostéricos y en el desarrollo de nuevos fármacos.

Los sitios alostéricos son zonas de la enzima con capacidad de reconocer y unir determinadas moléculas en la célula. Las uniones a las que dan lugar son débiles y no covalentes, y generan un cambio en la conformación estructural de la enzima que repercute en el sitio activo, afectando así a la velocidad de reacción. Las interacciones alostéricas pueden tanto inhibir como activar enzimas, y son una forma muy común de controlar las enzimas en las células.

Algunas enzimas no precisan ningún componente adicional para mostrar una total actividad. Sin embargo, otras enzimas requieren la unión de moléculas no proteicas denominadas cofactores para poder ejercer su actividad. Los cofactores pueden ser compuestos inorgánicos, como los iones metálicos y los complejos ferrosulfurosos, o compuestos orgánicos, como la flavina o el grupo hemo. Los cofactores orgánicos pueden ser a su vez grupos prostéticos, que se unen fuertemente a la enzima, o coenzimas, que son liberados del sitio activo de la enzima durante la reacción. Las coenzimas incluyen compuestos como el NADH, el NADPH y el adenosín trifosfato. Estas moléculas transfieren grupos funcionales entre enzimas.

Un ejemplo de una enzima que contiene un cofactor es la anhidrasa carbónica, en la cual el zinc (cofactor) se mantiene unido al sitio activo, tal y como se muestra en la figura anterior (situada al inicio de la sección "Estructuras y mecanismos"). Estas moléculas suelen encontrarse unidas al sitio activo y están implicadas en la catálisis. Por ejemplo, la flavina y el grupo hemo suelen estar implicados en reacciones redox.

Las enzimas que requieren un cofactor pero no lo tienen unido son denominadas "apoenzimas" o "apoproteínas". Una apoenzima junto con cofactor(es) es denominada "holoenzima" (que es la forma activa). La mayoría de los cofactores no se unen covalentemente a sus enzimas, pero sí lo hacen fuertemente. Sin embargo, los grupos prostéticos pueden estar covalentemente unidos, como en el caso de la tiamina pirofosfato en la enzima piruvato deshidrogenasa. El término "holoenzima" también puede ser aplicado a aquellas enzimas que contienen múltiples subunidades, como en el caso de la ADN polimerasa, donde la holoenzima es el complejo con todas las subunidades necesarias para llevar a cabo la actividad enzimática.

Las coenzimas son pequeñas moléculas orgánicas que transportan grupos químicos de una enzima a otra. Algunos de estos compuestos, como la riboflavina, la tiamina y el ácido fólico son vitaminas (las cuales no pueden ser sintetizados en cantidad suficiente por el cuerpo humano y deben ser incorporados en la dieta). Los grupos químicos intercambiados incluyen el ion hidruro (H) transportado por NAD o NADP, el grupo fosfato transportado por el ATP, el grupo acetilo transportado por la coenzima A, los grupos formil, metenil o metil transportados por el ácido fólico y el grupo metil transportado por la S-Adenosil metionina.

Debido a que las coenzimas sufren una modificación química como consecuencia de la actividad enzimática, es útil considerar a las coenzimas como una clase especial de sustratos, o como segundos sustratos, que son comunes a muchas enzimas diferentes. Por ejemplo, se conocen alrededor de 700 enzimas que utilizan la coenzima NADH.

Las coenzimas suelen estar continuamente regenerándose y sus concentraciones suelen mantenerse a unos niveles fijos en el interior de la célula: por ejemplo, el NADPH es regenerado a través de la ruta de las pentosas fosfato y la "S"-Adenosil metionina por medio de la metionina adenosiltransferasa. Esta regeneración continua significa que incluso pequeñas cantidades de coenzimas son utilizadas intensivamente. Por ejemplo, el cuerpo humano gasta su propio peso en ATP cada día.

Al igual que sucede con todos los catalizadores, las enzimas no alteran el equilibrio químico de la reacción. Generalmente, en presencia de una enzima, la reacción avanza en la misma dirección en la que lo haría en ausencia de enzima, solo que más rápido. Sin embargo, en ausencia de enzima, podría producirse una reacción espontánea que generase un producto diferente debido a que en esas condiciones, dicho producto diferente se forma más rápidamente.

Además, las enzimas pueden acoplar dos o más reacciones, por lo que una reacción termodinámicamente favorable puede ser utilizada para favorecer otra reacción termodinámicamente desfavorable. Por ejemplo, la hidrólisis de ATP suele ser utilizada para favorecer otras reacciones químicas.

Las enzimas catalizan reacciones químicas tanto en un sentido como en el contrario. Nunca alteran el equilibrio, sino únicamente la velocidad a la que es alcanzado. Por ejemplo, la anhidrasa carbónica cataliza su reacción en una u otra dirección dependiendo de la concentración de los reactantes, como se puede ver a continuación:

Si el equilibrio se ve muy desplazado en un sentido de la reacción, es decir, se convierte en una reacción muy exergónica, la reacción se hace efectivamente irreversible. Bajo estas condiciones, la enzima únicamente catalizará la reacción en la dirección permitida desde un punto de vista termodinámico.

La cinética enzimática es el estudio de cómo las enzimas se unen a sus sustratos y los transforman en productos. Los datos de equilibrios utilizados en los estudios cinéticos son obtenidos mediante ensayos enzimáticos.

En 1902, Victor Henri propuso una teoría cuantitativa sobre la cinética enzimática, pero sus datos experimentales no fueron muy útiles debido a que la importancia de la concentración del ion de hidrógeno aún no era considerada. Después de que Peter Lauritz Sørensen definiera la escala logarítmica del pH e introdujera el concepto de "tampón" ("buffer") en 1909, el químico alemán Leonor Michaelis y su postdoctoral canadiense Maud Leonora Menten repitieron los experimentos de Henri confirmando su ecuación, que actualmente es conocida como cinética de Henri-Michaelis-Menten (o simplemente cinética de Michaelis-Menten). Su trabajo fue desarrollado más en profundidad por George Edward Briggs y J. B. S. Haldane, quienes obtuvieron las ecuaciones cinéticas que se encuentran tan ampliamente extendidas en la actualidad.

La mayor contribución de Henri fue la idea de dividir las reacciones enzimáticas en dos etapas. En la primera, el sustrato se une reversiblemente a la enzima, formando el complejo enzima-sustrato (también denominado complejo Michaelis). En la segunda, la enzima cataliza la reacción y libera el producto.

Las enzimas pueden catalizar hasta varios millones de reacciones por segundo. Por ejemplo, la descarboxilación no enzimática de la orotidina 5'-monofosfato tiene una vida media de 78 millones de años. Sin embargo, cuando la enzima orotidina 5'-fosfato descarboxilasa está presente en el medio, ese mismo proceso tarda apenas 25 milisegundos. Las velocidades de las enzimas dependen de las condiciones de la solución y de la concentración de sustrato. Aquellas condiciones que desnaturalizan una proteína, como temperaturas elevadas, pHs extremos o altas concentraciones de sal, dificultan o impiden la actividad enzimática, mientras que elevadas concentraciones de sustrato tienden a incrementar la actividad. Para encontrar la máxima velocidad de una reacción enzimática, la concentración de sustrato se incrementa hasta que se obtiene una tasa constante de formación de producto (véase la curva de saturación representada en la figura de la derecha). La saturación ocurre porque, cuando la concentración de sustrato aumenta, disminuye la concentración de enzima libre, que se convierte en la forma con sustrato unido (ES). A la máxima velocidad ("V") de la enzima, todos los sitios activos de dicha enzima tienen sustrato unido, y la cantidad de complejos ES es igual a la cantidad total de enzima.
Sin embargo, "V" es solo una de las constantes cinéticas de la enzima. La cantidad de sustrato necesario para obtener una determinada velocidad de reacción también es importante. Este parámetro viene dado por la constante de Michaelis-Menten ("K"), que viene a ser la concentración de sustrato necesaria para que una enzima alcance la mitad de su velocidad máxima. Cada enzima tiene un valor de "K" característico para un determinado sustrato, el cual puede decirnos cómo de afín es la unión entre el sustrato y la enzima. Otra constante útil es "k", que es el número de moléculas de sustrato procesadas por cada sitio activo por segundo.

La eficiencia de una enzima puede ser expresada en términos de "k"/"K", en lo que se denomina constante de especificidad, que incorpora la constante de velocidad de todas las fases de la reacción. Debido a que la constante de especificidad contempla tanto la afinidad como la capacidad catalítica, es un parámetro muy útil para comparar diferentes enzimas o la misma enzima con diferentes sustratos. El valor máximo teórico de la constante de especificidad es denominado límite de difusión tiene un valor de 10-10 (M s). Llegados a este punto, cada colisión de la enzima con su sustrato da lugar a la catálisis, con lo que la velocidad de formación de producto no se ve limitada por la velocidad de reacción, sino por la velocidad de difusión. Las enzimas que poseen esta propiedad son llamadas "enzimas catalíticamente perfectas" o "cinéticamente perfectas". Ejemplos de este tipo de enzimas son la triosa fosfato isomerasa, la anhidrasa carbónica, la acetilcolinesterasa, la catalasa, la fumarasa, la beta-lactamasa y la superóxido dismutasa.

La cinética de Michaelis-Menten depende de la ley de acción de masas, que se deriva partiendo de los supuestos de difusión libre y colisión al azar. Sin embargo, muchos procesos bioquímicos o celulares se desvían significativamente de estas condiciones, a causa de fenómenos como el crowding macromolecular, la separación de etapas entre enzima-sustrato-producto, o los movimientos moleculares uni- o bidimensionales. No obstante, en estas situaciones se puede aplicar una cinética de Michaelis-Menten fractal.

Algunas enzimas presentan una cinética más rápida que la velocidad de difusión, lo que en principio parecería ser imposible. Se han propuesto diversos mecanismos para tratar de explicar este fenómeno. Uno de los modelos propone que algunas proteínas podrían tener la capacidad de acelerar la catálisis secuestrando el sustrato y orientándolo mediante campos eléctricos dipolares. Otro modelo propone un mecanismo de efecto túnel cuántico, donde un protón o un electrón pueden formar un túnel a través de barreras de activación, aunque existe cierta controversia en cuanto al efecto túnel que pueda generar un protón. El efecto túnel mediado por protones ha sido observado en triptamina. Esto sugiere que la catálisis enzimática podría ser definida más exactamente como una "barrera", en lugar de como hace el modelo tradicional, donde el sustrato requiere a la enzima para alcanzar una barrera energética más baja.

Los inhibidores son moléculas que regulan la actividad enzimática, inhibiendo su actividad. A grandes rasgos, pueden clasificarse en reversibles e irreversibles. Las irreversibles se unen covalentemente a la enzima sin posibilidad de revertir la modificación, siendo útiles en farmacología. Algunos de los fármacos que actúan de este modo son la eflornitina, utilizada para tratar la tripanosomiasis africana, la penicilina y la aspirina.

Las reversibles se unen de forma reversible a la enzima, pudiendo clasificarse a su vez, según la forma en que intervienen en la reacción, en competitivas, acompetitivas y mixtas. Habitualmente, por su amplia presencia en multitud de procesos, se habla también de inhibición no competitiva, que en realidad no es más que una variante de la ya mencionada inhibición mixta. Sin embargo, por sus características se suele presentar como opuesta a la competitiva, con la que es comparada frecuentemente.

En muchos organismos, los inhibidores pueden actuar como parte de un mecanismo de realimentación. Si una enzima produce una sustancia en demasiada cantidad en el organismo, esta misma sustancia podría actuar como un inhibidor de la enzima al inicio de la ruta que lo produce, deteniendo así dicha producción cuando haya una cantidad suficiente de la sustancia en cuestión. Este sería una forma de realimentación negativa. Las enzimas que se encuentran sujetas a este tipo de regulación suelen ser multiméricas y poseer sitios alostéricos donde se unen sustancias reguladoras. Las gráficas que representan la velocidad de la reacción frente a la concentración de sustrato de estas enzimas no son hipérboles, sino sigmoidales (forma de S).

Debido a que los inhibidores modulan la función de las enzimas, suelen ser utilizados como fármacos. Un típico ejemplo de un inhibidor que es utilizado como fármaco es la aspirina, la cual inhibe las enzimas COX-1 y COX-2 implicadas en la síntesis de un intermediario inflamatorio, las prostaglandinas, con lo que suprime así los efectos derivados, el dolor y la inflamación. Sin embargo, otros inhibidores enzimáticos actúan como venenos. Por ejemplo, el cianuro es un inhibidor irreversible que se une a los átomos de hierro y cobre en el sitio activo de la citocromo c oxidasa de células animales (las plantas son resistentes al cianuro), bloqueando así la respiración celular.

Las enzimas presentan una amplia variedad de funciones en los organismos vivos. Son indispensables en la transducción de señales y en procesos de regulación, normalmente por medio de quinasas y fosfatasas. También son capaces de producir movimiento, como es el caso de la miosina al hidrolizar ATP para generar la contracción muscular o el movimiento de vesículas por medio del citoesqueleto. Otro tipo de ATPasas en la membrana celular son las bombas de iones implicadas en procesos de transporte activo. Además, las enzimas también están implicadas en funciones mucho más exóticas, como la producción de luz por la luciferasa en las luciérnagas. Los virus también pueden contener enzimas implicadas en la infección celular, como es el caso de la integrasa del virus HIV y de la transcriptasa inversa, o en la liberación viral, como la neuraminidasa del virus de la gripe.

Una importante función de las enzimas es la que presentan en el sistema digestivo de los animales. Enzimas tales como las amilasas y las proteasas son capaces de degradar moléculas grandes (almidón o proteínas, respectivamente) en otras más pequeñas, de forma que puedan ser absorbidas en el intestino. Las moléculas de almidón, por ejemplo, que son demasiado grandes para ser absorbidas, son degradadas por diversas enzimas a moléculas más pequeñas como la maltosa, y finalmente a glucosa, la cual sí puede ser absorbida a través de las células del intestino. Diferentes enzimas digestivas son capaces de degradar diferentes tipos de alimentos. Los rumiantes que tienen una dieta herbívora, poseen en sus intestinos una serie de microorganismos que producen otra enzima, la celulasa, capaz de degradar la celulosa presente en la pared celular de las plantas.

Varias enzimas pueden actuar conjuntamente en un orden específico, creando así una ruta metabólica. En una ruta metabólica, una enzima toma como sustrato el producto de otra enzima. Tras la reacción catalítica, el producto se transfiere a la siguiente enzima y así sucesivamente. En ocasiones, existe más de una enzima capaz de catalizar la misma reacción en paralelo, lo que permite establecer una regulación más sofisticada: por ejemplo, en el caso en que una enzima presenta una actividad constitutiva pero con una baja constante de actividad y una segunda enzima cuya actividad es inducible, pero presenta una mayor constante de actividad.

Las enzimas determinan los pasos que siguen estas rutas metabólicas. Sin las enzimas, el metabolismo no se produciría a través de los mismos pasos, ni sería lo suficientemente rápido para atender las necesidades de la célula. De hecho, una ruta metabólica como la glucólisis no podría existir sin enzimas. La glucosa, por ejemplo, puede reaccionar directamente con el ATP de forma que quede fosforilada en uno o más carbonos. En ausencia de enzimas, esta reacción se produciría tan lentamente que sería insignificante. Sin embargo, si se añade la enzima hexoquinasa que fosforila el carbono 6 de la glucosa y se mide la concentración de la mezcla en un breve espacio de tiempo se podrá encontrar únicamente glucosa-6-fosfato a niveles significativos. Por tanto, las redes de rutas metabólicas dentro de la célula dependen del conjunto de enzimas funcionales que presenten.

La actividad enzimática puede ser controlada en la célula principalmente de estas cinco formas:

Debido a que es necesario un fuerte control de la actividad enzimática para la homeostasis, cualquier fallo en el funcionamiento (mutación, incremento o reducción de la expresión o deleción) de una única enzima crítica puede conducir al desarrollo de una enfermedad genética. La importancia de las enzimas se pone de manifiesto en el hecho de que una enfermedad letal puede ser causada por el mal funcionamiento de un único tipo de enzima de todos los miles de tipos que existen en nuestro cuerpo.

Un ejemplo de esto es el tipo más común de fenilcetonuria. En esta enfermedad genética se produce una mutación de un único aminoácido en la fenilalanina hidroxilasa, una enzima que cataliza la primera reacción de la ruta de degradación de la fenilalanina y de compuestos relacionados. Al ser esta enzima inactiva, se acumulan una serie de productos que terminan dando lugar a la aparición de retardo mental si no se recibe tratamiento.

Otro ejemplo es cuando se produce una mutación en los genes de la línea germinal que codifican las enzimas implicadas en la reparación del ADN. En este caso, al no repararse adecuadamente el ADN de las células, se acumulan mutaciones que suelen derivar en el desarrollo de diversos tipos de cáncer hereditarios, como la xerodermia pigmentosa.

El nombre de una enzima suele derivarse del sustrato o de la reacción química que cataliza, con la palabra terminada en -asa. Por ejemplo, lactasa proviene de su sustrato lactosa; alcohol deshidrogenasa proviene de la reacción que cataliza que consiste en "deshidrogenar" el alcohol; ADN polimerasa proviene también de la reacción que cataliza que consiste en polimerizar el ADN.

La Unión Internacional de Bioquímica y Biología Molecular ha desarrollado una nomenclatura para identificar a las enzimas basada en los denominados Números EC. De este modo, cada enzima queda registrada por una secuencia de cuatro números precedidos por las letras "EC". El primer número clasifica a la enzima según su mecanismo de acción. A continuación se indican las seis grandes clases de enzimas existentes en la actualidad:

Las enzimas son utilizadas en la industria química, y en otros tipos de industria, en donde se requiere el uso de catalizadores muy especializados. Sin embargo, las enzimas están limitadas tanto por el número de reacciones que pueden llevar a cabo como por su ausencia de estabilidad en solventes orgánicos y altas temperaturas. Por ello, la ingeniería de proteínas se ha convertido en un área de investigación muy activa donde se intentan crear enzimas con propiedades nuevas, bien mediante diseño racional, bien mediante evolución "in vitro". Estos esfuerzos han comenzado a tener algunos éxitos, obteniéndose algunas enzimas que catalizan reacciones no existentes en la naturaleza.

A continuación se muestra una tabla con diversas aplicaciones industriales de las enzimas:




</doc>
<doc id="3267" url="https://es.wikipedia.org/wiki?curid=3267" title="Oxígeno">
Oxígeno

El oxígeno es un elemento químico de número atómico 8 y representado por el símbolo O. Su nombre proviene de las raíces griegas ὀξύς (oxys) («ácido», literalmente «punzante», en referencia al sabor de los ácidos) y –γόνος (-gonos) («productor», literalmente «engendrador»; es decir, "productor de ácidos"), porque en la época en que se le dio esta denominación se creía, incorrectamente, que todos los ácidos requerían oxígeno para su composición. En condiciones normales de presión y temperatura, dos átomos del elemento se enlazan para formar el dioxígeno, un gas diatómico incoloro, inodoro e insípido con fórmula O. Esta sustancia constituye una importante parte de la atmósfera y resulta necesaria para sostener la vida terrestre.

Forma parte del grupo de los anfígenos en la tabla periódica y es un elemento no metálico altamente reactivo que forma fácilmente compuestos (especialmente óxidos) con la mayoría de elementos, excepto con los gases nobles helio y neón. Asimismo, es un fuerte agente oxidante y tiene la segunda electronegatividad más alta de todos los elementos, solo superada por el flúor. Medido por su masa, el oxígeno es el tercer elemento más abundante del universo, tras el hidrógeno y el helio, y el más abundante en la corteza terrestre ya que forma, prácticamente, la mitad de su masa. Debido a su reactividad química, no puede permanecer en la atmósfera terrestre como elemento libre sin ser reabastecido constantemente por la acción fotosintética de los organismos que utilizan la energía solar para producir oxígeno elemental a partir del agua. El oxígeno elemental O solamente empezó a acumularse en la atmósfera después de la aparición de estos organismos, aproximadamente hace 2500 millones de años. El oxígeno diatómico constituye el 20,8 % del volumen de la atmósfera terrestre.

Dado que constituye la mayor parte de la masa del agua, es también el componente mayoritario de la masa de los seres vivos. Muchas de las moléculas más importantes que forman parte de los seres vivos, como las proteínas, los ácidos nucleicos, los carbohidratos y los lípidos, contienen oxígeno, así como los principales compuestos inorgánicos que forman los caparazones, dientes y huesos animales. El oxígeno elemental se produce por cianobacterias, algas y plantas y todas las formas complejas de vida lo usan para su respiración celular. Resulta tóxico para los organismos de tipo anaerobio obligado, las formas tempranas de vida que predominaban en la Tierra hasta que el O comenzó a acumularse en la atmósfera. Otra forma (alótropa) del oxígeno, el ozono (O), ayuda a proteger la biosfera de la radiación ultravioleta a gran altitud, en la llamada capa de ozono, pero es contaminante cerca de la superficie, donde es un subproducto del esmog. A altitudes aún mayores de la órbita baja terrestre, el oxígeno atómico tiene una presencia significativa y causa erosión en las naves espaciales.

Carl Wilhelm Scheele descubrió el oxígeno de forma independiente en Upsala en 1773, o incluso antes, y Joseph Priestley, en Wiltshire en 1774, pero el honor suele adjudicársele a Priestley debido a que publicó su trabajo antes. Antoine Lavoisier, cuyas investigaciones ayudaron a desacreditar la entonces popular teoría del flogisto de combustión y corrosión, acuñó el nombre «oxígeno» en 1777. Este se produce industrialmente mediante la destilación fraccionada de aire licuado, el uso de zeolita con ciclos de presión para concentrar el oxígeno del aire, la electrólisis del agua y otros medios. El oxígeno se utiliza en la producción de acero, plásticos y textiles; los propulsores de cohetes; la oxigenoterapia; y la asistencia para la respiración en aeronaves, submarinos, vuelos espaciales y submarinismo.

En condiciones normales de presión y temperatura, el oxígeno es un gas incoloro e inodoro con fórmula molecular O, en el que dos átomos de oxígeno se enlazan con una configuración electrónica en estado triplete. Este enlace tiene un orden de enlace de dos y se suele simplificar en las descripciones como un enlace doble o como una combinación de un enlace de dos electrones y dos enlaces de tres electrones.

El oxígeno triplete —no debe confundirse con el ozono, O— es el estado fundamental de la molécula O, que cuenta con dos electrones desparejados que ocupan dos orbitales moleculares degenerados. Estos orbitales se clasifican como antienlaces —debilitan el orden de enlace de tres a dos—, de manera que el enlace del dioxígeno es más débil que el triple enlace del nitrógeno diatómico, en el que todos los orbitales de los enlaces moleculares se rellenan, pero algunos orbitales de antienlace no lo están.

En su forma normal de triplete, las moléculas de O son paramagnéticas; es decir, que en presencia de un campo magnético forman un imán, debido al momento magnético del espín de los electrones desparejados en la molécula y la interacción de canje negativa entre moléculas de O contiguas. Un imán atrae al oxígeno líquido hasta tal punto que, en demostraciones de laboratorio, un hilo de oxígeno líquido puede sostenerse contra su propio peso entre los polos de un imán potente.

El oxígeno molecular singlete es un nombre dado a varias especies de O de mayor energía, en las que todos los espínes de los electrones se emparejan. Es mucho más reactivo con moléculas orgánicas habituales que el oxígeno molecular en sí mismo. En la naturaleza, el oxígeno singlete se suele formar con el agua en la fotosíntesis, usando la energía solar. También se produce en la troposfera a causa de la fotolisis del ozono por la luz de onda corta, así como por el sistema inmunitario como una fuente de oxígeno activo. En los organismos fotosintéticos —y posiblemente también en los animales—, los carotenoides juegan un papel fundamental en la absorción de energía del oxígeno singlete y la conversión de este a su estado no excitado antes de que pueda causar daño a los tejidos.
Energía de disociación de las moléculas diatómicas O-X a 25°C en kJ/mol (en condiciones de laboratorio):
El alótropo más normal del oxígeno elemental es el llamado dioxígeno (O), que tiene una longitud de enlace de 121 pm y una energía de enlace de 498 kJ•mol. Esta es la forma que usan las formas de vida complejas, como los animales, en su respiración celular (véase rol biológico) y es la forma que tiene una gran importancia en la composición de la atmósfera terrestre (véase Abundancia).

El trioxígeno (O) se conoce habitualmente como ozono y es un alótropo muy reactivo, dañino para el tejido pulmonar. El ozono se produce en la atmósfera superior cuando el O se combina con el oxígeno atómico a causa de la división del O por la radiación ultravioleta. Ya que el ozono es un poderoso absorbente en la región ultravioleta del espectro electromagnético, la capa de ozono de la atmósfera superior funciona como un escudo protector de la radiación que recibe el planeta. Cerca de la superficie terrestre, no obstante, es un contaminante formado como subproducto de las emisiones de automóviles. La molécula metaestable del tetraoxígeno (O) no fue descubierta hasta 2001, y se dio por descontado que existía en una de las seis fases del oxígeno sólido. En 2006 se demostró que esta fase, creada mediante la presurización del O a 20 GPa, es, de hecho, un clúster O de sistema trigonal. Este clúster tiene potencial para ser un oxidante mucho más potente que el O y el O y podría, por tanto, ser usado como propulsor de cohetes. En 1990 se descubrió una fase metálica cuando el oxígeno sólido se somete a una presión superior a 96 GPa y se demostró en 1998 que a temperaturas muy bajas se convierte en superconductor.

El oxígeno es más soluble en agua que el nitrógeno; esta contiene aproximadamente una molécula de O por cada dos moléculas de N, comparado con la proporción en la atmósfera, que viene a ser de 1:4. La solubilidad del oxígeno en el agua depende de la temperatura, disolviéndose alrededor del doble (14,6 mg•L) a 0 °C que a 20 °C (7,6 mg•L). A 25 °C y 1 atmósfera de presión, el agua dulce contiene alrededor de 6,04 mililitros (ml) de oxígeno por litro, mientras que el agua marina contiene alrededor de 4,95 ml por litro. A 5 °C la solubilidad se incrementa hasta 9,0 ml (un 50 % más que a 25 °C) por litro en el agua y 7,2 ml (45 % más) en el agua de mar.

El oxígeno se condensa a 90,20 K (−182,95 °C, −297,31 °F) y se congela a 54,36 K (−218,79 °C, −361,82 °F). Tanto el O líquido como el sólido son sustancias con un suave color azul cielo causado por la absorción en el rojo, en contraste con el color azul del cielo, que se debe a la dispersión de Rayleigh de la luz azul. El O líquido de gran pureza se suele obtener a través de la destilación fraccionada de aire licuado. El oxígeno líquido también puede producirse por condensación del aire, usando nitrógeno líquido como refrigerante. Es una sustancia altamente reactiva y debe separarse de materiales inflamables.

El oxígeno que encontramos en la naturaleza se compone de tres isótopos estables: O, O y O, de los que el O es el más abundante (99,762 % de abundancia natural).

La mayor parte del O se sintetiza al final del proceso de combustión del helio en una estrella masiva, pero otra parte se produce en el proceso de combustión del neón. El O surge fundamentalmente por la combustión del hidrógeno en helio durante el ciclo CNO, convirtiéndolo en un isótopo común en las zonas de combustión de hidrógeno en las estrellas. Por su parte, la mayoría del O se produce cuando el N —que abunda debido a la combustión CNO— captura un núcleo de He, lo que origina una gran abundancia de O en las zonas ricas en helio de las estrellas masivas.

Se han caracterizado catorce radioisótopos, de los que los más estables son el O con un periodo de semidesintegración de 70,606 segundos. Todos los restantes isótopos radiactivos tienen periodos de semidesintegración inferiores a 27 segundos y la mayor parte de estos, inferiores a 83 milisegundos. La forma de descomposición de los isótopos más ligeros que el O es la descomposición β para producir nitrógeno y, para los más pesados que el O, la desintegración beta para formar flúor.

El oxígeno es el elemento químico más abundante, por masa, en la biosfera, el aire, el mar y el suelo terrestres. Es, asimismo, el tercero más abundante en el universo, tras el hidrógeno y el helio. Alrededor del 0,9 % de la masa del Sol es oxígeno, que constituye también el 49,2 % de la masa de la corteza terrestre y es el principal componente de los océanos de la Tierra (88,8 % de su masa total). El oxígeno gaseoso es el segundo componente más abundante en la atmósfera terrestre, ya que supone un 20,8 % de su volumen y el 23,1 % de su masa (unas 10 toneladas). La Tierra es una excepción entre los planetas del Sistema Solar por la alta concentración de oxígeno gaseoso en su atmósfera; por ejemplo, Marte (con un 0,1 % de O del total de su volumen) y Venus tienen concentraciones mucho menores. Sin embargo, el O que rodea a estos planetas proviene exclusivamente de la reacción que sufren moléculas que contienen oxígeno, como el dióxido de carbono, por efecto de la radiación ultravioleta.

La inusualmente alta concentración de oxígeno gaseoso en la Tierra es el resultado del ciclo de circulación. Este ciclo biogeoquímico describe el movimiento del oxígeno en el interior de sus tres principales reservas en el planeta: la atmósfera, la biosfera y la litosfera. El factor de conducción más importante en este ciclo es la fotosíntesis, responsable de la atmósfera moderna de la Tierra, que libera oxígeno en la atmósfera, mientras que los procesos de respiración y descomposición lo eliminan. En el equilibrio actual, la producción y el consumo tienen lugar con un ratio aproximado de 1/2000 de la totalidad del oxígeno atmosférico por año.

El oxígeno no combinado también se da en soluciones en las masas de agua del planeta. La mayor solubilidad del O a baja temperatura (véase Propiedades físicas) tiene implicaciones importantes para la vida marina, ya que los océanos polares sostienen una densidad de vida mucho mayor debido a su superior contenido de oxígeno. La cantidad de O en el agua puede haberse visto reducida por la contaminación hídrica, debido a la acción de la descomposición de las algas y otros biomateriales por un proceso llamado eutrofización. Los científicos evalúan este aspecto de la calidad del agua a través de la medición de su demanda biológica de oxígeno, o cantidad de O necesaria para restaurarla a una concentración normal.

El oxígeno es liberado por las bacterias fotosintéticas, las algas y las plantas mediante la fotosíntesis. En el proceso inverso los organismos aerobios, mediante la respiración, usan el oxígeno para convertir los nutrientes en energía (ATP). La disminución de oxígeno provoca hipoxemia y su falta total, anoxia, lo que puede provocar la muerte del organismo.

En la naturaleza el oxígeno no combinado se produce por la fotodescomposición del agua durante la fotosíntesis. Según algunas estimaciones, las algas verdes y las cianobacterias de ambientes marinos proporcionan alrededor del 70 % del producido en la Tierra, y las plantas terrestres, el resto. Unos investigadores estiman que la contribución oceánica al oxígeno atmosférico es aún mayor, mientras que otros la sitúan por debajo, en torno a un 45 % del oxígeno atmosférico total del planeta cada año.

Una fórmula global simplificada de la fotosíntesis es:

6 CO + 6 HO + fotones → CHO + 6 O 
dióxido de carbono + agua + luz solar → glucosa + dioxígeno

La evolución fotolítica del oxígeno tiene lugar en las membranas tilacoides de los organismos fotosintéticos y requiere la energía de cuatro fotones. Están implicados muchos procesos, pero el resultado es la formación de un gradiente de un protón a través de la membrana tilacoide, que se usa para sintetizar adenosín trifosfato (ATP) por la fotofosforilación. El O restante tras la oxidación de la molécula de agua se libera a la atmósfera.

El dioxígeno molecular es esencial para la respiración celular en todos los organismos aerobios, ya que las mitocondrias lo usan para ayudar a generar adenosín trifosfato durante la fosforilación oxidativa. La reacción para la respiración aerobia es básicamente lo contrario que la fotosíntesis y se simplifica de la siguiente forma:

CHO + 6 O → 6 CO + 6 HO + 2880 kJ•mol

En los vertebrados el O se difunde por membranas pulmonares hacia los glóbulos rojos. La hemoglobina envuelve el O y hace que cambie su color de un rojo azulado a un rojo brillante (el CO se libera desde otra parte de la hemoglobina mediante el efecto Bohr). Otros animales usan la hemocianina (moluscos y algunos artrópodos) o la hemeritrina (arañas y langostas). Un litro de sangre puede disolver 200 cm³ de O.

Las especies reactivas de oxígeno, como el ion superóxido (O) y el peróxido de hidrógeno, son peligrosos subproductos del uso de oxígeno en los organismos. Algunas partes del sistema inmunitario de organismos más avanzados, sin embargo, crean peróxido, superóxido y oxígeno singlete para destruir microbios invasores. Las especies reactivas de oxígeno también tienen un rol importante en la respuesta hipersensible de las plantas contra ataques patógenos.

Un adulto humano en reposo respira de 1,8 a 2,4 gramos de oxígeno por minuto. Sumada la cantidad inhalada por todas las personas del planeta, hace un total de 6000 millones de toneladas de oxígeno por año.

El contenido de oxígeno en el cuerpo de un ser vivo es normalmente mayor en el sistema respiratorio y disminuye a lo largo de cualquier sistema arterial, los tejidos periféricos y el sistema venoso, respectivamente. El contenido de oxígeno en este sentido se suele dar como la presión parcial, que es la presión que tendría el oxígeno si ocupase por sí solo el volumen de las venas.

El oxígeno gaseoso no combinado era casi inexistente en la atmósfera terrestre antes de la evolución de las bacterias y arqueobacterias fotosintéticas. Apareció por primera vez en cantidades significativas durante el Paleoproterozoico (hace alrededor de 2500 y 1600 millones de años). En un principio, el oxígeno se combinó con Hierro disuelto en los océanos para crear formaciones de hierro bandeado. Los océanos comenzaron a exhalar oxígeno no combinado hace 2700 millones de años, y se alcanzó el 10 % de su nivel actual hace unos 1700 millones de años.

La presencia de grandes cantidades de oxígeno no combinado disuelto en los océanos y la atmósfera pudo haber conducido a la extinción de la mayoría de los organismos anaerobios que vivían entonces, durante la Gran Oxidación ("catástrofe del oxígeno") hace unos 2400 millones de años. Sin embargo el uso de O en la respiración celular permite producir a los organismos aerobios mucho más ATP que los anaerobios, lo que ayuda a los primeros a dominar la biosfera de la Tierra. La fotosíntesis y la respiración celular del O permitieron la evolución de las células eucariotas y, finalmente, la aparición de organismos multicelulares complejos como plantas y animales.

Desde el comienzo del periodo Cámbrico hace 540 millones de años, los niveles de O han fluctuado entre el 15 % y el 30 % por volumen. Hacia finales del Carbonífero (hace unos 300 millones de años) el nivel de O en la atmósfera alcanzó un volumen máximo del 35 %, que pudo haber contribuido al gran tamaño de los insectos y anfibios de aquella época. La actividad humana, aún si se considera la combustión de 7000 millones de toneladas de combustible fósil cada año, ha tenido un impacto muy pequeño en la cantidad de oxígeno combinado en la atmósfera. Con los niveles actuales de fotosíntesis, llevaría unos 2000 años regenerar la cantidad total de O en la atmósfera actual.

Uno de los primeros experimentos conocidos sobre la relación entre la combustión y el aire lo desarrolló el escritor sobre mecánica de la Antigua Grecia Filón de Bizancio, en el S. II a. C. En su obra "Pneumática", Filón observó que al invertir un recipiente sobre una vela prendida y rodear el cuello de este con agua, una parte del líquido subía por el cuello. Supuso, de forma incorrecta, que algunas partes del aire en el recipiente se convertían en elemento clásico del fuego y, entonces, era capaz de escapar a través de poros en el cristal. Muchos siglos después, Leonardo da Vinci observó que una porción del aire se consume durante la combustión y la respiración.

A finales del S. XVII, Robert Boyle probó que el aire es necesario para la combustión. El químico inglés John Mayow perfeccionó su trabajo mostrando que solo requería de una parte del aire, que llamó "spiritus nitroaereus" o simplemente "nitroaereus". En un experimento descubrió que al colocar tanto un ratón como una vela encendida en un contenedor cerrado sobre agua, hacía que esta subiera y reemplazara un catorceavo del volumen del aire antes de que se apagara la vela o muriera el ratón. Debido a esto supuso que el "nitroaereus" se consume tanto por la respiración como por la combustión.

Mayow observó que el antimonio incrementaba su peso al calentarse e infirió que el "nitroaereus" debía haberse combinado con él. Pensó también que los pulmones separaban el "nitroaereus" del aire y lo pasaban a la sangre y que el calor animal y el movimiento muscular eran producto de la reacción del "nitroaereus" con ciertas sustancias en el cuerpo. Publicó informes sobre estos experimentos y otras ideas en 1668, en su obra "Tractatus duo", en el tratado «De respiratione».

Robert Hooke, Ole Borch, Mijaíl Lomonósov y Pierre Bayen produjeron oxígeno durante experimentos entre los siglos XVII y XVIII, pero ninguno de ellos lo reconoció como un elemento. Esto pudo deberse en parte a la prevalencia de la filosofía de la combustión y la corrosión, denominada teoría del flogisto, que por aquel entonces era la explicación predilecta para esos procesos.

Esta teoría, establecida en 1667 por el químico alemán Johann Joachim Becher y modificada por el también químico Georg Stahl en 1731, postulaba que todos los materiales combustibles constaban de dos partes; una, llamada flogisto, que era emitida al quemar la sustancia en cuestión, y otra, denominada desflogisticada, que se tenía por su verdadera forma o "calx" (ceniza; creta en latín).

Los materiales altamente combustibles que dejan poco residuo, como la madera o el carbón, se creían hechos en su mayor parte por flogisto, mientras las sustancias no combustibles que se corroen, como el hierro, contienen muy poco. El aire no tenía ningún papel en la teoría del flogisto ni se realizaron experimentos cuantivativos para poner a prueba la idea; por el contrario, se basaba en observaciones de lo que sucedía cuando algo se quemaba: los objetos más comunes parecían volverse más ligeros y perder algo en el proceso. El hecho de que una sustancia como la madera realmente "ganara" peso en su conjunto durante el quemado se ocultaba por la flotabilidad de los productos gaseosos de la combustión. Una de las primeras pistas sobre la falsedad de la teoría del flogisto fue que los metales también ganaban peso en la oxidación (cuando supuestamente perdían flogisto).

El oxígeno fue descubierto por el farmacéutico sueco Carl Wilhelm Scheele, que produjo oxígeno gaseoso al calentar óxido de mercurio y varios nitratos alrededor de 1772. Scheele llamó al gas «aire del fuego», porque era el único apoyo conocido para la combustión, y escribió un informe de su descubrimiento en un manuscrito que tituló «Chemische Abhandlung von der Luft und dem Feuer» («Tratado químico del aire y del fuego») y envió a su editor en 1775, si bien no se publicó hasta 1777.

Entretanto, el 1 de agosto de 1774, el clérigo británico Joseph Priestley condujo un experimento en el que enfocó la luz solar sobre óxido de mercurio (II) (HgO) en el interior de un tubo de cristal, lo que liberó un gas que él llamó «aire desflogisticado». Notó que las velas prendían más vívamente en el gas y que el ratón estaba más activo y vivía más tiempo mientras lo respiraba. Tras inhalar él mismo el gas, escribió: «La sensación del gas en mis pulmones no era perceptiblemente diferente al del aire normal, pero sentí mi pecho particularmente ligero y desahogado durante un rato después». Priestley publicó sus hallazgos en 1775 en un artículo titulado «An Account of Further Discoveries in Air» («Informe de más descubrimientos en el aire»), que incluyó en el segundo volumen de su libro titulado "Experiments and Observations on Different Kinds of Air". Debido a que publicó sus hallazgos primero, Priestley suele ser considerado el autor del descubrimiento.

El renombrado químico francés Antoine Lavoisier reclamó posteriormente haber descubierto la sustancia de forma independiente. No obstante, Priestley visitó a Lavoisier en octubre de 1774 y le habló sobre su experimento y cómo había liberado el nuevo gas. Scheele también escribió una carta a Lavoisier, el 30 de septiembre de ese mismo año, en la que describía su propio descubrimiento de la sustancia antes desconocida, pero el francés nunca accedió a recibirla. Después de la muerte de Scheele se encontró una copia de la carta entre sus pertenencias.

Aunque fue cuestionado en su época, Lavoisier condujo los primeros experimentos cuantitativos adecuados sobre la oxidación y dio la primera explicación correcta acerca del funcionamiento de la combustión. Usó estos y otros experimentos similares, que comenzaron en 1774, para desacreditar la teoría del flogisto y demostrar que la sustancia descubierta por Priestley y Scheele era un elemento químico.

En un experimento Lavoisier observó que no se producía un incremento global en el peso cuando el estaño y el aire se calentaban en un contenedor cerrado. Notó que, cuando abrió el contenedor, el aire entró súbitamente en él, lo que indicaba que parte del aire atrapado se había consumido. También notó que el estaño había aumentado su peso y que el aumento era igual al del peso del aire que volvió al contenedor cuando lo abrió. Este y otros experimentos sobre la combustión se documentaron en su libro "Sur la combustion en général", publicado en 1777. En esa obra probó que el aire es una mezcla de dos gases: el «aire esencial», fundamental para la combustión y la respiración, y el "azote" (del griego "ἄζωτον", sin vida), que no servía para ninguna de las dos y se denominaría posteriormente nitrógeno.

Lavoisier renombró al «aire esencial» como "oxígeno" en 1777, desde las raíces griegas "ὀξύς (oxys)" (ácido, literalmente «punzante», por el sabor de los ácidos) y "-γενής (-genēs)" (productor, literalmente «engendrador»), porque pensaba, erróneamente, que el oxígeno era un constituyente de todos los ácidos.Los químicos —en particular sir Humphry Davy en 1812— al cabo de un tiempo determinaron que Lavoisier se equivocó en su apreciación pues, de hecho, es el hidrógeno el que forma la base de los ácidos, pero el nombre ya se había popularizado.

La hipótesis atómica original de John Dalton asumía que todos los elementos eran monoatómicos y que los átomos de los compuestos tendrían normalmente las relaciones atómicas más simples. Por ejemplo, Dalton pensaba que la fórmula del agua era HO, y presentaba la masa atómica del oxígeno como 8 veces la del hidrógeno, en vez de 16, el valor que se le da hoy en día. En 1805, Louis Joseph Gay-Lussac y Alexander von Humboldt mostraron que el agua está formada por dos volúmenes de hidrógeno y uno de oxígeno y, en 1811, Amedeo Avogadro dio con la correcta interpretación de la composición del líquido, basado en la que hoy se denomina Ley de Avogadro y en la suposición de moléculas diatómicas elementales.

A finales del S. XIX los investigadores se dieron cuenta de que el aire podía licuarse y sus componentes aislarse mediante compresión y enfriamiento. Utilizando un método de cascada, el químico y físico suizo Raoul Pictet evaporó dióxido de azufre para licuar dióxido de carbono, que por su parte era evaporado para enfriar el oxígeno gaseoso lo suficiente como para pasarlo a líquido. Envió un telegrama a la Academia de Ciencias de Francia el 22 de diciembre de 1877 en el que anunciaba su descubrimiento del oxígeno líquido. Solo dos días después, el físico francés Louis Paul Cailletet anunció su propio método para licuar oxígeno molecular. En ambos casos solo se produjeron unas pocas gotas del líquido, por lo que no se pudo llevar a cabo un análisis concluyente. El oxígeno fue licuado de forma estable por primera vez el 29 de marzo de 1883 por los científicos polacos de la Universidad Jagellónica Zygmunt Wróblewski y Karol Olszewski.

En 1891 el químico escocés James Dewar pudo producir la suficiente cantidad de oxígeno líquido para estudiarlo. El primer proceso viable comercialmente para producir oxígeno líquido fue desarrollado en 1895 de forma independiente por los ingenieros Carl von Linde, alemán, y William Hampson, británico. Redujeron la temperatura del aire hasta que se licuó y, entonces, destilaron los componentes gaseosos haciéndolos bullir uno a uno y capturándolos. Más tarde, en 1901, la soldadura de oxiacetileno se demostró por primera vez al quemar una mezcla de acetileno y O comprimido. Este método de soldadura y cortado del metal se convertiría después en habitual.

El físico William Thomson, en 1898, calculó que el oxígeno que permanece en el planeta tiene solo unos 400 o 500 años, basándose en el ritmo de uso de los combustibles fósiles en la combustión.

En 1923 el científico estadounidense Robert Goddard se convirtió en la primera persona en desarrollar un motor cohete que usaba gasolina como combustible y oxígeno líquido como oxidante. El 16 de marzo hizo volar con éxito un pequeño cohete propulsado por combustible líquido durante 56 m a 97 km/h, en Auburn (Massachusetts).

Se emplean principalmente dos métodos para producir 100 millones de toneladas de O extraídas del aire para usos industriales cada año. El más común consiste en destilar fraccionadamente aire licuado en sus diversos componentes, con el N destilado como vapor y el O dejado como líquido.

El otro método principal de obtención de O gaseoso consiste en pasar un chorro de aire limpio y seco a través de un lecho de tamices moleculares de zeolita, que adsorben el nitrógeno y dejan pasar un chorro de gas que es de un 90 a un 93 % O. Simultáneamente, el otro lecho de zeolita saturada de nitrógeno libera este gas al reducir la presión de funcionamiento de la cámara e introducir en ella a contracorriente parte del oxígeno separado en el lecho productor. Después de cada ciclo completo, los lechos se intercambian, lo que permite un suministro constante de oxígeno. Esto se conoce por adsorción por oscilación de presión y se utiliza para producir oxígeno a pequeña escala.

El oxígeno también puede producirse mediante la electrólisis del agua, descomponiéndola en oxígeno e hidrógeno, para lo cual debe usarse una corriente continua; si se usara una corriente alterna, los gases de cada extremo consistirían en hidrógeno y oxígeno en la explosiva relación 2:1. Contrariamente a la creencia popular, la relación 2:1 observada en la electrólisis de corriente continua del agua acidificada no demuestra que la fórmula empírica del agua sea HO, a menos que se asuman ciertas premisas sobre la fórmula molecular del hidrógeno y el oxígeno. Un método similar es la evolución electrocatalítica del O de óxidos a oxoácidos. También se pueden usar catalizadores químicos, como en el generador químico de oxígeno o en las velas de oxígeno que se usan en el equipamiento de apoyo en submarinos y que aún son parte del equipamiento estándar en aerolíneas comerciales para casos de despresurización. Otra tecnología de separación del aire consiste en forzar la disolución del aire a través de membranas de cerámica basadas en dióxido de zirconio, ya sea por alta presión o por corriente eléctrica, para producir O gaseoso prácticamente puro.

Para grandes cantidades el precio del oxígeno líquido era en 2001 de, aproximadamente, 0,21 USD/kg. El coste de la energía necesaria para licuar el aire supone el principal coste de producción, por lo cual el coste del oxígeno varía en función del precio de la energía. Por razones de economía el oxígeno se suele transportar en grandes cantidades en estado líquido, almacenado en tanques especialmente aislados, ya que un litro de oxígeno licuado equivale a 840 litros de oxígeno gaseoso a presión atmosférica y 20 °C (68 °F). Estas cisternas se usan para rellenar los grandes contenedores de oxígeno líquido que se encuentran en el exterior de los hospitales y demás instituciones que necesitan ingentes cantidades de oxígeno gaseoso puro. El oxígeno líquido se pasa por unos intercambiadores de calor que convierten el líquido criogénico en gas antes de que entre en el edificio. El oxígeno también se almacena y envía en cilindros que contienen el gas comprimido, lo que resulta útil para ciertas aplicaciones médicas portátiles y oxicorte.

El 55 % de la producción mundial de oxígeno se consume en la producción de acero. Otro 25 % se dedica a la industria química. Del 20 % restante la mayor parte se usa para aplicaciones medicinales, oxicorte, como oxidante en combustible de cohetes y en tratamiento de aguas.

El propósito esencial de la respiración es tomar el O del aire y, en medicina, se usan suplementos de oxígeno. El tratamiento no solo incrementa los niveles de oxígeno en la sangre del paciente, sino que tiene el efecto secundario de disminuir la resistencia al flujo de la sangre en muchos tipos de pulmones enfermos, lo que facilita el trabajo de bombeo del corazón. La oxigenoterapia se usa para tratar el enfisema, la neumonía, algunas insuficiencias cardíacas, algunos desórdenes que causan una elevada presión arterial pulmonar y cualquier enfermedad que afecte a la capacidad del cuerpo para tomar y usar el oxígeno.

Los tratamientos son lo suficientemente flexibles como para ser usados en hospitales, la vivienda del paciente o, cada vez más, con instrumentos móviles. Así, las tiendas de oxígeno se solían usar como suplementos de oxígeno, pero han ido sustituyéndose por las máscaras de oxígeno y las cánulas nasales.

La medicina hiperbárica (de alta presión) usa cámaras especiales de oxígeno para aumentar la presión parcial del O en el paciente y, cuando son necesarias, en el personal médico. La intoxicación por monóxido de carbono, la mionecrosis (gangrena gaseosa) y el síndrome de descompresión a veces se tratan con estos aparatos. El aumento de la concentración del O en los pulmones ayuda a desplazar el monóxido de carbono del hemogrupo de hemoglobina. El oxígeno es tóxico para la bacteria anaerobia que causa la gangrena gaseosa, de manera que aumentar su presión parcial ayuda a acabar con ellas. El síndrome de descompresión les sucede a los buzos que salen demasiado rápido del mar, lo que resulta en la formación de burbujas de gas inerte, sobre todo nitrógeno, en su sangre.

También se usa oxígeno para pacientes que necesitan ventilación mecánica, normalmente a concentraciones superiores al 21 % encontrado en el aire ambiental. Por otra parte, el isótopo O se usó de forma experimental en la tomografía por emisión de positrones.

Una aplicación notable del O como gas respirable de baja presión se encuentra en los trajes espaciales modernos, que envuelven el cuerpo de sus ocupantes con aire presurizado. Estos dispositivos usan oxígeno casi puro a una presión de alrededor de un tercio de la común, lo que da como resultado una presión parcial normal en el O de la sangre. Este intercambio de oxígeno de alta concentración para una baja presión es necesario para mantener la flexibilidad de los trajes espaciales.

Los buceadores y los tripulantes de submarinos también usan O artificialmente proporcionado, pero la mayoría usan una presión normal o una mezcla de oxígeno y aire. El uso de O puro o casi puro en buceo a presiones por encima del nivel del mar se limita generalmente a los descansos, descompresiones y tratamientos de emergencia a relativamente poca profundidad (~6 metros o menos). El buceo a mayor profundidad requiere una dilución significativa de O con otros gases, como nitrógeno o helio, para ayudar a prevenir el efecto de Paul Bert (toxicidad del oxígeno).

Los escaladores de montaña y los que viajan en aviones no presurizados a veces tienen un suplemento de O. Los pasajeros de aviones comerciales (presurizados) tienen un suministro de O para emergencias, que les es puesto automáticamente a su disposición en caso de despresurización de la cabina. Una pérdida repentina de presión en la cabina activa generadores químicos de oxígeno sobre cada asiento y hace caer máscaras de oxígeno. Al tirar de la máscara para comenzar el flujo de oxígeno, tal y como indican las instrucciones de seguridad, se fuerzan las limaduras de hierro en el clorato de sodio dentro del recipiente. Se produce, entonces, un chorro constante de oxígeno debido a la reacción exotérmica.

El oxígeno, como un supuesto eufórico suave, tiene una historia de uso recreativo en deportes y bares de oxígeno. Estos son establecimientos que aparecieron en Japón, California y Las Vegas a finales de los años 1990 que ofertan exposiciones a niveles de O superiores a lo normal a cambio de una determinada tarifa. Los atletas profesionales, especialmente en fútbol americano, también salen del campo en ocasiones, durante los descansos, para ponerse máscaras de oxígeno y obtener una estimulación en su juego. El efecto farmacológico es dudoso y el efecto placebo es la explicación más factible. Existen estudios que respaldan esa estimulación con mezclas de O enriquecido, pero solo si se inhalan "durante" el ejercicio aeróbico.

La fundición de mena de hierro en acero consume el 55 % del oxígeno producido comercialmente. En este proceso, el O es inyectado mediante una lanza de alta presión en el molde de hierro, que expulsa las impurezas de Azufre y el exceso de Carbono, en forma de sus respectivos óxidos, SO y CO. Las reacciones son exotérmicas y la temperatura asciende hasta los 1700 Cº.

Otro 25 % de este oxígeno se dedica a la industria química. El etileno reacciona con el O para crear óxido de etileno, que, a su vez, se convierte en etilenglicol, el material usado como base para fabricar una gran variedad de productos, entre otros los anticongelantes y los polímeros de poliéster (los precursores de muchos plásticos y textiles).

El oxígeno se usa en el oxicorte quemando acetileno con O para producir una llama muy caliente. En este proceso, el metal de hasta 60 centímetros de grosor se calienta primero con una pequeña llama de oxiacetileno para después ser rápidamente cortado por un gran chorro de O.

Los paleoclimatólogos miden la relación entre el oxígeno-18 y el oxígeno-16 en los esqueletos y exoesqueletos de los organismos marinos para determinar cómo era el clima hace millones de años. Las moléculas de agua de mar que contienen el isótopo más ligero, el oxígeno-16, se evaporan a un ritmo ligeramente mayor que las moléculas que contienen oxígeno-18 (un 12 % más pesado); esta disparidad se incrementa a bajas temperaturas. En periodos con una temperatura global más baja, la nieve y la lluvia procedentes de esa agua evaporada tienden a ser más ricas en oxígeno-16, mientras que el agua marina que queda tiende a serlo en oxígeno-18. Los organismos marinos, por tanto, incorporan más oxígeno-18 en sus esqueletos y exoesqueletos de lo que harían en un medio más cálido. Los paleoclimatólogos también miden directamente esta relación en las moléculas de agua de muestras de núcleo de hielo que se han conservado durante varios cientos de miles de años.

Los geólogos planetarios han medido las diferencias en la abundancia de isótopos de oxígeno en muestras de la Tierra, la Luna, Marte y meteoritos, pero no han estado lejos de poder obtener valores de referencia para las relaciones entre isótopos del Sol, que se creen iguales a aquellas de la nebulosa protosolar. Sin embargo, el análisis de una oblea de Silicio expuesta al viento solar en el espacio y devuelta a la Tierra por la sonda Génesis desveló que el Sol tiene una proporción de oxígeno-16 mayor que nuestro planeta. La medición implica que un proceso desconocido agotó el oxígeno-16 del disco protoplanetario del Sol antes de la fusión de los granos de polvo que formaron la Tierra.

El oxígeno presenta dos bandas de absorción espectrofotométrica con máximos en longitudes de onda de 687 y 760 nanómetros. Algunos científicos de detección remota han propuesto usar la medición del resplandor procedente de los doseles de vegetación en aquellas bandas para caracterizar la salud de las plantas desde una plataforma satelital. Esta aproximación explota el hecho de que en esas bandas es posible distinguir la reflectividad de la vegetación de su fluorescencia, que es mucho más débil. La medición tiene una alta dificultad técnica, debido a la baja relación señal/ruido y la estructura física de la vegetación, pero se ha propuesto como un posible método de monitoreo del ciclo del carbono desde satélites a escala global.

El estado de oxidación del oxígeno es -2 en casi todos los compuestos conocidos del oxígeno. Por su parte, el estado de oxidación -1 se encuentra en unos cuantos compuestos, como los peróxidos. Los compuestos en otro estado de oxidación son muy poco comunes: −1/2 (superóxidos), −1/3 (ozónidos), 0 (elemental, hipofluoroso), +1/2 (dioxigenil), +1 (difluoruro de dioxígeno) y +2 (difluoruro de oxígeno).

El agua (HO) es el óxido de hidrógeno y es el compuesto de oxígeno más común. Los átomos de hidrógeno están enlazados covalentemente al oxígeno en la molécula de agua, pero también tienen una atracción adicional (sobre 23,3 kJ•mol por átomo de hidrógeno) con un átomo de oxígeno adyacente de una molécula diferente. Estos enlaces de hidrógeno entre las moléculas de agua las mantienen aproximadamente un 15 % más cerca de lo que sería esperable en un líquido simple solo con las fuerzas de Van der Waals.
Debido a su electronegatividad, el oxígeno forma enlaces químicos con casi todos los demás elementos a temperaturas elevadas para dar los óxidos correspondientes. Sin embargo, algunos elementos forman óxidos directamente a condiciones normales de presión y temperatura, como el orín formado del Hierro. La superficie de metales como el aluminio y el titanio se oxidan en presencia del aire y se cubren con una fina capa de óxido que pasiva el metal y ralentiza la corrosión. Algunos de los óxidos metálicos de transición se encuentran en la naturaleza como compuestos no estequiométricos, con ligeramente menos metal de lo que la fórmula química sugiere. Por ejemplo, el FeO (wustita), que se forma de manera natural, se escribe en realidad como FeO, donde la «x» está normalmente en torno a 0,05.

El oxígeno como compuesto está presente en la atmósfera en pequeñas cantidades en forma de dióxido de carbono (CO). La roca de la corteza terrestre se compone de grandes partes de óxidos de Silicio (dióxido de silicio SiO, que se encuentra en el granito y la arena), aluminio (alúmina AlO, en la bauxita y el corindón), hierro (óxido férrico FeO, en la hematita y el orín) y calcio (carbonato cálcico CaCO, en la caliza). El resto de la corteza terrestre se compone también de compuestos de oxígeno, en particular varios silicatos complejos. En el manto terrestre, de una masa mucho mayor que la corteza, abundan los silicatos de hierro y Magnesio.

Los silicatos solubles en agua con las formas NaSiO, NaSiO y NaSiO se utilizan como detergentes y adhesivos. El oxígeno también actúa como ligazón para metales de transición, formando enlaces de O metálico con el átomo de iridio en el complejo de Vaska, con el platino en el PtF y con el centro de hierro en el grupo hemo de la hemoglobina.

Entre las clases más importantes de compuestos orgánicos que contienen oxígeno están los siguientes (donde «R» es un grupo orgánico): alcoholes (R-OH), éteres (R-O-R), cetonas (R-CO-R), aldehídos (R-CO-H), ácidos carboxílicos (R-COOH), Ésteres (R-COO-R), anhídridos de ácido (R-CO-O-CO-R) y amidas (R-C(O)-NR). Hay muchos disolventes orgánicos importantes que contienen oxígeno, entre ellos: acetona, metanol, etanol, alcohol isopropílico, furano, tetrahidrofurano, éter etílico, dioxano, etanoato de etilo, dimetilformamida, dimetilsulfóxido, ácido acético y ácido fórmico. La acetona (CH(CO)CH) y el fenol (CHOH) se usan como materiales en la síntesis de muchas sustancias diferentes. Otros compuestos orgánicos importantes que contienen oxígeno son: glicerol, formaldehído, glutaraldehído, ácido acético y acetamida. Los epóxidos son éteres en los que el átomo de oxígeno forma parte de un anillo de tres átomos.

El oxígeno reacciona espontáneamente con muchos compuestos orgánicos a temperatura ambiente o inferior, en un proceso llamado autooxidación. La mayor parte de los compuestos orgánicos que contienen oxígeno no se producen por la acción directa del O. Los compuestos orgánicos importantes en la industria y el comercio producidos por oxidación directa de un precursor incluyen al óxido de etileno y el ácido peracético.

El elemento se encuentra en casi todas las biomoléculas importantes para (o generadas por) la vida. Solo unas cuantas biomoléculas complejas comunes, como el escualeno y el caroteno, no contienen oxígeno. De los compuestos orgánicos con relevancia biológica, los carbohidratos contienen la mayor proporción de oxígeno en su masa. Todas las grasas, ácidos grasos, aminoácidos y proteínas contienen oxígeno (debido a la presencia de grupos carbonilos en esos ácidos y sus residuos de éster). El oxígeno también está presente en grupos de fosfato (PO) en las moléculas biológicamente importantes que transportan energía, ATP y ADP, en la columna vertebral y las purinas (excepto la adenina y las pirimidinas de ARN y ADN) y en los huesos como fosfato cálcico e hidroxiapatita.

El O gaseoso puede ser tóxico a presiones parciales elevadas, produciendo convulsiones y otros problemas de salud. La toxicidad generalmente comienza a aparecer con presiones parciales de más de 50 kPa o 2,5 veces la presión parcial del O a nivel del mar (21 kPa; igual a alrededor del 50 % de la composición del oxígeno a presión normal). Esto no resulta un problema excepto para pacientes con ventilación mecánica, debido a que el gas administrado a través de las máscaras de oxígeno se compone típicamente de solo un 30 %-50 % de O por volumen (sobre 30 kPa a presión normal), aunque estas cifras varían sensiblemente dependiendo del tipo de máscara.

Durante un tiempo, los bebés prematuros se colocaban en incubadoras que contenían aire rico en O, pero esta práctica cesó después de que algunos de estos niños perdieran la visión.

La respiración de O puro en aplicaciones espaciales, como en algunos trajes aeroespaciales modernos o en naves pioneras como la Apolo, no causa daños debido a las bajas presiones totales utilizadas. En el caso de los trajes, la presión parcial del O en el gas respiratorio se encuentra, en general, sobre 30 kPa (1,4 veces lo normal) y la presión parcial resultante en la sangre arterial del astronauta solo está marginalmente por encima de lo normal al nivel del mar.

La toxicidad del oxígeno para los pulmones y el sistema nervioso central también puede darse en el buceo profundo y en el buceo profesional. La respiración prolongada de una mezcla de aire con una presión parcial de O mayor a 60 kPa puede llegar a producir una fibrosis pulmonar permanente. La exposición a presiones parciales superiores a 160 kPa (~1,6 atmósferas) podría causar convulsiones, normalmente fatales para los buzos. La toxicidad aguda puede producirse al respirar una mezcla de aire con más de un 21 % de O a 66 o más metros de profundidad; lo mismo puede ocurrir al respirar un 100 % de O a solo 6 metros.

Las fuentes de oxígeno que están altamente concentradas estimulan una rápida combustión. Los riesgos de fuego y explosión se dan cuando los oxidantes concentrados y los combustibles se sitúan demasiado cerca entre sí; sin embargo, la ignición, ya sea por el calor o por una chispa, es necesaria para iniciar la combustión. El oxígeno en sí mismo no es un combustible, sino un oxidante. Los riesgos de la combustión también se aplican a compuestos de oxígeno de alto potencial oxidante, como los peróxidos, cloratos, nitratos, percloratos y dicromatos, porque pueden dar oxígeno al fuego.

El O concentrado permite una combustión rápida y enérgica. Las tuberías y los recipientes de acero usados para almacenar y trasmitir tanto el oxígeno líquido como el gaseoso actúan como combustible; por tanto, el diseño y la fabricación de los sistemas de O requieren una atención especial para asegurar que las fuentes de ignición se minimizan. El incendio que acabó con la vida de la tripulación del Apolo 1 en una prueba en la plataforma de lanzamiento se extendió tan rápidamente debido a que la cápsula estaba presurizada con O puro, pero a una presión ligeramente mayor que la atmosférica, en lugar de una presión de 1/3 de la normal que debía usarse en la misión.

En caso de un derrame de oxígeno líquido, si este llega a empaparse en materia orgánica como madera, productos petroquímicos y asfalto puede provocar que estos materiales detonen de forma impredecible al sufrir un impacto mecánico posterior. Al igual que otros líquidos criogénicos, en contacto con el cuerpo humano puede causar congelamiento en piel y ojos.




</doc>
<doc id="3268" url="https://es.wikipedia.org/wiki?curid=3268" title="Masa atómica">
Masa atómica

La masa atómica es la masa de un átomo, más frecuentemente expresada en unidades de masa atómica unificada. 
La masa atómica en algunas veces es usada incorrectamente como un sinónimo de masa atómica relativa, masa atómica media y peso atómico; estos últimos difieren sutilmente de la masa atómica. *La masa atómica está definida como la masa de un átomo, que solo puede ser de un isótopo a la vez, y no es un promedio ponderado en las abundancias de los isótopos.* En el caso de muchos elementos que tienen un isótopo dominante, la similitud/diferencia numérica real entre la masa atómica del isótopo más común y la masa atómica relativa o peso atómico estándar puede ser muy pequeña, tal que no afecta muchos cálculos bastos, pero tal error puede ser crítico cuando se consideran átomos individuales.

El peso atómico estándar se refiere a la media de las masas atómicas relativas de un elemento en el medio local de la corteza terrestre y la atmósfera terrestre, como está determinado por la "Commission on Atomic Weights and Isotopic Abundances" (Comisión de Pesos Atómicos y Abundancias Isotópicas) de la IUPAC. Estos valores son los que están incluidos en una tabla periódica estándar, y es lo que es más usado para los cálculos ordinarios. Se incluye una incertidumbre en paréntesis que frecuentemente refleja la variabilidad natural en la distribución isotópica, en vez de la incertidumbre en la medida. Para los elementos sintéticos, el isótopo formado depende de los medios de síntesis, por lo que el concepto de abundancia isotópica natural no tiene sentido. En consecuencia, para elementos sintéticos, el conteo total de nucleones del isótopo más estable (esto es, el isótopo con la vida media más larga) está listado en paréntesis en el lugar del peso atómico estándar. El litio representa un caso único, donde la abundancia natural de los isótopos ha sido perturbada por las actividades humanas al punto de afectar la incertidumbre en su peso atómico estándar, incluso en muestras obtenidas de fuentes naturales, como los ríos.

La masa atómica relativa es un sinónimo para peso atómico y está cercanamente relacionado con la masa atómica promedio (pero no es un sinónimo de masa atómica), la media ponderada de las masas atómicas de todos los átomos de un elemento químico encontrados en una muestra particular, ponderados por abundancia isotópica. Esto es usado frecuentemente como sinónimo para peso atómico relativo, y este uso no es incorrecto, dado que los pesos atómicos estándar son masas atómicas relativas, aunque es menos específico. La masa atómica relativa también se refiere a ambientes no terrestres y ambientes terrestres altamente específicos que se desvían de la media o tienen diferentes certidumbres (número de cifras significativas) que los pesos atómicos estándar.

La masa isotópica relativa es la masa relativa de un isótopo dado (más específica, cualquier núclido solo), escalado con el carbono-12 como exactamente 12. No hay otros núclidos distintos al carbono-12 que tengan exactamente un número entero de masas en esta escala. Esto es debido a dos factores:

La cantidad que las masas atómicas se desvían de su número de masa es como sigue: la desviación empieza, positiva en el hidrógeno-1, disminuyendo hasta alcanzar un mínimo en el hierro-56, hierro-58 y níquel-62, luego aumenta a valores positivos en los isótopos más pesados, conforme aumenta el número atómico. Esto corresponde a lo siguiente: la fisión nuclear en un elemento más pesado que el hierro produce energía, y la fisión de cualquier elemento más ligero que el hierro requiere energía. Lo opuesto es verdadero para las reacciones de fusión nuclear: la fusión en los elementos más ligeros que el hierro produce energía, y la fusión en los elementos más pesados que el hierro requiere energía.

El proceso que se siguió históricamente para determinar las masas reales de los átomos de los diferentes elementos fue similar al seguido en el modelo clips, trabajando inicialmente con gases y comparando las masas de gases situados en recipientes con las mismas condiciones de presión, volumen y temperatura: como las masas eran distintas, pero había el mismo número de partículas (de acuerdo con el modelo de materia y el principio de Avogadro), se debía a que las partículas tenían masas reales diferentes.Actualmente la comparación directa y medición de las masas de los átomos se logra con la utilización de un espectrómetro de masas.

La unidad científica estándar para manejar átomos en cantidades macroscópicas es el mol, que está definido arbitrariamente como la cantidad de sustancia que tiene tantos átomos u otra unidad como átomos hay en 12 gramos de carbono del isótopo C-12. El número de átomos en un mol es denominado número de Avogadro, cuyo valor es aproximadamente 6,022 x 10 mol. Un mol de una sustancia siempre contiene exactamente la "masa atómica relativa" o "masa molar" de dicha sustancia, expresado en gramos; sin embargo, esto no es cierto para la "masa atómica". Por ejemplo, el peso atómico estándar del hierro es 55,847 g/mol, y en consecuencia un mol de hierro como se suele encontrar en la Tierra tiene una masa de 55,847 gramos. La "masa atómica" del isótopo Fe es 55,935 u, y un mol de Fe pesará, en teoría, 55,935 g, pero no se ha encontrado tales cantidades puras de isótopo Fe en la Tierra.

La fórmula para la conversión entre unidad de masa atómica y la masa SI en gramos para un solo átomo es:

donde formula_2 es la constante de masa molar y formula_3 es el número de Avogadro.

Se aplican definiciones similares a las moléculas. Se puede calcular la masa molecular de un compuesto por adición de las masas atómicas-moleculares de sus átomos constituyentes (núclidos). También se puede calcular la masa molar indefinida por la adición de las masas atómicas relativas de los elementos dados en la fórmula molecular. En ambos casos, la multiplicidad de los átomos (el número de veces que está presente) debe ser tomado en cuenta, generalmente multiplicando cada masa única por su multiplicidad inversa.

En la historia de la química, los primeros científicos en determinar los pesos atómicos fueron John Dalton entre 1803 y 1808, y Jöns Jakob Berzelius entre 1808 y 1826. Los pesos atómicos fueron definidos originalmente en relación al elemento hidrógeno, el más ligero, tomándolo como 1, y en 1820, la hipótesis de Prout indicaba que las masas atómicas de todos los elementos deberían ser un múltiplo entero del peso del hidrógeno. Sin embargo, Berzelius pronto probó que esta hipótesis no siempre se sostenía, y en algunos casos, como el cloro, el peso atómico caía casi exactamente entre dos múltiplos del peso del hidrógeno. Posteriormente, se mostró que esto se debía a un efecto causado por los isótopos, y que la masa atómica de los isótopos puros, o núclidos, era múltiplo de la masa del hidrógeno, en un margen de diferencia del 0,96%.

En la década de 1860, Stanislao Cannizzaro refinó los pesos atómicos aplicando la ley de Avogadro (en el Congreso de Karlsruhe de 1860). Formuló una ley para determinar los pesos atómicos de los elementos: "las distintas cantidades del mismo elemento contenido en distintas moléculas son todas múltiplos enteros del peso atómico", y determinó los pesos atómicos y pesos moleculares comparando la densidad de vapor de un conjunto de gases con moléculas conteniendo uno o más del elemento químico en cuestión.

A principios del siglo XX, hasta la década de 1960, los químicos y físicos utilizaban dos escalas de masa atómicas. Los químicos usaban una escala tal que la mezcla natural de isótopos de oxígeno tenía una masa atómica de 16, mientras que los físicos asignaron el mismo número 16 a la masa atómica del isótopo de oxígeno más común (que contiene ocho protones y ocho neutrones). Sin embargo, debido a que también están presentes en el oxígeno natural, tanto el oxígeno-17 como el oxígeno-18, esto conducía a 2 tablas diferentes de masas atómicas. La escala unificada, basada en el carbono-12, C, cumplía el requerimiento de los físicos de basar la escala en un isótopo puro, a la vez que se hacía numéricamente cercana a la escala de los químicos.




</doc>
<doc id="3269" url="https://es.wikipedia.org/wiki?curid=3269" title="Punto de fusión">
Punto de fusión

El punto de fusión (o, rara vez, el punto de licuefacción) de una sustancia es la temperatura a la que cambia de estado sólido a líquido. En el punto de fusión, la fase sólida y líquida existen en equilibrio termodinámico. El punto de fusión de una sustancia depende de la presión y generalmente se específica a una presión estándar, como 1 atmósfera o 100 kPa.

Cuando se considera como la temperatura del cambio inverso de líquido a sólido, se le conoce como el punto de congelación o el punto de cristalización. Debido a la capacidad de algunas sustancias para sobreenfriarse, el punto de congelación no se considera una propiedad característica de una sustancia. Cuando se determina el "punto de congelación característico" de una sustancia, de hecho, la metodología real es casi siempre "el principio de observar la desaparición en lugar de la formación de hielo", es decir, el punto de fusión.

Para la mayoría de las sustancias, los puntos de fusión y de congelación son aproximadamente iguales. Por ejemplo, el punto de fusión y el punto de congelación del mercurio es de 234.32 kelvins (−38.83 °C o −37.89 °F). Sin embargo, ciertas sustancias poseen diferentes temperaturas de transición sólido-líquido. Por ejemplo, el agar se funde a 85 ° C (185 ° F) y se solidifica a partir de 31 ° C (88 °F; 304 K); tal dependencia de la dirección se conoce como histéresis. El punto de fusión del hielo a 1 atmósfera de presión es muy cercano a 0 ° C (32 °F; 273 K); Esto también se conoce como el punto de hielo. En presencia de sustancias nucleantes, el punto de congelación del agua no siempre es el mismo que el punto de fusión. En ausencia de nucleadores, el agua puede existir como un líquido subenfriado hasta −48.3 °C (−55 °F, 224.8 K) antes de la congelación.

El elemento químico con el punto de fusión más alto es el tungsteno, a 3414 ° C (6177 °F; 3687 K); esta propiedad hace que el tungsteno sea excelente para usar como filamentos en bombillas. El carbón que se cita con frecuencia no se funde a la presión ambiental, sino que se sublima a unos 3.726,85 ° C (6.740,33 °F; 4.000,00 K); solo existe una fase líquida por encima de las presiones de 10 MPa (99 atm) y se estima en 4.030–4.430 ° C (7.290–8.010 °F; 4.300–4.700 K) (consulte el diagrama de la fase de carbono). El carburo de tantalio hafnio (TaHfC) es un compuesto refractario con un punto de fusión muy alto de 4215 K (3942 °C, 7128 °F). En el otro extremo de la escala, el helio no se congela en absoluto a presión normal, incluso a temperaturas cercanas al cero absoluto; Es necesaria una presión de más de veinte veces la presión atmosférica normal.

La tabla siguiente muestra las temperaturas de fusión de los elementos en °C (a una atmósfera de presión):

Existen muchas técnicas de laboratorio para la determinación de los puntos de fusión. Un banco Kofler es una tira de metal con un gradiente de temperatura (desde la temperatura ambiente hasta los 300 ° C). Cualquier sustancia se puede colocar en una sección de la tira, revelando su comportamiento térmico a la temperatura en ese punto. La calorimetría diferencial de barrido proporciona información sobre el punto de fusión junto con su entalpía de fusión.

Un aparato básico para el análisis del punto de fusión de sólidos cristalinos consiste en un baño de aceite con una ventana transparente (el diseño más básico: un tubo Thiele) y una lupa simple. Los diversos granos de un sólido se colocan en un tubo de vidrio delgado y se sumergen parcialmente en el baño de aceite. El baño de aceite se calienta (y se agita) y con la ayuda de la lupa (y la fuente de luz externa) se puede observar la fusión de los cristales individuales a una cierta temperatura. En dispositivos grandes/pequeños, la muestra se coloca en un bloque de calefacción y la detección óptica se automatiza..

La medición también se puede realizar de forma continua con un proceso operativo. Por ejemplo, las refinerías de petróleo miden el punto de congelación del combustible diésel en línea, lo que significa que la muestra se toma del proceso y se mide automáticamente. Esto permite mediciones más frecuentes ya que la muestra no tiene que ser recolectada manualmente y llevada a un laboratorio remoto.

Para materiales refractarios (por ejemplo, platino, tungsteno, tantalio, algunos carburos y nitruros, etc.), el punto de fusión extremadamente alto (generalmente considerado como superior a 1800 °C) puede determinarse calentando el material en un horno de cuerpo negro y medición de la temperatura del cuerpo negro con un pirómetro óptico. Para los materiales de fusión más altos, esto puede requerir extrapolación por varios cientos de grados. Se sabe que la luminosidad espectral de un cuerpo incandescente es una función de su temperatura. Un pirómetro óptico combina el brillo de un cuerpo en estudio con el brillo de una fuente que se ha calibrado previamente en función de la temperatura. De esta manera, la medición de la magnitud absoluta de la intensidad de la radiación es innecesaria. Sin embargo, deben usarse temperaturas conocidas para determinar la calibración del pirómetro. Para temperaturas superiores al rango de calibración de la fuente, se debe emplear una técnica de extrapolación. Esta extrapolación se logra utilizando la ley de radiación de Planck. Las constantes en esta ecuación no se conocen con suficiente precisión, lo que hace que los errores en la extrapolación se hagan más grandes a temperaturas más altas. Sin embargo, se han desarrollado técnicas estándar para realizar esta extrapolación.

Considere el caso de usar oro como fuente (pf = 1063 ° C). En esta técnica, la corriente a través del filamento del pirómetro se ajusta hasta que la intensidad de la luz del filamento coincide con la de un cuerpo negro en el punto de fusión del oro. Esto establece la temperatura de calibración primaria y se puede expresar en términos de corriente a través de la lámpara del pirómetro. Con la misma configuración actual, el pirómetro se ve en otro cuerpo negro a una temperatura más alta. Un medio absorbente de transmisión conocida se inserta entre el pirómetro y este cuerpo negro. La temperatura del cuerpo negro se ajusta hasta que existe una coincidencia entre su intensidad y la del filamento del pirómetro. La verdadera temperatura más alta del cuerpo negro se determina a partir de la Ley de Planck. Luego se retira el medio absorbente y se ajusta la corriente a través del filamento para que coincida la intensidad del filamento con la del cuerpo negro. Esto establece un segundo punto de calibración para el pirómetro. Este paso se repite para llevar la calibración a temperaturas más altas. Ahora, se conocen las temperaturas y sus correspondientes corrientes de filamento de pirómetro y se puede dibujar una curva de temperatura versus corriente. Esta curva puede ser extrapolada a temperaturas muy altas.

Para determinar los puntos de fusión de una sustancia refractaria por este método, es necesario tener condiciones de cuerpo negro o conocer la emisividad del material que se está midiendo. La contención del material de alto punto de fusión en estado líquido puede introducir dificultades experimentales. Las temperaturas de fusión de algunos metales refractarios se han medido observando la radiación de una cavidad del cuerpo negro en muestras de metal sólido que eran mucho más largas que anchas. Para formar tal cavidad, se perfora un agujero perpendicular al eje largo en el centro de una varilla del material. Estas barras se calientan pasando una corriente muy grande a través de ellas, y la radiación emitida desde el agujero se observa con un pirómetro óptico. El punto de fusión se indica por el oscurecimiento del agujero cuando aparece la fase líquida, destruyendo las condiciones del cuerpo negro. En la actualidad, se emplean técnicas de calentamiento por láser sin contenedor, combinadas con pirómetros rápidos y espectrómetros, para permitir un control preciso del tiempo durante el cual la muestra se mantiene a temperaturas extremas. Dichos experimentos de duración inferior a un segundo abordan varios de los desafíos asociados con las mediciones más tradicionales del punto de fusión realizadas a temperaturas muy altas, como la vaporización de la muestra y la reacción con el contenedor.

Para que un sólido se derrita, se requiere calor para elevar su temperatura hasta el punto de fusión. Sin embargo, se necesita suministrar más calor para que la fusión tenga lugar: esto se denomina calor de fusión y es un ejemplo de calor latente.

Desde el punto de vista de la termodinámica, en el punto de fusión, el cambio en la energía libre de Gibbs (G) del material es cero, pero la entalpía (H) y la entropía (S) del material están aumentando (ΔH, ΔS> 0) . El fenómeno de fusión ocurre cuando la energía libre de Gibbs del líquido se vuelve más baja que el sólido para ese material. A varias presiones esto sucede a una temperatura específica. También se puede demostrar que:
Aquí T, ΔS y ΔH son respectivamente la temperatura en el punto de fusión, el cambio de entropía de fusión y el cambio de entalpía de fusión.

El punto de fusión es sensible a cambios extremadamente grandes en la presión, pero en general esta sensibilidad es un orden de magnitud menor que la del punto de ebullición, porque la transición sólido-líquido representa solo un pequeño cambio en el volumen. Si, como se observa en la mayoría de los casos, una sustancia es más densa en el sólido que en el estado líquido, el punto de fusión aumentará con el aumento de la presión. De lo contrario se produce el comportamiento inverso. En particular, este es el caso del agua, como se ilustra gráficamente a la derecha, pero también de Si, Ge, Ga, Bi. Con cambios extremadamente grandes en la presión, se observan cambios sustanciales en el punto de fusión. Por ejemplo, el punto de fusión del silicio a presión ambiente (0.1 MPa) es de 1415 ° C, pero a presiones superiores a 10 GPa disminuye a 1000 ° C.

Los puntos de fusión se utilizan a menudo para caracterizar compuestos orgánicos e inorgánicos y para determinar su pureza. El punto de fusión de una sustancia pura siempre es más alto y tiene un rango más pequeño que el punto de fusión de una sustancia impura o, más generalmente, de mezclas. Cuanto mayor sea la cantidad de otros componentes, menor será el punto de fusión y más amplio será el rango del punto de fusión, a menudo denominado "rango pastoso". La temperatura a la que comienza la fusión para una mezcla se conoce como ""solidus"", mientras que la temperatura donde se completa la fusión se llama ""liquidus"". Los eutécticos son tipos especiales de mezclas que se comportan como fases simples. Se funden bruscamente a una temperatura constante para formar un líquido de la misma composición. Alternativamente, al enfriar un líquido con la composición eutéctica se solidificará como cristales mixtos dispersos uniformemente, pequeños (de grano fino) con la misma composición.

A diferencia de los sólidos cristalinos, los vidrios no poseen un punto de fusión; en el calentamiento experimentan una transición suave de vidrio a un líquido viscoso. Para un mayor calentamiento, se ablandan gradualmente, lo que puede caracterizarse por ciertos puntos de reblandecimiento.

El punto de congelación de un solvente se reduce cuando se agrega otro compuesto, lo que significa que una solución tiene un punto de congelación más bajo que un solvente puro. Este fenómeno se utiliza en aplicaciones técnicas para evitar la congelación, por ejemplo, agregando sal o etilenglicol al agua.

En química orgánica, la regla de Carnelley, establecida en 1882 por Thomas Carnelley, establece que la "simetría molecular alta se asocia con un alto punto de fusión". Carnelley basó su regla en el examen de 15,000 compuestos químicos. Por ejemplo, para tres isómeros estructurales con fórmula molecular CH, el punto de fusión aumenta en la serie isopentano −160 ° C (113 K) n-pentano −129.8 ° C (143 K) y neopentano −16.4 ° C (256.8 K) Del mismo modo, en los xilenos y también en los diclorobencenos, el punto de fusión aumenta en el orden meta, orto y luego para. La piridina tiene una simetría más baja que el benceno, por lo tanto su punto de fusión más bajo, pero el punto de fusión aumenta nuevamente con diazina y triazinas. Muchos compuestos de tipo jaula como el adamantano y el cubano con alta simetría tienen puntos de fusión relativamente altos.

Un alto punto de fusión resulta de un alto calor de fusión, una baja entropía de fusión o una combinación de ambos. En moléculas altamente simétricas, la fase cristalina está densamente empaquetada con muchas interacciones intermoleculares eficientes que resultan en un mayor cambio de entalpía en la fusión.

Un intento de predecir el punto de fusión a granel de los materiales cristalinos se realizó por primera vez en 1910 por Frederick Lindemann. La idea detrás de la teoría fue la observación de que la amplitud promedio de las vibraciones térmicas aumenta al aumentar la temperatura. La fusión se inicia cuando la amplitud de la vibración se vuelve lo suficientemente grande como para que los átomos adyacentes ocupen parcialmente el mismo espacio. El criterio de Lindemann establece que se espera una fusión cuando la amplitud cuadrática media de la raíz de vibración excede un valor de umbral.

Suponiendo que todos los átomos en un cristal vibran con la misma frecuencia ν, la energía térmica promedio puede estimarse utilizando el teorema de equipartición como

donde "m" es la masa atómica, "v" es la frecuencia, u es la amplitud de vibración media, "k" es la constante de Boltzmann y T es la temperatura absoluta. Si el valor de umbral de u es ca, donde c es la constante de Lindemann y a es el espaciado atómico, entonces el punto de fusión se estima como

Se pueden obtener otras expresiones para la temperatura de fusión estimada dependiendo de la estimación de la energía térmica promedio. Otra expresión de uso común para el criterio de Lindemann es

De la expresión para la frecuencia de Debye para ν, tenemos

donde θD es la temperatura de Debye y h es la constante de Planck. Los valores de c van desde 0.15–0.3 para la mayoría de los materiales.

En febrero de 2011, Alfa Aesar lanzó más de 10,000 puntos de fusión de compuestos de su catálogo como datos abiertos. Este conjunto de datos se ha utilizado para crear un modelo de bosque aleatorio para la predicción del punto de fusión que ahora está disponible de forma gratuita. Los datos de punto de fusión abierto también están disponibles en "Nature Precedings". Tetko et al. publicaron datos de alta calidad extraídos de patentes y también modelos [19] desarrollados con estos datos.



</doc>
<doc id="3271" url="https://es.wikipedia.org/wiki?curid=3271" title="Punto de ebullición">
Punto de ebullición

El punto de ebullición de una sustancia es la temperatura a la cual la presión de vapor del líquido es igual a la presión que rodea al líquido y se transforma en vapor.

El punto de ebullición de un líquido varía según la presión ambiental que lo rodea. Un líquido en un vacío parcial tiene un punto de ebullición más bajo que cuando ese líquido está a la presión atmosférica. Un líquido a alta presión tiene un punto de ebullición más alto que cuando ese líquido está a la presión atmosférica. Por ejemplo, el agua hierve a 100 °C (212 °F) a nivel del mar, pero a 93.4 °C (200.1 °F) a 1,905 metros (6,250 pies) de altitud. Para una presión dada, diferentes líquidos hervirán a diferentes temperaturas.

El punto de ebullición normal (también llamado punto de ebullición atmosférico o punto de ebullición a presión atmosférica) de un líquido es el caso especial en el que la presión de vapor del líquido es igual a la presión atmosférica definida a nivel del mar, 1 atmósfera. A esa temperatura, la presión de vapor del líquido llega a ser suficiente para superar la presión atmosférica y permitir que se formen burbujas de vapor dentro de la mayor parte del líquido. El punto de ebullición estándar ha sido definido por IUPAC desde 1982 como la temperatura a la cual ocurre la ebullición bajo una presión de 1 bar.

El calor de vaporización es la energía requerida para transformar una cantidad dada (un mol, kg, libra, etc.) de una sustancia de un líquido en un gas a una presión dada (a menudo presión atmosférica).

Los líquidos pueden transformarse en vapor a temperaturas por debajo de sus puntos de ebullición a través del proceso de evaporación. La evaporación es un fenómeno de superficie en el que las moléculas ubicadas cerca del borde del líquido, que no están contenidas por suficiente presión de líquido en ese lado, se escapan a los alrededores como vapor. Por otro lado, la ebullición es un proceso en el cual las moléculas en cualquier parte del líquido se escapan, lo que resulta en la formación de burbujas de vapor dentro del líquido.

La temperatura de una sustancia o cuerpo depende de la energía cinética media de las moléculas. A temperaturas inferiores al punto de ebullición, solo una pequeña fracción de las moléculas en la superficie tiene energía suficiente para romper la tensión superficial y escapar. Este incremento de energía constituye un intercambio de calor que da lugar al aumento de la entropía del sistema (tendencia al desorden de los puntos materiales que componen su cuerpo).

El punto de ebullición depende de la masa molecular de la sustancia y del tipo de las fuerzas intermoleculares de esta sustancia. Para ello se debe determinar si la sustancia es covalente polar, covalente no polar, y determinar el tipo de enlaces (dipolo permanente —dipolo inducido o puentes de hidrógeno—).

El punto de ebullición no puede elevarse en forma indefinida. Conforme se aumenta la presión, la densidad de la fase gaseosa aumenta hasta que, finalmente, se vuelve indistinguible de la fase líquida con la que está en equilibrio; esta es la temperatura crítica, por encima de la cual no existe una fase líquida clara. El helio tiene el punto normal de ebullición más bajo (–268,9 °C) de los correspondientes a cualquier sustancia y el wolframio, el más alto (5930 °C).

En las tablas termodinámicas de productos químicos, no se indica todo el diagrama de fase, solo la temperatura de ebullición en el estado estándar, es decir, con una presión de una atmósfera (1013,25 hPa). Este punto de ebullición se denomina punto de ebullición normal y la Temperatura de ebullición normal. El término "punto de ebullición" se utiliza a menudo para referirse al punto de ebullición normal.

La siguiente tabla muestra las temperaturas de ebullición en el estado estándar (1 atm) en °C:

Un "líquido saturado "contiene tanta energía térmica como puede sin hervir (o, a la inversa, un "vapor saturado" contiene la menor energía térmica posible sin condensación).

La temperatura de saturación significa punto de ebullición. La temperatura de saturación es la temperatura para una presión de saturación correspondiente a la que un líquido hierve en su fase de vapor. Se puede decir que el líquido está saturado de energía térmica. Cualquier adición de energía térmica resulta en una transición de fase.

Si la presión en un sistema permanece constante (isobárica), un vapor a temperatura de saturación comenzará a condensarse en su fase líquida a medida que se elimina la energía térmica (calor). De manera similar, un líquido a temperatura y presión de saturación hervirá en su fase de vapor a medida que se aplique energía térmica adicional.

El punto de ebullición corresponde a la temperatura a la cual la presión de vapor del líquido es igual a la presión ambiental circundante. Por lo tanto, el punto de ebullición depende de la presión. Los puntos de ebullición pueden publicarse con respecto al NIST, presión estándar de EE. UU. De 101.325 kPa (o 1 atm), o la presión estándar IUPAC de 100.000 kPa. En elevaciones más altas, donde la presión atmosférica es mucho menor, el punto de ebullición también es menor. El punto de ebullición aumenta con el aumento de la presión hasta el punto crítico, donde las propiedades del gas y del líquido se vuelven idénticas. El punto de ebullición no se puede aumentar más allá del punto crítico. Del mismo modo, el punto de ebullición disminuye con la presión decreciente hasta que se alcanza el punto triple. El punto de ebullición no puede reducirse por debajo del punto triple.

Si se conoce el calor de vaporización y la presión de vapor de un líquido a una cierta temperatura, el punto de ebullición se puede calcular utilizando la ecuación de Clausius-Clapeyron, por lo tanto:

formula_1

La presión de saturación es la presión para una temperatura de saturación correspondiente a la que un líquido hierve en su fase de vapor. La presión de saturación y la temperatura de saturación tienen una relación directa: a medida que aumenta la presión de saturación, también lo hace la temperatura de saturación.

Si la temperatura en un sistema permanece constante (un sistema isotérmico), el vapor a presión y temperatura de saturación comenzará a condensarse en su fase líquida a medida que aumenta la presión del sistema. De manera similar, un líquido a presión y temperatura de saturación tenderá a destellar en su fase de vapor a medida que disminuye la presión del sistema.

Existen dos convenciones con respecto al punto de ebullición estándar del agua: el punto de ebullición normal es 99.97 ° C (211.9 ° F) a una presión de 1 atm (es decir, 101.325 kPa). El punto de ebullición estándar recomendado por la IUPAC del agua a una presión estándar de 100 kPa (1 bar) es de 99.61 ° C (211.3 ° F). . A modo de comparación, en la cima del monte Everest, a una elevación de 8,848 m (29,029 ft), la presión es de aproximadamente 34 kPa (255 Torr) y el punto de ebullición del agua es de 71 ° C (160 ° F). .
La escala de temperatura Celsius se definió hasta 1954 por dos puntos: 0 ° C se definió por el punto de congelación del agua y 100 ° C se definió por el punto de ebullición del agua a la presión atmosférica estándar.

Cuanto mayor sea la presión de vapor de un líquido a una temperatura dada, menor será el punto de ebullición normal (es decir, el punto de ebullición a presión atmosférica) del líquido.

El gráfico de presión del vapor a la derecha ha graphs de las presiones de vapor versus temperaturas para una variedad de líquidos. Cuando puede ser visto en el gráfico, los líquidos con las presiones de vapor más altas tienen el más bajos normales hirviendo puntos.

Por ejemplo, en cualquier temperatura dada, cloruro de metilo tiene la presión de vapor más alta de cualquier de los líquidos en el gráfico. También tiene el más bajo normal hirviendo punto (−24.2 °C), el cual es donde la curva de presión del vapor de cloruro de metilo (la línea azul) cruza la línea de presión horizontal de una atmósfera (atm) de presión de vapor absoluto.

El punto crítico de un líquido es la temperatura más alta (y la presión) a la que realmente hervirá.

El elemento con el punto de ebullición más bajo es el helio. Tanto los puntos de ebullición de renio como de tungsteno superan los 5000 K a presión estándar; Debido a que es difícil medir temperaturas extremas precisamente sin sesgo, ambos han sido citados en la literatura por tener un punto de ebullición más alto.

Como puede verse en la gráfica anterior del logaritmo de la presión de vapor frente a la temperatura para cualquier compuesto químico puro dado, su punto de ebullición normal puede servir como una indicación de la volatilidad general de ese compuesto. Un compuesto puro dado tiene solo un punto de ebullición normal, si lo hubiera, y el punto de ebullición y el punto de fusión normales de un compuesto pueden servir como propiedades físicas características de ese compuesto, enumeradas en los libros de referencia. Cuanto más alto sea el punto de ebullición normal de un compuesto, menos volátil será el compuesto en general y, a la inversa, cuanto más bajo sea el punto de ebullición normal de un compuesto, más volátil será el compuesto en general. Algunos compuestos se descomponen a temperaturas más altas antes de alcanzar su punto de ebullición normal, o incluso a veces su punto de fusión. Para un compuesto estable, el punto de ebullición varía desde su punto triple hasta su punto crítico, dependiendo de la presión externa. Más allá de su punto triple, el punto de ebullición normal de un compuesto, si lo hubiera, es más alto que su punto de fusión. Más allá del punto crítico, las fases líquidas y de vapor de un compuesto se fusionan en una sola fase, que puede denominarse gas sobrecalentado. A cualquier temperatura dada, si el punto de ebullición normal de un compuesto es más bajo, ese compuesto generalmente existirá como un gas a la presión externa atmosférica. Si el punto de ebullición normal del compuesto es mayor, entonces ese compuesto puede existir como un líquido o sólido a esa temperatura dada a la presión externa atmosférica, y así existirá en equilibrio con su vapor (si es volátil) si sus vapores están contenidos. Si los vapores de un compuesto no están contenidos, algunos compuestos volátiles pueden evaporarse a pesar de sus puntos de ebullición más altos.

En general, los compuestos con enlaces iónicos tienen puntos de ebullición normales altos, si no se descomponen antes de alcanzar temperaturas tan altas. Muchos metales tienen altos puntos de ebullición, pero no todos. Generalmente, en compuestos con moléculas unidas covalentemente, a medida que aumenta el tamaño de la molécula (o masa molecular), aumenta el punto de ebullición normal. Cuando el tamaño molecular se convierte en el de una macromolécula, polímero o, por lo demás, muy grande, el compuesto a menudo se descompone a alta temperatura antes de que se alcance el punto de ebullición. Otro factor que afecta el punto de ebullición normal de un compuesto es la polaridad de sus moléculas. A medida que aumenta la polaridad de las moléculas de un compuesto, aumenta su punto de ebullición normal, siendo iguales otros factores. Está estrechamente relacionada con la capacidad de una molécula para formar enlaces de hidrógeno (en estado líquido), lo que dificulta que las moléculas abandonen el estado líquido y, por lo tanto, aumentan el punto de ebullición normal del compuesto. Los ácidos carboxílicos simples se dimerizan formando enlaces de hidrógeno entre las moléculas. Un factor menor que afecta los puntos de ebullición es la forma de una molécula. Hacer que la forma de una molécula sea más compacta tiende a disminuir ligeramente el punto de ebullición normal en comparación con una molécula equivalente con más área de superficie.

La mayoría de los compuestos volátiles (en cualquier lugar cercano a la temperatura ambiente) pasan por una fase líquida intermedia mientras se calientan desde una fase sólida para transformarse finalmente en una fase de vapor. En comparación con la ebullición, una sublimación es una transformación física en la que un sólido se convierte directamente en vapor, lo que ocurre en algunos casos selectos, como con el dióxido de carbono a presión atmosférica. Para tales compuestos, un punto de sublimación es una temperatura a la cual un sólido que se convierte directamente en vapor tiene una presión de vapor igual a la presión externa.

En la sección anterior, se cubrieron los puntos de ebullición de los compuestos puros. Las presiones de vapor y los puntos de ebullición de las sustancias pueden verse afectados por la presencia de impurezas disueltas (solutos) u otros compuestos miscibles, dependiendo el grado de efecto de la concentración de las impurezas u otros compuestos. La presencia de impurezas no volátiles tales como sales o compuestos de una volatilidad muy inferior al compuesto del componente principal disminuye su fracción molar y la volatilidad de la solución y, por lo tanto, eleva el punto de ebullición normal en proporción a la concentración de los solutos. Este efecto se llama elevación del punto de ebullición. Como ejemplo común, el agua salada hierve a una temperatura más alta que el agua pura.

En otras mezclas de compuestos miscibles (componentes), puede haber dos o más componentes de volatilidad variable, cada uno con su propio punto de ebullición de componente puro a cualquier presión dada. La presencia de otros componentes volátiles en una mezcla afecta las presiones de vapor y, por lo tanto, los puntos de ebullición y de rocío de todos los componentes de la mezcla. El punto de rocío es una temperatura a la cual un vapor se condensa en un líquido. Además, a cualquier temperatura dada, la composición del vapor es diferente de la composición del líquido en la mayoría de estos casos. Para ilustrar estos efectos entre los componentes volátiles de una mezcla, se usa comúnmente un diagrama de punto de ebullición. La destilación es un proceso de ebullición y [generalmente] condensación que aprovecha estas diferencias en la composición entre las fases líquida y vapor.



</doc>
<doc id="3273" url="https://es.wikipedia.org/wiki?curid=3273" title="Microscopio">
Microscopio

El microscopio (del griego μικρός "micrós", ‘pequeño’, y σκοπέω "scopéo", ‘mirar’) es una herramienta que permite observar objetos que son demasiado pequeños para ser observados a simple vista. El tipo más común y el primero que fue inventado es el microscopio óptico. Se trata de un instrumento que contiene dos lentes que permiten obtener una imagen aumentada del objeto y que funciona por refracción. 
La ciencia que investiga los objetos pequeños utilizando este instrumento se llama microscopía.
El microscopio fue inventado por Zacharias Janssen en 1590. En 1665 aparece en la obra de William Harvey sobre la circulación sanguínea al mirar al microscopio los capilares sanguíneos, y Robert Hooke publicó su obra "Micrographia". 

En 1665 Robert Hooke observó con un lente un delgado corte de corcho y notó que el material era poroso; contenía cavidades poco profundas a modo de celditas a las que llamó "células". Se trataba de la primera observación de células muertas. Unos años más tarde, Marcello Malpighi, anatomista y biólogo italiano, observó células vivas. Fue el primero en estudiar tejidos vivos al microscopio.

A mediados del siglo XVII el neerlandés Anton van Leeuwenhoek, utilizando microscopios simples de fabricación propia, describió por primera vez protozoos, bacterias, espermatozoides y glóbulos rojos. El microscopista Leeuwenhoek, sin ninguna preparación científica, puede considerarse el fundador de la bacteriología. Tallaba él mismo sus lupas, sobre pequeñas esferas de cristal, cuyos diámetros no alcanzaban el milímetro (su campo de visión era muy limitado, de décimas de milímetro). Con estas pequeñas distancias focales alcanzaba los 275 aumentos. Observó los glóbulos de la sangre, las bacterias y los protozoos; examinó por primera vez los glóbulos rojos y descubrió que el semen contiene espermatozoides. Durante su vida no reveló sus métodos secretos, y a su muerte, en 1723, 26 de sus aparatos fueron cedidos a la Royal Society de Londres.

Durante el siglo XVIII continuó el progreso y se lograron objetivos acromáticos por la asociación de Chris Neros y Flint Crown, obtenidos en 1740 por H. M. Hall y mejorados por John Dollond. De esta época son los estudios efectuados por Isaac Newton y Leonhard Euler. En el siglo XIX, al descubrirse que la dispersión y la refracción se podían modificar con combinaciones adecuadas de dos o más medios ópticos, se lanzan al mercado objetivos acromáticos excelentes.

Durante el siglo XVIII el microscopio tuvo diversos adelantos mecánicos que aumentaron su estabilidad y su facilidad de uso, aunque no se desarrollaron por el momento mejoras ópticas. Las mejoras más importantes de la óptica surgieron en 1877, cuando Ernst Abbe publicó su teoría del microscopio y, por encargo de Carl Zeiss, mejoró la microscopía de inmersión sustituyendo el agua por aceite de cedro, lo que permite obtener aumentos de 2000. A principios de los años 1930 se había alcanzado el límite teórico para los microscopios ópticos, no consiguiendo estos aumentos superiores a 500X o 1000X. Sin embargo, existía un deseo científico de observar los detalles de las estructuras celulares (núcleo, mitocondria, etc.).
El microscopio electrónico de transmisión (TEM) fue el primer tipo de microscopio electrónico desarrollado. Utiliza un haz de electrones en lugar de luz para enfocar la muestra, consiguiendo aumentos de 100.000X. Fue desarrollado por Max Knoll y Ernst Ruska en Alemania en 1931. Posteriormente, en 1942 se desarrolla el microscopio electrónico de barrido.




</doc>
<doc id="3274" url="https://es.wikipedia.org/wiki?curid=3274" title="Evolución (desambiguación)">
Evolución (desambiguación)

El término evolución puede referirse a:

En biología y antropología:

En física:
En geología y mineralogía:


En astronomía:


En el arte:

En automovilismo:

Otros usos:

</doc>
<doc id="3275" url="https://es.wikipedia.org/wiki?curid=3275" title="Evolución biológica">
Evolución biológica

La evolución biológica es el conjunto de cambios en caracteres fenotípicos y genéticos de poblaciones biológicas a través de generaciones. Dicho proceso ha originado la diversidad de formas de vida que existen sobre la Tierra a partir de un antepasado común. Los procesos evolutivos han producido la biodiversidad en cada nivel de la organización biológica, incluyendo los de especie, población, organismos individuales y molecular (evolución molecular). Toda la vida en la Tierra procede de un último antepasado común universal que existió entre hace aproximadamente unos 4250 millones de años.

La palabra evolución se utiliza para describir los cambios y fue aplicada por primera vez en el siglo XVIII por un biólogo suizo, Charles Bonnet, en su obra "Consideration sur les corps organisés". No obstante, el concepto de que la vida en la Tierra evolucionó a partir de un ancestro común ya había sido formulado por varios filósofos griegos, y la hipótesis de que las especies se transforman continuamente fue postulada por numerosos científicos de los siglos XVIII y XIX, a los cuales Charles Darwin citó en el primer capítulo de su libro "El origen de las especies". Sin embargo, fue el propio Darwin en 1859, quien sintetizó un cuerpo coherente de observaciones y profundizó el mecanismo de cambio llamado selección natural, lo que consolidó el concepto de la evolución biológica hasta convertirlo en una verdadera teoría científica. Anteriormente, el concepto de selección natural ya había sido aportado en el por Al-Jahiz (776-868), en su "Libro de los animales", con postulados claves sobre la lucha por la supervivencia de las especies, y la herencia de características exitosas mediante reproducción.

La evolución como propiedad inherente a los seres vivos no es materia de debate en la comunidad científica dedicada a su estudio; sin embargo, los mecanismos que explican la transformación y diversificación de las especies se hallan bajo intensa y continua investigación científica.

Dos naturalistas, Charles Darwin y Alfred Russel Wallace, propusieron de forma independiente en 1858 que la selección natural era el mecanismo básico responsable del origen de nuevas variantes genotípicas y en última instancia, de nuevas especies.

Desde la década de 1950 la teoría de la evolución combina las propuestas de Darwin y Wallace con las leyes de Mendel y otros avances posteriores en la genética; por eso se la denomina síntesis moderna o «teoría sintética». Según esta teoría, la evolución se define como un cambio en la frecuencia de los alelos de una población a lo largo de las generaciones. Este cambio puede ser causado por diferentes mecanismos, tales como la selección natural, la deriva genética, la mutación y la migración o flujo genético. La teoría sintética recibe en la actualidad una aceptación general de la comunidad científica, aunque también algunas críticas. Los avances de otras disciplinas relacionadas, como la biología molecular, la genética del desarrollo o la paleontología han enriquecido la teoría sintética desde su formulación, en torno a 1940. Actualmente siguen surgiendo hipótesis sobre los mecanismos del cambio evolutivo basadas en datos empíricos tomados de organismos vivos.

La evidencia del proceso evolutivo surge del conjunto de pruebas que los científicos han reunido para demostrar que la evolución es un proceso característico de la materia viva y que todos los organismos que viven en la Tierra descienden de un último antepasado común universal. Las especies actuales son un estado en el proceso evolutivo y su riqueza relativa y niveles de complejidad biológica son el producto de una larga serie de eventos de especiación y de extinción.

La existencia de un ancestro común puede deducirse a partir de unas características simples de los organismos. Primero, existe evidencia proveniente de la biogeografía: tanto Charles Darwin como Alfred Russell Wallace se percataron de que la distribución geográfica de especies diferentes depende de la distancia y el aislamiento de las áreas que ocupan, y no de condiciones ecológicas y climatológicas similares, como sería de esperar si las especies hubieran aparecido al mismo tiempo ya adaptadas a su medio ambiente. Posteriormente, el descubrimiento de la tectónica de placas fue muy importante para la teoría de la evolución, al proporcionar una explicación para las similitudes entre muchos grupos de especies en continentes que se encontraban unidos en el pasado. Segundo, la diversidad de la vida sobre la Tierra no se resuelve en un conjunto de organismos completamente únicos, sino que los mismos comparten una gran cantidad de similitudes morfológicas. Así, cuando se comparan los órganos de los distintos seres vivos, se encuentran semejanzas en su constitución que señalan el parentesco que existe entre especies diferentes. Estas semejanzas y su origen permiten clasificar a los órganos en homólogos, si tienen un mismo origen embrionario y evolutivo, y análogos, si tienen diferente origen embrionario y evolutivo, pero la misma función. Los estudios anatómicos han encontrado homología en muchas estructuras superficialmente tan diferentes como las espinas de los cactos y las trampas de varias plantas insectívoras que indican que son simplemente hojas que han experimentado modificaciones adaptativas. Los procesos evolutivos explican asimismo la presencia de órganos vestigiales, que están reducidos y no tienen función aparente, pero que muestran claramente que derivan de órganos funcionales presentes en otras especies, tales como los huesos rudimentarios de las patas posteriores presentes en algunas serpientes.

La embriología, a través de los estudios comparativos de las etapas embrionarias de distintas clases de animales, ofrece otro conjunto de indicios del proceso evolutivo. Se ha encontrado que en estas primeras etapas del desarrollo, muchos organismos muestran características comunes que sugieren la existencia de un patrón de desarrollo compartido entre ellas, lo que, a su vez, sugiere la existencia de un antepasado común. El hecho de que los embriones tempranos de vertebrados como los mamíferos y aves posean hendiduras branquiales, que luego desaparecen conforme avanza el desarrollo, puede explicarse si se hallan emparentados con los peces.

Otro grupo de pistas proviene del campo de la sistemática. Los organismos pueden ser clasificados usando las similitudes mencionadas en grupos anidados jerárquicamente, muy similares a un árbol genealógico. Si bien las investigaciones modernas sugieren que, debido a la transferencia horizontal de genes, este árbol de la vida puede ser más complicado que lo que se pensaba, ya que muchos genes se han distribuido independientemente entre especies distantemente relacionadas.

Las especies que han vivido en épocas remotas han dejado registros de su historia evolutiva. Los fósiles, conjuntamente con la anatomía comparada de los organismos actuales, constituyen la evidencia paleontológica del proceso evolutivo. Mediante la comparación de las anatomías de las especies modernas con las ya extintas, los paleontólogos pueden inferir los linajes a los que unas y otras pertenecen. Sin embargo, la investigación paleontológica para buscar conexiones evolutivas tiene ciertas limitaciones. De hecho, es útil solo en aquellos organismos que presentan partes del cuerpo duras, tales como caparazones, dientes o huesos. Más aún, ciertos otros organismos, como los procariotas ―las bacterias y arqueas― presentan una cantidad limitada de características comunes, por lo que sus fósiles no proveen información sobre sus ancestros.

Un método más reciente para probar el proceso evolutivo es el estudio de las similitudes bioquímicas entre los organismos. Por ejemplo, todas las células utilizan el mismo conjunto básico de nucleótidos y aminoácidos. El desarrollo de la genética molecular ha revelado que el registro evolutivo reside en el genoma de cada organismo y que es posible datar el momento de la divergencia de las especies a través del reloj molecular basado en las mutaciones acumuladas en el proceso de evolución molecular. Por ejemplo, la comparación entre las secuencias del ADN del humano y del chimpancé ha confirmado la estrecha similitud entre las dos especies y ha ayudado a elucidar cuándo existió el ancestro común de ambas.

El origen de la vida, aunque atañe al estudio de los seres vivos, es un tema que no es abordado por la teoría de la evolución; pues esta última solo se ocupa del cambio en los seres vivos, y no del origen, cambios e interacciones de las moléculas orgánicas de las que estos proceden.

No se sabe mucho sobre las etapas más tempranas y previas al desarrollo de la vida, y los intentos realizados para tratar de desvelar la historia más temprana del origen de la vida generalmente se enfocan en el comportamiento de las macromoléculas, debido a que el consenso científico actual es que la compleja bioquímica que constituye la vida provino de reacciones químicas simples, si bien persisten las controversias acerca de cómo ocurrieron las mismas. Sin embargo, los científicos están de acuerdo en que todos los organismos existentes comparten ciertas características ―incluyendo la presencia de estructura celular y de código genético― que estarían relacionadas con el origen de la vida.

Tampoco está claro cuáles fueron los primeros desarrollos de la vida (protobiontes), la estructura de los primeros seres vivos o la identidad y la naturaleza del último antepasado común universal. Las bacterias y arqueas, los primeros organismos que dejaron una huella en el registro fósil, son demasiado complejas para haber surgido directamente de los materiales no vivos. La falta de indicios geoquímicos o fósiles de organismos anteriores ha dejado un amplio campo libre para las hipótesis. Aunque no hay consenso científico sobre cómo comenzó la vida, se acepta la existencia del último antepasado común universal porque sería prácticamente imposible que dos o más linajes separados pudieran haber desarrollado de manera independiente los muchos complejos mecanismos bioquímicos comunes a todos los organismos vivos.

Se ha propuesto que el inicio de la vida pueden haber sido moléculas autorreplicantes como el ARN, o ensamblajes de células simples denominadas nanocélulas. Una hipótesis alternativa es la del comienzo de la vida en otras partes del Universo, desde donde habría llegado a la Tierra en cometas o meteoritos, en el proceso denominado panspermia.

Detallados estudios químicos basados en isótopos de carbono de rocas del eón Arcaico sugieren que las primeras formas de vida emergieron en la Tierra probablemente hace más de 4250 millones de años, al final de la era Hádica, y hay claros indicios geoquímicos ―tales como la presencia en rocas antiguas de isótopos de azufre producidos por la reducción microbiana de sulfatos― que indican su presencia en la era Paleoarcaica, hace tres mil cuatrocientos setenta millones de años. Los estromatolitos ―capas de roca producidas por comunidades de microorganismos― más antiguos se reconocen en estratos de 3700 millones de años, mientras que los microfósiles filiformes más antiguos, morfológicamente similares a cianobacterias, se encuentran en estratos de sílex de 3500 millones de años hallados en Australia.

Asimismo, los fósiles moleculares derivados de los lípidos de la membrana plasmática y del resto de la célula ―denominados «biomarcadores»― confirman que ciertos organismos similares a cianobacterias habitaron los océanos arcaicos hace más de 2700 millones de años. Estos microbios fotoautótrofos liberaban oxígeno, que comenzó a acumularse en la atmósfera hace aproximadamente 2200 millones de años y transformó definitivamente la composición de esta. La aparición de una atmósfera rica en oxígeno tras el surgimiento de organismos fotosintéticos puede también rastrearse por los depósitos laminares de hierro y las bandas rojas de los óxidos de hierro posteriores. La abundancia de oxígeno posibilitó el desarrollo de la respiración celular aeróbica, que emergió hace aproximadamente 2000 millones de años.

Desde la formación de estas primeras formas de vida compleja, los procariotas, hace 4250 millones de años, pasaron miles de millones de años sin ningún cambio significativo en la morfología u organización celular en estos organismos, hasta que surgieron los eucariotas a partir de la integración de una arquea del clado Asgard y una alfaproteobacteria formando una asociación cooperativa denominada endosimbiosis. Los eucariotas cladisticamente se consideran un clado más dentro de las arqueas. Las bacterias incorporadas a las células hospedantes arqueanas, iniciaron un proceso de coevolución, por el cual las bacterias originaron las mitocondrias o hidrogenosomas en los eucariotas.

Un segundo evento independiente de endosimbiosis se dio hace 1500-1600 millones de años que llevó a la formación de los cloroplastos a partir de una cianobacteria y un protozoo los cuales darían origen a las algas rojas y algas verdes, de las algas verdes posteriormente evolucionaron las plantas durante el Cámbrico. La evidencia tanto molecular como paleontológica indica que las primeras células eucarióticas surgieron hace unos 2309 millones o antes. Los fósiles más antiguos que se considerarían eucariotas correspoden a la biota francevillense de 2100 millones de años que probablemente fueron mohos mucilaginosos que representarían los primeros indicios de vida pluricelular. Los organismos midieron alrededor de 12 cm y consistían en discos planos con una morfología característica e incluía individuos circulares y alargados. También habían formas de aspecto vermiforme. En ciertos aspectos son parecidos a algunos organismos de la Biota Ediacara. Además según los estudios científicos podían tener un estado de vida pluricelular y uno unicelular, ya que se desarrollarían de agregados celulares capaces de formar cuerpos fructíferos plasmodiales. 

La historia de la vida sobre la Tierra fue la de los eucariotas unicelulares, procariotas y arqueas hasta hace aproximadamente 580 millones de años, momento en el que los primeros organismos multicelulares aparecieron en los océanos en el período denominado Ediacárico. Estos organismos son conocidos como la Biota del periodo Ediacárico.

Es posible que algunos organismos ediacáricos estuvieran estrechamente relacionados con grupos que predominaron más adelante, como los poríferos o los cnidarios. No obstante, debido a la dificultad a la hora de deducir las relaciones evolutivas en estos organismos, algunos paleontólogos han sugerido que la biota de Ediacara representa una rama completamente extinta, un «experimento fallido» de la vida multicelular, y que la vida multicelular posterior volvió a evolucionar más tarde a partir de organismos unicelulares no relacionados. En cualquier caso, la evolución de los organismos pluricelulares ocurrió en múltiples eventos independientes, en organismos tan diversos como las esponjas, algas pardas, cianobacterias, hongos mucosos y mixobacterias.

Poco después de la aparición de los primeros organismos multicelulares, una gran diversidad de formas de vida apareció en un período de diez millones de años, en un evento denominado explosión cámbrica, un lapso breve en términos geológicos, pero que implicó una diversificación animal sin paralelo documentada en los fósiles encontrados en los sedimentos de Burgess Shale, Canadá. Durante este período, la mayoría de los filos animales actuales aparecieron en los registros fósiles, como así también una gran cantidad de linajes únicos que ulteriormente se extinguieron. La mayoría de los planes corporales de los animales modernos se originaron durante este período.

Entre los posibles desencadenantes de la explosión cámbrica se incluye la acumulación de oxígeno en la atmósfera debido a la fotosíntesis.

Aproximadamente hace 500 millones de años, las plantas y los hongos colonizaron la tierra y les siguieron rápidamente los artrópodos y otros animales.

Los anfibios aparecieron en la historia de la Tierra hace alrededor de 300 millones de años, seguidos por los primeros amniotas, y luego por los mamíferos, hace unos 200 millones de años, y las aves, hace 150 millones de años. Sin embargo, los organismos microscópicos, similares a aquellos que evolucionaron tempranamente, continúan siendo la forma de vida predominante en la Tierra, ya que la mayor parte de las especies y la biomasa terrestre está constituida por procariotas.

Varios filósofos griegos de la antigüedad contemplaron la posibilidad de cambios en los organismos vivos a través del tiempo. Anaximandro (ca. 610-546 a. C.) sugirió que los primeros animales vivían en el agua y que dieron origen a los animales terrestres. Empédocles (ca. 490-430 a. C.) escribió que los primeros seres vivos provenían de la tierra y las especies surgieron mediante procesos naturales sin un organizador o una causa final. Tales propuestas sobrevivieron hasta la época romana. El poeta y filósofo Lucrecio, siguió a Empédocles en su obra maestra "De rerum natura" ("Sobre la naturaleza de las cosas") donde el universo funciona a través de mecanismos naturalistas, sin ninguna intervención sobrenatural. Si la visión mecanicista se encuentra en estos filósofos, la teleológica ocurre en Heráclito, quien concibe el proceso como un desarrollo racional, de acuerdo con el "Logos". El desarrollo, así como el proceso de convertirse, en general, fue negado por los filósofos eleáticos.

Las obras de Aristóteles (384-322 a. C.), el primer naturalista cuyo trabajo se ha conservado con detalle, contienen observaciones e interpretaciones muy sagaces, si bien mezcladas con mitos y errores diversos que reflejan el estado del conocimiento en su época; es notable su esfuerzo en exponer las relaciones existentes entre los seres vivos como una "scala naturae" ―tal como se describe en "Historia animalium"― en la que los organismos se clasifican de acuerdo con una estructura jerárquica, «escalera de la vida» o «cadena del Ser», ordenados según la complejidad de sus estructuras y funciones, con los organismos que muestran una mayor vitalidad y capacidad de movimiento descritos como «organismos superiores». En contraste con estos puntos de vista materialistas, el aristotelismo consideraba todas las cosas naturales como actualizaciones de posibilidades naturales fijas, conocidas como formas. Esto era parte de una comprensión teleológica de la naturaleza en la que todas las cosas tienen un papel destinado a jugar en un orden cósmico divino. Toda la transición de potencialidad a actualidad (de"dynamis" a "entelecheia") no es más que una transición de lo inferior a lo superior, a lo perfecto, a lo Divino. Aristóteles criticó la teoría evolutiva materialista de Empédocles, en la que accidentes azarosos pudieran conducir a resultados ordenados, sin embargo, no argumenta que las especies no puedan cambiar o extinguirse y aceptó que nuevos tipos de animales pueden ocurrir en casos muy raros.

Los estoicos siguieron a Heráclito y Aristóteles en las líneas principales de su física. Con ellos, todo el proceso se lleva a cabo de acuerdo a los fines de la Divinidad. Las variaciones de esta idea se convirtieron en la comprensión estándar de la Edad Media y se integraron al cristianismo. San Agustín toma una visión evolutiva como base para su filosofía de la historia. Erigena y algunos de sus seguidores parecen enseñar una especie de evolución. Tomás de Aquino no detectó ningún conflicto en un universo divinamente creado y desarrollado con el tiempo a través de mecanismos naturales, argumentando que la autonomía de la naturaleza era signo de Dios ("Quinta vía"). 

Algunos antiguos pensadores chinos expresaron igualmente la idea de que las especies biológicas cambian. Zhuangzi, un filósofo taoísta que vivió alrededor del , mencionó que las formas de vida tienen una habilidad innata o el poder (hua 化) para transformarse y adaptarse a su entorno.

Según Joseph Needham, el taoísmo niega explícitamente la inmutabilidad de las especies biológicas y los filósofos taoístas especularon que las mismas desarrollaron diferentes atributos en respuesta a distintos entornos. De hecho, el taoísmo se refiere a los seres humanos, la naturaleza y el cielo como existentes en un estado de «constante transformación», en contraste con la visión más estática de la naturaleza típica del pensamiento occidental.

Si bien la idea de la evolución biológica ha existido desde épocas remotas y en diferentes culturas —por ejemplo, en la sociedad musulmana la esbozaron en el siglo IX Al-Jahiz y en el siglo XIII Nasir al-Din al-Tusi respectivamente—, la teoría moderna no se estableció hasta llegados los siglos XVIII y XIX, con la contribución de científicos como Christian Pander, Jean-Baptiste Lamarck y Charles Darwin.

En el siglo XVII, el nuevo método de la ciencia moderna rechazó el enfoque aristotélico, la idea de las causas finales. Buscó explicaciones de los fenómenos naturales en términos de leyes físicas que eran las mismas para todas las cosas visibles y que no requerían la existencia de ninguna categoría natural fija u orden divino cósmico. En biología, sin embargo, persistió durante más tiempo la teleología, es decir, la visión según la cual existen fines en la naturaleza. Este nuevo enfoque tardó en arraigarse en las ciencias biológicas, el último bastión del concepto de tipos naturales fijos. John Ray aplicó uno de los términos anteriormente más generales para los tipos naturales fijos, "especies", a los tipos de plantas y animales, pero identificó estrictamente cada tipo de ser vivo como especie y propuso que cada especie pudiera definirse por las características que perpetuaban ellos mismos generación tras generación. La clasificación biológica introducida por Carlos Linneo en 1735 reconoció explícitamente la naturaleza jerárquica de las relaciones entre especies, pero aún consideraba a las especies como fijas según un plan divino.

En el siglo XVIII, la oposición entre fijismo y transformismo fue ambigua. Algunos autores, por ejemplo, admitieron la transformación de las especies a nivel de géneros, pero negaban la posibilidad de que cambiaran de un género a otro. Otros naturalistas hablaban de «progresión» en la naturaleza orgánica, pero es muy difícil determinar si con ello hacían referencia a una transformación real de las especies o se trataba, simplemente, de una modulación de la clásica idea de la "scala naturae". Entre los filósofos alemanes, Herder estableció la doctrina de un desarrollo continuo en la unidad de la naturaleza, de lo inorgánico a lo orgánico, de la piedra a la planta, de la planta al animal y del animal al hombre. Kant también se menciona a menudo como uno de los primeros maestros de la teoría moderna de la descendencia.

Jean-Baptiste Lamarck (1744-1829) formuló la primera teoría de la evolución y propuso que los organismos, es toda su variedad, habían evolucionado desde formas simples creadas por Dios y postuló que los responsables de esa evolución habían sido los propios organismos por su capacidad de adaptarse al ambiente: los cambios en ese ambiente generaban nuevas necesidades en los organismos y esas nuevas necesidades conllevarían una modificación de los mismos que sería heredable. Se apoyó para la formulación de su teoría en la existencia de restos de formas intermedias extintas.
Con esta teoría Lamarck se enfrentó a la creencia general por la que todas las especies habían sido creadas y permanecían inmutables desde su creación y también se opuso al influyente Georges Cuvier (1769-1832) que justificaba la desaparición de las especies no porque fueran formas intermedias entre las primigenias y las actuales, sino porque se trataba de formas de vida diferentes, extinguidas en los diferentes cataclismos geológicos sufridos por la Tierra. Mientras tanto, las ideas de John Ray y de diseño benevolente habían sido desarrolladas por William Paley en la "Teología Natural" (1802), que propuso adaptaciones complejas como evidencia del diseño divino y quien fue admirado por Charles Darwin.

No fue sino hasta la publicación de "El origen de las especies" de Charles Darwin cuando el hecho de la evolución comenzó a ser ampliamente aceptado. Una carta de Alfred Russel Wallace, en la cual revelaba su propio descubrimiento de la selección natural, impulsó a Darwin a publicar su trabajo en evolución. Por lo tanto, a veces se les concede a ambos el crédito por la teoría de la evolución, llamándola también "teoría de Darwin-Wallace".

Un debate particularmente interesante en el campo evolutivo fue el que sostuvieron los naturalistas franceses Georges Cuvier y Étienne Geoffroy Saint-Hilaire en el año 1830. Ambos discrepaban en los criterios fundamentales para describir las relaciones entre los seres vivos; mientras Cuvier se basaba en características anatómicas funcionales, Geoffroy daba más importancia a la morfología. La distinción entre función y forma trajo consigo el desarrollo de dos campos de investigación, conocidos respectivamente como anatomía funcional y anatomía trascendental. Gracias al trabajo del anatomista británico Richard Owen, los dos puntos de vista empezaron a reconciliarse, proceso completado en la teoría de la evolución de Darwin. A partir de <q>la lucha por la existencia</q>, término acuñado por Thomas Malthus en "Ensayo sobre el principio de la población", Darwin planteó un proceso natural de selección de las especies de acuerdo con su capacidad para adaptarse a un medio ambiente determinado.

A pesar de que la teoría de Darwin sacudió profundamente la opinión científica con respecto al desarrollo de la vida, llegando incluso tener influencias sociales, no pudo explicar la fuente de variación existente entre las especies, y además la propuesta de Darwin de la existencia de un mecanismo hereditario (pangénesis) no satisfizo a la mayoría de los biólogos. No fue recién hasta fines del siglo XIX y comienzos del XX, que estos mecanismos pudieron establecerse.

Cuando alrededor del 1900 se «redescubrió» el trabajo que Gregor Mendel llevó a cabo a fines del siglo XIX sobre la naturaleza de la herencia, se estableció una discusión entre los mendelianos (Charles Benedict Davenport) y los biométricos (Walter Frank Raphael Weldon y Karl Pearson), quienes insistían en que la mayoría de los caminos importantes para la evolución debían mostrar una variación continua que no era explicable a través del análisis mendeliano. Finalmente, los dos modelos fueron conciliados y fusionados, principalmente a través del trabajo del biólogo y estadístico Ronald Fisher.
Este enfoque combinado, que aplica un modelo estadístico riguroso a las teorías de Mendel de la herencia vía genes, se dio a conocer en los años 1930 y 1940 y se conoce como la teoría sintética de la evolución.

En los años de la década de 1940, siguiendo el experimento de Griffith, Avery, MacLeod y McCarty lograron identificar de forma definitiva al ácido desoxirribonucléico (ADN) como el «principio transformador» responsable de la transmisión de la información genética. En 1953, Francis Crick y James Watson publicaron su famoso trabajo sobre la estructura del ADN, basado en la investigación de Rosalind Franklin y Maurice Wilkins. Estos avances iniciaron la era de la biología molecular y condujeron a la interpretación de la evolución como un proceso molecular.

A mediados de la década de 1970, Motoo Kimura formuló la teoría neutralista de la evolución molecular, estableciendo de manera firme la importancia de la deriva génica como el principal mecanismo de la evolución. Hasta la fecha continúan los debates en esta área de investigación. Uno de los más importantes es acerca de la teoría del equilibrio puntuado, una teoría propuesta por Niles Eldredge y Stephen Jay Gould para explicar la escasez de formas transicionales entre especies.

Esta etapa del pensamiento evolutivo se inicia con la publicación en agosto de 1858 de un trabajo conjunto de Darwin y Wallace, al que siguió en 1859 el libro de Darwin "El origen de las especies", en el que designa el principio de la selección natural como el principal motor del proceso evolutivo y acepta la tesis lamarckiana de la herencia de los caracteres adquiridos como una fuente de variabilidad biológica; por este motivo, aunque Wallace rechazaba el lamarckismo, se acepta la denominación de «Lamarck-Darwin-Wallace» para referirse a este estadio.

El trabajo de 1858 contenía «una muy ingeniosa teoría para explicar la aparición y perpetuación de las variedades y de las formas específicas en nuestro planeta» según palabras del prólogo escrito por Charles Lyell (1797-1895) y William Jackson Hooker (1785-1865). De hecho, este trabajo presentó por primera vez la hipótesis de la selección natural. Esta hipótesis contenía cinco afirmaciones fundamentales: (1) todos los organismos producen más descendencia de la que el ambiente puede sostener; (2) existe una abundante variabilidad intraespecífica para la mayoría de los caracteres; (3) la competencia por los recursos limitados lleva a la lucha «por la vida» (según Darwin) o «por la existencia» (según Wallace); (4) se produce descendencia con modificaciones heredables; y (5) como resultado, se originan nuevas especies. Lyell y Hooker reconocieron a Darwin como el primero en formular las ideas presentadas en el trabajo conjunto, adjuntando como prueba un ensayo de Darwin de 1844 y una carta que envió a Asa Gray en 1857, ambos publicados junto con un artículo de Wallace. Un análisis comparativo detallado de las publicaciones de Darwin y Wallace revela que las contribuciones de este último fueron más importantes de lo que usualmente se suele reconocer, 

Treinta años más tarde, el codescubridor de la selección natural publicó una serie de conferencias bajo el título de «Darwinism» que tratan los mismos temas que ya había tratado Darwin, pero a la luz de los hechos y de los datos que eran desconocidos en tiempos de Darwin, quien falleció en 1882. Sin embargo, en su "Origen de las especies"', Darwin fue el primero en resumir un conjunto coherente de observaciones que solidificó el concepto de la evolución de la vida en una verdadera teoría científica ―es decir, en un sistema de hipótesis―.

La lista de las propuestas de Darwin presentadas en esta obra se expone a continuación:
El gran logro de Darwin fue demostrar que es posible explicar teleología aparente en términos no-teleológicos o términos causales corrientes. La vida no es direccional, no está encaminada de antemano.

Neodarwinismo es un término acuñado en 1895 por el naturalista y psicólogo inglés George John Romanes (1848-1894) en su obra "Darwin and after Darwin".
El término describe un estado en el desarrollo de la teoría evolutiva que se remonta al citólogo y zoólogo germano August Weismann (1834-1914), quien en 1892 aportó evidencia experimental en contra de la herencia lamarckiana y postuló que el desarrollo del organismo no influye en el material hereditario y que la reproducción sexual en cada generación introduce nuevas variaciones en la población de individuos. La selección natural, entonces, puede actuar sobre la variabilidad de la población y determina el curso del cambio evolutivo.
El neodarwinismo enriqueció el concepto original de Darwin, al destacar el origen de las variaciones entre individuos y excluir la herencia lamarckiana como una explicación viable del mecanismo de herencia. Wallace, quien popularizó el término «darwinismo» en 1889, incorporó plenamente las nuevas conclusiones de Weismann y fue, por consiguiente, uno de los primeros proponentes del neodarwinismo.

Este sistema de hipótesis del proceso evolutivo se originó entre 1937 y 1950.
En contraste con el neodarwinismo de Weismann y Wallace, que daba primacía a la selección natural y postulaba la genética mendeliana como el mecanismo de transmisión de caracteres entre generaciones, la teoría sintética incorporó datos de campos diversos de la biología, como la genética molecular, la sistemática y la paleontología e introdujo nuevos mecanismos para la evolución. Por estas razones, se trata de diferentes teorías aunque a veces se usen los términos indistintamente.

De acuerdo a la gran mayoría de los historiadores de la Biología, los conceptos básicos de la teoría sintética están basados esencialmente en el contenido de seis libros, cuyos autores fueron: el naturalista y genetista ruso americano Theodosius Dobzhansky (1900-1975); el naturalista y taxónomo alemán americano Ernst Mayr (1904-2005); el zoólogo británico Julian Huxley (1887-1975); el paleontólogo americano George G. Simpson (1902-1984); el zoólogo germano Bernhard Rensch (1900-1990) y el botánico estadounidense George Ledyard Stebbins (1906-2000).

Los términos «síntesis evolutiva» y «teoría sintética» fueron acuñados por Julian Huxley en su libro "" (1942), en el que también introdujo el término Biología evolutiva en vez de la frase «estudio de la evolución».
De hecho Huxley fue el primero en señalar que la evolución «debía ser considerada el problema más central y el más importante de la biología y cuya explicación debía ser abordada mediante hechos y métodos de cada rama de la ciencia, desde la ecología, la genética, la paleontología, la embriología, la sistemática hasta la anatomía comparada y la distribución geográfica, sin olvidar los de otras disciplinas como la geología, la geografía y las matemáticas».

La llamada «síntesis evolutiva moderna» es una robusta teoría que actualmente proporciona explicaciones y modelos matemáticos de los mecanismos generales de la evolución o los fenómenos evolutivos, como la adaptación o la especiación. Como cualquier teoría científica, sus hipótesis están sujetas a constante crítica y comprobación experimental. 






En la época de Darwin los científicos no conocían cómo se heredaban las características. Posteriormente se descubrió la relación de la mayoría de las características hereditarias con entidades persistentes llamadas genes, fragmentos de las moléculas lineales de ácido desoxirribonucleico (ADN) del núcleo de las células. El ADN varía entre los miembros de una misma especie y también sufre cambios, mutaciones, o reorganizaciones por recombinación genética.

El fenotipo de un organismo individual es el resultado de su genotipo y la influencia del ambiente en el que vive y ha vivido. Una parte sustancial de la variación entre fenotipos dentro de una población está causada por las diferencias entre sus genotipos. La síntesis evolutiva moderna define la evolución como el cambio de esa variación genética a través del tiempo. La frecuencia de cada alelo fluctúa, siendo más o menos prevalente en relación con otras formas alternativas del mismo gen. Las fuerzas evolutivas actúan mediante la dirección de esos cambios en las frecuencias alélicas en uno u otro sentido. La variación de una población para un gen dado desaparece cuando se produce la fijación de un alelo que ha reemplazado enteramente a todas las otras formas alternativas de ese mismo gen.

La variabilidad surge en las poblaciones naturales por mutaciones en el material genético, migraciones entre poblaciones (flujo genético) y por la reorganización de los genes a través de la reproducción sexual. La variabilidad también puede provenir del intercambio de genes entre diferentes especies, por ejemplo a través de la transferencia horizontal de genes en las bacterias o la hibridación interespecífica en las plantas. A pesar de la constante introducción de variantes nuevas a través de estos procesos, la mayor parte del genoma de una especie es idéntica en todos los individuos que pertenecen a la misma. Sin embargo, aun pequeños cambios en el genotipo pueden llevar a modificaciones sustanciales del fenotipo. Así, los chimpancés y los seres humanos, por ejemplo, solo difieren en aproximadamente el 5 % de sus genomas.

Darwin no conocía la fuente de las variaciones en los organismos individuales, pero observó que parecían ocurrir aleatoriamente. En trabajos posteriores se atribuyó la mayor parte de estas variaciones a las mutaciones. La mutación es un cambio permanente y transmisible en el material genético ―usualmente el ADN o el ARN― de una célula, producido por «errores de copia» en el material genético durante la división celular o por la exposición a radiación, sustancias químicas o la acción de virus. Las mutaciones aleatorias ocurren constantemente en el genoma de todos los organismos, creando nueva variabilidad genética. Las mutaciones pueden no tener efecto alguno sobre el fenotipo del organismo, o ser perjudiciales o beneficiosas. A modo de ejemplo, los estudios realizados sobre la mosca de la fruta ("Drosophila melanogaster"), sugieren que, si una mutación determina un cambio en la proteína producida por un gen, ese cambio será perjudicial en el 70 % de los casos y neutro o levemente beneficioso en los restantes.

La frecuencia de nuevas mutaciones en un gen o secuencia de ADN en cada generación se denomina tasa de mutación. En escenarios de rápido cambio ambiental, una tasa de mutación alta aumenta la probabilidad de que algunos individuos tengan una variante genética adecuada para adaptarse y sobrevivir; por otro lado, también aumenta el número de mutaciones perjudiciales o deletéreas que disminuyen la adaptación de los individuos y eleva la probabilidad de extinción de la especie. Debido a los efectos contrapuestos que las mutaciones pueden tener sobre los organismos, la tasa de mutación óptima para una población es una compensación entre costos y beneficios, que depende de la especie y refleja la historia evolutiva como respuesta a los retos impuestos por el ambiente. Los virus, por ejemplo, presentan una alta tasa de mutación, lo que supone una ventaja adaptativa ya que deben evolucionar rápida y constantemente para sortear a los sistemas inmunes de los organismos que afectan.

La duplicación génica introduce en el genoma copias extras de un gen y, de ese modo, proporciona el material de base para que las nuevas copias inicien su propio camino evolutivo. Si el gen inicial sigue funcionando normalmente, sus copias pueden adquirir nuevas mutaciones sin perjuicio para el organismo que los alberga y llegar con el tiempo a adoptar nuevas funciones. Por ejemplo, en los seres humanos son necesarios cuatro genes para construir las estructuras necesarias para detectar la luz: tres para la visión de los colores y uno para la visión nocturna. Los cuatro genes han evolucionado a partir de un solo gen ancestral por duplicación y posterior divergencia. Otros tipos de mutación pueden ocasionalmente crear nuevos genes a partir del denominado ADN no codificante. También pueden surgir nuevos genes con diferentes funciones a partir de fragmentos de genes duplicados que se recombinan para formar nuevas secuencias de ADN.

Las mutaciones cromosómicas ―también denominadas, aberraciones cromosómicas― son una fuente adicional de variabilidad hereditaria. Así, las translocaciones, inversiones, deleciones, translocaciones robertsonianas y duplicaciones, usualmente ocasionan variantes fenotípicas que se transmiten a la descendencia. Por ejemplo, en el género "Homo" tuvo lugar una fusión cromosómica que dio lugar al cromosoma 2 de los seres humanos, mientras que otros simios conservan 24 pares de cromosomas. No obstante las consecuencias fenotípicas que pueden tener tales mutaciones cromosómicas, su mayor importancia evolutiva reside en acelerar la divergencia de las poblaciones que presentan diferentes configuraciones cromosómica: el flujo genético entre ellas se reduce severamente debido a la esterilidad o semiesterilidad de los individuos heterocigóticos. De este modo, las mutaciones cromosómicas actúan como mecanismos de aislamiento reproductivo que conducen a que las diferentes poblaciones mantengan su identidad como especies a través del tiempo.

Los fragmentos de ADN que pueden cambiar de posición en los cromosomas, tales como los transposones, constituyen una importante fracción del material genético de plantas y animales y pueden haber desempeñado un papel destacado en su evolución. Al insertarse en o escindirse de otras partes del genoma estas secuencias pueden activar, inhibir, eliminar o mutar otros genes y, por ende, crear nueva variabilidad genética. Asimismo, ciertas de estas secuencias se repiten miles o millones de veces en el genoma y muchas de ellas han adoptado funciones importantes, como por ejemplo, la regulación de la expresión genética.

La recombinación genética es el proceso mediante el cual la información genética se redistribuye por transposición de fragmentos de ADN entre dos cromosomas durante la meiosis ―y más raramente en la mitosis―. Los efectos son similares a los de las mutaciones, es decir, si los cambios no son deletéreos se transmiten a la descendencia y contribuyen a incrementar la diversidad dentro de cada especie.

En los organismos asexuales, los genes se heredan en conjunto, o ligados, ya que no se mezclan con los de otros organismos durante los ciclos de recombinación que usualmente se producen durante la reproducción sexual. En contraste, los descendientes de los organismos que se reproducen sexualmente contienen una mezcla aleatoria de los cromosomas de sus progenitores, la cual se produce durante la recombinación meiótica y la posterior fecundación. La recombinación no altera las frecuencias alélicas sino que modifica la asociación existente entre alelos pertenecientes a genes diferentes, produciendo descendientes con combinaciones únicas de genes. La recombinación generalmente incrementa la variabilidad genética y puede aumentar también las tasas de evolución. No obstante, la existencia de la reproducción asexual, tal como ocurre en las plantas apomícticas o en los animales partenogenéticos, indica que este modo de reproducción puede también ser ventajoso en ciertos ambientes. Jens Christian Clausen fue uno de los primeros en reconocer formalmente que la apomixis, particularmente la apomixis facultativa, no necesariamente conduce a una pérdida de variabilidad genética y de potencial evolutivo. Utilizando una analogía entre el proceso adaptativo y la producción a gran escala de automóviles, Clausen arguyó que la combinación de sexualidad (que permite la producción de nuevos genotipos) y de apomixis (que permite la producción ilimitada de los genotipos más adaptados) potencia la capacidad de una especie para el cambio adaptativo.

Aunque el proceso de recombinación posibilita que los genes agrupados en un cromosoma puedan heredarse independientemente, la tasa de recombinación es baja ―aproximadamente dos eventos por cromosoma y por generación―. Como resultado, los genes adyacentes tienden a heredarse conjuntamente, en un fenómeno que se denomina ligamiento. Un grupo de alelos que usualmente se heredan conjuntamente por hallarse ligados se denomina haplotipo. Cuando uno de los alelos en haplotipo es altamente beneficioso, la selección natural puede conducir a un barrido selectivo que aumenta la proporción dentro de la población del resto de los alelos en el haplotipo; este efecto se denomina arrastre por ligamiento o «efecto autostop» (en inglés, "genetic hitchhiking").

Cuando los alelos no se recombinan, como es el caso en el cromosoma Y de los mamíferos o en los organismos asexuales, los genes con mutaciones deletéreas se acumulan, lo que se denomina trinquete de Muller ("Muller ratchet" en inglés). De este modo, al romper los conjuntos de genes ligados, la reproducción sexual facilita la eliminación de las mutaciones perjudiciales y la retención de las beneficiosas, además de la aparición de individuos con combinaciones genéticas nuevas y favorables. Estos beneficios deben contrarrestar otros efectos perjudiciales de la reproducción sexual, como la menor tasa reproductiva de las poblaciones de organismos sexuales y la separación de combinaciones favorables de genes. En todas las especies sexuales, y con la excepción de los organismos hermafroditas, cada población está constituida por individuos de dos sexos, de los cuales solo uno es capaz de engendrar la prole. En una especie asexual, en cambio, todos los miembros de la población tienen esa capacidad, lo que implica un crecimiento más rápido de la población asexual en cada generación. Otro costo del sexo es que los machos y las hembras deben buscarse entre ellos para aparearse y la selección sexual suele favorecer caracteres que reducen la aptitud de los individuos. Este costo del sexo fue expresado por primera vez en términos matemáticos por John Maynard Smith. Las razones de la evolución de la reproducción sexual son todavía poco claras y es un interrogante que constituye un área activa de investigación en Biología evolutiva, que ha inspirado ideas tales como la hipótesis de la Reina Roja. El escritor científico Matt Ridley, que popularizó el término en su libro "", sostiene que existe una carrera armamentista cíclica entre los organismos y sus parásitos y especula que el sexo sirve para preservar los genes circunstancialmente desfavorables, pero potencialmente beneficiosos ante futuros cambios en las poblaciones de parásitos.

Como se ha descrito previamente, desde un punto de vista genético la evolución es un cambio intergeneracional en la frecuencia de los alelos dentro de una población que comparte un mismo patrimonio genético. Una población es un grupo de individuos de la misma especie que comparten un ámbito geográfico. Por ejemplo, todas las polillas de una misma especie que viven en un bosque aislado forman una población. Un gen determinado dentro de la población puede presentar diversas formas alternativas, que son las responsables de la variación entre los diferentes fenotipos de los organismos. Un ejemplo puede ser un gen de la coloración en las polillas que tenga dos alelos: uno para color blanco y otro para color negro. El patrimonio o acervo genético es el conjunto completo de los alelos de una población, de forma que cada alelo aparece un número determinado de veces en un acervo génico. La fracción de genes del patrimonio genético que están representadas por un alelo determinado recibe el nombre de frecuencia alélica, por ejemplo, la fracción de polillas en la población que presentan el alelo para color negro. La evolución tiene lugar cuando hay cambios en la frecuencia alélica en una población de organismos que se reproducen entre ellos, por ejemplo, si el alelo para color negro se hace más común en una población de polillas.

Para comprender los mecanismos que hacen que evolucione una población, es útil conocer las condiciones necesarias para que la población no evolucione. El principio de Hardy-Weinberg determina que la frecuencia de los alelos de una población suficientemente grande permanecerá constante solo si la única fuerza que actúa es la recombinación aleatoria de alelos durante la formación de los gametos y la posterior combinación de los mismos durante la fertilización. En ese caso, la población se encuentra en "equilibrio de Hardy-Weinberg" y, por lo tanto, no evoluciona.

El flujo genético es el intercambio de genes entre poblaciones, usualmente de la misma especie. Como ejemplos de flujo génico se pueden mencionar el cruzamiento de individuos tras la inmigración de una población en el territorio de otra, o, en el caso de las plantas, el intercambio de polen entre poblaciones diferentes. La transferencia de genes entre especies conlleva la formación de híbridos o la transferencia horizontal de genes.

La inmigración y la emigración de individuos en las poblaciones naturales pueden causar cambios en las frecuencias alélicas, como así también la introducción ―o desaparición― de variantes alélicas dentro de un acervo genético ya establecido. Las separaciones físicas en el tiempo, espacio o nichos ecológicos específicos que puede existir entre las poblaciones naturales restringen o imposibilitan el flujo génico. Además de estas restricciones al intercambio de genes entre poblaciones existen otros mecanismos de aislamiento reproductivo conformados por características, comportamientos y procesos fisiológicos que impiden que los miembros de dos especies diferentes puedan cruzarse o aparearse entre sí, producir descendencia o que esta sea viable o fértil. Estas barreras constituyen una fase indispensable en la formación de nuevas especies ya que mantienen las características propias de las mismas a través del tiempo al restringir o eliminar el flujo genético entre los individuos de diferentes poblaciones.

Las especies distintas pueden ser interfértiles, dependiendo de cuánto han divergido desde su ancestro común; por ejemplo, la yegua y el asno pueden aparearse y producir la mula. Tales híbridos son generalmente estériles debido a las diferencias cromosómicas entre las especies parentales, que impiden el emparejamiento correcto de los cromosomas durante la meiosis. En este caso, las especies estrechamente relacionadas pueden cruzarse con regularidad, pero la selección natural actúa contra los híbridos. Sin embargo, de vez en cuando se forman híbridos viables y fértiles que pueden presentar propiedades intermedias entre sus especies paternales o poseer un fenotipo totalmente nuevo.

La importancia de la hibridación en la creación de nuevas especies de animales no está clara, aunque existen ejemplos bien documentados como el de la rana "Hyla versicolor". La hibridación es, sin embargo, un mecanismo importante de formación de nuevas especies en las plantas, ya que estas toleran la poliploidía ―la duplicación de todos los cromosomas de un organismo― más fácilmente que los animales; la poliploidía restaura la fertilidad en los híbridos interespecíficos debido a que cada cromosoma es capaz de aparearse con un compañero idéntico durante la meiosis.

Hay dos mecanismos básicos de cambio evolutivo: la selección natural y la deriva genética. La selección natural favorece a los genes que mejoran la capacidad de supervivencia y reproducción del organismo. La deriva genética es el cambio en la frecuencia de los alelos, provocado por transmisión aleatoria de los genes de una generación a la siguiente. La importancia relativa de la selección natural y de la deriva genética en una población varía dependiendo de la fuerza de la selección y del tamaño poblacional efectivo, que es el número de ejemplares de esa población capaces de reproducirse. La selección natural suele predominar en las poblaciones grandes, mientras que la deriva genética predomina en las pequeñas. El predominio de la deriva genética en poblaciones pequeñas puede llevar incluso a la fijación de mutaciones ligeramente deletéreas. Como resultado de ello, los cambios en el tamaño de una población pueden influir significativamente en el curso de la evolución. Los denominados «cuellos de botella», o descensos drásticos temporarios del tamaño efectivo de la población, suponen una pérdida o erosión de la variabilidad genética y conllevan la formación de poblaciones genéticamente más uniformes. Los cuellos de botella pueden ser el resultado de catástrofes, variaciones en el medio ambiente o alteraciones en el flujo genético causadas por una migración reducida, la expansión a nuevos hábitats, o una subdivisión de la población.

La selección natural es el proceso por el cual las mutaciones genéticas que mejoran la capacidad reproductiva se vuelven, y permanecen, cada vez más frecuentes en las sucesivas generaciones de una población. Se la califica a menudo de «mecanismo autoevidente», pues es la consecuencia necesaria de tres hechos simples: (a) dentro de las poblaciones de organismos hay variación heredable (b) los organismos producen más descendientes de los que pueden sobrevivir, y (c) tales descendientes tienen diferentes capacidades para sobrevivir y reproducirse.

El concepto central de la selección natural es la aptitud biológica de un organismo. La aptitud, ajuste o adecuación influye en la medida de la contribución genética de un organismo a la generación siguiente. Sin embargo, la aptitud no es simplemente igual al número total de descendientes de un determinado organismo, ya que también cuantifica la proporción de generaciones posteriores que llevan los genes de ese organismo. Por ejemplo, si un organismo puede sobrevivir y reproducirse, pero sus descendientes son demasiado pequeños o enfermizos como para llegar a la edad reproductiva, la contribución genética de ese organismo a las futuras generaciones será muy baja y, por ende, su aptitud también lo es.

Por consiguiente, si un alelo aumenta la aptitud más que otros, con cada generación el alelo será más común dentro de la población. Se dice que tales rasgos son «seleccionados favorablemente». Una mejora de la supervivencia o una mayor fecundidad son ejemplos de rasgos que pueden aumentar la aptitud. En cambio, la menor aptitud causada por un alelo menos beneficioso o deletéreo hace que este sea cada vez más raro en la población y sufra una «selección negativa». Hay que subrayar que la aptitud de un alelo no es una característica fija: si el ambiente cambia, los rasgos que antes eran neutros o nocivos pueden ser beneficiosos y viceversa. Por ejemplo, la polilla "Biston betularia" presenta dos colores, uno claro denominado forma "typica" y otro oscuro llamado forma "carbonaria". La forma "typica", como su nombre indica, es la más frecuente en esta especie. No obstante, durante la revolución industrial en el Reino Unido los troncos de muchos árboles sobre los que se posaban las polillas se ennegrecieron por el hollín, lo que les daba las polillas de color oscuro una mayor oportunidad de sobrevivir y producir más descendientes al pasar más fácilmente desapercibidas para los depredadores. Sólo cincuenta años después de que se descubriera la primera polilla melánica, casi la totalidad de las polillas del área industrial de Manchester eran oscuras. Este proceso se revirtió a causa de la «Ley del aire limpio» ("Clean Air Act") de 1956, que redujo la polución industrial. Al aclararse el color de los troncos, las polillas oscuras volvieron a ser más fácilmente visibles por los depredadores y su número disminuyó. Sin embargo, aunque la dirección de la selección cambie, los rasgos que se hubiesen perdido en el pasado no pueden volver a obtenerse de forma idéntica ―situación que describe la Ley de Dollo o «Ley de la irreversibilidad evolutiva»―. De acuerdo con esta hipótesis, una estructura u órgano que se ha perdido o descartado durante el transcurso de la evolución no volverá a aparecer en ese mismo linaje de organismos.

Según Richard Dawkins, esta hipótesis es «una declaración sobre la improbabilidad estadística de seguir exactamente la misma trayectoria evolutiva dos veces o, de hecho, una misma trayectoria particular en ambas direcciones».

Dentro de una población, la selección natural para un determinado rasgo que varía en forma continua, como la altura, se puede categorizar en tres tipos diferentes. El primero es la «selección direccional», que es un cambio en el valor medio de un rasgo a lo largo del tiempo; por ejemplo, cuando los organismos cada vez son más altos. En segundo lugar se halla la «selección disruptiva» que es la selección de los valores extremos de un determinado rasgo, lo que a menudo determina que los valores extremos sean más comunes y que la selección actúe en contra del valor medio; esto implica, en el ejemplo anterior, que los organismos bajos y altos tienen una ventaja, pero los de altura media no. Finalmente, en la «selección estabilizadora», la selección actúa en contra de los valores extremos, lo que determina una disminución de la varianza alrededor del promedio y una menor variabilidad de la población para ese carácter en particular; si se diera este tipo de selección, todos los organismos de una población adquirirían paulatinamente una altura similar.

Un tipo especial de selección natural es la selección sexual, que actúa a favor de cualquier rasgo que aumente el éxito reproductivo por aumentar el atractivo de un organismo para sus parejas potenciales. Ciertos rasgos adquiridos por los machos por selección sexual ―tales como los cuernos voluminosos, cantos de apareamiento o colores brillantes― pueden reducir las posibilidades de supervivencia, por ejemplo, por atraer a los depredadores. No obstante, esta desventaja reproductiva se compensa por un mayor éxito reproductivo de los machos que presentan estos rasgos.

Un área de estudio activo es la denominada «unidad de selección»; se ha dicho que la selección natural actúa a nivel de genes, células, organismos individuales, grupos de organismos e incluso especies. Ninguno de estos modelos es mutuamente exclusivo, y la selección puede actuar en múltiples niveles a la vez. Por ejemplo, debajo del nivel del individuo, hay genes denominados transposones que intentan replicarse en todo el genoma. La selección por encima del nivel del individuo, como la selección de grupo, puede permitir la evolución de la cooperación.

La deriva genética es el cambio en la frecuencia de los alelos entre una generación y la siguiente, y tiene lugar porque los alelos de la descendencia son una muestra aleatoria de los padres, y por el papel que juega el azar en la hora de determinar si un ejemplar determinado sobrevivirá y se reproducirá. En términos matemáticos, los alelos están sujetos a errores de muestreo. Como resultado de ello, cuando las fuerzas selectivas están ausentes o son relativamente débiles, la frecuencia de los alelos tiende a «derivar» hacia arriba o hacia abajo aleatoriamente (en un paseo aleatorio). Esta deriva se detiene cuando un alelo se convierte finalmente fijado, es decir, o bien desaparece de la población, o bien sustituye totalmente el resto de genes. Así pues, la deriva genética puede eliminar algunos alelos de una población simplemente debido al azar. Incluso en la ausencia de fuerzas selectivas, la deriva genética puede hacer que dos poblaciones separadas que empiezan con la misma estructura genética se separen en dos poblaciones divergentes con un conjunto de alelos diferentes.

El tiempo necesario para que un alelo quede fijado por la deriva genética depende del tamaño de la población; la fijación tiene lugar más rápido en poblaciones más pequeñas. La medida precisa de las poblaciones que es importante en este caso recibe el nombre de tamaño poblacional efectivo, que fue definida por Sewall Wright como el número teórico de ejemplares reproductivos que presenten el mismo grado observado de consanguinidad.

Aunque la selección natural es responsable de la adaptación, la importancia relativa de las dos fuerzas, selección natural y deriva genética, como impulsoras del cambio evolutivo en general es actualmente un campo de investigación en la biología evolutiva. Estas investigaciones fueron inspiradas por la teoría neutralista de la evolución molecular, que postula que la mayoría de cambios evolutivos son el resultado de la fijación de mutaciones neutras, que no tienen ningún efecto inmediato sobre la aptitud de un organismo. Por tanto, en este modelo, la mayoría de los cambios genéticos en una población son el resultado de una presión de mutación constante y de deriva genética.

La adaptación es el proceso mediante el cual una población se adecua mejor a su hábitat y también el cambio en la estructura o en el funcionamiento de un organismo que lo hace más adecuado a su entorno. Este proceso se produce por selección natural durante muchas generaciones y es uno de los fenómenos básicos de la biología. 

Las especies tienden a adaptarse a diferentes nichos ecológicos para reducir al mínimo la competencia entre ellas. Esto se conoce como principio de exclusión competitiva en ecología: dos especies no pueden ocupar el mismo nicho en el mismo ambiente por un largo tiempo. 

La adaptación no implica necesariamente cambios importantes en parte física de un cuerpo. Como ejemplo puede mencionarse a los trematodos ―parásitos internos con estructuras corporales muy simples, pero con un ciclo de vida muy complejo― en los que sus adaptaciones a un medio ambiente tan inusual no son el producto de caracteres observables a simple vista sino en aspectos críticos de su ciclo vital. En general, el concepto de adaptación incluye, además del proceso adaptativo mismo, todos los aspectos de los organismos, de las poblaciones o de las especies que son su resultado. Mediante la utilización del término «adaptación» para el "proceso" evolutivo y «rasgo o carácter adaptativo» para el "producto" del mismo, los dos sentidos del concepto se distinguen perfectamente. Según Theodosius Dobzhansky la «adaptación» es el proceso evolutivo por el cual un organismo se vuelve más capaz de vivir en su hábitat o hábitats, mientras que la «adaptabilidad» es el estado de estar adaptado, o sea, el grado en que un organismo es capaz de vivir y reproducirse en un determinado conjunto de hábitats. Finalmente, un «carácter adaptativo» es uno de los aspectos del desarrollo de un organismo que aumenta su probabilidad de sobrevivir y reproducirse.

La adaptación a veces implica la ganancia de una nueva característica; ejemplos notables son la evolución en laboratorio de las bacterias "Escherichia coli" para que puedan ser capaces de utilizar el ácido cítrico como un nutriente, cuando las bacterias de tipo silvestre no lo pueden hacer; la evolución de una nueva enzima en "Flavobacterium" que permite que estas bacterias puedan crecer en los subproductos de la fabricación del nylon; y la evolución de una vía metabólica completamente nueva en la bacteria del suelo "Sphingobium" que le permite degradar el pesticida sintético pentaclorofenol. En ocasiones, también puede darse la pérdida de una función ancestral. Un ejemplo que muestra los dos tipos de cambio es la adaptación de la bacteria "Pseudomonas aeruginosa" a la fluoroquinolona con cambios genéticos que modifican la molécula sobre la que actúa y por el aumento de la actividad de los transportadores que bombean el antibiótico fuera de la célula. Una idea todavía controvertida, es que algunas adaptaciones pueden aumentar la capacidad de los organismos para generar diversidad genética y para adaptarse por selección natural ―o sea, aumentarían la capacidad de evolución―.

Una consecuencia de la adaptación es la existencia de estructuras con organización interna similar y diferentes funciones en organismos relacionados. Este es el resultado de la modificación de una estructura ancestral para adaptarla a diferentes ambientes y nichos ecológicos. Los huesos de las alas de los murciélagos, por ejemplo, son muy similares a los de los pies del ratón y los de las manos de los primates, debido a que todas estas estructuras estaban presentes en un ancestro común de los mamíferos. Dado que todos los organismos vivos están relacionados en cierta medida, incluso las estructuras que presentan diferencias profundas, como los ojos de los artrópodos, del calamar y de los vertebrados, o las extremidades y las alas de artrópodos y vertebrados, dependen de un conjunto común de genes homólogos que controlan su desarrollo y funcionamiento, lo que se denomina homología profunda.

Durante la adaptación pueden aparecer estructuras vestigiales, carentes de funcionalidad en una especie; sin embargo, la misma estructura es funcional en la especie ancestral o en otras especies relacionadas. Los ejemplos incluyen los pseudogenes, los ojos de los peces cavernícolas ciegos, las alas en las especies de aves que no vuelan y los huesos de la cadera presentes en las ballenas y en las serpientes. En los seres humanos también existen ejemplos de estructuras vestigiales, como las muelas de juicio, el coxis, el apéndice vermiforme, y reacciones involuntarias como la piel de gallina y otros reflejos primitivos.

Algunos rasgos que parecen ser simples adaptaciones son, de hecho, exaptaciones: estructuras originalmente adaptadas para una función, pero que coincidentalmente se hicieron útiles para otro propósito. Un ejemplo es el lagarto africano "Holaspis guentheri" que desarrolló una cabeza y tronco muy aplastados para esconderse en las grietas, como puede observarse en otros lagartos del mismo género. Sin embargo, en esta especie, el cuerpo achatado le permite planear de árbol en árbol. Los pulmones de los peces pulmonados ancestrales son una exaptación de las vejigas natatorias de los peces teleósteos empleadas como regulador de la flotación.

Una rama de la biología evolutiva estudia el desarrollo de las adaptaciones y de las exaptaciones. Esta área de investigación aborda el origen y la evolución del desarrollo embrionario y cómo surgen nuevas características a partir de modificaciones del desarrollo. Estos estudios han demostrado, por ejemplo, que las mismas estructuras óseas de los embriones que forman parte de la mandíbula en algunos animales, en cambio forman parte del oído medio en los mamíferos. Cambios en los genes que controlan el desarrollo también pueden causar que reaparezcan estructuras perdidas durante la evolución, como por ejemplo, los dientes de embriones de pollos mutantes, similares a los de los cocodrilos. De hecho, cada vez es más claro que la mayoría de las alteraciones en la forma de los organismos se deben a cambios en un pequeño conjunto de genes conservados.

La interacción entre organismos puede producir conflicto o cooperación. Cuando interactúan dos especies diferentes, como un patógeno y un hospedador, o un depredador y su presa, la evolución de una de ellas causa adaptaciones en la otra; estos cambios en la segunda especie causan, a su vez, nuevas adaptaciones en la primera. Este ciclo de selección y respuesta recibe el nombre de coevolución. Un ejemplo es la producción de tetradotoxina por parte del tritón de Oregón y la evolución de la resistencia a esta toxina en su predador, la serpiente de jarretera. En esta pareja predador-presa, la carrera armamentista evolutiva ha tenido como resultado una elevada producción de toxina en el tritón, y un aumento correspondiente de resistencia a ella en la serpiente.

La especiación (o cladogénesis) es el proceso por el cual una especie diverge en dos o más especies descendientes. Los biólogos evolutivos ven las especies como fenómenos estadísticos y no como categorías o tipos. Este planteamiento es contrario al aún muy arraigado concepto clásico de especie como una clase de organismos ejemplificados por un «espécimen tipo» que posee todas las características comunes a dicha especie. En su lugar, una especie se concibe ahora como un linaje que comparte un único acervo genético y evoluciona independientemente. Según esta descripción, los límites entre especies son difusos, a pesar de que se utilizan propiedades tanto genéticas como morfológicas para ayudar a diferenciar entre linajes estrechamente relacionados. De hecho, la definición exacta del término «especie» está todavía en discusión, particularmente para organismos basados en células procariotas; es lo que se denomina «problema de las especies». Diversos autores han propuesto una serie de definiciones basadas en criterios diferentes, pero la aplicación de una u otra es finalmente una cuestión práctica, dependiendo en cada caso concreto de las particularidades del grupo de organismos en estudio. Actualmente, la unidad de análisis principal en biología es la población, un conjunto observable de individuos "que interactúan", en lugar de la especie, un conjunto observable de individuos "que se parecen entre sí".
La especiación ha sido observada en múltiples ocasiones tanto en condiciones de laboratorio controladas como en la naturaleza. En los organismos que se reproducen sexualmente, la especiación es el resultado de un aislamiento reproductivo seguido de una divergencia genealógica. Hay cuatro modalidades de especiación. La más habitual en los animales es la especiación alopátrica, que tiene lugar en poblaciones que quedan geográficamente aisladas, como en el caso de la fragmentación de hábitat o las migraciones. En estas condiciones, la selección puede causar cambios muy rápidos en la apariencia y el comportamiento de los organismos. Debido a los procesos de selección y deriva genética, la separación de poblaciones puede tener como resultado la aparición de descendientes que no se pueden reproducir entre ellos.

La segunda modalidad de especiación es la especiación peripátrica, que tiene lugar cuando poblaciones pequeñas quedan aisladas en un nuevo medio. Se diferencia de la especiación alopátrica en que las poblaciones aisladas son numéricamente mucho menores que la población madre. Esto causa una especiación rápida por medio de la aceleración de la deriva genética y la selección en un acervo génico pequeño, proceso conocido como el efecto fundador.

La tercera modalidad de especiación es la especiación parapátrica. Se parece a la especiación peripátrica en que una pequeña población coloniza un nuevo hábitat, pero se diferencia en que no hay ninguna separación física entre las dos poblaciones. En cambio, la especiación es el resultado de la evolución de mecanismos que reducen el flujo génico entre ambas poblaciones. Generalmente, esto ocurre cuando ha habido un cambio drástico en el medio dentro del hábitat de la especie madre. Un ejemplo es la hierba "Anthoxanthum odoratum", que puede sufrir una especiación parapátrica en respuesta a contaminación metálica localizada proveniente de minas. En este caso, evolucionan plantas con una resistencia a niveles altos de metales en el suelo; el aislamiento reproductivo ocurre porque la selección favorece una época de floración distinta a la de las especie madre para las nuevas plantas, que no pueden perder entonces por hibridación los genes que les otorgan la resistencia. La selección en contra de los híbridos puede reforzarse por la diferenciación de los rasgos que promueven la reproducción entre miembros de la misma especie, así como por el aumento de las diferencias de apariencia en el área geográfica en la que la se solapan.

Finalmente, en la especiación simpátrica, las especies divergen sin que haya aislamiento geográfico o cambios en el hábitat. Esta modalidad es rara, pues incluso una pequeña cantidad de flujo génico puede eliminar las diferencias genéticas entre partes de una población. En general, en los animales, la especiación simpátrica requiere la evolución de diferencias genéticas y un apareamiento no aleatorio, para que se pueda desarrollar un aislamiento reproductivo.

Un tipo de especiación simpátrica es el cruce de dos especies relacionadas para producir una nueva especie híbrida. Esto no es habitual en los animales, porque los cromosomas homólogos de progenitores de especies diferentes no pueden aparearse con éxito durante la meiosis. Es más habitual en las plantas, que duplican a menudo su número de cromosomas para formar poliploides. Esto permite a los cromosomas de cada especie parental formar una pareja complementaria durante la meiosis, ya que los cromosomas de cada padre ya son representados por una pareja. Un ejemplo de este tipo de especiación tuvo lugar cuando las especies vegetales "Arabidopsis thaliana" y "Arabidopsis arenosa" se cruzaron para producir la nueva especie "Arabidopsis suecica". Esto tuvo lugar hace aproximadamente 20 000 años, y el proceso de especiación ha sido reproducido en el laboratorio, lo que ha permitido estudiar los mecanismos genéticos implicados en este proceso. De hecho, la duplicación de cromosomas dentro de una especie puede ser una causa habitual de aislamiento reproductivo, pues la mitad de los cromosomas duplicados quedarán sin pareja cuando se aparean con los de organismos no duplicados.

Los episodios de especiación son importantes en la teoría del equilibrio puntuado, que explica la presencia en el registro fósil de rápidos momentos de especiación intercalados con periodos relativamente largos de estasis, durante los que las especies permanecen prácticamente sin modificar. En esta la teoría, la especiación está relacionada con la evolución rápida, y la selección natural y la deriva genética actúan de forma particularmente intensa sobre los organismos que sufren una especiación en hábitats nuevos o pequeñas poblaciones. Como resultado de ello, los períodos de estasis del registro fósil corresponden a la población madre, y los organismos que sufren especiación y evolución rápida se encuentran en poblaciones pequeñas o hábitats geográficamente restringidos, por lo que raramente quedan preservados en forma de fósiles.

La extinción es la desaparición de una especie entera. No es un acontecimiento inusual, y, de hecho, la práctica totalidad de especies animales y vegetales que han vivido en la Tierra están actualmente extinguidas, por lo que parece que la extinción es el destino final de todas las especies. Las extinciones tienen lugar continuamente durante la historia de la vida, aunque el ritmo de extinción aumenta drásticamente en los ocasionales eventos de extinción. La extinción del Cretácico-Terciario, durante la cual se extinguieron los dinosaurios, es la más conocida, pero la anterior extinción Permo-Triásica fue aún más severa, causando la extinción de casi el 96 % de las especies. La extinción del Holoceno todavía dura y está asociada con la expansión de la humanidad por el globo terrestre en los últimos milenios. El ritmo de extinción actual es de 100 a 1000 veces mayor que el ritmo medio, y hasta un 30 % de las especies pueden desaparecer a mediados del siglo XXI. Las actividades humanas son actualmente la causa principal de la extinción; es posible que el calentamiento global la acelere aún más en el futuro.

Las causas de la extinción determinan su impacto en la evolución. Las mayoría de las extinciones, que tienen lugar continuamente, podrían ser el resultado de la competencia entre especies por recursos limitados (exclusión competitiva). Si la competencia entre especies altera la probabilidad de extinción, se podría considerar la selección de especies como un nivel de la selección natural. Las extinciones masivas intermitentes también son importantes, pero en lugar de actuar como fuerza selectiva, reducen drásticamente la diversidad de manera indiscriminada y promueven explosiones de rápida evolución y especiación en los supervivientes.

Microevolución es un término usado para referirse a cambios en pequeña escala de las frecuencias génicas en una población durante el transcurso de varias generaciones. La adaptación de los insectos al uso de plaguicidas o la variación del color de piel de los humanos son ejemplos de microevolución. Estos cambios pueden deberse a varios procesos: mutación, flujo génico, deriva génica o selección natural. La genética de poblaciones es una rama de la biología evolutiva aplica métodos bioestadísticos al estudio de los procesos de la microevolución,

Los cambios a mayor escala, desde la especiación hasta las grandes transformaciones evolutivas ocurridas en largos períodos, son comúnmente denominados macroevolución. La evolución de los anfibios a partir de un grupo de peces óseos es un ejemplo de macroevolución. Los biólogos no acostumbran hacer una separación absoluta entre macroevolución y microevolución, pues consideran que macroevolución es simplemente microevolución acumulada durante escalas de tiempo grandes. Una minoría de teóricos, sin embargo, considera que los mecanismos de la teoría sintética para la microevolución no bastan para hacer esa extrapolación y que se necesitan otros mecanismos. La teoría de los equilibrios puntuados, propuesta por Gould y Eldredge, intenta explicar ciertas tendencias macroevolutivas que se observan en el registro fósil.

En las últimas décadas se ha hecho evidente que los patrones y los mecanismos evolutivos son mucho más variados que los que fueran postulados por los pioneros de la Biología evolutiva (Darwin, Wallace o Weismann) y los arquitectos de la teoría sintética (Dobzhansky, Mayr y Huxley, entre otros). Los nuevos conceptos e información en la biología molecular del desarrollo, la sistemática, la geología y el registro fósil de todos los grupos de organismos necesitan ser integrados en lo que se ha denominado «síntesis evolutiva ampliada». Los campos de estudio mencionados muestran que los fenómenos evolutivos no pueden ser comprendidos solamente a través de la extrapolación de los procesos observados a nivel de las poblaciones y especies modernas. En las próximas secciones se presentan los aspectos considerados como la ampliación de la síntesis moderna.

En el momento en que Darwin propuso su teoría de evolución, caracterizada por modificaciones pequeñas y sucesivas, el registro fósil disponible era todavía muy fragmentario y no se habían hallado fósiles previos al período Cámbrico. El dilema de Darwin, o sea, la inexistencia aparente de registros fósiles del Precámbrico, fue utilizado como el principal argumento en contra de su propuesta de que todos los organismos de la Tierra provienen de un antepasado común.

Además de la ausencia de fósiles antiguos, a Darwin también le preocupaba la carencia de formas intermedias o enlaces conectores en el registro fósil, lo cual desafiaba su visión gradualística de la especiación y de la evolución. De hecho en tiempos de Darwin, con la excepción de "Archaeopteryx", que muestra una mezcla de características de ave y de reptil, virtualmente no se conocían otros ejemplos de formas intermedias o eslabones perdidos, como se los denominó coloquialmente.

Incluso en 1944, cuando se publicó el libro de Simpson "Tempo and mode in evolution", no se conocían todavía fósiles del Precámbrico y solo se disponía de unos pocos ejemplos de formas intermedias en el registro fósil que enlazaran las formas antiguas con las modernas. Desde entonces los científicos han explorado el período Precámbrico con detalle y se sabe que la vida es mucho más antigua de lo que se creía en los tiempos de Darwin. También se sabe que esas antiguas formas de vida fueron los ancestros de todos los organismos aparecidos posteriormente en el planeta. Asimismo desde finales del siglo se han descubierto, descrito y analizado una gran cantidad de ejemplos representativos de formas fósiles intermedias que enlazan a los principales grupos de vertebrados e, incluso, fósiles de las primeras plantas con flor. Como resultado de estos y otros avances científicos, ha surgido una nueva disciplina de la Paleontología, denominada Paleobiología.

Un ejemplo de forma transicional entre los peces y los anfibios es el género extinto "Panderichthys", que habitó la tierra hace unos 370 millones de años y es el enlace intermedio entre "Eustenopteron" (género de peces de 380 millones de años) y "Acanthostega" (anfibios de hace 363 millones de años). Entre los animales terrestres surgió el género "Pederpes" hace 350 millones de años, que conecta a los principales anfibios acuáticos del Devónico superior con los tetrápodos tempranos. Asimismo, la historia evolutiva de varios grupos de organismos extintos, tales como los dinosaurios, ha sido reconstruida con notable detalle. Un ejemplo de eslabón entre los reptiles y los mamíferos es el "Thrinaxodon", un reptil con características de mamífero que habitó el planeta hace 230 millones de años. El "Microraptor", un dromeosáurido con cuatro alas que podía planear y que vivió hace 126 millones de años, representa un estado intermedio entre los terópodos y las primitivas aves voladoras como "Archaeopteryx". Una forma transicional entre los mamíferos terrestres y la vaca marina es "Pezosiren", un sirénido cuadrúpedo primitivo con adaptaciones terrestres y acuáticas que vivió hace 50 millones de años. Los mamíferos terrestres con pezuñas y las ballenas se hallan conectados a través de los géneros extintos "Ambulocetus" y "Rodhocetus" que habitaron el planeta hace 48 a 47 millones de años. Para finalizar esta enumeración de ejemplos de formas transicionales, el ancestro común de los chimpancés y de los seres humanos es el género "Sahelanthropus", un homínido con aspecto de mono que exhibía un mosaico de caracteres de chimpancé y de hominino y que habitó África hace 7 a 5 millones de años.

En su libro "Variation and evolution in plants" (1950), Stebbins también se lamentaba por la ausencia de un registro fósil que permitiera comprender el origen de las primeras plantas con flores, las angiospermas. De hecho, el propio Darwin caracterizó al origen de las angiospermas como un «abominable misterio». No obstante, este vacío de conocimiento está siendo rápidamente completado con los descubrimientos realizados desde fines del siglo XX y hasta la actualidad. En 1998 se descubrió en China, en los estratos provenientes del Jurásico Superior (de más de 125 millones de años de antigüedad), un fósil de un eje con frutos, que se ha denominado "Archaefructus" Este descubrimiento, que permite establecer la edad de las angiospermas más antiguas, hizo mundialmente famosa a la Formación Yixian, donde fue hallado este fósil. En la misma formación se encontraron el fósil de otra angiosperma, "Sinocarpus", y, en 2007, una flor que presenta la organización típica de las angiospermas, con la presencia de tépalos, estambres y gineceo. Esta especie ha sido bautizada como "Euanthus" (del griego, «flor verdadera») por sus descubridores, y demuestra que en el Cretácico inferior ya existían flores como las de las angiospermas actuales.

Darwin no solo consideró el origen, sino también la disminución y la desaparición de las especies; propuso que la competencia interespecífica por recursos limitados era una causa importante de la extinción de poblaciones y especies: durante el tiempo evolutivo, las especies superiores surgirían para reemplazar a especies menos adaptadas. Esta perspectiva ha cambiado en los últimos años, con una mejor comprensión de las causas de las extinciones masivas, episodios de la historia de la Tierra, donde parecen no cumplirse las «reglas» de la selección natural.
Mayr planteó la nueva perspectiva en su libro "Animal species and evolution", en el que señaló que la extinción debe considerarse como uno de los fenómenos evolutivos más conspicuos. Mayr discutió las causas de los eventos de extinción y especuló que la aparición de nuevas enfermedades, la invasión de un ecosistema por otras especies o los cambios en el ambiente biótico pueden ser los responsables: 
Esta hipótesis, no sustentada por hechos probados cuando fue propuesta, ha adquirido desde entonces un considerable apoyo. El término «extinción masiva», mencionado por Mayr sin una definición asociada, se utiliza cuando una gran cantidad de especies se extinguen en un plazo geológicamente breve; los eventos pueden estar relacionados con una causa única o con una combinación de causas, y las especies extintas son plantas y animales de todo tamaño, tanto marinos como terrestres. Al menos han ocurrido cinco extinciones masivas: la extinción masiva del Cámbrico-Ordovícico, las extinciones masivas del Ordovícico-Silúrico, la extinción masiva del Devónico, la extinción masiva del Pérmico-Triásico y la extinción masiva del Cretácico-Terciario.

La extinción biológica que se produjo en el Pérmico-Triásico hace unos 250 millones de años representa el más grave evento de extinción en los últimos 550 millones de años. Se estima que en este evento se extinguieron alrededor del 70 % de las familias de vertebrados terrestres, muchas gimnospermas leñosas y más del 90% de las especies oceánicas. Se han propuesto varias causas para explicar este evento, como el vulcanismo, el impacto de un asteroide o un cometa, la anoxia oceánica y el cambio ambiental. No obstante, es aparente en la actualidad que las gigantescas erupciones volcánicas, que tuvieron lugar durante un intervalo de tiempo de sólo unos pocos cientos de miles de años, fueron la causa principal de la catástrofe de la biosfera durante el Pérmico tardío. El límite Cretácico-Terciario registra el segundo mayor evento de extinción masivo. Esta catástrofe mundial acabó con el 70 % de todas las especies, entre las cuales los dinosaurios son el ejemplo más popularmente conocido. Los pequeños mamíferos sobrevivieron para heredar los nichos ecológicos vacantes, lo que permitió el ascenso y la radiación adaptativa de los linajes que en última instancia se convertirían en "Homo sapiens" y otras especies actuales. Los paleontólogos han propuesto numerosas hipótesis para explicar este evento; las más aceptadas en la actualidad son las del impacto de un asteroide y la de fenómenos de vulcanismo.

En resumen, la hipótesis de los trastornos ambientales como causas de las extinciones masivas ha sido confirmada, lo cual indica que si bien gran parte de historia de la evolución puede ser gradual, de vez en cuando ciertos acontecimientos catastróficos han marcado su ritmo de fondo. Es evidente que los pocos «afortunados sobrevivientes» determinan los subsecuentes patrones de evolución en la historia de la vida.

Determinadas características en una especie son sexualmente atractivas aunque carezcan de otro significado adaptativo. Por ejemplo, la atracción de las hembras de algunas especies de aves por los machos más capaces de hinchar los cuellos, ha traído como resultado —en el transcurso de las generaciones― la selección de machos que pueden hinchar los cuellos hasta un nivel extraordinario. Darwin concluyó que si bien la selección natural guía el curso de la evolución, la selección sexual influye su curso aunque no parezca existir ninguna razón evidente para ello. Los argumentos de Darwin a favor de la selección sexual aparecen en el capítulo cuarto de "El origen de las especies" y, muy especialmente, en "The Descent of Man, and Selection in Relation to Sex" de 1871. En ambos casos, se esgrime la analogía con el mundo artificial:

En su libro "The Descent of Man" describió numerosos ejemplos, tales como la cola del pavo real y de la melena del león. Darwin argumentó que la competencia entre los machos es el resultado de la selección de los rasgos que incrementan el éxito del apareamiento de los machos competidores, rasgos que podrían, sin embargo, disminuir las posibilidades de supervivencia del individuo. De hecho, los colores brillantes hacen a los animales más visibles a los depredadores, el plumaje largo de los machos de pavo real y de las aves del paraíso, o la enorme cornamenta de los ciervos son cargas incómodas en el mejor de casos. Darwin sabía que no era esperable que la selección natural favoreciera la evolución de tales rasgos claramente desventajosos, y propuso que los mismos surgieron por selección sexual, 

Para Darwin, la selección sexual incluía fundamentalmente dos fenómenos: la preferencia de las hembras por ciertos machos ―"selección intersexual", "femenina", o "epigámica"― y, en las especies polígamas, las batallas de los machos por el harén más grande ―"selección intrasexual"―. En este último caso, el tamaño corporal grande y la musculatura proporcionan ventajas en el combate, mientras que en el primero, son otros rasgos masculinos, como el plumaje colorido y el complejo comportamiento de cortejo los que se seleccionan a favor para aumentar la atención de las hembras. Las ideas de Darwin en este sentido no fueron ampliamente aceptadas y los defensores de la teoría sintética (Dobzhansky, Mayr y Huxley) en gran medida ignoraron el concepto de selección sexual.

El estudio de la selección sexual sólo cobró impulso en la era postsíntesis. Se ha argumentado que, según propuso Wallace, los machos con plumaje brillante demuestran de ese modo su buena salud y su alta calidad como parejas sexuales. De acuerdo con esta hipótesis de la «selección sexual de los buenos genes», la elección de pareja masculina por parte de las hembras ofrece una ventaja evolutiva. Esta perspectiva ha recibido apoyo empírico en las últimas décadas. Por ejemplo, se ha hallado una asociación, aunque pequeña, entre la supervivencia de la descendencia y los caracteres sexuales secundarios masculinos en un gran número de taxones, tales como aves, anfibios, peces e insectos). Además, las investigaciones con mirlos han proporcionado la primera evidencia empírica de que existe una correlación entre un carácter sexual secundario y un rasgo que incrementa la supervivencia ya que los machos con los más brillantes colores presentan un sistema inmune más fuerte. Así, la selección femenina podría promover la salud general de las poblaciones en esta especie. Estos y otros datos son coherentes con el concepto de que la elección de la hembra influye en los rasgos de los machos e, incluso, que puede ser beneficiosa para la especie en formas que no tienen ninguna relación directa con el éxito del apareamiento.

En el mismo contexto y desde la publicación del "Origen de las especies", se ha argumentado que el comportamiento altruista, los actos abnegados realizados en beneficio de los demás, es incompatible con el principio de la selección natural. Sin embargo, el comportamiento altruista, como el cuidado de las crías por los padres y el mutualismo, se ha observado y documentado en todo el reino animal, desde los invertebrados hasta en los mamíferos. Una de las formas más notorias de altruismo se produce en ciertos insectos eusociales, como las hormigas, abejas y avispas, que tienen una clase de trabajadoras estériles. La cuestión general de la evolución del altruismo, de la sociabilidad de ciertos insectos o de la existencia de abejas u hormigas obreras que no dejan descendientes ha sido contestada por la teoría de la aptitud inclusiva, también llamada "teoría de selección familiar". De acuerdo con el principio de Darwin/Wallace la selección natural actúa sobre las diferencias en el éxito reproductivo (ER) de cada individuo, donde ER es el número de descendientes vivos producidos por ese individuo durante toda la vida. Hamilton (1972) amplió esta idea e incluyó los efectos de ER de los familiares del individuo: la aptitud inclusiva es el ER de cada individuo, más el ER de sus familiares, cada uno devaluado por el correspondiente grado de parentesco. Numerosos estudios en una gran variedad de especies animales han demostrado que el altruismo no está en conflicto con la teoría evolutiva. Por esta razón, es necesario realizar una modificación y ampliación de la visión tradicional de que la selección opera sobre un solo organismo aislado en una población: el individuo aislado ya no parece tener una importancia central desde el punto de vista evolutivo, sino como parte de una compleja red familiar.

Cuando se define macroevolución como el proceso responsable del surgimiento de los taxones de rango superior, se está utilizando un lenguaje metafórico. En sentido estricto, solo «surgen» nuevas especies, ya que la especie es el único taxón que posee estatus ontológico. La macroevolución da cuenta de la emergencia de discontinuidades morfológicas importantes entre grupos de especies, razón por la cual se las clasifica como grupos marcadamente diferenciados, es decir, pertenecientes a unidades taxonómicas distintas y de alto rango. Es en los mecanismos que explican el surgimiento de estas discontinuidades que las diferentes concepciones y aproximaciones disciplinarias se contraponen.
El gradualismo es el modelo macroevolucionista ortodoxo. Explica la macroevolución como el producto de un cambio lento, de la acumulación de muchos pequeños cambios en el transcurso del tiempo. Este cambio gradual debería reflejarse en el registro fósil con la aparición de numerosas formas de transición entre los grupos de organismos. Sin embargo, el registro no es abundante en formas intermedias. Los gradualistas atribuyen esta discrepancia entre su modelo y las pruebas halladas a la imperfección del propio registro geológico —según Darwin, el registro geológico es una narración de la que se han perdido algunos volúmenes y muchas páginas—. El modelo del equilibrio puntuado, propuesto en 1972 por N. Eldredge y S. J. Gould, sostiene en cambio que el registro fósil es un fiel reflejo de lo que en realidad ocurrió. Las especies aparecen repentinamente en los estratos geológicos, se las encuentra en ellos por 5 a 10 millones de años sin grandes cambios morfológicos y luego desaparecen abruptamente del registro, sustituidas por otra especie emparentada, pero distinta. Eldredge y Gould utilizan los términos estasis e interrupción, respectivamente, para designar estos períodos. Según su modelo, las interrupciones abruptas en el registro fósil de una especie reflejarían el momento en que esta fue reemplazada por una pequeña población periférica ―en la cual el ritmo de evolución habría sido más rápido― que compitió con la especie originaria y terminó por sustituirla. De acuerdo con este patrón, la selección natural no solo opera dentro de la población, sino también entre especies, y los cambios cualitativamente importantes en los organismos ocurrirían en períodos geológicos relativamente breves separados por largos períodos de equilibrio.

En biología evolutiva, un monstruo prometedor es un organismo con un fenotipo profundamente mutante que tiene el potencial para establecer un nuevo linaje evolutivo. El término se utiliza para describir un evento de especiación saltacional que puede ser el origen de nuevos grupos de organismos. La frase fue acuñada por el genetista alemán Richard Goldschmidt, quien pensaba que los cambios pequeños y graduales que dan lugar a la microevolución no pueden explicar la macroevolución. La relevancia evolutiva de los monstruos prometedores ha sido rechazada o puesta en duda por muchos científicos que defienden la Teoría sintética de evolución biológica.
En su obra "The material basis of evolution" (La base material de la evolución), Goldschmidt escribió que «el cambio desde una especie a otra no es un cambio que no implica más y más cambios atomísticos, sino una modificación completa del patrón principal o del sistema de reacción principal en uno nuevo, el que, más tarde puede nuevamente producir variación intraespecífica por medio de micromutaciones».

La tesis de Goldschmidt fue universalmente rechazada y ampliamente ridiculizada dentro la comunidad científica, que favorecía las explicaciones neodarwinianas de R. A. Fisher, J. B. S. Haldane y Sewall Wright.

No obstante, varias líneas de evidencia sugieren que los monstruos prometedores juegan un papel significativo durante el origen de innovaciones clave y de planes corporales noveles por evolución saltacional, más que por evolución gradual. Stephen Jay Gould expuso en 1977 que los genes o secuencias reguladoras ofrecían cierto apoyo a los postulados de Goldschmidt. De hecho, arguyó que los ejemplos de evolución rápida no minan el darwinismo ―como Goldschmidt suponía―, pero tampoco merecen un descrédito inmediato, como muchos neodarwinistas pensaban. Gould insistió en que la creencia de Charles Darwin en el gradualismo no fue jamás un componente esencial de su teoría de evolución por selección natural. Thomas Henry Huxley también advirtió a Darwin que había sobrecargado su trabajo con una innecesaria dificultad al adoptar sin reservas el principio "Natura non facit saltum". Huxley temía que ese supuesto podría desalentar a aquellos naturalistas que creían que los cataclismos y los grandes saltos evolutivos jugaban un papel significativo en la historia de la vida. En este sentido, Gould escribió:

Aunque Darwin discutió en detalle la biología del desarrollo —antes llamada embriología—, esta rama de la biología no contribuyó a la síntesis evolutiva. Ernst Mayr, en su ensayo "What was the evolutionary synthesis?" («Qué fue la síntesis evolutiva?»), explicó que varios de los embriólogos del período en que surgió la síntesis moderna tenían una postura contraria a la teoría evolutiva: En las dos últimas décadas, sin embargo, la biología del desarrollo y la biología evolutiva se han unido para formar una nueva rama de la investigación biológica llamada Biología evolutiva del desarrollo o, coloquialmente, «Evo-devo», que explora cómo han evolucionado los procesos del desarrollo y cómo ha surgido la organización de las diversas partes del cuerpo de los organismos antiguos y actuales.

El principal descubrimiento responsable de la integración de la biología del desarrollo con la teoría de la evolución fue el de un grupo de genes reguladores, la familia de genes homeóticos (genes HOX). Estos genes codifican proteínas de unión al ADN (factores de transcripción) que influyen profundamente en el desarrollo embrionario. Por ejemplo, la supresión de las extremidades abdominales de los insectos está determinada por los cambios funcionales en una proteína llamada Ultrabithorax, que es codificada por un gen Hox. La familia de genes Hox existe en los artrópodos (insectos, crustáceos, quelicerados, miriápodos), cordados (peces, anfibios, reptiles, aves, mamíferos), y hay análogos entre las especies de plantas y hongos.
Los genes HOX influyen en la morfogénesis de los embriones de los vertebrados al expresarse en diferentes regiones a lo largo del eje anteroposterior del cuerpo. Esta familia de genes es homóloga tanto funcional como estructuralmente al complejo homeótico (HOM-C) de "Drosophila melanogaster". Sobre la base de la comparación de genes de varios taxones, se ha logrado reconstruir la evolución de los grupos de genes HOX en vertebrados. Los 39 genes que comprenden la familia de genes HOX en humanos y ratones, por ejemplo, están organizados en cuatro complejos genómicos localizados en diferentes cromosomas, HOXA en el brazo corto del cromosoma 7, HOXB en el 17, HOXC en el 12 y HOXD en el 2, y cada uno de ellos comprende de 9 a 11 genes acomodados en una secuencia homóloga a la del genoma de "D. melanogaster".
Aunque el ancestro común del ratón y del humano vivió hace alrededor de 75 millones de años, la distribución y arquitectura de sus genes HOX son idénticas. Por lo tanto, la familia de genes HOX es muy antigua y aparentemente muy conservada, lo que tiene profundas implicaciones para la evolución de los patrones y procesos de desarrollo.

Las primeras teorías evolucionistas prácticamente ignoraron la microbiología, debido a la escasez de rasgos morfológicos y la falta de un concepto de especie particularmente entre los procariotas. Los recientes avances en el estudio de la genómica microbiana han contribuido a comprender mejor la fisiología y ecología de estos organismos y a facilitar la investigación de su la taxonomía y evolución.
Estos estudios han revelado niveles de diversidad totalmente inesperados entre los microbios.

Particularmente importante fue el descubrimiento en 1959 en Japón de la transferencia horizontal de genes. El intercambio de material genético entre diferentes especies de bacterias ha desempeñado un papel señalado en la propagación de la resistencia a los antibióticos, y, al albor de un mejor conocimiento de los genomas, ha surgido la teoría de que la importancia de la transferencia horizontal de material genético en la evolución no se limita a los microorganismos, sino que alcanza a todos los seres vivos. Los altos niveles de transferencia horizontal de genes han llevado a cuestionar el árbol genealógico de los organismos: en efecto, como parte de la teoría endosimbiótica del origen de los orgánulos, la transferencia horizontal de genes fue un paso crítico en la evolución de eucariotas como los hongos, las plantas y los animales.

La evolución de los primeros eucariontes desde los procariotas ha recibido una considerable atención por parte de los científicos. Este acontecimiento clave en la historia de la vida se produjo entre hace 2300 y 1800 millones de años, durante el Paleoproterozoico. Existen dos hipótesis no excluyentes mutuamente para explicar el origen de los eucariotas: la endosimbiosis y la autogénesis. La hipótesis o teoría de la endosimbiosis postula que la evolución de las primeras células eucariotas es el resultado de la incorporación permanente en una célula hospedadora arqueana de lo que fueron células procariotas fisiológicamente diferentes y autónomas. De acuerdo con este concepto, las mitocondrias han evolucionado a partir de una antigua alfaproteobacteria aerobia, mientras que los cloroplastos provienen de células procariotas del tipo de las cianobacterias. Estos procesos simbiogenéticos se habrían iniciado con el contacto de la célula huésped con una bacteria, en una relación que podría ser al principio parasitaria para devenir mutualista con el paso del tiempo al obtener el hospedador ventajas provenientes de características y especialidades del hospedado. De no llegar a este punto, la selección natural penalizaría esta relación, y el número de estos individuos disminuirían en el conjunto de la población; por el contrario, una relación fructífera se vería favorecida por la selección natural y los individuos simbióticos proliferarían, portando parte o el conjunto de los dos genomas originales. Esta hipótesis es la que ha contado con mayor aceptación y ha sido respaldada por los análisis de filogenética molecular, en el que se sugiere que los eucariotas forman un subgrupo dentro de las arqueas.
En contraste, la hipótesis autogénica sostiene que las mitocondrias y los cloroplastos ―así como otros orgánulos eucariotas tales como el retículo endoplasmático― se desarrollaron como consecuencia de las presiones de selección para la especialización fisiológica dentro de una antigua célula procariota. Según esta hipótesis, la membrana de la célula hospedadora se habría invaginado para encapsular porciones fisiológicamente diferentes de la célula ancestral. Con el transcurso del tiempo, estas regiones unidas a la membrana se convirtieron en estructuras cada vez más especializadas hasta conformar los diferentes orgánulos que actualmente definen la célula eucariota. No obstante, varias observaciones sobre la estructura de la membrana, el tipo de reproducción, la secuencia de ADN y la susceptibilidad a los antibióticos de los cloroplastos y de las mitocondrias tienden a sustentar la hipótesis simbiogenética.

Existen formas de variación hereditaria que no están basadas en cambios de la información genética, sino en su decodificación. Los procesos que producen estas variaciones, conocidos conjuntamente como herencia epigenética, dejan intacta la información genética y son con frecuencia reversible. La trasmisión de información no basada en el ADN se produce a través de la meiosis o mitosis y puede incluir fenómenos como la metilación del ADN o la herencia estructural. Se sigue investigando si estos mecanismos permiten la producción de variaciones específicas beneficiosas en respuesta a señales ambientales. De ser éste el caso, algunas instancias de la evolución podrían ocurrir fuera del marco darwiniano, dentro del cual no hay conexión entre el ambiente y la producción de variaciones hereditarias, aunque los genes involucrados en los mecanismos epigenéticos, como por ejemplo los genes de la enzima ADN-metiltransferasa, histonas, etc, estarían sujetos a los procesos evolutivos clásicos.

Richard Lenski, profesor de la Universidad Estatal de Míchigan, inició en 1989 un experimento para estudiar la evolución de las bacterias, propiciado por la rápida reproducción de estos microorganismos. Lenski estableció varios subcultivos a partir de una cepa de "Escherichia coli", con el objetivo de observar si se producía alguna diferencia entre las bacterias originales y sus descendientes. Los diferentes cultivos se mantenían en condiciones estables y cada setenta y cinco días —aproximadamente, unas quinientas generaciones— los investigadores extraían una muestra de cada uno de ellos la congelaban, procediendo del mismo modo para los subcultivos de estos subcultivos. Hacia 2010, el experimento abarcaba unas cincuenta mil generaciones de bacterias.

Tras diez mil generaciones, las bacterias ya mostraban bastantes diferencias con la cepa ancestral. Las nuevas bacterias eran más grandes y se dividían mucho más rápidamente en el medio de cultivo DM, utilizado para el experimento. El cambio más llamativo consistió en que en uno de los subcultivos en la generación 31 500, las bacterias comenzaron a consumir el citrato presente en el medio DM y que "E. coli" no es normalmente capaz de metabolizar. Por lo tanto, las bacterias en ese subcultivo evolucionaron para adaptarse y crecer mejor en las condiciones de su ambiente.Otro cambio evolutivo importante se produjo pasadas veinte mil generaciones: las bacterias de un segundo subcultivo experimentaron un cambio en su tasa de mutación, provocando una acumulación de mutaciones en su genoma (fenotipo hipermutable). Al tratarse de un cultivo en medio ambiente constante, la mayor parte de las nuevas mutaciones fueron neutrales, pero también se observó un incremento de las mutaciones beneficiosas en los descendientes de este subcultivo.

Los resultados de Lenski propiciaron que se establecieran otros experimentos parecidos, pero con diferentes condiciones de temperatura y fuentes de alimentación, con presencia de antibióticos. También se investigaron diferentes microorganismos: "Pseudomonas fluorescens", "Myxococcus xanthus", e incluso levaduras. En todos ellos se encontraron resultados parecidos: los microorganismos cambiaban, evolucionaban y se adaptaban a las condiciones del cultivo.

Salvo excepciones, la evolución biológica es un proceso demasiado lento para ser observado directamente. Por ello se recurre a disciplinas como la Paleontología, Biología evolutiva o Filogenia, entre otras áreas, para lo observación y el estudio indirecto de la evolución. Desde la última década del siglo XX, el desarrollo de la bioinformática ha propiciado el uso de herramientas informáticas para el estudio de diversos aspectos del proceso evolutivo.

Una de las aplicaciones de las herramientas informáticas al estudio de la evolución, consiste en la simulación "in silico" del proceso evolutivo, usando organismos digitales, una serie de programas que utilizan los recursos disponibles en el procesador para sobrevivir y reproducirse. Un ejemplo de esta aplicación es el programa Tierra, desarrollado en 1990 por el ecólogo Thomas Ray para el estudio de la evolución y la ecología. Herramientas semejantes se utilizan para investigar la base evolutiva de conductas como el altruísmo, la tasa de mutaciones, y la genética de poblaciones.

En el siglo XIX, especialmente tras la publicación de "El origen de las especies", la idea de que la vida había evolucionado fue un tema de intenso debate académico centrado en las implicaciones filosóficas, sociales y religiosas de la evolución. Hoy en día, el hecho de que los organismos evolucionan es indiscutible en la literatura científica, y la síntesis evolutiva moderna tiene una amplia aceptación entre los científicos. Sin embargo, la evolución sigue siendo un concepto controvertido entre los grupos religiosos.

El progresivo aumento en el conocimiento de los fenómenos evolutivos ha tenido como resultado la revisión, rechazo o, por lo menos, el cuestionamiento de las explicaciones tradicionales creacionistas y fijistas de algunas posturas religiosas y místicas y, de hecho, algunos conceptos, como el de la descendencia de un ancestro común, aún suscitan rechazo en algunas personas, que creen que la evolución contradice con el mito de creación en su religión. Como fuera reconocido por el propio Darwin, las repercusiones más controvertidas de la biología evolutiva conciernen los orígenes del hombre. En algunos países ―notoriamente en los Estados Unidos― esta tensión entre la ciencia y la religión ha alimentado la controversia creación–evolución, un conflicto religioso que aún dura, centrado en la política y la educación pública. Mientras que otros campos de la ciencia, como la cosmología, y las ciencias de la Tierra también se contradicen con interpretaciones literales de muchos textos religiosos, la biología evolutiva encuentra una oposición significativamente mayor por parte de muchos creyentes religiosos.

El impacto más importante de la teoría evolutiva se da a nivel de la historia del pensamiento moderno y la relación de este con la sociedad, debido a la naturaleza no teleológica de los mecanismos evolutivos: la evolución no sigue un fin u objetivo. Las estructuras y especies no «aparecen» por necesidad ni por designio divino sino que a partir de la variedad de formas existentes solo las más adaptadas se conservan en el tiempo. Este mecanismo «ciego», independiente de un plan, de una voluntad divina o de una fuerza sobrenatural ha sido en consecuencia explorado en otras ramas del saber. La adopción de la perspectiva evolutiva para abordar problemas en otros campos se ha mostrado enriquecedora y muy vigente; sin embargo en el proceso también se han dado abusos ―atribuir un valor biológico a las diferencias culturales y cognitivas― o deformaciones de la misma ―como justificativo de posturas eugenéticas― las cuales han sido usadas como «"Argumentum ad consequentiam"» a través de la historia de las objeciones a la teoría de la evolución.

Por ejemplo, Francis Galton utilizó argumentos eugenésicos para promover políticas de mejoramiento del patrimonio génico humano, como incentivos para la reproducción de aquellos con «buenos» genes, y la esterilización forzosa, pruebas prenatales, contracepción e, incluso, la eliminación de las personas con «malos» genes. Otro ejemplo de este uso de la teoría de la evolución es el darwinismo social, concebida por Herbert Spencer, que popularizó el término de «supervivencia del más apto» y que se utilizó para justificar la desigualdad social, el racismo y el imperialismo como consecuencias inevitables de las leyes naturales. Sin embargo, los científicos y filósofos contemporáneos consideran que estas ideas no se hallan implícitas en la teoría evolutiva ni están respaldadas por la información disponible.

Los filósofos Karl Marx y Friedrich Engels vieron en la nueva comprensión biológica de la evolución por selección natural de Darwin como esencial para la nueva interpretación del socialismo científico, ya que según Marx, proporciona una "base en las ciencias naturales para la lucha de clases en la historia". El mismo Marx se consideraba como un admirador de Darwin y lo citó varias veces a en sus obras. En "El Capital", el cual le regalo una copia, concluyó que “se debe escribir una historia de la tecnología como la que Darwin ha escrito en el mundo natural sobre la formación de los órganos animales y vegetales”. 

Por otro lado, Engels sostuvo que la aplicación de los implementos de trabajo desempeñó un papel decisivo en la diferenciación del hombre respecto del mundo animal. Engels recurrió a Lewis H. Morgan y su teoría de la evolución social en su obra "El origen de la familia, la propiedad privada y el estado". Según Alexander Vucinich: "Engels dio crédito a Marx por extender la teoría de Darwin al estudio de la dinámica interna y el cambio en la sociedad humana". En 1902, el naturalista y anarquista ruso Kropotkin publicó su libro "El apoyo mutuo", que ofrecía una visión alternativa de la supervivencia humana y animal. Contrario al darwinismo social, en él declaró que "fue un énfasis evolutivo en la cooperación en lugar de la competencia en el sentido darwiniano lo que contribuyó al éxito de las especies, incluido el humano". Kropotkin acuñó el término "evolución progresiva" para describir cómo la ayuda mutua se convirtió en la "condición sine qua non" de toda la vida social, animal y humana.

Antes de que la geología se convirtiera en una ciencia, a principios del siglo XIX, tanto las religiones occidentales como los científicos descontaban o condenaban de manera dogmática y casi unánime cualquier propuesta que implicara que la vida fuera el resultado de un proceso evolutivo. Sin embargo, cuando la evidencia geológica empezó a acumularse en todo el mundo, un grupo de científicos comenzó a cuestionar si estos nuevos descubrimientos podían reconciliarse con una interpretación literal de la creación relatada en la Biblia judeocristiana. Algunos geólogos religiosos, como Dean William Auckland en Inglaterra, Edward Hitchcock en Estados Unidos y Hugh Miller en Escocia siguieron explicando la evidencia geológica y fósil solo en términos de un Diluvio universal; pero una vez que Charles Darwin publicara su "Origen de las especies" en 1859, la opinión científica comenzó a alejarse rápidamente del literalismo bíblico. Este debate temprano acerca de la validez literal de la Biblia se llevó a cabo públicamente, y desestabilizó la opinión educativa en Europa y América, hasta instigar una «contrarreforma» en la forma de un renacimiento religioso en ambos continentes entre 1857 y 1860.

En los países o regiones en las que la mayoría de la población mantiene fuertes creencias religiosas, el creacionismo posee un atractivo mucho mayor que en los países donde la mayoría de la gente posee creencias seculares. Desde los años 1920 hasta el presente en los Estados Unidos, han ocurrido varios ataques religiosos a la enseñanza de la teoría evolutiva, particularmente por parte de cristianos fundamentalistas evangélicos o pentecostales. A pesar de la abrumadora evidencia de la teoría de la evolución, algunos grupos consideran verdadera la descripción bíblica de la creación de los seres humanos y de cada una de las otras especies, como especies separadas y acabadas, por un ser divino. Este punto de vista es comúnmente llamado creacionismo, y sigue siendo defendido por algunos grupos integristas religiosos, particularmente los protestantes estadounidenses; principalmente a través de una forma de creacionismo llamada diseño inteligente. Los «lobbies» religiosos-creacionistas desean excluir la enseñanza de la evolución biológica de la educación pública de ese país; aunque actualmente es un fenómeno más bien local, ya que la enseñanza de base en ciencias es obligatoria dentro de los currículos, las encuestas revelan una gran sensibilidad del público estadounidense a este mensaje, lo que no tiene equivalente en ninguna otra parte del mundo. Uno de los episodios más conocidos de este enfrentamiento se produjo a finales de 2005 en Kansas, donde el Consejo de Educación del Estado de Kansas (en inglés: Kansas State Board of Education) decidió permitir que se enseñaran las doctrinas creacionistas como una alternativa de la teoría científica de la evolución. Tras esta decisión se produjo una fuerte respuesta ciudadana, que tuvo una de sus consecuencias más conocidas en la creación de una parodia de religión, el pastafarismo, una invención de Bobby Henderson, licenciado en Física de la Universidad Estatal de Oregón, para demostrar irónicamente que no corresponde y es equivocado enseñar el diseño inteligente como teoría científica. Posteriormente, el Consejo de Educación del Estado de Kansas revocó su decisión en agosto de 2006. Este conflicto educativo también ha afectado a otros países; por ejemplo, en el año 2005 en Italia hubo un intento de suspensión de la enseñanza de la teoría de la evolución.

En respuesta a las pruebas científicas de la teoría de la evolución, muchos religiosos y filósofos han tratado de unificar los puntos de vista científico y religioso, ya sea de manera formal o informal; a través de un «creacionismo proevolución». Así por ejemplo algunos religiosos han adoptado un enfoque creacionista desde la evolución teísta o el creacionismo evolutivo, y defienden que Dios provee una chispa divina que inicia el proceso de la evolución, y (o) donde Dios creó el curso de la evolución.

A partir de 1950 la Iglesia católica tomó una posición neutral con respecto a la evolución con la encíclica "Humani generis" del papa Pío XII. En ella se distingue entre el alma, tal como fue creada por Dios, y el cuerpo físico, cuyo desarrollo puede ser objeto de un estudio empírico:

Por otro lado, la encíclica no respalda ni rechaza la creencia general en la evolución debido a que se consideró que la evidencia en aquel momento no era convincente. Permite, sin embargo, la posibilidad de aceptarla en el futuro:

En 1996, Juan Pablo II afirmó que «la teoría de la evolución es más que una hipótesis» y recordó que «El Magisterio de la Iglesia está interesado directamente en la cuestión de la evolución, porque influye en la concepción del hombre». El papa Benedicto XVI ha afirmado que «existen muchas pruebas científicas en favor de la evolución, que se presenta como una realidad que debemos ver y que enriquece nuestro conocimiento de la vida y del ser como tal. Pero la doctrina de la evolución no responde a todos los interrogantes y sobre todo no responde al gran interrogante filosófico: ¿de dónde viene todo esto y cómo todo toma un camino que desemboca finalmente en el hombre?».

La reacción musulmana a la teoría de la evolución fue sumamente variada, desde aquellos que creían en una interpretación literal de la creación según el Corán, hasta la de muchos musulmanes educados que suscribieron una versión de evolución teísta o guiada, en la que el Corán reforzaba más que contradecía a la ciencia. Esta última reacción se vio favorecida debido a que Al-Jahiz, un erudito musulmán del siglo IX, había propuesto un concepto similar al de la selección natural.
Sin embargo, la aceptación de la evolución sigue siendo baja en el mundo musulmán ya que figuras prominentes rechazan la teoría evolutiva como una negación de Dios y como poco fiable para explicar el origen de los seres humanos. Otras objeciones de los eruditos y escritores musulmanes reflejan en gran medida las presentadas en el mundo occidental.

Independientemente de su aceptación por las principales jerarquías religiosas, las mismas objeciones iniciales a la teoría de Darwin siguen siendo utilizadas en contra de la teoría evolutiva actual. Las ideas de que las especies cambian con el tiempo a través de procesos naturales y que distintas especies comparten sus ancestros parece contradecir el relato del Génesis de la Creación. Los creyentes en la infalibilidad bíblica atacaron al darwinismo como una herejía. La teología natural del siglo XIX se caracterizó por la analogía del relojero de William Paley, un argumento de diseño todavía utilizado por el movimiento creacionista. Cuando la teoría de Darwin se publicó, las ideas de la evolución teísta se presentaron de modo de indicar que la evolución es una causa secundaria abierta a la investigación científica, al tiempo que mantenían la creencia en Dios como causa primera, con un rol no especificado en la orientación de la evolución y en la creación de los seres humanos.

Richard Dawkins, en su obra "El gen egoísta" de 1976, hizo la siguiente afirmación:

Aunque la evolución biológica es un hecho aceptado desde el siglo XVIII, su explicación científica ha suscitado muchos debates. La teoría denominada síntesis evolutiva moderna (o simplemente teoría sintética), es el modelo actualmente aceptado por la comunidad científica para describir los fenómenos evolutivos; y aunque no existe hoy una sólida teoría alternativa desarrollada, científicos como Motō Kimura o Niles Eldredge y Stephen Jay Gould han reclamado la necesidad de realizar una reforma, ampliación o sustitución de la teoría sintética, con nuevos modelos capaces de integrar por ejemplo la biología del desarrollo o incorporar dentro de la teoría actual una serie de descubrimientos biológicos cuyo papel evolutivo se está debatiendo; tales como ciertos mecanismos hereditarios epigenéticos, la transferencia horizontal de genes, o propuestas como la existencia de múltiples niveles jerárquicos de selección o la plausibilidad de los fenómenos de asimilación genómica para explicar procesos macroevolutivos.

Los aspectos más criticados y debatidos de la teoría de la síntesis evolutiva moderna son:
el gradualismo, que ha obtenido como respuesta el modelo del equilibrio puntuado de Eldredge y Gould;
la preponderancia de la selección natural frente a los procesos puramente estocásticos;
la falta de una explicación satisfactoria al comportamiento altruista y
el reduccionismo geneticista que contradiría las características holísticas y las propiedades emergentes inherentes a cualquier sistema biológico complejo.
A pesar de lo indicado, sin embargo, hay que considerar que el actual consenso científico es que la teoría misma (en sus fundamentos) no ha sido rebatida en el campo de la biología evolutiva, siendo solo perfeccionada; y por ello se la sigue considerando como la «piedra angular de la biología moderna».

Entre otras hipótesis minoritarias, destaca la de la bióloga estadounidense Lynn Margulis, quién consideró que, del mismo modo que las células eucariotas surgieron a través de la interacción simbiogenética de varias células procariotas, muchas otras características de los organismos y el fenómeno de especiación eran la consecuencia de interacciones simbiogenéticas similares. En su obra "Captando Genomas. Una teoría sobre el origen de las especies" Margulis argumentó que la simbiogénesis es la fuerza principal en la evolución. Según su teoría, la adquisición y acumulación de mutaciones al azar no son suficientes para explicar cómo se producen variaciones hereditarias, y postuló que las organelas, los organismos y las especies surgieron como el resultado de la simbiogénesis. Mientras que la síntesis evolutiva moderna hace hincapié en la competencia como la principal fuerza detrás de la evolución, Margulis propuso que la cooperación es motor del cambio evolutivo. Argumenta que las bacterias, junto con otros microorganismos, ayudaron a crear las condiciones que se requieren para la vida en la Tierra, tales como una concentración alta de oxígeno. Margulis sostuvo que estos microorganismos son la principal razón por la que las condiciones actuales se mantienen. Afirmó, asimismo, que las bacterias son capaces de intercambiar genes con mayor rapidez y facilidad que los eucariotas, y debido a esto son más versátiles y las artífices de la complejidad de los seres vivos.

Igualmente, Máximo Sandín rechazó vehementemente cualquiera de las versiones del darwinismo presentes en la actual teoría y propuso una hipótesis alternativa para explicar la evolución. En primer lugar, justiprecia la obra de Lamarck, y sugiere que las hipótesis o predicciones, conocidas como Lamarckismo, realizadas por este biólogo se ven corroboradas por los hechos. Por ejemplo, Sandín formuló su hipótesis a partir de la observación de que las circunstancias ambientales pueden condicionar, no sólo la expresión de la información genética (fenómenos epigenéticos, control del splicing alternativo, estrés genómico…), sino la dinámica del proceso de desarrollo embrionario, y postuló que el cimiento fundamental de los ecosistemas es el equilibrio y no la competencia.
Conforme a sus ideas, se puede apreciar la tendencia de las formas orgánicas a una mayor complejidad, como consecuencia de unas leyes que gobiernan la variabilidad de los organismos, y que están, de alguna manera, inscritas en los organismos. Habida cuenta de que el 98,5 % del genoma humano, por ejemplo, está compuesto por secuencias repetidas con función reguladora así como una notable cantidad de virus endógenos, Sandín concluye que esa conformación del genoma no puede ser el resultado del azar y de la selección naturaly que se produce en cambio por la presión del medio ambiente, lo que provoca que ciertos virus se inserten en el genoma o determinadas secuencias génicas se modifiquen y, como consecuencia, se generen organismos completamente nuevos, con sustanciales diferencias non respecto a sus predecesores. Según esta teoría, que rechaza la tesis del «ADN egoísta» de Dawkins, el mecanismo fundamental del cambio evolutivo es solo la capacidad de integración de los virus en genomas ya existentes mediante la transferencia horizontal de sus genes. Además, Sandín opina que el medio ambiente, y no las mutaciones aleatorias, es la causa de que determinados grupos de seres vivos asuman muevas características, no de forma gradual, sino en episodios específicos y sin fases intermedias. Según el filósofo Maurício Abdalla, la hipótesis sostenida por Sandín está fundamentada por una gran cantidad de datos científicos y abre una nueva área de investigación en el campo de la biología.






</doc>
