<doc id="16796" url="https://es.wikipedia.org/wiki?curid=16796" title="Economía de San Marino">
Economía de San Marino

San Marino no tiene moneda propia. Se usaba la lira de San Marino convertible a la lira italiana antes de que desapareciera. Desde el 1 de enero de 2002 se emplea el Euro.

PIB (Producto Interior Bruto)

860 millones de dólares EE. UU. (Convertidos según Paridad del poder de compra). Año 2000

935,8 millones de € (Tipo conversión: 1 dólar = 1,0882 € al 30/01/01)
PIB "per capita"

32.000 dólares EE. UU. (Convertidos según Paridad del poder de compra). Año 2000

34.822,4 € (Tipo conversión: 1 dólar = 1,0882 € al 30/01/01)
Distribución del PIB por sectores

Sector Primario: 0,1% del PIB.

Sector Secundario: 46,5% del PIB.

Sector Terciario: 53,4% del PIB.

Crecimiento PIB estimado

4,3% (2007)
Tasa de inflación

-3.5% (estimaciones 2008)

El comercio de San Marino, está basado principalmente en el área turística, y de deporte, millones de personas fanáticas del fútbol asisten a ver jugar a la selección de mayor corazón de todo el planeta tierra. Con más de 15 millones de personas, repartidas en todo el año, San Marino recibe más de 2000 MM de dólares que son invertidos en el área social y económica como parte de la política social-demócrata. 

Importaciones

3744 Billones € (2007)
Exportaciones

4628 Billones € (2007)
Saldo (Exportaciones-Importaciones)

+884 Billones € (2007)

Población ocupada

18.500 aprox. (2000)
Población ocupada por sectores

Servicios: 62,2%

Industria: 37,7%

Agricultura: 0,1% 

(Estimaciones 2008) 
Tasa de paro

3,1% (2008)

Población bajo el umbral de la pobreza

Ingresos: 690,6 Millones € (2006)

Gastos: 652,9 Millones € (2006)

Superávit: 37,7 Millones €.



</doc>
<doc id="16798" url="https://es.wikipedia.org/wiki?curid=16798" title="Sacarosa">
Sacarosa

La sacarosa, sucrosa, azúcar común o azúcar de mesa es un disacárido formado por glucosa y fructosa.

Su nombre químico es alfa-D-Glucopiranosil - (1→2) - beta-D-Fructofuranósido, y su fórmula es CHO.

Es un disacárido que no tiene poder reductor sobre el reactivo de Fehling y el reactivo de Tollens.

El cristal de sacarosa es transparente, el color blanco es causado por la múltiple difracción de la luz en un grupo de cristales.

El azúcar de mesa es el edulcorante más utilizado para endulzar los alimentos y suele ser sacarosa. En la naturaleza se encuentra en un 20 % del peso en la caña de azúcar y en un 15 % del peso de la remolacha azucarera, de las que se obtiene el azúcar de mesa. La miel también es un fluido que contiene gran cantidad de sacarosa parcialmente hidrolizada.

La sacarosa, azúcar de mesa o azúcar de caña, es un disacárido de glucosa y fructosa. Se sintetiza en plantas, pero no en animales superiores. No contiene ningún átomo de carbono anomérico libre,
puesto que los carbonos anoméricos de sus dos unidades monosacáridos constituyentes se hallan unidos entre sí, covalentemente mediante un enlace O-glucosídico.
Por esta razón, la sacarosa no es un azúcar reductor y tampoco posee un extremo reductor.

Su nombre abreviado puede escribirse como Glc(a -1à 2)Fru o como Fru(b 2à 1)Glc. La sacarosa es un producto intermedio principal de la fotosíntesis, en variados vegetales constituye la forma principal de transporte de azúcar desde las hojas a otras partes de la planta. En las semillas germinadas de plantas, las grasas y proteínas almacenadas se convierten en sacarosa para su transporte a partir de la planta en desarrollo.

Una curiosidad de la sacarosa es que es triboluminiscente, lo cual significa que produce luz mediante una acción mecánica.
Posee un poder rotatorio de +66.

El enlace que une los dos monosacáridos es de tipo O-glucosídico. Además, dicho enlace es dicarbonílico ya que son los dos carbonos reductores de ambos monosacáridos los que forman el enlace alfa(1-2) de alfa-D-glucosa y beta-D-fructosa.

La enzima encargada de hidrolizar este enlace es la sacarasa, también conocida como invertasa, ya que la sacarosa hidrolizada es llamada también "azúcar invertido".

La sacarosa tiene como función principal en el organismo humano ayudar en la generación de energía y transporte de carbohidratos.

La sacarosa se usa en los alimentos por su poder endulzante. Su valor calórico se encuentra incluso por debajo de la regla "4 kilocalorías/gramo" de los hidratos de carbono en general; siendo en el caso de la sacarosa 1,619 kJ o 387 Kcal / 100 gramos. Al llegar al estómago sufre una hidrólisis ácida y una parte se desdobla en sus componentes glucosa y fructosa. El resto de sacarosa pasa al intestino delgado, donde la enzima sacarasa la convierte en glucosa y fructosa.

Existen muchas controversias sobre el daño que ocasiona el consumo de sacarosa, y varias teorías al respecto. El mayor debate está centrado en la producción de caries, diabetes tipo 2 (en ningún caso la diabetes tipo 1 tiene que ver con el consumo de azúcar), obesidad, arteriosclerosis, y otras patologías.

Sin embargo, se han destacado sus propiedades específicas como nutriente para el organismo humano: se digiere con facilidad y no genera productos tóxicos durante su metabolismo.

Se discute el índice glicémico que puede contener, pero en general se asume que es muy elevado, debido a que posterior a su consumo incrementa de forma importante la cifra de glicemia en sangre, desencadenando una alta secreción de Insulina, que con el tiempo puede ser nociva para la salud.
Por su sabor agradable el ser humano tiende a un consumo exagerado, lo que raramente se da en la naturaleza. Sin embargo, en la sociedad industrializada, su disponibilidad es alta y su precio bajo, por lo que se sobrepasa con facilidad los límites razonables de su consumo. Debido a ello, la sacarosa es limitada en la dieta por razones de salud, ya que un consumo descontroladamente alto produce una carga glucémica elevada.

En los humanos y otros mamíferos, la sacarosa se desdobla en sus dos azúcares monosacáridos constitutivos, glucosa y fructosa, por la acción de las enzimas sacarasa o la isomaltasa (glucosidasas), las cuales están ubicadas en la membrana celular de los microvilli del duodeno. Como resultado, las moléculas de glucosa y fructosa son absorbidas hacia el torrente sanguíneo.

El consumo de sacarosa en grandes cantidades está relacionado con enfermedades, como la caries dental, debido a que las bacterias de la boca convierten los azúcares en ácidos que atacan el esmalte dental.

La sacarosa, como carbohidrato puro, contiene 3.94 kilocalorías por gramo, o 17 kilojulios por gramo. Cuando se consumen grandes cantidades de alimentos con sacarosa, nutrientes benéficos pueden desplazarse de la dieta, lo cual contribuye a problemas de salud. Se ha sugerido que la sacarosa contenida en las bebidas (como las gaseosas) está relacionada con la obesidad y podría estarlo en la resistencia a la insulina.

La sacarosa puede contribuir a desarrollar el síndrome metabólico. En un experimento con ratas que fueron alimentadas con una dieta en la que un tercio de su alimento era sacarosa mostraron primero elevados niveles de triglicéridos, lo que generó grasa visceral, seguido de resistencia a la insulina. Otro estudio en ratas encontró que una dieta rica en sacarosa desarrolló hipertrigliceridemia, hiperglucemia y resistencia a la insulina.

Cuando la sacarosa se calienta, funde pasando al estado líquido. Debido a su bajo punto de fusión, este proceso ocurre de forma muy rápida, y se adhiere al recipiente que lo contiene con facilidad.

Como se mencionó, su consumo excesivo puede causar obesidad, diabetes tipo 2, caries, o incluso la caída de los dientes. Hay personas que sufren intolerancia a la sacarosa, debido a la falta de la enzima sacarasa, y que no pueden tomar sacarosa, ya que les provoca problemas intestinales.

La sacarosa es el edulcorante más utilizado en el mundo industrializado, aunque ha sido en parte reemplazada en la preparación industrial de alimentos por otros endulzantes tales como jarabes de glucosa, o por combinaciones de ingredientes funcionales y endulzantes de alta intensidad.

Generalmente se extrae de la caña de azúcar, de la remolacha o del maíz y entonces es purificada y cristalizada. Otras fuentes comerciales (menores) son el sorgo dulce y el jarabe de arce.

La extensa utilización de la sacarosa se debe a su poder endulzante y sus propiedades funcionales como consistencia. Por tal motivo es importante para la estructura de algunos alimentos incluyendo panecillos y galletas, nieve y sorbetes, además es auxiliar en la conservación de alimentos, siendo un aditivo comúnmente utilizado en la preparación de la denominada comida basura.



</doc>
<doc id="16799" url="https://es.wikipedia.org/wiki?curid=16799" title="Sun SPARC">
Sun SPARC

SPARC (del inglés "Scalable Processor ARChitecture") es una arquitectura RISC big-endian. Es decir, una arquitectura con un conjunto de instrucciones reducidas.

Fue originalmente diseñada por Sun Microsystems en 1985, se basa en los diseños RISC I y II de la Universidad de California en Berkeley que fueron definidos entre los años 1980 y 1982.

La empresa Sun Microsystems diseñó esta arquitectura y la licenció a otros fabricantes como Texas Instruments, Cypress Semiconductor, Fujitsu, LSI Logic entre otros.

SPARC es la primera arquitectura RISC abierta y como tal, las especificaciones de diseño están publicadas, así otros fabricantes de microprocesadores pueden desarrollar su propio diseño. 

Una de las ideas innovadoras de esta arquitectura es la ventana de registros que permite hacer fácilmente compiladores de alto rendimiento y una significativa reducción de memoria en las instrucciones load/store en relación con otras arquitecturas RISC. Las ventajas se aprecian sobre todo en programas grandes.

La CPU SPARC está compuesta de una unidad de enteros (IU), que procesa la ejecución básica y una unidad de coma flotante (FPU) que ejecuta las operaciones y cálculos de números reales. La IU y la FPU pueden o no estar integradas en el mismo chip. 

Aunque no es una parte formal de la arquitectura, las computadoras basadas en sistemas SPARC de Sun Microsystems tienen una unidad de manejo de memoria (MMU) y un gran caché de direcciones virtuales (para instrucciones y datos) que están dispuestos periféricamente sobre un bus de datos y direcciones de 32 bits.


La arquitectura SPARC tiene cerca de 50 instrucciones enteras, unas pocas más que el anterior diseño RISC, pero menos de la mitad del número de instrucciones enteras del 6800 de Motorola.

Las instrucciones de SPARC se pueden clasificar en cinco categorías:






Un rasgo único caracteriza al diseño SPARC, es la ventana con solape de registros. El procesador posee mucho más que 32 registros enteros, pero presenta a cada instante 32. Una analogía puede ser creada comparando la ventana de registros con una rueda rotativa. Alguna parte de la rueda siempre está en contacto con el suelo; así al girarla tomamos diferentes porciones de la rueda (el efecto es similar para el overlap de la ventana de registros). El resultado de un registro se cambia a operando para la próxima operación, obviando la necesidad de una instrucción Load y Store extra.

Se acordó para la especificación de la arquitectura, poder tener 32 registros "visibles" divididos en grupos de 8.


Los registros globales son "vistos" por todas las ventanas, los locales son solo accesibles por la ventana actual y los registros de salida se solapan con los registros de entrada de la ventana siguiente (los registros de salida para una ventana deben ponerse como registros de entrada para la próxima, y deben estar en el mismo registro).

El puntero de ventana mantiene la pista de cual ventana es la actualmente activa. Existen instrucciones para "abrir" y "cerrar" ventanas, por ejemplo para una instrucción "call", la ventana de registros gira en sentido anti horario; para el retorno desde una instrucción "call", esta gira en sentido horario.

Una interrupción utiliza una ventana fresca, es decir, abre una ventana nueva. La cantidad de ventanas es un parámetro de la implementación, generalmente 7 u 8.

La alternativa más elaborada para circundar lentamente la ventana de registros es colocar los registros durante el tiempo de compilación. Para lenguajes como C, Pascal, etc., esta estrategia es difícil y consume mucho tiempo. Por lo tanto, el compilador es crucial para mejorar la productividad del programa.

"Recientes investigaciones sugieren que la ventana de registros, encontradas en los sistemas SPARC pero no en otras máquinas RISC comerciales, están en condiciones de proveer excelente rendimiento para lenguajes de desarrollo como Lisp y Smalltalk." (R. Blau, P.Foley, etc. 1984).

El diseño SPARC soporta un set total de traps o interrupciones. Son manejados por una tabla que soporta 128 interrupciones de hardware y 128 traps de software. Sin embargo las instrucciones de coma flotante pueden ejecutarse concurrentemente con la instrucciones de enteros, los traps de coma flotante deben ser exactos porque la FPU provee (desde la tabla) las direcciones de las instrucciones que fracasan.

Algunas instrucciones SPARC son privilegiadas y pueden ser ejecutadas únicamente mientras el procesador esta en modo supervisor. Estas instrucciones ejecutadas en modo protegido aseguran que los programas de usuario no sean accidentalmente alterados por el estado de la máquina con respecto a sus periféricos y viceversa. El diseño SPARC también proporciona protección de memoria, que es esencial para las operaciones multitarea. 

El SPARC tiene muchas similitudes con el diseño de Berkeley, el RISC II. Semejante al RISC II, él usa una ventana de registros para reducir el número de instrucciones Load y Store.





Utilizado por Sun Microsystems, Cray Research, Fujitsu / ICL y otros.

Esta tabla contiene las especificaciones de ciertos procesadores SPARC: frecuencia (megahertz), versión de la arquitectura, año de lanzamiento, número de hilos (hilos por núcleo multiplicado por el número de núcleos), proceso de fabricación (nanómetros), número de transistores (millones), tamaño de la matriz (mm), número de pines de entrada/salida, energía disipada (watts), voltaje y tamaños de las cachés de datos, instrucciones, L2 y L3 (kibibytes).
Notas:


</doc>
<doc id="16801" url="https://es.wikipedia.org/wiki?curid=16801" title="Arquitectura de Rusia">
Arquitectura de Rusia

La arquitectura de Rusia se refiere a la arquitectura practicada en el territorio actual de la Federación de Rusia y en los del histórico Imperio ruso. También se usa para referirse a los edificios erigidos bajo la influencia rusa o por los arquitectos rusos en ciertas épocas en otras partes del mundo, particularmente en el territorio de la Unión Soviética, en las antiguas repúblicas socialistas soviéticas.

La arquitectura rusa sigue una tradición cuyas raíces se encuentran en la arquitectura de madera rusa temprana (incluyendo varios elementos vernáculos) y en la arquitectura de la Rus de Kiev con sus centros en Veliki Nóvgorod y en Kiev. La arquitectura rusa también fue influenciada por el Imperio bizantino. Si bien la arquitectura, y la cultura, rusa se inspiraron en muchos casos en la arquitectura bizantina, esa influencia no impidió que gran parte se desarrollase de forma independiente, con características nacionales y locales. Después de la caída de Kiev, la historia de la arquitectura rusa continuó en el principado de Vladímir-Súzdal (1157-1363), en la república de Nóvgorod (1136-1478) y en los sucesivos estados del zarato de Rusia. Las grandes iglesias de la Rus de Kiev, construidas después de la adopción del cristianismo en 988, fueron los primeros ejemplos de arquitectura monumental en la región eslava oriental. Las primeras iglesias ortodoxas orientales se construyeron principalmente en madera, con su forma más simple conocida como iglesia celular. Las catedrales a menudo presentaban muchas cúpulas pequeñas, lo que ha llevado a algunos historiadores del arte a inferir cómo pueden haber aparecido los templos eslavos paganos.
En su segunda edad de oro el arte bizantino se había extendido a Armenia, y en Kiev se construyó la iglesia de Santa Sofía en el año 1017, siguiendo fielmente los influjos de la arquitectura de Constantinopla: se estructuró en forma basilical de cinco naves terminadas en ábsides.

La catedral de Santa Sofía de Nóvgorod (1044-1052), por otro lado, expresó un nuevo estilo que ejerció una fuerte influencia en la arquitectura de las iglesias rusas. Sus austeros y gruesos muros, sus ventanas pequeñas y estrechas tienen mucho en común con la arquitectura románica de Europa occidental. La forma de las cúpulas acebolladas serán una característica distintiva de la arquitectura rusa. Otras desviaciones del modelo bizantino son evidentes en las siguientes catedrales de Nóvgorod: San Nicolás (1113), San Antonio (1117-1119) y San Jorge (1119). La arquitectura secular de la Rus de Kiev apenas ha sobrevivido. Hasta el siglo XX, solo la Puerta Dorada de Vladímir, a pesar de las muchas restauraciones del siglo XVIII, podían considerarse un auténtico monumento del período premongol. Durante la década de 1940, el arqueólogo Nikolái Voronin descubrió los restos bien conservados del palacio de Andréi Bogoliubski en Bogoliúbovo (que data de 1158 a 1165).

La ciudad de Nóvgorod conservó su arquitectura durante la invasión mongola. Las primeras iglesias fueron encargadas por los príncipes, pero después del siglo XIII, comerciantes, gremios y comunidades comenzaron a encargar catedrales. Los ciudadanos del Nóvgorod del siglo XIII destacaron por su astucia, diligencia y prosperidad, expandiéndose desde el Báltico hasta el mar Blanco. La arquitectura en Nóvgorod no comenzó a florecer hasta principios del siglo XII. La catedral de Santa Sofía se inspiró en la iglesia original de Santa Sofía de Kiev; es similar en apariencia pero más pequeña, más estrecha y (en un desarrollo de la arquitectura del norte de Rusia) las cúpulas en forma de cebolla reemplazan a las cúpulas. La construcción fue supervisada por trabajadores de Kiev, que también importaron ladrillos. Los materiales de construcción principales fueron piedra de campo y sillares de mampostería de piedra caliza desnudos. Se piensa que los interiores estuvieron pintados con frescos, que ahora han desaparecido. Las puertas estaban hechas de bronce.

El katholikon del monasterio de Yúriev fue encargado en 1119 por el príncipe Vsévolod de Pskov. El arquitecto era conocido como el maestro Pedro, uno de los pocos arquitectos del que hay constancia registral en ese momento en Rusia. El exterior se caracteriza por las ventanas estrechas y los nichos de doble empotramiento, que avanzan rítmicamente en la fachada; los muros interiores alcanzan una altura de 20 metros. Sus pilares están muy espaciados, enfatizando la altura de los techos abovedados. El interior estaba cubierto de frescos de los talleres del príncipe, incluidas algunas de las pinturas rusas más raras de la época.

La Iglesia de la Transfiguración del Salvador fue un monumento a Ilyá Múromets. Durante la invasión mongola, Ilyá tenía fama de haber salvado la ciudad; la iglesia fue construida en su honor en la calle Elijah en 1374. En esa época, la ciudad-estado de Nóvgorod estableció un distrito separado para los príncipes, subdividiendo la ciudad en una serie de calles donde todavía se encuentra la iglesia. Las ventanas de la iglesia son más detalladas, los nichos más profundos y la cúpula (vista en catedrales más grandes) se ve aumentada por una cubierta inclinada.

Otra iglesia que se parece mucho a la iglesia de la Transfiguración es la Iglesia de los Santos Pedro y Pablo en Kozhévniki. Fue construida en 1406, y la diferencia principal está en el material de construcción. El detalle se centra en las fachadas oeste y sur. Aparecen en ese momento nuevos motivos ornamentales en el ladrillo. El ladrillo también se usó para las pilastras que delinean la fachada. Originalmente estaba enlucida, pero se restauró después de que fuese dañada durante la Segunda Guerra Mundial. Su ábside apunta hacia el río, que ofrece una bienvenida agradable a los barcos que se acercan desde el Báltico. La cubierta de tejas se asemeja a los techos de bochka (techo de barril) populares en ese momento. Los muros fueron construidos con piedra de canteras locales, que contrastaba con los ladrillos rojos. La planta de la iglesia es casi cuadrada con cuatro pilares, un ábside y una cúpula.

Posteriormente, destaca la catedral de San Basilio, en la Plaza Roja de Moscú, realizada en tiempos de Iván el Terrible (1555-1560), cuyas cinco cúpulas, la más alta y esbelta en el crucero y otras cuatro situadas en los ángulos que forman los brazos de la cruz, resaltan por su coloración, por los elevados tambores y por su característicos perfiles bulbosos.

Aparte de la arquitectura religiosa, también destacan construcciones civiles tales como las fortalezas.

Rusia tiene numerosos edificios y monumentos protegidos y muchos de ellos integran la Lista del Patrimonio Mundial de la UNESCO, tanto a nivel individual —iglesias de Kizhi Pogost (1990), Kremlin y Plaza Roja de Moscú (1990), Laura de la Trinidad y San Sergio (1993), Iglesia de la Ascensión en Kolómenskoye (1994), monasterio de Ferapóntov (2000), Kremlin de Kazán (2000), monasterio Novodévichi (2004), monasterio de Sviajsk (2017)— como parte de conjuntos y cascos históricos —San Petersburgo (1990), Nóvgorod y sus alrededores (1992), islas Solovetsky (1992), Vladímir y Súzdal (1992), Derbent (2003), Yaroslavl (2005) y Bólgar (2014)— (ver ).

El estado feudal de la Rus de Kiev fue, al menos culturalmente y, en este caso, arquitectónico, el antepasado de Rusia, Bielorrusia y Ucrania. Las grandes iglesias de Kiev, construidas después de la adopción del cristianismo en 988 en la región, son los ejemplos más antiguos de arquitectura monumental en las regiones eslavas orientales. El estilo arquitectónico del reino de Kiev, que se estableció rápidamente, estuvo muy influenciado por la arquitectura bizantina. Las primeras iglesias ortodoxas orientales, con su característica geometría de «iglesia de células», fueron en su mayoría construidas con carpinterías de madera, con su forma más simple conocida como iglesia celular. Las principales catedrales a menudo tenían varias cúpulas pequeñas, lo que ha llevado a algunos historiadores del arte a inferir cómo pudieron haber aparecido a partir de los templos paganos eslavos.
La catedral de Santa Sofía de Nóvgorod (1044-1052), por otro lado, expresó un nuevo estilo que ejercerá una gran influencia en la arquitectura religiosa rusa. Sus austeras murallas gruesas, sus huecos pequeños y estrechos y sus cúpulas con cascos tienen mucho en común con la arquitectura románica de Europa occidental. Otras desviaciones del modelo bizantino son evidentes en las sucesivas catedrales de Nóvgorod: en San Nicolás (1113), en San Antonio (1117-1119) y en San Jorge (1119). La arquitectura secular de la Rus de Kiev apenas ha sobrevivido. Hasta el siglo XX, solo la Puerta Dorada de la ciudad de Vladímir, a pesar de que fue en gran parte restaurada en el siglo XVIII, podría considerarse como un auténtico monumento de la época premongola. Después, durante la década de 1940, el arqueólogo Nikolái Voronin descubrió los restos bien conservados del palacio de Andréi Bogoliubski en Bogoliúbovo (que datan de 1158 a 1165)

La ciudad de Nóvgorod logró preservar su arquitectura durante la invasión mongola. La construcción de las primeras iglesias habían sido encargos de los príncipes; sin embargo, después del siglo XIII, los comerciantes, sus gremios y las ciudades comenzaron también a financiar las catedrales. Los ciudadanos del Nóvgorod del siglo XIII, que destacaban por su astucia, diligencia y prosperidad, lograron expandirse desde el mar Báltico hasta el mar Blanco. La arquitectura en Nóvgorod no comenzó a florecer hasta el cambio de siglo XII. La catedral de Santa Sofía de Nóvgorod fue modelada según la catedral original de Santa Sofía en Kiev; es similar en apariencia pero las cúpulas en forma de cebolla son más pequeñas, más angostas y (en un desarrollo de la arquitectura del norte de Rusia) reemplazan a las cúpulas. La construcción fue supervisada por trabajadores llegados de Kiev, que también importaron la manpostería de ladrillo. Los materiales de construcción eran piedra y guijarros del lugar y sillería de piedra caliza ensamblados en mampostería seca, sin revestir. Se piensa que los interiores ya estuvieron pintados con frescos, que ahora han desaparecido. Las puertas fueron hechas en bronce. 

La construcción del katholikon —la iglesia principal de un monasterio o diócesis en la Iglesia ortodoxa— del monasterio de Yúriev fue encargada en 1119 por el príncipe Vsévolod Mstislávich. El arquitecto, uno de los raros artistas citados en los documentos de ese momento en Rusia, es mencionado como «maestro Pedro». El exterior de este edificio se caracteriza por los huecos angostas y los nichos de doble hueco algo retirados, cuya alternancia ritma la fachada; los muros interiores alcanzan una altura de . Sus pilares están muy espaciados, destacando la altura de los techos abovedados. El interior estaba cubierto de frescos realizados por los artesanos del príncipe, que se encuentran entre los raros especímenes de la pintura medieval auténticamente rusa.

La iglesia de la Transfiguración del Salvador de Nóvgorod fue un monumento al Nuestra Señora del Signo que durante la invasión mongola, habría defendido y salvado milagrosamente la ciudad; la iglesia fue construida en su honor en la calle Elías en 1374. Durante esa época, la ciudad-estado de Nóvgorod estableció un distrito reservado para la aristocracia, subdividiendo la ciudad en una serie de calles donde la iglesia aún se mantiene. En comparación con las iglesias anteriores, sus huecos son más altos, los nichos están más hundidos y la cúpula (hasta entonces reservada para las catedrales) está cernida por cubiertas a dos aguas.

Otra iglesia que se parece mucho a la Iglesia de la Transfiguración es la iglesia de los Santos Pedro y Pablo en Kozhévniki. Fue construida en 1406, y la principal diferencia está en la elección del material de construcción: en ese momento, el ladrillo comienza a adornarse con motivos decorativos; las ornamentación se centra en las fachadas oeste y sur. El ladrillo también se utilizó para las pilastras que puntúan la fachada. Originalmente estaba cubierta con yeso, pero con las depredaciones de la Segunda Guerra Mundial, fue profundamente restaurada. Su ábside apunta hacia el río, dibujando una vista agradable a los barcos que se aproximasen desde el Báltico. El techo de tejas se asemeja a los techos de bochka populares en ese momento. Las muros fueron construidos con piedra de cantera local, que contrasta con los ladrillos rojos. La planta de la iglesia es casi cuadrada, con cuatro pilastras, un ábside y una cúpula.

Los mongoles habían asolado el país hasta el punto de hacer, durante al menos medio siglo, que incluso las ciudades más grandes (como Moscú o Tver) fuesen incapaces de financiar la construcción de iglesias de piedra. Sin embargo, las ciudades de Nóvgorod y Pskov habían escapado del yugo de los mongoles y prosperado como repúblicas comerciales boyantes; se han conservado docenas de iglesias medievales en estas ciudades (las más antigua del siglo XII). Las iglesias de Nóvgorod (como la iglesia de la Transfiguración de la calle Ilyiná, construida en 1374), tienen cubiertas inclinadas y las paredes están toscamente talladas; algunas de ellas albergan magníficos frescos medievales. Las pequeñas y pintorescas iglesias de Pskov tienen muchas características innovadoras: arcos conopiales ("kokóshniks"), pórticos de iglesia, galerías exteriores y campanarios. Estas nuevas características fueron importadas en Moscovia por albañiles llegados de Pskov, que construyeron muchos edificios en el siglo XV (incluyendo la iglesia de la Deposición del Kremlin de Moscú (1462) y la iglesia del Santo Espíritu de la Laura de la Trinidad, construida en 1476).

Las iglesias de Moscovia del siglo XIV son poco numerosas, y la fecha de su construcción es muy controvertida. Los monumentos más característicos de este período —que se pueden ver en Nikólskoia (cerca de Ruza, y probablemente de los años 1320) o en Kolomna (posiblemente de la segunda mitad del XIV— son pequeñas iglesias fortificadas de cúpula única, hechas de manpuestos casi en bruto que son capaces de soportar pequeños asientos. La transformación de Moscú en un fuerte centro político condujo a un rápido desarrollo de la arquitectura en la ciudad y en el territorio del principado. Las tradiciones arquitectónicas del principado de Vladímir-Súzdal fueron adoptadas con éxito por los arquitectos de Moscú, y a finales del siglo XVI ya se puede hablar propiamente de una escuela de arquitectura de Moscú.

Con la construcción de la iglesia de la Dormición en Gorodok (ca. 1399/1400), en Zvenígorod, los albañiles moscovitas rivalizaron con el dominio de sus sus predecesores de antes de las invasiones e incluso lograron resolver algunos problemas originales que habían sido irresolubles para sus predecesores. Es una de las iglesias de Moscú de piedra blanca completamente conservadas de ese período. Es un pequeño templo de planta en cruz inscrita, con cuatro pilastras y coronado con una única cúpula. En el lado este, el templo tiene tres ábsides de altar y las fachadas occidental, sur y norte tienen una división tradicional en tres arcadas verticales, rematadas por zakomaras. La gracia de las proporciones y la belleza de la rica decoración ornamental distinguen al templo de entre otras catedrales de la misma época. Sobreviven más ejemplos de la arquitectura antigua moscovita en la catedral de la Laura de la Trinidad (1423), en el monasterio Savvin de Zvenígorod (quizás de 1405) y en el monasterio Andrónikov de Moscú (1427).

El ascenso de la arquitectura de Moscú estuvo ligado a los éxitos políticos y económicos del principado a finales del siglo XV, durante el reinado de Iván III (r. 1462-1505). Moscovia se había vuelto tan pujante como estado que su prestigio requería edificios magníficos de varias cúpulas que fueran capaces de rivalizar con las catedrales premongolas de Nóvgorod y Vladímir. Pero como los maestros de obra rusos eran incapaces de concebir obras de ese género, Iván III hizo llamar a arquitectos italianos. Se ha especulado sobre el porqué recurrió a los italianos (Friázines) en vez de a los alemanes, vecinos más próximos y con los que mantenía relaciones desde la época de la Hansa y ya establecidos desde hacía tiempo en Nóvgorod y Pskov. Esa elección se explica por varias razones: las relaciones comerciales de Moscú con las colonias genovesas y venecianas del mar Negro por ruta del Don; la penetración del Renacimiento italiano en los reinos vecinos de Hungría y Polonia; y por último, pero no menos, el matrimonio italiano de Iván III en 1472 con una princesa bizantina criada en Roma, Sofía Paleóloga. En 1475, llegó el florentino Aristóteles Fioravanti que construyó en el Kremlin de Moscú la catedral de la Dormición (1475-1479) (también conocida como de la Asunción). Templo de seis pilastras, cinco cúpulas —símbolos de Jesucristo y los Cuatro Evangelistas— y cinco ábsides, fue construida en piedra blanca en combinación con ladrillo. El diseño de la nueva iglesia, inspirado en las catedrales medievales de Vladímir con ornamentación a la italiana, se mostró inmensamente popular, y fue tomado como modelo para muchas otras iglesias en toda Rusia. En 1547 tuvo lugar en esa catedral la coronación del primer zar ruso, Iván el Terrible y será en adelante el lugar de coronación de los monarcas rusos, incluso cuando la capital fue trasladada a San Petersburgo. La instalación ritual de metropolitanos y patriarcas de la Iglesia Ortodoxa Rusa también tuvo lugar en esta catedral, y sus tumbas se encuentran en ella.

En 1484-1490, arquitectos de Pskov con albañiles locales construyeron una nueva catedral en el kremlin, algo más pequeña, la catedral de la Anunciación. Originalmente, tenía tres cúpulas (dos más fueron añadidas alrededor de 1572). Estaba rodeada por una explanada en tres de sus lados. En 1562-1564, se le agregaron cuatro capillas laterales. Las entradas norte y oeste fueron decoradas con portales de piedra caliza en el siglo XVI. Este edificio ya estuvo muy influenciado por la arquitectura renacentista italiana, que pudieron aprender de la experiencia de Fioravanti. Las puertas de bronce de las portadas norte y oeste están doradas con oro fino. El piso del edificio fue cubierto con jaspe llegado de la catedral de Rostov.

En 1505-1509, Iván hizo llamar a un veneciano, Aleviz Novy () o Aleviz Friazin, para construir una nueva catedral cerca de la Asunción, la catedral del Arcángel Miguel. El aspecto del Renacimiento veneciano marca el exterior de la catedral. Las piedras de los muros eran blancas, pero fueron pintadas de rojo, según un documento antiguo. Se pueden admirar las pilastras con capiteles, un frontón circular sobre el portal central y una cornisa horizontal que da la ilusión de que el edificio tiene dos plantas. Los bulbos dorados del siglo XVIII decoran la catedral para reemplazar a una única cúpula central original, dañada.

Estas ambiciosas catedrales del Kremlin sirvieron de modelo en toda Rusia a lo largo del siglo XVI, siendo cada nuevo edificio diseñado para superar a sus predecesores en tamaño y ornamentación (por ejemplo, la catedral Odighitria del monasterio Novodévichi, construida en la década de 1520).

Al mismo tiempo, se estaba desarrollando la construcción civil, y se erigen varios edificios en el Kremlin de Moscú, el más famoso de los cuales es palacio de las Facetas (1487-1492). En 1485 comenzó la construcción de nuevas murallas y torres del Kremlin, que se terminó bajo el reinado de Basilio III de Moscú en 1516. A esta era también corresponden la construcción activa de otras fortificaciones: monasterios fortificados, fortalezas, kremlins. Se construyeron los kremlin de Tula (1514), Kolomna (1525), Zaraisk (1531), Mozhaisk (1541), Sérpujov (1556), y así sucesivamente.

Aparte de las iglesias, muchas otras construcciones datan del reinado de Iván III: entre ellas, fortificaciones en el Kitái-górod, el Kremlin de Moscú (cuyas torres visibles hoy son, sin embargo posteriores), Ivangorod), campanarios (campanario de Iván el Grande) y palacios (palacio de las Facetas y palacio Úglich). La cantidad y variedad de los edificios se explica por el abandono, a instancias de los maestros italianos, de la piedra caliza, piedra prestigiosa, costosa y poco manejable en favor del ladrillo, económico y más fácil de usar.

En el siglo XVI, la introducción de la cubierta en pabellón y la arquitectura del ladrillo marcaron un punto de inflexión en la tradición eclesiástica. Los techos inclinados parecen haberse asentado primero en el norte de Rusia, donde evitaban la acumulación excesiva de nieve y la sobrecarga de las carpinterías durante los largos inviernos. En las iglesias de carpintería (incluso recientes), este tipo de techo se convirtió en algo común. La primera iglesia de ladrillo con techo inclinado fue la iglesia de la Ascensión en Kolómenskoye (1532), erigida para el jubileo de Iván el Terrible. El origen de su inusual diseño es muy controvertido; es probable que la singularidad de ese edificio (cuyo estilo es desconocido en las otras naciones ortodoxas) refleje las ambiciones del naciente Estado ruso y la emancipación del arte del país de los cánones bizantinos después de la caída de Constantinopla en 1453. De hecho, la Iglesia rusa había cambiado mucho desde el final de la Edad Media y su peso se había vuelto preponderante en el mundo ortodoxo después de esa caída. Moscú había tenido su propio metropolitano desde 1448 y los rusos desconfiaban cada vez más de la Iglesia griega: ¿no habría castigado Dios a los griegos por sus vicios al dejar que los turcos tomaran Constantinopla? Moscú, patriarcado libre, se mantuvo en la tradición ortodoxa y tuvo vocación de tomar el control de la capital bizantina. Así fue como la ciudad se impuso gradualmente en mentalidades como la «Tercera Roma» y que los grandes príncipes de Moscovia tomaran el título de «zar», variante eslava de "César". La idea de una "Tercera Roma", sin embargo, nunca fue formalmente aceptada por la Iglesia Ortodoxa, y los patriarcas rusos siempre afirmaron para ellos la primacía de Constantinopla. Pero en la conciencia popular, como en el bajo clero, esta idea alimentó violentas controversias sobre el futuro religioso de Rusia. Por un lado, algunos abogaban por una estrecha alianza entre la Iglesia y la monarquía, y una preservación de los grandes dominios del clero que se utilizan para practicar la caridad y la enseñanza. Creían que el reino moscovita debía esforzarse por realizar el reino de Dios en la tierra y que una monarquía muy vinculada a la Iglesia sería beneficiosa. Su líder era el abad Iósif de Vólok (o Volokolamsk, 1440-1515). Frente a ellos, liderados por un asceta, el starets Nil de Sora (1433-1508), otros temían que una estrecha alianza entre la Iglesia y el zar conduciría, más tarde, a una alienación del clero al poder político, y predicaban la pobreza monástica y la fidelidad a Constantinopla.

Las iglesias con techos inclinados se multiplicaron bajo el reinado de Iván el Terrible (1547-1584). Dos ejemplos primitivos que datan de su reinado presentan aguadas multicolores y de geometría exótica, combinadas de una manera compleja: así la iglesia de San Juan Bautista en Kolómenskoye (1529-1547) y la catedral de San Basilio en la Plaza Roja (1555-1561). Esta última iglesia combina no menos de nueve secciones de inclinación variable en una curiosa composición circular.

San Basilio fue construida entre 1555 y 1561 por orden de Iván el Terrible y conmemora la captura de Kazán (1552) y de Astracán, que supuso la derrota final final de los tártaros del kanato de Kazán. Fue el edificio más alto de la ciudad hasta la finalización del campanario de Iván el Grande en 1600. El edificio original, conocido como la iglesia de la Trinidad y más tarde como catedral de la Trinidad, tenía ocho iglesias dispuestas alrededor de la novena iglesia central de la Intercesión; la décima iglesia fue erigida en 1588 sobre la tumba del venerado santo local Vasili (Basilio). En los siglos XVI y XVII, la iglesia, percibida como el símbolo terrenal de la ciudad celestial, como sucede con todas las iglesias en la Cristiandad bizantina, se la conocía popularmente como la "Jerusalén" y servía como alegoría del Templo de Jerusalén en la procesión anual del Domingo de Ramos a la que asistían el Patriarca de Moscú y el zar.

La época de las revueltas (1598-1613) había dejado al Estado y a la Iglesia arruinados, ambos incapaces de financiar ninguna construcción; fueron los ricos mercaderes de Yaroslavl, en el Volga, quienes tomaron el testigo de las grandes construcciones. A lo largo del siglo XVII, construyeron grandes iglesias con cinco torres de bulbos, circunscritas por campanarios y alas radiantes. Al principio, la composición de estas iglesias estaba fundamentalmente desequilibrada, y las diferentes partes se equilibraban entre sí a la manera de los brazos de una balanza (por ejemplo la iglesia del profeta Elías (1647-1650); pero poco a poco evolucionaron en Yaroslavl hacia una simetría perfecta, sobresaliendo las cúpulas del resto del edificio con cubiertas de tejas polícromas (por ejemplo. en la iglesia de San Juan Crisóstomo en Koróvniki en Yaroslavl, 1649-1654). La arquitectura de la cuenca del Volga culminó con la iglesia de San Juan Bautista (construida entre 1671 y 1687), la más grande de Yaroslavl, con sus 15 cúpulas y más de 500 frescos. El revestimiento exterior de ladrillo, desde las cúpulas hasta los porches elevados, ha sido trabajado minuciosamente y decorado con azulejos policromos.

Las iglesias moscovitas del siglo XVII también ofrecen una ornamentación exuberante, pero son mucho más modestas en tamaño. A principios de ese siglo, los moscovitas aún preferían los techos radiantes, que recuerdan la forma de una tienda de campaña. La obra maestra de este tipo fue la iglesia de la Asunción Milagrosa de Uglich (1627): presentaba una sucesión de tres pabellones cónicos, evocando tres velas. Esta composición fue retomada con exuberancia en la iglesia Odighitria de Vyazma (1638) y en la iglesia de la Natividad de Putinki, Moscú (1649-1652). Teniendo en cuenta que tales construcciones iban en contra de la tradición bizantina, el patriarca Nikon las denunció como heterodoxas. Él era favorable a los edificios suntuosos, como el kremlin de Rostov (1670-1683) a orillas del lago Nero, con sus cinco iglesias esbeltas, su profusión de torres y sus muchos palacios. Nikon eligió para su propia residencia el monasterio de la Nueva Jerusalén (1656-1685), dominado por una catedral en rotonda, la primera de este tipo en Rusia.

Los tejados en pabellón fueron prohibidos entonces y los arquitectos moscovitas tuvieron que reemplazarlos con sucesiones de arcos conopiales ("kokóshniki"), y este esquema se convirtió en el símbolo de la arquitectura moscovita del siglo XVII. Uno de los primeros ejemplo de este estilo fue la catedral de Nuestra Señora de Kazán en Moscú (1632-1636), en Moscú. A finales de siglo, más de 100 iglesias de este estilo audaz habían surgido en Moscú, y tal vez al menos otras tantas en la región. Entre los mejores ejemplos a citar, la Iglesia de San Nicolás, en Kolomna (1716-1719), la iglesia de la Santa Trinidad de Nikítniki (Moscú, 1628—1651), la iglesia de San Nicolás de Jamóvniki (1679-1682), y la iglesia de la Santa Trinidad de Ostánkino (Moscú, 1677-1692 ). El edificio más representativo debe haber sido la iglesia de San Nicolás (la «Gran Cruz») del distrito de Kitái-górod, arrasada por orden de Stalin.

Cuando la arquitectura rusa se hundió en el "decorum", comenzó a inspirarse en el barroco polaco y en el barroco ucraniano. Las primeras iglesias barrocas fueron simplemente capillas construidas en las tierras de la familia Naryshkin cerca de Moscú, de ahí la designación barroco Naryshkin para este estilo. Algunas de estas iglesias son torres de pisos superpuestos cúbicos y octogonales (la iglesia de San Salvador de Oubory, 1697); otras tienen una composición escalonada, terminada por un campanario (Iglesia de la Intercesión en Filí (1693-1696). La ornamentación de los estilos moscovita y barroco a menudo es tan exuberante que parece obra de un joyero más que de un albañil (véase, por ejemplo, la iglesia de la Santísima Trinidad de Lykovo, 1696). El ejemplo más exitoso del barroco Naryshkin es, sin duda, la iglesia de la Asunción de la calle Pokrovka, en Moscú (1696-1699, demolida en 1929), con sus múltiples cúpulas. Su arquitecto también realizó la reconstrucción «en rojo y blanco» de varios monasterios de Moscú, incluidos el monasterio Novodévichi y el monasterio Donskói (reformado con una nueva catedral y murallas, 1684-1711).
El estilo barroco se extendió rápidamente en Rusia y reemplazó a los cánones anteriores en casi todas partes. Una familia de comerciantes de pieles, los Stróganov, financió la construcción de los magníficos edificios barrocos de Nizhni Nóvgorod (la Iglesia de la Natividad, desde 1703) y en los confines de la tundra (la catedral de la Presentación en Solvychegodsk, desde 1693). Algunas construcciones barrocas notables aparecieron en las ciudades más meridionales en las primeras décadas del XVIII, como en Kazán, Solikamsk, Verjoturie, Tobolsk e Irkutsk. Las iglesias tradicionales de carpintería de madera del norte de Rusia también presentan características interesantes. Ensambladas sin clavos ni grapas, tienen estructuras tan singulares como la de la iglesia de la Intercesión, de 24 cúpulas, en Výtegra (1708, completamente incendiada en 1963) o la iglesia de la Transfiguración de 22 cúpulas en la isla de Kijí (1714).

Después de regresar de su Gran Embajada en 1698, Pedro I de Rusia se embarcó en una política de occidentalización y expansión que transformaría el Zarato ruso en el Imperio ruso y en una gran potencia europea. Enfrascado desde 1700 en la Gran Guerra del Norte, Pedro I conquistó el 1 de mayo de 1703, la fortaleza sueca de Nyenschantz y la ciudad de Nyen, en la zona del delta del río Nevá. Fundó allí la ciudad de San Petersburgo, en honor de su patrón San Pedro, el 27 de mayo de 1703 y la construcción fue emprendida bajo condiciones climáticas y geográficas adversas. La alta tasa de mortalidad requirió un suministro constante de trabajadores y Pedro ordenó un reclutamiento anual de 40 000 siervos, un conscripto por cada nueve a dieciséis familias. 

La cultura y el diseño de la nueva ciudad se concibieron como un rechazo consciente de la arquitectura rusa de influencia bizantina tradicional, como el barroco Naryshkin, entonces en boga, en favor de la arquitectura de inspiración clásica que prevalecía en las grandes ciudades de Europa. El zar tenía la intención de que su nueva ciudad fuera diseñada en un estilo renacentista flamenco, más tarde conocido como barroco petrino, y ese fue el estilo que seleccionó para su nuevo palacio en la ciudad. La primera residencia real en el sitio había sido una humilde cabaña de troncos conocida entonces como Dómik Petrá I (Casita de Pedro I), construida en 1704, que daba al río Nevá. En 1711, fue transportado a la Petróvskaya Náberezhnaya (el malecón de Pedro), donde todavía se encuentra. Con el sitio despejado, el zar se embarcó en la construcción de una casa más grande entre 1711 y 1712. Esta casa, hoy conocida como el primer Palacio de Invierno, fue diseñada por Domenico Trezzini.
El siglo XVIII fue un período de gran desarrollo en la arquitectura real europea, ya que la necesidad de tener residencias fortificadas disminuyó gradualmente. Este proceso, que había comenzado a finales del siglo XVI, se aceleró y los grandes palacios clásicos reemplazaron rápidamente a los castillos fortificados en los países europeos más poderosos. Uno de los ejemplos más antiguos y notables fue el Versalles de Luis XIV. Completamente terminado en 1710, Versalles, con su tamaño y esplendor, aumentó la rivalidad entre los soberanos de Europa. Pedro el Grande, interesado en promover todos los conceptos occidentales, deseaba tener un palacio moderno como sus compañeros soberanos. Sin embargo, a diferencia de algunos de sus sucesores, Pedro nunca aspiró a rivalizar con Versalles.

El primer Palacio de Invierno fue un modesto edificio de dos plantas principales bajo un techo de pizarra. Parece que Pedro pronto se cansó del primer palacio, ya que en 1721, la segunda versión del Palacio de Invierno fue construida bajo la dirección del arquitecto Georg Mattarnovy. El palacio de Mattarnovy, aunque todavía era muy modesto en comparación con los palacios reales de otras capitales europeas, tenía dos plantas sobre una planta baja rusticada, con una proyección central bajo de un frontón sostenido por columnas. Fue aquí donde Pedro el Grande murió en 1725.

El Palacio de Invierno no era el único palacio en la ciudad inconcluso, ni siquiera el más espléndido, ya que Pedro había ordenado a sus nobles construir residencias y pasar la mitad del año allí. Esta fue una orden impopular; San Petersburgo estaba entonces sobre un pantano, con poca luz solar, y se decía que allí solo crecerían coles y nabos. Estaba prohibido derribar árboles para obtener combustible, por lo que se permitía el uso de agua caliente solo una vez a la semana. Solo la segunda esposa de Pedro, la emperatriz Catalina, fingió disfrutar la vida en la nueva ciudad.

El primer edificio de la nueva ciudad fue la fortaleza de San Pedro y San Pablo, dispuesta en la isla Záyachi, en la orilla derecha del Nevá, a cinco kilómetros tierra adentro desde el Golfo. La marisma fue drenada y la ciudad se extendió desde la fortaleza bajo la supervisión de ingenieros alemanes y neerlandeses a quienes Pedro había invitado a Rusia. Pedro restringió la construcción de edificios de piedra en toda Rusia fuera de San Petersburgo para que todos los canteros fueran a ayudar a construir la nueva ciudad. Al mismo tiempo, Pedro contrató a un gran número de ingenieros, arquitectos, constructores navales, científicos y hombres de negocios de todos los países de Europa. La inmigración sustancial de profesionales educados finalmente convirtió a San Petersburgo en una ciudad mucho más cosmopolita que Moscú y el resto de Rusia. Los esfuerzos de Pedro por impulsar la modernización en Moscú y el resto de Rusia fueron completamente incomprendidos por la anticuada nobleza rusa y finalmente fracasaron, causándole muchos problemas con la oposición, incluidos varios atentados contra su vida y la traición de su propio hijo.
Pedro trasladó la capital de Moscú a San Petersburgo en 1712, nueve años antes del tratado de Nystad, a una nueva ciudad que quería acondicionar según el modelo neerlandés, en el estilo barroco. Llamada la "ventana a Europa", era un puerto marítimo y también una base para la armada protegida por la fortaleza de Kronstadt. La primera persona que construyó una casa en San Petersburgo fue Cornelis Cruys, comandante de la Flota del Báltico. Inspirado por Venecia y Ámsterdam, Pedro el Grande propuso barcos y coracles como medio de transporte en su ciudad de canales. Inicialmente, solo había 12 puentes permanentes sobre las vías fluviales más pequeñas, mientras que el Bolshaya Nevá era atravesado por embarcaciones en el verano y a pie o en carruajes de caballos durante el invierno. Un puente de pontones sobre el Nevá era construido cada verano.

Pedro había quedado impresionado por Versalles y otros palacios en Europa. Su palacio oficial de una importancia comparable en Peterhof fue el primer palacio suburbano utilizado permanentemente por el zar como residencia oficial principal y lugar de la recepciones oficiales y bailes estatales. El palacio frente al mar, Monplaisir y el Gran Palacio de Peterhof se construyeron entre 1714 y 1725. En 1716, el rey de Prusia presentó un regalo al zar Pedro: la cámara de Ámbar. Aleksandr Danílovich Ménshikov, el mejor amigo de Pedro, fue el primer Gobernador General de la Gobernación de San Petersburgo en 1703-1727 y para él se construyó como residencia oficial el palacio Ménshikov (1710-1711), construido por el italiano Giovanni Maria Fontana y, más tarde, el alemán Gottfried Johann Schädel. En 1724 se estableció en la ciudad la Academia de Ciencias de San Petersburgo. Después de la muerte de Pedro el Grande, Ménshikov fue arrestado y exiliado a Siberia. En 1728, Pedro II de Rusia devolvió la capital a Moscú, pero 4 años más tarde, en 1732, San Petersburgo volvió a ser la capital y permaneció como sede del gobierno durante aproximadamente dos siglos. El logro arquitectónico más importante de su reinado, aparte de los palacios, fue la catedral de San Pedro y San Pablo (San Petersburgo) (1712-1733).
San Petersburgo prosperara bajo el gobierno de dos de las mujeres más poderosas en la historia de Rusia: la emperatriz Isabel y Catalina la Grande, que reinó durante 34 años, de 1762 a 1796. Bajo su mandato, que ejemplificó la de un déspota ilustrado, se construyeron más palacios en San Petersburgo que en cualquier otra capital en el mundo.

 

Ana (r. 1730-1740) no prestó especial atención a los grandes proyectos pero sí Isabel (r. 1741-1762), que era amante de las grandes fiestas y bailes y de la música. La hija de Pedro reinó sin una sola ejecución en 22 años. Redujo los impuestos, redujo el tamaño del gobierno, y fue conocida por sus bailes de mascaradas y festejos, acumulando un vestuario de unos 12000 vestidos, la mayoría de ellos conservados ahora como piezas de arte museístico. Apoyó a la Academia Rusa de Ciencias y financió muchos proyectos de construcción durante su reinado, liderados por el exuberante estilo barroco de Bartolomeo Rastrelli; fueron obras maestras del italiano la catedral de Smolny (1744-1764), el palacio de Catalina (1752-1756) y el palacio de Invierno (1762-1796), que Isabel no pudo ver completado.
El convento Smolny había sido emprendido como hogar para Isabel cuando su padre Pedro la había apartado de la línea sucesoria y consideró que se convirtiese en monja. Ya como emperatriz, el convento fue uno de los muchos edificios religiosos erigidos por su patrocinio, utilizando los fondos de la nación (en lugar de los de la iglesia). Según Robert Nisbet Bain, «Ningún otro soberano ruso ha erigido tantas iglesias». La catedral (1744-1764), el edificio principal del convento, está considerada como una de las obras principales del estilo barroco de Rastrelli. El monumento inicial debía conllevar un campanario que le haría ser el edificio más alto de San Petersburgo. La muerte de Isabel en 1762 no permitió la finalización de la construcción.

En 1733, la emperatriz Ana había encargado a Mijaíl Zemtsov y Andréi Kvásov que ampliaran el antiguo palacio de Catalina, su palacio de verano. La emperatriz Isabel, sin embargo, consideró que la residencia de su madre estaba pasada de moda y que era incómoda, y en mayo del año 1752 pidió a su arquitecto de corte, Rastrelli, que demoliera la antigua edificación y la reemplazara con un edificio mucho más grande en un llamativo estilo rococó. La construcción tardó cuatro años en realizarse y el 30 de julio de 1756, el arquitecto presentó el flamante nuevo palacio de 325 metros de largo a la emperatriz, a sus aturdidos cortesanos y a los estupefactos embajadores extranjeros.

A petición de Isabel, Rastrelli, que trabajaba también en las obras de ampliación del palacio de Invierno (el tercero desde el primero construido por Pedro el Grande), ideó un esquema completamente nuevo en 1753, en una escala colosal. La finalización acelerada del palacio se convirtió en una cuestión de honor para la emperatriz, que consideraba el palacio como un símbolo de prestigio nacional. Los trabajos no se interrumpían en eel invierno y seguían durante todo el año, incluso en los meses más severos del invierno. Ni las privaciones del pueblo ruso y en el ejército causadas por la Guerra de los Siete Años en curso, fueron obstáculos y se asignaron al proyecto , una suma recaudada por un impuesto a las tabernas estatales. Aunque los trabajadores ganaban un salario mensual de solo un rublo, el costo del proyecto excedió el presupuesto, tanto que el trabajo cesó debido a la falta de recursos a pesar del deseo obsesivo de la Emperatriz de completarlo rápidamente. En última instancia, se aumentaron los impuestos sobre la sal y el alcohol para financiar los costos adicionales, aunque el pueblo ruso ya estaba agobiado por los impuestos para pagar la guerra. El costo final fue de . En 1759, poco antes de la muerte de Isabel, un Palacio de Invierno verdaderamente digno de ese nombre ya estaba a punto de completarse.

Otras obras barrocas destacadas del reinado de Isabel fueron el campanario de la Laura de la Trinidad y San Sergio —de los arquitectos Iván Michurin y Dmitri Ujtomski—, que con sus 88 metros, era una de las edificaciones más altas construidas en Rusia hasta esa fecha, y la Puerta Roja (demolida en 1928).

Catalina la Grande, que reinó durante 34 años, de 1762 a 1796, despidió a Rastrelli y prefirió a arquitectos neoclásicos invitados de Escocia y de Italia. Catalina residió en los palacios de Invierno y de Verano y tenía la reputación de ser una mecenas de las artes, la literatura y la educación. Fundó la soberbia colección de arte con la que cuenta hoy el Museo del Hermitage de San Peterburgo, que ahora ocupa el conjunto del Palacio de Invierno. En su afán por reunir un acervo artístico equiparable (o superior) a los de otras cortes europeas, gastó cuantiosas cifras en comprar cientos de pinturas y esculturas, en ocasiones colecciones enteras de nobles y magnates de Francia e Inglaterra como la el barón Pierre Crozat y del político Robert Walpole.

Entre las construcciones más importantes de su reinado se encuentran el palacio Petrovski (1776-1780) en honor a la victoria de la Rusia imperial sobre el Imperio otomano, después de la guerra ruso-turca de 1768-1774, un palacio imperial, construido por Matvéi Kazakov, sirvió de lugar de descanso de Catalina antes de la llegada a Moscú de la carretera de San Petersburgo; la catedral de la Santísima Trinidad del monasterio de Alejandro Nevski (1776-1790) (diseñada por Iván Stárov); el Palacio del Senado (1776-1788), un proyecto neoclásico de Matvéi Kazakov, para albergar el Senado moscovita, la máxima institución legislativa de la Rusia imperial (en el siglo posterior el edificio fue utilizado por la Corte Regional de Moscú); el Teatro del Hermitage (1782-1785), como teatro palladiano siguiendo un proyecto de Giacomo Quarenghi quien se inspiró en el Teatro Olímpico de Vicenza; el palacio de Alejandro (1792-1796), un proyecto neoclásico encargado por Catalina a Giacomo Quarenghi para su nieto favorito y futuro emperador Alejandro I de Rusia con motivo de su matrimonio con la gran duquesa Elizaveta Alekséievna, nacida princesa Luisa María Augusta de Baden; Bajo su mandato, que ejemplificó la de un déspota ilustrado, se construyeron más palacios en San Petersburgo que en cualquier otra capital en el mundo.

Bajo el reinado de Catalina, los moscovitas Vasili Bazhénov y Matvéi Kazakov fueron los inspiradores del renacimiento del estilo gótico ruso.

Pablo I encargara a los arquitectos Vincenzo Brenna y Vasili Bazhénov el Castillo Mijáilovski (1797-1801) para ser su residencia real en el centro histórico de San Petersburgo. 
Alejandro I favoreció el estilo Imperio, que será "de facto" el único estilo en el que expresarse en ese momento, como se puede ver en la catedral de Kazán, el palacio de Almirantazgo de San Petersburgo (1806-1823), el Teatro Bolshói (1821-1825), la catedral de San Isaac (1819-1858) y el arco Triunfal de Narva en San Petersburgo. La influencia del estilo Imperio fue aún más fuerte en Moscú, donde fue necesario reconstruir las miles de casas quemadas durante la invasión napoleónica de Rusia.

Instituto Smolny (1806-1808), de estilo palladiano situado en San Petersburgo y que ha sido testigo de importantes acontecimientos de la historia de Rusia. Su construcción fue encargada a Giacomo Quarenghi por la "Sociedad para la Educación de Nobles Doncellas" y construido para ser la sede del Instituto Smolny para Nobles Doncellas, fundado por Iván Betskói en 1764 y que tomó su nombre del cercano convento Smolny. En 1917, el edificio fue elegido por Vladímir Lenin como cuartel general bolchevique durante la Revolución de Octubre.

En la década de 1830, Nicolás I liberó la expresión arquitectónica, allanando el camino para las primeras manifestaciones del eclecticismo. Konstantín Thon buscó para sus iglesias la inspiración en los edificios ruso-bizantinos (catedral de Cristo Salvador, 1832-1883), y así estableció el tono para la arquitectura cristiana posterior; para las construcciones civiles, era parte de la tradición renacentista, como se ve en el Gran Palacio del Kremlin (1838-1849) y en la Armería del Kremlin (1844-1851). Los reinados posteriores de Alejandro II y de Alejandro III vieron el renacimiento de un estilo ruso-bizantino en la arquitectura religiosa, mientras que la arquitectura civil siguió los pasos de otras naciones europeas, seguidores del eclecticismo (véase arquitectura ecléctica en Rusia); evoca el surgimiento de los sentimientos nacionalistas y la rehabilitación de un folclore nacional, real o fantaseado (por ejemplo, la cabaña (izbá) de Pogodin ("Pogódinskaya izbá") y el Museo Estatal de Historia de Moscú (1875-1881)).

La primera obra modernista en Rusia se considera la dacha del gran duque Borís Vladímirovich Románov, construida por los arquitectos Sherborn y Scott en 1897 en Pushkin. Uno de los monumentos más notables y típicos del "art nouveau" en San Petersburgo es la Casa del Cantante (ahora "Casa de los Libros") en la Perspectiva Nevsky. Por un lado, el edificio no está conectado a la zona circundante, considerado un error urbanístico, por otro lado, es un ejemplo de una distribución exitosa en las difíciles condiciones de la sección atestada (1902-1904, arquitecto Pavel Suzor). Otro ejemplo vivo es la tienda Eliséiev, ubicada cerca (1902-1903, arquitecto G.V. Baranovsky). Los monumentos del "art nouveau" ruso también incluyen el Hotel Astoria en San Petersburgo (F.I. Lidval 1913-1914).

El primer edificio modernista en Moscú fue la mansión de O.A. List diseñada por el arquitecto Lev Kékushev de 1898-1899. Los brillantes ejemplos del estilo modernista de Moscú son los famosos palacios de la ópera de Stepán Ryabushinski y Derozhin de F.O. Shekhtel, la residencia de Iván Mindovsky y la casa del arquitecto de Lev Kékushev, la casa de alquiler de MV Sokol del arquitecto I.P. Mashkov. Muchas de las llamadas "casas de renta" de principios de siglo fueron construidas en estilo Liberty.

Excelentes obras del modernismo son la estación Yaroslavsky, de Franz Schechtel, los grandes almacenes centrales (anteriormente Murray y Meriliz), el Hotel Metropol en Moscú y muchos otros. En Moscú trabajaron una serie de arquitectos, Schechtel, Klein RI, Fomin IV, que crearon una rama del estilo, llamada el moderno Moscú.

Durante un breve período, 1895-1905, la arquitectura rusa (con arquitectos de la talla de Lev Kékushev (1862-1913/7?), Franz Schechtel (1859-1926) y William Walcot (1874-1943) se enamoró de estilo "art nouveau", muy presente en Moscú. Este estilo, que siguió siendo popular hasta la Segunda Guerra Mundial, condujo a un "revival" neoclásico ruso en los años 1905-1914, una síntesis del estilo imperio y del paladianismo apoyándose en técnicas de construcción contemporáneas.

En los años posteriores a la Revolución de Octubre, los arquitectos que se negaron a emigrar, junto con los de la nueva generación, comenzaron a denunciar la herencia burguesa y adoptaron un sesgo formalista. Se dibujaron los planes funcionales de las grandes ciudades industriales de la técnica futura. El proyecto más ambicioso de este período fue la Torre Tatlin, diseñada en 1919 por Vladímir Tatlin (1885-1953): es una espiral de 400 m, envolviendo un eje inclinado con galerías móviles translúcidas. Un proyecto en gran medida quimérico, esta Torre Tatlin inspiró a una generación de arquitectos constructivistas en Rusia y en otros lugares. La torre de Shújov (1920-1922), que domina Moscú desde la altura de sus , fue una torre de telecomunicaciones que se completó en 1922. Según los primeros planos, la estructura hiperboloide de Vladímir Shújov (de de altura) tenía una masa estimada de , mientras que la Torre Eiffel (de de altura) pesa .
Una de las prioridades del período soviético fue la reconstrucción de las ciudades. En 1918, Alekséi Shchúsev (1873-1949) e Iván Zholtovski crearon el Taller de Arquitectura "Mossovet" (Consejo municipal de Moscú), donde se elaboraron los planes para el nuevo Moscú, entonces la capital de los Soviets. Los jóvenes arquitectos de este taller más tarde se convertirían en líderes de la vanguardia rusa. Al mismo tiempo, la enseñanza de la arquitectura, exclusivamente enseñada en las Vjutemás, se debatía entre «neos» y «modernistas».

Bajo el impulso del arquitecto neoclásico Iván Fomín (1872-1936), el urbanismo y el formación arquitectónica experimentaron un giro idéntico en Petrogrado desde 1919. Las otras grandes ciudades siguieron su ejemplo, lo que tuvo la consecuencia una agitación en el urbanismo de las ciudades históricas de Rusia. Fue en Petrogrado donde nacieron los modelos urbanos ("generalny plan"). La ciudad ahora se veía como una red de grandes bulevares, dominada por edificios ciclópeos, y favorecía el alojamiento de los trabajadores con viviendas con calefacción y agua corriente. El primer edificio de apartamentos de este tipo fue inaugurado en 1923 y dio la señal de partida para la construcción de grandes complejos en 1925-1929.

El primer ejemplar de este nuevo estilo vio la luz en Petrogrado entre 1917 y 1919 con un monumento del Campo de Marte: «Combatientes de la Revolución», diseñado por Lev Rúdnev (1886-1956). Es un conjunto de simples monolitos de granito. Hará escuela e inspirará todos los desarrollos posteriores de la arquitectura monumental y de la escultura soviética. Pero el monumento más famoso de la época sigue siendo el Mausoleo de Lenin (1929-1930) de Alekséi Shchúsev. Originalmente concebido como un simple chalet de madera coronado por una pirámide, con dos alas (para entrada y salida), fue reconstruido en piedra en 1930. La combinación de labradorita marrón y negra subraya su esbeltez y la finura de sus líneas.

El rápido desarrollo de las tecnologías y los avances en la ciencia de los materiales jugaron un papel en la evolución del constructivismo. Si la construcción de la central hidroeléctrica de Vóljov (1918-1926, de los arquitectos O. Munts y V. Pokrovski) todavía se apoya en el modelo tradicional de las bóvedas (a pesar del uso del hormigón armado), la central hidroeléctrica del Dniéper (1927-1932), construido por un grupo de arquitectos liderado por Víktor Vesnín (1882-1950), innova con una presa de arco cuyos contrafuertes ritman el paramento aguas abajo.

Algunos grupos creativos desempeñaron un papel significativo en la expresión arquitectónica de la Rusia de los años 1920: entre ellos, la Asociación de Nuevos Arquitectos ("ASNOVA"), formada en 1923, que buscaba una síntesis del arte, la arquitectura convirtiéndose en una extensión de la escultura. Los edificios debían servir como puntos de orientación para los hombres. Fueron miembros de "ASNOVA" los que diseñaron los primeros rascacielos de Moscú, ninguno de los cuales se realizó en ese momento (1923-1926).

Pero la Rusia revolucionaria creó un nuevo tipo de institución: los Palacio de los trabajadores y los Palacio de la Cultura, que dieron a los arquitectos un nuevo campo de expresión, donde compitieron en la combinación de grandes volúmenes con motivos que evocaban la maquinaria y la producción industrial. El más famoso es probablemente el Club de Zúiev (1927-1929), construido en Moscú por Ilyá Gólosov (1883-1945), cuya composición se apoya en un contraste entre las superficies acristaladas de formas simples, paralepipédicas y cilíndricas, y los ángulos vivos de la fachada.

La expresión simbólica en la construcción fue una de las principales características de las obras de Konstantín Mélnikov (1890-1974), incluido el Club de Rusakov de Moscú (1927-1929). El edificio parece un engranaje; cada uno de los tres balcones de hormigón en voladizo del gran auditorio evoca un diente de engranaje. La precisión de esta composición (que Mélnikov describe como un «músculo vendado») y la convertibilidad del espacio interior lo convierten en uno de los ejemplos más importantes de la arquitectura soviética.

 
La arquitectura estalinista, como la arquitectura nazi, apreciaba la monumentalidad de inspiración conservadora. A lo largo de la década de 1930, la URSS experimentó una rápida urbanización en aplicación de las políticas estalinistas, y las autoridades promovieron un concurso internacional para el Palacio de los Soviets de Moscú.

Después de 1945, no contenta con la reconstrucción de las ciudades destruidas durante la Segunda Guerra Mundial, la Unión Soviética se embarcó en una política de prestigio y comenzó la construcción de siete rascacielos en lugares simbólicos de la región de Moscú, con el fin de para competir con las construcciones estadounidenses. La Universidad Estatal de Moscú (1948-1953), de Lev Rúdnev y asociados, es notable por el uso del espacio. Otro ejemplo es el Centro de Exposiciones de Moscú, construido con motivo de la 2.ª Feria Agrícola de la Unión ("VSJV") en 1954. Es una sucesión de pabellones, cada uno de un estilo diferente. Las estaciones del Metro de Moscú y del Metro de San Petersburgo, construidas en los años 1940 y 1950, son famosas por su concepción extravagante y su "decorum" convenido. En general, la arquitectura estalinista alteró la apariencia de las ciudades rusas y ucranianas de la posguerra, y sigue imprimiendo su marca en Rusia con sus avenidas desmesuradas y edificios ciclópeos públicos.

Cuando Stalin murió en 1953, el viraje social y político trasformó el país, y las prioridades de la construcción y la arquitectura se vieron naturalmente afectadas. En 1955, Nikita Jruschov, confrontado con el retraso en la construcción de viviendas, pidió medidas drásticas para forzar el ritmo. Consistieron en promover la producción en masa y economizar en el "decorum"del período precedente.

Estas medidas finalmente supusieron la sentencia de muerte de la arquitectura estalinista; pero la transición fue lenta. En 1955, la mayoría de los programas, tanto de pre-proyecto como de construcción, quedaron directamente involucrados; las grandes obras evolucionaron así hacia una asimetría improvisada: un ejemplo famoso es la avenida Jreschátyk y el parque en la Plaza de la Independencia, en la capital ucraniana, Kiev: tenía que ser originalmente la organización de una gran plaza rodeada de edificios estalinistas; pero a medida que los edificios se estaban terminando, los arquitectos se vieron obligados a modificar los planos y la obra fue abandonada hasta que se reanudaron los trabajos en la década de 1980. En particular, el Hotel Ucrania, supuestamente con vistas al parque con una arquitectura que evocaría los siete rascacielos de Moscú, quedó en un paralelepípedo crudo, desprovisto de ornamentación.

Sin embargo, a medida que la geometría de los edificios evolucionó hacia las formas cúbicas despojadas, su arquitectura dio origen a un nuevo estilo, inspirado en la conquista del espacio: la funcionalidad. El Palacio Estatal del Kremlin recuerda los esfuerzos de los arquitectos para hacer evolucionar sus planes al ritmo de los cambios en la política estatista. La Torre Ostánkino, obra de Nikolái Nikitin, simbolizaba el progreso.

Además de los edificios desnudos, la década de 1960 estuvo marcada por la política de alojamiento. Los arquitectos imaginaron un edificio-tipo de cinco pisos, que se podría construirse a partir de paneles de hormigón prefabricados. Estos "Pyatietazhki" (edificios de cinco plantas) se convirtieron en el módulo habitable de las ciudades urbanas rusas. Construidos a toda prisa, eran de una calidad muy inferior a los edificios del período estalinista; su aspecto gris y repetitivo dio a las ciudades de la Unión Soviética su apariencia aburrida y deprimente.

A principios de la década de 1970, Leonid Brézhnev dio a los arquitectos más libertad y pronto se vio afectada la variedad de edificios: los bloques de edificios con cubiertas planas ganaron altura y fueron adornados con colores variados; el recurso a los mosaicos en los silencios se convirtió en el sello distintivo de la época. En casi todos los casos, estos fueron construidos no como una construcción independiente, sino como parte de grandes complejos de viviendas) que pronto se convirtió en una característica central de las ciudades socialistas. En contraste con las casas construidas en los años 1950-60, que tenían hasta 5 pisos, los nuevos edificios residenciales eran más altos y podían tener hasta 9 o más pisos, aunque siguieron edificándose bloques de menos pisos. Cada complejo incluía un área extensa con un patio para caminar, un parque infantil con columpios, un cajón de arena para los juegos y sitios para estacionamiento de vehículos, que a menudo se complementan con garajes para automóviles, alineados por separado de los edificios residenciales.En la mayoría de los casos, estos edificios formaban parte de grandes complejos. La arquitectura de los edificios públicos se inspiró en múltiples temas. Algunos de ellos (como la «Casa Blanca de Moscú» (1965-1981) fueron guiños a la arquitectura de la década de 1950, con sus fachadas de mármol blanco y los bajorrelieves que cubrían las paredes laterales.

Después de la desintegración de la Unión Soviética, muchos de sus proyectos quedaron en suspenso, y algunos fueron cancelados por completo. Sin embargo, por primera vez, el Estado ya no ejercía ningún control sobre qué tema o qué altura podía alcanzar un edificio. Como resultado de ello, con la mejora general de las condiciones financieras, la creciente concentración de capital y el retorno de las inversiones internacionales en Rusia, causaron un boom arquitectónico. Los inversores trajeron consigo los métodos modernos de construcción de rascacielos, como ejemplifica el ambicioso centro de negocios que se está construyendo en Moscú, el Centro Internacional de Negocios de Moscú. En otros casos, los arquitectos se volvieron a los diseños más exitosos de la arquitectura estalinista, que dio lugar a edificios como el Palacio del Triunfo en Moscú.
Según el historiador ruso Borís Rybakov, la típica cúpula acebollada de las iglesias ortodoxas rusas tiene un origen nativo a partir de influencias pre-mongoles, con ejemplos constructivos a partir del siglo XII, en tanto la arquitectura mogol y el estilo difundido en Asia por el Islam presenta sus primeros ejemplos recién en el siglo XV.

Mientras que en las iglesias rusas primitivas, especialmente en Kiev la primera capital, las cúpulas seguían el modelo esférico del estilo bizantino, los edificios posteriores comenzaron a utilizar las cúpulas acebolladas, una forma especialmente útil para evitar la acumulación de nieve en el clima nórdico.

La influencia ortodoxa se trasmitió a la arquitectura persa y regiones más orientales, como lo demuestran las cúpulas icónicas del Taj Mahal, construido en 1630.

El ejemplo más conocido lo constituye la Catedral de San Basilio, construida entre 1555 y 1561 en Moscú por orden de Iván el Terrible en conmemoración de la captura del Janato de Kazán. Coronada por un total de diez torres con cúpulas acebolladas, la catedral ha sido desde su creación un símbolo de Moscú como centro de síntesis entre oriente y occidente.





</doc>
<doc id="16804" url="https://es.wikipedia.org/wiki?curid=16804" title="Robert Louis Stevenson">
Robert Louis Stevenson

Robert Louis Balfour Stevenson (Edimburgo, Escocia, 13 de noviembre de 1850-Vailima, cerca de Apia, Samoa, 3 de diciembre de 1894) fue un novelista, poeta y ensayista británico. Su legado es una vasta obra que incluye crónicas de viaje, novelas de aventuras e históricas, así como lírica y ensayos. Se lo conoce principalmente por ser el autor de algunas de las historias fantásticas y de aventuras más clásicas de la literatura como "La isla del tesoro", la novela de aventuras "Secuestrado", la novela histórica "La flecha negra" y la popular novela de horror "El extraño caso del doctor Jekyll y el señor Hyde", dedicada al tema de los fenómenos de la personalidad escindida y que puede ser clasificada como novela psicológica de horror. Varios de sus novelas y cuentos continúan siendo populares y algunos de estos han sido adaptados más de una vez al cine y a la televisión, principalmente del siglo XX. Fue importante también su obra ensayística, breve pero decisiva en lo que se refiere a la estructura de la moderna novela de peripecias. Fue muy apreciado en su tiempo y siguió siéndolo después de su muerte. Tuvo influencia sobre autores como Joseph Conrad, Graham Greene, G. K. Chesterton, H. G. Wells,Adolfo Bioy Casares y Jorge Luis Borges.

Robert Louis Stevenson nació en Edimburgo, Escocia, en una casa ubicada en el número 8 de Howard Place. Fue el hijo único del abogado y constructor de faros Thomas Stevenson y de Margaret Isabella Balfour (1830-1897). Originalmente fue bautizado como Robert Lawes Balfour, pero cuando contaba con veinte años, su padre hizo que le cambiaran el nombre Lawes por la versión francesa Louis para evitar las asociaciones con un político radical de igual nombre. Su abuelo, Robert Stevenson, sus tíos Alan Stevenson y David Stevenson, sus primos, David Alan Stevenson y Charles Alexander Stevenson así como también Alan Stevenson (1891-1971), familiar en segundo grado de consanguinidad, fueron todos ingenieros y constructores de faros. La familia de su madre debía su apellido a Alexander Balfour, quien poseía tierras en la región de Fife en el siglo XV. El padre de Margaret, Lewis Balfour (1777-1860), había sido pastor de la Iglesia de Escocia en la localidad aledaña de Colinton, donde Stevenson solía pasar sus vacaciones en la infancia. El escritor Graham Greene era, en la línea materna, un sobrino nieto de Robert Louis Stevenson.

Los padres de Stevenson también eran presbiterianos. La salud de su madre estaba constitucionalmente debilitada y padecía de enfermedades respiratorias, debilidad de la cual también Stevenson sufrió durante toda su vida. El clima escocés de veranos frescos e inviernos lluviosos y nublados era muy inconveniente tanto para la madre como para el hijo, que por consejo del médico de la familia pasaban muchas mañanas en cama. Para aliviar a la madre la familia contrató en 1852 a la niñera Alison Cunningham (1822-1910), llamada «Cummy», quien impresionaba tanto al pequeño Louis con su calvinismo austero y sus historias nocturnas truculentas que provocaron que el niño comenzara a tener pesadillas por las noches. La familia se mudó en 1853 a una casa en el número 1 de Inverleith Terrace pero la ubicación de esta vivienda era aún más inconveniente de modo que en 1857 volvieron a mudarse, esta vez al número 17 de Heriot Row.
Cuando apenas contaba con dos años, su familia llevaba ya al pequeño Louis a la iglesia. Allí escuchaba las prédicas con historias, por ejemplo, sobre Caín y Abel, el Libro de Daniel o sobre del diluvio universal. Se agregaban a estos estímulos los relatos truculentos de Cummy sobre la oscura historia de la iglesia escocesa, los cuales asustaban al niño pero, al mismo tiempo, le producían gran fascinación. Su obra fue fuertemente influida por las experiencias infantiles tempranas. Cummy se preocupaba por él de manera conmovedora cuando yacía enfermo en cama y le leía pasajes de algunas obras como "Pilgrim’s Progress" de John Bunyan y de la Biblia. Su obra "A Child’s Garden of Verses", que apareció en 1885 y que hasta hoy sigue siendo un favorito en Gran Bretaña tiene una dedicatoria a su niñera Cummy, muestra del recuerdo de aquella época de Stevenson, a sus treinta y cinco años.

A su primera ocupación favorita de «jugar a la iglesia» (con un púlpito construido con sillas y mesas, desde donde recitaba y cantaba como pastor) le siguió la afición por rimar e inventar historias. Según consigna su madre en un diario sobre él, Stevenson escribió el primer quinteto en septiembre de 1855, cuando estaba a punto de cumplir los cinco años. Margaret Stevenson llevó un diario sobre la vida de su hijo, a quien llamaba familiarmente «Lou» o «Smout» (en escocés: «salmón de un año»), hasta que cumplió treinta y nueve años, por lo cual los años tempranos de Stevenson están bien documentados.

A partir de septiembre de 1857 Stevenson asistió a la Mr Henderson’s School, aunque por razones de salud solo podía participar en clases durante dos horas diarias. Tras pocas semanas, una bronquitis acabó con su asistencia regular a la escuela y comenzó a recibir clases particulares. Al cabo de cuatro años ingresó en la Edinburgh Academy, una escuela superior que a su vez abandonó a la edad de trece años. Luego de una breve estadía en el internado de Spring Grove en las cercanías de Londres, regresó para asistir desde 1864 a una escuela privada de su ciudad natal.

Durante su infancia escribía constantemente ensayos e historias. Su padre lo comprendía bien puesto que él mismo había escrito en su tiempo libre hasta que su propio padre le había dicho que dejara esa insensatez y se dedicara a los negocios. El primer libro histórico del joven Stevenson, "Pentland Rising", que escribió en la tradición de las novelas de "sir" Walter Scott, apareció en 1866, editado en Edimburgo por Andrew Elliot. Para los editores no constituía riesgo alguno, puesto que su padre se había tenido que comprometer a comprar los ejemplares que hasta una fecha determinada no hubiesen sido vendidos, práctica que por aquel entonces era frecuente. Y ese fue el caso. La novela era de escaso valor literario. Veinte años más tarde, sin embargo, cuando el autor ya era famoso, la obra llegó a alcanzar precios de fantasía.

En 1867 Thomas Stevenson adquirió una casa de campo como residencia de veraneo, el Swanston Cottage, cerca de Edimburgo. Con el correr de los años esta casa, ubicada a los pies del área montañosa de Pentland Hills, se transformó en el refugio frecuente del futuro escritor entre los meses de marzo y octubre.

En los años de su adolescencia Robert acompañó a su padre en sus frecuentes viajes, lo que inspiró algunas de sus obras recientemente plasmadas en libros.

Ingresó en la Universidad de Edimburgo como estudiante de Ingeniería Náutica. Sin embargo, la elección de la carrera fue más por la influencia de su padre, que era ingeniero, que por gusto propio. Esto le llevó al abandono de la ingeniería en pos del estudio de derecho. En 1875 empezó a practicar la abogacía. Tampoco tuvo una carrera brillante en este campo, ya que su interés se concentraba en el estudio de la lengua.

Enseguida aparecieron en él los primeros síntomas de la tuberculosis e inició una serie de viajes por el continente. En 1876, a los veintiséis años, en Grez (Francia), conoció a Fanny Osbourne, una norteamericana que estaba separada. Stevenson y Fanny se enamoraron. Él publicó su primer libro en 1878. Ella partió a California, para tramitar su divorcio, y Stevenson la siguió, un año después. Se casó con Fanny en 1880, a los treinta años. La pareja vivió un tiempo en Calistoga, en el Lejano Oeste. Escribió historias de viajes, aventuras y romance. Su obra es muy versátil: ficción y ensayo, entre otras.

A partir de ese año, la salud de Stevenson comenzó a empeorar. El matrimonio se mudó a Edimburgo, luego a Davos, Suiza, y finalmente se instaló en una finca que el padre de Stevenson les regaló, en el balneario de Bournemouth. Tres años más tarde partieron a Nueva York, donde Stevenson hizo amistad con Mark Twain, autor de "Las aventuras de Tom Sawyer" y con Henry James
. Tras una breve estancia en San Francisco, decidieron realizar un viaje hacia las islas del Pacífico Sur, donde finalmente se establecieron con los hijos de Fanny, la hija de esta, Belle, y la señora Stevenson (el padre del novelista había muerto para entonces). La relación de Stevenson con los aborígenes —que lo bautizaron como "Tusitala" («el que cuenta historias»)— era cordial. Stevenson, por otra parte, se implicó en la política local: de hecho, el escritor tomó partido por uno de los jefes locales contra la dominación alemana del archipiélago y escribió en la prensa británica sobre la penosa situación samoana. También escribió una conocida carta abierta, la "Defensa del Padre Damián" en Sídney, Australia, el 25 de febrero de 1890, contra el reverendo Dr. C. M. Hyde, de Honolulu, en Hawái.

Murió en 1894 de una hemorragia cerebral, una hora después de que Stevenson terminará de dictar a Osbourne un párrafo de su novela más ambiciosa, Weir of Hermiston. Un año antes había relatado en una carta: «Durante catorce años no he conocido un solo día efectivo de salud. He escrito con hemorragias, he escrito enfermo, entre estertores de tos, he escrito con la cabeza dando tumbos». Era conocida su afición al alcohol, lo que le había acarreado diversos problemas de salud. Su cuerpo fue enterrado en la misma isla, en el monte Vaea.

Ante la aparición de la novela naturalista o psicológica, Stevenson reivindicó el relato clásico de aventuras, en el que el carácter de los personajes se dibuja en la acción. Su estilo elegante y sobrio y la naturaleza de sus relatos y sus descripciones influyeron en escritores del siglo XX como ya se citó anteriormente.





Libros de viajes:

Otras obras:


Al menos dos de las grandes obras de Stevenson han sido llevadas al cine. "El planeta del tesoro" es la más reciente versión en película animada de la obra "La isla del tesoro". Por su parte, "El extraño caso del doctor Jekyll y el señor Hyde" ha sido llevado al cine en múltiples versiones. En la película "The Pagemaster" ("El guardián de las palabras"), una producción de Turner Pictures, se hace alusión a estas dos historias. En ella, el protagonista, un niño de diez años llamado Richard Tyler es convertido en un dibujo animado y tiene que lidiar con personajes de diferentes obras de ficción, como con el mismísimo Dr. Jekyll, y su alter ego, Mr. Hyde, con el capitán Achab, de la novela "Moby-Dick" de Herman Melville, y con John Silver, otro de los personajes de Stevenson.


También la novela "La flecha negra" ha sido llevada al cine y la televisión en varias ocasiones. La primera adaptación data de 1911, dirigida por Oscar Apfel. Entre los largometrajes y series, destacan la dirigida en 1948 por Gordon Douglas y la tv movie de 1985, de John Hough y con actores como Oliver Reed, Fernando Rey, Benedict Taylor o Georgia Slowe.



</doc>
<doc id="16806" url="https://es.wikipedia.org/wiki?curid=16806" title="Resumen">
Resumen

El resumen es un escrito que sintetiza las ideas principales de un texto.

La extensión del resumen puede variar, pero no suele pasar el 25 % de la extensión del original. En el resumen se han de evidenciar los vínculos lógicos de las ideas explicadas en él texto de partida, aunque esto suponga cambiar el orden en que aparecen, y la redacción debe adoptar un tono objetivo, independientemente del punto de vista del autor del texto base.

Un resumen es una exposición abreviada, concisa y fiel sobre los puntos más importantes de un texto. Como tal, puede realizarse de manera oral o escrita, y debe ser objetivo, claro y coherente. Un resumen no se trata de copiar y pegar, "resumir" tal cual un texto consiste en rescatar las ideas principales y secundarias de una forma clara parafraseando lo que estamos leyendo, sin cambiar el contexto.

Los resúmenes pueden elaborarse con diferentes objetivos:

El resumen documental o "abstract", requiere una metodología y puede abordarse mediante diferentes paradigmas y modelos. 

La Asociación Española de Normalización y Certificación (AENOR), a través de sus normas, hace recomendaciones de cómo preparar resúmenes siguiendo unos estándares de calidad.

Con la tecnología en recuperación de información se han creado sistemas de resumen automático de documentos, que requieren un tratamiento de la información digital en el procesamiento del lenguaje natural.



</doc>
<doc id="16808" url="https://es.wikipedia.org/wiki?curid=16808" title="René Barrientos Ortuño">
René Barrientos Ortuño

René Barrientos Ortuño (Tarata, Cochabamba, 30 de mayo de 1919-Arque, Cochabamba, 27 de abril de 1969) fue un militar y político boliviano, 47.º presidente de Bolivia.

Hizo sus estudios primarios en su pueblo natal para luego ingresar al convento de Tarata, pero salió pronto de ahí ya que sus gustos personales no coincidían con el hábito de monje. 

En 1938, con 19 años de edad, después de una discusión con el principal del convento, decidió abandonarlo con la idea dedicarse a la carrera militar, viajando para ello a la ciudad de La Paz para ingresar al Colegio Militar del Ejército, de donde egresó como subteniente en 1943. Después realizó también estudios en la Escuela Militar de Aviación «Boquerón» (actualmente denominado Colegio Militar de Aviación). En 1945 estudió como piloto en Estados Unidos.

Durante el gobierno del presidente Mamerto Urriolagoitia Harriague, participó en la guerra civil de 1949 a favor del Movimiento Nacionalista Revolucionario (MNR), motivo por el cual fue dado de baja de las Fuerzas Armadas de Bolivia.

Tres años después, en 1952, fue reincorporado con el grado de capitán. Al crearse la Fuerza Aérea Boliviana (FAB) como nueva rama del ejército boliviano en 1957, siendo ya general, fue nombrado comandante en jefe de la Fuerza Aérea de Bolivia.

Fue elegido vicepresidente de Bolivia acompañando al presidente Víctor Paz Estenssoro en su tercer gobierno, cargo del que se posesionó el 6 de agosto de 1964. En la huelga nacional del 29 al 31 de octubre, se encargó personalmente de reprimir a los obreros y mineros, y tres días después, el 4 de noviembre de 1964, dio un golpe de Estado, traicionando a su propio presidente.

Se nombró presidente de la Junta Militar (1964-1965), al año siguiente (1965) tuvo que aceptar un copresidente, Alfredo Ovando Candía. En 1966, Barrientos fue elegido presidente constitucional. Llevó adelante un gobierno de desarrollo económico. Favoreció a los campesinos y se enfrentó contra los obreros y mineros. En 1967 promulgó una nueva Constitución política del Estado, que estuvo vigente durante 42 años, hasta 2009, cuando fue cambiada durante el primer gobierno del presidente Evo Morales Ayma.

El 7 de noviembre de 1966, se inició la guerrilla comandada por Ernesto "Che" Guevara. En marzo de 1967, casi medio año después de su llegada, el Che y su grupo tuvieron el primer choque con el ejército boliviano en Ñancahuazú en el departamento de Santa Cruz. René Barrientos y el jefe de Estado Mayor, Alfredo Ovando Candia, dedicaron todos sus recursos a aplastar al comandante “Che” Guevara. Contrariamente a lo que él esperaba, Guevara no recibió la ayuda del campesinado boliviano; por el contrario, estos daban un apoyo total a Barrientos.

Guevara anotó, en su "Diario de Bolivia", al respecto: 

En abril de 1967, fue capturado Regis Debray; en octubre cayeron, fueron apresados o huyeron dispersos los últimos guerrilleros sobrevivientes; el “Che” herido en combate, fue fusilado horas después en la escuelita de La Higuera, el 9 de octubre de 1967.

Durante su gobierno, Barrientos nombró al criminal de guerra nazi de la segunda guerra mundial Klaus Barbie, que se hacía llamar en Bolivia, Klaus Altmann, presidente de la Sociedad Naviera del Estado (Transmarítima), que en la época contaba con un solo barco, y que según informaciones reservadas, se dedicaba al comercio internacional ilegal de armas. Barbie también fue nombrado por Barrientos asesor de los Servicios de Inteligencia de Bolivia. Particularmente elevado fue el número de víctimas durante su dictadura. Según Amnistía Internacional, solo entre 1966 y 1968 se ejecutaron varios asesinatos por parte de los escuadrones de la muerte. Incluida también la llamada Masacre de San Juan de 1967 en la que miembros del Ejército de Bolivia atacaron a la población de los centros mineros de Catavi y Siglo XX.

Barrientos, llamado “"El General del Pueblo"”, tuvo un amplio apoyo popular campesino; sin embargo, poco es lo que hizo durante su período, pues se dedicó más a la política y a viajar semanalmente a todos los distritos del país ,especialmente en el departamento de Cochabamba. Barrientos fue uno de los pocos presidentes de Bolivia que viajó por casi todo el país. 

Precisamente en uno de esos viajes, que le alejaba de la sede de gobierno, fue asesinado por Faustino Ricotoro por encargo de Ovando Candia . El día 27 de abril de 1969 Barrientos había visitado el pueblo de Arque y cuando su helicóptero, llamado "Holofernes" levantaba vuelo para retornar a la ciudad de Cochabamba, este impactó con unos cables de telégrafo, cayendo el helicóptero a tierra e incendiándose inmediatamente. Barrientos falleció en el accidente junto a su edecán de servicio y el piloto. Hasta la fecha no se ha despejado el rumor de que se trató de un accidente, ya que otra versión afirmaba que la caída del helicóptero había sido provocada intencionadamente.

Inmediatamente después de la muerte de Barrientos, su vicepresidente Luis Adolfo Siles Salinas se hizo cargo de la presidencia de Bolivia. El entierro de René Barrientos fue uno de los más grandes y apoteósicos del país, ya que toda las personas, desde el campesinado hasta la clase media asistieron a su entierro, quizá muy comparable y similar al entierro ocurrido 104 años antes con el expresidente Manuel Isidoro Belzu en 1865.



</doc>
<doc id="16809" url="https://es.wikipedia.org/wiki?curid=16809" title="Render sólido">
Render sólido

Algoritmo de renderización algo más complejo que el Wireframe en el que se usan técnicas de sombreado rudimentarias.

Cada malla de la escena está formada por polígonos, cada uno de un color. El render sólido calcula el vector normal a cada polígono y calcula (frecuentemente mediante un producto escalar) lo perpendicular que es el polígono a una fuente de luz puntual. De esta forma, cuanto más se acerca el ángulo formado pr la normal y el vector que está en el centro del polígono a cero se considera que el polígono debe de estar más iluminado.

El sombreado es uniforme para todo el polígono.

[Categoría:Algoritmos]

</doc>
<doc id="16811" url="https://es.wikipedia.org/wiki?curid=16811" title="Lenguas del Reino Unido">
Lenguas del Reino Unido

Las lenguas del Reino Unido comprenden las lenguas habladas por la población que reside en territorio británico, es decir, que incluye tanto los territorios insulares de Europa occidental como numerosos territorios de ultramar el los demás continentes.

La lengua oficial "de facto" del Reino Unido es el inglés, que es hablado como lengua primaria del 95% de la población del Reino Unido. Junto a esta lengua oficial existen otras lenguas regionales habladas sólo en algunas regiones del territorio, como el idioma galés que es también una lengua oficial en Gales, y es la segunda lengua más hablada en el Reino Unido. Además, existen varias lenguas vivas autóctonas al territorio, diversos dialectos regionales y lenguas habladas por numerosas poblaciones de inmigrantes recientes y los que han aprendido como segunda lengua.

Junto con el inglés originado en la isla de Gran Bretaña se hablan todavía otras lenguas europeas, dichas lenguas se adscriben a tres subfamilias indoeuropeas:

También se usan lenguas criollizadas como el anglorromaní que es una versión criollizada de romaní muy influida por el inglés o el shelta de base gaélica. Además de estas lenguas orales, la comunidad de sordos de Reino Unido usa ampliamente el Lengua británica de señas ("BSL" o "British Sign Language") que no está filogenéticamente emparentada con las lenguas de señas del continente, ni siquiera con el la lengua de señas norteamericana.

En la antigüedad y en la Edad Media se hablaron además otras lenguas actualmente extintas entre ellas:

En sus pequeñas colonias territoriales ultramarinas de Asia, América, África y Oceanía, muchos de ellos conservan sus lenguas nativas. En otro buen número de territorios dependientes de Gran Bretaña se habla lenguas criollas de base léxica inglesa que se desarrollaron de manera autóctona en esos territorios.

Junto con las lenguas anteriores, en Reino Unido existen numerosos grupos de población inmigrante que continúan usando su lengua originaria en el contexto familiar. Las principales lenguas de la inmigración en Reino Unido son
el polaco (0,6 % de la población),
el tamil (0,5 %),
el hindi-urdu,
el oriental y el occidental,
el bengalí,
el francés,
el español,
el cantonés,
el malaialam,
el griego,
el italiano,
el criollo caribeño,
el guyarati y
el cachemiro.
Entre todas superan los cien mil hablantes.



</doc>
<doc id="16814" url="https://es.wikipedia.org/wiki?curid=16814" title="KV">
KV

KV puede referir a:



</doc>
<doc id="16815" url="https://es.wikipedia.org/wiki?curid=16815" title="Reduccionismo">
Reduccionismo

El reduccionismo es el enfoque filosófico según el cual la reducción es necesaria y suficiente para resolver diversos problemas de conocimiento.

Puesto que la reducción, una operación epistémica, se puede practicar sobre diferentes objetos, la estrategia reduccionista constituye, en realidad, un conjunto de tesis ontológicas, gnoseológicas y metodológicas acerca de la relación entre diferentes ideas o campos científicos. Lo que esas tesis tienen en común es la idea de que las propiedades (reducción ontológica), conceptos, explicaciones o métodos (reducción gnoseológica) de un campo de investigación pueden ser reducidos (según el caso: analizados en términos de, identificados con, explicados por o sustituidos por) las propiedades, conceptos, explicaciones o métodos de otro campo de investigación que, por lo general, se refiere a un nivel de investigación inferior.

Por ejemplo, se ha intentado en diversas ocasiones reducir la biología a la química o la física. En este caso, el reduccionista afirma que la biología «no es más que» o «es en última instancia» química o física, con lo que niega que la biología se refiera a propiedades que están más allá del alcance de la química o la física o incluya conceptos, explicaciones o métodos propios, que no pertenecen al ámbito de la química o física. Los correspondientes supuestos reduccionistas ontológicos serían que los organismos "no son más que" agregados de sustancias químicas y que las sustancias químicas "no son más que" átomos físicos. Con lo dicho, queda claro que el problema del reduccionismo o, mejor dicho, el problema de la reducción, es pertinente respecto de otros problemas básicos de la filosofía y, en particular, de la filosofía de la ciencia, entre ellos los de la estructura de las teorías científicas, las relaciones interdisciplinarias, la naturaleza de la explicación, la unidad del método científico y de la ciencia en general, así como con respecto a problemas metafísicos tales como el de la emergencia.

Es importante notar que si bien el reduccionismo siempre está basado en la reducción, el uso de la reducción no supone necesariamente el reduccionismo. Como cualquier otra herramienta, la reducción puede ser utilizada de manera moderada o extrema. Es este último caso el que constituye la columna vertebral del reduccionismo radical. Es por ello que la ciencia no tiene por qué responder necesariamente a la filosofía reduccionista, a pesar de su uso intensivo de la reducción y de los enormes éxitos que la estrategia reductiva ha reportado en términos de conocimiento científico. Así pues, se puede sostener que los procesos mentales son reducibles a procesos cerebrales (hipótesis de la identidad mente-cerebro), lo que constituye una reducción ontológica, y a la vez rechazar la reducción (total) de la psicología a la neurofisiología. Aun en sus casos más exitosos, lo más habitual es que las reducciones solo sean parciales, no totales.




</doc>
<doc id="16816" url="https://es.wikipedia.org/wiki?curid=16816" title="Red irregular de triángulos">
Red irregular de triángulos

Una red irregular de triángulos (, abreviado TIN) es una representación de superficies continuas derivada de una estructura de datos espacial generada a partir de procesos de triangulación. Una malla TIN conecta una serie de puntos a través de una red de triángulos irregulares cuyos vértices se corresponden con dichos puntos, los cuales tienen las coordenadas "x", "y" y "z" de donde se localizan. La teselación resultante configura el modelo de superficie.


</doc>
<doc id="16818" url="https://es.wikipedia.org/wiki?curid=16818" title="Recursión (ciencias de computación)">
Recursión (ciencias de computación)

"Para un tratamiento más general de los fenómenos recursivos, ver el artículo de" Recursión"."

Recursión es, en ciencias de la computación, una forma de atajar y solventar problemas. De hecho, recursión es una de las ideas centrales de ciencia de computación. Resolver un problema mediante recursión significa que la solución depende de las soluciones de pequeñas instancias del mismo problema. 

La mayoría de los lenguajes de programación dan soporte a la recursión permitiendo a una función llamarse a sí misma desde el texto del programa. Los lenguajes imperativos definen las estructuras de "loops" como codice_1 y codice_2 que son usadas para realizar tareas repetitivas. Algunos lenguajes de programación funcionales no definen estructuras de "loops" sino que posibilitan la recursión llamando código de forma repetitiva. La teoría de la computabilidad ha demostrado que estos dos tipos de lenguajes son matemáticamente equivalentes, es decir que pueden resolver los mismos tipos de problemas, aunque los lenguajes funcionales carezcan de las típicas estructuras codice_1 y codice_2.

Un algoritmo recursivo es un algoritmo que expresa la solución de un problema en términos de una llamada a sí mismo. La llamada a sí mismo se conoce como llamada recursiva o recurrente.

Generalmente, si la primera llamada al subprograma se plantea sobre un problema de tamaño u orden "N", cada nueva ejecución recurrente del mismo se planteará sobre problemas, de igual naturaleza que el original, pero de un tamaño menor que "N". De esta forma, al ir reduciendo progresivamente la complejidad del problema que resolver, llegará un momento en que su resolución sea más o menos trivial (o, al menos, suficientemente manejable como para resolverlo de forma no recursiva). En esa situación diremos que estamos ante un caso base de la recursividad.

Las claves para construir un subprograma recurrente son:

Es frecuente que los algoritmos recurrentes sean más ineficientes en tiempo que los iterativos aunque suelen ser mucho más breves en espacio.

Un método frecuente para simplificar es dividir un problema en problemas derivados de menor tamaño del mismo tipo. Esto se conoce como "dialecting". Como técnica de programación se denomina divide y vencerás y es pieza fundamental para el diseño de muchos algoritmos de importancia, así como parte esencial de la programación dinámica.

Virtualmente todos los lenguajes de programación modernos permiten la especificación directa de funciones y subrutinas recursivas. Cuando se llama una función de este tipo, el ordenador, para la mayoría de los lenguajes en casi todas las arquitecturas basadas en una pila ("stack") o en la implementación del lenguaje, lleva la cuenta de las distintas instancias de la función, en numerosas arquitecturas mediante el uso de un "call stack", aunque no de forma exclusiva. A la inversa, toda función recursiva puede transformarse en una función iterativa usando un "stack".

La mayoría (aunque no todas) de las funciones y subrutinas que pueden ser evaluadas por un ordenador, pueden expresarse en términos de una función recursiva (sin tener que utilizar una iteración pura); a la inversa, cualquier función recursiva puede expresarse en términos de una iteración pura, dado que la recursión es, de por sí, también iterativa. Para evaluar una función por medio de la recursión, tiene que definirse como una función de sí misma (ej. el factor n! = n * (n - 1)! , donde 0! se define como 1). Resulta evidente que no todas las evaluaciones de funciones se prestan a un acercamiento recursivo. Por lo general, todas las funciones finitas pueden describirse directamente de forma recursiva; las funciones infinitas (ej. las series de e = 1/1! + 2/2! + 3/3!...) necesitan un criterio extra para detenerse, ej. el número de iteraciones, o el número de dígitos significativos, en caso contrario una iteración recursiva resultaría en un bucle infinito.

A modo de ilustración: Si se encuentra una palabra desconocida en un libro, el lector puede anotar la página actual en un papel y ponerlo en una pila (hasta entonces vacía). El lector consulta la palabra en otro artículo y, de nuevo, descubre otra palabra desconocida, la anota y la pone en la pila, y así sucesivamente. Llega un momento que el lector lee un artículo que donde todas las palabras son conocidas. El lector retorna entonces a la última página y continua la lectura desde ahí, y así hasta que se retira la última nota de la pila retornando entonces al libro original. Este "modus operandi" es recursivo.

Algunos lenguajes diseñados para programación lógica y programación funcional ofrecen la recursión como el único medio de repetición directa disponible para el programador. Estos lenguajes suelen conseguir que la recursión de cola sea tan eficiente como la iteración, permitiendo a los programadores expresar otras estructuras repetitivas (tales como codice_5 y codice_2 de scheme) en términos de recursión.

La recursión está profundamente anclada en la teoría de computación, con la equivalencia teórica de función microrecursiva y máquinas de Turing en la cimentación de ideas sobre la universalidad del ordenador moderno.

Crear una subrutina recursiva requiere principalmente la definición de un "caso base", y entonces definir reglas para subdividir casos más complejos en el caso base. Para una subrutina recursiva es esencial que con cada llamada recursiva, el problema se reduzca de forma que al final llegue al caso base.

Algunos expertos clasifican la recursión como "generativa" o bien "estructural". La distinción se hace según de donde provengan los datos con los que trabaja la subrutina. Si los datos proceden de una estructura de datos similar a una lista, entonces la subrutina es "estructuralmente recursiva"; en caso contrario, es "generativamente recursiva".

Un ejemplo clásico de una subrutina recursiva es la función usada para calcular el factorial de un entero.

"Definición de la función":

Una relación recurrente es una ecuación que relaciona términos posteriores en la secuencia con términos previos.

"Relación recurrente de un factorial":

Esta función factorial también puede describirse sin usar recursión haciendo uso de típicas estructuras de bucle que se encuentran en lenguajes de programación imperativos:

El lenguaje de programación scheme es, sin embargo, un lenguaje de programación funcional y no define estructuras de "loops" de cualquier tipo. Se basa únicamente en la recursión para ejecutar todo tipo de "loops". Dado que scheme es recursivo de cola, se puede definir una subrutina recursiva que implementa la subrutina factorial como un proceso iterativo, es decir, usa espacio constante pero tiempo lineal.

Otra popular secuencia recursiva es el Número de Fibonacci. Los primeros elementos de la secuencia son: 0, 1, 1, 2, 3, 5, 8, 13, 21...

Definición de la función:

Relación recurrente para Fibonacci:
b = b + b
b = 1, b = 0

Este algoritmo de Fibonacci es especialmente malo pues cada vez que se ejecuta la función, realizará dos llamadas a la función a sí misma, cada una de las cuales hará a la vez dos llamadas más y así sucesivamente hasta que terminen en 0 o en 1. El ejemplo se denomina "recursión de árbol", y sus requisitos de tiempo crecen de forma exponencial y los de espacio de forma lineal.

Otro famosa función recursiva es el algoritmo de Euclides, usado para computar el máximo común divisor de dos enteros. 

"Definición de la función":

"Relación recursiva del máximo común denominador", donde formula_6 expresa el resto de la división entera formula_7:

Nótese que el algoritmo "recursivo" mostrado arriba es, de hecho, únicamente de cola recursiva, lo que significa que es equivalente a un algoritmo iterativo. En el ejemplo siguiente se muestra el mismo algoritmo usando explícitamente iteración. No acumula una cadena de operaciones deferred, sino que su estado es, más bien, mantenido completamente en las variables "x" e "y". Su ""number of steps grows the as the logarithm of the numbers involved. "", al español "número de pasos crece a medida que lo hace el logaritmo de los números involucrados."

El algoritmo iterativo requiere una variable temporal, e incluso supuesto el conocimiento del Algoritmo de Euclides es más difícil de entender el proceso a simple vista, aunque los dos algoritmos son muy similares en sus pasos.

Para una detallada discusión de la descripción de este problema, de su historia y de su solución, consúltese el artículo principal. El problema, puesto de forma simple, es el siguiente: Dadas 3 pilas, una con un conjunto de N discos de tamaño creciente, determina el mínimo (óptimo) número de pasos que lleva mover todos los discos desde su posición inicial a otra pila sin colocar un disco de mayor tamaño sobre uno de menor tamaño.

"Definición de la función":

"Relación de recurrencia para hanoi":

Ejemplos de implementación:

Aunque no todas las funciones recursivas tienen una solución explícita, la secuencia de la Torre de Hanói puede reducirse a una fórmula explícita. 

El algoritmo de búsqueda binaria es un método de búsqueda de un dato en un vector de datos ordenado dividiendo el vector en dos tras cada pasada. El truco es escoger un punto cerca del centro del vector, comparar en ese punto el dato con el dato buscado para responder entonces a una de las siguientes 3 condiciones: se encuentra el dato buscado, el dato en el punto medio es mayor que el valor buscado o el dato en el punto medio es menor que el valor buscado. 

Se usa recursión en este algoritmo porque tras cada pasada se crea un nuevo vector dividiendo en original en dos. La subrutina de búsqueda binaria se llama entonces de forma recursiva, cada vez con un vector de menor tamaño. El tamaño del vector se ajusta normalmente cambiando el índice inicial y final. El algoritmo muestra un orden logaritmo de crecimiento porque divide esencialmente el dominio del problema en dos tras cada pasada.

Ejemplo de implementación de la búsqueda binaria:

Una aplicación de importancia de la recursión en ciencias de la computación es la definición de estructuras de datos dinámicos tales como listas y árboles. Las estructuras de datos recursivos pueden crecer de forma dinámica hasta un tamaño teórico infinito en respuesta a requisitos del tiempo de ejecución; por su parte, los requisitos del tamaño de un vector estático deben declararse en el tiempo de complicación.
"Los algoritmos recursivos son especialmente apropiados cuando el problema que resolver o los datos que manejar son definidos en términos recursivos."
Los ejemplos en esta sección ilustran lo que se conoce como "recursión estructural". Este término se refiere al hecho de que las subrutinas recursivas se aplican a datos que se definen de forma recursiva.
En la medida en que un programador deriva una plantilla de una definición de datos, las funciones emplean recursión estructural. Es decir, las recursiones en el cuerpo de una función consumen una determinada cantidad de un compuesto dado de forma inmediata.
A continuación se describe una definición simple del nodo de una lista enlazada. Nótese como se define el nodo por sí solo. El siguiente elemento del nodo del "struct" es un puntero a un nodo de "struct".
struct node

// LIST no es otra cosa que un nodo de "struct" *.
typedef struct node *LIST;
Las subrutinas que operan en la estructura de datos de LIST pueden implementarse de forma natural como una subrutina recursiva porque la estructura de datos sobre la que opera (LIST) es definida de forma recursiva. La subrutina "printList" definida a continuación recorre la lista hacia abajo hasta que ésta se vacía (NULL), para cada nodo imprime el dato (un número entero). En la implementación en C, la lista permanece inalterada por la subrutina "printList".
void printList(LIST lst)

Más abajo se muestra una definición simple de un nodo de árbol binario. Al igual que el nodo de listas enlazadas, se define a sí misma (de forma recursiva). Hay dos punteros que se refieren a sí mismos – "left" (apuntando a l aparte izquierda del subárbol) y "right" (a la parte derecha del subárbol).
struct node

// TREE no es otra cosa que un nodo " struct "
typedef struct node *TREE;
Las operaciones en el árbol pueden implementarse usando recursión. Nótese que, debido al hecho de que hay dos punteros que se referencian a sí mismos (izquierda y derecha), esas operaciones del árbol van a necesitar dos llamadas recursivas. Para un ejemplo similar, véase la función de Fibonacci y la explicación siguiente.
void printTree(TREE t) {
El ejemplo descrito ilustra un árbol binario de orden transversal. Un árbol de búsqueda binaria es un caso especial de árbol binario en el cual los datos de cada árbol están en orden.

En el ejemplo "factorial" la implementación iterativa es probablemente más rápida en la práctica que la recursiva. Esto es casi definido por la implementación del algoritmo euclidiano. Este resultado es lógico, pues las funciones iterativas no tienen que pagar el exceso de llamadas de funciones como en el caso de las funciones recursivas, y ese exceso es relativamente alto en muchos lenguajes de programación (nótese que mediante el uso de una "lookup table" es una implementación aún más rápida de la función factorial).

Hay otros tipos de problemas cuyas soluciones son inherentemente recursivas, porque estar al tanto del estado anterior. Un ejemplo es el árbol transversal; otros incluyen la función de Ackermann y el algoritmo divide y vencerás tales como "Quicksort". Todos estos algoritmos pueden implementarse iterativamente con la ayuda de una "pila", pero la necesidad del mismo, puede que anule las ventajas de la solución iterativa.

Otra posible razón para la utilización de un algoritmo iterativo en lugar de uno recursivo es el hecho de que en los lenguajes de programación modernos, el espacio de "stack" disponible para un hilo es, a menudo, mucho menos que el espacio disponible en el montículo, y los algoritmos recursivos suelen requerir más espacio de "stack" que los algoritmos iterativos. Véase, por otro lado, la sección siguiente que trata el caso especial de la recursión de cola.

Funciones de recursión de cola son funciones que finalizan con una llamada recursiva que no crea ninguna operación deferida. Por ejemplo, la función gcd (se muestra de nuevo más abajo) es recursiva de cola; sin embargo, la función factorial (que también se muestra más abajo) no es recursiva de cola porque crea operaciones diferidas que tienen que realizarse incluso después de que se complete la última llamada recursiva. Con un compilador que automáticamente optimiza llamadas recursivas de cola, una función recursiva de cola, como por ejemplo gcd, se ejecutará usando un espacio constante. Así, el proceso que genera es esencialmente iterativo y equivalente a usar estructuras de control de lenguaje imperativo como los bucles codice_2 y codice_1.

La importancia de recursión de cola es que cuando se realiza una llamada recursiva de cola, la posición de retorno de la función que llama necesita grabarse en el call stack; cuando la función recursiva retorna, continuará directamente a partir de la posición de retorno grabada previamente. Por ello, en compiladores que dan soporte a optimización de recursión de cola, este tipo de recursión ahorra espacio y tiempo.

El orden de invocación de una función puede alterar la ejecución de una función, véase este ejemplo en C:

void recursiveFunction(int num) {

void recursiveFunction(int num) {

Se habla de recursión directa cuando la función se llama a sí misma. Se habla de recursión indirecta cuando, por ejemplo, una función A llama a una función B, que a su vez llama a una función C, la cual llama a la función A. De esta forma es posible crear largas cadenas y ramificaciones, véase Parser descendente recursivo.




</doc>
<doc id="16819" url="https://es.wikipedia.org/wiki?curid=16819" title="Radiosidad">
Radiosidad

La radiosidad es un conjunto de técnicas para el cálculo de la iluminación global que tratan de resolver el problema básico de la renderización de la forma más realista posible en el campo de los gráficos 3D por computadora. Dicho problema es:

El transporte de la luz sólo se puede modelar de forma óptima considerando que cada fuente luminosa emite un número enorme de fotones, que rebotan al chocar contra una superficie describiendo una cantidad de trayectorias imposibles de simular en un computador.

Una de las técnicas empleadas en el cálculo de la radiosidad es el método de Montecarlo para resolver este problema mediante números aleatorios y de forma estadística. 

El auge de la radiosidad y otros métodos eficientes de renderización han posibilitado un auge en la infografía, siendo muy habitual encontrar por ejemplo películas que aprovechan estas técnicas para realizar efectos especiales.


</doc>
<doc id="16821" url="https://es.wikipedia.org/wiki?curid=16821" title="Radiación infrarroja">
Radiación infrarroja

La radiación infrarroja, o radiación IR es un tipo de radiación electromagnética, de mayor longitud de onda que la luz visible, pero menor que la de las microondas. Por ello, tiene menor frecuencia que la luz visible y mayor que las microondas. Su rango de longitudes de onda va desde unos 0,7 hasta los 1000 micrómetros. La radiación infrarroja es emitida por cualquier cuerpo cuya temperatura sea mayor que 0 Kelvin, es decir, −273,15 grados Celsius (cero absoluto).

Los infrarrojos son clasificados, de acuerdo a su longitud de onda, de este modo:


La materia, por su caracterización energética (véase cuerpo negro) emite radiación térmica. En general, la longitud de onda donde un cuerpo emite el máximo de radiación es inversamente proporcional a la temperatura de esta (Ley de Wien). De esta forma la mayoría de los objetos a temperaturas cotidianas tienen su máximo de emisión en el infrarrojo. Los seres vivos, en especial los mamíferos, emiten una gran proporción de radiación en la parte del espectro infrarrojo, debido a su calor corporal.

La potencia emitida en forma de calor por un cuerpo humano, por ejemplo, se puede obtener a partir de la superficie de su piel (unos 2 metros cuadrados) y su temperatura corporal (unos 37 °C, es decir 310 K), por medio de la Ley de Stefan-Boltzmann, y resulta ser de alrededor de 100 vatios.

Esto está íntimamente relacionado con la llamada "sensación térmica", según la cual podemos sentir frío o calor independientemente de la temperatura ambiental, en función de la radiación que recibimos (por ejemplo del Sol u otros cuerpos calientes más cercanos): Si recibimos más de los 100 vatios que 
emitimos, tendremos calor, y si recibimos menos, tendremos frío. En ambos casos la temperatura de nuestro cuerpo es constante (37 °C) y la del aire que nos rodea también. Por lo tanto, la sensación térmica en aire quieto, solo tiene que ver con la cantidad de radiación (por lo general infrarroja) que recibimos y su balance con la que emitimos constantemente como cuerpos calientes que somos. 
Si en cambio hay viento, la capa de aire en contacto con nuestra piel puede ser reemplazada por aire a otra temperatura, lo que también altera el equilibrio térmico y modifica la sensación térmica.

Los infrarrojos fueron descubiertos en 1800 por William Herschel,
un astrónomo inglés de origen alemán. Herschel colocó un termómetro de mercurio en el espectro obtenido por un prisma de cristal con el fin de medir el calor emitido por cada color. Descubrió que el calor era más fuerte al lado del rojo del espectro y observó que allí no había luz. Esta es la primera experiencia que muestra que el calor puede transmitirse por una forma invisible de luz. Herschel denominó a esta radiación "rayos calóricos", denominación bastante popular a lo largo del siglo XIX que, finalmente, fue dando paso al más moderno de radiación infrarroja.

Los primeros detectores de radiación infrarroja eran bolómetros, instrumentos que captan la radiación por el aumento de temperatura producido en un detector absorbente.

Los infrarrojos se utilizan en los equipos de visión nocturna cuando la cantidad de luz visible es insuficiente para ver los objetos. La radiación se recibe y después se refleja en una pantalla. Los objetos más calientes se convierten en los más luminosos. 

Un uso muy común es el que hacen los mandos a distancia (ó telecomandos) que generalmente utilizan los infrarrojos en vez de ondas de radio ya que no interfieren con otras señales como las señales de televisión. Los infrarrojos también se utilizan para comunicar a corta distancia los ordenadores con sus periféricos. Los aparatos que utilizan este tipo de comunicación cumplen generalmente un estándar publicado por la "Infrared Data Association".

La luz utilizada en las fibras ópticas es generalmente de infrarrojos.

El infrarrojo cercano es la región de longitud de onda más corta del espectro infrarrojo, situada entre la luz visible y el infrarrojo medio, aproximadamente entre 800 y 2500 nanómetros, aunque no hay una definición universalmente aceptada.

En astronomía, la espectroscopía en infrarrojo cercano se utiliza para estudiar las atmósferas de estrellas frías. En este rango pueden observarse líneas de transiciones rotacionales y vibracionales de moléculas como el óxido de titanio, cianógeno y monóxido de carbono, que dan información sobre el tipo espectral de la estrella. También se utiliza para estudiar moléculas en otros objetos astronómicos, como las nubes moleculares.

Otra de las muchas aplicaciones de la radiación infrarroja es la del uso de equipos emisores de infrarrojo en el sector industrial. En este sector las aplicaciones ocupan una extensa lista pero se puede destacar su uso en aplicaciones como el secado de pinturas o barnices, secado de papel, termofijación de plásticos, precalentamiento de soldaduras, curvatura, templado y laminado del vidrio, entre otras. La irradiación sobre el material en cuestión puede ser prolongada o momentánea teniendo en cuenta aspectos como la distancia de los emisores al material, la velocidad de paso del material (en el caso de cadenas de producción) y la temperatura que se desee conseguir.

Generalmente, cuando se habla de equipos emisores de infrarrojo, se distinguen cuatro tipos en función de la longitud de onda que utilicen:




</doc>
<doc id="16824" url="https://es.wikipedia.org/wiki?curid=16824" title="Transporte en Puerto Rico">
Transporte en Puerto Rico

21 aeropuertos, tres con vuelos internacionales. Prinair era la línea aérea nacional. Eastern Airlines hacía del aeropuerto de San Juan uno de sus centros de operaciones, aunque posteriormente ha pasado a ocurrir esto con American Airlines.

Puerto Rico contó con un sistema de ferrocarril en el siglo XIX. Desde finales del siglo XX sólo hay un tren que funciona, y se trata de una atracción turística en Bayamón. Para el 2003, hay planes de inaugurar un nuevo sistema ferroviario a través de la zona metropolitana que componen San Juan, Guaynabo, Bayamon y Carolina.


</doc>
<doc id="16827" url="https://es.wikipedia.org/wiki?curid=16827" title="Lenguas de Puerto Rico">
Lenguas de Puerto Rico

Las lenguas de Puerto Rico usuales son el español y el inglés; cada una tiene sus propios ámbitos de uso y el estatus jurídico varía según dicho ámbito.

La lengua oficial general de Puerto Rico es el castellano o español hablada por la gran mayoría de la población, coexistiendo con el inglés. Las lenguas oficiales del ejecutivo de Puerto Rico son el español y el inglés. Sin embargo, todos los asuntos oficiales del juzgado de distrito se manejan exclusivamente en inglés.

El español es la lengua materna de la mayor parte de la población y se usa en la educación primaria. Sólo una pequeña minoría, menos del 5%, usa el inglés como lengua principal. El español es la lengua principal en los negocios, la educación, la vida diaria y es hablada por el 95% de la población. Así pues la lengua vernácula es el español puertorriqueño que ha mostrado una gran capacidad de supervivencia y es considerado por la mayoría como un elemento significativo de su identidad colectiva. En el sincretismo cultural que refleja la historia puertorriqueña, la lengua española, adaptada al medio en siglos de acriollamiento, es el instrumento comunicativo natural que comparte hoy la sociedad isleña.

La enseñanza en escuelas públicas de Puerto Rico se lleva a cabo enteramente en español. Existen programas pilotos en una docena de las 1400 escuelas públicas, para llevar a cabo educación íntegramente en inglés. La población concibe el inglés como una segunda lengua que es materia obligaroia desde los niveles elementales hasta la secundaria. La comunidad sorda usa la lengua de señas americana y su variante local la lengua de señas de Puerto Rico.

El español de Puerto Rico ha evolucionado desarrollando muchas particularidades propias en el vocabulario y en la sintaxis, que lo diferencia del español de otras regiones. Si bien Puerto Rico recibió durante siglos migrantes de muchas áreas del dominio del español, se considera que el dialecto canario podría haber sido la principal fuente del mismo. El léxico del español de Puerto Rico incluye algunas palabras de origen taíno, especialmente en lo que se refiere a la flora local, los fenómenos naturales y los instrumentos musicales tradicionales. Igualmente, algunas palabras atribuidas a las lenguas de África son de uso general en los ámbitos de la comida, la música o la danza, especialmente en las localidades de la costa con mayores concentraciones descendientes de africanos.

De acuerdo con un estudio de la Universidad de Puerto Rico, nueve de cada diez puertorriqueños que residen en Puerto Rico no hablan inglés a un nivel avanzado. Más recientemente, de acuerdo con el "2005–2009 Population and Housing Narrative Profile for Puerto Rico", entre las personas mayores de 5 años que residen en Puerto Rico, el 95% hablan en casa una lengua que no es inglés, especialmente español. De los que no hablan inglés en casa, el 100% habla español y menos del 0,5% habla alguna otra lengua que no es el español, el 85% de los encuestados considera que no habla inglés "muy bien".

En el siglo XV cuando se produjo el primer contacto con los europeos, Puerto Rico, estaba poblado por los llamados taínos clásicos, una lengua muy extendida en todo el Caribe oriental de la familia arawak. Algunas palabras de esta lengua pasaron como préstamos léxicos al español de Puerto Rico.



</doc>
<doc id="16831" url="https://es.wikipedia.org/wiki?curid=16831" title="Psilotum nudum">
Psilotum nudum

Viridiplantae, Embryophyta, Tracheophyta, Euphyllophyta, Monilophyta, Clase Psilotopsida, Orden Psilotales, familia Psilotaceae, género "Psilotum", especie Psilotum nudum.

Sinónimos: "Whisk ferns" (en inglés), nombre con el que a veces también se refieren a toda la familia Psilotaceae, incluyendo a "Tmesipteris").

"Psilotum nudum" puede llegar a alcanzar entre 10 y 45 centímetros. Su tallo se encuentra ramificado en sus porciones terminales y tiene sección trígona. Los micrófilos adpresos tienen entre 1 y 2 mm. Poseen esporangios del tipo pseudosinangio trilobulados y globosos de color pardo amarillento que se abren por sendas hediduras longitudinales. Las esporas monoletas dan lugar a un protalo cilíndrico y ramificado que se desarrolla micorrizado bajo el sustrato. Planta en peligro de extinción (falta referencia de esta última afirmación).

Para el resto, ver caracteres de Psilotaceae.

Este helecho se desarrolla en grietas húmedas de arenisca. "Psilotum nudum" es una especie de distribución pantropical de América del norte, central y del sur, África, Asia, Macaronesia y Sur de la Península ibérica. Puede hibridar con "Psilotum complanatum", pero ocupan hábitats diferentes.






</doc>
<doc id="16834" url="https://es.wikipedia.org/wiki?curid=16834" title="Potencial evocado">
Potencial evocado

Potencial evocado se trata de una exploración neurofisiológica que evalúa la función del sistema sensorial acústico, visual, somatosensorial y sus vías por medio de respuestas provocadas frente a un estímulo conocido y normalizado. Se estudia la respuesta del sistema nervioso central a los estímulos sensoriales, analizando las vías nerviosas que desde la periferia aportan la información hacia el cerebro. 

Suelen ser pruebas no invasivas. El potencial evocado designa la modificación del potencial eléctrico producido por el sistema nervioso en respuesta a una estimulación externa, especialmente sensorial (un sonido, una imagen, etc.), pero también a un evento interno como una actividad cognitiva (atención, la preparación del motor, etc.) y se guarda a través de técnicas como la electroencefalografía (EEG) o la electromiografía (EMG). Cuando un tren de estímulos sensoriales de cualquier tipo llega al cerebro, provoca secuencias características de ondas en el trazado electroencefalográfico (EEG), que denominamos potenciales evocados. Son diferentes para cada modalidad sensorial y su variabilidad también depende de la intensidad del estímulo. Característicamente presentan una relación estable en el tiempo respecto al estímulo.
Gracias a los potenciales evocados se pueden estudiar diversos constructos. Por ejemplo, se ha correlacionado negativamente con el nivel de inteligencia (las personas con un cociente de inteligencia alto suelen presentar PE de latencia más corta).

1- Sensoriales : Es la respuesta neurofisiologica del sistema nervioso a un estímulo sensorial o de un tronco nervioso.

2-Motores: Es la respuesta de uno o varios músculos a la estimulación de un Tronco nervioso periférico o de algún punto en el sistema nervioso central en forma directa o transcraneana.

3- Reflejos: Es la respuestas motora de un músculos o grupo muscular a una estimulacion sensorial.

POTENCIALES EVOCADOS SENSORIALES 

Se pueden clasificar en función del tiempo de latencia y en relación con el órgano sensorial estimulado.

Corta : son independientes del estado de conciencia de la persona y en general se modifican poco por los anestesicos si estos están en rango apropiado.

Mediana latencia: son potenciales intermedios, en el caso de los auditivos son inconstantes, y se modifican con el nivel de anestesia.

Larga latencia: En general dependen fuertemente del grado de colaboración del observador y son fuertemente deprimidos por la anestesia. 

Se puede obtener:






</doc>
<doc id="16835" url="https://es.wikipedia.org/wiki?curid=16835" title="Postulados de Koch">
Postulados de Koch

Los postulados de Koch (o postulados de Henle-Koch) fueron formulados por Robert Koch y originalmente eran unos criterios destinados a establecer la relación de causa y efecto que vinculaban a un microbio y una enfermedad. Esos postulados fueron formulados en 1884 para establecer la etiología de la tuberculosis, y luego fueron redefinidos y publicados por Koch en 1890.

Utilizados para confirmar el papel etiológico de un microorganismo en otras enfermedades, esos postulados fueron modificados a lo largo del siglo XX de acuerdo con el estado del conocimiento, los problemas encontrados y la aparición de nuevas técnicas. Desde la década de 1980, los postulados han tenido una adaptación basada en técnicas moleculares.

Los postulados fueron formulados a partir de los experimentos de Robert Koch con "Bacillus anthracis". Demostró que al inyectar una pequeña cantidad de sangre de un ratón enfermo en uno sano, en el último aparecía carbunco. Tomando sangre del segundo animal e inyectándola en otro, obtenía de nuevo los síntomas de la enfermedad. Luego de repetir la operación una veintena de veces, consiguió cultivar la bacteria en caldos nutritivos fuera del animal y demostró que, incluso después de muchas transferencias de cultivo, la bacteria podía causar la enfermedad cuando se reinoculaba a un animal sano. Fueron aplicados para establecer la etiología del carbunco, pero ha sido generalizado para el resto de las enfermedades infecciosas con objeto de saber cuál es el agente participante. Los postulados son los siguientes:


</doc>
<doc id="16838" url="https://es.wikipedia.org/wiki?curid=16838" title="Portage (software)">
Portage (software)

Portage es el gestor de paquetes oficial de la distribución de Linux Gentoo y de sus derivadas Funtoo, Calculate Linux, así como de Google Chrome OS.

Portage (implementado en Python y Bash) está inspirado en los Ports BSD, aunque implementa ciertas características avanzadas que no están presentes en los ports BSD: gestión de dependencias, afinamiento preciso de los paquetes a gusto del administrador, instalaciones falsas (al estilo OpenBSD), entornos de prueba durante la compilación, desinstalación segura, perfiles de sistema, paquetes virtuales, gestión de los ficheros de configuración y múltiples ranuras para distintas versiones de un mismo paquete.

Portage dispone de un árbol local que contiene las descripciones de los paquetes de software, así como los scripts necesarios para instalarlos. Este árbol se puede sincronizar con un servidor remoto mediante una orden:

Cuando un paquete de software es seleccionado para ser instalado, Portage descarga los archivos con el código fuente y los compila en ese momento, generando los archivos ejecutables y documentación correspondiente. Es posible especificar las optimizaciones que emplear en la compilación, así como utilizar una variable llamada "USE" que indica la compatibilidad con otros programas. 

La posibilidad de indicar las optimizaciones y el parámetro "USE" permiten crear una distribución a medida. De todas formas, Portage también soporta la instalación de binarios, ya sean paquetes precompilados por el mismo sistema o paquetes que se encuentran exclusivamente en formato binario.

Portage permite mantener el software actualizado y controlar las versiones que se encuentran instaladas, proporcionando unas posibilidades similares a las de APT de Debian (excepto que APT utiliza por defecto binarios precompilados). Así, por ejemplo, es posible actualizar todos los paquetes instalados a la última versión estable sin necesidad de intervención del usuario:




</doc>
<doc id="16839" url="https://es.wikipedia.org/wiki?curid=16839" title="Popocatépetl">
Popocatépetl

El Popocatépetl (español) (náhuatl) es un volcán activo localizado en el centro de México. Se encuentra en los límites territoriales de los estados de Morelos, Puebla y el estado de México. Se localiza unos 72 km al sureste de la Ciudad de México, a 43 km de Puebla, a 63 km de Cuernavaca, y a 53 km de Tlaxcala. 

Tiene una forma cónica simétrica, y está unido por la parte norte con el Iztaccíhuatl mediante un paso montañoso conocido como Paso de Cortés. El volcán tiene glaciares perennes cerca de la boca del cono, en la punta de la montaña. Es el más alto de México, con una altitud máxima de 5500 metros sobre el nivel del mar, solo después del Citlaltépetl, de 5747 m.

El 3 de junio de 2019, registró dos nuevas explosiones que llevaron al gobierno mexicano a declarar alerta amarillo tipo 3, advirtiendo una posible caída de cenizas en los municipios de Ecatzingo y Atlautla, en el estado de México; Ciudad Ayala, Cuautla, Jantetelco, Jonacatepec, Ocuituco, Temoac, Tetela del volcán, Yecapixtla y Zacualpan, en el estado de Morelos, y en Acteopan, Atzizihuacan, Cohuean y Tochimilco, en el estado de Puebla. 

Su nombre, proveniente de la lengua náhuatl, compuesto por "Popōca" «que humea» y "tepētl" «montaña», en conjunto significa «montaña que humea», debido a su ya constante actividad desde la época prehispánica.

El Popocatépetl es un estratovolcán, y los estudios paleomagnéticos que se han hecho de él indican que tiene una edad aproximada de 730,000 años. Su altura es de 5,500 msnm, es de forma cónica, tiene un diámetro de 25 km en su base y la cima es el corte elíptico de un cono y tiene una orientación noreste-suroeste. La distancia entre las paredes de su cráter varía entre los 660 y los 840 m.

El Popocatépetl ha estado desde siempre en actividad, a pesar de haber estado en reposo durante buena parte de la segunda mitad del siglo XX. En 1991 se inició un incremento en su actividad y a partir de 1993 las fumarolas eran ya claramente visibles desde distancias de alrededor de 50 kilómetros.

Además, existe una gran cantidad de registros desde la antigüedad sobre los periodos de actividad del volcán, e incluso está registrada una erupción en 1927, que fue artificialmente provocada por la dinamitación del cráter para extraer azufre del mismo. La última erupción violenta del volcán se registró del 18 al 19 diciembre de 2000. El 25 de diciembre de 2005 se produjo en el cráter del volcán una nueva explosión, que provocó una columna de humo y cenizas de 3 kilómetros de altura y la expulsión de lava.

En vista de que la lava puede salir por cualquier fisura que se produzca en sus laderas y no solo por su cráter, es difícil conocer por adelantado cuáles serían las zonas afectadas en caso de erupción. Lo más que se puede decir es que si la lava saliera del lado norte o noreste, o este y sureste, el estado de Puebla se vería afectada . Si saliera del lado sur se vería afectado el estado de México y posiblemente el estado de Morelos, y si saliera del lado oeste y suroeste se vería afectada la región en donde se encuentra la población de Amecameca. El área de la superficie afectada dependerá de la viscosidad de la lava. Como última posibilidad teórica, si se llenara el cráter con lava (hecho poco probable), esta se desparramaría por el lado noreste, dirección en que se encuentra el borde más bajo del mismo.

Las zonas que serían afectadas por las cenizas y los gases del Popocatépetl dependerían de la dirección de los vientos, principalmente a la altura del cráter. A grandes rasgos, se puede decir que si las emisiones ocurrieran de noviembre a abril, el valle de Puebla sería el afectado. Si la erupción ocurriera de junio a septiembre, la región sur del estado de México y el estado de Morelos serían las regiones de mayores riesgo, aunque también podría sufrir daño el extremo sur del Distrito Federal (México).

Sin embargo, conocer todo esto no es suficiente para salvar vidas, ya que aún sabiendo que en una erupción grande que ocurriera por ejemplo en enero, los vientos acarrearían la nube de cenizas y gases hacia el estado de Puebla, probablemente no habría tiempo suficiente para organizar una evacuación, debido a que en la actualidad no es posible predecir con suficiente antelación cuándo va a ocurrir el fenómeno. Por esta razón se están haciendo mediciones de las deformaciones del volcán y de su actividad hidrotérmica, y se están realizando registros de la actividad sísmica que proviene de las entrañas del volcán, que permitan poner en marcha planes eficientes y adecuados para salvar a la población de un desastre.

Por otra parte, el volumen de hielo que contienen los glaciares del Popocatépetl es mayor de 17 millones de metros cúbicos. Estos glaciares se encuentran en la cara noroeste-norte y si se derritieran súbitamente, la corriente de agua probablemente se canalizaría por la barranca central y la barranca del Ventorrillo. En esta situación, Santiago Xalitzintla, San Nicolás de los Ranchos y San Pedro Benito Juárez podrían ser algunos de los poblados más afectados. En temporada de lluvias es de esperar que el flujo de lodo afecte una mayor superficie debido a que el suelo tiene menor capacidad para absorber o infiltrar agua por encontrarse saturado por las aguas. También podría llegar a provocar lluvia ácida en caso de una explosión violenta.

El primer ascenso registrado a este volcán fue hecho mucho antes de la época del Imperio mexica en 1289, por los tecuanipas. El segundo ascenso hecho por los españoles fue dirigido por Diego de Ordás en 1519, para conseguir azufre para su pólvora.

Hernán Cortés lo describió así: 
El Popocatépetl ha sido uno de los volcanes más activos de México. Desde 1354 se han registrado 18 erupciones. En 1927 ocurrió una erupción de consideración, para iniciar así un periodo de reposo. Después, el 21 de diciembre de 1994, tras varios años de inactividad, el volcán registró una explosión que produjo gas y cenizas que fueron transportados por los vientos dominantes a más de 25 km de distancia. Actualmente su actividad es moderada, pero constante, con emisión de fumarolas, compuestas de gases y vapor de agua, y repentinas e imprevistas expulsiones menores de ceniza y material volcánico. La última erupción violenta del volcán se registró en diciembre de 2000, lo que, siguiendo las predicciones de científicos, motivó la evacuación de miles de personas en las áreas cercanas al volcán. El 25 de diciembre de 2005 se produjo en el cráter del volcán una nueva explosión, que provocó una columna de humo y cenizas de tres kilómetros de altura y la expulsión de lava. Posteriormente en la mañana del 3 de junio de 2011, el Popocatépetl volvió a emitir grandes fumarolas sin causar daños. El 20 de noviembre de 2011 tuvo lugar una gran explosión que hizo temblar la tierra, escuchándose en las poblaciones cercanas a las laderas, pero sin mayor alteración. El volcán registró la mañana del 16 de enero de 2012 una fumarola de vapor de agua y ceniza, sin que esto represente riesgos para la población aledaña al coloso.

El 16 de abril de 2012 el CENAPRED (Centro Nacional Para la Prevención de desastres), elevó el semáforo de alerta volcánica de fase amarillo 2 a fase amarillo 3 debido a la gran actividad que se ha estado presentando, sin que hasta el momento represente un peligro grave para la sociedad.

A las 3:23 del 30 de abril de 2013, el volcán Popocatépetl arrojó fragmentos incandescentes a 800 metros del cráter sobre la ladera noreste, informó el Centro Nacional de Prevención de Desastres (CENAPRED).

El 12 de mayo de 2013, luego del fuerte estruendo que se sintió en la localidad de Atlixco, la Coordinación Nacional de Protección Civil de la Secretaría de Gobernación, informó un cambio en el semáforo de la alerta volcánica, de amarillo fase 2 a fase 3 debido al incremento en la actividad del Volcán , por lo que entró en acción el Plan Operativo Popocatépetl, A través de un comunicado, la SEGOB dio a conocer que en una reunión con el Comité Científico Asesor, en el Centro Nacional de Prevención de Desastres (CENAPRED), que debido a que las dos semanas anteriores se observaron explosiones, eventos vulcano-tectónicos, episodios de tremor y trenes de exhalaciones, el Comité concluyó por consenso, emitir la recomendación. Sin embargo el 2 de junio de 2013 el CENAPRED regresó el nivel de alerta a amarillo fase 2.

Los días 17 y 18 de junio el volcán registró varios eventos explosivos de mayor magnitud, registrando Fumarolas que alcanzaron los 4 km sobre el nivel del cráter y expulsiones de roca incandescente que alcanzaron las faldas en el lado Sur-Oeste del coloso. La alerta se mantuvo en amarillo fase 2.

El volcán entró en actividad el 7 de julio de 2013, lanzando ceniza claramente visible en poblaciones cercanas, la ceniza también alcanzó Ciudad de México, expulsando flujos piroclásticos e incandescencia. El semáforo volcánico se situó en amarillo fase 3.

El volcán registró una explosión el 22 de enero de 2019, lanzando material incandescente y ceniza, esta explosión se pudo sentir en zonas aledañas al volcán (zonas del estado de Puebla y el Estado de México) sin embargo el semáforo volcánico se situó en amarillo fase 2, el 9 y 18 de febrero el volcán lanzó fuertes fumarolas, las cenizas llegaron a Puebla y Tlaxcala, el 14 de febrero el volcán volvió a lanzar material incandescente y ceniza, el semáforo volcánico siguió en amarillo fase 2, el 22 de marzo de 2019 el volcán registró una fuerte explosión, nuevamente lanzando material incandescente acompañada de ceniza, llegó a quemar algunos pastizales cerca del volcán, el estruendo se llegó a escuchar en las zonas aledañas al volcán, el 26 y 27 de marzo el volcán nuevamente registró explosiones de material incandescente acompañada de lava, ceniza y gases tóxicos, llegó a quemar varios pastizales, que estaban en las faldas del volcán, el 28 de marzo de 2019 por la mañana lanzó fuertes fumarolas que llegaban a los 4 km sobre el nivel del cráter, ese mismo día el semáforo se situó en amarillo fase 3.

En 1994, los monasterios del siglo XVI, construidos en sus laderas, fueron declarados Patrimonio de la Humanidad por la Unesco.

Las erupciones volcánicas pueden ser precedidas por cambios en la actividad sísmica y vulcanomagnética, en la composición química de los gases, del agua de manantiales y algunas veces por deformación. Para hacer un pronóstico volcánico adecuado es necesario reconocer estas señales indicativas de una erupción y su temporalidad. Las erupciones del Popocatépetl que comenzaron el 21 de diciembre 1994, fueron precedidas por aumentos en los eventos sísmicos vulcanotectónicos (VT), cambios en temperatura y concentración de sulfatos y cloruros en el lago del cráter y en la pCO en los manantiales. También hubo un descenso del pH en algunos manantiales varios meses antes de la erupción. Los eventos sísmicos de periodo largo también aumentaron antes de algunas erupciones y los episodios de tremor armónico así como las anomalías magnéticas negativas antecedieron a la formación de domos y están ligadas al ascenso de magma. La energía sísmica acumulativa de los eventos sísmicos vulcanotectónicos muestra una aceleración en la tasa antes de las erupciones principales. Hubo precursores claros antes de las erupciones de diciembre a enero de 2001, como son las anomalías magnéticas negativas, correlacionadas con el incremento en la sismicidad, así como pequeños cambios en los manantiales. Estos cambios ocurrieron dos meses antes de la erupción. Adicionalmente, unos días antes se presentaron episodios de tremor armónico de gran amplitud y aumento en el flujo de SO, esto, junto con el análisis de los datos del RSAM permitió hacer una evaluación y pronóstico adecuado de la erupción de 2000.

A las 22:54 del 4 de noviembre ya era visible una gran columna de humo sobre el volcán la cual terminó a la 1:35 de la mañana, durante este periodo de tiempo se observaron diversas explosiones que contenían material incandescente, vapor de agua, pequeñas cantidades de ceniza a las 11:45&; declararon que el volcán había entrado en un estado eruptivo, pero que esas pequeñas erupciones estaban previstas en el nivel de alerta volcánica amarillo fase 2 por lo que no era necesario modificar el nivel de alerta. Durante esta actividad no hubo lesionados y ningún incidente ya que la ceniza y material incandescente cayó únicamente en el Paso de Cortés.

En la madrugada del 18 de abril de 2016, aproximadamente a las 2:15 horas (hora local) el volcán empezó a tener actividad. Primero empezaron las emisiones de ceniza seguidas por una serie de pequeñas erupciones. Posteriormente, alrededor de las 3:00 horas se dio una fuerte fase eruptiva seguida de una lluvia de material incandescente, que alcanzaron un radio de 1.6 kilómetros de distancia. Esto generó una fumarola que se extendió varios kilómetros, lo que ocasionó una lluvia de ceniza en ciudades cercanas.

El Popocatépetl es conocido por su alta actividad volcánica que se presenta comúnmente. Desde que se reactivó en 1994 hasta el momento ha presentado una serie de erupciones de las cuales la más violentas han sido la del año 2000, y la madrugada del 18 de abril de 2016. 
Otra erupción fue la del 16 de febrero de 2018 tras el sismo de 7.2 en la escala de Richter que azotó México. El volcán expulsó una gran fumarola de agua y ceniza que alcanzó los 700 metros de altura a las 18:24 hora local.

La última erupción registrada ha sido la del 11 de abril de 2020.

El volcán es uno de los más monitoreados del mundo y también es uno de los más peligrosos y que amenaza más de 26 millones de personas.

El Popocatépetl es un volcán activo, que actualmente se encuentra restringido el acceso a público en general, solo se permite el ascenso a profesionistas y personal de protección civil, previa identificación y elaboración de un permiso; especificando objetivos así como certificando la personalidad y capacidad del solicitante. Se mantiene un monitoreo continuo de la actividad volcánica, para poder alertar y dar protección a los habitantes de poblados vecinos.

Para los amantes de la naturaleza, en el Parque Nacional Iztaccíhuatl-Popocatépetl se pueden realizar diversas actividades como ciclismo de montaña, deportes extremos como alpinismo en el volcán Iztaccíhuatl, o practicar senderismo, donde es posible apreciar bellos parajes y bosques donde habitan venados de cola blanca, conejos, zorrillos, ardillas. La mejor temporada para visitar el lugar es de noviembre a marzo.

Cuenta la leyenda que en tiempos prehispánicos vivieron Itzaccíhuatl, una princesa tlaxcalteca de gran belleza, y Popocatépetl, un guerrero apuesto y valiente. antes de partir a la guerra en la que los tlaxcaltecas se encontraban inmersos contra sus enemigos acérrimos, los aztecas. Popocatépetl pidió al cacique de su pueblo la mano de la princesa Iztaccíhuatl. Este se la concedió a condición de que volviera sano y salvo de la guerra para desposarla. Así, el guerrero partió a la batalla mientras que la princesa esperaba el retorno de su amor. Sin embargo, la lengua de un celoso rival de Popocatéptl medió de mala fe engañando a la princesa e informándole que su amado había muerto en combate. Arrastrada por el desconsuelo y el quebranto, víctima del engaño, ella murió de tristeza por la pérdida de su amado. Poco tiempo después, Popocatépetl regresó victorioso de la batalla dispuesto a tomar matrimonio con Iztaccihuatl; sin embargo, a su llegada recibió la funesta noticia de su fallecimiento. Durante varios días y noches, cuentan que el joven vagó por las calles hasta encontrar la manera de honrar el gran amor que ambos se profesaban, y fue así como ordenó erigir una gran tumba bajo el sol amontonando 10 cerros para levantar una enorme montaña. Una vez construida, tomó el cuerpo inerte de su princesa, y recostándola sobre la cima de la montaña, la besó por última vez para después, con una antorcha humeante en mano, arrodillarse para velar su sueño eternamente. Desde entonces, permanecen juntos, uno frente a otro. Con el tiempo, la nieve cubrió sus cuerpos, que se convirtieron en dos enormes volcanes y que permanecerían inmutables hasta el final de los tiempos.

Otra leyenda relacionada con este volcán es la relacionada con uno de los cariñosos apodos que la población de las localidades cercanas a otorgado a la montaña. A este volcán también se le conoce como "Don Goyo", apócope de Gregorio, pues se dice que, de vez en vez, un anciano se aparece rumbo a “alguna parte” en los diferentes poblados de la zona y se hace llamar Don Gregorio o Gregorio Chino. La gente del lugar asegura que este anciano es la personificación del volcán que viene a asegurarse de que las personas que habitan la zona obren de buena fe y muestren respeto al volcán para que, de ser así, la buena fortuna les sonría.


Franco-Ramos, O., Vázquez-Selem, L., Zamorano-Orozco, J. J., Villanueva-Díaz, J., 2017, Edad, dinámica geomorfológica y tipología de barrancas en el sector norte del volcán Popocatépetl, México: Boletín de la Sociedad Geológica Mexicana, 69(1), 1-19. 




</doc>
<doc id="16841" url="https://es.wikipedia.org/wiki?curid=16841" title="Polimastia">
Polimastia

La polimastia es la existencia de más de dos mamas en los mamíferos. Se dan casos esporádicos en los seres humanos.

Cada mama "de más" se denomina "mama supernumeraria" y tiene una situación anormal, aunque casi siempre se localiza dentro de una línea imaginaria situada a cada lado del cuerpo humano, desde el vértice de la axila hasta la cara lateral del labio mayor de la vulva (base del escroto en el varón) del mismo lado. La presencia de pezones supernumerarios se conoce como politelia.


</doc>
<doc id="16842" url="https://es.wikipedia.org/wiki?curid=16842" title="Polaquiuria">
Polaquiuria

La polaquiuria es un signo urinario, componente del síndrome miccional, caracterizado por el aumento del número de micciones (frecuencia miccional) durante el día, que suelen ser de escasa cantidad y que refleja una irritación o inflamación del tracto urinario. Suele acompañarse de nicturia y de otros síntomas del síndrome miccional como tenesmo vesical y disuria.

Se habla de polaquiuria nocturna cuando el aumento anormal del número de micciones se produce exclusivamente por la noche.

Este término no debe confundirse con poliuria, que es la micción muy abundante.

La causa más frecuente de polaquiuria suele ser una infección urinaria, sobre todo en mujeres. Algunos medicamentos como la difenhidramina pueden causar polaquiuria. También puede presentarse como síntoma de irritaciones de órganos adyacentes al tracto urinario como apendicitis, vulvovaginitis, endometritis o gastroenteritis.

Durante la gestación en la mujer se considera como signo normal, aunque se debe descartar la existencia de infección urinaria.

Lo más frecuente es que este signo clínico sea debido a enfermedades originarias de las vías urinarias. Pero en ocasiones se genera por la existencia de un tumor benigno o maligno próximo a la vejiga que la comprima, como es el caso de los tumores de ovario en las mujeres.

En los hombres, especialmente en mayores de 50 años, son menos frecuentes las infecciones urinarias, por lo que no hay que descartar una hiperplasia benigna de próstata o un cáncer de próstata.

El tratamiento de la polaquiuria dependerá de la causa que la origine. Por ejemplo: si es por una infección de la vía urinaria, se tratará con un antibiótico.



</doc>
<doc id="16846" url="https://es.wikipedia.org/wiki?curid=16846" title="Pila solar">
Pila solar

Las pilas solares producen electricidad por un proceso de conversión fotoeléctrica. La fuente de electricidad es una sustancia semiconductora fotosensible, como un cristal de silicio al que se le han añadido impurezas. Cuando la luz incide contra el cristal, los electrones se liberan de la superficie de éste y se dirigen a la superficie opuesta. Allí se recogen como corriente eléctrica. Las pilas solares tienen una vida muy larga y se utilizan sobre todo en los aviones como fuente de electricidad para el equipo de a bordo.


</doc>
<doc id="16847" url="https://es.wikipedia.org/wiki?curid=16847" title="Ir de picos pardos">
Ir de picos pardos

Ir de picos pardos es una expresión equivalente a ir de parranda o salir de fiesta.

Esta expresión se ha venido utilizando desde hace siglos:

En el Renacimiento el término empezó a utilizarse porque las mujeres llevaban una falda que era un lienzo de forma cuadrada, con una abertura en el centro. Esta abertura se ajustaba a la cintura y la falda resultante tenía cuatro picos. En el Quijote se habla de la condesa Trifaldi, y explica Cervantes que lleva una falda con tres picos en vez de cuatro.

El Diccionario de la lengua española de la Real Academia Española, en su 3.ª edición de 1791, decía que «"Andarse, o irse, de picos pardos" es frase con que se da a entender que alguno, pudiendo aplicarse a cosas útiles y provechosas, se entrega a las inútiles e insustanciales, por no trabajar y por andarse a la briba».

Montoto, en "Un paquete de cartas", escribe: «Los picos o los mantos con picos pardos fueron, según leí no recuerdo en cuál autor, distintivo de las mujeres de vida airada, mozas de partido, etc. En tiempos pasados, las tales tenían que vestir como se les ordenaba. Según las Ordenanzas de la Casa Pública de Sevilla, no habían de usar vestidos talares, ni sombrillas, ni guantes, sino una mantilla para los hombros, corta y encarnada».

Carlos III impuso a las prostitutas la obligación de distinguirse mediante sayas de color pardo cortadas por los bajos en picos, aunque también se dice que "Ir de picos pardos" tiene que ver con las costumbres ligeras de los estudiantes del Siglo de Oro y sus acompañantes. Ellas, para identificar su condición de prostitutas, llevaban un cintillo pardo en el borde de la falda.

Ya en el siglo XX, la frase se comenzó a usar por ambos sexos, como irse de parranda con personas del otro sexo.

En la actualidad, se utiliza la expresión "Ir de Picos Pardos" para referirse a ir por ahí, salir de fiesta y pasarlo bien. De hecho, en Zaragoza existe un blog de rutas llamado "DE PICOS PARDOS ZARAGOZA" dónde se proponen diferentes rutas temáticas para irse de picos pardos por Zaragoza. Una manera de adaptar esta expresión antigua a la lengua actual de manera moderna.


</doc>
<doc id="16848" url="https://es.wikipedia.org/wiki?curid=16848" title="Phytodiniaceae">
Phytodiniaceae

Familia de organismos unicelulares del Orden de los Phytodinales de la división Dinophyta, clase Dinophyceae.


</doc>
<doc id="16849" url="https://es.wikipedia.org/wiki?curid=16849" title="Kengyilia">
Kengyilia

Kengyilia es un género de plantas de la familia de las poáceas, compuesto por varias especies de hierbas perennes nativas del desierto de Gobi y la provincia de Qinghai, en el norte de China. Fuertemente xerofítica, son unas de las pocas plantas que crecen en las condiciones de extrema aridez de la región, sobre suelos pétreos y con precipitaciones infrecuentes.
Crecen en matas de tallos erectos y glabros de hasta 70 cm de altura, sin ramificaciones visibles, brotando de dos o tres nodos basales. No presentan una roseta en la base; las hojas son lineales, estrechas, de unos 6 mm de ancho y hasta 19 cm de largo, variando en forma según las especies.

Son hermafroditas, presentando flores de ambos géneros en la espiga única; esta alcanza 12 cm de largo y 8 mm de ancho, y es erecta o curvada, con varias espigüelas y nodos internos densamente pilosos. Las espigüelas alcanzan los 20 mm de largo, y son ovoides, verde-amarillentas, pubescentes, con el eje principal protuberando del racimo floral. Presenta dos brácteas basales, más cortas que las espigüelas, glabras y apuntadas o aserradas. Las flores presentan tres estámenes, y anteras de dos a tres mm de longitud; el ovario es piloso.

La polinización es probablemente anemófila; los frutos son cariópsides de entre 6 y 7 mm de largo, de color pardo oscuro, fusiformes, acanalados longitudinalmente, con hilum marcado en la parte superior.
El nombre del género fue otorgado en honor de Yi-Li Keng, agrostólogo experto en Triticeae. 
Son hexaploides, conteniendo 42 juegos de cromosomas.



</doc>
<doc id="16850" url="https://es.wikipedia.org/wiki?curid=16850" title="Peso (moneda de Chile)">
Peso (moneda de Chile)

El peso es la moneda de curso legal de Chile desde el año 1975. Su código es CLP, su número ISO 4217 es 152, y su símbolo, común con el de monedas que llevan el mismo nombre, es $.

Fue establecido en 1817, junto con la independencia del país, y en 1851 se estableció el sistema decimal en el peso, que quedó constituido por 100 centavos. Se mantuvo como la moneda de curso legal en Chile hasta el 31 de diciembre de 1959, cuando fue reemplazado por el escudo.

Por medio del decreto ley 1123 de 1975, el peso fue retomado como unidad monetaria a partir del 29 de septiembre de dicho año, con una tasa de conversión de un peso por cada mil escudos. La subdivisión en centavos se mantuvo hasta el 1 de enero de 1984, cuando la contabilidad se empezó a llevar en pesos enteros, eliminando los centavos.

Históricamente, ha sido fabricado por la Casa de Moneda de Chile (1743) y regulado por el Banco Central de Chile (1925), encargado de la emisión de monedas y billetes.

Pese a que la adopción del peso en reemplazo del real colonial se remonta a 1817, con el inicio del periodo de la Patria Nueva, se siguió utilizando el sistema español de moneda en que 8 reales equivalían a 1 peso y 2 pesos a 1 escudo. Esta costumbre persistió en Chile hasta 1851, cuando se adoptó el sistema decimal, en que 1 peso quedó constituido por 10 décimos o 100 centavos.

En 1925 la circulación de dinero fue controlada por el entonces recién creado Banco Central de Chile. La Ley Monetaria de dicho año estableció al peso con un contenido equivalente de 6 peniques de oro y que 10 unidades de peso constituirían «un cóndor»; se estableció también que en toda moneda de 10 pesos o más se estamparía en letras y cifras su valor en pesos, y en letras de menor dimensión su equivalencia en cóndores. En 1955, como consecuencia de la inflación, se dispuso que todas las obligaciones se pagaran en pesos enteros, sin centavos.

En el contexto de una política de saneamiento de la economía nacional y control inflacionario emprendida por el gobierno de Jorge Alessandri, entre 1960 y 1975 el peso fue reemplazado por el escudo (Eº). La tasa de conversión fue de 1000 pesos por 1 escudo.

La hiperinflación existente durante el gobierno de Salvador Allende, que llegó a cifras de alrededor del 600 y 800 %, fue una de las principales causas de la crisis económica que enfrentó su administración. Tras el Golpe de estado de 1973, una de las reformas económicas establecidas por la dictadura de Pinochet fue la reconversión al peso. Así, el 29 de septiembre de 1975 se estableció un cambio de 1000 escudos por 1 peso mediante el decreto ley 1123.

Las primeras monedas emitidas bajo el nuevo sistema fueron las de 1, 5, 10, 50 centavos, y 1 peso; más tarde se añadieron los valores de 5 y 10 pesos. Las monedas de 1, 5, 10 y 50 centavos tuvieron una emisión limitada debido a las crecientes devaluaciones que ocurrieron durante la década de 1980.

Entre 1979 y 1982, la tasa de cambio se mantuvo fija en 39 pesos por dólar estadounidense; sin embargo, la presión inflacionaria hizo que la economía no pudiese aguantar dicha fijación y en 1982 se comenzó lentamente a devaluar la moneda nacional. Ya en 1984, el precio estaba a 100 pesos por dólar.

Hacia la década de 1990, la inflación hizo desaparecer tanto las monedas de centavos como los billetes de 5, 10, 50 y 100 pesos, reemplazados por monedas. Hacia fines de dicha década surgieron, además, el billete de 2000 y el de 20 000, junto con una moneda de 500 pesos, que eliminó el billete de homóloga denominación. Cambios posteriores en los billetes permitieron la introducción de símbolos para su reconocimiento por personas con problemas visuales y se incorporaron billetes de polímero, en reemplazo del papel, para los billetes de 1000, 2000 y 5000.

Todas las monedas acuñadas a partir de 1975, cuando fue adoptado el peso como moneda de curso legal, tienen validez en el país. Sus denominaciones y diseños han cambiado a lo largo del tiempo: inicialmente, solo se emitieron monedas de 1, 5, 10 y 50 centavos y 1 peso; posteriormente, se descontinuaron los centavos y se añadieron las monedas de 5, 10, 50, 100 y 500 pesos en reemplazo de los billetes de iguales denominaciones. La moneda de 500 pesos fue la primera moneda bimetálica producida en el país.

En 2009 se presentó un proyecto para crear monedas de 20 y 200 pesos, pero fue rechazado. El 26 de abril de 2016, el Banco Central le propuso al Ministerio de Hacienda el envío de un proyecto al Congreso para eliminar las monedas de 1 y 5 pesos debido al poco uso y a su alto costo de fabricación. El 2 de mayo siguiente, se presentó a la presidenta Michelle Bachelet un proyecto para una nueva familia de monedas, que entraría en circulación el segundo semestre de 2017. En agosto de dicho año, el Congreso despachó un proyecto de ley que incluyó la eliminación de las monedas de 1 y 5 pesos. Desde el 1 de noviembre de 2017, las monedas de 1 y 5 pesos dejaron de emitirse por el Banco Central y entró en vigencia el redondeo para los pagos en efectivo.

El 10 de octubre de 2018 el Banco Central anunció el inicio del retiro de circulación de las antiguas monedas de 100 pesos, fabricadas entre 1981 y 2000; sin embargo, siguen vigentes como medio de pago y la medida implica principalmente el retiro de la moneda por parte de las instituciones financieras para disminuir su convivencia con las monedas actuales.

Aparte de las denominaciones de uso regular, la Casa de Moneda de Chile ha acuñado monedas conmemorativas, que tienen valor vigente según la ley, en las siguientes oportunidades:

En febrero de 2010, se descubrió que parte de la producción de monedas de 50 pesos, fabricadas por la Casa de Moneda en 2008, tenía la leyenda «REPÚBLICA DE » en lugar de «REPÚBLICA DE CHILE» en el anverso. Esto, sumado a problemas anteriores de gestión, le costó el puesto a Gregorio Íñiguez, gerente general de dicha entidad.

Al restablecerse el peso como moneda oficial en 1975, se introdujeron billetes de 5 y 10 pesos —producto de la inflación, al año siguiente fueron reemplazados por monedas de las mismas denominaciones— además de los billetes de 50 y 100 pesos; en los años siguientes, estos últimos también fueron reemplazados por monedas, y aparecieron los billetes de 500, 1000 y 5000 pesos. A fines de la década de 1980, se imprimieron únicamente billetes de 500, 1000, 5000 y 10 000 pesos —este último fue introducido en 1989—.

En 1997 entró en circulación el billete de 2000 pesos, al año siguiente el de 20 000 pesos y dos años después dejó de circular el de 500 pesos, reemplazado por una moneda. Durante 2009, los billetes en producción eran los de 1000, 2000, 5000, 10 000 y 20 000 pesos; en dicho año comenzó la introducción de una nueva serie de billetes. En la actualidad, muchos de estos billetes siguen en circulación, aunque es cada vez menos frecuente encontrarlos. Los billetes con denominaciones menores a 1000 pesos no están en circulación, pero siguen siendo legalmente válidos en todo el país y mantienen su valor nominal original.

El Banco Central de Chile comenzó en el segundo semestre de 2009 la producción de una nueva serie de billetes para conmemorar el bicentenario del país. Si bien sus denominaciones y los personajes retratados no se modificaron, se cambiaron los tamaños, que variaron en función de la denominación, y se añadieron características de seguridad para evitar su falsificación. Tres de los billetes de la nueva serie son de polímero y dos, de papel de algodón.

El diseño fue unificado, presentando el rostro de cada personaje en el anverso junto al antú —representación mapuche del Sol— y un corte transversal del corazón de un copihue, la flor nacional. En tanto, en el reverso de cada billete, se incorporaron paisajes de diversos parques nacionales junto a una especie animal nativa:

Expresiones coloquiales antiguas que han sobrevivido en el español chileno hasta nuestros días son: «no tengo ni un veinte», en referencia a la moneda de veinte centavos que circuló entre 1907 y 1941, y «no tengo ni un cobre», referido a las monedas de un peso de cobre que circularon entre 1942 y 1959.

Además, en el español chileno coloquial se usan ciertos sobrenombres para algunas monedas y billetes —dos de estos últimos son ocasionalmente denominados por el nombre del personaje retratado; sin embargo ninguno de estos apodos es usado por gran parte de la población—:

En contextos muy informales, para referirse a la suma de un millón de pesos también se utilizan las expresiones un guatón y un palo.

Aunque suene extraño, a veces en el argot popular, y preferentemente en el coa, se da un valor diferente a los billetes y montos, dividiéndolos por mil: al billete de mil pesos se le dice «un peso»; al de dos mil, «dos pesos»; al de cinco mil, «cinco pesos»; al de diez mil, «diez pesos»; y al de veinte mil, «veinte pesos». La suma de cien mil pesos es denominada «cien pesos», etcétera.







</doc>
<doc id="16857" url="https://es.wikipedia.org/wiki?curid=16857" title="Pentodo">
Pentodo

Se denomina pentodo a la válvula termoiónica formada por cinco electrodos. Muy parecida funcionalmente al triodo, tiene tres rejillas en vez de una sola. Fue inventado por los neerlandeses Gilles Holst y Bernardus Dominicus Hubertus Tellegen, de la empresa Philips en 1926.
La razón para añadir una tercera rejilla a la válvula de cuatro electrodos o tetrodo es que aunque con la segunda rejilla se aumentaba la amplificación, había un inconveniente: se producía una emisión secundaria en la placa. Los electrones liberados en esta emisión secundaria son captados por la rejilla pantalla (positiva), introduciendo una gran distorsión en las señales amplificadas.

Es por ello que, para evitar esta emisión secundaria, se añadió una nueva rejilla, llamada supresora que, adecuadamente polarizada (más negativa que la placa), elimina este efecto indeseado, repeliendo los electrones secundarios nuevamente hacia el ánodo. En muchos pentodos la rejilla supresora va unida internamente al cátodo.

La segunda rejilla (pantalla) hace que funcione mejor en frecuencias más altas y la tercera (supresora) elimina la distorsión, por emisión secundaria.

Existen pentodos en los que se busca una gran sensibilidad a las variaciones de tensión en las rejillas, no sólo la de control. Esto permite utilizar la rejilla supresora como segunda rejilla de control, por ejemplo, en mezcladores. El tipo 5636, creado por Radio Corporation of America es un pentodo de corte abrupto. También son conocidos como pentodos de corte rápido o neto (Sharp cutoff).

En los equipos receptores de radio suelen manejarse señales de muy variadas intensidades. En una válvula de ganancia fija, dentro de los límites normales de uso, variaciones grandes de señales producirán diferencias notables de volumen y hasta problemas de saturación o corte. Las diferencias de volumen fueron corregidas con circuitos que variaban la polarización de las válvulas de frecuencia intermedia y hasta de la etapa de alta. Al principio de los conoció como "control automático de ganancia" (C.A.G) o "control automátivo de volumen" (C.A.V.), pero más tarde se adoptó el nombre de "control automático de sensibilidad" (C.A.S.). No obstante, es deseable una válvula cuyo corte esté muy alejado de la tensión 0 de la grilla de control y que mantenga una salida más o menos uniforme con respecto a las señales muy intensas o débiles. Así nació el pentodo de corte alejado, muy variable o supercontrol. Estas válvulas para amplificación de tensión de pequeña señal llegan al corte con una tensión de grilla muy negativa, que correspondería a una señal de radio intensa. Antes de ese límite, la construcción especial de la grilla hace que la ganancia para valores muy negativos de grilla sea baja en comparación con valores menos negativos. Por lo general el bobinado de la grilla no es uniforme con respecto a la separación de vueltas. Al tener una amplificación mayor con valores menos negativos de grilla e inversamente para valores muy negativos, se logra que la tensión de salida se mantenga constante frente a las variaciones de intensidades. En inglés suele denominarse "remote cutoff". Como un tipo intermedio existen, también, válvulas de corte semi-alejado (semi-remote cutoff).

Los pentodos son particularmente apreciados en las etapas de amplificación de potencia de los transmisores fijos de radio profesionales y de radioaficionados, donde se usan por varias razones:




</doc>
<doc id="16858" url="https://es.wikipedia.org/wiki?curid=16858" title="Patricia Kaas">
Patricia Kaas

Patricia Kaas (Forbach, 5 de diciembre de 1966) es una cantante francesa que representó a su país en el Festival de la Canción de Eurovisión en 2009 y fue candidata a Marianne (personificación de la República) en 2000.

Creció en Stiring-Wendel en una familia de siete hermanos, de madre alemana y padre minero francés. Con la ayuda de sus padres, Patricia empieza a dar conciertos a la edad de 8 años. Cantaba las canciones de Sylvie Vartan, Claude François y Mireille Mathieu, y también canciones estadounidenses como "New York, New York".

A los 13 años firma un contrato con el cabaret alemán Rumpelkammer de Sarrebruck, donde cantará todas las noches de sábado durante 7 años. En 1985 es "descubierta" por el actor Gérard Depardieu, quien decide producir la primera canción de Kaas, "Jalouse", que resulta un mediano fracaso.

Dos años después se encontrará con Didier Barbelivien, que le escribirá la canción "Mademoiselle chante le blues", que se graba en 1988. A partir de ahí Patricia venderá millones de discos entre 1988 y 1999 gracias a su carisma, su voz excepcional y sus buenos compositores (en particular Jean-Jacques Goldman y Pascal Obispo).

En 1993 y 1994 realizó una gran gira por el mundo "Tour de Charme" de la cual se editó un CD. En 2002 partició junto a Jeremy Irons en un film de Claude Lelouch ("And Now... Ladies and Gentlemen"), su primera experiencia en la gran pantalla y cuya banda sonora está incluida en su álbum "Piano Bar by Patricia Kaas", un homenaje a la canción francesa.

Patricia fue la representante de Francia en el Festival de la Canción de Eurovisión 2009 en Moscú con la canción "Et s'il fallait le faire". La gala del Festival se celebró el 16 de mayo de aquel año, día en el que Kaas nunca canta por ser en aniversario del fallecimiento de su madre.

Su actuación fue una de las más aplaudidas de la noche. Siendo la única artista ovacionada tres veces de manera espontánea por el público en toda la historia del festival de Eurovisión. Quedó clasificada en la 8ª posición de la Final del concurso con 107 puntos.


 Live álbumes




</doc>
<doc id="16859" url="https://es.wikipedia.org/wiki?curid=16859" title="Parte (derecho)">
Parte (derecho)

En derecho, una parte, es cada una de las posiciones que puede haber enfrentadas en un litigio (juicio, arbitraje o conciliación) o que celebran un contrato o un tratado internacional.

En derecho procesal, es la persona o conjunto de personas que actúa en el proceso judicial defendiendo su derecho o interés frente a un conflicto actual sometido a la decisión de un tribunal de justicia.

Las partes directas son el demandante y el demandado, aunque en algunos procedimientos judiciales pueden recibir un nombre diverso.

Las partes indirectas son los terceros, que intervienen defendiendo intereses armónicos, disimiles o independientes de las partes principales (como los "amicus curiae"). En Derecho penal, por ejemplo, existe la acusación particular o la acusación popular que pueden actuar independientemente del ministerio público).

En los actos judiciales no contenciosos, llamados impropiamente también actos de jurisdicción no contenciosa o voluntaria, la parte recibe el nombre de interesado o solicitante, porque no hay una contra parte contra la cual entablar conflicto, sino que el procedimiento comienza con una solicitud ante el tribunal respectivo.

En el derecho de los contratos las partes son las personas físicas o jurídicas que dan su consentimiento y celebran el contrato.

Son una parte esencial del contrato, hasta el punto de que en el caso de cambiar alguna de las partes se produce la extinción del contrato, y la novación del mismo, por medio de la subrogación de una persona nueva en el lugar en el que se encontraba alguna de las anteriores.

Por otro lado, una de las partes puede estar formada por una o más personas, siempre y cuando estas actúen de forma conjunta, ya sea de forma mancomunada o solidaria.



</doc>
<doc id="16860" url="https://es.wikipedia.org/wiki?curid=16860" title="Parque">
Parque

Un parque (del francés parc y del inglés park) es un espacio natural o semi-natural que puede estar situado en el interior de una población y se utiliza como prado, jardín o arbolado para esparcimiento y recreo de los ciudadanos.<br>
Las grandes extensiones protegidas por el estado, como parques naturales y nacionales se dedican a la protección de la vida salvaje y los hábitats naturales.

También se conocen como parques otros recintos, protegidos pública o privadamente, donde se celebran actividades lúdicas o de exhibición, como un Parque de atracciones o un Parque zoológico.

Los parques que se mantenían en la antigüedad de forma privada para disfrute de sus propietarios están en la actualidad abiertos al público, como los jardines de Versalles, Jardines del Retiro de Madrid o los antiguos parques de caza de nobles y reyes, como el bosque de Fontainebleau.<br> 
Muchas casas de campo en el Reino Unido e Irlanda todavía tienen parques de este tipo, los cuales desde el siglo XVIII han sido a menudo ajardinados por estética. Normalmente son una mezcla de praderas abiertas con árboles dispersos y zonas boscosas y suelen estar delimitados por altas vallas. La zona inmediata a la casa es el jardín y, en algunos casos, esta también presenta praderas y árboles dispersos, sin embargo lo que diferencia básicamente el parque del jardín en una casa de campo es que el parque está habitado por animales, mientras que estos están excluidos del jardín.

Los parques públicos, dentro de las grandes ciudades juegan un rol muy importante en el mejoramiento de la calidad de vida de los habitantes. La OMS establece el límite mínimo de jardines públicos o áreas verdes, para las ciudades, en 9 m²/habitante.

También se usa el término parque industrial para referirse a un área urbana destinada a la instalación de industrias.

La palabra "parque" también puede designar un conjunto de vehículos - parque móvil -, armas - parque de artillería - o un conjunto de empresas relacionadas con la tecnología: parque tecnológico

En fitogeografía, un parque fitogeográfico es un tipo de formación vegetal formado por una comunidad de entre una y no más de tres especies arbóreas, constituyendo así un tipo de ecosistema.



</doc>
<doc id="16861" url="https://es.wikipedia.org/wiki?curid=16861" title="Pares craneales">
Pares craneales

Los pares craneales, también llamados nervios craneales, son doce pares de nervios que surgen directamente del cerebro o a nivel del tronco del encéfalo para distribuirse a través de los agujeros de la base del cráneo en la cabeza, cuello, tórax y abdomen. La Nomenclatura Anatómica Internacional incluye al nervio terminal como nervio craneal, a pesar de ser atrófico en los humanos y estar estrechamente relacionado con el nervio olfatorio.

Los nervios craneales tienen un origen aparente que es el lugar donde el nervio sale o entra en el encéfalo. El origen real es distinto de acuerdo a la función que cumplan. Las fibras de los pares craneales con función motora (eferente) se originan de grupos celulares que se encuentran en la profundidad del tallo encefálico (núcleos motores) y son homólogas de las células del asta anterior de la médula espinal. Las fibras de los pares craneales con función sensitiva o sensorial (aferente) tienen sus células de origen (núcleos de primer orden) fuera del tallo encefálico, por lo general en ganglios que son homólogos de los de la raíz dorsal de los nervios raquídeos. Los núcleos sensitivos de segundo orden se encuentran en el tallo encefálico.

Los núcleos de donde parten los pares craneales se ubican en una región generalizada conocida como tegmentum que recorre el tronco del encéfalo. 

Según su aspecto funcional, se agrupan así:


Los componentes del sistema nervioso sensorial de la cabeza se derivan de la cresta neural y de una población de células embrionarias que se desarrolla en las proximidades, las placodas sensoriales craneales (las placas olfatoria, del cristalino, ótica, trigémino, epibranquial y paratimpánica). Los nervios craneales se forman a partir de la contribución de dos poblaciones de células embrionarias especializadas, la cresta neural craneal y las placodas ectodérmicas. Los pares craneales de doble origen se resumen en la siguiente tabla :

Contribuciones de las células de la cresta neural y placodas a los ganglios y los nervios craneales

Abreviaturas: NC, nervio craneal; m, nervio puramente motor; mix, nervio mixto (sensorial y motor);CCN, células de las crestas neurales; CN, cresta neural; AP, arco faríngeo (branquial); r, rombómero; s, nervio puramente sensorial. * No se conoce ningún ganglio del nervio accesorio. La parte craneal del nervio accesorio envía ramas ocasionales al ganglio superior del nervio vago.



</doc>
<doc id="16862" url="https://es.wikipedia.org/wiki?curid=16862" title="OS/400">
OS/400

OS/400 es un sistema operativo utilizado en la línea de miniordenadores AS/400 (actualmente servidores IBM_eServer iSeries) de IBM.

El sistema operativo OS/400 apareció en el mercado en 1988 al mismo tiempo que la línea de miniordenadores AS/400, llamados en la jerga de IBM, servidores "midrange". El desarrollo conjunto de hardware y sistema operativo da como resultado un intenso aprovechamiento de los recursos
de aquel.

Entre sus características iniciales más destacadas podríamos señalar la integración a nivel del propio sistema de la base de datos DB2/400, que no solo se ofrece como soporte para los datos de aplicaciones y usuarios, sino también como un almacenamiento estructurado para todos los objetos del sistema operativo, incluyendo un sistema de librerías mononivel. Como es usual en los sistemas medios, tiene la posibilidad de generar "subsistemas", es decir asignar recursos (memoria, procesadores, etc) a funciones o entornos concretos, permitiendo un control más profundo de los mismos que el existente en otras arquitecturas.

Tiene subsistemas incorporados que le permiten ejecutar aplicaciones de los Sistemas/3x de IBM en el hardware del AS/400 de forma nativa o bien modificado. En las últimas versiones también pueden ejecutarse aplicaciones AIX de manera nativa e instalarse GNU/Linux en particiones lógicas (LPAR).

Aun tratándose de un sistema operativo que no incorpora una interfaz gráfica nativa, el producto bajo licencia iSeries Access incluye "iSeries Navigator" con versiones para Windows y para web, que permite la administración del sistema y de la base de datos mediante una interfaz gráfica. También incluye administración web para el servidor web "Apache" y para el servidor de aplicaciones "Websphere Application Server".

En estos momentos OS/400 se conoce como i5 OS



</doc>
<doc id="16864" url="https://es.wikipedia.org/wiki?curid=16864" title="Oveja Dolly">
Oveja Dolly

La oveja Dolly (5 de julio de 1996-14 de febrero de 2003) fue el primer mamífero clonado a partir de una célula adulta. Sus creadores fueron los científicos del Instituto Roslin de Edimburgo (Escocia), Ian Wilmut y Keith Campbell. Su nacimiento no fue anunciado sino hasta siete meses después, el 22 de febrero de 1997.

Dolly fue en realidad una oveja resultado de una combinación nuclear desde una célula donante diferenciada a un óvulo no fecundado y anucleado (sin núcleo). La célula de la que venía Dolly era una ya diferenciada o especializada, procedente de un tejido concreto, la glándula mamaria, de un animal adulto (una oveja finlandesa-Dorset de seis años), lo cual suponía una novedad. Hasta ese momento se creía que solo se podían obtener clones de una célula embrionaria, es decir, no especializada. Cinco meses después nacía Dolly, que fue el único cordero resultante de 277 fusiones de óvulos anucleados con núcleos de células mamarias.

Dolly vivió siempre en el Instituto Roslin. Allí fue cruzada con un macho Welsh Mountain para producir seis crías en total. De su primer parto nace Bonnie, en abril de 1998. Al año siguiente, Dolly produce mellizos: Sally y Rosie, y en el siguiente parto trillizos: Lucy, Darcy y Cotton. En el otoño de 2001, a los cinco años, Dolly desarrolla artritis comenzando a caminar dolorosamente, siendo tratada exitosamente con pastillas antiinflamatorias.

El 14 de febrero de 2003, Dolly fue sacrificada debido a una enfermedad progresiva pulmonar. Fue un animal de la raza finlandesa-Dorset, cuyos individuos tienen una expectativa de vida de cerca de 11 a 12 años. Sin embargo, Dolly vivió solo seis años y medio. La necropsia mostró que tenía una forma de cáncer de pulmón llamada Jaagsiekte, que es una enfermedad de ovejas causada por el retrovirus JSRV. Los técnicos de Roslin nunca pudieron certificar que haya conexión entre esa muerte prematura y el ser clonada, pues otras ovejas del mismo rebaño sufrieron y murieron de la misma enfermedad. Tales enfermedades pulmonares son un particular peligro en las estabulaciones internas, como fue la de Dolly por razones de seguridad.

Sin embargo, algunos han especulado que era ciega , debido a sus pezuñas torcidas. Había un factor agravante en el deceso de Dolly y era que, ya al nacer, tenía una edad genética de seis años, la misma edad de la oveja de la cual fue clonada. Una base para esta idea fue el hallazgo de sus telómeros cortos, que son generalmente el resultado del proceso de envejecimiento. Sin embargo, el Roslin Institute ha establecido que los controles intensivos de su salud no revelaron anormalidad alguna en Dolly, que pudieran hacer pensar en envejecimiento prematuro. Los restos disecados de la oveja Dolly están expuestos en el Museo Real de Escocia.




</doc>
<doc id="16865" url="https://es.wikipedia.org/wiki?curid=16865" title="Osmolaridad">
Osmolaridad

La osmolaridad es la medida para expresar la concentración total de sustancias en disoluciones usadas en medicina. El prefijo "osmo-" indica la posible variación de la presión osmótica en las células, que se producirá al introducir la disolución en el organismo. En la osmolalidad(véase que es diferente a osmolaridad), la concentración queda expresada como osmoles por kilogramo de agua, referentes también a algunas sustancias específicas.

La concentración osmótica, normalmente conocida como osmolaridad, es la medición de la concentración de solutos, definida como el número de osmoles (Osm) de un soluto por litro (L) de solución (osmol/ L or Osm/L). La osmolaridad de una solución esta usualmente expresada en Osm/L (pronunciado Osmolar), de la misma manera que la molaridad de una solución está expresada como "M" (pronunciado Molar"). Mientras que la molaridad mide el número de moles de un soluto por unidad de volumen de una solución; la osmolaridad mide el número de osmoles de soluto participantes por unidad de volumen de una solución.

La osmolaridad normal de los fluidos corporales por litro de solución, es similar a una solución al 0,9% de NaCl.

Una solución o disolución de NaCl 0,1 M daría 0,1 moles de Na y 0,1 moles de Cl por litro, siendo su osmolaridad 0,2. Si esa disolución se inyecta a un paciente sus células absorberían agua hasta que se alcanzase el equilibrio, provocando una variación en la presión sanguínea.



</doc>
<doc id="16866" url="https://es.wikipedia.org/wiki?curid=16866" title="Osa Menor">
Osa Menor

La Osa Menor ("ursa" en latín) es una constelación del hemisferio norte. Comparte el mismo nombre que la Osa Mayor, debido a que su cola se asemeja al mango de una cuchara: consta de siete estrellas con la forma de carro; cuatro de ellas forman lo que es la parte honda del carro y las otras tres son el mango del carro. Fue una de las 48 constelaciones enumeradas originalmente por el astrónomo Claudio Ptolomeo en el siglo II, y desde entonces forma parte de las 88 constelaciones contemporáneas. 

El elemento más conocido de la Osa Menor es la estrella polar, llamada Polaris, la cual se encuentra situada aproximadamente en la prolongación del eje de la Tierra, de modo que permanece casi fija en el cielo y señala el polo norte geográfico, por lo que ha sido empleado por navegantes como punto de referencia en sus travesías.
Dada su ubicación, la Osa Menor solo se puede ver en el hemisferio norte, pero a cambio, en dicho hemisferio se ve todo el año. Junto con la Osa Mayor, es uno de los elementos más característicos del firmamento del hemisferio norte.

En la mitología griega, hay varias versiones sobre el origen de la Osa Menor. Una de ella sería Fénice, transformada en osa por Artemisa tras haber sido seducida por Zeus. Este relato es muy similar al de Calisto, que fue catasterizada en la Osa Mayor y por ello algunos autores creen que originalmente debió haber un relato con dos catasterismos de un mismo personaje (Zeus habría convertido a Calisto en la Osa Mayor y posteriormente Artemisa la habría convertido en la Osa Menor). 

En otra versión se dice que se trataba de Cinosura, nodriza de Zeus y ninfa del Monte Ida.

Polaris (α Ursae Minoris) es la estrella más brillante de la constelación. Distante unos 432 años luz de la Tierra, es una supergigante blanco-amarilla que varía entre tipo espectral F7Ib y F8Ib, siendo la variable cefeida más prominente del firmamento. Es un sistema estelar triple: la supergigante está acompañada por dos estrellas de la secuencia principal de tipo F7V y F3V separadas 17 y 2400 ua de ella.

Kochab (β Ursae Minoris), el segundo astro más brillante, es una gigante naranja de tipo K4III; la orbita un exoplaneta gigante a 1,4 ua.
Le sigue en brillo γ Ursae Minoris, llamada Pherkad, una gigante blanca de tipo espectral A2III 11 000 veces más luminosa que el Sol. Es una variable Delta Scuti con un período de 3,43 horas.

11 Ursae Minoris es también una gigante de tipo K4III con un planeta que tarda 516 días en completar una órbita en torno a ella.
Otra estrella en donde se ha descubierto un exoplaneta es Baekdu, nombre oficial de 8 Ursae Minoris.

En esta constelación se localiza 1RXS J141256.0+792204, informalmente conocida como Calvera, una estrella de neutrones solitaria que no forma parte de un sistema estelar. A una incierta distancia de 625 años luz, es uno de los objetos de su clase más próximos a la Tierra.

Otra variable de la constelación es RR Ursae Minoris, gigante roja y variable semirregular cuyo brillo máximo alcanza +4,53. Es, además, una binaria espectroscópica cuyo periodo orbital es de 748,9 días.

La Osa Menor contiene pocos objetos de cielo profundo relevantes. 
Entre ellos está NGC 6217, galaxia espiral barrada distante 67 millones de años luz.
Mucho más próxima, a unos 225 000 años luz de distancia de la Tierra, la Enana de la Osa Menor es una galaxia esferoidal enana descubierta por Albert George Wilson en 1955.







</doc>
<doc id="16869" url="https://es.wikipedia.org/wiki?curid=16869" title="Oriente inmutable">
Oriente inmutable

El Oriente Inmutable es un mito histórico de Occidente, según el cual los pueblos orientales vivirían en una suerte de limbo histórico, sin que sus sociedades hayan cambiado en lo más mínimo a lo largo de la historia.

Este mito se desarrolló debido a la falta de estudios sobre las sociedades orientales, las que tendieron a ser calificadas como bárbaras, paganas o infieles, y por lo tanto descartadas como inferiores. Este criterio europeocéntrico empezó a ceder en el siglo XVII, a medida que las exploraciones geográficas fueron revelando antecedentes cada vez más abundante sobre la historia y la evolución de las sociedades orientales. En la actualidad puede estimarse este mito como superado, en la tradición historiográfica occidental.


</doc>
<doc id="16870" url="https://es.wikipedia.org/wiki?curid=16870" title="Krigeaje">
Krigeaje

El krigeaje, krigeado o kriging (del francés "krigeage"), también conocido como regresión en procesos Gaussianos, es un método de interpolación geoestadístico de estimación de puntos. Utiliza un modelo de variograma para la obtención de los ponderadores que se dan a cada punto de referencia usado en la estimación. Esta técnica de interpolación se basa en la premisa de que la variación espacial continúa con un mismo patrón homogéneo.
Fue desarrollada inicialmente por Danie G. Krige a partir del análisis de regresión entre muestras y bloques de mena, las cuales fijaron la base de la geoestadística lineal.

El kriging puede ser entendido como una "predicción lineal" o una forma de inferencia bayesiana. Parte del principio: "puntos próximos en el espacio tienden a tener valores más parecidos que los puntos más distantes".
La técnica de kriging asume que los datos recogidos de una determinada población se encuentran correlacionados en el espacio. Esto es, si en un vertedero de residuos tóxicos y peligrosos la concentración de zinc en un punto p es x, será muy probable que se encuentren resultados muy próximos a x cuanto más próximos se esté del punto p (principio de geoestadística). Sin embargo, desde una cierta distancia de p, ciertamente no se encontrarán valores próximos a x porque la correlación espacial puede dejar de existir.

Se considera al método de kriging del tipo MELI (Mejor Estimador Lineal Insesgado) o ELIO (Estimador Lineal Insesgado Óptimo): es lineal porque sus estimaciones son combinaciones lineales ponderadas de los datos existentes; y es insesgado porque procura que la media de los errores (desviaciones entre el valor real y el valor estimado) sea nula; es el mejor (óptimo) porque los errores de estimación tienen una variancia (variancia de estimación) mínima.
El término kriging abarca una serie de métodos, el más común es el siguiente:

Asume que las medias locales son relativamente constantes y de valor muy semejante a la media de la población que es conocida. La media de la población es utilizada para cada estimación local, en conjunto con los puntos vecinos establecidos como necesarios para la estimación.

Las medias locales no son necesariamente próximas de la media de la población, usándose apenas los puntos vecinos para la estimación. Es el método más ampliamente utilizado en los problemas ambientales.
si es...

Es una extensión de las situaciones anteriores en las que dos o más variables tienen una dependencia espacial y esa variable se estima que no se muestra con la intensidad con la que otros son variables dependientes, con estos valores y sus dependencias para estimar la variable que se requiere.

El método de Kriging utiliza diversas teorías explayadas en la estadística. En tanto, para que esta teoría estadística se vea más clara en el ámbito de aplicación; se explican algunos conceptos.

Una semivariancia es la medida del grado de dependencia espacial entre dos muestras. La magnitud de la semivariancia entre dos puntos depende de la distancia entre ellos, implicando en semivariancias menores para distancias menores y semivariancias mayores para distancias mayores. El gráfico de las semivariancias en función de la distancia a un punto es llamado de semivariograma. A partir de una cierta distancia, la semivariancia no más aumentará con la distancia y se estabilizará en un valor igual a la "variancia media", dando a esa región el nombre de meseta, silo o patamar ("sill"). La distancia entre el inicio del semivariograma al comienzo del silo recibe el nombre de rango. Al extrapolar la curva del semivariograma para la distancia cero, podemos llegar a un valor no nulo de semivariancia. Ese valor recibe el nombre de efecto pepita ("Nugget Effect").

En el Método de Kriging normalmente son usados cuatro tipos de variogramas: usadas las siguientes variables:
Este modelo no presenta silla y es muy simple. Su curva puede ser representada por:

Una forma esférica es la más utilizada en el silo. Su forma es definida por:

La curva de variograma exponencial respeta la siguiente ecuación:
Una forma gaussiana es dada por:

Tomando como base una simulación de un sistema de dos dimensiones (2 D) que contienen un número finito de puntos donde es posible una medición de cualquier tamaño. Luego de la adquisición de estos datos, se iniciará la interpolación Kriging buscando alcanzar una mayor resolución. El primer paso es construir un semivariograma experimental. Para tal, se calcula la semivariancia de cada punto en relación a los demás y se ve en un gráfico de la semivariancia por la distancia.
A partir de ese gráfico se estima el modelo de variograma que mejor se aproxima a la curva obtenida. El efecto pepita puede estar presente en el semivariograma experimental y debe ser considerado. Determinado el modelo de semivariograma a ser usado, se inicia la fase de cálculos. Siendo el semivariograma una función que depende de la dirección, es natural que presente valores diferentes conforme la dirección, recibiendo este fenómeno el nombre de anisotropía. Un caso de semivariograma presente una forma semejante en todas las direcciones del espacio, va a depender de h, diciéndose que es una estructura isotrópica, "i. e.", sin direcciones privilegiadas de variabilidad.

Considere, para el cálculo del kriging, la siguiente fórmula:
donde formula_12 es el número de muestras obtenidas, formula_13 es el valor obtenido en el punto formula_14 y formula_15 es el peso designado al punto formula_14.
A fin de obtener los pesos de cada uno de los formula_12 puntos, para cada uno de ellos se realiza un cálculo de formula_18. Tal procedimento depende del tipo de kriging que está siendo utilizado. Hacemos hincapié en la siguiente notación:

En ese caso es utilizada la media local de los puntos mostrados. Por consiguiente, debe normalizarse la media de los pesos. Consecuentemente, se tiene un resultado más preciso del Kriging Simple. El uso será de las siguientes ecuaciones para determinar los valores de los pesos en el "p-enésimo" punto:

Para este caso, utilizar la media de todos los datos. Implicando, por tanto, que no se normalice en la ubicación promedio de los pesos, como en el anterior. Así, tenemos casi la misma ecuación, excepto por la exclusión de formula_24 y por la última ecuación. La característica principal de este método es la generación de gráficos más lisos y más estéticamente suaves. Cabe señalar que este caso es menos exacto que el caso anterior. Los valores de los pesos para el "p-ésimo" punto serán dados por:

Cuando llegamos a los valores de formula_18, se calculan los valores de formula_27:
De esa manera, se calcula el valor interpolado para todos los puntos deseados. Se resalta que solamente deben ser utilizados los valores adquiridos arriba.
La obtención del valor interpolado en otro punto requiere la repetición de todos los cálculos realizados a partir de la obtención del modelo de variograma. De esa forma, para aumentar la resolución que se pretendía, se debe recurrir a métodos matemáticos para la resolución computacional. Diversos códigos se han desarrollados para esa resolución, mas uno de los mejores algoritmos puede ser obtenido del link de abajo. Fue inicialmente hecho para lenguaje Fortran, y puede ser recodificado para C con la ayuda de la biblioteca fortran2c , presentándose totalmente en C:



</doc>
<doc id="16871" url="https://es.wikipedia.org/wiki?curid=16871" title="Neodiapsida">
Neodiapsida

Los neodiápsidos (Neodiapsida, gr. "diápsidos nuevos") son un clado mayor de saurópsidos que incluye todos los diápsidos salvo algunos tipos primitivos conocidos como Araeoscelidia.

El siguiente cladograma muestra los grupos de neodiápsidos y sus relaciones filogenéticas:


</doc>
<doc id="16872" url="https://es.wikipedia.org/wiki?curid=16872" title="Neocolonialismo">
Neocolonialismo

El neocolonialismo es la práctica geopolítica que se encarga de utilizar el mercantilismo, la globalización empresarial y el imperialismo cultural para influir en un país en el que grupos de pocas personas que hablan el mismo idioma y tienen la misma ciudadanía que los neo-colonizados, establezcan una élite para dirigir las poblaciones, apropiarse de las tierras.

Durante las primeras décadas del siglo XVIII, el imperialismo de tipo militar, político y cultural dio paso al imperialismo económico. De esta forma las potencias prefirieron que sus colonias fueran mercados para sus productos de las industrias antes que otros militares y políticos. Esta situación se produjo porque los territorios colonizados independentistas acabaron con el dominio militar y político en sus territorios.

El neocolonialismo es diferente al colonialismo, que se caracteriza por un control directo. Así, se emplea el ejército para la ocupación del país y se establecen colonos procedentes de la metrópolis en el territorio sujeto a la dominación. Los terratenientes, pertenecientes a lo que se denominó la "hacienda tradicional", continuaron produciendo para su propia subsistencia y la de la población campesina, vinculada a la hacienda por relaciones de tipo servil y, en algunos casos, abasteciendo a un mercado de amplitud regional.

Tras la liberación política de las colonias, se mantuvieron generalmente las antiguas estructuras económicas. La dependencia de las importaciones de la metrópolis, la concentración de la producción en ciertas materias primas para exportar a Europa y la carencia de los medios técnicos y del capital, y la conservación en ciertos casos de la propiedad de la industria en manos de colonizadores suponen la continuación del control económico sobre estos países.
La devaluación de las materias primas que exportan y la venta de bienes manufacturados de mayor valor añadido generan un déficit comercial nocivo para estos países. La ilegítima deuda externa asumida por muchos países es también un factor relevante en el proceso. 
El neocolonialismo es el control y la tutela que siguen ejerciendo las potencias coloniales, sobre sus antiguas colonias. La descolonización no supuso independencia económica para los países denominados "subdesarrollados", sino que los estados imperialistas se encargaron de organizar la economía y la política mundial, de manera que se conservase la explotación colonial. El neocolonialismo sería la herencia del colonialismo histórico y a la vez, la continuidad del sistema capitalista globalizador. Esta nueva fase, permite seguir con el sometimiento (hoy en día sin una ocupación y control directo), sino más bien a través de complejas estrategias económicas y políticas.
“La tendencia profunda del capitalismo se ha abierto camino y hoy la expansión ya no requiere la anexión de territorios y su cobijo dentro de fronteras nacionales. Hoy la expansión capitalista “salta” las fronteras e invade los territorios sin necesidad de conquistarlos y anexionarlos”. (Vidal Villa, 1998)

La descolonización que supuestamente inauguraba la aparición de países “libres y soberanos”, supuso que los territorios colonizados se sumiesen en una situación de dependencia económica y política más dependiente que nunca. En un contexto en el que el flujo de mercancías y personas traspasa todos los límites territoriales; la división entre países centrales y periféricos ha llegado a su máxima expresión.

Para alcanzar el objetivo de la globalización del sistema capitalista, las potencias han entramado organismos que posibiliten la hegemonía política, económica y militar; de una manera más sutil que en la época del colonialismo. Se sigue implantando la ideología colonizadora a través del pretexto de “misión civilizadora” o simplemente reafirmando su posición en las relaciones de poder actuales. La inserción de los países “subdesarrollados” en el mercado mundial tiene un formato periférico, por lo que a pesar de la riqueza de recursos naturales que puedan tener, se encuentran sumidos en una situación de pobreza absoluta.

La usurpación de territorios ajenos impulsado por fines económicos y de poder, tiene consecuencias de todo tipo en los países explotados. La llegada de las multinacionales ha supuesto el deterioro del ecosistema por y para las exportaciones masivas, de manera que ha sido la naturaleza la que ha tenido que adaptarse al hombre. Entre las consecuencias sociales, la globalización capitalista ha supuesto también la globalización cultural, mutilando las tradiciones y modos de vivir autóctonos.

“Los países desarrollados están en una posición en la que pueden utilizar, en su beneficio y por multitud de canales, los recursos del resto de países "subdesarrollados". Ese es el fundamento del orden económico mundial. A los ojos de la mayoría de la humanidad se presenta como un orden tan caduco e injusto como el colonialismo del que arranca su origen y esencia

La independencia de las colonias europeas en África fue consecuencia de muchos factores, entre ellos el deseo de los pueblos africanos de independizarse, inspirados por la independencia de la India, y el resentimiento popular contra el racismo y la desigualdad. Pero, además, las dos nuevas potencias surgidas tras la Segunda Guerra Mundial, la URSS y Estados Unidos, no habían participado en el reparto de África y querían asegurar su influencia en la zona. Las dos superpotencias financiaron los intereses independentistas y a los nuevos Estados. Trataban así de relanzar su industria de armamento, extender su ideología y obtener el control económico de la región.

Para poder alimentar, educar y modernizar a sus masas, África tomó prestadas grandes cantidades de dinero de varios países, banqueros y compañías. Gran parte de este dinero fue despilfarrado por dictadores corruptos y no revirtió en el bienestar de los pueblos; además, la deuda mermó la independencia de los Estados africanos.

Por la conquista de Siberia y del Turquestán, Rusia llegó a ser una gran potencia asiática. El Imperio Ruso de Asia, prolongación de Rusia europea, extendíase en 1914 sobre 16 millones de kilómetros cuadrados, o sea, una y media veces la superficie de toda Europa. Era el más vasto Imperio del mundo.

La conquista de Siberia por los rusos empezó a fines del siglo XVI, pero se hizo lentamente porque los rusos debían atender el frente europeo de su Imperio y Siberia solo fue tierra de castigo o presidio inmenso, a dónde fueron deportados, durante varios siglos, los desterrados políticos y los condenados de derecho común. Pero a mediados del siglo XIX y después de la Guerra de Crimea y el Tratado de París (30), los rusos volvieron de nuevo la vista al Asia.

Las partes del litoral del Pacífico ya ocupadas tenían el inconveniente de estar invadidas durante 7 u 8 meses por los hielos. De aquí que los rusos buscasen adquirir, a expensas de China, costas más meridionales. Los chinos fueron primero expulsados de la desembocadura del Amur, después, en 1858- 1860, mientras China estaba en guerra con Francia e Inglaterra, obtuvo los territorios que formaron la provincia marítima. En su extremidad meridional, en frente de Japón, crearon un puerto militar con el ambicioso nombre de Vladivostok, “El Dominador de Oriente”.
china
Hemos visto ya Que China es el más antiguo de los Estados actualmente existentes. Es más extenso que Europa entera y la fertilidad de sus llanuras atravesadas por dos ríos enormes y las innumerables minas de sus montañas hace de ella una de las tierras más ricas del globo. Hacia 1838, según estadísticas chinas, el Imperio tenía alrededor de 350 millones de habitantes. Inmovilizados en el respeto del pasado, los chinos no tuvieron por mucho tiempo más que desprecio por las ideas nuevas y desconfianza y odio por todo lo que venía fuera de los “diablos rojos”, es decir, los europeos de tez sanguínea.

La transformación de Japón es, por su instantaneidad, uno de los hechos más sorprendentes de la historia. Se puede decir que, en algunos años, Japón adquirió la experiencia que Occidente se demoró varios siglos en lograr.

Muchos países latinoamericanos recurrieron durante la década de los años setenta a créditos de bancos multinacionales o empresas privadas de esos países se endeudaron y posteriormente su deuda privada se convirtió en deuda pública. Esto fue posible por la clase dirigente con intereses extra nacionales, con una visión neoliberalista, o por gobiernos militares impuestos desde afuera, en el caso de Latinoamérica muchos de estos gobiernos fueron impuestos por los Estados Unidos, como en la denominado Operación Cóndor. A estos países les resultó extremadamente difícil pagar la deuda externa y las potencias aprovecharon estas deudas, junto con acciones militares, como por ejemplo el golpe a Salvador Allende, o intimidación sindical, para convertir tales países en sus neocolonias, instalando bases militares, obteniendo acceso a sus recursos naturales a precios marginalmente bajos o implantando políticas que resultaran de beneficio para el país.



</doc>
<doc id="16873" url="https://es.wikipedia.org/wiki?curid=16873" title="Nelumbo nucifera">
Nelumbo nucifera

Nelumbo nucifera es una de las dos especies pertenecientes al género "Nelumbo". Recibe el nombre vulgar de loto sagrado o loto indio, y a veces el de rosa del Nilo. Es famosa la longevidad de sus semillas, que pueden germinar después de diez siglos.
Es una planta herbácea acuática. <br>
Las hojas son flotantes o emergentes, peltadas, glaucas, de limbo orbicular, de (25-)30-100 cm de diámetro, glabro, de borde frecuentemente ondulado, hidrófobo; pecíolo normalmente con acúleos, de 1 a 2 m o más de largo, fistuloso. Arrancan desde el rizoma, que puede alcanzar 20 m de largo, es grueso, ramificados, con numerosos catáfilos, profundamente enraizado en el fondo.<br>
Flores de 16-23 cm de diámetro, rosa vivo a pálido o blancas, olorosas, con pétalos cóncavos, oblongo-elípticos a obovados, 5-10 × 3-5 cm; anteras de 1-2 cm de largo; pedúnculos normalmente con acúleos, sobrepasando la altura de las hojas.<br>
Los frutos complejos están formados por un receptáculo elipsoidal, de 5-10 cm de diámetro, de lados rugosos a débilmente estriados, con el ápice truncado y plano, y las núculas insertas en fosetas en él. Núculas de 10-20 × 7-13 mm, ovoides, usualmente más de 1,5. Florece al final de primavera y en verano.

La especie se distribuye de manera natural por el sur de Rusia (delta del Volga), Cercano Oriente (Azerbaiyán, Irán), Siberia oriental, China, Pakistán, Bután, Nepal, India, Sri Lanka, Laos, Japón, Corea, Taiwán, Birmania, Tailandia, Vietnam, Camboya, Indonesia, Malasia, Filipinas, Nueva Guinea y Australia; ha sido introducida en Estados Unidos y está naturalizada en parte del sur de Europa (Rumanía). En estado natural vegetal en estanques y lagunas, entre 0 y 400 msnm.

Tiene un uso muy extendido en jardinería para cubrimiento de superficies de agua, a pesar de sus flores efímeras. Se utilizan numerosos cultivares con diferentes pautas florales. Los rizomas y semillas se comen tostadas o cocidas. También se usa en medicina popular. Se considera planta sagrada en la India y China, así como lo fue en el Antiguo Egipto. También se comen los rizomas.

Las flores de loto, ya sea por su llamativa belleza, ya sea por surgir del «fondo» de las aguas han resultado simbólicas (por el medio de la metáfora, ya sea tal metáfora consciente o inconsciente) en las religiones del Antiguo Egipto, la India y luego de la China.

En el Antiguo Egipto, junto al escarabajo pelotero, el ave fénix y el mismo sol (Re o Ra), los lotos representan la resurrección, en el caso del loto por emerger resplandeciente desde las profundas aguas. 

En la India el loto es llamado "padma" en sánscrito. Quizás haya tenido inicialmente el mismo significado que en el Antiguo Egipto, a tal significado se añadió el simbolismo según el cual los principales dioses y diosas nacieron en padmas o lotos; el padma hindú suele servir de modelo para figurar mandalas o para configurar los chakras. En China, Japón y en todos los lugares en donde ha llegado el budismo una oración ritual característica menciona al loto, tal fórmula suele ser "om mani padme hum" («¡"om" joya en el loto "hūṃ"!»).

Las escuelas budistas que emergieron del culto creado por el monje japonés Nichiren toman como texto supremo el Sutra del Loto (妙法蓮華経 "Myōhō Renge Kyō", o abreviado 法華経 "Hokkekyō"), y en sus liturgias el mantra más importante recitado reza 南無妙法蓮華経 "Namu Myōhō Renge Kyō" («Alabada sea la Verdad del Maravilloso Sutra del Loto»)

Por ese motivo, lotos muy estilizados suelen aparecer representados en diversos objetos del arte de las culturas reseñadas.

Dependiendo el color de la flor se le atribuyen diferentes significados simbólicos, representando el azul la sabiduría y el conocimiento, el color blanco la naturaleza inmaculada y la pureza, el rojo la compasión y el color rosa a personajes divinos. 

Miquelianina (Quercetina 3-O-glucuronide), un compuesto fenólico, está presente en "N. nucifera".

"Nelumbo nucifera" fue descrito por Joseph Gaertner y publicado en "De Fructibus et Seminibus Plantarum..." 1: 73. 1788. La especie tipo es: "Dicrocaulon pearsonii" N.E. Br. 

El término específico hace referencia a sus frutos (latín: "nucifer, -a, -um", que lleva nueces).





</doc>
<doc id="16874" url="https://es.wikipedia.org/wiki?curid=16874" title="Negación (gramática)">
Negación (gramática)

La negación es un elemento lingüístico que sirve para negar un elemento oracional o una oración entera mediante un sema lexicalizado, una palabra, normalmente adverbio, o una locución. El hecho de negar implica la expresión de la no existencia de algo o la no realización de una acción. Desde el punto de vista sintáctico la gramática generativa moderna analiza la negación oracional mediante la presencia de un sintagma de negación cuyo núcleo debe ser una partícula de polaridad negativa. En lenguas como el español y las lenguas romances existe concordancia de polaridad, a diferencia de las lenguas germánicas donde usualmente no es posible la doble negación.

La negación en español se produce usualmente anteponiendo al verbo el adverbio negativo "no" ("No tengo hambre").

También mediante otros adverbios, por ejemplo:

Mediante verbos que la implican, como por ejemplo.

Locuciones:

Nótese las expresiones anteriores cuya interpretación semántica es la negación de una oración más simple y por eso son formas de negación semántica. Sin embargo, estructuralmente son muy diferentes; en concreto (3) y (4) son frases sintácticamente declarativas afirmativas, y la interpretación como negación está lexicalizada en el verbo. A continuación se examina la diferencia entre negación semántica (acorde al significado) y negación sintáctica (acorde a la estructura interna).

Las oraciones interrogativas frecuentemente pueden equivaler a una frase imperativa afirmativa y constituyen una alternativa pragmáticamente neutralizada para una orden directa:

Dentro de la gramática tradicional la negación se trata como un simple modificador adverbial. Sin embargo, el comportamiento morfosintáctico de la negación es más complejo que el de los adverbios convencionales. Por ejemplo, en español, "no" es incompatible con algunas formas del verbo como el modo imperativo:
El comportamiento de (7b) es paralelo al comportamiento del complementador "que" en (8):
Lo cual sugiere que sintácticamente la negación ocupa una posición fuera del sintagma verbal. Otros hechos muestran más paralelos entre las oraciones interrogativas, negativas y las que usan el modo imperativo:
Algunos autores han propuesto la existencia de un sintagma de negación cuyo núcleo sintáctico debe estar ocupado por un elemento negativo. Algunas peculiaridades como la de algunos dobles negativos en español son explicables conjeturando la existencia de dicho tipo de sintagma:
La oración (11c) es incorrecta, ya que en la posición preverbal sólo puede aparecer un elemento negativo en la posición de núcleo de negación. Aunque "nunca" puede aparecer en varias posiciones, si la posición preverbal está ocupada por "no", no se admite la adjunción de ningún otro elemento negativo; por esa razón (11c) no está bien formada. Eso sugiere que el "nunca" de (11a) tiene una naturaleza diferente que el de (11b) y podría estar en la misma posición que los adverbios, aunque como muestra el hecho de que (11d) sea incorrecta, la posición adverbial sólo admite un elemento de polaridad negativa, si el núcleo de negación está presente, por lo que es posible que existe alguna forma de concordancia entre el núcleo del sintagma de negación y otros elementos de polaridad negativa de la frase.

La expresión de la negación en las lenguas del mundo es muy variada, ya que puede incluir simples marcas de negación lógica ("no"), como palabras de polaridad negativa y contenido semántico adicional ("ninguno, nadie, nada, nunca"), y la negación puede realizarse mediante morfemas independientes o mediante afijos o clíticos. Una de las cuestiones mejor estudiadas es la negación lógica mediante medios sintácticos, atendiendo al orden. Entre las lenguas del mundo se dan seis posibilidades para el orden relativo del sujeto, del objeto y del verbo, siendo los más frecuentes por orden: SOV, SVO, VSO y VOS (también existen OVS y OSV pero son muy infrecuentes en las lenguas del mundo), y existirán 24 posibles órdenes para la posición de la negación en una oración transitiva con sólo sujeto, objeto, verbo y elemento negativo. Cuando se examinan los mejores datos disponibles se observa que la posición de la negación no es del todo arbitraria y está fuertemente correlacionada con la posición de los otros constituyentes, siendo comunes sólo 5 de los 24 tipos posibles, que se recogen en la siguiente tabla:

A diferencia de otros idiomas, como el inglés, la doble negación en español no es una afirmación sino que continúa siendo una negación.
Dada la importante función comunicativa de la negación es muy común que muchas lenguas del mundo recurran a la doble negación como una manera de marcar redundantemente. Técnicamente la doble negación es de hecho un fenómeno de concordancia de polaridad negativa. La negación simple tiene el riesgo de que si al oyente le pasa inadvertido el elemento negativo aparecen problemas de malinterpretación, la doble negación o negación redundante es una estrategia que disminuye este problema.

M. S. Dryer, sobre una muestra de 345 lenguas, muestra que 20 usan regularmente la doble negación; esa tasa de redundancia no es común en otras categorías gramaticales. Un ejemplo de uso regular de doble negación es el francés escrito:
El latín igualmente presenta complicaciones cuando interactúan varios elementos de polaridad negativa:


</doc>
<doc id="16875" url="https://es.wikipedia.org/wiki?curid=16875" title="Navegación aérea">
Navegación aérea

La navegación aérea es el conjunto de técnicas y procedimientos que permiten pilotar eficientemente una aeronave a su lugar de destino, asegurando la integridad de los tripulantes, pasajeros, y de los que están en tierra. La navegación aérea se basa en la observación del cielo, del terreno, y de los datos aportados por los instrumentos de vuelo.

La navegación aérea se divide en dos tipos (dependiendo si la aeronave necesita de instalaciones exteriores para poder guiarse):

La navegación aérea autónoma es aquella que no necesita de ninguna infraestructura o información exterior para poder completar con éxito el vuelo. A su vez, esta se divide en:

La navegación aérea no autónoma, al contrario, sí necesita de instalaciones exteriores para poder realizar el vuelo, ya que por sí sola la aeronave no es capaz de "navegar". Las instalaciones necesarias para su guiado durante el vuelo reciben el nombre de ayudas a la navegación. Estas ayudas se pueden dividir a su vez dependiendo del tipo de información que transmiten, así como del canal a través del cual lo hacen. Así, las radioayudas pueden ser:

Dependiendo de las condiciones mínimas de visibilidad, distancia de las nubes, y del tipo de espacio aéreo atravesado, existen dos conjuntos de reglas de obligado cumplimiento: las "reglas de vuelo visual" (visibilidad mayor de 5 [kilómetros] [8 km] y techo de nubes por encima de los 1500 m) y las "reglas de vuelo instrumental" (operada mediante instrumentos). Los aviones de línea, por razones de seguridad, operan solamente bajo las reglas de vuelo instrumental, independientemente de las condiciones meteorológicas.

El elemento responsable en tierra de la navegación aérea es el control de tráfico aéreo, apoyado en la información proporcionada por los pilotos y por los sistemas de radar.




</doc>
<doc id="16876" url="https://es.wikipedia.org/wiki?curid=16876" title="Geografía de Namibia">
Geografía de Namibia

Namibia se encuentra situada al suroeste de África, en la costa atlántica. Limita al norte con Angola, al este con Botsuana, al sur con Sudáfrica, al noreste con Zambia y al oeste con el Océano Atlántico.
A pesar de su extendida costa la mayor parte del territorio es desértico, destacándose dos importantes desiertos, el del Namib en el oeste y el de Kalahari en el este. La razón para que el desierto del Namib llegue a lindar con el océano en la Costa de los Esqueletos y de la gran sequedad atmosférica aun en las playas se debe a que el agua de esta parte del Atlántico es fría por las corrientes que proceden de la Antártida: el agua fría no produce suficiente evaporación como para que se formen importantes nubes de lluvia.

Avanzando desde el océano hacia el este el territorio se eleva enseguida para formar una amplia meseta que ocupa la mayor parte del país. Namibia está recorrida de sur a norte por una serie de cordilleras de montañas muy antiguas y por esto bastante redondeadas por la erosión, (la toponimia aún utilizada es en gran medida la colonial alemana y la afrikáner ya que un mismo accidente geográfico puede recibir nombres muy distintos según las diversas etnias nativas). La mayor altitud es el monte Konigstein de 2606 msnm; en el centro norte se destaca el monte Omatako de 2286 m; a poca distancia de este se ubica el Etig con 2085; unos 700 km al sudoeste de los montes recién mencionados se destaca el Schroffenstein con 2202 m; y en el extremo sudoeste se encuentra el macizo aislado de los montes Hunsberge.

Los dos principales ríos son exógenos (nacen fuera de Namibia) al norte el Kunene que señala parte del límite con Angola y al sur el Orange que señala gran parte del límite con Sudáfrica.

Si observamos Namibia en el mapa de África, nos encontramos con una estrecha franja que se extiende entre el norte de Botsuana y el sur de Angola y que es territorio namibio; es la franja de Caprivi.

El principal accidente costero es la bahía de la Ballena (Walvis Bay).

El agua fría del océano que baña a las costas namibias es muy rica en recursos pesqueros, por otra parte el subsuelo del interior desértico posee importantes yacimientos minerales entre los que se destacan los de oro y diamantes.

Namibia, cortada por el trópico de Capricornio, tiene clima subtropical, desértico a lo largo de la costa y en el sur, y árido, con una época de lluvias entre noviembre y marzo, en el centro-norte y en el nordeste.

En la costa las lluvias son casi inexistentes, en Swakopmund caen apenas 8 mm al año y llueve un solo día de media, pero las nieblas provocadas por la corriente fría de Benguela, que recorre la costa de sur a norte, hacen que las temperaturas oscilen entre los 9 C y los 21 C de medias mínimas y máximas todo el año, con 16 C de máxima entre agosto y octubre. El viento llamado "Oosweer", procedente del interior, puede hacer que las temperaturas suban súbitamente. Basta con alejarse de la costa unos kilómetros para notar el calor.

La zona más seca se encuentra en la costa, en el desierto del Namib, y en el sur, que forma parte del desierto de Kalahari, lo bastante húmedo para contener plantas xerófilas y suculentas. 

La mayor parte del interior de Namibia está ocupado por una meseta de 1200 a 1700 m de altitud. En el sur, más seco, el calor es más intenso antes de la época de lluvias. En Keetmanshoop, a 1000 m de altitud en el desierto de Kalahari, caen 147 mm en 22 días, la mayor parte entre enero y abril. Las temperaturas medias máximas superan los 30 C entre octubre y marzo, con las escasas lluvias, y las mínimas bajan de 10 C entre mayo y septiembre, con cielos despejados.

En el centro del país, en Windhoek, a 1650 m de altitud en plena meseta, caen 371 mm entre octubre y abril, con temperaturas medias que oscilan entre 7-21 C en junio y julio, y 17-30 C entre noviembre y febrero.

En el norte, cerca del parque nacional Etosha, en Tsumeb, a 1300 m de altitud, caen 556 mm en 59 días, entre octubre y abril, con mínimas de 8 C en junio y julio y máximas que superan los 30 C entre septiembre y febrero.

La zona más lluviosa se encuentra en la región de Kavango y en la franja de Caprivi, en el nordeste. En Katima Mulilo caen unos 680 mm entre mediados de octubre y primeros de abril. Las temperaturas oscilan entre los 4 C y los 25 C en invierno; en época de lluvias, en diciembre y enero, con más de 160 mm cada mes, las temperaturas oscilan entre los 18 C y los 24 C. Sin embargo, cuando empiezan las lluvias, en octubre, oscilan entre 17 C y 35 C.

La administración y organización de la conservación de la naturaleza en Namibia se hace a cargo del Ministerio de Medio Ambiente y Turismo. En 2013 estaban bajo protección estatal directa 138,163.7 km, algo menos del 16.8 por ciento del área de Namibia. Además, otros 177.435 km (aproximadamente el 21,5 por ciento de la superficie), que están bajo protección parcial del estado (las denominadas Conservancies).



</doc>
<doc id="16878" url="https://es.wikipedia.org/wiki?curid=16878" title="Momento magnético nuclear">
Momento magnético nuclear

El momento magnético nuclear es el momento magnético que poseen los núcleos atómicos y que se debe a la estructura compleja del núcleo atómico. El momento magnético nuclear se explica tanto por el momento angular asociado a los protones orbitando en el interior del núcleo así como al momento magnético de espín.

Cada átomo tiene asociado un valor de momento magnético, ocasionado por el movimiento del núcleo (portador de una carga eléctrica) al girar sobre sí mismo (esta interpretación clásica sirve para entender el concepto, una interpretación cuántica sirve para hacer cálculos cuantitativos).
El enfoque clásico del momento magnético nuclear lo representa como un vector, el vector momento magnético nuclear. Este vector se representa como m y su valor es de:

Donde:
A su vez el módulo del momento magnético nuclear viene dado por:

Donde formula_3 es una constante que depende de la estructura interna del núcleo atómico.

En el seno de la mecánica cuántica el momento magnético nuclear debe ser tratado como un operador lineal vectorial acotado:

Cumpliéndose la siguiente identidad entre el observable asociado al momento magnético nuclear y el operador de momento angular:
Donde formula_4 el número cuántico orbital principal del núcleo.


</doc>
<doc id="16881" url="https://es.wikipedia.org/wiki?curid=16881" title="Modelismo naval">
Modelismo naval

El Modelismo naval consiste en la construcción de modelos de barcos a escala, existiendo dos grandes corrientes; una de modelismo estático, y otra de modelismo navegable.

Los inicios del modelismo en general se remontan a muchos miles de años atrás, desde que el hombre primitivo empezó a crear réplicas de animales y plantas de su entorno. En Egipto se encontraron representaciones de barcos egipcios que datan del año 2000 AC.

La escala representa las veces en la que el barco real es dividido para su creación, es decir, si el barco real mide 250 metros y la escala es 1:1000 corresponde dividir los 250 metros del barco entre 1000, lo cual da una medida de 25 centímetros, es decir, que un modelo en escala 1:1000 de un barco de 250 metros mide 25 centímetros. Las escalas más comunes son 1:350 o 1:400 para barcos más grandes y 1:700 para barcos pequeños.

Este tipo de modelo pretende realizar un modelo reducido lo más parecido posible a una nave real, existente o que haya existido, pero tratando de conseguir la mayor fidelidad posible con respecto al original.

Existen básicamente 2 formas de representar el barco a escala, a casco completo (fullhull) o en línea de navegación (waterline). En la primera aparece el modelo completo, tanto la obra muerta de la nave (lo que se encuentra sobre la línea de flotación) como la obra viva (casco sumergido con hélices y timón). En este caso es común sostener el navío con algún tipo de base o pedestal. Para las presentaciones waterline normalmente se opta por crear una base que simule el mar con lo cual se produce un diorama (representación de una escena).

El Modelismo Estático es básicamente de exhibición, tanto en colecciones personales como en museos. Al no ser modelos que serán navegables, pueden tener más nivel de detalle ya que no sufrirán los daños propios de su uso. Muchas partes de estos modelos, como cableados, barandas y otras pequeñas piezas son muy frágiles por lo que su manipulación sin cuidado puede estropearlos. No son juguetes, por lo tanto no están diseñados para su uso como tales.

El modelismo estático, por lo general, requiere de piezas creadas de diversos materiales por el modelista o adquiridas en un kit, además de pegamentos, pinceles y pinturas.

Prima la posibilidad de navegar del modelo, aunque en segundo plano, también se le añaden toda clase de detalles realísticos. En modelos navegables se llega incluso a crear lanzaderas de misiles con petardos representando los proyectiles, radares y cañones móviles o chimeneas que expulsan vapor.

En el maquetismo navegable se puede optar por dos tipos; el que viene fabricado, tales como las lanchas rápidas, que pueden ser adquiridas en tiendas de juguetes. También está el caso de los barcos, lanchas y veleros que pueden ser fabricados, la mayoría de los que se hacen, suelen ser de diseño propio, y en madera, adicionalmente el usuario suele introducirles el equipamiento necesario para que puedan navegar. 

Adicionalmente, están los kits de preparación de barcos, que vienen listos para que las piezas sean montadas.

El material más difundido en el modelismo naval es el plástico inyectado. Los fabricantes ofrecen kits en cajas conteniendo planchas de plástico con piezas desglosables las cuales generalmente se entregan sin pintar y en color base gris claro. La asociación internacional que agrupa a este tipo de modelismo es la IPMS.

La madera se usa generalmente para crear modelos de barcos de vela fabricados en la realidad en madera como el galeón o el navío. Se pueden construir a partir de kits de fabricantes o bien partiendo de planos distribuidos por fabricantes o museos y creando uno mismo las piezas de madera de manera artesanal. Se complementan los modelos con piezas de latón, fundición u otros metales para cañones, mascarones de proa o decoraciones de popa. El modelismo en madera es uno de los más admirados por el público en general por la complejidad del trabajo y la vistosidad de las velas y jarcias.

Otro material que es usado es el papel, los diseñadores hacen las piezas de los barcos en papel a manera de planos pero con la ventaja de que ya vienen en color para recortar, doblar y pegar. Si bien la mayoría de modelos terminados no obtiene un nivel de presentación superior a los materiales antes mencionados si existen algunas firmas como GPM que producen estos papermodels con un nivel de detalle extraordinario, incluso logrando superar al plástico y la madera pero están diseñados para modelistas de papel expertos. El punto fuerte del modelismo en papel es el bajo costo de los kits, encontrándose muchos gratuitos en Internet como por ejemplo los de la firma Total Navy. 

Uno de los países con mayor producción y difusión de los papermodels es Polonia, que durante años post-guerras tuvo restricciones en el uso del plástico, por lo que la creatividad de los modelistas polacos logró evolucionar este tipo de modelismo. Otro tipo de materiales usados son los cerillos de fósforos, resina y poliestireno. 

El llamado Scratchbuild es considerado uno de los retos más desafiantes en el modelismo naval ya que no parte de piezas pre-fabricadas sino que usa todo tipo de materiales como fibra de vidrio o láminas de plástico, además de metal, alambre, etc. A diferencia de la madera que representa generalmente barcos que eran de ese material, el scratchbuild representa navíos de acero.

Para aumentar el detalle de los barcos a escala se cuenta con calcas, banderas y piezas de foto grabados (photoetched) que son láminas de metal con piezas forjadas en ese material con más nivel de detalle que las de plástico.

En casi todos los museos navales del mundo se encuentran colecciones de barcos a escala, existiendo también colecciones privadas de coleccionistas que compran los barcos hechos por otros modelistas. A nivel mundial hay 2 colecciones creadas por modelistas que destacan:




</doc>
<doc id="16883" url="https://es.wikipedia.org/wiki?curid=16883" title="Milímetro cuadrado">
Milímetro cuadrado

El "milímetro cuadrado" es la superficie que ocupa un cuadrado de un milímetro de lado. Equivale a una millonésima parte de un metro cuadrado.




</doc>
<doc id="16884" url="https://es.wikipedia.org/wiki?curid=16884" title="Microorganismo">
Microorganismo

Un microorganismo, también llamado microbio (del griego científico μικρόβιος ["microbios"]; de μικρός ["micrós"], "pequeño", y βίος ["bíos"], ‘vida’; "ser vivo diminuto"), es un ser vivo o un sistema biológico que solo puede visualizarse con el microscopio. Son organismos dotados de individualidad (unicelulares) que presentan, a diferencia de las plantas y los animales superiores, una organización biológica elemental. La disciplina científica que estudia los microorganismos es la Microbiología. 

El concepto de microorganismo es operativo y carece de cualquier implicación taxonómica o filogenética, dado que engloba organismos unicelulares heterogéneos, que no están relacionados evolutivamente entre sí, tales como bacterias (procariotas), protozoos (eucariotas, algunos filum de algas) y hongos unicelulares. Incluye también entidades biológicas acelulares de tamaño ultramicroscópico (visibles con microscopio electrónico) como los virus que también se incluyen en el campo de estudio de la Microbiología.

Los microbios tienen múltiples formas y tamaños. Si un virus de tamaño promedio tuviera el tamaño de una pelota de tenis, una bacteria sería del tamaño de media cancha de tenis y una célula eucariota sería como un estadio entero de fútbol.

Algunos microorganismos son patógenos y causan enfermedades a personas, animales y plantas, algunas de las cuales han sido un azote para la humanidad desde tiempos inmemoriales. No obstante, la inmensa mayoría de los microbios no son en absoluto perjudiciales y bastantes juegan un papel clave en la biosfera al proporcionar oxígeno (algas y cianobacterias), y, otros, descomponer la materia orgánica, mineralizada y hacerla de nuevo accesible a los productores, cerrando el ciclo de la materia.

Antonie van Leeuwenhoek (1632–1723) fue uno de los primeros en observar los microorganismos, utilizando microscopios de diseño propio. Robert Hooke, un contemporáneo de Leeuwenhoek, también utilizó microscopios para observar la vida microbiana; en su libro de 1665, "Micrographia" describió esas observaciones y acuñó el término de célula.

Antes del descubrimiento de los microorganismos de Leeuwenhoek en 1675, había sido un misterio por qué las uvas podían convertirse en vino, la leche en queso, o por qué los alimentos se echaban a perder. Leeuwenhoek no hizo la conexión entre estos procesos y los microorganismos, pero usando un microscopio estableció que había allí signos de vida que no eran visibles a simple vista. El descubrimiento de Leeuwenhoek, junto con las observaciones posteriores de Spallanzani y Pasteur, terminaron con la antigua creencia de que la vida aparecía espontáneamente a partir de sustancias muertas durante el proceso de deterioro.

Lazzaro Spallanzani (1729–1799) encontró que hirviendo caldo lo esterilizaba, matando a los microorganismos en él. También encontró que los nuevos microorganismos sólo podían instalarse en un caldo si el caldo se exponía al aire.

Louis Pasteur (1822–1895) amplió los hallazgos de Spallanzani mediante la exposición de caldos hervidos al aire, en recipientes que contenían un filtro que evitaba que cualquier partícula pasara al medio de crecimiento, y también en recipientes sin ningún filtro, que admitían aire a través de un tubo curvado que no permitía que las partículas de polvo entrasen en contacto con el caldo. Hirviendo el caldo con antelación, Pasteur se aseguró de que no había [microorganismos] supervivientes en los caldos al comienzo del experimento. Nada crecía en los caldos en el curso del experimento de Pasteur. Esto significaba que los organismos vivos que crecían en estos caldos venían desde afuera, como esporas en polvo, en lugar de generarse espontáneamente en el caldo. Por lo tanto, Pasteur dio el golpe a la teoría de la generación espontánea, dando apoyo a la teoría microbiana de la enfermedad.

En 1876, Robert Koch (1843–1910) estableció que los microorganismos pueden causar enfermedades. Encontró que la sangre del ganado que estaba infectado con ántrax siempre tenía un gran número de "Bacillus anthracis".

Koch descubrió que podía transmitir el ántrax de un animal a otro, tomando una pequeña muestra de sangre del animal infectado e inyectándola en uno sano, que hacía que el animal enfermase. También descubrió que podía hacer crecer la bacteria en un caldo nutriente, luego lo inyectaba en un animal sano, y causaba la enfermedad. Basándose en estos experimentos, ideó los criterios para establecer una relación causal entre un microorganismo y una enfermedad, ahora conocidos como los postulados de Koch. Aunque estos postulados no pueden aplicarse en todos los casos, conservan su importancia histórica en el desarrollo del pensamiento científico y todavía se utilizan hoy.

El 8 de noviembre de 2013 se informó del descubrimiento de lo que pueden ser los primeros signos de vida en la Tierra: los fósiles completos más antiguos de una estera microbiana (asociada con arenisca en Australia occidental) que se estima que tienen 3480 millones de años.

En los microorganismos están representados cinco grupos de seres: bacterias, arqueas, protozoos, hongos y virus.

Los virus son sistemas biológicos que presentan incluso tamaños ultramicroscópicos (los más pequeños y los de tamaños medianos solo se pueden observar mediante microscopio electrónico), los cuales pueden causar infecciones y solo se reproducen en células huésped. Los virus fuera de células huésped están en forma inactiva. Los virus constan de una cubierta protectora proteica o cápside que rodea el material genético. Su forma puede ser espiral, esférica o como células pequeñas, de tamaño entre 10 y 300 nm. Al tener un tamaño menor que las bacterias, pueden pasar filtros que permiten la retención de las mismas.

Al contrario que las bacterias y los protozoos parásitos, los virus contienen un solo tipo de ácido nucleico (ARN o ADN). No se pueden reproducir por sí solos, sino que necesitan de la maquinaria metabólica de la célula huésped para asegurar que su información genética pase a la siguiente generación.

Al contrario que las bacterias, los virus no están presentes en el ser humano de manera natural (excepto como un elemento viral endógeno). Cuando las personas quedan afectadas por un virus, estos generalmente se eliminan del cuerpo humano mediante secreciones.

En las últimas décadas se han empezado a utilizar virus en medicina, por ejemplo para la debilitación de bacterias, la creación de antitoxinas, la utilización para librerías genómicas, como vectores en terapia génica, para la destrucción de células tumorales

Las bacterias y las arqueas son microorganismos procariontes de forma esférica (cocos), de bastón recto (bacilos) o curvado (vibrios), o espirales (espirilos). Pueden existir como organismos individuales, formando cadenas, pares, tétradas, masas irregulares, etc. Las bacterias son una de las formas de vida más abundantes en la tierra. Tienen una longitud entre 0,4 y 14 μm. Consecuentemente solo se pueden ver mediante microscopio. Las bacterias se reproducen mediante la multiplicación del ADN, y división en dos células independientes; en circunstancias normales este proceso dura entre 30 y 60 minutos.

Cuando las condiciones del medio son desfavorables, cuando cambia la temperatura o disminuye la cantidad de los nutrientes, determinadas bacterias forman endosporas como mecanismo de defensa, caracterizadas por presentar una capa protectora resistente al calor, a la desecación, a la radiación y a la trituración mecánica y que protege la bacteria de manera muy eficiente. De esta manera, pueden soportar temperaturas elevadas, periodos de sequía, heladas, etc. Cuando las condiciones del medio mejoran, se desarrolla una nueva bacteria que continúa el crecimiento y la multiplicación.

Las bacterias tienen un papel funcional ecológico específico. Por ejemplo, algunas realizan la degradación de la materia orgánica, otras integran su metabolismo con el de los seres humanos.

Si bien algunas bacterias son patógenas (causantes de diversas enfermedades), una gran parte de ellas son inocuas o incluso buenas para la salud.

Se denomina eucariotas a todas las células que tienen su material hereditario (su información genética) encerrado dentro de una doble membrana, la envoltura nuclear, que delimita un núcleo celular.

Hay tres tipos de microorganismos eucariotas, los protozoos (heterótrofos y sin pared celular), las algas microscópicas (autótrofos y con pared celular de celulosa) y los hongos microscópicos (heterótrofos y con pared celular de quitina).

Los protozoos son microorganismos unicelulares eucarióticos cuyo tamaño va de 10-50 μm hasta más de 1 milímetro, y pueden fácilmente ser vistos a través de un microscopio. Son heterótrofos, fagótrofos, depredadores o detritívoros, a veces mixótrofos (parcialmente autótrofos), que viven en ambientes húmedos o directamente en medios acuáticos, ya sean aguas saladas o aguas dulces. La reproducción puede ser asexual por bipartición y también sexual por isogametos o por conjugación intercambiando material genético. En este grupo encajan taxones muy diversos con una relación de parentesco remota, que se encuadran en muchos filos distintos del reino protista, definiendo un grupo polifilético, sin valor en la clasificación de acuerdo con los criterios actuales.

El reino Fungi incluye una variedad de especies macroscópicas que en absoluto encajan en la definición de microorganismo, pero también forma microscópicas, como las levaduras, que son campo de estudio de la microbiología. Además, numerosos hongos producen enfermedades infecciosas en animales y plantas y tienen un gran interés sanitario y agropecuario.

Algunos microorganismos son capaces de penetrar y multiplicarse en otros seres vivos, a los que perjudican, originando una infección; son los denominados microorganismos patógenos. Los problemas que causa una infección dependen del tipo de patógeno, el modo en que se transfiere, dosis o concentración de patógenos, persistencia de los microorganismos y la resistencia del organismo infectado.

La dosis de infección significa el número de microorganismos. Esta dosis es muy baja para los virus y protozoos parásitos. La persistencia de los microorganismos depende del tiempo viable de los microorganismos cuando no se encuentran en el huésped humano. Por ejemplo, las bacterias son generalmente menos persistentes mientras los quistes de los protozoos son los más persistentes.

Los jóvenes, personas mayores y enfermos de otras patologías son los menos resistentes a las enfermedades y por lo tanto son más vulnerables. Cuando una persona es infectada, los patógenos se multiplican en ella, y esto supone un riesgo de infección o enfermedad.

Las personas que enferman pueden contagiar y extender la enfermedad mediante las secreciones y mediante contacto directo de alguna manera con la mucosa del infectado.

Existen dos grandes clasificaciones en cuanto a los métodos de cultivo de microorganismos: aerobios y anaerobios. Normalmente, se incuban en condiciones aerobias, es decir, en condiciones atmosféricas normales; esta técnica es la más sencilla. Con ella proliferan del mismo modo microorganismos aerobios y anaerobios facultativos. Sin embargo, algunas bacterias aisladas tan solo se reproducen en condiciones de estricta anaerobiosis. Así pues, hay que recurrir a un medio de cultivo en el que previamente ha sido eliminado todo el oxígeno atmosférico y ha sido substituido por otro gas (nitrógeno).



</doc>
<doc id="16885" url="https://es.wikipedia.org/wiki?curid=16885" title="Micromagia">
Micromagia

Aunque en la definición de ilusionismo se hable de micromagia como magia a muy corta distancia y no más de 4 personas alrededor del mago", en realidad el concepto de micromagia hace referencia a magia con objetos pequeños, y de esto se deriva el nombre "micro = pequeño". Habitualmente para este tipo de magia se utilizan objetos comunes de los que se pueden encontrar en una casa como son: palillos, clips, imperdibles, cerillas, monedas (aunque tengan un tipo específico denominado numismagia), papeles, billetes, agujas, gomas elásticas, lápices, etc. La consecuencia es que debido al tamaño de los objetos, solo se trabaja para pocas personas; aunque mediante el uso de cámaras en televisión y en ciertos locales se hacen sesiones de micromagia para multitud de espectadores mediante la visión en pantalla.

Véase también:


</doc>
<doc id="16886" url="https://es.wikipedia.org/wiki?curid=16886" title="Michel Gordillo">
Michel Gordillo

Miguel Ángel Gordillo, más conocido como Michel Gordillo, nació en 1955 en Duala, Camerún. Fue comandante de Iberia pilotando Airbus A319, A320 y A321. En total tiene más de 15 000 horas de vuelo. También es piloto de vuelo a vela y tiene el título C de plata de esta modalidad. Habla correctamente español, francés e inglés.Actualmente, vive en Campo Real.

Se formó en el Ejército del Aire como componente de la XXXII Promoción de la Academia General del Aire de San Javier. Pilotó durante 7 años aviones P-3 Orion en la patrulla marítima y más tarde Falcon 20 en el Escuadrón del Rey transportando a importantes autoridades españolas —la familia real, ministros, etc—. En 1982 obtuvo la graduación en el Curso de Navegador Avanzado en la Base Aérea de Mather en Sacramento, California.

En 1987 iba a ser ascendido a comandante, lo que suponía en la práctica dejar de volar y realizar tareas de despacho. Por ello, abandonó el ejército y empezó a trabajar para Iberia como copiloto de DC-9 y posteriormente de MD-87 y A340. En 1998 fue ascendido a comandante y empezó a pilotar Airbus A319, A320 y A321.

En 1998 fue el primer piloto que voló en un ultraligero —un Kitfox IV— desde Madrid hasta Oshkosh, Wisconsin, Estados Unidos. Este viaje hizo historia porque recorrió la ruta larga, por el este, atravesando Europa, Asia, Rusia y Canadá.

En el verano de 2001 dio la vuelta al mundo en un avión monoplaza experimental MCR01 construido por él mismo en el garaje de su casa. El vuelo duró 44 días y fue el primer español en realizar tal hazaña. El viaje comenzó el 19 de junio, finalizó el 1 de agosto y constó de las siguientes escalas: Salamanca, Cuatro Vientos y San Javier (España), Túnez, Sicilia (Italia), Corfú (Grecia), Alejandría (Egipto), Dhahran (Arabia Saudí), Emiratos Árabes, Omán, Karachi (Pakistán), Nagpur y Bangladesh (India), Mandalay (Birmania), Udon Thani (Tailandia), Camboya, Vietnam, Manila (Filipinas), Japón, Isla Shemya (Aleutianas), Cold Bay (Estados Unidos), Anchorage (Alaska), Seattle (Estados Unidos), Canadá, Oshkosh (Estados Unidos), Groenlandia, Islandia, Inglaterra, Francia y finalmente Salamanca.

El 5 de enero de 2006, actuando como comandante del vuelo 161 de Iberia, se negó a despegar por razones de seguridad al no haber sido reparado un detector de fuego del avión. Como consecuencia de ello fue despedido. En el procedimiento judicial subsiguiente, cuya vista fue celebrada el 22 de junio —tras un aplazamiento inicial— el despido se declaró improcedente. A pesar de ello, a fecha de 15 de noviembre de 2006, Michel Gordillo seguía sin ser readmitido en Iberia.

Se jubiló a los 58 años de edad.

A principios de 2014, Michel Gordillo creó el proyecto Sky Polaris con el apoyo del Ejército del Aire, así como de otras fuerzas aéreas extranjeras, el Comité Polar Español y el Centro Andaluz de Medio Ambiente. Este tenía como objetivo realizar un vuelo en un "Van´s Aircraft" RV-8 alrededor del mundo pasando por los polos Norte y Sur, mientras se estudian los efectos de las partículas de carbono negro (hollín)en la atmósfera. Ha sido la primera vez que un avión de menos de 1500 kg sobrevuele los polos. La fecha de partida estaba programada para el 15 de noviembre de 2015, pero tuvo que retrasarse debido a problemas legales con la obtención de los permisos para sobrevolar la Antártida.

El viaje empezó en Madrid y realizó escalas en [Jerez (España), [Dakar] (Senegal), Natal(Brasil)Manaos (Brasil), Medellín (Colombia), México, D. F. (México), Bahamas, Winsor (Canada) y Resolute (Canadá), Longyearbyen y Ålesund (Noruega), Wurzburgo (Alemania) y nuevamente llegando a Madrid- Cuatro Vientos. , Malta, Egipto, Jartum (Sudan), Kenia, Gan Maldivas, Isla del Coco, Learmonth, Perth, Ayers Rock y Hobart (Australia), Mario Zucchelli y Marambio (Antártida), Ushuaia (Argentina), e Iguazú (Argentina), São Paulo, Brasilia, Natal y Praia (Cabo Verde) y via Lanzarote a Madrid Cuatro Vientos


</doc>
<doc id="16887" url="https://es.wikipedia.org/wiki?curid=16887" title="Michael Palin">
Michael Palin

Sir Michael Edward Palin, KCGM CBE (Sheffield, Yorkshire; 5 de mayo de 1943) es un actor, guionista, comediante y presentador de televisión británico.

Estudió en la secundaria Shrewsbury School donde coincidió con el futuro DJ de Radio London y de la BBC, John Peel. Cursó estudios de Historia en la Universidad de Oxford. Se casó con Helen Gibbins con quien tuvo tres hijos. Uno de ellos, Thomas, realizó el papel de Sir Galahad en la película "Los caballeros de la mesa cuadrada". Fue miembro del grupo humorístico Monty Python.

El "Python agradable", es, siguiendo a John Cleese y Eric Idle, el Python más conocido por su trabajo como actor. Tomó parte en "Un pez llamado Wanda" y su "secuela" "Criaturas feroces" al lado de Kevin Kline, Jamie Lee Curtis y John Cleese. Actuó como secundario en la película de culto "Brazil" dirigida por su antiguo compañero en los Monty Python Terry Gilliam en 1985. 

También presentó varias series acerca de viajes para la BBC. Participó con John Cleese en algunos de los mejores sketches de Monty Python's Flying Circus: El Ministerio de Andares Tontos, el Loro muerto o el Centro de discusión. Realizó los papeles de Bevis, el barbero medio psicópata travestido que quería ser leñador en el sketch "La Canción del Leñador" y de Sir Galahad en Los caballeros de la mesa cuadrada. Aparecía al principio de cada episodio de "Monty Python's Flying Circus" como el náufrago que decía "It's...".

En 2019, fue condecorado como Caballero comendador de la Orden de San Miguel y San Jorge por sus contribuciones a la cultura. Palin es el único de los Monty Python en recibir el título de caballero, (John Cleese había rechazado un CBE en 1996, calificándolo de "demasiado tonto"). 


Todos sus libros de viajes se pueden leer de forma gratuita, completa e íntegra, en su página web.











</doc>
<doc id="16889" url="https://es.wikipedia.org/wiki?curid=16889" title="Metaplasia">
Metaplasia

En histología, se llama metaplasia a la transformación citológica de un epitelio maduro en otro que puede tener un parentesco próximo o remoto. Los fenómenos de metaplasia son completamente normales en los tejidos embrionarios que tienden naturalmente a diversificar, madurar y especializar sus células. También tienen lugar a partir de células madre totipotenciales o pluripotenciales, según se hable de tejidos embrionarios o adultos. En ciertas ocasiones la metaplasia implica una regresión en la especialización o maduración de las células hacia formas más primitivas para más tarde madurar hacia otra clase de células. La metaplasia puede presentarse como una respuesta adaptativa fisiológica frente al estrés celular y es reversible una vez cesa el estímulo agresor. No se considera una lesión neoplásica o premaligna. La metaplasia más común es la de epitelio columnar a epitelio escamoso.

Sin embargo, es conocido en medicina humana que la zona de transición entre el tejido normal y el metaplásico por ser una zona muy activa mitóticamente puede ser el asiento para la generación de células displásicas con el consiguiente potencial de malignidad.

En medicina existe una controversia en considerar la metaplasia intestinal del esófago (esófago de Barrett) como lesión premaligna, pero en líneas generales se considera como un mecanismo adaptativo que revierte con el cese del estímulo. Si el estímulo agresor persiste puede inducir la transformación neoplásica a partir de mutaciones en el genoma de la célula existiendo diversos marcadores de mutación como inactivación del gen P16 y p53.



</doc>
<doc id="16890" url="https://es.wikipedia.org/wiki?curid=16890" title="Mesopotamia">
Mesopotamia

Mesopotamia (del "Mesopotamia" ‘tierra entre dos ríos’, árabe "bilād al-rāfidayn", traducción del persa antiguo "Miyanrudan" ‘la tierra entre ríos’, o del siríaco ܒܝܬ ܢܗܪܝܢ "beth nahrin" ‘entre dos ríos’) es el nombre por el cual se conoce a la zona del Oriente Próximo ubicada entre los ríos Tigris y Éufrates, si bien se extiende a las zonas fértiles contiguas a la franja entre ambos ríos, y que coincide aproximadamente con las áreas no desérticas del actual Irak y la zona limítrofe del norte-este de Siria.

El término alude principalmente a esta zona en la Edad Antigua que se dividía en Asiria (al norte) y Babilonia (al sur). Babilonia (también conocida como Caldea), a su vez, se dividía en Acadia (parte alta) y Caldea (parte baja). Sus gobernantes eran llamados patesi.

Los nombres de ciudades como Ur o Nippur, de héroes legendarios como Gilgameš, del Código Hammurabi, de los asombrosos edificios conocidos como zigurats, provienen de Mesopotamia Antigua. Y episodios mencionados en la Biblia o en la Torá, como los del diluvio universal o la leyenda de la Torre de Babel, aluden a hechos ocurridos en esta zona.

La historia de Mesopotamia está dividida en 5 etapas: periodo sumerio, Imperio acadio, Imperio babilónico, Imperio asirio e Imperio neobabilónico.

El sistema social estaba ligado a la economía, por lo que no había castas ni estratificación, solo diferenciación en las posiciones económicas.

La economía de Mesopotamia se basaba en la agricultura y la división de tierras de la siguiente forma:

También podían encontrarse las siguientes distinciones socio-económicas dentro de la población, lo cual estaba sujeto a su nivel de dependencia o independencia económica: 

El topónimo regional "Mesopotamia" (/m ɛ s ə p ə t eɪ m i ə/, griego antiguo: Μεσοποταμια "[la tierra] entre ríos"; árabe: Balad ٱ lrafdyn Bilad 'ar-Rafidayn' o árabe: Internacional ٱ lnhryn 'AN-Nahrayn Bayn'; persa: myanrvdan miyan "Rudan"; siríaco: ܒܝܬ ܢܗܪܝܢ"Beth Nahrain" "tierra de ríos") proviene de las antiguas palabras griegas μέσος ("mesos") "medio" y ποταμός ("potamos") "río" y se traduce como "(tierra) entre ríos". Se utiliza en toda la Septuaginta griega (c. 250 a. C.) para traducir el hebreo y el arameo equivalente "Naharaim". Un uso griego anterior del nombre "Mesopotamia" es evidente en "La anabasis de Alejandro", que se escribió a fines del siglo II d. C., pero se refiere específicamente a las fuentes de la época de Alejandro Magno. En el "Anabasis", Mesopotamia se utilizó para designar la tierra al este del Éufrates, en el norte de Siria.

El término arameo "biritum / birit narim" correspondía a un concepto geográfico similar. Más tarde, el término Mesopotamia se aplicó de manera más general a todas las tierras entre el Éufrates y el Tigris, incorporando así no solo partes de Siria, sino también casi todo el Iraq y el sureste de Turquía. Las estepas vecinas al oeste del Éufrates y la parte occidental de las montañas Zagros también se incluyen a menudo bajo el término más amplio Mesopotamia. 

Por lo general, se hace una distinción adicional entre Mesopotamia norte o superior y Mesopotamia sur o inferior. La Alta Mesopotamia, también conocida como "Jazira", es el área entre el Éufrates y el Tigris desde sus fuentes hasta Bagdad, mientras que la Baja Mesopotamia es el área desde Bagdad hasta el Golfo Pérsico e incluye Kuwait y partes del oeste de Irán. 

En el uso académico moderno, el término "Mesopotamia a" menudo también tiene una connotación cronológica. Por lo general, se usa para designar el área hasta las conquistas musulmanas, con nombres como "Siria", "Jazira" e "Irak" para describir la región después de esa fecha. Se ha argumentado que estos eufemismos posteriores son términos eurocéntricos atribuidos a la región en medio de varias invasiones occidentales del siglo XIX.

"Artículo principal:" Geografía de Mesopotamia

Mesopotamia abarca la tierra entre los ríos Éufrates y Tigris, los cuales tienen sus cabeceras en las montañas Taurus. Ambos ríos son alimentados por numerosos afluentes, y todo el sistema fluvial drena una vasta región montañosa. Las rutas terrestres en Mesopotamia generalmente siguen al Éufrates porque las orillas del Tigris son con frecuencia empinadas y difíciles. El clima de la región es semiárido con una vasta extensión desértica en el norte que da paso a una región de pantanos, lagunas, marismas y bancos de caña de 15 000 kilómetros cuadrados (5 800 millas cuadradas) en el sur. En el extremo sur, el Éufrates y el Tigris se unen y desembocan en el Golfo Pérsico.

El ambiente árido que abarca desde las áreas del norte de la agricultura de secano hasta el sur, donde el riego de la agricultura es esencial para obtener un excedente de energía en la energía invertida (EROEI). Este riego es ayudado por una capa freática alta y por el deshielo de las altas cumbres de las montañas del norte de Zagros y de las tierras altas armenias, la fuente de los ríos Tigris y Éufrates que dan nombre a la región. La utilidad del riego depende de la capacidad de movilizar mano de obra suficiente para la construcción y mantenimiento de canales, y esto, desde el primer período, ha ayudado al desarrollo de asentamientos urbanos y sistemas centralizados de autoridad política.

La agricultura en toda la región se ha complementado con el pastoreo nómada, donde los nómadas que vivían en tiendas de campaña pastorearon ovejas y cabras (y luego camellos) de los pastizales del río en los meses secos de verano, hacia tierras de pastoreo estacionales en la franja del desierto en la estación húmeda de invierno. El área generalmente carece de piedra de construcción, metales preciosos y madera, por lo que históricamente se ha basado en el comercio de productos agrícolas a larga distancia para asegurar estos artículos de las áreas periféricas. En las marismas al sur del área, ha existido una compleja cultura de pesca a base de agua desde la prehistoria, y se ha agregado a la mezcla cultural.

Se han producido interrupciones periódicas en el sistema cultural por varias razones. De vez en cuando, la demanda de mano de obra ha llevado a aumentos de la población que superan los límites de la capacidad de carga ecológica, y en caso de que se produzca un período de inestabilidad climática, puede colapsar el gobierno central y disminuir las poblaciones. Alternativamente, la vulnerabilidad militar a la invasión de las tribus de las montañas marginales o los pastores nómadas ha llevado a períodos de colapso comercial y negligencia de los sistemas de riego. Igualmente, las tendencias centrípetas entre los Estados de la ciudad han significado que la autoridad central sobre toda la región, cuando se impone, tiende a ser efímera, y el localismo ha fragmentado el poder en unidades tribales o unidades regionales más pequeñas. Estas tendencias han continuado hasta nuestros días en Iraq.

En el interior de Mesopotamia, la agricultura y la ganadería se impusieron entre el 6000 y el 5000 a. C., suponiendo la entrada de lleno al Neolítico. Durante este período, las nuevas técnicas de producción que se habían desarrollado en el área neolítica inicial se expandieron por las regiones de desarrollo más tardío, entre ellas Mesopotamia interior. Este hecho conllevó el desarrollo de las ciudades, siendo algunas de las primeras Bouqras, Umm Dabaghiyah y Yarim y, más tardíamente, Tell es-Sawwan y Choga Mami, que formaron la llamada cultura Umm Dabaghiyah. Posteriormente ésta fue sustituida por las culturas de Hassuna-Samarra, entre el 5000 y el 5600 a. C., y por la cultura Halaf entre el 5600 y el 4000 a. C. (Halaf tardío).

Aproximadamente en el 3000 a. C., apareció la escritura, en aquella época utilizada solo para llevar las cuentas administrativas de la comunidad. Los primeros escritos que se han hallado están grabados sobre arcilla (muy frecuente en aquella zona) con unos dibujos formados por líneas (pictogramas).

La civilización urbana siguió avanzando durante el período de El Obeid(5000 a. C.–3700 a. C.) con avances en las técnicas cerámicas y de regadío y la construcción de los primeros templos urbanos.

Tras El Obeid, se sucede el Período de Uruk, en el cual la civilización urbana se asentó definitivamente con enormes avances técnicos como la rueda y el cálculo, realizado mediante anotaciones en tablillas de barro y que evolucionaría hacia las primeras formas de escritura.

La sumeria fue la primera civilización mesopotámica. Después del año 3000 a. C. los sumerios crearon en la baja Mesopotamia un conjunto de ciudades-Estado: Uruk, Lagaš, Kiš, Uma, Ur, Eridu y Ea cuya economía se basaba en el regadío. En ellas gobernaba un rey absoluto, que se hacía llamar «vicario» del dios protector de la ciudad. Los sumerios fueron los primeros en utilizar la escritura (escritura cuneiforme) y también construyeron grandes templos (zigurat).

La difusión de los avances de la cultura de Uruk por el resto de Mesopotamia meridional dio lugar al nacimiento de la cultura sumeria. Estas técnicas permitieron la proliferación de las ciudades por nuevos territorios y regiones. Estas ciudades pronto se caracterizaron por la aparición de murallas, lo que parece indicar que las guerras entre ellas fueron frecuentes. También destaca la expansión de la escritura que saltó desde su papel administrativo y técnico hasta las primeras inscripciones dedicatorias en las estatuas consagradas de los templos.

Pese a la existencia de las listas reales sumerias la historia de este período es relativamente desconocida, ya que gran parte de los reinados expuestos en ellas tienen fechas imposibles. En realidad, estas listas se confeccionaron a partir del siglo XVII a. C., y su creación se debió probablemente al deseo de los monarcas de remontar su linaje hasta tiempos épicos. Algunos de los reyes son probablemente reales pero de muchos otros no hay constancia histórica y otros de los que se sabe su existencia no figuran en ellas.

La prosperidad de los sumerios atrajo a diversos pueblos nómadas. Desde la península arábiga, las tribus semitas (árabes, hebreos y sirios) invadieron constantemente la región mesopotámica a partir del 2500 a.C., hasta que establecieron su dominio definitivo.

Hacia 3000 a. C. se extendieron hacia el norte, creando diferentes grupos como los amorreos, en los que se incluyen fenicios, israelitas y arameos. En Mesopotamia el pueblo semita que adquirió mayor relevancia fueron los acadios.

Hacia 2350 a. C., Sargón, un usurpador de origen acadio, se hizo con el poder en la ciudad de Kiš. Fundó una nueva capital, Agadé y conquistó el resto de ciudades sumerias, venciendo al rey de Umma hasta entonces dominante, Lugalzagesi. Este fue el primer gran Imperio de la historia y sería continuado por los sucesores de Sargón, que se tendrían que enfrentar a constantes revueltas. Entre ellos destacó el nieto del conquistador, Naram-Sin. Esta etapa marcó el inicio de la decadencia de la cultura e idioma sumerios en favor de los acadios.

El Imperio se deshizo hacia el 2220 a. C., debido a las constantes revueltas y las invasiones de los nómadas gutis y amorreos. Tras su caída, la región entera cayó bajo el dominio de esta tribu, que se impuso sobre las ciudades-Estado de la región, especialmente en el entorno de la destruida Agadé. Las crónicas sumerias los describen constantemente de forma negativa, como "horda de bárbaros" o "dragones de montaña", pero es posible que la realidad no fuese tan negativa; en algunos centros se produjo un verdadero florecimiento de las artes, como la ciudad de Lagaš por ejemplo, especialmente durante el gobierno del "patesi" Gudea. Además de la calidad artística, en las obras de Lagaš se utilizaron materiales provenientes de regiones lejanas: madera de cedro del Líbano o diorita, oro y cornalina del valle del Indo; lo que parece indicar que el comercio no se debió ver especialmente lastrado. Las ciudades meridionales, más alejadas del centro de poder guti, compraban su libertad a cambio de importantes tributos; Uruk y Ur prosperaron durante sus IV y II dinastías.

Según una tablilla conmemorativa fue Utu-hegal, rey de Uruk, quien en torno a 2100 a. C., derrotó y expulsó a los gobernantes gutis de las tierras sumerias. Su éxito no le sería de mucho provecho ya que poco después fue vencido por Ur-Nammu, el rey de Ur, que pasó a ser la ciudad hegemónica en toda la región durante el período de la Tercera Dinastía de Ur (también se suele denominar a este período Renacimiento sumerio). El Imperio surgido a raíz de esta hegemonía sería tan extenso o más que el de Sargón, del que tomaría la idea de Imperio unificador, influencia que se aprecia incluso en la denominación de los monarcas, que a imitación de los acadios se harán llamar "reyes de Sumeria y Acad".

A Ur-Nammu le sucederá su hijo Shulgi, quien combatió contra el reino oriental de Elam y las tribus nómadas de los Zagros. A éste le sucedió su hijo Amar-Sin y a éste, primero un hermano suyo, Shu-Sin y después otro Ibbi-Sin. En el reinado de este último los ataques de los amorreos, provenientes de Arabia, se hicieron especialmente fuertes y en el 2003 a. C. cayó el último Imperio predominantemente sumerio. En adelante será la cultura acadia la que predomine y posteriormente Babilonia heredará el papel de los grandes imperios sumerios.

Con la caída de la hegemonía de Ur no se repitió un período de oscuridad como el que había acontecido con la del Imperio acadio. Esta etapa estará marcada por el ascenso progresivo de dinastías amorritas en prácticamente todas las ciudades de la región.

Durante los primeros 50 años parece que fue la ciudad de Isin la que trató sin éxito de imponerse en la región. Posteriormente, hacia 1930 a. C. serán los monarcas de Larsa los que se lancen a la conquista de las ciudades vecinas, atacando Elam y las ciudades del Diyala y conquistando Ur, pese a lo cual no consiguieron un dominio completo en la región, aunque conservaron su hegemonía hasta prácticamente el surgimiento del Imperio paleobabilónico de Hammurabi, salvo un período entre 1860 y 1803 a. C. en el que la vecina Uruk consiguió desafiar su liderazgo.

En Elam la influencia acadia se hizo más fuerte y el reino pasó a inmiscuirse cada vez más en la política mesopotámica. En Mesopotamia septentrional empezaron a surgir los primeros Estados fuertes, posiblemente reformados por el comercio existente entre las áreas meridionales y Anatolia, destacando principalmente el nuevo reino de Asiria, el cual llegaría a expandirse hasta el Mediterráneo bajo el reinado de Šamši-Adad I.

En 1792 a. C. Hammurabi llega al trono de la hasta entonces poco importante ciudad de Babilonia, a partir de la cual comenzará una política de expansión; en primer lugar se liberó de la tutela de Ur para, en 1786, enfrentarse al vecino rey de Larsa, Rim-Sin I, arrebatándole Isin y Uruk; con la ayuda de Mari, en 1762 venció a una coalición de ciudades de la ribera del Tigris, para, un año después, conquistar la ciudad de Larsa. Tras esto se autoproclamó como "rey de Sumeria y Acad", título que había surgido en tiempos de Sargón de Acad, y que se había venido utilizando por los monarcas que conseguían el dominio de toda la región de Mesopotamia. Tras un nuevo enfrentamiento con una nueva coalición de ciudades conquistó Mari, tras lo cual, en 1753, completó su expansión con la anexión de Asiria y Ešnunna, al norte de Mesopotamia.

Con el paso de los siglos la imagen del monarca fue mitificada, no solo debido a sus conquistas, sino también a su actividad constructora y de mantenimiento de los canales de riego, y a la elaboración de códigos de leyes, como el conocido código de Hammurabi. 

Hammurabi murió en 1750 a. C., siendo sucedido por su hijo Samsu-iluna, quien tuvo que enfrentarse a un ataque de los nómadas casitas. Esta situación se repetiría en 1708 a.C., durante el reinado de Abi-Eshuh. En efecto, desde la muerte del conquistador, los problemas con los casitas se habían multiplicado. Esta presión fue constante y en progreso durante el siglo XVII a. C., lo que fue desgastando el Imperio. Fue un ataque del rey hitita, Mursili I, lo que le dio el golpe de gracia a Babilonia, tras lo cual la región cayó bajo el poder de los casitas.

Hacia el se establecieron en el norte de Babilonia los asirios, quienes tomaron el control de todo el país. Sus ciudades más importantes fueron Assur y Nínive, y entre sus monarcas más ilustres destacaron: Assurnasirpal, Assurbanipal, Salmanasar III, Sargón II y Senaquerib. Babilónicos y medos se aliaron y entraron a Asiria desde la meseta de Irán, y finalmente, en el año 612 a. C. tomaron e incendiaron Nínive.

Babilonia resurgió con los caldeos, otra tribu semita, cuando fue refundada por su rey Nabopolasar, a finales del siglo VII. Su hijo, Nabucodonosor II "el Grande", fue su sucesor y es considerado uno de los reyes babilónicos más importantes pues sus dominios llegaron desde Mesopotamia hasta Siria y la costa del Mediterráneo.

En el año 539 a.C., el rey persa Ciro, el nuevo rey de Asia, ocupó Babilonia y estableció su poder en toda Mesopotamia.

Los primeros sondeos en la región fueron realizados en 1786 por el vicario general de Bagdag, Joseph de Beauchamps, pero habría que esperar hasta 1842 para la primera excavación arqueológica real, promovida por el cónsul francés en Mosul, Paul Émile Botta, que se centró en el área de tell Kujunjik, cerca de Nínive. Los resultados no fueron interesantes pero, luego de trasladar la excavación por consejo de un aldeano, aparecieron unos bajorrelieves asirios que supusieron el primer hallazgo histórico de las civilizaciones mesopotámicas, de las que, hasta entonces, solo se sabía por las menciones en la Biblia.

A partir de este momento la investigación estuvo marcada por la rivalidad entre ingleses y franceses. Los primeros, dirigidos por Austen Henry Layard, descubrieron la importantísima biblioteca de Asurbanipal; los segundos, el palacio de Sargón II en Khorsabad, cuyos hallazgos tuvieron un desgraciado fin al hundirse en el Tigris una embarcación con 235 cajas de material.

En el área del sur, en la década de 1850, se descubrieron las ciudades de Uruk, Susa, Ur y Larsa, si bien no fue a partir de 1875 cuando se hallaron evidencias de la civilización sumeria. Hasta los primeros años del siglo XX aparecieron gran cantidad de restos, incluido un gran número de estatuas de Gudea. En esta etapa también comienzan a progresar las excavaciones de alemanes y estadounidenses.

Una de las principales características de los yacimientos arqueológicos de la zona es que se han encontrado en gran abundancia textos escritos en cuneiforme, fundamentalmente sobre tablillas de arcilla cruda, que resistieron bien el paso del tiempo, lo que ha permitido conservar algunas de las primeras páginas de la historia de la humanidad.

Las culturas de Mesopotamia fueron pioneras en muchas de las ramas del conocimiento: desarrollaron la escritura que se denominó cuneiforme, en principio pictográfica, y más adelante la fonética; en el campo del derecho, crearon los primeros códigos de leyes; en arquitectura, desarrollaron importantes avances como la bóveda y la cúpula, crearon un calendario de 12 meses y 360 días e inventaron el sistema de numeración sexagesimal.

Sus restos, aunque quizás todavía hay muchos por descubrir, muestran una cultura que ejerció una poderosa influencia en otras civilizaciones del momento y por ende en el desarrollo de la cultura occidental.

La matemática y la ciencia mesopotámicas se basaron en un sistema de numeración sexagesimal (base 60). Esta es la fuente de la hora de 60 minutos, el día de 24 horas y el círculo de 360 grados. El calendario sumerio se basó en la semana de siete días. Esta forma de matemática fue instrumental en la creación temprana de mapas. Los babilonios también tenían teoremas sobre cómo medir el área de varias formas y sólidos. Midieron la circunferencia de un círculo como tres veces el diámetro y el área como una doceava parte del cuadrado de la circunferencia, lo que sería correcto si π se fijara en 3. El volumen de un cilindro se tomó como el producto del área de la base y la altura. El tronco de un cono o una pirámide cuadrada se tomó incorrectamente como el producto de la altura y la mitad de la suma de las bases. Además, hubo un descubrimiento reciente en el que una tableta usaba π como 25/8 (3.125 en lugar de 3.14159 ~). Los babilonios también son conocidos por la milla de Babilonia, que era una medida de distancia igual a unas siete millas modernas (11 km). Esta medida de distancias finalmente se convirtió en una milla de tiempo utilizada para medir el viaje del Sol, por lo tanto, representa el tiempo.

Desde la época sumeria, los sacerdotes del templo habían intentado asociar eventos actuales con ciertas posiciones de los planetas y las estrellas. Esto continuó hasta la época asiria, cuando las listas de Limmu se crearon como una asociación de eventos año tras año con posiciones planetarias, que, cuando han sobrevivido hasta nuestros días, permiten asociaciones precisas de relación relativa con datación absoluta para establecer la historia de Mesopotamia.

Los astrónomos babilónicos eran muy expertos en matemáticas y podían predecir eclipses y solsticios. Los estudiosos pensaban que todo tenía algún propósito en astronomía. La mayoría de estos relacionados con la religión y los presagios. Los astrónomos mesopotámicos elaboraron un calendario de 12 meses basado en los ciclos de la luna. Dividieron el año en dos estaciones: verano e invierno. Los orígenes de la astronomía y la astrología datan de esta época.

Durante los siglos VIII y VII a. C., los astrónomos de Babilonia desarrollaron un nuevo enfoque de la astronomía. Comenzaron a estudiar filosofía sobre la naturaleza ideal del universo primitivo y comenzaron a emplear una lógica interna dentro de sus sistemas planetarios predictivos. Esta fue una contribución importante a la astronomía y la filosofía de la ciencia y algunos estudiosos se han referido a este nuevo enfoque como la primera revolución científica. Este nuevo enfoque de la astronomía fue adoptado y desarrollado en astronomía griega y helenística.

En los tiempos seléucida y parta, los informes astronómicos eran completamente científicos; cuánto antes se desarrollaron sus conocimientos y métodos avanzados es incierto. El desarrollo babilónico de métodos para predecir los movimientos de los planetas se considera un episodio importante en la historia de la astronomía.

El único astrónomo greco-babilónico conocido que apoyó un modelo heliocéntrico de movimiento planetario fue Seleuco de Seleucia (n. 190 a. C.). Seleuco es conocido por los escritos de Plutarco. Apoyó la teoría heliocéntrica de Aristarco de Samos donde la Tierra giraba alrededor de su propio eje que a su vez giraba alrededor del Sol. Según Plutarco, Seleuco incluso probó el sistema heliocéntrico, pero no se sabe qué argumentos usó (excepto que teorizó correctamente sobre las mareas como resultado de la atracción lunar).

La astronomía babilónica sirvió de base para gran parte de la astronomía griega, india clásica, sasánida, bizantina, siria, islámica medieval, asiática central y de Europa occidental. 

Los textos babilónicos más antiguos sobre medicina se remontan al antiguo período babilónico en la primera mitad del segundo milenio antes de Cristo. Sin embargo, el texto médico babilónico más extenso es el "Manual de diagnóstico" escrito por el "ummânū", o erudito principal, Esagil-kin-apli de Borsippa, durante el reinado del rey babilónico Adad-apla-iddina (1069-1046 a. C.) 

Junto con contemporánea medicina egipcia, los babilonios introdujeron los conceptos de diagnóstico, el pronóstico, el examen físico, los enemas y las recetas. Además, el "Manual de diagnóstico" introdujo los métodos de terapia y etiología y el uso del empirismo, la lógica y la racionalidad en el diagnóstico, el pronóstico y la terapia. El texto contiene una lista de síntomas médicos y observaciones empíricas a menudo detalladas junto con las reglas lógicas utilizadas para combinar los síntomas observados en el cuerpo de un paciente con su diagnóstico y pronóstico. 

Los síntomas y enfermedades de un paciente fueron tratados a través de medios terapéuticos como vendajes, cremas y píldoras. Si un paciente no podía curarse físicamente, los médicos de Babilonia a menudo confiaban en el exorcismo para limpiar al paciente de cualquier maldición. El "Manual" de "diagnóstico de" Esagil-kin-apli se basó en un conjunto lógico de axiomas y suposiciones, incluida la visión moderna de que a través del examen e inspección de los síntomas de un paciente, es posible determinar la enfermedad del paciente, su etiología, su desarrollo futuro, y las posibilidades de recuperación del paciente. 

Esagil-kin-apli descubrió una variedad de enfermedades y enfermedades y describió sus síntomas en su "Manual de diagnóstico". Estos incluyen los síntomas de muchas variedades de epilepsia y enfermedades relacionadas, junto con su diagnóstico y pronóstico. 

Antes del desarrollo de la literatura, el lenguaje escrito se usaba para llevar las cuentas administrativas de la comunidad. Con el tiempo, se le empezó a dar otros usos, como explicar hechos, citas, leyendas o catástrofes.

La literatura sumeria comprende tres grandes temas: mitos, himnos y lamentaciones. Los mitos se componen de breves historias que tratan de perfilar la personalidad de los dioses mesopotámicos: Enlil, principal dios y progenitor de las divinidades menores; Inanna, diosa del amor y de la guerra; o Enki, dios del agua dulce, frecuentemente enfrentado a Ninhursag, diosa de las montañas. Los himnos son textos de alabanza a los dioses, reyes, ciudades o templos. Las lamentaciones relatan temas catastróficos como la destrucción de ciudades o palacios y el resultante abandono de los dioses.

Algunas de estas historias es posible que se apoyasen en hechos históricos como guerras, inundaciones o la actividad constructora de un rey importante, magnificados y distorsionados con el tiempo.

Una creación propia de la literatura sumeria fue un tipo de poemas dialogados basados en la oposición de conceptos contrarios. También los proverbios forman parte importante de los textos sumerios.

La religión era politeísta; en cada ciudad se adoraba a distintos dioses, aunque había algunos comunes. Entre estos figuran:
En el siglo XVII a. C., el rey Hammurabi unificó el Estado, hizo de Babilonia la capital del imperio e impuso como dios principal a Marduk. Este dios fue el encargado de restablecer el orden celeste, de hacer surgir la tierra del mar y de esculpir el cuerpo del primer hombre antes de repartir los dominios del universo entre los demás dioses.

Algo que caracterizaba a estos dioses era que estaban asociados a distintas actividades; es decir, existían dioses de la ganadería, escritura, confección, etc., lo que hizo que hubiera un panteón muy amplio

"Véase: ".

Las numerosas civilizaciones del área influenciaron las religiones abrahámicas, especialmente la Biblia hebrea. Sus valores culturales y su influencia literaria son especialmente evidentes en el Libro del Génesis. 

Giorgio Buccellati cree que los orígenes de la filosofía se remontan a la sabiduría mesopotámica temprana, que encarnaba ciertas filosofías de la vida, particularmente la ética, en forma de dialéctica, diálogos, poesía épica, folclore, himnos, letras, obras en prosa y proverbios. La razón babilónica y la racionalidad se desarrollaron más allá de la observación empírica. 

La forma más temprana de lógica fue desarrollada por los babilonios, especialmente en la rigurosa naturaleza no ergódica de sus sistemas sociales. El pensamiento babilónico era axiomático y es comparable a la "lógica ordinaria" descrita por John Maynard Keynes. El pensamiento babilónico también se basaba en una ontología de sistemas abiertos que es compatible con los axiomas ergódicos. La lógica se empleó hasta cierto punto en la astronomía y medicina babilónicas.

El pensamiento babilónico tuvo una influencia considerable en los principios de la antigua filosofía griega y helenística. En particular, el texto babilónico "Diálogo del pesimismo" contiene similitudes con el pensamiento agonista de los sofistas, la doctrina heracliteana de la dialéctica y los diálogos de Platón, así como un precursor del método socrático. El filósofo jónico Tales fue influenciado por las ideas cosmológicas de Babilonia.

El desarrollo temprano de la agricultura en la región pudo haber permitido que numerosos pequeños grupos humanos se expandieran independientemente por la región, causando que la diversidad lingüística de esta fuera inicialmente muy grande. Esta situación contrasta con la que se presenta cuando grupos humanos agrícolas con una tecnología superior penetran en un territorio menos densamente poblado por poblaciones seminómadas, lo que da lugar a una diversidad mucho menor, como lo acontecido en Europa con la entrada de los pueblos indoeuropeos.

En Mesopotamia se reconocen dos grandes familias lingüísticas: la indoeuropea (cuya presencia se debe a varias olas, por lo que existen lenguas de diferentes ramas) y la semítica (de la que se testimonian dos ramas). Junto con estas existe un número importante de lenguas aisladas (sumerio, elamita) o cuasiaisladas (hurrita-uratiano), y un número de lenguas mal documentadas cuya filiación no puede precisarse adecuadamente (casita, hatti, kaskas). Muchas de las lenguas aisladas, cuasi-aisladas y no clasificadas parecen tener rasgos ergativos, lo cual las acerca tipológicamente a algunas lenguas caucásicas aunque esto no es prueba de parentesco, ya que dichos rasgos podrían ser muestra de que en el pasado habría existido un área lingüística.

Los antiguos mesopotámicos tenían ceremonias cada mes. El tema de los rituales y festivales de cada mes estuvo determinado por al menos seis factores importantes:


"Artículo principal:" Música de Mesopotamia

Algunas canciones fueron escritas para los dioses, pero muchas fueron escritas para describir eventos importantes. Aunque la música y las canciones divertían a los reyes, también las disfrutaban personas comunes a quienes les gustaba cantar y bailar en sus hogares o en los mercados. Se cantaron canciones a los niños que las transmitieron a sus hijos. Así, las canciones se transmitieron a través de muchas generaciones como una tradición oral hasta que la escritura fue más universal. Estas canciones proporcionaron un medio para transmitir a través de los siglos información muy importante sobre eventos históricos.

El Oud (en árabe: العود) es un pequeño instrumento musical de cuerda utilizado por los mesopotámicos. El registro pictórico más antiguo del Oud se remonta al período Uruk en el sur de Mesopotamia hace más de 5 000 años. Está en un sello cilíndrico actualmente alojado en el Museo Británico y adquirido por el Dr. Dominique Collon. La imagen muestra a una mujer agachada con sus instrumentos en un bote, jugando con la mano derecha. Este instrumento aparece cientos de veces a lo largo de la historia mesopotámica y nuevamente en el antiguo Egipto a partir de la dinastía XVIII en variedades de cuello largo y corto. El oud es considerado como un precursor de los europeos. laúd. Su nombre se deriva de la palabra árabe العود al-'ūd 'the wood', que es probablemente el nombre del árbol del que se hizo el oud. (El nombre árabe, con el artículo definido, es la fuente de la palabra 'laúd'). 

La caza era popular entre los reyes asirios. El boxeo y la lucha con frecuencia aparecen con frecuencia en el arte, y alguna forma de polo era probablemente popular, con hombres sentados sobre los hombros de otros hombres en lugar de sobre caballos. También jugaron "majore", un juego similar al rugby deportivo, pero jugaron con una pelota de madera. También jugaron un juego de mesa similar al senet y al backgammon, ahora conocido como el "Juego Real de Ur ".

Mesopotamia, como lo demuestran los sucesivos códigos legales, los de Urukagina, Lipit Ishtar y Hammurabi, a lo largo de su historia se convirtió cada vez más en una sociedad patriarcal, en la que los hombres eran mucho más poderosos que las mujeres. Por ejemplo, durante el primer período sumerio, el ""en"", o sumo sacerdote de dioses masculinos era originalmente una mujer, la de las diosas femeninas, un hombre. Thorkild Jacobsen, así como muchos otros, ha sugerido que la sociedad mesopotámica primitiva estaba gobernada por un "consejo de ancianos" en el que hombres y mujeres estaban igualmente representados, pero que con el tiempo, a medida que el estatus de las mujeres disminuía, el de los hombres aumentaba. En cuanto a la escolarización, solo los hijos reales y los hijos de los ricos y profesionales, como los escribas, los médicos y los administradores del templo, asistieron a la escuela. A la mayoría de los niños se les enseñó el oficio de su padre o aprendieron otro distinto, mientras que las niñas tuvieron que quedarse en casa con sus madres para aprender a limpiar, cocinar y cuidar a los niños más pequeños. Algunos niños ayudarían a triturar granos o limpiar pájaros. Inusualmente para ese período histórico, las mujeres de Mesopotamia tenían derechos. Podían poseer propiedades y, si tenían una buena razón, divorciarse. 

Cientos de tumbas han sido excavadas en distintas partes de Mesopotamia, revelando información sobre los hábitos de entierro mesopotámico. En la ciudad de Ur, la mayoría de las personas fueron enterradas en tumbas familiares debajo de sus casas, junto con algunas posesiones. Se han encontrado algunos envueltos en esteras y alfombras. Los niños fallecidos fueron puestos en grandes "frascos" que fueron colocados en la capilla de la familia. Se han encontrado otros restos enterrados en cementerios comunes de la ciudad. Se han encontrado 17 tumbas con objetos muy preciosos en ellas. Se supone que se trataba de tumbas reales. Rico de varios períodos, se ha descubierto que han buscado entierro en Baréin, identificado con Dumemun sumerio.

En la zona fértil de una y otra llanura, abundantemente regada en su parte inferior por los dos ríos que delimitan esta civilización, se produjo muy pronto la sedentarización de los pueblos nómadas que la atravesaban, convirtiéndose en agricultores y desarrollando una cultura y un arte con una sorprendente variedad de formas y estilos.

Con todo, el arte en general mantiene bastante unidad en cuanto a su intencionalidad, que da como resultado un arte algo rígido, geométrico y cerrado, pues, ante todo, tiene una finalidad práctica y no estética y se desarrolla al servicio de la sociedad.

La escultura representa tanto a dioses como a soberanos o funcionarios, pero siempre como personas individualizadas (a veces con su nombre grabado), y busca sustituir a la persona más que representarla. La cabeza y el rostro estaban desproporcionados respecto al cuerpo, por lo que se dice que desarrollaron el llamado realismo conceptual: simplificaban y regularizaban las formas naturales mediante la ley de la frontalidad (parte derecha e izquierda absolutamente simétricas) y el geometrismo (figura dentro de un esquema geométrico que solía ser el cilindro o el cono). Las representaciones humanas mostraban una total indiferencia por la realidad, aunque en los animales se presentaba un mayor realismo.

Algunos temas recurrentes de la escultura mesopotámica son toros monumentales, muy estilizados y realistas (genios protectores, monstruosos y fantásticos como todo lo sobrenatural en Mesopotamia).

Sus técnicas principales fueron el relieve monumental, la estela, el relieve parietal, el relieve de ladrillos esmaltados y el sello: otras formas de esculpir y desarrollar auténticos cómics o narraciones en ellos.

Debido a las características del país, existen muy pocas muestras de pintura; sin embargo, el arte es muy parecido al arte del período magdaleniense de la prehistoria. La técnica era la misma que en el relieve parietal, sin perspectiva. Al igual que los mosaicos (más perdurables y característicos) tenía un fin más decorativo que las otras facetas del arte.

En la pintura y el grabado, la jerarquía se mostraba de acuerdo al tamaño de las personas representadas en la obra: los de más alto rango se mostraban más grandes en comparación con el resto.

La pintura fue estrictamente decorativa, pues se utilizó para embellecer la arquitectura. Carece de perspectiva, y es cromáticamente pobre: solo prevalecen el blanco, el azul y el rojo. Se usaba la técnica del temple, que se puede apreciar en los mosaicos decorativos o azulejos. La pintura se empleaba en la decoración doméstica. Los temas eran escenas de guerras y de sacrificios rituales con mucho realismo, y se representaban figuras geométricas, personas, animales y monstruos, sin representar las sombras.

Los mesopotámicos tenían una arquitectura muy particular debido a los recursos disponibles. Hicieron uso de los dos sistemas constructivos básicos: el abovedado y el adintelado.

Construyeron mosaicos pintados en colores vivos, como el negro, verdes o bicolores, a manera de murales. Los edificios no tenían ventanas y la luz se obtenía del techo. Se preocupaban de la vida terrenal y no de la de los muertos, por tanto las edificaciones más representativas eran el templo y el palacio.

El templo era el centro religioso, económico y político. Tenía tierras de cultivo y rebaños, almacenes (donde se guardaban las cosechas) y talleres (donde se hacían utensilios, estatuas de cobre y de cerámica). Los sacerdotes organizaban el comercio y empleaban a campesinos, pastores y artesanos, quienes recibían como pago parcelas de tierra para cultivo de cereales, dátiles o lana. Además, los zigurats tenían un amplio patio con habitaciones para alojar a las personas que habitaban en este pueblo.

El urbanismo regulado estuvo presente en algunas ciudades, como la Babilonia de Nabucodonosor III, mayoritariamente con diseño en damero. En cuanto a las obras de ingeniería, destaca la muy extensa y antigua red de canales que unían los ríos Tigris, Éufrates y sus afluentes, propiciando la agricultura y la navegación.

El desarrollo de la tecnología en Mesopotamia estuvo condicionado en muchos aspectos a los avances en el dominio del fuego, conseguidos mediante la mejora de la capacidad térmica de los hornos, con los cuales era posible conseguir yeso (a partir de los 300 °C), y cal (a partir de los 800 °C). Con estos materiales se podían recubrir recipientes de madera lo que permitía ponerlos al fuego directo, una técnica predecesora de la cerámica a la que se ha llamado «vajilla blanca».

Los inicios de esa técnica se han encontrado en Beidha, al sur de Canaán, y datan del IX milenio a. C. aproximadamente; desde los milenios posteriores se extiende hacia el norte y al resto del Próximo Oriente, cubriéndolo por completo entre 5600 y 3600 a. C.

En Mesopotamia, la cerámica comienza a desarrollarse ya empezado el Neolítico, por lo que se habla de un Neolítico Precerámico. Tras este, se da un período en el que la cerámica aparece de forma intermitente en los restos. Esto es debido, más que a una serie de descubrimientos y olvidos, a que la "vajilla blanca" era aún suficiente para la mayor parte de las aplicaciones. Hacia el IV milenio a. C. la cerámica alcanzó un desarrollo pleno, con hornos donde el fuego y la cámara de cocción estaban bien diferenciados.

A partir de aquí y con el dominio de temperaturas aún superiores, surgió una nueva técnica: la vitrificación de la pasta. Hacia el III milenio a. C., durante el período Jemdet Nasr, se conseguía fabricar perlas de vidrio y un milenio después ya se dominaba la técnica del vidriado. Finalmente, durante el II milenio a. C., se logró la fabricación de objetos de vidrio.

La utilización de pequeños objetos metálicos tallados había sido una constante en la región desde el VI milenio a. C., sin embargo no fue hasta el desarrollo de hornos más potentes cuando se generalizó el uso de estos materiales mediante la aparición de la metalurgia. Este cambio puede situarse a mediados el III milenio a. C.; empieza a encontrarse mayor cantidad de objetos metálicos; por su composición, se aprecia que estos objetos son obtenidos mediante fundición, no por el tallado de metales en estado natural y se empieza a experimentar con aleaciones.

Con el desarrollo de las aleaciones se produjo el nacimiento de la metalurgia del bronce, que se diferenció en dos vertientes según los metales con los que se obtenía la aleación, bien fuesen cobre y estaño o cobre y arsénico. El bronce arsenioso se desarrolló en las áreas del Cáucaso, este de Anatolia, sur de Mesopotamia y Levante mediterráneo, trazando un eje norte sur. El bronce de estaño predomina en Irán, toda Mesopotamia, el norte de Siria y en Cilicia, trazando un eje este-oeste. El punto de cruce de estos dos ejes es el sur de Mesopotamia, esto es, la cuna de la civilización sumeria. Esta situación se mantiene durante los milenios IV y III a. C., hasta que en el segundo el bronce arsenioso desaparece.

Entre el 1200 y el 1000 a. C. se produce un nuevo avance: el hierro, que hasta entonces había sido escaso hasta el punto de costar igual que el oro, se populariza debido probablemente al descubrimiento de nuevas técnicas, conseguidas en el área del norte de Siria o en la tierra de los hititas.

La geografía de Mesopotamia tuvo un profundo impacto en el desarrollo político de la región. Entre los ríos y arroyos, el pueblo sumerio construyó las primeras ciudades junto con canales de riego que estaban separados por vastas extensiones de desierto abierto o pantano donde vagaban tribus nómadas. La comunicación entre las ciudades aisladas era difícil y, a veces, peligrosa. Así, cada ciudad sumeria se convirtió en una ciudad-Estado, independiente de las demás y protectora de su independencia. A veces una ciudad intentaba conquistar y unificar la región, pero tales esfuerzos fueron resistidos y fracasaron durante siglos. Como resultado, la historia política de Sumeria es una de guerra casi constante. Finalmente, Sumer fue unificado por Eannatum, pero la unificación fue tenue y no duró ya que los acadios conquistaron Sumeria en 2331 a. C. solo una generación después. El Imperio acadio fue el primer imperio exitoso que duró más de una generación y vio la sucesión pacífica de reyes. El imperio fue relativamente efímero, ya que los babilonios los conquistaron en unas pocas generaciones.

"Artículos principales:" Lista de reyes sumerios y 

Los mesopotámicos creían que sus reyes y reinas descendían de la Ciudad de los Dioses, pero a diferencia de los antiguos egipcios, nunca creyeron que sus reyes fueran dioses reales. La mayoría de los reyes se llamaban a sí mismos "rey del universo" o "gran rey". Otro nombre común era " pastor ", ya que los reyes tenían que cuidar a su gente.

Cuando Asiria se convirtió en un imperio, se dividió en partes más pequeñas, llamadas provincias. Cada uno de estos lleva el nombre de sus principales ciudades, como Nínive, Samaria, Damasco y Arpad. Todos tenían su propio gobernador que tenía que asegurarse de que todos pagaran sus impuestos. Los gobernadores también tuvieron que convocar soldados para la guerra y suministrar trabajadores cuando se construyó un templo. También fue responsable de hacer cumplir las leyes. De esta manera, era más fácil mantener el control de un gran imperio. Aunque Babilonia era un Estado bastante pequeño en el sumerio, creció enormemente durante el gobierno de Hammurabi. Era conocido como "el legislador", y pronto Babilonia se convirtió en una de las principales ciudades de Mesopotamia. Más tarde se llamó Babilonia, que significaba "la puerta de entrada de los dioses". También se convirtió en uno de los mayores centros de aprendizaje de la historia.

Con el final de la fase Uruk, las ciudades amuralladas crecieron y muchas aldeas Ubaid aisladas fueron abandonadas, lo que indica un aumento de la violencia comunitaria. Se suponía que uno de los primeros reyes de Lugalbanda había construido los muros blancos alrededor de la ciudad. A medida que las ciudades-Estado comenzaron a crecer, sus esferas de influencia se superpusieron, creando discusiones entre otras ciudades-Estado, especialmente sobre la tierra y los canales. Estos argumentos se registraron en tabletas varios cientos de años antes de cualquier guerra importante: la primera grabación de una guerra ocurrió alrededor del 3200 a. C., pero no fue común hasta aproximadamente el 2500 a. C. Un rey dinástico temprano II (Ensi) de Uruk en Sumer, Gilgamesh (c. 2600 a. C.), fue elogiado por las hazañas militares contra Humbaba, guardián de la montaña del cedro, y más tarde se celebró en muchos poemas y canciones posteriores en los que se afirmaba que era un dios de dos tercios y solo un tercio humano. La estela de los buitres posterior al final del período dinástico temprano III (2600–2350 a. C.), que conmemora la victoria de Eannatum de Lagash sobre la vecina ciudad rival de Umma, es el monumento más antiguo del mundo que celebra una masacre. A partir de este momento, la guerra se incorporó al sistema político mesopotámico. A veces, una ciudad neutral puede actuar como árbitro para las dos ciudades rivales. Esto ayudó a formar sindicatos entre ciudades, lo que llevó a los Estados regionales. Cuando se crearon los imperios, fueron a la guerra más con países extranjeros. El rey Sargón, por ejemplo, conquistó todas las ciudades de Sumer, algunas ciudades en Mari, y luego fue a la guerra con el norte de Siria. Muchas paredes del palacio asirio y babilónico estaban decoradas con las imágenes de las luchas exitosas y el enemigo escapaba desesperadamente o se escondía entre los juncos.

Las ciudades-Estado de Mesopotamia crearon los primeros códigos legales, extraídos de la precedencia legal y las decisiones tomadas por los reyes. Se han encontrado los códigos de Urukagina y Lipit Ishtar. El más famoso de estos fue el de Hammurabi (creado hacia 1780 a. C.),debido a su conjunto de leyes, siendo uno de los primeros conjuntos de leyes encontrados y uno de los ejemplos mejor conservados de este tipo de documento de la antigua Mesopotamia. Codificó más de 200 leyes para Mesopotamia. El examen de las leyes muestra un debilitamiento progresivo de los derechos de la mujer y una gravedad cada vez mayor en el tratamiento de esclavos.

Algunas de las creaciones que les debemos a las civilizaciones que habitaron Mesopotamia son:




</doc>
<doc id="16891" url="https://es.wikipedia.org/wiki?curid=16891" title="Meridiano de Greenwich">
Meridiano de Greenwich

El meridiano de Greenwich (/ɡrɛnɪtʃ/) , también conocido como meridiano cero, meridiano base o primer meridiano, es el meridiano a partir del cual se miden las longitudes. Sustitutivo del meridiano de París, se corresponde con la circunferencia imaginaria que une los polos y recibe su nombre por "cruzar" por el distrito londinense de Greenwich, en concreto por su antiguo observatorio astronómico.

El meridiano fue adoptado como referencia en una conferencia internacional celebrada en octubre de 1884 en Washington D. C., auspiciada por el presidente de los Estados Unidos a la que asistieron delegados de 25 países. En dicha conferencia se adoptaron los siguientes acuerdos:


La segunda resolución se aprobó con la oposición de Santo Domingo (actual República Dominicana) y las abstenciones de Francia (cuyos mapas siguieron utilizando el meridiano de París durante algunas décadas más) y Brasil.

Un huso horario se extiende sobre quince grados de longitud (porque 360 grados corresponden a 24 horas y 360/24 = 15).

La línea opuesta al meridiano de Greenwich, es decir, la semicircunferencia que completa una vuelta al mundo, corresponde a la línea internacional de cambio de fecha, que atraviesa el océano Pacífico. Por razones prácticas —fundamentalmente, no tener varios husos horarios en algunos archipiélagos— se ha adaptado esta línea a la geografía (ya no es recta en la superficie del globo), al igual que otras que limitan husos horarios, por lo que no coinciden con los meridianos.

Antiguamente la mayoría de las marinas de la Europa continental usaban el meridiano de El Hierro, que pasaba por la Punta de la Orchilla, en el oeste de esta isla de las Canarias. Sin embargo, existieron muchas otras referencias.

Existe una diferencia angular de cinco con tres segundos entre el meridiano de Greenwich y el meridiano de referencia utilizado por el sistema GPS WGS84 (denominado IRM). Es consecuencia del procedimiento utilizado para la puesta en marcha en 1958 del primer Sistema de Posicionamiento Global por satélite, cuando se usaron como base de partida del nuevo sistema geodésico las coordenadas en el sistema NAD27 de la estación de observación de satélites situada en las inmediaciones de Baltimore. La mayor precisión del nuevo método por satélite se tradujo en un desplazamiento del Meridiano 0º del Sistema GPS (utilizando la longitud de Baltimore como referencia de partida), quedando situado unos 102 metros al este del meridiano de Greenwich materializado en el Observatorio. Esto es debido a la corrección de diversos errores de concordancia entre los sistemas cartográficos europeo y norteamericano, difícilmente apreciables por los métodos de geodesia clásicos. Cuando se constató esta diferencia en 1969, se descartó la posibilidad de reajustar todo el sistema GPS para eliminar este desfase. Para más detalle, véase el artículo Meridiano internacional de referencia del IERS.

El meridiano de Greenwich, también conocido como meridiano 0°, pasa por los siguientes países, ordenados de norte a sur:



</doc>
<doc id="16892" url="https://es.wikipedia.org/wiki?curid=16892" title="Geografía de Vietnam">
Geografía de Vietnam

La geografía de Vietnam o de la República Socialista de Vietnam trata del relieve, hidrología y clima de este país que cuenta con una extensión de 333.000 km² y más de 96 millones de habitantes. 

Limita al norte con China, al oeste con Laos (mitad norte) y Camboya (mitad sur) y es bañado por el Mar de la China Meridional al este y al sur. El relieve es muy montañoso y accidentado.

Los dos cursos de agua dulce más importantes son los ríos Rojo, al norte, y Mekong, al sur, en sus deltas se aloja la mayor parte de la población.
El río Rojo fluye directamente hacia el sureste desde las regiones montañosas del noreste, mientras que el Mekong fluye por un trazado irregular desde Camboya y desemboca en el mar de la China Meridional. Ambos ríos han sido encauzados para evitar los posibles daños ocasionados por las crecidas.

Vietnam se divide a grandes rasgos en las zonas montañosas y el delta del río Rojo en el norte, y en las montañas del sur, conocidas como altiplano central, las tierras bajas costeras y el delta del río Mekong en el sur. 

El delta del río Rojo es una región triangular de 15.000 km intensamente cultivada y más densamente poblada que el delta del río Mekong. Formado en el golfo de Tonkin por los depósitos aluviales dejados por los ríos a lo largo de milenios, el delta avanza un centenar de metros en el golfo anualmente. Es el hogar ancestral de la etnia vietnamita, y en él se concentraba el 70 por ciento de la agricultura y el 80 por ciento de la industria de Vietnam del Norte antes de 1975.

El río Rojo, que nace en la provincia china de Yunnan, tiene 1200 km de longitud. Sus dos principales tributarios, el río Negro y el río Lô, contribuyen a su gran volumen de agua, unos 4300 m/s. La región entera del delta, a los pies de las montañas boscosas, no supera los 3 m sobre el nivel del mar, uy gran parte tiene 1 m o menos. El área está sometida a frecuentes inundaciones, y en algunos lugares las marcas del agua alcanzan los 14 metros. Durante siglos, el control de las inundaciones ha formado parte de la cultura del lugar, y se ha construido un extenso sistema de diques y canales para controlar las crecidas y mantener el cultivo del arroz. Siguiendo el modelo chino, se ha conseguido una alta densidad de población y una doble cosecha de arroz en más de la mitad de la región.

Las montañas y mesetas del norte y noroeste están habitadas por grupos tribales minoritarios. La cordillera Annamita se origina en el noroeste de China, tiene más de 1.100 km de longitud y forma la frontera vietnamita con Laos. Termina en el delta del río Mekong, al norte de Ho Chi Minh.

La parte central de estas montañas, que poseen varias mesetas, son irregulares en altura y forma. La zona norte es estrecha y muy agreste; el pico más alto del país, Fansipan (3.143 m), se encuentra en el extremo noroeste. La porción sur tiene numerosas estribaciones que dividen la estrecha franja costera en una serie de compartimentos. Durante siglos, estas características topográficas han dificultado las comunicaciones y formado una barrera natural para evitar la mezcla con las poblaciones de la cuenca del río Mekong.

Dentro de la porción meridional de Vietnam hay una meseta conocida como Altiplano central que tiene aproximadamente unos 51.800 km² de abruptos picos montañosos, extensos bosques y ricos suelos. Comprende 5 mesetas relativamente llanas de suelo basáltico que se extienden por las provincias de Đắk Lắk, Gia Lai y Kon Tum y abarcan un 16 por ciento del suelo cultivable y un 22 por ciento de los bosques del país. Hasta 1975, Vietnam del Norte consideraba el Altiplano central y la cordillera Annamita áreas estratégicas de gran importancia esenciales no solo para el dominio de Vietnam del Sur sino también de parte del sur de Indochina. Desde 1975, el altiplano ha servido para recolocar a población procedente de las superpobladas zonas bajas.

Una estrecha y llana franja costera se extiende desde el delta del río Rojo hasta el delta del río Mekong. En la zona interior, las montañas se precipitan sobre la costa, llegando en algunos casos hasta el mar. Esta estrecha franja es muy fértil y se cultiva intensamente con arroz.

El delta del río Mekong cubre un área de 40.000 km, y es una extensa planicie de no más de 3 m sobre el nivel del mar, atravesada por ríos y canales. Los sedimentos arrastrados por las distintas ramas y afluentes del río Mekong hacen avanzar el delta de 6 a 8 m por año. Se calcula que afluyen 1000 millones de m de sedimentos, unas 13 veces la cantidad depositada por el río Rojo. Unos 10.000 km del delta están cultivados con arroz, uno de los mayores arrozales del mundo. El extremo sur, conocido como provincia o península de Ca Mau está cubierto por una densa jungla y manglares pantanosos.

El río Mekong tiene 4.220 km de longitud. Nace en el Tíbet y forma frontera entre Laos y Mianmar, luego entre Laos y Tailandia y después atraviesa Camboya. En Nom Pen se funde con el río procedente del lago Tonlé Sap, del mismo nombre, y se divide en dos ramales, el río Hậu Giang, conocido como río Bassac en Camboya, y el río Tiền, luego atraviesa el sur de Camboya y sigue la cuenca del Mekong hasta desembocar en el mar del Sur de la China a través de nueve canales bocas conocidas como los nueve dragones (Cửu Long). El río es muy legamoso, pero es navegable para embarcaciones de poco calado desde la ciudad de Kompung Cham. El río Tonlé Sap que se une al Mekong en Nom Pen actúa de embalse natural para estabilizar las crecidas, ya que cuando el Mekong viene muy crecido y el delta no puede asumir la enorme cantidad de agua, esta fluye hacia atrás hacia el lago de Tonlé Sap e inunda una zona de 10.000 km. Cuando baja la inundación, el agua del lago vuelve a bajar hacia el Mekong. Este efecto reduce las inundaciones que causaría el Mekong, que aun así inunda los campos que lo rodean con 2 m de agua.

En Vietnam se pueden encontrar tres tipos de climas: el clima subtropical, en las regiones del norte y del interior; con inviernos secos y veranos húmedos. En las zonas central y suroriental predomina el clima monzónico, con altas temperaturas y precipitaciones muy abundantes. En el suroeste se dan dos estaciones bien diferenciadas, una húmeda y otra seca, aquí las temperaturas son más elevadas que en el norte. En Hanói (ciudad del norte del país) las temperaturas oscilan entre los 15 °C en enero y los 33 °C en julio, mientras que en Ciudad Ho Chi Minh (en el sur) las temperaturas oscilan entre los 21 °C en enero y los 30 °C en julio.

En el norte puede hacer frío. En Cao Bang, casi en la frontera con China, las media de enero oscilan entre 10 C de mínima y 18 C de máxima, pero en verano oscilan entre 24 y 32 C, con una precipitación de 1415 mm, con extremos de 272 mm en junio y de 15 mm en enero, es decir, totalmente veraniegas. En Hanoi caen 1675 mm, con menos de 30 mm entre diciembre y febrero y más de 200 mm entre junio y septiembre, con 320 mm en agosto. 

En las zonas montañosas hace más frío. En Sa Pa, a 1500 m de altitud, en enero oscilan entre 5 C de mínima y 11 C de máxima, y en verano entre 18 y 23 C. En julio caen 467 mm, y a lo largo del año, una media de 2223 mm.

En el centro del país, el clima monzónico es más relevante, pero llueve sobre todo con la retirada del monzón veraniego, que provoca lluvias en octubre de hasta 680 mm. En Da Nang caen 2045 mm, con un máximo en octubre de 615 mm en 21 días, menos de 40 mm entre febrero y abril y más de 300 mm en septiembre y noviembre. El invierno se suaviza hacia el sur de la costa central y las lluvias aumentan. En la ciudad de Huế caen 500 mm en septiembre, 900 mm en octubre y 680 mm en noviembre, aunque es más normal el tiempo de Da Nang, con 2045 mm anuales, más de 600 mm en octubre, más de 300 mm en septiembre y noviembre, y menos de 100 mm entre enero y julio. Más al sur y hacia el interior, las lluvias se quedan, como en Pleiku, en 1870 mm, con una máxima de 300 mm en octubre.

En el sur, hace calor todo el año. En Ho Chi Min, en enero oscilan entre 21 y 32 C, y en abril, antes de las lluvias, entre 26 y 35 C de medias mínimas y máximas. Las lluvias son de 1930 mm anuales, con más de 200 mm entre mayo y octubre, y casi nada de lluvia entre enero y marzo.

Los tifones se producen entre el 20 de mayo y el 10 de diciembre, sobre todo en el norte, en el golfo de Tonkin y el delta del río Rojo.

En las tierras altas del norte del país hay minerales de gran valor, como hierro, antricta, cinc, cromo, estaño y apatito. 

Frente a la costa se han descubierto yacimientos de petróleo y gas natural. Además el 30 % del país está cubierto de bosques que albergan maderas de gran valor, pero el gobierno ha prohibido la exportación de maderas debido a que hasta finales del siglo XX se mantuvieron muy altas las tasas de deforestación anuales (1,4% durante 1990-1996) actualmente (1990-2000) esta tasa está en un -0,54% en parte causada por el cultivo de café.

Estos bosques albergan una gran variedad de especies que son aprovechadas por algunas tribus montañesas.

Los suelos del país son productivos en mayor y menor medida. En los deltas de los ríos Rojo y Mekong, los suelos son aluviales y muy ricos, mientras que los suelos de la meseta son más pobres debido a la excesiva lixiviación de los nutrientes del suelo producida por las abundantes precipitaciones.











En Vietnam hay 57 zonas protegidas reconocidas internacionalmente, que ocupan 24.994 km, el 7,58 por ciento de los aproximadamente 330.000 km del país, así como 3.630 km de áreas marinas, de los 647.232 km que pertenecen a Vietnam. En este conjunto hay 15 parques nacionales, 21 reservas naturales y 21 lugares históricos y culturales. En Vietnam hay además 8 sitios Ramsar, humedales de importancia internacional, que cubren un total de 1.178 km. BirdLife International reconoce 62 IBAs (Important Bird Areas), que cubren un total de 16.420 km y 835 especies de aves, de las que 328 son especies migratorias, 10 son endémicas y 59 son especies amenazadas. 


</doc>
<doc id="16894" url="https://es.wikipedia.org/wiki?curid=16894" title="Geografía de Mauritania">
Geografía de Mauritania

Mauritania se encuentra situada a orillas del océano Atlántico. Limita al norte con Sáhara Occidental, al noreste con Argelia, al este con Malí y al sur con Senegal y Malí. El río Senegal es el que sirve de frontera a ambos países.

El país se encuentra dominado por el desierto del Sahara que ocupa casi la totalidad del territorio, a excepción de una estrecha banda litoral, donde se encuentran casi todas las ciudades importantes del país: Nuakchot, Nuadibú... Las principales ciudades del interior son Tidjikja, Atar, Chingueti, etc.

El relieve es eminentemente llano, formado por vastas y áridas llanuras rotas por escarpes y emergencias rocosas. Mauritania está rodeada por el océano Atlántico, entre Senegal y el Sahara Occidental, Malí y Argelia. Es considerado a la vez parte del Sahel y el Magreb. Una serie de escarpes, entre el centro y el sudoeste, separados por mesetas de arenisca, cortan la planicie. La más alta es la meseta de Adrar, de unos 500 m de altitud. Al pie de los escarpes hay algunos manantiales. Algunos picos aislados se elevan sobre las mesetas. Los pequeños se llaman "guelbs" y los más grandes "kedias". Destaca la estructura de Richat, un "guelb" de casi 50 km de diámetro que tiene forma circular concéntrica y una combinación de rocas calizas y volcánicas.

Las mesetas descienden gradualmente hacia el nordeste, la enorme cuenca sedimentaria de El Djouf, a unos 320 m de altitud, una inmensidad de dunas separadas por serranías erosionadas que se extiende hasta Malí. Esta región forma parte del desierto del Sahara.

Hacia el oeste, entre el océano y las mesetas, alternan zonas de llanuras pedregosas, reg, con zonas de dunas, erg. Las dunas son movedizas y suelen aumentar en tamaño hacia el norte. La llanura costera tiene menos de 45 m de altitud, mientras las mesetas oscilan entre 180 y 230 m, aunque hacia el interior se elevan gradualmente mediante escarpes que más bien son cuestas, aunque también se encuentran inselberg, entre los que destaca el monte Ijill o Kediet Ijill, de 915 m, un enorme bloque de hematita, mineral de hierro.

Tres cuartas partes de Mauritania son desérticas o semidesérticas. Además, la sequía está expandiendo el desierto desde la década de 1960. Hacia el sudoeste aparecen cinturones de vegetación que se convierten en sabana y trazas de bosque tropical en las cercanías del río Senegal.

Geológicamente, Mauritania está dividida en cuatro zonas. La primera, en el norte y noroeste, está formada por rocas precámbricas (2.700 millones de años), que emergen para formar el esqueleto de la sierra de Reguibat y la serie de rocas Akjoujt, que forma una vasta penillanura salpicada de inselbergs. 

La segunda zona, en el centro y el nordeste, consiste en una serie de sinclinales: el de Tindouf, cubierto de areniscas, en la frontera con Argelia, y el de Taoudeni, que ocupa las dos terceras partes del cratón de África Occidental, bordeado por la meseta de Adrar, la meseta de Tagant y la meseta de Assaba, y el anticlinal de Affollé, con la depresión de Hodh. 

La tercera zona está formada por la cadena de las Mauritanidas, llamada 'cinturón de la diorita', formada a causa de movimientos orogénicos durante el final de proterozoico y el paleozoico, en el margen occidental del cratón de África Occidental. Se extiende de norte a sur y ocupa unos 2.500 km entre Senegal y Marruecos, pasando por Mauritania. Está formado por rocas sedimentarias, eruptivas y metamórficas, del precámbrico al paleozoico. 

La cuarta zona es la cuenca sedimentaria senegalesa-mauritana, que incluye la costa mauritana y el río Senegal en el sudoeste. Está formada por rocas sedimentarias cenozóicas, del cretácico inferior y el cuaternario. Calizas arcillosas, areniscas y arcillas. En la costa, cuatro transgresiones marinas que han dado lugar a dunas, areniscas, calizas, arenales y conchales.

El clima de Mauritania se caracteriza por temperaturas extremas y lluvias escasas e irregulares. Las variaciones anuales son pequeñas, pero las variaciones diurnas son muy altas. El viento de harmatán, seco y caliente, y a menudo cargado de polvo, sopla desde el Sahara durante la larga estación seca y es el viento predominante, excepto en la franja costera, donde prevalecen los vientos oceánicos. La mayor parte de las lluvias caen durante la corta estación húmeda, de julio a septiembre, y la media de precipitaciones oscila entre los 500-600 mm en el extremo sur, y menos de 100 mm en el norte, en las dos terceras partes del país.

En la costa, la corriente de las Islas Canarias, una corriente marina de agua fría que circula de norte a sur, provoca que en la mitad norte del litoral mauritano las temperaturas sean frescas y agradables todo el año. En Nuadibú, las temperaturas oscilan entre los 15 y los 23 C del invierno, con nieblas frecuentes, y los 20 y los 27 C del verano, salvo cuando sopla el viento del desierto, que puede hacer subir el termómetro a 37-38 C. La temperatura del mar en esta zona es de 19 C en enero y de 24 C en septiembre. La precipitación media en Nuadibú es de 18 mm.

En las zonas del interior del norte de Mauritania, en pleno desierto, hace más frío en invierno, pero en julio y agosto se superan los 40 oC a diario, y en el nordeste se llaga a los 50 oC. En Zuérate, en enero, las temperaturas oscilan entre 12 y 22 C, y en agosto entre 27 y 40 C. Las precipitaciones alcanzan los 55 mm, con un máximo de 20 mm en septiembre.

La región central del sur pertenece al Sahel y se ve afectada por los monzones. Nuakchot, en la costa, es cálida todo el año, con temperaturas en enero entre 15 y 27 C y en septiembre entre 26 y 33 C. Caen entre 100 y 160 mm de lluvia al año, entre julio y octubre, con un máximo en agosto.

En el extremo sur, en el interior, las temperaturas aumentan, en Kiffa, a 500 km del mar, las temperaturas en enero oscilan entre 17 y 28 C y en mayo y junio entre 30 y 41 C. Las lluvias en esta región oscilan entre 200 y 400 mm, con un máximo de en torno a 100 mm en agosto.

En la frontera con Senegal, las lluvias aumentan hasta los 400-600 mm, con un clima saheliano-sudanés que conlleva precipitaciones de hasta 200 mm en agosto. En Sélibaby, caen 475 mm entre junio y octubre, con temperaturas de 16 y 33 C en enero, y de 28 a 42 C en mayo, antes de las lluvias.

En Mauritania hay 2 parques nacionales, 1 reserva satélite costera en Cabo Blanco, 1 reserva de la biosfera en el delta del río Senegal y 4 sitios Ramsar que incluyen los 2 parques nacionales.







En la República Islámica de Mauritania hay una población estimada en 2019 de 4,66 millones de habitantes., aunque algunas estimaciones no superan los 4 millones. Se estima que el 30-40% de la población es árabe blanca (árabes-bereberes, beidanes o moros blancos); otro 30-40% serían haratines, descendientes de esclavos subsaharianos arabizados (moros negros), y el 20-30% restante serían negros no arabizados, que incluyen los pueblos fula o fulani, toucouleur, bambara, soninke, serer, wolof y haratin. Varios grupos étnicos son agricultores y ganaderos: los beidan, también llamados moros blancos, por contraste con los haratin, que son de piel más oscura (moros negros) y que son descendientes de esclavos negros liberados, los soninke y los serer, mientras que los fulas son agricultores y pastores seminómadas. El término moro no define a ningún grupo étnico, sino que lo usaban los cristianos europeos para definir a las poblaciones árabes o bereberes del norte de África, como el término sarracenos.

En general, los árabes son más numerosos en el norte y los negros, en el sur, aunque no existen regiones habitadas únicamente por negros. Regiones como Guidimaka, al sur, albergan un 50% de soninkes, un 25% de fulas y un 25% de árabes. En la región de Gorgol, también al sur, hay moros blancos y negros, fulas o peuls, soninkés, bambaras y wolof, pero la lengua común es el idioma fula o peul. En Nuakchot, la capital, hay una mezcla de árabes, occidentales y negros de todas las etnias. Hay barrios donde están mezclados y otros donde predominan los negros. En general los moros blancos y negros son mayoría. Los barrios ricos están al norte.

El sistema social es igual en todas las etnias. Los grupos se dividen en clases jerárquicas. A la cabeza se encuentran los nobles, que lo son por nacimiento, le siguen los sirvientes y por último los esclavos. Los moros nobles están encabezados a su vez por guerreros, descendientes de Beni Hassan y conocidos como "hassanis", y los murabit o morabitos, consagrados al estudio de la religión. Los guerreros, de ascendencia árabe, conocidos como "zawaya", reciben tributo de los beidan, que son sus vasallos. En esta jerarquía, la base la forman dos grupos de artesanos, los herreros y los griot, o contadores de historias. Las clases serviles se dividen en esclavos y liberados. Los haratin son bastante autónomos, pero están limitados por su economía más nómada.


La prohibición en 1981 de la esclavitud nunca acabó de acatarse del todo en Mauritania. Según Amnistía Internacional y otras ONGs entre el 10 y el 20 por ciento de la población vive bajo régimen de esclavitud, en un número que supera las 90.000 personas según el Índice Global de Esclavitud, 21,5 de cada 1000 individuos. No obstante, la esclavitud está prohibida y castigada con la cárcel. La practican los árabes blancos, pero también los wolof, los peul y los soninké. Se aprecia en forma de trabajo infantil y doméstico, matrimonios de niños, prostitución y tráfico de personas. Los niños de los haratin se venden a veces a Arabia Saudí y Kuwait, donde practican oficios que los árabes blancos consideran viles para ellos.

La tasa de fertilidad es de 4 hijos por mujer y casi el 60% de la población tiene menos de 25 años. La escolarización es insuficiente, sobre todo en las mujeres. La alfabetización es del 50%. La persistencia de las tradiciones esclavistas no contribuye a la educación de los haratin o moros negros, a los que se sigue sometiendo a matrimonios forzados y a la ignorancia. La sequía, la pobreza y el desempleo empujan a la emigración desde los años 1970 hacia los países de África Occidental como Senegal, Malí, Costa de Marfil y Gambia. La guerra Mauritania-Senegal de 1989 forzó a muchos mauritanos de color a emigrar a Senegal, desde donde fueron a Libia, los países del Golfo Pérsico y Europa. Actualmente, es un importante punto de tránsito de la migración hacia Europa, sobre todo hacia España a través de las Islas Canarias, Ceuta y Melilla.



</doc>
<doc id="16895" url="https://es.wikipedia.org/wiki?curid=16895" title="Martín Chambi Jiménez">
Martín Chambi Jiménez

Martín Chambi Jiménez (n. Coasa - Puno, Perú, 7 de noviembre de 1891- † m. Cuzco, Perú, 13 de septiembre de 1973) fue un fotógrafo peruano de origen quechua. Es considerado como pionero de la fotografía de retrato. Reconocido por sus fotos de profundo testimonio biológico y étnico, ha retratado profundamente a la población peruana, tanto a los indígenas como a la población en general. 
Martín Chambi buscó siempre saber más de su oficio, aprender de sus mayores en Arequipa (donde muy joven conoció a los hermanos Vargas), en el Cuzco, en Lima o en el extranjero.

Martín Chambi nace en una familia de campesinos quechuahablantes a finales del siglo XIX. En su condición de indio y desheredado, la pobreza y la muerte del cabeza de familia hace emigrar al joven Martín Chambi, con sólo catorce años, a buscar trabajo en las multinacionales que explotan las minas de oro de Carabaya en la selva a orillas del río Inambari.

La fortuna hace que sea allí donde traba su primer contacto con la fotografía, aprendiendo sus rudimentos de los fotógrafos ingleses que trabajan para la "Santo Domingo Mining Co". Ese encuentro fortuito con la nueva técnica prende en él la chispa que le decide a buscarse el sustento como fotógrafo. Para ello emigra en 1908 a la ciudad de Arequipa, donde la fotografía está muy desarrollada y donde descuellan figuras de fotógrafos notables que venían tiempo marcando un estilo propio y manejando una técnica impecable.

El contexto social y cultural en que se desarrolló fue el óptimo, pues una ola creciente de interés turístico e histórico y de investigaciones arqueológicas (la ciudadela de Machu Picchu fue descubierta oficialmente en 1911), así como la llegada al sur de los beneficios modernos de la tecnología (motocicletas, automóviles, vuelos aéreos, nuevas carreteras), fueron, indudablemente los acicates visuales de su inquieto espíritu observador. Chambi fue uno de los protagonistas de la denominada Escuela de Fotografía Cusqueña. Expuso en vida por lo menos diez veces, tanto en el Perú como fuera de él. 

Muchos críticos aseguran que dividió su trabajo en dos grupos: el de índole comercial, que incluía los retratos por encargo, en estudio y exteriores así como los grandes retratos grupales y el otro de carácter personal, que incluía su registro antropológico, básicamente retratos de la etnia andina y registro de tradicionales locales, también estarían sus numerosas vistas urbanas del Cuzco y sus vistas de restos arqueológicos. Si bien esta parte de la obra es cuantitativamente menor, se distingue por haber sido realizada con notable persistencia y continuidad.

Las tomas famosas en las que capta instantes cruciales de la vida moderna de la antigua capital del Tahuantinsuyo (por ejemplo, el primer vuelo aéreo a cargo de Velasco Astete) estarían, más bien, en el punto intermedio de ambas modalidades.

El investigador peruano Jorge Heredia, radicado en Ámsterdam, Holanda, asevera que la obra del fotógrafo ha sido revalorada desde fines de los años 1970 con resultados muy diversos, quizá tan heterogéneos como la naturaleza del mismo legado, cuya densidad, agrega, permite destacar cualquier punto de apoyo para todo tipo de presentación.

Heredia también afirma que el artista puede ser tomado ""como un fotógrafo documental al pie de la letra"" y también puede ""acercársele a cierto formalismo o ser considerado sin más como un llano producto artístico, así como hizo el pictorialismo en su época"".

Se dice que tuvo un claro sentido práctico como profesional de la imagen. Esto lo indican especialistas en la materia como el cineasta José Carlos Huayhuaca, autor del libro "Martín Chambi, fotógrafo", quien sentencia que éste era un hombre ""con los pies en la tierra"", aunque no al punto de hacer cosas por razones monetarias, pues de lo contrario se hubiese quedado en Arequipa, donde tenía más posibilidades que en el Cuzco. Una de las etapas de su vida pocas veces mencionada en detalle ha sido su labor de reportero gráfico para el diario "La Crónica" y la revista "Variedades" (1920-1927), publicaciones peruanas que ilustraron muchas de sus páginas durante el oncenio de Augusto B. Leguía y Salcedo con fotografías inéditas de Chambi, todas ellas muy sugestivas, nítidas y perfectamente concebidas.

Acontecimientos, curiosidades, hechos singulares, noticias en suma, era lo que el lente puneño, adoptado por el Cuzco, reveló en el trabajo diario, y no sólo para la capital limeña, sino también para la ciudad cosmopolita de Buenos Aires, donde colaboró en el diario "La Nación".

Y es que su obra trasciende preocupaciones personales y llega a calar a fondo en el alma colectiva del pueblo. En su caso, el arte fotográfico no deviene verticalmente de parámetros indigenistas, como podría creerse, aunque aquel estímulo de reivindicación lo ayudó a tomar conciencia de su identidad cultural, sino que se enriquece verdaderamente de sí mismo, como artista que fue en el esfuerzo por captar lo singular de cada persona, situación o paisaje.

Tras disfrutar en vida del reconocimiento de la crítica, de la prensa y del público, sufrió un decaimiento de su salud y quizá también de su obra. Pese a ello, en 1958, al celebrar sus bodas de oro como profesional, su figura se renovó e incluso recobró presencia en los medios de comunicación en entrevistas y reportajes. Parte importante del archivo Chambi, estuvo bajo el cuidado de su hija Julia, y hasta el fallecimiento de ésta el 15 de octubre de 2006, ha viajado por distintos países de Latinoamérica. La iniciativa de observar las reproducciones partió de las mismas instituciones y asociaciones extranjeras, como el Colectivo de fotógrafos de Uruguay; el Museo San Martín de Argentina; el Palacio de Bellas Artes de Chile y los Amigos de la Fotografía de São Paulo, Brasil.

El archivo cuyas placas se conservan bien por el clima seco del Cuzco y la atención de la familia, debería contar de todas formas con una inmejorable infraestructura que proteja el valioso material.
A pesar de sus declaraciones anteriores, el nieto del artista, reconoce, sin embargo, la necesidad de una sistematización digital del trabajo, para que así ya no se manipulen directamente las placas o las fotos.

Sólo después de su muerte, acaecida en 1973, su obra ha vuelto a ser estudiada, apreciada y admirada por todo el mundo, a partir de exposiciones internacionales, como la que se realizó a mediados de la década de los años 1990 en el Círculo de Bellas Artes de Madrid, España, o la más reciente, en noviembre de 2001, en París, Francia, en los sobrios ambientes del Instituto Cervantes.

Quedan en la memoria fotos notables como "Víctor Mendívil con un campesino de Paruro" (1932), "Organista en la capilla de Tinta" (1936), "Orquesta de la familia Echave" (Cuzco, 1931), así como la titulada "Chicha y sapo, costumbres cusqueñas" (1930), entre otras tomas.

Es necesario mencionar que, no obstante el esfuerzo del propio fotógrafo por difundir su obra (exposiciones en el interior, en Lima y fuera del país así lo comprueban), ésta no logró quedar en la memoria de los hombres y mujeres de su país sino hasta hace pocos años, en que recién el nombre de Martín Chambi nos dice tanto como sus impresionantes imágenes.

Chambi nació en una aldea surandina en el seno de una comunidad campesina de tradiciones indígenas quechuas. 
En Chambi se dio la concurrencia afortunada de varias circunstancias históricas, siendo las principales,-sin entrar en detalle-, la llegada tardía de la revolución industrial a los Andes, con toda la secuela de encuentros de la modernidad con la tradición; el relativo auge económico local, motivado principalmente por el aumento del comercio, las mejoras de la comunicación y los servicios, y el consiguiente interés turístico creciente por el Cuzco;y la emergencia de programas sociales y políticos pro-indígenas surgidos desde los centros urbanos con su importante correlato de movimientos artístico-literarios que se permeaban en el quehacer cultural. 

Mientras que Chambi se aprovechó con resolución, sagacidad y talento de la situación en que se encontraba, ésta es totalmente irrepetible. Es más, Chambi fue un caso aislado. La posibilidad que tuvo de realizar su obra tal como la hizo fue tan excepcional como su ascenso social.
Lo cual no quiere decir en absoluto que Chambi fuera el único fotógrafo en el Cuzco de aquel entonces, es más, parece que la efervescencia social, económica y cultural de aquellos días propició en el Cuzco el clima necesario para que una ola de fotógrafos desarrollaran obras peculiares; pero hasta el momento es el único que remonta de ese modo la escala social y también, un tanto injustamente, el único cuya obra ha sido ampliamente reconocida.
La fotografía en el Cuzco de las primeras décadas del siglo XX fue un signo más de la pujante modernidad que empapaba la sociedad, conjuntamente con el ferrocarril, la motocicleta, el automóvil y el aeroplano, cuya llegada ha sido fielmente documentada por Chambi. 

La fotografía en ese nuevo ambiente fue a su vez huella y marca, fue medio de registro y medio de darle forma a la inmigración, de uno mismo, de los demás, y también del ambiente en que se operaba. De hecho hubo un significativo encuentro de las formas de inmigración tradicionales, no sólo las reinantes en el medio burgués que apoyaba a Chambi, sino también de las formas de los grupos menos favorecidos, los mestizos y los indígenas. Y todas estas imaginaciones a su vez se encontraron con las ventajas, limitaciones y demandas de la modernidad. Por sus características particulares, el fenómeno fotográfico mismo fue un escenario que influyó tremendamente en las formas de inmigración y dejó impreso estos encuentros, todos los choques, tropezones y magulladuras. Chambi casi lo único que hace es responder con habilidad tanto al estímulo de la cultura reinante como a la de sus orígenes; se alimenta de cultura para retroalimentar a su vez la cultura.






</doc>
<doc id="16896" url="https://es.wikipedia.org/wiki?curid=16896" title="Lepidoptera">
Lepidoptera

Los lepidópteros (Lepidoptera, del griego «lepis», escama, y «pteron», ala) son un orden de insectos holometábolos, casi siempre voladores, conocidos comúnmente como mariposas; las más conocidas son las mariposas diurnas, pero la mayoría de las especies son nocturnas (polillas, esfinges, pavones, etc.) y pasan muy inadvertidas. Sus larvas se conocen como orugas y se alimentan típicamente de materia vegetal, y algunas especies pueden ser plagas muy dañinas para la agricultura. Muchas especies cumplen el papel de polinizadoras de plantas y cultivos.

Este taxón representa el segundo orden con más especies entre los insectos (siendo superado solamente por el orden Coleoptera); de hecho, cuenta con más de 165 000 especies clasificadas en 127 familias y 46 superfamilias. La mariposa diurna más grande que existe es la "Ornithoptera alexandrae" hembra, que puede llegar a tener 31 cm de envergadura (el macho es un poco más pequeño), vive al sudeste de Nueva Guinea.

Hay unas 127 familias dentro del orden Lepidoptera, pero las opiniones de cuáles son éstas cambian con frecuencia entre los científicos. El tratamiento que se da aquí es el adoptado por la base de datos del Museo de Historia Natural de Londres. Ver .

Durante muchos años, el orden de los lepidópteros fue subdividido en dos subórdenes, los ropalóceros, o mariposas diurnas, y los heteróceros, polillas o mariposas nocturnas. La cladística moderna ha demostrado que esta antigua clasificación es artificial y, en la actualidad se admiten los subórdenes Aglossata, Heterobathmiina, Zeugloptera y Glossata. Los tres primeros contienen unas pocas especies, mientras que Glossata incluye el 99% de los lepidópteros actuales.


Las relaciones filogenéticas de los cuatro subórdenes son las siguientes:

Hay muy pocos fósiles de mariposas cuando se los compara con otros grupos de insectos. La distribución y abundancia de los fósiles más comunes indican que debe haber habido grandes migraciones de mariposas durante el Paleógeno en el Mar del Norte, que es donde se encuentran muchos fósiles de este grupo.

También se encuentran algunos fósiles en ámbar y en algunos sedimentos finos. Los restos dejados por larvas de minadores de hojas Pueden ser valiosos, pero su interpretación no es fácil.

Las mariposas poseen dos pares de alas membranosas cubiertas de escamas coloreadas, que utilizan en la termorregulación, el cortejo y la señalización. Su aparato bucal es de tipo probóscide (véase Insecto) provisto de una larga trompa que se enrolla en espiral (espiritrompa) que permanece enrollada en estado de reposo y que les sirve para libar el néctar de las flores que polinizan.

El cortejo de los machos es muy variable en las diferentes familias del orden, pero básicamente consiste en exhibiciones y en la producción de feromonas sexuales. Con las maniobras de vuelo los machos cubren a las hembras con el olor de estas feromonas. Tras el apareamiento los machos pueden evitar que la hembra tenga una nueva cópula taponando su genitalia con una secreción pegajosa.

Su desarrollo es holometábolo: del huevo sale una larva u oruga que se transformará en pupa y esta dará lugar al adulto. La larva, a diferencia del adulto, presenta un aparato bucal de tipo masticador; la mayoría de las larvas son fitófagas. En menos del 1% las larvas son carnívoras o aun caníbales. Podemos distinguir las larvas de lepidópteros de las de otros insectos porque poseen una serie de 5 patas falsas —las de los himenópteros sínfitos poseen 7 o más— al final del abdomen, lo que en algunos casos conlleva que su forma de caminar sea como la de un acordeón abriéndose y cerrándose alternativamente. Los lepidópteros son insectos terrestres y solo ocasionalmente algunas larvas son acuáticas.

En el orden Lepidoptera la coloración, especialmente la de las alas, alcanza la máxima especialización. Morfológicamente, la superficie alar está recubierta de escamas cuya superficie posee multitud de aristas longitudinales (separadas a veces a menos de 1 μm, es decir, la milésima parte de un milímetro) que alteran la reflexión de la luz produciendo colores muy llamativos y frecuentemente tornasolados e iridiscentes.

Las venas de las alas de las mariposas forman un diseño característico y único según las especies o familias de las que se trate. Conocer este patrón es, en algunos casos, imprescindible para la correcta determinación de una especie concreta de mariposa. Para poder describir con claridad y precisión este diseño se utiliza la siguiente terminología entomológica:






El ala de la mariposa se desarrolla en la larva como una bolsa epidermal (disco imaginal) la cual se invagina en la metamorfosis para formar un ala inmóvil durante el estado de pupa. Las escamas pigmentadas son secretadas en el estadio tardío de la pupa, pero la interacción epidermal de la célula se determina en estadios más tempranos y determina el color de la escama definiendo el patrón en el adulto. Por su parte la mancha ocular es especificada a partir de una señalización en la región central (French, 1997). Por otro lado se cree que el patrón de coloración se organiza alrededor de un foco hipotético y que este sirve como fuente de información para la posición y síntesis del pigmento apropiado. El patrón específico aparece por las variaciones en el número de focos en el ala y la variación en la que la información de la posición es interpretada.

En otro estudio se evidenció la existencia de un foco que determina el largo de la mancha ocular en el ala posterior de "Precis coenia". Al cauterizar 300 células en el centro de la supuesta mancha ocular en el desarrollo temprano del ala, puede inhibirse completamente el desarrollo de este. Estas mismas células pueden ser trasplantadas a otra región del ala e inducir un pigmento en forma de anillo en el tejido alrededor del injerto. Este estudio demostró que el foco es una entidad fisiológica.

En estudios histológicos en la epidermis del ala se reveló que la formación de las escamas siempre ocurre en filas paralelas, próximas al eje del ala. Esta formación celular de las alas parece estar formada por diferenciación in situ y no por migración. Los pigmentos que generan el patrón de coloración en las alas son sintetizados exclusivamente en las escamas. Este patrón es formado por cuatro colores de melanina diferentes; las enzimas específicas para la síntesis de esta son incorporadas en formas insolubles en la cutícula de las escamas. La síntesis de estos pigmentos comienzan cuando el sustrato de melanización comienza a ser suministrado por el sistema circulatorio

Finalmente se ha encontrado que la expresión de genes homólogos en el patrón de apéndices de "Drosophila" también están involucrados en el patrón de coloración. El gen "angrailed" es expresado en la parte posterior y "apterous" en la superficie dorsal del disco del ala. Por su parte wingless es expresada alrededor de la margen dorso-ventral en el disco del ala. La proteína "Wg" junto con el gen Decapentaplegic han mostrado tener una función como gradiente de morfógenos en "Drosophila" controlando la expresión génica y consecuentemente el patrón morfológico en los ejes dorso-vetral y antero-posterior.

Las orugas se alimentan de la materia vegetal que las rodea: hojas, flores, frutos, tallos, raíces, lo que les da gran importancia agrícola al constituir plagas importantes de cultivos. Algunas especies son especializadas en una o unas pocas especies relacionadas, otras son polífagas, pueden alimentarse de una gran variedad de plantas de diferentes familias. Algunas especies son capaces de minar (generar túneles) en las superficies de las que se alimentan. Otras, en cambio, aprovechan las manufacturas humanas, o bien productos almacenados (harinas, granos…).

Un pequeño número de especies son carnívoras. Son de destacar las familias Epipyropidae y Lycaenidae.

Los adultos, a excepción de los representantes de la familia Micropterigidae (cuya alimentación, derivada de su capacidad masticatoria, abarca a polen, esporas de hongos, etc.), se alimentan libando, es decir, absorbiendo néctar u otras sustancias líquidas mediante su aparato bucal lamedor-chupador (espiritrompa). No obstante, existen especies cuyo ciclo vital exige una corta fase de imago: en estos casos, el adulto ni se alimenta, sino que destina todas sus energías a la reproducción.

Los lepidópteros son insectos holometábolos, es decir que tienen metamorfosis completa y pasan por los estadios: huevo, larva, pupa y adulto o imago. La gran mayoría de las mariposas son herbívoras, es decir que se alimentan de plantas. Solo unas pocas especies son carnívoras o comen lana u otros materiales. Las hembras ponen sus huevos en las plantas.

Nacen como larvas semejantes a gusanos, llamadas orugas y se alimentan de las hojas de esa planta o tallos u otras partes de la planta a la vez que crecen rápidamente. Cada especie requiere una o unas pocas especies de plantas para su alimentación, y la extinción de una planta puede arrastrar la de una mariposa.

En un momento de su desarrollo, la oruga se protege en un lugar resguardado y allí se transforma en crisálida. En este estadio no se alimenta, y sufre grandes cambios metabólicos y morfológicos, cuyo conjunto es llamado metamorfosis. La mariposa adulta sale rompiendo el esqueleto externo de la crisálida.

La mayoría de las mariposas adultas se alimentan libando el néctar de las flores con su espiritrompa, una estructura bucal extensible evolucionada a partir de algunas de las piezas bucales articuladas típicas de los insectos. Los adultos de unas pocas especies tienen una vida muy corta, carecen de piezas bucales y no se alimentan.

Esta "lengua enrollada" es flexible y muy sensible. Puede introducirse dentro de una flor, pero también puede inclinarse abruptamente, de manera que la mariposa puede alimentarse desde diferentes ángulos sin tener que mover, tan siquiera, su esqueleto. Una vez que la mariposa ha terminado de alimentarse, la lengua se retrae enroscándose y encaja exactamente debajo de la cabeza del insecto. Machos y hembras se buscan activamente, usando como guía visual su aleteo característico, y empleando el sentido del olfato. Tras la fecundación, la hembra pone varios cientos o miles de huevos. En algunos casos la vida adulta es breve, no durando más que el tiempo necesario (a veces un solo día) para asegurar la reproducción.

Algunas especies son migratorias. Entre ellas algunas se cuentan entre los insectos que cubren las mayores distancias en sus viajes.

Entre las especies migratorias mejor conocidas se cuentan la mariposa monarca ("Danaus plexippus plexippus"), el esfíngido picaflor ("Macroglossum stellatarum"), la vanesa de los cardos ("Vanessa cardui"), el almirante rojo ("Vanessa atalanta") y "Colias croceus".





</doc>
<doc id="16898" url="https://es.wikipedia.org/wiki?curid=16898" title="Geografía de Malaui">
Geografía de Malaui

Malaui es un país que se encuentra situado en el este de África. Limita al oeste con Zambia, al oeste, sur y este con Mozambique y al norte con Tanzania. Se encuentra administrativamente dividido en tres regiones: la región norte, con capital en Mzuzu, la región centro, con capital en Lilongüe, que además es capital de la nación, y la región sur con capital en Blantyre. Al este del país se ubica el lago Malaui, el tercero mayor del continente africano.

Geográficamente, en cuanto se va desplazando desde la frontera con Zambia y Mozambique hasta el lago Nyassa el país va descendiendo en altura hasta llegar al lago Malaui y al valle del río Shire, que desemboca en el río Zambeze. De igual manera el clima varía de templado en las alturas a 25 °C en las zonas bajas.

Malaui es uno de los países de África subsahariana más densamente poblados. Con 16,5 millones de habitantes tiene una densidad de 130 hab/km². La capital, Lilongüe, tenía en 2013 más de 800.000 habitantes. La segunda ciudad del país es Blantyre, en el sur, con 750.000 habitantes, seguida a larga distancia por Mzuzu, con 154.150 habitantes, y Zomba, con 98.800 habitantes.

Malaui está atravesado de norte a sur por el Gran Valle del Rift, en el fondo del cual se encuentra el lago Malaui, también llamado lago Niassa, que ocupa el 25 por ciento del país y las tres cuartas partes de la frontera oriental con Tanzania y Mozambique. El lago Niassa es llamado a veces el lago Calendario porque tiene 365 millas de longitud y 52 millas de anchura, 587 km y 84 km respectivamente. El río Shire fluye desde el extremo sur del lago, atraviesa Malaui, cruza la frontera con Mozambique y 400 km después se une al río Zambeze.

La parte occidental de Malaui es una altiplanicie montañosa que sigue paralelamente el valle del Rift de sur a norte; en el sur y el centro, la meseta se alza a 900-1.200 m, pero en el norte se encuentra la meseta Nyika, una zona protegida que se adentra en Zambia y supera los 2.000 m, con cima en el monte Nganda, de 2.605 m. Toda la región occidental del lago forma parte del ecosistema sabana arbolada de miombo del Zambeze central.

Al sur de Malaui y el lago Niassa, se encuentra la meseta de Shire, al este del río Shire, una alargada altiplanicie de 130 km de longitud con alturas de 600 a 900 m y cimas en las elevaciones de Thyolo (1.462 m), Ndirande (1.612 m) y Chiradzulu (1.773 m). El punto más alto es la meseta de Zomba, de 1.800 m, con extensas matas de cedros, pinos y cipreses. Desde la cima, es posible ver el lago Chilwa, al norte; el río Shire, al oeste, y el macizo de Mulanje, al este, un grupo de montañas aisladas que se eleva desde las altiplanicies circundantes del distrito de Chiradzulu y los cultivos de té del distrito de Mulanje, que alcanzan 3.002 m en el pico de Sapitwa, el más alto del país.

Malaui está al sur del ecuador; tiene clima tropical cálido con una zona más templada en las montañas del noroeste, y dos estaciones muy marcadas, una cálida y lluviosa de noviembre a abril, el verano del hemisferio sur, y otra relativamente fresca y seca de mediados de mayo a mediados de agosto. Antes de la estación de las lluvias, en noviembre, las temperaturas alcanzan un máximo; por encima de 1.000 m de altitud son mucho más suaves. El sur, a menor altitud, padece el clima más caluroso. 

En la mayor parte del país, las lluvias oscilan entre 800 y 1.300 mm, algo más abundantes en el norte y en las laderas del macizo de Mulanje, al sur, donde se superan los 2.000 mm. 

La capital, Lilongüe, a 1.100 m de altitud, en la parte norte del tercio sur del país, recibe 846 mm al año en 58 días, entre los meses de noviembre y mediados de abril, superando los 200 mm en enero y febrero, con temperaturas esos meses entre 14 C y 26 C. Entre mayo y octubre no llueve, y las temperaturas bajan mucho por la noche, con mínimas medias de 6-7 c y máximas de 23-25 C entre junio y agosto.

En Blantyre, la segunda ciudad del país, a 1.000 m de altitud, el clima es similar, llueve un poco más, 1086 mm, con una media anual de 20.7 C. Sin embargo, las mínimas apenas bajan de 12 C en julio.

A orillas del lago Niassa, en Karonga, en el norte, a 478 m de altitud, caen 1.170 mm, casi todo entre noviembre y mayo; las temperaturas medias van de 21 C en enero a 26 C en noviembre, cuando hace más calor.

En el lejano sur, a orillas del río Shire, en Nsanje, a solo 55 m de altura, caen 900 mm entre noviembre y marzo, pero no deja de llover del todo el resto del año, y las temperaturas superan los 36 C de media en octubre y noviembre, con mínimas medias de 14 C en junio y julio.

Por último, en Mzuzu, tercera ciudad del país, en el norte, a 1.250 m de altitud, caen cerca de 1.300 mm al año, las temperaturas bajan de 7 C en julio y agosto y alcanzan los 28 C de media en noviembre.

Malaui tiene una gran riqueza natural, que incluye la mayoría de los grandes mamíferos del continente, incluidos los cinco grandes de África: búfalo, elefante, león, leopardo y rinoceronte. En conjunto, hay unas 170 especies de mamíferos y unas 600 especies conocidas de aves.

Al ser un país muy poblado, los animales se ven relegados a los cinco parques nacionales y a las diversas reservas de caza, incluido el lago Malaui, que contiene especies de peces únicas. De norte a sur:


En Malaui hay también cuatro reservas de caza adyacentes a los parques nacionales: 



</doc>
<doc id="16901" url="https://es.wikipedia.org/wiki?curid=16901" title="MP3">
MP3

MPEG-1 Audio Layer III o MPEG-2 Audio Layer III, más comúnmente conocido como MP3, es un formato de compresión de audio digital que usa un algoritmo con pérdida para conseguir un menor tamaño de archivo. Es un formato de audio común utilizado para música tanto en ordenadores como en reproductores de audio portátil.

MP3 fue desarrollado por el Moving Picture Experts Group (MPEG) para formar parte del estándar MPEG-1 y del posterior y más extendido MPEG-2. Un MP3 creado usando una compresión de 128 kbit/s tendrá un tamaño de aproximadamente unas 11 veces menor que su homónimo en CD. Un MP3 también puede comprimirse usando una mayor o menor tasa de bits por segundo, resultando directamente en menor calidad de audio final, así como en el tamaño del archivo resultante.

Este formato fue desarrollado principalmente por Karlheinz Brandenburg, director de tecnologías de medios electrónicos del Instituto Fraunhofer IIS, perteneciente al Fraunhofer-Gesellschaft —red de centros de investigación alemanes— que junto con Thomson Multimedia (renombrada como Technicolor) controlaba el grueso de las patentes relacionadas con el formato MP3.

El científico alemán Karlheinz Brandenburg es considerado como «el padre del MP3» y formó parte del equipo que le dio nombre al formato MPEG: «Grupo de Expertos de Imágenes en Movimiento» («"Moving Pictures Experts Group"»). El grupo cedió el nombre al método digital para comprimir señales de audio y video para facilitar su emisión y almacenamiento. El proceso llamado MPEG Audio Capa III (MPEG Audio Layer 3) es bastante común hoy en día pero su desarrollo no era fácil. Además otros soportes de audio ya usaban técnicas de compresión como el MPEG-1 Audio Layer I del casete compacto digital de Philips o el ATRAC usado por Sony para sus Minidisc.

La historia inicia en 1982, cuando Brandenburg participa en la creación del formato. Su tutor de tesis doctoral había deseado patentar un método para transferir datos sin éxito. Lo que se pretendía patentar era una forma de transferir música usando líneas telefónicas, algo que la oficina de patentes germana consideraba imposible. No aparecería cierto progreso hasta 1986, cuando en la universidad de Ilmenau obtuvieron mejores computadoras para mejorar la capacidad de trabajo. Se tuvieron que realizar muchos esfuerzos para lograr el resultado deseado. Inicialmente se pensó en usar un sistema por división de capas de sonido, pero fue desechado por ser considerado demasiado rígido, y entonces se cambió por uno nuevo que aprovecha las limitaciones del oído humano.

La primera de ellas fue registrada en 1987. En ese año, los alemanes intentaban resolver el dilema de cómo difundir el sonido digital en el laboratorio de tecnologías de medios electrónicos. Los archivos en CD eran pesados y engorrosos, las lectoras de CD eran novedad, también el instalarlas en una PC.

En 1988 la Organización Internacional de Normalización convoca al equipo de MPEG para crear un estándar de codificación de audio. En este momento se realizaban las pruebas del nuevo sistema. Parecía que finalmente lograron su objetivo, pero al momento de probarlo con «Tom's Dinner» de Suzanne Vega mostró fallos graves, puesto que el formato de canción a capela y el escaso sonido ambiental representaban un enorme desafío para el sistema. Entonces se solicitó la colaboración de varias instituciones. Brandenburg trabajó con Jim Johnston de AT&T en el desarrollo de nuevos métodos de compresión para conservar la calidad de la voz. Por fin consiguieron evitar que el sistema dañara la voz de la cantante, y tuvieron que trabajar más para obtener finalmente una calidad similar a la de un CD.

En 1992 la ISO incluyó al MP3 como un estándar de compresión de audio, pero no fue hasta el año siguiente cuando fue finalmente formalizado con la llegada del MPEG-1 Capa de Audio III (MPEG-1 Audio Layer III), con velocidades de muestreo de 33, 44,1 y 48 kHz. Entonces decidieron comercializarlo a empresas para transferir la música a los estudios de radio mediante RDSI. 

Registraron varias patentes más en 1991, pero fue en julio de 1995 cuando Brandenburg usó por primera vez la extensión codice_1 para los archivos relacionados con el MP3 que guardaba en su ordenador. En el proceso de desarrollo del formato participó también el ingeniero Leonardo Chiariglione, quien tuvo la idea de los estándares que podrían ser útiles para este fin. Un año después su instituto ingresaba en concepto de patentes 1,2 millones de euros. Diez años más tarde esta cantidad ha alcanzado los 26,1 millones.

Entre 1994 y 1995 identificaron a la Internet como una área atractiva. Entonces decidieron dar a los archivos el nombre definitivo de codice_1. El modelo de negocio se planeaba como herramientas de codificación costosas para empresas y decodificadores económicos para los consumidores. Uno de los productos decodificadores que lograron el mayor éxito y reconocimiento fue Winamp.

Sin embargo, no pasó mucho tiempo antes de que el formato fuera despojado del control de la ISO y Fraunhofer, dado que un ciudadano australiano había comprado el codificador usando una tarjeta de crédito robada de Taiwán para posteriormente empaquetarlo y cargarlo a un servidor FTP de una universidad estadounidense. Se ignora hasta el momento la identidad y el paradero del infractor. Esto inició el conflicto entre la industria discográfica y el MP3, convirtiéndolo en su principal enemigo, aunque en Asia el éxito persistente de los discos compactos esté ayudando a conservar las ventas en formato físico. 

Tras el desarrollo de reproductores portátiles y su integración en estéreos para automóviles, teléfonos móviles, reproductores de DVD, auriculares, consolas de videojuegos, altavoces y minisistemas de sonido hogareños, el formato MP3 en la actualidad llega más allá del mundo de la informática.

El formato MP3 se convirtió en el estándar utilizado para "streaming" de audio y compresión de audio con pérdida de mediana fidelidad gracias a la posibilidad de ajustar la calidad de la compresión, proporcional a la tasa de bits (bitrate) y en consecuencia el tamaño final del archivo, permitiendo reducir hasta 12 e incluso 15 veces el del archivo original antes de su compresión.

Fue el primer formato de compresión de audio popularizado gracias a Internet, ya que hizo posible el intercambio de ficheros musicales. Los procesos judiciales contra empresas como Napster, AudioGalaxy y Megaupload son resultado de la facilidad con que se comparten legal e ilegalmente este tipo de ficheros, suponiendo el principal auge de la batalla por la propiedad intelectual en Internet.

A principios de la década de los 2000 Thomson Multimedia renueva el formato con el nombre MP3Pro para suplir limitaciones importantes en la calidad (especialmente en altas frecuencias), paralelamente a la aparición de formatos de compresión de audio competidores, como Windows Media Audio (de Microsoft), Ogg Vorbis, ATRAC y AAC, que empiezan a ser masivamente incluidos en programas de audio para computación, dispositivos, sistemas operativos, teléfonos celulares y reproductores portátiles, lo que hizo prever que el MP3 compartiera popularidad con los nuevos formatos, de mejor calidad.

Un factor que posiblemente influyó en la aparición de tanta competencia es que el formato MP3 tenía patentes, lo cual no implicaba que su calidad sea mala, pero lo convertía en un estándar cerrado. Eso impidió que la comunidad pueda mejorarlo y puede obligar a pagar por la utilización del códec, lo cual ocurre en el caso de los dispositivos que lo usan, como los teléfonos celulares y las tabletas. Aun así, hoy día el formato MP3 continúa siendo el más usado y el que goza de más éxito con una presencia cada vez mayor. Algunas tiendas en línea como Amazon y Google Play Music venden su música en este formato por cuestiones de compatibilidad. En 2017 expiraron todas las patentes relacionadas con el formato MP3.

En esta capa existen varias diferencias respecto a los estándares MPEG-1 y MPEG-2, entre las que se encuentra el llamado banco de filtros para que el diseño tenga mayor complejidad. Esta mejora de la resolución frecuencial empeora la resolución temporal introduciendo problemas de preeco que son predichos y corregidos. Además, permite calidad de audio en tasas tan bajas como 64 kbps.

Los archivos MPEG-1 corresponden a las velocidades de muestreo de 32, 44,1 y 48 kHz.

Los archivos MPEG-2 corresponden a las velocidades de muestreo de 16, 22,05 y 24 kHz.

El banco de filtros utilizado en esta capa es el llamado banco de filtros híbrido polifase/MDCT. Se encarga de realizar el mapeado del dominio del tiempo al de la frecuencia tanto para el codificador como para los filtros de reconstrucción del decodificador. Las muestras de salida del banco están cuantificadas y proporcionan una resolución en frecuencia variable, 6x32 o 18x32 subbandas, ajustándose mucho mejor a las bandas críticas de las diferentes frecuencias.
Usando 18 puntos, el número máximo de componentes frecuenciales es: 32 × 18 = 576. Dando lugar a una resolución frecuencial de: 24000/576 = 41,67 Hz (si fs = 48 kHz). Si se usan 6 líneas de frecuencia la resolución frecuencial es menor, pero la temporal es mayor, y se aplica en aquellas zonas en las que se espera efectos de preeco (transiciones bruscas de silencio a altos niveles energéticos).

La Capa III tiene tres modos de bloque de funcionamiento: dos modos donde las 32 salidas del banco de filtros pueden pasar a través de las ventanas y las transformadas MDCT y un modo de bloque mixto donde las dos bandas de frecuencia más baja usan bloques largos y las 30 bandas superiores usan bloques cortos.
Para el caso concreto del MPEG-1 Audio Layer 3 (que concretamente significa la tercera capa de audio para el estándar MPEG-1) específica cuatro tipos de ventanas: (a) NORMAL, (b) transición de ventana larga a corta (START), (c) 3 ventanas cortas (SHORT)

La compresión se basa en la reducción del margen dinámico irrelevante, es decir, en la incapacidad del sistema auditivo para detectar los errores de cuantificación en condiciones de enmascaramiento. Este estándar divide la señal en bandas de frecuencia que se aproximan a las bandas críticas, y luego cuantifica cada subbanda en función del umbral de detección del ruido dentro de esa banda. El modelo psicoacústico es una modificación del empleado en el esquema II, y utiliza un método denominado predicción polinómica. Analiza la señal de audio y calcula la cantidad de ruido que se puede introducir en función de la frecuencia, es decir, calcula la «cantidad de enmascaramiento» o umbral de enmascaramiento en función de la frecuencia.

El codificador usa esta información para decidir la mejor manera de gastar los bits disponibles. Este estándar provee dos modelos psicoacústicos de diferente complejidad: el modelo I es menos complejo que el modelo psicoacústico II y simplifica mucho los cálculos. Estudios demuestran que la distorsión generada es imperceptible para el oído experimentado en un ambiente óptimo desde los 192 kbps y en condiciones normales. Para el oído no experimentado, o común, con 128 kbps o hasta 96 kbps basta para que se oiga «bien» (a menos que se posea un equipo de audio de alta calidad donde se nota excesivamente la falta de graves y se destaca el sonido de «fritura» en los agudos). Las personas que tienen experiencia en la parte auditiva de archivos digitales de audio, especialmente música, desde 192 hasta 256 kbps basta para oír bien, pero la compresión en 320 kbps es la óptima para cualquier escucha. . La música que circula por Internet, en su mayoría, está codificada entre 128 y 192 kbps, aunque hoy debido al aumento de ancho de banda es cada vez más frecuente compartir archivos en calidad máxima de compresión.

La solución que propone este estándar en cuanto a la repartición de bits o ruido, se hace en un ciclo de iteración que consiste de un ciclo interno y uno externo. Examina tanto las muestras de salida del banco de filtros como el SMR (signal-to-mask ratio) proporcionado por el modelo psicoacústico, y ajusta la asignación de bits o ruido de cuantificación, según el esquema utilizado, para satisfacer simultáneamente los requisitos de tasa de bits y de enmascaramiento. Dichos ciclos consisten en:

El ciclo interno realiza la cuantización no-uniforme de acuerdo con el sistema de punto flotante (cada valor espectral MDCT se eleva a la potencia 3/4). El ciclo escoge un determinado intervalo de cuantización y, a los datos cuantizados, se les aplica codificación de Huffman en el siguiente bloque. El ciclo termina cuando los valores cuantizados que han sido codificados con Huffman usan menor o igual número de bits que la máxima cantidad de bits permitida.

Ahora el ciclo externo se encarga de verificar si el factor de escala para cada bandas tiene más distorsión de la permitida (ruido en la señal codificada), comparando cada banda del factor de escala con los datos previamente calculados en el análisis acústico. El ciclo externo termina cuando una de las siguientes condiciones se cumple:


Este bloque toma las muestras cuantificadas del banco de filtros, junto a los datos de asignación de bits/ruido y almacena a agapio el audio codificado y algunos datos adicionales en las tramas. Cada trama contiene información de 1152 muestras de audio y consiste de un encabezado, de los datos de audio junto con el chequeo de errores mediante CRC y de los datos particulares (estos dos últimos opcionales).

La normalización de volumen, también conocido como Normalización de audio, básicamente consiste en la nivelación del volumen de las pistas que conforman un álbum, lo que permite escuchar las canciones que lo componen siempre con el mismo volumen, evitando el salto entre una canción que «suena bajo» con otra que «suena alto». Para ello se utilizan programas como QMP3Gain.

Un fichero MP3 se constituye de diferentes tramas que a su vez se componen de una cabecera y los datos en sí. Esta secuencia de datos es la denominada «Stream Elemental». Cada una de las tramas es independiente, es decir, pueden ser cortadas las tramas de un fichero MP3 y después reproducirlos en cualquier reproductor MP3 del Mercado.
La cabecera consta de una palabra de sincronismo que es utilizada para indicar el principio de una trama válida. A continuación siguen una serie de bits que indican que el fichero analizado es un fichero Standard MPEG y si usa o no la capa 3. Después de todo esto, los valores difieren dependiendo del tipo de archivo MP3. Los rangos de valores quedan definidos en la norma ISO/IEC 11172-3.

En matemáticas, la transformada de Fourier discreta, designada con frecuencia por la abreviatura DFT (del inglés "discrete Fourier transform"), y a la que en ocasiones se denomina transformada de Fourier finita, es una transformada de Fourier ampliamente empleada en tratamiento de señales y en campos afines para analizar las frecuencias presentes en una señal muestreada, resolver ecuaciones diferenciales parciales y realizar otras operaciones, como convoluciones. Es utilizada en el proceso de elaboración de un fichero MP3.

La transformada de Fourier discreta puede calcularse de modo muy eficiente mediante el algoritmo FFT.



</doc>
<doc id="16903" url="https://es.wikipedia.org/wiki?curid=16903" title="Luis Segalá y Estalella">
Luis Segalá y Estalella

Luis Segalá y Estalella (Barcelona, 21 de junio de 1873 - ibíd., 17 de marzo de 1938) fue un lingüista español.
En 1895 accedió al cargo de profesor auxiliar de la Universidad de Barcelona. Fue catedrático de griego en la Universidad de Sevilla (1899-1906) y en la Universidad de Barcelona desde 1906.

A partir de 1910 se desempeñó como Miembro Numerario de la Real Academia de las Buenas Letras de Barcelona. Fue miembro de la Sección Filología del Instituto de Estudios Catalanes a partir de 1911. En 1912 fue nombrado Individuo Correspondiente de la Real Academia Sevillana de Buenas Letras.Fue codirector de la «Colección de Clásicos griegos y Latinos» y de la «Biblioteca de autores griegos y latinos».

Sus traducciones de la "Ilíada" y la "Odisea" se han reeditado numerosas veces y son las más conocidas. Murió durante el bombardeo de Barcelona por la aviación italiana fascista.




</doc>
<doc id="16905" url="https://es.wikipedia.org/wiki?curid=16905" title="Luser">
Luser

Luser, castellanizado Lúser, contracción del inglés entre "loser" (perdedor, fracasado) y "user" (usuario), es un término utilizado por hackers y BOFHs para referirse a los usuarios comunes ("" en inglés o l-users, de ahí el término), de manera despectiva y como burla. "Luser" es el usuario común, que generalmente se encuentra en desventaja frente a los usuarios expertos, quienes pueden controlar todos los aspectos de un sistema.

El término fue acuñado en 1975 en el MIT después de que algunos estudiantes modificaran la programación de la computadora ITS para que en vez de informar de "XX users online", informara de "XX losers online". Finalmente la palabra "losers" (perdedores, fracasados) fue cambiada por "lusers" que se pronuncia igual pero no significaba nada ya que era un término nuevo, que mezclaba "losers" con "users" (usuarios).

También es utilizado para designar a todo usuario leecher que no busca las oportunidades de aprender, y que en cambio espera obtener las máximas facilidades de uso.

El término fue reutilizado años después en 1991 cuando Linus Torvalds creó LINUX. Sus primeras e incipientes comunidades de usuarios eran vistas como "Lusers" = Linux Users




</doc>
<doc id="16906" url="https://es.wikipedia.org/wiki?curid=16906" title="Luminosidad">
Luminosidad

En física de partículas se define la luminosidad instantánea porque es el número de partículas por unidad de superficie y por unidad de tiempo en un haz. Se mide en unidades inversas de sección eficaz por unidad de tiempo. Al integrar esta cantidad durante un período se obtiene la luminosidad integrada, la cual se mide en unidades inversas de sección eficaz (como por ejemplo el pb). Cuanto mayor es esta cantidad mayor es la probabilidad de que se produzcan sucesos interesantes en un experimento de altas energías. 
Dado un proceso cuya sección eficaz, σ, conocemos, para una luminosidad integrada, "L", dada, podemos estimar el número de veces que se va a producir ese suceso simplemente multiplicando ambas cantidades:

En astronomía, la luminosidad es la potencia (cantidad de energía por unidad de tiempo) emitida en todas direcciones por un cuerpo celeste. Está directamente relacionada con la magnitud absoluta del astro. Este valor no es constante si se consideran períodos suficientemente largos, ya que la estrella va cambiando su luminosidad según el estado en que se encuentre, pero se mantiene constante en períodos usuales para el humano. Si bien puede llevar a confusión, en astronomía la luminosidad es un concepto diferente al de brillo; el brillo depende fundamentalmente de la distancia a la que nos encontramos de un determinado objeto, mientras que la luminosidad es una propiedad física.

La luminosidad del Sol, L o L es la unidad clásica usada en astronomía para comparar la luminosidad de otros astros. Su valor aproximado es de 

Se observa que esta es una cantidad constante, y que no depende de ninguna distancia de medición.

Podemos calcular una aproximación de la constante con pocos datos. La densidad de potencia que la Tierra recibe del Sol es aproximadamente:

Una esfera de radio "R" igual a 1 UA tiene una superficie de

Si suponemos que la densidad de potencia que emite el Sol se mantiene constante en todas las direcciones, podemos calcular la potencia total emitida como:



</doc>
<doc id="16907" url="https://es.wikipedia.org/wiki?curid=16907" title="Lissamphibia">
Lissamphibia

Los lisanfibios (Lissamphibia), también conocidos como anfibios modernos, son un clado de tetrápodos que, como el nombre común lo indica, incluye a todos los anfibios actuales, los cuales están representados por más de 7.420 especies. El grupo está conformado por los clados Gymnophiona (cecilias), Caudata (salamandras), Salientia (ranas) y el grupo extinto Albanerpetontidae. Comparten numerosas características, entre las que se pueden nombrar la presencia de dientes pedicelados en los que la base y la corona están separadas por una zona de tejido fibroso, costillas reducidas y generalmente soldadas a las apófisis transversas vertebrales, perdida del hueso yugal, presencia en el oído medio de un elemento sensorial para captar vibraciones de baja frecuencia ("papila amphibiorum"), piel vascularizada como adaptación a la respiración cutánea, cuerpos grasos asociados con las gónadas, etc.

El origen del grupo aún es incierto, pudiendo dividirse las hipótesis actuales en tres principales categorías. En la primera Lissamphibia es considerado como un grupo monofilético derivado de los temnospóndilos en cuyo caso el grupo hermano puede ser el género "Doleserpeton", "Doleserpeton" y "Amphibamus", Branchiosauridae o un subgrupo de este último. Aunque es posible que estas similitudes de ciertos anfibios modernos (Batrachia) con especies del grupo Dissorophoidea, como "Doleserpeton", correspondan a condiciones simplesiomórficas de los tetrápodos. Por otra parte, la segunda hipótesis también establece a Lissamphibia como un grupo monofilético, pero derivado de los lepospóndilos, mientras que la tercera hipótesis sugiere un carácter polifilético (difilético y en algunos estudios trifilético) de los lisanfibios, con un origen de las ranas y las salamandras a partir de los temnospóndilos, mientras que las cecilias (y a veces las salamandras) derivarían de los lepospóndilos.

Esta última hipótesis se vio reforzada por el descubrimiento de la especie "Eocaecilia micropodia", una cecilia del Jurásico Inferior que fue asociada con el género "Rhynchonkos" (Pérmico Inferior) del grupo Microsauria. Sin embargo, esta visión de una relación entre "Eocaecilia" y Microsauria ha sido rechazada por ciertos investigadores, quienes argumentan que las apomorfías atribuidas a esa relación son homoplásticas, reflejando adaptaciones convergentes a un estilo de vida fosorial. Por otra parte, un origen monofilético de los anfibios modernos con respeto a los linajes actuales de amniotas está fuertemente avalado por los análisis moleculares. Por lo tanto, si los lepospóndilos son un grupo más cercano a los amniotas, la hipótesis del origen polifilético puede ser indirectamente rechazada ya que requeriría de una relación más cercana de las cecilias con los amniotas que con el clado Batrachia.

Kumar & Hedges (1998) estimaron el origen de los lisanfibios en 360 Ma, mientras que Zhang "et al." (2005) lo determinó en 337 Ma. Estos hallazgos fueron cuestionados debido a la contradicción con el registro fósil de los tetrápodos del Paleozoico y porque favorecerían la hipótesis del origen polifilético. Los registros de fósiles de lisanfibios son escasos (siendo los más antiguos los de "Triadobatrachus" y "Czatkobatrachus") y se encuentran en edades geológicas mucho menos antiguas de las que suguieren estos resultados. Los estudios moleculares recientes han reducido esta discrepancia con el registro fósil, rescatando ciertos análisis una fecha entre 322 y 274 Ma respectivamente, mientras que otros estiman el origen de los lisanfibios en 294 Ma y el del clado Batrachia en 264 Ma, estando esta última fecha cercana a los registro de "Triadobatrachus" (250 Ma) y "Czatkobatrachus". Por lo general estas estimaciones son controversiales, con una diferencia de entre 87 y 103 Ma entre las estimaciones menos antiguas y las más antiguas respectivamente, aunque como se señalaba, los últimos estudios han tendido a favorecer una divergencia más reciente.
Cladograma basado en Marjanovic & Laurin (2007).




</doc>
<doc id="16909" url="https://es.wikipedia.org/wiki?curid=16909" title="Linfedema">
Linfedema

El linfedema se refiere al tipo de edema producido por una obstrucción en los canales linfáticos del organismo. 

Tal situación se produce por la acumulación de la linfa (compuesta por un líquido claro rico en lípidos y fibroblastos) en los espacios intersticiales (área existente entre las distintas células de un tejido), dentro del tejido celular subcutáneo. 

Obedece por lo general a un fallo o a una insuficiencia en el sistema linfático, y trae como consecuencia el aumento del volumen de las extremidades, en forma completa o parcial, y la desaparición de los relieves que por debajo de la piel se aprecian. 
Hay que vigilarlo estrechamente en extirpación de la mama (por un tumor mamario) y linfadenectomía.

Una acumulación de la linfa en algún punto del cuerpo provoca un linfedema, que a su vez puede ser primario o secundario.

Ocurre cuando el sistema de conductos y/o ganglios linfáticos de una zona tiene dificultades o directamente es incapaz de transportar las proteínas grandes y otras moléculas para ser absorbidas de nuevo por el sistema venoso. 

Es consecuencia de una cirugía o una radioterapia que hayan requerido la extirpación o la radiación de los ganglios linfáticos, provocando una posterior anomalía en el proceso de drenaje.

Los linfedemas por lo general se presentan en una gran variabilidad de formas.

Cuando se trata de linfedemas primarios, que por lo general obedecen a alguna alteración anatómica o congénita de los conductos linfáticos, la presentación puede incluso ser desde el nacimiento o la infancia, pero con más frecuencia aparecen a partir los 35 años, como consecuencia de un pequeño traumatismo o esguince en una extremidad. Por lo general comienza como un edema en tarso y tobillo.

Por su parte, los linfedemas secundarios se relacionan con la existencia de tumores que afectan a las cadenas ganglionares (próstata, ovario, mama,…) o con la extirpación quirúrgica o radioterapia de estos tumores y las zonas periféricas. Puede ser inmediata su aparición, aunque también se dan casos en que lo hacen muchos años después del tratamiento y sin un aparente motivo desencadenante.

Si bien existen diferentes pruebas de imagen que facilitan el diagnóstico de un linfedema (como son la TAC, RNM, linfografía), hay consenso acerca de que la prueba que mayor información proporciona es la linfografía isotópica. Se trata de imágenes denominadas gammagrafías del sistema linfático.

Esta alternativa se encuentra en dentro de la llamada medicina nuclear, que recurre a cantidades muy pequeñas de material radioactivo para diagnosticar o bien para tratar diferentes enfermedades, incluyendo muchos tipos de cáncer, enfermedades cardíacas y ciertas otras anomalías corporales. 

Se trata de procedimientos no invasivos que por lo general no llevan aparejados dolores. Las imágenes de medicina nuclear tienen la gran ventaja de aportar información precisa para la elaboración de diagnósticos. 

Para la elaboración de imágenes se recurre a radiofármacos, que son materiales radioactivos. Según el tipo de examen de que se trate, el radiofármaco podrá ser inyectado en una vena, ingerido en forma oral o inhalado como gas, aunque en general de forma ambulatoria. Finalmente, este material se acumula en el área del cuerpo que se pretende examinar y desde allí emite energía en forma de rayos gamma.

El proceso se completa cuando esta energía es detectada por un dispositivo llamado gammacámara, un escáner y/o sonda para PET (tomografía por emisión de positrones). Todos estos receptores trabajan en sintonía con una computadora que logra medir la cantidad de radiofármaco absorbido por el cuerpo, y sobre la base de ello genera imágenes especiales que proporcionan detalles tanto de la estructura como de la función de los órganos y tejidos. 

Se utiliza una pequeña aguja para inyectar el radiofármaco por debajo de la piel, o incluso a mayor profundidad. Acto seguido, la gammacámara comenzará a registrar imágenes del área del cuerpo que se pretende monitorear. Incluso puede la cámara realizar algunos movimientos rotatorios alrededor del paciente, o en contrapartida, se le podrá pedir a éste que cambie de posición ante una cámara fija. El procedimiento puede afectar a personas que padecen claustrofobia, por lo cual es necesario informarlo previamente.

Con esta nueva tecnología se han logrado reemplazar procedimientos algo más complejos que eran los que anteriormente se utilizaban para evaluar el sistema linfático. Pero además permite determinar la diseminación de un cáncer hacia los ganglios linfáticos (linfangiografía). 


La terapia física descongestiva compleja es considerada el tratamiento más eficaz contra el linfedema. Se trata de un conjunto de técnicas cuyo propósito es eliminar el edema y luego procurar normalizar la función del sistema linfático generando conductos “neolinfáticos”. Bajo esta terapia se incluyen diversos recursos que se combinan en un solo tratamiento.

Se trata de la activación manual del transporte líquido intersticial a través de los canales prelinfáticos y de la linfa a través de vasos linfáticos. Lo que se busca con el DLM es reproducir en forma manual aquellos movimientos que por alguna razón el sistema linfático ya no puede hacer por sí mismo. Con esto se pretende eliminar el edema y desarrollar potenciales nuevos conductos linfáticos en un área determinada o una extremidad. 

El DLM consiste en un masaje superficial, muy suave y lento. Por lo general se inicia en una zona alejada a la enferma, pero paulatinamente avanza hacia ella procurando lograr que los tejidos estén favorecidos para evacuar el edema distal hacia ellos, especialmente el acumulado en la piel y debajo de ella (el tejido celular subcutáneo, situado entre la piel y la capa muscular).

Son muchas las ventajas del DLM, tanto en sus efectos fisiológicos como los terapéuticos. Estos son algunos:

La aparición de infecciones locales resulta sumamente amenazante para los pacientes de linfedemas. Esto es así debido a que pueden afectar y lesionar a los conductos linfáticos existentes y con ello agravar el linfedema. Por esa razón es tan necesario el cuidado de la piel, que obliga a vigilar diariamente si existen pequeñas lesiones cutáneas (padrastros, uñas encarnadas, cortes, foliculitis, pie de atleta, etc.) en la zona afectada, y que puedan ser puerta de entrada para infecciones. En tal situación, se hace imprescindible la inmediata toma de antibióticos.

Además el edema crónico produce sequedad de la piel, aparición de lesiones eccematosas, prurito (picores) y lesiones de rascado. Por esa razón es imprescindible una correcta y abundante hidratación de la extremidad en forma diaria para prevenir este tipo de complicaciones.

Parte del tratamiento contra los linfedemas implica la realización de ciertos ejercicios físicos, diseñados específicamente para actuar en tres niveles: 

A título general, cualquier ejercicio físico que favorezca el control del sobrepeso será favorable para quienes padecen linfedemas. Los más recomendables son la natación o aquagym y el Tai Chi. Sin embargo, se deberían evitar ejercicios como el aerobic o el trampolín, que pueden ocasionar daños. Por su parte, los chorros de agua fría también pueden ser beneficiosos para mejorar el linfedema.

Resultan parte fundamental del tratamiento y control del linfedema, y se realizan ya sea con vendajes compresivos o con medias elásticas, con la recomendación general de que se utilicen los tejidos más finos que la compresión necesaria permita. Esto es así porque se deben evitar definitivamente zonas de estrangulamiento en la piel, y además garantizar que la compresión sea confortable y decreciente (mayor en pierna o antebrazo y menor en muslo o brazo).

Las vendas se utilizan mientras se realiza el drenaje linfático manual y deben ser colocadas por el fisioterapeuta tras la finalización de cada sesión y mantenidas durante el descanso nocturno. 

Las medias elásticas hechas a medida deben tener una compresión extra-fuerte (>60 mmHg). Al igual que las vendas, se utilizan al concluir la sesión de DLM. Por lo general su colocación puede ser dificultosa, sobre todo para pacientes mayores. Deben colocarse por la mañana, antes de levantarse de la cama y retirase al finalizar el día para sustituirlas por el vendaje compresivo. Por lo general, para la colocación de este último en forma correcta es necesaria la asistencia de una persona.

Se trata de una técnica nacida hacia la década de 1970 en Corea y Japón. Se trata de una cinta elástica adhesiva fabricada con un grosor, peso y elasticidad similares al de la piel humana. Además de ser hipoalergénica, es resistente al agua y elástica longitudinalmente. Fue desarrollada para facilitar los movimientos y simularlos durante el reposo, ayudando en la función muscular sin limitar los movimientos corporales.

Basada en el concepto de que la actividad muscular es imprescindible para recuperar la salud, la kinesiotape logra mejorar la circulación sanguínea y linfática, pero además tiene efectos analgésicos, mejora de la movilidad articular y normaliza el tono muscular. 

El mecanismo de actuación sobre el linfedema se genera al producir una elevación de la piel, creando más espacio en la zona del subcutáneo, donde se encuentran los vasos iniciales linfáticos (linfangiones), los capilares y diversos receptores aferentes y eferentes. Tal elevación disminuye de modo inmediato la presión, restableciendo la circulación sanguínea y la evacuación linfática. Pero además, el movimiento del paciente provoca que el kinesiotape realice un bombeo que estimula la circulación linfática durante todo el día.

La colocación del kinesiotape dependerá de la zona a tratar. No obstante, lo común es que se recurra a tiras largas y finas con una ligera tensión. Es muy importante la dirección de colocación de las tiras para favorecer el retorno linfático en el sentido correcto, procurando lograr una anastomosis artificial.

Como efecto adicional, el kinesiotape favorece la cicatrización, ayuda a eliminar o reducir adherencias y facilita la circulación linfática a ambos lados de la cicatriz. También se utiliza con éxito para ayudar en la reabsorción de las equimosis. 
Esta técnica sólo debe ser practicada por personal calificado.

Algunas contraindicaciones:




Existen otros tratamientos que pueden utilizarse para el linfedema, entre ellos algunos de carácter farmacológicos.

Si bien hay muchas alternativas, hasta el momento los únicos que han demostrado cierta efectividad son la Benzopironas, aunque en ciertos grupos de pacientes resultan hepatotóxicos, razón por la cual han sido retiradas del mercado en numerosos países.

Los diuréticos deben evitarse, ya que utilizados a largo plazo producen efectos secundarios que pueden empeorar el cuadro. 

La presoterapia sólo es útil si se usa como complemento del drenaje linfático. 

Las dietas son un complemento, pero solo cuando procuran evitar sobrepeso, evitando el consumo de grasas y el exceso de proteínas.



Referencias
1.-Pereira de Godoy JM, Pereira de Godoy HJ, Lopes Pinto R, Facio FN Jr, Guerreiro Godoy MF. Maintenance of the Results of Stage II Lower Limb Lymphedema 
Treatment after Normalization of Leg Size. Int J Vasc Med. 2017;2017:8515767. doi: 10.1155/2017/8515767. Epub 2017 Aug 1.
2.Pereira de Godoy HJ, Budtinger Filho R, Godoy MF, de Godoy JM.Evolution of Skin during Rehabilitation for Elephantiasis Using Intensive Treatment.
Case Rep Dermatol Med. 2016;2016:4305910. doi: 10.1155/2016/4305910. Epub 2016 Nov 24.


</doc>
<doc id="16910" url="https://es.wikipedia.org/wiki?curid=16910" title="Geografía de Libia">
Geografía de Libia

Libia es un país perteneciente al África mediterránea. Libia limita con el Mar Mediterráneo al norte, al oeste con Túnez y Argelia, al suroeste con Níger, al sur con Chad, al sureste con Sudán y al este con Egipto.

El país se caracteriza por sus grandes extensiones de desierto (desierto del Sáhara) que cubren la totalidad del país a excepción de una estrecha franja litoral que es donde se encuentran los principales núcleos de población del país (Trípoli, Bengasi...).

El Desierto de Libia, que se enmarca dentro del Sahara, ocupa todo el sur del país y se extiende hasta Egipto y Sudán. Es una de las zonas más inhóspitas del planeta.

Las dos zonas geográficas principales de Libia son la costa mediterránea y el Sahara. En el desierto, se encuentran varias mesetas pero ninguna cadena montañosa con excepción del macizo de Tibesti, cerca de la frontera con Chad, en el sur. 

En la Tripolitania, la región noroccidental de Libia, hay una llanura costera, Djeffara, que tiene unos 14.300 km. Se extiende desde Gabes, en Túnez, hasta unos 20 km al este de Trípoli. La zona costera es una estrecha franja de playas arenosas con pequeñas hondonadas que se llenan de agua con las lluvias y forman planicies saladas. Le sigue una zona esteparia hacia el interior que se eleva de 50 a 200 m, hasta unos 120 km de la costa, donde se encuentra el piedemonte del Yebel Nefusa, una zona mesetaria con escarpes que culmina a 968 m. En Djeffara caen entre 125 y 250 mm de lluvia, que aumenta hacia Trípoli, con 380 mm. Crecen palmeras datileras y hay rebaños de ovejas y cabras.

El Yebel Nefusa tiene más de 10.000 km, está poblado por bereberes infusen y se extiende desde la frontera con Túnez hasta la ciudad de Garian a lo largo de unos 190 km. Luego, se curva hacia el Mediterráneo unos 150 km más casi hasta la costa, en Al-Khums. Está coronado por una meseta de unos 25 km de anchura cubierta de maleza en el norte y de yermos de basalto y lava al sur. Las lluvias oscilan entre 50 y 400 mm. Se cultivan olivos, higueras, tabaco y esparto para cuerdas y calzado además de los rebaños.

Detrás, se extiende la meseta del Hamadah Al Hamra, una región desértica de 84.000 km que se extiende hasta Ghadamés por el sudoeste, cerca de la frontera con Argelia y Túnez. Es famosa por el meteorito de su nombre (HaH 280), de condrita carbonácea.

Al sur de la Tripolitania se encuentra la región de Fezzan, que se extiende hasta Chad, donde encuentra el macizo de Tibesti. Una gran parte de la región está cubierta por los ergs de Idehan Ubari o desierto de Ubari, de 50.000 km y el Idehan Murzuq o desierto de Murzuq, de 58.000 km, que forman parte del desierto de Libia. Al norte de Fezzan se encuentra el wadi Ashati, que da nombre al distrito de Wadi Al Shatii y se extiende a lo largo de unos 140 km. Fezzan era la tierra de los garamantes, que cruzaban las rutas comerciales del desierto en tiempos de Cartago y Roma.

Al sudoeste de Fezzan se encuentran las montañas Acacus, unos 100 km al sur de la localidad de Ghat. En medio de un paisaje de dunas con profundos wadis emergen parajes rocosos que forman imponentes arcos de piedra (Afzejare y Tin Khlega). Tadrart Acacus es conocido por sus pinturas rupestres. Toda esta región del sur forma parte del desierto de Libia.

La mitad este de Libia es conocida como la Cirenaica. En su parte occidental, en el centro de Libia, al sur del golfo de Sirte, se encuentra el Haruj o Hulayq al Kabir, un macizo volcánico que contiene unos 150 volcanes, incluidos numerosos conos basálticos de escoria y unos 30 pequeños campos volcánicos formados por lava solidificada, con una altura máxima de 1.200 m. 

Al este del golfo de Sirte, ya en la Cirenaica, se encuentra el yebel Ajdar o Montaña Verde, a unos 16 km del mar, con un altitud de 900 m y una longitud de unos 300 km, hasta el golfo de Bomba. Es una de las pocas regiones de Libia con bosques, con una precipitación de 600 mm. El resto de la costa hacia el este está formado en su mayor parte por un escarpe de unos 300 m, con dos puertos en Bomba y Tobruk. Cuando más al este, hacia la histórica Marmarica, la tierra es más seca.

Al sur y al este del yebel Ajdar se encuentra el yebel el-Akabah, más bajo y separado por una depresión. El resto de la Cirenaica está formado por el desierto de Libia, que se extiende desde Egipto hasta montañas Acacus, y es una extensión de más de 1 millón de km, que tiene unos mil kilómetros de lado, formada por llanuras, dunas, sierras y algunas depresiones, pero ningún río. En la frontera con Egipto y Sudán se encuentra el yebel Uweinat, que se alza por encima de 1.900 m. Los oasis más importantes son Al Jaghbub y Jalo, en el este, y Kufra y Murzuk, en Fezzan. Los mares de arena forman un anillo alrededor del desierto. A los ya mencionados Idehan Ubari e Idehan Murzuq se unen el Gran Mar de Arena, de 72.000 km, que comparte con Egipto, el Mar de arena de Rebiana, de 65.000 km, y el Mar de arena de Calanshio, de 62.000 km.
Las tierras fértiles se encuentran en la costa y en las montañas que se encuentran inmediatamente detrás, tanto en la Tripolitania como en la Cirenaica, separadas por el golfo de Sirte, que tradicionalmente separa el Magreb del Máshrek.

La frontera entre Tripolitania y Túnez es objeto de cruces continuos por migrantes legales e ilegales. Ninguna frontera natural señala la frontera, y la composición étnica, lenguaje y cultura de los dos pueblos son casi idénticos. Lo mismo sucede entre la Cirenaica y Egipto. Por contraste, la frontera con Argelia, Níger y Chad es raramente cruzada debido al vacío de la inmensidad del desierto. En la frontera con Chad se encuentra la franja de Aouzou, que fue disputada durante muchos años por los dos países.

No hay ríos permanentes en Libia debido a la escasez de las lluvias. El clima dominante es el de desierto cálido con largas jornadas asfixiantes y noches algo más frescas en las zonas más altas. Menos del 2 por ciento del territorio recibe suficiente lluvia para cultivar la tierra. Solo en la Cirenaica, en el yebel Ajdar, caen entre 400 y 600 mm (Al Baida, 540 mm); el resto de la costa recibe entre 200 y 350 mm, en el golfo de Sirte caen entre 100 y 200 mm, y al este, cerca de Egipto, menos de 100 mm. Las lluvias costeras se deben al paso de depresiones por el mar Mediterráneo, asimismo, la brisa marina refresca la temperatura. En Trípoli, la ciudad más húmeda, caen 335 mm en 52 días, casi todo entre octubre y mayo; en verano, las temperaturas son soportables, entre 20 C y poco más de 30 C. En invierno, entre 9 y 19 C. En Bengasi caen 260 mm y en Misrata, cerca del golfo de Sirte, 180 mm. En Tobruk, al este, caen entre 94 y 110 mm.

En el interior, las temperaturas suben a 35-37 C en las zonas cercanas a la costa, y superan los 40 C en el sur. En las montañas Nefusa, a casi 1.000 m en Nalut, Zintan, Lefren y Garian, los inviernos pueden ser fríos. Libia sufre los efectos de un viento llamado "ghibli", seco y cálido, que hace caer la humedad y subir la temperatura súbitamente, entre abril y octubre, incluso en la costa, donde se pueden alcanzar los 30 C en invierno.

En el gran desierto del Sahara, caen menos de 50 mm. En verano, las temperaturas pueden superar los 50 C. Las lluvias son raras, y apenas hay población salvo en los oasis. En Sabha, en Fezzan, a 400 m de altura, las temperaturas oscilan entre junio y agosto entre 24 y 39 C, y en invierno, entre 6 y 20 C.

No hay ríos permanente es Libia. Hay numerosos wadis o ríos secos que provocan súbitas inundaciones cuando llueve, pero se secan rápidamente. Los más importantes son el Wadi Zamzam y el Wadi Bayy al-Kabīr. Ambos desembocan en la costa oriental del golfo de Sirte. Hay otros wadis importantes en la zona del campo petrolífero de Zaltan, 150 km al sur del puerto de Marsa Brega, en el golfo de Sirte, y en Fezzan.

En Libia hay numerosos acuíferos fósiles en Fezzan y el sudeste del país, además de pozos y manantiales. En tiempos de Gadaffi se construyó el llamado Gran Río Artificial, una red de tuberías que proveen agua al desierto del Sahara desde acuíferos fósiles, principalmente el sistema acuífero de piedra arenisca de Nubia. Consiste en más de 1.300 pozos, la mayoría de ellos de más de 500 metros de profundidad, y provee 6.500.000 m³ (6,5 hm³) de agua dulce por día a las ciudades de Trípoli, Bengasi, Sirte y otras, que antes eran dependientes de la desalinización.

El reservorio de Nubia tiene entre 10.000 y 1 millón de años de antigüedad y está formado por agua que se filtró a través de las arenas del desierto. Fue descubierto a finales de la década de 1950, mientras se buscaba petróleo. Posee entre 100.000 y 150.000 km de aguas subterránea. El proyecto del Gran Río Artificial posee varias fases. La más occidental, cerca de Túnez, extrae aguas de pozos cerca de Ghadamés y la lleva a las ciudades costeras de Zauiya y Zuara, al oeste de Trípoli. La más oriental, cerca de Egipto, lleva el agua desde la depresión de Al Jaghbub hasta Tobruk, al este del país. El sistema está preparado para llevar 6,5 millones de m diarios de agua a lo largo de 4.000 km de tuberías, montadas mediante piezas de hormigón de 7 m de largo y 4 m de altura, y desde lugares tan lejanos como Jabal Hasouna, en Fezzan, y Kufra y Tazerbo, en el sudeste del país. Un proyecto enlazaría ciudades de la costa. En el sistema se han construido distribuidores de agua abiertos que forman lagos de más de 1 km de diámetro, como en Ajdabiya.

En las llanuras costeras occidentales, hay varios lagos salados o sebkhas, que se forman por evaporación de las aguas que se forman en las hondonadas en época de lluvias.

Desde 2012, hay extensas zonas de Libia temporalmente cerradas por culpa de la situación bélica que padece el país, concretamente las fronteras con Sudán, Chad, Níger y Argelia. Han sido declaradas zonas militares Ghadamés, Ghat, Obari, Al-Shati, Sebha, Murzuq y Kufra, de manera que en este contexto precario, los parques nacionales y las zonas naturales de interés carecen de protección. En 2014, dos parlamentos rivales se disputaban el país; a ellos se añadió el Estado islámico, que ocupó un área en torno al golfo de Sirte, y en 2016 se añadió un cuarto contendiente, el Government of National Accord (GNA).

En Libia había, en febrero de 2018, 3.437 km de tierras protegidas, el 0.21% del territorio, y 2.278 km de superficie marina, el 0,64% de los 357.895 km de área marina total que pertenecen al país. En esa extensión, hay 4 parques nacionales (antes había 7), 4 reservas de la naturaleza (había 5) y 14 áreas protegidas (había 24), además de 2 sitios Ramsar, Ain Elshakika (33 ha) y Ain Elzarga (50 ha). también había 5 lugares arqueológicos considerados patrimonio de la humanidad: Cirene, Leptis Magna, Sabratha, Ghadamés y Tadrart Acacus. 










</doc>
<doc id="16912" url="https://es.wikipedia.org/wiki?curid=16912" title="Lexicografía">
Lexicografía

La lexicografía es la disciplina aplicada al lenguaje que se ocupa de la elaboración y el análisis crítico de diccionarios. Para ello, no solo se sustenta en los principios teóricos y metodológicos de la lingüística, sino que cuenta con los suyos propios. Al igual que la lexicología, posee una dimensión teórica y una práctica.

El término proviene de la técnica realizada por el lexicógrafo, que a su vez proviene del griego "leksikográphos", compuesto por "leksikós" (λεξικόν) que significa colección de palabras o vocablos de una lengua, y "gráphein" (γραφειν), escribir. Por lo que corresponde a la técnica de coleccionar palabras que deben entrar en un léxico, o simplemente quien escribe diccionarios.

La "lexicografía" es una muy antigua disciplina que busca una sistemática colección y explicación de "todas" las palabras (o más estrictamente, "unidades léxicas") de un lenguaje, pero generalmente en amplitud más que en profundidad, cosa que hace su disciplina hermana la lexicología. Entre estas "unidades léxicas" no solo se incluyen palabras individuales sino que también modismos, palabras compuestas e incluso morfemas dependientes.

La disciplina lexicográfica no se circunscribe exclusivamente a "compilar diccionarios" sino que también abarca todo un conjunto de análisis de índole teórica en lo que se conoce normalmente como lexicografía teórica o metalexicografía. Esta disciplina teórica repasa tanto los orígenes de la elaboración de diccionarios, como aspectos relacionados con su estructura formal, la tipología, los métodos de compilación, o los vínculos existentes entre ésta y otras disciplinas ya sean lingüísticas o no.

Un diccionario es, según el de la Real Academia de la Lengua Española, "un repertorio en forma de libro o en soporte electrónico en el que se recogen, según un orden determinado, las palabras o expresiones de una o más lenguas, o de una materia concreta, acompañadas de su definición, equivalencia o explicación".

Según José Martínez de Sousa, pueden clasificarse según los criterios de soporte (en formato papel o en formato digital), de número de lenguas (monolingües —a cada entrada va asociada una definición— o plurilingües, y dentro de estos bilingües (por ejemplo, un diccionario español-inglés) o multilingües); de ordenación de entradas (orden alfabético u ordenación semasiológica o bien orden sistemático, según temas) y de criterio de la presentación o campo (terminológicos -con la terminología de un determinado campo-, diccionarios enciclopédicos -con información amplia de cada entrada-, diccionarios visuales -con fotografías o ilustraciones que acompañan las entradas- y glosarios).

Para Günther Haensch, los diccionarios de carácter general se dividen en cuatro tipos:
Otro criterio de distinción de diccionarios redunda en su carácter normativo (de palabras correctas) o descriptivo (sin valoraciones: incluye barbarismos, extranjerismos o vulgarismos), y en su carácter sincrónico (que refleja la lengua de un tiempo concreto) o diacrónico (que refleja una evolución del significado y forma de las palabras a lo largo del tiempo); sincrónicos son también los diccionarios de arcaísmos, los de neologismos o los diccionarios de voces de uso actual; diacrónicos los diccionarios etimológicos o los históricos.

Las partes de un diccionario se denominan entradas. Cada entrada se divide mínimamente de la siguiente manera:




</doc>
<doc id="16913" url="https://es.wikipedia.org/wiki?curid=16913" title="Lago Chad">
Lago Chad

El lago Chad es un lago endorreico poco profundo que se encuentra, situado en la frontera entre Chad, Níger, Nigeria y Camerún, en África. Su capacidad ha ido menguando con el paso del tiempo y debido, sin duda, a la desertización provocada por la cercanía del desierto del Sahara y por la captación de aguas para irrigación de cultivos.

Su mayor tributario es el río Chari, el cual le suministra el 90% de sus aguas. Existen numerosas islas y bancos de limo, y en sus orillas abundan las zonas pantanosas. A causa de su escasa profundidad, de solo siete metros en su punto más profundo, el área es particularmente sensible a cambios en la profundidad media, y muestra fluctuaciones de tamaño según la época del año. Aparentemente las aguas no tienen salida, aunque se filtran en las depresiones de Soro y Bodele.

Existen evidencias científicas sobre la existencia de un lago mucho mayor que cubría un área de algo menos de 400.000 km² en dos pulsos húmedos del Holoceno. Sus cambios radicales de extensión tienen que ver con su poca profundidad. Cuando fue descubierto por los europeos en 1823 era uno de los mayores lagos del mundo, pero se ha reducido considerablemente desde entonces. La demanda creciente de agua del lago, y el cambio clímático, específicamente la reducción de las precipitaciones, han acelerado su degeneración en los últimos cuarenta años.

En los años 1960 el área cubierta por sus aguas era de 26.000 km², una extensión similar a la superficie de la isla de Sicilia, lo cual le convertía en el cuarto mayor lago de África. En 2000 su extensión se había reducido a menos de 1.500 km², y en 2006 era de tan solo 900 km²; las causas son la reducción de las precipitaciones junto al aumento de la extracción de agua para regadíos y otros usos, tanto del mismo lago como de los ríos tributarios. Los pronósticos indican que el lago continuará reduciéndose e incluso acabará secándose a lo largo del siglo XXI.

Las diversas observaciones realizadas sobre los niveles y superficies permiten definir las diferentes cuencas del lago y su topografía.
Tres cuencas pueden ser individualizadas:
Estas cuencas están separadas por umbrales:

En el lago “Chad normal”, en el sentido definido por Tilho, el lago tiene un solo cuerpo de agua, a una altitud de más de 280 msnm, con dos grandes cuencas, al sur y al norte, separadas por una garganta. Un archipiélago, que consiste en un "erg" (el “erg” es una región arenosa de un desierto, y se contrapone a “hamada”, el desierto pedregoso) fósil, se hunde gradualmente en el lago desde el noreste. El archipiélago se extiende hacia el interior del lago por islas de vegetación, llamadas "bancos de islas", que corresponden a dunas sumergidas y colonizadas por fanerógamas acuáticas. Chad Normal se caracteriza por la extensión de las áreas de aguas abiertas, el espacio navegable entre las islas de los archipiélagos y una franja limitada de vegetación a lo largo de las orillas.

Como resultado de las variaciones climáticas, la evolución de Chad Normal se intercala con fases de bajo nivel. Desde el comienzo del siglo han ocurrido tres fases de “Pequeño Chad”, la primera (1904-1917) ha sido descrita en detalle por Tilho, ya citado. El segundo, alrededor de 1940, está documentado solo por la tradición oral.

El último paso al estado de “Pequeño Chad” tuvo lugar en 1973 y, desde esa fecha, el lago opera bajo un nuevo régimen.
Los paisajes actuales de “pequeño Chad” son consecuencia de la topografía y su historia hidrológica reciente: Las áreas de aguas abiertas de la cuenca sur corresponden a las áreas más profundas de esta cuenca que no estuvieron expuestas al comienzo del período de sequía.


La evolución moderna del lago, en el curso del último milenio se ha podido reconstruir gracias a los datos de la palinología y la geología, pero también con base en acontecimientos históricos, asociando los diversos estados del lago con las fechas de ocurrencias de los eventos históricos.
Algunos datos puntuales:

El lago estuvo prácticamente seco en 1908 y nuevamente en 1984 con solo 1,5 m de profundidad. 

La posición geográfica de este lago en África constituye un motivo de inestabilidad: estando casi en el límite del desplazamiento de la zona intertropical de convergencia, es particularmente sensible a las variaciones interanuales de este. El "Lago Chad Normal", como existía a finales de la década de 1960, tenía una superficie de 19 000 km² para una costa de 281,5 msnm y consistía en un solo cuerpo de agua.




</doc>
<doc id="16914" url="https://es.wikipedia.org/wiki?curid=16914" title="Tradiciones de El Salvador">
Tradiciones de El Salvador

Las tradiciones más comunes en todo el territorio salvadoreño son las fiestas patronales de los diferentes pueblos, además hay bailes, grupos de música regional, comida típica, instrumentos y música propia del cafe salvadoreño. <br>
Una de las fiestas patronales es representación bailada y declamada conocida como "Pero solo a las personas que le considera importantes en toda las cosas de la Historiantes" en la cual se forman dos grupos de los monos. Un grupo ataviado muy elegante y con máscaras de tez blanca y barbas rubias. El otro grupo ataviado con trajes exóticos y caretas negras. Estos festejos datan de la época de la conquista de América y representan la lucha entre Moros y Cristianos en la península ibérica.

En la actualidad algunas de las tradiciones de El Salvador, es comer pan (una masa especial ), asistir a las fiestas patronales que se celebran una vez por año en cada ciudad o municipio.

En la región oriental e insular aún se encuentra una variedad de costumbres antiguas tales como el baile del despacito, Los Emplumados, La Danza de La Santa y muchas de Los Pueblos Indígenas]] en la ONU, en las comunidades del Valle Encantado de Managuara se proclamó la re institución de la dinastía Maya Lenca, de esta manera se instauró a la anciana Francisca B.G.R como ana Real del Jaguar y cabeza de la etnia lenca.
Ella ha auspiciado la recopilación de la lengua Lenca de Managuara, la colección de leyendas pertenecientes a la tradición dinástica real "Los Cantares del Pinol" y muchas otras más.
En 2018, el heredero cultural establece La Embajada Real Maya Lenca en Australia.
Los lencas tienen costumbres muy peculiares que datan de muchos miles de años y por habitar la región más descuidada del país, su cultura ha logrado sobrevivir en muchas comunidades.



</doc>
<doc id="16915" url="https://es.wikipedia.org/wiki?curid=16915" title="Lapiaz">
Lapiaz

Un lapiaz, lenar o pavimento de caliza es una zona o superficie pétrea irregular de rocas carbonáticas o evaporíticas modeladas por la disolución química y el hielo con múltiples surcos, orificios y aristas agudas. La superficie de cada surco u oquedad suele ser de dimensiones pequeñas o medianas, separado por tabiques o paredes de roca en algunos casos agudos. Sus dimensiones son decimétricas, aunque en profundidad a veces, pueden superar la decena de metros. En realidad el lapiaz varía entre unos pocos milímetros, microlapiaz, a varios metros. Los lapiaces aparecen en afloramientos de calizas o yesos afectados por procesos kársticos y son, por lo tanto, formas kársticas elementales.

Su génesis se produce por la disolución superficial de la caliza afectada por agua de escorrentía o almacenada superficialmente en puntos donde la microtopografía permite una mejor retención o canalización del agua o la humedad.
La disolución superficial de las calizas se acelera durante las lluvias debido a la acidez por el anhídrido carbónico del aire, que por hidratación se convierte en ácido carbónico, y la acción de ácidos húmicos. La caliza es un carbonato cálcico que no es soluble en el agua, pero reacciona con el ácido carbónico convirtiéndose en bicarbonato cálcico, que sí es soluble en el agua, por lo que el suelo calcáreo irá profundizándose en los lugares donde se concentran las pequeñas corrientes de agua.

Cuando se encuentran en pendiente las aristas de los lapiaces pueden presentar cierta continuidad. En ese caso se habla de lapiaces lineales, que pueden adoptar formas sinuosas que asemejan cursos fluviales, incluso se habla de meandros de lapiaz. Cuando las fisuras configuran una trama cuadrangular, normalmente un sistema de diaclasas que facilita la penetración de la disolución en la roca, definen una mesa de lapiaz, formada por una trama de losas delimitadas por las fracturas.
Si las cavidades son más o menos circulares se llaman lapiaces alveolares.

En las rocas dolomíticas el lapiaz presenta muros, puentes naturales, pitones y pasillos repartidos de manera desordenada por la región. Se habla entonces de relieve cárstico ruiniforme.

Estas formas kársticas pueden aparecer en asociación ocupando una superficie grande y accidentada. En algunos casos pueden alcanzar importantes dimensiones métricas y estar muy separados entre sí constituyendo macrolapiaces que definen paisajes conocidos popularmente como "ciudades encantadas", como es el caso del Mar de Piedra, en la Ciudad Encantada de Cuenca.


</doc>
<doc id="16918" url="https://es.wikipedia.org/wiki?curid=16918" title="Geografía de Birmania">
Geografía de Birmania

Birmania está ubicado entre la meseta del Tíbet y la península malaya. Se encuentra rodeado en el este, norte y oeste por montañas que limitan un valle central, surcado por los ríos Irrawady, Sittang y Salween. Allí se concentran los campos agrícolas –productores de arroz– y la mayor parte de la población. 

Al igual que la mayoría de los países del Sudeste de Asia, Birmania tiene una geomorfología y una red hidrográfica orientada de norte a sur, que han condicionado la ocupación humana.

La mayoría del relieves se orienta de norte a sur, aislando el país de la India al oeste y de Tailandia el este. El norte, montañoso, anuncia al Himalaya. Se encuentra allí el
Hkakabo Razi, que culmina a 5581 metros, por lo que es el punto más alto del país, y también de todo el Sureste de Asia. Al este, las montañas Shan forman una continuidad con las de Tailandia y Laos (también poblada por los thaïs).

La única llanuras se encuentran en el centro y el sur del país, a lo largo de los ríos y estuarios: el Irawadi, la cuna del pueblo birmano propiamente, y a lo largo de las costas, al sudeste, territorio histórico de los Mons.

El país desciende de norte a sur de tal manera que suele ser dividido en Alta Birmania ("Upper Burma") y Baja Birmania ("Lower Burma"). La llanura central, por la que discurren los ríos Chindwin e Irawadi, está atravesada por pequeñas sierras onduladas, Zeebyu Taungdan, Min-wun Taungdan, Hman-kin Taungdan y Gangaw Taungdan. Cuando ambos ríos se unen y la llanura central está dominada por el Irawadi en una zona absolutamente llana, otra sierra, Pegu Yoma, Bago Yoma o montes Bago, separa la gran cuenca de este río por el este de la más pequeña del río Sittang, que desemboca en el mar de Andamán.

Las montañas del norte de Birmania están formadas por la vertiente meridional de las montañas Hengduan, que se encuentran principalmente en China y forman el reborde sudoriental de la meseta tibetana. Al oeste de Birmania se encuentra, de norte a sur, la cordillera Arakan, de unos 960 km de longitud, que culmina en el monte Nat Ma Taung o monte Victoria, de 3.053 m, y separa el país de la India. Esta a su vez esta formada, en su parte norte, por los montes Patkai, las montañas Naga que culminan en el monte Saramati, a 3.826 m, ya en la India, y los montes Chin. En el extremo sur, la cordillera Arakan continúa y se sumerge en el golfo de Bengala, para emerger otra vez en las islas Nicobar, que pertenecen a la India. En esta región predomina una ecorregión denominada bosques de montaña de los montes Chin-Arakan Yoma.

Al este de Birmania se encuentran las montañas Shan, que también se conocen como meseta de Shan ("Shan Plateau") o colinas de Shan ("Shan Hills"), un sistema montañoso de 560 km de largo por 330 km de ancho que se eleva abruptamente 600 m por encima de la llanura aluvial, y que en su mitad oriental está formado por numerosas sierras que se elevan de 1.800 a 2.600 m. La altitud media de las Shan Hills es de 1.000 m y culminan en el Loi Leng, de 2.673 m. Son la fuente del tercer gran río de Birmania, el río Salween, que también desemboca en el mar de Andamán, en el golfo de Martaban. Las montañas Shan se prolongan más allá del Salween en las montañas Daen Lao, y después, en la sierra de Dawna, que culmina en el Mela Taung, a 2.080 m y separa el país de Tailandia y Laos. Esta sierra enlaza por el sur con los montes Tenasserim, que se prolongan hacia el sur por la península de Malaca, sirviendo de separación con Tailandia, y que culminan en el monte Tahan, de 2.187 m, ya en Malasia. Estos sistemas están formados por estrechos valles y anbruptas serranías paralelas.

El monte Hkakabo, punto más alto del país, en el norte, con sus casi seis mil metros forma un complejo entramado geológico donde se encuentran el límite nordoriental de la placa indoaustraliana y el borde meridional de la placa euroasiática, que han estado chocando los últimos 50 millones de años para formar el sistema montañoso de Birmania.

Tres quintas partes de Birmania, más de 400.000 km, están drenadas por el río Irawadi y sus afluentes. El Irawadi es navegable a lo largo de 1.600 km. En el ápice del delta (entre 35.000 y 50.000 km, una parte de las cuales fue destruida por el ciclón Nargis en 2008) se abre en una vasta red de canales que terminan en múltiples bocas en el mar de Andamán. El río Chindwin drena la parte occidental; el río Pathein drena la parte inferior de la cordillera Arakan y el río Yangón drena los montes Bago. 

En las tierras altas lo suelos son rojos oscuros y marrones, ricos en hierro y muy fértiles; cuando están cubiertos de bosque absorben bien el agua de las lluvias monzónicas, pero cuando se desforesta, se erosionan rápidamente. En las tierras bajas, los suelos son limosos y arcillosos, pobres en materia orgánica, muy lavados por las lluvias, y necesitan abonarse. En las tierras centrales, los suelos son marrones, ricos en calcio y magnesio, pero cuando el contenido en arcilla es bajo, el calor hace que amarilleen y se salinicen.

El clima es tropical, con lluvias monzónicas, entre mayo y octubre. Buena parte de la vegetación es selvática. La deforestación destruyó dos terceras partes de la selva tropical.

El monzón tropical se da en las tierras bajas por debajo de 2000 m; nublado, lluvioso, caliente, veranos húmedos (monzón del suroeste, de junio a septiembre); menos nublado, lluvia escasa, temperaturas suaves, menor humedad durante el invierno (monzón del noreste de diciembre a abril). El clima varía en las tierras altas dependiendo de la elevación; clima templado subtropical en alrededor de 2500 m, templado en 3000 m, fresco, alpino a 3500 m y por encima de la zona alpina, frío, tundra dura y el clima ártico. Las elevaciones más altas están sujetas a fuertes nevadas y el mal tiempo.

Se pueden distinguir las siguientes zonas climáticas: 






El Irawadi es la arteria vital de Birmania. Nace en las altas tierras del Estado kachin y recorre aproximadamente 2000 km antes de desembocar en el mar de Andamán dividiéndose en numerosos brazos. El río es navegable hasta Bhamaw en el sur del Estado de Kachin.

El Chindwin nace de varios arroyos en el norte de la (provincia) de Sagaing y desemboca en el Irawadi entre el Mandalay y Pagan. Los buques se remonta a Leik Maw. Además, sólo pueden seguir las pequeñas embarcaciones. Durante la estación seca (febrero a mayo), los grandes buques no pueden navegar hasta Kale Wa.

El Sittang nace de varios arroyos en las montañas de Pegu y las colinas del estado de Kayin y al sur del estado Shan y desemboca en el golfo de Martaban, en la parte norte del mar de Andaman. El río no es navegable para barcos de pasajeros debido a sus fuertes corrientes y a los rápidos. Algunas partes del río y de sus afluentes se utilizan para el transporte de troncos.

El Salween tiene su origen en China y atraviesa los Estados Shan, el estado Kayah, Kayin, Mon y desemboca en el Golfo de Martaban cerca de Mawlamyaing. Forma la frontera entre Tailandia y el estado de Kayin. Los grandes barcos de pasajeros puedan remontar hasta Shwe Gun en época de aguas altas (de junio a noviembre). La parte norte está formada por rápidos y fuertes corrientes en las montañas.

El Mekong forma la frontera entre Laos y el estado de Shan.

La IUCN considera que en Myanmar (también Birmania) hay 59 áreas protegidas que cubren una extensión de 42,878 km, el 6,37% del territorio nacional, y 11.964 km de áreas marinas, el 2,33% de los 514.147 km que pertenecen al país. De estas, 6 son parques nacionales, 4 son ASEAN Heritage Parks (Parques patrimonio de la ASEAN, Association for Southeast Asian Nations), 2 son reservas naturales, 4 son áreas protegidas, 28 son santuarios de la naturaleza, 3 son santuarios de aves, 1 es un parque natural, 1 es un parque de montaña, 2 son santuarios de la naturaleza y parques patrimonio de la ASEAN y 1 es un parque de elefantes ("elephant range"). También hay 2 reservas de la biosfera de la Unesco y 5 sitios Ramsar.


En Birmania hay 13 grupos étnicos reconocidos por el gobierno, agrupados en ocho grupos principales. La población bamar es más numerosa porque el gobierno agrupa en esa etnia a todos los grupos mixtos. Por orden: bamar, chin, kachin, kayin, karenni, mon, rakhine y shan. Los grupos mayores se agrupan en primer lugar por la región, más que por la lengua o la afiliación étnica, por ejemplo, el grupo étnico nacional shan incluye 33 etnias que hablan lenguas que corresponden al menos a cuatro familias lingüísticas diferentes. Existen muchos grupos étnicos no reconocidos, el mayor de los cuales es la población china de Birmania y los panthays, los musulmanes de origen chino que, juntos, forman el 3% de la población de Birmania, los indios birmanos, que forman el 2% de la población, los anglobirmanos y los gurkha birmanos. Los rohinyás formarían el 1,8% de la población antes de su expulsión.


</doc>
<doc id="16938" url="https://es.wikipedia.org/wiki?curid=16938" title="Capacidad">
Capacidad

Capacidad se refiere a los recursos o actitudes que tiene un individuo, entidad o institución, para desempeñar una determinada tarea o cometido.

En contextos más concretos, la capacidad se puede referir a los siguientes conceptos:



</doc>
<doc id="16939" url="https://es.wikipedia.org/wiki?curid=16939" title="AS/400">
AS/400

El sistema AS/400 es un equipo de IBM de gama media y alta, para todo tipo de empresas y grandes departamentos.

Se trata de un sistema multiusuario, con una interfaz controlada mediante menús y comandos CL (Control Language) intuitivos que utiliza terminales y un sistema operativo basado en objetos y bibliotecas, denominado OS/400. Un punto fuerte del OS/400 es su integración con la base de datos DB2/400, siendo los objetos del sistema miembros de la citada base de datos. Ésta también da soporte para los datos de las aplicaciones, dando como resultado un sistema integrado potente y estable. Actualmente, con la denominación IBM i, anteriormente conocida como "System i" e "iSeries", soporta otros sistemas operativos tales como GNU/Linux, AIX o incluso Windows en una placa Intel integrada, soportando también de forma nativa múltiples aplicaciones antes reservadas a Windows.

La máquina se basó originalmente en una CPU CISC de IBM, pero en 1996 se migró a una familia de CPU RISC basada en microprocesadores PowerPC de 64 bits. Hasta marzo de 2010, los últimos modelos, que bajo la denominación unificaron las plataformas y de IBM, se basan en el procesador .

La capacidad de supervivencia de la máquina es debida a su capa de MI o Machine Interface, que aísla el hardware y permite, mediante el uso de API, que el sistema operativo y los programas de aplicaciones se aprovechen de los avances en hardware sin tener que recompilarlo y de su adaptación al entorno empresarial crítico, en donde la estabilidad y fiabilidad del sistema son fundamentales.

Puede trabajar con los lenguajes de programación RPG, PHP, C, Java, COBOL, SQL, BASIC y REXX. También se dispone de varias herramientas CASE, como ADP/400, , AS/SET, Lansa, Delphi/400 for Windows, Delphi/400 for PHP, CA Plex (inicialmente llamado Obsydian), o Genexus.

Se diseñó como sustituto del y partiendo de su arquitectura, cuyos orígenes se remontan a los años 1978 y 1979.

El 2 de diciembre de 2006, a partir de una iniciativa surgida en el foro Help400 se creó un wiki de iSeries externo a Wikipedia, aunque desarrollado en el mismo entorno, en el cual se pretendía que la comunidad de profesionales de iSeries de habla hispana pudiera participar para recopilar la información y conceptos más elementales de esta plataforma. Sin embargo, el proyecto finalizó a causa de la acción de los crackers.



</doc>
<doc id="16942" url="https://es.wikipedia.org/wiki?curid=16942" title="Cambridge">
Cambridge

Cambridge es un distrito no metropolitano del Reino Unido, una ciudad universitaria inglesa muy antigua y la capital del condado de Cambridgeshire, a orillas del río Cam. 

Se encuentra aproximadamente a ochenta kilómetros de Londres y la rodean varias villas y pueblos. Su fama la debe a la Universidad de Cambridge, que incluye a los Laboratorios Cavendish (denominados así en honor a Henry Cavendish), el hospital Addenbrooke, el coro de la capilla del King's College y la Biblioteca de la Universidad. Estos dos últimos edificios sobresalen respecto del resto de la ciudad. En la ciudad también se encuentra un campus de la Universidad Anglia Ruskin.

De acuerdo con el censo de 2001, la ciudad cuenta con 108 863 habitantes (de ellos, 22 153 son estudiantes).

El nombre de la ciudad significa «puente del [río] Cam».

En abril de 2011, la ciudad le da su nombre al título de nobleza «duque de Cambridge» al príncipe Guillermo Arturo Felipe Luis tras su matrimonio con Catalina ("Kate") Middleton, debido a la tradición británica de que a los príncipes reales se les conceda un título nobiliario al contraer matrimonio.

Se sabe de la existencia de asentamientos humanos en el área desde la época del Imperio romano. La más antigua e inobjetable evidencia de ocupación del lugar, un conjunto de armas de caza, corresponde al final de la Edad del Bronce, alrededor del año 1000 a. de C. Hay aún más pruebas de que en la Edad del Hierro, una tribu alemana ("Belgics" en el texto en inglés) se asentó en Castle Hill en el siglo I A.C. Castle Hill hizo de Cambridge un punto militarmente estratégico, debido a que desde ese lugar se podía vigilar el río Cam. También era el punto de cruce de la Vía Devana, que conectaba Colchester, en Essex, con las barracas en Lincoln, Inglaterra, y hacia el norte. Probablemente de ahí viene la etimología de su nombre Cam-Bridge o Puente-Cam. Este asentamiento romano posiblemente se denominaba Durolipons.

El asentamiento siguió siendo un centro regional, incluso 350 años después de la ocupación romana, alrededor del año 400 a. C. Aún pueden verse en el lugar los muros de edificaciones y los caminos romanos.

En 1068, dos años después de la batalla de Hastings, Guillermo I de Inglaterra construyó un castillo en "Castle Hill". De esta etapa data también la conocida como iglesia redonda ("Round Church").

En 1209 se funda la Universidad de Cambridge. El "college" más antiguo que aún existe, Peterhouse, se fundó en 1284. Sin embargo, el más famoso, el "King's College", comenzó a ser construido en 1446 por el rey Enrique VI. Fue finalizado en 1515 durante el reinado del rey Enrique VIII. Durante la guerra civil inglesa de 1642-1645 la ciudad fue un importante centro controlado por los parlamentarios. 

En el siglo XIX el municipio empezó a crecer debido al gran aumento de la población que se produjo en todo el país, producto del aumento de la esperanza de vida y de la productividad agraria. En 1951 la población recibe el título de ciudad.

Según la Oficina Nacional de Estadística británica, Cambridge tiene una superficie de 40,7 km².
a una altitud entre 6 y 24 metros al nivel del mar.

Según el censo de 2001, Cambridge tenía 108 863 habitantes (49,89% varones, 50,11% mujeres) y una densidad de población de 2674,77 hab/km². El 14,72% eran menores de 16 años, el 78,4% tenían entre 16 y 74 y el 6,89% eran mayores de 74. La media de edad era de 36,03 años. 

La mayor parte (80,86%) eran originarios del Reino Unido. El resto de englobaban al 7,49% de la población, mientras que el 1,98% había nacido en África, el 5,66% en Asia, el 2,43% en América del Norte, el 0,41% en América del Sur, el 1,05% en Oceanía y el 0,11% en cualquier otro lugar. Según su grupo étnico, el 89,44% de los habitantes eran blancos, el 1,97% mestizos, el 3,75% asiáticos, el 1,34% negros, el 2,14% chinos y el 1,37% de cualquier otro. El cristianismo era profesado por el 57,65%, el budismo por el 1,05%, el hinduismo por el 1,19%, el judaísmo por el 0,78%, el islam por el 2,44%, el sijismo por el 0,19% y cualquier otra religión por el 0,49%. El 26,61% no eran religiosos y el 9,61% no marcaron ninguna opción en el censo.

El 56,48% de los habitantes estaban solteros, el 30,97% casados, el 1,53% separados, el 5,91% divorciados y el 5,11% viudos. Había 42 658 hogares con residentes, de los cuales el 35,79% estaban habitados por una sola persona, el 8,2% por padres solteros con o sin hijos dependientes, el 47,74% por parejas (37,48% casadas, 10,26% sin casar) con o sin hijos dependientes, y el 8,26% por múltiples personas. Además, había 1135 hogares sin ocupar y 231 eran alojamientos vacacionales o segundas residencias.




</doc>
<doc id="16945" url="https://es.wikipedia.org/wiki?curid=16945" title="Oxford">
Oxford

Oxford es una ciudad universitaria británica ubicada en el condado de Oxfordshire, en Inglaterra, y es la sede de la Universidad de Oxford, la universidad más antigua en el mundo anglófono. Corresponde a la ubicación de la Torre Carfax, a la que se considera el centro de la ciudad. 

Se la conoce como «la ciudad de las agujas de ensueño», expresión acuñada por Matthew Arnold para describir la armonía en la arquitectura de los edificios universitarios. Siempre ha sido un asunto de mucho interés la relación ocasionalmente tensa entre "el pueblo y la academia", que en 1355 derivó en una revuelta con varios estudiantes universitarios muertos. A diferencia de su gran rival, Cambridge, Oxford es una ciudad industrial, asociada principalmente con la industria automotriz en el suburbio de Cowley.

Oxford se estableció por primera vez en los tiempos sajones y fue conocida inicialmente como "Oxenaforda", que significa "Ford of the Oxen" ("vado de los bueyes") (según la Sociedad de las nomenclaturas de lugares de Inglaterra, que se basan en una referencia en la obra de Florence de Worcester, "Chronicon ex chronicis"); los vados eran más comunes que los puentes en ese momento. Comenzó con el establecimiento de un cruce en el río para los bueyes alrededor del 900 d.C. En el siglo X, Oxford se convirtió en una importante frontera militar entre los reinos de Mercia y Wessex y en varias ocasiones fue atacada por los danos.

Oxford fue fuertemente dañada durante la invasión normanda de 1066. Después de la conquista, la ciudad fue asignada a un gobernador, Robert D'Oyly, quien ordenó la construcción del Castillo de Oxford para reafirmar la autoridad normanda sobre la zona. Se cree que el castillo nunca ha sido utilizado con fines militares y sus restos han sobrevivido hasta nuestros días. D'Oyly estableció una comunidad monástica en el castillo consistente en una capilla y cuartos para los monjes ("St George in the Castle"). La comunidad nunca creció mucho, pero ganó su lugar en la historia como uno de los lugares más antiguos de educación formal de Gran Bretaña. Fue allí donde en 1139 Godofredo de Monmouth escribió su "Historia Regum Britanniae", una recopilación de materia de Bretaña.

Oxford está situada a unos 80 kilómetros (50 millas) al noroeste de Londres; las ciudades están unidas por la autopista M40, que también enlaza al norte con Birmingham.

Mediante tren se puede ir a Londres, (Paddington o Marylebone), Bournemouth, Worcester (a través de la Cotswold Line), y Bicester. La ciudad también tiene servicios regulares de tren hacia el norte a Birmingham, Coventry, Mánchester, Escocia etc. El servicio ferroviario que conectaba Oxford y Cambridge, conocido como la Varsity Line (Línea universitaria), dejó de funcionar en 1968.

El Canal de Oxford conecta con el río Támesis en Oxford.

El aeropuerto de Oxford en Kidlington ofrece servicios aéreos de negocios y generales

Hay dos universidades en la ciudad de Oxford: la University of Oxford y la Brookes University. 

Además, la escuela de negocios francesa EM Normandie posee un campus en el centro de Oxford desde 2014. Comparte sus edificios con el City of Oxford College (Activate Learning). Recibe cada año aproximadamente 200 estudiantes de Francia o estudiantes de intercambio de las universidades asociadas a EM Normandie.

Algunos de los autores famosos de Oxford son:


Oxford ha sido usada por muchos escritores como escenario de sus novelas. Algunas de ellas son:


La ciudad de Oxford ha tenido, a lo largo de su historia, diversos hermanamientos con ciudades de varios continentes:


</doc>
<doc id="16946" url="https://es.wikipedia.org/wiki?curid=16946" title="Continental Airlines">
Continental Airlines

Continental Airlines, Inc. (IATA: CO, ICAO: COA, indicativo: CONTINENTAL) () fue una compañía aérea certificada de Estados Unidos. Con sede en el Centro de Chicago, Illinois, fue la cuarta aerolínea más grande de los Estados Unidos en términos de ingreso pasajero/millas. Desde 1998, el eslogan de Continental ha sido ""Work Hard, Fly Right"." El 3 de mayo de 2010 se anunció que Continental Airlines y United Airlines se fusionarían para dar lugar a la compañía aérea más grande del mundo.
El 30 de noviembre de 2011 la Administración Federal de Aviación (FAA) anunció que a partir de ese día, Continental y United Airlines operaría como una sola (United).

Continental efectuaba vuelos a destinos en los Estados Unidos, Canadá, América Latina, Europa, y las regiones del Asia-Pacífico. Tenía más de 6.000 salidas diarias, atendiendo unos 151 destinos nacionales y 190 internacionales y tenía 85.200 empleados (a marzo de 2007). Las principales operaciones parten de sus aeropuertos principales del Aeropuerto Internacional Libertad de Newark (en Newark (Nueva Jersey), el Aeropuerto Intercontinental George Bush (en Houston, Texas), y el Aeropuerto Internacional de Cleveland-Hopkins (en Cleveland, Ohio). Continental Micronesia, una filial propiedad total de la compañía, efectuaba rutas en la Micronesia desde su base de vuelos del Aeropuerto Internacional Antonio B. Won Pat en Guam y conectaba la región de la Micronesia con destinos en el Este de Asia, Sureste de Asia, Honolulú y Cairns, Australia.

Continental Airlines era propietaria minoritaria de ExpressJet Airlines, que opera bajo la marca de Continental Express pero tenía una dirección totalmente independiente de Continental y se trata de una compañía pública. Cape Air, Colgan Air, CommutAir, y Gulfstream International Airlines alimentaban los vuelos de Continental operando como Continental Connection, así como Chautauqua Airlines lo hacía bajo el nombre de Continental Express, aunque Continental no tiene participación alguna en estas compañías.

Desde septiembre de 2005, Continental fue miembro de la alianza SkyTeam, donde participaba junto a Northwest Airlines, Delta Air Lines, Air France, Aeroméxico, Alitalia, KLM, etc. Además de contar con acuerdos de código compartido con las aerolíneas pertenecientes a la alianza SkyTeam, la aerolínea también tenía acuerdos de código compartido con los servicios ferroviarios de Amtrak con algunas ciudades del noreste de los Estados Unidos, con US Helicopter que volaba del Aeropuerto Internacional Libertad de Newark a Manhattan, y con los ferrocarriles galos a destinos de Francia. En enero de 2009, Continental anunció que abandonaría la alianza SkyTeam el 24 de octubre de 2009 y entraría en Star Alliance el 27 de octubre de 2009.

Continental Airlines comenzó a operar en 1934 como Varney Speed Lines (nombre procedente de uno de sus primeros propietario, Walter T. Varney, quien fue el fundador también de United Airlines) efectuando vuelos desde El Paso y que se extendían a Albuquerque, Santa Fe, y de Las Vegas, NM a Pueblo, CO. La aerolínea comenzó sus operaciones con el Lockheed Vega, un avión de un solo motor que podía transportar hasta cuatro pasajeros. La aerolínea operó más tarde con otros aviones de Lockheed, incluyendo el Lockheed L-9 Orion, el Lockheed Electra Junior, y el Lockheed Lodestar.

Después de la cancelación de todos los acuerdos de correo aéreo por la administración Roosevelt en 1934, Robert Six vio una oportunidad de compra en la división suroeste de Varney Speed Lines, que necesitaba dinero para atender la recién otorgada ruta de Pueblo a El Paso. Six fue introducido en la compañía por Louis Mueller (quien estuvo de Consejero delegado de Continental hasta el 28 de febrero de 1966). Mueller ayudó a consolidar la división suroeste de Varney en 1934 junto con Walter T. Varney. Como resultado, Six entró en la aerolínea aportando 90.000 dólares y convirtiéndose en director general el 5 de julio de 1936. Varney fue premiada con un contrato del 17 del correo aéreo entre Pueblo y El Paso; así como efectuaba vuelos de pasaje. La compañía fue bautizada como Continental el 8 de julio de 1937. Six reubicó la base de la aerolínea en el aeropuerto de Denver Union (más tarde Stapleton) en Denver en octubre de 1937.

Robert F. Six fue uno de los más ingeniosos innovadores, pioneros, y visionarios (incluyendo a Juan Trippe, a William A. Patterson, a Jack Frye, C.R. Smith, y a Eddie Rickenbacker) que establecieron e hicieron crecer, a la industria aérea de los Estados Unidos. Aunque en su vida, Six ha tenido reputación de ejecutivo combativo y arriesgado imagen con la que ha permanecido más de 40 años.

Durante la Segunda Guerra Mundial las instalaciones de mantenimiento de Continental en Denver se convirtieron en un centro de conversión donde las aerolíneas convertían sus aviones en B-17, B-29 y P-51 para la fuerza aérea de los Estados Unidos. Los beneficios del transporte militar y de la conversión de aviones permitió a Continental contemplar la ampliación y adquisición de nuevos tipos de aviones que estuvieron disponibles después de la guerra.

Entre estos tipos estaban el DC-3, el Convair 240 y el Convair 340. Algunos de los DC-3 fueron adquiridos de los excedentes de aviones militares que aparecieron después de la Segunda Guerra Mundial. Los Convairs fueron los primeros aviones operados por Continental que estaban presurizados (ver foto).

La primera red de rutas de la aerolínea estaba limitada a la ruta de El Paso a Denver, así como las rutas que habían sido añadidas durante la Segunda Guerra Mundial de Denver y Albuquerque a destinos en Kansas, Oklahoma, y Texas. En 1946 Continental amplió sus rutas desde Denver a Kansas City y a Tulsa/Oklahoma City, y desde El Paso y Albuquerque a San Antonio. Cada una de estas rutas incluía paradas intermedias en alguna de las 22 ciudades pequeñas que también tenía como destinos. En 1953 Continental registró su primera gran expansión tras fusionarse con Pioneer Airlines, obteniendo permiso para volar a 16 ciudades adicionales en Texas y Nuevo Mexico. Estos destinos de Pioneer se integraron perfectamente con las rutas de Continental posteriores a la Segunda Guerra Mundial, y generaron un impulso en la Administración de aeronáutica civil (CAB), regulador de la industria, para después incentivar desde Denver a los principales destinos en Texas, Nuevo México, Kansas y Oklahoma. Sin embargo, Continental era, como muchas de las compañías de los Estados Unidos del momento, esencialmente una aerolínea con vuelos regionales limitados. Bob Six estaba enormemente insatisfecho con esta situación. Él presionó fuertemente a la CAB para obtener rutas de mayor distancia a ciudades más grandes, como parte de su plan para transformar la compañía de operaciones regulares en una aerolínea importante como United, TWA, y American. Simultáneamente, se encontraba en conversaciones con Boeing para que Continental se convirtiese en una de las primeras aerolíneas del mundo en operar el 707, avión de reacción que sería presentado poco tiempo después. La planificación era vital, puesto que las nuevas rutas podían justificar los 707, y viceversa.

A finales de los 50, la estrategia de Six se había consolidado. Continental Airlines estaba experimentado una mejora en sus rutas, gracias a la responsabilidad del CAB y a los persistentes esfuerzos de Six, quien se refería frecuentemente a su compañía como, "la aerolínea que necesita crecer." En 1957 voló por primera vez de Chicago a Los Ángeles (ambos directos, y vía Denver); y vuelos directos de Denver y Los Ángeles a Kansas City. Continental Airlines introdujo vuelos turbohélice con los Vickers Viscount, en las nuevas rutas de media distancia. La CAB permitió a Continental abandonar los vuelos a muchas de las pequeñas ciudades de su red, permitiendo a los nuevos aviones de la compañía operar de manera más económica entre puntos con mayor radio de distancia. Antes de la introducción de los aviones Boeing 707, Continental adquirió el popular DC-7 para operar sus rutas directas desde Los Ángeles a Chicago, así como la Denver-Los Ángeles y la Chicago-Kansas City (ver fotos).

A finales de los 50 y comienzos de los 60, Six se estableció claramente a sí mismo como el líder que abogaba por las tarifas bajas. Él había predicho correctamente que el incremento del tráfico, y no las tarifas altas, eran la respuesta a los problemas de la industria aérea. Six sorprendió a la industria cuando introdujo la tarifa económica en la ruta Chicago-Los Ángeles en 1962. Más tarde se convirtió en pionero al introducir tarifas bajas o con descuento que permitió efectuar viajes aéreos a muchos que de otra manera no habrían podido. Una de las primeras innovaciones en Continental Airlines fue la creación de un amplio sistema de tarifas de viaje económicas que posibilitó un recorte de las tarifas en más de un 25 por ciento.

Como Six había planeado, Continental fue uno de los primeros operadores del Boeing 707, al recibir el primero de sus cuatro 707 en la primavera de 1959. Aunque Pan Am y TWA inauguraron los vuelos del 707 unas pocas semanas antes de que lo hiciese Continental, Continental fue la primera aerolínea del mundo en utilizar el Boeing 707 en vuelos internos, utilizándolo por primera vez en la ruta directa Chicago-Los Ángeles el 8 de junio de 1959. Sin embargo, como la flota de 707s de Continental era relativamente pequeña con respecto a otras compañías, se necesitaban innovaciones radicales en el programa de mantenimiento del 707. Para mantener su pequeña flota de reactores, Continental desarrolló una primera industria: el innovador programa "mantenimiento progresivo" permitió a Continental volar con sus 707 los siete días de la semana, 16 horas al día, obteniendo mejores utilizaciones del avión que ningún otro operador de este avión en la industria aérea.

Six, no satisfecho con el único servicio del 707, introdujo innovaciones exclusivas y comida de lujo en las operaciones de los 707 de Continental que fueron descritos como "...sin renunciar al lujo" por el Los Angeles Times, y, "...claramente, la mejor de la industria aérea" por el Chicago Tribune.

A comienzos de los 60 Continental añadió rutas desde Los Ángeles a Houston, ambos directos y con vuelos de una y dos paradas a Houston pasando por Phoenix, Tucson, El Paso, Midland-Odessa, Austin, y San Antonio. También se inauguraron nuevos vuelos desde Denver a Seattle, Portland, Nueva Orleans, y Houston (a Houston: ambos directos, y con una y dos paradas pasando por Wichita/Tulsa/Oklahoma City). En 1963 la base de la compañía fue trasladada desde Denver a Los Ángeles.

A finales de los 60, la compañía se deshizo del último de sus aviones turbohélice y de pistón—una de las primeras aerolíneas de los Estados Unidos en hacerlo. Continental reemplazó la flota de aviones Viscount con los DC-9s de Douglas Aircraft y comenzó una agresiva campaña de adquisición de aviones Boeing 727. Estos dos aviones (DC-9 y B-727) se convirtieron en el caballo de batalla de la flota de Continental desde finales de los 60, y durante los veinte años siguientes. En 1968 un nuevo sistema de libreas de Continental Airlines fue presentado, unas rayas dorada y naranja paralelas adornadas con una "turbulencia" negra conformaban el logo (diseñado por el amigo de Six, el famoso diseñador gráfico Saul Bass) de las colas de los aviones (el logo fue más tarde modificado por el rojo; ver foto del 747). Los eslóganes adoptados en 1968 y utilizados durante una década aproximadamente, fueron "La aerolínea que está orgullosa de su construcción" y, "El pájaro orgulloso con la cola dorada.".

Durante la Guerra de Vietnam, Continental, proporcionó un buen número de transportes de carga y tropas para la Armada de los Estados Unidos y la infantería de marina a las bases de Asia y el Pacífico. Los 707 de Continental fueron los aviones no militares más comunes que pasaban por Saigon, en el Aeropuerto Internacional Tan Son Nhat. Como resultado de la experiencia que Continental adquirió en las operaciones del Pacífico, la compañía creó la antigua filial Air Micronesia en mayo de 1968, inaugurando rutas interinsulares entre Yap/Saipan/Guam, Majuro, Rota, Truk, Ponape (Pohnpei) y Honolulú. "Air Mike", como era conocida la nueva aerolínea, inicialmente operó con aviones Boeing 727-100 especialmente dotados de medidas de supervivencia para aterrizaje sobre el océano, radar doppler, y un sinfín de complementos (incluyendo amortiguadores). Un mecánico veterano voló a bordo de todos los vuelos de Air Mike hasta finales de los 70. Air Micronesia opera ahora como la filial Continental Micronesia.

En septiembre de 1969 se consiguió un objetivo prioritario: la introducción de vuelos de Continental desde Los Ángeles a Honolulú/Hilo; y en 1970, Continental fue premiada con rutas desde Seattle y Portland a San José, el Aeropuerto Hollywood-Burbank, y Ontario, California—todos ellos mercados aéreos emergentes. Los vuelos directos de San Francisco a Albuquerque y Dallas fueron añadidos el mismo año.

En 1963, Continental contrató al primer piloto afro-americano que trabajase en cualquiera de las grandes compañías de los Estados Unidos, Marlon D. Green, después de una decisión de la Corte Suprema de los Estados Unidos de una ley anti-discriminación en Colorado aplicada en este caso contra Continental. Green voló con Continental desde 1965 hasta su jubilación en 1978. La contratación por parte de Continental de Marlon Green sembró el camino para la contratación de otros pilotos por parte de todas las compañías aéreas, un hito en la industria que se plasmó finalmente en 1977 después de que Southern Airways y Piedmont contratasen a sus primeros pilotos de color.

Debido a la insistencia de Six, Continental (con Pan Am y Trans World Airlines) fue una aerolínea de lanzamiento del avión Boeing 747. El 26 de junio de 1970 Continental fue la primera compañía en introducir el 747 en los vuelos internos en los Estados Unidos. Su piso superior con asientos de primera clase y su planta principal con los asientos "Polynesian Pub" ganaron premios en todo el mundo por el interior de cabina más refinado de todas las aerolíneas, así como los servicios de comida desarrollados por el chef de Continental, Lucien DeKeyser. Los vuelos de Continental con 747 desde Chicago y Denver a Los Ángeles y Honolulu fijaron los estándares de vuelo del oeste de Estados Unidos. Cuando fue preguntado por un empleado de atención al cliente de Denver, en 1974, por qué volaba con Continental a cualquier sitio que fuese, la leyenda de Hollywood Henry Fonda señaló, "¡Esta operación es maravillosa; estrictamente maravillosa!"

El 1 de junio de 1972 los vuelos de Continental con el avión de fuselaje ancho DC-10 comenzaron. Six insistió en que Continental había efectuado un gran pedido de DC-10 con el fabricante McDonnell Douglas. Esta decisión volvió cumplir con las predicciones, puesto que la publicidad asociada con la entrada en servicio del 747 de Continental en el corredor Chicago-Denver-Los Ángeles-Honolulú que incrementó no solo el porcentaje de mercado si no que, incrementó los números de tráfico en todas las compañías. Además, Denver, Houston y Seattle experimentaron un gran incremento. Los DC-10 rápidamente asumieron muchos de los vuelos entre Denver y Chicago, a Los Ángeles, Houston y Seattle (y entre Houston-Los Ángeles).

Durante los 70, Denver continuó siendo la base de operaciones de la red de vuelo de Continental. Los 747 fueron ubicados en las rutas Chicago-Los Ángeles-Honolulú, con un vuelo de ida y vuelta desde Denver. Los aviones DC-10 operados en los mercados de gran distancia entre mercados de ciudades importantes (normalmente desde Los Ángeles a Chicago, Denver, Houston y Honolulu; y desde Denver a Chicago, Los Ángeles, Seattle y Houston). Los DC-9 y los B-727 predominaban en el sistema de vuelo, así como frecuencias adicionales en los mercados del DC-10. Junto con Braniff, Continental operó pocos tipos de aviones (cuatro: el B-747, el McDonnell Douglas DC-10, el B-727-200, y el DC9-30) durante este periodo que ninguna aerolínea grande de Estados Unidos, ahorrándose una buena cantidad en partes de aviones, mantenimiento y entrenamiento de los tripulantes.

El DC-10 probó ser una incorporación muy oportuna a la flota de Continental, al permitir a la aerolínea colocarse a la cabeza del crecimiento de pasajeros de los mercados del oeste de Estados Unidos. Continental fue testigo de crecimientos de pasajeros anuales en todos los mercados donde estaba el DC-10 durante los 70, hasta igualar sus cifras a las de United, el principal competidor en muchas de las rutas del DC-10. Las mismas innovaciones de servicio introducidos en la flota del 747 comenzaron a ser implementadas en la de DC-10s, incluyendo la clase "Polynesian Pub"; aunque después de la Crisis del Petróleo de 1973-aumentando los precios de combustible, se necesitaba incrementar el número de asientos para ser más beneficiosos-, los asientos pubs de los DC-10 fueron retirados.

De acuerdo con Robert Serling, biógrafo de Six, relató de manera totalmente exacta cada uno de los detalles de las operaciones de Continentalen los 60 y los 70. En una anotación anecdótica de la pasión de Six por dar los mejores servicios al pasajero, en cada una de las páginas del manual de servicios al cliente de la aerolínea aparecían las siguientes palabras inscritas: "Nada en este manual suplanta al sentido común." Bob Six en su implacable merodeo del sistema de vuelos de Continental, y de los vuelos de la competencia, para poder efectuar una búsqueda de ideas que se pudiesen aplicarse a la red de vuelos de Continental. En un continuo tributo a la pasión de Six por dar la mejor calidad de servicio al pasajero—y en un periodo de dificultades que resultó en un deterioro de la calidad de servicio entre 1982 y 1994—Continental obtuvo más galardones por su trato preferente a los pasajeros y por su profesionalidad en la industria de viajes que cualquier otra aerolínea.

En 1974, después de años de retrasos y procedimientos legales, Continental inauguró vuelos entre Houston y Miami, y el 21 de mayo de 1976, Continental fue autorizado a volar entre San Diego y Denver -ambas rutas habían sido largamente esperadas, y supuso un nuevo episodio de rápido crecimiento de Continental. El Presidente Carter y el consejero del Consejo de Aeronáutica Civil, Alfred Kahn, promovieron la desregularización de la industria de las aerolíneas (véase Ley de Desregularización de Aerolíneas), que disolvería la CAB y por primera vez en la historia de la industria de compañías estadounidenses las aerolíneas podían decidir donde debían de volar, sin la supervisión del gobierno, y pudiendo fijar las tarifas de manera libre. En este contexto, 1977 fue un año histórico para Continental y toda la industria aérea, mientras el CAB comenzó a perder sus beneficios de épocas pasadas. Continental comenzó a volar desde Denver a Miami/Ft. Lauderdale y Tampa/St. Petersburg. En el mismo año, el Presidente Carter autorizó a Continental a efectuar vuelos diarios entre los destinos de Air Micronesia a Saipán y Japón, y aprobó la ruta de Continental desde Los Ángeles a Australia vía Honolulú, Samoa, Fiyi, Nueva Zelanda y Australia. Los vuelos al sur del pacífico comenzaron el 1 de mayo de 1979.

Tras el 1978 con la comprobación de efectos del Acto de la Desregularización de las Aerolíneas, Continental, se embarcó en un agresivo plan de ampliación de rutas. En octubre de 1978 Continental comenzó a volar desde el área de aeropuertos de Nueva York a Houston y Denver, y desde Denver a Phoenix. Ese mismo mes, Continental inauguró vuelos con el DC-10 entre Los Ángeles y Taipéi, vía Honolulú y Guam. Los vuelos entre Houston y Washington D. C. comenzaron en enero de 1979. En junio de 1979, Continental conectó Denver con Washington D. C., Las Vegas, San Francisco y San José y también comenzaron los vuelos entre Houston y Tampa. En el momento de la adquisición de la compañía por parte de Texas Air Corp. en 1981, la Continental posterior a la desregularización había crecido hasta el punto de estar en todos los mercados principales de Estados Unidos (y en todos los mercados regionales) desde sus bases de operaciones en Denver y Houston; y la rápida ampliación aérea tuvo como respuesta la ampliación de las instalaciones de los vuelos de largo radio en cada uno de estos aeropuertos. En Denver, el rápido crecimiento de Continental propició un impulso final para la construcción del nuevo Aeropuerto Internacional de Denver, que quedaría completado quince años más tarde.

Durante 1978, Continental exploró la posibilidad de fusionarse con Western Airlines. Western también tenía su base en el aeropuerto internacional de Los Ángeles (LAX) y operaba principalmente una flota en la que predominaba los B-727 y los DC-10 como en la flota de Continental. El sistema de rutas hubiese sido complementario, con mínimos solapamientos; porque, aunque ambos operaban en los estados del oeste, Continental tenía una fuerte presencia en Hawái, la zona sur y los estados de la Gran Llanura; mientras Western era fuerte en el mercado interestatal de California, Alaska, México, y el oeste de la intermontañosa. Ambas aerolíneas operaban al Noroeste del Pacífico y los estados de las Montañas Rocosas, pero manteniendo rutas diferentes desde Los Ángeles, Denver, San Francisco, Seattle y Phoenix. Esta fusión nunca llegó a consumarse, sin embargo, los cambios de la industria albergaban un panorama totalmente diferente para el futuro de Continental.

Al contrario que algunas aerolíneas (especialmente Braniff cuya ampliación fue tan rápida e insostenible que los costes adicionales se tornaron en una inversión imposible, y la compañía terminó en bancarrota y liquidación), Continental tuvo una tasa de expansión basándose en el comportamiento del mercado tras la desregularización, ajustándose más a lo apropiado. Los mercados que fueron añadidos, fueron los que reportarían beneficios, y dotasen a la compañía de una fuerte base financiera para afrontar los reveses del futuro y hacerles frente como hicieron entre 1982 y 1994.

En 1981 Texas Air Corporation, una compañía de aerolíneas controlada por el apasionado de la aviación y corredor estadounidense Frank Lorenzo, adquirió Continental después de una batalla contenciosa con la dirección de Continental que estaba en contra de la adquisición de Lorenzo. Los sindicatos de Continental también se mostraron totalmente contrarios a la compra, lanzando una campaña de desgaste con el lema "Las tácticas de desregularización de Lorenzo", con la que pretendían sembrar la idea de que Continental quedaría segregada. Durante el conflicto, el presidente de Continental Airlines, A. L. Feldman, se suicidó, el 9 de agosto de 1981, en su oficina.

Finalmente, la oferta de Texas Air Corp. prevaleció. Frank Lorenzo se convirtió en el nuevo presidente y consejero delegado de Continental. El 31 de octubre de 1982 Continental se fusionó con Texas International (la fusión mantuvo el nombre, la marca y la identidad de Continental de la compañía; la marca e identidad de Texas International desapareció), ofreciendo vuelos a tres continentes (América, Asia y Oceanía) con una flota de 112 aeronaves. La "nueva Continental" reubicó su base en donde estaba la de Texas Air, en Houston, Texas. La fusión propició una gran expansión en la base de operaciones de Continental: el aeropuerto intercontinental de Houston y una ampliación de nuevas rutas a México y a la zona central y sur de Estados Unidos.

La fusión de las aerolíneas impulsó fuertemente a Lorenzo y Continental. Los juzgados federales, no fueron capaces de detener la reorganización de las empresas. No obstante, persuadieron al congreso de que promulgasen una ley anti-bancarrota con la que detener la reestructuración. La ley se promulgó demasiado tarde sin afectar a Continental ni a los costes e impuestos de la liquidación.

Frank Lorenzo introdujo a Continental en el Capítulo 11 de bancarrota el 23 de septiembre de 1983, después de no lograr alcanzar un acuerdo para reducir los sueldos con los sindicatos. La reconstrucción de la empresa comenzó de inmediato. Después de la bancarrota, Continental estaba libre de las obligaciones contractuales e impuso una nueva serie de acuerdos laborales con los sindicatos de sus trabajadores, permitiendo reducir los costes laborales de la aerolínea al reducir los costes de la moratoria con sus empleados. Este movimiento hizo a Continental mucho más competitiva y, con las nuevas bases de las aerolíneas emergió y volvió a dominar el margen suroeste de los Estados Unidos, pero tuvo un importante impacto negativo en las actitudes y valores de los empleados. En términos financieros, la aerolínea suprimió el tratado de bancarrota—a finales de 1984, Continental obtuvo 50 millones de dólares de beneficios. El 30 de junio de 1986, Continental abandonó el capítulo 11 de bancarrota. Continental ostenta la distinción de ser la primera aerolínea estadounidense en superar una bancarrota.

Durante este periodo, Continental se vio obligada a abandonar su pequeña base de operaciones en Los Ángeles, aunque mantuvo sus rutas desde LAX a Denver, Chicago, Houston, y al sur del Pacífico.

El 28 de abril de 1985, Continental comenzó su reflote, con la inauguración de sus primeros vuelos regulares a Europa con vuelos desde Newark y Houston a Londres. Poco tiempo después, se pusieron vuelos también a París, Fráncfort del Meno, Madrid-Barajas y Múnich.

En octubre de 1985, Texas Air Corp. hizo una oferta por la aerolínea regional con base en Denver Frontier Airlines, lo que dio pie a una enconada competición con People Express, liderada por el antiguo asociado de Lorenzo de TI Don Burr. People Express pagó una cantidad ingente por las operaciones de gran coste de Frontier. La adquisición con cargo a la deuda de la compañía no fue vista con buenos ojos por los observadores industriales según el criterio de la integración de rutas y la filosofía general de la operación aunque fue, según los analistas industriales, una forma por parte de Burr de llamar la atención de su antiguo jefe, Frank Lorenzo.
El 24 de agosto de 1986, Frontier cayó en bancarrota y canceló todas sus operaciones. Con el enorme sangrado económico de People Express, Texas Air adquirió PeopleExpress el 15 de septiembre de 1986, a la vez que se hacían con la fuerte red de destinos de Frontier en las Grandes Llanuras y al oeste de la zona intermontañosa reforzando el ya de por si fuerte centro de operaciones de Continental en Denver. Puesto que era la mayor aerolínea que efectuaba vuelos en el mercado de Nueva York, el centro de operaciones de PeopleExpress en Newark permitió a Continental ampliar fuertemente su red de destinos en la costa este por primera vez en su historia. Continental pronto se convirtió en la tercera aerolínea más grande en los Estados Unidos, y el principal operador en los mercados aéreos de Nueva York, Denver y Houston. Continental salió de su bancarrota en 1986 con características mejoradas y una importante caja y una estructura de rutas más competitivas con rutas a todas las ciudades importantes de Estados Unidos desde sus principales centros de operaciones de Denver y Houston.

El 1 de febrero de 1987, People Express, Frontier, New York Air, y algunas otras aerolíneas de alimentación se fusionaron con Continental Airlines creando la tercera aerolínea más importante de Estados Unidos (y la sexta aerolínea más grande del mundo). En consecuencia, Continental se convirtió en una de las principales participantes en los mercados del noreste. En 1987 se planteó la creación del programa de viajeros frecuentes de Continental, OnePass (en unión con Eastern Airlines); y, en 1988 Continental creó la primera estrategia de grupo (y la primera alianza internacional de este tipo) con SAS.

En 1990, Frank Lorenzo se retiró tras 18 años al frente de Texas International y más tarde de Texas Air y Continental Airlines, vendiendo la mayoría de su capital accionarial a Scandinavian Airlines System (SAS). De acuerdo con William F. Buckley, en su artículo del 17 de septiembre de 1990 en "National Review", la venta a SAS estaba condicionada a que Lorenzo abandonase la compañía.

El 3 de diciembre de 1990, Continental entró en su segunda bancarrota en una década. Hubo un buen número de circunstancias detrás de esta segunda bancarrota, entre las que las más importantes son: Lorenzo había dedicado casi todo su tiempo a la adquisición de Eastern Air Lines y a negociar nuevas condiciones laborales; la invasión de Kuwait por Irak y la posterior Guerra del Golfo en 1990 produjo un fuerte incremento del precio del combustible; y People Express tampoco estaba muy fuerte tras su fusión en Continental, tras haber adquirido Frontier Airlines tan solo dos años antes. Además Lorenzo se había embarcado en solventar las deudas de otras compañías, intentando también consolidar a las diferentes aerolíneas en un único grupo. Esto propició que la aerolínea contase con un número bastante elevado de aeronaves de distintos tipos, evidentes en las muy diversas libreas que comprendieron la flota de Continental durante años.

A finales de los 80, tras una reducción de vuelos por parte de United Airlines y el intento fallido de USAir en establecer rutas punto a punto, Continental amplió sus vuelos en el Aeropuerto Internacional de Cleveland Hopkins y estableció la que acabaría siendo su tercera base de operaciones de su amplia red de destinos. Continental rápidamente se hizo con la práctica totalidad de puertas en el módulo C del aeropuerto (anteriormente posesión de United), y más tarde amplió sus puertas con la construcción del nuevo módulo D.

El 12 de febrero de 1991, Continental presentó su nueva librea azul y gris, con el logo de un "globo terráqueo". Esta continúa siendo la seña de identidad de la flota de Continental y de sus instalaciones.

En 1993 Air Canada, junto a Air Partners y Texas Pacific Group, posibilitaron que Continental emergiese de la bancarrota invirtiendo 450 millones de dólares en la aerolínea. Bajo el nuevo liderazgo del antiguo ejecutivo de Boeing Gordon Bethune, quien se convirtió en su presidente en octubre de 1994, Continental comenzó un gran trabajo de reflote de la compañía. Bethune comenzó por pedir nuevos aviones en un esfuerzo por convertir su flota en una de aviones únicamente de Boeing fleet. Después de la apertura del Aeropuerto Internacional de Denver el 28 de febrero de 1995, la dirección de Continental decidió que la base de operaciones de Denver - su base de operaciones histórica y centro de su sistema de vuelos durante 60 años - vio reducir sus vuelos hasta quedar con una mínima parte (con vuelos solo a Houston, Newark, y Cleveland). Esta reducción estaba basada en la reducción de costes, puesto que las tasas y cargos del nuevo aeropuerto eran considerablemente más elevados que en Stapleton, al que había reemplazado este nuevo aeropuerto. Bethune también inició un "plan de avance", creado para reparar la mermada moral de sus empleados y para suprimir otros problemas de la aerolínea. Sus decisiones quedaron reflejadas en el libro publicado en 1999 "De lo peor a lo primero".

En septiembre de 1997 la aerolínea anunció que pretendía consolidar su base de Houston en el complejo Continental Center.

A comienzos de 1998, Continental se volvió a embarcar en un plan de ampliación de vuelos internacionales.Ese mismo año inauguró vuelos a Irlanda y Escocia, y en octubre de 1998 la aerolínea recibió su primer avión Boeing 777, permitiendo vuelos directos de Newark y Houston a Tokio, Japón, y desde Newark a Tel Aviv, Israel. Continental alcanzó ese mismo año acuerdos con Northwest Airlines, Copa, Avant Airlines, Transbrasil, y Cape Air, y Continental y America West Airlines se convirtieron en las dos primeras aerolíneas de Estados Unidos en lanzar el servicio de billetes electrónicos.

El 1 de marzo de 2001, Continental lanzó vuelos directos de Newark a Hong Kong, efectuando su ruta sobre el círculo polar norte. Este vuelo fue el primero de larga distancia directo con una duración superior a las 16 horas. El SARS en Asia provocó que los vuelos fuesen suspendidos hasta el 1 de agosto de 2003. La inauguración en 2001 comenzó con una breve batalla entre Continental, United Airlines y Cathay Pacific sobre los derechos de vuelos directos entre Hong Kong y Nueva York.

Continental introdujo nuevos vuelos directos a Oslo, Noruega, en 2004. En 2005, Continental amplió sus vuelos desde Newark a Pekín después de que las autoridades chinas les diesen permiso para efectuar el vuelo. En ese mismo año, se añadieron cinco nuevos destinos: Aeropuerto de Estocolmo-Arlanda en Suecia, Belfast y Bristol en el Reino Unido, y Hamburgo y Berlín en Alemania. Se añadieron vuelos a Colonia, Alemania, en 2006 y a Atenas, Grecia en 2007. De las aerolíneas estadounidenses, solo Delta opera a más destinos europeos que Continental.

En 2005 los vuelos a Asia fueron ampliados por Continental con la introducción de vuelos diarios directos entre Newark y Nueva Delhi, India. El éxito de la ruta de Newark a Nueva Delhi presagió la creación de una segunda ruta en India con el anunció diario de vuelos directos a Bombay. La inauguración de vuelos a Bombay implicó que Continental se convirtiese en la aerolínea que más vuelos directos diarios ofrecía de Estados Unidos a India.

En mayo de 2006, el tráfico de pasajeros de la compañía sobrepasó al de Northwest Airlines, y Continental se convirtió en la aerolínea más grande de los Estados Unidos, el primer cambio entre las cinco aerolíneas principales de pasajeros desde 2001.

El "The Wall Street Journal" anunció el 12 de diciembre de 2006 que Continental estaba inmersa en discusiones con United Airlines. Según el artículo Northwest Airlines, podría tener una acción de oro de Continental que dataría de la relación entre ambas compañías de finales de los 90, y la desaparición de la base de operaciones que Continental tiene en Guam. Las negociaciones no eran "seguras o inminentes", con charlas, en el mejor de los casos, preliminares.

Reconociendo las limitaciones en la cantidad de operaciones en Newark, Continental anunció sus planes de ampliar la utilización de su centro de operaciones de Cleveland con la implantación de más vuelos internacionales en Cleveland. El 14 de septiembre de 2007, Continental diseñó su plan de dos años en su base de operaciones de Cleveland, incluyendo nuevos vuelos desde Cleveland a París que comenzaron el 22 de mayo de 2008. Se espera continuar la ampliación de vuelos internacionales en breve, tan pronto esté completada la nueva oficina de la Inspección de Vuelos Federales en el módulo principal de Continental en Cleveland.

En cuanto a los vuelos internos, el plan de ampliación contempla dos fases. La primera fase comprende doce destinos que serían operados por aviones de reacción regionales, con los nuevos vuelos previstos en mayo de 2008. Más tarde, en 2009, estaban previstos 20 nuevos destinos, principalmente con aviones de la matriz. Continental sostuvo que la ampliación estaría completada a tiempo para la temporada de viajes del verano de 2009, aportando así 700 nuevos trabajos en la base de operaciones de Cleveland. Sin embargo, la crisis económica de 2008 acabó con estos planes y, de hecho, propició una reducción de vuelos en el centro de operaciones de Cleveland.
En mayo de 2008, Continental Airlines vendió las 4,38 millones de acciones que tenían en la compañía de bandera panameña Copa a un precio de 35,75 dólares por acción, obteniendo así 149,8 millones de dólares. Continental había sido uno de los principales accionistas en Copa.
Continental dijo el 5 de junio de 2008 que debido a las condiciones económicas nacionales e internacionales, se verían obligados a recortar 3.000 empleos y que el consejero delegado y el presidente reducirían sus salarios el resto del año. La aerolínea también dijo que reduciría su capacidad y eliminaría 67 aviones de la flota de la compañía principal a finales de 2009, retirando todos los 737-300 y dejando solo 35 de los 737-500 de Continental.

El 19 de junio de 2008, Continental anunció que planeaba abandonar la alianza SkyTeam y pretendía entrar en la Star Alliance con el fin de colaborar más de cerca con United Airlines y otras aerolíneas de Star Alliance. La nueva relación Continental-United ha sido visto en algunos círculos como una "fusión virtual". Continental dijo que su afiliación a SkyTeam había sido, sin embargo, la mejor forma de incrementar su tráfico de pasajeros de negocios. Continental había estado en negociaciones con United Airlines desde comienzos de 2008 con la intención de que ambas aerolíneas acabasen fusionándose, pero Continental dejó claro en las negociaciones su intención de continuar operando como aerolínea independiente.

El 19 de agosto de 2008, el "USA Today" anunció que Continental despediría entre 140 a 180 pilotos. El artículo también mencionó que más de 2.500 trabajos ya habían sido eliminados, muchos de ellos con los programas de prejubilaciones voluntarias. Continental dijo en junio que pretendía reducir su capacidad en vuelos internos en Estados Unidos en un 11% después del incremento de viajeros de la temporada de verano.

En septiembre de 2008, Continental anunció que efectuaría nuevos vuelos estacionales entre Houston y Río de Janeiro. El nuevo vuelo directo pretende ofrecer conexiones de vuelo desde el centro de operaciones de Continental de Houston a más de 160 ciudades de los Estados Unidos, Canadá, Centro-América, Europa, y Asia.

El 7 de enero de 2009, Continental fue la conductora del primer vuelo de demostración del uso del biodiésel por parte de un avión comercial en los Estados Unidos. La demostración de vuelo estaba dotado de un combustible especial de componentes derivados de algas y cultivos sostenibles, combustibles de segunda generación que no tiene impacto sobre la comida o agua de la población mundial y no contribuye a la deforestación.

El 29 de enero de 2009, Continental anunció pérdidas en su cuarto trimestre de 2008 por una cantidad neta de 266 millones de dólares achacables al coste de la retirada de pilotos y el incremento del precio del combustible.

El 3 de mayo de 2010 se anunció que Continental Airlines y United Airlines se fusionarían para dar lugar a la primera compañía aérea del mundo.


Continental, junto con Continental Express y Continental Micronesia, ofrecían más de 3.100 salidas diarias a destinos en América, Europa y la región del Asia-Pacífico. La programación de verano de 2008 mostraba que Continental atendía 145 destinos nacionales y 138 internacionales con más de 550 destinos más a través de los compañeros de SkyTeam.

Continental Airlines operaba principalmente en una red de rutas de bases de operaciones, con las principales de Norteamérica en Cleveland, Houston, y Newark, y su centro de operaciones del pacífico oeste en Guam. La mayor parte de sus vuelos estaban operados desde sus bases de operaciones, con algunas excepciones (las principales son Seattle-Anchorage y Los Ángeles-Honolulú). Algunas aerolíneas afiliadas utilizaban el nombre de Continental Connection para efectuar también vuelos que no partan de una base de operaciones, como por ejemplo Gulfstream International Airlines, que operaba vuelos dentro del estado de Florida y entre Florida y las Bahamas.

Durante más de 40 años, Continental operó una gran base de operaciones en Denver, Colorado, pero tomó la decisión de cerrar esta base en 1995 inmediatamente después de la apertura del Aeropuerto Internacional de Denver (DIA). DIA presentaba unos mayores costes de operación que el antiguo aeropuerto Stapleton, al que el DIA había reemplazado. La inesperada naturaleza de este cambio dolió mucho en Denver, que estaba viendo grandes crecimientos de pasajeros, en buena parte, gracias a Continental. La inevitablemente marcha de Continental permitió el establecimiento de la "nueva" Frontier Airlines (una nueva compañía, sin relación con la desaparecida del mismo nombre). Frontier se expandió rápidamente para ocupar el hueco que había creado Continental con su cierre de la base de operaciones de Denver.

Durante sus cuarenta primeros años de existencia, Continental era una aerolínea de vuelos internos; sin embargo, especialmente después de la incorporación de las rutas de Texas International, operaba a más destinos mexicanos que ninguna otra aerolínea estadounidense a mediados de los 80.

La primera entrada de Continental en el mercado transatlántico se produjo en abril de 1985, con la inauguración de los vuelos de Houston a Londres Gatwick. Aunque su intención era volar a Londres-Heathrow el acuerdo Bermuda II lo impedía, Continental ha mantenido sus vuelos a Londres-Gatwick, donde en 2007 al menos seis vuelos al día viajaban a Newark, Houston, y Cleveland.

En marzo de 2008, entró en vigor un Acuerdo de Cielos Abiertos entre los Estados Unidos y la Unión Europea, invalidando las restricciones del Bermuda II que limitaba el número de compañías y ciudades de los Estados Unidos que se podían efectuar desde Londres-Heathrow. En noviembre de 2007 Continental anunció que volaría a Londres-Heathrow desde sus bases del Aeropuerto Intercontinental George Bush y del Aeropuerto Internacional Libertad de Newark con dos vuelos diarios directos que pretendía estrenar el 29 de marzo de 2008. Los vuelos reemplazaron a los que antes había a Londres-Gatwick y que estaban operados con Boeing 777-200ER y 767-200.

Durante la Guerra de Vietnam, Continental efectuó un buen número de vuelos chárter militares implementando su presencia de la región del Pacífico que culminó con la creación de las operaciones de Air Micronesia. Los vuelos a Japón fueron inaugurados en los 70 desde Guam y Saipan, y a finales de los 80, los vuelos directos entre Seattle y Tokio fue durante un tiempo efectuados con Boeing 747, que pronto fue reemplazado por el vuelo Honolulú-Tokio (Narita). Durante los 90, Continental mantuvo una presencia mínima en el mercado de larga distancia transpacífico, hasta la entrega de los 777 en 1998 que permitió la apertura de vuelos directos a Tokio desde Houston y Newark. En 2007, Hong Kong y Pekín fueron añadidos a la red de rutas, y Shanghái les seguirá la estela en 2009, todos ellos desde la base de vuelos de Newark. Continental operó con Australia en el pasado con aviones Douglas DC-10 y desde Hawái con Boeing 747; si bien Continental redujo muchos de sus vuelos con Australia, pero continuó operando con los Boeing 737-800 de Air Micronesia entre Cairns y Guam.

Continental efectúa el mayor número de frecuencias regulares de todas las compañías de Estados Unidos a India, Japón, México, y el Reino Unido, y es la única aerolínea de Estados Unidos que vuela a Noruega, los Estados Federados de Micronesia, las Islas Marshall, y Palaos. Continental comenzó a volar desde Newark a Bombay, India el 1 de octubre de 2007 convirtiéndose en el segundo destino indio de Continental.

El 24 de septiembre de 2007 el Departamento de Transportes intentó premiar a Continental con servicios de vuelos diarios directos entre Newark y Shanghái, que comenzó en marzo de 2009. La ruta transpacífica se efectúa con el Boeing 777-200ER, cuyo vuelo tiene origen y final en Cleveland con un cambio de avión hasta Newark.

Continental estaba considerando a operar vuelos desde su base de operaciones de Houston a Dubái, Roma, Milán, y Madrid que planea inaugurar cuando comience a recibir los aviones 787 después de 2010.

Continental anunció el 12 de junio de 2008 sus planes de concluido sus vuelos a quince destinos como parte de su esfuerzo por reducir los costes debido al incremento de costes y la reducción de la demanda. La aerolínea cerrará sus puertas y puestos de facturación en cada uno de estos aeropuertos. Los vuelos a las siguientes ciudades serán cancelados totalmente: Denpasar, Bali, Indonesia; Cali, Colombia; Colonia, Alemania; Guayaquil, Ecuador; Monclova, México; Santiago, República Dominicana; Oakland, California; Palm Springs, California; Reno, Nevada; Sarasota, Florida; Tallahassee, Florida; Green Bay, Wisconsin; Chattanooga, Tennessee; Toledo, Ohio y Montgomery, Alabama.

Como resultado de las condiciones económicas actuales, los vuelos a otros destinos también se reducirán o eliminarán desde los centros de operaciones de Continental en Newark, Houston, Cleveland y Guam. Los viajeros de Houston y Cleveland se verán fuertemente afectados por los planes de reducción de vuelos de la compañía.

La flota de Continental, estaba compuesta en exclusiva de aviones Boeing tenía una media de edad de 9,5 años en diciembre de 2009. La flota se componía de cuatro tipos de aviones (Boeing 737, 757, 767, y 777) en once variantes, y a la espera de que dos variantes del Boeing 787 Dreamliner entren en servicio en 2011. Continental ha sido siempre una de las principales operadoras de aviones de consumo eficiente en la escena de la aviación. La utilización diaria de los aviones de la compañía era la más alta de la industria.

La flota de Continental consistía en los siguientes aviones:

Continental Airlines fue una de las tres compañías (junto a American Airlines y Delta Air Lines) en firmar un acuerdo de exclusividad con Boeing a finales de los 90. Cuando Boeing adquirió McDonnell Douglas, la Unión Europea obligó a Boeing a anular todos los contratos. Ambas partes llegaron a un acuerdo de caballeros finalmente.

Continental fue una de las primeras grandes aerolíneas en volar el Boeing 757 en rutas transatlánticas. Hubo un buen número de restricciones de alcance en los vuelos transatlánticos en dirección oeste debido a los fuertes vientos de frente provocando que se efectuasen paradas intermedias no contempladas en las programaciones, si bien estas escalas técnicas no son muy frecuentes. El uso de los 757 con su reducida capacidad de asientos permitió la operación en rutas "delgadas" (rutas con poco tráfico de pasajeros) convirtiéndolas en económicamente rentables. Esto posibilitó vuelos directos desde pequeñas ciudades, como Bristol, Inglaterra y Hamburgo, Alemania a Nueva York. Previamente, los clientes originales de estas y similares ciudades necesitaban conectar en alguna base europea como Londres, París o Fráncfort para viajar a Nueva York.



Continental Airlines tienia una configuración de dos clases de servicio, Primera/BusinessFirst y clase turista, para los aviones de la flota de la aerolínea principal.

Continental recientemente anunció, aunque aún no los ha instalado, asientos BusinessFirst que quedarían totalmente horizontales, reclinándose 180 grados y proporcionando 2 m de espacio para dormir cuando el asiento está completamente extendido en estos aviones de fuselaje ancho.El recostemiento del asiento ofrece una anchura de cuando el reposabrazos está colocado a ras del respaldo del colchón. Los controles electrónicos "one-touch" permiten a los pasajeros colocar fácilmente su asiento en la posición normal de viaje, en diferentes posiciones de descanso y algunos controles más para ajustar el respaldo, el soporte lumbar y el apoyo de piernas y pies. También está disponible una conexión para el iPod en los nuevos asientos de la clase Business. Los nuevos asientos BusinessFirst también tienen seis posiciones de reposacabezas, y una luz de lectura individual en el techo permitiendo al pasajero Business leer en cama sin molestar a su vecino y que le da una sensación de total confidencialidad.

Los nuevos asientos BusinessFirst se prevé que comiencen a funcionar en otoño de 2009. No hay una fecha fija para la entrada de los aviones a mantenimiento para cambiar sus asientos.

La primera clase nacional se ofrecía en vuelos internos. Está disponible en todos los aviones de la familia Boeing 737, así como de los Boeing 757-300. Los asientos tienen desde 20.75 a de ancho, y tienen entre 37 y de separación entre asientos. Los pasajeros a bordo de esta clase recibían comida, refrescos y bebidas alcohólicas de manera gratuita. Los pasajeros podían ver películas en las pantallas de TV del techo que hay por toda la cabina. A comienzos de 2009, Continental planó añadir LiveTV y servicios Wi-Fi a todos los Boeing 737 de nueva generación y Boeing 757-300 que serán gratuitos para todos los pasajeros de primera clase.

La clase turista estaba disponible en todos los vuelos internacionales. Los asientos tienian de 17.2 a de ancho, y tienian entre 31 y de separación entre asientos. Los pasajeros a bordo de esta clase recibían de manera gratuita comidas, aperitivos y bebidas no alcohólicas; las bebidas alcohólicas podían ser adquiridas por cinco dólares por bebida o un cupón de bebida de Continental.

Los aviones Boeing 757-200 disponían de sistemas de entretenimiento a la demanda (AVOD) en la parte trasera de todos los asientos. Los Boeing 767 y 777 estában equipados con una televisión personal en la parte trasera de todos los asientos, utilizando un sistema de cintas. Todos los Boeing 777-200 estarián equipados con AVOD a finales de 2009. En todos los Boeing 757-200 y los Boeing 777-200 que serán dotados de AVOD, estarián equipados con enchufes (dos enchufes por cada grupo de 3 asientos) que no requieren de adaptadores especiales o cables.

La clase turista estaba disponible en todos los vuelos internos. Los asientos median de ancho, y tenían entre 31 y de separación entre asientos. Los pasajeros a bordo de esta clase reciben de manera gratuita comidas, aperitivos y bebidas no alcohólicas. Las bebidas alcohólicas pueden ser adquiridas por 5 dólares o un cupón por bebida. Los pasajeros de algunos Boeing 737-300 y todos los Boeing 737-700, -800, -900, -900ER, y 757-300 pueden ver películas en las pantallas situadas en el techo que hay en toda la cabina y los auriculares pueden ser adquiridos por 1 dólar. En enero de 2009, Continental comenzó a añadir LiveTV y servicios Wi-Fi a todos los Boeing 737NG y Boeing 757-300 que costará, su uso por parte de los pasajeros de clase turista, 6.00 dólares.

Fundado en 1987, OnePass es el programa de viajero frecuente de Continental Airlines, Copa Airlines y Copa Airlines Colombia. OnePass ofrece al viajero regular la posibilidad de obtener billetes gratuitos, mejoras en vuelo a la primera clase, acceso a las salas vip en cada aeropuerto (President's Club), y otros tipos de recompensas. Los clientes acumulan millas de sus segmentos de vuelo que hayan efectuado con Continental Airlines y sus asociadas. Las tarjetas OnePass son Silver, Gold y Platinum y dan beneficios como la mejora de clase de vuelo gratuita, millas adicionales, facturación prioritaria, embarque prioritario, y mucho más. Continental tuvo un programa de viajeros frecuentes antes del OnePass, que se había iniciado poco después de que American Airlines comenzase el suyo propio en 1981 y cuando la mayoría de aerolíneas estadounidenses seguían sus pasos, pero se fusionó con el programa de viajeros frecuentes de Eastern Airlines en 1987 para conformar OnePass. El nombre "OnePass" se refería a la posibilidad de acumular millas en dos grandes aerolíneas, llamadas Continental y Eastern, en un único programa de viajeros frecuentes. El programa Onepass dará por finalizada su operación el 31 de diciembre de 2011 ya que a partir del 1 de enero de 2012 continuará sus operaciones con MileagePlus (programa de viajero frecuente de United Airlines)

Además de sus asociadas Continental Express, Continental Connection, y la alianza Star Alliance, Continental tiene programas de viajeros frecuentes con las siguientes aerolíneas:

Los miembros de OnePass también pueden conseguir millas a través de las compañías de alquiler de coches y hoteles asociados. Debido a su acuerdo con Amtrak, las millas también pueden obtenerse en algunos trenes Amtrak.

El Presidents Club es el programa de salas aeroportuarias de los pasajeros de Continental Airlines, Copa Airlines y Copa Airlines Colombia. Todas las salas disponen de bares. Continental fue la primera aerolínea en ofrecer wi-fi de forma gratuita en sus salas. Hay 26 salas en todo el mundo y tiene beneficios recíprocos con las 40 salas adicionales operadas por los miembros de SkyTeam que incluyen a Delta Air Lines, Aeroméxico, Alitalia, y Northwest Airlines. Los miembros del Presidents club también tienen acceso a las salas de Alaska Airlines y del Amtrak Acela. El Presidents Club tenía un coste en noviembre de 2008 para los miembros normales de OnePass de 5.500 dólares. Los pasajeros de clase BusinessFirst que efectúen itinerarios internacionales así como los pasajeros de la clase Business internacional tienen derecho a acceder a estas salas. Los viajeros BusinessFirst pueden llevar a dos invitados y los miembros del Presidents Club pueden llevar dos invitados o a su familia inmediata (esposa y niños de menos de 21 años de edad). Los miembros de las tarjetas American Express Platinum y Centurion tienen garantizado el acceso a los Presidents Club si van a tomar un vuelo ese día, operado por Continental con número de vuelo de esta.

Las ubicaciones de las Presidents Club son las siguientes:

En los quioscos de Continental Airlines en los aeropuertos los pasajeros pueden adquirir las "Continental Currency", una tarjeta de prepago para adquirir auriculares y bebidas alcohólicas en vuelo.

Continental permite a sus pasajeros comprar la "Continental Currency" por las siguientes cantidades:

Continental Airlines tiene acuerdos de código compartido con las siguientes aerolíneas en enero de 2011:

"Continental Connection" tiene un acuerdo de código compartido con American Eagle (el equivalente en American Airlines de Continental Express), aunque no con American Airlines. Aunque, American Eagle tampoco opera como Continental Connection, se limita a efectuar código compartido con Continental Connection, no con Continental Airlines. Los operadores de Continental Connection son:

Continental Airlines ha efectuado muchos esfuerzos para minimizar los efectos negativos al medio ambiente de las aerolíneas comerciales.
Los empleados de Continental han efectuado grandes esfuerzos para modificar el patrón de operaciones y así reducir el impacto medioambiental. La compañía invirtió 12.000 millones de dólares para adquirir 270 aviones de consumo eficiente y sus equipos. Estos esfuerzos ayudaron a reducir significativamente las emisiones de gases causantes del efecto invernadero, y mejoraron el consumo de combustible en un 45% en los últimos 10 años.

Continental creó un programa que permitía a los pasajeros compensar las emisiones de su vuelo con la posibilidad de pagar dos euros extra. El dinero recaudado se destina a la plantación de aquellas zonas de reforestación. Los pasajeros también pueden contribuir con 50 dólares o más a un fondo para la creación de proyectos de energías renovables como los de energía solar o eólicas, o el restablecimiento de la flora marina de los océanos o la reforestación a gran escala.

La Agencia de Protección Medioambiental (EPA) de los Estados Unidos otorgó el premio "Diseño del plan medioambiental" a Continental (2008) por el uso de preparados de tratamiento de superficie del avión libre de cromo que son medioambientalmente viables. Continental Airlines es la primera compañía que utiliza esta tecnología en sus aviones. El producto, "PreKote", elimina los residuos químicos que son normalmente utilizados en la fase de pretratamiento previa a la pintura del avión. Esta tecnología permitió una mejora de las condiciones de los empleados, mientras también se reducen los procesos de depurado.

Continental Airlines está planeando efectuar pruebas de vuelo empleando un avión propulsado por biocombustible. El 7 de enero de 2009, Continental en unión con GE Aviation efectuó un vuelo de demostración con biocombustible, convirtiendo a Continental en la primera compañía estadounidense que efectuó vuelos utilizando biocombustibles. La base de pruebas, un 737, en uno de sus motores estaba propulsado por una mezcla de queroseno, algas, y otros compuestos.<ref name="http://www.latimes.com/business/la-fi-biofuel8-2009jan08,0,761065.story"></ref>

Continental Airlines ha sido reconocida por la NASA y Fortune Magazine por su importante contribución al medio ambiente.

Los siguientes son los mayores accidentes e incidentes que ha sufrido la flota de Continental Airlines.






</doc>
<doc id="16948" url="https://es.wikipedia.org/wiki?curid=16948" title="College">
College

College (léase /ˈkɒlɪdʒ/ en idioma inglés) es el término utilizado para denominar a una institución educativa, pero su significado varía en los países de habla inglesa; del mismo modo que en francés la variedad de instituciones educativas denominadas "collège" (con acento grave; léase /kɔ.lɛʒ/).

El uso de la palabra "college" en el Reino Unido es muy amplio e incluye gran variedad de instituciones:

Las dos universidades antiguas de Inglaterra (Oxford y Cambridge, a veces llamadas en conjunto "Oxbridge"), son generalmente federaciones de facultades o "colleges" autónomos. Tienen un funcionamiento similar a los antiguos colegios mayores. Proveen alojamiento, comida, bibliotecas, actividades deportivas y sociales, también nombran tutores encargados de seguir el desempeño de los estudiantes. Por otra parte, la universidad provee las clases, realiza los exámenes y otorga los títulos. Los "colleges" son entidades totalmente independientes, propietarias de sus inmuebles, con personal propio y su propio presupuesto. En algunos casos los "colleges" pueden tener mejores condiciones financieras que las universidades a las que están asociados.

La Universidad de Durham también está organizada en "colleges", y estos también tienen su identidad legal propia. Sin embargo, los "colleges" de Durham no tienen independencia financiera y sólo ofrecen servicios estudiantiles, sin enseñanza. Universidades recientes, como Lancaster, York y Kent, tienen una estructura similar, salvo que sus "colleges" no tienen identidad propia. Oficialmente, la Universidad de Londres está formada por una serie de "colleges", sin embargo, la federación es mucho más flexible que en Oxford o Cambridge, al punto que se pueden considerar estas instituciones como universidades independientes.

En Estados Unidos el término "college" tiene un uso más restringido que en el Reino Unido y se reserva generalmente para instituciones de educación superior que pueden ofrecer titulaciones, tanto de pregrado como de postgrado. En la práctica, no existe diferencia entre la denominación "universidad" ("university") o "college", aunque originalmente un "college" era una facultad y una "universidad" era una institución con varias facultades. Hoy en día las universidades incluyen facultades ("colleges") y escuelas ("schools"), pero algunas de las más prestigiosas universidades de los Estados Unidos, como Boston, Harvard o Dartmouth, han mantenido la palabra "college" en sus nombres por razones históricas aun teniendo varias facultades y otorgando titulaciones en una gran variedad de áreas y niveles.

Una variante de estas instituciones son los colegios universitarios, denominados community colleges, junior colleges, technical colleges, o city colleges. Se trata de centros que solamente imparten programas de dos años de duración, es decir, que solamente otorgan el Grado de asociado o titulaciones propias.

En Australia, un "college" puede utilizarse al mencionar a un instituto de educación superior más pequeño que una universidad, cuya administración puede estar a cargo de una universidad o ser independiente. Debido a una reforma realizada en los años 1980, muchos de los "colleges" independientes hasta entonces pasaron a ser parte de universidades más grandes. Muchas escuelas secundarias también son denominadas "colleges" en Australia. El término también se utiliza al hacer mención de las residencias de estudiantes, como en el Reino Unido, pero al comparar sus programas de tutoría, las instituciones australianas resultan más pequeñas y no realizan actividades de enseñanza conducentes a la obtención de un grado académico, a excepción de una o dos instituciones de enseñanza teológica.

En Canadá, un "college" es una escuela técnica, de artes aplicadas o ciencias aplicadas –una institución que entrega diplomas de educación superior, que no corresponde necesariamente a una universidad, si bien existen excepciones–. En Quebec, puede referirse particularmente a CEGEP, una forma de educación superior que es parte del sistema educacional de Quebec.

Un "collège" (en idioma francés, con acento grave; léase /kɔ.lɛʒ/) es un centro de educación secundaria de primer ciclo. Consta de cuatro cursos que se suelen atender de los once a los quince años. El segundo ciclo de educación secundaria se cursa en un "lycée". 

En algunos casos, es un organismo de investigación:

En un caso muy especial, se ha hecho un uso irónico del concepto:

En Perú y Chile algunos colegios incluyen el término "college" como una manera de identificarse como bilingües (Markham College y Casuarinas College en Perú; Saint George's College, Santiago College y algunos Colegios Británicos en Chile). 

Desde 2009 la Pontificia Universidad Católica de Chile lo utiliza para darle nombre a una carrera de pregrado que, siguiendo el modelo de algunas universidades angloparlantes, combina "majors" y "minors".



</doc>
<doc id="16949" url="https://es.wikipedia.org/wiki?curid=16949" title="León Febres-Cordero Ribadeneyra">
León Febres-Cordero Ribadeneyra

León Esteban Francisco Febres-Cordero Ribadeneyra (Guayaquil, 9 de marzo de 1931 - "ib.", 15 de diciembre de 2008), fue un político ecuatoriano. Fue dirigente del Partido Social Cristiano de su país; Presidente del Ecuador entre los años 1984 y 1988; legislador entre los años 1970 y 1984, 2002-2004; miembro de la Asamblea Constituyente entre 1966 y 1967; Senador entre los años 1968 y 1970; Alcalde de Guayaquil en dos períodos, el primero de 1992 a 1996, año en que es reelegido, ocupando el cargo hasta el año 2000.

León Febres-Cordero nacido en Guayaquil, Sus padres fueron Agustín Febres Cordero Tyler y María Ribadeneyra Aguirre. Fue el sexto de siete hermanos (Nicolás, Agustín, Mercedes, Delia, María Auxiliadora y Leonoer). 

Se educó en el Colegio Salesiano Cristóbal Colón y viajó a los Estados Unidos a la edad de 16 años, donde completó su secundaria en Charlotte Hall Military Academy, y The Mercersburg Academy de Pensilvania. Tras ello, obtuvo su título de Ingeniero Mecánico en el Stevens Institute of Technology, en Hoboken (Nueva Jersey), en el año de 1953.

Desempeñó cargos como ejecutivo en importantes empresas de la nación, Cervecería Nacional, Empresa Eléctrica de Guayaquil, Industrial Molinera, Sociedad Anónima San Luis, Sociedad Anónima San Alfonso, Cartonería Ecuatoriana, Papelería Nacional y Textil Interamericana de Tejidos. En la actividad gremial incursionó al frente de la Cámara de Industrias de Guayaquil, de la que logró ser tres veces presidente entre los años 1974 y 1980, y de la Federación Nacional de Cámaras de Industrias.

Se casó en la ciudad de Guayaquil con la peruana María Eugenia Cordovez Pontón, pariente política por su prima María Febres-Cordero Carbo. Del enlace nacieron cuatro hijas:

Después de 34 años, la pareja se divorció en el año 1988, pocos meses después de terminado el periodo presidencial, casándose el mismo año con Cruz María Massuh, matrimonio que duró hasta la muerte de Febres Cordero.

La Junta Militar que gobernaba al país en 1966 fue depuesta por un grupo de notables ecuatorianos, quienes designan a Clemente Yerovi Indaburu como presidente interino. Éste, en su corta administración cumplió la misión que se le había encargado: realizar una Asamblea Constituyente, misma que se reunió en noviembre de 1966, para redactar una nueva constitución. A los 35 años, Febres-Cordero formó parte como asambleísta constituyente del órgano legislativo, participando en la redacción de la constitución, que fue aprobada finalmente el 25 de mayo de 1967.

En 1968 regresa al Congreso como Senador Funcional como representante de los sectores productivos, Luego fue parte de la segunda Comisión de Economía y Finanzas del Congreso, hasta que Velasco Ibarra se declaró dictador en 1970 y disolvió el Parlamento ecuatoriano. y luego

En 1973 cuando trabajaba para la Bananera Noboa que le pertenecía a Luis Noboa Naranjo se le pidió que girara cheques para el gobierno del presidente de aquella época Guillermo Rodríguez Lara, este se opuso férreamente y fue llevado a prisión por 93 días.

En 1978 se afilió al Partido Social Cristiano, y fue elegido diputado para el período entre 1979 y 1983. 

Durante su período como legislador, realizó varias interpelaciones a funcionarios públicos. La primera acción de este tipo por su parte, ocurrió en septiembre de 1980. En aquella primera ocasión, cuestionó al entonces Ministro de Finanzas Rodrigo Paz, por haber suscrito el decreto 343 en el mes de junio, que reforma el arancel de importación entonces vigente, que era inconstitucional por ser contrario a los artículos 53 y 59. En esta acción, la Cámara de Representantes no censuró al ministro.

Obtuvo notoriedad en septiembre de 1981 al encabezar un juicio contra Carlos Feraud Blum, entonces ministro de Gobierno del presidente Osvaldo Hurtado, a quien acusó de anomalías en la importación de juguetes que estaban destinados a los hijos de la fuerza policial. El monto de aquel contrato era de 6,7 millones de sucres.

Luego ocurre una interpelación contra el Ministro de Recursos Naturales, Eduardo Ortega Gómez. Febres-Cordero plantea junto a Hugo Caicedo un juicio político por la administración de recursos petroleros, en los trabajos del Golfo de Guayaquil y elevación de las tarifas eléctricas. Esta acción resultó favorable, dando paso a que la Cámara de Representantes declarara culpable al Ministro Ortega, el 8 de septiembre de 1982. En el mismo año, cuestionó el plan de expropiación de terrenos de la Isla Santay, por hallar un sobreprecio de 200 millones de sucres, que la Contraloría luego dictaminó como afirmativo. En este proceso, señaló directamente a Juan Pablo Moncagatta y John Klein, gobernador del Guayas y subsecretario de obras públicas, respectivamente.

Durante su permanencia como congresista, mantuvo estrecha amistad con quien más adelante sería su compañero de fórmula y binomio presidencial, Blasco Peñaherrera Padilla.

En 1982 adquirió protagonismo al entablar un juicio de peculado en la compraventa de la Isla Santay y su posterior plan de expropiación contra el entonces gobernador de Guayas, Juan Pablo Moncagatta. La Contraloría del Estado halló un sobreprecio de más de 200 millones de sucres, por lo que se emitieron órdenes de captura contra Moncagatta y otros implicados.

El , se realizaron las elecciones presidenciales, en las que se candidatearon 17 binomios presidenciales, el Partido Social Cristiano estructuró la alianza conservadora Frente de Reconstrucción Nacional y escogió como candidato a León Febres-Cordero para participar en la contienda. Al proclamarse los resultados, se anunció que el binomio conformado por Rodrigo Borja y Aquiles Rigaíl de la Izquierda Democrática encabezó estos con un 28,7% de la votación, mientras Febres Cordero y Blasco Peñaherrera del PSC alcanzaron el segundo puesto, logrando así pujar la segunda vuelta electoral.

En la segunda vuelta, el , gracias al Frente de Reconstrucción Nacional, coalición capitaneada por el PSC, Febres-Cordero gana las elecciones con el 51,54% de los votos, por tres puntos de diferencia, obteniendo 1,381,709 votos. El binomio Rodrigo Borja Cevallos y Aquilés Rigaíl obtienen 1,299,084 votos, el 48,46% de la votación total.

Tomó el cargo de mandatario para el periodo 1984-1988 después de ganar en las elecciones del 6 de mayo de 1984, junto con su compañero de fórmula Blasco Peñaherrera Padilla. El eslogan de su campaña fue «Pan, techo y empleo», y de ella es recordado el debate televisivo entre él y Rodrigo Borja Cevallos, candidato por la Izquierda Democrática.

Se posesionó el 10 de agosto de 1984, y durante los seis primeros meses de su período presidencial se caracterizaron por el enfrentamiento con el Parlamento. Coherente con las medidas de ajuste no elevó los sueldos y salarios sino en una proporción algo superior a la inflación. Gobernó con "decretos económicos urgentes", en total 26, para gestionar así el gasto público, convirtiendo de este modo la excepción en regla de gobierno.

Durante su mandato, culminó y entregó el ahora desaparecido edificio del Ministerio de Agricultura, Ganadería y Pesca en Guayaquil conocido como «La Licuadora», aportó a través de la Unidad Ejecutora para el Deporte de su gobierno 200 millones de sucres para la construcción del Estadio Monumental Isidro Romero Carbo de Barcelona en la ciudad de Guayaquil, el Estadio Olímpico de Ibarra en Imbabura, el Estadio Reales Tamarindos de Portoviejo en Manabí para la realización de los VI Juegos Nacionales de Manabí en 1985, construyó carreteras como Ibarra - San Lorenzo, construcción y la reparación en general de carreteras en Litoral, Sierra, Región Amazónica e Insular. Durante su gobierno aumentó las exportaciones no petroleras, que en 1988 llegaron a los 1.800 millones de dólares, cuando dos años antes, previo al inicio de su mandato, habían sido apenas de 600.

En su gestión se firmó mediante decreto la creación del Fondo Nacional de la Cultura, conocido también como FONCULTURA, que se hallaba integrado por un porcentaje del 15% del fondo que el Banco Central del Ecuador destinaba a los proyectos culturales en general, sumado al 5% de las utilidades anuales del Banco Ecuatoriano de Desarrollo, y las asignaciones anuales del presupuesto del Estado para la ejecución de los proyectos que entren en esta competencia. Existía para la calificación de proyectos culturales el Consejo Nacional de Cultura, y también en aporte a otras instituciones como la Casa de la Cultura Ecuatoriana el destino de un porcentaje de fondos que ciertas actividades generaban y eran gestionados por el Banco Central.

Durante su gobierno, Febres-Cordero ejecutó la construcción de los hospitales del IESS en Tena y el Civil de Ibarra, el Hospital de Niños Baca Ortiz en Quito, además de centros y subcentros de salud en distintos lugares del país. Impulsó el programa de atención médica y entrega de medicinas gratuitas a menores de cinco años llamado Megramé 5, que llevó a cabo a través del Ministerio de Salud Pública, y el apoyo de su entonces esposa María Eugenia Cordovez.

Durante su gobierno se impulsó la construcción de la Vía Perimetral de Guayaquil. Este corredor vial tenía como propósito ser la circundante del cantón Guayaquil en la provincia del Guayas. En el tramo de esta vía que corresponde a los territorios del Cantón Daule, se renombró en 2009 a Avenida León Febres-Cordero Ribadeneyra, en homenaje póstumo, por ordenanza municipal del mismo cantón.

En 1983 surgió la actividad subversiva del grupo terrorista "Alfaro Vive, ¡Carajo!" que en agosto de 1985 secuestró al banquero Nahím Isaías. El propio Presidente dirigió el operativo militar de rescate de la víctima, que murió con los secuestradores en condiciones que nunca fueron plenamente determinadas durante el mencionado asalto. La «lucha contra el terrorismo» se convirtió en política oficial del régimen. Las medidas económicas de ajuste ayudaron a que el PIB crezca y hubo superávit en 1984 y 1985. Pero en el segundo semestre de 1986 el precio del petróleo ecuatoriano en el mercado internacional cayó de 27 a 8 dólares, y el 7 de marzo de 1986, aduciendo motivos éticos, el general Frank Vargas Pazzos se rebeló en la base aérea de Manta y el 15 de marzo en la de Quito.

En 1986, el Comandante General de las Fuerzas Armadas, Frank Vargas Pazzos, acusa Luis Piñeiro, Ministro de Defensa de Febres-Cordero, por sobreprecio en la compra de un avión Fokker F-28 para TAME. Se inicia un proceso de investigación el 19 de marzo de 1986, por una Comisión de Fiscalización del Congreso, el 24 de abril la comisión dictamina que no existió irregularidades en la compra del avión, a pesar de que la Contraloría General del Estado estableció glosas por 200 millones de sucres, sin establecer implicados en las irregularidades.

En 1987, unos comandos de la Fuerza Aérea cercanos a Vargas secuestraron al presidente Febres-Cordero y a su comitiva en la Base Aérea de Taura durante 12 horas, y negociaron la libertad de los secuestrados a cambio de la libertad del general Vargas, prisionero desde marzo de 1986 por sus actos de rebelión, y también a cambio de que el Presidente no tomara represalias contra los secuestradores. El Congreso en su mayoría opositor aprovechó la coyuntura para pedir la renuncia del Presidente, pedido que finalmente no prosperó. A este hecho, los medios de comunicación le denominaron "El Taurazo".

En 1988, el régimen se debilitó por varios escándalos de corrupción en las altas esferas del gobierno, la caída del precio de petróleo, y la interrupción de las exportaciones petroleras debido a un terremoto, agravando la crisis económica, y disminuyendo los ingresos hasta por 3 mil millones de dólares. El gobierno a partir de entonces incrementó el gasto público y el endeudamiento estatal, tomando medidas que estaban orientadas a la promoción de sector exportador, y del capital financiero. El equipo económico se desbandó y hasta el vicepresidente de la república, Blasco Peñaherrera Padilla, se alejó del presidente. A pesar de todo esto, el gobierno no alteró el plan de gasto para el último año de la administración, pues Febres-Cordero quiso terminar los proyectos que empezó.
El desmesurado gasto público durante el último año de gobierno, cuando la economía estaba en crisis, reflejó la adopción de medidas calificadas como populistas, antepuestas a las principales que marcaron una tendencia hacia la liberalización de la economía. El PIB decayó en un 6% tras el terremoto de 1987, y creció la inflación en un 85,7%

En el gobierno de Febres Cordero hubo varias denuncias de corrupción y abusos a los derechos humanos. Entre los casos de corrupción se encuentran la huida de Joffre Torbay, Secretario de la Administración Pública, luego de ser sindicado por la compra de 350 carros recolectores de basura a la empresa mexicana DINA que dejó una deuda de cuatro mil millones de sucres; denuncias de presunto sobreprecio para la vía Perimetral y robo de orejeras de oro, pinturas y obras de arte del Palacio de Carondelet 

En mayo de 1987, Xavier Neira, Ministro de Industria del Gobierno de Febres-Cordero es sindicado por un supuesto caso de peculado en prestación de servicios con la empresa Ecuahospital. Durante los 33 meses que duró el proceso legal, Neira paso 18 meses en Miami. En febrero de 1990 el caso fue sobreseído por Ramiro Larrea, Presidente de la Corte Suprema de Justicia.

El 18 de enero de 1990, Ramiro Larrea, Presidente de la Corte Suprema de Justicia, dictamina orden de prisión preventiva contra Febres-Cordero, por la entrega de 150 mil dólares en diciembre de 1986, al asesor de seguridad, el israelí Ran Gazit, quien colaboró en el combate contra la guerrilla en el Ecuador, también fue implicado el yerno de Febres-Cordero, Miguel Orellana, el caso fue sobreseído definitivamente en agosto de 1990 por la cuarta sala de la Corte Suprema de Justicia.

Durante su gobierno se produjeron graves violaciones a los derechos humanos, especialmente casos de desaparecidos, hechos que provocaron la condena de la Corte Interamericana de Derechos Humanos al Estado Ecuatoriano, imponiéndole la obligación de reparar a la víctimas y de investigar y sancionar a quienes cometieron dichos actos. Sin embargo sus seguidores sostienen que algunas de estas acusaciones no estuvieron sustentadas con pruebas documentadas e imparciales. También persiguió tenazmente a sus opositores políticos y atentó contra la independencia de las otras funciones del Estado. Rodeó con tanques de guerra la Corte Suprema de Justicia, para así evitar la toma de posesión de su nuevo presidente, el cual según el gobierno de Febres-Cordero era ilegal.

Entre los casos de atropello a los derechos humanos durante su presidencia , uno de los más conocidos es el caso de la desaparición de los hermanos Carlos y Pedro Restrepo Arismendi, y el de la tortura, violación y ejecución extrajudicial de la profesora Consuelo Benavides, detenida por miembros de la Fuerza Naval de Ecuador. Durante tres años, hasta diciembre de 1988, las familias no conocieron cuál era el paradero de los desparecidos, a pesar de solicitar información repetidamente a las autoridades ecuatorianas. En el mismo periodo, la Comisión Ecuménica de Derechos Humanos de Ecuador, Amnistía Internacional y el Grupo de Trabajo de las Naciones Unidas sobre Desapariciones Forzadas e Involuntarias realizaron peticiones similares a las autoridades. El gobierno ecuatoriano de entonces no facilitó en ninguno de los casos información suficiente a estas entidades sobre sus paraderos.

Durante su ataque a la delincuencia e insurgencia del país, se presume que se crearon «escuadrones de la muerte» dedicados a castigos y ejecuciones sumarias.
El , el presidente ecuatoriano y opositor de Febres-Cordero, Rafael Correa, creó en Quito una "Comisión de la Verdad" para investigar los crímenes ocurridos durante la presidencia de Febres-Cordero. Se espera que los elementos encontrados por la Comisión sirvan de base para iniciar procesos judiciales en contra de los principales cabecillas. A pesar de ello hay controversia alrededor de la comisión: sus opositores alegan que se debería investigar también a los acusadores, especialmente a los exguerrilleros; otros afirman que debería haber fuertes sanciones a los implicados.

La Comisión de la Verdad en 2013 llegó a la conclusión que durante el gobierno de Febres-Cordero ocurrieron crímenes de lesa humanidad, comprobándose que existen indicios de que ocurrieron desapariciones forzadas, tortura, arrestos y detenciones arbitrarias y violencia sexual en contra de supuestos miembros de AVC, ante lo cual la Fiscalía llamo a juicio a varios miembros de la cúpula de las Fuerzas Armadas de la época y jefes de la policía.

León Febres-Cordero anunció su candidatura para la alcaldía de Guayaquil por el Partido Social Cristiano el 6 de febrero de 1992.

En 1992, fue elegido alcalde de Guayaquil tras haber ganado las elecciones municipales, posesionándose como tal el 10 de agosto de 1992. 

Su primera orden fue cerrar el Municipio de Guayaquil por varias semanas, misma que anunció en una cadena televisiva de 45 minutos donde daba cuentas del estado en el que halló el edificio y la administración municipal. Entonces eliminó a 2 499 «pipones» del Municipio, y el sindicato de Aseo de Calles, remodeló el edificio municipal al que calificó de «nido de ratas» y dio inicio a un proceso de regeneración de la ciudad.

Dos meses después de iniciar su gestión como burgomaestre de Guayaquil, inició la campaña cívica-educativa denominada "Ahora o nunca: Guayaquil vive por ti". Esta campaña iba enfocada a rescatar el civismo entre los guayaquileños y residentes en Guayaquil, además de dar identidad al habitante con la urbe. Esta campaña se acordó mediante la expedición de un Acuerdo Ministerial con el entonces Ministro de Educación, Dr. Eduardo Peña Triviño, e incluía como mandato la difusión de carácter obligatorio del proyecto en los establecimientos educativos del cantón. En la campaña se incluyó la imagen de Juan Pueblo, obra de Virgilio Salinas.

Entre las obras de su gestión como alcalde se destacan sobre todo la transformación del ornato, la vialidad, sistemas de pasos elevados para descongestionar el tránsito vehicular, la construcción de mercados, la legalización de tierras para 80 mil familias en invasiones, la regeneración en la Pedro Pablo Gómez, la recolección de basura de la ciudad de Guayaquil, que en la década de 1980, Guayaquil fue llamada la "Calcuta de América" se había convertido en foco de suciedad y falta de higiene y, estaba casi completamente carente de obras públicas emprendidas por el municipio. Así mismo reestructuró el Municipio de Guayaquil, colocando a la institución en un proceso de modernización que ha servido de ejemplo para otras entidades municipales latinoamericanas. En lo económico, practicó la economía social de mercado durante su administración.

En el año de 1996, fue reelegido como alcalde en las elecciones municipales, obteniendo una victoria con el 86% de los votos. Desde este nuevo período, se destacó creando obras de gran escala como las bases del actual Malecón 2000, administrado ya no como obra pública sino de una manera descentralizada, estableciendo así un novedoso y ágil sistema de administración que prioriza las alianzas público-privadas.

Entre los 3 períodos en los cuales Febres-Cordero actuó como legislador, en el año 2002 fue elegido como diputado del Congreso Nacional, siendo el congresista más votado por los electores. En la palestra del poder legislativo, tuvo a su cargo varios proyectos de ley y realizó denuncias de corrupción a ministros y funcionarios de gobiernos anteriores, pero fue marcado su ausentismo a las sesiones del Parlamento, en el cual casi nunca intervino por motivos de salud (su hipertensión arterial le impedía viajar a la altura de más de 2.500 metros de Quito). En el 2006 volvió a incurrir en la carrera por un curul en el Congreso Nacional, siendo nuevamente elegido para la misma dignidad. A principios de 2007, el día anterior a su posesión como legislador, presentó la renuncia al cargo, por problemas de salud que ya no le permitían seguir en el mismo, permitiendo que su curul la ocupe Dimitri Durán del PSC, quien fue designado diputado alterno de él.

Fue intervenido quirúrgicamente para trata un incipiente cáncer de vejiga. Esta fue la única vez que se lo operó en Guayaquil.

En 1996, en otro proceso quirúrgico, se le colocaron tres "by-pass" coronarios para tratar obstrucción de arterias.

Durante 1996, empezaron los problemas médicos en el ojo derecho a causa de un glaucoma. Cuando era Alcalde de Guayaquil, en 1997, se le desprendió la retina de su ojo derecho y se le practicaron tres cirugías, siendo que se presentaron problemas en la recuperación. El 30 de marzo de 2005 tuvo una infección severa a causa de un glaucoma en su ojo derecho, un desplazamiento de retina y cataratas, le obligan a los médicos del hospital Bascom Palmer Eye Institute en Miami a retirarle el globo ocular derecho, el ojo extirpado fue reemplazado por una prótesis, conectada quirúrgicamente a nervios y músculos internos, permitiéndole movilidad.

En 1998, se le realizó una cirugía en la arteria carótida izquierda, a causa de una isquemia .

En 2007, en Miami, Estados Unidos, se sometió a una operación de la pierna derecha, en el mes de febrero, por un problema cardiovascular.

El fue sometido a exámenes para determinar el avance del cáncer de pulmón, se le realizó un broncoscopia, recibió sesiones de radioterapia para quemar y reducir el tamaño de los tumores. Febres-Cordero decidió retornar a Guayaquil, los familiares firmaron un documento eximiendo de responsabilidades a la clínica.

El expresidente murió el lunes 15 de diciembre de 2008, víctima de una doble complicación a sus pulmones (cáncer y enfisema), en la clínica Guayaquil.

Los Funerales se realizaron durante tres días, del 15 al 17 de diciembre de 2008, en la Catedral Metropolitana de Guayaquil, las calles aledañas a la basílica estuvieron cerradas al tráfico vehicular, las 3 puertas de la Catedral permanecieron abiertas desde las siete de la mañana hasta las doce de la noche para recibir una multitud de seguidores que constantemente entraban y salían de la catedral, acompañados de autoridades y personalidades que asistieron a las honras fúnebres de Febres-Cordero, durante tres días se declaró duelo nacional, mediante decreto presidencial recibió todos los honores. En su velatorio que se llevó a cabo en la Catedral Metropolitana, se realizaron misas cada hora por una decena de sacerdotes y curas, obispos, los medios de comunicación le dieron cobertura en vivo durante la entrada y salida de la catedral. Una vez terminada la misa de cuerpo presente, los restos del exmandatario recorrieron por las calles de Guayaquil, donde él fue Alcalde durante ocho años






</doc>
<doc id="16951" url="https://es.wikipedia.org/wiki?curid=16951" title="Epístola a los gálatas">
Epístola a los gálatas

La Epístola a los gálatas es un libro de la Biblia en el Nuevo Testamento. Es una carta escrita por Pablo de Tarso a los cristianos que habitaban la provincia romana de Galacia, en Asia Menor (que correspondía a la actual zona sur del Asia menor, donde estaban la región de Licaonia y las ciudades de Iconio, Listra, Derbe y Antioquia de Pisidia).

Se escribió entre los años 50 a 56 d. C. aproximadamente. Se sabe que la escribió luego de dos visitas a esa provincia y que, conforme el Libro de los Hechos, Pablo y Bernabé visitaron la zona entre los años 47 y 48 d.C. por primera vez, y luego volvió Pablo con Silas cuando volvían de la reunión o concilio de Jerusalén en el año 49 d.C. Puede que Pablo la escribiera desde Corinto en su estadía allí de casi dos años, entre el 50 y el 52 d.C. Otros la ubican en una fecha más tardía, alrededor del 56 d.C.

Es la vindicación del Santo Evangelio de Jesucristo, en contraposición con los preceptos judíos (Ley ceremonial) que se habían mezclado dentro de la iglesia cristiana de ese lugar. La epístola revaloriza y asienta orientación y rumbo, pues los gálatas comenzaron a ir para atrás, y volvían a la Ley mosaica, creyendo así afirmar su salvación. La carta es una clara enseñanza contra los judaizantes.

La carta es fiel en demostrar muchos rasgos de los habitantes de esas ciudades. Los judaizantes eran una fuerte secta en el cristianismo primitivo, y al parecer había calado profundamente, ya que estos negaban el apostolado de Pablo. Y usaban la zona del Asia Menor como un lugar predilecto para divulgar sus enseñanzas.

La autenticidad está dada por los registros más antiguos encontrados. Esta carta fue utilizada por Policarpo de Esmirna en el siglo II d.C.; figura en el fragmento Muratori, y en los escritos de Ireneo de Lyon. Además, se encontró con ocho cartas más en el llamado manuscrito de Chester Beatty del año 200 d.C. También otros patriarcas de la iglesia primitiva la mencionan, tales como Clemente de Alejandría, Tertuliano y Orígenes. Se la menciona por nombre en el canon reducido de Marción. Todo el canon anterior al concilio de Cartago, en el año 397 d.C., la incluían en los escritos como auténtica. Además existe una clara correlación y estilo con los otros escritos de Pablo.













Pablo habla de la verdadera libertad, no de esa que cubre los deseos de la carne sino el de ser esclavos de Cristo y habla que el esclavo de los deseos carnales no heredará el reino de Dios tal como lo hace el que da frutos en Cristo. Estos no serán condenados por la ley si se dejan llevar por el Espíritu.

Sorprende que en una carta donde la ley no ha sido considerada precisamente como algo positivo, ahora se hable de "ley de Cristo". ¿Qué ley es esta que Pablo atribuye a Cristo y a la que alude en otros pasajes de sus cartas? Puede decirse, por supuesto, que la ley de Cristo es simple y pura y que es simplemente el Amor. Pero, dando un paso más, puede también decirse que la ley de Cristo es el propio Cristo en cuanto que se ha hecho para nosotros modelo y norma suprema de conducta.






</doc>
<doc id="16952" url="https://es.wikipedia.org/wiki?curid=16952" title="Epístola a los filipenses">
Epístola a los filipenses

La Epístola a los filipenses o simplemente Filipenses es un libro de la Biblia en el Nuevo Testamento. Se trata de una carta que tiene en Pablo de Tarso su autor prácticamente indisputado, y en los cristianos de Filipos sus destinatarios. Escrita entre los años 54 y 61 d. C. en prisión, consta de 4 capítulos. Su propósito principal fue agradecer a los cristianos de Filipos la ofrenda que ellos le enviaron. Pablo trata también temas como la humildad, el gozo, la unidad y la vida cristiana.

Filipos era una ciudad griega de la provincia de Macedonia, donde Pablo había fundado una comunidad cristiana cerca del año 50 d. C. durante su segundo viaje misional.

Las dataciones de la Epístola a los filipenses suelen agruparse según se sostenga que fue escrita en Éfeso (hacia el año 56), en Cesarea (58-60) o en Roma (61).

Según la datación tradicional, la epístola habría sido escrita alrededor del año 60 a 62 d. C., desde la prisión en Roma, la denominada «primera prisión». Se sabe que fue redactada en prisión porque así lo señala la misma carta, al hacer referencia a sus «prisiones» o «cadenas» (; ) y al «pretorio» (). La datación tradicional sostiene que el primer periodo de prisión de Pablo en Roma data del 59 d. C. al 61 d. C.

La gran mayoría de los autores modernos datan la carta más tempranamente. Joseph A. Fitzmyer señala que la Epístola a los filipenses habría sido escrita muy probablemente a raíz de un encarcelamiento en Éfeso, ca. 56 d.C. Vidal García la data de 53-54, también en la prisión de Pablo en Éfeso, y no en las posteriores en Cesarea y en Roma. También la Escuela bíblica y arqueológica francesa de Jerusalén data esta carta de la prisión de Éfeso en 56-57 d.C. Las alusiones al pretorio () y a la «casa del César» () no ofrecen dificultad, porque había destacamentos pretorianos en todas las grandes ciudades —tal el caso de Éfeso— y no solo en Roma.


Insiste en ello poco después cuando dice: "mis cadenas se han a conocer en todo el pretorio" ().

San Pablo exhorta a los filipenses a mantener la unidad y la paz en su comunidad, y a tal fin los invita a seguir el ejemplo de humildad dado por el Señor: «Tened entre vosotros los sentimientos propios de una vida en Cristo Jesús. Él, a pesar...» (v. 5); estas palabras enlazan con el texto del Cántico que para Nácar-Colunga es de extrema importancia dogmática porque en él se declara el triunfo de Cristo por la cruz y el anonadamiento sin dejar de ser Dios.]

Se rebajó, por eso Dios lo levantó.

Pablo está urgiendo a la comunidad de Filipos la unidad eclesial, cuyo presupuesto básico es la humildad (Flp 2,1-4). Les propone ahora, como acicate, un formidable ejemplo: la humillación de Cristo que desemboca en su glorificación.

Los vv. 6-11 constituyen un precioso himno a Jesucristo. En él aparecen los elementos característicos de los himnos cristológicos.

El tema central de la perícopa es el contraste entre la humillación de Cristo y la gloria de su resurrección, por la que queda constituido Señor de cielos y tierra.

Pablo piensa en el Cristo histórico, en el complejo teándrico: Dios y hombre. Pues bien, como Hijo de Dios, tenía por esencia todos los atributos divinos. Pudo haber manifestado exteriormente la gloria, que desde siempre poseía, y, por lo tanto, aparecer glorioso en su humanidad. Pero no lo hizo así. Hecho hombre, asumió la condición puramente humana, como uno de tantos, cargado con las debilidades comunes a los mortales, excepto el pecado. Su humillación culminó en la obediencia a la muerte de cruz.

Por este anonadamiento y obediencia, el Padre lo glorificó constituyéndolo sobre toda la creación, y ordenando que toda criatura reconozca a Jesucristo como Señor, como Dios.

En Cristo se cumplió, como en ningún otro, lo que él había advertido a los demás: «El que se enaltece será humillado, y el que se humilla será enaltecido» (Mt 23,12).

6Cristo, a pesar de su condición divina,
no hizo alarde de su categoría de Dios;
7al contrario, se despojó de su rango
y tomó la condición de esclavo,
pasando por uno de tantos.

Y así, actuando como un hombre cualquiera,
8se rebajó hasta someterse incluso a la muerte,
y una muerte de cruz.

9Por eso Dios lo levantó sobre todo
y le concedió el «Nombre-sobre-todo-nombre»;
10de modo que al nombre de Jesús toda rodilla se doble
en el cielo, en la tierra, en el abismo,
11y toda lengua proclame:
Jesucristo es Señor, para gloria de Dios Padre.

http://www.franciscanos.org/oracion/canticofilip2.htm


Estas advertencias y el polémico contenido que viene después sustenta la hipótesis de que esta epístola es en realidad una fusión de dos cartas independientes o que fue escrita con alternancia de dos estados de ánimo diferentes. Los capítulos 1 y 2 serían de la carta "laudatoria" y el capítulo 3 de la carta polémica. En cuanto al capítulo 4 es en su mayor parte tranquilo aunque algunos versículos pueden considerarse como pertenecientes a la polémica.


La copia más antigua que se conserva de la Epístola a los filipenses es el papiro 16 (en la numeración Gregory-Aland), designado como formula_1. Contiene los pasajes 3,10-17 y 4,2-8. Ese manuscrito fue datado paleográficamente de finales del siglo III. Fue descubierto por Grenfell y Hunt en Oxirrinco en 1910.





</doc>
<doc id="16953" url="https://es.wikipedia.org/wiki?curid=16953" title="Epístola a los colosenses">
Epístola a los colosenses

La Epístola a los Colosenses es uno de los veintisiete libros que constituyen el Nuevo Testamento. Es una breve carta dirigida a los creyentes en el mesías que habitaban en la antigua ciudad de Colosas, otrora situada en Frigia, al sudoeste de Asia Menor. La carta se presenta como obra de Pablo de Tarso, autor de otras epístolas incluidas en el Nuevo Testamento, y la tradición eclesiástica no cuestionó su autoría. Sin embargo, desde principios del siglo XIX se ha puesto en cuestionamiento que fuese Pablo el auténtico autor. En la actualidad, su autoría está en debate.

La tradición eclesiástica ha venido atribuyendo la epístola a el apóstol Pablo, y sólo desde el siglo XIX se ha cuestionado esta idea. En la actualidad, las opiniones están divididas.

Los autores modernos partidarios de la autenticidad de la epístola se basan sobre todo en:

Quienes descartan que la epístola sea auténticamente paulina se basan en:

En la epístola hay elementos que permiten afirmar que fue escrita en prisión (cf. 4,10; 4,18). Por ello, los partidarios de la autoría de Pablo consideran que fue escrita durante alguno de los períodos de encarcelamiento del apóstol narrados en los Hechos de los Apóstoles: su primera prisión en Roma, durante la cual disfrutó de una relativa libertad para predicar (cf. Hch 28,16-28), su segundo encarcelamiento en dicha ciudad, su prisión en Cesarea Marítima (cf. Hch 23,12-27,1), o incluso en Éfeso (cf. Hch 9). En todo caso, debió ser compuesta poco antes de la Epístola a los Efesios. Quienes niegan la autoría paulina, en general, no se pronuncian sobre una fecha y lugar de composición concretos, aunque consideran que debió de ser escrita en fecha relativamente próxima a la muerte del apóstol, y, en todo caso, antes de Efesios.

Los autores que aceptan su atribución a Pablo en Roma durante su primer encarcelamiento allí, probablemente en la primavera de 57 o, según otros, en el año 62. Poco después escribió la Epístola a los efesios.

La carta va dirigida a la comunidad creyente en el mesías de la ciudad de Colosas, en Frigia, región situada en el sudoeste de Asia Menor. Colosas era una ciudad pequeña, relativamente cercana a Éfeso y Mileto. La comunidad creyente de Colosas estaba en contacto con las de otras dos localidades próximas, Hierápolis y Laodicea (cf. 4,13-16), De acuerdo con el propio texto de la epístola, la comunidad no ha sido fundada por Pablo, ya que el autor (sea Pablo o uno de sus seguidores, habla, en cualquier caso, en nombre del apóstol) afirma que ni ellos ni los de Laodicea lo han visto nunca personalmente (cf. 2,1), sino probablemente por Epafrás (cf. 1,7) compañero de Pablo cuando estuvo en la milicia.

El motivo de la epístola son las disensiones que han surgido en el seno de la comunidad a causa de la predicación de algunas personas cuyos nombres no se citan. La doctrina de estos predicadores puede reconstruirse a partir de la propia epístola. Se trata de una "filosofía" (cf. 2,8) que postula la existencia de poderes intermedios entre Dios y los hombres (en el texto llamados "principados" y "potestades", cf. 2,10), que pueden asimilarse a los ángeles. Dada la insistencia del autor de Colosenses en que únicamente en el mesías reside la plenitud de la Deidad (cf. 2,9), puede deducirse que para los predicadores de Colosas el mesías ocupaba un lugar subordinado con respecto a estos "principados" y "potestades", en 2,18 el autor de la epístola advierte explícitamente contra los poderes de las tinieblas esta filosofía prescribía además ciertas prácticas (cf. 2,20-22), relacionadas con la comida y la bebida, así como con festividades como la del novilunio y la del sábado (cf. 2,16).

Según Gabriel Pérez Rodríguez, la estructura de la epístola es la siguiente:

Esta estructura coincide con la de otras epístolas paulinas, como Romanos y Gálatas.






</doc>
<doc id="16956" url="https://es.wikipedia.org/wiki?curid=16956" title="Mirabilis">
Mirabilis

Mirabilis es la empresa de software israelí responsable de la creación del servicio de mensajería instantánea ICQ, el cual fue inmensamente popular en la década de los 90. Mirabilis fue creada en 1996 por Arik Vardi, Yair Goldfinger, Sefi Vigiser y Amnon Amir y fue comprada dos años más tarde por la firma estadounidense AOL. El actual propietario de Mirabilis es Time Warner.


</doc>
<doc id="16957" url="https://es.wikipedia.org/wiki?curid=16957" title="Jano">
Jano

Jano (Ianus), en la mitología romana, es el dios de las puertas, los comienzos, los portales, las transiciones y los finales. Por eso le fue consagrado el primer mes del año y se le invocaba públicamente el primer día de enero, mes que derivó de su nombre (que en español pasó del latín "Ianuarius" a "Janeiro" y "Janero" y de ahí derivó a "enero").

Jano es representado con dos caras, mirando hacia ambos lados de su perfil, las cuales representan el pasado y el futuro. No tiene equivalente en la mitología griega. El Janículo, colina ubicada en Roma, debe su nombre a este dios.

Dentro de los muchos apelativos que recibe el dios, vale la pena destacar dos: Jano Patulsio ("Patulsius"), que era usado para invocar la cara del dios que se ubicaba delante de la puerta por quien deseaba atravesarla (para entrar o salir). Como complemento, la cara que se le opone a esta del otro lado de la puerta, es invocada como Jano Clusivio ("Clusivius"). Ambos nombres declaran la doble funcionalidad del dios.

Según la leyenda, cuando los sabinos intentaron tomar el Capitolio, Jano hizo brotar aguas hirvientes sobre los enemigos, repeliéndolos. Por ello se le invocaba al comenzar una guerra, y mientras esta durara, las puertas de su templo permanecían siempre abiertas, con el fin de que acudiera en ayuda de la ciudad; cuando Roma estaba en paz, las puertas se cerraban.

Al igual que Prometeo, Jano es una clase de héroe cultural, ya que se le atribuye entre otras cosas la invención del dinero, la navegación y la agricultura. Según los romanos, este dios aseguraba buenos finales. En su tratado sobre los "Fastos", Ovidio caracteriza a Jano como aquel que él solo custodia el Universo. Jano es padre de Fontus, dios de las fuentes, cascadas y pozos.

En el lenguaje, "Jano" puede representar a una persona que manifiesta aspectos muy disímiles entre sí; o como alusión a la hipocresía. En este sentido este dios es citado en la novela de Albert Camus, "La caída". El médico psiquiatra argentino Hugo Marietan utiliza la imagen de Jano en su teoría para explicar la forma de relacionarse con el mundo de una persona con alguna psicopatía.



</doc>
<doc id="16959" url="https://es.wikipedia.org/wiki?curid=16959" title="Pico">
Pico

Pico hace referencia a varios artículos:








</doc>
<doc id="16960" url="https://es.wikipedia.org/wiki?curid=16960" title="Millardo">
Millardo

Un millardo es el número natural equivalente a mil millones () o, en notación científica, 10. En el Sistema Internacional de Unidades equivale al prefijo giga.
Es una palabra derivada de la francesa "milliard", que existe en la gran mayoría de los idiomas europeos (en Italia y Alemania desde el siglo XVIII), pero que no correspondía a ningún uso en España ni en la mayor parte de Hispanoamérica. La excepción está en Venezuela, donde se usa corrientemente en los periódicos de circulación nacional, como por ejemplo "El Nacional" y "El Universal", así como en todos los grandes medios de comunicación de ese país.

Esta palabra fue introducida por la Real Academia Española en el año 1995, a petición del entonces presidente de Venezuela Rafael Caldera, también miembro de la Academia Venezolana de la Lengua, y después de haber sido aprobada por la Asociación de Academias de la Lengua Española.

La razón para introducir este término fue la de impedir que la palabra estadounidense "billion" fuera traducida erróneamente como "billón" y contaminara así la numeración vigente en el español. El hecho de que no ha existido promoción alguna (la noticia salió en los medios un 28 de diciembre, día de los Santos Inocentes), y de que "mil millones" no es ambiguo y es entendible por todos, y de que además "millardo" no sea parte de la nomenclatura corriente de los números (es un caso similar a "docena", "veintena" o "centena"), explica que el término "millardo" no haya tenido difusión masiva fuera de Venezuela.

Además de haber sido incorporado al "Diccionario", se recomienda en el "Diccionario panhispánico de dudas", editado también por la Academia, frente a la traducción errónea del billón del inglés estadounidense. Sin embargo, hay que tener en cuenta que millardo no es parte del sistema de adjetivos numerales, sino que es un nombre que opera de modo similar a "docena" o "centena" y que por tanto no puede ir seguido de adjetivos numerales. Así, no es apropiado leer como «cien millardos doscientos mil», siendo lo correcto «cien mil millones doscientos mil».

En Occidente existen básicamente dos maneras de nombrar a los grandes números: la «escala larga» y la «escala corta». Ambas fueron "inventadas" (o por lo menos "teorizadas") y exportadas por Francia (como lo hizo con las unidades de peso y medida: el gramo y el metro), en dos épocas distintas.

La escala larga es la siguiente:
Esta numeración es la vigente en francés, español, alemán, neerlandés, sueco, finés, húngaro, noruego, checo, polaco, rumano y en italiano (con ciertos matices).

La escala corta es la siguiente: 
El factor entre «billón», «trillón» y los siguientes puede variar. 

Es la numeración vigente en Estados Unidos, y se ha impuesto a todos los países de habla inglesa, en ruso (exceptuando la denominación de "milliardo" para 10), y en Brasil.

Para los usos cotidianos, la diferencia entre estos dos sistemas se resume en el valor del billón: "¿un millón de millones como en España e Hispanoamérica o mil millones, como en Estados Unidos y Brasil?" Se puede defender la posición de la Academia Venezolana de la Lengua, que pretende que el empleo del "millardo" fortalezca la numeración actual, aún más sabiendo que el uso del "billón" ha variado mucho en la segunda mitad del siglo XX: 





</doc>
<doc id="16961" url="https://es.wikipedia.org/wiki?curid=16961" title="Chaos Computer Club">
Chaos Computer Club

El Club de Computación Caos ("Chaos Computer Club") es la mayor asociación de hackers de Europa. El CCC tiene su sede en Alemania y otros países de habla alemana. 

Los distintos temas de interés del Club de Computación Caos según su página web son:

El CCC fue fundado en Berlín en 12 de septiembre de 1981 en la sede del periódico "die tageszeitung", por Wau Holland y otros, anticipándose al rol que tendría la informática en la forma en que la gente vive y se comunica. Es ampliamente conocido por las demostraciones públicas de problemas de seguridad.

El Club de Computación Caos se hizo famoso mundialmente cuando hackeó la red Alemana Bildschirmtext y consiguieron que un banco en Hamburgo transfiriera 134.000 Marcos (67.000 euros) a las cuentas del club. El dinero se devolvió al día siguiente ante la prensa.

En 1989 el CCC estuvo tangencialmente envuelto en el primer caso de ciberespionaje que tuvo repercusiones internacionales. Un grupo de hackers alemanes liderados por Karl Koch (afiliado al CCC) fue arrestado por hackear servidores del gobierno de Estados Unidos y vender el código del sistema operativo al KGB soviético.

En 2011 el CCC mediante la publicación de un documento denunció al gobierno alemán de construir y lanzar a la red un troyano que permite espiar a los ciudadanos a través de sus ordenadores personales. El programa actúa como keylogger, hace capturas de pantalla y también graba audio o video.

El Ministerio del Interior alemán emitió un comunicado negando que este programa haya sido utilizado por la Oficina Federal de Policía Criminal (BKA), pero no negaba el posible uso por las policías de algunos estados y admitió que se vendió a las autoridades de Baviera en 2007. DigiTask confirmó la venta del software a las autoridades alemanas austriacas, suizas y de los Países Bajos.



</doc>
<doc id="16964" url="https://es.wikipedia.org/wiki?curid=16964" title="Alquízar">
Alquízar

Alquízar es un municipio y ciudad localizado en la provincia cubana de Artemisa. Hasta finales de 2010 perteneció a la provincia de La Habana. 

El poblado de Alquizar fue fundado en 1616, y es la población más antigua de la actual provincia de Artemisa. El municipio incluye además del pueblo de Alquízar los poblados de Pulido, Dágame y el poblado pesquero de Guanímar.

Por su situación en la sección occidental de la llanura roja Habana-Matanzas su suelo es rico y fértil y muy aprovechado para el cultivo de vegetales, tubérculos y hortalizas debido a la demanda de los mismos por el mercado de la capital de la Isla. 

Posee también una importante industria textil (Alquitex) y es la sede de la Universidad de Ciencias Pedagógicas de la Provincia "Rubén Martínez Villena", que recibe su nombre del destacado revolucionario comunista.

Del 16 de marzo de 1799 data la iglesia parroquial católica Purísima Concepción y San Agustín, construida en unos terrenos donados por Juana de la Osa, siendo su primer párroco Ambrosio de María Escobar.

El 20 de octubre de 1863 llegó el ferrocarril a Alquízar.

Entre las personalidades ilustres oriundas de Alquízar, se destacan:




</doc>
<doc id="16966" url="https://es.wikipedia.org/wiki?curid=16966" title="Distribución exponencial">
Distribución exponencial

En estadística la distribución exponencial es una distribución de probabilidad continua con un parámetro formula_1 cuya función de densidad es:

Su función de distribución acumulada es:

Donde formula_2 representa el número e.

De forma adicional esta distribución presenta una función adicional que es función Supervivencia (S), que representa el complemento de Función de distribución.

El valor esperado y la varianza de una variable aleatoria X con distribución exponencial son:

La distribución exponencial es un caso particular de distribución gamma con "k" = 1. Además la suma de variables aleatorias que siguen una misma distribución exponencial es una variable aleatoria expresable en términos de la distribución gamma.

Ejemplos para la distribución exponencial es la distribución de la longitud de los intervalos de una variable continua que transcurren entre dos sucesos, que se distribuyen según la distribución de Poisson.


Se pueden calcular una variable aleatoria de distribución exponencial formula_3 por medio de una variable aleatoria de distribución uniforme formula_4:

o, dado que formula_5 es también una variable aleatoria con distribución formula_6, puede utilizarse la versión más eficiente:

La suma de formula_7 variables aleatorias independientes de distribución exponencial con parámetro formula_8 es una variable aleatoria de distribución de Erlang.

En la hidrología, la distribución exponencial se emplea para analizar variables aleatorias extremos de variables como máximos mensuales y anuales de la precipitación diaria.


Se puede usar software y un programa de computadora para el ajuste de una distribución de probabilidad, incluyendo la exponencial, a una serie de datos:



</doc>
<doc id="16968" url="https://es.wikipedia.org/wiki?curid=16968" title="Distribución beta">
Distribución beta

En estadística la distribución beta es una distribución de probabilidad continua con dos parámetros formula_9 y formula_10 
cuya función de densidad para valores formula_11 es

Aquí formula_13 es la función gamma.

El valor esperado y la varianza de una variable aleatoria X con distribución beta son

Un caso especial de la distribución beta es cuando formula_16 y formula_17 que coincide con la distribución uniforme en el intervalo [0, 1].


</doc>
<doc id="16972" url="https://es.wikipedia.org/wiki?curid=16972" title="The Durutti Column">
The Durutti Column

The Durutti Column es una banda post-punk formada en Gran Mánchester en 1978 por el guitarrista Vini Reilly y los fundadores de Factory Records, Tony Wilson y Alan Erasmus. La banda estuvo siempre asociada con Factory, siendo una de las primeras agrupaciones que firmaron con ésta y su jefe y fundador, Wilson, quien fuera su representante durante muchos años. 

El estilo de la banda integra elementos de jazz, folk, música clásica y rock, siempre sustentados por el sonido singular y característico de la guitarra de Reilly.

Vini Reilly ha sido el único miembro permanente del grupo, desempeñando como guitarrista, pianista ocasional y, casi siempre, cantante. Desde los años 1980, la banda está integrada por Bruce Mitchell en batería. Otros miembros actuales son Keir Stewart, en bajo, teclados y armónica, y Poppy Morgan, en piano.

En 1978, Tony Wilson y Alan Erasmus, quienes iban a fundar el sello Factory Records, llamaron al baterista Chris Joyce y al guitarrista Dave Rowbotham, ambos de la banda punk Fast Breeder, para que ambos formen una banda, y adicionando luego al cantante Phil Rainford, al guitarrista Vini Reilly (ex-Ed Banger and the Nosebleeds), al bajista Tony Bowers (de Alberto Y Lost Trios Paranoias) y a Phil Rainford en voz. Este último duró poco tiempo en el grupo, partiendo en julio de 1978, dedicándose luego a producir para Nico y Suns Of Arqa. Rainford fue reemplazado por Colin Sharp.

El nombre del grupo, deriva del de la columna de milicianos anarquistas durante la guerra civil española, a cuyo frente se encontraba Buenaventura Durruti, llamada "Columna Durruti". Parece ser que Vini Reilly vio el nombre, con falta de ortografía incluida, en el póster de un grupo político situacionista inglés, y que no conocía la relación del nombre con España hasta que actuó en este país.

Al empezar sus actividades en Factory, la banda grabó dos canciones para un EP de varios artistas, llamado "A Factory Sample". Otros artistas que grabaron en el EP fueron Joy Division y Cabaret Voltaire, además del comediante John Dowie, quienes también habían firmado recientemente con Factory. Las canciones de The Durutti Column y Joy Division fueron producidas por el aclamado productor de Mánchester Martin Hannett. 

Sin embargo, después de eso, la banda sufrió cambios abruptos. Sharp, Rowbotham, Bowers y Joyce dejaron la banda para formar The Mothmen, otra agrupación post-punk. Bowers y Joyce alcanzarían el éxito años más tarde con Simply Red.

Como Reilly queda solo, llama a sus ex compañeros de The Nosebleeds, Toby Tomanov en la batería y Pete Crookes en el bajo, para colaborar en el primer álbum de la banda, "The Return Of The Durutti Column", producido también por Martin Hannett y lanzado en 1980. 

Para la grabación del siguiente disco, entra Bruce Mitchell en la batería en reemplazo de Toby. Mitchell había tenido una trayectoria más larga, habiendo vivido la época sicodélica de los años sesenta y setenta, y siendo miembro de Greasy Bear y Alberto Y Lost Trios Paranoias. Mitchell también participó en "A Factory Sample", tocando batería para John Dowie.

En noviembre de 1991, Dave Rowbotham, miembro fundador de la banda, es encontrado muerto en su apartamento en Burnage, Manchester. Había sido asesinado a golpes de hacha. Un año después, la banda Happy Mondays lanzó su álbum "Yes Please!", que contenía una canción en homenaje a él, "Cowboy Dave".

En 2007, fallece Tony Wilson, mánager de The Durutti Column durante muchos años. Wilson tenía cáncer y Reilly fue uno de los últimos en verlo con vida.

En septiembre de 2009, Colin Sharp, cantante de The Durutti Column durante la grabación de "A Factory Sample" y luego actor, profesor y escritor, fallece producto de una hemorragia cerebral. Sharp había destacado desde 2007 por escribir un libro titulado "Who Killed Martin Hannett? The Story of Factory Records' Musical Magician", que explicaba la vida del aclamado productor Martin Hannett, de quien era amigo cercano.

El 24 de enero de 2010 publicaron un disco doble en homenaje a Tony Wilson, fundador de Factory Records, al que titularon "A Paean To Wilson".







</doc>
<doc id="16978" url="https://es.wikipedia.org/wiki?curid=16978" title="Hanyū (Saitama)">
Hanyū (Saitama)

Según datos de 2003, la ciudad tiene una población estimada de 57.292 habitantes y una densidad de 978,51 personas por km². El área total es de 58,55 km².

La ciudad fue fundada el 1 de septiembre de 1954.




</doc>
<doc id="16979" url="https://es.wikipedia.org/wiki?curid=16979" title="Hasuda (Saitama)">
Hasuda (Saitama)

En japonés "hasu" (蓮) y "da" ("ta", 田) significan 'loto' y 'arrozal'. Una leyenda dice que un monje budista se alojó en un pequeño templo en una noche del año 743. Este monje, Yoshizumi (義澄), se despertaba de mañana y se sorprendía con el inesperado paisaje lleno de muchos lotos bonitos. Yoshizumi dio al templo el nombre "Renge In" (蓮華院). Renge es 'loto bonito'. La tradición dice que éste es el origen del nombre de Hasuda. Es sólo una leyenda pero en Hasuda en el pasado no faltaron los lotos, ya que se ubica en las tierras bajas de la llanura Kanto. 

Los pueblos de Ayase (綾瀬), Kurohama (黒浜) y Hirano (平野) fueron fundados en 1889. Ayase se cambió su nombre a Hasuda debido a la leyenda en 1934. Los otros dos pueblos se fusionaron con Hasuda en 1954. Una parte de Iwatsuki se trasladó a Hasuda en 1966. Hasuda obtuvo el estatus de ciudad el 1 de octubre de 1972 con una población de 35.274.

El territorio actual de Hasuda estaba demasiado cercano al río Ara en la antigedad y el medievo. Después del cambio del ruta del río en el siglo XVII del período Edo, se desarrollaron muchos arrozales de agua. En 1895 el cultivo de pera se comenzó. Las producciones de arroz y pera han continuado hasta hoy. Además, se edificaron algunas fábricas en el segunda mitad del siglo XX. Sin embargo, la población de Hasuda ha incrementado principalmente como un suburbio de Tokio y de las ciudades del sur de la prefectura de Saitama. La mayor área residencial está alrededor de la estación Hasuda de la Línea Mayor del ferrocarril Tōhoku.



</doc>
<doc id="16980" url="https://es.wikipedia.org/wiki?curid=16980" title="Hanyu">
Hanyu

Hanyu puede referirse a:


</doc>
<doc id="16983" url="https://es.wikipedia.org/wiki?curid=16983" title="Hatogaya (Saitama)">
Hatogaya (Saitama)

Según datos de 2003, la ciudad tiene una población estimada de 56.183 habitantes y una densidad de 9.032,64 personas por km². El área total es de 6,22 km².

La ciudad fue fundada el 1 de marzo de 1967.



</doc>
<doc id="16984" url="https://es.wikipedia.org/wiki?curid=16984" title="Hidaka (Saitama)">
Hidaka (Saitama)

Según datos de 2003, la ciudad tiene una población estimada de 53.597 habitantes y una densidad de 1.128,36 personas por km². El área total es de 47,50 km².

La ciudad fue fundada el 1 de octubre de 1991.



</doc>
<doc id="16995" url="https://es.wikipedia.org/wiki?curid=16995" title="Ciudad romana">
Ciudad romana

La ciudad romana es heredera directa de la griega y tuvo un desarrollo gradual e ininterrumpido durante todo el Imperio. Inicialmente tenía un desarrollo orgánico, resultado de ir añadiendo casas al núcleo original. La ciudad romana por antonomasia es Roma, la "Urbs" (Urbe).

Los romanos fundaron multitud de colonias en las tierras que dominaron y ahí apareció otro tipo de urbanismo. Tiene planta en damero, además de lo que ya tenían las viejas ciudades romanas: lugares públicos donde se reúne el pueblo para tomar las decisiones políticas y en donde divertirse, templos y palacios. Si el plano es ortogonal, no todas las calles son iguales: hay dos calles principales, que cruzan la ciudad de parte a parte: el cardo, con dirección norte-sur, y el decumano, con dirección este-oeste. El resto de las calles son más estrechas y se inscriben dentro de una de las manzanas ("insulae") en que se divide el rectángulo. Esta es la disposición de las ciudades nuevas, frecuentemente de origen militar.

La expansión del Imperio romano se tradujo en la fundación de colonias en los territorios conquistados, en los que se fundaba una nueva ciudad o "civitas". Más adelante, cuando ya dominaban extensos territorios, los romanos fundaron más ciudades por razones comerciales, defensivas o, simplemente, para asentar poblaciones. Son de planta romana Florencia, Turín y Verona en la Italia actual, Cartagena, Córdoba, Mérida, León, Barcelona, Valencia, Zaragoza, en la península ibérica, Constantinopla, Lutecia (la actual París), Narbona, Timgad, Tingis (la actual Tánger), en otras partes. 

El caso de Florencia es muy interesante porque el casco antiguo, de planta netamente ortogonal, con su cardo y decumano bien definidos, se encuentra muy bien conservado y contrasta nítidamente con los desarrollos urbanos de la Edad Media, con sus calles radiales y plano más desordenado alrededor de dicho casco central.

También en Valencia o "Valentia Edetanorum" se conserva en el subsuelo del Centro Arqueológico de l'Almoina las losas originales tanto del cardo y decumano como las trazas de unos baños y horreum del siglo II a.C. así como los pozos fundacionales del 138 a. C. y del 38 a. C., además del basamento de la doble curia de la ciudad, dos construcciones gemelas, quizás expresión arquitectónica de la singularidad jurídica de Valentia que contaba con un doble senado (veterani et veteres), en época imperial. 

Además de la herencia griega, la ciudad romana desarrolla su propia morfología. Los romanos tratarán de hacer del entorno urbano un lugar digno para vivir, por lo que son necesarios el alcantarillado, la traída de aguas (acueductos), las fuentes, los puentes, las termas, los baños, el pavimento, el servicio de incendios y de policía, los mercados y todo aquello que es necesario para que viva la gente lejos del campo y con todos los refinamientos posibles para mejorar la salud pública.

Había edificios públicos para el gobierno, el culto y la diversión: los palacios, templos, foros, basílicas, teatros, anfiteatros, circos, mercados, baños, etc.; todos ellos construidos de nueva planta. Además, había motivos de adorno y conmemoración como las columnas y los arcos de triunfo.

El resto de la ciudad estaba ocupada por viviendas. Los ricos vivían en una casa unifamiliar que se llamaba "domus". Los más humildes habitaban en casas de pisos, llamadas "insulae" (islas).

De lo que en principio carecieron estas ciudades fue de muralla, ya que el poderío del Imperio servía para disuadir los intentos de atacar los núcleos urbanos. Hasta que comenzaron las invasiones germánicas, en el siglo III, las ciudades no se amurallaron, se colmataron y la calidad de la vida urbana descendió. Esto fue un golpe mortal para una civilización urbana como la romana. Las ciudades se convirtieron en lugares congestionados y poco saludables, y que en épocas de peligro no podían proporcionar a sus habitantes los productos básicos; así que los señores hacendados comenzaron a construir casas en el campo, las villas romanas, que se procuraban todo lo que necesitaban y se defendían a sí mismas. Fue el comienzo de la Edad Media: la sociedad se ruralizó y la economía se feudalizó.




</doc>
<doc id="17017" url="https://es.wikipedia.org/wiki?curid=17017" title="IPv4">
IPv4

El Protocolo de Internet versión 4 (, IPv4), es la cuarta versión del "Internet Protocol" (IP), un protocolo de interconexión de redes basados en Internet, y que fue la primera versión implementada en 1983 para la producción de ARPANET. Definida en el RFC 791, el IPv4 usa direcciones de 32 bits, limitadas a formula_1 = 4 294 967 296 direcciones únicas, muchas las (LAN). Por el crecimiento enorme que ha sea hace todo esto seguridad electrónico y automatización combinado con el hecho de que hay desperdicio de direcciones en muchos casos (consultar las secciones que siguen), ya hace varios años se observó que escaseaban las direcciones IPv4.

Esta limitación ayudó a estimular el estudio sobre la factibilidad de implantación de un nuevo protocolo IPv6, que en el año 2016 ya está en las primeras fases de pruebas, y que se espera que termine reemplazando a actual protocolo IPv4.

Véase que las direcciones disponibles en la reserva global de IANA pertenecientes al protocolo IPv4 se agotaron oficialmente el lunes 31 de enero de 2011. Los Registros Regionales de Internet deben, desde ahora, manejarse con sus propias reservas, que se estima, alcanzarán hasta el año 2020, y no por mucho más tiempo.

El IPv4 utiliza direcciones de 32 bits que limitan el espacio de direcciones a (2) direcciones posibles. 

El IPv4 (Protocolo de Internet versión 4) reserva bloques de direcciones especiales para redes privadas (en total direcciones, o sea, (2), así como direcciones de multidifusión ( direcciones, o sea, 2).

Las direcciones IPv4 pueden representarse en cualquier notación que exprese un valor entero de 32 bits. La mayoría de las veces se escriben en la notación decimal, la que consta de cuatro octetos de la dirección expresada individualmente en números decimales, y separados uno del siguiente por puntos. 

Por ejemplo, la dirección IP de cuatro puntos 192.0.2.235 representa el número decimal de 32 bits 3221226219, que en formato hexadecimal es 0xC00002EB. Esto también puede expresarse en formato hexadecimal de puntos como 0xC0.0x00.0x02.0xEB, o con valores en formato octal como 0300.0000.0002.0353.

La notación CIDR combina la dirección con su prefijo de enrutamiento en un formato compacto, en el que a la dirección le sigue un carácter de barra (/) y el conteo de 1 bits consecutivos en el prefijo de enrutamiento (máscara de subred).

En el diseño original de IPv4, una dirección IP se dividió en dos partes: el identificador de red era el octeto más significativo de la dirección, y por su parte, el identificador de host (anfitrión o huésped) era el resto de la dirección. Este último también fue llamado el campo de descanso. Esta estructura permitía un máximo de 256 identificadores de red, que rápidamente se encontró que eran inadecuados.

Para superar este límite, el octeto de dirección más significativo se redefinió en 1981 para crear clases de red, en un sistema que más tarde se conoció como redes con clase. El sistema revisado definió cinco clases. Las clases A, B y C tenían diferentes longitudes de bits para la identificación de la red. El resto de la dirección se usó como anteriormente para identificar un host dentro de una red. Debido a los diferentes tamaños de campos en diferentes clases, cada clase de red tenía una capacidad diferente para direccionar a sus huéspedes. Además de las tres clases para direccionar hosts, la Clase D se definió para el direccionamiento de multidifusión, y la Clase E se reservó para aplicaciones futuras.

La división de las redes con clase existentes en subredes comenzó en 1985 con la publicación del RFC 950. Esta división se hizo más flexible con la introducción de máscaras de subred de longitud variable (VLSM) en el RFC 1109 en 1987. En 1993, basado en este trabajo, el RFC 1517 introdujo el "Classless Inter-Domain Routing" (CIDR), que expresa el número de bits (de los más significativos) como, por ejemplo, /24, y el esquema basado en clases se denominaba con clase, en contraste. El CIDR fue diseñado para permitir la repartición de cualquier espacio de direcciones, de modo que se pudieran asignar bloques de direcciones más pequeños o más grandes a los distintos usuarios. La estructura jerárquica creada por el CIDR fue administrada por la Autoridad de Números Asignados de Internet (IANA) y los registros regionales de Internet (RIR). Cada RIR mantiene una base de datos WHOIS de búsqueda pública, la que proporciona información sobre las asignaciones de direcciones IP.

El Grupo de Trabajo de Ingeniería de Internet (IETF) y la Autoridad de Números Asignados de Internet (IANA) han restringido el uso general de varias direcciones IP reservadas para fines especiales. En particular, estas direcciones se utilizan para el tráfico de multidifusión y para proporcionar espacio de direccionamiento para usos no restringidos en redes privadas.<section begin="Direcciones de uso especial" />

<section end="Direcciones de uso especial" />

De los aproximadamente cuatro mil millones de direcciones definidas en IPv4, cerca de 18 millones de direcciones en tres rangos están reservadas para su uso en redes privadas. Las direcciones de paquetes en estos rangos no son enrutables en la Internet pública; son ignorados por todos los enrutadores públicos. Por lo tanto, los hosts privados no pueden comunicarse directamente con las redes públicas y requieren la traducción de direcciones de red en una puerta de enlace de enrutamiento para este propósito.
<section begin="RedesPrivadasIPv4" />
<section end="RedesPrivadasIPv4" />

Dado que dos redes privadas, por ejemplo, dos sucursales, no pueden interoperar directamente a través de la Internet pública, las dos redes deben conectarse a través de Internet a través de una red privada virtual (VPN) o un túnel IP, que encapsula los paquetes, incluidos sus encabezados que contienen el Direcciones privadas, en una capa de protocolo durante la transmisión a través de la red pública. Además, los paquetes encapsulados se pueden cifrar para que la transmisión a través de redes públicas asegure los datos.

La <nowiki>RFC 3927</nowiki> define el bloque de dirección especial 169.254.0.0/16 para el direccionamiento de enlace-local. Estas direcciones solo son válidas en enlaces (como un segmento de red local o conexión punto a punto) conectados a un host. Estas direcciones no son enrutables. Al igual que las direcciones privadas, estas direcciones no pueden ser el origen o destino de los paquetes que atraviesan Internet. Estas direcciones se utilizan principalmente para la configuración automática de direcciones (Zeroconf) cuando un host no puede obtener una dirección IP de un servidor DHCP u otros métodos de configuración interna.

Cuando se reservó el bloque de direcciones, no existían estándares para la configuración automática de direcciones. Microsoft creó una implementación llamada direccionamiento IP privado automático (APIPA), que se implementó en millones de máquinas y se convirtió en un estándar de facto. Muchos años después, en mayo de 2005, el IETF definió un estándar formal en <nowiki>RFC 3927</nowiki>, titulado Configuración dinámica de direcciones de enlace local IPv4.

La red de clase A 127.0.0.0 (red sin clase 127.0.0.0/8) está reservada para "loopback". Los paquetes IP cuyas direcciones de origen pertenecen a esta red nunca deben aparecer fuera de un host. El modus operandi de esta red se expande sobre el de una interfaz de "loopback":


Es posible que las redes con máscaras de subred de al menos 24 bits, es decir, redes Clase C en redes con clase, y redes con sufijos CIDR /24 a /30 (255.255.255.0–255.255.255.252) no tengan una dirección que termine en 0 o 255.

El direccionamiento con clase prescribió solo tres posibles máscaras de subred: Clase A, 255.0.0.0 o /8; Clase B, 255.255.0.0 o /16; y Clase C, 255.255.255.0 o /24. Por ejemplo, en la subred 192.168.5.0/255.255.255.0 (192.168.5.0/24) el identificador 192.168.5.0 se usa comúnmente para referirse a la subred completa. Para evitar la ambigüedad en la representación, la dirección que termina en el octeto 0 está reservada.

Una dirección multidifusión es una dirección que permite que la información se envíe a todas las interfaces en una subred determinada, en lugar de a una máquina específica. En general, la dirección de difusión se encuentra obteniendo el complemento de bits de la máscara de subred y realizando una operación OR a nivel de bits con el identificador de red. En otras palabras, la dirección de transmisión es la última dirección en el rango de direcciones de la subred. Por ejemplo, la dirección de transmisión para la red 192.168.5.0 es 192.168.5.255. Para redes de tamaño /24 o más, la dirección de transmisión siempre termina en 255.

Sin embargo, esto no significa que todas las direcciones que terminen en 0 o 255 no puedan usarse como una dirección de host. Por ejemplo, en la subred /16 192.168.0.0/255.255.0.0, que es equivalente al rango de direcciones 192.168.0.0–192.168.255.255, la dirección de transmisión es 192.168.255.255. Se pueden usar las siguientes direcciones para los hosts, aunque terminen con 255: 192.168.1.255, 192.168.2.255, etc. Además, 192.168.0.0 es el identificador de red y no debe asignarse a una interfaz. Las direcciones 192.168.1.0, 192.168.2.0, etc., pueden asignarse, a pesar de terminar con 0.

En el pasado, surgía un conflicto entre las direcciones de red y las direcciones de difusión porque algunos programas utilizaban direcciones de difusión no estándar con ceros en lugar de unos.

En redes más pequeñas que /24, las direcciones de difusión no terminan necesariamente con 255. Por ejemplo, una subred CIDR 203.0.113.16/28 tiene la dirección de difusión 203.0.113.31.

Los hosts en Internet generalmente se conocen por sus nombres, por ejemplo, www.example.com, no principalmente por su dirección IP, que se usa para el enrutamiento y la identificación de la interfaz de red. El uso de nombres de dominio requiere la traducción, llamada resolución, a direcciones y viceversa. Esto es análogo a buscar un número de teléfono en una guía telefónica con el nombre del destinatario.

La traducción entre las direcciones y los nombres de dominio se realiza mediante el Sistema de nombres de dominio (DNS), un sistema de nombres jerárquico y distribuido que permite la subdelegación de espacios de nombres a otros servidores DNS.

El Protocolo de Internet (IP) permite a las redes comunicarse unas con otras. El diseño acomoda redes de naturalezas físicas diversas; es independiente de la tecnología usada en la capa inmediatamente inferior, la Capa de Enlace. Las redes con diferente hardware difieren usualmente no solo en velocidad de transmisión, sino que también en su Unidad Máxima de Transmisión (MTU). Cuando una red quiere transmitir datagramas a una red con un MTU inferior, debe fragmentar sus datagramas. En IPv4, esta función es realizada en la capa de Intenet, y es llevada a cabo en routers IPv4, los cuales solo requieren esta capa como la más alta implementada en su diseño.

En contraposición, IPv6, la nueva generación del Protocolo de Internet, no permite a los routers a llevar a cabo dicha fragmentación; los hosts son los que determinan el MTU antes de enviar datagramas.

Cuando un enrutador recibe un paquete, este examina la dirección de destino y determina la interfaz de salida a utilizar y el MTU de ella. Si el tamaño del paquete es mayor que el MTU y el bit de No Fragmentación (DF) es 0 en la cabecera del paquete, el enrutador tendrá que fragmentar dicho paquete.

El enrutador divide el paquete en fragmentos. El tamaño máximo de cada fragmento es el MTU menos el tamaño de la cabecera IP (entre 20 y 60 bytes). El enrutador pone cada fragmento dentro de su paquete. Estos fragmentos reciben los siguientes cambios:
Por ejemplo, para un MTU de 1500 bytes y un tamaño de cabecera de 20 bytes, los offsets del fragmentos serían múltiplos de (1500-20)/8 = 185. Estos múltiplos son 0,370,555,740…

Es posible que un paquete sea fragmentado en un enrutador y estos a su vez sean fragmentados en otro enrutador. Por ejemplo, supongamos una Capa de Transporte con un tamaño de 4500 bytes, sin opciones, y un tamaño de cabecera IP de 20 bytes. Así, el tamaño de paquete sería de 4520 bytes.

Asumiendo que el paquete viaja en un enlace con un MTU de 2500 bytes, quedaría algo talque así:

Observar que los fragmentos conservan el tamaño de datos: 2480 + 2020 = 4500 Bytes.

Observar también cómo averiguar los offsets del tamaño de datos:
Asumiendo que estos fragmentos alcanzan un enlace con un MTU de 1500 bytes. Cada fragmento se convertiría en dos fragmentos:

Observar que los fragmentos conservan el tamaño de datos:
Observar también que el bit de “Más Fragmentos” permanece a 1 para todos los fragmentos que vinieron con dicho 1 y que al llegar al último fragmento, dicho bit se establecerá a 0. Por supuesto, el campo de Identificación continúa con el mismo valor en todos los fragmentos refragmentados. De esta forma, incluso si los fragmentos son re-fragmentados, el receptor sabe que inicialmente todos empezaron en el mismo paquete.

Observar cómo conseguimos los offsets de los tamaños de datos:

Podemos utilizar el último offset y el último tamaño de datos para calcular el tamaño total: 495*8 + 540 = 4500

Un receptor sabe que un paquete es un fragmento si se cumple al menos una de las siguientes condiciones:
El receptor identifica fragmentos coincidentes utilizando direcciones locales y foráneas, el protocolo ID y el campo Identificación. El receptor reensamblará los datos de fragmentos con el mismo ID utilizando tanto el offset del fragmento como la bandera de “Más Fragmentos”. Cuando el receptor recibe el último fragmento (que tiene la bandera de “Más Fragmentos” a 0), puede calcular la longitud de la carga útil de datos, multiplicando el offset del último fragmento por 8 y añadiendo su tamaño de datos también. En el ejemplo superior, este cálculo es de 495 x 8 + 540 = 4500 Bytes.

Cuando el receptor tiene todos los fragmentos, puede colocarlos de nuevo en el orden correcto utilizando los offsets para ello.

Será entonces cuando puede pasar sus datos a la pila para su posterior proceso.

Las direcciones IPv4 se pueden escribir de forma que expresen un entero de 32 bits, aunque normalmente se escriben con decimales separados por puntos. A estos números decimales de 3 dígitos se les llama "octetos", porque en binario requieren de 8 dígitos (8 bits) para ser representados. La siguiente tabla muestra varias formas de representación de direcciones IPv4:

El desperdicio de direcciones IPv4 se debe a varios factores.

Uno de los principales es que inicialmente no se consideró el enorme crecimiento que iba a tener Internet; se asignaron bloques de direcciones grandes (de 16 271 millones de direcciones) a países, e incluso a empresas.

Otro motivo de desperdicio es que en la mayoría de las redes, exceptuando las más pequeñas, resulta conveniente dividir la red en subredes. Dentro de cada subred, la primera y la última dirección no son utilizables; de todos modos no siempre se utilizan todas las direcciones restantes. Por ejemplo, si en una subred se quieren acomodar 80 "hosts", se necesita una subred de 128 direcciones (se debe redondear a la siguiente potencia en base 2), en este ejemplo, las 48 direcciones IP restantes ya no se utilizan.



</doc>
<doc id="17029" url="https://es.wikipedia.org/wiki?curid=17029" title="Compuesto orgánico">
Compuesto orgánico

Compuesto orgánico o molécula orgánica es un compuesto químico que contiene carbono, formando enlaces carbono-carbono y carbono-hidrógeno. En muchos casos contienen oxígeno, nitrógeno, azufre, fósforo, boro, halógenos y otros elementos menos frecuentes en su estado natural. Estos compuestos se denominan moléculas orgánicas. Algunos compuestos del carbono, carburos, los carbonatos y los óxidos de carbono, no son moléculas orgánicas. La principal característica de estas sustancias es que arden y pueden ser quemadas (son compuestos combustibles). La mayoría de los compuestos orgánicos se producen de forma natural, pero también existen artificiales los cuales son creados mediante síntesis química.

Las moléculas orgánicas se dividen en dos partes:


La línea que divide las moléculas orgánicas de las inorgánicas ha originado polémicas e históricamente ha sido arbitraria, pero generalmente, los compuestos orgánicos tienen carbono con enlaces de hidrógeno, y los compuestos inorgánicos, no. Así el ácido carbónico es inorgánico, mientras que el ácido fórmico, el primer ácido carboxílico, es orgánico. El anhídrido carbónico y el monóxido de carbono, son compuestos inorgánicos. Por lo tanto, todas las moléculas orgánicas contienen carbono, pero no todas las moléculas que contienen carbono son moléculas orgánicas. calcio

La etimología de la palabra «orgánico» significa que procede de órganos, relacionado con la vida; en oposición a «inorgánico», que sería el calificativo asignado a todo lo que carece de vida. Se les dio el nombre de «orgánicos» en el siglo XIX, por la creencia de que sólo podrían ser sintetizados por organismos vivos. La teoría de que los compuestos orgánicos eran fundamentalmente diferentes de los «inorgánicos», fue refutada con la síntesis de la urea, un compuesto «orgánico» por definición ya que se encuentra en la orina de organismos vivos, síntesis realizada a partir de cianato de potasio y sulfato de amonio por Friedrich Wöhler (síntesis de Wöhler). Los compuestos del carbono que todavía se consideran inorgánicos son los que ya existen.

La clasificación de los compuestos orgánicos puede realizarse de diversas maneras, atendiendo a su origen (natural o sintético), a su estructura (p.ejm.: alifático o aromático), a su funcionalidad (por ejemplo:alcoholes o cetonas), o a su peso molecular (p.ejem.: monómeros o polímeros).

Los compuestos orgánicos pueden dividirse de manera muy general en:


La clasificación por el origen suele englobarse en dos tipos: natural o sintético. Aunque en muchos casos el origen natural se asocia a el presente en los seres vivos no siempre ha de ser así, ya que la síntesis de moléculas orgánicas cuya química y estructura se basa en el carbono, también se sintetizan "ex-vivo", es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Hale-Bopp.

Los compuestos orgánicos presentes en los seres vivos o "biosintetizados" constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en bioquímica, medicina, farmacia, perfumería, cocina y muchos otros campos más.

Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados "azúcares" pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa).
Se suelen clasificar según su grado de polimerización en:

Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se los llama incorrectamente grasas, ya que las grasas son sólo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides).

Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así "poliamidas naturales" ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia importantísima de moléculas en los seres vivos pero en especial en el reino animal. Ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña.

Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por las partículas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato.Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. (Ver artículo "Ácidos nucleicos").

Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran "pequeñas" aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina.

Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales.

El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos.

En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp.Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final.

Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras textiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos.
Los hidrocarburos son compuestos químicos formados por átomos de carbono (C) y de Hidrógeno (H). El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de los enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser:

Los radicales o grupos alquilo son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono met-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente...) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el "metilpropano". En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el "5-metil, 2-etil, 8-butil, 10-docoseno".

Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos, azo, nitro o sulfóxido, entre otros.

Son cadenas de carbonos con uno o varios átomos de oxígeno y pueden ser:


El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras.


Es decir, el grupo carbonilo H-C=O está unido a un solo radical orgánico.
El grupo funcional carbonilo consiste en un átomo de carbono unido con un doble enlace covalente a un átomo de oxígeno.
El tener dos átomos de carbono unidos al grupo carbonilo, es lo que lo diferencia de los ácidos carboxílicos, aldehídos, ésteres. El doble enlace con el oxígeno, es lo que lo diferencia de los alcoholes y éteres. Las cetonas suelen ser menos reactivas que los aldehídos dado que los grupos alquílicos actúan como dadores de electrones por efecto inductivo.



Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol.

Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4"n"+2 electrones en orbitales π (n=0,1,2...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en:

Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química pero distintas estructuras y propiedades.
Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional.
El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula CH puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula CHO.

Los compuestos orgánicos pueden ser obtenidos por purificación a partir de organismos o del petróleo y por síntesis orgánica.

La mayoría de los compuestos orgánicos puros se producen hoy de forma artificial, aunque un subconjunto importante todavía se extrae de fuentes naturales porque sería demasiado costosa su síntesis en laboratorio. Estos últimos son utilizados en reacciones de semi-síntesis.

El análisis estadístico de estructuras químicas se llama informática química. La base de datos de Beilstein contiene una amplia colección de compuestos orgánicos. Un estudio informático que implicaba 5,9 millones de sustancias y 6,5 millones de reacciones, demostró que el universo de compuestos orgánicos consiste en una base de alrededor de 200.000 moléculas muy relacionadas entre sí y de una periferia grande (3,6 millones de moléculas) a su alrededor. La base y la periferia están rodeadas por un grupo de pequeñas islas no-conectadas que contienen 1,2 millones de moléculas, un modelo semejante al www.

Más estadísticas:



</doc>
<doc id="17033" url="https://es.wikipedia.org/wiki?curid=17033" title="Ciclohexano">
Ciclohexano

El ciclohexano es un cicloalcano (o hidrocarburo alicíclico) formado por 6 átomos de carbono, y 12 átomos de hidrógeno, por lo que su fórmula es CH. La cadena de carbonos se encuentra cerrada en forma de anillo. Es un disolvente apolar muy utilizado con solutos del mismo tipo.
Se obtiene de la ciclación de compuestos alifáticos, o de la reducción del benceno con hidrógeno a altas presiones en presencia de un catalizador. Se funde al llegar a los 7°C.
Una de sus aplicaciones más importantes es la producción del nailon (nylon). 

Conformación del ciclohexano


</doc>
<doc id="17045" url="https://es.wikipedia.org/wiki?curid=17045" title="Neuroembriología">
Neuroembriología

La neuroembriología es la ciencia que estudia el desarrollo embrionario del sistema nervioso.

Dentro del desarrollo embrionario, el sistema nervioso es el primero que comienza a diferenciarse. 

Durante la tercera semana del desarrollo la capa dorsal del disco embrionario (ectodermo), se engrosa para dar origen a la placa neural. El conjunto tiene forma de una zapatilla, piriforme, con la extremidad craneal más ancha que la caudal.
Esa placa neural se hunde originando el surco neural y, a cada lado del surco, se originan los pliegues neurales, los cuales se unen en la línea media a nivel de la cuarta somita para dar origen al tubo neural. El tubo neural se comunica con la cavidad amniótica a través de los neuroporos craneal y caudal. El neuroporo craneal cierra dos días antes (hacia el día 24º) que el neuroporo caudal.

El tubo neural queda introducido en el mesodermo, que lo rodea (situado por encima del endodermo, y por el ectodermo de revestimiento. Entre éste y el tubo neural se forma una hilera de células ectodérmicas: las células de la cresta neural que da origen a los ganglios de los nervios espinales y craneales, a los ganglios vegetativos, a las células de la médula suprarrenal y a los melanocitos. También origina otras estructuras de la cabeza: parte del esqueleto, iris, meninges blandas (leptomeninges), etc.

El extremo posterior del tubo neural da origen a la médula espinal. En cambio, el extremo anterior crece mucho más rápido y da origen a las tres vesículas cerebrales primitivas: anterior (prosencéfalo), media o mesencefálica, y posterior (rombencéfalo).


</doc>
<doc id="17049" url="https://es.wikipedia.org/wiki?curid=17049" title="Concepción (Chile)">
Concepción (Chile)

Concepción es una comuna y de Chile, centro geográfico y demográfico del área metropolitana del Gran Concepción, y capital de la provincia homónima y de la Región del Biobío. El núcleo urbano de Concepción ejerce un significativo impacto en el comercio nacional al ser parte de una de las regiones con mayor industrialización del país, y es una de las tres urbes chilenas más pobladas, junto con la de Valparaíso, después de Santiago. A nivel comunal, limita al norte con Hualpén, Talcahuano y Penco; al sur con Chiguayante y Hualqui; al este con la comuna de Florida y al oeste con el río Biobío y la comuna de San Pedro de la Paz.

La ciudad fue emplazada, en primera instancia, en la Bahía de Concepción, en el territorio que después se convirtió en la comuna de Penco, actualmente parte de la conurbación de Concepción. El gentilicio de la ciudad, «penquista», hace alusión al lugar de su fundación original. El centro y casco histórico de la ciudad se encuentra en el Valle de la Mocha, donde se trasladó luego de ser destruida por un terremoto.

El origen de Concepción se remonta a 1550, cuando fue fundada por Pedro de Valdivia a título de la corona española, bajo el nombre de "Concepción de María Purísima del Nuevo Extremo", y fue la capital del Reino de Chile entre 1565 y 1573, manteniéndose luego como el centro militar y político del reino por todo el resto del periodo colonial chileno. Posteriormente, formó parte del primer intento independentista de Chile en 1810, a manos del abogado Juan Martínez de Rozas. Ha sido el centro de la conurbación más poblada del sur del país desde inicios del siglo XX, y posee como símbolo cultural la Plaza de la Independencia, lugar donde se llevó a cabo la declaración solemne sobre la liberación chilena del realismo español.

Concepción concentra su actividad en el área de servicios y funciona como el centro financiero de toda la metrópoli. Es conocida por ser una ciudad universitaria, ya que posee numerosas instituciones educacionales, entre las que destacan la Universidad de Concepción, la Universidad del Bío-Bío, y la Universidad Católica de la Santísima Concepción. La comuna alberga además variados puentes históricos, murales, parques y lagunas, así como importantes centros culturales como el Teatro Biobío, la Casa del Arte, el Museo de Historia Natural y el Teatro Universidad de Concepción. Dentro de su oferta turística, también destaca la variedad de bares y locales de entretenimiento que le brinda una activa vida nocturna a la ciudad.

Su nombre se debe a su fundador, el conquistador español Pedro de Valdivia, quien se encomendó a la Virgen de la Inmaculada Concepción con el fin de ganar la Guerra de Arauco y conquistar Chile.

Cuando, el 5 de octubre de 1550, Pedro de Valdivia fundó la ciudad, este habría decidido, como manera de homenaje a la Virgen, bautizarla como «La Concepción de María Purísima del Nuevo Extremo».

Concepción ha tenido dos emplazamientos. La zona fue descubierta por el Capitán Juan Bautista Pastene a bordo de su buque «San Pedro» el 27 de septiembre de 1544. Su primer avistamiento fue la boca del río Biobío, sitio que toma de manera simbólica por el nombre «la provincia de Concepción». La toma de posesión fue en buque, ya que un temporal impidió el desembarco.

El primer emplazamiento de la ciudad se debió a las campañas de conquista en 1546 de Pedro de Valdivia. Su primera interacción con la comarca se dio al llegar a las orillas del río Bíobío junto a la bahía que después llamaría Bahía de Concepción. A su inmediata llegada se enfrentó con el pueblo mapuche que residía en la zona, librándose la Batalla de Quilacura. En apuntes de Góngora Marmolejo, en la batalla participaron más de 80 000 personas. Esta es la primera de muchas batallas que se librarían en el sector como parte de la llamada Guerra de Arauco.

Habiendo sido repelido el ataque araucano, pero vencidas sus tropas por los nativos, Valdivia regresó a Santiago de Nueva Extremadura, pero cuatro años más tarde realizó una nueva expedición, adentrándose hasta la altura de Andalién con una comitiva conformada por alrededor de doscientos soldados y un grupo de nativos. El 22 de febrero de ese año, en la Batalla de Andalién, los mapuches atacaron la hueste en un intento por repeler la conquista. Según cuenta Daniel de la Vega, cronista de la época:
Tres días más tarde Valdivia traslada su campamento a orillas del mar esperando buques de refuerzo venidos de Valparaíso. El sitio de campamento era denominado por los indígenas como «Penguco» o «Penco», lo que dio origen al gentilicio, «penquista».

El 3 de marzo de 1550 se trazó el plano de la ciudad, se repartieron los solares y se dio inicio a las primeras construcciones. El 5 de octubre de 1550 se decretó oficialmente la fundación de la "ciudad de Concepción del Nuevo Extremo" y se procedió a formar su respectivo cabildo.

Dos años después, en 1552, el asentamiento fue reconocido como ciudad a través de una que le otorga un escudo de armas que aún se encuentra en vigencia.

Durante el periodo colonial de Chile, en La Concepción se desarrolló la Guerra de Arauco, en la cual se transformó en el núcleo militar de este enfrentamiento al estar ubicada en la frontera hispano-mapuche. La ciudad era constantemente despoblada y repoblada, y sufría asedios y ataques militares. En sus primeros 10 años de vida fue destruida tres veces por el pueblo araucano.

En los siglos XVI y XVII, fue atacada por piratas y corsarios ingleses y neerlandeses como Oliver Van Noort y Francis Drake y también por araucanos como Caupolicán, Lautaro en el siglo XVI. En 1554 Francisco de Villagra toma el mando tras la batalla de Tucapel en un intento por detener los ataques de los nativos pero no logra buenos resultados. Su sucesor, García Hurtado de Mendoza, consiguió en 1558 repoblar Concepción y obtuvo éxitos militares para España en la Araucanía. España no logra mayor ventaja hasta el Sitio de Concepción donde muere el cacique Caupolicán. Durante este periodo Concepción se convirtió en una industria más militar.

Al año siguiente, 1565, Concepción se convirtió en el asiento de la Real Audiencia, el principal tribunal de justicia de la Corona española en el Reino de Chile.

En 1589, Alonso de Ercilla asienta paso en la región para escribir parte de La Araucana, poema épico que relata la historia de la Guerra de Arauco y la época contemporánea de Concepción.

Ya en 1604 se crea entonces el primer ejército profesional de Chile para defender la ciudad logrando así mejores resultados.

Se construye en los años sucesivos, con el fin de proporcionar mejor defensa a la ciudad, el Fuerte de Penco. También se edificó la Catedral de Concepción en 1730.

El terremoto de 1751, de 8.5 grados Richter, y su respectivo maremoto marcaron la nueva fundación de Concepción en su sitio actual, en el Valle de la Mocha, dejando atrás sus restos en la ahora comuna de Penco. El traslado fue aplazado 14 años, por la oposición de un influyente grupo de vecinos, encabezado por el obispo José de Toro y Zambrano Romo. La muerte del obispo, y el hecho de que su reemplazante estuviese de acuerdo con el traslado, facilitaron su realización, llevada a cabo entre enero y marzo de 1765.

Mientras Concepción se traslada a su actual ubicación, fue prohibida la ocupación territorial de Penco, por una disposición de la administración colonial. 

El 20 de febrero de 1835, a las 11:30, tuvo lugar un terremoto de 8,5 M que afectó severamente a la ciudad. El maremoto posterior arrasó el área ubicada entre los ríos Cachapoal y Valdivia.Destruyó totalmente la ciudad de Concepción. El sismo y sus efectos fueron descritos por Charles Darwin, quien casualmente se encontraba en la zona.

En 1842 la zona de Penco volvió a poblarse, y en ese mismo año el Estado derogó la prohibición de su poblamiento, denominándola Villa Penco, dependiente de la municipalidad de Concepción. En 1898 Penco fue declarada ciudad autónoma.

Desde su fundación, Concepción ha sido una de las ciudades más grandes de Chile, teniendo un importante papel en el desarrollo económico, administrativo y militar del país. Los puertos de Talcahuano, Tomé, San Vicente y Lirquén, que están próximos a la ciudad, la hicieron un centro de exportaciones y, antiguamente, un lugar de llegada de numerosos inmigrantes cuando las costas chilenas eran ruta obligada para los barcos que venían desde Europa.

En el siglo XIX, Concepción impulsó en Chile varias revueltas político-sociales. Juan Martínez de Rozas lideró a los exaltados, tomó el control de la Primera Junta Nacional de Gobierno y creó el congreso nacional. También promocionó su posición y conformó las tres primeras provincias de Chile.

Formó la juventud de Ramón Freire e impulsó las revueltas sociales que impidieron el monopolio de la ciudad de Santiago durante la Patria Vieja, la Patria Nueva y la época de los ensayos constitucionales. El general Manuel Bulnes también nació en esta época.

En el siglo XX, Concepción se transformó en una cuna de ideales políticos y culturales, llevados a cabo gracias a los grandes capitales derivados de la fiebre del oro, la activación del salitre y la extracción de carbón en Lota. Se construye el Liceo Enrique Molina y, en 1919, se funda la Universidad de Concepción.

En abril de 1987, el papa Juan Pablo II, en el marco de su visita a Chile, estuvo en Concepción durante dos días. Fidel Castro también visitó la ciudad en su momento. El presidente Pedro Aguirre Cerda logra ganar fama y apoyo político desde Concepción. Con Juan Antonio Ríos la situación fue similar.

En 1996, la comuna de Concepción es administrativamente dividida, creándose las comunas de Chiguayante y San Pedro de la Paz, que se convirtieron en ciudades dormitorio de Concepción, y más tarde se conurbaron con la misma.

El crecimiento de Concepción ha sido especialmente rápido a partir de la segunda mitad del siglo XX, llegando a fusionarse con otras localidades de la zona como la ciudad de Talcahuano, y las comunas de San Pedro de la Paz, Chiguayante y Hualpén, esta última separada de Talcahuano en 2004.

La mayoría de los hitos urbanos de Concepción se dan en esta época: la Plaza de la Independencia, la Catedral de la Santísima Concepción, el Mercado Central de la ciudad, el Museo de Historia Natural, la Galería de la Historia, el Hospital regional Guillermo Grant Benavente, Casa del Arte y su mural, todos fueron desarrollados en el siglo pasado.

Recientes cambios se han agregado a la estructura urbanista con la incorporación de varios proyectos, como Biovías y el Proyecto Bicentenario. Actualmente, la ciudad y conurbación vive una explosión demográfica, cultural e inmobiliaria que ha experimentado, y que la mantiene como una de las urbes más importantes de Chile.

El 27 de febrero de 2010, se produjo un terremoto de magnitud 8,8 en la escala de Richter, con epicentro a 90 kilómetros al noroeste de la ciudad, causando numerosos daños materiales y un total de 524 muertos. Sin embargo, las consecuencias sociales del sismo fueron casi tan devastadoras como el mismo evento natural, en lo que se conoce como el «terremoto social». Esto, debido a que en las horas y días posteriores al gran sismo, se registraron numerosos saqueos y robos a supermercados, grandes tiendas, casas particulares y todo tipo de almacenes, donde se registraron hurtos de víveres.

La ciudad de Concepción posee un importante desarrollo económico y demográfico, teniendo desde su fundación una gran relevancia histórica, social y cultural en el país. Ha llegado a ser considerada como uno de los núcleos urbanos, demográfico, administrativo, financiero y comercial más relevantes de Chile, junto a Santiago y Valparaíso.

Si bien la ciudad ya no posee la importancia que alcanzó a tener como sede del Ejército del Reino de Chile y de la Real Audiencia a comienzos de la Guerra de Arauco, hoy es el núcleo de una de las conurbaciones más grandes del país, y la más poblada del sur de Chile, alcanzando una población alrededor del millón de habitantes.

En lo cultural Concepción ha sido, en ciertos períodos, un importante polo de desarrollo, particularmente desde la fundación de la Universidad de Concepción en 1919. Al alero de esta última se crearon, por ejemplo, instituciones tan importantes como el Teatro de la Universidad de Concepción (TUC), escuela y compañía de teatro que funcionó entre los años 1945 y 1973, y que llegó a ser considerada como una de las mejores del país; la Revista Atenea, la cual desde 1924 tiene como objetivo difundir la investigación y la reflexión crítica en el ámbito cultural chileno y latinoamericano, especialmente relacionadas con la literatura, sociología, artes plásticas, historia o filosofía, entre otras disciplinas; y la Orquesta Sinfónica de la Universidad de Concepción, fundada en 1952 bajo la dirección de Wilfried Junge e integrada a la universidad en 1958.

En lo político, la Universidad de Concepción fue la cuna del Movimiento de Izquierda Revolucionaria (MIR), un grupo de extrema izquierda que tuvo su apogeo durante los años sesenta y la Dictadura militar.

La ciudad está ubicada en el Hemisferio sur en Sudamérica, a 36º 46' 22" S de latitud y 73º 03' 47" O de longitud, con una elevación promedio de 12 msnm, en el Valle de la Mocha entre las planicies litorales y la Cordillera de la Costa. Se encuentra en la Zona Central de Chile, en el centro geográfico del país.
Concepción limita al norte con Hualpén, Talcahuano y Penco, al sur con Chiguayante y Hualqui, al este con Florida y al oeste con el río Biobío y San Pedro de la Paz. La ciudad es marcada geográficamente al encontrarse a los pies de la cordillera de la Costa.

Fitogeográficamente, se encuentra en un paisaje propio de las cuencas fluviomarinas, con bosques esclerófilos secundarios.

El clima es mediterráneo con influencia oceánica y una estación seca breve en verano, según la clasificación climática de Köppen es un «Csb», asemejándose mucho al clima de la ciudad de San Francisco, California, en una latitud similar del hemisferio norte. Su temperatura promedio anual es de 12,7 °C, mientras que la promedio en verano es de 17 °C y en invierno de 8 °C.

Las oscilaciones térmicas son moderadas si consideramos su latitud, esto debido a su cercanía al océano Pacífico. Los veranos son templados y los inviernos suaves. Las precipitaciones se concentran en los meses más fríos, siendo el período de mayo a agosto el que concentra la mayor parte de la lluvia. Por el contrario el verano es seco. Durante el año caen 1110 mm promedio.

La ciudad es colindada geográficamente por dos importantes ríos: el Biobío al oeste y el Andalién al norte. Estos demarcan geográficamente a la ciudad. Además es cruzada por el Estero Nonguén, que nace en el valle.

En Concepción, hay además cinco lagunas urbanas: Lo Méndez (5,2 hectáreas), Lo Custodio, Lo Galindo, Las Tres Pascualas (5,9 hectáreas) y Laguna Redonda (4,1 hectáreas). A ellas se suma en el sector rural la laguna Pineda. 

Adicionalmente, en la ciudad se encuentra el humedal Paicaví, que es uno de los más extensos dentro del Gran Concepción, resguardando una gran cantidad de especies, entre ellas, más de 70 especies de aves. En el mes de diciembre de 2017, vecinos y el comité de Protección del Humedal Paicaví protestaron ante la municipalidad por obras de la empresa inmobiliaria Madesal que rellenarían el costal izquierdo del humedal, dañando el cauce del canal, sin permiso municipal. El humedal además cumple una función de evacuación de aguas lluvias.

Concepción se encuentra en el Valle de la Mocha, el cual está sentado sobre suelos rocosos. Sobre estos hay terreno arcilloso.

Debido a que el río Biobío ha sufrido por miles de años cambios en la dirección de su caudal, hay sectores de Concepción por los que antiguamente pasaban brazos del río. Generalmente son estos sectores los más afectados por inundaciones.

La geomorfología de la ciudad es irregular debido a su cercanía con la cordillera de la Costa. Concepción, por tanto, es marcada por muchos hitos geográficos como cerros, colinas y depresiones.

La comuna de Concepción se encuentra en una zona de alto riesgo sísmico. Según estudios realizados en el año 1967 por Carlos Galli, la ciudad tiene varias fallas de consideración: Caracol, Chepe, Chacabuco y La Pólvora. Su localización no está claramente definida ni hay conocimiento de su profundidad y alcance. El Plano Regulador Comunal de 2005 señala que "este desconocimiento obliga a tener prudencia y a descartar la edificación de grandes estructuras en sus inmediaciones, y a obras viales que la intercepten". 

Concepción ha sufrido a lo largo de su historia al menos cinco grandes terremotos: en 1751, 1835, 1939 (con epicentro en Chillán), 1960 y 2010 (con epicentro a 90 kilómetros de la ciudad).

Concepción concentra una población mayoritariamente urbana, pero también existen localidades rurales principalmente ubicadas en las afueras de la ciudad, hacia la comuna de Florida, tales como, Chaimávida, Villa San Jorge, Agua de la Gloria y Los Puentes.

Por otra parte, la ciudad posee varios sectores, villas y poblaciones. La población urbana se concentra en su Norte desde las carreteras (camino a Penco - Aeropuerto Carriel Sur), así como también en el Oeste desde los límites comunales con Talcahuano y Hualpén (Avenida Jorge Alessandri), en el Este desde sector Palomares - Nonguén, desde el Sureste por Camino a Chiguayante - Pedro de Valdivia y desde el Sur por el mismo Río Biobío (límite con San Pedro de la Paz).



Según los datos recolectados el 2002 en el censo del Instituto Nacional de Estadísticas, la comuna posee una superficie de 221,6 km² y una población de 216 061 habitantes, de los cuales 103 860 son hombres y 112 201 son mujeres.

Concepción acoge al 11,61 % de la población total de la región. Un 1,88 % (4058 personas) corresponde a población rural y un 98,12 % (212 003) a población urbana.

Entre 1970 y 1982 hay un gran incremento en la población. Una de las causas es la inclusión de San Pedro, segregada de la comuna de Coronel.
Concepción disminuyó fuertemente su población debido a que en 1996 se dividió, creándose Chiguayante y San Pedro de la Paz. En estas comunas la población ha crecido rápidamente ya que se han transformado en «comunas dormitorio» de Concepción, es decir, sectores más residenciales, que no tienen un centro con mucho comercio, como Concepción.

Según el censo de 2017 la población de la comuna es de 223 574 habitantes.

Al ser la capital provincial y regional es sede de la Gobernación Provincial y la Intendencia Regional, respectivamente.

La administración de Concepción corresponde a la Municipalidad de Concepción, corporación autónoma de derecho público, con personalidad jurídica y patrimonio propio. La autoridad de dicha municipalidad es el alcalde Álvaro Ortiz Vera (PDC). Este es asesorado por el Concejo Municipal, integrado por los siguientes concejales:

Concepción, en el trascurso de su historia, ha sido administrada de distintas maneras, siendo sede de varias instituciones, cuyas autoridades, administrativas y judiciales, han sido de carácter local, regional y nacional. Estas instituciones han ido cambiando desde La Colonia hasta nuestros días.

Durante época colonial, la institución encargada de los asuntos locales y ciudadanos es el Cabildo de La Concepción. También desde 1565 a 1575 estuvo la Real Audiencia de Chile, en esta ciudad.
Por otro lado, la Capitanía General de Chile se dividía en provincias (denominadas «corregimientos», ya que eran dirigidas por un corregidor). Así, existía la Provincia o Corregimiento de La Concepción, que se encargaba el territorio bajo jurisdicción de la ciudad. Esta provincia, hacia el s. XVIII, fue disminuyendo su tamaño conforme se iban fundado nuevas poblaciones (ciudades y villas), para una mejor ocupación y manejo del territorio, y se crean nuevas provincias o corregimientos.
En 1786, con las reformas borbónica, se creó la Provincia o Intendencia de Concepción, el que se dividía en partidos (La Concepción, Cauquenes, Itata, Chillán, Puchacay, Rere), regidos por un subdelegado, que eran los sucesores de las antiguas provincias o corregimientos.
El Gobernador - Intendente de Concepción, tenía su sede en la actual calle Aníbal Pinto entre la avenida Libertador Bernardo O'Higgins y la calle Diego Barros Arana, frente a la actual Plaza de la Independencia.
Se conoce que el límite norte de la Intendencia de Concepción era el Río Maule. Posteriormente se crean otros partidos (Linares, Isla de La Laja, Osorno y Valdivia)

Después de la Primera Junta Nacional, se instaura el Primer Congreso Nacional de Chile y se conserva la organización territorial colonial adoptada en 1786, con algunas modificaciones (creación de la Provincia o Intendencia de Coquimbo). Durante el transcurso de la década de 1820, se instituyen, sucesivamente, nuevos marcos de división político-administrativa. Con la Constitución de 1822, se derogan las Intendencias, se crean los departamentos (regidos por un delegado directorial), y se conservan los distritos y cabildos.
Hacia 1823, se crean nuevos partidos (Parral, San Carlos, Coelemu y Lautaro), con lo que el Partido de La Concepción, disminuye su jurisdicción a la ciudad de La Concepción y el puerto de Talcahuano.

Con la Constitución de 1823, se instauran las delegaciones, subdelegaciones y distritos. Los cabildos se transforman en Municipalidades. Los antiguos partidos se transforman en delegaciones. Así surge la Delegación de Concepción. En 1826, con las leyes federales, Chile se divide en 8 provincias, instaurándose la Provincia de Concepción, en el sector central de la antigua intendencia colonial. Al norte queda la provincia de Maule y al sur la provincia de Valdivia. La nueva provincia de Concepción es integrada por 7 delegaciones (Concepción, Chillán, Coelemu, Puchacay, Rere, Lautaro y La Laja). Esta división es luego reconocida por la Constitución de 1828.
Con la Constitución de 1833, Chile se divide en provincias, departamentos, subdelegaciones y distritos.
Así, la provincia de Concepción estaba dividida en: Concepción, Chillán, Coelemu, Lautaro, La Laja, Rere y Puchacay).
La Provincia y el Departamento de Concepción estaban regidos por el Intendente de Concepción, que tenía su sede en el mismo lugar de la antigua Intendencia de Concepción.

Con el paso del tiempo, a partir de la subdivisión de la primitiva Provincia de Concepción, se fueron creando otras provincias (Ñuble (1848), Arauco (1852)) y otros departamentos (Arauco, Nacimiento, Talcahuano (1850)). En el plano departamental la jurisdicción alcanza a la ciudad, y Chiguayante (5a subdelegación). En el plano provincial, la colonización y administración de los territorios al sur del Biobío y al norte de la Provincia de Valdivia, pasan a ser competencia del nuevo Intendente de Arauco, con sede en Los Ángeles. A medida que avanza la ocupación y colonización del territorio al sur del río Biobío, se modifica la organización territorial de esta zona. Así, la Provincia de Arauco origina la Provincia de Biobío, la nueva Provincia de Arauco y el Territorio de Colonización de Angol en 1875. En 1887, se crean la Provincia de Malleco y Provincia de Cautín, a partir de la subdivisión y reorganización de la nueva Provincia de Arauco y el Territorio de Colonización de Angol.

Durante el siglo XIX, nace la Ilustre Municipalidad de Concepción, que continúa con los labores del antiguo cabildo, en la administración local del Departamento de Concepción. La municipalidad es dirigida por tres alcaldes y varios regidores. Es presidida por el Intendente de Concepción.

Durante la segunda mitad del siglo XIX la pujanza de la ciudad fue notoria, y en 1886 se construyó una empresa local de tranvías tirados por caballos (carros de sangre). Estos eran de manufactura estadounidense (John Stephenson Co. de Nueva York), pero de dos pisos como los construidos en Inglaterra. Aunque de efímera duración, los carros o tranvías penquistas estuvieron entre los primeros del país.

En 1876, los tres alcaldes comenzaron a ser elegidos. En 1891, con la Ley de Comuna Autónoma, las municipalidades pasan a tener más atribuciones en el orden local y una mayor autonomía. Debido a esta Ley, se dicta el Decreto de Creación de Municipalidades, que crea dos nuevas municipalidades en el Departamento de Concepción: Penco (subdelegaciones 8.ª Palomares y 9.ª Penco) y Hualqui (subdelegaciones 5.ª Chiguayante, 6.ª Nonquén y 7.ª Hualqui). La Municipalidad de Concepción administra las subdelegaciones 1.ª San José, 2.ª Santo Domingo, 3.ª San Agustín y 4.ª La Merced.

A comienzos del siglo XX, la firma W.R. Grace & Company, una firma de vapores de los EE.UU., adquiere la franquicia para construir un tranvía eléctrico en la ciudad, que constituía la tercera más grande del país y el segundo puerto más importante. La "Compañía Eléctrica de Concepción" contrató a la General Electric (G.E.) para la instalación, y ordenó tranvías eléctricos de la empresa Brill, de Filadelfia, abriendo una línea interurbana, de 15 km de extensión, entre Concepción y Talcahuano en 1908. Líneas locales de tranvías eléctricos se inauguraron en ambas ciudades año más tarde, los que también fueron instalados por la firma G.E., que empleaban carros Brill de dos pisos, conducidos por chóferes de género femenino.

En 1925, se crea la comuna de Chiguayante. En 1927, debido a la Constitución de 1925, la municipalidad penquista empieza a ser dirigida por un alcalde y varios regidores, quienes son elegidos por votación popular.

En febrero de 1928, entra en vigencia el DFL N.º 8582, que suprime la Provincia de Arauco, los departamentos de Talcahuano, Puchacay, Coelemu, Rere, Lautaro, Lebu y Cañete; modifica el Departamento de Arauco y crea los departamentos de Tomé, Yumbel y Coronel. Con el DFL 8583, el Departamento de Concepción pasa a ser integrado por las comunas-subdelegaciones de Concepción, Talcahuano, Penco y Hualqui.
Tiempo después, en el plano provincial se restituye la Provincia de Arauco, y en el plano local, el Departamento de Talcahuano con su comuna-subdelegación homónima.

Desde octubre de 1973, durante toda la dictadura militar, los alcaldes de la Municipalidad de Concepción fueron designados por la autoridad a nivel central.

Durante los años 70 se estableció un nuevo modelo para la organización territorial chilena. El país estaría subdividido en regiones (dirigidas por intendentes), divididas en provincias (dirigidas por gobernadores provinciales). Las provincias son conformadas por comunas, administradas por las municipalidades, que son dirigidas por un alcalde.

En el marco de esta modificación se crea la VIII Región del Biobío, dividida en cuatro provincias: Arauco, Biobío, Concepción y Ñuble. Concepción queda como capital provincial y regional.

Con la vuelta a la democracia en los años 90, y junto con la nueva Ley de Municipalidades, estas (incluida la penquista) pasan a ser regidas por un alcalde, que preside el Concejo Municipal (integrado por concejales). Estos puestos vuelven a ser elegidos por votación popular en 1994. En 1995, se crean las Municipalidades de Chiguayante y San Pedro de la Paz, que administran los sectores respectivos.

La comuna de Concepción es representada en el Senado de la República por los senadores Alejandro Navarro del Progresista y Jacqueline Van Rysselberghe del UDI. A su vez, en la Cámara de Diputados desde el 11 de marzo de 2018 es representado por los diputados:

Históricamente, tanto la región como la urbe se han caracterizado por presentar una fuerte presencia de la industria manufacturera. Esto, junto con ser un importante centro de distribución y servicios, sienta las bases de la economía penquista.

En 2018, la cantidad de empresas registradas en Concepción fue de 13.128. El Índice de Complejidad Económica (ECI) en el mismo año fue de 2,88, mientras que las actividades económicas con mayor índice de Ventaja Comparativa Revelada (RCA) fueron Reparación de Transmisores de Radio y TV, Aparatos para Telefonía y Telegrafía con Hilos (29,58), Cultivos Hidropónicos y Hortalizas en Invernaderos (29,45) y Cría de Aves de Corral para Producción de Huevos (19,57).

El polo comercial de la ciudad se encuentra concentrado en el centro de esta, alrededor de la Plaza Independencia, el Paseo Peatonal Alonso de Ercilla y Zúñiga y en las cercanías de las avenidas más importantes de la ciudad.

La calle Diego Barros Arana es un claro ejemplo de esta concentración. Aquí se desarrolla gran parte del comercio de la ciudad y del Gran Concepción. De hecho hasta 1907 esta vía se denominaba "calle Comercio".

Sobre su trazado, entre las calles Carlos Castellón y Aníbal Pinto, se ubica una sección del Paseo Peatonal Alonso de Ercilla y Zúñiga construido en 1981. En él está concentrada una gran cantidad de tiendas comerciales. Otro espacio público que está sobre aquella calle es el Bulevar Diego Barros Arana, usando el espacio comprendido por seis cuadras entre Avenida Arturo Prat y calle Caupolicán. En el marco de los proyectos del Bicentenario de Chile, se ha estado desarrollando otro polo comercial alrededor del Barrio Estación y el nuevo Barrio Cívico de Concepción.
Por otra parte, un importante porcentaje del comercio penquista se desarrolla fuera de la ciudad, en comunas como Hualpén, Talcahuano y San Pedro de la Paz, donde existen centros comerciales, como el Mall Plaza del Trébol, muy concurridos, y donde también se está desarrollando un renovado desarrollo comercial.

Otros focos comerciales, relacionados con la alimentación, son la Vega Monumental y el Mercado Central de Concepción, lugares donde se ofrecen servicios variados y productos agrícolas y ganaderos. Este último fue afectado por un incendio el día 28 de abril de 2013, el cual afectó gran parte de su estructura, forzando a los dueños de diferentes locales a vender sus productos fuera de las ruinas del mercado. El nuevo "Mercado del Gran Concepción", fue inaugurado en julio del 2017, como medida provisoria a la espera de la reubicación final de los locatarios.

Uno de los hitos comerciales más importantes del centro de la ciudad corresponde a las galerías comerciales ya que a su relevancia comercial se agrega a que poseen una gran importancia desde el punto de vista urbanístico y social.

Concepción es una ciudad con una intensa actividad cultural, debido a su amplia movilidad estudiantil, la presencia de importantes museos y por ser el lugar de origen de varios artistas reconocidos tanto a nivel nacional como internacional.

Entre las edificaciones más características de la ciudad se encuentran:

Concepción es considerada, por los propios penquistas, como «La cuna del rock chileno», ya que numerosas bandas de ese género se han fundado en esta ciudad, como las bandas reconocidas internacionalmente: Los Tres, Los Bunkers, De Saloon; y también bandas a nivel nacional, como: Emociones Clandestinas (el primer grupo de la ciudad que se dio a conocer en Chile), Santos Dumont, Machuca, Niño Cohete, Cuti Aste, Julius Popper, Mantarraya, Cholomandinga. Asimismo, reconocidos músicos han declarado que sus primeras presentaciones masivas se han dado en esta ciudad, como es el caso de Los Prisioneros.

Concepción posee una importante cantidad de universidades y otras instituciones educacionales que le dan la calidad de ciudad universitaria, y que representan además una alternativa para varias regiones de Chile. Es por esto que en Concepción, se da un efecto conocido como "feedback" (en inglés: retroalimentación), ya que es una ciudad conformada por muchos jóvenes que proceden de otras partes de Chile. Esto hace que la ciudad se caracterice por la cultura juvenil en ámbitos como la música, el arte, las demandas sociales, etc.

El más importante es la Casa del Arte, perteneciente a la Universidad de Concepción, y que alberga la más completa colección de pintura chilena, compuesta por obras de distintas épocas que ascienden al día de hoy a más de 1800 obras, además del mural Presencia de América Latina, Monumento Histórico Nacional creado por el mexicano Jorge González Camarena. El campus de la Universidad sobre el cual se emplaza, conocida como Ciudad Universitaria de Concepción, es a su vez uno de los núcleos culturales más importantes de la Región del Biobío, donde se encuentra el mayor patrimonio arquitectónico de la ciudad.

A nivel comunal es destacable también la Galería de la Historia, que cuenta, como lo dice su nombre, la historia de la ciudad.

También está el Museo de Historia Natural de Concepción, el cual posee una de las colecciones arqueológicas más importantes de Chile, y bajo la tutela de la Universidad de Concepción, está la Casa del Arte, también conocida como Pinacoteca, museo que alberga la colección pictórica chilena más valiosa e importante del país.

Recientemente se han hallado restos de alfarería, en el área del humedal Los Batros, se afirma que correspondería a una cultura existente 8 siglos atrás.

En el año 2014, se comenzó a gestar la realización del festival "Rock en Conce", realizado al aire libre en el Parque Bicentenario de la ciudad y cuyo acceso sería gratuito. El objetivo del festival sería reivindicar el foco musical que representaba la ciudad y que le ganó la reputación de ser "la cuna del rock chileno" y, de paso, convertirse en un gran atractivo turístico para la ciudad. Su primera edición tuvo lugar el sábado, 7 de marzo de 2015, y desde entonces se ha realizado con gran éxito todos los años a finales del verano, poniendo en escena a bandas locales emergentes junto con importantes y consagrados artistas de la escena nacional e internacional.

Además de "Rock en Conce", la festividad más importante desarrollada en la ciudad es el tradicional "Día de Concepción", que se celebra el 5 de octubre de cada año, en conmemoración de la fundación de la ciudad. Las celebraciones incluyen actos cívicos, presentaciones especiales, y un carnaval. Ya más recientemente la expresión «día de Concepción» se ha extendido a «mes de Concepción», ya que se acostumbran a realizar actividades durante todo octubre.
Otra celebración que se realiza es la del Día de la Hispanidad, en el que la numerosa colonia española residente en Concepción se hace presente. Esta festividad se realiza todos los 12 de octubre.

Durante las Fiestas Patrias, se celebra la Fiesta de la Chilenidad penquista, teniendo como núcleo el Parque Ecuador y el Parque Bicentenario, donde se instalan fondas y ferias.

Uno de los más importantes focos culturales de la ciudad, y que es reconocido a nivel nacional, es la Feria Internacional de Arte Popular. En esta feria exponen artesanos no solamente de la zona, sino que también de todo el mundo. Es la feria de su tipo más grande a nivel nacional.

La Plaza de la Independencia está emplazada entre las calles O'Higgins, Caupolicán, Barros Arana y Aníbal Pinto. Junto a la plaza destacan la Catedral de la Santísima Concepción, el Teatro Universidad de Concepción, el Edificio Pedro de Valdivia y los Estacionamientos Subterráneos Catedral. En la Catedral de la Santísima Concepción se encuentra el Museo de Arte Sagrado de la Universidad Católica de la Santísima Concepción.
Está también el Campus de la Universidad de Concepción, en donde algunos de sus edificios, como el Arco Universidad de Concepción (Facultad de Ciencias Biológicas) y el Campanil, son íconos penquistas y tradicionales postales de la ciudad. En la Casa del Arte de esta casa de estudios, junto a la Plaza Perú, se encuentra el Mural Presencia de América Latina, pintado en 1964 por el artista mexicano Jorge González Camarena.
El conjunto de lagunas urbanas de Concepción son un atractivo natural de la ciudad. Las más importantes son la Laguna Redonda, en el barrio Lorenzo Arenas, y la Laguna Las Tres Pascualas, en donde se encuentra la Universidad San Sebastián, próxima a Avenida Paicaví esquina Manuel Bulnes. Laguna Lo Méndez, Lo Custodio, Lo Galindo y Los Patos son las lagunas restantes.

El Parque Ecuador está extendido a lo largo de 10 cuadras, desde Avenida Arturo Prat hasta calle Tucapel y entre calle Víctor Lamas y el Cerro Caracol, constituyendo el pulmón verde más grande de la ciudad. En él se ubica el museo Galería de la Historia.

Otra área verde de importancia es el Parque Costanera a un costado de la ribera norte del Río Biobío. Ambas áreas verdes cuentan con una red de ciclovías.

La Plaza Acevedo es un área verde de la ciudad, con forma de triángulo, ubicada entre las calles Irarrázabal, Maipú y Collao. Dentro de la plaza se ubica el Museo de Historia Natural de Concepción, creado el año 1902 por el naturalista británico Edwin Reed Brookman y cuyo actual edificio fue inaugurado el 24 de mayo de 1980. Desde noviembre de 2008 existe en ella un pequeño parque temático, denominado Parque Jurásico, en donde se exhiben juegos y atracciones relacionadas con distintos dinosaurios del mundo.

Por otra parte, al centro del sector de zona de recuperación de la Ribera Norte del Biobío, se encuentra el Parque Central el cual ha sido construido por etapas (la primera en 2006). Este parque une la Estación Concepción con la Ribera Norte del Biobío, y en él se encuentra en construcción un Memorial de Los Detenidos Desaparecidos de Concepción, todavía sin inaugurar.

Finalmente, el zoológico de Concepción, Zooconcepción, ubicado al final de la Villa Nonguén, consta de 290 animales, entre los cuales se destacan el tigre de bengala, osos, leones africanos, jabalíes europeos, pumas y otras 87 especies. En cuanto a las aves, Zooconcepción ha desarrollado una laguna artificial donde habitan cisnes, patos y flamencos, entre otras.

La población penquista se refiere popularmente a su ciudad como «La ciudad universitaria» debido a la gran cantidad de universidades y centro de formación técnico profesional que existen en ella, superando en proporción de habitantes a cualquier otra comuna del país, solo tras la comuna de Santiago. En Concepción se han formado universidades de gran importancia a nivel nacional.

Desde tiempos coloniales, los habitantes de Concepción se preocuparon de dotar a la ciudad con casas de estudios superiores. La primera institución de su tipo fue la Pontificia Universidad Pencopolitana de La Concepción, autorizada oficialmente en 1730. Se constituyó por iniciativa del Obispado de Concepción, que la entregó a la administración de la Compañía de Jesús.

El terremoto y tsunami de 1751 la afectó severamente, al perder en la salida de mar infraestructura y su valiosa biblioteca. Este desastre natural motivó a las autoridades de la época a reconstruir la ciudad distanciada del mar, en el Valle de la Mocha. En la misma época en que la universidad se establecía en su nuevo emplazamiento, la actividad académica de las universidades conventuales disminuía, producto de la creación de la Real Universidad de San Felipe, en Santiago. La Universidad Pencopolitana siguió otorgando grados hasta la expulsión de los jesuitas, en 1767, lo que significó su cierre.

Con el inicio de la República, Concepción no volvió a tener una universidad propiamente tal. Sin embargo, en 1865 la Universidad de Chile impartía en el Liceo de Concepción el Curso Fiscal de Leyes, conducente al título de Abogado. Entre sus alumnos, estuvo Enrique Urrutia Manzano, quien ocupara décadas después la Presidencia de la Corte Suprema de Justicia de Chile.

Por iniciativa de un comité de vecinos, entre los que destacan Enrique Molina Garmendia y Virginio Gómez, se funda la Universidad de Concepción en 1919. Hoy, esta universidad integra el Consejo de Rectores de las Universidades Chilenas. Es la tercera más antigua de Chile, la primera creada en la zona centro-sur del país, y la primera en constituirse como corporación de derecho privado. En el ranking 2012 del QS World University Ranking, la Universidad se posicionó en el lugar número 9 a nivel latinoamericano, obteniendo el tercer puesto dentro de las universidades chilenas. Además figura tercera a nivel nacional en los ranking de AméricaEconomía y el Ranking Iberoamericano SIR 2012. Acorde esta última clasificación, está entre las 30 mejores universidades de América Latina.

En 1947, llega la Sede Regional de la Universidad Técnica del Estado. Por las leyes promulgadas durante el período de la dictadura militar, la UTE regional se transforma en la Universidad del Bío-Bío. Así mismo, la sede regional Talcahuano de la Pontificia Universidad Católica de Chile es transformada el año 1991 en la Universidad Católica de la Santísima Concepción, por iniciativa del Arzobispo de Concepción. Las facilidades legales para la creación de nuevos centros de estudios superiores privados, hace que durante las décadas de 1990 y 2000 se establezcan diversas universidades e institutos profesionales, siendo en 1990 la Universidad del Desarrollo una de las primeras.
La Corporación Club Deportivo Universidad de Concepción es el consorcio deportivo más grande de la ciudad. Esta corporación cobija a un club de fútbol, que actualmente participa en la Primera División, un club de básquetbol, que participa en la Liga Nacional de Básquetbol, dos equipos de gimnasia (uno artístico y otro rítmico) y un equipo de rugby.

La hípica penquista se desarrolla en torno al Club Hípico de Concepción, ubicado en la vecina comuna de Hualpén. Recibe cientos de apuestas diariamente y organiza, además de carreras, campeonatos y competencias.

Este deporte es uno de los más emblemáticos de la ciudad ya que ha obtenido bastantes logros representando a Concepción.

El equipo profesional de básquetbol más conocido es el Club Deportivo Universidad de Concepción, el que participa en la Liga Nacional de Básquetbol. Anteriormente, compitió en la Dimayor nacional, donde obtuvo tres campeonatos.

También posee un equipo amateur reconocido a nivel nacional, el Deportivo Alemán.

Concepción posee tres equipos de fútbol dentro de la liga chilena. Como único representante en la Primera División A, actúa el Club Deportivo Universidad de Concepción, mientras que Fernández Vial y Deportes Concepción militan en la Segunda División Profesional de Chile. El último de estos ascendió a dicha categoría, luego de ser desafiliado por problemas económicos desde Primera B (ANFP). Anteriormente, la Universidad San Sebastián jugó en Tercera División (ANFA) entre 1995 y 1998.

La ciudad de Concepción a partir del año 2009 cuenta con el primer equipo de fútbol americano del sur de Chile llamado Club Deportivo Treiles del Bio Bío, el cual pertenece a la Federación Deportiva Nacional de Fútbol Americano de Chile (FEDACH) y por lo tanto es miembro de la Liga nacional de football americano. Su nombre «Los Treiles» se debe a la particular ave Vanellus chilensis que habita principalmente en zonas húmedas de América del Sur.

En Concepción existen dos equipos de rugby que participan en el torneo de primera división de la disciplina: Old John's, fundado en 1991; y Los Troncos, fundado en 1978. El equipo de Los Troncos ha salido campeón del torneo nacional de Chile y ha participado en campeonatos internacionales. Actualmente ambos clubes forman parte de la Liga de Rugby de Chile, que reúne a los equipos de fuera de la capital.

El principal recinto para la práctica de este deporte es la sede de Los Troncos, Estadio Tineo Park, ubicado en la comuna de Penco, en el límite con la comuna de Concepción.

La práctica del tenis se desarrolla en la ciudad en torno a la Casa del Deporte de la Universidad de Concepción, con una cancha de superficie dura (cemento), y el Club de Tenis de Concepción, con su sede en las faldas del Cerro Caracol, en el Parque Ecuador, con varias canchas de arcilla. Varios tenistas destacados a nivel nacional han partido allí, como Adrián García.

En Concepción nació la campeona nacional de la rama, Paulina Vega, quien ha ganado múltiples campeonatos nacionales dentro de sus palmarés y representó a Chile en los Juegos Olímpicos de Atenas 2004, compitiendo junto a la tenimesista nacional Berta Rodríguez en la disciplina de dobles.

Concepción está inserta dentro del sistema integrado de transporte urbano del Gran Concepción, Biovías, que comenzó a funcionar en 2005 y que incluye el sistema de buses Biobús, que funciona como soporte para el sistema de trenes suburbano Biotrén y un sistema de ciclovías.

Por otro lado el sistema licitado de buses cruza los ejes principales de la ciudad, dentro de los cuales están el corredor segregado de buses de Avenida Paicaví y el par vial Avenida Arturo Prat - Avenida Padre Alberto Hurtado, también con vías segregadas, exclusivas para buses.

El transporte urbano más importante desde fuera de Concepción proviene de zonas aledañas a la comuna, pertenecientes a la metrópoli, como Lirquén, Penco, Talcahuano, Hualpén, San Pedro de la Paz, Chiguayante, y Hualqui.

La ciudad cuenta con tres terminales de buses principales:
Además existen algunos servicios de buses rurales a través de los cuales se puede viajar a localidades cercanas de la provincia de Concepción u otros lugares cercanos de la región.

Concepción es parte del núcleo urbanístico del Gran Concepción, ya que se encuentra entre las comunas de Talcahuano, Hualpén, Chiguayante, Hualqui, Coronel y San Pedro de la Paz. Todas estas comunas están interconectadas a través del sistema de ferrocarriles Biotrén, inaugurado el 1 de diciembre de 1999.

En cuanto al ferrocarril interprovincial, este solo cuenta con el servicio Regional Talcahuano-Renaico, mejor conocido como Corto Laja, con recorridos a las comunas de Laja, San Rosendo y a las localidades rurales de la comuna de Hualqui (Gomero, Talcamávida, Unihue y Quilacoya). Hasta el año 2003 existía también recorrido a Renaico, y también existía un servicio nocturno a Santiago.

También hay un proyecto para construir un tranvía en Concepción.

Si bien la comuna de Concepción propiamente tal no cuenta con aeropuerto, en otra comuna del Gran Concepción, Talcahuano, se ubica el núcleo del tráfico aéreo de la zona centro sur de Chile, a través del Aeropuerto Internacional Carriel Sur, que recibe vuelos nacionales desde todo el país, así como esporádicos vuelos internacionales cuando en Santiago los aviones no pueden aterrizar por razones climáticas. Desde 2019, este aeropuerto también cuenta con rutas internacionales, siendo Lima el primer destino internacional disponible.

En cuanto al transporte marítimo, la comuna de Concepción tampoco cuenta con una, debido a que el papel de puerto del Gran Concepción lo desempeñan las comunas de Penco y principalmente Talcahuano, a través de sus puertos de Lirquén y San Vicente respectivamente.

Los medios de comunicación en la ciudad de Concepción se empezaron a masificar durante el siglo XIX, teniendo algunos de los periódicos más antiguos del país, entre ellos "El Sur", fundado en 1882 y el único que sigue activo en la actualidad. En 2006, Diario El Sur S.A. fue comprada por la empresa El Mercurio S.A.P., adquiriendo de esta manera además la propiedad del periódico "La Estrella de Concepción" (fundado en 1995 y llamado "Crónica" hasta 2009). La Universidad de Concepción, en alianza con Copesa, edita el "Diario Concepción" desde 2007.

Con respecto a los medios radiofónicos, desde la ciudad han aparecido algunas de las más importantes cadenas radiales nacionales. Este es el caso de la "Radio Bío-Bío".

Respecto a los medios televisivos, estos son mucho más recientes. Partieron desde Concepción hacia toda la región a fines del siglo XX. Son tres los que actualmente funcionan en la ciudad: "Canal 9 Bío-Bío Televisión", "TV8" y "TVU"; además, se suman dos canales locales que son filiales de canales nacionales que son: "TVN Red Biobío" (filial de Televisión Nacional de Chile) que emite para toda la región, y anteriormente, "Canal 13 Concepción" (filial de Canal 13) que emitía señales solo al Gran Concepción. En lo que respecta a señales tipo abiertas y de pago por cableoperador; "Canal 9 Bío-Bío Televisión", "TVU" y"TVN Red Biobío" se transmiten en ambos tipos, mientras que "TV8" se transmite solo por TV cable a través de VTR, al ser esta empresa la propietaria de ese canal.

Respecto a la señal abierta digital (TVD), "TVU" transmitió durante un tiempo en fase de experimentación, pero actualmente ninguna señal local transmite en este modo. Sin embargo, si existen señales nacionales que se transmiten en TVD, que son Mega (señal HD / móvil), Canal 13 (señal HD / móvil), Chilevisión (señal HD / SD-Simulcast / móvil), Canal Nuevo Tiempo (Dos señales HD / Radio / Móvil), CDTV (señal HD / móvil), y TV Senado (señal HD). Según lo informado por la Subsecretaría de Comunicaciones en el Diario Oficial del 27 de julio de 2016, gran parte de las señales de TVD se transmiten y transmitirán desde las antenas del Cerro Centinela (comuna de Talcahuano).




- La banda de rock chilena Los Prisioneros dedicó una canción a esta ciudad para el álbum homónimo del año 2003, siendo una de las canciones más trascendentales de dicho lanzamiento. 

- El cantautor chileno Jorge González (cantautor) en su álbum solista Mi destino nombra varias zonas del Gran Concepción en la canción "El Viejo que bailaba el nuevo estilo de baile" en compañía del cantautor penquista y líder de la banda Los Tres Álvaro Henriquez

- El Grupo de Cueca brava Los Provincianos de Choapa tienen una cueva llamada "Concepción y Talcahuano".






</doc>
<doc id="17051" url="https://es.wikipedia.org/wiki?curid=17051" title="Teclado">
Teclado

El término teclado hace referencia a varios artículos:




</doc>
<doc id="17060" url="https://es.wikipedia.org/wiki?curid=17060" title="Linspire">
Linspire

Linspire, anteriormente conocida como LindowsOS, es un sistema operativo comercial basado en Debian GNU/Linux y Ubuntu y actualmente es propiedad de PC/OpenSystems LLC. De 2001 a 2008, fue propiedad de Linspire. Inc. y de 2008 a 2017 de Xandros.

El 1 de julio de 2008, los accionistas de Linspire decidieron cambiar el nombre de la compañía a Digital Cornerstone, y todos los activos fueron adquiridos por Xandros.

El 8 de agosto de 2008, Andreas Typaldos, CEO de Xandros, anunció que Linspire sería descontinuado a favor de Xandros; Freespire cambiaría su código base de Ubuntu a Debian; y la marca Linspire dejaría de existir.

El 2 de enero de 2018, se anunció que PC/OpenSystems LLC había comprado Linspire y Freespire de Xandros, y que Linspire 7 estaba disponible por $79.99, mientras que Freespire 3 sería gratuito.

Su sistema de gestión de paquetes de software, CNR: "Click-N-Run", permite a los usuarios, instalar programas con sólo un clic de su ratón; antes era de pago, en una versión de suscripción, pero al parecer, después de negociar con Canonical (empresa que representa legalmente al S.O. Ubuntu linux), liberó este servicio.

Con sede en San Diego, California, Lindows, Inc. fue fundada en agosto de 2001 por Michael Robertson con el objetivo de desarrollar un sistema operativo basado en Linux capaz de ejecutar las principales aplicaciones de Microsoft Windows. Basó su compatibilidad con Windows en la API de Wine. Más tarde, la compañía abandonó este enfoque en favor de intentar hacer que las aplicaciones Linux sean fáciles de descargar, instalar y usar. Para ello, se desarrolló un programa llamado "CNR": basado en el Advanced Packaging Tool de Debian, que proporciona una interfaz gráfica de usuario fácil de usar y un sistema de paquetes ligeramente modificado por una tarifa anual. La primera versión pública de Lindows fue la versión 1.0, publicada a finales de 2001.

En 2002, Microsoft demandó a Lindows, Inc. reclamando que el nombre Lindows constituía una infracción de su marca registrada de Windows. Las acusaciones de Microsoft fueron rechazadas por el tribunal, que afirmó que Microsoft había utilizado el término windows para describir las interfaces gráficas de usuario antes de que el producto Windows se lanzara al mercado, y que la utilización de ventanas ya había sido implementada por Xerox y Apple Computer muchos años antes. Microsoft solicitó un nuevo juicio y, después de que éste se aplazó en febrero de 2004, ofreció resolver el caso. Como parte del acuerdo de licencia, Microsoft pagó aproximadamente $20 millones y Lindows, Inc. transfirió la marca registrada de Lindows a Microsoft y cambió su nombre por el de Linspire, Inc.

El 15 de junio de 2005, Michael Robertson renunció como CEO de Linspire, Inc. Él continuó como presidente y fue reemplazado como CEO por Kevin Carmony Carmony renunció a Linspire el 31 de julio de 2007.

Linspire se convirtió en miembro de Interop Vendor Alliance, fundada en 2006.

El 8 de febrero de 2007, Linspire, Inc. y Canonical Ltd, el principal patrocinador y desarrollador del sistema operativo Ubuntu, anunciaron planes para una nueva alianza tecnológica, con el objetivo de que Linspire "empiece a basar...[sus] ofertas de Linux de escritorio en Ubuntu".

El 13 de junio de 2007, Linspire y Microsoft anunciaron un acuerdo de colaboración en materia de interoperabilidad centrado en la compatibilidad de formatos de documentos, mensajería instantánea, medios digitales, búsqueda web y acuerdos de patentes para los clientes de Linspire. Este acuerdo ha sido criticado, sobre todo por el sitio web de Groklaw, por ser poco duradero y limitado, y en contra del espíritu de la Licencia Pública General GNU. Kevin Carmony, en una de las cartas regulares de Linspire, afirmó que el acuerdo" traería aún más opciones a los usuarios de escritorio Linux y ofrecería una "mejor" experiencia Linux."

Linspire lanza sus nombres de código de productos basado en peces que se encuentran cerca de su sede central: Linspire/LindowsOS 4.5 se llamaba código Coho; Linspire Five-0 (5.0 y 5.1) y Freespire 1.0, Marlín; y Freespire 2.0 y Linspire 6.0, Skipjack.

En agosto de 2005, Andrew Betts lanzó Freespire, un Live CD basado en Linspire. Algunos usuarios confundieron esto con un producto de Linspire, Inc. Linspire, Inc. que ofrecía a los usuarios un "Linspire gratuito" (precio de compra descontado a $0) usando el código de cupón "Freespire" hasta el 9 de septiembre de 2005. El 24 de abril de 2006, Linspire anunció su propio proyecto llamado "Freespire". Esto siguió el modelo de versiones orientadas a la comunidad de Red Hat y Novell en forma de Fedora y openSUSE. Freespire era un proyecto impulsado y apoyado por la comunidad vinculado a la distribución comercial de Linspire, e incluía elementos previamente propietarios de Linspire, como el CNR Client, mientras que otros elementos, que Linspire, Inc. licencia pero no posee, como las bibliotecas de compatibilidad de audio de Windows Media, siguen siendo fuentes cerradas. En consecuencia, hay dos versiones de Freespire, una con las librerías de código cerrado y otra, llamada Freespire OSS Edition, que incluye sólo componentes de código abierto. Freespire 1.0 fue liberado el 7 de agosto de 2006, tres semanas antes de lo previsto.23] Se sabe ahora que Freespire cambiará su base de códigos de Ubuntu a Debian en futuras versiones.24] El 10 de julio de 2007, Linspire publicó Linspire 6.0, basado en Freespire 2.0. El lanzamiento final de Freespire fue de 2.0.8, publicado el 30 de noviembre de 2007. Esto se basó en Ubuntu 7.04, que recibió apoyo durante 18 meses y llegó al final de su vida útil el 19 de octubre de 2008. Por lo tanto, en este momento, Freespire no recibe ninguna actualización de seguridad desde el upstream. La distribución ahora es considerada "Discontinuada" por DistroWatch.

Linspire ha atraído algunas críticas de la comunidad del software libre. Esto por incluir software propietario, el fundador de GNU Richard Stallman comentó: "Ninguna otra distribución de GNU/Linux ha retrocedido tan lejos de la libertad. Cambiar de MS Windows a Linspire no te da libertad, sólo te da un amo diferente." Además, tras el anuncio inicial de Freespire, Pamela Jones del sitio web de Groklaw publicó un artículo titulado "Freespire: A Linux Distro For When You Co couldn' t Care Less About Freedom;" que fue altamente crítico con Linspire, Inc, y el proyecto Freespire, por incluir componentes de código cerrado y anunciarlos como un punto favorable - una acción que ella calificó como ignorar los valores de la comunidad de software libre y de código abierto (FOSS) en una distribución "impulsada por la comunidad", afirmando que "el Software Libre no se trata de controladores propietarios" y que "los códecs, controladores y aplicaciones propietarios no son Open Source o abiertos de ninguna manera". En respuesta, el director ejecutivo de Linspire, Kevin Carmony declaró a través de un periodista del sitio web de Linspire que en diez años de mantenerse firme, la comunidad de FOSS ha hecho relativamente pocas ganancias, que muchos usuarios ya están usando software propietario y, aunque algunos aguantan más, la mayoría preferiría tener algo que funcione en lugar de nada. También afirmó que la compañía creía en el software de código abierto, pero también en la libertad de los individuos para elegir el software que quieran.



</doc>
<doc id="17065" url="https://es.wikipedia.org/wiki?curid=17065" title="Las Palmas de Gran Canaria">
Las Palmas de Gran Canaria

Las Palmas de Gran Canaria es una ciudad y municipio español, capital de la isla de Gran Canaria, de la Provincia de Las Palmas y de la Comunidad Autónoma de Canarias (capitalidad compartida con Santa Cruz de Tenerife). Con una población de habitantes en 2019, es la ciudad más poblada de Canarias y la novena de España.

La ciudad fue fundada en 1478, siendo considerada la capital de facto (sin significado jurídico y real) del archipiélago canario hasta el siglo XVII. Es sede de la Delegación del Gobierno de España, así como sede de su correspondiente subdelegación provincial, de la presidencia del Gobierno de Canarias en períodos legislativos alternos con Santa Cruz de Tenerife, de la Presidencia del Tribunal Superior de Justicia de Canarias, de la Diócesis de Canarias (que engloba a la provincia de Las Palmas), del Consejo Económico y Social de Canarias, así como otras instituciones de diversa importancia como la Casa África. El Carnaval de Las Palmas de Gran Canaria es uno de los eventos más importantes de Canarias, y goza de una importante proyección nacional e internacional.

Los municipios colindantes a la ciudad forman un área metropolitana de más de 600000 habitantes, constituyendo el área metropolitana más grande de Canarias y novena de España. El municipio tiene una extensión de 100,55km² (ISTAC, 2003). Su altitud es de 8 metros sobre el nivel del mar (en la parte más meridional). El clima es de escasas precipitaciones, con una temperatura media de unos 22 °C.

La ciudad presenta un clima árido cálido (BWh) de acuerdo con la clasificación climática de Köppen, altamente influenciado por los vientos alisios. Debido a la variabilidad geográfica y climática del municipio de Las Palmas de Gran Canaria, este presenta otros cuatro tipos de clima, dándose en el siguiente orden conforme nos alejamos del mar y por tanto se incrementa la altitud: El clima semiárido cálido (BSh), el clima semiárido frío (BSk), el clima mediterráneo (Csa) y el clima oceánico mediterráneo (Csb).Los inviernos de la ciudad están justo por encima de la media para ser considerado un clima tropical (18 °C de media en enero, el mes más frío). Unido a la situación del archipiélago junto al trópico de Cáncer, estos factores proporcionan a Las Palmas de Gran Canaria temperaturas medias de 19 °C en invierno y 25 °C en verano. Los vientos alisios —llegados del norte europeo— traen aire fresco y húmedo. Las nubes procedentes del continente filtran los rayos solares y la corriente marina de aguas frías del Golfo regula las oscilaciones térmicas.

El clima de la ciudad es, según un estudio realizado por el climatólogo Thomas Whitmore en 1996, el más «agradable» de las 600 ciudades del mundo analizadas en dicho estudio.

A continuación se muestran los valores climatológicos registrados en la estación meteorológica del Aeropuerto de Gran Canaria en el periodo 1981-2010 excepto las extremas, que son del periodo 1951-2016. A pesar de que la estación está a unos 18km de la ciudad de Las Palmas de Gran Canaria (en el municipio de Ingenio), los valores registrados en esta estación pueden tomarse de referencia para describir el clima de la ciudad.

La ciudad cuenta con cinco playas, de las cuales la más importante es la de Las Canteras. Ubicada en el norte de la ciudad, la singularidad de esta playa de arena dorada reside fundamentalmente en su barra, un arrecife calcáreo de más de 100000 años de antigüedad. La barra casi recorre los 3100 metros de longitud de la playa en paralelo a la orilla y a una distancia de 200 metros, por lo que constituye un rompeolas natural y proporciona unas aguas siempre calmas en buena parte de ella. Con una temperatura media anual de 21º - 20º en el agua-, posee además un ecosistema único, con diferentes hábitats y abundante riqueza piscícola.

Las Canteras es la playa urbana más importante del Archipiélago Canario y una de las más destacadas de Europa. Así lo acredita el certificado UNE-EN ISO 14001, concedido por AENOR en junio de 2004. Está bien conectada mediante transporte público y durante todo el año se organizan actividades de ocio y esparcimiento en ella. Las otras cuatro playas que tiene la capital son Las Alcaravaneras, El Confital, San Cristóbal y La Laja. Además, la playa de Las Canteras cuenta con el certificado UNE-EN ISO 14001, de AENOR, que solo poseen en España las playas de La Concha –en San Sebastián– y La Victoria –en Cádiz– por la implantación de un sistema para la gestión integral del Medio Ambiente. La otra playa más concurrida de la ciudad es Las Alcaravaneras, está situada junto al Muelle Deportivo y dentro de las aguas del Puerto de la Luz, que está enmarcada entre dos clubes náuticos.

En Las Palmas de Gran Canaria residen 378517 personas (INE, 2018), una población que crece cada mañana por la afluencia de quienes se desplazan desde núcleos urbanos próximos (tales como Telde, Arucas, Gáldar, etc.), para desarrollar sus labores profesionales en la capital insular. Está integrada en el Área metropolitana de Las Palmas de Gran Canaria que es la décima de España, con 616903 habitantes.

Según la Universidad de Syracuse, Las Palmas de Gran Canaria es la ciudad con el mejor clima del mundo. Este estudio, publicado en 1996, analiza 600 ciudades con popularidad como destino turístico. Se basa en variables climáticas como la temperatura media anual, que en la capital grancanaria es de 22 °C. A este benigno clima contribuye el hecho de que la ciudad se extienda linealmente entre dos franjas costeras (por un lado, el eje Avenida Marítima/Playa de Las Alcaravaneras; por otro, la playa de las Canteras): la doble brisa que se recibe de ambas permite una mejor limpieza de la contaminación y una mayor refrigeración ambiental.

La ciudad posee diversos parques y plazas como los de Santa Catalina, San Telmo, Doramas y Romano, a los que se han ido sumando otros más recientes como el Parque de Las Rehoyas (con 100000m²) y el Parque Juan Pablo II (con 120000m²), el mayor parque de Canarias. Y el último en añadirse a la lista es el Parque de La Mayordomía, en el barrio de Tamaraceite. Este espacio municipal ocupa 3.728 metros cuadrados y cuenta con 35 parcelas de 27 metros cuadrados para la gestión y cultivo. Este huerto urbano que se encuentra dentro del Parque, viene a ampliar la ya significativa cifra de los existentes en la ciudad como los habilitados de El Polvorín, El Pambaso, Siete Palmas y Pino Apolinario, además de otro que se ejecuta en estos momentos en el barrio de El Lasso, en el Distrito Vegueta-Cono Sur-Tafira. La ciudad dispone actualmente de seis huertos urbanos y avanza para contar con siete, unas áreas que persiguen el fomento del desarrollo sostenible y la calidad medioambiental, resaltando a su vez el clima acogedor y participativo de la ciudad.

La ciudad cuenta con infraestructuras de diversas épocas históricas. La catedral de Canarias, situada en el barrio de Vegueta (el más antiguo de las Palmas de Gran Canaria) es un edificio emblemático de la ciudad. En cuanto a edificios modernos, destaca el auditorio Alfredo Kraus, en el cual se realizan eventos internacionales y nacionales. Está situado junto a la playa de las Canteras y fue nombrado en honor al tenor Alfredo Kraus, nacido en la ciudad. También el Teatro Pérez Galdós es un edificio emblemático de la ciudad, reformado recientemente. Otra muestra es la torre Woermann, un buen ejemplo de arquitectura contemporánea en la ciudad, que con sus 76 metros destaca en la ciudad como uno de los edificios más altos.

La ciudad baja presenta una distribución lineal a lo largo de la costa, con una arteria principal, la Avenida Marítima, que la recorre de una punta a otra. Desde principios del milenio, con la creación de la carretera de circunvalación, muchos puntos de la ciudad son accesibles sin tener que atravesar el centro urbano.

Los orígenes fundacionales de la ciudad de Las Palmas de Gran Canaria se remontan al año 1478, concretamente al 24 de junio (día de San Juan), momento en el cual Juan Rejón, capitán de la Corona de Castilla, inició la conquista de la isla de Gran Canaria. Esta comenzó en la desembocadura del barranco de Guiniguada, donde asentó el El Real de Las Palmas y hoy es el barrio de Vegueta.

La lucha se prolongó por un periodo de cinco años, costando un gran número de vidas, sobre todo en el bando aborigen, que carecía de medios suficientes para defenderse frente a los ejércitos enviados por los Reyes Católicos. Aún así, la resistencia fue feroz. El final de la conquista llegaría en 1483, con la incorporación de la isla a la Corona de Castilla por parte de Pedro de Vera, quien logró el sometimiento de los aborígenes de Gáldar en la zona noroeste de la isla.

En 1485 se trasladó la diócesis desde El Rubicón (Lanzarote) hasta el Real de Las Palmas. La importancia de la ciudad crecería paulatinamente, constituyéndose el Obispado de Canarias, el primer Tribunal de la Santa Inquisición, la Real Audiencia de Canarias y la residencia de Capitanes Generales de Canarias. Aunque la capitalidad, tal y como se entiende a partir del siglo XIX, no existía como tal en el Archipiélago, dado que la Residencia del Capitán General estaba en Las Palmas se puede considerar que esta fue la capital de Canarias durante parte de los siglos XVI y XVII; después, aunque sin significado jurídico y real, continuó siendo considerada capital honorífica del archipiélago canario.

Prueba de la importancia que fue adquiriendo la urbe es la escala que realizó Cristóbal Colón en agosto de 1492 para efectuar unas reparaciones en el timón de la nave Pinta, además de cambiar el velamen original de La Niña (las velas triangulares por unas cuadradas, hecho que la convirtió en la carabela más rápida de la expedición), antes de partir hacia La Gomera. Esta fue la penúltima escala antes del descubrimiento de América.

Durante estos primeros siglos de vida, la ciudad se convirtió en un punto muy activo económicamente, debido sobre todo al comercio de la caña de azúcar. En el siglo XVII se produjo una recesión a causa del freno que sufrieron las exportaciones agrarias tanto a América como a Europa. Durante la época de esplendor se asistió a numerosos ataques piráticos, que se prolongaron en el tiempo hasta el siglo XVIII.

Desde finales del siglo XV, la ciudad se hallaba defendida solo por una fortaleza, enclavada en las montañas de la península de La Isleta. Este fortín, a cinco kilómetros de la urbe, en las inmediaciones de donde hoy se levanta el castillo de la Luz, era el más próximo para asistirla en caso de ataque. Tal precariedad defensiva se mantuvo hasta los últimos decenios del siglo XVI, cuando ya se había hecho notar la amenaza de corsarios y flotillas extranjeras. Desde entonces se empieza a dotar a la ciudad de un sistema de fortificaciones más apropiado. Así, se levantaron pequeños baluartes en el litoral, de los que ha llegado hasta nuestros días el Torreón de San Pedro Mártir, conocido popularmente como "Castillo de San Cristóbal", del año 1577. De esta misma época datan las murallas que cerraban la ciudad por sus flancos norte y sur, que vinieron a marcar los límites a su expansión urbana. Aún hoy se conservan algunos restos de ellas, justo en las cercanías del llamado castillo de Mata, hoy restaurado y convertido en el museo de la ciudad.
Estas fortificaciones no hicieron desistir a la escuadra de navíos ingleses, comandada por John Hawkins y Francis Drake, que a finales del siglo XVI (1595) pretendió sin éxito desembarcar en el litoral de Las Palmas con la intención de saquearla. Aquel ataque supuso el primer combate de la desastrosa expedición inglesa contra la América española, que acabaría en una estrepitosa derrota inglesa, costando la vida tanto a Drake como a Hawkins. Tampoco arredraron a la Gran Armada holandesa, mandada por el almirante Pieter van der Does, que se presentó ante la ciudad el 26 de junio de 1599. En esta ocasión, Las Palmas fue asediada durante dos días y finalmente, tras duros y cruentos combates, tomada en la tarde del 28 de junio por las fuerzas holandesas, formadas por más de seis mil soldados y 74 navíos. Hostigados por las milicias isleñas, que consiguieron hacerles frente y ganarles algunas batallas, los invasores permanecieron en la ciudad algunos días más. Durante este tiempo, saquearon la catedral de Canarias dedicada a Santa Ana, patrona principal de la ciudad de Las Palmas de Gran Canaria, las Casas Consistoriales, conventos y numerosas iglesias, así como algunas casas privadas y mansiones. Finalmente, el 4 de julio, después de sufrir la derrota en la batalla de El Batán, a manos de las fuerzas grancanarias que los emboscaron en el barranco de Santa Brígida los holandeses tuvieron que marcharse, no sin antes proceder al incendio de la ciudad. Las llamas afectaron a numerosas casas, conventos, hospitales, ermitas e iglesias y edificios públicos, algunos de los cuales quedaron completamente destruidos. También se perdieron numerosas obras de arte, entre ellas los retablos, altares e imaginería de la catedral. Sin embargo, no se pudo destruir el templo catedralicio gracias a la solidez de su construcción. Fue ésta, por tanto, la mayor invasión en la historia de la ciudad.

En el siglo XIX se produjo un hecho de importancia vital para la economía de la ciudad: la instauración de los puertos francos. Se trataba de un régimen económico especial que favorecía las relaciones comerciales del archipiélago. Ello hizo que numerosos barcos y navieras recalaran en la isla, sembrando la semilla de lo que posteriormente se convertiría en la principal fuente de riqueza de la actualidad: el turismo. De este interés inicial por el turismo nace en 1890 el primer hotel de Gran Canaria, el Hotel Santa Catalina, que en la actualidad sigue abierto y tras su renovación de 2019 obtuvo el galardón de mejor hotel histórico de lujo de Europa.

En 1927, un Real Decreto de la dictadura de Miguel Primo de Rivera puso fin a la provincia de Canarias. Ello supuso el nacimiento de las nuevas provincias de Santa Cruz de Tenerife y Las Palmas. Las Palmas de Gran Canaria se convirtió en capital de esta última, que integró a las islas de Gran Canaria, Lanzarote y Fuerteventura. Con esta medida se intentó poner fin a la lucha por el control económico y político de las islas que hizo nacer el llamado pleito insular.

Francisco Franco, como general de división comandante militar de las Islas Canarias, después de declarar el Estado de Guerra en todo el archipiélago partió el 18 de julio de 1936 desde Las Palmas de Gran Canaria hacia África, en lo que representó el comienzo de la sublevación que condujo a la Guerra Civil Española. En el Hotel Madrid se conserva intacta la habitación en la que hizo noche el general el día anterior al golpe de estado contra la república.

En 1937 y aún en plena Guerra Civil Española, el municipio de San Lorenzo (Gran Canaria) es anexionado a Las Palmas de Gran Canaria tras el fusilamiento de su alcalde Juan Santana Vega y parte del consistorio electo durante la II República, quedando este reducido a un mero distrito de la capital insular, pasándose por alto la Ley de Municipal de 1935.

Varios lustros después de la finalización de la Segunda Guerra Mundial se notaron ciertos síntomas de recuperación turística, que se materializaron en la Navidad de 1957 cuando aterrizó en el Aeropuerto de Gran Canaria, un avión de la compañía sueca Transair AB con 54 pasajeros. Acababa de iniciarse la era del turismo, principal motor económico de la isla y del archipiélago canario en la actualidad. Durante los años 70 y 80, Las Palmas de Gran Canaria perdió su carácter turístico en beneficio de los municipios del sur de la isla.

Tras la restauración democrática de 1977, la ciudad ha tenido alcaldes de distinto signo, si bien solo Juan Rodríguez Doreste (PSOE) y José Manuel Soria López (PP) han disfrutado de mandatos duraderos. El actual alcalde de Las Palmas de Gran Canaria es Augusto Hidalgo, del PSOE, elegido en los comicios de 2015 y renovado en los de 2019.

Según el estudio de Indicadores Urbanos que elabora el INE, de entre las dos capitales canarias, Las Palmas de Gran Canaria es la que tiene la esperanza de vida más baja con 80,9 años.

El área metropolitana de Las Palmas de Gran Canaria es la más poblada de Canarias. Puede distinguirse un primer anillo inmediato y otro más amplio. El primero incluye los municipios limítrofes (Telde, Arucas, Santa Brígida y Teror), con una población de 540415 habitantes (2005), donde se concentra buena parte de la actividad industrial y comercial de la isla. Por su parte, el anillo más amplio coincide prácticamente con lo que establece la ley de grandes ciudades, abarcando un radio de 20km: dentro de este anillo habitan 619565 personas. Si tomamos como referencia las conurbaciones norte, sur y centro de Las Palmas de Gran Canaria con otras localidades de la isla, esta cifra asciende a más de 700000 habitantes.

El municipio de Las Palmas de Gran Canaria está gobernado por el Ayuntamiento de Las Palmas de Gran Canaria, cuyos representantes se eligen cada cuatro años por sufragio universal de todos los ciudadanos españoles y de la Unión Europea mayores de 18 años de edad que estén empadronados en el término municipal. En las elecciones del 24 de mayo de 2015 ganó el Partido Popular, con diez concejales de 29, seguido por el Partido Socialista con siete concejales, otros seis concejales para Las Palmas de Gran Canaria Puede, dos concejales para Ciudadanos, otros dos concejales para Nueva Canarias y, por último, dos concejales para Unidos por Gran Canaria. De esta manera el socialista Augusto Hidalgo, con el apoyo de Las Palmas de Gran Canaria Puede y Nueva Canarias, se convirtió en alcalde el 13 de junio de 2015, sucediendo al "popular" Juan José Cardona, que gobernaba la ciudad con mayoría absoluta entre 2011 y 2015.

Las Palmas de Gran Canaria está dividida administrativamente en cinco distritos, que a su vez se subdividen en barrios, no necesariamente coincidentes con los barrios tradicionales. Cada uno de los distritos está administrado por una Junta Municipal de Distrito, con competencias centradas en la canalización de la participación ciudadana de los mismos. La última división administrativa de Las Palmas de Gran Canaria data del año 2004 y estructura a la ciudad y su municipio en los siguientes distritos y barrios (datos poblacionales de 2006):
Dt1 - Vegueta, Cono Sur y Tafira (75877 hab.): Aglutina cuatro diseminados (La Montañeta, Los Hoyos, Marzagán y Tafira) y a los barrios de Campus Universitario, Casablanca I, Cuesta Ramón, El Batán, El Fondillo, El Lasso, El Secadero, Hoya de La Plata, Jinámar (Fase III), La Calzada, La Cantera, La Data, La Montañeta, Llano de Las Nieves, Llanos de La Barrera, Lomo Blanco, Lomo de Enmedio, Lomo El Sabinal, Lomo Verdejo, Los Hoyos, Marzagán, Mercalaspalmas, Monteluz, Montequemado, Pedro Hidalgo, Pico Viento, Salto del Negro, San Cristóbal, San Francisco de Paula, San Juan-San José, San Roque, Santa Margarita, Tafira Alta, Tafira Baja, Tres Palmas, Vega de San José, Vegueta, Zárate y Zurbarán.

Dt2 - Centro (88546 hab.): Barrios de Alcaravaneras, Canalejas, Casablanca III, Ciudad del Mar, Ciudad Jardín, Fincas Unidas, La Paterna, Lomo Apolinario, Los Tarahales, Lugo, Miller, Miller Industrial, San Francisco-San Nicolás, Triana; y diseminado de Las Palmas de Gran Canaria.

Dt3 - Isleta-Puerto-Guanarteme (72 345 hab.): Guanarteme, El Confital, El Rincón, El Sebadal, La Isleta, La Puntilla, Las Coloradas, Nueva Isleta, Santa Catalina y Las Canteras.

Dt4 - Ciudad Alta (101684 hab.): Altavista, Chumberas, Cueva Torres, Díaz Casanova, Don Zoilo, El Cardón, Escaleritas, La Feria, La Minilla, Las Rehoyas, Las Torres, Las Torres Industrial, Rehoyas Altas, San Antonio, San Lázaro (Urbanización Siete Palmas) y Schamann.

Dt5 - Tamaraceite-San Lorenzo-Tenoya (39191 hab.): Diseminados de Almatriche, Los Giles, San Lorenzo, Tamaraceite y Tenoya; y barrios de Almatriche Alto, Almatriche Bajo, Cañada Honda, Casa Ayala, Ciudad del Campo, Costa Ayala, Cruz del Ovejero, Cuevas Blancas, Dragonal Alto, Dragonal Bajo, El Pintor, El Román, El Roque, El Toscón, El Zardo, Hoya Andrea, Isla Perdida, La Cazuela, La Cruz, La Galera, La Milagrosa, La Palma, La Suerte, Ladera Alta, Las Cuevas, Las Majadillas, Las Mesas, Las Perreras, Llanos de María Rivera, Lomo Corcobado, Lomo Los Frailes, Los Giles, Masapez, Piletas, Risco Negro, San José del Álamo, San Lorenzo, Siete Puertas, Tamaraceite y Tenoya.

Se tratan de los barrios fundacionales de la ciudad, y sus calles atesoran un notable patrimonio cultural e histórico-artístico. Vegueta fue el origen de la primera ciudad fundada por la Corona de Castilla en el Atlántico, en un momento histórico inmediatamente anterior a su expansión por tierras americanas. Se explica así que, siendo Las Palmas de Gran Canaria la primera capital de Castilla en las Islas Canarias después de San Marcial del Rubicón Lanzarote y Betancuria en Fuerteventura, se establecieran en Vegueta los edificios de las instituciones político-administrativas que habrían de regir el archipiélago, acompañados por las mansiones y terrenos de los primeros señores. Todo ello motivó el relativamente rápido crecimiento de la urbe en su primera etapa de expansión, lo que llevó a parte de sus habitantes a establecerse al otro margen del barranco de Guiniguada, donde se constituyó el barrio de Triana.

En el barrio de Vegueta se hallan las Casas Consistoriales, la Catedral de Canarias, el Palacio Episcopal, la Casa Regental y la Casa de Colón.

La Catedral de Canarias comenzó a construirse en 1497, y se abrió al culto en 1570. Un gran número de arquitectos participaron en el proyecto, por lo que en su construcción se observan varios estilos arquitectónicos: posee una fachada neoclásica, tiene retablos de estilo barroco y su sacristía es de estilo plateresco. La catedral posee 13 capillas.

La Casa de Colón es un conjunto de varios edificios localizados en el núcleo de la ciudad. La tradición indica que Cristóbal Colón pasó por ella en el primero de sus viajes a América. Destaca la gran portada en la Plaza del Pilar Nuevo, creada por Néstor Álamo. El patio central posee características renacentistas, y en el patio de armas destaca su pozo gótico.

En pleno barrio histórico de Vegueta, desde 1951, la Casa de Colón se ha centrado en estudiar, investigar y difundir la historia de Canarias y sus relaciones con América. Dentro de la misma, el museo, la biblioteca y el centro de estudios especializados, conforman un espacio singular.

Uno de los grandes legados que alberga la Casa de Colón es su colección de pinturas que van del siglo XVI al XIX. Interesantes muestras que van desde tablas flamencas a grabados de Goya.

En concreto se sitúa entre la plaza del Pilarillo Seco y el callejón que lleva a la ermita de San Antonio Abad. Este edificio arquitectónico es muestra de la arquitectura señorial.

Cruzando el barranco de Guiniguada con dirección al Puerto de la Luz se emplaza el barrio de Triana, llamado así por las similitudes que, en sus orígenes, tenía con el barrio homónimo de Sevilla. El barrio se estructuró alrededor de la Calle Mayor de Triana, vía de gran belleza arquitectónica con una gran muestra de edificios modernistas y larga tradición comercial. Cerca de ella se alzan edificios como el Teatro Pérez Galdós, el Gabinete Literario, o la Iglesia de San Francisco.

Se está elaborando una propuesta conjunta para que los barrios de Vegueta y Triana sean declarados Patrimonio Mundial de la Humanidad por la UNESCO. No obstante, desde hace algunos años ya se han venido reconociendo los valores patrimoniales que encierra el casco histórico de la ciudad. Vegueta fue declarada Conjunto Histórico-Artístico Nacional en 1973 y, años más tarde, en 1993, fue reconocido el núcleo histórico de Triana.

Con este nombre es popularmente conocida la zona que rodea al Puerto de la Luz y de Las Palmas, uno de los principales motores económicos de la isla. Aquí está el Parque de Santa Catalina, donde se desarrollan algunas de las fiestas de la ciudad como los carnavales. También se halla en esta área la zona comercial de Mesa y López, con numerosos comercios y oficinas, y la Playa de Las Canteras, verdadero pulmón de la ciudad.

Según un estudio llevado a cabo para la revista "Mundo Científico", la playa de Las Canteras es una de las mejores playas urbanas del mundo. Su litoral arenoso tienen unos cuatro kilómetros de longitud, extendiéndose desde la Puntilla hasta el Auditorio Alfredo Kraus, donde se celebran anualmente el Festival Internacional de Cine de Las Palmas de Gran Canaria, el Festival de Opera y, conjuntamente con Santa Cruz de Tenerife y otras localidades canarias, los conciertos del Festival de Música de Canarias. La playa posee una barra natural a unos 100 metros de la costa, que reduce el oleaje sobre la orilla y conforma un ecosistema muy valioso.

El precio del suelo en esta zona es de los más caros de la ciudad, llegando a superar los 5000 euros/m en la primera línea del paseo de Las Canteras.

En las proximidades del Puerto encontramos barrios de carácter popular como los de La Isleta, Guanarteme y Alcaravaneras. En este último está enclavada la playa del mismo nombre.

En el año 2011, el Puerto de la Luz, uno de los más importantes de España, fue galardonado por la prestigiosa revista internacional "Dream World Cruise Destinations" con el premio al puerto con la mejor conexión, ofertas de transporte, hoteles, manejo de equipajes y nivel turístico mundial, consolidándose la ciudad como uno de los mejores destinos turísticos de este tipo.

También cuenta con el acuario Poema del Mar, perteneciente a la empresa tinerfeña Loro Parque.

Los consulados con sede en Las Palmas de Gran Canaria son:


La ULPGC fue creada el 23 de abril de 1989 a partir de la Universidad Politécnica de Canarias y de varios centros hasta ese momento adscritos a la Universidad de La Laguna. Cuenta en la actualidad con cerca de 23.000 alumnos.

Los centros escolares internacionales de la ciudad son el "Deutsche Schule Las Palmas" o Colegio Oficial Alemán de las Palmas de Gran Canaria, "The British School of Gran Canaria" o colegio británico; y el Colegio Americano de Las Palmas.Además el Liceo Francés de Gran Canaria se encuentra en la vecina ciudad de Telde.

También contó con el Colegio Japonés de Las Palmas, un colegio japonés en el extranjero. Localizada en Tafira Baja, se abrió en el octubre de 1973; fue el colegio japonés más antiguo de España y el tercero más antiguo de Europa. Se cerró permanentemente en el marzo de 2001.

La Escuela Complementaria Japonesa de Las Palmas, un instituto complementario japonés a tiempo parcial para nacionales japoneses, tenía clases de japonés.

En Las Palmas de Gran Canaria se encuentran los tres hospitales generales de la isla de Gran Canaria. El Hospital Universitario de Gran Canaria Doctor Negrín, atiende a toda la ciudad salvo el cono sur, además de atender a la población del norte, oeste y las medianías de la isla (excepto Valsequillo). En el cono sur se encuentra el Complejo Hospitalario Materno-Insular, constituido por el Hospital Universitario Insular de Gran Canaria, que además de atender a los barrios cercanos atiende a la población del este y el sur grancanarios, y por el Hospital Universitario Materno-Infantil de Canarias Ambas instituciones están afiliados a la Universidad de Las Palmas de Gran Canaria. La ciudad también cuenta con centros de salud públicos así como varías clínicas privadas.

El servicio de autobuses, localmente conocidos por el nombre de guaguas, es ofrecido por la empresa Guaguas Municipales y cuenta con una estación principal (San Telmo, compartida con la empresa de transporte interurbano Global), dos terminales especiales (Teatro y Manuel Becerra, conocida como Puerto) y cuatro intercambiadores (Santa Catalina y Tamaraceite, compartido con Global; Hoya de la Plata y Auditorio Alfredo Kraus, conocido como Auditorio).

Guaguas Municipales ofrece 40 líneas de transporte urbano, que recorren tanto la parte baja como la alta de la ciudad. Las líneas principales son la 1 (Teatro - Puerto), 2 (Guiniguada - Puerto), 12 (Puerto - Hoya de la Plata), la 17 (Teatro - Auditorio), las líneas 25 y 26 (Auditorio y Santa Catalina - Campus Universitario), 33 (Guiniguada - Puerto, por Ciudad Alta), la 47 (Puerto - Tamaraceite) y la 91 (Teatro-Tamaraceite, por Siete Palmas y La Feria). Además, existen tres líneas de servicio nocturno, denominado Luna. L1 (Hoya Plata - Santa Puerto), L2 (Santa Catalina - Teatro, por Ciudad Alta) y L3 (Teatro - Tamaraceite). Las 3 líneas Luna tienen correspondencia en el Teatro para facilitar el transporte nocturno por la ciudad y funcionan de 23:00 a 05:00.

Global, compañía de guaguas interurbanas, posee 110 líneas, muchas con origen o destino a la capital. Esta compañía surgió en el año 2000, producto de la fusión de las anteriores compañías interurbanas Salcai (concesión sur-sureste) y Utinsa (concesión norte-centro).

Existe también un servicio de Guagua Turística, que recorre los sitios de mayor interés turístico de la ciudad con guía en varios idiomas. El vehículo cuenta con dos plantas, una inferior cerrada y la superior abierta. Desde ella se puede divisar la ciudad, sus monumentos, museos, centros comerciales con una panorámica de 360º a 4 m de altura. La guagua cuenta con un sistema de audio multilingüe individual que ofrece la información en 8 idiomas. La Guagua Turística es una de las mejores alternativas para conocer la ciudad de Las Palmas de Gran Canaria en un solo día. Presta sus servicios de lunes a domingo y comienza a las 9.30 horas y termina a las 18:00. Cada 30 minutos parte una unidad desde el Parque Santa Catalina. El ticket -que se puede adquirir en la propia guagua- vale para todo el día y permite subir y bajar libremente en las paradas.
La MetroGuagua es un proyecto de BRT, que recorrerá 11,8 kilómetros de la ciudad baja de Sur a Norte, con una frecuencia media de entre 4 y 5 minutos y un tiempo de recorrido de 35 minutos.

Esta línea de alta capacidad estará dotada con 22 vehículos biarticulados de 24 metros de longitud, dos estaciones subterráneas, una en el extremo sur, en el barrio de Hoya de la Plata y otra en el Parque Santa Catalina y una terminal en la Plaza de Manuel Becerra en el extremo norte del recorrido. Las cocheras y centro de control de la MetroGuagua, estarán ubicadas en la estación de Hoya de la Plata.

Desde 2011 existe un servicio de préstamo de bicicletas municipales. Inicialmente denominado "Biciambiental" tuvo escaso éxito por contar con menos de 50 vehículos y solo 11 estaciones. En 2015 cambió su nombre a "Las Palmas Bybike". El nuevo servicio sumó 2 nuevas estaciones e incorporó 150 nuevas bicicletas con mejores prestaciones.

Además se amplió y acondicionó una serie de carriles bici que han hecho a Gran Canaria una de las islas con el carril bici más amplio, carril cuya vía se extiende desde el antiguo barrio de Vegueta hasta las Canteras, y que recorre entre otras: Plaza del Doctor O'Shanahan, el Corredor Playa, que conecta el Parque Santa Catalina con el Auditorio, calles Fernando Guanarteme, Avenida Marítima, Franchy Roca, la Base Naval, el paseo de Alcaravaneras, Rotonda de Julio Luengo y Secretario Padilla, Luis Morote, Eufemiano Jurado y Olof Palme.

En abril de 2018 se produjo una nueva reforma del sistema de préstamo, denominándose ahora "Sítycleta", que amplia el número de estaciones hasta 40 e incorpora casi 400 bicicletas "inteligentes", y 20 vehículos eléctricos. Paralelamente se inicia la primera fase de la Red de Carriles Bici, con la construcción de los primeros 28 kilómetros de ciclovías que recorrerán toda la ciudad. Para ello, se ha procedido a la supresión de varios carriles de vehículos a motor y la eliminación de zonas de estacionamiento.

El Tren de Gran Canaria (TGC) es un proyecto de ferrocarril propuesto inicialmente como alternativa para ir de Las Palmas de Gran Canaria a Maspalomas en menos de una hora, aunque posteriormente se planteó su prolongación hasta Gáldar, habiendo 2 líneas de tren. Recientemente ha culminado la redacción de los proyectos del trazado y de las once estaciones del futuro tren del sur, a la espera de la financiación para la ejecución de las obras. 

Los monumentos declarados Bien de Interés Cultural del municipio son los siguientes:

Las Palmas de Gran Canaria ofrece una agenda cultural relativamente amplia y variada: teatro, cine, ópera, conciertos, artes plásticas y danza son espectáculos habituales en las carteleras de la ciudad, destacando especialmente el Festival de Música de Canarias, el de Teatro y Danza y el Festival Internacional de Cine.







La ciudad cuenta con una amplia red de bibliotecas, repartidas por los diversos distritos. Junto a las 11 bibliotecas municipales hay que reseñar estos cuatro centros:


La patrona de la ciudad de Las Palmas de Gran Canaria y patrona histórica de la isla de Gran Canaria, es Santa Ana, cuya imagen se encuentra presidiendo la Catedral de Canarias y cuya festividad se celebra cada 26 de julio. El patrono del Ayuntamiento de la ciudad es el Santísimo Cristo de la Vera Cruz que se encuentra en la Parroquia Matriz de San Agustín de Hipona, su fiesta se celebra el 14 de septiembre. Pero la imagen de mayor devoción en la ciudad es Nuestra Señora de la Soledad, la cual es aclamada popularmente como "Señora de la ciudad", su procesión del Retiro es la procesión por antonomasia de la ciudad. La imagen de la Virgen se venera en la Parroquia de San Francisco de Asís y Santuario Mariano de Nuestra Señora de la Soledad desde el siglo XVI, aunque la actual imagen es del XVII.

Es tradición que ciertos años, la imagen de la Virgen del Pino (patrona de Gran Canaria) se traslade en peregrinación desde la Villa Mariana de Teror hasta la ciudad de Las Palmas de Gran Canaria para conmemorar diferentes efemérides religiosas. Durante estas estancias de la Virgen en la ciudad, que duran aproximadamente dos semanas, la Virgen es visitada por miles de fieles. La última bajada de la Virgen a la capital grancanaria fue entre mayo y junio de 2014. Anteriores bajadas fueron en 2000, 1988, 1965, 1954 y 1936.

El barrio histórico de Vegueta ha recobrado una gran popularidad en los últimos años, siendo el punto de encuentro preferido de muchos jóvenes que los fines de semana llenan sus bares de copas y discotecas. En la zona del puerto se encuentran también numerosos bares y discotecas, sobre todo en las proximidades del Parque Santa Catalina.

Durante el carnaval de Las Palmas de Gran Canaria, normalmente entre enero y febrero, se puede disfrutar de múltiples actividades, como los populares mogollones (verbenas) y las galas de elección de la reina y de la "drag queen".

Mención especial merece el Festival WOMAD (World Of Music, Art & Dance), que generalmente en el mes de noviembre de cada año tiene como escenario a la capital grancanaria. El primer festival se celebró en Las Palmas de Gran Canaria en 1993 ininterrumpidamente hasta 2012, siendo retomado, tras 3 ediciones en 2014, 2015 y 1016 en Fuerteventura, en 2017 hasta el presente.

Diversos clubes que militan o han militado en las máximas categorías españolas tienen su sede en esta ciudad:

La ciudad fue subsede del Mundobasket 2014 que organizó la Federación Española de Baloncesto, para lo cual se construyó un nuevo pabellón polideportivo en el barrio de Siete Palmas

Destacan los deportes acuáticos, con las tradicionales regatas de vela latina, que se realizan cada fin de semana en la bahía de Las Palmas de Gran Canaria, así como varías instituciones que fomentan dichos deportes y el golf:

Entre otras pruebas atléticas las más notables:

Oficialmente, la ciudad de Las Palmas de Gran Canaria está hermanada con tres localidades:


Además, el municipio ha aprobado en el Pleno su voluntad de hermanamiento con las siguientes ciudades, si bien aún no se han oficializado dichos hermanamientos:






</doc>
<doc id="17067" url="https://es.wikipedia.org/wiki?curid=17067" title="Península balcánica">
Península balcánica

La península balcánica o península de los Balcanes es una de las tres grandes penínsulas del sur de Europa, continente al que está unida por los montes Balcanes al este (cordilleras que han dado nombre a la península) y los Alpes Dináricos, al oeste.

Se encuentra rodeada de mares por tres de sus lados: el Adriático y el Jónico, al oeste; el Egeo, al sur; y el Mármara y el Negro al este. Al norte, se delimita la península generalmente por el curso de los ríos Danubio —el principal de la zona—, Sava y Kupa. Está separada de Asia por los estrechos de los Dardanelos y del Bósforo. Al oeste, los Alpes Dináricos separan el interior del mar Adriático. En el sur, diversos ríos —entre ellos el Vardar y el Struma— que desembocan en el Egeo facilitan el acceso al centro peninsular. La principal ruta de comunicación norte-sur la componen los ríos Morava y Vardar, que en conjunto casi cruzan toda la península.

Esta región comprende una superficie total de más de 550 000 km² y tiene una población de casi 53 millones de habitantes. Su nombre proviene de la cadena montañosa homónima en turco, situada en el centro de Bulgaria.

La península, administrativamente, pertenece a los siguientes Estados: Albania, Bosnia y Herzegovina, Bulgaria, Croacia, Eslovenia, Grecia, Macedonia del Norte, Montenegro, Rumania, Serbia y la región turca de Tracia Oriental. Diversas ciudades sirven de canal comercial con el interior de la península: en el oeste, Dubrovnik, Split; en el este Constanza, Burgas y Varna; y en el sur, la principal, Salónica.

Una cadena montañosa de unos 650 msnm atraviesa dicha península, separando Bulgaria de Rumanía, entre las cuencas del Danubio, el Mármara, el archipiélago de las islas Espóradas y los Dardanelos, a la que corresponde la montaña de Yumkusal (2380 m).

La vertiente meridional es mucho más abrupta que la del norte. Bosques de coníferas y caducifolias. El valle de Iskar y los puertos de Sipka y de Trojan son los pasos más importantes.

Con el topónimo "Hemo" se aludía en la antigüedad al macizo montañoso de los Balcanes, en Bulgaria, que se prolonga de oeste a este desde Serbia hasta el mar Negro.

Los países y entidades incluidos en la región son los siguientes:

Aunque estrictamente Hungría, Moldavia y Ucrania no estén dentro de la península de los Balcanes, suelen ser incluidas en la región de los Balcanes (países balcánicos) por motivos históricos y culturales. La complicada historia de esta región, caracterizada por las frecuentes divisiones y subdivisiones de los Estados desde al menos la segunda mitad del , ha dado origen al concepto de «balcanización» que se aplica, incluso, a territorios muy distantes de los Balcanes. «Balcanización» significa la división generalmente violenta y artificial por potencias extrarregionales de los territorios de los países que integran una región.

En los Balcanes se hablan multitud de lenguas de familias lingüísticas muy diferentes: entre las mayoritarias un grupo de eslavas (el búlgaro, el serbo-croata, el esloveno y el macedonio, entre otras), la griega, la albanesa y un grupo de lenguas neolatinas (la rumana, la moldava –identificada con la rumana– y la aromuna –o valaca–, entre otras). Hay pequeñas áreas donde se habla el húngaro, el alemán, el turco y el italiano; y en comunidades dispersas por toda la península el romaní (comunidades gitanas) y los idiomas vinculados a las comunidades judías (yiddish y judeo-español).




</doc>
<doc id="17072" url="https://es.wikipedia.org/wiki?curid=17072" title="Monitor Huáscar">
Monitor Huáscar

El monitor "Huáscar" es un buque de guerra del siglo XIX que tuvo una relevante participación en la Guerra del Pacífico. Fue construido en el Reino Unido en 1864 por orden del gobierno peruano y sirvió en la Marina de Guerra de Perú hasta el 8 de octubre de 1879, cuando fue capturado por la escuadra chilena en el combate naval de Angamos. El "Huáscar" sirvió activamente en la Armada de Chile hasta 1897, cuando fue dado de baja. Actualmente sirve como museo marítimo flotante en el puerto chileno de Talcahuano, Región del Biobío. Es el segundo blindado (acorazado) a flote más antiguo del mundo después del HMS "Warrior".

Las dimensiones principales del monitor "Huáscar" son: 59,4 m de eslora (largo), 10,6 m de manga (ancho) y 4,5 m de calado (profundidad). La carena tiene un tonelaje de 1100 B.O.M. y un desplazamiento total (incluyendo combustible, agua dulce, armamento, municiones, víveres y tripulación) de aproximadamente 1745 t en máxima carga.

El casco, construido en hierro remachado, está dividido longitudinalmente en cinco compartimentos estancos, por cuatro mamparos del mismo material de 15 mm (5/8 de in) de espesor. El "Huáscar" tiene dos cubiertas: la cubierta principal, que se eleva 1,37 m (4,5 ft) sobre la línea de flotación de máxima carga; y la segunda cubierta, que se encuentra aproximadamente 2,5 m por debajo la cubierta principal. La proa del "Huáscar" cuenta, como era costumbre en los diseños de fines del siglo XIX, con un espolón. La forma de la popa es de crucero. El casco fue considerado como "muy maniobrable", para los estándares de la época, siendo capaz de completar un giro de 180° en 2 minutos y 0,3 segundos.

La superestructura del monitor estaba compuesta originalmente por (de proa a popa): el castillo de proa, el mástil trinquete (trípode), la torre de artillería, la torre de mando de forma hexagonal, la chimenea telescópica de 8 metros de altura, el mástil mayor y el castillo de popa. Para evitar el embarque de agua en mar gruesa, la cubierta principal contaba con falcas metálicas rebatibles, las cuales se abrían al momento de usar la torre de artillería. 

El "Huáscar" cuenta con dos salas de gobierno o puentes de mando: la sala de gobierno principal que está ubicada en la cubierta principal a proa del castillo de popa y la sala de gobierno de combate situada en la segunda cubierta bajo la torre de mando o "conning-tower". Las acomodaciones de la oficialidad se encuentran en la segunda cubierta a popa del mástil mayor. Las acomodaciones del resto tripulación se encuentran en la segunda cubierta a proa de la torre de artillería. También en la segunda cubierta, pero a popa de la sala de gobierno de combate, se encuentra la cocina.

El armamento principal del "Huáscar" se encuentra alojado en la torre de artillería, conocida como torre Coles, la cual está situada sobre la cubierta principal entre el castillo de proa y la torre de mando. La torre es de forma cilíndrica con un diámetro de 6,7 m (22 ft) y un peso de 37 toneladas. El armamento de la torre estaba compuesto originalmente de dos cañones de ánima rayada Armstrong, de avancarga, de 254 mm (10 in) capaces de disparar proyectiles de aproximadamente 136 kg (300 lb) de peso. Cada cañón pesaba 12,5 toneladas. 

La torre descansa y gira sobre una senda de roletes ubicada en la segunda cubierta. Tiene además un eje guía (pinzote) fijo que gira sobre un tintero empernado a la quilla. Originalmente la ronza de la torre era manual y necesitaba el esfuerzo de 16 hombres y 15 minutos para cubrir todo el campo de tiro. Debido a lo alto del castillo de proa y las estructuras situadas tras la torre de artillería, el campo de tiro de la torre está limitado a 138º por cada banda (los ángulos muertos son de 64° y 20° a popa y proa respectivamente). 

Como armamento secundario el "Huáscar" posee dos cañones rayados Armstrong de 120 mm (5 in) capaces de disparar proyectiles de aproximadamente 18 kg (40 lb) ubicados en las bandas de babor y estribor respectivamente y un cañón rayado Armstrong de 76 mm (3 in) capaz de disparar proyectiles de aproximadamente 5,5 kg (12 lb) ubicado en popa bajo el castillo. El "Huáscar" contaba además, durante la guerra del Pacífico, con una ametralladora Gatling de 11 mm (calibre .44) instalada en la cofa del palo mayor.

El "Huáscar" tiene una coraza lateral de hierro de de espesor en el centro del buque, que disminuye hasta hacia proa y popa y se extiende a bajo la línea de flotación de desplazamiento máximo. Entre el casco y el blindaje posee un forro de separación de madera de teca de entre de espesor para reducir el impacto de los proyectiles. La cubierta principal está protegida por planchas de de espesor. El blindaje de la torre de artillería es de reforzado en la zona que rodea de las troneras con planchas de espesor y, al igual que el blindaje del casco, está empernado a una estructura metálica con un forro interior de madera de teca de de espesor. La torre de mando original contaba con un blindaje de de espesor respaldado por de madera de teca.

Su sistema de propulsión es mixto, máquina a vapor y vela. La máquina de vapor es del tipo Maudslay (horizontal, de dos cilindros). Cada cilindro tiene un diámetro de 1,37 m (54 in) y una carrera de 914,4 mm (3 ft). Esta máquina era capaz generar 1,23 MW (1650 HP) a 78 revoluciones por minuto. La máquina principal hace girar una hélice de cuatro palas de 4,49 m (14 ft 9 in) de diámetro y 5,41 m (17 ft 9 in) de paso. Este sistema propulsor le permitió alcanzar al "Huáscar", en el viaje de prueba (sin cañones, sin provisiones a bordo y cargando solamente 100 toneladas de carbón en las carboneras), una velocidad máxima de 12,25 nudos. 

Cada cilindro descarga a condensadores independientes en los cuales se mezclaba el vapor de descarga con el agua de mar para producir la condensación. Este proceso suple las pérdidas de agua de alimentación a medida que se consumía la que se almacena en el doble fondo. El vacío es producido por una bomba de aire situada inmediatamente debajo del condensador, la cual es accionada por un vástago acoplado directamente al émbolo de la máquina motriz. Al eje cigüeñal de la máquina motriz van conectadas cuatro bombas de tronco accionadas por excéntricas, dos de las cuales son bombas de agua salada para él condensador y dos de achique de sentinas.

El departamento de calderas tiene cuatro calderas horizontales, dos de cuatro fogones y dos de tres que trabajan a una presión máxima de 172,3 kPa (25 psi). Su alimentación se efectúa por medio de una máquina de múltiple propósito situada en el mismo departamento.

El aparejo del monitor era de bergantín con el trinquete en forma de trípode, según patente del capitán Coles para facilitar el movimiento y manejo de los cañones en la torre. En junio de 1879, Grau decidió que en el Callao al "Huáscar" se le retire el trinquete para así poder aumentar el arco de fuego.

En 1864, el Perú se encontraba en una delicada situación diplomática con España, la cual desembocaría posteriormente en la guerra hispano-sudamericana. La crisis se vio agravada debido a que las unidades de la marina de guerra peruana carecían del poder de fuego necesario para poder enfrentar a la fuerza naval española. Esta circunstancia motivó al gobierno de Juan Antonio Pezet a encargar en Inglaterra la construcción de nuevos buques de guerra para reforzar su escuadra. 

El 12 de agosto de 1864, se firmó en Birkenhead (Reino Unido) el contrato de construcción de una nueva nave de guerra, entre el capitán de navío José María Salcedo, en representación del gobierno peruano, y el astillero Laird & Brothers. El valor contractual fue de 71 000 libras esterlinas sin incluir artillería, siendo el costo total de 81 247 libras esterlinas, unos 406 325 soles de plata de la época. El plazo inicial de entrega fue de 12 meses.

El astillero asignó a la nueva construcción el número 321. El diseño de la nave estuvo a cargo de Cowper Phipps Coles, oficial de la Marina Real Británica, tristemente célebre por otro de sus diseños, el del HMS Captain. Tanto la maquinaria propulsora como las calderas fueron provistas por la compañía Penn e Hijos, mientras que la artillería estuvo a cargo de la firma Armstrong y Cía. El nuevo buque fue lanzado el 7 de octubre de 1865, siendo bautizado como "Huáscar" en honor al penúltimo Inca del imperio, hijo del Inca Huayna Cápac y hermano del último Inca Atahualpa.

El monitor "Huáscar" zarpó desde Birkenhead el 17 de enero de 1866, comandado por el capitán quien supervisó la construcción del buque por parte de la armada peruana. El viaje estuvo plagado de dificultades y marcado por serias diferencias entre Aurelio García y García (comandante de la fragata Independencia) y José María Salcedo (nombrado jefe de la división naval de blindados).

El 20 de enero, el "Huáscar" enfrentó un fuerte temporal que lo obligó a recalar en el puerto francés de Brest el día 23 del mismo mes. Una vez en Brest se reagrupó con la fragata "Independencia" y el vapor inglés "Thames" (fletado por el gobierno peruano). El convoy zarpó, en cruce del Atlántico, el día 24 de febrero. En la víspera del zarpe, se produjo un intento de amotinamiento, el cual terminó con 5 tripulantes ingleses heridos de bala. Durante el viaje, el 28 de febrero, la "Independencia" colisionó en alta mar con el "Huáscar", cuya máquina se había detenido sin notarlo el oficial de guardia. El convoy arribó en Río de Janeiro (Brasil) el 1 de abril. El Huáscar entró al dique de Río de Janeiro por 5 días para reparar su hélice. El capitán Salcedo recibió una fuerte reprimenda por parte del ministro plenipotenciario peruano Benigno Vigil a sugerencia de la oficialidad del "Huáscar". Dieciocho tripulantes del "Huáscar" desertaron durante su estadía en Río de Janeiro.

El convoy zarpó otra vez rumbo al Estrecho de Magallanes el 29 de abril, produciéndose nuevamente un motín antes del zarpe. Esta vez el motín terminó con un condestable inglés herido en la cabeza y arrojado a tierra. El 5 de mayo, el "Huáscar" capturó al bergantín español "Manuel" que se dirigía a Montevideo, el cual fue posteriormente incendiado. Al día siguiente el monitor capturó al velero "Petita Victorina", el cual fue dotado con tripulación y enviado a Chile. El 24 de mayo el convoy, junto con la corbeta estadounidense "USS Dakotah", ingresó al Estrecho de Magallanes. Al día siguiente el convoy creció aún más al unirse con la corbeta América. En Punta Arenas, el convoy repostó carbón desde una barca prusiana y zarpó rumbo a Ancud (Chiloé, Chile) el 29 de mayo. En la medianoche los buques fueron azotados por una tempestad que hizo perder al "Huáscar" su lancha auxiliar a vapor. 

El convoy arribó finalmente en Ancud el 6 de junio de 1866, donde se reunió con el resto de la escuadra chileno-peruana junto a la que continuó su viaje hacia Valparaíso el día 11 de junio. Esa misma noche, la flotilla enfrentó mal tiempo y el "Huáscar" debió remolcar a la "Apurímac" durante 5 horas.

Una vez arribada la escuadra a Valparaíso, el capitán Salcedo fue relevado de su cargo por orden del gobierno peruano y reemplazado por el capitán de fragata norteamericano David McCorkle, nombrado por el nuevo jefe de la Escuadra John Tucker. El "Huáscar" permaneció una larga temporada en Valparaíso debido a que se preparaba una campaña contra la escuadra española y a que se preveía una incursión de esta escuadra desde el Atlántico. Tucker renunció el 15 de marzo de 1867, mientras el "Huáscar" era carenado en el dique de Valparaíso. Tras su renuncia, el gobierno peruano designó como nuevo comandante al capitán de fragata Alejandro Muñoz, quien permaneció en Valparaíso hasta que el estallido de la guerra civil peruana provocó la renuncia del presidente Mariano Ignacio Prado en enero de 1868. El gobierno entrante ordenó al "Huáscar" navegar hacia Perú, arribando por primera vez al puerto de El Callao el 2 de febrero de 1868. El nuevo gobierno del general Pedro Díez-Canseco nombró como nuevo comandante al capitán de fragata Miguel Grau, quien asumió el mando el día 26 de febrero del mismo año.

El 6 de mayo de 1877 seguidores del caudillo Nicolás de Piérola se sublevaron en contra del gobierno del general Mariano Ignacio Prado tomando el control del "Huáscar". El mando de la nave fue tomado por el capitán de corbeta Germán Astete. El 9 de mayo el Huáscar interceptó dos buques de bandera británica de la Pacific Steam Navigation Company lo cual hace que la escuadra británica del Pacífico se involucre en el conflicto. El 28 de mayo de 1877 la escuadra del Perú al mando de Juan Guillermo More Ruiz enfrenta al blindado en el combate de Punta Pichalo sin poder capturarlo. Al día siguiente el buque es confrontado por la escuadra británica del Pacífico, al mando del contraalmirante Algernon Frederick Rous De Horsey en el combate naval de Pacocha con idéntico resultado. El Huáscar por su parte se hizo célebre en esta acción al convertirse en el primer barco en la historia naval en evadir el ataque de torpedos autopropulsados. El 31 de mayo de 1877, tras intentar en vano que el resto de la escuadra peruana se uniera a la sublevación, la tripulación rebelde decidió rendirse en el puerto de Iquique.

 

Desde 1878, el "Huáscar" era comandado por el capitán de fragata Gregorio Pérez y, aunque el monitor estaba en condiciones de navegar, se encontraba desartillado y con su marinería sin entrenamiento. El 24 de marzo de 1879, tras la declaración de guerra y la ocupación de Antofagasta, Miguel Grau Seminario, quien era hasta entonces diputado por Paita en el congreso peruano, fue nombrado como nuevo comandante del "Huáscar", asumiendo el mando el 28 de ese mes. 

Bajo el comando de Grau, el monitor "Huáscar" se transformó en la nave de más destacada participación de la escuadra de la marina de guerra del Perú durante la Guerra del Pacífico. Las correrías del Huáscar mantuvieron en jaque a la escuadra y al gobierno chileno durante seis meses (5 de abril-8 de octubre de 1879). Entre las acciones más destacadas del "Huáscar" se cuentan: el combate naval de Iquique (21 de mayo de 1879), el primer combate naval de Antofagasta (26 de mayo de 1879), la captura del vapor Rímac (23 de julio de 1879) y el segundo combate naval de Antofagasta (28 de agosto de 1879). Al regresar al Callao en junio de 1879, Grau hizo modificaciones en el barco en junio de 1879. Se le quitó al monitor el palo trinquete que entorpecía el empleo de la torreta hacia la proa, con el fin de acelerar su marcha y mejorar el accionamiento de la torre Coles. Además en esa ocasión fue acortado el castillo de proa en varios pies para mejorar el campo de tiro de la misma torre.

Estas acciones detonaron una crisis en el gobierno de Chile que provocó la caída del contralmirante Juan Williams Rebolledo. Chile debió movilizar seis buques de su escuadra, incluyendo sus dos blindados, para poder capturarlo. Fue interceptado en Punta Angamos, el 8 de octubre de 1879. Tras una hora y media de intenso combate, y a pesar de que la nave comenzaba a hundirse por orden de su comandante el Teniente 1ro. Pedro Gárezon Thomas, el monitor "Huáscar" fue capturado por la Armada de Chile. De un total de 204 tripulantes, 33 resultaron muertos, incluido su comandante Miguel Grau, y 27 resultaron heridos.

Durante el combate naval de Angamos, el "Huáscar" presentó daños severos, siendo los más graves los causados por dos impactos directos en la torre Coles. Estos impactos causaron la perforación de 4 planchas de hierro y del forro de teca y afectaron la fijación de la torre paralizando la ronza. El cañón de babor fue impactado en el muñón izquierdo por un proyectil que lo desmontó y le arruinó los soportes de la cureña, habiendo sido el otro cañón afectado en su sistema de mira. Los cañones fueron desmontados y remplazados por cañones de retrocarga de 254 mm (10 in). La dotación chilena tuvo serios problemas con la nueva artillería y esta es vuelta a cambiar por una compuesta de cañones de marca Elswick de 203 mm (8 in) que son los que tiene en la actualidad.
Tras su captura, el blindado de tipo monitor operó bajo la bandera de Chile durante el resto de la Guerra del Pacífico. El "Huáscar" participó en acciones como el combate naval de Arica (27 de febrero de 1880), donde pereció su comandante, Manuel Thomson, y el bloqueo al puerto peruano del Callao, bajo el mando de Carlos Condell.

Una vez finalizada la guerra, el "Huáscar" sufrió varias modificaciones. En 1885 se le instalaron cuatro nuevas calderas en los astilleros Lever, Murphy & Co. de caleta de la Barca —hoy Caleta Abarca—, una hélice nueva diseñada por astilleros Laird Brothers, una nueva chimenea, un poco más alta que la original, y se renovaron las cajas de humo y las camisas de la chimenea. La segunda cubierta fue renovada y se cambiaron tablones en la cubierta principal. También se le reparó el doblefondo y las máquinas. En 1887 se le instaló una máquina de vapor de dos cilindros horizontales, construida por la firma Morrison, para mover la torre Coles. 

En mayo de 1888, el "Huáscar", al mando de Luis Uribe, fue encargado de trasladar los restos de los marinos chilenos Arturo Prat, Ignacio Serrano y Juan de Dios Aldea, fallecidos en el combate naval de Iquique, desde esta localidad al puerto de Valparaíso.

Durante la Guerra Civil de 1891 que se vivió en Chile, el Huáscar formó parte de las fuerzas congresistas que derrotaron al presidente José Manuel Balmaceda. Al atardecer del 7 de enero de 1891, la fragata Cochrane remolcó al "Huáscar", que tenía sus máquinas desmontadas, desde la bahía de Valparaíso, fondeándolo frente a Las Salinas, donde se procedió a ponerlo en servicio.

El 23 de enero el "Huáscar" y el transporte Amazonas se apoderan de Taltal y obtienen la adhesión de la ciudad a la causa congresista. El 12 de febrero la nave frustró el intento de desembarco de las tropas presidencialistas embarcadas en el transporte "Imperial", en la localidad de Patillos. El monitor participó también en el bombardeo de Iquique, ciudad en manos de los partidarios de Balmaceda, el día 19 de febrero. Posteriormente el monitor "Huáscar" fue destinado a labores de escolta de los transportes que conducían a las tropas congresistas y a la vigilancia costera.

El monitor Huáscar formó parte activa de la escuadra chilena hasta 1896, cuando fue dado de baja por la explosión de una de sus calderas. En 1905 se hicieron estudios para transformar al Huáscar en un cañonero moderno, para lo cual se tendría que remover la torre Coles. El consejo naval de la armada acordó no aceptar el proyecto. En 1917 fue destinado a ser buque madre de las tripulaciones de la flotilla de submarinos Clase H, recientemente adquiridos por la Armada de Chile.

Se le restauró como reliquia histórica en 1934, siendo fondeado frente a la base naval de Talcahuano. Entre 1951 y 1952, gracias a la iniciativa del contraalmirante Pedro Espina Ritchie, se inició la total restauración del monitor, con la intención de dejarlo tal como era en 1897. La labor se vio facilitada al encontrarse numerosos objetos (muebles, accesorios, etc.) en los almacenes del Arsenal Naval de Talcahuano y gracias a la cooperación de un sinnúmero de personas e instituciones de la provincia de Concepción.

La restauración se realizó teniendo en cuenta el aspecto que mostraba cuando finalizó su servicio en la armada de Chile en 1897. En la cámara del comandante se instalaron los retratos del almirante peruano Miguel Grau y de los capitanes chilenos Arturo Prat y Manuel Thomson, que murieron en su cubierta y en lo que fue el departamento de calderas, se instalaron una galería de retratos y un oratorio con la finalidad de que este sea un lugar de honra y de veneración a quienes ofrendaron su vida por su patria.

Entre 1971 y 1972, el contraalmirante Carlos Chubretovich decidió emprender la segunda etapa de la restauración (casco y estructura). Las labores de restauración estuvieron a cargo de ASMAR Talcahuano. En 1995, el World Ship Trust otorgó a la Armada de Chile el premio "Maritime Heritage Award" por la excelente restauración del monitor "Huáscar" y por el testimonio que representa para Chile y Perú.

Tras el terremoto de Chile de 2010, se temió por el buque al no encontrarlo en un primer momento; sin embargo, el "Huáscar" únicamente había cambiado de posición y no se había visto afectado por el tsunami.




</doc>
<doc id="17075" url="https://es.wikipedia.org/wiki?curid=17075" title="Distribución geométrica">
Distribución geométrica

En teoría de probabilidad y estadística, la distribución geométrica es cualquiera de las dos distribuciones de probabilidad discretas siguientes:



La distribución geométrica es un caso especial de la distribución binomial negativa con parámetro "k" = 1. Más generalmente, si "Y" ...,"Y" son variables independientes distribuidas geométricamente con parámetro "p", entonces formula_11 sigue a una distribución binomial negativa con parámetros "k" y "p".

Si "Y"...,"Y" son variables independientes distribuidas geométricamente (con diferentes parámetros de éxito "p" posibles ), entonces su mínimo formula_12 es también geométricamente distribuido, con parámetro

formula_13.



</doc>
<doc id="17076" url="https://es.wikipedia.org/wiki?curid=17076" title="Huáscar">
Huáscar

Huáscar, nacido como Tupic Cusi Hualpa (Huascarpata, Cusco; 1491 - Andamarca, 1533) fue el penúltimo Inca del Tahuantinsuyo, uno de los diez hijos de sangre de Huayna Cápac y uno de los terceros con acceso al trono. Las versiones oficiales dicen que Huayna Cápac y quien fue nombrado su sucesor, Ninan Cuyuchi, fallecieron en 1527 de viruela, cuya epidemia empezaba a brotar por el norte del Imperio incaico tras la invasión europea de América del Sur. Ante este hecho, escoger al nuevo Inca fue difícil y finalmente se eligió a Huáscar, pues este había sido nombrado como "Incap antin" (vicegobernador) de Cuzco por su padre.

Varios pretendieron el trono , entre ellos su medio hermano, Atahualpa, quien terminaría derrocándolo en 1532, apresándolo y posteriormente acabando con su vida; todo lo cual sucedió poco antes que Atahualpa mismo fuera apresado por Francisco Pizarro, quien lo ejecutó por haber mandado a asesinar a su hermano, por el pecado de poligamia y el delito de la esclavitud contra los nativos.

Probablemente nació en Huascarpata, con el nombre de "Tupic Cusi Hualpa" (que luego cambiaría al tomar el poder). Fue designado como Sapa Inca por los "orejones" (nobles de sangre o panaca real) de Cuzco, mas no por su propio padre quien falleció junto a su legítimo sucesor. En la decisión tomada por los "orejones" tuvo influencia la experiencia que tenía Huáscar en gobernar una importantísima ciudad como era Cuzco.

Durante el comienzo de su gobierno, todos lo reconocían como Sapa Inca, nombrando a su hermano Atahualpa "incap rantin" (vicegobernador) de Quito y su zona de influencia. Esta solicitud fue aceptada por Huáscar.

A pocos meses de asumir el gobierno, Huáscar descubrió una vasta conspiración, donde estaban implicados varios de sus hermanos que querían encumbrar como heredero a Cusi Atauchi, muy estimado y admirado en el Cusco. La furia del Sapa Inca fue implacable, mandó ejecutar a todos los conjurados entre los que se encontraban orejones de importantes panacas, principalmente de la saya Hanan Cusco. Para sentirse seguro, Huáscar se alejó de la nobleza cusqueña y se rodeó de nobles leales a la sucesión hereditaria como su hermano Tito Atauchi quien lo asistió de consejero en las campañas de Pomacocha, Honda, Comacocha y Chupat, en la lejana región de los chachapoyas.

Ante estos hechos Huáscar empezó a desconfiar de todos, y algunas crónicas españolas, por otra parte poco objetivas, mencionan que en ocasiones mató a sospechosos sin tener pruebas contundentes.

Huáscar veía en Atahualpa la mayor amenaza a su poder, ya que este había pasado una década combatiendo en las campañas de su padre y tenía el apoyo de los generales y gente de Quito. No se opuso a que permaneciera como gobernador de Quito, por respeto a los deseos de su difunto padre, pero con dos condiciones: que no hiciera campañas militares para expandir sus territorios y que se reconociera vasallo suyo y le pagara tributos, y Atahualpa las aceptó.

Lo cierto es que el territorio bajo el dominio de Atahualpa era un área muy rica y poblada, teniendo este la posibilidad de realizar campañas de conquista a los ricos pueblos al norte de esta, algo a lo que, por cierto, ya no podía aspirar Huáscar, pues su frontera norte quedaba prácticamente cerrada por los dominios de su hermano. Huáscar comprendió que Atahualpa podía fácilmente fortalecerse hasta llegar a tener la capacidad de enfrentársele para someterlo. Atahualpa contaba además con las mejores tropas del imperio y los generales más experimentados de las campañas de su padre.

Una tensa paz duró no más de cinco años, sin que ninguno de los dos realizara alguna campaña militar y dedicándose a disfrutar de las riquezas que heredaron. Huáscar aprovechó ese tiempo para conseguir el apoyo de los cañaris, una poderosa etnia que dominaba extensos territorios del norte del imperio y mantenían rencores hacia Atahualpa, pues este los había combatido durante las campañas de su padre.

La primera batalla la ganó Huáscar con un ejército comandado por su hermano, el general Atoc. Atahualpa aprovechó una tregua (en época de cosecha se supone que ninguna etnia andina debe realizar campañas bélicas) y tomó algunas ciudades norteñas. Poco a poco llegó a Cuzco casi invicto y finalmente derrotó a Huáscar.

Una vez derrotado el ejército cuzqueño, Huáscar fue conducido descalzo, semidesnudo y atado del cuello hasta donde se encontraba Atahualpa (quien también se encontraba prisionero por los españoles). Sin embargo, antes de que se encontraran, Atahualpa, temiendo que Pizarro liberara a Huáscar y le devolviera el poder, ordenó su ejecución en el poblado de Andamarca. Sus restos fueron arrojados al río Yanamayo.

Según las crónicas de la conquista, el cuerpo de Huáscar fue arrojado al río Yanamayo o río de Andamarca, cerca del pueblo de Andamarca. Respecto a la ubicación de Andamarca, se tienen dos sitios, el primero se ubicaba a 30 km al sur de Huamachuco, cerca de Cajamarca. Mientras que el segundo se ubica en la región Ayacucho.





</doc>
<doc id="17078" url="https://es.wikipedia.org/wiki?curid=17078" title="Kenjutsu">
Kenjutsu

El kenjutsu (剣術) es un arte marcial japonés tradicional del "koryū budō". Existen varias escuelas ("ryu") cuyo objetivo es enseñar a combatir de manera eficiente con el sable japonés. 
La práctica puede desarrollarse de muchas formas dependiendo del "ryu" (escuela) practicado.

Los estilos de "kenjutsu", como conocidos hoy fueron tomando forma a partir del periodo Muromachi (siglos XV y XVI).
Hola te vengo a avisar que esta página se ha modificado el 25 de agosto del 2020, no sabemos quién lo ha hecho, pero puede que lo que estás leyendo ahora sea contenido falso, Gracias! 
Entre los estilos de esa época se destacan:


El kenjutsu tuvo una gran expansión durante el periodo Edo (siglos XVI a XIX), registrándose más de 500 estilos. Al final de este periodo algunos estilos empezaron a utilizar "shinai", la espada de bambú y "bōgu", la armadura de protección, para traer más seguridad a los entrenamientos. Este entrenamiento fue el precursor del kendo moderno.

Algunos de los principales estilos de kenjutsu que se desarrollaron en el periodo Edo fueron:


Con la Restauración Meiji, al final del siglo XIX y, la prohibición del porte de espadas, varios estilos terminaron desapareciendo, hecho que se repitió luego de la derrota japonesa en la Segunda Guerra Mundial (1939-1945). Pero aún existen varios que sobreviven hasta el día de hoy.
Actualmente existen dos organizaciones, la Nihon Kobudō Kyōkai y la Nihon Kobudō Shinkōkai que congregan varios de los estilos de "kenjutsu" existentes actualmente.

Entre los estilos que aún existen, practicados en una línea continua, destacamos el Niten Ichi Ryu, Suio Ryu, Katori Shinto Ryu, Kashima Shinto Ryu, Ittō-ryū, y Kashima Shin Ryu. 
En la mayoría de estos estilos no existen cinturones de colores para determinar el nivel de los practicantes. En lugar de los grados se entregan licencias como la "menkyo kaiden".
Ejemplos modernos de "gendai budō" o artes marciales contemporáneas derivados del arte de la espada son: el kendo y el "iaidō".

El entrenamiento de sable clásico o kenjutsu varía de acuerdo con el estilo en cuestión. En la mayor parte se fundamenta en "kata" (o formas preeestablecidas). En algunos estilos, la práctica de los "kata" se complementa con entrenamientos de combate utilizando armadura de protección.

En los entrenamientos de formas o "kata" normalmente se utiliza una espada de madera semejante a la "katana", llamada "bokken" o "bokuto". Cada estilo de "kenjutsu" suele imponer medidas específicas de largo, ancho y curva para su "bokuto".

En el entrenamiento de combate, los "dōjō" que lo hacen utilizan alguna forma de protección para evitar graves lesiones. La mayor parte utiliza la misma armadura del kendo, llamada "bōgu" (armadura hecha de bambú, laca, cordones y revestimientos de tejidos) y "shinai" (sable de bambú). Otra arma que también puede ser utilizado para luchar es el fukuro-shinai, una espada semejante al "shinai" usado en el arte moderno del sable o kendo, pero, con el mismo largo de una "katana" y, construido a partir de varias tiras de bambú, cubiertas con un revestimiento de cuero.
Algunos estilos utilizan sables de metal con corte ("shinken") para entrenamiento de corte.
Cabe señalar que cada estilo tiene características propias al entrenamiento. El practicante ya empieza con la espada desenvainada. En algunos estilos, como por ejemplo el Niten Ichi Ryu, existen técnicas específicas para la utilización de dos espadas, una en cada mano.

El "kenjutsu" es una disciplina física, espiritual y mental; para su práctica es necesario el equilibrio entre cuerpo y mente, más que fuerza física y vigor. La enseñanza más profunda del "kenjutsu" posee un aspecto físico-religioso bastante fuerte, bajo influencias principalmente del sintoísmo, confucionismo y del budismo zen.

Varias artes marciales descienden del kenjutsu o han sufrido influencias, tales como: el kendo, su versión moderna, que posee un mayor énfasis "deportivo"; el "iaidō", o el arte de desenvainar el sable. Y entre las artes marciales tradicionales modernas o "gendai budō". El "aikidō" incorpora principios de entrenamiento con el sable, dentro de varios de sus movimientos. Sin embargo el entrenamiento con el sable no es enfatizado, excepto en el estilo Iwama Ryu.

Muchas escuelas de "kenjutsu" consiguieron sobrevivir al final de los samuráis y llegar hasta nuestros días, en una línea continua sin interrupciones.

Los Dojos de "kenjutsu" se mantienen por todo el territorio japonés. Anualmente son realizadas demostraciones organizadas por la Nihon Kobudō Kyōkai y la Nihon Kobudō Shinkokai.

Lejos de ser una actividad popular, la mayoría de los dojos de "kenjutsu" posee un número relativamente pequeño de practicantes. En parte se debe al carácter marcial y tradicional de la práctica y también por la falta de interés de las nuevas generaciones en preservar aspectos más tradicionales de la cultura japonesa.



</doc>
<doc id="17088" url="https://es.wikipedia.org/wiki?curid=17088" title="Los ocho inmortales">
Los ocho inmortales

Los ocho inmortales () son un grupo de deidades de la mitología china, según la cual existieron terrenalmente y nacieron durante las dinastías Tang o Song, practicando las técnicas de la alquimia y los métodos de la inmortalidad. Fueron descritos por primera vez durante la dinastía Yuan y en la literatura occidental anterior a los años 1970 se les conoce a veces como "Los Ocho Genios". Son adorados dentro del taoísmo, pero también en la cultura china popular.

Los miembros del grupo rara vez aparecen por separado y los poderes de cada uno de ellos pueden ser transferidos a sendos utensilios que pueden dar la vida o destruir el mal y que son conocidos como "An Baxian" (暗八仙, "Àn Bāxiān", "Los Ocho Inmortales Escondidos"). En chino dan nombre a la hortensia (八仙花，"Bāxiān huā", "Flor de los Ocho Inmortales").

Los Ocho Inmortales son:

Los Ocho Inmortales han servido de base a muchas obras de arte, tanto en el campo de la escultura como en la pintura o en la literatura. Dentro de este último destaca "El viaje hacia el este de los Ocho Inmortales" (八仙出处东游记, 八仙出處東游記, Bāxiān chū chù dōng yóu jì) de tiempos de la dinastía Ming.

También en esa época se escribió, por un autor anónimo, "Los Ocho Inmortales cruzan el mar" (八仙过海, 八仙過海, Bāxiān guò hǎi), donde se narra su viaje para asistir a la fiesta de aniversario de la diosa Xi Wangmu, llamada "Encuentro del melocotonero de la Inmortalidad" (蟠桃会, 蟠桃會, pán táo huì). Al llegar al mar, en vez de utilizar sus nubes para cruzar, Lü Dongbin sugiere que unan sus poderes para atravesar las aguas. De aquí proviene el "los Ocho Inmortales cruzan el mar, cada uno revela su poder divino" (八仙過海 各顯神通, 八仙过海 各显神通, Bāxiān guò hǎi gè xiǎn shén tōng), utilizado en las situaciones en que todo el mundo ofrece sus habilidades para conseguir una meta común.

Cada uno de los inmortales representa una faceta de la sociedad china y por ende la historia de "los ocho inmortales cruzan el mar" representa las tareas que puede cumplir la nación cuando hay armonía entre cada una de sus partes. Los grupos que representan se ven claramente en la biografía de cada uno. Zhongli Quan representa a los militares; Lü Dongbin a los burócratas; Li Tieguai a los enfermos y heridos; Han Xiangzi a los sabios; Cao Guojiu representa a la nobleza; Zhang Guo Lao a los ancianos; Lan Caihe a los pobres y He Xiangu a las doncellas.

En Xi'an existe un templo de la dinastía Song llamado el "Palacio de los Ocho Inmortales" (八仙宫, 八仙宮, bāxiān gōng ), llamado antes el "Templo de los Ocho Inmortales" (八仙庵, Bāxiān ān ), donde se pueden ver sus estatuas en la "Sala de los Ocho Inmortales" (八仙殿, Bāxiān diàn). En Muzha (木栅, 木柵, Mùzhà), en Taipéi (Taiwán) está el "Palacio del Sur" (南宫, 南宮, Nángōng), conocido como el "Templo de los Ocho Inmortales" (八仙廟, Bāxiān miào ).

En la mitología e historia chinas han existido otros grupos de ocho personas denominados como "inmortales", como por ejemplo:



</doc>
<doc id="17089" url="https://es.wikipedia.org/wiki?curid=17089" title="Nosferatu, eine Symphonie des Grauens">
Nosferatu, eine Symphonie des Grauens

Nosferatu, eine Symphonie des Grauens (en alemán Nosferatu: Una sinfonía del horror, nombre con que es conocida en España y en México, ya que en Argentina se le llama simplemente Nosferatu) es una película muda alemana de 1922, dirigida por Friedrich Wilhelm Murnau. Se trata de la primera película relacionada con la novela de Bram Stoker "Drácula". Inicialmente no aceptada por la viuda de Bram Stoker, que abrió una demanda, pero pronto se convirtió en una película de culto.

Hutter y su mujer Ellen son un joven matrimonio que vive en la ciudad de Wisborg en el año 1838. Un día, un agente inmobiliario llamado Knock envía a Hutter a Transilvania para cerrar un negocio con el conde Orlok. Se trata de una posada, donde ojea un viejo tratado sobre vampiros que encuentra en su habitación. Una vez en el castillo, es recibido por el siniestro conde. Al día siguiente, Hutter amanece con dos pequeñas marcas en el cuello, que interpreta como picaduras de mosquito. Una vez firmado el contrato, descubre que el conde es, en realidad, un vampiro. Al verle partir hacia su nuevo hogar, Hutter teme por Ellen.

El estudio detrás de "Nosferatu", Prana Film (llamado así por el concepto budista de prana), fue un efímero estudio de cine alemán de la era muda fundado en 1921 por Enrico Dieckmann y el artista y ocultista Albin Grau. Su intención era producir películas con temática de ocultismo y sobrenaturales. "Nosferatu" fue la única producción de Prana Film, ya que se declaró en quiebra con el fin de esquivar las demandas por infracción de derechos de autor de Florence Balcombe, la viuda de Bram Stoker.

Grau tuvo la idea de hacer una película de vampiros; la inspiración surgió de una experiencia de guerra de Grau: en el invierno de 1916, un granjero serbio le dijo que su padre era un vampiro y uno de los no muertos.

Diekmann y Grau le dieron a Henrik Galeen, discípulo de Hanns Heinz Ewers, la tarea de escribir un guion inspirado en la novela "Drácula" de Bram Stoker, a pesar de no haber obtenido los derechos o el permiso para producir una película sobre ella. Galeen era un especialista experimentado en el romanticismo oscuro que ya había trabajado en "Der Student von Prag" ("El estudiante de Praga") en 1913, y el guion de "Der Golem, wie er in die Welt kam" en 1920. Galeen ambientó el relato en una ficticia ciudad portuaria del norte de Alemania llamado Wisborg y cambió los nombres de los personajes. El título "Drácula" se cambió por 'Nosferatu", y también se cambiaron los nombres de los personajes: el conde Drácula es aquí el conde Orlok, por ejemplo. Su papel fue interpretado por Max Schreck (en alemán, "schreck" significa ‘susto’). Harker es Hutter, Mina es Ellen y la Inglaterra victoriana es la ciudad de Viborg ―o de Bremen en la versión francesa, así como también en la inglesa―. Añadió también la idea del vampiro trayendo la plaga de Wisborg a través de ratas en el barco.

El rodaje comenzó en julio de 1921, con tomas exteriores en Wismar. Una toma de la torre de la iglesia de Santa María sobre el mercado de Wismar con la fuente de abastecimiento de agua sirvió como el plano general de la escena de Wisborg. Otros lugares fueron el Wassertor (Puerta del Agua), el lado sur de la Iglesia de San Nicolás, el patio de la Iglesia "Espíritu Santo" y el puerto. En Lübeck, el almacén de sal abandonado sirvió como la nueva casa de Nosferatu en Wisborg. Otras tomas exteriores siguieron en Lauenburg, Rostock y en Sylt. Los exteriores de la película que se desarrolla en Transilvania fueron en realidad filmados en locaciones en el norte de Eslovaquia, incluyendo el Alto Tatra, el Valle Vrátna, el castillo de Orava, el río Váh, y el castillo . El equipo rodó tomas interiores en el estudio JOFA y más tomas exteriores en el bosque de Tegel.

Por razones de coste, el camarógrafo Fritz Arno Wagner sólo tenía una cámara disponible, y por lo tanto sólo había un negativo original. El director siguió el guion de Galeen con cuidado, siguiendo las instrucciones escritas a mano sobre el posicionamiento de la cámara, la iluminación y otros asuntos relacionados. Murnau se preparó cuidadosamente; hubo bocetos que iban a corresponder exactamente a cada escena filmada, y uso un metrónomo para controlar el ritmo de la actuación.

Florence Balcombe no tuvo conocimiento de la existencia de "Nosferatu" hasta que recibió una carta anónima de Berlín. El documento incluía el programa de un evento cinematográfico de 1922, con acompañamiento orquestal completo, que había tenido lugar en el jardín de mármol del jardín zoológico de Berlín. La película alemana estaba descrita en el folleto como "una adaptación libre de la obra Bram Stoker, Drácula". Sin embargo, la viuda de Bram Stoker demandó la película por infracción de derechos de autor y ganó el juicio.

El tribunal ordenó que se destruyeran todas las cintas de "Nosferatu", pero un reducido número de copias de la película ya se habían distribuido por todo el mundo, y permanecieron escondidas por particulares hasta la muerte de la viuda de Bram Stoker. Con el paso de los años se hicieron más copias de esas cintas (algunas de muy baja calidad y con cortes importantes). 

"Nosferatu" se labró la reputación de ser una de las mejores películas sobre el mito del vampiro y uno de los máximos exponentes del expresionismo alemán. Rodada en escenarios naturales, una práctica poco habitual que la aleja de los postulados del cine expresionista alemán, abundantes planos de "Nosferatu" están inspirados en pinturas románticas.

En Estados Unidos, la obra "Nosferatu" de Murnau pertenece al dominio público, y existe un gran número de copias en vídeo, generalmente de muy baja calidad ya que provienen de copias hechas a partir de otras copias de las primeras cintas distribuidas para la exhibición internacional. Muchas de ellas presentan diferencias notables de metraje, puesto que en cada país se exhibió una versión diferente de la película. Así pues, la copia francesa no es la misma que la alemana, por dar un ejemplo. No obstante, recientemente se han publicado ediciones restauradas de la película en las que se ha recuperado casi todo el metraje completo de la película original. La reconstrucción más fiel de la película fue presentada en el Festival de Berlín de 1984.

En 1979, Werner Herzog dirigió una remake de "Nosferatu, el vampiro", titulada "". Filmado con un presupuesto escaso, como era habitual en Alemania durante los años setenta, el "Nosferatu" de Herzog fue una revisión del legendario filme original, con cambios en el argumento. Se convirtió en un éxito de crítica y obtuvo buenas cifras en la taquilla. Desde su estreno hasta la actualidad es considerada un sentido homenaje a la obra de Murnau y una excelente película por derecho propio. Klaus Kinski ocupó el papel principal, acompañado de Isabelle Adjani y Bruno Ganz.

En el año 2000 se estrenó el filme de terror "La sombra del vampiro", dirigida por E. Elias Merhige. La cinta, que posee rasgos de humor negro, relata una historia ficticia sobre el rodaje de la versión muda de "Nosferatu". Protagonizada por John Malkovich y Willem Dafoe, se trata de una historia fantástica de horror en la que un director (Murnau, interpretado por Malkovich) crea una película de vampiros completamente realista gracias a que contrata a un auténtico vampiro (interpretado por Dafoe) para que interprete el papel de Nosferatu.

José Fors adaptó una ópera rock titulada "Orlok el vampiro".

El conde Orlok ha aparecido en cómics, como en la novela gráfica de Viper Comics del 2010 titulada Nosferatu, que cuenta la historia de Orlok en un entorno moderno.

También apareció en un episodio de Bob Esponja titulado "Turno de Ultratumba". Al final del episodio se revela que él es el responsable del parpadeo de las luces en la historia de terror de Calamardo Tentáculos. Cuando lo descubren, él sonríe y el episodio termina.

Así mismo, uno de los personajes del juego League of Legends, Vladimir, tiene una apariencia alternativa basada en Nosferatu, siendo esta llamada "Vladimir Nosferatu".

La partitura original de "Nosferatu" fue realizada por Hans Erdmann.

Más tarde se realizaron otras obras inspiradas en esta película:




</doc>
<doc id="17094" url="https://es.wikipedia.org/wiki?curid=17094" title="Euscaldún">
Euscaldún

Euscaldún, euscalduna, euskaldún o euskalduna es una palabra que en euskera quiere decir «vascohablante». La Real Academia Española lo admite con esta acepción y como sinónimo del término «vasco». 

Etimológicamente conjuga la palabra "euskara" (lengua vasca) con el sufijo "-dun" («que lo tiene»). En consecuencia, aquel que posee la lengua vasca es el vascohablante, sea cual sea su origen. En el País Vasco francés y en Navarra se utiliza también la variante "eskualdun" (en los dialectos de esas zonas, se utiliza la palabra "eskuara" para referirse a la lengua). En zonas de Sola (Zuberoa), el término "üskualdün" se utiliza sólo para los hablantes de suletino, mientras que a otros vascohablantes se les llama "manex".

A quien habla euskera como lengua materna se le llama en dicha lengua "euskaldun zahar" (literalmente, «vascohablante viejo»), mientras que quien lo ha aprendido posteriormente es denominado "euskaldun berri" («vascohablante nuevo»).

En sentido amplio, en euskera también se ha utilizado el término "euskaldun" para referirse a personas que son vascas, independientemente de que sepan y utilicen habitualmente el euskera o no. En este caso es más preciso el término "euskal herritar" («ciudadano vasco»): hoy en día, se emplea cada vez menos el término "euskaldun" para referirse a un vasco no vascoparlante.

Por oposición, en euskera existe la forma "erdaldun" (también "erdeldun", en zonas occidentales del ámbito de la lengua), que hace alusión al hablante de un idioma diferente del euskera ("erdara" en euskera batúa, "erdera" en dialectos occidentales y centrales), habitualmente el español o el francés. A la lengua que no es euskera, cualquiera que sea, se la denomina en euskera "erdara", que etimológicamente podría equivaler a «lengua venida» (de "erdu", «venir», y "-era", «manera/modo») o bien a «cuasi lengua», (de "erdi", «casi, medio» y "-era", «manera/modo»). El hablante de "erdara" sería el "erdaldun". El uso de la forma "erdara" o "erdera" actualmente es muy común y no tiene un sentido despectivo, como podría tenerlo el «bárbaro» romano.


</doc>
<doc id="17096" url="https://es.wikipedia.org/wiki?curid=17096" title="Iain Banks">
Iain Banks

Iain Menzies Banks (Dunfermline, Fife, Escocia; 16 de febrero de 1954 - 9 de junio de 2013) fue un filólogo, filósofo, psicólogo y escritor de ciencia ficción británico.

Banks nació en Dunfermline, Fife, Escocia, el 16 de febrero de 1954. Su madre era patinadora profesional y su padre oficial del Almirantazgo. Fue hijo único. Banks vivió en el norte de Queensferry hasta los nueve años, cerca de los astilleros navales de Rosyth, donde trabajaba su padre, hasta que por motivos laborales la familia de Banks se trasladó a Gourock debido a las exigencias del trabajo de su padre. Por esa época se introdujo en la ciencia ficción a través de la saga de libros "Kemlo" de Reginald Alec Martin. 

Estudió en las escuelas secundarias de Gourock y Greenock. Después estudió filología inglesa, filosofía y psicología en la Universidad de Stirling.

Se casó en 1992 y vivió en North Queensferry, Escocia.

En abril de 2013 el autor anunció en su página oficial que estaba enfermo de cáncer enfermedad que terminó con su vida el 9 de junio del mismo año, consiguiendo terminar contrarreloj su última novela: "The Quarry" ("La cantera").

Completó su primera novela "The Hungarian Lift-Jet" a los 16 años y su segunda novela "TTR" (también conocida como "The Tashkent Rambler") durante su primer año en la Universidad de Stirling en 1972. Su primera novela publicada fue "The Wasp Factory", que se publicó en 1984 cuando tenía treinta años.

Su segunda novela, "Walking on Glass", fue publicada en 1985. "The Bridge" en 1986, y "Espedair Street" en 1987. Esta última fue adaptada por la BBC como serie radiofónica. Su libro Pensad en Flebas fue publicado en 1987 y fue la primera de varias novelas de su aclamado universo de ficción «La Cultura». 

"The Crow Road", publicado en 1992, fue adaptado como una serie de televisión de la BBC. Es una novela cuyos planteamientos entran dentro de la ciencia ficción (lugar del ser humano en el universo, el orden natural...) pero que transciende del género, siendo difícil encuadrarla, por su escritura, implicaciones y recursos.

Banks continuó escribiendo novelas de ciencia ficción y novelas de la corriente dominante, con su última novela, "The Quarry" ("La cantera"), publicada en junio de 2013, el mes de su muerte.

Banks ha citado a Robert A. Heinlein, Isaac Asimov, Arthur C. Clarke, Brian Aldiss, M. John Harrison y Dan Simmons como sus influencias literarias del género.

Banks participó en la producción teatral "The Curse of Iain Banks", escrita por Maxton Walker y representada en el festival Fringe de Edimburgo en 1999. Banks colaboró frecuentemente con el compositor de la banda sonora de la obra Gary Lloyd, incluyendo una colección de canciones que co-compusieron en homenaje a la banda de ficción 'Frozen Gold' de la novela de Banks "Espedair Street". Lloyd también compuso la partitura para una palabra hablada y la producción musical de la novela de Banks, "The Bridge" que fue escrita por el mismo Banks. Lloyd grabó a Banks para incluirlo en la obra como una voz sin cuerpo apareciendo como él mismo en uno de los sueños de uno de los miembros del elenco.

Al igual que su amigo Ken MacLeod (otro escritor escocés de ciencia ficción) mostró historias impregnadas de ideología de izquierdas. Conocido defensor de la independencia de Escocia, ha hecho campaña con el Partido Socialista Escocés.

A finales de 2004 Banks fue miembro destacado de un grupo británico compuesto por políticos y figuras públicas que hizo una campaña para acusar al entonces primer ministro, Tony Blair, de prevaricación tras la Invasión de Irak de 2003. En protesta por la misma, rompió su pasaporte y lo envió al 10 de Downing Street.

Fue miembro honorario de la Sociedad Nacional Laica, sociedad británica que promueve el laicismo.



En muchas de sus obras de ciencia ficción trata de una gran organización pangaláctica llamada La Cultura, descrita con gran lujo de detalle:

Otras novelas de ciencia ficción que no pertenecen a la serie de La Cultura son:

Banks escribió pocos relatos cortos, pero ha publicado un libro recopilatorio bajo el nombre Iain M. Banks:



Banks fue considerado en muchos círculos como el iniciador, con su serie de La Cultura, de la llamada nueva "space opera" británica.

Al igual que ocurre con Ken MacLeod (otro escritor de ciencia ficción técnica y social, también escocés y amigo de Banks), en sus escritos demostró un gran interés por la historia de la izquierda. En sus obras alegaba que una economía de la abundancia hace viable la anarquía (por no decir inevitable).



</doc>
<doc id="17097" url="https://es.wikipedia.org/wiki?curid=17097" title="World Trade Center (1973-2001)">
World Trade Center (1973-2001)

El World Trade Center (literalmente, Centro Mundial de Comercio) fue un complejo de edificios en Bajo Manhattan, ciudad de Nueva York, Estados Unidos, que incluía a las emblemáticas Torres Gemelas, inauguradas el 4 de abril de 1973, y destruidas en los atentados del 11 de septiembre de 2001, junto con el World Trade Center 7. Los otros edificios del complejo fueron dañados en los ataques y sus restos fueron posteriormente demolidos. Actualmente, el sitio está reconstruido con cinco nuevos rascacielos, un memorial a las víctimas de los ataques y una terminal de transporte. El One World Trade Center (WTC 1) es el edificio principal del nuevo complejo, con un total de 94 pisos; es el edificio más alto del hemisferio occidental.

Al momento de su finalización, los originales "World Trade Center 1" (la Torre Norte) y "World Trade Center 2" (la Torre Sur), conocidos en conjunto como las ""Torres Gemelas"", eran los edificios más altos del mundo. Los otros edificios incluían al "WTC 3 (Marriott World Trade Center)", el "WTC 4" (donde además de oficinas funcionaban diversas bolsas de valores), el "WTC 5", el "WTC 6" (que contenía a la Oficina de Aduanas y Protección Fronteriza) y el "WTC 7". Todos estos edificios fueron construidos entre 1975 y 1985, con un costo de 400 millones de dólares (2300 millones en dólares de 2014). El complejo se ubicó en el corazón del distrito financiero de Nueva York, con un espacio total de 1,24 millones de metros cuadrados para oficinas. 

El complejo fue diseñado a principios de la década de 1960 por Minoru Yamasaki y Asociados, de Troy (Míchigan), y Emery Roth e Hijos, de Nueva York. Las Torres Gemelas, de 110 pisos cada una, usaron un marco de tubo como diseño estructural. Para obtener la aprobación para el proyecto, la Autoridad Portuaria de Nueva York y Nueva Jersey accedió a tomar el Ferrocarril de Hudson y Manhattan, el cual se transformó en la Autoridad Portuaria Trans-Hudson (PATH, por sus siglas en inglés). La piedra fundamental del World Trade Center se colocó el 5 de agosto de 1966. La Torre Norte se completó en diciembre de 1972 y la Torre Sur fue finalizada en julio de 1973. El proceso de construcción incluyó la extracción de una gran cantidad de material, utilizado luego como relleno para construir la Battery Park City, en el lado oeste del Bajo Manhattan.

El restaurante Windows on the World estaba ubicado en los pisos 106 y 107 del World Trade Center 1 (la Torre Norte), mientras que la plataforma de observación "Top of the World" lo estaba en el piso 107 del World Trade Center 2 (la Torre Sur).
El World Trade Center sufrió un incendio el 13 de febrero de 1975, un atentado con bomba el 26 de febrero de 1993 y un robo el 14 de enero de 1998. En 1998, la Autoridad Portuaria decidió privatizar el World Trade Center, haciendo una licitación pública para que una empresa privada gestionase el edificio, y otorgó la licitación a Silverstein Properties en julio de 2001.

En la mañana del martes 11 de septiembre de 2001, secuestradores miembros de Al-Qaeda estrellaron dos aviones Boeing 767 contra el complejo, uno contra cada una de las torres gemelas, en un ataque terrorista coordinado. Tras arder por 56 minutos, la Torre Sur (WTC 2) se derrumbó, seguida media hora después por la Torre Norte (WTC 1). Los ataques al World Trade Center tuvieron como resultado unas 2 753 muertes. El WTC 7 se derrumbó más tarde ese mismo día, y otros edificios, a pesar de que no se derrumbaron, debieron ser demolidos debido a que el daño que presentaban era irreparable. El proceso de limpieza y recuperación del sitio llevó ocho meses.

En 1943, se propuso por primera vez la idea de establecer un (Centro Mundial de Comercio) World Trade Center en la ciudad de Nueva York. La Legislatura del Estado de Nueva York autorizó a Thomas E. Dewey, el entonces Gobernador de Nueva York, a iniciar el desarrollo de los planes para el proyecto, pero dichos planes fueron puestos en suspenso en 1949. Durante finales de los años 1950 y 1960, el crecimiento económico de Nueva York se concentró en el centro de la ciudad, Manhattan, mientras que el Bajo Manhattan fue apartado a un segundo plano. Para estimular la renovación urbana, David Rockefeller sugirió que la Autoridad Portuaria construyese el World Trade Center en el Bajo Manhattan.

Los planes iniciales, hechos públicos en 1961, identificaban a un sitio sobre el río Este como la zona de construcción del World Trade Center. Como agencia biestatal, la Autoridad Portuaria requería, para nuevos proyectos, de la aprobación tanto del Gobernador de Nueva York como del de Nueva Jersey. Robert B. Meyner, entonces Gobernador de Nueva Jersey, se opuso a que Nueva York recibiera un proyecto de 335 millones de dólares. Hacia el final de 1961, las negociaciones con Meyner, saliente Gobernador de Nueva Jersey, quedaron estancadas.

En ese entonces, la cantidad de pasajeros del Ferrocarril de Hudson y Manhattan (H&M), de Nueva Jersey, había bajado considerablemente de un pico de 113 millones de pasajeros en 1927 a 26 millones en 1958, después de que nuevos túneles y puentes para automóviles se abrieran a través del río Hudson. En una reunión de diciembre de 1961 entre Astin J. Tobin, director de la Autoridad Portuaria, y Richard J. Hughes, recientemente electo Gobernador de Nueva Jersey, la Autoridad Portuaria ofreció tomar el Ferrocarril de Hudson y Manhattan y transformarlo en el Autoridad Portuaria Trans-Hudson (PATH). La Autoridad Portuaria también decidió trasladar el proyecto del World Trade Center al sitio del edificio de la Hudson Terminal, en el lado oeste del Bajo Manhattan, una ubicación más conveniente para los pasajeros de Nueva Jersey que llegaran por el PATH. Con la nueva ubicación y la adquisición del Ferrocarril H&M por parte de la Autoridad Portuaria, Nueva Jersey aceptó apoyar el proyecto del World Trade Center.

También fue necesaria la aprobación del Alcalde de la ciudad de Nueva York, John Lindsay, y del Consejo de Nueva York. Los desacuerdos con la ciudad se centraron en los asuntos relacionados con los impuestos. El 3 de agosto de 1966, se alcanzó un acuerdo, según el cual la Autoridad Portuaria haría pagos anuales a la ciudad en lugar de pagar los impuestos correspondientes a la porción del World Trade Center licitada a arrendatarios privados. En años posteriores, los pagos fueron aumentando, al aumentar la tasa de impuesto sobre bienes raíces.

El 20 de septiembre de 1962, la Autoridad Portuaria de Nueva York y Nueva Jersey anunció la elección de Minoru Yamasaki como el arquitecto principal y de Emery Roth e Hijos como arquitectos asociados. Yamasaki ideó un plan que incorporaba al complejo dos torres gemelas; en el plan original de Yamasaki, cada una tenía 80 pisos de altura. Para alcanzar el requerimiento de la Autoridad Portuaria de que hubiese 930 000 m de espacio para oficinas, cada torre debía ser de 110 pisos de altura.

Una limitación importante en este tipo de construcciones es el tema de los ascensores; cuanto más alto sea el edificio, más ascensores se necesitan para servir al mismo, lo que consume mucho espacio. Yamasaki y los ingenieros decidieron utilizar un nuevo sistema con dos vestíbulos especiales, que permitían a los usuarios pasar de ascensores expresos de alta capacidad (se detenían solo en ciertos pisos) a un ascensor local (que se detenía en todos los pisos de una sección). Esto permitió el diseño de apilar ascensores locales dentro de un mismo hueco de ascensor. Ubicados en los pisos 44 y 78 de cada torre, los vestíbulos especiales permitían que los ascensores fuesen utilizados eficientemente, incrementando la cantidad de espacio utilizable en cada piso en un 62-75 %, al reducir el número de huecos de ascensor. En conjunto, el World Trade Center contaba con 95 ascensores, entre expresos y locales. Este sistema se inspiró en el utilizado por el Metro de Nueva York, cuyas líneas incluyen estaciones expresas, donde se detienen trenes tanto expresos como locales, y estaciones locales, donde solo se detienen trenes locales.

El diseño de Minoru Yamasaki para el World Trade Center, revelado al público el 18 de enero de 1964, mostraba para las torres una base cuadrada de aproximadamente 63 metros de cada lado. Los edificios fueron diseñados con ventanas estrechas de 46 centímetros de ancho en las oficinas, lo cual reflejaba el miedo a las alturas de Yamasaki así como su deseo de que los inquilinos se sintiesen seguros dentro de los edificios. El diseño de cada torre contaba con fachadas revestidas en aleación de aluminio, cuyas piezas se ensamblaban una a una a medida que las torres ganaban altura. El World Trade Center fue una de las implementaciones más importantes de la ética arquitectónica de Le Corbusier, al igual que la mayor expresión de las tendencias góticas modernistas de Yamasaki.

Además de las torres gemelas, el plan para el complejo World Trade Center incluía otros cuatro edificios de poca altura, que fueron construidos a principios de los años 1970. El edificio World Trade Center 7, de 47 pisos, se construyó en los años 1980, al norte del complejo principal. En conjunto, el complejo World Trade Center ocupaba una supermanzana de 65 000 m.

La compañía de ingeniería estructural Worthington, Skilling, Helle & Jackson trabajó para poner en práctica el diseño de Yamasaki, desarrollando el marco de tubo estructural usado en la fachada de las Torres Gemelas. El departamento de ingeniería de la Autoridad Portuaria cumplió la función de los ingenieros fundacionales, Joseph R. Loring & Associates la de los ingenieros eléctricos y Jaros, Baum & Bolles la de los ingenieros mecánicos del proyecto. Tishman Realty & Construction fue la contratista general en el proyecto del World Trade Center. Guy F. Tozzoli, director del World Trade Department en la Autoridad Portuaria, y Rino M. Monti, el ingeniero jefe de la Autoridad Portuaria, supervisaron el proyecto. Como agencia interestatal, la Autoridad Portuaria no estaba sujeta a leyes locales ni a regulaciones de la ciudad de Nueva York, incluyendo los códigos de edificación. Sin embargo, los ingenieros estructurales del World Trade Center terminaron siguiendo los borradores de los nuevos códigos de edificación de 1968. El diseño del marco de tubo, introducido anteriormente por Fazlur Khan, fue un nuevo enfoque que permitió realizar planos de pisos más abiertos que en el diseño tradicional, el cual distribuía columnas a través del interior para soportar las cargas del edificio. Las torres del World Trade Center utilizaban fuertes y resistentes columnas de acero perimetrales, conocidas como celosías Vierendeel, que se encontraban a poca distancia una de otra, formando así una estructura de pared fuerte y rígida, soportando prácticamente todas las cargas laterales, como las del viento, y compartiendo la carga de la gravedad con las columnas centrales. La estructura perimetral, que contenía 59 columnas por lado, fue construida con un gran uso de piezas modulares prefabricadas, cada una compuesta de tres columnas, de tres pisos de altura, conectadas por placas de antepecho. Estas placas estaban soldadas a las columnas para crear las piezas modulares fuera del lugar, en el taller de fabricación. Módulos adyacentes fueron atornillados entre sí con los empalmes en el medio del vano de las columnas y las placas de antepecho. Las placas fueron ubicadas en cada piso, transmitiendo la tensión cortante entre las columnas, permitiendo a las mismas que trabajen en conjunto para resistir cargas laterales. Las uniones entre módulos fueron escalonadas verticalmente, de modo que los empalmes de las columnas entre módulos adyacentes no se encontraran en el mismo piso.

El centro de las torres almacenaba los huecos de ascensores y de servicio, aseos, tres escaleras y otros espacios de apoyo. El centro de cada torre era un área rectangular de 27 por 41 metros y contenía 47 columnas de acero que iban desde la base hasta la cima de la torre. El gran espacio entre el perímetro y el centro, libre de columnas, fue segmentado verticalmente por vigas de piso prefabricadas. Los pisos soportaban su propio peso, al igual que cargas vivas, dando estabilidad lateral a las paredes exteriores y distribuyendo las cargas del viento entre estas paredes. Los pisos consistían en ligeras losas de hormigón de 10 centímetros de espesor, ubicadas sobre una cubierta de acero acanalada. Una red de vigas-puente ligeras y vigas principales servían de apoyo para los pisos. Las vigas se conectaban con el perímetro en columnas alternadas y se encontraban sobre bases de 2,03 metros. Las líneas superiores de las vigas fueron atornilladas a asientos soldados a las placas de antepecho en el lado exterior y a un canal soldado a las columnas centrales en el lado interior. Los pisos estaban conectados a las placas perimentrales de antepecho con amortiguadores viscoelásticos que ayudaban a reducir el balanceo percibido por los ocupantes del edificio.

Entre el piso 107 y la cima de cada torre se dispuso una gran viga, diseñada para soportar una alta antena de comunicaciones sobre la cima de cada torre. Sin embargo, solamente el WTC 1 (Torre Norte) contó con una antena, la cual fue agregada en 1978. El sistema de celosía consistía en seis cerchas dispuestas en el lado largo del núcleo, y cuatro en el lado corto, que unían el núcleo central con el entramado perimetral del edificio. Este sistema de entramado permitía la redistribución de la carga entre el perímetro y las columnas centrales, ayudando a estabilizar los esfuerzos provocados por la antena de transmisión.

El diseño del marco de tubo, que utilizaba un centro de acero y columnas perimetrales recubiertas con un material resistente al fuego, creó una estructura relativamente liviana, que se balancearía más en respuesta al viento que estructuras tradicionales, como las del edificio Empire State, que para protegerse del fuego cuentan con una mampostería gruesa y pesada, con elementos estructurales de acero. Durante el proceso de diseño, se realizaron pruebas en el túnel de viento para establecer las tensiones causadas por el viento a las que los edificios del World Trade Center podrían enfrentarse, al igual que una respuesta estructural a esas fuerzas. También se realizaron experimentos para evaluar cuánto balanceo podrían tolerar cómodamente los ocupantes; sin embargo, muchos sujetos experimentaron mareos y otros efectos adversos. Uno de los ingenieros principales, Leslie Robertson, trabajó con el ingeriero canadiense Alan G. Davenport para desarrollar amortiguadores viscoelásticos para absorber parte del balanceo. Estos amortiguadores, utilizados en las estructuras en las uniones entre las vigas de piso y las columnas perimetrales, junto con otras modificaciones estructurales, redujeron el balanceo del edificio a un nivel aceptable.

En marzo de 1965, la Autoridad Portuaria comenzó a adquirir propiedades en el emplazamiento del World Trade Center. Los trabajos de demolición comenzaron el 21 de marzo de 1966 para despejar trece manzanas de edificios de baja altura en Radio Row para la construcción del World Trade Center. La piedra fundamental de la construcción del complejo fue colocada el 5 de agosto de 1966.

El sitio del World Trade Center se encontraba en un relleno sanitario con la base rocosa ubicada 20 metros por debajo. Para construir el WTC, fue necesario hacer una "bañera" con un muro pantalla alrededor del lado del sitio que da a la calle Oeste, para mantener fuera el agua del río Hudson. El método de contención, seleccionado por John M. Kyle, Jr., ingeniero jefe de la Autoridad Portuaria, incluía la excavación de una zanja, la cual luego fue rellenada con una mezcla pastosa compuesta de bentonita y agua, que tapó los agujeros y mantuvo fuera al agua subterránea. Cuando la zanja fue cavada, se insertó en el lugar una caja de acero y se arrojó hormigón, forzando a la mezcla a salir. Tomó catorce meses completar el muro pantalla, el cual fue necesario para poder comenzar con la excavación de material del interior del sitio. Los 917 000 m de material excavado fueron utilizados (junto con otro material de rellenado) para expandir la costa de Manhattan a través de la calle Oeste, y poder así formar la Battery Park City.
En enero de 1967, la Autoridad Portuaria otorgó 74 millones de dólares en contratos a varios proveedores de acero, y Karl Koch fue contratado para erigir el acero. Tishman Realty & Construction fue contratada en febrero de 1967 para supervisar la construcción del proyecto. El trabajo de construcción comenzó con la Torre Norte en agosto de 1968; la construcción de la Torre Sur ya estaba en marcha en enero de 1969. Los originales Hudson Tubes, que llevaban a los trenes PATH a la Hudson Terminal, se mantuvieron en servicio como túneles elevados durante el proceso de construcción, hasta 1971, cuando abrió una nueva estación PATH.

La ceremonia de culminación del WTC 1 (Torre Norte) fue el 23 de diciembre de 1970, mientras que la ceremonia del WTC 2 (Torre Sur) ocurrió más tarde, el 19 de julio de 1971. Los primeros inquilinos se mudaron a la Torre Norte en diciembre de 1970; la Torre Sur aceptó inquilinos en enero de 1972. Cuando las Torres Gemelas del World Trade Center fueron completadas, los costos totales para la Autoridad Portuaria habían alcanzado los 900 millones de dólares. La ceremonia de inauguración se realizó el 4 de abril de 1973.

Los planes para construir el World Trade Center fueron polémicos. El sitio para el World Trade Center era donde se ubicaba el Radio Row, hogar de cientos de comercios e inquilinos industriales, dueños de propiedades, pequeños negocios y aproximadamente 100 residentes, muchos de los cuales presentaron una firme resistencia a la reubicación forzosa. Un grupo de pequeños negocios afectados interpusieron una medida cautelar desafiando al poder de expropiación de la Autoridad Portuaria. El caso se hizo camino a través del sistema judicial hasta llegar a la Corte Suprema de los Estados Unidos, la cual se negó a aceptarlo.

Promotores inmobiliarios privados y miembros de la Junta de Bienes Raíces de Nueva York, liderada por Lawrence A. Wien, propietario del edificio Empire State, expresaron su preocupación por esta gran cantidad de espacio para oficinas "subsidiado" que se encontraba en el mercado abierto, compitiendo con el sector privado cuando ya había un exceso de vacantes. El World Trade Center en sí no fue completamente alquilado hasta después de 1979. Otros cuestionaron si la Autoridad Portuaria realmente debería aceptar un proyecto descrito por algunos como una "equivocada prioridad social".

El diseño del World Trade Center trajo críticas por su estética de parte del Instituto Americano de Arquitectos y otros grupos. Lewis Mumford, autor de "The City in History" y otros libros sobre planeamiento urbano, criticó el proyecto y describió al mismo y a otros nuevos rascacielos como "simples archivadores de vidrio y metal". Las estrechas ventanas de oficina de las Torres Gemelas, con solo 46 centímetros de ancho y enmarcadas con pilares que restringían las vistas de cada lado a ranuras estrechas, no fueron del agrado de muchos. La activista y socióloga Jane Jacobs también criticó los planes para la construcción del WTC, argumentando que la línea costera debería quedar abierta para el disfrute de los habitantes de Nueva York.

La supermanzana del centro de comercio, en sustitución del denso vecindario, más tradicional, fue considerada por algunos críticos como un entorno inhóspito que interrumpía la complicada red de tráfico típica de Manhattan. Por ejemplo, en su libro "The Pentagon of Power", Lewis Mumford denunció al centro como un «ejemplo del gigantismo sin propósito y el exhibicionismo tecnológico que hoy destripan el tejido vivo de cada gran ciudad».

Por muchos años, la inmensa Plaza Austin J. Tobin (conocida también como Plaza World Trade Center) fue frecuentemente acosada por enérgicos vientos a nivel del suelo. De hecho, algunas ráfagas fueron tan fuertes que los peatones debieron ser ayudados por cuerdas para poder andar. En 1999, la plaza exterior reabrió tras pasar por renovaciones de unos 12 millones de dólares, que incluyeron el remplazo de adoquines de mármol por piedras grises y rosas de granito, el agregado de nuevos bancos, macetas, nuevos restaurantes, quioscos de comida y áreas de comedor al aire libre.

En la década de 1980, con la construcción del World Trade Center 7, el World Trade Center tuvo un total de siete edificios, pero los más notables fueron las dos torres principales. Cada una se levantaba con una altura de 410 metros, y ocupaba alrededor de uno de los 16 acres (65 000 m) de la tierra ocupada por el complejo. Durante una conferencia de prensa en 1973, un periodista preguntó a Yamasaki, «¿Por qué dos edificios de 110 pisos? ¿Por qué no un edificio de 220 pisos?». Su respuesta fue: «No quería perder la escala humana».

Cuando en 1972 quedó completo, el World Trade Center 1 (la Torre Norte) pasó a ser, durante dos años, el edificio más alto en el mundo, sobrepasando al edificio Empire State, el cual había ostentado el título durante 40 años. La Torre Norte tenía 417 metros de altura y presentaba una antena o mástil de telecomunicaciones que fue añadida a la azotea en 1978 y tenía 110 metros de altura. Con esta antena, el punto más alto de la Torre Norte alcanzaba los 527 metros. El World Trade Center 2 (la Torre Sur) se convirtió en el segundo edificio más alto del mundo cuando fue completado en 1973. La plataforma de observación ubicada en la azotea de la Torre Sur tenía 415 metros de altura y la plataforma de observación interior de dicha torre tenía 400 metros de altura. Las torres del World Trade Center mantuvieron el título de altura solo por un breve tiempo: la Torre Willis, en Chicago, culminada en mayo de 1973, alcanzó los 440 metros hasta la azotea. A lo largo de su existencia, sin embargo, las torres del WTC contaron con más pisos (110) que cualquier otra edificio. Este número no fue sobrepasado sino hasta la construcción del Burj Khalifa, en Dubái, que abrió en 2010.

De los 110 pisos, ocho fueron mantenidos aparte para servicios técnicos en las plantas técnicas del nivel B5/B6 (pisos 7/8, 41/42, 75/76 y 108/109), las cuales son cuatro áreas de dos pisos espaciadas uniformemente en el edificio. Todos los pisos restantes estaban libres para oficinas de planta abierta. Cada piso de las torres tenía 3 700 m de espacio para ser ocupado. Cada torre tenía 350 000 m de espacio para oficinas. En conjunto, todo el complejo de siete edificios tenía 1 040 000 m de espacio.
Inicialmente concebido como un complejo dedicado a compañías y organizaciones que directamente tomaban parte en el "mundo del comercio", en un principio no tuvo éxito en atraer a los clientes esperados. Durante los primer años, varias organizaciones gubernamentales pasaron a ser inquilinos clave del World Trade Center, incluyendo al Estado de Nueva York. No fue hasta la década de 1980 que el peligroso estado financiero de la ciudad mejoró, tras ello un creciente número de compañías privadas, principalmente firmas financieras asociadas a Wall Street, se transformaron en inquilinos. Durante la década de 1990, aproximadamente 500 compañías tenían oficinas en el complejo, incluyendo a muchas compañías financieras, tales como Morgan Stanley, Aon Corporation, Salomon Brothers y a la propia Autoridad Portuaria. La confluencia del sótano del World Trade Center incluía al Centro Comercial del World Trade Center junto con una estación del PATH. La Torre Norte se convirtió en la sede corporativa de Cantor Fitzgerald, al igual que en la sede de la Autoridad Portuaria de Nueva York y Nueva Jersey.

El servicio eléctrico de las torres fue suministrado por Consolidated Edison (ConEd) a 13 000 voltios. Este servicio pasaba a través del Centro Primario de Distribución del World Trade Center (PDP, por sus siglas en inglés) y era enviado por el centro del edificio a subestaciones eléctricas situadas en las plantas técnicas. Las subestaciones bajaban el voltaje primario, de 13 800 voltios, a un voltaje secundario, de 480/277 voltios, y posteriormente a 120/208 voltios, que era el voltaje del servicio de energía general y de alumbrado. El complejo también contaba con generadores de emergencia situados en los subniveles de las torres y en la azotea del WTC 5.

El piso 110 del World Trade Center 1 (la Torre Norte) era hogar de equipos de transmisión de radio y televisión. La azotea del WTC 1 contenía una amplia gama de antenas de transmisión, incluyendo al mástil de la antena central, de aproximadamente 110 metros, reconstruida en 1999 por Dielectric Inc. para poder funcionar con DTV. El mástil central contenía las señales de televisión de casi todas las emisoras de la ciudad de Nueva York: WCBS-TV 2, WNBC-TV 4, WNYW 5, WABC-TV 7, WPIX 11, WNET 13 Newark, WPXN-TV 31 y WNJU 47 Linden. También tenía cuatro emisoras FM de Nueva York: WPAT-FM 93.1, WNYC 93.9, WKCR 89.9 y WKTU 103.5. El acceso a la azotea era controlado desde el Centro de Control de Operaciones del WTC (OCC, por sus siglas en inglés), ubicado en el nivel B1 del WTC 2.

A pesar de que la mayor parte del espacio en el complejo del World Trade Center estaba fuera del alcance del público, la Torre Sur presentaba un área pública interior y exterior de observación, conocida como Centro de Observatorios del World Trade Center, en sus pisos 107 y 110. Los visitantes pasaban por controles de seguridad agregados después del atentado con bomba al World Trade Center de 1993, luego eran trasladados al observatorio interior en el piso 107, a una altura de 400 metros. Las columnas a cada lado del edificio fueron reducidas en este nivel para permitir que hubiesen 71 centímetros de vidrio entre ellas. La Autoridad Portuaria renovó el observatorio en 1995, y luego lo arrendó a Ogden Entertainment para que lo operase. Las atracciones agregadas a la plataforma de observación incluían un vuelo en helicóptero simulado a través de la ciudad. El área de comida del piso 107 fue diseñada con la temática de un vagón del metro y presentaba a Sbarro y a Nathan's Famous Hot Dogs. Si el clima lo permitía, los visitantes podían tomar dos cortos trayectos por escaleras desde el área de observación del piso 107 a una plataforma de observación exterior en el piso 110, a una altura de 420 metros. En un día despejado, los visitantes podían ver a más de 80 kilómetros de distancia. Una valla para prevenir suicidios fue colocada en el propio techo, ubicando la plataforma de observación hacia atrás y sobre la misma, requiriendo únicamente una barandilla y dejando la vista sin obstaculizar, al contrario de los que ocurre en la plataforma de observación del edificio Empire State.

La Torre Norte tenía un restaurante en sus pisos 106 y 107 conocido como Windows on the World, que abrió en abril de 1976. El restaurante fue desarrollado por Joe Baum con un costo de más de 17 millones de dólares. Además del restaurante principal, en la cima de la Torre Norte se ubicaban dos derivaciones: Hors d'Oeuvrerie (ofrecía un "buffet" danés durante el día y sushi en la noche) y Cellar in the Sky (un pequeño bar de vinos). Windows on the World también tenía un programa de escuela de vinos dirigido por Kevin Zraly. Windows on the World fue cerrado tras el atentado con bomba al World Trade Center de 1993. Tras reabrir en 1996, Hors d'Oeuvrerie y Cellar in the Sky fueron remplazados por el Greatest Bar on Earth y Wild Blue. En 2000, su último año completo en operación, "Windows on the World" reportó ganancias de 37 millones de dólares, haciéndolo el restaurante de mayor recaudación en Estados Unidos. El Skydive Restaurant, abierto en 1976 en el piso 44 de la Torre Norte, también era operado por el restaurante Windows on the World, pero solamente servía el almuerzo.

Alrededor del bloque de 65 000 m se ubicaban cinco edificios de menor tamaño. Uno era un hotel de 22 pisos, que abrió en 1981 como el Vista Hotel, y en 1995 se transformó en el Marriott World Trade Center (WTC 3), en el rincón sudoeste del sitio. Alrededor de la plaza también estaban tres edificios de baja altura (WTC 4, WTC 5 y WTC 6), con el mismo diseño de tubo hueco que presentaban las torres. El World Trade Center 6, en el rincón noroeste del sitio, era hogar de la Oficina de Aduanas y Protección Fronteriza de los Estados Unidos y de la Bolsa de Comercio de los Estados Unidos. El World Trade Center 5 estaba ubicado en el rincón noreste, sobre la estación del PATH, y el World Trade Center 4 estaba en el rincón sudeste. En 1987 fue construido un edificio de oficinas de 47 pisos, al norte del bloque, llamado World Trade Center 7. Debajo del complejo World Trade Center se encontraba un centro comercial subterráneo, que a su vez tenía conexiones con varios servicios importantes de transporte, incluyendo al sistema del Metro de Nueva York y a los propios trenes PATH de la Autoridad Portuaria, conectando a Manhattan con Jersey City, Hoboken y Newark.

Debajo del World Trade Center se encontraba uno de los mayores depósitos de oro del mundo, perteneciente a un grupo de bancos comerciales. La bomba de 1993 estalló cerca de la bóveda que lo contenía. Siete semanas después de los ataques del 11 de septiembre, se retiraron 230 millones de dólares en metales preciosos de las bóvedas del sótano del WTC 4, incluyendo 3 800 barras de oro de 100 onzas troy y 30 000 barras de plata de 1 000 onzas troy.

En un típico día laborable, 50 000 personas trabajaban en las torres, con otras 200 000 pasando como visitantes. El complejo era tan grande que tenía su propio código ZIP: 10048. Las torres ofrecían una gran vista desde la plataforma de observación Top of the World Trade Center Observatories, en lo alto de la Torre Sur, y desde el restaurante Windows on the World, en lo alto de la Torre Norte. Las Torres Gemelas se hicieron conocidas en todo el mundo, apareciendo en numerosas películas y programas de televisión, al igual que en postales y otros medios de comercialización, y comenzaron a ser vistas como un ícono de Nueva York, al mismo nivel que el edificio Empire State, el edificio Chrysler o la Estatua de la Libertad.

El funambulista francés Philippe Petit caminó entre las torres sobre una cuerda floja en 1974, como se muestra en la película documental "Man on Wire". Cruzó ocho veces los 43 metros que separaban los dos edificios. En total fue una aventura de 45 minutos a más de 400 metros del suelo.

George Willing, fabricante de juguetes de Brooklyn, escaló el exterior de la Torre Sur en 1977. En 1983, en el Día de los Caídos, el activista, bombero y rescatista de gran altura Dan Goodwin escaló exitosamente el exterior de la Torre Norte del WTC. Su acción estaba destinada a llamar la atención sobre la imposibilidad de rescatar personas potencialmente atrapadas en pisos superiores de rascacielos.

El Campeonato Mundial de Ajedrez de 1995 se jugó en el piso 107 de la Torre Sur.

El 13 de febrero de 1975, se desató un incendio en la Torre Norte del WTC, que se propagó a lo largo de la planta 11. Este incendio se extendió a través del núcleo a las plantas 9 y 14 tras incendiarse el aislamiento de los cables de teléfono ubicados en un hueco de servicio que corría verticalmente entre los pisos. El fuego que llegó a otras áreas fue extinguido casi inmediatamente, y el incendio original fue apagado en pocas horas. La mayoría de los daños se concentraron en el piso 11, debido a que el fuego fue alimentado por armarios llenos de papel, líquido a base de alcohol para máquinas de oficina y otro material de oficina. No hubo daños estructurales para la torre, ya que el acero estaba recubierto por un material resistente al fuego. Sin ser los daños causados por el fuego, algunos pisos de abajo sufrieron daños causados por el agua utilizada para la extinción del incendio de arriba. En esa época, el World Trade Center no tenía sistemas rociadores de incendios.

El 26 de febrero de 1993, a las 12:17, un camión de la empresa Ryder que contenía 680 kilogramos de explosivos, estacionado por Ramzi Yousef, explotó en el estacionamiento subterráneo de la Torre Norte. La explosión abrió un agujero de 30 metros a través de cinco subniveles, ocurriendo el mayor daño en los niveles B1 y B2 y un daño estructural considerable en el nivel B3. A lo largo de los 110 pisos de la torre, seis personas murieron y otros 50 000 trabajadores y visitantes quedaron faltos de aire para respirar normalmente. Muchas personas dentro de la Torre Norte fueron forzadas a descender a oscuras por escaleras que no contaban con iluminación de emergencia, algunas demorando más de dos horas para ponerse a salvo.

Tras el atentado, Yousef huyó a Pakistán, pero fue detenido en Islamabad en febrero de 1995 y extraditado a Estados Unidos para ser sometido a juicio. El jeque Omar Abdel Rahman fue condenado en 1996 por participar en el atentado y en otras conspiraciones. Yousef y Eyad Ismoil fueron condenados en noviembre de 1997 por llevar a cabo el atentado. Otras cuatro personas fueron condenadas en mayo de 1994 por su participación en el mismo. Según un juez del Tribunal Supremo, el principal objetivo del ataque era desestabilizar la Torre Norte y hacerla caer sobre la Torre Sur, derribándola.

Los pisos afectados por el atentado debieron ser reparados para restablecer el apoyo estructural que brindaban a las columnas. El muro pantalla estaba en peligro tras el atentado y perdió las losas del suelo que proporcionaban el apoyo lateral contra la presión ejercida del lado opuesto por las aguas del río Hudson. La planta de refrigeración en el subnivel 5, que proporcionaba el servicio de aire acondicionado a todo el complejo World Trade Center, fue seriamente dañada. Después del atentado, la Autoridad Portuaria instaló señales fotoluminiscentes en las escaleras. El sistema de alarma contra incendios del complejo entero debió ser remplazado como consecuencia de que parte importante del cableado y de la señalización del sistema original fue destruido. Como un monumento a las víctimas del atentado a la torre, se instaló un espejo de agua con los nombres de aquellos que murieron en la explosión. Sin embargo, el monumento fue destruido en los ataques del 11 de septiembre. Los nombres de las víctimas del atentado de 1993 fueron incluidos en el Museo y Memorial Nacional del 11 de septiembre.

En enero de 1998, Ralph Guarino, miembro de la Mafia que había obtenido un acceso de mantenimiento al World Trade Center, formó una pandilla de tres hombres para robar más de 2 millones de dólares, que eran trasladados de un camión de la empresa Brinks al piso 11 del WTC.

En 1998, la Autoridad Portuaria aprobó planes para privatizar el World Trade Center. En 2001, la Autoridad Portuaria buscó arrendar el World Trade Center a una entidad privada. Las ofertas de licitación vinieron de Vornado Realty Trust, de una propuesta conjunta entre Brookfield Properties Corporation y Boston Properties y de otra propuesta conjunta entre Silverstein Properties y Westfield Group. La privatización del World Trade Center lo agregaría a la lista de contribuyentes y otorgaría fondos para otros proyectos de la Autoridad Portuaria. El 15 de febrero de 2001, la Autoridad Portuaria anunció que Vornado Realty Trust había conseguido el contrato de arrendamiento del World Trade Center, a través del pago de 3,25 mil millones de dólares por el mismo, de 99 años de duración. Vornado Realty ofreció 600 millones de dólares más que Silverstein, a pesar de que Silverstein aumentó su oferta a 3,22 mil millones. Sin embargo, Vornado insistió en realizar cambios de última hora al acuerdo, lo que incluyó un contrato de arrendamiento más corto, de 39 años, que la Autoridad Portuaria consideró innegociable. Posteriormente, Vornado se retiró, y la oferta de Silverstein por el contrato de arrendamiento del World Trade Center fue aceptada el 26 de abril de 2001, y cerrada el 24 de julio de 2001.

El 11 de septiembre de 2001, un grupo de terroristas islámicos secuestraron el vuelo 11 de American Airlines y lo estrellaron contra la fachada norte de la Torre Norte, a las 8:46:40 h; el avión impactó entre los pisos 93 y 99. Diecisiete minutos después, a las 9:02:59, un segundo equipo de terroristas estrellaron el también secuestrado (y de manera similar al anterior) vuelo 175 de United Airlines contra la Torre Sur; el impacto ocurrió entre los pisos 77 y 85. El daño causado a la Torre Norte por el vuelo 11 destruyó toda vía de escape desde arriba del área de impacto, atrapando a 1 344 personas. El vuelo 175, en comparación con el del vuelo 11, tuvo un impacto mucho menos centrado, y una única escalera quedó intacta; sin embargo, solo unas pocas personas lograron pasar por la misma exitosamente antes de que la torre se derrumbara. A pesar de que la Torre Sur fue impactada más abajo que la Torre Norte, afectando así a más pisos, un número menor de personas, algo menos que 700, murieron instantáneamente o quedaron atrapadas.

A las 9:59, la Torre Sur se derrumbó, tras arder aproximadamente 56 minutos. El fuego provocó que los elementos estructurales de acero, ya debilitados por el impacto del avión, fallaran. La Torre Norte se derrumbó a las 10:28, tras arder aproximadamente 102 minutos. A las 17:20 del 11 de septiembre de 2001, el World Trade Center 7 comenzó a derrumbarse, con la caída del alero este, y se derrumbó completamente a las 17:21, debido a que el incendio descontrolado causó una falla estructural.

El World Trade Center 3, un hotel Marriott, fue destruido durante el derrumbe de las dos torres. Los tres edificios restantes en la plaza WTC fueron seriamente dañados por los escombros y fueron luego derrumbados. El edificio del Deutsche Bank, al otro lado de la calle Liberty desde el complejo World Trade Center, fue luego condenado debido a las inhabitables condiciones tóxicas de su interior; fue deconstruido, completándose el trabajo a principios de 2011. El Fiterman Hall de la Comunidad Universitaria del Municipio de Manhattan, ubicado en el número 30 de Broadway Oeste, también fue condenado, debido al serio daño recibido en los ataques, y su deconstrucción está programada.
Inmediatamente después de los ataques, los reportes de la prensa sugerían que decenas de miles de personas podrían haber muerto en los ataques, ya que más de 50 000 podrían estar adentro de las torres. Recientemente, fueron presentados 2753 certificados de defunción (excluyendo a aquellos de los secuestradores) en relación a los ataques del 11 de septiembre en Nueva York, incluso uno presentado para Felicia Dunn-Jones, quien fue agregada a la cifra oficial de muertos en mayo de 2007; Dunn-Jones murió cinco meses después por una afección pulmonar conectada con su exposición al polvo durante el derrumbe del World Trade Center. Otras tres víctimas fueron luego añadidas a la cifra oficial de muertos por la oficina del forense de la ciudad: la Dra. Sneha Anne Philip, vista por última vez el día anterior a los ataques, Len Heyward, un hombre que desarrolló un linfoma y luego murió en el 2008 como resultado de la aspiración de polvo durante los eventos posteriores a los ataques a las Torres Gemelas, y Jerry Borg, quien murió en diciembre de 2010 de sarcoidosis pulmonar, la cual se determinó en junio de 2011 como resultado del polvo de los ataques. Cantor Fitzgerald L.P., un banco inversor ubicado en los pisos 101 y 105 del One World Trade Center, perdió 658 empleados, cifra considerablemente mayor que la de cualquier otro empleador, mientras que Marsh & McLennan Companies, ubicado inmediatamente debajo de Cantor Fitzgerald, en los pisos 91-101 (el sitio de impacto del vuelo 11), perdió 295 empleados, y murieron 175 empleados de Aon Corporation. Además, 343 de los muertos eran bomberos de la ciudad de Nueva York, 84 eran empleados de la Autoridad Portuaria, de los cuales 37 eran miembros del Departamento de Policía de la Autoridad Portuaria, y otros 23 eran oficiales del Departamento de Policía de la Ciudad de Nueva York. Diez años después de los ataques, solo 1 629 víctimas han sido identificadas. De todas las personas que aún se encontraban en las torres cuando se derrumbaron, solo 20 fueron rescatadas con vida. Will Jimeno y el Sgto. John McLoughlin, oficiales de policía de la Autoridad Portuaria, fueron los supervivientes 18 y 19. La última de quienes sobrevivieron al colapso fue encontrada tras 27 horas de trabajos de rescate. Tras eso, un número desconocido de personas que pudieron haber sobrevivido al derrumbe tal como los 20 que pudieron ser rescatados, perdieron la vida con el paso de las horas.

Inmediatamente después de la destrucción del complejo original, se inició el debate sobre la construcción de un nuevo complejo que lo sustituyera. En noviembre de 2001, el gobernador Pataki creó la Lower Manhattan Development Corporation (LMDC), una comisión oficial para supervisar el proceso de reconstrucción. En un comunicado de prensa de agosto de 2002, la LMDC anunció que se realizaría un concurso de diseño para el plan maestro del nuevo World Trade Center. El 27 de febrero de 2003, el Studio Daniel Libeskind fue declarado ganador del concurso y se convirtió así en el arquitecto del nuevo World Trade Center. A pesar de que Libeskind diseñó el plan maestro del complejo, los edificios serían diseñados por diferentes arquitectos.

La propuesta original de Libeskind, titulada "Memory Foundations" («Cimientos de la memoria»), fue sometida a extensas revisiones en colaboración con Silverstein y Skidmore, Owings & Merrill, el estudio de arquitectura contratado por Silverstein. Aunque no todas las ideas de Libeskind se mantuvieron en el diseño final, consiguió que se respetara su idea de que las huellas de las Torres Gemelas debían convertirse en un memorial y no ser usadas para fines comerciales gracias al apoyo que cosechó del público. En el diseño del nuevo del World Trade Center había numerosas partes interesadas, incluidos Silverstein y la Autoridad Portuaria. Además, las familias de las víctimas, los residentes de los barrios de los alrededores y otros colectivos querían participar en las decisiones. Las negociaciones sobre el diseño definitivo del complejo se prolongaron durante varios años y se han considerado la operación inmobiliaria más compleja de la historia debido a la complejidad de los temas a tratar, los numerosos grupos de interés involucrados y la dificultad de llegar a un consenso.

Tras años de retrasos y controversia, en marzo de 2006 empezó la reconstrucción del World Trade Center. El nuevo complejo incluye al One World Trade Center, 3 World Trade Center, 4 World Trade Center, 7 World Trade Center y otro rascacielos de oficinas, el 2 World Trade Center, cuyas obras están paradas. El nuevo World Trade Center también incluye un museo y memorial, y un centro de transporte de tamaño similar a la Grand Central Terminal.

El One World Trade Center, que tiene 541 metros de altura y es el edificio principal del complejo, se completó el 30 de agosto de 2012 y el último componente de su aguja se instaló el 10 de mayo de 2013. El 4 World Trade Center abrió sus puertas en noviembre de 2014, y el 7 World Trade Center, primer edificio construido del complejo, se inauguró el 23 de mayo de 2006. El National September 11 Memorial & Museum está terminado: el museo abrió sus puertas el 21 de mayo de 2014, y el memorial el 11 de septiembre de 2011. El World Trade Center Transportation Hub abrió al público el 4 de marzo de 2016. El 3 World Trade Center inaugurado el 11 de junio de 2018. La construcción del 2 World Trade Center se paralizó en 2009, y en 2015 se anunció un nuevo diseño.

Durante los esfuerzos de limpieza, se recuperaron un pequeño número de banderas estadounidenses que, durante los ataques, ondeaban cerca del World Trade Center. Una fue encontrada por Gerald Kane, sargento del Departamento de Policía de la Ciudad de Nueva York, y el detective Peter Friscia, a las 5:30 de la mañana del 12 de septiembre de 2001. Mientras colaboraban con los equipos de rescate en la "Zona Cero", notaron que la gran bandera estadounidense que una vez ondeara frente al WTC, en la calle Church, había sido separada del asta durante el derrumbe de los edificios y colgaba al revés de un poste del alumbrado público, a varias cuadras de distancia. Los dos hombres reclutaron a varios soldados y bomberos en el área, quienes alzaron una escalera hasta lo alto del poste. El detective Friscia subió los peldaños de la escalera hasta la cima, recuperó la bandera, la descolgó y la bajó a la calle. Luego, Kerik dio la bandera a oficiales de la NASA, y la misma fue transportada en el transbordador espacial Endeavour (STS-108), como parte de su misión del 5-17 de diciembre de 2001 a la Estación Espacial Internacional. El 14 de enero de 2002, en el Día de la Bandera, la bandera estadounidense fue regresada a los habitantes de la ciudad de Nueva York por Sean O'Keefe, de la NASA, y por el comandante Dom Gorie y los miembros de la tripulación del Endeavour, en una ceremonia en el Rose Center del Museo Americano de Historia Natural. La bandera es asegurada y mantenida por el Comisionado de Registros de la ciudad de Nueva York y es parte de la ceremonia anual del 11-S, en la Zona Cero.

Otra bandera, que originalmente ondeaba en lo alto de una de las torres, fue recuperada tres días después de los ataques, y, debido a su mal estado, fue entregada a la Guardia Nacional para su destrucción ceremonial. Luego, la Guardia Nacional descubrió, gracias a una etiqueta, que la bandera pertenecía a la Autoridad Portuaria, y en lugar de retirarla, la devolvió. La misma fue luego usada durante la ceremonia de homenaje a varios de los socorristas, y se la hizo ondear durante las Series Mundiales de 2001 y el Super Bowl XXXVI. La bandera fue también utilizada, con polémica, en la ceremonia de apertura de los Juegos Olímpicos de invierno de 2002, llevados a cabo en Salt Lake City. En un principio, el Comité Olímpico Internacional se negó a permitir que la bandera fuera utilizada en la ceremonia, temiendo que se viera como demasiado proestadounidense, extremadamente patriótico, y que creara complicaciones durante las futuras ceremonias. Pero finalmente se llegó a un acuerdo entre los organizadores del evento y el IOC, el cual permitió que la bandera fuera llevada al Estadio Olímpico Rice-Eccles tanto por atletas estadounidenses como por socorristas del 11-S. La bandera fue llevada al estadio en silencio, tras lo cual los portadores de la bandera se detuvieron frente al Coro del Tabernáculo Mormón, permitiéndoles cantar "The Star-Spangled Banner", mientras otra bandera estadounidense era izada.

También se recuperaron varias banderas de otros países del sitio del World Trade Center. Se presentó una bandera de Nueva Zelanda al entonces primer ministro del país, Helen Clark, por representantes del Departamento de Bomberos de la Ciudad de Nueva York. Dos de las víctimas eran neozelandesas. Conservadores montaron y enmarcaron la bandera, la cual ahora cuelga en la escalera principal de la Casa del Parlamento, el edificio principal del complejo del Parlamento de Nueva Zelanda. Una bandera de la Unión, del Reino Unido, también fue recuperada, y actualmente se encuentra en el Imperial War Museum North de Manchester, Inglaterra. El Reino Unido sufrió el mayor número de víctimas después de Estados Unidos, con entre 60 y 70 británicos asesinados durante los ataques.

El World Trade Center original fue una construcción simbólica que apareció en numerosas películas, así como en muchos programas de televisión, dibujos animados, cómics, videojuegos y vídeos musicales. Quizás las películas más destacables en las que aparece sean "Godspell" (1973), "Raise the Titanic" (1980), la segunda película de "King Kong" (1976), "Trading Places" (1983) "Working Girl" (1988), "" (1992), "Daylight" (1996), "Godzilla" (1998) y "The Walk" (2015).

Los acontecimientos sobre los atentados del 11 de septiembre fueron retratados en varios documentales y películas, incluidas dos importantes películas realizadas en 2006, "World Trade Center", de Oliver Stone, y "United 93", de Paul Greengrass. En varias películas estrenadas después del 11 de septiembre de 2001 se decidieron eliminar las torres digitalmente. Tras los atentados, la mayoría de los programas reemitidos por la televisión optaron por no exhibir las torres, cortando las partes donde las mismas aparecían, como sucedió con algunos episodios de "Friends" y "Los Simpson". Las tomas del World Trade Center fueron retiradas de las secuencias de apertura de las series de HBO "Sex and the City" y "Los Soprano", en episodios realizados después de la destrucción de los edificios, como una señal de respeto por las víctimas del 11 de septiembre. En el final de temporada de la serie de la Fox "Fringe", el World Trade Center se ve intacto en un universo paralelo de la ciudad de Nueva York.

La trama del episodio piloto de "The Lone Gunmen" (un "spin-off" de "Expediente X"), que salió al aire en marzo de 2001 (seis meses antes de los ataques), se centra en un complot dirigido por servicios secretos del gobierno para hacer estrellar un avión de pasajeros contra una de las Torres Gemelas, utilizando el piloto automático. La idea era hacer recaer la responsabilidad del hecho en un país extranjero para lograr aumentar el presupuesto de defensa militar. En el capítulo, el objetivo es frustrado por los protagonistas, quienes suben en el condenado avión y desactivan el malicioso sistema de pilotaje automático segundos antes de que el avión se estrelle contra el World Trade Center.

Habían pensado incluir a las torres en la película "Spiderman", estrenada en Estados Unidos en mayo del 2002. De hecho, en el primer avance un grupo de ladrones escapan en un helicóptero de un robo a un banco, pero luego son atrapados por una gran telaraña tendida entre las Torres Gemelas del World Trade Center original. Tras los ataques, este avance salió de circulación, y toda referencia a las torres en la película desapareció. También cabe destacar una breve aparición de las Torres Gemelas en el OVA 1 de "Shin Getter Robo vs Neo Getter Robo", publicado en 2001.





</doc>
<doc id="17100" url="https://es.wikipedia.org/wiki?curid=17100" title="Ácido nucleico">
Ácido nucleico

Los ácidos nucleicos son grandes polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar tamaños gigantescos, de millones de nucleótidos encadenados. Existen dos tipos básicos, el ADN y el ARN.

El descubrimiento de los ácidos nucleicos se debe a Johan Friedrich Miescher que, en el año 1869, aisló de los núcleos de las células una sustancia ácida a la que llamó "nucleína", nombre que posteriormente se cambió a ácido nucleico. Posteriormente, en 1953, James Watson y Francis Crick descubrieron la estructura del ADN a partir de la Fotografía 51, realizada por Rosalind Franklin empleando la técnica de difracción de rayos X.

Todos los organismos poseen estas biomoléculas que dirigen y controlan la síntesis de sus proteínas,
proporcionando la información que determina su especificidad y características biológicas,(para eso esta el ácido ribonucleico (ARN), que son las participan en la síntesis de las proteínas y el,ácido desoxirribonucleico (ADN) que codifica la información para crear las proteínas), ya que contienen las instrucciones necesarias para realizar los procesos vitales y son los responsables de todas las funciones básicas en el organismo.

Existen dos tipos de ácidos nucleicos : ADN (ácido desoxirribonucleico) y ARN (ácido ribonucleico), que se diferencian:

Las Bases Nitrogenadas son las que contienen la información genética, estas presentan una estructura cíclica que contiene carbono, nitrógeno, hidrógeno y oxígeno. Se dividen en tres tipos:

La presencia de los átomos de nitrógeno le da un carácter básico a estos compuestos. Son aromáticas y por lo tanto son planas, también son insolubles en agua y pueden establecer interacciones hidrofóbicas entre ellas; estas interacciones sirven para estabilizar la estructura tridimensional de los ácidos nucleicos.
La existencia de distintos radicales hace que puedan aparecer varias bases nitrogenadas, las cuales son:


Un nucleósido es una unidad conformada por una pentosa (ribosa o desoxirribosa) unida a una base nitrogenada. La unión se realiza mediante un enlace "N"-glucosídico, con configuración beta (β), el cual es una variante del enlace glucosídico, que se forma cuando un hemicetal intramolecular reacciona con una amina, en lugar de hacerlo con un alcohol, liberándose una molécula de agua. En los nucleósidos se lleva a cabo entre el carbono 1 (carbonilo) del azúcar y uno de los átomos de nitrógeno de la base nitrogenada, si esta es una pirimidina se une a la posición 1' y si es una purina en la posición 9'.

Los planos de la base y el azúcar son perpendiculares entre sí pero las bases pueden presentar dos conformaciones diferentes:

Existen dos tipos de nucleósidos:

Para nombrar estos compuestos se debe tomar en cuenta qué base nitrogenada es y a qué azúcar está unida; cuando es una base púrica se añade al nombre de esta la terminación “-osina” y la terminación “-idina” si es una pirimidina y se antepone el prefijo “desoxi-” en el caso de los desoxirribonucleósidos.

Los nucleótidos son las unidades básicas de los ácidos nucleicos y químicamente son los ésteres fosfóricos de los nucleósidos, es decir que son el resultado de la unión entre una ribosa, una base nitrogenada y un ácido fosfórico. La unión entre el nucleósido y el ácido fosfórico se lleva a cabo mediante un enlace éster que puede producirse en cualquiera de los grupos hidroxilo libres de la pentosa, pero como regla general tiene lugar en el grupo alcohol del carbono 5'. Los nucleótidos pueden contener de uno a tres grupos fosfato, unidos uno tras otro, por ejemplo el monofosfato que solo contienen un grupo fosfato, el difosfato con dos, trifosfato con tres. La presencia del grupo fosfato que a pH 7 se encuentra ionizado, le confiere a la molécula un carácter marcadamente ácido.

Al igual que los nucleósidos, los nucleótidos también se dividen en dos grupos dependiendo de la ribosa que contenga:

Para nombrar estos compuestos existen diferentes maneras, la forma más utilizada y la más sencilla es en donde cada nucleótido se identifica con tres letras mayúsculas. La primera de ellas corresponde a la base nitrogenada que contenga el nucleótido, la segunda letra indica si es un mono-, di- o trifosfato y la tercera es la inicial del grupo fosfato, la cual es una P y por último, en el caso de los desoxirribonucleótidos se antepone una d minúscula antes de las tres letras. Otra forma de nombrarlos consiste en poner la palabra ácido al inicio y en seguida se coloca el nombre de la base nitrogenada con la terminación -ílico, pero este sistema de nomenclatura puede ser un poco ambiguo ya que no se puede saber la cantidad de grupos fosfatos que contiene el nucleótido. También se suelen nombrar como los fosfatos de los correspondientes nucleósidos.

Por ejemplo: Se quiere nombrar el nucleótido compuesto de una adenina con un grupo fosfato y una ribosa.

Además de formar la estructura de los ácidos nucleicos los nucleótidos tienen
otras funciones relevantes:


El ADN es bicatenario, está constituido por dos cadenas polinucleotídicas unidas entre sí en toda su longitud. Esta doble cadena puede disponerse en forma lineal (ADN del núcleo de las células eucarióticas) o en forma circular (ADN de las células procarióticas, así como de las mitocondrias y cloroplastos eucarióticos). La molécula de ADN porta la información necesaria para el desarrollo de las características biológicas de un individuo y contiene los mensajes e instrucciones para que las células realicen sus funciones. Dependiendo de la composición del ADN (refiriéndose a composición como la secuencia particular de bases), puede desnaturalizarse o romperse los puentes de hidrógenos entre bases pasando a ADN de cadena simple o ADNsc abreviadamente.

Excepcionalmente, el ADN de algunos virus es monocatenario.

El ADN es un polímero relativamente estable. Las reacciones espontáneas, como la desanimación de ciertas bases, la hidrólisis de los enlaces base-azúcar "N"-glucosídicos, la formación de dímeros de pirimidina inducida por radiación, ocurren lentamente, pero son importantes debido a que la célula tiene una baja tolerancia a los cambios en el material genético.

Se puede determinar la secuencia del ADN y se pueden sintetizar polímeros de ADN por un reglamento que incorpora métodos químicos y enzimáticos.


El ARN difiere del ADN en que la pentosa de los nucleótidos constituyentes es ribosa en lugar de desoxirribosa, y en que, en lugar de las cuatro bases A, G, C, T, aparece A, G, C, U (es decir, uracilo en lugar de timina). Las cadenas de ARN son más cortas que las de ADN, aunque dicha característica es debido a consideraciones de carácter biológico, ya que no existe limitación química para formar cadenas de ARN tan largas como de ADN, al ser el enlace fosfodiéster químicamente idéntico. El ARN está constituido casi siempre por una única cadena (es monocatenario), aunque en ciertas situaciones, como en los ARNt y ARNr puede formar estructuras plegadas complejas y estables.

Mientras que el ADN contiene la información, el ARN expresa dicha información, pasando de una secuencia lineal de nucleótidos, a una secuencia lineal de aminoácidos en una proteína. Para expresar dicha información, se necesitan varias etapas y, en consecuencia existen varios tipos de ARN:




El ADN y el ARN pueden desnaturalizarse.

La elevación de la temperatura y los valores extremos de pH producen la desnaturalización del ADN de doble hélice (generalmente sucede a la temperatura de su punto de fusión). Esto provoca el desenrrollamiento de la doble hélice, debido a las desestabilización de los puentes de hidrógeno entre los pares de bases, no hay ruptura de enlaces covalentes.

La renaturalización es un proceso rápido que consiste en un solo paso, para esto deberá existir un segmento de doble hélice de una docena o más residuos que mantengan unidas las dos hebras. Cuando el pH y la temperatura regresan a valores normales, lo que estaba desenrrollado se vuelve a enrollar espontáneamente. Pero si las dos hebras están totalmente separadas, se lleva a cabo en dos pasos. En el primero, el proceso es lento, las hebras de ADN se reconocen al azar y forman un pequeño fragmento de doble hélice. En el segundo, el proceso es más rápido y las bases que se encuentran no apareadas, se aparean progresivamente para formar la doble hélice.

Efecto hipocrómico.

Cuando se dan interacciones próximas del apilamiento de las bases de los ácidos nucleicos, estos producen una disminución de la absorción del la luz UV, en relación con la absorción de una disolución de nucleótidos libres de la misma concentración; la adsorción disminuye cuando se forma la doble cadena. A este fenómeno se le conoce como efecto hipocrómico.

Cuando se desnaturaliza un ácido nucleico se produce un efecto contrario, hay un incremento de adsorción, se le llama hipercrómico.

Las moléculas de ADN de un virus o de una bacteria en disolución se desnaturalizan en su punto de fusión ("t" es la temperatura a la que la mitad del ADN, las hebras están separadas).  Dependiendo del contenido de C≡G, es mayor el punto de fusión, debido a que son tres puentes de hidrógeno los que se deben romper.

Existen, aparte de los naturales, algunos ácidos nucleicos no presentes en la naturaleza (análogos de ácidos nucleicos), sintetizados en el laboratorio.








</doc>
<doc id="17105" url="https://es.wikipedia.org/wiki?curid=17105" title="Philip Glass">
Philip Glass

Philip Glass (Baltimore, Maryland, 31 de enero de 1937) es un compositor de música clásica minimalista estadounidense. Estudió en la Juilliard School de Nueva York. Su reconocimiento internacional aumentó desde la aparición de su ópera "Einstein on the Beach" (1975).

Prolífico compositor, ha trabajado en diversos ámbitos como la ópera, la música orquestal, la música de cámara o el cine. Trabaja habitualmente con el Philip Glass Ensemble. Ha colaborado con Paul Simon, Linda Ronstadt, Yo-Yo Ma, Doris Lessing y Robert Wilson.

Philip Glass es nieto de inmigrantes judíos originarios de Lituania. Nació y creció en Baltimore y de niño estudió flauta en el conservatorio Peabody.

A los 15 años comenzó un curso acelerado en la Universidad de Chicago donde estudió matemáticas y filosofía. A los 19 años obtuvo su diploma y entró a la Juilliard School de Nueva York donde tuvo como profesor a Darius Milhaud y empezó a tocar casi exclusivamente el piano. Dada su insatisfacción, desde 1963 hasta 1965 partió a estudiar a París, con Nadia Boulanger, en el Conservatorio americano de Fontainebleau el análisis de composiciones de Johann Sebastian Bach ("El clave bien temperado"), Mozart ("los conciertos para piano"), a Wagner y Beethoven. Glass descubre asimismo el serialismo de Pierre Boulez, pero afirmó que no le produjo «ninguna emoción». Este periodo en París le sirvió para descubrir el teatro de Jean-Louis Barrault en el Odéon así como la Nouvelle Vague francesa.

Tras estudiar con Nadia Boulanger y trabajar estrechamente con Ravi Shankar en Francia, Glass viajó en 1966 al norte de la India, principalmente por razones religiosas, donde entró en contacto con los refugiados tibetanos. Se hizo budista y conocería al Dalái Lama en 1972 así como al poeta Allen Ginsberg, quien es un gran defensor de la causa tibetana. Fue su trabajo con Ravi Shankar y su percepción del ritmo aditivo en la música india, lo que le condujo a su singular estilo. 
Cuando volvió a Nueva York en 1967, renunció a todas sus composiciones anteriores al estilo de Darius Milhaud y de Aaron Copland y empezó a escribir piezas austeras basadas en ritmos aditivos y con un sentido del tempo influido por Samuel Beckett, cuyo trabajo descubrió componiendo para obras de teatro experimentales.

El poco aprecio que siente hacia los intérpretes y los espacios tradicionales lo llevan a formar su propio grupo musical, el Philip Glass Ensemble, con el que empieza a tocar principalmente en galerías de arte y en otros ambientes de la cultura underground. Estos tiempos durísimos, que abarcaron casi la totalidad de la década de los 70, le obligaron a trabajar como taxista y reparador de electrodomésticos a la vez que componía e interpretaba.

La música de esta primera época es extremadamente repetitiva, austera y complicada para el oyente, lo que le supuso una gran incomprensión por parte de la crítica y el público. El propio Glass comentaba que cuando alguien del público se quedaba hasta el final, le invitaban a cenar. Sólo empezó a ser reconocido a partir de su colaboración con el director escénico y renovador teatral, también minimalista, Robert Wilson (director), con quien realizó la ópera experimental "Einstein on the Beach", un alegato antinuclear con libreto escrito por un psicótico donde cada elemento clásico del género operístico se ve renovado y alterado de modo consciente. Aun así, a pesar del relativo éxito de la obra, tiene que seguir trabajando como reparador durante un tiempo antes de poder dedicarse totalmente a la música.

Habitualmente, Philip Glass rehúsa encuadrar sus creaciones dentro del estilo minimalista, pero se ha definido a sí mismo como un compositor de música que se basa en estructuras repetitivas.

La realización de nuevas óperas así como una dulcificación de su estilo a principios de los 80, más accesible para el gran público, hizo avanzar la fama de Glass, así como su relevancia dentro de la cultura musical alternativa. Los primeros escarceos con músicos pop (como Mike Oldfield en su LP "Platinum", donde interpreta una pieza de Glass) contribuyeron a darle a conocer en círculos más amplios.

Es posible que la fama a nivel mundial y cierto estatus de genio le llegara a través de la película experimental "Koyaanisqatsi", dirigida por Godfrey Reggio (1981-1982) y producida por Francis Ford Coppola. Algunas piezas que no se usaron en la película (como "Facades") al final aparecieron en el álbum "Glassworks" (1982, CBS Records), que llevó la música de Glass a un público más amplio.

La "trilogía de retratos" se terminó con "Akhnaten" (1982–1983, estrenada en 1984), una composición vocal y orquestal cantada en acadio, hebreo bíblico y egipcio antiguo. Además, esta ópera presentaba a un actor recitando antiguos textos egipciones en el idioma del público. Glass colaboró de nuevo con Robert Wilson en otra ópera, "" (1983, estrenada en 1984), que también funcionó como la parte final ("la sección de Roma") de la obra épica de Wilson del mismo nombre, originalmente planeada para un "festival artístico internacional que acompañaría a los Juegos Olímpicos en Los Ángeles". (Glass también compuso una obra muy prestigiosa para coro y orquesta para la inauguración de los Juegos, "The Olympian: Lighting of the Torch and Closing "). El estreno de "The CIVIL warS" en Los Ángeles nunca tuvo lugar y la ópera al final se estrenó en la Ópera de Roma. La ópera de Glass y Wilson incluye música puesta a textos latinos del dramaturgo del siglo I, Séneca y alusiones a música de Giuseppe Verdi y de la Guerra Civil Americana, presentando a figuras del siglo XIX Giuseppe Garibaldi y Robert E. Lee como personajes.

A pesar de su fama, su reconocimiento como compositor no es unánime; algunos melómanos e incluso varios compositores contemporáneos suyos tales como Ned Rorem o Milton Babbitt, cuestionan su obra por falta de rigor y consideran su música empalagosa y superficial.

Otras obras vocales de los años ochenta fueron dos conjuntos de canciones, "Three Songs for chorus" (1984, sobre poemas de Leonard Cohen, Octavio Paz y Raymond Levesque), y un ciclo de canciones iniciado por CBS Masterworks Records: "Songs from Liquid Days" (1985), con textos de letristas como David Byrne, Paul Simon, en que el Cuarteto Kronos aparece con un papel destacado (como en "Mishima"). Glass también continuó con su serie de óperas con adaptaciones de textos literarios como "The Juniper Tree" (una ópera en colaboración con el compositor Robert Moran, 1984), "La caída de la casa Usher" de Edgar Allan Poe (1987), y también trabajó con la novelista Doris Lessing en la ópera "The Making Of The Representative For Planet 8" (1985–86, e interpretada por la Houston Grand Opera y la Ópera Nacional Inglesa en 1988).

Durante el resto de los años 80 siguió produciendo música en solitario y con su grupo pero no escatimó en colaboraciones con otros músicos, tanto pop como minoritarios o de otras culturas, y en la realización de música de cine. Desde entonces y hasta la actualidad Glass ha orquestado algunas partes instrumentales de los discos de David Bowie "Low" y "Heroes" ("Low Symphony" y "Heroes Symphony") así como muchas películas; el "biopic" dirigido por Errol Morris "A Brief History of Time" (basado en el libro divulgativo de física de Stephen Hawking); Mishima, de Paul Schrader o "Kundun", de Martin Scorsese.

A finales de los años ochenta y principios de los noventa, los proyectos de Glass incluyeron dos encargos de ópera muy prestigiosos, basados en la vida de dos exploradores, Cristóbal Colón ("The Voyage" [1990], encargo de la Metropolitan Opera, con un libreto de David Henry Hwang), y Vasco da Gama ("White Raven") [1991], una colaboración con Robert Wilson y compuesta para la inauguración de la Expo '98.

Ya en los años 90 Philip Glass adquirió fama universal. Su música durante todo este periodo se ha alejado cada vez más del minimalismo y de sus planteamientos personales iniciales para llegar a posturas más comerciales y llenas de clichés «glasianos», como sus característicos arpegios o transiciones tonales. Ya a inicios del siglo XXI su música ha continuado siendo motivo de admiración y crítica. Entre sus principales obras se encuentran su Séptima Sinfonía (Tolteca), inspirada en la música de los pueblos originarios de México; su Octava Sinfonía, llena de variaciones melódicas y armónicas; y la ópera sobre la Guerra Civil estadounidense Appomattox.

En Brasil, Glass ha recibido críticas de los defensores del medio ambiente debido a lo que llaman su insensatez en escribir la música "Itaipu" (1989) en la cual, usando orquesta sinfónica y coro, el compositor elogió el gran proyecto hidroeléctrico construido por los militares brasileños durante la dictadura. En esa época los militares prohibían protestas contra la construcción de la presa. El embalse que se creó, destruyó para siempre inmensas áreas de flora tropical y ahogó los Saltos del Guairá, hasta entonces la cascada más grande del mundo. 




</doc>
<doc id="17118" url="https://es.wikipedia.org/wiki?curid=17118" title="Energía mareomotriz">
Energía mareomotriz

La energía mareomotriz es la energía que se obtiene aprovechando las mareas: mediante el uso de un alternador se puede utilizar el sistema para la generación de electricidad, transformando así la energía mareomotriz en energía renovable, una forma energética, más segura y aprovechable. Es un tipo de energía renovable, en tanto que la fuente de energía primaria no se agota por su explotación, y es limpia ya que en la transformación energética no se producen subproductos contaminantes gaseosos, líquidos o sólidos. Sin embargo, la relación entre la cantidad de energía que se puede obtener con los medios actuales y el coste económico y ambiental de instalar los dispositivos para su proceso han impedido una implementación notable de este tipo de energía.

Otras formas de extraer energía del mar son: las olas (energía undimotriz), de la diferencia de temperatura entre la superficie y las aguas profundas del océano, el gradiente térmico oceánico; de la salinidad, de las corrientes marinas o la energía eólica marina.

En España, el Gobierno de Cantabria y el Instituto para la Diversificación y Ahorro Energético (IDAE) quieren crear un centro de i+d+i en la costa de Santoña. La planta podría atender al consumo doméstico anual de unos 2500 hogares.

Los métodos de generación mediante energía de marea pueden clasificarse en tres distintas formas: 

Los generadores de corriente de marea "tidal stream generators" (o TSG por sus iniciales inglés) hacen uso de la energía cinética del agua en movimiento a las turbinas de la energía, de manera similar al viento (aire en movimiento) que utilizan las turbinas eólicas. Este método está ganando popularidad debido a costos más bajos.

Las presas de marea hacen uso de la energía potencial que existe en la diferencia de altura (o pérdida de carga) entre las mareas altas y bajas. Las presas son esencialmente los diques en todo el ancho de un estuario, y sufren los altos costes de la infraestructura civil, la escasez mundial de sitios viables y las cuestiones ambientales.

La energía mareomotriz dinámica es una tecnología de generación teórica que explota la interacción entre las energías cinética y potencial en las corrientes de marea.
Se propone que las presas muy largas (30 a 50 km de longitud) se construyan desde las costas hacia afuera; en el mar o el océano, sin encerrar un área. Se introducen por la presa diferencias de fase de mareas, lo que lleva a un diferencial de nivel de agua importante (por lo menos 2.3 metros) en aguas marinas ribereñas poco profundas con corrientes de mareas que oscilan paralelas a la costa, como el Reino Unido, China y Corea del Sur.

En el estuario del río Rance, EDF instaló una central eléctrica con energía mareomotriz. Funciona desde el año 1966 y produce electricidad para cubrir las necesidades de 225 000 habitantes, equivalente a una ciudad como Rennes (el 9 % de las necesidades de Bretaña). La central en sí tiene 390 m de largo y 33 de ancho. Está constituida de 24 turbinas de tipo "bulbo" con generadores de 10 MW cada una, por las que pasa un caudal total de 6600 m³ por segundo. Dispone de un embalse de 22 km² que alberga 184 000 000 m³ de agua regulada por seis compuertas de 10 m de alto por 15 de ancho.

La planta mareomotriz es una central hidroeléctrica reversible, que aprovecha tanto la marea alta como la marea baja ya que sus turbinas funcionan en ambos sentidos, en la fase de llenado y de vaciado del embalse. Las turbinas permiten también bombear agua: en marea baja, la planta funciona "al revés" y bombea agua de mar para elevar todavía más el nivel de agua del embalse. El bombeo permite aumentar la producción porque aumenta la altura de la caída de las aguas y disminuye el período de tiempo entre la pleamar y la bajamar.

La presa de 750 m de largo cierra el estuario del río y comprende una esclusa que permite el paso de unos 20 000 barcos al año. Una carretera con un tráfico medio de 30 000 vehículos al día (hasta 60 000 en verano) aprovecha su recorrido para unir los pueblos de Saint-Malo y Dinard.

El coste del kW/h resultó similar o más barato que el de una central eléctrica convencional, sin el coste de emisiones de gases de efecto invernadero a la atmósfera ni consumo de combustibles fósiles ni los riesgos de las centrales nucleares (13 metros de diferencia de marea).

El impacto ambiental fue bastante grave, como aterramiento del río, cambios de salinidad en el estuario en sus proximidades y cambio del ecosistema antes y después de las instalaciones.

Otros proyectos exactamente iguales, como el de una central mucho mayor prevista en Francia en la zona del monte Saint-Michel, o el de la bahía de Fundy, en Canadá, donde se dan hasta 15 metros de diferencia de marea, o el del estuario del río Severn, en el Reino Unido, entre Gales e Inglaterra, no han llegado a ejecutarse por el riesgo de un fuerte impacto ambiental.

El funcionamiento de una planta mareomotriz, es sencillo, cuando se eleva la marea se abren las compuertas del dique la cual ingresa en el embalse. Después cuando llega a su nivel máximo en el embalse, se cierran las compuertas. Luego, cuando la marea desciende por debajo del nivel del embalse alcanzando su amplitud máxima entre este y el mar, se abren las compuertas dejando pasar el agua por las turbinas a través de los estrechos conductos.

Actualmente, la electricidad se transporta mediante cables en islas artificiales, en las que hay más turbinas.




</doc>
<doc id="17120" url="https://es.wikipedia.org/wiki?curid=17120" title="Energía hidráulica">
Energía hidráulica

Energía hidráulica, energía hídrica o hidroenergía es aquella que se obtiene del aprovechamiento de las energías cinéticas y potenciales de la corriente del agua, saltos de agua o mareas.
Se puede transformar a diferentes escalas. Existen, desde hace siglos, pequeñas explotaciones en las que la corriente de un río, con una pequeña represa, mueve una rueda de palas y genera un movimiento aplicado generalmente a molinos o batanes. 

Generalmente se considera como un tipo de energía renovable puesto que no emite productos contaminantes. Otros consideran que produce un gran impacto ambiental debido a la construcción de las presas, que inundan grandes superficies de terreno y modifican el caudal del río y la calidad del agua.

La principal aplicación de la energía hidráulica en la actualidad es la obtención de electricidad. Las centrales hidroeléctricas generalmente se ubican en regiones donde existe una combinación adecuada de lluvias y desniveles geológicos favorables a la construcción de represas. La energía hidráulica se obtiene a partir de la energía potencial y cinética de las masas de agua que transportan los ríos, provenientes de la lluvia y del deshielo. En su caída entre dos niveles del cauce, se hace pasar el agua por una turbina hidráulica, la cual transmite la energía a un alternador que la convierte en energía eléctrica.

Otro sistema que se emplea es conducir el agua de un arroyo con gran desnivel, por una tubería cerrada, en cuya base hay una turbina. El agua se recoge en una presa pequeña y la diferencia de altura proporciona la energía potencial necesaria.

Otro más consiste en hacer en el río una presa pequeña y desviar parte del caudal por un canal con menor pendiente que el río, de modo que unos kilómetros más adelante habrá ganado una cierta diferencia de nivel con el cauce y se hace caer el agua a él por una tubería, con una turbina especial.


Además, los embalses que se construyen para generar energía hidráulica:

La gran ventaja de la energía hidráulica o hidroeléctrica es la eliminación de combustibles. El coste de operar una planta hidráulica es casi inmune a la volatilidad de los precios de los combustibles fósiles como petróleo, el carbón o el gas natural. Además, no hay necesidad de importar combustibles de otros países.

Las plantas hidráulicas también tienden a tener vidas económicas más largas que las plantas eléctricas que utilizan combustibles. Hay plantas hidráulicas que siguen operando después de 50 a 99 años. Los costos de operación son bajos porque las plantas están automatizadas y necesitan pocas personas para su operación normal.

Como las plantas hidráulicas no queman combustibles, no producen directamente dióxido de carbono. Se produce muy poco dióxido de carbono durante el período de construcción de las plantas, pero es poco, especialmente en comparación a las emisiones de una planta equivalente que quema combustibles.

A lo largo de la segunda mitad del siglo XX se ha visto crecer en forma importante la conciencia ambiental, de la gente, de los gobiernos y de las instituciones internacionales de crédito, que son en última instancia quienes financian los grandes proyectos hidroeléctricos.

Actualmente las medidas de mitigación ambiental forman parte integrante de todos los proyectos financiados por instituciones de crédito multilaterales, y los costos de las medidas de mitigación tienen que incluirse en el costo del proyecto.



</doc>
<doc id="17123" url="https://es.wikipedia.org/wiki?curid=17123" title="Jean-Jacques Rousseau">
Jean-Jacques Rousseau

Jean-Jacques Rousseau, también mentado como Juan Jacobo Rousseau (Ginebra, 28 de junio de 1712 - Ermenonville, 2 de julio de 1778) fue un polímata suizo francófono. Fue a la vez escritor, pedagogo, filósofo, músico, botánico y naturalista, y aunque fue definido como un ilustrado, presentó profundas contradicciones que lo separaron de los principales representantes de la Ilustración, ganándose por ejemplo la feroz inquina de Voltaire y siendo considerado uno de los primeros escritores del prerromanticismo. 

Sus ideas imprimieron un giro copernicano a la pedagogía centrándola en la evolución natural del niño y en materias directas y prácticas, y sus ideas políticas influyeron en gran medida en la Revolución francesa y en el desarrollo de las teorías republicanas, aunque también se le considera equívocamente uno de los precursores del totalitarismo del siglo XX, aunque lo que promovió fue que todos los seres humanos nacemos iguales y confirió la soberanía al pueblo, con lo que cuestionó el principio mismo de la monarquía y por eso fue uno de los pilares que dieron origen a la Comuna de París.

Fue crítico con el pensamiento político y filosófico desarrollado por Hobbes y Locke. Para él, los sistemas políticos basados ​​en la interdependencia económica y el interés propio conducen a la desigualdad, el egoísmo y, en última instancia, a la sociedad burguesa (un término que fue uno de los primeros en utilizar). Incorporó a la filosofía política conceptos incipientes como el de voluntad general (que Kant transformaría en su imperativo categórico) y alienación. Su herencia de pensador radical y revolucionario está probablemente mejor expresada en sus dos frases más célebres, una contenida en "El contrato social", «El hombre nace libre, pero en todos lados está encadenado», la otra, presente en su "Emilio, o De la educación", «El hombre es bueno por naturaleza».

Rousseau se hizo amigo de Denis Diderot en 1742, y más tarde escribiría sobre los problemas románticos de Diderot en sus "Confesiones". Durante el período de la Revolución Francesa, Rousseau fue el más popular de los filósofos entre los miembros jacobinos. Fue enterrado como héroe nacional en el Panteón de París junto con Voltaire, en 1794, 16 años después de su muerte.

La familia Rousseau procedía de hugonotes franceses y se instaló en Ginebra unos cien años antes de que Isaac Rousseau (Ginebra, 1672-Nyon, 1747) y Suzanne Bernard (Ginebra, 1673-"ibidem", 1712), hija del calvinista Jacques Bernard, tuvieran al futuro escritor Jean-Jacques. Nueve días después de dar a luz, Suzanne falleció y el pequeño Rousseau consideró a sus tíos paternos como sus segundos padres, debido a que desde muy pequeño pasó mucho tiempo con ellos y fueron los que lo cuidaron.

Cuando Rousseau tenía 10 años (1722), su padre, un relojero bastante culto, tuvo que exiliarse por una acusación infundada y su hijo quedó al cuidado de su tío Samuel, aunque ya había tomado de él un gran amor por la lectura y un sentimiento patriótico de admiración por el gobierno de la República de Ginebra que Jean-Jacques conservó toda su vida. Con esta familia disfrutó de una educación que él consideraría ideal, calificando esta época como la más feliz de su vida, y leyó a Bossuet, Fontenelle, La Bruyère, Molière y sobre todo a Plutarco, del cual interiorizó importantes nociones sobre la historia de la Roma republicana; en sus "Confesiones", escritas hacia el final de su vida, dirá que fue este autor su lectura predilecta; también recomendará en su "Émile" la lectura del "Robinson Crusoe" de Daniel Defoe. Junto con su primo, Rousseau fue enviado como pupilo a la casa del calvinista Lambercier durante dos años (1722-1724). A su regreso en 1725, trabajó como aprendiz de relojero y, posteriormente, con un maestro grabador (aunque sin terminar su aprendizaje), con quienes desarrolló la suficiente experiencia para vivir de estos oficios ocasionalmente.

A los 16 años (1728) empezó a vagabundear y abandonó su ciudad natal. Tras estar peregrinando un tiempo y desempeñando los oficios más dispares, al borde de entrar en la marginalidad, abjuró del calvinismo y abrazó el catolicismo, del que más tarde también renegó (en el futuro expondrá sus ideas deístas sobre una religión natural en su "Profesión de fe del vicario saboyano") y se estableció en Annecy, siendo tutelado por Madame de Warens, una dama católica ilustrada sin hijos, trece años mayor que él, que le ayudó en su discontinua educación y en su afición por la música, y además le fue buscando distintos trabajos. A ojos de Rousseau, ella sería la madre que había perdido y, a partir de 1733, una amante. Residió seis semanas de 1737 en Montpellier por una enfermedad grave, y a su regreso madame Warens le consiguió el puesto de preceptor en Lyon de los hijos del hermano de dos famosos escritores ilustrados, Gabriel Bonnot de Mably (1740), sobre el cual ejerció una fuerte influencia, y el filósofo Condillac; además traba amistad con Fontenelle, Diderot (que lo fichó como colaborador en materia musical de su "Enciclopedia", 1751-1772, y con quien se habrá de enemistar al cabo) y Marivaux (quien le corrige, por cierto, su pieza teatral en un acto "Narciso o el amante de sí mismo", que estrenará en 1752). Forjó entonces un carácter de "paseante solitario" amante de la naturaleza. Pero, siempre descontentadizo, Rousseau ejerció de periodista y de muchos otros oficios ocasionales más. En 1742 presenta un innovador sistema de notación musical a la Real Academia de las Ciencias de París, con poco fruto (su sistema solo se interesaba por la melodía y no por la armonía, y además un sistema similar ya había sido inventado sesenta y cinco años atrás por el monje Souhaitti), y al año siguiente publica su "Disertación sobre la música moderna" (1743), en que critica muy duramente la francesa, para él muy inferior a la italiana. Conoce a madame Dupin, de la que será luego secretario; también en ese año es nombrado secretario del inepto embajador de Francia en la República de Venecia, Pierre-François de Montaigu, con quien no llegó a concordar, hasta el punto de que al año siguiente fue despedido (1744). 

En 1745 y ya con 33 años, vuelve a París, donde convive con Thérèse Levasseur, una modista analfabeta con quien tiene cinco hijos y a quien convence para entregarlos al hospicio conforme van naciendo; así hizo en 1746 con el primero. Al principio dijo que carecía de medios para mantener una familia, pero más tarde, en el volumen IX de sus "Confesiones", sostuvo haberlo hecho para apartarlos de la nefasta influencia de su familia política: «Pensar en encomendarlos a una familia sin educación, para que los educara aún peor, me hacía temblar. La educación del hospicio no podía ser peor que eso».

En esta época contacta con Voltaire, D'Alembert, Rameau y, de nuevo, con Diderot, y escribe sus obras más reconocidas. Cuando la Academia de Dijon propuso en 1749 un concurso de disertaciones sobre la siguiente cuestión: «Si el restablecimiento de las ciencias y las artes ha contribuido a mejorar las costumbres», Rousseau ganó al año siguiente con su "Discours sur les sciences et les arts" respondiendo que no, pues las artes y las ciencias a su juicio suponen una decadencia cultural.

Pero, además, el cultivo de las ciencias y las artes era responsable para él también del declive de la moral, de la inocencia perdida y del desarrollo "del lujo, la disolución y la esclavitud". A partir de aquí, alcanza una discutida y polémica celebridad; incluso el depuesto rey de Polonia y duque de Lorena, Estanislao I Leszczynski, intentó refutar a Rousseau con otro discurso. En 1751 dimite de su puesto de secretario de madame Dupin y se dedica a copiar partituras musicales para ganarse la vida y en 1752 estrena con éxito en Fontainebleau, en presencia del rey Luis XV, su ópera en un acto "El adivino del pueblo", atreviéndose a rechazar una audiencia con el propio monarca. En 1754 publica su "Discurso sobre economía política" y abjura del catolicismo y al año siguiente, en 1755, publicará un texto aún más importante, su "Discurso sobre el origen y los fundamentos de la desigualdad entre los hombres", que había presentado para otro concurso de la Academia de Dijon sin obtener premio esta vez. Este discurso disgustó por igual a Voltaire y a la iglesia católica, la cual lo acusó de negar el pecado original y de adherirse a la herejía del pelagianismo. Rousseau había enviado un ejemplar a Voltaire, residente por entonces en su patria chica, Ginebra, y este le contestó que estaba "escrito contra la raza humana... jamás se desplegó tanta inteligencia para querer convertirnos en bestias". Fue el comienzo de una creciente enemistad entre estos dos ilustrados, cuya segunda fase aconteció cuando Voltaire publicó su "Poema sobre el desastre de Lisboa" (1755), en que afirmaba sin ambages su pesimismo y negaba la providencia divina, al que el ginebrino respondió con una "Carta sobre la Providencia" (1756) en que intentaba refutarlo. La respuesta de Voltaire sería justamente celebrada: su novela corta "Cándido o el optimismo". Aún se enconó más el odio de Voltaire cuando Rousseau imprimió su "Carta a D'Alembert sobre los espectáculos" (1758), en la que declaraba (siendo él mismo autor dramático) que el teatro era uno de los productos más perniciosos para la sociedad, generando lujo e inmoralidad; es más, se mostraba sumamente misógino al escribir frases como esta:

Voltaire se había obstinado en crear un teatro en Ginebra donde pudiese presentar sus piezas y actuar en ellas, y esta carta vino a darle la puntilla a toda posibilidad de congraciarse con Rousseau, quien, por su parte, empezaba a asistir a salones parisinos y criticaba la música francesa en la "Querelle des Buffons" con el apoyo de los enciclopedistas y su, por aquel entonces, íntimo amigo Frédéric-Melchior Grimm, con quien comparte el amor de madame d'Epinay.

Las exigencias de sus amigos y sus opiniones lo distancian de ellos, Rousseau se siente traicionado y atacado y abandona Ermitage, casa rural que le amuebló Mme. d'Epinay en 1756. Se traslada en ese año a Mont Louis, también en los bosques de Montmorency, y recibe la propuesta de convertirse en bibliotecario de honor de Ginebra, que rechaza. En 1757 se enamora apasionadamente de madame Sophie d'Houdetot, compitiendo con su otro amante, el poeta y académico Jean François de Saint-Lambert, pero su relación no llega a ser más que platónica. A ella dirigirá sus "Cartas morales" (1757-1758), que permanecieron inéditas hasta 1888. En 1758 publica su "Carta a d'Alembert sobre los espectáculos" y en 1761 su novela epistolar "Julia, o la nueva Eloísa".

1762 fue un año fundamental en su creación literaria, pues redacta una pieza teatral originalísima, "Pygmalion", considerada la creadora de un nuevo género dramático-musical, el melólogo, que solo podrá representarse en 1770, y publica dos obras capitales: "Emilio, o De la educación" y "El contrato social, o Principios del derecho político". La primera de estas obras era sobre todo un cañonazo en toda regla contra la pedagogía tradicional y las religiones culturales y aprendidas, no naturales, que habrá de tener consecuencias importantísimas en esas disciplinas; en pedagogía imprimió un giro copernicano que desarrollará otro escritor suizo, Pestalozzi, centrando la educación en el niño y en su evolución mental, y primando las materias prácticas frente a las teóricas y abstractas, mientras que en cuestiones religiosas Rousseau proponía, despreciando la teología como inútil, una religión natural con papel secundario y menos importante que otras disciplinas prácticas; la segunda obra era una crítica fundamentada y de raíz de los principios políticos del Antiguo Régimen que partía de una cuestión que se hizo justamente célebre: «El hombre nace libre y, sin embargo, donde quiera que va está encadenado. ¿Por qué este cambio?». En teoría constitucional, a diferencia de Thomas Hobbes y de modo más acentuado aún que John Locke, Rousseau no admitía ninguna restricción en cuanto a los derechos y libertades individuales: el hombre que no goza de una libertad completa no es un hombre; bosqueja un principio filosófico de amplio futuro, la alienación, así como otro político-jurídico, la voluntad general. Las heterodoxas ideas expresadas en estas obras lo hacen tremendamente impopular, hasta el punto de que el 9 de junio el Parlamento de París da orden de arrestarlo por su "Emilio"; avisado previamente, Rousseau decidió refugiarse en su natal tierra suiza, más en concreto en Yverdon; allí se entera de que además el arzobispo de París Christophe de Beaumont ha escrito una carta pastoral contra sus obras; el 19 de junio el cantón de Ginebra le expende orden de arresto por sus obras "Emilio" y "Contrato social" y el 10 de julio es expulsado de Yverdon por el cantón de Berna; así que atraviesa la sierra del Jura y se refugia en Môtiers-Travers bajo la protección de Julie Emélie Willading, nacida Boy de la Tour (1751-1826); en 1763 escribe una "Carta a Christophe de Beumont" para defenderse de la persecución del arzobispo católico y después renuncia a la ciudadanía ginebrina; en septiembre de 1764 recibe una oferta de Pasquale di Paoli para redactar una constitución para la efímera República Corsa (1755-1769). También en 1764 Voltaire publica un panfleto anónimo contra Rousseau, "El sentimiento de los ciudadanos", en el que revela el destino de sus cinco hijos, entregados al cuidado de orfanatos porque Rousseau pensaba no ser capaz de mantenerlos por sus condiciones económicas (esta fue su principal justificación en las "Confesiones"):

Rousseau se tomó la molestia de rebatir con informes médicos su presunta sífilis y el infundio de haber matado a la madre de su amante, republicando el folleto anónimo con sus notas en París, pero ocultando sin embargo la verdad del abandono de sus hijos. Desde ese momento adoptó como lema "Vitam impendere vero" ("dedicar la vida a la verdad", Juvenal, sátira IV), que antepuso a una publicación que hizo en diciembre, sus "Cartas de la montaña"; pero el clero protestante (sobre todo el pastor calvinista de Ginebra Jean Sarasin) y católico despotricaba contra él y en 1765 su casa en Môtiers fue apedreada por una turba furiosa; unos días después Rousseau decidió refugiarse en la isla de San Pedro, en el lago de Bienne, en casa de un síndico de Berna; pero también se vio forzado a marcharse de allí. Rousseau se desespera por primera vez y pide a las autoridades de Berna que le encarcelen donde sea, que ya no escribirá nada más; pero no lo encarcelan y se instala en Bienne, donde recibe sobre todo la visita de diversos ingleses (Daniel Malthus, padre del economista; James Boswell...), pues sus dos discursos y sus tres grandes libros, traducidos estos últimos por William Kenrick, habían sido ampliamente divulgados también en el mundo anglófono. Recibió peticiones para que viajara a Prusia (del mariscal George Keith), al Reino Unido (de David Hume) e incluso a Rusia (de Cyril Razoumovsky).

La persecución empezaba a suscitar en Rousseau una paranoia o manía persecutoria a la que ya era proclive; además, estaba seriamente enfermo de vejiga. Así que el 4 de enero de 1766, con David Hume y Jean-Jacques de Luze, se puso en camino para Londres. Su amigo Hume lo acogió junto con Thérèse en Inglaterra, pero el filósofo suizo no aguantaba la ciudad y Hume tuvo que buscarle a la pareja una residencia campestre a su gusto, y la encontró en Chiswick; sin embargo el ilustrado francés era invitado con frecuencia a otras fincas, como Mundan House (Surrey) a media milla de Wotton Place, y sobre todo Wootton Hall (Statford), en casa de Richard Davenport, que fue el lugar donde por más tiempo residieron; pasaron en Inglaterra dos agitados años (1765-1767), hostigados por la opinión que la mayoría de los ingleses tenía de él: un loco, malo y peligroso hombre que vivía en pecado con Thérèse. Hume tenía que buscar artimañas hasta para llevar al teatro Drury Lane al caprichoso, antojadizo y paranoico francés; al llegar al espectáculo, su extraño atavío (Rousseau vestía habitualmente al modo armenio) causó alboroto y al finalizar la representación fue conducido a la tertulia del gran actor Garrick. Horace Walpole le gastó una broma pesada escribiéndole una carta falsa como si fuese Federico el Grande de Prusia, Therèse le engañó con Boswell, y el perro de Rousseau, "Sultán", no hacía otra cosa que escaparse y Rousseau se pasaba el día quejándose y protestando. En fin, Hume acabó harto de los líos, rarezas (por ejemplo, rechazar una pensión secreta del rey Jorge III de cien libras que Hume se había forzado en conseguirle y el francés había aprobado al principio) y paranoias (pensaba que Hume se había aliado con Voltaire, d'Alembert, Diderot y otros enemigos suyos para desacreditarlo, llevando este altercado incluso a la imprenta, a lo cual respondió Hume también con un impreso) de Rousseau. En 1767, con 55 años, recibe pese a todo la pensión de Jorge III, pero decide volver a Francia con el nombre falso de Jean-Joseph Renou, cuando ya sus agobiados amigos ingleses se habían dado cuenta de que algo le pasaba, que estaba trastornado. El príncipe de Conti pone a su disposición una casa en Trye-le Chateâu y se publica su "Diccionario de música." Pero en 1768 marcha a Lyon y Grenoble y el 30 de agosto se casó con su amada Thérèse en Bourgoin. En 1770 se le permitió regresar oficialmente con su nombre: pero bajo la condición de no publicar nada más.

Terminó sus memorias, las "Confesiones", en 1771, un intento de resolver o al menos dar testimonio de sus tremendas contradicciones, y se dedicó a vivir de sus patrones y de lecturas públicas de estas memorias. En 1772 Mme. d'Epinay, escritora amante de él y Grimm al tiempo (lo que provocará su enemistad), escandalizada por lo que Rousseau relata de su relación con ella, pide a la policía que prohíban tales lecturas, y eso es lo que ocurre. Con un estado anímico sombrío, se aleja definitivamente del mundo. Comienza a redactar en 1772 sus "Diálogos", pero el daño que le habían causado los violentos ataques de Voltaire (quien dijo de él que se valía de la sensiblería y la hipocresía para prosperar) así como los de otros personajes de su época terminó apartándolo finalmente de la vida pública sin poder aprovechar la fama y el reconocimiento de su obra, que inspiraría al romanticismo. Alarga sus "Consideraciones sobre el Gobierno de Polonia" y en los años siguientes trabajó en "Cartas sobre botánica a la señora Delessert" (1771-1773), "Rousseau juez de Jean-Jacques" (1772-1776) y la ópera "Daphnis et Chloé" (1774-1776). En 1776 empieza a redactar sus "Ensoñaciones de un paseante solitario" (1776-1778 ), cuya redacción quedará inconclusa por su súbita muerte, cuando andaba retirado en Ermenonville por consejo médico, de un paro cardíaco en 1778, cuando contaba 66 años.

Sus restos descansan en el Panteón de París a pocos metros de Voltaire y el sitio exacto está marcado claramente por un busto conmemorativo. Póstumas aparecieron diversas obras: en 1781 su "Ensayo sobre el origen de las lenguas" y una continuación del "Emilio", "Émile et Sophie, ou les Solitaires", así como las "Confesiones" (1782-1789). Las "Cartas morales" solo serán publicadas en 1888.

Dado su alejamiento de los enciclopedistas de la época y su enfrentamiento con la Iglesia católica, por sus polémicas doctrinas, su estilo literario cambió. Sus obras autobiográficas dieron un vuelco fundamental en la literatura europea; a tal punto que es considerado un autor prerromántico o precursor del Romanticismo. Las obras suyas que más influyeron en su época fueron "Julia, o la Nueva Eloisa" (1761) y "Emilio, o De la educación" (1762), ya que transformaron las ideas sobre la familia.

Otras obras muy importantes son "El contrato social" y el "Discurso sobre el origen de la desigualdad entre los hombres".

Rousseau produjo uno de los trabajos más importantes de la época de la Ilustración; a través de su "El contrato social", hizo surgir una nueva política. Esta nueva política está basada en la "volonté générale", voluntad general, y en el pueblo como depositario de la soberanía. Expone que la única forma de gobierno legal será aquella de un Estado republicano, donde todo el pueblo legisle; independientemente de la forma de gobierno, ya sea una monarquía o una aristocracia, no debe afectar la legitimidad del Estado. Rousseau da gran importancia al tamaño del Estado, debido a que una vez la población del Estado crece, entonces la voluntad de cada individuo es menos representada en la voluntad general, de modo que cuanto mayor sea el Estado, su gobierno debe ser más eficaz para evitar la desobediencia a esa voluntad general.

En sus estudios políticos y sociales Rousseau desarrolló un esquema social, en el cual el poder recae sobre el pueblo, argumentando que es posible vivir y sobrevivir como conjunto sin necesidad de un último líder que fuese la autoridad. Es una propuesta que se fundamenta en la libertad natural, con la cual, Rousseau explica, ha nacido el hombre. En "El Contrato Social", Rousseau argumenta que el poder que rige a la sociedad es la voluntad general que mira por el bien común de todos los ciudadanos. Este poder solo toma vigencia cuando cada uno de los miembros de una sociedad se une mediante asociación bajo la condición, según expone Rousseau, de que «Cada uno de nosotros pone en común su persona y todo su poder bajo la suprema dirección de la voluntad general; y cada miembro es considerado como parte indivisible del todo». En fin, Rousseau plantea que la asociación asumida por los ciudadanos debe ser «capaz de defender y proteger, con toda la fuerza común, la persona y los bienes de cada uno de los asociados, pero de modo tal que cada uno de éstos, en unión con todos, solo obedezca a sí mismo, y quede tan libre como antes».

La obra rousseauniana argumenta que esta asociación de los hombres no es algo natural. El hombre sale de su estado natural de libertad porque le surgen necesidades de supervivencia que le imponen la creación de algo artificial, ya que el hombre no es sociable por naturaleza y no nació para estar asociado con otros. Es voluntariamente que se unen los unos a los otros y fundamentan este vínculo con el desarrollo de la moralidad y la racionalidad para satisfacer las necesidades que la naturaleza le ha impuesto. La moral y la razón se hacen evidentes en la sociedad al establecer un modelo normativo capaz de crear un orden social que evite la dominación de unos sobre otros y que involucre una representación participativa de todos los miembros de la sociedad.

Mediante "El Contrato Social", Rousseau le abre paso a la democracia, de modo tal que todos los miembros reconocen la autoridad de la razón para unirse por una ley común en un mismo cuerpo político, ya que la ley que obedecen nace de ellos mismos. Esta sociedad recibe el nombre de república y cada ciudadano vive de acuerdo con todos. En este Estado social son necesarias las reglas de la conducta creadas mediante la razón y reflexión de la voluntad general que se encarga de desarrollar las leyes que regirán a los hombres en la vida civil. Según Rousseau, es el pueblo, mediante la ratificación de la voluntad general, el único calificado para establecer las leyes que condicionan la asociación civil. De acuerdo con la obra de Rousseau, todo gobierno legítimo es republicano, es decir, una república emplea un gobierno designado a tener como finalidad el interés público guiado por la voluntad general. Es por esta razón que Rousseau no descarta la posibilidad de la monarquía como un gobierno democrático, ya que si los asociados a la voluntad general pueden convenir, bajo ciertas circunstancias, la implementación de un gobierno monárquico o aristocrático, entonces tal es el bien común.

En su modelo político, Rousseau atribuye al pueblo la función de soberano. A este término no le asigna características que designan a una sola clase o nación, sino la representación de una comunidad de los que desean formar un Estado y vivir bajo las mismas leyes que son la expresión de la voluntad general. El pueblo, como soberano, debe llevar a cabo una deliberación pública, que ponga a todos los ciudadanos asociados en un plano de igualdad, en la cual el cuerpo no puede decidir nada que atente contra los intereses legítimos de cada uno. Las leyes en la república de Rousseau están desarrolladas conforme al orden social, establecido por la naturaleza del pacto social y no por las convenciones humanas de un solo individuo. Las leyes deben fundamentarse en las convenciones que traducen en reglas las exigencias de la racionalidad y moralidad humana, al tiempo que no atentan contra el ideal de la justicia que impone que todos los asociados se respeten los unos a los otros. Rousseau establece que las reglas de la asociación deben ser el resultado de la deliberación pública, ya que en ella se encuentra el origen de la soberanía. Las leyes nacidas de la deliberación no serán justas y la soberanía no será legítima si la deliberación no respeta el interés común y si los ciudadanos no aceptan las condiciones por las que las reglas son iguales para todos. Estas leyes no instituyen ninguna forma específica de gobierno, sino que fijan las reglas generales de la administración y definen la constitución, por la cual el pueblo ha de regirse, ya que son la máxima expresión de la voluntad general.

El ideal político planteado por Rousseau en "El Contrato Social" se basa en la autonomía racional. Esta es la asociación que supone el reino de la ley común, en la cual cada uno de los asociados, al entregarse al pacto social, se obedece a sí mismo porque las leyes se fundamentan en la voluntad general, en la cual cada ciudadano es a su vez legislador, al deliberar públicamente en la creación de las reglas, y súbdito, al someterse libremente a la obediencia de las mismas.

El ideal político de "El Contrato Social" puede realizarse bajo cualquier forma de gobierno. Rousseau argumenta que cualquier forma de gobierno es válida y legítima si se ejerce dentro de los parámetros regidos por la ley común. En su obra, Rousseau define una república como “todo Estado regido por leyes, cualquiera que sea su forma de administración”.

En el modelo político de Rousseau, el pueblo aparece en una doble dimensión, en la cual es sujeto y objeto del poder soberano. Cada individuo es sujeto de la soberanía porque entrega todos sus derechos a la comunidad, pero, al mismo tiempo, es objeto porque, al ser parte de un todo, se los entrega a sí mismo. Al establecerse este pacto, la soberanía reside en el pueblo y, como resultado, la misma es inalienable, indivisible, absoluta e infalible, ya que es contradictorio que el soberano como pueblo implemente algo contra sí mismo como súbdito.

Lo que caracteriza el modelo político que Rousseau desarrolla en "El Contrato Social" es la idea clave roussoniana de "voluntad general". Tal voluntad se diferencia de la voluntad de todos por su carácter universalista y su aspecto normativo. No es una voluntad cualitativa, sino que se forma por una cualificación moral, en la cual se requiere que los hombres actúen de acuerdo a los intereses universalistas. Una vez se forma esta voluntad, su mandato es inapelable, ya que lo que persigue es el interés colectivo que no es diferente del interés individual. Es por ello que, si algún asociado intentase resistir la voluntad general, se verá obligado por el cuerpo social a obedecerle.

Rousseau concebía la democracia como un gobierno directo del pueblo. El sistema que defendía se basaba en que todos los ciudadanos, libres e iguales, pudieran concurrir a manifestar su voluntad para llegar a un acuerdo común, a un contrato social. En "El contrato social" diría que «toda ley que el pueblo no ratifica, es nula y no es ley» y que «la soberanía no puede ser representada por la misma razón que no puede ser enajenada». Como "voluntad general" no puede ser representada, defendía un sistema de democracia directa que inspira, hasta cierto punto, la constitución federal suiza de 1849.

La relación de las teorías de Rousseau con el nacionalismo moderno es uno de los temas abundados por la teoría política y la historia de las ideas. En sus obras, Rousseau planteó las bases para el nacionalismo moderno atribuyéndole los sentimientos de identificación con la república o sociedad a la cual el hombre se ha asociado, aunque argumentó que estos sentimientos solo hubiesen sido posibles en Estados pequeños y democráticos.

Mientras que Hobbes pensaba que el hombre era malo por naturaleza, Rousseau establece que el hombre es por naturaleza bueno, pero la sociedad lo corrompe después; así lo resume en una carta al prelado Christophe de Beaumont, escrita en noviembre de 1762, que no sirvió de nada, ya que este eclesiástico condenó su "Émile" en un largo ensayo de 1763:

Rousseau opone el hombre natural al hombre histórico, pero para no destruir la sociedad (revolución) propone como solución de esta contradicción la reforma de la sociedad y un tercer hombre, el hombre civil, en su "Contrato social", y un gobierno por consenso mediante la voluntad general expresa en leyes comunes e iguales para todos.

Rousseau consideraba que toda aquella persona que participe del contrato social es soberano, por ende es un bien común el que se obtiene a través de este contrato. Por esta razón no puede existir una distinción entre soberano e individuo y se debe legislar bajo la voluntad general. Este tipo de gobierno comienza una vez el pueblo ha madurado moral y políticamente para lograr comprender e implementar la voluntad general, y que esta sea libre de interferencias.Debido a esto, la ley siempre es general, porque considera a las acciones y a las masas, nunca a un individuo. Acerca de las leyes, Rousseau, hace una diferenciación entre la voluntad general y la voluntad común. Y estas leyes o contratos no pueden ser creados por la voluntad común, debido que la voluntad común puede ser buena o mala, pero esta no necesariamente se dirige hacia la voluntad general, cuyo fin es el bien común.

Estas leyes son divididas entre las Fundamentales, Civiles y Criminales:

Rousseau planteó algunos de los precedentes políticos y sociales que impulsaron los sistemas de gobiernos nacionales de muchas de las sociedades modernas estableciendo la raíz de la desigualdad que afecta a los hombres; para él, el origen de dicha desigualdad era a causa de la constitución del derecho de propiedad:

Así pues, se opone a John Locke, quien pensaba que el derecho de propiedad era uno de los derechos humanos fundamentales y naturales del hombre. A medida que la especie humana se fue domesticando, los hombres comenzaron a vivir como familia en cabañas y acostumbraban ver a sus vecinos con regularidad. Al pasar más tiempo juntos, cada persona se acostumbró a ver los defectos y virtudes de los demás, creando el primer paso hacia la desigualdad. “«Aquel que mejor cantaba o bailaba, o el más hermoso, el más fuerte, el más diestro o el más elocuente, fue el más considerado». En este aspecto, la formación de la sociedad hizo necesaria la creación de entidades que regularan los derechos y deberes de los hombres, perdiendo estos así la libertad de tomar posesión de lo que tenían a mano, y los adoctrinó a olvidarse de sus antiguos sentimientos y manera de vivir sencilla y los impulsó a superar a sus semejantes provocando la pérdida de la igualdad, o mejor dicho, dando nacimiento a la desigualdad.

En su estudio sobre la desigualdad, estableció las diferencias entre el hombre civilizado y el hombre salvaje, determinando que las situaciones que estos enfrentaban en su diario vivir definían su comportamiento con los demás. El hombre civilizado, motivado por un deseo de ser superior a los otros, crea una especie de antifaz que le presenta al mundo, con el propósito de crear distinción entre ellos y los demás. En esta nueva sociedad, «Las almas no son ya visibles, ni la amistad posible, ni la confianza duradera, porque ya nadie se atreve a parecer lo que es». En este mundo artificial, la comunicación humana se hizo imposible. El hombre salvaje no presentaba este problema, él no vivía en sociedad porque no lo necesitaba, pues la naturaleza le proporcionaba todas sus necesidades. Cuando sentía hambre contaba con los animales de la selva para saciarla, al anochecer buscaba refugio en una cueva, su relación con los demás se llevaba en armonía, siempre que ambas partes así lo requirieran y que no se presentaran conflictos, y así mismo todos por igual tenían derecho a una parte de las tierras que habitaban. Según Rousseau, a medida que el hombre salvaje dejó de concebir lo que la naturaleza le ofrecía como lo prescindible para su subsistencia, empezó a ver como su rival a los demás hombres, su cuerpo no fue más su instrumento, sino que empleó herramientas que no requerían de tanto esfuerzo físico, limitando por ello sus acciones y concentrándose en el mejoramiento de otros aspectos de su nueva forma de vida, transformándose así en el hombre civilizado.

En el "Origen de la desigualdad entre los hombres", afirma: “tal es, en efecto, la causa de todas estas diferencias: el salvaje vive para sí mismo; el hombre social, siempre fuera de sí, no sabe vivir más que en la opinión de los demás; y de ese único juicio deduce el sentimiento de su propia existencia”. Esta naturaleza humana, que Rousseau supone del hombre salvaje, no es sino una hipótesis de trabajo, pues él mismo admite en esta obra que no es posible mostrar que dicho estado salvaje haya existido.

A pesar de que algunos de sus escritos parecían atacar la estructura de la sociedad, este era, según Rousseau, el modo de pensar de sus adversarios, como lo expresa aquí “¿en qué quedamos? ¿Es preciso destruir la sociedad, confundir lo tuyo y lo mío y volver a vivir en las selvas como los osos? Esta es una consecuencia del modo de pensar de mis adversarios, que tanto me gusta prevenir como dejarles la vergüenza de deducirla”. Su intención no fue la de desmantelar dicha potencia, sino el de hacer de la misma una comunidad de igualdad donde todos tuvieran la libertad para expresar su pensar y tomar las decisiones que beneficien a todos, como se puede apreciar en "El Contrato Social".

Rousseau hace un estudio de la formación del hombre individual antes de éste "ingresar a la sociedad", con sus primeras obras que incluyen: "Discurso sobre las ciencias y las artes", "Ensayo sobre el origen de las lenguas" y "Emilio, o De la educación". En la primera y en la segunda, Rousseau identifica los vicios y las virtudes, y en la tercera, la más importante, propone encaminar al hombre a la virtud haciendo a un lado los vicios mediante una educación ajustada a la naturaleza.

Una de las definiciones: Vicio: lo artificial, las artes: las letras, las lenguas, música.las ciencias, excesivo uso de razón, expresión de sentimientos que no existen. "palabras vacías", la armonía; virtud: lo puro, natural, la melodía, expresión sincera de sentimientos y el "conocimiento necesario".

Las artes, según Rousseau, traen el conocimiento que hace al individuo comportarse de una manera para "ser de agrado a los demás", y no es un comportamiento natural; en vez de crear una unión entre seres humanos, crean la desigualdad entre ellos. Se crea una esclavitud a ellas y una esclavitud entre los hombres, se explica con su famosa cita: "las ciencias, las letras y las artes, menos despóticas y más potentes acaso, tienden guirnaldas de flores sobre las cadenas de hierro de que están cargados, sofocan en ellos el sentimiento de esa libertad original para la que parecían haber nacido". Por lo que entra la educación, que involucra a las artes como parte del proceso, sin uso excesivo de ellas, a "transformar al individuo liberándolo de las perversiones".

En el "Emilio o De la eduación" imprime un giro copernicano a la pedagogía de la sociedad estamental de entonces centrándola en el niño y no en lo que debe aprender; le interesaban más los artesanos que los científicos y más la educación elemental que la avanzada. Quería crear ciudadanos activos, que estimasen el trabajo por encima de todo. Los principios que establece son estos:

Todas estas ideas de Rousseau son nuevas para el siglo XVIII y fueron desarrolladas por la pedagogía posterior.

Rousseau aunque en un primer momento parece obviar al género femenino no es que lo ignore, sino que va definiendo su papel en sociedad como mero acompañante del ser humano que debe poseer todos los derechos, el hombre. 

En sus primeros "Discursos" apenas la nombra. Cuando habla de hombres de ciencia y racionalistas, criticándolos, se dirige solo a estos, pues a la mujer no le era permitido participar en este tipo de actividades. En el "Discurso sobre la desigualdad" añora esa ley natural del ser humano en el estado de naturaleza. En él tampoco hace referencia al género femenino, sin embargo, esta ley natural le servirá de base, posteriormente, para justificar y argumentar a favor de esa posición de la mujer como mero apéndice del hombre, del lugar que debe ocupar en sociedad “por naturaleza”. En "La nueva Eloísa" reproduce ese modelo de hembra ideal, representada por Julia, la baronesa de d´Hochetat, mujer virtuosa donde las haya cuyo deber y máxima aspiración es cumplir las apariencias, ser virtuosa y evitar la censura en sociedad. 

En el "Emilio, o De la educación" toda la riqueza de su aportación a la educación de la época en la que se tiene en cuenta al niño como persona en sí misma, no como mero boceto de preparación para la adultez, queda desvalorizado cuando se trata de las niñas. Un determinismo natural pauta su educación, enfocada a agradar al macho y darle hijos, o sea, a ser madre y esposa como función vital. Sofía, esposa de Emilio, será más o menos libre y se casará por amor, pero su crecimiento como persona estará condicionado al papel que se le asigna al lado de Emilio.

Es en su "Carta a D'Alembert" donde se revelan sus prejuicios respecto a la mujer, dejándola de lado en la defensa de la justicia y la igualdad entre los seres humanos. Cuenta sobre ellas que «ni son expertas, ni pueden ni desean serlo en ningún arte, que les falta el ingenio, que los libros salidos de su pluma son todos fríos y bonitos como ellas, que les falta razón para sentir el amor e inteligencia para saber describirlo». La mujer se muestra, simplemente, como el instrumento que facilita la vida política del hombre y su dedicación al estudio y a su desarrollo personal. Siendo así no la ve como persona en sí misma, soberana y libre —ni aún en estado de naturaleza—, sino como ser para, es decir, como mero medio: «deben aprender muchas cosas, pero solo las que conviene que sepan».

El propio D'Alembert le contestó con un alegato en favor de la mujer y, pocas décadas después, Olympe de Gouges con su "Declaración de los derechos de la mujer y de la ciudadana". «Extraño, ciego, hinchado de ciencias y degenerado, en este siglo de luces y de sagacidad, en la ignorancia más crasa quiere mandar como un déspota sobre un sexo que recibió todas las facultades intelectuales», indica Olympe. Poco después, en Inglaterra, será Mary Wollstonecraft la que asumirá el papel de dar respuesta rigurosa a ese supuesto orden natural de varón pensante mujer acompañante, para demostrar que tal distinción es puramente artificial, producto de una educación discriminatoria dentro de una sociedad patriarcal. 

Carole Pateman ha designado a ese contrato implícito que subordina a la mujer respecto al hombre como el contrato sexual, que parte de la reorganización patriarcal que adapta la visión rousseauniana de la Ilustración a la sociedad actual, instituyendo salarios más bajos, acoso sexual, falta de reconocimiento social, violencia de género, etc.

Rousseau descubre tardíamente la botánica, hacia sus 65 años, gustando de herborizar, actividad que lo tranquilizaba, luego de tanta jornada de reflexionar, que lo fatigaba y lo entristecía, como él mismo escribió en la séptima "Ensoñación del paseante solitario". Así sus "Cartas sobre la botánica" le permiten continuar una reflexión sobre la cultura, en un sentido inmenso, comenzando con el "Émile", su tratado de educación, y su romance "Julie, ou la nouvelle Héloïse", donde se interroga sobre el arte de la jardinería.

El hombre, si está desnaturalizado, si carece de instintos, no puede contemplar la naturaleza, únicamente hace áreas habitables y cultivables, desnaturalizadas, «contorneadas a su modo» en «campiñas artificiales» donde si bien pueden vivir, no resulta más que en un país pobre. Y van quedando cada vez menos posibilidades de acceder a lo natural «deberían conocerse y ser dignos de ser admirados... La naturaleza semeja estar desordenada a los ojos humanos, y pasar sin atraer la mirada de los poco sensibles, y que a su vez han desfigurado... Están quienes le aman e intentar buscar y no lo pueden hallar» continúa Rousseau en su novela, donde va describiendo cómo Julie instala al fondo de su vergel un jardín secreto, jugando con lo agradable a lo útil de manera de hacer un poco de paseo que recuerde a la pura naturaleza: «es verdad, dice ella que la naturaleza hace todo, mas bajo mi dirección, no habrá más quien le ordene».

Rousseau describe el jardín del hombre que concilia a la vez al humanista y al botánico, como un aspecto útil y placentero donde pueda estar sin artificios visibles, ni a la francesa, ni a la inglesa: el agua, la verdura, la sombra y las siembras, como se ve en la naturaleza, sin usar la simetría ni alinear los cultivos y los bordes. El hombre de gusto «no se inquietará al punto de su percepción de bellas perspectivas: el gusto de los puntos de vista solo visibles a muy pocos».

El trabajo de mejorar el suelo y de hacer injertos no devolverá lo natural quitado a la naturaleza. Además de que no volverá, sigue extendiéndose catastróficamente nuestra civilización urbana con consecuencias, mas puede forzarse otro destino. Y si el trabajo de un vergel y de campos sea una necesidad para el hombre, el jardín de «el hombre de gusto» funcionará permitiendo desahogarse, descansar de momentos de esfuerzo.

Para Rousseau, las melodías y el jardín son del orden de lo humano, de la perfectibilidad, de la imaginación y de las pasiones simples. Él habla de una música de una temporalidad melódica, por lo tanto habrá procesos educativos que permitan a los humanos esperar un devenir «todo lo que podamos ser» o hacer que la naturaleza no nos haga sufrir.

Rousseau gustaba de ofrecer pequeños herbarios a sus amistades y a sus allegados, además de que él mismo reunió un herbario personal constituido por hasta 15 clasificadores llenos de pliegos de especímenes, algunos de ellos considerados hoy en día como tipos. Tras la muerte de Rousseau, su herbario tuvo diferentes propietarios hasta que en 1953 fue adquirido por el Museo Nacional de Historia Natural de Francia, institución que lo incluyó en las colecciones de la Galería de Botánica, en el Jardín de las Plantas en París, intengrándolo así en el herbario nacional francés, el mayor del mundo con casi 8 millones de especímenes.

Jean Jaques Rousseau era más bien un filósofo político, no un pedagogo; pero, a través de su novela "Emilio, o De la educación" promueve pensamientos filosóficos sobre la educación, siendo este uno de sus principales aportes en el campo de la pedagogía. En este libro, exalta la bondad del hombre y de la naturaleza a la vez que plantea temas que más adelante desarrollará en "Del Contrato Social". Rousseau concibe su paradigma del hombre encadenado en Emilio, o De la educación. Al igual que en "Discurso sobre el origen y los fundamentos de la desigualdad entre los hombres" quiere apartar la formación del hombre en Emilio, o De la educación de su indagación, «los hombres, diseminados entre ellos, observan, imitan su industria, y se elevan de esta manera hasta el instinto de las bestias; se alimentan igualmente de la mayoría». Rousseau crea un sistema de educación que deja al hombre, o en este caso al niño, que viva y se desarrolle en una sociedad corrupta y oprimida. Como dice el estudio preliminar de "Emilio, o De la educación": «asignad a los niños más libertad y menos imperio, dejadles hacer más por sí mismos y exigir menos de los demás».

Esta novela filosófica educativa, escrita en 1762, fundamentalmente describe y propone una perspectiva diferente de la educación, que es aplicada en Emilio. Rousseau, partiendo de su idea de que la naturaleza es buena y que el niño debe aprender por sí mismo en ella, quiere que el niño aprenda a hacer las cosas, que tenga motivos para hacerlas por sí mismo. Como Jurgen Oelkers, escritor del artículo "Rousseau and the image of ‘modern education’" dice, «La educación debe tener su lugar dentro de la naturaleza para que el potencial del niño pueda desarrollarse según el ritmo de la naturaleza y no al tiempo de la sociedad». Rousseau cree que todo hombre y niño es bueno. Sobre todo, especula que la humanidad que plantea una educación a base de un transcurso natural sería una sociedad más libre. Sandro de Castro y Rosa Elena, en su artículo «Horizons of dialogue in Environmental Education: Contributions of Milton Santos, Jean-Jacques Rousseau and Paulo Freire» dicen: «"Escribiendo Emilio, o De la educación", Rousseau coloca la base para una educación capaz de formar a un hombre verdadero, porque ante todo hay que formar al hombre. Formar al hombre es la primera tarea, la segunda es formar al ciudadano, porque no se puede formar a ambos al mismo tiempo». 

Rousseau atacó al sistema educativo a través de esta novela, en la que presenta que los niños deben ser educados a través de sus intereses y no por la estricta disciplina.

La novela está dividida en cinco partes. Las tres primeras se dedican a la niñez, la cuarta se consagra a la adolescencia y la última se refiere a la educación de Sofía, mujer ideal, y a la vida paternal, política y moral de Emilio.

Desde el vientre de la madre se puede decir que uno está vivo. Así pues, mientras el niño va creciendo, según Rousseau, debe por su propia voluntad ir adquiriendo conocimiento. Él dice: «Nacemos capacitados para aprender, pero no sabiendo ni conociendo nada», al igual que dice que la educación del hombre empieza al nacer, a base de experiencias propias y adquisiciones generales. Sin darnos cuenta, desde que nacemos somos libres y por nuestra propia voluntad conocemos lo que es placer, dolor y rechazo.

Rousseau también afirma que el aprendizaje es muy necesario, especialmente en esta etapa de la vida. Volviendo a su tema de la libertad, Luiz Felipe Netto en el artículo «The notion of liberty in Emile Rousseau» dice: «Más bien, un niño está libre cuando puede lograr su voluntad». Piensa que debemos dejar al niño manifestar su voluntad y curiosidad por lo que le rodea. Es decir, dejar al niño tocar, saborear, poner en práctica sus sentidos sensoriales para aprender.

En esta sección Rousseau dice: «La naturaleza formó a los niños para que fuesen amados y asistidos». También dice que si los niños escuchasen a la razón, no necesitarían que los educaran. A los niños se les debe tratar con suavidad y paciencia; explica que al niño no se le debe obligar a pedir perdón, ni imponer un castigo. La norma de hacer bien es la única virtud moral que debe imponerse.

Esta sección sigue refiriéndose a la niñez, entre los doce y trece años. El cuerpo sigue desarrollándose y la curiosidad natural también. Rousseau dice: «El niño no sabe algo porque se lo hayas dicho, sino porque lo ha comprendido él mismo», sugiriendo que el niño se inspire por su voluntad, que solo se le den métodos para despertar su interés y no su aburrimiento. Entonces es cuando Rousseau empieza a enseñarle a conservar, de modo que tenga más derecho moral.

También afirma que el niño debe aprender del intercambio de pensamientos e ideas; ve un beneficio social en que el niño pueda integrarse en la sociedad sin que lo perturben.

Con esta sección comienza la adolescencia. Rousseau afirma que «el niño no puede ponerse en el lugar de otros, pero una vez se alcanza la adolescencia, puede y hace así: Emilio por fin puede ser introducido en la sociedad». Ya en la adolescencia, Emilio tiene un mejor entendimiento de los sentimientos, pero también se exaltan las pasiones. Rousseau dice que «Nuestras pasiones son los principales instrumentos de nuestra conservación», pues para él, el sexo, la pasión y el amor son producto de un movimiento natural.

Formar al hombre a partir de la naturaleza no es hacerlo salvaje, sino no dejar que se gobierne. También en esta parte, se expone a Emilio a la religión, pero no logra verla como algo significativo para él.

Finaliza la adolescencia a los veinte años, cuando Emilio y su prometida Sofía van alcanzando la madurez y la vida matrimonial.


Logró identificar y nombrar 21 nuevas especies (IPNI).








</doc>
<doc id="17126" url="https://es.wikipedia.org/wiki?curid=17126" title="Biblioteca Nacional del Perú">
Biblioteca Nacional del Perú

La Biblioteca Nacional del Perú (BNP) es una institución pública ubicada en la ciudad de Lima, y que depende del Ministerio de Cultura, entidad ejecutiva del Estado Peruano. Alberga una colección de libros, periódicos, revistas, manuscritos, diversos documentos históricos, públicos, comunales y particulares; filmes, fotografías y otros análogos, erigiéndose así como orgullo y símbolo vivo y útil de la nación peruana. 

Actualmente cuenta con dos sedes, ambas en la capital del Perú y abiertas al público: la más moderna ubicada en la Avenida de la Poesía, en el distrito de San Borja diseñada por el arquitecto peruano Franco Vella, proyecto ganador del Hexágono de oro en la XII Bienal de Arquitectura del Perú en el año 2006, y el edificio original situado en la Avenida Abancay en el distrito de Lima.

En 1821 el general don José de San Martín y Matorras fundó en Lima la Biblioteca Nacional del Perú, como consecuencia de una iniciativa de su Ministro de Guerra y de Gobierno, Bernardo de Monteagudo, mediante el "Decreto de Creación de la Biblioteca Nacional" de 28 de agosto del mismo año. La saludó como: "(...) "uno de los medios más eficaces para poner en circulación los valores intelectuales"". El mismo San Martín dona cerca de 700 libros a la biblioteca, en tanto que Monteagudo donó su biblioteca personal.

En sus inicios la biblioteca contaba con 11000 libros que provenían de las confiscaciones que realizaron las autoridades del Virreinato del Perú a la orden de los jesuitas, que fueron expulsados de los dominios españoles en 1767. La orden de los jesuitas mantenía una biblioteca copiosa de diversas ciencias y humanidades en Lima.

Sin embargo, la BNP ha debido enfrentar duros acontecimientos a lo largo de su historia.

Entre 1823 y 1824, en medio del proceso de consolidación de la independencia del Perú, sufrió el ingreso de las tropas realistas a Lima, lo que ocasionó la pérdida de buena parte de la colección con la que contaba al momento de su inauguración.

Posteriormente, el 10 de marzo de 1881, tras la entrada y ocupación de Lima por parte del ejército chileno durante la Guerra del Pacífico, la tropa chilena comenzó a ocupar diversos recintos culturales, entre ellos la Biblioteca de Lima, lugares desde los cuales se incautaron objetos y bienes científicos y culturales, tales como instrumentos, herramientas, mobiliario y libros con el fin de ser llevados a Chile, por vía marítima.
La biblioteca contaba con una cifra estimada de unos 35000 a 50000 volúmenes, entre los cuales se encontraban incunables, manuscritos y libros que habían sido impresos por primera vez en América, allí también se encontraban las Memorias de los Virreyes (documentos que cada virrey del Perú realizaba después de su mandato). Dicho material fue objeto de requisición por parte de las tropas chilenas; sin embargo, varios textos de la biblioteca así como material científico, se perdió en las aduanas en el trayecto a Chile, ya que la prioridad era el armamento, quedando un buen número en manos de particulares en territorio peruano.

A Chile arribaron, en dos envíos de la Intendencia General del Ejército, un total de 103 grandes cajones y otros 80 bultos, que fueron recibidos y catalogados por Ignacio Domeyko y Diego Barros Arana, y en agosto de 1881 se publicó el inventario realizado, bajo el título "Lista de libros traídos de Perú", en el Diario Oficial.

A fines de noviembre de 1883, Ricardo Palma fue nombrado director de la BNP. A los pocos días, Palma informó que quedaban poco más de 700 libros en la biblioteca. El mismo Ricardo Palma, casi en solitario y personalmente, realizó una campaña de recolección de libros de casa en casa, lo cual le llevó a ganarse el mote de ""el bibliotecario mendigo"".

En 1884, la biblioteca fue reinaugurada. Ese año, Ricardo Palma solicitó a Chile la devolución del material sustraído por las tropas chilenas, lo cual tuvo eco en Santiago y, por orden del presidente Domingo Santa María, recibió la devolución de 30000 libros. De todos modos, diversos libros peruanos permanecieron en Chile mucho después y los gobiernos de ambos países iniciaron conversaciones para su devolución.

A su vez, por órdenes del Gobierno peruano, el arquitecto Michele Trefogli reformó substancialmente el edificio de la Biblioteca Nacional de Lima.

Posteriormente, otro hecho trágico ha marcado la historia en la BNP. El 10 de mayo de 1943, un incendio destruyó valiosísimo material de la Biblioteca que era, junto con las de la Ciudad de México y Río de Janeiro, una de las más ilustres de América, y redujo el edificio a escombros. En tal ocasión, su director era Carlos A. Romero, un octogenario, quien ante el abandono material y tecnobiblográfico, en su oportunidad, no hizo las denuncias correspondientes, con tal de mantener su puesto y, además, no colaboró con los jóvenes catalogadores. La BNP contaba para aquel entonces con cerca de 200000 volúmenes, incluyendo manuscritos e incunables.

Tras el incendio, el gobierno de Manuel Prado nombró a Jorge Basadre como Director de la Biblioteca Nacional. Basadre, con una paciente labor, logró levantar a la BNP de sus cenizas y la convirtió en una institución altamente técnica. El historiador tacneño emprendió la inmediata reestructuración del material bibliográfico, la formación técnica del personal y la reconstrucción del edificio principal. Creó la Escuela Nacional de Bibliotecarios en 1944 y es considerado como "El Padre de la Bibliotecología Peruana", por su contribución al desarrollo del movimiento bibliotecario en el Perú.

En 1970, se iniciaron las labores de ampliación del edificio de la BNP.

En 1986, durante la dirección de Juan Mejía Baca, se consigue el terreno para la futura nueva sede de la biblioteca, cuya construcción se inició en 1992, siendo concluida e inaugurada la nueva sede, distrito de San Borja, en 2006.

El 5 de noviembre de 2007, luego de una investigación histórica, bibliográfica y de sus catálogos, la Dirección de Bibliotecas, Archivos y Museos de Chile, procedió a la devolución de 3788 libros originalmente de propiedad de la Biblioteca de Lima, por los sellos y rúbricas que poseían, y que se encontraban en la Biblioteca Nacional de Chile y en la Biblioteca Santiago Severín de Valparaíso.

La Biblioteca Nacional del Perú es una institución dedicada a la administración eficaz del patrimonio cultural documental bibliográfico, así como del capital universal que posee con la finalidad de coadyuvar al desarrollo cultural, científico y tecnológico, contribuyendo al desarrollo económico y social, y apoyando la formación de ciudadanos y asociaciones, informados y proactivos.

Recreando verazmente todas las raíces biográficas de los grandes literatos, hechos y hazañas de la historia del Perú, esta biblioteca es uno de los más grandes legados que don José de San Martín dejó al Perú en 1821, después de su campaña conquistadora. Hoy en día emblema Cultural a nivel Nacional.


El 17 de septiembre de 1822 se inauguró la Biblioteca Nacional, que contaba con 11 mil 256 volúmenes que procedían de la antigua biblioteca de los jesuitas y de donaciones particulares, entre ellos 600 volúmenes de propiedad del General San Martín. Como Primer Bibliotecario fue nombrado el clérigo arequipeño y brillante orador del Congreso Constituyente, don Mariano José de Arce.

Después del incendio del 10 de mayo de 1943, nombrado como Director Jorge Basadre, designa a Emilio Harth Terré elaborar los nuevos planos de la Biblioteca Nacional. El local de la avenida Abancay, sede principal cuya construcción data de mediados del siglo XX, fue declarado por el Instituto Nacional de Cultura (INC) como Monumento Histórico.

Durante 190 años la BNP ocupó el histórico local de la Av. Abancay, en el centro de la capital, ahora convertido en la Gran Biblioteca Pública de Lima.

El 27 de marzo de 2006 se inauguró un segundo y moderno local, diseñado por los arquitectos Guillermo Claux Alfaro, Franco Vella Zardín, Walter Morales Llanos y Augusta Estremadoyro de Vella, en el distrito limeño de San Borja en la intersección de la Avenida Javier Prado con la Avenida Aviación frente al Museo de la Nación del Perú. La gestión para lograrlo la inició Juan Mejía Baca cuando fue su director en 1986. En ese año se logró conseguir el terreno en San Borja para el nuevo local.

El proyecto Arquitectónico de la Biblioteca Nacional del Perú recibió la máxima distinción de la arquitectura peruana que es el Premio Hexágono de Oro en la XII Bienal de Arquitectura Peruana en el año 2006, premio otorgado por el Colegio de Arquitectos del Perú.

Mediante un Decreto Legislativo, promulgado el 31 de diciembre de 1989, se estableció que una tercera parte de la tasa de 30.00 dólares estadounidenses del Impuesto de Salida al Exterior se destinara al proyecto de construcción del nuevo local. Este decreto fue derogado en agosto de 1992.
La construcción de la primera etapa de esta nueva sede se inició en enero de 1996 pero se detuvieron las obras en marzo de 1997 por falta de financiamiento.

Unos años después se realizó la campaña "Un nuevo sol para la Biblioteca Nacional del Perú" para recaudar fondos que ayuden a reiniciar los trabajos de construcción. Finalmente se retomaron en marzo del 2004.

En presencia del presidente Alejandro Toledo, el Director de la Biblioteca Sinesio López y otras personalidades, el 27 de marzo de 2006 se inauguró la nueva sede de San Borja. Consta de un edificio de 20.000 metros cuadrados, depósitos climatizados para la apropiada conservación de las obras, doce salas de lectura con capacidad para 554 lectores, mobiliario especial, casilleros personales y computadoras con conexión a Internet para el uso del público. El 17 de abril del mismo año se inició oficialmente la atención al público.




</doc>
<doc id="17128" url="https://es.wikipedia.org/wiki?curid=17128" title="Consejo Nacional de Camélidos Sudamericanos">
Consejo Nacional de Camélidos Sudamericanos

Dependiente del Ministerio de Agricultura del Perú, el Consejo Nacional de Camélidos Sudamericanos tenía su sede en Lima. Sus actividades estaban relacionadas con las cuatro especies de camélidos: las alpacas, llamas, vicuñas y guanacos. Fue desactivado en 2007 y sus funciones transferidas a los gobiernos regionales. Se han propuesto iniciativas para generar un nuevo organismo con un rol similar. 

Era un organismo especializado que promueve el desarrollo de la producción de camélidos sudamericanos, buscaba el mejoramiento y desarrollo de productos y la ampliación de mercados, a través de la organización productiva de las comunidades campesinas y los pequeños productores, consolidando su capacidad de gestión y su competitividad. 




</doc>
<doc id="17131" url="https://es.wikipedia.org/wiki?curid=17131" title="Dead Can Dance">
Dead Can Dance

Dead Can Dance es un grupo musical británico-australiano formado en Melbourne en 1981 por el británico Brendan Perry y la australiana Lisa Gerrard. La banda se asentó en Londres en mayo de 1982. Es una de las bandas de culto más importantes de las escenas del rock gótico, dark wave, post-punk y world music.

El historiador de la música australiano Ian McFarlane describe el estilo de Dead Can Dance como «paisajes sonoros construidos de grandeza fascinante y solemne belleza; polirritmias africanas, folklore gaélico, canto gregoriano, mantras de Oriente Medio y art rock».

Después de haberse disuelto en 1998, la banda se reunió brevemente en 2005 para una gira mundial y se reformó en 2011, publicando un nuevo álbum ("Anastasis") y embarcándose en varias giras.

Dead Can Dance se formó en Melbourne en el año 1981 estableciéndose inicialmente en Australia. Al no tener grandes perspectivas de éxito en ese país, se trasladaron a Londres, donde tras un año firmaron para el legendario sello de rock alternativo 4AD, convirtiéndose en una de sus bandas más importantes. Continuaron trabajando juntos hasta adentrados los años 1990, cuando comenzaron a trabajar por separado. Lisa Gerrard retornó a Australia, mientras Brendan Perry se fue a Irlanda, lugar en que compró una vieja iglesia, Quivy Church, en la cual vive y trabaja.

Asignar un género musical a Dead Can Dance es difícil, puesto que su estilo era particularmente ecléctico. Sin embargo, aunque a menudo sus primeros trabajos son clasificados como música gótica, los propios miembros han desmentido esto. Cada uno de los discos del grupo está influenciado por una o varias culturas, líneas estéticas y movimientos musicales desde lo más antiguo como los cantos gregorianos y la música tradicional persa. Sin embargo, siempre mantienen raíces en la música étnica antigua. Sus últimos discos son extraordinariamente diferentes a los tres primeros.

El nombre "Dead Can Dance" significa poner vida nuevamente dentro de algo que está muerto, o que hace mucho no ha sido utilizado. Algunos de los instrumentos que usan son antiguos o poco frecuentes en la música occidental. Quizás lo primero que debemos tratar de descifrar antes de hablar de Dead Can Dance, es el origen y significado de su nombre, el que según declaraciones de Gerrard y Perry nace de un extenso proceso de meditación, ante la necesidad de inyectar nueva vida a música e instrumentos que podían ser considerados como muertos y obsoletos. Es precisamente esta interpretación la que justifica la utilización de toda una gama de elementos musicales, que van desde los más comunes, como lo son la guitarra, los teclados, la percusión y los sintetizadores, hasta otros de matices mucho más clásicos, como la gaita, el violín, el violonchelo, la tuba, el yangqin o la zanfoña

Se separaron temporalmente en 1998 y se reunieron para una gira por Europa, Canadá, Estados Unidos y México en 2005. Aprovecharon esa gira para publicar su disco recopilatorio "Memento," siguiendo la estela en versión reducida del que editaron en 2001 que se suponía que iba a ser su despedida Box Set.

Brendan prometió un álbum y una gira para el 2012, y cumplió con su palabra, ya que el 8 de agosto de 2012 arrancó una nueva gira mundial que promocionó su álbum "Anastasis" (2012), lanzado en agosto de ese mismo año.

La canción "The Host of Seraphim" perteneció a la banda sonora de las películas Baraka, The Mist y .








</doc>
<doc id="17134" url="https://es.wikipedia.org/wiki?curid=17134" title="PCR">
PCR

Las siglas PCR pueden hacer referencia a:


Diversos partidos políticos:



</doc>
<doc id="17138" url="https://es.wikipedia.org/wiki?curid=17138" title="Teorema fundamental del cálculo">
Teorema fundamental del cálculo

El teorema fundamental del cálculo consiste (intuitivamente) en la afirmación de que la derivación e integración de una función son operaciones inversas. Esto significa que toda función acotada e integrable (siendo continua o discontinua en un número finito de puntos) verifica que la derivada de su integral es igual a ella misma. Este teorema es central en la rama de las matemáticas denominada análisis matemático o cálculo infinitesimal.

El teorema fue fundamental porque hasta entonces el cálculo aproximado de áreas -integrales- en el que se venía trabajando desde Arquímedes, era una rama de las matemáticas que se seguía por separado del cálculo diferencial que se venía desarrollando por Isaac Newton, Isaac Barrow y Gottfried Leibniz en el siglo XVIII, y dio lugar a conceptos como el de las derivadas. Las integrales eran investigadas como formas de estudiar áreas y volúmenes, hasta que en ese punto de la historia ambas ramas convergieron, al demostrarse que el estudio del "área bajo una función" estaba íntimamente vinculado al cálculo diferencial, resultando la integración la operación inversa a la derivación.

Una consecuencia directa de este teorema es la regla de Barrow, denominada en ocasiones segundo teorema fundamental del cálculo, y que permite calcular la integral de una función utilizando la integral indefinida de la función al ser integrada.

El teorema fundamental del cálculo se refiere a la diferenciación e integración, demostrando que estas dos operaciones son esencialmente inversas la una de la otra. Antes del descubrimiento de este teorema, no se reconoció que estas dos operaciones estaban relacionadas. Los antiguos matemáticos griegos sabían cómo calcular el área a través de los infinitesimales, una operación que ahora llamaríamos integración. Los orígenes de la diferenciación son también anteriores al teorema fundamental del cálculo en cientos de años; por ejemplo, en el siglo XIV las nociones de "continuidad" de funciones y de "movimiento" eran estudiadas por los calculadores de Oxford y otros estudiosos. La relevancia histórica del teorema fundamental del cálculo no es la capacidad de calcular estas operaciones, sino la constatación de que estas dos operaciones distintas en apariencia (cálculo de áreas geométricas y cálculo de velocidades) estaban finalmente en estrecha relación.

La primera declaración publicada y prueba de una versión restringida del teorema fundamental fue hecha por James Gregory (1638–1675). Isaac Barrow (1630–1677) demostró una versión más generalizada del teorema, mientras que el estudiante de Barrow, Isaac Newton (1642–1727), completó el desarrollo de la teoría matemática concernida. Gottfried Leibniz (1646–1716) sistematizó el conocimiento en un cálculo de las cantidades infinitesimales e introdujo la notación utilizada en la actualidad.

Supóngase que se tiene una función continua "y" = f("x") cuya representación gráfica es una curva. Entonces, para cada valor de "x" tiene sentido de manera intuitiva pensar que existe una función "A"("x") que representa el área bajo la curva entre 0 y "x" aún sin conocer su expresión.

Supóngase ahora que se quiere calcular el área bajo la curva entre "x" y "x"+"h". Se podría hacer hallando el área entre 0 y "x"+"h" y luego restando el área entre 0 y "x". En resumen, el área sería "A"("x"+"h") − "A"("x").

Otra manera de estimar esta misma área es multiplicar "h" por f("x") para hallar el área de un rectángulo que coincide aproximadamente con la "loncha". Nótese que la aproximación al área buscada es más precisa cuanto más pequeño sea el valor de "h".

Por lo tanto, se puede decir que "A"("x"+"h") − "A"("x") es aproximadamente igual a f("x") · "h", y que la precisión de esta aproximación mejora al disminuir el valor de "h". En otras palabras, ƒ("x")·"h" ≈ "A"("x"+"h") − "A"("x"), convirtiéndose esta aproximación en igualdad cuando "h" tiende a 0 como límite.

Dividiendo los dos lados de la ecuación por "h" se obtiene
Cuando "h" tiende a 0, se observa que el miembro derecho de la ecuación es sencillamente la derivada "A"’("x") de la función "A"("x") y que el miembro izquierdo se queda en ƒ("x") al ya no estar "h" presente.

Se muestra entonces de manera informal que ƒ("x") = "A"’("x"), es decir, que la derivada de la función de área "A"("x") es en realidad la función ƒ("x"). Dicho de otra forma, la función de área "A"("x") es la antiderivada de la función original.

Lo que se ha mostrado es que, intuitivamente, calcular la derivada de una función y "hallar el área" bajo su curva son operaciones "inversas", es decir, el objetivo del teorema fundamental del cálculo integral.

Usando la Regla de la cadena obtenemos como consecuencia directa del primer teorema fundamental del cálculo infinitesimal:

Siendo formula_1una función integrable sobre el intervalo ["a"("x"),"b"("x")] con "a"("x") y "b"("x") derivables.

Sea h>0. Entonces formula_2.

Se define formula_3 y formula_4 como:

Aplicando el 'lema' se observa que:
Por lo tanto,

Sea formula_5. Sean

Aplicando el 'lema' se observa que:

Como:
entonces,
Puesto que formula_5, se tiene que:

Y como formula_7 es continua en c se tiene que

y esto lleva a que

El segundo teorema fundamental del cálculo integral (o regla de Newton-Leibniz, o también regla de Barrow, en honor al matemático inglés Isaac Barrow, profesor de Isaac Newton) es una propiedad de las funciones continuas que permite calcular fácilmente el valor de la integral definida a partir de cualquiera de las primitivas de la función.

Como se puede integrar inmediatamente.





</doc>
<doc id="17144" url="https://es.wikipedia.org/wiki?curid=17144" title="Universidad de Oxford">
Universidad de Oxford

La Universidad de Oxford es una universidad de investigación ubicada en Oxford, Reino Unido. Sin conocerse la fecha exacta de su fundación, hay evidencias de una institución de enseñanza ya en 1096, lo que la convierte en la primera universidad de habla inglesa y la segunda universidad más longeva del mundo. La universidad creció rápidamente desde 1167 cuando Enrique II de Inglaterra prohibió que los estudiantes ingleses asistieran a la Universidad de París. Como consecuencia de disputas entre los estudiantes y los habitantes de Oxford en 1209, algunos profesores se marcharon al noreste, a Cambridge, donde fundaron la que sería luego la Universidad de Cambridge. Estas dos antiguas universidades son a menudo nombradas conjuntamente con el sobrenombre de «Oxbridge». La historia e influencia de la Universidad de Oxford la ha convertido en una de las más prestigiosas del mundo.

La universidad está compuesta de varias instituciones, 38 "colleges" constituyentes y un amplio abanico de departamentos académicos que están organizados en cuatro divisiones. Todos los "colleges" son instituciones con autogobierno dentro de la universidad, controlan sus miembros y tienen su propia estructura interna y actividades. Oxford es una ciudad universitaria que no cuenta con un campus principal, pues sus edificios y facultades están repartidos por todo el centro de la ciudad. La enseñanza de pregrado está organizada alrededor de tutorías semanales en los "colleges" y salones, apoyadas por clases, conferencias, seminarios y trabajo de laboratorio ofrecidos por las facultades y departamentos universitarios. Algunas enseñanzas de posgrado incluyen tutorías también organizadas por facultades y departamentos. Oxford cuenta con el museo universitario más antiguo del mundo, así como con la mayor editorial universitaria y la biblioteca académica de ámbito nacional más extensa. La Universidad de Oxford suele figurar entre las mejores instituciones de enseñanza superior del mundo según las valoraciones de diferentes organizaciones.

Oxford ha educado muchos alumnos destacados, incluyendo 29 galardonados con el premio Nobel, 27 Primeros Ministros del Reino Unido e innumerables jefes de estado y de gobierno de todo el mundo. A fecha de 2017, 69 ganadores del premio Nobel, 3 Medallas Fields y 6 ganadores del Premio Turing han estudiado, trabajado o colaborado con la Universidad de Oxford. Sus alumnos han logrado 160 medallas olímpicas. Oxford además concede la Beca Rhodes, una de las becas internacionales más antiguas.

Se desconoce la fecha de fundación de la universidad, y tal vez no existió como un suceso en concreto, pero hay evidencia de actividades de enseñanza desde el año 1096. Cuando Enrique II de Inglaterra prohibió a los estudiantes ingleses la asistencia a los colegios de estudios superiores de París, en el año 1167, Oxford empezó a crecer con rapidez. La fundación de las primeras residencias estudiantiles, que luego devinieron en los "colleges", data desde esta época en adelante. Después del asesinato de dos estudiantes acusados de violación en 1209, la Universidad fue disuelta. El 20 de junio de 1214, la Universidad volvió a Oxford con una carta de aceptación negociada por Nicolás de Romanis, delegado papal, y en 1231 recibe la carta de Universidad.

El principal rival de la Universidad de Oxford es la Universidad de Cambridge, fundada poco tiempo después. El conjunto de ambas instituciones se conoce popularmente como Oxbridge. Tradicionalmente se le ha considerado a Cambridge superior en temas científicos, y a Oxford en humanidades. Aunque esto es más una percepción que una realidad, dicho sesgo deriva del énfasis que históricamente las universidades han dado a algunas disciplinas. Ambas universidades pertenecen al Grupo Russell de universidades británicas dedicadas a la investigación.

Oxford es una universidad colegiada lo cual puede resultar confuso para quien no esté familiarizado con ella. La Universidad es esencialmente una federación compuesta por cerca de cuarenta "Halls" ("Permanent Private Halls, PPHs") y colegios (en inglés "colleges") autogobernados con una administración central que tiene a su cabeza un Vicecanciller. Los departamentos académicos están adscritos a su estructura central. Estos no pertenecen a ningún colegio en particular. Los departamentos persiguen desarrollar la investigación, proveer facilidades para la enseñanza y el aprendizaje, organizar conferencias, seminarios y determinar los planes de estudio y las pautas para la enseñanza de los estudiantes. Los colegios se encargan de organizar las clases académicas para sus miembros de pregrado. Los miembros de un departamento académico están diseminados a través de distintos colegios. Ciertos colegios tienen determinadas fortalezas (por ejemplo, el Nuffield College especializado en ciencias sociales), pero son la excepción, porque la mayoría tienen un amplia variedad de profesores y alumnos de todas las ramas del estudio. Se proporcionan instalaciones como bibliotecas a todos los niveles: por la estructura central (la Biblioteca Bodleiana), por los departamentos (bibliotecas departamentales como la Biblioteca de la Facultad de Leyes) y por los colegios (los cuales generalmente mantienen bibliotecas multidisciplinares para uso de sus miembros.

El sistema colegiado de Oxford tiene su origen en que la universidad empezó a existir a través de la aglomeración gradual de instituciones independientes en la ciudad de Oxford. (Véanse también Colegios de la Universidad de Oxford y la lista de Colegios hermanos de Cambridge).

Además del nivel colegiado de organización, la universidad se subdivide en departamentos según áreas, como muchas otras universidades. Los departamentos desempeñan un papel importante en la educación de posgrado, y cada vez más en la de pregrado, ofreciendo conferencias y clases y organizando evaluaciones. Los departamentos son también centros de investigación, apoyados económicamente por instituciones externas que incluyen los principales consejos de investigación; si bien los colegios tienen interés en la investigación, la mayoría no son especialistas en algún área en términos de organización.

El principal cuerpo legislativo de la universidad es la Congregación ("Congregation", en inglés), que es la asamblea de todos los académicos que enseñan en la universidad. Otro cuerpo, la Convocatoria (en inglés "Convocation"), engloba a todos los graduados de la universidad, y era el principal órgano legislativo de la universidad. Hasta 1949, la "Convocation" elegía a los dos miembros del parlamento para la Universidad. Ahora tiene funciones muy limitadas, la principal de las cuales es elegir al Canciller de la Universidad (título en gran parte simbólico). La elección más reciente tuvo lugar en 2003, cuando fue elegido Chris Patten.

El cuerpo ejecutivo de la universidad es el Consejo Universitario. Este está compuesto por el vice-canciller (Dr. John Hood), los directores de departamento y otros miembros elegidos por la Congregación, además de observadores de la Unión Estudiantil. Además de la actual Casa de la Congregación, también hay una Antigua Casa de la Congregación, que de alguna manera sobrevivió a las reformas en el siglo XIX y que hoy sólo se usa para otorgar los grados.

El curso académico se divide en tres períodos, cada uno con una duración de ocho semanas. El período "Michaelmas" va desde comienzos de octubre hasta diciembre; "Hilary" normalmente va desde enero hasta antes de la pascua; y "Trinity" se extiende desde después de la pascua hasta junio. Estos períodos son de los más cortos de cualquier universidad británica, y la carga de trabajo es intensa.

Los colegios son entidades totalmente independientes, propietarias de sus inmuebles, con personal propio y su propio presupuesto.

Hay 38 "colleges" de la Universidad de Oxford y seis "halls", cada uno con su propia estructura interna y actividades. Todos los estudiantes y la mayoría del profesorado deben ser miembros tanto de la Universidad como de algún colegio o "hall". Aquellos que presiden los colegios de Oxford se conocen por varios títulos (según el colegio), incluyendo (en inglés) "warden", "provost", "principal", "president", "rector", "master" o "dean". Los colegios se reúnen normalmente en la Conferencia de los Colegios para discutir políticas administrativas y académicas.

Muchas de las oficinas de atención a los estudiantes son específicas de cada colegio, pero estos son mucho más que residencias estudiantiles. Proveen alojamiento, comida, bibliotecas, actividades deportivas y sociales, también nombran tutores encargados de seguir el trabajo de los estudiantes. Por otra parte, la universidad provee las clases, realiza los exámenes y otorga los títulos. Los colegios tienen la responsabilidad de admitir estudiantes de pregrado y de organizar su tutoría académica. Por el contrario, son los diferentes departamentos académicos los que se encargan de la formación de los estudiantes de posgrado.

La siguiente lista muestra los colegios constituyentes de la Universidad de Oxford y de sus colegios hermanos en la Universidad de Cambridge:

La admisión en la universidad es muy rigurosa y se basa en los méritos académicos y en el potencial del candidato. Los colegios son los que llevan a cabo las admisiones de pregrado, trabajando juntos para asegurar que los mejores estudiantes tengan un lugar en la universidad. La selección se hace con base a las referencias escolares, los ensayos personales, resultados conseguidos, resultados predichos, trabajo escrito, pruebas escritas y entrevistas.

La selección de los estudiantes de posgrado la realiza primero el departamento en el cual estudiará cada uno, y luego de forma secundaria el colegio al cual está asociado el departamento.

Al igual que Cambridge, Oxford se ha considerado tradicionalmente como un lugar para gente acomodada, aunque hoy día no es ese el caso. El coste de los estudios, en los días previos a la disponibilidad de becas estudiantiles, era prohibitivo a no ser que se tuviera beca o se fuera, antiguamente, un criado (que tenía que servir a sus compañeros como compensación por su matrícula). Las escuelas privadas y de gramática preparaban a sus pupilos específicamente para las pruebas de ingreso, e incluso algunas llegaban tan lejos como para instar a los alumnos a permanecer un año más estudiando solo para el examen. Los alumnos de otras escuelas públicas rara vez podían permitirse tal lujo.

Últimamente Oxford ha hecho grandes esfuerzos para atraer alumnos de las escuelas públicas, y la admisión tanto en Oxford como en Cambridge sigue basándose en los méritos académicos y en el potencial. Aproximadamente la mitad de los estudiantes de Oxford vienen de escuelas públicas. Sin embargo, todavía hay un gran debate público en la Gran Bretaña sobre si podría hacerse algo más para atraer a aquellos que vienen de capas sociales más deprimidas.

Hay muchos notables 'oxonians' (nombre otorgado a los egresados de la Universidad):

Veintiséis primeros ministros del Reino Unido han estudiado en la Universidad de Oxford, incluyendo a William Gladstone, Herbert Asquith, Clement Attlee, Harold Macmillan, Harold Wilson, Edward Heath, Margaret Thatcher, Tony Blair, David Cameron y Theresa May.

Al menos treinta dirigentes políticos internacionales han estudiado en Oxford. Y aquí se incluyen a Harald V de Noruega, Abdalá II de Jordania, tres Primeros Ministros de Australia (John Gorton, Malcolm Fraser y Bob Hawke), dos Primeros Ministros de Canadá (Lester B. Pearson, y John Turner), dos Primeros Ministros de la India (Manmohan Singh e Indira Gandhi), cuatro Primeros Ministros de Pakistán (Liaquat Ali Khan, Huseyn Shaheed Suhrawardy, Zulfiqar Ali Bhutto, y Benazir Bhutto), S. W. R. D. Bandaranaike de Sri Lanka, Norman Washington Manley de Jamaica, Eric Williams (Primer Ministro de Trinidad y Tobago), Pedro Pablo Kuczynski (Presidente de Perú), Víctor Raúl Haya de la Torre (intelectual, ideólogo y fundador del APRA), Álvaro Uribe (expresidente de Colombia), Abhisit Vejjajiva (Primer Ministro de Tailandia), Bill Clinton y Arthur Mutambara (Viceprimer Ministro de Zimbabue). La activista pro-democrática Birmana, y , Aung San Suu Kyi, fue una estudiante del colegio de St. Hugh. Cuarenta y siete ganadores del Premio Nobel han estudiado en Oxford.

Oxford ha también producido doce santos y veinte Arzobispos de Canterbury, incluyendo Rowan Williams, quien estudió en Wadham College y fue luego Profesor Canon en Christ Church. Otra figura religiosa fue Mirza Nasir Ahmad, el tercer Khalifatul Masih de la Comunidad Islámica Ahmadía y Shoghi Effendi, uno de los dirigentes elegidos del bahaísmo. El fundador de metodismo, John Wesley, estudió en Christ Church y fue elegido miembro del Lincoln College.

Unos cincuenta medallistas olímpicos tienen conexiones académicas con la Universidad, incluyendo a Sir Matthew Pinsent, cuatro veces medallista de oro por remo. T. E. Lawrence (mejor conocido como "Lawrence de Arabia", militar, arqueólogo y escritor británico) fue estudiante de Jesus College. Otros estudiantes ilustres incluyen al explorador, cortesano y hombre de letras Sir Walter Raleigh y al magnate de los medios de comunicación australiano, Rupert Murdoch.

La larga lista de escritores asociados con la Universidad incluye a Owen Barfield, John Fowles, Theodor Geisel, Thomas Middleton, Samuel Johnson, Robert Graves, Mary Renault, Evelyn Waugh, Lewis Carroll, Aldous Huxley, Oscar Wilde, C. S. Lewis, J. R. R. Tolkien (escritor de "El Señor de los Anillos"), Graham Greene, V.S.Naipaul, Phillip Pullman, Joseph Heller, Vikram Seth, los poetas anglosajones Percy Bysshe Shelley, John Donne, A. E. Housman, W. H. Auden, T. S. Eliot, Wendy Perriam y Philip Larkin, y siete poetas laureados (Thomas Warton, Henry James Pye, Robert Southey, Robert Bridges, Cecil Day-Lewis, Sir John Betjeman, y Andrew Motion).

Los economistas Adam Smith, Alfred Marshall, E. F. Schumacher y Amartya Sen, y los filósofos, John Locke, Thomas Hobbes, Jeremy Bentham e Isaiah Berlin, gozaron de muchos de sus días en Oxford, como también lo hicieron muchos pioneros científicos como Albert Einstein y Erwin Schrödinger. Algunos pioneros de la Revolución científica, tal como Robert Hooke y Robert Boyle, estudiaron o son asociados con la Universidad. Algunos lingüistas, tal como Ghil'ad Zuckermann, estudiaron en la Universidad.

Algunos científicos contemporáneos también estudiaron en la Universidad incluyendo Silvia Maureen Williams, Stephen Hawking, Richard Dawkins, el ganador del Premio Nobel de la Física Anthony James Leggett, Tim Berners-Lee (co-inventor de la World Wide Web), y la química Dorothy Hodgkin.

Los compositores Hubert Parry, George Butterworth, John Taverner, William Walton, y Andrew Lloyd-Webber han estado todos involucrados con la Universidad.

Los actores y actrices Hugh Grant, Kate Beckinsale, Dudley Moore, Michael Palin,Emma Watson y Terry Jones fueron en algún momento estudiantes de Oxford, tal como el ganador del Oscar Florian Henckel von Donnersmarck y los cineastas Ken Loach y Richard Curtis.

Existe una segunda universidad en Oxford, la Universidad Oxford Brookes (1), conocida como "Politécnico de Oxford". Sus campus están ubicados en su mayoría en los barrios del este de la ciudad. Su escuela de arquitectura figura entre las mejores del mundo. También hay un cierto número de «colegios» independientes que funcionan de manera independiente pero bajo el nombre de la universidad de Oxford. Cada uno tiene su historia, longevidad y tradiciones propias.

La novela "Todas las almas", del escritor español Javier Marías, narra la historia de un joven profesor español que imparte clases de traducción en la Tylor Institution de Oxford y que casi mantiene relaciones esporádicas con Clare Bayes, una mujer casada. Se trata de una novela autobiográfica, donde la ficción es la mínima parte de la historia real del autor, Javier Marías, cuando impartió clases de literatura española en la Tylor de Oxford, a mediados de los años 80.

En palabras de Marías el hecho de que el protagonista de la novela diera clases durante dos años en la Tylor era un préstamo literario. «Poco de lo que en el libro se cuenta coincide con lo que yo viví o supe en Oxford, o sólo lo más accesorio y que no afecta a los hechos: el ambiente amortiguado de la ciudad reservada o esquiva y sus profesores atemporales […], las oscuras y minuciosas librerías de viejo…», dijo Marías años más tarde en "Negra espalda del tiempo".




</doc>
<doc id="17149" url="https://es.wikipedia.org/wiki?curid=17149" title="Máser">
Máser

Un máser es un amplificador de microondas por la emisión estimulada de radiación, un amplificador similar al láser pero que opera en la región de microondas del espectro electromagnético y sirve para recibir señales muy débiles. La palabra deriva del acrónimo en inglés MASER, por "Microwave Amplification by Stimulated Emission of Radiation". 

Su funcionamiento está basado en el fenómeno de emisión estimulada de radiación, enunciado por Albert Einstein en 1916. En la Universidad de Columbia, Estados Unidos, Charles Hard Townes propuso en 1951 su existencia y en 1954 construyó el primero.

Cuando una molécula o un átomo se hallan en un estado energético adecuado y pasan cerca de una onda electromagnética, ésta puede inducirles a emitir energía en forma de otra radiación electromagnética con la misma longitud de onda que refuerza la onda de paso y desencadena una cascada de fenómenos que llevan a aumentar mucho la intensidad del impulso original. En algunas nubes de materia interestelar excitada por la radiación de estrellas cercanas se produce el mismo fenómeno, que conduce a la formación de un intenso haz de radiación con longitud de onda bien definida.


</doc>
<doc id="17151" url="https://es.wikipedia.org/wiki?curid=17151" title="Efecto Doppler">
Efecto Doppler

El efecto Doppler, llamado así por el físico austriaco Christian Andreas Doppler, es el cambio de frecuencia aparente de una onda producido por el movimiento relativo de la fuente respecto a su observador.

Hay ejemplos cotidianos del efecto Doppler en los que la velocidad a la que se mueve el objeto que emite las ondas es comparable a la velocidad de propagación de esas ondas. La velocidad de una ambulancia (50 km/h) puede parecer insignificante respecto a la velocidad del sonido al nivel del mar (unos 1235 km/h), sin embargo, se trata de aproximadamente un 4 % de la velocidad del sonido, fracción suficientemente grande como para provocar que se aprecie claramente el cambio del sonido de la sirena desde un tono más agudo a uno más grave, justo en el momento en que el vehículo pasa al lado del observador.

En el caso del espectro visible de la radiación electromagnética, si el objeto se aleja, su luz se desplaza a longitudes de onda más largas, produciéndose un corrimiento hacia el rojo. Si el objeto se acerca, su luz presenta una longitud de onda más corta, desplazándose hacia el azul. Esta desviación hacia el rojo o el azul es muy leve incluso para velocidades elevadas, como las velocidades relativas entre estrellas o entre galaxias, y el ojo humano no puede captarlo, solamente medirlo indirectamente utilizando instrumentos de precisión como espectrómetros. Si el objeto emisor se moviera a fracciones significativas de la velocidad de la luz, sí sería apreciable de forma directa la variación de longitud de onda.

Doppler propuso este efecto en 1842 en su tratado "Über das farbige Licht der Doppelsterne und einige andere Gestirne des Himmels" ("Sobre el color de la luz en estrellas binarias y otros astros"). El científico neerlandés Christoph Hendrik Diederik Buys Ballot investigó esta hipótesis en 1845 para el caso de ondas sonoras y confirmó que el tono de un sonido emitido por una fuente que se aproxima al observador es más agudo que si la fuente se aleja. Hippolyte Fizeau descubrió independientemente el mismo fenómeno en el caso de ondas electromagnéticas en 1848. En Francia este efecto se conoce como «efecto Doppler-Fizeau» y en los Países Bajos como «efecto Doppler-Gestirne». En Gran Bretaña, John Scott Russell hizo un estudio experimental del efecto Doppler (1848).

En física clásica, donde las velocidades del emisor (también denominado «fuente») y del receptor (o también «observador») con respecto al medio son inferiores a la velocidad de las ondas en el propio medio, la relación entre la frecuencia observada formula_1 y la frecuencia emitida formula_2 viene dada por:

"(Conclusión: la frecuencia aumenta cuando fuente y observador se acercan entre sí, y se reduce cuando se alejan)".

En la fórmula anterior se supone que la fuente está acercándose (o alejándose) «directamente» del observador. Si la fuente se acerca al observador con velocidad constante, pero en una trayectoria «no incidente» (como por ejemplo, un avión en vuelo respecto a un observador situado en tierra), entonces:

Cuando el observador se encuentra muy cerca de la trayectoria del objeto, la transición de alta a baja frecuencia es muy abrupta. En cambio, cuando el observador está lejos de la trayectoria del objeto, la transición de alta a baja frecuencia es gradual.

Si las velocidades formula_6 y formula_5 son pequeñas en comparación con la velocidad de la onda, la relación entre la frecuencia observada formula_1 y la frecuencia emitida formula_2 es aproximadamente

En los cuatro gráficos siguientes se incluye una animación con el análisis del comportamiento de las ondas sonoras en los cuatro casos característicos del efecto Doppler en relación con la velocidad del emisor respecto a la velocidad de propagación del sonido en el aire (Mach 1):

Para entender lo que sucede, considérese la siguiente analogía. Alguien lanza una bola cada segundo a un hombre. Se asume que las bolas viajan con velocidad constante. Si el lanzador está parado, el hombre va a recibir una bola cada segundo. Sin embargo, si el lanzador se está moviendo hacia el hombre, este va a recibir las bolas con mayor frecuencia debido a que las bolas estarán menos espaciadas. El inverso es cierto si el lanzador se aleja del hombre. Por lo que en realidad es la "longitud de onda" la que es afectada; como consecuencia, la frecuencia recibida también se ve afectada. También puede afirmarse que la velocidad de la onda permanece constante, mientras que se producen cambios en la longitud de onda; y por lo tanto, la frecuencia cambia también.

Para un observador en reposo respecto al medio, si una fuente en movimiento está emitiendo ondas con una frecuencia real dada formula_2 (en este caso, la longitud de onda cambia pero la velocidad de transmisión de la onda se mantiene constante, por lo que la "velocidad de transmisión" de la onda no depende de la "velocidad de la fuente"), entonces el observador detecta ondas con una frecuencia formula_1 dada por:

Un análisis similar para un "observador" en movimiento y una fuente estacionaria (en este caso, la longitud de onda se mantiene constante, pero debido al movimiento, la velocidad a la que el observador recibe las ondas, y por lo tanto la "velocidad de transmisión" de la onda [con respecto al observador] cambia) produce la frecuencia observada:

Esto se puede generalizar en la ecuación que se presentó en la sección anterior:

Un efecto interesante fue predicho por lord Rayleigh en su libro clásico sobre el sonido: si la fuente se acerca al observador a dos veces la velocidad del sonido, una pieza musical emitida por dicha fuente se oiría en el tiempo y tono correcto, pero "al revés" (es decir, las notas del final de la pieza llegarían al observador antes que las de su comienzo). El efecto Doppler sobre el sonido solo se percibe claramente con objetos moviéndose a bastante velocidad: el cambio en la frecuencia de tono musical implica una velocidad de alrededor de 40 metros por segundo (144km/h). Sin embargo, los cambios de amplitud de los sonidos de los emisores en movimiento pueden ser fácilmente confundidos con pequeños cambios en la frecuencia. Neil A Downie ha demostrado cómo el efecto Doppler puede hacerse mucho más fácilmente audible mediante el uso de un emisor ultrasónico (por ejemplo, de 40 kHz) emitiendo desde objetos en movimiento. El observador debe utilizar un convertidor de frecuencia heterodino (como los que se utilizan en muchos detectores de murciélagos) con capacidad para trabajar en la banda de hasta los 40 kHz. En este caso, con el receptor ajustado para "traducir" las ondas recibidas a la banda de los 2000 Hz para hacerlas audibles, basta con que el emisor se desplace a tan solo 2 metros por segundo para que el observador perciba un desplazamiento de frecuencia de un tono entero (240 Hz).

En el caso de la sirena de un vehículo de emergencia en movimiento que pasa cerca de un observador, comenzará a oírse con una frecuencia más alta que la de su tono estacionario. Irá bajando a medida que se acerque, y continuará reduciéndose (por debajo de su tono estacionario) a medida que se aleje del observador. El astrónomo John Dobson explicó el efecto de este modo:

En otras palabras, si la sirena se acercase al observador directamente (con velocidad constante), la frecuencia percibida permanecería invariable hasta que el vehículo lo alcanzara, para saltar inmediatamente a un nuevo tono más bajo en cuanto empezara a alejarse. Debido a que el vehículo no pasa por el punto exacto que ocupa el observador, si no que lo rebasa a cierta distancia, su velocidad relativa presenta una componente radial que no permanece constante, sino que varía como una función del ángulo entre su línea de visión y la velocidad del vehículo que porta la sirena:

donde formula_19 es el ángulo entre la velocidad de avance del objeto y la línea de visión desde el objeto hacia el observador.

Imaginemos que un observador O se mueve con una velocidad formula_20 que tiene una dirección y sentido hacia una fuente de sonido S que se encuentra en reposo. El medio es aire y también se encuentra en reposo. La fuente emite un sonido de velocidad formula_21, frecuencia formula_22 y longitud de onda formula_23. Por lo tanto, la velocidad de las ondas respecto del observador no será formula_21, sino la siguiente:

Sin embargo, como la velocidad del medio no cambia, la longitud de onda será la misma, por lo tanto, si:

Pero como mencionamos en la primera explicación, el observador al acercarse a la fuente oirá un sonido más agudo, esto implica que su frecuencia es mayor. A esta frecuencia mayor captada por el observador se la denomina frecuencia aparente, que la denominamos formula_27.

El observador escuchará un sonido de mayor frecuencia debido a que formula_29

Analicemos el caso contrario: cuando el observador se aleja de la fuente, la velocidad será formula_30 y de manera análoga podemos deducir que formula_31

En este caso la frecuencia aparente percibida por el observador será mayor que la frecuencia real emitida por la fuente, lo que genera que el observador perciba un sonido más agudo.

Por tanto, la longitud de onda percibida para una fuente que se mueve con una velocidad formula_32 será:

Como formula_34 podemos deducir que:

Haciendo un razonamiento análogo para el caso contrario: fuente alejándose; podemos concluir que la frecuencia percibida por un observador en reposo con una fuente en movimiento será:

Cuando la fuente se acerque al observador se pondrá un signo (-) en el denominador, y cuando la fuente se aleje se reemplazará por (+).

Al terminar de leer lo anteriormente expuesto surge la siguiente pregunta: ¿Qué pasará si la fuente y el observador se mueven al mismo tiempo?. En este caso particular se aplica la siguiente fórmula, que no es más que una combinación de las dos:

El sentido del desplazamiento de la fuente y el observador son inversos:

Si el observador se acerca a la fuente el numerador es positivo, si se aleja negativo.

Si la fuente de sonido se acerca al observador el denominador es negativo, si se aleja es positivo.

Se puede dar el caso de numerador y denominador sean una suma, y también de numerador y denominador sean una resta.

Un observador se mueve a una velocidad de 42m/s hacia un trompetista en reposo. El trompetista está tocando (emitiendo) la nota la (440Hz). ¿Qué frecuencia percibirá el observador, sabiendo que formula_38 = 340m/s

Solución: Si el observador se acerca hacia la fuente, implica que la velocidad con que percibirá cada frente de onda será mayor, por lo tanto la frecuencia aparente será mayor a la real (en reposo). Para que esto ocurra debemos aplicar el signo (+) en la ecuación.

En este caso particular, el trompetista emite la nota la a 440Hz; sin embargo, el observador percibe una nota que vibra a una frecuencia de 494,353Hz, que se aproxima altamente a la frecuencia perteneciente a la nota si. Musicalmente hablando, el observador percibe el sonido con un tono más agudo del que se emite realmente.

En el caso de ondas electromagnéticas la fórmula de efecto Doppler es:formula_41
Siendo f la frecuencia del emisor, f' la que ve el receptor, v la velocidad del emisor respecto al receptor y formula_42 el factor de Lorentz dado por:

El efecto Doppler sobre las ondas electromagnéticas como la luz es de gran utilidad en astronomía, y se manifiesta en los denominados corrimiento al rojo o corrimiento al azul. Se ha utilizado para medir la velocidad a la que estrellas y galaxias están acercándose o alejándose de la Tierra; es decir, sus velocidades radiales. Este fenómeno físico se utiliza para detectar estrellas binarias, para medir la velocidad de giro de las estrellas y galaxias, o para detectar exoplanetas. (Debe tenerse en cuenta que el desplazamiento al rojo también se utiliza para medir la expansión del espacio, aunque en este caso no se trata realmente de un efecto Doppler).

El uso del efecto Doppler sobre la luz en astronomía depende del conocimiento que se tiene de que los espectros de las estrellas no son homogéneos. Exhiben líneas de absorción definidas de las frecuencias que están en correspondencia con las energías requeridas para excitar los electrones de varios elementos de un nivel a otro. El efecto Doppler es reconocible en el hecho de que los patrones conocidos de las líneas de absorción no aparecen siempre coincidiendo con las frecuencias que se obtienen a partir del espectro de una fuente de luz estacionaria. Dado que la luz azul tiene una frecuencia más alta que la luz roja, las líneas espectrales de una fuente de luz astronómica que se acerca exhiben un corrimiento al azul, y las de uno que se aleja experimentan un corrimiento hacia el rojo.

Entre las a la Tierra, las mayores velocidades radiales con respecto al Sol son +308 km/s (BD-15°4041, también conocida como LHS 52, situada a 81,7 años luz de distancia) y –260 km/s (Woolley 9722, también conocida como Wolf 1106 y LHS 64, situada a 78,2 años luz de distancia). Una velocidad radial positiva significa que la estrella se está alejando del Sol, negativa que se está acercando.

El efecto Doppler se utiliza en algunos tipos de radar, para medir la velocidad de los objetos detectados. Un haz de radar se dispara a un blanco móvil (por ejemplo, un automóvil, como en el uso que hace la policía del radar para detectar la velocidad de los vehículos) a medida que se acerca o se aleja de la fuente de radar. Cada onda sucesiva de radar tiene que viajar más lejos para alcanzar el coche, antes de ser reflejada y detectada de nuevo cerca de la fuente. Como cada onda tiene que moverse más lejos, la distancia entre cada onda aumenta, produciendo un aumento de la longitud de onda. En algunas situaciones, el haz del radar se utiliza con el coche en movimiento y, si se acerca al vehículo observado, entonces cada onda sucesiva recorre una distancia menor, produciendo una disminución de la longitud de onda. En cualquiera de estas situaciones, los cálculos del efecto Doppler permiten determinar con precisión la velocidad del vehículo observado por el radar. Por otra parte, la espoleta de proximidad, desarrollada durante la Segunda Guerra Mundial, se basa en el radar Doppler para detonar explosivos en el momento adecuado en función de su altura sobre el suelo o su distancia al objetivo.

Debido a que el desplazamiento Doppler afecta a la onda incidente en el objetivo, así como a la onda reflejada de nuevo al radar, el cambio en la frecuencia observado por un radar en movimiento respecto a un objetivo también en movimiento es función de su velocidad relativa formula_44, y es doble del que se registraría directamente entre el emisor y el receptor:

Una ecocardiografía puede, dentro de ciertos límites, producir una evaluación precisa de la dirección del flujo sanguíneo y de la velocidad de la sangre y el tejido cardíaco en cualquier punto arbitrario usando el efecto Doppler. Una de las limitaciones es que el haz de ultrasonidos debe ser lo más paralelo posible a la dirección del flujo de la sangre. Las mediciones de velocidad permiten la evaluación de las áreas de las válvulas cardíacas y de su funcionamiento; de todas las posibles comunicaciones anormales entre el lado izquierdo y el derecho del corazón; de cualquier fuga de sangre a través de las válvulas (insuficiencia valvular); y el cálculo del "gasto cardíaco".

Aunque el término "Doppler" se ha convertido en un sinónimo de "medición de la velocidad" en la imagen médica, en muchos casos, no es el desplazamiento de frecuencia (efecto Doppler) de la señal recibida lo que se mide, sino el cambio de fase (es decir, "cuando" llega la señal recibida, lo que permite calcular distancias).

Las mediciones de la velocidad del flujo de la sangre también se utilizan en otros campos de la medicina ecográfica, como obstetricia y en neurología. La medición de la velocidad del flujo sanguíneo en las arterias y las venas basada en el efecto Doppler es una herramienta eficaz para el diagnóstico de problemas vasculares como la estenosis.

Instrumentos como el velocímetro láser Doppler (LDV en inglés), y el velocímetro acústico Doppler (ADV en inglés) se han desarrollado para medir velocidades en el flujo de un fluido. El LDV emite un haz de luz y el ADV emite un tren de ondas acústicas ultrasónicas, y midiendo el efecto Doppler en las longitudes de onda de los reflejos de las partículas que se mueven con el flujo del fluido. El flujo real se calcula como una función de la velocidad del líquido y de la fase sólida. Esta técnica permite realizar mediciones de caudal no invasivas, con alta precisión y con alta frecuencia.

Originalmente desarrollado para mediciones de velocidad en aplicaciones médicas (flujo sanguíneo), la velocimetría de ultrasonidos Doppler (UDV) permite medir prácticamente en tiempo real el perfil completo de velocidad en casi cualquier fluido que contenga partículas en suspensión, como el polvo, las burbujas de gas, o las emulsiones. Los flujos pueden ser pulsantes, oscilantes, laminares o turbulentos, estacionarios o transitorios. Esta técnica es completamente no invasiva.

Los satélites se mueven muy rápidamente y pueden tener un desplazamiento Doppler de decenas de kilohercios respecto a una estación terrestre. La velocidad de los satélites, de la que depende la magnitud del efecto Doppler, sufre cambios debido a la curvatura de la Tierra. Para evitar este problema se ha ideado la compensación Doppler dinámica, mediante la que se modifica la frecuencia de la señal varias veces durante la transmisión, de forma que el satélite reciba una señal de frecuencia constante.

El "Leslie speaker", comúnmente asociado con los populares órganos Hammond, utiliza el efecto Doppler mediante el uso de un motor eléctrico que hace girar una bocina acústica alrededor de un altavoz, haciendo rotar 360° la orientación del sonido con cada vuelta. Esto se traduce en el oído humano en que las frecuencias fluctúan rápidamente para cada nota del teclado.

Un vibrómetro láser Doppler (LDV) es un método mediante el que se puede obtener la medición de vibraciones sin necesidad de contacto. El haz láser se dirige a la superficie a examinar desde el LDV, y la amplitud de la vibración y su frecuencia se extraen a partir del desplazamiento Doppler de la frecuencia del haz láser debido al movimiento de la superficie.

Durante la segmentación de los embriones de los vertebrados, el proceso de expresión genética produce una serie de ondas de barrido a través del mesodermo presomítico, el tejido del que se forman los precursores de los vertebrados (somitas). Un nuevo somita se forma a la llegada de una onda al final anterior del mesodermo presomítico. En el pez cebra, se ha demostrado que el acortamiento del mesodermo presomítico durante la segmentación produce un efecto Doppler que a través de las ondas orienta los movimientos del tejido del extremo anterior. Este efecto Doppler contribuye al control del período de segmentación.

Desde 1968, científicos como Victor Veselago han especulado sobre la posibilidad de un efecto Doppler inverso. El experimento que afirmó haber detectado este efecto fue llevado a cabo por Nigel Seddon y Trevor Bearpark en Bristol, Reino Unido en 2003.

Los investigadores de muchas universidades como la Swinburne University of Technology y la University of Shanghai for Science and Technology mostraron que este efecto también se puede observar en frecuencias ópticas. Esto fue posible gracias a la generación de un cristal fotónico sobre el que proyectaron un rayo láser. Esto hizo que el cristal se comportase como un superprisma, pudiendo observarse el efecto Doppler inverso.




</doc>
<doc id="17153" url="https://es.wikipedia.org/wiki?curid=17153" title="Max Schreck">
Max Schreck

Friedrich Gustav Max Schreck (Berlín, 6 de septiembre de 1879 — Múnich, 20 de febrero de 1936), fue un actor alemán célebre por su papel protagonista en la película de F.W. Murnau "Nosferatu, eine Symphonie des Grauens".

Actor de teatro, trabajó en la compañía de Max Reinhardt, muchos de cuyos actores acabarían actuando en el cine. Debutó en la pantalla grande en 1920 con la película muda "Der Richter von Zalamea", de Ludwig Berger, basada en la obra de teatro de Calderón de la Barca, "El alcalde de Zalamea". En 1922 rodará "Nosferatu, eine Symphonie des Grauens" y en 1923, "Die Strasse" de Karl Grune. Se sabe que su filmografía es más extensa, pero muchas de las películas en que participó se han perdido.

Estuvo casado con la actriz Fanny Normann. Schreck murió el 20 de febrero de 1936 después de un ataque al corazón. 

La supuesta falta de información sobre su vida se une a la leyenda negra surgida alrededor del inquietante personaje que representa en la película, el Conde Orlok, un álter ego del Conde Drácula. El mito dice que el actor realmente era un vampiro, y que el director F.W. Murnau le pagó para que en la escena final de la película mordiera el cuello de la protagonista.

Esta leyenda sirvió de base a la película "La sombra del vampiro" de E. Elias Merhige (2000), que narra el rodaje de "Nosferatu, eine Symphonie des Grauens", protagonizada por John Malkovich y Willem Dafoe. Dafoe fue nominado al Oscar por Mejor Actor de Reparto por encarnar a Schreck. 

Otro rumor sugiere que Schreck (que significa «miedo» en alemán) es en realidad un seudónimo de Alfred Abel, actor de cierto prestigio que actuó, entre otras, en las obras maestras de Fritz Lang "El doctor Mabuse", de 1922 y "Metropolis", de 1927. 

Tales cosas son inverosímiles.real mente no se sabe mucho si es cierto que se carezca de información suficiente sobre la vida de Max Schreck. La monumental biografía de Stefan Eickhoff ("Max Schreck. Gespenstertheater", Belleville, 2009) aporta una cantidad ingente de datos sobre la dilatadísima carrera teatral y cinematográfica del actor berlinés.

En "Batman Returns", de 1992, Tim Burton le hace un velado homenaje, ya que el malvado que interpreta Christopher Walken se llama Max Shreck.



</doc>
<doc id="17161" url="https://es.wikipedia.org/wiki?curid=17161" title="Gondwana">
Gondwana

Gondwana es el nombre que se le da a un antiguo bloque continental meridional que resultó de la partición en dos de Pangea, cuando se extendió el mar de Tethys hacia el oeste, lo que lo separó de Laurasia,durante el Jurásico y el Cretácico. Gondwana fue extinguiéndose y esto, dio lugar a las masas continentales de las actuales Sudamérica, África, Australia, Zealandia, el Indostán, la isla de Madagascar y la Antártida, un proceso de partición y alejamiento que continuó durante el Cenozoico y permanece activo.

Fue nombrado por primera vez en 1861 por el geólogo austríaco Eduard Suess (1831-1914) por el nombre de una región del norte de la India, Gond ("gondwana", en sánscrito, "bosque de Gond"), en la que se habían descrito secuencias sedimentarias del Pérmico-Triásico que él pensó serían de un viejo continente. Luego Suess escribió sobre él llamándolo Gondwána-Land en su libro "Las caras de la Tierra" ("Das Antlitz der Erde"), publicado entre 1883 y 1901.

En el Pérmico (hace más de 250 millones de años) todas las masas continentales estaban reunidas en un único supercontinente, al que llamamos ahora Pangea. Hace unos 200 millones de años ésta se había partido en dos supercontinentes: Laurasia, al norte y Gondwana, al sur. Los separaba entonces el océano Tethys, que se extendía desde el sur de Asia, por la actual cuenca del Mediterráneo, hasta la actual América, separada en dos por sus aguas, pues Norteamérica estaba unida a Europa y Sudamérica a África. Posteriormente el continente de Gondwana se fue subdividiendo en grandes bloques separados por fracturas de la litosfera continental. Esos fragmentos, continentes o subcontinentes, se dispersaron en un proceso que aún continúa, complementado ahora con una convergencia general de los continentes del norte (laurásicos) contra los del sur (gondwánicos). De este modo, Gran Adria habría empujado contra los continentes boreales el geosinclinal mediterráneo, plegando los alpes euroasiáticos y provocando en ellos grandes corrimientos hacia el norte. A la vez, Norteamérica y Sudamérica confluyeron dando lugar al levantamiento del istmo de Panamá.

África, que constituía el núcleo central de Gondwana, sigue fragmentándose. En un pasado relativamente reciente se separó de Arabia, que sigue alejándose hacia el noroeste al ir ensanchándose el rift del mar Rojo. Ahora, aunque muy lentamente, se está desgajando otro fragmento del continente africano, al este del llamado Gran Valle del Rift, que es una enorme fractura que parte en la desembocadura del río Zambeze y va hasta el mar Rojo, jalonada por los lagos Malaui, Tanganica, Victoria y Rodolfo. También se desplaza muy lentamente hacia el Mediterráneo y acabará incrustándose contra los países del sur de Europa.

Es importante biogeográficamente, pues explica la distribución geográfica de muchos grupos taxonómicos, que surgieron allí, y se diseminaron luego algunos a los continentes septentrionales derivados de Laurasia; o que, nacidos en Laurasia, han irrumpido luego en los continentes meridionales, como en el caso de los mamíferos placentarios que pasaron a Sudamérica y sirvió de protección a algunas especies como las placerias y koolasuchus, o la distribución actual de la flora antártica.



</doc>
<doc id="17162" url="https://es.wikipedia.org/wiki?curid=17162" title="Basalto">
Basalto

El basalto es una roca ígnea extrusiva de color oscuro, de composición máfica —rica en silicatos de magnesio y hierro y en sílice—, que constituye una de las rocas más abundantes en la corteza terrestre.

Los basaltos suelen tener una textura porfídica, con fenocristales de olivino, augita, plagioclasa y una matriz cristalina fina. En ocasiones puede presentarse en forma de vidrio, denominado sideromelano, con muy pocos cristales o sin ellos.

El basalto es la roca volcánica más común y supera en cuanto a superficie cubierta de la Tierra a cualquier otra roca ígnea, incluso juntas: forma la mayor parte de los fondos oceánicos. También hay grandes extensiones de basalto llamadas "traps" sobre los continentes. Islas oceánicas y arcos volcánicos continentales e insulares son otros lugares donde se puede hallar basalto.

Rocas similares y a menudo emparentadas con basaltos incluyen la diabasa, el gabro y la andesita.

También se encuentra en las superficies de la Luna y de Marte, así como en algunos meteoritos.

El basalto cubre cerca del 70 % de la superficie terrestre y supera en la superficie que cubre a todas las demás rocas ígneas juntas. Esta roca es particularmente abundante en los fondos oceánicos ya que forma la capa superior de la corteza oceánica (sin contar los sedimentos que la cubren en parte). En contextos científicos se denomina "MORB", una abreviación de "mid-ocean ridge basalt" en inglés, al basalto que origina en las dorsales centro-oceánicas y compone las capas superiores de la corteza oceánica. Aparte de la corteza oceánica ordinaria existen grandes extensiones predominantemente de basalto llamados "traps", que pueden cubrir miles de km², con coladas individuales con volúmenes de más de 2000 km³. Algunos de los principales "traps" se encuentran en la cuenca del Paraná, Siberia, la meseta del Decán, el Karoo y en la cuenca del río Columbia. Otras zonas donde se presenta el basalto es en arcos volcánicos continentales e insulares y en islas oceánicas.
Al salir a la superficie durante erupciones volcánicas el basalto tiene temperaturas entre 1100 y 1250 °C. En forma de lava, el basalto fluye relativamente fácil pudiendo formar volcanes en escudo los cuales están principalmente compuestos de esta roca. El fácil fluir del basalto se debe a su bajo contenido de sílice, que permite que coladas de basalto avancen más de 20 km y los gases del magma escapen sin llegar a formar columnas eruptivas.

El basalto puede presentarse de variadas formas como lava, avalanchas ardientes, en flujos de lodo, hialoclastitas, como piroclastos y ceniza. Cuando el basalto ocurre en forma de lava puede tomar la forma de lava acojinada, lava Aa, lava pahoehoe y formar tubos de lava.

Un magma basáltico que cristaliza en un dique forma el equivalente subvolcánico del basalto, la diabasa, mientras que si el mismo magma cristaliza en una cámara de magma se forma gabro, el equivalente plutónico del basalto.

El basalto también se presenta en las superficies de otros cuerpos del sistema solar, como Marte, Venus o la Luna, donde cubre aproximadamente el 17 % de la superficie. El basalto lunar tiene algunas diferencias con el terrestre, entre ellas un contenido mayor de ilmenita. Algunos meteoritos de tipo acondrita son basaltos, lo que evidencia actividad volcánica en el cuerpo celeste del cual se originaron. Existen acondritas basálticas que derivan de la Luna mientras que otro grupo de acondritas basálticas llamadas «shergottitas» provienen de la superficie de Marte.

El basalto es de color oscuro y rico en hierro y magnesio. Comparado con otras rocas ígneas el basalto tiene un bajo contenido en sílice. Aunque el basalto puede ocurrir en forma de vidrio, sin o con muy pocos cristales, a menudo contiene fenocristales de olivino, augita y plagioclasa. Los basaltos a menudo tienen una textura porfídica con los fenoscristales anteriormente mencionados y una matriz cristalina fina.

Ejemplo de una composición química de basalto expresada en porcentaje de masa de óxidos:

Existe una disputa sobre si el basalto en estado de magma es "primario" (se originaría directamente de la fusión de rocas) o si deriva de otro tipo de magma más máfico. En cualquier caso existen varias rocas que tienen los elementos necesarios para que, mediante su fusión directa o su fusión y posterior refinamiento, produzcan magma basáltico. Estas son: la peridotita, la piroxenita, la hornblendita, el basalto mismo y otras rocas procedentes de basaltos metamorfizados, como la anfibolita y la eclogita. Por una serie de razones se han descalificado a varias de estas rocas como posible fuente de magma basáltico, siendo favorecida la tesis de que las peridotitas dan origen a los basaltos, sin embargo una minoría de científicos se inclina por las eclogitas.

La causa de la fusión parcial de rocas de la cual deriva, directa o indirectamente, el magma basáltico varía dependiendo del ambiente tectónico. En las dorsales centro-oceánicas la sucesiva separación de las placas tectónicas provoca el ascenso de material (peridotita) del manto terrestre y su fusión parcial por decompresión. Los basaltos originados sobre zonas de subducción se producen al haber fusión parcial en el manto tras ser invadido por fluidos acuosos provenientes de la placa subducida. Los basaltos que ocurren en el interior de placas tectónicas y no en sus bordes (como dorsales oceánicas y zonas de subducción) se consideran por la mayoría de los científicos como expresiones de fusión parcial provocada por las altas temperaturas de plumas del manto.

El magma basáltico puede producir rocas distintas al basalto como la andesita, dacita y riolita mediante cristalización fraccionada, aunque la asimilación de rocas de la corteza también juega un rol importante en formación de estas rocas. Según algunos experimentos de laboratorio, se podría generar magma félsico directamente a partir de la fusión parcial de basalto. En el caso de las riolitas de Islandia hay dos hipótesis y ambas involucran al basalto: una que postula que las riolitas provienen de la fusión parcial del basalto, y otra que postula que la cristalización fraccionada y la asimilación cortical por parte del magma basáltico generan el magma riolítico.

El basalto puede ser protolito de una vasta gama de rocas metamórficas dependiendo de las condiciones de temperatura y presión. Algunas de las rocas metamórficas que pueden derivar del basalto (metabasaltos) son esquisto azul, esquisto verde, anfibolita y granulita. Las distintas facies metamórficas llevan el nombre de las rocas formadas a partir de un protolito de basalto.

Las eclogitas son rocas de composición basáltica que han sido expuestos a presiones extremas en el manto o en zonas de subducción.

Basaltos alterados por circulación hidrotermal cerca de dorsales meso-oceánicas forman espilitas.

En cuanto a la meteorización química, los componentes del basalto tienden a decaer en el siguiente orden: vidrio, olivino, plagioclasa, piroxeno y, al final, minerales opacos. La meteorización química del basalto consume dióxido de carbono, y el 70 % de este consumo se debe a la meteorización de aluminosilicatos con magnesio y calcio.

El sideromelano, como se le llama al vidrio basáltico, se altera en contacto con agua en un material llamado palagonita, antes de decaer finalmente en esmectita, mineral del grupo de las arcillas.

A través de la historia el basalto se ha empleado como material de construcción por diversas culturas, entre ellas los olmecas de México, el Antiguo Egipto, y el pueblo rapanui, por mencionar unas pocas. Hoy en día se utilizan fibras artificiales de basalto para reforzar estructuras de hormigón.

A pesar de ser impermeable, su uso no es aconsejable para ciertas obras hidráulicas debido a su excesiva fracturación. Otro defecto es que las superficies de basalto tienden a formar pequeñas manchas blancas en donde el mineral analcima se ha alterado, posiblemente producto de la radiación solar.

El basalto tiene un coeficiente de dilatación térmica más bajo que el granito, la caliza, la arenisca, la cuarcita, el mármol, o la pizarra, por lo que recibe poco daño en incendios. Dado el bajo albedo de los basaltos, las superficies de esta roca tienden calentarse más que otras, producto de la radiación solar, llegando a registrar temperaturas de casi 80 °C en el Sahara. El basalto masivo (sin vesículas) tiene una densidad de 2,8 a 2,9 g/cm³ siendo más denso que el granito y el mármol pero menos que el gabro. En la escala de dureza de Mohs se ha estimado que el basalto tiene una dureza que puede variar de aproximadamente de 4,8 a 6,5.

Durante las décadas alrededor del año 1800 se gestó una controversia científica en torno a al origen del basalto. Discípulos y seguidores del geólogo alemán Abraham Gottlob Werner sostenían que el basalto era una roca sedimentaria que tenía su origen en la precipitación en un gran océano ancestral. A esta teoría se le llama «neptunismo». Dos bandos se oponían a esta teoría: los seguidores de James Hutton, posteriormente conocidos como «plutonistas», que afirmaban que el basalto era una roca intrusiva, y los «vulcanistas» que consideraban al basalto como una roca volcánica. Algunos de los argumentos de los neptunistas en contra del origen volcánico del basalto era su presencia en lugares como la Calzada del Gigante y Sajonia donde no hay volcanes activos, además de presuntos hallazgos de fósiles en basalto. La confusión que causaba el hallazgo de basalto sin volcanes aparentes también se dio en América, donde Juan Ignacio Molina se percató de los basaltos de Chiloé donde en la actualidad no hay volcanes, descartando así un origen volcánico. Contra la formación de basalto en erupciones volcánicas, los neptunistas argumentaron que esto se debía más bien a la fusión de basalto neptuniano bajo los volcanes. Hacia el año 1830 el bando de los neptunistas se había desintegrado, perdiendo la mayoría de sus seguidores, quienes reconocieron el origen volcánico del basalto, en algunos casos gracias a visitas a los volcanes y basaltos de Chaîne des Puys, en Francia.




</doc>
<doc id="17163" url="https://es.wikipedia.org/wiki?curid=17163" title="Pangea">
Pangea

Pangea fue el gran supercontinente que existió al final de la era Paleozoica y comienzos de la era Mesozoica que agrupaba la mayor parte de las tierras emergidas del planeta. Se formó por el movimiento de las placas tectónicas, que hace unos 335 millones de años unió todos los continentes anteriores en uno solo; posteriormente, hace unos 175 millones de años, comenzó a fracturarse y a dispersarse hasta alcanzar la situación actual de los continentes, en un proceso que aún continúa. Este nombre, aparentemente usado por primera vez por el alemán Alfred Wegener, principal autor de la teoría de la deriva continental en 1912, procede del prefijo griego ""pan"" que significa "todo" y de la palabra en griego ""gea"" que es "suelo" o "tierra" ("Γαῖα" Gaîa, "Γαῖη" Gaîē o "Γη" Gē), y significaría ""toda la tierra"". 

Se cree que la forma original de Pangea era una masa de tierra con forma de "U" o de "C" distribuida a través del ecuador. Ya que el tamaño masivo de Pangea era muy amplio, las regiones internas de tierra debieron ser muy secas por la falta de precipitación. En el gran supercontinente los animales terrestres habrían podido emigrar libremente de un extremo a otro. 

Entre los animales que vivieron durante los 160 millones de años de existencia de Pangea se encuentran los traversodóntidos o el alokotosaurio "Shringasaurus indicus", que habitó en lo que hoy es la India. Las investigaciones también sugieren que posiblemente los primeros dinosaurios caminaron por Pangea. 

Se estima que Pangea se formó a finales del período Carbonífero (hace aproximadamente 335 millones de años) cuando los continentes, que antes estaban separados, se unieron formando un solo supercontinente rodeado por un único mar, Panthalassa.

Pangea habría comenzado a fragmentarse entre finales del Triásico y comienzos del Jurásico (hace aproximadamente 175 millones de años), producto de los cambios y movimientos de las placas tectónicas. El proceso de fragmentación de este supercontinente condujo primero a dos continentes, Gondwana al oeste y Laurasia al norte, separados por un mar circumecuatorial (mar de Tetis) y posteriormente a los continentes que conocemos hoy. Dicho proceso geológico de desplazamiento de las masas continentales se mantiene en marcha al día de hoy.

Rodinia se formó hace 1100 millones de años durante el Proterozoico, fue el supercontinente del que derivaron todos los continentes subsecuentes. No se descarta la posibilidad de la existencia de supercontinentes anteriores a Rodinia, formados y desintegrados cíclicamente durante los 4.600 millones de años de existencia de la Tierra. Rodinia se fragmentó hace unos 750 millones de años y después los fragmentos volvieron a reunirse en el supercontinente Pannotia hace 600 millones de años. Pero una vez más, el supercontinente único se vuelve a fragmentar. Hace 540 millones de años, solo después de 60 millones de años de su formación, Pannotia se divide en dos fragmentos: Gondwana al sur y Proto-Laurasia, más pequeño, al norte.

El supercontinente menor, Proto-Laurasia se desplazó lejos de Gondwana a través del océano Pantalásico. Un océano nuevo se formó entre los dos continentes, el océano Proto-Tetis. Inmediatamente, Proto-Laurasia se partió en varios segmentos para crear Laurentia, Siberia y Báltica. Esta separación también propició la generación de dos océanos nuevos, el Iapetus y Khanty. Báltica permaneció al este de Laurentia y Siberia se asentó al noreste de Laurentia.

Durante el Cámbrico, el continente independiente de Laurentia (que posteriormente se convirtió en Norteamérica) estuvo fijo en el Ecuador, rodeado por tres océanos, el océano Pantalásico al norte y al oeste, el océano Iapetus al sur, y el océano Khanty al este. Al inicio del Ordovícico, el microcontinente de Avalonia (una masa de tierra que se convertiría en los Estados Unidos, Nueva Escocia e Inglaterra) se separó de Gondwana y comenzó su viaje hacia Laurentia.

Hacia el final del Ordovícico, Báltica chocó con Laurentia, y el norte de Avalonia chocó con Báltica y Laurentia. Entonces, Laurentia, Báltica y Avalonia se unieron para conformar al supercontinente menor de Euramérica o Laurusia, cerrando el océano Iapetus, mientras que el océano Rheico se expandió hacia la costa meridional de Avalonia. La colisión también dio lugar a la formación de los Apalaches norteños. Siberia se asentó cerca de Euramérica con el océano Khanty entre los dos continentes. Mientras todo esto estaba sucediendo, Gondwana se desplazó lentamente hacia el polo sur. Este fue el primer paso de la formación de Pangea.

El segundo paso en la formación de Pangea fue la colisión de Gondwana con Euramérica y su unión a ella. Durante el Silúrico, Báltica ya había chocado con Laurentia para formar Euramérica. Avalonia no había chocado aún con Laurentia, y una vía marítima entre ellos (que era un remanente del océano Iapetus) todavía se contraía al mismo tiempo que Avalonia avanzaba lentamente hacia Laurentia. Mientras tanto, Europa meridional se separó de Gondwana y comenzó a dirigirse hacia Euramérica a través del recientemente formado océano Rheico y colisionó con Báltica meridional durante el Devónico. Sin embargo, este microcontinente tan solo era una placa oceánica. El océano Khanty (el océano hermano de Iapetus) también se contrajo al mismo tiempo que un arco insular desgajado de Siberia chocaba con Báltica del este (ahora parte de Euramérica). Detrás de este arco insular se estaba formando un océano nuevo, el océano Ural.

Al final del Silúrico, los microcontinentes de China del Norte y China del Sur se desgajaron de Gondwana y comenzaron a dirigirse hacia el norte a través del océano Proto-Tetis, abriendo desde el sur el océano Paleo-Tetis. En el período Devónico, Gondwana se desplazó hacia Euramérica, lo que causó que el océano Rheico se contrajera.

Al inicio del Carbonífero, el noroeste de África había tocado la costa sudeste de Euramérica, creando la porción meridional de las montañas Apalaches y las Montañas Atlas. Sudamérica se movió hacia el norte con dirección a Euramérica meridional, mientras que la porción del este de Gondwana (India, Antártida y Australia) se dirigió hacia el polo sur desde el ecuador.

China del Norte y China del Sur se encontraban en continentes independientes. Hacia la mitad del Carbonífero, el microcontinente de Kazakhstania había chocado con Siberia (el continente siberiano había sido un continente separado durante millones de años desde la fragmentación del supercontinente Pannotia). Al final del Carbonífero, el oeste de Kazakhstania chocó con Báltica, cerrando los océanos Ural y Proto-Tetis entre ellos (orogenia Uraliana), lo que causó la formación de las montañas de los Urales y la formación del supercontinente de Laurasia. 

Mientras tanto, Sudamérica había chocado con el sur de Laurentia, cerrando el océano Rheico y formando la parte sur de los Apalaches y las montañas de Ouachita. Para este tiempo, Gondwana se posicionó cerca del polo sur, y se formaron glaciares en la Antártida, la India, Australia, África meridional y Sudamérica. El bloque del norte de China chocó con Siberia al final del Carbonífero, cerrando por completo el océano Proto-Tetis.

Para el inicio del Pérmico temprano, la placa Cimmeriana se desgajó de Gondwana y se dirigió hacia Laurasia, formando un océano nuevo en su extremo meridional, el océano Tetis, y cerrando el océano Paleo-Tetis. La mayoría de las masas de tierra estaban reunidas en una sola entidad. Para el período Triásico, Pangea rotó ligeramente en dirección al sudoeste. La placa Cimmeriana todavía viajaba a través del cada vez más pequeño océano Paleo-Tetis, hasta la mitad del Jurásico. Paleo-Tetis se cerró de oeste a este, creando la orogenia Cimmeriana. Pangea parecía una "C", con un océano dentro de la "C", el nuevo océano Tetis. No obstante, Pangea se desunió durante el Jurásico Medio, y esta fragmentación se explica en el siguiente apartado.

Hubo tres fases importantes en la desintegración de Pangea. La primera fase comenzó al principio-mitad del Jurásico, cuando en Pangea se creó una grieta que abarcaba desde el océano Tetis al este hasta el Pacífico al oeste. Esta grieta separó Norteamérica de África y produjo múltiples fallas, siendo el río Misisipi la más grande de ellas. La grieta produjo un nuevo océano, el océano Atlántico. Este océano no se abrió uniformemente, sino que el desplazamiento comenzó en el Atlántico Norte-Central; el Atlántico sur no se abriría hasta el Cretáceo. Laurasia comenzó a rotar hacia la derecha y se movió hacia el norte, con Norteamérica al norte y Eurasia al sur. El movimiento de Laurasia siguiendo las manecillas del reloj también condujo al cierre del océano Tetis. Mientras tanto, en el otro lado, en África se formaron nuevas grietas a lo largo de los márgenes adyacentes de África, de Antártida y del este de Madagascar, lo que conduciría a la formación del océano Índico, que también se abriría durante el Cretáceo.

La segunda fase importante de la desintegración de Pangea comenzó al inicio del Cretáceo (hace 150-140 millones de años), cuando el supercontinente Gondwana se dividió en cuatro continentes más pequeños (África, Sudamérica, India y Antártida/Australia). Hace cerca de 200 millones de años, el continente de Cimmeria, según lo mencionado arriba ("la formación de Pangea"), chocó con Eurasia. Sin embargo, a la vez que se producía esta colisión, se formó la nueva zona de subducción que se denomina fosa de Tetis. Esta fosa produjo la subducción de la dorsal oceánica de Tetis, responsable de la expansión del océano Tetis. Esta subducción probablemente causó que África, la India y Australia se movieran hacia el norte. Al inicio del Cretáceo, Atlántica, la Sudamérica de hoy, y África finalmente se separaron de Gondwana (es decir, se separaron de la Antártida, India y Australia), causando la apertura de un "océano Índico del sur". En el Cretáceo medio, Gondwana se fragmentó para abrir el Océano Atlántico del sur mientras Sudamérica comenzó a moverse hacia el oeste alejándose de África. El Atlántico del sur no se desarrolló uniformemente, se separó de sur al norte como una cremallera. También al mismo tiempo, Madagascar y la India comenzaron a separarse de la Antártida y se movieron hacia el norte, abriendo el océano Índico. Madagascar y la India se separaron hace aproximadamente de 100 a 90 millones de años durante el Cretáceo tardío. La India continuó moviéndose hacia el norte con dirección a Eurasia a una velocidad de 15 centímetros por año (un récord de movimiento tectónico), cerrando el océano Tetis, mientras que Madagascar se detuvo y encalló con la placa Africana. Nueva Zelanda y Nueva Caledonia comenzaron a moverse desde Australia hacia el este en dirección del Pacífico, abriendo el Mar del Coral y el Mar de Tasmania. Desde entonces, han sido islas independientes.

La tercera fase principal (y final) de la desintegración de Pangea ocurrió al inicio del Cenozoico (Paleoceno - Oligoceno). Norteamérica/Groenlandia finalmente se separó de Eurasia, abriendo el mar Noruego hace cerca de 60-55 millones de años. Los océanos Índico y Atlántico continuaron expandiéndose, cerrando el océano Tetis. Mientras tanto, Australia se separó de la Antártida y se movió rápidamente hacia el norte, así como lo había hecho la India hace más de 40 millones de años antes; en la actualidad se encuentra en curso de colisión con el este de Asia. Australia y la India se están moviendo actualmente en dirección noreste a una velocidad de 5-6 centímetros por año. La Antártida ha estado en (o muy cerca de) el polo sur desde la formación de Pangea (desde hace 280 millones de años). La India comenzó a chocar con Asia hace cerca de 35 millones de años, formando la orogenia Himalaya y cerrando finalmente con esto la vía marítima de Tetis; esta colisión aún continúa hoy. La placa africana empezó a cambiar su dirección, del oeste al noroeste hacia Europa, mientras que Sudamérica comenzó a moverse en dirección al norte, se separó de la Antártida y permitió por primera vez la completa circulación oceánica alrededor de Antártida, causando un rápido enfriamiento del continente y la formación de los glaciares. Otros acontecimientos importantes ocurrieron durante el Cenozoico, tales como la apertura del golfo de California, el levantamiento de los Alpes y la apertura del Mar del Japón. La desintegración de Pangea continúa hoy día en la grieta al este de África; además, las colisiones en curso pueden indicar la creación incipiente de un nuevo supercontinente.




</doc>
<doc id="17167" url="https://es.wikipedia.org/wiki?curid=17167" title="Meteorización">
Meteorización

Se llama meteorización a la descomposición de minerales y rocas que ocurre sobre o cerca de la superficie terrestre cuando estos materiales entran en contacto con la atmósfera, hidrosfera y la biosfera. Sin embargo existen varias definiciones más, lo que ha hecho que el término signifique diferentes cosas para distintos autores. Ejemplo de otras definiciones son:

Existen principalmente dos tipos de meteorización: la meteorización química y la meteorización física. A veces se incluye la meteorización biológica como un tercer tipo. La meteorización se considera como un proceso exógeno y es importante entre otras cosas para el estudio de las formas del relieve y también para entender los suelos y sus nutrientes.

Se pueden considerar los 100 °C y 1 kbar como la temperatura y presión máxima bajo las cuales la meteorización ocurre.

La meteorización física produce desintegración o ruptura en la roca, sin afectar a su composición química o mineralógica. En estos procesos la roca se va fracturando, es decir, se va disgregando en materiales de menor tamaño y ello facilita el proceso de erosión y transporte posterior. Las rocas no cambian sus características químicas pero sí las físicas. Está causada por las condiciones ambientales (agua, calor, sal, etc.). Los agentes que la provocan son:

Produce una transformación química de la roca provocando la pérdida de cohesión y alteración de la roca. Los procesos más importantes son los atmosféricos, el vapor de agua, el oxígeno y el dióxido de carbono que están implicados en:

La meteorización biológica u orgánica consiste en la ruptura de las rocas por la actividad de animales y plantas. La construcción de madrigueras y la acción de las raíces de los árboles pueden provocar una acción mecánica, mientras que los efectos de la presencia de agua y diversos ácidos orgánicos, así como el aumento del dióxido de carbono, pueden complementar la meteorización alterando la roca. Así pues, los efectos de la meteorización biológica combinan los procesos de disgregación y los de alteración.

La vegetación desempeña un papel decisivo en los procesos de meteorización química, ya que aportan iones y ácidos de disolución al agua. La descomposición orgánica genera humus más o menos ácido que provoca fenómenos de podsolización.

La meteorización desintegra las rocas existentes y aporta materiales para formar otras nuevas. Sin embargo, la meteorización desempeña también un papel importantísimo en la creación de los suelos que cubren la superficie de la Tierra y sustentan toda vida. Un suelo refleja, hasta cierto grado, el material rocoso del cual se derivó, pero la roca basal no es el único factor que determina el tipo de suelo, ya que diferentes suelos se desarrollan sobre rocas idénticas en áreas distintas cuando el clima varía de un área a otra. Por lo tanto, otros factores ejercen influencias importantes sobre el desarrollo del suelo, como el relieve, el tiempo y el tipo de vegetación. La composición de un suelo varía con la profundidad. El afloramiento natural o artificial de un suelo revela una serie de zonas diferentes entre sí. Cada una de estas zonas constituye un horizonte, que representan, desde la superficie hacia adentro, las capas más meteorizadas o descompuestas y con diferentes acumulaciones de minerales por lixiviación o lavado del suelo, hasta llegar a la roca madre o fresca, de la cual se derivó el suelo.Estos horizontes de suelo se han desarrollado a partir del material original subyacente. Cuando este material queda expuesto por vez primera en la superficie, la parte superior queda sujeta a la meteorización intensa y la descomposición actúa rápidamente. Conforme avanza la descomposición del material, el agua que percola hacia abajo comienza a lixiviar algunos de los minerales y los deposita en niveles inferiores, los cuales con el paso del tiempo, se vuelven más gruesos y alcanzan mayores profundidades.




</doc>
<doc id="17168" url="https://es.wikipedia.org/wiki?curid=17168" title="Gneis">
Gneis

Se denomina gneis a una roca metamórfica compuesta por los mismos minerales que el granito (cuarzo, feldespato y mica) pero con orientación definida en bandas, con capas alternas de minerales claros y oscuros. A veces presenta concreciones feldespáticas distribuidas con regularidad, denominándose en este caso gneis ocelado.

Los gneis reciben diferentes denominaciones en función de los componentes (gneis biotítico, moscovítico), el origen (ortognéis si es producto del metamorfismo de rocas ígneas y paragnéis, si lo es de rocas sedimentarias), o la textura (por ej. gneis ocelados).

La etimología de la palabra «Gneiss» no está clara y permanece disputada. Según algunas fuentes procede del verbo del medio alto alemán "gneist" (chispear; esto se ha supuesto porque el gneis produce brillos al ser percutido), en inglés se usa desde 1757.

El gneis se utiliza en construcción de peldaños, adoquines, mampostería, entre otros. También es usado en elementos decorativos.


</doc>
<doc id="17169" url="https://es.wikipedia.org/wiki?curid=17169" title="Granito">
Granito

El granito es una roca ígnea intrusiva de color claro, de composición félsica formada esencialmente por cuarzo, feldespato alcalino, plagioclasa y mica. 

El término "granito" abarca varias rocas de aspecto granular y de colores claros, pero con proporciones diferentes entre sus minerales. Para referirse a todas ellas los geólogos han definido el término granitoide. Según los estándares de la Unión Internacional de Ciencias Geológicas, son granitoides las rocas plutónicas cuyo contenido en cuarzo está comprendido entre el 20 y el 60 %. Esto incluye rocas como las tonalitas y las sienitas con cuarzo. 

El granito "sensu stricto" se refiere a las rocas que, dentro del grupo anterior, tienen una relación entre ambas clases de feldespatos — alcalinos y plagioclasas— desde el 50% o más favorable hacia los feldespatos alcalinos. En función de esta proporción, los granitos se denominan:
Los granitoides son las rocas más abundantes de la corteza continental superior. Forman el 4,5 % de la corteza terrestre y el 15 % de los continentes.

Los granitoides se forman al solidificarse magma con alto contenido en sílice —lo que se conoce como "magma saturado—" a gran profundidad bajo la corteza terrestre, en condiciones de alta presión y enfriamiento lento. Si un magma de composición granítica alcanza la superficie se forma una roca volcánica denominada riolita.

Los granitos se forman a partir de magmas solidificados, y estos a su vez pueden tener orígenes diferentes: hay magmas que provienen de la fusión parcial o anatexia de rocas de la corteza, mientras otros tienen su origen en el manto subyacente. Según el origen del magma los granitoides se clasifican en cuatro tipos, nombrados con las letras "I", "S", "A" y "M, iniciales de Igneo, Sedimentario, Anorogénico y Manto" El tipo «I» deriva de magmas originados en la zona de contacto entre la corteza inferior y el manto. El tipo «S» proviene de magma que se forma por la fusión parcial de rocas sedimentarias o de rocas de la corteza superior. Al contrario de los tipos "I" y "S" que son comunes en las zonas de orogénesis, el tipo "A", anorogénico y alcalino, ocurre en contextos que no están asociados a la formación de cordilleras. El tipo "M" se distingue de los demás por tener una proveniencia directa de magmas del manto.

Los granitoides originados de magma proveniente de la corteza inferior han sido relacionados por científicos con migmatitas de forma que se han interpretado estas últimas rocas de tres maneras: el producto de anatexia que origina a magma granítico, el producto de la inyección de magma granítico a rocas metamórficas, el producto de un proceso de transformación de roca metamórfica en granito en el sitio.

Los granitos se forman a partir de masas formidables de magma, que ascienden por la corteza terrestre porque tienen menor densidad que el material que los rodea. Antes de solidificarse el magma llenaba una cámara magmática, desde la cual también puede alcanzar la superficie por fenómenos volcánicos, aunque esto ocurre rara vez con los magmas ácidos, graníticos. A la «flotabilidad» del magma se contraponen los efectos de la viscosidad, (que es característicamente alta en los magmas de composición granítica), y al hecho de que para que un cuerpo de magma ocupe un lugar debe haber desplazamiento de otro material en cantidad proporcional. Los granitos se emplazan preferentemente siguiendo fracturas y fallas preexistentes en la corteza terrestre. El ascenso del magma puede ser forzoso o pasivo; en el primer caso el magma se abre paso por su propia fuerza, comprimido por los materiales que le rodean, abriendo fracturas y desplazando material; y en el segundo, las tensiones en la corteza crean espacios que son rellenados por magma. Cuando el magma se encuentra en equilibrio gravitacional (como un témpano de hielo flotando en el mar) se estanca.

La meteorización del granito afecta a la roca de forma gradual, acabando por disgregarla. En climas fríos actúa la meteorización física, que debido a la compacidad e impermeabilidad del granito apenas ataca la capa superficial. La meteorización química, que actúa oxidando e hidrolizando los feldespatos es mucho más eficaz, y puede disgregar espesores considerables de roca. Una forma característica de meteorización del granito y otras rocas ígneas forma en la superficie capas concéntricas, separadas del núcleo no afectado por diaclasas de exfoliación netas; por ello, a esta meteorización se la llama «meteorización en capas de cebolla». En los inicios de meteorización de granito la biotita pierde potasio para transformarse en hidro-biotita y finalmente en vermiculita. En estados más avanzados de meteorización la plagioclasa comienza a disolverse y el anfíbol a hidratarse. La plagioclasa se altera transformándose en caolín. El feldespato potásico y cuarzo son los minerales que más se resisten a la meteorización. Si la plagioclasa y la biotita se han transformado en caolín, smectita y goethita y el cuarzo y el feldespato potásico mantienen la estructura de la roca, esta puede considerarse un saprolito.

El granito se utiliza ampliamente en construcción desde la prehistoria gracias a la tenacidad del material y su resistencia a la erosión, comparado con otros tipos de roca (especialmente la caliza que es frágil y soluble). Tradicionalmente era llamado piedra berroqueña y el trabajo con ella era considerado el más penoso de todos. Actualmente ya no se utiliza como elemento estructural pero sí con fines decorativos que aprovechan sus dibujos característicos. Para ello suele usarse cortado en placas de algunos centímetros de espesor, las cuales se pulen y se utilizan como revestimiento. Hay que hacer notar que el pulido fino del granito era extremadamente difícil en la antigüedad, por lo que los edificios de granito no-modernos suelen tener una factura aparentemente tosca, incluso cuando los sillares están bien tallados, como en el Monasterio de El Escorial.

Los egipcios esculpían en la roca de granito desde el período predinástico para elaborar recipientes. Se han encontrado muchas vasijas de las primeras dinastías en Saqqara.

La Cámara del Rey de la Gran Pirámide de Guiza está construida con grandes bloques de granito, también se encuentra en varias hiladas del revestimiento de las otras dos pirámides de Guiza.

Los obeliscos egipcios fueron grandes monolitos de granito tallados y transportados por el Nilo desde las canteras del actual Asuán. También se utilizó para elaborar estatuas.

Otros usos en el Antiguo Egipto incluyen, columnas, puertas, dinteles, etc.

Aún es motivo de debate saber cómo los egipcios trabajaron el granito. El arqueólogo Patrick Hunt postula que usaban abrasivos, mostrando su poder de dureza en la escala de Mohs.

También fue usado en la construcción de la terraza de Baalbeck.

El granito ha sido usado ampliamente como recubrimiento en edificios públicos y monumentos. Al incrementarse la lluvia ácida en los países desarrollados, el granito está reemplazando al mármol como material de monumentos, ya que es mucho más duradero. El granito pulido es muy popular en cocinas debido a su alta durabilidad y cualidades estéticas. El granito Black Galaxy de Cheemakurthy, Andhra Pradesh en India es mundialmente conocido por su elegancia. El color de granito más abundante por naturaleza es el gris.

Los ingenieros han usado tradicionalmente el granito pulido para dar un plano de referencia, dado que es relativamente duro e inflexible.
Otros usos del granito pueden ser:



</doc>
<doc id="17170" url="https://es.wikipedia.org/wiki?curid=17170" title="Datación radiométrica">
Datación radiométrica

La datación radiométrica, datación radioactiva o datación por radioisótopos es una técnica utilizada para datar materiales como rocas, minerales y restos orgánicos (carbono), en los que se incorporaron de manera selectiva impurezas radiactivas cuando se formaron. El método se basa en la comparación de la abundancia de un radionucleido de ocurrencia natural dentro del material con la abundancia de sus productos de descomposición, que se forman a unatasa constante de desintegración conocida. El uso de la datación radiométrica fue publicado por primera vez en 1907 por el radioquímico estadounidense Bertram Boltwood (1870-1927) —a partir de una idea ya avanzado por el británico Ernest Rutherford en 1905— y ahora es la principal fuente de información sobre la edad absoluta de las rocas y otras características geológicas, incluida la edad de las formas de vida fosilizadas o la propia edad de la Tierra, y ​​también se puede utilizar para datar una amplia gama de materiales naturales y artefactos antiguos. Al permitir el establecimiento de escalas de tiempo geológicas, proporciona una importante fuente de información sobre las edades de los fósiles y las tasas deducidas de cambio evolutivo. 

Junto con los principios estratigráficos, los métodos de datación radiométrica se utilizan en geocronología para establecer la escala temporal geológica. Entre las técnicas más conocidas están la datación potasio-argón, la datación uranio-plomo y la datación por radiocarbono (basada en la desintegración del isótopo carbono 14), comúnmente utilizada para la datación de restos orgánicos relativamente recientes, de hasta . (Otros dataciones posibles son K/Ar, U/Pb, Rb/Sr, Sm/Nd, etc.)

Los diferentes métodos de datación radiométrica varían según sea la escala de tiempo en la que son precisos y de los materiales a los que se pueden aplicar.

Toda la materia ordinaria se compone de combinaciones de elementos químicos, cada uno con su propio número atómico, que indica el número de protones en el núcleo atómico. Además, los elementos pueden existir en diferentes isótopos, con cada isótopo de un elemento difiriendo en el número de neutrones en el núcleo. Un isótopo particular de un elemento particular se llama nucleido. Algunos nucleidos son inherentemente inestables. Es decir, en algún momento en el tiempo, un átomo de tal nucleido sufrirá un decaimiento radioactivo y se transformará espontáneamente en un nucleido diferente. Esta transformación se puede lograr de varias maneras diferentes, incluida la desintegración alfa (emisión de partículas alfa) y la desintegración beta (emisión de electrones, emisión de positrones o captura de electrones). Otra posibilidad es la fisión espontánea en dos o más nucleidos.

Si bien el momento en el que un núcleo particular se desintegra es impredecible, una colección de átomos de un nucleido radiactivo decae exponencialmente a una tasa descrita por un parámetro conocido como vida media, generalmente dada en unidades de años cuando se estudian las técnicas de datación. Después de que haya transcurrido una vida media, la mitad de los átomos del nucleido en cuestión se habrán desintegrado en un nucleido «hijo» o producto de desintegración. En muchos casos, el nucleido «hijo» en sí mismo es radioactivo, lo que resulta en una nueva cadena de desintegración, que finalmente termina con la formación de un nucleido «hijo» estable (no radioactivo); cada paso en tal cadena se caracteriza por una vida media distinta. En estos casos, generalmente la vida media de interés en la datación radiométrica es la más larga de la cadena, que es el factor limitante de la velocidad en la transformación final del nucleido radioactivo en su «hijo» estable. Los sistemas isotópicos que han sido explotados para la datación radiométrica tienen vidas medias que van desde solo unos 10 años (por ejemplo, tritio) hasta los más de 100 mil millones de años (por ejemplo, el ).

Para la mayoría de los nucleidos radiactivos, la vida media depende únicamente de las propiedades nucleares y es esencialmente una constante. No se ve afectado por factores externos como la temperatura, la presión, el entorno químico o la presencia de un campo magnético o eléctrico. Las únicas excepciones son los nucleidos que se descomponen por el proceso de captura de electrones, como el , el y el , cuya tasa de decaimiento puede verse afectada por la densidad electrónica local. Para todos los demás nucleidos, la proporción del nucleido original en relación a sus productos de desintegración cambia de manera predecible a medida que el nucleido original decae con el tiempo. Esta previsibilidad permite que las abundancias relativas de los nucleidos relacionados se utilicen como un reloj para medir el tiempo desde la incorporación de los nucleidos originales en un material hasta el presente.

La ecuación básica de la datación radiométrica requiere que ni el nucleido «padre» ni el producto «hijo» puedan entrar o salir del material después de su formación. Los posibles efectos de confusión de la contaminación de los isótopos «padre» e «hijo» deben considerarse, al igual que los efectos de cualquier pérdida o ganancia de dichos isótopos desde que se creó la muestra. Por lo tanto, es esencial contar con la mayor cantidad de información posible sobre el material que se está datando y verificar posibles signos de alteración. La precisión se mejora si las mediciones se toman en múltiples muestras de diferentes ubicaciones del cuerpo de roca. Alternativamente, si se pueden datar varios minerales diferentes de la misma muestra y se supone que fueron formados por el mismo evento y que estaban en equilibrio con el yacimiento cuando se formaron, deberían formar una isócrona. Esto puede reducir el problema de la contaminación. En la datación uranio-plomo, se utiliza el diagrama de concordia que también disminuye el problema de la pérdida de nucleidos. Finalmente, la correlación entre los diferentes métodos de datación isotópica puede ser necesaria para confirmar la edad de una muestra. Por ejemplo, se determinó que la edad de los gneises de Amitsoq, en el oeste de Groenlandia, era de utilizando la datación plomo-uranio y de utilizando la datación de plomo-plomo, resultados que coinciden entre sí.

Una datación radiométrica precisa generalmente requiere que el «padre» tenga una vida media lo suficientemente larga como para que esté presente en cantidades significativas en el momento de la medición (excepto, como se describe a continuación, en la «Datación con radionucleidos extintos de vida corta»), que la vida media del «padre» se conozca con precisión y que se produzca suficiente cantidad del producto «hijo» para medirlo y distinguirlo de la cantidad inicial del «hijo» presente en el material. Los procedimientos utilizados para aislar y analizar los nucleidos «padre» e «hijo» deben ser precisos y seguros. Esto normalmente implica el uso de espectrometría de masas de relación isotópica.

La precisión de un método de datación depende en parte de la vida media del isótopo radioactivo involucrado. Por ejemplo, el carbono 14 tiene una vida media de . Después de que un organismo haya estado muerto desde hace , queda tan poco carbono 14 que no se puede establecer una datación precisa. Por otro lado, la concentración de carbono-14 cae tan abruptamente que la edad de los restos relativamente jóvenes se puede determinar con una precisión de unas pocas décadas.

Si un material que rechaza selectivamente el nucleido hijo se calienta, cualquier nucleido hijo que se haya acumulado a lo largo del tiempo se perderá por difusión, lo que pondrá a cero el "reloj" isotópico. La temperatura a la que sucede esto se conoce como temperatura de cierre o temperatura de bloqueo y es específica de cada material en particular y sistema isotópico. Estas temperaturas se determinan experimentalmente en el laboratorio mediante el restablecimiento artificial de los minerales de muestra utilizando un horno de alta temperatura. A medida que el mineral se enfría, comienza a formarse la estructura cristalina y la difusión de los isótopos es menos fácil. A cierta temperatura, la estructura cristalina se ha formado lo suficiente como para evitar la difusión de isótopos. Esa temperatura es lo que se conoce como temperatura de cierre y representa la temperatura por debajo de la cual el mineral es un sistema cerrado para los isótopos. Por lo tanto, una roca o masa fundida ígnea o metamórfica, que se está enfriando lentamente, no comienza a mostrar una disminución radiactiva mensurable hasta que se enfríe por debajo de la temperatura de cierre. La edad que se puede calcular por datación radiométrica es, por lo tanto, el momento en que la roca o el mineral se enfriaron hasta la temperatura de cierre. La datación de diferentes minerales y/o sistemas de isótopos (con diferentes temperaturas de cierre) dentro de la misma roca puede, por lo tanto, permitir el seguimiento de la historia térmica de la roca en cuestión en el tiempo, y por lo tanto la historia de los eventos metamórficos puede ser conocida en detalle. Este campo se conoce como termocronología o termocronometría.

La expresión matemática que relaciona la desintegración radioactiva con el tiempo geológico es: 
o
siendo:

La ecuación se expresa mejor en términos de la cantidad medida formula_4 en lugar del valor inicial constante formula_10.

La ecuación anterior hace uso de la información sobre la composición de los isótopos «padre» e «hijo» en el momento en que el material que se está probando se enfrió por debajo de su temperatura de cierre. Esto está bien establecido para la mayoría de los sistemas isotópicos. Sin embargo, la construcción de una isócrona no requiere información sobre las composiciones originales, utilizando simplemente las relaciones actuales de los isótopos «padre» e «hijo» de un isótopo estándar. El trazado de una isócrona se utiliza para resolver gráficamente la ecuación de edad y calcular la edad de la muestra y la composición original.

Esta ecuación es válida siempre que el modo de decaimiento del isótopo «padre» sea único y que el isótopo «hijo» sea estable. Para otros casos se pueden obtener ecuaciones más complejas, en las que se tienen en cuenta múltiples decaimientos posibles.

La datación radiométrica se conoce desde 1905, cuando fue ideada por Ernest Rutherford como un método por el cual se podría determinar la edad de la Tierra. En el siglo transcurrido desde entonces, las técnicas se han mejorado y ampliado enormemente. La datación se puede realizar ahora en muestras muy pequeñas, del orden de un nanogramo, usando un espectrómetro de masas. El espectrómetro de masas se inventó en la década de 1940 y comenzó a usarse en la datación radiométrica en la década de 1950. Opera generando un haz de átomos ionizados a partir de la muestra a prueba. Luego, los iones viajan a través de un campo magnético, que los desvía hacia diferentes sensores de muestreo, conocidos como «copas de Faraday», según sean su masa y nivel de ionización. Al impactar en las copas, los iones establecen una corriente muy débil que puede medirse para determinar la tasa de impactos y las concentraciones relativas de los diferentes átomos en los haces.

Este método implica la inspección de un corte pulido de un material para determinar la densidad de las marcas de "traza" ("track") dejadas en él por la fisión espontánea de impurezas de uranio 238. El contenido de uranio de la muestra debe ser conocido, pero eso puede determinarse colocando una película de plástico sobre la rebanada pulida del material y bombardeando con neutrones lentos. Esto provoca una fisión inducida de U, en oposición a la fisión espontánea de U. Las trazas de fisión producidas por este proceso se registran en la película de plástico. El contenido de uranio del material se puede calcular a partir del número de trazas y del flujo de neutrones.

Este esquema tiene aplicación en una amplia gama de fechas geológicas. Para fechas de hasta unos pocos millones de años, se utilizan mejor las micas, las tectitas (fragmentos de vidrio de erupciones volcánicas) y los meteoritos. Los materiales más antiguos se pueden fechar utilizando circonio, apatita, titanita, epidota y granate que tienen una cantidad variable de contenido de uranio. Debido a que las huellas de fisión se curan con temperaturas de más de 200 °C, la técnica tiene tanto limitaciones como beneficios. La técnica tiene aplicaciones potenciales para detallar la historia térmica de un yacimiento.

Entre 1952 y 1958, se produjeron grandes cantidades del, por lo demás raro, (semivida ~ 300 ka) durante la detonación atmosférica de las armas nucleares. El tiempo de residencia del Cl en la atmósfera es de aproximadamente 1 semana. Por lo tanto, como un marcador de eventos de los años 1950 de agua en el suelo y de agua subterránea, el Cl también es útil para la datación de aguas de menos de 50 años antes del presente. El Cl ha sido usado en otras áreas de las ciencias geológicas, como la datación de hielos y sedimentos.

Los métodos de datación por luminiscencia no son métodos de datación radiométrica porque no dependen de la abundancia de isótopos para calcular la edad. En cambio, son una consecuencia de la radiación de fondo en ciertos minerales. Con el tiempo, la radiación ionizante es absorbida por los granos minerales en sedimentos y materiales arqueológicos como el cuarzo y el feldespato de potasio. La radiación hace que la carga permanezca dentro de los granos en «trampas de electrones» estructuralmente inestables. La exposición a la luz solar o al calor libera esas cargas, lo que «blanquea» la muestra y restablece el reloj a cero. La carga atrapada se acumula con el tiempo hasta una tasa determinada por la cantidad de radiación de fondo en el lugar donde se enterró la muestra. La estimulación de estos granos minerales mediante la luz (luminiscencia estimulada ópticamente o datación de luminiscencia estimulada con infrarrojos) o el calor (datación por termoluminiscencia) hace que se emita una señal de luminiscencia a medida que se libera la energía electrónica inestable almacenada, cuya intensidad varía dependiendo de la cantidad de radiación absorbida durante el entierro y de las propiedades específicas del mineral.

Estos métodos se pueden usar para fechar la edad de una capa de sedimento, ya que las capas depositadas en la parte superior evitarían que los granos se "blanqueasen" y se restableciesen con la luz solar. Los fragmentos de cerámica se pueden fechar hasta la última vez que experimentaron un calor significativo, generalmente cuando fueron horneados en un kiln.

Otros métodos son los siguientes:

La datación radiométrica absoluta requiere que una fracción mensurable del núcleo «padre» permanezca en la roca de muestra. Para las rocas que se remontan al principio del sistema solar, ello requiere isótopos «padre» de vida extremadamente larga, lo que hace que la medición de las edades exactas de tales rocas sea imprecisa. Para poder distinguir las edades relativas de las rocas de ese material antiguo y obtener una mejor resolución temporal que la disponible en los isótopos de larga vida, se pueden usar isótopos de corta duración que ya no estén presentes en la roca, conocidos como radionucleidos extintos.

Al comienzo del sistema solar, había varios radionucleidos de vida relativamente corta como Al, Fe, Mn y I presentes en la nebulosa solar. Esos radionucleidos, posiblemente producidos por la explosión de una supernova, se han extinguido hoy en día, pero sus productos de desintegración pueden detectarse en material muy antiguo, como el que constituye los meteoritos. Al medir los productos de descomposición de los radionucleidos extintos con un espectrómetro de masas y utilizar isocronplots, es posible determinar las edades relativas de diferentes eventos en la historia temprana del sistema solar. Los métodos de datación basados ​​en radionucleidos extintos también se pueden calibrar con el método U-Pb para obtener edades absolutas. Por lo tanto, se puede obtener tanto la edad aproximada como una resolución de tiempo alta. En general, una vida media más corta conduce a una resolución de tiempo mayor a expensas de la escala de tiempo.

El I decae a Xe con una vida media de . El cronómetro de yodo-xenón es una técnica isócrona. Las muestras se exponen a una radiación de neutrones en un reactor nuclear. Esto convierte al único isótopo estable de yodo (I) en Xe a través de la captura de neutrones seguido de una desintegración beta (del I). Después de la irradiación, las muestras se calientan en una serie de pasos y se analiza la firma isotópica del xenón en el gas evolucionado en cada paso. Cuando se observa una relación constante de Xe / Xe en varios pasos de temperatura consecutivos, se puede interpretar como correspondiente a un momento en el que la muestra dejó de perder el xenón.

Las muestras de un meteorito llamado Shallowater generalmente se incluyen en la irradiación para monitorear la eficiencia de conversión de I a Xe. La diferencia entre las relaciones medidas de Xe / Xe de la muestra y del Shallowater corresponde a las diferentes relaciones de I/I cuando cada una de ellas dejó de perder el xenón. Esto, a su vez, corresponde a una diferencia en la edad de cierre en el sistema solar temprano.

Otro ejemplo de la datación de radionucleidos extintos de corta duración es el cronómetro de Al – Mg, que se puede usar para estimar las edades relativas de los cóndrulos. El Al se desintegra en Mg con una vida media de 720 000 años. La datación es simplemente una cuestión de encontrar la desviación de la abundancia natural del Mg (el producto de la desintegración del Al) en comparación con la relación de los isótopos estables Al/Mg.

El exceso de Mg (a menudo designado Mg*) se encuentra comparando la relación de Mg/Mg con la de otros materiales del sistema solar.

El cronómetro de Al – Mg da una estimación del período de tiempo para la formación de meteoritos primitivos de solo unos pocos millones de años (1,4 millones de años para la formación del condrulo).



</doc>
<doc id="17171" url="https://es.wikipedia.org/wiki?curid=17171" title="Acantilado">
Acantilado

Un acantilado es un accidente geográfico que consiste en una pendiente o una abrupta vertical. Normalmente se alude a acantilado cuando está sobre la costa, pero también pueden ser considerados como tales los que existen en montañas, fallas y orillas de los ríos. Cuando un acantilado costero de forma tabular alcanza grandes dimensiones se le denomina farallón.

Los acantilados suelen estar compuestos por rocas resistentes a la erosión y al desgaste por la acción atmosférica, generalmente rocas sedimentarias como la limonita, arenisca, caliza, dolomita, aunque también pueden apreciarse rocas ígneas como el basalto o el granito en estas formaciones.

Un escarpe es un caso particular de acantilado, formada por el movimiento de una falla tectónica o un derrumbe. La mayoría de los acantilados acaban en forma dependiente en su base; en áreas áridas o debajo de grandes acantilados, el talud es generalmente una acumulación de rocas desprendidas, mientras que en áreas de mayor humedad, las rocas del talud quedan cubiertas por una capa de tierra compactada por la humedad, formando un suelo.

Muchos acantilados también presentan cascadas y grutas excavadas en la base. A veces los acantilados mueren al final de una cresta, creando estructuras pétreas singulares.

El acantilado costero más alto del mundo es el Thumbnail, que se trata de la cara este del Agdlerussakasit, situado al sur de Groenlandia, con cerca de 1500 msnm.

Otros acantilados que se encuentran entre los más altos del mundo son el risco de Faneque perteneciente al municipio de Agaete en la isla de Gran Canaria (Islas Canarias) con 1027 m, y uno situado en Kaulapapa (Hawái) con 1010 metros.

En Galicia, al noroeste de España, se encuentran los acantilados más altos de Europa continental, que alcanzan altitudes de más de 600 metros en la zona de la Garita Herbeira.




</doc>
<doc id="17172" url="https://es.wikipedia.org/wiki?curid=17172" title="Cuarcita">
Cuarcita

La cuarcita o metacuarcita es una roca metamórfica dura con alto contenido de cuarzo. En composición la mayoría de las cuarcitas llegan a ser más de 90 % de cuarzo y algunas incluso 99 %. El término "cuarcita" a menudo es usado erróneamente para designar a la cuarzoarenita u ortocuarcita, roca sedimentaria cementada con sílice que ha precipitado de aguas intersticiales durante su diagénesis.

Las cuarcita se forma por recristalización a altas temperaturas y presión. La cuarcita carece de foliación. Si presenta capas de hojuelas paralelas de mica blanca la roca obtiene una estructura esquistosa y pasa a llamarse "esquisto de cuarzo".

Tiene una meteorización lenta y produce suelos inusualmente delgados y magros. Su resistencia a la erosión hace que formaciones de cuarcita sobresalgan en el paisaje, como es el caso de numerosas crestas en los montes Apalaches.

La cuarcita pura es empleada como una fuente natural de cuarzo para procesos metalúrgicos y para fabricar ladrillos de sílice. En la industria se emplea cuarcita de alta pureza para fabricar ferrosilicona, arena de sílice, sílice puro y carburo de silicio. Se usa como balasto en caminos y ferrovias. Otros usos son como rocas ornamentales en la construcción y en esculturas.

Durante la Edad de Piedra la cuarcita fue usada como un sustituto del sílex, aunque era de menor calidad.



</doc>
<doc id="17174" url="https://es.wikipedia.org/wiki?curid=17174" title="Lutita">
Lutita

La lutita (del latín "lutum", 'lodo') es una roca sedimentaria clástica de grano muy fino, textura pelítica, variopinta; es decir, integrada por detritos clásticos constituidos por partículas de los tamaños de la arcilla y del limo. En las lutitas negras el color se debe a existencia de materia orgánica. Si la cantidad de esta es muy elevada se trata de lutitas bituminosas.

Colores gris, gris azulado, blanco y verde son característicos de ambientes deposicionales ligeramente reductores. Coloraciones rojas y amarillas representan ambientes oxidantes.

Las lutitas son porosas y a pesar de esto son impermeables, porque sus poros son muy pequeños y no están bien comunicados entre ellos. Pueden ser rocas madre de petróleo y de gas natural. Por metamorfismo se convierten en pizarras o en filitas. Su diagénesis corresponde a procesos de compactación y deshidratación.

Según su forma de fragmentación, las lutitas pueden ser físiles o no físiles. Las "físiles" se escinden en planos paralelos espacialmente próximos. Las "no físiles", en cambio, se escinden en fragmentos o bloques. 
Su contenido mineralógico está conformado por minerales arcillosos, cuarzo, feldespato y micas.

El gran rendimiento económico del petróleo y el gas natural junto con el método de perforación mediante fracturación hidráulica, impulsados por el descubrimiento de la propiedad «endurecedora» de agua del fruto pulverizado de la leguminosa "guar", que permite inyectar arena en pozos horizontales; ha propiciado emprender la extracción de estos hidrocarburos contenidos en lutitas.


</doc>
<doc id="17181" url="https://es.wikipedia.org/wiki?curid=17181" title="Litificación">
Litificación

Litificación es el proceso, generalmente de compactación y cementación, por el cual los sedimentos se convierten en rocas sedimentarias. Una arena al litificarse se transforma en una arenisca, las gravas se convierten en conglomerados y brechas, el limo en limolita, la arcilla en lutita...



</doc>
<doc id="17182" url="https://es.wikipedia.org/wiki?curid=17182" title="Placa tectónica">
Placa tectónica

Una placa tectónica o placa litosférica es un fragmento de litosfera relativamente rígido que se mueve sobre la astenosfera, una zona relativamente plástica del manto superior. Toda la litosfera está dividida en placas tectónicas, quince de ellas de gran tamaño y más de cuarenta microplacas. En los bordes de las placas se concentra actividad sísmica, volcánica y tectónica. Esto da lugar a la formación de grandes cadenas montañosas y cuencas sedimentarias. La palabra tectónica deriva del griego antiguo τέκτων, τέκτωνος: nominativo y genitivo de singular de "constructor, carpintero", y del sufijo ικα: "relativo a".

La tectónica de placas es la teoría que explica la estructura y dinámica de la superficie terrestre. Establece que la litosfera (la zona dinámica superior, la más externa y rígida de la Tierra) está fragmentada en una serie de placas que se desplazan sobre la astenosfera. Esta teoría también describe el movimiento de las placas, sus direcciones e interacciones y explica fenómenos como el cinturón de fuego del Pacífico, los arco-isla o las fosas oceánicas. 

La Tierra es el único planeta del sistema solar con placas tectónicas activas, aunque hay evidencias de que en tiempos remotos Marte, Venus y alguno de los satélites galileanos, como Europa, fueron tectónicamente activos.

Aunque la teoría de la tectónica de placas fue formalmente establecida en las décadas de 1960 y 1970, en realidad es producto de más de dos siglos de observaciones geológicas y geofísicas. En el siglo XIX se observó que en el pasado remoto de la Tierra existieron numerosas cuencas sedimentarias, con espesores estratigráficos de hasta diez veces los observados en el interior de los continentes, y que –posteriormente– procesos desconocidos las deformaron y originaron cordilleras: sucesiones montañosas de enormes dimensiones que pueden incluir sierras paralelas.
A estas cuencas se les denominó geosinclinales, y al proceso de deformación, orogénesis. Otro descubrimiento del siglo fue una cadena montañosa o "dorsal" en medio del océano Atlántico, que observaciones posteriores mostraron que se extendía formando una red continua por todos los océanos.
Un avance significativo en el problema de la formación de los geosinclinales y sus orogenias ocurrió entre 1908 y 1912, cuando Alfred Wegener, al mirar las líneas de costa a ambos lados del Océano Atlántico y tras considerar cierta información geológica (rocas del mismo tipo y edad coincidían con otras situadas hoy en día a larga distancia), paleontológica (encontró fósiles de los mismos animales terrestres en continentes separados) y paleo climática (supuso que al norte se hallaban bosques tropicales y al sur glaciares), hipotetizó que las masas continentales estaban en movimiento y que se habían fragmentado de un supercontinente que denominó Pangea. Tales movimientos habrían deformado los sedimentos geosinclinales acumulados en sus bordes y originado nuevas cadenas montañosas.
Wegener creía que los continentes se deslizaban sobre la superficie de la corteza terrestre bajo los océanos como un bloque de madera sobre una mesa, y que esto se debía a las fuerzas de marea producidas por la deriva de los polos. Sin embargo, pronto se demostró que estas fuerzas son del orden de una diezmillonésima a una centésima de millonésima de la fuerza gravitatoria, lo cual hacía imposible plegar y levantar las masas de las cordilleras.
Mediante la teoría de la tectónica de placas se explicó finalmente que todos estos fenómenos (deriva continental, formación de cordilleras continentales y submarinas) son manifestaciones de procesos de liberación del calor del interior de la Tierra. Hay cuatro procesos a los que se debe dicho calor:

Las placas litosféricas son esencialmente de dos tipos, según la clase de corteza que forma la superficie. Hay dos clases de corteza: la oceánica y la continental.



Actualmente existen las siguientes placas tectónicas en la superficie de la Tierra con límites más o menos definidos, que se dividen en 15 placas mayores (o principales) y 43 placas menores (o secundarias).

Las placas limitan entre sí por tres tipos de situaciones:


Las zonas de las placas contiguas a los límites —los bordes de placa— son las regiones de mayor actividad geológica interna del planeta. En ellas se concentran:



</doc>
<doc id="17183" url="https://es.wikipedia.org/wiki?curid=17183" title="Plegamiento">
Plegamiento

Plegamiento o pliegue, es una deformación de las rocas, generalmente sedimentarias, en la que elementos de carácter horizontal, como los estratos o los planos de esquistosidad (en el caso de rocas metamórficas), quedan curvados formando ondulaciones alargadas y de direcciones más o menos paralelas entre sí.

Los pliegues se originan por esfuerzos de compresión sobre las rocas que no llegan a romperlas; en cambio, cuando sí lo hacen, se forman las llamadas fallas. Por lo general se ubican en los bordes de las placas tectónicas y obedecen a dos tipos de fuerzas: laterales, originados por la propia interacción de las placas (convergencia) y verticales, como resultado del levantamiento debido al fenómeno de subducción a lo largo de una zona de subducción más o menos amplia y alargada, en la que se levantan las cordilleras o relieves de plegamientos.



Los pliegues se pueden clasificar atendiendo a varias características:








Los pliegues no se suelen encontrar aislados, sino que se asocian. Las asociaciones más sencillas de pliegues son:



</doc>
<doc id="17184" url="https://es.wikipedia.org/wiki?curid=17184" title="Barlovento">
Barlovento

Barlovento hace referencia a varios artículos:




</doc>
<doc id="17186" url="https://es.wikipedia.org/wiki?curid=17186" title="Glaciar de circo">
Glaciar de circo

Un glaciar de circo es un tipo de glaciar que se limita a su cuenca o circo de acumulación. Apenas tiene lengua glaciar y tiene una reducida zona de ablación debido a la poca abundancia de hielo.

El glaciar de circo es un glaciar de montaña. Suele tener un fondo poco inclinado, y puede ser el resultado de un retroceso glaciar en el que la lengua glaciar ha desaparecido. También puede ser, en una fase de crecimiento, la etapa anterior a la aparición de la lengua glaciar, cuando la acumulación de hielo aún no permite que el glaciar se desborde de su circo.

Resulta a veces difícil distinguir un glaciar de circo de un glaciar suspendido, debido a que su morfología es muy parecida. En el glaciar suspendido, la topografía abrupta impide el crecimiento del glaciar fuera de su circo de acumulación y el exceso de hielo se elimina mediante caídas de seracs.

El glaciar de circo no debe confundirse con un circo glaciar que es una parte de un glaciar. El circo glaciar es la cuenca circular o semi circular producida por la acción del hielo de un glaciar en su zona de acumulación o de alimentación. La masa de hielo comprimido se mueve por deslizamiento y esta acción forma, por la abrasión, la concavidad rocosa circular o circo. En el caso de un glaciar de valle, el circo glaciar se sitúa en su cabecera, punto de partida del río glaciar. Los circos pueden tener forma de silla o anfiteatro, y tienen flancos abruptos o casi verticales rodeados de cimas.

El circo suele dividirse en dos partes: la baja, donde se acumula la nieve y hielo y la alta, con pendientes mayores pero con un hielo más compacto por su temperatura más baja. Ambas zonas suelen quedar separadas por una grieta más o menos transversal u horizontal que se denomina rimaya.









</doc>
<doc id="17188" url="https://es.wikipedia.org/wiki?curid=17188" title="Juana I de Castilla">
Juana I de Castilla

Juana I de Castilla, llamada «la Loca» (Toledo, 6 de noviembre de 1479-Tordesillas, 12 de abril de 1555), fue reina de Castilla de 1504 a 1555, y de Aragón y Navarra, desde 1516 hasta 1555, si bien desde 1506 no ejerció ningún poder efectivo y a partir de 1509 vivió encerrada en Tordesillas, primero por orden de su padre, Fernando el Católico, y después por orden de su hijo, el rey Carlos I.

Por nacimiento, fue infanta de Castilla y Aragón. Desde joven mostró signos de indiferencia religiosa que su madre trató de mantener en secreto. En 1496 contrajo matrimonio con su primo tercero Felipe el Hermoso, archiduque de Austria, duque de Borgoña, Brabante y conde de Flandes. Tuvo con él seis hijos. Por muerte de sus hermanos Juan e Isabel y de su sobrino Miguel de la Paz, se convirtió en heredera de las coronas de Castilla y de Aragón, así como en señora de Vizcaya, título que ya entonces iba unido a la corona de Castilla y que Juana heredó de su madre Isabel I de Castilla. A la muerte de su madre, Isabel la Católica, en 1504 fue proclamada reina de Castilla junto a su esposo; y a la de su padre, Fernando el Católico, en 1516 pasó a ser la nominal reina de Navarra y soberana de la corona de Aragón. Por lo tanto, el 25 de enero de 1516, se convirtió –en teoría– en la primera reina de las coronas que conformaron la actual España; sin embargo, desde 1506 su poder solo fue nominal, fue su hijo Carlos el rey efectivo de Castilla y de Aragón. El levantamiento comunero de 1520 la sacó de su cárcel y le pidió encabezar la revuelta, pero ella se negó, y cuando su hijo Carlos derrotó a los comuneros volvió a encerrarla. Más adelante Carlos ordenaría que la obligasen a recibir los sacramentos, aunque fuese mediante tortura.

Fue apodada «la Loca» por una supuesta enfermedad mental alegada por su padre y por su hijo para apartarla del trono y mantenerla encerrada en Tordesillas de por vida. Se ha escrito que la enfermedad podría haber sido causada por los celos hacia su marido y por el dolor que sintió tras su muerte. Esta visión de su figura fue popularizada en el Romanticismo tanto en pintura como en literatura.

La aceptación de la «locura» de doña Juana se ha mantenido en mayor o menor medida durante el , pero está siendo revisada en el , sobre todo a raíz de los estudios de la investigadora estadounidense Bethany Aram y de los españoles Segura Graíño y Zalama que han sacado a la luz nuevos datos sobre su figura.

La reina Juana fue la tercera de los hijos de Fernando II de Aragón y de Isabel I de Castilla. El 6 de noviembre de 1479 nació en Toledo y fue bautizada con el nombre del santo patrón de su familia, al igual que su hermano mayor, Juan.

Desde pequeña, recibió la educación propia de una infanta e improbable heredera al trono, basada en la obediencia más que en el gobierno, a diferencia de la exposición pública y las enseñanzas del gobierno requeridos en la instrucción de un príncipe heredero. En el estricto e itinerante ambiente de la corte castellano-aragonesa de su época, Juana estudió comportamiento religioso, urbanidad, buenas maneras propias de la corte, sin desestimar artes como la danza y la música, el entrenamiento como amazona y el conocimiento de lenguas romances propias de la península ibérica, además del francés y del latín. Entre sus principales preceptores se encontraban el sacerdote dominico Andrés de Miranda, Beatriz Galindo y su madre, la reina, que trató de moldearla a su «hechura devocional».

El manejo de la casa de la infanta y, por ende, de su ambiente inmediato estaba totalmente dominado por sus padres. La casa incluía personal religioso, oficiales administrativos, personal encargado de la alimentación, criadas y esclavas, todos seleccionados por sus padres sin intervención de ella misma. A diferencia de Juana, su hermano Juan, príncipe de Asturias y de Gerona, comenzó a hacerse cargo de su casa y de posesiones territoriales como entrenamiento en el dominio de sus futuros reinos.

Ya en 1495 Juana daba muestras de escepticismo religioso y poca devoción por el culto y los ritos cristianos. Este hecho alarmaba a su madre, que ordenó que se mantuviese en secreto.

Como era costumbre en la Europa de esos siglos, Isabel y Fernando negociaron los matrimonios de todos sus hijos con el fin de asegurar objetivos diplomáticos y estratégicos. A fin de reforzar los lazos con el emperador Maximiliano I de Habsburgo contra los monarcas franceses de la dinastía Valois, ofrecieron a Juana en matrimonio a su hijo, Felipe, archiduque de Austria. A cambio de este enlace, los Reyes Católicos pedían la mano de la hija de Maximiliano, Margarita de Austria, como esposa para el príncipe Juan. Con anterioridad, Juana había sido considerada para el delfín Carlos, heredero del trono francés, y en 1489 pedida en matrimonio por el rey Jacobo IV de Escocia, de la dinastía Estuardo.

En agosto de 1496, la futura archiduquesa partió de Laredo en una de las carracas genovesas al mando del capitán Juan Pérez. La flota también incluía, para demostrar el esplendor de la corona castellano-aragonesa a las tierras del norte y su poderío al hostil rey francés, otros diecinueve buques, desde naos a carabelas, con una tripulación de 3500 hombres, al mando del almirante Fadrique Enríquez de Velasco, y pilotada por Sancho de Bazán. Se le unieron asimismo unos sesenta navíos mercantes que transportaban la lana exportada cada año desde Castilla. Era la mayor flota en misión de paz montada hasta entonces en Castilla. Juana fue despedida por su madre y hermanos, e inició su rumbo hacia Flandes, hogar de su futuro esposo. 

La travesía tuvo algunos contratiempos que, en primer lugar, la obligaron a tomar refugio en Portland, Inglaterra, el 31 de agosto. Cuando finalmente la flota pudo acercarse a Middelburg, Zelanda, una carraca genovesa que transportaba a 700 hombres, las vestimentas de Juana y muchos de sus efectos personales, encalló en un banco de piedras y arena y tuvo que ser abandonada. 

Juana, por fin en las tierras del norte, no fue recibida por su prometido. Ello se debía a la oposición de los consejeros francófilos de Felipe a las alianzas de matrimonio pactadas por su padre el emperador. Aún en 1496, los consejeros albergaban la posibilidad de convencer a Maximiliano de la inconveniencia de una alianza con los Reyes Católicos y las virtudes de una alianza con Francia.

La boda se celebró formalmente, por fin, el 20 de octubre de 1496 en la iglesia colegiata de San Gumaro de la pequeña ciudad de Lier, gracias a la influencia de la familia Berghes. El obispo de Cambrai, que posteriormente sería el líder de la facción españolista, Enrique de Bergen, realizó la ceremonia oficial de la boda. El ambiente de la corte con el que se encontró Juana era radicalmente opuesto al que vivió en su España natal. Por un lado, la sobria, religiosa y familiar corte de Fernando e Isabel contrastaba con la desinhibida y muy individualista corte borgoñona-flamenca, muy festiva y opulenta gracias al comercio de tejidos que sus mercados dominaban desde hacía un siglo y medio. En efecto, a la muerte de María de Borgoña, la casa de Felipe, de cuatro años, había sido rápidamente dominada por los grandes nobles borgoñones, principalmente a través de consejeros adeptos y fieles a sus intereses.

Aunque los futuros esposos no se conocían, se enamoraron al verse. No obstante, Felipe pronto perdió el interés en la relación, lo cual hizo nacer en Juana unos celos que han sido considerados patológicos por varios autores. 

Al poco tiempo llegaron los hijos, con periodos de abstinencia conyugal que agudizaron los celos de Juana. El 15 de noviembre de 1498, en la ciudad de Lovaina (cerca de Bruselas) nació su primogénita, Leonor, llamada así en honor de la abuela paterna de Felipe, Leonor de Portugal. Juana vigilaba a su esposo todo el tiempo y, pese al avanzado estado de gestación de su segundo embarazo, del que nacería Carlos (llamado así en honor al abuelo materno de Felipe, Carlos el Temerario), el 24 de febrero de 1500, asistió a una fiesta en el palacio de Gante. Aquel mismo día tuvo a su hijo, según se dice, en un "retrete" del palacio. Al año siguiente, el 18 de julio de 1501, en Bruselas, nació una hija, llamada Isabel en honor de la madre de Juana, Isabel la Católica.

Varios sacerdotes enviados a Flandes por los Reyes Católicos informaron en este tiempo de que Juana seguía resistiéndose a confesarse y a asistir a misa.

Muertos sus hermanos Juan (1497) e Isabel (1498), así como el hijo de esta, el infante portugués Miguel de Paz (1500), Juana se convirtió en heredera de Castilla y Aragón. En noviembre de 1501 Felipe y Juana, dejando a sus hijos en Flandes, emprendieron camino hacia Castilla por tierra desde Bruselas. Tardaron seis meses en llegar a Toledo , donde prestaron juramento como herederos ante las cortes castellanas en la catedral de Toledo el 22 de mayo de 1502.

En 1503 el marido de Juana, Felipe, regresó a Flandes a fin de resolver unos asuntos mientras que Juana, embarazada, permanecía en España a petición de sus padres, quienes deseaban que ella conociera a sus futuros súbditos. Estar alejada de su marido e hijos la sumió en una gran tristeza. El 10 de marzo de 1503, en la ciudad de Alcalá de Henares, dio a luz un hijo al que llamó Fernando en honor a su abuelo materno, Fernando el Católico. Tras el parto, y con sus tres hijos mayores en Bruselas, Juana volvió a pedir autorización para regresar a Flandes, pero su madre se opuso. La guerra con Francia convertía en inviable el camino por tierra. Ante la insistencia de Juana, Isabel ordenó al obispo Fonseca que recluyera a su hija en el castillo de la Mota. Madre e hija terminaron en disputa y, al final, Isabel consintió que Juana regresase a Flandes, donde llegó en junio de 1504. El episodio del castillo de la Mota, en el que la hija incurrió en desacato, había causado tanto disgusto a la reina que se vio obligada a justificarla delante de distintas personalidades. Rogó a su esposo que, cuando Juana llegara a Flandes, la vigilara gente de su confianza para evitar nuevos desacatos, aunque esperaba que la reunión con el esposo produjera un efecto beneficioso en el carácter de su hija.

La reina Isabel murió el 26 de noviembre de 1504, planteándose el problema de la sucesión en Castilla. Según el historiador Gustav Bergenroth, su madre desheredó a Juana en su testamento porque no iba a misa ni quería confesarse. Sin embargo, su padre, Fernando, la proclamó reina de Castilla y siguió él mismo gobernando el reino.

Pero el marido de Juana, el archiduque Felipe, no estaba dispuesto a renunciar al poder, y en la concordia de Salamanca (1505) se acordó el gobierno conjunto de Felipe, Fernando el Católico y la propia Juana. Entretanto, Felipe y Juana permanecieron en la corte de Bruselas, donde el 15 de septiembre de 1505 ella dio a luz a su quinto hijo, una niña llamada María (llamada así en honor a su abuela paterna, María de Borgoña). Mientras tanto, se preparó una gran flota para transportar a la nueva familia real castellana a su reino.

A finales de 1505, Felipe estaba impaciente por llegar a Castilla y por ello ordenó que zarpase la flota cuanto antes, a pesar del riesgo que suponía navegar en invierno. Partieron el 10 de enero de 1506, con 40 barcos. En el canal de la Mancha, una fuerte tormenta hundió varios navíos y dispersó al resto. Se temió por la vida de los reyes, que al final recalaron en Portland. La armada tuvo que permanecer durante tres meses en Inglaterra. En Londres, Juana pudo visitar durante un día a su hermana Catalina, a la que no veía desde hacía diez años. Zarparon de nuevo en abril de 1506 y en vez de dirigirse a Laredo, donde se los esperaba, pusieron rumbo a La Coruña, probablemente para ganar tiempo y poder reunirse con nobles castellanos antes de presentarse ante Fernando. Felipe consiguió el apoyo de la mayoría de la nobleza castellana, por lo que Fernando tuvo que firmar la concordia de Villafáfila (27 de junio de 1506) y retirarse a Aragón con una serie de compensaciones económicas. Felipe fue proclamado rey de Castilla en las Cortes de Valladolid con el nombre de Felipe I.

El 25 de septiembre de ese año murió Felipe I el Hermoso en el Palacio de los Condestables de Castilla; según algunos, envenenado, y entonces circularon rumores sobre una supuesta locura de Juana. En ese momento ella decidió trasladar el cuerpo de su esposo desde Burgos, donde había muerto y en el que ya había recibido sepultura, hasta Granada, tal como él mismo había dispuesto viéndose morir (excepto su corazón, que deseaba que se mandase a Bruselas, como así se hizo), viajando siempre de noche. Pero su padre se mostró reacio a permitir que su yerno estuviera enterrado en Granada antes que él mismo, y los desplazamientos se limitaron en un espacio reducido en Castilla. La reina Juana no se separaría ni un momento del féretro y este traslado se prolongaría durante ocho fríos meses por tierras castellanas. Acompañaron al féretro gran número de personas, entre las que se contaban religiosos, nobles, damas de compañía, soldados y sirvientes diversos. Ello hizo que las murmuraciones sobre la locura de la reina aumentasen cada día entre los habitantes de los pueblos que atravesaban. Después de unos meses, los nobles, «obligados» por su posición a seguir a la reina, se quejaron de estar perdiendo el tiempo en esa «locura» en lugar de ocuparse, como deberían, de sus tierras. En la ciudad de Torquemada (Palencia), el 14 de enero de 1507, Juana daba a luz a su sexto hijo y póstumo de su marido, una niña bautizada con el nombre de Catalina (llamada así en honor a su hermana pequeña, Catalina de Aragón).

En cuanto al gobierno del reino, el 24 de septiembre, la víspera de la muerte de Felipe I, los nobles acordaron formar un Consejo de Regencia interina para gobernar provisionalmente el reino presidido por Cisneros y formado por el almirante de Castilla, el condestable de Castilla; Pedro Manrique de Lara y Sandoval, duque de Nájera; Diego Hurtado de Mendoza y Luna, duque del Infantado; Andrés del Burgo, embajador del emperador; y Filiberto de Vere, mayordomo mayor del rey Felipe. La nobleza y las ciudades contendieron acerca de quién debía desempeñar la Regencia, pues por un lado estaban los que querían al emperador Maximiliano durante la minoría del príncipe Carlos, como los Manrique, Pacheco y Pimentel; y por otro lado, los que querían la regencia de Fernando el Católico tal y como quedó establecida en el testamento de Isabel la Católica y las cortes de Toro de 1505, como los Velasco, Enríquez, Mendoza y Álvarez de Toledo. Sin embargo, la reina Juana trató de gobernar por sí misma, revocó e invalidó las mercedes otorgadas por su marido, para lo cual intentó restaurar el Consejo Real de la época de su madre.
Sin consultar a Juana, Cisneros acudió a Fernando el Católico para que regresara a Castilla. Pero a pesar de los intentos de Cisneros, nobles y prelados, la reina no reclamó a su padre para gobernar y de hecho llegó a prohibir la entrada del arzobispo a palacio. Para dar legalidad al nombramiento de regente a Fernando el Católico, el Consejo Real y Cisneros buscaron encauzar el vacío de poder con la convocatoria de Cortes, pero la reina se negó a convocarlas, y los procuradores abandonaron Burgos sin haberse constituido como tales.

Tras regresar de tomar posesión del Reino de Nápoles, Fernando el Católico se entrevistó con su hija el 28 de agosto de 1507, y volvió a asumir el gobierno de Castilla. En febrero de 1509, Fernando ordenó encerrar a Juana en Tordesillas para evitar que se formase un partido nobiliario en torno de su hija, encierro que mantendría su hijo Carlos I más adelante. El encierro de Juana también estuvo motivado para impedir las apetencias del rey de Inglaterra y el emperador sobre el gobierno de Castilla. El rey Enrique VII de Inglaterra manifestó su interés en casarse con Juana, y Fernando tuvo que salvar diplomáticamente el asunto presentando a su nieto Carlos, príncipe de Asturias, como su hijo y sucesor, y planteando el matrimonio del príncipe con María Tudor hija del rey inglés; Enrique VII murió en 1509 y su sucesor Enrique VIII casó con la hija de Fernando, Catalina de Aragón, zanjando la oposición inglesa a la regencia de Fernando. Solo quedaba la oposición del emperador Maximiliano I, que amenazó con traer a su nieto, el príncipe de Asturias, a Castilla y gobernar en su nombre, al temer que el segundo matrimonio de Fernando podría engendrar un hijo varón que podría poner en peligro la sucesión de su nieto, el príncipe Carlos. Fernando aprovechó la debilidad del emperador en Italia frente a Venecia para asegurarse un acuerdo favorable en Blois en diciembre de 1509, que respetaba la voluntad de Isabel la Católica a cambio de unas no excesivas compensaciones económicas, por lo que el emperador renunciaba a sus pretensiones de regencia en Castilla, y en las Cortes de 1510 ratificaron a Fernando como regente.

En 1515 Fernando incorporó a la Corona de Castilla el Reino de Navarra, que había conquistado tres años antes. En 1516 murió el rey y, por su testamento, Juana se convirtió en reina nominal también de Aragón. Sin embargo, varias instituciones de la Corona aragonesa no la reconocieron como tal en virtud de la complejidad institucional de los fueros. Ejercieron la regencia de Aragón el arzobispo de Zaragoza, Alonso de Aragón, hijo natural de Fernando el Católico, y la de Castilla el cardenal Cisneros hasta la llegada del príncipe Carlos desde Flandes.

Carlos se benefició de la coyuntura de la incapacidad de Juana para proclamarse rey, de forma que se apropió de los títulos reales que le correspondían a su madre. Así, oficialmente, ambos, Juana y Carlos, correinaron en Castilla y Aragón. De hecho, Juana nunca fue declarada incapaz por las Cortes de Castilla ni se le retiró el título de reina. Mientras vivió, en los documentos oficiales debía figurar en primer lugar el nombre de la reina Juana. Pero, en la práctica, Juana no tuvo ningún poder real porque Carlos mantuvo a su madre encerrada. De hecho, ordenó que la obligasen a asistir a misa y confesarse, empleando tortura si fuere necesario.

Desde que su padre la recluyera, en 1509, la reina Juana permaneció cuarenta y seis años en una casona-palacio-cárcel de Tordesillas, vestida siempre de negro y con la única compañía de su última hija, Catalina, hasta que esta salió en 1525 para casarse con Juan III de Portugal. Murió el 12 de abril de 1555. Según algunos autores, Juana y su hija fueron ninguneadas y maltratadas física y psicológicamente por sus carceleros. Especialmente duros fueron los largos años de servicio de los segundos marqueses de Denia, Bernardo de Sandoval y Rojas y su esposa, Francisca Enríquez. El marqués cumplió su función con gran celo, como parecía jactarse en una carta dirigida al emperador en la que aseguraba que, aunque doña Juana se lamentaba constantemente diciendo que la tenía encerrada «como presa» y que quería ver a los grandes, «porque se quiere quejar de cómo la tienen», el rey debía estar tranquilo, porque él controlaba la situación y sabía dar largas a esas peticiones. El confinamiento de doña Juana, por su presunta incapacidad mental, era esencial para la legitimidad en el trono castellano, primero de su padre, Fernando, y después de su hijo, Carlos I. Ante cualquier sospecha de que la reina estaba, en realidad, mentalmente estable, los adversarios del nuevo rey podrían derrocarlo por usurpador. De ahí que la figura de doña Juana se convirtiera en una pieza clave para legitimar el movimiento de las Comunidades.

Los reyes Fernando y Carlos trataron de borrar cualquier vestigio documental del encierro de la reina Juana. No existe rastro alguno de la correspondencia intercambiada entre Fernando y Luis Ferrer; Carlos V parece haber tenido el mismo cuidado. Incluso Felipe II ordenó quemar ciertos papeles relativos a su abuela.

El levantamiento comunero (1520) la reconoció como soberana en su lucha contra Carlos I. Después del incendio de Medina del Campo, el gobierno del cardenal Adriano de Utrecht se tambaleó. Muchas ciudades y villas se sumaron a la causa comunera, y los vecinos de Tordesillas asaltaron el palacio de la reina obligando al marqués de Denia a aceptar que una comisión de los asaltantes hablara con doña Juana. Entonces se enteró la reina de la muerte de su padre y de los acontecimientos que se habían producido en Castilla desde ese momento. Días más tarde Juan de Padilla se entrevistó con ella, explicándole que la Junta de Ávila se proponía acabar con los abusos cometidos por los flamencos y proteger a la reina de Castilla, devolviéndole el poder que le había sido arrebatado, si es que ella lo deseaba. A lo cual doña Juana respondió: «"Sí, sí, estad aquí a mi servicio y avisadme de todo y castigad a los malos"». El entusiasmo comunero, después de esas palabras, fue enorme. Su causa parecía legitimada por el apoyo de la reina.

A partir de ahí el objetivo de los comuneros sería, en primer lugar demostrar que doña Juana no estaba loca y que todo había sido un complot, iniciado en 1506, para apartarla del poder; y después, que la reina, además de con sus palabras, avalara con su firma los acuerdos que se fueran tomando. Para ello, la Junta de Ávila se trasladó a Tordesillas, que se convertiría por algún tiempo en centro de actuación de los comuneros. Después de estos cambios, todos, incluso el cardenal, afirmaban que doña Juana «parece otra» porque se interesaba por las cosas, salía, conversaba, cuidaba de su personal y, por si fuera poco, pronunciaba unas atinadas y elocuentes palabras ante los procuradores de la Junta; palabras que recogieron notarios y se comenzaron a difundir. Pero la Junta necesitaba algo más que palabras de la reina, necesitaba documentos, necesitaba la firma real para validar sus actuaciones. Una firma que podía suponer el final del reinado de Carlos, como recuerda a este el cardenal Adriano: «si firmase su alteza, que sin duda alguna todo el Reino se perderá». Pero en esto los comuneros, como antes los partidarios del rey, tropezaron con la férrea negativa de doña Juana, a la que ni ruegos ni amenazas hicieron firmar papel alguno.

A finales de 1520, el ejército imperial entró en Tordesillas, restableciendo en su cargo al marqués de Denia. Juana volvió a ser una reina cautiva, como aseguraba su hija Catalina, cuando comunicaba al emperador que a su madre no la dejaban siquiera pasear por el corredor que daba al río: «y la encierran en su cámara que no tiene luz ninguna».

La vida de doña Juana se deterioró progresivamente, como testimoniaron los pocos que consiguieron visitarla. Sobre todo cuando su hija menor, que procuró protegerla frente al despótico trato del marqués de Denia, tuvo que abandonarla en 1525 para contraer matrimonio con el rey de Portugal. Desde ese momento, los episodios depresivos se sucedieron cada vez con más intensidad.

En los últimos años, a la presunta enfermedad mental se unía la física, completamente cierta. Tenía grandes dificultades en las piernas, las cuales finalmente se le paralizaron. Entonces volvió a ser objeto de discusión su indiferencia religiosa, sugiriendo algunos religiosos que podía estar endemoniada. Por ello, su nieto, Felipe II, pidió a un jesuita, el futuro san Francisco de Borja, que la visitara y averiguara qué había de cierto en todo ello. Después de hablar con ella, el jesuita aseguró que las acusaciones carecían de fundamento y que, dado su estado mental, quizá la reina no había sido tratada adecuadamente. Sin embargo, en su lecho de muerte se negó a confesarse al serle administrada la extremaunción.

La versión oficial en el fue que la reina Juana había sido retirada del trono por su incapacidad debida a una enfermedad mental. Se ha escrito que pudo padecer de melancolía,trastorno depresivo severo, psicosis, esquizofrenia heredada o, más recientemente, un trastorno esquizoafectivo. Hay debate sobre el diagnóstico de su enfermedad mental, considerando que sus síntomas se agravaron por un confinamiento forzoso y el sometimiento a otras personas. También se ha especulado que pudo heredar alguna enfermedad mental de la familia de su madre, ya que su abuela materna, Isabel de Portugal, reina de Castilla, padeció por lo mismo durante su viudez después de que su hijastro la exiliara al castillo de Arévalo, en Ávila.

Gustav Bergenroth fue el primero, en los años 1860, que halló documentos en Simancas y en otros archivos que mostraban que la hasta entonces llamada Juana «la Loca» en realidad había sido víctima de una confabulación tramada por su padre, Fernando el Católico, y luego confirmada por su hijo, Carlos I.

El recuerdo de Juana se fue desvaneciendo con el paso del tiempo pero su figura resultó muy atractiva para el romanticismo, porque reunía una serie de características muy valoradas por este: la pasión arrebatadora de un amor no correspondido, la locura por desamor y los celos desmedidos. En 1836, el pintor francés Charles de Steuben plasmó en un cuadro todos los tópicos de la leyenda sobre la reina y más tarde en España se dio rienda suelta a la imaginación y se fijó la imagen de la locura por amor de Juana.

Numerosos artistas consagraron alguna de sus obras al personaje: Eusebio Asquerino y Gregorio Romero de Larrañaga ("Felipe el Hermoso"), Manuel Tamayo y Baus ("Locura de amor"), Emilio Serrano ("Doña Juana la Loca"), Lorenzo Vallés ("La demencia de doña Juana de Castilla"), Santiago Sevilla ("Juana la Loca Tragedia en Cuatro Actos"). Pero, sin duda, la obra más famosa inspirada en la reina fue el cuadro "Doña Juana "la Loca"" (1877), de Francisco Pradilla y Ortiz, actualmente en el Museo del Prado.

Los últimos meses de la vida de la reina se recrean en la obra de teatro "Santa Juana de Castilla", de Benito Pérez Galdós, estrenada en el Teatro María Guerrero de Madrid el 8 de mayo de 1918, con un elenco encabezado por la actriz Margarita Xirgu.

El personaje también es recreado en la pieza "El Cardenal de España" (1960), del francés Henry de Montherlant, centrada en los últimos meses de la vida del Cardenal Cisneros. En el estreno mundial, en París, el personaje fue interpretado por la actriz francesa Louise Conte y en la adaptación española, de 1962, por Luisa Sala. En la obra "Los Comuneros" (1974), de Ana Diosdado, la reina Juana aparece como uno de los personajes principales. Fue interpretado por Irene Gutiérrez Caba. Curiosamente, tres años más tarde el personaje sería interpretado por su hermana Julia y cuarenta años más tarde, por su nieta Irene Escolar, en ambos casos para televisión.

En 2012, con su obra "Juana la loca" María Jesús Romero se adentró en las reflexiones de doña Juana, a la que muestra atrapada en una profunda crisis de identidad. Se trata de un monólogo para una sola actriz, que no ha sido llevado al teatro. En 2013 se estrenó "Juana, la reina que no quiso reinar", de Jesús Carazo, obra en la que aparece una Juana cercana al mito romántico: casada de adolescente contra su voluntad y encerrada después durante cuarenta y seis años por la única locura de ser mujer antes que reina y defender el amor por encima del poder. En este caso, doña Juana es interpretada por Gema Matarranz.

En 2016 el personaje histórico fue interpretado por Concha Velasco, en un monólogo titulado "Reina Juana", escrito por Ernesto Caballero y dirigido por Gerardo Vera, estrenado el 28 de abril de 2016 en el Teatro de La Abadía de Madrid. El argumento arranca con la confesión de Juana I de Castilla ante el padre Francisco de Borja la noche anterior a su muerte. A partir de ahí la reina va desgranando los momentos más importantes de su vida, mientras en su divagar induce al espectador a recorrer una buena parte de la historia de España. En este gran "flashback" doña Juana alza la voz con lucidez contra todos aquellos que la llevaron al destierro convirtiéndola en una sombra: primero su marido, Felipe el Hermoso; después su padre, Fernando el Católico, que la recluye en Tordesillas; y finalmente, su hijo Carlos V, que la ignora. Según la visión del autor, todos ellos la hicieron pasar por enajenada para poder incapacitarla en sus funciones y dar rienda suelta a sus ambiciones.






Desde 2014 la calle de Leopoldo de Castro de Valladolid fue rebautizada como Calle de Juana de Castilla.

Con su esposo Felipe I el Hermoso tuvo seis hijos:





</doc>
<doc id="17190" url="https://es.wikipedia.org/wiki?curid=17190" title="ICab">
ICab

iCab es un navegador web privativo para Mac OS y Mac OS X desarrollado en Alemania por Alexander Clauss.

Actualmente se ofrece servicio técnico para tres versiones de iCab: iCab 2 para procesadores de la línea Motorola 68000 (68k); iCab 2 para Mac OS clásico, que se ejecuta en procesadores PowerPC; e iCab 3 basado en Carbon para Mac OS X. iCab es uno de los pocos navegadores que todavía se desarrollan para Mac OS 9 y versiones anteriores, y el único para sistemas 68k que ofrece navegación por pestañas.

Aunque todas estas versiones se encuentran disponibles para descarga de manera gratuita, sólo funcionan por un periodo aproximado superior al año; iCab requiere instalar la última versión disponible para continuar funcionando luego de esa fecha. Por otro lado, iCab 3, aunque también está disponible de manera gratuita, restringe el uso de ciertas características como el modo de pantalla completa y filtro de contenidos a usuarios que hayan pagado por una licencia de software.

El motor de renderizado propietario de iCab es generalmente criticado por no tener soporte para CSS ni DOM, dificultando el trabajo de realizar sitios modernos para este navegador. Sin embargo, la primera beta pública de iCab 3 liberada en mayo de 2005 presentó mejoras significativas en este campo, como soporte para CSS2, aunque sólo está disponible para 68k. iCab 3 también tiene soporte para Unicode, usando AITSU en lugar del antiguo WorldScript; ésta es la razón por la que se requiere Mac OS 8.5 o superiores.

Una característica distintiva de iCab es el iCab-Smiley. Dependiendo de si el código HTML del sitio web visitado en ese momento es válido o no, la cara se mostrara alegre y sonriendo o triste. Haciendo clic sobre el smiley se muestran una lista con los errores de la página, aunque si se realizar el clic sosteniendo además la tecla shift se activa un huevo de pascua. Esta característica apareció antes en el navegador web Crystal Atari Browser, un desarrollo anterior del mismo autor para computadoras compatibles con Atari TOS.

Otras prestaciones incluidas en iCab:



</doc>
<doc id="17196" url="https://es.wikipedia.org/wiki?curid=17196" title="Papaloapan">
Papaloapan

Papaloapan es un topónimo náhuatl que significa "Lugar de Mariposas" y hace referencia a artículos relacionados con México:




</doc>
<doc id="17200" url="https://es.wikipedia.org/wiki?curid=17200" title="Radio de Schwarzschild">
Radio de Schwarzschild

El radio de Schwarzschild es la medida del tamaño de un agujero negro de Schwarzschild, es decir, un agujero negro de simetría esférica y estático. Se corresponde con el radio aparente del horizonte de sucesos, expresado en coordenadas de Schwarzschild.

Puesto que el tamaño de un agujero negro depende de la energía absorbida por el mismo, cuanto mayor es la masa del agujero negro, tanto mayor es el radio de Schwarzschild, que viene dada por:

donde:

Esta expresión la halló Karl Schwarzschild en 1916 y constituye parte de una solución exacta para el campo gravitatorio formado por una estrella con simetría esférica no rotante. La solución de Schwarzschild fue la primera solución exacta encontrada para las ecuaciones de la relatividad general. El radio de Schwarzschild es proporcional a la masa del objeto. El radio de Schwarzschild para la masa del Sol es de 3 km mientras que el radio de Schwarzschild de un objeto de la masa terrestre es de tan solo 8.89 mm. El agujero negro supermasivo del centro galáctico tiene una masa de unos 4 millones de masas solares y su radio es, aproximadamente, de 12 millones de kilómetros (unos 40 segundos luz).




</doc>
<doc id="17202" url="https://es.wikipedia.org/wiki?curid=17202" title="Humanidades">
Humanidades

Las humanidades (del latín "humanitas") son un conjunto de disciplinas académicas relacionadas con la cultura humana. Existen otras denominaciones genéricas, como el concepto de «letras», que se utilizan habitualmente por oposición a las denominadas «ciencias» (debate de las dos culturas). No obstante, existen otras denominaciones cuya identificación, asociación o diferenciación con la de «humanidades» es más problemática (según la intención de quien las utilice) y que conllevan distintas consideraciones epistemológicas y metodológicas: las de «ciencias sociales» y «ciencias humanas». Se supone a las disciplinas humanísticas un mayor carácter ideográfico: el estudio de particularidades sin crear leyes o postulados generales. En su origen (los "studia humanitatis" del humanismo renacentista), los saberes humanísticos o letras humanas se definían por oposición a las letras divinas.

Como elemento básico y definitorio de la civilización occidental y del sistema educativo tradicional (en este último también llegan a denominarse "formación humanística"), las humanidades están especialmente vinculadas a los denominados estudios clásicos: el arte y la cultura fundamentada en la Antigüedad grecorromana y que con diversas adiciones a lo largo de los siglos fue conformando el denominado canon occidental, lo que es acusado de distintos sesgos por los críticos de esta perspectiva (intelectualismo, machismo, eurocentrismo, obsolescencia) resumidos en la expresión peyorativa "dead white males" («varones blancos muertos»).

Considerar o no como «humanidades» o «ciencias sociales» a unas u otras disciplinas es un problema académico que trasciende la mera consideración organizativa o universitaria, puesto que implica la condición científica o no de unos u otros saberes (sea cual sea el alcance de tal definición, dado que el criterio de cientificidad tampoco es universalmente aceptado). Tal «cientificidad» es para algunos autores precisamente lo que no pretenden buscar los saberes que aspiran a aproximarse a la condición humana y construir la convivencia social a través de "el cultivo del pasado por medio del estudio filológico y hermenéutico".

Entre las disciplinas o campos de estudio que pueden considerarse como parte de las humanidades (sin que exista un consenso generalizado en ninguna enumeración de ellas), están la filosofía, la filología (lingüística, la semiología, la literatura, la historia de la literatura, la crítica literaria), la historia, la geografía, el derecho, la economía, la ciencia política, la psicología, la antropología, la sociología, los estudios de arte (de artes plásticas, las artes escénicas y la música, la musicología, la estética, la teoría del arte, la crítica de arte), las ciencias de la comunicación (periodismo, publicidad, documentación, biblioteconomía), etc. Paradójicamente, a pesar de la oposición terminológica inicial, los estudios de religión («divinidades» —"divinities" en lengua inglesa—) también se suelen considerar como parte de las «humanidades».

Las ciencias humanas tratan de completar el estudio de la humanidad incluyendo en él el origen evolutivo, la estructura del ser humano, su funcionamiento, sus características hereditarias y su conducta, como individuos y como sociedad. En cuanto a la evolución de la humanidad, los grandes aportes provienen de la antropología física presentando como resultado del último episodio evolutivo al ser humano moderno. La anatomía se fundó sobre la observación directa de la estructura humana en Alejandría hacia el año 300 a. C. La fisiología tuvo sus comienzos en la época en que el inglés William Harvey fue a estudiar a Padua en 1598. La expresión "ciencias morales" tenía la ventaja de indicar que tales ciencias trataban de los productos de la actividad mental del ser humano y no tenían por objeto el estudio del organismo, pero para el siglo XVIII los autores llamados moralistas eran en realidad psicólogos. Las ciencias humanas nacen, según Michel Foucault, en el siglo XIX bajo un modelo de racionalidad científica. A las ciencias humanas también se les llama ciencias del espíritu a partir de la propuesta de Wilhelm Dilthey, cuyo objeto de estudio es el medio histórico cultural en el que el ser humano está inmerso.

En las tres grandes áreas del pensamiento humano, ha tomado forma el mundo de las ideas: las ciencias del espíritu. Se considera que la religión es la que se ha centrado en el espíritu, mientras que la ciencia se ha centrado en la materia. La filosofía ha tratado de vincular a estas dos escuelas a partir de la reflexión consciente y ha planteado recientemente una teoría que podría integrar a las tres: el constructivismo. Para el humanismo la dignidad del hombre estaba constituida por el poder creador del intelecto. Durante la ilustración se trató de sustituir la fe supersticiosa y sumisa por la razón iluminada e iluminante. Las humanidades se han centrado en las actividades netamente humanas, como son el pensamiento y la lengua, que se sistematizan como conocimiento en la filosofía y en la lingüística y a la vez se convierten en medios para que el ser humano desarrolle autoconciencia. El estudio del pensamiento y el lenguaje como cognición, y por tanto de los símbolos y las representaciones, dio origen a la ciencia cognitiva. Para Howard Gardner el lenguaje y las matemáticas son dos de las inteligencias compartidas por todos los seres humanos, sistemas de significado ideados culturalmente para procesar formas importantes de información.En la educación se han desarrollado estas inteligencias mediante la lectura, la escritura y el cálculo. Desde la pedagogía también se plantea el constructivismo como una forma de integrar las formas de aprendizaje que han sido privilegiadas en la adquisición del conocimiento. Por lo tanto las humanidades son las disciplinas que estudian al hombre y su comportamiento en la sociedad.

Edgar Morin plantea que en la educación del futuro es necesario enseñar la condición humana y propone diversas tríadas, llamadas por él bucles, que dan soporte al concepto de lo humano como son: cerebro-mente-cultura, razón-afecto-impulso, individuo-sociedad-especie. Termina su libro invitando a la continuación de la hominización en humanización, vía ascenso a la ciudadanía terrestre.



</doc>
<doc id="17211" url="https://es.wikipedia.org/wiki?curid=17211" title="Mar Tirreno">
Mar Tirreno

El mar Tirreno es la parte del mar Mediterráneo que se extiende al oeste de la península italiana entre las islas de Córcega, Cerdeña y Sicilia y las costas continentales de Toscana, Lacio, Campania y Calabria. Está unido al mar Jónico por el estrecho de Mesina y separado del mar de Liguria por la isla de Elba. Su profundidad máxima es de 3731 metros. Se encuentra próximo a la falla que divide África de Europa por lo que abundan las cadenas montañosas y los volcanes.

Toma el nombre del antiguo pueblo tirreno, más conocido como etrusco; Heródoto cuenta que, desde Anatolia occidental, el pueblo lidio emigró buscando una nueva patria guiado por el príncipe "Tirreno", y que al llegar a las costas de la península italiana, en agradecimiento a su príncipe, tomaron el nombre de tirrenos.

La máxima autoridad internacional en materia de delimitación de mares, la Organización Hidrográfica Internacional, considera el mar Tirreno como una subdivisión del mar Mediterráneo. En su publicación de referencia mundial, "Limits of oceans and seas" (‘Límites de océanos y mares’, 3.ª edición de 1953), le asigna el número de identificación 28 (e) y define sus límites de la forma siguiente:



</doc>
<doc id="17213" url="https://es.wikipedia.org/wiki?curid=17213" title="Yoshito Usui">
Yoshito Usui

Yoshihito Usui nació el 21 de abril de 1958 en la ciudad de Shizuoka, prefectura de Shizuoka. En 1977, se graduó en la escuela secundaria de Saitama. Poco después, comenzó a trabajar en un supermercado durante el día, mientras que por las noches asistía a clases de diseño. En 1979, entró a trabajar en una empresa de publicidad, y en 1985 ganó el premio a dibujantes debutantes de la revista "Manga Action", lo que le impulsó a publicar el manga "Darakuya Store Monogatari".

En 1990, publicó el manga "Office Lady Gumi" y, en septiembre de ese mismo año, la obra que lo haría famoso a nivel internacional, "Shin-chan". Dicho manga siguió en curso hasta su muerte. Dos años más tarde, comenzó a emitirse una serie de televisión de "Shin-chan" a través de TV Asahi y a partir de ese año se estrenó en los cines japoneses la primera película sobre el manga.

Otras obras menos conocidas de Usui son "Mix Connection", "Scrambled Egg", "Super Mix" y "Unbalance Zone".

El 11 de septiembre de 2009, Usui desapareció después de haber ido a practicar senderismo a una zona montañosa en el monte Arafune, en la prefectura de Gunma. Antes de partir, Usui había asegurado a sus familiares que regresaría ese mismo día. Sin embargo no lo hizo. Su familia reportó su desaparición el 12 de septiembre. 

El 19 de septiembre, unos excursionistas avistaron un cuerpo desde lejos, pero no pudieron acceder a la zona. La noticia de la desaparición de Usui se expandió rápidamente cuando los excursionistas regresaron. La policía encontró un cuerpo cuyas ropas coincidían con las descritas por la familia del dibujante. El 20 de septiembre se rescató el cuerpo del barranco en cuyo fondo se había encontrado y tras las correspondientes pruebas, se confirmó que el cadáver era el de Usui. Su cámara fue recuperada y reveló que su última foto había sido tomada desde el borde del acantilado. Según la autopsia realizada, Usui probablemente murió el mismo día 11 o poco después, tras sufrir una caída de 120 metros.

Su funeral se llevó a cabo de forma privada el 23 de septiembre. Asistieron 3.000 personas.

Usui estaba casado y tenía dos hijas, quienes para el momento de su muerte ambas ya habían abandonado el hogar familiar.



</doc>
