<doc id="8125" url="https://es.wikipedia.org/wiki?curid=8125" title="Asma">
Asma

El asma es una enfermedad del sistema respiratorio caracterizada por una inflamación crónica de la vía aérea, cuyas manifestaciones clínicas son heterogéneas y variables en el tiempo y consisten en sibilancias, dificultad respiratoria, opresión torácica y tos.

El origen de la enfermedad es complejo e involucra la inflamación, la obstrucción intermitente y la hiperreactividad (incremento en la respuesta broncoconstrictora) de las vías respiratorias. La presencia de edema y secreción de mucosidad contribuye tanto con la obstrucción como con el aumento de reactividad. La enfermedad puede tener un curso agudo, subagudo o crónico, dependiendo de la frecuencia, duración e intensidad de sus manifestaciones clínicas.

Las manifestaciones del asma se dan en respuesta a numerosos estímulos desencadenantes tanto endógenos (internos a la persona), como exógenos (del ambiente). Los mecanismos subyacentes incluyen el estímulo directo sobre la musculatura lisa del árbol bronquial y el estímulo indirecto para que células propias secreten sustancias activas que producen la reacción inflamatoria y la broncoconstricción. Entre estos estímulos desencadenantes están la exposición a un medio ambiente inadecuado (frío, húmedo o con alérgenos), el ejercicio o esfuerzo y reacciones a alimentos u otras sustancias como consecuencia de un aumento de la permeabilidad intestinal. Enfermedades víricas y bacterianas de las vías respiratorias altas y el estrés emocional pueden empeorar los síntomas.

Los síntomas del asma son la respiración sibilante, la falta de aire, la opresión en el pecho y la tos improductiva durante la noche o temprano en la mañana. Estos síntomas se dan con distinta frecuencia e intensidad, intercalándose períodos asintomáticos donde la mayoría de los pacientes se sienten bien. Por el contrario, cuando los síntomas del asma empeoran, se produce una crisis de asma. Puede ser una crisis respiratoria de corta duración, o prolongarse con ataques asmáticos diarios que pueden persistir por varias semanas. En una crisis grave, las vías respiratorias pueden cerrarse impidiendo que los pulmones realicen su función de intercambio gaseoso (recibir oxígeno y expulsar el dióxido de carbono) al punto que los órganos vitales tampoco puedan funcionar. En esos casos, la crisis asmática puede provocar la muerte.

Existen múltiples alternativas terapéuticas para evitar los síntomas, controlarlos y aliviarlos, así como para la recuperación de crisis de asma, siendo su uso por medio de inhaladores lo más frecuente.


Basada en los patrones de obstrucción bronquial medida a través de aparatos de registro de tipo flujometría o espirometría:
Como fundamento en los niveles de control del paciente ya diagnosticado con asma:

El asma es una enfermedad frecuente que varía mucho de un país a otro. Afecta alrededor del 3 al 7 % de la población adulta, siendo más frecuente en edades infantiles. Es una de las más importantes enfermedades crónicas, es decir, de duración prolongada, en niños. Es más frecuente en el sexo masculino en una relación de 2:1, pero al llegar a la pubertad, esta relación tiende a igualarse. En los últimos veinte años se ha registrado un aumento en su incidencia debido en parte a la contaminación ambiental y las consecuencias de esta, y en parte al aumento de la población mundial. Por ejemplo, la Organización Mundial de la Salud reportó que un 8 % de la población suiza padecía de asma, comparado con solo 2 % hace 25-30 años.

La enfermedad tiene un fuerte componente hereditario, expresado como un antecedente familiar de rinitis, urticaria y eccema, por ejemplo. Sin embargo, muchos asmáticos no tienen antecedentes familiares que indiquen una asociación atópica. Hasta el momento no se ha demostrado ninguna de las hipótesis infecciosas propuestas como origen del cuadro.

Los niveles más elevados de asma mundial, de acuerdo con el "Global Initiative for Asthma" (GINA) en febrero de 2004 ocurrieron en aproximadamente 30 % de los niños en el Reino Unido, Nueva Zelanda y Australia o 20 % de los niños en el Perú, Nueva Zelanda y Australia (varía dependiendo del método de investigación usado para los cálculos) y aproximadamente 25 % de los adultos en Gran Bretaña, Australia y Canadá.

Hay un mayor porcentaje de fumadores y de enfermedades concomitantes alérgicas tales como rinitis, conjuntivitis y dermatitis entre los pacientes diagnosticados de asma alérgica que en otros pacientes.

El asma bronquial es común en personas jóvenes con una historia clínica de catarros recurrentes, o con antecedentes familiares asmáticos. Las crisis de asma se relacionan con el consumo de ciertos alimentos o la presencia de determinados agentes alérgenos.

Las causas que provocan el asma bronquial y motivan la respuesta de los mecanismos principalmente inmunológicos se clasifican en:

Existen varios factores de riesgo ambientales que están asociados al asma:


Además de por inhalación, el asma puede ser causada por reacciones a alimentos o sustancias que penetran a través del intestino. Por esta razón, el aumento de la permeabilidad intestinal parece jugar un papel clave. Se ha demostrado que aproximadamente el 40 % de las personas asmáticas tienen una mayor permeabilidad intestinal.

El intestino posee en su interior una capa de células que forman una barrera. Su misión es, además de digerir nutrientes, actuar defendiendo al organismo del enemigo exterior del ambiente (sustancias que ingerimos y microorganismos presentes en el intestino). Esto lo logra manteniendo cerradas las uniones estrechas intercelulares (los "poros" del intestino), para impedir el acceso descontrolado de sustancias, toxinas, químicos, microorganismos y macromoléculas, que de lo contrario podrían pasar al torrente sanguíneo. Cuando dichas uniones estrechas intercelulares no funcionan bien y en lugar de estar cerradas o prácticamente cerradas, como deberían estar, están abiertas sin control, se produce un aumento de la permeabilidad intestinal ("poros" demasiado abiertos). Esta apertura provoca que entren sustancias en el cuerpo y que, dependiendo de la predisposición genética de la persona, puedan desarrollarse enfermedades autoinmunes, inflamatorias, infecciones, alergias, asma o cánceres, tanto intestinales como en otros órganos.

Los dos factores más potentes que provocan aumento de la permeabilidad intestinal son ciertas bacterias intestinales y la gliadina (principal fracción tóxica del gluten), independientemente de la predisposición genética, es decir, tanto en celíacos como en no celíacos. Otras posibles causas son la prematuridad y la exposición a radiación, quimioterapia o ciertas toxinas.

Se ha asociado a más de 100 genes con el asma, por lo menos en un estudio genético. Aunque en el estudio aún se necesita añadir un componente de replicación genética, para el 2005, unos 25 genes se habían asociado con el asma en seis o más poblaciones diferentes, entre ellos GSTM1, IL-10, IL-4, IL-13, CTLA-4, CD14, TNF y el receptor β-2 adrenérgico (ADRB2).

Muchos de estos genes se relacionan con el sistema inmunitario o modulan los procesos de la inflamación. Sin embargo, a pesar de esta lista de genes y su posible asociación con el asma, los resultados no son del todo consistentes entre las diferentes poblaciones, es decir, estos genes no se asocian al asma bajo todas las condiciones, sino que la causa genética del asma es una interacción compleja de diversos factores adicionales.

La patogenia del asma es compleja, pero incluye tres componentes fundamentales:


Aunque se conoce que el asma es una condición causada por una inflamación (del latín, "inflammatio", encender, hacer fuego) aguda, subaguda o crónica de las vías aéreas, la presencia de edema o secreciones contribuye tanto con la obstrucción como con la hiperreactividad. Muchas células inflamatorias están implicadas en esta inflamación: mastocitos, eosinófilos, células epiteliales, macrófagos y linfocitos T activados. La activación de estas células y la subsecuente producción de mediadores inflamatorios puede que sean de mayor relevancia que la sola presencia de estas células en las vías aéreas.

Este proceso de inflamación crónica puede que conduzca a cambios estructurales, tales como la fibrosis, el engrosamiento o hipertrofia del músculo liso bronquial, hipertrofia de las glándulas y la angiogénesis lo que puede dar lugar a una obstrucción irreversible de la vía aérea.
Estudios iniciales en pacientes que fallecían de "status asmaticus" reveló una marcada inflamación del árbol bronquial con estudios histológicos de las vías afectadas confirman la presencia de células inflamatorias de larga data.

El uso del lavado bronquioalveolar ha demostrado que los mastocitos juegan un importante papel como mediadores de la respuesta inmediata al alérgeno, la inflamación de las vías aéreas y la hiperreactividad inducida por el esfuerzo y el relacionado con alimentos. Otras células responsables de la inflamación crónica que provocan la hiperreactividad bronquial son los macrófagos, los eosinófilos y linfocitos T, todos ellos aparecen activados en el asma, incluso en pacientes asintomáticos. Una línea de linfocitos T, las células T2, están programadas para producir ciertas citoquinas que conducen a la característica inflamación eosinofílica. Esta programación comprende a células presentadoras de antígeno, tales como las células dendríticas en el epitelio de las vías aéreas. Otros mediadores de la inflamación producen reacciones tales como congestión vascular, edema, aumento de la producción de moco, etc.

Sobre el epitelio bronquial se demuestra una expresión aumentada del complejo mayor de histocompatibilidad tipo II y otras células infiltrantes como los monocitos. La severidad clínica del asma tiene una cercana correlación con la severidad de la respuesta inflamatoria, lo que sugiere una complicada interacción entre estas células y los mediadores que generan, como la IL-3, IL-5 y el GM-CSF. Por ejemplo, los macrófagos alveolares de pacientes asmáticos producen dos veces más GM-CSF que en sujetos no asmáticos. La IL-4 y el Interferón gamma son elementales en la biosíntesis de IgE.

La obstrucción al flujo aéreo se produce por varios factores: broncoconstricción aguda, edema de la vía aérea, formación de tapones mucosos crónicos y remodelamiento de la vía aérea. La broncoconstricción se produce por la liberación de mediadores dependientes de la inmunoglobulina E al exponerse a un alergeno aéreo y es el primer componente de la respuesta asmática temprana. El edema se produce entre 6 a 24 horas después del contacto con el alergeno, y su aparición se corresponde con la respuesta asmática tardía. La formación de tapones mucosos ocurre por la acumulación de exudado rico en proteínas y residuos celulares, y toma semanas en desarrollarse.La remodelación de la vía aérea se asocia con cambios estructurales del árbol bronquial debido a la inflamación mantenida en el tiempo y afecta en forma muy importante la reversibilidad de la obstrucción.

Los cambios morfológicos vistos en el asma han sido descritos principalmente a partir de muestras de pacientes fallecidos por "status asmaticus", es decir, un síndrome agudo de asma, sin embargo, parece que la histopatología en casos no mortales es muy similar. Macroscópicamente, los pulmones se ven distendidos debido a una sobreinflación y pueden aparecer zonas con atelectasia. El hallazgo más notorio es la oclusión de los bronquios y bronquiolos por gruesos tapones de moco.

Bajo el microscopio, los tapones mucosos contienen espirales de células epiteliales formando los llamados espirales de Curschmann, que pueden habitualmente ser observados en el esputo de pacientes asmáticos. Además, el moco contiene cristales de Charcot-Leyden y eosinófilos. Otra característica histológica del asma incluye un engrosamiento de la membrana basal del epitelio bronquial, edema y un infiltrado sobre las paredes bronquiales con prominencia de eosinófilos e hipertrofia del músculo de la pared bronquial así como de las glándulas submucosas, reflejando una broncoconstricción prolongada.

En el lavado bronquial de pacientes asmáticos, se observan un número incrementado de células inflamatorias, incluyendo eosinófilos, macrófagos y linfocitos en comparación con pacientes no asmáticos, aún en pacientes asmáticos con funciones pulmonares normales y asintomáticos —incluyendo pacientes con asma alérgica como con asma no alérgica.

Los síntomas más característicos del asma bronquial son la disnea o dificultad respiratoria de intensidad y duración variable y con la presencia de espasmos bronquiales, habitualmente acompañados de tos, secreciones mucosas y respiración sibilante. También se caracteriza por la rigidez torácica en el paciente y su respiración sibilante. En algunos pacientes estos síntomas persisten a un nivel bajo, mientras que en otros, los síntomas pueden ser severos y durar varios días o semanas. Bajo condiciones más graves, las funciones ventilatorias pulmonares pueden verse alteradas y causar cianosis e incluso la muerte.

La evolución de la reacción asmática constituye un proceso cuyas principales fases son:

Cuando el asma o sus síntomas, como la tos, no mejoran, es posible que se deba a:

El diagnóstico del asma comienza con una evaluación del cuadro clínico, la historia familiar y antecedentes de riesgo o crisis anteriores tomando en consideración el tiempo de evolución del cuadro y las crisis. La mayoría de los casos de asma están asociados a condiciones alérgicas, de modo que diagnosticar trastornos como rinitis y eczema conllevan a una sospecha de asma en pacientes con la sintomatología correcta: tos, asfixia y presión en el pecho. El examen físico por lo general revela las sibilancias que caracterizan al asma. Es importante evaluar si el paciente ya recibió algún tratamiento antes de la consulta médica, así como los eventos desencadenantes de la crisis.

El examen físico es vital, a veces no da tiempo examinar con detalle, sin embargo, se perciben las características sibilancias a la auscultación. En algunos casos severos, la broncoobstrucción es tal que se presenta un silencio auscultatorio, sin embargo, el resto de la clínica es tan florida por la incapacidad respiratoria que el diagnóstico no amerita la percepción de sibilancias para el tratamiento de estas crisis grave de asma. Esa angustia y sed de aire puede indicar una crisis grave que amerita tratamiento de rescate inmediato para revertir el broncoespasmo antes de continuar con el examen físico detallado. La inspección del tórax puede mostrar tiraje o retracción subcostal o intercostal. Las dimensiones del tórax en pacientes asmáticos crónicos varían en comparación con niños no asmáticos, característico el aplanamiento costal. La inspección puede revelar detalles al diagnóstico, como el descubrimiento de dermatitis alérgicas, conjuntivitis, etc.

Existen varios exámenes que ayudan al diagnóstico del asma, entre ellas pruebas de función pulmonar, pruebas de alergia, exámenes de sangre, radiografía del pecho y senos paranasales.

Los exámenes de la función pulmonar incluyen:

La radiografía de tórax permite identificar algunas características en los órganos torácicos que se relacionan con el asma bronquial, así como para confirmar o descartar otras enfermedades asociadas. De esta forma, el examen se indica cuando el paciente debuta; a todo niño asmático conocido que tenga síntomas súbitos para descartar la posibilidad de un cuerpo extraño, laringotraqueobronquitis, neumonía u otras patologías; cuando la respuesta al tratamiento sea mala; cuando se ausculten sonidos agregados además de las sibilancias y cuando el paciente tenga un cambio en el comportamiento de sus crisis de asma.

Las imágenes de atrapamiento de aire tienden hacia los espacios intercostales lo que horizontaliza las costillas en la radiografía. Como hay edema, se puede apreciar un infiltrado rodeando los hilios pulmonares bilaterales que no es más que edema peribronquial con tal contenido mucoso que se visualiza en la radiografía como aumento de la trama bronquiovascular.

El tratamiento convencional del asma bronquial puede ser:

El tratamiento debe incluir la identificación de los elementos que inicien la crisis, tales como polenes, ácaros, pelos de mascotas o la aspirina y limitando o, de ser posible, eliminando la exposición a dichos factores. Si resulta insuficiente evitar los factores estimulantes, entonces se puede recurrir al tratamiento médico. La desensitización (proceso gradual por el que se elimina la respuesta a un estímulo mediante la repetición del estímulo hasta que no se produce más respuesta) es, por el momento, la única "cura" disponible para esta enfermedad. Otras formas de tratamiento incluyen el alivio farmacológico, los medicamentos de prevención, los agonistas de larga acción de los receptores β2, y el tratamiento de emergencia.

El tratamiento farmacológico específico recomendado para pacientes con asma depende en la severidad de su enfermedad y la frecuencia en la aparición de los síntomas. Los tratamientos específicos para el asma se clasifican "grosso modo" en medicinas preventivas y de emergencia. El reporte EPR-2 (por sus siglas en inglés "Expert Panel Report 2"), un protocolo para el diagnóstico y manejo del asma, así como el reporte de otras sociedades internacionales son usados y apoyados por muchos médicos. 

La kinesiología respiratoria KTR se indica como pilar central en el tratamiento.

Los broncodilatadores se recomiendan para el alivio a corto plazo en prácticamente todos los pacientes con asma. Para quienes tienen solo ataques ocasionales, no se necesita otro tipo de medicamento. Para quienes tienen una persistencia de los síntomas de manera moderada, es decir, más de dos crisis por semana, se sugieren glucocorticoides inhalados de baja concentración o, alternativamente, se puede administrar un modificador de leucotrienos oral, un estabilizador de la membrana de los mastocitos o la teofilina. Para los individuos que presenten crisis diarias, se sugiere una dosis más elevada de glucocorticoide en conjunto con agonistas β-2 de larga acción inhalados o bien un modificador de los leucotrienos o la teofilina, pueden sustituir al agonista β-2. En los ataques asmáticos severos, se puede añadir glucocorticoides orales a estos tratamientos durante las crisis graves.

El descubrimiento en el año 2006 de que el asma puede ser causado por la sobreproliferación de un tipo especial de linfocito NK puede conllevar últimamente al desarrollo de un mejor y más específico grupo de medicamentos. Los linfocitos T del grupo NK parece ser resistente a los corticosteroides, una de las principales líneas de tratamiento actual. Otras prometedoras opciones en estadios de investigación incluyen el uso de estatinas, que son medicamentos que disminuyen el nivel de colesterol en el plasma sanguíneo y el uso de suplementos con aceite de pescado, para reducir la inflamación en las vías respiratorias.

Se ha identificado que el tratamiento de corticosteroides tiene una mejor eficacia que el tratamiento con fármacos antagonistas del receptor de leucotrieno.

Los nebulizadores proveen una dosis más continua y duradera al vaporizar la medicina diluida en solución salina, el cual el paciente inhala hasta que se administra la dosis completa. No hay evidencias de que sean más efectivas que un "spacer". El alivio de crisis asmáticas incluye medicamentos:


Los broncodilatadores de acción prolongada tienen una estructura molecular muy similar a los agonistas β de corta duración, pero tienen cadenas laterales más largas lo que resulta en un efecto de 12 horas de duración, de modo que son usados para el alivio sintomático durante la noche. A pesar de que los individuos que usan estos medicamentos reportan una mejora en el control de los síntomas, no son medicamentos que sustituyen el requerimiento de preventivos rutinarios, además que por tardar en surtir efecto se puede hacer necesario el uso de dilatadores de acción corta. En 2005, la Administración de Drogas y Alimentos estadounidense escribió un reporte alertando al público que se ha notado que el uso de agonistas β de acción prolongada puede producir un empeoramiento de los síntomas asmáticos y algunos casos de muerte súbita.

Algunos de los agonistas β de larga duración disponibles en el mercado incluyen el salmeterol, formoterol, babmuterol y una preparación oral de albuterol. Las combinaciones de estos agonistas β de acción prolongada junto con esteroides inhalados se han vuelto más comunes, la más frecuente de ellas es la combinación de salmeterol y fluticasona.

La inmunoterapia con alérgenos específica (vacunas) ha demostrado su utilidad en el manejo del asma bronquial en atención primaria de salud (APS) al ser el único tratamiento capaz de cambiar el curso natural de la enfermedad. Su mayor efectividad es en las formas intermitentes y persistentes leves y moderadas.

La inmunoterapia sublingual ha demostrado ser un tratamiento efectivo en niños con rinitis y asma causadas por sensibilización a alérgenos estacionales (como la alergia al polen) y actualmente su uso en varios países europeos supera al de la inmunoterapia subcutánea clásica. En el caso de las sensibilizaciones provocadas por alérgenos perennes (que se manifiestan durante todo el año, tales como la alergia a los ácaros del polvo, pelo de animales, etc.), las evidencias no son concluyentes y son precisos más estudios para demostrar la eficacia de la inmunoterapia sublingual.

La inmunoterapia con alérgenos, administrada tanto por vía subcutánea como por vía sublingual, está absolutamente contraindicada en niños menores de dos años, personas con asma incontrolada o mal controlada, pacientes con enfermedades autoinmunes activas y enfermos de sida. Asimismo, está absolutamente contraindicado su inicio durante el embarazo, pero podría continuarse un tratamiento preexistente bien tolerado, con precaución y bajo supervisión médica. En niños con edades comprendidas entre los dos y los cinco años, la inmunoterapia con alérgenos solo está indicada en casos concretos.

El control de los síntomas durante las crisis de asma incluye la reducción de las sibilancias y la dificultad respiratoria, lo cual, por lo general ocurre eficazmente con el uso de broncodilatadores de acción rápida. Se acostumbra proveer estos medicamentos en la forma de inhaladores portátiles de dosis medida. En pacientes más jóvenes, para quienes les resulte difícil la coordinación de los inhaladores, o quienes encuentren difícil sostener su respiración por los 10 segundos después de la inhalación, como las personas ancianas, se puede recomendar el uso de un spacer, que es un cilindro plástico que mezcla el medicamento con el aire en un solo tubo, haciendo que sea más fácil para el paciente recibir una dosis completa de la medicina y permite que el agente activo se disperse en porciones más reducidas e inhalables.

Estos son casos relativamente leves e intermitentes con síntomas o crisis ocurriendo menos de 1 vez por semana o con síntomas nocturnos unas 2 veces por mes, una presión parcial de CO de 35 a 45 mmHg, una presión parcial de O de 80 mmHg y una FEV de 800 ml y con un pico de flujo >50 % del valor estipulado. Son casos que, durante una crisis, se recomienda nebulizar con 3-5 ml de solución fisiológica con uno de los siguientes broncodilatadores, con una frecuencia de cada 4 a 6 horas:
Si no mejora se puede administrar terbutalina subcutánea cada hora por 2 dosis o en infusión intravenosa.

Estos son casos relativamente leves, pero persistentes con síntomas o crisis ocurriendo más de 1 vez por semana, pero menos que 1 vez por día o con síntomas nocturnos más de 2 veces por mes, una presión parcial de CO de 35 mmHg, una presión parcial de O entre 60 y 80 mmHg y una FEV entre 300-800 ml y con un pico de flujo <50 % del valor estipulado. Son casos que, durante una crisis, se recomienda nebulizar igual que el estadio I y si no mejora se utiliza:

Estos son casos moderados que requieren el uso de beta-miméticos a diario o con alteración de su actividad física a diario y síntomas nocturnos más de 1 vez por semana, una presión parcial de CO de 40-45 mmHg, una presión parcial de O menor de 60 mmHg y con un pico de flujo <30 % del valor estipulado. Son casos que, durante una crisis, se recomienda nebulización y aminofilina igual que el estadio II y si no mejora se utiliza:

Estos son pacientes en franca insuficiencia respiratoria, cianosis, tórax silencioso a la auscultación y un esfuerzo inspiratorio débil. El tratamiento suele ser similar al estadio III con intubación endotraqueal y ventilación mecánica.

En la mayoría de los casos de asma, la enfermedad produce intervalos de dificultad respiratoria y asfixia, lo cual puede ser desalentador o incluso discapacitante para el paciente y no mortal. Con el tratamiento adecuado y la observancia de las recomendaciones terapéuticas, los pacientes con asma pueden mantener una vida productiva. Ocasionalmente, la enfermedad desaparece espontáneamente. En sus formas más graves, la hiperinflación pulmonar puede progresar en el tiempo hasta eventualmente causar enfisema. Las infecciones bacterianas superimpuestas al asma pueden conllevar a bronquitis crónica, bronquiectasis o neumonía. En algunos casos menos frecuentes, especialmente en pacientes adultos, el asma no controlada puede producir cor pulmonale e insuficiencia cardíaca.
Actuar sobre la causa del asma si es de origen alérgico, llevando a cabo el desalojo (separación) del alergeno, ya sea por intentar una desensibilización al alérgeno. Los resultados son buenos con ácaros, pólenes y menos buenos con animales; con el fin de prevenir las convulsiones, los asmáticos pueden utilizar dispositivos (pico-flujo) para medir su pico espiratorio velocidad de flujo, control de la obstrucción bronquial y por lo tanto adaptar su tratamiento con el resultado obtenido (broncodilatador decisión de acción por ejemplo, ayuno o modificación del tratamiento de fondo en colaboración con el médico). Por lo tanto, es muy importante desarrollar una educación terapéutica del paciente.

Según varios ensayos clínicos, un aumento en el consumo de frutas, verduras y cereales ayuda a detener la progresión del asma. En el caso de cambiar a una dieta vegetariana, el 71 % de los sujetos informó de una reducción significativa en los síntomas después de 4 meses y 92 % después de un año. Los suplementos de antioxidantes (vitamina A, C, E), vitamina B8 o ácidos grasos poliinsaturados (omega 3 y 6), sin embargo, no tienen un efecto probado.




</doc>
<doc id="8126" url="https://es.wikipedia.org/wiki?curid=8126" title="Ecuación de Starling">
Ecuación de Starling

Formulada en 1896, por el fisiólogo británico Ernest Starling, la ecuación de Starling ilustra el rol de las fuerzas hidrostáticas y oncóticas (llamadas también fuerzas de Starling) en el movimiento del flujo a través de las membranas capilares. Permite predecir la presión de filtración neta para un determinado líquido en los capilares. La ecuación es:

siendo:

Todas las presiones son medidas en milímetros de mercurio (mm Hg), y el coeficiente de filtración se mide en mililitros por minuto por milímetros de mercurio (mL·min·mm Hg). Por ejemplo:

Según la ecuación, P(Q)arteriolar=(37-1)-(25-0)=11 y P(Q) venular= (17-0)-(25-0)= -9. La filtración es por lo tanto mayor que la reabsorción. La diferencia es recuperada entonces por el sistema linfático para retornar a la circulación.

La solución a la ecuación es el flujo de agua desde los capilares al intersticio (Q). Si es positiva, el flujo tenderá a dejar el capilar (filtración). SI es negativo, el flujo tenderá a entrar al capilar (reabsorción). Esta ecuación tiene un importante número de implicaciones fisiológicas, especialmente cuando los procesos patológicos alteran de forma considerable una o más de estas variables.


</doc>
<doc id="8130" url="https://es.wikipedia.org/wiki?curid=8130" title="Editor de texto">
Editor de texto

Un editor de texto es un programa informático que permite crear y modificar archivos digitales compuestos únicamente por textos sin formato, conocidos comúnmente como archivos de texto o “texto plano”. El programa lee el archivo e interpreta los bytes leídos según el código de caracteres que usa el editor. Es comúnmente de 7- u 8-bits en ASCII o UTF-8, rara vez EBCDIC.

Por ejemplo, un editor ASCII de 8 bits que lee el número binario codice_1 (decimal 97 o hexadecimal 61) en el archivo lo representará en la pantalla por la figura codice_2, que el usuario reconoce como la letra "a" y ofrecerá al usuario las funciones necesarias para cambiar el número binario en el archivo.

Los editores de texto son incluidos en el sistema operativo o en algún paquete de software instalado, y se usan cuando se deben crear o modificar archivos de texto como archivos de configuración, lenguaje de programación interpretado ("scripts") o el código fuente de algún programa.

El archivo creado por un editor de texto incluye por convención en DOS y Microsoft Windows la extensión ".txt", aunque pueda ser cambiada a cualquier otra con posterioridad. Tanto Unix como Linux dan al usuario total libertad en la denominación de sus archivos.

Al trasladar archivos de texto de un sistema operativo a otro se debe considerar que existen al menos dos convenciones diferentes para señalar el término de una línea o lo que es lo mismo una "nueva línea": Unix y Linux usan solo retorno de carro en cambio Microsoft Windows utiliza retorno de carro y salto de línea.

Los editores de textos "planos" se distinguen de los procesadores de texto en que se usan para escribir solo texto, sin formato y sin imágenes, es decir sin diagramación.



Hay una gran variedad de editores de texto. Algunos son de uso general, mientras que otros están diseñados para escribir o programar en un lenguaje. Algunos son muy sencillos, mientras que otros tienen implementadas gran cantidad de funciones.
El editor de texto debe ser considerado como una herramienta de trabajo del programador o administrador de la máquina. Como herramienta permite realizar ciertos trabajos, pero también requiere de aprendizaje para que el usuario conozca y obtenga destreza en su uso. La llamada "curva de aprendizaje" es una representación de la destreza adquirida a lo largo del tiempo de aprendizaje. Un editor puede ofrecer muchas funciones, pero si su curva de aprendizaje es muy larga, puede desanimar el aprendizaje y terminará siendo dejado de lado. Puede que un editor tenga una curva de aprendizaje muy empinada y corta, pero si no ofrece muchas funciones el usuario le reemplazará por otro más productivo. Es decir la elección del editor más apropiado depende de varios factores, alguno de ellos muy subjetivos. Esta coyuntura de intereses ha dado lugar a largas discusiones sobre la respuesta a la pregunta: ¿cuál es el mejor editor de texto? Muchos editores originalmente salidos de Unix o Linux, han sido portados a otros sistemas operativos, lo que permite trabajar en otros sistemas sin tener que aprender el uso de otro editor.

Algunos editores son sencillos mientras que otros ofrecen una amplia gama de funciones.

Editores para profesionales deben ser capaces de leer archivos de gran extensión, mayor que la capacidad de la memoria de acceso aleatorio de la máquina y también arrancar rápidamente, ya que el tiempo de espera disminuye la concentración y disminuye de por sí la productividad. Los editores de texto sirven para muchas cosas porque facilitan el trabajo.

Algunos editores de texto incluyen el uso de lenguajes de programación para automatizar engorrosos o repetidos procedimientos a realizar en el texto. Por ejemplo, Emacs puede ser adaptado a las necesidades del usuario, incluso las combinaciones de teclas para ejecutar funciones pueden ser adaptadas y es programable en Lisp.

Muchos editores de texto incluyen coloreado de sintaxis y funciones que ofrecen al usuario completar una palabra iniciada usando para ello la configuración.

Algunas funciones especiales son:

Es la función que marca, visualmente o no, una parte del texto para ser elaborada con otras funciones. La región puede contener varias líneas del texto (región horizontal) o bien varias columnas adyacentes del texto (región vertical).

El proceso de búsqueda de una palabra o una cadena de caracteres, en un texto plano y su reemplazo por otra. Existen diferentes métodos: global, por región, reemplazo automático, reemplazo con confirmación, búsqueda de texto o búsqueda de una expresión regular.

Sirve para copiar, trasladar o borrar una región marcada.

Los editores de texto permiten automatizar las únicas funciones de formateo que utilizan: quebrar la línea, indentar, formatear comentarios o formatear listas.

Agregar o insertar el contenido de un archivo en el archivo que se está editando. Algunos editores permiten insertar la salida o respuesta a un programa cualquiera ejecutado en la línea de comandos al archivo que se está editando.

Algunos editores de texto permiten hacer pasar las líneas del texto o de una región por algún programa para modificarlas u ordenarlas. Por ejemplo, para ordenar alfabéticamente una lista de nombres o sacar un promedio de una lista de números.

Un editor para trabajar en la administración de una red de computadoras debe ofrecer la funcionalidad de editar archivos en máquinas remotas, ya sea por medio del "File Transfer Protocol" (FTP), "Secure Shell" (SSH) o algún otro protocolo de red. Emacs lo puede hacer mediante el "plugin" tramp (ampliamente configurable con SSH, FTP, SCP, SFTP, etcétera), Ultraedit, del ambiente Windows, lo hace mediante FTP.




</doc>
<doc id="8134" url="https://es.wikipedia.org/wiki?curid=8134" title="Kanji">
Kanji

Los son los sinogramas utilizados en la escritura del idioma japonés. Los "kanji" son uno de los tres sistemas de escritura japoneses junto con los silabarios "hiragana" y "katakana", para los que existen reglas generales a la hora de combinarlos, pues cada uno tiene una función diferente.

Los "kanji" se usan en su mayoría para expresar conceptos, a diferencia del chino, donde pueden emplearse también en su carácter fonético. Así mismo existen combinaciones de "kanjis" que no obedecen a su significado original y que solo se toman por su valor fonético. A estas combinaciones se les denomina «ateji» (当て字). Como ejemplos tenemos «ofuro» (お風呂, baño) y «sewa» (世話, cuidado, atención). En todo caso, dichas combinaciones fonéticas no son tan frecuentes como en el idioma chino.

A cada "kanji" le corresponde un significado y se usa como determinante de la raíz de la palabra; las derivaciones, conjugaciones y accidentes se expresan mediante el "kana" (sobre todo "hiragana"), que en dicho caso se denomina "okurigana". De esta forma, conviven tanto el sistema de escritura autóctono (pero sacado de la misma escritura "han") y el sistema importado (es decir, los "kanji").

El japonés mantiene prácticamente inalterados los "kanjis", al igual que Taiwán, Hong Kong y Macao, aunque hubo una simplificación en algunos kanjis complejos. El cambio no fue tan profundo como en China continental, donde fueron oficializadas las versiones simplificadas durante la Revolución Cultural de los años 1960 de alrededor de 2700 caracteres. Sin embargo, las simplificaciones responden en muchos casos a usos comunes en práctica durante siglos, o un retorno a versiones antiguas de los caracteres.

Los Caracteres chinos llegaron por primera vez a Japón en los sellos oficiales, cartas, espadas, monedas, espejos y otros artículos de decoración importados de China. El ejemplo más antiguo conocido de una importación tal era el sello de oro del rey de Na dado por el Emperador de Han Guangwu a un emisario Yamato en el 57 d. C. Monedas chinas desde el siglo I d.C. se han encontrado en el período Yayoi de sitios arqueológicos. Sin embargo, los japoneses de la época probablemente no tenían ninguna comprensión de la secuencia de instrucciones, y se mantendrían analfabetos hasta el siglo V d.C. Según el Nihon Shoki y Kojiki, un erudito semilegendario llamado Wani (王仁) fue enviado a Japón por el Reino de Baekje durante el reinado del emperador Ōjin a principios del siglo V, trayendo consigo el conocimiento del confucianismo y los caracteres chinos.

Un "kanji" puede tener diferentes pronunciaciones, o «lecturas», dependiendo del contexto, uso en combinación y su localización en la oración. Estas lecturas son categorizadas dependiendo de si provienen del chino original, o si fueron adaptadas a la lengua nativa . La mayor parte de los "kanjis" poseen dos lecturas, una de "kun'yomi" y otra de "on'yomi", con sus alteraciones fonéticas occidentales, pero algunos "kanjis" (muchos de ellos de uso diario) tienen diez o más posibles lecturas.

La manera en que se lee un "kanji" depende del contexto y de dos reglas generales (en las cuales hay muchas excepciones):


A principios del siglo XX tuvieron lugar debates sobre la conveniencia de reformas ortográficas, pero fueron bloqueadas por el poder en turno, y no fue sino después de la Segunda Guerra Mundial cuando dichas reformas pudieron llevarse a cabo.

En primer lugar se reformaron los dos silabarios ("kana", es decir vago y raro ), para que su escritura fuera solo fonética, conforme a la pronunciación actual del japonés. El resultado es que toda frase japonesa puede escribirse fonéticamente usando únicamente uno de los dos silabarios, los que cada sílaba posee un solo sonido y cada sonido se representa por una sola sílaba. Únicamente hubo dos excepciones: Dos partículas monosilábicas (へ y は) que son elementos gramaticales japoneses. Dichas partículas, que normalmente se leen "he" y "ha" (con la pronunciación del inglés como en 'head' o 'halloween'), se pronuncian "e" y "wa" cuando actúan como elementos gramaticales (助詞 - joshi)

También hubo una reforma de los "kanjis" y de su empleo. En 1946 se editó una lista de "kanjis" de uso corriente, los llamados , que comprendía 1850 caracteres. En 1948 se determina que 881 de ellos deben ser conocidos al salir de los seis primeros años escolares, al tiempo que se redujo el número de lecturas de muchos de ellos.

En 1949 se simplificó la forma de varios "kanjis" dando origen a los "Shinjitai 新字体" (Literalmente: "Nueva forma del carácter).

En 1951 la lista de los "Tōyō kanji" aumentó en 92 "kanjis", pudiendo ser utilizados para nombres propios.

Sin embargo, el número de caracteres, 1942, es juzgado inadecuado por un gran número de japoneses, así como muchas de las elecciones que fueron hechas, pues muchos había incluidos caracteres de uso rarísimo, en tanto que no se encontraban algunos de uso cotidiano, como «almohada» o «año nuevo». Entre 1973 y 1980 se hicieron varias adiciones, y finalmente en 1981, el Ministerio de Educación publicó una nueva lista de kanjis «comunes» llamados que contiene un total de 1.945 kanji.

En abril de 1990 salió una lista de los , una lista de 284 caracteres suplementarios aceptados por el juzgado civil para los nombres y apellidos.

Los primeros 1006 kanjis que aprenden los japoneses a lo largo de los seis años de primaria y que son parte de la lista de los "jōyō kanji", son los , y están divididos precisamente por el año de aprendizaje en la .

Fuera de la lista oficial de los "kanjis" de uso común, existen otros muchos empleados en campos especializados (medicina, ingeniería, filosofía...) o bien para los nombres y apellidos de las personas y lugares. Un buen diccionario de "kanjis" tiene en repertorio más de 4000. El estándar JISX0208 actualizado en 1990 define un conjunto de caracteres informáticos de 6879 caracteres, de los cuales 6335 están repartidos en dos bloques. El primero incluye 2965 "kanjis", ordenados por el orden de aparición más frecuente; un segundo bloque incluye 3390 "kanjis" ordenados por radical y por número de trazos. Ese mismo año salió el estándar JISX0212, que definía un conjunto de caracteres suplementarios para usar junto con el precedente y que comprende 6067 caracteres suplementarios, de los cuales 5081 son "kanjis".

Cabe mencionar que actualmente una computadora con un soporte de japonés instalado tiene a disposición no menos de 11.436 "kanji" diferentes. Estos pueden ser convertidos automáticamente del hiragana, o buscarse ya sea por el número total de trazos que los componen, o bien por un radical más el número de trazos adicionales.

El conocimiento de un gran número de "kanjis" es un símbolo cultural y de erudición; los profesores de literatura llegan a conocer hasta diez mil "kanjis". En las publicaciones oficiales, los "kanjis" no oficiales deben ir acompañados de una guía de lectura, que son pequeños caracteres en hiragana o katakana, ya sea en la parte superior en el caso de la escritura de tipo occidental o en el costado derecho en la escritura oriental. Esta práctica se conoce como furigana.

El estudio de los "kanjis" requiere esfuerzo y práctica constante, pues para cada "kanji" hay que memorizar:




Solo tras haber estudiado o bien buscado en el diccionario, sabremos que la manera correcta de escribir estas tres palabras es

En efecto, aunque las tres palabras contienen la sílaba "sha", que fonéticamente es idéntica, en el momento de escribirla y sabiendo el significado de la palabra es posible determinar el uso del "kanji" adecuado.

Asimismo, el número relativamente limitado de sílabas en el sistema fonético japonés hace que este sea un idioma rico en homófonos, palabras cuya pronunciación es la misma pero con diferente significado. En una conversación, dicho significado se deduce del contexto, pero al leer, los "kanjis" son aún más útiles para discernir el verdadero significado. Por ejemplo, せいこう (seikou), que puede significar éxito (成功), siderurgia (製鋼) o acto sexual (性交), entre otros, dependiendo del "kanji" que se utilice.

Como se ha mencionado anteriormente, se denomina a la práctica de asociar un "kanji" a palabras que no tienen caracteres asociados, utilizando los ideogramas fonéticamente, es decir, tomando su sonido, sin ninguna relación semántica.

Existen varios vocablos del japonés que usan ateji, tales como (significa "mucho" – los kanjis significan pantano y montaña), (significado: "tonto" – kanjis: caballo y ciervo) o ("de todos modos" – kanjis: conejo y ángulo).

Se emplea igualmente representar con kanji palabras extranjeras, en palabras como , . Sin embargo, es más común usar el silabario katakana para este tipo de palabras.

Diversos países también tienen asociados a su nombre "ateji" que asemejan su pronunciación (si bien dicha pronunciación es la que corresponde al idioma inglés y no necesariamente a la del idioma que se habla en dicho país). Por ejemplo, , , , o . Asimismo es una práctica común el uso del "kanji" inicial como abreviatura de países. Así es que, por ejemplo, en una lista de películas que están siendo proyectadas, probablemente se indique el país de origen de la película mediante el primer "kanji" del ateji: 独 para , 仏 para , 英 para el , etc.

Sin embargo, es importante remarcar que, exceptuando textos en extremo formales, lo más corriente es utilizar katakana para escribir nombres de países.

Aparte de la escritura de idiomas del este asiático, los "kanjis" tienen poco o nulo uso en lugares donde domina el alfabeto latino. Popularmente, y por la complejidad de sus trazos y lo críptico que pueden llegar a ser, se emplean como ornamentos, los cuales incluso alcanzan la cultura del tatuaje. Con base en la estética de los caracteres, también se han utilizado como proyectos de diseño en las escuelas de la materia.




</doc>
<doc id="8136" url="https://es.wikipedia.org/wiki?curid=8136" title="Ángulo recto">
Ángulo recto

Un ángulo recto es aquel que mide 90 (sexagesimales). Su amplitud medida en otras unidades es: π/2 radianes y 100 (centesimales). Sus dos lados son dos semirrectas perpendiculares, y el vértice es el origen de dichas semirrectas.

Euclides lo define de este modo: «Cuando una línea recta que está sobre otra hace que los ángulos adyacentes sean iguales, cada uno de los ángulos es recto, y la recta que está sobre la otra se llama  perpendicular a la otra recta».

Los ángulos rectos se encuentran en muchas figuras geométricas planas, por ejemplo:



</doc>
<doc id="8137" url="https://es.wikipedia.org/wiki?curid=8137" title="Radián">
Radián

El radián (símbolo: rad) es una unidad de ángulo en el plano en el Sistema Internacional de Unidades. El radián mide el ángulo presentado como central a una circunferencia y su medida es igual a la razón entre la longitud del arco que comprende de dicha circunferencia y la longitud del radio, es decir, mide la cantidad de veces que la longitud del radio cabe en dicho arco. Hasta 1995 tuvo la categoría de unidad suplementaria en el Sistema Internacional de Unidades, junto con el estereorradián. A partir de ese año, y hasta el momento presente, ambas unidades figuran en la categoría de unidadesderivadas. 

Esta unidad se utiliza primordialmente en física, cálculo infinitesimal, trigonometría, goniometría, etc.

Un radián es la unidad de medida de un ángulo con vértice en el centro de un círculo cuyos lados son cortados por el arco de la circunferencia, y que además dicho arco tiene una longitud igual a la del radio.
El ángulo formado por dos radios de una circunferencia, medido en radianes, es igual a la longitud del arco que delimitan los radios dividida entre el radio; es decir, "θ" = "s"/"r", donde "θ" es el ángulo, "s" es la longitud de arco, y "r" es el radio. Por tanto, el "ángulo completo", formula_1, que subtiende una circunferencia de radio r, medido en radianes, es:

El radián es una unidad sumamente útil para medir ángulos, puesto que simplifica los cálculos, ya que los más comunes se expresan mediante sencillos múltiplos o divisores de π.

El radián es la unidad natural en la medida de los ángulos. Por ejemplo, la función seno de un ángulo "x" expresado en radianes cumple:

Análogamente los desarrollos Taylor de las funciones seno y coseno son:


donde x se expresa en radianes.


1 radián = 57,29577951… grados sexagesimales y 

1 grado sexagesimal = 0,01745329252… radianes.


La tabla muestra la conversión de los ángulos más comunes.

Otras unidades de medida de ángulos convencionales son el grado sexagesimal, el grado centesimal y, en astronomía, la hora.


formula_6, que simplificada es: formula_7, o bien: formula_8.<br>

Es decir que, para pasar una cantidad "x" de rpm a rad/s tenemos que multiplicarla por π/30:

formula_9formula_10formula_11<br>

Análogamente, para pasar una cantidad "y" de rad/s a rpm tenemos que multiplicarla por 30/π:

formula_12formula_13formula_14<br>

Los grados y los radianes son dos diferentes sistemas para medir ángulos. Un ángulo de 360° equivale a 2π radianes; un ángulo de 180° equivale a π radianes (recordemos que el número π ≈ 3,14159265359…).

Las equivalencias de los principales ángulos se muestran en las siguientes figuras:

Para convertir grados en radianes o viceversa, partimos de que 180° equivalen a π radianes; luego planteamos una regla de tres y resolvemos.


Convertir 38° a radianes:

Primero planteamos la regla de tres. Nótese que la "x" va arriba, en la posición de los radianes.

Despejamos "x", también simplificamos.

Por último obtenemos el equivalente decimal:

"x" = 0,6632 radianes.


Convertir 2,4 radianes a grados.
Primero planteamos la regla de tres. Nótese que la "x" va abajo, en la posición de los grados.

Despejamos "x". 

Por último obtenemos el equivalente decimal:

Los tres son unidades de medida de ángulos planos, y se diferencian así:





</doc>
<doc id="8138" url="https://es.wikipedia.org/wiki?curid=8138" title="Arco">
Arco

Arco hace referencia a varios artículos:









</doc>
<doc id="8139" url="https://es.wikipedia.org/wiki?curid=8139" title="Ángulos complementarios">
Ángulos complementarios

Los ángulos complementarios son aquellos ángulos cuyas medidas suman formula_1 grados sexagesimales, es decir, que si dos ángulos complementarios son a su vez consecutivos, los lados no comunes de estos forman un ángulo recto.




Relaciones aritméticas entre ángulos: 

Relaciones posicionales entre ángulos:

Determinados por dos paralelas y una transversal


</doc>
<doc id="8140" url="https://es.wikipedia.org/wiki?curid=8140" title="Triángulo">
Triángulo

Se llama triángulo o trígono, en geometría plana, al polígono de tres lados. Los puntos comunes a cada par de lados se denominan vértices del triángulo. 

Un triángulo tiene tres ángulos interiores, tres pares congruentes de ángulos exteriores, tres lados y tres vértices entre otros elementos.

Si está contenido en una superficie plana se denomina triángulo o trígono, un nombre menos común para este tipo de polígonos. Si está contenido en una superficie esférica se denomina triángulo esférico. Representado, en cartografía, sobre la superficie terrestre, se llama triángulo geodésico.

Cada uno de los puntos que determinan un triángulo. Tal como los vértices de un polígono, suelen ser denotados por letras latinas mayúsculas: formula_1. Si formula_2, no existe triángulo que determine formula_3 y formula_4.

Un triángulo se nombra, entonces, como cualquier otro polígono, designando sucesivamente sus vértices, por ejemplo ABC.
En el caso del triángulo, los vértices pueden darse en cualquier orden, porque cualquiera de las 6 maneras posibles (ABC, ACB, BAC, BCA, CAB, CBA), corresponde a un recorrido de su perímetro. Esto ya no es cierto para polígonos con más vértices.

Cada par de vértices determina un segmento, que se conoce como lado del triángulo. No interesa el orden de los vértices para nombrar un lado de modo AB, BA nombran a un mismo lado.

Los lados del triángulo se denotan, como todos los segmentos, por sus extremos: AB, BC y AC.

Para nombrar la "longitud" de un lado, por lo general se utiliza el nombre del vértice opuesto, convertido a minúscula latina: formula_5 para BC, formula_6 para AC, formula_7 para AB.

La suma de los lados de un triángulo se conoce como perímetro, denotado por "p" o 2"s"; cumple la ecuación formula_8

Cada par de lados con origen común al vértice de un triángulo y que contienen dos de esos lados concurrentes se llama ángulo del triángulo u -ocasionalmente- ángulo interior.

La notación general para el ángulo entre dos segmentos OP y OQ prolongados y que concurren en el extremo O es formula_9

También es posible utilizar una letra minúscula -habitualmente una letra griega- coronada por un acento circunflejo (en rigor, los ángulos deben ser designados por letras mayúsculas y su medida por minúsculas, pero a menudo se utilizan los mismos nombres para los dos con el fin de simplificar la notación). En el caso de un triángulo, el ángulo entre dos lados todavía puede, por tolerancia y en ausencia de ambigüedad, ser designado por el nombre del vértice común, coronado por un acento circunflejo. En resumen, en el ejemplo se pueden observar los ángulos:
EL ángulo cuyo vértice coincide con uno de los vértices del triángulo y sus lados: son la prolongación de un lado triangular y el otro lado angular contiene a un lado triangular, se llama ángulo externo. En cada vértice triangular hay dos ángulos externos.

Los triángulos se pueden clasificar por la relación entre las longitudes de sus lados o por la amplitud de sus ángulos.

Por la medida de sus lados, todo triángulo se clasifica en:



Un triángulo es isósceles cuando tiene dos lados iguales; esto no descarta que los tres lados sean iguales, de modo que todo triángulo equilátero sea isósceles, pero no se cumple el enunciado recíproco.

Sea el triángulo ABC isósceles, donde b = c entonces los ángulos opuestos son iguales, i.e B = C. También se cumple que B' = C' siendo estos los ángulos externos.Además se cumplen las igualdades

A + 2B = A +2C = 180º;

A + 2B = A + 2C = 360º; A = 2C = 2B; B=C=A+B= A+C

formula_12 donde formula_13 son la mediana, altura del lado a y bisectriz de su ángulo A opuesto.


Por la amplitud de sus ángulos los triángulos se clasifican en:


Los triángulos acutángulos pueden ser:

Los triángulos rectángulos pueden ser:

Los triángulos obtusángulos pueden ser:

La medida de la calidad de triángulo (abreviada como "CT") se determina por el producto de tres factores que se obtienen de la suma de dos de sus lados menos el tercero en forma cíclica, dividido por el producto de sus tres lados; y se representa mediante la siguiente fórmula:

donde a, b, c son las longitudes de los lados del triángulo.

Por lo tanto, si

En otras palabras, la calidad del triángulo se aproxima a cero cuando la distancia euclidiana de uno de sus lados es cercana a cero o cuando los tres puntos del triángulo tienden a ser colineales.

La calidad de los triángulos tiene muchas aplicaciones en los métodos de triangulación como es el caso de la Triangulación de Delaunay porque se necesitan generar una serie de puntos en el espacio para que la malla que se genere sea de buena calidad debido a la cantidad de puntos que se encuentran bien distribuidos en un espacio de dos dimensiones porque cuando se le asigne un valor o magnitud a cada punto de la malla la aproximación del triángulo va a tener un error mayor y la solución seria continuar asignando punto en el espacio de dos dimensiones para que la aproximación ser mejor y el error disminuya.

Dos triángulos son congruentes si hay una correspondencia entre sus vértices de tal manera que el ángulo del vértice y los lados que lo componen, en uno de los triángulos, sean congruentes con los del otro triángulo.



Dos triángulos rectángulos son semejantes si cumplen con al menos uno de los criterios siguientes:


Se consideran dos triángulos semejantes con lados paralelos y con circuncentro común ( centro de la circunferencia circunscrita). La intersección del exterior del triángulo de menor área con el interior del triángulo de mayor área unida con los dos triángulos forma una región en el plano que se llama corona triangular.

La "frontera" de esta región es la unión de los dos triángulos. Un punto es "interior" si está entre las intersecciones que determina un rayo con origen en el circuncentro con los lados homólogos. El conjunto de los puntos interiores es el "interior" de la región. Un punto está en el "exterior" de la región si no está en la frontera ni en el interior. El interior es convexo, abierto y conexo. La frontera es la unión disjunta de dos poligonales cerradas. El exterior es un conjunto desconexo, abierto y no convexo. La corona triangular es un conjunto cerrado, conexo y convexo. La corona triangular es homeomorfa con la corona circular, tienen las mismas propiedades topológicas.

Un "triángulo" puede ser definido como un polígono de tres lados, o como un polígono con tres vértices.
El triángulo es el polígono más simple y el único que no tiene diagonal. Tres puntos no alineados definen siempre un triángulo (tanto en el plano como en el espacio).

Si se agrega un cuarto punto coplanar y no alineado, se obtiene un cuadrilátero que puede ser dividido en triángulos como el de la figura de la izquierda. En cambio, si el cuarto punto agregado es no coplanar y no alineado, se obtiene un tetraedro que es el poliedro más simple y está conformado por 4 caras triángulares.

Todo polígono puede ser dividido en un número finito de triángulos, esto se logra por triangulación. El número mínimo de triángulos necesarios para esta división es n-2, donde "n" es el número de lados del polígono.
El estudio de los triángulos es fundamental para el estudio de otros polígonos, por ejemplo para la demostración del Teorema de Pick.

En geometría euclidiana la suma de los tres ángulos internos de un triángulo es siempre 180°, lo que equivale a π radianes:

Euclides había demostrado este resultado en sus "Elementos" (proposición I-32) de la siguiente manera: se traza una paralela a la línea (AB) que pasa por C. Siendo paralelas, esta recta y la recta (AB) forman con la recta (AC) ángulos iguales, codificados en color rojo en la figura de la derecha (ángulos alternos-internos). Del mismo modo, los ángulos codificados en color azul son iguales (ángulos correspondientes). Por otro lado, la suma de los tres ángulos del vértice "C" es el ángulo llano. Así que la suma de las medidas del ángulo de color rojo, del ángulo verde y del azul es un ángulo de 180° (o π radianes). En conclusión, la suma de los ángulos de un triángulo es 180°.

Esta propiedad es el resultado de la geometría euclidiana. No se verifica en general en la geometría no euclidiana.








De la ecuación anterior se deducen fácilmente 3 fórmulas de aplicación práctica:


Geométricamente se pueden definir varios casos que están ligados a un triángulo:






El único caso en que el baricentro, incentro, ortocentro y circuncentro coínciden es en el triángulo equilátero.

En general, hay varios métodos aceptados para calcular la longitud de un lado y la medida de un ángulo. Mientras que ciertos métodos pueden ser adecuados para calcular los valores de un triángulo rectángulo, otros pueden ser requeridos en situaciones más complejas.

Para resolver triángulos ("en general") se suele utilizar los teoremas del seno y del coseno, para el caso especial de triángulos rectángulos se utiliza generalmente el Teorema de Pitágoras.

En triángulos rectángulos, las razones trigonométricas del seno, el coseno y la tangente pueden ser usadas para encontrar los ángulos y las longitudes de lados desconocidos. Los lados del triángulo se denominan como sigue, con respecto a uno de los ángulo agudos:


El seno de un ángulo es el cociente entre la longitud del cateto opuesto y la longitud de la hipotenusa.

El coseno de un ángulo es el cociente entre la longitud del cateto del lado adyacente y la longitud de la hipotenusa.

La tangente de un ángulo es el cociente entre la longitud del cateto opuesto y la longitud del cateto adyacente.

Nota: Los cocientes de las tres relaciones anteriores no dependen del tamaño del triángulo rectángulo.

Las funciones trigonométricas inversas pueden ser usadas para calcular los ángulos internos de un triángulo rectángulo al tener la longitud de dos lados cualesquiera.

Arcsin (arcoseno) puede ser usado para calcular un ángulo con la longitud del cateto opuesto y la de la hipotenusa.

Arccos (arcocoseno) puede ser usado para calcular un ángulo con la longitud del cateto adyacente y la de la hipotenusa.

Arctan (arcotangente) puede ser usada para calcular un ángulo con la longitud del cateto opuesto y la del cateto adyacente.

En los cursos introductorios de geometría y trigonometría, la notación sin, cos, etc., es frecuentemente utilizada en lugar de arcsin, arccos, etc. Sin embargo, la notación de arcsin, arccos, etc., es estándar en matemáticas superiores donde las funciones trigonométricas son comúnmente elevadas a potencias, pues esto evita la confusión entre el inverso multiplicativo y la función inversa.

Dado un punto en el plano euclídeo, diremos que este es interior a un triángulo si al trazar una recta por él, dicho punto se halla entre los cortes con los lados del triángulo. De otro modo un punto es punto "interior" de un triángulo, si está en el interior de cada ángulo del triángulo . Si consideramos una región triangular su "interior" coincide con el interior definido según la topología usual del plano.

Los tres lados de un triángulo constituyen su frontera y los puntos del plano que no están en el interior ni en la frontera están en el exterior del triángulo. La unión del interior, del triángulo (frontera) y del exterior es igual al plano del triángulo. Cada par de los conjuntos aludidos tiene intersección vacía o son conjuntos mutuamente disjuntos.

Cualquier triángulo es equivalente a una curva simple cerrada; en particular a una circunferencia. Esto es, entre una circunferencia y un triángulo se puede establecer una aplicación biyectiva y bicontinua. 

Ceviana es una recta que pasa por un vértice de un triángulo y por la recta que contiene al lado opuesto; algunos autores incluyen como ceviana a los lados del triángulo. Se consideran cevianas interiores, si contiene puntos del interior triangular; y cevianas exteriores, cuando pasa por el exterior del triángulo. 

El segmento de recta que va de un vértice al punto medio del lado opuesto de un triángulo se llama mediana. En algunos países ("por ej: Chile") se las llama transversales de gravedad, reservando en esos lugares el término mediana para lo que habitualmente se denomina paralela media.

Algunas propiedades de las medianas son:



Del teorema de Apolonio, también llamado "teorema de la mediana", pueden deducirse varias fórmulas prácticas ("válidas para cualquier triángulo"), estas permiten calcular a partir del conocimiento de tres elementos, un cuarto elemento desconocido ("los elementos en cuestión son lados y medianas"). La siguiente tabla muestra un resumen de las mismas (con notación acorde a la figura de la propia tabla):


Se llama mediatriz de un lado de un triángulo a la recta perpendicular a dicho lado trazada por su punto medio ("también llamada simetral"). El triángulo tiene tres mediatrices, una por cada uno de sus lados formula_27, formula_28 y formula_29.

Las tres mediatrices de un triángulo son concurrentes en un punto formula_30 equidistante de los tres vértices. La circunferencia de centro formula_30 y radio formula_32 que pasa por cada uno de los tres vértices del triángulo es la circunferencia circunscrita al triángulo, y su centro se denomina circuncentro.


Un triángulo es rectángulo si y sólo si el centro de su circunferencia circunscrita es el punto medio de su hipotenusa.
Las bisectrices de un triángulo son las bisectrices de sus ángulos. Existen bisectrices internas (las usuales) y externas a estos ángulos.

Las tres bisectrices internas de un triángulo son concurrentes en un punto O. La circunferencia inscrita del triángulo es la única circunferencia tangente a los tres lados del triángulo y es interior al triángulo. Tiene por punto central el incentro, que es el centro de la circunferencia inscrita en el triángulo.

Además, las bisectrices exteriores de dos ángulos concurren con la bisectriz interior del ángulo restante en puntos denominados exincentros, que son los centros de las circunferencias exinscritas del triángulo. Hay 3 exincentros, al igual que 3 circunferencias exinscritas. Las circunferencias exinscritas son tangentes a un lado y a la extensión de los otros dos.

La distancia desde un vértice el triángulo hasta los puntos de intersección de la circunferencia inscrita en el triángulo con los lados que se cruzan en dicho vértice por potencia de un punto es la misma por lo que las longitudes de los lados de un triángulo son a=x+y, b=y+z, c=z+x, a esta forma de denotar a los lados de un triángulo se le conoce como Transformación de Ravi, en un triángulo rectángulo los lados son x+r, r+y, y+x con r el radio de la circunferencia inscrita en el triángulo.

formula_33 
donde vA es la bisectriz del ángulo A; a, b, c, lados del triángulo y p el semiperímetro; siendo
formula_34 

Siendo formula_35 radios de las circunferencias exinscritas de un triángulo ABC; R y r radios de la circunfrencia circunscrita e inscrita en el mismo triángulo, respectivamente, entonces se cumple la ecuación que sigue:

Para cualquier triángulo, donde h = la menor altura, l = la menor bisectriz y S el área, se cumple lo siguiente

Se nombra simediana a la recta (ceviana) que es simétrica a la mediana, siendo el eje de simetría la bisectriz con el mismo vértice. 
El punto de Lemoine es un punto interior de un triángulo para el que la suma de los cuadrados de sus distancias a los lados es la mínima. Se nombra también punto de Lemoine- L' Huiller 

Se llama altura de un triángulo al segmento de recta perpendicular que une un vértice del triángulo con el lado opuesto de este o su prolongación. El lado opuesto se llama base del triángulo. Todos los triángulos tienen tres alturas. Estas 3 alturas se cortan en un punto único formula_38 (son "concurrentes"), llamado ortocentro del triángulo.

Para un triángulo ΔABC cualquiera, conociendo la longitud de sus lados (a, b, c), se pueden calcular las respectivas longitudes de las alturas (h, h, h) aplicando las siguientes fórmulas:

Donde h es la altura correspondiente al lado a, h es la altura correspondiente al lado b, h es la altura correspondiente al lado c y el término formula_42 es:

formula_43

La altura del lado a puede hallarse mediante la siguiente fórmula 
formula_44
donde h es la altura indicada; a, b, c los lados y p el semiperímetro del triángulo. Para las otras dos alturas basta cambiar el denominador por el lado respectivo en la fórmula formula_45.
Dado un triángulo y sus tres alturas h, h y h y el radio r de su círculo inscrito (inradio), cabe la siguiente igualdad:

Los tres puntos formula_38, formula_48 y formula_30 están alineados en una línea recta llamada recta de Euler del triángulo y verifica la relación de Euler:

Los puntos medios de los tres lados, los tres pies de las alturas y los puntos medios de los segmentos formula_51, formula_52 y formula_53 están en una misma circunferencia llamada circunferencia de Euler o circunferencia de los nueve puntos del triángulo.

El teorema de Carnot establece que, para un triángulo acutángulo de vértices ABC, la suma de las distancias respectivas formula_54 desde el circuncentro a los lados del triángulo es igual a la suma de los radios formula_55 de las circunferencias circunscrita e inscrita, respectivamente, del triángulo:

Usando un lado que se llama, en este contexto, "base" y su "altura", perpendicular ( y medida) trazada del vértice puesto a dicho lado o a su prolongación. La altura correspondiente se subindiza con letra del lado.

El área de un triángulo es igual al semiproducto de la base por su altura respectiva.

Esto es cierto para cualquier triángulo rectilíneo.El área es la medida de una región triangular, esto es, la unión de los tres segmentos y su interior. Se deduce en base al área de un paralelogramo.

Conociendo la longitud de los tres lados a, b y c, se puede calcular el área para cualquier triángulo euclideo. Primero se calcula el semiperímetro s y luego se aplica la fórmula de Herón, ("no se requiere conocer la altura").


Si se aplica la Transformación de Ravi a los lados del triángulo tenemos que los lados son x+y, y+z, z+x y el área del triángulo es

Conociendo la longitud de los tres lados a, b y c, se puede calcular el área para cualquier triángulo euclideo, ("éstas fórmulas no requieren precalcular el semiperímetro ni conocer la altura"). 

Si A es el área de un triángulo y a,b y c sus lados se verifica la siguiente inecuación

formula_58

formula_59

formula_60

formula_61

formula_62

formula_63

donde S es el área; además formula_64 son los lados y el semiperímetro del triángulo; R, radio de la circunferencia circunscrita o circunradio; r, radio de la circunferencia inscrita o inradio ; formula_65 radios de sendas circunferencias exinscritas formula_66 son las respectivas alturas.

Si en la fórmula área = ah/2, siendo h la altura medida sobre la base a, se tiene en cuenta que

e igualmente:

Si en la fórmula área = a b sen C / 2 se tiene en cuenta que de acuerdo con el teorema del seno b = a sen B / sen A, se obtiene que:

y teniendo en cuenta que A = formula_15 - ( B + C ); y que sen(formula_15 - S) = sen(S)

e igualmente:

Si un triángulo cualquiera ("en el plano euclidiano" ℝ²), tiene alguno de sus vértices ("supongamos el" A) ubicado en (0, 0) —"el origen de las coordenadas cartesianas"—, y las coordenadas de los otros dos vértices ("supongamos" B "y" C) vienen dadas por B = ("x", "y") y C = ("x", "y"), entonces el área puede ser calculada como ½ del valor absoluto del determinate ("reducido a los dos vértices arbitrarios B y C").

Si un triángulo genérico ("en el plano euclidiano" ℝ²), tiene sus tres vértices ubicados de modo arbitrario ("ninguno en el origen"), entonces la ecuación es:

Para un triángulo genérico ("en el espacio euclidiano" ℝ³), cuyas coordenadas son { A = ("x", "y", "z"), B = ("x", "y", "z") y C = ("x", "y", "z") }, entonces el área viene dada por la suma pitagórica de las áreas de las respectivas proyecciones sobre los tres planos principales (es decir "x" = 0, "y" = 0 y "z" = 0):


Esta fórmula es válida aún en el plano ℝ ( por tanto en el plano complejo), con el cuidado de considerar la tercera coordenada igual a 0. Sin embargo para ℝ, n > 3, uno de los vectores se usa como base, luego se obtiene el coseno del ángulo que forman los lados concurrentes en A, por medio del producto escalar de los vectores correspondiente a dichos lados. Después el seno de tal ángulo, que propicia hallar la altura del triángulo.
En estos espacios está definido el producto escalar ( interno) de vectores . Sean a y b dos vectores de n componentes cualesquiera de un espacio euclídeo. El producto interno es = suma de ab para i = 1,2..., n

Cuando consideramos la obtención de triángulos rectángulos con lados enteros se encuentra la solución general de la ecuación formula_76:

formula_77
Ver, también, terna pitagórica

En estas fórmulas, "u" y "v" son dos enteros positivos arbitrarios de distinta paridad tales que "u" > "v" y son primos entre sí. El entero positivo m es uno cualquiera que cubre los casos en los que los elementos de la terna pitagórica tienen un factor común. Cuando "m" = 1, tenemos las ternas pitagóricas con elementos primos entre sí dos a dos. Como el lector puede apreciar, aunque estas fórmulas fueron diseñadas para obtener ternas con lados enteros, al ser una identidad, también son válidas para lados reales, exceptuando el caso en que ambos catetos son iguales (que la hipotenusa sea diagonal de un cuadrado).

Si realizamos el cálculo del área sobre la base de las expresiones encontradas para los catetos, pues la superficie de un triángulo rectángulo es igual al semiproducto de los catetos, nos queda una forma cúbica:

formula_78

Los números de la forma formula_79, cuando "u" y "v" son "u" > "v" y enteros positivos impares y primos entre sí, son números congruentes de Fibonacci, introducidos en su Liber Quadratorum (1225). No hay razón conocida para que "u" y "v" no puedan ser de distinta paridad. Fibonacci demostró que el producto de un congruente por un cuadrado también es congruente.

Como el área de cualquier triángulo puede ser descompuesto en la suma o resta del área de dos triángulos rectángulos, tenemos dos expresiones para el área de triángulos no rectángulos:

Acutángulo: formula_80
Obtusángulo: formula_81

Sin olvidar que esto solamente es válido para pares de triángulos rectángulos que no tengan catetos iguales. Es una forma más complicada de calcular el área de un triángulo, y también es poco conocida. Pero en algunos casos, su escritura puede echar luz sobre cuestiones que de otra forma pasan inadvertidas.
Un triángulo de lados a,b y c, inscrito en una circunferencia de radio R, con perímetro 2p constante alcanza su máxima área cuando los tres lados son iguales.

El triángulo es la forma de las caras de tres poliedros regulares:

En otros casos, las caras laterales de una pirámide son triángulos dos a dos con arista común; de la misma manera, las caras laterales de un antiprisma son triángulos .

La arquitectura monumental de la III Dinastía y la IV Dinastía de Egipto es una prueba notable de que los egipcios de esa época tenían conocimientos relativamente sofisticados de geometría, especialmente en el estudio de los triángulos; si bien ningún documento matemático del Antiguo Imperio ha llegado hasta nosotros.

El cálculo del área de esta figura se analiza en los problemas R51 del papiro Rhind, M4, M7 y M17 del papiro de Moscú, que datan todos del Imperio Medio. El problema R51 constituye en la historia mundial de las matemáticas, el primer testimonio escrito que trata del cálculo del área de un triángulo.

Enunciado del problema R51 del papiro Rhind:
El término "mryt" significa probablemente la altura o el lado. Sin embargo, la fórmula utilizada para calcular el área hace pensar en la interpretación en favor de la primera solución.El escriba tomaba la mitad de la base del triángulo y calculaba el área del rectángulo formado por ese lado y la altura; es decir
formula_82
equivalente a la fórmula común utilizada en nuestros días:
formula_83

El hecho de que un triángulo de lados 3-4-5 es un triángulo rectángulo también era conocido por los antiguos egipcios y mesopotámicos.

Euclides, en el Libro I de sus " Elementos ", hacia el 300 antes de Cristo, enunció la propiedad de la suma de los ángulos del triángulo.

Tipos de triángulos:




</doc>
<doc id="8141" url="https://es.wikipedia.org/wiki?curid=8141" title="Tappi Tíkarrass">
Tappi Tíkarrass

Tappi Tíkarrass fue una banda islandesa surgida en 1982 que combinaba la música punk y el pop. Dentro de sus cuatro integrantes se destacaba la presencia de Björk, quien era la vocalista de la banda.

El significado del nombre Tappi Tíkarrass proviene de una frase del padre del bajista de la banda quien exclamó que ésta "encajaba como el corcho en el culo de una perra"; por lo tanto, Tappi Tíkarrass significa en islandés "Tapona el Culo de la Perra".

En 1981 junto al exbajista de Exodus Jakob Magnússon formaron otra banda, Tappi Tíkarrass (en islandés significa “Taponéa el Culo de la Perra”), y lanzaron un álbum en 1982 con el título de Bítið Fast Í Vítið. Su álbum Miranda fue lanzado en 1983.
El grupo desarrolló un estilo after-punk más experimental y con referencias a Siouxsie and the Banshees y la primera etapa de The Cure.

Tappi Tíkarrass colaboró en repetidas oportunidades con Purrkur Pillnikk y Þeyr. Con el lanzamiento de Miranda, la banda había experimentado bastante y Björk había colaborado con la banda de covers Cactus en diferentes clubes, y una presentación en el intento de batir el récord en la presentación más larga del mundo con el grupo Stigrim, cuyo video, fue dirigido por Óskar Jónasson, su novio. La banda sonora titulada como la película, Rokk Í Reykjavík (“Rock en Reykjavík”) sirvió para afianzar los lazos entre los miembros de Purrkur Pillnikk y Þeyr con Tappi. Para ese momento Björk también participó como vocalista de fondo y batería de Rokka Rokka Drum.
Solamente lanzaron dos álbumes y aparecieron un par de veces en películas, siendo el más famoso un documental islandés "Rokk Í Reykjavík" con dos canciones, Björk también se convirtió en la chica cover del video. Posteriormente Tappi tocó en varias presentaciones con otras bandas, Purrkurr Pillnikk y Þeyr.

Un miembro notable de Purrkurr Pillnikk era el cantante/trompetista Einar Örn Benediktsson quien posteriormente se uniría con Björk como vocalista en The Sugarcubes.

Después de lanzar su segundo álbum y haber explorado todas sus posibilidades musicales Tappi Tíkarrass decide separarse en 1983.


Presentaciones y colaboraciones:


</doc>
<doc id="8145" url="https://es.wikipedia.org/wiki?curid=8145" title="Miranda (álbum)">
Miranda (álbum)

Miranda, lanzado en diciembre de 1983 fue el segundo y último álbum de la banda islandesa de música punk Tappi Tíkarrass en la que se encontraba la cantante Björk.

El álbum fue lanzado en formato LP y bajo el sello discográfico Gramm.

Lado A

Lado B


</doc>
<doc id="8148" url="https://es.wikipedia.org/wiki?curid=8148" title="The Sugarcubes">
The Sugarcubes

The Sugarcubes fue una banda islandesa de rock-pop que se formó en 1986 y se separó en 1992. Fueron aclamados por la crítica y el público tanto en el Reino Unido como en los Estados Unidos.

Su primer álbum fue "Life's Too Good" por el cual se dieron a conocer mundialmente con el sencillo "Birthday" gracias a la voz de la cantante Björk.

Después de la separación de los Sugarcubes, Björk continuó con su carrera solista en Londres y se convirtió en una de las cantantes más innovadoras de su género.


El 8 de junio de 1986 es la fecha que es citada como el nacimiento oficial de Sykurmolarnir que finalmente sería traducida a su equivalente en inglés: The Sugarcubes (“Los Terrones de Azúcar”). Björk dio a luz a Sindri Eldon Þórsson ese mismo día. Para ese momento Einar Örn y Þór Eldon estaban al frente de una nueva organización llamada Smekkleysa u oficialmente conocida como Bad Taste, un sello discográfico contracultural que fomentaba el trabajo artístico de jóvenes islandeses. Brevemente después del nacimiento de Sindri, Björk tuvo un papel en la película "The Juniper Tree" (1987) y dos años más tarde, Björk y Þór se separan, pero siguieron siendo amigos.

También en 1987 participó de "Glerbrot", una obra de Matthias Johannsen para la televisión islandesa, interpretó a una adolescente conflictiva. Para el mismo año, la hermana de Björk, Inga Hrönn, participó de la banda de new wave Blatt Afram que solamente grabó dos canciones en su único álbum en formato casete lanzado con el nombre de "Snarl 2".

Bad Taste dio lugar a la formación de un grupo de pop con Björk, Siggi Baldursson, Einar Örn y Einar Melax de KUKL, con Þór Eldon, Bragi Ólafsson y el guitarrista de Purrkur Pillnikk Friðrik Erlingsson; Einar Melax sería reemplazado más tarde por Margrét Örnólfsdóttir en teclados.

El primer sencillo de los Sugarcubes "Amæli" ("Birthday", en su versión al inglés), se convirtió en un gran éxito en Inglaterra, dejando una buena impresión con el surrealismo expresado con la letra y el extravagante concepto sonoro general.

"Melody Maker" es el medio más entusiasta eligiendo a "Birthday" como sencillo de la semana y agregando comentarios halagadores de su arquitectura sonora. Otros medios también presentaron a los Sugarcubes en portada, como "New Musical Express" y la desaparecida "Sounds".

De esta manera ganaron una significativa popularidad en el Reino Unido y en los Estados Unidos y las ofertas de compañías discográficas empezaron a llegar. Seguidamente, la banda firmó con One Little Indian y grabaron su primer álbum, "Life's Too Good", un álbum que los llevó a la fama mundial -la primera banda islandesa en lograr semejante éxito. Las críticas fueron todavía mejores porque el álbum presentaba una amplia mezcla de referentes provenientes del pop, rock, el funk y hasta incluso el jazz convirtiéndolo en una rareza de la escena indie de finales de la década de 1980. el álbum logró vender 150.000 ejemplares en el difícil mercado del Reino Unido y cerca de 500 mil en los Estados Unidos.

Mientras tanto, hay algunas convulsiones dentro del grupo: después de separarse de Björk, Þór Eldon se casa con la teclista, Magga Örnólfsdóttir. Siggi Baldursson y Bragi Ólafsson se casan con unas hermanas gemelas y más tarde, el mismo año, surge la noticia de que Einar Örn y Bragi se casaron en Dinamarca amparados por la ley a favor del matrimonio gay, cuando en realidad se trató de una conversación en broma entre ambos y que fue accidentalmente distribuida por la oficina de prensa de One Little Indian.

La suerte de los Sugarcubes empieza a decaer cuando en 1989 lanzaron su segundo álbum llamado "Here Today, Tomorrow, Next Week!", título que proviene de una frase del libro "El viento en los sauces", de Kenneth Graeme.

El álbum no sirvió para consolidar la posición internacional del grupo, la creciente intromisión de Einar Örn y el carácter continuista de canciones como “Speed Is The Key”, “A Day Called Zero” o “Water” producen un estancamiento en The Sugarcubes, a pesar de canciones destacables como “Planet” y “Tidal Wave”.

Mientras que el éxito de los Sugarcubes iba decayendo, Björk participó en otros proyectos adicionales: bajo el nombre de Betula Jónasson tocó el clarinete en Hljómsveit Konráðs B (La Banda de Konráð B, en islandés), grupo derivado de Caviare, el dúo de Siggi Baldursson y Magga Örnólfsdóttir.

En 1990 grabó "Gling-Gló", una colección de jazz popular y trabajo original con el grupo de música bebop que se dio a conocer como Björk Guðmundsdóttir & Tríó Guðmundar Ingólfssonar, lanzado en Islandia.

El álbum, producido por el guitarrista del grupo Stuðmenn Tómas Tómasson, contiene temas en islandés y tres versiones: “Ruby Baby”, de Jerry Leiber y Mike Stoller y los estándares de jazz: “I Can’t Help Loving That Man” (de Oscar Hammerstein II y Jerome Kern) y “Þad Sést Ekki Sætari Mey” (versión en islandés de "You Can't Get A Man With A Gun"), adaptación de Irving Berlin.

El grupo desapareció poco después del lanzamiento del álbum, cuando el pianista Guðmundur Ingólfsson muere de cáncer; también contribuyó como vocalista para la grabación de "Ooops" y "Qmart", pertenecientes al álbum "" de la banda 808 State, al mando de Graham Massey; una colaboración que cultivó su interés en la música house.

Mientras tanto, The Sugarcubes queda en un periodo de inactividad.

The Sugarcubes vuelven a reunirse en Nueva York junto al productor Paul Fox para preparar su próximo álbum "Stick Around For Joy", que fue editado en febrero de 1992 y precedido por el sencillo “Hit”. "Stick Around For Joy" es el álbum más famoso con el cual Björk se daría a conocer definitivamente en la escena musical británica, pero a pesar del éxito alcanzado, las tensiones aumentaron entre Björk y Einar Örn y para finales de ese año, sale a la venta un álbum con remixes titulado "It's-It", el último lanzamiento antes de que la banda se separe. Este disco tiene la colaboración de DJs como Todd Terry, Justin Robertson y Marius De Vries.






</doc>
<doc id="8150" url="https://es.wikipedia.org/wiki?curid=8150" title="Bragi Ólafsson">
Bragi Ólafsson

Bragi Ólafsson (Reikiavik, Islandia, 11 de agosto de 1962) es un escritor y músico islandés.

Ólafsson fue conocido primero por su trabajo como bajista de los Sugarcubes, una innovadora banda de pop de Islandia que llevó a la fama a Björk, quien siguió con éxito su carrera como solista en el dance-pop.

The Sugarcubes, quienes favorecían melodías inusuales se reunieron en 1986, justo en el día en que Björk dio a luz a su hijo Sindri. Pero antes de que el grupo se formara, los miembros de los Sugarcubes ya habían tocado en diferentes grupos. Ólafsson y Einar Örn Benediksson (trompeta / vocalista) habían lanzado álbumes bajo la discográfica Gramm records del mismo Benediktsson.

Ólafsson se había convertido en poeta al comenzar su trabajo con los Sucarcubes. Es autor de varios poemarios, obras de teatro y novelas, entre ellas se ha traducido al castellano su novela Las mascotas. Es uno de los escritores de más importancia en Islandia.


</doc>
<doc id="8151" url="https://es.wikipedia.org/wiki?curid=8151" title="Life's Too Good">
Life's Too Good

Life's Too Good es el primer álbum de estudio de el grupo islandés de rock alternativo The Sugarcubes. Fue lanzado en abril de 1988 por One Little Indian en el Reino Unido y Europa y en mayo de 1988 por Elektra Records en los Estados Unidos. El álbum fue un éxito inesperado y atrajo atención internacional, especialmente hacia la cantante Björk, quien pondría en marcha una exitosa carrera como solista en 1993.

Conformada por veteranos de la cultura roQUera de los 80 en Reykjavík, la banda tomó elementos del sonido post punk que caracterizaba a la escena, con la intención de crear una interpretación jocosa del optimismo característico de la música pop, aspecto que se refleja en el título del disco. A pesar de nunca tener intención de ser tomados en serio y debido al éxito de su debut y obligaciones contractuales, The Sugarcubes lanzaron otros dos álbumes de estudio.



</doc>
<doc id="8152" url="https://es.wikipedia.org/wiki?curid=8152" title="Here Today, Tomorrow, Next Week!">
Here Today, Tomorrow, Next Week!

Here Today, Tomorrow, Next Week! fue lanzado por The Sugarcubes en octubre de 1989 y fue el segundo álbum de cuatro.

El nombre del álbum fue inspirado por el Sr. Toed del famoso libro infantil de Kenneth Grahame "The Wind In The Willows".

La versión en islandés de este álbum se llama "Illur Arfur!" y contiene el mismo listado de canciones (en islandés) y con una pequeña variación de música en algunos temas.



</doc>
<doc id="8154" url="https://es.wikipedia.org/wiki?curid=8154" title="Stick Around For Joy">
Stick Around For Joy

Stick Around For Joy es el tercer álbum de la banda de rock islandesa The Sugarcubes. Salió a la venta en febrero de 1992.




</doc>
<doc id="8155" url="https://es.wikipedia.org/wiki?curid=8155" title="It's It">
It's It

It's It fue el cuarto y último álbum lanzado por The Sugarcubes en octubre de 1992. Posteriormente los Sugarcubes se separaron y la cantante Björk inició su carrera como solista.

Este álbum es una recopilación de los tres álbumes publicados anteriormente.



</doc>
<doc id="8157" url="https://es.wikipedia.org/wiki?curid=8157" title="Cromosoma">
Cromosoma

En biología y citogenética, se denomina cromosoma (del griego χρώμα, -τος "chroma", color y σώμα, -τος "soma", cuerpo o elemento) a cada una de las estructuras altamente organizadas, formadas por ADN y proteínas, que contiene la mayor parte de la información genética de un ser vivo.

En las divisiones celulares (mitosis y meiosis) el cromosoma presenta su forma más conocida, cuerpos bien delineados en forma de X, debido a su alto grado de compactación y duplicación. 

En la interfase no pueden ser visualizados mediante el microscopio óptico de manera nítida ya que ocupan territorios cromosómicos discretos. En las células eucariotas y en las arqueas (a diferencia que en las bacterias), el ADN siempre se encontrará en forma de cromatina, es decir, asociado fuertemente a unas proteínas denominadas histonas y no-histonas. La cromatina, organizada en cromosomas, se encuentra en el núcleo de las células eucariotas y se visualiza como una maraña de hebras delgadas. Cuando comienza el proceso de duplicación y división del material genético llamado (cariocinesis), esa maraña de hebras inicia un fenómeno de condensación progresivo que permite visualizar cada uno de los cromosomas.

Cuando se examinan con detalle durante la mitosis, se observa que cada uno de los cromosomas presenta una forma y un tamaño característicos.

Cada cromosoma tiene una región condensada, o constreñida, llamada centrómero, que confiere la apariencia particular a cada cromosoma y que permite clasificarlos según la posición del centrómero a lo largo del cromosoma.

Otra observación que se puede realizar es que el número de cromosomas de los individuos de la misma especie es constante. Esta cantidad de cromosomas se denomina número o Ploidía y se simboliza como "2n" o "4n" o "1n" dependiendo del tipo de célula.

Cuando se examina la longitud de tales cromosomas y la situación del centrómero surge el segundo rasgo general: para cada cromosoma con una longitud y una posición del centrómero determinada existe en el núcleo otro cromosoma con características idénticas, o sea, en las células diploides 2n los cromosomas se encuentran formando pares. Los miembros de cada par se denominan cromosomas homólogos.
En la figura de la derecha se presentan todos los cromosomas de interfase de una niña (obsérvese los dos cromosomas X abajo derecha), ordenados por parejas de homólogos y por su longitud, lo que se denomina cariotipo. Puede observarse que en este cariotipo hay 46 cromosomas (o sea, 2n=46) que es el número cromosómico de la especie humana. Se puede advertir, también, que cada cromosoma tiene una estructura doble, con dos cromátidas hermanas que yacen paralelas entre sí y unidas por un único centrómero. Durante la mitosis las cromátidas hermanas, que son idénticas, se separan una de otra hacia dos nuevas células.

Las parejas de cromosomas homólogos que se observan en la imagen tienen, además, una semejanza genética fundamental: presentan los mismos genes situados en los mismos lugares a lo largo del cromosoma (tales lugares se denominan "locus" o "loci" en plural). Esto indica que cada miembro del par de homólogos lleva información genética para las mismas características del organismo. En organismos con reproducción sexual, uno de los miembros del par de cromosomas homólogos proviene de la madre (a través del óvulo) y el otro del padre (a través del espermatozoide). Por ello, y como consecuencia de la herencia biparental, cada organismo diploide tiene dos copias de cada uno de los genes, cada una ubicada en uno de los cromosomas homólogos.

Una excepción importante al concepto de parejas de cromosomas homólogos es que en muchas especies los miembros de la pareja de los cromosomas sexuales, no tienen el mismo tamaño, ni igual situación del centrómero, ni la misma proporción entre los brazos o, incluso, no tienen los mismos "loci". 
Por ejemplo, el cromosoma Y (que determina el sexo masculino en humanos) es de menor tamaño y carece de la mayoría de los "loci" que se encuentran en el cromosoma X.

Desde el punto de vista etimológico, la palabra «cromosoma» procede del griego y significa «cuerpo que se tiñe»; mientras que la palabra «cromatina» significa «sustancia que se tiñe».

Los cromosomas fueron observados en células de plantas por el botánico suizo Karl Wilhelm von Nägeli en 1842 e, independientemente, por el científico belga Edouard Van Beneden en lombrices del género "Ascaris". El uso de drogas basofílicas (p. ej. las anilinas) como técnica citológica para observar el material nuclear fue fundamental para los descubrimientos posteriores. Así, el citólogo alemán Walther Flemming en 1882 definió inicialmente la cromatina como «la sustancia que constituye los núcleos interfásicos y que muestra determinadas propiedades de tinción».

Por lo tanto, las definiciones iniciales de cromosoma y cromatina son puramente citológicas. La definición biológica solo se alcanzó a principios del siglo XX, con el redescubrimiento de las leyes de Mendel: tanto la cromatina como el cromosoma constituyen el material genético organizado. Para ello, fueron fundamentales los trabajos del holandés Hugo de Vries (1848-1935), del alemán Carl Correns (1864-1933) y del austríaco Erich von Tschermak-Seysenegg (1871-1962), cuyos grupos de investigación redescubrieron independientemente las leyes de Mendel y asociaron los factores genéticos o genes a los cromosomas. Un breve resumen de los acontecimientos asociados a la historia del concepto de cromosoma se provee a continuación.

El primer investigador que aisló ADN fue el suizo Friedrich Miescher, entre 1868 y 1869, cuando realizaba sus estudios postdoctorales en el laboratorio de Felix Hoppe-Seyler (uno de los fundadores de la bioquímica, la fisiología y la biología molecular) en Tübingen. Miescher estaba analizando la composición química del pus de los vendajes usados del hospital, para lo cual aisló núcleos y comprobó que estaban formados por una única sustancia química muy homogénea, no proteica, a la que denominó "nucleína". Sin embargo, fue Richard Altmann en 1889 quien acuñó el término ácido nucleico, cuando se demostró que la nucleína tenía propiedades ácidas. En 1881, E. Zacharias demostró que los cromosomas estaban químicamente formados por "nucleína", estableciendo la primera asociación entre los datos citológicos y bioquímicos.

Las primeras observaciones de la división celular (la mitosis, durante la cual la célula madre reparte sus cromosomas entre las dos células hijas), se realizaron entre 1879 y 1882 por Walther Flemming y Robert Feulgen, de forma independiente, gracias al desarrollo de nuevas técnicas de tinción. La asociación entre herencia y los cromosomas se realiza poco después (1889) por August Weismann, de manera teórica, casi intuitiva. Pero los primeros datos experimentales que permitieron a Walter Sutton y Theodor Boveri proponer que los «factores» de Mendel eran unidades físicas que se localizan en los cromosomas (lo que se denomina a menudo la teoría cromosómica de Sutton y Boveri) datan de 1902. Estas ideas permanecieron controvertidas hasta que Thomas Hunt Morgan realizó los experimentos que hoy se consideran clásicos sobre los rasgos genéticos ligados al sexo, publicados en 1910, lo que le valió el Premio Nobel en 1933. 

La demostración de que los genes están en los cromosomas se realizó por Calvin Bridges y Nettie Stevens en 1912 y fue Alfred Henry Sturtevant quien probó que los genes se hallan dispuestos linealmente a lo largo del cromosoma, elaborando el primer mapa genético de un organismo, "Drosophila melanogaster". Las bases fundamentales de la herencia quedaron definitivamente establecidas en 1915, cuando apareció el libro "El mecanismo de la herencia mendeliana" escrito por Morgan, Strurtevant, Müller y Bridges. En 1919 Phoebus Levene identificó que un nucleótido está formado por una base, un azúcar y un fosfato, iniciando así el análisis molecular del ADN, que llevaría a la comprensión de los mecanismos moleculares de la herencia (véase también Historia del ADN).


Los cromosomas eucarióticos son moléculas muy largas de ADN de doble hélice que están estrechamente relacionadas con proteínas llamadas histonas y proteínas llamadas no histonas. Los cromosomas se pueden hallar desde estados laxos o poco compactados, como en los núcleos de las células en interfase, hasta en estados altamente compactados, como sucede en la metafase mitótica.
Los principales componentes que se obtienen cuando se aísla la cromatina de los núcleos interfásicos son el ADN, las proteínas histónicas, las proteínas no histónicas y el ARN. 

Las histonas son proteínas básicas, ricas en residuos de lisina y arginina, que muestran una elevada conservación evolutiva y que interaccionan con el ADN formando una subunidad que se repite a lo largo de la cromatina denominada nucleosoma. Los principales tipos de histonas que se han aislado en los núcleos interfásicos en diferentes especies eucariontes son: H1, H2A, H2B, H3 y H4. Además de estas histonas, también existen otras que son específicas de tejido como la histona H5 muy rica en lisina (25 moles%) específica de eritrocitos nucleados de vertebrados no mamíferos, y las histonas del endosperma. Asimismo, la cromatina centromérica se caracteriza por la presencia de una isoforma específica de la histona H3, denominada CENP-A en vertebrados.

Una de las características más destacables es su elevado conservadurismo evolutivo, sobre todo de las histonas H3 y H4. La histona H4 de guisante y de timo de ternera se diferencian solamente en dos aminoácidos. Este dato indica que las interacciones entre el ADN y las histonas para formar la cromatina deben ser muy semejantes en todos los organismos eucariontes.

Los genes que codifican las histonas se encuentran agrupados en nichos (o "clusters") que se repiten decenas o centenas de veces. Cada "clúster" o grupo contiene el siguiente orden de genes que codifican histonas: H1-H2A-H3-H2B-H4. Estos genes son ricos en pares G-C, ya que codifican proteínas con un elevado contenido en lisina y arginina, pero están separados por secuencias espaciadoras ricas en pares A-T.

La cromatina de núcleos en interfase, cuando se observa mediante técnicas de microscopia electrónica, se puede describir como un collar de cuentas o un rosario, en el que cada cuenta es una subunidad esférica o globular que se denomina nucleosoma; los nucleosomas se hallan unidos entre sí mediante fibras de ADN. Se sigue, entonces, que la unidad básica de la estructura de la cromatina es el nucleosoma.
Un nucleosoma típico está asociado a 200 pares de bases (pb) de ADN y está formado por una médula ("core" en inglés) y un ligador (o "linker"). La médula está formada por un octámero constituido por dos subunidades de las histonas H2A, H2B, H3 y H4. En otras palabras, se trata de un dímero: 2×(H2A, H2B, H3, H4). Los trabajos de Aaron Klug y colaboradores sobre la disposición de las histonas en la médula del nucleosoma le valieron el Premio Nobel de Química en 1982.

Alrededor de la médula se enrolla el ADN (140 pb) dando casi dos vueltas (una vuelta y tres cuartos). El resto del ADN (60 pb) forma parte del ligador ("linker"), que interacciona con la histona H1. La cantidad de ADN asociado con un nucleosoma varía de una especie a otra, de 154 pb a 241 pb; esta variación se debe fundamentalmente a la cantidad de ADN asociada al ligador ("linker"). 

Las fibras de ADN dúplex desnudo tienen un grosor de 20 Å. La asociación del ADN con las histonas genera los nucleosomas, que muestran unos 100 Å de diámetro. A su vez, los nucleosomas se pueden enrollar helicoidalmente para formar un solenoide (una especie de muelle) que constituye las fibras de cromatina de los núcleos intefásicos con un diámetro aproximado de 300Å. Los solenoides pueden volverse a enrollar para dar lugar a supersolenoides con un diámetro de 4000 Å a 6000 Å que constituirían las fibras de los cromosomas metafásicos.

Las proteínas cromosómicas no histónicas son proteínas diferentes de las histonas que se extraen de la cromatina de los núcleos con cloruro sódico (NaCl) 0.35 mol/L (disolución salina), tienen un alto contenido en aminoácidos básicos (25 % o más), alto contenido en aminoácidos ácidos (20-30 %), una elevada proporción de prolina (7 %), bajo contenido en aminoácidos hidrofóbicos y una alta movilidad electroforética. Las proteínas cromosómicas no histónicas que se extraen de la cromatina de los núcleos varían mucho dependiendo de la técnica de aislamiento empleada. Un grupo de estas proteínas cromosómicas no histónicas presentan alta movilidad electrofóretica y se denominan abreviadamente HMG (grupo de alta movilidad). 
La cantidad de proteínas no histónicas puede variar de unos tejidos a otros en el mismo individuo y dentro del mismo tejido a lo largo del desarrollo.

Estas proteínas se agrupan en una superfamilia por sus similitudes físicas y químicas, y porque todas ellas actúan como elementos arquitectónicos que afectan múltiples procesos dependientes de ADN en el contexto de la cromatina. Todas las HMG tienen un terminal carboxilo rico en aminoácidos de tipo ácido, y se clasifican en tres familias (HMGA, HMGB y HMGN), cada una con un motivo funcional único, que induce cambios específicos en sus sitios de unión y participa en funciones celulares diferentes. 

La familia HMGA consta de cuatro miembros, y todos ellos contienen un motivo funcional característico, denominado «gancho AT» ("AT hook"). A través de estas secuencias, las HMGA se unen preferencialmente a secuencias ricas en AT de ADN en forma-B e inducen cambios de conformación que inducen la unión de componentes adicionales. Las proteínas HMGA tienen una cola C-terminal ácida, que podría ser importante para la interacción con otras proteínas. Tradicionalmente, este grupo se denominaba HMG-I/Y. 

La familia HMGB consta de tres variantes, cada una de las cuales contiene dos motivos funcionales (las cajas HMG) y un extremo C-terminal muy ácido. Las cajas HMG están formadas por tres α-hélices plegadas conjuntamente para formar una estructura en forma de L, que en parte se introduce en la hendidura menor del ADN, plegándolo intensamente. Existen ligeras diferencias entre las cajas HMG de las diferentes HMGB, lo que confiere especificidad a cada una de ellas. Las colas acídicas modulan la afinidad por una variedad de estructuras de ADN distorsionado. Tradicionalmente estas proteínas se denominaban proteínas HMG-1/-2.

La familia de proteínas HMGN se caracteriza por un dominio cargado positivamente, el dominio de unión a nucleosomas, y por una cola C-terminal ácida, el dominio de desplegado de la cromatina. Las proteínas HMGN se unen específicamente a los nucleosomas y alteran tanto la estructura local como la estructura de nivel superior de la cromatina. Estas proteínas se conocen tradicionalmente como la subfamilia HMG-14/-17.

Se han detectado más de 20 proteínas HMG; las proteínas HMG-1/-2 (HMGB) y HMG-14/-17 (HMGA) se han identificado en todas las especies de mamíferos, aves y peces estudiadas hasta el momento. Las proteínas HMG-1/-2 se encuentran solo en el núcleo, están implicadas en la replicación, se unen preferentemente a ADN de hélice sencilla, desenrollan el ADN dúplex y se estima que existe una molécula de HMG-1 o HMG-2 por cada 15 nucleosomas. Las proteínas HMG-14/-17 se encuentran en el núcleo y en el citoplasma, están relacionadas con la regulación de la transcripción y se estima que existe una molécula de HMG14 o HMG-17 por cada 10 nucleosomas.

Muchos estudios citogenéticos muestran que el ADN está intensamente enrollado, en los cromosomas, cuando se observa al microscopio. El primer nivel de compactación lineal del ADN es el obtenido por el plegamiento de la fibra del ADN alrededor de los nucleosomas, responsable del primer nivel de plegamiento lineal (de 6 a 7 veces). El siguiente nivel de plegamiento corresponde a la denominada «fibra de 30 nm», que es lo que se observa en núcleos en interfase. Aunque ha habido mucha controversia para describir esta estructura, la fibra de 30 nm se considera normalmente como el enrollamiento helicoidal de las fibras de nucleosomas, que genera la compactación de otras 6-7 veces. En mitosis, la fibra de 30 nm debe compactarse otras 200-500 veces hasta alcanzar el diámetro observado al microscopio para las fibras cromosómicas durante la división celular (–700nm). Por tanto, se han tenido que producir nuevos superenrollamientos. Sin embargo, la explicación de estos plegamientos de orden superior ha generado gran controversia.
Laemmli y colaboradores en 1977 consiguieron aislar cromosomas metafásicos desprovistos de histonas mediante un tratamiento con sulfato de dextrano y heparina. Estos cromosomas metafásicos desprovistos de histonas presentan una médula central densamente teñida que ha sido denominada "scaffold" (armazón). Este armazón proteico ("scaffold") es resistente a la acción de la ADNasa, ARNasa y también a soluciones de ClNa 2M. Sin embargo, desaparece por tratamientos con urea 4M y dodecil sulfato sódico o por tratamiento con enzimas proteolíticas. Por tanto, se trata de un armazón proteico. 

La observación a microscopía electrónica pone de manifiesto que de este armazón proteico ("scaffold") salen y llegan lazos o fibras que pueden hacerse desaparecer mediante tratamiento con ADNasa. Por tanto, estos lazos o dominios que arrancan del armazón proteico son lazos de ADN. Uno de los principales componentes del armazón proteico es la enzima topoisomerasa II α (topoIIα), una enzima que produce cortes en el ADN dúplex a nivel de ambas hélices. La topoisomerasa II (girasa) interviene durante la replicación del ADN creando o relajando los superenrollamientos. En mamíferos se encuentran dos isoformas de esta enzima (α y ß), con propiedades similares "in vitro". Sin embargo, aunque topoIIα y β se comportan "in vivo" de forma similar en interfase, en mitosis tienen un comportamiento diferente: solo topoIIα está asociado mayoritariamente a los cromosomas. La aparición de la topoisomerasa II α solo en el armazón proteico sugiere que se encuentra en la base de los lazos o dominios de ADN, indicando que esta organización en dominios podría estar relacionada con la replicación y transcripción. Otras enzimas, como la topoisomerasa I que produce cortes en el ADN dúplex a nivel de una sola hélice y la HMG-17, se encuentran solo en los lazos o dominios y no en el armazón proteico.
La evidencia existente hasta el momento sugiere que las fibras de solenoides (30 nm) formarían los lazos o dominios que emanan del armazón proteico y que este armazón estaría a su vez enrollado formando una espiral.

Además de la enzima topoisomerasa II α, el otro componente fundamental propuesto del armazón proteico es la condensina 13S. La tinción doble con anticuerpos contra topoIIα y condensina genera un armazón con aspecto de un «polo de barbero» (un cilindro con bandas espirales rojas y blancas que simboliza la antigua doble profesión de los barberos como cirujanos), en la cual alternan «cuentas» enriquecidas en topoIIα y en condensina. Esta estructura parece estar generada por dos cadenas yuxtapuestas. Parece ser que el ensamblaje de este armazón proteico tiene lugar en dos fases, ya que la condensina solo se asocia en la transición de profase a metafase durante la mitosis. Sin embargo, el papel estructural de la topoIIα en la organización de los cromosomas aún se discute, ya que otros grupos argumentan que esta enzima se intercambia rápidamente tanto en los brazos cromosómicos como en los cinetocoros durante la mitosis.

Los dominios de ADN parecen estar unidos al armazón proteico por unas regiones específicas denominadas abreviadamente SAR ("scaffold associated regions", también denominadas MAR, "matrix attachment regions") que se detectan cuando los cromosomas metafásicos desprovistos de histonas se tratan con endonucleasas de restricción. Después de este tratamiento quedan regiones de ADN unidas al armazón que a su vez resisten la digestión con exonucleasas gracias a que están protegidas por una proteína. Cuando se digiere esta proteína, las regiones de ADN protegidas contienen secuencias de varios cientos de pares de bases que son muy ricas en AT y que presentan sitios de unión para topoisomerasa II e histona H1. Estas regiones de unión específicas de los dominios al armazón proteico son las regiones SAR. Se ha sugerido que estas regiones juegan un papel global durante la condensación de los cromosomas mitóticos y son necesarias para el mantenimiento de la estructura de los cromosomas. Las regiones SAR también podrían estar implicadas en la expresión génica, al facilitar tanto la transición como la expansión de una estructura abierta de la cromatina.

Es cada vez más evidente que incluso con los métodos de fijación más utilizados se pueden producir cambios significativos en la localización de las proteínas cromosómicas, y estas dificultades técnicas han estado presentes en la mayor parte de las preparaciones cromosómicas utilizadas para realizar los estudios estructurales. Por ello, parece necesario utilizar muestras vivas siempre que sea posible, así como aproximaciones alternativas que permitan un análisis complementario.

Un modo alternativo para el análisis estructural de los cromosomas es el biofísico. Las medidas precisas de la rigidez y la elasticidad de los cromosomas pueden guiar la construcción de los modelos estructurales. Estudios realizados en diferentes laboratorios indican que los cromosomas presentan una elasticidad remarcable: tanto dentro de las células como en tampones fisiológicos, los cromosomas pueden estirarse hasta varias veces su longitud normal y volver de nuevo a su longitud original. Sin embargo, los datos obtenidos por diferentes laboratorios son muy variables, probablemente debido a la variedad de tampones utilizado por los distintos grupos. Un estudio de Poirier y Marko en 2002 mostró que la elasticidad de los cromosomas es muy sensible a nucleasa. Estos datos sugieren que la integridad mecánica de los cromosomas mitóticos se mantiene por enlaces entre las fibras cromosómicas, no por la existencia de un armazón proteico. La naturaleza de estos enlaces no está clara, pero este estudio estima su frecuencia en 10-20 kb como mínimo.

Un método convencional y muy potente para entender una estructura biológica consiste en establecer una lista que incluya todos sus componentes. Los estudios iniciales de la estructura cromosómica se enfrentaron a muchos problemas técnicos para conseguir aislar bioquímicamente los cromosomas mitóticos de las células, aunque métodos sofisticados permitieron el aislamiento de los cromosomas completos y la identificación del armazón proteico.

Un método alternativo consiste en la utilización de extractos libres de células procedentes de huevos de anfibios. Este sistema permite la reconstitución "in vitro" de cromosomas mitóticos a partir de sustratos simples (por ejemplo, cromatina de esperma) en condiciones fisiológicas, de manera que los componentes proteicos de las estructuras que se ensamblan pueden aislarse por centrifugación en un solo paso y caracterizarse de forma sistemática. Además de las histonas centrales y una histona de ligamiento, la fracción así aislada contiene topoIIα (CAP-B en ese estudio), un complejo de cinco subunidades denominado condensina (CAP-C, -E, -D2, -G y -H), cromokinesina (CAP-D/Klp1) y la ATPasa remodeladora de cromatina ISWI (CAP-F). Una de las conclusiones más importantes de estos estudios es que las ATPasas son componentes importantes de los cromosomas. La energía de hidrólisis del ATP es utilizada en muchos casos para inducir cambios locales o globales en los cromosomas, mientras que en otros casos sirve para soportar el movimiento de los cromosomas anclados a los microtúbulos.

Una observación sorprendente fue la identificación de la proteína titina como uno de los componentes de los cromosomas en embriones de "Drosophila". La titina es una proteína filamentosa gigante (–3MDa) que funciona como un componente integral del filamento grueso en el sarcómero de las células musculares. Se ha propuesto que, en analogía con su función muscular, la isoforma de la titina que se encuentra en los cromosomas puede funcionar por un lado como una «regla molecular» que determina la longitud cromosómica, y por otro como un «muelle molecular» que proporciona elasticidad a los cromosomas.

El ARN parece jugar algún papel en el plegamiento del cromosoma eucariótico. Al menos en humanos y en "Drosophila" se han encontrado evidencias de este papel estructural del ARN. Sin embargo, hay que tener en cuenta que el armazón proteico descrito por Laemmli y colaboradores (1978) no se ve afectado por el tratamiento con ARNasa. Podría ser que las propias proteínas del armazón protegieran al ARN de la acción del ARNasa. En cualquier caso, es conveniente recordar que el ADN del cromosoma bacteriano también está organizado en dominios y que el ARN podría jugar algún papel en el mantenimiento de dicha estructura. En organismos con características intermedias entre las de procariontes y eucariontes como los dinoflagelados, también existen datos que apoyan el papel estructural del ARN en la organización cromosómica.

La cromatina (la sustancia que compone los núcleos de las células y que resulta de la interacción del ADN con las proteínas histónicas, no histónicas y ARN) puede presentar distintos grados de empaquetamiento o contracción. Cuando los cromosomas se tiñen con sustancias químicas que se unen al ADN aparecen regiones densamente teñidas y regiones menos densamente teñidas. La cromatina mayoritaria, la que constituye la mayor parte del núcleo recibe el nombre de eucromatina y la minoritaria el de heterocromatina. Mientras que la eucromatina representa la fracción que contiene la mayor parte de los genes activos, la heterocromatina interviene en varios procesos nucleares, como la función centromérica, el silenciamiento de genes y la organización nuclear.

La heterocromatina puede aparecer más densamente teñida que la eucromatina (heteropicnosis positiva) o menos densamente teñida que la eucromatina (heteropicnosis negativa). La aplicación de determinados tratamientos experimentales en combinación con diferentes tipos de tinción de los cromosomas, puede producir la aparición de zonas heterocromáticas en los cromosomas de muchas especies. Estas zonas heterocromáticas presentan una distribución característica o patrón de bandas típico de cada cromosoma, que permite identificar cromosomas distintos. Estas técnicas reciben el nombre de "técnicas de bandeo cromosómico" y son enormemente útiles en la identificación individual de los cromosomas y en la construcción de cariotipos. 





Se pueden distinguir dos clases de heterocromatina: 


En la especie humana, todos los cromosomas X que están en exceso de uno aparecen más intensamente teñido que el resto de los cromosomas ("heteropicnosis positiva") en los núcleos de células en interfase. Por tanto, las mujeres normales que tienen dos cromosomas X, tienen un cromosoma X que aparece más intensamente teñido y que está inactivado. Sin embargo, durante las primeras etapas del desarrollo embrionario (durante los 16 primeros días de gestación en la especie humana) ambos cromosomas X son activos. 

En algunas especies eucariontes, el ADN satélite o ADN minoritario que presenta un contenido en G+C distinto al ADN principal o mayoritario, está constituido por unas secuencias cortas de ADN que están repetidas millones de veces. En concreto en ratón se ha demostrado que el ADN satélite está localizado en la zona centrómerica. Este ADN satélite constituye un ejemplo de heterocromatina constitutiva cuya presencia y acción es constante en el cromosoma. 

La organización de la cromatina no es uniforme a lo largo de la estructura del cromosoma. De hecho, se pueden distinguir una serie de elementos diferenciados: los centrómeros (o constricciones primarias), los telómeros (o extremos cromosómicos), las regiones organizadoras del nucléolo (NORs según la abreviatura en inglés) y los cromómeros, todos ellos caracterizados por contener secuencias específicas de ADN.

El centrómero es la constricción primaria que utilizando tinciones tradicionales aparece menos teñida que el resto del cromosoma. Es la zona por la que el cromosoma interacciona con las fibras del huso acromático desde profase hasta anafase, tanto en mitosis como en meiosis, y es responsable de realizar y regular los movimientos cromosómicos que tienen lugar durante estas fases. Las estructuras centroméricas que interaccionan con las fibras del huso se denominan cinetocoros. Además, el centrómero contribuye a la nucleación de la cohesión de las cromátidas hermanas. En la estructura del centrómero intervienen tanto el ADN centromérico, que consta fundamentalmente de heterocromatina constitutiva, como proteínas centroméricas.

En la levadura de gemación ("Saccharomyces cerevisiae") el ADN centromérico consta únicamente de 125 pb y está conservado entre los diferentes cromosomas. Sin embargo, el ADN centromérico en metazoos puede constar de megabases, y no contiene secuencias consenso fácilmente identificables (ver la revisión de Choo en 1997). A pesar de las diferencias entre el ADN centromérico de levaduras y metazoos, el cinetocoro se ensambla en ambos casos sobre nucleosomas centroméricos que contienen una forma especializada de histona H3 (Cse4p en levaduras o su homólogo CENP-A en metazoos).

La palabra telómero procede del griego "telos", «final» y "meros", «parte». Los telómeros son los extremos de los cromosomas. Son regiones de ADN no codificante, altamente repetitivas, cuya función principal es la estabilidad estructural de los cromosomas en las células eucariotas, la división celular y el tiempo de vida de las estirpes celulares. Además están involucradas en enfermedades tan importantes como el cáncer. En los organismos procariotas, los cromosomas son circulares y no poseen telómeros. 

Los telómeros fueron descubiertos por Hermann Joseph Muller durante la década de 1930. Desde entonces, se ha avanzado mucho en el conocimiento de los telómeros, gracias a las técnicas de la genética molecular. 

Además de las constricciones primarias, en algunos cromosomas se puede distinguir otro tipo de «adelgazamiento» denominada constricción secundaria, las que se hallan relacionadas normalmente con la presencia de las secuencias de ADN ribosómico. Tales regiones se denominan regiones organizadoras del nucléolo (o, sencillamente, "NOR" por el acrónimo en inglés para "nucleolus organizer regions"). Las secuencias de ADN ribosómico quedan englobadas dentro del nucléolo, que permanece adosado a las NOR durante buena parte del ciclo celular. Los cromosomas portadores de NOR en muchos casos presentan un segmento que une a esta región con el telómero, el cual se denomina "satélite" o "trabante".

Los cromómeros son «engrosamientos» o regiones más compactadas de la eucromatina, que se distribuyen de manera más o menos uniforme a lo largo de los cromosomas y se pueden visualizar durante las fases de la mitosis o de la meiosis de menor condensación de la cromatina (profase). Su naturaleza molecular sigue siendo controvertida, pero podrían ser consecuencia de un cierto grado de compartimentalización en la distribución de las secuencias de ADN y en la organización de los cromosomas. Desde hace varios años, el grupo de Giorgio Bernardi en Italia, sostiene que hay una distribución compartimentalizada de secuencias relativamente grandes de ADN (llamadas «isócoras») en el genoma de los vertebrados de sangre caliente, de modo tal que cada isócora tiene un contenido en bases (porcentaje de C+G) relativamente homogéneo pero diferente al de las demás. Después de publicado el primer borrador del Proyecto Genoma Humano, parece confirmarse la existencia de cinco isócoras en el genoma de los humanos, dos de ellas ricas en A y T, y tres ricas en G y C. La distribución alternante de ambos tipos de isócoras podría ser la explicación molecular de la existencia de cromómeros.

El estudio de la estructura externa de los cromosomas de cualquier especie eucariótica consiste en analizar la forma, tamaño y número de los cromosomas que posee. El mejor momento para llevar a cabo dicho estudio suele ser aquel en el que los cromosomas han alcanzado su máximo grado de contracción y tienen sus bordes perfectamente definidos. Dicho momento suele ser la metafase mitótica. El estudio de la estructura externa de los cromosomas culmina con la obtención del cariotipo.

Los cromosomas se pueden estudiar en distintos momentos según la especie y dependiendo de los objetivos planteados. Algunas especies tienen cromosomas que se pueden observar con gran detalle en interfase, tal es el caso de "Drosophila melanogaster", que posee cromosomas politénicos gigantes que se observan en las glándulas salivales de dicho insecto, y el de "Chironomus tentans", otro díptero. El cariotipo se confecciona usualmente después de un apropiado pre-tratamiento y tinción de las células, para hacer más visibles los cromosomas individuales. Al diagrama simplificado de los cromosomas metafásicos del cariotipo se lo denomina idiograma, que se construye con el número genómico.

Para realizar el ordenamiento de los cromosomas tanto en cariotipos como idiogramas se debe tener en cuenta el tamaño cromosómico (ubicados de mayor a menor, con el brazo corto «bc» o «p» hacia arriba y el brazo largo «bl» o «q» hacia abajo); posición del centrómero (generalmente alineados) y presencia de constricciones secundarias y satélites.

Usualmente las especies animales y vegetales tienen un número de cromosomas constante y determinado que constituyen su cariotipo (ley de la constancia numérica de los cromosomas), aunque existen especies con una alta variabilidad cariotípica, no solo en número sino en forma y tamaño de los cromosomas.

El número de cromosomas de una especie (o fase vital) diploide se identifica como "2n" mientras que ese número en una especie (o fase vital) haploide se identifica con la letra "n". En aquellas especies que presentan un número repetido de cromosomas superior a dos complementos se habla de poliploidía, representándose el múltiplo por delante de la letra "n". Así: "3n" indicaría un complemento cromosómico triploide, "4n" un tetraploide, etc. Todas estas son situaciones de euploidía. Con la indicación "x" se quiere expresar el número básico de cromosomas de una especie que presenta individuos con diversos grados de "ploidía" o el de una línea filogenética a partir de la cual diversos taxones han alcanzado situaciones aneuploides variadas, siendo en este caso el número cromosómico una variación del número original con aumento o disminución del número básico, por pérdida, fusión o división de cromosomas (p. ej., n+1 o n-1). Un ejemplo de esta situación anormal la tenemos en los individuos de la especie humana que presentan el llamado síndrome de Down, situación de aneuploidía (2n=47) por la presencia de un ejemplar más de lo habitual del cromosoma 21 (trisomía).

El número de cromosomas 2n varía mucho de unas especies a otras y no existe relación entre el número de cromosomas y la complejidad de los mismos: existen especies vegetales con pocos cromosomas como "Haplopappus gracilis" (2n=4), "Crepis capillaris" (2n=6) y "Secale cereale" (2n=14), especies vegetales con bastantes cromosomas como "Triticum aestivum" (2n=42) y especies vegetales con muchos cromosomas como "Ophioglossum petiolatum" (n >500). En animales sucede algo semejante, hay especies con pocos cromosomas como la hormiga australiana "Myrmecia pilosula" cuyos machos tienen un cromosoma (2n=1) y las hembras dos cromosomas (2n=2), especies con bastantes cromosomas como la humana "Homo sapiens" (2n=46) y especies con muchos cromosomas como el lepidóptero "Lysandra atlantica" (2n=434-466). No existe ninguna relación entre el número de cromosomas 2n y la complejidad evolutiva, ni entre el número de cromosomas y la cantidad de ADN. Un ejemplo claro de esta situación es el de los ciervos del género "Muntiacus" en el que hay especies muy similares (denominadas especies gemelas) una con 2n=6 ("M. muntjak") y otra con 2n=46 ("M. reevesi").

En muchos organismos, uno de los pares de los cromosomas homólogos es distinto al resto, realizando la determinación del sexo del individuo. A estos cromosomas se les llama cromosomas sexuales o heterocromosomas e incluso gonosomas, porque determinan el sexo.


La forma de los cromosomas es para todas las células somáticas constante y característica de cada especie. La forma depende fundamentalmente de las constricciones que presente el cromosoma y de su localización en la cromátida.

El cromosoma se encuentra constituido básicamente por el centrómero que divide el cromosoma en un brazo corto o "brazo p" y un brazo largo o "brazo q". Algunos cromosomas presentan satélites en el brazo corto.

Según la posición del centrómero, los cromosomas se clasifican en:

El par de gonosomas o sexocromosomas se constituyen por un cromosoma X (submetacéntrico mediano) y un cromosoama Y considerado acrocéntrico sin satélites, aunque en algunas revisiones de la literatura se le refiere como submetacéntrico.

Los cromosomas sufren grandes variaciones en su tamaño a lo largo del ciclo celular, pasando de estar muy poco compactados (interfase) a estar muy compactados (metafase), por tal motivo, los estudios sobre el tamaño suelen realizarse en metafase mitótica. Además, es necesario tener en cuenta que los tratamientos para teñir los cromosomas y para obtener las metafases mitóticas influyen de manera muy importante en el tamaño de los cromosomas. En cualquier caso, en general es posible decir que hay especies eucarióticas con cromosomas grandes y especies con cromosomas pequeños. Las monocotiledóneas (vegetales) y los anfibios y ortópteros (animales) poseen cromosomas muy largos (de 10 a 20 micras). Las dicotiledóneas, las algas, los hongos y la mayoría de las especies animales poseen cromosomas pequeños (longitud inferior a 5 micras). Naturalmente, existen algunas excepciones en los ejemplos citados. El cromosoma 1 humano tiene 0,235 pg de ADN, que equivalen a una longitud total de ADN doble hélice de 7,3 cm y en metafase mitótica presenta una longitud aproximada de 0,001 cm.

En algunas especies los pares cromosómicos no pueden diferenciarse claramente considerando solo sus componentes distintivos en sentido longitudinal; en estos casos se debe recurrir a técnicas citológicas especiales para la tinción de los cromosomas, que evidencian «bandas» transversales (oscuras y claras) a lo largo de los mismos, y que corresponden a los distintos tipos de cromatina. En una especie dada, estas variantes de la cromatina presentan un tamaño y disposición constante.
Las técnicas de bandeo cromosómico más usadas son:


El ser humano presenta 23 pares de cromosomas en sus células somáticas: 22 autosomas y un par de cromosomas sexuales (dos X en el caso de las mujeres y un cromosoma X y un Y en el caso de los varones). El tamaño total aproximado del genoma humano es de 3 200 millones de pares de bases de ADN (3 200 Mb) que contienen unos 20 000-25 000 genes. De las 3 200 Mb unas 2 950 Mb corresponden a eucromatina y unas 250 Mb a heterocromatina. El Proyecto Genoma Humano produjo una secuencia de referencia del genoma humano eucromático, usado en todo el mundo en las ciencias biomédicas. 

La secuencia de ADN que conforma el genoma humano contiene codificada la información necesaria para la expresión, altamente coordinada y adaptable al ambiente, del proteoma humano, es decir, del conjunto de proteínas del ser humano. El genoma humano presenta una densidad de genes muy inferior a la que inicialmente se había predicho, con solo en torno al 1,5 % de su longitud compuesta por exones codificantes de proteínas. Un 70 % está compuesto por ADN extragénico y un 30 % por secuencias relacionadas con genes. Del total de ADN extragénico, aproximadamente un 70 % corresponde a repeticiones dispersas, de manera que, más o menos, la mitad del genoma humano corresponde a secuencias repetitivas de ADN. Por su parte, del total de ADN relacionado con genes se estima que el 95 % corresponde a ADN no codificante: pseudogenes, fragmentos de genes, intrones, secuencias UTR, entre otras. Aunque tradicionalmente esas secuencias de ADN han sido consideradas regiones del cromosoma sin función, hay datos que demuestran que esas regiones desarrollan funciones relacionadas con la regulación de la expresión génica.

En la siguiente tabla se listan los cromosomas humanos, el número de genes que presenta cada uno, su tamaño en pares de bases y su morfología.

Es posible visualizar los cromosomas por medio de la microscopía de luz y de tinciones especiales. El proceso para obtener el material cromosómico se realiza en diversos pasos, que incluyen la obtención de una muestra viva, la siembra e incubación de la misma y la posterior tinción y lectura.

Existen algunos tipos de cromosomas presentes solo en algunos tipos celulares o en poblaciones concretas de una especie. Entre ellos, destacan los cromosomas politénicos, en escobilla, cromosomas B e isocromosomas.

Las células de las glándulas salivares de los insectos del orden de los Dípteros presentan núcleos que se hallan en una interfase permanente. Durante el crecimiento y desarrollo de las larvas de estos insectos, la división celular se detiene en algunos tejidos pero las células continúan su crecimiento por incremento de volumen. Este proceso ocurre, por ejemplo, en los tubos de Malpighi, en las células nutricias de los ovarios, en el epitelio intestinal y en las células de las glándulas salivares. En las células de tejidos mencionados, los cromosomas sufren rondas repetidas de duplicaciones pero sin separarse, proceso conocido como endomitosis. Esto lleva a la producción de cromosomas constituidos por varios cientos o aun miles de hebras. Durante este proceso de politenización o politenia, los cromosomas incrementan tanto su longitud como su diámetro. De hecho, la longitud de los cromosomas de "Drosophila" en una metafase es del orden de 7,5 μm mientras que el largo total de los cromosomas en un núcleo de las glándulas salivares es de alrededor de 2000μm.

Además del cambio en el tamaño, los cromosomas politénicos presentan otras dos características. En primer lugar, los cromosomas homólogos están asociados entre sí en toda su extensión. Esta condición, denominada "apareamiento somático" es propia de la mitosis de la mayoría de los Dípteros. La otra característica peculiar es que los cromosomas muestran un patrón particular de bandeo transversal que consiste en zonas más oscuras, llamadas "bandas", que alternan con zonas claras, llamadas "interbandas". Cuando se observan al microscopio óptico se identifican como bandas oscuras y claras transversales alternantes. Aunque la mayoría de las bandas son continuas a través del cromosoma, otras aparecen como una serie de puntos. Este bandeo es reproducible de núcleo a núcleo, formando un patrón constante de tal manera que los cromosomas pueden ser identificados y mapeados en toda su longitud. Hay aproximadamente 5000 bandas y 5000 interbandas en total en el genoma de "Drosophila melanogaster". Debido a que el patrón de bandeo que presentan los cromosomas politénicos es un reflejo constante de las secuencias de ADN, las bandas sirven como marcadores para localizar varias características genéticas (lugar de los genes, o cambios en el genoma debido a reordenamientos cromosómicos, por ejemplo deleciones, duplicaciones de bandas y translocaciones) y se han utilizado en diversos estudios genéticos y evolutivos. 

En "D. melanogaster" el patrón de bandeo no se distingue en aquellas regiones heterocromáticas presentes en región centromérica de todos sus cromosomas (n=4). Las regiones heterocromáticas están asociadas formando un "cromocentro". Ya que dos miembros del complemento haploide de esta especie son metacéntricos (los cromosomas II y III) y dos son acrocéntricos (cromosoma sexual X o Y y el cromosoma IV), los cromosomas politénicos en esta especie aparecen como cinco brazos desiguales que irradian del cromocentro: un brazo correspondiente al cromosoma X, los dos brazos del cromosoma II y los dos brazos del cromosoma III (3L y 3R). En algunos casos se puede visualizar un sexto brazo muy pequeño que representa el cromosoma IV.

Los cromosomas en escobilla (también llamados "cromosomas plumosos"), observados por primera vez por Walther Flemming en 1882 en oocitos de salamandra "(Ambystoma mexicanum)", son uno de los tipos de cromosomas más grandes y se hallan en los oocitos de la mayoría de los animales, exceptuando a los mamíferos. Se hallan durante el estadio de la meiosis I denominado diploteno. Luego de este relativamente largo período de la meiosis I, los cromosomas en escobilla vuelven a compactarse durante el período de metafase I. Son estructuras transitorias, específicamente bivalentes (es decir, dos cromosomas apareados cada uno de los cuales está formado por dos cromátidas hermanas). Cada uno de los dos cromosomas está constituido por dos largas hebras que forman muchos «rulos» o «bucles», a la manera de un cepillo o escobilla, a lo largo del eje mayor del cromosoma. Esos «rulos» permiten que el ADN se halle disponible para el proceso de transcripción durante la maduración del ovocito. De hecho, la presencia de cromosomas en escobilla en una célula es indicador de que está ocurriendo la transcripción del ARN mensajero. El término «cromosomas en escobilla» ("lampbrush chromosome") fue acuñado por J. Rückert en 1892, quien asimiló la forma de estos cromosomas a un cepillo del siglo XIX, bastante equivalente a lo que actualmente se denomina «limpiatubos».

La mayoría de los organismos son habitualmente muy poco tolerantes a la adición o pérdida de material cromosómico, incluso en cantidades ínfimas. Así, alteraciones cromosómicas como las deleciones, duplicaciones y aneuploidías (el exceso o defecto respecto al número cromosómico normal en una especie dada) provocan en el individuo afectado desde malformaciones hasta inviabilidad en diferentes niveles del desarrollo. Sin embargo, una excepción a este hecho en muchas especies animales y vegetales consiste en la existencia de cromosomas supernumerarios o cromosomas B. La distinción entre cromosomas B y los del complemento normal (cromosomas A) fue realizada por primera vez por Randolph en 1928. En general, los cromosomas accesorios presentan las siguientes características:

Sin embargo, el término «cromosoma B» integra un conjunto heterogéneo de cromosomas, que varían tanto en su comportamiento como en su forma y tamaño, por lo que las generalizaciones deben realizarse con precaución.

Un isocromosoma es un cromosoma metacéntrico anormal originado durante la meiosis o mitosis cuando la división del centrómero se produce según el plano horizontal en vez de vertical. Como consecuencia, uno de los brazos del cromosoma original se pierde y los brazos del isocromosoma resultante son genéticamente idénticos entre sí pero en sentido inverso.

En los humanos, los isocromosomas se hallan asociados a ciertas enfermedades. Así, por ejemplo, se hallan en algunas niñas que presentan el síndrome de Turner, en los pacientes con el síndrome de Pallister-Killian y en algunos tumores. El isocromosoma «17q» (o sea, el isocromosoma formado por dos brazos largos del cromosoma 17 y que ha perdido el brazo corto) y el isocromosoma «14q» están asociados a ciertos tipos de leucemia. Además, los individuos portadores de isocromosomas pueden tener descendientes con mayor número de cromosomas que el normal.

Los procariotas, bacteria y archaea, presentan típicamente un solo cromosoma circular, si bien existen algunas variantes a esta regla. El cromosoma bacteriano puede tener un tamaño desde 160 000 pares de bases (como en el endosimbionte "Carsonella ruddii", a 12 200 000 pares de bases en la bacteria del suelo "Sorangium cellulosum". 

Las bacterias usualmente tienen un solo punto en su cromosoma desde el cual se inicia la duplicación, mientras que algunas archeas presentan múltiples sitios de inicio de la duplicación. Por otro lado, los genes de los procariotas están organizados en operones y no contienen intrones. 

Los procariotas no poseen un núcleo verdadero, en cambio su ADN está organizado en una estructura denominada "nucleoide". El nucleoide es una estructura distintiva y ocupa una región definida en la célula bacteriana. Esta estructura es muy dinámica y se halla mantenida y remodelada a través de la acción de proteínas similares a histonas, las cuales se asocian al cromosoma bacteriano. En archaea, el ADN en el cromosoma se halla todavía más organizado, con el ADN empacado dentro de estructuras similares a los nucleosomas eucarióticos.

Los cromosomas artificiales son cromosomas que han sido manipulados a través de herramientas de ingeniería genética para que presenten estructuras precisas que permiten su integración, permanencia y duplicación en determinados organismos. El cromosoma artificial de levadura o "YAC" (acrónimo inglés por "yeast artificial chromosome") es un tipo de vector de clonación de alta capacidad siendo, de hecho, el de mayor capacidad (200kb a 3000kb). Fueron descritos por primera vez en 1983. Es un vector que imita las características de un cromosoma normal de una levadura, ya que porta un centrómero y los telómeros terminales. Esto permite clonar (es decir, multiplicar) en levaduras secuencias de ADN de hasta un millón de pares de bases o más, al comportarse como un cromosoma propio de la levadura. Son utilizados en construcción de genotecas genómicas, siendo muy extendido su uso en los primeros años del Proyecto Genoma Humano. Sin embargo, son más inestables que otros vectores, tales como BAC (acrónimo inglés de "bacterial artificial chromosome" o cromosoma artificial bacteriano), que han acabado imponiéndose. Estos últimos son también vectores de clonación usados para clonar fragmentos de ADN de 100 a 300 kb de tamaño en la bacteria "Escherichia coli". Su estructura es análoga a la del plásmido factor-F encontrado de modo natural en esa especie bacteriana.





</doc>
<doc id="8165" url="https://es.wikipedia.org/wiki?curid=8165" title="Barroco">
Barroco

El Barroco fue un período de la historia en la cultura occidental originado por una nueva forma de concebir el arte (el «estilo barroco») y que, partiendo desde diferentes contextos histórico-culturales, produjo obras en numerosos campos artísticos: literatura, arquitectura, escultura, pintura, música, ópera, danza, teatro, etc. Se manifestó principalmente en la Europa occidental, aunque debido al colonialismo también se dio en numerosas colonias de las potencias europeas, principalmente en Latinoamérica. Cronológicamente, abarcó todo el siglo XVII y principios del XVIII, con mayor o menor prolongación en el tiempo dependiendo de cada país. Se suele situar entre el Manierismo y el Rococó, en una época caracterizada por fuertes disputas religiosas entre países católicos y protestantes, así como marcadas diferencias políticas entre los Estados absolutistas y los parlamentarios, donde una incipiente burguesía empezaba a poner los cimientos del capitalismo.

Como estilo artístico, el Barroco surgió a principios del siglo XVII (según otros autores a finales del XVI) en Italia —período también conocido en este país como "Seicento"—, desde donde se extendió hacia la mayor parte de Europa. Durante mucho tiempo (siglos XVIII y XIX) el término «barroco» tuvo un sentido peyorativo, con el significado de recargado, engañoso, caprichoso, hasta que fue posteriormente revalorizado a finales del siglo XIX por Jacob Burckhardt y, en el XX, por Benedetto Croce y Eugenio d'Ors. Algunos historiadores dividen el Barroco en tres períodos: «primitivo» (1580-1630), «maduro» o «pleno» (1630-1680) y «tardío» (1680-1750).

Aunque se suele entender como un período artístico específico, estéticamente el término «barroco» también indica cualquier estilo artístico contrapuesto al clasicismo, concepto introducido por Heinrich Wölfflin en 1915. Así pues, el término «barroco» se puede emplear tanto como sustantivo como adjetivo. Según este planteamiento, cualquier estilo artístico atraviesa por tres fases: arcaica, clásica y barroca. Ejemplos de fases barrocas serían el arte helenístico, el arte gótico, el romanticismo o el modernismo.

El arte se volvió más refinado y ornamentado, con pervivencia de un cierto racionalismo clasicista pero adoptando formas más dinámicas y efectistas y un gusto por lo sorprendente y anecdótico, por las ilusiones ópticas y los golpes de efecto. Se observa una preponderancia de la representación realista: en una época de penuria económica, el hombre se enfrenta de forma más cruda a la realidad. Por otro lado, a menudo esta cruda realidad se somete a la mentalidad de una época turbada y desengañada, lo que se manifiesta en una cierta distorsión de las formas, en efectos forzados y violentos, fuertes contrastes de luces y sombras y cierta tendencia al desequilibrio y la exageración.

Se conoce también con el nombre de barroquismo el abuso de lo ornamental, el recargamiento en el arte.

El término «barroco» proviene de un vocablo de origen portugués ("barrôco"), cuyo femenino denominaba a las perlas que tenían formas irregulares (como en castellano el vocablo «barruecas»). Surgió en el contexto de la crítica musical: en 1750, el ensayista francés Noël-Antoine Pluche comparó la forma de tocar de dos violinistas, uno más sereno y otro más extravagante, comentando de este último que «trata a toda costa de sorprender, de llamar la atención, con sonidos desaforados y extravagantes. No parece sino que de esta manera se tratase de bucear en el fondo de los mares para extraer berruecos ["baroc" en francés] con grandes esfuerzos, mientras en tierra firme sería posible encontrar con mucha más facilidad joyas valiosas».

«Barroco» fue en origen una palabra despectiva que designaba un tipo de arte caprichoso, grandilocuente, excesivamente recargado. Así apareció por vez primera en el "Dictionnaire de Trévoux" (1771), que define «en pintura, un cuadro o una figura de gusto barroco, donde las reglas y las proporciones no son respetadas y todo está representado siguiendo el capricho del artista».

Otra teoría lo deriva del sustantivo "baroco", un silogismo de origen aristotélico proveniente de la filosofía escolástica medieval, que señala una ambigüedad que, basada en un débil contenido lógico, hace confundir lo verdadero con lo falso. Así, esta figura señala un tipo de razonamiento pedante y artificioso, generalmente en tono sarcástico y no exento de polémica. En ese sentido lo aplicó Francesco Milizia en su "Dizionario delle belle arti del disegno" (1797), donde expresa que «barroco es el superlativo de bizarro, el exceso del ridículo».

El término «barroco» fue usado a partir del siglo XVIII con un sentido despectivo, para subrayar el exceso de énfasis y abundancia de ornamentación, a diferencia de la racionalidad más clara y sobria de la Ilustración. En ese tiempo, barroco era sinónimo de otros adjetivos como «absurdo» o «grotesco». Los pensadores ilustrados vieron en las realizaciones artísticas del siglo anterior una manipulación de los preceptos clasicistas, tan cercanos a su concepto racionalista de la realidad, por lo que sus críticas al arte seiscentista convirtieron el término «barroco» en un concepto peyorativo: en su "Dictionnaire d'Architecture" (1792), Antoine Chrysostome Quatremère de Quincy define lo barroco como «un matiz de lo extravagante. Es, si se quiere, su refinamiento o si se pudiese decir, su abuso. Lo que la severidad es a la sabiduría del gusto, el barroco lo es a lo extraño, es decir, que es su superlativo. La idea de barroco entraña la del ridículo llevado al exceso».

Sin embargo, la historiografía del arte tendió posteriormente a revalorizar el concepto de lo barroco y a valorarlo por sus cualidades intrínsecas, al tiempo que empezó a tratar el Barroco como un período específico de la historia de la cultura occidental. El primero en rechazar la acepción negativa del Barroco fue Jacob Burckhardt ("Cicerone", 1855), afirmando que «la arquitectura barroca habla el mismo lenguaje del Renacimiento, pero en un dialecto degenerado». Si bien no era una afirmación elogiosa, abrió el camino a estudios más objetivos, como los elaborados por Cornelius Gurlitt ("Geschichte des Barockstils in Italien", 1887), August Schmarsow ("Barock und Rokoko", 1897), Alois Riegl ("Die Entstehung der Barockkunst in Rom", 1908) y Wilhelm Pinder ("Deutscher Barock", 1912), que culminaron en la obra de Heinrich Wölfflin ("Renaissance und Barock", 1888; "Kunstgeschichtliche Grundbegriffe", 1915), el primero que otorgó al Barroco una autonomía estilística propia y diferenciada, señalando sus propiedades y rasgos estilísticos de una forma revalorizada. Posteriormente, Benedetto Croce ("Saggi sulla letteratura italiana del Seicento", 1911) efectuó un estudio historicista del Barroco, enmarcándolo en su contexto socio-histórico y cultural, y procurando no emitir ninguna clase de juicios de valor. Sin embargo, en "Storia dell'età barocca in Italia" (1929) volvió a otorgar un carácter negativo al Barroco, al que calificó de «decadente», justo en una época en que surgieron numerosos tratados que reivindicaban la valía artística del período, como "Der Barock als Kunst der Gegenreformation" (1921), de Werner Weisbach, "Österreichische Barockarchitektur" (1930) de Hans Sedlmayr o "Art religieux après le Concile de Trente" (1932), de Émile Mâle.

Posteriores estudios han dejado definitivamente asentado el concepto actual de Barroco, con pequeñas salvedades, como la diferenciación efectuada por algunos historiadores entre «barroco» y «barroquismo», siendo el primero la fase clásica, pura y primigenia, del arte del siglo XVII, y el segundo una fase amanerada, recargada y exagerada, que confluiría con el Rococó —en la misma medida que el manierismo sería la fase amanerada del Renacimiento—. En ese sentido, Wilhelm Pinder ("Das Problem der Generation in der Kunstgeschichte", 1926) sostiene que estos estilos «generacionales» se suceden sobre la base de la formulación y posterior deformación de unos determinados ideales culturales: así como el manierismo jugó con las formas clásicas de un Renacimiento de corte humanista y clasicista, el barroquismo supone la reformulación en clave formalista del sustrato ideológico barroco, basado principalmente en el absolutismo y el contrarreformismo.

Por otro lado, frente al Barroco como un determinado período de la historia de la cultura, a principios del siglo XX surgió una segunda acepción, la de «lo barroco» como una fase presente en la evolución de todos los estilos artísticos. Ya Nietzsche aseveró que «el estilo barroco surge cada vez que muere un gran arte». El primero en otorgar un sentido estético transhistórico al Barroco fue Heinrich Wölfflin ("Kunstgeschichtliche Grundbegriffe", 1915), quien estableció un principio general de alternancia entre clasicismo y barroco, que rige la evolución de los estilos artísticos.

Recogió el testigo Eugenio d'Ors ("Lo barroco", 1936), que lo definió como un «eón», una forma transhistórica del arte («"lo" barroco» frente a «"el" barroco» como período), una modalidad recurrente a todo lo largo de la historia del arte como oposición a lo clásico. Si el clasicismo es un arte racional, masculino, apolíneo, lo barroco es irracional, femenino, dionisíaco. Para d'Ors, «ambas aspiraciones [clasicismo y barroquismo] se complementan. Tiene lugar un estilo de economía y razón, y otro musical y abundante. Uno se siente atraído por las formas estables y pesadas, y el otro por las redondeadas y ascendentes. De uno a otro no hay ni decadencia ni degeneración. Se trata de dos formas de sensibilidad eternas».

El siglo XVII fue por lo general una época de depresión económica, consecuencia de la prolongada expansión del siglo anterior causada principalmente por el descubrimiento de América. Las malas cosechas conllevaron el aumento del precio del trigo y demás productos básicos, con las subsiguientes hambrunas. El comercio se estancó, especialmente en el área mediterránea, y solo floreció en Inglaterra y Países Bajos gracias al comercio con Oriente y la creación de grandes compañías comerciales, que sentaron las bases del capitalismo y el auge de la burguesía. La mala situación económica se agravó con las plagas de peste que asolaron Europa a mediados del siglo XVII, que afectaron especialmente a la zona mediterránea. Otro factor que generó miseria y pobreza fueron las guerras, provocadas en su mayoría por el enfrentamiento entre católicos y protestantes, como es el caso de la Guerra de los Treinta Años (1618-1648). Todos estos factores provocaron una grave depauperación de la población; en muchos países, el número de pobres y mendigos llegó a alcanzar la cuarta parte de la población.

Por otro lado, el poder hegemónico en Europa basculó de la España imperial a la Francia absolutista, que tras la Paz de Westfalia (1648) y la Paz de los Pirineos (1659) se consolidó como el más poderoso estado del continente, prácticamente indiscutido hasta la ascensión de Inglaterra en el siglo XVIII. Así, la Francia de los Luises y la Roma papal fueron los principales núcleos de la cultura barroca, como centros de poder político y religioso —respectivamente— y centros difusores del absolutismo y el contrarreformismo. España, aunque en decadencia política y económica, tuvo sin embargo un esplendoroso período cultural —el llamado Siglo de Oro— que, aunque marcado por su aspecto religioso de incontrovertible proselitismo contrarreformista, tuvo un acentuado componente popular, y llevó tanto a la literatura como a las artes plásticas a cotas de elevada calidad. En el resto de países donde llegó la cultura barroca (Inglaterra, Alemania, Países Bajos), su implantación fue irregular y con distintos sellos peculiarizados por sus distintivas características nacionales.
El Barroco se forjó en Italia, principalmente en la sede pontificia, Roma, donde el arte fue utilizado como medio propagandístico para la difusión de la doctrina contrarreformista. La Reforma protestante sumió a la Iglesia católica en una profunda crisis durante la primera mitad del siglo XVI, que evidenció tanto la corrupción en numerosos estratos eclesiásticos como la necesidad de una renovación del mensaje y la obra católica, así como de un mayor acercamiento a los fieles. El Concilio de Trento (1545-1563) se celebró para contrarrestar el avance del protestantismo y consolidar el culto católico en los países donde aún prevalecía, sentando las bases del dogma católico (sacerdocio sacramental, celibato, culto a la Virgen y los santos, uso litúrgico del latín) y creando nuevos instrumentos de comunicación y expansión de la fe católica, poniendo especial énfasis en la educación, la predicación y la difusión del mensaje católico, que adquirió un fuerte sello propagandístico —para lo que se creó la Congregación para la Propagación de la Fe—. Este ideario se plasmó en la recién fundada Compañía de Jesús, que mediante la predicación y la enseñanza tuvo una notable y rápida difusión por todo el mundo, frenando el avance del protestantismo y recuperando numerosos territorios para la fe católica (Austria, Baviera, Suiza, Flandes, Polonia). Otro efecto de la Contrarreforma fue la consolidación de la figura del papa, cuyo poder salió reforzado, y que se tradujo en un ambicioso programa de ampliación y renovación urbanística de Roma, especialmente de sus iglesias, con especial énfasis en la basílica de San Pedro y sus aledaños. La Iglesia fue el mayor comitente artístico de la época, y utilizó el arte como caballo de batalla de la propaganda religiosa, al ser un medio de carácter popular fácilmente accesible e inteligible. El arte fue utilizado como un vehículo de expresión "ad maiorem Dei et Ecclesiae gloriam", y papas como Sixto V, Clemente VIII, Paulo V, Gregorio XV, Urbano VIII, Inocencio X y Alejandro VII se convirtieron en grandes mecenas y propiciaron grandes mejoras y construcciones en la ciudad eterna, ya calificada entonces como "Roma triumphans, caput mundi" («Roma triunfante, cabeza del mundo»).

Culturalmente, el Barroco fue una época de grandes adelantos científicos: William Harvey comprobó la circulación de la sangre; Galileo Galilei perfeccionó el telescopio y afianzó la teoría heliocéntrica establecida el siglo anterior por Copérnico y Kepler; Isaac Newton formuló la teoría de la gravitación universal; Evangelista Torricelli inventó el barómetro. Francis Bacon estableció con su "Novum Organum" el método experimental como base de la investigación científica, poniendo las bases del empirismo. Por su parte, René Descartes llevó a la filosofía hacia el racionalismo, con su famoso «pienso, luego existo».
Debido a las nuevas teorías heliocéntricas y la consecuente pérdida del sentimiento antropocéntrico propio del hombre renacentista, el hombre del Barroco perdió la fe en el orden y la razón, en la armonía y la proporción; la naturaleza, no reglamentada ni ordenada, sino libre y voluble, misteriosa e inabarcable, pasó a ser una fuente directa de inspiración más conveniente a la mentalidad barroca. Perdiendo la fe en la verdad, todo pasa a ser aparente e ilusorio (Calderón: "La vida es sueño"); ya no hay nada revelado, por lo que todo debe investigarse y experimentarse. Descartes convirtió la duda en el punto de partida de su sistema filosófico: «considerando que todos los pensamientos que nos vienen estando despiertos pueden también ocurrírsenos durante el sueño, sin que ninguno entonces sea verdadero, resolví fingir que todas las cosas que hasta entonces habían entrado en mi espíritu, no eran más verdaderas que las ilusiones de mis sueños» ("Discurso del método", 1637). Así, mientras la ciencia se circunscribía a la búsqueda de la verdad, el arte se encaminaba a la expresión de lo imaginario, del ansia de infinito que anhelaba el hombre barroco. De ahí el gusto por los efectos ópticos y los juegos ilusorios, por las construcciones efímeras y el valor de lo transitorio; o el gusto por lo sugestivo y seductor en poesía, por lo maravilloso, sensual y evocador, por los efectos lingüísticos y sintácticos, por la fuerza de la imagen y el poder de la retórica, revitalizados por la reivindicación de autores como Aristóteles o Cicerón.

La cultura barroca era, en definición de José Antonio Maravall, «dirigida» —enfocada en la comunicación—, «masiva» —de carácter popular— y «conservadora» —para mantener el orden establecido—. Cualquier medio de expresión artístico debía ser principalmente didáctico y seductor, debía llegar fácilmente al público y debía entusiasmarle, hacerle comulgar con el mensaje que transmitía, un mensaje puesto al servicio de las instancias del poder —político o religioso—, que era el que sufragaba los costes de producción de las obras artísticas, ya que Iglesia y aristocracia —también incipientemente la burguesía— eran los principales comitentes de artistas y escritores. Si la Iglesia quería transmitir su mensaje contrarreformista, las monarquías absolutas vieron en el arte una forma de magnificar su imagen y mostrar su poder, a través de obras monumentales y pomposas que transmitían una imagen de grandeza y ayudaban a consolidar el poder centralista del monarca, reafirmando su autoridad.

Por ello y pese a la crisis económica, el arte floreció gracias sobre todo al mecenazgo eclesiástico y aristocrático. Las cortes de los estados monárquicos —especialmente los absolutistas— favorecieron el arte como una forma de plasmar la magnificencia de sus reinos, un instrumento propagandístico que daba fe de la grandiosidad del monarca (un ejemplo paradigmático es la construcción de Versalles por Luis XIV). El auge del coleccionismo, que conllevaba la circulación de artistas y obras de arte por todo el continente europeo, condujo al alza del mercado artístico. Algunos de los principales coleccionistas de arte de la época fueron monarcas, como el emperador Rodolfo II, Carlos I de Inglaterra, Felipe IV de España o la reina Cristina de Suecia. Floreció notablemente el mercado artístico, centrado principalmente en el ámbito holandés (Amberes y Ámsterdam) y alemán (Núremberg y Augsburgo). También proliferaron las academias de arte —siguiendo la estela de las surgidas en Italia en el siglo XVI—, como instituciones encargadas de preservar el arte como fenómeno cultural, de reglamentar su estudio y su conservación, y de promocionarlo mediante exposiciones y concursos; las principales academias surgidas en el siglo XVII fueron la Académie Royale d'Art, fundada en París en 1648, y la Akademie der Künste de Berlín (1696)

El Barroco fue un estilo heredero del escepticismo manierista, que se vio reflejado en un sentimiento de fatalidad y dramatismo entre los autores de la época. El arte se volvió más artificial, más recargado, decorativo, ornamentado. Destacó el uso ilusionista de los efectos ópticos; la belleza buscó nuevas vías de expresión y cobró relevancia lo asombroso y los efectos sorprendentes. Surgieron nuevos conceptos estéticos como los de «ingenio», «perspicacia» o «agudeza». En la conducta personal se destacaba sobre todo el aspecto exterior, de forma que reflejara una actitud altiva, elegante, refinada y exagerada que cobró el nombre de "préciosité".

Según Wölfflin, el Barroco se define principalmente por oposición al Renacimiento: frente a la visión lineal renacentista, la visión barroca es pictórica; frente a la composición en planos, la basada en la profundidad; frente a la forma cerrada, la abierta; frente a la unidad compositiva basada en la armonía, la subordinación a un motivo principal; frente a la claridad absoluta del objeto, la claridad relativa del efecto. Así, el Barroco «es el estilo del punto de vista pictórico con perspectiva y profundidad, que somete la multiplicidad de sus elementos a una idea central, con una visión sin límites y una relativa oscuridad que evita los detalles y los perfiles agudos, siendo al mismo tiempo un estilo que, en lugar de revelar su arte, lo esconde».

El arte barroco se expresó estilísticamente en dos vías: por un lado, hay un énfasis en la realidad, el aspecto mundano de la vida, la cotidianeidad y el carácter efímero de la vida, que se materializó en una cierta «vulgarización» del fenómeno religioso en los países católicos, así como en un mayor gusto por las cualidades sensibles del mundo circundante en los protestantes; por otro lado, se manifiesta una visión grandilocuente y exaltada de los conceptos nacionales y religiosos como una expresión del poder, que se traduce en el gusto por lo monumental, lo fastuoso y recargado, el carácter magnificente otorgado a la realeza y la Iglesia, a menudo con un fuerte sello propagandístico.
El Barroco fue una cultura de la imagen, donde todas las artes confluyeron para crear una obra de arte total, con una estética teatral, escenográfica, una "mise en scène" que pone de manifiesto el esplendor del poder dominante (Iglesia o Estado), con ciertos toques naturalistas pero en un conjunto que expresa dinamismo y vitalidad. La interacción de todas las artes expresa la utilización del lenguaje visual como un medio de comunicación de masas, plasmado en una concepción dinámica de la naturaleza y el espacio envolvente.

Una de las principales características del arte barroco es su carácter ilusorio y artificioso: «el ingenio y el diseño son el arte mágico a través del cual se llega a engañar a la vista hasta asombrar» (Gian Lorenzo Bernini). Se valoraba especialmente lo visual y efímero, por lo que cobraron auge el teatro y los diversos géneros de artes escénicas y espectáculos: danza, pantomima, drama musical (oratorio y melodrama), espectáculos de marionetas, acrobáticos, circenses, etc. Existía el sentimiento de que el mundo es un teatro ("theatrum mundi") y la vida una función teatral: «todo el mundo es un escenario, y todos los hombres y mujeres meros actores» ("Como gustéis", William Shakespeare, 1599). De igual manera se tendía a teatralizar las demás artes, especialmente la arquitectura. Es un arte que se basa en la inversión de la realidad: en la «simulación», en convertir lo falso en verdadero, y en la «disimulación», pasar lo verdadero por falso. No se muestran las cosas como son, sino como se querría que fuesen, especialmente en el mundo católico, donde la Contrarreforma tuvo un éxito exiguo, ya que media Europa se pasó al protestantismo. En literatura se manifestó dando rienda suelta al artificio retórico, como un medio de expresión propagandístico en que la suntuosidad del lenguaje pretendía reflejar la realidad de forma edulcorada, recurriendo a figuras retóricas como la metáfora, la paradoja, la hipérbole, la antítesis, el hipérbaton, la elipsis, etc. Esta transposición de la realidad, que se ve distorsionada y magnificada, alterada en sus proporciones y sometida al criterio subjetivo de la ficción, pasó igualmente al terreno de la pintura, donde se abusa del escorzo y la perspectiva ilusionista en aras de efectos mayores, llamativos y sorprendentes.
El arte barroco buscaba la creación de una realidad alternativa a través de la ficción y la ilusión. Esta tendencia tuvo su máxima expresión en la fiesta y la celebración lúdica; edificios como iglesias o palacios, o bien un barrio o una ciudad entera, se convertían en teatros de la vida, en escenarios donde se mezclaba la realidad y la ilusión, donde los sentidos se sometían al engaño y el artificio. En ese aspecto tuvo especial protagonismo la Iglesia contrarreformista, que buscaba a través de la pompa y el boato mostrar su superioridad sobre las iglesias protestantes, con actos como misas solemnes, canonizaciones, jubileos, procesiones o investiduras papales. Pero igual de fastuosas eran las celebraciones de la monarquía y la aristocracia, con eventos como coronaciones, bodas y nacimientos reales, funerales, visitas de embajadores o cualquier acontecimiento que permitiese al monarca desplegar su poder para admirar al pueblo. Las fiestas barrocas suponían una conjugación de todas las artes, desde la arquitectura y las artes plásticas hasta la poesía, la música, la danza, el teatro, la pirotecnia, arreglos florales, juegos de agua, etc. Arquitectos como Bernini o Pietro da Cortona, o Alonso Cano y Sebastián Herrera Barnuevo en España, aportaron su talento a tales eventos, diseñando estructuras, coreografías, iluminaciones y demás elementos, que a menudo les servían como campo de pruebas para futuras realizaciones más serias: así, el baldaquino para la canonización de Santa Isabel de Portugal sirvió a Bernini para su futuro diseño del baldaquino de San Pedro, y el "quarantore" (teatro sacro de los jesuitas) de Carlo Rainaldi fue una maqueta de la iglesia de Santa Maria in Campitelli.

Durante el Barroco, el carácter ornamental, artificioso y recargado del arte de este tiempo traslucía un sentido vital transitorio, relacionado con el "memento mori", el valor efímero de las riquezas frente a la inevitabilidad de la muerte, en paralelo al género pictórico de las "vanitas". Este sentimiento llevó a valorar de forma vitalista la fugacidad del instante, a disfrutar de los leves momentos de esparcimiento que otorga la vida, o de las celebraciones y actos solemnes. Así, los nacimientos, bodas, defunciones, actos religiosos, o las coronaciones reales y demás actos lúdicos o ceremoniales, se revestían de una pompa y una artificiosidad de carácter escenográfico, donde se elaboraban grandes montajes que aglutinaban arquitectura y decorados para proporcionar una magnificencia elocuente a cualquier celebración, que se convertía en un espectáculo de carácter casi catártico, donde cobraba especial relevancia el elemento ilusorio, la atenuación de la frontera entre realidad y fantasía.

Cabe destacar que el Barroco es un concepto heterogéneo que no presentó una unidad estilística ni geográfica ni cronológicamente, sino que en su seno se encuentran diversas tendencias estilísticas, principalmente en el terreno de la pintura. Las principales serían: naturalismo, estilo basado en la observación de la naturaleza pero sometida a ciertas directrices establecidas por el artista, basadas en criterios morales y estéticos o, simplemente, derivados de la libre interpretación del artista a la hora de concebir su obra; realismo, tendencia surgida de la estricta imitación de la naturaleza, ni interpretada ni edulcorada, sino representada minuciosamente hasta en sus más pequeños detalles; clasicismo, corriente centrada en la idealización y perfección de la naturaleza, evocadora de elevados sentimientos y profundas reflexiones, con la aspiración de reflejar la belleza en toda su plenitud.

Por último, cabe señalar que en el Barroco surgieron o se desarrollaron nuevos géneros pictóricos. Si hasta entonces había preponderado en el arte la representación de temas históricos, mitológicos o religiosos, los profundos cambios sociales vividos en el siglo XVII propiciaron el interés por nuevos temas, especialmente en los países protestantes, cuya severa moralidad impedía la representación de imágenes religiosas por considerarlas idolatría. Por otro lado, el auge de la burguesía, que para remarcar su estatus invirtió de forma decidida en el arte, trajo consigo la representación de nuevos temas alejados de las grandilocuentes escenas preferidas por la aristocracia. Entre los géneros desarrollados profusamente en el Barroco destacan: la pintura de género, que toma sus modelos de la realidad circundante, de la vida diaria, de temas campesinos o urbanos, de pobres y mendigos, comerciantes y artesanos, o de fiestas y ambientes folklóricos; el paisaje, que eleva a categoría independiente la representación de la naturaleza, que hasta entonces solo servía de telón de fondo de las escenas con personajes históricos o religiosos; el retrato, que centra su representación en la figura humana, generalmente con un componente realista aunque a veces no exento de idealización; el bodegón o naturaleza muerta, que consiste en la representación de objetos inanimados, ya sean piezas de ajuar doméstico, flores, frutas u otros alimentos, muebles, instrumentos musicales, etc.; y la "vanitas", un tipo de bodegón que alude a lo efímero de la existencia humana, simbolizado generalmente por la presencia de calaveras o esqueletos, o bien velas o relojes de arena.

La arquitectura barroca asumió unas formas más dinámicas, con una exuberante decoración y un sentido escenográfico de las formas y los volúmenes. Cobró relevancia la modulación del espacio, con preferencia por las curvas cóncavas y convexas, poniendo especial atención en los juegos ópticos ("trompe-l'œil") y el punto de vista del espectador. También cobró una gran importancia el urbanismo, debido a los monumentales programas desarrollados por reyes y papas, con un concepto integrador de la arquitectura y el paisaje que buscaba la recreación de un "continuum" espacial, de la expansión de las formas hacia el infinito, como expresión de unos elevados ideales, sean políticos o religiosos.

Al igual que en la época anterior, el motor del nuevo estilo volvió a ser Italia, gracias principalmente al mecenazgo de la Iglesia y a los grandes programas arquitectónicos y urbanísticos desarrollados por la sede pontificia, deseosa de mostrar al mundo su victoria contra la Reforma. La principal modalidad constructiva de la arquitectura barroca italiana fue la iglesia, que se convirtió en el máximo exponente de la propaganda contrarreformista. Las iglesias barrocas italianas se caracterizan por la abundancia de formas dinámicas, con predominio de las curvas cóncavas y convexas, con fachadas ricamente decoradas y repletas de esculturas, así como gran número de columnas, que a menudo se desprenden del muro, y con interiores donde predominan igualmente la forma curva y una profusa decoración. Entre sus diversas planimetrías destacó —especialmente entre finales del siglo XVI y principios del XVII— el diseño en dos cuerpos, con dos frontones concéntricos (curvo el exterior y triangular el interior), siguiendo el modelo de la fachada de la Iglesia del Gesù de Giacomo della Porta (1572).

Uno de sus primeros representantes fue Carlo Maderno, autor de la fachada de San Pedro del Vaticano (1607-1612) —al que además modificó la planta, pasando de la de cruz griega proyectada por Bramante a una de cruz latina—, y la iglesia de Santa Susana (1597-1603). Pero uno de los mayores impulsores del nuevo estilo fue el arquitecto y escultor Gian Lorenzo Bernini, el principal artífice de la Roma monumental que conocemos hoy día: baldaquino de San Pedro (1624-1633) —donde aparece la columna salomónica, posteriormente uno de los signos distintivos del Barroco—, columnata de la plaza de San Pedro (1656-1667), San Andrés del Quirinal (1658-1670), Palacio Chigi-Odescalchi (1664-1667). El otro gran nombre de la época es Francesco Borromini, arquitecto de gran inventiva que subvirtió todas las normas de la arquitectura clásica —a las que pese a todo aún se aferraba Bernini—, a través del uso de superficies alabeadas, bóvedas nervadas y arcos mixtilíneos, creando una arquitectura de carácter casi escultórico. Fue autor de las iglesias de San Carlo alle Quattre Fontane (1634-1640), Sant'Ivo alla Sapienza (1642-1650) y Sant'Agnese in Agone (1653-1661). El tercer arquitecto de renombre activo en Roma fue Pietro da Cortona, que también era pintor, circunstancia quizá por la cual creó volúmenes de gran plasticidad, con grandes contrastes de luz y sombra (Santa Maria della Pace, 1656-1657; Santi Luca e Martina, 1635-1650). Fuera de Roma cabe destacar la figura de Baldassare Longhena en Venecia, autor de la iglesia de Santa Maria della Salute (1631-1650); y Guarino Guarini y Filippo Juvara en Turín, autor de la Capilla del Santo Sudario (1667-1690) el primero, y de la Basílica de Superga (1717-1731) el segundo.

En Francia, bajo los reinados de Luis XIII y Luis XIV, se iniciaron una serie de construcciones de gran fastuosidad, que pretendían mostrar la grandeza del monarca y el carácter sublime y divino de la monarquía absolutista. Aunque en la arquitectura francesa se percibe cierta influencia de la italiana, esta fue reinterpretada de una forma más sobria y equilibrada, más fiel al clasicismo renacentista, por lo que el arte francés de la época se suele denominar como clasicismo francés.

Las primeras realizaciones de relevancia corrieron a cargo de Jacques Lemercier (iglesia de la Sorbona, 1635) y François Mansart (palacio de Maisons-Lafitte, 1624-1626; Iglesia de Val-de-Grâce, 1645-1667). Posteriormente, los grandes programas áulicos se centraron en la nueva fachada del palacio del Louvre, de Louis Le Vau y Claude Perrault (1667-1670) y, especialmente, en el palacio de Versalles, de Le Vau y Jules Hardouin-Mansart (1669-1685). De este último arquitecto conviene también destacar la iglesia de San Luis de los Inválidos (1678-1691), así como el trazado de la plaza Vendôme de París (1685-1708).

En España, la arquitectura de la primera mitad del siglo XVII acusó la herencia herreriana, con una austeridad y simplicidad geométrica de influencia escurialense. Lo barroco se fue introduciendo paulatinamente sobre todo en la recargada decoración interior de iglesias y palacios, donde los retablos fueron evolucionando hacia cotas de cada vez más elevada magnificencia. En este período fue Juan Gómez de Mora la figura más destacada, siendo autor de la Clerecía de Salamanca (1617), el Ayuntamiento (1644-1702) y la plaza Mayor de Madrid (1617-1619). Otros autores de la época fueron: Alonso Carbonel, autor del palacio del Buen Retiro (1630-1640); Pedro Sánchez y Francisco Bautista, autores de la Colegiata de San Isidro de Madrid (1620-1664).

Hacia mediados de siglo fueron ganando terreno las formas más ricas y los volúmenes más libres y dinámicos, con decoraciones naturalistas (guirnaldas, cartelas vegetales) o de formas abstractas (molduras y baquetones recortados, generalmente de forma mixtilínea). En esta época conviene recordar los nombres de Pedro de la Torre, José de Villarreal, José del Olmo, Sebastián Herrera Barnuevo y, especialmente, Alonso Cano, autor de la fachada de la catedral de Granada (1667).

Entre finales del siglo XVII y comienzos del siglo XVIII se dio el estilo churrigueresco (por los hermanos Churriguera), caracterizado por su exuberante decorativismo y el uso de columnas salomónicas: José Benito Churriguera fue autor del Retablo Mayor de San Esteban de Salamanca (1692) y la fachada del palacio-iglesia de Nuevo Baztán en Madrid (1709-1722); Alberto Churriguera proyectó la Plaza Mayor de Salamanca (1728-1735); y Joaquín Churriguera fue autor del Colegio de Calatrava (1717) y el claustro de San Bartolomé (1715) en Salamanca, de influencia plateresca. Otras figuras de la época fueron: Teodoro Ardemans, autor de la fachada del Ayuntamiento de Madrid y el primer proyecto para el Palacio Real de La Granja de San Ildefonso (1718-1726); Pedro de Ribera, autor del Puente de Toledo (1718-1732), el Cuartel del Conde-Duque (1717) y la fachada de la iglesia de Nuestra Señora de Montserrat de Madrid (1720); Narciso Tomé, autor del Transparente de la Catedral de Toledo (1721-1734); el alemán Konrad Rudolf, autor de la fachada de la Catedral de Valencia (1703); Jaime Bort, artífice de la fachada de la catedral de Murcia (1736-1753); Vicente Acero, que proyectó la catedral de Cádiz (1722-1762); y Fernando de Casas Novoa, autor de la fachada del Obradoiro de la catedral de Santiago de Compostela (1739-1750).

En Alemania, hasta mediados de siglo no se iniciaron construcciones de relevancia, debido a la Guerra de los Treinta Años, y aún entonces las principales obras fueron encargadas a arquitectos italianos. Sin embargo, a finales de siglo hubo una eclosión de arquitectos alemanes de gran valía, que hicieron obras cuyas innovadoras soluciones apuntaban ya al Rococó: Andreas Schlüter, autor del Palacio Real de Berlín (1698-1706), de influencia versallesca; Matthäus Daniel Pöppelmann, autor del palacio Zwinger de Dresde (1711-1722); y Georg Bähr, autor de la Iglesia de Frauenkirche de Dresde (1722-1738). En Austria destacaron Johann Bernhard Fischer von Erlach, autor de la iglesia de San Carlos Borromeo en Viena (1715-1725); Johann Lukas von Hildebrandt, autor del palacio Belvedere de Viena (1713-1723); y Jakob Prandtauer, artífice de la abadía de Melk (1702-1738). En Suiza cabe nombrar la abadía de Einsiedeln (1691-1735), de Kaspar Moosbrugger; la iglesia de los jesuitas de Solothurn (1680), de Heinrich Mayer; y la Colegiata de Sankt Gallen (1721-1770), de Kaspar Moosbrugger, Michael Beer y Peter Thumb.

En Inglaterra pervivió durante buena parte del siglo XVII un clasicismo renacentista de influencia palladiana, cuyo máximo representante fue Inigo Jones. Posteriormente se fueron introduciendo las nuevas formas del continente, aunque reinterpretadas nuevamente con un sentido de mesura y contención pervivientes de la tradición palladiana. En ese sentido la obra maestra del período fue la catedral de San Pablo de Londres (1675-1711), de Christopher Wren. Otras obras de relevancia serían el castillo de Howard (1699-1712) y el palacio de Blenheim (1705-1725), ambos de John Vanbrugh y Nicholas Hawksmoor.

En Flandes, las formas barrocas, presentes en un desbordado decorativismo, convivieron con antiguas estructuras góticas, órdenes clásicos y decoración manierista: cabe destacar las iglesias de Saint-Loup de Namur (1621), Sint-Michiel de Lovaina (1650-1666), Saint-Jean-Baptiste de Bruselas (1657-1677) y Sint-Pieter de Malinas (1670-1709). En los Países Bajos, el calvinismo determinó una arquitectura más simple y austera, de líneas clásicas, con preponderancia de la arquitectura civil: Bolsa de Ámsterdam (1608), de Hendrik de Keyser; Palacio Mauritshuis de La Haya (1633-1644), de Jacob van Campen; Ayuntamiento de Ámsterdam (1648, actual Palacio Real), de Jacob van Campen.

En los países nórdicos, el protestantismo propició igualmente una arquitectura sobria y de corte clásico, con modelos importados de otros países, y características propias tan solo perceptibles en la utilización de diversos materiales, como los muros combinados de ladrillo y piedra de cantería, o los techos de cobre. En Dinamarca destacan el edificio de la Bolsa de Copenhague (1619-1674), de Hans van Steenwinkel el Joven; y la iglesia de Federico V (1754-1894), de Nicolai Eigtved. En Suecia cabe destacar el palacio de Drottningholm (1662-1685) y la iglesia de Riddarholm (1671), de Nicodemus Tessin el Viejo, y el palacio Real de Estocolmo (1697-1728), de Nicodemus Tessin el Joven.

En Portugal, hasta mediados de siglo —con la independencia de España— no se inició una actividad constructora de envergadura, favorecida por el descubrimiento de minas de oro y diamantes en Minas Gerais (Brasil), que llevó al rey Juan V a querer emular las cortes de Versalles y el Vaticano. Entre las principales construcciones destacan: el Palacio Nacional de Mafra (1717-1740), de Johann Friederich Ludwig; el palacio Real de Queluz (1747), de Mateus Vicente; y el Santuario de Bom Jesus do Monte, en Braga (1784-1811), de Manuel Pinto Vilalobos.

En Europa oriental, Praga (Chequia) fue una de las ciudades con un mayor programa constructivo, favorecido por la aristocracia checa: palacio Czernin (1668-1677), de Francesco Caratti; palacio Arzobispal (1675-1679), de Jean-Baptiste Mathey; iglesia de San Nicolás (1703-1717), de Christoph Dientzenhofer; Santuario de la Virgen de Loreto (1721), de Christoph y Kilian Ignaz Dietzenhofer. En Polonia destacan la catedral de San Juan Bautista de Breslavia (1716-1724), de Fischer von Erlach; el palacio Krasiński (1677-1682), de Tylman van Gameren; y el [palacio de Wilanów]] (1692), de Agostino Locci y Andreas Schlüter. En Rusia, donde el zar Pedro I el Grande llevó a cabo un proceso de occidentalización del estado, se recibió la influencia del barroco noreuropeo, cuyo principal exponente fue la Catedral de San Pedro y San Pablo de San Petersburgo (1703-1733), obra del arquitecto italiano Domenico Trezzini. Más tarde, Francesco Bartolomeo Rastrelli fue el exponente de un barroco tardío de influencia francoitaliana, que ya apuntaba al Rococó: Palacio de Peterhof, llamado «el Versalles ruso» (1714-1764, iniciado por Le Blond); Palacio de Invierno en San Petersburgo (1754-1762); y Palacio de Catalina en Tsárskoye Seló (1752-1756). En Ucrania, el Barroco se distingue del occidental por medio de una ornamentación más moderada y unas formas más simples: monasterio de las Cuevas de Kiev, monasterio de San Miguel de Vydubichi en Kiev. En el Imperio Otomano el arte occidental influyó durante el siglo XVIII a las tradicionales formas islámicas, como se denota en la mezquita de los Tulipanes (1760-1763), obra de Mehmet Tahir Ağa. Otro exponente fue la mezquita Nuruosmaniye (1748-1755), obra del arquitecto griego Simon el Rum y patrocinada por el sultán Mahmud I, el cual mandó traer planos de iglesias europeas para su construcción.

La arquitectura barroca colonial se caracteriza por una profusa decoración (Portada de La Profesa, México; fachadas revestidas de azulejos del estilo de Puebla, como en San Francisco Acatepec en San Andrés Cholula y San Francisco de Puebla), que resultará exacerbada en el llamado «ultrabarroco» (Fachada del Sagrario de la Catedral de México, de Lorenzo Rodríguez; Iglesia de Tepotzotlán; Templo de Santa Prisca de Taxco). En Perú, las construcciones desarrolladas en Lima y Cuzco desde 1650 muestran unas características originales que se adelantan incluso al Barroco europeo, como en el uso de muros almohadillados y de columnas salomónicas (Iglesia de la Compañía, Cuzco; San Francisco, Lima). En otros países destacan: la catedral Metropolitana de Sucre en Bolivia; el santuario del Señor de Esquipulas en Guatemala; la catedral de Tegucigalpa en Honduras; la catedral de León en Nicaragua; la Iglesia de la Compañía en Quito, Ecuador; la iglesia de San Ignacio en Bogotá, Colombia; la catedral de Caracas en Venezuela; la Audiencia de Buenos Aires en Argentina; la iglesia de Santo Domingo en Santiago de Chile; y la catedral de La Habana en Cuba. También conviene recordar la calidad de las iglesias de las misiones jesuitas en Paraguay y de las misiones franciscanas en California.

En Brasil, al igual que en la metrópoli, Portugal, la arquitectura tiene una cierta influencia italiana, generalmente de tipo borrominesco, como se percibe en las iglesias de San Pedro dos Clérigos en Recife (1728) y Nuestra Señora de la Gloria en Outeiro (1733). En la región de Minas Gerais destacó la labor de Aleijadinho, autor de un conjunto de iglesias que destacan por su planimetría curva, fachadas con efectos dinámicos cóncavo-convexos y un tratamiento plástico de todos los elementos arquitectónicos (São Francisco de Assis en Ouro Preto, 1765-1775).

En las colonias portuguesas de la India (Goa, Damao y Diu) floreció un estilo arquitectónico de formas barrocas mezcladas con elementos hindúes, como la catedral de Goa (1562-1619) y la basílica del Buen Jesús de Goa (1594-1605), que alberga la tumba de San Francisco Javier. El conjunto de iglesias y conventos de Goa fue declarado Patrimonio de la Humanidad en 1986.

En Filipinas destacan las iglesias barrocas de Filipinas (designadas como Patrimonio de la Humanidad en 1993), con un estilo que es una reinterpretación de la arquitectura barroca europea por los chinos y los artesanos filipinos: iglesia de San Agustín (Manila), iglesia de Nuestra Señora de la Asunción (Santa María, Ilocos Sur), iglesia de San Agustín (Paoay, Ilocos Norte) e Iglesia de Santo Tomás de Villanueva (Miagao, Iloílo).

Durante el Barroco la jardinería estuvo muy vinculada a la arquitectura, con diseños racionales donde cobró preferencia el gusto por la forma geométrica. Su paradigma fue el jardín francés, caracterizado por mayores zonas de césped y un nuevo detalle ornamental, el parterre, como en los Jardines de Versalles, diseñados por André Le Nôtre. El gusto barroco por la teatralidad y la artificiosidad conllevó la construcción de diversos elementos accesorios al jardín, como islas y grutas artificiales, teatros al aire libre, "ménageries" de animales exóticos, pérgolas, arcos triunfales, etc. Surgió la "orangerie", una construcción de grandes ventanales destinada a proteger en invierno naranjos y otras plantas de origen meridional. El modelo de Versalles fue copiado por las grandes cortes monárquicas europeas, con exponentes como los jardines de Schönbrunn (Viena), Charlottenburg (Berlín), La Granja (Segovia) y Petrodvorets (San Petersburgo).

La escultura barroca adquirió el mismo carácter dinámico, sinuoso, expresivo y ornamental que la arquitectura —con la que llegará a una perfecta simbiosis sobre todo en edificios religiosos—, destacando el movimiento y la expresión, partiendo de una base naturalista pero deformada a capricho del artista. La evolución de la escultura no fue uniforme en todos los países, ya que en ámbitos como España y Alemania, donde el arte gótico había tenido mucho asentamiento —especialmente en la imaginería religiosa—, aún pervivían ciertas formas estilísticas de la tradición local, mientras que en países donde el Renacimiento había supuesto la implantación de las formas clásicas (Italia y Francia) la perduración de estas es más acentuada. Por temática, junto a la religiosa tuvo bastante importancia la mitológica, sobre todo en palacios, fuentes y jardines.

En Italia destacó nuevamente Gian Lorenzo Bernini, escultor de formación aunque trabajase como arquitecto por encargo de varios papas. Influido por la escultura helenística —que en Roma podía estudiar a la perfección gracias a las colecciones arqueológicas papales—, logró una gran maestría en la expresión del movimiento, en la fijación de la acción parada en el tiempo. Fue autor de obras tan relevantes como "Eneas, Anquises y Ascanio huyendo de Troya" (1618-1619), "El rapto de Proserpina" (1621-1622), "Apolo y Dafne" (1622-1625), "David lanzando su honda" (1623-1624), el Sepulcro de Urbano VIII (1628-1647), "Éxtasis de Santa Teresa" (1644-1652), la Fuente de los Cuatro Ríos en Piazza Navona (1648-1651) y "Muerte de la beata Ludovica Albertoni" (1671-1674). Otros escultores de la época fueron: Stefano Maderno, a caballo entre el Manierismo y el Barroco ("Santa Cecilia", 1600); François Duquesnoy, flamenco de nacimiento pero activo en Roma ("San Andrés", 1629-1633); Alessandro Algardi, formado en la escuela boloñesa, de corte clásico ("Decapitación de San Pablo", 1641-1647; "El papa San León deteniendo a Atila", 1646-1653); y Ercole Ferrata, discípulo de Bernini ("La muerte en la hoguera de Santa Inés", 1660).

En Francia la escultura fue heredera del clasicismo renacentista, con preeminencia del aspecto decorativo y cortesano, y de la temática mitológica. Jacques Sarrazin se formó en Roma, donde estudió la escultura clásica y la obra de Miguel Ángel, cuya influencia se trasluce en sus "Cariátides" del Pavillon de l'Horloge del Louvre (1636). François Girardon trabajó en la decoración de Versalles, y es recordado por su "Mausoleo del Cardenal Richelieu" (1675-1694) y por el grupo de "Apolo y las Ninfas" de Versalles (1666-1675), inspirado en el "Apolo de Belvedere" de Leócares ("circa" -). Antoine Coysevox también participó en el proyecto versallesco, y entre su producción destaca la "Glorificación de Luis XIV" en el Salón de la Guerra de Versalles (1678) y el "Mausoleo de Mazarino" (1689-1693). Pierre Puget fue el más original de los escultores franceses de la época, aunque no trabajó en París, y su gusto por el dramatismo y el movimiento violento le alejaron del clasicismo de su entorno: "Milón de Crotona" (1671-1682), inspirada en el "Laocoonte".
En España perduró la imaginería religiosa de herencia gótica, generalmente en madera policromada —a veces con el añadido de ropajes auténticos—, presente o bien en retablos o bien en figura exenta. Se suelen distinguir en una primera fase dos escuelas: la castellana, centrada en Madrid y Valladolid, donde destaca Gregorio Fernández, que evoluciona de un manierismo de influencia juniana a un cierto naturalismo ("Cristo yacente", 1614; "Bautismo de Cristo", 1630), y Manuel Pereira, de corte más clásico ("San Bruno", 1652); en la escuela andaluza, activa en Sevilla y Granada, destacan: Juan Martínez Montañés, con un estilo clasicista y figuras que denotan un detallado estudio anatómico ("Cristo crucificado", 1603; "Inmaculada Concepción", 1628-1631); su discípulo Juan de Mesa, más dramático que el maestro ("Jesús del Gran Poder", 1620); Alonso Cano, también discípulo de Montañés, y como él de un contenido clasicismo ("Inmaculada Concepción", 1655; "San Antonio de Padua", 1660-1665); y Pedro de Mena, discípulo de Cano, con un estilo sobrio pero expresivo ("Magdalena penitente", 1664). Desde mediados de siglo se produce el «pleno barroco», con una fuerte influencia berniniana, con figuras como Pedro Roldán (Retablo Mayor del Hospital de la Caridad de Sevilla, 1674) y Pedro Duque Cornejo (Sillería del coro de la Catedral de Córdoba, 1748). Ya en el siglo XVIII destacó la escuela levantina en Murcia y Valencia, con nombres como Ignacio Vergara o Nicolás de Bussi, y la figura principal de Francisco Salzillo, con un estilo sensible y delicado que apunta al rococó ("Oración del Huerto", 1754; "Prendimiento", 1763).

En Alemania meridional y Austria la escultura tuvo un gran auge en el siglo XVII gracias al impulso contrarreformista, tras la anterior iconoclasia protestante. En un principio las obras más relevantes fueron encargadas a artistas holandeses, como Adriaen de Vries ("Aflicción de Cristo", 1607). Como nombres alemanes cabe destacar a: Hans Krumper ("Patrona Bavariae", 1615); Hans Reichle, discípulo de Giambologna (coro y grupo de "La Crucifixión" de la catedral de San Ulrico y Santa Afra de Augsburgo, 1605); Georg Petel ("Ecce Homo", 1630); Justus Glesker ("Grupo de la Crucifixión", 1648-1649); y el también arquitecto Andreas Schlüter, que recibe la influencia berniniana ("Estatua ecuestre del Gran Elector Federico Guillermo I de Brandemburgo", 1689-1703). En Inglaterra se combinó la influencia italiana, presente especialmente en el dinámico dramatismo de los monumentos funerarios, y la francesa, cuyo clasicismo es más apropiado para las estatuas y los retratos. El escultor inglés más importante de la época fue Nicholas Stone, formado en Holanda, autor de monumentos funerarios como el de Lady Elisabeth Carey (1617-1618) o el de "sir" William Curle (1617).

En los Países Bajos la escultura barroca se limitó a un único nombre de fama internacional, el también arquitecto Hendrik de Keyser, formado en el manierismo italiano ("Monumento funerario de Guillermo I", 1614-1622). En Flandes en cambio sí surgieron numerosos escultores, muchos de los cuales se instalaron en el país vecino, como Artus Quellinus, autor de la decoración escultórica del Ayuntamiento de Ámsterdam. Otros escultores flamencos fueron: Lukas Fayd'herbe ("Tumba del arzobispo André Cruesen", 1666); Rombout Verhulst ("Tumba de Johan Polyander van Kerchoven", 1663); y Hendrik Frans Verbruggen (Púlpito de la Catedral de San Miguel y Santa Gúdula de Bruselas, 1695-1699).

En América destacó la obra escultórica desarrollada en Lima, con autores como el catalán Pedro de Noguera, inicialmente de estilo manierista, que evolucionó hacia el Barroco en obras como la sillería de la catedral de Lima; el vallisoletano Gomes Hernández Galván, autor de las Tablas de la Catedral; Juan Bautista Vásquez, autor de una escultura de la Virgen conocida como "La Rectora", actualmente en el Instituto Riva-Agüero; y Diego Rodrigues, autor de la imagen de la "Virgen de Copacabana" en el Santuario homónimo del Distrito del Rímac de Lima. En México destacó el zamorano Jerónimo de Balbás, autor del Retablo de los Reyes de la Catedral Metropolitana de la Ciudad de México. En Ecuador destacó la escuela quiteña, representada por Bernardo de Legarda y Manuel Chili (apodado Caspicara). En Brasil destacó nuevamente la figura del Aleijadinho, que se encargó de la decoración escultórica de sus proyectos arquitectónicos, como la iglesia de São Francisco de Assis en Ouro Preto, donde realizó las esculturas de la fachada, el púlpito y el altar; o el Santuario del Buen Jesús de Congonhas, donde destacan las figuras de los doce profetas.

La pintura barroca tuvo un marcado acento diferenciador geográfico, ya que su desarrollo se produjo por países, en diversas escuelas nacionales cada una con un sello distintivo. Sin embargo, se percibe una influencia común proveniente nuevamente de Italia, donde surgieron dos tendencias contrapuestas: el naturalismo (también llamado caravagismo), basado en la imitación de la realidad natural, con cierto gusto por el claroscuro —el llamado tenebrismo—; y el clasicismo, que es igual de realista pero con un concepto de la realidad más intelectual e idealizado. Posteriormente, en el llamado «pleno barroco» (segunda mitad del siglo XVII), la pintura evolucionó a un estilo más decorativo, con predominio de la pintura mural y cierta predilección por los efectos ópticos ("trompe-l'oeil") y las escenografías lujosas y exuberantes.

Como hemos visto, en un primer lugar surgieron dos tendencias contrapuestas, naturalismo y clasicismo. La primera tuvo su máximo exponente en Caravaggio, un artista original y de vida azarosa que, pese a su prematura muerte, dejó numerosas obras maestras en las que se sintetizan la descripción minuciosa de la realidad y el tratamiento casi vulgar de los personajes con una visión no exenta de reflexión intelectual. Igualmente fue introductor del tenebrismo, donde los personajes destacan sobre un fondo oscuro, con una iluminación artificial y dirigida, de efecto teatral, que hace resaltar los objetos y los gestos y actitudes de los personajes. Entre las obras de Caravaggio destacan: "Crucifixión de San Pedro" (1601), "La vocación de San Mateo" (1602), "Entierro de Cristo" (1604), etc. Otros artistas naturalistas fueron: Bartolomeo Manfredi, Carlo Saraceni, Giovanni Battista Caracciolo y Orazio y Artemisia Gentileschi. También cabe mencionar, en relación con este estilo, un género de pinturas conocido como «bambochadas» (por el pintor holandés establecido en Roma Pieter van Laer, apodado "il Bamboccio"), que se centra en la representación de personajes vulgares como mendigos, gitanos, borrachos o vagabundos.

La segunda tendencia fue el clasicismo, que surgió en Bolonia, en torno a la denominada escuela boloñesa, iniciada por los hermanos Annibale y Agostino Carracci. Esta tendencia suponía una reacción contra el manierismo, buscando una representación idealizada de la naturaleza, representándola no como es, sino como debería ser. Perseguía como único objetivo la belleza ideal, para lo que se inspiraron en al arte clásico grecorromano y el arte renacentista. Este ideal encontró un tema idóneo de representación en el paisaje, así como en temas históricos y mitológicos. Los hermanos Carracci trabajaron juntos en un principio (frescos del Palazzo Fava de Bolonia), hasta que Annibale fue llamado a Roma para decorar la bóveda del Palazzo Farnese (1597-1604), que por su calidad ha sido comparada con la Capilla Sixtina. Otros miembros de la escuela fueron: Guido Reni ("Hipómenes y Atalanta", 1625), Domenichino ("La caza de Diana", 1617), Francesco Albani ("Los Cuatro Elementos", 1627), Guercino ("La Aurora", 1621-1623) y Giovanni Lanfranco ("Asunción de la Virgen", 1625-1627).

Por último, en el «pleno barroco» culminó el proceso iniciado en la arquitectura y la escultura, tendentes a la monumentalidad y el decorativismo, a la figuración recargada y ampulosa, con gusto por el "horror vacui" y los efectos ilusionistas. Uno de sus grandes maestros fue el también arquitecto Pietro da Cortona, influido por la pintura veneciana y flamenca, autor de la decoración de los palacios Barberini y Pamphili en Roma y Pitti en Florencia. Otros artistas fueron: il Baciccia, autor de los frescos de la iglesia del Gesù (1672-1683); Andrea Pozzo, que decoró la bóveda de la iglesia de San Ignacio de Roma (1691-1694); y el napolitano Luca Giordano, artífice de la decoración del Palazzo Medici-Riccardi de Florencia (1690), y que también trabajó en España, donde es conocido como Lucas Jordán.

En Francia también se dieron las dos corrientes surgidas en Italia, el naturalismo y el clasicismo, aunque el primero no tuvo excesivo predicamento, debido al gusto clasicista del arte francés desde el Renacimiento, y se dio principalmente en provincias y en círculos burgueses y eclesiásticos, mientras que el segundo fue adoptado como «arte oficial» por la monarquía y la aristocracia, que le dieron unas señas de identidad propias con la acuñación del término clasicismo francés. El principal pintor naturalista fue Georges de La Tour, en cuya obra se distinguen dos fases, una centrada en la representación de tipos populares y escenas jocosas, y otra donde predomina la temática religiosa, con un radical tenebrismo donde las figuras se vislumbran con tenues luces de velas o lámparas de bujía: "Magdalena penitente" (1638-1643), "San Sebastián cuidado por Santa Irene" (1640). También se engloban en esta corriente los hermanos Le Nain (Antoine, Louis y Mathieu), centrados en la temática campesina pero alejados del tenebrismo, y con cierta influencia bambochante.

La pintura clasicista se centra en dos grandes pintores que desarrollaron la mayor parte de su carrera en Roma: Nicolas Poussin y Claude Lorrain. El primero recibió la influencia de la pintura rafaelesca y de la escuela boloñesa, y creó un tipo de representación de escenas —de temática generalmente mitológica— donde evoca el esplendoroso pasado de la antigüedad grecorromana como un paraíso idealizado de perfección, una edad dorada de la humanidad, en obras como: "El triunfo de Flora" (1629) y "Los pastores de la Arcadia" (1640). Por su parte, Lorrain reflejó en su obra un nuevo concepto en la elaboración del paisaje basándose en referentes clásicos —el denominado «paisaje ideal»—, que evidencia una concepción ideal de la naturaleza y del hombre. En sus obras destaca la utilización de la luz, a la que otorga una importancia primordial a la hora de concebir el cuadro: "Paisaje con el embarque en Ostia de Santa Paula Romana" (1639), "Puerto con el embarque de la Reina de Saba" (1648).

En el pleno barroco la pintura se enmarcó más en el círculo áulico, donde se encaminó principalmente hacia el retrato, con artistas como Philippe de Champaigne ("Retrato del cardenal Richelieu", 1635-1640), Hyacinthe Rigaud ("Retrato de Luis XIV", 1701) y Nicolas de Largillière ("Retrato de Voltaire joven", 1718). Otra vertiente fue la de la pintura académica, que buscaba sentar las bases del oficio pictórico sobre la base de unos ideales clasicistas que, a la larga, acabaron constriñéndolo en unas rígidas fórmulas repetitivas. Algunos de sus representantes fueron: Simon Vouet ("Presentación de Jesús en el templo", 1641), Charles Le Brun ("Entrada de Alejandro Magno en Babilonia", 1664), Pierre Mignard ("Perseo y Andrómeda", 1679), Antoine Coypel ("Luis XIV descansando después de la Paz de Nimega", 1681) y Charles de la Fosse ("Rapto de Proserpina", 1673).

En España, pese a la decadencia económica y política, la pintura alcanzó cotas de gran calidad, por lo que se suele hablar, en paralelo a la literatura, de un «Siglo de Oro» de la pintura española. La mayor parte de la producción fue de temática religiosa, practicándose en menor medida la pintura de género, el retrato y el bodegón —especialmente "vanitas"—. Se percibe la influencia italiana y flamenca, que llega sobre todo a través de estampas: la primera se produce en la primera mitad del siglo XVII, con predominio del naturalismo tenebrista; y la segunda en el siguiente medio siglo y principios del XVIII, de procedencia rubeniana.

En la primera mitad de siglo destacan tres escuelas: la castellana (Madrid y Toledo), la andaluza (Sevilla) y la valenciana. La primera tiene un fuerte sello cortesano, por ser la sede de la monarquía hispánica, y denota todavía una fuerte influencia escurialense, perceptible en el estilo realista y austero del arte producido en esa época. Algunos de sus representantes son: Bartolomé y Vicente Carducho, Eugenio Cajés, Juan van der Hamen y Juan Bautista Maíno, en Madrid; Luis Tristán, Juan Sánchez Cotán y Pedro Orrente, en Toledo. En Valencia destacó Francisco Ribalta, con un estilo realista y colorista, de temática contrarreformista ("San Bruno", 1625). También se suele incluir en esta escuela, aunque trabajó principalmente en Italia, a José de Ribera, de estilo tenebrista pero con un colorido de influencia veneciana ("Sileno borracho", 1626; "El martirio de San Felipe", 1639). En Sevilla, tras una primera generación que aún denota la influencia renacentista (Francisco Pacheco, Juan de Roelas, Francisco de Herrera el Viejo), surgieron tres grandes maestros que elevaron la pintura española de la época a cotas de gran altura: Francisco de Zurbarán, Alonso Cano y Diego Velázquez. Zurbarán se dedicó principalmente a la temática religiosa —sobre todo en ambientes monásticos—, aunque también practicó el retrato y el bodegón, con un estilo simple pero efectista, de gran atención al detalle: "San Hugo en el refectorio de los Cartujos" (1630), "Fray Gonzalo de Illescas" (1639), "Santa Casilda" (1640). Alonso Cano, también arquitecto y escultor, evolucionó de un acentuado tenebrismo a un cierto clasicismo de inspiración veneciana: "Cristo muerto en brazos de un ángel" (1650), "Presentación de la Virgen en el Templo" (1656).

Diego Velázquez fue sin duda el artista de mayor genio de la época en España, y de los de más renombre a nivel internacional. Se formó en Sevilla, en el taller del que sería su suegro, Francisco Pacheco, y sus primeras obras de enmarcan en el estilo naturalista de moda en la época. En 1623 se estableció en Madrid, donde se convirtió en pintor de cámara de Felipe IV, y su estilo fue evolucionando gracias al contacto con Rubens (al que conoció en 1628) y al estudio de la escuela veneciana y el clasicismo boloñés, que conoció en un viaje a Italia en 1629-1631. Entonces abandonó el tenebrismo y se aventuró en un profundo estudio de la iluminación pictórica, de los efectos de luz tanto en los objetos como en el medio ambiente, con los que alcanza cotas de gran realismo en la representación de sus escenas, que sin embargo no está exento de un aire de idealización clásica, que muestra un claro trasfondo intelectual que para el artista era una reivindicación del oficio de pintor como actividad creativa y elevada. Entre sus obras destacan: "El aguador de Sevilla" (1620), "Los borrachos" (1628-1629), "La fragua de Vulcano" (1630), "La rendición de Breda" (1635), "Cristo crucificado" (1639), "Venus del espejo" (1647-1651), "Retrato de Inocencio X" (1649), "Las meninas" (1656) y "Las hilanderas" (1657).

En la segunda mitad de siglo los principales focos artísticos fueron Madrid y Sevilla. En la capital, el naturalismo fue sustituido por el colorido flamenco y el decorativismo del pleno barroco italiano, con artistas como: Antonio de Pereda ("El sueño del caballero", 1650); Juan Ricci ("Inmaculada Concepción", 1670); Francisco de Herrera el Mozo ("Apoteosis de San Hermenegildo", 1654); Juan Carreño de Miranda ("Fundación de la Orden Trinitaria", 1666); Juan de Arellano ("Florero", 1660); José Antolínez ("El tránsito de la Magdalena", 1670); Claudio Coello ("Carlos II adorando la Sagrada Forma", 1685); y Antonio Palomino (decoración del Sagrario de la Cartuja de Granada, 1712). En Sevilla destacó la obra de Bartolomé Esteban Murillo, centrado en la representación sobre todo de Inmaculadas y Niños Jesús —aunque también realizó retratos, paisajes y escenas de género—, con un tono delicado y sentimentalista, pero de gran maestría técnica y virtuosismo cromático: "Adoración de los pastores" (1650); "Inmaculada Concepción" (1678). Junto a él destacó Juan de Valdés Leal, antítesis de la belleza murillesca, con su predilección por las "vanitas" y un estilo dinámico y violento, que desprecia el dibujo y se centra en el color, en la materia pictórica: lienzos de las Postrimerías del Hospital de la Caridad de Sevilla (1672).

La separación política y religiosa de dos zonas que hasta el siglo anterior habían tenido una cultura prácticamente idéntica pone de manifiesto las tensiones sociales que se vivieron en el siglo XVII: Flandes, que seguía bajo el dominio español, era católica y aristocrática, con predominio en el arte de la temática religiosa, mientras que los recién independizados Países Bajos fueron protestantes y burgueses, con un arte laico y más realista, con gusto por el retrato, el paisaje y el bodegón.

En Flandes la figura capital fue Peter Paul Rubens, formado en Italia, donde recibió la influencia de Miguel Ángel y de las escuelas veneciana y boloñesa. En su taller de Amberes empleó a gran cantidad de colaboradores y discípulos, por lo que su producción pictórica destaca tanto por su cantidad como por su calidad, con un estilo dinámico, vital y colorista, donde destaca la rotundidad anatómica, con varones musculosos y mujeres sensuales y carnosas: "El desembarco de María de Médicis en el puerto de Marsella" (1622-1625), "Minerva protege a Pax de Marte" (1629), "Las tres Gracias" (1636-1639), "Rapto de las hijas de Leucipo" (1636), "Juicio de Paris" (1639), etc. Discípulos suyos fueron: Anton van Dyck, gran retratista, de estilo refinado y elegante (""Sir" Endymion Porter y Anton van Dyck", 1635); Jacob Jordaens, especializado en escenas de género, con gusto por los temas populares ("El rey bebe", 1659); y Frans Snyders, centrado en el bodegón ("Bodegón con aves y caza", 1614).

En Holanda destacó especialmente Rembrandt, artista original de fuerte sello personal, con un estilo cercano al tenebrismo pero más difuminado, sin los marcados contrastes entre luz y sombra propios de los caravaggistas, sino una penumbra más sutil y difusa. Cultivó todo tipo de géneros, desde el religioso y mitológico hasta el paisaje y el bodegón, así como el retrato, donde destacan sus autorretratos, que practicó a lo largo de toda su vida. Entre sus obras destacan: "Lección de anatomía del Dr. Nicolaes Tulp" (1632), "La ronda de noche" (1642), "El buey desollado" (1655), y "Los síndicos de los pañeros" (1662). Otro nombre relevante es Frans Hals, magnífico retratista, con una pincelada libre y enérgica que antecede al impresionismo ("Banquete de los arcabuceros de San Jorge de Haarlem", 1627). El tercer nombre de gran relevancia es Jan Vermeer, especializado en paisajes y escenas de género, a los que otorgó un gran sentido poético, casi melancólico, donde destaca especialmente el uso de la luz y los colores claros, con una técnica casi puntillista: "Vista de Delft" (1650), "La lechera" (1660), "La carta" (1662). El resto de artistas holandeses se especializaron por lo general en géneros: de interior y temas populares y domésticos (Pieter de Hooch, Jan Steen, Gabriel Metsu, Gerard Dou, Escuela caravaggista de Utrecht); paisaje (Jan van Goyen, Jacob van Ruysdael, Meindert Hobbema, Aelbert Cuyp); y bodegón (Willem Heda, Pieter Claesz, Jan Davidsz de Heem).

En Alemania hubo escasa producción pictórica, debido a la Guerra de los Treinta Años, por lo que muchos artistas alemanes tuvieron que trabajar en el extranjero, como es el caso de Adam Elsheimer, un notable paisajista adscrito al naturalismo que trabajó en Roma ("La huida a Egipto", 1609). También en Roma se afincó Joachim von Sandrart, pintor y escritor que recopiló diversas biografías de artistas de la época ("Teutschen Academie der Edlen Bau-, Bild- und Mahlerey-Künsten", 1675). Igualmente, Johann Liss estuvo peregrinando entre Francia, Países Bajos e Italia, por lo que su obra es muy variada tanto estilísticamente como de géneros ("La inspiración de San Jerónimo", 1627). Johann Heinrich Schönfeld pasó buena parte de su carrera en Nápoles, elaborando una obra de estilo clasicista e influencia poussiniana ("Desfile triunfal de David", 1640-1642). En la propia Alemania, se desarrolló notablemente el bodegón, con artistas como Georg Flegel, Johann Georg Hainz y Sebastian Stoskopff. En Austria destacó Johann Michael Rottmayr, autor de los frescos de la Iglesia colegial de Melk (1716-1722) y la Iglesia de San Carlos Borromeo de Viena (1726). En Inglaterra, la escasa tradición pictórica autóctona hizo que la mayoría de encargos —generalmente retratos— fuese confiada a artistas extranjeros, como el flamenco Anton van Dyck ("Retrato de Carlos I de Inglaterra", 1638), o el alemán Peter Lely ("Louise de Kéroualle", 1671).

Las primeras influencias fueron del tenebrismo sevillano, principalmente de Zurbarán —algunas de cuyas obras aún se conservan en México y Perú—, como se puede apreciar en la obra de los mexicanos José Juárez y Sebastián López de Arteaga, y del boliviano Melchor Pérez de Holguín. La Escuela cuzqueña de pintura surgió a raíz de la llegada del pintor italiano Bernardo Bitti en 1583, que introdujo el manierismo en América. Destacó la obra de Luis de Riaño, discípulo del italiano Angelino Medoro, autor de los murales del templo de Andahuaylillas. También destacaron los pintores indios Diego Quispe Tito y Basilio Santa Cruz Puma Callao, así como Marcos Zapata, autor de los cincuenta lienzos de gran tamaño que cubren los arcos altos de la Catedral de Cuzco. En Ecuador se formó la escuela quiteña, representada principalmente por Miguel de Santiago y Nicolás Javier de Goríbar.

En el siglo XVIII los retablos escultóricos empezaron a ser sustituidos por cuadros, desarrollándose notablemente la pintura barroca en América. Igualmente, creció la demanda de obras de tipo civil, principalmente retratos de las clases aristocráticas y de la jerarquía eclesiástica. La principal influencia fue la murillesca, y en algún caso —como en Cristóbal de Villalpando— la de Valdés Leal. La pintura de esta época tiene un tono más sentimental, con formas más dulces y blandas. Destacan Gregorio Vázquez de Arce en Colombia, y Juan Rodríguez Juárez y Miguel Cabrera en México.

Las artes gráficas tuvieron una gran difusión durante el Barroco, continuando el auge que este sector tuvo durante el Renacimiento. La rápida profusión de grabados a todo lo largo de Europa propició la expansión de los estilos artísticos originados en los centros de mayor innovación y producción de la época, Italia, Francia, Flandes y Países Bajos —decisivos, por ejemplo, en la evolución de la pintura española—. Las técnicas más empleadas fueron el aguafuerte y el grabado a punta seca. Estos procedimientos permiten a un artista confeccionar un diseño sobre una placa de cobre en sucesivas etapas, pudiendo ser retocado y perfeccionado sobre la marcha. Los diversos grados de desgastamiento de las placas permitían realizar unas 200 impresiones al aguafuerte —aunque siendo solo las 50 primeras de una calidad excelente—, y unas 10 a la punta seca.

En el siglo XVII los principales centros de producción de grabados estaban en Roma, París y Amberes. En Italia fue practicado por Guido Reni, con un dibujo claro y firme de corte clasicista; y Claude Lorrain, autor de aguafuertes de gran calidad, especialmente en los sombreados y la utilización de líneas entrelazadas para sugerir distintos tonos, por lo general en paisajes. En Francia destacaron: Abraham Bosse, autor de unos 1500 grabados, generalmente escenas de género; Jacques Bellange, autor de representaciones religiosas, influido por Parmigianino; y Jacques Callot, formado en Florencia y especializado en figuras de mendigos y seres deformes, así como escenas de la novela picaresca y la commedia dell'arte —su serie de "Grandes miserias de la guerra" influyó en Goya—. En Flandes, Rubens fundó una escuela de burilistas para divulgar más eficazmente su obra, entre los que destacó Lucas Vorsterman I; también Anton van Dyck cultivó el aguafuerte. En España el grabado fue practicado principalmente por José de Ribera, Francisco Ribalta y Francisco Herrera el Viejo. Uno de los artistas que más empleó la técnica del grabado fue Rembrandt, que alcanzó cotas de gran maestría no solo en el dibujo sino también en la creación de contrastes entre luces y sombras. Sus grabados fueron muy cotizados, como se puede comprobar con su aguafuerte "Cristo curando a un enfermo" (1648-1650), que se vendió por cien florines, una cifra récord en la época.
Las artes decorativas y aplicadas también tuvieron una gran expansión en el siglo XVII, debido principalmente al carácter decorativo y ornamental del arte barroco, y al concepto de «obra de arte total» que se aplicaba a las grandes realizaciones arquitectónicas, donde la decoración de interiores tenía un papel protagonista, como medio de plasmar la magnificencia de la monarquía o el esplendor de la Iglesia contrarreformista. En Francia, el lujoso proyecto del palacio de Versalles conllevó la creación de la Manufacture Royale des Gobelins —dirigida por el pintor del rey, Charles Le Brun—, donde se manufacturaban todo tipo de objetos de decoración, principalmente mobiliario, tapicería y orfebrería. La confección de tapices tuvo un significativo incremento en su producción, y se encaminó a la imitación de la pintura, con la colaboración en numerosos casos de pintores de renombre que elaboraban cartones para tapices, como Simon Vouet, el propio Le Brun o Rubens en Flandes —país que también fue un gran centro productor de tapicería, que exportaba a todo el continente, como los magníficos tapices de "Triunfos del Santo Sacramento", confeccionados para las Descalzas Reales de Madrid—.

La orfebrería también alcanzó niveles de elevada producción, especialmente en plata y piedras preciosas. En Italia surgió una nueva técnica para revestir telas y objetos como altares o tableros de mesa con piedras semipreciosas como el ónice, la ágata, la cornalina o el lapislázuli.
En Francia, como el resto de manufacturas fue objeto de protección real, y fue tal la profusión de objetos de plata que en 1672 se promulgó una ley que limitaba la producción de objetos de este metal. La cerámica y el vidrio continuaron generalmente con las mismas técnicas de elaboración que en el período renacentista, destacando la cerámica blanca y azul de Delft (Holanda) y el vidrio pulido y tallado de Bohemia.
El vidriero de Murano Nicola Mazzolà fue artífice de un tipo de vidrio que imitaba la porcelana china.
También continuó la elaboración de vidrieras para iglesias, como las de la iglesia parisina de Saint-Eustache (1631), diseñadas por Philippe de Champaigne.
Uno de los sectores que cobró más relevancia fue la ebanistería, caracterizada por las superficies onduladas (cóncavas y convexas), con volutas y diversos motivos como cartelas y conchas. En Italia destacaron: el armario toscano de dos cuerpos, con balaustradas de bronce y decoración de taracea de piedras duras; el escritorio ligur de dos cuerpos, con figuras talladas y superpuestas ("bambochos"); y el sillón entallado veneciano ("tronetto"), de exuberante decoración. En España surgió el "bargueño", cofre rectangular con asas, con numerosos cajones y compartimentos. El mobiliario español continuó con la decoración de estilo mudéjar, mientras que el Barroco se denotaba en las formas curvas y el uso de columnas salomónicas en las camas. Aun así, predominó la austeridad de signo contrarreformista, como se denota en el sillón llamado "frailero" (o "misional" en Hispanoamérica). La edad de oro de la ebanistería se produjo en la Francia de los Luises, donde se alcanzaron altos niveles de calidad y refinamiento, sobre todo gracias a la obra de André-Charles Boulle, creador de una nueva técnica de aplicación de metales (cobre, estaño) sobre materiales orgánicos (carey, madreperla, marfil) o viceversa. Entre las obras de Boulle destacan las dos cómodas del Trianón, en Versalles, y el reloj de péndulo con el "Carro de Apolo" en Fontainebleau.

La literatura barroca, como el resto de las artes, se desarrolló bajo preceptos políticos absolutistas y religiosos contrarreformistas, y se caracterizó principalmente por el escepticismo y el pesimismo, con una visión de la vida planteada como lucha, sueño o mentira, donde todo es fugaz y perecedero, y donde la actitud frente a la vida es la duda o el desengaño, y la prudencia como norma de conducta. Su estilo era suntuoso y recargado, con un lenguaje muy adjetivado, alegórico y metafórico, y un empleo frecuente de figuras retóricas. Los principales géneros que se cultivaron fueron la novela utópica y la poesía bucólica, que junto al teatro —que por su importancia se trata en otro apartado—, fueron los principales vehículos de expresión de la literatura barroca. Como ocurrió igualmente con el resto de las artes, la literatura barroca no fue homogénea en todo el continente, sino que se formaron diversas escuelas nacionales, cada una con sus peculiaridades, hecho que fomentó el auge de las lenguas vernáculas y el progresivo abandono del latín.

En Italia, la literatura se forjó sobre los cimientos de la dicotomía realismo-idealismo renacentista, así como el predominio nuevamente de la religión sobre el humanismo. Su principal sello lingüístico fue el uso y abuso de la metáfora, que lo impregna todo, con un gusto estético un tanto retorcido, con preferencia por lo deforme sobre lo bello. La principal corriente fue el marinismo —por Giambattista Marino—, un estilo ampuloso y exagerado que pretende sorprender por el virtuosismo del lenguaje, sin prestar especial atención al contenido. Para Marino, «el fin del poeta es el asombro»: su principal obra, "Adonis" (1623), destaca por su musicalidad y por la abundancia de imágenes, con un estilo elocuente y, pese a todo, sencillo de leer. Otros poetas marinistas fueron: Giovanni Francesco Busenello, Emanuele Tesauro, Cesare Rinaldi, Giulio Strozzi, etc.

En Francia surgió el preciosismo, una corriente similar al marinismo que otorga especial relevancia a la riqueza del lenguaje, con un estilo elegante y amanerado. Estuvo representado por Isaac de Benserade y Vincent Voiture en poesía, y Honoré d'Urfé y Madeleine de Scudéry en prosa. Más adelante surgió el clasicismo, que propugnaba un estilo simple y austero, sujeto a cánones clásicos —como las tres unidades aristotélicas—, con una rígida reglamentación métrica. Su iniciador fue François de Malherbe, cuya poesía racional y excesivamente rígida le restaba cualquier atisbo de emocionalidad, al que siguieron: Jean de La Fontaine, un impecable fabulista, de intención didáctica y moralizadora; y Nicolas Boileau-Despréaux, poeta elegante pero falto de creatividad, por su insistencia en someter la imaginación al imperio de la norma y la reglamentación. Otros géneros cultivados fueron: el burlesco (Paul Scarron), la elocuencia (Jacques-Bénigne Bossuet), la novela psicológica (Madame de La Fayette), la novela didáctica (François Fénelon), la prosa satírica (Jean de La Bruyère, François de la Rochefoucauld), la literatura epistolar (Jean-Louis Guez de Balzac, la Marquesa de Sévigné), la religiosa (Blaise Pascal), la novela fantástica (Cyrano de Bergerac) y el cuento de hadas (Charles Perrault).
En Inglaterra surgió el eufuismo —por "Eufues o la Anatomía del Ingenio", de John Lyly (1575)—, una corriente similar al marinismo o el preciosismo, que presta más atención a los efectos lingüísticos (antítesis, paralelismos) que al contenido, y que mezcla elementos de la cultura popular con la mitología clásica. Este estilo fue practicado por Robert Greene, Thomas Lodge y Barnabe Rich. Posteriormente surgió una serie de poetas llamados «metafísicos», cuyo principal representante fue John Donne, que renovó la lírica con un estilo directo y coloquial, alejado de fantasías y virtuosismos lingüísticos, con gran realismo y trasfondo conceptual ("Sonetos sagrados", 1618). Otros poetas metafísicos fueron: George Herbert, Richard Crashaw, Andrew Marvell y Henry Vaughan. Figura aparte y de mayor relevancia es John Milton, autor de "El paraíso perdido" (1667), de influjo puritano, con un estilo sensible y delicado, y que gira en torno a la religión y el destino del hombre, al que otorga un espíritu de rebeldía que sería recogido por los románticos. El último gran poeta de la época es John Dryden, poeta y dramaturgo de tono satírico. En prosa destacó la época de la Restauración, con un estilo racional, moral y didáctico, con influencia del clasicismo francés, representado por diversos géneros: literatura religiosa (John Bunyan, George Fox); narrativa (Henry Neville); y memorias y diarios (Samuel Pepys).

En Alemania, la literatura estuvo influida por la "Pléiade" francesa, el gongorismo español y el marinismo italiano, aunque tuvo un desarrollo diferenciado por la presencia del protestantismo y el mayor peso social de la burguesía, que se denota en géneros como el "Schuldrama" («teatro escolar») y el "Gemeindelied" («canto parroquial»). Pese al desmembramiento del territorio alemán en numerosos estados, surgió una conciencia nacional de la lengua común, que fue protegida a través de las "Sprachgesellschaften" («sociedades de la lengua»). En el terreno de la lírica destacaron las denominadas Primera escuela de Silesia, representada por Martin Opitz, Paul Fleming, Angelus Silesius y Andreas Gryphius, y Segunda escuela de Silesia, donde destacan Daniel Casper von Lohenstein y Christian Hofmann von Hofmannswaldau. En la narrativa destaca igualmente Lohenstein, autor de la primera novela alemana plenamente barroca ("La maravillosa historia del gran príncipe cristiano alemán Hércules"), y Hans Jakob Christoph von Grimmelshausen, autor de "El aventurero Simplicíssimus" (1669), una novela costumbrista similar al género picaresco español.

En Portugal, la anexión a la corona española originó un período de cierta decadencia, viéndose la literatura portuguesa sometida al influjo de la española. Numerosos poetas siguieron el estilo épico de Camões, el gran poeta renacentista autor de "Os Lusíadas", al que imitaron autores como Vasco Mouzinho de Quebedo ("Alfonso Africano", 1616), Francisco Sá de Meneses ("Malaca conquistada", 1634), Gabriel Pereira de Castro ("Lisboa edificada", 1686), y Braz Garcia de Mascarenhas ("Viriato trágico", 1699). En la primera mitad de siglo destacaron el novelista y poeta Francisco Rodrigues Lobo, autor de novelas pastoriles que alternan el verso y la prosa ("El pastor peregrino", 1608); y Francisco Manuel de Melo, autor de poemas gongorinos, diálogos y tratados históricos ("Obras métricas", 1665). También destacó la prosa religiosa, cultivada por Bernardo de Brito, João de Lucena, António Vieira y Manuel Bernardes.

En Holanda, la independencia de España supuso una revitalización de la literatura, donde el siglo XVII suele ser descrito como una «Edad de oro» de las letras neerlandesas. Sin embargo, estilísticamente la literatura holandesa de la época no encaja del todo en los cánones del Barroco, debido principalmente a las peculiaridades sociales y religiosas de este país, como se ha visto en el resto de las artes. En Ámsterdam surgió el denominado "Muiderkring" («Círculo de Muiden»), un grupo de poetas y dramaturgos liderado por Pieter Corneliszoon Hooft, escritor de poesía pastoral y de tratados de historia, que sentaron las bases de la gramática holandesa. A este círculo perteneció también Constantijn Huygens, conocido por sus epigramas espirituales. La cumbre de la poesía lírica de la Edad de oro holandesa fue Joost van den Vondel, que influido por Ronsard destacó por el verso sonoro y rítmico, relatando con un estilo algo satírico los principales acontecimientos de su época ("Los misterios del altar", 1645). En Middelburg destacó Jacob Cats, autor de poemas didácticos y morales. En prosa cabe citar a Johan van Heemskerk, autor de "Arcadia Bátava" (1637), el primer romance escrito en holandés, género que fue rápidamente imitado, como en el "Mirandor" (1675) de Nikolaes Heinsius el Joven.

En España, donde el siglo XVII sería denominado el «Siglo de oro», la literatura estuvo más que en ningún otro sitio al servicio del poder, tanto político como religioso. La mayoría de obras van encaminadas a la exaltación del monarca como elegido por Dios, y de la Iglesia como redentora de la humanidad, al mismo tiempo que se procura una evasión de la realidad para diluir la penosa situación económica de la mayoría de la población. Sin embargo, pese a estas limitaciones, la creatividad de los escritores de la época y la riqueza del lenguaje desarrollado produjeron un elevado nivel de calidad, que convierten a la literatura española de la época en el paradigma de la literatura barroca y en una de las más altas cimas de la historia de la literatura. La descripción de la realidad se basa en dos ejes vertebradores: la transitoriedad de los fenómenos terrenales, donde todo es vanidad ("vanitas vanitatum"); y el omnipresente recuerdo de la muerte ("memento mori"), que hace apreciar con más intensidad la vida ("carpe diem").

La base conceptual de la literatura barroca española proviene de la cultura grecolatina, aunque adaptada, como se ha descrito, a la apología político-religiosa. Así, la estética literaria se vertebra alrededor de tres tópicos de origen clásico: la contraposición entre juicio e ingenio, que si bien en el humanismo renacentista estaban equilibrados, en el Barroco será el segundo el que asumirá mayor relevancia; el tópico horaciano "delectare et prodesse" («deleitar y aprovechar»), por el que se produce una simbiosis entre los recursos estilísticos y el proselitismo a favor del poder establecido, y por el que en última instancia se llega a la fórmula "ars gratia artis" («el arte por el arte»), en que la literatura se abandona al placer de la simple belleza; y el también tópico horaciano "ut pictura poesis" («la poesía como la pintura»), máxima por la cual el arte debe imitar la naturaleza para conseguir la perfección —como expresó Baltasar Gracián: «lo que es para los ojos la hermosura, y para los oídos la consonancia, eso es para el entendimiento el concepto»—.
En la lírica se dieron dos corrientes: el culteranismo (o cultismo), liderado por Luis de Góngora (por lo que también se le llama «gongorismo»), donde destacaba la belleza formal, con un estilo suntuoso, metafórico, con abundancia de paráfrasis y una gran proliferación de latinismos y juegos gramaticales; y el conceptismo, representado por Francisco de Quevedo y donde predominaba el ingenio, la agudeza, la paradoja, con un lenguaje conciso pero polisémico, con múltiples significados en pocas palabras. Góngora fue uno de los mejores poetas de principios del siglo XVII, actividad que cultivaba en sus ratos libres como sacerdote. Su obra está influida por Garcilaso, aunque sin el sentido armónico y equilibrado que mostró este en toda su producción. El estilo de Góngora es más ornamental, musical, colorista, con abundancia de hipérbatos y metáforas, por lo que resulta difícil de leer y se dirige especialmente a minorías cultas. En cuanto a temática, predomina la amorosa, la satírica-burlesca y la religioso-moral. Empleó métricas como las silvas y las octavas reales, pero también formas más populares como sonetos, romances y redondillas. Sus principales obras son la "Fábula de Polifemo y Galatea" (1613) y "Soledades" (1613). Otros poetas culteranistas fueron: Juan de Tassis, conde de Villamediana, Gabriel Bocángel, Pedro Soto de Rojas, Anastasio Pantaleón de Ribera, Salvador Jacinto Polo de Medina, Francisco de Trillo y Figueroa, Miguel Colodrero de Villalobos y fray Hortensio Félix Paravicino.

Por su parte, Quevedo osciló en su vida personal entre importantes cargos políticos o la cárcel y el destierro, según su relación temperamental con las autoridades. En su obra se vislumbra un sentimiento desgarrado por la realidad cotidiana de su país, donde predomina el desengaño, la presencia del dolor y la muerte. Esta visión se desarrolla en dos líneas contrapuestas: o bien la cruda descripción de la realidad, o bien burlándose de ella y caricaturizándola. Sus poemas fueron publicados tras su muerte en dos volúmenes: "Parnaso español" (1648) y "Las tres Musas" (1670). Otros poetas conceptistas fueron: Alonso de Ledesma, Miguel Toledano, Pedro de Quirós y Diego de Silva y Mendoza, conde de Salinas. Aparte de estas dos corrientes merece destacarse la figura de Lope de Vega, un gran dramaturgo que también cultivó la poesía y la novela, tanto de inspiración religiosa como profana, a menudo con un trasfondo autobiográfico. Utilizó principalmente la métrica de romances y sonetos, como en "Rimas sacras" (1614) y "Rimas humanas y divinas del licenciado Tomé de Burguillos" (1634); y también realizó poemas épicos, como "La Dragontea" (1598), "El Isidro" (1599) y "La Gatomaquia" (1634).
La prosa estuvo dominada por la gran figura de Miguel de Cervantes, que si bien se sitúa entre el Renacimiento y el Barroco supuso una figura de transición que marcó a una nueva generación de escritores españoles. Militar en su juventud —participó en la batalla de Lepanto—, estuvo prisionero de los turcos durante cinco años; posteriormente ocupó diversos cargos burocráticos, que compaginó con la escritura, que si bien le proporcionó una inicial fama no impidió que muriese en la pobreza. Cultivó la novela, el teatro y la poesía, aunque esta última con escaso éxito. Pero indudablemente su talento estaba en la prosa, que oscila entre el realismo y el idealismo, a menudo con una fuerte intención moralizadora, como en sus "Novelas ejemplares". Su gran obra, y una de las cumbres de la literatura universal, es "Don Quijote" (1605), la historia de un hidalgo que emprende una serie de alocadas aventuras creyéndose un gran paladín como los de las novelas de caballería. Si bien la primera intención de Cervantes era hacer una parodia, conforme se fue gestando la historia adquirió un fuerte sello filosófico y de introspección de la mente y el sentimiento humanos, pasando del humor a la fina ironía que, sin embargo, está exenta de resentimiento o acritud, y pone de manifiesto que la cualidad esencial del ser humano es su capacidad de soñar.

Otro terreno donde se desarrolló la prosa barroca española fue la novela picaresca, continuando la tradición iniciada el siglo anterior con el "Lazarillo de Tormes" (1554). Estuvo representada principalmente por tres nombres: Francisco de Quevedo, autor de "La vida del Buscón" (1604), de aspecto amargo y crudamente realista; Mateo Alemán, que firmó el "Guzmán de Alfarache" (en dos partes: 1599 y 1604), quizá la mejor en su género, donde el pícaro es más un filósofo que un pobre vagabundo; y Vicente Espinel, que en "El escudero Marcos de Obregón" (1618) ofrece una visión agridulce del pícaro, que pese a sus infortunios encuentra el lado amable de la vida. Otro género fue el de la novela pastoril, cultivada principalmente por Lope de Vega, autor de "La Arcadia" (1598) y "La Dorotea" (1632), esta última un drama en prosa cuyos largos diálogos la hacen irrepresentable como drama teatral. Por último, otra vertiente de la prosa de la época fue la conceptista, que en paralelo a la poesía desarrolló un estilo de escritura intelectual y cultivado, que se servía de los recursos de la lingüística y la sintaxis para describir la realidad circundante, generalmente de forma realista y desengañada, reflejando la amargura de una época donde la mayoría sobrevivía en duras condiciones sociales. Su principal exponente fue Baltasar Gracián, autor de "Agudeza y arte de ingenio" (1648), un tratado que desarrolla las posibilidades de la retórica; y "El Criticón" (1651-1655), novela de corte filosófico cuyo argumento es una alegoría de la vida humana, que oscila entre la civilización y la naturaleza, entre la cultura y la ignorancia, entre el espíritu y la materia. Como escritor conceptista también merece nombrarse a Luis Vélez de Guevara, autor de "El diablo cojuelo" (1641), novela satírica cercana a la picaresca pero sin sus elementos más comunes, por lo que cabría más calificarla de costumbrista.

En Latinoamérica, la literatura recibió en general los principales influjos de la metrópoli, aunque con diversas peculiaridades regionales. Destacaron especialmente el teatro y la poesía, esta última de influencia principalmente gongorina, a la que se sumaba el sello indígena y el estilo épico iniciado con "La Araucana" de Alonso de Ercilla: tenemos así "El Bernardo" (1624) de Bernardo de Balbuena; "Espejo de paciencia" (1608), del cubano Silvestre de Balboa; o "La Cristiada" (1611), de Diego de Hojeda. En México la poesía gongorina alcanzó cotas de gran calidad, con poetas como Luis de Sandoval y Zapata, Carlos de Sigüenza y Góngora, Agustín de Salazar y Torres y, principalmente, Sor Juana Inés de la Cruz, que inició un tipo de poesía didáctica y analítica que entroncaría con la Ilustración ("Inundación castálida", 1689). La prosa tuvo escasa producción, debido a la prohibición desde 1531 de cualquier introducción en las colonias de «literatura de ficción», y destacó solamente en el terreno de la historiografía: "Histórica relación del Reyno de Chile" (1646), de Alonso de Ovalle; "Historia general de las conquistas del Nuevo Reino de Granada" (1688), de Lucas Fernández de Piedrahíta; "Historia de la conquista y población de la Provincia de Venezuela" (1723), de José de Oviedo y Baños. En Brasil destacó Gregório de Matos Guerra, autor de sátiras y poesías religiosas y seculares con influencia de Góngora y Quevedo.

Si bien resulta complicado literariamente hablar de teatro barroco en Europa, el Barroco supuso un período de esplendor del teatro como género literario y como espectáculo que se extendió desde Italia al resto de Europa en el siglo XVII. Los teatros nacionales, que se conformaron durante el siglo XVII, tienen características propias y diversas.

Durante el Barroco se definieron los límites estructurales de la sala y se introdujo la utilización de medios y aparatos mecánicos que potenciasen el componente visual del espectáculo.
Las realizaciones sobre el edificio teatral, las maquinarias y tramoyas ("tramoggie") desarrolladas en Italia se llevaron al resto de países europeos (España, Francia y Austria principalmente).
El nuevo teatro dejó de ser un ambiente único para dividirse en sala y escenario, separados y comunicados a la vez por el proscenio. Descorrido el telón, el escenario se presentaba como una escena ilusoria, apoyada en el notable desarrollo de la escenografía. La aplicación de la perspectiva de la escena a la italiana, respuesta a una visión del mundo que confiaba en las leyes científicas, alcanzó una gran sofisticación, con complicadísimos juegos de planos y puntos de fuga. La evolución de los corrales de comedias hasta las salas a la italiana propició la aparición de los edificios y salas teatrales contemporáneos.

El teatro del Barroco fue un espectáculo global que se convirtió en un negocio de distintas variantes. Por un lado estaba el teatro popular, que se trasladó del espacio público a locales específicamente dedicados a ello, como los corrales de comedias en España o los teatros isabelinos en Inglaterra. En Madrid, las cofradías de socorro (instituciones de asistencia social que, bajo advocación religiosa, proliferaron conforme crecía la Villa convertida en corte real) consiguieron el monopolio de la gestión comercial del teatro popular, lo que contribuyó a su desarrollo debido a la utilidad pública de la beneficencia, permitiendo superar la reticencia de predicadores, eclesiásticos e intelectuales hacia el teatro comercial profano, que consideraban «una fuente de pecado y malas costumbres». Se trataba de un teatro narrativo; en ausencia de telón y escenografía, los cambios de localización y tiempo se introducían a través del texto y eran habituales largos soliloquios, apartes y discursos prolongados.

El uso de artificios visuales y decoración transformó la escena con representaciones donde predominaba el espectáculo visual sobre el texto, diferenciándose de la «vulgar comedia». La «comedia de teatro» fue un género ligado al desarrollo de técnicas y artes, especialmente el edificio y la maquinaria teatral que se creaba para su representación. Arquitectos y escenógrafos de Italia llevaron los inventos sobre el edificio teatral, las maquinarias y tramoyas ("tramoggie") al resto de países europeos, España, Francia y Austria principalmente.

Las veladas teatrales del Barroco, fueran en los teatros de corrales o en los escenarios cortesanos, no consistían como actualmente en la representación de una sola pieza u obra; se trataba de toda una "fiesta teatral", una sucesión de piezas de distintos géneros entre los que ocupaba un papel primordial la comedia. Estas sesiones seguían una estructuración fija, en la que piezas menores de distintos géneros se intercalaban entre los actos del drama principal, normalmente una comedia o un auto sacramental. Estos géneros se diferenciaban básicamente por su función dentro de la representación y por el mayor o menor peso del componente cantado, bailado o representado. La fiesta teatral barroca pervivió, con ligeras variaciones, durante los dos primeros tercios del siglo XVIII.

A finales del siglo XVI una serie de artistas e intelectuales desarrollaron en Florencia una estética teatral que buscaba imitar «la grandiosidad e impacto expresivo del espectáculo griego»; partiendo de los textos de Aristóteles y Platón, la nueva estética giraba sobre los recursos expresivos de la voz en la declamación y sobre el papel de la música como soporte y acompañamiento del texto poético.
El ulterior desarrollo de sus teorías dio origen a nuevos géneros musicales como la ópera, la semiópera y la zarzuela.

En Italia triunfaba la Commedia dell'Arte, teatro popular basado en la improvisación que se extendió por toda Europa y perduró hasta principios del siglo XIX. Las compañías italianas adaptaron para su repertorio una buena cantidad de comedias españolas. Los componentes trágicos propios del teatro español, que no eran del gusto del público italiano, eran minimizados o eliminados de la obra, al tiempo que se dilataban o introducían situaciones cómicas que permitieran la aparición de los personajes y máscaras propios, como "Pulcinella" o el "Dottore".
Su acogida fue tan buena que la antigua "Vía del Teatro dei Fiorentini" de Nápoles (por entonces parte de la corona de España) llevaba para 1630 el sobrenombre de "Via della commedia spagnola".

En Francia, la tardía influencia del Renacimiento condujo a sus dramaturgos a desarrollar un teatro clasicista dirigido a una audiencia privilegiada. Autores como Molière, Racine y Corneille se pronunciaron a favor de los preceptos clásicos del teatro y la regla de las tres unidades dramáticas, basados en la "Poética" de Aristóteles.

No obstante, la obra de dramaturgos como Corneille acusa la influencia del teatro barroco español. Se dio así el Debate de los antiguos y los modernos ("Querelle des Anciens et des Modernes"), entre los partidarios del clasicismo y una generación de dramaturgos (la Generación de 1628) que defendían la libertad creadora y el respeto al gusto del público. Enmarcada en este debate, "Le Cid" (1637) de Corneille fue protagonista de una de las polémicas más célebres de la historia literaria de Francia, la "Querelle du Cid". Pese a ser una de las obras más aplaudidas del siglo XVII francés, "Le Cid" fue fuertemente criticada por no respetar los preceptos clásicos, especialmente la verosimilitud, el decoro y la finalidad educativa.

En 1680 Luis XIV fundó la Comédie-Française, compañía nacional francesa de teatro, producto de la fusión de varias compañías teatrales y le otorgó el monopolio de las representaciones en francés en París y sus arrabales. Su nombre surgió por contraposición con la Comédie Italienne (comedia italiana), una compañía italiana especializada en representaciones de la Comedia del arte con la que sostenían una especial competencia.

La influencia renacentista fue también tardía en Inglaterra, por lo que no suele hablarse de teatro barroco en la literatura inglesa del XVII, sino del teatro isabelino y de la comedia de la Restauración. Entre los dramaturgos de la época isabelina cabe destacar a Christopher Marlowe, iniciador de la nueva técnica teatral que puliría William Shakespeare, máximo exponente de la literatura inglesa y uno de los más célebres escritores de la literatura universal.
Como en España, el teatro se profesionalizó y trasladó el escenario de las plazas a salas públicas y privadas especialmente destinadas al espectáculo dramático. Entre los primeros teatros construidos en Londres se cuentan "The Theatre" (El Teatro), "The Curtain" (El Telón), "The Swan" (El Cisne), "The Globe" (El Globo) y "The Fortune" (La Fortuna).

Tras un paréntesis de dieciocho años en los que la facción puritana del parlamento inglés consiguió mantener los teatros ingleses clausurados, la Restauración monárquica de Carlos II en 1660 abrió paso a la comedia de la Restauración, una manifestación de las propuestas estéticas italianas de carácter popular, libertino, frívolo y extravagante.

Comparado con al extraordinario desarrollo en el contexto europeo, el teatro alemán del siglo XVII no realizó grandes aportes. El dramaturgo alemán más conocido podría ser Andreas Gryphius, que tomó como modelos el teatro de los jesuitas, al neerlandés Joost van den Vondel y a Corneille. Cabe mencionar también a Johannes Velten, quien combinó la tradición de los comediantes ingleses y la comedia del arte con el teatro clásico de Corneille y Molière. Su compañía de teatro ambulante se cuenta entre las más importantes del siglo XVII.

El Barroco tuvo su realización más característica en la católica y contrarreformista España, que sucedió a Italia en el liderazgo literario que le había pertenecido durante el Renacimiento. El teatro hispano del Barroco buscaba contentar al público con una realidad idealizada, en la que se manifiestan fundamentalmente tres sentimientos: el religioso católico, el monárquico y patrio y el del honor, procedente del mundo caballeresco.

Suelen apreciarse dos períodos o ciclos en el teatro barroco español, cuya separación se acentuó hacia 1630; un primer ciclo cuyo principal exponente sería Lope de Vega y en el que cabe mencionar también a Tirso de Molina, Gaspar de Aguilar, Guillén de Castro, Antonio Mira de Amescua, Luis Vélez de Guevara, Juan Ruiz de Alarcón, Diego Jiménez de Enciso, Luis Belmonte Bermúdez, Felipe Godínez, Luis Quiñones de Benavente o Juan Pérez de Montalbán; y un segundo ciclo, del que sería exponente Calderón de la Barca y que incluye a dramaturgos como Antonio Hurtado de Mendoza, Álvaro Cubillo de Aragón, Jerónimo de Cáncer y Velasco, Francisco de Rojas Zorrilla, Juan de Matos Fragoso, Antonio Coello y Ochoa, Agustín Moreto o Francisco de Bances Candamo.
Se trata de una clasificación relativamente laxa, puesto que cada autor tiene su propio hacer y puede en ocasiones adherirse a uno u otro planteamiento de la fórmula establecida por Lope. La «manera» de Lope es quizá más libre que la de Calderón, más sistematizada.

Félix Lope de Vega y Carpio introdujo con su "Arte nuevo de hacer comedias en este tiempo" (1609) la "comedia nueva", con la que estableció una nueva fórmula dramática que rompía con las tres unidades aristotélicas de la escuela de poética italiana (acción, tiempo y lugar), así como con una cuarta unidad, también esbozada en Aristóteles, la de estilo, tanto mezclando en una misma obra elementos trágicos y cómicos como valiéndose de distintos tipos de verso y estrofa según lo que se representa. Aunque Lope tenía un buen conocimiento de las artes plásticas, no dispuso durante la mayor parte de su carrera ni del teatro ni de la escenografía que se desarrolló posteriormente. La comedia lopesca otorgaba un papel secundario a los aspectos visuales de la representación teatral, que descansaban sobre el propio texto.

Tirso de Molina fue, junto a Lope de Vega y Calderón, uno de los tres dramaturgos más importantes de la España del Siglo de Oro. Su obra, que destaca por su sutil inteligencia y por una profunda comprensión de la humanidad de sus personajes, puede considerarse un puente entre la primitiva comedia lopesca y el más elaborado drama calderoniano. Aunque parte de la crítica discute su autoría, Tirso de Molina es conocido sobre todo por dos obras magistrales, "El condenado por desconfiado" y "El burlador de Sevilla", principal fuente del mito de Don Juan.

La llegada a Madrid de Cosme Lotti llevó a la corte española las técnicas teatrales más avanzadas de Europa. Sus conocimientos técnicos y mecánicos se aplicaron en exhibiciones palaciegas llamadas «fiestas» y en «fastuosos despliegues sobre ríos o fuentes artificiales» denominados «naumaquias». Tuvo a su cargo los diseños de los jardines del Buen Retiro, de la Zarzuela y de Aranjuez y la construcción del edificio teatral del Coliseo del Buen Retiro. Las fórmulas lopescas comenzaron a verse desplazadas por el afianzamiento del teatro palaciego y el nacimiento de nuevos conceptos cuando comenzó la carrera como dramaturgo de Pedro Calderón de la Barca.
Marcado al principio por las innovaciones de la comedia nueva lopesca, el estilo de Calderón marcó algunas diferencias, con un mayor cuidado constructivo y atención a su estructura interna. Sus obras alcanzaron una gran perfección formal, un lenguaje más lírico y simbólico. La libertad, el vitalismo y la espontaneidad lopesca dieron paso en Calderón a la reflexión intelectual y la precisión formal. En sus comedias predominan la intención ideológica y doctrinal sobre las pasiones y la acción, e hizo que el auto sacramental alcanzase sus más altas cotas. La "comedia de teatro" es un género «politécnico, multiartístico, híbrido en cierta manera». El texto poético se imbricó con medios y recursos procedentes de la arquitectura, la pintura y la música, liberándose de la descripción que en la comedia lopesca suplía la falta de decorados y dedicándose al diálogo de la acción.

Siguiendo la evolución marcada desde España, a finales del siglo XVI las compañías de comediantes, esencialmente trashumantes, comenzaron a profesionalizarse. Con la profesionalización vino la regulación y la censura: al igual que en Europa, el teatro oscilaba entre la tolerancia e incluso protección del gobierno y el rechazo (con excepciones) o la persecución por parte de la Iglesia. El teatro resultaba útil a las autoridades como instrumento de difusión del comportamiento y modelos deseados, el respeto al orden social y a la monarquía, escuela del dogma religioso.

Los corrales se administraban en beneficio de hospitales que compartían los beneficios de las representaciones. Las compañías itinerantes (o «de la legua»), que llevaban el teatro en tablados improvisados al aire libre por las regiones que no disponían de locales fijos, precisaban una licencia virreinal para poder trabajar, cuyo precio o "pinción" era destinado a limosnas y obras piadosas. Para las compañías que trabajaban de forma estable en las capitales y ciudades importantes una de sus principales fuentes de ingresos era la participación en las festividades del "Corpus Christi", que les proporcionaba no solo beneficios económicos, sino también reconocimiento y prestigio social. Las representaciones en el palacio virreinal y las mansiones de la aristocracia, donde representaban tanto las comedias de su repertorio como producciones especiales con grandes efectos de iluminación, escenografía y tramoya, eran también una importante fuente de trabajo bien pagada y prestigiosa.

Nacido en el virreinato de Nueva España aunque asentado posteriormente en España, Juan Ruiz de Alarcón es la figura más destacada del teatro barroco novohispano. Pese a su acomodo a la nueva comedia de Lope, se han señalado su «marcado laicismo», su discreción y mesura y una agudísima capacidad de «penetración psicológica» como características distintivas de Alarcón frente a sus coetáneos españoles. Cabe destacar entre sus obras "La verdad sospechosa", una comedia de caracteres que reflejaba su constante propósito moralizante. La producción dramática de Sor Juana Inés de la Cruz la sitúa como segunda figura del teatro barroco hispanoamericano. Cabe mencionar entre sus obras el auto sacramental "El divino Narciso" y la comedia "Los empeños de una casa".

Entre los especialistas se acepta que la música entre los albores del siglo XVII y mediados del siglo XVIII tiene una serie de características que permite clasificarla como un período estilístico, el denominado Barroco en la historia musical occidental. También hay coincidencia en que, aunque el período pueda acotarse entre 1600 y 1750, algunas de las características de esta música ya existían en la Italia de la segunda parte del siglo XVI y otras se mantuvieron en zonas periféricas de Europa hasta finales del siglo XVIII. Algunos autores dividen a su vez el barroco musical en tres subperíodos: temprano, hasta mediados del siglo XVII; medio, hasta finales del siglo XVII; y tardío, hasta las muertes de Bach y Händel.

Se han estudiado paralelismos y similitudes entre los rasgos musicales de esta época con los de las otras artes de este período histórico como la arquitectura y la pintura. Sin embargo, otros autores estiman excesivas esas analogías, prefiriendo señalar los rasgos estilísticos únicamente musicales que pueden ser calificados de barrocos simplemente por ser contemporáneos de las artes plásticas y la literatura, y por tener una unidad espiritual y artística con el período post-Renacimiento.

La música barroca a menudo tenía una textura homofónica, donde la parte superior desarrollaba la melodía sobre una base de bajos con importantes intervenciones armónicas. La polaridad que resultó del triple y del bajo llevó desde la transición entre los siglos XVI y XVII al uso habitual del bajo continuo: una línea de bajo instrumental sobre la que se improvisaban en acordes los tonos intermedios. El bajo continuo era una línea independiente que duraba toda la obra, por eso recibe el nombre de continuo. Apoyado en la base del bajo se improvisaban melodías mediante acordes con un instrumento que los pudiese producir, normalmente un teclado. Estos acordes se solían especificar en el pentagrama mediante números junto a las notas del bajo, de allí el nombre de bajo cifrado. El bajo continuo fue esencial en la música barroca, llegándose a denominar la «época del bajo continuo».

Entre los muchos compositores barrocos destacan los italianos Claudio Monteverdi, Arcangelo Corelli, Alessandro Scarlatti, Domenico Scarlatti, Antonio Vivaldi y Tommaso Albinoni; los franceses Jean-Baptiste Lully, François Couperin, Jean Philippe Rameau y Marc-Antoine Charpentier; los alemanes Heinrich Schütz, Georg Philipp Telemann, Johann Pachelbel y Johann Sebastian Bach; y los ingleses Henry Purcell y Georg Friedrich Händel (alemán de nacimiento).

Para Francisco Camino, en los 150 años de este período la música occidental cobró un gran impulso, convirtiéndose en una de las artes "más variadas, extendidas y vigorosas". La variedad la aportaban los géneros y formas que se establecieron en este período: aria de capo, cantata, ópera, oratorio, sonata (para tres instrumentos o para uno solo), concierto grosso, concierto para un instrumento solista, preludio, fuga, fantasía, coral, suite y tocata. La extensión geográfica de la música barroca alcanzó a toda Europa desde Italia: la música sonaba en todos los lugares, palacios, teatros, iglesias, conventos, colegios, etc. El vigor de las formas barrocas se siguió expandiendo en los siglos siguientes con una fuerza que hoy todavía continúa.

Fue un período de experimentación dominado por la supremacía de los estilos monódicos en diversos géneros, como el madrigal, el aria para solista, la ópera, el concierto sacro vocal, la sonata para solista o la sonata en trío. Así describe el musicólogo Adolfo Salazar la innovación de la monodia frente a la polifonía: se abandonaron las múltiples combinaciones de voces superpuestas del Renacimiento, la nueva idea de la monodia abogaba por una sola voz protagonista que buscaba que el texto solista y la melodía desnuda se escucharan con claridad expresando los afectos. Acompañando al texto y sustentando la melodía, el bajo continuo con sus acordes se ocupaba de la armonía.

Los compositores abandonaron las formas renacentistas de continuidad melódica y rítmica plana con texturas homogéneas y se decantaron por la discontinuidad dentro de la misma obra. Se buscaba el contraste de diversas formas: entre suave y fuerte, entre solos y tuttis, entre los variados colores vocales o instrumentales, entre rápido y lento, entre diferentes voces e instrumentos.

La nueva estética musical cambió el estilo vocal; buscando ser más expresivo se dejaron de emplear las voces polifónicas renacentistas. La nueva forma se basaba sobre todo en una voz solista. En Venecia comenzó la ópera y se construyeron teatros de ópera financiados por las familias nobles poderosas, lo que favoreció el desarrollo de la misma y el público de la ciudad se volcó en ello. Los compositores experimentaron con el nuevo estilo y, entre ellos, Monteverdi exploró todas las posibilidades del teatro musical, tanto vocales como instrumentales, llegando en sus últimas óperas a desarrollar completamente el género, siendo el primero en dotar a los elementos esenciales (drama, música, acción y expresión) de unidad y cohesión.

La ópera tiene un papel destacado en la cultura desde entonces. En Venecia en el siglo XVII se estima que se estrenaron más de mil óperas y otras mil en el siglo XVIII. En los 400 años de historia de la ópera en Italia se han estrenado unas 30 000 óperas y en el mundo se estiman unos 50 000 estrenos de obras de ópera.

Fue un período de consolidación. La disonancia se acotó de forma más estricta, mientras que el recitativo expresivo desarrollado en el tiempo anterior perdió relieve. Entre las innovaciones de este período medio apareció el estilo vocal belcantista, que fomentó el virtuosismo del cantante. Se desarrolló el lenguaje tonal, fomentando la aparición de nuevas formas y géneros musicales. El contrapunto se volvió a desarrollar, aunque de manera totalmente nueva. Así, la cantata, formada a partir de arias y recitativos, relegó a la monodia lírica. El oratorio y la cantata religiosa en los países protestantes elevaron el concierto sacro. La sonata para solista y para trío alcanzó un modelo estable.

Desde su amplio florecimiento en Italia, la nueva música se difundió por toda Europa. El compositor italiano Jean-Baptiste Lully emigró a Francia y allí adaptó la nueva música y la ópera al gusto francés, dando preferencia en la misma al ballet, que tenía un gran predicamento en la corte francesa. Bajo la dirección de Lully, los instrumentistas de la corte francesa adquirieron un gran nivel técnico y su orquesta de cuerda, que acompañaba sus óperas, era muy admirada en toda Europa. Aunque los instrumentos de cuerda habían dominado el panorama musical, en Francia instrumentos como la trompeta y el oboe tuvieron un importante desarrollo técnico y sus intérpretes impulsaron las posibilidades de estos instrumentos, que fueron incorporados a las orquestas.

En Italia en este período el violín se destacó como el instrumento más importante de la orquesta. Luthiers italianos, como las familias Amati, Stradivarius y Guarneri, perfeccionaron la construcción del violín, estudiaron sus óptimas medidas, el grosor de sus tablas hasta conseguir una sonoridad más potente e intensa manteniendo su calidad. De sus talleres salieron el violín y sus familiares, la viola actual, el violonchelo y el contrabajo. Paralelo al perfeccionamiento en la construcción de la familia de cuerda, la intuición de los compositores mejoró la intensidad y calidad de su sonoridad gracias a nuevas técnicas de ejecución de estos instrumentos. Los principales compositores italianos compusieron fundamentalmente para estos instrumentos de cuerda, tanto en grandes masas orquestales como en pequeños grupos de instrumentos. El concierto y la sonata para instrumento solo o en trío adquirieron un gran desarrollo.

El florecimiento de la música italiana originó que los artistas italianos fuesen reclamados en toda Europa y estos fueron emigrando e instalándose en otros países, difundiendo el estilo de música de su país. A finales del siglo XVII, la música italiana, tanto la instrumental como la vocal, tenía una gran influencia en Europa, con especial énfasis en la ópera. En Austria y Alemania se impuso tanto la música italiana como la francesa, en cambio en Inglaterra predominó la influencia italiana.

En el Barroco tardío, la tonalidad quedó definitivamente establecida mediante normas adquiriendo esquemas más amplios, mientras la armonía se fusionó con la polifonía. Con estas técnicas compositivas las formas alcanzaron grandes dimensiones. También el estilo antiguo de música instrumental y religiosa que se había mantenido durante todo el siglo XVII se renovó en el estilo fugado tonalmente ordenado por J. S. Bach y otros compositores.

La música italiana continuó un desarrollo inmenso, especialmente la ópera. Arcangelo Corelli y otros compositores italianos exploraron y extendieron la música instrumental, que alcanzó en desarrollo a la música vocal. Apareció el concierto para un instrumento solista que los compositores, especialmente Antonio Vivaldi, consolidaron y llevaron a su esplendor. La ópera se enriqueció con el incremento de la participación orquestal y los compositores exploraron las potencialidades expresivas del género. Algunos cantantes de ópera alcanzaron gran fama y eran muy populares, especialmente los "castrati".

La música barroca llegó a su plenitud en las composiciones de Johann Sebastian Bach y Georg Friedrich Händel, junto a la de Domenico Scarlatti, Antonio Vivaldi, Jean Philippe Rameau y Georg Philipp Telemann.

Johann Sebastian Bach, desde la tradición de la iglesia alemana protestante, fusionó los conocimientos musicales de su época. Analizó la obra de los otros compositores copiando y arreglando sus partituras. Así conoció los estilos de los principales compositores de Italia, Francia, Alemania y Austria. De los italianos y, sobre todo, de Vivaldi, aprendió a desarrollar los temas con concisión y en grandes proporciones, así como a ajustar el esquema armónico. Los elementos que asimiló los desarrolló en toda su potencialidad, lo que unido a su maestría en el contrapunto, dio origen a su personal "estilo bachiano". Bach compuso sus obras maestras a partir de 1720, cuando un nuevo estilo forjado en los teatros de ópera italianos se extendía ya por Europa, pareciendo ya anticuada su forma de componer. Por ello el conocimiento completo de su obra debió esperar al siglo XIX.

En cambio Haendel, también alemán de nacimiento pero con una formación musical tanto alemana como italiana, se estableció en Londres y compuso en un lenguaje musical totalmente cosmopolita: óperas italianas, creó el oratorio inglés y dio nuevos significados a otros estilos tradicionales.

La danza no tenía en el siglo XVII la misma consideración de arte que tiene hoy día, y era considerada más bien un pasatiempo, un acto lúdico, aunque con el tiempo fue cobrando protagonismo y empezó a ser considerada como una actividad elevada. Asimismo, si bien en un principio era tan solo un acompañamiento de otras actividades, como el teatro o diversos géneros musicales, progresivamente fue cobrando autonomía respecto a estas modalidades, hasta que en el siglo XVIII se consolidó definitivamente como una actividad artística autónoma. A finales del siglo XVI el principal país donde se otorgaba una cierta importancia a la danza era Francia, con el denominado "ballet de cour", el cual incluso hizo evolucionar la música instrumental, de melodía única pero con una rítmica adaptada a la danza. Aun así, su utilización en la corte francesa era más que nada un acto propagandístico con el que demostrar la magnificencia de la realeza, o con que agasajar a visitantes y diplomáticos, y donde se valoraban más la escenografía, el porte y la elegancia que la coreografía o la habilidad física.

Sin embargo, a principios del siglo XVII el epicentro de la danza varió de Francia a Inglaterra, donde fue favorecida por los Tudor —y posteriormente los Estuardo— con un tipo de espectáculo llamado "masque", donde se conjugaba la música, la poesía, el vestuario y la danza. Una variante de esta modalidad fue la "antimasque", aparecida en 1609 como un complemento a la anterior, donde frente al canto y al diálogo se desarrolló un tipo de espectáculo donde predominaba la actuación y el gesto, el movimiento puramente coreográfico. Con el tiempo, la "antimasque" se separó de la "masque" y pasó a ser un espectáculo autónomo, poniendo los cimientos de la danza moderna.

A mediados del siglo XVII, sin embargo, las mayores innovaciones se dieron nuevamente en Francia, gracias sobre todo al patrocinio del rey Luis XIV, así como al mecenazgo del cardenal Mazarino, que introdujo el gusto por la ópera —género recién surgido en Italia—, en cuyas representaciones era habitual la presencia de ballets en los entreactos. Sin embargo, el hecho de que las óperas eran representadas por aquel entonces en italiano hizo que el público francés prefiriese los ballets que acompañaban a las óperas a estas mismas, por lo que poco a poco fueron ganando importancia. De ello se dio cuenta el músico Jean-Baptiste Lully, que empezó una serie de reformas que convirtieron el ballet en un arte escénico, cercano al que conocemos hoy día. Lully fue el autor del "Ballet Royal de la Nuit" (1653), un gran espectáculo que duró trece horas y donde intervino el propio rey caracterizado de Apolo dios del sol —de donde viene su apodo de Rey Sol—.

Luis XIV favoreció la profesionalización de la danza, para lo que creó la Academia real de Danza en 1661, la primera de esta modalidad en el mundo. En ella desarrolló su labor Pierre Beauchamp, quizá el primer coreógrafo profesional, creador de la "danse d'école", el primer sistema pedagógico de la danza. Beauchamp introdujo el "en dehors" —la rotación de las piernas hacia fuera, uno de los pasos tradicionales del ballet clásico—, así como las cinco posiciones de los pies, que varían en diferentes grados de apertura respecto al eje central del cuerpo. Por otro lado, la Academia favoreció la transformación del ballet en grandes espectáculos donde, además de la danza, destacaban los elementos dramático y musical. Así como el principal referente musical fue, como se ha visto, Lully, a nivel dramático jugó un papel esencial Molière, creador del "comédie-ballet", un género de danza inspirado en la "commedia dell'arte" italiana. Por último, cabría mencionar a Raoul Auger-Feuillet, que en 1700 desarrolló un nuevo sistema de notación de danza, gracias al cual han sobrevivido numerosas coreografías de la época.






</doc>
<doc id="8166" url="https://es.wikipedia.org/wiki?curid=8166" title="Síntoma">
Síntoma

En el ámbito de las ciencias de la salud, un síntoma (del griego: σύμπτωμα, «accidente» o «desgracia») es la referencia subjetiva u objetiva que da un enfermo de la percepción que reconoce como anómala o causada por un estado patológico o una enfermedad. El cansancio o fatiga es un ejemplo de síntoma subjetivo y la fiebre de uno objetivo.. Se diferencia de un signo en que este es un dato observable por parte del especialista.

El síntoma es un aviso útil de que la salud puede estar amenazada por algo psíquico, físico, social o combinación de las mismas.

La palabra "síntoma" fue heredada por el español a través del latín desde el griego ("sýmptōma"), y en realidad es un sustantivo creado a partir del verbo ("sympíptō"), que literalmente significa "caer al mismo tiempo" y, en un sentido más amplio, "concurrir", "ocurrir al mismo tiempo". Galeno hablaba del síntoma como una situación distinta de la enfermedad, los síntomas como "sombras que acompañan a la enfermedad".

El término «síntoma» no se debe confundir con el término «signo», ya que este último es un dato objetivo y objetivable por un especialista.

En medicina y en las ciencias de la salud en general, se entiende por signo clínico cualquier manifestación objetivable consecuente a una enfermedad o alteración de la salud, y que se hace evidente en la biología del enfermo.

La semiología clínica es la disciplina de la que se vale el médico para indagar, mediante el examen psicofísico del paciente, sobre los diferentes signos que puede presentar.

Un signo clínico es un elemento clave que el médico puede percibir en un examen físico, en contraposición a los síntomas que son los elementos subjetivos percibidos solo por el paciente.

Ejemplos de signos clínicos:

Ejemplos de síntomas:


Desde el punto de vista del psicoanálisis, el síntoma es una formación de compromiso (junto con el sueño), el chiste y los actos fallidos (errores al hablar o al escribir) entre el sistema consciente y el sistema inconsciente.



</doc>
<doc id="8168" url="https://es.wikipedia.org/wiki?curid=8168" title="Diwali">
Diwali

El Diwali (también Divali, Deepavali, Deepawali o "festival de las luces"), es un festival hindú que dura cinco días que se celebra en el mes de Kartika. 
El festival comienza en el día denominado Dhanteras, que se celebra el décimo tercer día lunar de Krishna Paksha (cuarto menguante) del mes Ashvin del calendario hindú y finaliza en Bhau-beej, y celebrado el segundo día lunar de Shukla Paksha (cuarto creciente) del mes Kartik Dhanteras, y por lo general cae dieciocho días después de Dussehra. En el calendario Gregoriano, Diwali cae entre mediados de octubre a mediados de noviembre. Festividades similares son celebradas por los miembros de varias religiones en India, como el hinduismo, el sijismo y el jainismo.

Durante el Diwali, celebrado una vez al año, la gente estrena ropa nueva, comparte dulces y hacen explotar petardos y fuegos artificiales. Es la entrada del año nuevo hindú, y una de las noches más significativas y alegres del año.

La divinidad que preside esta festividad es Lakshmí, consorte del dios Vishnú. Ella es quien otorga la prosperidad y la riqueza, por eso es especialmente importante para la casta de los comerciantes "(vaisyas)". También el dios Ganesha es especialmente venerado ese día. En el este del país se venera particularmente a la diosa Kali.

En esa ocasión, los sikhs celebran la liberación de su sexto gurú, Hargonbind, y hacen un homenaje a los diez gurús espirituales del sikhismo.

La fiesta tiene lugar en el decimoquinto día de la quincena oscura del mes de kārttika (que cada año puede caer entre el 21 de octubre y el 18 de noviembre), y puede durar cuatro o cinco días. Conmemora la muerte del demonio Narakasura a manos de Krishna y la liberación de dieciséis mil doncellas que este tenía prisioneras. Celebra también el regreso a la ciudad de Ayodhyā del príncipe Rāma tras su victoria sobre Rāvaṇa, rey de los demonios. Según la leyenda, los habitantes de la ciudad llenaron las murallas y los tejados con lámparas para que Rāma pudiera encontrar fácilmente el camino. De ahí comenzó la tradición de encender multitud de luces durante la noche.

Las casas se limpian de forma especial y se adornan con diversos motivos y lámparas de aceite o velas que se encienden al atardecer. Es usual celebrar una comida compuesta de sabrosos platos y dulces, hacer regalos a las personas cercanas y familiares, los fuegos artificiales y los juegos. Es el momento para renovar los libros de cuentas, hacer limpieza general, reemplazar algunos enseres del hogar, pintarlo y decorarlo para el año entrante. Es tradición que la diosa favorecerá de forma especial a quienes se reconcilien con sus enemigos.

Se aconseja instalar un altar en un lugar preferente de las casas donde este presente una imagen de Lakshmí a la que se le ofrecerán flores, incienso y monedas mientras se repite el mantra:

Al anochecer se abren todas las ventanas y puertas de las casas y en cada una de ellas se realiza un ofrecimiento de luz con una lámpara de aceite o una vela, repitiendo el mismo mantra, para que Lakshmí entre para el resto del año. También se lanzan barcos de papel o lamparillas encendidas a los ríos sagrados, cuanto más lejos vayan, mayor será la felicidad en el año venidero y se elaboran unos diseños llamados "manorā", que son unos dibujos hechos en las paredes y que se adornan durante el festival. A la salida del sol es de ritual lavarse la cabeza, lo que tiene el mismo mérito que bañarse en el sagrado río Gangā (el Ganges).

El simbolismo de la fiesta consiste en la necesidad del hombre de avanzar hacia la luz de la Verdad desde la ignorancia y la infelicidad, es decir, obtener la victoria del dharma (la virtud) sobre adharma (falta de virtud).

Según los jainistas, en este día de Diwali el Tirthankara Mahavirá alcanzó la liberación (moksha/nirvana/siddha) (549 – 477 a. C.). Mahavira es conocido como el creador del jainismo.


</doc>
<doc id="8171" url="https://es.wikipedia.org/wiki?curid=8171" title="Río">
Río

Un río o flujo de agua es una corriente natural de agua que fluye con continuidad por un cauce en la superficie terrestre.
Posee un caudal determinado, rara vez es constante a lo largo del año, y desemboca en el mar, en un lago o en otro río, en cuyo caso se denomina afluente. La parte final de un río es su desembocadura. Algunas veces terminan en zonas desérticas donde sus aguas se pierden por infiltración y evaporación.

Este vocablo etimológicamente viene del latín «rivus» que quiere decir arroyo.

Por lo general los ríos, especialmente los más grandes, se dividen en tres partes principales de acuerdo con su capacidad erosiva y de transporte de sedimentos:

El curso superior de un río es donde estos nacen. Generalmente, coincide con las áreas montañosas de una cuenca determinada. Aquí el potencial erosivo es mucho mayor y los ríos suelen formar valles en forma de V al encajarse en el relieve. Cuando esta parte de un río se encuentra en un clima seco pueden denominarse a veces barrancos, ramblas o torrentes.

Generalmente, en el curso medio de un río suelen alternarse algunas de las áreas o zonas donde el río erosiona y donde deposita parte de sus sedimentos, lo cual se debe, principalmente, a las fluctuaciones de la pendiente y a la influencia que reciben con respecto al caudal y sedimentos de sus afluentes. A lo largo del curso medio, la sección transversal del río habitualmente se irá suavizando, tomando forma de palangana seccionada en lugar de la forma de V que prevalece en el curso superior.
A lo largo del curso medio, el río sigue teniendo la suficiente energía como para mantener un curso aproximadamente recto, excepto que haya obstáculos, como por ejemplo diversas curvas o montículos.

Es la parte en donde el río fluye en áreas relativamente planas, donde suele formar meandros: establece curvas regulares, pudiendo llegar a formar lagos en herradura. Al fluir el río, acarrea grandes cantidades de sedimentos, los que pueden dar origen a islas sedimentarias, llamadas deltas y también puede ocasionar la elevación del cauce por encima del nivel de la llanura, por lo que muchos ríos suelen discurrir paralelos al mismo por no poder desembocar por la mayor elevación del río principal: son los ríos tipo Yazoo. De un río que termina en una boca muy ancha y profunda se denomina estuario. 

El río principal suele ser definido como el curso con mayor caudal de agua (medio o máximo) o bien con mayor longitud o mayor área de drenaje. Este concepto de río principal, como el de "nacimiento" de un río o la distinción entre río principal y afluente, son arbitrarios.En muchos casos se presentan dudas acerca del nombre y recorrido de los ríos, sobre todo en cuencas hidrográficas de relieve heterogéneo y de gran extensión, en las que no ha existido un criterio común acerca de las dimensiones del río principal y de sus afluentes. En otros casos, existen varias denominaciones para un mismo río, a lo largo de su recorrido. Ejemplos de ríos cuyos nombres se han discutido con relación a dónde podemos fijar su nacimiento o con afluentes más importantes que el río principal podemos señalar:

En otros casos, un mismo río tiene nombres distintos a lo largo de su recorrido, especialmente en los casos en que se forman brazos o canales a partir de un cauce y cada uno de esos brazos toma un nombre distinto. Es el caso del Apure (Apure Seco, Apure Viejo y Apurito). También el Magro podría incluirse en este caso (rambla de la Torre, río Magro, Alcalá, rambla de Algemesí, etc.)

Los ríos pueden ser clasificados desde diversos puntos de vista, entre los que se destacan:

Algunos ríos cortos y torrentes pueden fluir desde su cabecera o inicio hasta el mar sin convertirse en afluentes o tributarios de otro mayor, ni recibir agua de otros ríos. En general, un río forma parte de una red de drenaje (o sistema fluvial) ocupando una cuenca hidrográfica. Algunas cuencas abarcan pocos kilómetros cuadrados, en cambio la cuenca del Amazonas se extiende a lo largo de 6,14 millones de km² (véase la ).

Las cuencas de los ríos y sus redes de drenaje pueden cambiar de forma natural en periodos relativamente cortos de tiempo como consecuencia de capturas fluviales.

Los ríos erosionan rocas y sedimentos, llegando a abrir cauces y valles, modelando el paisaje en lo que se denomina modelado fluvial. El cauce profundo del río Colorado, ha recortado en algunos lugares hasta una profundidad de 1,5km, formando el Gran Cañón. Y el cañón del río Majes o río Camaná, en el Perú, es todavía más profundo, con unos 3km de profundidad.

Los valles fluviales en general tienen forma de V, sobre todo, en las zonas montañosas de levantamiento reciente, pero esta forma se modifica a lo largo del curso del río, ampliando además su tamaño, pendiente, perfil transversal, capacidad de transporte de sedimentos y otras muchas características.

La flora y fauna de los ríos es diferente a la que se encuentra en los océanos porque el agua tiene distintas características, especialmente la salinidad. Las especies que habitan los ríos se han tenido que adaptar a las corrientes y a los desniveles. Sin embargo, existen numerosas excepciones, como es el caso de los salmones que desovan en las cuencas superiores o montañosas de los ríos o el de los tiburones de agua dulce de Nicaragua, y también en el caso de las especies marinas que penetran en los deltas oceánicos llevados por la pleamar de las mareas y corrientes oceánicas, tal como sucede en los deltas del Orinoco y del Amazonas. Lo mismo sucede con los estuarios de los ríos, aunque en este caso, la entrada de especies marinas en los ríos suele ser momentánea durante el flujo o pleamar lo cual se debe a que se vacían durante el reflujo o bajamar mientras que en los deltas, lo que cambia durante las mareas es la mayor o menor salinidad de sus aguas.

Algunos peces de agua dulce son: 

El agua es un recurso renovable en peligro por culpa de la actividad humana. Toda el agua pura procedente de las lluvias, ya antes de llegar al suelo recibe su primera carga contaminante, cuando disuelve sustancias como anhídrido carbónico, óxido de azufre y de nitrógeno que la convierten en lluvia ácida. Ya en el suelo, el agua discurre por la superficie o se filtra hacia capas subterráneas. Al atravesar los campos el agua del río se carga de pesticidas y cuando pasa por ciudades arrastra productos como naftas, aceites de automóvil, metales pesados, etc. Los ríos muestran una cierta capacidad de deshacerse de los contaminantes, pero para eso necesitan tener un tramo muy largo en el cual las bacterias puedan realizar su trabajo depurador. En un río contaminado por materia orgánica se distinguen tres zonas a partir del punto de contaminación:

a) Zona polisaprobia: Es la más contaminada. Elevada población de bacterias.

b) Zona mesosaprobia: Contaminación media. Las bacterias ya han eliminado gran parte de la contaminación orgánica.

c) Zona oligosaprobia: El agua está en condiciones similares a las que tenía antes de que se hubiera producido la contaminación.

Resulta difícil medir la longitud exacta de un río debido a las propiedades del terreno por donde fluye. A continuación se listan los 10 mayores ríos del mundo con una longitud aproximada:

Todo Estado ejerce soberanía territorial sobre el curso o porción del curso de un río que forma parte de su territorio.

Existen dos categorías de ríos: 

Los ríos internacionalizados son aquellos en los cuales existe libertad de navegación que según sea más o menos amplia puede ser a favor de todas las banderas o solo de los ribereños.

La policía y administración de un río abierto a la libre navegación suscita numerosos problemas: el régimen aduanero, el pilotaje, los reglamentos de puerto, las tasas, etc. 
La norma general es que en principio la administración de cada sector del río es ejercida por el respectivo ribereño.
Una fórmula que ha llegado a evitar problemas, sobre todo en los ríos europeos donde la concentración de la navegación es muy grande, consiste en el establecimiento de comisiones internacionales de administración fluvial.

Potamología es el estudio de las aguas fluviales (del griego Ποταμός "potamós" = ‘río’), que abarca conceptos como los de su caudal, cauce, cuenca, curso o corriente, régimen fluvial, dinámica fluvial, perfiles (longitudinal y transversal), afluentes y su importancia, ecología, flora, fauna, recursos hídricos e hidroeléctricos o navegación fluvial, entre otros. Vendría a ser una parte de la hidrografía.


</doc>
<doc id="8174" url="https://es.wikipedia.org/wiki?curid=8174" title="VIH/sida">
VIH/sida

La infección por el virus de la inmunodeficiencia humana y el síndrome de inmunodeficiencia adquirida (VIH/sida) son un espectro de enfermedades causadas por la infección causada por el virus de la inmunodeficiencia humana (VIH). Tras la infección inicial, una persona puede no notar síntoma alguno o bien puede experimentar un periodo breve de cuadro tipo influenza. Típicamente, le sigue un periodo prolongado sin síntomas. A medida que la infección progresa, interfiere más con el sistema inmunitario, aumentando el riesgo de infecciones comunes como la tuberculosis, además de otras infecciones oportunistas y tumores que raramente afectan a las personas con un sistema inmunitario indemne. Estos síntomas tardíos de infección se conocen como sida, etapa que a menudo también está asociada con pérdida de peso.

El VIH se contagia principalmente por sexo desprotegido (incluido sexo anal y oral), transfusiones de sangre contaminada, agujas hipodérmicas y de la madre al niño durante el embarazo, parto o lactancia. Algunos fluidos corporales, como la saliva y las lágrimas, no transmiten el VIH. Entre los métodos de prevención se encuentran el sexo seguro, los programas de intercambio de agujas, el tratamiento a los infectados y la circuncisión. La infección del bebé a menudo puede prevenirse al dar medicación antirretroviral tanto a la madre como el niño. No hay ninguna cura o vacuna; no obstante, el tratamiento antirretroviral puede retrasar el curso de la enfermedad y puede llevar a una expectativa de vida cercana a la normal. Se recomienda iniciar el tratamiento apenas se haga el diagnóstico. Sin tratamiento, el tiempo de vida promedio después de la infección es 11 años.

En 2014 aproximadamente 36,9 millones de personas vivían con VIH y causó 1,2 millones de muertes. La mayoría de los infectados viven en el África subsahariana. Entre su descubrimiento y el 2014 el sida ha causado un estimado de 39 millones muertes en todo el mundo. El VIH/sida se considera una pandemia: un brote de enfermedad presente en un área grande y con propagación activa. Sobre la base de estudios genéticos, se ha determinado que el VIH es una mutación del VIS que se transmitió a los humanos entre 1910 y 1930, en el centro-oeste de África. El sida fue reconocido por primera vez por los Centros para el Control y Prevención de Enfermedades de los Estados Unidos en 1981 y su causa (la infección por VIH) se identificó a principios de dicha década.

El VIH/sida ha tenido un gran impacto en la sociedad, como enfermedad y como fuente de discriminación. La enfermedad también tiene fuertes impactos económicos. Hay muchas ideas equivocadas sobre el VIH/sida como la creencia de que puede transmitirse por contacto casual no sexual. La enfermedad ha sido centro de muchas controversias relacionadas con la religión, incluida la decisión de la Iglesia católica de no apoyar el uso de preservativo como prevención. El VIH/sida ha atraído la atención internacional médica y política así como financiación masiva desde su identificación en los años 1980.

En la siguiente tabla se contemplan los diferentes estados de la infección por VIH.



El VIH se multiplica, después de la fase aguda primaria de la infección, en los órganos linfoides, sobrecargándolos con un esfuerzo que termina por provocar una reducción severa de la producción de linfocitos. El debilitamiento de las defensas abre la puerta al desarrollo de infecciones oportunistas por bacterias, hongos, protistas y virus. En muchos casos los microorganismos responsables están presentes desde antes, pero desarrollan una enfermedad solo cuando dejan de ser contenidos por los mecanismos de inmunidad celular que el VIH destruye. Ninguna de estas enfermedades agrede solo a los VIH positivos, pero algunas eran casi desconocidas antes de la epidemia de VIH y en muchos casos las variantes que acompañan o definen al sida son diferentes por su desarrollo o su epidemiología.

La era del sida empezó oficialmente el 5 de junio de 1981, cuando los Centers for Disease Control and Prevention (CDC) —Centros para el Control y Prevención de Enfermedades de Estados Unidos— convocaron una conferencia de prensa donde describieron cinco casos de neumonía por "Pneumocystis carinii" en Los Ángeles. Al mes siguiente se constataron varios casos de sarcoma de Kaposi, un tipo de cáncer de piel. Las primeras constataciones de estos casos fueron realizadas por el doctor Michael Gottlieb de San Francisco.

Pese a que los médicos conocían tanto la neumonía por "Pneumocystis carinii" como el sarcoma de Kaposi, la aparición conjunta de ambos en varios pacientes les llamó la atención. La mayoría de estos pacientes eran hombres homosexuales sexualmente activos, muchos de los cuales también sufrían de otras enfermedades crónicas que más tarde se identificaron como infecciones oportunistas. Las pruebas sanguíneas que se les hicieron a estos pacientes mostraron que carecían del número adecuado de un tipo de células sanguíneas llamadas T CD4+. La mayoría de estos pacientes murieron en pocos meses.

Por la aparición de unas manchas de color rosáceo en el cuerpo del infectado, la prensa comenzó a llamar al sida, la «peste rosa», causando una confusión, atribuyéndola a los homosexuales, aunque pronto se hizo notar que también la padecían los inmigrantes haitianos en Estados Unidos, los usuarios de drogas inyectables y los receptores de transfusiones sanguíneas, lo que llevó a hablar de un "club de las cuatro haches" que incluía a todos estos grupos considerados de riesgo para adquirir la enfermedad. En 1982, la nueva enfermedad fue bautizada oficialmente con el nombre de "Acquired Immune Deficiency Syndrome (AIDS)", nombre que sustituyó a otros propuestos como "Gay-related immune deficiency" (GRID).

Hasta 1984 se sostuvieron distintas teorías sobre la posible causa del sida. La teoría con más apoyo planteaba que el sida era una enfermedad básicamente epidemiológica. En 1983 un grupo de nueve hombres homosexuales con sida de Los Ángeles, que habían tenido parejas sexuales en común, incluyendo a otro hombre en Nueva York que mantuvo relaciones sexuales con tres de ellos, sirvieron como base para establecer un patrón de contagio típico de las enfermedades infecciosas.

Otras teorías sugieren que el sida surgió a causa del excesivo uso de drogas y de la alta actividad sexual con diferentes parejas. También se planteó que la inoculación de semen en el recto durante la práctica de sexo anal, combinado con el uso de inhalantes con nitrito llamados "poppers", producía supresión del sistema inmunológico. Pocos especialistas tomaron en serio estas teorías, aunque algunas personas todavía las promueven y niegan que el sida sea producto de la infección del VIH.

La teoría más reconocida actualmente, sostiene que el VIH proviene de un virus llamado «virus de inmunodeficiencia en simios» (SIV, en inglés), el cual es idéntico al VIH y causa síntomas similares al sida en otros primates. Según un estudio publicado en 2014, el virus entraría en los seres humanos por primera vez en los años 20 del siglo XX, en el centro de África.

En 1984, dos científicos franceses, Françoise Barré-Sinoussi y Luc Montagnier del Instituto Pasteur, aislaron el virus de sida y lo purificaron. Robert Gallo, estadounidense, pidió muestras al laboratorio francés, y adelantándose a los franceses lanzó la noticia de que había descubierto el virus y que había realizado la primera prueba de detección y los primeros anticuerpos para combatir a la enfermedad. Después de diversas controversias legales, se decidió compartir patentes, pero el descubrimiento se le atribuyó a los dos investigadores originales que aislaron el virus, y solo a ellos dos se les concedió el Premio Nobel conjunto, junto a otro investigador en el 2008, reconociéndolos como auténticos descubridores del virus, aceptándose que Robert Gallo se aprovechó del material de otros investigadores para realizar todas sus observaciones. En 1986 el virus fue denominado VIH (virus de inmunodeficiencia humana). El descubrimiento del virus permitió el desarrollo de un anticuerpo, el cual se comenzó a utilizar para identificar dentro de los grupos de riesgo a los infectados. También permitió empezar investigaciones sobre posibles tratamientos y una vacuna.

En esos tiempos las víctimas del sida eran aisladas por la comunidad, los amigos e incluso la familia. Los niños que tenían sida no eran aceptados por las escuelas debido a las protestas de los padres de otros niños; este fue el caso del joven estadounidense Ryan White. La gente temía acercarse a los infectados ya que pensaban que el VIH podía contagiarse por un contacto casual como dar la mano, abrazar, besar o compartir utensilios con un infectado.

En un principio la comunidad homosexual fue culpada de la aparición y posterior expansión del sida en Occidente. Incluso algunos grupos religiosos llegaron a decir que el sida era un castigo de Dios a los homosexuales (esta creencia aún es popular entre ciertas minorías de creyentes cristianos y musulmanes). Otros señalan que el estilo de vida «depravado» de los homosexuales era responsable de la enfermedad. Aunque en un principio el sida se expandió más deprisa a través de las comunidades homosexuales, y que la mayoría de los que padecían la enfermedad en Occidente eran homosexuales, esto se debía, en parte, a que en esos tiempos no era común el uso del condón entre homosexuales, por considerarse que este era solo un método anticonceptivo. Por otro lado, la difusión del mismo en África fue principalmente por vía heterosexual.

El sida pudo expandirse rápidamente al concentrarse la atención solo en los homosexuales, esto contribuyó a que la enfermedad se extendiera sin control entre heterosexuales, particularmente en África, el Caribe y luego en Asia.

Gracias a la disponibilidad de tratamiento antirretrovirales, las personas con VIH pueden llevar una vida normal, la correspondiente a una enfermedad crónica, sin las infecciones oportunistas características del sida no tratado. Los antirretrovirales están disponibles mayormente en los países desarrollados. Su disponibilidad en los países en desarrollo está creciendo, sobre todo en América Latina; pero en África, Asia y Europa Oriental muchas personas todavía no tienen acceso a esos medicamentos, por lo cual desarrollan las infecciones oportunistas y mueren algunos años después de la seroconversión.

El VIH está emparentado con otros virus que causan enfermedades parecidas al sida. Se cree que este virus se transfirió de los animales a los humanos a comienzos del siglo XX. Existen dos virus diferenciados que causan sida en los seres humanos, el VIH-1 y el VIH-2. Del primero la especie reservorio son los chimpancés, de cuyo virus propio, el SIVcpz, deriva. El VIH-2 procede del SIVsm, propio de una especie de monos de África Occidental. En ambos casos la transmisión entre especies se ha producido varias veces, pero la actual pandemia resulta de la extensión del grupo M del VIH-1, procedente según estimaciones de una infección producida en África Central, donde el virus manifiesta la máxima diversidad, en la primera mitad del siglo XX.

La pandemia actual arrancó en África Central, pero pasó inadvertida mientras no empezó a afectar a población de países ricos, en los que la inmunosupresión del sida no podía confundirse fácilmente con depauperación debida a otras causas, sobre todo para sistemas médicos y de control de enfermedades muy dotados de recursos. La muestra humana más antigua que se sepa que contiene VIH fue tomada en 1959 a un marino británico, quien aparentemente la contrajo en lo que ahora es la República Democrática del Congo. Otras muestras que contenían el virus fueron encontradas en un hombre estadounidense que murió en 1969 y en un marino noruego en 1976. Se cree que el virus se contagió a través de actividad sexual, posiblemente a través de prostitutas, en las áreas urbanas de África. A medida que los primeros infectados viajaron por el mundo, fueron llevando la enfermedad a varias ciudades de distintos continentes.

En la actualidad, la manera más común en que se transmite el VIH es a través de actividad sexual desprotegida y al compartir agujas entre usuarios de drogas inyectables. El virus también puede ser transmitido desde una madre embarazada a su hijo (transmisión vertical). En el pasado también se transmitió el sida a través de transfusiones de sangre y el uso de productos derivados de esta para el tratamiento de la hemofilia o por el uso compartido de material médico sin esterilizar; sin embargo, hoy en día esto ocurre muy raramente, salvo lo último en regiones pobres, debido a los controles realizados sobre estos productos. No es posible para los mosquitos u otros insectos transmitir el VIH.

No todos los pacientes infectados con el virus VIH tienen sida. El criterio para diagnosticar el sida puede variar de región en región, pero el diagnóstico generalmente requiere:


La persona infectada por el VIH es denominada «seropositiva» o «VIH positivo» (VIH+) y a los no infectados se les llama «seronegativos» o «VIH negativo» (VIH–). La mayoría de las personas seropositivas no saben que lo son.

La infección primaria por VIH es llamada «seroconversión» y puede ser acompañada por una serie de síntomas inespecíficos, parecidos a los de una gripe, por ejemplo, fiebre, dolores musculares y articulares, dolor de garganta y ganglios linfáticos inflamados. En esta etapa el infectado es más transmisor que en cualquier otra etapa de la enfermedad, ya que la cantidad de virus en su organismo es la más alta que alcanzará. Esto se debe a que todavía no se desarrolla por completo la respuesta inmunológica del huésped. No todos los recién infectados con VIH padecen de estos síntomas y finalmente todos los individuos se vuelven asintomáticos.

Durante la etapa asintomática, cada día se producen varios miles de millones de virus VIH, lo cual se acompaña de una disminución de las células T CD4+. El virus no solo se encuentra en la sangre, sino en todo el cuerpo, particularmente en los ganglios linfáticos, el cerebro y las secreciones genitales.

El tiempo que demora el diagnóstico de sida desde la infección inicial del virus VIH es variable. Algunos pacientes desarrollan algún síntoma de inmunosupresión muy pocos meses después de haber sido infectados, mientras que otros se mantienen asintomáticos hasta 20 años.

La razón por la que algunos pacientes no desarrollan la enfermedad y por qué hay tanta variabilidad interpersonal en el avance de la enfermedad, todavía es objeto de estudio. El tiempo promedio entre la infección inicial y el desarrollo del sida varía entre ocho a diez años en ausencia de tratamiento.

El 3 de octubre de 2016, apareció otra posible cura al aplicarle un tratamiento nuevo a un paciente británico, ya que al realizarle exámenes de sangre, no se encontró rastro alguno del virus presente en él. El es parte de una serie de pruebas llevadas a cabo por investigadores de las universidades de Oxford, Cambridge, Imperial College London, University College London y King's College London.

La nueva terapia trabaja en dos fases. En la primera, una vacuna ayuda a que el cuerpo reconozca las células infectadas de VIH para poder destruirlas. En la segunda fase, una nueva droga llamada Vorinostat activa las células T latentes para ser identificadas por el sistema inmune.

Los resultados de las pruebas se espera que estén listos para el 2018.

En los países occidentales el índice de infección con VIH ha disminuido ligeramente debido a la adopción de prácticas de sexo seguro por los varones homosexuales y (en menor grado) a la existencia de distribución gratuita de jeringas y campañas para educar a los usuarios de drogas inyectables acerca del peligro de compartir las jeringas. La difusión de la infección en los heterosexuales ha sido un poco más lenta de lo que originalmente se temía, posiblemente porque el VIH es ligeramente menos transmisible por las relaciones sexuales vaginales —cuando no hay otras enfermedades de transmisión sexual presentes— de lo que se creía antes.

Sin embargo, desde finales de los años 1990, en algunos grupos humanos del Primer Mundo los índices de infección han empezado a mostrar signos de incremento otra vez. En el Reino Unido el número de personas diagnosticadas con VIH se incrementó un 26% desde 2000 a 2001. Las mismas tendencias se notan en EE. UU. y Australia. Esto se atribuye a que las generaciones más jóvenes no recuerdan la peor fase de la epidemia en los ochenta y se han cansado del uso del condón. El sida continúa siendo un problema entre las prostitutas y los usuarios de drogas inyectables. Por otro lado el índice de muertes debidas a enfermedades relacionadas con el sida ha disminuido en los países occidentales debido a la aparición de nuevas terapias de contención efectivas (aunque más costosas) que aplazan el desarrollo del sida.

En países subdesarrollados, en particular en la zona central y sur de África, las malas condiciones económicas (que llevan por ejemplo a que en los centros de salud se utilicen jeringas ya usadas) y la falta de educación sexual debido a causas principalmente religiosas, dan como resultado un altísimo índice de infección (ver sida en África). En algunos países más de un cuarto de la población adulta es VIH-positiva; solamente en Botsuana el índice llega al 35,8% (estimado en 1999, fuente en inglés World Press Review). La situación en Sudáfrica —con un 66% de cristianos y con el presidente Thabo Mbeki, que comparte, aunque ya no de manera oficial, la opinión de los «disidentes del sida»— se está deteriorando rápidamente. Solo en 2002 hubo casi 4,7 millones de infecciones. Otros países donde el sida está causando estragos son Nigeria y Etiopía, con 3,7 y 2,4 millones de infectados el año 2003, respectivamente. Por otro lado, en países como Uganda, Zambia y Senegal se han iniciado programas de prevención para reducir sus índices de infección con VIH, con distintos grados de éxito.

Las tasas de infección de VIH también han aumentado en Asia, con cerca de 7,5 millones de infectados en el año 2003. En julio de 2003, se estimaba que había 4,6 millones de infectados en India, lo cual constituye aproximadamente el 0,9% de la población adulta económicamente activa. En China, la cantidad de infectados se estimaba entre 1 y 1,5 millones, aunque algunos creen que son aún más los infectados. Por otra parte, en países como Tailandia y Camboya se ha mantenido constante la tasa de infección por VIH en los últimos años.

Recientemente ha habido preocupación respecto al rápido crecimiento del sida en la Europa oriental y Asia central, donde se estima que había 1,7 millones de infectados a enero de 2004. La tasa de infección del VIH ha ido en aumento desde mediados de los 1990, debido a un colapso económico y social, aumento del número de usuarios de drogas inyectables y aumento del número de prostitutas. En Rusia se reportaron 257 000 casos en 2004 de acuerdo a información de la Organización Mundial de la Salud; en el mismo país existían 15 000 infectados en 1995 y 190 000 en 2002. Algunos afirman que el número real es cinco veces el estimado, es decir, cerca de un millón. Ucrania y Estonia también han visto aumentar el número de infectados, con estimaciones de 500 000 y 3700 a comienzos de 2004, respectivamente.

Según el Fondo de las Naciones Unidas para las Mujeres (UNIFEM), a pesar de que la infección del VIH comenzó concentrándose básicamente en hombres, a día de hoy, las mujeres suponen el 50% de las personas infectadas con el VIH. Incluso en regiones como el África Subsahariana, las mujeres representan el 60% del total de la población con VIH.

Los hombres que tienen sexo con hombres corresponden al 2% de la población estadounidense, pero el 55% de los nuevos casos de VIH en dicho país en 2013.

Una vez que un individuo contrae el VIH, es altamente probable que en el transcurso de su vida llegue a desarrollar sida. Si bien algunos portadores permanecen en estado asintomático por largos períodos de tiempo, la única manera de evitar el sida consiste en la prevención de la infección por VIH. La única vía para la transmisión del virus es a través de los fluidos corporales como la sangre. Este virus no se puede transmitir a través de la respiración, la saliva, el contacto casual por el tacto, dar la mano, abrazar, besar en la mejilla, masturbarse mutuamente con otra persona o compartir utensilios como vasos, tazas o cucharas. En cambio, teóricamente es posible que el virus se transmita entre personas a través del beso boca a boca, si ambas personas tienen llagas sangrantes o encías llagadas, pero ese caso no ha sido documentado y además es considerado muy improbable, ya que la saliva contiene concentraciones mucho más bajas que por ejemplo el semen.

La infección por VIH por las relaciones sexuales ha sido comprobado de hombre a mujer, de mujer a hombre, de mujer a mujer y de hombre a hombre. El uso de condones de látex se recomienda para todo tipo de actividad sexual que incluya penetración. Es importante enfatizar que se debe usar el condón hecho del material látex, pues otro condón (de carnero) que existe en el mercado, hecho a base de material orgánico, no es efectivo para la prevención. Los condones tienen una tasa estimada del 90-95% de efectividad para evitar el embarazo o el contagio de enfermedades, y usado correctamente, esto es, bien conservado, abierto con cuidado y correctamente colocado, es el mejor medio de protección contra la transmisión del VIH. Se ha demostrado repetidamente que el VIH no pasa efectivamente a través de los condones de látex intactos.

El sexo anal, debido a la delicadeza de los tejidos del ano y la facilidad con la que se llagan, se considera la actividad sexual de más riesgo. Por eso los condones se recomiendan también para el sexo anal. El condón se debe usar una sola vez, tirándolo a la basura y usando otro condón cada vez. Debido al riesgo de rasgar (tanto el condón como la piel y la mucosa de la paredes vaginales y anales) se recomienda el uso de lubricantes con base acuosa. La vaselina y los lubricantes basados en aceite o petróleo no deben usarse con los condones porque debilitan el látex y lo vuelven propenso a rasgarse.

En términos de trasmisión del VIH, se considera que el sexo oral tiene menos riesgos que el vaginal o el anal. Sin embargo, la relativa falta de investigación definitiva sobre el tema, sumada a información pública de dudosa veracidad e influencias culturales, han llevado a que muchos crean, de manera incorrecta, que el sexo oral es seguro. Aunque el factor real de trasmisión oral del VIH no se conoce aún con precisión, hay casos documentados de transmisión a través de sexo oral por inserción y por recepción (en hombres). Un estudio concluyó que el 7,8% de hombres recientemente infectados en San Francisco probablemente recibieron el virus a través del sexo oral. Sin embargo, un estudio de hombres españoles que tuvieron sexo oral con compañeros VIH+ a sabiendas de ello no identificó ningún caso de trasmisión oral. Parte de la razón por la cual esa evidencia es conflictiva es porque identificar los casos de transmisión oral es problemático. La mayoría de las personas VIH+ tuvieron otros tipos de actividad sexual antes de la infección, por lo cual se hace difícil o imposible aislar la transmisión oral como factor. Factores como las úlceras bucales, etc., también son difíciles de aislar en la transmisión entre personas "sanas". Se recomienda usualmente no permitir el ingreso de semen o fluido preseminal en la boca. El uso de condones para el sexo oral (o protector dental para el cunnilingus) reduce aún más el riesgo potencial. El condón que haya sido utilizado ya para la práctica del sexo oral, debe desecharse. En caso de que exista coito posterior, se utilizará un nuevo profiláctico; ya que las microlesiones que se producen en el látex por el roce con las piezas dentarias, permiten el paso del virus.

Se sabe que el VIH se transmite cuando se comparten agujas entre usuarios de drogas inyectables, y este es uno de las maneras más comunes de transmisión. Todas las organizaciones de prevención del sida advierten a los usuarios de drogas que no compartan agujas, y que usen una aguja nueva o debidamente esterilizada para cada inyección. Los centros y profesionales del cuidado de la salud y de las adicciones disponen de información sobre la limpieza de agujas con lejía. En los Estados Unidos y en otros países occidentales están disponibles agujas gratis en algunas ciudades, en lugares de intercambio de agujas, donde se reciben nuevas a cambio de las usadas, o en sitios de inyecciones seguras.

Los trabajadores médicos pueden prevenir la extensión del VIH desde pacientes a trabajadores y de paciente a paciente, siguiendo normas universales de asepsia o aislamiento contra sustancias corporales, tales como el uso de guantes de látex cuando se ponen inyecciones o se manejan desechos o fluidos corporales, y lavándose las manos frecuentemente.

El riesgo de infectarse con el virus VIH a causa de un pinchazo con una aguja que ha sido usada en una persona infectada es menor de 1 entre 200. Una apropiada profilaxis postexposición con medicamentos anti-VIH logra contrarrestar ese riesgo, reduciendo al mínimo la probabilidad de seroconversión.

Un estudio de 2005 informaba que el estar circuncidado podría reducir significativamente la probabilidad de que un hombre se infecte de una mujer seropositiva por penetración vaginal. Los rumores en este sentido, producidos a partir de trabajos anteriores no concluyentes, han aumentado ya la popularidad de la circuncisión en algunas partes de África. Un trabajo relacionado estima que la circuncisión podría convertirse en un factor significativo en la lucha contra la extensión de la epidemia.

Investigaciones recientes confirmaron que de hecho existen personas más resistentes al Virus, debido a una mutación en el genoma llamada "CCR5-delta 32". Según se cree, habría aparecido hace 700 años, cuando la peste bubónica diezmó a Europa. La teoría dice que los organismos con ese gen impiden que el virus ingrese en el glóbulo blanco. Este mecanismo es análogo al de la peste negra. El VIH se desarrolla en estas personas de manera más lenta, y han sido bautizados como "no progresores a largo plazo".

Después de la sangre, la saliva fue el segundo fluido del cuerpo donde el VIH se aisló. El origen del VIH salivar son los linfocitos infectados de las encías (gingiva). Estas células emigran dentro de la saliva en una tasa de un millón por minuto. Esta migración puede aumentar hasta 10 veces (diez millones de células por minuto) en enfermedades de la mucosa oral, las cuales son frecuentes en un huésped inmunodeficiente (tal como un individuo con infección por VIH). Estudios inmunocitoquímicos recientes muestran que en los pacientes con sida hay una concentración más alta de VIH en los linfocitos salivares que en los linfocitos de la sangre periférica. Esto sugiere que los linfocitos infectados reciben una estimulación antigénica por la flora oral (bacterias en la boca) lo que da lugar a una mayor expresión del virus". (A. Lisec, "Za zivot", izdanje "U pravi trenutak", Dakovo 1994. s.27O-271.)

Edward Green, director del Aids Prevention Research Project de Harvard, asegura que «El preservativo no detiene el sida. Solo un comportamiento sexual responsable puede hacer frente a la pandemia».
Por otra parte, según algunos estudios, los programas que preconizan la abstinencia sexual como método preventivo exclusivo no han demostrado su utilidad para disminuir el riesgo de contagio del virus en países desarrollados.

En el África subsahariana, y otros países subdesarrollados, se ha mostrado eficaz en la lucha contra el sida el fomento de la monogamia y el retraso de la actividad sexual entre los jóvenes.

Según un estudio publicado en la revista científica especializada Science Translational Medicine, un equipo de investigadores del Servicio de Enfermedades Infecciosas y Sida del Hospital Clínico de Barcelona ha dado un paso más en este camino al presentar una vacuna terapéutica que ha mostrado en los ensayos resultados alentadores.

En las pruebas realizadas a 36 pacientes que seguían una terapia antirretroviral (conocida como TAR), tras la vacunación de prueba "cambió el equilibrio virus / huésped a favor del huésped", o lo que es lo mismo, el virus perdía la batalla de la infección. Según los datos, tras 12 semanas, la reducción de la carga viral gracias a la vacuna era del 90 por ciento, aunque posteriormente el virus se hace resistente y consigue paliar el efecto de la vacuna.

Para conseguir frenar el avance del virus del sida los investigadores pulsaron células dendríticas (aquellos leucocitos que presentan antígenos al sistema inmunitario) de los propios pacientes con VIH y las inactivaron con calor. De este modo, cuando las células dendríticas "presentaban" al virus a los linfocitos encargados de eliminar al agente infeccioso externo, el VIH no conseguía infectar al linfocito (como ocurre normalmente), sino que consigue transmitir adecuadamente el mensaje para activar el sistema inmunitario y terminar con el agente externo infectante.

En 2017 se consiguió que un grupo de vacas generaran anticuerpos contra el virus del sida después de recibir una inyección de proteínas con la molécula BG505 SOSIP.

El tratamiento consiste en el uso de antirretrovirales, fármacos que inhiben enzimas esenciales para la replicación del [[VIH], como la transcriptasa inversa, [[retrotranscriptasa]] o la proteasa. De esta manera se frena el progreso de la enfermedad y la aparición de infecciones oportunistas. Aunque la infección por VIH no puede propiamente curarse, sí puede convertirse con el uso continuado de esos fármacos en una enfermedad crónica compatible con una vida larga y casi normal. La [[enzima]] del VIH, la retrotranscriptasa, es una enzima que convierte el ARN a ADN, por lo que se ha convertido en una de las principales dianas en los tratamientos antirretrovirales.

La droga llamada AZT fue lanzada en 1987 mediante una procedimiento abreviado, en respuesta a la urgencia a la crisis del sida.

En el año 2007 la [[Agencia Europea de Medicamentos]] (EMEA por sus siglas en inglés) autoriza el fármaco [[Atripla]] que combina tres de los antirretrovirales más usuales en una única pastilla. Los principios activos son el [[efavirenz]], la [[emtricitabina]] y el [[disoproxilo de tenofovir]]. El medicamento está indicado para el tratamiento del virus-1 en adultos.
El común denominador de los tratamientos aplicados en la actualidad es la combinación de distintas drogas antiretrovilares, comúnmente llamada "cóctel". Estos "cócteles" reemplazaron a las terapias tradicionales de una sola droga que solo se mantienen en el caso de las embarazadas VIH positivas. Las diferentes drogas tienden a impedir la multiplicación del virus y, hacen más lento el proceso de deterioro del sistema inmunitario. El "cóctel" se compone de dos drogas inhibidoras de la transcriptasa inversa (las drogas) AZT, DDI, DDC, 3TC y D4T) y un inhibidor de otras enzimas las proteasas.

Al inhibir diferentes enzimas, las drogas intervienen en diferentes momentos del proceso de multiplicación del virus, impidiendo que dicho proceso llegue a término.
La ventaja de la combinación reside, justamente, en que no se ataca al virus en un solo lugar, sino que se le dan "simultáneos y diferentes golpes". Los inhibidores de la transcriptasa inversa introducen una información genética equivocada" o "incompleta" que hace imposible la multiplicación del virus y determina su muerte. Los inhibidores de las proteasas actúan en las células ya infectadas impidiendo el «ensamblaje» de las proteínas necesarias para la formación de nuevas partículas virales.

En 2010 se comprobó la eliminación del virus de un paciente con leucemia al recibir un trasplante de médula de un donante con una muy rara mutación genética que lo vuelve inmune a una infección con VIH; se recuperó de ambas enfermedades. Siendo una mutación muy rara y una operación con altos riesgos, la posibilidad de que esto se vuelva una solución práctica es casi inexistente de momento. A pesar de los resultados, las operaciones de este tipo exigen dosis de [[inmunosupresor]]es para toda la vida. El defecto genético en cuestión hace que las células T no expresen el receptor [[CCR5]] o CXCR4 que el virus necesita reconocer para entrar a la célula.

En las personas con enteropatía por el VIH, se ha documentado que la [[dieta sin gluten]] produce la mejoría de la diarrea y permite la recuperación de peso. De hecho, las lesiones intestinales halladas en estos casos son similares a las que provoca la [[celiaquía|enfermedad celíaca]].

Según un trabajo elaborado en el año 2007 por científicos de las universidades de [[Ulm]] y [[Hannover]], en conjunto con científicos españoles, se ha descubierto una proteína en el [[semen]] humano, que facilita la transmisión del virus [[VIH]].

Con frecuencia la cantidad de virus existente en el semen no alcanza los niveles mínimos esperables para que pueda generarse contagio. Sin embargo, esta [[proteína]] llamada SEVI, desempeña un rol de facilitador para la propagación de la infección, con concentraciones de VIH en semen que de otro modo jamás hubieran producido contagio.

Esta proteína se manifiesta en dos formatos o arquitecturas diferentes. Es la SEVI de estructura amiloidea, la que cuenta con capacidad de convertirse en patógena o mutar sus propiedades biológicas. Esta proteína favorece considerablemente el contagio por semen, facilitando la infección y distribución del virus.

El SEVI actúa concentrando el virus en la superficie de la [[célula]], que luego va a ingresar en forma masiva hacia el [[citoplasma]].



[[Categoría:Sida]]
[[Categoría:Enfermedades de transmisión sexual]]
[[Categoría:Inmunodeficiencias]]

</doc>
<doc id="8182" url="https://es.wikipedia.org/wiki?curid=8182" title="Absceso">
Absceso

Un absceso es una infección e inflamación del tejido del organismo caracterizado por la hinchazón y la acumulación de pus. Puede ser externo y visible, sobre la piel, o bien interno. Cuando se encuentra supurado se denomina apostema.

Los abscesos aparecen cuando se infecta un área de tejido y el cuerpo es capaz de "aislar" la infección y evitar que se extienda. Los glóbulos blancos, que son la defensa del organismo contra algunos tipos de infección, migran a través de las paredes de los vasos sanguíneos al área de la infección y se acumulan dentro del tejido dañado. Durante este proceso, se forma el pus, que es una acumulación de líquidos, glóbulos blancos vivos y muertos (principalmente Polimorfonucleados Neutrófilos, PMNN), tejido muerto y bacterias o cualquier otro material o invasor extraño.

Los abscesos pueden formarse en casi cualquier parte del organismo y pueden ser causados por organismos infecciosos, parásitos y materiales extraños. Los abscesos en la piel son fácilmente visibles, de color rojo, elevados y dolorosos; mientras que los abscesos que se forman en otras áreas del cuerpo pueden no ser tan obvios, pero pueden causar mucho daño si comprometen órganos vitales.

Por lo general un absceso se manifiesta con un dolor intenso, continuo y punzante en la zona, la cual se halla generalmente enrojecida, hinchada y caliente al tacto. Puede además cursar con fiebre.

Con frecuencia se obtiene una muestra de líquido del absceso y se le hace un cultivo para determinar los organismos causantes del mismo. Ver los tipos individuales de abscesos.

Se debe buscar asistencia médica si la persona cree tener algún tipo de absceso. No se deben tomar antibióticos si no han sido prescritos por un médico.

Generalmente, el médico, si es necesario, drena el absceso mediante técnicas quirúrgicas y luego administrará un antibiótico contra la infección.

En algunos casos se administra una profilaxis con toxoide tetánico, especialmente si la causa es exógena.

La mayoría de los tipos de abscesos son tratables.

La prevención de los abscesos depende de su localización y causa. Por ejemplo, una buena higiene es importante para la prevención de los abscesos cutáneos y con la higiene dental y los cuidados de rutina se previenen los abscesos dentales.

Absceso cerebral

Celulitis (inflamación)

Empiema


</doc>
<doc id="8183" url="https://es.wikipedia.org/wiki?curid=8183" title="Arteria">
Arteria

Una arteria es cada uno de los vasos que llevan la sangre con oxígeno desde el corazón hacia los capilares del cuerpo . Nacen de un ventrículo y sus paredes son muy resistentes y elásticas para resistir la presión que ejerce la sangre al salir bombeada del corazón.
Etimología: el término "arteria" proviene del griego "ἀρτηρία", «tubo, conducción (que enlaza)».

El aparato circulatorio, compuesto por arterias y venas, es fundamental para mantener la vida. Su función es la entrega de oxígeno y nutrientes a todas las células, así como la retirada del dióxido de carbono y los productos de desecho, el mantenimiento del pH fisiológico, y la movilidad de los elementos, las proteínas y las células del sistema inmunitario. En los países desarrollados, las dos causas principales de fallecimiento, el infarto de miocardio y el derrame cerebral, son ambos el resultado directo del deterioro lento y progresivo del sistema arterial, un proceso que puede durar muchos años (ver aterosclerosis).

Las arterias son conductos membranosos, elásticos, con ramificaciones divergentes, encargados de distribuir por todo el organismo la sangre expulsada de las cavidades ventriculares del corazón en cada sístole.

Cada vaso arterial consta de tres capas concéntricas:

Los límites entre las tres capas están generalmente bien definidos en las arterias. Las arterias presentan siempre una lámina elástica interna separando la íntima de la media, y (a excepción de las arteriolas) presentan una lámina elástica externa que separa la media de la adventicia. La lámina elástica externa se continúa a menudo con las fibras elásticas de la adventicia.

En la circulación general o sistémica, la sangre que sale impulsada del corazón pasa a través de un sistema de vasos arteriales de diámetro cada vez más reducido, hasta llegar a los tejidos, para volver después al corazón a través del sistema venoso. En esquema, el ciclo se puede resumir como sigue:

Además de en el diámetro, los distintos vasos presentan diferencias en la composición de las tres capas.

Constituyen las arterias pequeñas y medianas del organismo. La media forma una capa compacta, esencialmente muscular, con una fina red de láminas elásticas. No hay lámina externa elástica. Ejemplo: las arterias coronarias.

Son las arterias más pequeñas y contribuyen de manera fundamental a la regulación de la presión sanguínea, mediante la contracción variable del músculo liso de sus paredes, y a la regulación del aporte sanguíneo a los capilares.

De hecho, la regulación principal del flujo sanguíneo global y de la presión sanguínea general se produce mediante la regulación colectiva de las arteriolas: son los principales tubos ajustables en el sistema sanguíneo, donde tiene lugar la mayor caída de presión. La combinación del gasto cardíaco y la resistencia vascular sistémica, que se refiere a la resistencia colectiva de todas las arteriolas del organismo, son los principales determinantes de la presión arterial en un momento dado.

Los capilares son las regiones del sistema circulatorio donde tiene lugar el intercambio de sustancias con los tejidos adyacentes: gases, nutrientes o materiales de desecho. Para favorecer el intercambio, los capilares presentan una única célula endotelial que los separa de los tejidos. Además, los capilares no están rodeados por músculo liso. El diámetro de un capilar es menor que el diámetro de un glóbulo rojo (que normalmente mide 7 micrómetros de diámetro exterior), por lo que a su paso por los capilares, los glóbulos rojos deben deformarse para poder atravesarlos. El pequeño diámetro de los capilares proporciona una gran superficie para favorecer el intercambio de sustancias.

En los distintos órganos, los capilares realizan funciones similares, pero se especializan en una u otra:

El sistema arterial es la porción del sistema circulatorio que posee la presión más elevada. La presión arterial varía entre el pico producido durante la contracción cardíaca, lo que se denomina presión sistólica, y un mínimo, o presión diastólica entre dos contracciones, cuando el corazón se expande y se llena. Esta variación de la presión en las arterias produce el pulso, que puede observarse en cualquier arteria, y que refleja la actividad cardíaca. Las arterias, debido a sus propiedades elásticas, también ayudan al corazón a bombear sangre, generalmente oxigenada, hacia los tejidos periféricos.
Entre los griegos clásicos, las arterias se consideraban como "tubos huecos" responsables del transporte de aire a los tejidos, conectadas a la tráquea. Esta interpretación se debe a que, en los organismos muertos, las arterias se encuentran vacías, porque toda la sangre pasa al sistema venoso.

En la edad media, se consideraba que las arterias transportaban un fluido, denominado "sangre espiritual" o "espíritu vital", diferente del contenido de las venas. Esta teoría se remonta hasta Galeno. En el periodo medieval tardío, la tráquea, y los ligamentos también se denominaban "arterias".

William Harvey describió y popularizó el concepto moderno del sistema circulatorio y las funciones de arterias y venas en el siglo XVII. Aunque el español Miguel Servet describió la circulación pulmonar un cuarto de siglo antes que Harvey naciera, lo escribió en un libro de Teología (Christianismi Restitutio, publicado en 1553), que fue considerado como herejía y le condujo a la hoguera por parte de la iglesia protestante. En consecuencia, casi todas las copias del mismo fueron quemadas excepto tres, que fueron descubiertas décadas más tarde.

Alexis Carrel a principios del siglo XX fue el primero en describir la técnica de sutura de vasos y anastomosis, y realizó con éxito muchos trasplantes de órganos en animales, abriendo así la vía a la moderna cirugía vascular.



</doc>
<doc id="8185" url="https://es.wikipedia.org/wiki?curid=8185" title="Asociación Internacional de Médicos para la Prevención de la Guerra Nuclear">
Asociación Internacional de Médicos para la Prevención de la Guerra Nuclear

La Asociación Internacional de Médicos para la Prevención de la Guerra Nuclear, AIMPGN (en inglés International Physicians for the Prevention of Nuclear War, IPPNW) es una organización creada por médicos y organizaciones médicas soviéticos y estadounidenses en los tiempos de la guerra fría para la prevención de la guerra nuclear y desarme de las armas nucleares. La AIMPGN, que mereció el en 1985 y ahora está presente en más de 60 países, fue fundada en 1980 por los doctores Bernard Lown (Estados Unidos) y Evgueni Chazov (Unión Soviética).

Los programas y campañas prioritarios de la AIMPGN son los siguientes:

La AIMPGN se define como una federación independiente de organizaciones médicas nacionales de 58 países, representando a decenas de miles de médicos, estudiantes de medicina, trabajadores de la salud y ciudadanos comprometidos con la meta común de crear un mundo más seguro y pacífico libre de la amenaza de la aniquilación nuclear.



</doc>
<doc id="8188" url="https://es.wikipedia.org/wiki?curid=8188" title="Acondicionamiento de aire">
Acondicionamiento de aire

El acondicionamiento de aire, según la normativa española, es el tratamiento del aire que modifica las condiciones de cierto lugar para adecuarlas a unas necesidades determinadas. Por ejemplo, en el caso de enfriar una habitación, el acondicionamiento de aire -mediante el uso del "aire acondicionado" (A/C)- realiza el proceso de eliminar el calor y la humedad del interior de un espacio para mejorar la comodidad de sus ocupantes. La primera unidad de acondicionador de aire eléctrica moderna fue inventada por Willis Carrier en 1902. en Buffalo (New York).

El acondicionamiento del aire se realiza mediante Unidades de Tratamiento de Aire (UTA), que son aparatos modulares en los que en cada módulo se realiza un tratamiento y se agrupan en función de las condiciones finales de aire requeridas. El tratamiento de aire más completo, es la climatización, en la que se necesitan la mayor parte de los módulos existentes, para garantizar las condiciones del bienestar térmico de las personas. Es, probablemente, por esta razón, por lo que las UTAs se conocen normalmente como climatizadores. Los módulos de calor y frío, funcionan con baterías de agua caliente y fría respectivamente, que obtienen de generadores independientes; la producción de agua caliente suele confiarse a calderas y la de agua fría a máquinas frigoríficas llamadas enfriadoras. 

La ciencia que estudia las propiedades de la mezcla aire-vapor de agua y establece las relaciones entre ellas para su cálculo y tratamiento, se llama psicrometría. Las fórmulas establecidas por la misma, facilitan también la construcción de diagramas de aire húmedo que facilitan el cálculo y proporcionan un resultado visual de la transformación. 

La popularidad del uso doméstico del aire acondicionado en los hogares ha aumentado considerablemente en el siglo XX, que inclusive en Estados Unidos cerca del 90% de los hogares cuentan con aire acondicionado; no así en Europa donde no es tan común debido al clima predominantemente fresco. Sin embargo, aparte del uso doméstico o particular (en oficinas o salones de eventos, compras, etc.) hay multitud de actividades que requieren unas condiciones de aire específicas o determinadas como: laboratorios de metrología y calibración, salas de ordenadores, salas de exposiciones, quirófanos y salas de vigilancia intensiva (UVI), salas blancas en general, fabricación de dulces, fabricación de textiles, etc. Un sinfín de procesos industriales que precisan unas condiciones ambientales fijas, que pueden ser muy diferentes de las condiciones de confort, pero determinantes para la manipulación o la calidad del producto final.

El módulo situado en cabeza de cualquier UTA, es siempre un ventilador que mueve un caudal másico de aire formula_1 tomado del ambiente a tratar, lo hace pasar por todos los módulos instalados en su aspiración y lo impulsa, ya tratado, de nuevo al ambiente.

En aquellas instalaciones en las que existe una amplia red de retorno o en aquellas en las que existe enfriamiento gratuito (free-cooling) del aire, se instalan dos ventiladores; uno en la impulsión y otro en el retorno, que suelen ser del mismo caudal y con una presión disponible correspondiente a la pérdida de carga de la parte de red de distribución a la que abastecen.

La función de filtrado se cumple en el "módulo de filtración" y en etapas de filtración instaladas en puntos clave de la distribución. Consiste en tratar el aire mediante filtros adecuados a fin de quitarle polvo, impurezas y partículas en suspensión. El grado de filtrado necesario dependerá del tipo de instalación de acondicionamientos a efectuar.
Para la limpieza del aire se emplean filtros que normalmente son del tipo mecánico, compuestos por sustancias porosas por las que se obliga a pasar al aire y en las que deja las partículas que lleva en suspensión.
En las instalaciones comunes de confort se usan filtros de poliuretano, lana de vidrio, microfibras sintéticas o metálicas de alambre con tejido de distinta malla de acero o aluminio embebidos en aceite. 
El filtro es el primer elemento, y muy comúnmente, también el último a instalar en la circulación del aire, porque no solo protege a los locales acondicionados, sino también al mismo equipo de acondicionamiento.

En el calentamiento sensible, el aire pasa a través del módulo de calefacción, que consiste en una batería por la que circula agua generalmente procedente de una caldera. En el paso, el aire aumenta su temperatura de formula_2 a formula_3 y su entalpía sin modificar la humedad específica, de tal forma que a la entrada:
Y a la salida:

Restando miembro a miembro:

es la energía térmica recibida por cada kg de aire, para pasar de formula_8 a formula_9 . La humedad máxima correspondiente a formula_10 habrá aumentado, por lo que su humedad relativa habrá disminuido.
La potencia de la batería de calor será el producto de la energía recibida por kg, multiplicada por el caudal másicoformula_11.

En los sistemas con expansión directa, la batería de agua caliente se sustituye por el condensador de una máquina frigorífica o bomba de calor, de forma que es la condensación del refrigerante la que aporta el calor necesario, por intercambio directo con el aire.

En el "calentamiento sensible", la disminución de la humedad relativa unida a la recirculación continua de un caudal formula_13, sin ningún tipo de renovación, hacen que al cabo de no mucho tiempo, el aire se deteriore, y sobre todo en el acondicionamiento de confort (climatización), que el usuario sienta la sequedad y el enrarecimiento del aire.
Debido a esto, las instalaciones solo calefacción no son las más aconsejables. Una mejora del sistema es incorporar ventilación, es decir, una entrada de una cantidad de aire exterior que diluya la concentración de contaminantes en el aire.

Si se añade una cantidad de aire exterior, en algún punto se produce una mezcla de dos flujos de aire con diferentes condiciones. El aire nuevo del edificio o aire de ventilación penetra a través de una reja de toma de aire, ubicada en el exterior, en un recinto llamado "módulo de mezcla", en él se mezcla el aire nuevo con el aire de retorno de los locales, regulándose sus caudales respectivos mediante "persianas" de accionamiento manual o automático.

Efectuando un balance de materia y de energía se obtiene:
Por conservación de la masa:

Por conservación de cantidad de vapor o balance de agua:
Donde formula_16 es la razón de humedad (kg vapor de agua/kg) de las corrientes.

Por conservación de energía:
Dondeformula_18 es la entalpía en (J/kg) de las corrientes.

De estas tres ecuaciones se obtiene:
También se utiliza con una aproximación bastante buena:

En el enfriamiento sensible, el aire pasa a través del módulo de refrigeración, que consiste en una batería por la que circula agua procedente de una enfriadora. En el paso, el aire disminuye su temperatura de formula_21 a formula_22 y su entalpía disminuye sin modificar la humedad específica. Para que ocurra el proceso de esta forma, es necesario que la temperatura de la batería, o del agua que circula por ella, esté por encima de la temperatura de rocío correspondiente al estado 1. De no ser así, habrá condensación y por tanto disminución de la humedad específica.
Por lo demás todo es al contrario que en el calentamiento sensible. La capacidad de absorción de humedad disminuye y por tanto aumenta la humedad relativa.

formula_24 será negativo , ya que formula_25. El signo no tiene más significado que la indicación del sentido del flujo de la energía térmica, que en este caso es extraída en lugar de aportada.

Es la transformación en la que aumenta el contenido de vapor del aire húmedo sin modificar su temperatura seca.
Este proceso se consigue con la aportación de una pequeña cantidad de vapor de agua al aire. La cantidad de vapor añadido por cada kg de aire seco será:
y se suele realizar por un generador de vapor.
Cuando se añade vapor a temperatura igual o superior a 100ºC, el proceso resultante determina una humidificación y ligero calentamiento del aire. En todo caso, la variación de temperatura seca es muy pequeña por lo que en la práctica se considera como un proceso isotérmico.
En este proceso hay una mezcla de dos masas, la del aire formula_27 y la del vapor de agua formula_28, con lo que la suma de ellos será:
Haciendo un balance de cantidad de agua se tiene que:
Sustituyendo : 
Por tanto:
De una manera menos precisa se puede calcular con caudales volumétricos, es decir:
Es la transformación inversa a la anterior y se puede realizar con algún tipo de producto desecante o absorbente que elimine la humedad del aire sin variar su temperatura, como gel de sílice.

Cuando se hace pasar una corriente de aire húmedo por una superficie fría que se mantiene a una temperatura inferior a la de rocío del mismo, condensará una parte del agua que contiene y por tanto disminuirá su humedad. En condiciones ideales, el aire abandonaría el sistema con una temperatura seca igual a la temperatura de la batería y con una humedad relativa del 100%.
La temperatura de rocío de la batería, formula_34, es la temperatura del aire tratado a la salida de la misma y coincide muy aproximadamente con la temperatura del fluido frío que circula por el interior de los tubos, debido a la pequeña resistencia térmica de la pared de los mismos.
De la entalpía que pierde el aire, una parte será calor sensible (disminución de la temperatura seca) y otra parte será calor latente (disminución de vapor de agua).
El calor total eliminado en la batería es el extraído del aire, más el calor que lleva el agua condensada. Este segundo término se desprecia porque suele ser muy pequeño:

Un proceso es adiabático cuando se realiza sin intercambio de calor con el medio exterior. Ocurre cuando un flujo de aire atraviesa una cortina de agua. El agua, impulsada por una bomba, se pulveriza en el aire y vaporiza parte de ella, aumentando la humedad específica del aire. Si la cámara fuera lo suficientemente larga el aire saldría saturado. La temperatura del agua deberá ser igual a la temperatura húmeda del aire.
El aire evoluciona manteniéndose constante su temperatura húmeda. Esto implica que el aire sale prácticamente con la misma entalpía que entró, aunque con un mayor contenido de humedad específica. El aire se enfría porque cede el calor necesario para la vaporización del agua que se incorpora al aire como calor latente. 

En la práctica las cámaras nunca son tan largas como para que el aire salga completamente saturado, por lo que la humidificación terminará en algún punto antes de llegar a la saturación. Considerando que el aire entra en la cámara en la condición 1 y sale en la 2, la eficiencia de la humidificación será:

siendo formula_37 la temperatura húmeda del aire e igual a la temperatura del agua y formula_38 y formula_39, las condiciones en la saturación.
Esta ecuación proporciona las condiciones de salida del aire a su paso a través de una cortina de agua que se encuentra a una temperatura formula_40 y tiene una eficiencia formula_41.
La humidificación de una corriente de aire mediante agua líquida pulverizada se consigue con los módulos llamados lavadores. También se consigue aunque con menos eficiencia, mediante un módulo con un panel adiabático.

En las zonas o espacios que requieren ambiente controlado, es indispensable un buen diseño y funcionamiento del sistema de tratamiento de aire. Temperatura, presión, humedad, limpieza y calidad de aire, así como su distribución y velocidad en el ambiente tratado, son parámetros que deben ser controlados para alcanzar y mantener las condiciones especificadas.

Las zonas de ambiente controlado pueden tener usos diversos y requerimientos muy especiales: Zonas limpias, zonas estériles, zonas de seguridad biológica, zonas antideflagrantes, etc... El sistema debe cumplir la normativa especificada para cada uso, sin perjuicio de las necesidades y características requeridas por los tratamientos de cada instalación. El control de las presiones diferenciales y del escalado de las mismas, creando sobrepresiones o depresiones en distintas zonas, permite reducir la introducción o retención de cualquier tipo de contaminación: microbiológica, por partículas de polvo, cruzada entre productos, o cualquier otra contaminación externa, incluida la que pueden producir los propios operarios. Por otra parte, los sistemas de distribución y de extracción de aire deben estar diseñados para conseguir un barrido máximo del ambiente, minimizando la retención de partículas en suspensión.
Cada vez más, el consumo energético de la instalación es otro de los factores relevantes a considerar, no solo desde el punto de vista económico, sino también de la eficiencia energética.

Los equipamientos propios de estas instalaciones son:


Todo este equipamiento lleva asociado un sistema de control que permite gestionar y visualizar el estado de las variables que son determinantes para la funcionalidad del proceso. Este sistema de control gestiona los ciclos de funcionamiento de los procesos, registrando o visualizando los valores de cada variable.
De esta manera, se obtiene el control directo de cada uno de los parámetros de la instalación, proporcionando en tiempo real la información de lo que está pasando, pudiéndose tomar decisiones sobre cada uno de ellos, tales como; selección de las condiciones interiores, fijación de consignas o parámetros de funcionamiento, temporizaciones, etc.
Adicionalmente a la optimización del proceso, es conveniente adoptar un "sistema de gestión integral" que posibilite la operación y regulación en toda la instalación del consumo energético, así como una disminución de los costos de mantenimiento.

Para conocer la capacidad del aire acondicionado que se debe comprar para determinado lugar se deben tener en cuenta varios factores , ellos son:

a) Número de personas que habitarán el recinto.

b) Potencia de los aparatos que se encuentran en el lugar que disipen calor (computadores, televisores, electrodomésticos en general). Toda la potencia se liberará como calor.

c) Ventilación (posibles fugas de aire que puedan haber como ventanas, puertas, etc.)

d) Volumen del lugar en metros cúbicos (m³) Largo X Ancho X Alto.
Para realizar el cálculo de capacidad se debe tener en cuenta lo siguiente:
1kW = 860 kcal/h

12.000 BTU/h = 1 TON. DE REFRIGERACION

1 kcal = 3,967 BTU

1 BTU = 0,252 kcal

1kcal/h = 3,967 BTU/h

1HP = 642 kcal/h
CÁLCULO DE CAPACIDAD

C = 230 x V + (#PyE x 476)
DONDE:
a) 230 = Factor calculado para América Latina "Temp máxima de 40 °C" (dado en BTU/hm³)

b) V = Volumen del ÁREA donde se instalará el equipo, Largo x Alto x Ancho en metros cúbicos m³

c) #PyE = # de personas + Electrodomésticos instalados en el área.

d) 476 = Factores de ganancia y pérdida aportados por cada persona y/o electrodoméstico (en BTU/h)





</doc>
<doc id="8192" url="https://es.wikipedia.org/wiki?curid=8192" title="COP">
COP

COP, Cop o CoP puede referirse a:






</doc>
<doc id="8196" url="https://es.wikipedia.org/wiki?curid=8196" title="Ludopatía">
Ludopatía

La ludopatía es un trastorno en el que la persona se ve obligada, por una urgencia psicológicamente
incontrolable, a jugar y apostar, de forma persistente y progresiva, afectando de forma negativa a la vida personal, familiar y vocacional. Aunque en anteriores ediciones del manual diagnóstico DSM había sido clasificado como un trastorno del control de los impulsos, ha sido conceptualizado y tratado como una adicción sin sustancia, hasta que en el DSM-V ha sido incluido finalmente dentro de la categoría de "Trastornos relacionados con sustancias y trastornos adictivos".

El juego patológico se clasifica en el DSM-IV-R en trastornos del control de los impulsos, que también incluyen la cleptomanía, piromanía y tricotilomanía, en los que estaría implicada la impulsividad, pero no presenta comorbilidad con dichos trastornos. Si bien el sistema DSM (III, III-R y IV1) y la CIE-10 incluye este trastorno entre las alteraciones debidas a un bajo control de los impulsos, lo cierto es que los criterios diagnósticos operativos DSM tienen exactamente el mismo diseño que el de las adicciones a sustancias, lo que muestra la concepción subyacente para la enfermedad en ese sistema: se trata de un problema adictivo "sin sustancia" incluido en un apartado diferente al suyo.
La ludopatía se caracteriza por la dificultad para controlar los impulsos. Tiende a manifestarse en la práctica compulsiva de uno o más juegos de azar. Puede afectar la vida diaria del adicto, de tal forma que la familia, el sexo o incluso la alimentación pasa a ser algo secundario. Por ello, no se le debe de confundir con un vicio, ya que la ludopatía es una grave enfermedad crónica, una adicción.

El juego patológico fue reconocido oficialmente como entidad nosológica de salud mental en el año 1980 cuando la Sociedad Americana de Psicología (APA) lo incluye por primera vez como trastorno en el Manual Diagnóstico y Estadístico de los Trastornos Mentales, en su tercera edición (DSM-III).


Según Becoña, las fases de la adicción son tres:

De acuerdo con el Illinois Institute for Addiction Recovery, las últimas evidencias indican que el juego patológico es una adicción similar a las químicas. Se ha visto que algunos jugadores patológicos tienen menores niveles de norepinefrina que los jugadores normales.

De acuerdo con un estudio dirigido por Alec Roy, M.D., antiguo miembro del National Institute on Alcohol Abuse and Alcoholism, la norepinefrina se secreta en condiciones de estrés o amenaza, de modo que los jugadores patológicos juegan para elevar sus niveles.

Abundando en esto, de acuerdo con un informe de la Harvard Medical School Division on Addictions se generó un experimento en el que a los sujetos se les presentaban situaciones en las que podían ganar o perder en un entorno que simulaba un casino. Las reacciones de los sujetos se medían utilizando RMNf, una técnica de neuroimagen muy similar a la Resonancia magnética nuclear. Y de acuerdo con el doctor Hans Breiter, codirector del Centro de neurociencia de la motivación y la emoción del Hospital General de Massachusetts, las "recompensas en metálico en un ambiente que reproduce un ambiente de juego produce una activación cerebral muy similar a la que se observa en un adicto a la cocaína recibiendo una dosis."

Las deficiencias de serotonina también pueden contribuir a una conducta compulsiva, lo cual incluye una adicción al juego.

A medida que se acumulan las deudas, los afectados pueden recurrir a "soluciones" desesperadas para conseguir dinero con el fin de recuperarlo a través del juego, como pequeños hurtos, o pedir nuevos créditos para tapar las deudas más difíciles de ocultar. La existencia del hecho delictivo depende de las circunstancias facilitadoras del medio para cometerlo y de la personalidad base del afectado.

Como consecuencia de la enfermedad, el afectado puede tener depresión, ansiedad, ataques cardíacos (consecuencia del estrés), puede tener ideaciones suicidas por desesperación si no recibe tratamiento.

Por otro lado un número considerable de afectados tiene trastorno por déficit de atención con hiperactividad (TDAH).

También se sabe que algunos antiparkinsonianos pueden provocar ludopatía.

En un estudio de 1991 sobre relaciones en varones estadounidenses se encontró que el 10% de los jugadores compulsivos se habían casado tres veces o más. Sólo el 2% de los no jugadores se habían casado más de dos veces.
Un estudio de la Comisión para el juego del Reino Unido, el "British Gambling Prevalence Survey 2007", concluyó que aproximadamente el 0.6% de la población adulta tenía problemas con el juego, el mismo porcentaje que en 1999. La mayor prevalencia de la ludopatía se encontró entre los participantes en apuestas por diferencias (14.7%), Terminales de apuestas fijas e intercambio de apuestas (11.2%)

En el metaanálisis de Shaffer y Hall en 1996 sobre la prevalencia del juego patológico entre adolescentes (de 13 a 20 años) la media estimada para el juego patológico o para graves problemas con el juego oscilaba entre el 4.4% y el 7.4%.

El conocimiento científico disponible parece indicar que la ludopatía es una tendencia interna y que los ludópatas tienden a arriesgar dinero en cualquier juego disponible, más que en uno en particular, generando ludopatía en otros individuos que, de otro modo, serían "normales". No obstante, las investigaciones también indican que los ludópatas en juegos de desarrollo rápido. Por ello es mucho más probable que pierdan dinero en la ruleta o en una máquina tragaperras, en el que los ciclos terminan rápido y existe una constante tentación de jugar una y otra vez o aumentar las apuestas, en oposición a las loterías nacionales, en las que el jugador debe esperar hasta el próximo sorteo para ver los resultados.

Henry Lesieur, un psicólogo del programa de tramiento para jugadores del Hospital de Rhode Island afirma que el 30 por ciento de los beneficios de las máquinas de juego proceden de ludópatas.

Se ha implicado a los agonistas de la dopamina, en particular el pramipexol (Mirapex) en el desarrollo del juego compulsivo y de otros patrones de conducta con excesos.

Para Isabel Sánchez Sosa, coordinadora de la Asociación de Jugadores Compulsivos de Argentina, "en el país la ludopatía está creciendo muchísimo porque la oferta es impresionante" y en ese sentido aseveró que la presencia de los bingos es una cuestión común en todos los barrios. En la provincia de Buenos Aires, se instalaron 46 bingos en los últimos quince años. A principios de los 90 no había ninguno. 

En Colombia, el psiquiatra Pablo Rodríguez afirma que la ludopatía puede destruir a una persona tanto como el alcoholismo y la drogadicción. Relató muchos casos en que personas perdieron hogares, automóviles, empresas, además de los familiares y amigos. "La ludopatía es una adicción sin fondo. Se puede caer más y más y más". A veces, en los casos extremos, se requiere que el paciente sea internado y medicado. En la mayoría de los casos funcionan los tratamientos ambulatorios, las consultas semanales, las reuniones con Jugadores Anónimos, el afecto pero también la firmeza: la familia no debe pagarle las deudas a un ludópata."

En Chile, Estudio de la U. de Santiago y la Corporación de Juego Responsable dice que el 2,4 % de los jugadores en el país son patológicos y que el 80 % de los ludópatas en Chile son mujeres. A su vez, el 35,2% de este tipo de jugadores se encuentra en el estrato etario que oscila entre 31 a 40 años. Daniel Martínez, psiquiatra y director de Buenas Prácticas de Juego Responsable de la CJR, aclara que los hombres prefieren los juegos de azar y casinos, relacionado al deseo de competencia. En las mujeres, en cambio, se da una mayor atracción a las máquinas tragamonedas, lo que aumenta el porcentaje de mujeres con patologías. "Ellas juegan como una forma de desconectarse de sus emociones. Además, muchas han vivido en función de los hijos y crianza, y cuando los hijos se van se quedan sin actividades y encuentran en el juego una forma de disfrutar y entretenerse. Es una característica del juego el desconectarse rápidamente". 

En España, más de medio millón de personas padecen de ludopatía. Desde que se aprobara la Ley de Regulación del Juego en 2011, el número de casinos y casas de apuestas –online especialmente- ha aumentado exponencialmente. Este se ha producido debido al incremento de usuarios, pero también al bajo coste que supone mantener un casino o una casa de apuestas "online", si lo comparamos con uno real. Además, el problema de las máquinas "tragaperras" parece haber pasado a un segundo plano y el juego "online" le ha ganado terreno siendo actualmente la segunda causa más frecuente de ludopatía.

Lizbeth García Quevedo, directora de la Coordinación con Entidades Federativas (CONADIC), habló de la ludopatía como una fuerte adicción en México: "Tiene conductas muy similares, por eso algunos expertos lo consideran una adicción porque se parece en las conductas, en los orígenes, algunos factores de riesgo que pueden disparar juego patológico, también pueden disparar consumo de drogas". En México podría haber entre uno y tres millones de personas adictas al juego. "Que estén al pendiente de que están haciendo sus hijos, y que por el otro lado motiven el juego pro activo, el juego sano", comentó Lizbeth García Quevedo. El documento de la Secretaría de Salud destaca que un estudio sobre juego patológico que analizó 46 estudios realizados en Canadá, Estados Unidos, Australia, Suecia, Noruega, Inglaterra, Suiza y España, reveló que la prevalencia de ludopatía es relativamente más elevada entre adolescentes, lo cual traza la continuidad del problema considerando que muchos jugadores patológicos declaran que comenzaron sus conductas de juego a temprana edad.

En Panamá, la adicción al juego ha crecido vertiginosamente en los últimos años, llegando a convertirse en un problema grave que afecta a cada vez más personas de numerosos sectores sociales, especialmente a los más pobres y a los adultos mayores. El doctor Carlos Smith, especialista en adicciones y coordinador del Centro de Estudios y Tratamiento de Adicciones, señaló que en el principio el azar puede ser recreacional y placentero, para posteriormente convertirse en una actividad que amenaza la integridad del sujeto que la práctica, y de su entorno socio-familiar.

Según EsSalud, el número de enfermos sube un 33% cada año. 5% de los personas en Perú, están afectados. Aunque en Perú no existen datos certeros para graficar la dimensión del problema, todo indica que tiende a crecer. El Instituto Nacional de Salud Mental (INSM) calcula el 5% de la población de Lima Metropolitana tiene complicaciones asociadas a la ludopatía.

Según cifras del Seguro Social de Salud (EsSalud), la ludopatía en Perú crece a un ritmo de 33% cada año. Sin embargo, los números del INSM indican que el porcentaje puede ser mayor. En el primer semestre de este año, la entidad ha diagnosticado 72 nuevos casos de este trastorno.

En Uruguay, el jugador compulsivo promedio tiene entre 40 y 55 años, revela un estudio. Una encuesta efectuada por la Dirección de Casinos, entre quienes han solicitado ayuda por la ludopatía, señala que el 86% de la enfermedad en la actualidad es causada por las máquinas tragamonedas (o máquinas de azar), seguido de manera lejana en un 10% por la ruleta. Con 1% se encuentran la quiniela, tómbola, 5 de Oro y carreras de caballos, y con 2% los juegos de cartas. Otro dato de importancia que ha arrojado la investigación, es que el 65% de los pacientes ludópatas presentan antecedentes de haberse criado con algún familiar que presentaba problemas con el juego. 

En Venezuela, en los últimos años, ha incrementado el índice de ludopatía, mayormente viéndose afectados las personas jóvenes, en apuestas deportivas denominadas "parley" y carreras de caballo. El psiquiatra César Sánchez Bello, aseguró que ha notado un incremento notable en personas enfermas al juego que van a consultas psiquiátricas por la adicción, en su mayoría por apuestas deportivas, superando a las personas que solicitan atención por adicción a las apuestas y juegos en casinos o bingos. También resaltó la importancia de prevenir que más personas caigan en la ludopatía, lo cual no solo afecta al paciente, sino al núcleo familiar, laboral y amistades.

El instrumento más habitual para detectar una "probable conducta de juego patológico" es el South Oaks Gambling Screen (SOGS) desarrollado por Lesieur y Blume (1987) en el South Oaks Hospital de New York City. Este test es sin duda el instrumento más citado en la literatura científica psicológica. En estos últimos años el uso del SOGS ha decaído debido a las crecientes críticas, entre las que se encuentran las que afirman que sobreestima los falsos positivos.

Los criterios diagnósticos del DSM-IV son una alternativa al SOGS, y se centran en las motivaciones psicológicas subyacentes al problema del juego, y fueron desarrolladas por la American Psychiatric Association. Se compone de diez criterios diagnósticos. Una prueba basada en los criterios del DSM-IV criteria es el National Opinion Research Center DSM Screen for Gambling Problems (NODS). Esta medición es utilizada con bastante frecuencia. El Canadian Problem Gambling Severity Index (PGSI) es otro instrumento de evaluación PGSI se centra en los daños y consecuencias asociadas con la ludopatía.

Existen una gran variedad de tratamientos para el juego patológico que incluyen el consejo, los grupos de autoayuda y la medicación psiquiátrica. Sin embargo, no se considera que ninguno de estos tratamientos sea el más eficaz, y no se ha aprobado ninguna medicación por parte de la FDA para el tratamiento del juego patológico.

Jugadores anónimos es un tratamiento comúnmente utilizado para la ludopatía. Modelado con base en el tratamiento de Alcohólicos Anónimos, utiliza un modelo en 12 pasos que hace hincapié en un enfoque de ayuda mutua.

Se ha visto que un enfoque, la terapia cognitivo-conductual reduce los síntomas y las urgencias relacionadas con el juego. Este tipo de terapia se centra en la identificación de los procesos mentales relacionados con el juego, las distorsiones cognitivas y del ánimo que incrementan la vulnerabilidad al juego incontrolado. Además, esta terapia utilizan técnicas de adquisición de competencias orientadas a la prevención de las recaídas, asertividad y rechazo del juego, resolución de problemas y refuerzo de las actividades e intereses inconsistentes con el juego. 

Existen evidencias de que la paroxetina es eficiente en el tratamiento del juego patológico. Además, para pacientes que sufren la comorbididad del trastorno bipolar y el juego patológico, la administración continuada de litio se ha mostrado eficaz en ensayos preliminares. El fármaco antagonista de los opiáceos conocido como malmefeno también ha resultado exitoso en los ensayos para el tratamiento del juego compulsivo.

En la ficción, Dostoievski escribió la obra "El jugador", en parte autobiográfica. En el psicoanálisis, Sigmund Freud escribió un ensayo basado en esta obra.

En el arte, Michelangelo Merisi da Caravaggio en 1594 realizó su obra "Jugadores de cartas".

En la serie de televisión estadounidense "How I Met Your Mother" se comenta numerosas veces la ludopatía de uno de sus protagonistas principales, Barney Stinson.

En la serie de televisión estadounidense "Los Simpson" el personaje de Marge Simpson sufre de algunas enfermedades, entre ellas la más destacada es la ludopatía que se ve claramente en el capítulo Springfield (Or, How I Learned to Stop Worrying and Love Legalized Gambling) (temporada 5), cuando el Sr. Burns abre un casino en Springfield.

En la octava temporada de la serie de televisión española "Cuéntame cómo pasó", el personaje Antonio Alcántara queda atrapado en el juego del póker y, como consecuencia, llega a deber la cantidad aproximada de 300.000 pesetas de la época. Para zanjar la deuda, recurrirá a su hermano Miguel, quien le dará un dinero ganado por la venta de unas tierras del pueblo. Finalmente salda la deuda en el capítulo 139 pero, aun así, la relación con su mujer Mercedes queda afectada.

En el videojuego Left 4 Dead 2 uno de los personajes principales, Nick, es un ludópata que estaba detenido, arrestado por fraude.

En el programa infantil chileno "31 minutos", el personaje Juan Carlos Bodoque es ludópata debido a su obsesión a apostar en las carreras de caballos, sobre todo a su yegua Tormenta China la cual nunca ha ganado.



</doc>
<doc id="8197" url="https://es.wikipedia.org/wiki?curid=8197" title="Psicología médica">
Psicología médica

La psicología médica trabaja desde aspectos tan diversos como la genética y la robótica hasta los conocimientos que están en relación con factores medioambientales e influidos por aspectos económicos, políticos y socio-culturales. No es por lo tanto una sorpresa que esta disciplina, por ejemplo, estudia al individuo a nivel neural, endócrino e inmunológico por un lado y por otro las relaciones en los niveles personales, familiares y sociales, como también utilice la alta tecnología para hacer sus investigaciones, diagnósticos y tratamientos.

Tal como muchas disciplinas, la psicología médica también hace uso de la información de las ciencias sociales. Dentro de ellas, la antropología, la psicología social y la sociología aportan a la psicología médica valiosos datos sobre el funcionamiento de los grupos humanos (la familia, las sociedades, las culturas y sus interacciones con el individuo).

Actualmente, los profesionales de la salud están favorecidos con: (a) los avances del conocimiento cada vez más minuciosos de la estructura y el funcionamiento de las partes que integran el organismo humano; (b) los métodos para identificar las disfunciones de los órganos y determinar su patología; (c) de sus recursos para prevenir y combatir las enfermedades. Estos avances son tan amplios que tienen que aceptar sus limitaciones dentro de su área de servicio y aprovechar de los otros profesionales para formar un equipo que permita revisar al individuo de manera integral.

Hace más de 130 años Claude Bernard dijo: “no hay enfermedades sino enfermos”, sin embargo muchos profesionales de la salud todavía no han asimilado esta frase. Más bien siguen la antigua división teórica cartesiana donde el cuerpo y la mente son tratados como entidades separadas y erróneamente lo aplican en su trato diario con los enfermos en lugar de considerar a la persona como un todo.

La formación del profesional de la salud debe ser integral. Necesita tanto del conocimiento científico como el saber afrontar, comprender y relacionarse con las personas, de como los seres humanos se relacionan entre sí e integrarlo de acuerdo a su salud. El profesional de salud necesita estar alerta que de su actitud hacia las personas también dependerá la facilidad o la dificultad para establecer un diagnóstico preciso y la instauración del tratamiento apropiado de las personas que sirve.

En algún momento psicología médica fue llamada la "psicología para los médicos". Morales Meseguer al hablar de la psicología médica, circunscrita dentro de la práctica médica, suscitaba la cuestión si debía hablarse de una disciplina formal o simplemente de un sector aplicativo de conocimientos y recomendó reflexionar la doble imagen que despierta, aborda los problemas psicológicos que se emplea en la práctica médica, que cabe atribuir a un “saber psicológico” que los factores psíquicos participan en la determinación de las enfermedades humanas y desde luego con su tratamiento.

La psicología médica resulta una empresa atractiva, hay quienes la entienden como una consecuencia a la existencia de la práctica médica (Alonso Fernández, 1989) o como la psicología en la educación médica, en la investigación y en la práctica clínica (Kerejarto, 1978) o como la aplicación de los métodos y conceptos psicológicos a los problemas médicos (Rachman, 1977).

El concepto antiguo de esta disciplina es que trata de aplicar los conocimientos y experiencias de la psicología general a los problemas de la medicina, abarcando todos los aspectos psicológicos de la actividad profesional del médico, la relación médico-paciente y la actitud del individuo o grupo, frente a la enfermedad y otros factores como la biografía personal o familiar, expectativa de muerte, curación o situaciones y conflictos vitales.Tradicionalmente, la psicología médica tenía como meta el preparar al médico en los conocimientos psicológicos con el objeto de que pueda comprender mejor al enfermo. Desde esa perspectiva, la psicología médica tiene dos funciones: formativa e informativa. Formativa: cambios en la personalidad, cambios en las motivaciones, cambios en las actitudes. Informativa: en las teorías de la personalidad, la relación médico-paciente, la personalidad de los médicos, diagnósticos personales y comprensivos, procedimientos psicoterapéuticos.

La psicología médica integra los conocimientos de las ciencias médicas y psicológicas que luego son usados por el profesional en beneficio de la persona. Es distinta a la psiquiatría que estudia los trastornos mentales y el modo en que son prevenidos, diagnósticados y tratados.

El diagnóstico emitido por el profesional experto se basa en el estudio de los signos y síntomas, el rol de los factores biológicos, psicológicos, culturales y sociales que inician o facilitan, mantienen, modifican y/o eliminan una enfermedad, la relación profesional de la salud-paciente y éstos con su medio, el comportamiento del enfermo ante el diagnóstico y el tratamiento, los recursos psicológicos para el tratamiento de la enfermedad.
Actualmente, los psicólogos clínicos (psicólogos clínicos y de la salud, neuropsicólogos clínicos), han ampliado su campo de trabajo así como lo están haciendo otros profesionales de la Salud. De ese modo, actualmente gracias a los estudios post-doctorales en el área de Medicina y Farmacología los psicólogos licenciados pueden ampliar su área de acción a fin de proveer un servicio más completo a sus pacientes.

En inglés, "Medical Psychology" (Médico Psicólogo), se refiere al doctor en psicología clínica, neuropsicología o con otra especialidad en el área la salud que tiene una maestría post doctoral o post PhD y capacidad de prescribir. Sus estudios adicionales son en las áreas de medicina y farmacología. Por parte de psicólogos clínicos en EE. UU. y el Canadá, es el término que emplean estos para diferenciarse de aquellos psicólogos que no pueden prescribir. Muchos de los estados, provincias y territorios están siguiendo el proceso legal que permita a los psicólogos médicos prescribir.

Es importante anotar que Medical Psychology es una sub-especialización dentro de la Psicología Clínica y aceptada por el Association of State and Provincial Psychologists Board (ASPPB) como tal.



</doc>
<doc id="8200" url="https://es.wikipedia.org/wiki?curid=8200" title="Elie Wiesel">
Elie Wiesel

Eliezer Wiesel (en húngaro: Wiesel Lázár; Sighetu Marmației, 30 de septiembre de 1928-Nueva York, 2 de julio de 2016) fue un escritor de lengua yiddish y francesa, de nacionalidad estadounidense, superviviente de los campos de concentración nazis. Dedicó toda su vida a escribir y hablar sobre los horrores del Holocausto, con la firme intención de evitar que se repita en el mundo una barbarie similar. Fue galardonado con el Premio Nobel de la Paz en 1986.

A los 14 años fue detenido por los alemanes, al igual que los demás judíos de su pueblo, cuando el nazi Ferenc Szálasi tomó el poder por la fuerza derrocando al regente húngaro Miklós Horthy. Sobrevivió a los campos de concentración de Auschwitz y Buchenwald, siendo liberado por las fuerzas aliadas el 11 de abril de 1945. Su padre Shlomo, su madre Sarah y su hermana menor Judith "«Tzipora»" perecieron; sin embargo, sus dos hermanas mayores Hilda y Beatrice lograron permanecer con vida. Estudió en la universidad de la Sorbona, en París, y posteriormente trabajó en periódicos de Israel, Francia y Estados Unidos, donde se estableció en 1956 y en 1963 obtuvo la nacionalidad estadounidense luego de haber sido apátrida durante décadas.

Autor de tres novelas sobre sus vivencias durante aquellos años de represión y muerte ("La noche", "El alba" y "El día", publicadas en español bajo el título de "Trilogía de la noche"), ganó el en 1986.

El 16 de mayo de 1944, la familia Wiesel, como otras tantas familias judías, se embarcó en un tren rumbo al campo de exterminio de Birkenau. "Es la primera parada, luego vienen Auschwitz y Buchenwald. Es noche cerrada, tinieblas exteriores a las que son arrojadas, junto a tantos judíos asesinados o supervivientes, nuestras entrañas de humanidad, nuestro manantial de profunda compasión." "La noche" (1956-1958) fue el título que más fama le dio a Wiesel.

Elie Wiesel murió el 2 de julio de 2016 a los 87 años, en Manhattan, Nueva York.




</doc>
<doc id="8201" url="https://es.wikipedia.org/wiki?curid=8201" title="Lech Wałęsa">
Lech Wałęsa

Lech Wałęsa (/lej vawensa/ en fonética española; Popowo, voivodato de Cuyavia y Pomerania, 29 de septiembre de 1943) es un político polaco, antiguo sindicalista y activista de los derechos humanos. Fue cofundador de Solidaridad, el primer sindicato libre en el Bloque del Este, ganó el en 1983, y fue presidente de Polonia de 1990 a 1995, siendo sucedido por Aleksander Kwaśniewski.

Lech Wałęsa nació el 29 de septiembre de 1943 en Popowo, Polonia, hijo de un carpintero. Estudió primaria y formación profesional antes de entrar en el Astillero Lenin, en Gdańsk, como técnico electricista en 1967. En 1969 se casó con Danuta Gołoś, y la pareja tuvo ocho hijos.

Fue miembro del comité ilegal de huelga en el astillero de Gdańsk en 1970. Tras el sangriento final de la huelga, en la que resultaron muertos alrededor de 80 trabajadores por la policía antidisturbios, Wałęsa fue detenido y condenado por «comportamiento antisocial», pasando un año en prisión.

A causa de razones político partidarias, se han hecho muchas acusaciones a gran parte de los políticos de Polonia, incluso una publicación editada por el Instituto de Memoria Nacional Polaca en 2008, aseguraba que Walesa había colaborado con los servicios secretos comunistas polacos en los años setenta, antes de pasar a la oposición.

En 1978 junto a Andrzej Gwiazda y Aleksander Hall, organizó el movimiento clandestino "Sindicato libre de Pomerania (Wolne Związki Zawodowe Wybrzeża)". Fue detenido varias veces en 1979 por desarrollar una organización «anti-estado», pero no fue declarado culpable en el juicio y fue liberado a principios de 1980, tras lo cual volvió al astillero de Gdańsk.

El 14 de agosto de 1980, tras el comienzo de una huelga laboral en el Astillero Lenin de Gdańsk, Wałęsa escaló su muro ilegalmente y se convirtió en líder de la huelga. Esta huelga fue seguida de forma espontánea por otras, por toda Polonia. Varios días después detuvo a los trabajadores que querían dejar el astillero de Gdańsk y los persuadió para organizar el Comité de Coordinación de Huelga "(Międzyzakładowy Komitet Strajkowy)" para dirigir y apoyar la huelga general espontánea en Polonia.

En septiembre de ese año, el gobierno comunista firmó y acordó con el Comité de Coordinación de Huelga permitir la legalización de la organización, pero no sindicatos realmente libres. El Comité de Coordinación de Huelga se legalizó como Comité de Coordinación Nacional del Sindicato Libre Solidaridad, y Wałęsa fue elegido presidente de ese comité.

Wałęsa permaneció en ese puesto hasta diciembre de 1981, cuando el Primer Ministro Wojciech Jaruzelski declaró la ley marcial. Fue encarcelado durante 11 meses en el sureste de Polonia, cerca de la frontera con la Unión Soviética hasta el 14 de noviembre de 1982.

En 1983 solicitó volver al Astillero de Gdańsk, a su antiguo puesto de electricista. Mientras fue tratado formalmente como un "simple empleado", estuvo prácticamente bajo arresto domiciliario hasta 1987. También en 1983 recibió el . No pudo recoger el premio por sí mismo, por miedo a que el gobierno no le dejase volver. Su mujer, Danuta Wałęsa, recibió el premio en su lugar. Wałęsa donó el importe del premio al movimiento Solidaridad, temporalmente exiliado, y con sede en Bruselas. La decisión de otorgarle el Premio Nobel de la Paz no ha carecido de polémica para los comunistas, ya que por algunos es considerado que Wałęsa contribuyó a la desestabilización política y económica de Polonia en su lucha contra el régimen comunista.

Después de ocho días, el gobierno accedió a entrar en conversaciones en una mesa redonda en septiembre. Wałęsa fue el líder informal del lado no gubernamental durante estas conversaciones. En ellas, el gobierno firmó y aceptó el restablecimiento del sindicato Solidaridad y organizar elecciones "semi-libres" al parlamento de Polonia.

En 1989 Wałęsa organizó y lideró el Comité Ciudadano del Presidente del Sindicato Solidaridad siendo únicamente un cuerpo de asesores, pero en la práctica era un tipo de partido político, que ganó las elecciones parlamentarias de 1989. (La oposición tomó todos los escaños del Sejm y todos menos uno de los escaños del recientemente restablecido Senado; de acuerdo con los Acuerdos de la Mesa Redonda sólo los miembros del Partido Comunista y sus aliados podían ocupar el restante 64% de los escaños del Sejm). 

Mientras que técnicamente era únicamente el presidente del Sindicato Solidaridad, Wałęsa tenía un papel clave en la política polaca. A finales de 1989 persuadió a líderes de aliados formales de los comunistas para formar una coalición gubernamental no comunista, que sería el primer gobierno no comunista en la esfera de influencia del bloque soviético. Después de este acuerdo, para gran sorpresa del Partido Comunista, el parlamento eligió a Tadeusz Mazowiecki como Primer Ministro de Polonia. Así, Polonia, que seguía siendo en teoría un país comunista, empezó a cambiar su economía a un sistema de libre mercado.
No obstante, su jefatura se vio marcada por una crisis económica que propició la derrota del carismático líder y llevó al poder al antiguo político comunista Aleksander Kwaśniewski.

El 9 de diciembre de 1990, Wałęsa ganó las elecciones presidenciales y se convirtió en presidente de Polonia para los siguientes cinco años. Durante su presidencia, empezó una llamada «guerra en la cabeza» que prácticamente suponía un cambio de gobierno anual. Según sus detractores, en este periodo Wałęsa sustrajo miles de documentos secretos del Ministerio del Interior de la RPP con información sobre sus actividades durante la década de 1980; algunos expertos en el tema han afirmado tener pruebas de que los archivos nunca fueron devueltos y de que el antiguo líder de Solidaridad trabajaba para los servicios de inteligencia occidental bajo seudónimo de «Bolek»; un comunista desertor, pero los defensores de Walesa niegan tales acusaciones y otros remiten a la intrincada política interior en Polonia como causa de estas acusaciones.

En 1993 dijo desvincularse totalmente del movimiento Solidaridad, luego de que no le permitieran crear un nuevo movimiento de cara a los comicios de septiembre de ese año. Bajo su presidencia Polonia cambió completamente, de un régimen comunista bajo la influencia de la Unión Soviética a un país capitalista llevó adelante el cierre de empresas estatales y un fuerte plan de reajuste financiero y monetario, que disparó una escalada en los precios y la hiperinflación.

La actitud de Walesa tuvo su manifestación más sonada el 5 de junio de 1992 cuando obligó a dimitir al primer ministro Jan Olszewski, que dirigía un gobierno de coalición de centristas, agrarios y nacional-católicos desde diciembre de 1991 cuando este acusó a Walesa de preparar un golpe de Estado con la implicación de un sector del Ejército.Sumado a varios actos de corrupción protagonizado por ministros y familiares de Walesa. Polonia era un caso ejemplar de transición hacia la economía de mercado. A 1995 Polonia ocupaba el penúltimo lugar entre 30 países en pobreza infantil y el cuarto en desigualdad del ingreso; el 10% más rico de la población obtuvo una mayor proporción del ingreso de mercado y pagó una proporción menor de la carga tributaria que en cualquier otro estado de la OCDE. Las privatizaciones afectaron con más fuerza a los sectores sociales (pensiones, atención sanitaria y educación), mientras el apoyo estatal se extendió a las empresas privadas pero no a los ciudadanos. La primera consecuencia fue la drástica devaluación del zloty (la moneda nacional) aproximadamente entre un 25% y un 30% en un lapso de seis meses – y el desplome de los salarios polacos. En 14 meses ue el desempleo subió del 3.99% al 12.8% desde la crisisCuando el líder del sindicato Solidaridad, Lech Walesa, fue elegido presidente en 1990, el país cayó en una crisis económica y financiera. El abastecimiento energético básico atravesó graves dificultades en los años noventa. Tras la liberalización económica la producción siderúrgica se estancó y declinó un 73 por ciento en el período 1990-1994 que junto con el cierre de fábricas estatales condujo al desempleo a un millón de obreros polacos. En el área agrícola hubo un descenso de entre el 40 % y el 50 % de las cosechas de cereal, patatas, azúcar, remolacha y frutas entre 1991 y 1995, triplicando los costos alimenticios.Para finales de 1994, el PIB de se había reducido en un 7,3% en comparación con 1990, la inflación llegó a un 87%, y las reservas nacionales en oro y divisas disminuyeron en más del 70 por ciento.

Dentro de Solidaridad maniobró también contra sus críticos, como Bronislaw Geremek, cuya destitución como asesor principal del KKS consiguió, o el propio Michnik, que pudo mantenerse como director de Gazeta Wyborzca, el órgano de prensa del sindicato, por la reacción unánime de la plantilla de periodistas. El 24 de junio se dieron de baja 63 miembros destacados, entre ellos Michnik y Kuron. Walesa hizo una campaña con tintes demagógicos y autoritarios de derechas, y se presentó como un candidato providencial que borraría de un plumazo los problemas de Polonia. Sin embargo, su estilo de presidencia fue fuertemente criticado por la mayoría de los partidos políticos, y perdió mucho del apoyo público inicial a finales de 1995. El Acuerdo de Centro (PC), lanzado el 12 de mayo de 1990 por los hermanos gemelos Jaroslaw y Lech Kaczynski para apoyar las aspiraciones presidenciales de Walesa, quedó, con el 8,7% de los votos y 44 escaños, en sexto lugar en las elecciones legislativas del 27 de octubre de 1991. En las siguientes elecciones Walesa pudo, pues, seguir adelante con su aspiración presidencial. Pero el día de las elecciones sólo recibió un testimonial 1,1% de los votos y se situó detrás de otros seis candidatos. Wałęsa apoyó la entrada de Polonia en la OTAN y la Unión Europea , los cuales ocurrieron después de su presidencia, en 1999 y 2004 , respectivamente. A principios de la década de 1990, propuso la creación de un sistema de seguridad subregional llamado OTAN bis. El concepto fue apoyado por movimientos de derecha y populistas en Polonia, pero obtuvo poco apoyo en el extranjero y de los vecinos de Polonia; algunos de los cuales (por ejemplo, Lituania ) lo vieron como un neoimperialismo polaco

Wałęsa perdió las elecciones presidenciales de 1995. Después de las elecciones anunció que iría a un retiro político, pero ha permanecido activo, tratando de establecer su propio partido político. En 1997 apoyó y ayudó a organizar Acción Electoral Solidaridad ("Akcja Wyborcza Solidarność"), que ganó las elecciones al parlamento. Sin embargo, su apoyo fue de menor significado y Wałęsa ocupó una posición muy baja en este partido. El líder real del partido y su principal organizador fue el nuevo líder del sindicato Solidaridad, Marian Krzaklewski.


Lech Wałęsa volvió a optar a la presidencia en las elecciones de 2000, pero recibió únicamente el 1% de los votos. Muchos polacos estuvieron descontentos con el hecho de que una vez más intentará recuperar su poder político tras haber anunciado su retirada. Desde ese momento ha estado dando conferencias sobre la historia y la política de Europa Central en varias universidades extranjeras.
El 10 de mayo de 2004, el aeropuerto internacional de Gdańsk fue renombrado oficialmente Aeropuerto de Gdańsk-Lech Wałęsa en su honor. Su firma se ha incorporado al logotipo del aeropuerto. Hubo alguna controversia acerca de si el nombre debía escribirse "Lech Walesa" (sin diacríticos, pero más fácilmente reconocible en el mundo) o "Lech Wałęsa" (con la grafía polaca, pero más difícil de escribir o pronunciar para extranjeros). Un mes más tarde, Wałęsa acudió a los Estados Unidos representando a Polonia para los funerales de estado de Ronald Reagan.

Lech Walesa ha dejado ver en múltiples ocasiones su descontento con el rumbo político que ha tomado el país, en particular con los hermanos Kaczyński, quienes fueron sus antiguos compañeros sindicales y a quienes ha acusado de demagogos oportunistas que han llevado a Polonia en la dirección equivocada.

Además de su Premio Nobel, Wałęsa ha recibido otros premios internacionales. Ha sido distinguido con el título de doctor "honoris causa", por varias universidades europeas y estadounidenses. La última, en enero de 2011, fue la Universidad Europea de Madrid, España.

Wałęsa es conocido por su ferviente catolicismo. El 1 de marzo de 2013, Wałęsa declaró que los diputados homosexuales deberían sentarse fuera del Parlamento, puesto que representan a una minoría.Según declararía después Lech Wałęsa: "«"No quiero que esta minoría, con la que no estoy de acuerdo pero que tolero, se manifieste en la calle y haga girar la cabeza a mis hijos y nietos"»".

Wałęsa también dijo que los homosexuales tienen poca importancia como minoría y por lo tanto tienen que «ajustarse a las cosas pequeñas», a lo que Robert Biedroń, miembro del Parlamento respondió: «Si aceptamos las reglas propuestas por Lech Wałęsa, ¿donde se sentarán los negros? Ellos también son minoría. ¿Y qué tal las personas con discapacidad?».

En 2008 Slawomir Cenckiewicz y Piotr Gontarczyk llevaron a cabo una investigación en nombre del Instituto de Memoria Histórica polaco y publicaron un libro que insinuaba sobre la colaboración de Walesa con el régimen comunista y su labor como confidente del Gobierno entre 1970 y 1972. “Bolek” (supuesto nombre en clave de Walesa) habría recibido dinero a cambio de denunciar las actividades de al menos 40 compañeros de trabajo en el astillero de Gdansk. La documentación aportada incluye recibos firmados supuestamente de puño y letra por Walesa con su nombre y apodo. En el año 2015, fue hallado en un domicilio un dossier que contenía 41 informes firmados por “Bolek” y recibos por un valor total de 11.700 zlotys. 

En 2008 el gobierno de Venezuela comunicó a Walesa que lo consideraba persona no grata.




</doc>
<doc id="8202" url="https://es.wikipedia.org/wiki?curid=8202" title="Teresa de Calcuta">
Teresa de Calcuta

Teresa de Calcuta (Uskub, Imperio otomano —actual Skopie, Macedonia del Norte—; 26 de agosto de 1910-Calcuta, India; 5 de septiembre de 1997), de nombre secular Agnes Gonxha Bojaxhiu () y también conocida como Santa Teresa de Calcuta o Madre Teresa de Calcuta, fue una monja católica de origen albanés naturalizada india, que fundó la congregación de las Misioneras de la Caridad en Calcuta en 1950. Durante más de 45 años atendió a pobres, enfermos, huérfanos y moribundos, al mismo tiempo que guiaba la expansión de su congregación, en un primer momento en la India y luego en otros países del mundo. Tras su muerte, fue beatificada por el papa Juan Pablo II. Su canonización fue aprobada por el papa Francisco en diciembre de 2015, después de que la Congregación para las Causas de los Santos reconociera como extraordinaria la curación de un brasileño enfermo en estado terminal. El acto oficial de canonización tuvo lugar en Roma en la mañana del domingo 4 de septiembre de 2016.

Agnes descubrió su vocación desde temprana edad, y para 1928 ya había decidido que estaba destinada a la vida religiosa. Fue entonces cuando optó por cambiar su nombre a «Teresa» en referencia a la santa patrona de los misioneros, Teresa de Lisieux. Si bien dedicó los siguientes 20 años a enseñar en el convento irlandés de Loreto, comenzó a preocuparse por los enfermos y por los pobres de la ciudad de Calcuta. Esto la llevó a fundar una congregación con el objetivo de ayudar a los marginados de la sociedad, primordialmente enfermos, pobres y personas que no tenían hogar.

En la década de 1970 era conocida internacionalmente y había adquirido reputación de persona humanitaria y defensora de los pobres e indefensos, en parte por el documental y libro "Something Beautiful for God", de Malcolm Muggeridge. Obtuvo el Premio Nobel de la Paz en 1979 y el más alto galardón civil de la India, el Bharat Ratna, en 1980, por su labor humanitaria. A ellos se sumaron una decena de premios y reconocimientos de primer nivel, tanto nacionales como internacionales.

Recibió elogios de muchas personas, gobiernos y organizaciones. Sin embargo, afrontó también una serie de críticas, como las objeciones de Christopher Hitchens, Michael Parenti, Aroup Chatterjee y el Consejo Mundial Hindú, que le achacaron una mentalidad reaccionaria y criticaron la deficiente atención en sus centros. En 2010, en el centenario de su nacimiento, fue homenajeada alrededor del mundo, y su trabajo elogiado por la presidenta india Pratibha Patil.

Agnes Gonxha Bojaxhiu («gonxha» significa «capullo de rosa» o «pequeña flor» en albanés) nació el 26 de agosto de 1910 en Uskub, entonces parte del Imperio otomano y actualmente Skopie, República de Macedonia del Norte, pero solía considerar como su fecha de nacimiento el 27 de agosto, ya que ese fue el día en que la bautizaron. Fue la menor de los hijos de un matrimonio acomodado de Shkodër, integrado por Nikollë Bojaxhiu (1878-1919) y Dranafile Bernai (1889-1972). Su familia pertenecía a la población albanesa proveniente de Kosovo asentada en Shkodër —su padre posiblemente era originario de Prizren y su madre de una villa cercana a Đakovica—. Su padre, involucrado en la política de Albania, murió repentina y misteriosamente en 1919 cuando Agnes contaba con apenas ocho años luego de ser trasladado al hospital, por causas desconocidas, aunque se presume que fue a causa de un envenenamiento. Tras la muerte de este, su madre la educó en el seno de la religión católica.

En su niñez, Agnes asistió a la escuela estatal y participó como soprano solista del coro de su parroquia; en ausencia del director, se encargaba incluso de la dirección del grupo. Además, pertenecía a una congregación mariana fundada en 1563 y conocida como Sodalicio de Nuestra Señora, donde comenzó a interesarse por las historias de los misioneros jesuitas de Yugoslavia que estaban en Bengala. Desde entonces sintió el deseo de trabajar al igual que ellos en la India. De acuerdo con la biografía escrita por Joan Graff Clucas, desde temprana edad Agnes se mostró fascinada por las historias de vida de los misioneros y sus obras en Bengala. A la edad de cinco años hizo la Primera comunión y a los seis, la Confirmación; con doce años ya estaba convencida de que debía dedicarse a la religión. Su resolución definitiva fue tomada el 15 de agosto de 1928, mientras rezaba en la capilla de la Virgen Negra de Letnice, donde acudía con frecuencia de peregrinación.

El 26 de septiembre de 1928, poco después de haber cumplido 18 años, se dirigió con una amiga a la Abadía de Loreto, perteneciente a la congregación religiosa católica Instituto de la Bienaventurada Virgen María, en Rathfarnham, Irlanda. A partir de ese momento, jamás volvería a ver a su madre ni a su hermana. Si bien originalmente acudió a ese lugar para aprender inglés (que era el idioma que las hermanas de Loreto enseñaban a los niños en la India), una vez ahí fue admitida como postulante y en noviembre de 1928 se trasladó por vía marítima hacia Calcuta, sitio a donde arribó el 6 de enero de 1929. En Darjeeling, cerca de las montañas del Himalaya, inició su noviciado y aprendió bengalí además de enseñar en la escuela de Santa Teresa, que se hallaba cerca de su convento. Después de hacer sus votos de pobreza, castidad y obediencia como monja el 24 de mayo de 1931, fue trasladada al Colegio de Santa María en Entally, al este de Calcuta. En ese período, eligió ser llamada con el mismo nombre que Teresa de Lisieux, la santa patrona de los misioneros. Sin embargo, debido a que una enfermera en el convento ya había elegido ese nombre, Agnes optó por usar el término castellanizado de «Teresa» (en vez de «Thérèse»). El 14 de mayo de 1937, Teresa hizo sus votos solemnes mientras enseñaba en el colegio del convento de Loreto. Trabajó ahí por casi veinte años como profesora de historia y geografía hasta que, en 1944, se convirtió en directora del centro.

Si bien disfrutaba enseñar en el colegio, cada vez se perturbaba más en razón de la pobreza existente en Calcuta. La hambruna de 1943 en Bengala trajo consigo miseria y muerte a la ciudad, mientras que la ola de violencia hindú-musulmana suscitada en agosto de 1946 hundió a la población en la desesperación y el terror.

El 11 de septiembre de 1946, nombrada ya encargada de un colegio de las Hermanas Santa Ana, Teresa experimentó lo que más tarde describió como la «llamada dentro de la llamada», en referencia a haber escuchado a Dios pidiéndole que dedicara su vida a los menos privilegiados de la sociedad. Esto ocurrió justamente en un viaje en tren rumbo al convento de Loreto, en Darjeeling, desde Calcuta para su retiro anual. «Estaba por dejar el convento y ayudar a los pobres mientras vivía entre ellos. Fue una orden. Fallar habría significado quebrantar la fe».

Tras haber recibido capacitación médica básica en París con el apoyo financiero de un empresario indio católico, comenzó a trabajar entre los pobres en 1948 enseñándoles a leer. Tras adoptar la ciudadanía india en 1950, recibió formación como enfermera durante tres meses en Patna con las Hermanas Misioneras Médicas de Norteamérica y finalmente se asentó en los barrios más pobres. Al principio, inauguró una escuela en Motijhil (Calcuta) y pronto empezó a enfocarse en las necesidades de los indigentes y de los hambrientos. A comienzos de 1949, se le unió un grupo de mujeres jóvenes y sentó las bases para crear una nueva comunidad religiosa que ayudara a los «más pobres entre los pobres». Pronto sus esfuerzos atrajeron la atención de funcionarios indios, entre ellos el primer ministro, quienes le expresaron su aprecio.

Teresa escribió en su diario personal que su primer año de trabajo con los pobres estuvo repleto de dificultades. No tenía ingresos y por ello se veía en la necesidad de pedir donaciones de alimentos y suministros. Según relató, durante los primeros meses experimentó duda, soledad e incluso, la tentación de volver a su vida en el convento. En sus propias palabras:

En 1948, envió un pedido al Vaticano para iniciar una congregación diocesana; sin embargo, en la India existían serias dificultades políticas como consecuencia de su reciente independencia. Por lo tanto, podría ser mal visto que una europea se dedicara a los pobres en la situación de ese entonces. Su permiso para abandonar el convento se le concedió en agosto de 1948 cuando abandonó el lugar solamente con cinco rupias para ayudar a los más necesitados. Madre Teresa comenzó a portar un sari blanco de algodón decorado con bordes azules en sustitución de su tradicional hábito de Loreto. El 7 de octubre de 1950, la Santa Sede le autorizó a inaugurar su nueva congregación, a la cual denominó las Misioneras de la Caridad. Según Teresa, su misión desde entonces fue cuidar a «los hambrientos, los desnudos, los que no tienen hogar, los lisiados, los ciegos, los leprosos, toda esa gente que se siente inútil, no amada, o desprotegida por la sociedad, gente que se ha convertido en una carga para la sociedad y que son rechazados por todos».

Aunque inicialmente la congregación tenía solo trece miembros en Calcuta, con el tiempo llegó a poseer más de cuatro mil integrantes presentes en orfanatos, hospicios y centros de sida de todo el mundo. La congregación ofreció caridad y cuidado a los refugiados, entre los que se contaban ciegos, discapacitados, alcohólicos, ancianos, pobres, personas sin hogar y víctimas de inundaciones, epidemias o hambrunas.

En 1952, inauguró el primer hogar para moribundos en Calcuta. Luego de obtener ayuda de diversos funcionarios indios, se convirtió un abandonado templo hindú en el Hogar para moribundos «Kalighat», un hospicio gratuito para los pobres. Tiempo después su nombre se modificó a «Kalighat, la casa del corazón puro». Todos aquellos que llegaban a Kalighat recibían atención médica y se les ofrecía la oportunidad de morir con dignidad de acuerdo a los rituales de su fe; los musulmanes leían el Corán, los hindúes recibían agua del Ganges y los católicos obtenían los últimos ritos. Según Teresa, «para personas que vivieron como animales, una muerte hermosa es morir como ángeles, amados y queridos».

En 1955, con el creciente aumento de niños abandonados, abrió la institución «Hogar del Niño del Inmaculado Corazón» para los huérfanos y los jóvenes sin hogar. Posteriormente, fundó el centro «Shanti Nagar» para aquellos individuos que padecían la enfermedad de Hansen, comúnmente conocida como lepra, junto con otras clínicas similares donde las Misioneras de la Caridad proporcionaban atención médica y alimentos.

En 1964, el papa Pablo VI, en ocasión de su viaje a Bombay por un congreso eucarístico, le regaló un vehículo Lincoln tipo limusina color blanco que luego fue subastado por la Madre Teresa; con el dinero obtenido, organizó un establecimiento para leprosos denominado «Ciudad de la Paz», muy similar a «Don de la Paz», un centro de rehabilitación fundado por Teresa con el dinero que obtuvo junto con el premio Juan XXIII en 1971. La Fundación Joseph P. Kennedy Jr. le concedió un bono de 15 000 USD que se destinó a un centro médico en Dum Dum. Para la década de 1960, ya había establecido una gran cantidad de hospicios, orfanatos y casas de leprosos en toda la India.

Su orden comenzó a propagarse por el mundo a partir de 1965, cuando su congregación se estableció en Venezuela con tan solo cinco hermanas. Hacia 1968, Madre Teresa había inaugurado establecimientos en Roma, Tanzania y Austria e incluso se extendió por gran parte de Asia, África, Europa y Estados Unidos. En el momento de su fallecimiento, la orden operaba 610 misiones en 123 países, incluidas tareas en hospicios y hogares para personas con sida, lepra y tuberculosis, comedores populares, programas de asesoramiento para niños y familias, orfanatos y escuelas.

La rama masculina de su congregación fue fundada en 1963 —los Hermanos Misioneros de la Caridad—. En esa ocasión, se inscribieron laicos católicos y no católicos como colaboradores de Teresa y compañeros de los enfermos. En respuesta a las peticiones de muchos sacerdotes, en 1981 inició el Movimiento Corpus Christi y en 1984 fundó los Padres Misioneros de la Caridad junto al padre Joseph Langford para combinar los objetivos profesionales de las hermanas con los recursos del sacerdocio ministerial. En 2007, la orden contaba con un número aproximado de 450 hermanos y 5000 monjas en todo el mundo que operaban 600 misiones en escuelas y hogares en 120 países.

Entre el 26 de marzo y el 16 de diciembre de 1971, ocurrió la Guerra de Liberación de Bangladesh, confrontación bélica entre la India y Pakistán, en la cual se produjeron violaciones a mujeres, razón por la cual muchas se habrían suicidado, enloquecido o huido. Además, se les había prohibido contraer matrimonio y tener hijos durante ese período. La Madre Teresa junto a sus hermanas establecieron sitios para acogerlas y brindarles todos los cuidados necesarios. El gobierno, por su parte, otorgó la asistencia de unas 15 hermanas más debido a la gran cantidad de refugiadas. Luego fueron alentadas para que volvieran a reconstruir su matrimonio, adoptar hijos y regresar a sus pueblos, motivo por el cual recibieron el agradecimiento del primer ministro, que relató que esas jóvenes deberían ser consideradas como «heroínas nacionales».

En 1982, a la altura del asedio de Beirut, la Madre Teresa rescató a 37 niños que estaban atrapados en un hospital de esa región tras negociar un cese al fuego entre el ejército israelí y las guerrillas palestinas. Acompañada por trabajadores de la Cruz Roja, se trasladó a través de la zona de guerra hacia el hospital devastado para evacuar a los pacientes jóvenes.

A finales de la década de 1980, amplió sus esfuerzos en los países comunistas que habían ignorado a las Misioneras de la Caridad anteriormente y se embarcó en decenas de proyectos. Visitó la República Soviética de Armenia después del terremoto de Spitak en 1988 y se reunió con Nikolai Ryzhkov, presidente del Consejo de Ministros. Además, viajó para asistir y atender a varios hambrientos en Etiopía al igual que a las víctimas del accidente de Chernóbil —motivo por el cual obtuvo la Medalla de Oro del Comité Soviético de Paz; cabe señalar que la Unión Soviética se consideraba una nación atea— y las de un terremoto de Armenia. En 1991, la Madre Teresa volvió por primera vez a su tierra natal y abrió una casa de Hermanos Misioneros de la Caridad en Tirana.

Para 1996, Teresa regentaba 517 misiones en más de 100 países. Con el paso de los años, las ayudantes de la Madre Teresa pasaron de ser trece a miles, colaborando en aproximadamente 450 centros de todo el mundo. La primera casa de los Misioneros de la Caridad en Estados Unidos se estableció en el sur del distrito del Bronx, Nueva York, en 1984, con el fin de operar en 19 establecimientos de todo el país.

Por otra parte, Teresa de Calcuta identificó como potencial patrono al padre Damián de Veuster, el apóstol de los leprosos, con un carisma similar al que caracteriza a la orden de las Misioneras de la Caridad. La Madre Teresa pidió explícitamente a Juan Pablo II por un santo que permitiera a la congregación continuar su trabajo de amor y curación:

La Madre Teresa estuvo presente en la misa de beatificación de Damián de Veuster en Bruselas, el 4 de junio de 1995, y le atribuyó más tarde «la eliminación del miedo de los corazones de los leprosos para reconocer la enfermedad, proclamarla y solicitar medicina, y el nacimiento de la esperanza de ser curados» y el cambio de actitud de la gente y de los gobernantes hacia las víctimas de la lepra: «más preocupación, menos miedo, y disposición para ayudar –en cualquier tiempo y en todo tiempo–».

Con el paso de los años, la salud de la Madre Teresa empezó a deteriorarse cada vez más a un ritmo acelerado. En 1983, sufrió un ataque cardíaco en Roma mientras visitaba al papa Juan Pablo II. Después de un segundo ataque en 1989, recibió un marcapasos artificial. En 1991, se sobrepuso de una neumonía durante una estancia en México, para lo cual fue tratada en un hospital de California. Afectada por nuevas dolencias cardíacas, ofreció renunciar a su puesto como líder de las Misioneras de la Caridad, pero las monjas de la orden, en un sufragio secreto, votaron unánimemente a favor de que se quedara y la Madre Teresa aceptó continuar con su labor al frente de la orden. En 1993 fue ingresada en el Hospital de las Naciones Unidas a raíz de una congestión pulmonar que le provocó, entre otros síntomas, fiebre. Ese mismo año desarrolló malaria, la cual se agravó debido a sus problemas pulmonares y cardíacos, y en Roma se rompió tres costillas.

En abril de 1996, la Madre Teresa se cayó y se fracturó la clavícula. Para agosto, sufría de insuficiencia en el ventrículo izquierdo de su corazón. Recibió una cirugía cardíaca pero su salud declinó de forma notable. Cuando enfermó nuevamente, tomó la controvertida decisión de internarse en un hospital bien equipado de California, lo que originó diversas críticas. Al ser hospitalizada por problemas cardíacos de nuevo, el arzobispo de Calcuta, Henry Sebastian D'Souza, ordenó a un sacerdote llevar a cabo un exorcismo en la Madre Teresa con su permiso porque pensaba que ella podía haber sido atacada por el diablo.

El 13 de marzo de 1997, renunció como jefa de las Misioneras de la Caridad debido a sus enfermedades y padecimientos. La hermana María Nirmala Joshi fue elegida para tomar su lugar pero rehusó adoptar el título de Madre. En sus palabras, «nadie puede reemplazar a la Madre Teresa». Teresa de Calcuta falleció el 5 de septiembre de 1997 a los 87 años a causa de un paro cardíaco, después de amanecer con fuertes dolores de espalda y problemas respiratorios. Se hallaba de reposo en Santo Tomás (Calcuta) una semana antes de su muerte, en septiembre de 1997. El gobierno indio le concedió un funeral de Estado y, como parte de este, su féretro fue trasladado por gran parte de la ciudad en el mismo carruaje en el que fueron llevados los restos de Mahatma Gandhi y Jawaharlal Nehru.

La Madre Teresa fue reconocida por primera vez por el gobierno indio cuando obtuvo el galardón «Padma Shri» en agosto de 1962 y el premio «Jawaharlal Nehru» para el Entendimiento Internacional en 1969. Continuó recibiendo más premios notables en la India en los siguientes años, incluyendo el «Bharat Ratna» (el más importante entregado a un civil en la India) el 22 de marzo de 1980, el «Rajiv Gandhi Sadbhavana» en 1993 y el galardón artístico «Dayawati Modi» en 1995. Su biografía oficial inclusive fue escrita por un ciudadano indio, Navin Chawla, y publicada en 1992.

El 28 de agosto de 2010, en conmemoración a su centenario, el gobierno indio emitió monedas especiales de cinco rupias con su imagen y la presidenta Pratibha Patil expresó: «Vestida con un sari blanco con bordes azules, ella y las hermanas de las Misioneras de la Caridad se convirtieron en un símbolo de esperanza para muchos ancianos, indigentes, desempleados, enfermos y abandonados por sus familias».

En 1962, el presidente de Filipinas le entregó el premio «Ramón Magsaysay», destinado a «perpetuar su ejemplo de integridad en el gobierno, valiente servicio a la gente y el idealismo pragmático en una sociedad democrática, destacando el trabajo en el suroeste de Asia.» A principios de la década de 1970, la Madre Teresa se había convertido en una figura relevante para la religión en todo el mundo. Su popularidad se debía posiblemente en gran parte al documental de 1969 "Something Beautiful for God", de Malcolm Muggeridge, quien publicó luego un libro con el mismo título en 1972. Por entonces, Muggeridge se hallaba en una etapa de búsqueda espiritual personal. Durante el rodaje, el material grabado se rodó en lugares con poca iluminación por lo que se creyó que iba a ser de baja calidad, pero al momento de editar el contenido el equipo se percató que el material se hallaba en condiciones aceptables. Tiempo después, Muggeridge definió el hecho como un milagro atribuido a la propia Madre Teresa, aunque esto fue negado por otros integrantes del filme que dijeron que se debió a que habían usado un nuevo tipo de película ultrasensitiva Kodak. Más tarde, Muggeridge se convirtió al catolicismo.
En esa misma época, el mundo católico comenzó a honrarla públicamente. El 6 de enero de 1971, el papa Pablo VI le entregó el premio internacional por la paz «Juan XXIII», elogiando su labor con los pobres, su manifestación de caridad cristiana y sus esfuerzos por la paz. El 16 de octubre de 1971, también se hizo acreedora del premio «Good Samaritan» por la Fundación Joseph P. Kennedy Jr, tras hablar en un simposio sobre el trato que había mantenido hasta entonces con toda la gente rechazada en las calles de Calcuta. En abril de 1973, se convirtió en la primera ganadora del premio Templeton otorgado en Londres por su labor de ayuda a los pobres y necesitados de Calcuta. De acuerdo a la descripción en la página web oficial del galardón: «su trabajo heroico trajo un verdadero cambio a aquellos a los que ella sirvió y continúa inspirando a millones en todo el mundo».

Fue honrada por gobiernos y organizaciones civiles, así como también resultó designada Compañera de Honor de la Orden de Australia en 1982 por «el servicio a la comunidad de Australia y de la humanidad en general». El Reino Unido y Estados Unidos le concedieron premios en varias ocasiones, entre ellos la Orden de Mérito en 1983 y la ciudadanía honoraria de Estados Unidos el 16 de noviembre de 1996. Su país natal, Albania, le otorgó el Honor de Oro de la Nación en 1994. Universidades, tanto de Occidente como de la India, le otorgaron títulos honoríficos.
Otros premios internacionales que recibió incluyen el «Mater et magistra» otorgado el 19 de junio de 1974 en los Estados Unidos por la Tercera Orden de San Francisco de Asís, una medalla acuñada exclusivamente para ella por la Organización para la Agricultura y la Alimentación de la ONU otorgada en Roma en agosto de 1975, el premio «Pacem in Terris» en 1976, el premio internacional «Balzan» (Roma, 1978) para la «promoción de la humanidad, la paz y la hermandad entre los pueblos» y el reconocimiento internacional «Albert Schweitzer» (Estados Unidos, 23 de octubre de 1975).

En 1979, recibió el premio Nobel de la Paz al «trabajo emprendido en la lucha por superar la pobreza y la angustia, que también constituyen una amenaza para la paz». Teresa rehusó asistir al banquete ceremonial ofrecido a los premiados y pidió que los fondos de 192 000 USD se entregaran a los pobres de la India. Cuando la Madre Teresa recibió el premio, se le preguntó: «¿Qué podemos hacer para promover la paz mundial?» y respondió «Vete a casa y ama a tu familia». En su conferencia sobre el premio que le entregó el rey Olaf V de Noruega, la religiosa dijo: «Lo acepto para la gloria de Dios y de su pueblo, el más pobre entre los pobres». También apuntó que el aborto es «uno de los mayores destructores de la paz».

Al momento de su muerte, el primer ministro de Pakistán Nawaz Sharif dijo que era «una persona extraña y única que vivió mucho tiempo para propósitos más elevados. Su devoción por la vida para el cuidado de los pobres, los enfermos y los desfavorecidos es uno de los mejores ejemplos de servicio a nuestra humanidad». El exsecretario general de la ONU Javier Pérez de Cuéllar expresó: «Ella es la Naciones Unidas, la paz en el mundo». Por su parte, el presidente Bill Clinton la definió como una «gigante de nuestra era», y luego de su muerte, Juan Pablo II declaró: «Sigue viva en mi memoria su diminuta figura, doblada por una existencia transcurrida al servicio de los más pobres entre los pobres, pero siempre cargada de una inagotable energía interior, la energía del amor de Cristo». Durante su vida, la Madre Teresa fue nombrada 18 veces en las encuestas Gallup sobre los hombres y mujeres más admirados del año, siendo electa en la categoría de las 10 mujeres más apreciadas por los estadounidenses en todo el mundo. En aquel rubro, ocupó el primer lugar varias veces en las décadas de 1980 y 1990. En 1999, fue considerada dentro de las «mujeres más admiradas del siglo XX» por una encuesta de Estados Unidos, en la cual sobrepasó a los otros candidatos por un amplio margen, posicionándose en el primer puesto en las principales categorías demográficas excepto en la de los más jóvenes.

La Madre Teresa ha sido tildada por uno de sus detractores, Christopher Hitchens, de tener una visión fundamentalista dentro de la propia ortodoxia de la Iglesia. Durante el Concilio Vaticano II, manifestó su oposición a cualquier reforma de la Iglesia católica. Según ella, lo que se necesitaba era más trabajo y más fe, no una revisión doctrinal. Por otra parte, una cuestión clave en la crítica a sus enseñanzas es su prédica constante del consuelo y el conformismo. Después de la explosión de la planta química de la multinacional Union Carbide en Bhopal (India), se presentó inmediatamente en el lugar de la tragedia, donde 2.500 personas habían muerto. «Perdonad, perdonad, perdonad», repitió nada más al bajarse del avión, sin motivar a que los afectados iniciaran acciones legales o se persiguiera a los culpables. «Estás sufriendo como Cristo en la cruz, así que Jesús te debe estar besando», le dijo Teresa de Calcuta a un enfermo de cáncer que se retorcía de dolor ante las cámaras. Desde su lecho, le respondió: «Por favor, dígale que pare de besarme». Esto último fue objeto de críticas igualmente pues Teresa sentía que el sufrimiento en las personas las hacía acercarse más a Jesús.

A este planteamiento respondió el doctor en sociología William A. Donohue, presidente de la Liga Católica por Derechos Religiosos y Civiles en los Estados Unidos:

A las críticas acerca de su firme posición contra el aborto y el divorcio respondió: «No importa quién lo dice, deben aceptar con una sonrisa y hacer su propio trabajo». Igualmente, su oposición a la inseminación artificial y el uso de anticonceptivos fue objeto de críticas; en sus palabras: «Yo no le daría un bebé de una de mis casas en adopción a una pareja que usa anticonceptivos. Los que usan anticonceptivos no comprenden el amor».

Las opiniones de los hindúes respecto de la Madre Teresa no eran uniformemente favorables. El importante partido político Bharatiya Janata Party, se opuso a la Madre pero la elogió después de su muerte, enviando un representante para su funeral. La organización Consejo Mundial Hindú, en cambio, se opuso a la decisión del gobierno de realizarle un funeral de Estado. Incluso, un recordatorio de la revista "Frontline" negó unas acusaciones propiciadas por Giriraj Kishore como «completamente falsas» y se publicó que lo que habían hecho «no influye en la percepción pública de su trabajo, especialmente en Calcuta». El autor del homenaje, a pesar de alabar su «desinteresada atención», su energía y vitalidad, fue crítico de sus campañas públicas en contra del aborto.

La calidad de la atención ofrecida a los pacientes con enfermedades terminales en los hogares para moribundos fue criticada igualmente por la prensa médica. El doctor Robin Fox, de la revista médica "The Lancet", hizo referencia a la insuficiencia de médicos, de tratamientos sistemáticos y de analgesia, mientras que Mary Loudon del "British Medical Journal", reportó la reutilización de agujas hipodérmicas, malas condiciones de vida, incluyendo el uso de agua fría para el aseo de los refugiados y un mal enfoque sobre la enfermedad y el sufrimiento, ya que se inhibió el uso de variados elementos indicados para la atención médica moderna como así también el diagnóstico sistemático. El doctor Robin Fox, editor de "The Lancet", tras su visita a los centros de Calcuta en 1994, constató que a los pacientes no se les diagnosticaban las enfermedades ni se les administraban analgésicos eficientes. Describió la atención médica como «fortuita, con voluntarios sin conocimientos médicos que tuvieron que tomar decisiones sobre el cuidado del paciente debido a la falta de médicos». Señaló que su orden no distinguía entre los pacientes curables e incurables, motivo por el cual gente que podía sobrevivir corría el riesgo de morir por infecciones o falta de tratamiento.

El autor de un periódico católico, David Scott, escribió que la Madre Teresa se «limitó a mantener viva a la gente en lugar de luchar contra la pobreza en sí». A su vez Sanal Edamaruku, presidente de la organización Rationalist International, criticó por escrito el hecho de que en algunos casos no otorgaban analgésicos en sus casas para moribundos y que podían oírse los gritos de sufrimiento por parte de la gente que tendría gusanos en sus heridas abiertas sin obtener alivio del dolor. En un principio, los analgésicos fuertes, incluso en los casos difíciles, no se daban.

En respuesta a las críticas de Robin Fox, y comentando explícitamente el tema de la disponibilidad de analgésicos, tres investigadores de instituciones inglesas (David Jeffrey, Joseph O'Neill y Gilly Burn) que acreditan trabajos científicos sobre la práctica de la medicina en la India, escribieron en la revista médica "The Lancet": «Incluso en 1994, la mayoría de los pacientes con cáncer vistos (en la India) no tenían acceso a ninguna analgesia, debido a la falta de medicamentos adecuados, de conocimiento acerca del uso de los medicamentos por parte de los médicos, como así también, en algunos casos, el desconocimiento sobre el manejo del dolor, agravado por la falta de recursos. La Madre Teresa es digna de elogio por, al menos, ofrecer bondad. Si Fox fuera a visitar las principales instituciones que están a cargo de la profesión médica en la India, rara vez vería él limpieza, atención de heridas y llagas, o bondad. Además, la analgesia podría no estar disponible».

Colette Livermore, una ex misionera de la caridad, describió las razones por las cuales abandonó la congregación en su libro "Hope Endures: Leaving Mother Teresa, Losing Faith, and Searching for Meaning". Según la propia Livermore, encontró lo que la religiosa denominó la «teología del sufrimiento», a la que definió como defectuosa. Sin embargo, calificó a Teresa como una persona buena y valiente. Aunque ella instruyó a sus seguidores sobre la importancia de la difusión del Evangelio a través de acciones en lugar de lecciones teológicas, Livermore no podía conciliar esto con algunas prácticas de la organización. Los ejemplos que citó fueron negarse innecesariamente a ayudar a los necesitados cuando éstos se acercaron a las monjas en un momento equivocado de acuerdo con sus agendas de horarios y desalentar a las monjas de buscar la formación médica para tratar las enfermedades que enfrentaban (con la justificación de que Dios permite a los débiles e ignorantes).

Chatterjee confesó que la Madre y sus biógrafos oficiales (entre los que más se destacan, Navin Chawla) se habían negado a colaborar en sus investigaciones y que Teresa no pudo «defenderse» de la cobertura crítica de la prensa occidental, dando como ejemplo el informe publicado por el diario británico "The Guardian", que atacó la condición de sus orfanatos, y del documental "Mother Teresa: Time for Change?", que fue difundido en varios países europeos.

Otras críticas de Hitchens estuvieron relacionados con los orígenes de algunas donaciones y las personas con quienes se vinculó. La religiosa aceptó dinero de la familia Duvalier (François Duvalier y su hijo Jean-Claude fueron dictadores de Haití) y los elogió públicamente. En el programa de la CBS "Sixty Minutes" afirmó públicamente de Michèle Bennett, esposa de Baby Doc: «Nunca he visto a los pobres ser tan familiares con sus jefes de Estado como lo son con ella. Para mí es una bella lección». Las imágenes de Teresa de Calcuta pronunciando estas palabras fueron reproducidas durante al menos una semana por la televisión pública haitiana. A ello replicó Donohue:

Hitchens también señaló que la Madre Teresa aceptó 1,25 millones de USD de Charles Keating, quien también le concedió el uso de un avión y portaba un crucifijo que ella le diera. Según Hitchens, Teresa lo apoyó después de su detención enviando una carta al juez del caso: «No sé nada de los negocios de Charles Keating. Solo sé que ha sido generoso con los pobres de Dios». Hitchens escribió que el fiscal Paul W. Turley habría quedado perplejo al leer la carta manuscrita de Teresa de Calcuta. En enero de 1992, Charles Keating, el «rey de los bonos basura», había estafado a 17 000 pequeños inversores en uno de los mayores escándalos de Estados Unidos. Según Hitchens, la justicia no atendió la que él llamó «petición de clemencia» de Teresa de Calcuta, y Keating fue condenado a 10 años de cárcel. En Calcuta, la directora de las Misioneras de la Caridad habría recibido una carta del fiscal en la que se le informaba de la naturaleza del dinero estafado: «Le ruego que devuelva el dinero que robó Keating a las personas que lo ganaron con su trabajo. La Madre Teresa no contestó. Pero William A. Donohue replicó a Hitchens:

En 1996, Irlanda celebró un referendo acerca de si su Constitución debería seguir prohibiendo el divorcio. La Madre Teresa tomó un avión desde Calcuta para apoyar la campaña a favor del voto negativo. Sin embargo, ese mismo año Teresa concedió una entrevista en la que decía que confiaba en que su amiga Diana de Gales fuera más feliz una vez que se hubiera librado de lo que evidentemente era un matrimonio desafortunado.

En el primer aniversario de su deceso, la revista alemana "Stern" lanzó un artículo que hablaba sobre las cuestiones financieras y el gasto de las donaciones. La prensa médica realizó críticas derivadas de diversas perspectivas y prioridades sobre las necesidades de los pobres. Otros comentarios provinieron de Tariq Ali, un miembro del comité de la editorial New Left Review, y del periodista de investigación irlandés Donal MacIntyre. Christopher Hitchens y la revista alemana "Stern" expresaron que la Madre Teresa no centró el uso del dinero en la reducción de la pobreza o en la mejora de las condiciones de sus centros, sino que lo utilizó para la apertura de nuevos conventos y el aumento de la labor misionera. William A Donohue repuso a Hitchens:
Un aspecto particularmente notable de la Madre Teresa es la profunda crisis de fe a la que se enfrentó por casi cinco décadas de su vida, misma que quedó evidenciada en el libro "Mother Teresa: Come Be My Light" editado por Brian Kolodiejchuk que recopila las cartas privadas escritas por la misionera. A pesar de ello, esto no fue obstáculo para el proceso de su beatificación llevado a cabo en 2003.

Tras analizar sus obras y logros, el papa Juan Pablo II dijo: «¿Dónde encontraba la Madre Teresa la fuerza y la perseverancia para ponerse totalmente al servicio de los demás? En la contemplación silenciosa de Jesucristo, su Santo Rostro, su Sagrado Corazón». En privado y durante casi 50 años hasta el final de su vida, la Madre Teresa experimentó dudas sobre sus creencias religiosas, en las cuales «no sentía la presencia de Dios en lo absoluto», «ni en su corazón ni en la eucaristía», según dijo su postulador, el reverendo Brian Kolodiejchuk. La Madre Teresa no solo sobrellevó el dolor provocado por su falta de fe, sino que también sintió graves dudas sobre la existencia de Dios:
Con referencia a las palabras anteriores, su postulador, Kolodiejchuk (el funcionario responsable de reunir las pruebas para su beatificación), indicó que existía el riesgo de que algunos pudieran malinterpretar lo dicho pero que la fe de la Madre de que Dios estaba trabajando a través de ella se mantuvo intacta, y si bien se lamentaba por el sentimiento de pérdida de cercanía con Dios, no puso en duda su existencia. Muchos otros santos tuvieron experiencias similares de aridez espiritual. Contrariamente a las creencias erróneas por parte de algunos que expresaron que esas dudas serían un impedimento para la canonización, este proceso se llevó a cabo sin ningún obstáculo en dicho rubro.

La Madre Teresa sintió, después de una década de dudas, un período breve de fe renovada. En el momento de la muerte del papa Pío XII en otoño de 1958, orando por él en una misa de réquiem, dijo que había sido relevada de la «larga oscuridad: aquel sufrimiento extraño». Sin embargo, cinco semanas más tarde, admitió regresar a sus dificultades para creer. Escribió muchas cartas a sus confesores y superiores durante un período de 66 años. Si bien había pedido que las mismas fueran destruidas por temor a que la gente «vaya a pensar más en mí y menos en Jesús», fueron recopiladas en "Mother Teresa: Come Be My Light" (Ed. Doubleday). En una carta que se dio a conocer públicamente a un confidente espiritual, el reverendo Michael van der Peet, Teresa escribió: «Jesús tiene un amor muy especial para ti. [Pero] En cuanto a mí, el silencio y el vacío son tan grandes, que miro y no veo, escucho y no oigo, mi lengua se mueve [en la oración] pero no habla... Quiero que reces por mí, que yo le dejo tener una mano libre».

Muchos medios informativos se refirieron a los escritos de la Madre Teresa como una indicación de crisis de fe. Algunos de sus críticos, como Christopher Hitchens, tomaron sus escritos como una evidencia de que su imagen pública fue creada principalmente para publicidad a pesar de sus creencias y acciones personales. Sin embargo, otros, como Brian Kolodiejchuk, editor de "Come Be My Light", la compararon con el poeta místico del siglo XVI San Juan de la Cruz, quien acuñó el término de «noche oscura del alma» para describir a una etapa particular del crecimiento de algunos maestros espirituales. El Vaticano indicó que las cartas no detendrían su camino hacia la santidad. De hecho, Kolodiejchuk fue su postulador.

En su primera encíclica, "Deus caritas est", Benedicto XVI mencionó a Teresa de Calcuta en tres ocasiones y también utilizó su obra para referirse a uno de los principales puntos de la encíclica. «La Beata Teresa de Calcuta es un ejemplo evidente de que el tiempo dedicado a Dios en la oración no sólo deja de ser un obstáculo para la eficacia y la dedicación al amor al prójimo, sino que es en realidad una fuente inagotable para ello». La Madre Teresa especificó que «sólo por la oración mental y la lectura espiritual podemos cultivar el don de la oración».

Aunque no hubo una conexión directa entre la congregación de Teresa y las órdenes franciscanas, confesó ser admiradora de San Francisco de Asís. En consecuencia, la vida de Teresa de Calcuta y el carácter de la orden muestran cierta influencia de la espiritualidad franciscana. Las hermanas de las Misioneras de la Caridad recitan la oración de paz de San Francisco todas las mañanas durante la acción de gracias después de la Comunión, y muchos de los votos y el énfasis de su ministerio son similares. San Francisco también hizo hincapié en la pobreza, castidad, obediencia y sumisión a Cristo, dedicando gran parte de su vida al servicio de los pobres, especialmente a los leprosos en la zona donde vivía.

Tras su muerte, la Santa Sede consideró que se podría iniciar el proceso de beatificación, considerado como el tercero de los cuatro pasos para alcanzar la canonización, en donde el papa declara al beato digno de veneración universal, aunque para ello se deben comprobar dos milagros (uno más adicional al milagro con el que se le catalogó como beata). El milagro que requería su beatificación sucedió en 1998 cuando, de manera aparentemente inexplicable, Mónica Besra, una mujer que padecía un tumor maligno en el abdomen, fue sanada. Besra comentó que había sido acogida en Roma por las Misioneras de la Caridad tras haber sido desahuciada por los médicos. Una de las hermanas le colocó sobre el abdomen una imagen de la Virgen María, que había permanecido sobre la túnica de la Madre Teresa durante la celebración de los premios Nobel. La sanación de aquella mujer ocurrió el 5 de septiembre de 1998, exactamente un año después del deceso de la misionera. Distintos médicos indios, la Asociación de Ciencias y Racionalismo de la India e incluso el marido de la propia Besra pusieron en duda su curación milagrosa al asegurar que la enfermedad desapareció por los medicamentos que debió ingerir durante nueve meses. Por otra parte, otro medio científico cita la curación de Mónica Besra de su tumor como uno de los elementos principales en el proceso de canonización de Teresa de Calcuta.

El proceso de beatificación de la Madre comenzó dos años después de su muerte gracias a una dispensa papal que evitaba el transcurso de cinco años desde su deceso, tal como establece el Derecho Canónico. El Vaticano citó a Christopher Hitchens para exponer algún testimonio que pudiera comprometer y entorpecer el proceso de beatificación. «Fue hablando con ella cuando descubrí, y me aseguró, que no estaba trabajando para aliviar la pobreza», dijo Hitchens. «Ella trabajaba para ampliar la cantidad de católicos. Me dijo: "No soy una trabajadora social. No lo hago por eso. Lo hago por Cristo. Lo hago por la Iglesia"». La Congregación para las Causas de los Santos se encargó de investigar sus declaraciones pero fueron desestimadas posteriormente.

El 19 de octubre de 2003, ante la presencia de unas 300 000 personas en la Plaza de San Pedro, fue proclamada beata por el papa Juan Pablo II. A la celebración asistieron medio millar de las Misioneras de la Caridad, 150 cardenales y 400 obispos. El papa también declaró formalmente el 5 de septiembre como la festividad de la Madre Teresa.

El 18 de diciembre de 2015, el papa Francisco aprobó la canonización de Teresa de Calcuta luego de que la Congregación para las Causas de los Santos reconociera como «extraordinaria» la curación de un brasileño enfermo con múltiples tumores cerebrales e hidrocefalia obstructiva, que había sido sujeto a trasplante renal y terapia inmunosupresora en 2008 sin resultados. La Iglesia católica sostuvo que esa patología cerebral resolvió de forma instantánea, completa y permanente el 9 de diciembre de 2008, y que tal resolución fue declarada por unanimidad como «científicamente inexplicable» por parte de un colegio integrado por siete médicos. La Iglesia atribuyó el milagro a Teresa de Calcuta, fallecida 11 años antes, debido a que la esposa del enfermo dijo haberse encomendado a la Madre Teresa para que salvase a su marido. El acto oficial de canonización, previsto dentro del jubileo de la Misericordia, tuvo lugar en la Plaza de San Pedro en la Ciudad del Vaticano en la mañana del 4 de septiembre de 2016. Decenas de miles de personas se reunieron para la ceremonia, entre ellas 15 delegaciones gubernamentales oficiales y 1500 personas sin hogar de toda Italia. La ceremonia fue televisada en directo por el canal del Vaticano. Skopie, la ciudad natal de Madre Teresa, anunció una semana de celebración por su canonización y en la India, se celebró una misa especial en la sede principal de las Misioneras de la Caridad en Calcuta.

El 6 de septiembre de 2017, Teresa de Calcuta fue nombrada copatrona de la Arquidiócesis de Calcuta, junto con san Francisco Javier, que lo era desde 1986.

En 1984, la editorial estadounidense Marvel Comics publicó una historieta basada en su vida y su trabajo. Los autores fueron el guionista David Michelinie y los dibujantes John Tartaglione y Joe Sinnott.

En marzo de 1998, se aplicó una placa a su residencia natal, que dice: «Aquí está la vivienda en la que el 26 de agosto de 1910 nació Agnes Gonxha Bojaxhiu, la Madre Teresa». Ese mismo año Lion Communications (Polygram Records) comercializó un álbum musical, "Mother, We'll Miss You", a manera de tributo póstumo que incluye la participación de varios cantantes de diferentes países entre ellos José Feliciano, el cantante gospel Walt Whitman y el grupo Soul Children of Chicago. El mismo contó con la producción del cantante escocés Dave Kelly. Tras el lanzamiento del compilatorio, diversos periódicos estadounidenses, como "Boston Globe" y "Philadelphia Inquirer", publicaron artículos relacionados con la vida y obra de la Madre Teresa.

Fue homenajeada asimismo a través de museos, nombrándosela patrona de varias iglesias, y con varios monumentos y caminos. En 2002, se le puso su nombre al Aeropuerto Internacional de Albania, algo similar a lo ocurrido con una plaza en Tirana donde se halla un monumento dedicado a la misionera, con una de las principales calles de Pristina, capital de Kosovo, y con un hospital civil albanés, el Hospital de la Madre Teresa (lugar de fallecimiento de Leka de Albania).

El 30 de agosto de 2009, se nombró «Mother Teresa Way» a una calle del barrio de Bronx. Ese tramo forma parte de la avenida Lydig y se logró su designación luego de que la Sociedad Albanesa de Estados Unidos insistiera durante 16 años para que la ciudad accediera a dedicar una vía a la Madre Teresa. En Skopie, se inauguró un museo que cuenta con una variada cantidad de objetos y pertenencias de la religiosa. En una de sus salas se halla una réplica de su vivienda natal realizada por el artista Vojo Georgievski y también posee un parque conmemorativo con su nombre.

La Universidad de la Mujer Madre Teresa, en Kodaikanal, se estableció en 1984 como una universidad pública por la gobernación de Tamil Nadu. Varios tributos han sido publicados en periódicos de la India y revistas con autoría de su biógrafo, Navin Chawla.

Indian Railways introdujo un nuevo tren, «Mother Express», en alusión a la Madre Teresa, el 26 de agosto de 2010 para conmemorar el centenario de su nacimiento. El gobierno del estado de Tamil Nadu organizó celebraciones con ocasión de su centenario el 4 de diciembre de 2010 en Chennai, encabezadas por el jefe de Gobierno Muthuvel Karunanidhi.

En Argentina, se entrega desde 2000 el premio Madre Teresa con el objetivo de «promover valores en la sociedad generando en la comunidad, en especial entre los jóvenes y adolescentes, alicientes y modelos a seguir e imitar; para su crecimiento humano y espiritual».




• «La madre Teresa no era una Santa» Artículo crítico con su inminente proceso de santificación.


</doc>
<doc id="8204" url="https://es.wikipedia.org/wiki?curid=8204" title="Willy Brandt">
Willy Brandt

Herbert Ernst Karl Frahm, más conocido como Willy Brandt (Lübeck; 18 de diciembre de 1913-Unkel; 8 de octubre de 1992), fue un político socialdemócrata alemán que ocupó el cargo de canciller de Alemania Occidental entre 1969 y 1974.

Willy Brandt fue el "nom de guerre" que asumió tras ser víctima de la persecución política del régimen de la Alemania nazi.

Miembro de las Juventudes Socialistas desde 1930, militó en el ala izquierda de la organización. Al ser expulsada ésta del Partido Socialdemócrata de Alemania (SPD) en 1931, pasó a formar parte del Partido de los Trabajadores Socialistas de Alemania (SAP). Fue representante de este Partido en la Guerra Civil Española. Al llegar los nazis al poder en Alemania, se refugió en Noruega, donde se nacionalizó noruego y trabajó como periodista. Al producirse la invasión alemana durante la Segunda Guerra Mundial, se trasladó a Suecia, y al término del conflicto regresó a Alemania, recuperó la nacionalidad alemana e ingresó en el SPD.

Establecido en Berlín Occidental, fue alcalde de la ciudad desde 1957, por lo que tuvo que enfrentar la crisis que supuso la construcción del Muro de Berlín en 1961. Presidente del SPD a partir de 1964, evolucionó hacia posturas más centristas y en 1966 fue Vicecanciller y Ministro de Asuntos Exteriores en el gabinete de la «Gran Coalición». Finalmente, en 1969 fue nombrado canciller de la RFA.

Willy Brandt nació con el nombre de Herbert Ernst Karl Frahm en la ciudad hanseática de Lübeck. Su madre, Martha Frahm, era una madre soltera que trabajaba como cajera en una tienda de la cooperativa "Konsumverein". Su padre, John Möller, que era contable en Hamburgo, nunca llegó a conocer a Brandt. Como su madre trabajaba seis días a la semana, fue criado principalmente por el padrastro de su madre, Ludwig Frahm, y su segunda esposa, Dora.

Después de graduarse la universidad en 1932, trabajó en FH Bertling como corredor naviero y armador. Se alistó en las "Juventudes Socialistas" en 1929 y en el Partido Socialdemócrata (SPD) en 1930. Dejó este último para afiliarse a un partido todavía más izquierdista, el Partido de los Trabajadores (SAPD), que se alió con el Partido Obrero de Unificación Marxista en España y con el Partido Laborista Independiente en Gran Bretaña. En 1933, usando sus conexiones en los astilleros, abandonó Alemania y marchó a Noruega para escapar de la persecución nazi. Fue en esta época cuando adoptó el seudónimo de "Willy Brandt" para evitar ser detectado por los agentes nazis. En 1934, participó en la fundación de la Oficina Internacional de Organizaciones Juveniles Revolucionarias y fue elegido para ocupar la Secretaría.

Brandt estuvo en Alemania de septiembre a diciembre de 1936, tras adoptar la identidad de un estudiante noruego llamado Gunnar Gaasland. Más tarde continuó en Berlín como corresponsal de guerra y hablando alemán con acento noruego. El auténtico Gunnar Gaasland se había casado en 1936 con Gertrud Meyer, compañera de Brandt desde su juventud en Lübeck y que lo había acompañado a Noruega en 1933. Gracias al matrimonio con Gaasland, Gertrud, que hasta 1939 siguió viviendo con Brandt, pudo conseguir la nacionalidad noruega y evitar la deportación. En abril de 1937 Brandt viajó como representante del SAPD a España en plena Guerra Civil Española. Nada más llegar a Barcelona estalló una guerra interna entre las fuerzas de la República y los grupos anarquistas y libertarios, entre estos últimos el POUM con el que confraternizaba el partido de Brandt.

En 1938, el gobierno alemán le revocó la ciudadanía, por lo que solicitó la ciudadanía noruega. En 1940, fue arrestado en Noruega por las fuerzas de ocupación alemanas, pero no fue identificado porque llevaba un uniforme noruego. Tras su liberación, huyó a la neutral Suecia. En agosto de 1940, se naturalizó noruego, recibiendo su pasaporte de la Embajada de Noruega en Estocolmo, donde vivió hasta el final de la guerra. Willy Brandt dio una conferencia en Suecia el 1 de diciembre de 1940 en la universidad Bommersvik sobre los problemas experimentados por los socialdemócratas en la Alemania nazi y los países ocupados al inicio de la Segunda Guerra Mundial. En su exilio nórdico, Brandt aprendió noruego y sueco. Brandt hablaba noruego con fluidez, y mantuvo una estrecha relación con Noruega.

A finales de 1946, Brandt regresó a Berlín, trabajando para el gobierno noruego. En 1948, se afilió al Partido Socialdemócrata de Alemania (SPD) y volvió a ser un ciudadano alemán, y se aprobó finalmente su seudónimo de Willy Brandt como su nombre legal.

Desde el 3 de octubre de 1957 hasta 1966, Willy Brandt fue durante un período de creciente tensión en las relaciones Este-Oeste que llevó a la construcción del muro de Berlín. En el primer año de Brandt como alcalde, también desempeñó el cargo de Presidente del Bundesrat en Bonn. Brandt se manifestó en contra de la represión soviética de la Revolución Húngara de 1956 y contra la propuesta de Nikita Jrushchov de 1958 de que Berlín recibiera el estatus de "ciudad libre". Fue apoyado por la influyente editorial Axel Springer. En su tiempo como alcalde, recibió visitas notables de John F. Kennedy y Robert F. Kennedy para apoyar a Berlín Oeste en los momentos más difíciles.

Brandt fue elegido presidente del Partido Socialdemócrata en 1964, cargo que ocupó hasta 1987, más que cualquier otro presidente del Partido desde su fundación por August Bebel. Fue, así mismo, el candidato del Partido Socialdemócrata a la Cancillería en 1961, pero perdió ante el conservador Konrad Adenauer de la Unión Demócrata Cristiana de Alemania (CDU). En 1965, Brandt volvió a presentarse como candidato, nuevamente perdiendo ante el popular Ludwig Erhard. El gobierno de Erhard fue de corta duración. Sin embargo, se formó en 1966 una gran coalición entre el SPD y la CDU, con Brandt como ministro de Asuntos Exteriores y Vicecanciller.

En las elecciones de 1969, de nuevo con Brandt como el principal candidato, el Partido Socialdemócrata se hizo más fuerte, y después de tres semanas de negociaciones, el partido formó un gobierno de coalición con el pequeño Partido Democrático Liberal (FDP). Brandt fue elegido Canciller de la República Federal de Alemania.

Como canciller, Brandt desarrolló su "Neue Ostpolitik" (Nueva Política Oriental). Brandt fue activo en la creación de un grado de acercamiento a la República Democrática Alemana y también en mejorar las relaciones con la Unión Soviética, Polonia, Checoslovaquia y otros países del Bloque del Este. Un momento crucial fue en diciembre de 1970, cuando tuvo lugar la famosa Genuflexión de Varsovia en la que Brandt, al parecer espontáneamente, se arrodilló ante el monumento a las víctimas del Levantamiento del Gueto de Varsovia. El levantamiento se produjo durante la Ocupación de Polonia (1939-1945), y el monumento está dedicado a los fallecidos en el motín -sofocado por tropas alemanas- y a los residentes del gueto que fueron trasladados a campos de exterminio.

En 1971, Brandt recibió el Premio Nobel de la Paz por su labor en la mejora de las relaciones con Alemania Oriental, Polonia y la Unión Soviética.

Brandt negoció un tratado de paz entre la República Federal de Alemania y Polonia, además de los acuerdos sobre las fronteras entre los dos países, lo que significaba el verdadero final de la Segunda Guerra Mundial. Brandt negoció tratados paralelos y acuerdos entre la República Federal y Checoslovaquia.

En Alemania Occidental, la "Neue Ostpolitik" de Brandt fue muy polémica, dividiendo a la población en dos campos: uno, que abarcaba todos los partidos conservadores y, en particular, los antiguos residentes de la zona oriental de Alemania ("die Heimatvertriebenen") expulsados del este por la limpieza étnica de las potencias aliadas. Estos grupos manifestaron su fuerte oposición a la política de Brandt, calificándola de "ilegal" y de "alta traición". El otro campo que lo apoyó, alentó la "Neue Ostpolitik" como el "Wandel durch Annäherung" ("El cambio a través del acercamiento"), fomentando el cambio a través de una política de compromiso con los comunistas del bloque del este, en lugar de aislar diplomática y comercialmente a dichos países. Los partidarios de Brandt afirmaron que la política había ayudado a romper "el Bloque Oriental con mentalidad de asedio", y también contribuyó a aumentar su conciencia de las contradicciones de su marca de socialismo/comunismo, que, junto con otros eventos, finalmente llevó a la caída del comunismo europeo.

En la década de 1960, Alemania Occidental vivió un período de cambios que se vio acrecentado con los disturbios ocasionados por los estudiantes universitarios. En aquellos momentos, la RFA parecía ser un país estable y pacífico, contento con los resultados de su "milagro económico" y con un auge de la natalidad gracias a la generación del "baby-boom". No obstante, esta generación de posguerra no igualó a la de sus progenitores, que habían vivido de lleno la Segunda Guerra Mundial. Eran jóvenes abiertos, antibelicistas y con posiciones ideológicas que viraban hacia la izquierda, abrazando posturas como el maoísmo y alabando como héroes públicos a Hồ Chí Minh, Fidel Castro y el Che Guevara, mientras vivían en un estilo de vida más promiscuo. Los estudiantes y jóvenes aprendices podían darse el lujo de salir de las casas de sus padres, y la política de izquierda se consideraba elegante, así como tomar parte en manifestaciones políticas al estilo estadounidense en contra de las fuerzas militares estadounidenses en Vietnam del Sur.

El antecesor de Brandt como canciller, Kurt Georg Kiesinger, había sido miembro del partido nazi y parecía salvaguardar los intereses de la Alemania burguesa y conservadora. Brandt, después de haber luchado contra los nazis y de haberse enfrentado a la Alemania Oriental durante varias crisis al tiempo que era alcalde de Berlín oeste, se convirtió en una figura polémica, pero creíble, en varias facciones diferentes. Los dos estadistas habían llegado a sus propios términos con los estilos de vida del nuevo "baby-boom". Kiesinger les consideró "una multitud vergonzosa de pelo largo abandonados que necesitaba un baño y alguien para disciplinarlos". Por otra parte, Brandt necesitaba tiempo para ponerse en contacto y para ganar credibilidad entre la "Außerparlamentarische Opposition" (APO) (oposición extraparlamentaria). Los estudiantes cuestionaron la sociedad de la Alemania Occidental en general, buscaban reformas sociales, legales y políticas. Además, los disturbios llevaron a un renacimiento de los partidos de derecha.

Brandt, sin embargo, representaba una figura de cambio y siguió un curso de reformas sociales, legales y políticas. En 1969, Brandt obtuvo una pequeña mayoría, formando una coalición con el FDP. En su primer discurso ante el Bundestag como canciller, expuso su trayectoria política de las reformas y terminó el discurso con sus famosas palabras: ""Wir wollen mehr Demokratie wagen"" (literalmente: "aventuremos más democracia"). Este discurso fue popular tanto en el SPD como entre la mayoría de los estudiantes, que soñaban con un país que fuese más abierto y menos autoritaria que en los años posteriores al final de la guerra. Sin embargo, la "Neue Ostpolitik" de Brandt perdió para él una gran parte de los refugiados alemanes del Este, votantes que se habían manifestado significativamente en favor de los socialdemócratas en los años de posguerra.

La Ostpolitik de Brandt dio lugar a un colapso de la coalición de la estrecha mayoría de Brandt que había disfrutado el Bundestag. En octubre de 1970, los diputados del Partido Democrático Liberal Erich Mende, Heinz Starke y Siegfried Zoglmann se unieron a la CDU. El 23 de febrero de 1972, el diputado socialdemócrata Herbert Hupka, que también era líder de la "Bund der Vertriebenen" (Asociación de desplazados), se unió a la CDU en desacuerdo con los esfuerzos de reconciliación de Brandt hacia el este. El 23 de abril de 1972, Wilhelm Helms (FDP) abandonó la coalición. Políticos del FDP, como Knud von Kühlmann-Stumm y Gerhard Kienbaum, también declararon que votarían contra Brandt, por lo que había perdido la mayoría. El 24 de abril de 1972, se propuso un voto de censura que fue votada tres días más tarde. Si la moción hubiera prosperado, Rainer Barzel habría sustituido a Brandt como canciller. Para sorpresa de todos, el movimiento fracasó: Barzel sólo obtuvo 247 votos de un total de 260 votos; para la mayoría absoluta se requerían 249 votos. Asimismo hubo 10 votos en contra y 3 votos nulos. Mucho más tarde se reveló que dos miembros del Bundestag (Julius Steiner y Leo Wagner, tanto de la CDU y la CSU) habían sido sobornados por la Stasi de Alemania Oriental para que votasen a favor de Brandt.

Aunque Brandt se mantuvo como canciller, había perdido su mayoría. Sus posteriores iniciativas en el Parlamento, sobre todo presupuestarias, fracasaron. A causa de este estancamiento, el Bundestag fue disuelto y se convocaron nuevas elecciones. Durante la campaña de 1972, muchos artistas populares de Alemania Occidental, intelectuales, escritores, actores y profesores dieron su apoyo a Brandt y al partido socialdemócrata. Entre ellos se encontraban Günter Grass, Walter Jens e incluso el futbolista Paul Breitner. La "Ostpolitik" de Brandt, así como sus políticas internas reformistas, eran populares entre la generación joven, y llevó a que su partido SPD consiguiera su mejor resultado electoral federal a finales de 1972. El "Willy-Wahl" (elección de Willy) de Brandt fue el principio del fin, y el papel de Brandt en el gobierno comenzó a declinar.

Muchas de las reformas de Brandt toparon con la resistencia de los gobiernos de los estados federados (dominados por la CDU/CSU). El espíritu de optimismo reformista se vio interrumpido por la crisis del petróleo de 1973 y la gran huelga de los servicios públicos de 1974, que dio lugar a que los sindicatos, dirigidos por Heinz Kluncker, exigieran un importante incremento salarial; todo ello acabó por reducir el margen de maniobra de Brandt para hacer nuevas reformas. Para hacer frente a cualquier intento de simpatizar con el comunismo o de ser blandos con los extremistas de izquierda, Brandt implementó una legislación estricta que prohibía el empleo de personas "radicales" en el servicio público.

Alrededor de 1973, los organismos de seguridad de la Alemania Occidental recibieron información de que uno de los asistentes personales de Brandt, Günter Guillaume, era un espía de los servicios de inteligencia de la Alemania Oriental. A Brandt se le pidió que siguiese trabajando como de costumbre, y él accedió a hacerlo, incluso pasó unas vacaciones privadas con Guillaume. Guillaume fue detenido el 24 de abril de 1974, y muchos culparon a Brandt por tener un espía comunista en su círculo íntimo. Así, caído en desgracia, Brandt renunció a su cargo como Canciller el 6 de mayo de 1974. No obstante, Brandt se mantuvo en el Bundestag y como Presidente del SPD hasta 1987.

Este asunto de espionaje es reconocido por haber sido simplemente el detonante de la renuncia de Brandt, pero no la causa fundamental. Brandt se vio sacudido por escándalos sobre el adulterio de serie, y al parecer también luchó con el alcohol y la depresión. También fueron las consecuencias económicas sobre la República Federal de Alemania de la crisis del petróleo de 1973 que causó el estrés suficiente para acabar con Brandt como Canciller. Como el propio Brandt dijo más tarde: "Yo estaba agotado, por razones que no tenían nada que ver con el proceso sucede hoy en día." Parece que "el proceso" fue el desarrollo del escándalo de espionaje de Guillaume.

Guillaume era un agente de espionaje de la Alemania Oriental, que había sido supervisado por Markus Wolf, el jefe de la "Administración Principal de Inteligencia" del Ministerio de Seguridad del Estado de Alemania Oriental. Wolf declaró después de la reunificación que la renuncia de Brandt no había sido prevista, y que la colocación y el manejo de Guillaume había sido uno de los mayores errores de los servicios secretos de la Alemania Oriental. Brandt fue sucedido como el Canciller de Alemania por su compañero socialdemócrata, Helmut Schmidt. Para el resto de su vida, Brandt mantuvo la sospecha de que su colega socialdemócrata (y viejo rival) Herbert Wehner había sido uno de los responsables de la caída de Brandt. Sin embargo, son escasas las pruebas de esta sospecha.

Después de su mandato como Canciller, Brandt mantuvo su escaño en el Bundestag y siguió siendo presidente del Partido Social Demócrata hasta 1987, año en que renunció para convertirse en el presidente honorario del mismo. Brandt fue también miembro del Parlamento Europeo desde 1979 hasta 1983.

Durante dieciséis años, Brandt fue el presidente de la Internacional Socialista (1976-1992), período durante el cual el número de partidos miembros de la Internacional Socialista, en su mayoría europeos, creció hasta que hubo más de un centenar de agrupaciones socialistas, socialdemócratas y obreras en todo el mundo. Durante los primeros siete años, este crecimiento del número de miembros del SI fue motivado por los esfuerzos del Secretario General de la Internacional Socialista, el sueco Bernt Carlsson. Sin embargo, a principios de 1983, surgió una controversia acerca de lo que percibió Carlsson como un enfoque autoritario del presidente de la SI. Carlsson luego reprendió Brandt diciendo: "Esta es una Internacional Socialista, no una Internacional Alemana".

A continuación, en contra de algunas voces de oposición, Brandt decidió trasladar el próximo Congreso de la Internacional Socialista de Sídney, Australia, a Portugal. A raíz de este Congreso de la IS en abril de 1983, Brandt dio represalias en contra de Carlsson, lo que le obligó a dimitir de su cargo. Sin embargo, el Primer Ministro de Austria, Bruno Kreisky, argumentó a favor de Brandt: "Es una cuestión de si es mejor ser puro o ser más numerosos".

En octubre de 1979, Brandt se reunió con algunos disidentes de Alemania Oriental, entre ellos Rudolf Bahro, que se encontraban en la Alemania Federal tras ser puesto en libertad con motivo de una amnistía promovida por los treinta años de la fundación de la RDA. No obstante, tanto Bahro como otros disidentes políticos de Alemania del Este seguían sintiéndose inseguros por la grave amenaza que suponía la Stasi, los servicios de seguridad de Alemania Oriental. Este grupo de disidentes apoyó desde Bonn varios escritos y manifiestos en contra de la RDA y pidiendo un cambio en la gestión política, promoviendo nuevas leyes, en lo que se llamó "el cambio desde dentro". 

A finales de 1989, Brandt se convirtió en uno de los primeros líderes de izquierda en la Alemania Occidental a favor de una reunificación rápida y pública de Alemania, en lugar de promover algún tipo de federación de dos estados u otro tipo de arreglo provisional. Brandt fue testigo presencial de la caída, en noviembre de ese año, del Muro de Berlín, que dividía las dos partes de la capital alemana. Más tarde también estuvo presente en la ceremonia de la reunificación alemana el 3 de octubre de 1990 en Berlín, al lado del entonces Canciller Helmut Kohl, que tuvo lugar al pie del Palacio de Reichstag, a pocos metros de la Puerta de Brandeburgo, hoy símbolo de Berlín reunificada.

Una de las últimas apariciones públicas de Brandt fue en un vuelo a Bagdad, Irak, para liberar a rehenes occidentales en poder de Saddam Hussein, tras la invasión iraquí de Kuwait en 1990. Brandt logró la liberación de un gran número de ellos, y el 9 de noviembre de 1990 su avión aterrizó con 174 rehenes liberados a bordo en el aeropuerto de Fráncfort del Meno.

Desde 1941 hasta 1946 Brandt estuvo casado con Anna Carlotta Thorkildsen (hija de un padre noruego y una madre germano-estadounidense). Tuvieron una hija, Nina Brandt (nacida en 1940). Después de que Brandt y Thorkildsen se divorciaran en 1946, Brandt se casó con la noruega Rut Hansen en 1948, con la que tuvo tres hijos: Peter Brandt (nacido en 1948), Lars Brandt (nacido en 1951) y Matthias Brandt (nacido en 1961). Hoy en día Peter es historiador, Lars es artista y Matthias es actor. Después de 32 años de matrimonio, Willy Brandt y Rut Brandt Hansen se divorciaron en 1980. El 9 de diciembre de 1983, Brandt se casó en terceras nupcias con Brigitte Seebacher (nacida en 1946).

Rut Brandt Hansen vivió varios años más después de divorciarse de Willy Brandt, pero murió el 28 de julio de 2006 en Berlín.

En 2003, Matthias Brandt hizo el papel de Guillaume en la película de ""Im Schatten der Macht"" ("A la sombra del poder"), dirigida por el cineasta alemán Oliver Storz. Esta película trata sobre el asunto Guillaume y la renuncia de Willy Brandt a la cancillería. Matthias causó una pequeña controversia en Alemania cuando se anunció que iba a representar al hombre que traicionó a su padre y que le llevó a dimitir en 1974. A principios de ese año, cuando Brandt y Guillaume tomaron unas vacaciones juntos en Noruega, era Matthias, que tenía entonces 12 años de edad, quien fue el primero en descubrir que Guillaume y su esposa "estaban escribiendo cosas misteriosas en máquinas de escribir toda la noche".

A principios de 2006, Lars Brandt publicó una biografía de su padre llamada ""Andenken"" ("Recuerdo"). Este libro ha sido objeto de cierta controversia. Algunos lo llegaron a ver como un recuerdo amoroso de la relación padre-hijo, pero otros lo han etiquetado como una declaración brutal de un hijo que sigue pensando que nunca había tenido un padre que realmente lo amara.

Willy Brandt murió de cáncer de colon en su casa en Unkel, una ciudad a orillas del río Rin, el 8 de octubre de 1992, a los 78 años. Recibió un funeral de Estado. Está enterrado en el cementerio de Zehlendorf, en Berlín.

Cuando el SPD trasladó su sede de Bonn a Berlín a mediados de la década de 1990, la nueva sede fue nombrada como "Willy Brandt Haus" (Casa Willy Brandt). Uno de los edificios del Parlamento Europeo en Bruselas lleva su nombre desde 2008.

En 2009, la Universidad de Erfurt renombró a su escuela de postgrado de la administración pública como la "Escuela de Políticas Públicas Willy Brandt". Una escuela secundaria privada de lengua alemana en Varsovia, Polonia, también lleva su nombre.

El 11 de diciembre de 2009, el nombre de Willy Brandt se adjuntó al Aeropuerto Internacional Berlín-Brandeburgo.




</doc>
<doc id="8205" url="https://es.wikipedia.org/wiki?curid=8205" title="Jimmy Carter">
Jimmy Carter

James Earl Carter, Jr. (Plains, Georgia; 1 de octubre de 1924), más conocido como Jimmy Carter, es un político estadounidense del Partido Demócrata que fue el trigésimo noveno presidente de los Estados Unidos (1977-1981); antes había ejercido de gobernador del estado de Georgia (1971-1975) y de senador en la Asamblea General de Georgia (1962-1966). Carter fue galardonado con el en 2002, por sus esfuerzos «para encontrar soluciones pacíficas a los conflictos internacionales, impulsar la democracia y los derechos humanos y fomentar el desarrollo económico y social».

Su mandato como presidente de Estados Unidos, estuvo marcado por importantes éxitos en política exterior, como los tratados sobre el canal de Panamá, los acuerdos de paz de Camp David (tratado de paz entre Egipto e Israel), el tratado SALT II con la Unión Soviética y el establecimiento de relaciones diplomáticas con la República Popular China y vivió sus momentos más tensos con la crisis de los rehenes en Irán. En política interior, su gobierno creó los ministerios de Energía y Educación y reforzó la legislación sobre protección medioambiental.

Desde que abandonó la Casa Blanca, se ha dedicado a la mediación en conflictos internacionales y al apoyo de causas humanitarias. En 1982, fundó junto con su esposa Rosalynn, el Centro Carter, una organización no gubernamental que lucha por el avance de los derechos humanos, la mediación en conflictos internacionales y que ha estado presente como observador en distintos procesos electorales.

Carter nació el 1 de octubre de 1924 en Plains, un pequeño pueblo agrícola, cercano a Americus, en el estado de Georgia. Los antepasados de Carter procedían del sur de Inglaterra (su familia paterna llegó a las Colonias americanas en 1635), y han vivido en el Estado de Georgia durante varias generaciones. Carter ha documentado antepasados suyos que lucharon en la revolución estadounidense y su abuelo, L.B. Walker Carter (1832–1874), combatió en el ejército de los Estados Confederados durante la guerra de Secesión.

Su padre fue James Earl Carter, un acomodado empresario agrícola que cultivaba algodón y cacahuetes y que ejercía el papel tradicional de terrateniente del sur de Estados Unidos. Carter lo describe como un brillante granjero y un estricto segregacionista que trataba a los trabajadores negros con respeto y justicia. Su madre fue Bessie Lillian Gordy, una enfermera diplomada en la Wise Clinic en Plains que transmitió a su hijo su afición por la lectura. Jimmy era el mayor de los 4 hijos de la pareja. En su infancia, durante la Gran depresión, la familia se trasladó a vivir a una granja que su padre había comprado, situada en Archery cerca de Plains. Según narra Carter en su libro de memorias "An Hour Before Daylight: Memoirs of a Rural Boyhood", en la granja, a pesar de ser una de las familias más prósperas de la comunidad, carecían de electricidad y agua corriente. La mayoría de sus vecinos eran afroamericanos, aparceros o peones de la explotación de su padre, Carter estaba en contacto permanente con ellos, comía en sus hogares y, cuando sus padres se encontraban fuera, pasaba la noche en casa de Rachel y Jack Clark, empleados de la granja familiar. Este contacto interracial solo era posible en la granja puesto que el rígido código legal de segregación racial existente establecía la separación en la escuela, la iglesia y otros lugares públicos. En cierta ocasión, Carter fue al cine a la ciudad de Americus, con su mejor amigo A.D. Davis, para ello tuvieron que viajar en vagones separados en el tren, en compartimentos para "blancos" y para "personas de color". Al llegar a la ciudad tuvieron que caminar hasta el teatro juntos, pero de forma separada, también tuvieron que separarse para ver la película y de nuevo para volver hasta su casa. Carter afirma que "No recuerdo siquiera cuestionar la separación racial obligatoria, que aceptábamos como la respiración o como despertar en Archery cada mañana".

Desde una edad temprana, Carter demostró ser un alumno aplicado al que le gustaba mucho la lectura, estudió secundaria en la Plains High School. Al terminar el instituto, en 1941, tenía la intención de ingresar en la Academia Naval de Estados Unidos, pero para acceder a esta institución era necesario el respaldo de un senador o congresista de Estados Unidos, que su padre no consiguió hasta el verano de 1942. Mientras tanto, se matriculó en la Universidad Georgia Southwestern College en Americus y en el Instituto de Tecnología de Georgia para mejorar su preparación en Ciencias. En verano de 1943 ingresó en la Academia Naval de los Estados Unidos en Annapolis, donde se graduó como alférez en 1946, en el puesto 59 de los 820 alumnos de su promoción, obteniendo igualmente una licenciatura en Ciencias. En febrero de ese mismo año contrajo matrimonio con Rosalynn Smith, una de las mejores amigas de su hermana. Posteriormente, cursó física nuclear y tecnología de reactores en el Union College, aunque no llegó a completar esos estudios.

En 1948 accedió a la Escuela de Submarinos, posteriormente fue destinado al Pacífico y escogido por el almirante Hyman Rickover para participar en el entonces novedoso programa de submarinos nucleares. En 1953, a pesar de que su intención era proseguir su carrera en la marina, el fallecimiento de su padre le llevó a dimitir de sus cargos militares para asumir la dirección del negocio familiar de cultivo de cacahuates, en su pueblo natal. También desde muy joven mostró un profundo sentimiento cristiano, impartiendo clases en la escuela dominical. Durante su carrera política manifestó que Jesucristo había marcado su vida; de hecho, durante su mandato presidencial oraba varias veces al día.

Jimmy Carter comenzó su carrera política participando en las juntas locales que administraban algunas escuelas, hospitales y bibliotecas de su comarca. En 1961, fue elegido miembro del Senado de Georgia, permaneciendo en este cargo durante 2 legislaturas.

Su elección de 1961, la narró en su libro "Turning Point: A Candidate, a State, and a Nation Come of Age"; la elección estuvo envuelta en un ambiente de corrupción dirigido por Joe Hurst, sheriff del Condado de Quitman, durante las votaciones se produjeron graves abusos, como el voto de personas fallecidas y recuentos llenos de listas de personas que supuestamente habían acudido a votar en orden alfabético. En este entorno fraudulento, significó un desafío, ganar su elección. Esta convocatoria electoral supuso también el final del régimen de voto imperante en el Estado de Georgia, al declarar la Corte Suprema de los Estados Unidos inconstitucional (sentencia Gray v. Sanders), en 1963, el sistema de "votos por condados" en lugar de "por personas".
En 1964 fue reelegido para ejercer un segundo mandato de dos años. En 1966, Carter rechazó ser candidato a una tercera reelección, para iniciar su candidatura para Gobernador del Estado. Su puesto en el senado estatal fue ocupado por su primo hermano, Hugh Carter, elegido por el Partido Demócrata.

En 1966, cuando terminaba su legislatura en el senado de Georgia, se planteó presentarse como candidato a la Cámara de Representantes de los Estados Unidos, pero cuando su rival republicano, Howard Callaway, retiró su candidatura a esta cámara para postularse como candidato para gobernador del estado de Georgia, Carter que no quería ver a un gobernador republicano en su Estado, también se retiró de la carrera para el Congreso y se incorporó como candidato a gobernador. En las primarias demócratas, Carter fue el tercero más votado por detrás de Ellis Arnall y Lester Maddox. La participación de Carter fue importante y trascendente ya que forzó una Segunda vuelta electoral en la que venció Maddox, candidato partidario de la segregación racial y que había sido el segundo más votado en la primera vuelta. Durante este proceso, Carter se presentó como una alternativa moderada, tanto frente a Arnall, más liberal, como frente al más conservador Maddox. Aunque Carter fue derrotado, la fortaleza de su posición se contempló como un éxito para un senador del Estado poco conocido.
En las elecciones participaron tres candidatos, Maddox por el partido demócrata, Callaway por los republicanos y Arnal que concurrió fuera de las listas. Callaway fue el más votado, pero Maddox fue nombrado gobernador del Estado por la Asamblea General de Georgia.

Durante los siguientes 4 años, Carter volvió a su explotación agrícola, dedicándose a preparar y planificar cuidadosamente la siguiente campaña para gobernador de 1970, participando durante esos 4 años en más de 1.800 actos políticos por todo el estado de Georgia.

En las elecciones de 1970, realizó una ardua campaña populista para la primarias del partido demócrata contra del exgobernador Carl Sanders, etiquetando a su oponente como "Cufflinks Carl". Carter nunca fue un segregacionista y se negó a unirse al segregacionista "Consejo de Ciudadanos Blancos", lo que provocó el boicot a su empresa de cacahuetes. Su familia fue también una de las dos únicas que votaron a favor de admitir a los negros a la Plains Baptist Church.
Sin embargo según el historiador E. Stanly Godbold, durante esta campaña, pronunció las palabras que los segregacionistas querían escuchar, se opuso a los transporte de escolares destinados a favorecer la integración, se pronunció a favor de las escuelas privadas y manifestó su disposición a invitar al gobernador de Alabama, George Wallace, conocido por su postura contraria a la integración racial, para que pronunciara un discurso en su campaña. En el mismo sentido sus ayudantes de campaña hicieron pública una fotografía de su oponente junto a dos jugadores de baloncesto negros. Después de su estrecha victoria sobre Sanders en las primarias, Carter fue elegido Gobernador al derrotar al candidato republicano, Hal Suit.

Tras su elección como gobernador, Carter hizo una declaración en su discurso inaugural que desagradó profundamente a los segregacionistas: «Les digo con toda franqueza, que el tiempo de la discriminación racial ha terminado.
Ninguna persona sea pobre, campesina, débil, o negra debería tener que soportar la carga adicional de ser privado de la oportunidad de una educación, un puesto de trabajo o la simple justicia.»

El senador de Georgia, Leroy Johnson, uno de los primeros senadores negros de este estado, reflexionaba sobre esta declaración, afirmando: "Nos quedamos muy contentos. Muchos de los segregacionistas blancos estaban disgustados y estoy convencido de que aquellas personas que lo apoyaron, no lo habrían hecho si hubieran sabido que iba a pronunciar esa declaración."

Carter juró como 76º gobernador de Georgia, el 12 de enero de 1971 y ocupó este cargo hasta el 14 de enero de 1975. Su vicegobernador fue su predecesor en el cargo, Lester Maddox, con el que mantuvo constantes enfrentamientos públicos durante sus cuatro años de mandato.

Como ya se ha manifestado, Carter declaró ya en su discurso inaugural, como gobernador, que la época de la segregación racial había terminado, y que la discriminación no tenía cabida en el futuro del Estado y así fue el primer cargo público estatal del denominado "Sur Profundo" en pronunciarse públicamente en este sentido. Durante su mandato, Carter nombró bastantes afroamericanos para cargos públicos y fue denominado a menudo como uno de los "Gobernadores del Nuevo Sur". Mucho más moderado que sus predecesores, apoyó la lucha por la ampliación de los derechos de los afroamericanos y contra la segregación racial.

Carter se oponía personalmente al aborto, aunque apoyó su legalización después de la histórica sentencia de 1973 de la Corte Suprema de los Estados Unidos, a raíz del Caso Roe contra Wade, en la que se reconocía, aunque con limitaciones, el derecho a la interrupción voluntaria del embarazo. Posteriormente, como presidente de Estados Unidos no apoyó el incremento de fondos federales para la práctica de abortos y fue criticado por la Unión Estadounidense por las Libertades Civiles por no hacer lo suficiente para encontrar alternativas al aborto.

Carter mejoró la eficiencia del gobierno mediante la fusión de alrededor de 300 agencias estatales en 30. Uno de sus ayudantes recordaba así al gobernador Carter: ""estaba allí con nosotros, trabajando igual de duro, profundizando en cada pequeño problema. Era su programa y trabajó en él tan duro como cualquiera y el producto final era claramente suyo"".

También impulsó reformas durante su legislatura como, la prestación de ayuda estatal iguales a las escuelas de las zonas ricas y pobres de Georgia, la creación de centros comunitarios para niños con discapacidad mental y la ampliación de los programas educativos para presos. Carter se sintió especialmente orgulloso de un programa que presentó para el nombramiento de magistrados y funcionarios del gobierno estatal, basado en el mérito, en lugar de la influencia política.

En 1972, cuando, George McGovern, senador de los Estados Unidos por Dakota del Sur, se presentó a las primarias del partido demócrata para elegir el candidato a presidente de Estados Unidos, Carter convocó una conferencia de prensa en Atlanta para advertir que McGovern era inelegible, criticando que era demasiado liberal tanto en política exterior como en política nacional. Cuando la nominación de McGovern ya era inevitable, Carter presionó para intentar convertirse en su vicepresidente. Durante la Convención Nacional Demócrata de 1972, apoyó la candidatura del senador Henry M. Jackson, de Washington. Sin embargo, Carter recibió 30 votos, en la caótica votación para vicepresidente en la convención. McGovern ofreció el segundo lugar a Reubin Askew, uno de los "nuevos gobernadores del sur", que la rechazó.

Durante sus campañas presidenciales, manifestó siempre su posición contraria a la pena de muerte (en la misma postura se encontraban el candidato demócrata que le precedió, George McGovern, y los dos siguientes, Walter Mondale y Michael Dukakis). En la actualidad, Carter es conocido por su oposición frontal a esta pena en todas sus formas y en su discurso del Premio Nobel, instó a la "prohibición de la pena de muerte".

Después de que en 1972 la Corte Suprema de los Estados Unidos anulase la pena de muerte en el Estado de Georgia, Carter propuso sustituirla por la cadena perpetua en la legislación estatal (una opción que antes no existía). Cuando la asamblea de Georgia aprobó una nueva ley de pena de muerte, Carter, a pesar de expresar sus reservas sobre su constitucionalidad, firmó el 28 de marzo de 1973 la nueva normativa que autorizaba la pena de muerte en casos de asesinato, violación y otros delitos y ponía en práctica los procedimientos judiciales que se ajustaban a los requisitos constitucionales recientemente anunciados. En 1976, el Tribunal Supremo confirmó esta nueva legislación en el caso de asesinato Coker v. Georgia, en el que la Corte Suprema dictaminó que la pena de muerte era inconstitucional en su aplicación a los delitos de violación.

El 31 de marzo de 1971, el teniente del ejército norteamericano, William Calley, fue condenado a cadena perpetua por el asesinato de 22 civiles vietnamitas en la masacre de My Lai, en Vietnam. El presidente Nixon, tres días después de la sentencia, conmutó esta pena por un arresto domiciliario de carácter permanente. Jimmy Carter, disconforme con la condena de cadena perpetua, instituyó el "Día del luchador estadounidense" pidiendo a los georgianos que condujesen durante una semana sus automóviles con las luces encendidas en apoyo a Calley. El Gobernador de Indiana pidió también que todas las banderas del estado ondeasen a media asta en honor a Calley y los gobernadores de Utah y Mississippi también mostraron su desacuerdo con el veredicto.

Cuando Carter inició su candidatura a la presidencia de Estados Unidos en 1976, era considerado un político con poca experiencia y escaso reconocimiento a nivel nacional, solo era reconocido por el 2% de los votantes, y con pocas posibilidades contra políticos más conocidos a nivel nacional. Cuando comunicó a su familia su intención de postularse a la presidencia de Estados Unidos, su madre le preguntó: "¿Presidente de qué?" Sin embargo, durante 1976, el escándalo Watergate seguía todavía fresco en la mente de los votantes y su posición como un político sin gran experiencia y ajeno a la politiquería de Washington D.C., se convirtió en un factor atractivo para los votantes, la pieza central de su plataforma de campaña fue la reorganización del gobierno.

Carter pronto se convirtió en el favorito al ganar el caucus de Iowa y las primarias de Nuevo Hampshire. Utilizó una estrategia de dos frentes: en el sur, en el que la mayoría había aceptado tácitamente a George Wallace de Alabama, Carter se presentó como un hijo predilecto de carácter moderado. Cuando Wallace demostró ser una fuerza agotada, Carter barrió en la región. En el Norte, donde Carter tenía pocas posibilidades de obtener grandes mayorías, hizo un llamamiento en gran medida a los votantes conservadores cristianos y a la población rural. Ganó en varios estados del Norte, construyendo el mayor bloque. La estrategia de Carter consistía en alcanzar una región antes que los otros candidatos pudiesen extender su influencia allí, viajó más de 50.000 kilómetros, visitó 37 estados y pronunció más de 200 discursos antes de que otros candidatos hubieran anunciado que estaban en la carrera por la presidencia. Rechazó, desde un principio, presentarse como un candidato de carácter regional, demostrando ser el único demócrata con una verdadera estrategia nacional y así finalmente logró la candidatura de su partido.

Su avance fue lento, según una encuesta de Gallup, todavía el 26 de enero de 1976, Carter era la primera opción de solo un cuatro por ciento de los votantes demócratas. Sin embargo, a mediados de marzo, según Shoup, Carter no solo iba muy por delante de los otros contendientes demócratas, sino que también marchaba por delante del presidente Ford por un pequeño porcentaje.

Eligió al senador Walter Mondale como candidato a la vicepresidencia. Atacó la falta de transparencia de la política de Washington en sus discursos, y ofreció un bálsamo de carácter religioso para sanar las heridas de la nación.

En su campaña electoral, Carter apeló a vagos principios morales, criticó la burocracia de Washington y prometió sinceridad y honradez. Planteó su campaña como un "outsider", dirigida a los electores que estaban hartos de políticos profesionales y de soluciones convencionales. Inició la campaña presidencial con una ventaja considerable sobre Ford, que fue reduciéndose en el transcurso de la misma, y que finalmente le permitió a Carter la victoria el 2 de noviembre de 1976 por un margen estrecho. Carter ganó el voto popular por un 50,1 % contra 48,0 % de Ford y recibió 297 votos electorales contra 240 de Ford, convirtiéndose en el primer presidente del denominado Sur Profundo desde la elección de Zachary Taylor en 1848.

Su comportamiento público desafió las normas establecidas, durante la campaña, a pesar de su proclamada condición de devoto cristiano y catequista, concedió una entrevista a Robert Scheer para la revista Playboy, en la que reconocía que "He mirado a muchas mujeres con lujuria y he cometido adulterio muchas veces en el fondo de mi corazón". La entrevista llegó a los quioscos un par de semanas antes de las elecciones.

Jimmy Carter fue el trigésimo noveno presidente de los Estados Unidos desde 1977 hasta 1981. Carter se destacó por un estilo relativamente heterodoxo, que no encajó en el establishment de Washington, ni contó con un apoyo sólido de su partido, y por sus originales opiniones y juicios, sin tener un programa demasiado definido. Su administración trató de hacer un gobierno "competente y compasivo", pero se encontró con una grave crisis económica, que dificultó el logro de sus objetivos, caracterizada por la subida de los precios de la energía y la estanflación. Al final de su periodo de gobierno, Carter había logrado reducir sustancialmente el desempleo y el déficit público, pero no fue capaz de acabar por completo con la recesión. Carter creó los departamentos de educación y de Energía, estableció una política energética nacional y reformó la seguridad social. En asuntos exteriores, Carter inició los Acuerdos de Camp David, los tratados del Canal de Panamá y la segunda ronda de los Acuerdos SALT. A lo largo de su trayectoria como presidente, Carter subrayó firmemente los derechos humanos. Devolvió la Zona del Canal de Panamá a Panamá, enfrentándose a las críticas en su país por su decisión, que fue vista como otra señal de debilidad de Estados Unidos y de su hábito de dar marcha atrás cuando ante la confrontación. El último año de su mandato presidencial estuvo marcado por varias crisis importantes, como la toma en 1979 de la embajada estadounidense en Irán y retención de rehenes por estudiantes iraníes, el intento sin éxito de rescate de los rehenes, una grave escasez de combustible y el comienzo de la Guerra de Afganistán.

En su discurso de toma de posesión pronunció:

Carter había hecho campaña con la promesa de eliminar los trapicheos de la denominada ""presidencia imperial"" que imperaba con Richard Nixon y comenzó su presidencia, de acuerdo con esa promesa, el día de su toma de posesión; al ir caminando por la Avenida Pennsylvania, desde el Capitolio hasta la Casa Blanca, en su desfile inaugural, rompiendo así con el protocolo y la historia reciente. Sus primeros pasos en la Casa Blanca fueron más lejos en esta dirección, reduciendo la plantilla de asesores en un tercio, suprimiendo los conductores para los miembros del gabinete y poniendo a la venta el yate presidencial, el USS Sequoia.

En el primer día de Carter en el cargo, el 20 de enero de 1977, cumplió una promesa de campaña mediante la publicación de un Decreto Ley que declaraba una amnistía incondicional para los insumisos de la Guerra de Vietnam.

Bajo la tutela de Carter, se aprobó la Ley de Desregulación de Aerolíneas de 1978, que eliminó la Junta Aeronáutica Civil, también impulsó la desregulación del transporte por carretera, ferrocarril, comunicaciones, finanzas e industrias.

Entre los presidentes que han servido al menos un periodo completo de presidencia, Carter es el único que no realizó ningún nombramiento en la Corte Suprema.

Carter fue el primer presidente que se propuso abordar el tema de los derechos de los homosexuales. Se opuso a la Iniciativa Briggs, un proyecto de ley de California que prohibía a los homosexuales y a los defensores de los derechos de los homosexuales ser maestros de escuelas públicas. La administración de Carter fue la primera en reunirse con un grupo de activistas de derechos de los homosexuales y en los últimos años actuó en favor de las uniones civiles y puso fin a la prohibición de homosexuales en el ejército. en este sentido declaró que ""se opone a todas las formas de la discriminación por motivos de orientación sexual y cree que debe haber igualdad de protección bajo la ley para las personas que difieren en la orientación sexual"".

A pesar de pedir una reforma del sistema fiscal durante su campaña presidencial, cuando llegó al poder no hizo gran cosa para cambiarlo.

Durante todos los años de la presidencia se produjo un déficit en el presupuesto del gobierno federal, aunque el porcentaje de deuda sobre el PIB, decreció ligeramente. Su Ley de Conservación de Intereses Nacionales de las Tierras de Alaska convirtió 103 millones de acres (417.000 km²) en Parque nacional en Alaska.

Carter realizó una exitosa campaña electoral definiéndose como un "extraño en Washington", en la que criticaba al presidente Gerald Ford y al Congreso de los Estados Unidos, controlado por los demócratas. Como presidente, continuó con esta línea, su negativa a jugar con las "reglas de Washington" contribuyó a una difícil relación de la administración Carter con el Congreso. Hamilton Jordan y Frank Moore, en particular, se enfrentaron desde el principio, con los líderes demócratas, como el Portavoz de la Cámara de Representantes, Tip O'Neill. Las relaciones con el Capitolio se agriaron por llamadas telefónicas no devueltas, insultos (tanto reales como imaginarios) y una falta de voluntad para intercambiar favores políticos y debilitó la capacidad del presidente para impulsar su ambiciosa agenda.

Durante los primeros 100 días de su presidencia, Carter remitió una carta al Congreso, proponiendo el rechazo de varios proyectos. Entre los que manifestaron su oposición a esa propuesta, se encontraba el senador Russell B. Long, un poderoso demócrata del Comité de Finanzas del Senado. El plan de Carter fue rechazado, lo que produjo un sentimiento de amargura en la administración Carter.

El rechazo abrió una brecha entre la Casa Blanca y el Congreso, Carter manifestó que la oposición más intensa y creciente a sus políticas provenían del ala liberal del propio Partido Demócrata, que atribuyó a la ambición de Ted Kennedy para reemplazarlo como presidente.

Pocos meses después de iniciado su mandato, y pensando que tenía el apoyo de cerca de 74 congresistas, Carter publicó una "lista negra" de 19 proyectos que, según Carter, suponían un "pork barrel" de gasto público, manifestando que vetaría cualquier iniciativa legislativa que incluyera cualquier proyecto de esta lista.

Esta lista se encontró con la oposición del líder del partido demócrata. Carter había incluido un proyecto de ley de ríos y puertos como innecesario y el portavoz de la Cámara de Representantes, Tip O'Neill, pensó que era desaconsejable que el presidente interfiriese en asuntos que tradicionalmente habían formado parte de la esfera de competencias del Congreso. Tras estos hechos, Carter quedó aún más debilitado y tuvo que firmar un proyecto de ley que contenía proyectos de su lista negra.

Más tarde, el Congreso rechazó aprobar las principales disposiciones de su ley de protección de los consumidores y su paquete de reforma laboral y Carter vetó un paquete de obras públicas calificándolas de "inflacionarias", puesto que contenía lo que él consideraba gastos innecesarios. Los líderes del Congreso percibieron que el apoyo público a la iniciativa legislativa de Carter era débil, y se aprovecharon de ella. Después de destripar el proyecto de ley de protección al consumidor, transformaron su plan de impuestos en nada más que gastos de especial interés, tras lo cual, Carter se refirió al Comité de impuestos del Congreso como "manada de lobos".

En 1973, durante la administración de Nixon, la Organización de Países Exportadores de Petróleo (OPEP) redujo los suministros de petróleo disponible en el mercado mundial, en parte debido a la depreciación del dólar causada por la salida del patrón oro de Nixon y en parte como reacción contra Estados Unidos, por el envío de armas a Israel durante la Guerra de Yom Kippur. Esto provocó la crisis del petróleo de 1973 que supuso una subida brusca de los precios del petróleo que empujó la inflación y desaceleró el crecimiento. El gobierno de EE.UU., tras el anuncio, impuso controles de precios en la gasolina y en el petróleo, que provocaron escasez y largas colas en las gasolineras. Las colas se evitaron al suprimir los controles de precios en la gasolina, estos controles del petróleo se mantuvieron hasta la presidencia de Reagan. Cuando en 1977 Carter llegó a la Casa Blanca, dijo a los estadounidenses que la crisis energética era "un peligro claro y presente para la nación" y "el equivalente moral de la guerra" y diseñó un plan para intentar hacer frente al problema y manifestó que la oferta mundial de petróleo probablemente solo podría cubrir la demanda estadounidense durante seis u ocho años más.

En 1977, Carter convenció a los demócratas en el Congreso para crear el Departamento de Energía de los Estados Unidos con el objetivo de promover el ahorro de energía. Carter estableció controles de precio al petróleo y al gas natural e instaló paneles solares para calentar agua caliente sanitaria, en la Casa Blanca, y habilitó una estufa de leña en su vivienda. Ordenó a la Administración de Servicios Generales cortar el agua caliente en algunas instalaciones federales, y pidió que los adornos públicos navideños permaneciesen sin luz en las navidades de 1979 y 1980. Se establecieron a nivel nacional, controles en los termostatos de los edificios gubernamentales y comerciales para que no sobrepasasen temperaturas en el invierno por encima de 18,33 °C ni disminuyesen en el verano por debajo de 25 °C.

Como reacción a la crisis energética y las creciente preocupación sobre la contaminación del aire, Carter también firmó la Ley Nacional de Energía y la Ley Política de Regulación de los Servicios Públicos. El propósito de estas leyes fue estimular la conservación energética y el desarrollo de los recursos energéticos nacionales, incluidos los renovables, como la energía solar y la eólica.

Sin embargo, durante la crisis de 1979, Carter reintegró algunos controles de precios en la gasolina, lo que volvió a ocasionar colas en las gasolineras. Durante su discurso del "malestar", anunció una liberalización gradual de los controles de precios, junto con la imposición de un "Impuesto sobre beneficios excepcionales" para financiar el iniciativas de eficiencia energética. El impuesto se instauró en 1980 gravando la producción nacional de petróleo, el impuesto se derogó en 1988, cuando los precios se derrumbaron, haciendo posible la supresión del impuesto. Este tributo no era un gravamen sobre los beneficios, sino un impuesto especial sobre la diferencia entre un " precio base y el precio de mercado.

La historia económica de la Administración Carter se puede dividir en dos períodos más o menos iguales. Los dos primeros años fueron una época de continua recuperación de la grave recesión de 1973-1975, que había dejado la inversión en capital fijo en su nivel más bajo desde la recesión de 1970 y el desempleo en el 9%. Los otros dos años estuvieron marcados por una inflación de dos dígitos con unos tipos de interés muy altos, la escasez de petróleo y el bajo crecimiento económico. La economía del país creció a un promedio de 3,4% durante la administración Carter (a la par con el promedio histórico). Sin embargo, cada uno de estos períodos de dos años, difieren radicalmente.

La economía de Estados Unidos, que había crecido un 5% en 1976, siguió a un ritmo similar durante 1977 y 1978. El desempleo disminuyó desde el 7,5% en enero de 1977 al 5,6% en mayo de 1979, con más de 9 millones de nuevos empleos netos creados durante ese íntervalo y el ingreso familiar per cápita creció un 5% entre 1976 y 1978. La recuperación de la inversión empresarial en evidencia durante el año 1976 se fortaleció también. La inversión privada fija (maquinaria y construcción) creció un 30% de 1976 a 1979, las ventas de viviendas y la construcción creció de igual manera en 1978 y la producción industrial, la producción de vehículos y las ventas lo hicieron en casi un 15%, con la excepción de la viviendas nuevas iniciadas, que se mantuvo ligeramente por debajo de su pico de 1972, cada uno de estos puntos de referencia alcanzaron niveles récord en 1978 o 1979.

La crisis energética de 1979 terminó este período de crecimiento, la inflación y las tasas de interés permanecieron altas, mientras que el crecimiento económico, la creación de empleo y la confianza de los consumidores se redujo drásticamente. La política monetaria relativamente flexible adoptada por el presidente de la Reserva Federal, William G. Miller, había contribuido a la generación de una inflación algo más elevada, con aumentos del 5,8% en 1976 al 7,7% en 1978. La brusca y repentina subida de los precios del crudo, por parte de la OPEP, condujo a la inflación a alcanzar niveles de dos dígitos, con un promedio del 11,3% en 1979 y el 13,5% en 1980. La repentina escasez de gasolina al comienzo de la temporada de vacaciones de 1979, exacerbó el problema, y vendría a simbolizar la crisis entre el público en general, la escasez, originada en el cierre de las instalaciones de refinamiento de Amerada Hess, dio lugar a una demanda del gobierno federal contra la empresa.

Carter, como su antecesor el presidente Ford, pidió al Congreso la imposición de controles en los precios de la energía, productos médicos y precios al consumidor, pero no pudo lograr la aprobación de estas medidas debido a la fuerte oposición del Congreso, pero utilizando una ley de conservación y política energética, aprobada por el congreso durante la presidencia de Gerald Ford, que daba a los presidentes la autoridad para desregular los precios del petróleo en el mercado norteamericano, consiguió fomentar la producción y el ahorro de petróleo. Las importaciones de este producto, que habían alcanzado un récord de 2.400 millones de barriles en 1977 (el cincuenta por ciento del suministro norteamericano), se redujeron a la mitad entre 1979 y 1983.

En 1987 el periodista William Greider escribió "Los secretos del Templo", un libro publicado poco después del desplome de la bolsa, en el que manifiesta que Volcker y sus protegidos posteriores han dominado la Reserva Federal posteriormente por lo menos durante varias décadas a través de varias administraciones de EE.UU. Durante la administración de Carter, la economía sufrió una inflación de dos dígitos, junto con tasas de interés muy altas, la escasez de petróleo, el alto desempleo y el crecimiento económico lento. El crecimiento de la productividad en los Estados Unidos se había reducido a una tasa media anual del 1%, frente al 3,2% de la década de 1960. Hubo también un creciente déficit del presupuesto federal, que aumentó a $ 66.000 millones. La década de 1970 se describe como un período de estanflación, así como mayores tasas de interés. La inflación de precios (un aumento del nivel general de precios) creó incertidumbre en la presupuestación y planificación y provocó huelgas por aumentos de sueldo más probable. Carter, como Nixon, pidió al Congreso que se impusiesen controles de precios sobre la energía, la medicina y los precios al consumidor, pero el Congreso no estaba de acuerdo.

A raíz de una reestructuración del gabinete en el que Carter solicitó la dimisión de varios miembros de su gabinete, Carter nombró a G. William Miller como Secretario del Tesoro, Miller había estado sirviendo como Presidente de la Reserva Federal. Para reemplazarlo y con el fin de calmar los mercados, Carter nombró a Paul Volcker como presidente de la Reserva Federal. Volcker llevó a cabo una política monetaria restrictiva para bajar la inflación. Volcker y Carter tuvieron éxito, pero solo tras pasar primero por una fase recesiva durante la que la economía se desaceleró y creció el desempleo. El alivio de la inflación vio sus resultados durante el primer mandato de Ronald Reagan, quien volvió a nombrar a Volcker como presidente de la Reserva.

Bajo la dirección de Volcker, la Reserva Federal elevó la tasa de descuento desde el 10%, que estaban cuando Volcker asumió la presidencia en agosto de 1979 hasta el 12%, en un plazo de dos meses. La tasa preferencial de interés alcanzó el 21,5% en diciembre de 1980, la tasa más alta en la historia de Estados Unidos. Carter aprobó mediante decreto ley un programa de medidas de austeridad que trató de justificar porque la inflación había alcanzado "un estado de crisis", la inflación y el tipo de interés a corto plazo alcanzaron el dieciocho por ciento en febrero y marzo de 1980. Las inversiones en renta fija (bonos, tanto en poder de Wall Street y las pensiones pagadas a los jubilados) eran cada vez de menos valor. Las altas tasas de interés conduciría a una fuerte recesión en la década de 1980, que coincidió con la campaña de Carter para la reelección.

En 1979, cuando se inició la crisis del petróleo, Carter estaba planeando dar su quinto mayor discurso sobre la energía, sin embargo, sintió que el pueblo estadounidense ya no le escuchaba. Carter se retiró a la residencia presidencial de Camp David. Durante más de una semana, un velo de secreto envolvió sus actuaciones, convocó en su residencia a decenas de destacados líderes del Partido Demócrata, miembros del Congreso, gobernadores, dirigentes sindicales, académicos y miembros del clero. Su analista, Pat Caddell, le dijo que el pueblo de los Estados Unidos se enfrentaba a una crisis de confianza motivada por los asesinatos de John F. Kennedy, Robert F. Kennedy y Martin Luther King, Jr; la guerra de Vietnam y el Escándalo Watergate. El 15 de julio de 1979, Carter dio un discurso televisado a nivel nacional en el que identificó lo que él creía que era una "crisis de confianza" entre el pueblo estadounidense. Esto llegó a ser conocido como su "discurso del malestar", aunque nunca apareció esta palabra en el discurso:

El discurso fue escrito por Hendrik Hertzberg y Gordon Stewart. Aunque se ha dicho a menudo que fue mal recibido, The New York Times publicó el siguiente titular una semana después "El discurso eleva la valoración de Carter hasta el 37%, el público se siente de acuerdo con la crisis de confianza, tocada la fibra sensible.

La posterior pérdida de la reelección de Carter provocó que otros políticos descartaran solicitar el ahorro de energía a los estadounidenses de una manera similar. Tres días después del discurso, Carter pidió la dimisión de todos los miembros de su gabinete, y en última instancia aceptó la de los cinco que se habían enfrentado con la Casa Blanca al máximo, incluyendo el secretario de Energía, James Schlesinger, y el Jefe de Salud, Educación y Bienestar, Joseph A. Califano, conocido como partidario del senador Ted Kennedy. Carter admitió más tarde en sus memorias que simplemente debería haber pedido la dimisión de solo los cinco miembros que renunciaron. En 2008, una información de Noticias de los EE.UU. y World Report indicaron:

«After campaigning that he would never appoint a Chief of Staff, Carter appointed Jordan as a new White House Chief of Staff. Many in the administration chafed when Jordan circulated a "questionnaire" that read more like a loyalty oath. "I think the idea was that they were going to firm up the administration, show that there was real change by these personnel changes, and move on," remembers Mondale. "But the message the American people got was that we were falling apart." Carter later admitted in his memoirs that he should simply have asked only those five members for their resignations. In 2008, a U.S. News and World Report piece stated»:

«Después de una campaña [en la que prometió] que no volvería a nombrar a un jefe de personal, Carter nombró a Jordan jefe de personal de la Casa Blanca. Muchos en la administración se sintieron irritados cuando Jordan hizo circular un "cuestionario" que se parecía más a un juramento de lealtad. "Creo que la idea era que iban a concretar el gobierno, mostrando que hubo un cambio real de estos cambios de personal, y seguir adelante", recuerda Mondale. "Pero el mensaje que el pueblo estadounidense recibió fue que se estaban viniendo abajo". Carter admitió más tarde en sus memorias que sólo debería haber pedido la dimisión de esos cinco miembros. En 2008, una pieza de Noticias EE.UU. y World Report declaró»:

Durante su primer mes en el cargo, Carter disminuyó el presupuesto de defensa en seis mil millones de dólares. Uno de sus primeros actos fue ordenar la retirada unilateral de todas las armas nucleares de Corea del Sur y anunciar su intención de reducir el número de tropas estadounidenses estacionadas en ese país. Algunos militares criticaron esta decisión en conversaciones privadas y en testimonios ante comités del Congreso, en 1977, el general John K. Singlaub, jefe del Estado Mayor de las fuerzas de EE.UU. en Corea del Sur, criticó públicamente la decisión de Carter de disminuir las tropas destinadas allí. El 21 de marzo de 1977, Carter lo relevó de su cargo, manifestando que sus opiniones manifestadas públicamente, resultaban "incompatibles con la política anunciada de seguridad nacional".

Carter tenía previsto la retirada en 1982 de todas las tropas, excepto 14.000 soldados de la Fuerza Aérea de EE.UU. y especialistas en logística, pero, en 1978, después de haber recortado solo 3.600 soldados, se vio obligado a abandonar el proyecto por la presión del Congreso y las oposición de sus generales.

El secretario de Estado del gabinete de Carter, Cyrus Vance, y el asesor de Seguridad Nacional, Zbigniew Brzezinski prestaron mucha atención al conflicto árabe-israelí. Las comunicaciones diplomáticas entre Israel y Egipto aumentaron significativamente después de la Guerra de Yom Kippur de 1973 y la administración de Carter creía que era el momento adecuado para una solución global del conflicto.

A mediados de 1978, Carter estaba muy preocupado, puesto que había expirado solo unos meses antes el Tratado de Separación entre Egipto e Israel. Carter decidió destinar un enviado especial a Oriente Medio. El embajador de Estados Unidos iba y venía entre El Cairo (Egipto) y Tel Aviv intentando reducir las discrepancias entre los dos países. Se sugirió entonces que los cancilleres se reunieran en el castillo de Leeds (Inglaterra) para discutir las posibilidades de paz. Trataron de llegar a un acuerdo, pero los ministros de exteriores no lo lograron. Posteriormente se alcanzaron los Acuerdos de Camp David de 1978, uno de los logros más importantes de Carter durante su presidencia.

Los acuerdos fueron un convenio de paz entre Israel y Egipto negociado por Carter, que culminó las negociaciones anteriores, realizadas en Oriente Medio. En estas negociaciones el rey Hasan II de Marruecos actuó como negociador entre los intereses árabes en Israel, y Nicolae Ceausescu de Rumania actuaba como intermediario entre Israel y la Organización para la Liberación de Palestina. Cuando las negociaciones iniciales finalizaron, el presidente egipcio Anwar Sadat se acercó a Carter para pedirle ayuda. Carter invitó al primer ministro israelí Menájem Beguin y Anwar el-Sadat a Camp David para continuar las negociaciones. Llegaron el 8 de agosto de 1978, ninguno de los líderes se habían encontrado desde la reunión de Viena. El presidente Carter actuó como mediador entre los dos líderes y habló con cada uno por separado para intentar alcanzar un acuerdo. Transcurrido un mes sin llegar a ninguna resolución, el presidente Carter decidió reunirse con ambos a lo largo de un viaje a Gettysburg, (Pensilvania) para romper el punto muerto. Allí les mostró el terreno donde se había desarrollado una batalla de la Guerra Civil Estadounidense, les explicó la historia de la batalla e hizo hincapié en lo importante que era alcanzar la paz con el fin de llevar la prosperidad al pueblo. Una lección que contribuyó a que cuando Beguin y Sadat regresaron a Camp David, comprendieran que debían firmar algún tipo de acuerdo.

El 12 de septiembre de 1978, el presidente Carter sugirió que se dividiesen las negociaciones sobre el Tratado de Paz en dos marcos: El marco 1 abordaría Cisjordania y la Franja de Gaza, mientras que el marco 2 se ocuparía del Sinaí.


Se preveía que después de estos pasos, el Estado de Palestina podría ser negociado.

El marco 1 no fue muy bien recibido por palestinos y jordanos que se opusieron al hecho de que Beguin y Sadat tomaron decisiones sobre su destino final, sin consultar con ellos o sus dirigentes.



El presidente Carter admitió que a pesar de todo "todavía existían grandes dificultades y muchas cuestiones difíciles de resolver.".

La reacción a esta propuesta en el mundo árabe fue muy negativa. En noviembre de 1978, se celebró una reunión de emergencia convocada por la Liga Árabe en Damasco. Una vez más, Egipto fue el tema principal de la reunión, y se condenó la propuesta de tratado que Egipto iba a firmar. Sadat fue también atacado por la prensa árabe por romper filas con la Liga Árabe y haber traicionado al mundo árabe. Las discusiones relativas al futuro tratado de paz se llevaron a cabo en ambos países. Israel insistió durante las negociaciones en que el tratado con Egipto debía sustituir todos los demás tratados de Egipto, incluidos los firmados con la Liga Árabe y los países árabes. Israel también quería el acceso al petróleo descubierto en la región del Sinaí. El presidente Carter intervino e informó a los israelíes que Estados Unidos proporcionaría a Israel el suministro de petróleo que necesitaba para los próximos 15 años, si Egipto decidía no proporcionárselo a Israel.

El parlamento israelí aprobó el tratado con una cómoda mayoría. Por otra parte, el gobierno egipcio estaba discutiendo acerca de diferentes cuestiones. No les gustaba el hecho de que el tratado propuesto sustituiría a todos los demás tratados. Además los egipcios se sentían decepcionados porque no habían logrado vincular la cuestión del Sinaí al problema de Palestina.

El 26 de marzo de 1979, Egipto e Israel firmaron el tratado de paz en Washington D.C., el papel de Carter fue esencial. Aaron David Miller entrevistó a muchos funcionarios en su libro "The Much Too Promised Land" (2008) y concluyó lo siguiente: "No importa a quien se le pregunte, estadounidenses, egipcios, o israelíes, la mayoría están de acuerdo: sin Carter no habría existido ningún tratado de paz."

Carter inicialmente se apartó de la política establecida de contención hacia la Unión Soviética. Promovió una política exterior que situó los derechos humanos entre sus prioridades, lo cual supuso una ruptura con la actitud de sus predecesores que no prestaban atención al incumplimiento de los derechos humanos que habían cometido los países aliados de Estados Unidos. La Administración Carter dejó de dar apoyo al régimen de Somoza en Nicaragua, históricamente respaldado por Estados Unidos y dieron su ayuda al nuevo gobierno del Frente Sandinista de Liberación Nacional que asumió el poder después del derrocamiento de Somoza. Sin embargo, Carter ignoró una petición del Arzobispo Óscar Romero en El Salvador para enviar ayuda militar a ese país. Romero fue asesinado más tarde por sus críticas por la violación de derechos humanos en El Salvador. Carter también fue criticado por la activista feminista Andrea Dworkin por desentenderse de los derechos de las mujeres en Arabia Saudita.

Carter continuó la política de sus antecesores de imposición de sanciones a Rodesia, y después de que el obispo Abel Muzorewa fuera elegido primer ministro, protestó por la exclusión de las elecciones de Robert Mugabe y Joshua Nkomo. La fuerte presión de Estados Unidos y el Reino Unido lograron nuevas elecciones en Rodesia (hoy Zimbabue), que llevaron a la elección de Robert Mugabe como Primer Ministro; después de lo cual, se levantaron las sanciones, y se le concedió al país el reconocimiento diplomático. Carter también fue conocido por sus críticas a Alfredo Stroessner de Paraguay y Augusto Pinochet de Chile, aunque ambos Stroessner y Pinochet, asistieron a la firma del Tratado del Canal de Panamá, también protestó contra el Apartheid en Sudáfrica.

Carter continuó la política de Richard Nixon para normalizar las relaciones con la República Popular China. El consejero de Seguridad Nacional, Zbigniew Brzezinski y Michel Oksenberg, viajaron a Pekín a principios de 1978, donde junto con Leonard Woodcock, director de la oficina de enlace, establecieron las bases de un acuerdo para alcanzar relaciones diplomáticas y comerciales completas con la República Popular de China. En el Comunicado Conjunto sobre el Establecimiento de Relaciones Diplomáticas (Joint Communiqué on the Establishment of Diplomatic Relations) de 1 de enero de 1979, Estados Unidos transfirió el reconocimiento diplomático de Taipéi a Pekín y reiteró el "Comunicado de Shanghái" que suponía el reconocimiento de la existencia de una única China y admitía que Taiwán formaba parte de ella. Pekín, por su parte, aceptó que Estados Unidos siguiera manteniendo relaciones comerciales, culturales y contactos de carácter no oficial con Taiwán. Estados Unidos siguió manteniendo contactos con Taiwán gracias a la Ley de Relaciones de Taiwán.

Uno de los momentos más controvertidos de la presidencia de Carter fue la negociación y firma de los Tratados del Canal de Panamá, en septiembre de 1977. Estos tratados, que en esencia suponían la transferencia del Canal, de los estadounidenses a la República de Panamá, fueron rechazados por el Partido Republicano, con el argumento de que se estaba transfiriendo un asentamiento estadounidense de gran valor estratégico a un país inestable y corrupto, dirigido por el general Omar Torrijos, que no había sido elegido democráticamente. Aquellos que apoyaban los tratados, por el contrario, defendían que el canal había sido construido dentro del territorio panameño y que por tanto, Estados Unidos mediante su control había ocupado parte de otro país, por lo que el acuerdo tenía por objeto devolver a Panamá la total soberanía sobre su territorio. Tras la firma de los Tratados del Canal en junio de 1978, Carter visitó Panamá con su esposa y doce senadores de Estados Unidos, en medio de disturbios estudiantiles generalizados en contra del gobierno de Torrijos. Más adelante Carter instó al régimen de Torrijos para que frenara su política dictatorial y avanzara gradualmente hacia la democracia en Panamá.

Una de las claves de la política exterior de Carter, que conllevó un laborioso trabajo, fue la firma del Tratado SALT II (Conversaciones De Limitación de Armas Estratégicas»), que redujo el número de armas nucleares producidas o mantenidas tanto por los Estados Unidos como por la Unión Soviética. El trabajo de Gerald Ford y Richard Nixon había conducido al tratado SALT I, que redujo el número de armas nucleares producidos, pero Carter deseaba profundizar más en la reducción de este tipo de armamento. El objetivo principal de Carter, como señaló en su discurso inaugural, consistía en la desaparición completa de las armas nucleares en el mundo.

Con este fin, Carter y el líder de la Unión Soviética, Leonid Brezhnev, llegaron en 1979, al acuerdo del Tratado SALT II. Sin embargo el Congreso negó su ratificación, ya que muchos pensaban que la firma de los tratados debilitaría las defensas de Estados Unidos. Tras la intervención soviética en Afganistán a finales de 1979, Carter retiró el tratado de la consideración del Congreso y nunca se ratificó, aunque fue firmado por Carter y Brezhnev. Aun así, ambas potencias cumplieron los compromisos establecidos en las negociaciones firmadas por ambos dignatarios.

Jimmy Carter se sorprendió mucho de la intervención soviética en Afganistán de diciembre de 1979 y rápidamente tomó medidas, entre ellas armar a los muyahidines. El vicepresidente Walter Mondale manifestó públicamente su desaprobación de la agresiva política que había adoptado la Unión Soviética.

Los soviéticos habían mantenido conversaciones previas con los dirigentes afganos que parecían indicar que no tenían intención de intervenir, sin embargo el Politburó, con muchas dudas, había considerado seriamente la posibilidad de actuar militarmente. Se ha argumentado que la ayuda financiera de Estados Unidos a los disidentes afganos, que incluían islamistas y otros militantes muyahidines afganos, y el deseo soviético de proteger al gobierno afgano de izquierda, fueron los factores que terminaron de convencer a los soviéticos para intervenir.

Por otra parte, buscando desestabilizar la zona, la CIA desde comienzos de la década de 1970 junto con los ingleses apoyaba los militantes muyahidines afganos y en 1975 había participado en un fallido intento de guerra civil, organizado desde Pakistán, que fue un rotundo fracaso; que proporcionó dinero y armas a los insurgentes fundamentalistas a través de Inter-Services Intelligence (ISI) (servicios secretos de Pakistán, en un programa denominado Operación Ciclón.

Estados Unidos comenzó a enviar en secreto ayuda financiera limitada para las facciones islamistas afganas el 3 de julio de 1978. En diciembre de 1979 la URSS derrocó al primer ministro Hafizullah Amín, a quien acusaba de agente de la CIA y que previamente había dado un golpe de Estado contra el gobierno legítimo de Nur Taraki. Los políticos estadounidenses, tanto republicanos como demócratas, decían que los soviéticos se estaban posicionando para dominar el petróleo del Medio Oriente. Otros creían que la Unión Soviética temía que la revolución e islamización de Afganistán se extendiera a la población musulmana de la URSS.

Después del derrocamiento de Amín, Carter anunció lo que se conoce como la Doctrina Carter, consistente en el compromiso norteamericano de usar la fuerza si fuera necesario para acceder a los recursos petrolíferos del Golfo Pérsico. El aumento de tensión entre los bloques causado por está doctrina culminó con el boicoteo de los Juegos Olímpicos de Moscú de 1980, al que la URSS y sus aliados contestarían con su ausencia en los Juegos de Los Ángeles en 1984.

Asimismo se puso fin al tratado de Trigo Ruso, que tenía por objeto establecer un comercio con la URSS y disminuir las tensiones de la Guerra Fría. Las exportaciones de cereales había sido beneficiosas para los agricultores, y el embargo de Carter marcó el inicio de dificultades para los agricultores estadounidenses.

Carter y Brzezinski iniciaron un programa encubierto de entrenamiento de los muyahidines en Pakistán y Afganistán con objeto de frustrar los planes soviéticos. Las políticas diplomáticas de Carter hacia Pakistán cambiaron drásticamente. La administración había cortado la ayuda financiera al país a principios de 1979 cuando los fundamentalistas religiosos, alentados por la corriente de la dictadura militar islámica en Pakistán, quemaron la Embajada de los EE.UU. allí. La participación internacional en Pakistán, aumentó considerablemente con la intervención soviética. Al entonces presidente de Pakistán, el general Muhammad Zia-ul-Haq, se le ofrecieron 400 millones de dólares para subsidiar a los anticomunistas muyahidines en Afganistán. El general Zia declinó la oferta como insuficiente y EE.UU. se vio obligado a aumentar la ayuda a Pakistán.

Reagan posteriormente expandió en gran medida este programa. Los críticos con esta política culparon a Carter y Reagan de la inestabilidad de los gobiernos post-soviéticos de Afganistán, que ocasionó el surgimiento de una teocracia islámica en la región.

El principal conflicto de Estados Unidos en materia de los derechos humanos llegó como consecuencia de las relaciones de Carter con el sah de Irán. El sah Mohammad Reza Pahlavi, había sido un fuerte aliado de los Estados Unidos desde la Segunda Guerra Mundial y uno de los denominados "pilares gemelos" en los que se basaba la política estratégica de EE.UU. en el Oriente Medio, (el otro era Arabia Saudita). Sin embargo, el sah había ejercido un gobierno fuertemente autocrático, que fue visto como desafiadoramente cleptocrático en su país. En 1953 organizó, junto con la administración de Eisenhower, un golpe de estado para eliminar al elegido primer ministro, Mohammed Mossadegh.

En una visita de estado a Irán durante 1978, Carter habló públicamente en favor del sah, denominándolo ""líder de la sabiduría suprema"" y pilar de la estabilidad en el volátil Oriente Medio, en un discurso que nunca fue difundido en la televisión estadounidense. Cuando poco después estalló la Revolución iraní y el sah fue derrocado, Estados Unidos no intervino directamente y el sah tuvo que marchar hacia un exilio permanente, en enero de 1979. Carter inicialmente le negó la entrada a Estados Unidos, incluso por razones de urgencia médica.

A pesar de su negativa inicial a la entrada del sah en los Estados Unidos, el 22 de octubre de 1979, Carter le concedió permiso de entrada y asilo temporal durante la duración de su tratamiento contra el cáncer, el sah volvió a Panamá el 15 de diciembre de 1979. Sin embargo, en noviembre de ese mismo año, como respuesta a la entrada del sah en EE.UU., militantes iraníes tomaron la embajada estadounidense en Teherán, reteniendo a 52 estadounidenses como rehenes. Los iraníes exigían a cambio de su liberación:


Aunque ese mismo año, el sah salió de Estados Unidos, moriría en Egipto en 1980. La crisis de los rehenes continuó y dominó el último año de la presidencia de Carter. La subsiguiente respuesta a la crisis - desde la estrategia de "Rose Garden" de permanecer dentro de la Casa Blanca, hasta el intento fallido de rescatar a los rehenes por medios militares (Operación Garra de Águila), fueron en gran medida los responsables en la derrota electoral de Carter en las en las presidenciales de 1980. El 14 de noviembre de 1979, después de la toma de los rehenes, Carter dictó la Orden Ejecutiva 12170, que bloqueaba las propiedades del Gobierno iraní, congelando cuentas bancarias del gobierno iraní en los bancos de Estados Unidos, por un total de 8.000 millones de dólares estadounidenses de ese tiempo. Estos embargos se utilizaron como moneda de cambio para la liberación de los rehenes.

En los días anteriores a que Ronald Reagan asumiera la presidencia de Estados Unidos después de su victoria electoral, el diplomático argelino Abdulkarim Ghuraib había iniciado las negociaciones entre EE.UU. e Irán, que culminaron en los "Acuerdos de Argel" del 19 de enero de 1981, justo un día antes de que finalizase la presidencia de Carter. Los acuerdos implicaban el compromiso de Irán de liberar a los rehenes de inmediato. Además, las Órdenes Ejecutivas 12277 y 12.285 dictadas por Carter, suponían la liberación de todos los bienes pertenecientes al gobierno iraní y de todos los activos pertenecientes al sah que se encontraban en los Estados Unidos, así como la garantía de que los rehenes no ejercerían ningún tipo de reclamación legal contra el gobierno iraní con motivo del secuestro. Irán, igualmente, accedió a colocar 100.000 millones de dólares de los activos congelados en una cuenta de garantía bloqueada, acordando Irán y los Estados Unidos la creación de un tribunal para dirimir las denuncias de ciudadanos norteamericanos por las pérdidas ocasionadas por el Gobiernop iraní. Este tribunal conocido como Tribunal de Reclamaciones Irán-Estados Unidos, otorgaría más de dos mil millones de dólares a reclamantes de EE.UU. y ha sido descrito como uno de los órganos de arbitraje más importantes en la historia del derecho internacional. 

Aunque la liberación de los rehenes fue negociada y garantizada por el gobierno de Carter, los rehenes no fueron liberados hasta el 20 de enero de 1981, momentos después de que Reagan hubiese tomado posesión como presidente.

A finales de 1979, un año antes de las elecciones, Carter marchaba muy por detrás en las preferencias de los ciudadanos, incluso los observadores políticos pensaban que podría ser sustituido por Edward Kennedy en el partido demócrata. Sin embargo algunos hechos del escanario internacional, como la toma de los rehenes de la embajada en Irán y la invasión rusa de Afganistán mejoraron su posición en las encuestas y lo empujaron lo suficiente para lograr la nominación de su partido. El posterior fracaso del rescate de los rehenes hundió de nuevo su imagen de cara a la reelección.

Carter perdió las elecciones contra el republicano Ronald Reagan. La distribución de votos fue 43,9 millones votos, que representaban el 50,7% del total, para Reagan y 35,5 millones de votos, que representaban el 41%, para Carter. El candidato independiente John B. Anderson obtuvo 5,7 millones de votos, un 6,6% del total. Sin embargo, el hecho de que los apoyos a Carter no se concentraran en una región geográfica concreta, provocó que Reagan obtuviera una victoria arrolladora del 91% de los compromisarios, dejando a Carter con solo seis estados y el Distrito de Columbia. Reagan consiguió un total de 489 votos electorales frente a 49 de Carter.

La derrota de Carter marcó la primera vez que un presidente electo no lograba obtener un segundo mandato desde Herbert Hoover en 1932. Carter cumpliría su promesa de la liberación con vida de los 52 rehenes de la embajada de Estados Unidos en Irán, pero no logró asegurar su liberación antes de las elecciones. Aunque Carter negoció en última instancia su liberación, Irán no accedió a la misma hasta unos minutos después de que Ronald Reagan asumiese el cargo de presidente. En reconocimiento de su intervención, Reagan le pidió que fuera a Alemania Occidental para recibir a los rehenes tras su liberación.

Durante la campaña, Carter fue objeto de burla por el denominado incidente del conejo, un encuentro con un conejo de pantano mientras pescaba, el 20 de abril de 1979.

En 1981, Carter regresó al cultivo de cacahuates en Georgia, que había colocado en un fideicomiso ciego durante su presidencia para evitar incluso la apariencia de conflicto de intereses. Encontró que los fideicomisarios habían manejado mal la actividad, dejándolo con más de un millón de dólares en deuda. En los años siguientes, ha llevado una vida activa, con la fundación del Centro Carter, la constitución de su biblioteca presidencial, la enseñanza en la Universidad Emory en Atlanta y con la publicación de varios libros.

Al dejar el cargo, su presidencia fue valorada por la mayoría como un fracaso. En la clasificación histórica de presidentes de Estados Unidos, la presidencia de Carter ha oscilado entre el puesto 19 y el 34. Si bien la presidencia de Carter recibió críticas mixtas de algunos historiadores, su lucha por la paz por encima de todo y sus esfuerzos humanitarios desde que abandonó la presidencia le han llevado a ser ampliamente reconocido como uno de los más exitosos expresidentes en la historia de EE.UU.

Jimmy Carter y su vicepresidente Walter Mondale han sido el equipo más longevo después de la presidencia en la historia estadounidense. El 11 de diciembre de 2006, cumplieron 25 años y 325 días desde que dejaron sus cargos, superando el récord anterior establecido por el presidente John Adams y Thomas Jefferson como vicepresidente, que murieron el 4 de julio de 1826. En agosto de 2012 ha superado a Herbert Hoover como presidente que más ha vivido desde el término de su mandato.

Jimmy Carter es uno de los cuatro presidentes, y el único en la historia moderna de Estados Unidos, que no tuvo la oportunidad de nombrar a un juez para que integrase en la Corte suprema.

El diario "The Independent" publicó:

Carter inició su mandato con un índice de 66% de aprobación, pero esta se redujo a un 34% en el momento de dejar el cargo, con el 55% de desaprobación.

Las encuestas a comienzos de la campaña presidencial de 1976 sugirieron que muchos no le perdonaban a Gerald Ford su relación con Richard Nixon y el escándalo Watergate. Carter, en comparación, parecía un honesto, sincero y bien intencionado sureño.

Esta situación cambió cuando Carter se postuló para la reelección, cuando la autoconfianza de Ronald Reagan contrastó con el temperamento serio e introspectivo de Carter. La atención personal de Carter a los detalles y su aparente indecisión y debilidad se vieron acentuados por el encanto que Reagan despertó entre los votantes. En última instancia, la combinación de los problemas económicos, la crisis de los rehenes de Irán, y la falta de cooperación de Washington le hizo fácil a Reagan presentar a Carter como un líder ineficaz.

Desde que dejó la presidencia, la reputación de Carter ha mejorado mucho. El índice de aprobación presidencial de Carter, que tuvo su cima en un 31% justo antes de las elecciones de 1980, se encontraba a principios de 2009 en el 64%. La pospresidencia de Carter también ha tenido una acogida favorable. Carter explica que una gran parte de este cambio se debió al sucesor de Reagan, George HW Bush, quien lo buscó activamente y fue mucho más amable e interesado en su consejo de lo que había sido Reagan. Carter ha mantenido relaciones de trabajo con los expresidentes George H.W. Bush, Bill Clinton y George W. Bush, y a pesar de sus diferencias políticas, los tres se han convertido en buenos amigos durante años trabajando juntos en varios proyectos humanitarios.

Como presidente, Carter expresó su objetivo de hacer un gobierno que fuera "competente y compasivo." En la búsqueda de esa visión, ha estado implicado en una variedad de políticas públicas nacionales e internacionales, la resolución de conflictos, derechos humanos y causas benéficas.

En 1982, fundó el Centro Carter en Atlanta para promover los derechos humanos y aliviar el sufrimiento humano innecesario. Esta institución no gubernamental, sin ánimo de lucro, promueve la democracia, busca la mediación y la prevención de conflictos y supervisa el proceso electoral en apoyo de elecciones libres y justas. Además, trabaja para mejorar la salud global a través del control y la erradicación de enfermedades como la Dracunculiasis, la oncocercosis, el paludismo, el tracoma, la filariasis linfática y la esquistosomiasis. También lucha para disminuir el estigma de enfermedades mentales y mejorar la nutrición mediante el aumento de la producción de los cultivos en África. Un gran logro de este Centro ha sido la eliminación de más del 99% de los casos de enfermedad del gusano de Guinea, un parásito debilitante que ha existido desde la antigüedad, de 3,5 millones de casos estimados en 1986 se ha pasado a menos de 10.000 en 2007. El Centro Carter ha supervisado 70 elecciones en 28 países desde 1989. Se ha trabajado para resolver los conflictos en Haití, Bosnia, Etiopía, Corea del Norte, Sudán, Venezuela y otros países. El Centro Carter apoya activamente a los defensores de los derechos humanos en todo el mundo y han intervenido con jefes de estado en su nombre.

En 2002, el expresidente Carter recibió el Premio Nobel de la Paz por su trabajo para encontrar soluciones pacíficas a los conflictos internacionales, promover la democracia, los derechos humanos y el desarrollo económico y social a través del Centro Carter. Tres presidentes, Theodore Roosevelt, Woodrow Wilson y Barack Obama, han recibido el premio durante sus presidencias, Carter es único en recibir el premio por sus acciones después de dejar la presidencia. Es, junto con Martin Luther King, Jr., uno de los dos georgianos nativos en recibir el Nobel.

En 1994, Corea del Norte había expulsado a los investigadores del Organismo Internacional de Energía Atómica y amenazaba con empezar a procesar combustible nuclear gastado. En respuesta, el entonces presidente Bill Clinton presionó para que se impusieran sanciones y ordenó el envío de grandes cantidades de tropas y vehículos a la zona para prepararse para la guerra.

Clinton reclutó a Carter en secreto, para llevar a cabo una misión de paz en Corea del Norte, bajo la apariencia de una misión privada de Carter, Clinton vio en Carter una manera de que el presidente de Corea del Norte, Kim Il-sung, diese marcha atrás sin quebrantar su reputación.

Carter negoció un acuerdo con Kim Il-sung, pero fue más allá y diseñó también un tratado, que anunció en la CNN, sin la autorización de la Casa Blanca, como una manera de forzar a los EE.UU. a la acción. La Administración de Clinton firmó una versión posterior del Agreed Framework, en virtud del cual Corea del Norte acordó congelar y finalmente desmantelar su programa nuclear y cumplir con sus obligaciones de no proliferación, a cambio de entregas de petróleo, la construcción de dos reactores de agua ligera para reemplazar sus reactores de grafito y discusiones para el eventual establecimiento de relaciones diplomáticas. El acuerdo fue aclamado en su momento como un logro diplomático importante. Sin embargo, en diciembre de 2002, los Acuerdos de Framework se vinieron abajo como consecuencia de una disputa entre el gobierno de George W. Bush y el gobierno de Corea del Norte de Kim Jong-il. En 2001, el presidente George W. Bush había decidido adoptar una posición de confrontación hacia Corea del Norte y en enero de 2002, incluyó a este país como parte de un "eje del mal". Mientras tanto, Corea del Norte comenzó a desarrollar la capacidad de enriquecer uranio. Bush y otros opositores de la Administración estadounidense a los Acuerdos de Framework creían que el gobierno de Corea del Norte nunca estuvo dispuesto a ceder su programa de armas nucleares, por el contrario los partidarios del acuerdo pensaban que este podría haber sido un éxito y que fue socavado.

Carter y los expertos del Centro Carter asistieron a las negociaciones no oficiales entre israelíes y palestinos para el diseño de un modelo de acuerdo de paz, llamado el Acuerdo de Ginebra, en el período 2002-2003. Carter también se ha convertido en los últimos años en un crítico frecuente de las políticas de Israel en el Líbano, Cisjordania y Gaza.

En abril de 2008, el periódico árabe con sede en Londres, Al-Hayat, informó que Carter se reunió con el líder de Hamás, Khaled Mashaal en su visita a Siria. El Centro Carter inicialmente no confirmó ni negó la historia. El Departamento de Estado de los Estados Unidos considera Hamás como una organización terrorista. En este viaje de Medio Oriente, Carter también colocó una ofrenda floral en la tumba de Yasser Arafat en Ramallah, el 14 de abril de 2008. Carter dijo el 23 de abril que ni Condoleezza Rice ni nadie en el Departamento de Estado le había advertido de que no se reuniera con los líderes de Hamás durante su viaje. Carter conversó con Mashaal respecto a varios asuntos, entre ellos "fórmulas de intercambio de prisioneros para obtener la liberación del cabo Shalit."

En mayo de 2007, mientras que el argumento de que Estados Unidos debe hablar directamente a Irán, Carter afirmó que Israel tenía 150 armas nucleares en su arsenal.

En diciembre de 2008, Carter visitó Damasco nuevamente, donde se reunió con presidente sirio Bashar Assad, y el líder del Hamás. Durante su visita, concedió una entrevista exclusiva a la revista "Adelante", la primera entrevista para cualquier presidente estadounidense, actual o anterior, con medios de comunicación sirios que han tenido éxito.

Carter celebró cumbres en Egipto y Túnez en el período 1995-1996 para abordar la violencia en la región de los Grandes Lagos de África y jugó un papel clave en la negociación del Acuerdo de Nairobi de 1999 entre Sudán y Uganda.

El 18 de julio de 2007, Carter se reunió con Nelson Mandela en Johannesburgo, Sudáfrica, para anunciar su participación en una nueva organización humanitaria llamada Global Elders. En octubre de 2007, Carter visitó Darfur con varios de los miembros de los Global Elders, entre ellos Desmond Tutu. Las seguridades sudanesas le impidieron visitar a un líder tribal de Darfur, lo que provocó una acalorada discusión.

El 18 de junio de 2007, Carter, acompañado de su esposa, llegó a Dublín, Irlanda, para sostener conversaciones con la presidenta Mary McAleese y Bertie Ahern en materia de derechos humanos. El 19 de junio, Carter asistió y habló en el Foro Anual de Derechos Humanos en Croke Park. Un acuerdo entre la Ayuda Irlandesa y el Centro Carter, se firmó también en este día.

En noviembre de 2008, Carter, el exsecretario general de las Naciones Unidas, Kofi Annan, y Graca Machel, esposa de Nelson Mandela, intentaron entrar en Zimbabue, para inspeccionar la situación de los derechos humanos, siéndole impedido el paso por parte del gobierno del presidente Robert Mugabe.

Carter encabezó una misión a Haití en 1994 con el senador Sam Nunn y el expresidente del Estado Mayor Conjunto de los Estados Unidos, el general Colin Powell, para evitar una invasión multinacional encabezada por Estados Unidos y restaurar el poder al presidente democráticamente electo de este país, Jean-Bertrand Aristide.

Carter visitó Cuba en mayo de 2002 y mantuvo conversaciones con Fidel Castro y el gobierno cubano. Se le permitió dirigirse al público sin censura en la televisión y la radio nacionales cubanas, con un discurso que escribió y presentó en español. En el discurso, pidió a los Estados Unidos poner fin a "un embargo económico ineficaz de 43 años de edad" y a Castro a celebrar elecciones libres, mejorar los derechos humanos, y permitir más libertades civiles. Se reunió con los disidentes políticos, visitó un hospital de SIDA, una escuela de medicina, un centro de biotecnología, una cooperativa de producción agrícola y una escuela para niños discapacitados y realizó un lanzamiento de honor en un partido de béisbol en La Habana. La visita de Carter marcó la primera vista a la isla desde la Revolución cubana de 1959, de un presidente de los Estados Unidos, dentro o fuera de la presidencia.

Carter visitó como observador las elecciones de Venezuela el 15 de agosto de 2004. Los observadores de la Unión Europea se había negado a participar, manifestando que el gobierno de Hugo Chávez les imponía demasiadas restricciones. El Centro Carter declaró que el proceso "sufrió numerosas irregularidades", pero dijo que no observó ni recibió "pruebas de fraude que hubieran cambiado el resultado de la votación". En la tarde del 16 de agosto de 2004, el día después de la votación, Carter y el Secretario General de la Organización de los Estados Americanos (OEA), César Gaviria, dieron una conferencia de prensa conjunta en la que aprobaban los resultados preliminares anunciados por el Consejo Nacional Electoral. Las conclusiones de la monitores ", coincidió con las declaraciones parciales anunciados hoy por el Consejo Nacional Electoral", dijo Carter, mientras que Gaviria agregó que los miembros de la misión de observación electoral de la OEA no había "encontrado ningún elemento de fraude en el proceso." Dirigiendo sus comentarios a las figuras de la oposición que hicieron las denuncias de "fraude generalizado" en la votación, Carter pidió a todos los venezolanos a "aceptar los resultados y trabajar juntos para el futuro". Sin embargo, una encuesta a pie de urnas realizado por Penn, Schoen & Berland Associates (PSB ) había predicho que Chávez perdería por 20%, cuando los resultados electorales mostraron una victoria del 20%, Schoen, comentó: "Creo que fue un fraude masivo".

A raíz de la ruptura de relaciones diplomáticas entre Ecuador y Colombia en marzo de 2008, Carter medió un acuerdo entre los presidentes de ambos países para el restablecimiento de relaciones diplomáticas de bajo nivel, que fue anunciado el 8 de junio de 2008.

En 2001, Carter criticó el controvertido indulto de Marc Rich, aprobado por el presidente Bill Clinton, calificándolo de "vergonzoso" sugiriendo que las contribuciones financieras de Rich al Partido Demócrata fueron un factor en la decisión de Clinton.

Carter también ha criticado la presidencia de George W. Bush y la guerra de Irak. En un editorial de 2003 del "The New York Times", Carter advirtió contra las consecuencias de una guerra en Irak y llamó a la moderación en el uso de la fuerza militar. En marzo de 2004, Carter también acusó a George W. Bush y Tony Blair de librar una guerra innecesaria, basada en mentiras y malas interpretaciones para derrocar a Saddam Hussein. En agosto de 2006, Carter criticó a Blair por ser "servil" a la administración de Bush y acusó a Blair de dar apoyo incondicional a las políticas de Bush en Irak. En mayo de 2007 en una entrevista con Arkansas Democrat-Gazette de Arkansas, manifestó:

El 19 de mayo de 2007, Blair hizo su última visita a Irak antes de dimitir como primer ministro británico y Carter aprovechó la ocasión para criticarlo una vez más. Carter dijo a la BBC que Blair era "aparentemente servil" a Bush y lo criticó por su "apoyo incondicional" de la guerra de Irak. Carter describió las acciones de Blair como "abominables" y afirmó que "el apoyo del primer ministro británico casi sin desviarse de las políticas desacertadas del presidente Bush en Irak habían sido una gran tragedia para el mundo. Carter manifestó que creía que Blair se había distanciado de la administración de Bush durante el período previo a la invasión de Irak en 2003, se pudo haber hecho una diferencia crucial a la opinión pública y política de América, y por lo tanto la invasión no hubiera seguido adelante. Carter expresó su esperanza de que el sucesor de Blair, Gordon Brown, fuese «menos entusiasta» con la política de Bush en Irak.

En junio de 2005, Carter instó el cierre de la prisión de la Bahía de Guantánamo en Cuba, que ha sido un punto focal para las demandas recientes de abusos de prisioneros.

En septiembre de 2006, Carter fue entrevistado por la BBC Newsnight que era un programa que trataba de asuntos de actualidad, expresando su preocupación por la creciente influencia de la Derecha Religiosa en la política de los EE.UU.

El 3 de junio de 2008, cerca del final de las primarias, Carter, en su condición de expresidente, fue nombrado superdelegado en la Convención Nacional Demócrata, anunciando su respaldo al Senador y posterior presidente, Barack Obama.

En 2009 puso peso detrás de las acusaciones de presidente venezolano Hugo Chávez, referentes a la participación de Estados Unidos en el intento golpe de estado en Venezuela de 2002 perpetrado por una Junta cívica-militar, manifestando que Washington conocía la existencia del golpe y pudo haber tomado parte.

Carter ha continuado manifestándose en contra de la pena de muerte en los EE.UU. y en el resto del mundo. Más recientemente, en su carta al gobernador de Nuevo México, Bill Richardson, le instó a firmar un proyecto de ley para eliminar la pena de muerte e instituir la cadena perpetua sin libertad condicional. La ley fue aprobado por la Cámara y el Senado estatal. Carter escribió:

Carter también ha pedido la conmutación de la pena de muerte para muchos presos ya condenados, entre los que se incluyen Brian K. Baldwin (ejecutado en 1999 en Alabama), Kenneth Foster (pena conmutada en Texas en 2007) y Troy Anthony Davis (de Georgia).

En una entrevista de 2008 con Amnistía Internacional, Carter criticó el uso de la tortura en la Bahía de Guantánamo, diciendo que "contraviene los principios básicos sobre los que se fundó esta nación." Dijo que el próximo presidente debería pedir disculpas públicamente en su toma de posesión, y declarar que los Estados Unidos "nunca más torturaría a los prisioneros".

Carter ha sido un autor prolífico en su post-presidencia, escribió entre 21 y 23 libros. Entre ellos se encuentra uno co-escrito con su esposa, Rosalynn, y un libro para niños ilustrado por su hija, Amy. Cubren una gran variedad de temas, incluyendo el trabajo humanitario, el envejecimiento, la religión, los derechos humanos, y la poesía.

Su hija más joven, Amy Carter, que tenía 9 años cuando empezó la presidencia de su padre, fue objeto de constante atención de los medios de comunicación, ya que desde la presidencia de John F. Kennedy, a principios de los sesenta, no habían vivido niños en la Casa Blanca.

Carter y su esposa, Rosalynn Carter, son también muy conocidos por su trabajo como voluntarios con la organización Habitat for Humanity, una institución filantrópica con sede en Georgia que ayuda a trabajadoras de bajos ingresos a construir y comprar sus propias casas. Es profesor de escuela dominical y diácono en la Iglesia Maranatha Bautista en su ciudad natal de Plains (Georgia). En el año 2000, Carter rompió los vínculos con la Convención Bautista del Sur, diciendo que sus doctrinas del grupo no se alineaban con sus creencias cristianas. En abril de 2006, Carter, el expresidente Bill Clinton y el presidente Bill Mercer de la Underwood University iniciaron el Nuevo Pacto Bautista. El movimiento ampliamente inclusiva busca unir a los bautistas de todas las razas, las culturas y las afiliaciones de convenciones. Dieciocho líderes bautistas que representaban a más de 20 millones de bautistas en América del Norte apoyado al grupo como una alternativa a la Convención Bautista del Sur. El grupo celebró su primera reunión en Atlanta, del 30 de enero al 1 de febrero de 2008.

Carter tiene aficiones que incluyen la pintura, la pesca con mosca, trabajar la madera, el ciclismo, el tenis y el esquí. Para hacer ejercicio, el presidente Carter boxea con Joe Walz.

Los Carter tienen tres hijos, una hija, ocho nietos, tres nietas, y dos bisnietos. Su hijo mayor, Jack, fue el candidato demócrata al Senado de los Estados Unidos en Nevada en 2006, perdiendo ante el que estaba actualmente, John Ensign. El hijo de Jack, Jason, fue elegido como presidente del Senado estatal de Georgia en 2010.

El 12 de agosto de 2015 dio a conocer que había sido diagnosticado de cáncer metastático tras haber sido sometido a una cirugía para extirpar un tumor en su hígado.




</doc>
<doc id="8206" url="https://es.wikipedia.org/wiki?curid=8206" title="Rendimiento">
Rendimiento

El término rendimiento puede referirse a:


</doc>
<doc id="8207" url="https://es.wikipedia.org/wiki?curid=8207" title="Coeficiente">
Coeficiente

Coeficiente hace referencia a varios artículos:

Expresión numérica que mediante alguna fórmula determina las características o propiedades de un cuerpo:

Sumas de polinomios




</doc>
<doc id="8208" url="https://es.wikipedia.org/wiki?curid=8208" title="Provincia de Ávila">
Provincia de Ávila

Ávila es una provincia del centro de España perteneciente a la comunidad autónoma de Castilla y León. Su capital es la ciudad de Ávila y está formada por 247 municipios. Su relieve está marcado por la presencia al sur del Sistema Central, que la divide en dos zonas: la mayor parte del territorio abulense se ubica en la submeseta norte, si bien incluye también una franja al sur de la sierra de Gredos. Ávila, que limita con Valladolid, Toledo, Cáceres, Segovia, Madrid y Salamanca, es una de las provincias menos pobladas del país, con 158 698 habitantes (INE 2018).

El escudo de la Provincia de Ávila reúne los blasones de poblaciones que han sido cabeza de los antiguos partidos judiciales; como Arenas de San Pedro con su castillo incendiado. En su parte central figuran las armas de la ciudad de Ávila, un campo de gules o rojo en el que aparece representado el rey Alfonso VII de León en el ábside de la catedral de Ávila junto al lema: "Ávila del Rey".

La provincia, que tiene una extensión de 8051,15 km², está situada al sur de la comunidad autónoma de Castilla y León. Limita con las provincias de Madrid, Toledo (Castilla-La Mancha) y Cáceres (Extremadura), aparte de con las provincias castellano y leonesas de Salamanca, Segovia y Valladolid también Ávila tiene 167 015 habitantes y 27,75 por kilómetro cuadrado.

Es propio de esta provincia su gran diversidad orográfica. Es la provincia de mayor altitud en promedio de España, con una altura media de 1131,8 m. Se distinguen tres grandes regiones:

La zona norte, continuación de la Meseta Norte y del valle del Duero está caracterizada por un paisaje llano con suelos formados por materiales sedimentarios. Comprende la comarca de La Moraña. Los principales municipios son Arévalo y Madrigal de las Altas Torres.

La zona central donde se localizan el Valle de Amblés, el del Corneja y las zonas de montaña (sierra de Gredos, sierra de Béjar, sierra de Villafranca, La Serrota, sierra de la Paramera, sierra de Ávila o la sierra de Malagón) presenta ingentes formaciones graníticas que alcanzan su mayor altitud en el Pico del Moro Almanzor, que con 2592 metros es la cumbre culminante de la sierra de Gredos y la más elevada de todo el Sistema Central. Su clima de montaña se caracteriza por temperaturas muy bajas en el periodo invernal y veranos cortos y no muy calurosos. Son localidades importantes Ávila, Las Navas del Marqués, El Barco de Ávila y Piedrahíta.

La zona al sur del Sistema Central que comprende la parte abulense del valle del Tiétar está caracterizada por su menor altitud y un clima más cálido. En esta parte es fácil encontrar naranjos, olivos y palmeras en los principales pueblos. Entre los municipios más importantes de esta zona se encuentran Arenas de San Pedro, Candeleda, Sotillo de la Adrada, Mombeltrán y Lanzahíta.

En el oriente de la provincia, en torno al valle del Alberche y a las estribaciones meridionales de la sierra de Guadarrama, se localizan municipios como El Tiemblo, Cebreros o El Hoyo de Pinares.

La provincia de Ávila, que es atravesada de suroeste a noreste por el Sistema Central, es divisoria de aguas entre la cuenca hidrográfica del Duero y la del Tajo. Entre las sierras de Gredos, y la alineación de Serrota-Paramera nacen los ríos Tormes y Alberche. 

El río Tormes recoge aguas del alto Gredos, sobre todo la escorrentía del Circo de Gredos, y lleva una dirección este-oeste hasta Barco de Ávila, donde gira hacia el norte, para recibir las aguas del río Corneja en La Horcajada, camino de Salamanca para desembocar en el río Duero. 

El río Alberche, nace en la vertiente sur de la sierra de Villafranca, y recoge aguas de la vertiente sur de La Serrota, encharcando el valle alto con un recorrido meandriforme lleva una dirección oeste-este, antes de girar hacia el sur y encajarse en las cercanías de la Cueva del Maragato y hasta la Venta de Rasquilla, en donde gira de nuevo hacia el este. El tramo alto, hasta la Venta Rasquilla con su codo de captura, pertenecía al río Tormes, que lleva sus aguas al Duero, pero la erosión remontante del Alberche afluente del río Tajo consiguió captar estas aguas de entre sierras hacia el sur. 

El río Tiétar recorre el sur de la provincia, de este a oeste camino del río Tajo del que es afluente, recibiendo las aguas de la vertiente sur (la de solana y a la vez de barlovento) de Gredos, la más lluviosa, por ser el primer murallón con el que se encuentran la borrascas atlánticas que entran por el suroeste de la península ibérica. 

Entre La Serrota y la sierra de Ávila, nace el río Adaja, que tras recorrer el valle de Amblés de oeste a este, y llegar a Ávila gira hacia el norte camino del río Duero, para ser represado en la Presa de Fuentes Claras y en la de las Cogotas, antes de llegar a Arévalo, donde recibe las aguas del río Arevalillo. 

También van al Duero el río Voltoya, que desemboca en el Eresma —afluente a su vez del Adaja—, del que se abastece Ávila ciudad, mediante el agua embalsada en el embalse de Serones; así como el río Trabancos y el Zapardiel.

Otros ríos de menos importancia son el río Aravalle, el Almar, el Gamo, el Margañán que acaban en el Tormes; y el río Cofio que desemboca en el Alberche.

A destacar dos acuíferos: el acuífero del valle de Amblés y el de los Arenales.

La orografía de la provincia es la causa de la diversidad en el clima de esta. En las series climáticas referidas al periodo 1960-1996 la estación meteorológica más lluviosa de la provincia fue la correspondiente al municipio de Guisando, en la falda sur de la sierra de Gredos, con un valor de 1931,1 mm. El promedio de precipitación media anual de las 79 estaciones termopluviométricas fue en ese mismo periodo de 728,6 mm, mientras que la temperatura media de estas fue de 11,8 ºC. La provincia se divide principalmente en 4 variedades climáticas atendiendo a la clasificación climática agraria de Papadakis: «mediterráneo subtropical», «mediterráneo templado», «mediterráneo templado fresco» y «mediterráneo continental».

Existen tres dominios principales de precipitación: En la vertiente de la sierra de Gredos las precipitaciones son muy abundantes, sobrepasándose con frecuencia los 1500 mm en promedio anual. Le corresponde una temperatura media anual en torno a los 15 ºC. La zona montañosa central entre las sierras de Gredos y Ávila presenta unos niveles decrecientes en sentido sur-norte, desde el máximo en la sierra de Gredos (los 1500 m mencionados anteriormente) a un mínimo de unos 500 mm. La temperatura media anual —aunque variable en función de la altitud— podría situarse en 9 ºC. Por último la llanura sedimentaria septentrional presenta unas precipitaciones muy escasas —con frecuencia por debajo de los 400 mm anuales- y una importante aridez estival. Por último la temperatura media anual en esta zona es de alrededor de 12 ºC.

Según el "Atlas climático ibérico" de la Agencia Estatal de Meteorología y de acuerdo a la clasificación climática de Köppen, en la provincia se dan las variedades de clima mediterráneo "Csb" (templado con verano seco y templado) y "Csa" (templado con verano seco y caluroso); el segundo principalmente en la parte sur de la provincia. En las cumbres más altas se da un clima mediterráneo de montaña de tipo "Dsb" con temperaturas medias del mes más frío por debajo de los 0 ºC.

La diversidad orográfica antes indicada hace de Ávila una de las provincias interiores españolas más ricas en ecosistemas y por tanto en biodiversidad. Así se pueden distinguir cuatro tipos de ecosistemas principales:

Situada en la parte norte de la provincia y compuesta por grandes planicies de campos de cultivo con bosques isla diseminados, la mayor superficie forestal se concentra en el corredor del río Adaja desde Villanueva de Gómez hasta Arévalo de unos 30 kilómetros de largo con bosque de pinar, en superficie le siguen los pinares próximos a Nava de Arévalo. En las márgenes de los principales ríos se encuentran interesantes bosques de ribera.

En esta zona el principal cultivo es cereal de secano, si bien en los últimos años se ha desarrollado extraordinariamente el cultivo de regadío irrigado desde el embalse de Las Cogotas (desde 2010 la zona de Nava de Arévalo riega con este embalse; y se prevé la puesta en regadío de 7000 ha en la zona) y con pozos subterráneos, hecho que tras la sobreexploatación, ha propiciado, el casi agotamiento del acuífero de Los Arenales y por tanto, ha aumentado el nivel de nitritos y arsénico en algunas muestras. Existen varias especies de aves y mamíferos, algunas de ellas de gran valor como la avutarda y el águila imperial.

Se da principalmente en la parte central y sur de la provincia, en la falda de sus principales sierras. Pese a no ser muy abundantes por la tala abusiva realizada a principios del siglo XX se extienden por grandes zonas alrededor de la capital.

Los encinares existentes en las inmediaciones de Bonilla de la Sierra, La Horcajada y en el valle del Corneja son especialmente valiosos por su antigüedad, porte y magnífico estado de conservación.

Los encinares proporcionan cobijo y alimento a gran variedad de fauna, siendo de especial valor ecológico el águila imperial que llegó a ser animal prácticamente extinguido, pero en los años 90 comenzó a recuperarse.

Al contrario que en otras provincias castellano-leonesas, Ávila cuenta en su extremo meridional con una zona templada, de clima mediterráneo, caracterizada por la existencia de bosques de coníferas y frondosas, además de vides, olivares, naranjos, higueras, cerezos y plantaciones de tabaco, pimentón y sandías.

Esta parte de la provincia, al abrigo de la sierra de Gredos, es la que mayor diversidad biológica posee; pero también la más amenazada por la especulación urbanística, las explotaciones mineras, el trazado indiscriminado de infraestructuras y los incendios forestales.

El ecosistema de alta montaña se puede encontrar principalmente en la sierra de Gredos, sierra de Béjar y también en las inmediaciones del pico Zapatero (sierra de la Paramera) y en La Serrota. 

Pese a la gran presión humana que sufre, especialmente los fines de semana, la sierra de Gredos conserva una de las poblaciones más importantes de cabra montés. La preocupación de la Corona por este animal evitó su extinción y hoy en día coloca a esta población en unos niveles que permiten afirmar su supervivencia.

Otra especie seriamente amenazada que vuelve a estos lugares a finales de 1990 es el lobo, aunque su población aún no se ha asentado definitivamente.


Antes de la llegada de los romanos el territorio actual de la provincia estaba principalmente habitada por los vetones. El límite nororiental del territorio vetón se ha fijado unos pocos kilómetros al norte de la capital provincial, en Cardeñosa. Los vacceos también ocuparon una parte del territorio actual de la provincia, en la actual comarca de La Moraña. 

La mayor parte de la población se concentraba en la parte central de la provincia. Los pobladores prerromanos —los vetones— crearon en este periodo grandes poblados fortificados emplazados en elevaciones del terreno., como El Raso, Las Cogotas, el Castro de la Mesa de Miranda, el Castro de la Era de los Moros o Ulaca. El castro de mayores dimensiones e importancia debió de ser el Castro de Ulaca. Una estimación sobre la población de este último ha arrojado un dato aproximado de unos 5900 habitantes. Los estudios de los ajuares de las tumbas encontradas apuntan a una sociedad vetona jerarquizada y piramidal, que estaría dominada por una élite militar, y en cuyo escalón más bajo no se descarta que hubiera podido haber quizás algunos esclavos. La base económica de los vetones fue la ganadería —en la que probablemente destacaría el ganado vacuno, con un papel secundario del ganado porcino, caprino y ovino—. Debido a que el territorio no disfrutaba de las mejores condiciones para el aprovechamiento agrícola, la agricultura quedó relegada a un segundo plano en cuanto a importancia; la caza se debió ver beneficiada por una buena calidad cinegética del territorio, mientras que la recolección de bellotas —complementada por las castañas o las nueces— debió de tener una notable importancia en la dieta de los vetones.

Los vetones erigieron un gran número de esculturas de piedra con forma de toros y cerdos, los verracos, en el territorio de la actual provincia. Aunque también se encuentran en zonas que correspondían a otros pueblos prerromanos de la península ibérica, los hallazgos se corresponden principalmente a zonas vetonas. Sólo en la provincia de Ávila se ha encontrado más 43 % del total de ejemplares documentados. Su función es discutida todavía hoy en día; bien pudieron tener una función relacionada con ritos funerarios, de indicadores de cañadas ganaderas, de delimitación de territorios, de protectores del ganado, o serían estatuas a las que se otorgaba un papel mágico relacionado con la fertilidad.

En los años 192 y 193 a. C. los vetones, en alianza en primera instancia con vacceos y celtíberos, fueron derrotados por tropas romanas comandadas por el pretor Marco Fulvio Nobilior en Toledo, que en el 193 a. C. tomó la ciudad. Se considera probable que las fuerzas vetonas correspondieran a las tribus más orientales, que habitaban el actual territorio de la provincia. No existen fuentes escritas que mencionen a los vetones significativamente entre este suceso y el comienzo de las Guerras Lusitanas (155 a. C). Durante este último conflicto, los vetones se unieron al grueso de las fuerzas de los lusitanos liderados por Púnico. Los académicos opinan mayoritariamente que este apoyo vetón debió mantenerse hasta el final de la guerra. Posteriormente la falta de noticias desde la guerra sertoriana sugiere que el territorio vetón estaba ya pacificado por aquel entonces.

Augusto reorganizó en dos ocasiones las fronteras de las provincias de la Hispania Romana (datadas tentativamente alrededor del 27 a. C. y del 7-2 a. C.). Sin embargo no debieron modificar sustancialmente la adscripción de la actual provincia de Ávila. Las informaciones contradictorias que referirían el territorio de los vetones tanto a la Citerior (la nueva Tarraconense) como a la Lusitania, ha llevado a una mayoría de investigadores a concluir que el territorio vetón estaría dividido entre las dos grandes provincias, aunque el territorio de la actual provincia de Ávila habría pertenecido mayoritariamente a Lusitania exceptuando algunas zonas del norte y del este.

Ya en el siglo  a. C., con la romanización del territorio se produjeron cambios en el tipo de poblamiento, y se favoreció el desarrollo de núcleos en zonas llanas cerca de las redes viarias —sin una gran preocupación por elegir emplazamientos en función de sus posibilidades defensivas— y el abandono de la mayoría de los castros que todavía existían por entonces, especialmente los más alejados de las calzadas. Ya fuera fundación "ex novo" o no, la ciudad de Ávila se convierte en este periodo romano en el único gran núcleo urbano en la zona.

Del periodo tardorromano existe una importante villa romana con su necrópolis adjunta —la llamada villa romana de El Vergel— en el término municipal de San Pedro del Arroyo, en concreto al noreste de la localidad, fechada en torno a los siglos -, mientras que en Niharra, a pocos kilómetros de la ciudad de Ávila, se encuentra otra villa, la de Pared de los Moros.


Durante este periodo es destacable en el territorio de la actual provincia —enmarcado esencialmente entonces dentro de la provincia de Lusitania— la existencia de una sede episcopal en la ciudad de Ávila, mencionada en el año 610 en un documento firmado por el obispo Iustiniano. En el plano arqueológico son reseñables los yacimientos de Los Corralillos (en Diego Álvaro) y de Solosancho, en los que se encontraron tumbas, construcciones, objetos de adorno, pizarras, cerámica y restos de arneses.

La muestra arqueológica más característica de la época visigoda en la provincia son las pizarras, encontradas principalmente en Diego Álvaro pero también en Arevalillo, Cabezas del Villar, Chamartín y Solosancho. Otras piezas aisladas han sido halladas también en Cardeñosa (una patena), Arevalillo y San Miguel de Serrezuela (restos cerámicos), Adanero (un jarro litúrgico) y Candeleda (monedas).


Tras consolidarse la reconquista en las extremaduras castellanas a partir de la toma de Toledo en 1085, el territorio del Obispado de Ávila se va conformando de tal modo que establece el Territorio histórico de Ávila desde la Edad Media hasta la Edad Moderna.

En el siglo en el territorio de la actual provincia se contabilizaba un total de 18 comunidades de villa y tierra: La Adrada, La Horcajada, Arenas, Arévalo, Ávila, El Barco, Bohoyo, Bonilla de la Sierra, Candeleda, Madrigal, El Mirón, Mombeltrán, Las Navas, Piedrahíta, Vahíllo, Villanueva del Campillo, Villafranca de la Sierra y Villatoro.

Con la reordenación administrativa del Conde de Floridablanca de 1785, bajo reinado de Carlos III, Ávila era una de las 31 provincias del reino de España. La provincia estaba a su vez dividida en nueve partidos judiciales: partido de Villatoro (con siete pueblos), partido de Bonilla (con ocho pueblos), partido de Villafranca (con tres pueblos) y los estados de las Navas (con tres pueblos), La Adrada (con siete pueblos), Miranda (con ocho pueblos), Mombeltrán (con doce pueblos) —la mayor parte del valle del Tiétar estaba incluido en la provincia de Toledo por aquel entonces—, el partido o Tierra de Ávila, formado por los sexmos de San Juan, Cobaleda, San Vicente, San Pedro, Serrezuela, Santiago y Santo Tomé; y el partido o Tierra de la Villa de Arévalo, formado por los sexmos de Orvita, La Vega, Sinlabajos, Aldeas y Rágama. Además existían pueblos sueltos o eximidos de sexmo o de Partido.

A comienzos del siglo se llevó a cabo otra serie de reformas administrativas que afectó a la provincia de Ávila, que cedió a la provincia de Toledo los estados de La Adrada, Mombeltrán, Navamorcuende, Miranda y Oropesa. En resumen, en el año 1805 todos los municipios del actual partido judicial de Arenas de San Pedro pertenecían administrativamente a la provincia de Toledo.

En 1820 la provincia de Ávila quedó organizada en ocho partidos: Ávila, Arévalo, Madrigal, Peñaranda, Villafranca, Mombeltrán, Navamorcuende y Oropesa. En 1821 se puso en tela de juicio la continuidad de la provincia, sobre la base de su pequeño tamaño. En la reforma de 1822 algunas localidades pertenecientes al partido de Arévalo pasaron a pertenecer a Segovia y a Valladolid, mientras que algunos pueblos del distrito de Villacastín pasaron por contrapartida a pertenecer a Ávila.

En 1833, tras la muerte de Fernando VII, el país vuelve a sufrir una nueva reordenación provincial, llevada a cabo por el ministro de Fomento Javier de Burgos. En dicha división, que fijó la mayor parte de los límites actuales de las provincias en España, la provincia de Ávila queda dividida en seis partidos: Ávila, Barco de Ávila, Arenas de San Pedro, Piedrahíta, Arévalo y Cebreros.

El territorio de la provincia se vio perjudicado por la ocupación francesa durante la Guerra de Independencia —saqueos de Ávila, Arévalo y Arenas de San Pedro—. Durante la Primera Guerra Carlista también fueron frecuentes las partidas de carlistas; una de ellas, por ejemplo, saqueó Candeleda en 1836. La agricultura y la ganadería eran la base de la economía. El sector industrial —artesano— tenía poca importancia en el siglo . Es de reseñar la existencia de dos modelos productivos: en el norte un monocultivo cerealista, capaz de producir excedentes en buenos años pero más expuesto a la miseria y hambruna de la población durante las crisis y en el sur un aprovechamiento más variado del sector agrario-ganadero, menos expuesto a las hambrunas periódicas pero también con menos capacidad para crear excedentes, excepciones hechas del pimentón de Candeleda o de las judías del Barco.

Durante el régimen de la Restauración, la provincia se dividía en cuatro distritos uninominales para las elecciones a Cortes: Arenas de San Pedro, Arévalo, Ávila y Piedrahíta. Los escaños de dos de los cuatro distritos (Arenas de San Pedro y Piedrahíta) fueron principalmente copados por la familia Silvela.
No sería hasta 1965 cuando la provincia de Ávila adoptaría el número de partidos judiciales existentes en la actualidad, cuatro, los partidos de Arévalo, Arenas de San Pedro, Ávila y Piedrahíta. Esta demarcación quedaría fijada definitivamente por la Ley de Demarcación y de planta Judicial el 28 de diciembre de 1988.<ref name="Ley 38/1988, de 28 de diciembre, de Demarcación y de Planta Judicial"></ref>

La unidad administrativa básica en la que se divide la provincia son los municipios. Existen 248 en la actualidad. El municipio con más habitantes es la capital provincial. El resto de municipios no alcanzan la cifra de 10 000 ciudadanos empadronados. Es destacable un elevado número de ellos con poblaciones por debajo de los 500 habitantes. La extensión promedio del municipio en la provincia es de 32,46 km². Aparte de la capital provincial, entre las localidades destacan en cuanto a población Arévalo y Madrigal de las Altas Torres en la parte norte de la provincia (en la comarca tradicionalmente conocida como La Moraña). En el suroeste de la provincia sobresalen El Barco de Ávila, Piedrahíta y La Horcajada. En el más poblado sur de la provincia, en la vertiente meridional de la sierra de Gredos los municipios con más habitantes de la comarca del valle del Tiétar son los de Arenas de San Pedro, Candeleda, Sotillo de la Adrada, La Adrada, Piedralaves, Casavieja, Mombeltrán y El Arenal. En la parte este de la provincia, en zonas como el valle del Alberche y la Tierra de Pinares caracterizadas por una mayor cercanía a la capital del Estado, Madrid, destacan municipios como Las Navas del Marqués, El Tiemblo, Cebreros, Navaluenga, El Hoyo de Pinares, El Barraco o Burgohondo. De acuerdo al padrón municipal del INE los veinte municipios más poblados de la provincia en 2017 fueron:

La provincia de Ávila es la 17.ª de España en que existe un mayor porcentaje de habitantes concentrados en su capital (36,37 %, frente a 31,96 % del conjunto de España).

En la actualidad solo existen 2 entidades locales menores (la denominación con la que se conoce en la comunidad autónoma de Castilla y León a las entidades de ámbito territorial inferior al municipio). Se tratan de Balbarda y Blacha, ambas pertenecientes al municipio de La Torre.
Atendiendo a la administración judicial la provincia de Ávila está dividida en cuatro partidos judiciales. Esta organización fue fijada en 1988 con la Ley de Demarcación y de planta judicial. Las cabezas de cada partido son los municipios homónimos.

Dentro de la provincia múltiples municipios mancomunan entre sí obras y servicios como la recogida de basuras, el abastecimiento de agua, la protección civil, o el fomento del turismo entre otros. Nótese que cada municipio puede pertenecer a diferentes comunidades, ya que éstas no comparten los mismas funciones.

Las mancomunidades inscritas en el registro de entidades locales son las siguientes:



La provincia no cuenta con una división comarcal administrativamente funcional, pero la diputación realiza una «comarcalización» del territorio con objeto de la promoción turística.

Según la Diputación Provincial:

En 2015 la Junta de Castilla y León presentó un plan de organización del territorio, que refrendado por 2/3 de los procuradores de las Cortes, dividiría la comunidad autónoma en unidades básicas (UBOST) para la prestación de servicios y organización del territorio, adjuntando un borrador con las diferentes divisiones en cada provincia. El borrador para la provincia de Ávila incluía 20 unidades básicas de ordenación del territorio rurales y una urbana (el municipio de Ávila). Una de las UBOST rurales, Tierra de Arévalo, incluyó en el borrador municipios de la provincia de Segovia, mientras que una UBOST de la provincia de Segovia incluyó el municipio abulense de Maello.


La diputación es la institución a la que corresponde el gobierno y la administración de la provincia de Ávila. La sede de la diputación se encuentra en la ciudad de Ávila, en el edificio del Torreón de los Guzmanes. De acuerdo a los resultados de las elecciones de locales de 2019, la composición del pleno de la diputación —formado por un total de 25 diputados provinciales y establecido mediante elección indirecta a partir de los resultados en las elecciones de los municipios— para la legislatura 2019-2023 es de 12 diputados del Partido Popular, 8 del Partido Socialista Obrero Español, 4 de Por Ávila ,1 de C's

La provincia cuenta con una delegación territorial de la Junta de Castilla y León —el órgano de gobierno de la comunidad autónoma— situada en la ciudad de Ávila. La circunscripción electoral para las elecciones a Cortes de Castilla y León coincide con las provincias, eligiéndose los procuradores de la provincia de Ávila mediante un sistema proporcional de listas cerradas tipo D'Hondt.
Le corresponden por tanto a Ávila 7 procuradores en las Cortes de Castilla y León —el parlamento autonómico—.


La provincia también es circunscripción electoral para las elecciones generales en España. De acuerdo a la ley electoral a la provincia de Ávila le corresponden 3 escaños en el Congreso de los Diputados además de elegir 4 senadores.

De los 158 498 habitantes empadronados en 2018 en la provincia, aproximadamente un 36% vive en la capital, la ciudad de Ávila. La densidad de población de la provincia, de solamente 19,69 hab./km², es muy inferior a la del promedio del estado, 92,91 hab./km². 

A partir de las últimas décadas del siglo y hasta la década de 1630 se produjo un gran decrecimiento de la población, pudiendo llegar este a un 40 %. La fase de crecimiento que continuó a este fue débil y no exenta de recesos puntuales. En los albores de la Edad Contemporánea es probable que aún no se hubiera recuperado el máximo poblacional del siglo . Tras la Guerra de Independencia, la población aumentó de manera más significativa que en el periodo anterior.

Los municipios que cuentan con un mayor crecimiento debido a su cercanía con la Comunidad de Madrid son Sotillo, La Adrada, Las Navas del Marqués y Arenas de San Pedro. Otros, como Arévalo, crecen debido a la cercanía a Valladolid, la capital de Castilla y León.

El casco histórico de la ciudad de Ávila, en el que destaca su recinto amurallado medieval y su catedral gótica, figura desde 1985 en la lista de lugares calificados por la UNESCO como Patrimonio de la Humanidad.
En la provincia existían en 2009 un total de 97 ítems catalogados como bienes de interés cultural, por debajo en este aspecto de otras provincias de la comunidad. Entre ellos se encuentran castillos y fortificaciones como las Murallas de Ávila, la Muralla de Madrigal de las Altas Torres, el Castillo-Palacio de Magalia en Las Navas del Marqués, el castillo de la Triste Condesa en Arenas de San Pedro o el castillo de Manqueospese; yacimientos arqueológicos como los castros de Ulaca, el Raso o el de la Mesa de Miranda; y edificios religiosos como la catedral del Salvador en Ávila, o las iglesias de Santa María la Mayor y San Martín en Arévalo. Los cascos históricos de Ávila y de las localidades de Arévalo, Piedrahíta, Guisando y Bonilla de la Sierra reciben también la calificación de bien de interés cultural con la categoría de «conjunto histórico».
Entre los castillos construidos en la Edad Media y comienzos de la Edad Moderna en la provincia se encuentran las siguientes fortificaciones:
En la provincia de Ávila existe un notable número de verracos de piedra. Los verracos son esculturas zoomorfas que suelen representar principalmente toros o cerdos erigidas en un área donde predominaba la cultura vetona —un pueblo prerromano de origen celta de la Edad del Hierro— que comprendía buena parte de la meseta norte y del valle del Tajo en España y también en Portugal. Algunos ejemplos de estas esculturas en Ávila son:


La autopista A-6 de la red estatal —o autopista del Noroeste—, que comunica la capital del estado, Madrid, con el noroeste de España, tiene un tramo que discurre por el noreste de la provincia en dirección SE-NO. Pasa entre otras localidades por Arévalo o Sanchidrián. Sus tramos de peaje reciben la denominación AP-6. La autovía A-50 es otra vía rápida que comunica la capital de provincia con Salamanca, capital de la provincia homónima.

La provincia cuenta con servicios de tren de viajeros de media y larga distancia operados por Renfe Operadora, que comunican la capital provincial y otras localidades de la provincia, como Arévalo, con ciudades como Madrid y Valladolid.
El 1 de mayo de 2007 entró en vigor en la capital abulense, y en diversos municipios del sur de la provincia, al igual que en otros municipios segovianos, el abono de transportes de la Comunidad de Madrid

Dicho abono está formado por un título concertado con las líneas de autobús y tren que enlazan con la capital madrileña, más el título C2 que permite la movilidad por toda la red de transportes de toda la Comunidad de Madrid.

Entre sus platos más emblemáticos se encuentran las patatas revolconas, la sopa de ajo castellana, las judías del Barco de Ávila guisadas, el chuletón de Ávila, el cochinillo asado (denominado también lechón asado), el cocido, la morcilla de cebolla, los torreznos, los huevos rotos, la ternera abulense, el hornazo, la gallina en pepitoria, las manos de cerdo, la sopa de pan, los huevos al plato, el conejo a la cazadora (asado), la bolla de chicharrones y las famosas yemas de Ávila también conocidas como yemas de Santa Teresa.




</doc>
<doc id="8209" url="https://es.wikipedia.org/wiki?curid=8209" title="Casandra">
Casandra

En la mitología griega, Casandra (en griego antiguo, Κασσάνδρα: «la que enreda a los hombres» o «hermana de los hombres») era hija de Hécuba y Príamo, reyes de Troya. Casandra fue sacerdotisa de Apolo, con quien pactó, a cambio de un encuentro carnal, la concesión del don de la profecía. Sin embargo, cuando accedió a los arcanos de la adivinación, Casandra rechazó el amor del dios; este, viéndose traicionado, la maldijo escupiéndole en la boca: seguiría teniendo su don, pero nadie creería jamás en sus pronósticos. Tiempo después, ante su anuncio repetido de la inminente caída de Troya, ningún ciudadano dio crédito a sus vaticinios. Ella, junto con Laocoonte, fueron los únicos que predijeron el engaño en el caballo de Troya.

Apolo amaba a Casandra infinitamente pero, cuando ella no le correspondió, él la maldijo: su don se convertiría en una fuente continua de dolor y frustración, ya que nadie creería sus predicciones. En algunas versiones de este mito, Apolo escupe en su boca al maldecirla; en otras versiones griegas este acto suele suponer la pérdida del don recientemente adquirido, pero el caso de Casandra es diferente. En "Orestes" ella promete a Apolo que se convertirá en su consorte, pero no lo cumple, por lo que desata su ira.

Aunque Casandra previó la destrucción de Troya, la muerte de Agamenón y su propia desgracia, fue incapaz de evitar estas tragedias, tal era la maldición de Apolo. Su familia creía que estaba loca y, en algunas versiones, la mantuvieron encerrada en casa o encarcelada, lo que la hace enloquecer. En otras versiones, simplemente era una incomprendida.

Una vez concluida la guerra de Troya, durante el saqueo de la ciudad, Áyax, hijo de Oileo encontró a Casandra refugiada bajo un altar dedicado a Atenea. Aunque la princesa se agarró a la sagrada estatua de la diosa, (bien fuera el Paladio, bien otra estatua distinta), en el frenesí del saqueo Áyax desoyó los ruegos, y la arrastró junto con la estatua. Según algunas fuentes la violó en ese preciso lugar; para otras fuentes, el sacrilegio cometido por Áyax había consistido en no respetar la sagrada estatua de la diosa. Este hecho condenó al guerrero, pues Poseidón, impelido por la humillada Atenea, hundió su barco causando una tormenta en las cercanías del promontorio de las rocas Giras, donde Áyax murió ahogado, o clavado a las rocas por el tridente de Poseidón según otra variante de la leyenda.

Casandra fue entregada como concubina al rey Agamenón de Micenas. Este ignoraba que, mientras guerreaba en Troya, su esposa Clitemnestra se había hecho amante de Egisto. Cuando Agamenón y Casandra regresaron a Micenas, Clitemnestra le pidió a su marido que anduviera por encima de una alfombra morada, el color que simboliza a los dioses. A pesar de que Casandra le avisó reiteradamente que no lo hiciera, el rey la ignoró y cruzó la alfombra, cometiendo así un sacrilegio. Clitemnestra y Egisto asesinaron a ambos. En algunas versiones, Casandra y Agamenón habían tenido gemelos: Telédamo y Pélope. Ambos fueron asesinados también por Egisto.

Télefo, hijo de Heracles, también amaba a Casandra. Sin embargo, ella se burlaba de él y le ayudó a seducir a Laódice, hermana de Casandra. 

Hay versiones alternativas de la historia en las que Casandra, siendo niña, pasó la noche en el templo de Apolo con su hermano gemelo Héleno y las serpientes del templo chuparon y limpiaron sus orejas, por lo que ambos serían capaces a partir de entonces de oír el futuro. Este es un tema recurrente en la mitología griega. Otras versiones sugieren que Casandra consiguió la habilidad de entender el idioma de los animales, en lugar de conocer el futuro.

Casandra aparece en el quinto libro de Geoffrey Chaucer, "Troilo y Crésida" ("Troilus and Criseyde", 1385), como la hermana de Troilo. Este sueña un día que su amada Crésida está enamorada de un cerdo, y pide consejo a Casandra. Ésta interpreta correctamente el sueño y le dice que Crésida ya no lo ama porque ahora quiere a Diomedes, guerrero griego (uno de cuyos ancestros era famoso por haber matado un gigantesco y feroz jabalí: el Jabalí de Calidón). Debido a la maldición, Troilo no cree a Casandra.

El mito también fue abordado por la escritora alemana Christa Wolf en su obra "Kassandra". El libro cuenta la historia desde el punto de vista de Casandra en el momento de su muerte. 

El grupo sueco ABBA en el tema "Cassandra" habla sobre una persona que se arrepiente de no haberle creído sobre su poder y hace referencia a varios puntos del mito. El tema fue escrito por Benny Andersson y Björn Ulvaeus y aparece en The Visitors (álbum).

La autora Marion Zimmer Bradley escribió una novela de fantasía histórica llamada "La antorcha" (1987) que recuenta la "Ilíada", también desde el punto de vista de Casandra.

En la novela de Markus Sedwick "The Foreshadowing", Alexandra, el personaje principal, tiene el don de ver el futuro, aunque principalmente ve la muerte y el sufrimiento ajeno. Además, al crecer en la Inglaterra de la Primera Guerra Mundial, su poder es temido y puesto en duda. En la novela, ella lee el mito de Casandra y se da cuenta del paralelismo con su propia existencia.

En "Age of Bronze: Sacrifice", de Eric Shanower, Casandra es violada en su infancia por un malvado sacerdote que pretende ser un dios.

En la novela de Clemence McLearn "Inside the Walls of Troy", Casandra tiene una gran amistad con la reina Helena de Esparta cuando llegó a Troya con el príncipe Paris. Casandra odiaba a Helena con toda su alma, pero se rindió a su alegría y felicidad continua y se convirtió en su confidente. Al final de la historia, Casandra no es violada ni se va con Agamenón. Simplemente se queda con sus hermanas Políxena y Laódice en el templo de Atenea. El resto de la historia no se cuenta.

En la literatura moderna, Casandra es a menudo usada como modelo de tragedia y romance, y a menudo simboliza el arquetipo de alguien cuya visión profética es oscurecida por la locura, convirtiendo sus revelaciones en cuentos o afirmaciones inconexas que no son comprendidas plenamente hasta que ocurre lo vaticinado.

El «síndrome de Casandra» es un concepto ficticio usado para describir a quien cree que puede ver el futuro, pero no puede hacer nada por evitarlo. Por ejemplo, en la película "Doce monos". la doctora Kathryn Railly investiga este síndrome y a quienes lo sufren.

En la película de Woody Allen "Poderosa Afrodita", Casandra aparece como uno de los personajes, avisando al protagonista de su mal futuro.

El escritor argentino Roberto Mateo, en su novela "La impronta de Casandra", toma la idea original del mito y la modifica dándole otros matices; como ejemplo, Apolo no solo le da el poder de predecir el futuro sino que, a petición de ella misma, también recibe el don de la inmortalidad, el don de la palabra justa y el deseo de convertirse en protectora de los artistas de la palabra. Con este giro en el mito original, el autor de esta novela consigue traer a Casandra hasta la época actual, generándole encuentros a través de la historia con escritores que en algún momento de su obra la mencionan; por este paseo histórico Casandra conoce, influye y ampara a Eurípides, Esquilo, Schiller y Rossetti, al igual que al personaje central de esta novela, a quien conoce en última instancia.

El poeta español Ernesto Filardi trata el mito de Casandra en la pieza homónima del poemario "Penúltimo momento" (Madrid, Sial, 2005). En el poema se establece una identificación de Troya con una relación de amor acabada, mientras Casandra se identifica con la amada que desde tiempo atrás ya anunciaba que la relación no tenía futuro.

El grupo de metal gótico Theatre of Tragedy tiene a Casandra como protagonista del primer corte de su álbum Aégis.

El grupo de rock argentino de los 70 Sui Generis le dedica un tema al mito de Casandra titulado "El tuerto y los ciegos". Charly García es su autor y figura por primera vez en el disco "Pequeñas anécdotas sobre las instituciones", aparecido en 1974.

En el año 2007, el cantautor español Ismael Serrano compuso una canción llamada "Casandra" para su disco "Sueños de un hombre despierto". Así mismo, el compositor canario Pedro Guerra compuso una canción con el mismo título.

En 2013, la dramaturga mexicana Silvia Peláez escribe su obra teatral Visiones o el complejo de Casandra en la que la protagonista es una periodista de televisión, ambientalista, que tiene el poder de predecir eventos terribles pero nadie la toma en cuenta, hasta que, presionada por subir el índice de audiencia del programa, realiza un acción límite frente a las cámaras. La autora hace un paralelismo entre el Jefe de la periodista con Apolo, y el compañero de trabajo, Ayante, con Áyax. En una primera versión, la obra se presentó en lectura dramatizada en el "Teatro La Gruta del Centro Cultural Helénico" en 2013.











</doc>
<doc id="8210" url="https://es.wikipedia.org/wiki?curid=8210" title="Las Navas del Marqués">
Las Navas del Marqués

Las Navas del Marqués es un municipio de España perteneciente a la provincia de Ávila, en la comunidad autónoma de Castilla y León. Con habitantes en era el cuarto municipio más poblado de la provincia, tras la capital provincial, Arévalo y Arenas de San Pedro.

El escudo y bandera municipales fueron aprobados oficialmente por decreto el . El blasón en el que se basa el escudo heráldico municipal es el siguiente:
La bandera se define así:
El escudo procede de las antiguas armas de la Casa Dávila, que ostentaba el Marquesado de Las Navas y el Señorío de Villafranca.

El municipio, situado en el extremo oriental de la provincia de Ávila, limita al norte con El Espinar, Villacastín, Navas de San Antonio (Segovia) y Peguerinos (Ávila), al este con Santa María de la Alameda y Valdemaqueda (Madrid), al sur con San Bartolomé de Pinares y al oeste con Navalperal de Pinares (Ávila).

Rodrigo Méndez Silva, en el siglo XVII, escribe en su obra "Población de España":

Si bien la tradición sitúa el nacimiento de Las Navas en la época de Nabucodonosor II, por judíos, no existe ninguna prueba que lo atestigüe y la teoría se considera, en la actualidad, descabellada. Sí existen pruebas de asentamientos humanos durante la Alta Edad Media, probablemente pastores, que utilizaban estas tierras al tratarse de "tierra de nadie", enclavadas entre el norte cristiano y el sur musulmán.
Las navas debe su nombre al marqués Don Pedro de Dávila, que mandó construir el castillo-palacio de Magalia hacia el 1540.

Se puede decir que la historia de Las Navas, la historia del último siglo, se va conociendo poco a poco. Y mal. No hay un estudio serio. Hay estudios parciales. Tenemos la "Alcabala del Viento. Respuestas Generales del Catastro de Ensenada. Nº 50. Las Navas del Marqués", que se cita, más abajo como libro. Así como las repercusiones de la Desamortización de Mendizabal. Y en cuanto al hecho más relevante y trágico como fue la guerra del 36-39 se pasa de puntillas aún. Se debe sin duda a que a perdura en el recuerdo y algunos de los que participaron viven. Se sabe que por aquí estuvo la mítica Columna Mangada, fiel a la República y que se dieron combates encarnizados. El pueblo, como la mayoría de los pueblos, quedó dividido entre los partidarios de una parte y de la otra.

Las tropas nacionales de Rada y Angulo tomaron Las Navas el .

El término municipal de Las Navas del Marqués cuenta con dos edificios de interés histórico catalogados con la figura de protección de Bien de Interés Cultural. Se tratan del Palacio Castillo de Magalia y del Convento de Santo Domingo y San Pablo.

El Castillo-Palacio de Magalia es de estilo renacentista. Pedro Dávila y Zúñiga ordenó su construcción hacia 1540. Fue declarado monumento histórico-artístico nacional (antecedente de la figura de bien de interés cultural) el .

El convento fue fundado en 1545 sobre unos terrenos cedidos a la Orden de predicadores. Los muros son de cantería de granito grande pero irregular, salvo en la fachada con cadenas de sillares, contrafuertes y ángulos. Muestra en planta una sola nave con cinco tramos, crucero ligeramente destacado y cabecera poligonal. La decoración de la cabecera es sobria y de estilo herreriano. Fue declarado por Real Decreto bien de interés cultural el . Fue abandonado tras la desamortización de Mendizábal pasando a manos particulares tras la subasta que tuvo lugar en 1845.


Iglesia de tres naves. En su interior cuenta con un retablo barroco y un órgano del siglo XVII.




Torre construida en 1873 en hierro y madera, atribuida popularmente a Gustave Eiffel, se empleó como atalaya forestal.

La cercanía a Madrid y el clima más suave de Las Navas en verano han atraído a numerosas personalidades de la cultura. Así, en Las Navas solían veranear Aniceto Marinas, Eusebio García Luengo, Manuel Villegas López, José García Nieto, Luís Ponce, Rafael Montesinos, José Posadas, Fernando Quiñones, Víctor Ruiz Iriarte, Eugenio Mediano Flores, Martín Albizanda, Dámaso Santos, Charles David Ley, etc. Aquí descubrió su vocación poética Vicente Aleixandre de la mano de Dámaso Alonso, también veraneante. En Las Navas terminó Camilo José Cela la novela "Pabellón de reposo" así como numerosos artículos, algunos sobre el pueblo, siendo multitud las anécdotas dejadas por el mismo ligado al conocido carácter de Cela. En Las Navas escribió Juan Antonio Bardem el guion de su película "Calle Mayor". Y Las Navas ha sido refugio de varios pintores: Evaristo Guerra, Manuel Calvo, Ángel Aragonés. Y tienen su residencia, casi habitual, Agustín García Calvo, Isabel Escudero o Fanny Rubio. Todos ellos han atraído hacia el pueblo a numerosas personalidades de la cultura y han contribuido a enriquecer la vida cultural de la villa. También veraneaba en el municipio la Infanta Eulalia, hija de Isabel II, aunque esta tenía el chalet en la Ciudad Ducal.

Entre los pintores que registraron su paso por Las Navas del Marqués está el valenciano José Garnelo y Alda (Enguera 1866-Montilla 1944) que dejó para la posteridad una de sus obras más conseguidas la "Capea en Las Navas del Marqués". Otro artista singular que dejó huella de su talento en Las Navas fue el escultor segoviano Aniceto Marinas que esculpió los dos cristos de estilo clásico que se veneran en el pueblo el Cristo de Gracia de la Ermita del Santísimo Cristo de Gracia, y el Cristo de la Salud con altar en la Iglesia de San Juan Bautista (Las Navas del Marqués).

Es esta una tradición que viene de antiguo pues ya en el siglo XVI, Lope de Vega, escribió una comedia titulada "El Marqués de Las Navas" cuyo protagonista es Pedro Dávila, de cuyo padre fue secretario el poeta, donde se dice:

También Benito Arias Montano, en el mismo siglo que Lope de Vega, visitó Las Navas del Marqués (concretamente en el verano 1567) donde escribió la obra "Comentario al profeta Oseas", terminando el escrito con un poema en latín donde recuerda a Las Navas:
Benito Arias Montano termina su obra, con estas palabras:

Cuya traducción es: ""Las Navas del Marqués, en el campo abulense, junto al Monasterio de San Lorenzo 'El Escorial', en el mes de agosto de 1567""

No solo de visitantes se nutre la orla de personalidades de Las Navas del Marqués, también hay algunos naveros que han destacado, y destacan, en algún campo: junto a Tomás García Yebra, periodista que ya lleva escritos dos libros sobre la historia de Las Navas, está otro periodista destacado que murió en un accidente de aviación, Manuel de Dompablo y Bernaldo de Quirós . Fue natural de Las Navas el Maestre de Campo y Caballero de Santiago Pedro Esteban Dávila, Gobernador del Río de la Plata desde 1631 a 1637. En la guerra del 36-39, un personaje que no se ha estudiado en absoluto, el guerrillero Florencio Moral 'El Murallas' que tuvo en jaque al ejército rebelde a la República durante un tiempo. Y hoy en día el escultor Javier Herranz Arcones o los artesanos Antonio Segovia y Mariano Rodríguez 'Cacha' que ya tienen en su haber algunas exposiciones de sus obras. Periodistas en ejercicio ha dado Las Navas del Marqués, como Félix Rosado, redactor en Diario de Ávila, Estrella Digital y Madrid Press, y autor de varios relatos, como Álvaro Mateos, redactor jefe de la Cadena COPE en Castilla-La Mancha. También podemos destacar a Sobanski, una figura en ciernes de la fotografía actual. Otro navero famoso en los últimos años es José Ignacio Salmerón, más conocido como Sinacio, conocido cómico y guionista, entre otras facetas. José Antonio de Segovia Botella, ciclista profesional y ganador del campeonato de España de contrarreloj en varias ocasiones.



Las fiestas patronales se celebran en la semana del segundo domingo de julio y están dedicadas al Cristo de Gracia. Durante cinco días todo el pueblo se lanza a la calle y disfruta de las tradicionales actividades, procesiones en honor al Santísimo Cristo de Gracia, Verbenas hasta altas horas de la madrugada, corridas de toros, concurso de potes naveros, el día de las tapas y otros actos con gran participación.

Además, la semana del segundo domingo de agosto, se celebra la conocida como "Semana Grande", en honor del Cristo de la Salud, con romería el día 15 al sitio denominado "El Valladar" y a la ermita del Barrio de la Estación.

La tradición folclórica es compartida con el resto de Castilla y así es popular la jota. Sin embargo, existe como peculiaridad el conocido como "Baile de Tres", que lo baila un hombre con dos mujeres y tiene por música "el Gerineldo". Romance medieval del que se conservan numerosas versiones, entre ellas una versión sefardí, en Tanger. La versión navera comienza así:

Destacan las patatas revolconas conocidas en el pueblo como pote navero que es un guiso a base de patatas y torreznos con pimentón , las morcillas y los cachuelos. Como en toda la provincia existe una buena variedad de carne de ternera, chuletones y solomillos asados.

Se pueden destacar muchas rutas, todas ellas en constante contacto con la naturaleza.Entre ellas se pueden destacar:







</doc>
<doc id="8211" url="https://es.wikipedia.org/wiki?curid=8211" title="Sierra de Malagón">
Sierra de Malagón

Sierra de Malagón es una sierra situada geográficamente dentro de la sierra de Guadarrama, perteneciente ésta a su vez al sistema Central de la península ibérica.

Al sur y oeste del puerto de Guadarrama se levanta el bloque de Malagón, una morfoestructura de forma cuadrangular que constituye el elemento principal de transición o enlace con el gran conjunto montañoso de Gredos y las Parameras de Ávila.

El borde oriental de este complejo relieve presenta gran continuidad con la alineción de La Peñota y se define como un horst que se eleva sobre el piedemonte meridional del Sistema Central siguiendo fallas de dirección NNE. Desde la citada Peñota hacia el sur, cumbres y laderas son graníticas, pero al sur de Cuelgamuros son los gneises del afloramiento metamórfico de El Escorial-Villa del Prado los que forman las sierras de San Juan (1735 m) y Abantos (1757 m). En ellos se ha reconocido la falla de Santa María de la Alameda, otro de los grandes cabalgamientos hercínicos. En los flancos de los pliegues hercinianos, marcado en estos gneises se han señalado algunas formas secundarias de relieves, pero estas alineaciones son fundamentalmente horsts disimétricos que se elevan sobre las rampas del piedemonte de El Escorial y enlazan con las superficies culminantes del dorso de Malagón. Al su del El Escorial el pequeño horst de las Machotas constituye un relieve granítico semidomático de topografía muy intrincada. Morfoestructuras similares aparecen al sur, jalonando también el bloque de Malagón: se trata de una serie de "morros" graníticos y la larga alineación de la "Almenara" (1259 m) que constituye el borde oriental de la fosa de Robledo de Chavela.

El borde septentrional del bloque de Malagón forma parte de la sierra del mismo nombre, que como la anterior es disimétrica; en este caso, el borde norte del bloque es un escarpe que se eleva sobre las fosas de El Espinar y el Voltoya. El máximo levantamiento se localiza en el sector oriental, donde esta sierra enlaza con el Guadarrama propiamente dicho en Cabeza Líjar (1824 m), culminando en Cueva Valiente (1904 m). La vertiente que cierra por el sur las depresión de El Espinar es la más elevada y escarpada, apareciendo cortada trnsversalmente por fallas NE. Al este la vertiente avanza hacia el norte en el contrafuerte de la Cabeza Renales (1757 m) que sirve de límite entre esta fosa y la del Voltoya de dirección E-O. Esta cabeza sirve, por tanto, de límite a las dos fosas y, al sur de ella, el relieve de Malagón va perdiendo altura. Las citadas fosas quedan cerradas al norte por un conjunto de cerros que las aíslan de las rampas del piedemonte: los del "Caloco" (1567 m) y "Rinconada" (1358 m) cierran la depresión de El Espinar; los de "Peña Morena" y "Atalaya" (1506 m), la del Voltoya. A poniente de esta fosa la pequeña sierra abulense de Ojos Albos (Cruz de Hierro 1660 m), constituida fundamentalmente por cuarcitas y pizarras ordovícicas, cierra la depresión a lo largo de otra fractura NE, la falla de la Paramera de Ávila-Cruz de Hierro.

El llamado "dorso" del bloque de Malagón constituye una especie de plano inclinado hacia el SSO, modelado en sus áreas culminantes por superficies de erosión y articulado internamente por un conjunto de fracturas de tendencia ortogonal. A lo largo de las más activas se han producido desniveles que no han roto la fisonomía general de las superficies de erosión, y que han sido resaltados por los ríos que se encajan en gargantas como las del Cofio y las del río de la Aceña.

Termina al oeste de la falla N-S de El Herradón y, al sur, con la depresión de El Tiemblo-Cebreros- San Martín, cortada por el río Alberche (que la atraviesa para salir a la cuenca del Tajo, prolongándose hasta las Parameras de Ávila, con las que enlaza por la Cuerda de los Polvisos

La falda norte de la Sierra de Malagón es suave en unos puntos y quebrada en otros, pasando rápidamente a la gran llanura de Campo Azálvaro, ya en tierras segovianas. Su vertiente sur, por el contrario, tiene más extensión. Sus contrafuertes, alineados de norte a sur, van perdiendo altitud hasta alcanzar prácticamente la margen izquierda del río Alberche. En estas estribaciones se suceden ásperas cumbres, fuertes pendientes y escasas llanuras, con espacios suaves entre collados, cerros y cañadas que toman el nombre de hoyos o navas. Multitud de vallejos y pequeños arroyos surcan el término, aportando sus irregulares caudales al Cofio.



</doc>
<doc id="8215" url="https://es.wikipedia.org/wiki?curid=8215" title="Los autos locos">
Los autos locos

Los autos locos (en inglés Wacky Races) es una serie de dibujos animados de la productora estadounidense Hanna-Barbera Productions sobre un grupo de 11 coches de carrera que compiten entre sí en diferentes carreras, con sus pilotos intentando ganar el título de «Piloto más loco del mundo». La serie era inusual por el gran número de personajes habituales, 23 en total.

Esta serie fue inspirada por la película "La carrera del siglo" (1965). Se emitió originalmente en el canal de televisión estadounidense CBS entre el 14 de septiembre de 1968 y el 5 de septiembre de 1970. Se produjeron diecisiete episodios, cada uno de los cuales incluía dos carreras distintas, haciendo pues un total de 34 carreras.

Entre los corredores estaban el estereotipado villano propio de la serie, Pierre Nodoyuna y su secuaz el perro Patán. Nodoyuna lograba una gran ventaja y entonces, como el Coyote en los dibujos animados de El Coyote y el Correcaminos, llevaba a cabo todo tipo de elaborados ardides para hacer que los demás corredores cayeran en trampas, se desviasen, pinchasen o se detuvieran, sólo para ver cómo le salía el tiro por la culata espectacularmente. La lección a aprender podría haber sido que Pierre contaba con uno de los coches probablemente más rápidos de la serie y habría ganado varias carreras si se hubiera concentrado en ellas, en lugar de preparar las trampas. Como Wile E. Coyote, Pierre Nodoyuna jamás ganaba. Muchos de los planes de Pierre son sospechosamente familiares a los usados en episodios del Correcaminos, lo que puede deberse al hecho de que Mike Maltese fuese guionista de ambas series.

Uno de los planes originales para la serie era que las propias carreras serían parte de un concurso de televisión real con Merrill Heatter y Bob Quigley, el equipo responsable de la serie de televisión "Hollywood Squares". El plan de Heatter y Quigley era que los concursantes apostasen a qué auto loco cruzaría primero la línea de meta.

Los once participantes con sus números eran (entre paréntesis se indican los nombres originales):


Anualmente, en el de Inglaterra, se exhiben réplicas bastante bien elaboradas de los autos locos.
El programa mostraba el resultado de las carreras al final de cada episodio, pero nunca hubo un determinado sistema de puntuación general. Si se utilizara el sistema actual del COI, este sería el orden de los resultados:

El personaje de Penélope Glamour protagonizó en 1969 una secuela, "Los peligros de Penélope Glamour". También en 1969, Pierre Nodoyuna y Patán aparecieron en su propia secuela, "El escuadrón diabólico", donde eran aviadores de la Primera Guerra Mundial y trataban de capturar a un palomo mensajero enemigo.

La idea básica en la que se basaban "Los autos locos" fue usada de nuevo por Hanna-Barbera en años sucesivos. A finales de los años 1970, la serie "La carrera espacial de Yogi" estaba protagonizada por personajes de Hanna-Barbera como el oso Yogui, Huckleberry Hound y otros que competían entre ellos por el espacio exterior (y daban esquinazo a un villano y su secuaz canino). A principios de los años 1990, la serie sindicada "Wake, Rattle and Roll" incluía un segmento titulado "Fender Bender 500", que una vez más protagonizaban Pierre Nodoyuna y Patán (y su "Súper Ferrari Especial"), esta vez compitiendo contra el oso Yogui, Winsome Witch y otras estrellas de Hanna-Barbera, en esta serie si se ven ganar algunas de las carreras en las que participan.

Sobre el año 2000, se produjo un videojuego basado en la serie de televisión para PS1 y Dreamcast.

En el año 2017, se produjo un reboot basado en la serie de televisión.



Se produjeron diecisiete episodios, cada uno incluía dos carreras distintas, haciendo un total de 34 carreras.
Volumen 1 

11 episodios


Volumen 2 

11 episodios 


Volumen 3 

12 episodios




</doc>
<doc id="8216" url="https://es.wikipedia.org/wiki?curid=8216" title="The Great Race">
The Great Race

The Great Race es una película estadounidense de 1965, del género comedia, dirigida por Blake Edwards, protagonizada por Jack Lemmon, Tony Curtis, Natalie Wood, Peter Falk, Keenan Wynn y Vivian Vance en los papeles principales. El guion fue escrito por Blake Edwards y Arthur A. Ross, con la música de Henry Mancini y la dirección de fotografía de Russell Harlan y estaba basado en la carrera: 1908 New York to Paris Race. Obuvo el (Treg Brown).

A principios del siglo XX, se celebra una loca carrera de coches de carácter internacional. Los participantes salen de Nueva York y la meta está en París. En medio, multitud de disparates y situaciones rocambolescas tienen lugar.
El coche de Tony Curtiss, el "Leslie Special", fue construido inspirándose en el Thomas Flyer, el coche que ganó la carrera de Nueva York a París en 1908. De acuerdo con el Petersen Automotive Museum, cuatro "Leslie Specials" fueron construidos para la película. Uno de ellos está expuesto en el Tupelo Automobile Museum en Tupelo (Misisipi).
Otro fue pintado de verde oscuro para aparecer en la película de 1970 "La balada de Cable Hogue"— es el coche que aparece justo al final y aun conserva las letras "Leslie Special".

El coche del villano se bautizó como "Hannibal Twin-8"; se montaron ocho. Uno está en el Petersen Automobile Museum, otro está en el Volo Auto Museum en Illinois.

Ambos vehículos estuvieron previamente expuestos en el museo Movie World's "Cars of the Stars" en Buena Park, California, hasta que fue cerrado a finales de los 70s.

La escena de la pelea de tartas en la pastelería real fue filmada a lo largo de cinco días. Durante ese tiempo se lanzaron más de 4,000 tartas, es la mayor pelea de tartas de la historia del cine. La escena dura algo más de cuatro minutos y costo US$200,000.

Se usaron tartas decoradas con grosellas, arándanos, fresas y limón.
Edwards les dijo a los actores que, por sí sola, la pelea no tenía gracia, la gracia estaba en crear tensión manteniendo inmaculada la ropa blanca de Leslie hasta el momento adecuado.

El rodaje de la escena se interrumpió el fin de semana y cuando volvieron el lunes, los residuos de tarta apestaban tanto que el plato tuvo que ser desinfectado.

Al principio los actores lo encontraron divertido, pero al final estaban hartos. Wood se atragantó con una tarta y Lemmon dijo que "cuando una de esas tartas te da en toda la cara es como una tonelada de cemento". Al final de la toma, cuando Edwars dijo "corten!", fue acribillado por cientos de tartas que el equipo había reservado especialmente para ese momento.

Toda la escena es un homenaje a Mack Sennett y otros artistas del cine mudo que usaron las peleas de tartas en sus comedias, como Charlie Chaplin; Stan Laurel y Oliver Hardy; y the Three Stooges.

Antes de que la película fuera oficialmente lanzada, la banda sonora fue pre-grabada en Hollywood por RCA Victor Records para lanzamiento en vinilo LP. Henry Mancini estuvo seis semanas componiendo la música, y las sesiones de grabación englobaban 80 músicos. Mancini colaboró con el letrista Johnny Mercer en varias canciones incluyendo "The Sweetheart Tree", un vals lanzado como single. La canción suena repetidamente en la película como tema principal instrumental (La versión a coro suena en el intermedio que en algunas ediciones no está incluido) y es interpretada en pantalla por Natalie Wood con la voz doblada por Jackie Ward (no acreditada).[20] La canción fue nominada al Oscar a la mejor canción original aquel año. Tres temas recurrentes aparecen en la película: El primero o tema principal aparece como balada, con banjo o con un estilo de 1900's. El segundo es "The great Race march" con las tres primeras notas del himno americano. Y el tercer tema, es la melodía del profesor Fate. Banda sonora:








</doc>
<doc id="8219" url="https://es.wikipedia.org/wiki?curid=8219" title="Henry Kissinger">
Henry Kissinger

Henry Alfred Kissinger, registrado al nacer como Heinz Alfred Kissinger (Fürth, Baviera, 27 de mayo de 1923), es un político estadounidense de origen judeoalemán que tuvo una gran influencia sobre la política internacional, no solo de Estados Unidos con respecto a los demás países, sino también sobre otras naciones. Ejerció como secretario de Estado durante los mandatos de Richard Nixon y Gerald Ford, desempeñando este papel preponderante en la política exterior de Estados Unidos entre 1969 y 1977, y fue consejero de Seguridad Nacional durante todo el mandato inicial del primero.

Kissinger se caracterizó por llevar las riendas de un proceder internacional fuerte pero al mismo tiempo negociador, siendo el artífice de la denominada «política de distensión» con la Unión Soviética y China, país con el cual logró, durante el mandato de Richard Nixon, consolidar relaciones pacíficas.

Tuvo que hacerse cargo de poner fin a la muy criticada Guerra de Vietnam y gestionar la crisis de la Guerra de Yom Kippur, concibiendo una nueva visión de como llevar la política exterior estadounidense, al colocar como último recurso la intervención militar, siendo este nuevo proceder el que lo llevó a obtener el Premio Nobel de la Paz en 1973, gracias al alto al fuego que logró establecer en Vietnam.

Aun así, la controversia ha persistido sobre su figura, debido a la intervención de la CIA en varios Golpes de Estado sucedidos en Latinoamérica durante la década de 1970. Sus críticos lo consideran instigador de genocidios sistemáticos de grupos políticos, estando ligado a varios regímenes autoritarios latinoamericanos, tales como la dictadura militar chilena de Augusto Pinochet o el Proceso de Reorganización Nacional de Argentina, así como por ser el responsable de planes represivos como lo sería la Operación Cóndor, cuya célula de origen habría sido la Escuela de las Américas. Todo esto ha ocasionado que existan numerosas iniciativas que persiguen conseguir su procesamiento ante instancias judiciales internacionales, así como la retirada de su Premio Nobel.

Ha pasado a actuar desde el sector privado, fundó la Kissinger Associates, y es accionista y cofundador de la Kissinger & McLarty Associates, así como miembro de las juntas directivas y asesor de las empresas The Hollinger Group y Gulfstream Aerospace. Además es rector de la Universidad de Georgetown y sirvió en Indonesia como Asesor General de Gobierno.

En el 2001, Kissinger fue llamado por el gabinete de George W. Bush para liderar un comité de crisis internacional a causa de los ataques del 11-S así como para que a través de su firma prestase asesoría diplomática y política al gobierno, no obstante Kissinger se retiró poco después de este proyecto.

Henry Kissinger es por mucho una de las figuras políticas y de la diplomacia más relevantes de la Historia de los Estados Unidos, tanto como controvertida. Si bien sus méritos en la política internacional son notables (apertura de relaciones con la URSS, China, entre otros), su negativa a devolver el Premio Nobel de la Paz que recibió gracias al alto al fuego que hubo en la Guerra de Vietnam y que posteriormente se rompió, así como las decenas de acusaciones de colaborar e incluso promover regímenes dictatoriales y acciones terroristas en diferentes partes del mundo, que cometieron severas violaciones a los Derechos Humanos, han ocasionado que su persona haya sido duramente criticada desde numerosas entidades tanto como por personalidades de la política o intelectuales, siendo algunos de los más conocidos el juez español Baltasar Garzón, asesor del Tribunal de la Haya, quien intentó fallidamente procesarlo por violaciones a los Derechos Humanos, y el periodista y escritor Christopher Hitchens, autor del best-seller "Juicio a Kissinger".

Henry Kissinger también ha recibido críticas por ser uno de los miembros fundadores del polémico y todavía activo Grupo Bilderberg, entidad no gubernamental, en la que se reúnen varias de las personas más poderosas e influyentes de todo el mundo, incluyendo monarcas, aristócratas, políticos, empresarios y magnates.

Heinz Alfred Kissinger nació en Fürth, Baviera, en una familia de judíos alemanes. Su padre, Louis Kissinger (1887-1982), fue un maestro de escuela; su madre, Paula Stern Kissinger (1901-1998), fue ama de casa. Kissinger tiene un hermano menor, Walter Kissinger. El apellido Kissinger fue adoptado en 1817 por su tatarabuelo Löb Meyer y hace referencia a la ciudad de Bad Kissingen. En 1938, huyendo de la persecución nazi, su familia se mudó a Nueva York. Kissinger pasó sus años de escuela secundaria en la sección de "Washington Heights" del alto Manhattan como parte de la comunidad de inmigrantes judíos alemanes allí. Aunque Kissinger rápidamente se asimiló a la cultura estadounidense, nunca perdió su pronunciado acento alemán, debido a la timidez infantil que le hizo reacio a hablar. Después de su primer año en la "George Washington High School", comenzó a asistir a la escuela por la noche y trabajó en una fábrica de brochas de afeitado durante el día.

Después de la escuela secundaria, Kissinger se inscribió en el City College de Nueva York, para estudiar contabilidad y además estudió Ciencias Políticas en la Universidad de Harvard, sobresaliendo académicamente como estudiante a tiempo parcial, y continuó trabajando mientras estudiaba, desempeñándose como profesor de la propia Harvard, de la cual además recibió la mención honorífica de Summa Cum Laude al graduarse en 1950 y posteriormente obtiene de la misma institución su Maestría y también su PhD en 1952 y 1954 respectivamente.

En 1952, estando todavía estudiando en Harvard, se desempeñó como Asesor de la Junta de Estrategia Psicológica. Elaboró su tesis doctoral acerca de las hazañas como estadistas de Castlereagh y Metternich, titulada "Paz, Legitimidad y Equilibrio".

Permaneció en Harvard, como miembro de la facultad en el Departamento de Gobierno y desempeñándose como profesor y catedrático. Además de 1956 a 1958 trabajó como Director del Proyecto de Estudios Especiales, el cual fue creado por él mismo y avalado por la "Rockefeller Brothers Fundation". Fue Director del programa de estudios de defensa de Harvard entre 1958 y 1971. También fue Director del Seminario Internacional de Harvard entre 1951 y 1971.

Además, como parte de su carrera, pasó a actuar como consultor, asesor y miembro de juntas directivas de variedad de empresas, de las cuales, la más sonada en sus inicios y donde actuó como asesor fue la Corporación RAND, una compañía de producción industrial cuyo cliente más importante era nada menos que el Ejército de los Estados Unidos, estando esta compañía estrechamente ligada con el gobierno y con múltiples programas de desarrollo tecnológico y armamentístico, todas estas cuestiones le valían a la misma el ser acusada de ser una organización militarista y frecuentemente ser involucrada en todo tipo de teorías de conspiración y acusaciones de «supuestos planes secretos» para fines bélicos.

Sus estudios se vieron interrumpidos a principios de 1943, cuando fue reclutado por el Ejército de Estados Unidos ante la entrada del país a la Segunda Guerra Mundial.

Kissinger recibió su formación militar básica en el "Campamento Croft" en Spartanburg, Carolina del Sur, donde fue nacionalizado estadounidense a su llegada. El ejército lo envió a estudiar ingeniería en el "Lafayette Collegede" en Pennsylvania, pero el programa fue cancelado y Kissinger fue reasignado a la 84.ª División de infantería. Allí hizo amistad con Fritz Kraemer, un oficial e inmigrante de su natal Alemania, con el que estableció una gran amistad, a pesar de la diferencia de edad, además de ser quien le señaló a Kissinger su gran inteligencia y fluidez con el alemán, talentos que Kraemer le incentivó a usar en su favor y que aprovechó para insertar al joven Henry Kissinger en la sección de "Inteligencia Militar de la División", para la cual desempeñó la misión de encargarse de las tareas de inteligencia de más riesgo durante la Batalla de las Ardenas.

Durante el avance estadounidense sobre Alemania, Kissinger fue de gran utilidad, siendo asignado a la "Bekennende" de la ciudad de "Krefeld", debido a la falta de traductores de alemán en el personal de Inteligencia de la División. Kissinger se basó en su conocimiento de la sociedad alemana para ir destituyendo y sacando a los nazis de los cargos civiles, así como para salvaguardar a las tropas de los espías del gobierno del Tercer Reich y así restaurar una administración civil eficiente en la ciudad, una vez derrotadas las fuerzas nazis, una tarea que se realizó durante 8 días.

Posteriormente, Kissinger, fue reasignado a los "Cuerpos de Inteligencia", ahora con el rango de sargento. Se le colocó a cargo de un equipo en Hannover, el cual había sido asignado para hacer un seguimiento de oficiales de la Gestapo y otros saboteadores, lo que llevó a cabo con éxito, siendo condecorado con la "Estrella de Bronce".

En junio de 1945, Kissinger fue nombrado comandante de un destacamento de la CIC en el distrito de Bergstraße en Hesse, con la responsabilidad de «desnazificar» el distrito, para lo cual se le dio plenitud de poderes en los ámbitos civil y militar. No obstante; aun cuando poseía autoridad absoluta y poderes de arresto, Kissinger tuvo cuidado de evitar abusos contra la población local por parte de su comando.

En 1946, Kissinger fue reasignado a la "Escuela de Inteligencia del Comando Europeo" en el "Campamento de Rey", para ejercer como profesor y adiestrar a los nuevos cuerpos de "Inteligencia Militar", llegando a servir en este papel como empleado civil incluso después de su separación del ejército.

Sus cargos académicos y sus conexiones políticas lo lleva a formar parte del Partido Republicano y a comenzar a ascender en la escena política nacional.

En 1955, se convierte en "Asesor del Consejo Nacional de Seguridad" y de la "Junta de Coordinación de Operaciones de Seguridad". En 1955 y 1956, fue también Director de Estudio en las Armas Nucleares y la política exterior en el "Consejo de Relaciones Exteriores". Publicó su libro de las armas nucleares y la política exterior al año siguiente. De 1956 a 1958 trabajó como director de su "Proyecto de Estudios Especiales" avalado por la "Rockefeller Brothers Foundation". Fue Director del programa de estudios de defensa de Harvard entre 1958 y 1971. También fue Director del seminario internacional de Harvard entre 1951 y 1971. Fuera de la academia, se desempeñó como consultor de varios organismos del Gobierno, incluyendo la "Oficina de Investigación de Operaciones", el Control de armas y desarme y el "Departamento de Estado" y la "Corporación RAND", una compañía de desarrollo industrial, tecnológico y armamentístico.

Deseoso de tener una mayor influencia en la política estadounidense, Kissinger fue partidario y asesor de Nelson Rockefeller, gobernador de Nueva York, que buscó la nominación del Partido Republicano para Presidente en 1960, 1964 y 1968; sin embargo, Kissinger obtendría su tan anhelado ascenso político, con «el candidato más improbable» de ese partido.

Richard Nixon, quien se vio reforzado tras el asesinato tanto de John F Kennedy como de Robert Kennedy, y por el desastre en el que estaba resultando la Guerra de Vietnam, condujo una campaña fuerte y exitosa, ganando la Presidencia en 1968.

Richard Nixon, convencido de las habilidades de Henry Kissinger, lo nombra Consejero de Seguridad Nacional, dejándole así la puerta abierta para comenzar su carrera en la alta política y poniéndolo al frente de todo lo referente a orden interno, seguridad y demás cuestiones referentes al ámbito.

Kissinger, pasará a desempeñar a plenitud su cargo, y se ocupará de organizar todo lo referente a la defensa y se convertirá en el asesor más cercano del presidente Nixon, así como en uno de sus más cercanos aliados y también en el único dentro del gabinete y en general de toda la Administración de Richard Nixon, que era capaz de enfrentarse de frente al presidente, aun cuando eran bien sabidos los desencuentros entre ambos.

Pero Kissinger no solo se limitó a cumplir sus funciones, sino que pasó a involucrarse en prácticamente todas las actividades del gobierno de Richard Nixon, algo que lo convirtió en indispensable para el propio presidente, quien frecuentemente hacía cambios completos de gabinetes y de directivas pero era incapaz de destituir a Kissinger, dado que aun cuando su cargo y su sueldo no le remuneraran ni le otorgaran responsabilidades sobre decenas de ámbitos, Kissinger se involucraba de lleno en cada asunto de la política estadounidense.

Robert Greene explica en su libro las 48 Leyes del Poder, esta estrategia que Kissinger empleó para prevalecer de la siguiente manera:

Kissinger, por otro lado tuvo que hacer frente a las protestas, manifestaciones y demás muestras de rebeldías por parte de las comunidades pacifista y de los sectores estudiantiles, obsesionados con el fin de la Guerra de Vietnam, así como también se encargó de tomar cartas sobre este asunto aconsejando a Nixon al respecto. Igualmente, como Asesor de Seguridad Nacional, en 1974, Kissinger dirigió el muy debatido 200° Memorando de Estudios de Seguridad Nacional

Posteriormente, Nixon se lanzaría en un intento de ser reelecto, obteniendo la nominación del Partido Republicano, con bastante ventaja y holgura, pasando a ganar las elecciones de 1972. Para el momento, Kissinger era uno de los políticos y personalidades más poderosas del gobierno y dentro del Partido Republicano. Su apoyo a la candidatura de Nixon fue crucial; con ella, la mayoría de los miembros del partido respaldaron al presidente Richard Nixon y gracias a las políticas aplicadas y sugeridas por Kissinger, gozaba de una considerable popularidad y aceptación, todos elementos que favorecieron la aspiración de reelección de Nixon.

Así, Richard Nixon, una vez reelecto, en 1972, premió a Kissinger con la Secretaría de Estado de los Estados Unidos, puesto con el cual Kissinger acabaría de grabar su nombre en la historia.

Kissinger había sido Asesor de Seguridad Nacional durante prácticamente todo el primer gobierno del presidente Richard Nixon, y continuó ejerciendo dicha posición hasta el final del mismo, tras lo cual su posición oficial sería únicamente la de Secretario de Estado, ejerciendo así este cargo desde de 1973 hasta 1977. Kissinger ya había tenido una implicación amplia en los asuntos internacionales desde su anterior puesto, pero ahora con la secretaría encargada del ámbito, pasaría a «gobernar la política internacional» al máximo.

Apenas asumió su nueva posición, Kissinger se dispuso a cambiar el enfoque que hasta ahora se le había venido dando a la política internacional, asumiendo un papel, no de consejo y obediencia como normalmente se entendería, sino dominante, fuerte y hasta cierto punto autosuficiente, respecto a la política exterior de Estados Unidos entre 1969 y 1977.

En ese período, extendió la política de distensión. Esta política llevó a una relajación significativa en las tensiones con la Unión Soviética y desempeñó un papel crucial en 1971 las conversaciones con el primer ministro chino Zhou Enlai. Las conversaciones concluyeron con un acercamiento entre Estados Unidos y la República Popular de China y la formación de un nuevo alineamiento estratégico chino-estadounidense antisoviético. Fue condecorado con el Premio Nobel de la Paz de 1973 por ayudar a establecer una cesación del fuego y el retiro de los Estados Unidos de Vietnam. El alto el fuego, sin embargo, no fue duradero. Mientras que el otro premiado, el vietnamita Lê Ðức Thọ, devolvió su premio por considerar que no lo merecía por haber retornado la guerra, Kissinger no lo devolvió.

Kissinger favoreció el mantenimiento de relaciones diplomáticas amistosas con las dictaduras militares de derechas en el Cono Sur y otras partes de Latinoamérica y más aún, está acusado de planificar el asesinato de una gran cantidad de militantes de izquierda en Chile, y posteriormente, en la Argentina y el Uruguay, así como de promover y respaldar a las propias dictaduras y desarrollar la "Academia de las Américas", y la Operación Cóndor.

Ya como Asesor de Seguridad Nacional bajo el primer gobierno Nixon, Kissinger fue pionero en la política de distensión con la Unión Soviética, que buscaba una reducción de las tensiones entre las dos superpotencias. Como parte de esta estrategia, llevó a cabo las conversaciones de limitación de armas estratégicas (que culminaron en el "Tratado SALT I" - Strategic Arms Limitation Talks) y el Tratado sobre misiles anti-balísticos con Leonid Brezhnev, Secretario General del Partido Comunista soviético. Las negociaciones sobre el desarme estratégico originalmente debían iniciar bajo la administración de Lyndon Baynes Johnson, pero fueron pospuestas en protesta por la invasión de Checoslovaquia, realizada por las tropas del Pacto de Varsovia en agosto de 1968.

Kissinger, también negoció el acercamiento con la República Popular de China, al ver de antemano una gran oportunidad de futura inversión para Estados Unidos; además también determinó que su acercamiento con China, rival de la URSS, sería un muy eficiente mecanismo de presión para la última, que de acuerdo a lo que Kissinger planteaba, reaccionaría tratando de acercarse aún más a Estados Unidos.
Así, Kissinger, intentó colocar presión diplomática en la Unión Soviética. Hizo dos viajes a la República Popular de China en julio y octubre de 1971 (el primero de los cuales se hizo en secreto) para conferir con el Ministro Zhou Enlai, entonces a cargo de la política exterior China. Esto allanó el camino para la innovadora Cumbre de 1972 entre Nixon, Zhou y el Presidente del Partido Comunista de China, Mao Zedong, así como para la formalización de las relaciones entre los dos países, tras 23 años de aislamiento diplomático y hostilidad mutua.

El resultado fue la formación de una alianza tácita, antisoviética, entre China y los Estados Unidos, cuyo fin era lograr crear aún más presión sobre la URSS y comenzar a ganarle terreno, quitándole el protagonismo internacional. Mientras que la diplomacia de Kissinger llevó a cabo intercambios económicos y culturales entre las dos partes y el establecimiento de oficinas de enlace en las capitales china y estadounidense, todo ello con varias dificultades en el proceso, pues la plena normalización de las relaciones con la República Popular de China no se produciría hasta 1979, en primera instancia debido al escándalo Watergate, que eclipsó el gobierno de Richard Nixon y condujo a su renuncia y en segunda porque los Estados Unidos y la ONU continuaron reconociendo al Gobierno de Taiwán, como «el verdadero gobierno chino», siendo este país el que conservó la silla de China en el Consejo de Seguridad hasta el año de 1979.

La participación de Kissinger en Indochina comenzó desde antes de su nombramiento, incluso como Consejero de Seguridad Nacional de Nixon. En Harvard, trabajó como asesor en política exterior en la Casa Blanca y el departamento de Estado. Kissinger dice que «en agosto de 1965 […] [Henry Cabot Lodge], un viejo amigo que actuaba como embajador en Saigón, le había pedido que visitara Vietnam como su asesor. Kissinger accedió a esta proposición al realizar una gira por Vietnam en primer lugar por dos semanas en octubre y noviembre de 1965, una vez más para unos diez días en julio de 1966 y una tercera vez por unos días en octubre de 1966 […] Lodge me dio carta blanca para examinar cualquier asunto de mi elección. Me convenció del sinsentido de victorias militares en Vietnam y me hizo caer en cuenta acerca de la realidad política que yacía en nuestro país, en la cual fácilmente, se podría llevar a cabo nuestra retirada final».

En una iniciativa de paz de 1967, él mismo medió entre Washington y Hanói, lo que representó el inicio de las conversaciones entre ambos bandos.

Richard Nixon había sido elegido en 1968 con la promesa de alcanzar la «paz con honor» y poner fin a la Guerra de Vietnam. En la Oficina, y asistida por Kissinger, Nixon implementó una política de vietnamización encaminada a retirar gradualmente las tropas durante la expansión de la función de combate del Ejército de Vietnam del Sur, propiciando defender de forma independiente su régimen contra el Frente nacional para la liberación de Vietnam del Sur, una organización guerrillera comunista y el ejército norvietnamita (Ejército Popular de Vietnam). Kissinger desempeñó un papel clave en una campaña secreta en Camboya, donde se procedería a realizar un bombardeo masivo, para interrumpir las operaciones de unidades del Ejército Popular de Vietnam y Viet Cong, así como para desmantelarlas. Planificó y puso en marcha el lanzamiento de incursiones en Vietnam del Sur desde dentro de las fronteras de Camboya y reabastecer sus fuerzas mediante la Ruta Hồ Chí Minh y otras rutas, así como la incursión de Camboya de 1970 y posterior bombardeo generalizado de Camboya.

La campaña de bombardeo contribuyó al caos de la Guerra Civil de Camboya, que incapacitó a las fuerzas del dictador lon Nol para retener extranjeros. Además apoyó la insurgencia de los Jemeres rojos, que buscaron derrocar a dicho dictador en 1975. Documentos descubiertos en los archivos soviéticos después de 1991 revelan que la invasión del Norte Vietnamita de Camboya en 1970, fue lanzada a petición explícita de los Jemeres Rojos y negociada por Pol Pot junto con su segundo al mando, Nuon Chea. El bombardeo estadounidense de Camboya causó la muerte de unos 40.000 combatientes y civiles.

El biógrafo de Pol Pot y reconocido historiador, especializado en la Historia de Camboya, David P. Chandler, afirma:

Por otro lado, el escritor y periodista británico, además de un reconocido crítico del Secretario Kissinger, Christopher Hitchens, asegura en su libro "Juicio a Kissinger":

Junto con miembro del Politburó Vietnam del Norte, Le Duc Tho, Kissinger fue galardonado con el Premio Nobel de la Paz el 10 de diciembre de 1973, por su trabajo en la negociación de la cesación del fuego contenidas en los Acuerdos de paz de París, donde se establecían las intenciones de «poner fin a la guerra y restablecer la paz en Vietnam», habiendo sido estos firmados en enero. Tho rechazó el premio, alegando que la paz no había sido, realmente restaurada en Vietnam del Sur. Kissinger, por su lado escribió al "Comité del Nobel" que aceptaba el premio «con humildad». El conflicto continuó hasta una invasión del sur por el ejército de Vietnam del Norte, que resultó en una victoria norvietnamita en 1975 y la evolución posterior del pathet Lao en Laos hacia la condición de mascarón de proa.

"Artículo principal: Guerra indo-pakistaní de 1971"
"Véase también: Guerra Civil Camboyana"

Bajo la dirección de Kissinger, el Gobierno de Estados Unidos apoyó a Pakistán en la Guerra de liberación de Bangladesh en 1971. Kissinger estaba particularmente preocupado acerca de la expansión Soviética en el Asia meridional como resultado del Tratado de Amistad, recientemente firmado por la India y la URSS, por lo cual trató de demostrar a la República Popular de China (aliado de Pakistán y enemigo tanto de la India como de la Unión Soviética) el valor de una alianza táctica con los Estados Unidos, planeando y en efecto logrando usar a China, como enclave de la influencia estadounidense sobre la región, estrategia que resultó más que eficiente.

La Guerra indo-pakistaní de 1971, es también uno de los flancos de donde Kissinger recibe mayor cantidad de críticas, debido no a la guerra, ni a sus acciones "per se", sino a la actitud que tomó al respecto, criticando fuertemente tanto en público como en privado a la primera ministra india Indira Gandhi y según fuentes internas, Kissinger habría llegado a insultarla, incluso en reuniones privadas con Richard Nixon, además de tener una actitud despreciativa hacia la población india en general, tratándolos como una «raza inferior».

Al respecto de la crítica situación que se daba en el Oriente Medio con el estado de Israel y también con el problema de los judíos retenidos en la URSS, donde eran llevados a campos de concentración soviéticos, Richard Nixon, se sentía particularmente preocupado por la posición del único hombre que no podía despedir de su gobierno y que de paso, era el encargado de la política exterior estadounidense, siendo Kissinger judío. Ante esto, Nixon, realmente creía que su Secretario de Estado, asumiría una posición más que activa con respecto esto, favoreciendo a la comunidad judía internacionalmente y al propio Israel, lo cual podría poner en riesgo la delicada posición de Estados Unidos al respecto.

Richard Nixon llegó incluso a considerar el sacar de este asunto a Kissinger. De hecho, según notas tomadas por H.R. Haldeman, Nixon habría ordenado lo siguiente:

A pesar de todo lo que Nixon había considerado y previsto, Kissinger resultó estar más que desinteresado del asunto de Israel y de los judíos, de hecho, los consideraba un estorbo para el proceder político internacional de Estados Unidos.

Llegando aún más lejos, en 1973, Kissinger no sentía que lo que estaba ocurriendo en la Unión Soviética con respecto a los judíos, donde eran perseguidos, discriminados, y no podían ocupar cargos de importancia, fuese asunto de interés para la política exterior de los Estados Unidos. A tal punto llegó esto, que en una conversación con Nixon, poco después de una reunión con Golda Meir, el 1 de marzo de 1973, Kissinger dijo:

Además, Kissinger sentía que el gobierno de Israel no era más que una molestia, un grupo de enfermos que constantemente pedían ayuda al gobierno estadounidense, criticándolos por no ser más efectivos.

En una conversación con el entonces Secretario de Defensa, Robert McNamara, dijo sobre los dirigentes israelíes:

El 6 de octubre de 1973, coincidiendo con la festividad hebrea de Yom Kipur, de donde recibió su nombre la guerra, Egipto y Siria, atacaron conjuntamente y por sorpresa a Israel, traspasando la línea establecida por el armisticio del Sinaí y de los Altos del Golán, todos estos territorios que habían sido conquistados por Israel durante la Guerra de los Seis Días en 1967. Los líderes de Egipto y Siria, Anwar el-Sadat y Hafez al-Asad respectivamente, estaban siendo respaldados y aconsejados por la URSS y su presidente Leonid Brézhnev.

Estados Unidos estaba estrechamente vinculado con Israel, pero su "política de distensión" con la Unión Soviética no le permitía actuar directamente en favor de dicho país, además la situación económica que se presentaría, de entrar a la guerra, sería difícil, así que Kissinger optó por la paz. Trató de negociar el fin de la Guerra del Yom Kippur, tras los sucesos ocurridos.

No obstante, Nixon decidió ir en contra de la oposición inicial de Kissinger. Ordenó que el ejército de Estados Unidos llevase a cabo la mayor operación militar aérea de la historia para ayudar a Israel el 12 de octubre de 1973. Esta acción estadounidense fue el desencadenante de la Crisis del petróleo de 1973, que afectó a los Estados Unidos y sus aliados de Europa Occidental.

Comienza, entonces, el 17 de octubre de 1973, la Crisis Petrolera de 1973, a raíz de la decisión de la Organización de Países Árabes Exportadores de Petróleo (que agrupaba a los países miembros árabes de la OPEP más Egipto y Siria), de no exportar más petróleo a los países que apoyasen a Israel durante la Guerra del Yom Kippur, medida que incluía a Estados Unidos y a sus aliados de Europa Occidental.

Al mismo tiempo, los miembros de la Organización de Países Árabes Exportadores de Petróleo acordaron utilizar su influencia sobre el mecanismo que fijaba el precio mundial del petróleo para cuadruplicar su precio, después de que fracasaran las tentativas previas de negociar con las "Siete Hermanas".

El aumento del precio unido a la gran dependencia que tenía el mundo industrializado del petróleo de la OPEP, provocó un fuerte efecto inflacionario y una disminución de la actividad económica de los países afectados. Estos países respondieron con una serie de medidas permanentes para frenar su dependencia exterior, siendo Kissinger clave para esta operación, pero la prioridad número uno del Secretario de Estado, así como de todo el gobierno estadounidense no era otra que poner fin a la Guerra del Yom Kipur, para luego tratar de levantar el bloqueo económico impuesto por parte de la Organización de Países Árabes Exportadores de Petróleo.
Entre tanto, ahora con apoyo militar y logístico estadounidense, Israel avanzaba con una serie de excepcionales victorias militares que lo llevaban a recuperar los territorio perdidos al inicio de la guerra y ganó nuevos territorios en Siria y Egipto, incluidas las tierras al este, de los previamente capturados, del Golán aunque perdieron parte del territorio en el lado oriental del Canal de Suez que había estado en manos israelíes desde el fin de la Guerra de seis días. Kissinger presionó a los israelíes, para que cedieran parte de los territorios recién capturados de nuevo a sus vecinos árabes, contribuyendo a las primeras fases de no-agresión entre Israel y Egipto. Esto condujo a una reanudación de las relaciones entre Estados Unidos y Egipto, amargas desde la década de 1950, dando como resultado que el país pasase de su anterior postura independiente a mantener una estrecha asociación con los Estados Unidos.

Finalmente, el 11 de febrero de 1973, Kissinger presenta su "Plan para el Desarrollo de Energías Alternativas", último paso con el cual le asesta un golpe fatal al bloqueo petrolífero. Ahora con el panorama internacional a favor, el Secretario de Estado estadounidense, se anota su gran éxito, cuando los Ministros de Energía árabes, a excepción de Libia, anuncian el fin del embargo contra Estados Unidos.

El 31 de octubre de 1973, el Ministro de Relaciones Exteriores egipcio Ismail Fahmi se reúne con Richard Nixon y Henry Kissinger, luego de lo cual, tras una semana, se pone fin a los combates en la Guerra de Yom Kipur.

EL proceso de paz en el Medio Oriente finalizaría cinco años después, en 1978, cuando el Presidente estadounidense Jimmy Carter mediará los Acuerdos de Camp David, durante los cuales Israel devolvería la península del Sinaí a cambio de un acuerdo egipcio para reconocer el estado de Israel.

"Consulte también: Operación Cóndor"<br>
"Consulte también: Dictadura militar (Chile)"
"Consulte también: Proceso de Reorganización Nacional"
"Consulte también: Dictadura cívico-militar en Uruguay (1973-1985)"

Bajo "el mando diplomático" de Kissinger Estados Unidos continuó reconociendo y manteniendo relaciones con gobiernos de derecha, ya fuesen dictatoriales o democráticos, respaldando y estableciendo tratados y alianzas estratégicas con cada uno de ellos, con el fin último de asegurar el predominio de las tendencias capitalistas y neoliberalistas, por sobre las izquierdistas, comunistas y socialistas en la región. Pero Kissinger llegaría más lejos que cualquiera de sus predecesores en esta región.

Kissinger respaldó y fue clave para el ascenso de tres importantes dictaduras en el Cono Sur, la Dictadura militar de Augusto Pinochet en Chile y el Proceso de Reorganización Nacional en Argentina y la Dictadura cívico-militar en Uruguay (1973-1985). Además desarrolló y aplicó la Operación Cóndor, un plan ofensivo en contra de las organizaciones populares en Latinoamérica.

Tras la Revolución Cubana, liderada por Fidel Castro, que se había consolidado en el poder desde 1960, Estados Unidos había venido teniendo todo tipo de confrontaciones diplomáticas con Cuba. El país, ahora orientado hacia el comunismo y el socialismo, se convirtió en un problema para EE.UU., que aspiraba mantener a la región latinoamericana limpia de cualquier tendencia de izquierda.

Bajo tales aspiraciones, las relaciones entre Cuba y los Estados Unidos fueron interrumpidas y el primero trató de consolidar todo tipo de problemas para la Isla Rebelde, logrando aislarla internacionalmente, además de aplicar embargos económicos, e incluso durante el gobierno de Kennedy, se llevó a cabo la Invasión de la Bahía de Cochinos, un ataque al territorio cubano.

No obstante, Kissinger inicialmente consideraba que era momento de mejorar las relaciones entre Cuba y los Estados Unidos, rotas desde 1961 (todo el comercio de Exxon Mobil cubano había sido bloqueado en febrero de 1962, unas semanas después de la exclusión de Cuba de la Organización de los Estados Americanos, debido a la presión estadounidense). Sin embargo, rápidamente cambió de opinión y siguió la política de John F. Kennedy. Después de la participación de las fuerzas armadas revolucionarias de Cuba en las luchas de liberación en Angola y Mozambique, Kissinger determinó que a menos que Cuba retirase sus fuerzas no podrían normalizarse las relaciones entre ambos países, a lo cual Cuba se negó.

La Alianza para el progreso de John F. Kennedy finalizó en 1973. En consecuencia, era necesario empezar a determinar cuales serían las acciones a tomar con respecto a un valioso activo comercial internacional, el Canal de Panamá. Kissinger, se encargó de que en 1974 comenzaran las negociaciones sobre un nuevo asentamiento en el Canal de Panamá. Si bien el Secretario de Estado estadounidense, no logró llevar a cabo la firma de un nuevo tratado, sí fue el responsable de prepararla, siendo a escasos meses de haber asumido la presidencia, que Jimmy Carter llevó a cabo los Tratados Torrijos-Carter y la entrega del canal al control panameño.

El candidato presidencial del Partido Socialista de Chile, Salvador Allende, había sido designado presidente por el congreso nacional debido a que obtuvo apenas el 36,3 % de los votos, una mayoría relativa, frente a Jorge Alessandri que obtuvo el 35.5 %. Para esto la Democracia Cristiana Chilena le exigió la firma del Estatuto de Garantías Constitucionales que se firmaría en el marco de "la vía chilena al socialismo".

Automáticamente, en Estados Unidos, la reacción no se hizo esperar, la preocupación general era incontrolable en Washington, D.C. debido al nuevo presidente chileno, abiertamente socialista y con inclinaciones a favor de Fidel Castro y la Revolución Cubana. Allende intentaría llevar a cabo pacíficamente su programa de gobierno de profundas reformas para la época en lo político, económico y cultural.

Kissinger fue, particularmente, el mayor defensor de la necesidad de intervenir en Chile, considerando su gobierno como «comunista», y una amenaza de germen peligroso para el orden en la región, llegando a afirmar lo siguiente:

Kissinger era presidente del poderoso Comité 40, una organización de alto nivel y al lado de representantes del Departamento del Estado, de la CIA y del Pentágono, tomaban decisiones y medidas acerca de las situaciones relacionadas con el comunismo nacional e internacionalmente. La administración de Nixon autorizó la Agencia Central de Inteligencia (CIA) para fomentar un golpe de Estado militar que impidiese la toma de posesión de Allende, pero el plan no tuvo éxito. El grado de participación de Kissinger en estos planes es mayor del que se creía, existiendo documentos y archivos desclasificados que demuestran que Kissinger no solo era consciente de las operaciones de EE.UU. y la CIA en Chile, sino que él era el principal artífice detrás de tales operaciones.

Mónica Gónzalez, reconocida periodista chilena ganadora del premio Cabot de la Universidad de Columbia y autora del libro La Conjura afirma:

Además, la misma Mónica González, afirmó respecto a Kissinger lo siguiente:

Las relaciones entre los gobiernos de Estados Unidos y Chile permanecieron heladas durante el mandato de Salvador Allende, siendo el hito que marcó el inicio de este distanciamientos la completa nacionalización de la industria minera de cobre chilena, hasta entonces, prácticamente propiedad de Estados Unidos, mediante la filial chilena de la estadounidense ITT Corporation, así como otras empresas chilenas. Estados Unidos afirmó que el gobierno chileno había devaluado enormemente una compensación equitativa para la nacionalización restando lo que consideró «beneficios extraordinarios». Por lo tanto, Estados Unidos consideró implementar sanciones de orden económico, pero nunca llegaron a ser aplicadas. La CIA también proporcionó financiamiento para las huelgas masivas antigubernamentales en 1972 y 1973.

La CIA, actuando en virtud de la aprobación del "Comité 40" (del que Kissinger era presidente), participó en varias acciones encubiertas en Chile durante este período, ideando lo que en efecto fue un golpe de estado constitucional y, cuando esto fracasó, permaneció en contacto con elementos antigubernamentales. La CIA se enteró de una gran cantidad de asociaciones, planes y organizaciones clandestinas que buscaban establecer una dictadura militar. Aunque intencionalmente se negó a ayudar materialmente a cualquiera de ellos, también alentó a varios de estos grupos y no hizo nada para evitarlos. Aseguró a los conspiradores que tal evento sería bienvenido en Washington y los Estados Unidos se encargaría de omitir cualquier mención acerca de posibles violaciones de los derechos humanos.

El 11 de septiembre de 1973, el presidente Allende se suicidó durante el golpe de estado lanzado por el comandante en jefe de Ejército Augusto Pinochet, quien se convirtió en Presidente de facto de Chile y que ejercería una dictadura sobre el país durante 17 años. Un documento publicado por la CIA en 2000 titulado "CIA, Actividades en Chile" reveló que la CIA apoyó activamente a la junta militar tras el derrocamiento de Allende y que tenía muchos de los oficiales de Pinochet en pagos contactos de la CIA, a pesar de que muchos eran conocidos por estar involucrados en notorias violaciones de los derechos humanos, hasta que el candidato Demócrata Jimmy Carter derrotó al Presidente Gerald Ford en 1976.

El 16 de septiembre de 1973, cinco días después de que Augusto Pinochet había asumido el poder, el siguiente intercambio sobre el golpe de estado tuvo lugar entre Kissinger y el Presidente Nixon:

En 1976, Kissinger canceló una carta que fue enviada a Chile advirtiéndoles contra la realización de cualquier asesinato político. Orlando Letelier fue asesinado poco después, en Washington, D.C. con un coche bomba el 21 de septiembre de 1976, justo el día que la carta iba a ser enviada. El Embajador de Estados Unidos en Chile dijo que Pinochet podría tomar como un insulto cualquier insinuación de estar involucrado en el asesinato (lo estaba).

Kissinger tomó una línea similar a la que había practicado en Chile cuando la milicia argentina, dirigida por Jorge Rafael Videla, derrocó el gobierno de Isabel Perón en 1976 con un proceso llamado "Proceso de reorganización nacional" por los militares, con la que consolidó el poder, lanzamiento de brutales represalias y desapariciones contra grupos guerrilleros como ERP y Montoneros. Durante una reunión con el Ministro de Relaciones Exteriores argentino, vicealmirante César Augusto Guzzetti, Kissinger le aseguró que Estados Unidos era un aliado, pero le urgió a "volver a procedimientos normales", rápidamente antes de que el Congreso de Estados Unidos volviese a reunirse y tuvieron la oportunidad de considerar las sanciones.

En ocasión del golpe de estado de Argentina, el 24 de marzo de 1976, alentó y apoyó a la Junta Militar a que tomara el poder. Lo han acusado de complicidad y del estímulo en la eliminación y desaparición sistemática de miles de personas cometidas por la Junta Militar de Argentina del autodenominado proceso de reorganización nacional o ""guerra sucia"" según han denominado varios historiadores. La aceptación de la ""guerra sucia"" argentina por parte de Kissinger (quien en aquel momento era Secretario de Estado de Estados Unidos) ha sido confirmada oficialmente por archivos desclasificados el Archivo Nacional de Seguridad de los Estados Unidos ("The National Security Archive"). En estos archivos se confirma de Kissinger, entre otras frases hacia la Junta militar en referencia a la guerra sucia:

En 1978 Kissinger visitó Argentina invitado por el dictador Videla. Lo hacía a título particular ya que había cesado en su cargo al ganar las elecciones presidenciales de 1976 el demócrata Jimmy Carter. En Argentina, Kissinger elogió a los militares por sus esfuerzos en lo que denominada "combatir el terrorismo". Ello provocó la indignación del embajador estadounidense, Raul Castro, que siguiendo las nuevas directrices de Carter estaba presionando a la junta militar para que respetase los derechos humanos.

La Operación Cóndor, fue un plan de coordinación de operaciones entre las cúpulas de los regímenes dictatoriales del Cono Sur de América —Chile, Argentina, Brasil, Paraguay, Uruguay y Bolivia— y con la CIA de los EE. UU., llevada a cabo en las décadas de 1970 y 1980.

Enmarcada en la Doctrina Truman, esta coordinación se tradujo en "el seguimiento, vigilancia, detención, interrogatorios con apremios psico-físicos, traslados entre países y desaparición o muerte de personas consideradas por dichos regímenes como "subversivas del orden instaurado o contrarias al pensamiento político o ideológico opuesto, o no compatible con las dictaduras militares de la región". El Plan Cóndor se constituyó en una organización clandestina internacional para la práctica del terrorismo de Estado que instrumentó el asesinato y desaparición de decenas de miles de opositores a las mencionadas dictaduras, la mayoría de ellos pertenecientes a movimientos de la izquierda política.

Se afirma que Kissinger, en su odisea por mantener Latinoamérica, considerada el "patio trasero" de los Estados Unidos, férreamente afianzada bajo predominio y control estadounidense, habría sido el responsable o por lo menos un personaje clave en la creación, planificación y puesta en marcha de la Operación Cóndor, dado que para el año en que ésta comenzó a funcionar, 1970, Kissinger ya era Consejero de Seguridad Nacional, además de que es bien sabido que a causa de él, el Cono Sur pasó a estar dominado por dictaduras y en su desempeño como Secretario de Estado, mantuvo una política eficaz en contra de cualquier intento de expansión ideológica y política de orden izquierdista, lo que el consideraba como "combate al comunismo", en las circunstancias de la Guerra Fría.

La Operación Cóndor, se desarrolló durante más de siete años, bajo la supervisión de Kissinger, siendo sólo en sus etapas finales, ya hacia 1980, cuando este dejó de ejercer la Secretaría de Estado de EE.UU., las únicas en las que quizá no haya tenido injerencia.

De acuerdo a las investigaciones de varios historiadores del siglo XX, han afirmado que durante el período del Frente Nacional, Kissinger ayudó a la emancipación y posterior extensión del mismo en la presidencia colombiana, aparte de que desde su puesto logró promover la asesoría militar al ejército de la nación suramericana por medio de asesores de la CIA en la enseñanza de técnicas de tortura y asesinato encubierto a líderes sindicales de la oposición local, en el marco de la muy célebre y odiada serie de dictaduras amparadas en el continente y conocidas por ser sostenidas desde las operaciones de asesinatos sistemáticos promovidas desde la Operación Cóndor.

En 1974 un golpe militar izquierdista derroca al gobierno de Marcelo Caetano en Portugal, a través de la denominada Revolución de los Claveles.
La "Junta de Salvación Nacional" liderada por el nuevo Primer Ministro de Portugal, Adelino da Palma Carlos, consolidados como el nuevo gobierno del país ibérico, rápidamente concede la independencia a las colonias de Portugal, creando una cadena de reacciones internacionales, con respecto al destino de África.

Cuba apoya abiertamiente a través de sus tropas en Angola, al "Movimiento Popular de izquierda para la liberación de Angola (MPLA)" en su lucha contra los rebeldes UNITA y el FNLA de derecha durante la guerra civil angoleña. Kissinger se encargó de apoyar a FNLA, liderado por Roberto Holden y a la UNITA, liderada por Jonas Savimbi, los insurgentes de La resistencia nacional mozambiqueña (RENAMO), así como la invasión de apoyo de la CIA de Angola por tropas sudafricanas. El FNLA fue derrotado y UNITA se vio obligado a llevar su lucha en la selva. Sólo bajo la Presidencia de Ronald Reagan Estados Unidos volvería a dar apoyo a la UNITA.

En septiembre de 1976, Kissinger participó activamente en las negociaciones acerca de la Guerra de Bush Rhodesia. Kissinger, junto con el primer ministro de Sudáfrica John Vorster, presionó al Primer Ministro de Rhodesia, Ian Smith para acelerar la transición de gobierno en favor de las mayorías negras del país. Con el FRELIMO en control de Mozambique y Sudáfrica, al Kissinger retirar su apoyo a manera de presión, el aislamiento de Rhodesia fue total. Según la autobiografía de Smith, Kissinger le comentó la admiración que la Sra. Kissinger le profesaba, pero Smith dijo que él pensó que Kissinger le estaba pidiéndo firmar el "Acta de defunción" de Rhodesia. Kissinger, llevando el peso de los Estados Unidos y acorralando a otras partes interesadas para presionar a Rhodesia, aceleró el fin del régimen de la minoría.

Hacia 1975, España entró en un período de crisis interna, con la enfermedad de Francisco Franco, situación que fue aprovechada por el rey Hassan II para enviar a más civiles y militares al territorio de Sahara Occidental, con el fin de reclamarlo y anexarlo a sus dominios.

Henry Kissinger, jugó un papel clave en este proceso, siendo él, quien planificó, asesoró y organizó a los marroquíes para la Marcha Verde (1975), y auspició las negociaciones entre los representantes marroquíes y el Gobierno español, que finalizaron con la salida del Ejército español de los territorios del Sáhara y el traspaso del mismo a Marruecos y Mauritania.

Estados Unidos, encausado por Kissinger, proporcionó a Marruecos equipos, armamentos, logística y una completa organización para la operación, mientras que Arabia Saudí aportó grandes sumas de dinero para la misma, que era en extremo favorecedora, siéndolo todavía hoy en día, para los intereses de orden militares, estratégicos y económicos de los EE. UU., sobre la región. El conflicto del Sáhara Occidental, continúa todavía sin resolverse.

El proceso de descolonización portuguesa atrajo la atención de Estados Unidos a la antigua colonia portuguesa de Timor Oriental, que se encuentra en el archipiélago indonesio y declaró su independencia en 1975. El presidente indonesio Suharto fue un fuerte aliado de EE.UU. en el sudeste asiático y comenzó a movilizar el ejército indonesio, preparándose para el estado naciente, que se había vuelto cada vez más dominado por el partido popular de FRETILIN izquierdista. En diciembre de 1975, Suharto discutió los planes de invasión durante una reunión con Kissinger y el Presidente Ford en la capital Indonesia de Yakarta. Ford y Kissinger dejaron en claro que las relaciones con Indonesia continuarían siendo fuertes y que no se oponían a la propuesta de anexión. La venta de armas de Estados Unidos a Indonesia continuó, y Suharto siguió adelante con el plan de anexión.

Se acusa a Kissinger de muchas violaciones a los derechos humanos, y de ser instigador de genocidios sistemáticos de grupos políticos.

El juez español Baltasar Garzón, célebre, entre otras muchas cosas, por haber tratado de llevar a juicio a Augusto Pinochet e intentar esclarecer los atentados contra la población civil en Argentina durante el directorio militar de Videla, envió una comisión rogatoria referente a violaciones de los Derechos Humanos a los Estados Unidos, pero el Departamento del Estado de EE.UU. la rechazó. Igualmente existen numerosas iniciativas que persiguen conseguir su procesamiento ante instancias judiciales internacionales, así como la retirada de su Premio Nobel.

Henry Kissinger también es uno de los más conocidos miembros del Grupo Bilderberg junto a David Rockefeller, además de ostentar la posición de ser uno de su miembros fundadores originales. Ambos colaboraron de pleno con el príncipe Bernardo de Lippe-Biesterfeld, junto con la Banca Rothschild, en los planes de nacimiento, organización y expansión del Grupo Bilderberg, entidad foco de una gran teoría conspirativa.

El Grupo Bilderberg, en sus reuniones anuales agrupa a algunos de los , para discutir acerca de todo lo que está pasando en el mundo y sus opiniones al respecto y en lo que al público concierne.

Kissinger, también ha sido objeto de críticas a causa de su relación con la muy polémica y criticada Corporación RAND, una poderosa empresa de desarrollo tecnológico, industrial y energético, que posee numerosas alianzas, contratos y acuerdos comerciales con el gobierno de los Estados Unidos, específicamente con las Fuerzas Armadas del mismo y con los Departamentos de Seguridad Nacional.

Kissinger, habría fungido de asesor de esta compañía desde el año de 1951, además de mantener estrechos vínculos con ella durante todo el período de tiempo que ejerció como Consejero de Seguridad Nacional y también como Secretario Estado.

Esta cercanía entre Henry Kissinger y la Corporación RAND, le ha valido al primero el recibir las mismas críticas que se le hacen a la empresa, acusándola de ser una organización militarista y de promover los conflictos militares, llegando al punto de haber teorías de conspiración, que vinculan a los conflictos bélicos que hubo durante el período de Kissinger en la Casa Blanca, con la Corporación RAND, basándose en la suposición de que estos conflictos representaron oportunidades multimillonarias de negocios para la misma. A pesar de todo, ninguna de estas acusaciones disponen de algún tipo de prueba que pueda justificarlas y lo cierto es que los acontecimientos que involucran a la RAND, ""per se"", se basan en suposiciones que son difíciles de verificar, debido a la falta de detalles acerca del trabajo, de alto secreto, llevado a cabo por la RAND para agencias de inteligencia y de defensa.

En ocasión del golpe de Estado de Argentina, el 24 de marzo de 1976, a Kissinger se le critica por haber alentado y apoyado a la Junta Militar a que tomara el poder, además de haberla respaldado tras el golpe y de paso, por haberla utilizado como herramienta para fortificar la influencia de Estados Unidos sobre el Cono Sur. Igualmente, ha sido acusado de complicidad y del estímulo en la eliminación y desaparición sistemática de miles de opositores, cometidas por la Junta Militar de Argentina del autodenominado Proceso de Reorganización Nacional o ""guerra sucia"" según han denominado varios historiadores. La aceptación de la ""guerra sucia"" argentina por parte de Kissinger (quien en aquel momento era Secretario de Estado de Estados Unidos) ha sido confirmada oficialmente por archivos desclasificados el "Archivo Nacional de Seguridad de los Estados Unidos" ("The National Security Archive"). En estos archivos se confirma de Kissinger, emitió entre muchas otras frases hacia la Junta militar, la siguiente, en referencia a la ""guerra sucia"":

Se afirma su participación en la organización del golpe de Estado contra el gobierno de Salvador Allende en Chile. Se lo acusa además de haber organizado la denominada Operación Cóndor, un plan sistemático de eliminación de opositores dirigido a "combatir el comunismo" en Latinoamérica.
Fue sometido a proceso en Estados Unidos por el asesinato del comandante en jefe del Ejército chileno René Schneider, fallando en 2006 la Corte Suprema de ese país que su responsabilidad había sido política y no criminal.

Durante el mandato del Frente Nacional, se supone que ayudó a la emancipación y posterior extensión del mismo en la presidencia colombiana, aparte de que desde su puesto logró promover la asesoría militar al ejército de la nación suramericana por medio de asesores de la CIA en la enseñanza de técnicas de tortura y asesinato encubierto a líderes sindicales de la oposición local, en el marco de la muy célebre y odiada serie de dictaduras amparadas en el continente y conocidas por ser sostenidas desde las operaciones de asesinatos sistemáticos promovidas desde la Operación Cóndor.</ref>

Henry Kissinger apoyó al régimen indonesio del general Suharto, acusado del genocidio contra la población de Timor Oriental, además de estar involucrado con las actividades de este gobierno, así como también en la invasión a Timor Oriental. Además sirvió a dicho gobierno como "Asesor General de Gobierno", otra acción que le ha sido tremendamente criticada.

Se conoce su implicación directa en los bombardeos secretos de Laos y Camboya, ordenados sin permiso del Congreso. Dichos bombardeos sirvieron para que los jemeres rojos accedieran al poder, del que se servirían para asesinar a más de dos millones de personas.

Kissinger asesoró y preparó a los marroquíes para la Marcha Verde (1975), y auspició las negociaciones entre los representantes marroquíes y el Gobierno español, que concluyeron con la retirada del Ejército español del Sáhara y la entrada al territorio por Marruecos y Mauritania. Estados Unidos proporcionó a Marruecos logística y armamento, y Arabia Saudí aportó grandes sumas de dinero para esta operación, que favorecía (y aún hoy favorece) a los intereses estratégicos y comerciales de los EE.UU. El conflicto del Sáhara Occidental aún no se ha resuelto.

Tras su salida de la Secretaría de Estado, inmediatamente le fue ofrecida a Kissinger, dictar una cátedra en la Universidad de Columbia, oferta que aceptó gustoso. Además fundó su propia firma de Asesoría Política y Diplomática, llamada "Kissinger & Associates" y es cofundador y actual accionista de la "Kissinger & McLarty Associates", otra compañía de asesoría. Además pasó a ejercer como "profesor de la Universidad de Georgetown" y a ser miembro de las juntas directiva y asesor general de las compañías: Hollinger Group, un grupo de medios masivos, canales de televisión, revistas, editoriales, periódicos y demás publicaciones con sede en Chicago y la Gulfstream Aerospace, una compañía especializada en la industria del transporte aéreo y en la construcción de aviones.

Además ha continuado publicando libros, artículos y ensayos, así como también ha dado conferencias y es asesor de múltiples otras compañías.

Su firma fue contratada por el gobierno de George W Bush, para dirigir un comité de acción tras los atentados del 11-S y para prestar sus servicios como compañía asesora, pero se desentendió de este contrato poco antes de que Bush hiciera pública su idea de invadir Irak y de comenzar la Guerra de Afganistán. Además, prestó sus servicios como "Asesor Político General al Gobierno de Indonesia."

El Secretario Kissinger ha escrito muchos libros y artículos sobre política exterior de Estados Unidos, los asuntos internacionales, y la historia diplomática. Entre los premios que ha recibido se encuentran:


Hablar de política exterior en el siglo XX sin nombrar a Kissinger es algo que resulta imposible. Henry Kissinger fue Secretario de Estado de los Estados Unidos de América en un período de tiempo bastante agitado en la política internacional y su proceder, así como su capacidad para solventar todo tipo de problemas que se le presentase, lo convirtieron en una referencia de la diplomacia estadounidense, siendo en gran medida el creador de variedad de tendencias o métodos de acción, en este ámbito, que todavía siguen usándose hoy por hoy (intervenciones militares, negociaciones, acciones en búsqueda de determinados intereses, entre otros).

Kissinger es admirado por unos a la vez que es odiado y temido por otros. La mayor crítica que se le hace es el sobreponer los intereses de su nación por encima del bienestar político de las demás, siendo responsable o estando vinculado a una gran variedad de dictaduras, lo que ha llevado a que muchos lo acusen de estar ligado a las múltiples violaciones a los Derechos Humanos acaecidas durante las mismas, algo que es y seguirá siendo motivo de especulación y controversia.

A pesar de todo, Kissinger inició la aplicación de un nuevo enfoque con respecto a la Guerra Fría, el cual, lastimosamente, no sería continuado por sus sucesores, que hubiera dado como resultado en el futuro una relación productiva no violenta entre la URSS y EE. UU. Muchos consideran esta política de distensión con la URSS la más adecuada, entendiendo el expansionismo e imperialismo como el enfoque correcto, que es retomado por la administración Reagan, con su proyecto "Guerra de las Galaxias" y sus ofensivas internacionales.

Andréi Gromyko, Ministro de Asuntos Exteriores de la Unión Soviética entre 1957 y 1985, dedicó algunas valoraciones sobre Kissinger en sus memorias. Lo consideraba un hombre con un comportamiento digno, competente, que preparaba bien las reuniones y con verdaderos deseos de alcanzar algún acuerdo. Matiza, sin embargo, los supuestos engaños que consiguió en algunos encuentros hacia los rusos que el exsecretario de Estado afirma en sus memorias. Gromyko coincide con Kissinger en que su oponente siguió una táctica de presión sobre el adversario en los diversos encuentros que mantuvieron:

Entre los logros de Kissinger destaca la apertura de relaciones entre China y Estados Unidos, la aplicación de una vasta serie de medidas para la producción energética alternativa, durante y posteriormente a la Crisis del Petróleo de 1973, la firma de los" Tratados SALT I" y la preparación del terreno para la posterior firma de su sucesor, los "Tratados SALT II", el inicio del proceso de colaboración y negociación en el Medio Oriente, específicamente en el conflicto del mundo árabe, sentando las bases para la realización de los Acuerdos de Camp David, y poner fin a un muy criticado y rechazado conflicto, la Guerra de Vietnam, sobrellevar con éxito tanto la Guerra del Yom Kipur como la Guerra indo-pakistaní de 1971 y en líneas generales, conseguir con su política consolidar el poderío internacional estadounidense, así como extender y asegurar sus zonas de influencia a lo largo del mundo.

Es por esta inmensa gama de actividades que Henry Kissinger es recordado como el más emblemático Secretario de Estado de Estados Unidos, al punto de que este puesto es directamente identificado con su persona. Pocos Secretarios de Estado, por no decir ninguno, han tenido un nivel de participación y protagonismo tan grande como el suyo en los sucesos acaecidos a lo largo del mundo, convirtiéndolo en un auténtico ícono para muchas corrientes capitalistas y de derechas, tanto de Estados Unidos como del mundo.













Otras fuentes




1. Obras de Henry Kissinger

Autobiografía

Memoirs, etc.

2. Libros sobre Kissinger (biografías)


3. Otros libros sobre Kissinger



</doc>
<doc id="8221" url="https://es.wikipedia.org/wiki?curid=8221" title="Tsunami">
Tsunami

Un tsunami, sunami (del japonés ["tsu"], «puerto o bahía», y ["nami"], «ola») o maremoto (del latín "mare", «mar», y "motus", «movimiento») es un evento complejo que involucra un grupo de olas en un cuerpo de agua de gran energía y de tamaño variable que se producen cuando se desplaza verticalmente una gran masa de agua por algún fenómeno extraordinario, por ejemplo, un terremoto, erupción volcánica, detonaciones submarinas, deslizamientos de terreno, desprendimientos de hielo glaciar, impacto de meteoritos y otros eventos. A diferencia de las olas oceánicas normales producidas por el viento, o las mareas, que son generadas por la atracción gravitatoria del Sol y la Luna, un tsunami es generado por el desplazamiento de agua.

Este tipo de olas desplazan una cantidad de agua muy superior a las olas superficiales producidas por el viento. Se calcula que el 90% de estos fenómenos son provocados por terremotos, en cuyo caso reciben el nombre más correcto y preciso de «tsunamis tectónicos». La energía de un maremoto depende de su altura, de su longitud de onda y de la longitud de su frente. La energía total descargada sobre una zona costera también dependerá de la cantidad de picos que lleve el tren de ondas. Es frecuente que un tsunami que viaja grandes distancias, disminuya la altura de sus olas, pero siempre mantendrá una velocidad determinada por la profundidad sobre la cual el tsunami se desplaza. Normalmente, en el caso de los tsunamis tectónicos, la altura de la onda de tsunami en aguas profundas es del orden de 1.0 metros, pero la longitud de onda puede alcanzar algunos cientos de kilómetros. Esto es lo que permite que aun cuando la altura en océano abierto sea muy baja, esta altura crezca en forma abrupta al disminuir la profundidad, con lo cual, al disminuir la velocidad de la parte delantera del tsunami, necesariamente crezca la altura por transformación de energía cinética en energía potencial. De esta forma una masa de agua de algunos metros de altura puede arrasar a su paso hacia el interior.

Antiguamente, el término "tsunami" se utilizaba para referirse a las olas producidas por huracanes y temporales que podían entrar tierra adentro, pero estas no dejaban de ser olas superficiales producidas por el viento. Tampoco se debe confundir con la ola producida por la marea conocida como macareo. Este es un fenómeno regular y mucho más lento, aunque en algunos lugares estrechos y de fuerte desnivel pueden generarse fuertes corrientes.

La mayoría de los tsunamis son originados por terremotos de gran magnitud bajo la superficie acuática. Para que se origine un tsunami, el fondo marino debe ser movido de manera abrupta en sentido vertical, de modo que una gran masa de agua del océano sea impulsada fuera de su equilibrio normal. Cuando esta masa de agua trata de recuperar su equilibrio genera olas. El tamaño del tsunami estará determinado por la magnitud de la deformación vertical del fondo marino entre otros parámetros como la profundidad del lecho marino. No todos los terremotos bajo la superficie acuática generan tsunamis, sino solo aquellos de gran magnitud, con hipocentro en el punto de profundidad adecuado.

Un tsunami tectónico producido en un fondo oceánico de 5 km de profundidad desplazará toda la columna de agua desde el fondo hasta la superficie. El desplazamiento vertical puede ser tan solo de centímetros; pero, si se produce a la suficiente profundidad, la velocidad será muy alta y la energía transmitida a la onda será enorme. Aun así, en alta mar la ola pasa casi desapercibida, ya que queda camuflada entre las olas superficiales. Sin embargo, destacan en la quietud del fondo marino, el cual se agita en toda su profundidad.

La zona más afectada por este tipo de fenómenos es el océano Pacífico, debido a que en él se encuentra la zona de sismos más activa del planeta, el cinturón de fuego. Por ello, es el único océano con un sistema de alertas verdaderamente eficaz.

No existe un límite claro respecto de la magnitud necesaria de un sismo como para generar un tsunami. Los elementos determinantes para que ocurra un tsunami son la magnitud del sismo originador, la profundidad del hipocentro y la morfología de las placas tectónicas involucradas. Esto hace que para algunos lugares del planeta se requieran grandes sismos para generar un tsunami, en tanto que para otros baste con de sismos de menor magnitud. En otros términos, la geología local, la magnitud y la profundidad focal son parte de los elementos que definen la ocurrencia o no de un tsunami de origen tectónico.

La velocidad de las olas puede determinarse a través de la ecuación:

formula_1,

donde D es la profundidad del agua que está directamente sobre el sismo y g, la gravedad terrestre (9,8 m/s²).

A las profundidades típicas de 4-5 km las olas viajarán a velocidades en torno a los 600 kilómetros por hora o más. Su amplitud superficial o altura de la cresta H puede ser pequeña, pero la masa de agua que agitan es enorme, y por ello su velocidad es tan grande; y no solo eso, pues la distancia entre picos (longitud de onda) también lo es. Es habitual que la longitud de onda de la cadena de olas de un tsunami sea de 100 km, 200 km o más.
El intervalo de tiempo entre cresta y cresta (período de la onda) puede durar desde menos de diez minutos hasta media hora o más. Cuando la ola entra en la plataforma continental, la disminución drástica de profundidad hace que la velocidad de la ola disminuya y empiece a aumentar su altura. Al llegar a la costa, la velocidad habrá decrecido hasta unos 50 kilómetros por hora, mientras que la altura ya será de unos 3 a 30 m, dependiendo del tipo de relieve que se encuentre. La distancia entre crestas (longitud de onda L) también se estrechará cerca de la costa.

Debido a que la onda se propaga en toda la columna de agua, desde la superficie hasta el fondo, se puede hacer la aproximación a la teoría lineal de la hidrodinámica. Así, el flujo de energía E se calcula como:

formula_2,

siendo 'd' la densidad del fluido.

La teoría lineal predice que las olas conservarán su energía mientras no rompan en la costa. La disipación de la energía cerca de la costa dependerá, de las características del relieve marino. La manera como se disipa dicha energía antes de romper depende de la relación H/h. Una vez que llega a tierra, la forma en que la ola rompe depende de la relación H/L. Como L siempre es mucho mayor que H, las olas romperán como lo hacen las olas bajas y planas. Esta forma de disipar la energía es poco eficiente, y lleva a la ola adentrarse en tierra como una gran marea.

A la llegada a la costa la altura aumentará, pero seguirá teniendo forma de onda plana. Se puede decir que hay un trasvase de energía de velocidad a amplitud. La ola se frena pero gana altura. Pero la amplitud no es suficiente para explicar el poder destructor de la ola. Incluso en un tsunami de menos de 5 m los efectos pueden ser devastadores. La ola arrastra una masa de agua mucho mayor que cualquier ola convencional, por lo que el primer impacto del frente de la onda viene seguido del empuje del resto de la masa de agua perturbada que presiona, haciendo que el mar se adentre mucho en tierra. Por ello, la mayoría de los tsunamis tectónicos se asemejan a una poderosa riada, en la cual es el mar el que inunda a la tierra, y lo hace a gran velocidad.

Antes de su llegada, el mar acostumbra a retirarse de la costa, que en caso de fondos relativamente planos, puede llegar a varios centenares de metros, como una rápida marea baja. Desde entonces hasta que llega la ola principal pueden pasar de 5 a 10 minutos, como también existen casos en los que han transcurrido horas para que la marejada llegue a tierra. A veces, antes de llegar la cadena principal de olas del tsunami, que realmente arrasará la zona, pueden aparecer «micro tsunamis» de aviso. Así ocurrió el 26 de diciembre de 2004 en las costas de Sri Lanka donde, minutos antes de la llegada de la ola fuerte, pequeños tsunamis entraron unos cincuenta metros playa adentro, provocando el desconcierto entre los bañistas antes de que se les echara encima la ola mayor. Según testimonios, «se vieron rápidas y sucesivas mareas bajas y altas, luego el mar se retiró por completo y solo se sintió el estruendo atronador de la gran ola que venía».
Debido a que la energía de los tsunamis tectónicos es casi constante, pueden llegar a cruzar océanos y afectar a costas muy alejadas del lugar del suceso. La trayectoria de las ondas puede modificarse por las variaciones del relieve abisal, fenómeno que no ocurre con las olas superficiales. En los tsunamis tectónicos, dado que se producen debido al desplazamiento vertical de una falla, la onda que generan suele ser un tanto especial. Su frente de onda es recto en casi toda su extensión. Solo en los extremos se va diluyendo la energía al curvarse. La energía se concentra, pues, en un frente de onda recto, lo que hace que las zonas situadas justo en la dirección de la falla se vean relativamente poco afectadas, en contraste con las zonas que quedan barridas de lleno por la ola, aunque estas se sitúen mucho más lejos. El peculiar frente de onda es lo que hace que la ola no pierda energía por simple dispersión geométrica, sobre todo en su zona más central. El fenómeno es parecido a una onda encajonada en un canal o río. La onda, al no poder dispersarse, mantiene constante su energía. En un tsunami existe, cierta dispersión pero, sobre todo, en las zonas más alejadas del centro del frente de onda recto.

Hay quienes sostienen que los tsunamis son ejemplos de un tipo especial de ondas no lineales denominadas solitones. 

El fenómeno físico de los solitones fue descrito, en el siglo XIX, por J. S. Russell en canales de agua de poca profundidad, y son observables también en otros lugares. Al respecto se ha expresado que:

Existen otros mecanismos generadores de tsunamis menos corrientes que también pueden producirse por erupciones volcánicas, deslizamientos de tierra, meteoritos, explosiones submarinas y de origen meteorológico conocidos como meteotsunami. Estos fenómenos pueden producir olas enormes, mucho más altas que las de los tsunamis corrientes. De todas estas causas alternativas, la más común es la de los deslizamientos de tierra producidos por erupciones volcánicas explosivas, que pueden hundir islas o montañas enteras en el mar en cuestión de segundos. También existe la posibilidad de desprendimientos naturales tanto en la superficie como debajo de ella. Este tipo de maremotos difieren drásticamente de los maremotos tectónicos.

En primer lugar, la cantidad de energía que interviene. Está el terremoto del océano Índico de 2004, con una energía desarrollada de unos 32.000 MT. Solo una pequeña fracción de esta se traspasará al maremoto. Por el contrario, un ejemplo clásico de este tipo de tsunamis es la explosión del volcán Krakatoa, cuya erupción generó una energía de 300 MT. Sin embargo, se midió una altitud en las olas de hasta 50 m, muy superior a la de las medidas por los tsunamis del océano Índico. La razón de estas diferencias estriba en varios factores. Por una parte, el mayor rendimiento en la generación de las olas por parte de este tipo de fenómenos, menos energéticos pero que transmiten gran parte de su energía al mar. En un seísmo (o sismo), la mayor parte de la energía se invierte en mover las placas. Pero, aun así, la energía de los maremotos tectónicos sigue siendo mucho mayor que la de los mega maremotos. Otra de las causas es el hecho de que un maremoto tectónico distribuye su energía a lo largo de una superficie de agua mucho mayor, mientras que los mega maremotos parten de un suceso muy puntual y localizado. En muchos casos, los mega maremotos también sufren una mayor dispersión geométrica, debido justamente a la extrema localización del fenómeno. Además, suelen producirse en aguas relativamente poco profundas de la plataforma continental. El resultado es una ola con mucha energía en amplitud superficial, pero de poca profundidad y menor velocidad. Este tipo de fenómenos son increíblemente destructivos en las costas cercanas al desastre, pero se diluyen con rapidez. Esa disipación de la energía no solo se da por una mayor dispersión geométrica, sino también porque no suelen ser olas profundas, lo cual conlleva turbulencias entre la parte que oscila y la que no. Eso comporta que su energía disminuya bastante durante el trayecto.
El ejemplo típico de megatsunami es el causado por la caída de un meteorito en el océano. Este evento produciría ondas curvas de gran amplitud inicial, bastante superficiales, que sí tendrían dispersión geométrica y disipación por turbulencia, por lo que, a grandes distancias, quizá los efectos no serían tan dañinos. Una vez más los efectos estarían localizados, sobre todo, en las zonas cercanas al impacto. El efecto es exactamente el mismo que el de lanzar una piedra a un estanque. Evidentemente, si el meteorito fuera lo suficientemente grande, daría igual cuán alejado se encontrara el continente del impacto, pues las olas lo arrasarían de todas formas con una energía inimaginable. Tsunamis apocalípticos de esa magnitud debieron producirse hace 65 millones de años cuando un meteorito cayó en la actual península de Yucatán. Este mecanismo generador es, sin duda, el más raro de todos; de hecho, no se tienen registros históricos de ninguna ola causada por un impacto.

Algunos geólogos especulan que un mega tsunami podría producirse en un futuro próximo (en términos geológicos) cuando se produzca un deslizamiento en el volcán de la parte inferior de la isla de La Palma, en las islas Canarias (cumbre Vieja). Sin embargo, aunque existe esa posibilidad (de hecho algunos valles de Canarias, como el de Güímar, en Tenerife, o el del Golfo, en El Hierro, se formaron por episodios geológicos de este tipo), no parece que eso pueda ocurrir a corto plazo.

Se conservan muchas descripciones de olas catastróficas en la Antigüedad, especialmente en la zona mediterránea.

Algunos arqueólogos afirman que la desaparición de la civilización minoica en el siglo XVI a. C. fue debida a un tsunami. Según esta hipótesis, las olas generadas por la explosión de la isla volcánica de Santorini después de destruir por completo la ciudad de Acrotiri, ubicada en ella y uno de los principales puertos minoicos, llegaron a Creta con alturas mayores a los cien metros. Estas olas destruyeron Amnisos, el puerto de Cnosos, e inhabilitaron su flota, los cultivos fueron afectados por la nube de cenizas y los años de hambruna que siguieron debilitaron al gobierno central. Esta catástrofe dejó a los minoicos a merced de las invasiones. La explosión de Santorini pudo ser muy superior a la del volcán Krakatoa. Se ha especulado que la narración platónica de la Atlántida se basa en un recuerdo deformado de este tsunami. 

Los investigadores Antonio Rodríguez Ramírez y Juan Antonio Morales González , de los Departamentos de Geodinámica-Paleontología y Geología de la Facultad de Ciencias Experimentales de la Universidad de Huelva, han estudiado abundantes restos de tsunamis en el golfo de Cádiz. Estos estudios se han centrado en el estuario del Tinto-Odiel y en el del Guadalquivir. Las evidencias más antiguas corresponden al Guadalquivir con un episodio del 1500-2000 años antes de nuestra era, afectando a áreas que distan más de 15km de la costa. En el estuario del Tinto-Odiel aparecen depósitos sedimentarios relacionados con tsunamis históricos del 382-395, 881, 1531 y 1755.

En el hubo un tsunami en la península ibérica. Se tomó el golfo de Cádiz como objeto de estudio principal y se ha llegado a la conclusión de que hubo una gigantesca ruptura de estratos. Un tsunami se hace reconocible por los destrozos impresionantes de los que quedan restos detectables siglos después; estos desastres ambientales de transformación del paisaje costero a través de la paleogeografía se puede reconstruir. Las ondas de tsunami llegan a zonas donde no llega habitualmente el agua marina y esos restos son los que prueban esas catástrofes. Esta se ha registrado en el estuario del Guadalquivir y en el área de Doñana. Luego el estudio se ha ampliado a la costa atlántica y se ha comparado con las consecuencias paleogeográficas producidas en el gran tsunami y terremoto de Lisboa de 1755.

Este estudio nos señala que existen zonas predispuestas a que haya tsunamis, es decir a sufrir esta expulsión de energía por parte de la naturaleza.

El historiador Amiano Marcelino describió con todo detalle el tsunami que tuvo lugar en Alejandría y devastó la metrópoli y las orillas del Mediterráneo oriental el 21 de julio del 365.

El 8 de julio a las 04:45 toda el área central de Chile fue remecida por un fuerte terremoto que causó daños en Valparaíso, La Serena, Coquimbo, Illapel, Petorca y Tiltil. El tsunami resultante afectó alrededor de 1000km de costa. Por primera vez en su historia, el puerto de Valparaíso fue inundado y severamente dañado. En las partes bajas de El Almendral todas las casas, fortificaciones y bodegas fueron destruidas por la inundación. También inundó el sector cubierto hoy en día por la avenida Argentina, llegando hasta los pies de Santos Ossa.

El terremoto y tsunami de 1730 inundó Valparaíso, arrasó Concepción, hizo retroceder las aguas del río Valdivia e incluso llegó a Perú. El tsunami también cruzó el Océano Pacífico hasta Japón, donde inundó casas y campos de arroz en la península de Oshika en Sendai.

El 28 de octubre de 1746 ocurrió un fuerte terremoto en toda la costa central del Perú, que tuvo su epicentro en el mar frente a Lima y el Callao, El fuerte sismo fue causado por el proceso normal de subducción de la Placa de Nazca bajo la Placa Sudamericana. Unos treinta minutos después del sismo se produjo un tsunami con olas de 10 a 15 metros de altura que inundaron y destruyeron el Puerto del Callao. Casi todos los habitantes de la ciudad que en esa época era de unos 5000 aproximadamente perecieron en este desastre. Puesto que el agua avanzó cerca de 1 legua o 5.57 km tierra adentro, aun alcanzó a aquellos que trataban de huir hacia Lima. solo 200 personas lograron salvarse aferrándose a objetos de madera y fueron lanzados entre el área de la costa y la isla San Lorenzo, a una distancia de hasta 8 km. De los 23 barcos anclados en el puerto, 19 se hundieron y 4 fueron llevados tierra adentro. 

Cuando el mar retrocedió la mayoría de las casas y edificios fueron arrancados de sus cimientos y llevados por las aguas, además dejaron expuestos los cadáveres, algunos desechos, resultado de la violencia de las aguas. Una gran parte de las murallas de la ciudad, incluyendo la puerta, fueron arrastradas también. A eso de las 04:00 del día siguiente, el Callao fue nuevamente inundado por otra ola. La máxima altura de inundación fue estimada en 24 metros, en el área de la costa verde. El maremoto llegó hasta Concepción (Chile); y en Acapulco (México),

Después de la tragedia, el mar nunca volvió a su límite anterior, es decir, gran parte del Callao se hundió.

El denominado terremoto de Lisboa de 1755, ocurrido el 1 de noviembre de dicho año, y al que se ha atribuido una magnitud de 9 en la escala de Richter (no comprobada ya que no existían sismógrafos en la época), tuvo su epicentro en la falla Azores-Gibraltar, a 37° de latitud Norte y 10° de longitud Oeste (a 800 km al suroeste de la punta sur de Portugal). Además de destruir Lisboa y hacer temblar el suelo hasta Alemania, el terremoto produjo un gran maremoto que afectó a todas las costas atlánticas. Entre treinta minutos y una hora después de producirse el sismo, olas de entre 6 y 20 metros sobre el puerto de Lisboa y sobre ciudades del suroeste de la península ibérica mataron a millares de personas y destruyeron poblaciones. Más de un millar de personas perecieron solamente en Ayamonte y otras tantas en Cádiz; numerosas poblaciones en el Algarve resultaron destruidas y las costas de Marruecos y Huelva quedaron gravemente afectadas. Antes de la llegada de las enormes olas, las aguas del estuario del Tajo se retiraron hacia el mar, mostrando mercancías y cascos de barcos olvidados que yacían en el lecho del puerto.
Las olas se propagaron, entre otros lugares, hasta las costas de Martinica, Barbados, América del Sur y Finlandia.

El 27 de agosto de 1883 a las diez y cinco (hora local), la descomunal explosión del Krakatoa, que hizo desaparecer al citado volcán junto con aproximadamente el 45% de la isla que lo albergaba, produjo una ola de entre 15 y 42 metros de altura, según las zonas, que acabó con la vida de aproximadamente 20.000 personas.

La unión de magma oscuro con magma claro en el centro del volcán fue lo que originó dicha explosión. Pero no solo las olas mataron ese día. Enormes coladas piroclásticas viajaron incluso sobre el fondo marino y emergieron en las costas más cercanas de Java y Sumatra, haciendo hervir el agua y arrasando todo lo que encontraban a su paso. Asimismo, la explosión emitió a la estratosfera gran cantidad de aerosoles, que provocaron una bajada global de las temperaturas. Además, hubo una serie de erupciones que volvieron a formar un volcán, que recibió el nombre de Anak Krakatoa, es decir, ‘el hijo del Krakatoa’.

En la madrugada del 28 de diciembre de 1908 se produjo un terrible terremoto en las regiones de Sicilia y de Calabria, en el sur de Italia. Fue acompañado de un tsunami que arrasó completamente la ciudad de Mesina, en Sicilia.
La ciudad quedó totalmente destruida y tuvo que ser levantada de nuevo en el mismo lugar. Se calcula que murieron cerca de 70.000 personas en la catástrofe (200.000 según estimaciones de la época).
La ciudad contaba entonces con unos 150.000 habitantes. También la ciudad de Regio de Calabria, situada al otro lado del estrecho de Mesina, sufrió importantes consecuencias. Fallecieron unas 15.000 personas, sobre una población total de 45.000 habitantes.

Un terremoto en el océano Pacífico provocó un maremoto que acabó con 165 vidas en Hawái y Alaska. Este maremoto hizo que los estados de la zona del Pacífico creasen un sistema de alertas, que entró en funcionamiento en 1949.

El 9 de julio de 1958, en la bahía Lituya, al noreste del golfo de Alaska, un fuerte sismo, de 8,3 grados en la escala de Richter, hizo que se derrumbara prácticamente una montaña entera, generando una pared de agua que se elevó sobre los 580 metros, convirtiéndose en la ola más grande de la que se tenga registro, llegando a calificarse el suceso de "megatsunami".

El terremoto de Valdivia (también llamado el Gran Terremoto de Chile), ocurrido el 22 de mayo de 1960, es el sismo de mayor magnitud registrado hasta ahora por sismógrafos a nivel mundial. Se produjo a las 15:11 (hora local), tuvo una magnitud de 9,5 en la escala de Richter y una intensidad de XI a XII en la escala de Mercalli, y afectó al sur de Chile. Su epicentro se localizó en Valdivia, a los 39,5º de latitud sur y a 74,5º de longitud oeste; el hipocentro se localizó a 35 km de profundidad, aproximadamente 700 km al sur de Santiago. El sismo causó un maremoto que se propagó por el océano Pacífico y devastó Hilo a 10 000 km del epicentro, como también las regiones costeras de Sudamérica. El número total de víctimas fatales causadas por la combinación de terremoto y maremoto se estima en 3000.

En los minutos posteriores un maremoto arrasó lo poco que quedaba en pie. El mar se recogió por algunos minutos y luego una gran ola se levantó acabando a su paso con casas, animales, puentes, botes y, por supuesto, muchas vidas humanas.
Cuando el mar se recogió varios metros, la gente pensó que el peligro había pasado y en vez de alejarse caminaron hacia las playas, recogiendo pescados, moluscos y otros residuos marinos. Para el momento en que se percataron de la gran ola, ya era demasiado tarde.

Como consecuencia del terremoto se originó un tsunami que arrasó con algunos lugares de las costas de Japón (142 muertes y daños por 50 millones de dólares), Hawái (61 fallecimientos y 75 millones de dólares en daños), Filipinas (32 víctimas y desaparecidos). La costa oeste de Estados Unidos también registró un maremoto, que provocó daños por más de medio millón de dólares estadounidenses.

Un terremoto importante de magnitud 8,1 grados Richter ocurrió a las 02:59:43 (UTC) el 12 de diciembre de 1979 a lo largo de la costa pacífica de Colombia y el Ecuador. El terremoto y tsunami asociado fueron responsables de la destrucción de por lo menos seis municipios de pesca y de la muerte de centenares de personas en el departamento de Nariño en Colombia. El terremoto se sintió en Bogotá, Pereira, Cali, Popayán, Buenaventura, Medellín y otras ciudades y partes importantes en Colombia, y en Guayaquil, Esmeraldas, Quito y otras partes de Ecuador. El tsunami de Tumaco causó, al romper contra la costa, gran destrucción en la ciudad de Tumaco y las poblaciones de El Charco, San Juan, Mosquera y Salahonda en el Pacífico colombiano. Este fenómeno dejó un saldo de 259 muertos, 798 heridos y 95 desaparecidos.

Un terremoto ocurrido en las costas del pacífico de Nicaragua, de entre 7,2 y 7,8 grados en la escala de Richter, el 2 de septiembre de 1992, provocó un tsunami con olas de hasta 10 metros de altura, que azotó gran parte de la costa del Pacífico de este país, provocando más de 170 muertos y afectando a más de 40.000 personas, en al menos una veintena de comunidades, entre ellas San Juan del Sur. 

Un tsunami imprevisto ocurrió a lo largo de la costa de Hokkaido en Japón, como consecuencia de un terremoto, el 12 de julio de 1993. Como resultado, 202 personas de la pequeña isla de Okushiri perdieron la vida, y centenares resultaron heridas.
Este maremoto provocó que algunas oficinas cayeran en quiebra, las olas adquirieron una altura de 31 metros, pero solo atacó a esta isla.

Hasta la fecha, el tsunami más devastador ocurrió el 26 de diciembre de 2004 en el océano Índico, con un número de víctimas directamente atribuidas al tsunami de aproximadamente 280.000 personas. Las zonas más afectadas fueron Indonesia y Tailandia, aunque los efectos destructores alcanzaron zonas situadas a miles de kilómetros: Malasia, Bangladés, India, Sri Lanka, las Maldivas e incluso Somalia, en el este de África. Esto dio lugar a la mayor catástrofe natural ocurrida desde el Krakatoa, en parte debido a la falta de sistemas de alerta temprana en la zona, quizás como consecuencia de la poca frecuencia de este tipo de sucesos en esta región.

El terremoto fue de 9,1 grados: el tercero más poderoso tras el terremoto de Alaska (9,2) y de Valdivia (Chile) de 1960 (9,5). En Banda Aceh formó una pared de agua de 10 o 18 m de altura penetrando en la isla 1 o 3 km desde la costa al interior; solo en la isla de Sumatra murieron 228.440 personas o más. Sucesivas olas llegaron a Tailandia, con olas de 15 metros que mataron a 5.388 personas; en la India murieron 10.744 personas y en Sri Lanka, hubo 30.959 víctimas. Este tremendo tsunami fue debido además de a su gran magnitud (9,1), a que el epicentro estuvo solo a 9 km de profundidad, y a que la rotura de la placa tectónica fue de 1.600 km de longitud (600 km más que en el terremoto de Chile de 1960).

El terremoto de Chile de 2010 fue un fuerte sismo ocurrido a las 3:34:17 hora local (UTC-3), del 27 de febrero de 2010, que alcanzó una magnitud de 8,8 MW de acuerdo al Servicio Sismológico de Chile y al Servicio Geológico de Estados Unidos. El epicentro se ubicó en la costa frente a la localidad de Cobquecura, aproximadamente 150 km al noroeste de Concepción y a 63 km al suroeste de Cauquenes, y a 47,4 km de profundidad bajo la corteza terrestre.

Un fuerte tsunami impactó las costas chilenas como producto del terremoto, destruyendo varias localidades ya devastadas por el impacto telúrico. El Archipiélago de Juan Fernández, pese a no sentir el sismo, fue impactado por las marejadas que arrasaron con su único poblado, San Juan Bautista, en la Isla Robinson Crusoe. La alerta de tsunami generada para el océano Pacífico se extendió posteriormente a 53 países ubicados a lo largo de gran parte de su cuenca, llegando a Perú, Ecuador, Colombia, Panamá, Costa Rica, la Antártida, Nueva Zelanda, la Polinesia Francesa y las costas de Hawái.

El sismo es considerado como el segundo más fuerte en la historia del país y uno de los diez más fuertes registrados por la humanidad. solo es superado a nivel nacional por el cataclismo del terremoto de Valdivia de 1960, el de mayor intensidad registrado mediante sismómetros. El sismo chileno fue 31 veces más fuerte y liberó cerca de 178 veces más energía que el devastador terremoto de Haití ocurrido el mes anterior. La energía liberada fue cercana a 100 000 bombas atómicas como la liberada en Hiroshima en 1945.

El 11 de marzo de 2011 un terremoto magnitud 9.0 en la escala de Richter golpeó Japón.

Tras el sismo se generó una alerta de maremoto (tsunami) para la costa pacífica del Japón y otros países, incluidos Nueva Zelanda, Australia, Rusia, Guam, Filipinas, Indonesia, Papúa Nueva Guinea, Nauru, Hawái, islas Marianas del Norte, Estados Unidos, Taiwán, América Central, México y las costas de América del Sur, especialmente Colombia, Ecuador, Perú y Chile. La alerta de tsunami emitida por el Japón fue la más grave en su escala local de alerta, lo que implica que se esperaba una ola de 10 metros de altura. La agencia de noticias Kyodo informó que un tsunami de 4 m de altura había golpeado la Prefectura de Iwate en Japón. Se observó un tsunami de 10 metros de altura en el aeropuerto de Sendai, en la prefectura de Miyagi, que quedó inundado, con olas que barrieron coches y edificios a medida que se adentraban en tierra.

Se habrían detectado, horas más tarde, alrededor de 105 réplicas del terremoto, una alerta máxima nuclear y 1.000 veces más radiación de lo que producía el Japón mismo debido a los incendios ocasionados en una planta atómica. Se temía más tarde una posible fuga radiactiva.

Finalmente el tsunami azotó las costas de Hawái y toda la costa sudamericana con daños mínimos gracias a los sistemas de alerta temprana liderados por el Centro de Alerta de Tsunamis del Pacífico.
Fue un tsunami causado por un sismo de magnitud 7,0 en la escala de Richter. Registrado el día viernes 28 de septiembre del año 2018. Dejando un total de 1995 fallecidos.

En Indonesia, el 22 de diciembre de 2018, el volcán Anak Krakatoa entró en erupción, provocando un tsunami que dejó más de 400 fallecidos.

Muchas ciudades alrededor del Pacífico, sobre todo en México, Perú, Japón, Ecuador, Estados Unidos y Chile disponen de sistemas de alarma y planes de evacuación en caso de tsunamis. Diversos institutos sismológicos de diferentes partes del mundo se dedican a la previsión de tsunamis, y la evolución de éstos es monitorizada por satélites. El primer sistema, bastante rudimentario, para alertar de la llegada de un tsunami fue puesto a prueba en Hawái en los años veinte. Posteriormente se desarrollaron sistemas más avanzados debido a los tsunamis del 1 de abril de 1946 y el 23 de mayo de 1960, que causaron una gran destrucción en Hilo (Hawái). Los Estados Unidos crearon el Centro de Alerta de Tsunamis del Pacífico ("Pacific Tsunami Warning Center") en 1949, que pasó a formar parte de una red mundial de datos y prevención en 1965.

Uno de los sistemas para la prevención de tsunamis es el proyecto CREST (Consolidated Reporting of Earthquakes and Seaquakes) (Información Consolidada sobre Terremotos y Maremotos), que es utilizado en la costa noroeste estadounidense (Cascadia), en Alaska y en Hawái por el Servicio Geológico de los Estados Unidos, la "National Oceanic and Atmospheric Administration" (la Administración Nacional Oceánica y Atmosférica de EE. UU.), la red sismográfica del noreste del Pacífico y otras tres redes sísmicas universitarias.

La predicción de tsunamis sigue siendo poco precisa. Aunque se puede calcular el epicentro de un gran terremoto subacuático y el tiempo que puede tardar en llegar un tsunami, es casi imposible saber si ha habido grandes movimientos del suelo marino, que son los que producen tsunamis. Como resultado de todo esto, es muy común que se produzcan alarmas falsas. Además, ninguno de estos sistemas sirve de protección contra un tsunami imprevisto.

A pesar de todo, los sistemas de alerta no son eficaces en todos los casos. En ocasiones el terremoto generador puede tener su epicentro muy cerca de la costa, por lo que el lapso entre el sismo y la llegada de la ola será muy reducido. En este caso, las consecuencias son devastadoras, debido a que no se cuenta con tiempo suficiente para evacuar la zona y el terremoto por sí mismo ya ha generado una cierta destrucción y caos previo, lo que hace que resulte muy difícil organizar una evacuación ordenada. Este fue el caso del tsunami del año 2004 pues, aun contando con un sistema adecuado de alerta en el océano Índico, quizá la evacuación no habría sido lo suficientemente rápida.

Un informe publicado por el PNUE sugiere que el tsunami del 26 de diciembre de 2004 provocó menos daños en las zonas en que existían barreras naturales, como los manglares, los arrecifes coralinos o la vegetación costera. Un estudio japonés sobre este tsunami en Sri Lanka estableció, con ayuda de una modelización sobre imágenes satelitales, los parámetros de resistencia costera en función de las diferentes clases de árboles.

Las marejadas se producen habitualmente por la acción del viento sobre la superficie del agua, sus olas suelen presentar una ritmicidad de 20 segundos, y suelen propagarse unos 150 m tierra adentro, como máximo total, tal y como observamos en los temporales o huracanes. De hecho, la propagación se ve limitada por la distancia, de modo que va perdiendo intensidad al alejarnos del lugar donde el viento la está generando.

Un tsunami, en cambio, presenta un comportamiento opuesto, ya que el brusco movimiento del agua desde la profundidad genera un efecto de «latigazo» hacia la superficie, el cual es capaz de lograr olas de magnitud impensable. Los análisis matemáticos indican que la velocidad es igual a la raíz cuadrada del producto del potencial gravitatorio (9,8 m/s²) por la profundidad. Para tener una idea, tomemos la profundidad habitual del océano Pacífico, que es de 4000 m. Esto daría una ola que podría moverse a unos 200 m/s, o sea, a 700 km/h. Y, como las olas pierden su fuerza en relación inversa a su tamaño, al tener 4000 m puede viajar a miles de kilómetros de distancia sin perder mucha fuerza.

solo cuando llegan a la costa comienzan a perder velocidad, al disminuir la profundidad del océano. La altura de las olas, sin embargo, puede incrementarse hasta superar los 30 metros (lo habitual es una altura de 6 o 7 m). Los maremotos son olas que, al llegar a la costa, no rompen. Al contrario, un maremoto solo se manifiesta por una subida y bajada del nivel del mar de las dimensiones indicadas. Su efecto destructivo radica en la importantísima movilización de agua y las corrientes que ello conlleva, haciendo en la práctica un río de toda la costa, además de las olas 'normales' que siguen propagándose encima del maremoto y arrasando, a su paso, con lo poco que haya podido resistir la corriente.

Las fallas presentes en las costas del océano Pacífico, donde las placas tectónicas se introducen bruscamente bajo la placa continental, provocan un fenómeno llamado subducción, lo que genera maremotos con frecuencia. Derrumbes y erupciones volcánicas submarinas pueden provocar fenómenos similares.

La energía de los tsunamis se mantiene más o menos constante durante su desplazamiento, de modo que, al llegar a zonas de menor profundidad, por haber menos agua que desplazar, la altura del tsunami se incrementa de manera formidable. Un maremoto que mar adentro se sintió como una ola no perceptible, debido a su larga longitud de onda puede, al llegar a la costa, destruir hasta kilómetros tierra adentro. Las turbulencias que produce en el fondo del mar arrastran rocas y arena, lo que provoca daño erosivo en las playas que puede alterar la geografía durante muchos años.

Japón, por su ubicación geográfica, es el país más golpeado por los tsunamis.




</doc>
<doc id="8222" url="https://es.wikipedia.org/wiki?curid=8222" title="Petrolero">
Petrolero

Un petrolero es un tipo de buque cisterna diseñado específicamente para el transporte de crudo o productos derivados del petróleo. Actualmente todos los petroleros en construcción son del tipo de doble casco, en detrimento de los más antiguos diseños de un solo casco (monocasco), debido a que son menos sensibles a sufrir daños y provocar vertidos en accidentes de colisión con otros buques o encallamiento.

A partir de este tipo de barcos, surgió el superpetrolero, de mayor capacidad de carga y destinado al transporte de crudo desde Medio Oriente alrededor del Cuerno de África. El superpetrolero "Knock Nevis" es la embarcación más grande del mundo.

Además del transporte por oleoducto, los petroleros son el único medio de transportar grandes cantidades de crudo, a pesar de que algunos han provocado considerables desastres ecológicos al hundirse cerca de la costa provocando el vertido de su carga al mar. Los desastres más famosos han sido los causados por los petroleros "Torrey Canyon", "Exxon Valdez", "Amoco Cadiz", "Erika", "Prestige", "Mar Egeo", "Urquiola", "Polycommander"...

Los buques petroleros, gaseros y cargueros se clasifican según su capacidad de carga en:




</doc>
<doc id="8223" url="https://es.wikipedia.org/wiki?curid=8223" title="Monocasco">
Monocasco

Se denomina monocasco a cierto tipo de chasis de vehículos construidos de una sola pieza, así como también a las embarcaciones cuyos cascos tienen una sola pared. El vocablo monocasco, derivado de la palabra francesa «monocoque», significa «un solo caparazón».

Son los buques que no poseen una doble barrera de separación a lo largo de toda la eslora de carga entre los tanques de carga (p.e. tanques de crudo) y el mar, a diferencia de los más modernos diseños de doble casco, en la marina mercante.

El uso de petroleros monocasco está prohibido en toda la UE, debido a que son más sensibles a sufrir daños y provocar vertidos en accidentes de colisión con otros buques o embarrancamiento. Esta medida se adoptó tras el hundimiento del "Prestige".

Respecto a la resistencia global de diseño, los parámetros son similares a los de los buques de doble casco.

En la navegación de recreo se llama monocasco a las embarcaciones de un solo casco y una sola quilla, a diferencia de los multicasco, como catamaranes y trimaranes.

Se denominan monocasco (o «carrocería autoportante») las carrocerías de los vehículos que incluyen el chasis y el habitáculo de componentes y de pasajeros en una sola pieza con punteras que sirven de soporte al motor. Este sistema es el usado en casi la totalidad de los turismos desde los años 1980. El primer automóvil en incorporar esta técnica constructiva fue el Lancia Lambda, de 1923. Luego otros de gran serie fueron el Chrysler Airflow y el Citroën Traction Avant. 

Tradicionalmente la carrocería se montaba sobre el chasis de bastidores; actualmente esta práctica solo es usada en los vehículos que tengan que desplazar grandes cargas como camionetas pickup/camiones y en algunos vehículos deportivos utilitarios. Los últimos coches con chasis independiente sobre bastidores fueron estadounidenses, en especial el Ford Crown Victoria hasta el 2011 y modelos del Chevrolet Caprice hasta 1996. Otros vehículos utilizan una sistema mixto, en el cual un chasis «semimonocasco» se combina con un chasis de bastidor parcial (subchasis) que soporta el motor, el puente delantero y la transmisión. Ejemplos de esta técnica son el Chevrolet Camaro, Opel Vectra y varios superdeportivos como el Lamborghini Aventador LP700-4.

Hoy en día casi todos los automóviles se construyen con la técnica de monocasco, realizándose las uniones entre las distintas piezas mediante soldadura de punto. Existen vehículos en los cuales hasta los cristales forman parte de sus estructuras, brindando fortaleza y rigidez a todo el conjunto.



</doc>
<doc id="8225" url="https://es.wikipedia.org/wiki?curid=8225" title="Robert Koch">
Robert Koch

Robert Heinrich Hermann Koch (, Clausthal, Reino de Hannover, 11 de diciembre de 1843-Baden-Baden, Gran Ducado de Baden, Imperio alemán, 27 de mayo de 1910) fue un médico y microbiólogo alemán.

Se hizo famoso por descubrir el bacilo de la tuberculosis en 1882, presentando sus hallazgos el 24 de marzo de 1882 a la Sociedad Fisiológica de Berlin, así como el bacilo del cólera en 1883 (después de otros que lo descubrieron independemiente pero no lograron dar a conocer sus descubrimientos internacionalmente) y por el desarrollo de los postulados de Koch. Es considerado uno de los fundadores de la bacteriología. Recibió el Premio Nobel de Medicina en 1905 por sus trabajos sobre la tuberculosis. Fue el primero en demostrar definitivamente, mediante experimentos científicos, que una enfermedad específica (el carbunco o ántrax) fue causada por un microorganismo específico. 

El trabajo de Koch consistió en aislar el microorganismo causante de esta enfermedad y hacerlo crecer en un cultivo puro, utilizándolo para inducir la enfermedad en animales de laboratorio, en su caso la cobaya, aislando de nuevo el germen de los animales enfermos para verificar su identidad comparándolo con el germen original.

Probablemente tan importante como su trabajo en la tuberculosis sean los llamados "postulados de Koch" que establecen las condiciones para que un organismo sea considerado la causa de una enfermedad.

Robert Koch nació en Clausthal en las montañas del Harz, entonces parte del reino de Hannover, como hijo de un ingeniero de minas. Luego de la Guerra austro-prusiana, en 1866, esa ciudad sería parte de Prusia. Estudió medicina bajo la tutela de Friedrich Gustav Jakob Henle en la Universidad de Gotinga y se graduó en 1866. Entonces sirvió en la Guerra Franco-Prusiana y posteriormente se convirtió en médico oficial del distrito en Wollstein (Wolsztyn), en la Prusia polaca. Trabajando con muy pocos recursos, llegó a ser uno de los fundadores de la bacteriología junto con Louis Pasteur.

Después de que Casimir Davaine demostrara la transmisión directa del bacilo del carbunco (también llamado ántrax) entre las vacas, Koch estudió con profundidad esta enfermedad. Inventó métodos para extraer el bacilo de las muestras de sangre y hacerlo crecer en cultivos puros. Descubrió que, mientras que era incapaz de sobrevivir durante periodos largos en el exterior del huésped, podía crear endosporas que sí podían hacerlo. También descubrió el agente causante de la enfermedad del carbunco.

Esas endosporas, incrustadas en el suelo, eran la causa de los inexplicables brotes "espontáneos" de ántrax. Koch publicó sus descubrimientos en 1876 y fue premiado con un trabajo en la Oficina Imperial de Sanidad en Berlín en 1880. En 1881, promovió la esterilización de los instrumentos quirúrgicos mediante el calor.

En Berlín mejoró los métodos que había usado en Wollstein, incluyendo las técnicas de tinción y purificación y los medios de crecimiento bacteriano, como las placas de agar (gracias al consejo de Angelina y Walther Hesse) y la placa de Petri (llamada así por su inventor, su ayudante Julius Richard Petri). Estos dispositivos aún se utilizan actualmente. Con estas técnicas, fue capaz de descubrir la bacteria causante de la tuberculosis ("Mycobacterium tuberculosis") en 1882 (anunció el descubrimiento el 24 de marzo). La tuberculosis era la causa de una de cada siete muertes a mitad del siglo XIX.

En 1883, Koch trabajó en un equipo de investigación francés en Alejandría, Egipto, estudiando el cólera. También trabajó en la India, donde aisló e identificó la bacteria vibrio que causaba el cólera. La bacteria había sido aislada previamente por el anatomista italiano Filippo Pacini en 1854, aunque su trabajo había sido ignorado por la presencia de la teoría miasmática de la enfermedad, en el mismo año también fue descrito por el catalán Joaquim Balcells i Pascual y en 1856 probablemente por los dos portugueses António Augusto da Costa Simões y José Ferreira de Macedo Pinto. Koch desconocía el trabajo de Pacini e hizo su descubrimiento independientemente, y su gran preeminencia permitió que el descubrimiento fuera difundido más ampliamente para el beneficio general. Sin embargo, en 1965 la bacteria fue renombrada "Vibrio cholerae" (Pacini, 1854).

En 1885, fue nombrado profesor de higiene en la Universidad de Berlín, y en 1891 se convirtió en Profesor Honorario de la Facultad de Medicina y director del Instituto Prusiano de Enfermedades Infecciosas (renombrado como Instituto Robert Koch en su honor), una posición a la que renunció en 1904. Comenzó a viajar por todo el mundo, estudiando enfermedades de Sudáfrica, India y Java. Visitó en Mukteshwar lo que ahora se llama Instituto de Investigación Veterinaria India (IVRI, Indian Veterinary Research Institute en inglés), a petición del Gobierno de la India para investigar una plaga en el ganado. El microscopio que usó durante este periodo se conserva en el museo mantenido por el IVRI.

Probablemente tan importante como su trabajo en la tuberculosis, por el que fue galardonado con el Premio Nobel en 1905, son los postulados de Koch, que afirman que para establecer que un organismo sea la causa de una enfermedad, este debe:

Los pupilos de Koch descubrieron los organismos responsables de la difteria, el tifus, la neumonía, la gonorrea, la meningitis cerebroespinal, la lepra, la peste pulmonar, el tétanos y la sífilis, entre otros, usando sus métodos.

Murió el 27 de mayo de 1910 por un ataque al corazón en Baden-Baden, a la edad de 66 años.



</doc>
<doc id="8226" url="https://es.wikipedia.org/wiki?curid=8226" title="Iván Pávlov">
Iván Pávlov

Iván P. Pávlov (; Riazán, -Leningrado, 27 de febrero de 1936) fue un fisiólogo ruso, laureado con el Premio Nobel de Fisiología o Medicina en 1904.

Era hijo de Peter Dmitrievich Pávlov (1823-1899), patriarca ortodoxo, y Varvára Uspénskaya (1826-1890). Comenzó a estudiar teología, pero la dejó para empezar medicina y química en la Universidad de San Petersburgo, siendo su principal maestro Vladímir Béjterev. Tras terminar el doctorado en 1883, amplió sus estudios en Alemania, donde se especializó en fisiología intestinal y en el funcionamiento del sistema circulatorio, bajo la dirección de Ludwid y Haidenhein.

En 1890 obtuvo la plaza de profesor de fisiología en la Academia Médica Imperial y fue nombrado director del Departamento de Fisiología del Instituto de Medicina Experimental de San Petersburgo. En la siguiente década centró su trabajo en la investigación del aparato digestivo y el estudio de los jugos gástricos, El científico dedicó más de 10 años a aprender a hacer orificios en el tracto intestinal. Era una operación muy complicada, ya que el jugo gástrico al salir del intestino corroía los tejidos de este y los de la pared abdominal. La técnica de Pávlov se basaba en introducir un tubo metálico por una pequeña incisura. Era imprescindible una sutura habilidosa de la piel y de la membrana mucosa y cerrar la salida de la cánula con un tapón. De esta manera pudo obtener jugo gástrico de cualquier parte del tracto intestinal, desde las glándulas salivales hasta el intestino grueso, trabajos por los que ganó el premio Nobel de Fisiología o Medicina en 1904, convirtiéndose así en el primer ruso que recibió esta distinción, Los resultados de las investigaciones de Pávlov fueron publicadas en 1897 en el libro "The Work of the Digestive Glands".

Pávlov es conocido sobre todo por formular la ley del reflejo condicional que, por un error en la traducción de su obra al idioma inglés, fue llamada «reflejo condicionado», la cual desarrolló a partir de 1901 con su asistente Iván Filíppovich Tolochínov, al tiempo que en EE. UU. Edwin Burket Twitmyer realizaba observaciones similares. Pávlov observó que la salivación de los perros que utilizaban en sus experimentos se producía ante la presencia de comida o de los propios experimentadores, y luego determinó que podía ser resultado de una actividad psicológica, a la que llamó «reflejo condicional». Esta diferencia entre «condicionado» y «condicional» es importante, pues el término «condicionado» se refiere a un estado, mientras que el término «condicional» se refiere a una relación, que es precisamente el objeto de su investigación.

Realizó el conocido experimento consistente en hacer sonar un metrónomo (a 100 golpes por minuto, aunque popularmente se cree que empleó una campana) justo antes de dar alimento en polvo a un perro, llegando a la conclusión de que, cuando el perro tenía hambre, comenzaba a salivar nada más al oír el sonido del metrónomo (aparato que en ocasiones usan los músicos para marcar el ritmo). Tolochinov, que llamó al fenómeno «reflejo a distancia», comunicó los primeros resultados en el Congreso de Ciencias Naturales en Helsinki en 1903. Posteriormente ese mismo año, Pávlov realizó una exposición detallada de los resultados en el 14º Congreso Médico Internacional en Madrid, donde leyó su trabajo bajo el título "The Experimental Psychology and Psychopathology of Animals".

La Guerra civil rusa y la llegada de los bolcheviques no influyeron en sus investigaciones. A pesar de no sentir simpatía por el nuevo régimen, no sufrió represalias por parte de los comunistas. Después de la Revolución de Octubre fue nombrado director de los laboratorios de fisiología en el Instituto de Medicina Experimental de la Academia de Ciencias de la URSS. En cierta ocasión llegó a declarar: «Por este experimento social que están realizando, yo no sacrificaría los cuartos traseros de una rana.» No hay evidencia de que se haya involucrado en la Revolución de Octubre ni, en general, en el movimiento comunista.

En la década de 1930, volvió a destacarse al anunciar el principio según el cual la función del lenguaje humano es resultado de una cadena de reflejos condicionados que contendrían palabras.

La fundación del conductismo como tal ha sido criticada por algunos filósofos y psicólogos al considerarla una escuela de la Psicología que se centra en la interacción entre el comportamiento y el ambiente, y cómo se puede aprender.

En agosto de 1935 la Unión Soviética celebró el Congreso Mundial de Fisiología en Moscú y Leningrado con la asistencia de más de 900 científicos del mundo. Iván Pávlov fue nombrado como el fisiólogo más importante del mundo. Pávlov clausuró las jornadas con un emotivo discurso: "Mi vida entera se compone de experimentos, nuestro gobierno también experimenta, solo que a más alto nivel".

El 27 de febrero de 1936 Iván Pávlov murió de neumonía. Está enterrado en San Petersburgo.

Las observaciones originales de Pávlov eran simples. Si se ponen alimentos o ciertos ácidos diluidos en el hocico de un perro hambriento, este empieza a segregar un flujo de saliva procedente de determinadas glándulas. Este es el reflejo de salivación, pero eso no es todo. Pávlov observó que el animal también salivaba cuando la comida todavía no había llegado al hocico: la comida simplemente vista u olida provocaba una respuesta semejante. Además, el perro salivaba ante la mera presencia de la persona que por lo general le acercaba la comida o cualquier otro estímulo que sistemáticamente la anunciara. Esto llevó a Pávlov a desarrollar un método experimental para estudiar la adquisición de nuevas conexiones de estímulo-respuesta. Indudablemente, las que había observado en sus perros no podían ser innatas o connaturales de esta clase de animal, por lo que concluyó que debían ser aprendidas (en sus términos, condicionales). El primer paso, cuando se realiza este experimento, es familiarizar al perro con la situación experimental que va a vivir, hasta que no dé muestras de alteración, sobre todo cuando se le coloca el arnés y se lo deja solo en una sala aislada. Se practica una pequeña abertura o fisura en la quijada del perro, junto al conducto de una de las glándulas salivares. Luego, se le coloca un tubito (cánula) de cristal para que salga por él la saliva en el momento en que se activa la glándula salivar. La saliva va a parar a un recipiente de cristal con marcas de graduación, para facilitar su cuantificación.

Uno de sus textos fundamentales, "Reflejos condicionados", se publicó en español en 1929 (Javier Morata, Madrid) con prólogo de Gregorio Marañón y unas palabras del propio autor para la edición española. En 1997 apareció una nueva edición de este texto (Editorial Morata, Madrid).

La magnitud de las respuestas a los diferentes estímulos puede medirse por el volumen total o el número de gotas segregadas en una determinada unidad de tiempo. Desde la habitación contigua, y a través de un cristal, el experimentador puede observar el comportamiento del perro, aplicando los estímulos y valorando las respuestas. Antes de empezar el experimento, Pávlov midió las reacciones de salivación a la comida en el hocico, que fue considerable, mientras que salivó muy poco sometido al estímulo del sonido. A continuación, inició las pruebas de condicionamiento. Hizo sonar el metrónomo (estímulo neutral), e inmediatamente después presentó comida al animal (estímulo incondicional), con un intervalo muy breve. Repitió la relación entre este par de estímulos muchas veces durante varias semanas, siempre cuando el perro estaba hambriento. Después, transcurridos varios días, hizo sonar solamente el metrónomo y la respuesta salival apareció al oírse el sonido, a pesar de que no se presentó la comida.

Se había establecido una relación condicional entre la respuesta de salivar y el sonido que originalmente no provocaba la salivación. Se dice entonces que la salivación del perro ante la comida es una respuesta incondicional; la salivación tras oír la campana es una respuesta condicional que depende de la relación que en la historia del sujeto ha existido entre el sonido y la comida. El estímulo del sonido del metrónomo que originalmente era neutro funciona ahora como un estímulo condicional. Este estímulo condicional (sonido), funciona para el sujeto con esa historia como una señal que avisa que el estímulo incondicional (comida), está a punto de aparecer.

Finalmente, se llamó refuerzo al fortalecimiento de la asociación entre un estímulo incondicional con el condicional. El reforzamiento es un acontecimiento que incrementa la probabilidad de que ocurra una determinada respuesta ante ciertos estímulos. La definición de condicionamiento clásico o respondiente es la formación (o reforzamiento) de una asociación entre un estímulo originalmente neutro y una respuesta (por lo general un reflejo o una secreción glandular, como en el caso de la salivación).Los principios del condicionamiento respondiente se utilizan, entre otros, para la adquisición de hábitos como el control de esfínteres. Los estímulos pueden clasificarse en sensoriales, propioceptivos y verbal.

Así denominó a la relación por la cual en el sistema nervioso central, en especial en el cerebro se establece una asociación, por ejemplo, entre un sonido, con el posible alimento: el sonido (u otro estímulo sustitutivo) funciona como una señal. Pávlov consideró que la mayoría de los animales se rige por un «pensamiento» basado en este sistema de sustituciones reflejas, un primer sistema de señales.

Pero, a diferencia de otros autores, Pávlov consideró que muchos «comportamientos humanos» son más complejos que un sistema de reflejos condicionales simples en un modelo «estímulo/respuesta» lineal. En el "Homo sapiens", Pávlov consideró que se produce un salto cualitativo respecto al primer sistema de señales; en el humano la cuestión ya no se restringe solamente a reflejos condicionales o a estímulos que funcionan de manera sustitutiva directa de la realidad. La complejidad de las funciones psicológicas humanas facilita un segundo sistema de señales que es el lenguaje verbal o simbólico. En este las sustituciones a partir de los estímulos parecen ser infinitas y, sin embargo, altamente ordenadas (lógicas). En gran medida Pávlov postula tal capacidad del segundo sistema de señales porque considera que en el ser humano existe una capacidad de autocondicionamiento (aprendizaje dirigido por uno mismo) que, aunque parezca contradictorio, le es liberador: el ser humano puede reaccionar ante estímulos que él mismo va generando y que puede transmitir (ver información).

La psicología preeminentemente experimental de Pávlov y sus epígonos se denomina reflexología, lo que lleva a confusión a algunas personas, que la confunden con la reflexogenoterapia, una forma de terapia a veces llamada «reflexología».

Pávlov ha influido en su país, durante el siglo XX, de un modo determinante sobre otros importantes investigadores de la Psicología: Lúriya, Leóntiev, Vygotski, Béjterev, Shaunyán, etc. Fuera de Rusia, Watson incorporó a su propia obra la terminología y conceptos pavlovianos. Algunas de las partes de la obra de Pávlov, que por lo general han permanecido ignoradas consistieron en las variaciones sistemáticas que introdujo en sus experimentos.

Por ejemplo, mostró que el intervalo óptimo entre la presentación del estímulo condicional y el incondicional para favorecer el aprendizaje (es decir, la presentación de una respuesta condicional) es de 0.5 segundos. Intervalos mayores o menores entre los estímulos requerían mayor cantidad de ensayos para que se diera el aprendizaje, y con frecuencia las respuestas son más débiles.

De manera semejante, mostró que el orden en la secuencia de presentación de los estímulos era crucial. Si intentaba lograr el establecimiento de nuevas relaciones condicionales presentando primero el estímulo incondicional y luego el neutro (al cual se intentaba que funcionara como condicional), el aprendizaje no ocurría.

Mostró asimismo que no todas las relaciones entre estímulos generaban nuevas respuestas, pues en caso de reflejos, como el rotuliano (estirar la pierna ante un ligero golpe en cierta región de la rodilla), no se aprendía a responder ante los estímulos que «anunciaban» el golpe (Millenson, 1974).

Pávlov también estudió fenómenos como la "generalización", es decir, la presentación de respuestas condicionales ante estímulos parecidos al estímulo condicional original. Descubrió que, a diferencia de los reflejos incondicionales (no aprendidos), la magnitud de la respuesta no era directamente proporcional a la intensidad de los estímulos (es decir, a mayor intensidad del estímulo, dentro de ciertos límites, se presenta una mayor magnitud en la respuesta), sino que en el caso de las relaciones condicionales, la mayor magnitud en la respuesta depende de qué tanto se parezca el estímulo que se presenta respecto al estímulo condicional original. Esto da lugar a una graduación (a veces llamada gradiente), de modo que estímulos ligeramente de menor o mayor intensidad respecto al estímulo condicional original dan lugar a respuestas condicionales de mayor magnitud que las que se presentan ante estímulos de mayor intensidad que el estímulo condicional, aunque la mayor magnitud de la respuesta condicional siempre se da ante el estímulo condicional original (Millenson, 1974).

Por otra parte, Pávlov estudió igualmente la «discriminación de estímulos», esto es, que tanto el sujeto aprende a comportarse de manera diferente ante estímulos distintos que anuncian a otros estímulos. En uno de los ejemplos más conocidos, logró que sus sujetos salivaran ante círculos que anunciaban la presencia de comida y se comportaran de la manera típica de su especie ante estímulos aversivos, tales como descargas eléctricas, en presencia de elipses. Es decir, los perros brincaban, aullaban, se tensaban, etc., ante elipses, pero salivaban ante círculos, si en su historia, cada uno de esos estímulos se presentaba consistentemente como «anuncio» de los estímulos incondicionales correspondientes (choques eléctricos ante las elipses y comida ante los círculos) (Millenson, 1974).

Pávlov estudió muchos otros aprendizajes, tanto en animales como en seres humanos, incluyendo lo que se denominó la inducción de «neurosis experimental», y prácticamente fundó el estudio experimental del comportamiento considerado «anormal» o «psicopatológico», así como su contraparte para modificar varios comportamientos indeseables, incluyendo fobias, tics y comportamientos «neuróticos», de manera que los sujetos aprendieran comportamientos adaptables y eliminaran la ansiedad y otras reacciones indeseables (Sandler y Davidson, 1980).
Pávlov es un ejemplo de que los grandes descubrimientos científicos con frecuencia incluyen una combinación de eventos «accidentales» y una observación de los mismos por personas con suficiente preparación como para no considerarlos como fallas o excepciones, sino como objetos de interés por sí mismos, los cuales son función de su relación con una o más variables independientes.

Uno de estos casos, de acuerdo con Sandler y Davidson (1980), ocurrió cuando una fuerte inundación puso en peligro la integridad de los perros con los que Pávlov experimentaba, pues el sótano en el que se encontraban sus jaulas comenzó a llenarse de agua. Pávlov y algunos de sus ayudantes fueron al laboratorio a pesar de las condiciones ambientales y pusieron a salvo a los perros. El hecho pudo no haber trascendido, pero ocurrió que, cuando se intentó reinstalar a los perros en el sótano, varios aspectos de su comportamiento presentaron variaciones «extrañas». Aunque antes se habían comportado de manera dócil ante los investigadores, ahora eran hostiles; además, dejaron de comer con regularidad, se aislaron, dejaron de tener relaciones sexuales y con frecuencia aullaban como si hubiera otros perros o personas, aunque no estuvieran ahí. Este comportamiento se podría considerar como «neurótico». Por otra parte, dicho comportamiento se aminoraba cuando los perros eran trasladados a ambientes muy diferentes al del sótano. Pávlov razonó, en sus términos, que la presencia intempestiva e intensa de fuertes estímulos aversivos había ocasionado un condicionamiento ante los estímulos que estaban presentes en el sótano.

Después de reflexionar sobre esto, instauró una manera sistemática para revertir los efectos de ese condicionamiento. Empezó dejando a los perros en un ambiente bastante diferente al del sótano y, cuando los perros se comportaron de manera «normal», comenzó a sustituir de manera cuidadosa y gradual distintos estímulos del nuevo ambiente (desvanecimiento por sustracción) por otros que habían estado presentes en el sótano (desvanecimiento por adición). Al final, los perros pudieron regresar al sótano, mientras su comportamiento permaneció completamente «normal».

Pávlov también notó que podía inducir comportamientos «neuróticos» al presentar discriminaciones muy difíciles. En el caso mencionado del círculo (ante el cual se presentaba comida) y la elipse (ante la que se presentaba una descarga eléctrica), los sujetos se comportaban de manera apropiada ante cada uno, después de una serie de ensayos (digamos, por ejemplo, 50 ensayos). Sin embargo, cuando el círculo y la elipse se hicieron cada vez más semejantes, llegó un punto en el cual los sujetos se comportaban de manera semejante a la de los perros que habían sufrido la experiencia aversiva en el sótano. Pero al restablecer las condiciones originales respecto al círculo y la elipse, los sujetos volvieron a comportarse gradualmente del modo adecuado ante cada uno, aunque el número de ensayos requeridos era aproximadamente el doble que el original (digamos, 100 ensayos). A medida que los sujetos discriminaron adecuadamente el círculo de la elipse, su comportamiento fuera de la situación experimental también cambió de «neurótico» a «normal».

El razonamiento de Pávlov fue del tipo: si se pudo inducir un comportamiento neurótico bajo ciertas condiciones (neurosis experimental), también se puede modificar si se cambian las variables independientes de las cuales es función. Pávlov de esta manera inauguró lo que se puede considerar la modificación experimental del comportamiento en Rusia.

Tanto el estudio científico del comportamiento «anormal», como su modificación, fueron influidos de manera notable por el tipo de hallazgos y razonamientos de Pávlov.






</doc>
<doc id="8227" url="https://es.wikipedia.org/wiki?curid=8227" title="Emil Adolf von Behring">
Emil Adolf von Behring

Emil Adolf von Behring (n. Hansdorf, Prusia Oriental, 15 de marzo de 1854-Marburgo, Alemania, 31 de marzo de 1917) fue un bacteriólogo alemán que recibió el primer Premio Nobel en Fisiología o Medicina en 1901.

Emil Adolf von Behring nació el 15 de marzo de 1854 en Hansdorf, localidad de la antigua Prusia Oriental, que actualmente corresponde con Ławice en Polonia. Fue el mayor de 13 hijos del segundo matrimonio de un profesor de colegio.

Ingresó en la Academia de Medicina Militar en Berlín en 1874, obteniendo la licenciatura en 1878 y aprobando el examen estatal en 1880. Trabajó como cirujano militar. Realizó prácticas de trabajo sobre los problemas relacionados con las enfermedades contagiosas. En 1889 abandonó el ejército para ingresar como ayudante de Robert Koch en el Instituto de Higiene de la Universidad de Berlín.

En 1891 se trasladó al Instituto de Enfermedades Infecciosas, dirigido por el mismo Koch. Fue catedrático de la Universidad de Halle en 1894 y, en 1895, director del Instituto de Higiene de Marburgo, hasta su fallecimiento el 31 de marzo de 1917.

En 1890 descubrió la antitoxina del tétanos junto con el bacteriólogo japonés Shibasaburo Kitasato. Descubrieron que al inyectar el suero sanguíneo de un animal afectado por el tétanos a otro, se genera inmunidad a la enfermedad en el segundo. Comprobaron que los animales inmunizados contra el tétanos presentaban esta cualidad porque debían disponer de alguna sustancia capaz de controlar la infección. Analizando la sangre de cuyes inmunizados contra el tétanos, comprobó que al inyectar el suero de estos animales en otros no inmunizados se podían conseguir buenos resultados terapéuticos.

En 1891 trataron con suero a una niña enferma de difteria salvando su vida. Esto le hizo sospechar a Behring la existencia de unas sustancias (que llamó antitoxinas) que eliminaban las toxinas segregadas por las bacterias, lo que supuso un gran avance en el conocimiento de las defensas corporales.

Poco después hizo públicos los resultados de su trabajo sobre la aplicación del suero contra la difteria, en el que demostraba que el poder de resistencia a la enfermedad no reside en las células del cuerpo, sino en el suero sanguíneo libre de células. Por este trabajo obtuvo el primer Premio Nobel de Fisiología y Medicina en 1901.

En el caso del tétanos y la difteria, von Behring provocaba la inmunidad con el suero de un animal previamente infectado. Tras nuevos trabajos en Marburgo con otras antitoxinas, introdujo en 1913 un sistema de inoculación, todavía en vigor, capaz de inmunizar a los niños contra la difteria.






</doc>
<doc id="8228" url="https://es.wikipedia.org/wiki?curid=8228" title="Ronald Ross">
Ronald Ross

Ronald Ross (Almora, India; 13 de mayo de 1857-Londres, 16 de septiembre de 1932) fue un naturalista, médico, matemático, zoólogo, entomólogo escocés, quien relacionó la malaria con los mosquitos.

Estudió medicina en el Hospital de St. Bartholomew de Londres. Médico militar en 1881, once años después comenzó a investigar la transmisión y el control de la malaria. Mientras dirigía una expedición por África Occidental en 1889, identificó la presencia de mosquitos portadores de la enfermedad y organizó su exterminio a gran escala.

En 1895 Ross puso en marcha una serie de experimentos que demostraron que la malaria es transmitida por mosquitos; descubrió también el ciclo vital del parásito de la malaria en el mosquito Anopheles. Por este descubrimiento fue galardonado en 1902 con el Premio Nobel de Fisiología y Medicina.

En 1913 fue nombrado médico de enfermedades tropicales del King's College Hospital, en Londres. Poco después fue nombrado director jefe del Instituto y Hospital para Enfermedades Tropicales Ross de Londres. Ross fue elegido miembro de la Royal Society en 1901 y nombrado sir en 1911.

También publicó poemas, novelas y estudios matemáticos.





























</doc>
<doc id="8230" url="https://es.wikipedia.org/wiki?curid=8230" title="Camillo Golgi">
Camillo Golgi

Bartolomeo Camillo Emilio Golgi (Corteno Golgi, Italia, 7 de julio de 1843 - Pavía, 21 de enero de 1926) fue un médico y citólogo italiano. Ideó los métodos de tinción celular a base de cromato de plata, procedimiento que permitió (tanto a él mismo como a otros investigadores) realizar importantes descubrimientos, especialmente acerca de las neuronas y su fisiología. Recibió el Premio Nobel de Medicina (conjuntamente con el español Santiago Ramón y Cajal) en 1906.

Estudió medicina en la Universidad de Pavía, donde se graduó en 1865. Trabajó algún tiempo en la clínica psiquiátrica del criminólogo Cesare Lombroso, pero pronto se interesó por la histología. En 1872 comenzó a trabajar en el pabellón de incurables de un hospital de Abbiategrasso. Ejerció como profesor de anatomía en las Universidades de Turín y Siena y como catedrático de histología en la Universidad de Pavía, de la que llegó a ser decano de la Facultad de Medicina y rector.

A pesar de los escasos medios con que contaba, logró importantes resultados con sus experimentos, entre los que destaca el método de la tintura mediante cromato de plata, que supuso una revolución en el estudio en laboratorio de los tejidos nerviosos. Empleando este método, identificó una clase de célula nerviosa dotada de unas extensiones (o dendritas) mediante las cuales se conectan entre sí otras células nerviosas. Este descubrimiento permitió a Wilhelm von Waldeyer-Hartz formular la hipótesis de que las células nerviosas son las unidades estructurales básicas del sistema nervioso, hipótesis que más tarde demostraría Santiago Ramón y Cajal, quien desarrolló la teoría neuronal.

En 1876, tras su regreso a la Universidad de Pavía, continuó el examen de las células nerviosas, y obtuvo pruebas de la existencia de una red irregular de fibrillas, cavidades y gránulos, que en su honor en adelante se denominaría aparato de Golgi y que desempeña un papel esencial en operaciones celulares como la construcción de la membrana, el almacenamiento de lípidos y proteínas o el transporte de partículas a lo largo de la membrana plasmática.

Entre 1885 y 1893 dedicó sus investigaciones al estudio del paludismo, y llegó a resultados tan importantes como la distinción entre el paludismo terciano y cuartano, en cuanto patologías provocadas por dos especies diferentes de un mismo protozoo parásito denominado Plasmodium, así como la identificación del acceso febril como originado por la liberación por parte de dicho organismo de esporas en el flujo sanguíneo.

En 1906 Golgi recibió el Premio Nobel de Medicina conjuntamente con Santiago Ramón y Cajal (1852-1934) por sus estudios sobre la estructura del sistema nervioso.

Murió en Pavía, Italia, en enero de 1926.

En Pavia existen varios lugares que rinden homenaje a Golgi.

Llevan el nombre de Golgi en su memoria:




</doc>
<doc id="8232" url="https://es.wikipedia.org/wiki?curid=8232" title="Derrame pleural">
Derrame pleural

El derrame pleural es una acumulación patológica de materia prima en el espacio pleural. También se le conoce como pleuresía o síndrome de interposición líquida. Es una enfermedad frecuente con más de 50 causas reconocidas incluyendo enfermedades locales de la pleura, del pulmón subyacente, enfermedades sistémicas, disfunción de órganos y fármacos.

En condiciones anatómicas y fisiológicas, existe una escasa cantidad de líquido pleural de no más 10-15 ml en cada hemitórax, que lubrica y facilita el desplazamiento de las dos hojas pleurales que delimitan la cavidad pleural. Existe un trasiego fisiológico de líquido que se filtra, pero cuando hay un desequilibrio entre la formación y la reabsorción se produce el derrame pleural.
Tanto la pleural visceral como la parietal tienen irrigación sanguínea dependiente de la circulación sistémica, pero difieren en el retorno venoso. Los capilares de la pleura visceral drenan en las venas pulmonares mientras que los de la parietal lo hacen en la vena cava.

El líquido puede tener dos orígenes distintos, puede ser el resultado de un exudado o de un trasudado.

El trasudado se da en casos de insuficiencia cardíaca congestiva (ICC) en un 40-72%, mientras que el exudado es más frecuente en cuadros paraneumónicos (50-70%), neoplasias (42-60%) y tuberculosis (23,5%).

Se dan principalmente en la Insuficiencia cardíaca ICC, el 80% son derrames bilaterales. Otras causas son la cirrosis hepática, la insuficiencia renal crónica, el síndrome nefrótico, la diálisis peritoneal.

El líquido del hidrotórax tiene un pH neutro, se caracteriza por tener menor densidad y menor concentración de proteínas < 3gr/dl), mientras que aumentan las LDH.

Las efusiones trasudativas y exudativas de la pleura se diferencian comparando la bioquímica del líquido pleural de aquellos de la sangre. De acuerdo al meta-análisis, dichas efusiones exudativas pleurales necesitan al menos uno de los siguientes criterios:

Su clínica dependerá en gran medida de la enfermedad de base. Habitualmente cursan con disnea, ortopnea, nicturia y edemas maleolares. Sus signos más característicos son la matidez hídrica a la percusión y la disminución tanto del murmullo vesicular como de las vibraciones vocales.

Las causas que producen el exudado pueden tener diversos orígenes:

En su clínica, predominan dolor, tos, disnea, cianosis, fiebre y arritmias. Sus signos son similares a los del hidrotórax, disminución de las vibraciones vocales y del murmullo vesicular y matidez.

Aunque es fundamental la historia clínica y examen físico en el cual los pacientes presentan datos sugestivos del derrame como: disminución de los movimientos respiratorios del lado afectado, disminución de vibraciones vocales, matidez a la percusión, así como disminución o ausencia de los ruidos respiratorios, es de gran relevancia también la toracocentesis, que permite analizar el líquido, realizando citología, antibiogramas y la bioquímica. La broncoscopia, puede aportar también información. Si el diagnóstico no es concluyente, se puede recurrir a la realización de una biopsia transparietal o por toracoscopía, e incluso la toracotomía.

Se indica en derrames pleurales de etiología desconocida y con más de 1 cm hasta la pared. Su realización nos permite analizar el líquido:

El derrame se hace visible en la radiografía cuando es mayor de 75 ml, puede aparecer libre o loculado. En caso de que existan dudas, es recomendable la realización de una radiografía en decúbito lateral del lado afecto. El derrame pleural puede presentar imágenes radiológicas atípicas como:

En algunos casos la ecografía de tórax puede proporcionar más información, siendo su mayor utilidad la detección de anormalidades subpulmonares y subfrénicas. También se utiliza para guiar la toracocentesis en derrames pequeños o loculados. No obstante no es práctico recomendar esta exploración a todos los pacientes.

La TAC se utilizará en caso de que exista patología pulmonar asociada o cuando se requiera definir mejor la localización anterior o posterior del proceso. También será útil para diferenciar una lesión pleural de una lesión en el parénquima pulmonar.

En casos de derrame masivo, el mediastino puede ser empujado por la presión ejercida por el líquido, pero si el mediastino está centrado hay que sospechar de obstrucción bronquial proximal. Las técnicas de diagnóstico por imagen ayudan a determinar este desplazamiento.

El tratamiento debe abordar la enfermedad causante y al derrame en sí. El trasudado generalmente responden al tratamiento de la causa subyacente, y la toracentesis terapéutica sólo se indica cuando exista un derrame masivo que cause disnea severa.

Los casos de derrame pleural causado por enfermedad maligna deben ser tratados con quimioterapia o radioterapia. Se puede intentar una pleurodesis química instalando algunos compuestos dentro del espacio pleural para producir una reacción fibrosa que oblitere el espacio en algunos pacientes seleccionados que tienen derrame maligno persistente a pesar de quimio o radioterapia.

El derrame paraneumónico no complicado por lo general responde a la terapia con antibióticos sistémicos. En los casos complicados se requiere del drenaje por un tubo de toracostomía en los casos de empiema, cuando la glucosa de líquido sea menor de 40 mg/dL o el pH sea menor de 7.2.

La mayoría de los hemotórax se drenan con un sello de tórax. Se requiere de toracotomía cuando no pueda controlarse el sangrado, para remover una cantidad de coágulos, o para tratar otras complicaciones del traumatismo torácico. Por el contrario los hemotórax pequeños y estables pueden resolverse de manera conservadora.




</doc>
<doc id="8233" url="https://es.wikipedia.org/wiki?curid=8233" title="Hemotórax">
Hemotórax

El hemotórax es la presencia de sangre en la cavidad pleural. Generalmente está causado por lesiones torácicas, (arterias)pero puede haber otras causas, tales como cáncer pulmonar o pleural, o incluso cirugías torácicas o del corazón.

En una lesión traumática con un objeto contundente, una costilla puede herir parte del tejido del pulmón o de una arteria, causando que la sangre entre en el espacio pleural, en el caso de una lesión cortopunzante o una herida de bala, puede haber compromiso de pulmón. Un hemotórax puede ir asociado con un neumotórax (entrada de aire en el espacio pleural), y dependiendo de la cantidad de sangre, el hemotórax puede complicarse con un estado de shock.

La cantidad de sangre acumulada varia de acuerdo con el diámetro del vaso sanguíneo roto y tiempo que ha transcurrido desde que se produjo la lesión, así tenemos que en un adulto se puede acumular 3 litros o más en cada espacio pleural. El origen de este sangrado puede ser: vasos intercostales, pulmones, vasos bronquiales, vasos pulmonares y los grandes vasos torácicos.


Los síntomas del hemotórax son: dificultad para respirar, dolor torácico, ansiedad o inquietud, y frecuencia cardíaca acelerada. El médico puede confirmar su diagnóstico con un examen físico que puede revelar una disminución de ruidos respiratorios, la aparición de matidez a la percusión, o por medio de una radiografía de tórax.

El tratamiento consiste en estabilizar al paciente, detener la hemorragia y extraer la sangre del espacio pleural, sin embargo, también se debe considerar la sangre, es decir, emplear la sangre extraída del tórax como una transfusión. Además se debe tratar la causa del hemotórax, pero en el caso de una lesión traumática, dependiendo de la gravedad, la simple colocación de un tubo de drenaje es suficiente, sin necesidad de una cirugía. El pronóstico es casi siempre favorable dependiendo de la causa del hemotórax y de la rapidez con la que se aplicó el tratamiento.

Por lejos, la causa más común del hemotórax es el trauma, por un objeto contundente o por herida penetrante, resultando en la ruptura de la membrana serosa tanto pleural como parietal. Esta ruptura provoca que la sangre se vacíe en el espacio pleural, igualando las presiones entre este y los pulmones. La pérdida de sangre puede ser masiva en personas con esta condición, puesto que cada lado del tórax puede contener 30-40% del volumen sanguíneo total de una persona. Incluso pequeñas lesiones en la caja torácica pueden llevar a una hemotórax de importancia.


</doc>
<doc id="8235" url="https://es.wikipedia.org/wiki?curid=8235" title="Acutancia">
Acutancia

La acutancia de una imagen es el grado de contraste que se observa en el límite entre detalles que difieren por su luminancia o densidad óptica. Cuanto más contrastado sea el límite entre una zona oscura y otra más clara, mayor es la acutancia y con ella la nitidez percibida en la imagen. La definición o resolución de la imagen "no" crecen cuando aumenta la acutancia, pero sí la capacidad para distinguir los detalles y la sensación subjetiva, que es la de un aumento de definición, de detalle. El sistema perceptivo visual humano es capaz de distinguir detalles más pequeños cuando su contraste es mayor.

La acutancia también describe la capacidad de registro de un observador o de una cámara fotográfica en términos de la definición del contraste percibida.

En este sentido la acutancia puede variar según el tipo de soporte sensible a la luz que emplee dicha cámara, sea de película emulsionada, negativo o sensor CCD de una cámara digital.


</doc>
<doc id="8236" url="https://es.wikipedia.org/wiki?curid=8236" title="Domingo Santos">
Domingo Santos

Pedro Domingo Mutiñó (Barcelona, 15 de diciembre de 1941 - Zaragoza, 2 de noviembre de 2018), cuyo seudónimo es Domingo Santos, fue un escritor, traductor, editor de colecciones de libros y revistas, antólogo y articulista español. Se le considera uno de los más notorios escritores españoles de ciencia ficción contemporáneos. Con más de 800 traducciones, 45 libros y más de 25 antologías, Domingo Santos es considerado uno de los mejores y primeros escritores de ciencia ficción española. Ha utilizado también otros seudónimos como Peter Danger o Peter Dean. Aunque la mayoría de libros, están firmados con el seudónimo de Domingo Santos.

Pedro Domingo colaboró con la revista Bang! Fundó luego la revista española de ciencia ficción "Nueva Dimensión" junto a Sebastián Martínez y Luis Vigil.

Publicó su primera novela en 1959 y desde entonces ha alternado las actividades de escritor, editor, recopilador, director de colecciones o traductor, siendo uno de los máximos promotores del género.

Autor de más de una veintena de novelas, entre sus obras destaca Gabriel, una de sus mejores, donde relata la historia de un robot demasiado humano que se encuentra en una especie de cruzada. Gabriel fue publicada en la colección Nebulae en los años 60 y traducida a diversos idiomas, constituyéndose en la primera novela de este género que traspasó las fronteras españolas. 

Fue jurado del Premio UPC durante los primeros cinco años de vida, y posteriormente ha sido finalista del galardón (1996) y ganador de la mención de la edición de 1997. 

Su nombre está ligado también al Premio Domingo Santos, que cada año organiza el congreso español de ciencia ficción (HispaCon), a instancias de la AEFCFT.



En colaboración con Luis Vigil:







</doc>
<doc id="8238" url="https://es.wikipedia.org/wiki?curid=8238" title="Gu ta gutarrak">
Gu ta gutarrak

Gu ta gutarrak ("Nosotros y los nuestros", en euskera) es un relato de ciencia ficción, en castellano y en clave de humor, escrito por la matemática argentina Magdalena Mouján Otaño y en el que se narra la paradójica conclusión de un viaje de un grupo de vascos en una máquina del tiempo. Obtuvo el primer premio en el concurso de cuentos de la Segunda Convención de Ciencia Ficción de la República Argentina, en Mar del Plata, en julio de 1968.

En 1970, la revista española "Nueva Dimensión" publicó el relato en su número 14. A pesar de haber sido presentado éste a Depósito Previo, y de haber sido convenientemente aprobada su distribución, pocos días después el Tribunal de Orden Público ordenó retirar de la circulación el número, pues consideraba que el cuento de Mouján Otaño contravenía el artículo segundo de la Ley de Prensa, obra del ministro de Información y Turismo, Manuel Fraga Iribarne. Según el fiscal especial que cursó la denuncia, "Gu ta gutarrak" atentaba contra la unidad de España. Consecuentemente, tras el secuestro cautelar del número 14, se sustituyeron las páginas del relato por varias historietas de Johnny Hart reunidas bajo el título de "Formicología", para poder continuar con su venta. El juicio contra "Nueva Dimensión" no llegó a llevarse a cabo, pero el caso produjo gran polémica en el "fandom" internacional. Cien números después, en la edición de julio-agosto de 1979, "Nueva Dimensión" publicó de nuevo el relato, como recordatorio de estos hechos y a modo de desagravio contra su autora.



</doc>
<doc id="8246" url="https://es.wikipedia.org/wiki?curid=8246" title="Yom Kipur">
Yom Kipur

Yom Kipur o Yom Kippur (en hebreo: יום כיפור), también conocido como Día de la Expiación, es el día más sagrado del año judío. Es conocido como el Día de la expiación, del perdón y del arrepentimiento de corazón o de un arrepentimiento sincero. Es, asimismo, el último de los diez días de arrepentimiento.

Es uno de los "Yamim Noraim" (en hebreo, «Días extremadamente santos»). Ellos comprenden "Rosh Hashaná" (Año Nuevo Judío), diez días del arrepentimiento, y su culminación, con el "Yom Kipur". En el calendario hebreo, el "Yom Kipur" comienza en el anochecer del noveno día del mes de "Tishrei" y continúa hasta el anochecer del siguiente día. 

"Yom Kipur" es el día judío del arrepentimiento, considerado el día más santo y más solemne del año. Su tema central es la expiación y la reconciliación. La comida, la bebida, el baño o cualquier tipo de limpieza corporal como el lavado de dientes, la utilización de cuero, el untamiento de cremas o bálsamos en el cuerpo y las relaciones conyugales están prohibidos. El ayuno empieza en el ocaso y termina al anochecer del día siguiente. Los servicios de oración de "Yom Kipur" comienzan con la oración conocida como "Kol Nidre", que debe ser recitada antes de la puesta del sol. El "Kol Nidre" (en arameo «todos los votos») es una abrogación pública de votos religiosos hechos por judíos durante el año precedente. Esto solo concierne a los votos incumplidos hechos entre la persona y Dios, y no anula votos hechos a otras personas. 

Un "talit" (manto de oración cuadrangular) se pone para las oraciones de la tarde - el único servicio de la tarde del año en el cual se hace esto. La oración de "Neilá" es un servicio especial que se celebra solo durante el día de "Yom Kipur", y marca el cierre de las fiestas. "Yom Kipur" culmina con el sonar del "shofar", que marca la conclusión del ayuno. Siempre se observa como un día festivo, tanto dentro como fuera de los límites de la Tierra de Israel. 

Los judíos sefardíes (los judíos de origen español, portugués y norteafricano) se refieren a "Yom Kipur" como «el ayuno blanco» y ello se debe a la tradición de vestirse de blanco durante los "Yamim Noraim".

Mes primero: Tishrei. 




</doc>
<doc id="8247" url="https://es.wikipedia.org/wiki?curid=8247" title="Macana">
Macana

La macana es un arma ofensiva, a manera de machete o de porra, hecha con maderas duras y a veces con filo de pedernal, que usaban los nativos americanos. El término macana de origen Taíno se emplea ampliamente para referirse a las mazas de madera que utilizaban los guerreros de los pueblos precolombinos en América central y Sudamérica, aunque también suele usarse para designar a los garrotes pesados.

Algunas macanas de las que se tiene constancia son el macuahuitl usado por los mexicas, que tenía navajas u hojas de obsidiana o pedernal incrustados en sus lados, y una especie de macana-lanza utilizada por los incas y otros pueblos andinos, que consistía en un asta de madera con una piedra u otro objeto contundente en un extremo, que tenía regularmente la forma de una estrella para maximizar el daño al golpear, pudiendo provocar heridas muy graves como fracturas y desgarraduras. Era el arma más común en el arsenal del ejército inca.

En el español moderno el término se usa (de forma coloquial) para referirse a un arma utilizada por guardias de seguridad o policías antidisturbios, con una forma muy similar a las tonfas de Okinawa.

En el lunfardo rioplatense significa un "despropósito" o "mentira" que se dice.


</doc>
<doc id="8249" url="https://es.wikipedia.org/wiki?curid=8249" title="Aceite vegetal">
Aceite vegetal

Un aceite vegetal es un triglicérido extraído de una planta. El término "aceite vegetal" puede definirse estrechamente como referido solo a los aceites vegetales que son líquidos a temperatura ambiente, o definidos ampliamente sin tener en cuenta el estado de la materia de la sustancia a una temperatura dada. Por esta razón, los aceites vegetales que son sólidos a temperatura ambiente a veces se llaman grasas vegetales. En contraste con estos triglicéridos, las ceras vegetales carecen de glicerina en su estructura. Aunque muchas partes de la planta pueden producir aceite, en la práctica comercial, el aceite se extrae principalmente de las semillas.

En el envasado de alimentos, el término "aceite vegetal" se utiliza a menudo en las listas de ingredientes en lugar de especificar la planta exacta que se está utilizando, especialmente cuando el aceite utilizado es menos conveniente para el consumidor o si se utiliza una mezcla, como los aceites de palma, colza, soja y cártamo (mientras que el aceite de coco y el aceite de oliva pueden ser percibidos como más deseables).

Los aceites extraídos de las plantas se han utilizado desde tiempos antiguos y en muchas culturas. Como ejemplo, en una cocina de 4.000 años de edad, desenterrada en el Parque Estatal de Charlestown de Indiana, el arqueólogo Bob McCullough de la Universidad de Indiana-Universidad Purdue Fort Wayne encontró evidencia de que grandes losas de roca se utilizaba para aplastar nueces de nogal y el aceite era entonces extraído con agua hirviendo. La evidencia arqueológica muestra que las aceitunas fueron convertidas en aceite de oliva por 6000 a. C. y 4500 a. C. en el actual Israel y Palestina.

Muchos aceites vegetales se consumen directamente, o indirectamente como ingredientes en los alimentos – un papel que comparten con algunas grasas animales, incluyendo mantequilla, ghee, manteca de cerdo y Schmaltz. Los óleos cumplen una serie de objetivos en este papel:
En segundo lugar, los aceites se pueden calentar y utilizar para cocinar otros alimentos. Los aceites adecuados para este objetivo deben tener un alto punto de inflamación. Tales aceites incluyen los aceites de cocina principales – soja, rabina, canola, girasol, cártamo, cacahuete, algodón, etc. Los aceites tropicales, tales como el coco, la palma, y los aceites de salvado de arroz, se valoran particularmente en las culturas asiáticas para cocinar a alta temperatura, debido a sus puntos de inflamación inusualmente altos.

Los aceites vegetales no saturados pueden ser transformados a través de la "hidrogenación" parcial o completa en aceites de mayor punto de fusión. El proceso de hidrogenación implica "aspersión" el aceite a alta temperatura y presión con hidrógeno en presencia de un catalizador, típicamente un compuesto de níquel en polvo. A medida que cada doble enlace carbono-carbono se reduce químicamente a un solo enlace, dos átomos de hidrógeno forman enlaces individuales con los dos átomos de carbono. La eliminación de los enlaces dobles mediante la adición de átomos de hidrógeno se denomina saturación; a medida que aumenta el grado de saturación, el aceite progresa hacia ser completamente hidrogenado. Un aceite puede ser hidrogenado para aumentar la resistencia a la rancia (oxidación) o para cambiar sus características físicas. A medida que aumenta el grado de saturación, aumenta la viscosidad del aceite y el punto de fusión.

El uso de aceites hidrogenados en los alimentos nunca ha sido completamente satisfactorio. Porque el brazo central del triglicérido es blindado algo por los ácidos grasos del extremo, la mayor parte de la hidrogenación ocurre en los ácidos grasos del extremo, así haciendo que la grasa resultante más quebradiza. Una margarina hecha de aceites naturalmente más saturados será más plástica (más "extendible") que una margarina hecha de aceite de soja hidrogenado. Mientras que la "hidrogenación completa" produce ácidos grasos en gran parte saturados, la hidrogenación parcial da como resultado la transformación de los ácidos grasos cis insaturados a los ácidos grasos trans no saturados en la mezcla de aceite debido al calor usado en la hidrogenación. Los aceites parcialmente hidrogenados y sus grasas trans se han relacionado con un mayor riesgo de mortalidad por enfermedad coronaria, entre otros riesgos de salud aumentados.

En los Estados Unidos, el Estándar de la Identidad para un producto etiquetado como "margarina de aceite vegetal" especifica solamente canola, cártamo, girasol, maíz, soja, o el aceite de cacahuete puede ser utilizado. Los productos no etiquetados "margarina de aceite vegetal" no tienen esa restricción.

Los aceites vegetales se utilizan como ingrediente o componente en muchos productos manufacturados.

Muchos aceites vegetales se utilizan para hacer jabones, productos de la piel, velas, perfumes y otros productos de cuidado personal y cosméticos. Algunos aceites son particularmente convenientes como aceites de secado, y se utilizan en la fabricación de pinturas y de otros productos de tratamiento de madera. El aceite Dammar (una mezcla de aceite de linaza y resina Dammar), por ejemplo, se utiliza casi exclusivamente para tratar los cascos de los barcos de madera. Los aceites vegetales se utilizan cada vez más en la industria eléctrica, ya que los aisladores como aceites vegetales no son tóxicos para el medio ambiente, biodegradables si se derraman y tienen altos puntos de inflamación y fuego. Sin embargo, los aceites vegetales son menos estables químicamente, por lo que se utilizan generalmente en sistemas donde no están expuestos al oxígeno, y son más caros que el destilado de petróleo crudo. Los tetraésteres sintéticos, que son similares a los aceites vegetales pero con cuatro cadenas de ácidos grasos comparados con los tres normales encontrados en un éster natural, son fabricados por la esterificación de Fischer. Tetraésteres generalmente tienen alta estabilidad a la oxidación y han encontrado uso como lubricantes de motores. El aceite vegetal se utiliza para producir el líquido hidráulico y lubricante, ambos biodegradables.

Un factor limitante en los usos industriales de los aceites vegetales es que todos estos aceites son susceptibles a volverse rancios. Los aceites que son más estables, tales como aceite de behen o aceite mineral, se prefieren así para los usos industriales. El aceite de ricino tiene aplicaciones industriales numerosas, debido a la presencia del grupo hidroxilo en el ácido graso. El aceite de ricino es un precursor del nilón 11.

El aceite vegetal se utiliza en la producción de algunos alimentos para mascotas. AAFCO define el aceite vegetal, en este contexto, como el producto de origen vegetal obtenido extrayendo el aceite de las semillas o de las frutas que se procesan para los propósitos comestibles.

"Artículo principal: Aceite vegetal combustible"

Los aceites vegetales también se usados para hacer biodiésel, que se puede utilizar como el diesel convencional. Algunas mezclas del aceite vegetal se utilizan en vehículos sin modificar pero el aceite vegetal recto, también conocido como aceite vegetal puro, necesitan ser vehículos especialmente preparados que tengan un método de calentar el aceite para reducir su viscosidad. El uso de aceites vegetales como energía alternativa está aumentando y la disponibilidad de biodiésel en todo el mundo va en aumento.

El NNFCC estima que el ahorro neto total de gases de efecto invernadero al utilizar aceites vegetales en lugar de alternativas basadas en combustibles fósiles para la producción de combustible, oscila entre el 18 y el 100%.

El proceso de producción del aceite vegetal implica el retiro del aceite de los componentes de la planta, típicamente semillas. Esto se puede hacer mediante extracción mecánica utilizando un molino de aceite o una extracción química utilizando un disolvente. El aceite extraído puede ser purificado y, si es necesario, refinado o alterado químicamente.

Los aceites se pueden quitar vía extracción mecánica, llamado "machacamiento" o "prensado." Este método se utiliza típicamente para producir los aceites más tradicionales (por ejemplo, oliva, coco etc.), y es preferido por la mayoría de los clientes de la "comida saludable" en los Estados Unidos y en Europa. Hay varios tipos diferentes de extracción mecánica. El expulsor-prensado para la extracción es común, aunque la prensa de tornillo, la prensa de espolón, y el Ghani (mortero) son también usados. Las prensas oleaginosas se utilizan comúnmente en los países en desarrollo, entre las personas para las que otros métodos de extracción serían prohibitivamente costosos; el Ghani se utiliza sobre todo en la India. La cantidad de aceite extraída usando estos métodos varía extensamente, según lo demostrado en la tabla siguiente para extraer la mantequilla de mowra en la India:
El procesamiento de aceite vegetal en aplicaciones comerciales es comúnmente realizado por extracción química, utilizando extractos de disolventes, lo que produce mayores rendimientos y es más rápido y menos costoso. El disolvente más común es el hexano derivado del petróleo. Esta técnica se utiliza para la mayor parte de los "más nuevos" aceites industriales tales como soja y aceites de maíz.

El dióxido de carbono supercrítico se puede utilizar como alternativa no tóxica a otros solvente.

Los aceites pueden ser parcialmente hidrogenados para producir varios aceites de ingredientes. Los aceites ligeramente hidrogenados tienen características físicas muy similares al aceite de soja regular, pero son más resistentes a volverse rancios. Los aceites de la margarina necesitan ser sobre todo sólidos en 32 °C (90 °F) de modo que la margarina no se derrita en cuartos calientes, con todo necesita ser totalmente líquido en 37 °C (98 °F), de modo que no deje un sabor "mantecoso" en la boca.

Endurecer el aceite vegetal se hace levantando una mezcla de aceite vegetal y un catalizador en un espacio prácticamente al vacío a temperaturas muy altas, e introduciendo el hidrógeno. Esto hace que los átomos de carbono del aceite rompan enlaces dobles con otros carbonos, cada carbono formando un nuevo enlace simple con un átomo de hidrógeno. La adición de estos átomos de hidrógeno al aceite lo hace más sólido, eleva el punto de humo, y hace que el aceite sea más estable.

Los aceites vegetales hidrogenados difieren en dos formas principales de otros aceites que están igualmente saturados. Durante la hidrogenación, es más fácil para el hidrógeno entrar en contacto con los ácidos grasos en el extremo del triglicérido, y menos fácil para que entre en contacto con el ácido graso del centro. Esto hace que la grasa resultante sea más frágil que un aceite tropical; las margarinas de soja son menos "separables". La otra diferencia es que los ácidos grasos trans (a menudo llamados grasas trans) se forman en el reactor de hidrogenación, y pueden ascender hasta un 40 por ciento en peso de un aceite parcialmente hidrogenado. Los aceites hidrogenados, especialmente los aceites parcialmente hidrogenados con sus mayores cantidades de ácidos grasos trans, se piensan cada vez más como insalubres.

En el procesamiento de aceites comestibles, el aceite se calienta bajo vacío a cerca del punto de humeo, y el agua se introduce en la parte inferior del aceite. El agua se convierte inmediatamente al vapor, que burbujea a través del aceite, llevando con él cualquier producto químico que sea soluble en agua. El burbujeo de vapor elimina las impurezas que pueden impartir sabores y olores no deseados al aceite. La desodorización es clave para la fabricación de aceites vegetales. Casi todos los aceites de soja, maíz y canola encontrados en los estantes de los supermercados pasan por una etapa de desodorización que elimina las cantidades trazas de olores y sabores, y aligera el color del aceite.

El aceite vegetal puede provenir de frutos o semillas como:

Según el Departamento de Agricultura de Estados Unidos el consumo mundial en el año 2007/08 de aceites vegetales fue:

La mayor parte de los aceites vegetales se usan para alimentar el ganado. El aceite vegetal más usado para consumo humano es el de girasol.

El aceite de palma, que es sólido a temperatura ambiente, se usa especialmente para jabones y cosméticos.

La mayor parte del aceite de colza producido en Europa se usa para producción de biodiésel, aunque puede ser producido con otros como el de girasol o el de marihuana. Aunque también se ha extendido el uso de estos aceites vegetales como combustibles para los motores diésel.

El aceite vegetal también se puede utilizar como combustible en vehículos híbridos o adaptados.



</doc>
<doc id="8251" url="https://es.wikipedia.org/wiki?curid=8251" title="Guerra de Vietnam">
Guerra de Vietnam

La guerra de Vietnam ("Vietnam War" en inglés, "Chiến tranh Việt Nam" en vietnamita), llamada también Segunda Guerra de Indochina, y conocida en Vietnam como Guerra de Resistencia contra América ("Resistance War Against America" en inglés, "Kháng chiến chống Mỹ" en vietnamita) fue un conflicto bélico librado entre 1955 y 1975 para impedir la reunificación de Vietnam bajo un gobierno comunista. En esta guerra participó la República de Vietnam (Vietnam del Sur, capitalista) con el apoyo de los Estados Unidos y otras naciones aliadas de los Estados Unidos contra la guerrilla local del Frente Nacional de Liberación de Vietnam (Viet Cong) y el Ejército de la República Democrática de Vietnam (Vietnam del Norte), respaldados por China y la Unión Soviética, todos ellos comunistas. Se calcula que murieron en total entre 3,8 y 5,7 millones de personas. Estados Unidos contabilizó 58 159 bajas y más de 1700 desaparecidos, constituyendo la contienda más larga de dicho país hasta la Guerra de Afganistán. Fue una de las guerras más importantes del periodo llamado Guerra Fría.

El conflicto surgió a partir de la Primera Guerra de Indochina (1946-1954), en la que las tropas coloniales francesas combatieron contra el Viet Minh liderado por los comunistas en la Indochina francesa. La mayor parte de la financiación del esfuerzo de guerra francés fue proporcionado por los Estados Unidos. Después de que los franceses abandonaran Indochina tras ser derrotados en 1954, en la Conferencia de Ginebra se decidió el abandono de la colonia asiática, la separación de Vietnam en dos estados soberanos (Vietnam del Norte y Vietnam del Sur) y la celebración de un referéndum un año después donde los vietnamitas decidirían su reunificación o su separación definitiva. Pero los dirigentes del Sur optaron por dar un golpe de estado y no celebrar este referéndum para evitar que ganara la reunificación. Por este motivo Vietnam del Norte comenzó las infiltraciones de soldados en apoyo del Vietcong para anexionarse a Vietnam del Sur. Entonces Estados Unidos, en virtud de la Doctrina Truman y la Teoría del dominó (contener la expansión del comunismo), envió recursos y a partir de 1964 tropas a Vietnam del Sur para evitar la conquista por el norte comunista, dando lugar a este conflicto.

Estados Unidos asumió el apoyo financiero y militar para el estado de Vietnam del Sur. El Việt Cộng, también conocido como "Front national de libération du Sud-Viêt Nam" o NLF (Frente de Liberación Nacional), un frente común de Vietnam del Sur bajo la dirección de Vietnam del Norte, inició una guerra de guerrillas en el sur. Vietnam del Norte también había entrado en Laos a mediados de la década de 1950 en apoyo de los insurgentes, estableciendo el camino de Hồ Chí Minh para abastecer y reforzar el Việt Cộng y aumentó su intervención en 1960. La participación de los Estados Unidos aumentó bajo el presidente John F. Kennedy a través del programa MAAG de poco menos de mil soldados en 1959 a 16,000 en 1963. Para 1963, los norvietnamitas habían enviado a 40,000 soldados a luchar en Vietnam del Sur. Vietnam del Norte fue fuertemente respaldado por la República Popular de China, que además de suministrar armas como lo hizo la URSS, también envió a cientos de miles de militares del EPL a Vietnam del Norte para desempeñar funciones de apoyo.

Con la entrada masiva de los Estados Unidos se recuperó parte de lo perdido. Pero, tras los sucesos de 1968 (Ofensiva del Tet), empezó la retirada progresiva de las tropas estadounidenses y la firma de los Acuerdos de paz de París en 1973, tras los cuales Vietnam del Sur luchó solo contra el Ejército de la República Democrática de Vietnam hasta su propia derrota final y la consiguiente reunificación del país el 2 de julio de 1976 como la República Socialista de Vietnam. Las tres primeras etapas se distinguieron por transcurrir sin la formación de las tradicionales líneas de frente, donde proliferaron acciones terroristas y la guerra de guerrillas, frente a las misiones de «búsqueda y destrucción», el uso de bombardeos masivos y el empleo extensivo de armas químicas por parte de los Estados Unidos. La última fase fue una guerra convencional. Pero el fin de la contienda solo resultó una pausa en los enfrentamientos de Indochina. Después se producirían las invasiones de Camboya y Laos por Vietnam y de ésta por China. Por el contrario, Estados Unidos vivió un repliegue de la política exterior. 

La cobertura realizada por los medios de comunicación fue permanente, estando considerado como el primer conflicto televisado de la historia. Esto permitió la denuncia de las frecuentes violaciones y abusos contra los derechos humanos cometidos por los dos bandos. Sin embargo, se discute si dicha cobertura constituyó la causa principal de la creciente oposición por parte de la opinión pública occidental hacia la intervención estadounidense.

Esta oposición y el hecho de ser la única derrota militar de los Estados Unidos en el siglo XX, creó un sentimiento de mala conciencia en el pueblo estadounidense ante una guerra considerada injusta, el llamado síndrome de Vietnam. El síndrome dio paso a un movimiento pacifista y se prolongó hasta los años 1980, durante el mandato de Ronald Reagan, hasta la Guerra del Golfo de 1992. La guerra de Vietnam se convirtió en un icono, perdurado en la actualidad, de los grupos sociales y partidos de izquierda en gran parte del mundo.

La historia de Vietnam comenzó en el siglo XII, con un grupo de pueblos desplazados desde el sur de China por la invasión mongol y que colonizaron la cuenca baja del río Rojo. En el acuerdo de paz firmado por el rey vietnamita Trần Nhân Tông en 1257, Vietnam accedió a pagar tributos a China para evitar más enfrentamientos. En esos 700 años de historia como pueblo, Vietnam fue alternando su posición de reino invadido por otros pueblos, sobre todo mongoles y chinos, a la de invasor de sus vecinos; pues siempre mostró interés en anexionar Laos y Camboya, cuando no toda la península de Indochina. Este período de independencia y expansión del reino concluyó a finales del cuando el país fue invadido por los europeos, sobre todo Francia y España.

Durante la Segunda Guerra Mundial, el imperio nipón también invadió buena parte de Asia, incluida Indochina. Mantuvo, eso sí, a los administradores franceses en sus puestos para no alterar el orden en la colonia. La contienda pareció enseñar dos lecciones. Por una parte, que los europeos en general y los franceses en particular distaban mucho de ser invencibles. Por otra que tratar de apaciguar al agresor solo le hace más agresivo, como se vio tras la Conferencia de Múnich. La primera lección contribuyó a espolear los levantamientos en Asia y África contra Francia, los Países Bajos, Portugal o el Reino Unido. La segunda, a una visión del comunismo como nuevo poder agresor, algo apoyado en teorías como la defendida tanto por Estados Unidos como por la URSS que postulaban una inevitable implantación del comunismo en todo el mundo, por la fuerza según los primeros y por las ventajas de su sistema según los segundos.

Para responder a los distintos movimientos independentistas, los gobiernos europeos enviaron a lo mejor de sus ejércitos contra los movimientos de liberación en Indochina, Indonesia, Guinea o la India. Sin embargo, el cambio en la percepción de los occidentales y el agotamiento provocado por la Segunda Guerra Mundial hacía muy difícil volver a la situación anterior. Como contestación a esta real o supuesta expansión del comunismo, en la década de 1940, Harry S. Truman ayudó a la monarquía griega a ganar su guerra civil contra las milicias del Partido Comunista de Grecia (KKE). También parecía obtener éxitos en Malasia, Indonesia o Filipinas, naciones con posibilidad de cambiar de aliados. No tuvo suerte con la China de Mao, que sí adoptó el régimen comunista. La guerra de Corea, ante la invasión de Corea del Norte, pareció dar un respiro, pero historiadores como John lo consideran una derrota en la práctica. Sí lo fueron para la Casa Blanca el paso de Vietnam del Norte, Birmania y Cuba a la esfera comunista, sin mencionar todas las naciones europeas bajo la ocupación soviética. Estados Unidos temía quedar rodeado por una constelación comunista de la que Vietnam del Sur sería una pieza más y el desencadenante de una sucesión de pérdidas en toda la península con la consiguiente pérdida de prestigio internacional (la así llamada «teoría del dominó»). En opinión de los distintos gobiernos estadounidenses, si la URSS no veía una oposición decidida, podrían repetirse las consecuencias de Múnich y revivir las acciones expansionistas del régimen nacional-comunista. En la década de 1950, Dwight D. Eisenhower profundizó en la doctrina de Truman con apoyo económico a militares golpistas de dictaduras como Filipinas, Singapur o Corea; además apostó por la Carrera espacial contra la Unión Soviética para conseguir que Indonesia y otros países de la región no cambiaran de bando.

Según John , los distintos gobernantes de la URSS también se veían amenazados por los occidentales. Estadounidenses y europeos habían apoyado al Ejército Blanco en su guerra civil. Habían confiado en Alemania y los invadió. Con el final de la Segunda Guerra Mundial veían cómo su territorio era rodeado por bases estadounidenses con armas nucleares en Alemania occidental, Japón, Turquía. Pero además, los posibles gobiernos que podían simpatizar con la URSS eran hostigados por occidentales, caso de Nasser en Egipto, o depuestos directamente como Lumumba en el Congo.

Francia deseó restablecer su mandato colonial en Indochina tras la rendición de Japón, pero Hồ Chí Minh había declarado la independencia de la República Democrática de Vietnam el 2 de septiembre de 1945. Según los franceses lo recibieron en París como guerrillero y no como jefe de estado. Un frente de nacionalistas y comunistas llamado "Viet-Nam Doc Lap Dong Minh Hoi" o Liga por la Independencia de Vietnam, Viet Minh en su contracción vietnamita, aceptó al principio el retorno de los franceses para evitar la amenaza de China; pero pronto la tensión con las fuerzas coloniales se hizo insoportable. Dicha liga estaba dirigida en lo político por Hồ Chí Minh, partidario de aguardar, y en lo militar por Vo Nguyen Giap, finalmente deseoso de comenzar los ataques. En 1946 se produjeron los primeros tiroteos en lo que se conoce a veces por la Guerra de Indochina, pese a no existir consenso entre los autores.

Francia contaba con el apoyo de buena parte de la colonia, especialmente los vietnamitas monárquicos. Sin embargo, los distintos gobiernos de París no deseaban enviar reclutas ni gastar muchos recursos en el conflicto, por lo que acudieron a Estados Unidos en busca de fondos y armas. Harry S. Truman, en 1950, comenzó contribuyendo con el 15 % de los gastos militares aproximadamente. Cuatro años después, Dwight D. Eisenhower ya soportaba más del 80 % del esfuerzo bélico para levantar, por ejemplo, una base fortificada en Dien Bien Phu, donde un tercio del material llevado allí formaba parte de la ayuda estadounidense. Dicha base perseguía cortar la conexión entre el Viet Minh y la guerrilla que operaba en Laos, además de pretender librar una batalla convencional donde las fuerzas de Võ Nguyên Giáp se presumían inferiores. Giáp estaba siguiendo el espíritu contenido en la frase:
Sin embargo, en Dien Bien Phu Giap «recogió el guante», emprendió una batalla convencional hasta convertirla en una de las mayores derrotas de Francia. En aquel valle, el Ejército Colonial francés perdió lo mejor de su fuerza de combate, poniendo al gobierno de París en desventaja para terminar la conferencia de Ginebra de 1954. Eisenhower no proporcionó las decenas de aviones necesarios que solicitaron los franceses, pero sí ofreció a los franceses dos armas nucleares, éstos las rechazaron por no considerarlas útiles.

Tras la derrota y los acuerdos firmados en la ciudad suiza, la Indochina francesa se dividía en las naciones independientes de Camboya y Laos, más Vietnam separado a su vez por el paralelo 17, el norte sería una zona para la reagrupación del Viet Minh y el sur para el ejército colonial francés, a la vez de concentración de la población simpatizante de cada bando enfrentado. Las dos divisiones pasaron a llamarse República Democrática de Vietnam, más conocida por Vietnam del Norte, y el Imperio de Annan, bajo el mando del emperador Bao Dai; pero se incluyó una cláusula por la cual se celebraría un referéndum en 1958 para decidir si los dos Vietnam seguirían separados o se reunificaban.

El 30 de abril de 1955 el general Ngo Dinh Diem dio un golpe de Estado con el apoyo de la CIA, declaró la República de Vietnam e impuso una dictadura basada en tres personas: él mismo, su hermano Ngo Dinh Nhu y la mujer de su hermano. También canceló las elecciones de 1956 ante su previsible derrota frente a Lao Dong. Para el referéndum para la reunificación tampoco se celebró al alegar el presidente Diem que los ciudadanos del Norte no eran libres para expresarse. Pero, según , la verdadera razón radicaba en las muchas posibilidades de que ganase el «Sí» en el sur, algo no deseado por los dirigentes de Saigón ni por la Administración Eisenhower.

La escasa identidad de Vietnam del Sur como país y la enorme corrupción existente en el gobierno provocaron que la dictadura de Ngo Dinh Diem se hiciese impopular. Además los gobernantes de Saigón, que solían ser católicos en un país mayoritariamente budista, no dudaban en reprimir a los seguidores de Buda. Años después, las protestas contra dicha represión dieron la vuelta al mundo cuando un monje budista se inmoló con combustible en plena calle, el ritual bonzo. Ante esta situación ocurrieron dos acciones paralelas y complementarias:

Los casos de Indonesia, Filipinas, Corea del Sur, Taiwán y especialmente Singapur estaban siendo exitosos, tanto política como económicamentes. Sus regímenes políticos permanecían estables y sus productos interiores brutos crecían, por lo que repetir la misma estrategia en Indochina se consideraba viable. Para ello, Eisenhower apoyó al régimen de Diem y al de sus sucesores con 1200 millones de dólares en cinco años y el envío de 700 asesores militares. El presidente Kennedy profundizó esa misma política.

Acciones armadas venían produciéndose desde el 1 de noviembre de 1955; pero fue en 1959 cuando comenzó la verdadera lucha. En ese momento antiguos guerrilleros del Viet Minh, monjes budistas, campesinos y varios grupos más empezaron a integrar el que después se llamaría Frente de Liberación Nacional. Las acciones armadas fueron la respuesta violenta a las políticas gubernamentales contra la población civil y los sucesivos incumplimientos de sus compromisos. Sus objetivos eran derrocar a Ngo Dinh Diem y reunificar el país. Este deseo de unidad nacional expresado en la frase «lucharemos durante mil años» fue algo que los estadounidenses no llegaron a entender y a la larga constituyó una causa más de su derrota.

La táctica del FNLV consistía en la guerra de guerrillas, que tantos éxitos les trajo en el conflicto anterior contra el régimen colonial francés. Así en julio de 1959 el comandante Dale Buis y el sargento Chester Ovnard fueron los primeros estadounidenses muertos en Vietnam durante los ataques a la base de Bien Hoa, pero en 1959 el FNL principalmente asesinaba a líderes locales leales al gobierno de Saigón. Sería en la siguiente década cuando comenzaron a emplear las pocas armas de que disponían, teniendo como núcleo a unos 10 000 veteranos de la lucha del Viet Minh contra los franceses ayudados por los comunistas del Norte. Por su parte, Vietnam del Norte necesitó varios años para organizar la estructura estatal y tomar las riendas de todo el país, por lo que hasta 1959 no pudo contar con dos comandos para el envío de suministros al Sur, principalmente por mar aunque también mandaron algunos suministros a través de la que se llamaría Ruta Ho Chi Minh, en honor del primer presidente del Vietnam moderno. Esta vía, finalmente clave para la victoria, distaba mucho de ser una carretera, o incluso un camino, sino miles de caminos, túneles y variantes, a través de Laos y Camboya.
La insurgencia se vio favorecida por el propio ERVN, ejército de Vietnam del Sur. Este resultaba muy ineficaz luchando en su propio país. Su armamento resultaba poco adecuado, contaba con escasos pilotos de helicópteros nativos; pero quizá su peor defecto era la gran corrupción e ineptitud de sus oficiales, la mayoría puestos por compromisos políticos entre familias de las élites católicas de Saigón. Consecuentemente los soldados del Sur no confiaban en sus mandos, se arriesgaban lo imprescindible, incluso viendo luchar a sus compañeros a escasas decenas de metros, y no recibían una mínima preparación militar, hasta el punto de hacer guardia con una radio a todo volumen. Pero el poco espíritu de lucha no faltaba solo en el Ejército, los dirigentes en Saigón irritaban a los estadounidenses queriendo negociar con el FNLV en lugar de combatirlo. En aquel momento la única experiencia exitosa para invertir ese tipo de situaciones la desarrollaron los británicos durante la llamada Emergencia Malaya con el nombre de Campaña Corazones y Mentes. Como en Malasia, los dirigentes vietnamitas y sus asesores estadounidenses trataron de crear «nuevas aldeas» con el doble propósito de controlar a la población y separarla de los guerrilleros a los que informaban y alimentaban. Se llamó "Strategic Hamlet Program" o Programa de Aldeas Estratégicas y comenzó en enero de 1962. Llegó a crear 7200 nuevos núcleos urbanos con unos 8 732 000 habitantes y resultó un completo fracaso porque los soldados del Sur no estaban entrenados para ganarse la confianza de los aldeanos; además, siendo tantas aldeas no se podían defender y solo lograron enemistarse con la población al trasladarla por la fuerza.

Si existía descontrol en las zonas rurales, no era menor en el gobierno. En 1963 el presidente Diem fue asesinado en un nuevo golpe militar patrocinado por la administración estadounidense de John Fitzgerald Kennedy, a quien no le convenía apoyar a un general católico dentro de un país con mayoría budista. Nguyen Van Thieu sustituyó a Diem, siendo uno más de los diez gobiernos que llegó a tener el país en un solo año, aumenta el periodo a 18 meses.

A pesar de las ventajas reunidas por los insurgentes y de la incompetencia de sus enemigos, sus principales victorias y la dominación masiva de territorio se dieron a partir del verano de 1964, cuando llegaron los hombres del Norte, como se les ha llamado algunas veces a los soldados del Ejército de Vietnam del Norte o EVN. Para Vietnam del Norte la cancelación del referéndum de reunificación no se vio como un escollo insalvable. Tanto el presidente Hồ Chí Minh y el ministro de Defensa Vo Nguyen Giap en particular como el politburó en general consideraban que la independencia de Francia constituía un paso dentro de una estrategia más larga que podía incluir hasta la posterior dominación de toda Indochina, viejo sueño vietnamita desde la Edad Media. Según esta estrategia, la reunificación por votación o por fuerza sería inevitable. Al no ser convocado el referéndum, quedaba la vía militar. Pero esta tampoco sería sencilla entre otros motivos porque, pese a lo impopular del régimen de Saigón, no todos los vietnamitas del Sur veían con buenos ojos a los comunistas. Aproximadamente un millón de personas habían emigrado al Sur huyendo de Hanoi al producirse la división del país, frente apenas cien mil que se desplazaron hacia el norte. Tampoco el EVN confiaba mucho en sus aliados del FNL y estos no terminaban de vencer sus reticencias a obedecer las órdenes dadas desde Hanoi. Por estas razones el régimen del Sur no se desmoronó inmediatamente; pero fue cediendo territorio poco a poco.

El Frente de Liberación Nacional de Vietnam o FNLV es más conocido por la contracción "Vietcong", del vietnamita "Vietnam Congsan", la cual se traduciría como "Vietnam Rojo" según . Pero existen discrepancias sobre los grupos a los que se refería una y otra palabra, pues no eran dos nombres para la misma organización sino el FNL el brazo político del Vietcong. En cualquier caso los vietcong no eran comunistas en su mayoría y su líder, Nguyen Huo Tho, tampoco lo era; pero no hay consenso sobre su autonomía respecto a Hanoi: autores como indican que la independencia del FLN de Hanói era solamente nominal. por contra afirman que su dependencia del Norte fue siempre considerable, como también el acatamiento de las órdenes dadas desde allí.

Como tal «frente» estaba integrado por una variedad de voluntarios, como monjes budistas, miembros de minorías y uno de cada quince antiguos combatientes del Viet Minh. En total debían ser poco más de 3000 guerrilleros en 1960, aunque le resultaba fácil conseguir voluntarios para terminar con un gobierno incompetente, represivo y corrupto. El FNL poseía fiereza, determinación y gran capacidad de sacrificio, algo que sorprendió a muchos soldados del Sur y después los estadounidenses, a menudo salidos de un reemplazo forzoso. Un miembro del FNLV escribió:

Al principio estaban mal armados. El FLNV obtenía la mayor parte de su material del ejército del Sur y utilizaban técnicas ancestrales para fabricar trampas, como las estacas punji cubiertas de excrementos para acelerar la gangrena. De las granadas, obuses y bombas sin explotar podía obtener unas 800 toneladas mensuales de explosivos para trampas. A esto se sumaban las pocas ayudas que conseguían en los países vecinos y las, en principio, escasas aportaciones del Norte. Ante dichas carencias, las armas constituían una prioridad, las demás necesidades ocupaban un segundo plano, por lo que sufría escasez de medicamentos, víveres e incluso agua. Afortunadamente para ellos, cerca de Saigón y otros lugares, contaban con una infraestructura de túneles subterráneos excavados durante la invasión japonesa y ampliados progresivamente durante la guerra contra Francia. En ellos podían descansar, preparar las incursiones y a veces recibir atención médica. Casi lo contrario al bando enemigo, donde la superioridad logística no acarreaba más que envidia y odio, y con ello ganas de golpear con más fuerza. Un exguerrillero recordaba:
Además, su adaptación al terreno les permitía vivir escondidos o trabajando durante el día, para realizar por la noche todo tipo de ataques y sabotajes, empleando el terreno, la vegetación y armas ligeras. De esta forma la noche realmente les pertenecía, porque durante esas horas, eran ellos quienes dominaban la jungla y a los aldeanos. No perder el apoyo de la población local resultaba de gran utilidad al FNLV al tener acceso a comida e información, imprescindible para el éxito de sus ataques. Para mantener este apoyo el Frente realizaba campañas de adoctrinamiento y también de terror contra la población civil que consideraba colaboracionistas con Saigón, en ocasiones realizando empalamientos para intimidar a los aldeanos. Algunas fuentes cifran en 30 000 el número de civiles asesinados.

Pese a sus logros iniciales, en el verano de 1964 Hanoi consideró que dicha fuerza no podría ganar la Contienda sola, por lo que comenzaron los envíos de unidades enteras del EVN, mejor equipadas, entrenadas y armadas, además de mandadas por oficiales expertos. Gracias a ello en parte, los guerrilleros sorprendieron a los estadounidenses organizando ataques a nivel de división, es decir, una unidad atacaba a otra inferior en número y cuando se solicitaban refuerzos para repeler la agresión, los refuerzos eran atacados por un contingente aún mayor. Así se conseguía aumentar la impaciencia y la desmoralización entre auxiliados y auxiliadores. Si los refuerzos eran demasiado grandes el FNLV y el EVN siempre podían desaparecer en la selva.

Las Fuerzas Armadas de la República Democrática de Vietnam también llamado Ejército Popular de Vietnam, EPV o EVN, por "Ejército de Vietnam del Norte", contaban en 1960 con unos 200 000 hombres entre las tres ramas. Al Sur bajaron en 1961 15 000, de los cuales la inmensa mayoría eran fuerzas terrestres con experiencia en la guerra de guerrillas, herederos del Viet Minh.

La guerra de Vietnam se ha comparado con cualquier otra confrontación donde los Estados Unidos, u otra potencia, no gana con la claridad que se espera de su armamento. Sin embargo, la de Vietnam cuenta con dos diferencias que no se han repetido desde entonces:

Con todo, el EVN demostró ser una maquinaria bélica muy eficaz, con soldados motivados y mandos preocupados por su tropa, casi lo contrario de los sudvietnamitas y estadounidenses. Tanto es así que contaba con una de las unidades más famosas, los «Voluntarios de la muerte vietnamitas» dispuestos a morir encadenados a los árboles para cubrir a sus compañeros. Asimismo, no se constató ningún caso de agresiones a un mando del EVN, cuando en el otro bando no bajaban de decenas al año.

En cuanto al material existe la idea de una desproporción enorme de un bando respecto al otro. Estados Unidos utilizó en Vietnam los más sofisticados productos electrónicos de que disponía, como detectores de movimiento, bombas "lazy dogs" cargadas con miles de cuchillas, helicópteros artillados… pero también adolecía de armamento mal diseñado frente a un enemigo equipado con algunas de las mejores armas del mundo. En varias publicaciones se han destacado proezas aéreas como las realizada por el teniente Randall Cunnigham a los mandos de su Phantom; pero lo cierto es que los pilotos vietnamitas derribaron multitud de cazas y bombarderos pilotando MiG-17 y MiG-21, pese a contar con menor mantenimiento y sobre todo menor entrenamiento que sus enemigos. Un vietnamita que no deseaba ser identificado lo describía de la siguiente manera:

El Ejército de la República de Vietnam o ERVN contaba en 1960 con unos 150 000 hombres, reclutados para un servicio de tres años de los que servían entre 60 y 90 días seguidos en campaña. Por lo tanto y en teoría, suponían una fuerza como mínimo de doble tamaño a la suma del FNLV y el contingente inicial del EVN. Sin embargo su número es engañoso por las decenas de miles de deserciones anuales, 132 000 solo en 1966. Pero, aun suponiendo que su cuantía teórica fuese real, no constituían un rival para el EVN, ni siquiera para el FNLV. Por ejemplo, enero de 1963 una división del ERVN, unos 10 000 hombres, no fue capaz de derrotar en Ap Bac a tres compañías del FNLV, unos 340 guerrilleros. Las razones eran variadas, como contar con poco poder aéreo, pocas piezas de artillería, utilizar fusiles estadounidenses no adaptados al clima y a la constitución de los vietnamitas, no contar con personal de mantenimiento suficiente…; pero especialmente por su baja moral. Los soldados del sur estaban mal pagados, sus familias debían seguirlos y vivir en chabolas cerca de las bases y la corrupción reinaba entre los mandos, además de la incompetencia. Los oficiales eran nombrados por afinidad política en lugar de por méritos y no solían arriesgarse a ir junto a sus hombres al combate. Un motivo más de desmoralización eran el odio que solía tenerles el pueblo por las torturas, robos y otros delitos que cometían.

Sin embargo, no todas las unidades del ERVN tenían bajo desempeño. Los Ranger o la 1.ª División de Infantería estaban mejor pagados y contaban con mandos más competentes, además dichas unidades las integraban en ocasiones exconvictos alistados para huir de la cárcel, por lo que contaban con alguna motivación para luchar. Estas unidades realizaron actos de valor reconocidos por los estadounidenses, como su participación en el levantamiento del Sitio de Khe Sanh, pero solo constituían el 5 % del ejército.

A lo largo de la década de 1960, los militares estadounidenses se habían visto envueltos en refriegas con algún muerto, en dos ocasiones. Fue la llamada «etapa de los asesores». Al principio los asesores militares estadounidenses estaban en el sureste asiático para formar una fuerza de irregulares en las Tierras Altas Centrales e instruir al Ejército de Vietnam del Sur en tácticas, mantenimiento de aeronaves y otras funciones auxiliares. No tenían permiso para intervenir en los combates, mucho menos para preparar acciones contra los guerrilleros; pero más de una vez se saltaron esta prohibición en la que sería, quizá, la primera de una larga lista de violaciones jurídicas e ilegalidades que harían famosa esta guerra.

Los informes enviados a Washington por los asesores y otros expertos concluían que la situación era muy mala y el Sur seguía con gobiernos precarios perdiendo claramente la guerra civil, así a finales de 1964 aproximadamente el 60 % del país estaba en poder del FLN y no había expectativas de un cambio en la tendencia. Las infiltraciones comunistas se habían triplicado, llegando a unos 34000 efectivos.

Para , lo que finalmente desencadenó la intervención total estadounidense fue una reunión mantenida por Johnson con sus asesores el 21 de julio de 1964. En ella trató de idear una manera para forzar al gobierno de Saigón a luchar en lugar de negociar, pero la opinión imperante fue que la retirada de los asesores residentes en el país no conseguiría eso, sino la rápida conquista por parte de Hanói. La intervención directa, opinaban los asesores presidenciales, resultaría muy larga, costosa y sangrienta por ser el Sur un país poco interesado en su supervivencia. Sin embargo, de no hacerlo y en opinión de los mismos asesores, Estados Unidos parecería un tigre de papel y podría degenerar en la Tercera Guerra Mundial, al no ver los soviéticos obstáculos insalvables para su expansión.

Doce días después de la reunión se produjo el «Incidente del Golfo de Tonkin», con un primer ataque al destructor estadounidense USS "Maddox" el 2 agosto de 1964. Al día siguiente se unió al USS "Maddox" el USS "Turner Joy" y la noche del 4 de agosto supuestamente se produjo un nuevo ataque, pese a no existir pruebas de dicho acto. El presidente Lyndon B. Johnson ordenó el 5 de agosto a los navíos USS "Ticonderoga" y USS "Constellation" acciones de represalia contra la flota norvietnamita. Siendo ciertos o no alguno de los ataques, el incidente legitimó a Johnson para solicitar y conseguir del Congreso el 6 de agosto la llamada Resolución del Golfo de Tonkín. Esta resolución conferiría plenos poderes para que los asesores militares presentes en Vietnam realizaran operaciones fuera del recinto de sus bases, además de incrementar la cantidad de tropas en ese país, al estar en campaña electoral Johnson necesitaba mostrar una imagen de fuerza frente al comunismo.

El 2 de marzo de 1965 se autorizó la Operación Rolling Thunder, planificada desde hacía un año, con 100 cazabombarderos y 200 toneladas de bombas cada uno con el objetivo de atacar instalaciones nortvietnamitas y doblegar su voluntad «destruyendo acero y hormigón», en palabras del presidente Johnson. En ese mismo mes desembarcaron en la base aérea de Da Nang 3500 marines para protegerla y unirse a 60 000 soldados ya destinados en Vietnam como asesores. Todo ello se realizó sin consultar a la opinión pública estadounidense, pero sí con su apoyo mayoritario; aunque ya en ese momento se organizaron protestas en contra y denuncias ante el descaradamente clasista sistema de reclutamiento. Desde el punto de vista del Derecho Internacional Estados Unidos no estaba en guerra contra ninguna nación, para ser así debería haber existido una declaración previa, tampoco fue una invasión de Vietnam del Sur, solo la llegada de más asesores.

 indica que la Casa Blanca marcó una meta propagandística y otra militar desde un principio.

El objetivo político pretendía dar a conocer las acciones del Norte y del FNLV tanto a los miembros del Congreso como a la opinión pública estadounidense y mundial, con el fin último de aislar internacionalmente a Vietnam del Norte y marginar al FNLV como interlocutor, en caso de llegar a una negociación. Para ello se utilizarían los medios de comunicación y las acciones diplomáticas.

El presidente Johnson trató de atraer a tantos países como pudo con la Campaña Más banderas, para dar una idea de que el «Mundo Libre» estaba luchando contra el comunismo, pese a que el adjetivo «Libre» es más un eufemismo que una realidad debido a la presencia de países como Corea del Sur o Filipinas. Muchas naciones enviaron ayuda, principalmente en forma de suministros médicos, algo bien visto por la población del país emisor y receptor; pero solo 7 destinaron soldados a la Península: la dictadura sudcoreana envió en 1965 200 hombres y fue aumentando el contingente hasta 47 829 soldados en 1967; Tailandia contribuyó con un total de 11 568 soldados, además permitió a Estados Unidos emplear su territorio para operar bombarderos B-52, cazas, aviones de reconocimiento y el Centro de Vigilancia de la Infiltración; Australia terminó destinando una división, primero con asesores en 1962, después con 1400 soldados, algunos veteranos de la lucha en las junglas malayas, y finalmente con un número máximo de 7672 soldados y oficiales en 1967, por lo que se convirtieron en un importante aliado estadounidense y experimentado en un territorio muy hostil como es la selva, se retirarían en diciembre de 1972. Participaciones más pequeñas fueron la de Filipinas (2000 soldados), Taiwán (31 hombres) y España con varios grupos de 13 médicos militares.

En el campo militar el objetivo marcado era demostrar al FNLV y a Hanoi que no podrían ganar la guerra debido a las numerosas bajas y derrotas que les infligirían. Por tanto, como indicó el propio presidente de los Estados Unidos, sería una guerra diferente, donde no existiría una capital que tomar o unas líneas de frente que romper. Para infligir esas derrotas y esas pérdidas, el presidente Johnson deseaba utilizar más los bombardeos que las acciones de infantería, pero para esto sería necesario levantar una serie de bases navales y aéreas que cubrieran todo el país. A su vez, dichas bases necesitaban tener garantizada su seguridad, por lo que se consideró necesario:
La primera misión podía seguir en manos de la flota de «aguas azules» destinada en el sureste asiático que habían realizado un buen trabajo hasta entonces. Para la segunda y la tercera se necesitan envíos masivos de hombres y material. Así, a finales de 1965, ya eran más de 100 000 los efectivos desplazados a Vietnam y se habían destinado 1000 millones de dólares para el envío de casi diez millones de toneladas mensuales en suministros y equipo. Toda esta ingente cantidad de materiales requería una enorme cadena logística que lastró mucho al Ejército. Por ejemplo, solo uno de cada siete soldados estadounidenses se vio realmente envuelto en combate, los demás pertenecían a cuerpos logísticos, administrativos, médicos, mecánicos, etc.

Para cumplir la tercera meta militar, el despliegue de potencia de fuego con la que cubrir a la infantería en misiones de búsqueda y destrucción, Estados Unidos haría uso de todo su poder aéreo. Por ejemplo, si las piezas artilleras aerotransportadas no podían descargarse por lo espeso de la selva, aviones de distintos tipos lanzarían bombas de cientos de kilos que abrían un cráter y permitir el aterrizaje.

En batallas más o menos convencionales, los guerrilleros vietnamitas aún tenían cartas que jugar frente a los soldados del Sur y lo demostraron en el mes de junio. El 51.º Batallón del ERVN cayó en un ataque sorpresa cerca del golfo de Tonkín y fue desintegrado por completo. Pero el resultado fue diferente cuando los estadounidenses entraron en acción:
Los éxitos en la Operación Starlight y en Ia Drang llevaron al jefe de las fuerzas estadounidenses en Vietnam, general William Westmoreland, a solicitar y conseguir los medios para realizar las acciones que pensaba que le llevarían a la victoria, entre las que destacaron:
Asimismo en diciembre de 1965, la Fuerza Aérea puso en marcha el Programa Big Belly, para permitir que los B-52 transportaran casi 10 000 kg de bombas y en abril del año siguiente fueron desplazados a la isla de Guam para poder alcanzar Vietnam del Sur. Desde allí se realizaron una media de 300 salidas al mes.

El primer año de la guerra, Estados Unidos venció en la práctica en la totalidad de las batallas, gracias a su potencia de fuego y a poder abastecer a sus hombre por aire sin sufrir los numerosas ataques que tantas pérdidas les costaron a los franceses . Esto les hizo pensar en una victoria rápida; pero de la que podían obtener experiencia en combate para sus oficiales, por lo que decidieron enviar allí a todos los posibles, rotando cada seis meses en lugar de cada doce. Esto causó un primer problema. Las estadísticas informaban de que un militar comenzaba a desenvolverse bien a los noventa días de servir en Vietnam y alcanzaba su óptimo operativo a los diez meses. La continua rotación fue imprimiendo un sentimiento en las unidades de ser mandadas por novatos ineptos, lo que les hacía candidatos a las temidas emboscadas, por tanto, no dudaban en eliminar a sus jefes y a cualquier recluta no demasiado hábil.

Westmoreland y sus aliados lanzaron una misión tras otra de las que se puede destacar la Operación Market Time, para cortar los suministros llegados por mar, y Operación Prairia, con el fin de detener los combates en el llamado "Cerro de los murmullos" en la zona desmilitarizada. También se autorizó el empleo del Agente Naranja para eliminar la cubierta vegetal que protegía las guaridas y las posiciones desde las que los guerrilleros atacaban a las tropas regulares. Todo ello daba una visión optimista a las opiniones públicas estadounidenses y de los distintos países que los apoyaban; pero la imagen que se tenía al llegar a cualquier parte de Vietnam del Sur era de inseguridad. Así lo comprobaron los soldados españoles cuando aterrizaron en Saigón en abril de 1966. Los edificios oficiales se veían protegidos por sacos terreros, el autobús que los transportaba llevaba las ventanillas cubiertas por rejas para impedir la entrada de granadas. Incluso en el propio hotel Península, donde se alojaron, tuvieron que interrumpir la emisión de una película por explosiones cercanas y el posterior contraataque helitransportado. Eso dentro de la propia capital del país.

Sin embargo, 1966 no resultó tan exitoso a los estadounidenses como 1965. El Mando de la Asesoría Militar y el propio Westmoreland reconocieron que el número de bajas estadounidenses resultaron desproporcionadamente altas y el número de victorias se había reducido, los vietnamitas estaban empezando a llevar la iniciativa. Westmoreland solicitó y obtuvo más soldados. Por lo demás, el método era seguir empleando la artillería, la aviación y el alto explosivo. De esta forma las operaciones siguieron sucediéndose una tras otra:

Gracias a toda esta ayuda y esfuerzo, el gobierno de Saigón fue recuperando buena parte del territorio perdido los años anteriores y en 1967 en Estados Unidos se creía que la victoria estaría de su lado en no mucho tiempo. Pero la desmesurada potencia de fuego utilizada estaba resultando contraproducente en muchas ocasiones. Un aldeano comentaba:

Del mismo modo, el empleo de un arma tan devastadora como los bombarderos estratégicos B-52 causó rechazo en buena parte del mundo, incluido en el propio Estados Unidos.

Los vietnamitas aprendieron mucho más de su oponente de aquellos reveses y decidieron seguir las siguientes pautas:
Así, la guerra de Vietnam se convirtió en una serie de larguísimos momentos de inactividad o de marcha, interrumpidos por algunos instantes de lucha sangrienta. Estas acciones tuvieron éxito dañando la moral estadounidense. Tanto el Mando de la Asesoría Militar en Vietnam, como los oficiales y soldados se sentía desmotivado por estas tácticas. Un miembro de las Fuerzas Especiales afirmó años después:

Si dura resultaba la campaña para los soldados, no lo era mucho menos para el Mando de la Asesoría Militar en Vietnam. El deseo de conseguir una batalla campal llegó a ser la particular obsesión del Pentágono, que organizaba operaciones con el fin de localizar el Cuartel General del FNLV o CGVC, ejército de Vietnam del Norte enviado al Sur. En su mente seguía fija la idea de que los guerrilleros defenderían aquella valiosa posesión con ahínco y, por tanto, tendrían una oportunidad para destruirlos. Pero por más operaciones que llevaron a cabo, el CGVC nunca apareció, suponiendo que el CGVC no fuera en realidad una oficina en Hanói. Nuevamente se hacía cierta la metáfora de Ho Chi Minh de la lucha entre el tigre y el elefante. Esa metáfora encierra la esencia cruel y a veces atroz de aquella guerra, como suelen ser todas las guerras de guerrillas.Un miembro del FNLV lo explicó así:

Asimismo la cita contiene otra de las bazas que supo jugar el pueblo vietnamita: la utilización del terreno en su propio beneficio. En la jungla podían ocultarse sin ser vistos, ni siquiera con visores Starlight o de infrarrojos. Sabían utilizar las ventajas que ofrecía la hostil selva, algo que los estadounidenses no llegaron a comprender del todo, como demuestra el deseo de terminar con la vegetación con defoliantes o convertir el terreno en un cenagal baldío a base de bombas.

Puesto que la flota de Estados Unidos hacía imposible el abastecimiento por mar, desde 1966 Vietnam del Norte decidió reforzar, ampliar y utilizar profusamente la ruta que abrió en 1959. Pese a que se ha sobrevalorado su importancia, esta ruta constituyó una pieza clave en la victoria del Norte gracias a los suministros transportados por ella, como también por el acceso que proporcionó al EVN al interior de Vietnam del Sur. Nunca pudo ser cortada ni detenida totalmente, pese a utilizarse todo tipos de técnicas, desde los bombardeos masivos hasta el sembrado de sensores inteligentes que detectaban las vibraciones producidas al caminar por personas o su sudor; pero por los animales, la vegetación, el clima, los innumerables caminos y la perseverancia de los vietnamitas, todos los esfuerzos resultaron inútiles. Con el tiempo, la Ruta fue sembrándose de zonas donde descansar y reponerse, además de cultivar alimentos para aliviar la presión sobre las mercancías transportadas. Estos centros fueron objetivos de bombardeos, de ataques por parte de mercenarios contratados por la CIA e incluso de incursiones en Camboya y Laos (ver más adelante). Pero nuevamente volvieron a resultar inútiles. Con el avance de la Guerra, la Ho Chi Minh fue una de las piezas claves para poder lanzar la ofensiva del Tet, después la ofensiva de Pascua y por último la ofensiva de primavera, que terminó con Vietnam del Sur. Incluso sería la vía de infiltración para ocupar Laos años más tarde y convertirlo en un protectorado vietnamita de facto.

Pese a las bajas y las manifestaciones en contra, la sensación mayoritaria entre los estadounidenses era de ir por el buen camino. Existían informes de inteligencia anunciando una gran ofensiva comunista, pero dichos informes no eran lo suficientemente claros o fiables, ya el año anterior se había lanzado una gran operación, la Cedar Falls, a raíz de otra también gran operación de inteligencia, la Rendezvous, y no se consiguieron más contactos con el FNL de los habituales. Con estos antecedentes las acciones de 1968 fueron una sorpresa para prácticamente todos los militares, políticos y analistas estadounidenses y dieron al traste con todas las expectativas estadounidenses de ir ganando.

El 21 de enero de 1968 dos divisiones del EVN y efectivos del FNLV comenzaron un fuerte bombardeo sobre la base de Khe Sanh, que permaneció sitiada durante 77 días. Pronto la prensa y el propio presidente Johnson realizaron paralelismos con Dien Bien Phu, la gran derrota francesa en Indochina. Los distintos informes negaban que las dos situaciones se pareciesen, pero el presidente se mostró muy preocupado ante la posibilidad de perder la Base y verse frente a una derrota de gran repercusión mediática.

El Mando de la Asesoría Militar en Vietnam realizó un esfuerzo considerable por mantener la posesión en su poder. No dejó de mandar suministros, cuando los aterrizajes fueron imposibles, desarrollaron la salida de la carga con paracaídas, llevaron a cabo bombardeos masivos, socorrieron a los sitiados movilizando unos 30 000 efectivos por medio de operación como la Pegasus, Los marines tomaron las colinas que rodeaban las instalaciones para no repetir la experiencia francesa... Parecía que aquella lucha sería una de las pocas de gran envergadura que las mermadas fuerzas guerrilleras podían realizar tras casi tres años de contienda.

El esfuerzo en mantener Keh Sanh fue tan grande que Westmoreland y su estado mayor decidieron abandonar la posición de Lang Vei a su suerte, aun cuando el EVN decidió utilizar por primera vez vehículos blindados. Los nueve Boinas Verdes y los cientos de montañeses no pudieron aguantar el ataque ante la inexistencia de refuerzos provenientes de Keh Sanh, siendo esta quizá la única batalla perdida por Estados Unidos en toda la contienda.

El sitio no terminó finalmente con una carga o un combate cuerpo a cuerpo; sino, según el oficial de marines Willian N. Dabney, con los atacantes calcinados por las bombas de napalm. Aun con todo el esfuerzo realizado, el 5 de julio la base se abandonó. La razón esgrimida fue: ya no resultaba necesaria tras haber retenido allí considerables tropas del EVN y el FNLV que, de otro modo, podría haber causado mucho daño. Por supuesto el abandono del lugar levantó críticas sobre la utilidad de desplazar tantos hombres y material para defender una posición innecesaria, esfuerzos que hubiesen sido útiles para disminuir los efectos de la ofensiva del Tet. Fuera como fuese, el sitio supuso una inversión del apoyo popular a la política de Johnson en el Sureste asiático.

A finales de enero de ese año, cuando se celebraba el año nuevo vietnamita —la festividad del Tet—, 38 de las 52 capitales provinciales de Vietnam del Sur fueron atacadas, y muchas prácticamente tomadas. Saigón estuvo en estado de sitio, la propia embajada de Estados Unidos fue asaltada por un comando suicida que casi llegó al interior del edificio y Hué, la antigua capital del Imperio vietnamita, cayó en poder de los rebeldes, tardando varios días en ser recuperada, tras lo cual se descubrió la llamada masacre de Hué con miles civiles asesinados sistemáticamente por los norvietnamitas.

La sorpresa fue total para los estadounidenses y para el ERVN, pese a los informes advirtiendo de la movilización. La inteligencia militar no pudo obtener información clara y concisa de lo que estaba pasando ni de lo que se avecinaba. Sin embargo la ofensiva también guardaba una pequeña sorpresa para el mando norvietnamita: los soldados del Sur resistieron el ataque con pocas deserciones y ganaron varias luchas encarnizadas.

Pronto la situación se invirtió. El poder aéreo barrió casi por completo a los guerrilleros del FNLV, unos 40 000 muertos según los estadounidenses, y pocos días después todo el territorio ganado por los guerrilleros era recuperado, habiendo perdido el EVN buena parte de los efectivos que tan penosamente consiguió llevar al Sur. La ofensiva del Tet volvía a ser un fracaso como también lo había sido 16 años antes. Pero peor resultó la situación para el FLN. Pese a que lanzaron posteriormente la llamada «ofensiva del mini Tet», la mayoría de sus efectivos habían caído muertos o prisioneros debido a las órdenes emitidas por Giap de resistir en sus posiciones. Las guerrillas desaparecieron del campo en los años posteriores y poco después el Programa Phoenix se ocuparía de descabezar el mermado FLN, en ocasiones literalmente.

Mucho se ha discutido sobre si la orden de resistir dada por Giap al FNLV ante un enemigo mucho más poderoso fue un error o un plan preconcebido. Giap había perdido antes muchas batallas contra los franceses y tuvo problemas para conservar su puesto en el Viet Minh tras algunas derrotas, como la del Vinh Yen. Sin embargo, el politburó de Hanói siempre consideró al FNLV como un aliado poco cómodo, aunque solo fuese por el hecho de ser una fuerza potencialmente independiente con base en el Sur. Autores como han indicado la posibilidad de que fuesen órdenes dadas con el fin ulterior de inmolar dicha fuerza. Sin embargo, el hecho de perder a miles o decenas de miles de personas era una práctica ya practicada por Giap y Ho Chi Minh desde la independencia, como asesinar a miles de «terratenientes» o provocar la huida de un millón de católicos. Así lo reconoció Giap con la frase autocrítica: «recurrimos al terror que se extendió en demasía», por lo que implícitamente se admitía un nivel de terror aceptable. El recurso de los asesinatos masivos se constató nuevamente tras recuperar Hué, donde descubrieron fosas con 3000 asesinatos perpetrados por el EVN con el fin de terminar con cualquier organización que no fuese la impuesta por ellos. Por lo tanto, no existe consenso sobre si la orden se debió a un plan para terminar con el FNLV o fue fruto del desinterés por la vida de sus hombres y la propia ineptitud de Giap en asuntos tácticos.

Paradójicamente, una victoria militar como la del Tet hizo ver a los estadounidenses que sus enemigos no solo podían dar un buen susto a sus soldados; sino que conservaban la capacidad de atacar cualquier lugar de Vietnam del Sur, incluso su embajada. ¿Habían resultado inútiles tantos bombardeos, tres años de lucha con abundantes muertos, la riada de millones enviados, y la multitud de manifestaciones y contramanifestaciones?

De poco sirvieron los comunicados sobre el gran número de bajas infligidas al FNL y al EVN, la resistencia que demostró el ERVN o los hallazgos de las Matanzas de Hué. Las manifestaciones de protesta se multiplicaron. Mucho más cuando en 1969 se hicieron públicos los sucesos acaecidos un año antes en el pueblo de My Lai, donde el ejército estadounidense pareció seguir el comportamiento de los nazis en Oradour-sur-Glane. Un acicate que dejaba a pocos indiferentes, especialmente al constatar que el sistema para medir el cumplimiento de los objetivos podía haber convertido al acto de My Lai en la punta del iceberg.

Pero si en Estados Unidos la población estaba dividida, en el sureste asiático la moral era muy baja, hasta el punto de que autores como lo han denominado «El colapso de la moral» por motivos como:
Los militares estadounidenses hicieron esfuerzos por atender y cuidar a sus hombres donde quiera que sirviesen. Estados Unidos siempre se ha enorgullecido de abastecer bien a sus soldados llevándoles cervezas frías, regalos de casa e incluso periódicos. Dicho esfuerzo se incrementó dándoles semanas libres en el destino de Asia que prefiriesen, contratando a investigadores para que analizaran los problemas raciales, llevándoles a estrellas de la música y el humor, etc., pero la moral seguía muy baja, por lo que Lyndon Johnson relevó a Westmoreland y ordenó los primeros planes de retirada.

Pese a que este término y esta idea ya la había planteado el presidente John Fitzgerald Kennedy a principios de los años 1960, no fue hasta la victoria de Nixon cuando comenzó a llevarse a la práctica por el analista Henry Kissinger. La vietnamización perseguía fortalecer y preparar al ERVN, para luego traspasarle la responsabilidad de defender el territorio del Sur. Al mismo tiempo debía crear un contexto para desahogar al régimen del presidente Thieu del acoso constante al que le sometían el FNLV y el EVN. Esto se realizó dando instrucción a los vietnamitas y «entregar ingentes cantidades de armas al ejército de Nguyen van Thieu». Además, la vietnamización supondría para Washington y Saigón una posición más fuerte de cara a unas futuras negociaciones con los comunistas, ya iniciadas en secreto por Kissinger en París durante febrero de 1969.

Se discute si, tras la Ofensiva del Têt en 1968, el presidente Johnson decidió el progresivo abandono del conflicto o si esta decisión se tomó unos meses después, tras la batalla de la Colina de la Hamburguesa. Lo que indudablemente sí sucedió fue la percepción de no contar ya con la opinión pública. Pese a todo, los envíos de tropas continuaron y en 1969 se aumentó el número de estadounidenses a más de 500 000; pero para entonces el Presidente ya sabía que aquella guerra le había costado la reelección y anunció su abandono de la política.

En enero de 1969 y Richard Nixon fue elegido nuevo presidente. Los ejes sobre los que basaría su política vietnamita serían:

El segundo punto del proyecto lo fue cumpliendo progresivamente. No se puede decir lo mismo de los demás. Este hecho, el prometer una cosa dentro de un tema de capital importancia, hacer exactamente lo contrario y volver a ganar las elecciones, ha quedado como ejemplo en muchos estadounidenses de cómo un gran «vendedor de autos» puede arrastrar a todo un pueblo. También prueba la determinación de Nixon para no ser el primer presidente de Estados Unidos en perder una guerra, incluso con bombardeos superiores a los de la Segunda Guerra Mundial.

Otro problema pendiente fue la recopilación de abundante información sobre la organización y disposición de las fuerzas enemigas. Hacia 1969 la CIA, cuyos agentes llevaban mucho tiempo insistiendo en que aquella guerra no podía ganarse por medios convencionales, ya tenía depurado su Programa Phoenix que había comenzado en 1967. Dicho programa pretendía terminar con sus enemigos de una manera más selectiva que con bombardeos y explosivo de alto poder. Pero, pese a los esfuerzos de varios mandos y oficiales, el Programa Phoenix terminaría siendo más terrorismo de estado que una fuente de información fidedigna, pues los agentes de la CIA tuvieron poco menos que carta blanca para matar a cualquier persona considerada miembro del FLN.

Mientras, el Ejército de los Estados Unidos llevó a cientos de oficiales del ERNV a cursos de instrucción para mandos, pilotos y personal de mantenimiento del costoso material que les regalaría. Con todo su fuerza aérea se colocó en la cuarta más grande de Asia. Pero los progresos resultaron muy lentos y se veían entorpecidos por la corrupción crónica o la selección de mandos según los "compromisos" de los dirigentes políticos, no por sus cualidades militares. En esta misma línea los oficiales estadounidenses comenzaron a ver que regalarles helicópteros y sustituirlos cuando fueran derribados no conducía a nada si los pilotos continuaban teniendo una capacitación mediocre a lo sumo. Pese a todos estos fallos, la retirada de tropas comenzó en 1970, empezando por el personal de infantería para terminar con los pilotos de los que siempre estaba necesitado el ERVN.

Al mismo tiempo un problema más estaba creciendo. Los dos neutrales vecinos de Vietnam del Sur, Laos y Camboya, se mostraban incapaces de contener la agresión de sus guerrilleros comunistas ni cortar la Ruta Ho Chi Minh. Si Estados Unidos pretendía que su aliado pudiera sobrevivir a una guerra con el Norte debía cortar esa vía de infiltración y, de paso, terminar con el Cuartel del ejército norvietnamita o del FLN ubicado en Camboya, según suposiciones de la inteligencia estadounidense.

En marzo de 1969 Richard Nixon, recién elegido, inició una campaña de bombardeos secretos sobre Laos y Camboya. Con el nombre en código de Operación Menú, la Fuerza Aérea atacó los dos países con el máximo secreto. Los pilotos debían despegar, ir a una posición determinada y esperar órdenes. Una vez allí los controladores les daban las coordenadas que debían atacar. A la vuelta, los mismos controladores deberían destruir todo documento sobre estas incursiones en territorio neutral. Pese a todas las precauciones, en menos de un mes "The New York Times" ya publicaba noticias sobre estos ataques, filtradas por miembros de la Fuerza Aérea disconformes con estas operaciones. Laos fue la nación más bombardeada de la Tierra, con más de 2 500 000 bombas de todos los tamaños. Estos bombardeos perseguían cortar la Ruta Ho Chi Minh, pero también demostrar a Vietnam del Norte que la nueva presidencia estaba dispuesta a todo con tal de terminar con aquella guerra, incluso la opción nuclear. Pero los vietnamitas del Norte no se amedrentaron.

El 14 de abril de 1970, el ERVN realizó una primera incursión en territorio camboyano y el 29 de abril el teniente general Do Cao Tri lanzó a sus 12 000 hombres sobre el "Pico de Loro" (véase mapa). Pero sería el 1 de mayo cuando el general Robert Shoemaker ordenó a los oficiales destacados en la frontera con Camboya avanzar sobre el "Pico de Loro" y "el Anzuelo". Algunos soldados aceptaron con resignación participar en estas incursiones, pero la mayoría vio con regocijo el poder golpear el santuario del FNLV y vengar todos los muertos que habían llegado flotando por el río Mekong.

La incursión estuvo precedida de grandes bombardeos que causaron muchos muertos entre los campesinos, lo que a la larga fue terrible para el gobierno pro-occidental de Camboya. Nixon era consciente de las repercusiones que traerían aquellas acciones; pero, como él mismo había declarado, prefería perder la reelección a ser el primer presidente en perder una guerra.

Las acciones en El Anzuelo encontraron alguna resistencia del EVN que, como era la costumbre, desaparecía en la selva tras un breve tiroteo. Ni siquiera en el pueblo de Snuol hubo amago de oponerse a la potencia de fuego desplegada por los M60 Patton. Pronto los emplazamientos de artillería del EVN fueron capturados y se enviaron cien M551 Sheridan que sí encontraron resistencia; pero la vencieron pronto. Saquearon el poblado, interrogaron a los campesinos y finalmente lo arrasaron. En esos interrogatorios los camboyanos informaron que había toda una ciudad guerrillera en la jungla. Poco después un helicóptero Loach avistó una casa bien camuflada y comenzó el bombardeo de artillería y aviación. Cuando los infantes pudieron entrar en lo que ellos mismos llamaron «La Ciudad», encontraron 400 cabañas de paja y 180 escondites con suministros médicos, alimentos y ropa, además de 480 fusiles y 120 000 cartuchos.

El 30 de junio, todos los soldados volvieron a sus bases, dejando graves pérdidas a la 9ª División vietnamita, encargada de la defensa en retaguardia. Por el otro bando se contabilizaron 354 estadounidenses muertos y 1689 heridos. El ERNV dijo haber perdido 866 hombres y tener heridos a otros 3274. El Presidente anunció la muerte de 11 349 enemigos y la captura de suficientes suministros y armas para cubrir las necesidades de todo un año, aunque la propia CIA calificó ese recuento de "altamente sospechoso". Para Nixon era como un regalo de Navidad y ordenó el envío de 31 000 soldados más a Camboya para destruir todo lo que no se pudiera transportar. Sin embargo, el famoso cuartel del EVN para Vietnam del Sur (el COSVN) no apareció y sí fuertes manifestaciones en Estados Unidos, siendo la de Kent State la más dura de todas..

Por contra, los bombardeos de la Operación Menú y los de la posterior "Freedom Deal" colocaron a la población camboyana en contra de su gobierno, aliado de los Estados Unidos. La USAF prosiguió los bombardeos en torno a la capital camboyana para impedir a los Jemeres Rojos tomarla, pero la caída de Nom Pen era cuestión de tiempo en cuanto las bombas cesaron. Además, el apoyo del depuesto príncipe Norodum Sihanouk a los Jemeres Rojos legitimó a estos en su insurgencia, que databa de 1969 en el norte del país..

En 1971 Laos era considerado el país más atravesado por la Ruta Ho Chi Minh, por lo que debía ser golpeada con gran contundencia. El 18 de enero de 1971, aniversario de una famosa victoria vietnamita sobre los chinos en 1427, el alto mando militar sudvietnamita ordenó al general de división Le Truong Tan comenzar la Operación Lam Son 719. Tenía como objetivos desbaratar cualquier posible ofensiva comunista sobre Vietnam del Sur durante todo un año y lograr con ello:

La Operación perseguía abrir un corredor de 25 km de ancho por 35 de largo entre la frontera de Vietnam del Sur y la ciudad laosiana de Tchepone. Eso cortaría la Ruta Ho Chi Minh, permitiría capturar abundante material y detendría las operaciones de los guerrilleros en el Sur. Se destinaron las mejores divisiones sudvietnamitas, como los marines, paracaidistas y Rangers, además de vehículos blindados. Todo bajo el mando del general Xuan Lam. Por desgracia para los sudvietnamitas, el EVN esperaba un ataque así y concentró cerca de la frontera un cuerpo de ejército formado por las divisiones 304, 308 y 320, más un regimiento acorazado y otro de artillería.

Cuando la ofensiva no había pasado la mitad del camino planificado, las numerosas bajas sufridas en la Carretera 9 y en las colinas al norte de Laos obligaron a detener el avance survietnamita. Poco después comenzó una evacuación con helicópteros de apoyo que terminó en una retirada. En este punto hay discrepancias ente los autores. Para ninguno de los dos bandos logró una victoria decisiva, pues los survietnamitas llegaron a Tchepone, pero la encontraron casi desierta. Para «no hay ninguna duda de que el intento de atacar el saliente laosiano fue un desastre», porque, si la incursión fue mal, la retirada resultó pésima, terminando en una desbandada y en una carnicería.

Las imágenes de decenas de helicópteros regresando a Vietnam del Sur atestados de soldados gravemente heridos, algunos atemorizados, echó por tierra las esperanzas de poder contar con el ERVN para defender Vietnam del Sur. Lan Som 719 había costado casi 9000 hombres. Sin embargo, dos años después, los sudvietnamitas demostraron que aún les quedaban cartas por jugar frente al mismo enemigo que tan duramente los había expulsado de Laos.

A las 02:00 horas del 30 de marzo de 1972, 12 000 proyectiles de la artillería y misiles del EVN atacaron las posiciones del ERVN en la Zona Desmilitarizada, un bombardeo parecido al de Khe Sanh. Seguidamente las divisiones 304 y 308 junto a 300 blindados se lanzaron contra las posiciones survietnamitas con el fin de arrollarlas cercar Quang Tri y volver a ocupar Hué, repitiendo el éxito de 1968. A estas acciones las llamaron la Ofensiva de Pascua.

El cinco de abril, desde Camboya, otro contingente avanzó por la región del "Anzuelo" y el "Pico de Loro", cercando las ciudades de An Loc y Tay Ninh, en el camino a Saigón. Una tercera oleada salió del sur de Camboya para infiltrarse en el delta del Mekong. Con todo, esto solo resultó un señuelo para distraer la atención del ataque principal, lanzado días después en el centro del país, sobre la ciudad de Kontum.

Las imágenes de carreteras inundadas por desplazados, aviones de transporte tratando de levantar sus rampas con hombres colgados de ellas y vehículos atestados de asustados vietnamitas, parecían dar la idea de que aquel régimen terminaría en pocos días. Giap lanzó sobre el Sur la práctica totalidad de su ejército con la intención del aterrorizar a sus enemigos y dar el golpe de gracia al régimen de Saigón. Sin embargo la realidad resultó diferente. Fue necesario una crisis así para que el timorato presidente Thieu relevara del mando al general Giai y al teniente general Ngo Dzu, para colocar al frente de sus hombres a Ngo Quang Truong, calificado por algunos como el mejor oficial de Vietnam del Sur. Este detuvo las retiradas y ordenó que todos los desertores y saqueadores fueran ejecutados. Con el nuevo mando y, quizá, con el miedo a nuevas matanzas sistemáticas, Hué pudo ser salvada, al mismo tiempo que Kontum y An Loc resistieron un ataque tras otro. Todo esto aumentó la confianza de los soldados en su ejército.

Al otro lado del Pacífico, Nixon declaró que lanzaría un ataque como el que jamás habrían visto y lo cumplió. Los 700 aviones desplazados al sureste asiático, los buques fondeados en las aguas de Vietnam del Sur y los B-52 de Tailandia y Guam realizaron un bombardeo que detuvo en unas ocasiones y desintegró en otras a las unidades del Norte. Giap volvió a su táctica de lanzar oleada tras oleada cosechando parecidos resultados a los de Dien Bien Phu.. El poder aéreo estadounidense aniquiló a buena parte de los efectivos norvietnamitas y los tanques T-54 recién traídos de la URSS fueron destrozados por los cazas o por los soldados del ERVN con sus lanzacohetes portátiles M72 LAW.

Finalmente las incursiones del EVN se detuvieron tras haber incrementado el terreno en su poder del 3,7 % al 9,7 %. Las pérdidas para Hanoi rondaron los 190 000 muertos, heridos o prisioneros, por lo que pasó el resto de 1972 tratando de mantener el terreno conquistado. Pese a todo, el 15 % de estas conquistas las perdería en los siguientes años frente al ERVN luchando ya en solitario.

Ciertamente el programa de vietnamización había logrado éxitos como:

Sin embargo el EVN y el FNLV habían logrado por su parte:

Los logros obtenidos por los vietnamitas comunistas coinciden casi totalmente con lo que la Administración Nixon pretendía evitar. Autores como afirman que Saigón podía resistir, pero necesitaba el apoyo aéreo estadounidense. Esta opinión parece confirmarla las órdenes de Nixon para bombardear masivamente a Vietnam del Norte y minar tanto los puertos como los estuarios, acciones casi desesperadas para obligar a Vietnam del Norte a negociar, frente a unas elecciones cada vez más cercanas. Así Nixon podría presentarse como el «pacificador», término que le gustaba.

Pese a lo que los acontecimientos demostraron después, en 1972 y 1973 la derrota del Sur no estaba clara para ninguna de las dos partes. Por un lado combatían ya solos, pero por el otro recuperaban territorio y los Estados Unidos les había entregado 2500 millones de dólares en armas y municiones, suficiente para resistir varios años. Tampoco las circunstancias internacionales se lo ponían fáciles a ninguno de los dos bandos. Pese al generoso arsenal que habían dejado los estadounidenses, su salida de la guerra redujo en dos ocasiones consecutivas las ayudas económicas al régimen de Saigón, primero Nixon las bajó a mil millones de dólares anuales y, tras su dimisión en agosto de 1974, el Congreso las dejó en 700 millones. Este recorte aumentó aún más en 1975, lo que obligó a dejar en tierra 200 aviones, la mitad de la fuerza aérea survietnamita.

La crisis del petróleo incrementó el precio de los alimentos y otros productos de primera necesidad en todo el Sur, obligando a muchos soldados a realizar trabajos extras fuera de las filas o a dejar su puesto para poder ganar el sustento de su familia. En cualquier caso supuso una merma de tiempo para entrenamiento y operaciones.

En el Norte las cosas no marchaban mucho mejor. La política de acercamiento a China emprendida por Estados Unidos, la famosa Diplomacia del Ping Pong de 1971 con la visita del propio Nixon a Pekín al año siguiente, hacía pensar en una disminución de la ayuda militar del gigante asiático a Vietnam del Norte. La URSS también bajó los fondos para el armamento regalado. Debía preocuparse de otros temas como la seguridad de su frontera con China, en la que llegaron a darse enfrentamientos esporádicos.

Nixon se mostró implacable con los bombardeos para obligar a los norvietnamitas a sentarse a la mesa de París cada vez que abandonaban la negociación. Se negociaron todos los detalles para que pareciera una paz honrosa, mientras continuaban los bombardeos y los combates.

El 8 de mayo de 1972 Richard Nixon suspendió las negociaciones de París por los continuos ataques del EVN y ordenó la Operación Linebacker con el fin de minar los puertos, destruir objetivos militares, vías férreas, instalaciones petrolíferas, aeródromos y los muelles de todo Vietnam del Norte. En esta ocasión los Phantom y los B-52 iban equipados con bombas inteligentes guiadas por láser, que tan famosas se harían en conflictos posteriores, y afirmaban atacar únicamente blancos militares o económicos, nunca zonas habitadas por civiles. De esta forma, decían ellos, la ferocidad de las bombas se vería compensada con su precisión. Esta vez se lanzaron 155 548 toneladas de bombas en 41 000 misiones. Las fábricas fueron casi destruidas por completo, lo mismo que las vías férreas, incluido el famoso puente de Thanh Hoa, las ciudades aún intactas de Hanói y Haiphong tampoco se salvaron. Sin embargo lo vietnamitas lo veían de una forma muy diferente, como comentaba un miembro de su comunidad:

El Presidente tenía muy presente que la Operación Rolling Thunder había desgastado mucho a su antecesor y una campaña mucho más dura haría lo mismo con él; pero era un hombre enérgico y no dudó en ordenar la salida de los B-52. Según las fuentes occidentales los bombardeos no perseguían solo llevar a Vietnam del Norte de nuevo a la mesa de negociaciones, sino demostrar a Vietnam del Sur que les seguirían apoyando pese a retirar sus soldados. Desde el punto de vista estadounidense las operaciones Linebacker menguaron la moral vietnamita y el gobierno de Hanói comenzó a pensar en volver a negociar. Ciertamente la situación en la que los aviones estadounidenses colocaron al pueblo vietnamita fue muy dura, un vietnamita relataba:
El 23 de octubre los bombardeos cesaron y se retomaron las negociaciones. Cuando los norvietnamitas volvieron a sentarse, Nixon lo presentó como una victoria; pero lo cierto es que Hanoi no cambió sustancialmente sus exigencias que obligaban al Sur, entre otras cosas, a no poder reconquistar territorio ni les exigía a ellos abandonar lo tomado. Pese a ello, unos meses después, los jerarcas de Vietnam del Norte se retiraron nuevamente. Por aquellas fechas habían recibido de la URSS misiles SAM (acrónimo en inglés de Surface-to-Air Missile, superficie-aire) y confiaban en presentar resistencia a los nuevos ataques. Nixon reanudó los bombardeos, la conocida extraoficialmente como Operación Linebacker II. Entre el 18 y el 29 de diciembre de 1972 cayeron 20 370 toneladas de bombas, matando a 1000 personas, deteniendo las comunicaciones internas, dañando la red eléctrica y terminando con la totalidad de la fuerza aérea norvietnamita. Solo se detuvieron el día de Navidad. Con todo, el precio fue alto para la USAF. Los norvietnamitas derribaron 26 aviones, quince de ellos B-52, y capturaron a varios pilotos, lo que aumentaba algo su margen de negociación en París porque la opinión pública estadounidenses siempre se mostró muy preocupada por el paradero de los pilotos desaparecidos en combate. Para Nixon se habían logrado casi todos los objetivos, para muchos vietnamitas la conclusión fue otra:

El 27 de enero de 1973 la delegación de Vietnam del Sur, la norvietnamita, la estadounidense y la del Gobierno Provisional de la República de Vietnam del Sur (el FNLV o Vietcong) firmaron los Acuerdos de paz de París. El documento se componía de 23 artículos con las misiones de cada bando. Fue arduamente preparado hasta en los más mínimos detalles. Lo firmado suponía:

Estos acuerdos daban a Estados Unidos un respiro. Con el final de su participación en la Guerra ahorraba unos 8100 millones de dólares y una gran tensión interna. Sin embargo para Vietnam, tanto del Norte como del Sur, no era más que una pausa en la lucha. Una survietnamita comentó en Saigón:

Por su parte, el gobierno de Saigón anunció que no permitiría elecciones en su territorio y acogió la noticia con indiferencia, convencido de afrontar un ataque del Norte. Estados Unidos había prometido continuar ayudándolo económicamente, pero dicha promesa quedó en poco menos que papel mojado tras la dimisión de Nixon por el caso Watergate, ya que Gerald Ford deseaba olvidar la guerra cuanto antes, igual que muchos estadounidenses.

Pese a que la victoria no se veía a corto plazo y a que los hombres de Giap no podían emprender una gran ofensiva tras las pérdidas cosechadas en la de Pascua, sí había indicios de que lo tomado en dicha ofensiva, y fijado en los Acuerdos de Paz de París, constituía una base sólida para el ataque final porque:

Lo que trataba de conseguir Vietnam del Norte era una posición más fuerte hasta recuperarse para la campaña final. Sin embargo, el general norvietnamita Tran Van Tra pedía una gran acometida. Él insistía en que se podía conseguir una victoria rápida partiendo de las Tierras Altas Centrales para tomar la ciudad de Pleiku y después cortar su conexión con Ban Me Thuot, algo parecido a lo intentado en 1965. En un principio se aplazó la petición, pero finalmente Hanoi decidió comenzar la ofensiva, y el general Van Tieng Dung fue enviado al Sur para preparar todas las actuaciones. Así, a principio de 1974 son atacadas las zonas de Quang Nam y Quang Ngai, en mayo se registraron intensos combates en Ben Cat y en la primavera de 1974 el EVN había recuperado lo perdido en el delta del Mekong; pero Thuong Duc fue reconquistada por el ERVN.

El 1 de marzo de 1975, el EVN cortó los enclaves terrestres con Ban Me Thuot, la ciudad cayó el 13 de ese mismo mes. El ataque hizo tomar al presidente Thieu dos de tantas decisiones equivocadas, pero que en aquellos momentos resultaron extraordinariamente trágicas:

La retirada se convirtió en una desbandada. La presión del ejército enemigo, el pánico de los civiles que huyeron aterrados y la ineptitud del mando ante quizá la operación más difícil que se le pueden pedir a un oficial, minaron por completo la cohesión y espíritu de lucha de los soldados. Estos huyeron entre la multitud que bajaba despavorida en lugar de defender las ciudades citadas. En un intento de evitar una derrota catastrófica, el presidente del Sur decretó en marzo la movilización general para tratar de contener la ofensiva que muy pocos veían remediable. El esfuerzo resultó inútil, Hué cayó el 25 de marzo y Da Nang el 30, perdiendo dos de las mejores unidades del Sur, la División de Infantería de Marina y la 1ª División. Las Tierras Altas Centrales cayeron en poder del Norte dos días después tras cundir el pánico en ellas.

Como reconoció posteriormente el general Van Tieng Dung, aquel fue un golpe de suerte con el que no contaban. Ante estas noticias, Le Duc Tho y los militares a las órdenes de Giap enviaron sendos cables aprobando la movilización solicitada por Dung. Finalmente se optó por atacar la región de Tay Nguyen al tener el Sur solo dos divisiones diseminadas; pero ni siquiera estas ofrecieron gran resistencia. Todo el país era un caos.

Al gobierno de Saigón solo le quedaba jugar la carta de luchar en las provincias del sur, las más ricas, a la espera del monzón que detuviera o ralentizara todo. De aguantar hasta las lluvias Saigón ganaría tiempo para conseguir apoyo aéreo estadounidense; pero en esta ocasión solo lograron buenas palabras, mientras el FNLV organizaba un Gobierno Revolucionario Provisional.

Aquel desmoronamiento en la parte norte del país y en las Tierras Altas Centrales cambió la percepción de Hanói sobre una victoria para 1976. También lo cambió para Saigón, que trató de entablar negociaciones con los comunistas. Estos exigieron y consiguieron la desaparición de Thieu de la escena política, dejó el poder el 21 de abril siendo sustituido por el general Duong Van Minh. A finales de marzo, el Buró Político se reunió nuevamente y decidió lanzar la Ofensiva de Primavera, llamada por ellos "Campaña Ho Chi Minh". Dung recordó el discurso lanzado tras la reunión:
El 22 de abril, varios aviones A-37 capturados al enemigo volaron hasta Tan Son Nhut. Valiéndose de su apariencia, atacaron la torre de control y destruyeron numerosos cazas. El humo pudo verse desde Saigón, con la consiguiente sensación de pánico. Mientras, unidades enteras se rendían al paso de los comunistas que avanzaban tomando una ciudad tras otra bajo el lema:
A las 00:00 del 29 de abril, la Hora H, Saigón fue atacada por todas las direcciones, excepto desde el mar. Por la zona desmilitarizada penetraron más unidades, lo mismo que desde Laos y desde el centro norte de Camboya.
Por la mañana, artillería norvietnamita bombardeó el puente Newport, la última conexión de Saigón con el mundo exterior. Tras horas de intensa lucha, la ciudad quedó completamente aislada.

En una plantación de caucho próxima a Dau Giay, aguardaba una unidad de ataque en profundidad formada por una brigada de tanques, un regimiento de infantería y otras unidades. Llevaban los vehículos camuflados con ramas, los brazos con cintas rojas para distinguirse y uniformes impecables para tomar la capital, mientras el general Cao Van Vien firmó la orden de resistir con la frase «defender hasta la muerte, hasta el final, la porción de la tierra que nos queda», poco después desertaba de su puesto y huía del país.
A las 15:00 del 29 de abril los transportes, los blindados y tanques de la unidad de ataque en profundidad salieron del bosque y llegaron a la capital aplastando toda resistencia. Al día siguiente penetraron en Saigón mientras la gente trataba de huir por cualquier medio. Tomaron el cuartel general del Estado Mayor, el Palacio de la Independencia, el cuartel general de la Zona Capital Especial, el Directorio General de la Policía y el aeródromo de Tan Son Nhut. La rapidez del avance sorprendió a los periodistas cuando recibieron la noticia de que habían penetrado en el palacio presidencial, por lo cual la tripulación de un tanque norvietnamita tuvo la cortesía de repetir el acto poco después para que lo pudiesen fotografiar. Saigón había caído.

Muchas personas trataron de huir en balsas y botes por mar, creando un problemas para las otras naciones que no estaban muy predispuestas a recibirlos. Algunas se suicidaban para evitar posibles represalias de los norvietnamitas, especialmente las que habían abandonado el Norte por cuestiones políticas o religiosas 25 años antes, mientras otras saqueaban todo lo que podían. Según Jonathan , Vietnam del Sur era un país en descomposición, carente de autoridad o incluso conciencia de país.

Estados Unidos inició la "Operación Frequent Wind" con el fin de sacar a su personal diplomático, sus ciudadanos y colaboradores vietnamitas, como el presidente Ky. Los dos portaaviones situados cerca de Saigón no daban abasto. Algunos helicópteros survietnamitas aterrizaron en dichos barcos sin que nadie los hubiese invitado. Los marinos tuvieron que arrojar al mar varios aparatos para dejar espacio en cubierta. Todo esto fue televisado y fotografiado, «venciendo la prepotencia del ejército más poderoso del mundo».

Los comunistas subieron las escaleras del palacio con sus banderas. Llegaron al despacho del presidente y entraron. Con cierta dignidad Minh dijo:

La respuesta fue:

Al reconstruir el escenario los historiadores se siguen haciendo la pregunta ¿el presidente Johnson arrastró a Vietnam a la guerra o se vio arrastrado por sus consejeros? El primer punto de vista es defendido por , para quien Vietnam del Sur no quería la guerra, sino la reunificación. Fueron los temores estadounidenses quienes le llevaron a continuar un enfrentamiento hasta la derrota total. La otra línea de pensamiento la apoyan autores como el exconsejero presidencial John Kenneth Galbraith, para quien Johnson no deseaba inmiscuirse tanto en Vietnam, pero el peso de sus consejeros para que interviniese fue demasiado grande.

Para entre otros, los Estados Unidos en general, y su Ejército en particular, tuvieron buena parte de culpa. Pese al extraordinario esfuerzo realizado y a la sensación inicial de triunfo, los Estados Unidos no comprendieron del todo el tipo de guerra y el tipo de pueblo contra quien luchaba. Así aquel atacaba donde su enemigo podía encajar mejor los golpes, en las bajas humanas, mientras se desgastaba un poco más cada vez. Una línea de pensamiento similar la defendió Robert McNamara quien, pese a ser uno de los primeros y más fervientes defensores de la intervención, comenzó a tener dudas en 1966 y a plantearse abiertamente la imposibilidad de ganar ya en 1967. Según él, la iniciativa de los combates la llevaban los comunistas; ellos podían elegir cuantas bajas sufrir y cuantas infligir a sus oponentes, de esta forma:

La CIA mantuvo una opinión similar al postular la imposibilidad de ganar el conflicto por medios únicamente militares.

Esta incomprensión se palpa en las continuas estadísticas e informes cuantitativos solicitados y manejados por los mandos, en varios casos exagerando los resultados, pero sin prestar excesiva atención a los discursos de los dirigentes comunistas, ni ganándose la confianza de los aldeanos, quienes podían proporcionarles buena información. Así, los militares estadounidenses se comportaban como en cualquier guerra convencional, donde lo importante son los datos del potencial enemigo, en lugar de una guerra de guerrillas, donde lo vital es separar a los guerrilleros del apoyo popular. Al abandonar este aspecto, algunos problemas no disminuyeron sino lo contrario:
El Ejército de los Estados Unidos defendió su actuación alegando que había luchado bien. Según ellos, fueron otros factores como las restricciones impuestas por los políticos o la creación de una larga cadena logística las que contribuyeron decididamente a la derrota. Por su parte, Harry G. lo culpa de la derrota, no tanto por combatir bien o mal, sino por no haber suministrado al ejecutivo estadounidense información precisa de cómo ganar la guerra, además de no haber plasmado correctamente la situación vivida. En este caso, hacen ver que muchas veces ni el propio Ejército conocía dicha situación. Pese a las toneladas de documentos incautados al enemigo en las distintas operaciones, a la dispersión de miles de sensores por la selva, al empleo de los muy sofisticados, para la época, ordenadores de tercera generación, el uso masivo de fotografía aérea y por satélite; no se llegó a conocer la situación real. Las distintas agencias de inteligencia, hasta quince a veces, no fueron conscientes de los preparativos para la ofensiva del Tet, ni la magnitud de los complejos de túneles que tanto ayudaron a ella, ni la existencia o no de un cuartel general del EVN en territorio survietnamita... Así se llegaba en muchas ocasiones a situaciones donde los agentes marcaban como blancos importantes lugares que no sabían realmente si lo eran o no; pero que en caso de serlo les haría subir puntos. Naturalmente esos lugares debían ser inspeccionados por la infantería, que se jugaba la vida por los agentes, en lugar de trabajar estos para evitar esos riesgos.

También se ha indicado la diferencia económica entre los combatientes. Para los hombres provenientes de regiones templadas, la jungla les puede resultar un lugar hostil, amiga de sus enemigos y enemiga suya, como creían los británicos en Birmania durante la Segunda Guerra Mundial. Los vietnamitas debían alimentarse de serpientes, ratas, lagartos y, cuando había suerte, arroz; por esta razón, podían sobrevivir de la selva cuando los alimentos faltaban sin que se resintiera su moral y cuando aquellos llegaban, se vivían momentos de euforia y satisfacción. Mientras, veían a los estadounidenses disfrutar de todo tipo de manjares, disponer de abundante dinero y recibir incluso cervezas frías en pleno campo, lo cual aumentaba la distancia con sus aliados y el odio de sus enemigos.

Otro factor apuntado en varias ocasiones fue la presencia de la prensa y su influencia negativa en la opinión pública. En 1965 la mayoría de los estadounidenses estaban a favor de la intervención, no fue hasta 1968 cuando los porcentajes comenzaron a invertirse. La publicación de las matanzas como la de My Lay, la presencia casi constante de la guerra en los informativos nocturnos, la revelación de los bombardeos secretos, las acciones del movimiento pacifista hablando con conocimiento de causa por tener a veteranos en sus filas o las declaraciones de algunos políticos cambiando de actitud, caso del propio McNamara, fueron presentando a la Guerra como algo injusto; siendo la subsiguiente falta de apoyo popular decisiva para la derrota. Por su parte, indica que no se puede perder lo que nunca se tuvo. Según él, las operaciones en Vietnam comenzaron sin consultar al pueblo estadounidense, pues la autorización del Congreso era para intervenir en los «alrededores de las bases», y cuando se solicitó el apoyo de la opinión pública esta se negó a concederlo, sorprendiendo a unos dirigentes convencidos de pisar la antesala de la Tercera Guerra Mundial.

Otro caso de laboriosidad sin desesperanza lo dieron los habitantes de Vietnam del Norte tras los bombardeos. Un miembro de la comunidad lo relataba de la siguiente manera:

En el aspecto político, el Norte fue más o menos estable, mientras en Saigón se sucedían los golpes militares y los deseos de terminar la guerra cuanto antes, mientras los atentados y ataques se repetían sin que los sudvietnamitas pareciesen querer arriesgarse. Por supuesto había excepciones entre los soldados y los oficiales, pero constituían una minoría. Para testigos y escritores como Jonathan en este conflicto no es que la voluntad general fuese un factor de gran importancia, es que resultó el factor decisivo. Por tanto, quebrarla debió haber sido el objetivo perseguido por los Estados Unidos y no lo consiguieron.

Probablemente pocas guerras hayan tenido tantas repercusiones en la Historia contemporánea como la de Vietnam y también pocos han atraído más la atención de novelistas y cineastas.

Los bombardeos masivos y la crueldad de la guerra retransmitida por vez primera con una libertad pocas veces repetida, comenzaron a cambiar la imagen que tenían los estadounidenses de sí mismos. La idea de un país enorme machacando a otro pequeño y la de sus soldados cometiendo matanzas fuera y dentro resultaron demoledoras, dejando aplastado el espíritu del Destino Manifiesto. En las elecciones de 1968 un presidente dedicado a las reformas sociales como Lyndon Johnson se enfrentó a fuertes desafíos por parte de dos demócratas opuestos a la guerra: los senadores Eugene McCarthy y Robert Kennedy, hermano del asesinado presidente Kennedy y asesinado también al final de la campaña. El 31 de marzo, en vista de una humillante derrota manifestada por las encuestas y de la incesante prolongación del conflicto en Vietnam, Johnson se retiró de la contienda presidencial y ofreció negociar el fin de la guerra. Más tarde, la reelección de Nixon en 1972 provocó un éxodo masivo de ciudadanos descontentos a países como Canadá.
La oposición a la guerra se extendió dentro y fuera de Estados Unidos entre la juventud, avivando el movimiento hippie que había comenzado antes. Las universidades estadounidenses fueron escenario de manifestaciones contra la implicación de Estados Unidos en esa guerra no declarada e injustificada en opinión de muchos. Hubo encuentros violentos entre los estudiantes y la policía con disparos y muertos. En octubre de 1967, 200 000 manifestantes marcharon frente al Pentágono, exigiendo la paz, siendo uno de los puntos más álgidos del movimiento pacifista. También es cierto que dicha situación coincidió con uno de los momentos de máxima prosperidad económica, lo que confería mucha seguridad a la juventud y posibilidades de cambiar de costumbres. Pero el factor principal de protesta resultó el servicio militar, obligatorio para todos los varones estadounidenses y con él la posibilidad de ser enviado a Vietnam.

El trauma de Vietnam les duró mucho más a los militares que a la sociedad en general. Las referencias a esta contienda en cualquier guion de cine que requiera ayuda del Pentágono son discutidas hasta la saciedad, incluso con amenaza de romper la colaboración si no se atiende a sus demandas. También lo fueron para los miembros de la administración Nixon que buscaron enemigos comunistas por el mundo para luchar contra ellos tras la derrota en Vietnam, apoyando militar y económicamente a dirigentes poco cualificados, como Holden Roberto, y cuando estos fallaron recurrieron a los mercenarios, alegando razones estratégicas inexistentes, para no reconocer la inquina que tenían por la derrota. Dicha derrota fue la principal causa esgrimida por políticos como Charlie Wilson para financiar a los muyahidines afganos en su guerra contra los soviéticos, aunque dicho apoyo se volvería contra ellos cuando uno de sus «protegidos», Osama Bin Laden, organizó los Atentados del 11 S y varios cabecillas más se manifestaron a favor.

Estados Unidos lanzó más de siete millones de toneladas de bombas sobre Indochina durante la guerra, más del triple de las 2,1 millones de toneladas lanzadas por Estados Unidos en Europa y Asia durante toda la Segunda Guerra Mundial y más de diez veces de las lanzadas en la guerra de Corea. 500 000 toneladas se lanzaron sobre Camboya, un millón sobre Vietnam del Norte y cuatro millones sobre Vietnam del Sur. En términos per cápita, las dos millones de toneladas de bombas que Estados Unidos lanzó sobre Laos convirtió al país asiático en la nación más bombardeada de la historia de la humanidad. El "New York Times" señaló que se lanzó casi una tonelada de bombas por cada habitante de Laos. Solo en este país, unos 80 millones de bombas, casi una de cada tres lanzadas, no explotaron y aún permanecen en su mayoría dispersas por todo el país. Esto ha provocado que vastas extensiones agrícolas no puedan cultivarse y más de 20 000 laosianos han muerto o resultado heridos desde el fin del conflicto, una cifra que aumenta en unas 50 personas cada año. Debido a que la Fuerza Aérea debía realizar muchas misiones con el fin de asegurarse financiación adicional durante la elaboración de sus presupuestos anuales, en muchas ocasiones el gran tonelaje de bombas gastado no se correspondían con el daño que provocaban.

La derrota de Saigón y sus Aliados fue proclamada como fiesta nacional vietnamita bajo el nombre "Día de la Paz", pero no trajo la paz al sureste asiático. Pocos años después la nación invadía Camboya y los hombres de las balsas (refugiados) siguieron aumentando sin que ningún país quisiera hacerse cargo de ellos. Aunque la invasión de su vecino trajo la liberación de los camboyanos del régimen quizá más sanguinario del Planeta, no logró la paz. Las luchas contra lo que quedaba de los Jemeres Rojos se prolongaron durante más de una década, con continuos anuncios de retirada que se aplazaban o no se cumplían, hasta que en los años 1990 se celebraron elecciones en aquel país (ver Historia de Camboya).

El antiguo Vietnam del Norte perdió el 70 % de su infraestructura industrial y de transportes, además de 3000 escuelas, 15 centros universitarios y 10 hospitales. Por su parte, el medio ambiente vietnamita quedó profundamente dañado por la utilización del Agente Naranja y otras armas químicas que defoliaron grandes extensiones de selva, con pocas posibilidades de recuperarse por la invasión del bambú y otras plantas. Pero peor aún fueron los efectos para la población en contacto con esas sustancias, aparentemente inocuas para los humanos, con miles de abortos prematuros, nacimientos con malformaciones, y esterilidad, especialmente dolorosa para las mujeres de medios rurales. A esto debe añadirse todos los hijos ilegítimos de rasgos caucásicos y africanos dejados en la pobreza y marginación.

Asimismo, la Guerra causó muchos daños a la agricultura y los campesinos, especialmente a los niños, debido a miles de municiones, explosivos y minas sin estallar ni retirar de bosques y arrozales. Estos efectos provocaron la baja de producción en las explotaciones agrícolas y el aumento de la población urbana que huía del campo, convertido en campo de batalla. Se han contabilizado 10 500 000 refugiados generando unas pérdidas estimadas en 200 000 millones de dólares.

Las enormes infraestructuras de túneles excavados por todo Vietnam ahora forman parte de las atracciones que visitan los turistas. Se pueden ver las entradas camufladas, recorrer sus galerías, sentarse en las salas de reuniones e incluso disparar los AK-47. Este «turismo de guerra» ha contribuido a levantar la economía del país, debilitada tras la caída de la URSS.

Uno de los aspectos más controvertidos del esfuerzo militar de Estados Unidos en el sudeste asiático fue el empleo generalizado de productos químicos defoliantes entre 1961 y 1971. Utilizados para defoliar grandes extensiones de campo y así evitar que el Viet Cong fuera capaz de ocultar sus armas y campamentos bajo el follaje, estos productos químicos acabarían cambiando el paisaje, causando enfermedades, defectos en bebés de padres expuestos y el envenenamiento de la cadena alimentaria.

Ya en los comienzos del esfuerzo bélico estadounidense en Indochina, se decidió su uso, dado que el enemigo ocultaba sus operaciones bajo las selvas de triple cúpula arbórea, y un primer paso útil sería defoliar ciertas áreas de estas características. Esto fue llevado a la práctica en la conocida como operación Ranch Hand. Empresas como Dow Chemical y Monsanto fabricaron los herbicidas necesarios. Funcionarios estadounidenses señalaron que los británicos ya habían utilizado previamente los químicos 2,4,5-T y el ácido 2,4-diclorofenoxiacético de una forma prácticamente idéntica y a gran escala durante la «Emergencia Malaya» de los años 50, con el fin de destruir los bosques y cultivos que los insurgentes comunistas utilizaban para ocultarse y preparar emboscadas contra convoyes. Incluso el secretario de Estado, Dean Rusk, había dicho al presidente John F. Kennedy que «el uso de defoliantes no viola ninguna norma del derecho internacional relativo a la conducción de la guerra química, y es una táctica de guerra aceptada con un precedente establecido por los británicos durante la situación de emergencia en Malasia, donde utilizaron aviones para la destrucción de cultivos por aspersión de químicos».

Los defoliantes, que se distribuían en bidones pintados con bandas codificadas por colores, incluían los llamados «herbicidas arco iris», entre los que se encontraban el Agente Rosa, el Agente Verde, el Agente Púrpura, el Agente Azul, el Agente Blanco y el más famoso y utilizado de ellos, el Agente Naranja, que contenía dioxina como subproducto de su proceso de fabricación. Entre 41,6 y 45,4 millones de litros de Agente Naranja se rociaron sobre Vietnam del Sur entre 1961 y 1971. El principal área de operaciones de Ranch Hand fue el delta del río Mekong, donde las patrulleras de la Armada de los Estados Unidos eran vulnerables a los ataques lanzados desde la maleza a orillas del agua. 
En 1961 y 1962, la administración Kennedy autorizó el uso de productos químicos para la destrucción de los cultivos de arroz. Entre 1961 y 1967, Estados Unidos roció 75,7 millones de litros de herbicidas sobre una superficie de 24 000 kilómetros cuadrados de cultivos y bosques, el 13 % del territorio de Vietnam del Sur. En 1965, el 42 % de los herbicidas se habían pulverizado sobre cultivos para alimentación. Otro objetivo en la utilización de químicos consistía en forzar a la población civil a desplazarse a zonas controladas por los sudvietnamitas.

Según el gobierno de Vietnam, 400 000 personas murieron por el uso de Agente Naranja y 500 000 niños nacieron con enfermedades congénitas; en 2006, también el gobierno vietnamita estimó que unas cuatro millones de víctimas sufrían envenenamiento por dioxina, a lo que el gobierno de Estados Unidos contestó negando que existieran evidencias científicas concluyentes entre el Agente Naranja y las víctimas vietnamitas envenenadas con dioxina. Al respecto, en algunas zonas del sur de Vietnam a principios del siglo XXI, los niveles de dioxina eran 100 veces mayores que el estándar internacional aceptado. El Departamento de Asuntos de los Veteranos de Estados Unidos ha enumerado numerosas enfermedades que sufren los niños de los veteranos que estuvieron expuestos al Agente Naranja, como cáncer de próstata y respiratorios, mieloma múltiple, diabetes mellitus tipo 2, linfoma o neuropatía periférica, entre otras.

Pese a ser uno de los conflictos más documentados por no aplicarse la censura militar, en países como España se produjo un "olvido interesado" durante los años 1980, siendo muy escasa la producción histórica. En ocasiones se tiene la sensación, comenta , de encontrarse ante un conflicto muy difícil de analizar, por la enorme cantidad de material existente (traducciones, reportajes, crónicas...). Frente a este fenómeno aparece en contraposición lo poco tratado que ha sido el punto de vista vietnamita para Occidente, creándose así, según Tad Szulc, una visión mitificada ante la falta de análisis de mayor profundidad.

Otra nefasta consecuencia fue la falta de atención prestada por Occidente al genocidio camboyano por ser un pueblo subdesarrollado que había logrado derrotar también a un aliado de Estados Unidos; por lo tanto, en la mentalidad izquierdista/revolucionaria, no podía ser malo y las informaciones aportadas por organizaciones como Amnistía Internacional se calificaban de falsas o manipuladas por los servicios de inteligencia estadounidenses.

La impresión de que un pueblo pobre, pero muy motivado podía derrotar a la mayor potencia mundial empleando la guerra de guerrillas caló muy hondo en la mayoría de los países. Hasta el punto de considerarse el medio definitivo de lucha de los militarmente débiles contra los militarmente fuertes, debieron llegar movimientos como el de los Sin Tierra latinoamericanos para desvincularse de dicha lucha. Esta supuesta invencibilidad de las guerrillas ha quedado también como un mito, pero la Historia posterior ha desmentido este supuesto:


Por su parte, Estados Unidos también aprendió muchísimo de lo vivido en Vietnam. Los políticos de aquel país tuvieron cuidado después en no hacer combatir a sus asesores al lado de las fuerzas locales en ninguna parte del mundo y, cuando estas acciones se llevaron a cabo, los distintos gobiernos reaccionaron con cierta rapidez.

La utilización masiva del helicóptero en una guerra asimétrica se demostró correcta, pese a la derrota final. Tanto es así que se han empleado masivamente durante las Invasiones de Irak y Afganistán se han demostrado como el mejor método para combatir a un enemigo disperso y extremadamente móvil. Así, la mayoría de los ejércitos de principios del siglo XXI tendieron a reforzar y diversificar sus flotas de helicópteros frente a los llamativos, pero menos eficaces, cazas y bombarderos.

Aunque inicialmente la guerra de Vietnam no llamó excesivamente la atención de la industria cinematográfica, desde finales de la década de 1970 y principios de la década de 1980, la producción de películas sobre el tema floreció con títulos muy destacados como "Apocalypse Now" o "Platoon". Esta atención de Hollywood contrasta con el escaso interés mostrado por el cine francés hacia su derrota.

Al contrario que los historiadores e incluso la propia sociedad estadounidense, el cine sí supo asimilar el fracaso, en opinión de Marc Leppson. De esta manera se pasó de la patriótica y poco creíble "Los boinas verdes", donde se muestra a unos entregados miembros de las Fuerzas Especiales de Estados Unidos en su lucha contra los malvados comunistas, soslayando las terribles torturas que el libro original relataba, a una más crítica "Apocalypse Now", que prefirió alcanzar presupuestos astronómicos antes que rendirse a la censura del Pentágono a cambio de sus helicópteros. Otro éxito fue la cinta "El Cazador" o "The Deer Hunter" de Michael Cimino con Robert De Niro, Christopher Walken, John Savage y Meryl Streep, de 1978, reflejó la influencia de la guerra en unos trabajadores del metal en Pittsburgh. Obtuvo cinco premios Oscar.

Por su parte, Oliver Stone, quien participó como soldado en Vietnam, realizó entre 1986 y 1993 tres obras sobre estos sucesos: "Platoon", "Nacido el 4 de julio" y "Cielo y Tierra". "Platoon" recibió cuatro premios Óscar y dejó algo descolocados a los veteranos estadounidenses porque no los retrataba como héroes, pues aparece el fragging, las violaciones a niñas, los asesinatos, el incendio de aldeas; pero también los muestra en situaciones muy duras, aceptadas por su condición de pobres, junto a héroes, como el sargento Elias Grodin interpretado por Willem Dafoe. "Nacido el 4 de julio" solo obtuvo dos premios Oscar, uno al mejor director, pero arrasó en los Globo de Oro. En "El cielo y la tierra", basada en los libros de Le Ly Hayslip, intentó acercarse a la visión vietnamita del conflicto. En el llamado a veces «año de las películas de Vietnam» cabe destacar títulos clásicos como "Full Metal Jacket" (conocida en España como "La chaqueta metálica" y en Latinoamérica como "Cara de guerra" o "Nacido para matar") de Stanley Kubrick y "La colina de la Hamburguesa", de John Irvin.

Ciertamente los distintos estudios han creado cintas de todo tipo. De esta forma quizá la visión cinematográfica más ficticia de este conflicto sea la dada por "Rambo"; un héroe que, en palabras de Marc Leppson, se parece tanto a un veterano de Vietnam como Superman a un policía. No obstante otras cintas ofrecen análisis más próximos a la realidad, como el mostrado por Francis Ford Coppola en "Jardines de Piedra", donde los maduros veteranos le dicen al impulsivo muchacho que aquella guerra no la pueden ganar y él les responde que olvidan su potencia de fuego portada por sus helicópteros contra los "arcos y flechas" vietnamitas, metáfora sobre una de las causas de la derrota, el pensar que se puede vencer a un pueblo subdesarrollado solo a base de bombas.

En el siglo XXI, la mala conciencia estadounidenses quedó limpia, según , estrenándose obras como "Forrest Gump" o "Across the Universe" (2007) dirigida por Julie Taymor y situada en Nueva York durante los movimientos antibélicos principalmente formados por jóvenes, al ver amigos y familiares ir contra su voluntad a Vietnam.

Una ausencia en muchos de estos largometrajes es la postura vietnamita, con alguna excepción como "Vietnam Vietnam", mostrando la participación australiana y las acciones del FNLV en los poblados. Fue en 2002 cuando se estrenó "We Were Soldiers" ("Cuando éramos soldados" en España, "Fuimos héroes" en Latinoamérica) adentrándose un poco más en la vida en los túneles vivida por los soldados del Norte, al estar basada en la reunión mantenida por con sus antiguos adversarios vietnamitas.





</doc>
<doc id="8254" url="https://es.wikipedia.org/wiki?curid=8254" title="Arminianismo">
Arminianismo

El arminianismo es una doctrina teológica cristiana fundada por Jacobo Arminio en la Holanda de comienzos del siglo XVII, a partir de la impugnación del dogma calvinista de la doble predestinación.

Sustenta la salvación en la cooperación del hombre con la gracia divina a través de la fe. Frente al concepto calvinista de predestinación (o “elección”) incondicional, el arminianismo enseña que la predestinación se ha basado en: (1) la presciencia de Dios, quien tiene el conocimiento previo de quién creerá y quién no creerá en Cristo; y (2) la voluntad del hombre, por asistencia divina, que es hecha libre para creer o rechazar a Cristo.

Después de la muerte de Arminio (en 1609), sus principios se formularon en el manifiesto de cinco puntos "Remonstrans", publicado en 1610 (por lo que sus seguidores también pasaron a denominarse “remonstrantes”).

En 1618 el arminianismo fue condenado por el sínodo de Dort o de Dordrecht, convocado a instancias del estatúder de Holanda Mauricio de Nassau, que apoyaba a los calvinistas intransigentes y monárquicos (Franciscus Gomarus y los denominados “gomaristas” o “contrarremonstrantes”). Johan van Oldenbarnevelt y otros dirigentes principales del arminianismo fueron entonces ejecutados, mientras que otros muchos, entre los que se encontraban Hugo Grocio y Simón Episcopius, tuvieron que exiliarse.

La teología arminiana contribuyó a la aparición del metodismo en Inglaterra. No todos los predicadores metodistas del siglo XVIII fueron arminianos, pero sí la mayor parte, como el propio John Wesley.

Arminio afirmaba firmemente la necesidad de la gracia de Dios para la redención de todo ser humano, pero consideraba que la gracia puede ser rechazada por el hombre en su libre albedrío. El arminianismo se opone a la postura calvinista, donde esta última enseña que algunos están predestinados para salvación y otros para perdición. Arminio consideraba que la expiación de Cristo es para todos y no solo para algunos elegidos, aunque no todos la aceptan y por lo tanto no reciben sus beneficios. Por lo tanto según los arminianos es posible “caer de la gracia” y no es correcto pensar que los que ya recibieron la gracia nunca se perderán. El calvinismo sostiene que: “Ya siendo salvo el individuo, siempre salvo”.

El arminianismo enseña que la destitución de Dios por causa de la rebelión es posible a pesar de haber sido parte de Su institución.

La posición arminianista empieza desde la perdición y separación de Dios, del mismísimo Luzbel (el diablo). Habiendo sido él un querubín, ocupando el más alto rango angelical, puesto sobre los ángeles creados, conociendo a Dios íntimamente, habiendo sido parte de Su reino por milenios, no obstante, decide por su libre albedrío rebelarse contra el Creador. Él junto con los ángeles que le siguieron, fueron destituidos de la gloria de Dios. Adán, habiendo sido creado por Dios junto con Eva su mujer, deciden por esa libertad otorgada comer del fruto prohibido, trayendo sobre sí y sobre la humanidad el pecado y la destitución. El pueblo judío fue liberado de la esclavitud de Egipto, lo cual tipifica ser liberado del pecado. Sin embargo, por sus tendencias pecaminosas no heredaron la tierra prometida. Solo Caleb y Josué con los suyos y la segunda y tercera generación de judíos entró en ella. El argumento más poderoso del arminianismo, sin duda alguna, es el siguiente: “Si un número predeterminado de seres humanos ya estaba predeterminado para salvación, la venida de Jesús, el Hijo de Dios, no hubiese sido requerida”. El pasado, presente y futuro son simultáneos para Dios. Él en su presciencia ya sabe quiénes lograron entrar en Su presencia, pero nosotros los hombres no. Por lo tanto, no podemos determinar quiénes califican y quiénes no.
Todos fuimos predestinados para salvación, es decir, con el objetivo de ser salvos. Pero eso no quiere decir que necesariamente todos seremos salvos, porque aunque Dios nos predestinó para salvación, también nos dio libertad para salvarnos o perdernos: el libre albedrío.

¿Existen personas que nacen condenadas al tormento eterno, incluso si se arrepienten y aceptan lo que hizo Jesús en la cruz? Eso no armonizaría con el carácter de Dios; pues Él dice: A los cielos y a la tierra llamo por testigos hoy contra vosotros, que os he puesto delante la vida y la muerte, la bendición y la maldición; "escoge", pues, la vida, para que vivas tú y tu descendencia.

Denominaciones arminianas son las diferentes Iglesias metodistas (Iglesia Metodista Episcopal, Iglesia Metodista Unida, Iglesia Metodista Libre), la Iglesia del Nazareno, el Ejército de Salvación (The Salvation Army), la Iglesia Adventista del Séptimo Día, la Iglesia Wesleyana, la Iglesia de Dios, la mayoría de las Iglesias pentecostales, la Iglesia Internacional del Evangelio Cuadrangular, las Iglesias de Cristo, las Asambleas de Dios, y otras del movimiento restauracionista (menonitas en su mayoría). Muchos anglocatólicos (como C.S. Lewis)También otras iglesias cristianas como la Iglesia copta, la Iglesia católica y la Iglesia ortodoxa creen en la libertad de la voluntad humana y que toda persona tiene la posibilidad de recibir salvación y que, una vez que recibe la salvación, también la puede perder; si bien las argumentaciones que dan al respecto son diferentes y de más vieja data.

Cabe anotar igualmente que cuando se habla de perder la salvación, no es porque Dios la arrebata nuevamente después de haberla otorgado en Jesús, sino que es el mismo hombre quien la desecha una vez que rompe su comunión con Dios a través del pecado.

1.- Libre albedrío o habilidad humana. Aunque la naturaleza humana fue totalmente afectada por la caída, sin embargo, Dios en su gracia capacita la voluntad del pecador para que libremente se arrepienta y crea, o rehuse hacerlo. Cada pecador, capacitado por la gracia de Dios, tiene libertad para creer o rehusar creer, y su destino eterno depende de cómo use dicha libertad. La libertad con la que Dios capacita al hombre caído, consiste en poder escoger libremente entre el bien y el mal en la esfera de lo espiritual. El pecador puede cooperar con el Espíritu de Dios y ser regenerado o resistir la gracia de Dios y perderse para siempre. El pecador necesita la asistencia del Espíritu Santo, pero no tiene que ser regenerado por el Espíritu antes de 
que pueda creer, ya que la fe es un don de Dios que el hombre puede recibir o rechazar libremente, y precede al nuevo nacimiento. La fe es un don de Dios; y el hombre lo puede recibir y ejercer para vida eterna, o rechazarlo para condenación.

2.- Elección condicional. Dios escogió para salvación, antes de la fundación del mundo, a todas aquellas personas que, asistidas por su gracia habilitadora, creen en Cristo. Esto se debe al hecho de que Dios vio de antemano que dichos individuos habrían de responder positivamente a su llamado, arrepintiéndose y creyendo en Cristo. Dios escogió solo a aquellos que él vio de antemano que voluntariamente creerían en el evangelio, asistidos por su gracia resistible.

3.- Redención universal o expiación general. La obra redentora de Cristo brinda a todos los hombres la oportunidad de ser salvos, y garantizó la salvación de todos los que habían creído y preservado hasta la muerte de Cristo, y también garantizó la salvación de todos los que habrían de creer y perseverar después de la muerte de Cristo. A pesar de que Cristo murió por todos los hombres, solo los que creen en él son salvados. Su muerte es suficiente para la salvación de todos los hombres, pero solo eficaz en los que creen.

4.- El Espíritu Santo puede ser resistido eficazmente. Él Espíritu Santo convence de pecado al mundo, y hace todo lo que se ha determinado para traer a cada pecador a la salvación. El llamado del 
Espíritu, sin embargo, puede ser resistido, ya que el hombre es hecho libre por la gracia de Dios. El Espíritu no regenera al pecador hasta que éste cree; la fe (que es un don de Dios que el hombre puede recibir o rechazar libremente) precede al nuevo nacimiento. Dios ha determinado que su llamado, a través del Espíritu Santo, pueda ser libre y voluntariamente aceptado o resistido. El Espíritu Santo obra eficazmente trayendo a Cristo solo a aquellos que no le resisten. El Espíritu no imparte vida hasta que el pecador responde, arrepintiéndose y creyendo voluntariamente en Cristo. Dios, por tanto, ha determinado que Su gracia no actúe de forma irresistible; sino que la misma puede ser resistida por el hombre.

5.- El caer de la gracia o el perder la salvación. Algunos arminianos creen que el ser humano, una vez salvo, no perderá su salvación y otros piensan que la salvación pueda perderse por no perseverar en la fe.



</doc>
<doc id="8255" url="https://es.wikipedia.org/wiki?curid=8255" title="Náhuatl">
Náhuatl

El náhuatl (autoglotónimo "nāhuatlahtōlli", que deriva de "nāhua-tl", «sonido claro o agradable» y "tlahtōl-li", «lengua o lenguaje») o mexicano es una macrolengua yuto-nahua que se habla en México. Existe, por lo menos, desde el siglo V, aunque al darse la diferenciación entre el yuto-nahua del sur y el proto-nahua (c. siglo III) ya es posible hablar de una lengua náhuatl. Con la expansión de la Cultura Coyotlatelco durante los siglos V y VI d.C.en Mesoamérica, el náhuatl comenzó su rápida difusión por el Eje Neovolcánico extendiéndose por la costa del Pacífico y dando origen al pochuteco y a otra rama hacia Veracruz que daría origen al náhuat de Centroamérica. Así, poco a poco el náhuatl pasó por encima de otras lenguas mesoamericanas hasta convertirse en "lengua franca" de buena parte de la zona mesoamericana; en una primera etapa fue difundida en el área central de México por los tepanecas; posteriormente, en una segunda etapa, desde el siglo XV esta lengua fue expandida en los territorios dominados por el Imperio mexica.

Durante los siglos que precedieron a la conquista española del imperio azteca, los mexicas habían incorporado a sus dominios gran parte del centro de México. La influencia imperial convirtió a la variante del náhuatl hablada por los habitantes de México-Tenochtitlan, capital del imperio, en lengua de prestigio en Mesoamérica. Después de la llegada de los españoles a México, se gramaticalizó el náhuatl, que hasta entonces no tenía grafía latina. Los españoles escribieron muchas crónicas, gramáticas, obras poéticas y documentos administrativos usando el alfabeto latino durante los siglos XVI y XVII. Esta temprana práctica escrita, generalmente basada en la variante de Tenochtitlan, ha sido denominada náhuatl clásico y es una de las lenguas más documentadas y estudiadas de América. Debido a la popularidad del idioma y, en parte, a la expansión territorial gracias a conquistadores tlaxcaltecas, Felipe II estableció el náhuatl como idioma oficial del Virreinato de Nueva España.

Actualmente, variantes del náhuatl son usadas en comunidades dispersas, mayormente en áreas rurales en el centro de México y a lo largo de la costa del Golfo. Hay diferencias considerables entre variantes y algunas no son mutuamente inteligibles. La región de la huasteca concentra una gran parte de los hablantes de náhuatl. Todas las variantes han tenido diferentes grados de influencia del español. Ninguna de las variantes contemporáneas es idéntica al náhuatl clásico, aunque las variantes centrales, habladas alrededor del Valle de México, están más estrechamente relacionadas con este que aquellas de la periferia. Evidencia dialectológica indica que las variantes modernas no parezcan ser evoluciones de la variante hablada en Tenochtitlan, sino evoluciones de variantes regionales ya existentes antes de la codificación del llamado náhuatl clásico. Siguiendo esta idea, se ha propuesto que la lengua de la capital mexicana fue una "koiné" resultante del contacto entre hablantes de distintas variantes.

El náhuatl es la lengua nativa con mayor número de hablantes en México, con aproximadamente dos millones, la mayoría bilingüe con el español o trilingüe con el inglés.

El náhuatl pertenece a la familia yuto-nahua (uto-azteca) el cual tiene una división prehistórica en “yuto-nahua del norte” y “yuto-nahua del sur”. De esta última rama se desarrollaron cuatro grupos, de los cuales, el grupo “Nahuatlano” también llamado “nahuano” o “aztecano” es el que da origen al náhuatl. La división Yuto-nahua del sur / Nahuatlano es el momento en que surge el Proto-nahua, que es el ancestro de todas las variantes. Según algunos autores la primera división del proto-nahua dio origen al extinto pochuteco, quedando por otro lado lo que los lingüistas norteamericanos llaman "General Aztec" o "náhuatl nuclear" según el INALI, el que a su vez se divide en dos ramas, tenemos "Náhuatl Occidental" y el "Náhuatl Oriental", por último, la rama occidental se divide en "Náhuatl de la Periferia Occidental" y "Náhuatl central". Todas las variantes dialectales actuales se desprenden de estos grupos.

La sub-clasificación actual del náhuatl se basa en las investigaciones de Canger (1980, 1988) y Lastra de Suárez (1986). Canger inicialmente introdujo el esquema de una agrupación central y dos grupos periféricos; Lastra concordó con esta noción aunque difiere en algunos detalles. Canger y Dakin (1985) replantearon una división básica más antigua de la comunidad de hablantes del proto-nahua en sólo dos ramas, la Occidental y la Oriental y así justificar y comprender las variaciones de las orientales que muestran una mayor profundidad temporal. Canger originalmente consideró la zona central como una sub-área innovadora dentro de la rama occidental, pero en 2011, sugirió que surgió como una lengua koiné urbana con características de ambas áreas, tanto occidental como oriental. Canger (1988) incluyó provisionalmente los dialectos de la Huasteca en el grupo central, mientras Lastra de Suárez (1986) los sitúa en la periferia oriental; Kaufman (2001) y la mayoría de los investigadores actuales aceptan estas conclusiones.

Desde un punto de vista tipológico, resalta su importancia como ejemplo de lengua polisintética y aglutinante, particularmente en la morfología verbal y en la formación del léxico. Tipológicamente es además una lengua de núcleo final, en la que el modificador suele preceder al núcleo modificado.

Sobre la cuestión del punto geográfico de origen, los lingüistas durante el siglo XX coincidieron en que la familia de lenguas yuto-nahuas se originó en el suroeste de los actuales Estados Unidos. Tanto la evidencia de la arqueología y la etnohistoria es compatible con una difusión hacia el sur a través del continente americano; este movimiento de comunidades hablantes se da en varias oleadas desde los desiertos del norte de México al centro de México. El proto-nahua por lo tanto surgió en la región entre Chihuahua y Durango donde al ocupar una mayor extensión de territorio, formó rápidamente dos variantes, una que continuó dispersando hacia el sur con cambios innovadores mientras la otra con rasgos conservadores del yuto-nahua se desplazó al oriente.

La migración propuesta de hablantes de la lengua proto-nahua en la región mesoamericana se ha colocado en algún momento alrededor del año 500, hacia el final del período Clásico Temprano en la cronología mesoamericana. Antes de llegar al centro de México, grupos pre-nahuas probablemente pasaron un periodo de tiempo en contacto con las lenguas cora y huichol del occidente de México (que también son uto-aztecas).

El surgimiento del náhuatl y sus variantes por lo tanto se da durante la época del apogeo de Teotihuacán. Las rutas comerciales teotihuacanas sirvieron para una rápida difusión de la nueva lengua. La identidad de la lengua hablada por los fundadores de Teotihuacán nos es desconocida, sin embargo, durante mucho tiempo ha sido objeto de debate; de esta manera en los siglos XIX y XX algunos investigadores creían que Teotihuacán había sido fundada por hablantes de náhuatl; más tarde hacia finales del siglo pasado la investigación lingüística y arqueológica tiende a contradecir ese punto de vista. Ahora se cree que es más probable que la lengua teotihuacana estuviera relacionada con el totonaco o fuera de origen mixe-zoqueano. Buena parte de la migración nahua al centro de México fue consecuencia y no causa de la caída de Teotihuacán. Desde estas épocas tempranas se dieron préstamos entre las diferentes familias lingüísticas e incluso a nivel morfosintáctico.

En Mesoamérica las familias de la lengua maya, otomangue y mixe-zoque habían coexistido durante milenios. La interacción de estas lenguas generó una serie de rasgos comunes en todas ellas que permiten que entendamos la zona mesoamericana como una sola a nivel lingüístico, independientemente de la evolución de cada lengua en su propio grupo. Después que los nahuas llegaron a la zona de alta cultura de Mesoamérica, su lengua también adoptó algunos de los rasgos que definen el área lingüística mesoamericana; así por ejemplo los nahuas adoptaron el uso de sustantivos relacionales y una forma de construcción posesiva típica de las lenguas mesoamericanas.

Teotihuacán ejercía un poder centralista y marcaba las pautas de los señoríos locales, quienes al parecer tenían que ser legitimados desde la metrópolis. Tras el colapso de la gran ciudad surgieron modelos nuevos para detentar el poder, junto con estos modelos al parecer se fue promoviendo la lengua náhuatl la cual se considera difundida por la cultura Coyotlatelco, sin embargo, la lengua no solo fue hablada por sus nativos, sino que poco a poco fue adoptada por las poblaciones otomangues con mayor antigüedad y que habían dependido de Teotihuacán. Al fundarse Tula Chico en el siglo VII ya se sentía la influencia nahua pero no era muy intensa; trescientos años después con la re-fundación de esta ciudad por el año 900, que a partir de entonces será conocida como “Tollan” (Tula), sus fundadores son reconocidos por las fuentes como “nahuas-chichimecas” quienes comparten el poder con los nonohualcas. Es en este momento que el náhuatl adquiere relevancia política, poco después se volverá el idioma oficial de los tepanecas (que hablaban originalmente una variante del otomí), y ya en el siglo XIV fue adoptado por los acolhuas de Tetzcoco.

Aunque los mexicas se cree que siempre hablaron el náhuatl es posible que también lo hayan adoptado. La influencia política y lingüística de este grupo llegó a extenderse en la América media y el náhuatl se convirtió en una lengua franca entre los comerciantes y las élites en Mesoamérica, por ejemplo entre los mayas quiché. Tenochtitlan creció hasta convertirse en el mayor centro urbano mesoamericano, esto atrajo a los hablantes de náhuatl de otras áreas donde se había extendido por siglos previamente, con lo que se dio a luz a una nueva forma urbana de náhuatl con rasgos de muchos dialectos. Esta variedad urbanizada de Tenochtitlan-Tlatelolco es lo que llegó a ser conocido como náhuatl clásico y fue ampliamente documentado en la época colonial.

Con la llegada de los españoles al corazón de México en 1519 la situación del idioma náhuatl cambiaría de manera significativa; por un lado comienza un desplazamiento por la lengua española; por el otro, su uso oficializado para la comunicación con los nativos generó el establecimiento de nuevos asentamientos; a la vez se dio la creación de una amplia documentación en escritura latina, con lo cual se asienta un registro fidedigno para su preservación y comprensión, por lo que el idioma siguió siendo importante en las comunidades nahuas bajo el dominio español.

Los españoles se dieron cuenta de la importancia que tenía la lengua y prefirieron continuar con su uso que cambiarla, también encontraron que el aprendizaje de todas las lenguas indígenas de lo que ellos llamarían Nueva España era imposible en la práctica, por lo que se concentraron en el náhuatl. Inmediatamente después de la Conquista, los misioneros franciscanos fundaron escuelas —como el Colegio de Santa Cruz de Tlatelolco en 1536— para la nobleza indígena con el propósito de re-educarlos dentro de los cánones occidentales, donde aprendían teología, gramática, música, matemáticas. A la vez los misioneros emprendieron la redacción de gramáticas, llamadas en esa época “artes”, de las lenguas indígenas para su uso por parte de sacerdotes. La primera gramática náhuatl escrita en 1531 por los franciscanos se encuentra perdida, la más antigua que se preserva fue escrita por Andrés de Olmos y publicada en 1547. Hacia el año de 1645 tenemos noticia de cuatro obras más publicadas cuyos autores son, en 1571 Alonso de Molina, en 1595 Antonio del Rincón, en 1642 Diego de Galdo Guzmán y en 1645 Horacio Carochi. Este último es considerado hoy en día el más importante de los gramáticos de la época virreinal. Carochi ha sido especialmente importante para los investigadores que trabajan en la nueva filología, debido a su enfoque científico que precede a las investigaciones lingüistas modernas, analiza más a detalle los aspectos fonológicos que sus predecesores e incluso sucesores, quienes no habían tomado en cuenta la pronunciación del cierre glotal (saltillo) que es en realidad una consonante o la longitud vocal.
En 1570 el rey Felipe II de España decretó que el náhuatl debía convertirse en la lengua oficial en la Nueva España con el fin de facilitar la comunicación entre los españoles y los nativos del virreinato. Durante este período la Corona española permite un alto grado de autonomía en la administración local de los pueblos indígenas, y en muchos pueblos la lengua náhuatl era la oficial de hecho, tanto escrita como hablada. Durante los siglos XVI y XVII, el náhuatl clásico se utilizó como lengua literaria, y un gran corpus de documentos de ese período sobrevivió hasta nuestros días. Las obras de este período incluyen historias, crónicas, poesía, obras de teatro, obras canónicas cristianas, descripciones etnográficas y documentos administrativos. Como ejemplos podemos citar el "Códice Florentino", un compendio de doce volúmenes de la cultura mexica compilado por el franciscano Bernardino de Sahagún; la "Crónica Mexicáyotl" de Fernando Alvarado Tezozómoc que relata el origen y el linaje real de Tenochtitlán; los "Cantares mexicanos" que son una colección de poemas en náhuatl; el diccionario compilado por Alonso de Molina náhuatl-español y español-náhuatl el cual sigue siendo básico para la lexicología moderna; y el "Huei tlamahuiçoltica", una de las descripciones en náhuatl de la aparición de la Virgen de Guadalupe.

Durante un tiempo, la situación lingüística en la Nueva España se mantuvo relativamente estable pero en 1686 el rey Carlos II emitió una real cédula que prohíbe el uso de cualquier idioma distinto del español en todo el Imperio español, reiterándola en 1691 y 1693, en las que dicta la creación de la “parcela escolar” para la enseñanza del idioma imperial. Otro decreto el 10 de mayo de 1770, ahora de Carlos III, estableció la creación de nuevos centros de enseñanza completamente en castellano para la nobleza indígena y se deshizo del náhuatl clásico como lengua literaria, aunque hasta la Independencia de México en 1821, los tribunales españoles aún admitían testimonios en náhuatl y documentación como prueba en los juicios, con traductores judiciales que exponían en español.

La situación indígena y del habla náhuatl al inicio del movimiento de la Independencia en realidad había sido sostenido pues el 66% de la población era indígena de los 6 millones de habitantes del país. Los indicadores demográficos muestran un crecimiento paralelo al de la población mestiza de México. Las comunidades nahuas ya habían asimilado el cristianismo de manera sincrética, además eran parte fundamental de la fuerza productiva del país; su desarrollo local se fincaba en una tradición ya consumada durante los últimos 300 años y que había generado pocos cambios en su organización social y cultural, de hecho, muchas de esas manifestaciones sobrevivieron hasta nuestros días.

A lo largo de la época moderna, la situación de las lenguas indígenas ha aumentado en precariedad cada vez más en México, y el número de hablantes de prácticamente todas las lenguas indígenas ha disminuido. A pesar de que el número absoluto de hablantes de náhuatl en realidad ha aumentado en el último siglo, las poblaciones indígenas se han vuelto cada vez más marginadas en la sociedad mexicana. Los grandes cambios en las comunidades indígenas se dieron a partir de las reformas agrarias emergidas del revolucionario Plan de Ayutla por medio de la “Ley Lerdo” a mediados del siglo XIX, con lo cual se abolian las tierras comunales y a partir de entonces los indígenas se vieron forzados a pagar una serie de nuevos impuestos y que bajo la coacción de hacendados y gobierno no pudieron pagar creándose los grandes latifundios, lo que provocó que poco a poco fueran perdiendo sus tierras, su identidad, su lengua, e incluso su libertad.

Este proceso aceleró los cambios en la relación asimétrica entre las lenguas indígenas y el castellano, así el náhuatl se vio cada vez más influenciado y modificado; como primera consecuencia es observable una zona de una rápida pérdida del habla y las costumbres cercana a las grandes ciudades, como segunda consecuencia vemos zonas donde la “castellanización” es más fuerte provocando un bilingüismo activo, en una tercera zona se mantuvieron los hablantes indígenas más aislados y conservaron más puras sus tradiciones. Las políticas porfirianas tendían a la eliminación de las lenguas nativas, buscando el desarrollo y el progreso del país bajo un nacionalismo mexicano, política seguida por los gobiernos post-revolucionarios. Solo hasta el gobierno cardenista surge un verdadero interés institucional por comprender y estudiar la cultura indígena, intentando revertir la tendencia de la incorporación forzada a la cultura nacional, lo que de hecho no pasó y continuó la pérdida hasta los ochenta.

Cambios significativos se dieron por lo menos desde mediados de la década de 1980, aunque las políticas educativas en México se centraron en la castellanización de las comunidades indígenas, para enseñar puramente español y desalentar el uso de las lenguas nativas, tuvo como resultado que hoy en día un buen número de hablantes de náhuatl estén en posibilidad de escribir tanto su lengua como el español; aun así su tasa de alfabetización en español sigue siendo muy inferior a la media nacional. A pesar de ello, el náhuatl todavía es hablado por más de un millón de personas, de los cuales alrededor del 10 % son monolingües. La supervivencia del náhuatl en su conjunto no está en peligro inminente, pero la supervivencia de ciertos dialectos sí lo está; y algunos dialectos ya se han extinguido durante las últimas décadas del siglo XX.

La década de 1990 vio la aparición de cambios diametrales en las políticas del gobierno mexicano hacia los derechos indígenas y lingüísticos. La evolución de los acuerdos en el ámbito de los derechos internacionales combinada con presiones internas condujeron a reformas legislativas y la creación de organismos gubernamentales descentralizados; así, ya para el 2001 el Instituto Nacional Indigenista desapareció para darle paso a la "Comisión Nacional para el Desarrollo de los Pueblos Indígenas" (CDI) y el INALI creado en 2003 con responsabilidades para la promoción y protección de las lenguas indígenas, en particular la Ley general de Derechos Lingüísticos de los Pueblos indígenas reconoce todas las lenguas indígenas del país, incluyendo el náhuatl, como "idiomas nacionales" y da a los indígenas el derecho a utilizarlos en todas las esferas de la vida pública y privada. En el artículo 11, que garantiza el acceso a la educación obligatoria, bilingüe e intercultural. Esta ley da origen al "Catálogo de las Lenguas Indígenas Nacionales" en 2007.

En 1895, el náhuatl era hablado por más del 5 % de la población. Para el año 2000, esta proporción había caído a 1,49 %. Teniendo en cuenta el proceso de marginación combinada con la tendencia de la migración a las zonas urbanas y a los Estados Unidos, algunos lingüistas están advirtiendo sobre la muerte inminente de las lenguas. En la actualidad se habla en náhuatl sobre todo en las zonas rurales por una clase empobrecida de agricultores de subsistencia indígenas. De acuerdo con el Instituto Nacional de Estadísticas de México, el INEGI, el 51 % de los hablantes de náhuatl están involucrados en el sector agrícola y 6 de cada 10 no reciben sueldos o ganan menos del salario mínimo.

Como etiqueta lingüística el término "náhuatl" comprende un continuum de variantes lingüísticas que puede verse, o bien como un grupo de lenguas estrechamente relacionadas, o como dialectos divergentes de una misma lengua. En cualquier caso, las variantes del náhuatl pertenecerían al grupo náhuatl (también llamado nahuatlano y nahuano) de la familia yuto-nahua. El Instituto Nacional de Lenguas Indígenas, por ejemplo, reconoce 30 variantes dentro del grupo lingüístico etiquetado como "Nahuatl". El catálogo Ethnologue, en cambio, reconoce 28 variantes identificadas con códigos ISO separados. Algunas veces, la etiqueta "náhuatl" se usa también incluyendo al Idioma Náhuat de El Salvador conocido incorrectamente como ""Pipil"".En México la lengua náhuatl se habla principalmente en cinco estados: Guerrero, Puebla, Hidalgo, San Luis Potosí y Veracruz, donde tiene en cada estado una población arriba de 100 mil hablantes. En los estados de Morelos y Tlaxcala cuenta con una población dispersa o en localidades pequeñas; en promedio se puede hablar de una población de alrededor de 20 mil hablantes. En los estados de Tabasco, Michoacán, México, Oaxaca, Nayarit y Durango, así como los habitantes de Milpa Alta la presencia es mínima y con posibilidad de perderse (excepto en Oaxaca).

Las variantes dialectales se agrupan en tres ramas, Náhuatl central, Náhuatl de la Periferia Occidental y Náhuatl Oriental. Algunos de los dialectos son:

Náhuatl central es una derivación del “náhuatl occidental” que durante los siglos XIV y XV desarrolló varias innovaciones a la vez que asimiló algunas características de las demás variantes. En el mismo siglo XV ya presentaba una diferenciación y eran considerados diferentes las hablas del centro (valle de México), de Morelos o de Tlaxcala-Puebla.

Náhuatl clásico es la denominación de la lengua registrada durante el Virreinato; ésta en realidad refleja una serie de variantes. Una gran parte de que parezca solo un idioma es debido al esfuerzo de los frailes que quisieron trasmitir, por medio de la lengua más culta, la fe cristiana. Posteriormente las autoridades virreinales, con una forma más coloquial, dieron cabida a la elaboración de documentos (testamentos, pleitos territoriales, denuncias, etc.) en lengua náhuatl. Los libros y documentos escritos en la ciudad de México contienen un estilo más elegante, el que quisieron rescatar los religiosos. Obra diversa procede de otras ciudades importantes como Colhuacán, Chalco, Cuauhtitlan, Tlaxcala, Tecamachalco. Las diferencias más marcadas entre la variante de México y otras las podemos ver al comparar el Nican mopohua con textos de Jalisco (Yáñez, 2001) o de Guatemala (Dakin, 1996).

Náhuatl de Morelos es una denominación genérica que designa a toda la población hablante de esta lengua en el estado, por lo que se presta a confusión. Para Ethnologue existen solo dos variantes, una con el código ISO 639-3 nhm que representa la mayoría de los nahuas, y con código nhg clasifica a los hablantes de Tetelcingo.

El INALI divide el estado en cuatro variantes; el más representativo y al que le correspondería el código “nhm” lo denomina mexicano de Tetela del Volcán (es el náhuatl de Hueyapan y Santa Catarina); las otras variante son el mexicano de Temixco (náhuatl de Cuentepec); el mexicano central bajo (hablado en los municipios de Ayala y Jojutla) y por supuesto el náhuatl de Tetelcingo que denomina mexicano de Puente de Ixtla.
La población total nahuahablante en el estado es de 19 241 personas.

Náhuatl de Tlaxcala es una de las variantes que presenta una gran asimetría en su relación con el español, buena parte de los pueblos históricos que la hablan están siendo absorbidos por la mancha urbana de la Zona Metropolitana de Puebla-Tlaxcala, por lo que la han modificada al correr de los años su estructura y fonología; es hablado con más frecuencia en los municipios de la región occidental del Volcán la Malintzi y sur del estado, como Tetlahnocan, Contla de Juan Cuamatzi, Chiautempan, Teolocholco y San Pablo del Monte. Es denominado por el INALI mexicano del oriente central y tiene alrededor de 19 mil hablantes.

El náhuatl de Tetelcingo o mösiehuali aunque emparentado con el náhuatl clásico tuvo una evolución que ha obligado a los investigadores a desarrollar un sistema de escritura muy particular. El primer estudio de esta variante lo hizo William Cameron Townsend en 1935. Es hablado por menos de 3500 personas en el municipio de Cuautla de Morelos (Morelos).

Por algunos considerada como la más conservadora o la que retomó características arcaicas como el uso del absolutivo en -t. Se tiene tres zonas muy marcadas, por una parte tenemos el “náhuatl de La Huasteca” con gran influencia de las variantes centrales; las variantes del centro de Veracruz y sur de Puebla con rasgos del Golfo; y las denominadas variantes del “Istmo” (o Golfo) con una fuerte sustitución del –tl por –t y un gran parecido al idioma náhuat de El Salvador.

El náhuatl guerrerense o náhuatl de Guerrero durante mucho tiempo fue clasificado en la rama central, sin embargo los estudios revelan un substrato de elementos orientales y los elementos centrales al parecer son innovaciones que se dieron a partir del siglo XIII. En el estado de Guerrero se presenta una variación dialectal que da origen a cuatro distintas variantes: dos de la “Periferia Occidental”; el “náhuatl de Tlamacazapa” y el “náhuatl de Coatepec”. El grueso de hablantes (cerca de 150 000) que son propiamente del “náhuatl de Guerrero” se ubican en la región centro-montaña del estado; la cuarta variante es el “náhuatl de Ometepec”, cerca de la Costa Chica y la cual tiene 430 hablantes. Esta última variante también es clasificada en la rama occidental.

El INALI engloba tres variantes bajo el nombre de náhuatl del Istmo (también llamado náhuatl istmeño), según los estudios del "Instituto Lingüístico de Verano" (SIL por sus siglas en inglés) le corresponde a la forma más extrema hacia el sur, en el municipio de Cosoleacaque, el código ISO 639-3 nhk, al de Mecayapan el código nhx y al de Pajapan aplica el código nhp.

La variante más representativa y estudiada es la de Mecayapan. Tiene alrededor de 20 000 hablantes, distribuidos también en el pueblo de Tatahuicapan.

En su proceso fonológico evolutivo esta lengua emparentada con el nawat tabasqueño y la usa de las letras /b/, /d/, /g/ y /r/. Además esta variante se caracteriza por el uso de vocales largas.

La región norte de Puebla presenta una geografía bastante accidentada, lo que provoca más aislamiento entre las comunidades; esta situación generó que en un territorio reducido tengan tres variantes según Ethnologue, cuatro según el INALI. Las denominaciones son confusas pues utilizan casi las mismas palabras como “norte”, “sierra”, como veremos a continuación. El náhuatl del noreste central, (denominación del INALI) es equivalente al "Nahuatl, Northern Puebla" de Ethnologue con código NCJ. Para la institución mexicana es la variante hablada en los municipios de Acaxochitlán (Hidalgo), en los municipios poblanos de Chiconcuauhtla, Honey, Huauchinango, Jopala, Juan Galindo, Naupan, Pahuatlán, Tlaola, Tlapapcoya, Xicotepec, Zihuateutla. Esta variante en realidad es derivada del náhuatl central. Tiene 71 040 hablantes. Uno de los primeros estudios de la zona lo hizo Yolanda Lastra en Acaxochitlán en 1980 (véase bibliografía).

Propiamente las variantes orientales son el náhuatl de la sierra noreste de Puebla, hablado en los municipios de Atempan, Ayotoxco, Cuautempan, Cuetzalan, Chignautla, Hueyapan, Hueytamalco, Huitzilán de Serdán, Ixtacamaxtitlán, Jonotla, Nauzontla, Tenampulco, Tetela de Ocampo, Teziutlán, Tlatlauquitepec, Tuzamapan, Xiutetelco, Xochiapulco, Xochitlán, Yaonahuac, Zacapoaxtla, Zautla, Zapotitlán, Zaragoza, Zoquiapan. Ethnologue le asigna el código AZZ ("Nahuatl, Highland Puebla"). Tiene 134 737 hablantes. En esta variante es muy marcado el uso de /t/ en las palabras en lugar de /tl/; los nativos llaman a su lengua “mexikanotlajtol” y se autodenominan “maseualmej” como equivalente de "gente indígena".

La otra variante llamada náhuatl de la sierra oeste de Puebla es hablada en Ahuacatlán, Aquixtla, Chignauapan, Tepetzintla, Zacatlán. Se le asocia el código NHI ("Nahuatl, Zacatlán-Ahuacatlán-Tepetzintla"). Tiene 19 482 hablantes.

La cuarta variante que reconoce el INALI la denomina náhuatl alto del norte de Puebla y es hablada únicamente en los municipios de Francisco Z. Mena y Venustiano Carranza. Tiene 1350 hablantes.

El náhuatl de La Huasteca, es una de las variedades con mayor número de hablantes. Aunque se reconocen como tres regiones distintas con particularidades la inteligibilidad es muy alta entre ellas. En el estado de Hidalgo (principalmente en los municipios de Huejutla, Jaltocán, Pisaflores y Tenango de Doria) es denominado por el INALI mexicano de la Huasteca hidalguense; en el noroeste de Veracruz es llamado náhuatl de la Huasteca veracruzana; y en el sureste de San Luis Potosí lo clasifican como náhuatl de la Huasteca potosina. Tiene alrededor de 464 mil hablantes en las tres áreas.

Corresponden a esta rama las variantes de los estados de Michoacán, Colima, Jalisco, Nayarit y Durango. Aunque prácticamente está extinto en Jalisco y Colima, no ha sido declarado así por ninguna institución. Además se incluyen las hablas de las poblaciones de Tlamacazapa (municipio de Taxco, Guerrero), la de Coatepec Costales (municipio de Teloloapan, Guerrero) y la de Temascaltepec (Estado de México). Estas variantes son reconocidas por el uso de /l/ donde las variantes centrales usan /tl/. También están variantes presentan mayores cambios fonológicos y de morfo-sintaxis, por lo que muestra una mayor diferenciación respecto a otras regiones.

El náhuatl de Michoacán o mexicano central de occidente es hablado en la costa de Michoacán y parte de la sierra pegada a esta; ha sufrido una gran pérdida de hablantes pero es a la vez una de las que más se apega a sus tradiciones. Para recuperar su lengua se han implementado la educación indígena obligatoria durante los tres primeros ciclos escolares de la educación básica y se promueve la escritura literaria. Tiene 2809 hablantes.

El náhuatl de Jalisco o mexicano del occidente era una variante que se habló al sur del estado y compartía rasgos con el “náhuatl de Michoacán”. Fue la primera variante en tener su propia gramática, fue elaborada en 1692 por Fray Juan Guerra y se llamó ""Arte de la lengua mexicana. Que fue usual entre los indios del obispado de Guadalajara y de parte de los de Durango y Michoacán"". Las imposiciones por parte de los gobiernos locales durante el Porfiriato fueron causa de una rápida pérdida del habla a principios del siglo XX, los gobiernos post-revolucionarios no mostraron una mejor disposición ni siquiera para su estudio, por lo que en la década de los sesentas y setentas cuando lingüistas e investigadores quisieron estudiarlo ya estaba moribundo.

Náhuatl de Durango o mexicano del noroeste es una variedad que también se le conoce como “náhuatl mexicanero” y se habla en el estado de Durango, en las poblaciones de San Pedro de las Jícoras, San Juan de Buenaventura entre otras. Los nahuas de esta región tienen una interacción muy activa con los tepehuanes, coras y huicholes con los que comparten territorio. Su supervivencia se ha debido al aislamiento y lo poco accesible de sus comunidades; viven de una economía autosuficiente; su lengua es la usual en la vida cotidiana. Aunque en su mayoría son bilingües no aprenden el español sino hasta después de los 5 ó 6 años. Cuenta con alrededor de 1300 hablantes.

El Nawat es una de las variantes relacionada históricamente con los habitantes de Cuzcatlán, hoy El Salvador y parte de Nicaragua. Se tiene registro que también era hablada en poblaciones de Honduras. Hoy en día en El Salvador está en desuso. Ethnologue reporta 500 personas según estudio del 2015.

El pochuteco fue un idioma nahua que se habló en la Costa Sur de Oaxaca, particularmente en el actual municipio de Pochutla, fue estudiado en 1912 por el antropólogo norteamericano Franz Boas, quien escribió una descripción hasta 1917 en "International Journal of American Linguistics". En la actualidad se considera por algunos estudiosos como la primera variante que evolucionó antes del surgimiento de las ramas oriental y occidental, sin embargo, su estudio no fue completo por lo que en realidad su clasificación es dudosa.

La enseñanza de las lenguas nativas al igual que el español o cualquier idioma en el mundo se da en el seno materno de un hogar, es decir, se trasmite de padres a hijos. Esto conlleva toda una serie de factores socioculturales que determinan distintos grados de asimilación y proporcionan distintos niveles de comprensión e interpretación.

Esencialmente podemos considerar dos tipos de hablantes: quienes simplemente reproducen el habla de su entorno incluso sin necesidad de aprender a escribirlo y por otro lado, quienes lo cultivan ya sea oral o escrito para desarrollar una mayor capacidad de comunicación clara.

La lengua náhuatl durante la época prehispánica carecía de un sistema de escritura para su enseñanza, lo cual era de hecho innecesario, ya que aparte del aprendizaje en el hogar existían instituciones de educación obligatoria donde aprendían a distinguir el habla común (macehuallahtolli) del habla elegante (tecpillahtolli). En las escuelas indígenas (Telpochcalli, Calmecac o Cuicacalli) se ponía mucho énfasis en la adquisición de habilidades en oratoria, se aprendían de memoria largos discursos morales, históricos, obras de teatro y cantos. Esto hacía de la enseñanza de la lengua y en sí de la educación, un modelo de éxito.

Al llegar los españoles y reconocer la utilidad del sistema lo aprovecharon, permitiendo que el náhuatl continuara en uso. Los peninsulares se vieron obligados a aprender el idioma nativo, lo que provocó a su vez que escribieran tratados para su comprensión y utilización para la catequización del indígena. En realidad la enseñanza del náhuatl siguió dándose de manera natural, por medio del uso generalizado. La creación de centros de estudio durante el Virreinato fue para el conocimiento y desarrollo de las ciencias —entre ellas la escolástica— y no para la alfabetización de los indígenas, sólo la nobleza nahua tenía derecho a aprender a leer y escribir su lengua, el grueso del pueblo tenía que acudir ante “escribanos” para la creación de algún documento.

La situación colonial se extendió hasta el siglo XX, donde al inicio de la revolución mexicana la población indígena era 90 % analfabeta. Con la creación de la Secretaria de Educación Pública durante el mandato de Álvaro Obregón, se planteó un departamento de educación indígena, sin embargo, continúa el debate entre el uso o desuso de las lenguas vernáculas; es hasta 1940 que con el apoyo del presidente Lázaro Cárdenas se implementan programas de educación en lengua materna, cuyos experimentos pilotos funcionan pero al establecerlos a nivel nacional pierden solidez. Gobiernos posteriores se centran en la castellanización, dejando de lado la enseñanza de las lenguas nativas, a mediados de los ochentas se cuenta ya con una tasa mayor al 70 % de indígenas alfabetizados, sin embargo, sólo el 20 % sabe escribir su propio idioma.

Nuevos programas pilotos durante los ochentas y noventas por parte de la SEP fueron la base de una educación bicultural enfocada primero en la escritura y habla de la lengua materna, y a partir del cuarto ciclo escolar el aprendizaje del español. Durante el ciclo 1993-94 aparecen los primeros libros de texto gratuito en lengua náhuatl que cubrían hasta el tercer grado de educación primaria, línea que continúa hasta la actualidad (2016) ya que no se han elaborado los materiales para los grados superiores.

Junto con la educación básica la SEP por medio del Instituto Nacional para la Educación de los Adultos (INEA) ha desarrollado material para alfabetizar a personas mayores de 15 años, esto durante la primera década de este siglo, cubriendo nueve zonas escolares indígenas y un número de variantes casi iguales, las cuales son:


El INEA trabaja bajo el programa “Modelo Educación para la Vida y el Trabajo”, abreviado “MEVyT Indígena”, también conocido como MIB. Consta de siete módulos, cuatro de los cuales totalmente en náhuatl; estos materiales están disponibles en línea para todo público y de manera gratuita.

La gramática más antigua que conocemos de la lengua náhuatl, titulada "Arte de la lengua mexicana", fue elaborada por el franciscano fray Andrés de Olmos. Fue concluida el primero de enero de 1547 en el convento de Hueytlalpan, ubicado en el Estado mexicano de Puebla. Destaca el hecho de haber sido desarrollada antes que muchas gramáticas de lenguas europeas como la francesa, y tan sólo 55 años después de la "Gramática de la lengua castellana" de Antonio de Nebrija.

El náhuatl clásico y la mayoría de variedades modernas emplean las siguientes consonantes según el Alfabeto fonético internacional:

La distinción entre vocales cortas y largas es importante y permite diferenciar muchos pares como "te-" 'piedra(s)' / "tē-" 'gente', "cuahuitl" 'árbol' / "cuāuhtli" 'águila' o "chichi" 'perro' / "chīchī" 'mamar'. Este tipo de ortografía se encuentra en uso activo en la de Wikipedia o "Huiquipedia".

El náhuatl tiene una morfología nominal reducida, la mayoría de nombres tiene dos formas diferentes según el caso (poseído/no-poseído) o sólo una forma indistinguida. El estado absolutivo en singular se marca con /-tl/ y el poseído con /-w/ (ambos sufijos derivados del proto-utoazteca /*-ta/ y /*-wa/). En cuanto a las marcas de plural se usan sobre todo los sufijos /-meh/ (procedente del proto-utoazteca /*-mi/) y a veces /-tin/ y /-h/, y en menor grado se usa la reduplicación de la sílaba inicial "coyotl" 'coyote' "cocoyoh" 'coyotes', aunque esto en náhuatl a diferencia de lenguas uto-aztecas como el guarijío o el pima es marginal.

La morfología verbal a diferencia de la nominal usa un gran número de morfemas prefijos o clíticos que indican, sujeto, objeto, direccional, marca reflexiva. El número se indica además de prefijo de persona mediante sufijos variados, igualmente el tiempo y el modo se indica mediante sufijos. La raíz verbal cambia de forma para indicar aspecto así para el náhuatl clásico se consideran tres tipos (llamados “"temas"”: largo, breve, medio) e incluye gran número de sufijos derivativos.

El orden de los constituyentes es bastante libre aunque en los dialectos modernos tiende a SVO en oposición a VSO que eran más frecuentes en las etapas más antiguas de la lengua. Además las variantes modernas tienden a incluir interjecciones, conjunciones y adverbios prestados del español.

El adjetivo suele preceder al nombre pero al igual que en la sintaxis en las variantes modernas el acomodo refleja la influencia del español, aunque en los sintagmas todavía prevalece que el modificador o complemento suele preceder al núcleo sintáctico. Eso se refleja también en el hecho de que la lengua tiene sufijos que funcionan como postposiciones (núcleos del sintagma apositivo) en lugar de usar preposiciones como en el español.

El náhuatl se distingue por usar un número reducido de lexemas para construir gran cantidad de palabras, lo cual hace que casi toda palabra admita una descomposición en raíces, en su mayoría bisilábicas. Esto da a las palabras una gran transparencia en términos semánticos. Algunos ejemplos de composición léxica se encuentran en topónimos y nombres propios:


Ciertas áreas del léxico moderno han sido muy influidas por el español, así el sistema numérico de base vigesimal ha sido abandonado en favor del sistema decimal del español, quedando solo las formas de 1 a 20 del sistema nativo básicamente. Otras áreas del léxico como la vivienda, el vestido y ciertos términos agrícolas han incorporado también términos del español considerados que describen mejor la realidad tecnológica más moderna.

Originalmente se trataba de una escritura pictográfica con rasgos silábicos o fonéticos tipo rebus. Este sistema de escritura fue adecuado para mantener registros tales como genealogías, información astronómica y listas de tributos, pero no representaba el vocabulario total de la lengua hablada de la forma en que los sistemas de escritura del "viejo mundo" o la escritura maya podían hacerlo. Ya en los tiempos coloniales los códices fueron usados para enseñar la doctrina cristiana dando origen a los catecismos testerianos que podían ser “leídos” a la usanza anterior a la conquista de México. El epigrafista Alonso Lacadena ha argumentado que en los albores de la invasión española los escribas nahuas de Tetzcoco habían desarrollado una escritura totalmente silábica que podía representar los sonidos de la lengua hablada en forma similar a la escritura maya. Otros epigrafistas han cuestionado esta teoría argumentando que, aunque rasgos silábicos son evidentes en códices coloniales (muy pocos códices precolombinos han sobrevivido), esta característica puede ser interpretada más como una innovación inspirada por la alfabetización en español que como una continuación de una práctica precolombina.

Los españoles introdujeron el alfabeto latino, el cual fue utilizado para registrar una gran cantidad de poesía y prosa mexicana, algo que de cierta forma compensó la pérdida de miles de manuscritos mexicas quemados por los invasores europeos. Importantes trabajos de léxico, como el "Vocabulario" de Fray Alonso de Molina en 1571, y descripciones gramaticales, por ejemplo, el "Arte" de Horacio Carochi en 1645, fueron producidos usando variaciones de esta ortografía. La ortografía de Carochi usó cuatro diferentes diacríticos: el macrón (ā) para las vocales largas; el acento (á) para vocales cortas; el circunflejo (â) y el grave (à) para el saltillo en diferente posición.

En 2016 surgió una propuesta de un abugida o alfasilabario del náhuatl formulada por Eduardo Tager y traducida al español en este artículo; http://unifont.org/nahuatl/
Aunque no ha tenido difusión entre los maestros y nahuatlatos, la propuesta es más precisa en cuanto a la adaptación de fonemas nahuas a la escritura que como lo ha sido el alfabeto latino, y tiene una mejor economía del espacio.

Actualmente, existen dos convenciones diferentes que usan diferentes subconjuntos del alfabeto latino: la ortografía tradicional y la ortografía moderna que ha aceptado la SEP. La Secretaría de Educación Pública de México (SEP) es la institución que regula las reglas de la ortografía, y la que ha establecido un sistema de escritura práctico que se enseña en los programas de educación primaria bilingües en las comunidades indígenas.

El siguiente cuadro recoge convenciones usadas en la ortografía clásica y en la ortografía de las variantes modernas para transcribir los diferentes fonemas:

A continuación se reproduce una lista de cognados de varias variantes dialectales en diferentes grupos que permiten reconocer los parentescos más cercanos y la evolución fonológica según la región:

Entre los pueblos nahuas existía un gran aprecio por la poesía, llamada «"In Xōchitl In Cuīcatl"», que significa «La Flor y El Canto» literalmente, aunque hay quien la interpreta como «palabra florida o florecida». La poesía era una de las actividades especialmente practicada entre la clase noble. Huexotzingo, Texcoco, Culhuacan eran las ciudades más renombradas por sus poesías. Ocasionalmente se organizaban encuentros poéticos en donde se reunían incluso aquellos dirigentes de ciudades en guerra. El más famoso ocurrió en Huexotzingo en 1490, organizado por Tecayehuatzin, señor de ese lugar. Detalles de este encuentro y muchas otras poesías se hallan en varios manuscritos recopilados después de la Conquista. El más famoso se llama "Cantares mexicanos", y data del siglo XVI. Existe también otra recopilación de poesía, hecha por Juan Bautista Pomar, nieto de Nezahualcóyotl.

Bernardino de Sahagún menciona que los mexicas disfrutaban de representaciones dramáticas, algunas cómicas, otras eróticas, y otras sobre la vida de sus dioses; estas representaciones se transformaron en los tiempos coloniales con tonos cristianos-sincréticos dando origen a la danzas de conquista y a las representaciones de “pastorelas” hasta nuestros días.

De los miles de manuscritos prehispánicos, solo sobrevive una docena de códices, dado que los europeos tenían la creencia que los pobladores indígenas eran adoradores del diablo y por consiguiente quemaron y destruyeron prácticamente toda su obra.

Existen también relaciones y documentos en náhuatl producidos por los «pilli» y tlacuilos, que poco después de la Conquista comenzaron a aprender a usar la escritura europea, como «Los anales de Tlatelolco» y el original náhuatl del "Códice Florentino".

Algunos autores novohispanos como Sor Juana Inés de la Cruz, escribieron algunas obras en náhuatl.
Poesía náhuatl ("vid" Nezahualcóyotl) y
Nican Mopohua ("vid" Antonio Valeriano).

El náhuatl clásico se caracteriza por la abundancia de recursos literarios, siendo particularmente importantes los siguientes:


Los archivos musicales conventuales y catedralicios de México también dan cuenta de música en lengua náhuatl, tal es el caso de los motetes "In ilhuicac cihuapille" y "Dios itlaço nantzine" atribuibles a Hernando Franco y extraídos del "Códice Valdés". Gaspar Fernandes, Maestro de Capilla de la Catedral de Puebla en el siglo XVII, compuso una enorme cantidad de villancicos en náhuatl solo, o náhuatl y español; varios de ellos se encuentran en el “Cancionero Musical de Gaspar Fernandes”, importante documento conservado en el Archivo Musical de la Catedral de Oaxaca.

Xochipitzahuac es la canción más popular en el mundo náhuatl. El canto a la virgen de Guadalupe se remonta al periodo colonial novohispano, se desconoce el autor pero se cree que esta canción ha sido de dominio popular, según investigaciones dicen que Xochipitzahuac se comenzó a cantar después de las apariciones de la Virgen María. En la actualidad la canción se sigue cantando en honor a la virgen de Guadalupe, diversas agrupaciones la han cantado como un huapango o con acompañamiento instrumentos cuerda (guitarras, bandurrias o violines).

En la actualidad, el "Coro Niño Jesús" (originario de Altepexi, Puebla), ha comenzado a grabar canciones tradicionales en náhuatl como Chokani (La Llorona) en forma de coro.

Recientemente el náhuatl ha sido estudiado y trabajado por las tecnologías de la información y en concreto por el Procesamiento de lenguajes naturales. La principal idea detrás de estas aplicaciones de la inteligencia artificial al lenguaje es poder acceder a herramientas comunes para los lenguajes más usados, cómo son traductores automáticos Traducción automática, correctores ortográficos y gramaticales, Búsqueda de respuestas, Reconocimiento del habla y Síntesis de habla, entre muchos otros. Hasta el momento el náhuatl cuenta con un traductor automático, un corpus paralelo entre el idioma y el español y un segmentador morfológico.

La convivencia de cinco siglos entre el náhuatl y el español en México ha tenido un impacto en ambas lenguas. La influencia del náhuatl en el español se refleja especialmente en la gran cantidad de préstamos léxicos que el español ha tomado del náhuatl. En menor medida el español mexicano presenta marginalmente algunos rasgos fonéticos en parte atribuibles al náhuatl, incluyendo la africada lateral alveolar sorda t͡ɬ, pronunciada en el dígrafo tl. La influencia en la gramática en el español mexicano es todavía menos clara, pero se ha argumentado que podría haber influido en la frecuencia de ciertas construcciones y tendencias ya presentes en el español general.

Por otra parte el amplio bilingüismo náhuatl-español entre los hablantes de náhuatl ha influido en el náhuatl, tanto a nivel léxico como a nivel gramatical también.

Varias lenguas de Filipinas han adoptado numeros nahuatlismos del español, especialmente nombres de plantas.

En la gramática, uno puede citar como influencia del náhuatl el uso del sufijo -le para darle un carácter enfático al imperativo. Por ejemplo: "brinca -> bríncale, come -> cómele, pasa -> pásale, etcétera". Se considera que este sufijo es un cruce del pronombre de objeto indirecto español le con las interjecciones excitativas nahuas, tales como cuele. Sin embargo, este sufijo no es un verdadero pronombre de objeto indirecto, ya que se usa aún en construcciones no verbales, tales como: "hijo -> híjole, ahora -> órale, que hubo -> quihúbole, etcétera."

La RAE acepta alrededor de 200 préstamos del náhuatl al español, incluyendo:

acocil, aguacate, ahuehuete, ajolote, amate, atole, ayate,
cacahuate, camote, canica, capulín, chalmichi, chamagoso, chapopote, chapulín,
chayote, chicle, chile, chipotle,
chocolate, comal, copal,
coyote, cuate, ejote, elote, epazote, escuincle,
guacamole, guajolote, huachinango, huipil, hule, jacal, jícama,
jícara, jitomate, macana, mecate, mezcal, milpa, mitote, mole,
nopal, ocelote, ocote, olote, papalote,
pepenar, petaca, petate, peyote, pinole, piocha, popote, tlalcoyote, pilcate, quetzal, tamal, tecolote, tejocote, tianguis, tiza, tomate,
tule, zacate, zapote, zopilote.

Además ha donado un sinfín de topónimos, incluyendo "México" (Mēxihco), "Guatemala" (Cuauhtemallān).

Las variedades modernas de náhuatl muestran diferente grado de impacto por el español. La influencia del español se refleja especialmente en el préstamo masivo de preposiciones, conjunciones y nexos del español. Esto ha generado una reestructuración de ciertas partes de la sintaxis especialmente en el orden sintáctico y en el uso de ciertas construcciones. También el viejo sistema de cuenta en base 20 ha sido abandonado en favor del sistema decimal del español, por lo que sólo se usan los nombres nativos de números para números inferiores a diez o veinte.

También la generalización del uso del numeral "ce" '1' como equivalente del español 'un(o), una' es notorio en algunas variantes de náhuatl. Y también naturalmente se da una influencia del léxico del español, especialmente para designar realidades tecnológicas nuevas o términos un tanto técnicos para el modo de vida rural de la mayoría de hablantes.

Entre los préstamos del español al náhuatl son los siguientes: "axno" (< asno), "cahuayoh" (< caballo), "cafetzin" (< café), "col", "hicox" (< higo), "huino" (< vino), "limon" (limón), "melon", "manzana", perexil (< perejil), "rahuano" (< rábano), "torazno" (< durazno), etc.







</doc>
<doc id="8257" url="https://es.wikipedia.org/wiki?curid=8257" title="Neurociencia">
Neurociencia

La neurociencia es un campo de la ciencia que estudia el sistema nervioso y todos sus aspectos; como podrían ser su estructura, función, desarrollo ontogenético y filogenético, bioquímica, farmacología y patología; y de cómo sus diferentes elementos interactúan, dando lugar a las bases biológicas de la cognición y la conducta.

El estudio biológico del cerebro es un área multidisciplinar que abarca muchos niveles de estudio, desde el puramente molecular hasta el específicamente conductual y cognitivo, pasando por el nivel celular (neuronas individuales), los ensambles y redes pequeñas de neuronas (como las columnas corticales) y los ensambles grandes (como los propios de la percepción visual) incluyendo sistemas como la corteza cerebral o el cerebelo, e incluso, el nivel más alto del sistema nervioso.

En el nivel más alto, las neurociencias se combinan con la psicología para crear la neurociencia cognitiva, una disciplina que al principio fue dominada totalmente por psicólogos cognitivos. Hoy en día, la neurociencia cognitiva proporciona una nueva manera de entender el cerebro y la consciencia, pues, se basa en un estudio científico que une disciplinas tales como la neurobiología, la psicobiología o la propia psicología cognitiva, un hecho que con seguridad cambiará la concepción actual que existe acerca de los procesos mentales implicados en el comportamiento y sus bases biológicas.

Las neurociencias ofrecen un apoyo a la psicología con la finalidad de entender mejor la complejidad del funcionamiento mental. La tarea central de las neurociencias es la de intentar explicar cómo funcionan millones de neuronas en el encéfalo para producir la conducta, y cómo a su vez, estas células están influidas por el medio ambiente. Tratando de desentrañar la manera de cómo la actividad del cerebro se relaciona con la psiquis y el comportamiento, revolucionando la manera de entender nuestras conductas y lo que es más importante aún: cómo aprende, cómo guarda información nuestro cerebro y cuáles son los procesos biológicos que facilitan el aprendizaje.

Es cierto que las formas espontáneas de desarrollo parecen una condición necesaria para las formas de funcionamiento cognoscitivo, pero no son condición suficiente. Existen nomenclaturas psicopatológicas, hoy aplicadas a los niños, que puede llevar a la medicalización de la infancia. 

Algunos de los problemas aún no resueltos de la neurociencia son:


Las neurociencias exploran campos tan diversos como:

Entre las áreas relacionadas con la neurociencia se encuentran:

En 1791 Luigi Galvani, un fisiólogo de Bolonia, descubrió la existencia de actividad eléctrica en los animales. Había colgado la pata de una rana en un gancho de cobre suspendido de un balcón de hierro. La interacción entre los dos metales hacía que la pata se contrajera.

El fisiólogo llamó a esta forma de producir energía "bioelectrogénesis". A través de numerosos y espectaculares experimentos —como electrocutar cadáveres humanos para hacerlos bailar la "danza de las convulsiones tónicas"— llegó a la conclusión de que la electricidad necesaria no provenía del exterior, sino que era generada en el interior del propio organismo vivo, que, una vez muerto, seguía conservando la capacidad de conducir el impulso y reaccionar a él consecuentemente

Hermann von Helmholtz descubrió que la generación de electricidad por parte de los axones de las células nerviosas no es un producto secundario de su actividad, sino un medio para transmitir mensajes de un extremo a otro. Logró medir, en 1859, la velocidad de propagación de tales mensajes, y llegó a la conclusión de que se propagan a 27 metros por segundo.

Camillo Golgi desarrolló un método de tinción con cromato de plata, que permite colorear una neurona entre muchas otras. Compartió el Premio Nobel de Medicina de 1906 con Santiago Ramón y Cajal.

Santiago Ramón y Cajal dio a la célula nerviosa el nombre de neurona, unidad elemental del sistema de señalización del sistema nervioso. Descubre que el axón de una neurona solo se comunica con las dendritas de otra en regiones especializadas: las sinapsis. Además, una neurona determinada solo se comunica con ciertas células, y no con otras. En el interior de la neurona, las señales fluyen en una dirección única. Este principio permite determinar el flujo de la información en los circuitos neurales. Encontró que existen tres tipos principales de neuronas: sensorial, motora e interneurona.

Charles Sherrington estudió los fundamentos neurales del comportamiento reflejo. Descubrió que es posible inhibir las neuronas además de excitarlas, y que la integración de esas señales determina la acción del sistema nervioso.

Edgar Adrian ideó métodos para registrar los potenciales de acción, que son las señales eléctricas utilizadas por las neuronas para la comunicación. Descubre que son señales de tipo "todo o nada", es decir, o bien se presentan completas o bien no se presentan en absoluto. Compartió el Premio Nobel de Medicina con Charles Sherrington.

Julius Bernstein, discípulo de Wilhelm Helmholtz, propuso en 1902 la hipótesis de la membrana porosa para describir el proceso de conducción eléctrica en las neuronas. Dedujo que hay una diferencia de potencial entre el interior y el exterior de la célula nerviosa, incluso cuando la célula está en reposo.

Alan Hodgkin y Andrew Huxley desarrollaron investigaciones sobre el axón gigante de las células nerviosas de los calamares. Confirman la hipótesis de Julius Bernstein de que el potencial de membrana en reposo se genera por el desplazamiento de iones de potasio hacia el exterior de la célula y de iones de sodio hacia su interior. Compartieron el Premio Nobel de Medicina de 1963 con John Eccles, por la investigación sobre las bases iónicas de la transmisión nerviosa.

Henry Dale y Otto Loewi propusieron la teoría química de la transmisión sináptica. Descubrieron, en forma independiente, que cuando el potencial de acción de una neurona del sistema nervioso autónomo llega a los terminales del axón, causa la liberación de una sustancia química en la hendidura sináptica. Recibieron el Premio Nobel de Medicina de 1936.

Edwin Furshpan y David Potter descubrieron, en una langosta de río, que también es posible la transmisión eléctrica entre dos células nerviosas, si bien la mayoría de las sinapsis son de origen químico.

Bernard Katz descubrió que cuando un potencial de acción ingresa en la terminal presináptica causa la apertura de los canales de calcio, lo que permite la afluencia de este elemento químico al interior de la célula. La abundancia de calcio, a su vez, determina la liberación de los neurotransmisores en la hendidura sináptica. El neurotransmisor se une a los receptores superficiales de la neurona postsináptica, y las señales químicas se retraducen a señales eléctricas. Compartió el Premio Nobel de Medicina de 1970 con Ulf von Euler y Julius Axelrod por los estudios realizados sobre neurotransmisores.

Rodolfo Llinás cambió el dogma establecido desde que Ramón y Cajal enunció su ley de la polarización sobre el aspecto funcional de las neuronas. Rodolfo Llinás presentó el nuevo punto de vista funcional sobre la neurona en su artículo "". Rodolfo Llinás con sus colaboradores investigó durante los años 80 el funcionamiento electrofisiológico de las neuronas en los vertebrados, descubriendo las propiedades electrofisiológicas. Anteriormente se habían observado propiedades intrínsecas en los invertebrados pero se pensaba que éstas eran únicamente una cuestión relativa a esa línea, pero Llinás y sus colaboradores demostraron que las neuronas de los vertebrados tienen propiedades electrofisiológicas intrínsecas. El nuevo punto de vista funcional sobre la neurona quedó resumido en lo que hoy es conocido por la Ley de Llinás.

El descubrimiento de cada sustancia química considerada mediadora de la intercomunicación neuronal aportaba nuevos elementos de conocimiento de la compleja red de conexiones entre células nerviosas y de sus correspondientes características funcionales.

Eric Kandel esclareció el papel de los transmisores en el complejo proceso de la memoria y el aprendizaje, estableciendo que la memoria es evocada por cambios directos en los millones y millones de sinapsis que forman los puntos de contacto entre las neuronas.

Antonio Alcalá Malavé consiguió en 2002 descubrir que las áreas cerebrales 17,18 y 19 de Brodman servían además de para inducir el fenómeno físico y químico de la visión, para informar del riesgo cardiovascular y algunas demencias. Ese "informe biológico" se traduce como fallo visual en la calidad, cantidad, color y contraste de las imágenes que llegaban al cerebro o que eran procesadas por el mismo aunque ya aberradas. Sus trabajos son verificables por campimetra computarizada y análisis computarizado cromático.

Roderick MacKinnon obtuvo en 2004 la primera imagen tridimensional de los átomos que forman la proteína de los dos canales iónicos: un canal pasivo de potasio y un canal de potasio activado por voltaje. Recibió el Premio Nobel de Química.

En 2014 los psicólogos y neurocientíficos noruegos Edvard Moser y su esposa May-Britt Moser compartieron con el británico John O’Keefe el Premio Nobel de Fisiología o Medicina por sus estudios sobre las células de lugar del hipocampo: una clase de neuronas que codifica la ubicación espacial en la que se encuentran los mamíferos como las ratas y los seres humanos, y les permiten orientarse en el espacio. Ciertos grupos de neuronas hipocampales se activan o no, dependiendo del lugar de una habitación en el que un sujeto se encuentre en un momento determinado.

Además de la secuencia histórica asociada a la neurona y a los conjuntos neuronales, es posible seguir la evolución de las neurociencias considerando la secuencia histórica de las teorías destinadas a establecer la función de cada sector del cerebro, o bien la consideración de que no existiría una locación concreta de las funciones cerebrales.

El neurólogo alemán Franz Joseph Gall (1758-1828) desarrolló el sistema frenológico, mediante el cual cada facultad psíquica tendría su asiento en determinado grupo de células cerebrales. Así, toda la corteza cerebral estaría constituida por "órganos" distintos.

El fisiólogo francés Pierre Flourens efectuaba la ablación de partes del cerebro de animales y estudiaba su conducta. De manera que, según lo que los animales dejaban de hacer, podía inferir las funciones de la parte extraída. Observó que con el tiempo se restablecía la función original, con independencia de la parte dañada.

Luego del fallecimiento de un paciente con trastornos en el lenguaje, el neurólogo y antropólogo francés Paul Broca estudió su cerebro y encontró una lesión en el tercio posterior de la circunvolución frontal inferior del hemisferio izquierdo. Estudió a otros pacientes con problemas similares y encontró las mismas lesiones en la ahora denominada "área de Broca". Este especialista llegó a afirmar: "Nosotros hablamos con el hemisferio izquierdo".

Carl Wernicke descubrió la que ahora se denomina "área de Wernicke", zona del cerebro cuyas lesiones producen perturbaciones en la comprensión del habla. Sus descubrimientos, junto a los de Paul Broca, estimularon los estudios localizacionistas durante el siglo XIX.

Walter Rudolf Hess descubrió la organización funcional del cerebro medio como coordinador de las actividades de los órganos internos. Empleando estimulación eléctrica en ciertas zonas del mesencéfalo, Hess pudo reproducir funciones autónomas espontáneas, modificaciones en la respiración o la circulación, entre otras respuestas.

Los estudios de Roger W. Sperry permitieron determinar que, aunque cada uno de los dos hemisferios del cerebro (izquierdo y derecho) intercambia información con el otro a través del cuerpo calloso y otras comisuras más pequeñas, existen notables diferencias en la forma de procesamiento de la información entre uno y otro.

David H. Hubel y Torsten Wiesel descubrieron las características del procesamiento de la información visual. Estudiando su desarrollo en gatos pequeños, detectaron la capacidad de las neuronas corticales para reorganizarse ante situaciones de privación sensorial y determinaron que la reorganización de las neuronas corticales ocurre solo en periodos determinados.


12. La relación ética, neurociencias y Derecho.J.F.Martinez, 2017-12 





</doc>
<doc id="8258" url="https://es.wikipedia.org/wiki?curid=8258" title="Niels Ryberg Finsen">
Niels Ryberg Finsen

Niels Ryberg Finsen (Tórshavn, Islas Feroe, 15 de diciembre de 1860 - Copenhague, 24 de septiembre de 1904) fue un médico danés, conocido por haber descubierto el efecto germicida de la luz ultravioleta. Recibió el Premio Nobel en 1903.

En 1882, viaja a Copenhague para estudiar medicina en su universidad, aprobando su examen final en 1890. Posteriormente sería profesor en dicha universidad.

Los hallazgos de Émile Duclaux sobre la capacidad destructora de los rayos ultravioleta, aplicados a colonias de bacterias fueron la base de su trabajo. Alcanzó la fama por sus investigaciones sobre los efectos fisiológicos de la luz, ya que descubrió las propiedades estimulantes y bactericidas de los rayos actínicos (azul, violeta y ultravioleta). Desarrolló una lámpara eléctrica de arco voltaico (luz de Finsen) para el tratamiento del lupus tuberculoso y otras afecciones cutáneas similares. 

Con objeto de poder continuar su trabajo y las aplicaciones de sus métodos fototerapéuticos se creó en Copenhague el Instituto Finsen en 1896.

Su enfermedad le impidió en 1903 recoger el Premio Nobel de Medicina que le fue otorgado.

Entre sus publicaciones más conocidas figuran una obra destinada a explicar la teoría general del efecto de la luz sobre el organismo vivo (1895) y otra sobre el empleo médico de los rayos de luz química concentrados ("Om Anvendelse in Medicinen af koncentrerede kemiske lysstraaler", 8 vv. 1896).




</doc>
<doc id="8259" url="https://es.wikipedia.org/wiki?curid=8259" title="Charles Louis Alphonse Laveran">
Charles Louis Alphonse Laveran

Charles Louis Alphonse Laveran (París, 18 de junio de 1845 - ibíd. 18 de mayo de 1922) fue un médico y naturalista francés, ganador del premio Nobel de Medicina en 1907 por su descubrimiento de los parásitos protozoarios como agente causal de la malaria. Fue el primer científico francés en obtener el premio Nobel. 

Alphonse Laveran nació en el Boulevard Saint-Michel en París. Sus padres fueron Louis Théodore Laveran y Marie-Louise Anselme Guénard de la Tour Laveran, quienes tuvieron dos hijos. Su familia tuvo ambiente militar. Su padre fue médico del ejército y profesor de medicina militar en la École de Val-de-Grâce. Su madre fue hija de un comandante del ejército. Siendo muy joven su familia fue a Argelia (en esa época colonia del Imperio Francés), acompañando a su padre en su servicio.

Fue educado en París, y completó su educación en "Collège Saint Barbe". Siguió a su padre en la medicina militar ingresando a "Public Health School" en Estrasburgo en 1863. En 1866 fue estudiante médico residente en los hospitales civiles de Estrasburgo. En 1867, presentó su tesis basada en la regeneración de los nervios con la cual obtuvo el título de médico de la Universidad de Estrasburgo.

Laveran fue médico Asistente Mayor en el ejército francés durante la Guerra Franco-Prusiana. Estuvo apostado en Metz, donde el ejército francés sería derrotado y la plaza ocupado por los alemanes. Trabajó en el hospital de Lille y en el Martin Hospital (ahora St Martin's House) en París.

En 1874 calificó en un examen de competencia para posteriormente ser nombrado "Chair of Military Diseases and Epidemics at the École de Val-de-Grâce", una posición que su padre había ocupado. Su tenencia terminó en 1878 y se fue a Argelia, donde permaneció hasta 1883. De 1884 a 1889 fue Profesor "Military Hygiene at the École de Val-de-Grâce". (Profesor de Higiene Militar en la escuela de Val-du-Grâce).

En 1894 asumió el cargo de "Chief Medical Officer of the military hospital at Lille and then Director of Health Services of the 11th Army Corps" (Director Médico Oficial del hospital Militar en Lille y Director de los Servicios de Salud el 11° Cuerpo del Ejército en Nantes. Por ese tiempo fue promovido al rango de "Principal Medical Officer of the First Class" (Oficial Médico Principal de Primera Clase). En 1896 fue Director del Instituto Pasteur como jefe honorario de tiempo completo en las enfermedades tropicles.


Laveran se casó con Sophie Marie Pidancet en 1885. No tuvieron hijos.
En 1922 sufrió un enfermedad indefinida por algunos meses y murió en París. Está enterrado en el Cimetière du Montparnasse en París. Fue ateo.






</doc>
<doc id="8260" url="https://es.wikipedia.org/wiki?curid=8260" title="Emil Theodor Kocher">
Emil Theodor Kocher

Emile Theodor Kocher (Burgdorf, Cantón de Berna; 25 de agosto de 1841-Berna, 27 de julio de 1917) fue un médico suizo galardonado con el Premio Nobel de Medicina en 1909 por sus trabajos sobre los tratamientos sobre las afecciones de la glándula tiroides. 

Kocher estudió en Berna, París, Berlín y Londres, obteniendo el doctorado en Berna en 1865. En 1872 sucedió a Georg Albert Lucke como profesor de cirugía de la Universidad de Berna y director de la Clínica Universitaria Quirúrgica de Berna, cargos que ocupó hasta 1911.

Publicó trabajos relativos a la glándula tiroides (ideando un método para su trasplante), tratamientos antisépticos, infecciones quirúrgicas, heridas de bala y osteomielitis aguda. Descubrió un método para reducir la luxación del hombro e introdujo nuevas técnicas en las operaciones de hernia y de cáncer de estómago. Realizó mejoras técnicas a numerosos tipos de operaciones quirúrgicas del pulmón, el estómago y la vesícula biliar.

En 1878 practicó con éxito la primera extirpación del bocio, operación que tuvo ocasión de repetir más de 2000 veces dado que vivía en una de las zonas más bociógenas del mundo. En 1912 preparó un coaguleno esterilizado que, inyectado, aumenta la coagulación de la sangre y sirve para prevenir y tratar hemorragias internas. 

Sus éxitos en el campo de la cirugía fueron debidos en buena parte por la mejora del instrumental y por la utilización sistemática de la asepsia. Se le debe la invención de las pinzas hemostáticas de grapas que llevan su nombre. Considerado como el mejor cirujano de su época, fue elegido presidente del Primer Congreso Internacional de Cirugía (1905) 

En 1909 se le otorgó el Premio Nobel de Medicina por sus trabajos sobre la fisiología, patología y cirugía de la glándula tiroides. Con el importe del premio ayudó a fundar el Instituto Kocher en Berna, escuela de cirugía en la que se formaron grandes cirujanos. 




</doc>
<doc id="8263" url="https://es.wikipedia.org/wiki?curid=8263" title="Pavlov">
Pavlov

Pavlov (o "Pávlov") es un apellido ruso, cuya forma femenina es Pavlova (o "Pávlova"). Puede hacer referencia a:


También, puede referirse a:

</doc>
<doc id="8266" url="https://es.wikipedia.org/wiki?curid=8266" title="Síndrome de Kocher">
Síndrome de Kocher

El síndrome de Kocher-Debre-Semelaigne es una enfermedad rara de niños caracterizada por hipotiroidismo moderado o severo de larga duración asociado a una pseudohipertrofia muscular o aumento de la masa muscular por aumento de volumen de los tejidos intersticiales pero con atrofia de células musculares con la subsecuente miotonia.

Aunque el síndrome fue reportado inicialmente por Emil Theoro Kocher (1892) fueron Rober Debré y George Semelaigne quienes lo describieron más ampliamente en 1935 destacando la asociación entre el problema endocrino y el trastorno muscular.

Los pacientes, generalmente niños entre los 18 meses y los 10 años, presentan signos clínicos del hipotiroidismo o manifestaciones típicas de cretinismo, incluyendo: disminución de la actividad y aumento del sueño, dificultad en la alimentación y estreñimiento, ictericia prolongada, facies mixedematosa, fontanelas grandes (especialmente las posteriores), macroglosia, abdomen distendido con hernia umbilical, e hipotonía. La pseudohipertrofia afecta a los músculos de extremidades, cinturas, tronco, manos y pies, siendo más prominente en las extremidades lo que conlleva un aspecto atlético del paciente.

Se desconoce la fisiopatología de la afección pero se ha propuesto que la pseudohipertrofia muscular puede ser el resultado de un hipotiroidismo de larga duración. El síndrome puede presentarse con todas las formas de hipotiroidismo.

El diagnóstico se basa en los signos clínicos y en los cambios miopáticos. El tratamiento se basa en suplementos tiroideos con el cual pueden revertirse la pseudohipertrofia y los síntomas clínicos complementado con medidas para la rehabilitación consistentes en terapia física, ejercicio dirigido moderado y terapia ocupacional, entre otros.

En adultos una afección similar se denomina síndrome de Hoffman.



</doc>
<doc id="8272" url="https://es.wikipedia.org/wiki?curid=8272" title="Astarté">
Astarté

Astarté (en fenicio 𐤀𐤔𐤕𐤓𐤕 , "ʾAshtart") es la asimilación fenicia-cananea de una diosa mesopotámica que los sumerios conocían como Inanna, los acadios, asirios y babilonios como Ishtar y los israelitas como Astarot. 

Representaba el culto a la madre naturaleza, a la vida y a la fertilidad, así como la exaltación del amor y los placeres carnales. Con el tiempo, se tornó también en diosa de la guerra y recibió cultos sanguinarios de sus devotos. Se la solía representar desnuda o apenas cubierta con un fino cinturón, de pie sobre un león.

Astarté es equiparada en nombre, origen y funciones con la diosa Ishtar de los textos de la Mesopotamia. Otra transliteración es ’Ashtart.


Todas ellas estaban identificadas invariablemente con el lucero del alba o planeta Venus, Azzuhara en árabe; Ahura Mazda sería: 'luz grande' .

De acuerdo con el libro "The Early History of God", Astarté sería la encarnación correspondiente a la Edad de Hierro (después del 1200 a. C.) de la diosa Ashera, de la Edad de Bronce (antes del 1200 a. C.).

Las diosas Astarté e Ishtar y equivalentes, están relacionadas con el planeta Venus. Son cognados del nombre hebreo Estēr (Esther).

Pero aunque suenen parecido cuando las pronunciamos a la manera occidental, no se parecen ni tienen relación etimológica con las palabras de raíz indoeuropea que equivalen al castellano "estrella", y que son cognadas entre sí: el latín "stella" y sus derivados romances (el francés "étoile", el español "estrella", el rumano "stea"...), el inglés "star", el alemán "stern", el sueco "stjärna", el griego clásico "astér" (moderno "astéras"), y el persa y hindi "setareh".




</doc>
<doc id="8273" url="https://es.wikipedia.org/wiki?curid=8273" title="RAE (desambiguación)">
RAE (desambiguación)

El acrónimo RAE puede referirse a varios artículos:



</doc>
<doc id="8275" url="https://es.wikipedia.org/wiki?curid=8275" title="Ishtar">
Ishtar

Ištar o "Ishtar" era la diosa babilónica del amor y la belleza, de la vida, de la fertilidad. Se asociaba principalmente con la sexualidad: su culto implicaba la prostitución sagrada; la ciudad sagrada Uruk se llamaba la "ciudad de las cortesanas sagradas", y ella misma fue la "cortesana de los dioses". Ištar tenía muchos amantes; sin embargo, como señala Guirand: 

Incluso para los dioses el amor de Ištar fue fatal. En su juventud la diosa había amado a Tammuz, dios de la cosecha y, de acuerdo con la epopeya de Gilgamesh, este amor causó la muerte de Tammuz. 

Se asocia en otras regiones con diosas como Inanna en Sumeria, Anahit en la antigua Armenia (Urartu), Astarté (Asera) en Canaán, Fenicia y en las religiones abrahámicas. Ištar, Inanna y estas diosas representan el arquetipo de la diosa madre.

En Sumeria era conocida como Inanna (siendo dos diosas distintas que representan lo mismo) y posteriormente en Babilonia, y en su zona de influencia cultural en todo Oriente Medio, recibe los títulos honoríficos de Reina del Cielo y Señora de la Tierra.

Para Joseph Campbell Ištar/Inanna, que amamanta al dios Tammuz, es la misma diosa que Afrodita y que la egipcia Isis, que alimenta a Horus.

Hija de Sin, dios de la Luna, y Nannar, la Luna. Hermana menor de Ereškigal y hermana gemela de Šamaš, en sumerio Utu, dios del Sol. Compañera de Tammuz, en sumerio Dumuzi.

Su número asociado en el panteón de la mitología mesopotámica es el 15.

Como primer arquetipo psicológico de la dinámica femenina en la historia, y en contraposición a su hermana Ereškigal o a Ki, la diosa de la tierra, Ištar no se puede considerar dentro del grupo de las diosas madre, puesto que su relación con los humanos es más como inspiración para la acción vital que como refugio. Con este carácter, Ištar aparece en la epopeya de Gilgamesh.

Se la asocia al planeta Venus, estrella de la mañana y del anochecer. Su símbolo es una estrella de ocho puntas. En su honor, los astrónomos han llamado Ishtar Terra a un continente de Venus. Su animal asociado es el león.

Ishtar era hija de Sin (dios lunar) o de Anu. En carácter de hija de aquel, era la dama bélica; como descendiente de éste, el exponente del amor, la licenciosidad y la intemperancia y la violencia caprichosa hasta el extremo. 

Bajo el aspecto guerrero se le rendía culto en Agadé y en Sippar, con el nombre de Anunit. También tiene un carácter astral, ya que personifica a varios astros: a Venus, al Sol, la Luna, y a las estrellas reunidas en constelaciones. 

Ishtar estaba asociada al planeta Venus como estrella de la mañana, y en las fronteras de Babilonia se la representaba mediante una estrella de ocho puntas. También, de pie, completamente desnuda, con las manos encima del vientre, o sosteniéndose los senos, o blandiendo un arco sobre un carro tirado por leones. 

En su aspecto de divinidad amorosa, Isthar es la protectora de las prostitutas y de los amoríos extramaritales, que por cierto no tenían connotación especial en Babilonia, ya que el matrimonio era un contrato solemne que perpetuaba la familia como sostén del Estado y como generadora de riquezas, pero en el que no se hablaba de amor o de fidelidad amorosa. 

Ištar no es una diosa del matrimonio, ni es una diosa madre. El matrimonio sagrado o la sacra hierogamia, que se representaba todos los años en el templo babilónico, no tiene un implicación moral ni es modelo de matrimonios terrestres, es un rito de fertilidad altamente estilizado con tonos litúrgicos. 

Su versión sumeria, Inanna, fue muy venerada a partir del reinado de Sargón.

También en la Biblia, en el libro del profeta Jeremías, se le nombra en el capítulo 44 refiriéndose a ella como La Reina del Cielo.

Ishtar recibió culto en el templo babilónico llamado E.tur.kalam.ma. En 1778 a. C. Hammurabi construyó un trono para hacer culto a Ishtar, y en 1775 a. C. confeccionó imágenes para esta misma.

Su primer esposo fue su hermano Tammuz. Al morir Tammuz, Ishtar descendió a los infiernos para arrancarle a su hermana, la terrible Ereškigal, el poder sobre la vida y la muerte.

Después de darle instrucciones a su sirviente Papsukal, de ir a rescatarla si no regresaba, descendió a la tierra de las tinieblas, Irkalla. Comenzó valiente y desafiante, gritando al portero que abriera la puerta antes de que la echase abajo. Pero en cada una de las siete puertas era despojada de una de sus prendas, y con ellas se iba despojando de su poder, hasta que llegó desnuda e indefensa ante Ereškigal, que la mató y colgó su cuerpo en un clavo. 

Con su muerte, todo el mundo comenzó a languidecer. Pero el fiel Papsukal llegó hasta los dioses y les pidió que creasen un ser capaz de entrar en el mundo de los muertos y resucitase a Ishtar con la comida y el agua de la vida. Así es como Ishtar volvió a la vida, pero tenía que pagar el precio: durante seis meses al año, Tammuz debe vivir en el mundo de los muertos. Mientras está allí, Isthar ha de lamentar su pérdida; en primavera, vuelve a salir y todos se llenan de gozo.

Algunos consideran a esta leyenda como el origen de la llamada "Danza de los siete velos".

Existen otros nombres relacionados con esta diosa: Astarté, Astaroth, Esther y Stára (en persa).



</doc>
<doc id="8277" url="https://es.wikipedia.org/wiki?curid=8277" title="Estrecho de Malaca">
Estrecho de Malaca

El estrecho de Malaca es un largo estrecho de mar del sudeste de Asia localizado entre la costa occidental de la península malaya y la isla indonesa de Sumatra, un importante corredor marítimo que une, al norte, el mar de Andamán, mar marginal del océano Índico, y al sur el mar de la China Meridional.

Se entiende que el límite occidental del océano Pacífico está en el estrecho de Malaca.

El estrecho se extiende en dirección SE-NO y tiene aproximadamente 800 km de longitud, con una anchura entre 50 km y 320 km. Tiene sólo 2,8 km de ancho en su punto más angosto, el estrecho de Philips, en el estrecho de Singapur. En su parte media se encuentra su mínima profundidad la que condiciona el calado de los buques que lo atraviesan (proximidades de Port Kelang, "One Fathom Bank"). En la parte sureste, el estrecho comunica con el estrecho de Singapur y está cerrado por varias islas del grupo del archipiélago de Riau que permiten la navegación por varios canales de paso.

Ha adquirido un importante papel estratégico, siendo la principal vía de abastecimiento de petróleo de dos de los principales consumidores mundiales, Japón y China. En promedio, 150 barcos pasan a diario a través del estrecho, que es una ruta de navegación importantísima, ya que vincula todo el mar de la China Meridional con el océano Índico y con Europa vía canal de Suez.
Los puertos más importantes son Malaca (Malasia) y Singapur, en el extremo meridional de este estrecho, uno de los más grandes del mundo en cuanto a volumen de carga anual, aunque Singapur está a orillas de otro estrecho independiente, el estrecho de Singapur.

El estrecho de Malaca recibe el nombre por la ciudad que se encuentra a sus orillas.

Desde una perspectiva económica y estratégica, el estrecho de Malaca es una de las rutas de navegación más importantes del mundo. El estrecho es el principal canal de transporte marítimo entre el océano Índico y el océano Pacífico, y une las principales economías asiáticas como India, China, Japón y Corea del Sur. Más de 50.000 buques pasan por el estrecho por año, llevando aproximadamente una cuarta parte del comercio mundial de mercancías como el petróleo, manufacturas chinas, café indonesio.

Alrededor de una cuarta parte del total de petróleo transportado por mar pasa a través del estrecho, principalmente de los proveedores del golfo Pérsico a los mercados asiáticos como China, Japón y Corea del Sur. En 2006, se estima que 15 millones de barriles por día (2.400.000 m³/d) fueron transportados a través del estrecho.

El tamaño máximo de los buques que pueden hacer la travesía del estrecho se denomina Malaccamax. El estrecho no es lo suficientemente profundo (unos 25 metros o 82 pies) para permitir que algunos de los más grandes buques (la mayoría de los petroleros) lo utilicen. En el canal de Phillips, cerca de Singapur, el estrecho de Malaca se reduce a 2,8 km de anchura, formando uno de los estrangulamientos de tráfico más importantes del mundo. Desde la erupción del Krakatoa en 1883, la navegación de grandes buques por el estrecho de Sonda no es segura, por lo que un buque que exceda Malaccamax debe utilizar como alternativa el estrecho de Lombok.

La piratería en el estrecho ha aumentado en los últimos años. Hubo alrededor de 26 ataques contra buques en 1994, 220 en 2000, y poco más de 150 en 2003 (un tercio del total mundial). Después, los ataques aumentaron de nuevo en el primer semestre de 2004, lo que llevó a que las armadas de Malasia, Indonesia y Singapur intensificaran sus patrullas en la zona en julio de 2004. Posteriormente, los ataques a buques en el Estrecho de Malaca ha caído, a 79 en 2005 y 50 en 2006.

En el estrecho hubo 34 naufragios documentados, algunos que datan de la década de 1880, recogidos en el Plan de Separación de Tráfico («Traffic Separation Scheme», TSS), el canal para buques comerciales. Estos pecios plantean un riesgo de colisión en los estrechos y zonas de aguas superficiales.

Otro riesgo es la neblina anual causada por los incendios forestales que asolan Sumatra, que puede reducir la visibilidad a 200 m, lo que obliga a que los buques ralenticen su discurrir por el concurrido estrecho. Los buques de más de 350 m utilizan de forma habitual el estrecho.

Tailandia ha elaborado varios planes para disminuir la importancia económica del estrecho. El gobierno tailandés, a lo largo de su historia, ha propuesto varias veces cortar un canal a través del istmo de Kra, con un ahorro de aproximadamente 960 km de viaje desde el océano Índico hasta el Pacífico. Esto también cortaría Tailandia en dos, aislando las provincias de mayoría musulmana del sur, Narathiwat, Yala y en especial la separatista Pattani. China se ha ofrecido a sufragar los gastos, según un informe filtrado al diario estadounidense "The Washington Times" en 2004. Sin embargo, y pese al apoyo de varios políticos de Tailandia, los prohibitivos costos financieros y ecológicos sugieren que el canal no seguirá adelante.

Una segunda alternativa es la construcción de un oleoducto a través del istmo para transportar petróleo a los buques que esperarían al otro lado. Los defensores dicen que reduciría el costo de la entrega de petróleo a Asia en cerca de 0,50 $/barril (3 $/m³). Myanmar también ha hecho una propuesta similar de oleoductos. Existe también una propuesta para un oleoducto de petróleo crudo desde Oriente Medio hasta Xinjiang (China), cuya construcción comenzó en octubre de 2004.

Los primeros comerciantes que procedían de Egipto, Roma, Arabia, África, Turquía, Persia e India, utilizaban el estrecho para llegar al estado malayo de Kedah, antes de llegar a Cantón. Kedah sirvió como puerto occidental de la península malaya. Estos comerciantes navegaban a Kedah, entre junio y noviembre, gracias a los vientos del monzón. Regresaban entre diciembre y mayo. Kedah proporcionaba alojamiento, entibadores, pequeñas embarcaciones, balsas de bambú, elefantes, y también facilitaba la recaudación de tasas por los bienes que eran transportados por tierra hacia los estados orientales de la península Malaya, como Kelantan. Los buques llegaban desde China para comerciar a estos puestos y puertos orientales. Kedah y Funan eran ya puertos famosos en el siglo VI, antes de que las expediciones comerciales comenzaran a utilizar el estrecho de Malaca como ruta marítima.

La máxima autoridad internacional en materia de delimitación de mares a efectos de navegación marítima, la IHO (International Hydrographic Organization: Organización Hidrográfica Internacional), considera el estrecho de Malaca como uno de sus mares, que forma parte del conjunto «estrechos de Malaca y Singapur». En su publicación de referencia mundial, «Limits of oceans and seas» (Límites de océanos y mares, 3.ª edición de 1953), le asigna el número de identificación 46a y lo define de la siguiente manera:



</doc>
<doc id="8279" url="https://es.wikipedia.org/wiki?curid=8279" title="Paul Ehrlich">
Paul Ehrlich

Paul Ehrlich ( ; Strehlen, Silesia; hoy Strzelin, Polonia; 14 de marzo de 1854 – Hamburgo, Imperio alemán; 20 de agosto de 1915) fue un eminente médico y bacteriólogo alemán, ganador del premio Nobel de Medicina en 1908.

Su laboratorio descubrió la arsfenamina (conocida entonces con el nombre de "Salvarsan"), el primer tratamiento medicinal eficaz contra la sífilis, iniciando y dando nombre al concepto de quimioterapia. Ehrlich popularizó en medicina el concepto de "bala mágica", como un producto específico capaz de eliminar por completo un determinado organismo patógeno sin efectos secundarios relevantes.

También hizo una contribución decisiva en el campo de la inmunología con el desarrollo de un suero para combatir la difteria y concibió un método terapéutico para la normalización de estos sueros.

Nacido el 14 de marzo de 1854 en Strehlen (Silesia, en lo que hoy es el suroeste de Polonia), Paul Ehrlich fue el segundo hijo de Rosa (Weigert) y de Ismar Ehrlich. Su padre era un posadero y destilador de licores y el colector de lotería real en Strehelen, un pueblo de unos 5.000 habitantes en la provincia de Baja Silesia, ahora en Polonia. Su abuelo, Heymann Ehrlich, había sido un exitoso destilador y tabernero. Ismar Ehrlich era uno de los dirigentes de la comunidad judía local.

Después de la escuela primaria, Paul Ehrlich asistió a la escuela secundaria "Maria Magdalene Gymnasium" de larga tradición en Breslavia (Breslau, actual Wrocław), donde conoció a Albert Neisser, que más tarde se convirtió en un colega profesional. Siendo todavía un escolar (inspirado por su primo Karl Weigert, que era dueño de uno de los primeros microtomos), quedó fascinado por el proceso de tinción de tejidos mediante sustancias colorantes para su observación microscópica. Conservó ese interés durante sus estudios médicos posteriores en la universidad.

Estudió en la Universidad de Breslavia y más tarde en las de Estrasburgo, Friburgo de Brisgovia y Leipzig, donde acabó sus estudios, doctorándose en 1878 con una tesis sobre la teoría y práctica de la tinción histológica. A él se debe la demostración de la existencia de la barrera hematoencefálica al tintar con anilina la sangre de un ratón y demostrar que esta sustancia no tintaba el cerebro.

Después de obtener un nuevo doctorado en 1882, comenzó a trabajar de ayudante en la clínica de la Universidad de Berlín a las órdenes de Theodor Frerichs, (el fundador de la medicina clínica experimental), centrándose en histología, hematología y tinciones. Fue nombrado profesor auxiliar de la misma en 1889 y al año siguiente catedrático de medicina interna. Fue director del Hospital de la Caridad, en Berlín, donde impulsó el campo de la hematología, desarrollando métodos para la detección y diferenciación de diversas enfermedades de la sangre.

Se casó con Hedwig Pinkus (entonces de 19 años de edad) en 1883. La pareja tuvo dos hijas, Stephanie y Marianne.

Después de completar su formación clínica y habilitación en la prominente escuela de medicina y hospital de enseñanza Charité de Berlín en 1886, Ehrlich viajó a Egipto y otros países en 1888 y 1889, en parte para curar un caso de tuberculosis que había contraído en el laboratorio. A su regreso estableció un consultorio médico privado y un pequeño laboratorio en Berlín-Steglitz.

En 1891, Robert Koch invitó a Ehrlich para que se integrara al personal de su Instituto de Enfermedades Infecciosas en Berlín, del que se derivó en 1896 el Instituto para la Investigación y Ensayo Serológicos ("Institut für Serumforschung und Serumprüfung"), establecido para la especialidad de Ehrlich, quien fue su primer director.

En 1896 fue nombrado director del Real Instituto Prusiano de Investigaciones y Ensayos de Sueros, donde desarrolló diversos métodos de tinción de los tejidos con anilina para estudiar las reacciones microquímicas de las toxinas. Una de sus mayores innovaciones consistió en el uso de diferentes tintes (azules de metileno y de indofenol) como tintes selectivos para diferentes tipos de células. En este sentido, fue el primero en investigar las vías del sistema nervioso, inyectando azul de metileno en las venas de conejos vivos, obteniendo extraordinarios resultados experimentales al tratar con un derivado azoico a animales que sufrían la enfermedad del sueño. En 1904 curó un ratón infectado de tripanosomiasis, inyectándole en la corriente sanguínea el colorante hoy conocido como rojo de trípano.

Inmunología: Su principal contribución a la medicina fue la teoría de la inmunidad de cadena lateral, que establecía la base química para la especificidad de la respuesta inmunológica y que explica cómo los receptores de la parte externa de las células se combinan con toxinas para producir cuerpos inmunes capaces de combatir la enfermedad. Su teoría era que las células tienen en su superficie moléculas receptoras específicas (cadenas laterales) que sólo se unen a determinados grupos químicos de las moléculas de toxina; si las células sobreviven a esta unión, se produce un excedente de cadenas laterales, algunas de las cuales son liberadas a la sangre en forma de antitoxinas circulantes (lo que hoy se denomina anticuerpos). 

Quimioterapia: También hizo importantes aportaciones en el campo de la quimioterapia, que incluyen el descubrimiento -en 1901- del 606 (por ser fruto de 606 experimentos), la que él mismo llamó "bala mágica" o salvarsán (arsfenamina), una preparación de arsénico orgánico empleada en el tratamiento de la sífilis y de la fiebre recurrente, y del neosalvarsán (neoarsfenamina). El neosalvarsán fue conocido durante mucho tiempo como «Ehrlich 914» por tratarse del 914º compuesto preparado por Ehrlich y su ayudante para combatir esas enfermedades.

En 1899 su instituto se trasladó a Fráncfort y se renombró como Instituto de Terapia Experimental ("Institut für experimentelle Therapie "), con Max Neisser como uno de sus colaboradores más importantes. En 1906 Ehrlich se convirtió en el director de la Casa Georg Speyer en Frankfurt, una fundación de investigación privada afiliada a su instituto. Aquí descubrió en 1909 el primer fármaco dirigido contra un patógeno específico: Salvarsan, un tratamiento para la sífilis, que era en ese momento una de las enfermedades infecciosas más letales en Europa. Entre los científicos extranjeros invitados que trabajaron con Ehrlich había dos ganadores del Premio Nobel, Henry Hallett Dale y Paul Karrer. El instituto fue renombrado Instituto Paul Ehrlich en honor de Ehrlich en 1947.

En 1908 compartió el Premio Nobel de Fisiología y Medicina con el bacteriólogo ruso Ilya Mechnikov en reconocimiento al trabajo de ambos en el terreno de la química inmunológica.

En 1914 Ehrlich firmó el controvertido Manifiesto de los Noventa y Tres, que era una defensa de la política militarista de Alemania en la Primera Guerra Mundial. El 17 de agosto 1915 Ehrlich sufrió un ataque al corazón y murió el 20 de agosto en Bad Homburg. El emperador alemán Guillermo II escribió en un telegrama de pésame, "Yo, junto con todo el mundo civilizado, lloro la muerte de este investigador meritorio por su gran servicio a la ciencia médica y la humanidad sufriente; el trabajo de su vida le asegura la fama inmortal y el agradecimiento tanto de sus contemporáneos como de las generaciones de la posteridad."

Paul Ehrlich fue enterrado en el Antiguo cementerio judío, Frankfurt (Bloque 114 N)





</doc>
<doc id="8282" url="https://es.wikipedia.org/wiki?curid=8282" title="Nota (sonido)">
Nota (sonido)

Las notas musicales se utilizan en la notación musical para representar la altura y la duración relativa de un sonido, se suele emplear la acepción «figura musical».

Tras varias reformas y modificaciones, las notas pasaron a ser estas, que se conocen actualmente:


El ejemplo anterior muestra una escala de "do" mayor. Actualmente la escala musical diatónica (sin alteraciones ni cambios en la tonalidad) está compuesta por siete sonidos.
En el caso de la mencionada escala mayor de "do", las notas son las siguientes: 

Los intervalos musicales correspondientes a cada una de las siete notas diatónicas son:

La convención de nomenclatura de nota especifica un monosílabo o bien una letra, cualquier alteraciones y un número de octava. Cualquier nota está a una distancia de un número entero de semitonos del "la" central. ("la") Esta distancia se denota "n". Si la nota está por encima de "la", entonces "n" es positivo, y si está por debajo de "la", entonces "n" es negativo. En el temperamento igual la frecuencia de la nota ("la") es:

Por ejemplo, se puede encontrar la frecuencia de "do", el primer "do" por encima de "la". Hay tres semitonos entre "la" y "do" ("la" → "la" → si → do), y la nota está por encima de "la", por lo que "n" = 3. La frecuencia de la nota será:

Para encontrar la frecuencia de una nota que está por debajo de "la", el valor de "n" es negativo. Por ejemplo, el "fa" por debajo de "la" es "fa". Hay cuatro semitonos ("la" → "la" → "sol" → "sol" → "fa"), y la nota está por debajo de "la", por lo que "n" = -4. La frecuencia de la nota será:

Finalmente puede observarse a partir de esta fórmula que las octavas automáticamente producen potencias de dos veces la frecuencia original, ya que "n" es un múltiplo de 12 (12 "k", donde "k" es el número de octavas hacia arriba o hacia abajo), y por lo que la fórmula se reduce a:

produciendo un factor de 2. De hecho, este es el medio por el que se obtiene esta fórmula, combinado con la noción de intervalos igualmente espaciados.

La distancia de un semitono en el temperamento igual se divide en 100 cents. Así 1200 cents equivalen a una octava, una relación de frecuencias de 2:1. Esto implica que un cent es precisamente igual a la raíz 1200.ª de 2, que es aproximadamente 1,000578.

Para el uso con el estándar MIDI (Musical Instrument Digital Interface), una asignación de frecuencias se define como:

Donde "p" es el número de nota MIDI. Y en sentido contrario, para obtener la frecuencia a partir de una nota MIDI "p", la fórmula se define como:

Para las notas en temperamento igual del "la"440, esta fórmula proporciona el número de nota MIDI estándar ("p"). Cualquier otra frecuencia llena el espacio entre los números enteros de manera uniforme. Esto permite que los instrumentos MIDI sean afinados con gran precisión en cualquier escala microtonal, incluidas las afinaciones tradicionales no-occidentales.

Además de los sonidos representados por estos siete monosílabos o notas, existen otros cinco sonidos que se obtienen subiendo o bajando uno o más semitonos. Para subir o bajar los sonidos se usan alteraciones como el bemol, el sostenido, el doble bemol, el doble sostenido y el becuadro. El bemol () baja un semitono la nota a la que acompaña, mientras que el sostenido () la sube un semitono. Para nominarlos, se usan las siete notas acompañadas o no, según corresponda, del nombre de la alteración. De esta forma, cada uno de los doce sonidos posee tres nomenclaturas, excepto en los casos como "do", "re" y "si", que son el mismo sonido, al igual que "fa", "sol" y "mi". A este fenómeno se le denomina enarmonía.
En el actual sistema de afinación (el temperamento igual), no hay diferencia entre las notas enarmónicas: por ejemplo, "do" sostenido suena exactamente igual que "re" bemol.
En los variados sistemas de afinación antiguos entre ambas notas había una diferencia audible que se denomina la coma.





</doc>
<doc id="8285" url="https://es.wikipedia.org/wiki?curid=8285" title="Negra">
Negra

La negra es una figura musical que equivale a ¼ del valor de la figura redonda. El antepasado de la negra es la semiminima de la notación mensural.

Las figuras de negras se representan con una cabeza de nota ovalada coloreada en negro (de ahí su nombre) y con una plica vertical sin adornos (como la blanca). La dirección de la plica depende de la posición de la nota. Al igual que sucede con todas las figuras que llevan plicas, las negras se dibujan con la plica a la derecha de la cabeza de la nota y hacia arriba, cuando el sonido representado está por debajo de la tercera línea del pentagrama. Mientras que, cuando la nota está en dicha línea media o por encima de esta, se dibujan con la plica a la izquierda de la cabeza de la nota y hacia abajo. No obstante, esta regla no es absoluta ya que puede variar cuando es necesario ligar varias notas o cuando se representa más de una voz (ver Figura 1). De hecho en las obras polifónicas la orientación de las plicas ayuda a distinguir las diferentes voces.

El silencio de negra es su silencio equivalente. La negra, como todas las figuras musicales, tiene un silencio de su mismo valor y supone que durante ese tiempo no se emite sonido alguno. Suele representarse mediante el símbolo aunque en ocasiones podemos encontrarnos con el símbolo antiguo . Ejemplos del antiguo símbolo se encuentran en música inglesa hasta principios del siglo XX. Se encuentran muestras de la forma más antigua en obras de los editores de música ingleses de principios del siglo XX, por ejemplo en la partitura vocal "Misa de Requiem" de Wolfgang Amadeus Mozart, editada por William Thomas Best y publicada en Londres: Novello, 1879.

En Unicode el símbolo de negra es U+2669 (♩).

La figura de negra equivale a la cuarta parte de una redonda, a la mitad de una blanca, a 2 corcheas, 4 semicorcheas, 8 fusas o 16 semifusas. 

En un compás de subdivisión binaria (2/4, 3/4, 4/4, etc.) la negra equivale a un tiempo. Por lo tanto, en un compás de 4/4 esta figura ocupa la cuarta parte de un compás. 

En un compás de subdivisión ternaria (3/8, 6/8, 9/8, 12/8, etc.) la negra equivale a dos terceras partes de un tiempo, por lo que no completa la duración de un compás. Por ejemplo, un compás de 3/8 equivale a 3 corcheas, es decir, una negra y media (o a una negra con puntillo); un compás de 6/8, equivale a 6 corcheas, es decir, dos negras con puntillo; y así sucesivamente.

Los nombres que se le dan a esta figura y a su silencio en diferentes lenguas varían enormemente:

Los nombres en catalán, español y francés de la nota significan «negro», acepción que deriva del hecho de que la "semiminima" era la nota más larga que se rellenaba coloreando con negro en la notación mensural blanca, lo cual se ha mantenido en la grafía actual.

El término empleado en inglés británico, usado en Reino Unido y Canadá, para la negra viene del vocablo del francés antiguo "crochet", que significa «pequeño gancho», diminutivo de «gancho», debido al gancho que se añadía a la plica para representar en la nota en la notación negra. Sin embargo, el término en francés moderno "croche" se refiere a la corchea debido a que el gancho apareció en la corchea en la notación blanca. 

Por su parte, en Estados Unidos escogen la acepción "quarter note" que significa «cuarto de nota» en relación con el valor de la redonda, llamada «nota completa» en esta nomenclatura. Los términos americanos son calcos semánticos de los términos alemanes, ya que cuando las orquestas estadounidenses se establecieron por primera vez en el siglo XIX fueron pobladas en gran medida por emigrantes alemanes.

Las denominaciones griega y china también aluden al concepto de «cuarto de nota» para la figura y «cuarto de pausa» para el silencio. La expresión pausa deriva del griego, en que todos los silencios musicales se llaman «pausas». 




</doc>
<doc id="8286" url="https://es.wikipedia.org/wiki?curid=8286" title="Redonda (figura)">
Redonda (figura)

Una redonda es una figura musical que posee una duración de cuatro pulsos de negra en la notación musical actual. Ellas, pueden utilizarse como un origen histórico de la redonda es la semibrevis o semibreve de la notación mensural.

Las figuras de redondas se representan con una cabeza de nota ovalada hueca (como la blanca), pero sin barra vertical o plica (ver Figura 1).

El silencio de redonda es su silencio equivalente. La redonda, como todas las figuras musicales, tiene un silencio de su mismo valor y supone que durante ese tiempo no se emite sonido alguno. Este signo se representa mediante un guion o barra horizontal que se encuentra por debajo de la cuarta línea del pentagrama.

En un compás de subdivisión binaria (2/4; 3/4; 4/4; etc.) la redonda equivale a cuatro tiempos. Por lo tanto, en un compás de 4/4 esta figura dura un compás completo. Las referencias de los tiempos en música se suelen determinar con respecto a la duración de la negra. Así pues, si en una composición la negra dura un segundo (60 negras por minuto), entonces la duración de la redonda es de 4 segundos.

La figura de redonda equivale a 2 blancas, 4 negras, 8 corcheas, 16 semicorcheas, 32 fusas o 64 semifusas. 
Por encima de la redonda hay algunas figuras de mayor duración pero han caído en desuso en la notación musical actual. Son: la cuadrada que equivale a ocho negras, la "longa" que equivale a 16 negras y la "maxima" que equivale a 32 negras. Por debajo de la semifusa también existen otras figuras de menor duración que tampoco se utilizan hoy en día. Son: la garrapatea que equivale a 1/128 de la redonda y la semigarrapatea que equivale a 1/256 de la redonda, esto es, 1/64 pulsos de negra.

En Unicode el símbolo de semifusa es U+1D15D.

En los preludios no medidos de la tradición francesa del siglo XVII, todos los sonidos eran representados mediante redondas independientemente de su duración. Será el intérprete quien determine esas duraciones dependiendo del carácter de la pieza, lo que determinó que esta fuera una música improvisada.

Esta figura suele ser empleada en orquestación para pedales armónicos.

La redonda y el silencio de redonda también pueden ser utilizados en músicas de ritmo libre, tales como el canto anglicano, para ser aplicadas a lo largo de un compás completo, con independencia de la duración de dicho compás. El silencio de redonda puede ser empleado de esta forma en casi todas o todas las formas de música.

Los nombres que se le dan a esta figura y a su silencio en diferentes lenguas varían enormemente:

Los nombres en francés y en español para esta figura musical (ambos significan «redondo») derivan del hecho de que la "semibreve" se distinguía por su forma redonda y sin plicas, lo cual sigue manteniéndose en la forma moderna (en contraste con la cuadrada o valores más cortos que llevan plicas). El nombre griego significa «entera».

La redonda deriva de la "semibreve" de la notación mensural y este es el origen del nombre que se utiliza en Reino Unido y Canadá "semibreve". En Estados Unidos se optó por el término "whole note", que es un calco semántico del alemán "ganze Note". Esto se debe a que cuando las orquestas estadounidenses se establecieron por primera vez en el siglo XIX fueron pobladas en gran medida por emigrantes alemanes. Michael Miller escribió, 



[[Categoría:Figuras musicales]]

</doc>
<doc id="8292" url="https://es.wikipedia.org/wiki?curid=8292" title="Economía informal">
Economía informal

La economía informal o economía irregular es la actividad económica que es invisible al Estado por razones de evasión fiscal o de controles administrativos (por ejemplo, el trabajo doméstico no declarado, la venta ambulante espontánea o la infravaloración del precio escriturado en una compraventa inmobiliaria). Emplea a más del 60 % de la población activa mundial. En cambio, la economía formal «es la que está dentro de los parámetros reguladores del Estado», es decir, cumple la normativa fiscal, laboral y medioambiental, paga impuestos, ha obtenido adecuadamente las necesarias licencias de actividad, se ha inscrito en los registros correspondientes y abona la seguridad social de sus trabajadores, a los que también ha inscrito.

La economía informal o irregular forma parte de la economía sumergida al lado de las actividades económicas ilegales (por ejemplo, la facturación falsa o falseada, el tráfico de drogas, el tráfico de armas, la prostitución, el blanqueo de capitales, el crimen organizado y el terrorismo).

La economía sumergida, en la medida en que se transforma en Renta Nacional y acaba integrada en la demanda agregada, resulta incluida en el dato estadístico del Producto Interior Bruto (PIB), como consecuencia del ajuste de los métodos empleados para estimarlo.

Aunque la economía informal se ha asociado frecuentemente a países en desarrollo y economías emergentes, todos los sistemas económicos, sin excepción, participan de ella.

El daño económico que causa la economía informal al fisco no se limita a los impuestos no recaudados, sino que se extiende al pago indebido de subvenciones, subsidios (por ejemplo el subsidio de desempleo que se le da a una persona que consta como parada, pero que en realidad tiene un empleo informal a tiempo completo) o pensiones.

Asimismo, la economía informal constituye uno de los supuestos más dañinos de competencia desleal entre los agentes económicos.

La economía sumergida en España representaba en 2008 más del 23 % del PIB, 

lo que equivaldría a 240 000 millones de euros.
De acuerdo con la Organización Internacional del Trabajo (OIT), «La causa esencial de la informalidad es la incapacidad para crear suficientes puestos de trabajo en la economía formal».
En cambio la OIT sí señala como obstáculos:


Las causas de la informalidad son múltiples y, en su mayoría, no guardan relación con la legalidad. Por ello, sería ineficaz proceder a la formalización únicamente mediante una reforma legislativa.

En otro documento, "Principales causas de la economía informal y algunas soluciones", la OIT divide las causas en:


Muchos jóvenes en países en desarrollo poseen empleos de baja calidad, con un escaso potencial de desarrollo profesional y poca posibilidad de contribuir a sus economías. Esto es particularmente problemático, dada la continua y significativa brecha en la productividad laboral que existe entre las regiones en vías de desarrollo y las ya desarrolladas. La enseñanza y formación técnica y profesional (EFTP) constituye un medio para brindar mayores oportunidades a los jóvenes marginados, mejorando sus resultados de empleabilidad. 

Una revisión de 26 estudios realizados en Latinoamérica, el Caribe, Europa, Asia, y África subsahariana, utilizando participantes con edades entre 15 y 24 años, concluyó que las intervenciones de EFTP, en general, tienen un efecto pequeño pero positivo sobre la empleabilidad y el empleo de los jóvenes. Asimismo, no se halló ningún modelo específico de intervención que fuera mejor que otros. Estos resultados, sin embargo, impiden sacar conclusiones sólidas, debido a limitaciones tanto en los estudios como en la revisión misma. Por ello, es necesario que más intervenciones de ETFP sean evaluadas, y sus resultados difundidos de forma eficiente.


La economía informal bajo cualquier sistema de gobierno es diversa e incluye miembros ocasionales de pequeña escala (a menudo vendedores ambulantes y recicladores de basura) así como empresas más grandes y regulares (incluidos sistemas de tránsito como el de Lima, Perú). Las economías informales incluyen a los trabajadores de la confección que trabajan desde sus hogares, así como al personal empleado informalmente en las empresas formales. Los empleados que trabajan en el sector informal se pueden clasificar como trabajadores asalariados, trabajadores no asalariados o una combinación de ambos.

Las estadísticas sobre la economía informal no son fiables por la propia naturaleza de su objeto, pero pueden proporcionar una imagen tentativa de su relevancia. Por ejemplo, el empleo informal representa el 58,7 % del empleo no agrícola en Medio Oriente - Norte de África -, 64,6 % en América Latina, 79,4 % en Asia y 80,4 % en África subsahariana. Si se incluye el empleo agrícola, los porcentajes aumentan más allá del 90 % en algunos países como India y muchos países del África subsahariana. Las estimaciones para los países desarrollados son de alrededor del 15 %. En encuestas recientes, la economía informal en muchas regiones ha disminuido en los últimos 20 años hasta 2014. En África, la participación de la economía informal ha disminuido a una estimación de alrededor del 40 % de la economía.

En los países en desarrollo la mayor parte del trabajo informal, alrededor del 70 %, es por cuenta propia. La economía informal es una mayor fuente de empleo para los hombres (63,0 % de la población activa) que para las mujeres (58,1 %). De los 2 000 millones de empleados informales en el mundo en 2018, poco más de 740 millones son mujeres.

El problema del desarrollo en Latinoamérica se encuentra fuertemente vinculado con los niveles de empleo informal. Entender la naturaleza de la informalidad es crucial para promover el desarrollo, dada sus implicaciones sobre factores como la productividad o la acumulación de habilidades. Por ello, se ha desarrollado un proyecto que explora el papel de los costos pecuniarios de solicitar una matrícula de comercio, las percepciones en cuanto a los beneficios potenciales de la formalidad, las sanciones por operar sin los instrumentos legales existentes, y la información sobre las instituciones jurídicas existentes para formalizarse.

Un estudio experimental que contó con una muestra de 1046 microempresarios de Bolivia, no inscritos en FUNDAEMPRESA para 2012, concluyó que ofrecer información sobre potenciales beneficios de formalizarse, régimen tributario, sanciones e instituciones a las cuáles acudir, incide positivamente en la tenencia de instrumentos de formalización. Asimismo, un descuento del 50% sobre el costo de adquirir la matrícula no incrementa de manera significativa la obtención del instrumento. Por último, los impuestos dependientes del tamaño de las empresas podrían distorsionar algunas decisiones de aquellos establecimientos cuyos niveles de capital o ventas, por ejemplo, se encuentren alrededor de los umbrales definidos por las políticas.



</doc>
<doc id="8293" url="https://es.wikipedia.org/wiki?curid=8293" title="Teoría de grafos">
Teoría de grafos

La teoría de grafos, también llamada teoría de gráficas, es una rama de las matemáticas y las ciencias de la computación que estudia las propiedades de los grafos. Los grafos no deben ser confundidos con las gráficas, que es un término muy amplio. Formalmente, "un grafo" formula_1 es una pareja ordenada en la que formula_2 es un conjunto no vacío de vértices y formula_3 es un conjunto de aristas. Donde formula_3 consta de pares no ordenados de vértices, tales como formula_5, entonces se dice que formula_6 e formula_7 son adyacentes; y en el grafo se representa mediante una línea no orientada que una dichos vértices. Si el grafo es dirigido se le llama "dígrafo", se denota formula_8, y entonces el par formula_9 es un par ordenado, esto se representa con una flecha que va de formula_6 a formula_7 y se dice que formula_12. 

La teoría de grafos tiene sus fundamentos en las matemáticas discretas y de las matemáticas aplicadas. Esta teoría requiere de diferentes conceptos de diversas áreas como combinatoria, álgebra, probabilidad, geometría de polígonos, aritmética y topología. Actualmente ha tenido mayor influencia en el campo de la informática, las ciencias de la computación y telecomunicaciones. Debido a la gran cantidad de aplicaciones en la optimización de recorridos, procesos, flujos, algoritmos de búsquedas, entre otros, se generó toda una nueva teoría que se conoce como análisis de redes.

El origen de la teoría de grafos se remonta al siglo XVIII con el problema de los puentes de Königsberg, el cual consistía en encontrar un camino que recorriera los siete puentes del río Pregel () en la ciudad de Königsberg, actualmente Kaliningrado, de modo que se recorrieran todos los puentes pasando una sola vez por cada uno de ellos. El trabajo de Leonhard Euler sobre el problema titulado "Solutio problematis ad geometriam situs pertinentis" ("La solución de un problema relativo a la geometría de la posición") en 1736, es considerado el primer resultado de la teoría de grafos. También se considera uno de los primeros resultados topológicos en geometría (que no depende de ninguna medida). Este ejemplo ilustra la profunda relación entre la teoría de grafos y la topología.

Luego, en 1847, Gustav Kirchhoff utilizó la teoría de grafos para el análisis de redes eléctricas publicando sus leyes de los circuitos para calcular el voltaje y la corriente en los circuitos eléctricos, conocidas como leyes de Kirchhoff, considerado la primera aplicación de la teoría de grafos a un problema de ingeniería.

En 1852, Francis Guthrie planteó el problema de los cuatro colores, el cual afirma que es posible, utilizando solamente cuatro colores, colorear cualquier mapa de países de tal forma que dos países vecinos nunca tengan el mismo color. Este problema, que no fue resuelto hasta un siglo después por Kenneth Appel y Wolfgang Haken en 1976, puede ser considerado como el nacimiento de la teoría de grafos. Al tratar de resolverlo, los matemáticos definieron términos y conceptos teóricos fundamentales de los grafos.

En 1857, Arthur Cayley estudió y resolvió el problema de enumeración de los isómeros, compuestos químicos con idéntica composición (fórmula) pero diferente estructura molecular. Para ello representó cada compuesto, en este caso hidrocarburos saturados CH, mediante un grafo árbol donde los vértices representan átomos y las aristas la existencia de enlaces químicos.

El término "«grafo»", proviene de la expresión inglesa "graphic notation" («notación gráfica»), usada por primera vez por Edward Frankland y posteriormente adoptada por Alexander Crum Brown en 1884 y que hacía referencia a la representación gráfica de los enlaces entre los átomos de una molécula.

El primer libro sobre teoría de grafos fue escrito por Dénes Kőnig y publicado en 1936.




Existen diferentes formas de representar un grafo (simple), además de la geométrica y muchos métodos para almacenarlos en una computadora. La estructura de datos usada depende de las características del grafo y el algoritmo usado para manipularlo. Entre las estructuras más sencillas y usadas se encuentran las listas y las matrices, aunque frecuentemente se usa una combinación de ambas. Las listas son preferidas en grafos dispersos porque tienen un eficiente uso de la memoria. Por otro lado, las matrices proveen acceso rápido, pero pueden consumir grandes cantidades de memoria.





Un problema común, denominado problema de isomorfismo de subgrafos, es encontrar un grafo fijo como subgrafo de un grafo dado. Una razón para estar interesado en esta cuestión es que muchas propiedades de grafos son heredadas de subgrafos, lo que significa que un grafo tiene una propiedad si y solo si todos sus subgrafos a su vez la poseen. Desafortunadamente, encontrar subgrafos máximos de un cierto tipo suele ser un problema NP-completo. Por ejemplo:

Un problema similar es encontrar un subgrafo inducido en un grafo dado. De nuevo, algunas propiedades importantes son heredadas con respecto a subgrafos inducidos, lo que significa que un grafo tiene una propiedad si y solo si todos los subgrafos inducidos la tienen. Encontrar subgrafos inducidos máximos de un determinado tipo es, de nuevo, un problema NP-completo. Como ejemplo:

Otro nuevo problema es el problema del menor contenido, que es encontrar un grafo fijo como menor de un grafo dado. Un menor o subcontración de un grafo es cualquier grafo obtenido tomando un subgrafo y contrayendo algunos bordes. Muchas propiedades de grafos son heredadas de menores, lo que significa que un grafo la tiene solo si todos sus menores la tienen también. Por ejemplo, el teorema de Wagner estipula que:

Un problema de las mismas características es el problema de la subdivisión del contenido. Una subdivisión o homeomorfismo de un grafo es cualquier grafo obtenido subdividiendo algunos bordes. La subdivisión del contenido está relacionada con las propiedades de los grafos tales como la "planeza". Por ejemplo, el teorema de Kuratowski establece que:
Otro problema en la subdivisión de contenido es la conjetura de Kelmans-Seymour:
Otro problemas de clases tienen que ver con el alcance para la cual varias especies y generalizaciones de grafos están determinadas por sus subgrafos de puntos eliminados. Por ejemplo, la conjetura de la reconstrucción.

Un ciclo es una sucesión de aristas adyacentes, donde no se recorre dos veces la misma arista, y donde se regresa al punto inicial. Un ciclo hamiltoniano tiene además que recorrer todos los vértices exactamente una vez (excepto el vértice del que parte y al cual llega).

Por ejemplo, en un museo grande, lo idóneo sería recorrer todas las salas una sola vez, esto es buscar un ciclo hamiltoniano en el grafo que representa el museo (los vértices son las salas, y las aristas los corredores o puertas entre ellas).

Se habla también de Camino hamiltoniano si no se impone regresar al punto de partida, como en un museo con una única puerta de entrada. Por ejemplo, un caballo puede recorrer todas las casillas de un tablero de ajedrez sin pasar dos veces por la misma: es un camino hamiltoniano. Un ejemplo de ciclo hamiltoniano es el grafo del dodecaedro.

Hoy en día, no se conocen métodos generales para hallar un ciclo hamiltoniano en tiempo polinómico, siendo la búsqueda por fuerza bruta de todos los posibles caminos u otros métodos excesivamente costosos. Existen, sin embargo, métodos para descartar la existencia de ciclos o caminos hamiltonianos en grafos pequeños.

El problema de determinar la existencia de ciclos hamiltonianos, entra en el conjunto de los NP-completos.

Cuando un grafo o multigrafo se puede dibujar en un plano sin que dos segmentos se corten, se dice que es plano.

Un problema muy conocido es el siguiente: Se dibujan tres casas y tres pozos. Todos los vecinos de las casas tienen el derecho de utilizar los tres pozos. Como no se llevan bien en absoluto, no quieren cruzarse jamás. ¿Es posible trazar los nueve caminos que juntan las tres casas con los tres pozos sin que haya cruces?

Cualquier disposición de las casas, los pozos y los caminos implica la presencia de al menos un cruce.

Sea K el grafo completo con "n" vértices, K es el grafo bipartito de "n" y "p" vértices.

El juego anterior equivale a descubrir si el grafo bipartito completo K es plano, es decir, si se puede dibujar en un plano sin que haya cruces, siendo la respuesta que no. En general, puede determinarse que un grafo "no" es plano, si en su diseño puede encontrase una estructura análoga (conocida como "menor") a K o a K.

Establecer qué grafos son planos no es obvio, y es un problema que tiene que ver con topología.

Si G=(V, E) es un grafo no dirigido, una coloración propia de G, ocurre cuando coloreamos los vértices de G de modo que si {a, b} es una arista en G entonces a y b tienen diferentes colores (por lo tanto, los vértices adyacentes tienen colores diferentes). El número mínimo de colores necesarios para una coloración propia de G es el número cromático de G y se escribe como C (G).
Sea G un grafo no dirigido sea λ el número de colores disponibles para la coloración propia de los vértices de G. Nuestro objetivo es encontrar una función polinomial P (G,λ), en la variable λ, llamada polinomio cromático de G, que nos indique el número de coloraciones propias diferentes de los vértices de G, usando un máximo de λ colores.

Descomposición de polinomios cromáticos. Si G=(V, E) es un grafo conexo y e pertenece a Ε, entonces: P (G,λ)=P (G+e,λ)+P (G/e,λ), donde G/e es el grafo se obtiene por contracción de aristas.

Para cualquier grafo G, el término constante en P (G,λ) es 0.

Sea G=(V, E) con |E|>0 entonces, la suma de los coeficientes de P (G,λ) es 0.

Sea G=(V, E), con "a", "b" pertenecientes al conjunto de vértices "V" pero {a, b}=e, no perteneciente a al conjunto de aristas E. Escribimos G+e para el grafo que se obtiene de G al añadir la arista e={a, b}. Al identificar los vértices a y b en G, obtenemos el subgrafo G++e de G.0000.

Este problema famoso relativo a los grafos trata acerca de la cantidad de colores que son necesarios para dibujar un mapa político, con la condición obvia que dos países adyacentes no puedan tener el mismo color. Se supone que los países son de un solo pedazo, y que el mundo es esférico o plano.El mapa siguiente muestra que tres colores no bastan: Si se empieza por el país central a y se esfuerza uno en utilizar el menor número de colores, entonces en la corona alrededor de a alternan dos colores. Llegando al país h se tiene que introducir un cuarto color. Lo mismo sucede en i si se emplea el mismo método.Sin embargo,si el mapa tiene forma de toroide, el teorema afirma que con cuatro colores siempre es posible realizar la coloración con las características requeridas.

La forma precisa de cada país no importa; lo único relevante es saber qué país toca a qué otro. Estos datos están incluidos en el grafo donde los vértices son los países y las aristas conectan los que justamente son adyacentes. Entonces la cuestión equivale a atribuir a cada vértice un color distinto del de sus vecinos.

Hemos visto que tres colores no son suficientes, y demostrar que con cinco siempre se llega, es bastante fácil. Pero el teorema de los cuatro colores no es nada obvio.
Prueba de ello es que se han tenido que emplear ordenadores para acabar la demostración (se ha hecho un programa que permitió verificar una multitud de casos, lo que ahorró muchísimo tiempo a los matemáticos). Fue la primera vez que la comunidad matemática aceptó una demostración asistida por ordenador, lo que creó en su día una cierta polémica dentro de dicha comunidad.

Un grafo es "simple" si a lo sumo existe una arista uniendo dos vértices cualesquiera. Esto es equivalente a decir que una arista cualquiera es la única que une dos vértices específicos.

Un grafo que no es simple se denomina multigrafo.

En la Teoría de grafos el concepto de grafo simple es muy recurrido en la definición de otros entes, como los de grafos completos, grafos bipartidos completos, árboles y otros más.

Las definiciones aportan una formalización lógica a hechos abstractos o naturales, muchas veces ya definidos de forma intuitiva. En este caso la imagen de grafo simple es fácil de reconocer ante otro que no lo es; bien por la presencia de lazos o de más de una arista entre los pares de vértices.

Un grafo es conexo si cada par de vértices está conectado por un camino; es decir, si para cualquier par de vértices (a, b), existe al menos un camino posible desde "a" hacia "b".

Un grafo es doblemente conexo si cada par de vértices está conectado por al menos dos caminos disjuntos; es decir, es conexo y no existe un vértice tal que al sacarlo el grafo resultante sea disconexo.

Es posible determinar si un grafo es conexo usando un algoritmo Búsqueda en anchura (BFS) o Búsqueda en profundidad (DFS).

En términos matemáticos la propiedad de un grafo (fuertemente) conexo permite establecer una relación de equivalencia para sus vértices, la cual lleva a una partición de estos en "componentes (fuertemente) conexos", es decir, porciones del grafo, que son (fuertemente) conexas cuando se consideran como grafos aislados. Esta propiedad es importante para muchas demostraciones en teoría de grafos.

Un grafo es "completo" si existen aristas uniendo "todos" los pares posibles de vértices. Es decir, todo par de vértices (a, b) debe tener una arista "e" que los une.

El conjunto de los grafos completos es denominado usualmente formula_16, siendo formula_17 el grafo completo de "n" vértices.

Un formula_17, es decir, grafo completo de formula_14 vértices tiene exactamente formula_20 aristas.

La representación gráfica de los formula_17 como los vértices de un polígono regular da cuenta de su peculiar estructura.

Un grafo G es bipartito si puede expresar como formula_22 (es decir, sus vértices son la unión de dos grupos de vértices), bajo las siguientes condiciones:

Bajo estas condiciones, el grafo se considera bipartito, y puede describirse informalmente como el grafo que une o relaciona dos conjuntos de elementos diferentes, como aquellos resultantes de los ejercicios y rompecabezas en los que debe unirse un elemento de la columna A con un elemento de la columna B.

Dos grafos formula_25 y formula_26 son homeomorfos si ambos pueden obtenerse a partir del mismo grafo con una sucesión de subdivisiones elementales de aristas.

Un grafo que no tiene ciclos y que conecta a todos los puntos, se llama un árbol. En un grafo con n vértices, los árboles tienen exactamente n - 1 aristas, y hay n árboles posibles. Su importancia radica en que los árboles son grafos que conectan todos los vértices utilizando el menor número posible de aristas. Un importante campo de aplicación de su estudio se encuentra en el análisis filogenético, el de la filiación de entidades que derivan unas de otras en un proceso evolutivo, que se aplica sobre todo a la averiguación del parentesco entre especies; aunque se ha usado también, por ejemplo, en el estudio del parentesco entre lenguas.

En muchos casos, es preciso atribuir a cada arista un número específico, llamado "valuación", "ponderación" o "coste" según el contexto, y se obtiene así un grafo valuado.
Formalmente, es un grafo con una función v: A → R.

Por ejemplo, un representante comercial tiene que visitar "n" ciudades conectadas entre sí por carreteras; su interés previsible será minimizar la distancia recorrida (o el tiempo, si se pueden prever atascos). El grafo correspondiente tendrá como vértices las ciudades, como aristas las carreteras y la valuación será la distancia entre ellas.Si embargo, hasta ahora no ha sido posible encontrar métodos generales para hallar un ciclo de valuación mínima, pero sí para los caminos desde "a" hasta "b", sin más condición.

En un grafo, la distancia entre dos vértices es el menor número de aristas de un recorrido entre ellos. El diámetro, en una figura como un grafo, es la mayor distancia entre todos los pares de puntos de la misma.

El diámetro de los K es 1, y el de los K, es 2. Un diámetro infinito puede significar que el grafo tiene una infinidad de vértices o simplemente que no es conexo. También se puede considerar el diámetro promedio, como el promedio de las distancias entre dos vértices.

Una aplicación de este concepto es la hipótesis conocida como los seis grados de separación, que plantea que, si cada uno de los habitantes de la Tierra se representa por un vértice y dos personas están conectadas por una arista si se conocen personalmente, la distancia entre dos personas escogidas al azar entre todos los habitantes de la Tierra es de seis aristas o menos.

Internet permite de ver desde otro enfoque la idea del diámetro: considérese por ejemplo que si se descartan los sitios que no tienen enlaces, y se escogen dos páginas "web" al azar, cabría preguntarse en cuántos "clics" se puede pasar del primer sitio al segundo. Si se supone que de cualquier sitio que enlace con otros sitios se puede llegar a cualquier otro, entonces las mayor cantidad de "clics" necesarios para llegar de cualquier web a otra sería el "diámetro" de la Red, vista como un grafo cuyos vértices son los sitios, y cuyas aristas son los enlaces entre los sitios.

Este concepto refleja mejor la complejidad de una red que el número de sus elementos.

Gracias a la teoría de grafos se pueden resolver diversos problemas como por ejemplo la síntesis de circuitos secuenciales, contadores o sistemas de apertura. Se utiliza para diferentes áreas como pueden ser el Dibujo computacional o en áreas de Ingeniería.

Los grafos se utilizan también para modelar trayectos como el de una línea de autobús a través de las calles de una ciudad, en el que se pueden obtener caminos óptimos para el trayecto aplicando diversos algoritmos como puede ser el algoritmo de Floyd.

Para la administración de proyectos, utilizamos técnicas como técnica de revisión y evaluación de programas (PERT) en las que se modelan los mismos utilizando grafos y optimizando los tiempos para concretar los mismos.

Una importante aplicación de la teoría de grafos es en el campo de la informática, ya que ha servido para la resolución de importantes y complejos algoritmos. Un claro ejemplo es el Algoritmo de Dijkstra, utilizado para la determinación del camino más corto en el recorrido de un grafo con determinados pesos en sus vértices.

Dentro de este campo, un grafo es considerado un tipo de dato abstracto TAD.

El científico estadounidense Donald Knuth estableció los grafos planos como base de determinados estudios y descubrimientos realizados por él.

Por otra parte, destaca el Algoritmo de Kruskal, el cual nos permite buscar un subconjunto de aristas que incluye todos los vértices, estableciendo como mínimo el valor de las aristas.

La teoría de grafos también ha servido de inspiración para las ciencias sociales, en especial para desarrollar un concepto no metafórico de red social que sustituye los nodos por los actores sociales y verifica la posición, centralidad e importancia de cada actor dentro de la red. Esta medida permite cuantificar y abstraer relaciones complejas, de manera que la estructura social puede representarse gráficamente. Por ejemplo, una red social puede representar la estructura de poder dentro de una sociedad al identificar los vínculos (aristas), su dirección e intensidad y da idea de la manera en que el poder se transmite y a quiénes.

Se emplea en problemas de control de producción, para proyectar redes de ordenadores, para diseñar módulos electrónicos modernos y proyectar sistemas físicos con parámetros localizados (mecánicos, acústicos y eléctricos).

Se usa para la solución de problemas de genética y problemas de automatización de la proyección (SAPR). Apoyo matemático de los sistemas modernos para el procesamiento de la información. Acude en las investigaciones nucleares (técnica de diagramas de Feynman).

Los grafos son importantes en el estudio de la biología y hábitat. El vértice representa un hábitat y las aristas (o "edges" en inglés) representa los senderos de los animales o las migraciones. Con esta información, los científicos pueden entender cómo esto puede cambiar o afectar a las especies en su hábitat.

Entre las aplicaciones de la Teoría de gráficas que se han vuelto importantes en la actualidad podemos encontrar el estudio de las redes sociales, cuya importancia radica en el adecuado almacenamiento de datos, puesto que el costo del tiempo de búsqueda de la información de cada miembro que pertenece a esta red puede tornarse demasiado alto debido al número de usuarios. Por ejemplo, el número de usuarios que hay actualmente en una importante red social tan solo en México es de 49 millones —cifra reportada por el periódico "El economista" en 2014—; si este número lo multiplicamos por 194, que es el número aproximado de países que hay en el mundo, se percibe la posibilidad de un grave problema de almacenamiento para los servidores que hay destinados para ello y para la búsqueda de información. Este mismo fenómeno pasa en otras redes de fotografías, mensajes, etc. 

El modelado de este tipo de problemas ha sido abordado principalmente por estudiantes de doctorado de universidades como Stanford, Massachusetts Institute of Technology (MIT), Berkeley, Oxford, Rice y también por la NASA; en México, tanto el Instituto Politécnico Nacional (IPN) como la Universidad Nacional Autónoma de México (UNAM) son los principales promotores en estas áreas a través de los grupos académicos de combinatoria y de computación científica. Podemos considerar que este tipo de problemas son tratados por expertos en matemáticas y ciencias de la computación, debido a su alto grado de complejidad.

El cerebro humano es una red compleja que interactúa en regiones conectadas por tractos de sustancia blanca. La caracterización de características estructurales y funcionales de una red tal en sujetos sanos y personas enfermas tiene la posibilidad de mejorar nuestra comprensión de la fisiopatología y las manifestaciones neurológicas y condiciones psiquiátricas. Esto ha llevado al uso de nuevas herramientas para el análisis de sistemas complejos para hacer frente enfermedades cerebrales. Entre estos, la teoría de grafos es un marco matemático que permite describir una red en forma de una gráfica, que consiste en una colección de los nodos (es decir, regiones del cerebro) y los bordes (es decir, estructurales y conexiones funcionales).

El uso de la teoría de grafos, para distintas modificaciones de la topología de red cerebro han sido identificados durante el desarrollo y el envejecimiento normal y se rompieron conectividades funcionales y estructurales han sido asociado con varios trastornos neurológicos y psiquiátricos, incluyendo demencia, esclerosis lateral amiotrófica, y la esquizofrenia. En este último enfoque se ha contribuido a probar la teoría de esta condición como un síndrome de desconexión. En la esclerosis múltiple (MS), la ocurrencia de la desconexión ha sido corroborada por estudios de resonancia magnética estructural de topología de la red cerebral que mostró una disminución de la conectividad estructural de las regiones de los lóbulos fronto-temporal.

Otra aplicación de las gráficas consiste en tomar datos de resonancia magnética del cerebro adquiridos en condición ausente ("estado de reposo") requieren nuevos análisis de datos técnicas que no dependen de un modelo de activación. Una alternativa son los métodos libres de parámetro sobre la base de una forma particular de la centralidad del vector propio asociado a un nodo llamado de centralidad; la centralidad del vector propio asigna atributos de un valor a cada vóxel del cerebro de manera que un vóxel recibe un valor grande si está fuertemente correlacionado con muchos otros nodos centrales dentro de la red. El algoritmo PageRank de Google es una variante del vector propio centralidad, el cual es utilizado en las búsquedas de internet. Hasta el momento, otras medidas de centralidad —en especial, la "centralidad de intermediación"— se han aplicado a datos de la fMRI usando un conjunto preseleccionado de nodos que consisten en varios cientos de elementos. Centralidad del Vector Propio es computacionalmente mucho más eficiente que centralidad de intermediación y no requiere de umbrales de valores de similitud de modo que se puede aplicar a miles de vóxeles en una región de interés que cubren la totalidad del cerebro que habría sido inviable el uso de centralidad de intermediación. Centralidad del Vector Propio se puede utilizar en una variedad de diferentes medidas de similitud. (Lohmann et al., 2010)
«La teoría de redes complejas juega un papel importante en una amplia variedad de disciplinas,
que van desde la informática, sociología, ingeniería y física, para molecular
y la biología de la población. Dentro de los campos de la biología y la medicina, el potencial de
aplicaciones de análisis de redes incluyen, por ejemplo, la identificación objetivo de drogas, determinando una función del gen de la proteína, o diseñar estrategias eficaces para el tratamiento de diversas enfermedades o proporcionar el diagnóstico precoz de trastornos». (Pavlopoulos et al., 2011)
La teoría de gráficas, es adecuada para que los informáticos modelen problemas, pero también es adecuado para los matemáticos que tienen interés en la complejidad computacional. La mayoría de los conceptos clásicos de la teoría de grafos teórica y aplicada (árboles de expansión, conectividad, género, coloración, fluye en las redes, los apareamientos y recorridos). Se usa en la solución de problemas. (Czumaj, Jansen, Meyer auf der Heide, & Schiermeyer, 2006)









</doc>
<doc id="8298" url="https://es.wikipedia.org/wiki?curid=8298" title="Misuri (desambiguación)">
Misuri (desambiguación)

El término Misuri (en inglés Missouri) puede referirse a:

</doc>
