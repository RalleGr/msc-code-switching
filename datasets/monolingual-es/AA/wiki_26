<doc id="4715" url="https://es.wikipedia.org/wiki?curid=4715" title="Shakira">
Shakira

Shakira Isabel Mebarak Ripoll (Barranquilla, Colombia; 2 de febrero de 1977), conocida artísticamente como Shakira, es una cantautora, productora discográfica, bailarina, empresaria, embajadora de buena voluntad de UNICEF y filántropa colombiana. 

Su alto nivel de ventas, versatilidad vocal y su éxito global la ha llevado a ser calificada por Sony Music y "Billboard" con el apodo de «Reina del pop latino». Diferentes conglomerados mediáticos la consideran un icono mundial de la música latina, haciendo énfasis en la gran influencia que ha tenido en la escena musical, desde su debut en los años 1990 hasta la actualidad, logrando la introducción de varios artistas latinos al mundo pop anglo, siguiendo una carrera artística de más de 28 años se estima que ha vendido más de 75 millones de álbumes y sencillos.

Luego de dos intentos fallidos en 1991 y 1993, Shakira debutó en el mercado discográfico hispanoamericano en 1995 con el álbum "Pies descalzos", colaborando con Luis Fernando Ochoa en la composición y producción de los once temas incluidos, en los que se destacan «Estoy aquí», «¿Dónde estás corazón?» y «Antología»; se estima que ha vendido más de 4 millones de copias. Años después, continuó trabajando con Ochoa y lanzó "¿Dónde están los ladrones?" (1998), obteniendo así ventas superiores a los 7 millones de álbumes vendidos. El éxito internacional le llegó en el 2001 con su álbum "Laundry Service", situándola en la cima del mercado anglosajón con más de 13 millones de álbumes vendidos.

Cuatro años más tarde, Shakira lanzó su sexto álbum de estudio, titulado "Fijación oral vol. 1" y volvió a colaborar con Luis Fernando Ochoa, y se convirtió en una de las artistas más aclamadas internacionalmente al iniciar la promoción de su quinto álbum de estudio, "Oral Fixation vol. 2", cuyo relanzamiento a inicios de 2006 involucró el lanzamiento de «Hips Don't Lie» ("Las caderas no mienten"), que la convirtió en la primera artista sudamericana en la historia en posicionar una canción número uno en la lista "Billboard" Hot 100, The Rolling Stone la ubicó en el puesto #41 de las 50 canciones latinas más exitosas de todos los tiempos, una versión diferente titulada "Hips Don't Lie / Bamboo" fue incluido en la banda sonora de la Copa Mundial de Fútbol de 2006. 

La BMI, la organización de derechos de música más grande de los Estados Unidos, define a Shakira como una pionera que redefinió el alcance internacional de los cantantes latinos.

Shakira es una de las discográficas de la historia. La revista Billboard ha situado su carrera como una de las más consolidadas en la historia de Latinoamérica, situando tres de sus sencillos entre los veinte más importantes en la historia de Billboard, «Chantaje» «La tortura» y «Suerte» situándolos en las posiciones #6 #7 y #16. Del mismo modo fue la única artista femenina en el top 50 de Billboard de las canciones latinas más exitosas de la década de los 2010, ubicando a «chantaje» en la posición #18. En 2020 la revista The Rolling Stone ubicó su álbum "¿Dónde están los ladrones? entre los 500 mejores de todos los tiempos, respectivamente en la posición 496, siendo el único álbum de rock en español en la lista."

Entre sus principales récords, fue la persona del año en los Latin Grammys 2011 siendo la artista más joven en obtener este homenaje, se convirtió en la primera artista en la historia de VEVO en lograr conseguir más de 100 millones de reproducciones con una misma canción en dos versiones, logro que ha repetido varias veces; primero con «Waka Waka (Esto es África)», luego con «La La La (Brasil 2014)», seguido por «Loba» y «Suerte» y finalmente con «Loca» tanto en las versiones en inglés como en español. La empresa Live Nation consideró a Shakira la artista más importante de su generación por su impacto global consolidado y la situó con su contrato como referencia entre los artistas más importantes del mundo. Según la revista "Forbes", es una de las cantantes que más dinero gana y está considerada una de las cien mujeres más influyentes del planeta.

En 1999, la transmisión de su primer álbum en vivo MTV Unplugged se convirtió en la primera presentación acústica internacional transmitida en español por MTV en Estados Unidos, y posteriormente a través de varias redes europeas. Es la primera y, hasta ahora, única persona junto con Cristiano Ronaldo que ha conseguido 100 millones de seguidores en Facebook y ha sido la única en participar musicalmente en tres mundiales de fútbol consecutivos: en 2006 con «Hips Don't Lie», en 2010 con «Waka Waka (Esto es África)» y en 2014 con una versión de la canción «Dare (La La La)». 

A fines de 2017, Shakira se convirtió en la primera artista en ingresar cuatro canciones en español de un solo álbum a "Billboard" Hot 100, la principal lista musical de los Estados Unidos. Asimismo, Shakira posee diversos premios, entre ellos están varios, 42 Latin Billboard Music Awards, 7 Billboard music awards, 3 Grammy Awards y 13 Latin Grammy. También ostenta de varios guinness world records y una estrella en el paseo de la fama de Hollywood.

Su gira musical "Sale el sol World Tour" (2010-2011), recaudó más de 120 millones de dólares y logró reunir a más de un millón de personas, siendo la segunda gira consecutiva de Shakira en lograr esto. 

El 2 de febrero de 2020 Shakira encabezó junto a Jennifer Lopez el Espectáculo de medio tiempo del Super Bowl LIV, Miami Gardens, Florida, se convirtió en uno de los shows de medio tiempo más vistos de todos los tiempos.

Nació el 2 de febrero de 1977 en Barranquilla, Colombia. Es la hija única de William Mebarak Chadid, un estadounidense nacido en Nueva York de origen libanés emigrado a Colombia a los cinco años y su segunda esposa Nidia del Carmen Ripoll Torrado, una colombiana de ascendencia española. Tiene 8 medio hermanos mayores del matrimonio anterior de su padre.

"Shakira" (شاكرة "šākira") significa "agradecida" en árabe. Es la forma femenina del nombre "Shakir" (شاكر "šākir"). Su segundo nombre, "Isabel", viene de su abuela materna y es de origen español. Su primer apellido, Mebarak (مبارك "məbārak"), es de origen árabe, y su segundo apellido, Ripoll, es de origen catalán.

Shakira escribió su primer poema, titulado «La rosa de cristal», cuando sólo tenía cuatro años. En su infancia quedó fascinada al ver a su padre escribir historias en una máquina de escribir, y pidió una como regalo de Navidad. A los siete años se cumplió su deseo y siguió escribiendo poesía, poemas que finalmente terminaron en canciones.

A los ocho años escribió su primera canción, titulada «Tus gafas oscuras» inspirándose en su padre, que durante años llevó gafas oscuras para ocultar el dolor de la muerte de uno de sus hijos, cuando Shakira tenía dos años.

Cuando ella tenía cuatro años, su padre la llevó a un restaurante local de Oriente Medio, donde por primera vez escuchó el derbake, un tambor tradicional usado en la música árabe, acompañamiento típico de la danza del vientre. Antes de darse cuenta, ya estaba bailando sobre una mesa, y los clientes del restaurante aplaudieron con entusiasmo. Fue entonces cuando supo que quería ser artista.

Le gustaba cantar para sus compañeros y profesores en su escuela católica, pero en segundo grado no fue aceptada en el coro de la escuela porque su vibrato era demasiado fuerte. El profesor de música le dijo que sonaba "como una cabra". En la escuela, era conocida como "la chica del baile del vientre", y todos los viernes mostraba un nuevo número que había aprendido.

A sus ocho años, el padre de Shakira se declaró en bancarrota, mientras que se ultimaban los detalles del proceso, fue a vivir con unos familiares en Los Ángeles. Al regresar a Barranquilla, se sorprendió al descubrir que gran parte de las posesiones de sus padres habían sido vendidas; más tarde, dijo: "En mi cabeza infantil, ése fue el fin del mundo". Para mostrarle que las cosas podían ser peores, su padre la llevó a un parque cercano para que viera los huérfanos que vivían allí. La imagen la impresionó y se dijo "un día voy a ayudar a estos niños, cuando me convierta en una artista famosa."

Participó por primera vez en el concurso televisivo "Buscando artista infantil" en el año 1988 (de la cadena regional de la costa colombiana Telecaribe), concurso que ganó durante tres años consecutivos.

Entre los diez y trece años de edad Shakira fue invitada a varios eventos en Barranquilla y obtuvo cierto reconocimiento en la zona. Como consecuencia de ello, conoció a una productora de teatro, quien la ayudó a darse a conocer fuera de Barranquilla. Conoció también a un ejecutivo de Sony Colombia, quien tras una prueba decidió promover a la cantante dentro de su productora. La audición de unas cintas no convenció en principio a los dirigentes de la misma, pero tras escucharla en directo, Shakira fue contratada para grabar tres álbumes.

Shakira se trasladó a Bogotá, Colombia y con catorce años de edad lanzó su primer álbum en 1991, titulado "Magia", el cual incluye canciones escritas por ella misma, entre las que se destacan «Magia», «Esta noche voy contigo» y «Tus gafas oscuras». Sin embargo, el álbum resultó ser un fracaso comercial, vendiendo solamente mil unidades.

En 1993, a días de haber cumplido los 16 años, participó en el XXXIV Festival Internacional de la Canción de Viña del Mar, donde obtuvo el tercer lugar de la competencia con la canción «Eres». Su segundo álbum, titulado "Peligro", fue lanzado ese mismo año y editado por la intérprete. Este álbum, a pesar de que fue mejor recibido que el anterior, también resultó ser otro fracaso para Shakira en cuanto a ventas y por ello decidió tomar un receso musical para terminar sus estudios. Actualmente, los álbumes "Magia" y "Peligro" se encuentran descatalogados y no forman parte de su discografía oficial, considerándose prácticamente una reliquia de la música latina. En 1994 protagonizó la miniserie "El Oasis", realizada por Cenpro TV, junto al actor "Pedro Rendon" en la cual se representaba un romance entre las familias sobrevivientes a la tragedia de Armero.

En 1995, el sello discográfico decide lanzar al mercado un disco de éxitos de artistas colombianos. Es así como piensan en Shakira y le comentan que incluirán una canción de alguno de sus trabajos discográficos anteriores, "Magia" y "Peligro", a lo que Shakira se niega y ofrece componer una canción especialmente para dicho disco recopilatorio. Es así como nace «¿Dónde estás corazón?», convirtiéndose en un éxito solicitado en las radios. La canción también comenzó a escucharse fuera de su Colombia natal.

En 1995, Shakira publicó su primer álbum de estudio oficial, titulado "Pies descalzos" (grabado el año anterior), que gracias a la colaboración de Luis Fernando Ochoa en el trabajo de producción, la convertiría en una figura del ámbito musical hispanoamericano. Dándose a conocer con el sencillo « ¿Dónde estás corazón? », canción que apareció en el recopilatorio ""Nuestro rock"" y que llevó a encontrar a los oyentes el sencillo «Estoy aquí». Las canciones «Un poco de amor», «Pies descalzos, sueños blancos», «Antología», y «Se quiere, se mata» fueron también éxitos populares. El álbum alcanzó ventas superiores a los 4 millones de ejemplares que aumentaron todavía más con las remezclas de su álbum "Pies descalzos". Su primer álbum de remezclas, titulado "The Remixes", fue lanzado en 1997 y también incluyó algunas versiones en portugués de sus canciones más conocidas, y vendió alrededor de 500.000 unidades a nivel mundial.

En septiembre de 1998, Shakira publicó su segundo álbum de estudio, titulado "¿Dónde están los ladrones?", con un coste de producción cercano a los tres millones de dólares estadounidenses, y se estima que ha vendido más de 10 millones de copias extendiendo su fama a otros mercados y países como Francia, Suiza y, especialmente, a los Estados Unidos, ya que solo en ese país vendió más de 1 millón de ejemplares. Fueron especialmente exitosos los sencillos «Ciega, sordomuda», «Tú», «Inevitable» y «Ojos así».

Debido al éxito del álbum, Shakira fue invitada por MTV en 1999 para producir su primer álbum en vivo, titulado "MTV Unplugged", el cual incluye todas las canciones de "¿Dónde están los ladrones?" (Excepto la canción «Que vuelvas»). Su "desenchufado" o "unplugged" fue el primero en transmitirse en los Estados Unidos siendo interpretado por una artista latina y en castellano. El álbum vendió más de 5 millones de unidades a nivel mundial. Fue lanzado al mercado en febrero de 2000.

En marzo de 2000 inició una gira de tres meses denominada "Tour Anfibio", que incluyó conciertos en América Latina y los Estados Unidos. La gira comenzó en Perú y concluyó en Argentina. En agosto siguiente, ganó un "MTV Video Music Award" en la extinta categoría de "Premio Internacional del Público" con la canción "Ojos así".

En noviembre de 2001, la cantante publicó su tercer álbum de estudio y el primero "bilingüe", titulado "Laundry Service" (conocido como "Servicio de lavandería" en Hispanoamérica y en España), el cual fusionó pop y rock con elementos de la música latina (por ejemplo, tango y música andina) o elementos árabes. El álbum también incluyó cuatro canciones en español, entre ellas «Que me quedes tú» y «Te dejo Madrid», las únicas que no fueron traducidas del español al inglés. Aún con críticas con respecto a su adecuación a la lengua inglesa, "Laundry Service" fue un éxito comercial, con ventas que, paulatinamente, superarían las 13 millones de unidades en todo el mundo, como se dice en la página oficial de Shakira. Fue especialmente exitoso su primer sencillo, «Whenever, Wherever» (su versión en español se tituló "Suerte") que ocupó el número uno en las listas de éxitos de varios países, y alcanzó el número seis en el Billboard Hot 100, igualmente el sencillo tuvo éxito en Europa, en países como Francia o en Suiza, donde la canción se mantuvo en el primer lugar durante 17 semanas seguidas. El segundo sencillo, «Underneath Your Clothes», fue también exitoso en varios países de Europa, América y Asia, en Estados Unidos el sencillo ocupó el puesto número 9 en el Billboard Hot 100. Para los países de habla hispana, se lanzó «Te dejo Madrid» como segundo sencillo.

Meses después, «Objection (Tango)» (cuya versión en español es «Te aviso, te anuncio (tango)») se convirtió en el tercer sencillo del álbum con éxito moderado. Finalmente, para Europa y Australia se lanzó como sencillo la canción «The One», que no tuvo demasiada repercusión. Mientras tanto, para los países de habla hispana se lanzó «Que me quedes tú».

En 2002, Shakira publicó un disco compacto con sus mayores éxitos en español hasta ese momento, que contenía canciones de sus álbumes "Pies descalzos", "¿Dónde están los ladrones?", "MTV Unplugged" y "Servicio de lavandería" y cuyo título fue simplemente "Grandes éxitos". Logró ventas de más de 3,5 millones de ejemplares.

Debido al éxito de "Servicio de lavandería", entre el 2002 y 2003 se realizó la gira "Tour de la Mangosta" con la cual Shakira realizó 61 conciertos internacionales. El nombre de la gira proviene según la propia Shakira de que ""La mangosta es el único animal que es inmune al veneno de la cobra. Para mí, la cobra simboliza todo el odio, enfermedad y negatividad en el mundo. Deberíamos ser más como la mangosta y atacar la cabeza de la cobra"". En los conciertos, haciendo referencia a esto, aparecía una proyección donde se mostraba cómo la mangosta derrotaba a la cobra. El video estaba dividido en dos secciones y terminaba al final del concierto con la leyenda "Muerdan el pescuezo del odio. Maten a la muerte".

Shakira publicó en febrero de 2004, un DVD recopilatorio como documento de esa gira en un concierto en Róterdam, Países Bajos; junto con un disco de audio, titulado "Live & Off The Record" ("En vivo y en privado"). El DVD contenía quince canciones en directo y un documental; y en el CD de audio solo se incluían diez canciones. Las ventas llegaron a los 4 millones de unidades.

Shakira tardó año y medio en preparar este proyecto. La decisión de sacar dos álbumes simultáneamente no estaba premeditada, sin embargo, el exceso de material compuesto le hizo tomar esta decisión. Shakira considera este proyecto como su ""tesis de grado"".

Cuatro años después del lanzamiento de "Servicio de lavandería", Shakira lanzó su cuarto álbum de estudio, titulado "Fijación oral vol. 1", durante el mes de junio de 2005 en Europa, Norteamérica, Australia y Latinoamérica. El álbum llegó al primer lugar de ventas en México, Colombia, Argentina, España y varios países de Latinoamérica. En los Estados Unidos, "Fijación oral vol. 1" debutó en el número cuatro dentro de la lista Billboard 200.

El primer sencillo se tituló «La tortura» con la colaboración del cantante español Alejandro Sanz. La canción es básicamente pop latino pero con ciertas influencias del dance-hall jamaicano. Esta canción la llevó a ocupar los primeros lugares en las listas musicales en varios países y se mantuvo durante 25 semanas consecutivas en el puesto número uno de Hot Latín Tracks de Billboard.
El segundo sencillo fue «No», una balada que no tuvo tanta presencia internacional como su predecesora. «Día de enero» (lanzada solo en Latinoamérica) y «La pared» se convertirían en los dos siguientes sencillos. «La pared» fue número uno en España durante 4 semanas y «Día de enero» tuvo bastante éxito en Latinoamérica.

En 2005, Shakira se convirtió en la primera cantante latina en interpretar un tema musical en español durante la ceremonia de los y en ser nominada a dichos premios por un vídeo de una canción en español.

En 2006 fue galardonada con su segundo premio Grammy en la categoría de "Mejor Álbum Latino Rock/Alternativo". En ese mismo año, ganó cuatro premios Grammy Latino por este álbum.

Como tema principal de la entonces nueva campaña de automóviles SEAT, «Las de la intuición» empezó a sonar en España en 2007, país donde este tema se convirtió en el segundo tema más popular de "Fijación oral vol. 1", después de «La tortura», y se mantuvo en el primer puesto de la lista de éxitos de la emisora "Los 40 Principales" durante ocho semanas no consecutivas. En ciertos lugares de Europa tuvo algo de presencia su versión inglesa, "Pure Intuition". Fue el último sencillo de "Fijación oral vol. 1".
El 28 de noviembre de 2005, la cantante lanzó su quinto álbum de estudio y el primero completamente en inglés, titulado "Oral Fixation vol. 2", una continuación de su álbum "Fijación oral vol. 1". Debutó en la posición número cinco en los Estados Unidos dentro del Billboard 200, pero rápidamente se hundió en el listado, precipitando una reedición seis meses más tarde. Aun así el disco remontó llegando a la posición 23 como uno de los discos más vendidos de 2006 en EE.UU.

A principios de 2006, recibió la certificación de trece discos de oro de distintos países (Canadá, México, España, Argentina, Chile, India, Alemania, Austria, Italia, Portugal, Noruega, Hungría y su país natal Colombia).

Su primer sencillo fue «Don't Bother» que logró un éxito moderado en Europa, pero no tuvo el éxito esperado en el mercado anglosajón, lo que condujo a una reedición, medio año después, que se reforzó con la adición de «Hips Don't Lie» a "Oral Fixation Vol. 2" que fue el segundo sencillo. En esos seis meses, no hubo ningún sencillo de "Oral Fixatión Vol. 2".

«Hips Don't Lie», que alcanzó gran éxito a nivel mundial, convirtiéndose en la canción más descargada por Internet, es una canción con una mezcla de cumbia, pop latino, reguetón, salsa y hip hop, escrita originalmente por el rapero y productor haitiano, Wyclef Jean y cuyo título era "Dance Like This". Shakira decidió versionarla con ayuda del propio Wyclef. El sencillo se ha convertido en la canción más exitosa en inglés de Shakira, alcanzó la posición número uno en las listas de popularidad de varios países, en Estados Unidos, fue la primera canción de Shakira que alcanzó el número uno en la lista Billboard Hot 100. Con el lanzamiento de «Hips Don't Lie» como una canción extra del álbum, incrementó las ventas de "Oral Fixation Vol. 2" aún más alrededor del mundo, pero especialmente en los Estados Unidos donde dio un salto de la posición número 98 a la número 6 en el Billboard 200, lo que generó que se le otorgara la certificación de disco de platino el 4 de abril de 2006 al haber superado el millón de unidades vendidas en los Estados Unidos.

Shakira fue la elegida para cantar una versión especial de «Hips Don't Lie» (versión Bamboo) en la ceremonia de clausura de la Copa Mundial de Fútbol Alemania 2006, siendo ésta la primera vez en la historia que una cantante femenina latina canta en este evento.

En noviembre de 2006 lanzó como último sencillo «Illegal». Es una balada que cuenta con la participación de Carlos Santana. Tuvo un éxito moderado en Europa y su salida como sencillo fue cancelada en los Estados Unidos.

En noviembre de 2006 se colocó una estatua de Shakira en las afueras del estadio Metropolitano de fútbol de su ciudad natal Barranquilla, Colombia, donada por el artista de origen alemán Dieter Patt.

En el verano de 2006, arrancó el "Tour Fijación Oral", gira que la llevó a recorrer distintos países en cinco continentes, siendo esta la más exitosa de la colombiana, quien con su Tour de la Mangosta recorrió 61 locaciones. El tour fue presentado en 125 lugares alrededor del mundo, en varios de los cuales rompió récords de asistencia. Europa ha sido el continente en que más éxito ha cosechado esta gira, gracias sobre todo a dos importantes plazas, España con 15 presentaciones y Alemania con 10. En Latinoamérica, México fue el país con mayor cantidad de presentaciones (21 en total), 10 de ellas en una primera visita en 2006 y las restantes en su regreso de cierre en el 2007 dio un concierto en el Zócalo donde tuvo una audiencia de más de 210.000 personas, el concierto más grande en la historia de la Ciudad de México, seguido por Colombia y Puerto Rico, con tres cada uno. La gira terminó oficialmente el 14 de julio de 2007 en Nigeria, dentro del Thisday Festival. Pero en diciembre de 2007 realizó un último concierto en Georgia, que fue gratuito.

Aunque no formaba parte de la gira, Shakira se presentó en el Live Earth en Alemania, dónde interpretó cuatro canciones, una de ellas con la colaboración del cantante argentino Gustavo Cerati.

En 2008, firmó un acuerdo con la compañía de moda y perfumes española Puig para la comercialización de los perfumes que llevan su nombre.

"She Wolf / Loba" es el sexto álbum de estudio de Shakira. Según las propias palabras de la cantante colombiana, el álbum es ""electrónico, con mucha fusión y muy bailable"". El 29 de junio de 2009 se estrenó la versión en español del primer sencillo del álbum titulado «Loba» («She Wolf» en inglés). Loba fue escrita por Shakira (en la versión en español colaboró Jorge Drexler) y coproducido entre Shakira y John Hill. El 31 de julio de 2009 se estrenó el vídeo musical de “She Wolf” que fue dirigido por Jake Nava. La portada del álbum fue desvelada el día 9 de agosto de 2009. El álbum fue lanzado en octubre de 2009 en la mayoría de los países y en noviembre de 2009 en los Estados Unidos, recibió comentarios positivos de los críticos. El álbum ha sido un éxito en las listas y desde entonces ha sido certificado disco de oro en Rusia, Irlanda, Suiza, Polonia, Francia, Argentina, Grecia y Hungría, de Platino en España, Reino Unido y el Oriente Medio, 2 veces Platino en Colombia y México, y 3 veces Platino en Taiwán. Los sencillos posteriores, «Did It Again / Lo hecho esta hecho», Give It Up to Me y «Gypsy/Gitana», recibieron un éxito moderado en las listas.

Durante la Copa Mundial de Fútbol Sudáfrica 2010, Shakira se convierte en la intérprete de la canción oficial del evento deportivo. La canción, llamada «Waka Waka (This Time For África)», usa fragmentos de una canción militar africana, usada esta vez con el objetivo de unir a las personas a través de la música y el fútbol. Shakira la interpreta con la colaboración del grupo sudafricano Freshlyground. La canción se convierte en uno de los mayores éxitos de Shakira en Latinoamérica, España y el resto de Europa, teniendo un éxito moderado en países como Estados Unidos y Reino Unido.

Se anunció inicialmente que Shakira lanzaría un álbum en español en 2010 después del lanzamiento de "She Wolf", similar a lo que ella había hecho con Las Fijaciones 1 & 2, pero Shakira indicó más adelante que el álbum sería un proyecto bilingüe, que contendría una mayoría de canciones en español y tres en inglés. La portada del álbum fue develada la madrugada del 1 de septiembre de 2010 al igual que el primer sencillo titulado «Loca». Su séptimo álbum de estudio, titulado "Sale el sol", fue lanzado en octubre de 2010, debutando en las primeras posiciones. Se lanzaron oficialmente tres sencillos más durante 2010 entre los cuales están «Sale el sol», «Rabiosa» y «Antes de las seis» este último promocionando su DVD "", alcanzando altas y medianas posiciones. En 2012, se lanzó el sencillo «Addicted to You». Shakira confirmó una gira mundial titulada "Sale el Sol World Tour". Su sitio web anunció que "La gira llevará el espectacular show de Shakira a los espacios principales y una serie de ciudades a través del verano y el otoño". La gira empezó en septiembre de 2010 y finalizó en octubre de 2011, recorriendo más de 40 países en 106 fechas, interpretando canciones como "Años Luz" y "Gordita". En noviembre de 2011, Shakira recibió su estrella en el Paseo de la Fama de Hollywood y posteriormente en los , la academia galardonó a Shakira como la ganadora del prestigioso premio por sus logros artísticos, así como sus contribuciones filantrópicas. La cantante de 34 años de edad, es la más joven en recibir el premio Persona del Año, Shakira también estuvo nominada para tres premios más, en la cual se incluye "" el cual ganó.

Shakira fue entrevistada por Billboard sobre su próximo proyecto. Cuando se le preguntó acerca de ello, ella respondió:

"Ya he empezado a escribir nuevo material. He comenzado a explorar en el estudio de grabación cada vez que tengo tiempo en Barcelona y aquí en Miami. Estoy trabajando con diferentes productores y DJs, y trato de comer fuera de eso y encontrar nuevas fuentes de inspiración y motivación musical. Estoy ansiosa por regresar al estudio. Mi cuerpo está pidiendo por ello".

A mediados de febrero de 2012, se confirmó por medio de "Roc Nation" (discográfica de Jay-Z) que Shakira había firmado un contrato con esa disquera. Shakira confirmó por medio de su cuenta en Twitter que había empezado a grabar un nuevo álbum, este álbum seria completamente en inglés; en este posible octavo álbum de estudio, cuya publicación estaba pautada para mediados de 2013, tendría un sonido más Dance y Electrónico, y contaría bajo la producción de artistas como Ne-Yo, Afrojack, RedOne, William, entre otros.

En ese mismo año la artista firmó un contrato con la compañía internacional de telefonía móvil T-Mobile con el propósito de establecer una promoción mutua de sus productos. El 9 de octubre la cantante ofreció un concierto especial en conjunto con la compañía para promover el nuevo servicio de telefonía celular “Simple Global” y a su vez utilizarlo como margen para su regreso musical. Posteriormente la estrella acudió a una entrevista, como complemento del convenio, manifestando su apoyo e interés con las telecomunicaciones y las propuestas implementadas por la empresa, y afirmando su segunda participación en el programa de canto The Voice y el lanzamiento de su primer sencillo de su nuevo material para finales de dicho año. En diciembre salió a la luz una supuesta colaboración con la cantante barbadense Rihanna en el primer sencillo, el cual ya había sido postergado para comienzos del primer mes del año siguiente. El 6 de enero de 2014 Rihanna confirmó el nombre de la canción cuando respondió a través de su cuenta de Twitter a un fanático que había hecho una publicación enunciando que el sencillo se llamaría Can't Remember to Forget You y sería lanzado el 14 de ese mes, aunque la cantante planteó que el mismo sería lanzado un día antes de lo previsto (13 de enero). Días antes, el rapero cubano-estadounidense Pitbull había confirmado la colaboración de ambas cuando en una entrevista aseguró haberle ofrecido una participación en su canción Timber -que luego obtuvo la colaboración de la cantante Kesha- y ésta haberla negado ya que colaboraría en el nuevo trabajo musical de Shakira. La cantante colombiana reconfirmó a través de sus redes sociales horas después de la publicación de Rihanna toda la información del sencillo. Tres días después publicó la imagen de lo que sería la portada del sencillo. el día 13 se efectuó el lanzamiento comenzando a las nueve de la mañana en los Estados Unidos y finalmente el 30 de enero se lanzó el video oficial en su cuenta de VEVO en YouTube; también el 13 de enero, en horas de la tarde Shakira anunció que el 25 de marzo de 2014 sería la fecha de lanzamiento de su octavo álbum oficial de estudio. El 22 de enero, en una carta abierta la cantante divulgó que su nueva producción discográfica tendrá un título homónimo (Shakira); dos días después publicó la portada de lo que será la versión de lujo del álbum. El 13 de julio de 2014, Shakira cantó la canción "La La La (Brasil 2014)" junto con el músico brasileño Carlinhos Brown en la ceremonia de clausura de la Copa Mundial de Fútbol Brasil 2014 en el Estadio Maracaná. Esta presentación se convirtió en su tercera aparición consecutiva en la Copa Mundial de la FIFA.

El 14 de agosto de 2015, en la Expo D23 de Disney, se anunció que Shakira interpretaría a un personaje de la película animada Zootopia de Disney, interpretando a Gazelle, la estrella pop más grande de Zootopia. Shakira también contribuyó una canción original a la película, titulada "Try Everything", que fue escrito por la cantante australiana Sia.

Shakira comenzó a trabajar en su noveno álbum de estudio a principios de 2016. En mayo de ese año, colaboró con el cantante colombiano Carlos Vives en la canción "La Bicicleta". A finales de octubre, Shakira lanzó el sencillo "Chantaje" en colaboración con el cantante urbano colombiano Maluma; aunque la canción hacía parte del álbum, no fue considerada como el principal sencillo. En marzo de 2017 lanzó el sencillo "Deja Vu" al lado del cantautor estadounidense de origen dominicano Prince Royce y el sencillo 'Comme Moi' al lado del rapero francés Black M; más tarde en abril y mayo de 2017, Shakira lanzó respectivamente "Me Enamoré" como segundo sencillo oficial y Perro Fiel como el tercer sencillo de su nuevo álbum "El Dorado," el cual fue publicado el 26 de mayo de 2017. El lanzamiento oficial de "Perro Fiel" como tercer sencillo tuvo lugar en septiembre de 2017, misma fecha en que el vídeo musical fue publicado. Después de ser lanzado como sencillo, fue certificado Oro en España por vender 20 000 copias en agosto de 2017.

El Dorado World Tour fue anunciado el 27 de junio de 2017 y será patrocinado por Rakuten., contando como socios con la Global Touring Division de Live Nation Entertainment (la cual había colaborado previamente con la cantante en su gira Sale el Sol World Tour) y Citigroup, los cuales son, respectivamente, el productor y la tarjeta de crédito para la manga norteamericana del tour. Como parte de la presentación de la gira, Live Nation publicó un video a través de sus redes sociales oficiales como un recordatorio adicional.

La gira comenzó el 8 de noviembre en Colonia, Alemania, pero debido a problemas relacionados con el esfuerzo vocal realizado por la cantante durante los ensayos, la fecha fue cancelada un día antes y reprogramada para una fecha posterior. Para el 9 de noviembre, por la misma razón, Shakira anunció la postergación de las dos presentaciones en París, así como las siguientes programadas en Antwerp y Ámsterdam. El 14 de noviembre, anunció por medio de sus redes sociales que había contraído una hemorragia en su cuerda vocal derecha a finales de octubre, durante su última etapa de ensayos y que necesitaba mantener su voz en reposo para curarse, de esta manera, toda la manga Europea de la gira fue pospuesta.

Las fechas para Latinoamérica fueron confirmadas posteriormente. Hubo planes para llevar el tour a países tales como República Dominicana. Además, un periodista de la versión brasilera del periódico portugués Destak anunció en su cuenta de Twitter que la cantante colombiana visitaría Brasil durante el mes de marzo. Sin embargo, de acuerdo al mismo periódico, debido al período que requiere la cantante para la recuperación del daño en sus cuerdas vocales, las fechas en América Latina fueron pospuestas para la segunda mitad de 2018.

En enero de 2018 ganó su tercer Grammy Awards con el álbum El Dorado, siendo la única artista latina en conseguirlo. Lo ganó en la categoría de Mejor álbum pop latino. Las ventas de su álbum llegaron a los 4,000,000 copias a nivel mundial. Lo celebró con la publicación de la canción Trap con Maluma.

En enero de 2018, anunció las fechas finales de su gira El Dorado World Tour, su tramo en Europa inició el 3 de junio en Hamburgo y finalizó el 7 de julio en Barcelona. Tuvo un corto paso por Asia, el 11 y 13 de julio. También confirmó sus fechas en Norteamérica, iniciando el 3 de agosto en Chicago y finalizando el 7 de septiembre en San Francisco. Su última confirmación fue en mayo, donde anunció sus fechas en Latinoamérica, que inició el 11 de octubre en Ciudad de México y finalizó el 3 de noviembre en Bogotá, Colombia.

Actualmente la cantante se encuentra grabando su décimo álbum de estudio.
El 11 de enero de 2020, anunció a través de su cuenta de Instagram sobre su nuevo sencillo titulado "Me gusta", contando con la colaboración del cantante puertorriqueño Anuel AA. La canción fue lanzada dos días después.

El 2 de febrero de 2020, Shakira junto a Jennifer Lopez encabezaron el espectáculo de medio tiempo del Super Bowl LIV, que se llevó a cabo en Miami Gardens, Florida. El evento fue visto por más de 103 millones de personas en los Estados Unidos.<ref>


</doc>
<doc id="4716" url="https://es.wikipedia.org/wiki?curid=4716" title="Culebrón">
Culebrón

Culebrón puede referirse a:


</doc>
<doc id="4718" url="https://es.wikipedia.org/wiki?curid=4718" title="Marruecos">
Marruecos

Marruecos (en árabe: المغرب "al-Maġrib", ‘el país del occidente’; en bereber: ⵎⵓⵔⴰⴽⵓⵛ "Murakuč" 'La tierra de Dios') —oficialmente denominado Reino de Marruecos (en bereber: ⵜⴰⴳⵍⴷⵉⵜ ⵏ ⵎⵓⵔⴰⴽⵓⵛ "Tageldit-n-Murakuč"; en árabe: "al-Mamlaka al-Maġribiyya") o Reino Alauita de Marruecos— es un país soberano situado en el Magreb, al norte de África, con costas en el océano Atlántico y el mar Mediterráneo.

Marruecos se independizó de Francia y de España el 25 de marzo de 1956. Se encuentra separado del continente europeo por el estrecho de Gibraltar. Limita con Argelia al este —la frontera se encuentra cerrada desde 1994—, al sur con la República Árabe Saharaui Democrática, al norte con España, su principal socio comercial con el que comparte tanto fronteras marítimas como terrestres —Ceuta, Melilla y las plazas de soberanía. 

Por otro lado habría que destacar el punto discordante por la soberanía del Sahara occidental el cual no esta reconocido oficialmente como país independiente a todos los efectos. Ocupa parte del Sahara Occidental, tras la "marcha verde" de 1975, la firma del Acuerdo Tripartito de Madrid, y la interrupción del proceso de descolonización y abandono de España del territorio.

En 1984 la asamblea de la Organización para la Unidad Africana (OUA), instancia predecesora de la UA y de la cual Marruecos era miembro fundador, aceptó como miembro a la República Árabe Saharaui Democrática (RASD). Como respuesta, Marruecos se retiró de la organización. Es miembro de la Liga Árabe, Unión del Magreb Árabe, la Organización Internacional de la Francofonía, la Organización de la Conferencia Islámica, la Unión por el Mediterráneo, la Unión Europea de Radiodifusión, el Grupo de los 77 y el Centro Norte-Sur. Es también un aliado importante no-OTAN de los Estados Unidos. Además es el país donde es más estudiado el idioma español, con más de 80000 estudiantes según la información provista por el Instituto Cervantes en 2015.

Desde 1984 hasta 2017, fue el único país africano que no era miembro de la Unión Africana. El Estado marroquí fue readmitido con mayoría absoluta, el 30 de enero de 2017, durante la XXVIII Cumbre de la Unión Africana, sucedida en Etiopía.

El nombre completo del país en árabe puede traducirse como "El Reino Occidental". Al-Magrib, que significa "el Poniente", es comúnmente usado. Para las referencias históricas, los historiadores usan Al-Maġrib al-Aqṣà ("El lejano Poniente") para referirse a Marruecos, diferenciándola de la histórica región llamada Magreb y que incluye los actuales Túnez y Argelia. El término Marruecos en otras lenguas procede del nombre de la antigua capital imperial Marrakech, proveniente de la expresión bereber que significa "Tierra de Dios".

El actual territorio de Marruecos ha estado poblado desde tiempos de la Prehistoria, por lo menos desde el año 8000 a. C. haciéndolo el lugar más antiguo donde se ha hallado un cráneo de "Homo sapiens" según la publicación de la revista "Nature" en junio del 2017 «New fossils from Jebel Irhoud, Morocco and the pan-African origin of Homo sapiens», atestiguado por rasgos de la cultura capsiana, en tiempos en que Magreb era menos árida de lo que es actualmente. Muchos teóricos creen que la lengua bereber apareció al mismo tiempo que la agricultura, y fue adoptada por la población existente, así como los inmigrantes que la trajeron. Análisis más modernos confirman que varios de esos pueblos han contribuido genéticamente en la población actual, incluyendo, además a los principales grupos étnicos —es decir, bereberes y árabes— fenicios, sefardíes, judíos y africanos subsaharianos. En el periodo clásico Marruecos fue conocida como Mauritania, que no debe confundirse con el actual país de Mauritania.

El norte africano y Marruecos y Fez fueron lentamente dibujados dentro del mundo Mediterráneo por las colonias de intercambio y poblados fenicios en el periodo clásico. La llegada de los fenicios anunció un largo dominio en el Mediterráneo, aunque esta estratégica región formaba parte del Imperio romano, conocida como Mauritania Tingitana. En el siglo V, al caer el Imperio romano, la región cayó bajo dominio de vándalos, visigodos y, posteriormente, bajo el Imperio bizantino en una rápida sucesión. Sin embargo, durante este tiempo, los territorios de las altas montañas permanecieron bajo el dominio de los habitantes bereberes.

A finales del siglo , con la llegada del islam, se produjo la conversión de muchos bereberes y la formación de estados como el Emirato de Nekor, en el actual Rif. Uqba ibn Nafi, conquistador del Magreb en el siglo , alcanzó las costas atlánticas en las playas de Massa, se introdujo en el océano con su caballo y puso a Alá como testigo de que no quedaban más tierras por conquistar. El país pronto se despojó del dominio del distante Califato abasí de Bagdad en tiempos de Idrís I, que fundó la dinastía de los idrisíes en el año 789. Marruecos se convirtió en centro cultural y en la mayor potencia regional.

El país alcanzó su máximo poderío cuando una serie de dinastías bereberes reemplazaron a los idrisíes árabes. La primera fue la de los almorávides, a la que siguió la de los almohades, que dominaron Marruecos y gran parte del noroeste africano, así como grandes territorios de la península ibérica o al-Ándalus. Pequeños estados de la región, como los de los Barghawata y los Banu Isam, fueron conquistados. El imperio se derrumbó a causa de un largo periodo de guerras civiles.

Las ciudades imperiales de Marruecos son las cuatro capitales históricas de Marruecos: Fez, Marrakech, Mequinez y Rabat.

En 1260, fue la batalla de Salé.

En 1399, Tetuán fue atacada por Enrique III el Doliente de Castilla.

En los siglos y , Portugal emprendió una política expansionista en África, en lo que llamó el Reino del Algarve de Ultramar, tuvo como objetivo controlar la costa y acabar con la piratería. La primera iniciativa importante fue la toma de Ceuta (1415) a los benimerines; a esta siguieron la conquista de Arcila y Tánger (1471), Mogador (1506), Safí (1508) y Mazagán (1513). Pese a que los portugueses erigieron las poderosas fortificaciones, pronto tuvieron que abandonar progresivamente las plazas del sur, debido a los constantes ataques musulmanes.

Tánger fue cedida por los portugueses a Inglaterra en 1661, como parte de la dote de Catalina de Braganza, cuando esta princesa se casó con el rey Carlos II. Los ingleses, frente a la continua presión marroquí, decidieron abandonarla el 6 de febrero de 1684.

La dinastía alauí, que hasta entonces controlaba la región de Tafilalet, consiguió unificar bajo su poder en 1666 un país en aquel momento dividido, y ha sido desde entonces la casa que rige Marruecos. Los alauitas tuvieron éxito en estabilizar su posición, frente a la presión de españoles y otomanos, y aunque el reino era más pequeño que los anteriores en la región, seguía manteniendo sus riquezas. En 1684 anexionaron Tánger a su territorio.

Marruecos fue uno de los primeros países en reconocer a los Estados Unidos como una nación independiente en el año 1777. El Tratado de Amistad marroquí-estadounidense es considerado como el más antiguo tratado no quebrado de los Estados Unidos. Firmado por John Adams y Thomas Jefferson, ha estado en continuo efecto desde 1783. El consulado de Estados Unidos en Tánger es la primera propiedad que el gobierno estadounidense posee en el exterior. El edificio actualmente funciona como museo.

A medida que Europa se industrializaba, el norte de África, con su riqueza y su interés estratégico, se fue volviendo un objetivo cada vez más atractivo para las potencias colonizadoras. Francia mostró un fuerte interés en Marruecos desde 1830. En 1860, una disputa sobre el enclave español en Ceuta llevó a España a declarar la guerra. Victoriosa, España ganó un nuevo enclave y una ampliación del asentamiento de Ceuta. En 1884, España creó un protectorado en la costa atlántica sahariana frente a las islas Canarias.

En 1904 Francia y España establecieron zonas de influencia en el país. El reconocimiento por el Reino Unido de la esfera de influencia francesa en Marruecos provocó una fuerte reacción del Imperio alemán; la crisis de junio de 1905 fue resuelta en la conferencia de Algeciras, realizada en España en 1906, en donde se formalizó una "especial posición" francesa y se le confió la política de Marruecos a Francia y España en común. Una segunda crisis marroquí provocada por Berlín incrementó las tensiones entre las potencias europeas.

El Tratado de Fez (firmado el 30 de marzo de 1912) convirtió a Marruecos en un protectorado de Francia. Por el mismo tratado, a partir del 27 de noviembre del mismo año, los territorios del norte (en torno a las ciudades de Ceuta y Melilla) y del sur (fronterizos con el Sáhara Español) se convirtieron en el protectorado español de Marruecos. Francia y España controlan la Hacienda, el ejército y la política exterior de Marruecos en sus respectivos protectorados. En teoría éstos no suponen ocupación colonial, están regulados por tratados y Marruecos es un Estado autónomo protegido por Francia y España pero bajo soberanía del sultán. En la práctica Marruecos se convirtió en colonia de Francia y España, especialmente desde 1930. La ciudad de Tánger, junto al estrecho de Gibraltar, obtiene carácter de ciudad internacional por el estatuto de 1923.
Una parte de la población marroquí se opuso a la ocupación colonial europea. Los franceses tuvieron que luchar contra las tropas de Al Hiba entre 1912 y 1919 en la zona de Marrakech. Los rifeños se rebelaron contra el protectorado español de Marruecos al mando de Abd el-Krim y proclamaron República del Rif, ocupando la parte norte del actual Marruecos entre 1921 y 1927, lo que vendría a ser el antiguo Reino de Nekor. Esta república no reconocía al sultán de Marruecos y declaraba su total independencia. La coalición franco-española logró derrotar a la República del Rif tras el desembarco de Alhucemas y el uso de armas químicas contra la población en el transcurso de la guerra del Rif.

En el protectorado francés se produjo una llegada masiva de colonos franceses (llegaron a ser medio millón en 1950), que cultivaron las mejores tierras. Los colonos adquirieron esas tierras, en total 1 100 000 hectáreas, de varias maneras: apropiándose de las tierras comunales ("melk"), y obligando a los campesinos minifundistas marroquíes a venderles sus tierras. Por otro lado, el Gobierno francés construyó carreteras, puertos, ferrocarriles, redes de telefonía y fomentó la navegación aérea. También se explotaron minas de hierro, cobre, manganeso, plomo, zinc y, sobre todo, los fosfatos de Juribga y Yusufía. La colonización francesa causó la ruina de la artesanía autóctona, desplazamientos bruscos y abusivos de población activa y se preocupó poco o nada en invertir en vivienda y educación. Muchos soldados marroquíes ("goumieres") que servían en el Ejército francés colaboraron con las tropas europeas y estadounidenses en la Primera y la Segunda Guerras Mundiales. Los sultanes de Marruecos en esta época fueron Muley Yúsuf (1912-1927) y Mohammed V (1927-1961).

Bajo el protectorado francés aparecieron diversos partidos políticos nacionalistas que basaban sus ideales para una futura independencia marroquí en la Carta Atlántica elaborada durante la Segunda Guerra Mundial (una declaración entre los Estados Unidos y el Reino Unido que disponía, entre otras cosas, el derecho de la población a elegir la forma de gobierno bajo la cual quería vivir). El manifiesto del Partido Istiqlal en 1944 fue una de las primeras demandas públicas de independencia. Posteriormente, el partido, dirigido por Allal al-Fasi, tendría el liderazgo del movimiento nacional.

En los años cincuenta el nacionalismo marroquí se había extendido por Casablanca, Rabat, Fez, Tetuán y Tánger y gozaba del apoyo de la burguesía urbana y, más tarde, también del de los campesinos. El partido Istiqlal consiguió el apoyo de Mohammed V y de la Liga Árabe en 1950. En ese año Mohammed V pidió la independencia. En 1952 el caso de Marruecos se expuso en la ONU.

El 23 de agosto de 1953, Francia envió al exilio en Madagascar a Mohammed V. El sultán fue sustituido por el impopular Mohammed Ben Aarafa, cuyo reinado fue percibido como ilegítimo. El cambio suscitó una activa oposición al protectorado francés por todo el país y dañó la imagen exterior de Francia, que no consiguió respaldo para esta decisión. Los árabes se rebelaron contra los franceses. Durante el verano de 1955, se produjo una oleada de atentados terroristas contra objetivos franceses en Marruecos, que desencadenó la represión policial. El hecho más notable ocurrió en Uchda, donde los marroquíes acometieron a franceses y otros residentes europeos en las calles. El nuevo Ejército de Liberación ("Armée de Libération") realizó atentados el día 1 de octubre de 1955. La Armée de Libération fue creada por el Comité de Libération du Maghreb Arabe (Comité de liberación del Magreb Árabe) en El Cairo (Egipto) para constituir un movimiento de resistencia contra la ocupación por parte del FNL en Argelia. Su meta era la vuelta del rey Mohammed V y la liberación de Argelia y Túnez. Durante el otoño de 1955 Aarafa renunció al trono y Francia permitió el regreso de Mohammed V. Las negociaciones para la independencia marroquí comenzaron al año siguiente.

Todos estos acontecimientos favorecieron la solidaridad entre el pueblo y el rey recién regresado. Por esta razón, la revolución de Marruecos, conocida como «La Revolución del Rey y su Pueblo» es celebrada cada 20 de agosto.

Marruecos logró su independencia política de Francia y de España el día 2 de marzo de 1956; y el día 7 de abril del mismo año Francia abandonó oficialmente su protectorado en Marruecos. Con acuerdos con España en 1956 y 1958, Marruecos recuperó territorios antes controlados por dicho país. Marruecos es miembro de la ONU desde el 12 de noviembre de 1956. La ciudad internacional de Tánger fue reintegrada a través del Protocolo de Tánger el 29 de octubre de 1956. En 1957 emprendió la Guerra de Ifni para conquistar otros territorios coloniales españoles cedidos por el Tratado de Wad-Ras de 1860, por el sultán Mohammed IV. En 1958 Marruecos recupera de España la provincia de Cabo Juby. Hassan II se proclamó rey de Marruecos el día 3 de marzo de 1961. Marruecos se constituyó como una monarquía constitucional y de derecho divino al mismo tiempo. La monarquía es el referente nacional.

Durante los últimos años de Mohammed V se creó un código de libertades públicas (1958), hubo elecciones comunales (1960), se formó un gobierno de coalición nacional (1960), se creó un banco popular cuyos usuarios son los accionistas y que financia PYMEs y viviendas (1960). El Istiqlal sufrió una escisión en 1959 de la que nace la Unión Nacional de Fuerzas Populares (UNFP).

En 1963 estalló una breve guerra fronteriza entre Marruecos y Argelia tras el rechazo del presidente argelino Ben Bella a las reivindicaciones marroquíes sobre territorios bajo la soberanía del sultán de Marruecos que habían sido incorporados por el régimen colonial francés a su entonces colonia, Argelia, (Béchar en 1903 y Tinduf en 1934). El conflicto, desencadenado por Marruecos, recibió el nombre de Guerra de las Arenas.
Durante los años sesenta las tierras pertenecientes a los colonos europeos pasaron a los terratenientes marroquíes. También se fomentó la escuela, la formación profesional y la . El 7 de diciembre de 1962 se aprobó la Constitución, pero desde 1962 hubo un alejamiento entre el rey y los partidos políticos. En el fondo a Hassan II (1961-1999) nunca le gustaron ni el parlamento ni la democracia. En 1963 dimitieron los ministros del Istiqlal. Meses después hubo elecciones en las que el Istiqlal gana en el campo y la UNFP en Casablanca, Rabat y Agadir. Desde entonces y hasta 1996 el Istiqlal y la UNFP han estado en la oposición. Entre 1962 y 1990 la Dirección Nacional de Seguridad y la policía reprimen a la población, mientras que la corrupción campa a sus anchas en ambas instituciones. Entre 1965 y 1970, por causa de una rebelión popular en Casablanca, Hassan II suspendió la constitución y proclamó el estado de excepción. En 1970 se aprobó una nueva constitución hecha a medida del rey, a la que se opusieron el Istiqlal y la UNFP. Una tercera constitución se aprobó en 1972, según la cual la administración elige a un tercio de los diputados. Hassan sufrió entre 1971 y 1973 tres intentos de asesinato por parte del ejército.

El gobierno marroquí estuvo marcado por un gran malestar político, y su respuesta despiadada ante los movimientos opositores se ganó el nombre de "Años de plomo". El enclave español de Ifni al sur del país, volvió a ser parte de la nueva Marruecos en 1969.
Marruecos reclamó el territorio del Sáhara Occidental desde su independencia en 1956. En agosto de 1974, España, que desea retirarse, anuncia la organización de un referéndum de autodeterminación para 1975. Marruecos se opone a cualquier referéndum que podría conducir a la independencia del territorio y pide a la Corte Internacional de Justicia que se pronuncie sobre su reclamo. Al día siguiente, la Corte Internacional de Justicia dicta su opinión: reconoce que el territorio del Sáhara Occidental no fue Terra nulliusantes de la colonización de España y que tenía lazos legales de lealtad con Marruecos. Sin embargo, no encuentra ningún vínculo de soberanía territorial. Concluye que estos vínculos no pueden obstaculizar «la aplicación del principio de autodeterminación a través de la expresión libre y genuina de la voluntad de las poblaciones del territorio».
En noviembre del año 1975 Marruecos inició una marcha civil y pacífica llamada marcha verde, hacia el Sahara español (ahora Sáhara Occidental, disputado con el Frente Polisario), lanzado por el rey marroquí Hassan II para recuperarlo, considerándolo históricamente vinculado a Marruecos y como parte del Sahara marroquí. La Marcha siguió a la opinión consultiva de la Corte Internacional de Justicia emitida a petición de Hassan II, esta opinión reconoce los vínculos de lealtad parcial pero niega cualquier vínculo de soberanía y reconoce el derecho a la autodeterminación.

Esta toma de posesión del territorio no es reconocida por la ONU y el Sáhara Occidental es legalmente un territorio no autónomo (considerado aún no descolonizado) sin autoridad administrativa.
El 6 de noviembre se convirtió en el día de una fiesta nacional en Marruecos. 

Hubo elecciones legislativas en 1979 y 1984. En 1981 hubo revueltas en Casablanca por la subida de los precios. El balance arrojó varios centenares de muertos. En enero de 1984 hubo revueltas del pan en Nador y Tetuán que terminaron con un centenar de muertos. Marruecos estaba endeudado, por lo que en 1983 el BM-FMI impuso un Plan de Ajuste Estructural. Este consistió en la privatización de los fosfatos, la telefonía y la industria textil y la reducción de los derechos de aduana del 60 % al 45 %.

Entre 1985 y 1990 la situación se volvió crítica y problemática. El paro subió, el dirham cayó, hubo fuga de capitales y se recortaron las subvenciones a los productos de primera necesidad. Para pagar la deuda externa el FMI y el BM imponen recortes en sanidad y educación y el cese a la contratación de funcionarios. En esos años hubo huelgas y manifestaciones. Se procedió a la reforma política desde arriba en 1991. En 1993 hubo elecciones legislativas, en las cuales hubo fraude. Se realizaron amnistías de presos políticos en 1994 y 1996. En 1995 se reconoció la enseñanza en bereber. El 13 de septiembre de 1996 se reformó la Constitución: todo el parlamento se comenzó a elegir por sufragio universal, y se creó una cámara de consejeros (especie de senado) cuyos miembros son elegidos por las comunidades, ayuntamientos, regiones, sindicatos y asociaciones profesionales. En 1997 se celebraron todas las elecciones; las legislativas arrojaron una gran división política: el partido más votado consiguió el 13,8 % de los votos.

Marruecos consiguió el estatus de Aliado importante no-OTAN en junio de 2004 y firmó un tratado de libre comercio con los Estados Unidos y con la Unión Europea (2000).

En 1999 muere Hassan II, su hijo mayor Mohammed VI le sucede al trono y promete realizar profundos cambios democráticos. Ese mismo año reformó el código jurídico de la mujer y en 2004 el código de la familia: se sube la edad mínima para casarse de 15 a 18 años, queda abolida la poligamia, la tutela del padre o del hermano mayor sobre la mujer adulta no casada, y las mujeres pueden elegir esposo y pedir el divorcio en igualdad de condiciones con respecto a los hombres en lo relativo a la custodia de los hijos.

El Gran Marruecos es un concepto desarrollado por el gobierno de Marruecos en los años 1950 y 1960. Este último publicó un documento para reclamar territorios que han pertenecido históricamente a Marruecos: Mauritania, parte de Malí, parte del Sáhara argelino y Sáhara Occidental. La afirmación del Gran Marruecos se basa principalmente en diferentes dinastías que gobernaron Marruecos desde los almorávides. La galería de imágenes de abajo muestra la extensión máxima de cada una de estas dinastías: Almorávide, Imperio almohade,Sultanato Benimerín y el Sultanato saadí, además de la extensión máxima de Marruecos bajo la dinastía gobernante actual, la Dinastía alauí.

En las elecciones de 2002 el Partido de la Justicia y el Desarrollo (PJD), de carácter islamista, avanzó notablemente.

En mayo de 2003, la ciudad más grande del país, Casablanca, sufrió un atentado terrorista. El ataque se produjo en lugares relacionados con occidentales y judíos, y se produjo la muerte de 33 personas y más de 100 heridos, en su mayoría marroquíes. Este atentado provocó una involución de las libertades civiles: se amplió la prisión preventiva, la policía puede entrar en viviendas particulares sin orden judicial, interceptar el correo, las llamadas telefónicas y las cuentas corrientes. En las elecciones municipales de septiembre de 2003 el PJD sube al segundo lugar.

De acuerdo con la Constitución de Marruecos, el país es una monarquía constitucional, con un Parlamento electo. El rey de Marruecos tiene amplios poderes ejecutivos, con la posibilidad de disolver el gobierno y el Parlamento y dirigir las fuerzas militares, además de otros poderes. Los partidos políticos de oposición están permitidos y varios se han presentado a las elecciones en los últimos años.

Según un decreto de febrero de 2015, Marruecos está dividido en doce regiones (incluyendo las Provincias Meridionales

Las regiones están divididas en 75 provincias y prefecturas y 1503 municipios.

La defensa del reino alauí es responsabilidad de las Fuerzas Armadas del Reino de Marruecos, compuestas por:

Marruecos cuenta con cuatro cordilleras: el Rif, el Atlas Medio, el Gran Atlas y el Anti-Atlas. La montaña más alta es el Toubkal, que alcanza los 4162 metros de altitud. Entre el Rif y el Atlas Medio está el valle del Sebú. Desde Larache hasta Agadir está la llanura atlántica y entre la anterior y el Atlas Medio hay una meseta situada por encima de los 500 metros de altitud. Al sur del Antiatlas ya comienza el desierto del Sáhara. Los ríos principales: Sebú, Muluya, Um Er-Rbia, Tensift, Sus y Draa.

Entre los países del Magreb, Marruecos se distingue por la gran altitud de sus montañas y sus llanuras. El Alto Atlas tiene el pico más alto de Marruecos y de toda África del Norte, pero las superficies planas de Marruecos son mucho más extensas que en Argelia o Túnez. Tres grandes grupos de relieve deben ser distinguidos: las montañas, las llanuras y mesetas situadas al norte del Atlas Medio y las mesetas áridas situadas al sur del Anti-Atlas hacia el este y el sur. Las montañas mismas están formadas por dos conjuntos que difieren en su génesis y geomorfología.

El clima es mediterráneo en las costas y más continental en el interior, con una distribución claramente invernal de las precipitaciones (que oscilan entre 300 y 800 mm (y 1000 mm en la región de Tánger-Tetuán) y unas temperaturas de enero que rondan los 12-13 °C en la costa y los 10 °C en Fez, Mequinez, Oujda y Marrakech. Las temperaturas de julio rondan los 25 °C en la costa y lugares del interior. En las montañas las precipitaciones son más abundantes y las temperaturas más bajas. En el Sáhara el clima ya es desértico y muy árido.

La flora de Marruecos, con cerca de 4200 especies, es una de las más ricas del norte de África y una de las más diversas de la región mediterránea. Valga el dato de que solo en el norte de Marruecos, en la región tingitana y el Rif, se acaban de catalogar 2915 especies y 344 taxones infraespecícicos. Vegetación de tipo mediterráneo, escalonada en pisos altitudinales. Principales especies: encina, alcornoque, cedro, pino. El bioma dominante en Marruecos es el bosque mediterráneo, dividido por WWF en tres ecorregiones: el bosque mediterráneo norteafricano, al norte, la estepa arbustiva mediterránea, en el centro-este, y el bosque seco mediterráneo y matorral suculento de acacias y erguenes, al suroeste. También están presentes el bosque montano norteafricano de coníferas, la estepa de enebros del Gran Atlas, en las montañas, y la estepa del Sahara septentrional, más desértica, en el sureste.
Entre los parques nacionales de Marruecos, se encuentran el parque nacional de Sus-Masa, el del Toubkal, el de Tazekka, el de Iriki y el de Talassantane. Una de las especies de aves más amenazadas del mundo es el ibis eremita, cuya última población natural se encuentra en el parque nacional de Sus-Masa.

La costa de Marruecos bordea el mar Mediterráneo en el norte y el océano Atlántico en el oeste. Sin incluir su reclamo de Sahara Occidental, la línea costera de Marruecos es de 1835 km. Si se le incluye Sáhara Occidental, cuenta con un total de 3500 kilómetros, siendo la más grande del continente de África, y abarcando 500 km de costa en el Mediterráneo y 3000 kilómetros en el Atlántico. Las aguas marroquíes están consideradas entre los caladeros más ricos del mundo. En el 2014 según la guía mundial de las ciudades urbanas, la ciudad marroquí de Alhucemas en el norte del país fue elegida la séptima ciudad más bella del mundo en lo que se refiere a la belleza de sus playas, y también la playa Legzira alrededores de Sidi Ifni fue elegida como una de las mejores playas del mundo.

Puesto que el centro del país es muy montañoso, los ríos se dirigen hacia las tres vertientes que las rodean, la atlántica, la mediterránea y la presahariana. En esta última solo se encuentra un río relevante, el río Ziz, con su afluente el río Rheis, que desaparece en el desierto, en Argelia.

En la vertiente mediterránea destaca el río Muluya (Moulouya), la mayor cuenca hidrográfica de Marruecos, en el que se han construido dos presas, que dan lugar al embalse de Mechra Homadi (1957) y al embalse de Mohamed V (1967). Tiene como afluente el río Za.

En la vertiente Atlántica los ríos son más numerosos y más cortos y caudalosos. Destacan, de norte a sur, el río Lukus, el río Sebú, el río Bou Regreg, el río Oum Er-Rbia, el río Tensift y el río Sus.

Por último, al sur del Atlas se encuentra el río Draa, el más largo, con más de 1000 km, que encadena una serie de oasis y kasbahs, y desemboca en el Atlántico.

En cuanto a los embalses de Marruecos, se han construido en torno a 140 de grandes dimensiones, en su mayor parte para el regadío, y el proyecto es construir uno o dos grandes embalses al año hasta alcanzar el millón de hectáreas de regadío.

Marruecos tiene una economía bastante estable con un crecimiento continuo durante el último medio siglo. El PIB per cápita creció 47 % en los años sesenta alcanzando un crecimiento máximo del 274 % en los setenta. Sin embargo, esto demostró ser insostenible y el crecimiento se redujo drásticamente a sólo un 8,2 % en los años ochenta y 8,9 % en los años noventa.

El crecimiento real del PIB se espera un promedio de 5,5 % en el período 2009-13, vista las perspectivas en el turismo y la industria no agrícola, como el crecimiento de la demanda en la zona del euro, principales mercados de exportación de Marruecos y el origen de los turistas se prevé que sea más moderado. El crecimiento será muy inferior al 10,8 % niveles que son ampliamente consideradas como necesarias para tener un gran impacto en la pobreza y el desempleo. El crecimiento económico también se ve obstaculizado por los intermitentes períodos de sequía en el secano del sector agrícola, el mayor empleador del país.

El Dírham marroquí es la moneda oficial del Reino de Marruecos. Su código ISO 4217 es MAD. Se divide en 100 céntimos (en árabe سنتيما o سنتيمات "santimat"; sing.: سنتيم "santim"). El Banco Central de Marruecos se encarga de emitir los billetes y monedas.

Marruecos produce y fabrica coches desde 1959, y también camiones de transporte en Casablanca, donde la sociedad marroquí SOMACA produce modelos de coches como el Dacia Logan, Peugeot Partner, Citroën Berlingo y Renault Kangoo. En febrero de 2007 más de 5000 coches marroquíes fueron exportados a España y Francia. En 2007, el grupo Renault-Nissan firmó contratos con el gobierno marroquí para la construcción de una planta de fabricación de automóviles cerca de Tánger por un valor de mil millones de euros y que garantiza la exportación de 400 000 vehículos para 2012, lo que lleva a 440 000 vehículos exportados por año. A principios de 2009, una exención de los préstamos bancarios por importe de 400 millones de dólares (en un plan de inversiones de 500 millones de dirhams) fue dirigida por SOMACA con tres bancos marroquíes. Lo que aumentará la capacidad de producción anual de 45 000 a 90 000 al fines de septiembre.

Según las cifras de 2017, Marruecos es el país que más automóviles produce en el continente africano, Sudáfrica en segundo lugar. Mientras que Marruecos produjo 335000 automóviles, Sudáfrica fabricó 331000 unidades.

La red de autopistas de Marruecos tiene una longitud 1626 kilómetros (julio de 2015), 1804 kilómetros se espera para el 2016.
La última carretera que se construirá es el tramo final de la autopista A8, entre Berrechid y Juribga con una longitud total de 77 kilómetros, 16 de julio de 2015.
Además de los 1626 kilómetros de carreteras de peaje, Marruecos cuenta con 751 kilómetros de autopistas (2x2 o 2x3) libre para completar su red, que debería ampliarse a 1014 kilómetros en 2015. La red de autopistas es administrada por Autopistes du Maroc (ADM).

El sistema marroquí es administrado por la Oficina Nacional de Ferrocarriles (ONCF), se extiende sobre una longitud de 2110 kilómetros. La red de ferrocarriles marroquíes es de las más modernas de África. Se compone de un eje principal de orientación general norte-sur que une Uchda al noreste con Marrakech al sur a través de Fez, Mequinez, Kenitra, Rabat y Casablanca. Dos talleres principales están disponibles para el mantenimiento del equipo, en Casablanca (motor eléctrico) y Mequinez (diésel). También están bien equipadas y capaces de depósitos de mantenimiento rutinario en Marrakech, Uchda, Sidi Kacem, Rabat y Fez. Marruecos en 2018 tendrá una línea de alta velocidad, la primera en el continente africano, entre Tánger y Kenitra inicialmente para continuar a Casablanca, Essaouira, Marrakech y Agadir, y para el año 2035 se ha previsto la construcción de una segunda línea que unirá Casablanca, Mequinez, Fez, Taza y Uchda.

Rabat ha inaugurado sus dos líneas de tranvía en diciembre de 2010 (tranvía de Rabat-Salé). Casablanca ha inaugurado su tranvía en diciembre de 2012 en colaboración con el grupo francés Alstom, que proporciona la pista y un servicio de transporte, la compañía que opera el tranvía se llama Casaway (Casablanca Tranvía). También se estudia en la actualidad tranvías para las ciudades de Tánger, Fez, Nador, Marrakech, Uchda, Mequinez.

El futuro metro aéreo Casablanca es un proyecto actualmente en discusión, con la posibilidad de extender el proyecto a otras ciudades importantes como Tánger, Marrakech y Agadir.

Marruecos cuenta con varias instalaciones aeroportuarias que incluyen 18 aeropuertos internacionales, 10 aeropuertos nacionales y otros pequeños aeropuertos para uso militar o deportes aéreos.

Marruecos tiene costa sobre el Mediterráneo y el Atlántico, el sector marítimo cuenta con 38 puertos de la siguiente manera:

Trece puertos de comercio internacional; pasajeros o embarcaciones de seis puertos. Diecinueve puertos pesqueros o exportación de productos de la pesca. Seis puertos para pasajeros o embarcaciones. Tras el éxito del mayor puerto comercial de África (Tánger Med), Marruecos planea construir tres nuevos futuros puertos similares: Nador West Med, Kenitra Atlántico y el Dajla Atlántico.

El turismo en Marruecos está bien desarrollado, con una fuerte industria turística centrada en el país, la cultura y la historia. Marruecos ha sido uno de los países políticamente más estables en el norte de África, lo que ha permitido que el turismo se desarrolle. El gobierno marroquí creó un Ministerio de Turismo en 1985.
Marruecos en 2013 se convierte en el país más turístico de África al alcanzar el umbral simbólico de 10 millones de visitantes. Para 2020-2022, Marruecos ha fijado la meta de llegar a 20 millones de turistas, para estar en el "Top 20 destinos en el mundo", dijo el ministro marroquí de Turismo en el 2014. Además, Marruecos cuenta con nueve sitios declarados patrimonio de la Humanidad por la Unesco. Los destinos más visitados de Marruecos son: Rabat, Casablanca, Marrakech, Fez y Tánger.

La población marroquí es bereber en su mayor parte, incluso si este patrimonio étnico-cultural nunca se menciona oficialmente, excepto cuando se trata de la producción de alfombras.
La contribución de la población árabe ha permanecido numéricamente limitada, incluso pequeña, pero culturalmente significativa. A través del Islam, el idioma árabe se ha impuesto gradualmente hasta el punto de relegar las lenguas bereberes a un segundo plano.

Algunas ciudades han recibido a los moriscos expulsados de la península ibérica que se establecieron entre 1609 y 1613

También podemos mencionar la presencia de judíos españoles que llegaron a engrosar la comunidad aborigen a fines del siglo XV y que abandonaron el país en la década de 1960, después de haber jugado un papel importante en el país.

No existe un censo sobre la etnicidad ni datos muy fiables, eso es porque a partir de la década los setenta se inició un proceso de arabización sistemática de los pueblos bereberes, llegando a prohibir el uso de símbolos, así como los dialectos amazigh del país.
Como todos los pueblos mediterráneos Marruecos es un país heterogéneo en cuanto a las etnias, siendo la cultura árabe, la predominante, aunque han surgido movimientos nacionalistas bereberes como el Partido Democrático Amazigh ilegalizado por el ministerio del interior al considerarlo un partido étnico y por ende no representa a todo el pueblo marroquí. Como se ha mencionado anteriormente, Marruecos es un país heterogéneo por los pueblos que han estado en actual territorio, desde los fenicios o romanos en la antigüedad, los visigodos y vándalos en la alta edad media, los árabes, y los moriscos y judíos expulsados de España en el siglo XVI. Actualmente existe un número importante de extranjeros procedentes sobre todo de África subsahariana con origen de Malí, Senegal, Nigeria yy Gambia. y europeos que buscan una segunda residencia en Marruecos o bien por trabajo.

La religión mayoritaria es el Islam sunita, ya que es practicada por un 98,3 % de la población. Hay también presentes minorías cristiana (6 %), no relacionada con el cristianismo que se implantó en el territorio en tiempos romanos, judía (1 %; los judíos están presentes en el territorio del actual Marruecos desde los tiempos romanos) y de otros cultos (1,8 % en 2000). El rey es la máxima autoridad religiosa islámica, como comendador de todos los creyentes. En 2010, la afiliación religiosa en el país se estimó por el Foro Pew Research Center como el 99,9 % de musulmanes, con todos los grupos restantes representan tan sólo el 0,1 % de la población.

Se estima que hay entre 3000 y 8000 musulmanes chiitas, la mayoría de ellos residentes en extranjeros mayormente en Líbano o Irak. Los seguidores de varias ramas del Islam como los sufíes musulmanes de todo el Magreb y África Occidental se comprometen hacer peregrinaciones anuales conjuntas en el país. La comunidad bahá'í, ubicada en las zonas urbanas, los números son de 350 a 400 personas.



El árabe clásico es el idioma de la legislación aunque las leyes también se traducen al francés y a veces al español.

La lengua mayoritaria hablada por la población es el árabe marroquí, poco a poco influido por la llamada lengua culta. De facto existe una diglosia e incluso una triglosia, motivada porque la gran mayoría de la población entiende y habla el francés. El rey de Marruecos cuando quiere que un discurso llegue a todo el mundo se ve obligado a utilizar para el mundo árabe, el árabe clásico, y para el resto del mundo, el francés.

Las zonas berberófonas, en sus tres dialectos de las lenguas bereberes (tarifit, tamazight y tachelhit) utilizan su lengua diariamente.

En las ciudades de Tetuán y Nador el conocimiento y uso del español es elevado; también en la población saharaui del antiguo Sáhara Español; en la mayor parte de la población de Larache, Tánger, Alhucemas y Sidi Ifni es usual. Existen grupos de hispanohablantes en ciudades como Rabat, Agadir, Kenitra, Casablanca, Taza, Fez, Marrakech, Mequinez y Uchda. Actualmente, existen seis centros del Instituto Cervantes, siendo una de las mayores concentraciones en un solo país de esta institución encargada de la difusión de la lengua española en el mundo. La población hispanoparlante en Marruecos asciende a unos 7 000 000 personas en 2017.

Los medios de comunicación marroquíes (prensa, radio, televisión) están disponibles en árabe, francés y español. En buena parte de la zona norte del país se pueden sintonizar sin problemas las emisoras de radio y los canales de televisión españoles.

Marruecos es un país polilingüístico. En el país se hablan el árabe, el bereber, el español (en el norte) y el francés (mayoritariamente por el sur) y en ocasiones todos ellos pueden proceder de un único hablante. No obstante ello, no todas las lenguas tienen la misma aceptación oficial.

Para entender la situación actual de las políticas lingüísticas en Marruecos hay que retroceder en el tiempo hasta el siglo XX porque estas, en cierta manera, fueron el resultado de la colonización. Después de la independencia, el gobierno pone en marcha una política de arabización que consistió en un proceso cuyo objetivo era reivindicar la vuelta a la lengua árabe, como forma de identidad lingüística, religiosa y cultural. En la Constitución de 1962, el Estado establece el árabe como lengua oficial. Para llevar a cabo este proceso, el Gobierno lo implementa en distintos ámbitos, a saber: sistema educativo, vida pública y medios de comunicación, estos tres ámbitos se convierten en el punto fuerte para llevar a cabo este proceso. En esta política no se tiene en cuenta el carácter multicultural del país. El gobierno marroquí pretendía seguir el modelo francés, y por lo tanto su objetivo principal era el monolingüismo como forma de identidad. Se adopta así la lengua árabe como identidad lingüística, religiosa y cultural del país; lengua que todos los pertenecientes a ella debían saber, destacando esta por encima del "amazigh" el francés y el español. Este proceso se consigue en todos los ámbitos excepto en el institucional, en el que el francés predomina hasta la actualidad. En el ámbito de la educación este proceso se implementa de manera más lenta; primero empieza en educación primaria, después pasan a la educación secundaria, y ya más tarde y con menos éxito a la educación universitaria. En el año 1988 el Estado arabiza todos los niveles de enseñanza primaria, secundaria y parte de la enseñanza universitaria en algunas facultades de letras.

Tanto las lenguas bereberes como el español (siempre en menor medida) pasan a un segundo plano ya que el gobierno considera que pueden interferir en el proceso de unificación lingüística, cultural y religiosa. A causa de las continuas reivindicaciones del pueblo bereber, en el año 2001 el rey Mohamed VI de Marruecos pronuncia un discurso en el que reconoce la diversidad lingüística de Marruecos y la pluralidad identitaria en el país:
Después del discurso, se crea una organización llamada Movimiento Cultural Amazigh (ACM) cuyo objetivo era defender los derechos del pueblo bereber, y se crea un nuevo modelo cuya finalidad era llevar la lengua amazigh a la enseñanza pública. Se produce así un cambio en la política lingüística de Marruecos. Se pasa del monolingüismo por el que desde la independencia se ha estado trabajando a una apertura de las lenguas otorgándole derecho a la lengua bereber. En la constitución de 2011, la lengua amazigh pasa a ser lengua cooficial del país junto con el árabe.

En varias encuestas que se han hecho a los habitantes de esta zona del Magreb, se ha reflejado el interés que tienen estos por aprenderlas. Veamos alguno de los resultados de estas encuestas:
Según un estudio realizado por R. Loulidi Mortada, para los jóvenes marroquíes es importante el bilingüismo. El 94,6 % de los encuestados creían que es importante saber hablar más lenguas, entre ellas el francés, el inglés o el español; mientras que solo un 2,5 % estaba en desacuerdo. Un 96,2 % quería que sus hijos, en un futuro, hablaran lenguas como el francés o el español. La presencia actual de la lengua española, sobre todo en la zona norte, es importante porque, además de las instituciones que tienen entre otros objetivos la promoción del español, como el Instituto Cervantes, existen otras formas que desempeñan un papel fundamental en la influencia cultural española en la zona, como son los medios de comunicación. En la zona norte se captan TVE, Antena 3, Cuatro, Tele5, La Sexta y Canal Sur, además de la televisión marroquí que emite también un informativo en español. Por ser el español una de las lenguas con mayor difusión a nivel internacional hay hablantes que prefieren su uso en algunos ámbitos. Además, el poder hablar y escribir esta lengua puede facilitarles el acceso a otros servicios y darles prioridad principalmente en los ámbitos profesionales. Asimismo, la numerosa inmigración hacia España (la población marroquí es la principal minoría extranjera) contribuye al aumento de hispanohablantes marroquíes, de ahí que el uso del español esté permitido en todas las oposiciones y exámenes que organiza el Estado para el acceso a cualquier puesto en la Administración Pública desde hace más de siete años. Un dato curioso es que según los resultados obtenidos de la Consejería de Educación y Ciencia de la Embajada de España en Marruecos, Marruecos es donde más invierte el Gobierno español para la difusión del castellano en todo el mundo. Aunque parece ser que el español es una lengua que se usa mucho en el territorio marroquí, el francés lo supera con creces.

A diferencia del español, el francés se usa en la Administración junto al árabe moderno y al mismo tiempo se usa en la enseñanza obligatoria, el cual es uno de los idiomas que se imparte desde los primeros niveles de enseñanza básica. También es la lengua vehicular de enseñanza en las facultades científicas y técnicas, en la facultad de Derecho y Ciencias económicas y en las Escuelas y Colegios Superiores. En las grandes ciudades, sobre todo en aquellas que estuvieron bajo el protectorado francés, esta lengua es hablada por la clase media-alta que ha sido educada en francés o como dice Francisco Moscoso «busca asimilarse a la cultura francesa como forma de acercarse a la modernidad aunque sin dejar de ser marroquíes». Las familias de clase media-alta suelen escolarizar a sus hijos en colegios privados franceses. Ennaji afirma que un estudiante marroquí que haya pasado la selectividad en francés y que haya estudiado en una escuela francesa privada, tendrá más oportunidades laborales que uno que lo haya hecho en una pública. Por lo tanto, como queda de manifiesto, el francés se ve como una lengua prestigiosa, mientras que el árabe no. Como ocurre con el español, el francés es usado en los medios de comunicación, habiendo en este último caso dos emisoras: Mediterráneo Internationale y Radio Magreb, las cuales emiten en árabe estándar moderno y francés. Además, también existe una cadena de televisión 2M que mayoritariamente emite en lengua francesa. Lo más notable de esta situación es que a pesar de su amplio uso y su función como lengua vehicular, la lengua francesa no tiene ningún estatus institucional u oficial, aunque el rey y el resto de autoridades en sus comparecencias fuera del mundo árabe se dirigen en ese idioma; es decir, que no está presente en la Constitución marroquí como lengua nacional ni como lengua oficial.

Por todo esto, podemos decir que el francés no ha quedado solo como una huella de la colonización francesa, sino también como un puente que tiene acceso a la modernidad, al mundo de Occidente.

Los "imazighen" son los pueblos indígenas del norte de África y aunque su origen es aún confuso, hay pruebas de que ya habitaban lo que hoy conocemos como el Magreb en la época de los fenicios. Durante el periodo de la colonización, se les designaba con el término bereber, que significa literalmente bárbaro. Hoy, se hacen llamar "amazigh", en español «hombre libre» y juegan una pieza clave dentro de las políticas lingüísticas de Marruecos. Es importante, antes de seguir, aclarar el significado de los términos que vamos a utilizar ya que pueden dar lugar a error. El término "amazigh", cuyo plural es "imazighen", por un lado hace referencia a la cultura y a la población de los pueblos autóctonos del norte de África y por otro engloba todas las diferentes lenguas que hablan (rifeño, "tashelhit", "tanfusit", tuareg…). Por cuestiones políticas, en los últimos años se ha querido utilizar la expresión "tamazight" para referirse a la lengua, pero este cambio no ha calado en la población, que sigue haciendo referencia al vocablo "amazigh". En España y en general en todo Occidente se las conoce también como lenguas bereberes. Aunque son, fundamentalmente, de tradición oral, hoy en día muchas de ellas tienen forma escrita gracias a un alfabeto llamado "tifinagh". Teniendo en cuenta todo lo anterior y por cuestiones de uso, nos referiremos a la lengua con los términos "amazigh" y lenguas bereberes.

La causa "amazigh" está envuelta en una gran polémica, ya que aparte de ser conflicto sociolingüístico, en los últimos años, se ha convertido en una guerra política. El gobierno marroquí aseguró tras el censo del año 2004 que solo el 28 % de la población habla "amazigh", mientras que los grupos que defienden los derechos del pueblo bereber aseguran que los hablantes de esta lengua representan más del 65 % (Handaine, 2013. El mes de septiembre del año 2014, semanas previas al sexto censo de población de Marruecos, la Asamblea Mundial Amazigh declaró en una rueda de prensa en Rabat como «en 2004 el Alto comisionado del censo falsificó el número de personas que hablan el "amazigh"». La Asociación afirma que el método que se sigue tiene el claro objetivo de «minimizar el papel de los bereberes en Marruecos». Alguno de los fallos que destaca la Asociación es preguntar solo a los ciudadanos "amazigh" si saben leer o escribir en su lengua materna, cuando en la Constitución del 2011 se establecieron como lenguas cooficiales el árabe y el "amazigh" siendo por tanto patrimonio de todos los magrebíes no solo de los de origen bereber.

El hecho de que la lengua "amazigh" se declarase lengua oficial del estado en la Constitución de 2011 es, sin duda alguna, un gran logro. Pero aún queda mucho por hacer, ya que el mero cambio de política no conlleva un cambio en la situación real. Por cuestiones legales y jurídicas el "amazigh" aún no tiene la presencia que debería como lengua oficial en el ámbito institucional. También, por falta de interés por parte del Ministerio de Educación, en el año 2012 la enseñanza de estas lenguas en los centros no tuvo éxito. Se dejó en manos de los directores la responsabilidad de fomentar y crear programas para su instrucción, cuando esto, como es lógico, debería haber venido de manos del ministro y de sus colaboradores. De igual manera sucede en la universidad, a la que solo se puede acceder si el alumno habla árabe y francés, lengua europea que no consta de reconocimiento en la Constitución. Tampoco se pueden utilizar las lenguas bereberes en los tribunales ni en el Parlamento. El 30 de abril de 2012 durante una sesión parlamentaria, Fátima Chahou habló en su lengua materna: el "amazigh". A raíz de este suceso, el presidente de la Cámara prohibió su uso a pesar de ser oficial alegando que la causa principal era la falta de intérpretes. Este hecho ha sido calificado de inconstitucional y "atentado contra la dignidad del bereber", ya que niega a miles de personas que la utilizan como lengua vehicular el derecho a saber qué está ocurriendo en su país. Además, como toda lengua materna, esta tiene una gran carga emocional y personal, así que muchos prefieren utilizarla en su día a día fuera del ámbito escolar y seglar.

Además, los "amazigh" se han tenido también que enfrentar a discriminación por parte del gobierno. El Alto Comisionado de Aguas y Bosques les impide la utilización de las tierras que han sido suyas durante siglos desde el año 2012. Actualmente existen asociaciones como el Congreso Mundial Amazigh, el IPAAC o el IRCAM que luchan por conservar el "amazigh."

La cocina marroquí es una cocina mayormente de dieta mediterránea que se caracteriza por su variedad de origen principalmente árabe y bereber con platos influenciados por la cocina judía. A pesar de su común con las cocinas de otros países del norte de África, la cocina marroquí ha mantenido su originalidad y sus características culturales únicas. La cocina marroquí ofrece una variedad de platos: cuscús, tajín, pastela, cordero asado, briouats (pequeños pasteles triangulares rellenas de carne o pescado). Hay además, otros platos típicos de Marruecos: "mrouzia", "tanjia marrakchí" (tradicional en la región de Marrakech), "harira" (sopa para romper el ayuno en Ramadán), la "seffa", ensalada marroquí, "shebbakiyya".

Marruecos es un país con una diversidad étnica y una rica herencia histórica y cultural. A través de los años, ha sido el hogar de varios grupos humanos provenientes del oriente (fenicios, cartagineses, judíos y árabes), del sur (subsaharianos) y del norte (romanos, bárbaros, andalusíes, moros y judíos). Todas estas civilizaciones han tenido un impacto en la estructura social de Marruecos, donde conviven diversas formas de creencias, desde paganismo, judaísmo hasta los cristianos y el islam.

Desde la independencia ha existido un florecimiento en las artes marroquíes, especialmente en la pintura, la escultura, la música popular, el teatro y la cinematografía. Además, por todo el país se celebran distintos festivales de arte y música, sobre todo durante el verano. Cada región posee características culturales propias que le brindan su identidad, y que al mismo tiempo contribuyen al legado histórico-cultural de la nación. Por esta razón, el gobierno marroquí ha puesto entre sus prioridades la protección, conservación y promoción del patrimonio cultural de Marruecos.

Marruecos está habitado principalmente por los árabes, junto con bereberes y otras minorías. Su música es predominantemente árabe, al-Ándalus y otras influencias importadas han tenido un efecto importante sobre el carácter musical del país. Los géneros tradicionales más populares en el centro del país son: andalusí, Chaabi y Gnawa mientras tanto en el norte es el género "reggada" (es un género musical marroquí de la región del Rif oriental, el noreste de Marruecos), este género proviene de una antigua danza para la guerra.

Marruecos junto a Etiopía (9) cuenta con más monumentos declarados Patrimonio de la Humanidad por la Unesco en África y el primero del mundo árabe. En la actualidad cuenta con nueve bienes declarados. Por otro lado, Marruecos cuenta con cuatro bienes culturales inmateriales, lo que la convierte en el primer país de África, junto con Argelia, en número de bienes declarados en la .

En Marruecos se celebran fiestas religiosas que siguen el calendario musulmán y fiestas nacionales que siguen el calendario gregoriano.

En el mes de septiembre de 1997 surge la Asociación de Escritores Marroquíes en Lengua Española con sede en Larache. Su primer presidente fue Mohamed Bouissef Rekab; actualmente la preside Mohamed Akalay; el secretario General es Mohamed Sibari; los vocales son: Mohamed Mamún Taha, Nasir El Moussati, Mohamed Lahchiri, Abderrahmán El Fathi, Abdellatif Limami, Mulay Ahmed El Gamoun, Mohamed El Khoutabi, Ahmed Oubali, Sara Alaui, Fatima Zohra Koui, Abdeljalil Rusi El Hassani y Mohamed Chakor.

La escuela es obligatoria en Marruecos para los niños menores de quince años. La tasa de analfabetismo de la población pasó de 43 % en 2004 al 28 % en 2012. En 2014, 53 % de los marroquíes son analfabetos, una tasa que alcanza el 71 % en las zonas rurales. El sistema educativo está marcado por una fuerte desigualdad. El sistema de educación pública arabizados en los años 80 es criticado regularmente por sus resultados y su pedagogía. Las familias de la burguesía y la clase media prefieren enviar a sus hijos a las escuelas privadas en la lengua francesa. La Universidad de Qarawiyyin ubicada en la ciudad de Fez, es considerada la institución universitaria más antigua todavía en funcionamiento según el Libro Guinness de los récords.

Tradicionalmente, los deportes más populares en Marruecos se centraban en el arte de la equitación hasta que los deportes europeos —como el fútbol, el polo, la natación y el tenis— se introdujeron a finales del siglo XIX. Ahora el fútbol es el deporte más practicado en el país, muy popular entre los jóvenes de las zonas urbanas, y en 1986 Marruecos se convirtió en el primer país musulmán y del continente africano en clasificarse a la segunda ronda del Campeonato Mundial de Fútbol.


El Reino de Marruecos posee una serie de elementos emblemáticos definidos por ley.



</doc>
<doc id="4721" url="https://es.wikipedia.org/wiki?curid=4721" title="Bereber">
Bereber

Bereber puede referirse a:


</doc>
<doc id="4724" url="https://es.wikipedia.org/wiki?curid=4724" title="Historia de al-Ándalus">
Historia de al-Ándalus

En el siglo VII, los musulmanes habían comenzado una rápida conquista en la que ocuparon Oriente Medio y el norte de África, llegando a la península ibérica a principios del siglo siguiente, en el marco del último proceso expansionista del Califato Omeya de Damasco.

En el año 711, el Reino Visigodo fue dividido entre dos candidatos luchando por el título de Rey de los Visigodos después la muerte de Witiza en 710: Roderigo, nieto de Chindasvinto y "dux" de Betica quién fue elegido por la nobleza visigoda en Toledo, y Agilla II del Ducado de Tarraconense. En este mismo año, tropas del Califato Omeya, compuestas por árabes y bereberes, cruzaron el estrecho de Gibraltar dirigidos por Tariq, lugarteniente del gobernador del Norte de África, Musa ibn Nusayr. En principio Tariq se atrincheró en el peñón que recibiría después su nombre Chabal Tariq, (Gibraltar), a la espera de la llegada del grueso de sus tropas. Sólo entonces inició su ofensiva con la toma de Carteia (Cádiz), después de lo cual se dirigió al oeste e instaló su base de operaciones en Al-Yazirat Al-Hadra, (en árabe: الجزيرة الخضراء) lo que hoy es Algeciras.

En ese mismo año Tariq vence a los visigodos en la trascendental batalla de Guadalete, y tras dar remate a lo que quedaba del ejército rival en Écija emprende una rápida conquista, primero en dirección a Toletum (Toledo), y posteriormente hacia Caesar Augusta (Zaragoza). Hacia el 718 la península ibérica, salvo las zonas montañosas del norte habitadas por vascones, cántabros y astures, estaba en manos del Califato Omeya.

Desde 716 la Península fue dirigida desde "Qurtuba", Córdoba, por un gobernador (wali) nombrado por el califa de Damasco. Los primeros gobernadores aparte de organizar el estado islámico y asentar a inmigrantes árabes, sirios y sobre todo bereberes, llevaron a cabo expediciones contra el reino franco hasta que después de la batalla de Poitiers en el 732, los francos emprendieron diversas campañas que expulsaron a los musulmanes de las tierras situadas al norte de los Pirineos hacia el 759.

En el territorio de al-Ándalus, los musulmanes respetaron a la población cristiana y judía a cambio de un tributo, por pertenecer a una de las religiones abrahámicas, que los dotaba de un estatus determinado, la "dhimma". En el caso de los cristianos si no pagaban tributo eran condenados a muerte. Este establecía que, aunque no formaran parte de la umma, comunidad islámica, quedarían protegidos, tendrían sus jueces y conservarían sus ritos. Estas circunstancias motivaron una política de pactos de capitulación donde muchos aristócratas visigodos pudieron conservar propiedades e incluso cierto grado de poder mediante nuevas fórmulas, como es el caso de Teodomiro (en árabe: تدمير Tūdmir), gobernador de la Provincia Carthaginense, que tras un acuerdo gobernó a título de rey un territorio cristiano visigodo autónomo dentro de al-Ándalus, denominado kora de Tudmir.

Este hecho, unido a que una parte de la población, cristianos unitarios y hebreos sobre todo, vieran con buenos ojos el nuevo poder musulmán que los libraba de la dura opresión que los visigodos habían ejercido contra ellos, podría explicar la rapidez de los moros 

La composición social de al-Ándalus fue muy compleja y varió a lo largo de su historia; por un lado se encuentran los que pertenecían a la comunidad islámica, Umma, que se dividían en libres y esclavos y étnicamente en árabes, sirios, bereberes, muladíes (cristianos conversos al islam y sus descendientes) saqalibas (de origen eslavo y que podían ser esclavos o libres), y también esclavos provenientes de África, aunque estos nunca llegaron a constituirse como un grupo social diferenciado. Entre los que no pertenecían a la Umma estaban los judíos y los mozárabes (cristianos de al-Ándalus).

En el año 750, en Damasco, la familia de los abasídas desplaza a los omeyas del poder, matando a todos sus miembros excepto a Abderramán I, y trasladan el poder a Bagdad.

En 756 Abderramán I huye a la península ibérica y consigue que ésta se separe del poder de Bagdad, haciendo que Córdoba se convirtiera en un emirato independiente. En la segunda mitad del siglo IX se erige la alcazaba de Madrid como defensa de Toledo.

La creación de los reinos de Asturias y de Pamplona, y de diversos condados en la zona pirenaica por parte de los francos conocidos como la Marca Hispánica (Aragón, Ribargoza, Girona, Barcelona, Osona, Ampurias, etc.), a finales del siglo VIII y primeros años del IX representó la primera reducción del territorio de al-Ándalus. Hasta el siglo XI, las fronteras entre al-Ándalus y los estados cristianos del norte experimentaron pocas variaciones, aunque la lucha entre ellos fue frecuente.

El estado andalusí estaba dirigido por visires (ministros) bajo la dirección del hagib el de más rango de ellos. También se formó un ejército profesional compuesto por mercenarios.

A comienzos del año 929 (final del año 316 de la hégira), el emir Abd al-Rahman III proclama el califato de Córdoba, y se nombra a sí mismo Emir al-Muminin (príncipe de los creyentes), lo cual le otorga, además del poder terrenal, el poder espiritual sobre la umma (comunidad de creyentes), de este modo se convirtió en el primer califa independiente de la Península. Por otra parte, la naturaleza misma del poder dinástico cambió a causa de este acontecimiento, y el alcance histórico, reconocimiento y adhesión del pueblo a los califas de al-Ándalus fue inmenso.

Este importante acontecimiento histórico encuentra sus fundamentos en la victoria definitiva que el poder cordobés había logrado unos meses antes sobre la interminable revuelta de Omar Ben Hafsún con la toma de Bobastro en enero del 928. Así mismo, se logró el restablecimiento de la autoridad del poder central de Córdoba sobre la mayor parte del territorio y la rendición de las últimas disidencias como la de Badajoz y de Toledo.

Dentro del contexto general del mundo musulmán en los primeros decenios del siglo X, hay otra causa del acontecimiento que es la creación del califato fatimí proclamado en 910 en Qairawan, norte de África, opuesto al abbassí; sin duda ésta fue una justificación implícita de la instauración del título califal en al-Ándalus.
La relación con los reinos vecinos fue tensa; por una parte se encontraba el califato fatimí en las fronteras cordobesas del norte de África; en el año 931, las tropas andalusíes entraron en Ceuta, donde se levantaron fortificaciones importantes. Desde entonces se establecieron tanto en Ceuta como en Melilla guarniciones andalusíes con carácter permanente. El califato omeya desplegó grandes esfuerzos para contener lo mejor posible el avance fatimí, siguiendo en su política de alianzas con las tribus Magrawa-Zanata del Magreb occidental, hostiles a los Sanhaya del centro que sostenían el poder fatimí.

Por el norte se encontraban los reinos cristianos que seguían con sus incursiones en territorio andalusí aprovechando cualquier debilidad del emirato cordobés. En el 932 Ramiro II atacó Madrid y derrotó a un ejército musulmán en Osma en el 933. Aliándose con el poderoso gobernador tuyibí de Zaragoza. Abd al-Rahman III intentó restablecer la situación del lado cristiano organizando una campaña contra el reino de León para restablecer la supremacía musulmana sobre la frontera del Duero. Abd-el-Rahman no alcanzó su objetivo y sufrió una derrota en la batalla de Simancas, seguida de otra en el barranco de Alhándega, aunque estas derrotas no tuvieron, de hecho, graves consecuencias territoriales porque igualmente se consiguieron otras victorias de importancia, los problemas internos paralizaron León y porque el poder cordobés, con su tenacidad, logró mantener una presión lo suficientemente fuerte sobre la frontera, y desplegó un gran esfuerzo para protegerla, edificando nuevas defensas y fortificando las ya existentes.

Abd al-Rahman III mandó edificar en el año 936 la ciudad palatina de Medina Azahara donde se trasladó con su gobierno y la corte.

Cuando llega al poder Al-Hakam II el Califato cordobés se encuentra consolidado tanto en el norte de la Península, con los reinos cristianos bajo vasallaje, como en el Magreb occidental, controlado por el Califato cordobés, bien mediante sus propias tropas, bien por medio de tribus aliadas o sometidas.

A su muerte, Al-Hakam II dejó el trono cordobés a un muchacho de once años sin ninguna experiencia política llamado Hisham, este joven califa contaba con el apoyo de su madre la concubina Subh de Navarra y el ministro Al-Musafi, además de la de un hombre llamado Abi Amir Muhammad, futuro al-Mansur (Almanzor para los cristianos), que mediante intrigas y movimientos políticos va ascendiendo en el poder hasta hacerse con el poder absoluto. Al-Mansur puso en marcha un programa de reformas en la administración civil y militar y supo atraerse a las clases populares con una política de intensa actividad militar contra los cristianos del norte.

Al-Mansur inició una serie de campañas o algaradas que se adentraron en territorio cristiano, llegando hasta Santiago, Pamplona, etc. Esta política provocó que los reinos cristianos crearan una coalición contra al-Ándalus.

Las taifas (palabra que en árabe significa "bando" o "facción") fueron hasta treinta y nueve pequeños reinos en que se dividió el califato de Córdoba después del derrocamiento del califa Hisham III (de la dinastía omeya) y la abolición del califato en 1031, como consecuencia de la guerra civil.

Finalmente en 1031, se produce la división del califato en reinos de Taifas.

Entre los años 718 y 1230 se forman los principales núcleos cristianos en la península en los reinos de Castilla, Portugal, Navarra y la Corona de Aragón.

En el siglo XIII, se produce un gran avance cristiano gracias a la victoria en la Batalla de las Navas de Tolosa (1212) que provoca que el poderoso imperio almohade entre en decadencia, aprovechando las monarquías cristianas para conquistar grandes territorios y arrasar las principales ciudades.

Queda sólo el Reino nazarí de Granada como último reducto musulmán en la Península, mientras la corona de Aragón inicia una política de expansión por el Mediterráneo y se confirma la unión de Castilla con León.

La Reconquista finaliza en 1492 con la toma de Granada por parte de los Reyes Católicos que lo anexionan a la Corona de Castilla. En este mismo año se produce la expulsión de los judíos y el descubrimiento de América, en nombre de Castilla, por Cristóbal Colón.




</doc>
<doc id="4726" url="https://es.wikipedia.org/wiki?curid=4726" title="Radiación de Hawking">
Radiación de Hawking

La radiación de Hawking es una radiación teóricamente producida cerca del horizonte de sucesos de un agujero negro y debida plenamente a efectos de tipo cuántico. La radiación de Hawking recibe su nombre del físico británico Stephen Hawking, quien postuló su existencia por primera vez en 1974 describiendo las propiedades de tal radiación y obteniendo algunos de los primeros resultados en gravedad cuántica. El trabajo de Hawking fue posterior a su visita a Moscú en 1973, donde los científicos rusos Yákov Zeldóvich y Alekséi Starobinski le demostraron que, de acuerdo con el principio de indeterminación de la mecánica cuántica, los agujeros negros en rotación deberían crear y emitir partículas.

La radiación de Hawking reduce la masa y la energía rotacional de los agujeros negros y, por lo tanto, también se conoce como "evaporación de agujeros negros". Debido a esto, se espera que los agujeros negros que no ganan masa por otros medios se encojan y finalmente desaparezcan. Se predice que los micro agujeros negros son ​​mayores emisores de radiación que los agujeros negros más masivos y, por lo tanto, deberían reducirse y disiparse más rápidamente.

En junio de 2008, NASA lanzó el telescopio espacial Fermi, que está buscando los destellos terminales de rayos gamma que se esperan de la evaporación de algún agujero negro primordial. En el caso de que las teorías especulativas gran dimensión extra sean correctas, CERN Gran Colisionador de Hadrones puede crear microagujeros negros y observar su evaporación. No se ha observado ningún micro agujero negro en el CERN.

Posteriormente Paul Davies y Bill Unruh probaron que un observador acelerado u observador de Rindler en un espacio-tiempo plano de Minkowski también detectaría radiación de tipo Hawking.

Una de las consecuencias del principio de indeterminación de Heisenberg son las fluctuaciones cuánticas del vacío. Estas consisten en la creación, durante brevísimos instantes, de pares partícula-antipartícula a partir del vacío. Estas partículas son "virtuales", pero la intensa gravedad del agujero negro las transforma en reales. Tales pares se desintegran rápidamente entre sí, "devolviendo" la energía "prestada" para su formación. Sin embargo, en el límite del horizonte de sucesos de un agujero negro, la probabilidad de que un miembro del par se forme desde el interior y el otro en el exterior no es nula, por lo que uno de los componentes del par podría escapar del agujero negro; si la partícula logra escapar, la energía procederá del agujero negro. Es decir, el agujero negro deberá perder energía para compensar la creación de las dos partículas que separó. Este fenómeno tiene como consecuencias la emisión neta de radiación por parte del agujero negro y la disminución de masa de este.

Según esta teoría, un agujero negro va perdiendo masa, a un ritmo inversamente proporcional a esta, debido a un efecto cuántico. Es decir, un agujero negro poco masivo desaparecerá más rápidamente que uno más masivo. Concretamente, un agujero negro de dimensiones subatómicas desaparecería casi instantáneamente.

Cabe mencionar que la disminución de masa de un agujero negro por radiación de Hawking sería únicamente perceptible en escalas de tiempo comparables a la edad del universo y tan solo en agujeros negros de tamaño microscópico remanentes quizás de la época inmediatamente posterior al Big Bang. Si esto es así, hoy podríamos ver explosiones de agujeros negros muy pequeños, algo de lo que no se tiene evidencia alguna.

Un agujero negro emite radiación de Hawking termalizada, según una distribución idéntica a la del cuerpo negro correspondiente a una temperatura formula_1 . La cual, expresada en términos de las unidades de Planck, resulta ser:

Donde formula_2 es un parámetro relacionado con la gravedad en la superficie del horizonte. Análogamente, un observador de Rindler con una aceleración uniforme percibe a su alrededor una radiación termalizada asociada a una temperatura de cuerpo negro:

Donde formula_3 es la aceleración en unidades de Planck, obviamente la expresión y resultan formalmente idénticas expresadas en unidades de Planck. 

Si reescribimos las dos ecuaciones anteriores en unidades convencionales, la radiación de Hawking para un agujero Schwarzschild y la radiación de Unruh para un observador acelerado son:

donde:

Aplicando las ecuaciones anteriores al caso solar, si éste se llegara a convertir en un agujero negro, tendría una temperatura de radiación de tan sólo 60 nK (nanokelvin). Esta temperatura de radiación es notablemente inferior a la temperatura debida a la radiación de fondo de microondas, que es superior a los 2.7 K, por lo que si existe la radiación de Hawking, ésta podría ser indetectable.

Cuando las partículas escapan, el agujero negro pierde una pequeña cantidad de su energía y, por lo tanto, parte de su masa (masa y energía están relacionadas por ecuación de Einstein ).

En 1976 Don Page<nowiki> calculó la potencia producida, y el tiempo de evaporación, para un agujero negro de Schwarzschild de masa solar,</nowiki> sin rotación ni carga. Los cálculos son complicados debido al hecho de que un agujero negro, siendo de tamaño finito, no es un cuerpo negro perfecto. La sección eficaz de absorción disminuye de una forma complicada (dependiente del espín) a medida que la frecuencia disminuye, especialmente cuando la longitud de onda se vuelve comparable al tamaño del horizonte de eventos. Téngase en cuenta que, al escribir su artículo en 1976, Page postuló erróneamente solo existen dos sabores de neutrinos y que estos no tienen masa, por lo que sus resultados de la vida de los agujeros negros no coinciden con los resultados modernos que tienen en cuenta los3 sabores de neutrinos con masas distintas de cero.

Para una masa mucho mayor que 10 gramos, Page deduce que la emisión de electrones puede ignorarse, y que los agujeros negros de masa M (en gramos en la fórmula) se evaporan a través de neutrinos (muónicos formula_5 y electrónicos formula_6), fotones formula_7 y gravitones sin masa en un tiempo formula_8 de

Para masa más pequeña de 10 g, pero mucho más grande que 5 x 10 g, la emisión ultrarelativista de electrones y positrones acelerará la evaporación, dando como resultado una vida de

Si un agujero negro se evapora vía radiación de Hawking, un agujero negro de masa solar (1 formula_11) se evaporará en 10 años.

Un agujero negro supermasivo con una masa de 10 (100 millardos) formula_11 se evaporará en alrededor de 2×10 años.

Se predice que algunos agujeros negros excepcionalmente grandes continuarán creciendo hasta quizá 10 formula_11 durante el colapso de superclusters de galaxias. Incluso estos agujeros acabarán evaporándose en una escala de tiempo superior a 10 años.





</doc>
<doc id="4729" url="https://es.wikipedia.org/wiki?curid=4729" title="Turismo">
Turismo

La palabra turismo —según la OMT Organización Mundial del Turismo— comprende «las actividades que realizan las personas durante sus viajes y estancias en lugares distintos a su entorno habitual durante un período de tiempo inferior a un año, con fines de ocio, negocios u otros». Si no se realiza pernoctación, se consideran excursionistas. Los turistas y excursionistas forman el total de visitantes.

El turismo puede ser doméstico (turistas dentro de su propio país) o internacional... Este último es hoy una importante fuente de ingresos para muchos países. En el año 2015 hubo 1187 millones de desplazamientos turísticos internacionales, siendo los países más visitados Francia (84 millones), Estados Unidos (77 millones), España (68 millones), China (56 millones) e Italia (46 millones).

A comienzos de la década de 1930, uno de los primeros teóricos de la Escuela Alemana de los estudios en Turismo, Arthur Bormann define el turismo como el conjunto de viajes realizados por placer o por motivos comerciales y otros análogos, durante los cuales la ausencia de la residencia habitual es temporal. No son turismo los viajes realizados para trasladarse al lugar de trabajo.
Según la OMT un visitante (interno, receptor o emisor) se clasifica como turista (o visitante que pernocta), si su viaje incluye una pernoctación, o como visitante del día (o excursionista) en caso contrario. Es decir, un excursionista es aquella persona que visita un destino pero no pernocta en él.

Los pioneros teóricos en Turismo, Walter Hunziker y Kurt Krapf, dicen en 1942 que «El turismo es el conjunto de relaciones y fenómenos producidos por el desplazamiento y permanencia de personas fuera de su domicilio, en tanto que dichos desplazamientos y permanencia no están motivados por una actividad lucrativa».

El término «turismología» surgió en la década de 1960, fue el pensador yugoslavo Živadin Jovičić (geógrafo en su formación académica) el científico considerado «padre de la turismología», quién lo popularizó cuando fundó la revista del mismo nombre en 1972. Jovicic consideraba que ninguna de las ciencias existentes podía realizar el estudio del turismo en toda su dimensión, por considerar que sus aportaciones son unilaterales. Esto lo permitiría la creación de una ciencia independiente, la turismología.

En la década de 1990, La Organización Mundial del Turismo (OMT) define al Turismo como "El turismo comprende las actividades que realizan las personas
durante sus viajes y estancias en lugares distintos al de su entorno habitual, por un período de tiempo consecutivo inferior a un año con fines
de ocio, por negocios y otros". 

Hacia finales del siglo XX, el profesor Jafar Jafari entiende al turismo como un fenómeno interdisciplinar, que implica una relación compleja entre perspectivas de la Economía, Educación, Geografía, Historia, Hospitalidad, Derecho, Ocio y Recreación, Marketing, Ciencia Política, Psicología, Religión, Sociología, Transporte, Planificación Urbana y Regional, Agricultura, Antropología, Negocios y Ecología. En su trabajo académico, también afirma que el "turismo es más que un arte —es una táctica para atraer, trasportar, recibir, dar acomodo, entretener y servir al turista. El turismo se ha convertido en una ciencia— una dialéctica de estudio, análisis y conexión con todas las estructuras que lo influyen y son influidas por él", asegurando que representa «el mayor movimiento pacífico de población en tiempo de paz de la historia de la humanidad». 

En su clásica publicación académica de 1997, el epistemólogo del Turismo de la Universidad de Surrey John Tribe define al turismo como "el conjunto de fenómenos y relaciones surgidas de la interacción en regiones generadoras y anfitrionas, de turistas, proveedores de negocios, gobiernos, comunidades y ambientes". 

Por otro lado, en su artículo académico de 2007, el profesor de la Universidade de São Paulo Alexandre Panosso Netto argumenta que "el turismo es experiencia, en el momento en que construye ese "ser" turista", y que "las impresiones internas de esa acción no se forman sólo en el viaje o en el desplazamiento propiamente dicho, sino también son vividas en los momentos que anticipan el acto del turismo y en los momentos que prosiguen después que el "ser" turista ha emprendido su viaje. 

En 2013, el investigador social de la Universidad de Palermo, Maximiliano Korstanje entiende al turismo como un fenómeno social relacionado con un sistema onírico (a partir de los sueños), constituyéndose como "una institución social cuya consolidación comercial fue impuesta por Inglaterra y la revolución industrial en forma de viaje comercializado con arreglo a un retorno dentro de los primeros 6 meses antes de la partida" y, "en tanto que proceso, requiere de un desplazamiento físico que responde a la necesidad psicológica de evasión". 

En su libro de 2016, periodista turístico Miguel Ledhesma no liga el turismo al desplazamiento efectivo de las personas y lo describe como: "todo el espectro que se genera a partir de la idea y/o de la acción que implica el desplazamiento de los seres humanos a un lugar diferente al de su residencia con posibilidades recreativas, es decir, con intenciones de descanso, diversión y/o contacto con el destino receptor. El turismo se presenta entonces como un fenómeno complejo y multidisciplinar que comprende aristas económicas, sociales, políticas, artísticas, antropológicas, medioambientales, históricas, geográficas, educativas, psicológicas, comunicativas... que involucra simultáneamente al sector empresarial, al estatal, al no gubernamental, al sector profesional, a las poblaciones que habitan cada destino turístico y a los turistas".

La carrera de Turismo se dicta en diferentes sitios, algunos países la ofrecen como un título intermedio, como técnico o se puede extender la cursada para alcanzar la licenciatura. Básicamente esta diferencia en el título obtenido es lo que varía la cantidad de años que durará la carrera, puede ser entre 3 o 4 años.
El programa de la Licenciatura en Turismo refleja cuán variada y dinámica es el sector turístico y enseña todos los aspectos de gestión y de negocios, así como temas como la integración social, la innovación y la sostenibilidad. Esta carrera está diseñada para prepararte para ejercer una posición en la gestión y la responsabilidad en el negocio del turismo. En ella se estudia todo acerca del turismo, el funcionamiento de las empresas turísticas, el comportamiento de los turistas y el impacto de los visitantes en un país. Así, podrás investigar a fondo la industria turística, para que puedas entrar en él con una sólida comprensión de cómo ha crecido, cómo funciona y cómo se sigue desarrollando.
Muchas de las carreras de Turismo que se brindan están relacionadas también a la administración hotelera, por lo que brindan no solo conocimientos de diversos destinos turísticos sino también los conceptos contables y organizativos necesarios para dominar ese ámbito. Por tal motivo, esta carrera le permite al estudiante insertarse en diferentes campos laborales. Por otro lado, las salidas turísticas tienen distintas actividades, se puede abocar al ecoturismo, turismo aventura, al deportivo. Las tareas varían dependiendo la meta de cada estudiante, desde guiar a un grupo turístico, administrar un hotel o salidas históricas del mismo, crear o manejar una entidad turística de viajes y excursiones.

El turismo como tal, nace en el siglo XIX, como una consecuencia de la Revolución industrial, con desplazamientos cuya intención principal es el ocio, descanso, cultura, salud, negocios o relaciones familiares. Estos movimientos se diferencian por su finalidad de otros tipos de viajes motivados por guerras, movimientos migratorios, conquista, comercio, entre otros. No obstante el turismo tiene antecedentes históricos claros.

En la Grecia clásica se daba gran importancia al ocio, y el tiempo libre lo dedicaban a la cultura, diversiones, religión y deporte.

Los desplazamientos más destacados eran los que realizaban con motivo de asistir a los Juegos Olímpicos Antiguos en la ciudad de Olimpia, a las que acudían miles de personas y donde se mezclaban religión y deporte. También existían peregrinaciones religiosas, como las que se dirigían a los oráculos de Delfos y de Dódona.

Durante el Imperio romano los romanos frecuentaban aguas termales (termas de Caracalla), eran asiduos de grandes espectáculos, como los teatros, y realizaban desplazamientos habituales hacia la costa (muy conocido es el caso de una villa de vacaciones a orillas del mar).

Estos viajes de placer fueron posibles debido a tres factores fundamentales: la Paz romana, el desarrollo de importantes vías de comunicación y la prosperidad económica que posibilitó a algunos ciudadanos medios económicos y tiempo libre.

Durante la Edad Media hay en un primer momento un retroceso debido a la mayor conflictividad y recesión económica consiguiente.

En esta época surge un tipo de viaje nuevo, las peregrinaciones religiosas. Estas ya habían existido en la época antigua y clásica pero tanto el cristianismo como el islam las extenderían a mayor número de creyentes y los desplazamientos serían mayores.

Son famosas las expediciones desde Venecia a Tierra Santa y las peregrinaciones por el Camino de Santiago (desde el 814 en que se descubrió la tumba del santo); fueron continuas las peregrinaciones de toda Europa, creándose así mapas, mesones y todo tipo de servicios para los caminantes.

En el mundo Islámico el Hajj o peregrinación a La Meca es uno de los cinco Pilares del Islam obligando a todos los creyentes a esta peregrinación al menos una vez en la vida.

Las peregrinaciones continúan durante la Edad Moderna. En Roma mueren 1500 peregrinos a causa de una plaga de peste bubónica. Es en este momento cuando aparecen los primeros alojamientos con el nombre de hotel (palabra francesa que designaba los palacios urbanos).

Como las grandes personalidades viajaban acompañadas de su séquito (cada vez más numeroso) se hacía imposible alojar a todos en palacio, por lo que se crearon estas construcciones. Ésta es también la época de las grandes expediciones marítimas de españoles, británicos y portugueses que despiertan la curiosidad y el interés por viajar.

A finales del siglo XVII surge la costumbre de mandar a los jóvenes aristócratas ingleses a hacer el Grand Tour al finalizar sus estudios con el fin de complementar su formación y adquirir ciertas experiencias. Era un viaje de larga duración (entre 3 y 5 años) que se hacía por distintos países europeos, y de ahí proceden las palabras: turismo, turista, etc.

El Grand Tour es un viaje motivado por la necesidad de instrucción de estos jóvenes aristócratas que en un futuro habrán de gobernar su país. Del conocimiento "in situ" de la grandeza de Roma, París o Atenas así como de los debates en los cafés de los grandes centros termales, los viajeros deberían aprender como llevar las riendas de un Imperio como el británico. Para algunos autores éste es el auténtico fenómeno fundacional del turismo moderno ya que surge como un fenómeno revolucionario en paralelo al resto de transformaciones que se dan en la Ilustración.

También en esta época hay un resurgir de las termas, que habían decaído durante la Edad Media. No solo se asiste a ellas por consejo médico, sino que también se pone de moda la diversión y el entretenimiento en los centros termales como por ejemplo en Bath (Inglaterra). También de esta época data el descubrimiento de los baños de barro como remedio terapéutico, playas frías (Niza, Costa Azul) a donde iban a tomar los baños por prescripción médica.

Los viajes de placer tuvieron sus inicios en los últimos años del siglo XIX y los primeros del siglo XX. Grandes cambios en la sociedad, en los estilos de vida, en la industria y la tecnología alteraban la morfología de la comunidad. Hay en la historia momentos de cambios excepcionales y de enorme expansión.

El siglo XIX fue testigo de una gran expansión económica, seguida de una revolución industrial y científica incluso mayor en la segunda mitad del siglo XX. El turismo fue uno de los principales beneficiarios, para llegar a ser a finales del siglo XX, la mayor industria del mundo. Con la Revolución industrial se consolida la burguesía que volverá a disponer de recursos económicos y tiempo libre para viajar.

En la Edad Contemporánea el invento de la máquina de vapor supone una reducción espectacular en los transportes, que hasta el momento eran tirados por animales. Las líneas férreas se extienden con gran rapidez por toda Europa y Norteamérica. También el uso del vapor en la navegación reduce el tiempo de los desplazamientos.

Inglaterra ofrece por primera vez travesías transoceánicas y domina el mercado marítimo en la segunda mitad del siglo XIX, lo que favorecerá las corrientes migratorias europeas a América. Es el gran momento del transporte marítimo y las compañías navieras.

Comienza a surgir el turismo de salud y también el turismo de montaña. Se construyen famosos sanatorios y clínicas privadas europeas, muchos de ellos llegan a nuestros días como pequeños hoteles con encanto. Es también la época de las playas frías (Costa Azul, canal de la Mancha, etc.).

En 1841 Thomas Cook organiza el primer viaje planeado de la historia. Aunque fue un fracaso económico, se considera un rotundo éxito en cuanto a precedente del paquete turístico, pues se percató de las enormes posibilidades económicas que podría llegar a tener esta actividad, creando así en 1851 la primera agencia de viajes del mundo, Thomas Cook and Son.

En 1850, Henry Wells y William Fargo fundaron American Express, que inicialmente se dedicaba al transporte de mercancías y que posteriormente se convirtió en una de las agencias más grandes del mundo.

Aunque Cook ya los había introducido, American Express extendió los sistemas de financiación y emisión de cheques de viaje, como por ejemplo el "traveler's cheque" (dinero personalizado canjeable por papel moneda de uso corriente que protege al viajero de posibles robos o pérdidas). En 1867 inventa el bono o "voucher", documento que permite la utilización en hoteles de ciertos servicios contratados y prepagados a través de una agencia de viajes.

César Ritz, es considerado padre de la hostelería moderna. Desde muy joven ocupó todos los puestos posibles de un hotel, hasta llegar a gerente de uno de los mejores hoteles de su tiempo. Mejoró todos los servicios del hotel: creó la figura del sumiller, introdujo el cuarto de baño en las habitaciones y revolucionó la administración hotelera. Ritz convirtió hoteles decadentes en los mejores de Europa, por lo que le llamaban "mago".

Al estallar la Primera Guerra Mundial en el verano de 1914, se considera que había aproximadamente 150 000 turistas americanos en Europa. Tras finalizar la guerra comenzó la fabricación en masa de autocares y automóviles. En esta época las playas y los ríos se convierten en el centro del turismo en Europa comenzando a adquirir gran importancia el turismo de costa.

El avión, utilizado por minorías en largas distancias, se va desarrollando tímidamente para acabar imponiéndose sobre las compañías navieras. La crisis del 1929 repercute negativamente en el sector turístico, limitando su desarrollo hasta bien entrado en 1932.

La Segunda Guerra Mundial paraliza absolutamente el turismo en el mundo y sus efectos se extienden hasta el año 1949. Entre 1950 y 1973 se comienza a hablar del "boom turístico". El turismo internacional crece a un ritmo superior de lo que lo había hecho en toda la historia. Este desarrollo es consecuencia del nuevo orden internacional, la estabilidad social y el desarrollo de la cultura del ocio en el mundo occidental. En esta época se comienza a legislar sobre el sector.

La recuperación económica, especialmente de Alemania y Japón, fue asombrosa elevando los niveles de renta de estos países y haciendo surgir una clase media acomodada que se empieza a interesar por los viajes. La recuperación, elevó el nivel de vida de los sectores más importantes de la población en los países occidentales. Surge la llamada sociedad del bienestar en la que una vez cubiertas las necesidades básicas aparece el desarrollo del nivel de formación y el interés por viajar y conocer culturas.

Por otra parte la nueva legislación laboral adoptando las vacaciones pagadas, la semana inglesa de 5 días laborales, la reducción de la jornada de 40 horas semanales, la ampliación de las coberturas sociales (jubilación, desempleo,…), potencian en gran medida el desarrollo del ocio y el turismo.

También éstos son los años en los que se desarrollan los grandes núcleos urbanos y se hace evidente la masificación, surge también el deseo de evasión, escapar del estrés de las ciudades y despejar las mentes de presión.

En estos años se desarrolla la producción de automóviles en cadena que los hace cada vez más asequibles, así como la construcción de carreteras y autopistas, permite un mayor flujo de viajeros. De hecho, la nueva carretera de los Alpes que atraviesa Suiza de Norte a Sur supuso la pérdida de la hegemonía de este país como núcleo receptor, ya que ahora los turistas cruzan Suiza para dirigirse a otros países con mejor clima.

El avión de hélice es sustituido por el de reacción, lo que supone un golpe definitivo para las compañías navieras, que se ven obligadas a destinar sus barcos a los cruceros o al desguace. Todos estos factores nos llevan a la era de la estandarización del producto turístico.

Los grandes tour operadores lanzan al mercado millones de paquetes turísticos idénticos. En la mayoría de los casos se utiliza el vuelo chárter, que abarata el producto y lo populariza. Al principio de este período (1950) había 25 millones de turistas, y al finalizar (1973) había 190 millones.

No obstante esta etapa también se caracteriza por la falta de experiencia, lo que implica las siguientes consecuencias como la falta de planificación (se construye sin hacer ninguna previsión ni de la demanda ni de los impactos medioambientales y sociales que se pueden sufrir con la llegada masiva de turistas) y el colonialismo turístico (hay una gran dependencia de los tour operadores extranjeros estadounidenses, británicos y alemanes fundamentalmente).

En los 70 la crisis energética y la consiguiente inflación, especialmente sentida en el transporte ocasionan un nuevo periodo de crisis para la industria turística que se extiende hasta 1978. Esta recesión supone una reducción de la calidad para abaratar costes y precios apostando por una masificación de la oferta y la demanda.

En los 80 el nivel de vida se vuelve a elevar y el turismo se convierte en el motor económico de muchos países. Esto es facilitado por la mejora de los transportes (nuevos y mejores aviones como el Concorde y el Túpolev, trenes de alta velocidad y la consolidación de los vuelos chárter, hasta suponer un duro competidor para las compañías regulares que se ven obligadas a crear sus propias filiales chárter.

En estos años se produce una internacionalización muy marcada de las grandes empresas hosteleras y de los tour operadores, que buscan nuevas formas de utilización del tiempo libre (parques temáticos, deporte, riesgo, salud, etc.) y aplican técnicas de marketing, pues el turista cada vez tiene mayor experiencia y busca nuevos productos y destinos turísticos, lo que crea una fuerte competencia entre ellos.

La multimedia y las comunicaciones transforman el sector, modificando el diseño de los productos, la prestación del servicio, la comercialización del mismo de una manera más fluida.

La década de los 90 incluye grandes acontecimientos como la caída de los regímenes comunistas europeos, la :Guerra del Golfo, la :reunificación alemana, las :Guerras yugoslavas, etc., que inciden de forma directa en la historia del turismo.

Se trata de una etapa de madurez del sector que sigue creciendo aunque de una manera más moderada y controlada. Se limita la capacidad receptiva (adecuación de la oferta a la demanda, se empieza a controlar la capacidad de aforo de monumentos, etc.), se diversifica la oferta (nuevos productos y destinos), se diversifica la demanda (aparecen nuevos tipos diferentes de turistas) y se mejora la calidad (al turista no le importa gastar más si la calidad es mejor).

El turismo entra como parte fundamental de la agenda política de numerosos países desarrollando políticas públicas que afectan a la promoción, planificación y comercialización como una pieza clave del desarrollo económico. Se mejora la formación desarrollando planes educativos especializados. El objetivo de alcanzar un desarrollo turístico sostenible mediante la captación de nuevos mercados y la regulación de la estacionalidad.

También las políticas a nivel supranacional consideran el desarrollo turístico con elementos tan importantes como el Tratado de Maastricht en 1992 (libre tráfico de personas y mercancías, ciudadanía europea,…), y en el 1995 la entrada en vigor del Acuerdo de Schengen y se eliminan los controles fronterizos en los países de la UE.

Existe de nuevo un abaratamiento de los viajes por vía aérea por medio de las compañías de bajo coste y la liberación de las compañías en muchos países y la feroz competencia de las mismas. Esta liberalización afecta a otros aspectos de los servicios turísticos como la gestión de aeropuertos y sin duda será profundizada cuando entre en vigor la llamada Directiva Bolkestein (de liberalización de servicios) en trámite en el Parlamento Europeo.

En años recientes, tras recuperarse lentamente de los efectos de la recesión económica de 2008-2009, la cual fue agravada todavía más en algunas regiones debido al brote de la gripe A (H1N1) de 2009, las llegadas de turistas internacionales alcanzaron un récord de más de 1000 millones de turistas por primera vez en la historia en 2012. China fue el país cuyos ciudadanos realizaron los mayores gastos en turismo internacional en 2012, alcanzando USD 102 mil millones, superando a Alemania y los Estados Unidos, países que por varios años ocuparon los primeros lugares. China y los mercados emergentes han incrementado en forma significativa sus gastos en turismo, con Rusia y Brasil como ejemplos destacados que han subido varias posiciones en la clasificación de países que más gastan en turismo en el exterior.

De acuerdo con las estadísticas de la Organización Mundial del Turismo (OMT), en 2008 las llegadas de turistas internacionales ascendieron a 917 millones visitantes, lo que representó un aumento de 1,76% con respecto a 2007. En 2009 los arribos de turistas internacionales cayeron a 882 millones, representando una disminución a nivel mundial del 4,4% con respecto a 2008. La región más afectada fue Europa con una caída del 5,6%, sin embargo Francia continúa siendo el país más visitado del mundo. La disminución en el flujo de turistas internacional obedeció principalmente a los efectos de la recesión económica de 2008-2009, cuyos efectos se comenzaron a sentir en el turismo desde junio de 2008, y que se agravó todavía más en algunas regiones debido al brote de la gripe A (H1N1) de 2009. En 2010 el número de llegadas de turistas se recuperó y subió para 940 millones, superando el récord que se había alcanzado en 2008. En 2012 el número de llegadas alcanzó el récor de 1035 millones de visitantes internacionales, superando los 983 millones de turistas internacionales de 2011.

En el 2018, hubo 1.401 billones de turistas internacionales, con un aumento de 5.4% comparado con el 2017. 
Las 10 locaciones con más turistas en el año 2018 fueron (2018 numbers are preliminary):

Para presencia de turistas internacionales

De acuerdo con las estadísticas de la Organización Mundial del Turismo (OMT), en 2008 los ingresos generados a nivel mundial por el turismo internacional alcanzaron 942 mil millones USD (641 mil €), su máximo histórico, pero debido a los efectos de la recesión económica de 2008-2009 los ingresos en 2009 cayeron para 852 mil millones USD (611 mil millones €), representando una disminución en términos reales del 5,8 %, esto es, ajustando los ingresos para considerar las fluctuaciones de la tasa de cambio y la inflación del dólar estadounidense con respecto al euro. En 2010 los ingresos totales sumaron 919 mil millones USD (693 mil millones €) y los países con la mayor entrada de divisas originadas en el turismo internacional se concentraron en Europa, sin embargo, el mayor receptor de ingresos en continúa siendo Estados Unidos con 103,5 millones USD seguido por España y Francia.

Según la OMT, con 77,7 mil millones USD, Alemania continúo siendo el país que genera los mayores gastos en turismo internacional en el mundo en 2010, seguido de cerca por Estados Unidos (75.5 mil millones USD). La República Popular de China continuó siendo el país de mayor crecimiento en términos de gastos en turismo de los últimos años, llegando a mantener el tercer lugar que alcanzó en 2009 después de haber desplazado al Reino Unido de esa posición.

Entre 2010 y 2008 los siguientes 10 países recibieron los mayores ingresos provenientes del turismo internacional y también se presentan los 10 países emisores de turismo internacional con los mayores gastos:

Durante varios años México ha sido el destino más visitado por el turismo internacional en América Latina, a nivel de turismo masivo, posee el mayor número de nombramientos y declaraciones patrimoniales por la UNESCO en todo el continente americano, también México es el principal destino de negocios en la región, y es un importante destino educativo dentro de sus universidades; Según el BID, los ingresos provenientes del turismo internacional son una importante fuente de divisas para varios de los países de América Latina, y representa un porcentaje importante del PIB y de las exportaciones de bienes y servicios, así como una importante fuente de empleo, donde destaca la República Dominicana.

Según la evaluación realizada por el Foro Económico Mundial (FEM) varios de los países de América Latina todavía presentan deficiencias en las áreas de infraestructura y el marco jurídico, pero son muy competitivas en los aspectos relativos a recursos culturales y naturales, factores por los que resulta atractivo realizar inversiones o desarrollar negocios en el sector de viajes y turismo de los países de la región. Por ejemplo, Brasil fue clasificado en el Índice de Competitividad en Viajes y Turismo de 2009 en la posición 45 a nivel mundial, pero entre los 133 países evaluados clasificó en la posición 2 en el aspecto recursos naturales, y en la posición 14 en recursos culturales, a pesar de clasificar en el lugar 110 en infraestructura terrestre y como 130 en seguridad pública.

Los ingresos del turismo es clave para la economía de varios países de América Latina. En 2010, México recibió el mayor número de turistas internacionales, con 22,3 millones de visitantes, seguido por Panamá, con 5,2 millones, Brasil, con 5,1 millones, República Dominicana, con 4,1 millones, y Chile, con 3,3 millones.

Desde 2010, Mastercard realiza un estudio de las 20 ciudades más visitadas por turistas internacionales en el mundo.

La revista "Travel + Leisure" publicó en 2011 un ranking de las 50 atracciones turísticas más visitadas del mundo. "Forbes" publicó una lista similar en 2007 y al momento muchos están de acuerdo en que las atracciones enumeradas merecían un lugar entre las 50 mejores. Las siguientes son las 10 mejores atracciones del mundo según "Travel + Leisure", y se presentan también algunas otras ubicadas dentro de las 50 del ranking:

El sector turístico ofrece productos (servicios + derechos de uso) a través de las diferentes empresas y diversas organizaciones públicas y privadas, cuyas características principales son:

Es aquel cuyo programa de actividades e itinerario son decididos por los viajeros sin intervención de operadores turísticos. Véase turismo de alpargata o mochilero.

Es aquel que se realiza masivamente por todo tipo de personas, sin importar su nivel económico por lo que no es un tipo de turismo exclusivo. Es el más convencional, pasivo y estacional. Es normalmente menos exigente y especializado. Aquí podemos encontrar el turismo de sol y playa.

Es el que precisa de recursos histórico-artísticos para su desarrollo, como museos y monumentos, incluido el llamado patrimonio inmaterial: idiomas, folklore, costumbres, gastronomía, música, literatura, religión, historia... Es más exigente y menos estacional.

El turismo cultural puede ser un positivo instrumento de desarrollo local y regional, entendido esto último desde una visión socio-económica que permita una equitativa distribución de los beneficios, ya sean de carácter económico, social y cultural en las comunidades anfitrionas, reflejado en una mejora de la educación, la formación, la creación de empleo, y la generación de ingresos, colaborando en la erradicación de la pobreza, por ejemplo en el caso de los países en desarrollo.

En este contexto, la implementación y el desarrollo de diversos programas, ya sea a nivel nacional como regional o local, no solo han estimulado el desarrollo turístico propiamente dicho, sino también han promovido la recuperación y conservación del patrimonio local, y el establecimiento de nuevas industrias culturales locales.

Por ejemplo, el programa de Turismo Rural implementado por la Secretaría de Turismo de la Nación conjuntamente con la Secretaría de Agricultura, Ganadería, Pesca y Alimentación se origina por la necesidad de impulsar el desarrollo regional, involucrando a pequeños y medianos productores rurales con posibilidades de realizar otras actividades para generar nuevos ingresos.

En este sentido es importante considerar que, de acuerdo a los conceptos vertidos por distintos autores (Sancho, 1998; Sarasa, 2000; Grande Ibarra, 2001), el turismo rural puede considerarse como una variante del turismo cultural, desde la perspectiva del descubrimiento del patrimonio, las costumbres y las actividades de las comunidades rurales.

En el caso, específico de Argentina, el turismo cultural incorpora prácticas culturales que habían quedado en esterilidad, revaloriza las costumbres y hábitos campesinos, y recupera antiguos procesos y actividades vinculadas a la producción agrícola ganadera, contribuyendo a preservar la memoria colectiva y la identidad local.

La certificación de la sustentabilidad es un mecanismo con el cual es posible medir cualitativa y cuantitativamente el desempeño del turismo a través de sus prácticas de operación. Los Programas de Certificación del Turismo Sustentable son instrumentos voluntarios que están por encima de los marcos legales y que, de acuerdo con la Organización Mundial del Turismo, cumplen una función cada vez más importante en la reglamentación de servicios turísticos.


El Turismo Cultural también se ha estudiado desde una perspectiva administrativa, de hecho, existe una revisión sistemática de literatura que describe las variables de marketing utilizadas en la literatura para evaluar la internacionalización de los destinos turísticos cuyas propuestas de valor que involucren Patrimonio Cultural , en el mismo se presentan las variables de marketing como lo son: las comunicaciones integradas de marketing (integrated marketing communications), los pronósticos de demanda y la gestión de marca del destino (destination marketing), asuntos que deben ser gestionados adecuadamente por operadores turísticos, el gobierno y las partes interesadas en lograr la internacionalización sostenible del turismo.

El Turismo natural se suele desarrollar en un ambiente natural, ya sea este un medio rural o área protegida, tratando siempre de realizar actividades recreativas en él, pero sin deteriorar el entorno. También se puede encontrar incorporado al área urbana, mediante la contemplación de plantas y animales fuera de sus hábitats naturales, en los jardines botánicos y zoológicos, como parques temáticos de flora y fauna.


El turismo activo es aquel que se realiza en espacios naturales, el turismo activo está estrechamente relacionado con el turismo rural y generalmente este tipo de actividades se realizan en un parque natural debido al interés ecológico que estos presentan.


El turismo de negocios es aquel que se desarrolla con objeto o fin de llevar a cabo un negocio o un acuerdo comercial, se desarrolla entre empresas por lo general.
Utilizado por empresarios, ejecutivos, comerciantes y otros profesionales para cerrar negocios, captar clientes o prestar servicios. La estacionalidad es invertida a la vacacional, por lo que es un producto muy importante para el sector. El cliente suele ser de alto poder adquisitivo. Se trata de un turismo fundamentalmente urbano y con necesidades de infraestructura muy concretas como la conexión a internet.


El turismo científico es una modalidad de turismo cuya motivación es el interés en la ciencia o la necesidad de realizar estudios e investigaciones científicas en lugares especiales como estaciones biológicas o yacimientos arqueológicos.

En ocasiones existe la necesidad de viajar para observar "in situ" alguna realidad que es objeto de estudio. El turismo científico se realiza de forma individual o en pequeños grupos para evitar alterar el objeto de estudio en un entorno natural.

El turismo científico para el público en general apareció por primera vez en países desarrollados, con el objeto de involucrar de una manera más directa y participativa a las personas en el conocimiento del mundo natural.

Esta modalidad de turismo, busca ofrecer vacaciones orientadas a un mejor entendimiento de la naturaleza desde un punto de vista científico. Este turismo es muy buena fuente de beneficios económicos.

Ahora, en aras de una nueva visión de la ciencia desde la perspectiva de la socialización del conocimiento, nace el Turismo Científico Social orientada al fomento de una cultura turística y científica que permite conocer, compartir y valorar el origen, las costumbres, la sabiduría de un pueblo de forma creativa (Tovar-J et al, 2009). Proporcional al desarrollo social y al beneficio que el turismo aporta al valuarte de una localidad.

El turismo espacial es una modalidad de turismo que se realiza a más de 100 kilómetros de altura de la Tierra, lo que se considera la frontera del espacio. Comenzó a principios del siglo XXI realizado por personas muy ricas y valientes, los riesgos de perder la vida en un viaje al espacio son elevados.

Afortunadamente para los intrépidos turistas espaciales, el fin de la Guerra Fría, la construcción de la Estación Espacial Internacional y, sobre todo, la capacidad de llevar al espacio tres personas en una cápsula cuando sólo son necesarias dos, abrieron la puerta a esta modalidad de ocio.

Los primeros viajes consistían en una estancia de 3 o más días en la Estación Espacial Internacional en la que realizaban fotografías del espacio y la Tierra, videos, conversaciones con personas en la estación, disfrutar de la ingravidez, colaborar con los tripulantes de la estación y también realización de pequeños experimentos.

Varias empresas trabajan en la construcción de naves capaces de realizar vuelos suborbitales y orbitales, así como en un hotel modulable para estancias más largas y asequibles que las actuales, valoradas en unos 21 millones de dólares.

El 18 de mayo de 1996 la "Fundación X Prize" abrió la competición para crear vuelos espaciales turísticos al ofrecer el Premio Ansari X Prize con 10 millones de dólares en metálico a quien pudiera diseñar un aparato que llevara a tres tripulantes a más de 100 km de la Tierra dos veces en menos de quince días.

Esta sorprendente variante del turismo de aventura o de riesgo, está teniendo un notable desarrollo, que tiene su correspondencia en las inversiones que se están realizando en las zonas contaminadas radiactivamente para acoger al creciente número de visitantes. Se trata de zonas en las que se han realizado pruebas nucleares, como Nevada, en EE.UU.; o Sinkiang, en China; o que han sufrido un accidente nuclear, como Prípiat, en Ucrania, donde se encuentra la central nuclear de Chernóbil.

Los medios reflejan este fenómeno no sin perplejidad, debido a la aparente despreocupación con la que los visitantes asumen las molestias (mareos, vómitos...) y los riesgos (desarrollo de cáncer, malformaciones en la descendencia por alteraciones en las células de los órganos reproductores...) inherentes a la exposición a la radiación ionizante y la contaminación radiactiva. Sin embargo, como bien se indica en dichos medios: "... Los riesgos (...) no parecen ser un impedimento para el desarrollo de este tipo de turismo, sino más bien una de las principales razones de su impulso..."

Por lo que respecta a la administración pública, el gobierno de Ucrania declara que el turismo nuclear es ilegal y que no puede garantizar la seguridad de los turistas. En el caso de España, el Ministerio de Sanidad, Servicios Sociales e Igualdad no incluye aún los riesgos para la salud de este tipo de turismo en su listado de consejos para el viajero.

El turismo sexual es aquel cuya finalidad principal es la práctica de la relación sexual, existiendo destinos con fama de proporcionar más facilidades para ello .

Hasta hace poco tiempo el turismo sexual ha sido considerado una práctica marginal, incluso moralmente inaceptable , que frecuentemente se ha vinculado a la prostitución (especialmente la infantil), pero la evolución social ha ido modificando esa vinculación. La generalización del turismo ha supuesto cambios también en el turismo sexual, entre los que se encuentra la necesidad de redefinirlo, así como la de identificar sus características y problemas. 

Un reflejo del momento actual de esa realidad puede encontrarse en productos cinematográficos como "Paraíso", dirigido por Ulrich Seidl.

El turismo electrónico hace referencia a un término muy nuevo. Este se basa en la ayuda, fomento y gestión del turismo a través de medios de comunicación, específicamente la internet. Este concepto esta apoyado básicamente en la Mercadotecnia y en las Tecnología de la información, estos últimos como mecanismos de apoyo al proceso del turismo electrónico, además de esto esta estrechamente relacionado con el Comercio electrónico, este último como medio que permite el turismo electrónico, ya que un factor muy importante del turismo electrónico es que este se caracteriza por permitir el pago de los diferentes factores relacionados al turismo en general (alimentación, hospedaje, trasporte, entre otros). 

El turismo electrónico esta enfocado a las nuevas generaciones, esto se debe principalmente al manejo que estas tiene en cuanto a medios tecnológicos tanto de pago como de información. Según la Organización Mundial del Turismo el 60% de los consumidores de paquetes turísticos buscan información de destinos en la red. Este permite, a diferencia del turismo tradicional, visualizar de manera clara el destino que se pretende visitar, lo cual no solo incluye los lugares de interés de un determinado sitio, también factores asociados a este tales como cultura, costo de vida, trasporte, alimentación, entre otros, que le permiten a la persona tener una idea clara del destino y visualizar de manera adecuada sus posibilidades económicas para visitarlo además de proporcionar una herramienta sólida al turista potencial, que le permita elegir de manera adecuada el mejor destino, basado en las experiencias que este busca.

El turismo familiar hace referencia a un turismo para disfrutar en familia. Establecimientos y destinos turísticos que apuesten por el público familiar . Se trata de avalar hoteles, centros culturales, espacios naturales, restaurantes, emplazamientos turísticos, etc., que tengan instalaciones, oferta de actividades, entorno, etc. pensados para las familias.

Según la Federación Española de Familias Numerosas las familias necesitan de un turismo especializado para cubrir unas necesidades de ocio y culturales muy particulares de un colectivo cada vez más demandado de actividades diferentes pensadas para ellos.

Tienen la consideración de servicios turísticos la prestación del:

La información turística es el conjunto de servicios que se ofrecen al turista con el objetivo de informarle, orientarle, facilitarle y atenderle durante su viaje o estancia vacacional en oficinas de información turística a través de informadores turísticos o guías, intérpretes, correos de turismo, acompañantes de grupo, videotex, etc.

La definición incluye aquellos servicios públicos dependientes por regla general de organismos públicos o instituciones que tienen como misión informar facilitar y orientar al turista durante su estancia vacacional o viajes facilitando gratuitamente información.

En Argentina está regulada la publicidad de lugares turísticos, y se establece que cuando estas publicidades usen imágenes o fotos de atractivos turísticos deben tener el nombre del atractivo fotografiado, el nombre de la localidad y de la provincia donde se encuentra.



Son aquellas relacionadas con el turismo. Hay dos grandes bloques las que producen bienes y servicios (productoras), y las que los distribuyen (distribuidoras).

La hotelería es la rama del turismo que presta el servicio del alojamiento al turista. Este puede tener diversas clasificaciones, según el confort y el lugar donde se encuentren. Cada instalación hotelera tiene sus propias cualidades.

Los establecimientos hoteleros se dividen en:

Otros establecimentos destinados al alojamiento turístico son:








Son establecimientos de restauración aquellos cuya actividad principal es la de suministrar habitualmente y mediante precio, alimentos y bebidas para su consumo ya sea dentro o fuera del local. Aunque estos establecimientos son considerados de utilización pública, sus propietarios podrán establecer normativas o consideraciones sobre la prestación de sus servicios y adecuación de sus instalaciones.

Los establecimientos de restauración se dividen en dos tipos:



Los principales establecimientos de este tipo de alimentación son:




Son compañías de transporte aquellas destinadas a trasladar o transportar viajeros de un punto a otro.
Se clasifican en:

Operadores turísticos: Son empresas que proyectan, elaboran, diseñan, organizan y operan sus productos y servicios dentro del territorio nacional, para ser ofrecidos y vendidos a través de agencias de viajes, pudiendo también ofrecerlos y venderlos directamente al turista.

Agencias de viajes: Son empresas distribuidoras de bienes y servicios turísticos (transporte, alojamiento, etc.) que, en posesión de un título o licencia, ejercen actividades de intermediación turística, con el objetivo de ponerlos a disposición de los turistas.

Las agencias de viajes se clasifican en tres grupos:

Las agencias también se pueden clasificar en:

Hay otras muchas empresas relacionadas con el turismo directa o indirectamente, como las de ocio (cines, teatros), de entretenimiento (parques temáticos, de atracciones) y de diversión (casinos, hipódromos). También empresas de equipamiento de hostelería (maquinarias, menaje), artículos de viaje (maletas, bolsos), publicaciones especializadas (revistas, guías turísticas), etc.


Son muy relevantes debido al nivel de negocio que representa. Según la OMT el volumen de sector turístico en el 2003 ya representaba aproximadamente el 6 % de las exportaciones mundiales de bienes y servicios; Esta cifra representaba el 30 % de la exportación de servicios. Estos flujos económicos debidos al turismo afectan tanto en términos macroeconómicos como microeconómicos tanto en las zonas emisoras como en las receptoras (aunque especialmente en éstas). Las repercusiones económicas del turismo se pueden clasificar en las siguientes:

La demanda turística depende sobre todo de la fuerte situación económica de los países avanzados. Cuando la economía crece también normalmente crece el dinero disponible de la población. Y una parte importante de este dinero disponible por la población se gasta en el turismo, particularmente en las economías en desarrollo. Una retracción económica normalmente reduce el gasto turístico.

En general el crecimiento de los desplazamientos turísticos sigue claramente las tendencias de crecimiento económico medidas por el PIB. Los años en que el crecimiento económico mundial excede el 4 %, el crecimiento del volumen turístico tiende a ser mayor y los años en los que el PIB es menor el turismo crece incluso menos. En el periodo 1975-2000 el turismo se incrementó una media de un 4,6 % anual.

El turismo requiere una considerable mano de obra y, sobre todo, el mantener una reserva de trabajadores especializados. El sector turístico ocupa alrededor de un 10 % de la población activa del mundo, no solo en empleo directo sino también en indirecto. El problema que plantea el turismo con respecto al empleo es la fuerte estacionalidad, pues un gran número de puestos de trabajo se crean en temporada alta. También el volumen turístico depende en gran medida de la situación macroeconómica.

Además este empleo presenta unas condiciones particulares. Por ejemplo en España, en según el INE en el 2004 el sector turístico en relación con la media de área de servicios tenía un salario medio inferior, una estabilidad de empleo menor y una participación mayor de la mujer. También este tipo de empleo constituye una parte más importante de los gastos empresariales (con una tasa del 64,7) y la menor tasa de valor añadido (43,7).

Tanta es la importancia actual del turismo que en varios países existe la carrera universitaria y estudios de posgrado dedicados al turismo.

El turismo supone en los países eminentemente turísticos un peso específico elevado en su PIB, por ejemplo en España según el INE representó un 11 % del PIB en 2004 con una facturación de 91 988,7 millones de euros, de los cuales el 41 346,3 millones (un 4,9 % del PIB) corresponden a lo que se denomina turismo receptor (de origen internacional).

Al incrementarse los precios en la temporada alta en los núcleos receptores, automáticamente afecta a toda la población de la zona. Hay una oferta monetaria excesiva, por lo que la demanda está dispuesta a pagar más por los alquileres, salarios, etc.

El turismo tiene un efecto multiplicador y equilibrador en los países desarrollados, puesto que al generar empleo disminuye en cierta medida las diferencias económicas entre la población.

El valor internacional del mercado de divisas tiene una incidencia directa sobre el turismo, pues una bajada o subida de las diferentes divisas repercute positiva o negativamente en el número de turistas en las distintas zonas. Por otra parte cuantos más turistas entran más sube la moneda local del país repercutiendo de esta manera en el mercado de divisas.

Se animan a construir macro-complejos turísticos de los cuales el Estado cobrará impuestos de estas millonarias inversiones inmobiliarias.

Los países receptores exportan turismo porque, aunque en realidad lo reciben, se trata de un producto de exportación solo que consumido en el lugar de producción. Los ingresos por turismo aumentan en relación a los pagos, por lo tanto la balanza se ve beneficiada. Además tiene un efecto positivo en las exportaciones de otros bienes, pues promociona los productos locales a través de los turistas que los han conocido in situ.

El desarrollo de las zonas turísticas crea inversiones por parte de las Administraciones Públicas en infraestructuras (aeropuertos, carreteras, etc.) para adecuar la oferta turística a la demanda. Además se embellecen y mejoran las ciudades (parques, limpieza...)para que sean agradables para el turista. Todo ello no hace sino mejorar la vida del ciudadano y la estancia del turista en el lugar de destino.

El turismo pone en contacto a diferentes culturas: la local y receptora con la foránea o emisora. Esto supone un intercambio de pautas de conducta, formas de vida, hábitos de diversa índole (gastronómicos, lingüísticos, estéticos, etc.).
Este efecto, aunque positivo en ocasiones, puede llegar a ser perjudicial para la población autóctona, ya que pueden perder su identidad al intentar adaptarse a los gustos y tradiciones del visitante (por ejemplo la artesanía de los Massai en Kenia cambió sus colores tradicionales adaptándose al gusto del turista). Entre los efectos culturales del turismo destacan los siguientes:

En la zona turística, las consecuencias derivan de la introducción desde fuera de una nueva realidad sociocultural a la cual tanto la población indígena como su sistema social tienen que adaptarse. La primera adaptación consiste en desarrollar una clase encargada de tratar a los forasteros. Esto se refleja en la estructura laboral local en una expansión de los servicios. Los turistas han de ser transportados, alojados, ayudados en muchos problemas que surgen y hay que proveerles de actividades recreativas. Estas consecuencia primarias del turismo irán acompañadas a menudo de conflictos psicológicos y sociales.

El modelo de aculturación puede aplicarse al contacto entre turistas y anfitriones. Este modelo explica que cuando dos culturas entran en contacto cada una de las dos tiende a asemejarse en parte a la otra mediante un proceso de préstamo. En el caso del turismo es carácteristico que este proceso de préstamo sea asimétrico. Los turistas tienen menos probabilidades de tomar determinado elementos de sus anfitriones. Esto provoca una cadena de transformaciones en la comunidad de la zona turística para convertirse en algo cada vez más parecido a la cultura de los turistas.
Durante esta interacción el turista a menudo altera su conducta cuando está lejos de su país y sus anfitriones aprenderán a menudo una serie de papeles destinados a encajar con los gustos de los visitantes. El turista a menudo desea enriquecer todo lo posible su viaje y su tiempo de ocio. Los motivos del turista se reflejarán sus expectativas y en su conducta en la zona turística.

Por lo tanto, mayor tolerancia y respeto hacia otras costumbres, lo que evitaría fenómenos racistas y xenófobos. Gran parte de muchos malentendidos culturales surgen de la ignorancia de estos (p. ej., ritos, creencias, mitologías), el conocer sus razones y verlos implican una mayor tolerancia. Un turista que ha crecido con estereotipos puede tener un cambio de paradigmas cuando conoce una cultura distinta.

El turismo favorece la paz y el entendimiento plasmado en la firma de convenios de amistad entre países que mantienen relaciones basadas en sus flujos turísticos.

No solo los turistas que viajan intentan aprender el idioma del país al que van, sino que en el país receptor se hacen esfuerzos por aprender la lengua de los países emisores para atender mejor a los turistas. La adquisición de una segunda lengua de cara a la actividad turística suele tener por resultado una mayor movilidad económica.

En algunos casos (Bali, Kuna Yala) el turismo ha servido para regenerar el comercio de las artesanías de forma tradicional, al brindarles una expansión de su mercado y la colocación de productos indígenas en plazas a los que antes no tenían acceso, basado principalmente en la promoción de sus productos.

No obstante también se desarrolla una degradación de la estética a que da lugar la comercialización de baratijas a través de tiendas de curiosidades y souvenirs a menudo de fabricación no indígena, que por su naturaleza misma queda claro que dichos productos ya no son considerados una artesanía, pero que sin embargo generan una competencia desleal para los artesanos.

El turismo es una actividad económica más, por lo que debe existir un control público en materia de fiscalidad, seguridad, etc.


También hay otro tipo de efectos sociopolíticos no relacionados exclusivamente con el control de las administraciones públicas.


El ecoturismo es uno de los focos de transmisión de zoonosis como leptospirosis, rickettsiosis, la fiebre hemorrágica vírica y enfermedades relacionadas con "Lyssavirus".



</doc>
<doc id="4739" url="https://es.wikipedia.org/wiki?curid=4739" title="DVD">
DVD

El DVD es un tipo de disco óptico para almacenamiento de datos.

La sigla DVD corresponde a "Digital Versatile Disc" (Disco Versátil Digital), de modo que coinciden los acrónimos en español e inglés. En sus inicios, la “V” intermedia hacía referencia a "video" ("Digital Video Disc" o Disco de Video Digital), debido a su desarrollo como reemplazo del formato VHS para la distribución de vídeo a los hogares.

El estándar del DVD surgió en 1995 Consorcio ("DVD Consortium").

La unidad de DVD es el dispositivo que hace referencia a la multitud de maneras en las que se almacenan los datos: DVD-ROM (dispositivo de lectura únicamente), DVD-R y DVD+R DVD-RW y DVD+RW (permiten grabar y luego borrar). También difieren en la capacidad de almacenamiento de cada uno de los tipos.

A comienzo de los años 1990, se estaban desarrollando dos estándares de almacenamiento óptico de alta densidad:

Philips Sony y Panasonic se unieron, y acordaron con Toshiba adoptar el SD, pero con una modificación: la adopción del EFM Plus de Philips, creado por Kees Immink, que a pesar de ser un 6% menos eficiente que el sistema de codificación de Toshiba (de ahí que la capacidad sea de 4,7 GB en lugar de los 5 GB del SD original), cuenta con la gran ventaja de que EFM Plus posee gran resistencia a los daños físicos en el disco, como arañazos o huellas. El resultado fue la creación del Consorcio del DVD, fundado por las compañías anteriores, y la especificación de la versión 1.5 del DVD, anunciada en 1995 y finalizada en septiembre de 1996. En mayo de 1997, el Consorcio ("DVD Consortium") fue reemplazado por el Foro DVD ("DVD Forum") con los siguientes miembros:


El "Foro DVD" creó los estándares oficiales:

"DVD+RW Alliance" creó otros estándares (para evitar pagar la licencia al Foro):

Dado que los discos DVD+R/RW no forman parte de los estándares oficiales, no muestran el logotipo oficial «DVD». En lugar de ello, llevan el logotipo «RW» incluso aunque sean discos que solamente puedan grabarse una vez, lo que ha suscitado cierta polémica en algunos sectores que lo consideran publicidad engañosa, además de confundir a los usuarios.

Los DVD se dividen en dos categorías: los de capa simple y los de capa doble. Además el disco puede tener una o dos caras, y una o dos capas de datos por cada cara; el número de caras y capas determina la capacidad del disco. Los formatos de dos caras apenas se utilizan fuera del ámbito de DVD-Video.

Los DVD de capa simple pueden almacenar hasta 4,7 gigabytes según los fabricantes en base decimal (unidad del sistema internacional), y aproximadamente 4,38  gibibytes o en base binaria (unidad ITC), alrededor de siete veces más que un CD estándar. Emplea un láser de lectura con una longitud de onda de 650 nm (en el caso de los CD, es de 780 nm) y una apertura numérica de 0,6 (frente a los 0,45 del CD); la resolución de lectura se incrementa en un factor de 1,65. Esto es aplicable en dos dimensiones, así que la densidad de datos física real se incrementa en un factor de 3,3. 

El DVD usa un método de codificación más eficiente en la capa física: los sistemas de detección y corrección de errores utilizados en el CD, como la comprobación de redundancia cíclica CRC, la codificación "Reed Solomon - Product Code" (RS-PC), así como la codificación de línea "Eight-to-Fourteen Modulation" (EFM), la cual fue reemplazada por una versión más eficiente, EFM Plus, con las mismas características que el EFM clásico. El subcódigo de CD fue eliminado. Como resultado, el formato DVD es un 47% más eficiente que el CD-ROM, que usa una tercera capa de corrección de errores.

A diferencia de los discos compactos, donde el sonido (CDDA) se guarda de manera fundamentalmente distinta que los datos, un DVD correctamente creado siempre contendrá datos siguiendo los sistemas de archivos UDF e ISO 9660.

Los DVD se pueden clasificar:


La velocidad de transferencia de datos de una unidad DVD está dada en múltiplos de 1350 KB/s. 

Las primeras unidades lectoras CD y DVD leían datos a velocidad constante (velocidad lineal constante o CLV). Los datos en el disco pasaban bajo el láser de lectura a velocidad constante. Como la velocidad lineal (metros/segundo) de la pista es tanto mayor cuanto más alejados esté del centro del disco (de manera proporcional al radio), la velocidad rotacional del disco se ajustaba de acuerdo a qué porción del disco se estaba leyendo. Actualmente, la mayor parte de unidades de CD y DVD tienen una velocidad de rotación constante (velocidad angular constante o CAV). La máxima velocidad de transferencia de datos especificada para una cierta unidad y disco se alcanza solamente en los extremos del disco. Por tanto, la velocidad media de la unidad lectora equivale al 50-70% de la velocidad máxima para la unidad y el disco. Aunque esto puede parecer una desventaja, tales unidades tienen un menor tiempo de búsqueda, pues nunca deben cambiar la velocidad de rotación del disco.

Los DVD siguen el sistema de archivos UDF ("universal disk format" o formato de disco universal) y Joliet. Se adoptó este sistema de archivos para reemplazar al estándar ISO 9660, y su principal uso es la grabación o regrabación de discos. Fue desarrollado por OSTA ( Asociación de la Tecnología de Almacenamiento Óptico, ).







</doc>
<doc id="4740" url="https://es.wikipedia.org/wiki?curid=4740" title="Embarcadero Delphi">
Embarcadero Delphi

Embarcadero Delphi, antes conocido como CodeGear Delphi, Inprise Delphi y Borland Delphi, es un entorno de desarrollo de software diseñado para la programación de propósito general con énfasis en la programación visual. En Delphi se utiliza como lenguaje de programación una versión moderna de Pascal llamada Object Pascal. Es producido comercialmente por la empresa estadounidense CodeGear (antes lo desarrollaba Borland), adquirida en mayo de 2008 por Embarcadero Technologies, una empresa del grupo Thoma Cressey Bravo, en una suma que ronda los 30 millones de dólares. En sus diferentes variantes, permite producir archivos ejecutables para Windows, MacOS X, iOS, Android, GNU/Linux y la plataforma .NET.

CodeGear ha sido escindida de la empresa Borland, donde Delphi se creó originalmente, tras un proceso que pretendía en principio la venta del departamento de herramientas para desarrollo .

Un uso habitual de Delphi, aunque no el único, es el desarrollo de aplicaciones visuales y de bases de datos cliente-servidor y multicapas. Debido a que es una herramienta de propósito múltiple, se usa también para proyectos de casi cualquier tipo, incluyendo aplicaciones de consola, aplicaciones de web (por ejemplo servicios web, CGI, ISAPI, NSAPI, módulos para Apache), servicios COM y DCOM, y servicios del sistema operativo. Entre las aplicaciones más populares actualmente destaca Skype, un programa de telefonía por IP.

Delphi inicialmente sólo producía ejecutables binarios para Windows: Delphi 1 para Win16 y con Delphi 2 se introdujo Win32.

Delphi está basado en una versión de Pascal denominada Object Pascal. Borland en los últimos años defendía que el nombre correcto del lenguaje es también "Delphi", posiblemente debido a pretensiones de marca, aunque en sus mismos manuales el nombre del lenguaje aparecía como "Object Pascal", por lo que la comunidad de programadores no ha adoptado mayoritariamente este cambio (supuesta "aclaración", según Borland). Object Pascal expande las funcionalidades del Pascal estándar:


La potencia del lenguaje de programación Delphi, se basa principalmente en la gestión y administración de Base de Datos, concepto mediante el cual se repotenció para constituirse en una herramienta importante para desarrolladores, que requerían velocidad y manejo de grandes cantidades de datos, basada en un lenguaje de programación extensamente variado Object Pascal.

Esto permitió a los desarrolladores noveles (principiantes) y avanzados contar con una herramienta clara y de fácil manejo, moderna, capaz de ejecutar con gran velocidad, procedimentos intrínsecos, que permiten obtener resultados inmediatos y a bajo costo. Entre sus características principales, destaca lo atractivo del producto final, debido al apoyo de su interfaz gráfica de desarrollo, por lo que se sitúa en la categoría de lenguajes visuales, pero no todo es bondad, una desventaja con los programas no visuales, es la gran cantidad de memoria que emplea y que su ejecutable resulta demasiado grande en comparación con uno hecho en lenguaje C, pero esta desventaja se compensa con el tiempo de desarrollo de una aplicación; se sacrifica espacio, pero se gana velocidad.

Esta herramienta de desarrollo, permite emular las características de los lenguajes denominados de bajo nivel. Trabaja tanto en modo consola, como en ambiente gráfico windows, actualmente existe una versión para Linux.

Delphi dio una implementación muy buena a la idea del uso de componentes, que son piezas reutilizables de código (clases) que pueden interactuar con el IDE en tiempo de diseño y desempeñar una función específica en tiempo de ejecución. Desde un enfoque más específico de la herramienta, se catalogan como componentes todos aquellos objetos que heredan de la clase "TComponent", donde se implementa la funcionalidad necesaria para interactuar con el entorno de desarrollo, la carga dinámica desde "streams" y la liberación de memoria mediante una jerarquía. Una gran parte de los componentes disponibles para Delphi son "controles" (derivados de "TControl"), que encapsulan los elementos de interacción con el usuario como botones, menús, barras de desplazamiento, etcétera.
Delphi incluye una biblioteca de clases bien diseñada denominada VCL ("Visual Component Library", Biblioteca de Componentes Visuales) y, en sus versiones 6 y 7, una jerarquía multiplataforma paralela denominada CLX. Esta también se incluye en Kylix. Estas jerarquías de objetos incluyen componentes visuales y no visuales, tales como los pertenecientes a la categoría de acceso a datos, con los que puede establecerse conexiones de forma nativa o mediante capas intermedias (como ADO, BDE u ODBC) a la mayoría de las bases de datos relacionales existentes en el mercado. La VCL también está disponible para el desarrollo en .NET.

Además de poder utilizar en un programa estos componentes estándar (botones, grillas, conjuntos de datos, etc.), es posible crear nuevos componentes o mejorar los ya existentes, extendiendo la funcionalidad de la herramienta. En Internet existe un gran número de componentes, tanto gratuitos como comerciales, disponibles para los proyectos a los que no les basten los que vienen ya con la herramienta.

Delphi permite de manera sencilla ejecutar trozos de código en respuesta a acciones o "eventos" (sucesos) que ocurren durante el tiempo que un programa se ejecuta. Por ejemplo, cuando se presiona un botón, la VCL captura la notificación estándar de Windows, y detecta si hay algún método asociado al evento "OnClick" del botón. Si lo hay, manda ejecutar dicho método.

Los eventos pueden generarse debido a la recepción de señales desde elementos de hardware como el ratón o el teclado, o pueden producirse al realizar alguna operación sobre un elemento de la propia aplicación (como abrir un conjunto de datos, que genera los eventos "BeforeOpen/AfterOpen"). La VCL ha demostrado estar bien diseñada y el control que se tiene a través de los eventos de la misma es suficiente para la gran mayoría de aplicaciones.

Una de las principales características y ventajas de Delphi es su capacidad para desarrollar aplicaciones con conectividad a bases de datos de diferentes fabricantes. El programador de Delphi cuenta con una gran cantidad de componentes para realizar la conexión, manipulación, presentación y captura de los datos, algunos de ellos liberados bajo licencias de código abierto o gratuitos. Estos componentes de acceso a datos pueden enlazarse a una gran variedad de controles visuales, aprovechando las características del lenguaje orientado a objetos, gracias al polimorfismo.

En la paleta de componentes pueden encontrarse varias pestañas para realizar una conexión a bases de datos usando diferentes capas o motores de conexión.

Hay motores que permiten conectarse a bases de datos de diferentes fabricantes tales como BDE, DBExpress o ADO, que cuentan con manejadores para los formatos más extendidos.

También hay componentes de conexión directa para un buen número de bases de datos específicas: Firebird, Interbase, Oracle, etcétera.

A continuación un breve resumen (aún recopilándose) de las capas de conexión disponibles para las bases de datos más populares:


Es un motor de conexión a bases de datos de uso bastante amplio y que permite manejar bases de datos de escritorio como dBase, Foxpro y Paradox, además de ofrecer la capacidad para conectarse a servidores SQL locales y remotos.

Su uso, va siendo cada vez menor, debido a la pobre gestión de memoria que realiza, sustituyéndolo por componentes más actualizados y especializados como DOAC (Direct Oracle Access Components), DBExpress o ZEOS, esto sumado a la fiabilidad que están presentando los nuevos gestores de Datos en especial tecnologías como RDO y ADO; los cuales son mantenidos por sus fabricantes, forzando la compatibilidad con las versiones preliminares; liberando al programador de actualizaciones en cuanto a gestión de datos. Actualmente ya no es desarrollado por Codegear.

Como entorno visual, la programación en Delphi consiste en diseñar los formularios que componen al programa colocando todos sus controles (botones, etiquetas, campos de texto, etc.) en las posiciones deseadas, normalmente usando un ratón. Luego se asocia código a los eventos de dichos controles y también se pueden crear módulos de datos, que regularmente contienen los componentes de acceso a datos y las reglas de negocio de una aplicación.

Entorno de desarrollo integrado (EDI) o IDE en inglés ("Integrated Development Environment"), es el ambiente de desarrollo de programas de Delphi. Se trata de un editor de formularios (que permite el desarrollo visual), un potente editor de textos que resalta la sintaxis del código fuente, la paleta de componentes y el depurador integrado, además de una barra de botones y un menú que nos permite la configuración de la herramienta y la gestión de proyectos. En las ediciones "Client/Server" y "Enterprise" el EID también ofrece integración con una herramienta de control de versiones (PVCS).

Es una potente característica que nos permite establecer puntos de ruptura (breakpoints), la ejecución paso a paso de un programa, el seguimiento de los valores de las variables y de la pila de ejecución, así como la evaluación de expresiones con datos de la ejecución del programa. Con su uso, un programador experimentado puede detectar y resolver errores lógicos en el funcionamiento de un aplicativo desarrollado con Delphi. En las ediciones "Client/Server" y "Enterprise" se añade la opción de depuración de programas corriendo en equipos remotos ("remote debugging"), lo que posibilita el uso de todas las características del depurador con un programa ejecutándose en su entorno normal de trabajo y no en el ordenador del programador (en donde muchas veces no ocurren los errores).

Existen desarrollos de la comunidad del software libre que intentan producir una versión multiplataforma y completamente independiente de Delphi. La más desarrollada es el llamado Lazarus.

El nombre "Delphi" hace referencia al oráculo de Delfos. Borland eligió ese nombre para resaltar su principal mejora con respecto a su antecesor (Turbo Pascal), que sería su conectividad con bases de datos Oracle ("oráculo", en inglés). El nombre se suele pronunciar "delfi" en Europa, incluida Gran Bretaña. Se usa "delfái" en inglés de Estados Unidos (existe una población "Delphi" con esa pronunciación), por lo que es la preferida por Borland.

En septiembre de 2006 Borland lanzó "TurboDelphi" como una versión reducida de Delphi. Hay dos versiones: Turbo Delphi for Windows" (Win32) y "Turbo Delphi for .NET, ambas disponibles en dos ediciones Professional y Explorer, esta última era gratuita.




</doc>
<doc id="4742" url="https://es.wikipedia.org/wiki?curid=4742" title="GnuLinEx">
GnuLinEx

gnuLinEx fue una distribución Linux libre basada en Debian GNU/Linux y GNOME, contando con OpenOffice.org como suite ofimática, entre otras aplicaciones. Hoy está descontinuada y el dominio de descarga fue comprado por una empresa alemana.

Estaba impulsada por la Consejería de Economía, Comercio e Innovación de la Comunidad Autónoma de Extremadura (España), siendo pionera y secundada por otros organismos públicos y privados del resto de España. Durante un periodo considerable de tiempo, la comunidad extremeña ofreció también apoyo a la de Andalucía (la cual se inspiró en GnuLinex para desarrollar Guadalinex) en la implantación de soluciones abiertas en colegios, administración, etc. Posteriormente han surgido proyectos similares en otras Comunidades Autónomas, como la Comunidad Valenciana (Lliurex), Madrid (Max), Galicia (Galinux), Cataluña (Linkat), Castilla-La Mancha (Molinux).

El 20 de octubre de 2010 se liberó LinEx 2010. El 13 de octubre de 2011 se liberó Linex 2011. El 31 de diciembre de 2011 la Junta de Extremadura, coincidiendo con el cambio de gobierno, transfiere su mantenimiento a una fundación estatal CENATIC (Centro Nacional de Referencia de Aplicación de las Tecnologías de la Información y la Comunicación basadas en Fuentes Abiertas) que se hace cargo de asumir las funciones del CESJE (Centro de Excelencia de Software José de Espronceda), hasta la fecha era la entidad encargada del mantenimiento del software. CENATIC apuesta por un desarrollo 100% en comunidad de la distribución regional (aprovechando su relevante experiencia a nivel europeo en la creación de comunidades libres público-privadas) para conseguir la mantenibilidad y sostenibilidad técnica de LinEx a futuro; situando de nuevo a Extremadura como región pionera en apoyo del software libre. El 11 de febrero de 2013, fruto de este trabajo, se lanza la versión de LinEx 2013, con una base totalmente actualizada a Debian Wheezy.



Es de especial importancia resaltar que estos sabores son en realidad paquetes especiales que permiten instalar una serie de paquetes de software que configuran el sabor elegido.

Los sabores incluidos en Linex 2011 son los siguientes:

Conjunto de aplicaciones, plugins y scripts para LinEx 2011, especializados en el Diseño Gráfico:


Conjunto de juegos para LinEx 2011:


Conjunto de aplicaciones, plugins y scripts para LinEx 2011, especializados en la creación y edición de archivos multimedia Audio:


Conjunto de aplicaciones, plugins y scritps para LinEx 2011, especializados en la creación y edición de archivos multimedia Vídeo:


Conjunto de compiladores y entornos de desarrollo destinados a la programación:






Es una variación de "GnuLinex" orientada a los educadores. Se compone de tres perfiles de usuario, cada uno de ellos, para un ciclo de primaria. Todos están personalizados y llenos de contenidos y software educativo para cada alumno. Linex Colegios está instalado en los ordenadores de los centros de Educación Primaria e Infantil.

En mayo de 2003, se habían distribuido 200.000 CD de LinEx gratuitamente mediante los diarios locales, se habían descargado 70.000 imágenes del sistema operativo desde el sitio web de la distribución. Alrededor del 10% de los habitantes de Extremadura podrían utilizar LinEx.


Actualmente gnuLinEx se encuentra principalmente en los institutos públicos de educación secundaria de la comunidad extremeña, contando con un ordenador (con gnuLinEx instalado) por cada dos alumnos o por cada mesa, para ser más exactos, aunque también en los equipos de profesores. También cuenta en los colegios de educación primaria y educación infantil.




</doc>
<doc id="4745" url="https://es.wikipedia.org/wiki?curid=4745" title="Gastronomía">
Gastronomía

La gastronomía (del griego γαστρονομία ["gastronomía"], del prefijo gastro = estómago y del sufijo -nomía = conocimiento.) es la ciencia y arte que estudia la relación del ser humano con su alimentación y su medio ambiente o entorno. El gastrónomo es el individuo que se preocupa de este arte. A menudo se cree erróneamente que el término gastronomía únicamente tiene relación con el arte culinario y la cubertería en torno a una mesa. Sin embargo, esta es una pequeña parte del campo de estudio de dicha disciplina: no siempre se puede afirmar que un cocinero es un gastrónomo. La gastronomía estudia varios componentes culturales, tomando como eje central la comida.

En la Antigua Roma la cocina evolucionó de la frugalidad de la época republicana, basada en vegetales, legumbres y cereales, a la riqueza de la época imperial, donde se importaban numerosos alimentos de países extranjeros, con gran influencia de la cocina griega. Los romanos practicaron la avicultura y la piscicultura, así como la elaboración de embutidos, y perfeccionaron las técnicas relacionadas con el vino y el aceite. Solían cocinar con especias y hierbas aromáticas, y les gustaba la mezcla de dulce y salado. También dieron mucha importancia a la presentación de la comida y al ceremonial del acto de comer, siendo famosos los fastuosos banquetes que organizaban los ricos y nobles romanos. Igualmente, existieron numerosos tratadistas que estudiaron el arte de la cocina, como Lúculo y Marco Gavio Apicio, autor del célebre recetario "Apitii Celii de Re Coquinaria libri decem", muy valorado en el Renacimiento.

Herederas de la cocina griega y romana fueron la bizantina y la árabe: de la primera destacó su repostería, así como la elaboración de quesos y el gusto por los rellenos y la carne picada; la segunda recogió todas las influencias anteriores, junto las derivadas de Persia y el Oriente, mientras que España (al-Ándalus) desarrolló nuevos productos agrícolas, como el arroz, la caña de azúcar, la granada y la berenjena. La cocina árabe influyó en buena medida en la gastronomía medieval, aunada a la rica tradición grecorromana. A pesar de las épocas de intensa hambruna, en el Medievo fue muy valorada la gastronomía, desarrollándose en gran medida la literatura gastronómica, con tratados como "The forme of Cury", escrito por el cocinero de Ricardo II de Inglaterra; "Daz Buch von guter Spise", obra anónima editada en Alemania; "Le Viandier", del francés Guillaume Tirel, apodado Taillevent; y "Llibre de Coch", del catalán Robert de Nola.

En el Renacimiento se revitalizó la cultura clásica, llegando la gastronomía a altas cotas de refinamiento y sofisticación. Destacó la cocina veneciana, que gracias a su comercio con el Oriente favoreció la importación de todo tipo de especias: pimienta, mostaza, azafrán, nuez moscada, clavo, canela, etc. Un factor determinante para una nueva gastronomía fue el descubrimiento de América, planeado originalmente para llegar por occidente a las islas de las especias (una idea comercial, aunque también gastronómica), y que resultó que del continente descubierto llegaron nuevos alimentos como el maíz, la papa, el tomate, el cacao, los frijoles, el cacahuete, el pimiento, la vainilla, la piña, la palta, el mango, el tabaco, etc. En el Barroco empezó a destacar la gastronomía francesa, que adquirió unas elevadas cotas de calidad de las que aún goza hoy día. El cultivo de las artes culinarias en Francia fue favorecido por los Borbones, especialmente por Luis XIV, monarca de gran paladar; sin embargo, estas delicias culinarias estaban reservadas a la aristocracia, mientras que la mayoría de la población solía pasar hambre. Entre los tratados gastronómicos de la época conviene resaltar el del español Francisco Martínez Motiño, titulado "Arte de cocina, pastelería, bizcochería y conservería" (1611).

La Revolución francesa marcó un punto de inflexión en la gastronomía europea, que se extendió a nivel popular, siendo un acervo común de todos los estamentos sociales, y no solo los privilegiados. Surgieron los restaurantes, se extendió el uso de la conserva de alimentos (proceso favorecido por la Revolución Industrial), y proliferó la literatura gastronómica, no ya en simples recetarios, sino en obras de investigación y divulgación, de teoría y ensayo, como la "Fisiología del gusto" de Brillat-Savarin (1826), o "Le Grand Dictionnaire de Cuisine" de Alexandre Dumas (1873); también apareció entonces la crítica gastronómica, con publicaciones como la Guía Michelin. En el siglo XX ha tenido una especial relevancia la industria conservera y la elaboración de alimentos precocinados, así como la tendencia a la comida rápida (con productos como hamburguesas y frankfurts) y los preparados para microondas. En sentido inverso, ha surgido una nueva preocupación por los alimentos sanos y equilibrados, que han favorecido el surgimiento de nuevos productos que destacan sus cualidades nutricionales. También hay que destacar la revalorización de la cocina regional, favorecida por el avance de los medios de transporte y el auge del turismo, que conllevó el retorno a una cocina natural y sencilla, hecho que marcó el punto de arranque de la "nouvelle cuisine", que aúna la tradición y la sencillez con los nuevos adelantos y un cierto afán de innovación y experimentación. Como cocineros de relevancia se podría citar a: Auguste Escoffier, Joël Robuchon, Paul Bocuse, Heston Blumenthal, Karlos Arguiñano, Juan Mari Arzak, Ferran Adrià, Santi Santamaria, etc.




</doc>
<doc id="4746" url="https://es.wikipedia.org/wiki?curid=4746" title="Periodismo fotográfico">
Periodismo fotográfico

El periodismo fotográfico, periodismo gráfico, fotoperiodismo o reportaje gráfico es un género del periodismo que tiene que ver directamente con la fotografía, el diseño gráfico y el vídeo. Los periodistas que se dedican a este género son conocidos por lo general como "reporteros gráficos" o "fotoperiodistas" y en su mayoría son fotógrafos versados en el arte. El desarrollo del periodismo gráfico puede rastrearse desde los mismos inicios del desarrollo de la fotografía y tiene una notable relación con la fotografía artística. El periódico utilizó la fotografía por primera vez en 1880, en el "Daily Graphic" de Nueva York y desde entonces el periodismo ha integrado la imagen como medio objetivo y representativo de un hecho.

Paradójicamente, el desarrollo de la fotografía periodística se ha realizado especialmente en los conflictos políticos de los diferentes países o en las confrontaciones bélicas, siendo considerado Roger Fenton el primer fotógrafo de guerra. El papel de los fotoperiodistas ha sido vital para registrar la historia de acontecimientos como la Guerra Civil Española, la Guerra de la Triple Alianza, la Guerra hispano-estadounidense, la Guerra de Vietnam y las dos guerras mundiales, entre muchas otras confrontaciones bélicas. Precisamente la denominada edad dorada del fotoperiodismo (1930-1950) coincide con la II Guerra Mundial, tiempo en el cual se dio un notable avance tecnológico de la fotografía de guerra.

Con el desarrollo de las técnicas de impresión gracias a la invención de la impresora ófset, la fotografía periodística alcanzó mejores niveles de calidad en las publicaciones impresas. Por su parte, la incursión de nuevas tecnologías como Internet originó una nueva etapa del periodismo a nivel global, conocida como periodismo digital, que afectó muy especialmente al fotoperiodismo. Los grandes acontecimientos que inauguraron el siglo XXI, como los atentados del 11 de septiembre de 2001, la invasión de Irak de 2003 y el Tsunami del océano Índico de 2004, entre otros, generaron una notable cantidad de información visual en la que participaron las nuevas tecnologías en manos de aficionados. Esto ha llevado a algunos observadores a hablar de "crisis del periodismo" y a otros a señalar el inicio de una nueva era en el oficio de la información.

El periodismo fotográfico es una forma periodística destinada a la adquisición, edición y presentación de material de actualidad en los medios de comunicación social, especialmente escritos, digitales y audiovisuales. Incluye, además, todo aquello relacionado con la imagen que cuenta una historia y, en tal caso, se refiere también al vídeo y al cine utilizado con fines periodísticos. El periodismo fotográfico se distingue de otras ramas de la fotografía como la documental, la fotografía callejera y la de estudio utilizada por ejemplo para el modelaje. El fotoperiodismo incluye todas las áreas de interés de la actualidad informativa como el periodismo de guerra, el periodismo deportivo, el seguimiento del mundo del espectáculo, la política, los problemas sociales y todo aquello en donde sea posible la creación de una imagen. De igual manera el fotoperiodismo repasa los géneros de la profesión informativa como la entrevista, la crónica, el reportaje y el documental en todas sus facetas. Se divide además en fotonoticia, fotorreportaje, gran reportaje fotográfico, ensayo fotoperiodístico, retrato fotoperiodístico y columna fotoperiodística.
El Fotoperiodismo de Boda es una tendencia fotográfica en reportajes sociales, principalmente Bodas, dónde el verdadero protagonista no es el Fotógrafo, si no las emociones que ocurren en un día cargado de emociones. Se acaban ya las directrices del fotógrafo pidiendo a los novios que hagan una cosa u otra, interrumpiendo así un día especial y convertirlo en un sucesivo posado a cámara sin sentido e infantil.
El fotógrafo debe pasar como un invitado más sin molestar, y al mismo tiempo que se sienta uno más, como conocido y no como un profesional del que no ha habido más de una hora de contacto en la contratación.
Las características esenciales para clasificar una fotografía como género periodístico son las siguientes:


La fotografía debe también cumplir con todos los rigores de la ética periodística en cuanto a veracidad, precisión y objetividad. Como en la redacción de la noticia, el periodista fotográfico es un reportero y su oficio suele ser por lo general riesgoso y obstaculizado por múltiples factores.

La práctica de ilustrar historias noticiosas con fotografías fue posible gracias al desarrollo de la imprenta y a las innovaciones de la fotografía que ocurrieron entre 1880 y 1897. Mientras eventos relevantes fueron fotografiados ya desde los años 50 del siglo XIX, la imprenta pudo hacerlos públicos en un medio escrito solo hasta los años 80 de ese siglo. Las fotos más primitivas eran daguerrotipos que tenían que ser reelaborados para ser impresos.

Si bien los daguerrotipos se expandieron bien pronto en Europa, Canadá, Estados Unidos y Latinoamérica tan temprano como 1838, se considera que los pioneros de la fotografía periodística se presentaron en la Guerra de Crimea (1853 - 1856) por parte de reporteros británicos como William Simpson del "Illustrated London News" y Roger Fenton cuyas obras fueron publicadas en grabados. De la misma manera, la Guerra de Secesión de los Estados Unidos tuvo periodistas gráficos como Mathew Brady que publicó sus obras en el "Harper's Weekly". Debido a que los lectores clamaban por representaciones más realistas, fue necesario que dichas primeras fotografías fueran exhibidas en galerías de arte o copiadas fotográficamente en números limitados. Los principales eventos mundiales de la segunda mitad del siglo XIX, especialmente aquellos que tuvieron que ver con confrontamientos armados, eventos políticos y personajes históricos, quedaron bien documentados en material fotográfico de la época. 

El 4 de marzo de 1880 el "Daily Graphic" de Nueva York hizo la primera publicación de noticias en fotografía real de la historia. Innovaciones posteriores siguieron a esta y en 1887 se inventó el flash, lo que permitió que los periodistas pudieran hacer tomas en exteriores y en condiciones pobres de luz. El primer documental fotográfico de la historia fue el del emigrante danés en Estados Unidos Jacob Riis quien lideró el reportaje de lo que llamó "Cómo vive la otra mitad" ("How the Other Half Lives", 1888). Para 1897 fue posible hacer publicaciones de fotografías impresas en imprentas sin mayores dificultades.

A pesar de las innovaciones, los límites persistieron y muchos de los periódicos sensacionalistas y revistas de historias fueron ilustradas con fotografía grabadas entre los años 1897 y 1927. En 1921 la "wirephoto" o telefoto permitió transmitir imágenes por teléfono tan rápido como noticias, aunque ya se había hecho telegráficamente desde la Exposición Universal de Londres de 1851 y comercialmente desde 1863 ("véase fax"). Sin embargo, no fue hasta el desarrollo de la cámara comercial "Leica" de 35mm en 1925 y el primer foco de flash entre 1927 y 1930 que se llegó la edad dorada del periodismo gráfico.

El siglo XX significó un gran desarrollo del fotoperiodismo, pero es conocida como la "edad dorada" del mismo al periodo comprendido entre 1930 y 1950 debido a avances muy significativos para la fotografía y una mayor amplitud en el oficio periodístico. Paradójicamente el desarrollo del fotoperiodismo se da muy especialmente entre las dos guerras mundiales en donde los periodistas y sus instrumentos de información se ponen a máxima prueba. Muchos de ellos incluso participaron como combatientes en dichas guerras. Algunas revistas como la "Picture Post" de Londres, la "Paris Match" de Francia, la "Arbeiter-Illustrierte-Zeitung" de Berlín, la "Life Magazine" y la "Sports Illustrated" de EE.UU, así como los periódicos "The Daily Mirror" de Inglaterra, el "New York Times" y otros, obtuvieron una gran lecturabilidad y reputación gracias al uso de amplio material fotográfico de la mano de célebres reporteros gráficos como Robert Capa, Alfred Eisenstaedt, Erich Salomon, Margaret Bourke-White y W. Eugene Smith.

En particular Henri Cartier-Bresson es generalmente considerado el padre del periodismo fotográfico. Las tomas de acciones congeladas en el tiempo son célebres, como la de un hombre que salta y que fue considerada una de las más espléndidas tomas del siglo XX. Su cámara Leica (introducida en 1925), es considerada versátil, la que le permitió capturar momentos decisivos en el tiempo justo. Esta cámara fue también la que utilizó otra gran figura del periodismo gráfico del siglo XX: Robert Capa. 

El soldado Tony Vaccaro es también reconocido como uno de los más prominentes fotógrafos de la II Guerra Mundial. Sus imágenes, tomadas con una sencilla cámara Argus C3, capturaron los horrorosos momentos de la guerra como la muerte en batalla del soldado Capa, quien estuvo también en el desembarco de la playa de Omaha en el Día D y quien también dejó importantes tomas de ese momento decisivo de la II Guerra Mundial. Vaccaro también es conocido por haber desarrollado sus propias imágenes en cascos de soldados y utilizar químicos que encontró en las ruinas de un laboratorio fotográfico en 1944. 

Hasta la década de los 80 del siglo XX la mayoría de las publicaciones utilizaban la tecnología de imprenta basada en una baja calidad de papel periódico, base de tinta y superficie rugosa. Mientras las letras resultaban de alta definición y legibilidad, los grabados eran formados por puntos fotográficos que en muchas ocasiones distorsionaban la imagen y producían efectos secundarios. De este modo, aunque la publicación utilizaba bien la fotografía –un tamaño respetable, bien enmarcada–, reproducciones opacas obligaban al lector a poner cuidadosa atención en la fotografía para entender su significado. El "Wall Street Journal" adoptó puntos de alta resolución en 1979 para publicar retratos y evitar las limitaciones de la impresión de letras. Solo hasta los 80 la mayoría de los periódicos cambiaron a las impresoras ófset que reproducen fotos con una alta fidelidad en papel blanco.

Por su parte, la revista estadounidense "Life", uno de los semanarios más populares desde 1936 y a través de la década de 1970 en cuanto a fotografía se refiere, comenzó a reproducir las mejores fotografías en tamaños once veces más grandes, páginas de 35,16 cm de dimensión, alta cualidad de tinta y papeles suaves. "Life" publica con frecuencia las mejores fotos de UPI o de la AP que se publiquen con anterioridad en otros medios, pero que al ser presentada por la revista estadounidense aparecen como versiones completamente diferente gracias a la cuidadosa atención que la revista le da.

La fotografía comparte dos mundos: por uno el de la tecnología y por otro el del arte. Sin embargo, las artes plásticas no aceptaron esto de manera inicial y vieron a la fotografía hasta la década de los 70 del siglo XIX con distancia. De la misma manera, el fotoperiodismo se divide entre la función de la información por un lado y la tendencia al arte. Lógicamente en la información lo más importante es aquello que es anunciado como generador de noticia, sin embargo, el profesional es aquel que sabe tener en cuenta las dimensiones estéticas en combinación con la información. Por lo general, los grandes fotoperiodistas son aquellos que dejaron obras en las que ambas dimensiones se entrelazan perfectamente. 

En gran medida porque sus fotos son lo suficientemente claras para ser apreciadas o porque sus nombres aparecen siempre con sus obras, los fotógrafos de las revistas alcanzan el estatus de celebridad. "Life" llegó a ser, por ejemplo, un modelo en el cual la crítica fotográfica se hace un juicio acerca del fotoperiodismo y muchos periodistas actuales se han hecho un nombre. En una selección de las mejores fotografías de "Life" realizada en 1973 se presentaron 39 fotógrafos famosos, pero los puntajes revelaron en dicha selección que las mejores fotografías fueron de anónimos de UPI y AP.

Debido a la "edad dorada" de la fotografía, las limitaciones de la impresión y los sistemas organizativos de las agencias noticias como UPI y AP, numerosos y excelentes fotógrafos trabajan en un relativo anonimato. Sin embargo, el desarrollo de la fotografía digital y del Internet abriría nuevos espacios para conocer a muchos fotoperiodistas anónimos cuyos trabajos superan en numerosas ocasiones a los de los profesionales mimados de la crítica internacional. En la actualidad, muchos periodistas exponen sus obras con frecuencia en los salones de arte.

La primera organización nacional de reporteros gráficos del mundo se fundó en 1912 en Dinamarca con la Unión de Fotógrafos de Prensa (Pressefotografforbundet) constituida inicialmente por seis fotógrafos de Copenague. En la actualidad tiene 800 miembros.

En 1946 se fundó en Estados Unidos la Asociación Nacional de Fotógrafos de Prensa y en la actualidad cuenta con cerca de 10 mil miembros. En Gran Bretaña se fundó en 1984 la Asociación Británica de Fotógrafos de Prensa y fue relanzada en 2003; cuenta actualmente con 450 miembros. En 1989 comenzó la Asociación de Fotógrafos de Prensa en Hong Kong; en 2000 una similar en Irlanda del Norte, en 1930 la Pressfotografernas Klubb de Suecia y la Pressefotografenes Klubb de Noruega.

Organizaciones noticiosas y escuelas de periodismo en muchos países del mundo son dirigidas por fotoperiodistas que han recibido un notable reconocimiento público. Entre los reconocimientos más célebres del mundo se encuentran la ""Fotografía Destacada"" ("Feature Photography") y la ""Fotografía Noticiosa en el Punto'" ("Spot News Photography"). Otro premio es el de ""Foto Noticiosa Mundial"" ("World Press Photo"), lo ""Mejor en Fotoperiodismo"" ("Best of Photojournalism") y la ""Foto del Año"", así como la ""Foto Noticiosa del Año"" entregado en Gran Bretaña. Todos los países hispanoamericanos cuentan con numerosas asociaciones de fotoperiodistas.

La historia de la fotografía y del periodismo en España y Latinoamérica ha sido tan intensa como en el mundo anglosajón y ha dejado grandes maestros en ambos campos. 

El 10 de noviembre de 1839 se realizó el primer daguerrotipo en la ciudad de Barcelona y desde ese momento este se hizo popular en el resto de la península ibérica.

Entre lo que se podría considerar como el primer documental fotográfico en España se encuentra el cubrimiento de acontecimientos que llevó a cabo el británico Charles Clifford, que se radicó en Madrid desde 1850. Clifford registró las visitas a diferentes ciudades españolas que la reina Isabel II realizó a partir de 1858 con el ánimo de afianzar la monarquía y de paso el fotógrafo británico dejó un precioso material no solo de la reina, sino de las ciudades de la época.

Pascual Pérez Rodríguez ilustraba el "Diario Mercantil" de Valencia con daguerrotipos que utilizaba con negativo de papel, lo que le permitía hacer numerosas reproducciones. De otro extranjero, esta vez el francés Jean Laurent, quien llegó a España como corresponsal de "La Crónica" de París en 1857, queda un amplio trabajo de registro de la cultura y la arquitectura de la época.

La popularización de la fotografía en España, sobre todo a partir de la década de los 80 del siglo XIX, permitió que muchas personas se dedicaran a retratar los paisajes urbanos de la época, así como los cuadros de costumbres y tradiciones, entre los cuales destaca el toreo. Sin embargo, el apogeo de la fotografía en España, ampliamente favorecida por el avance tecnológico, se convertiría bien pronto en un entretenimiento de las clases media y alta que tenderían a registrar solo aquello que estuviera dentro del interés de clase para dejar de lado lo que no interesaba o se consideraba carente de estética, especialmente hacia finales del siglo.

Otros personaje que forman parte de la historia del fotoperiodismo español es Juan Comba cuyo trabajo más destacados tiene que ver con el incendio de Toledo (1887) y la visita de la reina Victoria de Inglaterra. Pero la prueba de la gran popularidad que la fotografía fue adquiriendo para la prensa en España es la buena lista de publicaciones que ya desde finales del siglo XIX se presentan, pero más que de publicaciones, la abundancia de nombres asociados a la fotografía, muchos de ellos anónimos y espontáneos. Entre las publicaciones más destacadas que dieron un puesto primordial a la fotografía figuran ABC (1903), El Gráfico, La Vanguardia (1881) y El Imparcial. Un ejemplo de la presteza que la sociedad española de principios de siglo tenía ya por el periodismo gráfico lo constituye el atentado contra el rey Alfonso XIII el 31 de mayo de 1906 durante el día de su boda por parte del anarquista Mateo Morral. El fotoperiodista, que por ello quedó en la historia, fue Eugenio Mesonero Romanos quien sentó un precedente de la pronteza del periodismo en los momentos más sorpresivos. 

Una de las grandes figuras del fotoperiodismo del siglo XX tuvo también a España como su principal escenario y en donde dio su vida. Se trata de la periodista alemana Gerda Taro la cual hizo un intenso cubrimiento de la Guerra Civil Española en la cual perdió la vida el 26 de julio de 1937 a tan solo 27 años de edad, pero con una brillante carrera en esta profesión registrada para la historia en sus fotografías de periodismo de guerra. Pero la Guerra Civil atraería a muchos otros célebres reporteros extranjeros como John Dos Passos, Ernest Hemingway, George Orwell, Antoine de Saint Exupéry, André Malraux, G. L. Steer, Herbert Matthews, Indro Montanelli y otros.

En España, la época dorada del reporterismo gráfico va ligada principalmente a la llegada de los medios digitales a las agencias de noticias y los medios de comunicación social. Como camarógrafos de televisión, destacan las figuras de José Couso (Ferrol, 1965 - Bagdad, 2003) y Miguel Gil Moreno (Tarragona, 1967 - Sierra Leona, 2000) ambos corresponsales de guerra que fueron asesinados cuando cubrían diferentes conflictos bélicos. Dentro del fotoperiodismo destaca Manuel Pérez Barriopedro (Madrid, 1947), como uno de los precursores del fotoperiodismo moderno en España. Tras él hay tres grandes olas de destacados profesionales de la fotografía periodística: En la primera ola destacan Gervasio Sánchez (Córdoba, 1959), Javier Bauluz (Oviedo, 1960), premio Pulitzer compartido en 1995, y Sandra Balsells (Barcelona, 1966), todos ellos caracterizados por un periodismo social y una fotografía documental. En la segunda ola se puede destacar a Luis Valtueña (Madrid, 1965 - Gatonde, Ruanda, 18 de enero de 1997), Emilio Morenatti (Zaragoza, 1969) y Ricard García Vilanova (Barcelona, 1971) todos ellos más centrados en el seguimiento de conflictos bélicos y los conflictos sociales violentos. Y en la tercera ola, la más fructífera e internacional de todas, destacan nombres como David Ramos (Barcelona, 1977), Samuel Aranda (Barcelona, 1979), Maysun (Zaragoza, 1980), Manu Brabo (Zaragoza, 1981), Jesús Blasco de Avellaneda (Melilla, 1981), Olmo Calvo (Santander, 1982), Arturo Rodríguez (La Palma, 1977) o Santi Palacios (Madrid, 1985). Esta última hora es más heterogénea destacando principalmente la fotografía de movimientos migratorios, las repercusiones de los conflictos sociales, crisis económicas, de refugiados y la defensa de los derechos fundamentales, aunque también la fotografía documental, los retratos o la información deportiva.

De la misma forma la experiencia del periodismo gráfico en los países hispanoamericanos hunde sus raíces desde los inicios de la fotografía ya a mediados del siglo XIX. De la experiencia fotográfica Latinoamérica, dice la estudiosa Liliana Martínez:

Y cita a Zamora que dice:

Sin embargo, para muchos autores como Boris Kossoy la fotografía latinoamericana del siglo XIX es en realidad una etapa ""europea"", es decir, fue realizada desde una perspectiva de la experiencia europea y estadounidense y fue categorizada por los mismos como "exótica", lo que creó un estereotipo de lo que es "el ser y la imagen del pueblo latinoamericano" ante sus ojos. Esta situación, que desconocía incluso la enorme diversidad de la América hispánica y portuguesa, sería primordial para el desarrollo de una fotografía que buscaría su propia autenticidad a lo largo del siglo XX y sería contestataria de la etiqueta exoticista europea y estadounidense. Gracias a la enorme diversidad cultural que conforman los países hispanoamericanos, al Brasil y a las Antillas, la fotografía Latinoamérica se presenta en realidad como un inmenso universo de propuestas y personajes que deben ser revisados por países, más que de manera masificada. 

La primera cámara fotográfica que llegó a un país latinoamericano lo hizo en 1840 de manos de un religioso francés, el Abad Louis Compte, en Brasil. Es de destacar que incluso la fotografía como invento tiene que ver con Brasil como uno de los países del mundo en donde se vivió el experimento de manera independiente a Estados Unidos y Francia con dicho religioso. La fotografía llega entonces al continente en una época en la cual ella misma está en proceso de desarrollo y en que los países latinoaméricanos viven su génesis nacional como repúblicas independientes del colonialismo europeo. Su fotografía entonces tendría que ver mucho con las aspiraciones de la imagen política y social de lo que dichas naciones esperaban de sí mismas y se dedican a retratar personajes, la diversidad de paisajes y acontecimientos que son hoy una gran riqueza histórica. En general, y como sucedió en España, la fotografía latinoamericana del siglo XIX fue vista como un medio ideal para el retratismo. 

Numerosos personajes relacionados con la fotografía en Latinoamérica demuestran la gran actividad del género en el continente como John A. Bennet, Charles de Forest Friedricks, Thomas Helsby, Benito Panunzi, Eugenio Courret, Juan José de Jesús Yas, Federico Lessmann y Melitón Rodríguez entre muchos otros.

Entre los acontecimientos históricos de mayor importancia que en Latinoamérica quedaron registrados en imágenes, figura la Guerra de la Triple Alianza vista por los lentes de los fotógrafos enviados por Bate & Cia para registrar el evento. El periódico El Comercio, fundado en Lima en 1839, utilizó el primer fotograbado el 31 de julio de 1898 y abre la experiencia del fotoperiodismo en la sede de la civilidad Inca testimoniado por el activo número de publicaciones que se presentarían a lo largo del siglo como el semanario ilustrado "Actualidades", la revista "Prisma" y el tabloide "La Crónica" que marcan los inicios de la historia del fotoperiodismo en Perú. Se destacan además "El Correo" (1871), "El Perú Ilustrado" (1889), "Actualidades" (1903), "Variedades" (1908), el diario "La Crónica" (1912), "Mundial" (1920), "La Prensa", "Caretas" (1950), "La nueva Crónica" (1970) y "La República" (1980).

Los fotoperiodistas colombianos, más reconocidos en la historia del fotoperiodismo en Colombia son Leo Matiz 1917 - 1998. Fotógrafo nacido en Aracataca, aquí unas imágenes de la exposición - homenaje titulado: Macondo vista por los ojos de Leo Matiz, celebrada en Canadá en 2007. México, Centroamérica, Estados Unidos, los andes latinoamericanos, el Caribe, Palestina, Beirut, Tel Aviv y Venezuela, son algunos de los escenarios en los que revoloteó el alma indoblegable y apasionada del fotógrafo Leo Matiz, orientando su mirada hacia lo que Henri Cartier Bresson denominó el momento decisivo, ese instante irrepetible en el que convergen lo inesperado de la vida humana, una retina capaz de ir más allá de los visible y una sensibilidad extraordinaria para comprender el vértigo de la historia y el drama humano más allá del implacable ritmo de las rotativas de prensa. http://www.leomatiz.org/biografia/ 
Las fotografías pertenecen al foto-periodista del Grupo Memoria Histórica de la CNRR Jesús Abad Colorado quien nació en Medellín en 1967, es periodista egresado de la Universidad de Antioquia y uno de los pocos fotógrafos que ha documentado las diversas caras del conflicto armado en Colombia. Su mirada valora la humanidad, que se esconde incluso en los rostros de los guerreros, para tomar distancia del uso de la fotografía como espectáculo y para satisfacer la curiosidad morbosa. Es además, un relator de historias, un testigo del desastre y la desolación, pero a la vez de la resistencia y la fortaleza de la gente.
Su trabajo aspira a recuperar la memoria del pasado, porque para él, crear memoria histórica a través de la fotografía es un imperativo ético para enfrentar los retos del presente y asegurar un futuro digno. Ha participado en más de 40 exposiciones individuales y colectivas tanto en Colombia como en el exterior.
Entre ellas, en la del 12º Encuentro Internacional de Fotoperiodismo "Ciudad de Gijón" 2008. Es coautor del libro Relatos e imágenes: El desplazamiento Forzado y Desde la Prisión, realidades de las cárceles en Colombia.
Ha recibido el Premio Nacional de Periodismo Simón Bolívar en tres oportunidades y en el año 2006, obtuvo los premios internacionales de Caritas en Suiza por su trabajo comprometido en la búsqueda de la verdad y la justicia social y el Premio Internacional a la Libertad de Expresión CPJ de los Estados Unidos, otorgado por el Comité para la Protección de Periodistas. http://www.facebook.com/pages/Jes%C3%BAs-Abad-Colorado/89003222812?sk=info

En México el primer daguerrotipo se realizó ya en 1839 pero también aquí los primeros fotógrafos se entretienen en el retratismo de las clases privilegiadas, hasta que hacia finales del siglo comienzan las primeras obras sobre las clases populares y una auténtica competencia por la originalidad, para dar lugar al fotógrafo como autor de un estilo reconocible, lo que jugaría un papel vital en el desarrollo del periodismo. El estallido de la Revolución mexicana en 1910 revelaría a otro gran personaje del fotoperiodismo, el alemán Hugo Brehme quien se radicó en Veracruz y después en Ciudad de México para hacer parte de la "Agencia Fotográfica Mexicana". De su cámara quedan imágenes de personajes de la talla de Emiliano Zapata. México, como España con la Taro, tendría que ver con uno de los grandes maestros del fotoperiodismo del siglo XX: Robert Capa y su cubrimiento de la Guerra Civil Española. Numerosos negativos del maestro de la reportería gráfica se dieron por perdidos, pero sería precisamente en el país azteca en donde volverían a la luz después de más de 70 años desaparecidos. De alguna manera el general mexicano Francisco Javier Aguilar recibió tres cajas que contenían 127 rollos y que fueron confiadas a él en Francia. Las cajas fueron heredadas por la familia del general en México, hasta que fueron descubiertas y puestas por primera vez a la luz pública después de más de medio de siglo.

La historia de la fotografía en Cuba comienza con los primeros daguerrotipos en 1840 y el país se enorgullece de ser uno de los primeros en inaugurar los estudios fotográficos del mundo, aún antes que en París. Pero la gran prueba para el fotoperiodismo cubano sería precisamente la Guerra de la Independencia del mando español hacia finales del siglo XIX. La Revolución Cubana en particular sería el principal motor que llevaría al género del reporterismo gráfico del país a elaborar sus propios esquemas originales y sensibles a las realidades sociales para ubicar su producción periodística fotográfica en una de las más destacadas de Latinoamérica.

En Argentina, en 1942 se funda la Asociación de Reporteros Gráficos de Argentina (ARGRA). Surge tras la necesidad de institucionalizarse como tal para regular los derechos de los reporteros gráficos. 

Como contexto internacional, se encuentra atravesada por la Segunda Guerra Mundial. Esto les perjudicaba de manera directa ya que no recibían los insumos suficientes, por parte de Europa y Estados Unidos, para seguir con la labor periodística pese a las constantes insistencias en ellas. 

A nivel nacional, los reporteros denuncian al Ministro de Gobierno de Buenos Aires, Vicente Solano Lima, por maltrato recibido por parte de la policía para ejercer su profesión. En simultáneo, reciben amenazas coercitivas durante el gobierno dictatorial de 1943.

ARGRA comienza a tener presencia cuando Perón es nombrado como Secretario de Trabajo y Previsión Social, afianzándose esta relación durante su gobierno presidencial.

En Córdoba, en 1939 se celebra el Primer Congreso Nacional de Periodistas, declarando al 7 de junio como día del periodista. Uno de los pioneros en fotoperiodismo en Córdoba fue Antonio Novello, quien trabajó para La Voz Del Interior, enfocando sus intereses no solo en cuestiones políticas, sino también de índole social y cultural.

Cora Gamarnik, coordinadora del Área de Estudios sobre Fotografía de la Facultad de Ciencias Sociales (UBA), encuentra dos grandes hallazgos respecto a la imagen en la historia argentina: una fueron lo que hicieron las Madres de Plaza de Mayo con las fotos de sus hijos. Caminando con pancartas con sus retratos lograron volver a instalar en la escena social a sus hijos vía la fotografía. Ellas venían a decir que aquellos a quienes se consideraban «subversivos», que podían ser aniquilados, eran jóvenes, hijos, hijas, padres, madres, que se reían, que festejaban. El solo mostrar la foto contradecía todo el discurso de la dictadura. El segundo caso, es el que tuvo el movimiento de mujeres. Afirma que antes las imágenes que vinculaban al aborto eran terribles, sanguinarias, cruentas y que ahí hubo una gran inteligencia del feminismo. Crearon un nuevo símbolo, el pañuelo verde. Las imágenes de las marchas se poblaron de vitalidad. De mujeres juntas, peleando por un derecho. Las fotos de vida y alegría se contrapusieron a la imagen sanguinaria que se mostraba entre los que siguen sosteniendo la ilegalidad del aborto."Las que estamos a favor de la vida somos nosotras", eso dicen también las imágenes. 

En la historia de la fotografía se puede evidenciar el avance de la tecnología desde mediados del siglo XIX a principios del siglo XXI y cómo cada paso afecta directamente los contenidos, formas y maneras del periodismo fotográfico. En dicho proceso la forma física de las cámaras fotográficas ha cambiado desde tamaños paquidérmicos que hacía de la fotografía un oficio de pocos y casi un privilegio de las clases altas a tamaños cada vez más pequeños y livianos que facilitaron la labor del fotoperiodismo. Sin embargo, hacia finales del siglo XX comienza un proceso aún más extraordinario: la existencia misma de la cámara fotográfica como único medio físico para obtener las imágenes se pone en un plano de completa relatividad al ser desarrollados otros medios que eran inimaginables hace cincuenta años. En la actualidad una foto puede ser hecha con un teléfono móvil o la videocámara de un ordenador portátil más centenares de artefactos que hacen que cualquier persona pueda realizar una fotografía. 

El desarrollo de cámaras fotográficas más pequeñas y livianas ha facilitado la tarea de los fotoperiodistas. Desde la década de los 60 artefactos electrónicos como flashes, una gran variedad de tipos de lentes y otros aditamentos han sido incorporados para hacer cada vez más fácil la función de tomar fotografías. Las nuevas cámaras digitales liberan a los periodistas de los largos rollos de películas porque pueden almacenar cientos de imágenes en diminutos artefactos electrónicos y tarjetas digitales. 

El contenido, en cambio, permanece el elemento más importante en la fotografía periodística, pero la habilidad de adquirir el material fotográfico y editarlo en un tiempo mucho más inferior que antes ha producido cambios significativos. Tan solo en 1980 se necesitaban 30 min para escanear y transmitir una sola fotografía en color desde una locación remota a una sala de prensa para ser impresa. Ahora, equipados con una cámara digital, un teléfono móvil y un ordenador portátil, un fotoperiodista puede enviar una imagen de alta calidad en minutos e incluso en segundos casi de manera contemporánea a la sucesión del evento noticioso. Los video teléfonos y los satélites portátiles permiten en la actualidad la transmisión de imágenes desde casi todos los puntos de la tierra.

Existen sin embargo preocupaciones por parte de los reporteros gráficos de que su profesión puede cambiar de tal manera que llegue a ser irreconocible debido al desarrollo natural de la tecnología en la captación de imágenes. Por otra parte, los sistemas de almacenaje electrónico de imágenes como aquellas de dominio público, han permitido que personas aficionadas a la fotografía incursionen en el campo del reporterismo gráfico con todo lo que ello significa para una profesión que, como el periodismo, se desarrolló de todas maneras a partir de la afición por la información.

Tres eventos han marcado una nueva era de lo que se conoce como el fotoperiodismo del siglo XXI: Los atentados del 11 de septiembre de 2001, la invasión de Iraq entre el 18 de marzo y el 1 de mayo de 2003 y el Tsunami del Océano Índico del 26 de diciembre de 2004. Estos tres eventos tuvieron una gran repercusión en lo que era el fotoperiodismo y su desarrollo desde el siglo XIX y a lo largo del siglo XX. Los tres eventos tuvieron un cubrimiento informativo total y se conserva una gran cantidad de material visual en fotografía, videos, esquemas, gráficos, mapas animados, análisis gráficas de la situación y otros que los hacen un modelo de estudio de lo que significa el fotoperiodismo. Además de la cantidad descomunal de material de primera mano que se produjo en el mismo momento en que sucedían los eventos, la nota que marca el inicio de una nueva era para esta disciplina es que quienes produjeron dicho material de manera inmediata no fueron las grandes estrellas del periodismo internacional, sino aficionados. Cuando los grandes y medianos medios de comunicación gráfica y visual publicaron las primeras imágenes, al menos en lo que respecta a los atentados del 11 de septiembre de 2001 y el tsunami de 2004, publicaban imágenes que no fueron hechas por profesionales de la información sino por personas que estaban en ese momento en el lugar e hicieron las tomas con cámaras digitales, videocámaras y teléfonos móviles.

Las consecuencias de este fenómeno inciden en las maneras que se habían establecido como norma de la comunicación social. En primer lugar, los eventuales "reporteros" no siguen el estricto proceso de "edición" de la imagen, sino que la presentan de la manera en que esta fue tomada o, como señala D. Perlmutter, en muchas ocasiones retocadas. La imagen adquiere entonces un valor total en su significado que sobrepasa incluso a la preocupación por su calidad. Si la foto de las gigantescas olas que devastaron las playas meridionales de Asia tiene o no tiene calidad en la luz, eso no importa, puesto que se hace única, ya que fue tomada por un testigo ocular que, con suerte, tenía su liviana cámara digital con la cual tomaba fotos de sus paseos de verano. Posteriormente los aficionados suben sus fotos a las redes de Internet diseñadas para ello y estas pueden ser accedidas por millones de usuarios en todo el planeta. De la misma manera en que la producción fotográfica se convirtió en oficio accesible a todos, el nuevo control de edición de la fotografía y el video queda en manos de las cada vez más amplias comunidades que se forman alrededor de las redes de información libre como Wikimedia Commons, Flickr, Fotolog.com y muchos otros. Sin embargo, dichos sistemas comunitarios virtuales se preocupan más por las licencias de publicación que por la calidad misma de los archivos que los usuarios suben al sistema.

Si bien estos tres eventos de gran significado internacional se marcan como el inicio de una nueva era para el fotoreporterismo, es cierto que otros eventos previos ya eran el preludio de lo que venía con el desarrollo y la popularización de las nuevas tecnologías y especialmente del Internet.

Para algunos observadores este nuevo fenómeno de la popularización del fotoperiodismo significa su muerte desde el punto de vista que este puede ser desarrollado ya no de manera exclusiva por profesionales de la información, sino por cualquiera que tenga un teléfono móvil. Ante ello dice "La Tecla", la asociación de periodistas cubanos:

Para otros se trata de una crisis de la fotografía documental, así como se habla de una crisis de los medios. Mientras Jacob Riis hacia finales del siglo XIX elaboró todo un trabajo en los bajos fondos de Nueva York para sacar la realidad social marginal a la luz pública, en la actualidad las cámaras de los aficionados están prácticamente en todo el planeta y basta digitar en los motores de búsqueda cualquier término sobre cualquier problema o situación en cualquier parte del mundo para obtener fotografías. De ello "La Tecla" de Cuba analiza que dicha crisis implica que ningún acontecimiento puede escapar al reflejo de la imagen ""lo cual es beneficioso para el día a día que implica el periodismo convencional"". Es decir, los profesionales de la información deben ver esta nueva época como una oportunidad, más que como una amenaza a la profesión y el cambio en lo que significa la imagen para los habitantes del siglo XXI, así como lo fue durante el siglo XIX cuando a la pintura le surgió la fotografía misma. Por el contrario, el fotógrafo Clemente Bernad dice que no hay tal crisis desde que el fotoperiodismo siempre lo ha estado:

En el libro de Joan Fontcuberta, "El beso de Judas," habla de los esfuerzos de muchos autores para defender la fotografía documental, entendiéndola como una palabra que alberga conceptos totalmente objetivos. Por ejemplo, desde la exposición titulada "New Topographics", salieron voces como la de Bevan Davis hablando sobre " "el esfuerzo hecho para que la cámara vea por ella misma"", o Lewis Baltz ""Quiero que mi trabajo sea neutral y libre de cualquier postura estética o ideológica"". Frente a un documentalismo teóricamente neutral y de no-intervención, Fontcuberta defiende el término de "documentalismo subjetivo", argumentando que un fotógrafo acaba proyectando de manera inconsciente sus propias vivencias y manera de ver las cosas. ""Cada vez más lo subjetivo prevalece sobre lo documental, lo abstracto sobre lo descriptivo"."

Finalmente lanza una afirmación sobre la muerte del documentalismo, sosteniendo que:

Al largo de todo el libro sostiene el mismo pensamiento sobre la ficticia objetividad de la fotografía, criticando dicha creencia y reflexionando sobre su propia naturaleza

Sin embargo, la fotografía aficionada tiene sus límites y si muchas obras adquieren celebridad es por la inmediatez de la noticia y porque no existen versiones cualificadas mejores. De esta manera, una gran parte del material que circula en los medios de comunicación ha perdido la cualidad requerida y ha dado lugar a una cierta laxitud en la presentación de la imagen a la vista de algunos observadores. Sin embargo, en este punto, el profesional de la información encuentra de nuevo su espacio de autoridad, porque sus obras adquieren el dato distintivo de la cualidad que la inmediatez de la cámara digital de un turista no posee, aunque muchos aficionados en realidad tienen una perspectiva más que profesional. 

El rigor de la fotografía periodística tiene en cuenta los mismos elementos que la fotografía artística y muy especialmente se cuida de los elementos semánticos, es decir, de significado que esta posee. En muchos casos, la fotografía aficionada no tiene en cuenta el elemento más básico de la fotografía que es la luz ambiental y el manejo del flash, lo que causa que muchas de las fotos tengan un encuentro caótico de sombras, personajes con los ojos cerrados por la molestia de la luz de un flash automático y, muy especialmente, carencia de encuadres y estilos apropiados que hacen fotos repetitivas y monótonas. El fotógrafo profesional, es consciente de cosas como la relación de figura y fondo y la importancia de la armonía entre ambos. Para el aficionado ocasional lo que importa es el objeto a fotografiar y no se cuida de lo que se refleje en el contexto. Abundan fotos ausentes de la figura humana, malos encuadres, malos poses de personajes entrevistados que ignoran el "cuadro psicológico" y en muchas ocasiones una gran pobreza de imaginación.

Para el estudioso del tema, André Bazin, como para otros intelectuales de la fotografía, esta, a diferencia del arte, no crea, sino que "embalsama el tiempo". Dicha característica le da una grave responsabilidad al fotógrafo como asegura Mariela Cantú en su estudio ""Fotografía, video, digital: sobre los modos de repensar un medio"":

El fotoperiodismo puede informar mediante varias ópticas. Dentro de este hay cuatro tipologías diferentes, que, a pesar de que representan diferentes modalidades de la fotografía, están fuertemente relacionados: la fotografía informativa, publicada por medios con una finalidad puramente informativa y editorial; la fotografía testimonial, conocida también como fotografía documental; el ensayo (una forma autoral de expresión, opinión e interpretación de los hechos, generando un mensaje complejo con un punto de subjetividad) y por último la foto ilustración.

En el fotoperiodismo el mensaje está claramente determinado por objetivos informativos, teóricamente comprometidos con la sociedad.

La primera tipología responde a las cinco preguntas del periodismo clásico:¿"Quién?, ¿como?, ¿cuándo?, ¿dónde? Y ¿por qué?" Se encuentra sujeta al mercado de la noticia y cumple plenamente con los objetivos informativos. Caracterizada por su compromiso con "la realidad", uno de sus grandes paradigmas es la identidad del que se entiende como "la verdad".

Este tipo de fotografía no profundiza en el mensaje. Su prioridad es informar del hecho, rehuyendo a menudo de los conceptos profundos o detallados sobre el significado de la noticia. Los principales medios de la fotografía informativa son los masivos de comunicación, con una gran cobertura mediática y mucha rapidez para difundir información a una gran cantidad de gente mediante diarios, revistas u otros canales. 

Otra de sus características es la inmediatez. Esta se encuentra sujeta en el presente y con el paso del tiempo va perdiendo cierta vigencia, debido a que el mundo evoluciona y cambia diariamente. Por esta razón, entre otras, los medios de comunicación no suelen profundizar mucho en la información que transmiten, puesto que se da más importancia a comunicar que no analizar en detalle.

La foto informativa se entiende como un vehículo de comunicación por los medios, los cuales atienen a diferentes temáticas de información que van acuerdos con el interés de los lectores y de las líneas editoriales del mismo medio. En función de los temas fotografiados se puede dividir este subgénero en diferentes categorías: Política, Deportes, Sociales, Cultura, Vida cotidiana, Ciencia... Con posibles variaciones de temas.

Esta tipología tradicionalmente es conocida como fotografía documental. Nace de la práctica de observar fotográficamente cualquier aspecto del mundo. Con la imagen informativa comparte el compromiso con la realidad, a pesar de que esta, más allá de informar sobre una noticia, hay un análisis de esta. Su objetivo, además de informar, es concienciar y crear un pensamiento público, sustentándose en la opinión del fotógrafo. Los principales canales de difusión de esta tipología son los foros más especializados, dirigidos a un público concreto y restringido.

Se difunde mediante revistas temáticas, secciones especializadas en medios oficiales o portales de Internet, entre otros. Normalmente no se encuentra en medios masivos, por lo cual tiene una audiencia más limitada que la fotografía informativa.

Un ejemplo de este tipo de imágenes es el trabajo de en Jcob Riis sobre los guetos neoyorquinos, una fotografía comprometida con la realidad pero que a la vez describe fenómenos estructurales y sociales que transcienden de la inmediatez y la poca implicación del medio.

En la fotografía testimonial hay una implicación, análisis e interpretación de la realidad por parte del fotógrafo y el tiempo no es definitorio. Muchas veces este subgénero va ligado a proyectos de la denuncia política, injusticias sociales o situaciones de minorías marginadas, entre otras cosas.

Del mismo modo que la fotografía documental, el ensayo fotográfico está claramente sujeto al criterio y opinión de su autor, quien realiza un análisis en profundidad del tema tratado. Constituye una investigación temática iconográfica, casi siempre subordinada en su punto de vista del fotógrafo, quien expresa su forma de pensar en la manera en que explica el mensaje.

Con un gran parecido con tipología escrita, el núcleo del ensayo fotográfico es la tesis del autor y el posicionamiento que este asume ante el hecho fotografiado. Muestra una perspectiva personal desde la cual el fotógrafo enseña fenómenos con una emocionalidad técnica, creatividad y estética determinada.

Se entiende este tipo de fotografía como un trabajo puramente del autor. Sus imágenes suelen superar el documentalismo, deviniendo mensajes culturales o ideológicos. En este, los tiempos son mucho más dilatados: generalmente se realizan en grandes periodos donde hay una extensa investigación, reflexión y una cierta participación vivencial del fotógrafo con el tema retratado.

Esta modalidad ofrece una gran libertad creativa, temática y expresiva. Este hecho tiene como consecuencia que se abren muchas posibles discusiones y debates en torno a los temas tratados por el ensayo. A causa de su temática, su público y canales de difusión son más limitados que en otras tipologías de fotoperiodismo. Generalmente llega al público mediante galerías, museos, revistas especializadas, centros culturales...

El ensayo fotográfico tiene muchas similitudes con su expresión escrita: las imágenes que lo integran tienen una estructura lógica con un discurso jerarquizado en temas, argumentos y puntos de vista. Las relaciones que se establecen entre las imágenes y los temas sugieren secuencias, parecidos y enfatizan puntos de vista, entre otros.

Un ejemplo de esta tipología es la fotógrafa Graciela Iturbide.

Aunque la foto ilustración es definida como una categoría dentro de la fotografía de prensa, actualmente hay un debate sobre si considerar esta como una tipología del foto periodismo o entenderla como un género aparte.

Denominado por Pepe Baeza, este término lo podemos definir como toda imagen fotográfica, ya sea compuesta por fotografías en "collage," montaje; o imágenes combinadas con otros elementos gráficos que cumplen la función clásica de la ilustración. La finalidad de esta es la mejor comprensión de un objeto, hecho, concepto o idea, ya sea con una representación mediante mímesis o bien representando los rasgos esenciales para su comprensión.

La tipología comentada se caracteriza para depender de un texto previo que marca y origina la imagen, de forma que la foto ilustración tiene que explicar este para que el destinatario se pueda acercar de una manera lo más objetiva posible a su contenido. Normalmente tiene una vocación didáctica o divulgativa, y se encuentra ligada al desarrollo de los nuevos modelos de prensa. Aun así, no tiene ningún tipo de intención ligada a la representación de una realidad social, ni se refiere a noticias, puesto que su objetivo más que informar es ilustrar, ayudar a entender o motivar.

Uno de los campos en el cual la fotografía periodística se ha destacado es en el deporte. La misma agilidad deportiva ocasiona que el fotoperiodista deportivo demuestre sus habilidades artísticas e informativas tanto como el jugador lo hace en el campo de juego. Si alguien lee la historia del deporte o la historia de un deporte en particular, la imagen, sea fotográfica que de video, hace parte vital de dicha lectura. En la fotografía deportiva, es posible apreciar la habilidad de mostrar la velocidad, la fuerza, la grandeza del equipo o del deportista, el fuerte sentido psicológico de la derrota o el triunfo, el ambiente festivo de los aficionados o su sensación de desilusión. La importancia del fotoperiodismo deportivo ha sido un proceso paulatino que viene desde la mera redacción de los eventos deportivos, a la presencia obligada de la imagen para el relato.

 en realidad el fotoperiodismo dedicado a seguir los pasos del espectáculo, la farándula y los famosos tiene su importancia y ha contribuido de alguna manera al desarrollo del mismo periodismo. Lógicamente este tipo de periodismo tiene que ver también con el exceso de los paparazzi que han contribuido a demeritar el papel del fotoperiodista en este campo, confundiéndose con frecuencia con este tipo de persecutores empecinados de las estrellas del espectáculo hasta el punto de invadir su intimidad.

El periodismo fotográfico trabaja dentro de las mismas aproximaciones objetivas que se aplican a otros campos del periodismo. Cómo obturar, encuadrar las tomas y editar son consideraciones constantes.

Por lo general, conflictos éticos pueden ser mitigados o asumidos por las acciones de un subdirector o el editor gráfico quien toma control de las imágenes una vez que estas han sido consignadas a la organización noticiosa. El fotoperiodista pierde el dominio de su obra una vez esta es publicada. 

El surgimiento de la fotografía digital ha creado nuevas oportunidades para la manipulación, reproducción y transmisión de imágenes. Este hecho ha complicado todos los aspectos técnicos y legales que ello envuelve. Las asociaciones nacionales de periodismo fotográfico en los diferentes países y otras organizaciones profesionales y de los derechos humanos, mantienen códigos de ética a este respecto.

Los problemas mayores acerca de asuntos éticos concernientes al periodismo se inscriben en menor o en mayor grado en las materias legislativas de los diferentes países. Sin embargo, la materia legal se complica por el hecho de que los medios de comunicación, especialmente en los tiempos de la revolución tecnológica y digital, rompen las fronteras internacionales y las imágenes publicadas en un país bajo el respeto de las normas legales del mismo, llegan a otras naciones con diferentes leyes.

El desarrollo de las nuevas tecnologías y el inicio de una nueva era del fotoperiodismo como se menciona arriba, afecta también las normas éticas tradicionales. El aficionado ocasional o profesional que hace una fotografía o toma un vídeo de una situación noticiosa, apurado por la inmediatez, pocas veces se detiene a hacer consideraciones éticas o a pensar en otro tipo de consecuencias de lo que la publicación de imágenes puede acarrear. Incluso si ciertos estados autoritarios que suelen censurar a la prensa de sus países, tratan de poner el límite a la sensación de libertad de expresión que en muchas ocasiones garantiza la tecnología y el Internet, la circulación de imágenes entre los diferentes puntos del planeta a través de medios digitales se volvió tan sutil que ningún sistema político puede controlar de manera eficaz. Un ejemplo lo constituyó las protestas populares en contra del régimen militar birmano en agosto de 2007. A pesar de que la Junta Militar intentó censurar totalmente la emisión de fotografías y vídeos de las marchas y de los actos represivos hacia el exterior por parte de aficionados extranjeros, esto no fue posible en su totalidad y, por el contrario, el mundo fue testigo de uno de los actos de represión de la libertad de prensa frente a las mismas cámaras con la muerte de Kenji Nagai, un fotorreportero japonés de la "APF News" que fue asesinado el 28 de agosto de 2007 por militares cuando fotografiaba las marchas.






</doc>
<doc id="4751" url="https://es.wikipedia.org/wiki?curid=4751" title="Édouard Manet">
Édouard Manet

Édouard Manet (París; 23 de enero de 1832-"Ibídem"; 30 de abril de 1883) fue un pintor y grabador francés, reconocido por la influencia que ejerció sobre los iniciadores del impresionismo.

Nació en París el 23 de enero de 1832, en una familia acomodada. Sus días escolares pasaron sin acontecimientos destacables y terminó su formación sin obtener la calificación necesaria para estudiar derecho, para decepción de su padre, que era magistrado. Sus primeros contactos con el arte se produjeron en la etapa escolar. 

A los dieciséis años viajó a Río de Janeiro como marinero en prácticas, con intención de ingresar en la Academia Naval Francesa.

Con el consentimiento paterno, decide iniciar los estudios en el taller de Thomas Couture. Las clases se complementaban con visitas a museos. En 1856 abandona el taller ya que consideraba anticuadas las enseñanzas del maestro. Estuvo casi seis años como alumno de Thomas Couture, un pintor muy estrecho de miras como profesor. Al mismo tiempo pudo copiar en el Louvre cuadros no solo de Tiziano y Rembrandt, sino también de Goya, Delacroix, Courbet y Daumier. De Couture aprendió que para ser un gran maestro hay que escuchar las enseñanzas de los que lo han sido en el pasado

Desde 1853 hasta 1856 Manet se dedicó a viajar para acabar su formación por Italia, los Países Bajos, Alemania y Austria, copiando a los grandes maestros. También visitó España quedando impresionado con las costumbres, el folclore y el mundo de los toreros.

El 26 de octubre de 1863 contrajo matrimonio con la pianista neerlandesa Suzanne Leenhoff, con quien mantenía relaciones desde 1850. A pesar de la boda, el que casi con seguridad era su hijo, León, nacido en 1852, siguió llevando el apellido de la madre, y pasando por hermano de ella. Es uno de los personajes del cuadro "Homenaje a Delacroix", que Henri Fantin-Latour pintó en 1864.

En agosto de 1865 emprendió un viaje por España, organizado por su amigo Zacharie Astruc, en el que descubrió la pintura barroca española, en particular a Diego Velázquez, que tendrá una enorme influencia en su obra.

En 1869 tomó como discípula a Eva Gonzalès, hija de un conocido novelista, que le había sido presentada por un marchante de arte. La joven, que no carecía de talento pero a la que le faltaba iniciativa, fue la única alumna de Manet, quien la retrató en 1870.

Al estallar la guerra franco-prusiana Manet fue movilizado al igual que otros impresionistas. Envió a su familia a Oloron.Ste.-Marie, en los Pirineos, y envió trece cuadros a Duret, quien recibió un sin número de pinturas de Manet, antes de entrar como teniente al servicio de la Guardia Nacional durante el sitio de París. Tras la declaración del armisticio en 1871 formó parte, junto con otros catorce pintores y diez escultores, de la federación de artistas de la efímera Comuna de París. Mantuvo buenas relaciones con los jóvenes impresionistas, muy en especial con Claude Monet; aunque siempre se resistió a participar en las exposiciones independientes, prefería ofrecer sus obras al salón y exponerlas en su propio estudio si eran rechazadas.

En 1872 Paul Durand-Ruel adquirió por 35 000 francos veinticuatro obras de Manet y organizó la primera exposición de pintores impresionistas aunque no tuvo éxito comercial. Sin embargo, entre estos artistas iba surgiendo una conciencia de grupo que los llevaría a formar la Société anonyme des artistes para realizar exposiciones colectivas.

En esta época, en la que tuvo mucha relación con Monet, Édouard Manet empezó a adoptar las técnicas impresionistas, si bien rehusará a participar en las exposiciones colectivas. En cambio, organiza una exposición de sus obras en su propio taller de la parisina calle de St. Petersboug, que gozó de bastante popularidad, rumoreándose en la época que había tenido unas 4000 visitas.

Hacia 1880, su salud empezó a deteriorarse a causa de un problema circulatorio crónico que no mejoró a pesar de someterse a tratamientos de hidroterapia en Bellevue. En esta época se reconoció su talento con una medalla de segunda clase concedida por el Salón y también fue nombrado Caballero de la Legión de Honor.

El 20 de abril de 1883, a causa de su enfermedad circulatoria crónica, le fue amputada la pierna izquierda, y diez días más tarde falleció a los 51 años de edad.

De todos los artistas de su tiempo, Manet fue quizás el más contradictorio. Aunque se le consideraba un personaje controvertido y rebelde, Manet se pasó casi toda su vida buscando la fama y la fortuna y, lo que quizás sea más importante, un pintor que ahora es aceptado como uno de los grandes, solía mostrarse inseguro de su dirección artística y profundamente herido por las críticas hacia su obra. Tuvo que esperar al final de su vida para conseguir el éxito que su talento merecía. Pese a que se le considera uno de los padres del Impresionismo, nunca fue un impresionista en el sentido estricto de la palabra. Por ejemplo, jamás expuso con el grupo y nunca dejó de acudir a los Salones oficiales, aunque le rechazaran. Afirmaba que «no tenía intención de acabar con los viejos métodos de pintura ni de crear otros nuevos». Sus objetivos no eran compatibles con los de los impresionistas, por mucho que se respetaran mutuamente.

La notoriedad de Manet, al menos en las etapas tempranas de su carrera, se debió más a los temas de sus cuadros, considerados escandalosos, que a la novedad de su estilo. Hasta mediados de la década de 1870 no empezó a utilizar técnicas impresionistas. En este sentido, Bownes se muestra bastante convincente al demostrar que, de joven, sin llegar a considerarse un innovador, Manet sí trataba de hacer algo nuevo: buscaba crear un tipo libre de composición que estaría, sin embargo, tan herméticamente organizada en su superficie como los cuadros de Velázquez.

En 1859 presentó por primera vez al Salón su "Bebedor de absenta", un cuadro que permitía sin problemas adivinar su adoración por Frans Hals, pero que provocó una turbulenta reacción en el público y en el jurado, inexplicable sin duda para un Manet que durante toda su vida lo único que buscó fue el éxito dentro de la respetabilidad. 

En los años sesenta, sin embargo, su pintura de tema español, tan de moda por entonces en Francia, fue bastante bien acogida y en 1861 el Salón aceptó por primera vez un cuadro suyo, el "Guitarrista español".

El tono general de la obra de Manet no es el de un pintor radical únicamente preocupado por el mundo visual. Es un sofisticado habitante de la ciudad, un caballero que se ajusta en todo al concepto decimonónico de "dandi": un observador distante, refinado, que contempla desde una elegante distancia el espectáculo que le rodea. Desde este punto de vista, Manet concluye el que será, sin duda, uno de sus cuadros más escandalosos, rechazado en el Salón de 1863 y expuesto en el de los Rechazados, "Almuerzo sobre la hierba".

El reto lo planteaba una realidad contemporánea, los bañistas del Sena, y la escena estaba reformulada en el lenguaje de los viejos maestros (el cuadro está claramente inspirado en la "Fiesta campestre" del Giorgione), compitiendo con ellos y, al mismo tiempo, subrayando las diferencias. Las escenas con el tema del ocio en el campo estaban ya muy enraizadas en el arte occidental y abundaban tanto en las ilustraciones populares como en el arte académico, pero el cuadro de Manet pertenece a un orden distinto, desconcertante por la evidente inmediatez con que se enfrenta al espectador.

Sin embargo, pese a la aparente unidad del grupo, cada figura es una entidad separada, absorta en su propia actitud o meditación, de manera que ningún tipo de conexión narrativa puede explicar el conjunto. Y esta sensación de ruptura hace que el cuadro parezca desintegrarse en una especie de collage de partes independientes que solo por un instante se agrupan gracias a su parecido, prestado, con el orden renacentista.

Pero más escandalosa todavía fue la "Olympia", pintada en 1863 pero no presentada al Salón hasta 1865, donde fue rechazada. Entre las razones por las que este cuadro iba a resultar chocante no son las menos importantes el hecho no solo de que es una clara parodia de una obra renacentista, (la "Venus de Urbino" de Tiziano), sino también una flagrante descripción de los hábitos sexuales modernos.

Manet sustituye en él a una diosa veneciana del amor y la belleza por una refinada prostituta parisina. Pero lo que realmente desconcertó a los críticos de la época es que Manet no la sentimentaliza ni la idealiza, y Olimpia no parece ni avergonzada ni insatisfecha con su trabajo. No es una figura exótica o pintoresca. Es una mujer de carne y hueso, presentada con una iluminación deslumbrante y frontal, sobre la que el pintor muestra un perturbador distanciamiento que no le permite moralizar sobre ella.

Ambas obras entusiasmaron a los pintores más jóvenes por lo que suponían de observación directa de la vida contemporánea, por su naturalidad y por su emancipación técnica. Manet se convirtió así, casi sin quererlo, en el personaje principal del grupo que se reunía en el Café Guerbois, la cuna del Impresionismo.

En 1867, hacia la época de la Segunda Exposición Universal en París, Manet, muy desalentado por su mala acogida en el Salón oficial, decidió seguir el ejemplo de Courbet unos años antes y dispuso, con su propio dinero, un pabellón donde presentó cerca de cincuenta obras sin obtener, sin embargo, ningún éxito público. 

En el prólogo del catálogo es muy probable que le ayudara su amigo el novelista Zola porque, de hecho, para su pintura durante toda la década de 1860, Manet contó con el apoyo escrito de Zola desde su puesto de crítico de arte en la revista semanal "L'Evenement". Bajo estas circunstancias Manet pintó de él en 1867-68, un retrato a la vez extraño y programático.

Ningún pintor del grupo impresionista ha sido tan discutido como Manet. Para algunos fue el pintor más puro que haya habido jamás, completamente indiferente ante los objetos que pintaba salvo como excusas neutras para situar un contraste de líneas y sombras. Para otros, construyó simbólicos criptogramas en los que todo puede ser descifrado según una clave secreta pero inteligible. Para algunos, Manet fue el primer pintor genuinamente moderno que liberó al arte de sus miméticas tareas. Para otros, fue el último gran pintor de los viejos maestros, demasiado enraizado en una multitud de referencias histórico-artísticas. 

Algunos todavía creen que fue un pintor de técnica deficiente, completamente incapaz de conseguir una coherencia espacial o compositiva. Otros piensan que fueron precisamente estos "defectos" los que constituyeron su deliberada contribución a las drásticas y enormemente fructíferas transformaciones que introdujo en la estructura pictórica.

En el Salón de Otoño de 1905 se realizó una exposición en reconocimiento al artista Edouard Manet.






</doc>
<doc id="4753" url="https://es.wikipedia.org/wiki?curid=4753" title="Astronomía extragaláctica">
Astronomía extragaláctica

Se llama astronomía extragaláctica al estudio de objetos fuera de la Vía Láctea.

La astronomía extragaláctica nació como tal cuando Edwin Hubble descubrió las Cefeidas en la nebulosa de Andrómeda, confirmando que por su distancia debía estar fuera de nuestra galaxia y que por su tamaño debería ser una galaxia comparable o incluso más grande que la nuestra.

Más tarde se encontró que las galaxias no se encuentran aisladas, sino formando grupos de diferentes tamaños. Además, existe una organización jerárquica donde agrupaciones más pequeñas forman parte de agrupaciones mayores.



</doc>
<doc id="4754" url="https://es.wikipedia.org/wiki?curid=4754" title="George Lucas">
George Lucas

George Walton Lucas Jr. (Modesto, California; 14 de mayo de 1944), conocido internacionalmente como George Lucas, es un cineasta, filántropo y empresario estadounidense. Lucas es más conocido por crear las franquicias de "Star Wars" e "Indiana Jones" y fundar Lucasfilm, LucasArts e Industrial Light & Magic. Se desempeñó como presidente de Lucasfilm antes de venderlo a The Walt Disney Company en 2012. 

Después de graduarse de la Universidad del Sur de California en 1967, Lucas cofundó American Zoetrope con el cineasta Francis Ford Coppola. Lucas escribió y dirigió "THX 1138" (1971), basado en su corto estudiantil anterior "Electronic Labyrinth THX 1138 4EB", que fue un éxito crítico pero un fracaso financiero. La película fue elegida para competir en la Quincena de Realizadores en el Festival de Cannes de ese mismo año. Su siguiente trabajo como escritor-director fue la película "American Graffiti" (1973), inspirada en su juventud a principios de la década de 1960 en Modesto, California, y producida a través del recién fundado Lucasfilm. La película fue crítica y comercialmente exitosa, y recibió cinco nominaciones a los Premios de la Academia, incluyendo .

La siguiente película de Lucas, la épica ópera espacial "" (1977), tuvo una producción problemática, pero fue un éxito sorpresa, convirtiéndose en la película más taquillera de la época, ganando seis Premios de la Academia y provocando un fenómeno cultural. Lucas produjo y coescribió las secuelas "The Empire Strikes Back" (1980) y "Return of the Jedi" (1983). Con el director Steven Spielberg, creó, produjo y coescribió las películas de "Raiders of the Lost Ark" (1981), "Indiana Jones and the Temple of Doom" (1984), "Indiana Jones y la última cruzada" (1989) e "Indiana Jones y el reino de la calavera de cristal" (2008). También produjo y escribió una variedad de películas y series de televisión a través de Lucasfilm entre los años 1970 y 2010, entre las que se encuentran "Kagemusha" (1980) y "Mishima, una vida en cuatro capítulos" (1985).

En 1997, Lucas volvió a lanzar la Trilogía de Star Wars como parte de una edición especial con varias modificaciones; Las versiones de los medios domésticos con más cambios se lanzaron en 2004 y 2011. Volvió a dirigir con una trilogía de la precuela de Star Wars que comprende "" (1999), "" (2002) y "" (2005). La última vez que colaboró ​​en la serie de televisión animada por CGI "" (2008-2014, 2020) y la película de guerra "Red Tails" (2012).

Lucas es uno de los cineastas más exitosos e influyentes de la historia y ha sido nominado a cuatro premios de la Academia. Es considerado una figura significativa del movimiento "Nuevo Hollywood" y sus películas se encuentran entre las "películas con las mayores recaudaciones" de la taquilla norteamericana, ajustadas por la inflación del precio de las entradas.

Fue criado en un rancho de Modesto, California, lugar de cultivo de nueces. Su padre era dueño de una tienda de venta de escritorios y tenía tres hermanos. Durante su adolescencia estudió en la Downey High School y era un fanático de las carreras de automóviles, incluso tenía planeado convertirse en un piloto de carreras profesional. Sin embargo sufrió un terrible accidente con su coche poco tiempo después de graduarse en la secundaria, estuvo hospitalizado por varias semanas y estuvo a punto de morir. Este accidente cambió definitivamente su manera de ver la vida. Decidió ir a la preparatoria Modesto Junior College, donde tomó materias como antropología, sociología y literatura, entre muchas otras. También empezó a filmar con una cámara de 8mm.
Durante esta época, Lucas y su amigo John Plummer comenzaron a interesarse en el cine Canyon Cinema: proyecciones de cineastas asociados al underground y al avant-garde, como Jordan Belson, Stan Brakhage y Bruce Conner.

Lucas y Plummer también comenzaron a ver películas clásicas del cine europeo, como "À bout de souffle", de Jean-Luc Godard, "Jules y Jim" de François Truffaut y "8½" de Federico Fellini. “Ahí fue cuando George comenzó realmente a explorar”, ha dicho Plummer. Gracias a su interés en las carreras automovilísticas Lucas conoció al renombrado cinematógrafo Haskell Wexler quien también compartía su gusto por las carreras. Wexler, quien después trabajaría con Lucas en varias ocasiones, se impresionó por el talento del muchacho: «George tenía muy buen ojo y pensaba de una manera muy visual».

Lucas se mudó a la Universidad del Sur de California (USC), donde estudió en la Escuela de Artes Cinematográficas. USC fue una de las primeras universidades en Estados Unidos en contar con una escuela dedicada a la educación del cine. Durante sus años en USC, Lucas compartió el cuarto de los dormitorios universitarios con Randall Kleiser. En el campus también forjó amistad con compañeros como Walter Murch, Hal Barwood y John Milius, entre otros. Este grupo de amigos fue conocido en el campus como The Dirty Dozen (una referencia a la cinta del mismo nombre, estrenada en 1967 y dirigida por Robert Aldrich); los doce alcanzarían cierto éxito en la industria cinematográfica de Hollywood.
Lucas también se hizo muy buen amigo de un alumno destacado del campus, con quien colaboraría después en Indiana Jones: Steven Spielberg. Lucas se vio fuertemente influenciado por el curso de Expresión Fílmica impartido por el cineasta Lester Novros y agrupaba los elementos no narrativos de la forma fílmica como el color, la luz, el movimiento, el espacio y el tiempo. Otra inspiración fue el realizador serbio (y decano del Departamento de Cine de la USC) Slavko Vorkapich, un teórico del cine que realizó grandes secuencias de montajes para estudios de Hollywood como MGM, RKO y Paramount. Vorkapich enseñó la naturaleza autónoma de la forma cinemática, y enfatizaba la cualidad dinámica única del movimiento y la energía cinética inherente en las películas.

Como estudiante de cine hizo varios cortometrajes incluyendo: Freiheit (1966) y "Electronic Labyrinth THX 1138 4EB" ("Laberinto electrónico THX 1138") que ganó el primer premio en el festival nacional de películas de estudiantes 1967-68. También en 1967 Warner Bros le concedió una beca que le permitió observar el rodaje de "Finian's Rainbow" (1968), dirigida por Francis Ford Coppola. Cuando comenzó su aprendizaje en los estudios de Warner Brothers dijo que deseaba trabajar en el departamento de animación. Justamente, cuando él llegó era el último día de dicho departamento, que luego fue cerrado. Con el tiempo Lucas y Coppola se transformaron en buenos amigos y juntos crearon en 1969 una compañía llamada American Zoetrope. El primer proyecto de la compañía fue una versión largometraje de "THX-1138: 4EB" con el título apenas modificado: el clásico de culto "THX 1138".

En 1971, Coppola comenzó a dedicarse a la producción de la película "El padrino", que se estrenó en 1972, mientras Lucas decidía crear su propia compañía, Lucasfilm Ltd.

Durante la producción de THX 1138 (1971), el productor Francis Ford Coppola sugirió a George Lucas a escribir un guion que fuera atractivo para el público en general. Lucas abrazó la idea, utilizando sus experiencias adolescentes de principios de la década de 1960 crucero en Modesto, California. "El cruising desapareció y me sentí obligado a documentar toda la experiencia y lo que mi generación usó como una forma de conocer chicas", explicó Lucas. A medida que desarrollaba la historia en su mente, Lucas incluyó su fascinación con Wolfman Jack.

Añadiendo connotaciones semiautobiográficas, Lucas estableció la historia en su ciudad natal de Modesto de 1962. Los personajes Curt Henderson, John Milner y Terry "The Toad" Fields también representan diferentes etapas de su vida más joven. Curt sigue el modelo de la personalidad de Lucas durante la USC, mientras que Milner se basa en la adolescencia street-racing y los años universitarios junior de Lucas, y hot rod entusiastas que había conocido de Kustom Kulture en Modesto Toad representa los años de Lucas nerd como estudiante de primer año en la escuela secundaria, específicamente su "mala suerte" con las citas. El cineasta también se inspiró en I Vitelloni (1953) de Federico Fellini. La película ganó el Globo de Oro, el New York Film Critics y el premio de la National Society of Film Critics. Obtuvo también cinco nominaciones al Óscar, incluyendo el .

Durante los años 1973 y 1974 se dedicó exclusivamente a escribir el guion de su siguiente película, una fantasía espacial llamada "The Star Wars". Para escribir este guion se inspiró en el cómic "Flash Gordon" y la película "El planeta de los simios". Luego, en 1975 fundó Industrial Light & Magic (ILM) para producir los efectos especiales que necesitaba para la película. Otra compañía llamada Sprocket Systems se dedicaría a editar y mezclar el sonido de "Star Wars". Más tarde, esta compañía sería rebautizada como Skywalker Sound. No obstante, su proyecto de "Star Wars" era rechazado una y otra vez por varios estudios, hasta que tuvo una reunión con los ejecutivos de Twentieth Century Fox, quienes decidieron darle una oportunidad. Lucas llegó a un acuerdo con estos directivos para ceder su salario como director de la película a cambio de recibir el 40% de las ganancias de taquilla y todos los derechos del merchandising. Lo que parecía un buen negocio para Fox no fue así, ya que "Star Wars" se convirtió en 1977 en un tremendo éxito de taquilla, causando un fenómeno recordado hasta el presente y recibiendo incluso siete nominaciones de la Academia, algo casi impensable hasta ese momento para una película de ciencia ficción. Estuvo tan estresado durante el rodaje de "Star Wars" que tuvo que ir a un hospital, donde le diagnosticaron hipertensión y agotamiento, exacerbados por su diabetes.

Lucas inició la historia de "", episodio V, y "", episodio VI, de las cuales fue productor ejecutivo dejando las tareas de dirección a Irvin Kershner en "" y a Richard Marquand en "". Cuando comenzó la preproducción de "El retorno del Jedi", se le dio el nombre falso de "Blue Harvest" ("Cosecha Azul") para despistar a los fanáticos y a la industria en general. Le pidió a Spielberg que dirigiera "El retorno del Jedi", pero una disputa con el sindicato de directores se lo impidió. También se ofreció a David Lynch la dirección de la cinta. A pesar de su reputación de éxitos de Hollywood, todos los filmes de "Star Wars" son películas independientes, a excepción de "Star Wars: Una nueva esperanza". La única manera en que podría conseguir el financiamiento requerido para hacer la película era solicitarla al estudio. Con el éxito de la película y de su comercialización, Lucas ya no necesitó al estudio. Para los episodios V y VI, pidió préstamos bancarios, que pagó con las ganancias de cada película. Para las «precuelas», no necesitó ningún préstamo, teniendo bastante dinero para financiar cada película con sus propios ahorros personales.

En 1980 escribió y produjo "Raiders of the Lost Ark", que fue dirigida por Spielberg y obtuvo cinco premios de la academia. Logró lo que para ese entonces era un trato inusual para la película "Raiders of the Lost Ark": Paramount financió el presupuesto entero de 20 millones de dólares, a cambio, Lucas poseería más del 40% de la película y recogería casi la mitad de los beneficios después de que el estudio ganara cierta cantidad de dinero. Esto resultó ser un reparto muy lucrativo para Lucas. El ejecutivo de Paramount Michael Eisner dijo que él sentía que el guion para la película era el mejor que había leído. También fue coproductor y creador de la historia de "Indiana Jones y el templo maldito", estrenada en 1984, nominada a dos premios de la Academia y que obtuvo un Óscar por los efectos especiales.

Desde 1980 hasta 1985 Lucas invirtió su tiempo en la construcción del Skywalker Ranch, para reunir allí los recursos creativos, técnicos y administrativos de la ya importante Lucasfilm. Luego, George Lucas revolucionaría los cines con su sistema THX, que fue desarrollado para mantener los más altos estándares de calidad para la proyección de una película. También se convirtió en el presidente de The George Lucas Educational Foundation. 

Adoptó tres niños: Amanda Lucas (1981), Katie Lucas (1988) y Jett Lucas (1993). 

Se negó a prestarle un millón de dólares a Francis Ford Coppola para la compra de los Hollywood General Studios. Esto propició su separación hasta que finalmente Lucas le pidió perdón y se reconciliaron.

En 1992, Lucas recibió el premio Irving G. Thalberg por el Consejo Superior de la Academia de Artes y Ciencias del Cine por sus logros durante el curso de su vida. Lucasfilm Games, después renombrada LucasArts, es altamente reconocida dentro de la industria de los videojuegos. Vendió la división de gráficos por computadora de Lucasfilm a Steve Jobs de Apple Computers. Ahora se la conoce como Pixar.

El último proyecto de Lucasfilm fue terminar la nueva trilogía de "La guerra de las galaxias". "", episodio I de la nueva trilogía, fue escrita, dirigida y producida por George Lucas y salió al público el 19 de mayo de 1999. En mayo de 2002 se estrenó la segunda parte, "", episodio II, también dirigida y producida por él. El capítulo final de la nueva trilogía de "La guerra de las galaxias", "La venganza de los Sith", episodio III, que abre las puertas a la trilogía clásica, se estrenó el 19 de mayo de 2005, quedando la saga finalizada.

En 2007 se anunció que iba a producir una serie de televisión sobre "Star Wars" que tendría lugar entre los episodios y ; "", es un espectáculo de animación "CG" que debutó en el otoño de 2008. Antes de hacer la trilogía de precuelas de Star Wars, iba a realizar la cuarta parte de Indiana Jones en 1995, sin embargo no se pudo hacer hasta que Lucas, Ford y Spielberg fueron convencidos para comenzar el rodaje. La película se estrenó con el título "Indiana Jones y el reino de la calavera de cristal" en 2008 y tuvo buena aceptación por el público y la crítica especializada.

En enero de 2012, Lucas anunció su retiro de la producción a gran escala de películas taquilleras y que enfocara su carrera en las características más pequeñas, con un presupuesto independiente. En junio se anunció que la productora Kathleen Kennedy, una colaboradora a largo plazo con Spielberg y el productor de las películas de Indiana Jones, había sido nombrada co-presidenta de Lucasfilm. Se informó que Kennedy iba a trabajar al lado de Lucas, que se mantendría como el director ejecutivo y ella serviría como co-presidenta durante al menos un año, después de lo cual ella le sucedería como líder en solitario de la compañía.
El 30 de octubre de 2012, Lucas vendió Lucasfilm por 4050 millones de dólares a Disney y confirmó que habrá una tercera trilogía de Star Wars que comenzará en 2015 y estará formada por los episodios , y 

Produjo la película Red Tails que se trata sobre un grupo de pilotos negros distinguidos en la Segunda Guerra Mundial llamados Aviadores de Tuskegee que se estrenó en 2012 y escribió el musical animado en 3D Strange Magic, inspirado por El sueño de una noche de verano. George Lucas visitó el set de rodaje de y , las cuales dio su aprobación y disfrutó viendo las dos películas mencionadas. No se sabe qué está haciendo ahora pero se anunció que escribirá y producirá la quinta película de Indiana Jones, lo cual emocionó a los fanáticos de la saga ya que antes estaban desconcertados si él no volverá a la producción de la película. Muchos fanáticos esperan a que Lucas vuelva a la dirección de películas, pero había un rumor que fue confirmado por el ministro británico Boris Johnson que dirigiría un spin-off de Star Wars sobre Obi-Wan Kenobi, también es posible que dirija películas.

Lucas se ha comprometido a donar la mitad de su fortuna a organizaciones benéficas como parte de un esfuerzo llamado The Giving Pledge, liderado por Bill Gates y Warren Buffett para convencer a las personas más ricas de Estados Unidos a donar parte de su riqueza a obras benéficas.

Lucas hizo una aparición en la película de "Star Wars", "Star Wars: Episodio III - La venganza de los Sith", interpretando a un personaje extra llamado Baron Papanoida. También hizo un cameo en la película "Indiana Jones y el templo maldito" junto con Steven Spielberg, Frank Marshall y Dan Aykroyd.



</doc>
<doc id="4755" url="https://es.wikipedia.org/wiki?curid=4755" title="Historieta">
Historieta

Una historieta o cómic es una sucesión de dibujos que constituye un relato, con texto o sin texto, así como la serie de ellas que trate de la misma historia o del mismo concepto, y también el correspondiente medio de comunicación en su conjunto.

Partiendo de la concepción de Will Eisner de esta narrativa gráfica como un arte secuencial, Scott McCloud llega a la siguiente definición: «Ilustraciones yuxtapuestas y otras imágenes en secuencia deliberada con el propósito de transmitir información u obtener una respuesta estética del lector». Sin embargo, no todos los teóricos están de acuerdo con esta definición, la más popular en la actualidad, dado que permite la inclusión de la fotonovela y, en cambio, ignora el denominado humor gráfico.

El interés por la historieta «puede tener muy variadas motivaciones, desde el interés estético al sociológico, de la nostalgia al oportunismo». Durante buena parte de su historia fue considerado incluso un subproducto cultural, apenas digno de otro análisis que no fuera el sociológico, hasta que en la década de 1960 se asiste a su reivindicación artística, de tal forma que Morris y luego Francis Lacassin han propuesto considerarlo como el "noveno arte", aunque en realidad sea anterior a aquellas disciplinas a las que habitualmente se les atribuyen las condiciones de "octavo" (fotografía, de 1825) y "séptimo" (cine, de 1886). Seguramente, sean este último medio y la literatura los que más la hayan influido, pero no hay que olvidar tampoco que «su particular estética ha salido de las viñetas para alcanzar a la publicidad, el diseño, la moda y, no digamos, el cine».

Las historietas suelen realizarse sobre papel o en forma digital (e-comic, webcómics y similares), pudiendo constituir una simple tira en la prensa, una página completa, una revista o un libro (álbum, novela gráfica o tankōbon). Han sido cultivadas en casi todos los países y abordan multitud de géneros. Al profesional o aficionado que las guioniza, dibuja, rotula o colorea se le conoce como "historietista".

En los países hispanoparlantes se usan varios términos autóctonos, como "monos" y su variante "monitos" (antes muy usado en México), y, sobre todo, "historieta", que procede de Hispanoamérica y es el más extendido. Algunos países hispanohablantes mantienen, además, sus propias denominaciones locales: "muñequitos" en Cuba, y "tebeo" en España. En Venezuela también se les llama "comiquitas" por extensión. En el Perú se le denomina "chiste". Hacia la década de 1970 comenzó a imponerse en el mundo hispanoparlante el término de origen anglosajón "comic" (escrito hoy en día en lengua española bajo la forma «cómic» pero procedente, a través del inglés, del griego Κωμικός, "kōmikos", ‘de o perteneciente a la comedia’) que se debe a la supuesta comicidad de las primeras historietas. En inglés, se usaban además los términos "funnies" (es decir, divertidos) y "cartoon" (por el tipo de papel basto o cartón en donde se hacían), pero con el tiempo los "animated cartoons" o dibujos animados tendieron a reservarse la palabra ""cartoon"". Posteriormente aparece desde el movimiento contracultural el término "comix", primero en inglés y luego en otras lenguas, que suele reservarse para publicaciones de este estilo.

Se puede denominar a las historietas o cómic gráficos como narraciones que cuentan diferentes historias mediante una sucesión de imágenes o ilustraciones, las cuales son perfectamente complementadas con textos escritos, aunque también existen historietas mudas, es decir, carentes de texto.

Obviamente, las historietas no tienen por qué ser cómicas y por ello los franceses usan desde los años 1960 el término "bande dessinée" ('tiras dibujadas'), abreviado BD, que en realidad es una adaptación de "comic strip". El portugués tradujo del francés para crear "banda desenhada", mientras que en Brasil se la denomina "história em quadrinhos" (historia en cuadritos), haciendo así referencia al procedimiento sintáctico de la historieta, como también sucede con el término chino "liánhuánhuà" ('imágenes encadenadas').

En relación a nombres asiáticos, el término "manga" (漫画, 'dibujo informal') se ha impuesto en japonés a partir de Osamu Tezuka quien lo tomó a su vez de Hokusai, mientras que se reserva el término "komikkusu" (コミックス) para la historieta estadounidense. Los filipinos usan el similar komiks, pero lo aplican en general, mientras que en Corea y China usan términos derivados de manga, como "manhwa" y "manhua", respectivamente.

Finalmente, en Italia la historieta se denominó "fumetti" ("nubecillas", en castellano) en referencia al globo de diálogo.

Diversas manifestaciones de la Antigüedad y la Edad Media pueden ajustarse a la definición de cómic dada más arriba: Pinturas murales egipcias o griegas, relieves romanos, vitrales de iglesias, manuscritos iluminados con viñetas cuadradas que contienen dibujos en secuencia narrativa, como las "Cantigas de Santa María" de Alfonso X el Sabio, códices precolombinos, Biblia pauperum, etc. Con la invención de la imprenta (1446) se producen ya aleluyas y con la de la litografía (1789), se inicia la reproducción masiva de dibujos (las imágenes de Épinal, entre ellas).

En la primera mitad del siglo XIX, destacan pioneros como Rodolphe Töpffer, pero será en la prensa como primer medio de comunicación de masas, donde más evolucione la Historieta, primero en Europa y luego en Estados Unidos. Es en este país donde se implanta definitivamente el globo de diálogo, gracias a series mayoritariamente cómicas y de grafismo caricaturesco como "The Katzenjammer Kids" (1897), "Krazy Kat" (1911) o "Bringing up father" (1913). A partir de 1929, empiezan a triunfar las tiras de aventuras de grafismo realista, como "Flash Gordon" (1934) o "Príncipe Valiente" (1937). Estas invadirán Europa a partir de 1934 con "Le Journal de Mickey", aunque con resistencias como "Tintín" (1929) y "Le Journal de Spirou" (1938), y movimientos originales como el de la novela en imágenes. A partir de este año, sin embargo, las tiras de prensa estadounidenses empezarían a acusar la competencia de los comic-books protagonizados por superhéroes. 
Durante la postguerra, las escuelas argentina, franco-belga y japonesa adquieren un gran desarrollo, gracias a figuras como Oesterheld, Franquin y Tezuka, respectivamente. En general puede decirse que ""el grueso de la producción norteamericana, para la segunda mitad de los años sesenta, ha bajado de nivel y se halla por debajo de la producción francesa o italiana"". Será en ambos países donde se afiance una nueva conciencia del medio, orientándose los nuevos autores (Crepax, Moebius, etc.) hacia un público cada vez menos juvenil. Con ello, y con la competencia de nuevos medios de entretenimiento como la Televisión, el cómic va dejando de ser un medio masivo, salvo en Japón. Precisamente, su historieta conquistará el resto del mundo a partir de 1988, gracias al éxito de sus versiones en dibujos animados. Del mismo modo, las experiencias del cómic underground de la década de 1960 cristalizan en un sólido movimiento alternativo, ya en los años 1980, que da lugar a su vez al movimiento de la novela gráfica. Internet también constituye un nuevo factor a tener en cuenta.

El cómic puede ser un buen recurso educativo, dado el carácter tan dinámico que conlleva. El hecho de poder reproducir historias y crearlas aporta una motivación al proceso enseñanza-aprendizaje. Son muchos los rasgos lingüísticos que se pueden trabajar; desde diálogos, monólogos, expresiones más coloquiales, entre otros. Supone una mezcla de lenguaje visual e icónico que reconfigura nuestras capacidades comunicativas.Pero, para que sirva como verdadero recurso educativo, debe cumplir los siguientes objetivos (GuzmánLópez,2011):

Del relato expuesto más arriba, puede deducirse la existencia de 3 grandes tradiciones historietísticas a nivel global, todas con sus propios sistemas de producción y distribución:
De menor trascendencia global, podemos citar otras escuelas, como la:

Aparte de la producción argentina y española, puede destacarse la de otros países hispanos, como Chile, Cuba o México y, en menor medida, Colombia o Perú. Ya a finales de los años 1960, Oscar Masotta afirmaba que a través del cómic se estaba produciendo un verdadero intercambio de culturas o universalización cultural, de tal manera que «los italianos y los alemanes leen historietas producidas en Francia y viceversa, los pueblos de habla hispánica leen tiras producidas en países anglosajones, en los Estados Unidos en su mayor parte, etc.», contribuyendo así a borrar las particularidades nacionales. Sin embargo, este teórico no dejaba de mencionar, como un valor negativo, que En este mismo sentido, se extiende el libro "Para leer al Pato Donald" (1972) de Ariel Dorfman y Armand Mattelart.

Tradicionalmente, la industria del cómic ha requerido un trabajo colectivo, en el que, además de los propios historietistas, han participado editores, coloristas, grabadores, impresores, transportistas y vendedores. Siempre han existido autoediciones, como las del cómic underground, pero últimamente han aumentado por la crisis de determinados mercados y las facilidades logradas con el auge de la informática e Internet. Pueden distinguirse los siguientes formatos de publicación:

El canal de comercialización más habitual de la mayoría de estos cómics ha sido el quiosco hasta que, con el desarrollo del mercado de venta directa a principios de los años 1970, se empezó a imponer la librería especializada. Tanto los propios cómics como sus originales son objeto de un activo coleccionismo.

Con un objetivo comercial, pero también lúdico y didáctico, abundan los eventos de historieta (convenciones, festivales, jornadas, etc.) como un punto de encuentro entre profesionales y aficionados. Los festivales más importantes son el Salón Internacional del Cómic de Barcelona (España,1981), Comiket de Tokio (Japón, 1975), la Convención Internacional de Cómics de San Diego (Estados Unidos, 1970), el Festival Internacional de la Historieta de Angulema (Francia, 1974) y Comics & Games en Lucca (Italia, 1966).

Un género narrativo es un modelo o tradición de estructuración formal y temática que se ofrece al autor como esquema previo a la creación de historietas, además de servir para la clasificación, distribución y venta de las mismas. Todo género se clasifica según los elementos comunes de los cómics que abarca, originalmente según sus aspectos formales (grafismo, estilo o tono y, sobre todo, el sentimiento que busquen provocar en el lector), y temáticos (ambientación, situaciones, personajes característicos, etc), de tal forma que «las características de guion, planificación, iluminación y tratamiento» de una historieta variarán según el género al que pertenezca. Alternativamente, los géneros historietísticos se definen por el formato de publicación. Como explica Daniele Barbieri, «la división por géneros es distinta e independiente de la división por lenguajes», de tal forma que 
Actualmente no existe un consenso en cuánto a su número, pues las diversas clasificaciones no derivan tanto de la retórica clásica, con su división en lírico, épico y dramático, como de la novela popular y el cine, que se caracterizan por la escasa complejidad de su regulación. No es raro encontrar, por ejemplo, referencias a macrogéneros como "historieta de aventuras" o "de acción". Para complicar aún más el tema, los géneros también pueden ser combinados para formar géneros híbridos. Hay, sin embargo, algunos bastante definidos y con mucha tradición, como los que se distinguen en las monografías "Gente del cómic" y "Mangavisión":


Hasta la reciente evolución de la imagen generada por computadora, podía decirse que la proliferación de ciertos géneros, como la ciencia ficción o el fantástico, era debida a ""la facilidad y economía de medios con que un buen dibujante puede introducir a sus lectores en los ambientes más fantásticos"".

También se distingue, aunque ya fuera de cualquier clasificación por géneros, una historieta infantil, dirigida a niños, de otro cómic para adultos, mientras que apenas tiene predicamento el término historieta familiar, que si tiene equivalentes en el cine, para referirse a las obras que resultan atractivas a lectores de todas las edades. La historieta infantil ha constituido la mayoría del material clásico «de todos los países (Estados Unidos incluido)», mientras que el cómic adulto inició su auge en los años 1960 presentando relatos que podían ser tan imposibles y pueriles como los anteriores, pero que incluían mayores dosis de violencia, temas inquietantes, palabras malsonantes y sobre todo sexo explícito.

Antiguamente había en Occidente "reductos específicamente femeninos", ya sea en forma de revistas infantiles para niñas, o de melodramas románticos.

En Japón, donde hay cómics específicos para todo tipo de público, se distinguen también por el grupo de edad y sexo al que van dirigidos: "kodomo" (niño), "shōjo" (muchacha), "shōnen" (muchacho), "josei" (mujer) y "seinen" (hombre).

Por otro lado, hay que señalar que el cómic ha sido despreciado con frecuencia por élites culturales y representantes políticos. Esto se explica por el «viejo prejuicio que identifica la palabra escrita con lo culto y la imagen que la explica -o, como en este caso, la enriquece y transforma- con lo iletrado». Mauro Entrialgo es de la opinión de que:

El catedrático Juan Antonio Ramírez considera que este reconocimiento en el seno de la alta cultura se ha visto imposibilitado, paradójicamente, «por la consolidación y extensión del sistema del arte» y unos departamentos de literatura separados por ámbitos lingüísticos.

También hay que destacar que muchos cómics clásicos «ofrecen sólo una de las caras de sus personajes y ocultan todas las demás», quedando por lo tanto en la pura anécdota. El medio en su conjunto ha tendido a «traducir una ideología tradicional, conservadora e inmovilista durante muchos años», ya sea por las convicciones de sus autores o para no desagradar al conjunto de sus lectores y arriesgarse a perderlos o incluso sufrir los efectos de la censura, como ocurrió en regímenes como los de Mussolini o Franco y, respecto a las historieta de horror y crímenes en los Estados Unidos y la Gran Bretaña de los años 1950. Esto explica que las historietas que no se ceñían a los valores sociales imperantes se manifestasen a través de publicaciones underground y que temáticas como la homosexualidad no aflorasen a la superficie hasta los años 1980, conforme iba siendo aceptada en la cultura oficial, ni produjesen hasta entonces sus primeros autores reconocidos por crítica y público, como Ralf König o Nazario.

Del mismo modo, «sólo en las últimas décadas se han empezado a producir buenas historietas protagonizadas por mujeres», ya sea ejerciendo el papel del héroe tradicional o mostrando una psicología propia. Hay que destacar, a este respecto, la abundancia en los últimos años de memorias realizadas por mujeres, como Zeina Abirached o Marjane Satrapi.

Actualmente, las historietas son leídas mayormente por adolescentes y adultos jóvenes, por lo que cada vez las hay «más complicadas, más abiertas, más sensibles y más liberadas», es decir, más adultas.

Para Oscar Masotta, lo que determina en primer lugar el valor de una historieta es el grado en que permite manifestar e indagar las propiedades y características del lenguaje mismo de la historieta, revelar a la historieta como lenguaje. Jean Giraud afirma que

Históricamente, los personajes tipo han sido muy importantes para el medio, ya que el lector ""desea, quiere y espera que el "bueno" ponga cara de bueno, y el "malo" tenga cara de malo"".

En la historieta se figura, "con medios estáticos, el movimiento real", usando técnicas que ya practicaron los futuristas.

El texto no es necesario, pero suele estar presente, ya sea en forma de globos o bocadillos, cartelas, textos sueltos y onomatopeyas. Las palabras dichas por los personajes suelen recogerse en los globos, salvo que se presenten fuera para indicar que han subido el tono de voz.

Todos los textos suelen estar escritos en mayúsculas y las diferencias tipográficas, de tamaño y grosor sirven para destacar una palabra o frase, y maritza intensidades de voz. Masotta establece a este respecto un esquema con 7 oposiciones:

Toda historieta es una narración gráfica, es decir, desarrollada mediante una secuencia de dibujos, y no una serie de ilustraciones cuyo mérito radique en ellas mismas, de tal forma que «cada cuadro o viñeta debe estar relacionado de algún modo con el siguiente y con el anterior». En afortunada expresión de Román Gubern, la «viñeta es la representación gráfica del mínimo espacio y/o tiempo significativo. Al espacio que separa las viñetas se le conoce como "calle"» y al proceso por el que el lector suple ese vacío se le denomina "clausura". McCloud distingue 5 tipos de transiciones entre viñetas: 
Cuanto mayor sea el formato y el número de signos icónicos y verbales, más tiempo y atención deberemos prestar a una determinada viñeta.
La historieta usa variaciones del ángulo visual, encuadre y planos, términos estos que ha tomado del cine, para dinamizar la narración.

Debido a su condición de medio intersticial desde sus orígenes, la historieta se relaciona en primer lugar con las artes plásticas.

Dada la antigüedad y el prestigio de la literatura, «cualquier relación cercana entre uno y ojos, porque, se supone, da “categoría” a la niña». En la historieta, sin embargo, «los textos no viven una vida propia en su interior» como sí ocurre en la literatura.

En segundo lugar, ha de mencionarse también sus relaciones con:

El cine y el cómic comparten una larga historia de influencias mutuas. En este sentido, Federico Fellini manifestaba que «los cómics que se realizan acercándose demasiado a la técnica cinematográfica son para mí los menos hermosos, los menos logrados» de tal manera que los «que merecen consideración son aquellos que han inspirado al cine y no al revés». Citaba así a clásicos de la historieta de humor estadounidense, como los "Katzenjamer Kids" y "Bringing up father", que considera indudable inspiración de ciertos escenarios y personajes de Chaplin.

Respecto a otras disciplinas más modernas, Ana Merino era de la opinión que 

En un apartado anterior de la sección de historia, ya se ha mencionado la revolución que en los años 1930, supuso la imposición de un nuevo tipo de grafismo realista para los cómics "serios" en detrimento del grafismo distorsionado y caricaturesco que había predominado hasta entonces. A su vez, desde mediados de los años 1960, muchos autores han tendido a «la destrucción del realismo naturalista para encontrar nuevos caminos: el realismo fantástico, la deformación y la angulación, el montaje de mayor expresividad», etc.

En realidad, los estilos gráficos usados por los historietistas son tan variados como la intención y la habilidad del autor, distribuyéndose estos dentro un triángulo formado por tres vértices (abstracción, realidad y lenguaje) que comprende desde el realismo de filiación fotográfica (Luis García, Alex Ross, etc), a la caricatura.

En una misma viñeta pueden combinarse además varios estilos. McCloud denomina "efecto máscara" a la combinación de unos personajes caricaturescos con un entorno realista que podemos observar en la "línea clara" o el manga clásico de Osamu Tezuka.

A pesar de tamañas posibilidades, los dibujantes clásicos procuraban mantener siempre un mismo estilo a lo largo de toda su carrera, debido quizás a imposiciones de sus syndicates. Un autor más moderno, como el español Josep María Beá, a pesar de estimar grandemente a los que le precedieron, considera que el «estilo, cuando se perpetúa indefinidamente y no evoluciona, es signo de fosilización, de amaneramiento».




Otros comics





</doc>
<doc id="4756" url="https://es.wikipedia.org/wiki?curid=4756" title="30 de enero">
30 de enero

El 30 de enero es el 30.º (trigésimo) día del año del calendario gregoriano. Quedan 335 días para finalizar el año y 336 en los años bisiestos.












</doc>
<doc id="4757" url="https://es.wikipedia.org/wiki?curid=4757" title="31 de enero">
31 de enero

El 31 de enero es el 31.º (trigésimo primer) día del año en el calendario gregoriano. Quedan 334 días para finalizar el año y 335 en los años bisiestos.













</doc>
<doc id="4758" url="https://es.wikipedia.org/wiki?curid=4758" title="Asociación para el Desarrollo de la Informática y la Telemática">
Asociación para el Desarrollo de la Informática y la Telemática

El Desarrollo de la Informática o ADITEL es una organización sin ánimo de lucro dedicada a fomentar el uso e investigación de la informática y las telecomunicaciones, apoyando activamente el software libre. Creada en el año 1998, el ámbito de referencia de la asociación es la provincia de Castellón y mantiene contactos con la Universitat Jaume I y el tejido empresarial de la zona.

La asociación apoya algunos portales para usuarios castellanohablantes de software libre como el sitio de usuarios de la distribución Ubuntu o el del sistema de mensajería instantánea Jabber/XMPP.




</doc>
<doc id="4759" url="https://es.wikipedia.org/wiki?curid=4759" title="1 de febrero">
1 de febrero

El 1 de febrero es el 32.º (trigésimo segundo) día del año en el calendario gregoriano. Quedan 333 días para finalizar el año y 334 en los años bisiestos.





























</doc>
<doc id="4764" url="https://es.wikipedia.org/wiki?curid=4764" title="Colima">
Colima

Colima , oficialmente llamado Estado Libre y Soberano de Colima, es uno de los treinta y un estados que, junto con la Ciudad de México, forman México. Su capital y ciudad más poblada es Colima.

Está ubicado en la región oeste del país, limitando al norte con Jalisco, al sur con Michoacán y al oeste con el océano Pacífico. Con 711,235 habitantes en 2015, es el estado menos poblado, con 5,627 km², el cuarto menos extenso —por delante de Aguascalientes, Morelos y Tlaxcala, el menos extenso— y con 115.65 hab / km², el noveno más densamente poblado, por detrás del Estado de México, Morelos, Tlaxcala, Aguascalientes, Guanajuato, Puebla, Querétaro e Hidalgo. Fue fundado el 9 de diciembre de 1856.

Políticamente se divide en 10 municipios. Su capital recibe el mismo nombre: Colima. Otras localidades importantes son Manzanillo, Tecomán, Armería, Comala, Villa de Álvarez, Cuauhtémoc, Ixtlahuacán, Coquimatlán y Minatitlán.

Fundada en 1523 originalmente como Villa de San Sebastián, el nombre de Colima viene del náhuatl Acolman, que significa "lugar donde tuerce el agua" o "lugar donde hace recodo el río". El territorio de Colima, del que casi tres cuartas partes de superficie están cubiertas por montañas y colinas, queda comprendido dentro de una derivación de la Sierra Madre del Sur, que se compone de cuatro sistemas montañosos.

A pesar de ser una pequeña entidad, Colima posee monumentos históricos como su catedral basílica menor, construcción que se empezó en 1525 de estilo predominántemente neoclásico aunque también muestra algunos rasgos arquitectónicos de estilos barroco y gótico; el Palacio de Gobierno, con los magníficos murales del pintor colimense Jorge Chávez Carrillo, que ilustran temas históricos relativos a la Conquista, la Colonización y la Guerra de Independencia. Otros lugares culturales y arquitectónicos que destacan son: El Teatro Hidalgo, que data del siglo XIX; el Templo de San Francisco del Pilón, fundado en 1554; la Casa de la Cultura, con una increíble biblioteca, sala de exposiciones, auditorio y talleres de diversas actividades artísticas.

El estado costero de Colima continúa sus límites océano adentro, hasta las islas Revillagigedo; reservas ecológicas, pobladas únicamente por aves marinas y por los miembros de la armada que resguardan esta frontera del país. Las zonas conurbadas de la ciudad de Colima y de la ciudad de Villa de Álvarez, han engullido sitios que hasta ya muy avanzado el siglo XX se encontraban en las afueras de esa mancha urbana, como San Francisco de Almoloyan o las zonas arqueológicas de La Campana y El Chanal.

Durante la era prehispánica, la región que hoy ocupa el estado de Colima fue el punto de asiento de varios grupos étnicos que florecieron en el Occidente Mexicano. La región estuvo habitada por varios señoríos que se disputaban el territorio antes de la llegada de los conquistadores de Europa que ahora denominamos españoles. A principios del siglo XVI, los purépecha alcanzaron a dominar hasta las salitreras de Tzacoalco, propiedad de los tecos, a causa de esto el rey Colimán o Tlatoani Colimotl les hacía la guerra y en muchas ocasiones les derrotó, tras la Guerra del Salitre con la que los Tecos tomaron Sayula, Zapotlán y Amula e incluso alcanzaron a llevar su dominio hasta Mazamitla, logrando que el señorío de Colima se convirtiera en el grupo predominante.

Después de que los europeos llegaron a un acuerdo con los purépechas, un emisario de Hernán Cortés, de nombre Francisco Montaño, de los primeros que se había aventurado hasta la capital del imperio PurépechaTzintzuntzan, recogió y dio a conocer a sus superiores información que decía que al poniente del Señorío del Caltzontzin se encontraba un lugar dominado por el rey Colimotl. Hernán Cortés, pensó en conquistar Colima, pero Juan Rodríguez de Villafuerte precipitó sus planes al desobedecer sus órdenes y ser el primero en explorar la zona, a su llegada a Trojes es derrotado en una emboscada del Rey Colimán. Poco después Cortés decidió encargarle la empresa a Francisco Álvarez Chico. Este salió con un pequeño ejército por el camino de Toluca, hacia la costa del señorío Purépecha, hoy Michoacán.

Al cruzar con los suyos por un pequeño desfiladero, los guerreros del Rey Colimotl atacaron por sorpresa, Francisco Álvarez Chico junto con otros soldados fueron derrotados en el Paso de Alima y/o Palenque de Tecomán. Después de esta derrota se organizó una nueva expedición, llamada punitiva, que le fue confiada a Cristóbal de Olid que en 1522 había sido enviado a terminar de conquistar el Señorío Purépecha en nombre de Hernán Cortés, pero fracasa en llegar a la tierra del Rey Colimotl. Cortés confía la siguiente expedición a Gonzalo de Sandoval, que finalmente derrota en el Palenque de Tecomán a los tecos o colimecas, donde se rindió el rey Colimán.

Tras consumarse la conquista de México, el mismo Gonzalo de Sandoval fundó en Caxitlán la "Villa de Colima" en términos de Tecomán, el 25 de julio de 1523, pero debido a problemas de insalubridad tuvo que trasladar el asentamiento al lugar que hoy ocupa, denominándola Villa de San Sebastián. En 1533 Hernando de Grijalva zarpó desde las costas de la región para descubrir el Archipiélago de Revillagigedo. 

La Orden de la Merced, también conocida como Los Mercedarios, funda en 1607 un convento en toda forma en la entonces Villa de San Sebastián, pero lamentablemente no sobreviven las ruinas que atestiguaban tan importante institución educativa de Colima. Durante la época Virreinal el Puerto de Tzalahua (Manzanillo) se destacó como un importante sitio de defensa y comercio para la Nueva España. Durante el movimiento de independencia, la capital fue tomada por los insurgentes a finales de 1810 sin encontrar resistencia realista y fue recuperada por el ejército virreinal en 1811. Finalmente en 1857 Colima alcanzó la categoría de estado.

El 5 de noviembre de 1864 es la fecha que quedó inscrita en la historia de Colima, al quedar incorporada a la lista de ciudades durante la Segunda Intervención Francesa en México. 

Exiliado Benito Juárez en los lejanos territorios del norte del país, siendo el coronel Julio García gobernador del Estado, hizo su entrada a la ciudad la "Brigada Doway" con la que se otorgó protección y abrigo administrativo quedando el estado de Colima incorporado política y administrativamente al gobierno que encabezaba Maximiliano I.

El Estado de Colima se encuentra dividido políticamente en 10 municipios. Los municipios cuentan con un gobierno propio que radica en el Ayuntamiento, el cual es dirigido por un Presidente Municipal elegido por elección popular.

Los municipios del Estado de Colima:

Colima cuenta con un gran volcán, el Volcán de Fuego, situado en el límite entre los estados mexicanos de Colima y Jalisco pero mostrándose mayor visibilidad hacía Colima. Forma parte, junto con el Nevado de Colima (que se alza al norte del volcán), del área natural protegida que lleva el nombre de este último. Pese a su persistente actividad, las labores agropecuarias se han seguido desarrollando en la región colindante.

Los relieves montañosos cubren el oeste, el norte y la parte este de la entidad. Las penetraciones de las sierras jaliscienses forman las zonas más elevadas: Cerro Gordo, sierras de Perote, El Peón y las estribaciones del volcán de Colima. La serranía de Piscila limita por el sur el amplio valle de Colima, al sur, las llanuras de Tecomán terminan en un litoral bajo y arenoso. La entidad cuenta con dos extensas bahías: Manzanillo y la de Santiago, así como el archipiélago de Revillagigedo. 

Los principales ríos de Colima nacen en Jalisco. El Armería y sus afluentes, el Comala y el Colima, riegan su parte central; el Cihuatlán o Marabasco la del oeste y el Coahuayana, con su afluente el Salado, riega la parte oriental. En la zona costera se localizan las lagunas de Potrero Grande, de Miramar, de San Pedrito, de Alcuzahue, de Amela y la de Cuyutlán, rica en depósitos de sal. Cuenta con un clima cálido subhúmedo, exceptuando las sierras, en donde es semicálido subhúmedo y las llanuras de Tecomán, en donde es cálido semiseco.
El clima de Colima puede ser muy variado, aunque predomina la humedad. En el norte del Estado el clima es cálido subhúmedo, mientras que en las sierras se presenta un clima semicálido subhúmedo y las llanuras de Tecomán tienen un clima cálido semiseco. En la zona costera y en la cuenca del río Armería el clima es cálido y húmedo. La temperatura promedio anual en el Estado es de 28 °C y oscila entre una máxima de 38 °C y una mínima de 12 °C. Uno de los mayores atractivos de Colima es la benignidad de su clima. La pequeña geografía del Estado permite disfrutar, en un recorrido de hora y media, el clima templado de la montaña o el calor tropical de sus playas para una elección de climas. La temperatura media anual oscila alrededor de los 25 °C, con la máxima de 38 °C y la mínima de 7 °C. La precipitación pluvial anual media es de 983 milímetros. El clima de Colima se ve influenciado en gran manera por su relieve montañoso, el cual cubre el oeste, el norte y la parte este de la entidad. Las penetraciones de las sierras jaliscienses forman las zonas más elevadas: Cerro Gordo, sierras de Perote, El Peón y las estribaciones del volcán de Colima. La serranía de Picila limita por el sur el amplio valle de Colima, al sur, las llanuras de Tecomán terminan en un litoral bajo y arenoso. Estas sierras, por su latitud y exposición, permiten que las precipitaciones sean mayores y que el clima sea diferente en relación con las partes bajas del estado. En la zona costera y en la cuenca del río Armería el clima es cálido y húmedo, mientras que en la parte alta es templado y cálido en la zona sur. Su temperatura promedio anual es de 28 °C y oscila entre una máxima de 38 °C y una mínima de 12 C°

Existen diversas especies de flora, según la región de que se trate: en las partes altas predominan el pino, roble, encino, arrayán; en los valles hay especies forrajeras y frutales como mango, papaya, tamarindo, palma de coco; mientras que la costa cuenta con guamúchil, guayacán, mezquite, chicalite, crucillo y mangle. De este último podemos encontrar dos clases: el mangle blanco y el mangle rojo. Ambos juegan un papel muy importante en la preservación del equilibrio en los ecosistemas que se refieren al estero, que es donde se une el mar con agua dulce, además de que es una irreemplazable barrera natural contra posibles tsunamis, huracanes e inundaciones.

Por otra parte las palmas de coco (o palmeras, como se les conoce comúnmente en este Estado) son por excelencia uno de los principales símbolos colimenses y puede encontrárselas prácticamente en todo el Estado.

Por su diversidad de ecosistemas, en el Estado de Colima existe una extensa variedad de fauna silvestre: ardilla, jabalí de collar, venado cola blanca, ocelote, tigrillo, zorra en las sierras; mientras que en los valles hay especies como tapacaminos, torcaza, zanate, tlacuache, tzentzontle, conejo y coyote. Respecto de la fauna marina, ésta se conforma por pez dorado, tiburón, cornuda, aguijón, tortuga de carey, entre otros.

En el Estado de Colima existen diversos elementos orográficos: desde sierras, barrancas, valles, llanuras, mesetas, entre otras forman parte de las dos provincias fisiográficas que incluyen al Estado, la del Eje Neovolcánico y la de la Sierra Madre del Sur. En la porción Noreste y Norte de la entidad se localiza el Volcán de Colima y Cerro Grande respectivamente, este último es la continuación sur de la Sierra de Manantlán en el vecino Estado de Jalisco. Estas distintas monoestructuras determinan las condiciones e influyen en el comportamiento del clima y de la hidrología regional. Los valores precipitados y evapotranspirados tienen relación directa con el clima predominante que es el cálido subhúmedo y el semiseco muy cálido, donde la temperatura media anual varia de 24,8 a 26,6 °C, con lluvias en verano, cuya oscilación entre el mes más cálido y el más frío es inferior a 5 °C; debido además a la composición del suelo cuya dominancia es la baja permeabilidad de los materiales de mayor exposición, como son la andesita, la toba intermedia, la brecha volcánica intermedia, conglomerado, la arenisca, la caliza y el suelo aluvial de granulometría variable.

El Estado se encuentra comprendido dentro de las regiones hidrológicas 15 y 16, denominadas costa de Jalisco y Armería-Coahuayana, respectivamente. La mayor parte de los recursos hídricos que inciden en estas regiones son causados por altos niveles de infiltración y escurrimiento que provienen de las zonas de alta montaña en el sur de Jalisco. El comportamiento del régimen hidrológico trasciende en la entidad, pues los ríos en su mayoría, son corrientes que soportan actividades económicas.

El volcán de Colima o volcán de Fuego se eleva a 3,860 msnm, es uno de los símbolos de Colima. Este volcán lo comparten los estados de Jalisco y Colima. Los municipios afectados por la actividad del volcán son Comala y Cuauhtémoc, en Colima, y Tuxpan, Zapotitlán y Tonila en Jalisco. El tipo de volcán es un estratovolcán; sus erupciones se han considerado explosivas. A lo largo de 500 años el volcán ha tenido más de 40 explosiones desde 1576, de las cuales destacan las de 1585, 1606, 1622, 1690, 1818, 1890, 1903, la más violenta la de 1913 y las más recientes de febrero de 1999 y la del 6 de junio de 2005 a las 11:00 (hora local), se produjo una columna eruptiva que alcanzó sobre el volcán, arrojando cenizas de roca y piroclásticos. Ambos eventos son los de mayor energía liberada después del evento del 13 de enero de 1913, que cerró el cuarto ciclo de actividad. El volcán es vecino del Nevado de Colima, punto más alto del sector occidental de la Faja Volcánica Mexicana. Un antiguo macizo volcánico ubicado en el estado de Jalisco, en el occidente de la República Mexicana; en las inmediaciones entre los estados de Jalisco y Colima. Es un gran panorama ver los dos volcanes juntos y más cuando el Nevado de Colima se viste de nieve y el Volcán de Fuego nos saluda con una fumarola. El estado también cuenta con otros 2 volcánes, los cuales se encuentra en el archipiélago Revillagigedo, el volcán Evermann y el Volcán Bárcena en la isla Socorro.

Según las cifras que arrojó el "II Censo de Población y Vivienda" realizado por el Instituto Nacional de Estadística y Geografía (INEGI) con fecha censal del 12 de junio de 2015, el estado de Colima contaba hasta entonces con un total de 711,235 habitantes, de dicha cantidad, 350,791 eran hombres y 360,444 eran mujeres. La tasa de crecimiento anual para la entidad durante el período 2005-2015 fue del 2.8%.

La población en los diferentes municipios (considerando ciudades y aglomeraciones urbanas) del Estado son:


Manzanillo, Colima, Villa de Álvarez y Tecomán,
son los municipios con mayor población. Juntos
concentran un total de 541,006 personas, es decir,
83.2% de los residentes en el Estado.
Colima (39,148) y Manzanillo (36,277)
presentan los mayores crecimientos absolutos entre
los censos de 2000 y 2015, con tasas anuales de 3.90 y
2.49, respectivamente.
En contraste, Minatitlán e Ixtlahuacán en conjunto
representan 2.1% del total de la población
estatal, presentando tasas de crecimiento negativas
durante el mismo periodo.

Fuente: INEGI. XII Censo General de Población y Vivienda 2000; Censo de Población y Vivienda 2015.

Colima cuenta con una red de carreteras y caminos, entre los que se encuentran los ejes que conectan a las ciudades como Colima-Guadalajara, Tecomán-Lázaro Cárdenas, Manzanillo-Barra de Navidad-Puerto Vallarta, y Colima-Morelia, además de los caminos internos como los de Colima-Manzanillo, Colima-Armería, y Colima-Tecomán. En cuanto a comunicación aérea, existen dos aeropuertos en el estado de Colima, el Aeropuerto Internacional Playa de Oro en Manzanillo y el Aeropuerto Nacional Miguel de la Madrid en Buena Vista, Cuauhtémoc.

El estado de Colima cuenta también con el puerto de Manzanillo, el puerto más importante de México en el Pacífico. Manzanillo fue el tercer puerto que crearon los españoles en el Pacífico y en el desarrollo de su historia se encuentran hechos y personajes muy importantes.

El aforo vehicular es de 1000 camiones diarios al Puerto, 850 vehículos ligeros y 900 motos diarios en promedio. Más de 26 líneas navieras escalan el puerto en ruta regular, además opera más del 90% de la carga contenerizada que se maneja por el Pacífico Mexicano.

El 6 de julio de 2010, la Secretaría de Comunicaciones y Transportes inauguró el muelle especializado en cruceros turísticos del puerto de Manzanillo, que implicó una inversión de 100 millones de pesos en una primera etapa. La obra tienen capacidad para atender a dos cruceros de forma simultánea y en una segunda fase se prevé la construcción de un centro comercial, de acuerdo con las autoridades. Está programado el arribo del "Queen Elizabeth". el día viernes 1 de febrero de 2013 nave hermana del "Queen Victoria"

En el ámbito de transporte urbano, en la Zona Conurbada Colima-Villa de Álvarez en el Estado de Colima son dos empresas las que manejan el transporte urbano; que son SINTRA (Sistema Integral de Transporte S.A. de C.V.) y SOCACOVA (Sociedad Cooperativa de Autotransportes Colima-Villa de Álvarez S.C.L.); las cuales tenien un parque vehicular de un total de 227 unidades de las cuales se encuentran laborando el 50% de unidades en cada empresa.
Oficialmente llamados colimenses. Se mencionan otros gentilicios por ejemplo colimote(a), colimeño(a). Una leyenda colimense lo describe como "alguien tranquilo, relajado, hospitalario, feliz. Sentado bajo la apacible sombra de una palmera en una de las tantas plazas de la ciudad, se refresca bebiendo sorbos de una fría tuba". A su alrededor, pintorescas casas adornan las calles y numerosas palmeras cocoteras se mecen al compás del dulce viento con olor a sal. Uno de los grandes placeres de los colimenses es la cocina. Debido a lo cual numerosos platillos típicos adornan sus mesas, preparados principalmente a base de maíz, frutas, carne de cerdo, pescados y mariscos.

Diversas variedades de tamales de maíz, sopes cubiertos de picadillo, de pata, de lomo o de pollo; langostinos de río preparados en caldo, moyos o cangrejo moro guisado a la diabla, pozole de cerdo y el tatemado de carne de cerdo deleitan los paladares de locales y visitantes. Además, se distingue por su originalidad el ceviche colimense y el pescado a la talla. También, los dulces postres de frutas como alfajor de piña, cocada, rollos de guayaba, plátanos deshidratados y dulces de tamarindo destacan por su rico sabor. Para refrescarse basta con buscar un “tubero”, personaje típicamente colimense que no duda en trepar por una palmera para bajar con lo que será el néctar que sofocará cualquier calor, la tuba.

Y es que, aunque originario de Filipinas, en México el tubero es colimense. Profesión transmitida de generación en generación, el tubero obtiene el néctar de la espiga de las flores de la palma de coco para preparar una refrescante y deliciosa bebida que junto con el tejuino y el bate forman parte de la tradición colimeña de bebidas naturales. El “mariachi de arpa”, que sustituye a la tradicional trompeta por un arpa), es la expresión musical típica de los colimeños. Al compás de los sones y jarabes más tradicionales alegres danzantes y devotos festejan alguna de las muchas fiestas patronales católicas de cada comunidad. Los sones más representativos del estado son El Camino Real de Colima, Las Comaltecas, El Perico Loro, El Palmero, El Pasacalles, La Iguana de Tecomán, El Pedregal, El Pitayero y Los Morismas. Los tejidos de otate, de carrizo y del zopilotote construyen hermosas artesanías de gran valor artístico. Los artesanos de Suchitlán producen todo tipo de canastas y cestos que han dado a la región fama internacional.

Entre los platillos colimenses más gustados y representativos del Estado están los sopitos -pequeñas tostadas cubiertas con picadillo y bañadas en "jugo"-; los sopes gordos, de pata, lomo de cerdo o pollo; y las tostadas de las mismas carnes y preparadas sobre tortillas raspadas y doradas. El pozole (maíz cocido) con carne de cerdo es la merienda tradicional, con la característica de ser seco y jugoso, estos platillos se pueden degustar en la famosas cenadurías que existen por toda la ciudad. Otros guisos típicos son el tatemado -carne de cerdo macerada en vinagre de coco y guisada en chile colorado-, la pepena -vísceras de res guisadas-; y la cuachala -maíz martajado y cocido con pollo deshebrado-. Las variedades locales del tamal son pata de mula -de frijol, envueltos en "hoja" de maíz, no en totomoxtle-; los de carne y los de elote tierno, así como los de ceniza. Comala se distingue por la producción de productos lácteos, como quesillo ranchero, panela y crema, así como los populares ponches bebida alcohólica, preparada mezclando aguardiente de caña o licor con agua, azúcar, café, cacahuete y otras frutas siendo en especial el ponche de granada la combinación más popular, así mismo se distingue por sus botaneros alegres lugares para pasar un rato a gusto con la familia; también, junto con Villa de Álvarez, por su pan dulce, del que destacan los bonetes o picón de huevo.
Las bebidas tradicionales en tiempos de calor la tuba, el bate y el tejuino.
Durante la temporada de lluvias es posible disfrutar los chacales, o langostinos de río, preparados en caldo. Igualmente en ese tiempo, en la costa, los moyos -variedad del cangrejo moro-, guisado a la diabla. Existen criaderos de langostinos que aseguran el abasto permanente de esta delicia culinaria, disfrutable en caldos, a la mantequilla, al ajo o simplemente cocidos. Diferente a la forma que tiene de preparase en los estados vecinos, el ceviche de Colima se hace desmenuzando finamente el pescado (pez sierra de preferencia) y cociéndolo en jugo de limón de Tecomán y mezclándole zanahoria, cebolla, cilantro, jitomate; además que el ceviche se puede hacer con camarón. El pescado a la talla es una especialidad muy apreciada. Se prepara con un pescado entero, abierto y cubierto con verdura picada, luego envuelto en hoja de plátano y asado a las brasas.

En Colima la magia y la imaginación se han transformado en una rica y variada artesanía, que conjuga el uso cotidiano con el talento artístico. Los artesanos han empleado diversos materiales en la elaboración de sus artesanías, como madera, metal, cerámica y alfarería; fibras vegetales, textiles, talabartería, enconchados; todo ello trabajado en sus diferentes técnicas. La mejor artesanía del estado es la fabricación de bellísimos muebles de cedro rojo, con reminiscencias españolas y decorados al óleo. Se hacen sillones con asientos de vaqueta y se tallan máscaras ceremoniales muy originales, adornadas con largas cabelleras y barbas. Otra área donde se destaca la artesanía es en la ropa textil, dentro de la que encontramos confecciones de tela comercial: vestidos, blusas bordadas, así como el traje regional de manta blanca con la imagen de la Virgen de Guadalupe, bordada en color café en el huipil. También hay mucho trabajo de gancho, como carpetas, blusas, manteles y otros artículos de uso diario. En el trabajo de alfarería se utiliza el barro poroso con el que se elaboran copias de piezas prehispánicas, del occidente de Mesoamérica; entre ellas destacan grandes figuras humanas y las pipas. Por último, Colima cuenta con la artesanía de la cestería, que es una de las labores artesanales de mayor trascendencia, la que además conforma una de las herencias prehispánicas que han subsistido hasta el momento.

Dentro de las artesanías más populares y de mayor demanda por ser la representativa del Estado están lo famosos perritos bailadores colimenses, típicamente hechos en barro rojo, pero también se pueden encontrar en madera de parota, árbol emblemático del Estado.



En Universidad de Colima, las Facultades de Derecho y Contabilidad, fueron las primeras de la etapa inicial de su autonomía y con ellas además, se inicia el despliegue universitario, dándole con ello a los estudiantes del Estado la oportunidad de realizarse como profesionales de las leyes, aunado a que la carrera de Derecho ofrece cada día mayores oportunidades de trabajo tanto dentro de la Procuración de Justicia como de la carrera judicial. Es importante resaltar también que la Escuela de Derecho no solamente es la más antigua sino que también hasta hoy es la más prestigiosa de todas porque de ella han egresado abogados y abogadas que han puesto en alto a dicha entidad federativa; por mencionar uno, el Ministro de la Suprema Corte de Justicia de la Nación, José Ramón Cosío Díaz, destacado egresado de la Facultad de Derecho. La Universidad de Colima nace como Universidad Popular de Colima el 16 de septiembre de 1940, con un proyecto inspirado en el espíritu revolucionario. El 25 de agosto de 1962 le es otorgada la autonomía y se separa del sistema educativo estatal, estableciendo carreras universitarias fuertes en las áreas administrativa y agropecuaria, e iniciando una ardua labor para la obtención de recursos suficientes para su crecimiento.















El estado ha firmado los siguientes acuerdos de hermanamiento:





</doc>
<doc id="4765" url="https://es.wikipedia.org/wiki?curid=4765" title="Astrofísica estelar">
Astrofísica estelar

Se llama Astrofísica estelar al estudio de la física de las estrellas; su formación, evolución y final, así como sus propiedades y distribución.

Una herramienta fundamental en el estudio de las estrellas es el diagrama de Hertzsprung-Russell.

El estudio de las estrellas y de su evolución es imprescindible para avanzar en nuestro conocimiento del universo, puesto que ellas constituyen los módulos básicos que componen el mismo. La astrofísica estelar hace uso de la observación y el entendimiento teórico, así como también de simulaciones numéricas de la composición interna de las estrellas.

La formación de las estrellas se produce en regiones densas de polvo y gas molecular, conocidas como nebulosas interestelares. La fuerza de gravedad acerca a los átomos de hidrógeno hacia el centro de la acumulación, haciéndolo más y más denso. Llega un punto en que sus velocidades son tan grandes que el protón de un núcleo de hidrógeno logra vencer la repulsión eléctrica del núcleo en el que impacta, fusionándose con él y otros más hasta formar un núcleo estable de helio.

Una estrella desde su nacimiento tiene diferentes fases de evolución. En sus primeras etapas como embrión es rodeada por los restos de la nube de gas desde la cual se formó. Esa nube de gas es gradualmente disipada por la radiación que emana de la estrella, posiblemente quedando atrás un sistema de objetos menores como planetas, etc.

Pasada la etapa de la infancia, una estrella entra a su madurez, que se caracteriza por un período largo de estabilidad durante el cual, en su núcleo, el hidrógeno se va convirtiendo en helio, liberando enormes cantidades de energía. A esa etapa de estabilidad de la estrella se la llama secuencia principal. 

Las características de la estrella resultante dependerán de la magnitud de su masa. Cuanto más masiva sea la estrella, mayor será su luminosidad y con mayor velocidad agotará el hidrógeno de su núcleo, lo que la hará más luminosa, más grande y más caliente. La rápida fusión de hidrógeno en helio también implica un agotamiento de las reservas del primero más pronto en estrellas masivas que para las de menor tamaño. Para una estrella como el Sol su permanencia en la secuencia principal es de unos 10 mil millones de años; una estrella diez veces más masiva será 10 000 veces más brillante pero durará en la secuencia principal solo unos 100 millones de años. 

Cuando todo el hidrógeno del núcleo de la estrella se haya convertido en helio, ésta comenzará su desarrollo. La fusión del helio requiere una mayor temperatura en el núcleo, por lo que la estrella incrementará tanto su tamaño como la densidad de su núcleo.

No todas las estrellas evolucionan del mismo modo. La masa de la estrella es, de nuevo, determinante a la hora de hacer un estudio sobre las distintas fases que experimenta a lo largo de su vida.

Este tipo de estrellas tienen una vida larga. Nuestro conocimiento sobre su evolución es mera teoría, ya que su etapa en la secuencia principal tiene mayor duración que la actual edad del universo. Los astrofísicos consideran que deberían tener una evolución muy parecida a las estrellas de masa intermedia, a excepción de que en la fase final la estrella se enfriaría convirtiéndose tras un billón de años en una enana negra.

Nuestro Sol se encuentra dentro de esta división. Son estrellas que durante la fase de la secuencia principal transmutan hidrógeno en helio en su núcleo central, pero el primero, en millones de años, se va agotando hasta llegar a un instante en que las fusiones son insuficientes para generar las presiones necesarias para equilibrar la gravedad. Así, el centro de la estrella se empieza a contraer hasta que llega a una temperatura tan elevada que el helio entra en fusión y convierte en carbono. El remanente de hidrógeno se aloja como una cáscara quemándose y transmutándose en helio y las capas exteriores de la estrella se expanden. Esa expansión convierte a la estrella en una gigante roja más brillante y fría que en su etapa en la secuencia principal.

Durante esta fase, una estrella pierde muchas de sus capas exteriores las cuales son eyectadas hacia el espacio por la radiación que emana. Eventualmente, las estrellas más masivas de este tipo logran encender el carbono para que se transmute en elementos más pesados, pero lo normal es que la estrella se derrumbe hacia su interior debido a la presión de la gravedad transformándose en una enana blanca.

Son estrellas de rápida combustión. La corta extensión de sus vidas hace extrañas a las grandes estrellas, pues solo aquellas formadas en los últimos 30 millones de años -y no todas ellas- existen todavía. 

Al principio pasan rápidamente a través de casi las mismas fases que una estrella de masa intermedia, pero las estrellas masivas tienen núcleos tan calientes que transmutan hidrógeno en helio de una manera diferente, usando restos de carbono, nitrógeno y oxígeno. Una vez que la estrella haya agotado el hidrógeno en el núcleo y alojado el remanente de éste como cáscaras, entra a una fase que se conoce como de supergigante roja. Cuando sus núcleos se hayan convertido en helio, la enorme gravedad de las estrellas permite continuar la fusión, convirtiendo el helio en carbono, el carbono en neón, el neón en oxígeno, el oxígeno en silicio, y finalmente el silicio en hierro. Llegado a este punto, como el hierro no se fusiona, el núcleo de la estrella se colapsa, resultando de ello una explosión de supernova.

Se piensa que los restos de una supernova son generalmente una estrella de neutrones. Un púlsar en el centro de la Nebulosa del Cangrejo hoy se identifica con el núcleo de la supernova de 1054. 
En el caso de que la masa persistente de la estrella es de dos a tres veces la del Sol, la contracción continuará hasta formar un agujero negro.

Las estrellas binarias pueden seguir modelos de evolución mucho más complejos, podrían transferir parte de su masa a su compañera y generar una supernova.

Las nebulosas planetarias y las supernovas son muy necesarias para la distribución de metales a través del espacio, sin ellas, todas las nuevas estrellas (y sus sistemas planetarios) estarían formados exclusivamente de hidrógeno y helio.



</doc>
<doc id="4767" url="https://es.wikipedia.org/wiki?curid=4767" title="Daniel Passarella">
Daniel Passarella

Daniel Alberto Passarella (Chacabuco, Buenos Aires, Argentina; 25 de mayo de 1953) es un exfutbolista, exentrenador y exdirigente de fútbol argentino, desempeñándose también por un período como presidente del Club River Plate. Es también, la quinta persona en el mundo en ser jugador, director técnico y presidente de un club, los otros casos son Carlos Babington, Santiago Bernabéu y Franz Beckenbauer, Juan Sebastian Veron

Es considerado el mejor defensor de la historia del fútbol argentino, así como uno de los mejores de la historia del fútbol según la FIFA. Cuenta con la particularidad de ser el único futbolista jugador argentino hasta la fecha en ganar dos Mundiales de Fútbol, al ser capitán en 1978 y formar parte del plantel en el 1986.

Comenzó su carrera en Sarmiento de Junín, y continuó su carrera en clubes como River Plate, Fiorentina e Inter.

Se caracterizó por una gran personalidad y temperamento, así como por la gran cantidad de goles que convirtió, al punto de convertirse en el segundo defensor más goleador en la historia de los torneos de primera división en el mundo, con 134 goles en 451 partidos oficiales, detrás del holandés Ronald Koeman, según la IFFHS. Passarella tiene la particularidad de ser el único jugador argentino en estar en los dos planteles argentinos campeones del mundo (1978 y 1986). Él lo recuerda diciendo: "«En Argentina hay 44 medallas de campeones del mundo para 43 jugadores.»".

Nacido el 25 de mayo de 1953, firmó su primer contrato con el Sarmiento de Junín, que se encontraba en la Primera C, la tercera categoría del fútbol argentino de aquel tiempo. En uno de los partidos fue observado por "Pipo" Rossi, director técnico de River Plate en ese entonces, quien no dudó en llevarlo para su plantel.

Su primer partido fue un clásico de verano contra Boca Juniors en 1974. En ese partido Néstor Rossi le preguntó si se animaba a jugar, a lo que Passarella contestó: "Discúlpeme que le conteste, yo me animo a jugar, hay que ver si usted se anima a ponerme" .
Su debut oficial fue el 14 de abril de 1974 contra Rosario Central, de visitante, un partido que los ""millonarios"" perdieron por 1:0. Pero sus goles no se hicieron esperar, el 28 de julio de 1974 convirtió el gol del triunfo contra Argentinos Juniors, en un encuentro que finalizó 3:2.

En 1975 fue convocado por César Luis Menotti para integrar la selección juvenil que iba a disputar el Torneo Esperanzas de Toulon. Ese equipo estaba integrado entre otros por Américo Gallego, Jorge Valdano y Alberto Tarantini. La selección gana el torneo con la capitanía de Passarella, y desde ese momento "El Kaiser" se convirtió en uno de los preferidos de Menotti.

Con la llegada de Ángel Labruna a la dirección técnica de River Plate y de jugadores como Pablo Comelles, “Perico” Raimondo y Héctor Ártico, Passarella pierde la titularidad ya que se negó a jugar por la izquierda. Solo entraba en los complementos o cuando se lesionaba Roberto Perfumo o Héctor Ártico. Pero en noviembre de 1975, cuando se definía el Campeonato Nacional, volvió a ganarse la titularidad para nunca más dejarla. Ese año consiguió el Campeonato Metropolitano y el Campeonato Nacional, con los que River Plate cortaba 17 años de sequía. En 1976 juega la Copa Libertadores, pero River Plate cayó en la final contra el Cruzeiro.

En 1976 es convocado a la Selección mayor para una gira que se realizaría por Europa del Este. El 20 de marzo de 1976 es su debut contra la Unión Soviética y en 1977 se convirtió en su capitán.

Luego de la racha ganadora del Boca Juniors de Juan Carlos Lorenzo, en 1977 River conseguiría el Campeonato Metropolitano, con Passarella como una de sus figuras. En 1978 la Selección de fútbol de Argentina disputó como local la Copa Mundial de Fútbol de 1978. Passarella era uno de los pilares del equipo junto a Ubaldo Fillol, Mario Alberto Kempes (figura indiscutida de ese mundial), Leopoldo Luque, Daniel Bertoni y Américo Gallego. Ese año Argentina gana el Mundial de Fútbol al vencer a Países Bajos en la final. Passarella fue el encargado de levantar la primera Copa del Mundo que obtuvo la Argentina.

En River seguiría obteniendo títulos: el bicampeonato de 1979, el Metropolitano de 1980 y el Nacional de 1981. En 1982 no jugó muchos partidos debido a que estaba realizando con la selección nacional la preparatoria para el Mundial. Ese plantel, que incluía a Diego Armando Maradona, cayó en segunda ronda después de una pobre actuación.
Tras el mundial, Passarella es comprado por la Fiorentina en 2,5 millones de dólares. En ese club jugó hasta 1986, cuando fue transferido al Inter de Milán, marcando 39 goles y convirtiéndose en uno de los mejores jugadores que hayan pisado la institución.

En noviembre de 1982 llega a la dirección técnica de la Selección Argentina Carlos Salvador Bilardo. Con su llegada declaró que el único jugador que tenía asegurada la titularidad era Diego Armando Maradona, algo que molestó a Passarella, y le quitó la capitanía al "Kaiser" para dársela a Maradona. Esto fue visto como un castigo hacia Passarella por la estrecha relación que tenía con el antiguo entrenador, César Luis Menotti. Pero el nivel de Passarella no mermó ni en la Fiorentina ni en la selección, por lo que siguió siendo convocado. En la clasificación para el Mundial de Fútbol, Passarella dio el pase gol a Ricardo Gareca para el tanto decisivo frente a Perú que le dio a la Argentina la clasificación. Si bien el gol no fue de Passarella, todos fueron a abrazar a ""El Gran Capitán"".

Durante el Mundial de 1986, sufrió una infección intestinal que lo dejó fuera de las canchas. Cuando la bacteria desapareció, sufrió un desgarro. Si bien fue parte del plantel que viajó, por estos motivos no jugó ningún partido del Mundial que ganaría la Argentina, al vencer a Alemania Federal en la final.

Tras jugar dos temporadas para el Internazionale de Milán, vuelve a River Plate, equipo que era dirigido por César Luis Menotti. El equipo no consiguió el título, y Menotti fue reemplazado por un histórico del club: Reinaldo Merlo. El 27 de julio de 1989 tras ganar un clásico contra Boca Juniors por 2:1, decide retirarse de la actividad futbolística.

Después de las elecciones en River el 9 de diciembre de 1989, Reinaldo Merlo decide dejar su cargo. Es reemplazado por Passarella, quien nunca había dirigido un equipo y se había retirado meses atrás. En River, con Américo Gallego como ayudante de campo, consigue el Campeonato '89/'90, el Apertura de 1991 y el Apertura de 1993. Luego de dirigir 215 partidos (105 ganados, 65 empatados y 45 perdidos), es nombrado en 1994 para dirigir la Selección Argentina.

Durante ese período se presentó como un "duro". Con un balance flamante de las desprolijidades en el final del "ciclo Basile", Passarella apareció con un discurso de "mano dura" que incluyó alusiones a rinoscopías y prohibiciones varias: desde los aros hasta los homosexuales. Y una rotunda negativa al pelo largo. Fueron muchos los convocados, o aquellos que se veían cerca, que acataron la orden y se cortaron el pelo, como Gabriel Batistuta. Pero hubo dos figuras a las que no llamó durante largo tiempo, curiosamente ambas con el pelo largo: Claudio Caniggia y Fernando Redondo. Para septiembre de 1995, y con el equipo bastante desmejorado, Passarella decidió que Redondo, que brillaba en el Real Madrid, tenía que estar; y el futbolista (ofendido por esa exigencia que juzgaba inútil) rechazó la convocatoria. En el ´97, en la recta final para el Mundial, Passarella volvió a convocar al mediocampista, sin hablar antes con él. Redondo dijo que las diferencias eran irreconciliables y renunció a la Selección, un episodio que dejó en ridículo al "Kaiser." 

En 1995 ganó los Juegos Panamericanos realizados en la Ciudad de Mar del Plata. Obtuvo una Medalla de Plata en los Juegos Olímpicos de Atlanta 1996. Se quedó afuera de la Copa América 1995, en cuartos y con su némesis Brasil, y la edición ´97 (eliminado en cuartos, esta vez por Perú 2-1) Consiguió la clasificación al mundial de 1998, pero perdió en cuartos de final contra Países Bajos. Passarella renunció a su cargo después del Mundial, como ya lo había anunciado meses antes. 

Tuvo un corto paso como entrenador de la Selección de Uruguay, pero renunció en medio de las eliminatorias del Mundial 2002. Luego de un paso sin éxito por el Parma FC, a mediados del año 2002, es contratado por el Club de Fútbol Monterrey, con el cual termina clasificándose en el tercer puesto a la Liguilla. El 14 de junio de 2003 consigue el Torneo Clausura 2003 tras vencer al Morelia por marcador global de 3-1.

En el 2005 fue contratado por el Sport Club Corinthians Paulista, donde no tuvo buenos resultados y fue despedido.

El 10 de enero del 2006, Passarella vuelve a dirigir a River tras 12 años, reemplazando nuevamente a Merlo, que renunció en la madrugada argentina del día anterior. En este período de su carrera no logró ningún título. Además, al ser eliminado de la Copa Libertadores de América edición 2007, una importante cantidad de hinchas pidió su renuncia reiteradas veces en el hall del Club Atlético River Plate.

El 25 de mayo de 2007 (el día de su cumpleaños y aniversario del club), antes del comienzo del Apertura 2007, él mismo se estableció un plazo para renunciar, diciendo que en caso de que no ganara ningún título a fin de año del 2007, renunciaría al cargo de Director Técnico, sin cobrar el dinero de ese plazo.

Finalmente, el 15 de noviembre dimite como entrenador de River Plate después de la eliminación de su equipo en las semifinales de la Copa Sudamericana, a manos de Arsenal de Sarandí, en definición de penales.

Existió la posibilidad de ser el sucesor de Richard Páez en el puesto de director técnico de la selección venezolana de fútbol, aunque finalmente fue elegido César Farías.

El domingo 13 de diciembre de 2009 fue elegido como nuevo presidente de River por escaso margen y ante una división de opiniones en el electorado. D'Onofrio, el otro postulante con posibilidades, impugnó sin éxito el resultado electoral ante la justicia.

En diciembre de 2008 anuncia su candidatura a presidente de River Plate. El 5 de diciembre de 2009 es elegido presidente, luego de que un segundo recuento lo diera ganador por 6 votos (paradójicamente, el número que utilizó siempre en su espalda). Así cierra su ciclo al frente de River Plate logrando ser jugador, ídolo, director técnico y ahora presidente de la institución. Asumió el 9 de diciembre de 2009.

Passarella se encontró con la difícil situación que atravesaba River Plate, debida a la pésima gestión del anterior presidente de la institución, José María Aguilar. Por los malos resultados obtenidos en el torneo anterior, River arrancaría el 2010 muy mal posicionado en la tabla del descenso "imaginaria" para 2011 (ya que todavía faltaban definirse los descensos del 2010). En esas vacaciones de verano los medios encendieron las alarmas y todos los ojos empezaron a posarse sobre River, recién entonces la hinchada empezó a entender lo que podía pasar, aunque algunos seguían escépticos. Era difícil pensar en la posibilidad de que River descienda, ya que en más de 100 años de historia, los 3 equipos más grandes de Argentina nunca habían descendido. En los próximos 3 torneos, se decía, River tenía que obtener aproximadamente 30 puntos en cada uno.

Durante la primera parte del año 2010, Passarella concretó las contrataciones de Rodrigo Rojas, Juan Manuel Díaz y Alexis Ferrero. Sin embargo, el rendimiento del equipo durante el no fue el esperado, y Leonardo Astrada tuvo que abandonar el cargo a mitad del torneo. Para afrontar el resto del campeonato, Passarella tomó la decisión de contratar a Ángel Cappa como director técnico, decisión que no fue bien recibida por parte de los hinchas "millonarios", que reclamaban por la vuelta de Ramón Díaz. Cappa terminó el torneo de manera aceptable, con tres importantes victorias frente a Godoy Cruz, Vélez Sarsfield y Racing Club, y una catastrófica derrota por 5 a 0 de local frente a Tigre.

En las vacaciones de invierno y de cara al Torneo Apertura 2010 River tenía la necesidad imperiosa de sumar puntos, entonces Passarella contrató a varios refuerzos, entre ellos, Ballon, Arano, Adalberto Roman, Acevedo, Caruso, Juan Pablo Carrizo, Mariano Pavone y Jonatan Maidana. El equipo tuvo un comienzo auspicioso, con buen nivel de juego pero algunos problemas en defensa, ganando 3 partidos, empatando 1 y perdiendo 2. En la fecha 8 se produce la lesión de su capitán y mejor jugador Matías Almeyda, y entonces el rendimiento del equipo de Cappa decayó, con 5 empates seguidos y una derrota contra All Boys, luego de la cual Passarella decidió dimitir de su cargo al técnico y determinó que J.J. López afronte el resto del torneo. J.J. López cambió la táctica, volviéndola más defensiva con una línea de 3 centrales, dos laterales y un doble 5. Así, empezó nada menos que con una victoria contra Boca Juniors en el Monumental y terminó el campeonato de muy buena manera, consiguiendo en total 13 puntos de 18 en juego (4 victorias, un empate y una derrota). En total, River terminaría el torneo cuarto con 31 puntos (8 victorias, 7 empates y 4 derrotas).

Luego de esto, River debía afrontar el , en el que tenía que hacer una muy buena campaña para evitar el descenso definitivamente. Frente a esto, en vez de contratar a un DT con mayores pergaminos, Passarella decidió mantener a López en el cargo, a pesar de sus malos antecedentes frente a equipos en situación de descenso. Además de esto, incorporó a un solo jugador: Fabián Bordagaray, y dejó salir del club al máximo ídolo riverplatense de los últimos años, Ariel Ortega, por problemas extra futbolísticos. Estas medidas fueron fuertemente criticadas por los simpatizantes de River Plate.

River comenzó el de muy buena manera, incluso llegando a estar puntero del campeonato en la fecha 9. Sin embargo, el equipo de J.J. cayó en picada, probablemente por el exceso de presión, y de los últimos 21 puntos en juego solo sumó 4 unidades. El equipo finalizó en la novena posición con 26 puntos y 17º en la Tabla de Promedios, con un promedio de 1,236, a 0,027 (4 puntos) del último "salvado" que fue Tigre, por lo que debió jugar la promoción. En el encuentro de ida, insólitamente Passarella se pelearia con los referentes del plantel como Pavone, Acevedo, Maidana, etc, por lo cuál, JJ Lopez se ve obligado a mandar a la cancha a juveniles con poca experiencia en Primera como Roberto Pereyra, Ezequiel Cirgliano, Erik Lamela; Mauro Díaz; Rogelio Funes Mori, lo que seria un error garrafal de Passarella ya que River perdió en Córdoba por 2 a 0 frente a Belgrano, lo cuál seria ya el principio de la condena. En el partido de vuelta, River empató 1 a 1 y quedó condenado al descenso a la B Nacional por primera vez en su historia.

Por lo mencionado, la mayoría de los hinchas de River Plate consideran a Passarella y a J.J. López como los dos principales culpables del descenso de River Plate a la Primera B Nacional, ocurrido por única vez en la historia en junio de 2011, sobre todo porque Passarella contrató a Cappa en vez de a Ramón Díaz, por contratar a JJ Lopez con sus antedecendes en descender equipos, y por no traer ningún refuerzo en el torneo más importante de la historia de River Plate, teniendo en cuenta que Daniel rechazó una oferta de 9 millones de Euros del Benfica por Funes Mori, con lo cuál podría haber contratado más refuerzos, sin embargo, el equipo terminó décimo séptimo en la tabla de promedios de primera división, calculado sobre la base de los últimos 3 años de competencia, es decir, 6 torneos, 3 de los cuales (los primeros, los de peor rendimiento) fueron bajo la presidencia anterior:


Los primeros tres torneos fueron bajo la gestión anterior, de José María Aguilar, y dan un promedio de 20,7 puntos cada uno, mientras que los tres siguientes dan un promedio de 26,3 puntos, un promedio bajo para una institución como River, pero no tanto como para condenarlo al descenso. Por lo tanto, el descenso de River Plate comparte culpa entre la gestión de José María Aguilar, quien además dejó al Club en las condiciones económicas más nefastas de su historia, y por gran culpa de Passarella que con su soberbia y malas decisiones terminó condenando al club a lo más bajo de su historia. Por el lado de J.J. López, es el técnico de mayor eficacia de todos los que pasaron en esos seis torneos, con un 52%. En realidad, el técnico de menor eficacia en ese lapzo fue Diego Simeone, que en el Apertura 2008 renunció luego de la fecha 14 dejando al club último en la tabla de posiciones con sólo 10 puntos (24% de eficacia).

Para más detalles y estadísticas sobre el Descenso de River Plate a la Primera B Nacional entre en el enlace.

Apenas consumado el descenso, se supo la noticia de que el capitán, ídolo y mejor jugador del equipo por entonces, Matías Almeyda, pasaría a ser el nuevo técnico del club, sin tener experiencia como DT. La temporada siguiente, con un equipo con varios cambios y un estilo muy ofensivo, River saldría campeón de la Segunda División y conseguiría el ascenso directo, en la última fecha y luego de algunos partidos agónicos. Una vez conseguido ese logro, Passarella volvería a tener problemas con los hinchas. Alejandro "el Chori" Domínguez y Fernando Cavenaghi, dos ídolos que habían vuelto a préstamo al club para devolverlo a primera, salieron a los medios anunciando una ruptura de relaciones con la dirigencia, culpa de la cual no se intentaron negociaciones para mantener a los jugadores en el equipo.

El último año y medio de Passarella al frente de la institución transcurrió sin pena ni gloria, con un sostenimiento de las mejoras en el ámbito económico pero sin conseguir ningún título. De los tres torneos que jugó River en la gestión Passarella luego del ascenso, el primero fue dirigido casi en su totalidad por Matías Almeyda, pero en las últimas fechas y tras un rendimiento mediano y demasiados cambios en el equipo titular, él fue reemplazado por Ramón Díaz quien dirigió además los dos torneos restantes. En el Torneo Final 2013 River tuvo un muy buen nivel pero quedó segundo, a tres puntos del gran Newell's del Tata Martino. Luego, en el Torneo Inicial 2013 River saldría 17° con solo 21 puntos y un nivel pésimo. Pese a esto, Passarella (otra vez polémico) le renueva el contrato a Ramón con una cifra millonaria, sabiendo que si despedia al ídolo riverplante la hinchada quizás esta vez explotaría.

Passarella no se presentó en las elecciones de diciembre de 2013, a causa de su mala imagen y de su pésima gestión. En ellas se impuso Rodolfo D'Onofrio con el 55% de los votos, quien sostuvo a Ramón Díaz como DT. En este último torneo River salió campeón, pero luego Ramón renunció por problemas con la actual dirigencia. En junio de 2014 asume como DT Marcelo Gallardo y en poco tiempo se convierte en el técnico más ganador de títulos internacionales en la historia de River. Como logro residual, vale aclarar que casi todos los jugadores importantes en el logro de la Copa Sudamericana 2014 y la Copa Libertadores 2015 por parte de River fueron conseguidos por la dirección técnica de Ramon Angel Díaz, como ser Marcelo Barovero, Leonel Vangioni, Ramiro Funes Mori (inferiores), Jonatan Maidana, Gabriel Mercado, Ariel Rojas, Matías Kranevitter (inferiores), Leonardo Ponzio, Carlos Sánchez, Rodrigo Mora y Teófilo Gutiérrez. Esas elecciones finalmente las ganaría Rodolfo D'Onofrio y volvería a poner al club en lo más alto del continente.

La siguiente tabla detalla los encuentros disputados y los goles marcados por Passarella en la selección argentina absoluta.




</doc>
<doc id="4769" url="https://es.wikipedia.org/wiki?curid=4769" title="Julio César">
Julio César

Cayo Julio César o Gayo Julio César  () fue un político y militar romano del miembro de los patricios Julios Césares que alcanzó las más altas magistraturas del Estado romano y dominó la política de la República tras vencer en la guerra civil que le enfrentó al sector más conservador del Senado.

Nacido en el seno de la "gens" Julia, una familia patricia de escasa fortuna, estuvo emparentado con algunos de los hombres más influyentes de su época, como su tío Cayo Mario, quien influiría de manera determinante en su carrera política. En , a los 16 años, el popular Cinna lo nombró "flamen Dialis", cargo religioso del que fue relevado por Sila, con quien tuvo conflictos a causa de su matrimonio con la hija de Cinna. Tras escapar de morir a manos de los sicarios del dictador Sila, fue perdonado gracias a la intercesión de los parientes de su madre. Trasladado a la provincia de Asia, combatió en Mitilene como legado de Marco Minucio Termo. Volvió a Roma a la muerte de Sila en , y ejerció por un tiempo la abogacía. En sucedió a su tío Cayo Aurelio Cota como pontífice, y pronto entró en relación con los cónsules Pompeyo y Craso, cuya amistad le permitiría lanzar su propia carrera política. En César sirvió como cuestor en la provincia de Hispania y como edil curul en Roma. Durante el desempeño de esa magistratura ofreció unos espectáculos que fueron recordados durante mucho tiempo por el pueblo.

En 63 a. C. fue elegido pretor urbano al obtener más votos que el resto de candidatos a la pretura. Ese mismo año murió Quinto Cecilio Metelo Pío, "pontifex maximus" designado durante la dictadura de Sila, y, en las elecciones celebradas para sustituirle, venció César. Al término de su pretura sirvió como propretor en Hispania, donde capitaneó una breve campaña contra los lusitanos. En 59 fue elegido cónsul gracias al apoyo de sus dos aliados políticos, Pompeyo y Craso, los hombres con los que César formó el llamado Primer Triunvirato. Su colega durante el consulado, Bíbulo, se retiró para así entorpecer la labor de César que, sin embargo, logró sacar adelante una serie de medidas legales, entre las que destaca una ley agraria que regulaba el reparto de tierras entre los soldados veteranos.

Tras su consulado fue designado procónsul de las provincias de Galia Transalpina, Iliria y Galia Cisalpina, esta última tras la muerte de su gobernador, Céler. Su gobierno se caracterizó por una política muy agresiva con la que sometió a prácticamente la totalidad de los pueblos celtas en varias campañas. Este conflicto, conocido como la guerra de las Galias, finalizó cuando el general republicano venció en la batalla de Alesia a los últimos focos de oposición, encabezados por un jefe arverno llamado Vercingétorix. Sus conquistas extendieron el dominio romano sobre los territorios que hoy integran Francia, Bélgica, Países Bajos y parte de Alemania. Fue el primer general romano en penetrar en los inexplorados territorios de Britania y Germania.

Mientras César terminaba de organizar la estructura administrativa de la nueva provincia que había anexionado a la República, sus enemigos políticos trataban en Roma de despojarle de su ejército y cargo utilizando el Senado, en el que eran mayoría. César, a sabiendas de que si entraba en la capital sería juzgado y exiliado, intentó presentarse al consulado "in absentia", a lo que la mayoría de los senadores se negaron. Este y otros factores le impulsaron a desafiar las órdenes senatoriales y protagonizar el famoso cruce del Rubicón, momento en el que, al parecer, pronunció la inmortal frase "alea iacta est" («la suerte está echada»). Inició así una nueva guerra civil, en la que se enfrentó a los "optimates", que estaban liderados por su viejo aliado, Pompeyo. Sus victorias en las batallas de Farsalia, Tapso y Munda sobre los conservadores, le hicieron el amo de la República. El hecho de que estuviera en guerra con la mitad del mundo romano no evitó que se enfrentara a Farnaces II en Zela y a los enemigos de Cleopatra VII en Alejandría. A su regreso a Roma se hizo nombrar cónsul y "dictator perpetuus" —dictador vitalicio—, e inició una serie de reformas económicas, urbanísticas y administrativas.

A pesar de que bajo su gobierno la República experimentó un breve periodo de gran prosperidad, algunos senadores vieron a César como un tirano que ambicionaba restaurar la monarquía. Con el objetivo de eliminar la amenaza que suponía el dictador, un grupo de senadores formado por algunos de sus hombres de confianza como Bruto y Casio y antiguos lugartenientes como Trebonio y Décimo Bruto urdieron una conspiración con el fin de eliminarlo. Dicho complot culminó cuando, en los "idus de marzo", los conspiradores asesinaron a César en el Senado. Su muerte provocó el estallido de otra guerra civil, en la que los partidarios del régimen de César, Antonio, Octavio y Lépido, derrotaron en la doble batalla de Filipos a sus asesinos, liderados por Bruto y Casio. Al término del conflicto, Octavio, Antonio y Lépido formaron el Segundo Triunvirato y se repartieron los territorios de la República, aunque, una vez apartado Lépido, finalmente volverían a enfrentarse en Accio, donde Octavio, heredero de César, venció a Marco Antonio.

Al margen de su carrera política y militar, César destacó como orador y escritor. Redactó, al menos, un tratado de astronomía, otro acerca de la religión republicana romana y un estudio sobre el latín, ninguno de los cuales ha sobrevivido hasta nuestros días. Las únicas obras que se conservan son sus "Comentarios de la guerra de las Galias" y sus "Comentarios de la guerra civil". Se conoce el desarrollo de su carrera como militar y gran parte de su vida a través de sus propias obras y de los escritos de autores como Suetonio, Plutarco, Veleyo Patérculo o Eutropio.

Julio César fue miembro de los Julios Césares, una familia patricia de la "gens" Julia que aparece por primera vez en los registros históricos a finales del , durante la segunda guerra púnica. Sin embargo, no es hasta la República tardía, cuando se habían divido en dos ramas en tribus , que los Julios Césares se sitúan entre las familias aristocráticas principales.

Su padre, homónimo suyo, fue un senador de rango pretorio que se apartó de la política tras un gobierno provincial en Asia. Su madre fue Aurelia, miembro de los Aurelios Cotas, una destacada familia de la nobleza plebeya. Varios miembros de esta familia alcanzaron el consulado a finales del y principios del siguiente, lo que supondría una notable ayuda para la progresión política del futuro dictador. Más decisiva sería la relación con Cayo Mario, quien contrajo matrimonio con Julia, la tía paterna de Julio César. Mario y sus seguidores (el "partido" mariano) dominaron la política romana a finales del y fue esta relación la que aprovechó Julio César en sus inicios políticos.

Además de su tía Julia, el cónsul del año , Sexto Julio César, pudo haber sido también hermano de su padre. Sus dos hermanas, de las que apenas se sabe nada, casaron con acaudalados miembros de la aristocracia itálica: la mayor con un miembro de la "gens" Pedia; la menor con Marco Acio Balbo, emparentado con los Pompeyos. De otro matrimonio de la mayor de sus hermanas descendía Lucio Pinario Escarpo.

Julio César estuvo casado al menos en tres ocasiones. En primer lugar con Cornelia, hija de Lucio Cornelio Cinna, de familia patricia. Cinna era partidario de Mario y enemigo de Lucio Cornelio Sila, por lo que Julio César reforzaba sus lazos con los principales dirigentes de la Roma de los años ochenta del No obstante, tras la muerte de Cornelia, casó con Pompeya, nieta del propio Sila y de familia consular, de la que se divorció tras un escándalo durante las celebraciones de la Bona Dea. Su último matrimonio fue con Calpurnia, hija de Lucio Calpurnio Pisón Cesonino, un destacado miembro del Senado, como parte de la política matrimonial del primer triunvirato.

De su primer matrimonio, tuvo una hija, Julia, casada con su colega triunviral Cneo Pompeyo el Grande  y que murió al dar a luz. Sin herederos directos, adoptó por vía testamentaria a su sobrino nieto Cayo Octavio, el futuro emperador Augusto.

Julio César nació en Roma el 12 o 13 de julio del año Siguiendo las fuentes clásicas, la mayoría de estudiosos modernos coinciden en el año, excepto Theodor Mommsen () y Jérôme Carcopino (), que se basaron en presupuestos de las "leges annales". En cuanto al día, mientras unos autores sostienen el 13, otros prefieren el 12 y aun otros no se decantan por ninguno. Las dudas proceden de una cita de Casio Dion en la que dice que Julio César nació el día 13, pero se cambió al 12 por una ley triunviral. Este cambio se debió a que el 13 de julio se celebraban unos juegos en honor de Apolo y, según un oráculo, ningún otro festival en honor a una divinidad podía tener lugar el mismo día. El resto de fuentes clásicas refieren el 12 de julio como fecha de nacimiento.
Julio César entra en la historia en el año 84 a. C., cuando Cinna lo escogió para ser "flamen Dialis" y lo casó con su hija Cornelia tras romper el compromiso previo del joven con Cosucia. Por entonces Cinna controlaba la República. Sin embargo, la guerra civil que siguió a la muerte de este terminó con el triunfo de Sila y la anulación de todo su programa. Julio César recibió la orden de divorciarse de Cornelia. Su negativa impulsó a Sila a proscribirlo y obligó a aquel a huir de Roma. Fue la intervención de sus parientes y el respaldo de las vírgenes vestales, lo que permitió que el joven César evitara el destino de otros proscritos, acordando que mantendría su matrimonio y renunciaría al sacerdocio. Según Suetonio y Plutarco, en aquella ocasión Sila dijo que había en el joven muchos Marios.

Al percatarse de que el perdón de Sila podía ser revocado en cualquier momento, César juzgó que lo más seguro era alejarse de Roma durante un tiempo y decidió viajar a Oriente para participar en la guerra contra Mitrídates VI del Ponto bajo las órdenes del propretor, Marco Minucio Termo. Durante el sitio de Mitilene se le ordenó ir a Bitinia para solicitar a Nicomedes IV la cesión de una pequeña flota con la que asaltar la ciudad rebelde. Al parecer, el rey asiático quedó tan deslumbrado con la belleza del joven mensajero romano que lo invitó a descansar en su habitación y a participar en un festín donde sirvió de copero real durante el banquete. La aventura de César en Asia llegó muy pronto a oídos de los ciudadanos de Roma. En la política romana, acusar a alguien de mantener relaciones homosexuales pasivas era una estrategia común, pues la homosexualidad pasiva, a diferencia de la activa, era considerada una práctica vergonzosa. Sus enemigos políticos proclamaron que se había prostituido con un rey bárbaro y le apodaron «la reina de Bitinia», causando así un gran daño a su reputación. Sin embargo, César siempre desmintió este hecho. El resto de la campaña le valió una mejor reputación, porque mostró una gran capacidad de mando y un arrojo y valor personal encomiables, por los que Minucio Termo, tras la toma de Mitilene, le concedería la corona cívica, la condecoración al valor más alta que se otorgaba en la República Romana.

Después de la muerte de Sila en el , César regresó a Roma e inició una carrera como abogado en el Foro romano, con la que se dio a conocer por su cuidada oratoria. Su primer caso fue dirigido contra Cneo Cornelio Dolabela, un protegido de Sila que en el año había sido elegido cónsul y después, al año siguiente, procónsul en Macedonia, y donde al parecer había malversado los fondos del Estado. Dolabela, al enterarse del proceso en su contra, contrató para su defensa a uno de los más ilustres abogados de la época, Quinto Hortensio (llamado «El Bailarín» por su manera de moverse en los estrados), y al eminente Lucio Aurelio Cota —el propio tío de César, pero esto era normal—. A pesar de estos formidables enemigos, César mostró su calidad de orador, que, aunque no le sirvió para ganar la causa, sí le procuró la fama que buscaba.

Al año siguiente unas ciudades griegas que habían sido saqueadas por Cayo Antonio Híbrida durante la campaña de Sila en Grecia, le confiaron la defensa de su causa. César habló ante el pretor Marco Terencio Varrón Lúculo con mucha elocuencia y ganó el juicio, pero Híbrida apeló a los tribunos de la plebe, quienes ejercieron su derecho al veto, y dejaron en suspenso la sentencia dictada en su contra. En el año la muerte de su tío le abrió las puertas para ser elegido "pontifex" en su lugar y entró de esa manera en el Colegio de Pontífices, un organismo religioso de gran calado en la vida piadosa de Roma.

A pesar de este éxito, César decidió viajar a Rodas para ampliar su formación estudiando filosofía y retórica con el gramático Apolonio Molón, que era considerado el mejor de la época. Sin embargo, durante el viaje, su barco fue asaltado a la altura de la isla Farmacusa por los piratas, que lo secuestraron. Cuando exigieron un rescate de 20 talentos de oro (un talento equivalía a 26 kilos aproximadamente), César se rio y los desafió a pedir 50. En su cautiverio se dedicó a componer algunos discursos, teniendo por oyentes a los piratas, a quienes trataba de ignorantes y bárbaros cuando no aplaudían. Treinta y ocho días después, el rescate llegó y César fue liberado después de un cautiverio bastante cómodo, durante el cual, a pesar de tratar a sus secuestradores con amabilidad, les avisó en varias ocasiones de su negro futuro. Así, una vez recuperada su libertad, organizó una fuerza naval que partió del puerto de Mileto, capturó a los piratas en su refugio y los llevó a la prisión en Pérgamo. Una vez capturados fue en busca de Junio, gobernante de Asia, porque le competía a este castigar a los apresados. Junio se interesó más en el botín y dejó a los bandidos a juicio de César, quien los mandó crucificar, tal como les había prometido, aunque en un gesto de "compasión" ordenó que primero los degollaran.

En , Cornelia falleció mientras daba a luz a un niño que nació muerto y poco después César perdió a su tía Julia, viuda de Mario, a quien se había sentido muy unido. En contra de las costumbres de la época, César insistió en organizar sendos funerales públicos. Ambos funerales sirvieron también para desafiar las leyes de Sila, pues en el sepelio de Julia se exhibieron las imágenes de Gayo Mario y del hijo que había tenido con ella y que también había luchado contra Sila: su difunto primo, Gayo Mario el Joven; y en el sepelio de Cornelia, la imagen de su padre Lucio Cornelio Cinna. Todos ellos habían sido proscritos y las leyes del dictador prohibían mostrar sus imágenes en público, pero César no vaciló en quebrar las reglas. Este desafío fue muy apreciado por los plebeyos y los que formaban la facción de los "populares", y, en la misma medida, repudiado por los "optimates".

César fue elegido cuestor por los Comicios en el , con 30 años de edad, como estipulaba el "cursus honorum" romano. En el sorteo subsiguiente, le correspondió un cargo en la provincia romana de Hispania Ulterior, situada en lo que es hoy día Portugal y el sur de España. Según cuenta una leyenda local, en el Templo de Hércules Gaditano (Herakleión) de la ciudad de Gades, situado en lo que actualmente es el Islote de Sancti Petri, Julio César tuvo un sueño que le predecía el dominio del mundo después de haber llorado ante el busto de Alejandro Magno por haber cumplido su edad sin haber alcanzado un éxito importante. Allí, como cuestor, conoció a Lucio Cornelio Balbo "El Mayor", el cual posteriormente se convirtió en consejero y amigo del futuro dictador y propretor de la Hispania Ulterior en el año Gades proporcionó un gran apoyo a la flota romana en su campaña de Lusitania, donde Balbo ya era "praefectus fabrum", esto es, una especie de jefe de ingenieros, perteneciente a la plana mayor de las legiones.
Entonces Hispalis, Corduba, Gades, Malaca e Itálica eran "conventum civium romanum". Julio César llamó con su nombre a Iulia Romula Hispalis y se le considera el constructor de sus murallas.

De vuelta en Roma, César prosiguió su carrera como abogado hasta ser elegido edil curul en el año , el primer cargo del "cursus honorum" que se desempeñaba dentro de Roma. Las funciones de un edil pueden ser equiparadas, en cierto modo, a las de un moderno Presidente de una Junta Municipal, e incluían la regulación de las construcciones, del tránsito, del comercio y otros aspectos de la vida diaria, entre otras las funciones de jefe de policía. Pero el cargo, el primer peldaño público para llegar a la magistratura suprema del consulado, podía ser también el último que se desempeñara, pues incluía la organización de los juegos en el Circo Máximo, lo que, debido a lo limitado del presupuesto público, exigía al edil la utilización de fondos personales. Esto fue especialmente verdad en el caso de César, que pretendía realizar unos juegos memorables para impulsar su carrera política. Y, de hecho, empleó todo su ingenio para conseguirlo, llegando a desviar el curso del Tíber e inundar el Circo para ofrecer una naumaquia (un combate entre barcos). Acabó el año con deudas del orden de varios cientos de talentos de oro.

Sin embargo, su éxito como edil fue una ayuda importante para que, después de la muerte de Quinto Cecilio Metelo Pío en el año , César fuera elegido "Pontifex Maximus" dignidad que dotaba al electo de enorme "auctoritas" y "dignitas". El día de su elección había sospechas de un atentado contra él, lo que obligó a Julio César a decir a su madre: El cargo implicaba una casa nueva en el Foro, la "Domus Publica", la presidencia del Colegio de Pontífices y una cierta preeminencia en la vida religiosa de Roma, así como la asunción de los deberes y derechos del paterfamilias sobre las Vírgenes Vestales.

Su estreno como "Pontifex Maximus" fue marcado por un escándalo. Después de la muerte de Cornelia, César se había casado con Pompeya (hija de Cornelia Sila y Quinto Pompeyo Rufo), nieta de Sila. Como esposa del "Pontifex Maximus" y una de las mujeres más importantes de Roma, Pompeya era responsable de la organización de los ritos de la "Bona Dea" en diciembre, una liturgia exclusivamente femenina, donde los hombres no podían participar. Pero durante las celebraciones del año , Publio Clodio Pulcro (un joven líder demagogo, considerado como peligroso) consiguió entrar en la casa disfrazado de mujer, al parecer movido por el lascivo propósito de yacer con Pompeya. En respuesta a este sacrilegio, del cual ella probablemente no era culpable, Pompeya recibió una orden de divorcio. César declaró en público que él no la consideraba responsable, pero justificó su acción con la célebre máxima: Clodio fue perdonado por voluntad del pueblo, ya que nada había sido probado y el propio César no quiso declarar contra él.

En las elecciones para el , Marco Tulio Cicerón salió elegido cónsul "senior". Fue un año particularmente difícil no solo para César, sino también para Roma. Durante su consulado, Cicerón reveló una conspiración para destituir a los magistrados electos y reducir la funcionalidad del Senado, complot liderado por Lucio Sergio Catilina, un patricio frustrado por su falta de éxito político. Si bien no se celebró juicio contra ellos, en el sentido estricto del término, lo cierto es que casi todos los acusados en la conspiración, y desde luego Catilina, estuvieron presentes en las sesiones del Senado en las que se les "juzgó". En la tercera reunión, Cicerón descargó su responsabilidad sobre la curia haciendo que los senadores debatieran la pena a la que habría de condenarse a los conjurados. El resultado fue una sentencia de muerte para cinco prominentes romanos aliados de Catilina y para el propio Catilina. Todos estos extremos quedaron para la posteridad en las famosas Catilinarias escritas por el propio Cicerón.

César se opuso a la pena de muerte usando para esos fines su mejor oratoria, pero fue vencido por la insistencia de Marco Porcio Catón el Joven y los cinco hombres fueron ejecutados ese mismo día. Fue también en esta dramática reunión del Senado en la que el romance de César con Servilia, hermana de Marco Porcio Catón, salió a la luz. Los opositores políticos de César lo acusaron de formar parte de la conspiración de Lucio Sergio Catilina, lo que nunca fue probado ni perjudicó su carrera.

César fue elegido pretor urbano para el , el puesto de pretor más distinguido, ya que era el que se ocupaba de los asuntos entre ciudadanos romanos. Apoyó al tribuno de la plebe Quinto Cecilio Metelo Nepote cuando este presentó algunas leyes en favor de Pompeyo. Sin embargo, dichas leyes fueron vetadas por Catón y se produjeron luchas callejeras entre ambos bandos. Después de su complicado año como pretor, César fue nombrado propretor de Hispania Ulterior.

El gobierno de César en la provincia de Hispania no se encuentra bien documentado; se sabe que lideró una pequeña y rápida guerra en el norte de Lusitania que quizá le proporcionara algo de botín para saldar parte de las deudas generadas en su gestión como edil, y ganarse un buen crédito como líder castrense. Sin duda, el éxito militar fue importante, ya que el Senado le concedió un triunfo.

César abandonó su provincia antes incluso de la llegada de su sustituto y marchó a Roma con celeridad. Al llegar al Campo de Marte tuvo que detenerse a la entrada de la ciudad, —pues aún ostentaba el "imperium"— hasta haber celebrado el triunfo. Ante la imposibilidad de entrar en Roma, se instaló en la Villa Pública y se apresuró en presentar su candidatura al consulado por persona interpuesta o bien mediante una misiva al senado, pues no hay constancia de que este se reuniera "extra-pomerium", es decir "fuera del pomerio", para escuchar la petición. Tras demorarse un día, parecía que el Senado no tendría problemas en validarla.

Catón, portavoz de la facción "optimate" más conservadora, era reacio a que un político "popular" obtuviese el consulado y más aún si este político era César, a quien detestaba, y sabiendo que se debía votar antes de la puesta del Sol, siguió hablando hasta bien entrada la noche, por lo que no se pudo aprobar la moción anterior. Ante ello, César decidió prescindir de los laureles de su triunfo y presentarse personalmente como candidato.

Tras no haber podido neutralizar la entrada de César en las elecciones, los optimates se movieron rápidamente para encontrar un candidato que equilibrase la balanza, y que perteneciera a la esfera de las ideas conservadoras, con el fin de contrarrestar las medidas que César pudiese tomar. Pompeyo mientras tanto había empezado a repartir dinero entre su clientela y votantes, gastando cuanto fuese necesario para comprar los dos consulados. Mientras, Catón eligió como candidato a su yerno Marco Calpurnio Bíbulo, quien para los optimates interpretaba el papel de salvador de la República. En las elecciones del año César fue primero con diferencia y Bíbulo quedó en segundo puesto.

Todo parecía transcurrir con naturalidad para los conservadores, que, tras bloquear políticamente a Pompeyo, y ante la perspectiva para ellos inaceptable de permitir que un hombre como César, tan sediento de gloria y con dotes militares, fuese gobernador de una provincia, iniciaron maniobras para evitarlo. Catón planteó al Senado que una vez acabado el mandato de los cónsules, y estando Italia plagada de forajidos y bandidos tan solo diez años después de la rebelión de Espartaco, sería en bien de la República encargar a los cónsules que acabaran con ellos en una misión de un año de duración. El Senado acogió favorablemente la idea, que se convirtió en ley. La voluntad de Catón se cumplió perfectamente y parecía que César terminaría su consulado como policía, entre aldeanos y pastores italianos.

Fue una decisión arriesgada, no obstante, pero al tomarla el Senado se aseguraba de que si César no la aceptaba tendría que recurrir a la fuerza para revocarla y sería declarado un criminal, un segundo Catilina. La estrategia de Catón consistió siempre en identificarse con la tradición y arrinconar a sus enemigos contra ella hasta obligarlos a tomar el papel de revolucionarios. En el Senado los aliados de los optimates liderados por Catón mantenían una mayoría sólida, contando con Craso y su poderoso bloque, pues todo el mundo esperaba que este se opusiese a cualquier medida de Pompeyo.

En la primera reunión del Senado durante el consulado de César, este trató de ofrecer un generoso acuerdo para recompensar a los veteranos de Pompeyo. Catón no estaba dispuesto a que se aprobara y empezó a utilizar su táctica favorita: habló y habló hasta que César le impidió seguir, indicándoles con un gesto de la cabeza a sus lictores que se lo llevaran. Al verlo, algunos senadores comenzaron a abandonar sus puestos; al ser interrogados por César para conocer por qué se marchaban, uno de ellos le contestó que «prefiero estar en la cárcel con Catón, que en el Senado contigo».

Ante ello, se vio obligado a rectificar, pero su retirada fue puramente estratégica: llevó la campaña de su ley agraria directamente ante los Comicios. Roma empezó a llenarse de veteranos de Pompeyo, lo que alarmó a los conservadores. Sin embargo, César podía hacer que fuera aprobada por el pueblo la propuesta con fuerza de ley, pero ir contra la voluntad del Senado era una táctica poco ortodoxa, que arruinaría su crédito entre sus colegas y su carrera habría terminado. La estrategia de César se desveló en la recta final de la votación: no sorprendió a nadie que la primera persona en hablar en favor de sus veteranos fuese Pompeyo; pero la identidad de la segunda persona que apoyó la moción fue sorpresiva: Marco Licinio Craso. Los optimates, desbordados, vieron cómo caían todas sus esperanzas. Juntos los tres hombres, podrían repartirse la República como gustasen. Los historiadores designan esta unión como el primer triunvirato, o el "gobierno de los tres hombres". Para confirmar la alianza, Pompeyo se casó con Julia, la única hija de César, y a pesar de la diferencia de edades y de ambiente social, el matrimonio fue un éxito.

Las razones por las que estas tres personalidades de la vida pública romana decidieron unirse no deben buscarse más que en los intereses de cada uno. Pompeyo necesitaba a César para que se aprobaran las leyes agrarias que dotaran de tierras a sus veteranos; Craso quería un mando proconsular que le proporcionara verdadera gloria, que no había conseguido en su represión de la revuelta de Espartaco y César necesitaba del prestigio de Pompeyo y de los fondos de Craso para poder conseguir la provincia que ansiaba. Desde luego, no debe pensarse que el acercamiento de estos tres grandes personajes de la República fuera súbito, por más que constituyera una sorpresa para sus coetáneos, fue una maniobra política de cuya existencia se dieron cuenta más bien gradualmente.

Marco Bíbulo y los conservadores que lo apoyaban iniciaron una estrategia en la retaguardia: empezaron a usar el veto para oponerse a las propuestas de César; pero César no estaba dispuesto a que no le dejaran legislar, y llevó sus proyectos directamente ante los Comicios, donde se aprobaban, entre otras cosas, por el decidido apoyo físico de los veteranos de Pompeyo. Sin embargo, cuando en un altercado algunos elementos del "populus" arrojaron una cesta de estiércol a la cabeza de Bíbulo, este optó por retirarse de toda la vida política, aunque sin renunciar a su magistratura, con el pretexto de dedicarse a la observación de los cielos en busca de presagios. Esta decisión, aparentemente de espíritu religioso, estaba destinada a impedir a César aprobar leyes durante su consulado, pero César ignoró sistemáticamente los augurios desfavorables que publicaba diariamente Bíbulo y se apoyó para la toma de decisiones en los tribunos de la plebe y en los Comicios.

Como es sabido, los romanos denominaban los años por el nombre de los dos cónsules que regían dicho período. El año , tras la nula participación de Bíbulo, fue llamado por los propios romanos (con sentido del humor) el «año de Julio y César».

Tras un año difícil como cónsul, César recibió poderes proconsulares para gobernar las provincias de Galia Transalpina (actualmente el sur de Francia) e Iliria (la costa de Dalmacia) durante cinco años, gracias al apoyo de los otros dos miembros del triunvirato, que cumplieron con la palabra dada. A estas dos provincias se añadió la Galia Cisalpina tras la muerte inesperada de su gobernador, Quinto Cecilio Metelo Céler. Eran unas provincias muy buenas para alguien que, como César, y siguiendo la típica mentalidad del procónsul romano, no tenía intenciones de gobernar pacíficamente, pues estaba necesitado de bienes para pagar las fabulosas sumas que adeudaba.
La oportunidad se le presentó mediante una teórica amenaza de los helvecios, que pensaban emigrar al oeste de las Galias. Decidido a impedirlo y con la excusa política de que se acercarían demasiado a la provincia de la Galia Cisalpina —los helvecios querían instalarse en pago Santón, al norte de la Aquitania— reclutó tropas e inició las operaciones bélicas que, a la postre, darían lugar a lo que más tarde se denominó Guerra de las Galias (58 a. C.-), en la que conquistó la llamada "Galia Comata" o Galia melenuda (actualmente Francia, Holanda, Suiza y partes de Bélgica y Alemania), en varias campañas. César hizo una demostración de fuerza construyendo por dos veces un puente sobre el Rin e invadiendo en dos ocasiones Germania sin intención de conquistarla, e hizo otro alarde de fortaleza cruzando el Canal de la Mancha también por dos veces hacia las Islas Británicas, si bien es cierto que estas dos incursiones tenían un sentido más estratégico que colonial.

Entre sus legados (comandantes de legión) se contaban sus primos Lucio Julio César y Marco Antonio, Marco Licinio Craso, hijo de su compañero de triunvirato, así como Tito Labieno, cliente de Pompeyo, y Quinto Tulio Cicerón, el hermano más joven de Marco Tulio Cicerón, todos hombres que habrían de ser personajes importantes en los años siguientes.

En materia de tácticas, Julio César usó con gran resultado lo que se conoció como "celeritas caesaris", o «rapidez cesariana» (que puede comparase, salvando las distancias, a la denominada guerra relámpago del siglo ), aparte de su genio militar tanto en batallas campales como en asedio de ciudades. Además, supo conjugar sabiamente la fuerza, la diplomacia y el manejo de las rencillas internas de las tribus galas, para separarlas y vencerlas.
César derrotó a pueblos como los helvecios en , a la confederación belga y a los nervios en y a los vénetos en Finalmente, en , César venció a una confederación de tribus galas lideradas por Vercingétorix en la batalla de Alesia. Sus crónicas personales de la campaña están registradas en sus "Comentarios a la Guerra de las Galias" ("De Bello Gallico").

De acuerdo con Plutarco, la guerra se cerró con un balance de 800 ciudades tomadas (como la de Avarico, en la cual, de los 40000 defensores, solo quedaron 800), 300 tribus sometidas, un millón de galos reducidos a la esclavitud y otros tres millones muertos en los campos de batalla. Plinio habla de 1192000 muertos y más o menos los mismos prisioneros y Veleyo Patérculo dice que murieron 400000 galos y muchos más fueron tomados prisioneros, aunque las cifras de los antiguos historiadores deben tomarse con mucha precaución, incluidas las del propio Julio César.

Utilizó en varias ocasiones la táctica de sorprender al enemigo apareciendo ante él como por ensalmo y, a despecho de los días de marcha, hacía que sus soldados se enfrentasen directamente con el adversario, pese a que este consideraba que el cansancio invalidaría el empuje de sus legiones. Fue igualmente brillante en los asedios de ciudades, llegando al culmen en el sitio de Alesia, en donde ordenó construir una doble línea de fortificaciones de varios kilómetros de extensión, para blindarse frente a los casi trescientos mil galos que intentaban ayudar a los ochenta mil soldados de Vercingetórix asediados, a los que César tenía acosados dentro de la plaza fuerte. César, con menos de cincuenta mil efectivos correspondientes a diez legiones nunca completas tras ocho años de guerras en las Galias, venció a unos y a otros en la misma batalla en la que se decidió el destino de los galos.

Pero a pesar de sus éxitos y de los beneficios que la conquista de Galia llevó a Roma, César continuaba siendo impopular entre sus pares, en particular entre los conservadores que temían su ambición.

En el , el triunvirato se tambaleaba, pues Pompeyo no se fiaba de Craso y creía que era el que mantenía en la sombra a Clodio y sus secuaces, que estaban sembrando la violencia en Roma. Ante esta situación, que amenazaba su proconsulado, César convocó a una reunión a sus dos aliados en la ciudad de Lucca, pues él no podía ir a Roma sin renunciar a su "imperium". Al parecer, a dicho encuentro no solo asistieron ellos sino unos doscientos senadores (las dos terceras partes del Senado); en este concilio se acordó que tanto Pompeyo como Craso se presentaran al consulado al año siguiente y que, una vez cónsules, promulgarían una ley por la que el proconsulado de César se alargaría cinco años más. Este pacto se conoce en la Historia como el «Convenio de Lucca». Al año siguiente, como era de prever, sus aliados Cneo Pompeyo Magno y Marco Licinio Craso fueron elegidos cónsules y honraron el acuerdo establecido con César.

Sin embargo, en , Julia murió durante un parto, dejando al padre y marido muy apenados. Marco Licinio Craso, por su parte, murió en el en la batalla de Carrhae, frente a los partos, durante la desastrosa campaña de Persia, condenada al fracaso desde el inicio por una pésima planificación. Todavía en la Galia, César trató de asegurarse la alianza con Cneo Pompeyo Magno proponiéndole matrimonio con una de sus sobrinas, pero este prefirió casarse de nuevo con Cornelia, hija de Quinto Cecilio Metelo Escipión, perteneciente a la facción optimate.

El desastre de la batalla de Carrhae en la que Craso murió con sus legiones al enfrentarse a los partos y la muerte de Julia acabó por romper el triunvirato. Días después, tras la victoria de César en Alesia, Celio, como tribuno, lanzó una propuesta de ley adicional: César sería dispensado de la obligación de acudir a Roma para presentar su candidatura al consulado. Esta medida suponía que los opositores y enemigos de César que pretendían procesarle por los supuestos crímenes de su primer consulado perderían toda posibilidad de juzgarle, puesto que César en ningún momento dejaría de desempeñar una magistratura. Mientras fuese procónsul, César tendría inmunidad judicial, pero si se veía obligado a entrar en Roma para presentarse al consulado perdería su cargo y, durante un tiempo, podría ser atacado con toda una batería de demandas de sus enemigos.

El poder de César era visto por muchos senadores conservadores como una amenaza. Si César regresaba a Roma como cónsul, no tendría problemas para hacer que se aprobaran leyes que concediesen tierras a sus veteranos, y a él una reserva de tropas que superase o rivalizase con las de Pompeyo. Catón y los enemigos de César se opusieron frontalmente, con lo que el Senado se vio envuelto en largas discusiones sobre el número de legiones que debería de tener bajo su mando y sobre quién debería ser el futuro gobernador de la Galia Cisalpina e Iliria.

Pompeyo finalmente se decantó por favorecer a los tradicionalistas y emitió un veredicto claro: César debía abandonar su mando la primavera siguiente, cuando faltaban todavía meses para las elecciones al consulado, tiempo más que suficiente para juzgarle. Sin embargo, en las siguientes elecciones para tribuno de la plebe fue elegido Curión, que se reveló como cesariano, y vetó todos los intentos de apartar a César de su mando en las Galias. Jurídicamente, todos los intentos consulares de apartar a César de sus tropas se veían anulados por la "tribunicia potestas".

A finales del mismo año, César acampó en Rávena con la XIII legión. Pompeyo tomó el mando de dos legiones en Capua y empezó a reclutar levas ilegalmente, un acto que, como era predecible, aprovecharon los cesarianos en su favor. César fue informado de las acciones de Pompeyo personalmente por Curión, que en esos momentos ya había finalizado su mandato. Mientras tanto, su puesto de tribuno fue ocupado por Marco Antonio, que lo desempeñó hasta diciembre.

Pero cuando el Senado le contestó definitivamente impidiéndole concurrir al consulado y poniéndole en la disyuntiva de licenciar a sus legiones o ser declarado enemigo público, comprendió que, escogiera la alternativa que escogiera, se entregaba inerme en manos de sus enemigos políticos. El 1 de enero de , Marco Antonio leyó una carta de César en el Senado, en la cual el procónsul se declaraba amigo de la paz. Tras una larga lista de sus muchas gestas, propuso que tanto él como Pompeyo renunciaran al mismo tiempo a sus mandos. El Senado ocultó este mensaje a la opinión pública.

Metelo Escipión dictó una fecha para la cual César debería haber abandonado el mando de sus legiones o considerarse enemigo de la República. La moción se sometió inmediatamente a votación. Solo dos senadores se opusieron, Cayo Escribonio Curión y Marco Celio Rufo. Marco Antonio, como tribuno, vetó la propuesta para impedir que se convirtiera en ley. Tras el veto de Marco Antonio a la moción que obligaba a César a abandonar su cargo de gobernador de las Galias, Pompeyo notificó no poder garantizar la seguridad de los tribunos. Antonio, Celio y Curión se vieron forzados a abandonar Roma disfrazados como esclavos, acosados por las bandas callejeras.

El 7 de enero, el Senado proclamó el estado de emergencia y concedió a Pompeyo poderes excepcionales, nombrándole "cónsul sine collega". Catón y Marcelo instaron al Senado a que pronunciara la famosa frase que equivalía a dictar la ley marcial, e instaron a Pompeyo a trasladar inmediatamente sus tropas a Roma. La crisis había llegado a su punto álgido.

En vista del cariz que tomaban los acontecimientos, César arengó a una de sus legiones, la decimotercera, y les explicó a sus componentes la situación preguntándoles si estaban dispuestos a enfrentarse con Roma en una guerra donde serían calificados de traidores en caso de perderla. Los legionarios respondieron a la arenga de su general con la decisión de acompañarlo.

Entre el 7 y el 14 de enero de —muy probablemente el 10 de enero—, César recibió la noticia de la concesión de los poderes excepcionales a Pompeyo, e inmediatamente ordenó que un pequeño contingente de tropas cruzara la frontera hacia el sur y tomara la ciudad más cercana. Al anochecer, junto con la Legio XIII Gemina, César avanzó hasta el Rubicón, la frontera entre la provincia de la Galia Cisalpina e Italia y, tras un momento de duda, dio a sus legionarios la orden de avanzar. Algunas fuentes han sugerido que fue entonces cuando pronunció el famoso: "Alea iacta est" («La suerte está echada»).

Cuando los "optimates" conocieron la noticia, abandonaron la ciudad declarando enemigo de Roma a todo aquel que se quedase en ella. Luego, marcharon hacia el sur, sin saber que César estaba acompañado solo por su decimotercera legión. César persiguió a Pompeyo hasta el puerto de Brundisium en el sur de Italia, con alguna esperanza de poder rehacer su alianza, pero éste se replegó hacia Grecia con sus seguidores. Entonces, hubo de tomar una decisión: o perseguía a Pompeyo hasta Grecia, dejando sus espaldas desguarnecidas y expuestas a un ataque por parte de las legiones pompeyanas establecidas en Hispania o, dejando organizarse a Pompeyo en Grecia, se dirigía a Hispania para asegurar su retaguardia.

Tras ponderar la situación, César se dirigió a Hispania en una marcha forzada de apenas 27 días, para derrotar a los seguidores de Pompeyo en esa poderosa provincia. Allí había establecidas varias legiones al mando de legados pro-pompeyanos, a lo que había que añadir que la generalidad de las poblaciones autóctonas habían jurado fidelidad al propio Pompeyo (que seguía siendo Procónsul de esa provincia). Tras varias escaramuzas y batallas, César se midió contra sus enemigos en la batalla de Ilerda, cerca de la actual Lérida, donde los derrotó definitivamente.

Solo cuando consideró segura la retaguardia, y después de organizar las instituciones políticas en Roma, que había caído en la anarquía, César se dirigió a Grecia. El 10 de julio de , César fue derrotado en la batalla de Dirraquium. Sin embargo, Pompeyo no supo o no pudo aprovechar esta victoria para acabar con César, y este consiguió huir con su ejército casi intacto para luchar en otro momento. El encuentro final se dio poco después, el 9 de agosto, en la batalla de Farsalia. César obtuvo una victoria aplastante, gracias a un ardid táctico. Sin embargo, sus enemigos políticos consiguieron huir: Cneo Pompeyo Magno partió hacia Rodas y de ahí a Egipto; Quinto Cecilio Metelo Escipión y Marco Porcio Catón marcharon hacia el norte de África.

De regreso a Roma, fue nombrado dictador, con Marco Antonio como "Magister equitum", y fue, junto a Publio Servilio Vatia Isáurico como colega júnior, electo cónsul por segunda vez.
En , César se dirigió a Egipto en busca de Pompeyo, pero le sorprendió el hecho de que el viejo aliado y enemigo había sido asesinado el año anterior. Al saber de su suerte, César quedó apenado por su asesinato y por haber perdido la oportunidad de ofrecerle su perdón. Tal vez debido a esto y a los intereses de Roma en Egipto, César decidió intervenir en la política egipcia y substituyó al rey Ptolomeo XIII de Egipto, que ya tenía la dignidad de faraón, por su hermana Cleopatra que creía más afín a Roma. Durante su estancia, quemó sus naves para evitar que las usaran en su contra, lo que parece que provocó el incendio de un almacén de libros anexo a la Biblioteca de Alejandría. César tuvo un romance con la reina de Egipto y de la relación parece que nació un niño, el futuro Ptolomeo XV de Egipto (Cesarión), que sería el último faraón de Egipto, si bien César nunca llegó a reconocerlo oficialmente como hijo suyo.

Después de las campañas de Egipto, César se dirigió al Asia Menor, donde derrotó rápidamente a Farnaces rey del Ponto en la batalla de Zela, tras la que pronunció la famosa frase: "Veni, vidi, vici". Después se dirigió al norte de África para atacar a los líderes de la facción conservadora allí refugiados. En la batalla de Tapso en , César obtuvo una victoria más y vio desaparecer a dos de sus más encarnizados enemigos: Quinto Cecilio Metelo Escipión y Marco Porcio Catón. Pero los hijos de Pompeyo, Cneo y Sexto Pompeyo, así como su antiguo legado principal en las Galias, Tito Labieno, consiguieron huir a las provincias de Hispania.

César regresó a Roma a finales de julio de La victoria total de su facción dotó a César de un poder enorme y el Senado se apresuró a legitimar su victoria nombrándolo dictador por tercera vez en la primavera del , por un plazo sin precedentes de diez años.

En septiembre, celebró sus triunfos, ofreciendo cuatro desfiles triunfales que se desarrollaron entre el día 21 de septiembre y el día 2 de octubre. Galos, egipcios, asiáticos y africanos desfilaron encadenados ante la multitud, mientras jirafas, carros de guerra britanos y batallas en lagos artificiales dejaban boquiabiertos a sus conciudadanos. La guerra entre romanos fue enmascarada por las victorias contra extranjeros y las celebraciones no tuvieron precedentes en sus dimensiones y duración.
Durante las celebraciones fue ejecutado ritualmente Vercingetórix, que había permanecido en una cárcel de plata desde su captura después de Alesia; en ese mismo desfile, se rompió el eje de su carroza y estuvo a punto de caer al suelo. El desfile triunfal contra Farnaces II, contó con una carroza que portaba el lema «"Veni, vidi, vici"».

César no olvidó recompensar a sus tropas, y así entregó a cada legionario cinco mil denarios, equivalentes a lo que ganarían en los 16 años de servicio obligatorio, a cada centurión, diez mil y a cada tribuno y prefecto, veinte mil denarios. Además les asignó también terrenos, aunque no cercanos a Roma, para no despojar a ciudadanos y establecer así colonias romanas en territorios recientemente conquistados. Distribuyó al pueblo diez modios de trigo por cabeza y otras tantas libras de aceite con 300 sestercios, en cumplimiento de una antigua promesa que le había hecho, a los cuales agregó 100 más por la demora. Rebajó el alquiler de las casas: en Roma hasta la suma de 2000 sestercios, en el resto de Italia hasta quinientos. A todo ello añadió la distribución de carnes, y después del triunfo sobre Hispania dos festines públicos, y no considerando el primero bastante digno de sus magnificencias, el que ofreció cinco días después fue mucho más abundante. Dio también espectáculos de varios tipos, incluyendo combates de gladiadores y comedias en todos los barrios de la ciudad, que desempeñaron actores de todas las naciones y en todos los idiomas. Juegos en el circo, atletas y una naumaquia completaron el programa.

En el Foro, combatieron entre los gladiadores Furio Leptino, en cuya familia figuraban pretores, y Quinto Calpeno, que había formado parte del Senado y defendido causas delante del pueblo. Los hijos de muchos príncipes de Asia y de Bitinia bailaron la pírrica. El ciudadano romano Décimo Liberio representó en los juegos un mimo de su composición, recibiendo quinientos mil sestercios y un anillo de oro y pasó después desde la escena, por la orquesta, a sentarse entre los "equites".
En el Circo se ensanchó la arena por ambos lados; abrieron alrededor un foso, que llenaron de agua, y jóvenes nobilísimos corrieron en aquel recinto cuadrigas y bigas, o saltaron en caballos adiestrados al efecto. Niños divididos en dos bandos, según la diferencia de edad, ejecutaron los juegos llamados troyanos. Se dieron 5 días de combates de fieras, y finalmente se dio una batalla entre dos ejércitos: cada uno comprendía 500 infantes, 30 jinetes y 20 elefantes. Con objeto de dejar a las tropas mayor espacio, habían quitado las barreras del circo, formando a cada extremo un campamento.

Durante 3 días lucharon atletas en un estadio construido expresamente en las inmediaciones del Campo de Marte. Se hizo un lago en la Codeta menor (un lugar del otro lado del Tíber) y allí trabaron combate naval: birremes, trirremes, cuatrirremes, figurando dos flotas, una tiria y otra egipcia, cargadas de soldados. El anuncio de estos espectáculos había atraído a Roma a una gran cantidad de forasteros, cuya mayor parte durmió en tiendas de campaña, en las calles y las plazas, y muchas personas, entre ellas dos senadores, fueron aplastadas o asfixiadas por la multitud.

En el invierno del año , estalló una nueva rebelión en Hispania, liderada por los hijos de Pompeyo. Usando la antigua influencia de su padre y los recursos de la provincia, los hermanos Pompeyo y Tito Labieno consiguieron reunir un nuevo ejército de trece legiones compuestas por los restos del ejército constituido en África, las dos legiones de veteranos, una legión de ciudadanos romanos de Hispania, y el alistamiento de la población local. A finales del tomaron el control de casi toda Hispania Ulterior, incluyendo las colonias romanas de Itálica y de Corduba, la capital de la provincia. César, ante el peligro, regresó a Hispania y tras algunas escaramuzas, los derrotó finalmente en la batalla de Munda.

Mención aparte merece la actividad constructiva de César, que durante su dictadura emprendió numerosos proyectos de reforma de los edificios públicos de Roma y creó otros muchos nuevos, en general en torno al campo de Marte y el nuevo complejo del Foro. Cabe destacar entre ellos, el Foro Julio o Foro de César, construido en 46 a. C. en las pendientes del Capitolio y finalizado por Augusto; en el centro de la plaza se alzaba la estatua ecuestre de César, ante el templo de su divina antepasada, Venus Genetrix, obra destacada igualmente. En dicho templo se encontraba la estatua de la diosa, instalada en el ábside del templo, y que era obra de Arcesilas, cuyos bocetos alcanzaban según Plinio precios astronómicos.

Debe señalarse que no está históricamente demostrado que la intención de César fuera proclamarse rey; y, de haber querido serlo, no puede saberse qué tipo de rey, si un "rex" a la manera etrusca, como lo habían sido Servio Tulio o Lucio Tarquinio Prisco, uno a semejanza del faraón egipcio o, simplemente, al estilo de los "Basileus" helénicos. Lo cierto es que un análisis ponderado de los hechos, según han llegado de las fuentes, parece indicar que pensaba en instaurar un régimen autocrático de algún tipo, o, al menos, lo pensaban en las esferas más cercanas a él.
César, después de vencer tras el último intento de los pompeyanos, dirigido por Cneo Pompeyo, hijo de Pompeyo Magno, se mostró desconfiado, pensando en la posibilidad de un inminente intento de asesinato. Muestra de ello es que en diciembre del año 45 a. C., en vísperas de las Saturnales, fue a pasar unos días con el suegro de Gayo Octavio (su sobrino nieto) en la residencia que este poseía cerca de Puteoli (hoy Pozzuoli) e hizo que lo acompañara una escolta de 2000 hombres.

Cicerón, cuya villa colindaba con la de Lucio Marcio Filipo, había pedido a César que le hiciera el honor de cenar con él. El dictador aceptó. Los sucesos de aquella noche quedaron registrados en una célebre carta de Marco Tulio Cicerón a Tito Pomponio Ático. Según Cicerón, César llegó a la villa acompañado de toda la guardia. Tres salones especiales recibieron al séquito de César. La cena fue un gran éxito. "Como él [César] se había purgado", precisa Marco Tulio Cicerón, "bebió y comió con tanto apetito como energía". César se mostró conversador brillante e ingenioso. "Por otra parte", añade su anfitrión, "ni una palabra de asuntos serios. Conversación enteramente literaria". Al día siguiente, 20 de diciembre, partió a Roma.

El Senado había aprovechado la ausencia de César para votar en bloque los decretos relativos a los honores que le eran conferidos. "Así", explica Dión Casio, "esta labor no debía parecer el resultado de una coacción, sino la expresión de su libre voluntad". Cuando César estaba ya de regreso en Roma, antes de colocar los decretos a los pies de Júpiter Capitolino como era tradicional, los senadores decidieron presentárselos personalmente. De este modo, se subrayaba aún más la importancia del homenaje que el Senado le rendía.

César estaba en el vestíbulo del templo de Venus Genetrix, ocupado en discutir los planos de los trabajos que los arquitectos y artistas habían venido a someterle. Cuando se le anunció que el Senado "in corpore" había venido a verlo, precedido de los magistrados en ejercicio y de una multitud de ciudadanos de diversos rangos, hizo como que no le daba importancia alguna y continuó, sin interrumpirla, la conversación con sus colaboradores.
Uno de los senadores se adelantó para pronunciar un discurso apropiado a las circunstancias. Entonces César se volvió hacia él y se preparó a escucharlo, sin dignarse siquiera a levantarse de su asiento. Probablemente, se trataba de poner en evidencia su disgusto con la afrenta que le infligió el tribuno Aquila tres meses antes. Asimismo, su respuesta dejó anonadados a los senadores: En vez de alargar la lista de honores a él acordados, insistió más bien en reducirlos... Pero no obstante los aceptó. Esta actitud produjo una tremenda indignación entre los miembros del Senado y en la multitud que asistió a esta solemnidad.

César no se limitó a aceptar las distinciones honoríficas con las que lo había colmado el Senado, sino que, al mismo tiempo supo apoderarse de múltiples prerrogativas de un carácter más realista que le permitieron reunir en sus manos la totalidad del poder gubernamental. Exigió y obtuvo que todos sus actos fuesen ratificados por el Senado, los funcionarios públicos fueron obligados a prestar juramento, desde su entrada en funciones, de no oponerse jamás a medida alguna emanada de él y se hizo atribuir los privilegios de los tribunos de la plebe, con lo que obtuvo la "tribunicia potestas" y la inmunidad sacrosanta que los distinguía.

Como consecuencia, el Senado perdía su poder, permaneciendo como una asamblea consultiva que aprobaba resoluciones, resoluciones que el dictador podía pasar por alto, sin dar siquiera una explicación para hacerlo. En lo sucesivo sería César quien tendría el derecho exclusivo de disponer de las finanzas del estado, y quien prepararía la lista de los candidatos al consulado y demás magistraturas.

Así, de hecho, ya poseía todos los poderes de un monarca. No le faltaba más que el título. A este respecto, empezó una propaganda insinuante emprendida por ciertos agentes para preparar a la opinión pública, que era muy hostil a la idea de volver a la monarquía. Sus enemigos esperaban poder arruinarlo más fácilmente explotando su ambición y se organizaron para actuar. Como resultado, seguiría una guerra solapada pero implacable.

Esta comenzó cuando la estatua de oro que acababa de ser erigida de César en los rostra, fue coronada con una diadema portando una cintilla blanca, distinción de la realeza. Se trataba de una primera tentativa, todavía muy discreta, de sondear el terreno y simular un deseo popular en favor de la coronación de César como rey. Dos tribunos del pueblo ordenaron arrancar la diadema y lanzarla lejos, hecho esto simularon erigirse en defensores de la reputación cívica de César.

En los últimos días de enero tenían lugar en el Monte Albano, en las cercanías de Roma, las tradicionales fiestas latinas. César estaba llamado a asistir bien como Pontífice Máximo o como dictador. Optó por esta última calidad, lo cual le permitía, usando el privilegio que le había concedido el Senado, figurar en estas ceremonias vistiendo la toga púrpura y calzando las altas botas rojas. Al concluir las fiestas, César hizo su entrada en Roma a caballo. En medio de la multitud que lo esperaba, y desde que se le vio aparecer, resonaron aclamaciones, escuchándose voces que lo saludaban con el título de rey, quizá provenientes de satélites debidamente aleccionados. Inmediatamente el partido opuesto intervino y se escucharon exclamaciones de protesta. César salvó la situación respondiendo: «Mi nombre es César y no Rex», lo cual, en rigor, podría interpretarse como que él solo veía en los saludos de que era objeto una alusión a su parentesco con la gens "Marcci Reges", a la que pertenecía su madre.
Otro acto estaba previsto para el 15 de febrero, día de las fiestas Lupercales. Para asistir a ellas César usó el mismo ropaje que había usado en las fiestas latinas y ocupó un sitial de oro sito en medio de la tribuna de las arengas, delante del cual debía pasar la procesión conducida por Marco Antonio. Junto al dictador se situó el cuerpo de magistrados en ejercicio: su jefe de caballería Marco Emilio Lépido, los pretores, los ediles... Mientras desfilaba delante de la tribuna el colegio de sacerdotes Julianos, uno de ellos, Licinio, apareció a nivel del estrado y depositó a los pies de César una corona de laurel entrelazada con la cintilla de la diadema real, momento en que estallaron los aplausos. Entonces Licinio subió a la tribuna y puso la corona sobre la cabeza de César que hizo un gesto de protesta y se dirigió a Lépido para que lo ayudara, pero este no hizo nada.

Gayo Casio Longino, se adelantó y, quitando la corona de la cabeza de César, la puso sobre sus rodillas, pero César la rechazó. En el último minuto, Marco Antonio trató de componer las cosas. Escaló los rostra, se apoderó de la corona y la colocó de nuevo sobre la cabeza del dictador, pero César esta vez se quitó él mismo la corona y la arrojó lejos de sí. Esto le valió los aplausos de la multitud, pero algunos espectadores le pidieron que aceptara la ofrenda del pueblo. Marco Antonio aprovechó el momento para recoger el emblema, tratando de ceñírselo de nuevo y se escucharon gritos de "¡Salud, oh rey!", pero con ellos se mezclaban protestas indignadas. César se quitó la corona y ordenó llevarla al templo de Júpiter «donde será mejor colocada», y requirió al redactor de los actos públicos que hiciera constar allí «que habiéndole ofrecido el pueblo la realeza de manos del cónsul, él la había rechazado».

Mientras tanto, se recurrió a los libros sibilinos que, habiendo sido consumidos por las llamas en tiempos de Lucio Cornelio Sila, habían sido reemplazados desde entonces por copias espurias. Los encargados de la custodia de dichos libros anunciaron que ciertos pasajes dejaban entender que los ejércitos romanos no obtendrían la victoria sobre los partos en la guerra que iba a comenzar de un momento a otro, hasta que estuviesen mandados por un rey. Pronto circuló en Roma el rumor de que en la próxima sesión del Senado, que debía tener lugar el 15 de marzo, el quindecenviro Lucio Aurelio Cota, tío del dictador, tomaría la palabra para proponer que fuese conferido el título de rey a su sobrino.

César, poco antes de su muerte, había proyectado dos campañas militares: una contra el reino dacio de Berebistas y otra contra el Imperio parto de Orodes II. No había dudas de que ambos pueblos representaban un peligro potencial para el poderío romano,sin embargo, no puede olvidarse que una guerra de conquista de tal magnitud nacía más por un deseo de dominar el mundo. Ahora César se sentía invencible y deseaba emular a Alejandro Magno conquistando el Oriente. También podía estar motivado simplemente en la búsqueda de venganza por la derrota y muerte de Craso. Así es como dos historiadores de la Antigüedad describían su ambición:

Ya en el otoño del año César comenzó con intensos preparativos para la guerra y a establecer su control político sobre los tribunos, ya que se esperaba una larga ausencia del dictador. En Apolonia de Iliria se estaba concentrando un enorme ejército de dieciséis legiones y diez mil jinetes (en total unos noventa mil hombres) y se esperaba que la campaña comenzaría en la primavera del , tres días después del Idus de marzo. Con la muerte de César el proyecto se canceló, aunque Marco Antonio intentaría continuarlo sin éxito años más tarde y sería en parte completado por Trajano con las guerras dacias y con la anexión de Mesopotamia.

No es posible saber con certeza qué condiciones fueron las que llevaron a un grupo de senadores a pensar en el asesinato de César. Los intentos de establecer un régimen autocrático tal vez tuvieron mucho que ver, pero no se puede descartar que hubiera otras motivaciones no tan nobles.

El solo hecho de que un número relativamente alto de senadores estuviera dispuesto a participar en el complot y a matar a César en el propio senado —lo que constituía un sacrilegio— da muestra del estado de cosas al que se había llegado.

Los últimos acontecimientos acaecidos y, en particular, el rumor de lo que se preparaba para el 15 de marzo en el Senado, motivaron que lo que quedaba de la facción "optimate" y, entre ellos, Gayo Casio Longino, decidiesen pasar a la acción. Gayo Casio Longino se dirigió a algunos hombres en los que creía poder confiar, y que a su juicio compartían su idea de dar muerte al dictador librando así a Roma del destino que él creía que le esperaba: un nuevo imperio cosmopolita, dirigido desde Alejandría.

Sin embargo, Gayo Casio Longino no era probablemente el hombre adecuado para ser la cabeza visible de este tipo de acción, y se acordó tantear a Marco Junio Bruto, considerado como el personaje indicado para este papel.

Se especula que, tras una serie de reuniones, ambos estaban de acuerdo en que la libertad de la República estaba en juego, pero no tenían los mismos puntos de vista de cómo actuar; Marco Junio Bruto no pensaba asistir al Senado el día 15, sino que abogaba por la protesta pasiva (la abstención); pero Gayo Casio Longino le replicó que como ambos eran pretores, podían obligarlos a asistir. Entonces respondió Bruto: «En ese caso, mi deber será, no callarme, sino oponerme al proyecto de ley, y morir antes de ver expirar la libertad». Gayo Casio Longino rechazó de lleno esta solución, pues entendía que no era dándose muerte cómo se iba a salvar la República, y lo exhortó a la lucha, a pasar a la acción. Su elocuencia terminó por convencer a su interlocutor.

El nombre de Marco Junio Bruto atrajo varias adhesiones valiosas, no en vano se decía descendiente de aquel otro Bruto (Lucio Junio Bruto) que había dirigido la expulsión del último rey de Roma, Tarquinio el Soberbio en ; entre otras adhesiones a la trama, se produjo la de Décimo Junio Bruto Albino, un familiar del dictador, en quien este tenía entera confianza. En total, el número de los conjurados parece haber sido de unos sesenta. Durante las reuniones preliminares se elaboró un plan de acción. Se decidió por unanimidad atentar contra César en pleno Senado. De este modo, se esperaba que su muerte no pareciera una emboscada, sino un acto para la salvación de la patria, y que los senadores, testigos del asesinato, inmediatamente declararían su solidaridad. Los planes de los conjurados no solamente preveían el asesinato de César, sino que además deseaban arrastrar su cadáver al Tíber, adjudicar sus bienes al Estado y anular sus disposiciones.

Hay que tener en cuenta que las motivaciones de los magnicidas eran muy heterogéneas, ya que los había movidos por un auténtico sentido de salvación de la República y estos se les habían unido otras personas movidas por el rencor, la envidia, o por la idea de que si César acaparaba las magistraturas, a ellos no les tocaría nunca llegar al poder.

También se debe señalar que muchos de los conspiradores eran ex pompeyanos reconocidos, a los que César había perdonado la vida y la hacienda, incluso confiando en ellos para la administración del Estado (Casio y Bruto fueron gobernadores provinciales, nombrados por César).

En los idus de marzo del año , un grupo de senadores, pertenecientes a la conspiración arriba citada, convocó a César al Foro para leerle una petición, escrita por ellos, con el fin de devolver el poder efectivo al Senado. Marco Antonio, que había tenido noticias difusas de la posibilidad del complot a través de Servilio Casca, temiendo lo peor, corrió al Foro e intentó parar a César en las escaleras, antes de que entrara a la reunión del Senado.

Pero el grupo de conspiradores interceptó a César justo al pasar al Teatro de Pompeyo, donde se reunía la curia romana, y lo condujo a una habitación anexa al pórtico este, donde le entregaron la petición. Cuando el dictador la comenzó a leer, Tulio Cimber, que se la había entregado, tiró de su túnica, provocando que César le espetara furiosamente "Ista quidem vis est?" «¿Qué clase de violencia es esta?» (no debe olvidarse que César, al contar con la sacrosantidad de la "tribunicia potestas", y, por ser "Pontifex Maximus", era jurídicamente intocable). En ese momento, el mencionado Casca, sacando una daga, le asestó un corte en el cuello; el agredido se volvió rápidamente y, clavando su punzón de escritura en el brazo de su agresor, le dijo: «¿Qué haces, Casca, villano?», pues era sacrilegio portar armas dentro de las reuniones del Senado.

Casca, asustado, gritó en griego "ἀδελφέ, βοήθει"!, ("adelphe, boethei"! = «¡Socorro, hermanos!»), y, en respuesta a esa petición, todos se lanzaron sobre el dictador, incluido Marco Junio Bruto. César, entonces, intentó salir del edificio para recabar ayuda, pero, cegado por la sangre, tropezó y cayó. Los conspiradores continuaron con su agresión, mientras aquel yacía indefenso en las escaleras bajas del pórtico. De acuerdo con Eutropio y Suetonio, al menos 60 senadores participaron en el magnicidio. César recibió 23 puñaladas, de las que, si creemos a Suetonio, solamente una, la segunda recibida en el tórax, fue la mortal.

Las últimas palabras de César no están establecidas realmente, y hay una polémica en torno a las mismas, siendo las más conocidas:
Tras el asesinato, los conspiradores huyeron, dejando el cadáver de César a los pies de una estatua de Pompeyo, donde quedó expuesto por un tiempo. De allí, lo recogieron tres esclavos públicos que lo llevaron a su casa en una litera, de donde Marco Antonio lo recogió y lo mostró al pueblo, que quedó conmocionado por la visión del cadáver. Poco después los soldados de la decimotercera legión, tan unida a César, trajeron antorchas para incinerar el cuerpo de su querido líder. Luego, los habitantes de Roma, con gran tumulto, echaron a esa hoguera todo lo que tenían a mano para avivar más el fuego.

La leyenda cuenta que Calpurnia, la mujer de César, después de haber soñado con un presagio terrible, advirtió a César de que tuviera cuidado, pero César ignoró su advertencia diciendo: "«Sólo se debe temer al miedo»". En otras se cuenta cómo un vidente ciego le había prevenido contra los Idus de Marzo; llegado el día, César le recordó divertido en las escaleras del Senado que aún seguía vivo, a lo que el ciego respondió que los "idus" no habían acabado aún.

Las consecuencias de la muerte de César son numerosas, y no se limitan a la guerra civil posterior. El nombre «César», por ejemplo, se convirtió en común a todos los emperadores posteriores, debido a que Augusto, de nombre Cayo Octavio, al ser adoptado oficialmente por el dictador cambió su nombre por el de Cayo Julio César Octaviano; dado que todos los emperadores posteriores a Augusto hasta Nerón fueron adoptados, el "cognomen" César acabó siendo una especie de título más que un nombre, y, así, desde Vespasiano en adelante los emperadores lo ostentaron como tal sin haber sido adoptados por la familia César. Tanto prestigio acumuló el "cognomen" que de César provienen los apelativos káiser y zar.

Muchas de sus iniciativas quedaron en suspenso a su muerte, entre ellas:







En el lugar de la cremación de su cadáver se construyó un altar que serviría de epicentro para un templo a él dedicado, pues en el año el Senado le deificó con el nombre de Divino Julio ("Divus Iulius"), acción que se convertiría en costumbre a partir de ese momento, con lo que todos los emperadores desde Augusto fueron deificados a su muerte. Esta práctica es la que, al parecer, inspiró las últimas palabras de Vespasiano, que al sentirse morir parece ser que dijo ""creo que me estoy convirtiendo en dios"".

Después de la muerte de César, estalló una lucha por el poder entre su sobrino-nieto César Augusto, a quien en su testamento había nombrado heredero universal, y Marco Antonio, que culminaría con la caída de la República y el nacimiento de una especie de Monarquía, que se ha dado en denominar "Principado", con lo que la conspiración y el magnicidio se revelaron a la postre inútiles, ya que no impidieron el establecimiento de un sistema autocrático.

Según el historiador latino Suetonio, César sedujo a numerosas mujeres a lo largo de su vida y sobre todo a aquellas pertenecientes a la alta sociedad romana.

Según el autor, César habría seducido a Postumia, esposa de Servio Sulpicio Rufo, a Lolia (Lollia), esposa de Aulo Gabinio y a Tértula (Tertulla), esposa de Marco Licinio Craso. También parece haber frecuentado a Mucia, esposa de Pompeyo.
Asimismo, César mantuvo relaciones con Servilia, madre de Bruto, a la que parecía apreciar especialmente. Así, Suetonio refiere los distintos regalos y beneficios que ofreció a su amada, de los cuales destaca una magnífica perla con un valor de seis millones de sestercios. El amor de Servilia hacia César era conocido públicamente en Roma y su relación duró desde que se conocieron en hasta la muerte del general en 

La inclinación de César hacia los placeres del amor también ha sido confirmada por los versos cantados por sus soldados con ocasión de su triunfo en Roma por las campañas en la Galia, referidos por Suetonio: 

César mantuvo relaciones amorosas con Eunoë, esposa de Bogud, rey de Mauritania.

Sin embargo, su relación más famosa fue con Cleopatra VII. Suetonio cuenta que César remontó el Nilo con la reina egipcia en una nave provista de cabinas; y habría atravesado así todo Egipto y penetrado hasta Etiopía, si el ejército no se hubiese negado a seguirlos. La hizo ir a Roma colmándola de honores y de presentes. Para él era un buen modo de sujetar Egipto, donde quedaban presentes tres legiones, y cuyo papel en el aprovisionamiento de cereales para Italia empezaba a ser preponderante. Sea como fuere, Cleopatra estuvo presente en Roma en el momento del asesinato de César y volvió rápidamente a su país después del crimen.

Según Francisco Pina Polo, César «desarrolló una política de reconciliación nacional basada en la clemencia ("clementia") para con sus enemigos», y señala que «en la práctica, no hubo proscripciones ni confiscaciones masivas, sino que, por el contrario, César perdonó abiertamente a destacados pompeyanos —entre ellos Cicerón—». 

La labor de gobierno de César, como cónsul y como dictador, fue muy amplia, pese a que el tiempo en que realmente estuvo en el poder fue relativamente corto. Sin embargo, y como bien señala Adrian Goldsworthy, un análisis detallado de cada medida o posible medida que tomó sería excesivamente extenso, pues su obra legal fue ardua; aun así, podemos hacernos una idea de su trabajo en este campo por la lista de disposiciones legales que se encuentra en Suetonio y otros autores:



















Indiscutiblemente, uno de los aspectos más reconocidos de la personalidad de Julio César es, sin duda, su genio militar. Este genio fue puesto a prueba muchas veces a lo largo de su accidentada vida castrense, y César respondió a los retos casi siempre con innovaciones tácticas o añagazas que sorprendieron a sus contrarios y que le hicieron ganar ventajas en un terreno u otro.

Según Suetonio, César era un auténtico soldado, que compartía con sus "milites" las fatigas de la guerra; era experto en las armas y en equitación. También sabemos que era un general valiente,que dirigía sus tropas desde el propio frente de batalla, para que su ejemplo infundiera valor en los soldados, y era proclive a las arengas y mantenedor de una férrea disciplina. Sin embargo, sus soldados lo veneraban y fueron muy raros los casos de deserción, quizá debido al carácter magnánimo de César. También montaba un caballo de nombre Genitor que nació en los establos que el general tenía en su casa. El caballo presentaba atavismo en las patas, por lo que tenía varios dedos largos rematados en pezuña además de casco central, algo causado por la desactivación del gen inhibidor que impide el crecimiento de más dedos en los caballos aparte del tercero durante el desarrollo embrionario.

Para ofrecer una visión lo más amplia posible de la capacidad táctica de César se ha elegido ofrecer breves reseñas de algunas de sus batallas; quizá no las más representativas o fundamentales, pero sí de las que supusieron alguna innovación táctica o una muestra de cómo César dirigía sus tropas: la batalla de Bibracte como ejemplo de batalla contra fuerzas no romanas, la batalla de Alesia como ejemplo de asedio, la batalla de Farsalia como ejemplo de lucha entre romanos, la batalla de Ruspina por la manera en la que se convirtió de una derrota casi segura en una retirada ordenada, y la batalla de Tapso en África, que supuso la derrota de las fuerzas pompeyanas establecidas en esa provincia y, a la larga, la muerte de Catón y otras figuras señeras de la oposición a César.

En el año , César acababa de tomar posesión de su cargo de procónsul de la Galia, cuando fue advertido de que una confederación de pueblos germánicos, compuesta por los helvecios, los boios y los tulingios, habían decidido dejar sus tierras ancestrales y emigrar a la Galia Comata.

Ambas fuerzas coincidieron en las cercanías de la localidad de Bibracte, donde César había tomado posiciones en lo alto de una colina. Contaba con cuatro legiones veteranas, las VII, VIII, IX y X, que ordenó formar en "triplex acies" al pie de la subida; las legiones XI y XII, de novatos, y los auxiliares fueron desplegados bajo una elevación del terreno en la cima.
Las fuerzas helvecias, quizá unos 77 000 guerreros si hemos de creer al propio César en sus "Comentarii", avanzaron hacia los romanos en una formación que César describe como «una falange», lo que quiere decir que probablemente formaban una masa compacta que se agrupaba tras los escudos, no una formación de tipo macedonio.

Cuando la formación helvecia se encontró al alcance adecuado, o sea unos 15 metros, de las filas romanas salió la primera salva de pila. Esta jabalina pesada estaba diseñada para retorcerse al clavarse en el escudo, dejando así al guerrero atacante la opción de portar un pesado escudo con una jabalina clavada que dificultaba su manejo, o deshacerse del escudo y luchar sin protección.

La lluvia de "pila" tuvo el efecto de deshacer la formación de los helvecios, por lo que los romanos aprovecharon para cargar, amparados tras sus escudos, con sus gladius, aprovechando el desnivel y corriendo colina abajo; sin escudos y mal armados, los helvecios fueron obligados a retroceder hasta una colina que se hallaba como a un kilómetro y medio.

Las legiones los siguieron, confiando en una rápida victoria, cuando, de pronto, aparecieron en el campo de batalla los boios y los tulingios, en cantidad de unos 15 000 guerreros, amenazando el flanco derecho del ejército romano. El flanco derecho era el más peligroso, pues era el que no portaba escudo, que se llevaba en el brazo izquierdo.

Cogidos así entre la espada de los helvecios, que al ver aparecer a sus aliados se lanzaron al ataque con ánimo renovado, y a la pared de los boios y tulingios, César ordenó que la tercera línea de la "triplex acies" rotara hacia la derecha, colocándose en ángulo recto de cara a los nuevos atacantes, mientras que las fuerzas restantes, formadas en "duplex acies" hacían frente al renovado ataque de los helvecios.

Faltos del factor sorpresa en que habían confiado, peor armados que los romanos y los helvecios ya cansados por la lucha, fueron arrasados por las legiones.

La innovación táctica de César fue la rapidez en que, calculando el problema, había convertido la tradicional disposición legionaria en "triplex acies" en una formación novedosa, con un frente en "duplex acies", que se encargó de frenar a los helvecios, y uno en "simplex acies", que contuvo el ataque por el flanco y, eventualmente, le llevó a ganar la batalla.

Alesia estaba situada en la cima de una colina rodeada por valles y ríos y contaba con importantes defensas. Dado que un asalto frontal sobre la fortaleza sería suicida, César consideró mejor forzar un asedio de la plaza para rendir a sus enemigos por hambre. Considerando que había cerca de 80 000 hombres fortificados dentro de Alesia junto con la población civil, el hambre y la sed forzarían rápidamente la rendición de los galos. Para garantizar un bloqueo perfecto César ordenó la construcción de un perímetro circular de fortificaciones. Los detalles de los trabajos de ingeniería se encuentran en los "Comentarios a la Guerra de las Galias" ("De Bello Gallico") de Julio César y han podido ser confirmados por las excavaciones arqueológicas en la zona. Se construyeron muros de 18 km de largo y 4 metros de alto con fortificaciones espaciadas regularmente en un tiempo récord de 3 semanas. Esta línea fue seguida hacia el interior por dos diques de cuatro metros y medio de ancho y cerca de medio metro de profundidad. El más cercano a la fortificación se llenó de agua procedente de los ríos cercanos. Asimismo, se crearon concienzudos campos de trampas y hoyos frente a las empalizadas con el fin de que su alcance fuese todavía más difícil, más una serie de torres equipadas con artillería y espaciadas regularmente a lo largo de la fortificación.

La caballería de Vercingetórix a menudo contraatacaba los trabajos romanos para evitar verse completamente encerrados, ataques que eran contestados por la caballería germana que volvió a probar su valía para mantener a los atacantes a raya. Tras dos semanas de trabajo, parte de la caballería gala pudo escapar de la ciudad por una de las secciones no finalizadas. César, previendo la llegada de tropas de refuerzo, mandó construir una segunda línea defensiva exterior protegiendo sus tropas. El nuevo perímetro era de 21km, incluyendo cuatro campamentos de caballería. Esta serie de fortificaciones les protegería cuando las tropas de liberación galas llegasen: ahora eran sitiadores preparándose para ser sitiados.

Por estos tiempos, las condiciones de vida en Alesia iban empeorando cada vez más. Con los 80 000 soldados y la población local había demasiada gente dentro de la fortaleza para tan escasa comida.
Esto provocó que parte de la población civil saliera de la ciudad y pidiera víveres al ejército de César, e incluso trabajar como esclavos en las construcciones, antes que morir de hambre. César, advirtiendo su propia falta de víveres en caso de alimentarlos, los envió de vuelta a la ciudad, y cuando esto pasó, Vercingétorix ordenó cerrar las puertas de las murallas, ya que había demasiada población para la escasez de víveres. Esta parte de la población civil murió a los pocos días.

A finales de septiembre las tropas galas, dirigidas por Commio, acudieron en refuerzo de los fortificados en Alesia, y atacaron las murallas exteriores de César. Vercingetórix ordenó un ataque simultáneo desde dentro. Sin embargo, ninguno de estos intentos tuvo éxito y a la puesta del sol la lucha había acabado. Al día siguiente, el ataque galo fue bajo la cobertura de la oscuridad de la noche, y lograron un mayor éxito que el día anterior. César se vio obligado a abandonar algunas secciones de sus líneas fortificadas. Solo la rápida respuesta de la caballería, dirigida por Marco Antonio y Gayo Trebonio, salvó la situación. La pared interna también fue atacada, pero la presencia de trincheras, los campos plantados de ""lirios"" y de ""ceppos"", que los hombres de Vercingetórix tenían que llenar para avanzar, les retrasaron lo suficiente como para evitar la sorpresa. Para entonces, la situación del ejército romano también era difícil.

El día siguiente, el 2 de octubre, Vercasivellauno, un primo de Vercingetórix, lanzó un ataque masivo con 60 000 hombres, enfocado al punto débil de las fortificaciones romanas, que César había tratado de ocultar hasta entonces, pero que había sido descubierto por los galos. El área en cuestión era una zona con obstrucciones naturales en la que no se podía construir una muralla continua. El ataque se produjo combinando las fuerzas del exterior con las de la ciudad: Vercingetórix atacó desde todos los ángulos las fortificaciones interiores. César confió en la disciplina y valor de sus hombres, y ordenó mantener las líneas. Él personalmente recorrió el perímetro animando a sus legionarios.
La caballería de Labieno fue enviada a aguantar la defensa del área en donde se había localizado la brecha de las fortificaciones. César, con la presión incrementándose cada vez más, se vio obligado a contraatacar la ofensiva interna, y logró hacer retroceder a los hombres de Vercingetórix. Sin embargo, para entonces la sección defendida por Labieno se encontraba a punto de ceder. César tomó una medida desesperada, tomando 13 cohortes de caballería (unos 6000 hombres) para atacar el ejército de reserva enemigo (unos 60 000) por la retaguardia. La acción sorprendió tanto a atacantes como a defensores.

Viendo a su general afrontar tan tremendo riesgo, los hombres de Labieno redoblaron sus esfuerzos. En las filas galas pronto empezó a cundir el pánico, y trataron de retirarse. Sin embargo, como solía ocurrir en la antigüedad, un ejército en retirada desorganizada es una presa fácil para la persecución de los vencedores, y los galos fueron masacrados. César anotó en sus Comentarios que solo el hecho de que sus hombres estaban completamente exhaustos salvó a los galos de la completa aniquilación.

En Alesia, Vercingetórix fue testigo de la derrota del ejército exterior. Enfrentándose tanto al hambre como a la moral, se vio obligado a rendirse sin una última batalla. Al día siguiente, el líder galo presentó sus armas a Julio César, poniendo fin al asedio de Alesia y a la conquista romana de la Galia.

Después de haber sido derrotados en la batalla de Dyrrachium, los cesarianos se enfrentaron definitivamente en batalla campal a Pompeyo y sus aliados en las cercanías de Farsalia.

César tenía con él a las legiones VII, VIII, IX, X, XI, XII y XIII muy reducidas en cuanto a fuerza, pues probablemente no estaban compuestas por más de 2750 legionarios cada una de ellas, y, además las legiones VIII y IX, que habían sostenido el frente de batalla en Dyrrachium y habían quedado seriamente mermadas, a las que se les dio la orden de que actuaran como una sola y se protegieran una a la otra; además, contaba con un pequeño contingente de caballería. En el otro lado, Pompeyo dirigía una fuerza de once legiones, posiblemente de 4000 hombres cada una, y una caballería de 7000 jinetes, junto con un fuerte destacamento de arqueros y honderos. 

Ambos generales formaron sus ejércitos en "triplex acies", uno frente a otro, y la caballería apostada en las respectivas alas izquierdas, pues los flancos derechos de las formaciones se apoyaban en el río Enipeus, que protegía de esa manera el ala derecha. César colocó a las legiones IX y VIII en el flanco derecho, apoyadas en el río, y después fue colocando sucesivamente a la XI, XII, XIII, VI, VII y X. Pero tras la línea de caballería, ocultos tras una pequeña elevación del terreno, detrajo y colocó una cuarta fila, compuesta de seis cohortes, en sentido oblicuo a la caballería y que recibió órdenes estrictas de no moverse bajo circunstancia alguna hasta que le fuera señalado por un vexillum.

Pompeyo había formado en una sistema más clásico, con todas sus legiones por igual y la caballería apoyada por una densa formación de arqueros y honderos, colocada tras ella; sin embargo, los había dispuesto en una formación más estática, con la idea táctica de que ofrecieran un muro de contención a la infantería cesariana, pues Pompeyo había depositado sus esperanzas en la superioridad de la caballería.
La batalla, si creemos a César, se abrió con la carga suicida de un centurión primípalo, esto es, el centurión que mandaba la primera centuria de la primera cohorte, un puesto de gran prestigio. Dicho centurión, de nombre Crastino, arrastró a 120 voluntarios con él a cargar contra la líneas de Pompeyo, en las que, lógicamente, fueron arrasados.

Al ocurrir esto, las líneas primera y segunda de la formación cesariana cargaron, pero a mitad de camino pararon para coger aire al ver que las legiones de Pompeyo no contracargaban (quizá porque Pompeyo tenía la esperanza de que se cansaran previamente). Los cesarianos recompusieron sus líneas y en ese momento, Pompeyo dio orden a su caballería de cargar.

La caballería pompeyana salió al galope, dividida en sus turma individuales, seguida por los arqueros, con el fin de flanquear el ala izquierda de la formación de César, para atacar así desde la retaguardia y formar un martillo (caballería) y un yunque (infantería) para machacar a los cesarianos. La carga tuvo éxito con la caballería cesariana, que salió en desbandada.

Pero en ese momento, César ordenó a su línea escondida de seis cohortes que atacara. La caballería pompeyana se encontró con que, en vez de tomar por sorpresa por la retaguardia a las legiones cesarianas y desbaratarlas, una nueva línea de batalla se dirigía hacia ellos con ferocidad.

Las turmas que lideraban la carga entraron en pánico y huyeron, pero probablemente se toparon en su huida con los escuadrones que les seguían y que no sospechaban nada, sembrando así la consiguiente confusión. Los legionarios de César no arrojaron sus pila, sino que los usaron, por orden de su general, más como picas, enfrentándolos a la cara de los jinetes y sus caballos, aumentando de esta manera el pánico y la confusión; así, una fuerza de apenas 1650 legionarios puso en fuga a la caballería pompeyana y pudo dedicarse a destrozar a los ligeramente armados arqueros y honderos.

A continuación, se lanzaron al ataque del ahora desprotegido flanco izquierdo de los pompeyanos, apoyados en ese momento por una ataque furioso de la tercera línea de las legiones cesarianas, que, sustituyendo a las cansadas primera y segunda líneas, presionaron el frente de batalla.

Atacadas por tropas de refresco en el centro, flanqueadas por la izquierda y por la retaguardia, las tropas pompeyanas primero vacilaron y luego emprendieron una huida en toda regla, dejando en el campo a 15000 muertos, frente a los 200 de los cesarianos.

La genialidad de César fue prever que Pompeyo iba a usar su caballería para atacar, que la suya propia no tenía la fuerza para resistirla, y arbitrar un método completamente novedoso con la línea de 6 cohortes, tendiendo una celada a su enemigo, en la que cayó, y que le sirvió para ganar la batalla y destrozar a las principales fuerzas de los pompeyanos.

Tras Farsalia, una buena cantidad de tropas pompeyanas y de señaladas figuras de la facción, como Catón el Joven, Quinto Cecilio Metelo Pío Escipión Nasica Corneliano y el antiguo legado principal de César en las Galias, Tito Labieno, se replegaron a la provincia de África, para reorganizarse y plantar cara de nuevo al dictador; corría el año 46 a. C.

Este les persiguió, y después de desembarcar, fijó sus reales en Ruspina, cerca de la actual Al Munastir. Tras una serie de peripecias, salió en busca de trigo con una fuerza de 30 cohortes armadas «a la ligera», o sea, unos 13000 hombres más o menos, dos mil jinetes y ciento cincuenta arqueros.

Súbitamente, a unos cinco kilómetros del campamento, los exploradores de César le avisaron de que se aproximaba una gran fuerza de infantería hacia ellos: eran tropas pompeyanas al mando de Labieno. Consciente de su inferioridad, César ordena a su exigua caballería y a los pocos arqueros que tenía que salieran del campamento y le siguieran a corta distancia.

Mientras César estaba colocando a sus hombres, que dada la exigüidad de esta fuerza "expedicionaria", iban formados en "simplex acies" con la caballería en alas, Labieno desplegó sus fuerzas, que resultaron estar constituidas en su inmensa mayoría por caballería y no por infantería. Fue una hábil celada tendida por el comandante pompeyano, que había juntado al máximo sus líneas, intercalando una numerosa tropa de infantería ligera númida entre los jinetes para dar ese efecto desde la distancia.
Mientras los pompeyanos avanzaron en una línea simple de extrema longitud, César había desplegado sus tropas a fines de no verse flanqueado por las de su enemigo. Pero esto fue precisamente lo que ocurrió: mientras las pocas tropas de caballería luchaban en vano para no ser superadas, el centro de la formación de César se vio golpeado por la masa de la caballería pompeyana y la infantería ligera númida, que atacaban y se retiraban sucesivamente.

La infantería cesariana respondió como pudo, pero empezó a disgregarse. Al verlo, César ordenó que ningún soldado se alejara más de cuatro pasos de su unidad. Pero la superioridad numérica del enemigo, la escasa caballería cesariana, los heridos y caballos perdidos, hicieron que la formación de César empezara a colapsarse. En ese momento, César ordenó a sus tropas que adoptaran una formación defensiva, denominada "orbis" (literalmente: orbe), básicamente una formación en círculo que tenía como misión la de no ofrecer el flanco al enemigo.

Pero se encontró rodeado por todos lados por las tropas, mucho más numerosas y móviles, de Labieno —en un lejano eco de la desastrosa batalla de Cannas—, y algunos de sus más recientes reclutas comenzaron a fallar; ante ello César tomó una decisión: ordenó extender la línea de batalla en orden cerrado tan lejos como fuera posible. Esta maniobra fue siempre altamente desaconsejada por los tácticos romanos porque llevaba excesivo tiempo llevarla a cabo; sin embargo, esta vez las fuerzas de César lo hicieron rápidamente y una vez que se hallaron desplegadas en una sola línea, César dio otra orden: que cada cohorte par diera un paso atrás y se enfrentaran de cara a su enemigo, con lo que consiguió transformar la "simplex acies" en una "duplex acies".

En ese momento, la caballería cesariana apareció para romper definitivamente el círculo, forzando a los pompeyanos a formar dos líneas de batalla separadas por las tropas cesarianas. Entonces, los sorprendidos pompeyanos se vieron sometidos a una lluvia de "pila" por parte ambos lados de la formación contraria, lo que provocó que vacilaran y se echaran atrás una distancia, no lo suficientemente grande como para disgregarse, pero sí para que César ordenara la vuelta al campamento en orden de batalla.

Mientras volvían a sus reales, los pompeyanos se vieron reforzados por la inesperada llegada de una fuerza de 1600 jinetes y un gran número de infantes, al mando de Marco Petreyo y Gneo Pisón, que hizo que atacaran de nuevo con renovadas fuerzas, rodeando otra vez a los cesarianos, pero ahora desde más lejos a fines de que César no volviera a repetir la maniobra, y lanzando sobre sus tropas una lluvia de armas arrojadizas. Las tropas de César se pararon y, ante la avalancha, quizá formaron un «"testudo"» (tortuga), una formación en la que los legionarios se cubrían con los escudos.

A medida que las tropas pompeyanas se iban quedando sin jabalinas y que su energía combativa disminuía frente a la cerrada formación de César, este se dio cuenta de que llegaba el momento de romperla y atacar súbitamente, por lo que cursó órdenes de que a una señal suya, se levantara el muro de escudos para dejar pasar a unas cohortes selectas, que adoptando la formación "en cuña" golpearon a las tropas pompeyanas. Del relato de la "Guerra de África" no queda claro si este ataque se produjo en varios puntos determinados o fue un ataque masivo sobre un solo punto, pero lo cierto es que tuvo el efecto deseado y las tropas pompeyanas se abrieron, dejando expedito el paso a César y a sus hombres que se retiraron en formación hacia su campamento, donde se fortificaron.

Lo verdaderamente genial de esta batalla no es la derrota en sí de César, sino cómo mediante una serie de decisiones tácticas y variadas formaciones de batalla, logró que lo que podría haber sido una masacre se convirtiera en una retirada organizada, en la que conservó el mayor número posible de efectivos.

Tras la batalla de Farsalia, las tropas pompeyanas se habían refugiado en la provincia de África, donde al mando de destacados miembros de la facción conservadora, como Catón el Joven y Quinto Cecilio Metelo Escipión, habían logrado reorganizarse y estaban dispuestos a continuar la lucha. Los conservadores reunieron sus fuerzas a una velocidad impresionante. Su ejército incluía 40 000 hombres (unas 10 legiones), una poderosa caballería dirigida por el que fue anteriormente la mano derecha de César en la Galia, Tito Labieno, y fuerzas aliadas de reyes locales, entre ellos el númida Juba I y 60 elefantes de guerra. César tenía consigo al menos 5 legiones, aunque no podemos saber cómo estaban de completas, y una estimable fuerza de caballería.

Tras el incidente de Ruspina, siguieron una serie de encuentros no decisivos entre las tropas de ambas facciones, pequeñas batallas para medir sus fuerzas, y durante ese tiempo dos legiones de los conservadores desertaron para unirse a César. Mientras tanto, César esperaba refuerzos de Sicilia.

A comienzos de febrero, César llegó a Tapso y puso cerco a la ciudad, bloqueando la entrada sur con tres filas de fortificaciones. Los conservadores, bajo el mando de Metelo Escipión, no podían permitirse perder esa posición, por lo que se vieron obligados a entablar batalla.
Escipión desplegó sus tropas, formando las legiones en el centro en "cuadruplex acies", puso la caballería en las alas, delante de las cuales situó la mitad de sus elefantes de guerra (treinta en el ala derecha y treinta en el ala izquierda); por detrás de las filas legionarias, puso en el ala izquierda una formación de tropas ligeras y otra mixta de caballería e infantería ligera, y en el ala derecha una mixta de caballería e infantería ligera.

César formó con las legiones en el centro, en "triplex acies",(las X y VII a la derecha y las VIII y IX a la izquierda), situó la caballería en la alas, y frente a los elefantes desplegó a sus arqueros y honderos. Pero dividió la Legio V Alaudae en dos grupos de cinco cohortes cada uno, y los situó detrás de las formaciones de arqueros y honderos.

Aunque la batalla comenzó antes de lo que César hubiera deseado, debido a la impaciencia de sus soldados del ala derecha, tomó rápidamente el mando de la situación y ordenó el ataque. Los arqueros y honderos del ala derecha dispararon sus proyectiles contra los elefantes del ala izquierda de los pompeyanos, que al recibir la lluvia de flechas y piedras, se asustaron y dieron media vuelta, cargando contra sus propias filas. En ese momento, la caballería ligera númida apostada por Escipión en esa ala, cargó hacia el frente al verse desprotegida el muro de elefantes, pero fueron desbandados por la carga de las legiones, y la "Legio X" tomó posesión del campamento pompeyano, impidiendo así la huida de los enemigos.

Sin un lugar al que volver, con las tropas en desbandada, rendidas o muertas, los líderes pompeyanos abandonaron el campo de batalla a César, con lo que dieron por perdida la guerra.

La genialidad de César en la batalla fue el movimiento táctico de colocar infantería legionaria protegiendo a los arqueros y honderos de los elefantes, y asumir con prontitud el desarrollo de la misma, usando a su favor la precipitación con la que había comenzado.

La obra escrita que llega hasta nuestros días coloca a César entre los grandes maestros de la lengua latina. Sus trabajos conocidos incluyen:

No se puede asegurar que la autoría del llamado ""Corpus Cesariano"" o ""Tria Bella"", esto es la "Guerra de Alejandría", la "Guerra de África" y la "Guerra de Hispania", sea de César y entre sus traductores existe un consenso generalizado acerca de que no fueron escritas por él, aunque sí están posiblemente basadas en sus notas.

Tanto la "Guerra de las Galias" como la "Guerra Civil", son indiscutiblemente obra de César y están escritas en un latín de gran perfección sintáctica. Ambas son prueba de la erudición de su autor y fueron usadas, sobre todo, como propaganda ante el Senado y el pueblo de Roma. En ellas hace importantísimas referencias a múltiples aspectos de la vida cotidiana en el ejército romano de la tardorrepública, de su organización, tácticas, técnicas y armamento.

Asimismo, hizo descripciones etnográficas de pueblos celtas y germanos incluyendo temas como la organización social y militar, la religión o la lengua que aún hoy en día son de obligado estudio para los expertos en las diferentes materias.

Igualmente describió lugares geográficos, como la Selva Hercinia, y describe en sus escritos importantes aspectos que permiten comprender mejor la política de la República romana de los últimos años del y a figuras como Pompeyo, Cicerón, Catón y otros.

Además se sabe que sentía curiosidad por muchos temas, desde la filosofía griega hasta la astronomía, pasando por temas sagrados o lingüísticos. Por referencias en otros autores clásicos, se sabe que César compuso un tratado de astronomía, otro de lingüística y otro más sobre augurios, pero se han perdido y no se conoce ni siquiera un párrafo de ellos.

También se sabe por Suetonio que compuso un tratado en defensa suya llamado el "Anticatón", dos libros sobre la "Analogía" y, al menos, un poema llamado "El Camino"; en su juventud escribió las "Alabanzas de Hércules", una tragedia con el título de "Edipo" y una Colección de frases selectas. Parece ser que se conservaban sus oficios al Senado, sus cartas a Cicerón y su correspondencia privada. Sin embargo, Augusto prohibió a su bibliotecario que todos estos documentos fueran copiados o publicados, por lo que acabaron perdiéndose. Se sabe que era un magnífico orador, pues tanto Plutarco como Suetonio lo mencionan, y parece ser que también Cicerón y Cornelio Nepote avalaban esta opinión. También se conoce que empleaba un latín de gran perfección.

La obra conocida de César no puede tomarse como la de un historiador moderno, pues su intención no era esa. Las obras que se conservan y cuya autoría no es discutida, esto es, los Comentarios sobre las Guerras Galas y Civil, eran un instrumento de propaganda y un informe de progresos para el Senado, no una obra como las de Tácito o Polibio, por lo que todas sus afirmaciones, en especial las políticas, deben ser analizadas desde un ánimo crítico. El hecho de que la mayor parte de la obra literaria de César se haya perdido es un inconveniente que, no por habitual en la mayoría de los autores clásicos deja de ser lamentable y que ha impedido una crítica razonada de César como autor, ya que los historiadores sólo pueden basarse en unos libros que, pese a ser de los más importantes en la Historia Occidental, no dejaban de ser más un instrumento de propaganda que un alarde de erudición.

Aun así, con todas sus limitaciones, en muchas ocasiones, sus escritos son el único testimonio antiguo que se posee sobre muchos aspectos de los pueblos, usos y costumbres de la época.

Julio César ha sido representado con frecuencia en obras literarias y cinematográficas. En la literatura, destacan:











En cuanto al cine, el personaje ha aparecido en numerosos filmes, desde la pantalla grande hasta la televisión, bien como protagonista, bien como personaje secundario.

Uno de los más reputados, tanto por la calidad de la cinta como por la de sus actores, es la película del año 1953, "Julio César", dirigida por Joseph L. Mankiewicz y cuyos papeles principales los desempeñaban Marlon Brando (como Marco Antonio), Louis Calhern (como César), Deborah Kerr (como Porcia) y James Mason (como Bruto). Con música de Miklós Rózsa, el guion es una adaptación de la obra de teatro de Shakespeare. Fue nominada a cinco Premios Óscar, de los que ganó uno.

Otro de los filmes más premiados y conocidos, en el que César es protagonista, es la cinta "Cleopatra", de 1963. Dirigida por Joseph L. Mankiewicz, con fotografía de Leon Shamroy y música de Alex North, los papeles principales fueron interpretados por Elizabeth Taylor, como Cleopatra, Richard Burton, como Marco Antonio, y Rex Harrison como César. Fue nominada a ocho premios Óscar, de los que ganó cuatro.

En el mundo del cómic, sin duda una de las más célebres representaciones de César es el personaje salido de la pluma y el pincel de René Goscinny y Albert Uderzo, antagonista (soberbio, orgulloso, pero al final, siempre justo) de su célebre personaje Astérix.

En la cuarta temporada de la serie Spartacus, Cesar es interpretado (en una versión muy libre y poco histórica) por el actor Todd Lasance.

En Netflix, la serie "El imperio romano" describe, simultáneamente como documental y como serie de televisión, los periodos de Julio César, de Marco Aurelio y de Calígula.
















 


</doc>
<doc id="4770" url="https://es.wikipedia.org/wiki?curid=4770" title="Protocolo para transferencia simple de correo">
Protocolo para transferencia simple de correo

El protocolo para transferencia simple de correo (en inglés Simple Mail Transfer Protocol o SMTP) es un protocolo de red utilizado para el intercambio de mensajes de correo electrónico entre computadoras u otros dispositivos (PDA, teléfonos móviles, impresoras, etc). Definido inicialmente en agosto de 1982 por el RFC 821 (para la transferencia) y el RFC 822 (para el mensaje). Son estándares oficiales de Internet que fueron reemplazados respectivamente por el RFC 2821 y el RFC 2822, que a su vez lo fueron por el RFC 5321 y el RFC 5322.

El funcionamiento de este protocolo se da en línea, de manera que opera en los servicios de correo electrónico. Sin embargo, este protocolo posee algunas limitaciones en cuanto a la recepción de mensajes en el servidor de destino (cola de mensajes recibidos). Como alternativa a esta limitación se asocia normalmente a este protocolo con otros, como el POP o IMAP, otorgando a SMTP la tarea específica de enviar correo, y recibirlos empleando los otros protocolos antes mencionados (POP O IMAP).

En 1982 se diseñó el primer sistema para intercambiar correos electrónicos en ARPANET, definido en los "request for comments" RFC 821 y RFC 822. La primera de ellas define este protocolo y la segunda la forma del mensaje que este protocolo debía transportar.

SMTP se basa en el modelo cliente-servidor, donde un cliente envía un mensaje a uno o varios receptores. La comunicación entre el cliente y el servidor consiste enteramente en líneas de texto compuestas por caracteres Unicode, aunque originalmente estaba compuesto por caracteres ASCII. El tamaño máximo permitido para estas líneas es de 1000 caracteres.

Las respuestas del servidor constan de un código numérico de tres dígitos, seguido de un texto explicativo. El número va dirigido a un procesado automático de la respuesta por autómata, mientras que el texto permite que un humano interprete la respuesta.

En el protocolo SMTP todas las órdenes, réplicas o datos son líneas de texto, delimitadas por el carácter <CRLF>. Todas las réplicas tienen un código numérico al comienzo de la línea.

El correo electrónico es presentado por un cliente de correo (MUA, "agente de usuario de correo") a un servidor de correo (MSA, "agente de sumisión de correo") usando SMTP a través del puerto 587. Una gran parte de los proveedores de correo todavía permiten el envío a través del puerto 25. Desde allí, el MSA entrega el correo a su agente de transferencia postal mejor conocido como el MTA (Mail Transfer Agent, "Agente de Transferencia de Correo"). En algunas ocasiones, estos dos agentes son casos diferentes aunque hay que destacar que provienen del mismo software de donde fueron lanzados sólo que presentan opciones diferentes dentro de la misma máquina.

El procesamiento local que se presenta puede ser realizado en una sola máquina o partido entre varias aplicaciones; en este segundo caso, los procesos implicados pueden compartir archivos; aquí SMTP es usado para la transferencia de mensajes internamente, con cada uno de los hosts configurados para usar la siguiente aplicación como un anfitrión elegante.
Para lograr la localización del servidor objetivo, el MTA divisorio tiene que usar el sistema de nombre de dominio (DNS) para lograr la búsqueda del registro interno de cambiado de correo conocido como registro MX para la esfera del recipiente (la parte de la dirección a la derecha). Es en ese instante cuando el registro de MX devuelto contiene el nombre del anfitrión objetivo.

Luego el MTA se une al servidor de cambio como un cliente SMTP. Una vez que MX acepta el mensaje entrante, este a su vez se lo da a un agente de entrega de correo (MDA) para luego ser llevado a la entrega de correo local. El MDA, además de entregar mensajes es también capaz de salvar mensajes en un buzón de formato, y la recepción de correo puede ser realizada usando muchas computadoras. Hay dos formas en que un MDA puede entregar mensajes: ya sea enviándolos directamente al almacenamiento, o expedirlos sobre una red usando SMTP.
Una vez entregado al servidor de correo local, dicho correo es almacenado para la recuperación de la hornada. Su recuperación se logra por medio de las aplicaciones de usuario final, conocidas como clientes de correo electrónico, usando el Protocolo de Acceso de Mensaje de Internet (IMAP), este protocolo que facilita tanto el acceso para enviar, como el manejo de correo almacenado.

Los administradores de servidor pueden elegir si los clientes utilizan TCP puerto 25 (SMTP) o el puerto 587 (Presentación) para retransmitir el correo saliente a una inicial del servidor de correo. Las especificaciones y muchos servidores soportan ambos. Aunque algunos servidores soportan el puerto 465 para el legado SMTP seguro en violación de las especificaciones, es preferible utilizar los puertos estándar y comandos ESMTP estándar de acuerdo con RFC 3207, si se debe utilizar una sesión segura entre el cliente y el servidor.

Algunos servidores están configurados para rechazar toda la retransmisión en el puerto 25, pero los usuarios válidos de autenticación en el puerto 587 pueden retransmitir correo a cualquier dirección válida.
Algunos proveedores de servicios de Internet interceptan el puerto 25, volviendo a dirigir el tráfico a su propio servidor SMTP, independientemente de la dirección de destino. Esto significa que no es posible para sus usuarios acceder a un servidor SMTP fuera de la red del ISP a través del puerto 25.

Algunos servidores SMTP soportan el acceso autenticado en otro puerto que no sea 587 o 25 para permitir a los usuarios conectarse a ellos, incluso si el puerto 25 está bloqueado, pero 587 es el puerto estándar y ampliamente apoyada por los usuarios enviar correo nuevo.
Microsoft Exchange Server 2013 SMTP puede escuchar en los puertos 25, 587, 465, 475, y 2525, en función de servidor y si los roles se combinan en un solo servidor. Los puertos 25 y 587 se utilizan para proporcionar la conectividad del cliente con el servicio de transporte en la parte delantera de la función de servidor de acceso de cliente (CAS). Los puertos 25, 465 y 475 son utilizados por el servicio de transporte de buzón de correo. Sin embargo, cuando la función de buzón se combina con la función de CAS en un único servidor, el puerto 2525 se utiliza por la función de buzón de SMTP desde el servicio de transporte de extremo delantero del CAS, CAS, mientras que continúa para utilizar el puerto 25. Puerto 465 es utilizado por el servicio de transporte de buzón de correo para recibir las conexiones de cliente proxy de la función CAS. Puerto 475 es utilizado por la función de buzón para comunicarse directamente con otras funciones de buzón, la transferencia de correo entre el servicio de envío de transporte de buzón de correo y el servicio de entrega de transporte buzón.

SMTP es un protocolo orientado a la conexión basado en texto, en el que un remitente de correo se comunica con un receptor de correo electrónico mediante la emisión de secuencias de comandos y el suministro de los datos necesarios en un canal de flujo de datos ordenado fiable, normalmente un protocolo de control de transmisión de conexión (TCP). Una sesión SMTP consiste en comandos originados por un cliente SMTP (el agente de inicio, emisor o transmisor) y las respuestas correspondientes del SMTP del servidor (el agente de escucha, o receptor) para que la sesión se abra y se intercambian los parámetros de la sesión. Una sesión puede incluir cero o más transacciones SMTP. Una transacción de SMTP se compone de tres secuencias de comando / respuesta (véase el ejemplo a continuación). 

Ellos son:


Puede que el servidor SMTP soporte las extensiones definidas en el RFC 1651, en este caso, la orden HELO puede ser sustituida por la orden EHLO, con lo que el servidor contestará con una lista de las extensiones admitidas. Si el servidor no soporta las extensiones, contestará con un mensaje "500 Syntax error, command unrecognized".


De los tres dígitos del código numérico, el primero indica la categoría de la respuesta, estando definidas las siguientes categorías:


Una vez que el servidor recibe el mensaje finalizado con un punto puede almacenarlo si es para un destinatario que pertenece a su dominio, o bien retransmitirlo a otro servidor para que finalmente llegue a un servidor del dominio del receptor.

En primer lugar se ha de establecer una conexión entre el emisor (cliente) y el receptor (servidor). Esto puede hacerse automáticamente con un programa cliente de correo o mediante un cliente telnet.

En el siguiente ejemplo se muestra una conexión típica. Se nombra con la letra C al cliente y con S al servidor.

Como se muestra en el ejemplo anterior, el mensaje es enviado por el cliente después de que este manda la orden DATA al servidor. El mensaje está compuesto por dos partes:

El protocolo de transferencia de correo simple (SMTP) solo se encarga de entregar el mensaje. En un ambiente común el mensaje es enviado a un servidor de correo de salto siguiente a medida que llega a su destino. El correo se enlaza basado en el servidor de destino. Otros protocolos como el protocolo de oficina de correos (POP) y el protocolo de acceso a mensaje de internet (IMAP) su estructura es para usuarios individuales, recuperación de mensajes, gestión de buzones de correo. SMTP usa una función, el procesamiento de colas de correo en un servidor remoto, permite que un servidor de correo de forma intermitente conectado a mandar mensajes desde un servidor remoto. El IMAP y el POP son protocolos inadecuados para la retransmisión de correo de máquinas de forma intermitente-conectados, sino que están diseñados para funcionar después de la entrega final.

Es una característica de SMTP que permite a un host remoto para iniciar el procesamiento de la cola de correo en el servidor por lo que puede recibir mensajes destinados a ella mediante el envío del comando TURN. Esta característica se considera insegura pero usando el comando ETRN en la extensión RFC 1985 funciona de forma más segura.

On-Demand Mail Relay (ODMR por sus siglas en Inglés) es una extensión de SMTP estandarizada en la RFC 2645 que permite que el correo electrónico sea transmitido al receptor después de que él ha sido aprobado. Usa la orden de SMTP ampliada ATRN, disponible para la direcciones de IP dinámicas. 
El cliente publica EHLO y órdenes de AUTH de servicios ODMR de correo, ODMR comienza a actuar como un cliente SMTP y comienza a enviar todos los mensajes dirigidos a un cliente usando el protocolo SMTP, al iniciar sesión el cortafuegos o el servidor pueden bloquear la sesión entrante debido a IP dinámicas. Sólo el servidor ODMR, el proveedor del servicio, debe escuchar las sesiones SMTP en una dirección de IP fija.

Muchos usuarios cuyo lenguaje base no es el latín han tenido dificultades con el requisito de correo electrónico en América. 
RFC 6531 fue creado para resolver ese problema, proporcionando características de internacionalización de SMTP, la extensión SMTPUTF8. RFC 6531 proporciona soporte para caracteres de varios bytes y no para ASCII en las direcciones de correo electrónico. El soporte del internacionalización actualmente es limitada pero hay un gran interés en la ampliación del RFC 6531.
RFC en países como en China, que tiene una gran base de usuarios en América.

Un cliente de correo electrónico tiene que saber la dirección IP de su servidor SMTP inicial y esto tiene que ser dado como parte de su configuración (usualmente dada como un nombre DNS). Este servidor enviará mensajes salientes en nombre del usuario.

En un ambiente de servidores, los administradores deben tomar medidas de control en donde los servidores estén disponibles para los clientes. Esto permite implementar seguridad frente a posibles amenazas.
Anteriormente, la mayoría de los sistemas imponían restricciones de uso de acuerdo a la ubicación del cliente, sólo estaba permitido su uso por aquellos clientes cuya dirección IP es una de las controladas por los administradores del servidor.
Los servidores SMTP modernos se caracterizan por ofrecer un sistema alternativo, el cual requiere de una autenticación mediante credenciales por parte de los clientes antes de permitir el acceso.

Mediante este sistema, el servidor SMTP relativo al ISP no permitirá el acceso de los usuarios que están fuera de la red del ISP. Específicamente, el servidor solo puede permitir el acceso de aquellos usuarios cuya dirección IP fue proporcionada por el ISP, lo cual es equivalente a exigir que estén conectados a internet mediante el mismo ISP. Un usuario móvil suele estar a menudo en una red distinta a la normal de su ISP, y luego descubrir que el envío de correo electrónico falla
porque la elección del servidor SMTP configurado ya no es accesible.
Este sistema tiene distintas variaciones, por ejemplo, el servidor SMTP de la organización sólo puede proporcionar servicio a los usuarios en la misma red, esto se hace cumplir mediante cortafuegos para bloquear el acceso de los usuarios en general a través de Internet. O puede que el servicio realice comprobaciones de alcance en la dirección IP del cliente. Estos métodos son utilizados normalmente por empresas e instituciones, como las universidades que proporcionan un servidor SMTP para el correo saliente solo para su uso interno dentro de la organización. Sin embargo, la mayoría de estos organismos utilizan ahora métodos de autenticación de cliente, tal como se describe a continuación.
Al restringir el acceso a determinadas direcciones IP, los administradores de servidores pueden reconocer fácilmente la dirección IP de cualquier agresor. Como esta representa una dirección significativa para ellos, los administradores pueden hacer frente a la máquina o usuario sospechoso.
Cuando un usuario es móvil, y puede utilizar diferentes proveedores para conectarse a internet, este tipo de restricción de uso es costoso, y la alteración de la configuración perteneciente a la dirección de correo electrónico del servidor SMTP saliente resulta ser poco práctica. Es altamente deseable poder utilizar la información de configuración del cliente de correo electrónico que no necesita cambiar.

Una de las limitaciones del SMTP original es que no facilita métodos de autenticación a los emisores, así que se definió la extensión SMTP-AUTH en RFC 2554.

A pesar de esto, el "spam" es aún el mayor problema. No se cree que las extensiones sean una forma práctica para prevenirlo. Internet Mail 2000 es una de las propuestas para reemplazarlo.

Diferentes metodologías han aparecido para combatir el "spam". Entre ellas destacan DKIM, Sender Policy Framework (SPF) y desde 2012 Domain-based Message Authentication, Reporting and Conformance (DMARC).





</doc>
<doc id="4774" url="https://es.wikipedia.org/wiki?curid=4774" title="Sistema experto">
Sistema experto

Un sistema experto, es un sistema informático que emula el razonamiento humano actuando tal y como lo haría un experto en un área de conocimiento. 

Los sistemas expertos son una de las aplicaciones de la inteligencia artificial que pretende simular el razonamiento humano, de la misma manera que lo haría un experto en un área de especialización. 

Principalmente existen tres tipos de sistemas expertos:

En cada uno de ellos, la solución a un problema planteado se obtiene:




La monitorización es un caso particular de la interpretación, y consiste en la comparación continua de los valores de las señales o datos de entrada y unos valores que actúan como criterios de normalidad o estándares.
En el campo del mantenimiento predictivo los Sistemas Expertos se utilizan fundamentalmente como herramientas de diagnóstico. Se trata de que el programa pueda determinar en cada momento el estado de funcionamiento de sistemas complejos, anticipándose a los posibles incidentes que pudieran acontecer. Así, usando un modelo computacional del razonamiento de un experto humano, proporciona los mismos resultados que alcanzaría dicho experto.

Diseño es el proceso de especificar una descripción de un artefacto que satisface varias características desde un número de fuentes de conocimiento.

El diseño se concibe de distintas formas:


Los SE en diseño ven este proceso como un problema de búsqueda de una solución óptima o adecuada. Las soluciones alternas pueden ser conocidas de antemano o se pueden generar automáticamente probándose distintos diseños para verificar cuáles de ellos cumplen los requerimientos solicitados por el usuario, esta técnica es llamada “generación y prueba”, por lo tanto estos SE son llamados de selección. En áreas de aplicación, la prueba se termina cuando se encuentra la primera solución; sin embargo, existen problemas más complejos en los que el objetivo es encontrar la solución óptima.

La planificación es la realización de planes o secuencias de acciones y es un caso particular de la simulación. Está compuesto por un simulador y un sistema de control. El efecto final es la ordenación de un conjunto de acciones con el fin de conseguir un objetivo global.

Los problemas que presentan la planificación mediante SE son los siguientes:


Un sistema de control participa en la realización de las tareas de interpretación, diagnóstico y reparación de forma secuencial. Con ello se consigue conducir o guiar un proceso o sistema.
Los sistemas de control son complejos debido al número de funciones que deben manejar y el gran número de factores que deben considerar; esta complejidad creciente es otra de las razones que apuntan al uso del conocimiento, y por tanto de los SE.

Cabe aclarar que los sistemas de control pueden ser en lazo abierto, si en el mismo la realimentación o el paso de un proceso a otro lo realiza el operador, o en lazo cerrado si no tiene que intervenir el operador en ninguna parte del mismo.
Reparación, correcta o terapia.

La reparación, corrección, terapia o tratamiento consiste en la proposición de las acciones correctoras necesarias para la resolución de un problema.
Los SE en reparación tienen que cumplir diversos objetivos, como son:
Reparación lo más rápida y económicamente posible.
Orden de las reparaciones cuando hay que realizar varias.
Evitar los efectos secundarios de la reparación, es decir la aparición de nuevas averías por la reparación.

La simulación es una técnica que consiste en crear modelos basados en hechos, observaciones e interpretaciones sobre la computadora, a fin de estudiar el comportamiento de los mismos mediante la observación de las salidas para un conjunto de entradas. Las técnicas tradicionales de simulación requieren modelos matemáticos y lógicos, que describen el comportamiento del sistema bajo estudio.

El empleo de los SE para la simulación viene motivado por la principal característica de los SE, que es su capacidad para la simulación del razonamiento de un experto humano, que es un proceso complejo.

En la aplicación de los SE para simulación hay que diferenciar cinco configuraciones posibles:


Un sistema de instrucción realizara un seguimiento del proceso de aprendizaje. El sistema detecta errores ya sea de una persona con conocimientos e identifica el remedio adecuado, es decir, desarrolla un plan de enseñanza que facilita el proceso de aprendizaje y la corrección de errores.

Los Sistemas Expertos, con su capacidad para combinar información y reglas de actuación, han sido vistos como una de las posibles soluciones al tratamiento y recuperación de información, no sólo documental. La década de 1980 fue prolija en investigación y publicaciones sobre experimentos de este orden, interés que continua en la actualidad.

Lo que diferencia a este sistema de un sistema tradicional de recuperación de información es que estos últimos sólo son capaces de recuperar lo que existe explícitamente, mientras que un Sistema Experto debe ser capaz de generar información no explícita, razonando con los elementos que se le dan. Pero la capacidad de los SE en el ámbito de la recuperación de la información no se limita a la recuperación. Pueden utilizarse para ayudar al usuario, en selección de recursos de información, en filtrado de respuestas, etc. Un SE puede actuar como un intermediario inteligente que guía y apoya el trabajo del usuario final.




</doc>
<doc id="4775" url="https://es.wikipedia.org/wiki?curid=4775" title="Máquina de Turing">
Máquina de Turing

Una máquina de Turing es un dispositivo que manipula símbolos sobre una tira de cinta de acuerdo con una tabla de reglas. A pesar de su simplicidad, una máquina de Turing puede ser adaptada para simular la lógica de cualquier algoritmo de computador y es particularmente útil en la explicación de las funciones de una CPU dentro de un computador.

Originalmente fue definida por el matemático inglés Alan Turing como una «máquina automática» en 1936 en la revista "Proceedings of the London Mathematical Society". La máquina de Turing no está diseñada como una tecnología de computación práctica, sino como un dispositivo hipotético que representa una máquina de computación. Las máquinas de Turing ayudan a los científicos a entender los límites del cálculo mecánico

Turing dio una definición sucinta del experimento en su ensayo de 1948, «Máquinas inteligentes». Refiriéndose a su publicación de 1936, Turing escribió que la máquina de Turing, aquí llamada una máquina de computación lógica, consistía en:

Una máquina de Turing que es capaz de simular cualquier otra máquina de Turing es llamada una máquina universal de Turing (UTM, o simplemente una máquina universal). Una definición más matemáticamente orientada, con una similar naturaleza "universal", fue presentada por Alonzo Church, cuyo trabajo sobre el cálculo lambda se entrelaza con el de Turing en una teoría formal de la computación conocida como la tesis de Church-Turing. La tesis señala que las máquinas de Turing capturan, de hecho, la noción informal de un método eficaz en la lógica y las matemáticas y proporcionan una definición precisa de un algoritmo o 'procedimiento mecánico'.

La importancia de la máquina de Turing en la historia de la computación es doble: primero, la máquina de Turing fue uno de los primeros (si no el primero) modelos teóricos para las computadoras, viendo la luz en 1936. Segundo, estudiando sus propiedades abstractas, la máquina de Turing ha servido de base para mucho desarrollo teórico en las ciencias de la computación y en la teoría de la complejidad. Una razón para esto es que las máquinas de Turing son simples, y por tanto amenas al análisis. Dicho esto, cabe aclarar que las máquinas de Turing no son un modelo práctico para la computación en máquinas reales, las cuales precisan modelos más rápidos como los basados en RAM.

 Alan Turing introdujo el concepto de máquina de Turing en el trabajo "On computable numbers, with an application to the Entscheidungsproblem", publicado por la Sociedad Matemática de Londres en 1936, en el que se estudiaba la cuestión planteada por David Hilbert sobre si las matemáticas son decidibles, es decir, si hay un método definido que pueda aplicarse a cualquier sentencia matemática y que nos diga si esa sentencia es cierta o no. Turing ideó un modelo formal de computador, la máquina de Turing, y demostró que existían problemas que una máquina no podía resolver.

Con este aparato extremadamente sencillo es posible realizar cualquier cómputo que un computador digital sea capaz de realizar.)

Mediante este modelo teórico y el análisis de la complejidad de los algoritmos, fue posible la categorización de problemas computacionales de acuerdo a su comportamiento, apareciendo así, el conjunto de problemas denominados P y NP, cuyas soluciones pueden encontrarse en tiempo polinómico por máquinas de Turing deterministas y no deterministas, respectivamente.

Precisamente, la tesis de Church-Turing formulada por Alan Turing y Alonzo Church, de forma independiente a mediados del siglo XX caracteriza la noción informal de "computabilidad" con la computación mediante una máquina de Turing.

La idea subyacente es el concepto de que una máquina de Turing puede verse como un autómata ejecutando un procedimiento efectivo definido formalmente, donde el espacio de memoria de trabajo es ilimitado, pero en un momento determinado sólo una parte finita es accesible.

La máquina de Turing modela matemáticamente a una máquina que opera mecánicamente sobre una cinta. En esta cinta hay símbolos que la máquina puede leer y escribir, uno a la vez, usando un cabezal lector/escritor de cinta. La operación está completamente determinada por un conjunto finito de instrucciones elementales como "en el estado 42, si el símbolo visto es 0, escribe un 1; Si el símbolo visto es 1, cambia al estado 17; en el estado 17, si el símbolo visto es 0, escribe un 1 y cambia al estado 6; etc". En el artículo original ("Sobre números computables con una aplicación al Entscheidungsproblem"), Turing no imagina un mecanismo, sino una persona a la que él llama la "computadora", quien ejecuta servilmente estas reglas mecánicas deterministas (o como Turing pone, "de una manera desganada").

Más precisamente, una máquina de Turing consta de:


Note que cada parte de la máquina — su estado y colecciones de símbolos — y sus acciones — imprimir, borrar, movimiento de la cinta — es finito, discreto y distinguible; es la cantidad potencialmente ilimitada de cinta lo que le da una cantidad ilimitada de espacio de almacenamiento.

Una máquina de Turinges un modelo computacional que realiza una lectura/escritura de manera automática sobre una entrada llamada cinta, generando una salida en esta misma.

Este modelo está formado por un alfabeto de entrada y uno de salida, un símbolo especial llamado blanco (normalmente "b", "formula_1" o "0"), un conjunto de estados finitos y un conjunto de transiciones entre dichos estados. Su funcionamiento se basa en una función de transición, que recibe un "estado inicial" y una cadena de caracteres (la cinta, la cual puede ser infinita) pertenecientes al alfabeto de entrada. La máquina va leyendo una celda de la cinta en cada paso, borrando el símbolo en el que se encuentra posicionado su cabezal y escribiendo un nuevo símbolo perteneciente al alfabeto de salida, para luego desplazar el cabezal a la izquierda o a la derecha (solo una celda a la vez). Esto se repite según se indique en la función de transición, para finalmente detenerse en un "estado final" o "de aceptación", representando así la salida.

Una máquina de Turing con una sola cinta puede definirse como una 7-tupla
donde:


Existe en la literatura un abundante número de definiciones alternativas, pero todas ellas tienen el mismo poder computacional, por ejemplo se puede añadir el símbolo formula_13 como símbolo de "no movimiento" en un paso de cómputo.

La máquina de Turing consta de un cabezal lector/escritor y una cinta infinita en la que el cabezal lee el contenido, borra el contenido anterior y escribe un nuevo valor. Las operaciones que se pueden realizar en esta máquina se limitan a:


El cómputo se determina a partir de una tabla de estados de la forma:

Esta tabla toma como parámetros el estado actual de la máquina y el carácter leído de la cinta, dando la dirección para mover el cabezal, el nuevo estado de la máquina y el valor a escribir en la cinta.

La memoria es la cinta de la máquina que se divide en espacios de trabajo denominados celdas, donde se pueden escribir y leer símbolos. Inicialmente todas las celdas contienen un símbolo especial denominado "blanco". Las instrucciones que determinan el funcionamiento de la máquina tienen la forma, "si estamos en el estado "x" leyendo la posición "y", donde hay escrito el símbolo "z", entonces este símbolo debe ser reemplazado por este otro símbolo, y pasar a leer la celda siguiente, bien a la izquierda o bien a la derecha".

La máquina de Turing puede considerarse como un autómata capaz de reconocer lenguajes formales. En ese sentido, es capaz de reconocer los lenguajes recursivamente enumerables, de acuerdo a la jerarquía de Chomsky. Su potencia es, por tanto, superior a otros tipos de autómatas, como el autómata finito, o el autómata con pila, o igual a otros modelos con la misma potencia computacional.

Las máquinas de Turing pueden representarse mediante grafos particulares, también llamados diagramas de estados finitos, de la siguiente manera:

Es una secuencia de la forma formula_15 donde formula_16 y formula_17 que escribe el estado de una máquina de Turing. La cinta contiene la cadena formula_18 seguida de infinitos blancos. El cabezal señala el primer símbolo de formula_19.

Por ejemplo, para la máquina de Turing
con las transiciones
La descripción instantánea para la cinta 1011 es:

Definimos una máquina de Turing sobre el alfabeto formula_28, donde 0 representa el símbolo blanco. La máquina comenzará su proceso situada sobre un símbolo "1" de una serie. La máquina de Turing copiará el número de símbolos "1" que encuentre hasta el primer blanco detrás de dicho símbolo blanco. Es decir, posiciona el cabezal sobre el 1 situado en el extremo izquierdo, doblará el número de símbolos 1, con un 0 en medio. Así, si tenemos la entrada "111" devolverá "1110111", con "1111" devolverá "111101111", y sucesivamente.

El conjunto de estados es formula_29 y el estado inicial es formula_30. La tabla que describe la función de transición es la
siguiente:
El funcionamiento de una computación de esta máquina puede mostrarse con el siguiente ejemplo (en negrita se resalta la posición de la cabeza lectora/escritora):
La máquina realiza su proceso por medio de un bucle, en el estado inicial formula_30, reemplaza el primer 1 con un 0, y pasa al estado formula_32, con el que avanza hacia la derecha, saltando los símbolos 1 hasta un 0 (que debe existir), cuando lo encuentra pasa al estado formula_33, con este estado avanza saltando los 1 hasta encontrar otro 0 (la primera vez no habrá ningún 1). Una vez en el extremo derecho, añade un 1. Después comienza el proceso de retorno; con formula_34 vuelve a la izquierda saltando los 1, cuando encuentra un 0 (en el medio de la secuencia), pasa a formula_35 que continúa a la izquierda saltando los 1 hasta el 0 que se escribió al principio. Se reemplaza de nuevo este 0 por 1, y pasa al símbolo siguiente, si es un 1, se pasa a otra iteración del bucle, pasando al estado s1 de nuevo. Si es un símbolo 0, será el símbolo central, con lo que la máquina se detiene al haber finalizado el cómputo.

Una razón para aceptar la máquina de Turing como un modelo general de cómputo es que el modelo que hemos definido anteriormente es equivalente a muchas versiones modificadas que en principio pareciera incrementar el poder computacional.

La función de transición de la MT sencilla está definida por
la cual puede ser modificada como
Donde formula_13 significa «permanecer» o «esperar», es decir no mover el cabezal de lectura/escritura. Por lo tanto, formula_39 significa que se pasa del estado "q" al "p", se escribe formula_40 en la celda actual y la cabeza se queda sobre la celda actual.

Esta modificación se denota al igual que una MT sencilla, lo que la hace diferente es que la cinta es infinita tanto por la derecha como por la izquierda, lo cual permite realizar transiciones iniciales como formula_41.

Es aquella que mediante la cual cada celda de la cinta de una máquina sencilla se divide en subceldas. Cada celda es así capaz de contener varios símbolos de la cinta. Por ejemplo, la cinta de la figura tiene cada celda subdividida en tres subceldas.

Se dice que esta cinta tiene múltiples pistas puesto que cada celda de esta máquina de Turing contiene múltiples caracteres, el contenido de las celdas de la cinta puede ser representado mediante "n"-tuplas ordenadas. Los movimientos que realice esta máquina dependerán de su estado actual y de la "n"-tupla que represente el contenido de la celda actual. Cabe mencionar que posee un solo cabezal al igual que una MT sencilla.

Una MT con más de una cinta consiste de un control finito con "k" cabezales lectores/escritores y "k" cintas. Cada cinta es infinita en ambos sentidos. La MT define su movimiento dependiendo del símbolo que está leyendo cada uno de sus cabezales, da reglas de sustitución para cada uno de los símbolos y dirección de movimiento para cada uno de los cabezales. Inicialmente la MT empieza con la entrada en la primera cinta y el resto de las cintas en blanco.

Una MT multidimensional es aquella cuya cinta puede verse como extendiéndose infinitamente en más de una dirección, el ejemplo más básico sería el de una máquina bidimensional cuya cinta se extendería infinitamente hacia arriba, abajo, derecha e izquierda.

En la modificación bidimensional de MT que se muestra en la figura también se agregan dos nuevos movimientos del cabezal {U,D} (es decir arriba y abajo). De esta forma la definición de los movimientos que realiza el cabezal será {L,R,U,D}.

La entrada de una máquina de Turing viene determinada por el estado actual y el símbolo leído, un par (estado, símbolo), siendo el cambio de estado, la escritura de un nuevo símbolo y el movimiento del cabezal, las acciones a tomar en función de una entrada. En el caso de que para cada par (estado, símbolo) posible exista a lo sumo una posibilidad de ejecución, se dirá que es una máquina de Turing determinista, mientras que en el caso de que exista al menos un par (estado, símbolo) con más de una posible combinación de actuaciones se dirá que se trata de una máquina de Turing no determinista.

La función de transición formula_42 en el caso no determinista, queda definida como sigue:

¿Cómo sabe una máquina no determinista qué acción tomar de las varias posibles? Hay dos formas de verlo: una es decir que la máquina es "el mejor adivino posible", esto es, que siempre elige la transición que finalmente la llevará a un estado final de aceptación. La otra es imaginarse que la máquina se "clona", bifurcándose en varias copias, cada una de las cuales sigue una de las posibles transiciones. Mientras que una máquina determinista sigue un único "camino computacional", una máquina no determinista tiene un "árbol computacional". Si cualquiera de las ramas del árbol finaliza en un estado de aceptación, se dice que la máquina acepta la entrada.

La capacidad de cómputo de ambas versiones es equivalente; se puede demostrar que dada una máquina de Turing no determinista existe otra máquina de Turing determinista equivalente, en el sentido de que reconoce el mismo lenguaje, y viceversa. No obstante, la velocidad de ejecución de ambos formalismos no es la misma, pues si una máquina no determinista "M" reconoce una cierta palabra de tamaño "n" en un tiempo formula_44, la máquina determinista equivalente reconocerá la palabra en un tiempo formula_45. Es decir, el no determinismo permitirá reducir la complejidad de la solución de los problemas, permitiendo resolver, por ejemplo, problemas de complejidad exponencial en un tiempo polinómico.

El "problema de la parada" o "problema de la detención" ("halting problem" en inglés) para máquinas de Turing consiste en: dada una MT "M" y una palabra "w", determinar si "M" terminará en un número finito de pasos cuando se ejecuta usando "w" como entrada.

Alan Turing, en su famoso artículo «On computable numbers, with an application to the "Entscheidungsproblem"» (1936), demostró que el problema de la parada de la máquina de Turing es indecidible, en el sentido de que ninguna máquina de Turing lo puede resolver.

Toda máquina de Turing puede codificarse como una secuencia binaria finita, es decir una secuencia finita de ceros y unos. Para simplificar la codificación, suponemos que toda MT tiene un único estado inicial denotado por formula_46, y un único estado final denotado formula_47. Tendremos que para una MT "M" de la forma

Todos estos símbolos se codifican como secuencias de unos:
Los estados de una MT formula_53 se codifican también con secuencias de unos:

Las directrices de desplazamiento formula_12, formula_11 y formula_13 se codifican con 1, 11, 111, respectivamente. Una transición formula_57 se codifica usando ceros como separadores entre los estados, los símbolos del alfabeto de cinta y la directriz de desplazamiento formula_12. Así, la transición formula_59 se codifica como
En general, la codificación de una transición cualquiera formula_61 es
donde formula_63, según la dirección sea formula_64.

Una MT se codifica escribiendo consecutivamente las secuencias de las modificaciones de todas sus transiciones. Más precisamente, la codificación de una MT "M" es de la forma formula_65,
donde formula_66 es la codificación de la formula_67-ésima transición de "M". Puesto que el orden en que se representen las transiciones de una MT no es relevante, una misma MT tiene varias codificaciones diferentes. Esto no representa ninguna desventaja práctica o conceptual ya que no se pretende que las codificaciones sean únicas.

Una máquina de Turing computa una determinada función parcial de carácter definido e unívoca, definida sobre las secuencias de posibles cadenas de símbolos de su alfabeto. En este sentido se puede considerar como equivalente a un programa de ordenador, o a un algoritmo. Sin embargo es posible realizar una codificación de la tabla que representa a una máquina de Turing, a su vez, como una secuencia de símbolos en un determinado alfabeto; por ello, podemos construir una máquina de Turing que acepte como entrada la tabla que representa a otra máquina de Turing, y, de esta manera, simule su comportamiento.

En 1947, Turing indicó: 

Con esta codificación de tablas como cadenas, se abre la posibilidad de que unas máquinas de Turing se comporten como otras máquinas de Turing. Sin embargo, muchas de sus posibilidades son indecidibles, pues no admiten una solución algorítmica. Por ejemplo, un problema interesante es determinar si una máquina de Turing cualquiera se parará en un tiempo finito sobre una determinada entrada; problema conocido como problema de la parada, y que Turing demostró que era indecidible. En general, se puede demostrar que cualquier cuestión no trivial sobre el comportamiento o la salida de una máquina de Turing es un problema indecidible.

El concepto de Máquina de Turing universal está relacionado con el de un sistema operativo básico, pues puede ejecutar cualquier instrucción computable sobre él.

En 1985, Deutsch presentó el diseño de la primera Máquina cuántica basada en una máquina de Turing. Con este fin enunció una nueva variante la tesis de Church-Turing dando lugar al denominado "principio de Church-Turing-Deutsch".

La estructura de una máquina de Turing cuántica es muy similar a la de una máquina de Turing clásica. Está compuesta por los tres elementos clásicos:


El procesador contiene el conjunto de instrucciones que se aplica sobre el elemento de la cinta señalado por el cabezal. El resultado dependerá del qubit de la cinta y del estado del procesador. El procesador ejecuta una instrucción por unidad de tiempo.

La cinta de memoria es similar a la de una máquina de Turing tradicional. La única diferencia es que cada elemento de la cinta de la máquina cuántica es un qubit. El alfabeto de esta nueva máquina está formado por el espacio de valores del qubit.
La posición del cabezal se representa con una variable entera.

Dos modelos matemáticos equivalentes a los de las máquinas de Turing son las máquinas de Post, creadas en forma paralela por Emil Leon Post, y el cálculo lambda, introducido por Alonzo Church y Stephen Kleene en los años 1930, y también usado por Church para demostrar en 1936 el "Entscheidungsproblem".





</doc>
<doc id="4778" url="https://es.wikipedia.org/wiki?curid=4778" title="Bélgica">
Bélgica

Bélgica (, ; , ; , ), oficialmente Reino de Bélgica (en neerlandés: "Koninkrijk België", en francés: "Royaume de Belgique" y en alemán: "Königreich Belgien"), es uno de los veintisiete estados soberanos que forman la Unión Europea. Está situado en el noroeste europeo. El país cubre una superficie de kilómetros cuadrados y posee una población de 11 409 077 habitantes según la estimación de 2016. Su capital y la conurbación más poblada es Bruselas mientras su ciudad (municipio) más poblada es Amberes.

Es un Estado multilingüístico con tres lenguas oficiales: el 57 % de su población, en la región de Flandes principalmente, habla neerlandés, mientras que cerca del 42 % habla francés (en la región de Valonia, al sur, y en la Región de Bruselas-Capital, una región oficialmente bilingüe que acoge una mayoría de hablantes de francés). Menos de un 1 % de los belgas vive en la Comunidad germanófona, donde hablan alemán, junto a la frontera al este del país. A menudo, esta diversidad lingüística lleva a severos conflictos políticos y culturales, muy parecidos a los de otros países bilingües, reflejándose en el complejo sistema de gobierno de Bélgica y en su historia política.

Bélgica recibe su nombre de la denominación latina de la parte más septentrional de la Galia, "Gallia Belgica", el cual, a su vez, procede de un grupo de tribus celtas, los belgas. Históricamente, Bélgica ha sido parte de los Países Bajos de los Habsburgo, los cuales incluían los actuales Países Bajos y el Gran Ducado de Luxemburgo, ocupando una región algo mayor que el moderno Benelux.

Desde finales de la Edad Media hasta el siglo XVII, fue un floreciente centro de comercio y cultura. Desde el siglo XVIII hasta la Revolución belga de 1830, Bélgica, en aquella época llamada los Países Bajos del Sur, fue el lugar de muchas batallas entre las potencias europeas y es por ello que se ha ganado el apodo de "el campo de batalla de Europa" o "la cabina de Europa".

Es uno de los miembros fundadores de la Unión Europea, cuyas instituciones principales están ubicadas en el país, así como un número importante de otras organizaciones internacionales, como la OTAN.

El más antiguo uso de las voces "Belga" y "Bélgica" que nos ha llegado está en el "De Bello Gallico" de Julio César. En dicho libro, el conquistador romano dividía toda la Galia en tres partes: los galos propiamente tales, los aquitanos y los belgas. Estos últimos estaban separados de los galos por los ríos Sena y Marne.
Durante el principado de Augusto, Marco Agripa dividió la Galia en tres provincias y asignándoles a una de ellas el nombre de "Gallia Bélgica". Esta última se reorganizaría durante el imperio de Domiciano, quien la dividió a su vez en tres nuevas provincias, a saber: la "Gallia Belgica" y las dos "Germanias". La "Gallia Belgica", más tarde se volvió a repartir en dos provincias: la "Belgica Prima" y la "Belgica Secunda". La actual Bélgica tiene poco que ver con estas antiguas provincias romanas, ya que la mayor parte de su territorio se ubica en parte de la histórica Germania Inferior (más tarde, "Germania Secunda") y otra parte en la "Bélgica Secunda".

Estos términos casi desaparecieron por completo después de las invasiones bárbaras, y subsistieron solo en la pluma de algunos eruditos, mayormente clérigos. Volvieron a usarse en la segunda mitad del siglo IX, después de la escisión del imperio de Carlomagno, con la creación de la Lotaringia. Los clérigos de entonces, siguiendo una práctica común que consistía en utilizar los antiguos nombres latinos, usados por el Imperio, recuperaron la palabra "Bélgica" para designar al reino de Lotario II, en lugar del término "Lotaringia", para designar al territorio situado entre la "Gallia" de Carlos el Calvo y la "Germania" de Luis el Germánico. Las denominaciones "Belgae", "Bélgica", "Gallia Bélgica" desaparecieron de nuevo en el siglo XII, después de la desaparición de la Lotaringia.

El área ocupada por Bélgica ha experimentado significativos cambios demográficos, políticos y culturales. El primero bien documentado fue la conquista de la región por la República Romana en el siglo I a. C., seguida en el siglo V por los francos germánicos. Estos establecieron el Reino Merovingio, que pasó a ser el Imperio carolingio en el siglo VIII. Durante la Edad Media, los Países Bajos estaban fragmentados en pequeños Estados feudales. La mayor parte de ellos se unió durante los siglos XIV y XV con la casa de Borgoña, formando los Países Bajos borgoñones. Estos Estados ganaron el estatuto de autonomía en el siglo XV y fueron conocidos desde entonces como las Diecisiete Provincias.
La historia de Bélgica se puede distinguir de la de los Países Bajos desde el siglo XVI. La Guerra de los Ochenta Años (1568-1648) provocó la división de las Diecisiete Provincias en las Provincias Unidas al norte y los Países Bajos del Sur al sur, siendo éstas gobernadas sucesivamente por los Habsburgo españoles y austriacos.

Hasta la independencia de Bélgica en 1830, los Países Bajos del Sur eran un territorio muy codiciado por los conquistadores, siendo el telón de fondo de la mayor parte de las guerras franco-españolas y franco-austriacas durante los siglos XVII y XVIII. Tras las campañas de 1794 de las Guerras Revolucionarias Francesas, los Países Bajos —que incluían territorios que nunca habían estado bajo dominio de los Habsburgo, como el Obispado de Lieja— fueron invadidos por Francia, terminando con el mando español y austriaco en aquella zona. En efecto, la reunificación de los Países Bajos como Reino Unido de los Países Bajos tuvo lugar a finales del Imperio francés, en 1815, tras las campañas de Napoleón.
El triunfo de la Revolución belga de 1830 para independizarse de los Países Bajos se decidió en la capital, Bruselas, en las llamadas Cuatro Jornadas de Bruselas, bajo un Gobierno Provisional cuyo miembro más influyente era Charles Rogier y, con la dirección militar como comandante en jefe del exiliado español Juan Van Halen. La breve contienda llevó al establecimiento de una Bélgica independiente, católica y neutral, bajo un gobierno provisional.

Desde la instauración de Leopoldo I como rey en 1831, Bélgica ha sido una monarquía constitucional y una democracia parlamentaria. Entre la independencia y la II Guerra Mundial, el sistema democrático evolucionó de una oligarquía caracterizada por dos partidos principales, los católicos y los liberales, a un sistema de sufragio universal que ha incluido un tercero, el Partido Socialista, y un papel fuerte para los sindicatos. En sus orígenes, el francés, que era la lengua de la nobleza y la burguesía, era la lengua oficial. Desde entonces, el país ha desarrollado un sistema bilingüe en neerlandés y francés.
En la Conferencia de Berlín de 1885 se acordó entregar el Congo al rey Leopoldo II como posesión privada, llamada Estado Libre del Congo. En 1908, se cedió a Bélgica como colonia, pasándose a llamar Congo Belga. La neutralidad de Bélgica se quebrantó en 1914, cuando Alemania invadió Bélgica como parte del Plan Schlieffen. Las antiguas colonias alemanas de Ruanda-Urundi —que ahora son Ruanda y Burundi— fueron ocupadas por el Congo Belga en 1916. La Sociedad de Naciones las transfirió a Bélgica en 1924. Bélgica fue invadida de nuevo por Alemania en 1940, durante la Blitzkrieg. Estuvo ocupada hasta el invierno de 1944-45, en que fue liberada por las tropas Aliadas. El Congo Belga accedió a la independencia en 1960, durante la Crisis del Congo, mientras que Ruanda-Urundi se independizó en 1962.

La batalla de Lieja fue el inicio de la invasión alemana de Bélgica y la primera batalla de la Primera Guerra Mundial. Ello supuso un nuevo estímulo a la identidad flamenca que comenzó a gestarse durante el siglo XIX y que recibió un impulso político por parte del Gobierno de ocupación alemán; durante la Segunda Guerra, toda la región del Benelux (Bélgica, los Países Bajos, el Luxemburgo) fue ocupada por la Alemania nazi.

Durante el siglo XX, y especialmente desde la II Guerra Mundial, la historia de Bélgica ha estado dominada cada vez más por la autonomía de sus dos comunidades principales. Este periodo ha visto un aumento en las tensiones intercomunales y la unión del Estado belga se ha puesto en cuestión. Mediante reformas constitucionales en los años 70 y 80, la regionalización del Estado unitario condujo al establecimiento de un sistema federal estructurado en tres niveles, a la creación de comunidades lingüísticas y de gobiernos regionales y a la ratificación de un acuerdo concebido para minimizar las tensiones lingüísticas. Hoy en día, estas entidades federadas sostienen más poder legislativo que el parlamento bicameral nacional, mientras que el gobierno nacional aún controla casi toda la recaudación de impuestos, cerca del 80 % de las finanzas de los gobiernos comunitarios y regionales, y el 100 % de la seguridad social.

En 1830, Bélgica se separa de Países Bajos, con la que había formado el Reino Unido de los Países Bajos durante quince años. La misma razón por la que Bélgica se independiza tiene unas bases lingüísticas. Es decir, la zona que constituye la Bélgica actual había sido dominada durante mucho tiempo por los franceses, por lo que toda la burguesía administrativa flamenca se había afrancesado. Sin embargo, cuando se une esta zona a los Países Bajos, la élite administrativa flamenca, de mayoría francófona, es destituida y reemplazada por personas neerlandófonas, generalmente venidas de Países Bajos. Por ello, ya a partir de los primeros años del Reino Unido, la élite administrativa va perdiendo la confianza en el rey y la Unión. Cuando, además, el rey empieza a promulgar medidas proteccionistas contra las industrias meridionales para fomentar el desarrollo industrial de la zona septentrional, también pierde el apoyo de la élite industrial, en su mayor parte valones, y se produce la Revolución Brabanzona.

Por aquel entonces, según estima D’Haveloose (2000), Bélgica contaba con 4 millones de habitantes, de los que más o menos 2 200 000 hablaban neerlandés y más o menos 1 700 000 hablaban francés. Sin embargo, el nuevo Estado se define a partir de sus inicios como unitario y francófono, aunque la mayoría de la población hablaba neerlandés y el país constaba de dos partes económica y culturalmente diferentes. Entonces, la política lingüística de la época no tiene que verse tanto en términos de quién constituía la mayoría de la población, sino en términos de que la élite burguesa, que controlaba la política por el sufragio tributario, se expresaba en francés.

Sin embargo, gradualmente los flamencos se van oponiendo a la injusticia lingüística, por lo que, en 1889, se adopta la Ley de la Igualdad que estipulaba que el neerlandés y el francés fueran las lenguas oficiales del país. Durante la Primera Guerra Mundial, surge el Movimiento del Frente que quería acabar con el predominio del francés y hacer de Flandes un territorio monolingüe neerlandófono, un proceso que se va llevando a cabo entre 1932 y 1968.

El año 1963 es otro momento clave, porque entonces se adoptan unas leyes que dividían el país en zonas lingüísticas. Antes, el censo contaba cuántas personas hablaban francés, neerlandés o alemán en un municipio y el municipio se organizaba o bien en la lengua de la mayoría o en ambas, lo que originaba un aumento de los pueblos francófonos y bilingües.

La división del país en unas zonas neerlandófona, germanófona y francófona era un asunto muy delicado. Surgen muchas protestas, puesto que en la frontera lingüística convivían los dos grupos lingüísticos y además en el sistema anterior los habitantes tenían el derecho a ser atendidos en las dos lenguas. Para encontrar una solución de estos problemas, en 27 municipios que están en una de las fronteras lingüísticas se sigue no aplicando el principio de territorialidad. Asimismo, como en seis pueblos flamencos alrededor de Bruselas vivían muchos francófonos, por lo que, antes, la administración era bilingüe, se les dan facilidades lingüísticas. Por tanto, en muchos aspectos, el año 1963 no representa tanto muchos cambios, sino que más bien significa la consolidación de una vez por todas del "statu quo".
Según Peiren (1993), la instauración de esas zonas se va experimentando gradualmente, en contradicción con la estructura unitaria del país, por lo que una federalización era necesaria. Además habían surgido partidos políticos flamencos nacionalistas que insistían en la cuestión lingüística, como el Volksunie. Asimismo, en el seno de los partidos políticos nacionales surgían visiones fundamentalmente diferentes, lo que lleva a la organización regional de los mismos. Al mismo tiempo, muchas personas de ambas partes del país estaban convencidas de que sería mejor para todos si ciertos aspectos de la organización estatal fueran regionalizados. Según Willemyns (2002) ese sentimiento se origina por dos factores: uno es que durante el siglo XIX y la primera parte del siglo XX es Valonia la que poseía el mayor desarrollo industrial, pero a partir de los años 50-60, Flandes también empieza a desarrollarse industrialmente. Al mismo tiempo, la infraestructura de Valonia, que databa del siglo XIX, necesitaba ser actualizada, lo que genera una recesión de la cual hasta la fecha de hoy la zona no se ha recuperado. También existían diferencias ideológicas entre ambas partes del país, es decir, mientras que Valonia era claramente socialista (Parti Socialiste), Flandes era la base del poder del partido popular católico democrático (Christelijke VolksPartij). A raíz de esas diferencias, empieza en 1970 la federalización de Bélgica, un proceso que se va llevando a cabo principalmente entre dicho año y 1993.

En 1970, la demanda principal de los flamencos era la autonomía cultural, mientras que los valones insistían en una autonomía económica para fomentar su industria y garantías de que en la Bélgica federal, su situación demográfica y económica no sería marginada. El resultado de esas negociaciones fue la creación, por una parte, de tres comunidades culturales (la de habla neerlandesa, la francófona y la de habla germana) y, por otra, de tres Regiones (Flandes, Valonia y la Región capital de Bruselas). Además, se incorporaron en la Constitución garantías para proteger a la minoría francófona. Las reformas del Estado siguientes (1980, 1988 y 1993) extiendieron las competencias de las regiones y comunidades hasta obtener la organización del Estado actual.

A partir de los años 1970, la región flamenca se convirtió en la más productiva del país, en contraste con la declinación de la Valonia, producto de la desinversión relativa de las empresas cartelizadas. La tendencia comenzó a revertise en la década de 1990, con una mayor igualdad entre ambas regiones.

Bélgica jugó un papel de primer orden en la creación de la Unión Europea a partir de la conformación del Benelux en 1944, cuya puesta en práctica comenzó en 1948, la Comunidad Europea del Carbón y del Acero en 1951 y los Tratados de Roma en 1957. 

Desde 1949, además, es parte integrante de la OTAN; como tal tuvo gran importancia durante la Guerra Fría.

Tropas belgas participaron de las operaciones militares en la antigua Yugoslavia, así como en Libia y en Afganistán.

El país sufrió los atentados de Bruselas de 2016, donde fueron atacados con bombas el aeropuerto y el metro, dejando 35 muertos y 340 heridos.

El barrio bruselense de Molenbeek-Saint-Jean fue residencia de Hassan El Haski, uno de los autores de los atentados del 11 de marzo de 2004 en Madrid.

Bélgica es una monarquía federal constitucional y parlamentaria, que tras la II Guerra Mundial evolucionó de un Estado unitario a una federación. El parlamento bicameral está formado por un Senado y una Cámara de Representantes. El primero es una mezcla de políticos mayores elegidos directamente y de representantes de las comunidades y las regiones, mientras que la última representa a todos los belgas mayores de dieciocho años en un sistema de representación proporcional. Bélgica es uno de los pocos países en donde votar es obligatorio, y por ello tiene una de las tasas más altas de participación electoral del mundo.

El gobierno federal, nombrado formalmente por el rey, debe tener la confianza de la Cámara de Representantes. Está encabezado por el primer ministro. Los números de ministros hablantes de francés y de neerlandés debe ser iguales, tal como lo prescribe la Constitución. El rey o reina es el jefe de Estado, aunque tiene prerrogativas limitadas. El poder verdadero se les confiere al primer ministro y a los diferentes gobiernos del país. El sistema judicial está basado en el derecho civil y proviene del Código Napoleónico. El Tribunal de Apelaciones está un nivel por debajo de la Corte de Casación, una institución basada en la Corte de Casación francesa.

Las instituciones políticas de Bélgica son complejas; la mayoría de los poderes políticos están organizados alrededor de la necesidad de representar a las principales comunidades lingüísticas. (Véase más abajo) Los partidos más importantes de cada comunidad pertenecen a tres familias políticas principales: los liberales, los democristianos y los socialdemócratas. Otros partidos importantes, aunque más jóvenes, son los dos partidos Verdes ("Ecolo" y "Groen!") y, particularmente en Flandes, los partidos nacionalistas de ultraderecha. Influyen en la política varios grupos de presión, como los sindicatos y la Federación de Empresas de Bélgica.

El rey actual, Felipe, sucedió a su padre Alberto II por la abdicación de este en 2013. Las arcas públicas son las que mantienen a la familia real, se anunció que para 2014 un total de 38.742.000 euros del erario público será empleado para financiar la jefatura real del Estado, en dotaciones personales o remuneraciones que cada miembro de la familia real reciba, y las partidas que diferentes ministerios dedican a la misma. Desde 1999, el primer ministro Guy Verhofstadt, del VLD, ha encabezado una coalición de seis partidos, Liberal-Socialdemócrata-Verde, que es llamada con frecuencia "el gobierno arco iris". Este ha sido el primer gobierno sin democristianos desde 1958. Los resultados de las elecciones de 2003 permitieron a Verhofstadt realizar un segundo mandato, liderando una coalición liberal-socialdemócrata cuatripartita. En los últimos años, también se ha registrado un constante ascenso del partido flamenco separatista de ultraderecha Vlaams Blok, actual Vlaams Belang.

Un logro significativo de las dos legislaturas consecutivas de Verhofstadt ha sido el hecho de conseguir unos presupuestos equilibrados. Bélgica es uno de los pocos Estados miembros de la UE que lo ha conseguido. Durante la década de 1990, esta política se fue aplicando por los sucesivos gobiernos, bajo presión del Consejo Europeo. La caída del gobierno anterior a Verhofstadt se debió principalmente a la crisis de las dioxinas, un importante escándalo de intoxicación alimentaria en 1999, que condujo al establecimiento de la Agencia Federal para la Seguridad de la Cadena Alimentaria.

Este acontecimiento resultó en una representación inusualmente grande de los Verdes en el parlamento, y en un mayor énfasis en la política medioambiental durante el primer mandato de Verhofstadt. Una política Verde, por ejemplo, dio lugar a la legislación sobre el abandono de la energía nuclear, que ha sido modificada por el gobierno actual. La ausencia de democristianos en las filas del gobierno ha permitido a Verhofstadt abordar los asuntos sociales desde un punto de vista más liberal y desarrollar nuevas leyes sobre el uso de drogas suaves, el matrimonio del mismo sexo y la eutanasia. Durante las dos últimas legislaturas de Verhostadt, el gobierno ha promovido una diplomacia activa en África, se ha opuesto a intervenir militarmente durante la guerra de Irak, y ha aprobado una ley sobre crímenes de guerra. Ambos mandatos de Guy Verhofstadt estuvieron marcados por disputas entre las comunidades belgas. Los puntos más controvertidos fueron las rutas nocturnas del tráfico aéreo del Aeropuerto Internacional de Bruselas y la situación legal del distrito electoral de Bruselas-Halle-Vilvoorde.

La defensa del país recae en las Fuerzas Armadas de Bélgica, que tienen como misión mantener la integridad territorial del país, conservar la independencia y hacer cuidar y respetar la Constitución y las leyes. Su comandante en jefe es el ministro de Defensa, y en caso de guerra el primer ministro asume el mando.
Está dividida en tres ramas, cada una con un comandante en jefe, quienes responden ante el Ministerio de Defensa. Estas son el Ejército, la Armada y la Fuerza Aérea. Cuenta además con un cuerpo médico de sanidad integrado al ejército pero no dependiente de él, sumando un total de 39.400 efectivos.

Las Fuerzas Armadas de Bélgica responden ante la OTAN y forman parte de los Cascos Azules de las Naciones Unidas.

La ley del 15 de julio de 1993 se creó para establecer un único estado federal, basado en tres niveles:
Los conflictos entre los diferentes órganos se resuelven por el Tribunal de arbitraje. Esta disposición permite un acuerdo entre las diferentes culturas para que puedan convivir en paz.

La Comunidad Flamenca absorbió la Región flamenca en 1980 para formar el gobierno de Flandes. La superposición de los límites de las Regiones y las Comunidades ha creado dos peculiaridades notables: el territorio de la Región de Bruselas-Capital está incluido tanto en la Comunidad francesa como en la flamenca, mientras que el territorio de la Comunidad germanófona está totalmente dentro de la Región Valona. Las regiones flamenca y valona están subdivididas a su vez en entidades administrativas menores, las provincias.

El nivel más alto de esta organización de tres niveles es el gobierno federal, que dirige los asuntos exteriores, las ayudas al desarrollo, la defensa, la policía, la gestión de la economía, el bienestar social, los transportes, la energía, las telecomunicaciones y la investigación científica, además de competencias limitadas en la educación y la cultura, y la supervisión de los impuestos de las autoridades regionales. El gobierno federal controla más del 90 % de todos los impuestos. Los gobiernos de las comunidades son responsables de la promoción de la lengua, la cultura y la educación en la mayoría de las escuelas, bibliotecas y teatros.

El tercer nivel lo constituyen los gobiernos regionales, que gestionan principalmente asuntos relacionados con las tierras y las propiedades, como la vivienda, el transporte, etc. Por ejemplo, el permiso para construir el edificio de una escuela en Bruselas que perteneciese al sistema de educación pública sería regulado por el gobierno regional de Bruselas. No obstante, la escuela como institución quedaría bajo regulación del gobierno flamenco si la lengua principal de enseñanza es la neerlandesa, y bajo el gobierno de la Comunidad francesa si la lengua principal es la francesa.

El territorio de Bélgica tiene una extensión de 30 528 km² y se divide geográficamente en tres regiones: la planicie costera al noroeste, la meseta central y las altiplanicies de las Ardenas al sureste. Siguiendo el ejemplo de los Países Bajos, la planicie costera ha ganado algunos espacios del mar del Norte por medio de diques y canales. La meseta central, en el interior, es un área lisa y de poca altitud, que tiene muchos valles fértiles y es irrigada por numerosas vías navegables. Aquí también hay estructuras de un relieve más áspero, como cuevas y pequeñas gargantas.

La región de las Ardenas es más accidentada que las otras dos. Es una meseta densamente boscosa, muy rocosa y no muy apta para el cultivo, que se extiende hasta el norte de Francia. Aquí es donde se concentra la mayoría de la fauna salvaje de Bélgica. En esta región se localiza el punto más alto de Bélgica, la Signal de Botrange, con solo 694 metros de altitud.

El clima es marítimo templado, con precipitaciones significativas durante todo el año (Clasificación climática de Köppen: "Cfb"; la temperatura media es de 3 °C en enero y de 18 °C en julio, y la precipitación media es 65 milímetros en enero y 78 milímetros en julio).

A causa de su elevada densidad de población y a su posición en el corazón de Europa Occidental, Bélgica se enfrenta a serios problemas medioambientales. Un informe de 2003 indicó que el agua de los ríos de Bélgica tenía la peor calidad de Europa, y que se situaba a la cola de los 122 países estudiados.

Bélgica es un país densamente poblado y se localiza en el corazón de una de las regiones más industrializadas del mundo. Actualmente, la economía belga está orientada hacia los servicios y muestra una naturaleza dual, con una dinámica parte flamenca, siendo Bruselas su principal centro multilingüe y multiétnico con una renta per cápita de las más altas de la Unión Europea, y una economía valona más ruralizada y menos dinámica.
Bélgica fue el primer país de Europa continental en el que se desarrolló la Revolución industrial, a comienzos del siglo XIX. Lieja y Charleroi desarrollaron rápidamente una industria minera y acerera, que floreció hasta mediados del siglo XX. Sin embargo, por la década de 1840 la industria textil de Flandes estaba pasando por una aguda crisis y había hambruna (1846-50). Después de la II Guerra Mundial, Gante y Amberes experimentaron una rápida expansión del sector químico y petrolífero. Las crisis del petróleo de 1973 y 1979 causaron una prolongada recesión económica. La industria siderúrgica belga ha sufrido desde entonces por un grave retroceso, y este ha sido el responsable de inhibir el desarrollo económico de Valonia. En los años 1980 y 90, el centro económico del país continuó desplazándose hacia el norte, a Flandes.
La industria está concentrada en la poblada área flamenca del norte del país.

A finales de los años 1980, la política macroeconómica belga había dado lugar a una deuda gubernamental acumulada de aproximadamente el 120 % del PIB. Actualmente, los presupuestos están equilibrados y la deuda pública equivale al 94,3 % del PIB (finales de 2005). En 2004, se estimó la tasa de crecimiento real del PIB en un 2,7 % pero se espera que descienda a un 1,3 % en 2005.

Bélgica tiene una economía abierta. Ha desarrollado una excelente infraestructura de transportes (puertos, canales, ferrocarriles y autopistas) para integrar su industria con las de los países vecinos. Amberes es el segundo mayor puerto de Europa, por detrás del de Róterdam. Miembro fundador de la Unión Europea, Bélgica apoya la extensión de los poderes de las instituciones de las UE para integrar las economías de los estados miembros. En 1999, Bélgica adoptó el euro, la moneda única europea, que reemplazó definitivamente al franco belga en 2002. La economía belga está estrechamente orientada hacia el comercio exterior, especialmente productos de alto valor añadido. Las principales importaciones son productos alimenticios, maquinaria, diamantes, petróleo y derivados, sustancias químicas, vestimenta y accesorios y tejidos. Las exportaciones principales son automóviles, comida y productos alimenticios, hierro y acero, diamantes procesados, tejidos, plásticos, productos petrolíferos y metales no ferrosos. Desde 1922, Bélgica y Luxemburgo han constituido un único mercado comercial, con una unión aduanera y monetaria, Unión Económica Belgo-Luxemburguesa. Sus principales socios comerciales son Alemania, los Países Bajos, Francia, el Reino Unido, Italia, los Estados Unidos y España. 

En años recientes, y ante el debate surgido por su posible escisión en dos entes estatales separados, obviamente con sus consabidas y temidas consecuencias políticas y económicas; se cambió muy profundamente su sistema de beneficencia social y de protección social, así como se hicieron profundos recortes a estos, para luego redireccionarlos y adecuarlos a un escenario en donde la pérdida de competitividad de la parte valona, con un aporte al PIB del 35 %; sobre la parte flamenca; que es la más poderosa actualmente en cuanto a sus niveles económicos, aparte de un cambio de gabinete, hicieron que la situación mejorase sustancialmente, pero sin atenuar las fuertes discusiones sobre el asunto del equilibrio de poderes entre las comunidades belgas más influyentes, así como se inicia el proceso de cambio de enfoque industrial, de donde Valonia, antes destacado centro industrial siderúrgico del país belga; pasa ahora a convertirse en un centro de estudio y de investigación en tecnologías de la información y de mejoras industriales.

En cambio, la parte Flamenca, antes adormecida ante el gran potencial económico de Valonia, del que mucho tiempo dependió; ahora es el principal sustento de la nación belga actual, de donde proviene más del 60 % del PIB nacional. Con los resultados de las consultas hechas en el año 2010, se deja ver el ansia de disolución del país, pero los líderes de la parte francófona y flamenca se han puesto de acuerdo para "" dar fin"" a las especulaciones y al periodo turbio y sombrío por el que ha tenido que pasar una economía considerada ejemplar en Europa.

Más recientemente, y ante las secuelas dejadas por la crisis financiera y económica de varios países de la zona del euro, la calificación de la deuda soberana de Bélgica se ha visto amenazada por parte de las agencias de calificación de riesgos, como Moody's; que la rebajaron de "Aa2" a "Aa3", ante las sustanciales dudas dentro del mercado de inversores sobre sus planes para una sostenida y sostenible reactivacción económica de la zona euro, de la que Bélgica hace parte fundamental.

Bélgica tiene una población de 11 409 077 habitantes según la estimación de julio de 2016 y una densidad de población de 368,5 hab./km², siendo una de las más elevadas de Europa, después de los Países Bajos y de algunos microestados como Mónaco y Ciudad del Vaticano. Las áreas con mayor densidad de población son las que están alrededor de las aglomeraciones de Bruselas-Amberes-Gante-Lovaina —región conocida como el Diamante Flamenco— así como en otros centros urbanos importantes (principalmente Lieja, Charleroi, Brujas, Namur, Mons, Courtrai y Hasselt). La región de las Ardenas es la que tiene menor densidad de población del país. En 2005, la Región Flamenca tenía una población de aproximadamente 6 043 161 habitantes. La seguían Valonia con 3 395 942 y Bruselas con 1 006 749. Casi toda la población es urbana (97,3 % en 1999). Las ciudades principales (con su población entre paréntesis) son Bruselas (1 006 749 en la ciudad s.s. y unos 2 millones en su aglomeración), Amberes (457 749 en la comuna y 900 000 con su área metropolitana), Gante (230 951), Charleroi (201 373), Lieja (185 574 en el municipio y 600 000 en su aglomeración) y Brujas (117 253).

Desde la independencia, el catolicismo, aún contrarrestado por el librepensamiento y los movimientos francmasónicos, ha tenido un papel importante en la política belga. La Constitución laica permite la libertad de culto, y el Gobierno, por lo general, respeta este derecho en la práctica. En una encuesta realizada en 2001, el 47 % de la población del país se identificó católico, el 3,5 % musulmán, entre el 1,2 % y el 1,4 % protestante, el 0,7 % ortodoxo, entre el 0,4 % y el 0,5 % judío y el 0,1 % anglicano. Además, el 15 % declaró no identificarse con religión algunas y el 7,4 % se describió como laico. Una investigación llevada a cabo en 2006 en la región de Flandes —considerada más religiosa que Valonia— mostró que el 55 % de las personas se identifican como religiosos, y otro 36 % cree que algún dios es el creador del mundo.

Se estima que el 98% de la población adulta está alfabetizada. La educación es obligatoria entre los seis y los dieciocho años, pero muchos belgas continúan estudiando hasta los 23 años aproximadamente. En 1999, Bélgica tenía la tercera mayor proporción de jóvenes de 18 a 21 años matriculados en la educación superior de entre todos los países de la OCDE, con un 42 por ciento. Sin embargo, en los últimos años, el principal tema de preocupación es el analfabetismo funcional. En el periodo 1994-1998, el 18,4 por ciento de la población belga carecía de hábitos de lectura. Como reflejo de los conflictos políticos históricos entre el librepensamiento y los sectores católicos de la población, el sistema de enseñanza en cada comunidad se divide en una rama laica controlada por las comunidades, las provincias, o los municipios, y una rama religiosa —en su mayoría católica— subvencionada y controlada tanto por las comunidades como por las autoridades religiosas (en su mayoría diócesis). No obstante, cabe destacar que —al menos en el caso de las escuelas católicas— las autoridades religiosas tienen un poder muy limitado.

Las universidades y otras instituciones superiores son muy importantes, tanto por los estudiantes locales como internacionales. Entre ellas se cuentan la KU Leuven o Universidad Católica de Lovaina La Vieja fundada en 1425 —con un estudio de postgrado en castellano—, la Universidad Católica de Lovaina La Nueva (francófona), la Universidad de Amberes, la Universidad de Gante, la Universidad de Lieja y la Universidad de Namur.

Los idiomas oficiales de Bélgica son el neerlandés, el francés y el alemán. Cerca del 57 por ciento de la población de Bélgica tiene como lengua materna el neerlandés (lo conoce hasta el 70 % de la población, entre ellos un 20 % de valones), el 42 por ciento es francófona (el 70 % de la población total conoce la lengua francesa, incluido el 60 % de los flamencos), y menos del 1 por ciento es germanófona. Bruselas, con el 9 % de la población del país, es oficialmente bilingüe (francés y neerlandés).

El neerlandés y el francés que se hablan en Bélgica presentan pequeñas diferencias de vocabulario y de significado con respecto a las variedades de los Países Bajos y Francia. Si bien hoy mucha gente actualmente habla dialectos del neerlandés, la lengua valona, que antaño fuera la lengua principal de Valonia, solamente cuenta con pocos hablantes que suelen ser personas mayores. Estos dialectos, junto con otros como el picardo o el limburgués, no se usan en la vida pública. Sin embargo, el flamenco es mucho más utilizado en toda la región neerlandesa. El flamenco es hablado diariamente por la población, aunque la escuela, libros y demás se encuentran un neerlandés más estandarizado.

Según estimaciones de 2015, la esperanza de vida es de 80,88 años. Desde 1960, la esperanza de vida ha crecido un promedio de dos meses cada año, a la par con el promedio europeo. Las principales causas de muerte en Bélgica son las enfermedades cardiovasculares, neoplasias, enfermedades del aparato respiratorio y las causas no naturales de muerte (accidentes y suicidios). Las causas no naturales y el cáncer son la mayor causa de muerte en las mujeres mayores de 24 años y en los varones mayores de 44.

El sistema de salud es financiado por un sistema de seguridad social y los impuestos. El seguro de salud es obligatorio. El cuidado sanitario se realiza con un sistema en su mayor parte privatizado con médicos y hospitales independientes. La mayoría de las veces el paciente paga directamente por los servicios obtenidos y posteriormente se le rembolsa por las compañías de seguros. El sistema de salud belga es supervisado y financiado por el gobierno federal, las tres comunidades y las tres regiones, es decir, por seis ministerios distintos (la comunidad y la región de Flandes se fusionaron).

En Bélgica ha habido contribuciones al desarrollo de las ciencias y la tecnología de importancia internacional. Durante el florecimiento del siglo XVI de Europa Occidental, se puede citar entre los más influyentes científicos al cartógrafo Gerardus Mercator, al anatomista Andreas Vesalius, al botánico Rembert Dodoens y el matemático Simon Stevin.

El químico Ernest Solvay y el ingeniero Zenobe Gramme (École Industrielle de Liège) dieron sus nombres al proceso Solvay y a la "dinamo de Gramme", respectivamente, en la década de 1860. La bakelita fue desarrollada en 1907–1909 por Leo Baekeland. Ernest Solvay fue también un gran filántropo y dio su nombre al Instituto Solvay de Sociología, a la Escuela Solvay de Bruselas de Economía y Gestión y a los Institutos Internacionales Solvay de Física y Química que forman parte actualmente de la Universidad Libre de Bruselas. In 1911 Ernest Solvay comenzó una serie de conferencias, las Conferencias Solvay sobre Física y Química, que tuvieron un impacto profundo en la evolución de la física de los cuantos y en la química. Se debe también una contribución mayor a la ciencia fundamental al belga Georges Lemaître (Universidad Católica de Lovaina), al que se le atribuye la propuesta de la teoría del Big Bang sobre el origen del Universo en 1927.

Tres belgas han recibido premios Nobel en Fisiología o Medicina: Jules Bordet (Universidad Libre de Bruselas, en 1919), Corneille Heymans (Universidad de Gante, en 1938) y Albert Claude (Universidad Libre de Bruselas) junto con Christian De Duve (Universidad Católica de Lovaina), en 1974. François Englert (Universidad Libre de Bruselas) recibió el Premio Nobel de Física en 2013. Ilya Prigogine (Universidad Libre de Bruselas) recibió el Premio Nobel de Química in 1977. Dos matemáticos belgas han recibido la medalla Fields: Pierre Deligne en 1978 y Jean Bourgain en 1994.

Marc van Montagu (Universidad de Gante) descubrió el mecanismo de transferencia genética entre la bacteria Agrobacterium y ciertas plantas, que resultó en el desarrollo de métodos para convertir Agrobacterium en un sistema eficaz para crear plantas transgénicas. Van Montagu recibió el World Food Price, similar al Premio Nobel en el ámbito de la agricultura, en 2013.

Entre los escritores belgas se puede destacar a Maurice Maeterlinck (Premio Nobel de Literatura en 1911), que escribió en francés, así como Émile Verhaeren, Jean Ray, Georges Simenon, Marguerite Yourcenar o Amélie Nothomb. Entre los escritores en neerlandés el propio Jean Ray, Louis Paul Boon, o Hugo Claus.

Bélgica también ha dado pintores famosos en diversas épocas como René Magritte, Rubens, Brueghel, Van Dijck, Paul Delvaux, James Ensor, Félicien Rops o Léon Spilliaert.

A partir de 1797, Étienne Robertson, un científico y artista, presentó la linterna mágica, llamada "Fantascope". Con este aparato que permite a las sombras proyectadas cambiar de forma gracias a embriones de movimiento, presenta fantasmagorías que hacen sensación.

Los Hermanos Dardenne son un ejemplo de los muchos cineastas belgas actuales.

Bélgica y su capital Bruselas es el centro de producción más importantes de Europa en cuanto a la producción de historietas se refiere. "Comanche", "Lucky Luke", "Los Pitufos", "Tintín" ("Kuifje" en neerlandés), "Spirou y Fantasio", "Aquiles Talón" o "Casacas Azules" son algunas de las series más conocidas del cómic belga.

Una de las más famosas fiestas tradicionales es el Carnaval de Binche, cerca de Mons, celebrado antes de la Cuaresma. Durante el carnaval, la diversión y el baile son conducidos por "gilles", hombres vestidos con sombreros altos,emplumados y brillantes trajes.

Otra manifestación tradicional es el Ommegang de Bruselas, que es el recuerdo de la recepción a Carlos I a su llegada ("Joyeuse Entrée") a Bruselas desde España como emperador en 1549, que se conmemora cada año.

Otro espectáculo famoso es la procesión de la Sagrada Sangre, celebrada en Brujas en mayo.

El 6 de diciembre se celebra el día de San Nicolás, una tradición orientada a los niños, en donde se acostumbra que San Nicolás traiga dulces y algunas veces regalos a los niños que se hayan portado bien durante el año. Es parecido a lo que en otros países es Santa Claus o la llegada de los Reyes Magos (Epifanía).

Tomorrowland es un festival de música electrónica celebrado anualmente en la pequeña ciudad de Boom. Está organizado por ID&T y "Entertainment and Media Enterprise". La primera edición del festival se celebró el 14 de agosto de 2005. Se calcula que anualmente asisten 210.000 personas de 75 nacionalidades distintas. El festival tiene lugar en la ciudad de Boom, a 16 kilómetros al sur de Amberes, 32 kilómetros al norte de Bruselas.

Tomorrowland se ha convertido en el festival de música electrónica más importante del mundo.

Existen cientos de diferentes tipos de cerveza, siendo su producción considerada por muchos como un arte. Las más conocidas son las trapenses (hay seis oficiales: Achel, Chimay, Orval, Rochefort, Westmalle y Westvleteren), cervezas lambic (Kriek) y reconocidas cervezas artesanales por su alta calidad como "La Chouffe", "La Binchoise" o "Dolle Brouwers". Lo mismo ocurre con los chocolates, los más conocidos son: Godiva, Neuhaus, Cornet, Côte d'Or, Leonidas...

La gastronomía belga está muy influenciada por la cocina francesa. La cocina belga está entre las mejores de Europa: ha reinterpretado las tradiciones gastronómicas de la vecina Francia de una manera original, adaptándolas a los productos típicos de su propio territorio. La cocina belga posee características regionales que le dan una gran variedad de recetas e ingredientes. Muestras de la cocina belga son el chocolate, la cerveza, las patatas fritas, los gofres, las carnes (con razas desarrolladas en el país como la "bleu blanc belge" en bovino, o la "Pietrain" en porcino) o las coles de Bruselas.

También se dice a menudo de los belgas que es una nación de "Gourmands" en vez de "Gourmets" que se puede traducir que prefieren la «gran cocina» en vez de la "fine cuisine". En realidad esto se traduce en que es una cocina de «grandes porciones» y de gran calidad. La palabra en francés "Gourmandise" proviene originariamente de ‘glotón’, pero ha sido interpretada con otro significado en Francia (el término todavía es usado hoy en día, aunque con tintes un poco arcaicos). Es un dicho que en Bélgica se sirve la cantidad de comida de Alemania y la calidad de Francia.

Los deportes más populares en Bélgica son el ciclismo, el fútbol, el tenis y el automovilismo.

El ciclismo

El ciclismo en este país goza (junto con otros países de la zona) de una gran popularidad, siendo el ciclo-cross (una modalidad de este) el deporte más visto y practicado por los belgas. La mayoría de carreras de ciclismo en ruta profesional están celebradas en este país, sobre todo en las regiones de Flandes y Ardenas. La primera, cuenta con una gran red de pavé, un tipo de firme adoquinado muy característico, que además suele deparar mucho espectáculo. La falta de este país de montaña se compensa con los "muros" o cotas, unos tramos de carretera o adoquín bastante cortos, pero con altos porcentajes que hacen difícil su paso y también son muy habituales en las clásicas de estas zonas. En Bélgica se celebran dos de los cinco "monumentos" ciclistas de la temporada:
Además de estas importantes carreras, Bélgica cuenta con gran cantidad de carreras (la mayoría clásicas, como la Omloop Het Nieuwsblad o la Flecha Valona. En algunas ocasiones también ha sido salida del Tour de Francia, o transitada por ella, como en 2015, en la que una etapa finalizaba, en el Muro de Huy. Bélgica es también una cuna de buenos ciclistas, la mayoría especializados carreras de pavés, como el allí

Otros deportes populares

En el fútbol, el R.S.C. Anderlecht y el Club Brujas son considerados los dos clubes con más popularidad dentro del país. El R.S.C. Anderlecht, junto al RKV Malinas, son los únicos clubes nacionales con títulos internacionales (dos Recopas Europeas, dos Supercopas Europeas y una Copa UEFA por parte del R.S.C. Anderlecht; y una Recopa y Supercopa Europea del RKV Malinas); mientras que el Club Brujas es el único club que logró llegar a una final de la Copa de Europa, aunque no llegó a ganarla.

La selección de fútbol es una de las más importantes a nivel continental, logrando un tercer lugar de la Eurocopa en 1972 y un subcampeonato en 1980, además de ocupar el tercer lugar en el Mundial en 2018, siendo su mejor participación en la historia de la cita orbital. En los Juegos Olímpicos, ganó el bronce en París (año 1896) y el oro en su propio país, en 1920.

Por el lado del tenis, Kim Clijsters y Justine Henin son las dos tenistas que han alcanzado el número 1 de la clasificación mundial. Henin ganó siete torneos de Grand Slam y la medalla de oro en los Juegos Olímpicos de Atenas 2004, en tanto que Clijsters fue campeona de tres ediciones del Abierto de los Estados Unidos y una del Abierto de Australia. Ellas también lideraron el equipo de Fed Cup de Bélgica que ganó en 2001.

El circuito de Spa-Francorchamps es uno de los más prestigiosos del automovilismo mundial. Allí se han disputado el Gran Premio de Bélgica de Fórmula 1, el Gran Premio de Bélgica del Campeonato Mundial de Motociclismo, los 1000 km de Spa-Francorchamps del Campeonato Mundial de Resistencia, y las 24 Horas de Spa del Campeonato Europeo de Turismos, Campeonato Mundial de Turismos y Campeonato FIA GT. En circuitos se han destacado los pilotos Olivier Gendebien, Paul Frère, Jacky Ickx, Thierry Boutsen y Eric van de Poele, y en rally Bruno Thiry, Freddy Loix y François Duval.



Oficiales

Semioficiales


</doc>
<doc id="4780" url="https://es.wikipedia.org/wiki?curid=4780" title="Andrés Bello">
Andrés Bello

Andrés de Jesús María y José Bello López (Caracas, 29 de noviembre de 1781-Santiago, 15 de octubre de 1865) fue un humanista venezolano, quien fue a la vez filósofo, poeta, abogado, traductor, filólogo, ensayista, educador, político y diplomático. Considerado como uno de los humanistas más importantes de América, realizó contribuciones en innumerables campos del conocimiento.

En Caracas (Capitanía General de Venezuela) fue maestro de Simón Bolívar durante un corto período de tiempo y participó en el proceso que llevó a la independencia venezolana. Como parte del bando revolucionario integró, junto con Luis López Méndez y Simón Bolívar, la primera misión diplomática a Londres, ciudad en que residió entre 1810 y 1829.

En 1829 embarcó junto con su familia hacia Chile, contratado por el gobierno de dicho país, donde desarrolló grandes obras en el campo del derecho y las humanidades. En Santiago alcanzó a desempeñar cargos como senador y profesor, además de dirigir diversos periódicos locales. Como jurista, fue el principal impulsor y redactor del "Código Civil de Chile", una de las obras jurídicas americanas más novedosas e influyentes de su época. Bajo su inspiración y con su decisivo apoyo, en 1842 fue creada la Universidad de Chile, institución de la cual se erigió en primer rector por más de dos décadas. 

De entre sus principales obras literarias, destacan la "Gramática de la lengua castellana destinada al uso de los americanos" (1847), obra de referencia aún hoy imprescindible para los estudios gramaticales, los "Principios del derecho de gentes", el poema «Silva a la agricultura de la zona tórrida» y el ensayo "Resumen de la Historia de Venezuela", entre otras.

Nació en Caracas (Venezuela) el 29 de noviembre de 1781 como hijo primogénito de Bartolomé Bello, abogado y fiscal (1758-1804), y de Ana Antonia López. En su ciudad natal, cursó las primeras letras en la academia de Ramón Vanlonsten. Leyó los clásicos del siglo de oro, y desde muy joven frecuentó el Convento de Las Mercedes, donde aprendió latín de manos del padre Cristóbal de Quesada, a cuya muerte en 1796 Bello tradujo el libro V de la "Eneida".

En 1797 comenzó sus estudios en la Real y Pontificia Universidad de Caracas, donde se graduó de bachiller en artes el 14 de junio de 1800. Ese mismo año, antes de graduarse, recibió en Caracas al naturalista alemán Alexander von Humboldt y a su compañero, Aimé Bonpland, y los acompañó a escalar y explorar el Cerro Ávila, que separa la ciudad del Mar Caribe.

También realizó estudios inacabados de derecho y medicina, aprendió por su propia cuenta inglés y francés, y dio clases particulares, contándose el joven Simón Bolívar entre sus alumnos. Sus traducciones y adaptaciones de textos clásicos le dieron prestigio, y en 1802 ganó por concurso el rango de Oficial Segundo de Secretaría del gobierno colonial. Durante el periodo 1802-1810, Bello se convirtió en una de las personas intelectualmente más influyentes en la sociedad de Caracas, destacándose al desempeñar labores políticas para la administración colonial, además de ganar notoriedad como poeta, al traducir la tragedia "Zulima" de Voltaire. Al llegar la primera imprenta a Caracas en 1808, la gran notoriedad de Bello lo volvió el candidato ideal para asumir la dirección de la recién creada "Gaceta de Caracas", una de las primeras publicaciones venezolanas.

Los sucesos revolucionarios del 19 de abril de 1810, en los que participó Bello, iniciaron la independencia de Venezuela, siendo destituido el capitán general Vicente Emparan por el Cabildo de Caracas. La Junta Suprema de Caracas enseguida nombró a Bello Oficial Primero de la Secretaría de Relaciones Exteriores. El 10 de junio de ese año, zarpó en misión diplomática como representante de la naciente República: fue comisionado, junto con Simón Bolívar y Luis López Méndez, para lograr el apoyo británico a la causa de la independencia. Bello fue escogido por sus conocimientos y su dominio de la lengua inglesa, que había adquirido de forma autodidacta.

La corbeta en la cual viajaba la comisión llegó al puerto de Portsmouth el 10 de julio de 1810, lugar desde el que se dirigieron hacia Londres con el fin de establecer contactos con miembros de las altas esferas británicas. La misión encomendada a Bello, Bolívar y López encuentra graves problemas para desarrollar su labor, puesto que la situación política había cambiado el eje de los intereses ingleses respecto de América. Por un lado, la invasión napoleónica a España había acercado al Reino Unido con su tradicional enemigo, frente al peligro común que consistía Napoleón Bonaparte. Esto significó para el gobierno de Londres tener que ayudar a la causa hispana, otorgándole créditos y ayuda a la Junta Suprema Central que gobernaba en nombre del ""cautivo"" Fernando VII. Sin perjuicio de aquello, y utilizando un doble discurso, Londres toleraba la propaganda independentista americana en su territorio, en especial la realizada por el también venezolano Francisco de Miranda, al mismo tiempo que le otorgaba a los americanos la calificación de beligerantes. Los intereses británicos con la independencia de las colonias españolas de América no iban más allá.

Con esos antecedentes, la delegación venezolana fue recibida por el canciller británico Richard Wellesley, hermano del Duque de Wellington, en cinco entrevistas no oficiales realizadas en su domicilio particular. La postura británica fue clara y desde el principio dieron a entender que en esos momentos, el apoyo político a la causa de la independencia era imposible y trataron de desviar las negociaciones hacia acuerdos comerciales más acordes con los intereses británicos, en un intento además de presionar a España para que les dejase comerciar libremente con sus colonias. Otra de las razones para permitir el recibimiento informal de la embajada venezolana era el de evitar que los mismos tuvieran que recurrir a la ayuda francesa, pese al escaso interés mostrado por Bonaparte hacia la región. El fracaso de la misión provoca el regreso de Bolívar al Nuevo Mundo, con el fin de sumarse a la guerra que arreciaba entonces en el continente. Bello y López quedan entonces a cargo de la embajada, empezando a vivir diversas penurias económicas ante el cada vez más escaso aporte realizado por el gobierno de la naciente república.

En esta época Bello empieza a desenvolverse dentro de la sociedad londinense, trabando una breve pero influyente amistad, durante el escaso tiempo que confluyeron en dicha ciudad, con Francisco de Miranda. Este aprovechó los amplios conocimientos de Bello para sumar a distintos actores a la causa. Miranda en aquella época residía bajo el amparo británico en Londres, con el fin de escapar de la constante persecución española, quien lo había convertido en uno de sus principales enemigos. Bolívar, López y Bello fueron recibidos por Miranda en su casa de Grafton Street, a donde concurrieron reiteradamente con el fin de acceder a las esferas de influencia que Miranda había desarrollado.

Otro de los personajes que ejercería una amplia influencia sería su amigo José María Blanco White, protegido de Lord Holland. Sería este último, bajo instancias de Blanco, quien le proporcionaría cierta estabilidad a Bello, al contratarlo como su bibliotecario y profesor particular. Junto con éste se desempeña en el periódico "El Español", que no abogaba por una independencia total de España. En tal medio se desempeñó como redactor, y en su calidad de tal tomó contacto con personajes como Francisco Antonio Pinto, futuro presidente de Chile, Antonio José de Irisarri, encargado de negocios de Chile y quien impulsaría su viaje a Santiago; Servando Teresa de Mier, con quien colaboraría en "El Español"; James Mill, economista y político escocés y padre de John Stuart Mill; Jeremy Bentham, filósofo inglés, padre del utilitarismo; y los españoles Vicente Salvá, Bartolomé José Gallardo y Antonio Puigblanch, entre otros.

Pese a la ayuda recibida por Blanco White, la situación económica de Bello se hace cada vez más precaria. En 1812 manifiesta su intención de regresar a Venezuela, pero el gran terremoto que asoló Caracas el 26 de marzo de 1812 no permitió aquello e impidió además que su familia pudiera ayudarlo, dada la pérdida de buena parte del patrimonio familiar. Para agravar más la situación, la derrota patriota y la caída de la Primera República significan el fin de todo apoyo económico desde América y el encarcelamiento de su amigo Francisco de Miranda. Ante tales descalabros, Andrés Bello presentó una solicitud de amnistía que tentativamente había anunciado el gobierno español ante el fracaso momentáneo de la independencia americana. Tal solicitud aparece presentada en la embajada española en Londres, fechada el 31 de junio de 1813 (no 1812), un curioso error en un eficiente y minucioso funcionario público. En una parte de aquella petición Bello expresó:

La petición de Bello no tuvo ningún resultado. Al año siguiente trabó relación, por medio de "El Español", con el sacerdote Servando Teresa de Mier, destacado revolucionario mexicano que publicaría varios textos en defensa de la causa americana. Además, se relacionó con Francisco Antonio Pinto, quien en esos momentos se desempeñaba como agregado comercial en la capital británica. Este le dio a conocer a Bello que los patriotas chilenos se habían inspirado en el poema épico de "La Araucana", de Alonso de Ercilla, para su causa. Pinto, quien anteriormente se desempeñaba como agente comercial, había sido comisionado por el gobierno de Chile como su agente (embajador), primero en Buenos Aires y después en Londres. En este lugar confrontó, al igual que Bello, la caída del gobierno patriota tras la derrota de Rancagua, que sumió a Pinto en una situación de grave carestía. Pese a encontrarse en una situación similar, Bello ayudó en todo lo posible, junto a Manuel de Sarratea, al infortunado diplomático. Así trabaron los dos una profunda amistad, siendo Pinto uno de los escasos miembros de su círculo cercano. De regreso a Chile, Pinto tomaría parte en las victorias patriotas en Chacabuco y Maipú, formando parte de la cúpula política del país. En 1827, ante la renuncia del capitán general Ramón Freire a la primera magistratura, Pinto es elegido como Presidente de Chile. Durante su breve ejercicio del cargo, en vísperas de la guerra civil y la derrota liberal en Lircay, en uno de sus últimos decretos nombra a Bello como oficial segundo del Ministerio de Hacienda de Chile, cargo que no ocupó.

En mayo de 1814 Bello contrajo matrimonio con la inglesa de 20 años Mary Ann Boyland. De esta unión nacen sus primeros tres hijos Carlos (1815), Francisco (1817) y Juan Pablo Antonio (1820). Su vida familiar se ve constantemente afectada por la falta de sustento, que intenta mejorar solicitando un empleo al gobierno de Cundinamarca, en 1815, y al de las Provincias Unidas del Río de la Plata, al año siguiente. En este último caso, el trabajo fue concedido a Bello, pero por razones poco claras nunca lo asumió en propiedad. Su situación alcanza en 1816 a mejorar un poco al recibir alguna ayuda por parte del gobierno británico, con lo que pudo realizar algunas investigaciones en la biblioteca del Museo Británico. En este lugar se encontraba trabajando, cuando Thomas Bruce, conde de Elgin, presentó los mármoles del Partenón, en 1819. Al año siguiente colaboró con James Mill en la transcripción de los manuscritos de Jeremy Bentham. Su esposa se vio afectada por la tuberculosis, enfermedad de la que falleció el 9 de mayo de 1821, seguida por su hijo Juan Pablo, en diciembre de aquel año, siendo el primero de nueve de sus hijos que vio morir en vida.

En esta época trabaría también amistad con el granadino Juan García del Río y, más importante aún para su futuro, conoció en 1819 al guatemalteco Antonio José de Irisarri, quien se había desempeñado como director supremo interino de Chile en 1814, y después de la independencia de Chile como canciller de la nueva República. Ese mismo año le escribió a Irisarri solicitándole ayuda, con el fin de ser contratado en la legación chilena en Londres. Tal designación demoró más de seis meses, aunque Bello logró finalmente ser designado para un empleo estable, como secretario de la legación en junio de 1822. Sus dificultades económicas terminaron con ese cargo.

Durante su desempeño como secretario, Bello sigue las instrucciones de Irisarri, a quien se le encomienda lograr el reconocimiento de Chile por Francia y el Reino Unido, además de conseguir un empréstito para la naciente república. El encargado Irisarri responde a órdenes directas del director supremo Bernardo O'Higgins, quien se desempeña en el mando hasta su forzada abdicación, el 28 de enero de 1823. Irisarri se ve entonces interpelado por un nuevo delegado del gobierno, Mariano Egaña, quien mantenía una antigua disputa con Irisarri. Bello se ve envuelto en medio de un desagradable conflicto, en el cual se enfrenta con el titular del cargo y su superior directo (Egaña), al mismo tiempo que debe un gran aprecio a su antiguo jefe (Irisarri). Sin embargo, las suspicacias y temores iniciales de Egaña se disipan en el tiempo, al descubrir en Bello una mente brillante. No escatima entonces elogios para hablar de quien se convertiría en uno de sus grandes amigos, haciendo presente en una recomendación enviada en 1826, cuando Bello ya no se desempeñaba en la legación, con el fin de favorecer su contratación por parte del gobierno de Chile. Dice Mariano Egaña en su informe:

Durante esta época Bello realiza buena parte de su trabajo como escritor y poeta, dirigiendo y redactando en gran medida el "El Censor Americano" (1820), "La Biblioteca Americana" (1823) y siendo el director de "El Repertorio Americano" (1826). Todas estas obras constituyen por muchos la más grande manifestación europea del pensamiento americano, en la cual se publican diversas y variadas obras sobre ciencias eruditas, filología, estudios de críticas y análisis. En ellas se publican dos de los grandes poemas de Bello, la "Alocución a la poesía", de 1823, y la "Agricultura en la zona tórrida", de 1826. Se desempeña en la legación chilena hasta 1825, cuando termina su contrato. La situación de Bello mejoró temporalmente en 1822, cuando el guatemalteco Antonio José de Irisarri, ministro de Chile en Londres, lo nombró secretario interino de la legación. Bello le había escrito desesperado el 18 de marzo de 1821 pidiéndole el empleo, y una vez que lo obtuvo, incluso pudo casarse de nuevo, el 24 de febrero de 1824, con Isabel Antonia Dunn, con quien tuvo 12 hijos (3 nacidos en Londres, el resto en Chile). Este puesto lo dejó en 1824 al terminársele el contrato, pero por suerte la secretaría de la Gran Colombia había quedado vacante recientemente y el plenipotenciario Manuel José Hurtado lo nombró interino y propuso para el cargo permanente. El Ministro de Relaciones Exteriores, Pedro Gual, aprobó el contrato de Bello el 9 de noviembre de 1824, y Bello tomó el cargo el 7 de febrero de 1825.

Este golpe de suerte, sin embargo, no resultó como esperaba. Por un lado, debido a la crisis financiera en América, el modesto sueldo de Bello le era pagado irregularmente. Por otro, su relación con Simón Bolívar se deterioró progresivamente por circunstancias tanto presentes como pasadas. En este marco, el 21 de diciembre de 1826 Bello le escribió directamente a Bolívar solicitando que interviniese en mejorar su situación en Londres. Aparte de las razones económicas, su comunicación con Bolívar tenía que ver con la creencia de Bello de que, de alguna manera, su amistad con su antiguo pupilo había decaído. Esto probablemente por los rumores que corrían, desde su llegada a Londres, de que había sido él quien avisó a Vicente Emparan del fracasado alzamiento del 2 de abril de 1810 en Caracas. También por las diferencias ideológicas con el gobierno de Colombia, que había expresado en una carta al patriota mexicano Servando Teresa de Mier, en 1821. Esta carta había caído en manos de Pedro Gual, quien a pesar de haberle dado el trabajo, dudaba de su persona.(22) Quizás por esto o por otras circunstancias, Bello era tratado con indiferencia por el jefe de la legación en Londres, Manuel José Hurtado.

En abril de 1827, Bello le volvió a escribir a Bolívar, pero su situación no mejoró. Peor aún, debido a numerosos retrasos en negocios personales que Bello adelantaba en Londres para Bolívar (la venta de las minas de Aroa), la relación entre ambos se enfrió hasta el punto en que el Libertador nombró a otra persona para finiquitar el negocio y Bello finalmente comenzó a buscar trabajo en otra parte.

Bolívar finalmente escribiría a la legación en Londres el 27 de abril de 1829, informando del envío de 3000 pesos para Bello y su nombramiento como cónsul general de Colombia en París, pero entonces ya era demasiado tarde. Bello había partido hacia América el 14 de febrero de 1829, tras aceptar otra oferta de empleo del ministro Plenipotenciario de Chile en Londres, Mariano Egaña Fabres. En ese mismo año pasa a desempeñar labores iguales en la embajada de la Gran Colombia, en las cuales sufre una gran decepción al no ser designado titular del cargo, que ha quedado vacante por parte de Bolívar. En su intercambio epistolar Bello manifiesta su decepción por lo sucedido, manifestando su deseo de abandonar de manera definitiva Europa. En 1828, y ante reiteradas solicitudes de Mariano Egaña, el gobierno de Chile contrata a Bello para un puesto en el Ministerio de Hacienda, abandonando definitivamente el Reino Unido el 14 de febrero de 1829.

Bello llegó a Chile en 1829, junto con su esposa Isabel Antonia Dunn, con quien había contraído matrimonio el 24 de febrero de 1824. Casado dos veces, Bello tuvo quince hijos, de los cuales vio morir a nueve —entre ellos: Francisco (1817-1845), Carlos Bello Boyland (1815-1854), Juan (1825-1860) y Emilio Bello Dunn (1845-1875)—. Manuel Antonio Tocornal, su sucesor en la rectoría de la Universidad de Chile, señaló:
Su designación titular fue de Oficial Mayor del Ministerio de Hacienda, académico del Instituto Nacional y fue el fundador del Colegio de Santiago, rival del Liceo de Chile, creado por José Joaquín de Mora. Tuvo una importante participación en la actividad literaria y cultural en la llamada Sociedad Literaria de 1842. Ese mismo año, con la fundación de la nueva Universidad de Chile, se le otorgó el título de primer rector.

Colaboró en la edición del diario "El Araucano" entre 1840 y 1860, siendo el medio cultural de referencia casi obligatoria en aquella época. Participó en el debate y polémica sobre el carácter de la educación pública, junto con Domingo Faustino Sarmiento. En estos años, durante su estadía en Chile, publicó sus principales obras sobre gramática y derecho, y recibió distintos reconocimientos por tal labor, siendo el más importante el recibido en 1851, cuando fue nombrado miembro honorario de la Real Academia Española.

El Congreso Nacional le otorgó unánimemente la nacionalidad chilena, por gracia, el 17 de octubre de 1832. Sin embargo, este acuerdo no fue publicado en "El Araucano", el diario oficial de la época. Posteriormente, en la edición del 7 de diciembre de 1832 de ese periódico, se publicó un «aviso oficial» que señaló: «Se han dado cartas de naturaleza á favor de don Benito Fernández Maqueira, de don Carlos Eduardo Mitchall, de don Victorino Garrido, de don Andrés Bello y de don Tomas Ovejero». En consecuencia, Andrés Bello no recibió la nacionalidad por gracia, sino que él la solicitó conforme al reglamento sobre la materia publicado el 9 de noviembre de 1832, tal como cualquier otro extranjero.

Se desempeñó como senador por la ciudad de Santiago entre 1837 y 1864. Fue el principal y casi exclusivo redactor del Código Civil chileno entre 1840 y 1855, considerado una de las obras más originales de la legislación americana. Entre su obra literaria, destaca su traducción libre de la "Oración por todos", de Víctor Hugo, considerada por muchos la mejor poesía chilena del siglo XIX. Impulsor de la Universidad de Chile, fue designado su primer rector, cargo que desempeñó hasta su muerte.

Falleció en la ciudad de Santiago el 15 de octubre de 1865, y fue enterrado en el Cementerio General de dicha ciudad. Ignacio Domeyko señaló, para su funeral:


Durante los casi veinte años que Bello vivió en Londres (1810-1829), dedicó gran parte de su tiempo al estudio y la investigación, sobre todo en la antigua biblioteca del Museo Británico. En su labor usó 13 cuadernillos que fue llenando de notas y transcripciones de sus lecturas. «Dado que Bello utilizó algunas de estas notas en obras publicadas con posterioridad, los cuadernos revisten una singular importancia a la hora de establecer la cronología precisa de su desarrollo intelectual». Sin embargo, no hay que olvidar que se trata de apuntes personales que deben considerarse como "documentos de trabajo". En 2017, los profesores Tania Avilés e Iván Jaksić publicaron por primera vez los cuadernos en una muy completa edición, la única hasta ahora:









Bello es dramatizado en la serie "Bolívar" interpretado por Nicolás Prieto.



</doc>
<doc id="4782" url="https://es.wikipedia.org/wiki?curid=4782" title="Zygochloa paradoxa">
Zygochloa paradoxa

Zygochloa es un género monotípico de plantas herbáceas de la familia de las gramíneas o poáceas. Su única especie: Zygochloa paradoxa , es originaria de Australia.

Es una planta robusta, perenne, rizomatosa y dioica, formando matas o montículos de 1,5 m de altura, 1 m de ancho. Culmos duros, frágiles, de hasta 8 mm o más de diámetro. Lígula una hilera de pelos de 1 mm de largo; escasa hoja, rígida y plana, con nervios prominentes. Inflorescencia de dos tipos: Cabezas masculinas globulares, diámetro de 1-2 cm, la femenina también globular, de 2.5-3.5 cm de diámetro, las brácteas prominentes con puntas rígidas. Espiguillas dimorfas, unisexuales, ligeramente comprimidas. Espiguillas masculinas poco pediceladas, 6-8 mm de largo, algunos en una panícula punta-como, con 2 flósculos masculinos; glumas subiguales, rígidas, 5-7-nervada, lemas tiesos con márgenes translúcidos, pálea rígida. Espiguillas femeninas 6-10 mm de largo, solitarias, poco pediceladas, sostenidas por brácteas membranosas.

"Zygochloa paradoxa" fue descrita por (R.Br.) S.T.Blake y publicado en "Papers from the Department of Biology, University of Queensland Papers" 1(19): 7–8, t. 3. 1941.




</doc>
<doc id="4783" url="https://es.wikipedia.org/wiki?curid=4783" title="Alejandro Toledo">
Alejandro Toledo

Alejandro Celestino Toledo Manrique (Cabana, Áncash; 28 de marzo de 1946) es un economista y político peruano. Fue presidente de la República del Perú desde el 28 de julio de 2001 hasta el 28 de julio de 2006, el primero en ser escogido tras el retorno de la democracia posterior al gobierno transitorio de Valentín Paniagua. 

Estudió en la Universidad de San Francisco y luego en la Universidad de Stanford. Se vinculó originalmente al ámbito técnico y académico, desde donde participó como analista sobre política y economía en algunas oportunidades. Ingresó a la política activa al fundar el partido País Posible, participando por primera vez en las elecciones de 1995. En 2000 logró convertirse en el mayor líder de oposición al gobierno de Alberto Fujimori, ante el cual en medio de un proceso controvertido y accidentado, perdió por segunda vez los comicios electorales. Finalizada la etapa de transición y el retorno de la democracia en Perú, participó por tercera vez en las elecciones de 2001 junto a Lourdes Flores (UN) y Alan García (PAP); compitió con este último en la segunda vuelta, donde obtuvo la victoria con 53,08 % de votos válidos.

Su gobierno se caracterizó por el inicio del despunte macroeconómico del país, fomentando la inversión extranjera, la firma de tratados de libre comercio e implementación de varios proyectos de inversión en infraestructura y desarrollo humano. Durante su mandato, Toledo sufrió una crisis de gobernabilidad, escándalos en su vida personal y denuncias de corrupción contra su entorno, signos que golpearon su popularidad hasta caer a un 8% de aprobación por la ciudadanía. Posteriormente, intentaría regresar a Palacio de Gobierno siendo candidato presidencial por Perú Posible en las elecciones generales de 2011 y 2016, quedando en cuarto y octavo lugar respectivamente.

Tras su presidencia formó parte, como miembro distinguido residente, del Centro de Estudios Avanzados y Ciencias del Comportamiento de la Universidad de Stanford y ha sido profesor visitante en el Centro para la Democracia, el Desarrollo y el Cumplimiento de la Ley del Instituto Freeman Spogli. Toledo ha sido expositor de conferencias en distintos países sobre economía, inclusión y democracia, además de dirigente de su partido político. En 2006 fundó el Centro Global para el Desarrollo y la Democracia, organismo con el cual promueve democracias sostenibles; y entre 2009 y 2010 fue profesor visitante en la Escuela de Estudios Internacionales Avanzados en Política Exterior y Economía Global y Desarrollo de la Universidad Johns Hopkins y docente de Política Exterior en la Brookings Institution.

El 16 de julio de 2019 fue arrestado en Estados Unidos por mandato de extradición al Perú según informó el Ministerio Público. El 8 de agosto, Graham Archer, abogado de oficio del exmandatario solicitó un pedido de libertad bajo fianza ante el juez estadounidense Thomas S. Hixson. El 12 de septiembre de 2019 el juez dictaminó improcedente su pedido de reconsideración. Sin embargo, el 19 de marzo de 2020 se le concedió la libertad bajo fianza.

Alejandro Toledo nació en el Centro Poblado de Ferrer del distrito de Bolognesi, pero fue registrado en el vecino distrito de Cabana, en la provincia de Pallasca, en la Región Áncash, el 28 de marzo de 1946. Sus padres fueron Anatolio Toledo Campos (natural de Nazca, de ascendencia ayacuchana y arequipeña) y Margarita Manrique, una familia de campesinos de situación económica baja.

Fue el octavo de dieciséis hijos, de los cuales solo sobrevivieron nueve debido a las precarias condiciones en que vivía su familia. En 1950, su familia se mudó a Chimbote en busca de un futuro mejor por medio de la pesca de anchoveta.

Trabajó de empresario para contribuir a la economía familiar, la cual era muy precaria. Estas actividades las seguía paralelamente con sus estudios primarios.

Cursó sus estudios secundarios en la "Gran Unidad Escolar San Pedro de Chimbote", donde destacó por sus habilidades matemáticas; en su último año de estudios consiguió una beca a la Universidad de San Francisco mediante unos voluntarios del Cuerpo de Paz de los Estados Unidos.

Estudió Economía y Administración de empresas en la Universidad de San Francisco, de donde se graduó en 1970; luego de ello obtuvo una maestría en Educación (1972) y otra en Economía de los Recursos Humanos (1974), ambas en la Universidad de Stanford. Es en esta universidad donde conoció a Eliane Karp, con quien contrajo matrimonio el 20 de enero de 1979, en Sunnyvale, California, EE.UU, se divorciaron en 1992 y se volvieron a casar en 1997.

En 1974 se desempeñó como consultor en el Stanford Research Institute (SRI). Al año siguiente (1975) se mudó a París, en donde fue Investigador principal de la Organización para la Cooperación y el Desarrollo Económico (OCDE). Regresó al Perú en 1976 para ser parte del Comité de Asesores Económicos del Banco Central de Reserva del Perú y del Ministerio de Trabajo.

En 1978 fue consultor del Banco Interamericano de Desarrollo en Washington D.C. y profesor asociado de Economía en la American University. Al siguiente año, trabajó como Economista principal de Recursos Humanos en el Banco Mundial.

De 1983 a 1985 fue profesor de economía e investigador principal de la Universidad del Pacífico en Lima; a la par, brindó asesoría técnica al poder ejecutivo y enseñó en La Escuela de Administración de Negocios para Graduados (ESAN), donde se mantiene como profesor con licencia.

En 1989 viajó a Nueva York y fue líder de la misión del PNUD y de la OIT para la evaluación del: "Impacto de las Políticas Macroeconómicas sobre el Crecimiento, Empleo y Salarios" en seis países de América Central.

Desde 1991 hasta 1994 fue miembro del Instituto para el desarrollo Internacional de la Universidad de Harvard, además se desempeñó como profesor visitante en la Universidad de Waseda en Tokio. En 1993 regresó a la Universidad de Stanford para culminar un doctorado en Economía de los Recursos Humanos.

En diciembre de 1994 anunció su candidatura a las elecciones presidenciales de 1995 encabezando la agrupación electoral Perú Posible que estableció alianza con la Coordinadora Democrática, llamada CODE - País Posible.

Toledo fue designado candidato de la alianza y a pesar de alcanzar, en las encuestas previas, un creciente 11% (hecho que lo llevó a ser calificado como el "fenómeno Toledo") en los resultados oficiales sólo alcanzó un 3,5% de los votos válidamente emitidos. Esas elecciones las ganó con más del 62% de las preferencias el entonces presidente Fujimori.

Alejandro Toledo asistió a la recepción por el natalicio del emperador del Japón, dada en diciembre de 1996 en la residencia del embajador de Japón en San Isidro. La casa fue tomada por la organización terrorista peruana Movimiento Revolucionario Túpac Amaru, Toledo estuvo brevemente secuestrado en la llamada Toma de la residencia del embajador de Japón en Lima. Tras tensas negociaciones, Alejandro Toledo, Javier Diez Canseco y los embajadores de Brasil, Corea y Egipto son liberados no sin antes ser obligados a leer una proclama del MRTA. El resto de los secuestrados permaneció en el lugar, y fueron rescatados posteriormente por comandos del ejército que tomaron por asalto la residencia en la Operación Chavín de Huántar.

Para el proceso electoral de 2000, Toledo se presentó encabezando su propia agrupación llamada Partido Político Perú Posible. Este proceso estuvo envuelto en controversia debido a la decisión del entonces Presidente Alberto Fujimori de postular a un segundo mandato constitucional de acuerdo a la nueva Constitución ya vigente desde 1993.

En aras de la unidad contra el régimen Fujimorista, Toledo instó a los partidos políticos de oposición a presentar un candidato único, sosteniendo conversaciones con Luis Castañeda Lossio y Alberto Andrade Carmona para lograrlo. Al fracasar este intento, los líderes de oposición instan a votar por Toledo para prevenir que Fujimori venciera en primera vuelta. Para el Parlamento, muchos de sus candidatos fueron seleccionados de las filas del Partido Popular Cristiano.

Su plancha presidencial estuvo integrada por Carlos Ferrero Costa (Primera Vicepresidencia), abogado y antiguo aliado del Presidente Fujimori; y por David Waisman (Segunda Vicepresidencia), empresario y dirigente de Perú Posible.

En un proceso eleccionario plagado de críticas por parte de los observadores nacionales e internacionales, Toledo quedó en un disputado segundo lugar con un 40,3% de las preferencias frente a un 49,8% de Alberto Fujimori.


Debido a las denuncias de fraude que existieron después de la elección, Toledo anunció, el 18 de mayo de 2000, que no se presentaría en la segunda vuelta si es que esta no se aplazaba dos semanas y se subsanaban todas las observaciones formuladas. Ante la falta de respuesta, el 26 de mayo de ese año, Toledo desistió (pero no oficializó su renuncia ante el JNE) de participar en la segunda vuelta, solicitando a los votantes a que voten en blanco o viciado.

El día de las elecciones (28 de mayo), en el tradicional "flash electoral", existió un 25,6 % de votos válidamente emitidos a favor de Toledo en tanto que los votos en blanco y viciados alcanzaron el 31 %. Esto adjudicó la victoria a Alberto Fujimori.


Desde esa fecha, Toledo encabezó lo que llamó una "resistencia pacífica" en contra del tercer periodo de gobierno de Fujimori. El 28 de julio de 2000 se produjo una jornada de protesta nacional denominada La Marcha de los Cuatro Suyos, en la cual hubo personajes a nivel nacional así (congresistas electos, periodistas, personas de a pie que caminaron grandes distancias para llegar a la marcha) como también asistieron personajes de nivel internacional para demostrar su apoyo, como fue el caso del expresidente argentino Raúl Alfonsín (1983-1989).

Lamentablemente, la marcha se convertiría en tragedia, pues se produjo la infiltración de delincuentes enviados por Vladimiro Montesinos, jefe del Servicio de Inteligencia Nacional y mano derecha del hasta ese momento presidente Alberto Fujimori, quienes causaron destrozos en la propiedad pública y provocaron el incendio a uno de los locales más importantes del estatal Banco de la Nación, además se provocó la muerte de seis guardias de seguridad que allí se encontraban. Estos incidentes colocaron mayor presión política sobre el presidente Alberto Fujimori. Meses después de la marcha se revelarían los "Vladivideos" que marcarían el fin del régimen de Fujimori, quien unos meses más tarde se fugaría del país y se refugiaría en Japón, desde donde enviaría por fax su renuncia a la Presidencia de la República.

Luego de la renuncia de Fujimori y su autoexilio en el Japón, el 22 de noviembre de 2000, el Presidente del Congreso Valentín Paniagua Corazao asumió la presidencia de la República de manera transitoria y en su mandato convocó a elecciones para mayo de 2001.

La plancha presidencial de Toledo Manrique estuvo integrada por el empresario Raúl Díez-Canseco (Primera Vicepresidencia) y por David Waisman (Segunda Vicepresidencia), congresista y dirigente de Perú Posible.

Tal como indicaban las encuestadoras semanas antes de las elecciones, ningún candidato superó el 50 % más 1 voto requeridos, por lo que los dos participantes que obtuvieron mayor votación (Alejandro Toledo y Alan García), pasaron a una segunda vuelta, que se realizó el 3 de junio de 2001.


Con acuerdos entre los negociadores de los partidos que pasaron a la segunda vuelta y la ayuda de la Asociación Civil "Transparencia", dieron como resultado la programación del debate presidencial. Este se desarrolló el 19 de mayo, a las 20:00 horas entre los candidatos presidenciales de cada partido: Alejandro Toledo (Perú Posible) y Alan García (APRA), en el "Hotel Marriott". Fue moderador el periodista Güido Lombardi.

El día de las elecciones (3 de junio), en el tradicional "flash electoral" la diferencia entre ambos candidatos, si bien disminuyó considerablemente tras el debate, adjudicó la victoria a Perú Posible y la derrota de Alan García con más del 5 % de diferencia.


El 10 de noviembre de 2010 anunció por su cuenta oficial de Twitter su postulación a la presidencia, luego mediante una conferencia de prensa, anunció que sería candidato en las elecciones generales del Perú de 2011; igualmente lo hizo mediante comerciales en los programas más sintonizados del país.

Toledo quedó en cuarto lugar, quedando así fuera de la segunda vuelta a la que entraron Ollanta Humala y Keiko Fujimori

Toledo obtuvo el 1,07 % de votos emitidos y el 1,30% de votos válidos.

Toledo, ya como Presidente, procedió a normalizar las relaciones diplomáticas con el gobierno de Venezuela, después de estar deterioradas durante el gobierno interino de su predecesor Valentín Paniagua, por el caso de Vladimiro Montesinos. Estas, sin embargo, no serían duraderas.

Las relaciones con el gobierno de Japón se deterioraron desde un primer momento después de la negativa de este de extraditar al expresidente Fujimori, por el cargo de diversos delitos y en donde Toledo insistió en su entrega.

En el aspecto integracionista Toledo, estableció acuerdos económicos con Brasil, en donde destacan proyectos en conjunto para mejorar las comunicaciones entre ambos países y con Bolivia. Con Estados Unidos promovió y aceleró las negociaciones para el Acuerdo de Promoción Comercial Perú - E.U.A., junto con Ecuador y Colombia.

La Comunidad Sudamericana de Naciones fue una propuesta suya para realizar el sueño de Simón Bolívar de integración sudamericana. Esto fue fruto de las relaciones con sus homólogos sudamericanos.

Toledo, junto con los presidentes Tabaré Vázquez de Uruguay y Sebastián Piñera de Chile, han sido los únicos Jefes de Estado latinoamericanos en visitar Israel de manera oficial. En este viaje, se entrevistó con el presidente israelí Moshé Katsav (quien brindó honores militares a su llegada al país) y el entonces Primer Ministro Ariel Sharón, seguido por su presencia en una sesión especial del Knesset. El motivo de este viaje fue promover en Israel el sector exportador peruano y la inversión israelí en materia agropecuaria y tecnológica en el Perú.

De la misma manera, procedió hacia Jordania, donde dialogó acerca del intercambio económico entre los dos países y visitó la colonia peruana en dicha nación árabe. No se reunió, en cambio, con el Presidente Palestino, Mahmud Abbas.

El 28 de octubre de 2005 fue presentado un proyecto de ley en el Congreso de la República del Perú que indicaba el establecimiento de líneas de bases para la definición del dominio marítimo sobre el Océano Pacífico. Aprobado este proyecto el 3 de noviembre de 2005, fue promulgado por el Presidente Toledo el mismo día. La promulgación de esta ley generó un fuerte rechazo por parte del gobierno del entonces Presidente Chileno Ricardo Lagos, quien logró el apoyo del gobierno del Ecuador para fortalecer su posición. Este hecho marcó un distanciamiento en la relación entre el Perú y Chile, contrastando con las cálidas relaciones diplomáticas que existían al inicio de su gobierno.

A pesar de la controversia que se generó con dichos países, varias personalidades políticas manifestaron su apoyo a la postura del gobierno, que aseguró que esto era un tema interno del Perú. Inmediatamente, el gobierno empezó una campaña para suscribir a la nación a la "Convención del Mar".

La llegada sorpresiva del expresidente Alberto Fujimori a Chile, a escasos meses de las Elecciones Generales, generó un terremoto político en su gobierno debido a los cargos que se le seguían a Fujimori. Inmediatamente se procedió a buscar la extradición de Fujimori al Perú, logrando una victoria solventada por la izquierda chilena, al asegurar su detención y posterior enjuiciamiento en Chile. Después de un largo proceso Chile concedió la extradición de Fujimori basado en una serie de delitos entre ellos los de lesa humanidad por los cuales el exmandatario ha sido enjuiciado y declarado culpable

En el año 2006 se volvieron a deteriorar nuevamente las relaciones diplomáticas con Venezuela por discrepancias con respecto al AlCA, la Comunidad Andina y de acusar al Presidente Hugo Chávez de intromisión en los asuntos internos peruanos (específicamente al manifestar su apoyo incondicional al candidato presidencial Ollanta Humala en las elecciones 2006).

Su gobierno ha tenido como hitos los "Tratados de Libre Comercio" con el Mercosur, la Comunidad Andina y Tailandia y el muy importante Acuerdo de Promoción Comercial (APC) con Estados Unidos que fue aprobado por el Congreso Peruano el 26 de junio de 2006.

Alejandro Toledo comenzó su gobierno con aproximadamente 55% de aprobación. Algunos escándalos, explotados por la oposición, pronto harían mella en su aprobación; así, durante buena parte de su mandato este tuvo un solo dígito porcentual de aceptación, con picos de hasta sólo 7%. En junio y julio de 2002, hubo fuertes movilizaciones contra la privatización de "Egasa" y "Egesur" (adquiridas por la empresa belga "Tractebel") con gran intensidad de disturbios, especialmente en Arequipa. El hecho tuvo un impacto político aún mayor debido a la promesa hecha por Toledo durante su campaña electoral de no vender las firmas de energía. Las protestas llevaron a la caída del primer gabinete de Toledo y a la instauración de un "Estado de Emergencia" en dicha Región.

Ese mismo año, La Asociación Pro Derechos Humanos (APRODEH) y familiares de los insurgentes abatidos por el gobierno durante la Crisis de la Embajada Japonesa empezaron acciones legales contra el Estado. Estas se basaron primordialmente en el testimonio de algunos exrehenes, quienes aseguraron haber visto al menos a uno de los emerretistas con vida tras el asalto, lo cual fue desmentido posteriormente. Alejandro Toledo, comprometido con investigar los excesos cometidos por la administración de Alberto Fujimori y el respeto a los derechos humanos, brindó su total respaldo hacia las Fuerzas Armadas, aprobando que el Comando "Chavín de Huantar" liderara la Parada Militar del 2002.
En 2003, se vivió una huelga general de los maestros del Estado entre los meses de junio y julio, junto con otras de los médicos y enfermeras del sector público, el poder judicial y los agricultores y cocaleros, que dieron un clima de desgobierno que culminó con la declaración del Estado de emergencia o de excepción es decir el recorte de las garantías y libertades constitucionales por un mes. Los sucesos llevaron nuevamente a la dimisión de varios ministros.

En el año 2004, hubo fuertes rumores de una posible vacancia presidencial, con un paro nacional convocado por la Confederación General de Trabajadores (CGTP) y por la oposición, liderada por el APRA con Alan García el 14 de julio de ese año. Aunque la huelga fracasó y las aguas se calmaron y no se planteó formalmente la vacancia esta fue usada como elemento de presión de las fuerzas políticas.

El 1 de enero de 2005, el mayor retirado del Ejército Peruano Antauro Humala (hermano del exmilitar y expresidente Ollanta Humala) encabezó junto con un grupo de seguidores denominados etnocaceristas (nacionalistas indigenistas) el asalto y toma de la comisaría de Andahuaylas. Los sucesos llevaron de inmediato a la renuncia del entonces Ministro del Interior, César Reátegui, dirigente de Perú Posible. La acción subversiva fue develada el 3 de enero con un saldo de 4 policías y 2 etnocaceristas muertos, y Antauro Humala fue arrestado.
En el último día de su gobierno, el Presidente Toledo inauguró la llamada "Plaza de la Democracia", ubicada en el lugar donde se produjo el incendio del "Banco de la Nación" y la muerte de seis guardias de seguridad. Al día siguiente; minutos antes de llegar al Parlamento para entrega el poder; manifestó que a pesar de los altos y bajos de su administración, "Entré [a la Presidencia] por la puerta grande y salgo por la puerta grande".

En los últimos seis meses de su mandato se registró un incremento de aprobación de su gestión de entre el 10% al 30% a nivel nacional. Toledo terminó su mandato con un 47,1% de aprobación en el ámbito capitalino, según la encuestadora CPI.

Diversos analistas y políticos –como Rafael Rey, Martha Hildebrandt, el sociólogo Julio Cotler y el psicoanalista Jorge Bruce- consideran que el gobierno de Alejandro Toledo fue un período de relativa estabilidad política y económica.
Su gobierno emprendió medidas neoliberales, en donde las estadísticas macroeconómicas indican un sostenido crecimiento en la economía peruana y una mayoritaria aprobación de las mismas. Entre los motores del crecimiento que se inició con las empresas mineras instaladas principalmente en la década pasada, se pueden nombrar proyectos de gran envergadura, como el Gas de Camisea o la Carretera Interoceánica; así como la creciente expansión de diversos sectores como construcción, minería y agroindustria.

Las políticas económicas de Toledo pueden describirse como un libre comercio neoliberal o fuertemente favorable. Heredó una economía nacional que en la década anterior había experimentado un PIB inestable con períodos de crecimiento y contracción, así como déficits fiscales que con frecuencia ascendían a más del 2% del PIB. La inflación no había caído por debajo del 23% hasta 1995 y todavía era temida por muchos. En respuesta, Toledo desarrolló políticas centradas en combatir la pobreza, generar empleo, descentralizar el gobierno y modernizar el estado. 

Entre las iniciativas de Toledo diseñadas para generar ingresos y transformar la economía estaban los planes de privatizar las industrias nacionales. El primer gran esfuerzo de este tipo fue la venta de $ 167 millones de dos compañías eléctricas estatales. Las protestas en la ciudad de Arequipa se volvieron violentas cuando los peruanos reaccionaron con ira ante la perspectiva de despidos y electricidad a precios más altos. También recordaron que miles de millones de dólares obtenidos de la privatización bajo la administración Fujimori habían terminado llenando las cuentas bancarias personales del presidente. Toledo decidió no llevar a cabo la venta de compañías eléctricas, pero prometió continuar los esfuerzos de privatización, que fueron una disposición clave de un acuerdo alcanzado con el Fondo Monetario Internacional. Toledo había prometido recaudar 700 millones de dólares a través de la privatización en 2001 y mil millones de dólares en 2002. Aunque no logró cumplir estos objetivos, el FMI aprobó un desembolso de $ 154 millones a Perú en diciembre de 2002 y permitió al país elevar el objetivo de déficit fiscal en su acuerdo. <ref name="Amer Review 200 -Op/075 - Kogan Kogan Page - Google Books"> Page, Kogan (2003)Americas Review 2003/2004, Retrieved May 2011. </ref>

Aunque Toledo originalmente prometió recortes de impuestos, las protestas violentas de los funcionarios públicos provocaron el aumento en el gasto del sector social que Toledo también había prometido, lo que exigió aumentos de impuestos. Para abordar la reforma fiscal en junio de 2003, nombró a la primera ministra del Perú, Beatriz Merino, que rápidamente presentó propuestas al congreso. Entre las sugerencias estaban recortes salariales para los funcionarios del sector público mejor pagados, incluida una reducción salarial del 30% para el propio Toledo, un recorte general del 5% para todas las agencias y ministerios, aumentos de impuestos sobre la cerveza, los cigarrillos y el combustible, y una extensión del 18% del impuesto sobre las ventas y el valor agregado a, entre otras cosas, viajes en autobús de larga distancia y entretenimiento en vivo. El paquete final también incluyó la eliminación de exenciones impositivas, la introducción de un impuesto corporativo mínimo, el cierre de lagunas fiscales para los ricos y el fortalecimiento de los regímenes impositivos inmobiliarios del gobierno local. 

Durante los cinco años de Toledo como presidente, la economía peruana experimentó 47 meses consecutivos de crecimiento y creció a una tasa promedio de 6% anual, mientras que la inflación promedió 1.5% y el déficit se hundió tan bajo como 0.2% del PIB. Entre 2004 y 2006, el empleo creció a una tasa promedio del 6%, el porcentaje de personas que viven en la pobreza disminuyó y el consumo de alimentos por parte de los segmentos más pobres de la población aumentó dramáticamente. Gran parte de este crecimiento se ha atribuido a los acuerdos de libre comercio firmados con los Estados Unidos, China, Tailandia, Chile, México y Singapur. 

En un intento por aumentar las remesas de los peruanos en el extranjero, el Ministerio de Relaciones Exteriores de Toledo buscó fortalecer el vínculo entre los migrantes peruanos y su tierra natal a través de la creación de consejos consultivos. 

Muchos descentralistas valoran que convocó a elecciones regionales y municipales el año 2003. Toledo siempre decía "más vale la descentralización efectiva, que demorar en la regionalización política", parafraseando los criterios tomados del libro "Gobiernos Regionales" del ingeniero politólogo Andrés Tinoco Rondán.


En 2017, Toledo es acusado de lavado de activos al estar presuntamente involucrado en el Caso Odebrecht. El 3 de febrero de este año, a pedido del Ministerio Público, agentes de la PNP allanan la Residencia en Camacho de Alejandro Toledo luego de que Jorge Barata (Exrepresentante de Odebrecht en Perú) revelará haberle entregado 20 millones de dólares a cambio de favorecer a la Empresa Odebrecht en la concesión de los tramos II y III de la Ruta interoceánica Brasil-Perú, cuyos montos habrían sido depositados a las cuentas del multimillonario Israelí Josef Maiman. El 9 de febrero, el Juez Richard Concepción ordena 18 meses de prisión preventiva para el Ex Mandatario y el 10 de febrero el Gobierno lo incluye en la lista de los más buscados del Perú emitiendo una orden de captura internacional (extradición previa al Perú) para lo cual se ofrece una recompensa de S/100 000 por su paradero.

Toledo, que al momento de la orden de su detención preventiva estaba en paradero desconocido, a través de su abogado Heriberto Benítez reclamó la comparecencia restringida como condición para entregarse, lo que se le negó. La justicia peruana lo declaró prófugo e incluso el gobierno ofreció US$ 100 mil de recompensa para quien informara de su paradero. Conocida su ubicación en los Estados Unidos, se le abrió un cuadernillo de extradición. Para sustentar este pedido ante las autoridades estadounidenses, el Ministerio Público hizo un informe detallado sobre las diversas medidas que Toledo habría dado a favor de la empresa brasileña y que lograron dificultar la intervención y competencia de otras empresas. Entre estos actos, estarían la emisión de resoluciones y decretos relacionados con el proyecto de la Carretera Interoceánica, exoneración del SNIP y reducción de exigencias normativas.

El fiscal también señaló que Camargo Correa depositó 91 mil dólares a una cuenta del banco Citibank del Reino Unido perteneciente a la empresa Tailbridge LTD. Sin embargo, no se sabía quién era el dueño de la cuenta. En total se habría recibió 4 millones de la empresa Brasileña, esto fue confirmado por el colaborador y examigo de Toledo, Josef Maiman.

El 16 de julio de 2019, el expresidente Alejandro Toledo fue detenido preliminarmente en los Estados Unidos, por mandato de la justicia de dicho país. Ello, en atención a su primer pedido de extradición de parte de la justicia peruana, por delitos de tráfico de influencias, colusión y lavado de activos. Toledo actualmente se encuentra en la correccional Maguire, en el condado californiano de San Mateo, donde el sistema de visitas es más flexible.

Después de su período presidencial, Toledo fue a los Estados Unidos, en donde fue un distinguido profesor visitante en residencia en el Centro de Estudios Avanzados en Ciencias del Comportamiento en la Universidad de Stanford durante el año académico 2006-2008. A la par también fue Profesor Visitante en la Institución Hoover, también de la Universidad de Stanford.

De 2007 a 2008 fue Distinguido Profesor Visitante del Instituto Freeman Spogli y Profesor Visitante del Centro para la Democracia, el Desarrollo y el Estado de Derecho, en la Universidad de Stanford.

En el año 2009 entró a la Institución Brookings, en donde es Distinguido Miembro Senior (No Residente) en Política Exterior, Economía Global y Desarrollo. Ese mismo año fue nombrado Distinguido Profesor Visitante en la Escuela de Estudios Internacionales Avanzados Paul H. Nitze (Universidad Johns Hopkins), con sede en Washington D. C..

En el año 2012 regresó a la Universidad de Stanford como Profesor en el Centro para la Democracia, el Desarrollo y el Estado de Derecho.

En la Universidad Stanford es investigador del Centro para Estudios en las Ciencias del Comportamiento.

Desde octubre de 2006, forma parte del Centro Carter, del expresidente de EE.UU. Jimmy Carter (1977-1981); participando como colíder en la observación de las elecciones generales de Nicaragua el 5 de noviembre de 2006.

Al mismo tiempo, Alejandro Toledo fundó el Centro Global para el Desarrollo y la Democracia, con oficinas en América Latina, Estados Unidos y la Unión Europea. Junto con otros diecinueve expresidentes latinoamericanos, Alejandro Toledo trabajo del 2007 al 2009 en la elaboración de la Agenda Social para la Democracia en América Latina para los Próximos 20 Años, que fue presentada en noviembre de 2009 en el marco de la Cumbre Iberoamericana en Estoril, Portugal.

Toledo también es miembro del Comité de Desarrollo Humano de la Organización de las Naciones Unidas para Latinoamérica, así como del Club de Madrid.

En el año 2007, luego del polémico en Venezuela, Toledo escribió el artículo "Silence = Despotism (Silencio=Despotismo)" en New York Times en el cual criticó fuertemente a Hugo Chávez por atentar contra la libertad de expresión; además hizo un llamado a organismos internacionales como la OEA para que intervengan en el cierre del medio de comunicación.

En julio de 2010 el Senado de los Estados Unidos homenajeó a Toledo por sus políticas que contribuyeron en gran medida a la mejora de la economía peruana y porque ayudó a dar grandes pasos en las áreas de educación, salud y reducción de la pobreza.

A lo largo de los años ha recibido más de 65 doctorados en diferentes universidades del mundo, entre ellas la , la Universidad de Pekín, la Universidad Estatal de Tiflis y la Universidad Nacional Mayor de San Marcos.

Toledo se presentó nuevamente a las elecciones generales del Perú de 2011, donde quedó en cuarto lugar.

Desde entonces ha mantenido una apretada agenda internacional, que lo ha llevado, entre otras cosas, a reunirse con la Secretaria de Estado de los Estados Unidos, Hillary Rodham Clinton, para un replanteamiento de las relaciones de los EE. UU. con países de Iberoamérica. De igual forma, ha participado en eventos internacionales en Guatemala, donde abogó por integración, y desarrollo social y el fortalecimiento de la democracia, y en Israel, en el marco de la Tercera Conferencia Presidencial Israelí.

En el Oslo Freedom Forum de 2011, Toledo dejó en claro que no apoyaría ni a Ollanta Humala ni a Keiko Fujimori en las elecciones presidenciales de Perú, “Nos mantendremos vigilantes y defensores de la democracia en el Perú" añadió. También afirmó su papel en el futuro añadiendo: “Combatiremos a quienes siendo elegidos democráticamente luego cambian la Constitución y utilizan la careta de la democracia para gobernar autocráticamente". No obstante, días después apoyó abiertamente al candidato nacionalista en estos términos: 

Tras la victoria de Humala se iniciaron especulaciones sobre un posible co-gobierno, debido a las reuniones que tuvo el expresidente con Humala; sin embargo Toledo desmintió los rumores y anunció su apoyo a la gobernabilidad del país.

En septiembre de 2011, Toledo reunió en Lima a 13 expresidentes de Iberoamérica y Europa para la conmemoración de los 10 años de la Carta Democrática Interamericana, en el marco de la VI Cumbre de expresidentes.

En octubre de 2011 participó como Jefe de la misión de observadores del Instituto Nacional Demócrata para los Asuntos Internacionales en las elecciones de la Asamblea General en Túnez. En noviembre del mismo año fue invitado por el Rey Mohamed VI de Marruecos para hablar sobre los efectos de la primavera árabe y la democracia; en esta visita se le entregó las llaves de la ciudad de Rabat; luego de ello criticó ante la prensa internacional las formas de gobierno en Venezuela y Nicaragua.

En septiembre de 2012 fue nombrado entre los "20 Héroes de la Inclusión del Mundo" por la revista Quarterly Americas del Consejo de las Américas, por sus logros como presidente al disminuir la pobreza de 54,4 % a 45 %.

En marzo de 2013 participa como conferenciante principal en la XV Conferencia sobre Latinoamérica que realiza la Escuela de Negocios de la Universidad de Harvard, donde expuso sobre los cambios que la región latinoamericana y el desarrollo económico. Luego de ello participó en el Palm Beach Strategic Forum sobre el crecimiento económico, dado en Florida. Del mismo modo fue panelista del foro sobre Latinoamércia organizado por The Annenberg - Drier Commission - en California; del Ambrosetti Forum en Cernobbio, Italia y del World Business Forum Latinamerica en México.

En diciembre de 2013 participa en la conferencia anual del Club de Madrid, realizada en la ciudad de Coolum, Queensland. En dicha reunión, diferentes ex mandatarios hablaron sobre el futuro de América Latina y el impacto que tiene los cambios económicos mundiales en las sociedades.

A inicios de 2015 publica el libro "La sociedad compartida. Una visión para el futuro global de América Latina".




</doc>
<doc id="4784" url="https://es.wikipedia.org/wiki?curid=4784" title="Silesia">
Silesia

Silesia (; ; ; en dialecto alemán silesio: "Schläsing") es una región histórica de Europa Nororiental que hoy está casi enteramente en Polonia con pequeñas partes en la República Checa y Alemania. Su emblema regional es un águila negra en campo dorado. 

En 990, Miecislao I de Polonia conquistó esta región a Boleslao II de Bohemia. Después de haber pertenecido a Polonia, Silesia fue anexionada de nuevo a la corona de Bohemia, luego a Austria —que dominó Bohemia desde 1526— y después a Prusia, a partir de 1763 en virtud del tratado de Hubertusburg que puso fin a las guerras de Silesia.

El siglo XIX vio profundas transformaciones en la región cuando se explotó el carbón, con el nacimiento de grandes ciudades alrededor de la industria de la metalurgia. Después de 1871, formó parte de la Alemania unificada. El siglo XX también supuso grandes cambios en la región, recuperando Polonia una parte tras la I Guerra Mundial y finalmente casi toda la Silesia tras la II Guerra Mundial.

Históricamente la región se ha considerado dividida en la Baja Silesia, que corresponde al sector occidental, y la Alta Silesia, al sector más oriental y meridional.

Los nombres de Silesia, en los diferentes idiomas comparten probablemente su etimología —; ; en antiguo polaco: "Ślążsk[o]"; en silesio: "Ślůnsk"; ; en alemán silesio: "Schläsing"; ; ; ; en alto sorabo: "Šleska"; en bajo sorabo: "Šlazyńska")—. Todos los nombres están relacionados con el nombre de un río (ahora Ślęza) y una montaña (monte Ślęża) localizado en la parte centromeridional de Silesia. La montaña sirvió como lugar de culto.

"Ślęża" está listado como uno de los numerosos nombres topográficos pre-indoeuropeos en la región (véase en la Wikipedia en inglés: ).

Según algunos eslavistas polacos el nombre ‘Ślęża’ [ˈɕlɛ̃ʐa] o ‘Ślęż’ [ˈɕlɛ̃ʐ] está directamente relacionado con las palabras en antiguo eslavo 'ślęg' [ˈɕlɛ̃ɡ] o 'śląg' [ˈɕlɔ̃ɡ], que significan sin niebla, mojado o humedad. hay desacuerdo con la hipótesis de un origen para el nombre "Śląsk" [ˈɕlɔ̃sk] derivado del nombre de la tribu de los silingos, una etimología preferida por algunos autores alemanes.

Poblada hasta 406 por los germanos, silingos, bastarnos y vándalos y despoblada por estos cuando se desplazaron a conquistar al Imperio romano, fue repoblada en los siglos VI-VII por pueblos eslavos como los lusacios y sorabos o serbios blancos, en el oeste, y los croatas blancos en el este. Hacia fines del siglo VII la mayoría de los croatas blancos y gran parte de los serbios blancos marcharon hacia la península de los Balcanes estableciéndose en las actuales Croacia y Serbia, siendo entonces Silesia repoblada por polacos y checos.

Adquirida en 990 por el príncipe polaco Miecislao I a los bohemios (checos), Silesia fue escenario de numerosas guerras polaco-bohemias durante los siglos X al XII. En 1138, el príncipe polaco Boleslao III el Bocatorcida dividió el país entre sus cuatro hijos: Silesia pasó a depender de su hijo mayor, Vladislao II el Desterrado, que pronto tendría que salvarse con el destierro al ser atacado por sus hermanos menores. Al volver del exilio, sus hijos Boleslao el Alto y Miecislao el Piernas Torcidas, se dividieron la región, división que ha permanecido hasta la actualidad. (ver: Ducados de Silesia)

Con el dominio de numerosos príncipes Silesia, aunque dividida, permaneció como la región más desarrollada de Polonia. En el siglo XIII, con el acuerdo principal comenzó la recolonización alemana del sur de Silesia. Los príncipes fundaron numerosas ciudades y minas. En 1241, en la batalla de Liegnitz, el ejército del príncipe Enrique II —que murió en la batalla— logró detener la invasión de los mongoles.

Con el crecimiento de la hegemonía regional del unificado reino bohemio, en el siglo XIV los principados silesianos quedaron feudados por los reyes del sur, aunque todavía bajo el poder de los príncipes de la dinastía Piast. La reunificación del reino polaco no cambió la situación política, tampoco el corto reinado de la dinastía polaco-lituana en Praga. El último Piast murió en el año 1675. 

En 1335, en el contrato de Trenčín, el rey polaco Casimiro III renunció «para siempre» a Silesia en favor del rey de Bohemia. En 1355, el emperador romano germánico Carlos IV (que desde 1347 era rey de Bohemia) incorporó Silesia al Sacro Imperio Romano Germánico. 

En 1526, el reino bohemio, junto con toda Silesia, pasó a manos de los Habsburgo. Con la llegada de la Reforma comenzaron los conflictos entre la mayoría protestante de la población y la dinastía católica. 

Tras la Guerra de Sucesión Austriaca, la mayor parte de Silesia quedó bajo el control de la Prusia de Federico II el Grande. Con la intensa colonización e industrialización empezó un tiempo de gran prosperidad.

La creación del Imperio alemán en 1871 incluyó a Silesia dentro de la Unificación alemana impulsada por Otto von Bismarck.

Al final de la Primera Guerra Mundial, después del plebiscito y los tres levantamientos polacos, una gran parte de la Alta Silesia formó parte de la Polonia renacida. En 1920 estalló un conflicto militar fronterizo entre Polonia y Checoslovaquia, que acabó con la división de la parte sureña de Silesia. 

Después de la derrota alemana en la Segunda Guerra Mundial, la parte de Silesia todavía perteneciente a Alemania fue cedida a Polonia a cambio de las tierras orientales polacas anexadas por la Unión de Repúblicas Socialistas Soviéticas (URSS) en 1944, cuando expulsaron a los alemanes y declararon unilateralmente que su frontera con Polonia era la establecida en la Línea Curzon de 1919. Toda la población alemana (más de 4 millones de personas) fue desterrada y en su lugar la región fue colonizada con polacos, parcialmente polacos de las regiones de Vilna y Leópolis.

La parte checa de Silesia anexionada por la Alemania nazi en el marco de la integración de los Sudetes, también fue vaciada de su población alemana. Un 20% de la población polaca vive actualmente en Silesia, pero apenas hay familias cuyos antepasados sean naturales de la región. 

Desde 2004, la totalidad de Silesia forma parte de la Unión Europea.

La mayor parte de Silesia es relativamente llana, aunque su límite meridional es generalmente montañoso. Se encuentra principalmente en una franja que corre a lo largo de ambas orillas del río Oder (Odra) en su curso superior y medio (Odra), aunque se extiende hacia el este hasta la cuenca superior del río Vístula. La región también incluye muchos afluentes del Oder, entre ellos el Bóbr (y su afluente el Kwisa), el Barycz y el Nysa Kłodzka. Las montañas de los Sudetes corren a lo largo de la mayor parte del extremo sur de la región, aunque en su extremo sudeste alcanzan las Beskydy silesias y las Beskydy moravo-silesias, que pertenecen a la cordillera de los montes Cárpatos.

Históricamente, Silesia estaba limitada al oeste por los ríos Kwisa y Bóbr, mientras que el territorio al oeste del Kwisa estaba en Alta Lusacia (antes "Milsko"). Sin embargo, debido a que parte de la Alta Lusacia se incluyó en la provincia de Silesia en 1815, en la alemana Görlitz, Niederschlesischer Oberlausitzkreis y las zonas vecinas se consideran partes de la Silesia histórica. Esos distritos, junto con el polaco voivodato de Baja Silesia y partes del voivodato de Lubusz, conforman la región geográfica de la Baja Silesia.

Silesia ha experimentado una extensión nocional similar en su extremo oriental. Históricamente se extendía sólo hasta el río Brynica, que la separaba de Zagłębie Dąbrowskie en la región de la Pequeña Polonia. Sin embargo, para muchos polacos de hoy, Silesia ("Śląsk") se entiende para cubrir toda la zona alrededor de Katowice, incluyendo Zagłębie. Esta interpretación tiene sanción oficial en el uso del nombre de Silesia ("województwo śląskie") para la provincia que cubre esta área. De hecho la palabra "Śląsk" en polaco (cuando se usa sin calificación) ahora comúnmente se refiere exclusivamente a esta área (también llamada "Górny Śląsk" o Alta Silesia).

De igual modo que en área de Katowice, la Alta Silesia histórica también incluía la región de Opole (voivodato de Opole) y la Silesia checa. La Silesia Checa se compone de una parte de la Región de Moravia-Silesia y del Distrito de Jeseník en la Región de Olomouc.

La región histórica de Silesia pertenece actualmente casi por entero a Polonia, con pequeñas partes en la República Checa y en Alemania

La parte polaca de Silesia está dividida en cuatro voivodatos (provincias):


Hay que recalcar que estos son los voivodatos que organizan administrativamente partes de la región histórica de Silesia, aunque no todo su territorio pertenecía a Silesia. (Se puede hacer la comparación con La Mancha española, que toda está en Castilla-La Mancha, pero que no toda la comunidad autónoma española es parte de La Mancha).

La parte de Silesia que ahora pertenece a la República Checa depende de 14 provincias y limita con los distritos de Zlin y Olomouc en la República Checa, Zilina en Eslovaquia y Polonia con frontera polaca, con un área aproximada de y un millón de habitantes.

Las ciudades históricas más importantes de la región son las siguientes (con la población en 2006):




</doc>
<doc id="4785" url="https://es.wikipedia.org/wiki?curid=4785" title="Alan García">
Alan García

Alan Gabriel Ludwig García Pérez (Lima, 23 de mayo de 1949-Ib., 17 de abril de 2019) fue un abogado, orador y político peruano. Ejerció como presidente del Perú en dos mandatos no consecutivos: de 1985 a 1990 y de 2006 a 2011.

En su vida política se desempeñó como (1978-1979), (1980-1985) y (1990-1992). Como miembro del Partido Aprista, recibió formación política del líder fundador, Víctor Raúl Haya de la Torre, y fue uno de sus discípulos predilectos. Ocupó diversos cargos partidarios y fue el presidente de Alianza Popular Revolucionaria Americana. Desde sus inicios, se caracterizó por su inflamado verbo y oratoria castelariana.

Llegó a la presidencia en las elecciones generales de 1985, en las que ganó en la primera vuelta luego de la renuncia de Alfonso Barrantes. Esta primera gestión de gobierno se caracterizó por una insólita hiperinflación, un recrudecimiento de los embates del terrorismo liderado por Sendero Luminoso, y por diversos actos de corrupción que involucraban a gente del régimen aprista que repercutió en un gran descontento social.

A la caída del fujimorato, García retornó al país para postular en 2001; sin embargo, fue vencido por Alejandro Toledo, durante cuyo gobierno (2001-2006) fue el líder de la oposición. Triunfó en las elecciones de 2006 al derrotar a Ollanta Humala (UPP) en segunda vuelta. Su segundo gobierno se caracterizó por la continuación de proyectos de inversión pública, el crecimiento económico del país y la reestructuración de las relaciones diplomáticas; sin embargo, también fue acusado de actos de corrupción. Abandonó el poder tras las elecciones de 2011, siendo Humala su sucesor Se retiró de la política partidaria después de no superar la primera vuelta en las elecciones de 2016.

El 17 de abril de 2019, García se suicidó disparándose en la cabeza cuando la policía se preparaba para detenerlo preliminarmente por asuntos relacionados al caso Odebrecht. Fue trasladado en estado grave al hospital Casimiro Ulloa, en el cual permaneció por más de tres horas en la sala de operaciones, tiempo durante el cual tuvo tres paros cardiorrespiratorios antes de fallecer. Desde la historiografía peruana García se convirtió en el segundo jefe de Estado en suicidarse después de Gustavo Jiménez, que lo hizo en 1933.

Alan García Pérez nació el 23 de mayo de 1949 en la ciudad de Lima, en el seno de una familia de clase media, estrechamente ligada al APRA. Fue hijo de Carlos García Ronceros y Nytha Pérez Rojas. Cursó sus estudios en el Colegio Nacional José María Eguren del distrito limeño de Barranco.

Su madre, Nytha Pérez Rojas, hija de Alejandro Pérez Aragón y Celia Rojas Ladrón de Guevara, fue fundadora del APRA en Camaná. Su padre, Carlos García Ronceros, hijo de José Carlos García Grillo y Zoyla Luz Ronceros Rendón, fue secretario de organización de dicho partido durante el gobierno del general de división Manuel A. Odría, durante el cual se había declarado la ilegalidad del APRA. Fue arrestado durante el gobierno de este y por ese motivo, no conoció a su hijo sino hasta pasados cinco años.

En la etapa del colegio, Alan García descubrió el poder que tenían las palabras, objeto que le valió varios premios escolares en oratoria y un destacado "verbo" que le sería útil al iniciarse como militante aprista. Siendo aún muy joven, García se unió a la Juventud Aprista Peruana, recibiendo su carné de militante a los diecisiete años.

Realizó sus estudios de pregrado en la Pontificia Universidad Católica del Perú y luego en la Universidad Nacional Mayor de San Marcos, recibiendo su título en leyes de esta última en 1971.

En 1972 se mudó a Europa y asistió a la Universidad Complutense de Madrid donde realizó cursos de doctorado en derecho de 1972 a 1974, y a la Universidad de París donde realizó cursos doctorales en sociología de 1974 a 1977; no obstante, nunca llegó a obtener el grado respectivo, situación que solo fue de dominio público cuando la prensa investigó el tema.

Se casó en 1973 con Carla Francisca Buscaglia Castellano, con quien tuvo una hija en 1975: Carla García Buscaglia. La pareja se divorció en 1980.

En 1975, en un congreso en España, conoció a Pilar Nores, abogada argentina con quien contrajo matrimonio en 1983. García y Nores tuvieron cuatro hijos: Josefina (1977), Gabriela del Pilar (1984), Luciana Victoria (1985) y Alan Raúl (1988). En 2008, los entonces presidente y primera dama se separaron.

En febrero 2005 tuvo un sexto hijo con la economista Elizabeth Roxanne Cheesman Rajkovic.

Después de vivir varios años en París, García fue llamado por el fundador y entonces líder del APRA, Víctor Raúl Haya de la Torre, para regresar a la vida política peruana en 1978, después de que la administración de Francisco Morales Bermúdez presidiera el regresó al gobierno civil y permitiera la reorganización de otros partidos políticos. Llegaría al gobierno años después, siendo reconocido este periodo, en materia económica, como uno de los más difíciles en la historia del país, debido a la hiperinflación. Al terminar su mandato permaneció en el país hasta 1992, año del autogolpe de Alberto Fujimori, luego de lo cual se asiló en Colombia, posteriormente se dirigió a París y años después sería elegido nuevamente presidente del Perú.

Desde pequeño, Alan Gabriel frecuentaba la Casa del Pueblo (sede principal del Partido Aprista), donde recibía tratos con el líder fundador del APRA, Víctor Raúl Haya de la Torre. Alan tomaba a Víctor Raúl como alguien más que un líder: como un padre; en una entrevista el comentó lo siguiente sobre Haya de la Torre:

Junto a Alberto Borea Odría, Luis Alva Castro y otros adolescentes, Alan fue discípulo directo de Haya de la Torre. Por consejo del "patriarca del APRA" no postuló a la Universidad Federico Villarreal (ligada al APRA) sino a la Universidad Católica, con el fin de contrarrestar el dominio sobre esta de los socialcristianos y de la centroizquierda. Posteriormente se trasladaría a la Universidad San Marcos, donde se graduó en leyes.

Durante su permanencia en Madrid, García junto al también aprista Javier Valle Riestra realizaron conferencias y manifiestos en contra de la dictadura militar del Perú. Ante posibles represalias del consulado peruano en Madrid, García se trasladó a París para continuar con sus estudios y actividad contra el régimen militar.

En 1978, el presidente de la República del Perú, Francisco Morales Bermúdez convocó a la Asamblea Constituyente de 1978, para promulgar una nueva carta magna. El Partido Aprista participó de dicho proceso electoral, encabezando la lista Víctor Raúl Haya de la Torre. Junto con Haya de la Torre, fueron electas diversas personalidades apristas, incluidos el propio García Pérez. Así, García se convirtió en diputado constituyente, siendo junto con Xavier Barrón (PPC) el más joven de dicho ente.

García Pérez acompañó a su maestro Haya de la Torre, siendo su principal consejero. Él, junto con los otros miembros apristas vieron como la salud de Haya se deterioraba poco a poco. Terminada la Asamblea, García Pérez ya se había hecho conocido por sus dotes de oratoria y de convencimiento de las masas.

En 1980 fue elegido diputado por el Departamento de Lima, dos años después logró ser elegido secretario general del APRA candidato presidencial después, del APRA.

Desde la cámara de diputados formó parte de la oposición al gobierno de Fernando Belaúnde.

Fue elegido como candidato del APRA el 12 de febrero de 1984, luego de una elección por voto directo y secreto de las bases del partido, mecanismo democrático que por primera vez se aplicaba en un partido peruano. Sus discursos se centraron en la reivindicación del pueblo trabajador, llamando a la superación de los problemas de la nación; haciendo un llamado a la derecha y a la izquierda. En la encuesta de diciembre de 1984 de Apoyo S.A., García tenía un 47,4% de preferencias electorales, mientras que Alfonso Barrantes tenía 21,30%.

Su plancha presidencial para las elecciones generales de 1985 estuvo conformada por:

En las elecciones del 14 de abril de 1985, Alan García se presentó como candidato presidencial del APRA. Los resultados de la primera vuelta arrojaron que superó la barrera del 50% contabilizando los votos válidos, pero la Constitución requería que también se superara el 50% del total de votos emitidos, por lo cual correspondía realizarse una segunda vuelta entre el candidato aprista y el candidato por IU, Alfonso Barrantes Lingán.


Si bien la Constitución señalaba que para ser presidente se debía obtener el 50% más uno del total de votos emitidos (por lo cual correspondía una segunda vuelta), en la práctica, los resultados oficiales de la primera vuelta señalaban que la suma de los votos de todos sus adversarios no alcanzaban a igualar la cantidad de votos que había obtenido el candidato del APRA. La renuncia del candidato que quedó en segundo lugar, Alfonso Barrantes Lingán, a participar en la segunda vuelta electoral hizo que García fuese declarado ganador. Alan García, tenía entonces tan solo 35 años y se convirtió en el primer presidente aprista desde la fundación de su partido.

García asumió el gobierno en 1985, luego de ganar las elecciones generales de 1985 con un gran apoyo por parte del pueblo debido a su juventud, radicalismo y discurso anti-imperialista, muy de moda en aquel entonces. Por tales razones, en sus primeros meses de gobernante concurrió a conferencias internacionales como la Asamblea General de las Naciones Unidas y la FAO, en las cuales expuso su tesis de la deuda externa a favor de los países pobres, con lo cual fue inicialmente reconocido por parte de los jefes de estado asistentes relacionados con dichos regímenes, así como por un sector de la prensa extranjera.

Los primeros dos años del gobierno de García fueron conocidos por aparentar vitalidad y autoridad similares a los de su mentor, pero como presidente del Perú. García acostumbraba a dar "balconazos" (discursos en los balcones de palacio), mostrando sus dotes de oratoria y anunciando medidas de su gobierno; este era escuchado y aplaudido masivamente. La aprobación inicial de García en septiembre de 1985 fue de 96,4%, Cuando la capacidad de gasto del estado fue agotada entonces comenzaron múltiples problemas los malos manejos económicos alcanzó su más bajo nivel en enero de 1989 (9%).

A corto plazo, las medidas adoptadas aparentemente dieron resultados positivos en los primeros meses de gestión. Aumentando los salarios, reduciendo las tasas de interés bancario, devaluando la moneda y controlando el tipo de cambio, en septiembre de 1985, la tasa de inflación bajó a 3,5 % (comparada con el 12,5 % en abril del mismo año). Hacia el segundo trimestre de 1986, la economía dio señales de una supuesta recuperación. Los sectores que dependían de la demanda interna (manufactura, construcción, agricultura) crecieron a razón de subsidios, no así los sectores dedicados a la exportación primaria (minería, pesca). En consecuencia, a fines de 1986, la economía creció 10 %. Fue el mayor crecimiento desde los años 50, con ello García parecía disfrutar entonces de una popularidad récord en América Latina. Sin embargo, cuando la capacidad del gasto público se agotó, comenzaron las dificultades económicas pues el dinero de las reservas estaban agotándose.

La política económica de García se caracterizó por presentar, en su política cambiaría, dos tipos de cambio, uno oficial llamado dólar Mercado Único de Cambios (dólar MUC) y otro que existía en el mercado negro, denominado mercado paralelo. Y en su política monetaria, ejecutó excesivas emisiones inorgánicas de moneda nacional. Igualmente se rechazaron los consejos del Fondo Monetario Internacional y se limitó el pago de la deuda externa al 10% de los ingresos que por concepto de exportaciones obtenía el país. Esta decisión causó el retraso en el pago de la deuda externa y que el país fuera declarado, en un principio, como "valor deteriorado" y luego como "inelegible" por el FMI en 1986.

Sus medidas se caracterizaron por ser de corte marxista pues en ese momento las ideas socialistas aún se encontraban vigentes en el país, el punto que causó la ruptura de su gobierno fue la intención de estatizar la banca privada como una supuesta forma de controlar la inflación que, al 28 de julio de 1987, ya resultaba incontrolable. En efecto, los indicadores macroeconómicos señalan que el Perú, durante su mandato, llegó a sufrir una hiperinflación de 1722,3% en 1988 y 2775% en 1989. Para inicios de 1990 estos índices alcanzaron el 854% (inflación acumulada a julio de 1990). La devaluación de la moneda fue altísima y durante su gobierno hubo que cambiar dos veces la moneda oficial (sol e inti) debido a que quedó rápidamente sin valor. Ello derivó en una gran especulación y en la escasez de productos de primera necesidad.

Otro tema que sacudió el gobierno de García fue la actividad terrorista que se inició durante el anterior gobierno de Fernando Belaúnde Terry pero que alcanzó los picos más altos de violencia en los años de 1986 y 1988. Dentro de este contexto se produjo el caso de la matanza de terroristas amotinados en los distintos centros penitenciarios de Lima el 19 de junio de 1986 (Matanza de las prisiones).

Durante el gobierno de Alan García, junto a la violencia subversiva, que costó miles de vidas, se realizaron actos de represión militar, como la de la matanza de las prisiones y la masacre de decenas de campesinos en el pueblo ayacuchano de Cayara en 1988. Aunque inicialmente García mostró interés en frenar las violaciones a los derechos humanos, tras la matanza de los penales, permitió que continuase la violencia contrasubversiva de las fuerzas armadas y se formaron escuadrones de la muerte (Comando Rodrigo Franco), los que amedrentaron a sospechosos de terrorismo y a críticos de la política antiterrorista.

A partir de 1988 y 1989 los grupos terroristas intensificaron su ola de atentados en Lima y varias otras ciudades frente a la impotencia gubernamental.

La controversia se volvió a dar cuando a menos de veinte días de la transferencia al nuevo gobierno, Víctor Polay, "Comandante Rolando" y 47 militantes del Movimiento Revolucionario Túpac Amaru (MRTA) lograron fugar del penal de "máxima seguridad" Miguel Castro Castro a través de un túnel de 330 metros construido desde fuera del penal. La construcción no contaba con conexiones de agua ni desagüe, instalaciones de servicios de alumbrado y tampoco un respiradero que facilitaría el trabajo operativo. 

Más allá del hecho mismo, la repercusión obtenida por el MRTA, a nivel nacional e internacional, constituyó un duro cuestionamiento no solo a la estrategia antisubversiva del gobierno, sino también a la capacidad operativa de las autoridades policiales y penales.

La oposición al gobierno creció significativamente desde el intento de estatización de la banca, una medida que fue sumamente impopular y disparó un enérgico movimiento de protesta de la derecha encabezado por el escritor y periodista Mario Vargas Llosa, este movimiento finalmente evolucionaría en la alianza política FREDEMO (que incluía al Partido Popular Cristiano, Acción Popular y al Movimiento Libertad) que postuló sin éxito en las elecciones de 1990 con Vargas Llosa como candidato presidencial. En su último mensaje a la nación, el 28 de julio de 1990, el Congreso no le permitió hablar, interrumpiéndolo constantemente mediante carpetazos y pifias.

La inestabilidad económica y terrorismo provocaron el descontento de la población peruana que en las elecciones de 1990 eligió como presidente a Alberto Fujimori.

En 1990 García entregó la presidencia dentro de un marco totalmente contrario al de 1985. Su popularidad se encontraba en 21%, y en la ceremonia de entrega de mando a Alberto Fujimori, después de entregar la banda presidencial al presidente del Congreso, tal como lo establece el protocolo oficial, García abandonó el hemiciclo del Congreso y no presenció el primer mensaje de su sucesor. Luego abandonó el país, con acusaciones de corrupción y asesinatos extrajudiciales del comando Aprista Rodrigo Franco.

El Artículo 166 de la constitución de 1979 establecía que son senadores vitalicios los expresidentes constitucionales de la República, participando de las sesiones del congreso. García asistía al Senado e intervenía en las sesiones desde el inicio de la legislatura en 1990; sin embargo su función se vio envuelta en escándalos por las acusaciones de actos de corrupción de su gobierno.

El 18 de octubre de 1991, la cámara de senadores debatió la propuesta hecha por la cámara baja de encausar a García por presuntos delitos de enriquecimiento ilícito y contra la fe pública, supuestamente cometidos cuando desempeñó funciones públicas. El pleno del senado resolvió suspender al expresidente Alan García en el ejercicio de sus funciones como senador vitalicio y someterlo a juicio por presunto enriquecimiento ilícito durante su mandato presidencial con 38 votos a favor y 17 en contra.

El 24 de noviembre, el fiscal de la nación acusó ante la Corte Suprema de Justicia a Alan Garcia por enriquecimiento indebido a costa de los presupuestos del Estado; sin embargo, en diciembre del mismo año el vocal del caso manifestó que no había encontrado "pruebas suficientes" para sustentar la acusación contra García; tras ello la Corte Suprema declaró como improcedente juzgar al expresidente.

Culminado el proceso de investigación judicial, la Mesa Directiva de la Cámara de Senadores devolvió el fuero parlamentario García mediante resolución del 20 de marzo de 1992. De este modo, el expresidente recuperó su inmunidad parlamentaria y, con ella, todos los derechos reconocidos por la Constitución y las leyes peruanas a los Senadores Vitalicios.

El 5 de abril de 1992, Alberto Fujimori dio el autogolpe, mediante el cual cerró el congreso, intervino al Poder Judicial y a otros poderes públicos. Debido a estas acciones, diversos políticos fueron perseguidos e impedidos de salir de sus domicilios. 

A finales de mayo, Alan García ingresó a la residencia del embajador de Colombia en el Perú para pedir asilo político, el cual le fue concedido el día 1 de junio por el Gobierno del presidente César Gaviria. El expresidente abandonó el Perú mediante un salvoconducto que le permitió abordar un jet de la Fuerza Aérea Colombiana que lo trasladó, junto al entonces diputado Jorge Del Castillo, con destino a Bogotá.

García llegó al aeropuerto militar de Catam y en declaraciones a la prensa prometió luchar contra la dictadura de Alberto Fujimori. El régimen le abrió procesos por enriquecimiento ilícito y por diversas acusaciones de corrupción; tras ello se solicitó la extradición de García al gobierno colombiano, la cual fue denegada.

En 1994, la Comisión de Derechos Humanos de la Organización de Estados Americanos denunció al Gobierno de Fujimori por violación de los derechos a la libertad, a la seguridad y a la debida defensa de Alan García y le pidió al gobierno peruano dejar sin efecto los procesos iniciados.

En abril de 1995, el Congreso levantó la inmunidad parlamentaria a Alan García por las acusaciones de haber recibido sobornos por parte del consorcio italiano Tralima para la construcción del tren eléctrico de Lima. De esta manera, la Sala Civil de la Corte Suprema de Justicia pidió nuevamente la extradición de García al Gobierno de Colombia, la cual fue denegada debido a que García pasó a vivir en París.

Durante los años transcurridos entre 1993 y el 2001, Alan García no participó activamente en la política peruana, salvo en la publicación de algunas obras sobre la política de su primer gobierno y denunciando las violaciones a los derechos humanos cometidas por el gobierno del presidente Alberto Fujimori. En contadas ocasiones, Alan García apareció en la televisión y radio peruanas desde Bogotá, Colombia.

En 2001, La Corte Suprema de Justicia de Perú declaró prescritos los delitos que se le imputaron al finalizar su primer mandato. García no regresó al país hasta el año 2001, cuando ya habían prescrito los delitos relacionados con las denuncias de corrupción en su gobierno.

Su plancha presidencial estuvo compuesta por el burgomaestre de Trujillo, José Murgia Zannier (Primera Vicepresidencia), y por el congresista aprista, Jorge Del Castillo Gálvez (Segunda Vicepresidencia).

García regresó al país el 27 de enero de 2001 y postuló nuevamente a la presidencia el mismo año, cuya elección se daría el 8 de abril. Su candidatura fue muy controvertida por el mal gobierno que realizó (1985-1990), pese a todo logró su pase a la segunda vuelta con un gran apoyo popular principalmente de militantes y simpatizantes del Partido Aprista (concentrados en la costa norte peruana), desplazando a la candidata Lourdes Flores, favorita a pasar a la segunda vuelta junto con Alejandro Toledo. Tal como indicaban las encuestadoras semanas antes de las elecciones, ningún candidato superó el 50 % más 1 voto requeridos, por lo que los dos participantes que obtuvieron mayor votación (Alejandro Toledo y Alan García), pasaron a una segunda vuelta, a darse el 3 de junio de 2001.


La segunda vuelta se inició con duros ataques entre uno y otro candidato, y con la propuesta del voto en blanco como crítica a los candidatos dirigida principalmente por Jaime Bayly y Álvaro Vargas Llosa; y culminó con el tan esperado debate presidencial dado el 19 de mayo de 2001, en el hotel Marriott de Lima.

Las encuestas mostraban una alta diferencia entre ambos candidatos, resultando favorito Alejandro Toledo Manrique gracias a su mayor popularidad y a su lucha democrática contra el régimen de Alberto Fujimori. El día de las elecciones (3 de junio), en el tradicional "flash electoral" la diferencia entre ambos candidatos disminuyó notablemente, aunque de todos modos la victoria se la había asegurado Alejandro Toledo con más de 5 % de diferencia. Con estos resultados Alan García admitió su derrota y declaró un apoyo en la medida de lo posible al futuro presidente Alejandro Toledo.


Se dedicó a la docencia universitaria en la Universidad San Martín, donde era rector el Ing. Chang. También participó, en su condición de líder del Partido Aprista en distintas actividades organizadas por grupos opositores al régimen constitucional, entre las que se encuentra el Paro Nacional organizado por la CGTP, llevado a cabo el 14 de julio de 2004, donde el líder aprista Alan García pateó por la espalda al ciudadano Jesús Lora porque le obstaculizaba el paso, el hecho fue registrado por la prensa y desató un escándalo político.

Por otro lado, buscó acercamientos políticos con distintas agrupaciones para conformar el denominado "Frente Social" con miras a las elecciones presidenciales del año 2006.

Su plancha presidencial está integrada por el Almirante (R) de la Marina de Guerra del Perú, Luis Giampietri Rojas (Primera Vicepresidencia), quien logró conseguir una curul por el Callao y la Ex-Teniente Alcaldesa de Arequipa, Lourdes Mendoza del Solar (Segunda Vicepresidencia) quien logró adjudicarse una curul por dicho departamento.

A las 4:00 p.m. del domingo 9 de abril de 2006, cuando se dieron los tradicionales ""flashes"" electorales, se ubicó por encima de la candidata de Unidad Nacional, Lourdes Flores. Sin embargo conforme pasaba el tiempo se indicó que Flores le sobrellevaba por escasas décimas; esto cambió en los sondeos mayores del 60% donde se apreció que él superó nuevamente a la candidata Flores, manteniendo una tendencia a aumentar la diferencia aunque fuera por décimas.

Luego, con los conteos de los votos del extranjero (que en su mayoría favorecieron a Lourdes Flores) tanto García como Humala redujeron ligeramente sus porcentajes, lo que llevó a un lento acercamiento de la candidata de Unidad Nacional, ubicándose cerca de 0,60 % por debajo de Alan García. Ya pasando el 90 % de actas computadas, Alan García vuelve a alejarse de Lourdes Flores manteniendo esta importante diferencia que lo consolidó como el candidato que pasó con Ollanta Humala a la segunda vuelta.

El informe al 100% señaló que Ollanta Humala (UPP) obtuvo el 30,62 % de votos válidos, seguido por García (APRA) con 24,33 %. En la tercera posición, ya descartada, se ubicó Lourdes Flores (UN) con 23,80 %. Por lo tanto la segunda vuelta se dio el 4 de junio de 2006 entre el candidato de UPP, Ollanta Humala, y el expresidente y candidato por el APRA, Alan García.


Alan García se enfrentó con el candidato presidencial de UPP, Ollanta Humala en la segunda vuelta electoral, realizada el 4 de junio. Mientras Ollanta Humala empezó recorriendo la zona norte peruana (usualmente un sólido bastión aprista), Alan García empezó dirigiéndose al sur, para tratar de obtener algunos votos de una región principalmente nacionalista.

Estas actividades se vieron opacadas por los constante intercambios de palabras entre el presidente venezolano Hugo Chávez y Alan García; donde García calificó a Chávez de "sinvergüenza" y este le respondió calificándolo de "ladrón de cuatro esquinas", debido a su gobierno pasado.

Poco después, Alejandro Toledo sorpresivamente violó las leyes electorales al dar un discurso en el que dio un apoyo explícito a Alan García, al decir que en las elecciones se estaba eligiendo "entre la democracia y el autoritarismo", lo cual hizo que recibiese duras críticas.

Acuerdos entre los negociadores de los partidos que pasaron a la segunda vuelta, Jorge Del Castillo (APRA) y Carlos Torres Caro (UPP), dieron como resultado la programación del debate presidencial, que se desarrolló el domingo 21 de mayo, a las 8:00 p.m. entre los candidatos presidenciales de cada partidario: Alan García (APRA) y Ollanta Humala (UPP), siendo el lugar del debate: el Museo Nacional de Arqueología, Antropología e Historia del Perú, ubicado en Pueblo Libre, Lima, y el moderador el periodista Augusto Álvarez Rodrich.

El 4 de junio, los primeros resultados a boca de urna dieron como ganador a Alan García por un puntaje entre el 5 % y el 10 % sobre su contendor, el candidato de Unión por el Perú, Ollanta Humala. Esta diferencia fue confirmada horas después al conocerse las encuestas por "Conteo Rápido" donde ya se aseguraba la victoria de Alan García.

Mientras Ollanta Humala prefería esperar aun los resultados oficiales del avance de la ONPE, Alan García se dirigiría a la Casa del Pueblo, donde realizaría un discurso y una celebración por el (ya entonces casi seguro) triunfo. Cerca de las 10 de la noche, la ONPE confirmó con sus resultados cerca del 80 % que Alan García había resultado vencedor en la contienda electoral.


Luego de que la Oficina Nacional de Procesos Electorales confirmara al 100.00% de las actas escrutadas que Alan García era el nuevo presidente, el 21 de junio de 2006, el Jurado Nacional de Elecciones lo acreditó como Presidente Electo. Sucedió en el cargo a Alejandro Toledo, quien fuera su rival en el año 2001.

Durante el periodo de transición, García Pérez realizó numerosos anuncios que tomaría su futura administración. Entre los principales anuncios que realizó, destacan el de crear el Ministerio de Pesquería y los futuros Ministerio de Cultura y Ministerio del Deporte. También anunció que su Consejo de Ministros sería paritario, recibiendo elogios de Lourdes Flores, lideresa de la oposición, quien enfocó su campaña electoral en el tema de la igualdad de las mujeres.

García Pérez se reunió el 11 de julio con Lourdes Flores, presidenta del Partido Popular Cristiano y lideresa de Unidad Nacional. La cita que se llevó a cabo en casa de Flores Nano, duró una hora y se trataron los temas de realidad del país. García Pérez descartó haberle ofrecido algún puesto ministerial, anuncio que fue confirmado por Flores Nano en el programa dominical "Pulso Nacional". Sin embargo, unas semanas después, aclaró que le había ofrecido la Presidencia del Consejo de Ministros y un co-gobierno.

El 20 de julio, anunció a dos integrantes de su primer gabinete: José Antonio García Belaúnde como ministro de Relaciones Exteriores y a Luis Carranza como ministro de Economía y Finanzas. El nombramiento de García Belaúnde fue bien recibido por todos los sectores mientras que el Carranza solo fue recibido positivamente por el empresariado y por expertos en temas económicos. La Confederación General de Trabajadores del Perú y los representantes de Unión por el Perú calificaron el nombramiento de Carranza como un continuismo del modelo neoliberal de Alejandro Toledo y Alberto Fujimori, y como un acercamiento a la derecha de Lourdes Flores, quien calificó el nombramiento de Carranza en el MEF como "extraordinario".

Como parte de su vida privada, después de la publicación de un artículo del periodista César Hildebrandt, el 23 de octubre confirmó tener un sexto hijo, fuera del matrimonio con Pilar Nores de García. Federico Danton García Cheesman fue reconocido por el Presidente como su hijo, producto de una relación sentimental con Roxanne Elizabeth Cheesman Rajkovic. Inmediatamente, recibió algunos halagos de los parlamentarios por haber admitido públicamente a su hijo, pero otros como Lourdes Flores (lideresa de la oposición) o el congresista Daniel Abugattás, lo criticaron por ser "infiel" y a Pilar Nores de García por "aceptar cualquier agravio de su esposo".

En octubre de 2007 anunció también la creación de la Oficina Nacional Anticorrupción y a Carolina Lizárraga como Jefa de dicho organismo. El anuncio fue criticado por el Contralor de la República, la Fiscal de la Nación y el Presidente del Poder Judicial ya que podría darse una "duplicidad de funciones".

Para su transmisión de mando, el Ministerio del Interior dispuso que se desplegaran más de 11.000 policías. A la ceremonia asistieron nueve mandatarios y el entonces príncipe de Asturias, Felipe de Borbón. Según informes periodísticos, su discurso presidencial duraría únicamente 30 minutos, pero duró 105. Contrasta con los 120 que uso en su primer discurso en 1985.

A la ceremonia, acudieron los siguientes presidentes: Néstor Kirchner de Argentina, Luiz Inácio Lula da Silva de Brasil, Evo Morales de Bolivia, Michelle Bachelet de Chile, Álvaro Uribe Vélez de Colombia, Alfredo Palacio González de Ecuador, Elías Antonio Saca de El Salvador, entre otros. Representando a otros países, viajaron representantes de Argentina, Uruguay, Estados Unidos, Rusia, China, Luxemburgo, México, Argelia, Japón y demás delegaciones oficiales. Alan García comenzó su periodo presidencial a las 11:41 a.m. (hora Perú), rompiendo el protocolo ya que Mercedes Cabanillas no le puso la banda presidencial, sino que lo hizo él mismo debido a su alta estatura, aunque algunos piensan que lo hizo más bien por su afán de protagonismo.

El 13 de junio de 2006 se reunió en Brasilia con Luiz Inácio Lula da Silva, amigo de la juventud, realizando su primer viaje al exterior como presidente electo. Juntos recordaron que García Pérez fue el único líder latinoamericano que lo recibió en 1989, luego de perder una contienda electoral. En temas bilaterales se habló de la importancia de la Carretera Interoceánica, el cuidado de la zona amazónica y sobre un futuro Tratado de Libre Comercio. García Pérez incluso se animó a decir que de ser brasileño, votaría por Lula.

El 22 de junio viajó a Chile para encontrarse con la presidenta Michelle Bachelet, quien lo recibió en el Palacio de La Moneda. Al encuentro viajó con José Antonio García Belaúnde, su asesor en temas internacionales y posteriormente ministro de Relaciones Exteriores del Perú. García Pérez conversó con Bachelet sobre el fortalecimiento de las relaciones bilaterales, muy dañadas durante los gobiernos de Alejandro Toledo y Ricardo Lagos. Por propias declaraciones de García, se supo que no trataron los temas de Alberto Fujimori ni del conflicto de delimitación marítima. Sobre el tema, García Pérez mencionó que en esta primera reunión, debía hablarse de coincidencias y no de asuntos que distancien.

Encontrándose en Santiago de Chile, se reunió con los principales líderes de la Concertación de Partidos por la Democracia, en especial con los representantes del Partido Socialista de Chile, partido de Bachelet. Su visita a Chile se vio enturbiada cuando un grupo de peruanos residentes en ese país, presentaron una querella contra García Pérez, responsabilizándolo por las matanzas ocurridas durante su gobierno. El 5 de julio se reunió con Álvaro Uribe Vélez y el 6 del mismo mes con Alfredo Palacio González, acudiendo a las citas con García Belaúnde y con Pilar Nores de García. García fue recibido cariñosamente por los colombianos, quienes lo asilaron por un breve tiempo en 1992. En Ecuador sostuvo como principales temas de conversación, mejorar las relaciones bilaterales, muy dañadas durante los últimos años.

Un pilar en del gobierno de Alan García en el ámbito de las relaciones exteriores, es mejorar la relación con Chile. El 28 de julio se reunió con Michelle Bachelet, con quien dialogó sobre el retorno de su país a la Comunidad Andina de Naciones, recientemente abandonada por Venezuela. Bachelet permaneció en el Perú hasta después de la Gran Parada y Desfile Militar, como invitada de honor de García Pérez. El 7 de agosto de 2006, viajó a Colombia para asistir a la toma de mando del reelecto Álvaro Uribe Vélez, acompañado únicamente por José Antonio García Belaúnde y un agente de seguridad. El viaje lo hizo en vuelo comercial. Estando en Bogotá, Alan García junto a Alfredo Palacio González, Álvaro Uribe Vélez y Álvaro García Linera le entregó a Michelle Bachelet, una invitación formal para que Chile retornara a la Comunidad Andina. Muchos critican esta actitud, por considerarla una actitud pasiva ante las supuestas agresiones territoriales del país sureño.

El 17 de agosto de 2006, anunció el nombramiento del economista Hernando de Soto como "representante personal del presidente de la República del Perú" para impulsar el Tratado de Libre Comercio Perú-EE.UU. en el Congreso de los Estados Unidos, desempeñando el cargo de manera ad honorem. Cabe recordar que en la campaña se había mostrado opuesto a la firma de este tratado, anunciando inclusive que iba a retirar la firma de Toledo.

El 25 de agosto, se anunció que el Perú volvería a integrar el denominado Grupo de los 20, luego de una coordinación entre los cancilleres de Perú y Brasil. Así mismo, se confirmó su visita a Brasil para el 9 y 10 de noviembre. El 20 de septiembre de 2006, en presencia de José Antonio García Belaúnde por el Perú, Chile retornó a la Comunidad Andina como miembro asociado, luego de que el Perú liderara las invitaciones de los respectivos países miembros.

El 4 de octubre, recibió a Felipe Calderón Hinojosa, presidente electo de México en Palacio de Gobierno del Perú. El 10 de octubre realizó su primera visita a los Estados Unidos de América como presidente de la República del Perú, viajando únicamente acompañado por el canciller José Antonio García Belaúnde y en un vuelo comercial. Mantuvo reuniones con altos funcionarios norteamericanos como Condoleezza Rice y Carlos M. Gutiérrez, en pos de buscar la aprobación del Acuerdo Comercial con Estados Unidos por parte del Congreso de los Estados Unidos.

Pese a que confirmó viajaría a la toma de mando de Calderón en México, no pudo hacerlo por motivos de agenda, cancelando su visita a última hora, hecho que molestó a Lourdes Flores por ser socialcristiana. Así mismo viajó el 8 de diciembre a Cochabamba, Bolivia para asistir a la II Cumbre de la Comunidad Sudamericana de Naciones en calidad de representante del Perú. La cita sirvió también para reconciliarse con Hugo Chávez, con quien finalizó un periodo de ataques verbales. Inmediatamente después de finalizada la Cumbre, García recibió a Rafael Correa, presidente electo de Ecuador en Palacio de Gobierno como muestra de las buenas relaciones bilaterales.

El 4 de enero de 2007, recibió al canciller italiano Massimo d'Alema en La Casa de Pizarro, recibiendo la invitación para visitar Italia, a petición del presidente Giorgio Napolitano. Así mismo, su par peruano firmó un convenio con el gobierno italiano para un canje de deuda. Luego, desde el 6 del mismo mes, vino criticando un fallo de la Corte Interamericana de Derechos Humanos que exigía pagar indemnizaciones a terroristas confesos, caídos en el motín del Penal Castro Castro. García Pérez y el ministro Rafael Rey hablaron incluso del retiro del Perú de dicha instancia. Además, su bancada congresal planteó una acusación constitucional contra su antecesor Alejandro Toledo y su último ministro de Justicia Alejandro Tudela Chopitea por haberse allanado al fallo y permitir que la Corte del veredicto que actualmente está en debate. Así mismo, la congresista Keiko Fujimori y otros líderes del fujimorismo afirmaron que el allanamiento ante el fallo propiciado por Toledo, solo buscaba perjudicar a su padre para así acelerar su extradición, diciendo que el odio a su padre, ahora le costará millones al Perú. Esto es tomado por algunos sectores como una "alianza tácita" entre el fujimorismo y el APRA.

En el transcurso de su gestión, y luego de varios meses de negociaciones entre las cancillerías peruana y boliviana, se reactualizó el proyecto anteriormente denominado "BoliviaMar" y redenominado ahora "MarBolivia", en ocasión del encuentro de Evo Morales y Alan García el 19 de octubre de 2010 en Ilo donde se firmaron varios acuerdos bilaterales y complementarios de aquellos de 1992 y reafirmando la concesión por 99 años del Perú a Bolivia de una salida no soberana al océano Pacífico, estableciendo una zona franca industrial y económica especial (ZOFIE) y una zona franca turística (ZFT), que no serán un obstáculo en el caso que, en el marco de las conversaciones bilaterales boliviano – chilenas, se llegue a un acuerdo sobre el acceso al mar de Bolivia.

En ocasión de su Visita de Estado a Chile el 19 y 20 de enero de 2011 se suscribieron dos acuerdos bilaterales para reforzar la relación entre el Perú y dicho país (tránsito fronterizo y lucha contra las drogas) y se acordó trabajar conjuntamente en una multiplicidad de temas diferentes de mutuo interés, lo que causó una impresión positiva en la población de ambos países. Durante su estadía, el Jefe de Estado peruano entre otras cosas declaró: "“Yo soy un convencido de que la unión de Chile y el Perú en sus propósitos, proyectos y políticas va a ser la piedra clave de la verdadera integración de Sudamérica y de América Latina”". "“Señor presidente, vamos a seguir trabajando y yo hasta el último segundo en que sea presidente, y desde el primer segundo en que no sea presidente, por el tiempo de los tiempos que sea necesario, porque esta es una apuesta que aquí grabamos, este es un compromiso que aquí hacemos y no me detendré, y estoy seguro que usted tampoco, hasta que haya una sólida y vertebrada alianza sin temores entre nuestros países”".

La gestión de García Pérez comenzó el 28 de julio de 2006, presentando numerosos proyectos de ley y decretos supremos que reducían el salario de los cargos políticos del Estado peruano. Su ministro de Defensa, Allan Wagner Tizón anunció que se reanudaría la homologación de gastos militares con Chile, esta noticia fue confirmada por el ministro de Relaciones Exteriores de Chile Alejandro Foxley. El día 4 de agosto de 2006, García Pérez se reunió con María Teresa Fernández de la Vega, vicepresidenta primera del Gobierno de España, con quien trató temas sobre las relaciones entre España y el Perú. Fernández de la Vega también habló sobre el apoyo de España a las comunidades indígenas del Perú, así como de las inversiones de su país en Perú.

García Pérez viajó el 5 de agosto a Puno y Tacna, acompañado por el presidente del Consejo de Ministros Jorge Del Castillo, la ministra de Transportes y Comunicaciones Verónica Zavala Lombardi y el congresista aprista César Zumaeta. En su primer viaje al interior del Perú, promulgará en Puno la nueva zona franca y en Tacna para darle importancia al puente Almirante Grau y el ferrocarril del Departamento de Tacna.

Como parte de su promesa de campaña, el 8 de agosto, propuso someter a referéndum el retorno a la pena de muerte para los violadores de niños. Su propuesta fue inmediatamente respaldada por congresistas como Lourdes Alcorta Suero o Luisa María Cuculiza, sin embargo encontró oposición en Luis Giampietri Rojas y Lourdes Mendoza del Solar, integrantes del Partido Aprista. El Congreso de la República del Perú vería si lo debate internamente o si acepta la propuesta del presidente. El anuncio ha causado opiniones encontradas incluso en el propio Consejo de Ministros.

La ministra de Transportes Verónica Zavala, anunció que durante la gestión de García, se entregaría en concesión varios aeropuertos del interior del Perú. Así mismo, Pilar Mazzetti (Interior) puso en marcha el "Plan Telaraña 2" con la compra de más de 200 motocicletas para la Policía Nacional del Perú. También anunció que durante la gestión de García, la Policía podría experimentar aumentos de salarios en los próximos nueve meses ya que la política de austeridad lo permitiría. El 18 de agosto, recibió a la "Fragata BAP Bolognesi", en una reunión presidida por él, los vicepresidentes de la República Luis Giampietri Rojas y Lourdes Mendoza del Solar y el ministro de Defensa Allan Wagner Tizón. Ese mismo día, promulgó la ley que integra a numerosos mototaxistas al Sistema Integral de Salud en una ceremonia en Palacio de Gobierno del Perú, acompañado por el Congresista de la República del Perú Mauricio Mulder y varios miembros apristas.

El 20 de agosto, Construcción Civil anunció una huelga nacional contra el gobierno de García Pérez por supuestamente no haber cumplido sus promesas electorales. La clase política nacional, liderada por Luis Gonzales Posada ha rechazado esta manifestación por inconsistente e incluso ha acusado a Ollanta Humala de provocar este acto. Como muestra de austeridad, Alan García se inscribió en el Seguro Social de Salud (EsSalud), el 22 de agosto de 2006, renunciando también al seguro de vida que tenía Alejandro Toledo.

Tan solo al día siguiente, la Mina de Yanacocha cerró sus operaciones totalmente, ante las protestas de los comuneros. Rápidamente, Jorge Del Castillo, Juan Valdivia Romero y Pilar Mazzetti fueron tildados de ineficientes en sus cargos por permitir que el conflicto lleve a tal desenlace. El 29 de agosto, luego de una ardua negociación en la Presidencia del Consejo de Ministros, se llegó al acuerdo de atender las necesidades de Cajamarca y que Yanacocha vuelva a abrir sus puertas.

El 8 de septiembre de 2006, inició el proyecto contra la analfabetización, acompañador por José Antonio Chang y Susana Pinilla. Su gobierno invertirá cerca de 350 000 000 de soles al año para apoyar este proyecto, esperando acabar con la analfabetización en el año 2011. El 19 de septiembre, anunció que se evaluarían a los maestros y estudiantes, con el fin de apoyar el plan de alfabetización. Al día siguiente, recibió su primera gran manifestación en contra de su gobierno, liderada por la Confederación General de Trabajadores del Perú y Ollanta Humala, causando un gran congestionamiento en las principales vías de tránsito de Lima. La marcha tuvo como fin, demandarle a García el cumplimiento de sus promesas electorales.

Con el fin de que no haya más accidentes de tránsito, García lanzó el plan "Tolerancia Cero", con el cual busca que aquellos vehículos que no cumplan aunque sea un requista, no puedan circular por las carreteras. Fue acompañado por Verónica Zavala Lombardi. El 8 de enero de 2007, se realizó la evaluación a los maestros del Perú, programa que fue rechazado por Sindicato Unitario de Trabajadores en la Educación del Perú y su secretaria general Caridad Montes por cosiderarlo inadecuado, y además por la satanización de la que estaban siendo víctimas los maestros; el proceso fue propuesto por el Presidente García con la aprobación de importantes líderes políticos y sociales. El 11 de enero de 2007, el Congreso de la República rechazó su proyecto para aplicar la pena de muerte para aquellos que cometan actos de terrorismo, votando a favor el aprismo y el fujimorismo y en contra los nacionalistas, upepistas, Unidad Nacional y miembros de la Alianza Parlamentaria. Sin embargo el mismo día, propuso un referéndum para consultarle al pueblo sobre tal materia, aun cuando antes habría de realizarse un cambio constitucional ya que la Carta Magna no permite referéndums para restringir derechos fundamentales (en este caso la vida).

Sobre esta materia, recibió las críticas de los dos líderes opositores. Ollanta Humala expresó su rechazo a las opiniones del Presidente sobre el allanamiento ante la Corte Interamericana de Derechos Humanos, afirmando que el Perú debía acatar el fallo. De la misma manera, Lourdes Flores Nano afirmó que García no puede volver a "tener caprichos" como los tuvo durante su primera gestión en el caso de la estatización de la banca y el no pagar la deuda externa.

A escasos días de culminar su mandato constitucional, julio del año 2011, su segunda gestión era aprobada aproximadamente por el 42% de los peruanos, recuperándose de su nivel más bajo, 19%, alcanzado en septiembre del año 2008.

Las políticas de Alan García, quien a pesar de las pruebas fotográficas no ha reconocido la existencia de indígenas no contactados en el territorio de Perú, han sido criticadas rotundamente por grupos indígenas del país, porque ha facilitado a foráneos el acceso a las tierras indígenas para la explotación de sus recursos.
El 5 de junio de 2009, la Ministra del Interior Mercedes Cabanillas ordenó que la policía recuperara las carreteras tomadas por los indígenas amazónicos que las habían bloqueado en la región de Bagua. Los indígenas de la selva peruana, históricamente relegados por las políticas extractivistas de los gobiernos, se manifestaron contra los decretos especiales que habían sido decretados por el Poder Ejecutivo, los cuales regulaban la explotación de las tierras eriazas para la explotación de recursos naturales no renovables y renovables. Según la información oficial del estado, fallecieron en total 10 indígenas (aunque pobladores locales indican que fueron muchos más) y 24 policías. Según algunos testigos, los cuerpos de los indígenas asesinados fueron arrojados a los ríos. Parte de los policías enviados fueron al parecer sido ejecutados por un grupo de indígenas, a pesar de estar desarmados. El estado emitió un spot televisivo en donde llamaba a los indígenas "extremistas" y mostró imágenes de algunos policías asesinados; muchos calificaron aquel spot como promovedor del odio, por lo que fue emitido solo pocas veces en televisión. Dos investigadores Belgas presenciaron los incidentes y narraron los hechos en un vídeo de la ONG de derechos humanos Survival International. Estos hechos no pudieron ser confirmados ni por la Defensoría del Pueblo ni por la Misión especial enviada por la ONU. Existe una película al respecto, "El choque de dos mundos", donde se brinda detalles de los hechos sucedidos, con videos de los acusados y las declaraciones de los políticos y personajes influyentes de la televisión peruana, como por ejemplo "yo no me voy a quedar sin luz porque ustedes tienen un problema que no quieren dialogar" de Mariella Balbi, entre otros.

En 2016 y 2017, cinco de sus exministros son citados por casos de corrupción en relación con la empresa Odebrecht.

Con la misión de no volver a cometer sus mismos errores entre 1985 y 1990, Alan García continua la política económica de Alejandro Toledo, con una óptica diferente en cuestión social. Es así que los Tratados de Libre Comercio con Estados Unidos y Tailandia serán repotenciados con los Tratados con Chile y México. Su política de austeridad, permite un importante ahorro fiscal, del que se beneficiaría después el gobierno de Ollanta Humala.

En agosto de 2006, las reservas internacionales del Perú, alcanzaron un récord histórico. Como parte de la nueva integración con Chile, este país se ha comprometido a impulsar que el Acuerdo Estratégico Trans-Pacífico de Asociación Económica acepte al Perú como miembro pleno, con lo que pasaría a ser el "P5". El 20 de septiembre de 2006, se anunció que el futuro TLC con Singapur estaría en su tercera ronda de negociaciones.

El 27 de julio de 2006, Alan García anunció a los integrantes de su primer Consejo de Ministros, el cual es presidido por Jorge Del Castillo, íntimo amigo de García Pérez y Secretario General del Partido Aprista. Destacan cinco mujeres, el más alto en la historia peruana y uno de los más altos a nivel de América Latina, además de numerosos independientes y trabajadores del régimen toledista, mientras que solo siete apristas integran el Consejo. Con la renuncia de Mazzetti en febrero de 2007, quedaron cinco mujeres y se agregó un aprista al Gabinete.

Entre el 12 y 15 de agosto, el ministro de Vivienda Hernán Garrido Lecca tuvo un entercado verbal con el congresista de la República del Perú y extitular de ese sector Carlos Bruce, por supuestamente haberle mentido al país con obras que se realizarían sin licitaciones. Al día siguiente se reunieron con Jorge Del Castillo para arreglar sus diferencias. El mismo día, la ministra de Trabajo Susana Pinilla, sostuvo que su antecesor Carlos Almería, había incurrido en graves actos de corrupción con el Programa "A Trabajar Urbano", cobrando por los cupos, dinero que sería entregado a su gente de confianza e incluso a Alejandro Toledo. Así mismo, Pilar Mazzetti fue denunciada por supuestamente haber colocado a su amiga íntima en un puesto del Ministerio del Interior del Perú que había desaparecido en el 2005.

El día 25 de agosto de 2006, se presentaron ante el Congreso de la República del Perú en una anecdótica sesión, con el fin de obtener el voto de confianza. Durante su presentación, el parlamentario Víctor Andrés García Belaúnde anunció la muerte de Valentín Paniagua, interrumpiendo la presentación de Del Castillo. Posteriormente se confirmó que no había muerto, causando alivio pero malestar entre los presentes, quienes le habían dado un minuto de silencio como homenaje "póstumo". Luego, el congresista Miró Ruiz, le entregó unas "rodilleras" al Titular del MEM ya que, según él y sus representados, el gobierno negocia "de rodillas" con las mineras.

Finalmente, luego del debate en el pleno, obtuvieron el respaldo del Partido Aprista, Unidad Nacional, Grupo Parlamentario Fujimorista y la Alianza Parlamentaria. Los miembros de Unión por el Perú se abstuvieron, mientras que el Partido Nacionalista Peruano votó en contra. El 9 de septiembre de 2006, nombró a Arturo Woodman de Unidad Nacional como presidente del Instituto Peruano del Deporte y pese a no ser ministerio, tiene rango de tal.

El 24 de febrero de 2007, Pilar Mazzetti renunció al despacho del Interior, siendo la primera baja del Gabinete prescidido por Jorge Del Castillo. Mazzetti Soler fue reemplazada en el cargo por el compañero aprista Luis Alva Castro, juramentando el 26 de febrero del mismo año. Similar problema tuvo cuando Juan José Salazar renunció a la cartera de Agricultura, siendo reemplazado por el empresario agrícola Ismael Benavides Ferreyros.

En octubre de 2007, el presidente García anunció cambios ministeriales a cargo de Jorge Del Castillo, ratificándolo de alguna manera en el cargo. Esto podría considerarse como un "hito histórico" en el Perú pues es la primera vez que se habla de cambios ministeriales sin la salida del presidente del Consejo de Ministros. Entre los rumores de salida de la prensa se encontraban Verónica Zavala Lombardi, María Zavala Valladares y Carlos Vallejos Sologuren. Así mismo, se habló también de un "enroque" para que Susana Pinilla pasara al Ministerio de la Mujer y otro más para que Luis Alva Castro pasara al Ministerio de Defensa, aunque también se especulaba que Mercedes Cabanillas tomara esa posición.

Sin embargo, el 19 de diciembre se dieron a conocer los cambios Ministeriales y hubo 6 cambios, 2 rotaciones y 4 nuevos Ministros, que juraron el 20 de diciembre de 2007. Además, el 13 de mayo de 2008, se creó el Ministerio del Ambiente, siendo su primer titular, Antonio Brack Egg. Juramentó el 16 de mayo del mismo año, durante la V Cumbre ALC-UE.

Alan García, como ofreció durante la campaña electoral de 2011, se puso a disposición del nuevo Jefe de Estado para servir a los intereses del Perú en la forma que se le requiera.

Adicionalmente, también se dedicó a escribir artículos de opinión, principalmente sobre su visión interna y externa orientados prioritariamente a la disminución de la pobreza en el Perú, al incremento de la inversión externa y a temas relacionados con el crecimiento de la economía peruana con sensibilidad social.

El aumento de sueldo a los ministros realizado en febrero de 2014 fue duramente criticado por el expresidente, quien durante su gobierno redujo el sueldo de los miembros de su Gabinete a la mitad, lo que, según los voceros del oficialismo, provocó una fuga de talentos del aparato estatal. Por medio de su cuenta de Twitter, calificó la medida como “la gran repartija”.

En 2016 postuló a las elecciones presidenciales del Perú en búsqueda de un tercer mandato, esta vez con la alianza política Alianza Popular en la que se unieron el Partido Aprista Peruano y el Partido Popular Cristiano por primera vez, siendo la candidata a vicepresidente, Lourdes Flores. Sin embargo, García no alcanzó el éxito esperado obteniendo solo el 6 % de la votación.

En 2013 se formó una megacomisión la cual duró cinco años para investigar las presuntas irregularidades del segundo gobierno de Alan García, con el nacionalista Sergio Tejada como titular. De los ocho casos que analizó la megacomisión, en ninguno de ellos la comisión pudo continuar las investigaciones contra el expresidente debido a que este presentó una acción de amparo contra la Comisión alegando la vulneración al debido proceso. Como consecuencia de ello, el Poder Judicial anuló todo lo actuado respecto a García, impidiendo ello la prosecución de la investigación.

En sus últimos años García vivió entre Lima y Madrid. En noviembre de 2018 regresó al Perú para asistir a una citación fiscal. Sin embargo, los fiscales solicitaron impedimento de salida del país. 

Cuestionado públicamente en referencia a unos 24 millones de dólares en sobornos supuestamente pagados por la empresa "Odebrecht" a su gobierno, el 15 de noviembre de 2018 negando lo denunciado Alan García enfrentó a algunos medios peruanos, para finalmente declarar: 

""Entonces dicen, veinte millones (de dólares) menos los ocho que le dieron al viceministro (Jorge Cuba Hidalgo), los otros doce deben ser de Alan García. Demuéstrenlo pues, imbéciles, demuéstrenlo. Encuentren algo"".

El 17 de noviembre de 2018 el Segundo Juzgado de Investigación Preparatoria Anticorrupción, aceptando el pedido del fiscal José Domingo Pérez, emitió una orden de impedimento de salida del país por dieciocho meses contra Alan García por el caso "Línea 1" del Metro de Lima, siendo la obra pública cuestionada de la empresa "Odebrecht". Pese a que al conocer dicha noticia el expresidente manifestó que se allanaba al mandato judicial, asegurando incluso que no consideraba como una sanción estar dieciocho meses en su patria, este procedió a refugiarse en la embajada de Uruguay y solicitar asilo diplomático alegando una supuesta persecución política. Así, el 18 de noviembre de 2018 el Ministerio de Relaciones Exteriores del Perú informó que el embajador de la República Oriental del Uruguay le había comunicado el ingreso de expresidente a su residencia y el pedido de asilo diplomático. El ministro de Relaciones Exteriores de Uruguay, Rodolfo Nin Novoa, informó que Uruguay había decidido dar trámite a la solicitud de asilo. El 20 de noviembre de 2018, la cancillería peruana entregó una nota diplomática al embajador uruguayo, Carlos Barros, sobre la solicitud de asilo diplomático presentada por Alan García, expresando la postura del gobierno peruano y negando la existencia de una persecución política. El 3 de diciembre de 2018, el gobierno uruguayo le negó el asilo diplomático a García indicando que ""en el Perú funcionan autónomamente y libremente los tres poderes del Estados, especialmente el Poder Judicial que está llevando a cabo las investigaciones de eventuales delitos económicos""y que en tal sentido ""invitamos al Sr. Alan Garcia a retirarse de nuestra residencia diplomática"". Esa misma mañana García tuvo que abandonar dicha sede. Poco después se supo que el expresidente había intentado obtener asilo ante los gobiernos de Costa Rica y Colombia, siendo rechazado en ambos casos.

El 4 de enero Alan García se presentó ante fiscalía en calidad de testigo para declarar en la investigación seguida contra Miguel Atala (exvicepresidente de PetroPerú), por haber recibido un soborno por parte de la empresa Odebrecht. 
El 12 de abril se supo que la Fiscalía había solicitado impedimento de salida del país contra el exsecretario General de la Presidencia de la República durante el gobierno de Alan García Luis Nava Guibert y su hijo, al conocerse que entre ambos recibieron pagos ilegales por 4.5 millones de dólares de parte de Odebrecht. El 16 de abril el Poder Judicial ordenó la detención preliminar a Alan García por 10 días, así como a su exsecretario general de Presidencia Luis Nava.

El 4 de enero Alan García se presentó ante fiscalía en calidad de testigo para declarar en la investigación seguida contra Miguel Atala (exvicepresidente de PetroPerú), por haber recibido un soborno por parte de la empresa Odebrecht. 
El 12 de abril se supo que la Fiscalía había solicitado impedimento de salida del país contra el exsecretario General de la Presidencia de la República durante el gobierno de Alan García Luis Nava Guibert y su hijo, al conocerse que entre ambos recibieron pagos ilegales por 4.5 millones de dólares de parte de Odebrecht. El 16 de abril el Poder Judicial ordenó la detención preliminar a Alan García por 10 días, así como a su exsecretario general de Presidencia Luis Nava.

A las 06:27 horas (UTC-5) del 17 de abril de 2019, conocida la orden de detención preliminar, un representante del ministerio público acompañado de la Policía Nacional del Perú llegó a la casa del expresidente para detenerlo. Según manifestó el Ministro del Interior Carlos Morán, García entró a su habitación con la supuesta intención de llamar a su abogado, acto seguido (6:31 horas UTC-5) sonó un disparo que alertó al personal que se encontraba en la residencia. La policía forzó la puerta de la habitación y verificó que el líder aprista se había disparado en la cabeza. Fue llevado al Hospital Casimiro Ulloa por la Policía Nacional del Perú e ingresó en el hospital en estado muy grave. Tras cuatro horas de internamiento, dirigentes del partido aprista y Nidia Vílchez confirmaron que Alan García falleció como consecuencia del disparo.

En un comunicado, el hospital lamentó el fallecimiento producido a las 10:05 horas producto de una hemorragia cerebral masiva y por un paro cardiorrespiratorio.

Al conocerse el fallecimiento, por protocolo se izaron a media asta el Pabellón Nacional en los edificios públicos, bases militares, buques, establecimientos policiales y demás dependencias del Estado y se decretó duelo oficial. Tal como indica el protocolo se señaló a la familia que el traslado de los restos mortales del expresidente al lugar del velatorio sería «en privado» y las ceremonias religiosas tendrían «asistencia oficial» del Estado. El presidente Martín Vizcarra o un representante debía encabezar el cortejo fúnebre junto a los deudos, el cual rinde los mismos honores a los investidos con la banda presidencial y se otorga los honores militares contemplados en el Reglamento del Ceremonial Terrestre y Protocolo Militar para las Fuerzas Armadas y Policía Nacional. No obstante, la familia tomó la decisión de no aceptar el protocolo de funeral oficial, sino realizar un funeral independiente en la Casa del Pueblo, sede central del Partido Aprista Peruano. Al velatorio se acercaron diversas personalidades de la política y sociedad peruana, entre estas: Lourdes Flores Nano, Luis Bedoya Reyes, Jorge Muñoz Wells, Beatriz Merino, Ántero Flores Aráoz, Juan Luis Cipriani, Carlos Scull (representante del gobierno interino de Juan Guaidó en Venezuela), Ollanta Humala (impedido de acercarse al féretro por imposición de Federico Dantón, hijo menor de Alan García), Alfredo Barnechea, Luis Castañeda Lossio, entre otros.

Diversas autoridades y personalidades peruanas y extranjeras expresaron sus condolencias a la familia por el fallecimiento del exmandatario. En las redes sociales, la opinión pública fue más polarizada.










</doc>
<doc id="4787" url="https://es.wikipedia.org/wiki?curid=4787" title="Joaquín Sabina">
Joaquín Sabina

Joaquín Ramón Martínez Sabina (Úbeda, Jaén; 12 de febrero de 1949), conocido como Joaquín Sabina, es un cantautor, poeta y pintor español.

Ha publicado diecisiete discos de estudio y siete en directo y colaborado con distintos artistas cantando dúos y realizando . Los álbumes en directo son grabaciones de actuaciones en las que ha intervenido en solitario o junto con otros artistas: "La mandrágora" (1981), junto a Javier Krahe y Alberto Pérez; "Joaquín Sabina y Viceversa en directo" (1986), junto a la banda Viceversa; "Nos sobran los motivos" (2000); y "Dos pájaros de un tiro" (2007) junto a Joan Manuel Serrat. Se estima que ha vendido más de diez millones de discos y también ha compuesto para otros artistas como Ana Belén, Andrés Calamaro o Miguel Ríos, entre otros. En su faceta literaria ha publicado nueve libros con recopilaciones de letras de canciones o poemas publicados en el semanario "Interviú".

En 2001 sufrió un leve infarto cerebral que puso su vida en peligro, recuperándose unas pocas semanas más tarde sin sufrir secuelas físicas, pero viéndose inmerso en una importante depresión, lo que le llevó a abandonar los escenarios un tiempo. Durante su retiro, publicó "Dímelo en la calle" (2002), al que seguiría su decimoctavo álbum, "Alivio de luto" (2005), cuyas canciones reflejan cómo influyó el incidente en su forma de pensar. Consiguió tres discos de platino por "Vinagre y rosas" (2009) y uno de oro por "Lo niego todo" (2017), sus últimos discos hasta el momento.

Joaquín Sabina nació el 12 de febrero de 1949 en la localidad de Úbeda, Jaén, España, segundo hijo de Adela Sabina del Campo, ama de casa y de Jerónimo Martínez Gallego, inspector de policía. Cursó sus estudios primarios con las monjas carmelitas y con catorce años comenzó a escribir poemas y a componer música en una banda formada con sus amigos llamada Merry Youngs, que se dedicaban sobre todo a versionar a cantantes de "rock" como Elvis Presley, Chuck Berry o Little Richard.

Por esa época, tuvo a su primera novia, Chispa, que le sirvió como inspiración para crear algunos poemas de amor. Esta relación era un poco accidentada ya que el padre de Chispa, un notario de Úbeda, se opuso a aquella desde un principio y algunos años después, siendo Joaquín universitario, se llevó la hija consigo a Granollers con el fin de apartarla definitivamente de él. Pero Joaquín emprendió, en compañía de un amigo, un viaje en su busca, instalándose en una tienda de campaña junto a la casa familiar de Chispa. Los dos jóvenes se escaparon juntos recalando finalmente en el Valle de Arán, Lérida, donde vivieron juntos unos días.

Posteriormente cursó el bachillerato en los salesianos. En esa época siguió escribiendo versos y leyó a Fray Luis de León, Jorge Manrique y José Hierro pero también a Marcel Proust, James Joyce y Herbert Marcuse. El día en que aprobó cuarto y reválida su padre quiso recompensar a Joaquín con un reloj de pulsera, a lo que él se negó manifestando que prefería una guitarra; petición que fue satisfecha. En cambio, su hermano mayor sí que aceptó el reloj y, según Joaquín, ese pequeño detalle sería el que los empezaría a distanciar: su hermano se acabaría convirtiendo, como el padre de ambos, en policía y él en cantante.

En 1968 se trasladó a Granada para matricularse en la Facultad de Filosofía y Letras e iniciar los estudios de Filología Románica en dicha universidad, donde descubrió la poesía de César Vallejo y Pablo Neruda. Joaquín vivió por primera vez con una mujer, llamada Lesley, que preparaba su tesis de español en Granada.

Su ideología izquierdista le llevó a relacionarse con movimientos contrarios al régimen franquista. Este mismo año, cuando se proclamó el estado de excepción, su padre, que era comisario en Úbeda, recibió la orden de detenerlo por pertenecer al Partido Comunista. En 1970 comenzó a colaborar con la revista "Poesía 70", compartiendo páginas con Luis Eduardo Aute o Carlos Cano. En ese mismo año lanzó un cóctel mólotov contra una sucursal del Banco de Bilbao en Granada en protesta por el proceso de Burgos, por lo que se vio obligado a exiliarse. Como no tenía pasaporte, no pudo salir inmediatamente del país, pero conoció a un hombre, Mariano Zugasti, que, tras unas horas de conversación, le cedió el suyo. Con nombre falso y acompañado de Lesley, Joaquín puso rumbo a París, donde pasó unos meses, y posteriormente a Londres, donde vivió como "squatter" —okupa— durante su primer año de estancia en la ciudad.

Joaquín necesitó sensibilizar a la opinión pública a su favor, ya que de otro modo sería repatriado a España, y gracias a Lesley consiguió que le hicieran una entrevista y presentara su caso. El "Daily Mirror" publicó que a su vuelta a España le esperaría la pena de muerte, pero consiguió que las autoridades británicas le concedieran el asilo político por un año. Se marchó a vivir a Edimburgo con Lesley. Permanecieron allí cuatro meses, tras los cuales Joaquín se marchó a Londres abandonando a Lesley.

Durante esta época su casa en Londres sirvió de refugio para miembros de ETA. Años más tarde afirmó que «la izquierda de este país, a la que orgullosamente he pertenecido y creo pertenecer, debiera pedir perdón por su complacencia con ETA durante muchos años. Yo tuve en mi casa de Londres a etarras y era una gente encantadora que pegaban tiros en la nuca, algo que nos parecía una cosa muy graciosa en ese momento. Y hacíamos mal. Porque de aquellos polvos vinieron estos lodos. Así que creo que la gente como yo está muy obligada a estar muy en contra y a decirlo muy alto por cobardes que sean. Y yo lo soy como el que más».

Colaboró en Londres con el Club Antonio Machado, uno de los centros frecuentados por emigrantes y exiliados. En la capital inglesa escribió sus primeras canciones y organizó un cineclub donde se exhibían películas de Luis Buñuel, prohibido entonces en la España franquista. Reconstruyó el grupo de teatro Juan Panadero y montó polémicas obras teatrales como "La excepción de la regla", de Bertolt Brecht, y "El cepillo de dientes", de Jorge Díaz. Se ganó la vida cantando en el metro, restaurantes y cafés. En 1974, según una de las anécdotas más divulgadas sobre su vida, actuó ante George Harrison, quien celebraba su cumpleaños en un bar local llamado Mexicano-Taverna. El exbeatle le dio una propina de cinco libras. En algunas entrevistas, Sabina ha relatado que conserva el billete que recibió como un tesoro, en otras que lo perdió en una mudanza y en otras ocasiones ha desmentido su propia leyenda —«En realidad, me los bebí aquella misma noche»—.

Durante ese tiempo mantuvo una relación con una chica llamada Sonia.

En 1976 publicó el libreto de canciones "Memoria del exilio" y comenzó a organizar conciertos para la colonia de exiliados españoles en Inglaterra, en los que actuaron entre otros Paco Ibáñez, Lluís Llach, Francesc Pi de la Serra y Elisa Serna. Estos versos constituirían el grueso principal dos años más tarde de su primer disco, "Inventario". El libro fue editado por la editorial Nueva Voz, con una tirada de 1000 ejemplares que el propio Joaquín se encargó de distribuir por el área de Portobello Road, vendiendo hasta el último de ellos gracias a su don de gentes y a las muchas amistades trabadas en el más de medio lustro transcurrido en la capital británica. Más tarde compuso la banda sonora de la serie "The Last Crusade", de la BBC.

En 1977, dos años después de la muerte de Franco, consiguió volver a España gracias a un pasaporte legal facilitado por Fernando Morán, cónsul español en Londres. En ese mismo año se casó con Lucía Inés Correa Martínez, una argentina que había conocido en Londres durante su exilio. La ceremonia de enlace, eclesiástica, tuvo lugar el 18 de febrero de 1977. En realidad el enlace se celebró con el único propósito de conseguir el «pase de pernocta» —permiso que se da a los soldados para que puedan ir a dormir a sus casas— en el cuartel durante el servicio militar que se había visto obligado a cumplir en Mallorca tras regresar a España. Esto le permitió trabajar en el diario local "Última Hora". Su propietario, Pedro Serra, le ofreció quedarse en plantilla, pero rehusó y al acabar la "mili" en 1978 se instaló en Madrid con su mujer. Poco después consiguió editar su primer LP, "Inventario". El director de la discográfica CBS, Tomás Muñoz, le había ofrecido su primer contrato con la referencia de su tema «¡Qué demasiao!», que por aquel entonces, interpretada por el cantante Pulgarcito, sonaba en "Popgrama", espacio de Televisión Española presentado por Carlos Tena. De esa forma comenzó a actuar en el circuito de bares madrileños y en los mítines electorales del PSP, UGT, PCE y PSOE, así como en actos de la CNT. En esa época trabajó como entrevistador de "Carta de España". Al año siguiente comenzó a cantar junto a Javier Krahe y Alberto Pérez en el sótano del café madrileño La Mandrágora. Uno de los temas que interpretaron es «Con su bikini», versión paródica del tema de Bob Dylan «Man Gave Names to All the Animals» que, según parece, el propio autor le prohibió tocar. Al local acudió un día el periodista Fernando García Tola, que los invitó a su programa de televisión "Esta noche", presentado por Carmen Maura. 

Tras su primer disco, abandonó el perfil prototípico del cantautor, ya que, según él mismo afirma, el uso de ese término le hizo sentir como si le pusieran un ladrillo en la cabeza y «poeta» le parece «un traje que le queda demasiado ancho». En 1980 publicó su segundo trabajo, "Malas compañías", álbum en el que destacan varios temas que se convirtieron en clásicos, como «Calle Melancolía» o «¡Qué demasiao!», pero muy especialmente «Pongamos que hablo de Madrid», convertido para muchos en una especie de himno oficioso de la ciudad y que fue grabado primero por Antonio Flores, versión que alcanzó el número 1 en el programa de radio "Los 40 Principales". En 1981 apareció "La mandrágora", disco grabado en directo junto con Krahe y Pérez en el que intentaron recoger el espíritu de sus actuaciones en el local. Alternaron sus conciertos en pubs con la traducción de éxitos de la canción italiana para la discográfica CBS y empezó a componer para otros artistas como Miguel Ríos y Ana Belén. Comenzó a actuar con la que sería su primera banda, Ramillete de Virtudes y le añadió a su viejo repertorio nuevas composiciones cada vez más orientadas hacia el rock y con más ritmo como «Pisa el acelerador» y «Juana la Loca», canciones que, poco después, formarían parte del que sería su tercer elepé —sin contar el disco grabado en La Mandrágora—, "Ruleta rusa", publicado en 1984. En 1984 escribió para "Diario 16" un artículo de bienvenida a Bob Dylan y ese mismo año grabó con Gloria van Aerssen, de Vainica Doble, «Con las manos en la masa», la sintonía del programa de cocina homónimo de Elena Santonja en RTVE. Durante la temporada 1983-1984 actuó además asiduamente en el programa de TVE "Si yo fuera presidente", de Fernando García Tola.

Poco después, Sabina y Krahe decidieron separarse artísticamente para evitar repetirse. En 1985 abandonó CBS y se marchó a Ariola a cambio de la libertad artística y algo de dinero. Ese mismo año comenzó a trabajar con Viceversa, banda con la que en 1985 sacó el álbum "Juez y parte" y, un año después, el disco en directo "Joaquín Sabina y Viceversa en directo", grabado en el Teatro Salamanca de Madrid y que cuenta con la participación como invitados de Javier Gurruchaga y Ricardo Solfa, que interpretaron temas de su anfitrión, y de Luis Eduardo Aute, que le dedicó la canción «Pongamos que hablo de Joaquín». El álbum fue un éxito de ventas y supuso su salto al gran público. Participó en fiestas a favor de un referéndum para la salida de España de la OTAN. Además, estrenó «Si te he visto no me acuerdo», una canción que glosaba los tres años de gobierno socialista de Felipe González y en las elecciones municipales apoyó a su amigo Juan Barranco, candidato a la alcaldía de la capital. Además, en marzo de ese mismo año publicó "De lo cantado y sus márgenes", un conjunto de textos que reúne gran parte de los textos que formaron parte de "Memoria del exilio" y de las canciones de "Inventario".

Los éxitos comenzaron a sucederse con la publicación de sus siguientes elepés. En 1987 consolidó su éxito con la venta de más de 400 000 copias de "Hotel, dulce hotel". Su antigua compañía, viendo el éxito del artista, decidió editar, sin su consentimiento, un recopilatorio al que titula "Joaquín Sabina y todos sus éxitos". Dejó de actuar con el grupo Viceversa y se asoció con Víctor Claudín y Pedro Sauquillo para dirigir la sala de conciertos Elígeme, en el barrio de Malasaña de Madrid. En 1988 editó "El hombre del traje gris", que meses más tarde presentó en la Plaza de Toros de Las Ventas de Madrid. Acto seguido realizó una multitudinaria gira por México, Argentina y Venezuela. En ese trabajo se incluyó la banda sonora, escrita por Joaquín junto a Pancho Varona, retocada de la película "Sinatra", dirigida por Paco Betriu, y protagonizada por Alfredo Landa y Maribel Verdú. Sabina aparecía en un papel secundario. Ese mismo año produjo un álbum doble en directo del trío madrileño Los Chichos. Tiempo más tarde consiguió el divorcio de Lucía, su mujer.

En 1989 fundó junto a Pancho Varona, convertido en su inseparable guitarrista, Ripio, empresa editorial con la que a partir de ese momento registró todas sus canciones. En este mismo año, el 16 de enero, Joaquín es padre por primera vez, teniendo una hija, Carmela Juliana, fruto de su relación con Isabel Oliart, y a quien dedicaría la canción «Ay, Carmela» de su disco "Vinagre y rosas" de 2009. Su antigua discográfica, una vez más sin el consentimiento del artista, editó otra compilación: "Mucho Sabina".

Los discos y las giras se sucedieron en el comienzo de los noventa, con la publicación de "Mentiras piadosas" (1990), "Física y química" (1992), del cual se vendieron más de un millón de copias y en el que contó con la colaboración de Andrés Calamaro en el tema «Pastillas para no soñar» y que popularizó en Sudamérica a través de una macrogira internacional de 188 conciertos, y "Esta boca es mía" (1994).

El 26 de julio de 1992 nació su segunda hija con Isabel Oliart, Rocío, y comenzó una relación sentimental con la modelo mallorquina Cristina Zubillaga. En 1994 participó, junto a otros artistas, en los actos de protesta por el cierre del madrileño Teatro Alfil. En las elecciones legislativas de junio dejó clara su postura política apoyando a Izquierda Unida. Este mismo año participó, junto con otros importantes artistas, en la gira "Mucho más que dos" de Ana Belén y Víctor Manuel. En 1995 colaboró en el programa de televisión "Con Hermida y compañía", presentado por Jesús Hermida.

En 1996 publicó "Yo, mi, me, contigo", disco que le llevó de gira en compañía de Los Rodríguez dando más de 30 conciertos que se iniciaron el 18 de julio en Gijón, Asturias y continuaron por diversos países de Latinoamérica —Perú, México, Chile, Argentina y Uruguay—. El disco es el número uno de la lista de ventas de la Asociación Fonográfica y Videográfica Española (AFYVE), con 80 000 copias vendidas en su primera semana en el mercado. En él se incluye la canción «Y sin embargo» de la que Joaquín afirmó «Es mi canción de amor preferida».

En 1997, año en que es recibido por Fidel Castro, con quien conversó durante cinco horas, se embarcó en un proyecto con el músico argentino Fito Páez, que admiraba las cualidades poéticas de Sabina. El resultado es el disco "Enemigos íntimos", que salió a la venta en España en 1998, aunque la gira promocional programada fue suspendida por desavenencias entre los dos músicos. En esa oportunidad se cancelan más de 70 conciertos que tenían vendidos y promocionados alrededor del mundo. El escándalo es mayor cuando se conoce una carta que el mismo Joaquín Sabina le había escrito a Fito Páez en forma de poesía, donde resumía los motivos que determinaron el final de su relación laboral: «El rol del patito feo, no me va te lo aseguro, y menos el de hombre duro, que a ti te cuesta tan poco» recitaba Joaquín en la mencionada carta. Joaquín hizo una gira en solitario por teatros llamada "Sabina, viuda e hijos en paños menores", de importante éxito y que destacó por la gran duración de los recitales, que llegaban a las tres horas. En este tour se acompañó sólo de tres músicos: Pancho Varona —guitarra—, Antonio García de Diego —guitarra y teclados— y Olga Román —coros, percusiones y guitarra—. Tras romper con Cristina Zubillaga, comenzó a salir con una porteña de 23 años, Paula Seminara, relación que duró un año y medio.

Durante el mismo año 1998, Joaquín colaboró con el cantautor argentino Charly García en su disco "El aguante", cantando en el tema «Tu arma en el sur».

En 1999 publicó "19 días y 500 noches", disco que vende más de 500 000 copias en España y que le hace ganar cuatro de los Premios de la Música de la Sociedad General de Autores de España (SGAE) del año 2000, además del Premio Ondas a la mejor canción. Dentro de este disco se encuentran dos de sus canciones preferidas: «Una canción para la Magdalena» y «Noches de boda», esta última a dúo con la cantante Chavela Vargas, con quien años antes había entablado una estrecha amistad. Ese mismo año, su discográfica le rinde un homenaje en el Hotel Palace de Madrid para conmemorar la venta de más de cuatro millones de discos desde su primer trabajo en dicha discográfica, "Juez y parte", hasta "19 días y 500 noches". En diciembre de ese mismo año participa en el segundo concierto organizado por Los 40 Principales, Principales Solidarios, junto a La Oreja de Van Gogh, Hevia y Celtas Cortos para recaudar fondos destinados a los refugiados del conflicto de Los Balcanes.

En 2000 le fueron otorgados cuatro de los cinco galardones a los que optaba en los Premios de la Música en las categorías de mejor autor pop, mejor artista pop, mejor disco del año y por «19 días y 500 noches». Además comienza la gira acústica "Nos sobran los motivos", una revisión mejorada de "En paños menores". En septiembre termina la gira eléctrica de "19 días y 500 noches". El 6 de noviembre de ese mismo año recibe el Premio Ondas a la mejor canción por «19 días y 500 noches».

La madrugada del 24 de agosto de 2001, tras la publicación ese mismo año del álbum "Nos sobran los motivos", doble disco en directo, recopilatorio de la gira del mismo nombre, sufre un leve infarto cerebral que pondrá su vida en peligro. Aunque pocas semanas más tarde se recupera sin sufrir secuelas físicas, el incidente influye en su forma de pensar y se ve inmerso en una importante depresión. Todo esto hace recapacitar a Sabina sobre su modo de vida y su relación con las drogas por lo que decide dejar de consumir cocaína y afirmaría que «por las drogas sólo siento nostalgia». Sin embargo, en la entrevista durante el documental "Joaquín Sabina - 19 días y 500 noches" él mismo indica que había dejado de consumir cocaína cuatro meses antes de su accidente cerebrovascular. Durante ese tiempo también logra dejar de fumar durante 8 meses y llega a confesar que «fueron los ocho meses más largos de mi vida». Decidido a relanzar a su amiga María Jiménez, le cede sus temas para que lance el disco "Donde más duele", cantando con ella el tema «Con dos camas vacías».

En 2002, año en que posa desnudo para "El País Semanal", sale a la venta el libro "Con buena letra", que incluye ilustraciones y las letras de todas sus canciones, y el disco "Dímelo en la calle", que la crítica considera como uno de los álbumes más importantes de ese año, y que se da a conocer con el sencillo «69.G». También incluye el tema «Como un dolor de muelas», escrito parcialmente por Pancho Varona y el subcomandante Marcos, portavoz del EZLN y líder del levantamiento zapatista en Chiapas, México, el 1 de enero de 1994. Este disco también incluye la canción «Semos diferentes» que forma parte de la banda sonora de la película "" y por la que obtiene una nominación a los Premios Goya de 2002 como «mejor canción original». Sabina suspende la gira programada para promocionar el álbum argumentando problemas en las cuerdas vocales, aunque posteriormente haría público que el verdadero motivo fue la depresión que sufría. Sin embargo, en abril de 2003 saca un nuevo disco doble, "Diario de un peatón", que integra "Dímelo en la calle" con un segundo CD donde presenta algunos de sus temas recientes y otros antiguos que seguían inéditos. En el disco cuenta con la colaboración de Pablo Milanés en «La canción más hermosa del mundo», que antes ya había versionado con Pasión Vega.

En este tiempo sigue bajo los efectos de la depresión y reduce su actividad musical, pero potencia enormemente su faceta literaria como poeta. Como muestra de apoyo, surge el proyecto que finalizaría con el disco "Entre todas las mujeres", aparecido en octubre de 2003, donde trece artistas femeninas, como Rosario Flores, Ana Belén, Chavela Vargas o Julieta Venegas, versionan varios de sus temas.

Pese a su enfermedad, compone e interpreta en 2003 «Motivos de un sentimiento», el himno del Centenario del club de fútbol del que siempre se ha declarado fiel seguidor, el Atlético de Madrid. Joaquín se encarga de dar forma a tres versiones diferentes: una instrumental, otra al estilo de las chirigotas gaditanas y una última con sonido rock and roll, esta última cantada por Rosendo Mercado, Germán «Mono» Burgos, Lichis —cantante de La cabra mecánica—, Josele Santiago y él mismo. En 2004 crea con unos socios el restaurante La Cantina de la Mordida, en Madrid. Este mismo año compone la canción «La rubia de la cuarta fila» para la banda sonora de la película "Isi/Disi. Amor a lo bestia", con la que obtiene de nuevo una nominación a los Premios Goya de 2005 como mejor canción original. También participa en el proyecto colectivo en homenaje al poeta Pablo Neruda en su centenario, de título "Neruda en el corazón".

En 2005, el Alberto Ruiz-Gallardón, le ofreció ser el pregonero de las fiestas de San Isidro Labrador, patrón de la ciudad, honor que Sabina aceptó componiendo un pregón en verso que tuvo gran acogida popular. Publicó el disco "Alivio de luto" y gracias a ello y a su dedicación a la literatura logró salir de la depresión. Publicó la segunda edición de "Con buena letra", donde incluyó letras de canciones escritas por encargo o para amigos, para cine y televisión, y correspondientes a su disco "Alivio de luto".

Regresó a los escenarios con la "Gira ultramarina", en formato acústico y en pequeños escenarios o teatros, y supone el retorno del artista después de más de tres años de inactividad, rodeado de sus músicos habituales, Pancho Varona, Olga Román, Antonio García de Diego y Pedro Barceló. Uno de los conciertos de esta gira, en la ciudad de Gijón, fue suspendido por una laringitis aguda, lo que da una vez más lugar a comentarios en la prensa y entre el público acerca de su estado de salud.

En el año 2006, tras concluir la "Gira ultramarina", comenzó otra serie de conciertos bajo el nombre "Carretera y top manta". Esta referencia a la piratería musical le llevó a un agrio enfrentamiento con el cantante Ramoncín, miembro de la junta directiva de la SGAE. La gira tuvo carácter eléctrico y se realizó en grandes escenarios, comenzando en Gijón —resarciéndose así de lo que él mismo llamó «gatillazo»— y terminando a finales de año, después de recorrer gran parte de la geografía española, en Sudamérica.
Ese mismo año apareció un nuevo libro de entrevistas con Sabina bajo el título "Sabina en carne viva". Su autor es Javier Menéndez Flores, que ya escribió otro anterior, "Perdonen la tristeza", en el año 2000. El nuevo libro fue un éxito de ventas, aunque estuvo momentáneamente apartado de las librerías por motivos de lucha editorial. Al mismo tiempo, comenzó a colaborar con la revista "Interviú", que le cedió la tercera página para publicar sus sonetos.

En octubre de dicho año recibió de manos del rey Juan Carlos la Medalla de Oro al mérito en las Bellas Artes. Un mes después, en noviembre, se publicó la doble caja antológica "Punto... y seguido", que incluye todos sus discos más colaboraciones, directos y rarezas.

En 2007 realizó una gira junto a Joan Manuel Serrat llamada "Dos pájaros de un tiro", que comenzó el 29 de junio y que los llevó por 30 ciudades españolas y 20 americanas. En ella, el catalán interpretó las mejores canciones del ubetense mientras éste hacía lo propio con el repertorio del "noi del Poble-sec". De los conciertos celebrados en Madrid se grabó un disco en directo y un DVD con más material que fue puesto a la venta en diciembre de 2007.
En ese mismo año compuso la banda sonora de la película "Un mundo para Julius", basada en la novela homónima de Alfredo Bryce Echenique, que interpretaron Ana Belén y Luz Casal. También sacó a la venta "Esta boca sigue siendo mía", segunda parte de los sonetos publicados para "Interviú", y "A vuelta de correo", epistolario que recoge la correspondencia entre el cantautor y diferentes personalidades como el subcomandante Marcos o Fito Páez, entre otros.

En 2008, el director neerlandés Ramon Gieling dirigió una película sobre la vida de Joaquín Sabina titulada «19 días y 500 noches» y cuyo tema principal es la depresión que sufrió hace unos años. Ese mismo año se concretó la reconciliación y posterior encuentro entre Sabina y Fito Páez. Fito invitó a Sabina a su recital en Madrid y juntos grabaron una versión de «Contigo», que estaba incluida en el último CD del artista argentino titulado "No sé si es Baires o Madrid". El encuentro está registrado también en el DVD que acompañó al disco.

El 5 de marzo de 2009 se anunció la concesión a Joaquín Sabina, junto a José Tomás, Raúl González Blanco y Paloma O'Shea, de la Medalla de Oro de la Ciudad de Madrid que otorgó anualmente el Ayuntamiento como reconocimiento a personajes públicos que han contribuido con su trabajo a fomentar la buena imagen de la ciudad. El galardón lo recibió el 15 de mayo del mismo año. El 17 de noviembre de ese año publicó su decimoquinto álbum de estudio, titulado "Vinagre y rosas" y cuyo sencillo, «Tiramisú de limón», es cantado junto al grupo Pereza, el cual se encargó además de ponerle música así como de los coros y la producción. Para presentar este nuevo álbum inició una gira en Salamanca, donde dio sus dos primeros conciertos los días 20 y 21 del mismo mes. Esta gira, según ha afirmado, sería la última que realice por grandes escenarios. El 16 de noviembre de 2010, la revista Rolling Stone le otorgó el premio como Artista del año.

En 2011, el cantante inició la gira "El penúltimo tren" en la que recorrió Latinoamérica y en la que tenía previsto cantar por primera vez en Estados Unidos. Sin embargo, en mayo de 2011, Sabina suspendió los conciertos en ciudades mexicanas y estadounidenses debido a una «diverticulitis aguda con riesgo de complicaciones» y los pospuso para el mes de octubre. Ya recuperado, regresó a los escenarios el 2 de julio en el festival Músicos en la naturaleza, celebrado en Hoyos del Espino, Ávila, España, en el que actuó junto a Andrés Calamaro. Finalmente pudo actuar en el Manhattan Center de Nueva York el 16 de octubre, en el que fue su primer concierto en Estados Unidos. Dentro de su gira, también actuó en el Nokia Theater de Los Ángeles el 20 de octubre y la finalizó el 23 de octubre en el AmericanAirlines Arena de Miami.

El 6 de octubre de ese mismo año se estrenó el musical "Más de cien mentiras", basado en sus canciones y dirigido por David Serrano y con el propio cantante como director musical en compañía de Pancho Varona y José María Cámara.

El 6 de febrero de 2012, presentó junto a Joan Manuel Serrat "La orquesta del Titanic", su primer álbum de estudio grabado con el cantautor catalán. Además, anunciaron una gira de presentación del disco que los llevaría por Argentina, Chile, México, Estados Unidos, Costa Rica y España. En la campaña electoral de las Elecciones al Parlamento de Cataluña de 2012, dio su permiso para que utilizaran los versos de «Anteproyectos para la letra del himno nacional (con perdón)» para que el partido Ciudadanos-Partido de la Ciudadanía lo usara en dicha campaña.
En 2013, publicó el libro "Muy personal", que incluía dibujos, poemas empezados y letras de canciones inacabadas. Era la primera vez que publicaba sus obras plásticas —denominados «garabatos» por el artista— pintadas con rotuladores y extraídas de una quincena de cuadernos ilustrados. Los dibujos están incluidos entre fragmentos de poemas, de reflexiones personales, esbozos de letras de canciones, impresiones sobre sus conciertos, comentarios de sus viajes y entradas de un diario frustrado.

El 16 de octubre de 2015, lanzó la caja "Puro Sabina", que incluye trabajos de estudio y en directo tanto en solitario como al frente de Viceversa, la banda que lo acompañó a mediados de los ochenta.

El 3 de febrero de 2016, puso a la venta "Garagatos", un cuaderno de artista que incluye un libro de arte, el libro "Garagatos" y un desplegable de casi tres metros de largo. El libro de arte está compuesto por 66 dibujos facsimiles en forma de láminas de diferente tamaño. Cada dibujo está acompañado por fragmentos de canciones, versos o «guiños de humor» del propio Sabina. En el libro "Garagatos", diferentes personalidades hablan sobre las influencias, arquetipos y personajes empleados por el artista en su obra. Poco antes, se había sometido con éxito a una operación de estómago por una diverticulitis con riesgo de acabar en peritonitis. Ese mismo mes, fue nombrado Hijo Predilecto de Andalucía, título concedido por la Junta de Andalucía.

El 10 de marzo de 2017, estrenó su álbum "Lo niego todo", producido por Leiva y en el que colabora el poeta Benjamín Prado. El 18 de junio de 2018, Sabina se ve en la obligación de cancelar la gira de "Lo niego todo" por problemas de salud, al quedarse «totalmente mudo» durante el concierto celebrado el 16 de junio en el WiZink Center de Madrid.

El 12 de febrero de 2020 sufrió una caída durante el concierto que daba junto a Joan Manuel Serrat en el WiZink Center de Madrid como parte de su gira "No hay dos sin tres". El cantante se precipitó del escenario hasta caer al foso de seguridad, desde una altura de al menos un metro y medio. Al cegarse con un foco no se dio cuenta de que se encontraba en el mismo borde de las tablas, perdió pie y cayó al foso de seguridad entre el escenario y las primeras filas del público. Como consecuencia de la caída, sufrió un hematoma intracraneal del que tuvo que ser operado con éxito y quedó en la UCI. Tras el incidente, el propio Sabina salió en silla de ruedas al escenario, acompañado de Serrat, para aplazar el concierto y anunciar al público que se iba al hospital porque se había hecho daño en un hombro. 

En 2020 AtresPlayer estrena una serie documental sobre Joaquín, denominada "Pongamos que hablo de Joaquín Sabina". 

En la biografía de Sabina se han sucedido episodios fuera de lo habitual—como por ejemplo ser detenido por su padre o conocer a una persona que le cede su pasaporte sin apenas conocerse— y su obra no es la propia de un poeta, ya que gran parte de sus composiciones son canciones. Por ello, su vida no es menos importante que su obra, más allá de la autorreferencialidad que presentan las letras de sus canciones.

Sobre este tema, Marcela Romano apunta en "¿La enunciación en persona?", que «al modelo de productor individual, discretamente implicitado en la escritura, sucede otro fuertemente explícito, presente, quien, simultáneamente con el texto, exhibe la voz, el cuerpo, los gestos, la vestimenta», al que la estudiosa denomina «sujeto espectacular». Esa exhibición de la persona se confirma con el hecho de que los tres libros editados sobre Joaquín Sabina —al margen de los libros de poemas— son biografías o compilaciones de anécdotas, aunque en ellos aún se encuentren también referencias a su obra. Sin embargo, la exposición del artista postmoderno va mucho más allá y llega hasta los programas de televisión y de radio, los sitios de Internet, las revistas de interés general y la prensa del corazón, es decir, el sistema de producción y consumo del llamado mundo del espectáculo.

Joaquín Sabina se emancipa inmediatamente después de la edición en 1978 de "Inventario", su primer disco, de la musicalización de la poesía y lo que precisamente lo caracteriza es, salvo en muy contados casos de coautoría o de interpretación de canciones de otros autores, la preeminencia de sus letras, tanto en el sentido de que éstas son dominantes absolutas en su cancionero como en el de que posee una intervención limitada en su musicalización, de la que se encargan fundamentalmente desde mediados de los años 80 Pancho Varona y Antonio García de Diego. Cabe destacar que el único texto de los poemas que forman las canciones de "Inventario" que Sabina musicaliza es un texto medieval titulado el «Romance de la gentil dama y el rústico pastor». Resulta curioso por el hecho de que aunque gran parte de la poesía musicalizada por cantautores españoles e hispanoamericanos a partir de la década de 1960 ya posee una virtualidad oral: los "Cantares" o la "Saeta" de Antonio Machado, las "Nanas de la cebolla" de Miguel Hernández e interpretadas por Serrat; así como el son de Nicolás Guillén para el cubano Pablo Milanés; Sabina elige un texto anterior a la invención de la imprenta y lo remusicaliza, ya que en su contexto original era cantado. En este romance aparecen varios ejes temáticos sobre los cuales se desarrollará la temática posterior de las canciones de Joaquín Sabina: el amor, el sexo, el rechazo a la pareja formalizada y el estereotipo del varón solitario.

Desde los estudios realizados por Heinrich Wölfflin, es un tópico considerar que el arte se desarrolla en períodos sucesivos de afirmación y de crisis. El Barroco es considerado como un período de crisis y se vincula con la posmodernidad debido a su pesimismo e ironía esenciales. Se relacionan algunas canciones de Sabina como «Calle Melancolía», «Inventario» o «Siete crisantemos» con el "esprit du temps" barroco. El Barroco expresa la conciencia de una crisis, visible en los agudos contrastes sociales, el hambre, la guerra y la miseria. De la misma forma, España en los años 80, años en los que se publica la canción «Calle Melancolía», se caracteriza por ser «una sociedad marcada por el paro, la desesperanza, el miedo atómico, la frustración laboral y académica, el absentismo, el terrorismo... junto con unas ganas de vivir a toda prisa, cierta euforia cultural, la confianza en las instituciones democráticas; y todo ello cifrando su hipotética salvación en un individualismo abrumador». Esta situación se refleja en «Calle Melancolía», en la que encontramos versos con amargos desengaños «no hallo más que puertas que niegan lo que esconden»; dolor vital, «por las paredes ocres se desparrama el zumo / de una fruta de sangre crecida en el asfalto»; desesperación, «me enfado con las sombras que pueblan los pasillos»; desamparo, «trepo por tu recuerdo como una enredadera / que no encuentra ventanas donde agarrarse»; y, posiblemente, los versos que mejor definen la España de los primeros años del posfranquismo: «un barco enloquecido / que viene de la noche y va a ninguna parte».

Fredric Jameson afirmaría al respecto que lo postmoderno es «la lógica cultural del capitalismo tardío» y que, en rigor, no existe una ruptura epistémica con los postulados de la Modernidad. Umberto Eco define la posmodernidad como la «fase manierista de la Modernidad». La posmodernidad en la literatura española se inicia con los primeros poetas de posguerra y su giro hacia un «yo» autorreflexivo a la vez que la incorporación de la denominada «voz social», lo que deriva, según Laura Scarano, en «el programa poético de Gabriel Celaya en los años 50 con su propuesta de una poesía-canción», aunque ya se percibía este giro en autores de la generación del 27 como Federico García Lorca.

Las letras de Sabina poseen un amplio abanico de influencias que van desde los cancioneros del rock anglosajón —con autores como Bob Dylan, Leonard Cohen o The Rolling Stones—, el folklore latinoamericano (Atahualpa Yupanqui, Violeta Parra, Chavela Vargas o José Alfredo Jiménez), el tango (Enrique Santos Discépolo, Homero Manzi o Celedonio Flores) la canción melódica francesa —Georges Brassens— hasta poetas vanguardistas hispanoamericanos como César Vallejo, pero también Pablo Neruda, Raúl González Tuñón y Rafael Alberti o a los autores que forman parte de sus primeras lecturas en su juventud, que incluyen a Fray Luis de León y Jorge Manrique así como el resto de la tradición española. Por encima de todos estos autores destaca la influencia de Francisco de Quevedo, aunque Sabina insiste en que su máxima influencia entre la poesía española contemporánea es la de Jaime Gil de Biedma.

Sabina ha manifestado en reiteradas ocasiones una admiración por su compatriota la cantante María Dolores Pradera, con quien grabó en 2007 la canción «Jugar por jugar» para el disco "En buena compañía" de la artista. Esto los llevaría a fortalecer una amistad antigua que Sabina transcribe en las "Coplas a María Dolores Pradera" que le dedica en la presentación de "Canciones del alma", que graba Pradera en 2003.

El sarcasmo, la ironía y la mordacidad son determinantes en la obra poética de Joaquín Sabina, al igual que en la de Quevedo. Las características formales básicas del Barroco se hacen patentes asimismo en sus letras: léxico de uso corriente entrelazado con cultismos, equívocos, retruécanos, contrastes y antítesis, así como construcciones anafóricas y enumeraciones asindéticas, estos últimos, las dos principales figuras retóricas de la poética "sabiniana".

Los discos más significativos y en los que Sabina alcanza la cumbre de su barroquismo por encima del resto de álbumes de su discografía son "Yo, mí, me, contigo" y "19 días y 500 noches". En el primero, porque ha sido atiborrado deliberadamente de lecturas en clave, y en el segundo, porque se muestra definitivamente dueño de sus recursos de estilo. El título del disco "Yo, mí, me, contigo" revela la metatextualidad consciente de Sabina, ya que enuncia los pronombres de primera persona del singular y los contrapone con uno de la segunda persona en último lugar, elaborando un juego de palabras. Se pueden establecer comparaciones entre la canción «Contigo» de Sabina y el soneto de Quevedo «Amor constante más allá de la muerte».

«Contigo» se vale de la anáfora en las estrofas que constituyen la primera y segunda partes de la canción, donde el «Yo no quiero» se repite dieciocho veces a lo largo de ellas formando, por tanto, dieciocho versos endecasílabos, una de las métricas preferidas del Barroco, la mayoría de ellos consecutivos. Como efecto de significación, el «Yo no quiero» ofrece a la vez la preeminencia del enunciador en primera persona y su definición por la negativa, otro rasgo barroco, de una concepción del amor que reniega —al igual que ocurría en el «Romance de la gentil dama y el rústico pastor»— del amancebamiento/aburguesamiento del sujeto poético, para oponerlo antitéticamente, al final de cada parte, a la afirmación de «Lo que yo quiero».

El segundo recurso propio del Barroco lo encontramos en el uso arcaizante del ablativo absoluto «corazón cobarde», que puede ser una aposición del «yo» poético como un vocativo que apela al «tú» femenino —«lo que yo quiero, corazón cobarde, / es que mueras por mí»—. Por paralelismo con la segunda parte de la canción, se podría pensar que se trata de lo segundo, dado que los versos equivalentes son «lo que yo quiero, muchacha de ojos tristes, / es que mueras por mí», pero esta lectura restaría la ambigüedad buscada por el poeta a la hora de componer los versos.

El tercer caso puede calificarse como una reescritura que Sabina hace de Quevedo, es decir, la asimilación por parte de Sabina de un texto ajeno escrito por Quevedo desarrollando una escritura propia del mismo y superando la mímesis. Por tanto, el «Y morirme contigo si te matas / y matarme contigo si te mueres, / porque el amor cuando no muere mata, / porque amores que matan nunca mueren» podría considerarse una especie de glosa de todo el soneto «Amor constante más allá de la muerte» de Quevedo.

Por último, el estribillo de «Contigo» es otra clara muestra del barroquismo de la canción, ya que desarrolla en sus cuatro versos una estructura de paralelismo entre sí en los dos primeros y en los dos últimos, comenzando una vez más de forma anafórica —«Y...», «Porque...»— y a la vez un quiasmo versal entre el primero y el segundo y entre el tercero y el cuarto. Es decir, en cada par de versos se juega con lo especular, que se reduplica en la especularidad entre los dos pares. Además, las cuatro conjugaciones distintas de los verbos «matar» y «morir» son antitéticos entre sí.









</doc>
<doc id="4795" url="https://es.wikipedia.org/wiki?curid=4795" title="2 de febrero">
2 de febrero

El 2 de febrero es el 33.º (trigésimo tercer) día del año en el calendario gregoriano. Quedan 332 días para finalizar el año y 333 en los años bisiestos.
























</doc>
<doc id="4796" url="https://es.wikipedia.org/wiki?curid=4796" title="3 de febrero">
3 de febrero

El 3 de febrero es el 34.º (trigésimo cuarto) día del año en el calendario gregoriano. Quedan 331 días para finalizar el año y 332 en los años bisiestos.











</doc>
<doc id="4797" url="https://es.wikipedia.org/wiki?curid=4797" title="Zoysia">
Zoysia

Zoysia es un género de pastos nativos de China, Japón y otras partes del sudeste asiático.

El género fue nombrado en honor del botánico austríaco Karl von Zoys, 1756–1800.




</doc>
<doc id="4798" url="https://es.wikipedia.org/wiki?curid=4798" title="Zonotriche">
Zonotriche

Zonotriche es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de África tropical.
El género fue descrito por (C.E.Hubb.) J.B.Phipps y publicado en "Kirkia" 4: 113. 1964. La especie tipo es: "Zonotriche decora" (Stapf) J.B. Phipps

A continuación se brinda un listado de las especies del género "Zonotriche" aceptadas hasta febrero de 2013, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos: 



</doc>
<doc id="4799" url="https://es.wikipedia.org/wiki?curid=4799" title="Zizaniopsis">
Zizaniopsis

Zizaniopsis, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario del sur de Estados Unidos y Sudamérica.
El nombre del género se compone de "Zizania" (género de la misma familia) y "opsis" (similar), aludiendo a la similitud de los dos géneros. 



</doc>
<doc id="4800" url="https://es.wikipedia.org/wiki?curid=4800" title="Zingeria">
Zingeria

Zingeria, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia. Comprende 148 especies descritas y de estas, solo 5 aceptadas. 
El género fue descrito por Carlos Linneo y publicado en "Byulleten' Moskovskogo Obshchestva Ispytatelei Prirody, Otdel Biologicheskii" 51(2): 67. 1946. La especie tipo es: "Zingeria biebersteiniana" (Claus) P.A.Smirn.
Zingeria nombre genérico que fue otorgado en honor de Vasily Jakovlevich Zinger, botánico ruso. 
A continuación se brinda un listado de las especies del género "Zingeria" aceptadas hasta enero de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos.



</doc>
<doc id="4802" url="https://es.wikipedia.org/wiki?curid=4802" title="4 de febrero">
4 de febrero

El 4 de febrero es el 35.º (trigésimo quinto) día del año en el calendario gregoriano. Quedan 330 días para finalizar el año y 331 en los años bisiestos.









</doc>
<doc id="4803" url="https://es.wikipedia.org/wiki?curid=4803" title="1868">
1868

1868 (MDCCCLXVIII) fue un año bisiesto comenzando en miércoles según el calendario gregoriano.






































</doc>
<doc id="4804" url="https://es.wikipedia.org/wiki?curid=4804" title="World Wide Web">
World Wide Web

En informática, la World Wide Web (WWW) o red informática mundial es un sistema de distribución de documentos de hipertexto o hipermedia interconectados y accesibles a través de Internet. Con un navegador web, un usuario visualiza sitios web compuestos de páginas web que pueden contener textos, imágenes, vídeos u otros contenidos multimedia, y navega a través de esas páginas usando hiperenlaces.

La Web se desarrolló entre marzo de [8] y diciembre de 1989 por el inglés Tim Berners-Lee con la ayuda del belga Robert Cailliau mientras trabajaban en el CERN en Ginebra, Suiza, y se publicó como una propuesta formal en 1991. Desde entonces, Berners-Lee ha jugado un papel activo guiando el desarrollo de estándares Web (como los lenguajes de marcado con los que se crean las páginas web), y en los últimos años ha abogado por su visión de una Web semántica.

Utilizando los conceptos de sus anteriores sistemas de hipertexto como ENQUIRE, el físico británico Tim Berners-Lee, un científico de la computación y en ese tiempo de los empleados del CERN, ahora director del World Wide Web Consortium (W3C), escribió una propuesta en marzo de 1989 con lo que se convertiría en la World Wide Web. La propuesta de 1989 fue destinada a un sistema de comunicación CERN pero Berners-Lee finalmente se dio cuenta que el concepto podría aplicarse en todo el mundo. En la CERN, la organización europea de investigación cerca de Ginebra, en la frontera entre Francia y Suiza,
Berners-Lee y el científico de la computación belga Robert Cailliau propusieron en 1990 utilizar el hipertexto "para vincular y acceder a información de diversos tipos como una red de nodos en los que el usuario puede navegar a voluntad", y Berners-Lee terminó el primer sitio web en diciembre de ese año. Berners-Lee publicó el proyecto en el grupo de noticias alt.hypertext el 7 de agosto de 1991.

En el número de mayo de 1970 de la revista "Popular Science", Arthur C. Clarke predijo que algún día los satélites "llevarán el conocimiento acumulado del mundo a sus manos" con una consola que combinara la funcionalidad de la fotocopiadora, teléfono, televisión y un pequeño ordenador, que permitirá la transferencia de datos y videoconferencia en todo el mundo.

En marzo de 1989, Tim Berners-Lee escribió una propuesta que hace referencia ENQUIRE, una base de datos y proyectos de software que había construido en 1980, y describe un sistema de gestión de la información más elaborado.

La idea subyacente de la Web se remonta a la propuesta de Vannevar Bush en los años 40 sobre un sistema similar: a grandes rasgos, un entramado de información distribuida con una interfaz operativa que permitía el acceso tanto a la misma como a otros artículos relevantes determinados por claves. Este proyecto nunca fue materializado, quedando relegado al plano teórico bajo el nombre de Memex. Es en los años 50 cuando Ted Nelson realiza la primera referencia a un sistema de hipertexto, donde la información es enlazada de forma libre. Pero no es hasta 1980, con un soporte operativo tecnológico para la distribución de información en redes informáticas, cuando Tim Berners-Lee propone ENQUIRE al CERN (refiriéndose a "Enquire Within Upon Everything", en español "Preguntando de todo sobre todo"), donde se materializa la realización práctica de este concepto de incipientes nociones de la Web.

En marzo de 1989, Tim Berners-Lee, ya como personal de la división DD del CERN, redacta la propuesta, que referenciaba a ENQUIRE y describía un sistema de gestión de información más elaborado. No hubo un bautizo oficial o un acuñamiento del término web en esas referencias iniciales, utilizándose para tal efecto el término "malla". Sin embargo, el World Wide Web ya había nacido. Con la ayuda de Robert Cailliau, se publicó una propuesta más formal para la World Wide Web el 6 de agosto de 1991.

Berners-Lee usó un NeXTcube como el primer servidor web del mundo y también escribió el primer navegador web, WorldWideWeb en 1991. En las Navidades del mismo año, Berners-Lee había creado todas las herramientas necesarias para que una web funcionase: el primer navegador web (el cual también era un editor web), el primer servidor web y las primeras páginas web que al mismo tiempo describían el proyecto.

El 6 de agosto de 1991, envió un pequeño resumen del proyecto World Wide Web al newsgroup alt.hypertext. Esta fecha también señala el debut de la Web como un servicio disponible públicamente en Internet.

El concepto, subyacente y crucial, del hipertexto tiene sus orígenes en viejos proyectos de la década de los 60, como el Proyecto Xanadú de Ted Nelson y el sistema on-line NLS de Douglas Engelbart. Los dos, Nelson y Engelbart, estaban a su vez inspirados por el ya citado sistema basado en microfilm "memex", de Vannevar Bush.

El gran avance de Berners-Lee fue unir hipertexto e Internet. En su libro "Weaving the Web" (en español, "Tejiendo la red"), explica que él había sugerido repetidamente que la unión entre las dos tecnologías era posible para miembros de las "dos" comunidades tecnológicas, pero como nadie aceptó su invitación, decidió, finalmente, hacer frente al proyecto él mismo. En el proceso, desarrolló un sistema de identificadores únicos globales para los recursos web y también: el Uniform Resource Identifier.

World Wide Web tenía algunas diferencias de los otros sistemas de hipertexto que estaban disponibles en aquel momento:

El 30 de abril de 1993, el CERN presentó la World Wide Web de forma pública.

La primera página de Internet fue creada por Tim Berners-Lee en 1991 mediante un computador NeXT, el funcionamiento de esta como lo han sido todas las páginas de Internet era informar sobre la World Wide Web. En la página se define la hipermedia y muestra un ejemplo de como sería una página en hypertexto, se enseña como contribuir a la Web, menciona a las personas involucradas en ese proyecto, cómo se clasifica la información en la Web, los servidores y softwares que existían, enseña cómo insertar una bibliografía, proporciona la terminación de cada tipo de software que existe para así identificarlo más fácilmente, menciona el nacimiento de la página, da a conocer el colisionador de partículas CERN, e incluso viene un manual de usuario para utilizar la World Wide Web y proporciona ayuda en línea en los softwares Line Mode Browser, NeXTStep y MidasWWW. Toda esta información se muestra en cuatro enlaces en la página principal.
Esta página web se abrió el 30 de abril de 1993, durante mucho tiempo dejó de existir esta página, pero la abrieron 20 años después el 30 de abril de 2013 como conmemoración del nacimiento de la tecnología web.

ViolaWWW fue un navegador bastante popular en los comienzos de la Web que estaba basado en el concepto de la herramienta hipertextual de software de Mac denominada HyperCard. Sin embargo, los investigadores generalmente están de acuerdo en que el punto de inflexión de la World Wide Web comenzó con la introducción del navegador web Mosaic en 1993, un navegador gráfico desarrollado por un equipo del NCSA en la Universidad de Illinois en Urbana-Champaign (NCSA-UIUC), dirigido por Marc Andreessen. El apoyo para desarrollar Mosaic vino del "High-Performance Computing and Communications Initiative", un programa de fondos iniciado por el entonces gobernador Al Gore en el High Performance Computing and Communication Act of 1991, también conocida como la Gore Bill. Antes del lanzamiento de Mosaic, las páginas web no integraban un amplio entorno gráfico y su popularidad fue menor que otros protocolos anteriores ya en uso sobre Internet, como el protocolo Gopher y WAIS. La interfaz gráfica de usuario de Mosaic permitió a la WWW convertirse en el protocolo de Internet más popular de una manera fulgurante...

El primer paso consiste en traducir la parte nombre del servidor de la URL en una dirección IP usando la base de datos distribuida de Internet conocida como DNS. Esta dirección IP es necesaria para contactar con el servidor web y poder enviarle paquetes de datos.

El siguiente paso es enviar una petición HTTP al servidor web solicitando el recurso. En el caso de una página web típica, primer se solicita el texto HTML y luego es inmediatamente analizado por el navegador, el cual, después, hace peticiones adicionales para los gráficos y otros ficheros que formen parte de la página. Las estadísticas de popularidad de un sitio web normalmente están basadas en el número de páginas vistas o las peticiones de servidor asociadas, o peticiones de fichero, que tienen lugar.

Al recibir los ficheros solicitados desde el servidor web, el navegador representa (renderiza) la página tal y como se describe en el código HTML, el CSS y otros lenguajes web. Al final se incorporan las imágenes y otros recursos para producir la página que ve el usuario en su pantalla.

Destacamos los siguientes estándares:

Berners-Lee dirige desde 2007 el World Wide Web Consortium (W3C), el cual desarrolla y mantiene esos y otros estándares que permiten a los ordenadores de la Web almacenar y comunicar efectivamente diferentes formas de información.

Un avance significativo en la tecnología web fue la Plataforma Java de Sun Microsystems. Este lenguaje permite que las páginas web contengan pequeños programas (llamados applets) directamente en la visualización. Estos applets se ejecutan en el ordenador del usuario, proporcionando una interfaz de usuario más rico que simples páginas web. Los applets Java del cliente nunca obtuvieron la popularidad que Sun esperaba de ellos, por una serie de razones, incluyendo la falta de integración con otros contenidos (los applets fueron confinados a pequeñas cajas dentro de la página renderizada) y el hecho de que muchos ordenadores del momento eran vendidos a los usuarios finales sin una JVM correctamente instalada, por lo que se necesitaba que el usuario descargara la máquina virtual antes de que el applet comenzara a aparecer. Hasta la llegada de HTML5, Adobe Flash desempeñó muchas de las funciones que originalmente se pensaron que podrían hacer los applets de Java incluyendo la ejecución de contenido de vídeo, animaciones y algunas características superiores de GUI. En estos momentos Java se utiliza más como plataforma y lenguaje para el lado del servidor y otro tipo de programación.

JavaScript, en cambio, es un lenguaje de script que inicialmente fue desarrollado para ser usado dentro de las páginas web. La versión estandarizada es el ECMAScript. Si bien los nombres son similares, JavaScript fue desarrollado por Netscape y no tiene relación alguna con Java, aparte de que sus sintaxis derivan del lenguaje de programación C. En unión con el Document Object Model de una página web, JavaScript se ha convertido en una tecnología mucho más importante de lo que pensaron sus creadores originales. La manipulación del Modelo de Objetos de Documento después de que la página ha sido enviada al cliente se ha denominado HTML Dinámico (DHTML), para enfatizar un cambio con respecto a las visualizaciones de HTML "estático".

En su forma más simple, toda la información opcional y las acciones disponibles en las páginas web con JavaScript ya son cargadas la primera vez que se envía la página. Ajax ("Asynchronous JavaScript And XML", en español, JavaScript Asíncrono y XML) es una tecnología basada en JavaScript que puede tener un efecto significativo para el desarrollo de la Web. Ajax proporciona un método por el cual grandes o pequeñas partes "dentro" de una página web pueden actualizarse, usando nueva información obtenida de la red en respuesta a las acciones del usuario. Esto permite que la página sea mucho más confiable, interactiva e interesante, sin que el usuario tenga que esperar a que se cargue toda la página. Ajax es visto como un aspecto importante de lo que suele llamarse Web 2.0. Ejemplos de técnicas Ajax usadas actualmente pueden verse en Gmail, Google Maps, etc.

La Web, tal y como la conocemos hoy día, ha permitido un flujo de comunicación global a una escala sin precedentes en la historia humana. Personas separadas en el tiempo y el espacio, pueden usar la Web para intercambiar- o incluso desarrollar mutuamente- sus pensamientos más íntimos, o alternativamente sus actitudes y deseos cotidianos. Experiencias emocionales, ideas políticas, cultura, idiomas musicales, negocio, arte, fotografías, literatura... todo puede ser compartido y diseminado digitalmente con el menor esfuerzo, haciéndolo llegar casi de forma inmediata a cualquier otro punto del planeta. Aunque la existencia y uso de la Web se basa en tecnología material, que tiene a su vez sus propias desventajas, esta información no utiliza recursos físicos como las bibliotecas o la prensa escrita. Sin embargo, la propagación de información a través de la Web (vía Internet) no está limitada por el movimiento de volúmenes físicos, o por copias manuales o materiales de información. Gracias a su carácter virtual, la información en la Web puede ser buscada más fácil y eficientemente que en cualquier medio físico, y mucho más rápido de lo que una persona podría recabar por sí misma a través de un viaje, correo, teléfono, telégrafo, o cualquier otro medio de comunicación.

La Web es el medio de mayor difusión de intercambio personal aparecido en la Historia de la Humanidad, muy por delante de la imprenta. Esta plataforma ha permitido a los usuarios interactuar con muchos más grupos de personas dispersas alrededor del planeta, de lo que es posible con las limitaciones del contacto físico o simplemente con las limitaciones de todos los otros medios de comunicación existentes combinados.

Como bien se ha descrito, el alcance de la Red hoy día es difícil de cuantificar. En total, según las estimaciones de 2010, el número total de páginas web, bien de acceso directo mediante URL, bien mediante el acceso a través de enlace, es de más de 27 000 millones; es decir, unas tres páginas por cada persona viva en el planeta. A su vez, la difusión de su contenido es tal, que en poco más de 10 años, hemos codificado medio billón de versiones de nuestra historia colectiva, y la hemos puesto frente a 1.900 millones de personas. Es en definitiva, la consecución de una de las mayores ambiciones de la humanidad: desde la antigua Mongolia, pasando por la Biblioteca de Alejandría o la mismísima Enciclopedia de Rousseau y Diderot, la humanidad ha tratado de recopilar en un mismo tiempo y lugar todo el saber acumulado desde sus inicios hasta ese momento. El hipertexto ha hecho posible ese sueño.

Como la Web tiene un ámbito de influencia global, se ha sugerido su importancia en la contribución al entendimiento mutuo de las personas por encima de fronteras físicas o ideológicas.

La Web está disponible como una plataforma más englobada dentro de los mass media. Para "publicar" una página web, no es necesario acudir a un editor ni otra institución, ni siquiera poseer conocimientos técnicos más allá de los necesarios para usar un editor de texto estándar.

A diferencia de los libros y documentos, el hipertexto no necesita de un orden lineal de principio a final. No precisa de subdivisiones en capítulos, secciones, subsecciones, etc.

Aunque algunos sitios web están disponibles en varios idiomas, muchos se encuentran únicamente en su idioma local. Adicionalmente, no todos los softwares soportan todos los caracteres especiales, y lenguajes RTL. Estos factores son algunas de las puntualizaciones que faltan por unificarse en aras de una estandarización global. Por lo general, a exclusión de aquellas páginas que hacen uso de grafías no románicas, es cada vez más generalizado el uso del formato Unicode UTF-8 como codificador de caracteres.

Las facilidades gracias a las cuales hoy día es posible publicar material en la Web quedan patentes en el número al alza de nuevas páginas personales, en las aquellas con fines comerciales, divulgativas, blogueros, etc. El desarrollo de aplicaciones gratuitas capaces de generar páginas web de una manera totalmente gráfica e intuitiva, así como un número emergente de servicios de alojamiento web sin coste alguno han contribuido a este crecimiento sin precedentes.

En muchos países los sitios web publicados deben respetar la accesibilidad web, viniendo regulado dicho concepto por Normativas o Pautas que indican el nivel de accesibilidad de dicho sitio:

Una encuesta de 1207 sobre 2 024 millones de páginas web determinó que la mayoría del contenido web estaba en inglés (56,4 %), frente a un 7,7 % de páginas en alemán, un 5,6 % en francés y un 4,95 % en japonés. Otro estudio más reciente que realizaba búsquedas de páginas en 75 idiomas diferentes, determinó que había sobre 11 500 millones de páginas web en la Web pública indexable a finales de enero del 2005. No obstante, cabe reseñar que este dato ha sido extraído de los bancos de datos de Google atendiendo a los nombres de dominio y, por tanto, muchas de las referencias a las que apuntan son meros redireccionamientos a otras webs.

La frustración sobre los problemas de congestión en la infraestructura de Internet y la alta latencia que provoca la lenta navegación, ha llevado a crear un nombre alternativo para la World Wide Web: la "World Wide Wait" (en español, la "Gran espera mundial"). Aumentar la velocidad de Internet es una discusión latente sobre el uso de tecnologías de peering y QoS. Otras soluciones para reducir las esperas de la Web se pueden encontrar en W3C.

Las guías estándar para los tiempos de respuesta ideales de las páginas web son (Nielsen 1999, página 42):
Estos tiempos son útiles para planificar la capacidad de los servidores web.

En inglés, www. es el acrónimo de tres letras más largo de pronunciar, necesitando nueve sílabas. Según Douglas Adams:

La pronunciación correcta según la RAE es popularmente conocida como «triple uve doble, punto» o «uve doble, uve doble, uve doble, punto». Sin embargo, muchas veces se abrevia como «tres uves dobles, punto». En algunos países de habla hispana, como México, Colombia, Panamá y Rep. Dom., se suele pronunciar «triple doble u, punto», «triple doble v, punto» o «doble u, doble u, doble u, punto». Mientras que en Bolivia, Cuba, Argentina, Venezuela, Chile, Ecuador, Paraguay, Perú, Uruguay y Nicaragua «triple doble ve, punto» o «doble ve, doble ve, doble ve, punto».

En chino, la World Wide Web normalmente se traduce por "wàn wéi wǎng" (), que satisface las «www» y que significa literalmente «red de 10 mil dimensiones».

En italiano, se pronuncia «vu vu vu» y en alemán, «ve ve ve».

Lo siguiente es una lista de los documentos que definen los tres estándares principales de la Web:




Con el paso del tiempo, muchos recursos web enlazados por hiperenlaces desaparecen, se cambia su localización, o son reemplazados con distinto contenido. Este fenómeno se denomina en algunos círculos como enlaces rotos y los hiperenlaces afectados por esto suelen llamarse "enlaces muertos".

La naturaleza efímera de la Web ha hecho aparecer muchos esfuerzos de almacenar la Web. El archivo de Internet es uno de los esfuerzos más conocidos, llevan almacenando la Web desde 1996.

El mayor evento académico relacionado con la WWW es la serie de conferencias promovidas por IW3C2. Hay una lista con enlaces a todas las conferencias de las series.

Es muy común encontrar el prefijo "WWW" al comienzo de las direcciones web debido a la costumbre de nombrar a los host de Internet (los servidores) con los servicios que proporcionan. De esa forma, por ejemplo, el nombre de host para un servidor web normalmente es "WWW", para un servidor FTP se suele usar "ftp", y para un servidor de noticias, USENET, "news" o "nntp" (en relación al protocolo de noticias NNTP). Estos nombres de host aparecen como subdominio de DNS, como en "www.example.com".

El uso de estos prefijos no está impuesto por ningún estándar, de hecho, el primer servidor web se encontraba en "nxoc01.cern.ch" e incluso hoy en día existen muchos sitios Web que no tienen el prefijo "www". Este prefijo no tiene ninguna relación con la forma en que se muestra el sitio web principal. El prefijo "www" es simplemente una elección para el nombre de subdominio del sitio web.

Algunos navegadores web añaden automáticamente "www." al principio, y posiblemente ".com"" al final, en las URL que se teclean, si no se encuentra el host sin ellas. Internet Explorer, Mozilla Firefox y Opera también añadirán "<nowiki>http://www/.</nowiki>" y ".com" al contenido de la barra de dirección si se pulsan al mismo tiempo las teclas de y (tecla enter). Por ejemplo, si se teclea "ejemplo" en la barra de direcciones y luego se pulsa solamente o normalmente buscará <nowiki>"http://www.ejemplo.com"</nowiki>, dependiendo de la versión exacta del navegador y su configuración.

Las tecnologías web implican un conjunto de herramientas que nos facilitarán lograr mejores resultados a la hora del desarrollo de un sitio web.




Strum, C. (6 de agosto de 2012). Fayer Wayer. Consultado el 18 de noviembre de 2014, de Fayer Wayer: http://www.fayerwayer.com/2012/08/visita-la-primera-pagina-web-del-mundo-creada-hace-mas-de-20-anos/



</doc>
<doc id="4808" url="https://es.wikipedia.org/wiki?curid=4808" title="RPM Package Manager">
RPM Package Manager

RPM Package Manager (o RPM, originalmente llamado Red Hat Package Manager, pero se convirtió en acrónimo recursivo) es una herramienta de administración de paquetes pensada básicamente para GNU/Linux. Es capaz de instalar, actualizar, desinstalar, verificar y solicitar programas. RPM es el formato de paquete de partida del Linux Standard Base.

Originalmente desarrollado por Red Hat para Red Hat Linux, en la actualidad muchas distribuciones GNU/Linux lo usan, dentro de las cuales las más destacadas son Fedora, Mandriva, Mageia, PCLinuxOS, openSUSE, SuSE Linux. También se ha portado a otros sistemas operativos.

Para el administrador de sistemas que realice mantenimiento y actualización de software, el uso de gestor de paquetes en vez de construirlos manualmente tiene ventajas como simplicidad, consistencia y la capacidad de que aquellos procesos se automaticen.

Entre las características de RPM están:
Algunos Comandos: 

rpm -qa = muestra paquetes instalados.

rpm -qi foo = muestra la información de un paquete RPM.

rpm -ql foo = lista ficheros de un paquete RPM instalado.

rpm -qc foo = lista solo los ficheros de configuración.

rpm --checksig foo = verifica firma de un paquete RPM.

rpm -ivh "localFile.rpm" = instala un paquete.

rpm -e "localFile.rpm" = desinstala un paquete.




</doc>
<doc id="4811" url="https://es.wikipedia.org/wiki?curid=4811" title="San Luis Potosí">
San Luis Potosí

San Luis Potosí es uno de los treinta y un estados que, junto con la Ciudad de México, conforman México. Su capital y ciudad más poblada es la homónima San Luis Potosí. Está ubicado en la región centronorte del país, limitando al norte con Nuevo León y Tamaulipas, al este con Veracruz de Ignacio de la Llave, al sur con Hidalgo, Querétaro y Guanajuato, y al oeste con Zacatecas. Fue fundado el 3 de noviembre de 1592. Forma parte de la Alianza Bajío-Occidente.

Se divide en 58 municipios. Aparte de la capital, otros municipios importantes son Matehuala, Tamasopo, Río Verde, Tamuín, Ciudad Valles, Tamazunchale, Cerro de San Pedro, Vanegas, Cerritos, San Vicente Tancuayalab, Charcas.

En la época prehispánica el territorio que ahora ocupa el estado de San Luis Potosí comprendía las áreas culturales de Mesoamérica y Aridoamérica. Su parte norte y centro-oeste fue habitada por las tribus otomíes y chichimecas, los cuales eran muchos grupos indígenas, principalmente cazadores y recolectores, lo que los obligaba a vivir sin asiento fijo; en el este y sureste aún habitan los grupos huasteco, "xi, o'ui" o pame y náhuatl.

En 1592, se descubrieron depósitos de oro y plata, con esto se inició el desarrollo de la zona. Los mineros se congregaron a poblar donde ahora se asienta la ciudad de San Luis Potosí, y Juan de Oñate fue nombrado el primer alcalde de «Pueblo de San Luis de Mezquitique». Se le dio el nombre de "San Luis Rey" en honor a Luis IX de Francia, y "Potosí" porque se comparó con las ricas minas de plata en la actual Bolivia, en espera de rivalizar con estas, aunque esto nunca se cumplió. En los siglos XVII y XVIII, franciscanos, agustinos, y jesuitas se establecieron y empezaron a edificar iglesias y edificios, muchos de las cuales aún siguen en pie y se han convertido en universidades y museos.

A mediados de 1821, después de la Independencia de México, el general José Antonio Echavarri intimidó al Intendente y al Ayuntamiento a la rendición de la plaza de San Luis al Ejército de las Tres Garantías de Iturbide. Ellos se sometieron a su exigencia, pues no había manera de resistir, y así se proclamó la Independencia de San Luis Potosí. Después, se dictó la primera Constitución Política del Estado de San Luis Potosí el 16 de octubre de 1826, y esta estuvo vigente hasta 1835 en que el Congreso Nacional decretó el sistema Centralista. Así desaparecieron las Legislaturas locales y los gobernadores fueron nombrados por el gobierno central. Esta situación subsistió hasta que se promulgó la Constitución de 1857.

La participación del estado potosino en la Invasión Americana en los años de 1846-1847 hizo que fuera llamado «San Luis de la Patria» por haber aportado gran cantidad de caudillos y elementos. En la Guerra de Reforma, la participación del estado potosino fue muy destacada, y durante la Intervención Francesa en 1863, la ciudad de San Luis Potosí fue declarada capital del país por el presidente Benito Juárez.

Durante el régimen del emperador Maximiliano de Habsburgo, San Luis Potosí fue convertido en departamento. La ciudad estuvo en poder de los imperialistas hasta fines de 1866. En ese año fue inaugurada la línea telegráfica entre la ciudad de San Luis Potosí y la ciudad de México.

El estado de San Luis Potosí se encuentra localizado en la altiplanicie central mexicana.

La superficie total del estado es de 60 546.79 km² y representa aproximadamente el tres por ciento de la superficie total del país.

Los límites geográficos, en coordenadas geográficas, son 24°29' (norte), 21°10' (sur) de latitud norte; 98°20' (este), 102°18' (oeste) de longitud oeste. Sus colindancias son: al noreste, con Nuevo León y Tamaulipas; al este, con Veracruz; al sur, con Hidalgo, Querétaro y Guanajuato; y al oeste, con Zacatecas.

El estado de San Luis Potosí cuenta con 58 municipios, los cuales se encuentran distribuidos en 4 regiones principales: Región Huasteca, Región Media, Región Centro y Altiplano Potosino.

El estado cuenta con varios elementos protegidos incluidos en el sistema federal de áreas protegidas administradas por la Comisión Nacional de Áreas Naturales Protegidas (CONANP):


Tiene también un lugar declarado sitio Ramsar (n. ref 1766, declarado en 2008), Arroyos y Manantiales de Tanchachín

Además, tiene otras 12 áreas naturales protegidas de competencia estatal administradas por la SEGAM (Secretaría de Ecología y Gestión Ambiental):

El estado de San Luis Potosí es rico en minerales como: cobre, plata, zinc, antimonio, mercurio y estaño, además es conocido como uno de los principales productores mundiales de Fluorita. Sus industrias comprenden plantas de beneficios minerales como el antimonio en especial; hilados y tejidos, conservas alimenticias

En la época prehispánica, el territorio de lo que hoy es el Estado de San Luis Potosí fue frontera cultural de las regiones Aridoamérica y Mesoamérica permitiendo una gran diversidad de pueblos y costumbres.

La zona huasteca estuvo habitada por pueblos mesoamericanos, pueblos que compartieron cientos de años atrás la misma cultura que los mayas y que posiblemente se separaron de ellos en el preclásico cuando los zapotecas y olmecas fueron incrementando su presencia hacia el centro y sur de lo que hoy es Veracruz. Los huastecos dejaron huella de su presencia en los numerosos sitios arqueológicos que hoy todavía se encuentran en los Estados de Tamaulipas, Hidalgo, Veracruz y San Luis Potosí. En La Huasteca potosina dos sitios se encuentran en investigación: Tamtok y El Consuelo, ambos ubicados en el municipio de Tamuín. Durante el epiclásico La Huasteca fue invadida por culturas nahuas. Hoy día La Huasteca tiene presencia teenek y náhuatl.

La zona media de San Luis Potosí fue una zona poblada, en la época prehispánica, por las culturas pame y otomí. Estas culturas –como en toda frontera– compartían las formas de vida de ambas regiones. Es decir, compartían tanto la forma agrícola-sedentaria como la nómada-cazadora. Actualmente viven en la zona media culturas pame que se llaman a sí mismos xi'oi.

En el Altiplano Potosino, en el territorio conocido como el Gran Tunal se desarrolló la cultura chichimeca (zacatecos, copuces, guamares, jonaces, huachichiles, etc.). Estos pobladores chichimecas desarrollaron modelos culturales diferentes a los mesoamericanos (que destacan por su vida sedentaria y por su interés en construir ciudades y grandes centros teocráticos) extendiéndose este modo de vida por toda Norteamérica. Los chichimecas eran hábiles cazadores y guerreros. Los pueblos nahuas durante el postclásico intentaron dominar (como lo habían hecho por muchos territorios mesoamericanos) las tierras chichimecas, sin lograrlo. De ahí el odio cultural hacia la cultura chichimeca y el surgimiento de los primeros discursos que acusan a lo chichimeca como "salvaje", "inculto", "pagano", "inhumano", incluso "caribe". Los españoles en el momento de la conquista repetirán el mismo discurso al mantener durante 50 años una guerra infructuosa: la llamada "Guerra Chichimeca".

El estado de San Luis Potosí es rico en cultura, herencia e historia. La Tribu Pame se divide en dos regiones: Pame Norte, que comprende los municipios de Alaquines, Cárdenas, Ciudad del Maíz, partes de Rioverde, Tamasopo, y El Naranjo; y Pame Sur, que comprende los municipios de Rayón, Lagunillas, Santa Catarina, y ciertas partes de Tamasopo. Estas tribus indígenas hablan la lengua pame, pero ciertas palabras cambian entre Pame Norte y Pame Sur.

San Luis Potosí posee una provechosa ubicación en el territorio mexicano debido a que es un punto intermedio entre las tres ciudades más importantes del país: la Ciudad de México, Monterrey y Guadalajara y entre 4 grandes puertos de altura: Tampico, Altamira, Manzanillo y Mazatlán. Además, sus climas variados, así como su red carretera y ferroviaria la cual satisface sus necesidades de intercambio comercial, le permiten ser uno de los pocos estados del país en los que se puede desarrollar una infraestructura empresarial importante.

El impacto económico del estado se debe a varios factores como el turismo, la industria y su ubicación geográfica que permite el desplazamiento rápido de productos a casi cualquier parte de la República Mexicana. Sin embargo, este desarrollo económico solo se ve en la capital del estado, lo que ha motivado una gran migración a la zona conurbada de las zonas rurales así como a otros estados vecinos como Nuevo León y Tamaulipas y principalmente a los Estados Unidos.

No obstante ha experimentado en los últimos meses un crecimiento económico (especialmente en la ciudad capital San Luis Potosí), debido a políticas locales que han aprovechado su ubicación geográfica. Hoy en día se han desarrollado una gran cantidad de Parques Industriales que han impulsado al sector manufacturero e industrial. 

En especial en la industria automotriz, a partir de la llegada de una planta armadora de General Motors, instalada en el 2012, En el 2015 se inició la construcción de una planta armadora de BMW, lo que coloca a San Luis Potosí dentro de los tres estados con mayor enfoque automotriz en México.

Según los datos que arrojó el "II Censo de Población y Vivienda" realizado por el Instituto Nacional de Estadística y Geografía (INEGI) con fecha censal del 12 de junio de 2012, el estado de San Luis Potosí contaba hasta ese año con un total de 2 585 518 habitantes, de dicha cantidad, 1 260 366 eran hombres y 1 325 152 eran mujeres. La tasa de crecimiento anual para la entidad durante el período 2005-2010 fue del 1.4 %.

La siguiente tabla muestra las 20 ciudades más pobladas del estado de San Luis Potosí sin contar Área Conurbada, Municipio o Zona Metropolitana.

San Luis Potosí es un estado que cuenta con una oferta educativa amplia y distribuida en varias de sus regiones. 

Una de las instituciones de educación superior más importantes del estado es la Universidad Autónoma de San Luis Potosí cuya sede principal está en la ciudad capital del estado, contando además con "campus" en las ciudades de Matehuala, Rioverde, Ciudad Valles y Tamazunchale.

La formación de docentes ha estado a cargo de La Benemérita y Centenaria Escuela Normal del Estado de San Luis Potosí fundada en 1849, ésta, es la segunda institución formadora de docentes del país, desde el siglo XIX, además de la Universidad Pedagógica Nacional UPN Unidad 241 se estableció el 16 de octubre de 1979.

La Universidad Tangamanga, parte del grupo Aliat Universidades actualmente cuenta con 4 campus en todo el estado de San Luis Potosí: Tequis, Saucito, Huasteca e Industrias. Actualmente se posiciona como una de las 11 Universidades privadas más reconocidas a nivel de educación superior.

Otra institución de importancia es el Instituto Tecnológico y de Estudios Superiores de Monterrey Campus San Luis Potosí, parte del Sistema Tecnológico de Monterrey, que destaca por los centros de investigación e innovación con los que cuenta, centrados en el desarrollo económico y social de la región, del Tecnológico de Monterrey Campus San Luis Potosí.

El Tecnológico Nacional de México tiene en el Estado, planteles del Instituto Tecnológico de San Luis Potosí, del Instituto Tecnológico Superior de Tamazunchale (ITST), el Instituto Tecnológico Superior de Cd. Valles y del Instituto Tecnológico de Matehuala.

La oferta educativa se completa con la Universidad Abierta, el Centro de Investigación para la Administración Educativa, la Universidad Cuauhtémoc, Universidad TecMilenio, el Instituto Tecnológico de San Luis Potosí, la Universidad del Centro de México, la Universidad Marista de San Luis Potosí, la Universidad Politécnica de San Luis Potosí, la Universidad del Valle de México y la Universidad Tecnológica de San Luis Potosí, esta última, certificada en ISO 9001:2000. 

Recientemente se fundaron el Colegio de San Luis y el Instituto Potosino de Investigación Científica y Tecnológica (IPICYT).

La comida típica en San Luis Potosí se compone por tradición indígena principalmente de maíz, que al fusionarse con la comida española incorpora carnes como la de puerco y de pollo. Su variedad es grande y se pueden disfrutar platillos como el fiambre potosino o los famosos tacos rojos de queso, servidos con zanahorias y papas asadas, espolvoreados con queso fresco. Los postres incluyen el queso de tuna y los elaborados con leche de cabra como natillas y cajetas o las tradicionales campechanas de Santa María del Río.

Los platillos cambian dependiendo de la zona geográfica: en la Huasteca son tradicionales los tamales de hoja de plátano así como el zacahuil, en el Altiplano son característicos el asado de boda y la barbacoa de hoyo, en la Zona Media los antojitos como quesadillas, flautas y gorditas mientras que en la Zona Centro las enchiladas potosinas son el platillo principal.




</doc>
<doc id="4819" url="https://es.wikipedia.org/wiki?curid=4819" title="ARCNET">
ARCNET

ARCNET, siglas de "Attached Resource Computer NETwork", fue una arquitectura de red de área local que utiliza la técnica de acceso de paso de testigo, como Token Ring.

Fue desarrollado por "Datapoint Corporation", en 1977.

Conocido también como CamelCased, ARCnet, ARCANET.

La topología física es en forma de estrella, mientras que la topología lógica es en forma de anillo, utilizando cable coaxial y concentradores ("hub") pasivos (hasta cuatro conexiones) o activos.


La velocidad de trasmisión rondaba los 2 Mbps, aunque al no producirse colisiones el rendimiento era equiparable al de las redes Ethernet.

Las velocidades de sus transmisiones son de 2,6 Mbits/s.

Soporta longitudes de hasta 609 m (2000 pies).

ARCNET es un protocolo de la red de área local (LAN), similar en propósito a Ethernet o a Token Ring.

ARCNET fue el primer sistema extensamente disponible del establecimiento de una red para las microcomputadoras y llegó a ser popular en los años 1980, para las tareas de la ofimática.

Originalmente, ARCNET utilizó el cable coaxial de RG-62/U y los "hub" pasivos o activos en una topología en bus "star-wired".

A la hora de su renombre más grande, ARCNET gozó de dos ventajas importantes sobre Ethernet:

Por supuesto, ARCNET requirió un "hub" activo o pasivo entre los nodos si había más de dos nodos en la red, mientras que Ethernet finalmente permitió que los nodos fueran espaciados dondequiera a lo largo del cable coaxial lineal, aunque los "hubs" pasivos de ARCNET eran muy baratos.

Para mediar el acceso en bus, ARCNET utiliza un esquema símbolo que pasa, un poco diferente de ese usado por Token Ring. Cuando los pares son inactivos, un solo mensaje "simbólico" se pasa alrededor de la red de máquina a máquina, y no se permite a ningún par utilizar el bus a menos que tenga el símbolo. Si un par en particular desea enviar un mensaje, espera para recibir el símbolo, envía su mensaje, y después pasa el símbolo encendido a la estación siguiente. Cada acercamiento tiene sus ventajas: ARCNET agrega un pequeño retraso en una red inactiva mientras que una estación que envía espera para recibir el símbolo, pero el funcionamiento de Ethernet puede degradar drásticamente si muchos pares procuran también difundir en el mismo tiempo.

Empezaron a entrar en desuso en favor de Ethernet al bajar los precios de esta última.

w(en mi doke ? 2"
class="q



</doc>
<doc id="4820" url="https://es.wikipedia.org/wiki?curid=4820" title="Token Ring">
Token Ring

Token Ring es una arquitectura de red desarrollada por IBM en los años 1970 con topología lógica en anillo y técnica de acceso de paso de testigo, usando un frame de 3 bytes llamado token que viaja alrededor del anillo. Token Ring se recoge en el estándar IEEE 802.5. En desuso por la popularización de Ethernet; actualmente no es empleada en diseños de redes.

El IEEE 802.5 es un estándar por el "Institute of Electrical and Electronics Engineers" (IEEE), y define una red de área local LAN en configuración de anillo (Ring), con método de paso de testigo (Token) como control de acceso al medio. La velocidad de su estándar es de 4 o 16 Mbps cuando es implementado sobre cables de hilos de cobre, existen implementaciones de mayor velocidad tanto sobre hilos de cobre CDDI como sobre fibra óptica FDDI la cual llega a los 100 Mbps y 200 km de extensión.

El diseño de una red de Token Ring fue atribuido a E. E. Newhall en el año 1969. IBM publicó por primera vez su topología de Token Ring en marzo de 1982, cuando esta compañía presentó los papeles para el proyecto 802 del IEEE. IBM anunció un producto Token Ring en 1984, y en 1985 este llegó a ser un estándar de ANSI/IEEE.

Es casi idéntica y totalmente compatible con la red del token ring de IBM. De hecho, la especificación de IEEE 802.5 fue modelada después del token ring, y continúa a la sombra de ésta. Además, el token ring de la IBM especifica una estrella, con todas las estaciones del extremo unidas a un dispositivo al que se le llama "unidad del acceso multiestación" (MSAU). En contraste, IEEE 802.5 no especifica una topología, aunque virtualmente todo el IEEE 802.5 puesto en práctica se basa en una estrella, y tampoco especifica un tipo de medios, mientras que las redes del token ring de la IBM utilizan el tamaño del campo de información de encaminamiento.

El IEEE 802.5 soporta dos tipos de frames básicos: tokens y frames de comandos y de datos. El Token es una trama que circula por el anillo en su único sentido de circulación. Cuando una estación desea transmitir y el Token pasa por ella, lo toma. Este sólo puede permanecer en su poder un tiempo determinado (10 ms). Tienen una longitud de 3 bytes y consiste en un delimitador de inicio, un byte de control de acceso y un delimitador de fin. En cuanto a los Frames de comandos y de datos pueden variar en tamaño, dependiendo del tamaño del campo de información. Los frames de datos tienen información para protocolos mayores, mientras que los frames de comandos contienen información de control.

Existen varías diferencia notables entre Token Ring y Ethernet:




Tips:





</doc>
<doc id="4825" url="https://es.wikipedia.org/wiki?curid=4825" title="Red en árbol">
Red en árbol

La red en árbol es una topología de red en la que los nodos están colocados en forma de árbol. Desde una visión topológica, es parecida a una serie de redes en estrella interconectadas salvo en que no tiene un concentrador central. En cambio, tiene un nodo de enlace troncal, generalmente ocupado por un hub o switch, desde el que se ramifican los demás nodos. Es una variación de la red en bus, el fallo de un nodo no implica una interrupción en las comunicaciones. Se comparte el mismo canal de comunicaciones.

La topología en árbol puede verse como una combinación de varias topologías en estrella. Tanto la de árbol como la de estrella son similares a la de bus cuando el nodo de interconexión trabaja en modo difusión, pues la información se propaga hacia todas las estaciones, solo que en esta topología las ramificaciones se extienden a partir de un punto raíz (estrella), a tantas ramificaciones como sean posibles, según las características del árbol.

Los problemas asociados a las topologías anteriores radican en que los datos son recibidos por todas las estaciones sin importar para quién vayan dirigidos. Es entonces necesario dotar a la red de un mecanismo que permita identificar al destinatario de los mensajes, para que estos puedan recogerlos a su arribo. Además, debido a la presencia de un medio de transmisión compartido entre muchas estaciones, pueden producirse interferencia entre las señales cuando dos o más estaciones transmiten al mismo tiempo. Es la mejor topología de red que existe y con ella los datos fluyen de una manera más rápida que en los otros tipos de topologías de red.




</doc>
<doc id="4826" url="https://es.wikipedia.org/wiki?curid=4826" title="Red en bus">
Red en bus

Una red en bus es aquella topología que se caracteriza por tener un único canal de comunicaciones (denominado bus, troncal o backbone) al cual se conectan los diferentes dispositivos. De esta forma todos los dispositivos comparten el mismo canal.

Los extremos se terminan con una resistencia de acople denominada el "terminador", que además de indicar que no existen más ordenadores en el extremo, permiten cerrar el bus por medio de un acople de impedancias. 

Es la tercera de las topologías principales. Las estaciones están conectadas por un único segmento de cable. A diferencia de una red en anillo, el bus es pasivo, no se produce generación de señales en cada nodo o router.
En la topología de bus todos los nodos (computadoras) están conectados a un circuito común (bus). La información que se envía de una computadora a otra viaja directamente o indirectamente, si existe un controlador que enruta los datos al destino correcto. La información viaja por el cable en ambos sentidos a una velocidad aproximada de 10/100 Mbps y tiene en sus dos extremos una resistencia (terminador). Se pueden conectar una gran cantidad de computadoras al bus, si un computador falla, la comunicación se mantiene, no sucede lo mismo si el bus es el que falla. El tipo de cableado que se usa puede ser coaxial, par trenzado o fibra óptica. En una topología de bus, cada computadora está conectada a un segmento común de cable de red. El segmento de red se coloca como un bus lineal, es decir un cable largo que va de un extremo a otro de la red, y al cual se conecta cada nodo de ésta. El cable puede ir por el piso, las paredes, el techo o por varios lugares, siempre y cuando sea un segmento continuo.





</doc>
<doc id="4827" url="https://es.wikipedia.org/wiki?curid=4827" title="Cable de par trenzado">
Cable de par trenzado

En telecomunicaciones, el cable de par trenzado es un tipo de cable que tiene dos conductores eléctricos aislados y entrelazados para anular las interferencias de fuentes externas y diafonía de los cables adyacentes. Fue inventado por Alexander Graham Bell en 1881.

El cable de par trenzado consiste en grupos de hilos de cobre entrelazados en pares en forma helicoidal. Esto se hace porque dos alambres paralelos constituyen una antena simple. Cuando se entrelazan los alambres helicoidalmente, las ondas se cancelan, por lo que la interferencia producida por los mismos es reducida lo que permite una mejor transmisión de datos.

Así, la forma entrelazada permite reducir la interferencia eléctrica tanto exterior como de pares cercanos y permite transmitir datos de forma más fiable. Un cable de par trenzado está formado por un grupo de pares entrelazados (normalmente 2, 4 o 25 pares), recubiertos por un material aislante. Cada uno de estos pares se identifica mediante un color.

El entrelazado de cables que llevan señal en modo diferencial (es decir que una es la invertida de la otra), tiene dos motivos principales:


En la historia de las telecomunicaciones, el cable de par trenzado ha tenido un rol fundamental. Este tipo de cable es el más común y se originó como solución para conectar teléfonos, terminales y computadoras sobre el mismo cableado, ya que está habilitado para comunicación de datos permitiendo transmisiones con frecuencias más altas. Con anterioridad, en Europa, los sistemas de telefonía empleaban cables de pares no trenzados, para poder comunicarse.

Los primeros teléfonos utilizaban líneas telegráficas, o alambres abiertos de un solo conductor de circuitos de conexión a tierra. En la década de 1880 fueron instalados tranvías eléctricos en muchas ciudades de Estados Unidos, lo que indujo ruido en estos circuitos. Al ser inútiles las demandas por este asunto, las compañías telefónicas pasaron a los sistemas de circuitos balanceados, que tenían el beneficio adicional de reducir la atenuación, y por lo tanto, proporcionaban mayor alcance.

Cuando la distribución de energía eléctrica se hizo cada vez más común, esta medida resultó insuficiente. Dos cables, colgados a ambos lados de las barras cruzadas en los postes de alumbrado público, compartían la ruta con las líneas de energía eléctrica. En pocos años, el creciente uso de la electricidad trajo de nuevo un aumento de la interferencia, por lo que los ingenieros idearon un método llamado “transposición de conductores”, para cancelar la interferencia.

En este método, los conductores intercambiaban su posición una vez por cada varios postes. De esta manera, los dos cables recibirían similares interferencias electromagnéticas de las líneas eléctricas, pero alternativamente por uno u otro cable. Esto representó una rápida implementación del trenzado, a razón de unos cuatro trenzados por kilómetro, o seis por milla. Estas líneas balanceadas de alambre abierto con transposiciones periódicas aún subsisten, hoy en día, en algunas zonas rurales de Estados Unidos.

Los cables de par trenzado fueron inventados por el escocés Alexander Graham Bell en 1881 quien presentó una solicitud ante la Oficina de patentes de Estados Unidos el 4 de junio de ese año, siendo concedida un mes y 15 días después. En 1900, el conjunto de la red estadounidense de la línea telefónica era o de par trenzado o hilo abierto con la transposición a la protección contra interferencias. Hoy en día, la mayoría de los millones de kilómetros de pares trenzados en el mundo está fija en instalaciones aéreas, propiedad de las compañías telefónicas, y se utiliza para el servicio de voz, y sólo son manejados o incluso vistos por los trabajadores telefónicos.


La especificación "568A Commercial Building Wiring Standard" de la EIA/TIA (Alianza de Industrias Electrónicas (EIA) y la Asociación de la Industria de Telecomunicaciones (TIA) específica el tipo de cable UTP que se utilizará en cada situación y construcción. Dependiendo de la velocidad de transmisión, ha sido dividida en diferentes categorías de acuerdo a esta tabla:

Está limitado en distancia, ancho de banda y tasa de datos. También destacar que la atenuación es una función fuertemente dependiente de la frecuencia. La interferencia y el ruido externo también son factores importantes, por eso se utilizan coberturas externas y el trenzado. Para señales analógicas se requieren amplificadores cada 5 o 6 kilómetros, para señales digitales cada 2 o 3. En transmisiones de señales analógicas punto a punto, el ancho de banda puede llegar hasta 250kHz. En transmisión de señales digitales a larga distancia, la velocidad de datos no es demasiado grande, no es muy efectivo para estas aplicaciones o dispositivos En redes locales que soportan ordenadores locales, la velocidad de datos puede llegar a 10Mbps (Ethernet) y 100Mbps (Fast Ethernet).

En el cable par trenzado de cuatro pares, normalmente solo se utilizan dos pares de conductores, uno para recibir (cables 3 y 4) y otro para transmitir (cables 1 y 2), aunque no se pueden hacer las dos cosas a la vez, teniendo una trasmisión half-dúplex. Si se utilizan los cuatro pares de conductores la transmisión es full-dúplex.



ANSI/TIA/EIA-568-B.2 establece parámetros de certificación para los siguientes ítems:

"MAPA DE CABLEADO:" Comprueba que el mapa de cableado coincida con el estándar de comprobación de la instalación realizada.

"LONGITUD:" La longitud en todos los pares del cable comprobado en función a la medida de propagación, en su retraso y la media del valor NVP. Una estructura de cable de cobre no podrá superar los 99m, y en el caso de FO dependerá del tipo de fibra utilizada.

"PERDIDA POR INSERCIÓN:" También denominada ATENUACIÓN, comprueba la perdida de señal de los enlaces por su inserción.

"PERDIDA POR PARADIAFONIA:" Se especifica como NEXT "(near end cross talk)" y mide la interferencia que hace un par sobre otro en el mismo extremo cercano. Comprueba par a par con sus respectivos cercanos esta interferencia o inducción. Se mide en el total de rango de frecuencias

"TOTAL DE PERDIDAS DE PARADIAFONIA:" Denominada PSNEXT, realiza una comprobación de cómo le afecta a un par la transmisión de datos combinada por el resto de los pares cercanos, por tanto se deberá realizar para a par con los 8 pares que componen el cable. Se mide en el total de rango de frecuencias.

"PERDIDA POR PARADIAFONIA EN EL EXTREMO CERCANO PAR A PAR:" FEXT "(far end cross talk)" mide la interferencia que un par de hilos en el extremo lejano causa sobre el par de hilos afectado en ese mismo extremo. ELFEXT mide la intensidad de la paradiafonía en el extremo remoto relativa a la señal atenuada que llega al final del cable. Se producen 24 pares de combinaciones posibles que se comprueban.

"TOTAL DE PERDIDAS POR PARADIAFONIA EN EL EXTREMO CERCANO (PSELFEXT):" ELFEXT es un parámetro combinado que combina el efecto del FEXT de tres pares respecto a uno solo, PSELFEXT realizará la suma de todas estas combinaciones.

"PERDIDA DE RETORNO:" La pérdida de retorno (RETURN LOSS) mide la pérdida total de energía reflectada en cada par de hilos. Se mide en los dos extremos y en cada par, y todo para el total de rango de frecuencias.

"CERTIFICACIÓN DE RETARDO SESGADO (DELAY SKEW):" Este parámetro muestra la diferencia en el retardo de propagación entre los cuatro pares. El par con el retardo de propagación menor es la referencia 0 del retardo sesgado.

Las instalaciones que poseen estos parámetros dentro de los rangos especificados por ANSI/TIA/EIA-568-B.2, son llamadas "instalaciones certificadas." Existe instrumental específico que realiza estas comprobaciones en pocos segundos; empero esta certificación debe realizarse en todas las bocas de red, por lo que certificar una instalación puede ser un trabajo arduo y extendido en el tiempo. 




</doc>
<doc id="4828" url="https://es.wikipedia.org/wiki?curid=4828" title="Cable">
Cable

El término cable significa en su origen "cuerda", de latín "Capulum", o también del Hebreo "Kabel" de "cuerda fuerte", pero en dónde se ha extendido más su popular uso de esta palabra es como cable eléctrico, y es precisamente en Israel dónde se usa ya de antaño el término Kabel al conductor eléctrico, en dónde se le llama cable eléctrico a una manguera de material aislante y protector que contiene un conductor ( también hilo eléctrico) aislado o conjunto de conductores aislados (conductor eléctrico), generalmente de cobre o aluminio.

Los cables que se usan para conducir electricidad y se fabrican generalmente de cobre, debido a la excelente conductividad de este material, o de aluminio que aunque posee menor conductividad es más ligero para la misma capacidad y típicamente más económico que el cobre.

Generalmente cuenta con aislamiento en el orden de 500 µm hasta los 5 cm; dicho aislamiento es plástico, su tipo y grosor dependerá del nivel de tensión de trabajo, la corriente nominal, de la temperatura ambiente y de la temperatura de servicio del conductor.

Un cable eléctrico se compone de:









Conductores de luz, (inglés: "coil") en este caso, el recubrimiento, si bien protege el conductor propiamente dicho, también evita la dispersión de la luz y con ello la pérdida de señal. Por ello se utiliza para enviar información a largas distancias de forma rápida y muy alta calidad.

Empleados para la transmisión mecánica de movimiento, o de cargas entre otros elementos mecánicos, como palancas, ruedas, y poleas; realizan su trabajo en tracción o rotación.



</doc>
<doc id="4829" url="https://es.wikipedia.org/wiki?curid=4829" title="Cable coaxial">
Cable coaxial

El cable coaxial, coaxil, coaxcable o coax, creado en la década de 1930, es un cable utilizado para transportar señales eléctricas de alta frecuencia que posee dos conductores concéntricos, uno central, llamado núcleo, encargado de llevar la información, y uno exterior, de aspecto tubular, llamado malla, blindaje o trenza, que sirve como referencia de tierra y retorno de las corrientes. Entre ambos se encuentra una capa aislante dieléctrica, de cuyas características dependerá principalmente la calidad del cable. Todo el conjunto suele estar protegido por una cubierta aislante (también denominada camisa exterior). 

El conductor central puede estar constituido por un alambre sólido o por varios hilos retorcidos de cobre; mientras que el exterior puede ser una malla trenzada, una lámina enrollada o un tubo corrugado de cobre o aluminio. En este último caso resultará un cable semirrígido.

Debido a la necesidad de manejar frecuencias cada vez más altas y a la digitalización de las transmisiones, en años recientes se ha sustituido paulatinamente el uso del cable coaxial por el de fibra óptica, en particular para distancias superiores a varios kilómetros, porque el ancho de banda de esta última es muy superior.

La elección del diseño afecta al tamaño, flexibilidad y las propiedades eléctricas del cable. Un cable coaxial consta de un núcleo de hilo de cobre rodeado por un aislante (o dieléctrico), un apantallamiento o blindaje de metal trenzado y una cubierta externa.

El núcleo de un cable coaxial transporta señales electrónicas que constituyen la información. Este núcleo puede ser sólido (normalmente de cobre) o de hilos. Rodeando al núcleo existe una capa aislante dieléctrica que la separa de la malla de hilo. La malla de hilo trenzada actúa como masa, y protege al núcleo del ruido eléctrico y de la distorsión que proviene de los hilos adyacentes. El núcleo y la malla deben estar separados uno del otro. Si llegaran a tocarse, se produciría un cortocircuito, y el ruido o las señales que se encuentren perdidas en la malla, atravesarían el hilo de cobre.

El apantallamiento tiene que ver con el trenzado o malla de metal (u otro material) que rodea los cables. El apantallamiento protege los datos que se transmiten, absorbiendo el ruido, de forma que no pasa por el cable y no existe distorsión de datos. Al cable que contiene una lámina aislante y una capa de apantallamiento de metal trenzado se le llama cable apantallado doble. Para grandes interferencias, existe el apantallamiento cuádruple. Este apantallamiento consiste en dos láminas aislantes, y dos capas de apantallamiento de metal trenzado.

Un cortocircuito ocurre cuando dos hilos o un hilo y una tierra se ponen en contacto. Este contacto causa un flujo directo de corriente (o datos) en un camino no deseado. En el caso de una instalación eléctrica común, un cortocircuito causará el chispazo y el fundido del fusible o del interruptor automático. Con dispositivos electrónicos que utilizan bajos voltajes, el efecto es menor, y casi no se detecta. Estos cortocircuitos de bajo voltaje causan un fallo en el dispositivo y lo normal es que se pierdan los datos que se estaban transfiriendo.

Una cubierta exterior no conductora (normalmente hecha de goma, teflón o plástico) rodea todo el cable para evitar las posibles descargas eléctricas.
El cable coaxial es más resistente a interferencias y atenuación que el cable de par trenzado; por esto hubo un tiempo en que fue el más usado. 

La malla de hilos absorbe las señales electrónicas perdidas, de forma que no afecten a los datos que se envían a través del cable interno. Por esta razón, el cable coaxial es una buena opción para grandes distancias y para soportar de forma fiable grandes cantidades de datos con un sistema sencillo.

En los cables coaxiales los campos debidos a las corrientes que circulan por el interno y externo se anulan mutuamente.

La característica principal de la familia RG-58 es el núcleo central de cobre. Se consideran los siguientes tipos:


La mayoría de los cables coaxiales tiene una impedancia característica de 50, 52, 75 o 93 ohmios, siendo la de 75 la más usual. La industria de RF usa nombres de tipo estándar para cables coaxiales. En las conexiones de televisión (por cable, satélite o antena), los cables RG-6 son los más comúnmente usados para el empleo en el hogar, y la mayoría de conexiones fuera de Europa es por conectores F.

Aquí mostramos unas tablas con las características:

Existen múltiples tipos de cable coaxial, cada uno con un diámetro e impedancia diferentes. El cable coaxial no es habitualmente afectado por interferencias externas, y es capaz de lograr altas velocidades de transmisión de datos en largas distancias. Por esa razón, se utiliza en redes de comunicación de banda ancha (CATV) y cables de banda base (Ethernet).

El tipo de cable que se debe utilizar depende de la ubicación del cable. Los cables coaxiales pueden ser de dos tipos: 

El policloruro de vinilo es un tipo de plástico utilizado para construir el aislante y la cubierta protectora del cable en la mayoría de los tipos de cable coaxial. El cable coaxial de PVC es flexible y se puede instalar fácilmente en cualquier lugar. Sin embargo, cuando se quema, desprende gases tóxicos. Es más dado a daño por corrosión en exteriores; para ello se emplean las cubiertas de polietileno.

El plenum contiene materiales especiales en su aislamiento y en una clavija del cable. Estos materiales son resistentes al fuego y producen una mínima cantidad de humos tóxicos.

Se puede encontrar un cable coaxial:

Antes de la utilización masiva de la fibra óptica en las redes de telecomunicaciones, tanto terrestres como submarinas, el cable coaxial era ampliamente utilizado en sistemas de transmisión de telefonía analógica basados en la multiplexación por división de frecuencia (FDM), donde se alcanzaban capacidades de transmisión de más de 10 000 circuitos de voz. Asimismo, en sistemas de transmisión digital, basados en la multiplexación por división de tiempo (TDM), se conseguía la transmisión de más de 7000 canales de 128 kbps.

El cable utilizado para estos fines de transmisión a larga distancia necesitaba tener una estructura diferente al utilizado en aplicaciones de redes locales, ya que, debido a que se instalaba enterrado, tenía que estar protegido contra esfuerzos de tracción y presión, por lo que normalmente aparte de los aislantes correspondientes llevaba un armado exterior de acero.

Par trenzado cargado – Es un par trenzado al que se le adiciona de forma intencional inductancia, la cual es bastante común en las líneas de telecomunicaciones, aunque en ciertas frecuencias existen excepciones. Estos inductores añadidos se conocen como bobinas de Pupin y logran reducir la distorsión.

Par trenzado unido – Es una variante del cable par trenzado, en donde los pares se van uniendo de manera individual, consiguiendo que el cable sea más robusto. Las especificaciones eléctricas para el cable siguen siendo las mismas, pese al manejo de rudo.

Cable trenzado de cinta – Es una variante para el estándar de cable de cinta, en el cual se utilizan conductores adyacentes, los cuales están en modo esclavo y trenzados. Respecto a los pares trenzados se anota que son esclavos ligeramente uno de los otros con un formato de cinta. De forma periódica y según se avance por la cinta, van a darse secciones pequeñas en las que no hay trenzado para garantizar que los conectores y cabeceras con circuito impreso se terminen con técnicas usuales para un cable de cinta IDC.



</doc>
<doc id="4831" url="https://es.wikipedia.org/wiki?curid=4831" title="Banda ancha">
Banda ancha

En telecomunicaciones, se conoce como banda ancha a cualquier tipo de red con elevada capacidad para transportar información que incide en la velocidad de transmisión de esta. Así entonces, es la transmisión de datos simétricos por la cual se envían simultáneamente varias piezas de información, con el objeto de incrementar la velocidad de transmisión efectiva. En ingeniería de red de computadoras este término se utiliza también para los métodos en donde dos o más señales comparten un medio de transmisión. Así se utilizan dos o más canales de datos simultáneos en una única conexión, lo que se denomina multiplexación (ver sección más abajo).

Algunas de las variantes de los servicios de fibra hasta la casa ("Fiber To The Home") son de banda ancha. Los "routers" que operan con velocidades mayores a 100 Mbps también son banda ancha, pues obtienen velocidades de transmisión simétricas.

El concepto de banda ancha ha evolucionado con los años. La velocidad que proporcionaba la RDSI con 128 Kbps dio paso al SDSL con una velocidad de 256 Kbps. Posteriormente evolucionó, pasando los 25 y 50 Mbps simétricos hasta los 600 Mbps en la actualidad.

La banda ancha no es un concepto estático, toda vez que las velocidades de acceso a Internet se aumentan constantemente. Las velocidades se miden en bits por segundo, por ejemplo, kilobits por segundo (Kbps) o megabits por segundo (Mbps). La velocidad mínima para considerarse banda ancha varía entre los países, e incluso dentro de un país la autoridad puede considerar como banda ancha un valor de velocidad distinto de aquel que el operador estima como banda ancha. Se ha propuesto que una manera para determinar la existencia de banda ancha es aquella basada en los servicios a los que se puede tener acceso (p. ej., rápida descarga de archivos de Internet, calidad de audio equivalente a un CD, servicios de voz interactivos). La amplia disponibilidad de banda ancha se considera un factor para la innovación, la productividad, el crecimiento económico y la inversión extranjera. 

Al concepto de banda ancha hay que atribuirle otras características, además de la velocidad, como son la interactividad, digitalización y conexión o capacidad de acceso (función primordial de la banda ancha). 

Patterson ya hablaba de que la conexión de banda ancha depende de la red de comunicaciones, de las prestaciones del servicio. En su libro "Latency lags bandwidth. Communications of the ACM" escrito en 2004 cuenta que el retardo es un aspecto crítico para las prestaciones de un sistema real.

Las comunicaciones pueden utilizar distintos canales físicos simultáneamente; es decir multiplexar para tener acceso múltiple. Tales canales pueden distinguirse uno de otro por estar separados en tiempo (multiplexación por división de tiempo o TDM), frecuencia de portadora (multiplexación por división de frecuencia, FDM o multiplexación por división de longitud de onda, WDM), o por código (multiplexación por división de código, CDMA). Cada canal que toma parte en la multiplexación es por definición de banda estrecha (pues no está utilizando todo el ancho de banda del medio).

Implica el derecho a acceder a un Internet de alta velocidad. Es decir, el derecho a acceder a un servicio eficiente, de alta velocidad y gran capacidad de transmisión de información. La banda ancha resulta importante, porque con ella se puede acceder a otros servicios que ofrece el Internet. 

La Constitución Política señala, en su artículo sexto, el derecho de acceso a las tecnologías de la información, incluyendo la banda ancha e Internet.

Su ley reglamentaria, denominada "Ley Federal de Telecomunicaciones y Radiodifusión", establece en su artículo 2 que: “En la prestación de dichos servicios estará prohibida toda discriminación motivada por origen étnico o nacional, el género, la edad, las discapacidades, la condición social, las condiciones de salud, la religión, las opiniones, las preferencias sexuales, el estado civil o cualquier otra que atente contra la dignidad humana y tenga por objeto anular o menoscabar los derechos y libertades de las personas”.

Asimismo, define la banda ancha como “el acceso de alta capacidad que permite ofrecer diversos servicios convergentes a través de infraestructura de red fiable, con independencia de las tecnologías empleadas, cuyos parámetros serán actualizados por el Instituto Federal de Telecomunicaciones periódicamente”.

Dicha ley establece la obligatoriedad del Estado, de implementar políticas públicas de brindar acceso a las tecnologías de la información y comunicación, incluyendo el Internet de banda ancha para toda la población, haciendo especial énfasis en los sectores más vulnerables, con el propósito de cerrar la brecha digital entre individuos, hogares, empresas y áreas geográficas de distinto nivel socioeconómico.

De igual forma, establece que la Secretaría de Comunicaciones y Transportes implementará programas de acceso a la banda ancha en sitios públicos que identifiquen el número de sitios a conectar cada año de manera progresiva, hasta alcanzar la cobertura universal.

La Estrategia Digital Nacional programa las políticas públicas que permitan el derecho de conectividad de acceso a Internet de banda ancha a través del Programa México Conectado. Para ello, se prevé el acceso a Internet de banda ancha en sitios públicos identificando el número de edificios a conectar cada año, hasta alcanzar la cobertura universal. Se buscará que en los sitios públicos la conectividad de banda ancha cuente con capacidad suficiente para satisfacer la demanda y, por tanto, deberá considerarse el número potencial de usuarios en cada sitio.



</doc>
<doc id="4832" url="https://es.wikipedia.org/wiki?curid=4832" title="Banda base">
Banda base

En Telecomunicaciones, el término banda base se refiere a la banda de frecuencias producida por un transductor, tal como un micrófono, un manipulador telegráfico u otro dispositivo generador de señales que no es necesario adaptarlo al medio por el que se va a transmitir.

Banda base es la señal de una sola transmisión en un canal, banda ancha significa que lleva más de una señal y cada una de ellas se transmite en diferentes canales, hasta su número máximo de canal.

En los sistemas de transmisión, la banda base es generalmente utilizada para modular una portadora. Durante el proceso de demodulación se reconstruye la señal banda base original. Por ello, podemos decir que la banda base describe el estado de la señal antes de la modulación y de la multiplexación y después de la demultiplexación y demodulación. 

Las frecuencias de banda base se caracterizan por ser generalmente mucho más bajas que las resultantes cuando éstas se utilizan para modular una portadora o subportadora. Por ejemplo, es señal de banda base la obtenida de la salida de video compuesto de dispositivos como grabadores/reproductores de video y consolas de juego, a diferencia de las señales de televisión que deben ser moduladas para poder transportarlas vía aérea (por señal libre o satélite) o por cable.

En transmisión de facsímil, la banda base es la frecuencia de una señal igual en ancho de banda a la comprendida entre la frecuencia cero y la frecuencia máxima de codificación. En otras palabras, si el espectro de frecuencia de una señal se localiza alrededor de la frecuencia f = 0 Hz, se dice que la señal es de “banda base”.



</doc>
<doc id="4834" url="https://es.wikipedia.org/wiki?curid=4834" title="Concentrador">
Concentrador

Concentrador (hub) es el dispositivo que permite centralizar el cableado de una red de computadoras, para luego poder ampliarla. 

Trabaja en la capa física (capa 1) del modelo OSI o la capa de acceso al medio en el modelo TCP/IP. Esto significa que dicho dispositivo recibe una señal y repite esta señal emitiéndola por sus diferentes puertos (repetidor).

En la actualidad, la tarea de los concentradores la realizan, con frecuencia, los conmutadores ("switches").

Una red Ethernet se comporta como un medio compartido, es decir, sólo un dispositivo puede transmitir con éxito a la vez, y cada uno es responsable de la detección de colisiones y de la retransmisión. Con enlaces 10Base-T y 100Base-T (que generalmente representan la mayoría o la totalidad de los puertos en un concentrador) hay parejas separadas para transmitir y recibir, pero que se utilizan en modo "half duplex" el cual se comporta todavía como un medio de enlaces compartidos (véase 10Base-T para las especificaciones de los pines). 

Un concentrador, o repetidor, es un dispositivo de emisión bastante sencillo. Los concentradores no logran dirigir el tráfico que llega a través de ellos, y cualquier paquete de entrada es transmitido a otro puerto (que no sea el puerto de entrada). Dado que cada paquete está siendo enviado a través de cualquier otro puerto, aparecen las colisiones de paquetes como resultado, que impiden en gran medida la fluidez del tráfico. Cuando dos dispositivos intentan comunicar simultáneamente, ocurrirá una colisión entre los paquetes transmitidos, que los dispositivos transmisores detectan. Al detectar esta colisión, los dispositivos dejan de transmitir y hacen una pausa antes de volver a enviar los paquetes.

La necesidad de "hosts" para poder detectar las colisiones limita el número de centros y el tamaño total de la red. Para 10 Mbit/s en redes, de hasta 5 segmentos (4 concentradores) se permite entre dos estaciones finales. Para 100 Mbit/s en redes, el límite se reduce a 3 segmentos (2 concentradores) entre dos estaciones finales, e incluso sólo en el caso de que los concentradores fueran de la variedad de baja demora. Algunos concentradores tienen puertos especiales (y, en general, específicos del fabricante) les permiten ser combinados de un modo que consiente encadenar a través de los cables Ethernet los concentradores más sencillos, pero aun así una gran red Fast Ethernet es probable que requiera conmutadores para evitar el encadenamiento de concentradores.

La mayoría de los concentradores detectan problemas típicos, como el exceso de colisiones en cada puerto. Así, un concentrador basado en Ethernet, generalmente es más robusto que el cable coaxial basado en Ethernet. Incluso si la partición no se realiza de forma automática, un concentrador de solución de problemas la hace más fácil ya que las luces pueden indicar el posible problema de la fuente. Asimismo, elimina la necesidad de solucionar problemas de un cable muy grande con múltiples tomas.

Los concentradores sufrieron el problema de que como simples repetidores sólo podían soportar una única velocidad. Mientras que las computadoras personales normales con ranuras de expansión podrían ser fácilmente actualizados a "Fast Ethernet" con una nueva tarjeta de red, máquinas con menos mecanismos de expansión comunes, como impresoras, pueden ser costosas o imposibles de actualizar. Por lo tanto, un punto medio entre concentrador y conmutador es conocido como concentrador de doble velocidad. 

Este tipo de dispositivos consiste fundamentalmente en dos concentradores (uno de cada velocidad) y dos puertos puente entre ellos. Los dispositivos se conectan al concentrador apropiado automáticamente, en función de su velocidad. Desde el puente sólo se tienen dos puertos, y sólo uno de ellos necesita ser de 100 Mb/s.

Históricamente, la razón principal para la compra de concentradores en lugar de los conmutadores era el precio. Esto ha sido eliminado en gran parte por las reducciones en el precio de los conmutadores, pero los concentradores aún pueden ser de utilidad en circunstancias especiales:









</doc>
<doc id="4835" url="https://es.wikipedia.org/wiki?curid=4835" title="Capa física">
Capa física

En las siete capas del modelo OSI de la red informática, el nivel físico o capa física (Capa 1) se refiere a las transformaciones que se le hacen a la secuencia de bits para trasmitirlos de un lugar a otro. Esta capa puede ser implementado por un PHY. Siempre los bits se manejan dentro del PC como niveles eléctricos. Por ejemplo, puede decirse que en un punto del cable existe un 1 cuando hay presente un determinado nivel de voltaje y un cero cuando su nivel es de 0 voltios. Cuando se transmiten los bits siempre se transforman en otro tipo de señales de tal manera que en el punto receptor puede recuperar la secuencia de bits originales.

La Capa Física o Nivel 1 proporciona los medios mecánicos, eléctricos, funcionales y de procedimiento para activar, mantener y desactivar conexiones físicas.

Variando algunas propiedades físicas, voltaje o corriente, se puede lograr el envío de datos mediante un cable. El comportamiento de la señal se puede representar matemáticamente como se describirá en las siguientes subsecciones.

Una "serie de Fourier" es una serie finita que converge uniformemente a una función continua y periódica. Las series de Fourier constituyen la herramienta matemática básica del análisis de Fourier empleado para analizar funciones periódicas a través de la descomposición de dicha función en una suma infinitesimal de funciones senoidales mucho más simples (como combinación de senos y cosenos con frecuencias enteras).
El nombre se debe al matemático francés Jean-Baptiste Joseph Fourier que desarrolló la teoría cuando estudiaba la ecuación del calor. Fue el primero que estudió tales series sistemáticamente, y publicando sus resultados iniciales en 1807 y 1811. Esta área de investigación se llama algunas veces Análisis armónico.

Es una aplicación usada en muchas ramas de la ingeniería, además de ser una herramienta sumamente útil en la teoría matemática abstracta. Áreas de aplicación incluyen análisis vibratorio, acústica, óptica, procesamiento de imágenes y señales, y compresión de datos. 
En ingeniería, para el caso de los sistemas de telecomunicaciones, y a través del uso de los componentes espectrales de frecuencia de una señal dada, se puede optimizar el diseño de un sistema para la señal portadora del mismo.
Refierase al uso de un analizador de espectros.

Las series de Fourier tienen la forma:

Donde formula_1 y formula_2 se denominan coeficientes de Fourier de la serie de Fourier de la función formula_3

Si formula_4 es una función (o señal) periódica y su período es formula_5, la serie de Fourier asociada a formula_4 es:

Donde formula_7 y formula_8 son los coeficientes de Fourier que toman los valores:

Por la identidad de Euler, las fórmulas de arriba pueden expresarse también en su forma compleja:

Los coeficientes ahora serían:
La relación de lo presentado en la subsección anterior se puede ejemplificar mediante la transmisión del carácter ASCII "b", se va a transmitir la cadena binaria 01100010. El análisis de Fourier produce lo siguiente:

formula_10

formula_11

formula_12

Al transmitir datos se pierde cierta potencia durante el proceso, ningún emisor lo puede evitar. Si todos los parámetros de Fourier disminuyeran en forma proporcional, la señal producida se reduciría en amplitud pero no se distorsionaría. La distorsión se provoca porque todas las plantas de transmisión disminuyen los componentes de la serie de Fourier en diferentes valores. Las amplitudes se emiten, en la mayoría de los casos sin ninguna atenuación desde 0 hasta formula_13 (Usando el ciclo/seg o Hertz como unidad de medida) y todos los valores que superen este límite serán atenuados. El rango de frecuencias que se emite sin necesidad de atenuarse se lo conoce como ancho de banda. Este corte no se produce en forma abrupta en la práctica, el ancho de banda varía desde 0 hasta la frecuencia en la que el valor de la amplitud es disminuido a la mitad de su valor original.

En 1924, Harry Nyquist, trabajando para la empresa AT&T, llegó a la conclusión de que un canal incluso perfecto tiene una capacidad de transmisión limitada. Logró una ecuación que calcula la tasa máxima de un canal libre de ruido de ancho de banda finito. Shannon extendió en 1948 esta fórmula a un canal termodinámico, que tiene ruido aleatorio.

Nyquist demostró que si se emite una señal a través de un filtro que permita el paso de señales bajas de ancho de banda H, la señal puede ser recompuesta tomando 2H (exactas) muestras por segundo. Las señales que se pueden muestrear con una rapidez mayor a 2H veces por segundo ya han sido filtradas por lo que es inútil hacerlo.

Si la señal se compone de V valores discretos, el teorema de Nyquist establece:

tasa de datos máxima = 2Hformula_14

Un canal de 3kHz no puede transmitir señales binarias a una tasa mayor de 6000 bps, por ejemplo.

Para un canal con ruido la situación se complica notoriamente, el ruido aleatorio causado por la temperatura siempre está presente a causa del movimiento de las moléculas del sistema.
La relación señal a ruido es la cantidad de ruido térmico presente que se mide por la relación existente entre la potencia de la señal y la potencia del ruido. Si S es la potencia de la señal y N la potencia del ruido, la relación entre los valores es S/N y por lo general se usa la relación formula_15. Esta unidad se conoce como dB.
La fórmula principal de Shannon es:

número máximo de bits/seg=formula_16

Shannon dedujo su resultado aplicando argumentos de la Teoría de la Información y es válido para cualquier canal con ruido térmico.

El medio de transmisión constituye el canal que permite la transmisión de información entre dos terminales en un sistema de comunicación.

Las transmisiones se realizan habitualmente empleando medios físicos y ondas electromagnéticas, las cuales se vuelven susceptibles al ser transmitidas por el vacío.

La capa física le proporciona servicios a la capa de enlaces de datos con el objetivo que esta le proporcione servicios a la capa de red. La capa física recibe un flujo de bits e intenta enviarlo al destino, no siendo su responsabilidad entregarlos libre de errores. La capa de enlace de datos es la encargada de detectar y corregir los errores. Los errores pueden consistir en una mayor o menor cantidad de bits recibidos o diferencias en los valores que se emitieron y en los que se recibieron.

Un método común de detección de errores es que la capa de enlace de datos separe el flujo en tramas separadas y que realice la suma de verificación de cada trama. Cuando una trama llega a su destino se recalcula la suma de verificación. Si es distinta de la contenida en la trama es porque ha ocurrido un error y la capa de enlace debe solucionarlo.

Las principales funciones y servicios realizados por la capa física son:

La capa física se ocupa también de:

En una red de área local (LAN) o en una red de área metropolitana (MAN) que usa la arquitectura OSI, la "subcapa de señalización física" es la parte de la capa física que:

Fuente: Estándar Federal 1037C



"Nota: Capa física"
Asociado con la transmisión de cadenas de bits sin estructura sobre un enlace físico. Responsable de las características mecánicas, eléctricas y procedurales que establecen, mantienen y desactivan el enlace físico.



</doc>
<doc id="4836" url="https://es.wikipedia.org/wiki?curid=4836" title="Capa de enlace de datos">
Capa de enlace de datos

El nivel de enlace de datos (en inglés: "data link level") o capa de enlace de datos, es la segunda capa del modelo OSI, es responsable de la transferencia fiable de información a través de un circuito de transmisión de datos. Recibe peticiones de la capa de red y utiliza los servicios de la capa física. 

El objetivo de la capa de enlace es conseguir que la información fluya, libre de errores, entre dos máquinas que estén conectadas directamente (servicio orientado a la conexión). Para lograr este objetivo tiene que montar bloques de información (llamados tramas en esta capa), dotarles de una dirección de capa de enlace (Dirección MAC), gestionar la detección o corrección de errores, y ocuparse del “control de flujo” entre equipos (para evitar que un equipo más rápido desborde a uno más lento).

Cuando el medio de comunicación está compartido entre más de dos equipos es necesario arbitrar el uso del mismo. Esta tarea se realiza en la subcapa de control de acceso al medio. 

Dentro del grupo de normas IEEE 802, la subcapa de enlace lógico se recoge en la norma IEEE 802.2 y es común para todos tipos de redes (Ethernet o IEEE 802.3, IEEE 802.11 o Wi-Fi, IEEE 802.16 o WiMAX, etc.); todas ellas especifican una subcapa de acceso al medio así como una capa física distinta.

Otro tipo de protocolos de la capa de enlace son: protocolo punto a punto ("Point-to-Point Protocol", PPP); protocolo de enlace de alto nivel ("High-level Data Link Control", HDLC), entre otros.

En la práctica la subcapa de acceso al medio suele formar parte de la propia tarjeta de comunicaciones, mientras que la subcapa de enlace lógico estaría en el programa adaptador de la tarjeta ("driver").

La capa de enlace es la facilidad de área extensa por la que se pueden comunicar los sistemas mediante un protocolo de la capa de enlace de datos.

La capa de enlace de datos es responsable de la transferencia fiable de información a través de un circuito eléctrico de transmisión de datos. La transmisión de datos lo realiza mediante tramas que son las unidades de información con sentido lógico para el intercambio de datos en la capa de enlace. También hay que tener en cuenta que en el modelo TCP/IP se corresponde a la primera capa.

Sus principales funciones son:

La función de iniciación comprende los procesos necesarios para activar el enlace e implica el intercambio de tramas de control con el fin de establecer la disponibilidad de las estaciones para transmitir y recibir información.

Las funciones de terminación son de liberar los recursos ocupados hasta la recepción/envío de la última trama. También de usar tramas de control. La identificación es para saber a qué terminal se debe de enviar una trama o para conocer quién envía la trama. Se lleva a cabo mediante la dirección de la capa de enlace.

La segmentación surge por la longitud de las tramas ya que si es muy extensa, se debe de realizar tramas más pequeñas con la información de esa trama excesivamente larga.

Si estas tramas son excesivamente cortas, se ha de implementar unas técnicas de bloque que mejoran la eficiencia y que consiste en concatenar varios mensajes cortos de nivel superior en una única trama de la capa de enlace más larga.

En las transferencias de información en la capa de enlace es necesario identificar los bits y saber qué posición les corresponde en cada carácter u octeto dentro de una serie de bits recibidos.

Esta función de sincronización comprende los procesos necesarios para adquirir, mantener y recuperar la sincronización de carácter u octeto. Es decir, poner en fase los mecanismos de codificación del emisor con los mecanismos de decodificación del receptor.

La capa de enlace debe ocuparse de la delimitación y sincronización de la trama. Para la delimitación se puede usar tres métodos:


La “transparencia” se realiza mediante la ‘inserción de bits’. Consta de ir contando los unos consecutivos y cuando se encuentra con cinco caracteres "1" seguidos y consecutivos (codice_1) introduce el bit "0" después del quinto 1. Ejemplo: considere la trama 0101111110, al aplicar la transparencia pasa a ser 01011111010.

Proporciona detección y corrección de errores en el envío de tramas entre computadoras, y provee el control de la capa física. Sus funciones, en general, son:

Correctores de error : es opcional en esta capa, la encargada de realizar esta función es la capa de transporte, en una WAN es muy probable que la verificación, la realiza la capa de enlace.

Para la identificación de tramas puede usar distintas técnicas como:

El control de flujo es necesario para no 'agobiar' al receptor. Se realiza normalmente en la capa de transporte, también a veces en la capa de enlace. Utiliza mecanismos de retroalimentación. Suele ir unido a la corrección de errores y no debe limitar la eficiencia del canal.

Los métodos de control de errores son básicamente dos:

Las posibles implementaciones son:

La detección de errores la realiza mediante diversos tipos de códigos del que hay que resaltar:

La corrección de errores están basados en Código Hamming, por repetición, verificación de paridad cruzada, Reed-Solomon y de Goyle.

El "control de flujo" es necesario para no saturar al receptor de uno a más emisores. Se realiza normalmente en la capa de transporte, también a veces en la capa de enlace. Utiliza mecanismos de retroalimentación. Suele ir unido a la corrección de errores y no debe limitar la eficiencia del canal. El control de flujo conlleva dos acciones importantísimas que son la detección de errores y la corrección de errores.

La detección de errores se utiliza para detectar errores a la hora de enviar tramas al receptor e intentar solucionarlos. Se realiza mediante diversos tipos de códigos del que hay que resaltar el CRC, simple paridad (puede ser par, números de "1" par, o impar), paridad cruzada (Paridad horizontal y vertical) y Suma de verificación.

La corrección de errores surge a partir de la detección para corregir errores detectados y necesitan añadir a la información útil un número de bits redundantes bastante superior al necesario para detectar y retransmitir.
Sus técnicas son variadas. El Código Hamming, Repetición, que cada bit se repite tres veces y en caso de fallo se toma el bit que más se repite;
También puede hacerse mediante "verificación de paridad cruzada", Reed-Solomon y "de goyle".

También cabe destacar los protocolos HDLC que es un control de enlace de datos a alto nivel, orientado a bit y obedece a una ARQ de ventana deslizante o continuo. También existen protocolos orientados a carácter.

Se refiere a los procedimientos para detectar situaciones y recuperar al nivel de situaciones anómalas como la ausencia de respuesta, recepción de tramas inválidas, etc. Las situaciones más típicas son la pérdida de tramas, aparición de tramas duplicadas y llegada de tramas fuera de secuencia.

Si no se tratasen correctamente estos eventos se perderá información y se aceptarán datos erróneos como si fuesen correctos. Generalmente se suelen utilizar "contadores" para limitar el número de errores o reintentos de los procesos y procedimientos. También se pueden usar "temporizadores" para establecer plazos de espera ("timeout") de los sucesos.

La gestión atiende a dos tipos:

La coordinación se puede realizar mediante selección o contienda:



</doc>
<doc id="4839" url="https://es.wikipedia.org/wiki?curid=4839" title="Capa de sesión">
Capa de sesión

El nivel de sesión o capa de sesión es el quinto nivel del modelo OSI, que proporciona los mecanismos para controlar el diálogo entre las aplicaciones de los sistemas finales. En muchos casos, los servicios de la capa de sesión son parcialmente, o incluso, totalmente prescindibles. No obstante, en algunas aplicaciones su utilización es ineludible.

La capa de sesión proporciona los siguientes servicios:

Todas estas capacidades se podrían incorporar en las aplicaciones de la capa 7. Sin embargo ya que todas estas herramientas para el control del diálogo son ampliamente aplicables, parece lógico organizarlas en una capa separada, denominada capa de sesión.

La capa de sesión surge como una forma de organizar y sincronizar el diálogo y controlar el intercambio de datos.

La capa de sesión permite a los usuarios de máquinas diferentes establecer sesiones entre ellos. Una sesión permite el transporte ordinario de datos, como lo hace la capa de transporte, pero también proporciona servicios mejorados que son útiles en algunas aplicaciones. Se podría usar una sesión para que el usuario se conecte a un sistema remoto de tiempo compartido o para transferir un archivo entre dos máquinas.




El modelo TCP/IP no se ocupa de los detalles del modelo OSI de la semántica del protocolo de aplicación o transporte y, por lo tanto, no considera una capa de sesión. La gestión de la sesión de OSI en relación con los protocolos de transporte típicos (TCP, SCTP) está contenida en los protocolos de la capa de transporte, o de lo contrario se considera el ámbito de los protocolos de la capa de aplicación. Las capas de TCP/IP son "descripciones" de ámbitos operativos (aplicación, host-a-host, red, enlace) y no "prescripciones" detalladas de procedimientos operativos o semántica de datos.



</doc>
<doc id="4841" url="https://es.wikipedia.org/wiki?curid=4841" title="Capa de aplicación">
Capa de aplicación

El nivel de aplicación o capa de aplicación es el séptimo nivel del modelo OSI y el cuarto de la pila TCP.

Ofrece a las aplicaciones (de usuario o no) la posibilidad de acceder a los servicios de las demás capas y define los protocolos que utilizan las aplicaciones para intercambiar datos, como correo electrónico (POP y SMTP), gestores de bases de datos y protocolos de transferencia de archivos (FTP).

Cabe aclarar que el usuario normalmente no interactúa directamente con el nivel de aplicación. Suele interactuar con programas que a su vez interactúan con el nivel de aplicación pero ocultando la complejidad subyacente. Así por ejemplo un usuario no manda una petición «GET /index.html HTTP/1.0» para conseguir una página en html, ni lee directamente el código html/xml. O cuando chateamos con el Mensajero Instantáneo, no es necesario que codifiquemos la información y los datos del destinatario para entregarla a la capa de Presentación (capa 6) para que realice el envío del paquete.

En esta capa aparecen diferentes protocolos y servicios:

Protocolos:

Servicios:

Esta capa contiene las aplicaciones visibles para el usuario.
Algunas consideraciones son: seguridad y cifrado, DNS (Domain Name Service)
Una de las aplicaciones más usadas hoy en día en Internet es el WWW (World Wide Web).


</doc>
<doc id="4842" url="https://es.wikipedia.org/wiki?curid=4842" title="Gopher">
Gopher

Gopher es un servicio de Internet consistente en el acceso a la información a través de menús. La información se organiza en forma de árbol: sólo los "nodos" contienen menús de acceso a otros menús o a "hojas", mientras que las hojas contienen simplemente información textual. En cierto modo es considerado un predecesor de la Web, aunque sólo se permiten enlaces desde nodos-menús hasta otros nodos-menús o a hojas, y las hojas no tienen ningún tipo de hiperenlaces.

Gopher es uno de los sistemas de Internet para divulgar información que precedió a la world wide web. Fue creado en 1991 en la Universidad de Minnesota y fue el primer sistema que permitió pasar de un sitio a otro seleccionando una opción en el menú de una página. Esa es la razón por la que adquirió mayor popularidad que sus competidores, que acabaron siendo sustituidos por la Web.

Los servidores Gopher, igual que los servidores FTP almacenan archivos y documentos que puede verse en línea o transferirse al PC.

Del mismo modo que todos los sitios Web del mundo forman la World Wide Web, el espacio Gopher (Gopherspace) engloba los 5000 o más servidores de Gopher existentes. En gran medida acceder a un servidor de Gopher es parecido a utilizar un sitio FTP, ya que la información se presenta en menús que contienen archivos y carpetas. Se navega por estas últimas para buscar archivos, documentos u otras carpetas que dispongan de más niveles de información. Normalmente los archivos se visualizan o bajan haciendo clic en ellos. 

Para buscar información en sitios Gopher se puede recurrir a Verónica (retroacrónimo del inglés Very Easy Rodent-Oriented Netwide Index to Computerized Archives), un motor de búsqueda desarrollado en 1992 por Steven Foster y Fred Barrie de la Universidad de Nevada.

Aunque los servidores Gopher que quedan son testimoniales, el navegador Firefox admitía el protocolo hasta la versión 3. Internet Explorer lo eliminó en 2002, después de descubrirse una vulnerabilidad. Hoy en día, Firefox es compatible con Gopher mediante la extensión OverbiteFF. El navegador Lynx admite Gopher de forma nativa.

Para poder consultar información almacenada en sitios Gopher empleando los navegadores actuales, se puede recurrir a sitios proxy específicos.



</doc>
<doc id="4843" url="https://es.wikipedia.org/wiki?curid=4843" title="Archie (Buscador FTP)">
Archie (Buscador FTP)

Archie es un sistema para la localización de información sobre archivos y directorios, muy unido al servicio FTP. Es como una gran base de datos donde se encuentra registrada una gran cantidad de nombres de archivos y los servidores FTP. Al igual que gopher, ha sido reemplazado por la WWW.

Archie fue el primer motor de búsqueda que se ha inventado, diseñado para indexar archivos FTP, permitiendo a la gente encontrar archivos específicos. La implementación original se escribió en 1990 por Alan Emtage, Bill Heelan, y Peter J. Deutsch, entonces estudiantes en la Universidad McGill de Montreal.

Las primeras versiones de archie simplemente contactaban una lista de archivos FTP en bases regulares (contactando cada una apenas una vez cada mes, para no gastar muchos recursos en los servidores remotos) y requiriendo un listado. Estos listados eran almacenados en ficheros locales para ser buscados usando el comando grep de UNIX. Más tarde, se desarrollaron front- y back-ends más eficaces, y este sistema pasó de ser una herramienta local a un recurso para toda la red, a un servicio popular accesible desde múltiples sitios de Internet. A tales servidores se podía acceder de muchas formas: usando un cliente local (como archie o xarchie); haciendo telnet al servidor directamente, enviando queries por correo electrónico y más tarde con interfaces World Wide Web. El nombre archie viene de la palabra inglesa "archive", pero también está asociado con la serie de cómics americano “Archie”. Esta no era la intención original, pero fue lo que actuó como inspiración para los nombres de Jughead (oficialmente acrónimo de Jonzy's Universal Gopher Hierarchy Excavation And Display) y Verónica (acrónimo de "Very Easy Rodent-Oriented Net-wide Index to Computer Archives"), ambos sistemas de búsqueda para del protocolo Gopher. Con la aparición del World Wide Web la búsqueda de archivos se simplificó mucho, y actualmente hay muy pocos servidores activos. Se puede encontrar un gateway en Polonia y Japón.



</doc>
<doc id="4852" url="https://es.wikipedia.org/wiki?curid=4852" title="6 de febrero">
6 de febrero

El 6 de febrero es el 37.º (trigésimo séptimo) día del año en el calendario gregoriano. Quedan 328 días para finalizar el año y 329 en los años bisiestos.












</doc>
<doc id="4856" url="https://es.wikipedia.org/wiki?curid=4856" title="Biología marina">
Biología marina

La biología marina es la ciencia que estudia los seres vivos que habitan los ecosistemas marinos, así como la conservación de la vida marina, sus elementos biológicos, su flora y fauna. También investiga los elementos de orden físico y químico del medio acuático, siendo su objetivo principal el mantenimiento integral de todas las especies marinas y mejora de sus recursos. 
Más que una ciencia en sí misma, se trata de un sistema de aplicación de otras ciencias tales como la geología, la geografía, la química, la física y la biología, indispensables para el estudio global y correlacionado de los fenómenos que caracterizan el ambiente marino. La oceanografía se subdivide en tres ramas principales: oceanografía física, oceanografía química y oceanografía biológica. Los océanos cubren el 71 % de la corteza terrestre. Incluye desde el plancton microscópico, hasta cetáceos como las ballenas. Se estima que solo se ha investigado, hasta ahora, un 5 % de la vida en los océanos.

Generalmente se agrupan en su función, tamaño, y hábito de vida.

La microbiología marina es de gran importancia debido que realizan la descomposición de la materia orgánica y la producción primaria en un ecosistema. Los organismos fitoplanctonicos (vegetales) llamados diatomeas son los responsables de la mayor producción, por medio de la fotosíntesis, de oxígeno al año en todo el planeta; siendo mayor que la producción de todos los bosques, junglas, y selvas del planeta. Al año los océanos producen millones de toneladas de oxígeno.

La mayor parte de los microbios marinos son bacterias y algas azules. Estas bacterias están dispersas por todos los océanos soportando condiciones extremas.





</doc>
<doc id="4859" url="https://es.wikipedia.org/wiki?curid=4859" title="Orígenes del jazz">
Orígenes del jazz

El jazz es un género musical nacido en la segunda mitad del siglo XIX, Estados Unidos, que se expandió de forma global a lo largo de todo el siglo XX. 
En palabras del etnomusicólogo y folclorista Alan Lomax, el "jazz" es un gumbo musical, un resultado del crisol de razas, del "melting pot" que era el sur del país.

Geográficamente, el "jazz" surge en el estado de Luisiana, concretamente en la zona de influencia de Nueva Orleans (cuna del estilo musical y principal centro jazzístico durante la primera época del "jazz"), a donde llegaban grandes remesas de esclavos de color, fundamentalmente de la zona occidental de África, al sur del Sáhara, la zona denominada Costa de Marfil, "Costa del Oro" o "Costa de los esclavos". 

La palabra "jazz", en un sentido relacionado con la música, no fue usada en las primeras etapas de formación del "jazz". De hecho, aparece escrita por primera vez el 6 de marzo de 1913, en el periódico "San Francisco Bulletin", cuando, al reseñar el tipo de música ejecutada por una orquesta del ejército, señaló que sus integrantes entrenaban a ritmo de "ragtime" y "jazz". Según Walter Kingsley, colaborador del "New York Sun", "el término es de origen africano, común en la Costa del Oro africana y en las tierras del interior". Mucho más tarde aún, en enero de 1917 en Nueva York, apareció la palabra "jazz" como definidora de la música contenida en un disco, grabado por la Original Dixieland Band; durante ese año, además, se popularizaría el término, que probablemente había sido ya de uso común en lenguaje oral entre 1913 y 1917.


En muchas áreas del Sur de Estados Unidos, el batir de tambores estaba específicamente prohibido por la ley, de forma que los esclavos negros tuvieron que recurrir a la percusión mediante las palmas de las manos y el batir de los pies para disfrutar de sus fiestas y su música. Sin embargo, la prohibición no tuvo vigor en la llamada "Place Congo" "(Congo Square)" de Nueva Orleans, en la que los esclavos tenían libertad para reunirse, cantar y acompañarse de verdaderos instrumentos de percusión tales como calabazas resecas y rellenas de piedrecitas, el birimbao, las quijadas, el piano de dedo pulgar o sanza, y el banjo de cuatro cuerdas. Entre 1825 y 1845, la danza callejera se prohibió totalmente, aunque a partir de este último año se volvió a autorizar, entre las 16 y las 18,30 horas. Las reuniones del domingo en Congo Square se mantuvieron hasta mediados los años 1880, con cierto carácter ritual.

La música que se desarrollaba en estas sesiones, y en otras de carácter más reservado, era muy variada. De forma prominente, estaban los cantos y danzas del vudú antillano, un rito religioso de carácter sincrético, de origen dahomeico. También otros ritmos y danzas desarrolladas previamente en las islas caribeñas, como la calinda, la bamboula o el baile denominado congo.

Una de las manifestaciones más importantes de la música de los esclavos, de la que ya se tiene constancia en 1770, son los espirituales negros, cuya evolución duró casi un siglo, "desde los comienzos de la Guerra de la Independencia a los albores de la guerra civil". Es importante observar el hecho de que, a pesar de las divergencias en ritmo, armonía y estilo interpretativo, la tradición musical europea que los esclavos conocieron en Estados Unidos ofrecía puntos de contacto con su propia tradición: así, la escala diatónica era común a ambas culturas. Si a esto se le añade el relativo aislamiento cultural en que vivía gran número de esclavos y la tolerancia de los amos respecto de su música, la consecuencia fue que pudiesen mantener íntegro gran parte de su legado musical en el momento de fusionarse con los elementos compatibles de la música europea y estadounidense, con lo que se consiguió un híbrido con notable influencia africana.

De carácter mucho más africano eran las manifestaciones musicales relacionadas con el trabajo y la vida cotidiana. En especial las canciones de trabajo, los "hollers", los "shouts" y "ring shouts" y, finalmente, los "street cries" (gritos callejeros), que incluían tanto pregones de mercaderes, como cantos de celebración, noticias, etc. y que solían cantarse en patois o gombo (dialecto criollo del francés).


No sólo en la conservación de su música tradicional se manifestó el carácter musical de la población de origen africano. Como señala Sablosky, "del siglo XVII al XIX, en las casas de las plantaciones, era cosa familiar ver un violinista negro", que se encargaba de tocar gigas, minuetos y otra música bailable en las fiestas familiares. La figura del violinista tendría una gran importancia en los primeros tiempos del "jazz".

Una de las manifestaciones musicales originarias del sur y que más éxito alcanzó en el periodo anterior a la guerra, fue el "minstrel", un espectáculo que mezclaba elementos de la opereta con números musicales basados en los "cantos de las plantaciones". Se generalizaron a partir de 1820, interpretados por actores y cantantes blancos que actuaban con la cara tiznada, y su música provenía más bien de las óperas inglesas, sin relación alguna real con la música de origen africano, salvo la utilización de instrumentos como el banjo, el pandero y otros de de percusión, además del imprescindible violín. El primer gran éxito del "minstrelsy" fue "Jim Crow", un supuesto baile de los negros de las plantaciones, mucho más próximo sin embargo a las danzas escocesas, que Thomas Dartmouth Rice llevó incluso a Europa, en 1836, con su grupo Virginia Mistrels. Canciones de gran proyección e influencia en los primeros tiempos del jazz, como las de Stephen Foster, se compusieron en este marco.

Estrictamente blancas hasta la guerra civil, las bandas de música, sobre todo las militares, tuvieron también una importante presencia en el mundo musical estadounidense de la primera mitad del siglo XIX.

La Guerra de Secesión supuso un cambio importante en la vida musical estadounidense, viendo el nacimiento de nuevas prácticas y la desaparición de antiguas instituciones (las escuelas de canto, por ejemplo) que habían sido determinantes en la evolución de la música en el país, hasta entonces. En la música afroamericana, el impacto fue mayor aún, por cuanto la guerra destruyó completamente la estructura social en la que, hasta entonces, aquella se había desarrollado, por lo que la base sobre la que evoluciona es del todo diferente. En palabras del musicólogo Irving Sablosky: 

El minstrel, aunque permaneció casi hasta final del siglo XIX, perdió su carácter de parodia de la música negra e incorporó actores, cantantes y músicas realmente afroamericanas, introduciendo bailes como el cakewalk. Algunas de estas canciones, han permanecido como estándares del jazz, como "Carry me back to old Virigina" de James Bland (1878). De este proceso, y especialmente del ritmo del "cakewalk", unido a elementos de la música culta europea, nació un nuevo estilo llamado "ragtime", que era interpretado inicialmente por grupos de negros, aunque ha permanecido en la historia como un estilo eminentemente pianístico, gracias a las partituras editadas y a los rollos de pianola. Su nacimiento parece situarse en el medio oeste, en el área de San Luis, y su éxito vino de la mano de los "honky tonks" y "barrelhouses", cabaretuchos de mala fama y mucha clientela. Su auge se correspondió con la caída y desaparición del propio "minstrel".

Por otra parte, la guerra supuso un fuerte impulso de la música militar, y la incorporación de la población negra a las bandas de música. La finalización de la guerra supuso la llegada al mercado de gran cantidad de instrumentos musicales a bajo precio, accesible a los esclavos recién liberados, muchos de los cuales tomaron la música como forma de vida. Florecieron entonces las "bandas civiles", que pulularon por todo el sur, como 'marching bands' y bandas de música que, en la segunda mitad del siglo XIX, suponían el formato habitual en lo que a conciertos de música popular se refiere. 

El endurecimiento, a finales de siglo, de las "leyes de Jim Crow" en Luisiana, que promovían la segregación racial con el tristemente famoso "iguales pero separados", hizo que muchos músicos afroamericanos fueran expulsados de diversas bandas que mezclaban a blancos y a negros. La habilidad de estos artistas musicalmente formados, capaces de transcribir y leer aquello que en gran parte suponía un arte de improvisación, hizo posible conservar y diseminar sus innovaciones musicales, hecho que cobraría una importancia creciente en el desarrollo del jazz.

De entre las manifestaciones nuevas de la música de origen afroamericano, la más original y sorprendente, en su momento, fue la aparición del "blues". La mayor parte de las formas musicales del cancionero negro del XIX, se singularizan por su carácter netamente coral, incluso en aquellos que se ejecutaban de forma aislada, pues "representan la expresión genuina de un sector del pueblo". El "blues", sin embargo, se concibió desde su comienzo como canciones destinadas a ser entonadas por un solista y a manifestar opiniones o sentimientos individuales. Su origen musical está, indudablemente, en los espirituales, "shouts" y "work songs", aunque su aparición se debe precisamente a la ruptura de la fuerte conexión que estas música tenían en la propia esclavitud, y la aparición de un tipo social de afroamericano nuevo, más urbano y menos enraizado en su comunidad. Los "hollers" de los campos de algodón ("cottonfield hollers") son el nexo entre las antiguas canciones y el blues.

El más específico de los hallazgos del "blues" fue la "blue note", que es un intervalo de cuarta aumentada, y que está presente desde el principio de la aparición del género, incluso antes aún, en algunos "hollers". Algunas características asociadas al blues actualmente, como la estructura rítmica en doce compases, no son originarias, sino que se generalizan mucho más tardíamente (ya en los años 1930); y otras, como es el caso de su estructura melódica, recogen la fórmula "call and response", tradicional de casi toda la música afroamericana.

Es difícil precisar el momento en que el blues aparece como forma musical definida, aunque tiende a situarse su nacimiento entre 1870 y 1900. Paul Oliver cita un texto de Charlotte Forten, de 1862, en el que ya habla de "blues" como estado de ánimo y de cómo algunas work songs se cantaban de forma especial, para superar los "blues".

Muchos historiadores del jazz son reacios a retrasar su nacimiento más allá de comienzos del siglo XX, e incluso algunos se muestran aún más prudentes: 

Sin embargo, diversos autores han puesto de manifiesto que, ya antes de este cambio de centuria, existían bandas y músicos que tocaban una música que estaba compuesta, ciertamente, por himnos, marchas, mazurcas o valses, pero que se tocaba de una forma "hot", para agradar a un público formado por negros, en buena parte recién llegados del campo. Ortiz Oderigo cita, basándose en los trabajos de musicógrafos como James Monroe Trotter o Rudi Blesh, una serie de bandas que funcionaban desde mucho antes del cambio de siglo: la de Louis Ned, en 1874-75; la Young Man's Brass Band de Sam Thomas, en Memphis (Tennessee), hacia la misma época; la Kelly's Band y la Saint Bernard Brass Band, ambas de Nueva Orleans, hacia 1878; la Bluff City Band, de James L. Harris, que actuaba en los "riverboats"; o la "Brass Band" de Robert Baker, reunida en 1880. Estas bandas de músicos negros actuaban en desfiles callejeros, actos sociales y políticos (según Blesh, una docena de ellas estuvieron presentes en el entierro del presidente James Garfield, en 1881), así como en entierros y sepelios de personas prominentes de la comunidad afroamericana. También el hecho de que, según el músico W. C. Handy, en lugares como Memphis (Tennessee) ya existieran en 1905 muchas bandas que tocaban un "jazz" primitivo, y que estas desconocieran lo que se hacía en Nueva Orleans, es indicativo de que ambos derivaban de un proceso común anterior, al menos en varias décadas.

Los diferentes autores sí coinciden, en general, en considerar que la aparición del "jazz" fue consecuencia de la "prodigiosa conjunción de diversos elementos históricos, raciales, sociales, religiosos y musicales" que se dieron en Nueva Orleans, en las últimas décadas del siglo XIX. Entre estas, no fue la menor el desarrollo de un grupo social criollo, procedente de las relaciones de los miembros de la alta sociedad de origen franco-español con sus esclavas, que vivía en un barrio céntrico de la ciudad, con acceso a una educación musical "clásica" y que, a la vez, mantenían su vínculo con la música tradicional afroamericana. La ciudad también se pobló de un gran número de esclavos emancipados que abandonaron las plantaciones rurales y llegaban en busca de un trabajo que, desde muy pronto, ya en 1877, la retirada del ejército unionista de las zonas del sur y el endurecimiento de las leyes segregacionistas habían convertido en difícil de obtener. Esta nueva comunidad negra no tuvo, al menos en un primer momento, una buena relación con los criollos. 

El crítico Joachim E. Berendt, resume la serie de razones por las que Nueva Orleans se convirtió en la principal cuna del jazz:


De acuerdo con muchos músicos de Nueva Orleans que recordaban la época, las figuras clave en el desarrollo del nuevo estilo fueron el trompetista Buddy Bolden y los miembros de su banda. Bolden es recordado como el primero en tomar el "blues" –hasta el momento una música folclórica cantada y acompañada por guitarra o armónica– y arreglarlo para instrumentos de metal. La banda de Bolden tocó blues y otras canciones, "variando la melodía" constantemente (improvisando), creando una sensación en la ciudad y rápidamente fueron imitados por muchos otros músicos. Hacia principios del siglo XX, viajeros que visitaban Nueva Orleans remarcaban la habilidad de las bandas locales para tocar "ragtime" con una vitalidad que no se escuchaba en otros lados.

Para 1917, cuando el jazz comienza a recogerse en grabaciones sonoras, el proceso de formación del género ya se había completado e, incluso, había saltado al norte (Chicago, especialmente) y estaba presto para globalizarse.

Los primeros grupos musicales que tocaron una música que ya podemos considerar sin duda como jazz, nada más comenzar el siglo XX, se caracterizaban por poseer:


La utilización de estos instrumentos era muy específica del género:
Por otro lado, el percusionista añadió vivacidad y síncopa a estos primeros combos jazzísticos: 

Rítmicamente, los dos rasgos característicos del "jazz" de Nueva Orleans eran el "break", originador de homofonía, y el "stomp", con rasgos polifónicos, y que de alguna forma mantenían el carácter cruzado de la rítmica africana.

Cuando acaba el siglo XIX, hay ya algunos músicos que tienen cierto renombre en el mundo del incipiente jazz, y no solamente el ya citado Buddy Bolden (1863-1931), quizás el más famoso de todos ellos. Manuel Mello, que tocó en las primeras bandas de la época, afirmaba en 1945 a ""Basin street"", la revista de la National Jazz Foundation de Nueva Orleans, que él ya oía jazz antes de que apareciera Bolden en la escena local, destacando especialmente el grupo del violinista Johnny Schenk, que se formó en 1893 y se deshizo en 1898. Se sabe que, además de Schenk, lo integraban el cornetista Batt Steckler, John Weinmunson (guitarra) y Albert Bix (contrabajo), y que solía tocar en fiestas y en las celebraciones del Mardi Gras. 

Autores como Robert Goffin y André Hodeir afirman también que el cornetista Manuel Pérez (1879-1946) formó su primera banda antes de que lo hiciera Bolden. Lo cierto es que, este último, organizó su banda a comienzo de la década de 1890, oscilando entre cinco y siete miembros: Bolden, a la corneta; Willie Cornish o Frankie Dusen al trombón; Frank Lewis o William Warner, a veces los dos, al clarinete; Tom Adams, violín; Brock Mumford, guitarra; Jimmy Johnson, contrabajo; y Louis Ray, percusión. Como otras muchas bandas de ese momento, aún carecía de piano, y se presentaba como Buddy Bolden's Ragtime Band. Estas bandas trabajaban en locales nocturnos de Nueva Orleans, pero con frecuencia realizaban temporadas en los "riverboats", que unían la ciudad del delta con San Luis, remontando el Mississipi. El cornetista Bunk Johnson se unió a su banda alrededor de 1895, que aún no contaba con piano. Dado que Bolden fue internado en un hospital siquiátrico en 1907, no llegó a grabar ningún disco. 

También antes de acabar el siglo XIX, el violinista John Robechaux mantuvo una banda entre 1895 y 1900, en la que estaban James Williams (corneta), Ed Cornish (trombón), Lorenzo Tio y George Baquet (clarinetes), Buddy Scott (guitarra), Henry Kimball (contrabajo) y Dee Dee Chandler (batería). Entre 1900 y 1909, destacó la Olympia Band, dirigida sucesivamente por el trombonista Joseph Petit y el cornetista Freddie Keppard, y en la que, además, estuvo Alphonse Picou (clarinete), entre otros. La banda volvió a reunirse en 1912, con diversos cambios, e incorporando a Sidney Bechet. Por su parte, Frank Dusen mantuvo la banda de Bolden, tras la retirada de este, en 1907, incluyendo a figuras como Louis Ned y Papa Mutt Carey en las cornetas. También la Imperial Band de Manuel Pérez, fundada en 1909 y funcionando hasta 1912, con George Filhe en el trombón, entre otros.

Además, cabe citar a las bandas del violinista Armand Piron, que solía trabajar en los "riverboats", manteniéndose hasta 1923; la Onward Brass Band, en la que también militó Pérez; la Tuxedo Brass Band del cornetista Papa Celestin; los Brown Skinned Babies del trombonista Kid Ory, en la que comenzaron sus carreras King Oliver y, después, Louis Armstrong; la Noone-Petit Orchestra, de Jimmy Noone y Joseph Petit, que también incuyó a Richard M. Jones (piano); la banda de Papa Mutt Carey, y otras muchas que no llegaron a grabar nunca. Destaca entre estas la Original Creole Orchestra del contrabajista William Manuel Johnson, quien se reputa como el pionero en el estilo de tocar el instrumento sin arco, y del cornetista Freddie Keppard, que realizó giras por todo el país antes de su separación en 1917. Según indica William Russell, la compañía Victor ofreció a Keppard realizar una grabación en 1916, pero este se negó para que no copiaran su estilo. Sería Nick LaRocca, un trompetista blanco, quien tuviera el honor de realizar la primera grabación de jazz, con su banda Original Dixieland Jass Band, ya en 1917.

Es interesante señalar que, en aquellos tiempos, la música de baile popular no era el "jazz", aunque en ella se encontraban ya formas precursoras, por ejemplo con la incorporación de elementos del "blues" o el "ragtime". Compositores neoyorquinos (los conocidos como el grupo de la Tin Pan Alley, entre ellos Irving Berlin), incorporaron la influencia del ragtime a sus composiciones, aunque apenas utilizaban los mecanismos específicos que eran naturales a los artistas del jazz (los ritmos, las "blue notes"). Pocas cosas consiguieron más para hacer popular este jazz incipiente, que el éxito de Berlín en 1911, llamado "Alexander's Ragtime Band", que se convirtió en una auténtica locura tanto en los Estados Unidos como en Europa (especialmente en Viena). Aunque la canción no estaba escrita como un ragtime, la letra habla de una banda de música que convertía canciones populares a ritmos de jazz –como en la estrofa donde dice: "si quieres oír el ‘Swanee River’ tocado en ragtime…"–.

No sólo en Nueva Orleans se desarrolló una nueva música. W. C. Handy aseguraba que, ya en 1905, en Memphis, había bandas de jazz. Igual sucedió en otros lugares del país.

El pastor afroamericano Daniel J. Jenkins, de Charleston (Carolina del Sur), fue una figura insólita de gran importancia en el temprano desarrollo del "jazz". En 1891, Jenkins estableció el Orfanato Jenkins para niños y, cuatro años después, instituyó un riguroso programa musical en el cual los jóvenes del orfanato eran educados en música religiosa y secular contemporánea, incluyendo oberturas y marchas. Huérfanos precoces y fugitivos, algunos de los cuales tocaban "ragtime" en bares y burdeles, fueron enviados al orfanato para su "salvación" y rehabilitación, y también para que hicieran su contribución a la música. Siguiendo la moda de los Fisk Jubilee Singers y de la Universidad de Fisk, las bandas del orfanato Jenkins viajaron mucho, ganando dinero para mantener el orfanato. Jenkins recibía anualmente en el orfanato aproximadamente 125-150 "ovejas negras", y muchos de ellos recibieron entrenamiento musical formal. Menos de 30 años después, cinco bandas actuaban nacionalmente, y una de ellas viajaba a Inglaterra –en contra de la tradición Fisk–. Sería difícil exagerar la influencia de las bandas del Orfanato Jenkins en el temprano "jazz", dado que sus miembros llegaron a tocar con leyendas del género como Duke Ellington, Lionel Hampton y Count Basie. Entre ellos estuvieron los virtuosos trompetistas Cladys "Cat" Anderson, Gus Aitken y Jabbo Smith.

En el norte de Estados Unidos, se desarrolló un estilo "caliente" de tocar "ragtime", centrado en la ciudad de Nueva York, aunque se puede encontrar este estilo en las comunidades afroamericanas desde Baltimore hasta Maryland. Algunos comentaristas posteriores han categorizado esta forma musical como una temprana forma de "jazz", mientras que otros discrepan. Fue caracterizado por su ritmo jovial, pero carecía de la influencia "blues" de los estilos sureños. Las versiones solistas de piano del estilo norteño fueron tipificadas por pianistas como el célebre compositor Eubie Blake (un hijo de esclavos cuya carrera musical se extendió durante ocho décadas). James P. Johnson tomó el estilo norteño y en 1919 desarrolló un estilo propio para tocar que se denominó "stride", también conocido como "barrelhouse piano". En este estilo, la mano derecha toca la melodía, mientras que la izquierda camina o "da saltos" de un compás más rápido a uno más lento, manteniendo el ritmo. Johnson influyó a pianistas posteriores como Fats Waller, Willie "The Lion" Smith, Art Tatum y Billy Kyle.

El principal líder orquestal de este estilo fue James Reese Europe, y sus grabaciones de 1913 y 1914 preservan un raro vislumbre de la cumbre de este estilo. Fue durante estos tiempos cuando la música de Europe influyó en un entonces joven George Gershwin, quien compondría la clásica "Rhapsody in Blue" inspirada en el jazz. En la época en que Europe grabó nuevamente, en 1919, Gershwin incorporó, a su vez, la influencia que había tenido en él el estilo de Nueva Orleans. Las grabaciones de Tim Brymn dieron a las generaciones siguientes una mirada diferente del "caliente" estilo norteño sin ser demasiado evidente la influencia de Nueva Orleans.

A comienzos de la década de 1910 en Chicago, las bandas de baile seguían la moda de Nueva Orleans, aunque comenzaron a introducir un instrumento hasta entonces inusual, el saxofón. Bud Freeman fue uno de sus principales nombres. Con la llegada de sucesivas remesas de músicos desde el delta, en la ciudad fue aquilatándose un estilo propio, que se denominó "Chicago Jazz".

A las orillas del Mississippi desde Memphis, Tennessee, hasta Saint Louis, Misuri, se desarrolló otro estilo de bandas que incorporaron el blues como elemento principal. El compositor y líder más famoso de este estilo fue el llamado "Padre del Blues", W. C. Handy. Mientras que en algunos aspectos era similar al estilo de Nueva Orleans (la influencia de Bolden se difundió a lo largo del río), carecía de la libre improvisación de los estilos más sureños. El mismo Handy, durante muchos años, acusó al jazz de ser innecesariamente caótico, y en su estilo la improvisación estaba limitada a cortos rellenos entre frases considerándolo inapropiado para la melodía principal.



</doc>
<doc id="4863" url="https://es.wikipedia.org/wiki?curid=4863" title="Periférico (informática)">
Periférico (informática)

En informática, periférico es la denominación genérica para designar al aparato o dispositivo auxiliar e independiente conectado a la unidad central de procesamiento de una computadora.

Se consideran periféricos a las unidades o dispositivos de hardware a través de los cuales la computadora se comunica con el exterior, y también a los sistemas que almacenan o archivan la información, sirviendo de memoria auxiliar de la memoria principal.

Se considera periférico a los dispositivos que pertenece al núcleo fundamental de la computadora, formado por la unidad central de procesamiento (CPU) y la memoria principal, permitan realizar operaciones de entrada/salida (E/S) complementarias al proceso de datos que realiza la CPU. Estas tres unidades básicas en un computador, CPU, memoria central y el subsistema de E/S, están comunicadas entre sí por tres buses o canales de comunicación: 

A pesar de que el término periférico implica a menudo el concepto de «adicional pero no esencial», muchos de ellos son elementos fundamentales para un sistema informático. El monitor, es prácticamente el único periférico que la personas considera imprescindible en cualquier computadora personal (no lo fue en los primeros computadores) pero a pesar de ello, técnicamente no lo es. El ratón o "mouse" es posiblemente el ejemplo más claro de este aspecto. A principios de la década de 1990 no todas las computadoras personales incluían este dispositivo. El sistema operativo MS-DOS, el más común en esa época, tenía una interfaz de línea de comandos para lo que no era necesario el empleo de un ratón, todo se hacía mediante comandos de texto. Fue con la popularización de Finder, sistema operativo de la Macintosh de Apple y la posterior aparición de Windows cuando el ratón comenzó a ser un elemento imprescindible en cualquier hogar dotado de una computadora personal. Actualmente existen sistemas operativos con interfaz de texto que pueden prescindir del ratón como, por ejemplo, MS-Dos. El caso del teclado es también emblemático, pues en las nuevas computadoras tabletas, sistemas de juego o teléfonos móviles con pantalla táctil, el teclado se emula en la pantalla. Inclusive en casos de adaptaciones especiales los teclados dejan de ser el periférico de entrada más utilizado, llegando a desaparecer en algunos casos por el uso de programas reconocedores de voz.

Los periféricos pueden clasificarse en las siguientes categorías principales:


Son los que permiten introducir datos externos a la computadora para su posterior tratamiento por parte de la CPU. Estos datos pueden provenir de distintas fuentes, siendo la principal un ser humano. Los periféricos de entrada más habituales son: 

Un "teclado de computadora" es un periférico, físico o virtual (por ejemplo teclados en pantalla o teclados táctiles), utilizado para la introducción de órdenes y datos en una computadora. Tiene su origen en los teletipos y las máquinas de escribir eléctricas, que se utilizaron como los teclados de los primeros ordenadores y dispositivos de almacenamiento (grabadoras de cinta de papel y tarjetas perforadas). Aunque físicamente hay una miríada de formas, se suelen clasificar principalmente por la distribución del teclado de su zona alfanumérica, pues salvo casos muy especiales es común a todos los dispositivos y fabricantes (incluso para teclados árabes y japoneses).

El "mouse" (del inglés, pronunciado ) o ratón es un periférico de entrada de uso manual para computadora, utilizado como entrada o control de datos. Se utiliza con una de las dos manos del usuario y detecta su movimiento relativo en dos dimensiones por la superficie horizontal en la que se apoya, reflejándose habitualmente a través de un puntero o flecha en el monitor. Anteriormente, la información del desplazamiento era transmitida gracias al movimiento de una bola debajo del ratón, la cual accionaba dos rodillos que correspondían a los ejes X e Y. Hoy, el puntero reacciona a los movimientos debido a un rayo de luz que se refleja entre el ratón y la superficie en la que se encuentra.
Cabe aclarar que un ratón óptico apoyado en un espejo o sobre un barnizado por ejemplo es inutilizable, ya que la luz láser no desempeña su función correcta. La superficie a apoyar el ratón debe ser opaca, una superficie que no genere un reflejo, es recomendable el uso de alfombrillas.

El ratón es el tipo de dispositivo apuntador o señalador más utilizado; existen también: "gamepad", lápiz óptico, palanca de mando ("joystick"), "Touchpad", "Trackball", volante para videojuegos, etcétera.

Con el micrófono, además de grabar cualquier audio o sonido mediante alguna aplicación informática, también permite el uso de sistemas de reconocimiento del habla o reconocimiento de voz, disponible incluso en navegadores web para la búsqueda de información, tanto es computadoras portátiles o computadoras de escritorio, como en dispositivos móviles.

En informática, un escáner (del inglés: "scanner") es un periférico que se utiliza para convertir, mediante el uso de la luz, imágenes o cualquier otro impreso a formato digital. Actualmente vienen unificadas con las impresoras formando multifunciones.


Los periféricos de salida reciben la información procesada por la CPU y la reproducen, de modo que sea perceptible por el usuario.




El "monitor de computadora" o "pantalla de computadora" es el dispositivo de salida que mediante una interfaz muestra los resultados o los gráficos del procesamiento de una computadora. Existen varios tipos de monitores: 

Una "impresora" es un periférico de computadora que permite producir una copia permanente de textos o gráficos de documentos almacenados en formato electrónico, imprimiendo en papel de lustre los datos en medios físicos, normalmente en papel o transparencias, utilizando cartuchos de tinta o tecnología láser. Muchas impresoras son usadas como periféricos, y están permanentemente unidas a la computadora por un cable. Otras impresoras, llamadas impresoras de red, tienen una interfaz de red interna (típicamente wireless o Ethernet), y que puede servir como un dispositivo para imprimir en papel algún documento para cualquier usuario de la red. Actualmente se comercializan impresoras multifuncionales, que aparte de sus funciones de impresora funcionan simultáneamente como fotocopiadora y escáner, siendo este tipo de impresoras las más recurrentes en el mercado. Cabe destacar que las impresoras multifuncionales no pertenecen a los periféricos de salida solamente, sino que se las considera como periféricos de entrada/salida.


Los altavoces se utilizan para escuchar los sonidos emitidos por la computadora, tales como música, sonidos de errores, conferencias, etcétera.


Los periféricos de entrada/salida son los que utiliza la computadora para mandar y para recibir información. Su función es la de almacenar o guardar, de forma permanente o virtual, todo aquello que hagamos con la computadora para que pueda ser utilizado por los usuarios u otros sistemas.

Pantalla táctil

Es una pantalla que mediante un toque directo sobre su superficie permite la entrada de datos y órdenes al dispositivo, y a su vez muestra los resultados introducidos previamente; actuando como periférico de entrada y salida de datos, así como emulador de datos interinos erróneos al no tocarse efectivamente. Este contacto también se puede realizar por medio de un lápiz óptico u otras herramientas similares. Actualmente hay pantallas táctiles que pueden instalarse sobre una pantalla normal, de cualquier tipo o denominación (LCD, monitores y televisores CRT, plasma, etc.).

Multitáctil

Es el nombre con el que se conoce a una técnica de interacción persona-computador y al hardware que la aplica. La tecnología multitáctil consiste en una pantalla táctil o touchpad que reconoce simultáneamente múltiples puntos de contacto, así como el software asociado a esta que permite interpretar dichas interacciones simultáneas.

Una "impresora multifunción" o dispositivo multifuncional es un periférico que se conecta a la computadora y que posee las siguientes funciones dentro de un único bloque físico:
Impresora, escáner, fotocopiadora, ampliando o reduciendo el original,
fax (opcionalmente). Lector de memoria para la impresión directa de fotografías de cámaras digitales. En ocasiones, aunque el fax no esté incorporado, la impresora multifunción es capaz de controlarlo si se le conecta a un puerto USB. También, pueden poseer un disco duro (las unidades más grandes utilizadas en oficinas) para almacenar documentos e imágenes. 

Los dispositivos y soportes de almacenamiento guardan los datos que usa la CPU una vez que han sido eliminados de la memoria principal, porque la memoria se borra cada vez que se apaga la computadora. Pueden ser internos o portátiles, como un disco duro, o extraíbles, como un CD o DVD.


La memoria secundaria (auxiliar, periférica o externa) o almacenamiento secundario, es el conjunto de dispositivos y soportes de almacenamiento de datos que conforman el subsistema de memoria del sistema informático, junto con la memoria primaria o principal.

El disco duro es un sistema de grabación magnética digital, es donde en la mayoría de los casos reside el sistema operativo de la computadora. En los discos duros se almacenan los datos del usuario. En él encontramos dentro de la carcasa una serie de platos metálicos apilados girando a gran velocidad. Sobre estos platos se sitúan los cabezales encargados de leer o escribir los impulsos magnéticos.

Una unidad de estado sólido es un sistema de memoria no volátil. Están formados por varios chips de memoria NAND Flash en su interior unidos a una controladora que gestiona todos los datos que se transfieren. Tienen una gran tendencia a suceder definitivamente a los discos duros mecánicos por su gran velocidad y tenacidad. Al no estar formadas por discos en ninguna de sus maneras, no se pueden categorizar como tal, aunque erróneamente se tienda a ello.


Su función es permitir o facilitar la interacción entre dos o más computadoras, o entre una computadora y otro periférico externo a la computadora. Entre ellos se encuentran los siguientes:

Las placas o tarjetas de interfaz de red ("Network Interface Card", NIC) pueden estar integradas a la placa base o conectadas mediante ranuras de expansión.




</doc>
<doc id="4865" url="https://es.wikipedia.org/wiki?curid=4865" title="Monitor de computadora">
Monitor de computadora

El monitor de computadora (en Hispanoamérica) o monitor de ordenador o pantalla (en España) es el principal dispositivo de salida (interfaz), que muestra datos o información al usuario.

También puede considerarse un periférico de entrada/salida si el monitor tiene pantalla táctil o multitáctil.

Las primeras computadoras se comunicaban con el operador mediante unas pequeñas luces, que se encendían o se apagaban al acceder a determinadas posiciones de memoria o ejecutar ciertas instrucciones.

Años más tarde aparecieron ordenadores que funcionaban con tarjeta perforada, que permitían introducir programas en el computador.
Durante los años 50, la forma más común de interactuar con un computador era mediante un teletipo, que se conectaba directamente a este e imprimía todos los datos de una sesión informática. Fue la forma más barata de visualizar los resultados hasta la década de los 70, cuando empezaron a aparecer los primeros monitores de CRT (tubo de rayos catódicos). Seguían el estándar MDA (Monochrome Display Adapter), y eran monitores monocromáticos (de un solo color) de IBM.

Estaban expresamente diseñados para modo texto y soportaban subrayado, negrita, cursiva, normal e invisibilidad para textos.
Poco después y en el mismo año salieron los monitores CGA (Color Graphics Adapter –gráficos adaptados a color–) fueron comercializados en 1981 al desarrollarse la primera tarjeta gráfica a partir del estándar CGA de IBM. Al comercializarse a la vez que los MDA los usuarios de PC optaban por comprar el monitor monocromático por su costo.

Tres años más tarde surgió el monitor EGA (Enhanced Graphics Adapter - adaptador de gráficos mejorados) estándar desarrollado por IBM para la visualización de gráficos, este monitor aportaba más colores (16) y una mayor resolución.
En 1987 surgió el estándar VGA (Video Graphics Array - Matriz gráfica de video) fue un estándar muy acogido y dos años más tarde se mejoró y rediseñó para solucionar ciertos problemas que surgieron, desarrollando así SVGA (Super VGA), que también aumentaba colores y resoluciones, para este nuevo estándar se desarrollaron tarjetas gráficas de fabricantes hasta el día de hoy conocidos como S3 Graphics, NVIDIA o ATI entre otros.

Con este último estándar surgieron los monitores CRT que hasta no hace mucho seguían estando en la mayoría de hogares donde había un ordenador.


El tamaño de la pantalla es la distancia en diagonal de un vértice de la pantalla al opuesto, que puede ser distinto del área visible cuando hablamos de CRT , mientras que la proporción o relación de aspecto es una medida de proporción entre el ancho y el alto de la pantalla, así por ejemplo una proporción de 4:3 ( Cuatro tercios ) significa que por cada 4 píxeles de ancho tenemos 3 de alto, una resolución de 800x600 tiene una relación de aspecto 4:3, sin embargo estamos hablando de la proporción del monitor.

Estas dos medidas describen el tamaño de lo que se muestra por la pantalla, históricamente hasta no hace mucho tiempo y al igual que las televisiones los monitores de ordenador tenían un proporción de 4:3. Posteriormente se desarrollaron estándares para pantallas de aspecto panorámico 16:9 (a veces también de 16:10 o 15:9) que hasta entonces solo veíamos en el cine.

Las medidas de tamaño de pantalla son diferentes cuando se habla de monitores CRT y monitores LCD.

Los tamaños comunes de pantalla suelen ser de 15, 17, 19, 21 pulgadas. La correspondencia entre las pulgadas de CRT y LCD en cuanto a zona visible se refiere, suele ser de una escala inferior para los CRT, es decir una pantalla LCD de 17 pulgadas equivale en zona visible a una pantalla de 19 pulgadas del monitor CRT (aproximadamente).

Es el número máximo de píxeles que pueden ser mostrados en cada dimensión, es representada en filas por columnas. Está relacionada con el tamaño de la pantalla y la proporción.

Los monitores LCD solo tienen una resolución nativa posible, por lo que si se hacen trabajar a una resolución distinta, se escalará a la resolución nativa, lo que suele producir artefactos en la imagen.

Las resoluciones más usadas son:

Cada píxel de la pantalla tiene interiormente 3 subpíxeles, uno rojo, uno verde y otro azul; dependiendo del brillo de cada uno de los subpíxeles, el píxel adquiere un color u otro de forma semejante a la composición de colores RGB.

La manera de organizar los subpíxeles de un monitor varia entre los dispositivos. Se suelen organizar en líneas verticales, aunque algunos CRT los organizan en puntos formando triángulos. Para mejorar la sensación de movimiento, es mejor organizarlos en diagonal o en triángulos.
El conocimiento del tipo de organización de píxeles, puede ser utilizado para mejorar la visualización de imágenes de mapas de bit usando renderizado de subpíxeles.

La mayor parte de los monitores tienen una profundidad 8 bits por color (24 bits en total), es decir, pueden representar aproximadamente 16,8 millones de colores distintos.

En Hardware, un monitor es un periférico que muestra la información de forma gráfica de una computadora. Los monitores se conectan a la computadora a través de una tarjeta gráfica (o adaptador o tarjeta de video).






Básicamente, los monitores pueden clasificarse en dos tipos generales:

En Software, un monitor de un programa es toda aquella herramienta que viene con un programa que sirve para controlar alguna situación. Por ejemplo el monitor de un antivirus, encargado de monitorear continuamente la computadora para verificar que no se ejecute ningún virus.





Los principales fabricantes de monitores conocidos a nivel internacional son los siguientes:




</doc>
<doc id="4867" url="https://es.wikipedia.org/wiki?curid=4867" title="Tarjeta de red">
Tarjeta de red

La tarjeta de red, también conocida como placa de red, adaptador de red, adaptador LAN, Interfaz de red física, o sus términos en inglés Network Interface Card o Network interface controller (NIC), cuya traducción literal del inglés es «"tarjeta de interfaz de red"» (TIR), es un componente de hardware que conecta una computadora a una red informática y que posibilita compartir recursos (como archivos, discos duros enteros, impresoras e internet) entre dos o más computadoras, es decir, en una red de computadoras.

Las primeras "tarjetas de interfaz de red" se implementaban comúnmente en tarjetas de expansión que se conectaban en un bus de la computadora. El bajo costo y la ubicuidad del estándar Ethernet hizo posible que la mayoría de las computadoras modernas tengan una interfaz de red integrada en la placa base. Las placas base de servidor más nuevas pueden incluso tener interfaces de red duales incorporadas.

Las capacidades de Ethernet están ahora integradas en el chipset de la placa base o implementadas a través de un chip Ethernet dedicado de bajo costo, conectado a través del bus PCI (o el nuevo PCI Express), así que no se requiere una tarjeta de red por separado a menos que se necesiten interfaces adicionales o se utilice otro tipo de red.

Las modernas "Tarjetas de red" ofrecen funciones avanzadas como interfaz de interrupción y DMA para los procesadores host, soporte para múltiples colas de recepción y transmisión, particionamiento en múltiples interfaces lógicas y procesamiento de tráfico de red en controlador, como el motor de descarga TCP.

La NIC pueden utilizar una o más de las siguientes técnicas para indicar la disponibilidad de paquetes a transferir:

Además, los NIC pueden utilizar una o más de las siguientes técnicas para transferir datos de paquetes:

Además, un búfer de paquetes en la NIC puede no ser necesario y puede reducir la latencia. Existen dos tipos de DMA:

Una tarjeta de red Ethernet normalmente tiene un socket 8P8C donde está conectado el cable de red. Las NICs más antiguas también proporcionaban conexiones BNC, o AUI. Algunos LEDs informan al usuario si la red está activa y si se produce o no transmisión de datos. Las tarjetas de red Ethernet suelen soportar Ethernet de 10 Mbit/s, 100 Mbits/s y 1000 Mbits/s. Tales tarjetas son designadas como "10/100/1000", lo que significa que pueden soportar una tasa de transferencia máxima nocional de 10, 100 o 1000 Mbit/s. También están disponibles NIC de 10 Gbits/s 

La NIC implementa los circuitos electrónicos necesarios para comunicarse sobre una red de computadoras, ya sea utilizando de cables como Token Ring, Ethernet, fibra, o sin cables como Wi-Fi, es por tanto un dispositivo de capa física y uno de capa de enlace de datos ya que proporciona acceso físico a un medio de red y, para IEEE 802 y redes similares, proporciona un sistema de direccionamiento de bajo nivel mediante el uso de la dirección MAC que se asignan exclusivamente a las tarjetas de red.

Esto proporciona una base para una pila de protocolos de red completa, permitiendo la comunicación entre pequeños grupos de computadoras en la misma red de área local (LAN) y comunicaciones de red a gran escala a través de protocolos enrutables, como Internet Protocol (IP).

Aunque existen otras tecnologías de red, las redes IEEE 802, incluidas las variantes Ethernet, han alcanzado casi la ubicuidad desde mediados de los años noventa.

Cada tarjeta de red tiene un número de identificación único de 48 bits en hexadecimal que asignan los fabricantes legales de Hardware llamado dirección MAC ("Media Access Control"; control de acceso al medio) también conocido como "dirección física" que es independiente al protocolo de red que se utilice. Estas direcciones únicas de hardware son administradas por el “Instituto de Ingeniería Eléctrica y Electrónica” (IEEE, "Institute of Electronic and Electrical Engineers"). Los tres primeros octetos (24 bits) del número MAC, identifican al proveedor específico y es conocido como número OUI ("Organizationally unique identifier", identificador único de organización), designado por IEEE, que combinado con otro número de 24 bits forman la dirección MAC completa.

Por contraparte, una dirección IP es un número que identifica, de manera lógica y jerárquica, a una Interfaz en red (elemento de comunicación/conexión) de un dispositivo (computadora, tableta, portátil, teléfono inteligente) que utilice el protocolo IP (Internet Protocol), que corresponde al nivel de red del modelo TCP/IP. Actualmente se utiliza el protocolo IPv4 y se está integrando muy lentamente el protocolo IPv6.


Existen diversos tipos de tarjetas, placas o adaptadores de red, en función del tipo de cableado o arquitectura de red:

Las tarjetas para red Token Ring están prácticamente en desuso, debido a la baja velocidad y elevado costo respecto de Ethernet.
Tenían conector DB-9.
También se utilizó el conector RJ-45 para las NIC y las MAU ("Multiple Access Unit", unidad de múltiple acceso), que era el núcleo de una red Token Ring.

Las tarjetas para red ARCNET utilizaban principalmente conector BNC y/o puertos RJ-45.

Las tarjetas de red para Ethernet utilizan conectores:

El caso más habitual es el de la tarjeta con el conector RJ-45, aunque durante la transición del uso mayoritario de cable coaxial (10 Mbit/s) al cable de par trenzado (100 Mbit/s) abundaron las tarjetas con conectores BNC y RJ-45, e incluso BNC / AUI / RJ-45 (en muchas de ellas se pueden ver serigrafiados los conectores no usados).

Con la entrada de las redes Gigabit y el que en las casas sea frecuente la presencias de varias computadoras comienzan a verse tarjetas y placas base (con NIC integradas) con 2 y hasta 4 puertos RJ-45, que antes estaba reservado a los servidores.

Pueden variar en función de la velocidad de transmisión, normalmente 10 Mbit/s ó 10/100 Mbit/s. también se utilizan las de 100 Mbit/s, conocida como Gigabit Ethernet y en algunos casos 10 Gigabit Ethernet, utilizando también cable de par trenzado, de categorías: 6, 6a y Cat 7, que funcionan a frecuencias más altas.

Las velocidades especificadas por los fabricantes son teóricas, por ejemplo las de 100 Mbit/s realmente pueden llegar como máximo a 78,4 Mbit/s.

También son NIC las tarjetas inalámbricas ("wireless"), que vienen en diferentes variedades dependiendo de la norma a la cual se ajusten, usualmente son 802.11b, 802.11g y 802.11n. Las más populares son la 802.11b que transmite a 11 Mbit/s (1,375 MB/s) con una distancia teórica de 100 metros y la 802.11g que transmite a 54 Mbit/s (6,75 MB/s).

La velocidad real de transferencia que llega a alcanzar una tarjeta Wi-Fi con protocolo 11.b es de unos 4 Mbit/s (0,5 MB/s) y las de protocolo 11.g llegan como máximo a unos 20 Mbit/s. El protocolo 11.n se viene utilizando con capacidad de transmitir 600 Mbit/s. La capa física soporta una velocidad de 300 Mbit/s, con el uso de dos flujos espaciales dentro de un canal de 40 MHz. Dependiendo del entorno, esto puede traducirse en un rendimiento percibido por el usuario de 100 Mbit/s.




</doc>
<doc id="4869" url="https://es.wikipedia.org/wiki?curid=4869" title="Placa base">
Placa base

La placa base, también conocida como tarjeta madre, placa madre o placa principal ("motherboard" o "mainboard" en inglés), es una tarjeta de circuito impreso a la que se conectan los componentes que constituyen la computadora.

Es una parte fundamental para montar cualquier computadora personal de escritorio o portátil o algún dispositivo. Tiene instalados una serie de circuitos integrados, entre los que se encuentra el circuito integrado auxiliar (chipset), que sirve como centro de conexión entre el microprocesador (CPU), la memoria de acceso aleatorio (RAM), las ranuras de expansión y otros dispositivos. 

Está instalada dentro de una carcasa o gabinete que por lo general está hecha de chapa y tiene un panel para conectar dispositivos externos y muchos conectores internos y zócalos para instalar componentes internos.

La placa base, además incluye un "firmware" llamado BIOS, que le permite realizar las funcionalidades básicas, como pruebas de los dispositivos, vídeo y manejo del teclado, reconocimiento de dispositivos y carga del sistema operativo.

Una placa base típica admite los siguientes componentes:


Por uno o varios de estos conectores de alimentación, una alimentación eléctrica proporciona a la placa base los diferentes voltajes e intensidades necesarios para su funcionamiento.

El zócalo ("socket") de CPU es un receptáculo que encastra el microprocesador y lo conecta con el resto de componentes a través del bus frontal de la placa base.

Si la placa base dispone de un único zócalo para microprocesador, se denomina monoprocesador. En cambio, si dispone de dos o más zócalos, se denomina placa multiprocesador.

Las placas bases constan de ranuras ("slots") de memoria de acceso aleatorio, su número es de 2 a 
8 ranuras en una misma placa base común.

En ellas se insertan dichas memorias del tipo conveniente dependiendo de la velocidad, capacidad y fabricante requeridos según la compatibilidad de cada placa base y la CPU.

El "chipset" es una serie o conjunto de circuitos electrónicos, que gestionan las transferencias de datos entre los diferentes componentes de la computadora (procesador, memoria, tarjeta gráfica, unidad de almacenamiento secundario, etcétera).

El "chipset", generalmente se divide en dos secciones:

Las nuevas líneas de procesadores de escritorio tienden a integrar el propio controlador de memoria dentro del procesador.




Los buses son espacios físicos que permiten el transporte de información y energía entre dos puntos de la computadora.
Los buses generales son cinco:

Los buses de datos son las líneas de comunicación por donde circulan los datos externos e internos del microprocesador.

El bus de dirección es la línea de comunicación por donde viaja la información específica sobre la localización de la dirección de memoria del dato o dispositivo al que se hace referencia.

El bus de control es la línea de comunicación por donde se controla el intercambio de información con un módulo de la unidad central y los periféricos.

Los buses de expansión son el conjunto de líneas de comunicación encargado de llevar el bus de datos, el bus de dirección y el de control a la tarjeta de interfaz (entrada, salida) que se agrega a la placa principal. 

Todos los componentes de la placa base se vinculan a través del bus del sistema, mediante distintos tipos de datos del microprocesador y de la memoria principal, que también involucra a la memoria caché de nivel 2. La velocidad de transferencia del bus de sistema está determinada por la frecuencia del bus y el ancho.

Las placas base necesitan tener muchas dimensiones compatibles con las cajas que las contienen, de manera que desde los primeros computadores personales se han establecido características mecánicas, llamadas factor de forma. Definen la distribución de diversos componentes y las dimensiones físicas, como por ejemplo el largo y ancho de la tarjeta, la posición de los agujeros para los tornillos de sujeción y las características de los conectores. 

Con los años, varias normas se fueron imponiendo.

1983: "XT" (sigla en inglés de "eXtended Technology", «tecnología extendida») es el formato de la placa base de la computadora IBM PC XT (modelo 5160), lanzado en 1983. En este factor de forma se definió un tamaño exactamente igual al de una hoja A4 y un único conector externo para el teclado.

1984: "AT" ("Advanced Technology", «tecnología avanzada») es uno de los formatos más grandes de toda la historia de la PC (305×279–330 mm), definió un conector de potencia formado por dos partes. Fue usado de manera extensa de 1985 a 1995.

1995: "ATX" ("Advanced Technology eXtended", «tecnología avanzada extendida») Utiliza las conexiones exteriores en forma de un panel de E/S y definió un conector de 24 pines para la energía. Se usa en la actualidad en forma de algunas variantes, que incluyen conectores de energía extra o reducciones de tamaño.


2001: "ITX" ("Information Technology eXtended", «tecnología de información extendida»), con rasgos procedentes de las especificaciones microATX y FlexATX de Intel, el diseño de VIA se centra en la integración en placa base del mayor número posible de componentes, además de la inclusión del hardware gráfico en el propio chipset del equipo, siendo innecesaria la instalación de una tarjeta gráfica en la ranura AGP.

2004: "BTX" ("Balanced Technology eXtended", «tecnología balanceada extendida») fue retirada en muy poco tiempo por la falta de aceptación, resultó prácticamente incompatible con ATX, salvo en la fuente de alimentación. Fue creada para intentar solventar los problemas de ruido y refrigeración, como evolución de la ATX.

2007: "DTX" eran destinadas a las PC de pequeño formato. Hacen uso de un conector de energía de 24 pines y de un conector adicional de 2x2.

Durante la existencia del PC, muchas marcas han intentado mantener un esquema cerrado de hardware, denominado privativo, fabricando placas base incompatibles físicamente con los factores de forma con dimensiones, distribución de elementos o conectores que son atípicos. Entre las marcas más persistentes está Dell, que rara vez fabrica equipos diseñados con factores de forma de la industria.

Varios fabricantes se reparten el mercado de placas base, tales como: Advantech, Albatron, Aopen, ASUS, AsRock, Biostar, Chaintech, Dell, DFI, ECS EliteGroup, FIC, Foxconn, Gigabyte Technology, iBase, iEi, Intel, Lenovo, MSI, Pc Chips, Sapphire Technology, Super Micro, Tyan, VIA, XFX, Zotac.

Algunos diseñan y fabrican uno o más componentes de la placa base, mientras que otros ensamblan los componentes que terceros han diseñado y fabricado.

La mayoría de las placas de PC fabricadas después de 2001 se pueden clasificar en dos grupos:



Este tipo de placa base incluye zócalos para instalar varios procesadores (generalmente 2, 4, 8 o más). No nos estamos refiriendo a instalar un procesador con varios núcleos, sino a que podemos instalar varios procesadores físicos. Algunos fabricantes proveen placas base que pueden acoger hasta 8 procesadores (en el caso de "socket" 939 para procesadores AMD Opteron y sobre "socket" 604 para procesadores Intel Xeon).


</doc>
<doc id="4874" url="https://es.wikipedia.org/wiki?curid=4874" title="Alcázar de San Juan">
Alcázar de San Juan

Alcázar de San Juan es un municipio y ciudad española ubicada en el noreste de la provincia de Ciudad Real, en la comunidad autónoma de Castilla-La Mancha. Pertenece a la subcomarca de Campo de San Juan y comarca de La Mancha y cuenta con una población de  habitantes (INE ).

Situada en el noreste de la provincia de Ciudad Real, a 150 km de Madrid en dirección sur, limita al norte con la provincia de Toledo. Tiene un extenso término municipal que limita con los municipios de Villafranca de los Caballeros, Quero, Campo de Criptana, Argamasilla de Alba, Manzanares, Llanos del Caudillo y Herencia.

Presenta una orografía fundamentalmente llana, salpicada en la zona norte, más elevada, con algunos cerros de escasa altitud como los de Martín Juan, de Vallejo, de la Horca y de San Antón.

Atraviesan el término municipal los ríos Cigüela, Záncara, Guadiana Alto y Amarguillo, ninguno de los cuales suele fluir durante todo el año. Todos ellos aportan su caudal al río Guadiana. Abarcando buena parte del subsuelo del término municipal, se encuentra una importante reserva de agua subterránea, conocida como Acuífero 23.

Tienen además cierta importancia por su riqueza biológica las lagunas, principal exponente de La Mancha Húmeda, en particular el complejo lagunar de Alcázar de San Juan.
Próximo a la población, está compuesto por tres lagunas, llamadas La Veguilla, Laguna del Camino de Villafranca y Laguna de Las Yeguas. Es una zona de alto valor natural, lo que ha propiciado la adopción de figuras de protección y reconocimiento como: refugio de fauna, zona de especial protección de aves (ZEPA), Reserva de la biosfera de la Mancha Húmeda (UNESCO), Humedales de importancia internacional (RAMSAR). Su acopio permanente de agua las han convertido en un importante refugio de aves en tiempos de sequía, debido a la reducción de otros espacios como el Parque nacional de las Tablas de Daimiel. Durante todo el año pueden encontrarse aves como flamencos y garzas.

Además se encuentran las lagunas de Pajares, de los Carros y del Cerro Mesado, ninguna de las cuales tiene agua durante las estaciones secas.

Por su situación en plena Meseta Sur, el clima de Alcázar de San Juan es mediterráneo continental.

Alcázar de San Juan ocupa el décimo puesto de Castilla-La Mancha en población, y el cuarto en la provincia de Ciudad Real distribuida en tres núcleos de población: la propia ciudad, la entidad de ámbito inferior al municipio (EATIM) de Cinco Casas y la pedanía de Alameda de Cervera.

Se han encontrado en el término municipal hachas pulimentadas, puntas de flecha y cerámicas; se cree que pudo haber algunos asentamientos celtíberos e incluso pudo ser tal vez la antigua Alces, una ciudad prerromana celtíbera conquistada por el pretor romano Sempronio Graco el año 179 a. C. que es nombrada en sus "Anales" por el historiador romano Tito Livio, por más que en el "Itinerario de Antonino", del siglo  d. C., se la designa con el nombre de "Murum"; por otra parte Alces podría ser también la toledana Ocaña (del latín "Alcanea"), o la conquense Tresjuncos; de todas formas, situada junto a la vía o calzada romana que unía Augusta Emerita (Mérida) con Caesaraugusta (Zaragoza) a través de Toletum (Toledo), el lugar se hallaba bien situado para comerciar y es indudable que los romanos se asentaron allí, porque han aparecido mosaicos en 1953 que lo demuestran y se exponen ahora en el museo municipal; Julián San Valero Aparisi los fecha entre fines del siglo y principios del  d. C., mientras que Carmen García Bueno los pospone al siglo usando además criterios numismáticos y las tipologías cerámicas. Es más, las amplias dimensiones de las estancias excavadas hacen suponer que la Alcázar romana fue un importante centro de romanización.

Apenas se conservan restos del periodo visigodo, aunque hay elementos de este origen en el importante y hermoso templo de Santa María la Mayor. La invasión musulmana en el año 711 convirtió a La Mancha en tierra de nadie y el pueblo invasor creó un importante entramado defensivo que denominó en su lengua "Al-kasar", que significa "palacio fortificado", dando nombre a la población (aunque también existen evidencias de otra ciudad musulmana en el paraje de Piédrola).

Tras la desastrosa derrota cristiana en la batalla de Alarcos (1195) hubo una efímera retirada de las tropas castellanas, pero la victoria de las Navas de Tolosa (1212) supuso la cristianización definitiva de la comarca, aunque persistió aún una importante población morisca y judía. Se revitalizó la repoblación para cambiar este estado de cosas: abundan las franquicias y privilegios concedidos por los reyes a quienes deseen instalarse en estas tierras desde otros reinos.

Sancho IV autorizó a fines del XIII al Comendador de Consuegra para que fijara los términos municipales de Alcázar en las tierras de la Orden de San Juan, aunque ya el Gran Comendador había comenzado a reconstruirla; con el privilegio de Sancho IV (que es el pergamino más antiguo que contiene su Archivo Municipal) la población se convierte en Villa, adquiere escudo propio y queda delimitada con un término municipal muy extenso, lindante con tierras de las Órdenes de Santiago y Calatrava. Desde entonces se denominó a la villa bien como "Alcázar de Consuegra" o bien como "Alcázar de San Juan", por la Orden Militar que la protegía. La Orden Militar de los Hospitalarios de San Juan entró a España cuando fue expulsada de la isla de Malta y se estableció en 1189 en La Mancha formando el Gran Priorato de Castilla y León; en el siglo construye el edificio más característico de la villa, el Torreón del Gran Prior o Torreón de Don Juan de Austria. 

En el siglo la villa vivió un gran esplendor; en 1530 contaba 18 480 habitantes y vivían en ella muchos ricos hombres y cortesanos. Las familias Cervantes, Valdivielso y Díaz Morante le dieron lustre; son célebres también los dos pintores Barroso y Sánchez Cotán; los religiosos Juan Cobo y Diego de Torres Rubio evangelizan las Indias orientales y occidentales, uno aprendiendo la lengua china y otro la quechua. El 2 de marzo de 1532 se bendijo el Convento de San Francisco de Asís, de estilo gótico de transición, mandado construir por Diego de Toledo, Prior de la Orden de San Juan y duque de Alba, y en él se funda la franciscana Universidad de Alcázar con cátedras de Medicina, Teología, Historia sagrada y Filosofía, ampliada en el siglo con dos materias más, Gramática y Artes. 

En el reinado de Carlos I se divide la Orden de San Juan en dos grandes prioratos, el de Castilla con sede en Consuegra y el de León con sede en Alcázar, siendo el primer prior de este último Antonio de Zúñiga y el primero del de Castilla Diego de Toledo. El prior de León no residía en Alcázar, sino en la Corte, y para los asuntos de Alcázar era representado por un caballero de San Juan con título de Gobernador y Justicia Mayor, a lo que se agregaban algunos además el de "Lugarteniente del Gran Prior". Es en este siglo cuando se crea en Alcázar la famosa fábrica de pólvora, la más importante del reino y que llegó a emplear hasta 500 hombres en alguna época.

Además un breve pontificio del año 1537 del papa Paolo lll convierte la parroquia de Santa María la Mayor en colegiata de Santa María la Mayor y se crea en ella el Cabildo de San Pedro y San Pablo. En 1546 se levanta para atender la Ermita de la Inmaculada un convento atendido por monjas clarisas que vienen de Toledo, el Convento de Santa Clara; a ellas se atribuye la receta de las famosas tortas de Alcázar. En 1601 doña María de Pedroche dona una casa solariega para fundar un convento nuevo para estas clarisas porque se quedó pequeño el viejo; este convento nuevo es el de San José.

En 1603 se construyó una nueva iglesia en la antigua parroquia de Santa Quiteria que fuera más espaciosa según los planos de Juan de Herrera, constructor del Monasterio de El Escorial, de forma que la iglesia es de estilo herreriano.

En 1619 se abren dos cátedras más en la universidad franciscana: Gramática y Artes. En 1623, Diego de Toledo y Guzmán da orden de crear un corral de comedias en la villa, que participa así del gran esplendor cultural del Siglo de Oro.

En 1625 es bendecido y consagrado el convento de la Santísima Trinidad, dedicado a Nuestra Señora de Gracia, de estilo barroco y regentado por los padres trinitarios; entre 1665 y 1670 estuvo desterrado en el palacio de la Orden de San Juan por razones políticas, el príncipe Juan José de Austria, hijo bastardo de Felipe IV y la actriz María Calderón "La Calderona".

La Orden Hospitalaria pierde su carácter religioso y se convierte en nobiliaria; se fomenta la agricultura construyéndose el canal del Gran Prior y en 1742 se concluye en la Colegiata de Santa María el Camarín de la Virgen del Rosario, de planta cuadrada y estilo barroco, provisto de un zócalo y suelo de cerámica de Talavera.

Empieza este siglo con una gran decadencia; se persiguió a los liberales en el tumulto del 2 de mayo de 1823 y, tras la Década ominosa, la desigual desamortización concentró en pocas manos grandes latifundios y dejó a una gran masa de jornaleros sin la posibilidad de conquistar su independencia económica; además causó una gran destrucción del patrimonio artístico, por ejemplo en el convento de San Francisco, del que solo quedó la iglesia.

Sin embargo, en 1854, tras la llegada del ferrocarril a España, el trazado del ingeniero inglés Mister Creen situó en la villa una estación con un nudo ferroviario de extraordinaria importancia y el 24 de mayo de 1858 la reina Isabel II inauguró la línea Madrid-Alicante que pasaba por la ciudad. El comercio se fortaleció y se abrieron nuevos horizontes para exportar vino y queso manchego a coste más reducido. La fábrica de pólvora, que había reducido paulatinamente su producción, cerró definitivamente en 1868. La revolución de 1868 supuso el cierre del convento de Santa Clara, transformado en un cuartel. En 1877 Alfonso XII concedió a Alcázar el título de ciudad y a fines de siglo se reabren los conventos de Trinitarios, que crean un colegio en 1882, los franciscanos vuelven a Alcázar y abren de nuevo la iglesia monumental de San Francisco de Asís en 1899.

A principios del siglo se vive una mejoría de la economía debido a la influencia del nudo ferroviario establecido en la población. Se crean varias infraestructuras y sociedades alrededor de la misma, como la guarnición militar dependiente de la Academia de Infantería de Toledo en 1906. La llegada de este cuerpo de militares hizo que se precisase un estudio del abastecimiento de agua potable a la población en el que se involucraron muchas personas del pueblo, fundando ese mismo año la Sociedad de Aguas de Alcázar de San Juan. Tras un estudio de la calidad de varios pozos, se eligió la del Pozo de las Perdigueras, por aquel tiempo perteneciente a Miguel Henríquez de Luna, quien en 1907 accedió a vender el terreno necesario donde excavar un pozo capaz de abastecer al municipio.

En 1929 se derriba el antiguo Ayuntamiento, construido en 1622, y el edificio del antiguo Casino pasa a ser Casa Consistorial. Oliverio Martínez y Mier,el notario y diputado a cortes por Alcázar de San Juan contrata los servicios del arquitecto modernista Críspulo Moro Cabeza para la remodelación del edificio del casino y realizar un plan de reforma y modernización de las plazas del centro del pueblo, así como la construcción de una plaza de toros municipal.

En 1936, la guerra civil española da al traste con el incipiente progreso económico y social. Alcázar queda en territorio republicano y en octubre de 1936 se crea en Alcázar la 3.ª Brigada Mixta del Ejército Popular de la República, un cuerpo de carabineros que participó en casi todas las principales batallas que tuvieron lugar durante la contienda. El bando sublevado bombardea en varias ocasiones la ciudad y causa graves destrozos, debido a su especial importancia como nudo ferroviario y paso obligado para acceder al sur peninsular. 

Tras la guerra, Alcázar de San Juan, sufre la miseria de la posguerra como el resto de España. El abandono de la autarquía y el Plan de Estabilización de 1959 y los sucesivos Planes de desarrollo dan lugar a la aparición y crecimiento de un polígono industrial. Se asientan algunas empresas relacionadas con el nudo ferroviario y del sector servicios, convirtiendo a Alcázar en la cabeza de comarca de poblaciones cercanas, y recibiendo incrementos sucesivos de su población. Al finalizar el siglo , Alcázar de San Juan cuenta con 28 000 habitantes, siendo una de las ciudades más populosas de la provincia de Ciudad Real.

En 1994 se construye el Hospital Mancha-Centro, de gran importancia dentro del servicio de salud de la comunidad autónoma.

En noviembre de 2017 se inauguró el nuevo pabellón deportivo multiusos Vicente Paniagua.

Aunque enclavada en una región eminentemente agrícola y ganadera, la actividad industrial y comercial ha sido y es la base principal de la economía de Alcázar, atrayendo parte de la riqueza generada en su comarca.

Después del declive y extinción de la actividad de fabricación de pólvora, la economía local estuvo sostenida durante casi siglo y medio por el ferrocarril, que permitió la creación de grandes bodegas productoras de vino a granel y derivados, que utilizaban este medio de transporte para su comercialización, así como el establecimiento de fábricas de material ferroviario y la presencia de una importante plantilla de personal vinculado a la empresa ferroviaria, primero M.Z.A. y luego Renfe. Todo ello debido a su condición de importante nudo ferroviario, que comunica Madrid con el Levante español y con Andalucía.

El declive del transporte de mercancías por ferrocarril, el cierre de las fábricas de material ferroviario y la centralización de las tareas de mantenimiento y control del tráfico ferroviario, han disminuido considerablemente la importancia de este sector, paliado en parte por el establecimiento de industrias manufactureras y de transformación en uno de los primeros Polígonos de Descongestión Industrial de Madrid, situado al norte de la ciudad.

Por otra parte, la transformación de la explotación ferroviaria de viajeros, tendente a la minimización de los tiempos de parada y los transbordos y a la eliminación de los tráficos nocturnos, así como la creación de las nuevas líneas de Alta Velocidad Madrid-Andalucía y Madrid-Levante, que no pasan por Alcázar, han supuesto una merma considerable del tráfico de trenes y de la actividad hostelera en el entorno de la estación.

Con la definitiva extensión de estas líneas, los tráficos quedarán limitados en un futuro a circulaciones de media distancia, que está previsto mejorar con la construcción de la línea de Alta Velocidad Madrid-Alcázar de San Juan-Jaén, que seguirá utilizando la estación de la localidad.

A partir de la última década del siglo , Alcázar de San Juan se consolida como una ciudad de servicios, centro de atracción comercial de los pueblos cercanos, en buena parte debido a la puesta en marcha del Complejo Hospitalario Mancha-Centro, su mayor empresa.

Con los primeros años del siglo surgen nuevas oportunidades, como la producción de energía eléctrica por medio de plantas fotovoltaicas y centrales termosolares. Actualmente se encuentran en explotación cuatro centrales termosolares de 50 MW de potencia y se ha concedido la autorización administrativa para la construcción de otras cuatro, todas ellas en la zona sur, en el entorno de la carretera CM-3113, que comunica Villarta de San Juan y Cinco Casas.

En lo que al sector primario se refiere, Alcázar de San Juan posee una importante extensión de terreno cultivable, dedicada en su mayor parte a cultivos de secano: cereal (trigo y cebada), olivar, melón y viñedo. Este último se ha ido trasladando en parte a terrenos de regadío, en los que la productividad de la planta es más elevada. La mayoría de la producción, cada vez más rica en varietales, se comercializa como mosto y vino a granel, así como vino embotellado bajo la Denominación de Origen La Mancha, cuyo consejo regulador se encuentra en esta ciudad. Como en otros pueblos de la zona, la mayor parte de la producción agraria se gestiona en cooperativas agrícolas.

Actualmente, la cerámica, el cuero y la madera forman parte de la artesanía local.

Su gastronomía se corresponde con la típica manchega: , asado, , queso, ensalada de limón, pisto manchego, , , y 
, sin olvidar el zurra: bebida a base de vino, azúcar, agua y trozos de fruta. Destacan también las Tortas de Alcázar, unas tortas dulces redondas y muy esponjosas, con un recubrimiento de una concha blanca y crujiente.

Las fiestas locales se celebran del 2 al 8 de septiembre, justo antes de empezar la recogida de la uva; en las fiestas locales se dan cita multitud de peñas y grupos culturales locales para hacer gastronomía típica, cada día se hace una comida distinta. 

La festividad de la patrona de la localidad, la Virgen del Rosario Coronada se celebra el primer domingo del mes de octubre. 

Alcázar de San Juan celebra sus carnavales coincidiendo con las fiestas navideñas, entre el 26 y el 28 de diciembre. Unos carnavales que, por la singularidad de su fecha de celebración, los últimos del año, fueron declarados en 2018 Fiesta de Interés Turístico Nacional.

También se celebra el día 15 de mayo la romería de San Isidro Labrador.

En junio, coincidiendo con la festividad de San Juan Bautista, tienen lugar los festejos de Moros y Cristianos, declarados de Interés Turístico Regional. 

Otras fiestas de gran relevancia para la ciudad son San Antón el 17 de enero, en la placeta Santa María, y San Sebastián el 20 de enero; en dichas fiestas se hacen hogueras populares a lo largo de todo el pueblo donde se reúnen amigos y vecinos para festejar a los santos, haciendo las típicas y artesanales tortas en sartén, que según tradición se toman en estos días, mojándolas en chocolate o solas.

Existen algunos estudios que consideran a Alcázar de San Juan el lugar de nacimiento de Miguel de Cervantes Saavedra (1547-1616). Destacan por ejemplo los trabajos de Ángel Ligero Móstoles, Francisco Saludador Merino, Rafael Mazuecos Pérez-Pastor, Manuel Rubio Herguido o Bruno Redondo. Esta "tradición" cervantina se basa en la partida de bautismo encontrada en la Iglesia Parroquial de Santa María de un hijo de Blas Cervantes Saavedra y Catalina López, por nombre Miguel. En 1748, Blas Nasarre, bibliotecario mayor del reino y cervantista, escribió al margen de dicha partida "Este fue el autor de la Historia de don Quixote":

Este Miguel de Cervantes Saavedra alcazareño difiere en edad del Miguel de Cervantes nacido en Alcalá de Henares, pues aquel sería once años menor. Esto tiene implicaciones en los diferentes episodios de la vida de Cervantes, ya que impide su servicio como camarero del Cardenal Acquaviva, su participación en la batalla de Lepanto, o su cautiverio en Argel. Por otro lado, llama la atención la aparición del apellido "Saavedra" en la partida alcazareña, totalmente ausente en la hallada en Alcalá, y que el Cervantes alcalaíno no usaría hasta después de 1585. No obstante, era algo común en la época, la alteración y cambio de apellidos, como puede comprobarse en diversos coetáneos del autor, como Vicente Espinel, por ejemplo.

A pesar de la existencia de estos trabajos que indican que Cervantes pudo nacer en Alcázar de San Juan, la crítica cervantina considera, con el actual conocimiento de las fuentes que se posee, que el verdadero Miguel de Cervantes es el nacido en Alcalá de Henares, ya que así lo afirmó el mismo autor del "Quijote" en la "Declaración de Argel" de 1580. Esto no evita que siga viva la tradición cervantina en Alcázar, basada también en la existencia hasta hace pocos años de la llamada "Casa de Cervantes", (que fue derribada por su mal estado y donde hoy se alza una nueva construcción).

Por otro lado, los estudios de Ángel Ligero Móstoles, basados en los personajes no solo del Quijote, sino de toda la obra cervantina, sugieren que el famoso "lugar de La Mancha" era Alcázar de San Juan. Han surgido sin embargo otras teorías que sitúan dicho lugar en otras localidades de Castilla-La Mancha, como por ejemplo Argamasilla de Alba, Villanueva de los Infantes, Argamasilla de Calatrava, La Puebla de Almoradiel o es una combinación de las características de más de un "lugar" (una mixtificación).

Es tal el arraigo de esta creencia que, durante la guerra civil española, se cambió el nombre de la localidad por el de "Alcázar de Cervantes".

Alcázar de San Juan siempre ha tenido una gran afición por el baloncesto, gracias a Antonio Díaz-Miguel. 

El equipo de la localidad es el Grupo 76 Al-kasar, fundado en 1976, equipo con una gran cantera dentro de la comunidad castellano-manchega. Actualmente se encuentra disputando la 1ª División Nacional, tras varios años en la división autonómica.

El equipo profesional de la localidad es el ADEPAL (Fundación Amistad y Deporte Alcázar), fundado en 2006 disputó la 1ª División Nacional, llegando a jugar en la liga Adecco Oro.; y hoy ha desaparecido por problemas económicos.

El 22 de mayo, Seguros Soliss Alcázar Basket conseguía subir a LEB Plata tras quedar primero en su grupo en la fase de ascenso.También desaparecido en verano de 2018.





</doc>
<doc id="4877" url="https://es.wikipedia.org/wiki?curid=4877" title="Domótica">
Domótica

Se llama domótica a los sistemas capaces de automatizar una vivienda o edificación de cualquier tipo, aportando servicios de gestión energética, seguridad, bienestar y comunicación, y que pueden estar integrados por medio de redes interiores y exteriores de comunicación, cableadas o inalámbricas, y cuyo control goza de cierta ubicuidad, desde dentro y fuera del hogar. Se podría definir como la "integración de la tecnología en el diseño inteligente de un recinto cerrado".

El término "domótica" viene de la unión de las palabras "domus" (que significa "casa" en latín) y "autónomo" (del griego: αὐτόνομος; “que se gobierna a sí mismo”).

Los servicios que ofrece la domótica se pueden agrupar según cinco aspectos o ámbitos principales:

El ahorro energético no es algo tangible, sino legible con un concepto al que se puede llegar de muchas maneras. En muchos casos no es necesario sustituir los aparatos o sistemas del hogar por otros que consuman menos energía sino una "gestión eficiente" de los mismos.


El confort conlleva todas las actuaciones que se puedan llevar a cabo que mejoren la comodidad en una vivienda. Dichas actuaciones pueden ser de carácter tanto pasivo, como activo o mixtas.

Consiste en una red de seguridad encargada de proteger tanto los bienes patrimoniales, como la seguridad personal y la vida.

A modo de ejemplo, un detector de humo colocado en una cocina eléctrica, podría apagarla, cortando la electricidad que va a la misma, cuando se detecte un incendio.

Son los sistemas o infraestructuras de comunicaciones que posee el hogar.

Bajo este mecanismo se incluyen las aplicaciones o instalaciones de control remoto del entorno que favorecen la autonomía personal de personas con limitaciones funcionales, o discapacidad.

El concepto diseño para todos es un movimiento que pretende crear la sensibilidad necesaria para que al diseñar un producto o servicio se tengan en cuenta las necesidades de todos los posibles usuarios, incluyendo las personas con diferentes capacidades o discapacidades, es decir, favorecer un diseño accesible para la diversidad humana. La inclusión social y la igualdad son términos o conceptos más generalistas y filosóficos. La domótica aplicada a favorecer la accesibilidad es un reto ético y creativo pero sobre todo es la aplicación de la tecnología en el campo más necesario, para suplir limitaciones funcionales de las personas, incluyendo las personas discapacitadas o mayores. El objetivo no es que las personas con discapacidad puedan acceder a estas tecnologías, porque las tecnologías en si no son un objetivo, sino un medio. El objetivo de estas tecnologías es favorecer la autonomía personal. Los destinatarios de estas tecnologías son todas las personas, independientemente de su condición de enfermedad, discapacidad o envejecimiento.

Un sistema domótico orientado hacia el uso de personas con discapacidad incluye:

Desde el punto de vista de donde reside la inteligencia del sistema domótico, hay varias arquitecturas diferentes:




Existe un número de protocolos a seguir dependiendo de la actividad que se lleve a cabo:


Los que tienen mayor presencia en el Mercado son X10 y KNX.

Existen diferentes tipos de organizaciones especializadas en esta materia:







En Chile existen empresas que realizan trabajos de domótica, y varias de estas, se dedican al tema en forma exclusiva y completa. Dentro de los proyectos destacables de domótica en Chile podemos mencionar la automatización de las estaciones de las Líneas 4, 4A y 6 del Metro de Santiago, Aeropuerto de la Araucanía, y varios edificios de oficinas.

En España la domótica tiene presencia mediante multitud de empresas. Algunas de ellas fabrican equipamiento homologado de acuerdo a los estándares internacionales, mientras que otras se dedican a la implantación de estos sistemas desde hace más de 14 años. Muestra de la gran actividad en este país es el hecho de que es el segundo a nivel mundial con mayor número de KNX Partners, tan solo por detrás de Alemania. Cada dos años, empresas españolas participan en el concurso internacional KNX Awards, llegando a conseguirlo en varias ocasiones.

Existen diversas asociaciones, entidades públicas y agrupaciones empresariales sin ánimo de lucro cuyo principal objetivo es la implantación y la innovación de las empresas españolas en el ámbito de la domótica.

En Argentina la domótica surge de la mano de empresas de tecnología que incorporan el concepto y lo desarrollan. A comienzo de la década de 1990, estas empresas comienzan a hablar de domótica al referirse a la casa del futuro, y a realizar algunas aplicaciones de carácter parcial, participando en ferias y notas periodísticas que colaboran con la difusión del nuevo concepto. Conforme avanzan los años 90, las instalaciones se hacen más frecuentes e importantes comenzando a expandirse el mercado argentino, lo cual posibilita, llegado el fin del milenio, la aparición de otras compañías que comienzan a incorporarlo entre sus servicios o realizan desarrollos propios.
La crisis económica Argentina de fines del 2001 paraliza este desarrollo que recién se recupera con la expansión que se da en el área de la construcción casi tres años después.
En el año 2007 se realiza la primera expo exclusiva de domótica "expo casa domótica" y primer congreso de domótica.
En la provincia de Córdoba se formó una comisión de ingenieros especialistas que elaboró una Guía de Contenidos Mínimos para la elaboración de un Proyecto de Domótica. Dicha guía sirve como referencia y está disponible para cualquier persona que tenga interés en la actividad y como informativo del estado del arte. La Comisión de Domótica del CIEC nuclea a los profesionales de ésta materia en la provincia de Córdoba y vela por la calidad de los servicios que se prestan.

Existen múltiples centros privados y universidades que imparten una formación de postgrado y homologada (máster).
Además, existen centros de formación homologados por KNX association para la obtención de la certificación Partner KNX.

Por otro lado, la titulación oficial de Técnico en Instalaciones de telecomunicaciones, incluye entre sus funciones las de instalación y mantenimiento de instalador-mantenedor de sistemas domóticos.



</doc>
<doc id="4886" url="https://es.wikipedia.org/wiki?curid=4886" title="Tesauro">
Tesauro

Un tesauro es una lista de palabras o términos controlados, empleados para representar conceptos.

Proviene del latín "thesaurus" (‘tesoro’), y este a su vez del griego clásico "thēsaurós" (θησαυρός, ‘almacén’, ‘tesorería’). Es utilizado en literatura como thesaurus, thesauri o tesoro para referirse a los diccionarios, como por ejemplo el
Tesoro de la lengua castellana o española de Sebastián de Covarrubias, de 1611.

Adquiere al menos dos significados relacionados, en el campo de la literatura y en el campo de la biblioteconomía o ciencias de la información.

En cuanto a lo jurídico, son utilizadas bases de datos los cuales se enfocan en construir sistemas de búsqueda de ordenar contenidos y generar instrumentos lingüísticos,todo esto en relación y apoyo directo por thesaurus.
El mismo, se conceptualizaria como un sistema con el cual se busca la clasificación y ordenación de textos y documentos, siguiendo parámetros de jerarquía, inclusión, orden. En cuanto esto, el thesaurus opera en tres niveles:




Tesauro es una lista de palabras con significados similares sinónimos, habitualmente acompañada por otra lista de antónimos. Un ejemplo sería un tesauro dedicado a un campo especializado, que contiene la jerga que se emplea en dicho campo del conocimiento. En el mundo de habla inglesa, es clásico el "Tesauro de Roget" (publicado en 1852), cuya función es, según su autor, además de ayudar al escritor a encontrar la palabra que exprese mejor su pensamiento, también estimular su intelecto y sugerirle palabras o ideas relacionadas.

El conocimiento se organiza con miras a efectuar una normalización terminológica que permita mejorar el canal de acceso y comunicación entre los usuarios y las Unidades de Información (entiéndase "unidad de información" como: biblioteca, archivo o centro de documentación). El primer tesauro del idioma inglés, publicado en 1852, fue compilado por Peter Mark Roget.
El Tesauro posee un signo de representación lingüístico, un vocabulario controlado y una sintaxis poscoordinada.
Aunque en la práctica tradicional se habla de "unitérminos", en la actualidad se ha efectuado grandes variaciones dando incorporación a términos o descriptores compuestos, es decir, descriptores que se componen de dos o más palabras.

Se refiere a un léxico de términos ordenados alfabéticamente que comprende el vocabulario especializado de una disciplina académica o campo de estudio, que muestra las relaciones lógicas y semánticas entre los términos, particularmente una lista de encabezados de materias o descriptores utilizados como términos preferidos para indexar la literatura del campo.

Los términos que conforman el tesauro se interrelacionan entre ellos bajo tres modalidades de relación:


Es un intermediario entre el lenguaje que encontramos en los documentos (lenguaje natural) y el que emplean los especialistas de un determinado campo del saber (lenguaje controlado). Aunque los incluye, las entradas de un tesauro no deben ser consideradas sólo como una lista de sinónimos.

En líneas generales, un tesauro comprende lo siguiente:

Se puede establecer una profundidad de la jerarquía, indicado por niveles de importancia de los niveles preferidos. A mayor profundidad, mayor especificidad.

Un "descriptor" es cada uno de los términos o expresiones escogidos entre un conjunto de sinónimos o cuasi sinónimos para representar (en calidad de término preferente) generalmente de manera unívoca, un concepto susceptible de aparecer con cierta frecuencia en los documentos indizables, y en las consultas que se realicen. El descriptor corresponde normalmente a la etiqueta de un concepto, y es la unidad mínima de significado que integra un tesauro o una lista de descriptores. Suele acompañarse de una nota de alcance o, menos usualmente, de una definición en los casos que el mero registro del término puede provocar problemas de ambigüedad en su interpretación. El descriptor es el término por el cual efectivamente se indizará (por eso se llama también término de indización), y por el cual se recuperarán los documentos referidos a su temática.

Se reproduce aquí parte de la sección alfabética de un tesauro impreso. Nótese que los términos preferidos están en mayúscula (en este caso, la palabra «AUTOMÓVIL»):

En la sección temática del tesauro, encontraremos agrupados los temas, habitualmente de lo más general a lo más específico:

Se debe tener en cuenta que los tesauros no tienen por qué ser completos, en el sentido de abarcar todo el conocimiento. Generalmente se limitan a un área temática específica, y desde un cierto ángulo en particular. Es así que existen innumerables tesauros específicos. Como ejemplo de estos, podemos citar el "Tesauro de la Propiedad Industrial" (en español), el MeSH o encabezamientos temáticos médicos (en lengua inglesa), y a Agrovoc (tesauro multilingüe de términos de agricultura).

En rigor de verdad, no es la función de un tesauro definir las palabras (esta función es propia del diccionario o el glosario). Sin embargo, muchos tesauros contienen definiciones de los términos que emplean, con el fin de que el usuario del tesauro no tenga dudas al seleccionar un término.

Se debe tener en cuenta que ha habido un gran cambio al aparecer la posibilidad de tesauros digitales, porque las jerarquías y las relaciones hacen que muchas limitaciones de los tesauros impresos hayan desaparecido. Por ejemplo, es posible buscar casi instantáneamente los términos de un tesauro usando un motor de búsqueda, o simplemente recorrer las jerarquías en línea, o también usar la función de «mapeo» al término preferido en forma automática, introduciendo el usuario el término en lenguaje natural, siendo el programa informático el que relaciona este término con la lista de términos preferidos, y realiza posteriormente la búsqueda en la base de datos empleando los términos preferidos.






"


</doc>
<doc id="4887" url="https://es.wikipedia.org/wiki?curid=4887" title="Diccionario">
Diccionario

Un diccionario es una obra donde se puede consultar palabras o términos y se proporciona su significado, definición, etimología, ortografía, fija su pronunciación, separación silábica y forma gramatical. La información que proporciona varía según el tipo de diccionario del que se trate.

En muchos casos los diccionarios proporcionan el significado de las palabras, su etimología, su escritura, sinónimos y antónimos.

La disciplina que se encarga, entre otras tareas, de elaborar diccionarios es la Lexicografía. Se encuentran por lo general en la forma de un libro impreso, pero también hay diccionarios electrónicos.

Se considera que los primeros diccionarios aparecieron en Mesopotamia. Esta afirmación parte del descubrimiento de varios textos cuneiformes en la Biblioteca de Asurbanipal, en Nínive, que relacionaban palabras sumerias.

Existen varios tipos de diccionarios, según su función y su uso:

En ellos se recogen términos que se consideran correctos según la norma. Para la lengua española, el referente es el "Diccionario de la lengua española" ("DLE"), de la Real Academia Española, elaborado conjuntamente por las veintitrés Academias de la Asociación de Academias de la Lengua Española. El "Diccionario del estudiante" es la obra de referencia para los estudiantes de secundaria y bachillerato.

Recogen acepciones en las palabras que no son reconocidas por el órgano competente (como la Real Academia Española) pero que, sin embargo, se usan ampliamente en la sociedad. Es el caso, por ejemplo, del "Diccionario de uso del español" ("DUE"), de María Moliner; del "Diccionario Clave", de Concepción Maldonado; y del "Diccionario de uso del español actual" ("DEA"), de Manuel Seco, Olimpia Andrés y Gabino Ramos.

En ellos se explica brevemente el significado de las palabras de una lengua determinada. Estos diccionarios no contienen, a diferencia de los bilingües, definiciones que incluyen equivalentes en otras lenguas.

Diccionarios que consisten en traducir una palabra de un idioma a otro, por ejemplo, del español al inglés y viceversa. Generalmente se usan cuando se estudia un idioma diferente al idioma materno o cuando se busca una palabra que se escribe o habla en otro idioma y que no se conoce en el idioma materno.

Son diccionarios elaborados para estudiantes nativos o extranjeros. En ellos se ofrecen definiciones más sencillas que en diccionarios concebidos para el público general y se aporta mayor información sintagmática y paradigmática en los artículos. El número de ejemplos proporcionado por cada lema también es mayor. Un ejemplo de diccionario de aprendizaje para hablantes nativos del español es el "Diccionario del estudiante", ya mencionado. Algunos diccionarios de aprendizaje para estudiantes de español como lengua extranjera son el "Diccionario para la enseñanza de la lengua española", de VOX-Universidad de Alcalá ("DIPELE"); el "Diccionario Salamanca de la lengua española"", de Santillana; el "Diccionario de español para extranjeros", de SM; y el "Diccionario para estudiantes de español", de Espasa.

Son los diccionarios en los que se facilita información sobre el origen de las palabras de una determinada lengua. Quizá el diccionario etimológico más prestigioso de la lengua inglesa es el "Oxford English Dictionary". Quizá el diccionario etimológico más célebre (aunque ya no es el más actualizado) de la lengua española es el "Tesoro de la lengua castellana o española" (1611), obra de Sebastián de Covarrubias y Orozco, que no es solo diccionario etimológico, sino que aporta muchísimos datos históricos de la lengua utilizada en su época.

En estos diccionarios se relacionan palabras de significado similar y opuesto, para facilitar la elección de estas al redactar textos. Los más sencillos se limitan a dar una lista de palabras para cada entrada, pero algunos más complejos indican además las diferencias de matiz con la palabra buscada, sin llegar a ser un tesauro (véase más adelante); no todas las palabras tienen antónimos. En algunos casos, los diccionarios de sinónimos y antónimos también incluyen parónimos.

Se trata de diccionarios que están dedicados a palabras o términos que pertenecen a un campo o técnica determinados como, por ejemplo, la informática, la jardinería, la ingeniería, la computación, la genética, la heráldica, la gastronomía, el lenguaje SMS, los pesos y medidas, las abreviaturas, etc. Proporcionan breve información sobre el significado de tales palabras o términos. Pueden ser también diccionarios de idiomas en los que se indica la traducción a otra lengua o a otras lenguas de las palabras o términos que incluyen.

Son diccionarios de la lengua con la particularidad de que están ordenados alfabéticamente según las últimas letras de cada palabra, en vez de las primeras. Su uso principal es buscar palabras que rimen con otra(s), para la redacción de poemas y versos. Algunos diccionarios inversos reducidos no incluyen definiciones, sino solo la lista de palabras ordenadas de esta forma.

En estos diccionarios no se ordenan palabras, sino estructuras gramaticales. Su uso principal es para personas que están aprendiendo un idioma extranjero, ya que les permite buscar estructuras gramaticales de un texto y consultar en ellos su significado y construcción.

Recogen palabras y frases cuyo significado se ha desvirtuado y no significan en la sociedad lo que un diccionario de la lengua indica. Estos diccionarios ayudan a un redactor o escritor a usar los términos correctos, sin dejarse llevar por el significado popular. A diferencia del diccionario de uso práctico anterior, su objetivo no es dar a conocer el uso vulgar de una palabra, sino advertir de este, y proponer alternativas adecuadas para fines específicos.

Los tesauros son obras en las que se relacionan numerosas palabras que guardan una relación más o menos directa con la palabra u objeto de consulta. No son, pues, diccionarios de sinónimos, ya que estos últimos incluyen únicamente palabras con un significado similar y equivalente.

Se localizan las palabras según su asociación a una idea. Se parte de ideas generales y se va concretando hasta llegar a una lista de palabras entre las que se encontrará la buscada. Se diferencia del tesauro en que en aquel las palabras se relacionan con palabras con alguna relación, mientras que en este las palabras se agrupan con ideas. Por ejemplo, para localizar el nombre de un cierto color verde que no se recuerda se busca en el grupo "naturaleza"; dentro de este, en el grupo "luz"; dentro de este, en el grupo "color", luego en el grupo "verde" y ahí, entre otros, se encuentra "glauco", un tono específico de verde.

Es una especie de tesauro. Sus características hacen que se presenten en formato electrónico (DVD o página web). Se llama "conceptual" porque el acceso se realiza por medio de conceptos, no solo por medio de palabras. Por ejemplo, "demasiado cansada para" es un concepto multipalabra. Esta característica hace que la accesibilidad sea fácil para el usuario común.

En un diccionario visual, se utilizan principalmente imágenes para ilustrar el significado de las palabras. Los diccionarios visuales pueden organizarse por temas o por lista alfabética de las palabras. Para cada tema, una imagen se etiqueta con la palabra correcta, con objeto de identificar cada componente del tema en cuestión.

Diccionario que contiene información más específica y detallada y abarca temas mucho más amplios como, por ejemplo, acerca de países, continentes, océanos, personas famosas; también puede mencionar cómo se escribe cierta palabra en otros idiomas. No debe confundirse un diccionario enciclopédico con una enciclopedia. Como se ha dicho, el primero facilita una información breve sobre el significado de una palabra. Por el contrario, la persona que consulta una enciclopedia espera encontrar una amplia información acerca de un concepto o tema, a fin de conocer con suficiente detalle todo lo relativo a este. Wikipedia es ejemplo de un tipo específico de enciclopedia: la enciclopedia en línea que pueden modificar los propios usuarios.

Los diccionarios son tradicionalmente libros. Sin embargo, también existen diccionarios en soporte digital, como CD y DVD, y se pueden consultar algunos en Internet. También se han popularizado los diccionarios electrónicos portátiles, como una aplicación dentro de un teléfono o consistentes en un pequeño dispositivo independiente con pantalla y teclado, que suele contener varios diccionarios en su interior.

Los artículos del diccionario tienen las siguientes partes:

En un diccionario:




</doc>
<doc id="4888" url="https://es.wikipedia.org/wiki?curid=4888" title="Disco compacto">
Disco compacto

El disco compacto (conocido popularmente como CD por las siglas en inglés de "Compact Disc") es un disco óptico utilizado para almacenar datos en formato digital, consistentes en cualquier tipo de información (audio, imágenes, vídeo, documentos y otros datos). 

Tienen un diámetro de 12 centímetros, un espesor de 1,2 milímetros y pueden almacenar hasta 80 minutos de audio o 700 MB de datos. Los Mini-CD tienen 8 cm y son usados para la distribución de sencillos y de controladores guardando hasta 24 minutos de audio o 210 MB de datos.

Esta tecnología fue inicialmente utilizada para el CD audio, y más tarde fue expandida y adaptada para el almacenamiento de datos (CD-ROM), de video (VCD y SVCD), la grabación doméstica (CD-R y CD-RW) y el almacenamiento de datos mixtos (CD-i, Photo CD y CD EXTRA).

El disco compacto goza de popularidad en el mundo actual, especialmente en Asia, donde su éxito persiste.En el año 2007 se habían vendido 200 mil millones de CD en el mundo desde su creación. Aun así, los discos compactos se complementan con otros tipos de distribución digital y almacenamiento, como las memorias USB, las tarjetas SD, los discos duros, el almacenamiento en la nube y las unidades de estado sólido. Desde su pico en el año 2000, las ventas de CD han disminuido alrededor de un 50%.

El disco compacto es una evolución natural del LaserDisc. Los prototipos fueron desarrollados por Philips y Sony, primero de manera independiente y posteriormente de manera conjunta. Fue presentado en junio de 1980 a la industria, y se adhirieron al nuevo producto 40 compañías de todo el mundo mediante la obtención de las licencias correspondientes para la producción de reproductores y discos.

En 1974, Lou Ottens, director del grupo de la industria de audio dentro de la Corporación Tecnológica de Phillips tuvo la iniciativa de formar un grupo de proyecto de siete personas para desarrollar un disco de audio óptico con un diámetro de 20 cm con una calidad de sonido superior a la de los discos de vinilo grandes y frágiles. En marzo de 1974, durante una reunión del grupo de audio, dos ingenieros del laboratorio de investigación de Philips recomendaron el uso de un formato digital en el disco óptico de 20 cm, ya que se podría añadir un código de corrección de errores. No fue sino hasta 1977 que los directores del grupo decidieron establecer un laboratorio con la misión de crear un pequeño disco de audio digital óptico y un pequeño reproductor. Se eligió el término "disco compacto", en consonancia con otro producto de Philips, el casete compacto. En lugar de los 20 cm de tamaño original, el diámetro de este disco compacto se fijó en 11,5 cm, que es el tamaño de la diagonal de un casete compacto.

Mientras tanto, Sony Corporation mostró por primera vez públicamente un disco de audio digital óptico en septiembre de 1976. En septiembre de 1978, la compañía mostró un disco de audio digital óptico con un tiempo de 150 minutos de reproducción, grabado con una velocidad de muestreo de la señales de audio de 44.056 Hz, resolución lineal de 16 bits y código de corrección de errores de cruz-entrelazado, especificaciones similares a las que más tarde se establecieron en el formato estándar del Compact Disc en 1980.

Más tarde, en 1979, Sony y Philips crearon un grupo de trabajo conjunto de ingenieros para diseñar un nuevo disco de audio digital. Liderados por Kees Schouhamer Immink y Toshitada Doi, la investigación impulsó la tecnología del láser y el disco óptico que se inició de forma independiente por las dos empresas. Después de un año de experimentación y discusión, el grupo de trabajo produjo el Libro Rojo de estándar CD-DA. Publicado por primera vez en 1980, la norma fue adoptada formalmente por la Comisión Electrotécnica Internacional como estándar internacional en 1987, con varias enmiendas que comenzaron a formar parte de la norma en 1996.

Philips contribuyó al proceso de manufactura general, basado en la tecnología del LaserDisc para video. Philips también contribuyó con el sistema de modulación Eight-to-Fourteen (EFM), que ofrece una cierta resistencia a defectos tales como rasguños y huellas dactilares, mientras que Sony contribuyó con el método de corrección de errores CIRC.

La Historia de Compact Disc, contada por un exmiembro del grupo de trabajo, entrega antecedentes sobre las muchas decisiones técnicas, incluida la elección de la frecuencia de muestreo, tiempo de reproducción, y el diámetro del disco. El grupo de trabajo estuvo formado por alrededor de cuatro a ocho personas, aunque según Philips, el disco compacto fue "inventado colectivamente por un grupo grande de personas que trabajan como un equipo".

En 1981, el director de orquesta Herbert von Karajan convencido del valor de los discos compactos, los promovió durante el Festival de Salzburgo, y desde ese momento empezó su éxito. Los primeros títulos grabados en discos compactos en Europa fueron la Sinfonía Alpina de Richard Strauss, los valses de Frédéric Chopin interpretados por el pianista chileno Claudio Arrau, y el álbum "The Visitors" de ABBA, en 1983 se produciría el primer disco compacto en los Estados Unidos por CBS (hoy Sony Music) siendo el primer título en el mercado un álbum de Billy Joel. La producción de discos compactos se concentró por varios años en los Estados Unidos y Alemania, de donde eran distribuidos a todo el mundo.

Fue en octubre de 1982 cuando Sony y Philips comenzaron a comercializar el CD.

En el año 1984 salieron al mundo de la informática, permitiendo almacenar hasta 650 MB (74 min. de CD-A) y, a finales de los 90', hasta 700 MB (80 min. de CD-A).

A pesar de que puede haber variaciones en la composición de los materiales empleados en la fabricación de los discos, todos siguen un mismo patrón: los discos compactos se hacen de un disco grueso, de 1,2 mm, de policarbonato de plástico, al que se le añade una capa reflectante de aluminio, utilizada para obtener más longevidad de los datos. Así se reflejará la luz del láser (en el rango de espectro infrarrojo, y por tanto no apreciable visualmente); posteriormente se le añade una capa protectora de laca, que actúa como protector del aluminio y, opcionalmente, una etiqueta en la parte superior. Los métodos comunes de impresión en los CD son la serigrafía y la impresión ófset.
En el caso de los CD-R y CD-RW se usa oro, plata, y aleaciones de las mismas, que por su ductilidad permite a los láseres grabar sobre ella, cosa que no se podría hacer sobre el aluminio con láseres de baja potencia.


Un CD de audio se reproduce a una velocidad tal que se leen 150 KB por segundo. Esta velocidad base se usa como referencia para identificar otros lectores como los de ordenador, de modo que si un lector indica 24x, significa que lee 24 x 150 kB = 3.600 kB/s, aunque se ha de considerar que los lectores con indicación de velocidad superior a 4x no funcionan con velocidad angular variable como los lectores de CD-DA, sino que emplean velocidad de giro constante, siendo el radio obtenible por la fórmula anterior el máximo alcanzable.

Una vez resuelto el problema de almacenar los datos, queda el de interpretarlos de forma correcta. Para ello, las empresas creadoras del disco compacto definieron una serie de estándares, cada uno de los cuales reflejaba un nivel distinto. Cada documento fue encuadernado en un color diferente, dando nombre a cada uno de los «libros arcoíris» (Rainbow Books).

Para describir la calidad de un CD-ROM este es probablemente uno de los parámetros más interesantes. El tiempo de acceso se toma como la cantidad de tiempo que le lleva al dispositivo desde que comienza el proceso de lectura hasta que los datos comienzan a ser leídos. Este parámetro viene dado por: la latencia, el
tiempo de búsqueda y el tiempo de cambio de velocidad (en los dispositivos CLV). Téngase en cuenta que el
movimiento de búsqueda del cabezal y la aceleración del disco se realizan al mismo tiempo, por lo tanto no
estamos hablando de sumar estos componentes para obtener el tiempo de acceso sino de procesos que
justifican esta medida.

Este parámetro depende directamente de la velocidad de la unidad de CD-ROM ya que los
componentes de este también dependen de ella. La razón por la que el tiempo de acceso es mayor en los
CD-rom respecto a los discos duros es la construcción de estos. La disposición de cilindros de los discos
duros reduce considerablemente los tiempos de búsqueda. Por su parte los CD-ROM no fueron inicialmente
ideados para el acceso aleatorio sino para acceso secuencial de los CD de audio. Los datos se disponen en
espiral en la superficie del disco y el tiempo de búsqueda es por lo tanto mucho mayor.

Una cuestión a tener en cuenta es el reclamo utilizado en muchas ocasiones por los fabricantes, es decir, si las
tasas de acceso más rápidas se encuentran en los 100 ms (150 ms es un tiempo de acceso típico) intentarán
convencernos de que un CD-ROM cuya velocidad de acceso es de 90 ms es infinitamente mejor cuando la
realidad es que la diferencia es en la práctica inapreciable, por supuesto que cuanto más rápido sea un
CD-ROM mejor, pero hay que tener en cuenta qué precio estamos dispuestos a pagar por una característica
que luego no vamos a apreciar.

Los primeros CD-ROM operaban a la misma velocidad que los CD de audio estándar: de 210 a 539 RPM
dependiendo de la posición del cabezal, con lo que se obtenía una razón de transferencia de 150 KB/s,
velocidad con la que se garantizaba lo que se conoce como calidad CD de audio. No obstante, en aplicaciones
de almacenamiento de datos interesa la mayor velocidad posible de transferencia para lo que es suficiente
aumentar la velocidad de rotación del disco. Así aparecen los CD-ROM 2X, 4X... 24X,?X que
simplemente duplican, cuadriplican, etc. la velocidad de transferencia.

La mayoría de los dispositivos de menor velocidad que 12X usan CLV, los más modernos y rápidos, no
obstante, optan por la opción CAV. Al usar CAV, la velocidad de transferencia de datos varía según la
posición que ocupen estos en el disco al permanecer la velocidad angular constante.
Un aspecto importante al hablar de los CD-ROM de velocidades 12X o mayores es, a que nos referimos
realmente cuando hablamos de velocidad 12X, dado que en este caso no tenemos una velocidad de
transferencia 12 veces mayor que la referencia y esta ni siquiera es una velocidad constante. Cuando se dice que un CD-ROM CAV es 12X queremos decir que la velocidad de giro es 12 veces mayor en el borde del
CD. Así un CD-ROM 24X es 24 veces más rápido en el borde pero en el medio es un 60% más lento respecto
a su velocidad máxima.



El tiempo de búsqueda se refiere al tiempo que lleva mover el cabezal de lectura hasta la posición del disco en la que están los datos. Solo tiene sentido hablar de esta magnitud en media ya que no es lo mismo alcanzar un dato que está cerca del borde que otro que está cerca del centro. Esta magnitud forma parte del tiempo de acceso que es un dato mucho más significativo. El tiempo de búsqueda tiene interés para entender los componentes del tiempo de acceso pero no tanto como magnitud en sí.

En los CD-ROM de velocidad lineal constante (CLV), la velocidad de giro del motor dependerá de la posición que el cabezal de lectura ocupe en el disco, más rápido cuanto más cerca del centro. Esto implica un tiempo de adaptación para que este motor tome la velocidad adecuada una vez que conoce el punto en el que se encuentran los datos. Esto se suele conseguir mediante un microcontrolador que relaciona la posición de los datos con la velocidad de rotación.

En los CD-ROM CAV "no tiene sentido" esta medida ya que la velocidad de rotación es siempre la misma, así que la velocidad de acceso se verá beneficiada por esta característica y será algo menor; no obstante, se debe tener en cuenta que dado que los fabricantes indican la velocidad máxima para los CD-ROM CAV y esta velocidad es variable, un CD-ROM CLV es mucho más rápido que otro de la misma velocidad CAV cuanto más cerca del centro del disco.

La mayoría de los CD-ROM suelen incluir una pequeña caché cuya misión es reducir el número de accesos físicos al disco. Cuando se accede a un dato en el disco este se graba en la caché de manera que si volvemos a acceder a él, este se tomará directamente de esta memoria evitando el lento acceso al disco. Por supuesto, cuanto mayor sea la caché mayor será la velocidad de nuestro equipo pero tampoco hay demasiada diferencia de velocidad entre distintos equipos. Por este motivo ya que esta memoria solo nos evita el acceso a los datos más recientes que son los que van sustituyendo dentro de la caché a los que llevan más tiempo y dada la característica, en cuanto a volumen de información, de las aplicaciones multimedia nada nos evita el tener que acceder al dispositivo. Este es uno de los parámetros determinantes de la velocidad de este dispositivo. Obviamente, cuanto más caché tengamos mejor, pero teniendo en cuenta el precio que estamos dispuestos a pagar por ella.


La lectora de CD, también llamada reproductor de CD, es el dispositivo óptico capaz de reproducir los CD de audio, de video, de datos, etc. utilizando un láser que le permite leer la información contenida en dichos discos.

El lector de discos compactos está compuesto de:


Pasos que sigue el cabezal para la lectura de un CD:


Los discos compactos presentan una capa interna protegida, donde se guardan los bits mediante distintas tecnologías, siendo que en todas ellas dichos bits se leen merced a un rayo láser incidente. Este, al ser reflejado, permite detectar variaciones microscópicas de propiedades óptico reflectivas ocurridas como consecuencia de la grabación realizada en la escritura. Un sistema óptico con lentes encamina el haz luminoso, y lo enfoca como un punto en la capa del disco que almacena los datos.

En la actualidad, incluso se han desarrollado softwares que ayudan al grabado en las computadoras, igualmente, se ha añadido un método de grabación de CD de música en el reproductor de los sistemas de Windows.

Se puede grabar un CD por moldeado durante la fabricación.

Mediante un molde de níquel (CD-ROM), una vez creada una aplicación multimedia en el disco duro de una computadora es necesario transferirla a un soporte que permita la realización de copias para su distribución.

Las aplicaciones CD-ROM se distribuyen en discos compactos de 12 cm de diámetro, con la información grabada en una de sus caras. La fabricación de estos discos requiere disponer de una sala «blanca», libre de partículas de polvo, en la cual se llevan a cabo los siguientes procesos: sobre un disco finamente pulido en grado óptico se aplica una capa de material fotosensible de alta resolución, del tipo utilizado en la fabricación de microchips. Sobre dicha capa es posible grabar la información gracias a un rayo láser. Una vez acabada la transcripción de la totalidad de la información al disco, los datos que contiene se encuentran en estado latente. El proceso es muy parecido al del revelado de una fotografía.

Dependiendo de las zonas a las que ha accedido el láser, la capa de material fotosensible se endurece o se hace soluble al aplicarle ciertos baños. Una vez concluidos los diferentes baños se dispone de una primera copia del disco que permitirá estampar las demás. Sin embargo, la película que contiene la información y está adherida a la placa de vidrio es blanda y frágil, por lo cual se hace imprescindible protegerla mediante un fino revestimiento metálico, que le confiere a la vez dureza y protección.

Finalmente, gracias a una combinación de procesos ópticos y electroquímicos, es posible depositar una capa de níquel que penetra en los huecos y se adhiere a la película metálica aplicada en primer lugar sobre la capa de vidrio. Se obtiene de este modo un disco matriz o «máster», que permite estampar a posterior miles de copias del CD-ROM en plástico.

Una vez obtenidas dichas copias, es posible serigrafiar sobre la capa de laca filtrante ultravioleta de los discos imágenes e informaciones, en uno o varios colores, que permitan identificarlo. Todo ello, lógicamente, por el lado que no contiene la información.

La fabricación de los CD-ROMs de una aplicación multimedia concluye con el estuchado de los discos, que es necesario para protegerlos de posibles deterioros. Al estuche se añade un cuadernillo que contiene las informaciones relativas a la utilización de la aplicación.

Finalmente, la envoltura de celofán garantiza al usuario que la copia que recibe es original. Estos procesos de fabricación permiten en la actualidad ritmos de producción de hasta 600 unidades por hora en una sola máquina.

Otro modo de grabación es por la acción de un haz láser (CD-R y CD-RW, también llamado CD-E).
Para esto la grabadora crea unos pits y unos lands cambiando la reflectividad de la superficie del CD. Los pits son zonas donde el láser quema la superficie con mayor potencia, creando ahí una zona de baja reflectividad. Los lands, son justamente lo contrario, son zonas que mantienen su alta reflectividad inicial, justamente porque la potencia del láser se reduce.
Según el lector detecte una secuencia de pits o lands, tendremos unos datos u otros. Para formar un pit es necesario quemar la superficie a unos 250º C. En ese momento, el policarbonato que tiene la superficie se expande hasta cubrir el espacio que quede libre, siendo suficientes entre 4 y 11 mW para quemar esta superficie, claro que el área quemada en cada pit es pequeñísima.
Esto es posible ya que es una superficie algo "especial". En los discos grabables es un tinte orgánico. Para simular un pit, el grabador usa un rayo láser más potente de lo normal para dejar marcas en el tinte orgánico para que absorban la luz láser en el lector y sean interpretados como ceros. En los discos regrabables está formada en esencia por plata, telurio, indio y antimonio. Inicialmente (el disco está completamente vacío de datos...) esta superficie tiene una estructura policristalina o de alta reflectividad. Si el software le indica a la grabadora que debe simular un pit, entonces lo que hará será aumentar con el láser la temperatura de la superficie hasta los 600 o 700 °C, con lo que la superficie pasa a tener ahora una estructura no cristalina o de baja reflectividad. Cuando debe aparecer un land, entonces se baja la potencia del láser para dejar intacta la estructura policristalina.
Para borrar el disco se quema la superficie a unos 200 °C durante un tiempo prolongado (de 20 a 40 minutos) haciendo retornar a su estado cristalino inicial. En teoría deberíamos poder borrar la superficie unas 1000 veces, más o menos, aunque con el desgaste cotidiano jamás se alcance esta marca.

El último medio de grabación de un CD es por la acción de un haz láser en conjunción con un campo magnético (discos magneto-ópticos).

Los discos ópticos tienen las siguientes características, confrontadas con los discos magnéticos:

Los discos ópticos, además de ser medios extraíbles con capacidad para almacenar masivamente datos en pequeños espacios -por lo menos diez veces más que un disco rígido de igual tamaño- son portátiles y seguros en la conservación de los datos (que también permanecen si se corta la energía eléctrica). El hecho de ser portátiles proviene del hecho de que son extraíbles de la unidad.

Desde hace tiempo han surgido programas computacionales para grabar CD que nos permiten utilizar un disco CD-R como si de un disco regrabable se tratase. Esto no quiere decir que el CD se pueda grabar y posteriormente borrar, sino que se puede grabar en distintas sesiones, hasta ocupar todo el espacio disponible del CD.
Los discos multisesión no son más que un disco normal grabable, ni en sus cajas, ni en la información sobre sus detalles técnicos se resalta que funcione como disco Multisesión, ya que esta función no depende del disco, sino como está grabado.

Si se graba un CD y este no es finalizado, podemos añadirle una nueva sesión, desperdiciando una parte para separar las sesiones (unos 20 MB aproximadamente).
Haremos que un CD sea multisesión en el momento que realizamos la segunda grabación sobre él, este o no finalizado, sin embargo, al grabar un CD de música automáticamente el CD-R queda finalizado y no puede ser utilizado como disco Multisesión.

No todos los dispositivos ni los sistemas operativos, son capaces de reconocer un disco con multisesión, o que no esté finalizado.

Puede haber confusión entre un CD-R con grabado multisesión y un CD-RW. En el momento en que un disco CD-R se hace multisesión, el software le dará la característica de que pueda ser utilizado en múltiples sesiones, es decir, en cada grabación se crearán «sesiones», que sólo serán modificadas por lo que el usuario crea conveniente. Por ejemplo, si se ha grabado en un CD-R los archivos prueba1.txt, prueba2.txt y prueba 3.txt, se habrá creado una sesión en el disco que será leída por todos los reproductores y que contendrá los archivos mencionados. Si en algún momento no se necesita alguno de los ficheros o se modifica el contenido de la grabación, el programa software creará una nueva sesión, a continuación de la anterior, donde no aparecerán los archivos que no se desee consultar, o se verán las modificaciones realizadas, es decir, es posible añadir más archivos, o incluso quitar algunos que estaban incluidos. Al realizar una modificación la sesión anterior no se borrará, sino que quedará oculta por la nueva sesión dando una sensación de que los archivos han sido borrados o modificados, pero en realidad permanecen en el disco.
Obviamente las sesiones anteriores, aunque aparentemente no aparecen permanecen en el disco y están ocupando espacio en el mismo, esto quiere decir que algún día ya no será posible «regrabarlo», modificar los archivos que contiene, porque se habrá utilizado toda la capacidad del disco.
A diferencia de los CD-R, los discos CD-RW sí pueden ser borrados, o incluso formateados (permite usar el disco, perdiendo una parte de su capacidad, pero permitiendo grabar en el ficheros nuevos). En el caso de utilizar un CD-RW cuando borramos, lo borramos completamente, se pueden hacer también borrados parciales, que necesitan una mayor potencia del láser para volver a grabarse. Un disco CD-RW se puede utilizar como una memoria USB, con software adecuado, siempre que la unidad soporte esta característica, se pueden manipular ficheros como en una memoria USB, con la salvedad de que no se borra, sino que al borrar un fichero este sigue ocupando un espacio en el disco, aunque al examinarlo no aparezca dicho archivo. Los discos CD-RW necesitan más potencia del láser para poder grabarse, por esta razón los discos regrabables tienen una velocidad de grabación menor que los discos grabables (tardan más en terminar de grabarse).
Los DVD-RW, DVD+RW funcionan de manera análoga, los DVD-RAM también, pero están diseñados para escritura como las memorias USB (siglas que en inglés significan universal serial bus).

Las reacciones químicas entre sus componentes, además del calor y el maltrato, pueden destruir los datos digitales. Por lo tanto, hay que revisar periódicamente la información para detectar las fallas. Para prevenir el deterioro temprano de los compactos, solamente hay que tratarlos bien. Los CD-R, basados en tinturas orgánicas, son más perecederos y volátiles que los compactos y los CD-ROM. Hay que verificar la copia de seguridad cada 2 años o menos. Es conveniente, la práctica de hacer doble copia de todos los datos y respaldar la información cada dos años. Las siguientes son algunas recomendaciones para la adquisición y preservación de discos compactos grabados y en blanco:





</doc>
<doc id="4889" url="https://es.wikipedia.org/wiki?curid=4889" title="Autonomía (dispositivo)">
Autonomía (dispositivo)

En el ámbito de la técnica, la autonomía es el tiempo que un dispositivo con una fuente de alimentación independiente puede permanecer en activo, hasta el agotamiento de la fuente de alimentación. Así, un dispositivo eléctrico, como una linterna, tendrá una autonomía de horas en función de la capacidad de sus baterías y un automóvil podrá recorrer una cierta cantidad de kilómetros sin repostar en función de la capacidad de su depósito de combustible. Las mejoras en la autonomía de los equipos se pueden lograr bien incrementando la capacidad de la fuente de alimentación o bien aumentando el rendimiento y, por tanto, reduciendo el consumo del dispositivo. 

En robótica, un robot autónomo es aquel que actúa sin intervención humana. En algunos medios de transporte, como los vehículos eléctricos, se usa el término para referirse a la cantidad de tiempo que puede pasar un vehículo sin ser cargado nuevamente.

En informática, un dispositivo autónomo es aquel que no necesita estar conectado a la computadora para funcionar. Un dispositivo multifuncional puede tener funciones autónomas para:

Todo esto sin necesidad de que la computadora esté encendida.


</doc>
<doc id="4891" url="https://es.wikipedia.org/wiki?curid=4891" title="Escáner informático">
Escáner informático

Un escáner de ordenador (escáner proviene del idioma inglés "scanner") es un periférico que se utiliza para "copiar", mediante el uso de la luz, imágenes impresas o documentos a formato digital (a color o a blanco y negro).
El escáner nace en 1984 cuando Microtek crea el MS-200, el primer escáner blanco y negro que tenía una resolución de 200dpi. Este escáner fue desarrollado para Apple Macintosh. 
Los escáneres pueden tener accesorios como un alimentador de hojas automático o un adaptador para diapositivas y transparencias.
Al obtenerse una imagen digital se puede corregir defectos, recortar un área específica de la imagen o también digitalizar texto mediante técnicas de OCR. Estas funciones las puede llevar a cabo el mismo dispositivo o aplicaciones especiales.

Hoy en día es común incluir en el mismo aparato la impresora y el escáner. Son las llamadas impresoras multifunción. También están surgiendo el usar como escáner la cámara de los teléfonos inteligentes.

A los datos que obtienen los escáneres se les aplica cierto algoritmo y se envían a la computadora mediante una interfaz de entrada/salida (normalmente SCSI, USB o LPT en máquinas anteriores al estándar USB). La profundidad del color depende de las características del vector de escaneado (la primera de las características básicas que definen la calidad del escáner) que lo normal es que sea de al menos 24 bits. Imágenes con más profundidad de color (más de 24 bits) tienen utilidad durante el procesamiento de la imagen digital, reduciendo la posterización.

Otro de los parámetros más relevantes de la calidad de un escáner es la resolución, medida en píxeles por pulgada (ppp). Los fabricantes de escáneres en vez de referirse a la resolución óptica real del escáner, prefieren hacer referencia a la resolución interpolada, que es mucho mayor gracias a la interpolación software.

Por hacer una comparación entre tipos de escáneres mejores llegaban hasta los 5400 ppp. Un escáner de tambor tenía una resolución de 8000 a 14000 ppp.

El tercer parámetro más importante para dotar de calidad a un escáner es el rango de densidad. Si el escáner tiene un alto rango de densidad, significa que es capaz de reproducir sombras y brillos con una sola pasada. Son dispositivos encargados de incorporar la realidad de las dos dimensiones, digitalizándola, a un ordenador.

El tamaño del fichero donde se guarda una imagen escaneada puede ser muy grande: una imagen con calidad de 24 bits un poco mayor que un A4 y descomprimida puede ocupar unos 100 megabytes. Los escáneres de hoy en día generan esta cantidad en unos pocos segundos, lo que quiere decir que se desearía poseer una conexión lo más rápida posible.

Antes los escáneres usaban conexiones paralelas que no podían ir más rápido de los 70 kilobytes/segundo, SCSI-II se adoptó para los modelos profesionales y aunque era algo más rápido (unos cuantos megabytes por segundo) era bastante más caro.

Hoy los modelos más recientes vienen equipados con conexión USB, que poseen una tasa de transferencia de 1.5 megapixel por segundo para los USB 1.1 y de hasta 60 megapixel por segundo para las conexiones USB 2.0, lo que elimina en gran medida el cuello de botella que se tenía al principio.
Los dos estándares para interfaces existentes en el mercado de PC con Windows o Macs son:

Al escanear se obtiene como resultado una imagen RGB no comprimida que puede transferirse a la computadora. Algunos escáneres comprimen y limpian la imagen usando algún tipo de firmware embebido. Una vez se tiene la imagen en la computadora, se puede procesar con algún programa de tratamiento de imágenes como Photoshop, Paint Shop Pro o GIMP y se puede guardar en cualquier unidad de almacenamiento como el disco duro.

Normalmente las imágenes escaneadas se guardan con formato JPEG, TIFF, mapa de bits o PNG dependiendo del uso que se le quiera dar a dicha imagen más tarde.

Cabe mencionar que algunos escáneres se utilizan para capturar texto editable (no sólo imágenes como se había visto hasta ahora), siempre y cuando la computadora pueda leer este texto. A este proceso se le llama OCR (Optical Character Recognition).

El escaneado de documentos es distinto al de imágenes, aunque use algunas técnicas de este último. Aunque el escaneado de documentos puede hacerse en escáneres de uso general, la mayoría de la veces se realiza en escáneres especiales dedicados a este propósito, fabricados por Canon, Fujitsu o Kodak entre otros.
Los escáneres de documentos tienen bandejas de alimentación mayores a las de fotocopiadoras o escáneres normales.

Normalmente escanean a resolución inferior que los escáneres normales, de 150 ppp a 300 ppp, así evita ficheros de tamaño excesivo.

El escaneado se hace en escala de grises, aunque cabe la posibilidad de hacerlo en color. La mayoría son capaces de digitalizar a doble cara a velocidad máxima (de 20 a 150 páginas por minuto). Los más sofisticados llevan incorporado algún firmware que “limpia” el escaneo eliminando marcas accidentales. Normalmente se comprimen los datos escaneados al vuelo.

La mayoría de documentos escaneados se convierten en ficheros editables usando la tecnología OCR. Mediante los driver ISIS y TWAIN se escanea el documento a formato TIFF, para pasar las páginas escaneadas a un procesador de texto, que almacena el fichero correspondiente.

El escaneado de libros implica dificultades técnicas adicionales. Algunos fabricantes han desarrollado escáneres especiales para este cometido incluso haciendo uso de robots especiales encargados de pasar las páginas.

Los tipos principales de escáneres son los de tambor, plano (que a su vez puede ser Escáner CCD
o CIS), de película o diapositiva, de mano y de cámara de teléfono móvil.

Los escáneres de mano vienen en dos formas: de documentos y escáneres 3D. Los escáneres de mano de documentos son dispositivos manuales que son arrastrados por la superficie de la imagen que se va a escanear. Escanear documentos de esta manera requiere una mano firme, de forma que una velocidad de exploración desigual podría producir imágenes distorsionadas - un poco de luz sobre el escáner indicaría que el movimiento es demasiado rápido. Tienen generalmente un botón "inicio", que se pulsa por el usuario durante la duración de la exploración; algunos interruptores para ajustar la resolución óptica, y un rodillo, lo que genera un pulso de reloj para la sincronización con el ordenador. La mayoría de los escáneres tienen una pequeña ventana a través de la se que podría ver el documento que se escanea visto. Asimismo, llevan puerto USB, suelen guardar directamente el resultado en formato JPEG, en una tarjeta microSD que suele ser como mínimo de hasta 32 GB.

Los escáneres de cama plana son los más comunes, y se utilizan para copiar documentos, hojas sueltas, fotografías de diferentes tamaños, hasta un máximo de tamaño (generalmente una hoja de tamaño Letter, Legal u Oficio). Presenta varias mejoras con respecto a los escáneres de mano, como por ejemplo un aumento significativo de la calidad de escaneo (resolución óptica) y velocidad.

Muy utilizados en estudios de diseño gráfico o artístico, debido principalmente a su gran resolución óptica, son de gran tamaño y permiten escaneos por modelos de color CYMK o RGB.

Las cámaras de resolución superior instaladas en algunos teléfonos inteligentes pueden escanear documentos de calidad razonable mediante la captura de una foto con la cámara del teléfono y procesarla posteriormente con una aplicación de escaneo, ya sea para emblanquecer el fondo de una página, corregir la distorsión de la perspectiva a fin de arreglar la forma rectangular del documento, convertir a blanco y negro, etc. Algunas aplicaciones pueden escanear documentos de varias páginas con exposiciones de cámara de manera sucesiva y generar un documento de archivo único o archivos de varias páginas.


</doc>
<doc id="4894" url="https://es.wikipedia.org/wiki?curid=4894" title="Tarjeta de sonido">
Tarjeta de sonido

Una tarjeta de sonido o placa de sonido es una tarjeta de expansión para computadoras que permite la salida de audio controlada por un programa informático llamado controlador ("driver").

El uso típico de las tarjetas de sonido consiste en hacer, mediante un programa que actúa de mezclador, que las aplicaciones multimedia del componente de audio suenen y puedan ser gestionadas. Estas aplicaciones incluyen composición de audio y en conjunción con la tarjeta de videoconferencia también puede hacerse una edición de vídeo, presentaciones multimedia y entretenimiento (videojuegos). Algunos equipos (como computadoras personales) tienen la tarjeta ya integrada a la placa base, mientras que otros requieren tarjetas de expansión. También hay equipos que por su uso (como por ejemplo servidores) no requieren de dicha función.

Las tarjetas de sonido profesionales son habitualmente conocidas como "interfaces de audio", y a veces tienen la forma de unidades externas montables en rack que usan USB, FireWire o una interfaz óptica, para ofrecer suficiente velocidad de datos. En estos productos el énfasis está, en general, en tener múltiples conectores de entrada y salida, compatibilidad con "hardware" directo para múltiples canales de sonido de entrada y salida, así como fidelidad y frecuencias de muestreo más altos en comparación con la típica tarjeta de sonido para el gran público. En este sentido, su papel y su propósito es más parecido al de una grabadora de datos multicanal y procesador/mezclador en tiempo real, funciones que solo son posibles hasta cierto punto con las típicas tarjetas de sonido para el consumidor medio. 




</doc>
<doc id="4895" url="https://es.wikipedia.org/wiki?curid=4895" title="Televisión de alta definición">
Televisión de alta definición

La televisión de alta definición (HDTV) es un sistema de televisión analógico o digital que proporciona una resolución de imagen mucho más alta que la de la televisión de definición estándar. HDTV es el formato de vídeo estándar actual utilizado en la mayoría de emisiones: televisión digital terrestre (TDT), televisión por cable, televisión por satélite y vídeo por streaming.

Anteriormente el término se aplicaba a los estándares de televisión desarrollados en la década de 1930 para reemplazar a los modelos de prueba. También se usó para referirse a modelos anteriores de alta definición, particularmente en Europa, llamados D2 Mac, y HD Mac, pero que no pudieron implantarse ampliamente.

Los términos "HD ready" ("listo para alta definición") y "compatible HD" ("compatible con alta definición") están siendo usados con propósitos publicitarios. Estos términos indican que el dispositivo electrónico que lo posee, ya sea un televisor o un proyector de imágenes, es capaz de reproducir señales en Alta Definición; aunque el hecho de que sea compatible con contenidos en esta norma no implica que el dispositivo sea de alta definición o tenga la resolución necesaria, tal y como pasa con algunos televisores basados en tecnología de plasma con menos definición vertical que televisores de años atrás (833x480 en vez de los 720x576 píxeles -anamórficos equivalen a 940x576-), los cuales son compatibles con señales en alta definición porque reducen la resolución de la imagen para adaptarse a la resolución real de la pantalla.

HDTV tiene por lo menos el doble de resolución que el SDTV, razón por la cual se puede mostrar mucho más detalle en comparación a un televisor analógico o un DVD normal. Además, los estándares técnicos para transmitir HDTV permiten que se proyecte utilizando una relación de aspecto de 16:9 sin utilizar franjas de colores y por lo tanto se puede incrementar la resolución del contenido.

Las pioneras en tecnología de alta definición fueron las televisoras japonesas, que transmiten en HD desde hace más de 15 años. Japón comenzó con un sistema de 1035 líneas de resolución llamado MUSE, desarrollado por la empresa NHK en el año 1980. El principal problema de este sistema fue el excesivo uso de las bandas de transmisión, pues requería hasta 5 veces más espacio espectral que un canal de televisión estándar.

El HD-MAC ofrecía 1250 líneas, con 50 cuadros por segundo como el sistema PAL y la posibilidad de la transmisión vía satélite. Fue la opción europea para el mercado de la HDTV (HD MAC = High Definition Multiplexed Analog Components), se trataba de una compleja mezcla de señales de vídeo analógicas multiplexadas con sonido digital. La resolución era de 1250 líneas de las cuales eran visibles 1152, con 50 cuadros por segundo y aspecto de 16:9. Se utilizó ampliamente en los JJ.OO. de Barcelona 92.

Existen tres normas técnicas definidas: la estadounidense (ATSC), la europea (DVB-T) y la japonesa (ISDB-T).




En el país Norte Africano existen varios canales que transmiten en HD Laayoune TV HD, 2M TV HD, Arryadia Arrabia HD, Assadissa HD, Aflam TV HD, Tamazight TV HD. 

El Salvador

Dispone de señal HD Digital con el estándar ISDB-tb en todas sus transmisiones, a partir de 2018, se iniciaron prueba en la TV pública, a pesar de que se habían hecho pruebas con el estándar DVB-T desde 2003, el gobierno se ha decantado por el estándar ISDB-tb por razones de portabilidad, Se prevé el apagón analógico para el final de 2020.
Argentina fue uno de los primeros países en adoptar una norma de alta definición. Después del estudio por parte de una comisión técnica (ATA, cámara argentina de proveedores y fabricantes de equipos de radiodifusión, cámara argentina de industrias electrónica, asociación de fábricas argentinas terminales de electrónica, ADECUA y la CNC, el 22 de octubre de 1998 se estableció de forma oficial el estándar estadounidense ATSC lo que motivó protestas por parte de Brasil, pues consideraban que era mejor tomar una decisión conjunta. Poco tiempo después comenzaron las transmisiones de prueba, siendo ARTEAR la primera empresa en emitir en alta definición.

Así, El Trece inauguró las transmisiones en alta definición en los partidos de este país en Copa Mundial de Fútbol de Alemania 2006. Durante 2007 transmitió 12 horas de programación en sonido Dolby 5.1 y en HD (Alta Definición) formato panorámico 16:9 (utilizado en el cine) por la sintonía 12.1. Todo en norma legalmente aprobada ATSC.

Sin embargo, el gobierno argentino nuevamente crea una comisión técnica de estudio, quien tras reunirse en un par de ocasiones sugiere la adopción del estándar japonés ISDB-T. Así el 31 de agosto de 2009 se abandona la norma ATSC y se adopta el estándar japonés. La elección se rubricó con la firma de dos convenios. En uno Japón se comprometió a transferir tecnología, capacitar recursos humanos y equipar a Canal 7 (Televisión Pública) para que pudiera empezar con las transmisiones. El otro acuerdo se cerró con Brasil y contempla trabajar en conjunto para desarrollar el nuevo dispositivo de recepción en la región. Lo que llevó al Gobierno a inclinarse por este estándar, fueron las posibilidades de desarrollo industrial y generación de empleo calificado. Japón se comprometió a no cobrar royalties por el uso de la tecnología y junto con Brasil invitaron a Argentina a participar en el Foro de Desarrollo ISDB-T, donde se discutieron las futuras innovaciones tecnológicas de manera conjunta.

A partir de junio de 2010, comenzó la primera transmisión oficial en alta definición utilizando la nueva norma a través de Canal 7. A pesar de que la cantidad de contenido en alta definición era muy limitada debido a la falta de equipamiento para realizar este tipo de contenidos, se transmitió gran parte de la Copa Mundial de Fútbol de 2010 durante junio y julio de 2010 en HD.

Las primeras transmisiones fueron pruebas aisladas llevadas a cabo por algunas empresas utilizando las frecuencias otorgadas para tal fin. Un mecanismo que realmente no contribuye a seleccionar ordenadamente un estándar y que carece del rigor necesario para llevar a cabo una decisión oficial al respecto. Esta modalidad fue implementada en la década pasada a raíz de las presiones de EE.UU. a forzar la norma ATSC y comenzar cuanto antes las emisiones de prueba, contando con el apoyo de algunos de los radiodifusores de entonces. Tal es así que Telefe comenzó transmitiendo en ATSC apoyándolo con mucha vehemencia y luego permutó hacia DVB, tanto en la modulación como en la preferencia.

Actualmente, la venta de televisores LCD y LED HDTV está en aumento y cada vez es más frecuente encontrar en hogares de la Ciudad de Buenos Aires y el conurbano bonaerense, el servicio de televisión digital ofrecido por los operadores de cable. Mientras que en el resto del país sólo la Red Intercable a través de un sistema DVB-S como plataforma de contribución y DVB-C como plataforma de distribución, está presente en varias ciudades del interior del país.

El 29 de agosto de 2010 se realizó la primera transmisión en vivo desde exteriores, al emitirse un partido del Mundial femenino de hockey desde Rosario (Argentina - Sudáfrica). La empresa Pulsar Televisión HD fue contratada por la Televisión Pública para generar las imágenes que luego fueron vistas a través del sistema de Televisión Digital Terrestre y empresas de cable que emitían la señal TV Pública Digital.

Versiones de los canales argentinos de TV Abierta en HD








También hay versiones en alta definición en TV paga de los siguientes canales: C5N HD, TN HD, Rocket HD, Quiero HD, ESPN HD, Fox Sports HD, TyC Sports HD, TyC Sports 2 HD, TyC Sports 3 HD, TyC Max HD, Cable Sport HD, Cable Sport 2 HD, Cable Sport 3 HD, A24 HD, América Sports HD, Canal 26 HD, Multideportes HD y Cartoon Network HD

Brasil fue el único país donde emisoras y el equipo de las industrias emergentes eran parte de las pruebas de laboratorio y campo para comparar la eficacia técnica de las tres normas tecnológicas existentes. La Universidad Presbiteriana Mackenzie, junto con el equipo de NEC, realizaron varias pruebas que condujeron a la elección de ISDB-Tb,(b = brasilero) en forma modificada. El Instituto de la Universidad de São Paulo, en su laboratorio de sistemas integrados, creó una transmisión plenamente brasileño de forma predeterminada. TV digital en Brasil llegó a las 20:48 del 2 de diciembre de 2007, con un discurso del Presidente de la República. Inicialmente en São Paulo de forma predeterminada japonés con algunos ajustes. El 20 de abril, la señal en alta definición fue lanzada por la Rede Globo solo en la región metropolitana del Gran Río. También HDTV ya está presente en Campinas, Goiânia, Belo Horizonte, Porto Alegre, Curitiba, Manaos, Belém, Florianópolis y Salvador de Bahía, las dos últimas en fase experimental. Santos es la segunda ciudad del interior para recibir la señal digital, en marzo de 2009.

El día 14 de septiembre de 2009 la presidenta Michelle Bachelet anunció que Chile optó por la norma ISDB-Tb (MPEG-4). El ministro René Cortázar dijo que la nueva norma se escogió por la mejor calidad de recepción, por la mayor cantidad de señales y por la opción de operar TV por celular.

Anteriormente, el 30 de septiembre de 2008, el ministro secretario general de gobierno, Francisco Vidal, dijo en conversación con Radio Cooperativa: "«Pido perdón, porque he dicho como tres veces este año que "el próximo lunes ingresará el proyecto al Parlamento". Ahora estoy en condiciones de decir de verdad que la próxima semana estamos en condiciones de enviar la norma de televisión digital al Congreso».

El canal de la Pontificia Universidad Católica de Chile (Canal 13 - UCTV) ha realizado pruebas de HDTV bajo la norma ATSC en frecuencia 12.1 VHF por más de dos años, y desde noviembre de 2006 bajo la norma DVB-T en frecuencia 21 UHF y bajo la norma japonesa por el canal 24. Se planificó realizar pruebas bajo norma DVB-H en el corto plazo. Actualmente en Chile, Canal 13 hace pruebas con el sistema japonés, transmitiendo con muy baja potencia y abarcando gran parte de Santiago a nivel de recepción fija. En cuanto a la recepción móvil, ésta es recibida sin ningún problema.

Actualmente la emisión de Canal 13 en ISDB-T se hace en el canal 24, emitiendo un documental sobre Santiago de Chile en idioma japonés a 1080i.

Las transmisiones digitales comenzaron de forma experimental en 1995, emitiendo desde Punta Arenas, notas periodísticas en imagen digital comprimida, experiencia usada muy a menudo en el Departamento de Prensa y luego en otras regiones del país, como las de Arica Parinacota, de Los Ríos, de Los Lagos, del Bío Bío y de Aysén. Se suma a ello algunas transmisiones vía fibra óptica en Cuba.

Televisión Nacional de Chile (TVN), entre noviembre de 1999 y mayo de 2000 realizó transmisiones experimentales de HDTV bajo la norma ATSC en frecuencia 8 VHF. A noviembre de 2007, TVN transmite señal de prueba en ATSC en canal 33 UHF, 4 señales SDTV en la misma frecuencia.

Recientemente la Asociación Nacional de Televisión (ANATEL) afirmó en El Mercurio su rechazo a la norma estadounidense (ATSC) en favor de la norma japonesa (ISDB-T) como estándar definitivo de la Televisión Digital, previamente la blogósfera chilena había manifestado su apoyo a ISDB-T pese a que la ANATEL se mantuvo firme durante meses en pro de la norma estadounidense.

En tanto ARETEL BIOBIO está apoyando en forma culmínate la norma Europea, ya que posee más flexibilidad, interactividad con los televidentes, y por sobre todo más posibilidad de integrar a las personas de la comunidad.

En septiembre de 2007, se crea una comisión veedora para la TDT con un plan pionero en el mundo. Consta de elegir 160 familias que podrán utilizar las 3 normas en juego (ISDB-T, DVB-t y ATSC) al mismo tiempo y podrán evaluarlas. El gobierno mediante la subte llevarán todos los equipos hasta el hogar del voluntario, ya sean decodificadores o televisores HD, para que éste pueda dar sus impresiones sobre cada norma.

La polémica sobre el ancho de banda que se utiliza en Chile (6 MHz) estuvo presente, debido a que la norma europea utiliza 8 MHz dejando un HD más pobre en definición para los anchos de banda, como el chileno (6 MHz). Solamente 2 países con un ancho de banda de 6 MHz/NTSC han elegido DVB-T, creando un gran debate técnico (aclarando que ISDB-T y ATSC cumplen con el ancho de banda chileno sin mayores problemas).

El 14 de septiembre de 2009, Chile optó por la norma Japonesa-Brasileña para la Televisión Digital. Dada las condiciones geográficas, esta norma es la óptima certificada por estudios universitarios que avalan la elección. Ahora, sólo falta que en el Congreso se apruebe la regulación pertinente para que las transmisiones sean legales en el país.

Inmediatamente después de elegir la norma de TV Digital Subtel entregó licencias de pruebas de TDT (desde el día 15 de septiembre de 2009) a TVN, Chilevisión y Canal 13, los que están emitiendo bajo la norma ISDB-T HD/ONE-SEG en modo de prueba hasta que en el congreso apruebe la regulación.

Canal 13 fue la primera señal abierta que ofrece sus contenidos en alta definición, a partir del 16 de noviembre de 2009, a través de señal 813 del operador de cable VTR y el 26 de julio de 2010 a través de la señal 813 por Movistar TV Digital y de Gtd Manquehue por el canal 813 también. Actualmente, tiene cerca del 40% de la programación en calidad full HD (con resolución 1920 x 1080) y relación de aspecto 16:9. Asimismo, y con motivo de la Copa del Mundo Sudáfrica 2010, TVN HD comenzó a ser transmitida por las cable operadoras VTR (Canal 807) y TuVes HD (Canal 71). Esto, a pesar de la negativa de TVN de que se utilice su señal por proveedores de TV de pago.

En Costa Rica, Teletica canal 7 ha hecho pruebas en HD desde 2006, donde transmitía películas en HD y en 2009 donde hizo sus primeras transmisiones en HD con partidos de fútbol nacionales. Desde entonces Teletica ha comprado cámaras de estudio HDC-1000, que aparte de las vendidas a los dos grandes canales de México es la primera que vende Sony en toda Hispanoamérica, y también adquirió dos HDC-1400. Produciendo programación original en Alta Definición.

Repretel Canal 6, también transmite en HD en los canales RHD 6.1 y 6.2, aunque la mayoría de programación es reescalada.

El Gobierno de Costa Rica, eligió mediante una comisión la tecnología ISDB-T en 2010. Y en agosto de 2019 se llevará a cabo el apagón analógico para la capital (San José)para 2020 todo Costa Rica no tendría canales analógicos

En Ecuador, el sistema de Televisión digital terrestre elegido fue el ISDB-T. Es así que la televisora ecuatoriana Teleamazonas sacará al aire Teleamazonas HD, que será la señal de alta definición de Teleamazonas. Sus transmisiones se iniciaron el 1 de marzo de 2011, convirtiendo a la televisora en la primera en Ecuador en tener una señal HDTV al aire. Igualmente Ecuavisa con Ecuavisa HD, que será la señal de alta definición de Ecuavisa. Sus transmisiones se iniciaron el 1 de marzo de 2011, convirtiendo a la televisora en la segunda en Ecuador en tener una señal HDTV al aire, Canal Uno HD y Oromar HD otros canales como TC Televisión, Gamavisión, Ecuador TV, Canal Uno, RTS, RTU, Oromar Televisión, Telerama, Televicentro y el canal universitario UCSG Televisión se encuentran en transmisiones de prueba de la señal HDTV. Por otro lado Ecuavisa y TC Televisión son los únicos canales en HDTV en señal abierta pero en televisión de paga a través de Grupo TV Cable, por el momento éstas son las televisoras HDTV del Ecuador.

Actualmente se encuentran al aire los canales: Ecuavisa HD, TC HD, Teleamazonas HD, RTS HD, Televicentro HD, Gamavisión HD, Canal Uno HD, RTU HD, Oromar HD, TVS.FHD.RS, Telecuenca HD, TV Color HD, Telecosta HD, UTV HD, Canal Sur - Televisión Municipal de Loja HD, Televisión Manabita HD, Telesucesos HD, Zaracay TV HD y Unimax TV HD.

En El Salvador, la televisora más importante de ese país Telecorporación Salvadoreña inició las pruebas de transmisión en alta definición en 2007.

El proceso de preparación para el cambio de producción de análoga a digital se inicia en 2005, dos años más tarde, en 2007 TCS renueva toda su flota de cámaras y equipo de producción y adquiere equipo tipo Sony XDCAM HD. En 2010 Telecorporación Salvadoreña inicia la transmisión de algunos de sus programas y franjas en HD - alta definición, se realizan pruebas piloto a finales de 2009 y su implementación inició con la transmisión de la Copa Mundial de Fútbol de 2010.

En la actualidad su prime time es transmitido en formato HD compuesto por los programas ¿Quién quiere ser millonario? El Salvador, Bailando por un sueño El Salvador, Cantando por un sueño El Salvador, Ticket (programa de televisión), y eventos deportivos internacionales como Copa del mundo y la Liga de Campeones de la UEFA, entre otros.

En Estados Unidos se establecen las bases para el desarrollo de los sistemas de televisión de alta definición y analógica en 1991. Estados Unidos fue el primer país en transmitir la mejor calidad de la HDTV. En todo el país todos los canales son transmitidos en Alta Definición.
Canales hispanos que transmiten en formato HD son, Telemundo, divisiones de NBC UNIVERSAL, Univisión y la cadena hermana Telefutura, como otros también que son HBO Latino y Galavisión. Estados Unidos es uno de los países que transmite la mejor alta definición y fue el primero en tener la tecnología HDTV y 3D @max. Muchos canales de países latinoamericanos han tenido muchos conflictos con Estados Unidos a causa de la tecnología que este país hoy en día tiene y que ha desarrollado. Algunos de los países que han tratado de rebasarle a Estados Unidos en tecnología de alta definición son suramericanos.

Canales HDTV al aire

En Colombia, el sistema de televisión digital elegido es el europeo (DVB-T2). La decisión de la Comisión Nacional de Televisión fue anunciada el 28 de agosto de 2008, después de diferentes retrasos y negociaciones. Colombia decidió operar con el sistema de compresión MPEG-4.

El primer operador de televisión en ofrecer HD fue DirecTV (satelital). En el primer semestre de 2010 se empezó a ofrecer por parte de los operadores Claro (cable-satelital), Tigo UNE (IPTV) y (HFC) y Movistar Colombia (satelital) previendo la llegada de la copa mundial de fútbol de la FIFA Sudáfrica 2010.

Actualmente los canales privados Caracol y RCN transmiten su señal de alta definición en el formato de televisión digital (DVB-T) y DVB-T2 a las ciudades de Bogotá y Medellín, las ciudades de Barranquilla y Cali cuentan con DVB-T2 desde mayo de 2012

RTVC, Radio y Televisión Nacional de Colombia actualmente emite señales digitales desde 2012 a través de sus tres canales, Canal Uno, Señal Institucional y Señal Colombia, cubriendo el 85% de Bogotá.

Estos son los siguientes canales de Televisión Abierta disponibles en Alta Definición







Este país es el primero en Centroamérica en implementar televisión en alta definición en el año 2006, a tiempo para transmitir todos los partidos de la Copa mundial de ese año, a través de canales 3 y 7, con su repetidora en alta definición, canal 19. Todavía está en etapa experimental, pero se prevé que más canales en el futuro empiecen con la transición.

En Honduras 4 canales producen programación en alta definición. Siendo el primero Campus TV, que posee la mejor tecnología y calidad HD en Honduras y en Centroamérica, con su programación FullHD en 1080i en las frecuencias 59.1 y 59.2. Canal TEN (Televisión Educativa Nacional) también incursionó en esta área en la frecuencia 20.1.
Televicentro, la mayor televisora del país, transmitió 56 de los 64 partidos del Mundial de fútbol de Sudáfrica 2010 en alta definición y actualmente retransmite partidos de fútbol y eventos especiales producidos en esta calidad a través de su canal HD. A partir del 6 de diciembre de 2010, Televicentro inició su nueva era HD, con la transmisión de sus programas de noticias en este formato a través de Amnet/Tigo y Cablecolor.

En 2013, Televicentro presentó la única y más grande unidad móvil HD de Honduras. Capaz así, de transmitir eventos especiales y deportivos. Como los partidos de la Selección Nacional de Honduras y partidos de Liga Nacional, en HD.
Canal 11 también transmite todos sus noticieros y el programa deportivo "Todo Deportes" en Alta Definición a través de Cablecolor.
HCH Televisión Digital, desde hace 13 años transmite al aire, pero no siempre existió dicha tecnología hasta inicios de esta década.
Canal 11, transmite en alta definición partidos de la liga nacional de Honduras así como, telenovelas en alta definición.

La compañía de televisión mexicana Televisa empezó a hacer emisiones experimentales en HDTV a principio de los años 90 en colaboración con la compañía japonesa NHK pero estas eran señales analógicas en formato MUSE (formato que fue reemplazado por ISDB-T) lo que hizo a la primigenia HDTV Mexicana comercialmente inviable. Al día de hoy, y debido a la reforma a las telecomunicaciones del gobierno del presidente Enrique Peña Nieto, se ha logró convertir la totalidad de las señales analógicas a digitales antes de finalizar el 2015.

Durante la primera mitad de 2005, al menos un proveedor de cable en la Ciudad de México, Cablevisión (hoy Izzi Telecom), empezó a ofrecer cinco canales en HDTV a los suscriptores que comprasen un grabador digital de vídeo (DVR).

A mediados de abril de 2010, el servicio de pago satelital SKY lanzó su servicio HD para los suscriptores que comprasen un decodificador SKY+HD. Actualmente el servicio de SKY ofrece dos decodificadores con tecnología HD y Full HD, los cuales son SKYHD y SKY+HD; ambos decodificadores ofrecen 35 canales con estas tecnologías, además de ofrecer diferentes eventos comprados individualmente.

En diciembre de 2010, el servicio de pago satelital Dish Network México lanzó su servicio HD con 6 canales más una antena HD.

En 2004 Televisa realizó su primera telenovela en HD presentada en 1080i/16:9: Rubí.

El 1 de enero de 2008 Televisa comenzó la distribución de todo su contenido de noticias en HD 1080i/16:9.
El 7 de enero de 2008 TV Azteca comenzó la distribución de todo su contenido de noticias en HD 1080i/16:9.

El estándar seleccionado para la transmisión de estas señales fue el estadounidense ATSC.

El 27 de octubre de 2016 se emitió un decreto para reasignar canales virtuales a nivel nacional; actualmente, los canales virtuales son asignados por cadena (ya sea nacional o regional) por el IFT para su fácil identificación, por ejemplo, los retransmisores de Las Estrellas siempre se redireccionan al canal 2.1 sin importar la población o frecuencia del canal real. Sin embargo, algunas estaciones que transmiten en poblaciones en la frontera norte, tienen un canal distinto al asignado de manera provisional al estar estos canales siendo utilizados por otros canales en Estados Unidos.

El sistema ISDB-T fue adoptado para la televisión digital en el país.
<br>
En Paraguay hay versiones en alta definición de los siguientes canales de TV abierta:










También en TV Paga (cable y satélite) hay versiones en alta definición de los siguientes que son: Canal 5 HD, Unicanal HD, E40 TV HD, HEI HD, Tigo Music HD, Tigo Sports HD, Tigo Sports 2 HD, Tigo Sports 3 HD, Tigo Sports 4 HD, CDF HD, Personal Sports HD, One Sports HD, One Sports Plus HD, One Sports Extra HD y Canal 20 HD.

En noviembre de 2006, el Ministerio de Transportes y Comunicaciones publicó en el diario oficial "El Peruano", una norma acerca de la implementación de la televisión digital terrestre (TDT) en el país y las bases para iniciar las transmisiones experimentales. En tal sentido, se fijó la reserva de la banda 470-698 MHz correspondientes a los canales de TV (14 - 51) para el desarrollo de la TDT nacional.
Frecuencia Latina transmitió en HD el Mundial Femenino de Voleibol 2016 en lo que fue una de las primeras transmisiones en el Perú. El 19 de julio de 2007, el canal Andina de Televisión (ATV) empezó con las emisiones experimentales en alta definición usando el estándar digital ATSC estadounidense, realizadas en el canal 30 UHF de Lima. ATV HD transmitió algunas películas, clips musicales y eventos que la televisora emitía en su señal en definición estándar. Las pruebas en ATSC siguieron hasta agosto de 2009, cuando ATV migra al estándar ISDB-Tb, designado como el oficial por el Gobierno, dentro del canal 29 UHF de Lima. Previamente, en enero de 2009, América Televisión inicia transmisiones experimentales de televisión digital a través del canal 31 UHF para Lima, usando el estándar ISDB-Tb y contando con la asesoría de técnicos japoneses y brasileños. Tres meses después, el 23 de abril, el gobierno aprueba el mencionado estándar para las transmisiones de TV digital. El sistema finalmente utilizado y autorizado a partir del año 2009 es el ISDB-Tb. 

El 30 de marzo del 2010, se lanzó oficialmente la señal digital del canal TV Perú (estándar ISDB-Tb) a través del canal 16 UHF dentro del sistema de televisión digital terrestre, en una ceremonia realizada desde el Palacio de Gobierno, convirtiéndose en el primer canal peruano en transmitir en este nuevo formato. El 31 de marzo de 2010, ATV lanzó oficialmente sus transmisiones digitales en ISDB-Tb dentro del canal 18 UHF en Lima y Callao en una ceremonia inaugural que contó con la presencia del entonces Presidente de la República, Alan García Pérez. El primer programa regular emitido en HD fue la telenovela "¿Dónde está Elisa?", que se estrenó el 25 de marzo. Más adelante, se estrenaron las telenovelas "Los Victorinos" y "El cartel de los sapos" en HD. El 30 de agosto de 2010, ATV lanzó el programa "Magaly TeVe" en alta definición, lo que lo convirtió en el primer programa de televisión en Perú en ser producido en HD.

Posteriormente, los canales Frecuencia Latina, América Televisión y Global TV (actualmente América Next) lanzaron sus señales oficiales en HD entre 2010 y 2011. Los últimos canales nacionales de televisión abierta en lanzar la señal en alta definición fueron Panamericana Televisión, que inició sus pruebas en abril de 2012 y lanzó su señal oficial recién en abril de 2014, y RBC Televisión, que lanzó su señal HD en agosto del mismo año.

Actualmente, la gran mayoría de las producciones se emiten en HD. Además, durante la década de 2010 todos los canales modificaron sus señales estándar (SD) a la resolución 1080i . Se espera el apagón analógico para el año 2020. Los canales en alta definición en la televisión abierta son: 










También hay versiones en Alta Definición en TV Paga que son: Canal N HD, ATV+ HD, Movistar Deportes HD, Movistar Plus HD, RPP TV HD y Gol Perú HD.

En la República Dominicana el canal 5 de la televisión local, Telemicro, a mediados del verano de 2008, anuncia que para el primer trimestre de 2012 se emitirán transmisiones en HDTV. Tricom oferta servicios de canales en alta definición en sus planes de cable premium siendo en la actualidad una de las empresas que en la actualidad está dando en servicio en República Dominicana. También la empresa Claro está brindando servicio de canales de HD a través de su servicio de IPTV.

La mayoría de las compañías dominicanas de TV por cable y satélite iniciaron su conversión a señal en alta definición.

En Uruguay hay versiones en alta definición de los siguientes canales:








En TV Paga hay versiones en Alta Definición en los siguientes canales: VTV HD, VTV 2 HD, VTV 3 HD, VTV Sports HD, TCC Sports HD y MC Sports HD.

A nivel paneuropeo se han puesto versiones en Alta Definición (HD) de los siguientes canales




En Latinoamérica hay versiones de los canales en alta definición











Tras una larga polémica entre el gobierno y las emisoras, el formato ATSC fue elegido frente a DVB-T. 2005 fue la fecha en que los servicios digitales pasaron a estar disponibles en todo el país.

Se impuso que se emitieran al menos 10 horas cada semana contenidos en HD durante el primer año del servicio digital comercial.

Japón ha sido pionero en HDTV por décadas con una implementación analógica. Su antiguo sistema no es compatible con los nuevos estándares digitales. En Japón, la emisión terrestre de HD con codificación MPEG-2 por ISDB-T empezó en diciembre de 2003. Hasta la fecha se han vendido ya dos millones de receptores HD en Japón. Dentro de las ventajas está que es el único estándar que permite transmisión conjunta a aparatos fijos (casa), portátiles (TV portátil, notebook o PDA) y móviles (teléfonos celulares) con una señal transmitida, lo que lleva a un ahorro del espectro radioeléctrico y costos de infraestructura.

Por ahora, muchos de los países muestran interés por la HDTV. Lo más común es la EDTV usando DVB.

Aunque la HDTV aún es posible con DVB-T, en la mayoría de los países se prefiere que existan más señales en un solo canal múltiplex, en lugar de un solo canal para HDTV, más común en EE. UU., Canadá, Japón y Australia. Como un solo canal de HDTV ocupará el ancho de banda de cuatro canales de SDTV (8 MHz), la HDTV no conviene para las necesidades de emisión terrestre en Europa. Además, algunos gobiernos quieren pasar a adoptar la señal digital para televisión en lugar de reasignar las frecuencias VHF para otros usos. El códec H.264 puede ser la clave del futuro del éxito de la HDTV en Europa, Una nueva versión de DVB-T (DVB-T2), será clave para la implantación del HD en Europa, permitiendo colocar simultáneamente varios canales FullHD y 3DTV en el mismo multiplex.

En enero de 2005, la EICTA anunció planes para una etiqueta ""HD ready"" (apto para HD) para equipos que cumplan ciertos requisitos, incluyendo el soporte de 720p y 1080i a 50 y 60 Hz. Las pantallas deben incluir interfaces YUV y DVI o HDMI y tener una resolución vertical nativa de 720 líneas o más.

La Comisión Europea analizó el estado de las emisiones a 16:9, así como de la HDTV en el documento "The contribution of wide-screen and high definition to the global roll-out of digital televisión" ("La contribución de la pantalla ancha y la alta definición a la expansión global de la televisión digital").

Este documento expone que los anteriores objetivos para la introducción a ámbito europeo de la HDTV de 1999 (o HD-MAC en 1992) no se alcanzaron porque el mercado se enfocó en las tecnologías digitales y servicios más fáciles de implementar. Por tanto, los consumidores europeos nunca tuvieron la oportunidad de probar la HDTV.

También apunta algunas causas de la pobre representación de la HDTV en la Unión Europea (UE):

Más tarde, los comisionados sugirieron que se requería alguna coordinación en la UE para servicios HDTV para que ésta estuviese al alcance de todos los estados miembros.

La plataforma de pago Sky Deutschland, entonces llamada Premiere, comenzó a emitir tres canales HD en diciembre de 2005. Estos canales presentan distintos contenidos (películas, deportes y documentales). Igual que HD1 los canales de Premiere usan H.264 y DVB-S2 como método de compresión. En diciembre de 2006 el canal de alta definición de deportes fue integrado con el de películas. En la actualidad, Sky cuenta con varios canales en alta definición en su oferta.

En 2006, las cadenas privadas ANIXE, Sat.1 y ProSieben empezaron a emitir en alta definición. Más tarde se añadirían las cadenas del grupo RTL. Si bien las privadas empezaron emitiendo estas versiones en alta definición en abierto, en la actualidad son de pago, estando integradas en la plataforma HD+, así como por la televisión por cable.

En 2010, los operadores públicos ARD y ZDF empezaron sus emisiones en alta definición. Estos emiten a 720p, al contrario que las privadas y las de pago, que usan el formato 1080i.

En Alemania, la alta definición se distribuye por satélite y cable, pero no por TDT.

En España Aragón Televisión realiza las primeras pruebas de Televisión de Alta Definición sobre TDT en España el 15 de junio de 2006, emite tanto en 720/50p como en 1080/25i usando la compresión MPEG4/H.264 a 11 Mbit/s. La primera cadena con emisiones en Alta Definición en pruebas regulares fue TV3 (Televisión de Catalunya) que comenzaron el 23 de abril de 2007, coincidiendo con el Día de San Jorge, desde el repetidor de la montaña de Collserola, que da señal a la ciudad de Barcelona y alrededores. Durante el período de pruebas, TV3HD emitió una pequeña selección de series y contenidos de sus otras dos cadenas en un canal que hasta entonces era usado como canal de pruebas e información sobre la TDT. Desde el 11 de agosto de 2008, TV3HD fue sustituida en el múltiplex 43 por una emisión del canal autonómico valenciano Canal 9 en definición estándar, así como una versión digital de K3/33. La señal de TV3HD retomó sus emisiones el 18 de junio de 2009, por la misma frecuencia que en las pruebas anteriores. Desde agosto de 2009 la televisión autonómica de la Región de Murcia (7RM) tiene un canal de alta definición (7RM HD).

En la primera fase, TV3HD emitía en formato MPEG4 H.264 a 10,2 Mbit/s en su primera fase, con lo que se conseguía evitar usar más ancho de banda (respecto del que consumiría el MPEG-2) del ya limitado y saturado espectro español. La resolución usada fue 1440×1080i, con lo que los contenidos panorámicos eran anamórficos. La pista de sonido era Dolby Digital 2.0 a 192 kbit/s. Actualmente emite de nuevo en esa resolución y formato, pero con un ancho de banda inferior.

Aragón Televisión comenzó a emitir en pruebas de forma regular en junio de 2008 y con contenidos propios en junio de 2009, siendo el primer canal comercial de España que emite contenidos y programación HD en abierto. Además en 2008 se creó la Plataforma TDT HD entre la Corporación Catalana de Medios Audiovisuales y la Corporación Aragonesa de Radio y Televisión junto con TV3 y ATV. A dicha plataforma se han sumado las Corporaciones y Televisiones de Asturias, Baleares, Valencia y Murcia además de la FORTA como organismo autónomo. Su iniciativa se basa en el intercambio técnico y de contenidos en Alta Definición.

TVE lanzó en 2010, después del apagón analógico, TVE HD, canal en alta definición que durante los Juegos Olímpicos de Pekín 2008 estuvo disponible en las plataformas Canal+ e Movistar TV. El canal inició sus emisiones en pruebas a través de la TDT en la ciudad de Valladolid en junio de 2009. Actualmente, TVE HD está disponible en todo el territorio español.

La oferta privada de HD comenzó el 20 de septiembre de 2010 con el canal Telecinco HD, que emite actualmente un reescalado de la programación de Telecinco. A ese canal se le sumó el 28 de septiembre un nuevo canal: Antena 3 HD, el 1 de noviembre de 2010 dieron comienzo las emisiones de La Sexta HD cuyo formato de emisión es igual al de los canales de Gestevisión Telecinco y Antena 3.

Actualmente empiezan a despuntar la venta decodificadores de televisión digital (TDT) que soporten este formato en España, sin embargo los decodificadores compatibles con los servicios interactivos del estándar MHP apenas son comercializados a pesar de que ya ofrecen todas las cadenas españolas, con lo que la situación requerirá que los usuarios acaben por tener que cambiar su decodificador 1, 2 o incluso más veces para adaptarse a los cambios. El gobierno ha impulsado medidas que obligan a los fabricantes a integrar decodificadores digitales que cumplan la normativa HD MPEG-4 en los televisores de más de 21 pulgadas a partir de julio de 2010.

La empresa española Sogecable, propietaria de la plataforma de pago Digital+ efectuó algunas pruebas de transmisión de programas en Alta Definición sobre el satélite Astra el 16 de junio de 2005. En otoño de 2007 dicha plataforma empezó a distribuir un nuevo decodificador (iPlus) con soporte para la alta definición tanto en TDT como en satélite, con la intención de introducir más adelante su propia oferta de programación en alta definición. En enero de 2008 Sogecable lanzó el canal Canal+ HD, que emite en alta definición (1080i). Dicho canal emite desde enero de 2008 de forma regular. A esta primera señal se han ido sumando otras emisiones sobre la misma plataforma, a saber: Fox HD, Canal+ Liga HD, Canal+ HD Acción y HD Music.

Jazztel proveedor de Internet y telefonía preveía ofrecer algunos canales en Alta Definición a través de líneas ADSL2+ a mediados de 2006, pero es posible que no lo haga hasta que empiecen a instalar tecnología VDSL2 como Telefónica.

Telefónica ofrece desde 2007 TV de Alta Definición en su plataforma de televisión digital Movistar TV. En principio, sólo es posible para usuarios de Movistar TV conectados mediante VDSL2 y FTTH y no en ADSL2+.

La empresa propietaria de la plataforma de cable ONO anunció que lanzaría un decodificador adaptado para la HDTV a comienzos de 2008.

Además en España se pueden encontrar contenidos de Alta Definición en la emisión por satélite para toda Europa de los canales HD1, HD2 y HD5 (Plataforma Euro1080 que emiten en 1080i y las plataformas de videojuegos en Alta Definición tales como Xbox 360 (1080p) y PlayStation 3 (1080p) cuyos videojuegos y otros contenidos descargables son la mayoría en el formato 720p (aunque se pueden ver en 1080i/p). Alternativamente a través de un PC también es posible disponer de contenidos en Alta definición desde hace pocos años, si está equipado adecuadamente con una tarjeta gráfica para la reproducción de juegos en Alta definición o simplemente un procesador con capacidad de proceso suficiente para decodificar el contenido, en el caso de vídeo en alta definición. También es posible equipar el PC con una unidad lectora de discos Blu-ray o HD DVD para la reproducción de discos de alta definición, aunque determinados sistemas de protección DRM pueden requerir dispositivos específicos como cableado o pantalla de visualización.


En 2005, TF1, M6 y TPS expresaron su intención de emitir algunos programas en HD. Además desde septiembre de 2005 los canales de televisión por suscripción usan H.264.

Desde el 5 de abril de 2016, la televisión digital terrestre francesa (TNT), pasó a utilizar exclusivamente el códec MPEG-4 AVC (H.264) y por tanto emitiendo todos los canales de la TNT únicamente en alta definición.

En el Reino Unido la BBC ya produce algunos programas (principalmente documentales) en HD para mercados extranjeros, como Estados Unidos y Japón. La Corporación tiene intención de producir todos sus programas en HD para el año 2010. Se cree que la BBC aprobará 720p por su tecnología de barrido progresivo.

No hay planes para versiones en HDTV de Freeview y Top Up TV para servicios de televisión terrestre digital, debido a que no hay ancho de banda libre disponible. Esto debe cambiar después de que se desconecten las señales de televisión analógica, pero la fecha para esta desconexión aún se está debatiendo.

La plataforma de pago Sky tenía previsto lanzar sus servicios HD por satélite en 2006. Estará limitado a algunos canales y eventos especiales. Sky ha confirmado que estarán disponibles tanto 720p como 1080i.

Los informes recientes sugieren que Sky pondrá en disposición eventos deportivos de primera línea, como partidos de fútbol, para pubs antes del lanzamiento del servicio HD doméstico.

Casi se puede asegurar que los proveedores de televisión por cable se actualizarán a HD tan pronto los canales estén disponibles.

Australia empezó a emitir en HD en enero de 2002 pero el contenido en HD no fue obligatorio hasta agosto de 2003. La mayoría de las ciudades australianas de más de 40 000 habitantes disponen de al menos un canal de TDT (por ejemplo, en Albany (Australia Occidental), se dispone de TDT desde mayo de 2005). De todas formas, la mayoría de las emisoras australianas de TDT están aún experimentando con transmisiones en HDTV.

HDTV puede grabarse en D-VHS (Data-VHS), W-VHS, o en una grabadora de vídeo digital que soporte HDTV como la TiVo ofrecida por DirecTV o las DVR 921 y DVR 942 ofrecidas por DISH Network. A 2008, en los Estados Unidos la única opción de grabado es D-VHS. D-VHS graba en forma digital en una cinta VHS cualquiera, requiriendo un transporte digital FireWire (IEEE 1394) para acoplar la trama comprimida MPEG-2 desde el dispositivo modulador hasta la grabadora. También puede darse una captura directa de las señales HD a un dispositivo de almacenamiento como una grabadora de vídeo digital, o el disco rígido de una PC si ésta cuenta con una tarjeta de decodificación de HDTV, dado que no se involucra ningún tipo de recompresión inicial el tamaño del archivo depende de la tasa de bits de la transmisión original (hasta 19,2 Mbit/s), ya que esta simplemente se vuelca al disco rígido (es posible recomprimir el contenido posteriormente a un formato más avanzado -DivX, H.264- si se desea ahorrar espacio en Disco). Además ya empiezan a aparecer tarjetas de captura de vídeo que aparte de almacenar señales HDTV en disco rígido, pueden capturar contenidos en HD de otras fuentes que no sea la señal de televisión (Por ejemplo consolas de videojuegos como la Xbox 360 y la PS3) sin realizar ningún tipo de compresión, como la [ Intensity Pro] (sitio en español) de Blackmagic Design, cabe señalar que el poder de cómputo necesario para capturar y procesar esas señales es relativamente elevado para equipos medios de 2008, requiriendo discos rígidos con transferencia de 3 Gbs y ordenadores con procesadores de núcleo dual.

Como parte del acuerdo sobre "plug and play" que emitió la FCC, las compañías de cable deben de proveerles a un puerto funcional de IEEE 1394 a aquellos clientes que alquilen cajas HD (si estos lo pidiesen). Ningún proveedor de DBS ha ofrecido esta característica en ninguna de las cajas que ofertan. En julio de 2004 estos puertos todavía no aparecían en el mandato de la FCC. El contenido disponible está protegido por un cifrado que limita o bloquea completamente la capacidad de grabarlo.

La programación HD puede ser grabada a un disco óptico utilizando las tecnologías Blu-ray.
Actualmente los mayores impulsores de ambos formatos se pueden considerar Xbox 360, que se le puede conectar un reproductor de HD DVD externo (actualmente obsoleto) y tiene contenidos descargables en alta definición, y PlayStation 3, que incluye un reproductor Blu-ray tanto para los juegos como para ver películas en Alta Definición. Ambos sistemas de videojuegos tienen la mayoría de juegos y contenidos en 720p aunque permiten 1080p (las películas en HD DVD y Blu-Ray son todas 1080p). Este hecho puede suponer un gran impulsor de la alta definición.

En un esfuerzo por crear un formato de alta definición compatible con los "bit rates" para los vídeos de alta definición en los DVD-ROM estándar, Microsoft introdujo el códec del Windows Media 9 Series con la habilidad de comprimir un bitstream de alta definición en el mismo espacio que un bitstream NTSC convencional (que es de aproximadamente 5 a 9 Mbit/s para las resoluciones de 720p en adelante). Microsoft lanzó el códec de alta definición de la serie Windows Media 9 como el "WMV HD". Hace falta ver si el códec será adoptado ampliamente, o al menos como un estándar de la industria Hi-Fi. En noviembre de 2003 el formato WMV HD requería de un poder de procesamiento significante para poder codificar y descodificar una película, como resultado la única película disponible comercialmente que hacía uso del códec era ": Edición extrema" en DVD. Desde entonces más títulos han sido lanzados en el formato WMV HD DVD tal como el aclamado documental sobre el surf "Step Into Liquid" (título pendiente en español). A inicios del 2005 Microsoft recomendaba un procesador de 3,0 GHz con 512 MiB de memoria RAM y una tarjeta de vídeo de 128 MiB de memoria como requisitos mínimos para poder reproducir una película en la resolución 1080p en Windows XP aunque ya había reproductores en el mercado, como el KiSS DP-600, que ya podían reproducir discos de WMV HD DVD ROM en televisores con capacidad HD. El códec fue mandado a la SMPTE (Sociedad de ingenieros de películas y programas televisivos, por sus siglas en inglés) y se ha convertido en el estándar de la SMPTE, conocido como VC-1, incluido en todas las películas en formato HD DVD, y de los lanzamientos recientes en Blu-Ray (cuyos primeros títulos estaban en MPEG-2).

Aunque su salida ha sido muy posterior a la definición del formato, el propio sistema de videojuegos de alta definición de Microsoft, la XBOX 360, es compatible directamente o a través de un ordenador con el sistema operativo Windows XP Media Center, con el formato WMV HD desde la actualización de 31/10/2006, y desde enero de 2007 permite contenedores H.264 y MPEG-4 aunque no a través de Windows Media Center de momento.

Otros códecs, tales como el AVC (que es la parte 10 de MPEG-4 y también es conocido como "H.264"), han sido aprobados por los grupos de estándares ITU-T ("Sector de Normalización de las Telecomunicaciones de la Unión Internacional de las Telecomunicaciones", por sus siglas en inglés) y MPEG ("Grupo de Expertos en Imágenes Móviles", por sus siglas en inglés) y también se han aprobados los códecs "VP6" y "VP7" que fueron diseñados por "On2 Technologies".

Las compañías de difusión más grandes en Estados Unidos y Europa ya han adoptado el estándar H.264. Estas compañías incluyen: DirecTV y el [DISH Network] en Estados Unidos y BSkyB (sitio en inglés), Premiere (sitio en alemán), Canal+ y TPS (sitio en francés) en Europa. El estándar H.264 fue elegido por varias razones: la primera es que el estándar fue validado como un "estándar abierto" por lo menos un año antes que el VC-1 siquiera fuese considerado seriamente como un estándar y en aquel entonces existían dudas sobre los reglamentos que Microsoft podría imponer una vez que el algoritmo fuese adoptado. Hasta la fecha sólo unas pocas compañías de difusión han considerado el estándar VC-1. Se había pensado que el VC-1 hubiese sido mejor que el H.264 para el entorno de IPTV, pero de acuerdo a comunicados de prensa hechos por compañías que manufacturan STB (cajas de cable o satélite, por sus siglas en inglés) (tales como Amino (sitio en inglés), Pace, InfoMir (sitio en inglés) y Kreatel (sitio en inglés)) se ha demostrado que existen soluciones basadas en los estándares H.264.

Existen rumores de que Microsoft puede haber tomado el estándar H.264 y lo habría modificado y mejorado para comercializarlo como el VC-1 sin dar crédito alguno a la MPEG-LA. Sin embargo esto sigue siendo un rumor y nunca se ha confirmado o negado oficialmente.

Un ejemplo de las preocupaciones de los proveedores de cable se puede encontrar en este sitio (sitio en inglés).

On2 reportó que China había escogido el VP6 como el estándar para el formato Enhanced Versatile Disc (Disco versátil mejorado) (EVD). Supuestamente China quería evitar el tener que pagar por los derechos de uso del WM9 y el AVC. La ventaja de usar el VP6 hubiese sido que no se tendría que pagar derechos de uso en medios de grabación pero estos costos serían transferidos al precio de los reproductores a un costo similar al de otros códecs. A medida que China comienza a dominar la manufactura de televisores y reproductores de DVD, sus decisiones en cuanto a estándares cobra más peso.

El hecho que un códec tenga un bajo costo no significa que sea una ventaja sobre el formato DVD, además los reproductores serían incompatibles con el formato DVD-vídeo a menos que se paguen los derechos de uso de las tecnologías que son necesarias para hacer que el reproductor pueda reproducir DVD. Se lanzaron muy pocos videos en el formato VP6 por lo cual no se generó la suficiente fuerza como para obligar a que las personas compraran los reproductores VP6, los cuales no eran compatibles con el formato DVD. Es poco probable que este formato sea adoptado por un estudio fílmico en los Estados Unidos si no existe algún método de protección contra la piratería y esto tampoco se especificó. Poco tiempo después de que se anunciara que el VP6 sería el estándar de los EVD las negociaciones entre On2 y E-World (un grupo que apoyaba el uso del EVD como estándar) se deterioraron. On2 reportó varias violaciones de contrato por parte de E-World y On2 pidió que se arbitraran estas faltas pero en marzo de 2005 se falló en favor de E-World ya que se reconoció que E-World no había fallado en su parte del contrato y no le debía nada a On2. Nunca se clarificó si el gobierno chino realmente había adoptado el VP6 como estándar.

Recientemente el DVD Forum y la Blu-ray Association fallaron en llegar a un acuerdo en cuanto a los estándares para los discos de 12 cm de alta definición. En febrero de 2008, después de una guerra de formatos con su rival Blu-Ray, Toshiba abandonó el formato HD DVD, y el HD DVD Promotion Group, que promocionaba el estándar, se disolvió el 28 de marzo de 2008.

Los códecs de vídeo, tanto de Blu-Ray como del extinto HD-DVD son el MPEG-2 parte 2, VC-1 y H.264.

Actualmente ya se encuentra algunos reproductores de DVD que incluyen la capacidad de enviar señales de alta definición al televisor partiendo de DVD de definición estándar. Esto se hace escalando artificialmente la imagen, aunque algunas pantallas ya hacen el escalado por su cuenta para adaptar la imagen a la resolución real de la pantalla. La única mejora es la solidez de la imagen, al haber más píxeles representando el mismo píxel del contenido. Algunos fabricantes de reproductores de DVD, generalmente de marcas asiáticas poco conocidas, o algunas como Kiss o Philips, sacan licencias del códec DivX para que sus reproductores puedan reproducir contenido en 720p/1080i a partir de contenido grabado en discos DVD-R estándar.

El 19 de febrero de 2008, toshiba anuncia en su web que descontinuará el formato HD DVD debido a "diversos cambios producidos en el mercado".

Cita textual del comunicado:
"We carefully assessed the long-term impact of continuing the so-called 'next-generation format war' and concluded that a swift decision will best help the market develop," said Atsutoshi Nishida, President and CEO of Toshiba Corporation. "While we are disappointed for the company and more importantly, for the consumer, the real mass market opportunity for high definition content remains untapped and Toshiba is both able and determined to use our talent, technology and intellectual property to make digital convergence a reality".

"Hemos evaluado cuidadosamente los efectos a largo plazo de continuar lo que se ha denominado "Guerra de formatos de Nueva Generación" y se ha concluido que una rápida decisión es la mejor forma de ayudar al desarrollo de este mercado", dijo Atsutoshi Nishida, presidente y CEO de Toshiba Corporation. "Aunque esto es una decepción para la compañía, pero es más importante, para el consumidor, la oportunidad de un mercado masivo real de contenido en alta definición, que permanece sin explotar, y en Toshiba nos sentimos capaces y determinados a usar nuestro talento, tecnología y propiedades intelectuales para hacer la convergencia digital una realidad".

En 2003 JVC presentó la GR-HD1, la primera cámara digital de alta definición del mundo dirigida al mercado de consumo, grabando en 720/30p 16:9 con salida up-converted en componentes analógicos a 1080/60i y en 720/60p o salida vía firewire a 720/30p. Posteriormente en septiembre de 2004 Sony lanzó al mercado su primera cámara HD para uso personal llamada HDR-FX1. Dicha cámara puede grabar en el formato 1080i/60 (la versión PAL graba a 1080i/50) y es capaz de grabar en un cinta Mini-DV usando el formato HDV. La cámara utiliza el códec MPEG-2 para grabar vídeo y audio y el sistema "3-CCD" para añadir color correctamente. A causa de esto, la HDR-FX1 (en teoría) se aproxima mucho a una cámara HD profesional. Los programas iMovie HD, Final Cut Express HD y Final Cut Pro HD (con Lumiere HD instalado) de Apple son capaces de editar MPEG-2 HD/HDV en una manera muy estable. Se requiere de una Macintosh para poder ejecutar estos programas. Para los usuarios de PC, el Adobe Premiere Pro 1,5 y Sony Vegas 6 son capaces de editar HD. Cinelerra, un popular editor de vídeo de código abierto, también permite editar HDV y se puede ejecutar en una gama muy variada de arquitecturas de sistemas. Panasonic y Canon han lanzado cámaras que siguen el mismo formato que la cámara de Sony. Las cámaras utilizadas para transmisiones de televisión graban directamente a discos rígidos a través de un formato raw de input/output.




</doc>
<doc id="4897" url="https://es.wikipedia.org/wiki?curid=4897" title="Prostitución">
Prostitución

La prostitución es la práctica de mantener relaciones sexuales con otras personas a cambio de dinero u otros beneficios económicos. La prostitución es ejercida mayoritariamente por mujeres (llamadas «prostitutas») y niñas (prostitución infantil), mientras que los clientes son mayoritariamente hombres. También existe, en menor medida, la prostitución masculina, donde los clientes también son generalmente varones.

Tradicionalmente, la prostitución se ha ejercido en sitios destinados a este fin, llamados «burdeles» o «prostíbulos». Estos han sido habitualmente casas regentadas por un proxeneta, en las que hay prostitutas y habitaciones privadas para practicar la prostitución. También se practica en aceras de calles urbanas y laterales de carreteras industriales, así como en bares y discoteca, hoteles y a domicilio.

La figura de la prostituta está frecuentemente ligada a la del proxeneta, persona que induce a la prostitución obteniendo un beneficio económico de ello. Los proxenetas obtienen una parte de los beneficios de las prostitutas. Esta relación se puede dar de mutuo acuerdo a cambio de un servicio de mediación o protección, o bien se puede dar mediante extorsión, violencia física o secuestro. La prostitución forzada se engloba dentro del comercio ilegal de personas conocido como trata de personas.

La situación legal de la prostitución varía ampliamente en cada país. En la mayoría de los países se considera completamente ilegal. En otros la prostitución no es ilegal pero sí el proxenetismo. Algunos países nórdicos (Suecia, Noruega e Islandia) han adoptado un modelo donde el cliente comete un crimen, pero no la prostituta. También hay algunos países como Países Bajos o Alemania donde la prostitución es una profesión regulada.

Por otra parte, existen diferentes acepciones del término "prostitución". Así, por ejemplo, prostituir puede también ser considerado como ""deshonrar o degradar algo o a alguien abusando con bajeza de ellos para obtener un beneficio"", según el Diccionario de la lengua española de la Real Academia Española.

Otra acepción del término "prostitución" puede referirse al hecho de una persona prestarse a cosas moralmente censurables o vituperables (no necesariamente relacionadas con el aspecto sexual) por el simple hecho de obtener algún beneficio o prebendas.

El término «prostitución» proviene del latín "prostitutio", que tiene el mismo significado que el actual y que a su vez proviene de otro término latino, "prostituere", que significa literalmente "exhibir para la venta".

A lo largo de la historia ha existido una gran cantidad de términos tanto para referirse a la prostitución como a las personas que la practican, a los clientes, a los lugares y a las actividades relacionadas. Los distintos países de habla hispana usan distintos términos coloquiales como sinónimo de prostituta, con mayor o menor carga despectiva, existiendo una gran cantidad de términos en cada variante dialectal del español, algunos empleados históricamente, y otros aún en uso.

El término coloquial más extendido en los países de habla hispana para referirse a una prostituta es "puta", palabra que conlleva una fuerte connotación despectiva. De hecho, y debido a que suele emplearse como insulto, su uso ha sobrepasado el de la descripción de una profesión, y en muchos países se usa para adjetivar de forma grosera otro elemento, al estilo del término inglés "fucking".

En el latín vulgar "“puttus”" (muchacha o muchacho), proveniente del latín clásico "“putus”" (niña o niño). Existe un verso de fines del siglo I a. C. que usa dicha palabra con una connotación ofensiva, similar a la actual, donde se refiere a un "“amor de calle”". Sin embargo, en el portugués europeo, puto mantiene el significado de "‘muchacho’", sin connotación sexual alguna, mientras que en varios países de Hispanoamérica esta forma masculina se aplica despectivamente a los varones homosexuales no necesariamente prostitutos.

El término «loba» como equivalencia de «prostituta» viene de los ritos producidos en febrero en honor al dios Fauno Luperco. Eran llamadas lobas u originalmente "lupas" las que ejercían la prostitución sagrada con los sacerdotes de este dios, los "luperci", en el Ara Máxima.

De aquí deriva también «lupanar», que se emplea para referirse al prostíbulo (burdel o «casa de citas», es decir, el sitio al que llega el cliente a pagar por los servicios de una prostituta).

Una de las formas más antiguas de prostitución de la que existen registros históricos es la prostitución sagrada, practicada inicialmente en Sumeria. Ya desde el siglo XVIII a. C., en la antigua Mesopotamia se reconocía la necesidad de proteger los derechos de propiedad de las prostitutas. En el Código de Hammurabi se hallan apartados que regulan los derechos particulares de las hieródulas.

Por su parte, los antiguos historiadores Heródoto y Tucídides documentan la existencia en Babilonia de la obligación para todas las mujeres, al menos una vez en su vida, de acudir al Templo de Ishtar para practicar sexo con un extranjero como muestra de hospitalidad, a cambio de un pago simbólico. También en la Edad Antigua, la prostitución estaba bien presente en Cerdeña y Sicilia, así como en varias culturas fenicias, en las que se practicaba como rito religioso en honor de Astarté.

Sin embargo, el alcance y la naturaleza de este fenómeno está en disputa entre los historiadores.

La prostitución fue, desde la Época Arcaica, una actividad común en la vida cotidiana de las ciudades griegas más importantes. Particularmente en las zonas portuarias, daba trabajo, de forma legal, a un número significativo de personas, constituyendo una actividad económica de primer nivel. Ejercida tanto por hombres jóvenes como por mujeres de todas las edades, la clientela era mayoritariamente masculina.

Las prostitutas griegas pertenecían a distintas categorías, dependiendo de diversos factores relacionados con su trabajo: las "pornai", las prostitutas "independientes" y las "heteras"; además, existía una categoría específica de los templos sagrados, la de las "prostitutas sagradas", que se abastecía, habitualmente, de "heteras".

Las "pórnai" eran, normalmente, esclavas propiedad de un proxeneta. Este propietario podía ser un ciudadano (también un o una meteco), para el que ese negocio constituía una fuente de ingresos como cualquier otra y por el que tenía que pagar un impuesto proporcional a los beneficios que le generaba. En la época clásica, las "pórnai" son esclavas de origen bárbaro; a partir del período helenístico, se incorporan al gremio muchas jóvenes esclavas, que solo dejarían de serlo cuando fuesen adoptadas por su amo. Su trabajo se desarrollaba en los prostíbulos, generalmente en los barrios conocidos por esta actividad, tales como El Pireo (puerto de Atenas) o el Cerámico de Atenas. Son frecuentadas por los marinos y los ciudadanos pobres.

Las prostitutas independientes trabajaban directamente en la calle. Estas prostitutas son de orígenes diversos: mujeres metecas que no encuentran otro empleo en la ciudad de llegada, viudas pobres, antiguas "pornai" que han logrado independizarse. En Atenas, debían estar registradas y pagar un impuesto. Se puede, también, incluir en esta categoría a las músicos y bailarinas que ofician en los banquetes masculinos. Aristóteles, en la "Constitución de los atenienses" (L, 2), menciona entre las atribuciones específicas de diez magistrados (cinco "intra muros" y cinco para el Pireo), el "ἀστυνόμοι", "astynómoi", o cargo de velar porque «las instrumentistas de flauta, de lira y de cítara no sean alquiladas por más de dos dracmas por noche»; queda así claro que los servicios sexuales eran claramente parte del alquiler cuyo precio, a pesar del control practicado por los "astynomes", tiende a ser más elevado cuanto más corre el tiempo.

Las heteras constituyen la categoría más alta entre las prostitutas. A diferencia de las otras, no ofrecen sólo servicios sexuales y sus prestaciones no son puntuales. Comparables en cierta medida a las geishas japonesas, poseen una educación esmerada y son capaces de tomar parte en las conversaciones entre gentes cultivadas. Únicas entre todas las mujeres de Grecia, espartiatas aparte, son independientes y pueden administrar sus bienes.

La ofrenda a las divinidades en forma de mujeres-prostitutas no alcanzó en Grecia una amplitud comparable a la que existió en el Próximo Oriente antiguo; no obstante, se conocen varios casos. Por un lado, dentro del propio mundo griego, hubo prostitución sagrada en Sicilia, en Chipre, en el reino del Ponto o en Capadocia; por otro, la hubo también en Corinto, cuyo templo de Afrodita alojaba una importante tropa servil, al menos después de la época clásica. Así, en 464 a. C., un tal Jenofonte, ciudadano de Corinto y vencedor de la carrera a pie y del pentatlón en los Juegos Olímpicos, dedicó a Afrodita, en signo de agradecimiento, cien jóvenes mujeres al templo de la diosa.

La prostitución en la antigua Roma era símbolo de vergüenza. La falta de reputación era reflejada en la ley, la cual, en la República Tardía y principios del Principado, la clasifica a sus practicantes como «infames» —traducido como «falta de reputación»—. Los fragmentos de fuentes legales sobre la prostitución son primariamente encontrados en el Cuerpo de Derecho Civil que fue compilado en los primeros años del siglo VI.

La prostituta era un personaje sugestivo en la literatura de la antigua Roma. Era muchas veces invocada como recurso literario, una metáfora para lo corrompido. Eran notadas por su vestimenta, vestidos de colores chillones hechos de lino transparente. También se distinguían por usar una toga, que eran ropas usadas típicamente por hombres romanos. Por ende, se ha dicho que la prostituta no era ética para el hombre. Para muchos escritores romanos, la prostitución representaba la más degradante forma imaginable de existencia para una mujer, representando lo más profundo de la impureza. Las asociaban con la suciedad, lo que realzaba aún más su bajo rango.

Los proxenetas en la antigua Roma también eran sujetos de «infamia». El proxenetismo era el acto de obtener ganancia por las acciones de la prostituta. Esto era mediante el manejo de las mismas, buscando clientes o siendo dueños de un burdel. Estos tipos de asociaciones con la prostitución eran mirados con desdén y estigmatizados por la sociedad romana. Esto era reflejado claramente en la ley romana: «La ocupación de un proxeneta no es menos degradante que la práctica de la prostitución y el crimen por ello es incluido en las "Leges Juliae", como una pena reservada contra el marido que tenga ganancias monetarias por el adulterio de su esposa».
Durante la Baja Edad Media la prostitución fue objeto de críticas morales y una reglamentación más o menos permisiva. La prostitución podía estar confinada en determinados barrios y estar restringida en determinadas fechas, como la Semana Santa. La erradicación de la prostitución no se concebía posible, dado lo inevitable del pecado, y su papel de mal menor que evitaba que el deseo irrefrenable de los varones fuera en contra del honor de las doncellas y las mujeres "respetables" y se consideraba que evitaba la homosexualidad.

Algunos burdeles eran regentados por los propios municipios, y desde mediados del siglo XIV, estos concejos o asambleas de vecinos regulaban la prostitución arrendando los establecimientos a los padres de la mancebía que controlaban rigurosamente a las prostitutas, que debían ser solteras, con buena salud y someterse regularmente a inspecciones sanitarias y de higiene corporal. Entre los padres de la mancebía se encontraban caballeros de alto rango que participaban en un negocio muy lucrativo.

Mientras tanto, en la América precolombina, las prostitutas del pueblo azteca se clasificaban entre aquellas que se prostituían como parte de un intercambio económico, y las que cumplían una función ritual como acompañantes de los guerreros, con quienes tenían la posibilidad de casarse.

Hacia finales del siglo XV se endureció la visión negativa de la prostitución. Un brote de sífilis en Nápoles durante 1494, que más tarde se extendería por Europa, podría haber tenido origen en el intercambio colombino. La prevalencia de otras enfermedades de transmisión sexual a principios del siglo XVI emergió la asociación entre prostitutas, plagas y contagio, causando la prohibición de la prostitución y los burdeles por parte de las autoridades seculares. El derecho canónico definía una prostituta como "una mujer promiscua, independientemente de elementos económicos." La prostituta era considerada una “puta … disponible para la lujuria de muchos hombres,” y se asociaba estrechamente con la promiscuidad.

Además a una dama que vivía o servía en la Corte, cortesana también era en Occidente el nombre para las prostitutas de lujo, cuyos caros servicios solo podían permitirse hombres poderosos o adinerados. Al principio aludía a las amantes que algunos reyes mantenían en palacio, y, por extensión, desde el siglo XVIII se convirtió en sinónimo de prostituta de lujo. Célebres cortesanas fueron las emperatrices Mesalina y Teodora, Madame du Barry, amante del rey Luis XV en el siglo XVIII, Marie Duplessis y Lola Montez en el siglo XIX y Liane de Pougy, La Bella Otero y Mata Hari durante la Belle Epoque.

En el siglo XIX se desató una polémica pública tras la aprobación en Francia y más tarde en Reino Unido de leyes de enfermedades contagiosas. Esta legislación obligaba a las mujeres sospechosas de ser prostitutas a someterse a exámenes pélvicos, tanto en Francia y Reino Unido como en sus colonias. Muchas feministas lucharon por derogar estas leyes, bien porque la prostitución debería ser ilegal y, por lo tanto, no regulada gubernamentalmente, o bien porque forzaba a las mujeres a someterse a exámenes médicos degradantes. La situación era similar en el Imperio Ruso.

El Reino Unido adoptó una política de segregación social en el Raj británico (actual India), pero mantuvieron los burdeles llenos de mujeres indias. A finales del siglo XIX y principios del XX, existía una red que prostituía a mujeres chinas y japonesas en países como China, Japón, Corea, Singapur y el Raj británico. También existía una red que prostituía a mujeres europeas en India, Sri Lanka, Singapur, China y Japón durante el mismo periodo. El destino más común para las prostitutas europeas en Asia eran las colonias británicas de India y Ceilán, donde cientos de mujeres y niñas de la Europa continental y Japón servían a los soldados británicos.

En 1921, la Liga de las Naciones firmó la Convención Internacional para la Supresión de la Trata de Mujeres y Niños. En esta convención, algunas naciones declararon reservas respecto a la prostitución.

Los principales teóricos del comunismo se oponían a la prostitución. Los gobiernos comunistas a menudo tomaron pasos para reprimir la prostitución, aunque la práctica persistió. En los países que siguieron siendo nominalmente comunistas tras la Guerra Fría, especialmente en China, la prostitución siguió siendo ilegal y, sin embargo, común.

Durante la Segunda Guerra Mundial, los soldados del Imperio del Japón participaron en prostitución forzada durante sus invasiones en Asia Oriental y Sudeste Asiático. El término "mujeres de consuelo" se convirtió en un eufemismo para entre 20.000 y 400.000 mujeres coreanas y japonesas que fueron forzadas a prostituirse en burdeles del Ejército Imperial Japonés durante la guerra.

La Declaración de Viena sobre la eliminación de la violencia contra la mujer, aprobada por la Organización de Naciones Unidas en 1993, reconoce la prostitución como una forma de violencia contra las mujeres. La trata de personas se ha vuelto un tema prioritario para la Organización Internacional para las Migraciones (OIM), ya que las cifras conocidas dicen que hay cientos de miles de mujeres y niñas que son víctimas de la trata para explotación sexual a través de fronteras internacionales. 

La Convención sobre la Eliminación de Todas las Formas de Discriminación contra la Mujer sostiene, en su artículo 6, que los estados partes deberá tomar todas las medidas apropiadas, incluso de carácter legislativo, para suprimir todas las formas de trata de mujeres y explotación en la prostitución de la mujer. Considera que la trata de mujeres y la prostitución forzada son formas de violencia contra las mujeres. Sostiene que las causas fundamentales de la trata con fines de explotación sexual están directamente vinculadas al sistema social de la prostitución. Que la prostitución y la explotación sexual generan el tráfico de personas. También se afirma que los perpetradores gozan de una impunidad generalizada y que las mujeres son objeto de formas extremas de violencia. Por eso proponen desalentar la demanda sexual como forma de prostitución para desmantelar el sistema que utiliza a las mujeres en situación de vulnerabilidad. 

El Convenio para la represión de la trata de personas y de la explotación de la prostitución ajena de Organización de las Naciones Unidas (ONU) establece que los estados no tienen potestad para controlar, perseguir, someter a exámenes médicos, registrar o cobrar impuestos a las personas que estén en situación de prostitución y sí están obligados a perseguir a proxenetas y tratantes, como a generar políticas públicas para quienes quieran salir de la prostitución. También establece que se comprometen a castigar a toda persona que, para satisfacer las pasiones de otra, aun con el consentimiento de tal persona.

A finales del siglo XX emergió el turismo sexual como un aspecto controvertido del turismo occidental y la globalización. El turismo sexual es normalmente llevado a cabo por turistas internacionales provinientes de países más ricos.

En la prostitución callejera, la prostituta busca clientes en la vía pública, esperando en una acera o esquina. Una vez un cliente contacta, el acto sexual se suele dar en el coche del cliente, en un lugar apartado en la calle o en una habitación alquilada. Los hoteles habituales alquilan habitaciones por horas.

Los términos "burdel", "lupanar", "prostíbulo" y "mancebía" designan uno de los tipos de lugar en donde se practica la prostitución. En algunos casos en el establecimiento no hay ninguna relación formal entre la prostituta y el local. Por costumbre, los clientes van a sabiendas de la alta concentración de prostitutas, y viceversa. En otros casos, el local y la prostituta tienen una relación establecida entre ambos, a cambio de un salario mínimo o de una comisión en las bebidas que le invitan. Ella debe cumplir con un mínimo de normas de la casa, como por ejemplo ir a "trabajar" un mínimo de días a la semana y cumplir con un horario mínimo. En ambos casos la prostituta termina su jornada en cuanto consigue un cliente dispuesto a contratar sus servicios.

Con frecuencia en los bares en donde la relación local-prostituta equivale a la relación entre un patrón y su trabajador(a), el cliente debe pagar una compensación para que la prostituta/o pueda excusarse del trabajo, bajo el concepto de que al marcharse, ella/el deja de generar invitaciones a bebidas por parte de los clientes, y al haber menos chicas/os, el bar pierde atractivo en la noche, por lo cual se reduce la clientela. En ambos casos, relación libre o formal entre el local y la prostituta, la prostituta se beneficia de un entorno de trabajo más seguro, mientras que el bar se beneficia de la atracción que ejercen ellas haciendo que aumenten la clientela y el consumo de bebidas.

Existe también la modalidad de los ""salón de masajes"", donde los "masajistas", además de los servicios de masajes se avienen a prácticas sexuales a cambio de dinero, ya sea como parte de un trato particular o como parte de la oferta del local. Las relaciones sexuales generalmente se realizan en los mismos apartados en los que se practican los masajes, aunque es posible efectuar tratos para llevar el servicio fuera del local. En estos casos, al igual que en los bares, el local recibe una compensación para que el masajista pueda retirarse o se considera como "comisión de servicio", por los que el local establece una tarifa mayor.

En algunas grandes ciudades los burdeles se concentran en los llamados barrios rojos. Zonas establecidas donde se tolera la prostitución.

Los servicios de "escort" o chicas de compañía se diferencian de otras formas de prostitución en que las actividades sexuales no están publicitadas explícitamente como incluidas en el servicio. El pago está asociado al tiempo y compañía de la escort. Sin embargo, es habitual la expectativa de que las actividades sexuales están incluidas. En este caso, los servicios sexuales pueden darse en el domicilio del cliente o una habitación de hotel, o bien en el domicilio de la escort. Las escort pueden ser independientes o trabajar para una agencia. Los servicios se suelen publicitar en Internet, en publicaciones regionales o guías telefónicas.

El turismo sexual consiste en viajar con el fin de tener relaciones sexuales con prostitutas o participar en otras actividades sexuales. Entre las razones por las que se recurre al turismo sexual se cuentan: una mayor tolerancia a la prostitución que en el país de origen, precios más bajos, privacidad o la preferencia por determinados grupos étnicos. También es una práctica extendida entre pederastas, que buscan una edad de consentimiento menor o permisividad respecto a la prostitución infantil.

En la trata de personas algunas víctimas están obligadas a prostituirse. Frecuentemente se trata de un fenómeno relacionado con la inmigración ilegal donde las mafias operan para secuestrar y vender a estas personas a otros países para prostituirse. La Oficina de Naciones Unidas contra la Droga y el Delito (ONUDD) ha puesto en marcha varias iniciativas para luchar contra esta lacra del tráfico de personas, especialmente de mujeres y niños.

Esta oficina define, en su generalidad, la trata de personas como la acción de captar, transportar, trasladar, acoger o recibir personas, recurriendo a la amenaza o al uso de la fuerza u otras formas de coacción, al rapto, al fraude, al engaño, al abuso de poder o de una situación de vulnerabilidad o a la concesión o recepción de pagos o beneficios para obtener el consentimiento de una persona que tenga autoridad sobre otra con fines de explotación.

Las Naciones Unidas, ya en 1949, promovieron una convención para el control de la prostitución y la lucha contra el tráfico de personas esclavizadas generado a su alrededor. Las Naciones Unidas declaró en 2009 que las estimaciones muestran que podría haber alrededor de 270.000 víctimas de la trata de personas en la Unión Europea.

La llamada prostitución infantil consiste en la utilización de menores de edad con fines de prostitución. Es ampliamente considerado como una forma de explotación sexual y está generalmente ilegalizada. La diferenciación legal respecto a la prostitución en adultos es dada por el hecho de que los niños por debajo de la edad de consentimiento no se consideran con capacidad de consentir relaciones sexuales, y por lo tanto cae en el ámbito legal del abuso sexual.

La amplitud del fenómeno es tal, que a nivel mundial son desarticuladas redes de prostitución de menores todos los años, a quienes además se les incautan regularmente material de pornografía infantil.

La mayoría de las prostitutas son víctimas de agresiones físicas o violaciones. Las prostitutas callejeras están expuestas a un mayor riesgo de agresiones.

Las diversas posiciones se agrupan en torno a: el prohibicionismo, el abolicionismo, el reglamentarismo y el regulacionismo.

El prohibicionismo consiste en perseguir la prostitución en todos sus aspectos. Considera la prostitución como una actividad inmoral y tanto las prostitutas como los clientes son tratados como criminales. Estas posiciones están asociadas a corrientes ideológicas conservadoras.

El abolicionismo considera la prostitución como una forma de violencia contra la mujer que debe ser abolida por completo. Por lo tanto, la prostituta no es vista como una criminal, sino como una víctima de explotación; mientras que los clientes y proxenetas son vistos como explotadores. El modelo nórdico, vigente en Suecia, Noruega e Islandia, es el referente legal del abolicionismo, ya que ilegaliza comprar sexo, pero no venderlo. De esta forma se persigue a los clientes para reducir la demanda y no a las prostitutas.

Desde las posiciones abolicionistas se tiende a utilizar la denominación "«mujeres prostituidas»" o "«mujeres en situación de prostitución»".

El reglamentarismo de la prostitución es un modelo teórico jurídico que considera que la prostitución es necesaria socialmente y debe ser controlada por el Estado. El reglamentarismo utiliza un sistema de control sanitario y policial, que es ejercido únicamente sobre las prostitutas y no sobre los clientes consumidores, con el objetivo de prevenir contagios masivos de enfermedades venéreas. La prostitución es permitida en ciertas zonas delimitadas.

Las posiciones regulacionistas buscan la despenalización del trabajo sexual y regular la prostitución como una profesión legítima. Desde un punto de vista liberal se puede considerar que prohibir la prostitución supone limitar la libertad individual de prostitutas, proxenetas y clientes. Se argumenta que la prohibición no consigue que la prostitución desaparezca, sino que empuja a las prostitutas a una clandestinidad en la que sus condiciones de vida empeoran.

Desde las posiciones regulacionistas se tiende a utilizar la denominación "«trabajador sexual»" para cualquiera que ejerza la prostitución, así como otras profesiones relacionadas con el sexo; denominación que recibe críticas por parte de sectores abolicionistas y moralistas, al considerar que el sexo no es un bien de consumo, y, por lo tanto, el "trabajo sexual" no es un trabajo auténtico.

Según Tony Mac, activista y trabajadora sexual, la regulación es uno de los caminos efectivos para la lucha contra la trata de personas y explotación infantil. Al proveer un marco regulado se accede a una inmensa cantidad de información y datos para entender las diferentes situaciones de las trabajadoras sexuales. Según Clara Rojas, el modelo regulacionista provee a las trabajadoras sexuales con derechos iguales a los del resto de trabajadores, acceso a los sistemas de salud y garantías frente a la autoridad y la justicia.

El modelo de Nueva Zelanda suele ser el referente legal de las organizaciones regulacionistas y legalizadoras. La legalización es defendida por Amnistía Internacional, así como sindicatos y colectivos profesionales de prostitutas, incluyendo a AMMAR en Argentina, a OTRAS, Hetaira y CATS en España, a la Asociación Nacional de las Prostitutas de Nigeria, al Comité Durbar Mahila Samanwaya en la India, a EMPOWER en Tailandia, a la Alianza Escarlata en Australia y a ICRSE y TAMPEP a nivel europeo.

Por regla general, las religiones que rechazan el sexo sin intención reproductiva condenan abiertamente la prostitución, aunque su actitud hacia las prostitutas puede estar sujeta a cambios a lo largo de la historia.

En la Edad Media y en la Edad Moderna, la simple fornicación, es decir, los actos sexuales entre solteros, son pecado, en eso no hay ninguna duda por parte de los teólogos, moralistas, políticos, canonistas, etc., pero indican todos ellos que es el menor dentro del sexto mandamiento, siendo los más graves la sodomía, el adulterio y la zoofilia, que precisamente la prostitución ayuda a evitar. Esto la exime de ser pecado, aunque la Iglesia nunca se pronunciará ni a su favor ni en su contra. La legalidad o ilegalidad de las mujeres públicas también va a influir bastante en la opinión que tengan la Iglesia y sus creyentes, ya que la noción de delito contiene una ineludible correspondencia con la noción de pecado.

La Iglesia católica, después de haber pasado por etapas de intransigencia total hacia las prostitutas, ahora incluso las consideran sometidas a una forma de esclavitud de la que deben ser liberadas. En el libro publicado en 2010, cuyo autor es el periodista alemán Peter Seewald, titulado "La luz del mundo. El Papa, la iglesia y las señales del tiempo", el Papa Benedicto XVI admite el uso de preservativos en determinados usos como, por ejemplo, la prostitución.
"En España durante el siglo XVII se empiezan a fundar las llamadas “Casas de Arrepentidas”, instituciones destinadas a recoger a las prostitutas que quisieran abandonar voluntariamente el oficio para proporcionarles alimento y buscarles una ocupación; a veces, hasta pagando a hombres para que se casaran con ellas y así poder abandonar su vida anterior. Con la llegada de la Compañía de Jesús, estas instituciones se extienden por toda España (Madrid, Salamanca, Málaga, Córdoba, Sevilla…).

Del mismo modo aparecen organizaciones dedicadas a lo opuesto, las “Casas de Corrección”, destinadas a “mujeres de mala conducta o públicas pecadoras, donde el ingreso es forzoso y cuyo objetivo es acabar con la delincuencia femenina”. Felipe IV, el rey que presenció estos actos, contribuyó junto a la Iglesia a la realización de estos nuevos cuerpos, aunque, según Eva Carrasco, los límites entre obligación y voluntariedad eran poco claros.

Las posiciones y leyes sobre la prostitución varían ampliamente en diferentes países, reflejando distintas visiones de la victimización, explotación social, explotación laboral, desigualdad social, roles de género, igualdad de género, ética y moralidad, libertad de elección y normas sociales.

Actualmente, la prostitución es completamente ilegal en la mayoría de países. Los aspectos perseguidos y las penas varían notablemente, pudiendo ir desde la infracción administrativa con multa hasta la persecución penal con penas de prisión o incluso muerte. En otros casos, la prostitución no es ilegal, pero sí el proxenetismo.

En otros casos, la prostitución puede ser considerada una forma de explotación a abolir. Es la posición conocida como "modelo nórdico", por su adopción en Suecia, Noruega e Islandia, donde es ilegal comprar servicios sexuales pero no venderlos. Es decir, el cliente comete un crimen, pero no la prostituta.

Por último, algunos países consideran la prostitución una actividad legítima y está regulada como una profesión, como ocurre en Países Bajos o Alemania.

En 1949, la Asamblea General de las Naciones Unidas adoptó el Convenio para la represión de la trata de personas y de la explotación de la prostitución ajena declarando que "«la prostitución y el mal que la acompaña, la trata de personas para fines de prostitución, son incompatibles con la dignidad y el valor de la persona humana y ponen en peligro el bienestar del individuo, de la familia y de la comunidad.»" A fecha de 4 de junio de 2017, el convenio ha sido ratificado por 82 países.

La mayoría de países no tienen cifras oficiales del número de personas que ejercen la prostitución y, cuando estas existen, debido a la clandestinidad, suelen ser inferiores al número real. La Fondation Scelles, tras un estudio de la prostitución en 38 países en 2016 llegó a una estimación vaga a nivel global: «decenas de millones».

El porcentaje estimado de mujeres que ejercen la prostitución oscila desde el 0'1% hasta el 7'5%, dependiendo del país.

En cuanto a la prostitución forzada, según la Organización Internacional del Trabajo, el 98% de las víctimas son mujeres y niñas y el 2% hombres y niños. El 79% tienen 18 años o más, mientras que el 21% tienen 17 años o menos.

Ya que las prostitutas y los prostitutos mantienen habitualmente relaciones con un elevado número de clientes, la prostitución se asocia con la dispersión de infecciones de transmisión sexual. Entre éstas, el VIH es la que actualmente reviste un mayor riesgo. Dicha enfermedad está considerablemente más presente entre los hombres y las mujeres transexuales que ejercen la prostitución.

La mayoría de las prostitutas son víctimas de agresiones físicas o violaciones. Además, a esta violencia se asocia el desarrollo de estrés postraumático y abuso de drogas.



</doc>
